{"id": 203951869, "updated": "2023-10-06 22:19:49.729", "metadata": {"title": "FedMD: Heterogenous Federated Learning via Model Distillation", "authors": "[{\"first\":\"Daliang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Junpu\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 10, "day": 8}, "abstract": "Federated learning enables the creation of a powerful centralized model without compromising data privacy of multiple participants. While successful, it does not incorporate the case where each participant independently designs its own model. Due to intellectual property concerns and heterogeneous nature of tasks and data, this is a widespread requirement in applications of federated learning to areas such as health care and AI as a service. In this work, we use transfer learning and knowledge distillation to develop a universal framework that enables federated learning when each agent owns not only their private data, but also uniquely designed models. We test our framework on the MNIST/FEMNIST dataset and the CIFAR10/CIFAR100 dataset and observe fast improvement across all participating models. With 10 distinct participants, the final test accuracy of each model on average receives a 20% gain on top of what's possible without collaboration and is only a few percent lower than the performance each model would have obtained if all private datasets were pooled and made directly available for all participants.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1910.03581", "mag": "2980216952", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1910-03581", "doi": null}}, "content": {"source": {"pdf_hash": "0a9945cc7ce7f98403358d0c74e9aa2da34e8089", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1910.03581v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a0e94c9258d5f80da47b0e04a52b6fd45974735e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0a9945cc7ce7f98403358d0c74e9aa2da34e8089.txt", "contents": "\nFedMD: Heterogenous Federated Learning via Model Distillation\n\n\nDaliang Li daliang_li@fas.harvard.edu \nCenter for Fundamental Laws of Nature\nHarvard University\n\n\nJunpu Wang junpuwang@gmail.com \nHigh Energy Theory Group\nYale University\n\n\nCenter for Particle Cosmology\nUniversity of Pennsylvania\n\n\nFedMD: Heterogenous Federated Learning via Model Distillation\n\nFederated learning enables the creation of a powerful centralized model without compromising the data privacy of multiple participants. While successful, it does not incorporate the case where each participant independently designs its own model. Due to intellectual property concerns and heterogeneous nature of tasks and data, this is a widespread requirement in applications of federated learning to areas such as health care and AI as a service. In this work, we use transfer learning and knowledge distillation to develop a universal framework that enables federated learning when each agent owns not only their private data, but also uniquely designed models. We test our framework on the MNIST/FEMNIST dataset and the CIFAR10/CIFAR100 dataset and observe fast improvement across all participating models. With 10 distinct participants, the final test accuracy of each model on average receives a 20% gain on top of what's possible without collaboration and is only a few percent lower than the performance each model would have obtained if all private datasets were pooled and made directly available for all participants.\n\nIntroduction\n\nDeep learning has provided a potentially powerful framework to automate perception and inference. However, large datasets are required to fully realize this potential. In areas like health care, it is often difficult and costly to curate large datasets. For instance, typical hospitals in the US may have only dozens of MRI images of a particular disease that needs to be annotated by human experts and must be protected from potential privacy breaches. Federated learning and similar ideas [1,2] rise to this challenge and effectively train a centralized model while keeping users' sensitive data on device. In particular, federated learning [1,3,4] is optimized for faster communication and is uniquely capable of handling a large number of users.\n\nFederated learning faces many challenges [5], among which, of particular importance is the heterogeneity that appear in all aspects of the learning process. There is system heterogeneity when each participant has a different amount of bandwidth and computational power; this was partly resolved by the native asynchronous scheme of federated learning, which was further refined e.g. to enable active sampling [6,7] and improve fault tolerance [8]. There is also statistical heterogneity (the non i.i.d. problem) where clients have a varying amount of data coming from distinct distributions [9,10,11,12,13,14].\n\nIn this work, we focus on a different type of heterogeneity: the differences of local models. In the original federated framework, all users have to agree on the particular architecture of a centralized model. This is a reasonable assumption when the participants are millions of low capacity devices such as cell phones. In this work, we instead explore extensions to the federated framework that is realistic in a business facing setting, where each participant has capacity and desire to design their 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. own unique model. This arise in areas like health care, finance, supply chain and AI services. For example, when several medical institutions collaborate without sharing private data, they may need to craft their own model to meet distinct specifications. They may not be willing to share details of their models due to privacy and intellectual property concerns. Another example is AI as a service. A typical AI vendor of, e.g. customer service chat bots, may have dozens of client companies. Each client's model is distinct and solves different tasks. The standard practice is to train a client's model with only its own data. It would be immensely beneficial if data from other clients can be utilized without compromising privacy or independency. How can one perform federated learning when each participant has a different model that is a blackbox to others? This is the central question that we will answer in this work.\n\nThis question is intimately related to the non-i.i.d. challenge of federated learning because a natural way to tackle statistical heterogeneity is to have individualized models for each user. Indeed, existing frameworks result in sightly different models. For example, [10] provides a framework for multitask learning if the problem is convex. Approaches based on frameworks such as Bayesian [11], meta-learning [12] and transfer learning [14] also achieve good performance on non-i.i.d. data while allowing a certain amount of model customization. However, to our knowledge, all existing frameworks require a centralized control over the design of local models. Full model independency, while related to the non-i.i.d. problem, is an important new research direction in its own right.\n\nThe key to full model heterogeneity is communication. In particular, there must be a translation protocol enabling a deep network to understand the knowledge of others without sharing data or model architecture. This question touches on fundamental issues in deep learning, such as interpretability and emergent communication protocols. In principle, machines should be able to learn the best communication protocol that is adaptive to any specific use case. As a first step in this direction, we employ a more transparent framework based on knowledge distillation that solves the problem.\n\nTransfer learning is another major framework addressing the scarcity of private data. In this work, our private datasets can be as small as a few samples per class. Therefore using transfer learning from a large public dataset is imperative in addition to federated learning. We leverage the power of transfer learning in two ways. First, before entering the collaboration, each model is fully trained first on the public data and then on its own private data. Second, and more importantly, the blackbox models communicate based on their output class scores on samples from the public dataset. This is realized through knowledge distillation [15], which has been capable of transmitting learned information in a model agnostic way.\n\n\nContributions:\n\nThe primary contribution of this work is FedMD, a new federated learning framework that enables participants to independently design their models. Our centralized server does not control the architecture of these models and only requires limited black box access. We identify the key element of this framework to be the communication module that translates knowledge between participants. We implement such a communication protocol by leveraging the power of transfer learning and knowledge distillation. We test this framework using a subset of the FEMNIST dataset [16] and the CIFAR10/CIFAR100 datasets [17]. We find significant gains in performance of local models using this framework compared to what's possible without collaboration.\n\n\nMethods\n\nWe propose the following challenge:\n\n\nProblem definition\n\nThere are m participants in the federated learning process. Each owns a very small labeled dataset\nD k := {(x k i , y i )} N k i=1\nthat may or may not be drawn from the same distribution. There is also a large public dataset\nD 0 := {(x 0 i , y 0 i )} N0 i=1\nthat everyone can access. Each participant independently designs its own model f k to perform a classification task. The models f k can have different architectures. Furthermore, hyper-parameters need not to be shared among participants. The goal is to establish a framework of collaboration that improves the performance of f k beyond individual effort with locally accessible data D 0 and D k . Figure 1: A general framework for heterogeneous federated learning. Each agent owns a private dataset and a uniquely designed model. To communicate and collaborate without data leakage, the agents need to translate their learned knowledge to a standard format. A central server collects these knowledges, compute a consensus distributed across the network. In this work, the translator is implemented using knowledge distillation.\n\n\nThe framework for heterogeneous federated learning\n\nWe propose FedMD, Algorithm 1, that solves the problem stated in sec 2.1. We comment on key components of this framework. Revisit: Each party trains its model f k on its own private data for a few epochs. end Transfer learning: Before a participant starts the collaboration phase, its model must first undergo the entire transfer learning process. It will be trained fully on the public dataset and then on its own private data. Therefore any future improvements are compared to this baseline.\n\nCommunication: We re-purpose the public dataset D 0 as the basis of communication between models, which is realized using knowledge distillation. Each learner f k expresses its knowledge by sharing the class scores, f k (x 0 i ), computed on the public dataset D 0 . The central server collects these class scores and computes an averagef (x 0 i ). Each party then trains f k to approach the consensus f (x 0 i ). In this way, the knowledge of one participant can be understood by others without explicitly sharing its private data or model architecture. Using the entire large public dataset can cause a large communication burden. In practice, the server may randomly select a much smaller subset d j \u2282 D 0 at each round as the basis of communication. In this way, the cost is under control and does not scale with the complexity of participating models.\n\n\nResults\n\nWe test this framework in two different environments. In the first environment, the public data is the MNIST and the private data is a subset of the FEMNIST. We consider the i.i.d. case where each private dataset is drawn randomly from FEMNIST, as well as the non-.i.i.d. case where each participant, while only given letters written by a single writer during training, is asked to classify letters by all writers at test time.\n\nIn the second environment, the public dataset is the CIFAR10 and the private dataset is a subset of the CIFAR100, which has 100 subclasses that falls under 20 superclasses, e.g. bear, leopard, lion, tiger and wolf belongs to large carnivores. In the i.i.d. case, the task is for each participant to classify test images into correct subclasses. The non-i.i.d. case is more challenging: during training, each participant has data from one subclass per superclass; at test time, participants need to classify generic test data into the correct superclasses. For example, a participant who has only seen wolfs during training is expected to classify lions correctly as large carnivores. Therefore it has to rely on information communicated by other participants.\n\nIn each environment, 10 participants design unique convolution networks that can differ by number of channels and number of layers, see Table 1,2 for details. First they are trained on the public dataset until convergence, -these models typically have test accuracy around 99% on MNIST and 76% on CIFAR10. Secondly each participant trains its model on its own small private dataset. After these steps, they go through the collaborative training phase, during which the models acquire strong and fast improvements across the board, and quickly outperform the baseline of transfer learning. We use Adam optimizer [18] with an initial learning rate of 0.001; in each round of collaborative training we randomly select a subset d j \u2282 D 0 of size 5000 as the basis for communication. More details are given in the supplementary material. The code will be made publicly available after the workshop. \n\n\nDiscussion and conclusion\n\nIn this work we proposed FedMD, a framework that enables federated learning for independently designed models. Our framework is based on knowledge distillation and is tested to work on various tasks and datasets. In future we will explore more sophisticated communication module, such as feature transformations and emergent communication protocols that will further improve the performance of our framework. Our framework can also be applied to tasks involving NLP and reinforcement learning. We will extend our framework to extreme cases of heterogeneity involving large discrepancies in the amounts of data, in model capacities and very different local tasks. We believe that heterogeneous federated learning will be an essential tool in future in a broad spectrum of business facing applications of deep learning. \n\n\nMethod\n\nWe clarify important details about our implementation of Algorithm 1:\n\n1. In the communication phase, the models communicate and align their logits computed from public data without applying the softmax activation layer . We could also use the softmax score with a particular temperature [15], and we do not expect large effects from this distinction. 2. In the communication phase, instead of using the entire public dataset, we use a subset of size 5000 that is randomly selected at each round. This speeds up the process without sacrificing the performance. 3. The number of rounds and the batch size in the Digest and the Revisit phase control the stability of the learning process. A model may undergo transient retrogression in test performance that is quickly recovered in the next couple of rounds. This issue can be resolved by choosing smaller number of epochs in the revisit phase and larger batch size in the digest phase.\n\n4. In principle the consensus can be computed using a weighted averagef (x 0 i ) = k c k f k (x 0 i ). In this work we almost always choose the weights c k to be equal to 1/N parties . One exception is in the CIFAR case where we slightly suppress the contribution from two weaker models (0 and 9). These weights may become more important when we have extremely different models or data.\n\n\nResults\n\nWe discuss several interesting aspects of our results.\n\n1. We measure our results against the test accuracy that a model could have achieved if the private data of all participants were pooled and made directly available to the whole group. See Table 4. Usually our framework boosts the performance of all participants to a level only a few percent lower than this pooled data performance. 2. There are isolated cases where a model trained in our framework consistently outperforms the same model trained with pooled private data. In particular model-0 in the CIFAR non-i.i.d. case. Besides, its performance is mostly on the top of the herd. This model has the simplest architecture and is usually lagging behind its more sophisticated peers. It is interesting to understand the mechanism behind this success and utilize it to improve our framework. 3. Our framework can incorporate extreme cases of model heterogeneity. We have experimented with several models having much lower performance, such as two layer fully connected networks. If they contribute to the consensus with the same weight as the advanced models, they tend to hinder the accuracy of the herd. Our framework works better if we suppress their contribution with a lower weight.   \n\nAlgorithm 1 :\n1The FedMD framework enabling federated learning for heterogeneous models. Input: Public dataset D 0 , private datasets D k , independently designed model f k , k = 1 . . . m, Output: Trained model f k Transfer learning: Each party trains f k to convergence on the public D 0 and then on its private D k . for j=1,2...P do Communicate: Each party computes the class scores f k (x 0 i ) on the public dataset, and transmits the result to a central server. Aggregate: The server computes an updated consensus, which is an averag\u1ebd f (x 0 i ) = 1 m k f k (x 0 i ). Distribute: Each party downloads the updated consensusf (x 0 i ). Digest: Each party trains its model f k to approach the consensusf on the public dataset D 0 .\n\nFigure 2 :\n2FedMD improves the test accuracy of participating models beyond their baselines. A dashed line (on the left) represents the test accuracy of a model after full transfer learning with the public dataset and its own small private dataset. This baseline is our starting point and overlaps with the beginning of the corresponding learning curve. A dash-dot line (on the right) represents the would-be performance of a model if private datasets from all participants were declassified and made available to every participant of the group.\n\nTable 3 :\n3Summary of datasetsCollaborative Task \nPublic Dataset Private Classes \nNumber of Private \nData Samples per \nClass per Party \nFEMNIST/MNIST \nI.I.D. \n\nMNIST \nletters [a-f] classes \n3 \n\nFEMNIST/MNIST \nNon I.I.D. \n\nMNIST \nletters from one writers \naround 20 (varies) \n\nCIFAR I.I.D. \nCIFAR10 \nCIFAR100 \nsubclasses \n[0,2,20,63,71,82] \n\n3 \n\nCIFAR Non I.I.D. \nCIFAR10 \nCIFAR100 superclasses [0-\n5] \n\n20 \n\n\n\nTable 4 :\n4Performance of models trained with pooled private data.Collaborative TaskEach model's performance trained with pooled private data FEMNIST/MNIST I.I.D.\nAcknowledgmentsWe would like to thank Ethan Dyer, Jared Kaplan, Jaehoon Lee, Patrick (Langechuan) Liu, Sam McCandlish, Wenbo Shi, Gennady Voronov, Yunlong Wang, Sho Yaida, Xi Yin and Yao Zhao for discussions and comments on the manuscript. DL was supported by the Simons Collaboration Grant on the Non-Perturbative Bootstrap.Supplementary MaterialWe provide more details about the models, the datasets, the algorithm and the results in this supplementary material.ModelsWe list the architectures of the models used by each participant in the MNIST/FEMNIST environment in table 1 and those in the CIFAR enrionment in table 2.DataWe provide a summary of our public and private datasets in table 3.\nCommunication-efficient learning of deep networks from decentralized data. H B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, Conference on Artificial Intelligence and Statistics. H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \"Communication-efficient learning of deep networks from decentralized data,\" in Conference on Artificial Intelligence and Statistics. 2017.\n\nPrivacy-preserving deep learning. R Shokri, V Shmatikov, http:/doi.acm.org/10.1145/2810103.2813687Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security, CCS '15. the 22Nd ACM SIGSAC Conference on Computer and Communications Security, CCS '15New York, NY, USAACMR. Shokri and V. Shmatikov, \"Privacy-preserving deep learning,\" in Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security, CCS '15, pp. 1310-1321. ACM, New York, NY, USA, 2015. http://doi.acm.org/10.1145/2810103.2813687.\n\nTowards federated learning at scale: system design. K Bonawitz, H Eichner, W Grieskamp, D Huba, A Ingerman, V Ivanov, C Kiddon, J Konecny, S Mazzocchi, H B Mcmahan, T V Overveldt, D Petrou, D Ramage, J Roselander, Conference on Systems and Machine Learning. K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, T. V. Overveldt, D. Petrou, D. Ramage, and J. Roselander, \"Towards federated learning at scale: system design,\" in Conference on Systems and Machine Learning. 2019.\n\n. A I Webank, Group, Federated learning white paper v1.0,\"WeBank AI Group, \"Federated learning white paper v1.0,\".\n\nFederated learning: Challenges, methods, and future directions. T Li, A K Sahu, A Talwalkar, V Smith, T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, \"Federated learning: Challenges, methods, and future directions,\" 2019.\n\nClient selection for federated learning with heterogeneous resources in mobile edge. T Nishio, R Yonetani, International Conference on Communications. T. Nishio and R. Yonetani, \"Client selection for federated learning with heterogeneous resources in mobile edge,\" in International Conference on Communications. 2019.\n\nIncentive design for efficient federated learning in mobile networks: A contract theory approach. J Kang, Z Xiong, D Niyato, H Yu, Y.-C Liang, D I Kim, arXiv:1905.07479arXiv preprintJ. Kang, Z. Xiong, D. Niyato, H. Yu, Y.-C. Liang, and D. I. Kim, \"Incentive design for efficient federated learning in mobile networks: A contract theory approach,\" arXiv preprint arXiv:1905.07479 (2019) .\n\nFederated optimization for heterogeneous networks. T Li, A K Sahu, M Sanjabi, M Zaheer, A Talwalkar, V Smith, arXiv:1812.06127arXiv preprintT. Li, A. K. Sahu, M. Sanjabi, M. Zaheer, A. Talwalkar, and V. Smith, \"Federated optimization for heterogeneous networks,\" arXiv preprint arXiv:1812.06127 (2018) .\n\nFederated meta-learning for recommendation. F Chen, Z Dong, Z Li, X He, arXiv:1802.07876arXiv preprintF. Chen, Z. Dong, Z. Li, and X. He, \"Federated meta-learning for recommendation,\" arXiv preprint arXiv:1802.07876 (2018) .\n\nFederated multi-task learning. V Smith, C.-K Chiang, M Sanjabi, A Talwalkar, Advances in Neural Information Processing Systems. V. Smith, C.-K. Chiang, M. Sanjabi, and A. Talwalkar, \"Federated multi-task learning,\" in Advances in Neural Information Processing Systems. 2017.\n\nVariational federated multi-task learning. L Corinzia, J M Buhmann, arXiv:1906.06268arXiv preprintL. Corinzia and J. M. Buhmann, \"Variational federated multi-task learning,\" arXiv preprint arXiv:1906.06268 (2019) .\n\nAdaptive gradient-based meta-learning methods. M Khodak, M.-F Balcan, A Talwalkar, arXiv:1906.02717arXiv preprintM. Khodak, M.-F. Balcan, and A. Talwalkar, \"Adaptive gradient-based meta-learning methods,\" arXiv preprint arXiv:1906.02717 (2019) .\n\nSemi-cyclic stochastic gradient descent. H Eichner, T Koren, H B Mcmahan, N Srebro, K Talwar, International Conference on Machine Learning. H. Eichner, T. Koren, H. B. McMahan, N. Srebro, and K. Talwar, \"Semi-cyclic stochastic gradient descent,\" in International Conference on Machine Learning. 2019.\n\nFederated learning with non-iid data. Y Zhao, M Li, L Lai, N Suda, D Civin, V Chandra, arXiv:1806.00582arXiv preprintY. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, \"Federated learning with non-iid data,\" arXiv preprint arXiv:1806.00582 (2018) .\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, NIPS Deep Learning and Representation Learning Workshop. G. Hinton, O. Vinyals, and J. Dean, \"Distilling the knowledge in a neural network,\" in NIPS Deep Learning and Representation Learning Workshop. 2015. http://arxiv.org/abs/1503.02531.\n\nLeaf: A benchmark for federated settings. S Caldas, P Wu, T Li, J Kone\u010dn\u1ef3, H B Mcmahan, V Smith, A Talwalkar, arXiv:1812.01097arXiv preprintS. Caldas, P. Wu, T. Li, J. Kone\u010dn\u1ef3, H. B. McMahan, V. Smith, and A. Talwalkar, \"Leaf: A benchmark for federated settings,\" arXiv preprint arXiv:1812.01097 (2018) .\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, University of TorontoTechnical reportA. Krizhevsky and G. Hinton, \"Learning multiple layers of features from tiny images,\" Technical report, University of Toronto (2009) .\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, CoRR abs/1412.6980D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization.,\" CoRR abs/1412.6980 (2014) .\n", "annotations": {"author": "[{\"end\":162,\"start\":65},{\"end\":296,\"start\":163}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":73},{\"end\":173,\"start\":169}]", "author_first_name": "[{\"end\":72,\"start\":65},{\"end\":168,\"start\":163}]", "author_affiliation": "[{\"end\":161,\"start\":104},{\"end\":236,\"start\":195},{\"end\":295,\"start\":238}]", "title": "[{\"end\":62,\"start\":1},{\"end\":358,\"start\":297}]", "venue": null, "abstract": "[{\"end\":1489,\"start\":360}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1999,\"start\":1996},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2001,\"start\":1999},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2151,\"start\":2148},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2153,\"start\":2151},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2155,\"start\":2153},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2300,\"start\":2297},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2668,\"start\":2665},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2670,\"start\":2668},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2702,\"start\":2699},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2850,\"start\":2847},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2853,\"start\":2850},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2856,\"start\":2853},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2859,\"start\":2856},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2862,\"start\":2859},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2865,\"start\":2862},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4665,\"start\":4661},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4788,\"start\":4784},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4808,\"start\":4804},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4835,\"start\":4831},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6416,\"start\":6412},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7090,\"start\":7086},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7129,\"start\":7125},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11637,\"start\":11633},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13067,\"start\":13063}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16094,\"start\":15358},{\"attributes\":{\"id\":\"fig_1\"},\"end\":16641,\"start\":16095},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":17051,\"start\":16642},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":17215,\"start\":17052}]", "paragraph": "[{\"end\":2254,\"start\":1505},{\"end\":2866,\"start\":2256},{\"end\":4390,\"start\":2868},{\"end\":5177,\"start\":4392},{\"end\":5768,\"start\":5179},{\"end\":6501,\"start\":5770},{\"end\":7259,\"start\":6520},{\"end\":7306,\"start\":7271},{\"end\":7427,\"start\":7329},{\"end\":7553,\"start\":7460},{\"end\":8414,\"start\":7587},{\"end\":8962,\"start\":8469},{\"end\":9820,\"start\":8964},{\"end\":10259,\"start\":9832},{\"end\":11020,\"start\":10261},{\"end\":11916,\"start\":11022},{\"end\":12764,\"start\":11946},{\"end\":12844,\"start\":12775},{\"end\":13709,\"start\":12846},{\"end\":14097,\"start\":13711},{\"end\":14163,\"start\":14109},{\"end\":15357,\"start\":14165}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7459,\"start\":7428},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7586,\"start\":7554}]", "table_ref": "[{\"end\":11165,\"start\":11158},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14361,\"start\":14354}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1503,\"start\":1491},{\"end\":6518,\"start\":6504},{\"attributes\":{\"n\":\"2\"},\"end\":7269,\"start\":7262},{\"attributes\":{\"n\":\"2.1\"},\"end\":7327,\"start\":7309},{\"attributes\":{\"n\":\"2.2\"},\"end\":8467,\"start\":8417},{\"attributes\":{\"n\":\"3\"},\"end\":9830,\"start\":9823},{\"attributes\":{\"n\":\"4\"},\"end\":11944,\"start\":11919},{\"end\":12773,\"start\":12767},{\"end\":14107,\"start\":14100},{\"end\":15372,\"start\":15359},{\"end\":16106,\"start\":16096},{\"end\":16652,\"start\":16643},{\"end\":17062,\"start\":17053}]", "table": "[{\"end\":17051,\"start\":16673}]", "figure_caption": "[{\"end\":16094,\"start\":15374},{\"end\":16641,\"start\":16108},{\"end\":16673,\"start\":16654},{\"end\":17215,\"start\":17064}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7992,\"start\":7984}]", "bib_author_first_name": "[{\"end\":17988,\"start\":17987},{\"end\":17990,\"start\":17989},{\"end\":18001,\"start\":18000},{\"end\":18010,\"start\":18009},{\"end\":18020,\"start\":18019},{\"end\":18031,\"start\":18030},{\"end\":18033,\"start\":18032},{\"end\":18338,\"start\":18337},{\"end\":18348,\"start\":18347},{\"end\":18902,\"start\":18901},{\"end\":18914,\"start\":18913},{\"end\":18925,\"start\":18924},{\"end\":18938,\"start\":18937},{\"end\":18946,\"start\":18945},{\"end\":18958,\"start\":18957},{\"end\":18968,\"start\":18967},{\"end\":18978,\"start\":18977},{\"end\":18989,\"start\":18988},{\"end\":19002,\"start\":19001},{\"end\":19004,\"start\":19003},{\"end\":19015,\"start\":19014},{\"end\":19017,\"start\":19016},{\"end\":19030,\"start\":19029},{\"end\":19040,\"start\":19039},{\"end\":19050,\"start\":19049},{\"end\":19400,\"start\":19399},{\"end\":19402,\"start\":19401},{\"end\":19578,\"start\":19577},{\"end\":19584,\"start\":19583},{\"end\":19586,\"start\":19585},{\"end\":19594,\"start\":19593},{\"end\":19607,\"start\":19606},{\"end\":19821,\"start\":19820},{\"end\":19831,\"start\":19830},{\"end\":20153,\"start\":20152},{\"end\":20161,\"start\":20160},{\"end\":20170,\"start\":20169},{\"end\":20180,\"start\":20179},{\"end\":20189,\"start\":20185},{\"end\":20198,\"start\":20197},{\"end\":20200,\"start\":20199},{\"end\":20495,\"start\":20494},{\"end\":20501,\"start\":20500},{\"end\":20503,\"start\":20502},{\"end\":20511,\"start\":20510},{\"end\":20522,\"start\":20521},{\"end\":20532,\"start\":20531},{\"end\":20545,\"start\":20544},{\"end\":20793,\"start\":20792},{\"end\":20801,\"start\":20800},{\"end\":20809,\"start\":20808},{\"end\":20815,\"start\":20814},{\"end\":21006,\"start\":21005},{\"end\":21018,\"start\":21014},{\"end\":21028,\"start\":21027},{\"end\":21039,\"start\":21038},{\"end\":21294,\"start\":21293},{\"end\":21306,\"start\":21305},{\"end\":21308,\"start\":21307},{\"end\":21514,\"start\":21513},{\"end\":21527,\"start\":21523},{\"end\":21537,\"start\":21536},{\"end\":21755,\"start\":21754},{\"end\":21766,\"start\":21765},{\"end\":21775,\"start\":21774},{\"end\":21777,\"start\":21776},{\"end\":21788,\"start\":21787},{\"end\":21798,\"start\":21797},{\"end\":22054,\"start\":22053},{\"end\":22062,\"start\":22061},{\"end\":22068,\"start\":22067},{\"end\":22075,\"start\":22074},{\"end\":22083,\"start\":22082},{\"end\":22092,\"start\":22091},{\"end\":22320,\"start\":22319},{\"end\":22330,\"start\":22329},{\"end\":22341,\"start\":22340},{\"end\":22632,\"start\":22631},{\"end\":22642,\"start\":22641},{\"end\":22648,\"start\":22647},{\"end\":22654,\"start\":22653},{\"end\":22665,\"start\":22664},{\"end\":22667,\"start\":22666},{\"end\":22678,\"start\":22677},{\"end\":22687,\"start\":22686},{\"end\":22951,\"start\":22950},{\"end\":22965,\"start\":22964},{\"end\":23192,\"start\":23191},{\"end\":23194,\"start\":23193},{\"end\":23204,\"start\":23203}]", "bib_author_last_name": "[{\"end\":17998,\"start\":17991},{\"end\":18007,\"start\":18002},{\"end\":18017,\"start\":18011},{\"end\":18028,\"start\":18021},{\"end\":18039,\"start\":18034},{\"end\":18345,\"start\":18339},{\"end\":18358,\"start\":18349},{\"end\":18911,\"start\":18903},{\"end\":18922,\"start\":18915},{\"end\":18935,\"start\":18926},{\"end\":18943,\"start\":18939},{\"end\":18955,\"start\":18947},{\"end\":18965,\"start\":18959},{\"end\":18975,\"start\":18969},{\"end\":18986,\"start\":18979},{\"end\":18999,\"start\":18990},{\"end\":19012,\"start\":19005},{\"end\":19027,\"start\":19018},{\"end\":19037,\"start\":19031},{\"end\":19047,\"start\":19041},{\"end\":19061,\"start\":19051},{\"end\":19409,\"start\":19403},{\"end\":19416,\"start\":19411},{\"end\":19581,\"start\":19579},{\"end\":19591,\"start\":19587},{\"end\":19604,\"start\":19595},{\"end\":19613,\"start\":19608},{\"end\":19828,\"start\":19822},{\"end\":19840,\"start\":19832},{\"end\":20158,\"start\":20154},{\"end\":20167,\"start\":20162},{\"end\":20177,\"start\":20171},{\"end\":20183,\"start\":20181},{\"end\":20195,\"start\":20190},{\"end\":20204,\"start\":20201},{\"end\":20498,\"start\":20496},{\"end\":20508,\"start\":20504},{\"end\":20519,\"start\":20512},{\"end\":20529,\"start\":20523},{\"end\":20542,\"start\":20533},{\"end\":20551,\"start\":20546},{\"end\":20798,\"start\":20794},{\"end\":20806,\"start\":20802},{\"end\":20812,\"start\":20810},{\"end\":20818,\"start\":20816},{\"end\":21012,\"start\":21007},{\"end\":21025,\"start\":21019},{\"end\":21036,\"start\":21029},{\"end\":21049,\"start\":21040},{\"end\":21303,\"start\":21295},{\"end\":21316,\"start\":21309},{\"end\":21521,\"start\":21515},{\"end\":21534,\"start\":21528},{\"end\":21547,\"start\":21538},{\"end\":21763,\"start\":21756},{\"end\":21772,\"start\":21767},{\"end\":21785,\"start\":21778},{\"end\":21795,\"start\":21789},{\"end\":21805,\"start\":21799},{\"end\":22059,\"start\":22055},{\"end\":22065,\"start\":22063},{\"end\":22072,\"start\":22069},{\"end\":22080,\"start\":22076},{\"end\":22089,\"start\":22084},{\"end\":22100,\"start\":22093},{\"end\":22327,\"start\":22321},{\"end\":22338,\"start\":22331},{\"end\":22346,\"start\":22342},{\"end\":22639,\"start\":22633},{\"end\":22645,\"start\":22643},{\"end\":22651,\"start\":22649},{\"end\":22662,\"start\":22655},{\"end\":22675,\"start\":22668},{\"end\":22684,\"start\":22679},{\"end\":22697,\"start\":22688},{\"end\":22962,\"start\":22952},{\"end\":22972,\"start\":22966},{\"end\":23201,\"start\":23195},{\"end\":23207,\"start\":23205}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14955348},\"end\":18301,\"start\":17912},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/2810103.2813687\",\"id\":\"b1\",\"matched_paper_id\":20714},\"end\":18847,\"start\":18303},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":59599820},\"end\":19395,\"start\":18849},{\"attributes\":{\"id\":\"b3\"},\"end\":19511,\"start\":19397},{\"attributes\":{\"id\":\"b4\"},\"end\":19733,\"start\":19513},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5062760},\"end\":20052,\"start\":19735},{\"attributes\":{\"doi\":\"arXiv:1905.07479\",\"id\":\"b6\"},\"end\":20441,\"start\":20054},{\"attributes\":{\"doi\":\"arXiv:1812.06127\",\"id\":\"b7\"},\"end\":20746,\"start\":20443},{\"attributes\":{\"doi\":\"arXiv:1802.07876\",\"id\":\"b8\"},\"end\":20972,\"start\":20748},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3586416},\"end\":21248,\"start\":20974},{\"attributes\":{\"doi\":\"arXiv:1906.06268\",\"id\":\"b10\"},\"end\":21464,\"start\":21250},{\"attributes\":{\"doi\":\"arXiv:1906.02717\",\"id\":\"b11\"},\"end\":21711,\"start\":21466},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":128361946},\"end\":22013,\"start\":21713},{\"attributes\":{\"doi\":\"arXiv:1806.00582\",\"id\":\"b13\"},\"end\":22271,\"start\":22015},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7200347},\"end\":22587,\"start\":22273},{\"attributes\":{\"doi\":\"arXiv:1812.01097\",\"id\":\"b15\"},\"end\":22893,\"start\":22589},{\"attributes\":{\"id\":\"b16\"},\"end\":23145,\"start\":22895},{\"attributes\":{\"doi\":\"CoRR abs/1412.6980\",\"id\":\"b17\"},\"end\":23325,\"start\":23147}]", "bib_title": "[{\"end\":17985,\"start\":17912},{\"end\":18335,\"start\":18303},{\"end\":18899,\"start\":18849},{\"end\":19818,\"start\":19735},{\"end\":21003,\"start\":20974},{\"end\":21752,\"start\":21713},{\"end\":22317,\"start\":22273}]", "bib_author": "[{\"end\":18000,\"start\":17987},{\"end\":18009,\"start\":18000},{\"end\":18019,\"start\":18009},{\"end\":18030,\"start\":18019},{\"end\":18041,\"start\":18030},{\"end\":18347,\"start\":18337},{\"end\":18360,\"start\":18347},{\"end\":18913,\"start\":18901},{\"end\":18924,\"start\":18913},{\"end\":18937,\"start\":18924},{\"end\":18945,\"start\":18937},{\"end\":18957,\"start\":18945},{\"end\":18967,\"start\":18957},{\"end\":18977,\"start\":18967},{\"end\":18988,\"start\":18977},{\"end\":19001,\"start\":18988},{\"end\":19014,\"start\":19001},{\"end\":19029,\"start\":19014},{\"end\":19039,\"start\":19029},{\"end\":19049,\"start\":19039},{\"end\":19063,\"start\":19049},{\"end\":19411,\"start\":19399},{\"end\":19418,\"start\":19411},{\"end\":19583,\"start\":19577},{\"end\":19593,\"start\":19583},{\"end\":19606,\"start\":19593},{\"end\":19615,\"start\":19606},{\"end\":19830,\"start\":19820},{\"end\":19842,\"start\":19830},{\"end\":20160,\"start\":20152},{\"end\":20169,\"start\":20160},{\"end\":20179,\"start\":20169},{\"end\":20185,\"start\":20179},{\"end\":20197,\"start\":20185},{\"end\":20206,\"start\":20197},{\"end\":20500,\"start\":20494},{\"end\":20510,\"start\":20500},{\"end\":20521,\"start\":20510},{\"end\":20531,\"start\":20521},{\"end\":20544,\"start\":20531},{\"end\":20553,\"start\":20544},{\"end\":20800,\"start\":20792},{\"end\":20808,\"start\":20800},{\"end\":20814,\"start\":20808},{\"end\":20820,\"start\":20814},{\"end\":21014,\"start\":21005},{\"end\":21027,\"start\":21014},{\"end\":21038,\"start\":21027},{\"end\":21051,\"start\":21038},{\"end\":21305,\"start\":21293},{\"end\":21318,\"start\":21305},{\"end\":21523,\"start\":21513},{\"end\":21536,\"start\":21523},{\"end\":21549,\"start\":21536},{\"end\":21765,\"start\":21754},{\"end\":21774,\"start\":21765},{\"end\":21787,\"start\":21774},{\"end\":21797,\"start\":21787},{\"end\":21807,\"start\":21797},{\"end\":22061,\"start\":22053},{\"end\":22067,\"start\":22061},{\"end\":22074,\"start\":22067},{\"end\":22082,\"start\":22074},{\"end\":22091,\"start\":22082},{\"end\":22102,\"start\":22091},{\"end\":22329,\"start\":22319},{\"end\":22340,\"start\":22329},{\"end\":22348,\"start\":22340},{\"end\":22641,\"start\":22631},{\"end\":22647,\"start\":22641},{\"end\":22653,\"start\":22647},{\"end\":22664,\"start\":22653},{\"end\":22677,\"start\":22664},{\"end\":22686,\"start\":22677},{\"end\":22699,\"start\":22686},{\"end\":22964,\"start\":22950},{\"end\":22974,\"start\":22964},{\"end\":23203,\"start\":23191},{\"end\":23209,\"start\":23203}]", "bib_venue": "[{\"end\":18593,\"start\":18497},{\"end\":18093,\"start\":18041},{\"end\":18495,\"start\":18401},{\"end\":19105,\"start\":19063},{\"end\":19575,\"start\":19513},{\"end\":19884,\"start\":19842},{\"end\":20150,\"start\":20054},{\"end\":20492,\"start\":20443},{\"end\":20790,\"start\":20748},{\"end\":21100,\"start\":21051},{\"end\":21291,\"start\":21250},{\"end\":21511,\"start\":21466},{\"end\":21851,\"start\":21807},{\"end\":22051,\"start\":22015},{\"end\":22403,\"start\":22348},{\"end\":22629,\"start\":22589},{\"end\":22948,\"start\":22895},{\"end\":23189,\"start\":23147}]"}}}, "year": 2023, "month": 12, "day": 17}