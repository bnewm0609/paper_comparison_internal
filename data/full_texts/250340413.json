{"id": 250340413, "updated": "2023-12-14 07:45:08.097", "metadata": {"title": "Multi-Behavior Sequential Transformer Recommender", "authors": "[{\"first\":\"Enming\",\"last\":\"Yuan\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Zhicheng\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Huifeng\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Chengkai\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Ruiming\",\"last\":\"Tang\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In most real-world recommender systems, users interact with items in a sequential and multi-behavioral manner. Exploring the fine-grained relationship of items behind the users' multi-behavior interactions is critical in improving the performance of recommender systems. Despite the great successes, existing methods seem to have limitations on modelling heterogeneous item-level multi-behavior dependencies, capturing diverse multi-behavior sequential dynamics, or alleviating data sparsity problems. In this paper, we show it is possible to derive a framework to address all the above three limitations. The proposed framework MB-STR, a Multi-Behavior Sequential Transformer Recommender, is equipped with the multi-behavior transformer layer (MB-Trans), the multi-behavior sequential pattern generator (MB-SPG) and the behavior-aware prediction module (BA-Pred). Compared with a typical transformer, we design MB-Trans to capture multi-behavior heterogeneous dependencies as well as behavior-specific semantics, propose MB-SPG to encode the diverse sequential patterns among multiple behaviors, and incorporate BA-Pred to better leverage multi-behavior supervision. Comprehensive experiments on three real-world datasets show the effectiveness of MB-STR by significantly boosting the recommendation performance compared with various competitive baselines. Further ablation studies demonstrate the superiority of different modules of MB-STR.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sigir/YuanG0GLT22", "doi": "10.1145/3477495.3532023"}}, "content": {"source": {"pdf_hash": "718b8a48bf00784d6f05361fc0303ea4ff59fec5", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ad7956d7d06997c79a8ef4a4c026e90f8e9f6881", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/718b8a48bf00784d6f05361fc0303ea4ff59fec5.txt", "contents": "\nMulti-Behavior Sequential Transformer Recommender\nACMCopyright ACMJuly 11-15, 2022. 2022. July 11-15, 2022\n\nSpain Madrid \nTang \n\nInstitute for Interdisciplinary Information Sciences\nTsinghua University\nBeijingChina\n\n\nNoah's Ark Lab, Huawei Shenzhen\nShanghai Jiao Tong University Shanghai\nChina, China\n\nMulti-Behavior Sequential Transformer Recommender\n\nProceedings of the 45th Int'l ACM SIGIR Conference on Research and Develop-ment in Information Retrieval (SIGIR '22)\nthe 45th Int'l ACM SIGIR Conference on Research and Develop-ment in Information Retrieval (SIGIR '22)Madrid, Spain; New York, NY, USAACM11July 11-15, 2022. 2022. July 11-15, 202210.1145/3477495.3532023Noah's Ark Lab, Huawei Shenzhen, China Zhicheng He hezhicheng9@huawei.com Noah's Ark Lab, Huawei Shenzhen, China Huifeng Guo Noah's Ark Lab, Huawei Shenzhen, China Multi-Behavior Modeling, Sequential Recommendation, Transformer * Work done when he was a research intern at Noah's Ark Lab, Huawei. \u2020 Ruiming Tang is the corresponding author. This work is licensed under a Creative Commons Attribution International 4.0 License.\nIn most real-world recommender systems, users interact with items in a sequential and multi-behavioral manner. Exploring the finegrained relationship of items behind the users' multi-behavior interactions is critical in improving the performance of recommender systems. Despite the great successes, existing methods seem to have limitations on modelling heterogeneous item-level multi-behavior dependencies, capturing diverse multi-behavior sequential dynamics, or alleviating data sparsity problems. In this paper, we show it is possible to derive a framework to address all the above three limitations. The proposed framework MB-STR, a Multi-Behavior Sequential Transformer Recommender, is equipped with the multibehavior transformer layer (MB-Trans), the multi-behavior sequential pattern generator (MB-SPG) and the behavior-aware prediction module (BA-Pred). Compared with a typical transformer, we design MB-Trans to capture multi-behavior heterogeneous dependencies as well as behavior-specific semantics, propose MB-SPG to encode the diverse sequential patterns among multiple behaviors, and incorporate BA-Pred to better leverage multi-behavior supervision. Comprehensive experiments on three real-world datasets show the effectiveness of MB-STR by significantly boosting the recommendation performance compared with various competitive baselines. Further ablation studies demonstrate the superiority of different modules of MB-STR.CCS CONCEPTS\u2022 Information systems \u2192 Recommender systems.\n\nINTRODUCTION\n\nPersonalized recommender systems have been playing an important role in many online service platforms, from online advertising and online retailing [4,12,37] to music and video recommendation [5,6,35]. To provide precise and customized services, these systems attempt to recommend products that users are likely to be interested in based on their historical interaction data.\n\nIn most real-world recommendation scenarios, there are two important characteristics of user interaction data: sequential and multi-behavioral. Users interact with items in a sequential manner, and their interest patterns are intrinsically diverse and keep evolving [23,42,43]. Therefore, it is important to consider the informative sequential dynamics of user behaviors in the recommendation model. Additionally, users interact with items in a multibehavioral way. For example, interactions take many forms of behavior in an E-commerce platform, including click, add-to-favorite, add-to-cart, and purchase. Such multi-behavioral property brings us two significant benefits. First, different types of behaviors, such as click and purchase, reflect different user intentions. Therefore, the multi-behavior interaction data provides us with an opportunity to capture the fine-grained interest dynamic of users. Second, target behavior data (e.g., purchase on E-commerce platforms, which are conventionally the most concerned.) is usually very sparse. Severe cold-start problems will happen if we model target behavior data independently. Fortunately, this problem can be alleviated if we utilize the plentiful auxiliary behavior data reasonably.\n\nRecently, a bunch of work has been proposed to model user interaction data from either sequential (e.g., GRU4Rec [17], Caser [34], SASRec [20], BERT4Rec [33]) or multi-behavioral (e.g., NMTR [9], MATN [39], MB-GCN [19]) perspectives. However, despite the potential benefits of taking both sequential and multi-behavioral properties of user behavior into consideration, the multi-behavior sequential recommendation problem is still underexplored. Specifically, there are three major challenges making it a non-trivial problem:\n\nTopic 21: Sequential Recommendations SIGIR '22, July 11-15, 2022, Madrid, Spain \u2022 C1: One important characteristic of multi-behavior data is the behavior-specific semantics, since different behaviors reflect different user intentions. Moreover, items that interacted with different intentions will form complex fine-grained multi-behavior dependencies. For example, in Figure 1, the final purchasing decision is affected by other historical actions from different aspects, including the purchase of a matching shorts (arrow 1 ), the add-to-cart of the same T-shirt (arrow 2 ), and the recent click of other T-shirts (arrow 3 ). Previous work (e.g., MATN [39], MB-GCN [19], DIPN [13] and DMT [11]) follows a two-stage aggregation paradigm, which first aggregates items under each behavior to get a unified representation, and then models the dependencies for all behaviors through attention or weighted summation operation. However, it should be noted that these works only explicitly model the behavior-level dependencies (arrow 1 ) and fail to capture the fine-grained item-to-item relationship among multi-behavior (arrow 2 & 3 ), which we defined as item-level dependencies. Moreover, the heterogeneous properties of item relations should also be considered as there exist multiple behavior transition patterns if we model item-level dependencies. Therefore, Challenge 1 is: how to model heterogeneous multi-behavior dependencies at the fine-grained item-level? \u2022 C2: The dependencies across different types of user-item interactions become even more complex when we consider the sequential information. In Figure 1, take click and purchase as an example. Click represents short-term interest, so the recently clicked items (arrow 3 ) may impact the current purchasing decision, while the items clicked long ago usually have little influence on the current purchasing decision. However, for purchase, items purchased a long time ago may still strongly influence current interest (arrow 1 ). This leads to diverse multi-behavior sequential patterns. However, few existing multi-behavior models consider sequential information. Although DIPN [13] and DMT [11] aggregate features in a sequential manner, the sequential patterns among multiple types of behaviors are not distinguished, with a fixed single behavior pattern. Thus Challenge 2 is: how to model diverse multi-behavior sequential patterns effectively? \u2022 C3: The problem of target behavior data sparsity can be alleviated by integrating plentiful auxiliary behavior data. However, if we only use auxiliary behavior data as features instead of training signals (e.g., MATN [39], MBGCN [19]), rich supervision signals from auxiliary behavior are ignored. Moreover, simply using auxiliary behavior as supervision signals may lead to performance degradation and negative transfer due to the complex behavior correlation. Therefore, appropriate designs are required. So Challenge 3 is: how to effectively mine users' multi-behavior sequence with multi-behavior supervision signals?\n\nIn this paper, to address the above challenges, we propose Multi-Behavior Sequential Transformer Recommender (MB-STR) framework. First, to model the fine-grained multi-behavior dependencies in users' interaction sequences (C1), we design a novel Multi-Behavior Transformer layer (MB-Trans), which performs heterogeneous behavior aggregation and behavior-specific transformation. Second, to capture diverse multi-behavior sequential patterns (C2), a Multi-Behavior Sequential Pattern Generator (MB-SPG) is proposed. This module cooperates with the MB-Trans module to encode the diverse multi-behavior sequential patterns into attention bias matrices. Finally, to better leverage auxiliary data to facilitate target behavior prediction (C3), we propose the Behavior-Aware masked item Prediction module (BA-Pred), which can mine the multi-behavior interaction sequence effectively.\n\nIn summary, the major contributions of this work are as follows:\n\n\u2022 We highlight the sequential and multi-behavioral nature of realworld user interactions and summarize the challenges in the multi-behavior sequential recommendation problem. \u2022 To address the multi-behavior sequential recommendation problem, we propose a new framework named MB-STR, which has three key components. The heterogeneous item-level dependencies are modeled through a novel Multi-Behavior Transformer (MB-Trans) layer. Moreover, a multi-behavior sequential pattern generator module (MB-SPG) is incorporated to encode the diverse multi-behavior sequential pattern. Furthermore, a Behavior-Aware masked item Prediction module (BA-Pred) is used to facilitate the prediction of target behavior by training on both target behavior and auxiliary behavior data. \u2022 We conduct extensive experiments on three real-world datasets.\n\nExperimental results demonstrate the effectiveness of MB-STR as compared with several baseline models. Further ablation studies explain the superiority of our designed modules.\n\n\nRELATED WORK 2.1 Sequential Recommendation\n\nSequential recommenders are designed to model the sequential dynamics in user behaviors to which various sequence models have been applied, including Recurrent Neural Network (RNN) [28], Convolutional Neural Network (CNN) [22], and Transformer [36]. For example, Hidasi et al. use the Gated Recurrent Unit (GRU) network to model user click sequences [16,17]. By taking the sequence embedding matrix as an image, CNN is applied to extract user interest patterns in the Caser model [34]. Due to the high model capacity and effective training techniques, many Transformer-based sequential recommendation methods are proposed and achieve significant improvements, such as SASRec [20], TiSASRec [24], and BERT4Rec [33]. For the same reason, the proposed model in this paper is also based on Transformer, while many improvements are made for multi-behavior modeling.  \n\n\nMulti-Behavior Modeling\n\nSequential Information\n\n\nBehavior-Specific Prediction\n\nMATN [39] behavior-level NMTR [9] behavior-level MBGCN [19] behavior-level MB-GMN [40] behavior-level DIPN [13] behavior-level fixed single behavior DMT [11] behavior-level fixed single behavior MB-STR(our) heterogeneous item-level diverse multi-behavior\n\n\nMulti-Behavior Recommendation\n\nMulti-behavior recommendation utilizes multiple types of useritem interactions to enhance the performance regarding target behavior. Previous works can be divided into three categories. The first category of works considers the behavior dependencies and only uses the target behavior data as supervision signals such as the attentive memory network MATN [39] and the graph convolutional model MBGCN [19]. Another line of works further considers multitask learning to utilize both target and auxiliary behavior data for supervision, including the cascaded prediction model NMTR [9] and the multi-task graph meta network MB-GMN [40]. The last category of works considers both the sequential information and the supervision signals in multiple user behaviors, such as the multiview sequential network DIPN [13] and the MMoE-based [18,26] multi-sequence model DMT [11].\n\nAs shown in Table 1, we briefly compare the representative multi-behavior models and our MB-STR from three aspects:\n\n\u2022 Multi-behavior modeling: Models like MATN [39], MBGCN [19], DIPN [13] and DMT [11] follow a two-stage aggregation paradigm and only model coarse-grained behavior-level dependencies. Differently, our MB-STR can model heterogeneous item-level multi-behavior dependencies via the multi-behavior transformer. \u2022 Sequential information: Models like NMTR [9], MBGCN [19], MATN [39] and MB-GMN [40] do not consider sequential information. DIPN [13] and DMT [11] only capture the sequential pattern with a fixed single behavior. In MB-STR, we propose the MB-SPG component to model diverse multi-behavior sequential patterns. \u2022 Behavior-Specific Prediction: NMTR [9], MB-GMN [40], DIPN [13] and DMT [11] are trained in a multi-task manner with behavior specific prediction to utilize the rich supervision signals from auxiliary behaviors. However, they ignore the shared and taskspecific knowledge exists in multi-behavior recommendation. We further separate shared components and behavior-specific components explicitly to alleviate the conflicting parameter interference between different behaviors.\n\n\nPRELIMINARY 3.1 Problem Definition\n\nLet Then the multi-behavior sequential recommendation problem can be formulated as follows: Given the multi-behavior interaction sequences for all users X, the set of users U, the set of items V, and the set of behavior types B, we aim to train a model that takes multi-behavior interaction sequence ( ) of user as input, and estimates the probability that interacts with an item under the target behavior at time step + 1.\n\n\nTransformer in Recommendation\n\nImprovements in the field of Natural Language Processing have driven many recent advances in recommender systems, especially sequential recommendation [20,33]. In particular, equipped with higher model capacities and efficient training techniques, the Transformer architecture [36] significantly improves the effectiveness of sequential recommendation.\n\nAs the Transformer architecture is highly relevant to our work, we briefly describe the key components of Transformer here, including Multi-head Self-Attention (MSA) and Multi-Layer Perceptron (MLP). For a sequence input, MSA and MLP perform whole sequence aggregation and position-wise transformation, respectively.\n\n\nMSA.\n\nAt each head of the MSA, the inputs X \u2208 R \u00d7 are linearly transformed to three hidden representations, i.e., queries Q \u2208 R \u00d7 \u210e , keys K \u2208 R \u00d7 \u210e and values V \u2208 R \u00d7 \u210e , where indicates a specific head, is the sequence length, is the dimensionality of inputs and \u210e is the total number of heads. Then, the scaled dot-product attention is calculated as below:\nAttn(Q , K , V ) = Q K V .(1)\nMulti-head self-attention performs the above self-attention mechanism \u210e times in parallel, and then the outputs of each head are concatenated together and linearly projected to get the final output:\nMSA(X) = Concat(Attn(Q 1 , K 1 , V 1 ), ..., Attn(Q \u210e , K \u210e , V \u210e )) . (2)\n\nMLP.\n\nThe MLP is used for introducing non-linearity and feature transformation between MSA layers:\nMLP(X) = FC( (FC(X))), FC(X) = XW + b,(3)\nwhere b and W are the bias and weights of a fully-connected layer, and (\u00b7) is the activation function such as ReLU [10]. \n\n\nMB-STR\n\nWe now introduce the MB-STR framework. As illustrated in Figure  2, there are three key components: (1) the Multi-Behavior Transformer layer (MB-Trans); (2) the Multi-Behavior Sequential Pattern Generator (MB-SPG); and (3) Behavior-Aware masked item Prediction module (BA-Pred). As shown in Figure 2, we first transform the item sequence x to an input hidden representation matrix H (0) through an embedding matrix E \u2208 R |V |\u00d7 . Then, we iteratively compute hidden representations H ( ) at each MB-Trans layer (Section 4.1). Simultaneously, the MB-SPG module is coupled with the MB-Trans layer to inject multi-behavior sequential patterns (Section 4.2). Finally, the BA-Pred module predicts the masked items with respect to their corresponding behavior type (Section 4.3). Next, we will elaborate on these three components.\n\n\nMulti-Behavior Transformer Layer\n\nTypically, the inputs to the transformer, whether they are words [7,30], audio segments [3,32] or image patches [8,25], are essentially homogeneous. However, in the multi-behavior sequential recommendation scenario, items are interacted by different types of behaviors. This leads to behavior-specific semantics and fine-grained heterogeneous dependencies, which vanilla transformer fails to capture. Here, we propose a novel multi-behavior transformer (MB-Trans) layer, which captures fine-grained heterogeneous dependencies through a multi-behavior multi-head selfattention mechanism (MB-MSA) and models behavior semantics by behavior-specific MLPs.\n\n\nMulti-Behavior\n\nMulti-head Self-Attention. To endow MSA with the ability to capture multi-behavior heterogeneous dependencies, we generalize the original MSA to Multi-Behavior Multi-head \nAlgorithm 1: Multi-Behavior Multi-head Self-Attention Input: H ( \u22121) \u2208 R \u00d7 , b \u2208 B , P ( ) \u2208 R \u210e\u00d7 \u00d7 Output: G ( ) \u2208 R \u00d7 1 for head = 1 to \u210e do /* Step 1. Behavior-specific projection. */ 2 Q \u2190 (H ( \u22121) , b) 3 K \u2190 (H ( \u22121) , b) 4 V \u2190 (H ( \u22121) , b) /*G ( ) [ ] \u2190 A [ , ] \u00b7 W (b[ ],b[ ]) \u00b7 V [ ] 11 end 12 end 13 G ( ) \u2190 Concat(G ( ) 1 , ..., G ( ) \u210e )\nSelf-Attention (MB-MSA). Overall, MB-MSA differs from the original MSA in four folds: (1) behavior-specific linear projection is applied to get queries, keys and values; (2) behavior heterogeneity is considered when calculating attention scores; (3) sequential patterns are injected to the raw attention matrix; and (4) behavior heterogeneity is considered when aggregating values.\n\nThe details of MB-MSA are described in Algorithm 1 and illustrated in Figure 2. Let H ( \u22121) \u2208 R \u00d7 be the input of the -th layer, where is the behavior sequence length, is the model dimension.\n\nb \u2208 B is the corresponding behavior sequence indicating the behavior type at each position of H ( \u22121) .\n\nFirst, to distinguish intentions under different types of behaviors, we perform behavior-specific linear projection (\u00b7) (H ( \u22121) , b) :\nR \u00d7 \u2192 R \u00d7 \u210e to get Q, K and V, respectively (Step 1. in Algorithm 1). (\u00b7) (H ( \u22121) , b)\nis a wrapper of |B| linear layers, each corresponds to a specific type of behavior. At each position ,\nH ( \u22121) [ ] is pro- jected according to the behavior type b[ ].\nNext, we want to calculate the raw attention score with respect to multi-behavior dependencies. To achieve this, instead of directly calculating the dot product between the Queries and Keys, we construct a distinct parameter matrix W ( , ) \u2208 R \u210e \u00d7 \u210e for each behavior pair (Step 2. in Algorithm 1). Pairwise multi-behavior dependencies can be captured in this way. High-order dependencies can also be modeled by stacking multiple layers.\n\nFurthermore, the self-attention mechanism is not sensitive to sequence order, let alone the complex multi-behavior sequential patterns. To address this problem, we inject sequential information by adding multi-behavior sequential pattern matrix P ( ) generated by the MB-SPG module (Section 4.2) to the raw attention matrix A (Step 3. in Algorithm 1) and apply a softmax operation.\n\nFinally, we would aggregate information from the values V regarding behavior heterogeneity to alleviate the distribution difference between different behaviors. Thus, similar to the calculation of attention, we construct a distinct parameter matrix W ( , ) \u2208 R \u210e \u00d7 \u210e for each behavior pair (Step 4. in Algorithm 1).\n\nMB-MSA performs the above attention \u210e times in parallel. The outputs of \u210e heads are concatenated to get the final output G ( ) .\n\n\nBehavior-Specific Multi-Layer Perceptron.\n\nTo model behavior semantics and perform feature transformation, we apply Behavior-Specific MLPs (BS-MLP). Specifically, we specify a distinct MLP layer for each type of behavior. The BS-MLP is defined as follows:\nH ( ) = BS-MLP(G ( ) , b),(4)\nwhere BS-MLP(\u00b7) is a wrapper of |B| standard MLP layers, each corresponds to a specific type of behavior. At each position , the intermediate representation G ( ) [ ] is processed using behaviorspecific MLP specified by behavior b[ ].\n\n\nThe Multi-Behavior Transformer Layer.\n\nFinally, residual connections [14] and layer normalizations [2] are employed to connect the MB-MSA and BS-MLP modules to get the MB-Trans layer:\nG ( ) = LayerNorm H ( \u22121) + MB-MSA(H ( \u22121) , b, P ( ) ) H ( ) = LayerNorm G ( ) + BS-MLP(G ( ) , b)(5)\n\nMulti-Behavior Sequential Pattern Modeling\n\nOne unique characteristic of user interaction sequences is diverse sequential patterns across multiple behaviors. Take click and purchase as an example, click usually indicates short-term interest, so the recently clicked items may strongly impact the current behavior, while the items clicked long ago usually have little influence on the current behavior. However, items purchased long ago may still strongly influence current interest due to the long-term interest contained in purchase behaviors. To encode such diverse multi-behavior sequential patterns, we propose a Multi-Behavior Sequential Pattern Generator (MB-SPG).\n\n\nThe design rationale of MB-SPG.\n\nThere should be a sequential relationship between any two actions indexed by , in a user interaction sequence. Intuitively, this relationship depends on the temporal distance between two actions and their corresponding behavior types b[ ] and b[ ].\n\nBased on this intuition, we design a multi-behavior sequential pattern generator (MB-SPG) to encode such sequential dependency. As shown in Figure 2, the MB-SPG module is coupled with the MB-Trans layer to produce attention bias terms. Specifically, given an interaction sequence, the MB-SPG module generates multi-behavior relative positional embedding matrix P \u2208 R \u210e\u00d7 \u00d7 (the same dimension with raw attention scores), where each element P[ , , ] measures how much attention position should pay to position with respect to the relative distance ( \u2212 ), and behavior type b[ ] and b[ ] at a specific head .\n\nConsequently, the design of MB-SPG boils down to designing |B| \u00d7 |B| encoding functions ( , ) : R \u2192 R from the set R of relative positions to a real number strength term. Each encoding function ( , ) corresponds to a behavior type pair and encodes the temporal relationship of the two types of behaviors. Formally, at head , for any two actions indexed by and ,\nP[ , , ] = (b[ ],b[ ]) ( \u2212 ),(6)\nwhere (  . By doing so, multi-behavior sequential patterns are captured and further used for attention calculation. Next, we will describe the implementation of a single encoding function (\u00b7) in great detail. Similar to T5 [31] in natural language processing, we design a heuristic bucketing mechanism in the relative positional encoding function to alleviate the unbalanced issue. Specifically, is further expressed as the composition \u210e \u2022 of a bucketing function and an embedding lookup table \u210e. The former maps a relative position to a \"bucket\" index, and the latter encodes a bucket index as a bias term.\n\n\nRelative\n\n\u2022 indicates the function composition, where \u210e \u2022 ( ) = \u210e( ( )).\n\nFirst, we introduce the design of bucketing function . Suppose the max sequence length is and the number of buckets is , with indices from 0 to \u2212 1, where > . For the input relative position ( \u2212 ), the bucketing function (\u00b7) are defined as follows:  where half of the buckets are used for positive relative positions, and the other half for negative. ( ) is defined as follows:\n( \u2212 ) = ( \u2212 ) if ( \u2212 ) \u2265 0 (\u2212( \u2212 )) + 2 if ( \u2212 ) < 0(7)( ) = \u23a7 \u23aa \u23aa \u23aa \u23a8 \u23aa \u23aa \u23aa \u23a9 if 0 \u2264 < 4 min 4 + log 2 \u2212log 2 log 2 \u2212log 2 \u00b7 4 , 2 \u2212 1 if \u2265 4 .(8)\nWe plot the bucket function with = 32 and = 50 in Figure 3. Then, an embedding lookup table \u210e maps the bucket indices {0, 1, ..., \u22121} to real numbers. Let E \u2208 R \u00d7\u210e denote the bucketembedding matrix, where \u210e is the number of heads. E works like a normal embedding layer, which is randomly initialized and continuously updated during training.\n\n\nBehavior-Aware Masked Item Prediction\n\nThe item-level multi-behavior dependencies and sequential patterns can be modeled with the MB-Trans layer and MB-SPG module acting as the model backbones. Nevertheless, a new challenge arises: how to effectively mine information from users' multi-behavior interaction sequence? We propose a Behavior-Aware Prediction module (BA-Pred) and a behavior-aware masked item prediction objective to tackle this challenge.\n\n\nBehavior-Aware Prediction Module.\n\nPrevious works have shown that using multi-behavior supervision signals instead of only target behavior can significantly improve the recommendation performance [9,11,40]. Therefore, the multi-behavior recommendation is usually formulated as a multi-task learning problem. However, different types of behavior may be weakly correlated or even conflicted (e.g., like and dislike in a rating website), which may lead to performance degradation, called negative transfer. Therefore, to better capture the behavior-specific information and model the commonalities among different behaviors, we further design a Behavior-Aware Prediction module (BA-Pred).\n\nAs shown in Figure 2, the BA-Pred module is a parameter sharing structure like Multi-gated Mixture of Experts (MMoE) [27]. Compared with straightforwardly sharing parameters in MMoE, we separate shared experts and behavior-specific experts explicitly. In this way, BA-Pred can better alleviate conflicting parameter interference between shared and behavior-specific information.\n\nSpecifically, when making predictions at position , the BA-Pred structure can be formulated as:\n= (H ( ) [ ]) (H ( ) [ ]),(9)\nwhere H ( ) is the hidden representation of the last layer of MB-Trans, and (\u00b7) is the gating function to calculate gating weights of a specific behavior type = b[ ] through a linear transformation and a softmax operation:\n( ) = (W ),(10)\nwhere W \u2208 R ( + )\u00d7 is the gating parameter, and are the number of behavior specific experts and shared experts, respectively.\n\n(\u00b7) is the combination of behavior-specific experts , (\u00b7) and shared experts , (\u00b7):\n( ) = ,1 ( ), ,2 ( ), ..., , ( ), ,1 ( ), ,2 ( ), ..., , ( ) ,(11)\nwhere each expert is a simple linear layer. Finally, the prediction is made by a dot-product of the final behavior-specific representation and an item embedding matrix E \u2208 R |V |\u00d7 , which is shared with the input item embedding matrix to reduce the model size and alleviate overfitting:\np ( ) = ( \u00b7 E ),(12)\nwhere p ( ) \u2208 R |V | is the predicted probability distribution over all items \u2208 V at position .\n\n\nBehavior-Aware Masked Item Prediction Objective.\n\nIncorporating the BA-Pred module, we propose a novel training objective to mine the multi-behavior interaction sequences effectively. Specifically, given the multi-behavior interaction sequence = (x, b), we first construct a corrupted item sequencex by randomly setting a portion ( ) of items in x to a special mask symbol [MASK], while keeping the behavior sequence b unmasked. Let the masked items bex. Then the behavior-aware masked item prediction objective is to reconstructx based onx and b:   (x, b), we append the special token \"[MASK]\" to the end of item sequence x and append the target behavior type to the end of the behavior sequence b. Then, we predict the next item under target behavior based on the historical multi-behavior interaction data.  ( (|V| )) and parameters in the MB-Trans ( (|B| 2 )), MB-SPG ( ( )) and the behavior-aware prediction head ( (|B| 2 )). The total number of parameters is (|V| + |B| 2 + ), which is moderate compared to other methods (e.g., (|V| + 2 + ) for SASRec and BERT4Rec) since the |B| and are relatively small (4 and 16, respectively).\nmin \u2212 log (x |x, b) = \u2212 =1 log (x[ ] |x, b) ,(13)\n\nModel Learning and Complexity Analysis\n\n\nTesting. Given a multi-behavior interaction sequence =\n\n\nTime Complexity.\n\nSince MB-SPG module is composed of hard encoded bucketing functions and embedding lookup tables, and the behavior-aware prediction head consists of only linear experts. Therefore, the computational complexity of our MB-STR is mainly due to the MB-Trans layer. Specifically, MB-MSA and BS-MLP take ( 2 + 2 ), where the dominant term is typically ( 2 ) from the MB-MSA layer. However, a convenient property in our model is that the computation in each self-attention layer is fully parallelizable, which is amenable to GPU acceleration.\n\n\nEVALUATION\n\nWe conduct extensive experiments to answer the following questions: \n\n\nExperiment Settings\n\n\nDatasets.\n\nWe evaluate the proposed MB-STR model on the following three public datasets: Yelp Data. This is a widely used recommendation dataset collected from Yelp challenge. According to the explicit user rating scores (i.e., ranging from 1 (worst) to 5 (best)), the user interactions are split into three types of user behaviors: dislike (rating \u2264 2), neutral (rating >2 and <4), and like (rating \u22654). In addition to the user ratings, there is an additional tip behavior representing that a user writes tips on visited venues. Here, we regard like as target behavior. Taobao Data. This is a real-world e-commerce dataset collected from Taobao, one of the largest e-commerce platforms. There are four types of behaviors, i.e., click, add-to-favorite, add-to-cart and buy, in which is regarded as the target behavior. IJCAI Data. This dataset is released by IJCAI competition for user activity modeling from an online e-commerce platform. Four types of behavior are included, i.e., click, add-to-favorite, add-to-cart and buy, in which is regarded as the target behavior. For a fair comparison, we closely follow the settings of MB-GMN [40]. We use the same pre-processed datasets with MB-GMN. As the time information of the original used Beibei dataset in [40] is not available, we use the same pre-processed Yelp dataset from MATN [39] as an alternative. Table 2 shows the statistics of the three datasets. \n\n\nEvaluation Settings and Metrics.\n\nWe evaluate MB-STR and baseline models with two metrics, i.e., the Hit Ratio (HR@k) and the Normalized Discounted Cumulative Gain (NDCG@k), which have been widely used in top-N recommendation tasks [38][39][40].\n\n\nBaseline Models.\n\nTo comprehensively demonstrate the effectiveness of the proposed MB-STR model, we compare it with various baselines from different lines of research topics: (1) Single-Behavior Non-Sequential Models: MF [21], DMF [41], NGCF [38] and LightGCN [15], (2) Single-Behavior Sequential Models: SASRec [20] and BERT4Rec [33], (3) Multi-Behavior Non-Sequential Models: NGCF , LightGCN (we replace the original single-behavior graph with multi-behavioral graph to enhance NGCF and LightGCN), NMTR [9], MATN [39], MBGCN [19] and MB-GMN [40], (4) Multi-Behavior Sequential Models: DIPN [13], SASRec , BERT4Rec (we treat the multi-behavior sequences as single-behavior sequences to enhance SASRec and BERT4Rec.) and DMT [11].\n\n\nImplementation Details.\n\nThe MB-STR model is implemented in PyTorch [29]. The data and source code are available 1 . All parameters are initialized with the Gaussian distribution N (0, 0.02) and optimized using the Adam optimizer with a learning rate of 0.001. We set the training batch size to 128, dropout rate to 0.2. Also, we set max sequence length to = 50 and bucket number in MB-SPG to = 32 for all three datasets. For a fair comparison, we set the model dimensionality to 16, which is consistent with [39,40]. There are 2 MB-Trans layers and 2 heads per layer. The multi-behavior interaction sequence is randomly masked with a portion of = 0.2. We run the codes released by the authors for SASRec 2 , BERT4Rec 3 , LightGCN 4 , and DMT 5 , and obtain the best results from [39,40] directly for other models (MF, DMF, NGCF, NMTR, MATN, MBGCN, MB-GMN and DIPN).\n\n\nOverall Performance Evaluation (Q1)\n\nWe evaluate the performance of predicting the target behavior among all baseline models and our MB-STR model. The results on three datasets are reported in Table 3. From the results, we summarize the following observations. \u2022 The effectiveness of MB-STR model. Table 3 shows that MB-STR overperforms all baseline models on three real-world datasets. The average improvement of MB-STR to the best baseline model is 1.15% and 6.67% for Recall and NDCG on the Yelp dataset, 13.78% and 27.73% on Taobao dataset and 7.72% and 12.82% on IJCAI dataset, which demonstrates the effectiveness of MB-STR. Notice that the improvement in Yelp dataset is less Table 3: Overall model performance comparison with the metrics of HR@10 and NDCG@10. \"*\" indicates the statistically significant improvements (i.e., p-value < 0.05) over the best baseline (underlined values). The categories of baseline models are shown, where \"O\" and \"M\" represent using one single behavior or multi-behavior, respectively, and \"S\" and \"NS\" indicate sequential or non-sequential, respectively. significant than in the Taobao and IJCAI dataset. A possible reason is that the multiple behaviors (dislike, neutral, and like) in Yelp are generated by splitting users' ratings, which are mutually exclusive. Thus the modeling of multi-behavior dependencies is less beneficial for performance improvements. \u2022 Both sequential and multi-behavioral information bring benefits to model performance. Despite the big difference in model design between different baselines, there is a trend that considering either multi-behavioral or sequential information leads to performance improvement. For example, BERT4Rec and MB-GMN achieve much better performance than NGCF and LightGCN on all three datasets. However, special designs are required to better utilize these two characteristics, and naive adoption might lead to performance degradation. For example, LightGCN, which only uses single behavior, performs much better than NMTR with multi-behavior and DIPN with sequential information in Yelp and Taobao datasets. \u2022 MB-STR consistently outperforms multi-behavior sequential baseline models. There is a significant performance gain when comparing MB-STR with other multi-behavior sequential counterparts. This performance gain can be attributed to the effective modeling of heterogeneous item-level multi-behavior dependencies and diverse multi-behavior sequential patterns. For example, the evaluation results reveal the limitations of the DIPN and DMT, which follow a two-stage aggregation paradigm to aggregate different types of behavioral patterns via weighted summation. These approaches fail to capture the fine-grained dependencies across different types of behaviors. Besides, SASRec and BERT4Rec utilize vanilla transformer layers and positional encodings, thus cannot differentiate heterogeneous multi-behavior dependencies and diverse multi-behavior sequential patterns.\n\n\nModel Ablation Study (Q2)\n\nThere are three key components in the MB-STR model, each corresponding to a modeling consideration. To study the validity of each component, we introduce the following MB-STR variants according to their design purpose:\n\n\u2022 MB-STR w/o MB-Trans: The MB-Trans layer is replaced with a vanilla transformer layer [36]. It treats different behavior transition patterns the same at the transformer layer, thus ignoring multi-behavior heterogeneous dependencies. (Noted as \"homogeneous item-level\" in multi-behavior modeling in Table 4.) \u2022 MB-STR w/o MB-SPG: The MB-SPG module is removed. This model variant is insensitive to sequential information. \u2022 MB-STR w/o BA-Pred: The BA-Pred is replaced with a simple linear prediction head as in BERT4Rec [33], which can not make behavior specific predictions.\n\nTo further demonstrate the efficacy of each modeling consideration, we compare different variants of MB-STR with representative baselines categorized by their design purpose. The results are shown in Table 4, and we draw the following conclusions:\n\n\u2022   \n\n\nEffect of Auxiliary Behaviors (Q3)\n\nWe further perform a data ablation study to investigate the effectiveness of incorporating different auxiliary behaviors. We run four variants of the MB-STR on two real-world e-commerce dataset, where \"-click\", \"-fav\", and \"-cart\" indicates the MB-STR without incorporating the click, add-to-favorite and add-to-cart behavior. Also, \"only buy\" represents the variant that only uses the target behavior. The evaluation results are shown in Figure 4.\n\nThe results show that MB-STR with all four behaviors achieves the best performance compared with the ablation variants, emphasizing the necessity of integrating more auxiliary behaviors to help the recommendation under target behavior. Moreover, it demonstrates that MB-STR can effectively learn user intentions from multiple types of user behaviors. \n\n\nHyper-parameter Analysis (Q4)\n\nTo investigate the effect of different hyper-parameter settings on MB-STR, we perform experiments on Yelp and Taobao datasets with different configurations of key hyper-parameters, namely, the model dimensionality , and the masked item portion . As shown in Figure 5, we conclude as follows:\n\n\u2022 Model dimensionality . Figure 5a and Figure 5b show the performance when model hidden dimension grows from 8 to 128. When increases from 8 to 32, the performance grows on both Yelp and Taobao datasets, as higher hidden dimensionality leads to larger representation capacity. However, with the further increase of the model dimensionality from 32 to 64, the performance saturates on Yelp yet keeps improving on Taobao. When the model dimensionality reaches a threshold, will no longer be the performance bottleneck. Increasing the model dimension will not improve performance but increase computation overhead. Furthermore, the threshold depends on the characteristics of datasets. For example, the item set V of Taobao is 4 times larger than that of Yelp, so a higher hidden dimension is required for Taobao to achieve better performance. \u2022 Masked item portion . The masked item portion controls the proportion of items used as prediction targets. Figure 5c and Figure 5d show how affects the performances of MB-STR on Yelp and Taobao, respectively. We can observe that the performance increases first and then decreases. When is too small, the training signals will be insufficient as only a small portion of items are used as training targets; however, when is too large, the behavior sequence is corrupted by introducing too many artificial symbols [MASK]. Both circumstances will degrade the performance of MB-STR.\n\n\nInterpretability Analysis (Q5)\n\nTo further investigate the interpretability of MB-STR, we conduct analysis via visualizing the multi-behavior sequential patterns learned in MB-SPG (Section 4.2). Heat-maps of multi-behavior sequential patterns learned on the Taobao dataset are drawn in Figure  6. Specifically, each heat-map matrix in Figure 6 visualizes an interbehavior relative positional encoding function ( , ) ( \u2212 ) as formulated in Equation 6, with , in x-axis and y-axis representing different positions at the behavior sequence. Taking the heat-map indexed by (buy, cart) as an example, each element [ , ] represents the attention bias score from the buy behavior of the location to the cart behavior of the location . From Figure 6, meaningful patterns emerged, which are in line with our intuition. For example, the buy behavior pays long-range attention to other items interacted by buy (the heat-map at row 2, column 2), indicating that buy represents long-term user interest. Besides, buy actions pay more attention to the previous cart actions (the heat-map at row 2, column 4), which reflects the high propensity that cart can convert to buy. Correspondingly, cart actions pay more attention to the latter buy actions (the heat-map at row 2, column 4) and we can refer to this mechanism as \"looking to the future\". Note that such a mechanism can not be employed during the evaluation stage since \"future\" data are unavailable. Nevertheless, during the training stage, \"looking to the future\" plays an essential role in mining the sequential patterns and learning more comprehensive representations.\n\n\nCONCLUSION AND FUTURE WORK\n\nIn this paper, we proposed the Multi-Behavior Sequential Transformer Recommender (MB-STR) framework for addressing realworld recommendation problems. MB-STR gains performance improvement from these advantages: (1) it captures heterogeneous fine-grained item-level multi-behavior dependencies through Multi-Behavior Transformer Layer (MB-Trans); (2) it models diverse multibehavior sequential patterns via Multi-Behavior Sequential Pattern Generator (MB-SPG); (3) it is able to mine the multi-behavior interaction sequence effectively by the Behavior-Aware masked item Prediction module (BA-Pred). Comprehensive experiments on three real-world recommendation datasets demonstrate the effectiveness of the MB-STR model. In future works, we will further explore the integration of side information into MB-STR and model pre-training for cold-start problems.\n\nFigure 1 :\n1An example of multi-behavior sequential recommendation. A user interacts with items through different behaviors. Arrows indicate plausible explanations for the final purchasing decision.\n\nFigure 2 :\n2The model architecture of MB-STR. The overall structure is shown in the left panel. The zoom-in view of the MB-Trans module and MB-SPG module are shown in the middle and right panel, respectively.\n\n\n\u2212 ) is the distance of relative to , and (b[ ]\u2212b[ ]) (\u00b7) is the encoding function specified by the behavior type pair (b[ ], b[ ])\n\n\nPositional Encoding Function. A straightforward implementation of the relative positional encoding function is mapping a relative position to a real number through the trivial embedding lookup table. However, the number of position pairs with the same relative distance is unbalanced. For example, there are 49 pairs {(0,1),(1,2), ... (48,49)} with relative position of 1 for a sequence of length 50, and only 1 pair (0,49) with relative position of 49. Such an unbalanced issue might lead to insufficient learning of long-range sequential patterns.\n\nFigure 3 :\n3An illustration of the bucketing function with = 32 and = 50. The horizontal axis is the relative position ( \u2212 ). And the vertical axis is the output value of bucketing function ( \u2212 ).\n\n\nwhere = 1 indicates x[ ] is masked, is the model parameters, and the probability (x[ ] |x, b) is calculated as p ( = x[ ]) in Eq. 12. At a particular masked position , the prediction is made based on the contextual actions around position and the behavior type performed at position . By masking only items and keeping behavior types unmasked, the model performs behavior-aware predictions, which better captures diverse user preferences under different types of behaviors.\n\n\n4.4.1 Training.MB-STR is trained by optimizing the behavioraware masked item prediction objective defined in Eq. 13.\n\n\u2022 RQ1 :\nRQ1How does MB-STR perform against various recommendation baselines? \u2022 RQ2: Do all the designed components of MB-STR bring benefits in line with their design purposes? \u2022 RQ3: How do different types of behaviors contribute to the prediction of target behavior? \u2022 RQ4: How do different hyper-parameter settings impact the performances of MB-STR? \u2022 RQ5: How is the model interpretability of MB-STR? Can the multi-behavior sequential patterns captured by MB-STR be represented in a human-understandable way?\n\nFigure 4 :\n4Effect of auxiliary behaviors.\n\nFigure 5 :\n5Hyper-parameter study of the MB-STR.\n\nFigure 6 :\n6Heat-maps of multi-behavior sequential pattern learned on the Taobao dataset, where x-axis and y-axis denote different positions at the behavior sequence. The colors represent the attention weights, where lighter color implies higher level of importance.\n\nTable 1 :\n1Comparison of related works.\n\nTable 2 :\n2Dataset statistics.Dataset \n#users \n#items \n#interactions \nBehavior types \n\nYelp \n19,800 \n22,734 \n1.4 \u00d7 10 6 \n{Tip, Dislike, Neutral, Like} \nTaobao 147,894 \n99,037 \n7.6 \u00d7 10 6 \n{Click, Favorite, Cart, Buy} \nIJICAI \n423,423 874,328 \n3.6 \u00d7 10 7 \n{Click, Favorite, Cart, Buy} \n\n\n\n\nMATN and MB-GCN. This verifies the significance of modeling multi-behavior dependencies at heterogeneous item-level. And further, with the advantages of multi-behavior modeling and behavior-specific prediction, the w/o MB-SPG outperforms NMTR and MB-GMN by a big margin. These comparisons justify the effectiveness and superiority of our proposed MB-Trans layer, MB-SPG module and BA-Pred module for multi-behavior recommendation.Each of the three key components of the MB-STR brings \nbenefits to performance. Comparing the prediction perfor-\nmance of MB-STR and its three variants, there is a significant \nperformance degradation when any key components are removed \nor replaced by other counterparts. Particularly, the performance \ngap between MB-STR and the w/o MB-Trans variant indicates the \nadvantage of MB-Trans in modeling fine-grained heterogeneous \ndependencies. The MB-SPG module effectively captures diverse \nmulti-behavior sequential patterns, as the w/o MB-SPG variant \nhas a performance degradation up to 26.8% in terms of NDCG@10 \non Taobao dataset. Besides, the BA-Pred module shows positive \neffects on all datasets, which demonstrates the rationality of our \nbehavior-aware prediction design. \n\u2022 MB-STR and its variants show superior performances com-\npared to baselines with similar design purposes. Compared \nto SASRec and BERT4Rec , the better performance of the \nw/o MB-Trans & w/o BA-Pred variant verifies that modeling se-\nquential patterns with incorporating diverse behavior transition \npatterns is of great importance. We also observe that when se-\nquential information and behavior-specific prediction are both \nremoved, the w/o MB-SPG & w/o BA-Pred variant achieves large \nimprovements compared to \n\nTable 4 :\n4Model ablation study. Different variants of MB-STR and representative baselines are categorized according to three design purposes consistent with Table 1. The relative improvement between different variants and the full MB-STR model are highlighted in red. The relative improvement between different baselines and the MB-STR variants with similar design purposes are colored in blue. Best viewed in color.Model \nModel Properties \nYelp \nTaobao \nIJCAI \n\nMulti-Behavior \nModeling \n\nSequential \nInformation \n\nBehavior-Specific \nPrediction \nHR@10 \nNDCG@10 \nHR@10 \nNDCG@10 \nHR@10 \nNDCG@10 \n\nMB-STR \nheterogeneous item-level multi-behavior \n0.882 \n0.624 \n0.768 \n0.608 \n0.879 \n0.713 \nw/o MB-Trans homogeneous item-level multi-behavior \n0.861(-2.4%) \n0.596(-4.5%) \n0.749(-2.5%) \n0.585(-3.8%) \n0.869(-1.1%) \n0.690(-3.2%) \nw/o BA-Pred \nheterogeneous item-level multi-behavior \n0.863(-2.2%) \n0.599(-4.0%) \n0.729(-5.1%) \n0.558(-8.2%) \n0.859(-2.3%) \n0.689(-3.4%) \n\nw/o MB-SPG \nheterogeneous item-level \n0.868(-1.6%) \n0.604(-3.2%) 0.658(-14.3%) 0.445(-26.8%) 0.850(-3.3%) \n0.660(-7.4%) \nNMTR \nbehavior-level \n0.790(-9.0%) 0.478(-20.9%) 0.332(-49.5%) 0.179(-59.8%) 0.481(-43.4%) 0.304(-53.9%) \nMB-GMN \nbehavior-level \n0.870(+0.2%) 0.582(-3.6%) 0.491(-25.4%) 0.300(-32.6%) 0.532(-37.4%) 0.345(-47.7%) \n\nw/o MB-Trans \n& w/o BA-Pred \nhomogeneous item-level multi-behavior \n0.851(-3.5%) \n0.573(-8.2%) \n0.710(-7.6%) 0.542(-10.9%) 0.831(-5.5%) \n0.666(-6.6%) \n\nSASRec \nhomogeneous item-level single-behavior \n0.819(-3.8%) \n0.531(-7.3%) 0.637(-10.3%) 0.442(-18.5%) 0.795(-4.3%) \n0.611(-8.3%) \nBERT4Rec \nhomogeneous item-level single-behavior \n0.838(-1.5%) \n0.558(-2.6%) \n0.675(-4.9%) 0.476(-12.2%) 0.816(-1.8%) \n0.632(-5.1%) \n\nw/o MB-SPG \n& w/o BA-Pred \nheterogeneous item-level \n0.862(-2.3%) \n0.593(-5.0%) 0.640(-16.7%) 0.412(-32.2%) 0.840(-4.4%) \n0.654(-8.3%) \n\nMATN \nbehavior-level \n0.826(-4.2%) 0.530(-10.6%) 0.354(-44.7%) 0.209(-44.7%) 0.489(-41.8%) 0.309(-52.8%) \nMB-GCN \nbehavior-level \n0.796(-7.7%) 0.502(-15.3%) 0.369(-42.3%) 0.222(-46.1%) 0.463(-44.9%) 0.277(-57.6%) \n\n\nhttps://github.com/huawei-noah/benchmark/tree/main/FuxiCTR/model_zoo 2 https://github.com/kang205/SASRec 3 https://github.com/FeiSun/BERT4Rec 4 https://github.com/kuandeng/LightGCN 5 https://github.com/guyulongcs/CIKM2020_DMT\nACKNOWLEDGMENTSWe thank MindSpore[1]for the partial support of this work, which is a new deep learning computing framework.\n. Mindspore, 2020. MindSpore. https://www.mindspore.cn\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.06450Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza- tion. arXiv preprint arXiv:1607.06450 (2016).\n\nAbdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. Alexei Baevski, Henry Zhou, arXiv:2006.11477arXiv preprintAlexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. arXiv preprint arXiv:2006.11477 (2020).\n\n. Heng-Tze, Levent Cheng, Jeremiah Koc, Tal Harmsen, Tushar Shaked, Hrishi Chandra, Glen Aradhye, Greg Anderson, Wei Corrado, Mustafa Chai, Rohan Ispir, Zakaria Anil, Lichan Haque, Vihan Hong, Xiaobing Jain, Hemal Liu, Shah, Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.\n\nWide & Deep Learning for Recommender Systems. Proc. Workshop Deep Learning for Recommender Systems. Workshop Deep Learning for Recommender SystemsWide & Deep Learning for Recommender Systems. In Proc. Workshop Deep Learning for Recommender Systems.\n\nThe YouTube video recommendation system. James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, Proceedings of the fourth ACM conference on Recommender systems. the fourth ACM conference on Recommender systemsJames Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al. 2010. The YouTube video recommendation system. In Proceedings of the fourth ACM conference on Recommender systems. 293-296.\n\nContent-based video recommendation system based on stylistic visual features. Yashar Deldjoo, Mehdi Elahi, Paolo Cremonesi, Franca Garzotto, Pietro Piazzolla, Massimo Quadrana, Journal on Data Semantics. 5Yashar Deldjoo, Mehdi Elahi, Paolo Cremonesi, Franca Garzotto, Pietro Piazzolla, and Massimo Quadrana. 2016. Content-based video recommendation system based on stylistic visual features. Journal on Data Semantics 5, 2 (2016), 99-113.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.11929An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi- aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n\nNeural multi-task recommendation from multi-behavior data. Chen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, Fuli Feng, Yong Li, 2019 IEEE 35th international conference on data engineering (ICDE). IEEETat-Seng Chua, and Depeng JinChen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, Fuli Feng, Yong Li, Tat- Seng Chua, and Depeng Jin. 2019. Neural multi-task recommendation from multi-behavior data. In 2019 IEEE 35th international conference on data engineering (ICDE). IEEE, 1554-1557.\n\nDeep sparse rectifier neural networks. Xavier Glorot, Antoine Bordes, Yoshua Bengio, Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings. the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference ProceedingsXavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectifier neural networks. In Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 315-323.\n\nDeep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender Systems. Yulong Gu, Zhuoye Ding, Shuaiqiang Wang, Lixin Zou, Yiding Liu, Dawei Yin, Proceedings of the 29th ACM International Conference on Information & Knowledge Management. the 29th ACM International Conference on Information & Knowledge ManagementYulong Gu, Zhuoye Ding, Shuaiqiang Wang, Lixin Zou, Yiding Liu, and Dawei Yin. 2020. Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender Systems. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2493-2500.\n\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction. Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He, IJCAI. Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In IJCAI.\n\nBuying or browsing?: Predicting real-time purchasing intent using attention-based deep network with multiple behavior. Long Guo, Lifeng Hua, Rongfei Jia, Binqiang Zhao, Xiaobo Wang, Bin Cui, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningLong Guo, Lifeng Hua, Rongfei Jia, Binqiang Zhao, Xiaobo Wang, and Bin Cui. 2019. Buying or browsing?: Predicting real-time purchasing intent using attention-based deep network with multiple behavior. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1984-1992.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770-778.\n\nLightgcn: Simplifying and powering graph convolution network for recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. the 43rd International ACM SIGIR conference on research and development in Information RetrievalXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 639-648.\n\nRecurrent neural networks with top-k gains for session-based recommendations. Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Proceedings of the 27th ACM international conference on information and knowledge management. the 27th ACM international conference on information and knowledge managementBal\u00e1zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with top-k gains for session-based recommendations. In Proceedings of the 27th ACM international conference on information and knowledge management. 843-852.\n\nBal\u00e1zs Hidasi, Alexandros Karatzoglou, arXiv:1511.06939Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprintBal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015).\n\nAdaptive mixtures of local experts. A Robert, Michael I Jacobs, Jordan, J Steven, Geoffrey E Nowlan, Hinton, Neural computation. 3Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 79-87.\n\nMultibehavior recommendation with graph convolutional networks. Chen Bowen Jin, Xiangnan Gao, Depeng He, Yong Jin, Li, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information RetrievalBowen Jin, Chen Gao, Xiangnan He, Depeng Jin, and Yong Li. 2020. Multi- behavior recommendation with graph convolutional networks. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 659-668.\n\nSelf-attentive sequential recommendation. Wang-Cheng Kang, Julian Mcauley, 2018 IEEE International Conference on Data Mining (ICDM). IEEEWang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom- mendation. In 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 197-206.\n\nMatrix factorization techniques for recommender systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 42Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech- niques for recommender systems. Computer 42, 8 (2009), 30-37.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. 25Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi- cation with deep convolutional neural networks. Advances in neural information processing systems 25 (2012), 1097-1105.\n\nMulti-interest network with dynamic routing for recommendation at Tmall. Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang, Guoliang Kang, Qiwei Chen, Wei Li, Dik Lun Lee, Proceedings of the 28th ACM International Conference on Information and Knowledge Management. the 28th ACM International Conference on Information and Knowledge ManagementChao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang, Guoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-interest network with dynamic routing for recommendation at Tmall. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2615-2623.\n\nTime Interval Aware Self-Attention for Sequential Recommendation. Jiacheng Li, Yujie Wang, Julian J Mcauley, Proceedings of WSDM. James Caverlee, Xia (Ben) Hu, Mounia Lalmas, and Wei WangWSDMHouston, TX, USAACMJiacheng Li, Yujie Wang, and Julian J. McAuley. 2020. Time Interval Aware Self-Attention for Sequential Recommendation. In Proceedings of WSDM, James Caverlee, Xia (Ben) Hu, Mounia Lalmas, and Wei Wang (Eds.). ACM, Houston, TX, USA, 322-330.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, arXiv:2103.14030Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprintZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021).\n\nModeling task relationships in multi-task learning with multi-gate mixture-ofexperts. Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, Ed H Chi, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-of- experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1930-1939.\n\nModeling task relationships in multi-task learning with multi-gate mixture-ofexperts. Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, Ed H Chi, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-of- experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1930-1939.\n\nRecurrent neural network based language model. Tomas Mikolov, Martin Karafi\u00e1t, Lukas Burget, Jan Cernock\u1ef3, Sanjeev Khudanpur, Interspeech. 2Tomas Mikolov, Martin Karafi\u00e1t, Lukas Burget, Jan Cernock\u1ef3, and Sanjeev Khu- danpur. 2010. Recurrent neural network based language model.. In Interspeech, Vol. 2. Makuhari, 1045-1048.\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. (2017).\n\nImproving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya SutskeverAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im- proving language understanding by generative pre-training. (2018).\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim- its of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 (2019).\n\nSteffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli, arXiv:1904.05862wav2vec: Unsupervised pre-training for speech recognition. arXiv preprintSteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. 2019. wav2vec: Unsupervised pre-training for speech recognition. arXiv preprint arXiv:1904.05862 (2019).\n\nBERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, Peng Jiang, Proceedings of the 28th ACM international conference on information and knowledge management. the 28th ACM international conference on information and knowledge managementFei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep- resentations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management. 1441-1450.\n\nPersonalized top-n sequential recommendation via convolutional sequence embedding. Jiaxi Tang, Ke Wang, Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. the Eleventh ACM International Conference on Web Search and Data MiningJiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommenda- tion via convolutional sequence embedding. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. 565-573.\n\nDeep content-based music recommendation. A\u00e4ron Van Den, Sander Oord, Benjamin Dieleman, Schrauwen, Neural Information Processing Systems Conference (NIPS 2013). 26Neural Information Processing Systems Foundation (NIPS)A\u00e4ron Van Den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep content-based music recommendation. In Neural Information Processing Systems Conference (NIPS 2013), Vol. 26. Neural Information Processing Systems Founda- tion (NIPS).\n\nAttention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in NeurIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in NeurIPS 2017. 5998-6008.\n\nBillion-scale commodity embedding for e-commerce recommendation in alibaba. Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, Dik Lun Lee, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningJizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. 2018. Billion-scale commodity embedding for e-commerce recommendation in alibaba. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 839-848.\n\nNeural graph collaborative filtering. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua, Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval. the 42nd international ACM SIGIR conference on Research and development in Information RetrievalXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval. 165-174.\n\nMultiplex behavioral relation learning for recommendation via memory augmented transformer network. Lianghao Xia, Chao Huang, Yong Xu, Peng Dai, Bo Zhang, Liefeng Bo, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information RetrievalLianghao Xia, Chao Huang, Yong Xu, Peng Dai, Bo Zhang, and Liefeng Bo. 2020. Multiplex behavioral relation learning for recommendation via memory augmented transformer network. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2397- 2406.\n\nGraph meta network for multi-behavior recommendation. Lianghao Xia, Yong Xu, Chao Huang, Peng Dai, Liefeng Bo, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalLianghao Xia, Yong Xu, Chao Huang, Peng Dai, and Liefeng Bo. 2021. Graph meta network for multi-behavior recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 757-766.\n\nDeep Matrix Factorization Models for Recommender Systems. Xinyu Hong-Jian Xue, Jianbing Dai, Shujian Zhang, Jiajun Huang, Chen, IJCAI. Melbourne, Australia17Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. 2017. Deep Matrix Factorization Models for Recommender Systems.. In IJCAI, Vol. 17. Melbourne, Australia, 3203-3209.\n\nDeep interest evolution network for click-through rate prediction. Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, Kun Gai, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence33Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 5941-5948.\n\nDeep interest network for click-through rate prediction. Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningGuorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1059-1068.\n", "annotations": {"author": "[{\"end\":122,\"start\":109},{\"end\":128,\"start\":123},{\"end\":216,\"start\":129},{\"end\":302,\"start\":217}]", "publisher": "[{\"end\":54,\"start\":51},{\"end\":607,\"start\":604}]", "author_last_name": "[{\"end\":121,\"start\":115},{\"end\":127,\"start\":123}]", "author_first_name": "[{\"end\":114,\"start\":109}]", "author_affiliation": "[{\"end\":215,\"start\":130},{\"end\":301,\"start\":218}]", "title": "[{\"end\":50,\"start\":1},{\"end\":352,\"start\":303}]", "venue": "[{\"end\":470,\"start\":354}]", "abstract": "[{\"end\":2595,\"start\":1099}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2762,\"start\":2759},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2765,\"start\":2762},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2768,\"start\":2765},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2806,\"start\":2803},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2808,\"start\":2806},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2811,\"start\":2808},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3258,\"start\":3254},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3261,\"start\":3258},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3264,\"start\":3261},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4350,\"start\":4346},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4362,\"start\":4358},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4375,\"start\":4371},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4390,\"start\":4386},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4427,\"start\":4424},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4438,\"start\":4434},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4451,\"start\":4447},{\"end\":4824,\"start\":4803},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5418,\"start\":5414},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5431,\"start\":5427},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5442,\"start\":5438},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5455,\"start\":5451},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6907,\"start\":6903},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6920,\"start\":6916},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7396,\"start\":7392},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7408,\"start\":7404},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9984,\"start\":9980},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10025,\"start\":10021},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10047,\"start\":10043},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10153,\"start\":10149},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10156,\"start\":10153},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10283,\"start\":10279},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10478,\"start\":10474},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10493,\"start\":10489},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10512,\"start\":10508},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10753,\"start\":10749},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10777,\"start\":10774},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10803,\"start\":10799},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10830,\"start\":10826},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10855,\"start\":10851},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10901,\"start\":10897},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11390,\"start\":11386},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11435,\"start\":11431},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11612,\"start\":11609},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11662,\"start\":11658},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11839,\"start\":11835},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11863,\"start\":11859},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11866,\"start\":11863},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11896,\"start\":11892},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12064,\"start\":12060},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12076,\"start\":12072},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12087,\"start\":12083},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12100,\"start\":12096},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12369,\"start\":12366},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12381,\"start\":12377},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12392,\"start\":12388},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12408,\"start\":12404},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12458,\"start\":12454},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12471,\"start\":12467},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12674,\"start\":12671},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12687,\"start\":12683},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12698,\"start\":12694},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12711,\"start\":12707},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13760,\"start\":13756},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13763,\"start\":13760},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13886,\"start\":13882},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15203,\"start\":15199},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16144,\"start\":16141},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16147,\"start\":16144},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16167,\"start\":16164},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16170,\"start\":16167},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16191,\"start\":16188},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16194,\"start\":16191},{\"end\":17945,\"start\":17940},{\"end\":20058,\"start\":20055},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20206,\"start\":20202},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20235,\"start\":20232},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22606,\"start\":22602},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24587,\"start\":24584},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24590,\"start\":24587},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24593,\"start\":24590},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25196,\"start\":25192},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29591,\"start\":29587},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29712,\"start\":29708},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29788,\"start\":29784},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30099,\"start\":30095},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30103,\"start\":30099},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30107,\"start\":30103},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30336,\"start\":30332},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30346,\"start\":30342},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30357,\"start\":30353},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30375,\"start\":30371},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30427,\"start\":30423},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30445,\"start\":30441},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30619,\"start\":30616},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30630,\"start\":30626},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30642,\"start\":30638},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30658,\"start\":30654},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30707,\"start\":30703},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30840,\"start\":30836},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30916,\"start\":30912},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31357,\"start\":31353},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31360,\"start\":31357},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31628,\"start\":31624},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31631,\"start\":31628},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":35025,\"start\":35021},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":35457,\"start\":35453},{\"end\":38290,\"start\":38284}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41052,\"start\":40853},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41262,\"start\":41053},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41395,\"start\":41263},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41947,\"start\":41396},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42145,\"start\":41948},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42621,\"start\":42146},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42740,\"start\":42622},{\"attributes\":{\"id\":\"fig_7\"},\"end\":43253,\"start\":42741},{\"attributes\":{\"id\":\"fig_8\"},\"end\":43297,\"start\":43254},{\"attributes\":{\"id\":\"fig_9\"},\"end\":43347,\"start\":43298},{\"attributes\":{\"id\":\"fig_10\"},\"end\":43615,\"start\":43348},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43656,\"start\":43616},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":43944,\"start\":43657},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45675,\"start\":43945},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":47744,\"start\":45676}]", "paragraph": "[{\"end\":2986,\"start\":2611},{\"end\":4231,\"start\":2988},{\"end\":4758,\"start\":4233},{\"end\":7796,\"start\":4760},{\"end\":8676,\"start\":7798},{\"end\":8742,\"start\":8678},{\"end\":9574,\"start\":8744},{\"end\":9752,\"start\":9576},{\"end\":10661,\"start\":9799},{\"end\":10711,\"start\":10689},{\"end\":10998,\"start\":10744},{\"end\":11897,\"start\":11032},{\"end\":12014,\"start\":11899},{\"end\":13109,\"start\":12016},{\"end\":13571,\"start\":13148},{\"end\":13957,\"start\":13605},{\"end\":14275,\"start\":13959},{\"end\":14637,\"start\":14284},{\"end\":14866,\"start\":14668},{\"end\":15041,\"start\":14949},{\"end\":15205,\"start\":15084},{\"end\":16039,\"start\":15216},{\"end\":16727,\"start\":16076},{\"end\":16917,\"start\":16746},{\"end\":17649,\"start\":17268},{\"end\":17842,\"start\":17651},{\"end\":17947,\"start\":17844},{\"end\":18084,\"start\":17949},{\"end\":18275,\"start\":18173},{\"end\":18777,\"start\":18340},{\"end\":19160,\"start\":18779},{\"end\":19477,\"start\":19162},{\"end\":19607,\"start\":19479},{\"end\":19865,\"start\":19653},{\"end\":20130,\"start\":19896},{\"end\":20316,\"start\":20172},{\"end\":21091,\"start\":20465},{\"end\":21375,\"start\":21127},{\"end\":21982,\"start\":21377},{\"end\":22345,\"start\":21984},{\"end\":22986,\"start\":22379},{\"end\":23061,\"start\":22999},{\"end\":23440,\"start\":23063},{\"end\":23930,\"start\":23589},{\"end\":24385,\"start\":23972},{\"end\":25073,\"start\":24423},{\"end\":25453,\"start\":25075},{\"end\":25550,\"start\":25455},{\"end\":25803,\"start\":25581},{\"end\":25945,\"start\":25820},{\"end\":26030,\"start\":25947},{\"end\":26384,\"start\":26098},{\"end\":26501,\"start\":26406},{\"end\":27640,\"start\":26554},{\"end\":28342,\"start\":27808},{\"end\":28425,\"start\":28357},{\"end\":29860,\"start\":28461},{\"end\":30108,\"start\":29897},{\"end\":30841,\"start\":30129},{\"end\":31710,\"start\":30869},{\"end\":34684,\"start\":31750},{\"end\":34932,\"start\":34714},{\"end\":35508,\"start\":34934},{\"end\":35757,\"start\":35510},{\"end\":35763,\"start\":35759},{\"end\":36250,\"start\":35802},{\"end\":36603,\"start\":36252},{\"end\":36928,\"start\":36637},{\"end\":38350,\"start\":36930},{\"end\":39967,\"start\":38385},{\"end\":40852,\"start\":39998}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14667,\"start\":14638},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14941,\"start\":14867},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15083,\"start\":15042},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17167,\"start\":16918},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17267,\"start\":17167},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18172,\"start\":18085},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18339,\"start\":18276},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19895,\"start\":19866},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20419,\"start\":20317},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22378,\"start\":22346},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23496,\"start\":23441},{\"attributes\":{\"id\":\"formula_11\"},\"end\":23588,\"start\":23496},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25580,\"start\":25551},{\"attributes\":{\"id\":\"formula_13\"},\"end\":25819,\"start\":25804},{\"attributes\":{\"id\":\"formula_14\"},\"end\":26097,\"start\":26031},{\"attributes\":{\"id\":\"formula_15\"},\"end\":26405,\"start\":26385},{\"attributes\":{\"id\":\"formula_16\"},\"end\":27690,\"start\":27641}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11918,\"start\":11911},{\"end\":27323,\"start\":27315},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":29815,\"start\":29808},{\"end\":31913,\"start\":31906},{\"end\":32018,\"start\":32011},{\"end\":32403,\"start\":32396},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":35240,\"start\":35233},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":35717,\"start\":35710}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2609,\"start\":2597},{\"attributes\":{\"n\":\"2\"},\"end\":9797,\"start\":9755},{\"end\":10687,\"start\":10664},{\"end\":10742,\"start\":10714},{\"attributes\":{\"n\":\"2.2\"},\"end\":11030,\"start\":11001},{\"attributes\":{\"n\":\"3\"},\"end\":13146,\"start\":13112},{\"attributes\":{\"n\":\"3.2\"},\"end\":13603,\"start\":13574},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":14282,\"start\":14278},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":14947,\"start\":14943},{\"attributes\":{\"n\":\"4\"},\"end\":15214,\"start\":15208},{\"attributes\":{\"n\":\"4.1\"},\"end\":16074,\"start\":16042},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":16744,\"start\":16730},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":19651,\"start\":19610},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":20170,\"start\":20133},{\"attributes\":{\"n\":\"4.2\"},\"end\":20463,\"start\":20421},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":21125,\"start\":21094},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":22997,\"start\":22989},{\"attributes\":{\"n\":\"4.3\"},\"end\":23970,\"start\":23933},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":24421,\"start\":24388},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":26552,\"start\":26504},{\"attributes\":{\"n\":\"4.4\"},\"end\":27730,\"start\":27692},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":27787,\"start\":27733},{\"attributes\":{\"n\":\"4.4.4\"},\"end\":27806,\"start\":27790},{\"attributes\":{\"n\":\"5\"},\"end\":28355,\"start\":28345},{\"attributes\":{\"n\":\"5.1\"},\"end\":28447,\"start\":28428},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":28459,\"start\":28450},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":29895,\"start\":29863},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":30127,\"start\":30111},{\"attributes\":{\"n\":\"5.1.4\"},\"end\":30867,\"start\":30844},{\"attributes\":{\"n\":\"5.2\"},\"end\":31748,\"start\":31713},{\"attributes\":{\"n\":\"5.3\"},\"end\":34712,\"start\":34687},{\"attributes\":{\"n\":\"5.4\"},\"end\":35800,\"start\":35766},{\"attributes\":{\"n\":\"5.5\"},\"end\":36635,\"start\":36606},{\"attributes\":{\"n\":\"5.6\"},\"end\":38383,\"start\":38353},{\"attributes\":{\"n\":\"6\"},\"end\":39996,\"start\":39970},{\"end\":40864,\"start\":40854},{\"end\":41064,\"start\":41054},{\"end\":41959,\"start\":41949},{\"end\":42749,\"start\":42742},{\"end\":43265,\"start\":43255},{\"end\":43309,\"start\":43299},{\"end\":43359,\"start\":43349},{\"end\":43626,\"start\":43617},{\"end\":43667,\"start\":43658},{\"end\":45686,\"start\":45677}]", "table": "[{\"end\":43944,\"start\":43688},{\"end\":45675,\"start\":44377},{\"end\":47744,\"start\":46094}]", "figure_caption": "[{\"end\":41052,\"start\":40866},{\"end\":41262,\"start\":41066},{\"end\":41395,\"start\":41265},{\"end\":41947,\"start\":41398},{\"end\":42145,\"start\":41961},{\"end\":42621,\"start\":42148},{\"end\":42740,\"start\":42624},{\"end\":43253,\"start\":42753},{\"end\":43297,\"start\":43267},{\"end\":43347,\"start\":43311},{\"end\":43615,\"start\":43361},{\"end\":43656,\"start\":43628},{\"end\":43688,\"start\":43669},{\"end\":44377,\"start\":43947},{\"end\":46094,\"start\":45688}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5137,\"start\":5129},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6378,\"start\":6370},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15282,\"start\":15273},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15515,\"start\":15507},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17729,\"start\":17721},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21525,\"start\":21517},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23647,\"start\":23639},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25095,\"start\":25087},{\"end\":27060,\"start\":27054},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":36249,\"start\":36241},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36903,\"start\":36895},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36964,\"start\":36955},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36978,\"start\":36969},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":37889,\"start\":37880},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":37903,\"start\":37894},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":38648,\"start\":38639},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":38696,\"start\":38688},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":38802,\"start\":38792},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":39094,\"start\":39086}]", "bib_author_first_name": "[{\"end\":48158,\"start\":48153},{\"end\":48162,\"start\":48159},{\"end\":48172,\"start\":48167},{\"end\":48177,\"start\":48173},{\"end\":48193,\"start\":48185},{\"end\":48195,\"start\":48194},{\"end\":48512,\"start\":48506},{\"end\":48527,\"start\":48522},{\"end\":48777,\"start\":48771},{\"end\":48793,\"start\":48785},{\"end\":48802,\"start\":48799},{\"end\":48818,\"start\":48812},{\"end\":48833,\"start\":48827},{\"end\":48847,\"start\":48843},{\"end\":48861,\"start\":48857},{\"end\":48875,\"start\":48872},{\"end\":48892,\"start\":48885},{\"end\":48904,\"start\":48899},{\"end\":48919,\"start\":48912},{\"end\":48932,\"start\":48926},{\"end\":48945,\"start\":48940},{\"end\":48960,\"start\":48952},{\"end\":48972,\"start\":48967},{\"end\":49507,\"start\":49502},{\"end\":49526,\"start\":49518},{\"end\":49543,\"start\":49536},{\"end\":49555,\"start\":49549},{\"end\":49569,\"start\":49563},{\"end\":49573,\"start\":49570},{\"end\":49586,\"start\":49581},{\"end\":49599,\"start\":49594},{\"end\":49609,\"start\":49607},{\"end\":49618,\"start\":49614},{\"end\":49633,\"start\":49628},{\"end\":50119,\"start\":50113},{\"end\":50134,\"start\":50129},{\"end\":50147,\"start\":50142},{\"end\":50165,\"start\":50159},{\"end\":50182,\"start\":50176},{\"end\":50201,\"start\":50194},{\"end\":50562,\"start\":50557},{\"end\":50579,\"start\":50571},{\"end\":50593,\"start\":50587},{\"end\":50607,\"start\":50599},{\"end\":50850,\"start\":50844},{\"end\":50869,\"start\":50864},{\"end\":50886,\"start\":50877},{\"end\":50903,\"start\":50899},{\"end\":50924,\"start\":50917},{\"end\":50937,\"start\":50931},{\"end\":50958,\"start\":50951},{\"end\":50977,\"start\":50969},{\"end\":50993,\"start\":50988},{\"end\":51010,\"start\":51003},{\"end\":51493,\"start\":51489},{\"end\":51507,\"start\":51499},{\"end\":51517,\"start\":51512},{\"end\":51532,\"start\":51523},{\"end\":51543,\"start\":51539},{\"end\":51554,\"start\":51550},{\"end\":51961,\"start\":51955},{\"end\":51977,\"start\":51970},{\"end\":51992,\"start\":51986},{\"end\":52624,\"start\":52618},{\"end\":52635,\"start\":52629},{\"end\":52652,\"start\":52642},{\"end\":52664,\"start\":52659},{\"end\":52676,\"start\":52670},{\"end\":52687,\"start\":52682},{\"end\":53238,\"start\":53231},{\"end\":53251,\"start\":53244},{\"end\":53265,\"start\":53258},{\"end\":53277,\"start\":53270},{\"end\":53290,\"start\":53282},{\"end\":53583,\"start\":53579},{\"end\":53595,\"start\":53589},{\"end\":53608,\"start\":53601},{\"end\":53622,\"start\":53614},{\"end\":53635,\"start\":53629},{\"end\":53645,\"start\":53642},{\"end\":54197,\"start\":54190},{\"end\":54209,\"start\":54202},{\"end\":54225,\"start\":54217},{\"end\":54235,\"start\":54231},{\"end\":54670,\"start\":54662},{\"end\":54679,\"start\":54675},{\"end\":54691,\"start\":54686},{\"end\":54701,\"start\":54698},{\"end\":54714,\"start\":54706},{\"end\":54726,\"start\":54722},{\"end\":55314,\"start\":55308},{\"end\":55333,\"start\":55323},{\"end\":55757,\"start\":55751},{\"end\":55776,\"start\":55766},{\"end\":56145,\"start\":56144},{\"end\":56161,\"start\":56154},{\"end\":56163,\"start\":56162},{\"end\":56181,\"start\":56180},{\"end\":56198,\"start\":56190},{\"end\":56200,\"start\":56199},{\"end\":56463,\"start\":56459},{\"end\":56483,\"start\":56475},{\"end\":56495,\"start\":56489},{\"end\":56504,\"start\":56500},{\"end\":57032,\"start\":57022},{\"end\":57045,\"start\":57039},{\"end\":57343,\"start\":57337},{\"end\":57357,\"start\":57351},{\"end\":57369,\"start\":57364},{\"end\":57604,\"start\":57600},{\"end\":57621,\"start\":57617},{\"end\":57641,\"start\":57633},{\"end\":57643,\"start\":57642},{\"end\":57984,\"start\":57980},{\"end\":57996,\"start\":57989},{\"end\":58010,\"start\":58002},{\"end\":58020,\"start\":58015},{\"end\":58029,\"start\":58025},{\"end\":58041,\"start\":58036},{\"end\":58057,\"start\":58049},{\"end\":58069,\"start\":58064},{\"end\":58079,\"start\":58076},{\"end\":58091,\"start\":58084},{\"end\":58651,\"start\":58643},{\"end\":58661,\"start\":58656},{\"end\":58674,\"start\":58668},{\"end\":58676,\"start\":58675},{\"end\":59032,\"start\":59030},{\"end\":59044,\"start\":59038},{\"end\":59053,\"start\":59050},{\"end\":59062,\"start\":59059},{\"end\":59073,\"start\":59067},{\"end\":59084,\"start\":59079},{\"end\":59099,\"start\":59092},{\"end\":59112,\"start\":59105},{\"end\":59524,\"start\":59519},{\"end\":59532,\"start\":59529},{\"end\":59546,\"start\":59539},{\"end\":59556,\"start\":59551},{\"end\":59569,\"start\":59563},{\"end\":59578,\"start\":59576},{\"end\":59580,\"start\":59579},{\"end\":60134,\"start\":60129},{\"end\":60142,\"start\":60139},{\"end\":60156,\"start\":60149},{\"end\":60166,\"start\":60161},{\"end\":60179,\"start\":60173},{\"end\":60188,\"start\":60186},{\"end\":60190,\"start\":60189},{\"end\":60705,\"start\":60700},{\"end\":60721,\"start\":60715},{\"end\":60737,\"start\":60732},{\"end\":60749,\"start\":60746},{\"end\":60767,\"start\":60760},{\"end\":61020,\"start\":61016},{\"end\":61032,\"start\":61029},{\"end\":61047,\"start\":61040},{\"end\":61065,\"start\":61058},{\"end\":61080,\"start\":61074},{\"end\":61094,\"start\":61087},{\"end\":61109,\"start\":61103},{\"end\":61120,\"start\":61115},{\"end\":61136,\"start\":61132},{\"end\":61149,\"start\":61145},{\"end\":61420,\"start\":61416},{\"end\":61437,\"start\":61430},{\"end\":61716,\"start\":61711},{\"end\":61729,\"start\":61725},{\"end\":61743,\"start\":61739},{\"end\":61762,\"start\":61753},{\"end\":61774,\"start\":61768},{\"end\":61790,\"start\":61783},{\"end\":61804,\"start\":61799},{\"end\":61814,\"start\":61811},{\"end\":61826,\"start\":61819},{\"end\":62126,\"start\":62119},{\"end\":62144,\"start\":62138},{\"end\":62159,\"start\":62154},{\"end\":62178,\"start\":62171},{\"end\":62550,\"start\":62547},{\"end\":62559,\"start\":62556},{\"end\":62569,\"start\":62565},{\"end\":62582,\"start\":62574},{\"end\":62592,\"start\":62588},{\"end\":62603,\"start\":62598},{\"end\":62612,\"start\":62608},{\"end\":63170,\"start\":63165},{\"end\":63179,\"start\":63177},{\"end\":63607,\"start\":63602},{\"end\":63623,\"start\":63617},{\"end\":63638,\"start\":63630},{\"end\":64054,\"start\":64048},{\"end\":64068,\"start\":64064},{\"end\":64082,\"start\":64078},{\"end\":64096,\"start\":64091},{\"end\":64113,\"start\":64108},{\"end\":64126,\"start\":64121},{\"end\":64128,\"start\":64127},{\"end\":64142,\"start\":64136},{\"end\":64156,\"start\":64151},{\"end\":64471,\"start\":64466},{\"end\":64483,\"start\":64478},{\"end\":64495,\"start\":64491},{\"end\":64507,\"start\":64502},{\"end\":64523,\"start\":64515},{\"end\":64537,\"start\":64530},{\"end\":65039,\"start\":65034},{\"end\":65054,\"start\":65046},{\"end\":65063,\"start\":65059},{\"end\":65074,\"start\":65070},{\"end\":65089,\"start\":65081},{\"end\":65649,\"start\":65641},{\"end\":65659,\"start\":65655},{\"end\":65671,\"start\":65667},{\"end\":65680,\"start\":65676},{\"end\":65688,\"start\":65686},{\"end\":65703,\"start\":65696},{\"end\":66285,\"start\":66277},{\"end\":66295,\"start\":66291},{\"end\":66304,\"start\":66300},{\"end\":66316,\"start\":66312},{\"end\":66329,\"start\":66322},{\"end\":66853,\"start\":66848},{\"end\":66877,\"start\":66869},{\"end\":66890,\"start\":66883},{\"end\":66904,\"start\":66898},{\"end\":67212,\"start\":67206},{\"end\":67221,\"start\":67219},{\"end\":67231,\"start\":67227},{\"end\":67239,\"start\":67237},{\"end\":67250,\"start\":67244},{\"end\":67262,\"start\":67257},{\"end\":67278,\"start\":67269},{\"end\":67287,\"start\":67284},{\"end\":67718,\"start\":67712},{\"end\":67734,\"start\":67725},{\"end\":67746,\"start\":67740},{\"end\":67757,\"start\":67753},{\"end\":67766,\"start\":67763},{\"end\":67776,\"start\":67772},{\"end\":67788,\"start\":67781},{\"end\":67799,\"start\":67794},{\"end\":67808,\"start\":67805},{\"end\":67816,\"start\":67813}]", "bib_author_last_name": "[{\"end\":48106,\"start\":48097},{\"end\":48165,\"start\":48163},{\"end\":48183,\"start\":48178},{\"end\":48202,\"start\":48196},{\"end\":48520,\"start\":48513},{\"end\":48532,\"start\":48528},{\"end\":48769,\"start\":48761},{\"end\":48783,\"start\":48778},{\"end\":48797,\"start\":48794},{\"end\":48810,\"start\":48803},{\"end\":48825,\"start\":48819},{\"end\":48841,\"start\":48834},{\"end\":48855,\"start\":48848},{\"end\":48870,\"start\":48862},{\"end\":48883,\"start\":48876},{\"end\":48897,\"start\":48893},{\"end\":48910,\"start\":48905},{\"end\":48924,\"start\":48920},{\"end\":48938,\"start\":48933},{\"end\":48950,\"start\":48946},{\"end\":48965,\"start\":48961},{\"end\":48976,\"start\":48973},{\"end\":48982,\"start\":48978},{\"end\":49516,\"start\":49508},{\"end\":49534,\"start\":49527},{\"end\":49547,\"start\":49544},{\"end\":49561,\"start\":49556},{\"end\":49579,\"start\":49574},{\"end\":49592,\"start\":49587},{\"end\":49605,\"start\":49600},{\"end\":49612,\"start\":49610},{\"end\":49626,\"start\":49619},{\"end\":49644,\"start\":49634},{\"end\":50127,\"start\":50120},{\"end\":50140,\"start\":50135},{\"end\":50157,\"start\":50148},{\"end\":50174,\"start\":50166},{\"end\":50192,\"start\":50183},{\"end\":50210,\"start\":50202},{\"end\":50569,\"start\":50563},{\"end\":50585,\"start\":50580},{\"end\":50597,\"start\":50594},{\"end\":50617,\"start\":50608},{\"end\":50862,\"start\":50851},{\"end\":50875,\"start\":50870},{\"end\":50897,\"start\":50887},{\"end\":50915,\"start\":50904},{\"end\":50929,\"start\":50925},{\"end\":50949,\"start\":50938},{\"end\":50967,\"start\":50959},{\"end\":50986,\"start\":50978},{\"end\":51001,\"start\":50994},{\"end\":51016,\"start\":51011},{\"end\":51497,\"start\":51494},{\"end\":51510,\"start\":51508},{\"end\":51521,\"start\":51518},{\"end\":51537,\"start\":51533},{\"end\":51548,\"start\":51544},{\"end\":51557,\"start\":51555},{\"end\":51968,\"start\":51962},{\"end\":51984,\"start\":51978},{\"end\":51999,\"start\":51993},{\"end\":52627,\"start\":52625},{\"end\":52640,\"start\":52636},{\"end\":52657,\"start\":52653},{\"end\":52668,\"start\":52665},{\"end\":52680,\"start\":52677},{\"end\":52691,\"start\":52688},{\"end\":53242,\"start\":53239},{\"end\":53256,\"start\":53252},{\"end\":53268,\"start\":53266},{\"end\":53280,\"start\":53278},{\"end\":53293,\"start\":53291},{\"end\":53587,\"start\":53584},{\"end\":53599,\"start\":53596},{\"end\":53612,\"start\":53609},{\"end\":53627,\"start\":53623},{\"end\":53640,\"start\":53636},{\"end\":53649,\"start\":53646},{\"end\":54200,\"start\":54198},{\"end\":54215,\"start\":54210},{\"end\":54229,\"start\":54226},{\"end\":54239,\"start\":54236},{\"end\":54673,\"start\":54671},{\"end\":54684,\"start\":54680},{\"end\":54696,\"start\":54692},{\"end\":54704,\"start\":54702},{\"end\":54720,\"start\":54715},{\"end\":54731,\"start\":54727},{\"end\":55321,\"start\":55315},{\"end\":55345,\"start\":55334},{\"end\":55764,\"start\":55758},{\"end\":55788,\"start\":55777},{\"end\":56152,\"start\":56146},{\"end\":56170,\"start\":56164},{\"end\":56178,\"start\":56172},{\"end\":56188,\"start\":56182},{\"end\":56207,\"start\":56201},{\"end\":56215,\"start\":56209},{\"end\":56473,\"start\":56464},{\"end\":56487,\"start\":56484},{\"end\":56498,\"start\":56496},{\"end\":56508,\"start\":56505},{\"end\":56512,\"start\":56510},{\"end\":57037,\"start\":57033},{\"end\":57053,\"start\":57046},{\"end\":57349,\"start\":57344},{\"end\":57362,\"start\":57358},{\"end\":57378,\"start\":57370},{\"end\":57615,\"start\":57605},{\"end\":57631,\"start\":57622},{\"end\":57650,\"start\":57644},{\"end\":57987,\"start\":57985},{\"end\":58000,\"start\":57997},{\"end\":58013,\"start\":58011},{\"end\":58023,\"start\":58021},{\"end\":58034,\"start\":58030},{\"end\":58047,\"start\":58042},{\"end\":58062,\"start\":58058},{\"end\":58074,\"start\":58070},{\"end\":58082,\"start\":58080},{\"end\":58095,\"start\":58092},{\"end\":58654,\"start\":58652},{\"end\":58666,\"start\":58662},{\"end\":58684,\"start\":58677},{\"end\":59036,\"start\":59033},{\"end\":59048,\"start\":59045},{\"end\":59057,\"start\":59054},{\"end\":59065,\"start\":59063},{\"end\":59077,\"start\":59074},{\"end\":59090,\"start\":59085},{\"end\":59103,\"start\":59100},{\"end\":59116,\"start\":59113},{\"end\":59527,\"start\":59525},{\"end\":59537,\"start\":59533},{\"end\":59549,\"start\":59547},{\"end\":59561,\"start\":59557},{\"end\":59574,\"start\":59570},{\"end\":59584,\"start\":59581},{\"end\":60137,\"start\":60135},{\"end\":60147,\"start\":60143},{\"end\":60159,\"start\":60157},{\"end\":60171,\"start\":60167},{\"end\":60184,\"start\":60180},{\"end\":60194,\"start\":60191},{\"end\":60713,\"start\":60706},{\"end\":60730,\"start\":60722},{\"end\":60744,\"start\":60738},{\"end\":60758,\"start\":60750},{\"end\":60777,\"start\":60768},{\"end\":61027,\"start\":61021},{\"end\":61038,\"start\":61033},{\"end\":61056,\"start\":61048},{\"end\":61072,\"start\":61066},{\"end\":61085,\"start\":61081},{\"end\":61101,\"start\":61095},{\"end\":61113,\"start\":61110},{\"end\":61130,\"start\":61121},{\"end\":61143,\"start\":61137},{\"end\":61155,\"start\":61150},{\"end\":61428,\"start\":61421},{\"end\":61448,\"start\":61438},{\"end\":61723,\"start\":61717},{\"end\":61737,\"start\":61730},{\"end\":61751,\"start\":61744},{\"end\":61766,\"start\":61763},{\"end\":61781,\"start\":61775},{\"end\":61797,\"start\":61791},{\"end\":61809,\"start\":61805},{\"end\":61817,\"start\":61815},{\"end\":61830,\"start\":61827},{\"end\":62136,\"start\":62127},{\"end\":62152,\"start\":62145},{\"end\":62169,\"start\":62160},{\"end\":62183,\"start\":62179},{\"end\":62554,\"start\":62551},{\"end\":62563,\"start\":62560},{\"end\":62572,\"start\":62570},{\"end\":62586,\"start\":62583},{\"end\":62596,\"start\":62593},{\"end\":62606,\"start\":62604},{\"end\":62618,\"start\":62613},{\"end\":63175,\"start\":63171},{\"end\":63184,\"start\":63180},{\"end\":63615,\"start\":63608},{\"end\":63628,\"start\":63624},{\"end\":63647,\"start\":63639},{\"end\":63658,\"start\":63649},{\"end\":64062,\"start\":64055},{\"end\":64076,\"start\":64069},{\"end\":64089,\"start\":64083},{\"end\":64106,\"start\":64097},{\"end\":64119,\"start\":64114},{\"end\":64134,\"start\":64129},{\"end\":64149,\"start\":64143},{\"end\":64167,\"start\":64157},{\"end\":64476,\"start\":64472},{\"end\":64489,\"start\":64484},{\"end\":64500,\"start\":64496},{\"end\":64513,\"start\":64508},{\"end\":64528,\"start\":64524},{\"end\":64541,\"start\":64538},{\"end\":65044,\"start\":65040},{\"end\":65057,\"start\":65055},{\"end\":65068,\"start\":65064},{\"end\":65079,\"start\":65075},{\"end\":65094,\"start\":65090},{\"end\":65653,\"start\":65650},{\"end\":65665,\"start\":65660},{\"end\":65674,\"start\":65672},{\"end\":65684,\"start\":65681},{\"end\":65694,\"start\":65689},{\"end\":65706,\"start\":65704},{\"end\":66289,\"start\":66286},{\"end\":66298,\"start\":66296},{\"end\":66310,\"start\":66305},{\"end\":66320,\"start\":66317},{\"end\":66332,\"start\":66330},{\"end\":66867,\"start\":66854},{\"end\":66881,\"start\":66878},{\"end\":66896,\"start\":66891},{\"end\":66910,\"start\":66905},{\"end\":66916,\"start\":66912},{\"end\":67217,\"start\":67213},{\"end\":67225,\"start\":67222},{\"end\":67235,\"start\":67232},{\"end\":67242,\"start\":67240},{\"end\":67255,\"start\":67251},{\"end\":67267,\"start\":67263},{\"end\":67282,\"start\":67279},{\"end\":67291,\"start\":67288},{\"end\":67723,\"start\":67719},{\"end\":67738,\"start\":67735},{\"end\":67751,\"start\":67747},{\"end\":67761,\"start\":67758},{\"end\":67770,\"start\":67767},{\"end\":67779,\"start\":67777},{\"end\":67792,\"start\":67789},{\"end\":67803,\"start\":67800},{\"end\":67811,\"start\":67809},{\"end\":67820,\"start\":67817}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":48149,\"start\":48095},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b1\"},\"end\":48378,\"start\":48151},{\"attributes\":{\"doi\":\"arXiv:2006.11477\",\"id\":\"b2\"},\"end\":48757,\"start\":48380},{\"attributes\":{\"id\":\"b3\"},\"end\":49209,\"start\":48759},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3352400},\"end\":49459,\"start\":49211},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1145979},\"end\":50033,\"start\":49461},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15300820},\"end\":50473,\"start\":50035},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b7\"},\"end\":50842,\"start\":50475},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b8\"},\"end\":51428,\"start\":50844},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":160030052},\"end\":51914,\"start\":51430},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2239473},\"end\":52510,\"start\":51916},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":224282883},\"end\":53156,\"start\":52512},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":970388},\"end\":53458,\"start\":53158},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":196171043},\"end\":54142,\"start\":53460},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":54579,\"start\":54144},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":211043589},\"end\":55228,\"start\":54581},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1159769},\"end\":55749,\"start\":55230},{\"attributes\":{\"doi\":\"arXiv:1511.06939\",\"id\":\"b17\"},\"end\":56106,\"start\":55751},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":572361},\"end\":56393,\"start\":56108},{\"attributes\":{\"id\":\"b19\"},\"end\":56978,\"start\":56395},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":52127932},\"end\":57278,\"start\":56980},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":58370896},\"end\":57533,\"start\":57280},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":195908774},\"end\":57905,\"start\":57535},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":119120813},\"end\":58575,\"start\":57907},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":208780552},\"end\":59028,\"start\":58577},{\"attributes\":{\"doi\":\"arXiv:2103.14030\",\"id\":\"b25\"},\"end\":59431,\"start\":59030},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":50770252},\"end\":60041,\"start\":59433},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":50770252},\"end\":60651,\"start\":60043},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":17048224},\"end\":60976,\"start\":60653},{\"attributes\":{\"id\":\"b29\"},\"end\":61353,\"start\":60978},{\"attributes\":{\"id\":\"b30\"},\"end\":61626,\"start\":61355},{\"attributes\":{\"doi\":\"arXiv:1910.10683\",\"id\":\"b31\"},\"end\":62117,\"start\":61628},{\"attributes\":{\"doi\":\"arXiv:1904.05862\",\"id\":\"b32\"},\"end\":62448,\"start\":62119},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":119181611},\"end\":63080,\"start\":62450},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":39847715},\"end\":63559,\"start\":63082},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":7118498},\"end\":64019,\"start\":63561},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":13756489},\"end\":64388,\"start\":64021},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3742725},\"end\":64994,\"start\":64390},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":150380651},\"end\":65539,\"start\":64996},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":220730199},\"end\":66221,\"start\":65541},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235792434},\"end\":66788,\"start\":66223},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":27308776},\"end\":67137,\"start\":66790},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":52188056},\"end\":67653,\"start\":67139},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1637394},\"end\":68289,\"start\":67655}]", "bib_title": "[{\"end\":49255,\"start\":49211},{\"end\":49500,\"start\":49461},{\"end\":50111,\"start\":50035},{\"end\":51487,\"start\":51430},{\"end\":51953,\"start\":51916},{\"end\":52616,\"start\":52512},{\"end\":53229,\"start\":53158},{\"end\":53577,\"start\":53460},{\"end\":54188,\"start\":54144},{\"end\":54660,\"start\":54581},{\"end\":55306,\"start\":55230},{\"end\":56142,\"start\":56108},{\"end\":56457,\"start\":56395},{\"end\":57020,\"start\":56980},{\"end\":57335,\"start\":57280},{\"end\":57598,\"start\":57535},{\"end\":57978,\"start\":57907},{\"end\":58641,\"start\":58577},{\"end\":59517,\"start\":59433},{\"end\":60127,\"start\":60043},{\"end\":60698,\"start\":60653},{\"end\":62545,\"start\":62450},{\"end\":63163,\"start\":63082},{\"end\":63600,\"start\":63561},{\"end\":64046,\"start\":64021},{\"end\":64464,\"start\":64390},{\"end\":65032,\"start\":64996},{\"end\":65639,\"start\":65541},{\"end\":66275,\"start\":66223},{\"end\":66846,\"start\":66790},{\"end\":67204,\"start\":67139},{\"end\":67710,\"start\":67655}]", "bib_author": "[{\"end\":48108,\"start\":48097},{\"end\":48167,\"start\":48153},{\"end\":48185,\"start\":48167},{\"end\":48204,\"start\":48185},{\"end\":48522,\"start\":48506},{\"end\":48534,\"start\":48522},{\"end\":48771,\"start\":48761},{\"end\":48785,\"start\":48771},{\"end\":48799,\"start\":48785},{\"end\":48812,\"start\":48799},{\"end\":48827,\"start\":48812},{\"end\":48843,\"start\":48827},{\"end\":48857,\"start\":48843},{\"end\":48872,\"start\":48857},{\"end\":48885,\"start\":48872},{\"end\":48899,\"start\":48885},{\"end\":48912,\"start\":48899},{\"end\":48926,\"start\":48912},{\"end\":48940,\"start\":48926},{\"end\":48952,\"start\":48940},{\"end\":48967,\"start\":48952},{\"end\":48978,\"start\":48967},{\"end\":48984,\"start\":48978},{\"end\":49518,\"start\":49502},{\"end\":49536,\"start\":49518},{\"end\":49549,\"start\":49536},{\"end\":49563,\"start\":49549},{\"end\":49581,\"start\":49563},{\"end\":49594,\"start\":49581},{\"end\":49607,\"start\":49594},{\"end\":49614,\"start\":49607},{\"end\":49628,\"start\":49614},{\"end\":49646,\"start\":49628},{\"end\":50129,\"start\":50113},{\"end\":50142,\"start\":50129},{\"end\":50159,\"start\":50142},{\"end\":50176,\"start\":50159},{\"end\":50194,\"start\":50176},{\"end\":50212,\"start\":50194},{\"end\":50571,\"start\":50557},{\"end\":50587,\"start\":50571},{\"end\":50599,\"start\":50587},{\"end\":50619,\"start\":50599},{\"end\":50864,\"start\":50844},{\"end\":50877,\"start\":50864},{\"end\":50899,\"start\":50877},{\"end\":50917,\"start\":50899},{\"end\":50931,\"start\":50917},{\"end\":50951,\"start\":50931},{\"end\":50969,\"start\":50951},{\"end\":50988,\"start\":50969},{\"end\":51003,\"start\":50988},{\"end\":51018,\"start\":51003},{\"end\":51499,\"start\":51489},{\"end\":51512,\"start\":51499},{\"end\":51523,\"start\":51512},{\"end\":51539,\"start\":51523},{\"end\":51550,\"start\":51539},{\"end\":51559,\"start\":51550},{\"end\":51970,\"start\":51955},{\"end\":51986,\"start\":51970},{\"end\":52001,\"start\":51986},{\"end\":52629,\"start\":52618},{\"end\":52642,\"start\":52629},{\"end\":52659,\"start\":52642},{\"end\":52670,\"start\":52659},{\"end\":52682,\"start\":52670},{\"end\":52693,\"start\":52682},{\"end\":53244,\"start\":53231},{\"end\":53258,\"start\":53244},{\"end\":53270,\"start\":53258},{\"end\":53282,\"start\":53270},{\"end\":53295,\"start\":53282},{\"end\":53589,\"start\":53579},{\"end\":53601,\"start\":53589},{\"end\":53614,\"start\":53601},{\"end\":53629,\"start\":53614},{\"end\":53642,\"start\":53629},{\"end\":53651,\"start\":53642},{\"end\":54202,\"start\":54190},{\"end\":54217,\"start\":54202},{\"end\":54231,\"start\":54217},{\"end\":54241,\"start\":54231},{\"end\":54675,\"start\":54662},{\"end\":54686,\"start\":54675},{\"end\":54698,\"start\":54686},{\"end\":54706,\"start\":54698},{\"end\":54722,\"start\":54706},{\"end\":54733,\"start\":54722},{\"end\":55323,\"start\":55308},{\"end\":55347,\"start\":55323},{\"end\":55766,\"start\":55751},{\"end\":55790,\"start\":55766},{\"end\":56154,\"start\":56144},{\"end\":56172,\"start\":56154},{\"end\":56180,\"start\":56172},{\"end\":56190,\"start\":56180},{\"end\":56209,\"start\":56190},{\"end\":56217,\"start\":56209},{\"end\":56475,\"start\":56459},{\"end\":56489,\"start\":56475},{\"end\":56500,\"start\":56489},{\"end\":56510,\"start\":56500},{\"end\":56514,\"start\":56510},{\"end\":57039,\"start\":57022},{\"end\":57055,\"start\":57039},{\"end\":57351,\"start\":57337},{\"end\":57364,\"start\":57351},{\"end\":57380,\"start\":57364},{\"end\":57617,\"start\":57600},{\"end\":57633,\"start\":57617},{\"end\":57652,\"start\":57633},{\"end\":57989,\"start\":57980},{\"end\":58002,\"start\":57989},{\"end\":58015,\"start\":58002},{\"end\":58025,\"start\":58015},{\"end\":58036,\"start\":58025},{\"end\":58049,\"start\":58036},{\"end\":58064,\"start\":58049},{\"end\":58076,\"start\":58064},{\"end\":58084,\"start\":58076},{\"end\":58097,\"start\":58084},{\"end\":58656,\"start\":58643},{\"end\":58668,\"start\":58656},{\"end\":58686,\"start\":58668},{\"end\":59038,\"start\":59030},{\"end\":59050,\"start\":59038},{\"end\":59059,\"start\":59050},{\"end\":59067,\"start\":59059},{\"end\":59079,\"start\":59067},{\"end\":59092,\"start\":59079},{\"end\":59105,\"start\":59092},{\"end\":59118,\"start\":59105},{\"end\":59529,\"start\":59519},{\"end\":59539,\"start\":59529},{\"end\":59551,\"start\":59539},{\"end\":59563,\"start\":59551},{\"end\":59576,\"start\":59563},{\"end\":59586,\"start\":59576},{\"end\":60139,\"start\":60129},{\"end\":60149,\"start\":60139},{\"end\":60161,\"start\":60149},{\"end\":60173,\"start\":60161},{\"end\":60186,\"start\":60173},{\"end\":60196,\"start\":60186},{\"end\":60715,\"start\":60700},{\"end\":60732,\"start\":60715},{\"end\":60746,\"start\":60732},{\"end\":60760,\"start\":60746},{\"end\":60779,\"start\":60760},{\"end\":61029,\"start\":61016},{\"end\":61040,\"start\":61029},{\"end\":61058,\"start\":61040},{\"end\":61074,\"start\":61058},{\"end\":61087,\"start\":61074},{\"end\":61103,\"start\":61087},{\"end\":61115,\"start\":61103},{\"end\":61132,\"start\":61115},{\"end\":61145,\"start\":61132},{\"end\":61157,\"start\":61145},{\"end\":61430,\"start\":61416},{\"end\":61450,\"start\":61430},{\"end\":61725,\"start\":61711},{\"end\":61739,\"start\":61725},{\"end\":61753,\"start\":61739},{\"end\":61768,\"start\":61753},{\"end\":61783,\"start\":61768},{\"end\":61799,\"start\":61783},{\"end\":61811,\"start\":61799},{\"end\":61819,\"start\":61811},{\"end\":61832,\"start\":61819},{\"end\":62138,\"start\":62119},{\"end\":62154,\"start\":62138},{\"end\":62171,\"start\":62154},{\"end\":62185,\"start\":62171},{\"end\":62556,\"start\":62547},{\"end\":62565,\"start\":62556},{\"end\":62574,\"start\":62565},{\"end\":62588,\"start\":62574},{\"end\":62598,\"start\":62588},{\"end\":62608,\"start\":62598},{\"end\":62620,\"start\":62608},{\"end\":63177,\"start\":63165},{\"end\":63186,\"start\":63177},{\"end\":63617,\"start\":63602},{\"end\":63630,\"start\":63617},{\"end\":63649,\"start\":63630},{\"end\":63660,\"start\":63649},{\"end\":64064,\"start\":64048},{\"end\":64078,\"start\":64064},{\"end\":64091,\"start\":64078},{\"end\":64108,\"start\":64091},{\"end\":64121,\"start\":64108},{\"end\":64136,\"start\":64121},{\"end\":64151,\"start\":64136},{\"end\":64169,\"start\":64151},{\"end\":64478,\"start\":64466},{\"end\":64491,\"start\":64478},{\"end\":64502,\"start\":64491},{\"end\":64515,\"start\":64502},{\"end\":64530,\"start\":64515},{\"end\":64543,\"start\":64530},{\"end\":65046,\"start\":65034},{\"end\":65059,\"start\":65046},{\"end\":65070,\"start\":65059},{\"end\":65081,\"start\":65070},{\"end\":65096,\"start\":65081},{\"end\":65655,\"start\":65641},{\"end\":65667,\"start\":65655},{\"end\":65676,\"start\":65667},{\"end\":65686,\"start\":65676},{\"end\":65696,\"start\":65686},{\"end\":65708,\"start\":65696},{\"end\":66291,\"start\":66277},{\"end\":66300,\"start\":66291},{\"end\":66312,\"start\":66300},{\"end\":66322,\"start\":66312},{\"end\":66334,\"start\":66322},{\"end\":66869,\"start\":66848},{\"end\":66883,\"start\":66869},{\"end\":66898,\"start\":66883},{\"end\":66912,\"start\":66898},{\"end\":66918,\"start\":66912},{\"end\":67219,\"start\":67206},{\"end\":67227,\"start\":67219},{\"end\":67237,\"start\":67227},{\"end\":67244,\"start\":67237},{\"end\":67257,\"start\":67244},{\"end\":67269,\"start\":67257},{\"end\":67284,\"start\":67269},{\"end\":67293,\"start\":67284},{\"end\":67725,\"start\":67712},{\"end\":67740,\"start\":67725},{\"end\":67753,\"start\":67740},{\"end\":67763,\"start\":67753},{\"end\":67772,\"start\":67763},{\"end\":67781,\"start\":67772},{\"end\":67794,\"start\":67781},{\"end\":67805,\"start\":67794},{\"end\":67813,\"start\":67805},{\"end\":67822,\"start\":67813}]", "bib_venue": "[{\"end\":48504,\"start\":48380},{\"end\":49309,\"start\":49257},{\"end\":49709,\"start\":49646},{\"end\":50237,\"start\":50212},{\"end\":50555,\"start\":50475},{\"end\":51108,\"start\":51034},{\"end\":51625,\"start\":51559},{\"end\":52139,\"start\":52001},{\"end\":52783,\"start\":52693},{\"end\":53300,\"start\":53295},{\"end\":53747,\"start\":53651},{\"end\":54318,\"start\":54241},{\"end\":54844,\"start\":54733},{\"end\":55439,\"start\":55347},{\"end\":55908,\"start\":55806},{\"end\":56235,\"start\":56217},{\"end\":56625,\"start\":56514},{\"end\":57111,\"start\":57055},{\"end\":57388,\"start\":57380},{\"end\":57701,\"start\":57652},{\"end\":58189,\"start\":58097},{\"end\":58705,\"start\":58686},{\"end\":59205,\"start\":59134},{\"end\":59682,\"start\":59586},{\"end\":60292,\"start\":60196},{\"end\":60790,\"start\":60779},{\"end\":61014,\"start\":60978},{\"end\":61414,\"start\":61355},{\"end\":61709,\"start\":61628},{\"end\":62258,\"start\":62201},{\"end\":62712,\"start\":62620},{\"end\":63272,\"start\":63186},{\"end\":63720,\"start\":63660},{\"end\":64188,\"start\":64169},{\"end\":64639,\"start\":64543},{\"end\":65207,\"start\":65096},{\"end\":65819,\"start\":65708},{\"end\":66445,\"start\":66334},{\"end\":66923,\"start\":66918},{\"end\":67354,\"start\":67293},{\"end\":67918,\"start\":67822},{\"end\":49357,\"start\":49311},{\"end\":49759,\"start\":49711},{\"end\":52264,\"start\":52141},{\"end\":52860,\"start\":52785},{\"end\":53830,\"start\":53749},{\"end\":54382,\"start\":54320},{\"end\":54942,\"start\":54846},{\"end\":55518,\"start\":55441},{\"end\":56723,\"start\":56627},{\"end\":58268,\"start\":58191},{\"end\":58784,\"start\":58764},{\"end\":59765,\"start\":59684},{\"end\":60375,\"start\":60294},{\"end\":62791,\"start\":62714},{\"end\":63345,\"start\":63274},{\"end\":64722,\"start\":64641},{\"end\":65305,\"start\":65209},{\"end\":65917,\"start\":65821},{\"end\":66543,\"start\":66447},{\"end\":66945,\"start\":66925},{\"end\":67402,\"start\":67356},{\"end\":68001,\"start\":67920}]"}}}, "year": 2023, "month": 12, "day": 17}