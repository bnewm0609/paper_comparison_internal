{"id": 220546332, "updated": "2023-10-06 12:55:44.633", "metadata": {"title": "Shape Prior Deformation for Categorical 6D Object Pose and Size Estimation", "authors": "[{\"first\":\"Meng\",\"last\":\"Tian\",\"middle\":[]},{\"first\":\"Marcelo\",\"last\":\"Ang\",\"middle\":[\"H\"]},{\"first\":\"Gim\",\"last\":\"Lee\",\"middle\":[\"Hee\"]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": 7, "day": 16}, "abstract": "We present a novel learning approach to recover the 6D poses and sizes of unseen object instances from an RGB-D image. To handle the intra-class shape variation, we propose a deep network to reconstruct the 3D object model by explicitly modeling the deformation from a pre-learned categorical shape prior. Additionally, our network infers the dense correspondences between the depth observation of the object instance and the reconstructed 3D model to jointly estimate the 6D object pose and size. We design an autoencoder that trains on a collection of object models and compute the mean latent embedding for each category to learn the categorical shape priors. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach significantly outperforms the state of the art. Our code is available at https://github.com/mentian/object-deformnet.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.08454", "mag": "3107992529", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/TianAL20", "doi": "10.1007/978-3-030-58589-1_32"}}, "content": {"source": {"pdf_hash": "fddcc5d3c2f66e7973974af55a2b3ad7a8de6e42", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.08454v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.08454", "status": "GREEN"}}, "grobid": {"id": "9f73acf5c0deb234fb2f72afe2d81d178d412082", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fddcc5d3c2f66e7973974af55a2b3ad7a8de6e42.txt", "contents": "\nShape Prior Deformation for Categorical 6D Object Pose and Size Estimation\n\n\n\nNational University of Singapore\nSingapore\n\nShape Prior Deformation for Categorical 6D Object Pose and Size Estimation\ncategory-level pose estimation3D object detectionshape generationscene understanding\nMeng Tian [0000\u22120001\u22129937\u22128975] , Marcelo H Ang Jr [0000\u22120001\u22128277\u22126408] , Gim Hee Lee [0000\u22120002\u22121583\u22120475]Abstract. We present a novel learning approach to recover the 6D poses and sizes of unseen object instances from an RGB-D image. To handle the intra-class shape variation, we propose a deep network to reconstruct the 3D object model by explicitly modeling the deformation from a pre-learned categorical shape prior. Additionally, our network infers the dense correspondences between the depth observation of the object instance and the reconstructed 3D model to jointly estimate the 6D object pose and size. We design an autoencoder that trains on a collection of object models and compute the mean latent embedding for each category to learn the categorical shape priors. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach significantly outperforms the state of the art. Our code is available at https://github.com/mentian/object-deformnet.\n\nIntroduction\n\nAccurate 6D object pose estimation plays an important role in a variety of tasks, such as augmented reality, robotic manipulation, scene understanding, etc. In recent years, substantial progress has been made for instance-level 6D object pose estimation, where the exact 3D object models for pose estimation are given. Unfortunately, these methods [18,32,42] cannot be directly generalized to category-level 6D object pose estimation on new object instances with unknown 3D models. Consequently, the category, 6D pose and size of the objects have to be concurrently estimated. Although some other object instances from each category are provided as priors, the high variation of object shapes within the same category makes generalization to new object instances extremely challenging.\n\nTo the best of our knowledge, [24] is the first work to address the 6D object pose estimation problem at category-level. This approach defines 6D pose on semantically selected centers and trains a part-based random forest to recover the pose. However, building part representations upon 3D skeleton structures limits the generalization capability across unseen object instances. Another work arXiv:2007.08454v1 [cs.CV] 16 Jul 2020 [33] proposes the first data-driven solution and creates a benchmark dataset for this task. They introduce the Normalized Object Coordinate Space (NOCS) to represent different object instances within a category in a unified manner. A region-based network is trained to infer correspondences from object pixels to the points in NOCS. Class label and instance mask of each object are also obtained at the same time. These predictions are used together with the depth map to estimate the 6D pose and size of the object via point matching. However, the lack of explicit representation of shape variations limits their performance.\n\nIn this work, we propose to reconstruct the complete object models in the NOCS to capture the intra-class shape variation. More specifically, we first learn the categorical shape priors from the given object instances, and then train a network to estimate the deformation field of the shape prior (that is used to get the reconstructed object model) and the correspondences between object observation and the reconstructed model. The shape prior serves as prior knowledge of the category and encodes geometrical characteristics that are shared by objects of a given category. The deformation predicted by our network captures the instance-specific shape details, i.e. shape variation of that particular instance. We present a method which is applicable across different object categories and data representations to learn the shape priors. In particular, an autoencoder is trained on a collection of object models from various categories. For each category, we compute the mean latent embedding over all instances in the respective category. The categorical shape prior is constructed by passing the mean embedding through a decoder. Note that there is no restriction on the data representation (point cloud, mesh, or voxel) of shape priors or collected models as long as we choose a proper architecture for the encoder and decoder.\n\nWe use the Umeyama algorithm [29] to recover the 6D pose and metric size of the object from the correspondences estimated by our network that maps the point cloud obtained from the observed depth map to the points in NOCS. We evaluate our method on two standard benchmarks. Extensive experiments demonstrate the advantage of our network and prove the effectiveness of explicitly modeling the deformation. In summary, the main contributions of this work are:\n\n-We propose a novel deep network for category-level 6D object pose and size estimation; our network explicitly models the deformation from the categorical shape prior to the object model. -We present a learning-based method which utilizes the latent embeddings to construct the shape prior; our method is applicable across different categories and data representations. -Our network achieves significantly higher mean average precisions on both synthetic and real-world benchmark datasets.\n\n\nRelated Work\n\nInstance-Level Pose Estimation. Existing instance-level pose estimation approaches broadly fall into three categories. The first category casts votes in the pose space and further refines coarse pose with algorithms such as iterative closest point. LINEMOD [9] uses holistic template matching to find the nearest viewpoint. [26] generates a latent code for the input image and search for its nearest neighbor in the codebook. [11,27] aggregate the 6D votes cast by locally-sampled RGB-D patches. The second category directly maps input image to object pose. [10,37] extend 2D object detection network such that it can predict orientation as an add-on to the identity and 2D bounding box of the object. [15,32] regress 6D pose from RGB-D images in an end-to-end framework. The third category relies on establishing point correspondences. [2,12,17] regress the corresponding 3D object coordinate for each foreground pixel. [18,23,28] detect the keypoints of the object on image and then solve a Perspective-n-Point problem. [42] estimates a dense 2D-3D correspondence map between the input image and object model. Although our approach follows the approach from the third category, our task focuses on a more general setting where the object models are not available during inference.\n\nCategory-Level Object Detection. The task of 3D object detection aims to estimate 3D bounding boxes of objects in the scene. [25] runs sliding windows in 3D space and generates amodal proposals for objects. [6,14,21] first generate 2D object proposals and then lift the proposals to 3D space. [38,44] are single-stage detectors which directly detect objects from 3D data. Although the above mentioned methods address the problem at category-level, the considered objects are usually constrained to the ground surface, e.g. instances of typical furniture classes in indoor scenes, cars, pedestrians, and cyclists in outdoor scenes. Consequently, the assumption that rotation is constrained to be only along the gravity direction has to be made. On the contrary, our approach can recover the full 6D pose of objects.\n\nCategory-Level Pose Estimation. There are only a few pioneering works focusing on estimating 6D pose of unseen objects. [3] leverages a generative representation of 3D objects and produces a multimodal distribution over poses with mixture density network. However, only rotation is considered in their work. [24] introduces a part-based random forest which employs simple depth comparison features, but our approach deals with RGB-D images. [33] proposes a canonical representation for all instances within an object category. Our approach also makes use of this representation. Instead of directly regressing the coordinates in NOCS, we account for intra-class shape variation by explicitly modeling the deformation from shape prior to object model. [4] trains a variational autoencoder to generate the complete object model. However, the reconstructed shape is not utilized for pose estimation. In our network, shape reconstruction and pose estimation are integrated together. [31] proposes the first category-level pose tracker, while our approach performs pose estimation without using temporal information.\n\nShape Deformation. 3D deformation is commonly applied for object reconstruction from a single image. [13,20,41] use free-form deformation in conjunction with voxel, mesh and point cloud representations, respectively. [34,36] starts from a coarse shape and predicts a series of deformations to progressively improve the  We first obtain a foreground mask for each object instance. Next our network reconstructs the instance (bowl as an example) and establishes the correspondences between the observed points and the reconstructed model. Finally, the 6D pose is recovered by estimating a similarity transformation. Refer to Fig. 3 for the details of our network.\n\ngeometry. Similar to us, [35] also supervise the deformation with global feature of the target. However, we circumvent the fixed topology assumption of mesh representation by using point clouds instead.\n\n\nOur Method\n\nBackground. Given an RGB-D image as the input, our goal is to detect and localize all visible object instances in the 3D space. The object instances are not seen previously, but must come from known categories. Each object is represented by a class label and an amodal 3D bounding box parameterized by its 6D pose and size. The 6D pose is defined to be the rigid transformation (i.e. rotation and translation) that transforms the object from the reference to the camera coordinate frame. It is common to choose the coordinate frame of the given 3D object models as reference in instance-level 6D object pose estimation. Unfortunately, this is not viable for our category-level task since the instances of the 3D models are not available. To mitigate this problem, we leverage on the Normalized Object Coordinate Space (NOCS) -a shared canonical representation for all possible object instances within a category proposed in [33]. The categorical 6D object pose and size estimation problem is then reduced to finding the similarity transformation between the observed depth map of each object instance and its corresponding points in the NOCS (i.e. NOCS coordinates).\n\nOverview. In contrast to [33] that directly outputs the NOCS coordinates from a Convolutional Neural Network (CNN), we propose an intermediate step to estimate the deformation of a pre-learned shape prior to improve the learning of intra-class shape variation. Our shape priors are learned from a collection of models spanning all categories (Section 3.1). As shown in Fig. 1, our approach consists of three stages. The first stage performs instance segmentation on color image using an off-the-shelf network (e.g. Mask R-CNN [7]). Next we convert the masked depth map into a point cloud with camera intrinsic parameters for each instance and crop an image patch according to the bounding box of the mask. Taking the point cloud, image patch, and the corresponding shape prior as inputs, our network outputs a deformation field that deforms the shape prior into the shape of the desired object instance (a.k.a. reconstructed model). Furthermore, our network outputs a set of correspondences that associates each point in the point cloud obtained from the observed depth map of the object instance with the points of the reconstructed model. This set of correspondences is used to mask the reconstructed model into the NOCS coordinates (Section 3.2). Finally, the 6D pose and size of the object can be estimated by registering the NOCS coordinates and the point cloud obtained from the observed depth map (Section 3.3).\n\n\nCategorical Shape Prior\n\nAlthough object shape varies among different instances, an investigation over the 3D models reveals that objects of the same category (especially artificially generated objects) tend to have semantically and geometrically similar components. For example, cameras are usually made up of a nearly cuboid body and a cylindrical lens; and mugs are typically cylindrical with a handle. These categorical characteristics provide strong priors on the shape reconstruction of novel instances. We propose the learning of a mean shape to capture the high-level characteristics from all the available models for each respective category. To this end, we first train an autoencoder with all available object models and then compute the mean latent embedding of each object category with the encoder. These latent embeddings are passed into the decoder to get the mean shape priors for each object category. Unlike methods such as simple averaging [30] and principal component analysis (PCA) [3] that operate on voxel representations, our autoencoder framework can be easily altered to take any 3D representations.\n\nGiven a shape collection M = {M i c | i = 1, 2, \u00b7 \u00b7 \u00b7 , N ; c = 1, 2, \u00b7 \u00b7 \u00b7 , C}, where M i c is the 3D point cloud model of instance i from category c, we independently apply a similarity transformation to each model such that it is properly aligned in the NOCS. This step ensures that the learned shape prior has the same scale and orientation as the target shape to be reconstructed. The encoder \u03a6 takes the point cloud and outputs a low-dimensional feature vector, i.e. the latent embedding z i c \u2208 R n . The decoder \u03a8 takes this feature vector and outputs a point cloud that reconstructs the input:\nM i c = (\u03a8 \u2022 \u03a6)(M i c ) = \u03a8 (z i c ).(1)\nSpecifically, we adopt the PointNet-like encoder proposed in [40], and a threelayer fully-connected decoder as shown in Fig. 2a  measured by the Chamfer distance:\nd CD (M i c ,M i c ) = x\u2208M i c min y\u2208M i c x \u2212 y 2 2 + y\u2208M i c min x\u2208M i c x \u2212 y 2 2 .(2)\nThe autoencoder is trained on a shape collection by minimizing the reconstruction error. Once the training is converged, we obtain the latent embeddings {z i c } of all instances in M. Although not explicitly enforced during training, these latent embeddings form clusters in the latent space according to their categories. Fig. 2b visualizes the clustering effect of the embeddings. We use T-SNE [16] to further embed these features in R 2 for visualization. Similar clustering results are also observed on a different set of models [39]. Based on this observation, we compute the mean latent embedding for each category and then pass it through the decoder to construct the shape prior:\nM c = \u03a8 (z c ) = \u03a8 1 N c i z i c .(3)\nThe resulting categorical shape priors {M c } are shown in Fig. 2c.\n\n\nOur Network Architecture\n\nWe denote the observation of an object instance as (V, I), where V \u2208 R Nv\u00d73 is the point cloud and I \u2208 R H\u00d7W \u00d73 is the image patch. N v denotes the number On the consideration that the depth and color are two different modalities, we follow the pixel-wise dense fusion approach proposed in [32] to effectively extract RGB-D features from the observation. For point cloud V , we use an embedding network similar to PointNet [22] to generate per-point geometric features by mapping each point in V to the d g -dimensional feature space. The image patch I is processed with a fully convolutional network which follows an encoder-decoder architecture and maps I to R H\u00d7W \u00d7dc . Next we associate the geometric feature of each point with its corresponding color feature and concatenate the feature pairs. Since each point in V has a corresponding pixel in I, not vice versa, redundant color features are discarded. The concatenated features are termed \"instance point features\" and fed to another shared multilayer perceptron. An average pooling layer is used to generate the \"instance global feature\". The categorical shape prior M c is a point cloud with purely geometric information. We apply a simpler embedding network to extract the \"category point features\" and the \"category global feature\".\n\nThe shape prior M c provides the prior knowledge of the category, i.e. the coarse shape geometry and canonical pose. Although the observation (V, I) is partial, it provides instance-specific details of the target shape. A natural way to reconstruct the object in NOCS is to deform M c under the guidance of (V, I). Consequently, we concatenate the category and instance global features, and enrich the category point features with the concatenated features. The obtained feature vectors are successively convolved with 1 \u00d7 1 kernels to generate the deformation field D. Similar intuition and feature concatenation strategy also apply to the estimation of A. We combine the instance point features and global feature to aggregate both local and global information for each point. Each point in V gets mapped to the points of the reconstructed model through concatenation with the category global feature. We obtain the NOCS coordinates, denoted as P , of the points in V by multiplying A and M , i.e.\nP = A \u00d7 M = A(M c + D) \u2208 R Nv\u00d73 .(4)\n\n6D Pose Estimation\n\nOur goal is to estimate the 6D pose and size of the object instance. Given depth observation V and its NOCS coordinates P , the optimal similarity transformation parameters (rotation, translation, and scaling) can be computed by solving the absolute orientation problem using Umeyama algorithm [29]. We also implement the RANSAC algorithm [5] for robust estimation.\n\n\nLoss Functions\n\nIn this section, we define the loss functions used to train our network, and explain how we handle object symmetry during training. Correspondence Loss. It is impractical and unnecessary to pre-compute the ground-truth value for A. Alternatively, we supervise A indirectly via the NOCS coordinates P (which is a result of applying the correspondence matrix on the reconstructed model) since the ground-truth NOCS coordinates P gt can be obtained easily from the object model and its 6D pose through image rendering. We use the smooth L 1 loss function:\nL corr (P, P gt ) = 1 N v x\u2208P i=1,2,3 5(x i \u2212 y i ) 2 , if |x i \u2212 y i | \u2264 0.1 |x i \u2212 y i | \u2212 0.05, otherwise ,(5)\nwhere x = (x 1 , x 2 , x 3 ) \u2208 P , and y = (y 1 , y 2 , y 3 ) \u2208 P gt . Object symmetry is an inevitable problem for pose estimation algorithms, especially for those that require supervised training. For symmetrical objects, there exists at least one rotation such that appearance of the object is preserved under this rotation. In other words, two observations of a symmetric object can be very similar but with different rotation labels. We follow the solution proposed by [19] to map ambiguous rotations to a canonical one. More specifically, the Map operator for an arbitrary rotation R \u2208 SO(3) is defined as:\nMap(R) = R\u015c, with\u015c = arg min S\u2208S(M i c ) RS \u2212 I 3 F ,(6)\nwhere the proper symmetry group S(M i c ) is the set of rotations which preserve the appearance of a given object M i c . The experimental datasets assume continuous symmetry and the axis of symmetry is the y-axis of the NOCS. Hence,\u015c takes the following form: (7) where R 11 , R 13 , R 31 , and R 33 are the elements of R. During training, we apply the Map operator to the rotation label: R gt \u2190 R gt\u015c to eliminate the rotation ambiguity of any symmetric object with the ground-truth pose (R gt , T gt ). In practice, our network is supervised by ground-truth NOCS coordinates P gt . Equivalently, we transform P gt by\u015c T : P gt \u2190\u015c T P gt . Regularization Losses. Row A i of the matrix A represents the distribution over the correspondences between i-th point of V and the points in M . A i can be understood as a relaxed one-hot vector, since each point of V usually can be well approximated by at most three points of M . We encourage A i to be a peaked distribution by minimizing the average cross entropy: L entropy = 1 Nv i j \u2212A i,j log A i,j . We also regularize D to discourage large deformations:\nS = \uf8ee \uf8f0 cos\u03b8 0 \u2212 sin\u03b8 0 1 0 sin\u03b8 0 cos\u03b8 \uf8f9 \uf8fb , with\u03b8 = arctan 2(R 13 \u2212 R 31 , R 11 + R 33 ),L def = 1 N C di\u2208D d i 2 .\nMinimal deformation preserves the semantic consistency between shape prior and the reconstructed model. For example, we want that the point belongs to the handle of the \"mug\" prior remains in the handle after deformation. This consistency loss is beneficial for the prediction of the correspondence matrix A.\n\nIn summary, the overall objective is a weighted sum of all four losses:\nL = \u03bb 1 L cd + \u03bb 2 L corr + \u03bb 3 L entropy + \u03bb 4 L def .(8)\n\nExperiments\n\n\nExperimental Setup\n\nDatasets. The CAMERA [33] dataset is generated by rendering and compositing synthetic objects into real scenes in a context-aware manner. In total, there are 300K composite images, where 25K are set aside for evaluation. The training set contains 1085 object instances selected from 6 different categories -bottle, bowl, camera, can, laptop and mug. The evaluation set contains 184 different instances. The REAL [33] dataset is complementary to the CAMERA. It captures 4300 real-world images of 7 scenes for training, and 2750 real-world images of 6 scenes for evaluation. Each set contains 18 real objects spanning the 6 categories. The two evaluation sets are referred to as CAMERA25 and REAL275. Evaluation Metric. Following [33], we independently evaluate the performance of 3D object detection and 6D pose estimation. We report the average precision at different Intersection-Over-Union (IoU) thresholds for object detection. For 6D pose evaluation, the average precision is computed at n \u2022 mcm. We ignore the rotational error around the axis of symmetry for symmetrical object categories (e.g. bottle, bowl, and can). Specially, we treat mug as symmetric object in the absence of the handle, and asymmetric object otherwise.\n\nBaseline. Wang et al. [33] is currently the only publicly available code and datasets for the 6D object pose and size estimation task. Futhermore, it is also the state-of-the-art performance on the task. Hence, we choose [33] as our baseline for comparison.\n\n\nImplementation Details\n\nWe collect all the instances in the CAMERA training dataset to train the autoencoder. Shape priors are learned from this collection and used in all experiments. Each prior consists of 1024 points. We use the Mask R-CNN implemented by matterport [1] for instance segmentation. For each detected instance, we resize the image patch to 192 \u00d7 192, and randomly sample 1024 points by repetition (if insufficient point count) or downsampling (if sufficient point count). To extract instance color features, we choose the PSPNet [43] with ResNet-18 [8] as backbone. We randomly select 5 point-pairs to generate a hypothesis for the RANSAC-based pose fitting. The maximum number of iteration is 128 and inlier threshold is set to 10% of the object diameter. For the hyperparameters of the total loss, we empirically find that \u03bb 1 = 5.0, \u03bb 2 = 1.0, \u03bb 3 = 1e \u2212 4, and \u03bb 4 = 0.01 are good choices.\n\n\nComparison to Baseline\n\nWe compare our approach to the Baseline [33] on CAMERA25 and REAL275. Quantitative results are summarized in Table 1. CAMERA25. In the setting of estimating 6D object pose and size from an RGB-D image, we achieve a mAP of 83.1% for 3D IoU at 0.75, and a mAP of 54.3% for 6D pose at 5 \u2022 2cm. Our results are 14% and 22% higher than the Baseline [33], respectively. We naively remove the depth input and related sub-networks in our network (i.e. RGB image as the only input) to make fair comparisons with the Baseline [33], which takes an RGB image as its input. As shown in Table 1, our results without depth input are still significantly better than the Baseline [33] (i.e. +15.5% and +17.9%). On one hand, this experiment shows the advantage of explicit handling of the intra-class shape variation, and the effectiveness of our method which reconstructs the object via deformation. On the other hand, it also shows that adding depth to the network does help to improve overall performance, although our improved performance does not rely on it solely. Given that depth image is required to uniquely determine the scale of the object, we recommend it in practical applications. The top row of Fig. 4 shows the average precision at different error thresholds for all 6 object categories. It provides independent analysis for 3D IoU, rotation, and translation error. REAL275. The REAL training set only contains 3 object instances per category, we enlarge this training set such that the network can generalize well to unseen objects. Following the Baseline [33], we randomly select data from CAM-ERA and REAL training set according to a ratio of 3 : 1. In fair comparison to the Baseline [33], our approach improves the mAP by 23.1% for 3D IoU at 0.75 and 12.1% for 6D pose at 5 \u2022 2cm. In strict comparison, we can still outperform by 16.4% and 8.5%, respectively. These results provide further evidence to support our approach. Fig. 4 (bottom) shows more detailed analysis of the errors.\n\n\nEvaluation of Shape Reconstruction\n\nTo evaluate the quality of the reconstruction, we compute the CD metric (c.f. Eq.\n\n2) of the reconstructed model from our method with the ground truth model in the NOCS. We get a CD metric of 1.97 on CAMERA25 and 3.17 on REAL275.\n\nIn comparison, the CD metrics are 3.70 and 4.41 on the respective dataset for the shape priors from our autoencoder. The better CD metrics of the reconstructed models compared to the shape priors show that the deformation estimation in our framework improves the quality of the 3D model reconstruction. shows the CD metric of our reconstructed models and the shape priors for each category.\n\n\nAblation Studies\n\nDifferent shape priors. We first evaluate how different shape priors influence the performance. All settings are kept the same in this experiment except for the choices of the priors. Results are summarized in Table 3 and 4. \"Embedding\" refers to the priors obtained from decoding the mean latent embeddings. We also try the instance whose latent embedding has the minimum L 2 distance to the mean latent embedding (denoted as \"NN\"). In addition, we explore random selection of one instance per category from the shape collection to compose our priors (denoted as \"Random\"). In general, our approach remains stable under different priors. Our network can adapt to different shape priors because the deformation is explicitly estimated. We achieve the best result for accurate pose (i.e. 5 \u2022 2cm) estimation when the learned categorical shape prior is used. Since our main target is to recover the 6D pose, we choose \"Embedding\" as our best model. To validate whether the priors are necessary, we use a point cloud uniformly sampled from a sphere of diameter one as our prior (denoted as \"None\"). The mAP decreases by 3.7% on real dataset when there are no priors, but the best result is achieved for object size estimation. Although shape priors are beneficial for estimating 6D pose, they sometimes bias shape reconstruction. Directly regress the NOCS Coordinates? As indicated by Eq. 4, our approach decouples the NOCS coordinates P to shape reconstruction M and dense correspondences A. However, both the network architecture and the training will be much simpler when we follow [33] to regress P directly (denoted as \"Regression\" in Table 3 and 4). For 6D pose estimation, the mAP of \"Regression\" at 5 \u2022 2cm is notably lower than \"Embedding\" on CAMERA25 (-3.1%) and REAL275 (-5.6%). This result further supports the benefit of handling shape variation via reconstruction over naive regression of the NOCS coordinates. \"Regression\" achieves slightly better mAP for object size estimation since it only finds the NOCS coordinates for the observed part, while \"Embedding\" needs to complete the unknown part of the object. Regularization losses. To validate the necessity of the two regularization losses, we train the network without regularizing deformation or correspondence, while keeping all the rest same as \"Embedding\". The mean average precisions of both variants are still comparable to \"Embedding\" on synthetic dataset. However, mAP of 6D pose at 5 \u2022 2cm drops noticeably (-5.9% and -3.6%) on the more difficult real dataset.\n\n\nQualitative Results.\n\nIn Fig. 5, we provide several qualitative results from both synthetic and real instances. The 6D pose and object size can be reliably recovered from noisy point correspondences using RANSAC-based pose fitting. Shape reconstruction can capture the variations between instances. The qualities of our predictions are generally better on synthetic data than real data, which is an indication that observation noise needs more attention in our future work. Out of the six categories, camera shows less accurate reconstruction due to its more complicated and varying geometry.\n\n\nConclusions\n\nWe present a novel approach for category-level 6D object pose estimation. Our network explicitly models intra-class shape variation by the estimation of the deformation from a shape prior to the object model. Shape priors are learned form a collection of object models and constructed in the latent space. Experiments on synthetic and real datasets demonstrate the advantage of our proposed approach.\n\n\nA Comparison to CASS\n\nCASS [4] is the latest work on category-level 6D object pose and size estimation. Similar to our work, they reconstruct the complete object model in the canonical space as a by-product. However, they train a variational autoencoder to generate the point cloud, while we estimate the deformation field of the corresponding shape prior. In addition, they directly regresses the pose and size by comparing pose-independent and pose-dependent features, while we recover the pose by establishing dense correspondences. As shown in Table 5, our approach significantly outperforms CASS in pose accuracy. This demonstrates the superiority of our correspondence-based approach over direct pose regression. B Comparison to 6-PACK 6-PACK [31] is the state-of-the-art category-level 6D pose tracker. Although our approach does not require pose initialization nor leverages on temporal consistency, we still achieve comparable accuracy on REAL275 at 5 \u2022 5cm (30.4% compared to 33.3%). More importantly, the accuracy of 6-PACK drops below 30% when the first 40 frames of a sequence (460 frames on average) are excluded from evaluation. This indicates that 6-PACK is highly dependent on pose initialization for higher accuracy. In contrast, the accuracy of our method remains stable since it is a pose estimation method.\n\n\nC Qualitative Results\n\nFig . 6 shows the per-frame pose detection results of our approach. The results are better on synthetic data (top two rows) than on real data (bottom two rows). This performance gap is mainly induced by the observation noise, which has greater influence on objects with complicated geometric shape (e.g. camera).\n\n\nD Runtime Analysis\n\nGiven RGB-D images with resolution of 640 \u00d7 480 and mean object count of 4, our implementation approximately runs at 4 FPS on a desktop with an Intel Fig. 6. We show some qualitative results of our approach (red) and their ground truths (green) on CAMERA25 (top two rows) and REAL275 (bottom two rows).\n\nCore i7-5960X CPU (3.0 GHz) and a NVIDIA GTX 1080Ti GPU. Specifically, it takes an average time of 130 ms for instance segmentation, 100 ms for network inference, and 20 ms for pose alignment.\n\n\nE Visualization of Shape Priors\n\nIn Fig. 7, we visualize the different categorical shape priors used in our ablation studies.\n\n\nF Derivation of the Map Operator\n\nWe first give the proposition from [19], and then derive the Map operator used in our work as a corollary. Given an object M i c , the proper symmetry group S(M i c ) is defined as: \n\nwhere I 3 is an 3\u00d73 identity matrix. Then, Map(R 1 ) = Map(R 2 ) \u21d0\u21d2 I(M i c , R 1 ) = I(M i c , R 2 ) .\n\nThe proof is omitted for brevity, refer to [19] for the details. The Map operator used in our work can then be derived directly from Proposition 1. Corollary 1. The Map operator for symmetrical objects around the y-axis is given by:\nMap(R) = R\u015c, \u2200 R \u2208 SO(3),(11)\nwher\u00ea S = \uf8ee \uf8f0 cos\u03b8 0 \u2212 sin\u03b8 0 1 0 sin\u03b8 0 cos\u03b8 \uf8f9 \uf8fb , with\u03b8 = arctan 2(R 13 \u2212 R 31 , R 11 + R 33 ). (12) Proof. Assuming the object M i c is symmetrical around the y-axis of the object coordinate system, then S has the following form:\nS = \uf8ee \uf8f0 cos \u03b8 0 \u2212 sin \u03b8 0 1 0 sin \u03b8 0 cos \u03b8 \uf8f9 \uf8fb .(13)\nThe Froebenius norm can be rewritten as: \n\n\nRS\n\nWe minimize the Froebenius norm over \u03b8 to solve for the Map:  \n\n\u00d7\nMatrix multiplication + Matrix addition S Select and back-project foreground pixels C Crop an image patch\n\nFig. 1 .\n1Overview of our approach.\n\nFig. 2 .\n2(a) Architecture of the autoencoder. (b) The latent embeddings of all instances are mapped to R 2 with T-SNE for visualization. These instances are from 6 categoriesbottle, bowl, camera, can, laptop and mug. (c) Shape priors are reconstructed by passing mean latent embedding of each category through the decoder.\n\nFig. 3 .\n3Our Network Architecture. The upper-left and lower-left branches extract point and global features from the instance and the shape prior respectively. The upperright branch estimates the correspondence matrix, and the lower-right branch predicts the deformation field. The exchange of global features is the key part of our network. of foreground pixels with a valid depth value. The corresponding shape prior is M c \u2208 R Nc\u00d73 , where N c is the number of points in M c . Our network takes V , I and M c as inputs, and outputs the per-point deformation field D \u2208 R Nc\u00d73 and a correspondence matrix A \u2208 R Nv\u00d7Nc . The final reconstructed model is M = M c +D. Each row of A sums to 1 since it represents the soft correspondences between a point in V and all points in M . As shown in Fig. 3, our network is composed of four parts: (1) extracts features from the object instance; (2) extracts features from the shape prior; (3) regresses the deformation field D; and (4) estimates the correspondence matrix A.\n\n\nReconstruction Loss. We assume that ground-truth model M gt is available during training. The deformation field D is supervised indirectly by minimizing the Chamfer distance (c.f. Eq. 2) between M and M gt , i.e. L cd = d CD (M, M gt ) = d CD (M c + D, M gt ).\n\nFig. 5 .\n5Examples of qualitative results from CAMERA25 (top rows) and REAL275 (bottom rows). For each example, we visualize the results of pose estimation and the reconstructed model M . Our estimations are shown in red, while the ground truths are shown in green.\n\nFig. 7 .\n7= {s \u2208 SO(3) | \u2200 p \u2208 SO(3), I(M i c , p) = I(M i c , s \u00b7 p)},(9)where I(M i c , p) is the image of object M i c under pose p. Intuitively, S(M i c ) consists of rotations which preserve the appearance of a given object. Categorical shape priors used in ablation studies.Proposition 1. Given a proper symmetry group S(M i c ), \u2200 R \u2208 SO(3), the Map operator is defined as: Map(R) = R\u015c, with\u015c = arg min S\u2208S(M i c ) RS \u2212 I 3 F ,\n\nF\n= 6 \u2212 2Trace(RS) = 6 \u2212 2[R 11 cos \u03b8 + R 13 sin \u03b8 + R 22 + R 33 cos \u03b8 \u2212 R 31 sin \u03b8].\n\n\n11 + R 33 ) cos \u03b8 + (R 13 \u2212 R 31 ) sin \u03b8 = arctan 2(R 13 \u2212 R 31 , R 11 + R 33 ) .\n\n\nwith\u03b8 = arctan 2(R 13 \u2212 R 31 , R 11 + R 33 ).\n\n\n. The reconstruction error is128 \nInput \nnx3 \nnx128 \n1024 \nFC \n\nFC \n\nFC \nFC \n\nOutput \nnx3 \n\nShared MLP \n\n(64, 128) \n(256, 1024) \n\nShared MLP \nnx256 \n\nMaxPool \n\nEncoder \n\nDecoder \n\n512 \n\nLatent \nEmbedding \n\nMaxPool \n\n(a) Autoencoder \n\n(b) Latent Embeddings \n(c) Shape Priors \n\n\n\nTable 1 .\n1Comparisons on CAMERA25 and REAL275. We report the mAP w.r.t. different thresholds on 3D IoU, and rotation and translation errors.Data \nMethod \nmAP \n3D50 3D75 5 \u2022 2cm 5 \u2022 5cm 10 \u2022 2cm 10 \u2022 5cm \n\nCAMERA25 \n\nBaseline [33] \n83.9 69.5 \n32.3 \n40.9 \n48.2 \n64.6 \nOurs (RGB) \n93.1 84.6 \n50.2 \n54.5 \n70.4 \n78.6 \nOurs (RGB-D) 93.2 83.1 \n54.3 \n59.0 \n73.3 \n81.5 \n\nREAL275 \n\nBaseline [33] \n78.0 30.1 \n7.2 \n10.0 \n13.8 \n25.2 \nOurs (RGB) \n75.2 46.5 \n15.7 \n18.8 \n33.7 \n47.4 \nOurs (RGB-D) 77.3 53.2 \n19.3 \n21.4 \n43.2 \n54.1 \n\nTable 2. Evaluation of shape reconstruction with CD metric (\u00d710 \u22123 ). \n\nData \nModel \nBottle Bowl Camera Can Laptop Mug Average \n\nCAMERA25 \nReconstruction 1.81 1.63 \n4.02 0.97 1.98 1.42 \n1.97 \nShape Prior \n3.41 2.20 \n9.01 2.21 3.27 2.10 \n3.70 \n\nREAL275 \nReconstruction 3.44 1.21 \n8.89 1.56 2.91 1.02 \n3.17 \nShape Prior \n4.99 1.16 \n9.85 2.38 7.14 0.97 \n4.41 \n\n\n\nTable 2\n2Fig. 4. Average precision vs. error thresholds on CAMERA25 (top row) and REAL275 (bottom row).\n\nTable 3 .\n3Ablation studies on CAMERA25. Refer to text for more details.Network \nmAP \n3D50 3D75 5 \u2022 2cm 5 \u2022 5cm 10 \u2022 2cm 10 \u2022 5cm \n\nShape Priors \n\nEmbedding \n93.2 83.1 \n54.3 \n59.0 \n73.3 \n81.5 \nNN \n93.3 85.7 \n52.7 \n57.3 \n72.9 \n81.3 \nRandom \n93.3 85.7 \n53.4 \n58.0 \n72.8 \n81.0 \nNone \n93.3 85.8 \n54.0 \n58.8 \n73.1 \n81.6 \n\nNOCS Coords \nRegression \n93.3 85.3 \n51.2 \n55.6 \n73.8 \n82.1 \n\nRegularization \nw/o Def. \n93.2 85.1 \n53.9 \n58.7 \n73.1 \n81.4 \nw/o Entropy 93.2 85.1 \n53.2 \n57.9 \n73.2 \n81.8 \n\nTable 4. Ablation studies on REAL275. Refer to text for more details. \n\nNetwork \nmAP \n3D50 3D75 5 \u2022 2cm 5 \u2022 5cm 10 \u2022 2cm 10 \u2022 5cm \n\nShape Priors \n\nEmbedding \n77.3 53.2 \n19.3 \n21.4 \n43.2 \n54.1 \nNN \n75.9 52.6 \n17.0 \n19.0 \n42.0 \n51.6 \nRandom \n75.8 52.2 \n17.9 \n20.1 \n42.3 \n51.3 \nNone \n77.2 55.5 \n15.6 \n19.8 \n38.4 \n53.6 \n\nNOCS Coords \nRegression \n78.7 54.9 \n13.7 \n14.9 \n42.5 \n51.4 \n\nRegularization \nw/o Def. \n77.1 50.2 \n13.4 \n15.4 \n37.3 \n49.8 \nw/o Entropy 77.3 53.3 \n15.7 \n18.8 \n38.5 \n51.3 \n\n\n\nTable 5 .\n5Quantitative comparison with CASS on REAL275. mAP 3D25 3D50 3D75 5 \u2022 2cm 5 \u2022 5cm 10 \u2022 2cm 10 \u2022 5cmMethod \nBaseline [33] 84.8 78.0 30.1 \n7.2 \n10.0 \n13.8 \n25.2 \nCASS [4] 84.2 77.7 \n\u2212 \n\u2212 \n13.0 \n\u2212 \n37.6 \nOurs 83.4 77.3 53.2 19.3 \n21.4 \n43.2 \n54.1 \n\n\nAcknowledgements. This research is supported in parts by the Singapore MOE Tier 1 grant R-252-000-A65-114, and the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (Project #A18A2b0046).\nMask r-cnn for object detection and instance segmentation on keras and tensorflow. W Abdulla, Abdulla, W.: Mask r-cnn for object detection and instance segmentation on keras and tensorflow. https://github.com/matterport/Mask_RCNN (2017)\n\nLearning 6d object pose estimation using 3d object coordinates. E Brachmann, A Krull, F Michel, S Gumhold, J Shotton, C Rother, European conference on computer vision. SpringerBrachmann, E., Krull, A., Michel, F., Gumhold, S., Shotton, J., Rother, C.: Learn- ing 6d object pose estimation using 3d object coordinates. In: European conference on computer vision. pp. 536-551. Springer (2014)\n\nProbabilistic category-level pose estimation via segmentation and predicted-shape priors. B Burchfiel, G Konidaris, arXiv:1905.12079arXiv preprintBurchfiel, B., Konidaris, G.: Probabilistic category-level pose estimation via seg- mentation and predicted-shape priors. arXiv preprint arXiv:1905.12079 (2019)\n\nLearning canonical shape space for categorylevel 6d object pose and size estimation. D Chen, J Li, Z Wang, K Xu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Chen, D., Li, J., Wang, Z., Xu, K.: Learning canonical shape space for category- level 6d object pose and size estimation. In: Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR) (June 2020)\n\nRandom sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. M A Fischler, R C Bolles, Communications of the ACM. 246Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communi- cations of the ACM 24(6), 381-395 (1981)\n\nAligning 3d models to rgb-d images of cluttered scenes. S Gupta, P Arbel\u00e1ez, R Girshick, J Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionGupta, S., Arbel\u00e1ez, P., Girshick, R., Malik, J.: Aligning 3d models to rgb-d images of cluttered scenes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4731-4740 (2015)\n\nMask r-cnn. K He, G Gkioxari, P Doll\u00e1r, R Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionHe, K., Gkioxari, G., Doll\u00e1r, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 2961-2969 (2017)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n\nModel based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. S Hinterstoisser, V Lepetit, S Ilic, S Holzer, G Bradski, K Konolige, N Navab, Asian conference on computer vision. SpringerHinterstoisser, S., Lepetit, V., Ilic, S., Holzer, S., Bradski, G., Konolige, K., Navab, N.: Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. In: Asian conference on computer vision. pp. 548-562. Springer (2012)\n\nSsd-6d: Making rgbbased 3d detection and 6d pose estimation great again. W Kehl, F Manhardt, F Tombari, S Ilic, N Navab, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionKehl, W., Manhardt, F., Tombari, F., Ilic, S., Navab, N.: Ssd-6d: Making rgb- based 3d detection and 6d pose estimation great again. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1521-1529 (2017)\n\nDeep learning of local rgbd patches for 3d object detection and 6d pose estimation. W Kehl, F Milletari, F Tombari, S Ilic, N Navab, European Conference on Computer Vision. SpringerKehl, W., Milletari, F., Tombari, F., Ilic, S., Navab, N.: Deep learning of local rgb- d patches for 3d object detection and 6d pose estimation. In: European Conference on Computer Vision. pp. 205-220. Springer (2016)\n\nLearning analysis-by-synthesis for 6d pose estimation in rgb-d images. A Krull, E Brachmann, F Michel, Ying Yang, M Gumhold, S Rother, C , Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionKrull, A., Brachmann, E., Michel, F., Ying Yang, M., Gumhold, S., Rother, C.: Learning analysis-by-synthesis for 6d pose estimation in rgb-d images. In: Pro- ceedings of the IEEE International Conference on Computer Vision. pp. 954-962 (2015)\n\nDeformnet: Free-form deformation network for 3d shape reconstruction from a single image. A Kurenkov, J Ji, A Garg, V Mehta, J Gwak, C Choy, S Savarese, 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEEKurenkov, A., Ji, J., Garg, A., Mehta, V., Gwak, J., Choy, C., Savarese, S.: De- formnet: Free-form deformation network for 3d shape reconstruction from a single image. In: 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 858-866. IEEE (2018)\n\n2d-driven 3d object detection in rgb-d images. J Lahoud, B Ghanem, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLahoud, J., Ghanem, B.: 2d-driven 3d object detection in rgb-d images. In: Pro- ceedings of the IEEE International Conference on Computer Vision. pp. 4622-4630 (2017)\n\nA unified framework for multi-view multi-class object pose estimation. C Li, J Bai, G D Hager, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Li, C., Bai, J., Hager, G.D.: A unified framework for multi-view multi-class object pose estimation. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 254-269 (2018)\n\nVisualizing data using t-sne. L V D Maaten, G Hinton, Journal of machine learning research. 9Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne. Journal of machine learn- ing research 9(Nov), 2579-2605 (2008)\n\nGlobal hypothesis generation for 6d object pose estimation. F Michel, A Kirillov, E Brachmann, A Krull, S Gumhold, B Savchynskyy, C Rother, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMichel, F., Kirillov, A., Brachmann, E., Krull, A., Gumhold, S., Savchynskyy, B., Rother, C.: Global hypothesis generation for 6d object pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 462-471 (2017)\n\nPvnet: Pixel-wise voting network for 6dof pose estimation. S Peng, Y Liu, Q Huang, X Zhou, H Bao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPeng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561-4570 (2019)\n\nOn object symmetries and 6d pose estimation from images. G Pitteri, M Ramamonjisoa, S Ilic, V Lepetit, 2019 International Conference on 3D Vision (3DV). IEEEPitteri, G., Ramamonjisoa, M., Ilic, S., Lepetit, V.: On object symmetries and 6d pose estimation from images. In: 2019 International Conference on 3D Vision (3DV). pp. 614-622. IEEE (2019)\n\nIm-age2mesh: A learning framework for single image 3d reconstruction. J K Pontes, C Kong, S Sridharan, S Lucey, A Eriksson, C Fookes, Asian Conference on Computer Vision. SpringerPontes, J.K., Kong, C., Sridharan, S., Lucey, S., Eriksson, A., Fookes, C.: Im- age2mesh: A learning framework for single image 3d reconstruction. In: Asian Conference on Computer Vision. pp. 365-381. Springer (2018)\n\nFrustum pointnets for 3d object detection from rgb-d data. C R Qi, W Liu, C Wu, H Su, L J Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionQi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J.: Frustum pointnets for 3d object detection from rgb-d data. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 918-927 (2018)\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. C R Qi, H Su, K Mo, L J Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionQi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 652-660 (2017)\n\nBb8: a scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. M Rad, V Lepetit, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionRad, M., Lepetit, V.: Bb8: a scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. In: Pro- ceedings of the IEEE International Conference on Computer Vision. pp. 3828-3836 (2017)\n\nCategory-level 6d object pose recovery in depth images. C Sahin, T K Kim, Proceedings of the European Conference on Computer Vision (ECCV) Workshops. the European Conference on Computer Vision (ECCV) WorkshopsSahin, C., Kim, T.K.: Category-level 6d object pose recovery in depth images. In: Proceedings of the European Conference on Computer Vision (ECCV) Workshops (September 2018)\n\nDeep sliding shapes for amodal 3d object detection in rgb-d images. S Song, J Xiao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSong, S., Xiao, J.: Deep sliding shapes for amodal 3d object detection in rgb-d images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 808-816 (2016)\n\nImplicit 3d orientation learning for 6d object detection from rgb images. M Sundermeyer, Z C Marton, M Durner, M Brucker, R Triebel, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Sundermeyer, M., Marton, Z.C., Durner, M., Brucker, M., Triebel, R.: Implicit 3d orientation learning for 6d object detection from rgb images. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 699-715 (2018)\n\nLatent-class hough forests for 3d object detection and pose estimation. A Tejani, D Tang, R Kouskouridas, T K Kim, European Conference on Computer Vision. SpringerTejani, A., Tang, D., Kouskouridas, R., Kim, T.K.: Latent-class hough forests for 3d object detection and pose estimation. In: European Conference on Computer Vision. pp. 462-477. Springer (2014)\n\nReal-time seamless single shot 6d object pose prediction. B Tekin, S N Sinha, P Fua, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTekin, B., Sinha, S.N., Fua, P.: Real-time seamless single shot 6d object pose pre- diction. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 292-301 (2018)\n\nLeast-squares estimation of transformation parameters between two point patterns. S Umeyama, IEEE Transactions on Pattern Analysis & Machine Intelligence. 4Umeyama, S.: Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence (4), 376-380 (1991)\n\nFew-shot generalization for single-image 3d reconstruction via priors. B Wallace, B Hariharan, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionWallace, B., Hariharan, B.: Few-shot generalization for single-image 3d reconstruc- tion via priors. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 3818-3827 (2019)\n\n6-pack: Category-level 6d pose tracker with anchor-based keypoints. C Wang, R Mart\u00edn-Mart\u00edn, D Xu, J Lv, C Lu, L Fei-Fei, S Savarese, Y Zhu, International Conference on Robotics and Automation (ICRA). Wang, C., Mart\u00edn-Mart\u00edn, R., Xu, D., Lv, J., Lu, C., Fei-Fei, L., Savarese, S., Zhu, Y.: 6-pack: Category-level 6d pose tracker with anchor-based keypoints. In: International Conference on Robotics and Automation (ICRA) (2020)\n\nDensefusion: 6d object pose estimation by iterative dense fusion. C Wang, D Xu, Y Zhu, R Mart\u00edn-Mart\u00edn, C Lu, L Fei-Fei, S Savarese, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWang, C., Xu, D., Zhu, Y., Mart\u00edn-Mart\u00edn, R., Lu, C., Fei-Fei, L., Savarese, S.: Densefusion: 6d object pose estimation by iterative dense fusion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3343-3352 (2019)\n\nNormalized object coordinate space for category-level 6d object pose and size estimation. H Wang, S Sridhar, J Huang, J Valentin, S Song, L J Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWang, H., Sridhar, S., Huang, J., Valentin, J., Song, S., Guibas, L.J.: Normalized object coordinate space for category-level 6d object pose and size estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2642-2651 (2019)\n\nPixel2mesh: Generating 3d mesh models from single rgb images. N Wang, Y Zhang, Z Li, Y Fu, W Liu, Y G Jiang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., Jiang, Y.G.: Pixel2mesh: Gener- ating 3d mesh models from single rgb images. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 52-67 (2018)\n\n3dn: 3d deformation network. W Wang, D Ceylan, R Mech, U Neumann, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWang, W., Ceylan, D., Mech, R., Neumann, U.: 3dn: 3d deformation network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1038-1046 (2019)\n\nPixel2mesh++: Multi-view 3d mesh generation via deformation. C Wen, Y Zhang, Z Li, Y Fu, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionWen, C., Zhang, Y., Li, Z., Fu, Y.: Pixel2mesh++: Multi-view 3d mesh gener- ation via deformation. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1042-1051 (2019)\n\nPosecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Y Xiang, T Schmidt, V Narayanan, D Fox, Robotics: Science and Systems (RSS). Xiang, Y., Schmidt, T., Narayanan, V., Fox, D.: Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Robotics: Science and Systems (RSS) (2018)\n\nPixor: Real-time 3d object detection from point clouds. B Yang, W Luo, R Urtasun, Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. the IEEE conference on Computer Vision and Pattern RecognitionYang, B., Luo, W., Urtasun, R.: Pixor: Real-time 3d object detection from point clouds. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. pp. 7652-7660 (2018)\n\nFoldingnet: Point cloud auto-encoder via deep grid deformation. Y Yang, C Feng, Y Shen, D Tian, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYang, Y., Feng, C., Shen, Y., Tian, D.: Foldingnet: Point cloud auto-encoder via deep grid deformation. In: Proceedings of the IEEE Conference on Computer Vi- sion and Pattern Recognition. pp. 206-215 (2018)\n\nPcn: Point completion network. W Yuan, T Khot, D Held, C Mertz, M Hebert, 2018 International Conference on 3D Vision (3DV). IEEEYuan, W., Khot, T., Held, D., Mertz, C., Hebert, M.: Pcn: Point completion net- work. In: 2018 International Conference on 3D Vision (3DV). pp. 728-737. IEEE (2018)\n\nLearning semantic deformation flows with 3d convolutional networks. M E Yumer, N J Mitra, European Conference on Computer Vision. SpringerYumer, M.E., Mitra, N.J.: Learning semantic deformation flows with 3d convo- lutional networks. In: European Conference on Computer Vision. pp. 294-311. Springer (2016)\n\nDpod: 6d pose object detector and refiner. S Zakharov, I Shugurov, S Ilic, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZakharov, S., Shugurov, I., Ilic, S.: Dpod: 6d pose object detector and refiner. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1941-1950 (2019)\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2881-2890 (2017)\n\nVoxelnet: End-to-end learning for point cloud based 3d object detection. Y Zhou, O Tuzel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZhou, Y., Tuzel, O.: Voxelnet: End-to-end learning for point cloud based 3d object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4490-4499 (2018)\n", "annotations": {"author": "[{\"end\":122,\"start\":78}]", "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": "[{\"end\":121,\"start\":79}]", "title": "[{\"end\":75,\"start\":1},{\"end\":197,\"start\":123}]", "venue": null, "abstract": "[{\"end\":1275,\"start\":283}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1643,\"start\":1639},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1646,\"start\":1643},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":1649,\"start\":1646},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2112,\"start\":2108},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2499,\"start\":2497},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2513,\"start\":2509},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4504,\"start\":4500},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5696,\"start\":5693},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5764,\"start\":5760},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5866,\"start\":5862},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5869,\"start\":5866},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5998,\"start\":5994},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6001,\"start\":5998},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6142,\"start\":6138},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6145,\"start\":6142},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6276,\"start\":6273},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6279,\"start\":6276},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6282,\"start\":6279},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6361,\"start\":6357},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6364,\"start\":6361},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6367,\"start\":6364},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6462,\"start\":6458},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6849,\"start\":6845},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6930,\"start\":6927},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6933,\"start\":6930},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6936,\"start\":6933},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7017,\"start\":7013},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7020,\"start\":7017},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7659,\"start\":7656},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7848,\"start\":7844},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7981,\"start\":7977},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8290,\"start\":8287},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8519,\"start\":8515},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8754,\"start\":8750},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8757,\"start\":8754},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8760,\"start\":8757},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8870,\"start\":8866},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8873,\"start\":8870},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9341,\"start\":9337},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10457,\"start\":10453},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10726,\"start\":10722},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11226,\"start\":11223},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13082,\"start\":13078},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13125,\"start\":13122},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13956,\"start\":13952},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14545,\"start\":14541},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14682,\"start\":14678},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15261,\"start\":15257},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15394,\"start\":15390},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17618,\"start\":17614},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17662,\"start\":17659},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18849,\"start\":18845},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20766,\"start\":20762},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21157,\"start\":21153},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21473,\"start\":21469},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21999,\"start\":21995},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22198,\"start\":22194},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22505,\"start\":22502},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22783,\"start\":22779},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22802,\"start\":22799},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23214,\"start\":23210},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23518,\"start\":23514},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23690,\"start\":23686},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23837,\"start\":23833},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24730,\"start\":24726},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24861,\"start\":24857},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27424,\"start\":27420},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29417,\"start\":29414},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30140,\"start\":30136},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31775,\"start\":31771},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32072,\"start\":32068},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32390,\"start\":32386}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32794,\"start\":32686},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32831,\"start\":32795},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33156,\"start\":32832},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34172,\"start\":33157},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34435,\"start\":34173},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34702,\"start\":34436},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35138,\"start\":34703},{\"attributes\":{\"id\":\"fig_7\"},\"end\":35225,\"start\":35139},{\"attributes\":{\"id\":\"fig_8\"},\"end\":35309,\"start\":35226},{\"attributes\":{\"id\":\"fig_9\"},\"end\":35357,\"start\":35310},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35636,\"start\":35358},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36514,\"start\":35637},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36619,\"start\":36515},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37595,\"start\":36620},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37853,\"start\":37596}]", "paragraph": "[{\"end\":2076,\"start\":1291},{\"end\":3135,\"start\":2078},{\"end\":4469,\"start\":3137},{\"end\":4928,\"start\":4471},{\"end\":5419,\"start\":4930},{\"end\":6718,\"start\":5436},{\"end\":7534,\"start\":6720},{\"end\":8647,\"start\":7536},{\"end\":9310,\"start\":8649},{\"end\":9514,\"start\":9312},{\"end\":10695,\"start\":9529},{\"end\":12115,\"start\":10697},{\"end\":13244,\"start\":12143},{\"end\":13849,\"start\":13246},{\"end\":14053,\"start\":13891},{\"end\":14832,\"start\":14144},{\"end\":14938,\"start\":14871},{\"end\":16260,\"start\":14967},{\"end\":17261,\"start\":16262},{\"end\":17685,\"start\":17320},{\"end\":18256,\"start\":17704},{\"end\":18983,\"start\":18371},{\"end\":20146,\"start\":19041},{\"end\":20573,\"start\":20265},{\"end\":20646,\"start\":20575},{\"end\":21971,\"start\":20741},{\"end\":22230,\"start\":21973},{\"end\":23143,\"start\":22257},{\"end\":25157,\"start\":23170},{\"end\":25277,\"start\":25196},{\"end\":25425,\"start\":25279},{\"end\":25817,\"start\":25427},{\"end\":28373,\"start\":25838},{\"end\":28968,\"start\":28398},{\"end\":29384,\"start\":28984},{\"end\":30714,\"start\":29409},{\"end\":31052,\"start\":30740},{\"end\":31377,\"start\":31075},{\"end\":31571,\"start\":31379},{\"end\":31699,\"start\":31607},{\"end\":31918,\"start\":31736},{\"end\":32023,\"start\":31920},{\"end\":32257,\"start\":32025},{\"end\":32520,\"start\":32288},{\"end\":32616,\"start\":32575},{\"end\":32685,\"start\":32623}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13890,\"start\":13850},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14143,\"start\":14054},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14870,\"start\":14833},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17298,\"start\":17262},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18370,\"start\":18257},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19040,\"start\":18984},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20238,\"start\":20147},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20264,\"start\":20238},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20705,\"start\":20647},{\"attributes\":{\"id\":\"formula_10\"},\"end\":32287,\"start\":32258},{\"attributes\":{\"id\":\"formula_11\"},\"end\":32574,\"start\":32521}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23286,\"start\":23279},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23750,\"start\":23743},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26055,\"start\":26048},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27482,\"start\":27475},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29942,\"start\":29935}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1289,\"start\":1277},{\"attributes\":{\"n\":\"2\"},\"end\":5434,\"start\":5422},{\"attributes\":{\"n\":\"3\"},\"end\":9527,\"start\":9517},{\"attributes\":{\"n\":\"3.1\"},\"end\":12141,\"start\":12118},{\"attributes\":{\"n\":\"3.2\"},\"end\":14965,\"start\":14941},{\"attributes\":{\"n\":\"3.3\"},\"end\":17318,\"start\":17300},{\"attributes\":{\"n\":\"3.4\"},\"end\":17702,\"start\":17688},{\"attributes\":{\"n\":\"4\"},\"end\":20718,\"start\":20707},{\"attributes\":{\"n\":\"4.1\"},\"end\":20739,\"start\":20721},{\"attributes\":{\"n\":\"4.2\"},\"end\":22255,\"start\":22233},{\"attributes\":{\"n\":\"4.3\"},\"end\":23168,\"start\":23146},{\"attributes\":{\"n\":\"4.4\"},\"end\":25194,\"start\":25160},{\"attributes\":{\"n\":\"4.5\"},\"end\":25836,\"start\":25820},{\"attributes\":{\"n\":\"4.6\"},\"end\":28396,\"start\":28376},{\"attributes\":{\"n\":\"5\"},\"end\":28982,\"start\":28971},{\"end\":29407,\"start\":29387},{\"end\":30738,\"start\":30717},{\"end\":31073,\"start\":31055},{\"end\":31605,\"start\":31574},{\"end\":31734,\"start\":31702},{\"end\":32621,\"start\":32619},{\"end\":32688,\"start\":32687},{\"end\":32804,\"start\":32796},{\"end\":32841,\"start\":32833},{\"end\":33166,\"start\":33158},{\"end\":34445,\"start\":34437},{\"end\":34712,\"start\":34704},{\"end\":35141,\"start\":35140},{\"end\":35647,\"start\":35638},{\"end\":36523,\"start\":36516},{\"end\":36630,\"start\":36621},{\"end\":37606,\"start\":37597}]", "table": "[{\"end\":35636,\"start\":35389},{\"end\":36514,\"start\":35779},{\"end\":37595,\"start\":36693},{\"end\":37853,\"start\":37706}]", "figure_caption": "[{\"end\":32794,\"start\":32689},{\"end\":32831,\"start\":32806},{\"end\":33156,\"start\":32843},{\"end\":34172,\"start\":33168},{\"end\":34435,\"start\":34175},{\"end\":34702,\"start\":34447},{\"end\":35138,\"start\":34714},{\"end\":35225,\"start\":35142},{\"end\":35309,\"start\":35228},{\"end\":35357,\"start\":35312},{\"end\":35389,\"start\":35360},{\"end\":35779,\"start\":35649},{\"end\":36619,\"start\":36525},{\"end\":36693,\"start\":36632},{\"end\":37706,\"start\":37608}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":9278,\"start\":9272},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11072,\"start\":11066},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14018,\"start\":14011},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14475,\"start\":14468},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14937,\"start\":14930},{\"end\":24369,\"start\":24363},{\"end\":25104,\"start\":25098},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28407,\"start\":28401},{\"end\":30747,\"start\":30744},{\"end\":31231,\"start\":31225},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31616,\"start\":31610}]", "bib_author_first_name": "[{\"end\":38171,\"start\":38170},{\"end\":38390,\"start\":38389},{\"end\":38403,\"start\":38402},{\"end\":38412,\"start\":38411},{\"end\":38422,\"start\":38421},{\"end\":38433,\"start\":38432},{\"end\":38444,\"start\":38443},{\"end\":38808,\"start\":38807},{\"end\":38821,\"start\":38820},{\"end\":39111,\"start\":39110},{\"end\":39119,\"start\":39118},{\"end\":39125,\"start\":39124},{\"end\":39133,\"start\":39132},{\"end\":39650,\"start\":39649},{\"end\":39652,\"start\":39651},{\"end\":39664,\"start\":39663},{\"end\":39666,\"start\":39665},{\"end\":39960,\"start\":39959},{\"end\":39969,\"start\":39968},{\"end\":39981,\"start\":39980},{\"end\":39993,\"start\":39992},{\"end\":40366,\"start\":40365},{\"end\":40372,\"start\":40371},{\"end\":40384,\"start\":40383},{\"end\":40394,\"start\":40393},{\"end\":40728,\"start\":40727},{\"end\":40734,\"start\":40733},{\"end\":40743,\"start\":40742},{\"end\":40750,\"start\":40749},{\"end\":41192,\"start\":41191},{\"end\":41210,\"start\":41209},{\"end\":41221,\"start\":41220},{\"end\":41229,\"start\":41228},{\"end\":41239,\"start\":41238},{\"end\":41250,\"start\":41249},{\"end\":41262,\"start\":41261},{\"end\":41661,\"start\":41660},{\"end\":41669,\"start\":41668},{\"end\":41681,\"start\":41680},{\"end\":41692,\"start\":41691},{\"end\":41700,\"start\":41699},{\"end\":42142,\"start\":42141},{\"end\":42150,\"start\":42149},{\"end\":42163,\"start\":42162},{\"end\":42174,\"start\":42173},{\"end\":42182,\"start\":42181},{\"end\":42529,\"start\":42528},{\"end\":42538,\"start\":42537},{\"end\":42551,\"start\":42550},{\"end\":42564,\"start\":42560},{\"end\":42572,\"start\":42571},{\"end\":42583,\"start\":42582},{\"end\":42593,\"start\":42592},{\"end\":43052,\"start\":43051},{\"end\":43064,\"start\":43063},{\"end\":43070,\"start\":43069},{\"end\":43078,\"start\":43077},{\"end\":43087,\"start\":43086},{\"end\":43095,\"start\":43094},{\"end\":43103,\"start\":43102},{\"end\":43507,\"start\":43506},{\"end\":43517,\"start\":43516},{\"end\":43887,\"start\":43886},{\"end\":43893,\"start\":43892},{\"end\":43900,\"start\":43899},{\"end\":43902,\"start\":43901},{\"end\":44247,\"start\":44246},{\"end\":44251,\"start\":44248},{\"end\":44261,\"start\":44260},{\"end\":44493,\"start\":44492},{\"end\":44503,\"start\":44502},{\"end\":44515,\"start\":44514},{\"end\":44528,\"start\":44527},{\"end\":44537,\"start\":44536},{\"end\":44548,\"start\":44547},{\"end\":44563,\"start\":44562},{\"end\":45030,\"start\":45029},{\"end\":45038,\"start\":45037},{\"end\":45045,\"start\":45044},{\"end\":45054,\"start\":45053},{\"end\":45062,\"start\":45061},{\"end\":45480,\"start\":45479},{\"end\":45491,\"start\":45490},{\"end\":45507,\"start\":45506},{\"end\":45515,\"start\":45514},{\"end\":45841,\"start\":45840},{\"end\":45843,\"start\":45842},{\"end\":45853,\"start\":45852},{\"end\":45861,\"start\":45860},{\"end\":45874,\"start\":45873},{\"end\":45883,\"start\":45882},{\"end\":45895,\"start\":45894},{\"end\":46227,\"start\":46226},{\"end\":46229,\"start\":46228},{\"end\":46235,\"start\":46234},{\"end\":46242,\"start\":46241},{\"end\":46248,\"start\":46247},{\"end\":46254,\"start\":46253},{\"end\":46256,\"start\":46255},{\"end\":46696,\"start\":46695},{\"end\":46698,\"start\":46697},{\"end\":46704,\"start\":46703},{\"end\":46710,\"start\":46709},{\"end\":46716,\"start\":46715},{\"end\":46718,\"start\":46717},{\"end\":47224,\"start\":47223},{\"end\":47231,\"start\":47230},{\"end\":47672,\"start\":47671},{\"end\":47681,\"start\":47680},{\"end\":47683,\"start\":47682},{\"end\":48068,\"start\":48067},{\"end\":48076,\"start\":48075},{\"end\":48490,\"start\":48489},{\"end\":48505,\"start\":48504},{\"end\":48507,\"start\":48506},{\"end\":48517,\"start\":48516},{\"end\":48527,\"start\":48526},{\"end\":48538,\"start\":48537},{\"end\":48969,\"start\":48968},{\"end\":48979,\"start\":48978},{\"end\":48987,\"start\":48986},{\"end\":49003,\"start\":49002},{\"end\":49005,\"start\":49004},{\"end\":49315,\"start\":49314},{\"end\":49324,\"start\":49323},{\"end\":49326,\"start\":49325},{\"end\":49335,\"start\":49334},{\"end\":49761,\"start\":49760},{\"end\":50083,\"start\":50082},{\"end\":50094,\"start\":50093},{\"end\":50492,\"start\":50491},{\"end\":50500,\"start\":50499},{\"end\":50517,\"start\":50516},{\"end\":50523,\"start\":50522},{\"end\":50529,\"start\":50528},{\"end\":50535,\"start\":50534},{\"end\":50546,\"start\":50545},{\"end\":50558,\"start\":50557},{\"end\":50919,\"start\":50918},{\"end\":50927,\"start\":50926},{\"end\":50933,\"start\":50932},{\"end\":50940,\"start\":50939},{\"end\":50957,\"start\":50956},{\"end\":50963,\"start\":50962},{\"end\":50974,\"start\":50973},{\"end\":51469,\"start\":51468},{\"end\":51477,\"start\":51476},{\"end\":51488,\"start\":51487},{\"end\":51497,\"start\":51496},{\"end\":51509,\"start\":51508},{\"end\":51517,\"start\":51516},{\"end\":51519,\"start\":51518},{\"end\":51999,\"start\":51998},{\"end\":52007,\"start\":52006},{\"end\":52016,\"start\":52015},{\"end\":52022,\"start\":52021},{\"end\":52028,\"start\":52027},{\"end\":52035,\"start\":52034},{\"end\":52037,\"start\":52036},{\"end\":52401,\"start\":52400},{\"end\":52409,\"start\":52408},{\"end\":52419,\"start\":52418},{\"end\":52427,\"start\":52426},{\"end\":52819,\"start\":52818},{\"end\":52826,\"start\":52825},{\"end\":52835,\"start\":52834},{\"end\":52841,\"start\":52840},{\"end\":53253,\"start\":53252},{\"end\":53262,\"start\":53261},{\"end\":53273,\"start\":53272},{\"end\":53286,\"start\":53285},{\"end\":53569,\"start\":53568},{\"end\":53577,\"start\":53576},{\"end\":53584,\"start\":53583},{\"end\":53993,\"start\":53992},{\"end\":54001,\"start\":54000},{\"end\":54009,\"start\":54008},{\"end\":54017,\"start\":54016},{\"end\":54406,\"start\":54405},{\"end\":54414,\"start\":54413},{\"end\":54422,\"start\":54421},{\"end\":54430,\"start\":54429},{\"end\":54439,\"start\":54438},{\"end\":54737,\"start\":54736},{\"end\":54739,\"start\":54738},{\"end\":54748,\"start\":54747},{\"end\":54750,\"start\":54749},{\"end\":55020,\"start\":55019},{\"end\":55032,\"start\":55031},{\"end\":55044,\"start\":55043},{\"end\":55380,\"start\":55379},{\"end\":55388,\"start\":55387},{\"end\":55395,\"start\":55394},{\"end\":55401,\"start\":55400},{\"end\":55409,\"start\":55408},{\"end\":55812,\"start\":55811},{\"end\":55820,\"start\":55819}]", "bib_author_last_name": "[{\"end\":38179,\"start\":38172},{\"end\":38400,\"start\":38391},{\"end\":38409,\"start\":38404},{\"end\":38419,\"start\":38413},{\"end\":38430,\"start\":38423},{\"end\":38441,\"start\":38434},{\"end\":38451,\"start\":38445},{\"end\":38818,\"start\":38809},{\"end\":38831,\"start\":38822},{\"end\":39116,\"start\":39112},{\"end\":39122,\"start\":39120},{\"end\":39130,\"start\":39126},{\"end\":39136,\"start\":39134},{\"end\":39661,\"start\":39653},{\"end\":39673,\"start\":39667},{\"end\":39966,\"start\":39961},{\"end\":39978,\"start\":39970},{\"end\":39990,\"start\":39982},{\"end\":39999,\"start\":39994},{\"end\":40369,\"start\":40367},{\"end\":40381,\"start\":40373},{\"end\":40391,\"start\":40385},{\"end\":40403,\"start\":40395},{\"end\":40731,\"start\":40729},{\"end\":40740,\"start\":40735},{\"end\":40747,\"start\":40744},{\"end\":40754,\"start\":40751},{\"end\":41207,\"start\":41193},{\"end\":41218,\"start\":41211},{\"end\":41226,\"start\":41222},{\"end\":41236,\"start\":41230},{\"end\":41247,\"start\":41240},{\"end\":41259,\"start\":41251},{\"end\":41268,\"start\":41263},{\"end\":41666,\"start\":41662},{\"end\":41678,\"start\":41670},{\"end\":41689,\"start\":41682},{\"end\":41697,\"start\":41693},{\"end\":41706,\"start\":41701},{\"end\":42147,\"start\":42143},{\"end\":42160,\"start\":42151},{\"end\":42171,\"start\":42164},{\"end\":42179,\"start\":42175},{\"end\":42188,\"start\":42183},{\"end\":42535,\"start\":42530},{\"end\":42548,\"start\":42539},{\"end\":42558,\"start\":42552},{\"end\":42569,\"start\":42565},{\"end\":42580,\"start\":42573},{\"end\":42590,\"start\":42584},{\"end\":43061,\"start\":43053},{\"end\":43067,\"start\":43065},{\"end\":43075,\"start\":43071},{\"end\":43084,\"start\":43079},{\"end\":43092,\"start\":43088},{\"end\":43100,\"start\":43096},{\"end\":43112,\"start\":43104},{\"end\":43514,\"start\":43508},{\"end\":43524,\"start\":43518},{\"end\":43890,\"start\":43888},{\"end\":43897,\"start\":43894},{\"end\":43908,\"start\":43903},{\"end\":44258,\"start\":44252},{\"end\":44268,\"start\":44262},{\"end\":44500,\"start\":44494},{\"end\":44512,\"start\":44504},{\"end\":44525,\"start\":44516},{\"end\":44534,\"start\":44529},{\"end\":44545,\"start\":44538},{\"end\":44560,\"start\":44549},{\"end\":44570,\"start\":44564},{\"end\":45035,\"start\":45031},{\"end\":45042,\"start\":45039},{\"end\":45051,\"start\":45046},{\"end\":45059,\"start\":45055},{\"end\":45066,\"start\":45063},{\"end\":45488,\"start\":45481},{\"end\":45504,\"start\":45492},{\"end\":45512,\"start\":45508},{\"end\":45523,\"start\":45516},{\"end\":45850,\"start\":45844},{\"end\":45858,\"start\":45854},{\"end\":45871,\"start\":45862},{\"end\":45880,\"start\":45875},{\"end\":45892,\"start\":45884},{\"end\":45902,\"start\":45896},{\"end\":46232,\"start\":46230},{\"end\":46239,\"start\":46236},{\"end\":46245,\"start\":46243},{\"end\":46251,\"start\":46249},{\"end\":46263,\"start\":46257},{\"end\":46701,\"start\":46699},{\"end\":46707,\"start\":46705},{\"end\":46713,\"start\":46711},{\"end\":46725,\"start\":46719},{\"end\":47228,\"start\":47225},{\"end\":47239,\"start\":47232},{\"end\":47678,\"start\":47673},{\"end\":47687,\"start\":47684},{\"end\":48073,\"start\":48069},{\"end\":48081,\"start\":48077},{\"end\":48502,\"start\":48491},{\"end\":48514,\"start\":48508},{\"end\":48524,\"start\":48518},{\"end\":48535,\"start\":48528},{\"end\":48546,\"start\":48539},{\"end\":48976,\"start\":48970},{\"end\":48984,\"start\":48980},{\"end\":49000,\"start\":48988},{\"end\":49009,\"start\":49006},{\"end\":49321,\"start\":49316},{\"end\":49332,\"start\":49327},{\"end\":49339,\"start\":49336},{\"end\":49769,\"start\":49762},{\"end\":50091,\"start\":50084},{\"end\":50104,\"start\":50095},{\"end\":50497,\"start\":50493},{\"end\":50514,\"start\":50501},{\"end\":50520,\"start\":50518},{\"end\":50526,\"start\":50524},{\"end\":50532,\"start\":50530},{\"end\":50543,\"start\":50536},{\"end\":50555,\"start\":50547},{\"end\":50562,\"start\":50559},{\"end\":50924,\"start\":50920},{\"end\":50930,\"start\":50928},{\"end\":50937,\"start\":50934},{\"end\":50954,\"start\":50941},{\"end\":50960,\"start\":50958},{\"end\":50971,\"start\":50964},{\"end\":50983,\"start\":50975},{\"end\":51474,\"start\":51470},{\"end\":51485,\"start\":51478},{\"end\":51494,\"start\":51489},{\"end\":51506,\"start\":51498},{\"end\":51514,\"start\":51510},{\"end\":51526,\"start\":51520},{\"end\":52004,\"start\":52000},{\"end\":52013,\"start\":52008},{\"end\":52019,\"start\":52017},{\"end\":52025,\"start\":52023},{\"end\":52032,\"start\":52029},{\"end\":52043,\"start\":52038},{\"end\":52406,\"start\":52402},{\"end\":52416,\"start\":52410},{\"end\":52424,\"start\":52420},{\"end\":52435,\"start\":52428},{\"end\":52823,\"start\":52820},{\"end\":52832,\"start\":52827},{\"end\":52838,\"start\":52836},{\"end\":52844,\"start\":52842},{\"end\":53259,\"start\":53254},{\"end\":53270,\"start\":53263},{\"end\":53283,\"start\":53274},{\"end\":53290,\"start\":53287},{\"end\":53574,\"start\":53570},{\"end\":53581,\"start\":53578},{\"end\":53592,\"start\":53585},{\"end\":53998,\"start\":53994},{\"end\":54006,\"start\":54002},{\"end\":54014,\"start\":54010},{\"end\":54022,\"start\":54018},{\"end\":54411,\"start\":54407},{\"end\":54419,\"start\":54415},{\"end\":54427,\"start\":54423},{\"end\":54436,\"start\":54431},{\"end\":54446,\"start\":54440},{\"end\":54745,\"start\":54740},{\"end\":54756,\"start\":54751},{\"end\":55029,\"start\":55021},{\"end\":55041,\"start\":55033},{\"end\":55049,\"start\":55045},{\"end\":55385,\"start\":55381},{\"end\":55392,\"start\":55389},{\"end\":55398,\"start\":55396},{\"end\":55406,\"start\":55402},{\"end\":55413,\"start\":55410},{\"end\":55817,\"start\":55813},{\"end\":55826,\"start\":55821}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":38323,\"start\":38087},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16080844},\"end\":38715,\"start\":38325},{\"attributes\":{\"doi\":\"arXiv:1905.12079\",\"id\":\"b2\"},\"end\":39023,\"start\":38717},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":210919925},\"end\":39530,\"start\":39025},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":972888},\"end\":39901,\"start\":39532},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":145508},\"end\":40351,\"start\":39903},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":54465873},\"end\":40679,\"start\":40353},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206594692},\"end\":41081,\"start\":40681},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":42441056},\"end\":41585,\"start\":41083},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10655945},\"end\":42055,\"start\":41587},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2123767},\"end\":42455,\"start\":42057},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":5954518},\"end\":42959,\"start\":42457},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9585413},\"end\":43457,\"start\":42961},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":37447671},\"end\":43813,\"start\":43459},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4140974},\"end\":44214,\"start\":43815},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":5855042},\"end\":44430,\"start\":44216},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7359747},\"end\":44968,\"start\":44432},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":57189382},\"end\":45420,\"start\":44970},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":201124572},\"end\":45768,\"start\":45422},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4800351},\"end\":46165,\"start\":45770},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4868248},\"end\":46615,\"start\":46167},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5115938},\"end\":47087,\"start\":46617},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4392433},\"end\":47613,\"start\":47089},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":51892714},\"end\":47997,\"start\":47615},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206594775},\"end\":48413,\"start\":47999},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":52953916},\"end\":48894,\"start\":48415},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4649936},\"end\":49254,\"start\":48896},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4929149},\"end\":49676,\"start\":49256},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206421766},\"end\":50009,\"start\":49678},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":202235752},\"end\":50421,\"start\":50011},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":204852023},\"end\":50850,\"start\":50423},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":58006460},\"end\":51376,\"start\":50852},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":57761160},\"end\":51934,\"start\":51378},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4633214},\"end\":52369,\"start\":51936},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":72940813},\"end\":52755,\"start\":52371},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":199442313},\"end\":53159,\"start\":52757},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3440950},\"end\":53510,\"start\":53161},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52238978},\"end\":53926,\"start\":53512},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":8338993},\"end\":54372,\"start\":53928},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":51908879},\"end\":54666,\"start\":54374},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":27776989},\"end\":54974,\"start\":54668},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":102350792},\"end\":55346,\"start\":54976},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5299559},\"end\":55736,\"start\":55348},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":42427078},\"end\":56166,\"start\":55738}]", "bib_title": "[{\"end\":38387,\"start\":38325},{\"end\":39108,\"start\":39025},{\"end\":39647,\"start\":39532},{\"end\":39957,\"start\":39903},{\"end\":40363,\"start\":40353},{\"end\":40725,\"start\":40681},{\"end\":41189,\"start\":41083},{\"end\":41658,\"start\":41587},{\"end\":42139,\"start\":42057},{\"end\":42526,\"start\":42457},{\"end\":43049,\"start\":42961},{\"end\":43504,\"start\":43459},{\"end\":43884,\"start\":43815},{\"end\":44244,\"start\":44216},{\"end\":44490,\"start\":44432},{\"end\":45027,\"start\":44970},{\"end\":45477,\"start\":45422},{\"end\":45838,\"start\":45770},{\"end\":46224,\"start\":46167},{\"end\":46693,\"start\":46617},{\"end\":47221,\"start\":47089},{\"end\":47669,\"start\":47615},{\"end\":48065,\"start\":47999},{\"end\":48487,\"start\":48415},{\"end\":48966,\"start\":48896},{\"end\":49312,\"start\":49256},{\"end\":49758,\"start\":49678},{\"end\":50080,\"start\":50011},{\"end\":50489,\"start\":50423},{\"end\":50916,\"start\":50852},{\"end\":51466,\"start\":51378},{\"end\":51996,\"start\":51936},{\"end\":52398,\"start\":52371},{\"end\":52816,\"start\":52757},{\"end\":53250,\"start\":53161},{\"end\":53566,\"start\":53512},{\"end\":53990,\"start\":53928},{\"end\":54403,\"start\":54374},{\"end\":54734,\"start\":54668},{\"end\":55017,\"start\":54976},{\"end\":55377,\"start\":55348},{\"end\":55809,\"start\":55738}]", "bib_author": "[{\"end\":38181,\"start\":38170},{\"end\":38402,\"start\":38389},{\"end\":38411,\"start\":38402},{\"end\":38421,\"start\":38411},{\"end\":38432,\"start\":38421},{\"end\":38443,\"start\":38432},{\"end\":38453,\"start\":38443},{\"end\":38820,\"start\":38807},{\"end\":38833,\"start\":38820},{\"end\":39118,\"start\":39110},{\"end\":39124,\"start\":39118},{\"end\":39132,\"start\":39124},{\"end\":39138,\"start\":39132},{\"end\":39663,\"start\":39649},{\"end\":39675,\"start\":39663},{\"end\":39968,\"start\":39959},{\"end\":39980,\"start\":39968},{\"end\":39992,\"start\":39980},{\"end\":40001,\"start\":39992},{\"end\":40371,\"start\":40365},{\"end\":40383,\"start\":40371},{\"end\":40393,\"start\":40383},{\"end\":40405,\"start\":40393},{\"end\":40733,\"start\":40727},{\"end\":40742,\"start\":40733},{\"end\":40749,\"start\":40742},{\"end\":40756,\"start\":40749},{\"end\":41209,\"start\":41191},{\"end\":41220,\"start\":41209},{\"end\":41228,\"start\":41220},{\"end\":41238,\"start\":41228},{\"end\":41249,\"start\":41238},{\"end\":41261,\"start\":41249},{\"end\":41270,\"start\":41261},{\"end\":41668,\"start\":41660},{\"end\":41680,\"start\":41668},{\"end\":41691,\"start\":41680},{\"end\":41699,\"start\":41691},{\"end\":41708,\"start\":41699},{\"end\":42149,\"start\":42141},{\"end\":42162,\"start\":42149},{\"end\":42173,\"start\":42162},{\"end\":42181,\"start\":42173},{\"end\":42190,\"start\":42181},{\"end\":42537,\"start\":42528},{\"end\":42550,\"start\":42537},{\"end\":42560,\"start\":42550},{\"end\":42571,\"start\":42560},{\"end\":42582,\"start\":42571},{\"end\":42592,\"start\":42582},{\"end\":42596,\"start\":42592},{\"end\":43063,\"start\":43051},{\"end\":43069,\"start\":43063},{\"end\":43077,\"start\":43069},{\"end\":43086,\"start\":43077},{\"end\":43094,\"start\":43086},{\"end\":43102,\"start\":43094},{\"end\":43114,\"start\":43102},{\"end\":43516,\"start\":43506},{\"end\":43526,\"start\":43516},{\"end\":43892,\"start\":43886},{\"end\":43899,\"start\":43892},{\"end\":43910,\"start\":43899},{\"end\":44260,\"start\":44246},{\"end\":44270,\"start\":44260},{\"end\":44502,\"start\":44492},{\"end\":44514,\"start\":44502},{\"end\":44527,\"start\":44514},{\"end\":44536,\"start\":44527},{\"end\":44547,\"start\":44536},{\"end\":44562,\"start\":44547},{\"end\":44572,\"start\":44562},{\"end\":45037,\"start\":45029},{\"end\":45044,\"start\":45037},{\"end\":45053,\"start\":45044},{\"end\":45061,\"start\":45053},{\"end\":45068,\"start\":45061},{\"end\":45490,\"start\":45479},{\"end\":45506,\"start\":45490},{\"end\":45514,\"start\":45506},{\"end\":45525,\"start\":45514},{\"end\":45852,\"start\":45840},{\"end\":45860,\"start\":45852},{\"end\":45873,\"start\":45860},{\"end\":45882,\"start\":45873},{\"end\":45894,\"start\":45882},{\"end\":45904,\"start\":45894},{\"end\":46234,\"start\":46226},{\"end\":46241,\"start\":46234},{\"end\":46247,\"start\":46241},{\"end\":46253,\"start\":46247},{\"end\":46265,\"start\":46253},{\"end\":46703,\"start\":46695},{\"end\":46709,\"start\":46703},{\"end\":46715,\"start\":46709},{\"end\":46727,\"start\":46715},{\"end\":47230,\"start\":47223},{\"end\":47241,\"start\":47230},{\"end\":47680,\"start\":47671},{\"end\":47689,\"start\":47680},{\"end\":48075,\"start\":48067},{\"end\":48083,\"start\":48075},{\"end\":48504,\"start\":48489},{\"end\":48516,\"start\":48504},{\"end\":48526,\"start\":48516},{\"end\":48537,\"start\":48526},{\"end\":48548,\"start\":48537},{\"end\":48978,\"start\":48968},{\"end\":48986,\"start\":48978},{\"end\":49002,\"start\":48986},{\"end\":49011,\"start\":49002},{\"end\":49323,\"start\":49314},{\"end\":49334,\"start\":49323},{\"end\":49341,\"start\":49334},{\"end\":49771,\"start\":49760},{\"end\":50093,\"start\":50082},{\"end\":50106,\"start\":50093},{\"end\":50499,\"start\":50491},{\"end\":50516,\"start\":50499},{\"end\":50522,\"start\":50516},{\"end\":50528,\"start\":50522},{\"end\":50534,\"start\":50528},{\"end\":50545,\"start\":50534},{\"end\":50557,\"start\":50545},{\"end\":50564,\"start\":50557},{\"end\":50926,\"start\":50918},{\"end\":50932,\"start\":50926},{\"end\":50939,\"start\":50932},{\"end\":50956,\"start\":50939},{\"end\":50962,\"start\":50956},{\"end\":50973,\"start\":50962},{\"end\":50985,\"start\":50973},{\"end\":51476,\"start\":51468},{\"end\":51487,\"start\":51476},{\"end\":51496,\"start\":51487},{\"end\":51508,\"start\":51496},{\"end\":51516,\"start\":51508},{\"end\":51528,\"start\":51516},{\"end\":52006,\"start\":51998},{\"end\":52015,\"start\":52006},{\"end\":52021,\"start\":52015},{\"end\":52027,\"start\":52021},{\"end\":52034,\"start\":52027},{\"end\":52045,\"start\":52034},{\"end\":52408,\"start\":52400},{\"end\":52418,\"start\":52408},{\"end\":52426,\"start\":52418},{\"end\":52437,\"start\":52426},{\"end\":52825,\"start\":52818},{\"end\":52834,\"start\":52825},{\"end\":52840,\"start\":52834},{\"end\":52846,\"start\":52840},{\"end\":53261,\"start\":53252},{\"end\":53272,\"start\":53261},{\"end\":53285,\"start\":53272},{\"end\":53292,\"start\":53285},{\"end\":53576,\"start\":53568},{\"end\":53583,\"start\":53576},{\"end\":53594,\"start\":53583},{\"end\":54000,\"start\":53992},{\"end\":54008,\"start\":54000},{\"end\":54016,\"start\":54008},{\"end\":54024,\"start\":54016},{\"end\":54413,\"start\":54405},{\"end\":54421,\"start\":54413},{\"end\":54429,\"start\":54421},{\"end\":54438,\"start\":54429},{\"end\":54448,\"start\":54438},{\"end\":54747,\"start\":54736},{\"end\":54758,\"start\":54747},{\"end\":55031,\"start\":55019},{\"end\":55043,\"start\":55031},{\"end\":55051,\"start\":55043},{\"end\":55387,\"start\":55379},{\"end\":55394,\"start\":55387},{\"end\":55400,\"start\":55394},{\"end\":55408,\"start\":55400},{\"end\":55415,\"start\":55408},{\"end\":55819,\"start\":55811},{\"end\":55828,\"start\":55819}]", "bib_venue": "[{\"end\":39301,\"start\":39228},{\"end\":40142,\"start\":40080},{\"end\":40526,\"start\":40474},{\"end\":40897,\"start\":40835},{\"end\":41829,\"start\":41777},{\"end\":42717,\"start\":42665},{\"end\":43647,\"start\":43595},{\"end\":44025,\"start\":43976},{\"end\":44713,\"start\":44651},{\"end\":45209,\"start\":45147},{\"end\":46406,\"start\":46344},{\"end\":46868,\"start\":46806},{\"end\":47362,\"start\":47310},{\"end\":47824,\"start\":47765},{\"end\":48224,\"start\":48162},{\"end\":48663,\"start\":48614},{\"end\":49482,\"start\":49420},{\"end\":50227,\"start\":50175},{\"end\":51126,\"start\":51064},{\"end\":51669,\"start\":51607},{\"end\":52160,\"start\":52111},{\"end\":52578,\"start\":52516},{\"end\":52967,\"start\":52915},{\"end\":53735,\"start\":53673},{\"end\":54165,\"start\":54103},{\"end\":55172,\"start\":55120},{\"end\":55556,\"start\":55494},{\"end\":55969,\"start\":55907},{\"end\":38168,\"start\":38087},{\"end\":38491,\"start\":38453},{\"end\":38805,\"start\":38717},{\"end\":39226,\"start\":39138},{\"end\":39700,\"start\":39675},{\"end\":40078,\"start\":40001},{\"end\":40472,\"start\":40405},{\"end\":40833,\"start\":40756},{\"end\":41305,\"start\":41270},{\"end\":41775,\"start\":41708},{\"end\":42228,\"start\":42190},{\"end\":42663,\"start\":42596},{\"end\":43183,\"start\":43114},{\"end\":43593,\"start\":43526},{\"end\":43974,\"start\":43910},{\"end\":44306,\"start\":44270},{\"end\":44649,\"start\":44572},{\"end\":45145,\"start\":45068},{\"end\":45573,\"start\":45525},{\"end\":45939,\"start\":45904},{\"end\":46342,\"start\":46265},{\"end\":46804,\"start\":46727},{\"end\":47308,\"start\":47241},{\"end\":47763,\"start\":47689},{\"end\":48160,\"start\":48083},{\"end\":48612,\"start\":48548},{\"end\":49049,\"start\":49011},{\"end\":49418,\"start\":49341},{\"end\":49831,\"start\":49771},{\"end\":50173,\"start\":50106},{\"end\":50622,\"start\":50564},{\"end\":51062,\"start\":50985},{\"end\":51605,\"start\":51528},{\"end\":52109,\"start\":52045},{\"end\":52514,\"start\":52437},{\"end\":52913,\"start\":52846},{\"end\":53327,\"start\":53292},{\"end\":53671,\"start\":53594},{\"end\":54101,\"start\":54024},{\"end\":54496,\"start\":54448},{\"end\":54796,\"start\":54758},{\"end\":55118,\"start\":55051},{\"end\":55492,\"start\":55415},{\"end\":55905,\"start\":55828}]"}}}, "year": 2023, "month": 12, "day": 17}