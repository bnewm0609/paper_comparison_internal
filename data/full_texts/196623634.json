{"id": 196623634, "updated": "2023-10-07 00:29:40.997", "metadata": {"title": "Energy-Efficient Radio Resource Allocation for Federated Edge Learning", "authors": "[{\"first\":\"Qunsong\",\"last\":\"Zeng\",\"middle\":[]},{\"first\":\"Yuqing\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Kin\",\"last\":\"Leung\",\"middle\":[\"K.\"]},{\"first\":\"Kaibin\",\"last\":\"Huang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 7, "day": 13}, "abstract": "Edge machine learning involves the development of learning algorithms at the network edge to leverage massive distributed data and computation resources. Among others, the framework of federated edge learning (FEEL) is particularly promising for its data-privacy preservation. FEEL coordinates global model training at a server and local model training at edge devices over wireless links. In this work, we explore the new direction of energy-efficient radio resource management (RRM) for FEEL. To reduce devices' energy consumption, we propose energy-efficient strategies for bandwidth allocation and scheduling. They adapt to devices' channel states and computation capacities so as to reduce their sum energy consumption while warranting learning performance. In contrast with the traditional rate-maximization designs, the derived optimal policies allocate more bandwidth to those scheduled devices with weaker channels or poorer computation capacities, which are the bottlenecks of synchronized model updates in FEEL. On the other hand, the scheduling priority function derived in closed form gives preferences to devices with better channels and computation capacities. Substantial energy reduction contributed by the proposed strategies is demonstrated in learning experiments.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1907.06040", "mag": "3045027907", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icc/ZengDHL20", "doi": "10.1109/iccworkshops49005.2020.9145118"}}, "content": {"source": {"pdf_hash": "712d4c2ff125ab80a8aebba7384e9a61159390ae", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1907.06040v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1907.06040", "status": "GREEN"}}, "grobid": {"id": "1d28910823d4f10be4276a22653b3628d4724812", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/712d4c2ff125ab80a8aebba7384e9a61159390ae.txt", "contents": "\nEnergy-Efficient Radio Resource Allocation for Federated Edge Learning\n\n\nQunsong Zeng \nYuqing Du \nKin K Leung \nKaibin Huang \nEnergy-Efficient Radio Resource Allocation for Federated Edge Learning\n1\nEdge machine learning involves the development of learning algorithms at the network edge to leverage massive distributed data and computation resources. Among others, the framework of federated edge learning (FEEL) is particularly promising for its data-privacy preservation. FEEL coordinates global model training at a server and local model training at edge devices over wireless links. In this work, we explore the new direction of energy-efficient radio resource management (RRM) for FEEL. To reduce devices' energy consumption, we propose energy-efficient strategies for bandwidth allocation and scheduling. They adapt to devices' channel states and computation capacities so as to reduce their sum energy consumption while warranting learning performance. In contrast with the traditional ratemaximization designs, the derived optimal policies allocate more bandwidth to those scheduled devices with weaker channels or poorer computation capacities, which are the bottlenecks of synchronized model updates in FEEL. On the other hand, the scheduling priority function derived in closed form gives preferences to devices with better channels and computation capacities. Substantial energy reduction contributed by the proposed strategies is demonstrated in learning experiments.\n\nI. INTRODUCTION\n\nRecent years have witnessed a phenomenal growth in mobile data, most of which are generated in real-time and distributed at edge devices (e.g., smartphones and sensors) [1]. Uploading these massive data to the cloud for training artificial intelligence (AI) models is impractical due to various issues including privacy, network congestion, and latency. To address these issues, the federated edge learning (FEEL) framework has been developed [2]- [4], which implements distributed machine learning at the network edge. In particular, a server updates a global model Q. Zeng, Y. Du and K. Huang are with The University of Hong Kong, Hong Kong (Email: qszeng@eee.hku.hk, yqdu@eee.hku.hk, huangkb@eee.hku.hk). K. K. Leung is with Imperial College London, UK (Email: kin.leung@imperial.ac.uk). arXiv:1907.06040v1 [cs.IT] 13 Jul 2019 by aggregating local models (or stochastic gradients) transmitted by devices that are computed using local datasets. The updating of the global model using local models and the reverse are iterated till they converge. Besides preserving data privacy by avoiding data uploading, FEEL leverages distributed computation resources as well as allows rapid access to the realtime data generated by edge devices. One focus in the research area is communication-efficient FEEL where wireless techniques are designed to accelerate learning by reducing communication overhead and latency. However, the topic of energy-efficient communication for FEEL so far has not been explored. This is an important topic as training and transmission of large-scale models are energy consuming, while most edge devices especially sensors have limited battery lives.\n\nThis topic is investigated in the current work where novel radio-resource-management (RRM) strategies for joint bandwidth allocation and scheduling are proposed for minimizing the total device energy-consumption under a constraint on the learning speed.\n\nThe topic of communication-efficient FEEL has been extensively studied from different aspects. One branch of research focuses on edge-device selection so as to accelerate learning [5], [6]. In particular, a partial averaging scheme is proposed in [5], where only a portion of updates from fast-responding devices are used for global updating while those from stragglers are discarded. However, perfect device-update-uploading is assumed, which ignores the hostility of wireless channels, and at the same time overlooks the possibility of exploiting the sophisticated properties of wireless channels for improving the communication efficiency. By taking the properties into account, a joint device-selection and beamforming design is proposed for accelerating the federated edge learning [6]. Nevertheless, the device selection criterion is only based on the channel-state information (CSI) while ignoring the heterogeneous computation capacities of devices. On the other hand, to overcome the multi-access bottleneck, a broadband analog aggregation (BAA) multiple-access scheme is proposed in [2]. Specifically, by exploiting the waveform-superposition property of a multi-access channel, updates simultaneously transmitted by devices over broadband channels are analog aggregated \"over-the-air\" so as to reduce the multi-access latency. Given fixed communication cost per uploading, a control algorithm on uploading frequency is proposed in [4] by analyzing the convergence bound of distributed gradient descent to improve the learning performance. However, all these existing schemes are designed from the learning perspective while the energy-consumption issue of edge devices is out of scope, which is becoming increasingly important given the limited battery lives of devices.\n\nThis motivates the current work on energy-efficient FEEL.\n\nIn this work, we consider the problem of minimizing energy consumption of edge devices in the context of FEEL without compromising learning performance. To this end, two energyefficient RRM strategies are proposed for joint bandwidth allocation and scheduling. To the best known of authors' knowledge, this work represents the first attempt to consider the energyefficient RRM for FEEL.\n\nTo design the first energy-efficient RRM strategy, we assume a given set of edge devices and focus on bandwidth allocation. The optimal policy for energy minimization is derived in closedform. The solution suggests that each edge device should utilize all the allowed uploading time so as to minimize the energy consumption. Furthermore, it can be observed from the solution that under the constraint of synchronous updates, less bandwidth should be allocated to devices with more powerful computation capacities and better channel conditions. This is in contrast with the traditional rate-maximization design.\n\nThe second strategy extends the first to include scheduling, namely selecting devices to participate in FEEL. We propose a practical algorithm for iterating between solving two subproblems under the criterion of energy minimization: 1) scheduling and 2) bandwidth allocation using the first strategy. For scheduling, the optimal policy is derived in closed-form, indicating the selection priorities for devices. The solution suggests that a device with a poor computation capacity and a bad channel has a lower priority to be selected and vice versa.\n\nThe remainder of this paper is organized as follows. Section II introduces the system model.\n\nSections III and IV present two energy-efficient RRM strategies. Simulation results are provided in Section V, followed by the concluding remarks in Section VI.\n\nII. SYSTEM MODEL Consider a FEEL system consisting of a single edge server and K edge devices, denoted by a set K = {1, \u00b7 \u00b7 \u00b7 , K}. As described earlier and illustrated in Fig. 1, FEEL iterates between two steps: 1) updating the global model at the server by aggregating local models transmitted over a multi-access channel; 2) replacing the local models by broadcasting the global model.\n\nEach iteration is called a communication round. It is assumed that the edge server has perfect knowledge of the model size (determining the sizes of data transmitted by devices) as well as multiuser channel gains and local computation capacities, which can be obtained by feedback.  Using this information, for each communication round, the edge server needs to determine the energy-efficient strategy for scheduling and allocating bandwidth. Due to the fact that communication rounds are independent, it suffices to consider the problem for an arbitrary round without loss of generality.\n\n\nA. Multiple-access Model\n\nConsider orthogonal frequency-division multiple access (OFDMA) for local model uploading with total bandwidth B. Define \u03b3 k \u2208 [0, 1] as the bandwidth allocation ratio for device k, and the resulting allocated bandwidth is \u03b3 k B. Furthermore, let h k denote the corresponding channel gain. Given synchronous updates [7], a time constraint is set for local model training and model uploading in each communication round as follows\n(Time constraint) t comp k + t up k \u2264 T, \u2200k \u2208 K,(1)\nwhere t comp k and t up k denote the time for local model training time and model uploading time of device k, respectively. T is the maximum total time. The fact that edge devices have heterogeneous computation capacity is reflected in the differences among the values of {t comp k }. For ease of notation, we re-denote t up k as t k hereafter. Then, it follows that (1) can be rewritten as\nt k \u2264 T k , \u2200k \u2208 K,(2)\nwhere T k = T \u2212 t comp k is referred to as the allowed time for model uploading.\n\n\nB. Energy Consumption Model\n\nFor each communication round, the energy consumption of a typical edge device comprises two parts: one for transmission (model uploading) and the other for local model training, which are specified in the following.\n\n1) Energy consumption for model uploading: Let p k denote the transmission power (in Watt/Hz) of device k. The achievable rate (in bit/s), denoted by r k , can be written as\nr k = \u03b3 k B log 1 + p k h 2 k N 0 ,(3)\nwhere N 0 is the variance of the complex white Gaussian channel noise. Let L denote the data size (in bit), the data rate can then be calculated as\nr k = \u03b2 k L t k ,(4)\nwhere \u03b2 k is a state indicator for device k. Specifically, \u03b2 k = 1 if device k is selected for uploading, or 0 otherwise. By combining (3) and (4), the uploading energy consumption can be calculated as\nE up k = \u03b3 k Bp k t k = \u03b3 k Bt k N 0 h 2 k 2 \u03b2 k L \u03b3 k Bt k \u2212 1 .(5)\n2) Energy consumption for local training: Consider the local training of a neural network model via the well-known backpropagation (BP) algorithm on GPU. According to experiments reported in [8], the energy consumption of GPU only depends on the complexity of the BP algorithm and the size (or equivalently dimensions) of the model parameters. Since all edge devices train the same model of size L using the BP algorithm, the energy consumption of edge devices for local training is identical and denoted as E comp .\n\n\nC. Learning Speed Model\n\nIt is proved in [6], [9] that the convergence rate of distributed SGD is inversely proportional to the number of participating devices. Therefore, we use the total number of scheduled devices as the measurement of learning speed. By leveraging the indicator {\u03b2 k }, the learning speed can be expressed as\n(Learning speed) K k=1 \u03b2 k .(6)\nFrom the perspective of accelerating learning, it is desirable for the server to schedule as many devices as possible, which, however, is limited by finite radio resources.\n\n\nIII. ENERGY-EFFICIENT BANDWIDTH ALLOCATION\n\nIn this section, we consider the problem of RRM for a given set of active devices which can all meet the time constraint in (1) with \u03b2 k = 1, \u2200k \u2208 K. The goal is to minimize the total energy\nconsumption, i.e. K k=1 (E comp k + E up k )\n. Since the energy consumption for local model training, i.e. E comp , is uniform and fixed, the problem focuses on minimizing uploading energy and thus is formulated as\n(P1) min {\u03b3 k ,t k } K k=1 \u03b3 k Bt k N 0 h 2 k 2 \u03b2 k L \u03b3 k Bt k \u2212 1 s.t. K k=1 \u03b3 k = 1, 0 \u2264 \u03b3 k \u2264 1, k \u2208 K, 0 \u2264 t k \u2264 T k , k \u2208 K.\nBy solving the above problem, the server can optimally determine bandwidth partitioning, as specified by {\u03b3 k }, and the uploading time {t k } for devices. To begin with, one basic characteristic of Problem (P1) is given as follows.\n\nLemma 1. The objective of Problem (P1) is a non-increasing function in t k and \u03b3 k , \u2200k \u2208 K.\n\nThe result follows from observing the derivative of the objective with the details omitted for brevity. It can be inferred from the Lemma 1 that it is optimal to maximize the transmission time of each device, resulting in t k = T k , \u2200k \u2208 K, which is independent of the allocated bandwidth \u03b3 k . Then, it follows that the optimal RRM policy is obtained as follows.\n\nTheorem 1. (Optimal Bandwidth Allocation). The optimal policy for bandwidth allocation is\n\u03b3 k = \u03b2 k L ln 2 BT k 1 + W h 2 k \u03bd \u2212BT k N 0 BT k N 0 e , \u2200k \u2208 K,(7)t k = T k , \u2200k \u2208 K,(8)\nwhere W(\u00b7) is the Lambert W function, T k = T \u2212 t comp k is the restricted transmission time for device k, \u03bd is the solved value for the Lagrange multiplier and e is the Euler's number.\n\nProof: See Appendix A.\n\nNext, to gain more insight, a corollary is given as follows.\n\nCorollary 1. \u03b3 k is a non-increasing function with respect to T k and h 2 k , respectively.\n\nProof: See Appendix B.\n\nOne observation can be made from Corollary 1 is that more bandwidths should be allocated to edge devices with weaker computation capacities, namely smaller T k . The reason is that these devices are the bottlenecks in synchronized updates and sum energy minimization. To be specific, they require larger bandwidths so as to complete model uploading within the short allowed transmission/uploading time and also to reduce transmission power.\n\nFurthermore, it can be observed that more bandwidths should be allocated to devices with weaker channels. Overcoming the conditions requires boosting transmission power or more bandwidths. For energy minimization, the latter is preferred. \n\n\nIV. ENERGY-AND-LEARNING AWARE SCHEDULING\n\nIn the presence of devices with poor computation capacities or weak channels, scheduling only a subset of devices for model uploading can reduce sum energy consumption as well as meet the time constraint. By modifying Problem (P1) to include the learning speed in the objective, the current problem can be formulated as\n(P2) min {\u03b3 k ,t k ,\u03b2 k } K k=1 \u03b3 k Bt k N 0 h 2 k 2 \u03b2 k L \u03b3 k Bt k \u2212 1 \u2212 \u03bb K k=1 \u03b2 k s.t. \u03b2 k \u2208 {0, 1}, k \u2208 K, K k=1 \u03b3 k = 1, 0 \u2264 \u03b3 k \u2264 1, k \u2208 K, 0 \u2264 t k \u2264 T k , k \u2208 K,\nwhere the trade-off factor \u03bb > 0 is a pre-determined constant. Directly solving the above problem is difficult due to its non-convexity arising from the integer constraint. To solve this problem, we adopt the common solution method, referred to as relaxation-and-rounding. Specifically, it firstly relaxes the integer constraint \u03b2 k \u2208 {0, 1} as the real-value constraint 0 \u2264 \u03b2 k \u2264 1, and then the integer solution is determined using rounding techniques after solving the relaxed problem. It is also noted that after this relaxation, the continuous value of \u03b2 k can be viewed as the selection priority of edge device k. Mathematically, the relaxed problem can be written as\n(P3) min {\u03b3 k ,t k ,\u03b2 k } K k=1 \u03b3 k Bt k N 0 h 2 k 2 \u03b2 k L \u03b3 k Bt k \u2212 1 \u2212 \u03bb K k=1 \u03b2 k s.t. 0 \u2264 \u03b2 k \u2264 1, k \u2208 K, K k=1 \u03b3 k = 1, 0 \u2264 \u03b3 k \u2264 1, k \u2208 K, 0 \u2264 t k \u2264 T k , k \u2208 K.\nIt is easy to prove that (P3) is a convex problem. A standard solution approach is to use a numerical method since the optimization variables are all coupled. In the remainder of the section, we propose a more insightful and efficient approach that iterates between solving two subproblems: 1) the bandwidth-allocation in Problem (P1); 2) scheduling problem. To be specific, the first sub-problem is to allocate bandwidths given scheduled devices indicated by {\u03b2 k }, where the optimal solution is given in Theorem 1. The other sub-problem (scheduling) is to decide the selection priorities of edge devices, i.e. {\u03b2 k }, given {\u03b3 k , t k }, which can be mathematically written as\n(P4) min {\u03b2 k } K k=1 \u03b3 k Bt k N 0 h 2 k 2 \u03b2 k L \u03b3 k Bt k \u2212 1 \u2212 \u03bb K k=1 \u03b2 k s.t. 0 \u2264 \u03b2 k \u2264 1, k \u2208 K.\nIt is easy to show that Problem (P4) is convex and the closed-form solution is derived in the following Theorem.\n\nTheorem 2. (Edge-device Selection Priority). The optimal selection priority for device k is given as\n\u03b2 k = min max \u03b3 k BT k L log \u03bbh 2 k N 0 L ln 2 , 0 1 , k \u2208 K.(9)\nProof: See Appendix C.\n\n\nAlgorithm 1 Joint Bandwidth Allocation and Scheduling\n\nInitialization: Randomly set indicators {\u03b2 k } \u2208 [0, 1].\n\n\nIteration:\n\n\u2022 (Energy-efficient Bandwidth Allocation): Given fixed {\u03b2 k }, compute {\u03b3 k , t k } using (7) and (8);\n\n\u2022 (Energy-and-Learning Aware Scheduling): Given fixed {\u03b3 k , t k }, compute {\u03b2 k } using (9);\nUntil Convergence. Round indicators {\u03b2 k } to {0, 1}.\nCompute {\u03b3 k , t k } using (7) and (8).\n\nOutput the optimal solution {\u03b2 k , \u03b3 k , t k }.\n\nThis theorem is consistent with the intuition that device k with a high computation capacity and a good channel should have a high priority to be selected, i.e. \u03b2 k is large. (9)   as follows. First, the performance of the baseline is independent of T . The reason is that the learning performance only depends on the number of scheduled edge-devices for uploading (i.e. K k=1 \u03b2 k ), and this number is fixed for the baseline (i.e. K = 50). Second, the average learning accuracy of the proposed algorithm is an increasing function of T , whose performance approaches the baseline for the large T . This is because of the fact that as the allowed transmission time increases, more devices will be scheduled for model uploading, giving rise to the improvement of the learning performance. Furthermore, define the energy reduction ratio as r =\n\n\nRemark 2. (Effects of Parameters on Selection Priority). It can be observed from\nE baseline \u2212E proposed E baseline \u00d7 100%\n, where E proposed and E baseline denote the sum energy consumptions of the proposed scheme and the baseline, respectively. It can be observed that the energy reduction ratio r is a decreasing function of T , which approximately ranges from 70% to 98%. This is because that as T increases, the scheduled devices in the proposed scheme increases, and thereby the resulting sum energy consumption is larger. This reduces its difference to the sum energy consumption of the baseline, where all devices are scheduled for uploading.\n\n\nVI. CONCLUDING REMARKS\n\nIn this paper, we have proposed energy-efficient RRM (bandwidth allocation and scheduling)\n\nfor federated edge learning. By adapting to both channel states and computation capacities, the strategies effectively reduce sum device energy consumption while providing a guarantee on learning speed. This work makes the first attempt to explore the direction of energy-efficient RRM for federated edge learning. In the future, this work can be generalized into RRM for the asynchronous model-update scenario. Apart from the channel states and computation capacities, the sparsity of the updates can be also considered while allocating radio resources. Moreover, the effects of energy consumption model for local computing can be further taken into consideration to include the feature of local batch-size adaptation.\n\n\nAPPENDIX\n\nA. Proof of Theorem 1\n\nAs aforementioned, one can have that t k = T k , \u2200k. Next, we prove the optimal bandwidth allocation strategy. Substituting t k = T k into (P1), it follows that the original Problem (P1)\n\ncan be rewritten as\nmin \u03b3 k K k=1 \u03b3 k BT k N 0 h 2 k 2 \u03b2 k L \u03b3 k BT k \u2212 1 s.t. 0 \u2264 \u03b3 k \u2264 1, k \u2208 K, K k=1 \u03b3 k = 1.(10)\nSince the above problem is a convex problem, by introducing Lagrange multipliers \u00b5 = [\u00b5 1 , \u00b5 2 , \u00b7 \u00b7 \u00b7 , \u00b5 K ] T \u2208 R K for the inequality constraints \u03b3 0 with \u03b3 = [\u03b3 1 , \u03b3 2 , \u00b7 \u00b7 \u00b7 \u03b3 K ] T , and a multiplier \u03bd \u2208 R for the equality constraint 1 T \u03b3 = 1, the KKT conditions can be written as\nfollows \u03b3 0, 1 T \u03b3 = 1, \u00b5 0, \u00b5 k \u03b3 k = 0, k \u2208 K BT k N 0 h 2 k 2 \u03b2 k L \u03b3 k BT k \u2212 \u03b2 k L ln 2 \u03b3 k BT k 2 \u03b2 k L \u03b3 k BT k \u2212 1 \u2212 \u00b5 k + \u03bd = 0, k \u2208 K.\n(11) By solving the above equations, one can have\n\u03b3 k = \u03b2 k L ln 2 BT k 1 + W h 2 k \u03bd \u2212BT k N 0 BT k N 0 e ,(12)\nwhere W(\u00b7) is the Lambert W function, and the Lagrange multiplier value \u03bd is calculated by\nsolving K k=1 \u03b2 k L ln 2 BT k 1+W h 2 k \u03bd \u2212BT k N 0 BT k N 0 e = 1.\nThis completes the whole proof.\n\n\nB. Proof of Corollary 1\n\nFirst, we prove that \u03b3 k is non-increasing with respect to T k . Denote x = h 2 k \u03bd \u2212BT k N 0 BT k N 0 e , then it follows that T k = h 2 k \u03bd (x+ 1 e )BN0e\n\n. Substituting it to the expression for \u03b3 k , one can have\n\u03b3 k = \u03b2 k L ln 2 BT k 1 + W h 2 k \u03bd \u2212BT k N 0 BT k N 0 e = N 0 e\u03b2 k L ln 2 h 2 k \u03bd x + 1 e 1 + W(x) .(13)\nFurther, we denote\ny = x + 1 e 1 + W(x) = We W(x) + 1 e 1 + W(x) .(14)\nIt is easy to prove that y is non-decreasing with respect to W(x). Since W(x) is non-decreasing with respect to x and x(T k ) is non-increasing with respect to T k , it follows that \u03b3 k is nonincreasing with respect to T k .\n\nNext, we prove that \u03b3 k is non-increasing with respect to h 2 k . From x = h 2 k \u03bd \u2212BT k N 0 BT k N 0 e , one can have h 2 k = BN 0 eT k \u03bd x + 1 e . Substituting it into the expression for \u03b3 k , it follows that \u03b3 k = \u03b2 k L ln 2\nBT k 1 + W h 2 k \u03bd \u2212BT k N 0 BT k N 0 e = \u03b2 k L ln 2 BT k 1 1 + W(x) .(15)\nFurther, we let\nz = 1 1 + W(x) .(16)\nIt is obvious that z is non-increasing with respect to W(x). Since W(x) is non-decreasing with respect to x and x(h 2 k ) is non-decreasing with respect to h 2 k , we can conclude that \u03b3 k is non-increasing with respect to h 2 k . This completes the whole proof.\n\n\nC. Proof of Theorem 2\n\nDenote \u03b2 = [\u03b2 1 , \u03b2 2 , \u00b7 \u00b7 \u00b7 , \u03b2 K ] T \u2208 R K and define the function as follows:\nJ(\u03b2) = K k=1 \u03b3 k BT k N 0 h 2 k 2 \u03b2 k L \u03b3 k BT k \u2212 1 \u2212 \u03bb\u03b2 k ,(17)\nthen it follows that\n\u2202J(\u03b2) \u2202\u03b2 k = N 0 L ln 2 h 2 k 2 \u03b2 k L \u03b3 k BT k \u2212 \u03bb,(18)\nLet \u2202J(\u03b2) \u2202\u03b2 k = 0 and one can obtain the following result:\n\u03b2 k = \u03b3 k BT k L log \u03bbh 2 k N 0 L ln 2 .(19)\nWhen considering the constraint, it can be divided into three cases with respect to\u03b2 k :\n\n1) if\u03b2 k < 0, then the minimum will be obtained at \u03b2 k = 0;\n\n2) if 0 \u2264\u03b2 k \u2264 1, then the minimum will be obtained at \u03b2 k =\u03b2 k ;\n\n3) if\u03b2 k > 1, then the minimum will be obtained at \u03b2 k = 1.\n\nIn summary, the optimal point is\n\u03b2 k = min max \u03b3 k BT k L log \u03bbh 2 k N 0 L ln 2 , 0 , 1 .(20)\nThis completes the whole proof.\n\nFigure 1 .\n1A framework for FEEL system.\n\nRemark 1 .\n1(Rate-centric vs. Learning-centric RRM). The conventional RRM strategies for sum-rate maximization, such as water-filling, allocate more resources to users with stronger channels. In contrast, the proposed RRM policy for edge learning allocates more resources to users with weaker channels and/or poorer computation capacities. This reflects the differences in communication principles for the two paradigms of communication-computation separation and communication-computation integration.\n\nFigure 2 .\n2that \u03b2 k , indicating the selection priority of device k, scales with the allowed transmission time, i.e. T k , linearly and with the channel gain approximately as log(h k ). The former scaling is much faster than the latter. This shows that the allowed transmission time (or equivalently computation capacity) is dominant over the channel on determining the selection priority of the device. Based on the above results, the solution of Problem (P2) is provided in Algorithm 1 by iteratively solving (P1) and (P4) until convergence. V. SIMULATION RESULTS The simulation settings are as follows unless specified otherwise. There are K = 50 edge devices with local model training time, {t comp k }, following the uniform distribution in the range of (0, 10] ms. Consider an OFDMA system where the bandwidth B = 1 MHz. The channel gains {h k } are modeled as independent Rayleigh fading with average path loss set as 10 \u22124 . The variance of the complex white Gaussian channel noise is set as N 0 = 10 \u22128 W. For learning, the model size is set to be L = 10 4 bits and the task aims at classifying handwritten digits using the MNIST dataset. Each device is randomly assigned 20 samples. The model is a 6-layer convolutional neural network (CNN), consisting of two 5 \u00d7 5 convolution layers with rectified linear unit (ReLU) activation, which have 10 and 20 channels respectively, each followed by Sum device energy consumption vs. constrained time per communication round in a FEEL system. 2 \u00d7 2 max pooling, a fully connected layer with 50 units and ReLU activation, and a softmax output layer. 1) Energy-efficient bandwidth allocation: Consider the scenario that all edge-devices are scheduled for model uploading, the performance of the proposed RRM policy is benchmarked against the uniform bandwidth allocation policy, which allocates equal bandwidth to edge devices. Particularly, the curves of total energy consumption by edge devices versus the communication round time T are shown inFig. 2. Several observations can be made as follows. First, the total energy consumption reduces as T grows for both cases. This coincides with Lemma 1 that the energy consumption is smaller if the allowed transmission time is larger. Second, it can be found that the proposed optimal policy outperforms the baseline scheme, showing its effectiveness.2) Energy-and-learning aware scheduling: Consider the scenario that the communication time is short and the edge server needs to select the edge-devices for uploading. The performance of the proposed Algorithm 1 for joint bandwidth allocation and scheduling is benchmarked against the previous case that all edge-devices are selected. Particularly, the relationship between the average learning accuracy of the federated learning algorithm and the constrained time T is illustrated inFig. 3given the fixed communication round 10. Several observations can be made\n\nFigure 3 .\n3The learning accuracy vs. constrained communication-round time for the proposed scheme and the baseline are illustrated by the black solid and dashed lines, respectively. By defining the energy reduction ratio as r =E baseline \u2212E proposed E baseline \u00d7100%with E proposed and E baseline denoting the sum energy consumptions of the proposed scheme and the baseline, respectively, the relationship between the energy reduction ratio and constrained communication-round time is shown by the grey line. The implementation details are specified as follows. The total number of devices is K = 50 and the communication-round is set to be 10.\n\nTowards an intelligent edge: Wireless communication meets machine learning. G Zhu, D Liu, Y Du, C You, J Zhang, K Huang, G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. Huang, \"Towards an intelligent edge: Wireless communication meets machine learning.\" [Online]. Available: http://arxiv.org/abs/1809.00343\n\nLow-latency broadband analog aggregation for federated edge learning. G Zhu, Y Wang, K Huang, G. Zhu, Y. Wang, and K. Huang, \"Low-latency broadband analog aggregation for federated edge learning.\" [Online].\n\nMachine learning at the wireless edge: Distributed stochastic gradient descent over-the-air. M M Amiri, D G\u00fcnd\u00fcz, M. M. Amiri and D. G\u00fcnd\u00fcz, \"Machine learning at the wireless edge: Distributed stochastic gradient descent over-the-air.\" [Online]. Available: http://arxiv.org/abs/1901.00844\n\nWhen edge meets learning: Adaptive control for resource-constrained distributed machine learning. S Wang, T Tuor, T Salonidis, K K Leung, C Makaya, T He, K Chan, IEEE Conf. Computer Comm., INFOCOM. Honolulu, HI, USAS. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan, \"When edge meets learning: Adaptive control for resource-constrained distributed machine learning,\" in IEEE Conf. Computer Comm., INFOCOM, pp. 63-71, Honolulu, HI, USA, Apr 16-19 2018.\n\nClient selection for federated learning with heterogeneous resources in mobile edge. T Nishio, R Yonetani, T. Nishio and R. Yonetani, \"Client selection for federated learning with heterogeneous resources in mobile edge.\" [Online].\n\nFederated learning via over-the-air computation. K Yang, T Jiang, Y Shi, Z Ding, K. Yang, T. Jiang, Y. Shi, and Z. Ding, \"Federated learning via over-the-air computation.\" [Online]. Available: http://arxiv.org/abs/1812.11750\n\nCommunication-Efficient Learning of Deep Networks from Decentralized Data. B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, Proc. of the 20th Intel. Conf. Artificial Intell. and Statistics. of the 20th Intel. Conf. Artificial Intell. and StatisticsFort Lauderdale, FL, USA54B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \"Communication-Efficient Learning of Deep Networks from Decentralized Data,\" in Proc. of the 20th Intel. Conf. Artificial Intell. and Statistics, vol. 54, pp. 1273-1282, Fort Lauderdale, FL, USA, Apr 20-22 2017.\n\nA survey and measurement study of GPU DVFS on energy conservation. X Mei, Q Wang, X Chu, Digital Comm. and Networks. 32X. Mei, Q. Wang, and X. Chu, \"A survey and measurement study of GPU DVFS on energy conservation,\" Digital Comm. and Networks, vol. 3, no. 2, pp. 89-100, 2017.\n\nsignSGD: Compressed optimisation for non-convex problems. J Bernstein, Y.-X Wang, K Azizzadenesheli, A Anandkumar, Proc. of the 35th Intl. Conf. Mach. Learning (ICML). of the 35th Intl. Conf. Mach. Learning (ICML)Stockholmsmssan, Stockholm Sweden80J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar, \"signSGD: Compressed optimisation for non-convex problems,\" in Proc. of the 35th Intl. Conf. Mach. Learning (ICML), vol. 80, pp. 560-569, Stockholmsmssan, Stockholm Sweden, Jul 10-15 2018.\n", "annotations": {"author": "[{\"end\":87,\"start\":74},{\"end\":98,\"start\":88},{\"end\":111,\"start\":99},{\"end\":125,\"start\":112}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":82},{\"end\":97,\"start\":95},{\"end\":110,\"start\":105},{\"end\":124,\"start\":119}]", "author_first_name": "[{\"end\":81,\"start\":74},{\"end\":94,\"start\":88},{\"end\":102,\"start\":99},{\"end\":104,\"start\":103},{\"end\":118,\"start\":112}]", "author_affiliation": null, "title": "[{\"end\":71,\"start\":1},{\"end\":196,\"start\":126}]", "venue": null, "abstract": "[{\"end\":1482,\"start\":199}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1673,\"start\":1670},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1947,\"start\":1944},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1952,\"start\":1949},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3612,\"start\":3609},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3617,\"start\":3614},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3679,\"start\":3676},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4219,\"start\":4216},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4525,\"start\":4522},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4874,\"start\":4871},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8404,\"start\":8401},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10157,\"start\":10154},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10526,\"start\":10523},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10531,\"start\":10528}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":21941,\"start\":21900},{\"attributes\":{\"id\":\"fig_2\"},\"end\":22445,\"start\":21942},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25358,\"start\":22446},{\"attributes\":{\"id\":\"fig_4\"},\"end\":26005,\"start\":25359}]", "paragraph": "[{\"end\":3172,\"start\":1501},{\"end\":3427,\"start\":3174},{\"end\":5210,\"start\":3429},{\"end\":5269,\"start\":5212},{\"end\":5657,\"start\":5271},{\"end\":6269,\"start\":5659},{\"end\":6821,\"start\":6271},{\"end\":6915,\"start\":6823},{\"end\":7077,\"start\":6917},{\"end\":7467,\"start\":7079},{\"end\":8057,\"start\":7469},{\"end\":8514,\"start\":8086},{\"end\":8957,\"start\":8567},{\"end\":9061,\"start\":8981},{\"end\":9308,\"start\":9093},{\"end\":9483,\"start\":9310},{\"end\":9670,\"start\":9523},{\"end\":9893,\"start\":9692},{\"end\":10479,\"start\":9963},{\"end\":10811,\"start\":10507},{\"end\":11016,\"start\":10844},{\"end\":11253,\"start\":11063},{\"end\":11468,\"start\":11299},{\"end\":11831,\"start\":11599},{\"end\":11925,\"start\":11833},{\"end\":12291,\"start\":11927},{\"end\":12382,\"start\":12293},{\"end\":12660,\"start\":12475},{\"end\":12684,\"start\":12662},{\"end\":12746,\"start\":12686},{\"end\":12839,\"start\":12748},{\"end\":12863,\"start\":12841},{\"end\":13305,\"start\":12865},{\"end\":13546,\"start\":13307},{\"end\":13910,\"start\":13591},{\"end\":14754,\"start\":14081},{\"end\":15603,\"start\":14924},{\"end\":15817,\"start\":15705},{\"end\":15919,\"start\":15819},{\"end\":16007,\"start\":15985},{\"end\":16121,\"start\":16065},{\"end\":16238,\"start\":16136},{\"end\":16333,\"start\":16240},{\"end\":16427,\"start\":16388},{\"end\":16476,\"start\":16429},{\"end\":17318,\"start\":16478},{\"end\":17970,\"start\":17443},{\"end\":18087,\"start\":17997},{\"end\":18808,\"start\":18089},{\"end\":18842,\"start\":18821},{\"end\":19030,\"start\":18844},{\"end\":19051,\"start\":19032},{\"end\":19441,\"start\":19150},{\"end\":19636,\"start\":19587},{\"end\":19790,\"start\":19700},{\"end\":19890,\"start\":19859},{\"end\":20073,\"start\":19918},{\"end\":20133,\"start\":20075},{\"end\":20258,\"start\":20240},{\"end\":20535,\"start\":20311},{\"end\":20764,\"start\":20537},{\"end\":20855,\"start\":20840},{\"end\":21139,\"start\":20877},{\"end\":21246,\"start\":21165},{\"end\":21333,\"start\":21313},{\"end\":21449,\"start\":21390},{\"end\":21583,\"start\":21495},{\"end\":21644,\"start\":21585},{\"end\":21711,\"start\":21646},{\"end\":21772,\"start\":21713},{\"end\":21806,\"start\":21774},{\"end\":21899,\"start\":21868}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8566,\"start\":8515},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8980,\"start\":8958},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9522,\"start\":9484},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9691,\"start\":9671},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9962,\"start\":9894},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10843,\"start\":10812},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11298,\"start\":11254},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11598,\"start\":11469},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12452,\"start\":12383},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12474,\"start\":12452},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14080,\"start\":13911},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14923,\"start\":14755},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15704,\"start\":15604},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15984,\"start\":15920},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16387,\"start\":16334},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17442,\"start\":17402},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19149,\"start\":19052},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19586,\"start\":19442},{\"attributes\":{\"id\":\"formula_18\"},\"end\":19699,\"start\":19637},{\"attributes\":{\"id\":\"formula_19\"},\"end\":19858,\"start\":19791},{\"attributes\":{\"id\":\"formula_20\"},\"end\":20239,\"start\":20134},{\"attributes\":{\"id\":\"formula_21\"},\"end\":20310,\"start\":20259},{\"attributes\":{\"id\":\"formula_22\"},\"end\":20839,\"start\":20765},{\"attributes\":{\"id\":\"formula_23\"},\"end\":20876,\"start\":20856},{\"attributes\":{\"id\":\"formula_24\"},\"end\":21312,\"start\":21247},{\"attributes\":{\"id\":\"formula_25\"},\"end\":21389,\"start\":21334},{\"attributes\":{\"id\":\"formula_26\"},\"end\":21494,\"start\":21450},{\"attributes\":{\"id\":\"formula_27\"},\"end\":21867,\"start\":21807}]", "table_ref": null, "section_header": "[{\"end\":1499,\"start\":1484},{\"end\":8084,\"start\":8060},{\"end\":9091,\"start\":9064},{\"end\":10505,\"start\":10482},{\"end\":11061,\"start\":11019},{\"end\":13589,\"start\":13549},{\"end\":16063,\"start\":16010},{\"end\":16134,\"start\":16124},{\"end\":17401,\"start\":17321},{\"end\":17995,\"start\":17973},{\"end\":18819,\"start\":18811},{\"end\":19916,\"start\":19893},{\"end\":21163,\"start\":21142},{\"end\":21911,\"start\":21901},{\"end\":21953,\"start\":21943},{\"end\":22457,\"start\":22447},{\"end\":25370,\"start\":25360}]", "table": null, "figure_caption": "[{\"end\":21941,\"start\":21913},{\"end\":22445,\"start\":21955},{\"end\":25358,\"start\":22459},{\"end\":26005,\"start\":25372}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7257,\"start\":7251}]", "bib_author_first_name": "[{\"end\":26084,\"start\":26083},{\"end\":26091,\"start\":26090},{\"end\":26098,\"start\":26097},{\"end\":26104,\"start\":26103},{\"end\":26111,\"start\":26110},{\"end\":26120,\"start\":26119},{\"end\":26386,\"start\":26385},{\"end\":26393,\"start\":26392},{\"end\":26401,\"start\":26400},{\"end\":26617,\"start\":26616},{\"end\":26619,\"start\":26618},{\"end\":26628,\"start\":26627},{\"end\":26912,\"start\":26911},{\"end\":26920,\"start\":26919},{\"end\":26928,\"start\":26927},{\"end\":26941,\"start\":26940},{\"end\":26943,\"start\":26942},{\"end\":26952,\"start\":26951},{\"end\":26962,\"start\":26961},{\"end\":26968,\"start\":26967},{\"end\":27376,\"start\":27375},{\"end\":27386,\"start\":27385},{\"end\":27572,\"start\":27571},{\"end\":27580,\"start\":27579},{\"end\":27589,\"start\":27588},{\"end\":27596,\"start\":27595},{\"end\":27824,\"start\":27823},{\"end\":27835,\"start\":27834},{\"end\":27844,\"start\":27843},{\"end\":27854,\"start\":27853},{\"end\":27865,\"start\":27864},{\"end\":27867,\"start\":27866},{\"end\":28370,\"start\":28369},{\"end\":28377,\"start\":28376},{\"end\":28385,\"start\":28384},{\"end\":28640,\"start\":28639},{\"end\":28656,\"start\":28652},{\"end\":28664,\"start\":28663},{\"end\":28683,\"start\":28682}]", "bib_author_last_name": "[{\"end\":26088,\"start\":26085},{\"end\":26095,\"start\":26092},{\"end\":26101,\"start\":26099},{\"end\":26108,\"start\":26105},{\"end\":26117,\"start\":26112},{\"end\":26126,\"start\":26121},{\"end\":26390,\"start\":26387},{\"end\":26398,\"start\":26394},{\"end\":26407,\"start\":26402},{\"end\":26625,\"start\":26620},{\"end\":26635,\"start\":26629},{\"end\":26917,\"start\":26913},{\"end\":26925,\"start\":26921},{\"end\":26938,\"start\":26929},{\"end\":26949,\"start\":26944},{\"end\":26959,\"start\":26953},{\"end\":26965,\"start\":26963},{\"end\":26973,\"start\":26969},{\"end\":27383,\"start\":27377},{\"end\":27395,\"start\":27387},{\"end\":27577,\"start\":27573},{\"end\":27586,\"start\":27581},{\"end\":27593,\"start\":27590},{\"end\":27601,\"start\":27597},{\"end\":27832,\"start\":27825},{\"end\":27841,\"start\":27836},{\"end\":27851,\"start\":27845},{\"end\":27862,\"start\":27855},{\"end\":27873,\"start\":27868},{\"end\":28374,\"start\":28371},{\"end\":28382,\"start\":28378},{\"end\":28389,\"start\":28386},{\"end\":28650,\"start\":28641},{\"end\":28661,\"start\":28657},{\"end\":28680,\"start\":28665},{\"end\":28694,\"start\":28684}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":26313,\"start\":26007},{\"attributes\":{\"id\":\"b1\"},\"end\":26521,\"start\":26315},{\"attributes\":{\"id\":\"b2\"},\"end\":26811,\"start\":26523},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4883014},\"end\":27288,\"start\":26813},{\"attributes\":{\"id\":\"b4\"},\"end\":27520,\"start\":27290},{\"attributes\":{\"id\":\"b5\"},\"end\":27746,\"start\":27522},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14955348},\"end\":28300,\"start\":27748},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7353906},\"end\":28579,\"start\":28302},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7763588},\"end\":29082,\"start\":28581}]", "bib_title": "[{\"end\":26909,\"start\":26813},{\"end\":27821,\"start\":27748},{\"end\":28367,\"start\":28302},{\"end\":28637,\"start\":28581}]", "bib_author": "[{\"end\":26090,\"start\":26083},{\"end\":26097,\"start\":26090},{\"end\":26103,\"start\":26097},{\"end\":26110,\"start\":26103},{\"end\":26119,\"start\":26110},{\"end\":26128,\"start\":26119},{\"end\":26392,\"start\":26385},{\"end\":26400,\"start\":26392},{\"end\":26409,\"start\":26400},{\"end\":26627,\"start\":26616},{\"end\":26637,\"start\":26627},{\"end\":26919,\"start\":26911},{\"end\":26927,\"start\":26919},{\"end\":26940,\"start\":26927},{\"end\":26951,\"start\":26940},{\"end\":26961,\"start\":26951},{\"end\":26967,\"start\":26961},{\"end\":26975,\"start\":26967},{\"end\":27385,\"start\":27375},{\"end\":27397,\"start\":27385},{\"end\":27579,\"start\":27571},{\"end\":27588,\"start\":27579},{\"end\":27595,\"start\":27588},{\"end\":27603,\"start\":27595},{\"end\":27834,\"start\":27823},{\"end\":27843,\"start\":27834},{\"end\":27853,\"start\":27843},{\"end\":27864,\"start\":27853},{\"end\":27875,\"start\":27864},{\"end\":28376,\"start\":28369},{\"end\":28384,\"start\":28376},{\"end\":28391,\"start\":28384},{\"end\":28652,\"start\":28639},{\"end\":28663,\"start\":28652},{\"end\":28682,\"start\":28663},{\"end\":28696,\"start\":28682}]", "bib_venue": "[{\"end\":26081,\"start\":26007},{\"end\":26383,\"start\":26315},{\"end\":26614,\"start\":26523},{\"end\":27009,\"start\":26975},{\"end\":27373,\"start\":27290},{\"end\":27569,\"start\":27522},{\"end\":27939,\"start\":27875},{\"end\":28417,\"start\":28391},{\"end\":28747,\"start\":28696},{\"end\":27028,\"start\":27011},{\"end\":28023,\"start\":27941},{\"end\":28827,\"start\":28749}]"}}}, "year": 2023, "month": 12, "day": 17}