{"id": 218862749, "updated": "2023-10-06 15:11:51.123", "metadata": {"title": "Bootstrapping Named Entity Recognition in E-Commerce with Positive Unlabeled Learning", "authors": "[{\"first\":\"Hanchu\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Leonhard\",\"last\":\"Hennig\",\"middle\":[]},{\"first\":\"Christoph\",\"last\":\"Alt\",\"middle\":[]},{\"first\":\"Changjian\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Yao\",\"last\":\"Meng\",\"middle\":[]},{\"first\":\"Chao\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ECNLP", "journal": "Proceedings of The 3rd Workshop on e-Commerce and NLP", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "In this work, we introduce a bootstrapped, iterative NER model that integrates a PU learning algorithm for recognizing named entities in a low-resource setting. Our approach combines dictionary-based labeling with syntactically-informed label expansion to efficiently enrich the seed dictionaries. Experimental results on a dataset of manually annotated e-commerce product descriptions demonstrate the effectiveness of the proposed framework.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.11075", "mag": "3046143285", "acl": "2020.ecnlp-1.1", "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2005-11075", "doi": "10.18653/v1/2020.ecnlp-1.1"}}, "content": {"source": {"pdf_hash": "5912c48d43ff76f1ec382cc3b9bbd2d04c43338a", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2020.ecnlp-1.1.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.ecnlp-1.1.pdf", "status": "HYBRID"}}, "grobid": {"id": "6c2dba67eb3acadf2976b0387cce4d666f1fdc58", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5912c48d43ff76f1ec382cc3b9bbd2d04c43338a.txt", "contents": "\nBootstrapping Named Entity Recognition in E-Commerce with Positive Unlabeled Learning\nJuly 10, 2020\n\nHanchu Zhang zhanghc9@lenovo.com \nLenovo Research Artificial Intelligence Lab\n\n\nLeonhard Hennig leonhard.hennig@dfki.de \nGerman Research Center for Artificial Intelligence (DFKI)\n\n\nChristoph Alt christoph.alt@dfki.de \nGerman Research Center for Artificial Intelligence (DFKI)\n\n\nChangjian Hu \nLenovo Research Artificial Intelligence Lab\n\n\nYao Meng mengyao1@lenovo.com \nLenovo Research Artificial Intelligence Lab\n\n\nChao Wang wangchao31@lenovo.com \nLenovo Research Artificial Intelligence Lab\n\n\nBootstrapping Named Entity Recognition in E-Commerce with Positive Unlabeled Learning\n\nOnline\nthe 3rd Workshop on e-Commerce and NLP (ECNLP 3)July 10, 20201\nNamed Entity Recognition (NER) in domains like e-commerce is an understudied problem due to the lack of annotated datasets. Recognizing novel entity types in this domain, such as products, components, and attributes, is challenging because of their linguistic complexity and the low coverage of existing knowledge resources. To address this problem, we present a bootstrapped positiveunlabeled learning algorithm that integrates domain-specific linguistic features to quickly and efficiently expand the seed dictionary. The model achieves an average F1 score of 72.02% on a novel dataset of product descriptions, an improvement of 3.63% over a baseline BiL-STM classifier, and in particular exhibits better recall (4.96% on average).\n\nIntroduction\n\nThe vast majority of existing named entity recognition (NER) methods focus on a small set of prominent entity types, such as persons, organizations, diseases, and genes, for which labeled datasets are readily available (Tjong Kim Sang and De Meulder, 2003;Smith et al., 2008;Weischedel et al., 2011;Li et al., 2016). There is a marked lack of studies in many other domains, such as e-commerce, and for novel entity types, e.g. products and components.\n\nThe lack of annotated datasets in the ecommerce domain makes it hard to apply supervised NER methods. An alternative approach is to use dictionaries (Nadeau et al., 2006;Yang et al., 2018), but freely available knowledge resources, e.g. Wikidata (Vrande\u010dic and Kr\u00f6tzsch, 2014) or YAGO (Suchanek et al., 2007), contain only very limited information about e-commerce entities. Manually creating a dictionary of sufficient quality and coverage would be prohibitively expensive. This is amplified by the fact that in the e-commerce domain, entities are frequently ex-pressed as complex noun phrases instead of proper names. Product and component category terms are often combined with brand names, model numbers, and attributes (\"hard drive\" \u2192 \"SSD hard drive\" \u2192 \"WD Blue 500 GB SSD hard drive\"), which are almost impossible to enumerate exhaustively. In such a low-coverage setting, employing a simple dictionary-based approach would result in very low recall, and yield very noisy labels when used as a source of labels for a supervised machine learning algorithm. To address the drawbacks of dictionary-based labeling, Peng et al. (2019) propose a positive-unlabeled (PU) NER approach that labels positive instances using a seed dictionary, but makes no label assumptions for the remaining tokens (Bekker and Davis, 2018). The authors validate their approach on the CoNLL, MUC and Twitter datasets for standard entity types, but it is unclear how their approach transfers to the ecommerce domain and its entity types.\n\nContributions We adopt the PU algorithm of Peng et al. (2019) to the domain of consumer electronic product descriptions, and evaluate its effectiveness on four entity types: Product, Component, Brand and Attribute. Our algorithm bootstraps NER with a seed dictionary, iteratively labels more data and expands the dictionary, while accounting for accumulated errors from model predictions. During labeling, we utilize dependency parsing to efficiently expand dictionary matches in text. Our experiments on a novel dataset of product descriptions show that this labeling mechanism, combined with a PU learning strategy, consistently improves F1 scores over a standard BiLSTM classifier. Iterative learning quickly expands the dictionary, and further improves model performance. The proposed approach exhibits much better recall than the baseline model, and generalizes better to unseen entities.\n\nAlgorithm 1: Iterative Bootstrapping NER Input: Dictionary D seed , Corpus C, threshold K, max iterations I Result: Dictionary D + , Classifier L D + \u2190 D seed ; C dep \u2190 dependency parse(C); i \u2190 0; while not converged(D + ) and i < I do C lab \u2190 label(C, D + ); C exp \u2190 expand labels(C lab , C dep ); L \u2190 train classif ier(C exp ); C pred \u2190 predict(C exp , L); for e \u2190 C pred do if e / \u2208 D + and freq(e) > K then D + \u2190 add entity(D + , e); end end i \u2190 i + 1; end\n\n\nNER with Positive Unlabeled Learning\n\nIn this section, we first describe the iterative bootstrapping process, followed by our approach to positive unlabeled learning for NER (PU-NER).\n\n\nIterative Bootstrapping\n\nThe goal of iterative bootstrapping is to successively expand a seed dictionary of entities to label an existing training dataset, improving the quality and coverage of labels in each iteration (see Algorithm 1). In the first step, we use the seed dictionary to assign initial labels to each token. We then utilize the dependency parses of sentences to label tokens in a \"compound\" relation with already labeled tokens (see Figure 1). In the example \"hard drive\" is labeled a Component based on the initial seed dictionary, and according to its dependency parse it has a \"compound\" relation with \"dock\", which is therefore also labeled as a Component. We employ an IO label scheme, because dictionary entries are often more generic than the specific matches in text (see the previous example), which would lead to erroneous tags with schemes such as BIO.\n\nIn the second step, we train a NER model on the training dataset with new labels assigned. We repeat these steps at most I times, and in each subsequent iteration we use the trained model to predict new token-level labels on the training data. Novel entities predicted more than K times are included in the dictionary for the next labeling step. The  threshold K ensures that we do not introduce noise in the dictionary with spurious positively labeled entities.\n\n\nPU-NER Model\n\nAs shown in Figure 2, our model first uses BERT (Devlin et al., 2018) to encode the sub-word tokenized input text into a sequence of contextualized token representations {z 1 , ..., z L }, followed by a bidirectional LSTM (Lample et al., 2016) layer to model further interactions between tokens. Similar to Devlin et al. (2018), we treat NER as a tokenlevel classification task, without using a CRF to model dependencies between entity labels. We use the vector associated with the first sub-word token in each word as the input to the entity classifier, which consists of a feedforward neural network with a single projection layer. We use back propagation to update the training parameters of the Bi-LSTM and the final classifier, without fine-tuning the entire BERT model.\n\nDictionary-based labeling achieves high precision on the matched entities but low recall. This fits the positive unlabeled setting (Elkan and Noto, 2008), which assumes that a learner only has access to positive examples and unlabeled data. Thus, we consider all tokens matched by the dictionary as positive, and consider all other tokens to be unlabeled. The goal of PU learning is then to estimate the true risk regarding the expected number of positive examples remaining in the unlabeled data. We define the empirical risk asR l = 1 n n i l(\u0177 i , y i ) and assume the class prior to be equal to real distribution of examples in the data \u03c0 p = P (Y = 1), and \u03c0 n = P (Y = 0). As the model tends to predict the positive labels correctly during training, i.e. l(\u0177 i p , 1) declines to a small value. We follow Peng et al. (2019) and combine risk estimation with a non-negative constraint:\nR l = 1 n p np i l(\u0177 i p , 1) + max 0, 1 n u nu i l(\u0177 i u , 0) \u2212 \u03c0 p n p np i l(\u0177 i p , 0)\n3 Dataset E-commerce covers a wide range of complex entity types. In this work, we focus on electronic products, e.g. personal computers, mobile phones, and related hardware, and define the following entity types: Products, i.e. electronic consumer devices such as mobiles, laptops, and PCs. Products may be preceded by a brand and include some form of model, year, or version specification, e.g. \"Galaxy S8\" or \"Dell Latitude 6400 multimedia notebook\".\n\nComponents are parts of a product, typically with a physical aspect, e.g. \"battery\", or \"multimedia keyboard\". 1 Brand refers to producers of a product or component, e.g. \"Samsung\", or \"Dell\". Attributes are units associated with components, e.g. size (\"4 TB\"), or weight (\"3 kg\"). To create our evaluation dataset, we use the Amazon review dataset (McAuley et al., 2015), 2 a collection of product metadata and customer reviews from Amazon. The metadata includes product title, a descriptive text, category information, price, brand, and image features. We use only entries in the Electronics/Computers subcategory and randomly sample product descriptions of length 500-1000 characters, yielding a dataset of 24,272 training documents. We randomly select another 100 product descriptions to form the final test set. These are manually annotated by 2 trained linguists, with disagreements resolved by a third expert annotator. Token-level inter-annotator agreement was 1 Non-physical product features and software, such as \"Toshiba Face Recognition Software\", or \"Windows 7\" are not considered as components. \n\n\nExperiments\n\nTo evaluate our proposed model (PU), we compare it against two baselines: (1) dictionary-only labeling (Dictionary), and (2) our model with standard cross-entropy loss instead of the PU learning risk (BiLSTM). The BiLSTM model is trained in a supervised fashion, treating all non-dictionary entries as negative tokens. The BiLSTM and PU models were implemented using AllenNLP (Gardner et al., 2018). We use SpaCy 3 for preprocessing, dependency parsing, and dictionary-based entity labeling. We manually define seed dictionaries for Product (6 entries), Component (60 entries) and Brand (13 entries). For Attributes, we define a set of 8 regular expressions to pre-label the dataset. Following previous works, we evaluate model performance using token-level F1 score. There are two options to estimate the value of the class prior \u03c0 p . One approach is to treat \u03c0 p as a hyperparameter which is fixed during training. Another option is suggested in Peng et al. (2019), who specify an initial value for \u03c0 p to start bootstrapping, but recalculate \u03c0 p after several train-relabel steps based on the predicted entity type distribution. In our work, we treated \u03c0 p as a fixed hyperparameter with a value of \u03c0 p = 0.01.\n\nWe run our bootstrapping approach for I = 10 iterations, and report the F1 score of the best iteration. Table 1 shows the F1 scores of several model ablations by entity type on our test dataset. From the table, we can observe: 1) The PU algorithm outperforms the simpler models for most classes, which demonstrates the effectiveness of the PU learning framework for NER in our domain. 2) Dependency parsing is a very effective feature for Component and Product, and it strongly improves the overall F1 score. 3) The iterative training strategy yields a significant improvement for most classes. Even after several iterations, it still finds new entries to expand the dictionaries (Figure 3).\n\n\nResults and Discussion\n\nThe Dictionary approach shows poor performance on average, which is due to the low recall  Table 1: Token-Level F1 scores on the test set. The unmodified PU algorithm achieves an average F1 score of 68.95%. Integrating dependency parsing (Dep) and iterative relabeling (Iter) raises the F1 score to 72.02%, an improvement of 42.08% over a dictionary-only approach, and 3.63% over a BiLSTM baseline.\n\ncaused by very limited entities in the dictionary. PU greatly outperforms the dictionary approach, and has an edge in F1 score over the BiLSTM model. The advantages of PU gradually accumulate with each iteration. For Product, the combination of PU learning, dependency parsing-based labeling, and iterative bootstrapping, yields a 7% improvement in F1 score, for Component, it is still 5%. PU Learning Performance Figure 3 shows that the PU algorithm especially improves recall over the baseline classifier for Components, Products and Brands. With each iteration step, the PU model is increasingly better able to predict unseen entities, and achieves higher recall scores than the BiLSTM model. While the baseline curve on Brands stays almost flat during iterations, PU consistently improves recall as new entities are added into dictionary. For Attributes, however, both models exhibit about the same level of recall, which in addition is largely unaffected by the number of iterations. This suggests that PU learning better estimates the true loss in the model. In a fully supervised setting, a standard classification loss function can accurately describe the loss on positive and negative samples. However, in the positive unlabeled setting, many unlabeled samples may actually be positive, and therefore the computed loss should not strongly push the model towards the negative class. We therefore want to quantify how much the loss is overestimated due to false negative samples, so that we can appropriately reduce this loss using the estimated real class distribution.\n\nError Analysis Both PU and the baseline model in some cases have difficulties predicting Attributes correctly. This can be due to spelling differences between train and test data (e.g. \"8 Mhz\" vs \"8Mhz\"), but also because of unclean texts in the source documents. Another source of errors is the fixed word piece vocabulary of the pre-trained BERT model, which often splits unit terms such as \"Mhz\" into several word pieces. Since we use only the first word piece of a token for prediction, this means that signals important for prediction of the Attribute class may get lost. This suggests that for technical domains with very specific vocabulary, tokenization is important to allow the model to better represent the meaning of each word piece.\n\n\nRelated work\n\nRecent work in positive-unlabeled learning in the area of NLP includes deceptive review detection (Ren et al., 2014), keyphrase extraction (Sterckx et al., 2016) and fact check-worthiness detection (Wright and Augenstein, 2020), see also (Bekker and Davis, 2018) for a survey. Our approach extends the work of Peng et al. (2019) in a novel domain and for challenging entity types. In the area of NER for e-commerce, Putthividhya and Hu (2011) present an approach to extract product attributes and values from product listing titles. Zheng et al. (2018) formulate missing attribute value extraction as a sequence tagging problem, and present a BiLSTM-CRF model with attention. Pazhouhi (2018) studies the problem of product name recognition, but uses a fully supervised approach. In contrast, our method is semi-supervised and uses only very few seed labels.\n\n\nConclusion\n\nIn this work, we introduce a bootstrapped, iterative NER model that integrates a PU learning algorithm for recognizing named entities in a low-resource setting. Our approach combines dictionary-based labeling with syntactically-informed label expansion to efficiently enrich the seed dictionaries. Experimental results on a dataset of manually annotated e-commerce product descriptions demonstrate the effectiveness of the proposed framework.\n\nFigure 1 :\n1Red check marks indicate tokens labeled by the dictionary, black those based on label expansion using dependency information. The green box shows the true extent of the multi-token Component entity.\n\nFigure 2 :\n2Architecture of the positive unlabeled NER (PU-NER) model.\n\nFigure 3 :\n3Recall curves of the BiLSTM+Dep and PU+Dep model for Component, Product, Brand, and Attribute. PU+Dep boosts recall by 3.03% on average, with a max average difference of 4.96% after 5 iterations.\nhttps://spacy.io/\nAcknowledgmentsWe would like to thank the reviewers for their valuable comments and feedback. Christoph Alt and Leonhard Hennig have been partially supported by the German Federal Ministry for Economic Affairs and Energy as part of the project PLASS (01MD19003E).\nLearning from positive and unlabeled data: A survey. Jessa Bekker, Jesse Davis, abs/1811.04820Jessa Bekker and Jesse Davis. 2018. Learning from positive and unlabeled data: A survey. CoRR, abs/1811.04820.\n\nBERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, abs/1810.04805CoRRJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing. CoRR, abs/1810.04805.\n\nLearning classifiers from only positive and unlabeled data. Charles Elkan, Keith Noto, Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. the 14th ACM SIGKDD international conference on Knowledge discovery and data miningCharles Elkan and Keith Noto. 2008. Learning classi- fiers from only positive and unlabeled data. In Pro- ceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data min- ing, pages 213-220.\n\nAllenNLP: A deep semantic natural language processing platform. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F Liu, Matthew Peters, Michael Schmitz, Luke Zettlemoyer, 10.18653/v1/W18-2501Proceedings of Workshop for NLP Open Source Software (NLP-OSS). Workshop for NLP Open Source Software (NLP-OSS)Melbourne, AustraliaAssociation for Computational LinguisticsMatt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2018. AllenNLP: A deep semantic natural language pro- cessing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 1- 6, Melbourne, Australia. Association for Computa- tional Linguistics.\n\nNeural architectures for named entity recognition. Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer, 10.18653/v1/N16-1030Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsGuillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 260-270, San Diego, California. Association for Computational Linguistics.\n\nBiocreative v cdr task corpus: a resource for chemical disease relation extraction. Database : the journal of biological databases and curation. Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, Zhiyong Lu, Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci- aky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. 2016. Biocreative v cdr task cor- pus: a resource for chemical disease relation extrac- tion. Database : the journal of biological databases and curation, 2016.\n\nQinfeng Shi, and Anton van den Hengel. Julian J Mcauley, Christopher Targett, Image-based recommendations on styles and substitutes. Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-based rec- ommendations on styles and substitutes. Proceed- ings of the 38th International ACM SIGIR Confer- ence on Research and Development in Information Retrieval.\n\nUnsupervised named-entity recognition: Generating gazetteers and resolving ambiguity. David Nadeau, Peter D Turney, Stan Matwin, Advances in Artificial Intelligence. Berlin, Heidelberg; Berlin HeidelbergSpringerDavid Nadeau, Peter D. Turney, and Stan Matwin. 2006. Unsupervised named-entity recognition: Gen- erating gazetteers and resolving ambiguity. In Ad- vances in Artificial Intelligence, pages 266-277, Berlin, Heidelberg. Springer Berlin Heidelberg.\n\nAutomatic product name recognition from short product descriptions. Elnaz Pazhouhi, University of TwenteMaster's thesisElnaz Pazhouhi. 2018. Automatic product name recog- nition from short product descriptions. Master's the- sis, University of Twente.\n\nDistantly supervised named entity recognition using positive-unlabeled learning. Minlong Peng, Xiaoyu Xing, Qi Zhang, Jinlan Fu, Xuanjing Huang, 10.18653/v1/P19-1231Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsMinlong Peng, Xiaoyu Xing, Qi Zhang, Jinlan Fu, and Xuanjing Huang. 2019. Distantly supervised named entity recognition using positive-unlabeled learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2409-2419, Florence, Italy. Association for Compu- tational Linguistics.\n\nBootstrapped named entity recognition for product attribute extraction. Duangmanee Putthividhya, Junling Hu, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingEdinburgh, Scotland, UKAssociation for Computational LinguisticsDuangmanee Putthividhya and Junling Hu. 2011. Boot- strapped named entity recognition for product at- tribute extraction. In Proceedings of the 2011 Con- ference on Empirical Methods in Natural Language Processing, pages 1557-1567, Edinburgh, Scotland, UK. Association for Computational Linguistics.\n\nPositive unlabeled learning for deceptive reviews detection. Yafeng Ren, Donghong Ji, Hongbin Zhang, 10.3115/v1/D14-1055Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsYafeng Ren, Donghong Ji, and Hongbin Zhang. 2014. Positive unlabeled learning for deceptive reviews de- tection. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 488-498, Doha, Qatar. Association for Computational Linguistics.\n\nOverview of biocreative ii gene mention recognition. Larry Smith, Lorraine K Tanabe, Rie Johnson Nee Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger, Christoph M Friedrich, Kuzman Ganchev, Manabu Torii, Hongfang Liu, Barry Haddow, Craig A Struble, Richard J Povinelli, J Manuel, Jacinto Ma\u00f1a-L\u00f3pez, W. John Mata, Wilbur, Genome Biology. Andreas Vlachos, William A. Baumgartner, Lawrence E. Hunter, Bob Carpenter, Richard Tzong-Han Tsai, Hong-Jie Dai, Feng Liu, Yifei Chen, Chengjie Sun, Sophia Katrenko, Pieter Adriaans, Christian Blaschke, Rafael Torres9Larry Smith, Lorraine K. Tanabe, Rie Johnson nee Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger, Christoph M. Friedrich, Kuzman Ganchev, Manabu Torii, Hong- fang Liu, Barry Haddow, Craig A. Struble, Richard J. Povinelli, Andreas Vlachos, William A. Baumgart- ner, Lawrence E. Hunter, Bob Carpenter, Richard Tzong-Han Tsai, Hong-Jie Dai, Feng Liu, Yifei Chen, Chengjie Sun, Sophia Katrenko, Pieter Adri- aans, Christian Blaschke, Rafael Torres, Mariana Neves, Preslav Nakov, Anna Divoli, Manuel J. Ma\u00f1a-L\u00f3pez, Jacinto Mata, and W. John Wilbur. 2008. Overview of biocreative ii gene mention recognition. Genome Biology, 9:S2 -S2.\n\nSupervised keyphrase extraction as positive unlabeled learning. Lucas Sterckx, Cornelia Caragea, Thomas Demeester, Chris Develder, 10.18653/v1/D16-1198Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsLucas Sterckx, Cornelia Caragea, Thomas Demeester, and Chris Develder. 2016. Supervised keyphrase ex- traction as positive unlabeled learning. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1924-1929, Austin, Texas. Association for Computational Lin- guistics.\n\nYago: A core of semantic knowledge. Fabian M Suchanek, Gjergji Kasneci, Gerhard Weikum, 10.1145/1242572.1242667Proceedings of the 16th International Conference on World Wide Web, WWW '07. the 16th International Conference on World Wide Web, WWW '07New York, NY, USAAssociation for Computing MachineryFabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A core of semantic knowl- edge. In Proceedings of the 16th International Conference on World Wide Web, WWW '07, page 697-706, New York, NY, USA. Association for Computing Machinery.\n\nIntroduction to the conll-2003 shared task: Language-independent named entity recognition. Erik F Tjong, Kim Sang, Fien De Meulder, Proceedings of CoNLL-2003. CoNLL-2003Edmonton, CanadaErik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2003, pages 142-147. Ed- monton, Canada.\n\nWikidata: A free collaborative knowledgebase. Denny Vrande\u010dic, Markus Kr\u00f6tzsch, 10.1145/2629489Commun. ACM. 5710Denny Vrande\u010dic and Markus Kr\u00f6tzsch. 2014. Wiki- data: A free collaborative knowledgebase. Commun. ACM, 57(10):78-85.\n\nSameer Pradhan, Lance Ramshaw, and Nianwen Xue. Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha Palmer, Robert Belvin, OntoNotes: A Large Training Corpus for Enhanced Processing. Ralph Weischedel, Eduard Hovy, Mitchell Mar- cus, Martha Palmer, Robert Belvin, Sameer Prad- han, Lance Ramshaw, and Nianwen Xue. 2011. OntoNotes: A Large Training Corpus for Enhanced Processing.\n\nFact check-worthiness detection as positive unlabelled learning. ArXiv, abs. Dustin Wright, Isabelle Augenstein, Dustin Wright and Isabelle Augenstein. 2020. Fact check-worthiness detection as positive unlabelled learning. ArXiv, abs/2003.02736.\n\nDistantly supervised NER with partial annotation learning and reinforcement learning. Yaosheng Yang, Wenliang Chen, Zhenghua Li, Zhengqiu He, Min Zhang, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational LinguisticsYaosheng Yang, Wenliang Chen, Zhenghua Li, Zhengqiu He, and Min Zhang. 2018. Distantly su- pervised NER with partial annotation learning and reinforcement learning. In Proceedings of the 27th International Conference on Computational Linguis- tics, pages 2159-2169, Santa Fe, New Mexico, USA. Association for Computational Linguistics.\n\nOpentag: Open attribute value extraction from product profiles. Guineng Zheng, Subhabrata Mukherjee, Xin Dong, Feifei Li, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningGuineng Zheng, Subhabrata Mukherjee, Xin Dong, and Feifei Li. 2018. Opentag: Open attribute value extraction from product profiles. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.\n", "annotations": {"author": "[{\"end\":181,\"start\":102},{\"end\":282,\"start\":182},{\"end\":379,\"start\":283},{\"end\":439,\"start\":380},{\"end\":515,\"start\":440},{\"end\":594,\"start\":516}]", "publisher": null, "author_last_name": "[{\"end\":114,\"start\":109},{\"end\":197,\"start\":191},{\"end\":296,\"start\":293},{\"end\":392,\"start\":390},{\"end\":448,\"start\":444},{\"end\":525,\"start\":521}]", "author_first_name": "[{\"end\":108,\"start\":102},{\"end\":190,\"start\":182},{\"end\":292,\"start\":283},{\"end\":389,\"start\":380},{\"end\":443,\"start\":440},{\"end\":520,\"start\":516}]", "author_affiliation": "[{\"end\":180,\"start\":136},{\"end\":281,\"start\":223},{\"end\":378,\"start\":320},{\"end\":438,\"start\":394},{\"end\":514,\"start\":470},{\"end\":593,\"start\":549}]", "title": "[{\"end\":86,\"start\":1},{\"end\":680,\"start\":595}]", "venue": "[{\"end\":688,\"start\":682}]", "abstract": "[{\"end\":1485,\"start\":752}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1757,\"start\":1720},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1776,\"start\":1757},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1800,\"start\":1776},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1816,\"start\":1800},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2124,\"start\":2103},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2142,\"start\":2124},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2230,\"start\":2200},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2262,\"start\":2239},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3090,\"start\":3072},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3274,\"start\":3250},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3533,\"start\":3515},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6445,\"start\":6424},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6619,\"start\":6598},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6703,\"start\":6683},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7306,\"start\":7284},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7982,\"start\":7964},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8960,\"start\":8938},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10112,\"start\":10090},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10681,\"start\":10663},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14505,\"start\":14487},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14550,\"start\":14528},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14616,\"start\":14587},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14651,\"start\":14627},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14717,\"start\":14699},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14831,\"start\":14805},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14941,\"start\":14922},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15080,\"start\":15065}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":15915,\"start\":15704},{\"attributes\":{\"id\":\"fig_1\"},\"end\":15987,\"start\":15916},{\"attributes\":{\"id\":\"fig_2\"},\"end\":16196,\"start\":15988}]", "paragraph": "[{\"end\":1952,\"start\":1501},{\"end\":3470,\"start\":1954},{\"end\":4365,\"start\":3472},{\"end\":4827,\"start\":4367},{\"end\":5013,\"start\":4868},{\"end\":5895,\"start\":5041},{\"end\":6359,\"start\":5897},{\"end\":7151,\"start\":6376},{\"end\":8042,\"start\":7153},{\"end\":8587,\"start\":8134},{\"end\":9698,\"start\":8589},{\"end\":10928,\"start\":9714},{\"end\":11621,\"start\":10930},{\"end\":12046,\"start\":11648},{\"end\":13625,\"start\":12048},{\"end\":14372,\"start\":13627},{\"end\":15246,\"start\":14389},{\"end\":15703,\"start\":15261}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8133,\"start\":8043}]", "table_ref": "[{\"end\":11041,\"start\":11034},{\"end\":11746,\"start\":11739}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1499,\"start\":1487},{\"attributes\":{\"n\":\"2\"},\"end\":4866,\"start\":4830},{\"attributes\":{\"n\":\"2.1\"},\"end\":5039,\"start\":5016},{\"attributes\":{\"n\":\"2.2\"},\"end\":6374,\"start\":6362},{\"attributes\":{\"n\":\"4\"},\"end\":9712,\"start\":9701},{\"attributes\":{\"n\":\"4.1\"},\"end\":11646,\"start\":11624},{\"attributes\":{\"n\":\"5\"},\"end\":14387,\"start\":14375},{\"attributes\":{\"n\":\"6\"},\"end\":15259,\"start\":15249},{\"end\":15715,\"start\":15705},{\"end\":15927,\"start\":15917},{\"end\":15999,\"start\":15989}]", "table": null, "figure_caption": "[{\"end\":15915,\"start\":15717},{\"end\":15987,\"start\":15929},{\"end\":16196,\"start\":16001}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5473,\"start\":5465},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6396,\"start\":6388},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11619,\"start\":11610},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12470,\"start\":12462}]", "bib_author_first_name": "[{\"end\":16537,\"start\":16532},{\"end\":16551,\"start\":16546},{\"end\":16772,\"start\":16767},{\"end\":16789,\"start\":16781},{\"end\":16803,\"start\":16797},{\"end\":16817,\"start\":16809},{\"end\":17093,\"start\":17086},{\"end\":17106,\"start\":17101},{\"end\":17585,\"start\":17581},{\"end\":17599,\"start\":17595},{\"end\":17610,\"start\":17606},{\"end\":17626,\"start\":17620},{\"end\":17643,\"start\":17636},{\"end\":17658,\"start\":17652},{\"end\":17660,\"start\":17659},{\"end\":17673,\"start\":17666},{\"end\":17689,\"start\":17682},{\"end\":17703,\"start\":17699},{\"end\":18331,\"start\":18322},{\"end\":18346,\"start\":18340},{\"end\":18367,\"start\":18360},{\"end\":18387,\"start\":18381},{\"end\":18403,\"start\":18398},{\"end\":19294,\"start\":19290},{\"end\":19306,\"start\":19299},{\"end\":19317,\"start\":19312},{\"end\":19319,\"start\":19318},{\"end\":19336,\"start\":19329},{\"end\":19355,\"start\":19345},{\"end\":19367,\"start\":19361},{\"end\":19381,\"start\":19376},{\"end\":19387,\"start\":19382},{\"end\":19402,\"start\":19395},{\"end\":19404,\"start\":19403},{\"end\":19422,\"start\":19416},{\"end\":19424,\"start\":19423},{\"end\":19441,\"start\":19434},{\"end\":19818,\"start\":19812},{\"end\":19820,\"start\":19819},{\"end\":19841,\"start\":19830},{\"end\":20370,\"start\":20365},{\"end\":20384,\"start\":20379},{\"end\":20386,\"start\":20385},{\"end\":20399,\"start\":20395},{\"end\":20811,\"start\":20806},{\"end\":21079,\"start\":21072},{\"end\":21092,\"start\":21086},{\"end\":21101,\"start\":21099},{\"end\":21115,\"start\":21109},{\"end\":21128,\"start\":21120},{\"end\":21782,\"start\":21772},{\"end\":21804,\"start\":21797},{\"end\":22400,\"start\":22394},{\"end\":22414,\"start\":22406},{\"end\":22426,\"start\":22419},{\"end\":23022,\"start\":23017},{\"end\":23038,\"start\":23030},{\"end\":23040,\"start\":23039},{\"end\":23052,\"start\":23049},{\"end\":23079,\"start\":23071},{\"end\":23091,\"start\":23085},{\"end\":23107,\"start\":23099},{\"end\":23119,\"start\":23113},{\"end\":23130,\"start\":23125},{\"end\":23149,\"start\":23140},{\"end\":23151,\"start\":23150},{\"end\":23169,\"start\":23163},{\"end\":23185,\"start\":23179},{\"end\":23201,\"start\":23193},{\"end\":23212,\"start\":23207},{\"end\":23226,\"start\":23221},{\"end\":23228,\"start\":23227},{\"end\":23245,\"start\":23238},{\"end\":23247,\"start\":23246},{\"end\":23260,\"start\":23259},{\"end\":23276,\"start\":23269},{\"end\":23296,\"start\":23289},{\"end\":24270,\"start\":24265},{\"end\":24288,\"start\":24280},{\"end\":24304,\"start\":24298},{\"end\":24321,\"start\":24316},{\"end\":24921,\"start\":24915},{\"end\":24923,\"start\":24922},{\"end\":24941,\"start\":24934},{\"end\":24958,\"start\":24951},{\"end\":25526,\"start\":25522},{\"end\":25528,\"start\":25527},{\"end\":25539,\"start\":25536},{\"end\":25550,\"start\":25546},{\"end\":25874,\"start\":25869},{\"end\":25892,\"start\":25886},{\"end\":26107,\"start\":26102},{\"end\":26126,\"start\":26120},{\"end\":26141,\"start\":26133},{\"end\":26156,\"start\":26150},{\"end\":26171,\"start\":26165},{\"end\":26520,\"start\":26514},{\"end\":26537,\"start\":26529},{\"end\":26778,\"start\":26770},{\"end\":26793,\"start\":26785},{\"end\":26808,\"start\":26800},{\"end\":26821,\"start\":26813},{\"end\":26829,\"start\":26826},{\"end\":27452,\"start\":27445},{\"end\":27470,\"start\":27460},{\"end\":27485,\"start\":27482},{\"end\":27498,\"start\":27492}]", "bib_author_last_name": "[{\"end\":16544,\"start\":16538},{\"end\":16557,\"start\":16552},{\"end\":16779,\"start\":16773},{\"end\":16795,\"start\":16790},{\"end\":16807,\"start\":16804},{\"end\":16827,\"start\":16818},{\"end\":17099,\"start\":17094},{\"end\":17111,\"start\":17107},{\"end\":17593,\"start\":17586},{\"end\":17604,\"start\":17600},{\"end\":17618,\"start\":17611},{\"end\":17634,\"start\":17627},{\"end\":17650,\"start\":17644},{\"end\":17664,\"start\":17661},{\"end\":17680,\"start\":17674},{\"end\":17697,\"start\":17690},{\"end\":17715,\"start\":17704},{\"end\":18338,\"start\":18332},{\"end\":18358,\"start\":18347},{\"end\":18379,\"start\":18368},{\"end\":18396,\"start\":18388},{\"end\":18408,\"start\":18404},{\"end\":19297,\"start\":19295},{\"end\":19310,\"start\":19307},{\"end\":19327,\"start\":19320},{\"end\":19343,\"start\":19337},{\"end\":19359,\"start\":19356},{\"end\":19374,\"start\":19368},{\"end\":19393,\"start\":19388},{\"end\":19414,\"start\":19405},{\"end\":19432,\"start\":19425},{\"end\":19444,\"start\":19442},{\"end\":19828,\"start\":19821},{\"end\":19849,\"start\":19842},{\"end\":20377,\"start\":20371},{\"end\":20393,\"start\":20387},{\"end\":20406,\"start\":20400},{\"end\":20820,\"start\":20812},{\"end\":21084,\"start\":21080},{\"end\":21097,\"start\":21093},{\"end\":21107,\"start\":21102},{\"end\":21118,\"start\":21116},{\"end\":21134,\"start\":21129},{\"end\":21795,\"start\":21783},{\"end\":21807,\"start\":21805},{\"end\":22404,\"start\":22401},{\"end\":22417,\"start\":22415},{\"end\":22432,\"start\":22427},{\"end\":23028,\"start\":23023},{\"end\":23047,\"start\":23041},{\"end\":23069,\"start\":23053},{\"end\":23083,\"start\":23080},{\"end\":23097,\"start\":23092},{\"end\":23111,\"start\":23108},{\"end\":23123,\"start\":23120},{\"end\":23138,\"start\":23131},{\"end\":23161,\"start\":23152},{\"end\":23177,\"start\":23170},{\"end\":23191,\"start\":23186},{\"end\":23205,\"start\":23202},{\"end\":23219,\"start\":23213},{\"end\":23236,\"start\":23229},{\"end\":23257,\"start\":23248},{\"end\":23267,\"start\":23261},{\"end\":23287,\"start\":23277},{\"end\":23301,\"start\":23297},{\"end\":23309,\"start\":23303},{\"end\":24278,\"start\":24271},{\"end\":24296,\"start\":24289},{\"end\":24314,\"start\":24305},{\"end\":24330,\"start\":24322},{\"end\":24932,\"start\":24924},{\"end\":24949,\"start\":24942},{\"end\":24965,\"start\":24959},{\"end\":25534,\"start\":25529},{\"end\":25544,\"start\":25540},{\"end\":25561,\"start\":25551},{\"end\":25884,\"start\":25875},{\"end\":25901,\"start\":25893},{\"end\":26118,\"start\":26108},{\"end\":26131,\"start\":26127},{\"end\":26148,\"start\":26142},{\"end\":26163,\"start\":26157},{\"end\":26178,\"start\":26172},{\"end\":26527,\"start\":26521},{\"end\":26548,\"start\":26538},{\"end\":26783,\"start\":26779},{\"end\":26798,\"start\":26794},{\"end\":26811,\"start\":26809},{\"end\":26824,\"start\":26822},{\"end\":26835,\"start\":26830},{\"end\":27458,\"start\":27453},{\"end\":27480,\"start\":27471},{\"end\":27490,\"start\":27486},{\"end\":27501,\"start\":27499}]", "bib_entry": "[{\"attributes\":{\"doi\":\"abs/1811.04820\",\"id\":\"b0\"},\"end\":16683,\"start\":16479},{\"attributes\":{\"doi\":\"abs/1810.04805\",\"id\":\"b1\"},\"end\":17024,\"start\":16685},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4095446},\"end\":17515,\"start\":17026},{\"attributes\":{\"doi\":\"10.18653/v1/W18-2501\",\"id\":\"b3\",\"matched_paper_id\":3994096},\"end\":18269,\"start\":17517},{\"attributes\":{\"doi\":\"10.18653/v1/N16-1030\",\"id\":\"b4\",\"matched_paper_id\":6042994},\"end\":19143,\"start\":18271},{\"attributes\":{\"id\":\"b5\"},\"end\":19771,\"start\":19145},{\"attributes\":{\"id\":\"b6\"},\"end\":20277,\"start\":19773},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":17071982},\"end\":20736,\"start\":20279},{\"attributes\":{\"id\":\"b8\"},\"end\":20989,\"start\":20738},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1231\",\"id\":\"b9\",\"matched_paper_id\":174797886},\"end\":21698,\"start\":20991},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2747277},\"end\":22331,\"start\":21700},{\"attributes\":{\"doi\":\"10.3115/v1/D14-1055\",\"id\":\"b11\",\"matched_paper_id\":7152340},\"end\":22962,\"start\":22333},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":215780186},\"end\":24199,\"start\":22964},{\"attributes\":{\"doi\":\"10.18653/v1/D16-1198\",\"id\":\"b13\",\"matched_paper_id\":14100373},\"end\":24877,\"start\":24201},{\"attributes\":{\"doi\":\"10.1145/1242572.1242667\",\"id\":\"b14\",\"matched_paper_id\":207163173},\"end\":25429,\"start\":24879},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2470716},\"end\":25821,\"start\":25431},{\"attributes\":{\"doi\":\"10.1145/2629489\",\"id\":\"b16\",\"matched_paper_id\":14494942},\"end\":26052,\"start\":25823},{\"attributes\":{\"id\":\"b17\"},\"end\":26435,\"start\":26054},{\"attributes\":{\"id\":\"b18\"},\"end\":26682,\"start\":26437},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52011076},\"end\":27379,\"start\":26684},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":46939981},\"end\":27911,\"start\":27381}]", "bib_title": "[{\"end\":17084,\"start\":17026},{\"end\":17579,\"start\":17517},{\"end\":18320,\"start\":18271},{\"end\":19810,\"start\":19773},{\"end\":20363,\"start\":20279},{\"end\":21070,\"start\":20991},{\"end\":21770,\"start\":21700},{\"end\":22392,\"start\":22333},{\"end\":23015,\"start\":22964},{\"end\":24263,\"start\":24201},{\"end\":24913,\"start\":24879},{\"end\":25520,\"start\":25431},{\"end\":25867,\"start\":25823},{\"end\":26100,\"start\":26054},{\"end\":26768,\"start\":26684},{\"end\":27443,\"start\":27381}]", "bib_author": "[{\"end\":16546,\"start\":16532},{\"end\":16559,\"start\":16546},{\"end\":16781,\"start\":16767},{\"end\":16797,\"start\":16781},{\"end\":16809,\"start\":16797},{\"end\":16829,\"start\":16809},{\"end\":17101,\"start\":17086},{\"end\":17113,\"start\":17101},{\"end\":17595,\"start\":17581},{\"end\":17606,\"start\":17595},{\"end\":17620,\"start\":17606},{\"end\":17636,\"start\":17620},{\"end\":17652,\"start\":17636},{\"end\":17666,\"start\":17652},{\"end\":17682,\"start\":17666},{\"end\":17699,\"start\":17682},{\"end\":17717,\"start\":17699},{\"end\":18340,\"start\":18322},{\"end\":18360,\"start\":18340},{\"end\":18381,\"start\":18360},{\"end\":18398,\"start\":18381},{\"end\":18410,\"start\":18398},{\"end\":19299,\"start\":19290},{\"end\":19312,\"start\":19299},{\"end\":19329,\"start\":19312},{\"end\":19345,\"start\":19329},{\"end\":19361,\"start\":19345},{\"end\":19376,\"start\":19361},{\"end\":19395,\"start\":19376},{\"end\":19416,\"start\":19395},{\"end\":19434,\"start\":19416},{\"end\":19446,\"start\":19434},{\"end\":19830,\"start\":19812},{\"end\":19851,\"start\":19830},{\"end\":20379,\"start\":20365},{\"end\":20395,\"start\":20379},{\"end\":20408,\"start\":20395},{\"end\":20822,\"start\":20806},{\"end\":21086,\"start\":21072},{\"end\":21099,\"start\":21086},{\"end\":21109,\"start\":21099},{\"end\":21120,\"start\":21109},{\"end\":21136,\"start\":21120},{\"end\":21797,\"start\":21772},{\"end\":21809,\"start\":21797},{\"end\":22406,\"start\":22394},{\"end\":22419,\"start\":22406},{\"end\":22434,\"start\":22419},{\"end\":23030,\"start\":23017},{\"end\":23049,\"start\":23030},{\"end\":23071,\"start\":23049},{\"end\":23085,\"start\":23071},{\"end\":23099,\"start\":23085},{\"end\":23113,\"start\":23099},{\"end\":23125,\"start\":23113},{\"end\":23140,\"start\":23125},{\"end\":23163,\"start\":23140},{\"end\":23179,\"start\":23163},{\"end\":23193,\"start\":23179},{\"end\":23207,\"start\":23193},{\"end\":23221,\"start\":23207},{\"end\":23238,\"start\":23221},{\"end\":23259,\"start\":23238},{\"end\":23269,\"start\":23259},{\"end\":23289,\"start\":23269},{\"end\":23303,\"start\":23289},{\"end\":23311,\"start\":23303},{\"end\":24280,\"start\":24265},{\"end\":24298,\"start\":24280},{\"end\":24316,\"start\":24298},{\"end\":24332,\"start\":24316},{\"end\":24934,\"start\":24915},{\"end\":24951,\"start\":24934},{\"end\":24967,\"start\":24951},{\"end\":25536,\"start\":25522},{\"end\":25546,\"start\":25536},{\"end\":25563,\"start\":25546},{\"end\":25886,\"start\":25869},{\"end\":25903,\"start\":25886},{\"end\":26120,\"start\":26102},{\"end\":26133,\"start\":26120},{\"end\":26150,\"start\":26133},{\"end\":26165,\"start\":26150},{\"end\":26180,\"start\":26165},{\"end\":26529,\"start\":26514},{\"end\":26550,\"start\":26529},{\"end\":26785,\"start\":26770},{\"end\":26800,\"start\":26785},{\"end\":26813,\"start\":26800},{\"end\":26826,\"start\":26813},{\"end\":26837,\"start\":26826},{\"end\":27460,\"start\":27445},{\"end\":27482,\"start\":27460},{\"end\":27492,\"start\":27482},{\"end\":27503,\"start\":27492}]", "bib_venue": "[{\"end\":17296,\"start\":17213},{\"end\":17868,\"start\":17801},{\"end\":18722,\"start\":18574},{\"end\":20482,\"start\":20445},{\"end\":21332,\"start\":21245},{\"end\":21991,\"start\":21897},{\"end\":22639,\"start\":22549},{\"end\":24524,\"start\":24440},{\"end\":25144,\"start\":25067},{\"end\":25616,\"start\":25590},{\"end\":27003,\"start\":26916},{\"end\":27682,\"start\":27601},{\"end\":16530,\"start\":16479},{\"end\":16765,\"start\":16685},{\"end\":17211,\"start\":17113},{\"end\":17799,\"start\":17737},{\"end\":18572,\"start\":18430},{\"end\":19288,\"start\":19145},{\"end\":20017,\"start\":19851},{\"end\":20443,\"start\":20408},{\"end\":20804,\"start\":20738},{\"end\":21243,\"start\":21156},{\"end\":21895,\"start\":21809},{\"end\":22547,\"start\":22453},{\"end\":23325,\"start\":23311},{\"end\":24438,\"start\":24352},{\"end\":25065,\"start\":24990},{\"end\":25588,\"start\":25563},{\"end\":25929,\"start\":25918},{\"end\":26238,\"start\":26180},{\"end\":26512,\"start\":26437},{\"end\":26914,\"start\":26837},{\"end\":27599,\"start\":27503}]"}}}, "year": 2023, "month": 12, "day": 17}