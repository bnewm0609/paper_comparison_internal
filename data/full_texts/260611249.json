{
    "id": 260611249,
    "updated": "2023-10-04 21:33:40.044",
    "metadata": {
        "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
        "authors": "[{\"first\":\"Weiran\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Shelby\",\"last\":\"Heinecke\",\"middle\":[]},{\"first\":\"Juan\",\"last\":\"Niebles\",\"middle\":[\"Carlos\"]},{\"first\":\"Zhiwei\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yihao\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Le\",\"last\":\"Xue\",\"middle\":[]},{\"first\":\"Rithesh\",\"last\":\"Murthy\",\"middle\":[]},{\"first\":\"Zeyuan\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jianguo\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Devansh\",\"last\":\"Arpit\",\"middle\":[]},{\"first\":\"Ran\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Phil\",\"last\":\"Mui\",\"middle\":[]},{\"first\":\"Huan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Caiming\",\"last\":\"Xiong\",\"middle\":[]},{\"first\":\"Silvio\",\"last\":\"Savarese\",\"middle\":[]}]",
        "venue": "ArXiv",
        "journal": null,
        "publication_date": {
            "year": 2023,
            "month": null,
            "day": null
        },
        "abstract": "Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.",
        "fields_of_study": "[\"Computer Science\"]",
        "external_ids": {
            "arxiv": "2308.02151",
            "mag": null,
            "acl": null,
            "pubmed": null,
            "pubmedcentral": null,
            "dblp": "journals/corr/abs-2308-02151",
            "doi": "10.48550/arxiv.2308.02151"
        }
    },
    "content": {
        "source": {
            "pdf_hash": "81b10e64133e775dab53153cc82277d276efe1f7",
            "pdf_src": "Arxiv",
            "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.02151v1.pdf\"]",
            "oa_url_match": false,
            "oa_info": null
        },
        "grobid": {
            "id": "d32701ed365ce0cb48d24f79c8961e94d8cd9f56",
            "type": "plain-text",
            "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/81b10e64133e775dab53153cc82277d276efe1f7.txt",
            "contents": "\nRETROFORMER: RETROSPECTIVE LARGE LANGUAGE AGENTS WITH POLICY GRADIENT OPTIMIZATION\n\n\nWeiran Yao \n\u22c4 CTO Office\nSalesforce\n\nShelby Heinecke \n\u22c4 CTO Office\nSalesforce\n\nJuan Carlos Niebles \n\u22c4 CTO Office\nSalesforce\n\nZhiwei Liu \n\u22c4 CTO Office\nSalesforce\n\nYihao Feng \n\u22c4 CTO Office\nSalesforce\n\nLe Xue \n\u22c4 CTO Office\nSalesforce\n\nRithesh Murthy \n\u22c4 CTO Office\nSalesforce\n\nZeyuan Chen \n\u22c4 CTO Office\nSalesforce\n\nJianguo Zhang \n\u22c4 CTO Office\nSalesforce\n\nDevansh Arpit \n\u22c4 CTO Office\nSalesforce\n\nRan Xu \n\u22c4 CTO Office\nSalesforce\n\nPhil Mui \n\u22c4 CTO Office\nSalesforce\n\nHuan Wang \n\u22c4 CTO Office\nSalesforce\n\nCaiming Xiong cxiong@salesforce.com \n\u22c4 CTO Office\nSalesforce\n\nSilvio Savarese ssavarese@salesforce.com \n\u22c4 CTO Office\nSalesforce\n\nSalesforce Research \n\u22c4 CTO Office\nSalesforce\n\nRETROFORMER: RETROSPECTIVE LARGE LANGUAGE AGENTS WITH POLICY GRADIENT OPTIMIZATION\n\nRecent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.arXiv:2308.02151v1 [cs.CL] 4 Aug 2023 with a concrete direction to improve upon, helping it learn from prior mistakes and prevent repetitive errors to perform better in the next attempt.Although the self-reflection operation enables iterative refinement, generating useful reflective feedback from a pre-trained, frozen LLM is challenging, as showcased inFig. 1, since it requires the LLM to have a good understanding of where the agent made mistakes in a specific environment, i.e., the credit assignment problem(Sutton & Barto, 2018), as well as the ability to generate a summary containing actionable insights for improvement. The verbal reinforcement cannot be optimal, if the frozen language model has not been properly fine-tuned to specialize in credit assignment problems for the tasks in given environments. Furthermore, the existing language agents do not reason and plan in ways that are compatible with differentiable, gradient-based learning from rewards by exploiting the existing abundant reinforcement learning techniques. To address these limitations, this paper introduces Retroformer, a principled framework for reinforcing language agents by learning a plug-in retrospective model, which automatically refines the language agent prompts from environment feedback through policy optimization. Specifically, our proposed agent architecture can learn from arbitrary reward information across multiple environments and tasks, for iteratively fine-tuning a pre-trained language model, which refines the language agent prompts by reflecting on failed attempts and assigning credits of actions taken by the agent on future rewards.Lollipop Chainsaw featured Juliet Starling, who was voiced by a Canadian-American actress who has done voice roles for what Teen Titans spinoff series?\n\nINTRODUCTION\n\nRecently, we have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing tasks on their own, ultimately in the service of a goal, rather than simply responding to queries from human users. Prominent studies, including ReAct (Yao et al., 2023), Toolformer (Schick et al., 2023), Hug-gingGPT (Shen et al., 2023), generative agents (Park et al., 2023), WebGPT (Nakano et al., 2021), AutoGPT (Gravitas, 2023), BabyAGI (Nakajima, 2023), and Langchain (Chase, 2023), have successfully showcased the viability of creating autonomous decision-making agents by leveraging the capabilities of LLMs. These approaches use LLMs to generate text-based outputs and actions that can be further employed for making API calls and executing operations within a given environment.\n\nGiven the immense scale of LLMs with an extensive parameter count, the behaviors of most existing language agents, however, are not optimized or aligned with environment reward functions. An exception is a very recent language agent architecture, namely Reflexion (Shinn et al., 2023), and several other related work, e.g., Self-Refine (Madaan et al., 2023b) and Generative Agent (Park et al., 2023), which use verbal feedback, namely self-reflection, to help agents learn from prior failure. These reflective agents convert binary or scalar reward from the environment into verbal feedback in the form of a textual summary, which is then added as additional context to the prompt for the language agent. The self-reflection feedback acts as a semantic signal by providing the agent Figure 1: An example of uninformative self-reflections from a frozen LLM. The root cause of failure in prior trial is that the agent should have responded only the spinoff series \"Teen Titans Go\" not \"Teen Titans\" as the answer. The agent forgot its goal during a chain of thought, actions and lengthy observations. The verbal feedback from frozen LLM, however, only rephrases the prior actions sequences as the proposed new plan, resulting the same, incorrect actions in the next trial.\n\nWe conduct experiments on open-source simulation and real-world environments including Hot-PotQA (Yang et al., 2018), which consists of search-based question answering tasks to test the tool use abilities of a web agent which needs to call Wikipedia APIs in multiple steps for answering question. We observe Retroformer agents are faster learners compared with reflexion, which does not use gradient for reasoning and planning, and better decision-makers and reasoners. More concretely, Retroformer agents improve the success rate of search-based question-anwering tasks in HotPotQA (Yang et al., 2018) by 18% in 4 attempts, which demonstrates the effectiveness of gradient-based reasoning and planning for tool use under large state-action space.\n\nTo summarize, our contributions are the following:\n\n\u2022 The paper introduces Retroformer, which iteratively refines the prompts given to large language agents based on environmental feedback to improve learning speed and task completion.\n\nWe take a policy gradient approach with the Actor LLM being part of the environment, allowing learning from a wide range of reward signals for diverse tasks and environments. \u2022 The proposed method focuses on enhancing the retrospective model in the language agent architecture, without accessing the Actor LLM parameters or needing to propagate gradients through it. The agnostic nature of Retroformer makes it a flexible plug-in module for various types of cloud-based LLMs, such as GPT or Bard.\n\n\nRELATED WORK\n\nAutonomous Language Agents We summarize in Table 1 the recent language agent literature related to our work from five perspectives and differentiate our method from them. The completion of a complex task typically involves numerous stages. An agent must possess knowledge of these stages and plan accordingly. Chain-of-Thoughts or CoT (Wei et al., 2022) is the pioneering work that prompts the agent to decompose challenging reasoning tasks into smaller, more manageable steps. ReAct (Yao et al., 2023), on the other hand, proposes the exploitation of this reasoning and acting proficiency within LLM to encourage interaction with the environment (e.g. using the Wikipedia search API) by mapping observations to the generation of reasoning and action traces or API calls in natural language. This agent architecture has spawned various applications, such as HuggingGPT (Shen et al., 2023), generative agents (Park et al., 2023), WebGPT (Nakano et al., 2021), AutoGPT (Gravitas, 2023), BabyAGI (Nakajima, 2023), and Langchain (Chase, 2023).  (Yao et al., 2023) \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 Self-refine (Madaan et al., 2023b) \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 RAP (Hao et al., 2023) \u2717\n\u2717 \u2713 \u2713 \u2713 \u2713 Reflexion (Shinn et al., 2023) \u2717 \u2717 \u2713 \u2713 \u2713 \u2713 Retroformer (our method) \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nHowever, these approaches fail to learn from valuable feedback, such as environment rewards, to enhance the agent's behaviors, resulting in performances that are solely dependent on the quality of the pre-trained LLM. Self-refine (Madaan et al., 2023a) addresses this limitation by employing a single LLM as a generator, refiner, and provider of feedback, allowing for iterative refinement of outputs. However, it is not specifically tailored for real-world task-based interaction with the environment. On the other hand, RAP (Hao et al., 2023) repurposes the LLM to function as both a world model and a reasoning agent. It incorporates Monte Carlo Tree Search for strategic exploration within the extensive realm of reasoning with environment rewards. This approach enables effective navigation and decision-making in complex domains. Recently, Shinn et al. (2023) presents Reflexion, a framework that equips agents with dynamic memory and self-reflection capabilities, enhancing their reasoning skills. Self-reflection plays a pivotal role, allowing autonomous agents to iteratively refine past actions, make improvements, and prevent repetitive errors.\n\nTransformer Reinforcement Learning Reinforcement learning with a provided reward function or a reward-labeled dataset, commonly referred to as RLHF, has become a standard practice within the LLM fine-tuning pipeline. These endeavors have convincingly demonstrated the efficacy of RL as a means to guide language models towards desired behaviors that align with predefined reward functions encompassing various domains, including machine translation, summarization, and generating favorable reviews. Among the prevalent transformer RL methods are online RL algorithms such as Proximal Policy Optimization or PPO (Schulman et al., 2017), and offline RL techniques such as Implicit Language Q-Learning or ILQL (Snell et al., 2022) and Direct Preference Optimization or DPO (Rafailov et al., 2023). These methods have been implemented in TRL/TRLX (von Werra et al., 2020) distributed training framework.\n\n\nCHALLENGES\n\nAlthough LLMs are not designed to handle tool use or take actions, it has been observed (Gravitas, 2023;Nakajima, 2023;Chase, 2023) that empirically for text-rich environment, especially when the actions and states are accurately described using natural languages, LLMs work surprisingly well. However there are still plenty of challenges applying LLM-based agents. Here we list several below.\n\n\nSpurious Actions\n\nLLMs are not pre-trained or designed with an action-agent application in mind. Even some restrictions are explicitly specified in the prompt, the LLM model may still generate spurious actions that are not in the action space A.\n\nLimited Prompt Length LLM itself is stateless. However, in applications it is preferred to empower agents with states or memories for better performance. It has been observed that LLM based agents are easy to run into infinite loops if the states are not handled nicely. Many LLM agents concatenate all the previous state descriptions and actions into the prompt so that LLM as a way to bestow \"state\" to the LLM. Inevitably this methodology runs into the prompt length issues. As the trajectory grows longer, the prompt runs out of spaces.\n\nHeuristic Prompt Engineering Even though a lot of paradigms have been proposed to improve LLM agents' performance (Yao et al., 2023;Ahn et al., 2022), there is a lack of systematic methodologies for consistent model refinement. In fact, manual prompt tuning is still widely used in a lot of the application scenarios.\n\nProhibitive Training Most of the well-performing LLMs are too large to be fit in just one or two GPUs. It is technically challenging to optimize the LLMs directly as is done in the the classical reinforcement learning setting. In particular, OpenAI has not provided any solution for RL based finetuning. Most of the issues are caused by the fact that LLMs are not pre-trained or designed with an action-agent application in mind.\n\n\nINTUITION\n\nCompared to the LLM-based action agents, classical RL agents, though not able to handle text-based environments as nicely in the zero shot setting, are able to keep improving based on the feedback and rewards provided by the environment. Popular RL algorithms include Policy Gradient (Sutton et al., 2000), Proximal Policy Optimization Algorithm (PPO) (Schulman et al., 2017), Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), and Advantage Actor Critic methods (Mnih et al., 2016).\n\nIn this draft we are proposing a simple but powerful novel framework to tackle the challenges mentioned above. On one hand, we would like to leverage the classical RL based optimization algorithms such as policy gradient to improve the model performance. On the other hand, our framework avoids finetuning on the LLM directly. The key is, instead of training the LLM directly, we train a retrospective LM. The retrospective LM takes users' prompt, rewards and feedback from the environment as input. Its output will be prompt for the actual LLM to be consumed. RL algorithms are employed to optimize the weights in the retrospective LM model instead of directly on the LLM. In our framework the weights in the actual LLM is assumed to be fixed (untrainable), which aligns well with the application scenario when the LLM is either too large to tune or prohibited from any tuning.\n\nAnother perspective viewing our framework is, we train a retrospective LM to apply automatic prompt tuning for the LLM agents. In this case, the RL algorithms such as policy gradients are employed to optimize the prompts. Ideally the retrospective LM can help summarize the past \"experience\", the users' prompt, the environments' feedback into a condensed text with length limit so that it is easier for the LLM to digest. To some extent, in our setting the original LLM can be considered as part of the environment since its parameters are all fixed.\n\n\nNOTATION AND FORMULATION\n\nWe denote a large language model (LLM) based agent as a function L \u03be l : M \u2192 A, where M is the space of prompts, which may include the actual prompts m u provided by the users, as well as some contextual information c \u2208 C. Here C is the space of context as a representation of the current state S returned by the environment \u2126. A is the space of actions. Note the actions taken by most language model based agents are sampled auto-repressively, so L is a random function.\n\nThe subscript \u03be l denotes the re-parameterized random variables involved in the sampling process. Another note is, the LLM-based agent itself is stateless. All the states and possible memorization are characterized as text in the prompt.\n\nThe environment is defined as a tuple (T \u03beo , R). T \u03beo : S \u00d7 A \u2192 S is the state transition function, where S is the space of states and A is the action space. Here we assume the states and actions are represented using text. Again we used \u03be o to represent the randomness involved in the state transition.\n\nFor each state s \u2208 S, a reward function is defined as R : S \u2192 R. At each step of the play, the state s is described using natural language, and integrated into the context c. In the context, previous states may also be described and embedded to help LLMs making a good guess on the next action to take. As in all the reinforcement learning setting, the final goal is to maximize the cumulative rewards, or returns G cum = T t=0 R(s t ). In many situations, the rewards are sparse, i.e., R(s t ) are mostly zero except very few states.\n\nThe retrospective model takes the all the previous states s 1,\u00b7\u00b7\u00b7 ,t , actions a 1,\u00b7\u00b7\u00b7 ,t , rewards r 1,\u00b7\u00b7\u00b7 ,t , and the user prompt m u as input, and massage those information into a new prompt m to be consumed by the LLM agent:\n\u0393 \u03ber,\u0398 : [S i , A i , R i , M u i ] t i=1 \u2192 M,(1)\nwhere \u03be r stands for the randomness involved in the retrospective model, and \u0398 is the set of learnable parameters in the retrospective model. The goal of the RL optimization is\narg max \u0398 E \u03be l ,\u03beo,\u03ber T t=1 R(s t ) s.t. s t+1 = T \u03beo s t , L \u03be l \u2022 \u0393 \u03ber,\u0398 [s i , a i , r i , m u i ] t i=1 , \u2200t \u2208 {1, \u00b7 \u00b7 \u00b7 , T \u2212 1}(2)\nNote that the only learnable parameters are in the retrospective model M r . Since LLM agent model is fixed, it can be considered as a component of the environment. Specifically, if we construct another environment with the transition function T \u2032 = T (S, \u2022) \u2022 L : S \u00d7 M \u2192 S, and the same reward function R, then the objective in Eq. (2) is just a regular RL optimization so all the popular RL algorithms apply.\n\n\nOUR APPROACH: REINFORCING RETROSPECTIVE LANGUAGE AGENT\n\nAs illustrated in Fig. 2, our proposed framework Retroformer is comprised of two language model components: an actor LLM, denoted as M a , which generates reasoning thoughts and actions, and a retrospective LLM, denoted as M r , which generates verbal reinforcement cues to assist the actor in self-improvement by refining the actor prompt with reflection responses.\n\n\nActor LM Trajectory\n(s 1 , a 1 , r 1 , \u2026, s t ) Action a t Environment 1 , 2 , 3 \u2026 K (a t, r t, s t+1 ) Environment 1 Environment 2 Environment K \u2026\n\nRetrospective LM\n\n\nEpisode Returns\n\n\nPrompt\n\nRating for reflection response k We assume in this paper that the actor model is a frozen LLM whose model parameters are inaccessable (e.g., GPT, Bard) and the retrospective model is a smaller, local language model that can be fine-tuned under low-resource settings. In addition, Retroformer has an iterative policy gradient optimization step which is specifically designed to reinforce the reflection model with gradient-based approach. We provide in this section a detailed description of each of these modules and subsequently elucidate their collaborative functioning within the Retroformer framework. The implementation details are presented in Section 7.2.\nr = \u2206G !,# = G !,#$% \u2212 G !,# (a) Retrospective agent (b) Ratings for reflection responses Env 1 Returns G 1,i Env 2 Returns G 2,i Env K Returns G k,i \u2026 Env 1 Returns G 1,i+1 Env 2 Returns G 2,i+1 Env K Returns G k,i+1\n\nRETROSPECTIVE AGENT ARCHITECTURE\n\nAs illustrated in Fig. 2(a), for the base actor model and retrospective model, we apply a standard communication protocol modified from the Relexion agent architecture (Shinn et al., 2023), in which the retrospective model refines the actor prompt by appending verbal feedback to the prompt.\n\nActor Model The actor model is a LLM hosted in the cloud, whose model parameters are hidden and frozen all the time. The actor LM is instructed to generate actions with required textual content, taking into account the observed states. Similar to reinforcement learning, we select an action or generation, denoted as a t , from the current policy \u03c0 \u03b8 at time step t and receive an observation, represented by s t , from the environment. We use ReAct (Yao et al., 2023) as our actor prompt.\na k,i,t = M a [s k,i,\u03c4 , a k,i,\u03c4 , r k,i,\u03c4 ] t\u22121 \u03c4 =1 , s k,i,t .(3)\nRetrospective Model The retrospective model M r is instantiated as a local LM. Its primary function is to produce self-reflections, offering valuable feedback for diagnosing a possible reason for prior failure and devising a new, concise, high-level plan that aims to mitigate same failure. Operating under a sparse reward signal, such as binary success status (success/failure), the model detects the root cause of failure by considering the current trajectory alongside its persistent memory.\ny k,i = M r ([s k,i,\u03c4 , a k,i,\u03c4 , r k,i,\u03c4 ] T \u03c4 =1 , G k,i Reflection prompt x k,i ).(4)\nThis self-reflection feedback y k,i is appended to the actor prompt to prevent repetitive errors in a specific environment in future attempts. Consider a multi-step task, wherein the agent failed in the prior trial. In such a scenario, the retrospective model can detect that a particular action, denoted as a t , led to subsequent erroneous actions and final failure. In future trials, the actor LM can use these self-reflections, which are appended to the prompt, to adapt its reasoning and action steps at time t, opting for the alternative action a \u2032 t . This iterative process empowers the agent to exploit past experiences within a specific environment and task, thereby avoiding repetitive errors.\n\nMemory Module The actor model generates thoughts and actions, by conditioning on its recent interactions (short-term memory) and reflection responses (long-term memory).\n\n\u2022 Short-term memory. The trajectory history \u03c4 i of the current episode i serves as the short-term memory for decision making and reasoning. \u2022 Long-term memory. The reflection responses that summarize prior failed attempts are appended to the actor prompt as the long-term memory.\n\nTo facilitate policy optimization in Section 6.2, we store the prompts and responses of the retrospective model of each trial, as instruction-response pairs, together with the episode returns in a local dataset, which we call replay buffer. We sample from the replay buffer to reinforce the retrospective model at the end of each episode. The long and short-term memory components provide context that is specific to a given task over several failed trials and the replay buffer provides demonstrations of successful reflections across the tasks and environments, so that our Retroformer agent not only exploits lessons learned over failed trials in this task, but also explores by learning from successful reflections in other tasks.\n\n\u2022 Replay buffer. The memory D RL which stores the triplets (x k,i , y k,i , G k,i ) of the reflection prompt x k,i , reflection response y k,i and episode return G k,i of trial i and task k.\n\nReward Shaping We apply reward shaping to the binary rewards for obtaining more information.\n\nFor question answering tasks, instead of exactly matching the answer, we use f1 score grading to evaluate the alignment of the generated output with the expected answer as the reward function.\n\n\nPOLICY GRADIENT OPTIMIZATION\n\nThe actor model M a is regarded as an frozen LLM, such as GPT, with inaccessible model parameters. In this scenario, the most direct approach to enhancing actor performance in a given environment is by refining the actor LM's prompt. Consequently, the retrospective model M r , a smaller local language model, paraphrases the actor's prompt by incorporating a concise summary of errors and valuable insights from failed attempts. We therefore aim to optimize the M r model using environment reward. The desired behavior of M r is to improve the actor model M a in next attempts. Hence, the difference in episode returns between two consecutive trials naturally serve as a reward signal for iteratively fine-tuning the retrospective model M r , acting as a surrogate for reinforcement learning aided by gradient-based signals. The specifics of this approach are outlined as follows.  Figure 3: Policy gradient optimization of retrospective LM using RLHF training pipeline.\n\n\nRetrospective LM\n\n\nInstruction and Response Generation\n\nThe retrospective model generates a pair of instruction and response at the end of each trial i in the environment k. In the trial i, the actor produces a trajectory \u03c4 i by interacting with the environment. The evaluator then produces a score r i which is computed as r t = M e (\u03c4 i ). After the trial, to produce verbal feedback for refining the actor prompt, M r takes the set of {\u03c4 i , r i } as the instruction x k,i and is prompted to produce a reflection response y k,i . All these instruction-response pairs (x k,i , y k,i ) across tasks and trials are stored to a local dataset D RL , which we call \"replay buffer\", for fine-tuning the M r at the end of each episode.\n\nReflection Response Rating As illustrated in Fig. 2(b), let us assume a reflection prompt x k,i and the corresponding episode return G k,i , and the retrospective model M r generates the response y k,i that summarizes the mistakes in i, which results in the return R k,i+1 in the next attempt i + 1.\n\nBecause the actor is a frozen LM and the temperature is set to zero as default (Yao et al., 2023), i.e., T = 0, the injected randomness that leads to differences in returns \u2206G k,i = G k,i+1 \u2212 G k,i are from the reflection responses y k,i , in which positive \u2206G k,i indicates better responses that help the actor learn from prior errors, and hence should be rated with higher scores; negative or zero \u2206G k,i indicates worse responses that needs to be avoided and hence should be rated with lower scores. Therefore, we define the rating score of a reflection instruction-response pair (x k,i , y k,i ) as:\nr(x k,i , y k,i ) \u225c G k,i+1 \u2212 G k,i .(5)\nProximal Policy Optimization The optimization step of Retroformer is formalized as an iterative process in Fig. 3. We use the differences of episode returns as the ratings of the generated reflection responses. The retrospective language model is fine-tuned at the end of each episode following the standard RLHF training procedures (Ouyang et al., 2022) with proximal policy optimization:\nL PPO = E x\u223cDRL E y\u223cLLM RL \u03d5 (x) r(x, y) \u2212 \u03b2 log LLM RL \u03d5 (y|x) LLM Ref (y|x) ,(6)\nwhere (x, y) are sampled from the replay buffer, r(x, y) is the defined reward model, and the second term in this objective is the KL divergence to make sure that the fine-tuned model LLM RL does not stray too far from the frozen reference model LLM Ref .\n\nThe actor and retrospective model, and the policy gradient optimization module work together through trials in a loop until the environment deems \u03c4 t to be correct. In practice, we apply offline RL methods instead of online optimization. We collected the dataset D RL by rolling out a base policy, i.e., the frozen actor LM and the initialized retrospective LM, in the tasks in the training sets for N trials and compute the ratings. We apply the standard RLHF pipeline to fine-tune the retrospective model offline before evaluating the agent in the validation tasks. In online execution, we use best-of-n sampler, with the scores evaluated by the learned reward model from RLHF pipeline, as an alternative method of generating better retrospective responses in each trial.\n\n\nEXPERIMENTS\n\nExtensive experiments are conducted to evaluate our method, including comparisons with ReAct and Reflexion performances, and visualization and discussion of agent's generated text and actions.\n\n\nENVIRONMENT AND DATASET\n\nWe use open-source HotPotQA (Yang et al., 2018) environment, which consists of search-based question answering tasks to evaluate the agent's tool usage abilities under large state-action space.\n\nHotPotQA The agent is asked to solve a question answering task by searching in Wikipedia pages. At each time step, the agent is asked to choose from three action types or API calls:\n\n1. SEARCH [ENTITY], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search. 2. LOOKUP [KEYWORD], which returns the next sentence containing keyword in the last passage successfully found by Search. 3. FINISH [ANSWER], which returns the answer and finishes the task.\n\nDataset We collected 3,383 reflection samples by running the base rollout policy for 3 trials (N = 3) for 3,000 tasks in the training set, in which 1,084 instruction-response pairs have positive ratings.\n\n\nIMPLEMENTATION DETAILS\n\nModel We use GPT-3 (model: text-davinci-003) as the frozen actor model. For the retrospective model, we instantiate it from LongChat (model: longchat-7b-16k), which is a LM with 16k context length by fine-tuning llama-7b on instruction-following samples from ShareGPT. In all experiments, we set the temperature of actor LM as zero, i.e., T=0 and top p =1 to isolate the randomness of LM from the effects of reflections. We acknowledge that setting a higher temperature value can encourage exploration but it can obscure the impact of the proposed approaches, making it difficult to compare against existing baselines with T=0 (Yao et al., 2023;Shinn et al., 2023).\n\nSetup Our proposed learning framework is developed by using multiple open-source tools as follows. We use the OpenAI connectors from langchain to build our actor models M a . During inference of the retrospective model, we host an API server using FastChat and integrates it with langchain agents. The tool can host longchat-7b-16k with concurrent requests to speed up RL policy rollouts. For fine-tuning the retrospective model, we develop our training pipeline with trl, which supports transformer reinforcement learning with PPO trainer.\n\nWe present the details of the specific prompts we used and the full agent demonstrations and examples for each environment in Appendix A.\n\nTraining We fine-tune the retrospective model M r with 4-bit quantized LoRA adapters (r=1) on the offline RL datasets with epochs=4; batch size=8; lr=1.4e-5. The number of trainable parameters is 0.53M (0.015% of llama-7b). We first run supervised fine-tuning trainer on the samples with positive ratings for 2 epochs and then the RLHF pipeline, including reward modeling, and RL finetuning with PPO, on the whole offline rating dataset using the default settings for llama-7b model.\n\n\nEvaluation Metrics\n\nWe report the success rate and the average episode returns over validation tasks in an environment. The agent is evaluated on 100 validation tasks from the distractor dev split of open-source HotPotQA dataset.\n\nBaselines We experiment with two language agent baselines.\n\n\u2022 ReAct (Yao et al., 2023). This is the language agent architecture which AutoGPT (Gravitas, 2023) is built upon. At the same time, this agent does not learn from the environment rewards at all and can serve as a baseline for showing how the actor model performs without exploration and exploitation in the environment.\n\n\u2022 Reflexion (Shinn et al., 2023). This is the state-of-the-art language agent architecture that the authors identify from literature so far. This agent enhances from verbal feedback of the environment, and does not use gradient signals explicitly. It can serve as a baseline for showing the effectiveness of gradient-based learning for reasoning and planning.\n\n\nCOMPARISONS WITH BASELINES\n\nWe present the performances of our Retroformer in Table 2 and compare it against the baselines in Fig. 4. As shown in the table, we observe that the generated reflections consistently improve the agent performances over trials and the effects of reflection sampling are significant in the first few trials. This is expected as exploration in the early trials benefits exploration of the state action space.  Furthermore, as shown in Fig. 4, our agent outperforms the two strong baselines. Specifically, the results indicate that our reinforced model provides the language agents with better reflection responses in early trials, which enables the agents to learn faster, while also achieving better performances in the end. Our Retroformer agent achieves 53% success rate in 5 trials, which is better than the state-of-the-art 50% success rate reported in (Jang, 2023) that uses a much larger language model, i.e., GPT-3 (model: textdavinci-003) as the retrospective component. The results show the effectiveness of our policy gradient approach for finetuning the agent with offline samples.\n\n\nDISCUSSION OF THE REINFORCED AGENT BEHAVIORS\n\nWe examine how the retrospective model M r is improved with policy optimization by comparing the generated responses from the frozen retrospective LM and the ones from the fine-tuned, reinforced LM. We observe two general emerging behaviors that the fine-tuned LM learns to follow through trial and error. These two reinforced behaviors help the agents outperform the baselines in Fig. 4.\n\nPrecise Credit Assignment for Failed Trials. As shown in Fig. 5, we gave an example that illustrates why the reinforced responses better analyze the mistakes made in the prior failed trial and enable the agent to prevent from repetitive errors in the next attempt. The agent failed in the last attempt mainly because it submits both \"Teen Tians\" and \"Teen Titans Go\" as the answer to the Teen Titans spin-off series, while the correct answer should only include \"Teen Titans Go\"; The agent forgot its original goal during a chain of thought, actions and lengthy observations. While the reflection response from the frozen model seems to have a lot \"should have\" terms and is very fluent in language, it simply reiterates the prior action sequences that lead to failure as the steps that the agent should have done, which prompts the agent to follow these steps again in the next attempt, resulting in an infinite loop.  Figure 5: Response refinement from the reinforced retrospective model. Note that the lengthy observation step in the prior failed trial column is abbreviated for better presentation purposes.\n\nOn the contrary, our reinforced response prompts the agent to focus on spinoff series in the very beginning of the response and asks the agent to find the answer in the previous actions and observations with special attention on the search results of \"Tara Strong\". In summary, our proposed reinforcement learning approach presents better credit assignment and root cause analysis abilities, and has the power to recommend real actionable insights for the next attempt.\n\nReinforced model response Frozen model response I think the issue might have been with the search. I was not able to find the name \"Miller\" in the paragraph, which is why I was not able to find James Henry Miller's wife. I should have searched for \"Miller's wife\" instead of \"James Henry Miller's wife\" to avoid this confusion.\n\nNew Plan: To avoid confusion, I will search for \"Miller's wife\" instead of \"James Henry Miller's wife\" and directly look for the nationality of the wife.\n\nI got stuck in a loop where I kept searching for the same person, Henry Miller, instead of Beatrice Sylvas Wickens. It's possible that I may have misheard or misunderstood the question. To avoid this, I should have written down the question and made sure I understood it correctly before beginning the search. Additionally, I should have tried searching for \"James Henry Miller's wife\" instead of just \"James Henry Miller\". Structured Reflections and Action Plans. We discover one emergent behavior of the reinforced model that it can automatically paraphrase the original responses into two separate structured sections, namely Reflection section and New plan: section, although not being explicitly trained or prompted for. One such example is shown in Fig. 6. The paraphrased response retrospects in the first paragraph and provides actionable insights next, while the response from the frozen LM interleaved both parts in one paragraph, making it hard to comprehend. We can also observer from Fig. 5 that the reinforced response removes the messy, irrelevant \"Next trial:\" content in the end for cleaner format, which may very likely result from LLM hallucination.\n\n\nCONCLUSION\n\nIn this study, we present Retroformer, an elegant framework for iteratively improving large language agents by learning a plug-in retrospective model. This model, through the process of policy optimization, automatically refines the prompts provided to the language agent with environmental feedback. Through extensive evaluations on real-world datasets such as HotPotQA, the method has been proven to effectively improve the performances of large language agents over time both in terms of learning speed and final task completion performances.\n\nBy considering the Actor LLM as a component of the environment, our policy gradient approach allows learning from arbitrary reward signals from diverse environments and tasks. This facilitates the iterative refinement of a specific component within the language agent architecture -the retrospective model, in our case, while circumventing the need to access the Actor LLM parameters or propagate gradients through it. This agnostic characteristic renders Retroformer a concise and adaptable plug-in module for different types of cloud-hosted LLMs, such as GPT and Bard. Furthermore, our approach is not limited to enhancing the retrospective model alone; it can be applied to reinforce other components within the agent architecture, such as the memory and summarization module, or the actor prompt. By selectively focusing on the component to be fine-tuned while keeping the remainder fixed, our proposed policy gradient approach allows for iterative improvements of the component with reward signals obtained from the environment.\n\nFigure 2 :\n2Framework overview.\n\nFigure 4 :\n4We compare Retroformer with baselines and observe consistent and faster performance improvement in terms of success rate.\n\nFigure 6 :\n6Response refinement from the reinforced retrospective model with structured format.\n\nTable 1 :\n1Related work on large language agents.Approach \nGradient Arbitrary \nIterative \nHidden \nDecision Memory \nlearning \nreward \nrefinement constraints \nmaking \n\nCoT (Wei et al., 2022) \n\u2717 \n\u2717 \n\u2717 \n\u2717 \n\u2717 \n\u2717 \nReAct \n\nTable 2 :\n2Retroformersuccess rate (succ%) and average reward in N trials and best-of-n shots \nreflection sampling scored by the learned reward model in HotPotQA environment. The standard \ndeviation is calculated for the average rewards over 100 tasks and n shots. \n\nEnvironment Settings \n1 shot \n2 shots \n4 shots \n\nHotPotQA \n\nN=0 (initial trial) --------34%/0.43 \u00b1 0.46 --------\nN=1 \n39%/0.45 \u00b1 0.47 42%/0.48 \u00b1 0.46 45%/0.52 \u00b1 0.47 \nN=2 \n42%/0.48 \u00b1 0.46 45%/0.52 \u00b1 0.47 48%/0.54 \u00b1 0.47 \nN=4 \n50%/0.55 \u00b1 0.48 52%/0.58 \u00b1 0.46 53%/0.60 \u00b1 0.46 \n\n\n\n\nReflection (Green: Retroformer; Brown: Frozen LM) Prior failed trial QuestionI failed to find the Teen Titans spinoff series that Tara Strong has done voice roles for. Instead, I found Lollipop Chainsaw, a video game that Juliet Starling appeared in. I should have searched Tara Strong and Teen Titans or Teen Titans Go! specifically in my previous action to find the correct answer.Thought 1: I need to search Juliet Starling and find the Canadian-American actress who voiced her, then find the Teen Titans spinoff series she has done voice roles for.Action 1: Search[Juliet Starling] Observation 1: Could not find [Juliet Starling]. \u2026 Thought 2: Juliet Starling is a character from Lollipop Chainsaw, so I can search Lollipop Chainsaw. Action 2: Search[Lollipop Chainsaw] Observation 2: Lollipop Chainsaw is a \u2026 Thought 3: Juliet Starling is voiced by Tara Strong, so I need to search Tara Strong and find the Teen Titans spinoff series she has done voice roles for. Action 3: Search[Tara Strong] Observation 3: Tara Lyn Strong\u2026 Thought 4: Tara Strong has voiced characters in Teen Titans and Teen Titans Go!, so the answer is Teen Titans and Teen Titans Go!. Action 4: Finish[Teen Titans and Teen Titans Go!] Lollipop Chainsaw featured Juliet Starling, who was voiced by a Canadian-American actress who has done voice roles for what Teen Titans spinoff series? My thought process was strong but my execution was flawed. I should have searched for Lollipop Chainsaw first and looked up the Canadian-American actress who voiced Juliet Starling afterwards. I also should have looked up Tara Strong's filmography and searched for any voice roles she did specifically for Teen Titans or Teen Titans Go!.Next trial:Question: The novel was adapted into a film by a director known for his work on another iconic 1980s franchise. What novel is this film based on?Thought 1:\nAppendix for\"Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization\"A FULL EXAMPLES A.1 ACTOR PROMPT ENGINEERING An example of the HotPotQA actor language model prompt is shown below.A.2 RETROSPECTIVE PROMPT ENGINEERINGAn example of the HotPotQA retrospective instruction prompt is shown below.You are an advanced reasoning agent that can improve based on self reflection. You will be given a previous reasoning trial in which you were given access to an Docstore API environment and a question to answer. You were unsuccessful in answering the question either because you guessed the wrong answer with Finish[<answer>], or you used up your set number of reasoning steps. In a few sentences, Diagnose a possible reason for failure and devise a new, concise, high level plan that aims to mitigate the same failure. Use complete sentences.Here are some examples.(March 9, 1961-December 12, 2022was an American college football coach who primarily coached at the NCAA Division I FBS level. He was a two-time national coach of the year, three-time conference coach of the year and the mastermind behind the NCAA record-setting air raid offense. He was the head coach at Texas Tech University from 2000 to 2009, where he became the winningest coach in school history. After Texas Tech, he coached at Washington State University from 2012 to 2019, where he recorded the third-most wins of any coach in school history. He then coached at Mississippi State from 2020 until his death in 2022.Leach was known for directing offenses using lots of passing to several receivers, in a spread system known as the air raid, which Leach developed with Hal Mumme when Mumme was head coach and Leach was offensive coordinator at Iowa Wesleyan, Valdosta State, and Kentucky in the 1990s. Leach's offenses with Mumme, and later as a head coach himself, have broken numerous school and NCAA records. On eighteen occasions, Leach led his unranked team to victory over a team ranked in the AP poll, which is the most in the AP era.\nDo as i can, not as i say: Grounding language in robotic affordances. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, arXiv:2204.01691arXiv preprintMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n\n. Harrison Chase, Langchain, 2023. Significant Gravitas. Autogpt. Harrison Chase. Langchain. https://github.com/hwchase17/langchain, 2023. Significant Gravitas. Autogpt. https://github.com/Significant-Gravitas/ Auto-GPT, 2023.\n\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. arXiv preprintShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.\n\nCan llms critique and iterate on their own outputs? evjang.com. Eric Jang, Eric Jang. Can llms critique and iterate on their own outputs? evjang.com, Mar 2023. URL https://evjang.com/2023/03/26/self-reflection.html.\n\nLearning performance-improving code edits. Aman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, Amir Yazdanbakhsh, arXiv:2302.07867arXiv preprintAman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, and Amir Yazdanbakhsh. Learning performance-improving code edits. arXiv preprint arXiv:2302.07867, 2023a.\n\nSelf-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.17651arXiv preprintAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023b.\n\nAsynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, abs/1602.01783CoRRVolodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. CoRR, abs/1602.01783, 2016.\n\n. Yohei Nakajima, Babyagi, 2023Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. arXiv preprintReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo- pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744, 2022.\n\nJoon Sung Park, C Joseph, Carrie J O&apos;brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. arXiv preprintJoon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.\n\nDirect preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, D Christopher, Chelsea Manning, Finn, arXiv:2305.18290arXiv preprintRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.\n\nToolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.04761arXiv preprintTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\n\nTrust region policy optimization. John Schulman, Sergey Levine, Philipp Moritz, Michael I Jordan, Pieter Abbeel, abs/1502.05477CoRRJohn Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.\n\nProximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, abs/1707.06347CoRRJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.\n\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.17580arXiv preprintYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\n\nReflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv:2303.11366arXiv preprintNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.\n\nOffline rl for natural language generation with implicit language q learning. Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, Sergey Levine, arXiv:2206.11871arXiv preprintCharlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language q learning. arXiv preprint arXiv:2206.11871, 2022.\n\nPolicy gradient methods for reinforcement learning with function approximation. R S Sutton, D Mcallester, S Singh, Y Mansour, Advances in Neural Information Processing Systems. MIT Press12R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12, volume 12, pp. 1057-1063. MIT Press, 2000.\n\nReinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, The MIT Presssecond editionRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd. html.\n\nTrl: Transformer reinforcement learning. Younes Leandro Von Werra, Lewis Belkada, Edward Tunstall, Tristan Beeching, Nathan Thrush, Lambert, Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, and Nathan Lambert. Trl: Transformer reinforcement learning. https://github.com/lvwerra/trl, 2020.\n\nChain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.\n\nReAct: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR. 2023Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.\n",
            "annotations": {
                "author": "[{\"end\":122,\"start\":86},{\"end\":164,\"start\":123},{\"end\":210,\"start\":165},{\"end\":247,\"start\":211},{\"end\":284,\"start\":248},{\"end\":317,\"start\":285},{\"end\":358,\"start\":318},{\"end\":396,\"start\":359},{\"end\":436,\"start\":397},{\"end\":476,\"start\":437},{\"end\":509,\"start\":477},{\"end\":544,\"start\":510},{\"end\":580,\"start\":545},{\"end\":642,\"start\":581},{\"end\":709,\"start\":643},{\"end\":755,\"start\":710}]",
                "publisher": null,
                "author_last_name": "[{\"end\":96,\"start\":93},{\"end\":138,\"start\":130},{\"end\":184,\"start\":170},{\"end\":221,\"start\":218},{\"end\":258,\"start\":254},{\"end\":291,\"start\":288},{\"end\":332,\"start\":326},{\"end\":370,\"start\":366},{\"end\":410,\"start\":405},{\"end\":450,\"start\":445},{\"end\":483,\"start\":481},{\"end\":518,\"start\":515},{\"end\":554,\"start\":550},{\"end\":594,\"start\":589},{\"end\":658,\"start\":650},{\"end\":729,\"start\":721}]",
                "author_first_name": "[{\"end\":92,\"start\":86},{\"end\":129,\"start\":123},{\"end\":169,\"start\":165},{\"end\":217,\"start\":211},{\"end\":253,\"start\":248},{\"end\":287,\"start\":285},{\"end\":325,\"start\":318},{\"end\":365,\"start\":359},{\"end\":404,\"start\":397},{\"end\":444,\"start\":437},{\"end\":480,\"start\":477},{\"end\":514,\"start\":510},{\"end\":549,\"start\":545},{\"end\":588,\"start\":581},{\"end\":649,\"start\":643},{\"end\":720,\"start\":710}]",
                "author_affiliation": "[{\"end\":121,\"start\":98},{\"end\":163,\"start\":140},{\"end\":209,\"start\":186},{\"end\":246,\"start\":223},{\"end\":283,\"start\":260},{\"end\":316,\"start\":293},{\"end\":357,\"start\":334},{\"end\":395,\"start\":372},{\"end\":435,\"start\":412},{\"end\":475,\"start\":452},{\"end\":508,\"start\":485},{\"end\":543,\"start\":520},{\"end\":579,\"start\":556},{\"end\":641,\"start\":618},{\"end\":708,\"start\":685},{\"end\":754,\"start\":731}]",
                "title": "[{\"end\":83,\"start\":1},{\"end\":838,\"start\":756}]",
                "venue": null,
                "abstract": "[{\"end\":4155,\"start\":840}]",
                "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4512,\"start\":4494},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4546,\"start\":4525},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4579,\"start\":4560},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4618,\"start\":4599},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4648,\"start\":4627},{\"end\":4674,\"start\":4650},{\"end\":4700,\"start\":4684},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4729,\"start\":4716},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5316,\"start\":5296},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5390,\"start\":5368},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5431,\"start\":5412},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6420,\"start\":6401},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6906,\"start\":6887},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8156,\"start\":8138},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8305,\"start\":8287},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8691,\"start\":8672},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8730,\"start\":8711},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8760,\"start\":8739},{\"end\":8786,\"start\":8762},{\"end\":8812,\"start\":8796},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8841,\"start\":8828},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8862,\"start\":8844},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8909,\"start\":8887},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8944,\"start\":8926},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9289,\"start\":9267},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9581,\"start\":9563},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9902,\"start\":9883},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10828,\"start\":10805},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10921,\"start\":10901},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10987,\"start\":10964},{\"end\":11212,\"start\":11196},{\"end\":11227,\"start\":11212},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11238,\"start\":11227},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12425,\"start\":12407},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12442,\"start\":12425},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13360,\"start\":13339},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13430,\"start\":13407},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13495,\"start\":13472},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13551,\"start\":13532},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19300,\"start\":19280},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19873,\"start\":19855},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25057,\"start\":25039},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25959,\"start\":25938},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27391,\"start\":27372},{\"end\":27740,\"start\":27732},{\"end\":27908,\"start\":27899},{\"end\":28029,\"start\":28021},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28955,\"start\":28937},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28974,\"start\":28955},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30461,\"start\":30443},{\"end\":30533,\"start\":30509},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30788,\"start\":30768},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32014,\"start\":32002}]",
                "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37540,\"start\":37508},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37675,\"start\":37541},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37772,\"start\":37676},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37988,\"start\":37773},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38533,\"start\":37989},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40403,\"start\":38534}]",
                "paragraph": "[{\"end\":5030,\"start\":4171},{\"end\":6302,\"start\":5032},{\"end\":7051,\"start\":6304},{\"end\":7103,\"start\":7053},{\"end\":7288,\"start\":7105},{\"end\":7786,\"start\":7290},{\"end\":8946,\"start\":7803},{\"end\":10192,\"start\":9037},{\"end\":11093,\"start\":10194},{\"end\":11501,\"start\":11108},{\"end\":11749,\"start\":11522},{\"end\":12291,\"start\":11751},{\"end\":12610,\"start\":12293},{\"end\":13041,\"start\":12612},{\"end\":13552,\"start\":13055},{\"end\":14432,\"start\":13554},{\"end\":14985,\"start\":14434},{\"end\":15485,\"start\":15014},{\"end\":15724,\"start\":15487},{\"end\":16030,\"start\":15726},{\"end\":16566,\"start\":16032},{\"end\":16797,\"start\":16568},{\"end\":17024,\"start\":16848},{\"end\":17574,\"start\":17163},{\"end\":17999,\"start\":17633},{\"end\":18858,\"start\":18196},{\"end\":19403,\"start\":19112},{\"end\":19894,\"start\":19405},{\"end\":20458,\"start\":19964},{\"end\":21252,\"start\":20548},{\"end\":21423,\"start\":21254},{\"end\":21704,\"start\":21425},{\"end\":22440,\"start\":21706},{\"end\":22632,\"start\":22442},{\"end\":22726,\"start\":22634},{\"end\":22920,\"start\":22728},{\"end\":23924,\"start\":22953},{\"end\":24657,\"start\":23983},{\"end\":24958,\"start\":24659},{\"end\":25563,\"start\":24960},{\"end\":25994,\"start\":25605},{\"end\":26333,\"start\":26078},{\"end\":27108,\"start\":26335},{\"end\":27316,\"start\":27124},{\"end\":27537,\"start\":27344},{\"end\":27720,\"start\":27539},{\"end\":28078,\"start\":27722},{\"end\":28283,\"start\":28080},{\"end\":28975,\"start\":28310},{\"end\":29517,\"start\":28977},{\"end\":29656,\"start\":29519},{\"end\":30141,\"start\":29658},{\"end\":30373,\"start\":30164},{\"end\":30433,\"start\":30375},{\"end\":30754,\"start\":30435},{\"end\":31115,\"start\":30756},{\"end\":32237,\"start\":31146},{\"end\":32674,\"start\":32286},{\"end\":33787,\"start\":32676},{\"end\":34258,\"start\":33789},{\"end\":34587,\"start\":34260},{\"end\":34742,\"start\":34589},{\"end\":35912,\"start\":34744},{\"end\":36472,\"start\":35927},{\"end\":37507,\"start\":36474}]",
                "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9036,\"start\":8947},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16847,\"start\":16798},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17162,\"start\":17025},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18149,\"start\":18022},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19076,\"start\":18859},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19963,\"start\":19895},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20547,\"start\":20459},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25604,\"start\":25564},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26077,\"start\":25995}]",
                "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":7853,\"start\":7846},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31203,\"start\":31196}]",
                "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4169,\"start\":4157},{\"attributes\":{\"n\":\"2\"},\"end\":7801,\"start\":7789},{\"attributes\":{\"n\":\"3\"},\"end\":11106,\"start\":11096},{\"end\":11520,\"start\":11504},{\"attributes\":{\"n\":\"4\"},\"end\":13053,\"start\":13044},{\"attributes\":{\"n\":\"5\"},\"end\":15012,\"start\":14988},{\"attributes\":{\"n\":\"6\"},\"end\":17631,\"start\":17577},{\"end\":18021,\"start\":18002},{\"end\":18167,\"start\":18151},{\"end\":18185,\"start\":18170},{\"end\":18194,\"start\":18188},{\"attributes\":{\"n\":\"6.1\"},\"end\":19110,\"start\":19078},{\"attributes\":{\"n\":\"6.2\"},\"end\":22951,\"start\":22923},{\"end\":23943,\"start\":23927},{\"end\":23981,\"start\":23946},{\"attributes\":{\"n\":\"7\"},\"end\":27122,\"start\":27111},{\"attributes\":{\"n\":\"7.1\"},\"end\":27342,\"start\":27319},{\"attributes\":{\"n\":\"7.2\"},\"end\":28308,\"start\":28286},{\"end\":30162,\"start\":30144},{\"attributes\":{\"n\":\"7.3\"},\"end\":31144,\"start\":31118},{\"attributes\":{\"n\":\"7.4\"},\"end\":32284,\"start\":32240},{\"attributes\":{\"n\":\"8\"},\"end\":35925,\"start\":35915},{\"end\":37519,\"start\":37509},{\"end\":37552,\"start\":37542},{\"end\":37687,\"start\":37677},{\"end\":37783,\"start\":37774},{\"end\":37999,\"start\":37990}]",
                "table": "[{\"end\":37988,\"start\":37823},{\"end\":38533,\"start\":38012}]",
                "figure_caption": "[{\"end\":37540,\"start\":37521},{\"end\":37675,\"start\":37554},{\"end\":37772,\"start\":37689},{\"end\":37823,\"start\":37785},{\"end\":38012,\"start\":38001},{\"end\":40403,\"start\":38536}]",
                "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17657,\"start\":17651},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19139,\"start\":19130},{\"end\":23844,\"start\":23836},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24713,\"start\":24704},{\"end\":25718,\"start\":25712},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31250,\"start\":31244},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31585,\"start\":31579},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32673,\"start\":32667},{\"end\":32739,\"start\":32733},{\"end\":33604,\"start\":33596},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35505,\"start\":35499},{\"end\":35747,\"start\":35741}]",
                "bib_author_first_name": "[{\"end\":42517,\"start\":42510},{\"end\":42530,\"start\":42523},{\"end\":42543,\"start\":42539},{\"end\":42557,\"start\":42551},{\"end\":42572,\"start\":42568},{\"end\":42586,\"start\":42581},{\"end\":42601,\"start\":42594},{\"end\":42617,\"start\":42608},{\"end\":42639,\"start\":42634},{\"end\":42653,\"start\":42649},{\"end\":42971,\"start\":42963},{\"end\":43194,\"start\":43189},{\"end\":43202,\"start\":43200},{\"end\":43212,\"start\":43207},{\"end\":43223,\"start\":43217},{\"end\":43230,\"start\":43224},{\"end\":43241,\"start\":43237},{\"end\":43253,\"start\":43248},{\"end\":43257,\"start\":43254},{\"end\":43271,\"start\":43264},{\"end\":43625,\"start\":43621},{\"end\":43821,\"start\":43817},{\"end\":43839,\"start\":43830},{\"end\":43852,\"start\":43849},{\"end\":43864,\"start\":43859},{\"end\":43887,\"start\":43874},{\"end\":43907,\"start\":43901},{\"end\":43920,\"start\":43914},{\"end\":43933,\"start\":43929},{\"end\":44255,\"start\":44251},{\"end\":44269,\"start\":44264},{\"end\":44285,\"start\":44278},{\"end\":44299,\"start\":44293},{\"end\":44314,\"start\":44310},{\"end\":44325,\"start\":44320},{\"end\":44340,\"start\":44337},{\"end\":44352,\"start\":44347},{\"end\":44367,\"start\":44360},{\"end\":44386,\"start\":44380},{\"end\":44730,\"start\":44721},{\"end\":44742,\"start\":44737},{\"end\":44755,\"start\":44743},{\"end\":44768,\"start\":44763},{\"end\":44780,\"start\":44776},{\"end\":44796,\"start\":44789},{\"end\":44798,\"start\":44797},{\"end\":44813,\"start\":44810},{\"end\":44827,\"start\":44822},{\"end\":44841,\"start\":44836},{\"end\":45102,\"start\":45097},{\"end\":45209,\"start\":45200},{\"end\":45223,\"start\":45218},{\"end\":45238,\"start\":45232},{\"end\":45251,\"start\":45247},{\"end\":45260,\"start\":45256},{\"end\":45278,\"start\":45269},{\"end\":45295,\"start\":45284},{\"end\":45311,\"start\":45303},{\"end\":45324,\"start\":45318},{\"end\":45342,\"start\":45335},{\"end\":45780,\"start\":45776},{\"end\":45796,\"start\":45789},{\"end\":45803,\"start\":45801},{\"end\":45816,\"start\":45811},{\"end\":45833,\"start\":45826},{\"end\":45852,\"start\":45846},{\"end\":45867,\"start\":45862},{\"end\":45883,\"start\":45875},{\"end\":45901,\"start\":45893},{\"end\":45913,\"start\":45909},{\"end\":46275,\"start\":46266},{\"end\":46283,\"start\":46282},{\"end\":46298,\"start\":46292},{\"end\":46300,\"start\":46299},{\"end\":46323,\"start\":46315},{\"end\":46330,\"start\":46324},{\"end\":46341,\"start\":46336},{\"end\":46359,\"start\":46350},{\"end\":46764,\"start\":46758},{\"end\":46781,\"start\":46775},{\"end\":46794,\"start\":46790},{\"end\":46812,\"start\":46805},{\"end\":46821,\"start\":46820},{\"end\":46842,\"start\":46835},{\"end\":47178,\"start\":47174},{\"end\":47191,\"start\":47187},{\"end\":47211,\"start\":47204},{\"end\":47226,\"start\":47219},{\"end\":47242,\"start\":47237},{\"end\":47255,\"start\":47251},{\"end\":47275,\"start\":47269},{\"end\":47292,\"start\":47286},{\"end\":47605,\"start\":47601},{\"end\":47622,\"start\":47616},{\"end\":47638,\"start\":47631},{\"end\":47654,\"start\":47647},{\"end\":47656,\"start\":47655},{\"end\":47671,\"start\":47665},{\"end\":47890,\"start\":47886},{\"end\":47906,\"start\":47901},{\"end\":47923,\"start\":47915},{\"end\":47938,\"start\":47934},{\"end\":47952,\"start\":47948},{\"end\":48211,\"start\":48202},{\"end\":48224,\"start\":48218},{\"end\":48233,\"start\":48231},{\"end\":48248,\"start\":48239},{\"end\":48260,\"start\":48253},{\"end\":48272,\"start\":48265},{\"end\":48575,\"start\":48571},{\"end\":48591,\"start\":48583},{\"end\":48605,\"start\":48601},{\"end\":48620,\"start\":48614},{\"end\":48638,\"start\":48631},{\"end\":48657,\"start\":48651},{\"end\":48977,\"start\":48970},{\"end\":48989,\"start\":48985},{\"end\":49003,\"start\":49001},{\"end\":49016,\"start\":49008},{\"end\":49029,\"start\":49023},{\"end\":49339,\"start\":49338},{\"end\":49341,\"start\":49340},{\"end\":49351,\"start\":49350},{\"end\":49365,\"start\":49364},{\"end\":49374,\"start\":49373},{\"end\":49730,\"start\":49723},{\"end\":49732,\"start\":49731},{\"end\":49747,\"start\":49741},{\"end\":49749,\"start\":49748},{\"end\":50005,\"start\":49999},{\"end\":50030,\"start\":50025},{\"end\":50046,\"start\":50040},{\"end\":50064,\"start\":50057},{\"end\":50081,\"start\":50075},{\"end\":50359,\"start\":50354},{\"end\":50371,\"start\":50365},{\"end\":50382,\"start\":50378},{\"end\":50402,\"start\":50395},{\"end\":50412,\"start\":50410},{\"end\":50422,\"start\":50418},{\"end\":50432,\"start\":50427},{\"end\":50750,\"start\":50744},{\"end\":50761,\"start\":50757},{\"end\":50774,\"start\":50766},{\"end\":50788,\"start\":50782},{\"end\":50804,\"start\":50797},{\"end\":50806,\"start\":50805},{\"end\":50820,\"start\":50814},{\"end\":50847,\"start\":50836},{\"end\":50849,\"start\":50848},{\"end\":51267,\"start\":51261},{\"end\":51280,\"start\":51273},{\"end\":51291,\"start\":51287},{\"end\":51299,\"start\":51296},{\"end\":51309,\"start\":51304},{\"end\":51326,\"start\":51319},{\"end\":51343,\"start\":51339}]",
                "bib_author_last_name": "[{\"end\":42521,\"start\":42518},{\"end\":42537,\"start\":42531},{\"end\":42549,\"start\":42544},{\"end\":42566,\"start\":42558},{\"end\":42579,\"start\":42573},{\"end\":42592,\"start\":42587},{\"end\":42606,\"start\":42602},{\"end\":42632,\"start\":42618},{\"end\":42647,\"start\":42640},{\"end\":42660,\"start\":42654},{\"end\":42977,\"start\":42972},{\"end\":42988,\"start\":42979},{\"end\":43198,\"start\":43195},{\"end\":43205,\"start\":43203},{\"end\":43215,\"start\":43213},{\"end\":43235,\"start\":43231},{\"end\":43246,\"start\":43242},{\"end\":43262,\"start\":43258},{\"end\":43274,\"start\":43272},{\"end\":43630,\"start\":43626},{\"end\":43828,\"start\":43822},{\"end\":43847,\"start\":43840},{\"end\":43857,\"start\":43853},{\"end\":43872,\"start\":43865},{\"end\":43899,\"start\":43888},{\"end\":43912,\"start\":43908},{\"end\":43927,\"start\":43921},{\"end\":43946,\"start\":43934},{\"end\":44262,\"start\":44256},{\"end\":44276,\"start\":44270},{\"end\":44291,\"start\":44286},{\"end\":44308,\"start\":44300},{\"end\":44318,\"start\":44315},{\"end\":44335,\"start\":44326},{\"end\":44345,\"start\":44341},{\"end\":44358,\"start\":44353},{\"end\":44378,\"start\":44368},{\"end\":44391,\"start\":44387},{\"end\":44735,\"start\":44731},{\"end\":44761,\"start\":44756},{\"end\":44774,\"start\":44769},{\"end\":44787,\"start\":44781},{\"end\":44808,\"start\":44799},{\"end\":44820,\"start\":44814},{\"end\":44834,\"start\":44828},{\"end\":44853,\"start\":44842},{\"end\":45111,\"start\":45103},{\"end\":45120,\"start\":45113},{\"end\":45216,\"start\":45210},{\"end\":45230,\"start\":45224},{\"end\":45245,\"start\":45239},{\"end\":45254,\"start\":45252},{\"end\":45267,\"start\":45261},{\"end\":45282,\"start\":45279},{\"end\":45301,\"start\":45296},{\"end\":45316,\"start\":45312},{\"end\":45333,\"start\":45325},{\"end\":45351,\"start\":45343},{\"end\":45787,\"start\":45781},{\"end\":45799,\"start\":45797},{\"end\":45809,\"start\":45804},{\"end\":45824,\"start\":45817},{\"end\":45844,\"start\":45834},{\"end\":45860,\"start\":45853},{\"end\":45873,\"start\":45868},{\"end\":45891,\"start\":45884},{\"end\":45907,\"start\":45902},{\"end\":45917,\"start\":45914},{\"end\":46280,\"start\":46276},{\"end\":46290,\"start\":46284},{\"end\":46313,\"start\":46301},{\"end\":46334,\"start\":46331},{\"end\":46348,\"start\":46342},{\"end\":46365,\"start\":46360},{\"end\":46376,\"start\":46367},{\"end\":46773,\"start\":46765},{\"end\":46788,\"start\":46782},{\"end\":46803,\"start\":46795},{\"end\":46818,\"start\":46813},{\"end\":46833,\"start\":46822},{\"end\":46850,\"start\":46843},{\"end\":46856,\"start\":46852},{\"end\":47185,\"start\":47179},{\"end\":47202,\"start\":47192},{\"end\":47217,\"start\":47212},{\"end\":47235,\"start\":47227},{\"end\":47249,\"start\":47243},{\"end\":47267,\"start\":47256},{\"end\":47284,\"start\":47276},{\"end\":47300,\"start\":47293},{\"end\":47614,\"start\":47606},{\"end\":47629,\"start\":47623},{\"end\":47645,\"start\":47639},{\"end\":47663,\"start\":47657},{\"end\":47678,\"start\":47672},{\"end\":47899,\"start\":47891},{\"end\":47913,\"start\":47907},{\"end\":47932,\"start\":47924},{\"end\":47946,\"start\":47939},{\"end\":47959,\"start\":47953},{\"end\":48216,\"start\":48212},{\"end\":48229,\"start\":48225},{\"end\":48237,\"start\":48234},{\"end\":48251,\"start\":48249},{\"end\":48263,\"start\":48261},{\"end\":48279,\"start\":48273},{\"end\":48581,\"start\":48576},{\"end\":48599,\"start\":48592},{\"end\":48612,\"start\":48606},{\"end\":48629,\"start\":48621},{\"end\":48649,\"start\":48639},{\"end\":48661,\"start\":48658},{\"end\":48983,\"start\":48978},{\"end\":48999,\"start\":48990},{\"end\":49006,\"start\":49004},{\"end\":49021,\"start\":49017},{\"end\":49036,\"start\":49030},{\"end\":49348,\"start\":49342},{\"end\":49362,\"start\":49352},{\"end\":49371,\"start\":49366},{\"end\":49382,\"start\":49375},{\"end\":49739,\"start\":49733},{\"end\":49755,\"start\":49750},{\"end\":50023,\"start\":50006},{\"end\":50038,\"start\":50031},{\"end\":50055,\"start\":50047},{\"end\":50073,\"start\":50065},{\"end\":50088,\"start\":50082},{\"end\":50097,\"start\":50090},{\"end\":50363,\"start\":50360},{\"end\":50376,\"start\":50372},{\"end\":50393,\"start\":50383},{\"end\":50408,\"start\":50403},{\"end\":50416,\"start\":50413},{\"end\":50425,\"start\":50423},{\"end\":50437,\"start\":50433},{\"end\":50755,\"start\":50751},{\"end\":50764,\"start\":50762},{\"end\":50780,\"start\":50775},{\"end\":50795,\"start\":50789},{\"end\":50812,\"start\":50807},{\"end\":50834,\"start\":50821},{\"end\":50857,\"start\":50850},{\"end\":51271,\"start\":51268},{\"end\":51285,\"start\":51281},{\"end\":51294,\"start\":51292},{\"end\":51302,\"start\":51300},{\"end\":51317,\"start\":51310},{\"end\":51337,\"start\":51327},{\"end\":51347,\"start\":51344}]",
                "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2204.01691\",\"id\":\"b0\"},\"end\":42959,\"start\":42440},{\"attributes\":{\"id\":\"b1\"},\"end\":43187,\"start\":42961},{\"attributes\":{\"doi\":\"arXiv:2305.14992\",\"id\":\"b2\"},\"end\":43555,\"start\":43189},{\"attributes\":{\"id\":\"b3\"},\"end\":43772,\"start\":43557},{\"attributes\":{\"doi\":\"arXiv:2302.07867\",\"id\":\"b4\"},\"end\":44195,\"start\":43774},{\"attributes\":{\"doi\":\"arXiv:2303.17651\",\"id\":\"b5\"},\"end\":44665,\"start\":44197},{\"attributes\":{\"doi\":\"abs/1602.01783\",\"id\":\"b6\"},\"end\":45093,\"start\":44667},{\"attributes\":{\"id\":\"b7\"},\"end\":45198,\"start\":45095},{\"attributes\":{\"doi\":\"arXiv:2112.09332\",\"id\":\"b8\"},\"end\":45705,\"start\":45200},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":246426909},\"end\":46264,\"start\":45707},{\"attributes\":{\"doi\":\"arXiv:2304.03442\",\"id\":\"b10\"},\"end\":46676,\"start\":46266},{\"attributes\":{\"doi\":\"arXiv:2305.18290\",\"id\":\"b11\"},\"end\":47109,\"start\":46678},{\"attributes\":{\"doi\":\"arXiv:2302.04761\",\"id\":\"b12\"},\"end\":47565,\"start\":47111},{\"attributes\":{\"doi\":\"abs/1502.05477\",\"id\":\"b13\"},\"end\":47843,\"start\":47567},{\"attributes\":{\"doi\":\"abs/1707.06347\",\"id\":\"b14\"},\"end\":48126,\"start\":47845},{\"attributes\":{\"doi\":\"arXiv:2303.17580\",\"id\":\"b15\"},\"end\":48506,\"start\":48128},{\"attributes\":{\"doi\":\"arXiv:2303.11366\",\"id\":\"b16\"},\"end\":48890,\"start\":48508},{\"attributes\":{\"doi\":\"arXiv:2206.11871\",\"id\":\"b17\"},\"end\":49256,\"start\":48892},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1211821},\"end\":49680,\"start\":49258},{\"attributes\":{\"id\":\"b19\"},\"end\":49956,\"start\":49682},{\"attributes\":{\"id\":\"b20\"},\"end\":50281,\"start\":49958},{\"attributes\":{\"doi\":\"arXiv:2201.11903\",\"id\":\"b21\"},\"end\":50667,\"start\":50283},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52822214},\"end\":51199,\"start\":50669},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":252762395},\"end\":51634,\"start\":51201}]",
                "bib_title": "[{\"end\":45774,\"start\":45707},{\"end\":49336,\"start\":49258},{\"end\":50742,\"start\":50669},{\"end\":51259,\"start\":51201}]",
                "bib_author": "[{\"end\":42523,\"start\":42510},{\"end\":42539,\"start\":42523},{\"end\":42551,\"start\":42539},{\"end\":42568,\"start\":42551},{\"end\":42581,\"start\":42568},{\"end\":42594,\"start\":42581},{\"end\":42608,\"start\":42594},{\"end\":42634,\"start\":42608},{\"end\":42649,\"start\":42634},{\"end\":42662,\"start\":42649},{\"end\":42979,\"start\":42963},{\"end\":42990,\"start\":42979},{\"end\":43200,\"start\":43189},{\"end\":43207,\"start\":43200},{\"end\":43217,\"start\":43207},{\"end\":43237,\"start\":43217},{\"end\":43248,\"start\":43237},{\"end\":43264,\"start\":43248},{\"end\":43276,\"start\":43264},{\"end\":43632,\"start\":43621},{\"end\":43830,\"start\":43817},{\"end\":43849,\"start\":43830},{\"end\":43859,\"start\":43849},{\"end\":43874,\"start\":43859},{\"end\":43901,\"start\":43874},{\"end\":43914,\"start\":43901},{\"end\":43929,\"start\":43914},{\"end\":43948,\"start\":43929},{\"end\":44264,\"start\":44251},{\"end\":44278,\"start\":44264},{\"end\":44293,\"start\":44278},{\"end\":44310,\"start\":44293},{\"end\":44320,\"start\":44310},{\"end\":44337,\"start\":44320},{\"end\":44347,\"start\":44337},{\"end\":44360,\"start\":44347},{\"end\":44380,\"start\":44360},{\"end\":44393,\"start\":44380},{\"end\":44737,\"start\":44721},{\"end\":44763,\"start\":44737},{\"end\":44776,\"start\":44763},{\"end\":44789,\"start\":44776},{\"end\":44810,\"start\":44789},{\"end\":44822,\"start\":44810},{\"end\":44836,\"start\":44822},{\"end\":44855,\"start\":44836},{\"end\":45113,\"start\":45097},{\"end\":45122,\"start\":45113},{\"end\":45218,\"start\":45200},{\"end\":45232,\"start\":45218},{\"end\":45247,\"start\":45232},{\"end\":45256,\"start\":45247},{\"end\":45269,\"start\":45256},{\"end\":45284,\"start\":45269},{\"end\":45303,\"start\":45284},{\"end\":45318,\"start\":45303},{\"end\":45335,\"start\":45318},{\"end\":45353,\"start\":45335},{\"end\":45789,\"start\":45776},{\"end\":45801,\"start\":45789},{\"end\":45811,\"start\":45801},{\"end\":45826,\"start\":45811},{\"end\":45846,\"start\":45826},{\"end\":45862,\"start\":45846},{\"end\":45875,\"start\":45862},{\"end\":45893,\"start\":45875},{\"end\":45909,\"start\":45893},{\"end\":45919,\"start\":45909},{\"end\":46282,\"start\":46266},{\"end\":46292,\"start\":46282},{\"end\":46315,\"start\":46292},{\"end\":46336,\"start\":46315},{\"end\":46350,\"start\":46336},{\"end\":46367,\"start\":46350},{\"end\":46378,\"start\":46367},{\"end\":46775,\"start\":46758},{\"end\":46790,\"start\":46775},{\"end\":46805,\"start\":46790},{\"end\":46820,\"start\":46805},{\"end\":46835,\"start\":46820},{\"end\":46852,\"start\":46835},{\"end\":46858,\"start\":46852},{\"end\":47187,\"start\":47174},{\"end\":47204,\"start\":47187},{\"end\":47219,\"start\":47204},{\"end\":47237,\"start\":47219},{\"end\":47251,\"start\":47237},{\"end\":47269,\"start\":47251},{\"end\":47286,\"start\":47269},{\"end\":47302,\"start\":47286},{\"end\":47616,\"start\":47601},{\"end\":47631,\"start\":47616},{\"end\":47647,\"start\":47631},{\"end\":47665,\"start\":47647},{\"end\":47680,\"start\":47665},{\"end\":47901,\"start\":47886},{\"end\":47915,\"start\":47901},{\"end\":47934,\"start\":47915},{\"end\":47948,\"start\":47934},{\"end\":47961,\"start\":47948},{\"end\":48218,\"start\":48202},{\"end\":48231,\"start\":48218},{\"end\":48239,\"start\":48231},{\"end\":48253,\"start\":48239},{\"end\":48265,\"start\":48253},{\"end\":48281,\"start\":48265},{\"end\":48583,\"start\":48571},{\"end\":48601,\"start\":48583},{\"end\":48614,\"start\":48601},{\"end\":48631,\"start\":48614},{\"end\":48651,\"start\":48631},{\"end\":48663,\"start\":48651},{\"end\":48985,\"start\":48970},{\"end\":49001,\"start\":48985},{\"end\":49008,\"start\":49001},{\"end\":49023,\"start\":49008},{\"end\":49038,\"start\":49023},{\"end\":49350,\"start\":49338},{\"end\":49364,\"start\":49350},{\"end\":49373,\"start\":49364},{\"end\":49384,\"start\":49373},{\"end\":49741,\"start\":49723},{\"end\":49757,\"start\":49741},{\"end\":50025,\"start\":49999},{\"end\":50040,\"start\":50025},{\"end\":50057,\"start\":50040},{\"end\":50075,\"start\":50057},{\"end\":50090,\"start\":50075},{\"end\":50099,\"start\":50090},{\"end\":50365,\"start\":50354},{\"end\":50378,\"start\":50365},{\"end\":50395,\"start\":50378},{\"end\":50410,\"start\":50395},{\"end\":50418,\"start\":50410},{\"end\":50427,\"start\":50418},{\"end\":50439,\"start\":50427},{\"end\":50757,\"start\":50744},{\"end\":50766,\"start\":50757},{\"end\":50782,\"start\":50766},{\"end\":50797,\"start\":50782},{\"end\":50814,\"start\":50797},{\"end\":50836,\"start\":50814},{\"end\":50859,\"start\":50836},{\"end\":51273,\"start\":51261},{\"end\":51287,\"start\":51273},{\"end\":51296,\"start\":51287},{\"end\":51304,\"start\":51296},{\"end\":51319,\"start\":51304},{\"end\":51339,\"start\":51319},{\"end\":51349,\"start\":51339}]",
                "bib_venue": "[{\"end\":42508,\"start\":42440},{\"end\":43025,\"start\":42990},{\"end\":43350,\"start\":43292},{\"end\":43619,\"start\":43557},{\"end\":43815,\"start\":43774},{\"end\":44249,\"start\":44197},{\"end\":44719,\"start\":44667},{\"end\":45424,\"start\":45369},{\"end\":45968,\"start\":45919},{\"end\":46452,\"start\":46394},{\"end\":46756,\"start\":46678},{\"end\":47172,\"start\":47111},{\"end\":47599,\"start\":47567},{\"end\":47884,\"start\":47845},{\"end\":48200,\"start\":48128},{\"end\":48569,\"start\":48508},{\"end\":48968,\"start\":48892},{\"end\":49433,\"start\":49384},{\"end\":49721,\"start\":49682},{\"end\":49997,\"start\":49958},{\"end\":50352,\"start\":50283},{\"end\":50921,\"start\":50859},{\"end\":51407,\"start\":51349}]"
            }
        }
    },
    "year": 2023,
    "month": 12,
    "day": 17
}