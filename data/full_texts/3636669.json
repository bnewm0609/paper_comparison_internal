{"id": 3636669, "updated": "2023-09-29 13:29:08.301", "metadata": {"title": "Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications", "authors": "[{\"first\":\"Haowen\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Wenxiao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Nengwen\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Zeyan\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jiahao\",\"last\":\"Bu\",\"middle\":[]},{\"first\":\"Zhihan\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Youjian\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Pei\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Zhaogang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Honglin\",\"last\":\"University\",\"middle\":[\"Qiao\",\"Tsinghua\"]},{\"first\":\"Alibaba\",\"last\":\"Group\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 2018 World Wide Web Conference", "publication_date": {"year": 2018, "month": 2, "day": 12}, "abstract": "To ensure undisrupted business, large Internet companies need to closely monitor various KPIs (e.g., Page Views, number of online users, and number of orders) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. However, anomaly detection for these seasonal KPIs with various patterns and data quality has been a great challenge, especially without labels. In this paper, we proposed Donut, an unsupervised anomaly detection algorithm based on VAE. Thanks to a few of our key techniques, Donut greatly outperforms a state-of-arts supervised ensemble approach and a baseline VAE approach, and its best F-scores range from 0.75 to 0.9 for the studied KPIs from a top global Internet company. We come up with a novel KDE interpretation of reconstruction for Donut, making it the first VAE-based anomaly detection algorithm with solid theoretical explanation.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1802.03903", "mag": "3098957257", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/www/XuCZLBLLZPFCWQ18", "doi": "10.1145/3178876.3185996"}}, "content": {"source": {"pdf_hash": "239a8581a2baa06e310971ec0612aa63d0d3b121", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1802.03903v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://dl.acm.org/ft_gateway.cfm?id=3185996&type=pdf", "status": "BRONZE"}}, "grobid": {"id": "880456f434306b0a4fa679a0829007e367ecb27c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/239a8581a2baa06e310971ec0612aa63d0d3b121.txt", "contents": "\nUnsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications\n2018. April 23-27. 2018\n\nHaowen Xu \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nWenxiao Chen \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nNengwen Zhao \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nZeyan Li \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nJiahao Bu \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nZhihan Li \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nYing Liu \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nYoujian Zhao \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nDan Pei \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nYang Feng \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nJie Chen \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nZhaogang Wang \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nHonglin Qiao \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nHaowen Xu \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nWenxiao Chen \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nNengwen Zhao \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nZeyan Li \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nJiahao Bu \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nZhihan Li \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nYing Liu \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nYoujian Zhao \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nDan Pei \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nYang Feng \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nJie Chen \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nZhaogang Wang \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nHonglin Qiao \nAlibaba Group\nACM Reference Format\nTsinghua University\n\n\nUnsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications\n\nWWW 2018: The 2018 Web Conference\nLyon2018. April 23-27. 201810.1145/3178876.3185996, France. ACM, New York, NY, USA, 12 pages. https://doi.org/https://CCS CONCEPTS \u2022 Computing methodologies \u2192 Anomaly detection\u2022 Infor- mation systems \u2192 Traffic analysisKEYWORDS variational auto-encoderanomaly detectionseasonal KPI\nTo ensure undisrupted business, large Internet companies need to closely monitor various KPIs (e.g., Page Views, number of online users, and number of orders) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. However, anomaly detection for these seasonal KPIs with various patterns and data quality has been a great challenge, especially without labels. In this paper, we proposed Donut, an unsupervised anomaly detection algorithm based on VAE. Thanks to a few of our key techniques, Donut 1 greatly outperforms a state-of-arts supervised ensemble approach and a baseline VAE approach, and its best F-scores range from 0.75 to 0.9 for the studied KPIs from a top global Internet company. We come up with a novel KDE interpretation of reconstruction for Donut, making it the first VAE-based anomaly detection algorithm with solid theoretical explanation.\n\nINTRODUCTION\n\nTo ensure undisrupted business, large Internet companies need to closely monitor various KPIs (key performance indicators) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. KPIs are time series data, measuring metrics such as Page Views, number of online users, and number of orders. Among all KPIs, the most ones are business-related KPIs (the focus of this paper), which are heavily influenced by user behavior and schedule, thus roughly have seasonal patterns occurring at regular intervals (e.g., daily and/or weekly). However, anomaly detection for these seasonal KPIs with various patterns and data quality has been a great challenge, especially without labels.\n\nA rich body of literature exist on detecting KPI anomalies [1, 2, 5-8, 17, 18, 21, 23-27, 29, 31, 35, 36, 40, 41]. As discussed in \u00a7 2.2, existing anomaly detection algorithms suffer from the hassle of algorithm picking/parameter tuning, heavy reliance on labels, unsatisfying performance, and/or lack of theoretical foundations.\n\nIn this paper, we propose Donut, an unsupervised anomaly detection algorithm based on Variational Auto-Encoder (a representative deep generative model) with solid theoretical explanation, and this algorithm can work when there are no labels at all, and can take advantage of the occasional labels when available.\n\nThe contributions of this paper can be summarized as follows.\n\n\u2022 The three techniques in Donut, Modified ELBO and Missing Data Injection for training, and MCMC Imputation for detection, enable it to greatly outperform state-of-art supervised and VAE-based anomaly detection algorithms. The best Fscores of unsupervised Donut range from 0.75 to 0.9 for the studied KPIs from a top global Internet company. \u2022 For the first time in the literature, we discover that adopting VAE (or generative models in general) for anomaly detection requires training on both normal data and abnormal data, contrary to common intuition. \u2022 We propose a novel KDE interpretation in z-space for Donut, making it the first VAE-based anomaly detection algorithm with solid theoretical explanation unlike [2,36]. This interpretation may benefit the design of other deep generative models in anomaly detection. We discover a time gradient effect in latent z-space, which nicely explain Donut's excellent performance for detecting anomalies in seasonal KPIs. Decomposition [6]. An anomaly detection algorithm may not work well unless these local variations are properly handled. In addition to the seasonal patterns and local variations of the KPI shapes, there are also noises on these KPIs, which we assume to be independent, zero-mean Gaussian at every point. The exact values of the Gaussian noises are meaningless, thus we only focus on the statistics of these noises, i.e., the variances of the noises.\n\nWe can now formalize the \"normal patterns\" of seasonal KPIs as a combination of two components: (1) the seasonal patterns with local variations, and (2) the statistics of the Gaussian noises.\n\nWe use \"anomalies\" to denote the recorded points which do not follow normal patterns (e.g., sudden spikes and dips) , while using \"abnormal\" to denote both anomalies and missing points. See  Fig 1 for examples of both anomalies and missing points. Because the KPIs are monitored periodically (e.g., every minute), missing points are recorded as \"null\" (when the monitoring system does not receive the data) and thus are straightforward to identify. We thus focus on detecting anomalies for the KPIs.\n\nBecause operators need to deal with the anomalies for troubleshooting/mitigation, some of the anomalies are anecdotally labeled. Note that such occasional labels' coverage of anomalies are far from what's needed for typical supervised learning algorithms.\n\nAnomaly detection on KPIs can be formulated as follows: for any time t, given historical observations x t \u2212T +1 , . . . , x t , determine whether an anomaly occurs (denoted by y t = 1). An anomaly detection algorithm typically computes a real-valued score indicating the certainty of having y t = 1, e.g., p(y t = 1|x t \u2212T +1 , . . . , x t ), instead of directly computing y t . Human operators can then affect whether to declare an anomaly by choosing a threshold, where a data point with a score exceeding this threshold indicates an anomaly.\n\n\nPrevious Work\n\nTraditional statistical models. Over the years, quite a few anomaly detectors based on traditional statistical models (e.g., [6,17,18,24,26,27,31,40,41], mostly time series models) have been proposed to compute anomaly scores. Because these algorithms typically have simple assumptions for applicable KPIs, expert's efforts need to be involved to pick a suitable detector for a given KPI, and then fine-tune the detector's parameters based on the training data. Simple ensemble of these detectors, such as majority vote [8] and normalization [35], do not help much either according to [25]. As a result, these detectors see only limited use in the practice.\n\nSupervised ensemble approaches. To circumvent the hassle of algorithm/parameter tuning for traditional statistical anomaly detectors, supervised ensemble approaches, EGADS [21] and Opprentice [25], were proposed. They train anomaly classifiers using the user feedbacks as labels and using anomaly scores output by traditional detectors as features. Both EGADS and Opprentice showed promising results, but they heavily rely on good labels (much more than the anecdotal labels accumulated in our context), which is generally not feasible in large scale applications. Furthermore, running multiple traditional detectors to extract features during detection introduces lots of computational cost, which is a practical concern.\n\nUnsupervised approaches and deep generative models. Recently, there is a rising trend of adopting unsupervised machine learning algorithms for anomaly detection, e.g., one-class SVM [1,7], clustering based methods [9] like K-Means [28] and GMM [23], KDE [29], and VAE [2] and VRNN [36]. The philosophy is to focus on normal patterns instead of anomalies: since the KPIs are typically composed mostly of normal data, models can be readily trained even without labels. Roughly speaking, they all first recognize \"normal\" regions in the original or some latent feature space, and then compute the anomaly score by measuring \"how far\" an observation is from the normal regions.\n\nAlong this direction, we are interested in deep generative models for the following reasons. First, learning normal patterns can be seen as learning the distribution of training data, which is a topic of generative models. Second, great advances have been achieved recently to train generative models with deep learning techniques, e.g., GAN [13] and deep Bayesian network [4,39]. The latter is family of deep generative models, which adopts the graphical [30] model framework and variational techniques [3], with the VAE [16,32] as a representative work. Third, despite deep generative model's great promise in anomaly detection, existing VAE-based anomaly detection method [2] was not designed for KPIs (time series), and does not perform well in our settings (see \u00a7 4), and there is no theoretical foundation to back up its designs of deep generative models for anomaly detection (see \u00a7 5). Fourth, simply adopting the more complex models [36] based on VRNN shows long training time and poor performance in our experiments. Fifth, [2] assumes training only on clean data, which is infeasible in our context, while [36] does not discuss this problem.\n\n\nProblem Statement\n\nIn summary, existing anomaly detection algorithms suffer from the hassle of algorithm picking/parameter tuning, heavy reliance on labels, unsatisfying performance, and/or lack of theoretical foundations. Existing approaches are either unsupervised, or supervised but depending heavily on labels. However, in our context, labels are occasionally available although far from complete, which should be somehow taken advantage of.\n\nThe problem statement of this paper is as follows. We aim at an unsupervised anomaly detection algorithm based on deep generative models with solid theoretical explanation, and this algorithm can take advantage of the occasionally available labels. Because VAE is a basic building block of deep Bayesian network, we chose to start our work with VAE. Figure 2: Architecture of VAE. The prior of z is regarded as part of the generative model (solid lines), thus the whole generative model is denoted as p \u03b8 (x, z) = p \u03b8 (x|z) p \u03b8 (z). The approximated posterior (dashed lines) is denoted as q \u03d5 (z|x).\n\n\nBackground of Variational Auto-Encoder\n\nDeep Bayesian networks use neural networks to express the relationships between variables, such that they are no longer restricted to simple distribution families, thus can be easily applied to complicated data. Variational inference techniques [12] are often adopted in training and prediction, which are efficient methods to solve posteriors of the distributions derived by neural networks.\n\nVAE is a deep Bayesian network. It models the relationship between two random variables, latent variable z and visible variable x. A prior is chosen for z, which is usually multivariate unit Gaussian N (0, I). After that, x is sampled from p \u03b8 (x|z), which is derived from a neural network with parameter \u03b8 . The exact form of p \u03b8 (x|z) is chosen according to the demand of task. The true posterior p \u03b8 (z|x) is intractable by analytic methods, but is necessary for training and often useful in prediction, thus the variational inference techniques are used to fit another neural network as the approximation posterior q \u03d5 (z|x). This posterior is usually assumed to be N (\u00b5 \u03d5 (x), \u03c3 2 \u03d5 (x)), where \u00b5 \u03d5 (x) and \u03c3 \u03d5 (x) are derived by neural networks. The architecture of VAE is shown as Fig 2. SGVB [16,32] is a variational inference algorithm that is often used along with VAE, where the approximated posterior and the generative model are jointly trained by maximizing the evidence lower bound (ELBO, Eqn (1)). We did not adopt more advanced variational inference algorithms, since SGVB already works.\nlog p \u03b8 (x) \u2265 log p \u03b8 (x) \u2212 KL q \u03d5 (z|x) p \u03b8 (z|x) = L(x) (1) = E q \u03d5 (z|x) log p \u03b8 (x) + log p \u03b8 (z|x) \u2212 log q \u03d5 (z|x) = E q \u03d5 (z|x) log p \u03b8 (x, z) \u2212 log q \u03d5 (z|x) = E q \u03d5 (z|x) log p \u03b8 (x|z) + log p \u03b8 (z) \u2212 log q \u03d5 (z|x)\nMonte Carlo integration [10] is often adopted to approximate the expectation in Eqn (1), as Eqn (2), where z (l ) , l = 1 . . . L are samples from q \u03d5 (z|x). We stick to this method throughout this paper. \nE q \u03d5 (z |x) [f (z)] \u2248 1 L L l =1 f (z (l ) )(2)\n\nARCHITECTURE\n\nThe overall architecture of our algorithm Donut is illustrated as \n\n\nNetwork Structure\n\nAs aforementioned in \u00a7 2.1, the KPIs studied in this paper are assumed to be time sequences with Gaussian noises. However, VAE is not a sequential model, thus we apply sliding windows [34] of length W over the KPIs: for each point x t , we use x t \u2212W +1 , . . . , x t as the x vector of VAE. This sliding window was first adopted because of its simplicity, but it turns out to actually bring an important and beneficial consequence, which will be discussed in \u00a7 5.1. The overall network structure of Donut is illustrated in Fig 4, where the components with double-lined outlines (e.g., Sliding Window x, W Dimensional at bottom left) are our new designs and the remaining components are from standard VAEs. The prior p \u03b8 (z) is chosen to be N (0, I). Both x and z posterior are chosen to be diagonal Gaussian: p \u03b8 (x|z) = N (\u00b5 x , \u03c3 x 2 I), and q \u03d5 (z|x) = N (\u00b5 z , \u03c3 z 2 I), where \u00b5 x , \u00b5 z and \u03c3 x , \u03c3 z are the means and standard deviations of each independent Gaussian component. z is chosen to be K dimensional. Hidden features are extracted from x and z, by separated hidden layers f \u03d5 (x) and f \u03b8 (z). Gaussian parameters of x and z are then derived from the hidden features. The means are derived from linear layers:\n\u00b5 x = W \u22a4 \u00b5 x f \u03b8 (z) + b \u00b5 x and \u00b5 z = W \u22a4 \u00b5 z f \u03d5 (x) + b \u00b5 z .\nThe standard deviations are derived from soft-plus layers, plus a non-negative small number \u03f5:\n\u03c3 x = SoftPlus[W \u22a4 \u03c3 x f \u03b8 (z) + b \u03c3 x ] + \u03f5 and \u03c3 z = SoftPlus[W \u22a4 \u03c3 z f \u03d5 (x) + b \u03c3 z ] + \u03f5, where SoftPlus[a] =\nlog[exp(a) + 1]. All the W-s and b-s presented here are parameters of corresponding layers. Note when scalar function f (x) is applied on vector x, it means to apply on every component. We choose to derive \u03c3 x and \u03c3 z in such a way, instead of deriving log \u03c3 x and log \u03c3 z using linear layers as others do, for the following reason. The local variations in the KPIs of our interest are so small that \u03c3 x and \u03c3 z would probably get extremely close to zero, making log \u03c3 x and log \u03c3 z unbounded. This would cause severe numerical problems when computing the likelihoods of Gaussian variables. We thus use the soft-plus and the \u03f5 trick to prevent such problems.\n\nWe intentionally choose fully-connected layers as the structure of the hidden layers, making the overall architecture fairly simple. This is because our objective is to develop an VAE based anomaly detection algorithm with solid theoretical explanation, and a simple network structure would definitely make it easier to analyze the internal behavior in the perplexing \"variational auto-encoder\".\n\n\nTraining\n\nTraining is straightforward by optimizing the ELBO (Eqn (1)) with SGVB [16] algorithm. Since it is reported by [16] that one sample is already sufficient for computing the ELBO when training VAE with the SGVB algorithm, we let sampling number L = 1 during training. We also apply the re-parameterization trick as required by SGVB: instead of sampling z \u223c N (\u00b5 z , \u03c3 z 2 I), a dedicated random variable \u03be \u223c N (0, I) is sampled, such that we can rewrite z as z(\u03be ) = \u00b5 z + \u03be \u00b7 \u03c3 z . Sampling on \u03be is independent with the parameters \u03d5, which allows us to apply stochastic gradient descent as if VAE is an ordinary neural network. The windows of x are randomly shuffled before every epoch, which is beneficial for stochastic gradient descent. A sufficiently large number of x are taken in every mini-batch, which is critical for stabilizing the training, since sampling introduces extra randomness.\n\nAs discussed in \u00a7 2.2, the VAE based anomaly detection works by learning normal patterns, thus we need to avoid learning abnormal patterns whenever possible. Note that the \"anomalies\" in training are labeled anomalies, and there can be no labels for a given KPI, in which case the anomaly detection becomes an unsupervised one.\n\nOne might be tempted to replace labeled anomalies (if any) and missing points (known) in training data with synthetically generated values. Some previous work has proposed methods to impute missing data, e.g., [37], but it is hard to produce data that follow the \"normal patterns\" well enough. More importantly, training a generative model with data generated by another algorithm is quite absurd, since one major application of generative models is exactly to generate data. Using data imputed by any algorithm weaker than VAE would potentially downgrade the performance. Thus we do not adopt missing data imputation before training VAE, instead we choose to simply fill the missing points as zeros (in the Data Preparation step in Fig 3), and then modify the ELBO to exclude the contribution of anomalies and missing points (shown as Modified ELBO (M-ELBO for short hereafter) in the Training step in Fig 3).\n\nMore specifically, we modify the standard ELBO in Eqn (1) to our version Eqn (3). \u03b1 w is defined as an indicator, where a w = 1 indicates x w being not anomaly or missing, and a w = 0 otherwise. \u03b2 is defined as ( W w =1 \u03b1 w )/W . Note that Eqn (3) still holds when there is no labeled anomalies in the training data. The contribution of p \u03b8 (x w |z) from labeled anomalies and missing points are directly excluded by \u03b1 w , while the scaling factor \u03b2 shrinks the contribution of p \u03b8 (z) according to the ratio of normal points in x. This modification trains Donut to correctly reconstruct the normal points within x, even if some points in x are abnormal. We do not shrink q \u03d5 (z|x), because of the following two considerations. Unlike p \u03b8 (z), which is part of the generative network (i.e., model of the \"normal patterns\"), q \u03d5 (z|x) just describes the mapping from x to z, without considering\"normal patterns\". Thus, discounting the contribution of q \u03d5 (z|x) seems not necessary. Another reason is that\nE q \u03d5 (z|x) [\u2212 log q \u03d5 (z|x)]\nis exactly the entropy of q \u03d5 (z|x). This entropy term actually has some other roles in training (which will be discussed in \u00a7 5.3), thus might be better kept untouched.\nL(x) = E q \u03d5 (z|x) W w =1 \u03b1 w log p \u03b8 (x w |z) + \u03b2 log p \u03b8 (z) \u2212 log q \u03d5 (z|x) (3)\nBesides Eqn (3), another way to deal with anomalies and missing points is to exclude all windows containing these points from training data. This approach turns out to be inferior to M-ELBO. We will demonstrate the performance of both approaches in \u00a7 4.5.\n\nFurthermore, we also introduce missing data injection in training: we randomly set \u03bb ratio of normal points to be zero, as if they are missing points. With more missing points, Donut is trained more often to reconstruct normal points when given abnormal x, thus the effect of M-ELBO is amplified. This injection is done before every epoch, and the points are recovered once the epoch is finished. This missing data injection is shown in the Training step in Fig 3. \n\n\nDetection\n\nUnlike discriminative models which are designed for just one purpose (e.g., a classifier is designed for just computing the classification probability p(y|x)), generative models like VAE can derive various outputs. In the scope of anomaly detection, the likelihood of observation window x, i.e., p \u03b8 (x) in VAE, is an important output, since we want to see how well a given x follows the normal patterns. Monte Carlo methods can be adopted to compute the probability density of x, by p \u03b8 (x) = E p \u03b8 (z) [p \u03b8 (x|z)]. Despite the theoretically nice interpretation, sampling on the prior actually does not work well enough in practice, as will be shown in \u00a7 4.\n\nInstead of sampling on the prior, one may seek to derive useful outputs with the variational posterior q \u03d5 (z|x). One choice is to compute E q \u03d5 (z|x) [p \u03b8 (x|z)]. Although similar to p \u03b8 (x), it is actually not a well-defined probability density. Another choice is to compute E q \u03d5 (z|x) [log p \u03b8 (x|z)], which is adopted in [2], named as \"reconstruction probability\". These two choices are very similar. Since only the ordering rather than the exact values of anomaly scores are concerned in anomaly detection, we follow [2] and use the latter one. As an alternative, the ELBO (Eqn (1)) may also be used for approximating log p \u03b8 (x), as in [36]. However, the extra term E q \u03d5 (z|x) log p \u03b8 (z) \u2212 log q \u03d5 (z|x) in ELBO makes its internal mechanism hard to understand. Since the experiments in [36] does not support this alternative's superiority, we choose not to use it.\n\nDuring detection, the anomalies and missing points in a testing window x can bring bias to the mapped z, and further make the reconstruction probability inaccurate, which would be discussed in \u00a7 5.2. Since the missing points are always known (as \"null\"),  \nN N t =2 |x t \u2212 x t \u22121 | of the datasets,\nwhich is plotted by placing Gaussian kernels at each dataset. The values of x t are standardized to zero mean and unit variance beforehand. Triangles denote A, B and C, while circles denote the others. According to the CDF, A, B and C have relatively small, medium and large noises.\n\nwe have the chance to eliminate the biases introduced by missing points. We choose to adopt the MCMC-based missing data imputation technique with the trained VAE, which is proposed by [32]. Meanwhile, we do not know the exact positions of anomalies before detection, thus MCMC cannot be adopted on anomalies.\n\nMore specifically, the testing x is divided into observed and missing parts, i.e., (\nx o , x m ). A z sample is obtained from q \u03d5 (z|x o , x m ), then a reconstruction sample (x \u2032 o , x \u2032 m ) is obtained from p \u03b8 (x o , x m |z). (x o , x m ) is then replaced by (x o , x \u2032 m ), i.e.\n, the observed points are fixed and the missing points are set to new values. This process is iterated for M times, then the final (x o , x \u2032 m ) is used for computing the reconstruction probability. The intermediate x \u2032 m will keep getting closer to normal values during the whole procedure. Given sufficiently large M, the biases can be reduced, and we can get a more accurate reconstruction probability. The MCMC method is illustrated in Fig 5 and is shown in the Detection step in Fig 3. After MCMC, we take L samples of z to compute the reconstruction probability by Monte Carlo integration. It is worth mentioning that, although we may compute the reconstruction probability for each point in every window of x, we only use the score for the last point (i.e., x t in x t \u2212T +1 , . . . , x t ), since we want to respond to anomalies as soon as possible during the detection. We will still use vector notations in later texts, corresponding to the architecture of VAE. While it is possible to improve the detection performance by delaying the decision and considering more scores for the same point at different times, we leave it as a future work.\n\n\nEVALUATION 4.1 Datasets\n\nWe obtain 18 well-maintained business KPIs (where the time span is long enough for training and evaluation) from a large Internet company. All KPIs have an interval of 1 minute between two observations. We choose 3 datasets, denoted as A, B and C, according to Fig 6, so we can evaluate Donut for noises at different levels. We divide each dataset into training, validation and testing sets, whose ratios are 49%, 21%, 30% respectively. Figures of datasets A,  B and C are shown in Fig 1, while statistics are shown in Table 1. The operators of the Internet company labeled all the anomalies in these three datasets. For evaluation purpose, we can consider we have the ground truth of all anomalies in these three datasets. DataSet A B C  \n\n\nPerformance Metrics\n\nIn our evaluation, we totally ignore outputs of all algorithms at missing points (\"null\") since they are straightforward to identify. All the algorithms evaluated in this paper compute one anomaly score for each point. A threshold can be chosen to do the decision: if the score for a point is greater than the threshold, an alert should be triggered. In this way, anomaly detection is similar to a classification problem, and we may compute the precision and recall corresponding to each threshold. We may further compute the AUC, which is the average precision over recalls, given all possible thresholds; or the F-score, which is the harmonic mean of precision and recall, given one particular threshold. We may also enumerate all thresholds, obtaining all F-scores, and use the best F-score as the metric. The best F-score indicates the best possible performance of a model on a particular testing set, given an optimal global threshold. In practice, the best F-score is mostly consistent with AUC, except for slight differences (see Fig 8). We prefer the best F-score to AUC, since it should be more important to have an excellent F-score at a certain threshold than to have just high but not so excellent F-scores on most thresholds.\n\nIn real applications, the human operators generally do not care about the point-wise metrics. It is acceptable for an algorithm to trigger an alert for any point in a contiguous anomaly segment, if the delay is not too long. Some metrics for anomaly detection have been proposed to accommodate this preference, e.g., [22], but most are not widely accepted, likely because they are too complicated. We instead use a simple strategy: if any point in an anomaly segment in the ground truth can be detected by a chosen threshold, we say this segment is detected correctly, and all points in this segment are treated as if they can be detected by this threshold. Meanwhile, the points outside the anomaly segments are treated as usual. The precision, recall, AUC, F-score and best F-score are then computed accordingly. This approach is illustrated in Fig 7. In addition to the accuracy metric, we compute the alert delay for each detected segment, which is also important to the operators. The forth row shows the detector results after adjustment. We shall get precision 0.6, and recall 0.5. From the third row, the alert delay for the first segment is 1 interval (1 minute). Figure 8: AUC, the best F-Score, and the average alert delay corresponding to the best F-score. A, B and C are the three datasets. \"0%\", \"10%\" and \"100%\" are the ratio of the labels preserved in training. Note there is no result for Opprentice when there are 0% of anomaly labels. The black stick on top of each bar is the deviation of 10 repeated experiments.\n\nFor a true positive segment, the alert delay is the time difference between the first point and the first detected point in the segment.\n\n\nExperiment Setup\n\nWe set the window size W to be 120, which spans 2 hours in our datasets. The choice of W is restricted by two factors. On the one hand, too small a W will cause the model to be unable to capture the patterns, since the model is expected to recognize what the normal pattern is with the information only from the window (see \u00a7 5.1). On the other hand, too large a W will increase the risk of over-fitting, since we stick to fully-connected layers without weight sharing, thus the number of model parameters is proportional to W . We set the latent dimension K to be 3 for B and C, since the 3-d dimensional space can be easily visualized for analysis and luckily K = 3 works well empirically for for B and C. As for A, we found 3 is too small, so we empirically increase K to 8. These empirical choices of K are proven to be quite good on testing set, as will be shown in Fig 10. The hidden layers of q \u03d5 (z|x) and p \u03b8 (x|z) are both chosen as two ReLU layers, each with 100 units, which makes the variational and generative network have equal size. We did not carry out exhaustive search on the structure of hidden networks.\n\nOther hyper-parameters are also chosen empirically. We use 10 \u22124 as \u03f5 of the std layer. We use 0.01 as the injection ratio \u03bb. We use 10 as the MCMC iteration count M, and use 1024 as the sampling number L of Monte Carlo integration. We use 256 as the batch size for training, and run for 250 epochs. We use Adam optimizer [15], with an initial learning rate of 10 \u22123 . We discount the learning rate by 0.75 after every 10 epochs. We apply L2 regularization to the hidden layers, with a coefficient of 10 \u22123 . We clip the gradients by norm, with a limit of 10.0.\n\nIn order to evaluate Donut with no labels, we ignore all the labels. For the case of occasional labels, we down-sample the anomaly labels of training and validation set to make it contain 10% of labeled anomalies. Note that missing points are not down-sampled. We keep throwing away anomaly segments randomly, with a probability that is proportional to the length of each segment, until the desired down-sampling rate is reached. We use this approach instead of randomly throwing away individual anomaly points, because KPIs are time sequences and each anomaly point could leak information about its neighboring points, resulting in over-estimated performance. Such downsampling are done 10 times, which enables us to do 10 independent, repeated experiments. Overall for each dataset, we have three versions: 0% labels, 10% labels, and 100% labels.\n\n\nOverall Performance\n\nWe measure the AUC, the best F-Score, and the average alert delay corresponding to the best F-score in Fig 8 of Donut, and compared with three selected algorithms.\n\nOpprentice [25] is an ensemble supervised framework using Random Forest classifier. On datasets similar to ours, Opprentice is reported to consistently and significantly outperform 14 anomaly detectors based on traditional statistical models (e.g., [6,17,18,24,26,27,31,40,41]), with in total 133 enumerated configurations of hyper-parameters for these detectors. Thus, in our evaluation of Donut, Opprentice not only serves as a state-of-art competitor algorithm from the non deep learning areas, but also serves as a proxy to compare with the empirical performance \"upper bound\" of these traditional anomaly detectors.\n\nVAE baseline. The VAE-based anomaly detection in [2] does not deal with time sequences, thus we set up the VAE baseline as follows. First, the VAE baseline has the same network structure as Donut, as shown in Fig 4. Second, among all the techniques in Fig 3, only those techniques in the Data Preparation step are used. Third, as suggested by [2], we exclude all windows containing either labeled anomalies or missing points from training data.\n\nDonut-Prior. Given that a generative model learns p(x) by nature, while in VAE p(x) is defined as E p \u03b8 (z) [p \u03b8 (x|z)], we also evaluate the prior counterpart of reconstruction probability, i.e., E p \u03b8 (z) [log p \u03b8 (x|z)]. We just need a baseline of the prior, so we compute the prior expectation by plain Monte Carlo integration, without advanced techniques to improve the result.\n\nThe best F-score of Donut is quite satisfactory in totally unsupervised case, ranges from 0.75 to 0.9, better than the supervised Opprentice in all cases. In fact, when labels are incomplete, the best F-score of the Opprentice drops heavily in A and B, only remaining acceptable in C. The number of anomalies are much larger in C than A and B, while having 10% of labels are likely to be just enough for training. Donut has an outstanding performance in the unsupervised scenario, and we see that feeding anomaly labels into Donut would in general make it work even better. There is, however, an unusual behavior of Donut, where the best F-score in C, as well as the AUC in B and C, are slightly worse with 100% labels than 10%. This is likely an optimization problem, where the unlabeled anomalies might cause training to be unstable, and accidentally pull the model out of a sub-optimal equilibrium ( \u00a7 5.4). Such phenomenon seems to diminish when K increases from 3 (B and C) to 8 (A). Fortunately, it does not matter too much, so we would suggest to use labels in Donut whenever possible.\n\nDonut outperforms the VAE baseline by a large margin in A and B, while it does not show such great advantage in C. In fact, the relative advantage of Donut is the largest in A, medium in B, and the smallest in C. This is caused by the following reasons. Naturally, VAE models normal x. As a result, the reconstruction probability actually expects x to be mostly normal (see \u00a7 5.1). However, since x are sliding windows of KPIs and we are required to produce one anomaly score for every point, it is sometimes inevitable to have abnormal points in x. This causes the VAE baseline to suffer a lot. In contrast, the techniques developed in this paper enhances the ability of Donut to produce reliable outputs even when anomalies present in earlier points in the same window. Meanwhile, abnormal points with similar abnormal magnitude would appear relatively \"more abnormal\" when the KPI is smoother. Given that A is the smoothest, B is medium, and C is the least smoothest, above observation in the relative advantage is not surprising.\n\nFinally, the best F-score of the Donut-Prior is much worse than the reconstruction probability, especially when the dimension of z is larger. However, it is worth mentioning that the posterior expectation in reconstruction probability only works under certain conditions ( \u00a7 5.2). Fortunately, this problem does not matter too much to Donut (see \u00a7 5.2). As such, the reconstruction probability can be used without too much concern.\n\nThe average alert delays of Donut, Opprentice and VAE Baseline are acceptable over all datasets, whereas Donut-Prior is not. Meanwhile, the best F-score of Donut is much better than others. In conclusion, Donut could achieve the best performance without increasing the alert delay, thus Donut is practical for operators.\n\n\nEffects of Donut Techniques\n\nWe have proposed three techniques in this paper: (1) M-ELBO (Eqn (3)), (2) missing data injection, and (3) MCMC imputation. In Fig 10, we present the best F-score of Donut with four possible combinations of these techniques, plus the VAE baseline for comparison. These techniques are closely related to the KDE interpretation, which will be discussed further in \u00a7 5.2. M-ELBO alone contributes most of the improvement over the VAE baseline. It works by training Donut to get used to possible abnormal points in x, and to produce desired outputs in such cases. Although we expected M-ELBO to work, we did not expect it to work such well. In conclusion, it would not be a good practice to train a VAE for anomaly detection using only normal data, although it seems natural for a generative model ( \u00a7 5.2). To the best of our knowledge, M-ELBO and its importance have never been stated in previous work, thus is a major contribution of ours.\n\nMissing data injection is designed for amplifying the effect of M-ELBO, and can actually be seen as a data augmentation method. In fact, it would be better if we inject not only missing points, but also synthetically generated anomalies during training. However, it is difficult to generate anomalies similar enough to the real ones, which should be a large topic and is out of the scope of this paper. We thus only inject the missing points. The improvement of best F-score introduced by missing data injection is not very significant, and in the case of 0% labels on B and C, it is slightly worse than M-ELBO only. This is likely because the injection introduces extra randomness to training, such that it demands larger training epochs, compared to the case of M-ELBO only. We are not sure how many number of epochs to run when the injection is adopted, in order to get an objective comparison, thus we just use the same epochs in all cases, leaving the result as it is. We still recommend to use missing data injection, even with a cost of larger training epochs, as it is expected to work with a large chance.\n\nMCMC imputation is also designed to help Donut deal with abnormal points. Although Donut obtains significant improvement of best F-score with MCMC in only some cases, it never harms the performance. According to [32], this should be an expected result. We thus recommend to always adopt MCMC in detection.\n\nIn conclusion, we recommend to use all the three techniques of Donut. The result of such configuration is also presented in Fig 9. \n\n\nImpact of K\n\nThe number of z dimensions, i.e., K, plays an important role. Too small a K would potentially cause under-fitting, or sub-optimal equilibrium (see \u00a7 5.4). On the other hand, too large a K would probably cause the reconstruction probability unable to find a good posterior (see \u00a7 5.1). It is difficult to choose a good K in totally unsupervised scenario, thus we leave it as a future work.\n\nIn Fig 10, we present the average best F-score with different K on testing set for unsupervised Donut. This does not help us choose Figures are plotted by sampling z from q \u03d5 (z|x), corresponding to normal x randomly chosen from the testing set. K is chosen as 2, so the x-and y-axis are the two dimensions of z samples. We plot z samples instead of \u00b5 z of q \u03d5 (z|x), since we want to take into account the effects of \u03c3 z in the figures. The color of z a sample denotes its time of the day.\n\nthe best K (since we cannot use testing test to pick K), but can show our empirical choice of 8, 3, 3 is quite good. The best F-score reaches maximum at 5 for A, 4 for B and 3 for C. In other words, the best F-score could be achieved with fairly small K. On the other hand, the best F-score does not drop too heavily for K up to 21. This gives us a large room to empirically choose K. Finally, we notice that smoother KPIs seem to demand larger K. Such phenomenon is not fully studied in this paper, and we leave it as a future work. Based on the observations in Fig 10, for KPIs similar to A, B or C, we suggest an empirical choice of K within the range from 5 to 10.\n\n\nANALYSIS 5.1 KDE Interpretation\n\nAlthough the reconstruction probability E q \u03d5 (z|x) [log p \u03b8 (x|z)] has been adopted in [2,36], how it actually works has not yet been made clear. Some may see it as a variant of E q \u03d5 (z|x) [p \u03b8 (x|z)], but\nE q \u03d5 (z|x) [p \u03b8 (x|z)] = \u222b p \u03b8 (x|z)q \u03d5 (z|x)dz,\nwhich is definitely not a well-defined probability 2 . Thus neither of [2,36] can be explained by the probabilistic framework. We hereby propose the KDE (kernel density estimation) interpretation for the reconstruction probability, and for the entire Donut algorithm. The posterior q \u03d5 (z|x) for normal x exhibits time gradient, as Fig 11a shows. The windows of x at contiguous time (contiguous x for short hereafter) are mapped to nearby q \u03d5 (z|x), mostly with 2 In general it should give no useful information by computing the expectation of log p \u03b8 (x|z) upon the posterior q \u03d5 (z|x), using a potentially abnormal x. small variance \u03c3 z (see Fig 12). The q \u03d5 (z|x) are thus organized in smooth transition, causing z samples to exhibit color gradient in the figure. We name this structure \"time gradient\". The KPIs in this paper are smooth in general, so contiguous x are highly similar. The root cause of time gradient is the transition of q \u03d5 (z|x) in the shape of x (rather than the one in time), because Donut consumes only the shape of x and no time information. Time gradient benefits the generalization of Donut on unseen data: if we have a posterior q \u03d5 (z|x) somewhere between two training posteriors, it would be well-defined, avoiding absurd detection output.\n\nFor a partially abnormal x 3 , the dimension reduction would allow Donut to recognize its normal patternx, and cause q \u03d5 (z|x) to be approximately q \u03d5 (z|x). This effect is caused by the following reasons. Donut is trained to reconstruct normal points in training samples with best efforts, while the dimension reduction causes Donut to be only able to capture a small amount of information from x. As a result, only the overall shape is encoded in q \u03d5 (z|x). The abnormal information is likely to be dropped in this procedure. However, if a x is too abnormal, Donut might fail to recognize any normalx, such that q \u03d5 (z|x) would become ill-defined.\n\nThe fact that q \u03d5 (z|x) for a partially abnormal x would be similar to q \u03d5 (z|x) brings special meanings to the reconstruction probability in Donut. Since M-ELBO is maximized with regard to normal patterns during training, log p \u03b8 (x|z) for z \u223c q \u03d5 (z|x) should produce high scores for x similar tox, and vise versa. That is to say, each log p \u03b8 (x|z) can be used as a density estimator, indicating how well x follows the normal patternx. The posterior expectation then sums up the scores from all log p \u03b8 (x|z), with the weight q \u03d5 (z|x) for each z. This procedure is very similar to weighted kernel density estimation [11,14]. We thus carry out the KDE interpretation: the reconstruction probability E q \u03d5 (z|x) [log p \u03b8 (x|z)] in Donut can be seen as weighted kernel density estimation, with q \u03d5 (z|x) as weights and log p \u03b8 (x|z) as kernels 4 . Fig 12 is an illustration of the KDE interpretation. We also visualize the 3-d latent spaces of all datasets in Fig 13. From the KDE interpretation, we suspect the prior expectation would not work well, whatever technique is adopted to improve the result: sampling on the prior should obtain kernels for all patterns of x, potentially confusing the density estimation for a particular x.\n\n\nFind Good Posteriors for Abnormal x\n\nDonut can recognize the normal pattern of a partially abnormal x, and find a good posterior for estimating how well x follows the normal pattern. We now analyze how the techniques in Donut can enhance such ability of finding good posteriors.\n\nDonut is forced to reconstruct normal points within abnormal windows correctly during training, by M-ELBO. It is thus explicitly trained to find good posteriors. This is the main reason why M-ELBO plays a vital role in    A normal x is chosen, whose posterior q \u03d5 (z|x) is plotted at right: the cross denotes \u00b5 x and the ellipse denotes its 3-\u03c3 x region. We randomly set 15% x points as missing, to obtain the abnormal x \u2032 . We run MCMC over x \u2032 with 10 iterations. At first, the z sample is far from q \u03d5 (z|x). After that, z samples quickly approach q \u03d5 (z|x), and begin to move around q \u03d5 (z|x) after only 3 iterations.\n\nDespite these techniques, Donut may still fail to find a good posterior, if there are too many anomalies in x. In our scenario, the KPIs are time sequences, with one point per minute. For longlasting anomalies, having the correct detection scores and raise alerts at first few minutes are sufficient in our context 5 . The operators can take action once any score reaches the threshold, and simply ignore the following inaccurate scores. Nevertheless, the 5 In practice, ensuing and continuous alerts are typically filtered out anyway.\n\nKDE interpretation can help us know the limitations of reconstruction probability, in order to use it properly.\n\n\nCauses of Time Gradient\n\nIn this section we discuss the causes of the time gradient effect. To simplify the discussion, let us assume training x are all normal, thus M-ELBO is now equivalent to the original ELBO. M-ELBO can then be decomposed into three terms as in Eqn (4) (we leave out some subscripts for shorter notation).\nL(x) = E q \u03d5 (z|x) log p \u03b8 (x|z) + log p \u03b8 (z) \u2212 log q \u03d5 (z|x) = E [log p \u03b8 (x|z)] + E [log p \u03b8 (z)] + H [z|x](4)\nThe 1st term requires z samples from q \u03d5 (z|x) to have a high likelihood of reconstructing x. As a result, q \u03d5 (z|x) for x with dissimilar shapes are separated. The 2nd term causes q \u03d5 (z|x) to concentrate on N (0, I). The 3rd term, the entropy of q \u03d5 (z|x), causes q \u03d5 (z|x) to expand wherever possible. Recall the 2nd term sets a restricted area for q \u03d5 (z|x) to expand (see Fig 11c for the combination effect of the 2nd and 3rd term). Taking the 1st term into account, this expansion would also stop if q \u03d5 (z|x) for two dissimilar x reach each other. In order for every q \u03d5 (z|x) to have a maximal territory when training converges (i.e., these three terms reach an equilibrium), similar x would have to get close to each other, allowing q \u03d5 (z|x) to grow larger with overlapping boundaries. Since contiguous x are similar in seasonal KPIs (and vise versa), the time gradient would be a natural consequence, if such equilibrium could be achieved. Next we discuss how the equilibrium could be achieved. The SGVB algorithm keeps pushing q \u03d5 (z|x) for dissimilar x away during training, as illustrated in Fig 15. The more dissimilar two q \u03d5 (z|x) are, the further they are pushed away. Since we initialize the variational network randomly, q \u03d5 (z|x) are mixed everywhere when training just begins, as Fig 11b shows. At this time, every q \u03d5 (z|x) are pushed away by all other q \u03d5 (z|x). Since x are sliding windows of KPIs, any pair of x far away in time will be generally more dissimilar, thus get pushed away further from each other. This gives q \u03d5 (z|x) an initial layout. As training goes on, the time gradient is fine-tuned and gradually established, as Fig 16a shows. The training dynamics Figure 15: Suppose \u00b5 z (1) and \u00b5 z (2) are the mean of q \u03d5 (z|x) corresponding to training data x (1) and x (2) , with the surrounding circles represent \u03c3 z (1) and \u03c3 z (2) . When these two distributions accidentally \"overlaps\" during training, the sample z (1) from q \u03d5 (z|x (1) ) may get too close to \u00b5 z (2) , such that the reconstructed distribution will be close to p \u03b8 (x|z (2) ) with some z (2) for x (2) . If x (1) and x (2) are dissimilar, log p \u03b8 (x (1) |z (2) ) in the loss will then effectively push \u00b5 z (1) away from \u00b5 z (2) .  Figure 16: Evolution of the z space of dataset B during training. We sample normal x from validation set, and plot z samples accordingly. (a) converges to a good equilibrium, with a final F-score 0.871, while (b) converges to a sub-optimal one, with a final F-score 0.826. We plot step 4300 in (a), because it is a very important turning point, where the green points just begin to get away from the purple points.\n\nalso suggest that the learning rate annealing technique is very important, since it can gradually stabilize the layout.\n\nSurprisingly, we cannot find any term in M-ELBO that directly pulls q \u03d5 (z|x) for similar x together. The time gradient is likely to be caused mainly by expansion (H [z|x]), squeezing (E [log p \u03b8 (z)]), pushing (E [log p \u03b8 (x|z)]), and the training dynamics (random initialization and SGVB). This could sometimes cause trouble, and result in sub-optimal layouts, as we shall see in \u00a7 5.4.\n\n\nSub-Optimal Equilibrium\n\nq \u03d5 (z|x) may sometimes converge to a sub-optimal equilibrium. Fig 16b demonstrates such a problem, where the purple points accidentally get through the green points after the first 100 steps. The purple points push the green points away towards both sides, causing the green points to be totally cut off at around 5000 steps. As training goes on, the green points will be pushed even further, such that the model is locked to this sub-optimal equilibrium and never escapes. Fig 17 plots the training and validation loss of Fig 16b. Clearly, the model begins to over-fit soon after the green points are separated into halves. Such bad layout of z breaks the time gradient, where a testing x following green patterns might accidentally be mapped to somewhere between the green two halves and get recognized as purple. This would certainly downgrade the detection performance, according to the KDE interpretation.\n\nWhen there are unlabeled anomalies, the training would become unstable so that the model might be accidentally brought out of a sub-optimal equilibrium and achieve a better equilibrium afterwards. With the help of early-stopping during training, the best encountered equilibrium is chosen eventually. This explains why sometimes having complete labels would not benefit the performance. This effect is likely to be less obvious with larger K, since having more dimensions gives q \u03d5 (z|x) extra freedom to grow, reducing the chance of bad layouts. When sub-optimal equilibrium is not a vital problem, the convergence of training then becomes more important, while having more labels definitely helps stabilize the training. In conclusion, using anomaly labels in Donut is likely to benefit the performance, as long as K is adequately large.\n\n\nDISCUSSION 6.1 Broader Implications\n\nReconstruction. The dimension reduction in Donut throws away information of abnormal points. This implies that z for abnormal x might be indistinguishable from normal ones, thus cannot be used directly to detect anomalies. As a result, the reconstruction is thus an essential step in Donut. We believe this conclusion is also true in other algorithms involving dimension reduction. For example, the performance of PCA-based anomaly detection [19,20,33] is sensitive to the number of principle components. We suspect that using the reconstructed samples (as done in Donut) from these components could remove this sensitivity.\n\nKDE interpretation is the heart of Donut. We suspect this interpretation can also benefit the design of other deep generative models in anomaly detection. Meanwhile, the techniques of this paper (i.e., M-ELBO, missing data injection and MCMC) are designed to enhance the ability of finding good posteriors according to abnormal x, needed by the density estimation step. These techniques are also readily applicable to other deep generative models.\n\nThe time gradient effect may take place in more general types of seasonal or periodical sequences, rather than just the seasonal KPIs. Similar x are mapped to neighborhood q \u03d5 (z|x), such that Donut or its variants might potentially be useful in tasks dealing with similarities of sequences, in addition to anomaly detection, e.g., retrieving similar curves from a large database.\n\n\nFuture Work\n\nAs mentioned in \u00a7 4.6, we leave the task of choosing K as future work. The topology of z might be a key for solving this problem, as we have seen how sub-optimal equilibriums might affect the performance (see \u00a7 5.4). Also, we suspect the sub-optimal equilibriums might be less likely to take place with large K, although we may still need more experiments and analysis to prove this.\n\nWe did not discuss how to choose the right threshold for detection. This is also a quite difficult problem, especially in the unsupervised scenario. Some work (e.g., [21]) have been proposed w.r.t. choosing a proper threshold, which might be applicable to Donut.\n\nMore complicated architectures may be adopted to extend Donut. For example, the sequence-to-sequence RNN architecture [38] may be used to replace the fully connected layers in Donut, so as to handle larger windows, and to deal better with the correlations across points.\n\n\nCONCLUSION\n\nIn this paper, we proposed an unsupervised anomaly detection algorithm Donut based on VAE for seasonal KPIs with local variations. The new techniques enabled Donut to greatly outperform state-ofart supervised and VAE-based anomaly detection algorithms. The best F-scores of Donut range from 0.75 to 0.90 for the studied KPIs.\n\nDonut's excellent performance are explained by our theoretical analysis with KDE interpretation and the new discovery of the time gradient effect. Our experimental and theoretical analyses imply broader impacts: anomaly detection based on dimension reduction needs to use reconstruction; anomaly detection with generative models needs to train with both normal and abnormal data.\n\nFigure 1 :\n12.5-day-long fragments of the seasonal KPI datasets in our paper, with anomalies in red color and missing points (filled with zeros) in orange. Within each dataset, there are variations for the same time slot in different days.\n\nFigure 3 :Figure 4 :\n34Overall architecture of Donut.(a) Variational net q \u03d5 (z|x) (b) Generative net p \u03b8 (x|z) Network structure of Donut. Gray nodes are random variables, and white nodes are layers. The double lines highlight our special designs upon a general VAE.\n\nFig 3 .\n3The three key techniques are Modified ELBO and Missing Data Injection during training, and MCMC Imputation in detection.\n\nFigure 5 :\n5Illustration of one iteration in MCMC. x is decomposed as (x o , x m ), then x o is fixed and x m is replaced by x \u2032 m from the reconstruction sample, in order to get the new x \u2032 .\n\nFigure 6 :\n6CDF of 1\n\nFigure 7 :\n7Illustration of the strategy for modified metrics. The first row is the truth with 10 contiguous points and two anomaly segments highlighted in the shaded squares. The detector scores are shown in the second row. The third row shows the point-wise detector results with a threshold of 0.5.\n\nFigure 9 :\n9Best F-score of (1) VAE baseline, (2) Donut with M-ELBO, (3) M-ELBO + missing data injection, (4) M-ELBO + MCMC, and (5) M-ELBO + both MCMC and injection. The M-ELBO alone contributes most of the improvement.\n\nFigure 10 :Figure 11 :\n1011The best F-score of unsupervised Donut with different K, averaged over 10 repeated experiments. The z layout of dataset B with (a) Donut, (b) untrained VAE, (c) VAE trained using E [log p \u03b8 (z)]+H [z|x] as loss.\n\nFig 9 .\n9Missing data injection amplifies the effect of M-ELBO, with synthetically generated missing points. On the other hand, MCMC imputation does not change the training process. Instead, it improves the detection, by iteratively approaching better posteriors, as illustrated inFig 14.\n\nFigure 12 :\n12Illustration of the KDE interpretation. For a given x potentially with anomalies, Donut tries to recognize what normal pattern it follows, encoded as q \u03d5 (z|x). The black ellipse in the middle figure denotes the 3-\u03c3 z region of q \u03d5 (z|x). L samples of z are then taken from q \u03d5 (z|x), denoted as the crosses in the middle figure. Each z is associated with a density estimator kernel log p \u03b8 (x|z). The blue curves in the right two figures are \u00b5 x of each kernel, while the surrounding stripes are \u03c3 x . Finally, the values of log p \u03b8 (x|z) are computed from each kernel, and further averaged together as the reconstruction probability.\n\nFigure 13 :\n133-d latent space of all three datasets.\n\nFigure 14 :\n14MCMC visualization.\n\nFigure 17 :\n17Training and validation loss of Fig 16b.\n\nTable 1 :\n1Statistics of A, B and C.\nWe call a x partially abnormal if only a small portion of points within x are abnormal, such that we can easily tell what normal pattern x should follow.4 The weights q \u03d5 (z|x) are implicitly applied by sampling in Monte Carlo integration.\nACKNOWLEDGEMENTSThe work was supported by National Natural Science Foundation of China (NSFC) under grant No. 61472214 and No. 61472210, and Alibaba Innovative Research (AIR). We also thank Prof. Jun Zhu and his PhD. student Jiaxin Shi for helpful and constructive discussions.\nEnhancing one-class support vector machines for unsupervised anomaly detection. Mennatallah Amer, Markus Goldstein, Slim Abdennadher, Proceedings of the ACM SIGKDD Workshop on Outlier Detection and Description. the ACM SIGKDD Workshop on Outlier Detection and DescriptionACMMennatallah Amer, Markus Goldstein, and Slim Abdennadher. 2013. Enhanc- ing one-class support vector machines for unsupervised anomaly detection. In Proceedings of the ACM SIGKDD Workshop on Outlier Detection and Description. ACM, 8-15.\n\nVariational Autoencoder based Anomaly Detection using Reconstruction Probability. Jinwon An, Sungzoon Cho, Technical ReportSNU Data Mining Center. 1-18 pagesJinwon An and Sungzoon Cho. 2015. Variational Autoencoder based Anomaly Detection using Reconstruction Probability. Technical Report. SNU Data Mining Center. 1-18 pages.\n\nVariational algorithms for approximate Bayesian inference. Matthew James Beal, University of London LondonMatthew James Beal. 2003. Variational algorithms for approximate Bayesian inference. University of London London.\n\nPattern recognition and machine learning. M Christopher, Bishop, springerChristopher M Bishop. 2006. Pattern recognition and machine learning. springer.\n\nAnomaly detection: A survey. Varun Chandola, Arindam Banerjee, Vipin Kumar, ACM computing surveys (CSUR). 4115Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection: A survey. ACM computing surveys (CSUR) 41, 3 (2009), 15.\n\nA Provider-side View of Web Search Response Time. Yingying Chen, Ratul Mahajan, Baskar Sridharan, Zhi-Li Zhang, 10.1145/2486001.2486035Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM (SIGCOMM '13). the ACM SIGCOMM 2013 Conference on SIGCOMM (SIGCOMM '13)New York, NY, USAACMYingying Chen, Ratul Mahajan, Baskar Sridharan, and Zhi-Li Zhang. 2013. A Provider-side View of Web Search Response Time. In Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM (SIGCOMM '13). ACM, New York, NY, USA, 243-254. https://doi.org/10.1145/2486001.2486035\n\nHigh-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning. M Sarah, Sutharshan Erfani, Shanika Rajasegarar, Christopher Karunasekera, Leckie, Pattern Recognition. 58Sarah M Erfani, Sutharshan Rajasegarar, Shanika Karunasekera, and Christopher Leckie. 2016. High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning. Pattern Recognition 58 (2016), 121-134.\n\nMAWILab: Combining Diverse Anomaly Detectors for Automated Anomaly Labeling and Performance Benchmarking. Romain Fontugne, Pierre Borgnat, Patrice Abry, Kensuke Fukuda, 10.1145/1921168.1921179Proceedings of the 6th International COnference. the 6th International COnferenceACM8Romain Fontugne, Pierre Borgnat, Patrice Abry, and Kensuke Fukuda. 2010. MAWILab: Combining Diverse Anomaly Detectors for Automated Anomaly Labeling and Performance Benchmarking. In Proceedings of the 6th International COnference (Co-NEXT '10). ACM, Article 8, 12 pages. https://doi.org/10.1145/ 1921168.1921179\n\nSimilarity based vehicle trajectory clustering and anomaly detection. Zhouyu Fu, Weiming Hu, Tieniu Tan, Image Processing, 2005. ICIP 2005. IEEE International Conference on. IEEE2602Zhouyu Fu, Weiming Hu, and Tieniu Tan. 2005. Similarity based vehicle trajectory clustering and anomaly detection. In Image Processing, 2005. ICIP 2005. IEEE International Conference on, Vol. 2. IEEE, II-602.\n\nBayesian inference in econometric models using Monte Carlo integration. John Geweke, Econometrica: Journal of the Econometric Society. John Geweke. 1989. Bayesian inference in econometric models using Monte Carlo integration. Econometrica: Journal of the Econometric Society (1989), 1317-1339.\n\nWeighted samples, kernel density estimators and convergence. Francisco J Goerlich Gisbert, Empirical Economics. 28Francisco J Goerlich Gisbert. 2003. Weighted samples, kernel density estimators and convergence. Empirical Economics 28, 2 (2003), 335-351.\n\nDeep Learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT PressIan Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems. 2672-2680.\n\nWolfgang H\u00e4rdle, Axel Werwatz, Marlene M\u00fcller, Stefan Sperlich, Nonparametric density estimation. Nonparametric and Semiparametric Models. Wolfgang H\u00e4rdle, Axel Werwatz, Marlene M\u00fcller, and Stefan Sperlich. 2004. Nonparametric density estimation. Nonparametric and Semiparametric Models (2004), 39-83.\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, arXiv:1412.6980arXiv preprintDiederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimiza- tion. arXiv preprint arXiv:1412.6980 (2014).\n\nAuto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsDiederik P Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In Proceedings of the International Conference on Learning Representations.\n\nAdaptive kalman filtering for anomaly detection in software appliances. Florian Knorn, J Douglas, Leith, INFOCOM Workshops. Florian Knorn and Douglas J Leith. 2008. Adaptive kalman filtering for anomaly detection in software appliances. In INFOCOM Workshops 2008, IEEE. IEEE, 1-6.\n\nSketch-based change detection: methods, evaluation, and applications. Balachander Krishnamurthy, Subhabrata Sen, Yin Zhang, Yan Chen, Proceedings of the 3rd ACM SIGCOMM conference on Internet measurement. the 3rd ACM SIGCOMM conference on Internet measurementACMBalachander Krishnamurthy, Subhabrata Sen, Yin Zhang, and Yan Chen. 2003. Sketch-based change detection: methods, evaluation, and applications. In Pro- ceedings of the 3rd ACM SIGCOMM conference on Internet measurement. ACM, 234-247.\n\nDiagnosing Network-wide Traffic Anomalies. Anukool Lakhina, Mark Crovella, Christophe Diot, 10.1145/1015467.1015492Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM '04). the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM '04)New York, NY, USAACMAnukool Lakhina, Mark Crovella, and Christophe Diot. 2004. Diagnosing Network-wide Traffic Anomalies. In Proceedings of the 2004 Conference on Appli- cations, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM '04). ACM, New York, NY, USA, 219-230. https://doi.org/10.1145/ 1015467.1015492\n\nMining Anomalies Using Traffic Feature Distributions. Anukool Lakhina, Mark Crovella, Christophe Diot, 10.1145/1080091.1080118Proceedings of the 2005 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM '05). the 2005 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM '05)New York, NY, USAACMAnukool Lakhina, Mark Crovella, and Christophe Diot. 2005. Mining Anom- alies Using Traffic Feature Distributions. In Proceedings of the 2005 Confer- ence on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM '05). ACM, New York, NY, USA, 217-228. https: //doi.org/10.1145/1080091.1080118\n\nGeneric and scalable framework for automated time-series anomaly detection. Nikolay Laptev, Saeed Amizadeh, Ian Flint, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMNikolay Laptev, Saeed Amizadeh, and Ian Flint. 2015. Generic and scalable framework for automated time-series anomaly detection. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 1939-1947.\n\nEvaluating Real-Time Anomaly Detection Algorithms-The Numenta Anomaly Benchmark. Alexander Lavin, Subutai Ahmad, IEEE 14th International Conference on. IEEE. Machine Learning and Applications (ICMLA)Alexander Lavin and Subutai Ahmad. 2015. Evaluating Real-Time Anomaly Detection Algorithms-The Numenta Anomaly Benchmark. In Machine Learning and Applications (ICMLA), 2015 IEEE 14th International Conference on. IEEE, 38- 44.\n\nAnomaly detection in sea traffic-a comparison of the gaussian mixture model and the kernel density estimator. Rikard Laxhammar, Goran Falkman, Egils Sviestins, Information Fusion, 2009. FUSION'09. 12th International Conference on. IEEE. Rikard Laxhammar, Goran Falkman, and Egils Sviestins. 2009. Anomaly detection in sea traffic-a comparison of the gaussian mixture model and the kernel density estimator. In Information Fusion, 2009. FUSION'09. 12th International Conference on. IEEE, 756-763.\n\nThreshold compression for 3g scalable monitoring. Suk-Bok Lee, Dan Pei, Mohammadtaghi Hajiaghayi, Ioannis Pefkianakis, Songwu Lu, He Yan, Zihui Ge, Jennifer Yates, Mario Kosseifi, INFOCOM. Suk-Bok Lee, Dan Pei, MohammadTaghi Hajiaghayi, Ioannis Pefkianakis, Songwu Lu, He Yan, Zihui Ge, Jennifer Yates, and Mario Kosseifi. 2012. Threshold com- pression for 3g scalable monitoring. In INFOCOM, 2012 Proceedings IEEE. IEEE, 1350-1358.\n\nOpprentice: Towards Practical and Automatic Anomaly Detection Through Machine Learning. Dapeng Liu, Youjian Zhao, Haowen Xu, Yongqian Sun, Dan Pei, Jiao Luo, Xiaowei Jing, Mei Feng, 10.1145/2815675.2815679Proceedings of the 2015 ACM Conference on Internet Measurement Conference (IMC '15). the 2015 ACM Conference on Internet Measurement Conference (IMC '15)New York, NY, USAACMDapeng Liu, Youjian Zhao, Haowen Xu, Yongqian Sun, Dan Pei, Jiao Luo, Xiaowei Jing, and Mei Feng. 2015. Opprentice: Towards Practical and Automatic Anomaly Detection Through Machine Learning. In Proceedings of the 2015 ACM Conference on Internet Measurement Conference (IMC '15). ACM, New York, NY, USA, 211-224. https://doi.org/10.1145/2815675.2815679\n\nNetwork anomaly detection based on wavelet analysis. Wei Lu, Ali A Ghorbani, EURASIP Journal on Advances in Signal Processing. 4Wei Lu and Ali A Ghorbani. 2009. Network anomaly detection based on wavelet analysis. EURASIP Journal on Advances in Signal Processing 2009 (2009), 4.\n\nRapid detection of maintenance induced changes in service performance. Ajay Mahimkar, Zihui Ge, Jia Wang, Jennifer Yates, Yin Zhang, Joanne Emmons, Brian Huntley, Mark Stockert, Proceedings of the Seventh COnference on emerging Networking EXperiments and Technologies. the Seventh COnference on emerging Networking EXperiments and TechnologiesACM13Ajay Mahimkar, Zihui Ge, Jia Wang, Jennifer Yates, Yin Zhang, Joanne Emmons, Brian Huntley, and Mark Stockert. 2011. Rapid detection of maintenance induced changes in service performance. In Proceedings of the Seventh COnference on emerging Networking EXperiments and Technologies. ACM, 13.\n\nTraffic anomaly detection using k-means clustering. Gerhard M\u00fcnz, Sa Li, Georg Carle, GI/ITG Workshop MMBnet. Gerhard M\u00fcnz, Sa Li, and Georg Carle. 2007. Traffic anomaly detection using k-means clustering. In GI/ITG Workshop MMBnet.\n\nOne-Class Classification for Anomaly Detection with Kernel Density Estimation and Genetic Programming. Miguel Nicolau, James Mcdermott, European Conference on Genetic Programming. SpringerMiguel Nicolau, James McDermott, et al. 2016. One-Class Classification for Anomaly Detection with Kernel Density Estimation and Genetic Programming. In European Conference on Genetic Programming. Springer, 3-18.\n\nBayesian networks and decision graphs. Finn Thomas Dyhre Nielsen, Verner Jensen, Springer Science & Business MediaThomas Dyhre Nielsen and Finn Verner Jensen. 2009. Bayesian networks and decision graphs. Springer Science & Business Media.\n\nAnomaly detection in time series of graphs using arma processes. Brandon Pincombe, Asor Bulletin. 242Brandon Pincombe. 2005. Anomaly detection in time series of graphs using arma processes. Asor Bulletin 24, 4 (2005), 2.\n\nStochastic Backpropagation and Approximate Inference in Deep Generative Models. Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Proceedings of the 31st International Conference on International Conference on Machine Learning. the 31st International Conference on International Conference on Machine LearningBeijing, China3214II-1278-II-1286Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In Proceedings of the 31st International Conference on International Conference on Machine Learning -Volume 32 (ICML'14). JMLR.org, Beijing, China, II-1278-II- 1286.\n\nSensitivity of PCA for Traffic Anomaly Detection. Haakon Ringberg, Augustin Soule, Jennifer Rexford, Christophe Diot, Proceedings of the. theHaakon Ringberg, Augustin Soule, Jennifer Rexford, and Christophe Diot. 2007. Sensitivity of PCA for Traffic Anomaly Detection. In Proceedings of the 2007\n\n10.1145/1254882.1254895ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS '07). New York, NY, USAACMACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS '07). ACM, New York, NY, USA, 109-120. https: //doi.org/10.1145/1254882.1254895\n\nParallel networks that learn to pronounce English text. J Terrence, Charles R Sejnowski, Rosenberg, Complex systems. 1Terrence J Sejnowski and Charles R Rosenberg. 1987. Parallel networks that learn to pronounce English text. Complex systems 1, 1 (1987), 145-168.\n\nAccurate anomaly detection through parallelism. Shashank Shanbhag, Tilman Wolf, Network. 23IEEEShashank Shanbhag and Tilman Wolf. 2009. Accurate anomaly detection through parallelism. Network, IEEE 23, 1 (2009), 22-28.\n\nVariational inference for on-line anomaly detection in high-dimensional time series. Maximilian S\u00f6lch, Justin Bayer, Marvin Ludersdorfer, Patrick Van Der, Smagt, International Conference on Machine Laerning Anomaly detection Workshop. Maximilian S\u00f6lch, Justin Bayer, Marvin Ludersdorfer, and Patrick van der Smagt. 2016. Variational inference for on-line anomaly detection in high-dimensional time series. International Conference on Machine Laerning Anomaly detection Workshop (2016).\n\nMultiple imputation for missing data in epidemiological and clinical research: potential and pitfalls. A C Jonathan, Ian R Sterne, John B White, Michael Carlin, Patrick Spratt, Royston, G Michael, Angela M Kenward, James R Wood, Carpenter, Bmj. 3382393Jonathan AC Sterne, Ian R White, John B Carlin, Michael Spratt, Patrick Royston, Michael G Kenward, Angela M Wood, and James R Carpenter. 2009. Multiple imputation for missing data in epidemiological and clinical research: potential and pitfalls. Bmj 338 (2009), b2393.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems. 3104- 3112.\n\nHao Wang, Dit-Yan Yeung, arXiv:1604.01662Towards Bayesian deep learning: A survey. arXiv preprintHao Wang and Dit-Yan Yeung. 2016. Towards Bayesian deep learning: A survey. arXiv preprint arXiv:1604.01662 (2016).\n\nArima based network anomaly detection. H Asrul, Yaacob, K T Ian, Su Tan, Hon Khi Fong Chien, Tan, Communication Software and Networks, 2010. ICCSN'10. Second International Conference on. IEEE. Asrul H Yaacob, Ian KT Tan, Su Fong Chien, and Hon Khi Tan. 2010. Arima based network anomaly detection. In Communication Software and Networks, 2010. ICCSN'10. Second International Conference on. IEEE, 205-209.\n\nArgus: End-to-end service anomaly detection and localization from an ISP's point of view. He Yan, Ashley Flavel, Zihui Ge, Alexandre Gerber, Dan Massey, Christos Papadopoulos, Hiren Shah, Jennifer Yates, INFOCOM, 2012 Proceedings IEEE. He Yan, Ashley Flavel, Zihui Ge, Alexandre Gerber, Dan Massey, Christos Pa- padopoulos, Hiren Shah, and Jennifer Yates. 2012. Argus: End-to-end service anomaly detection and localization from an ISP's point of view. In INFOCOM, 2012 Proceedings IEEE. IEEE, 2756-2760.\n", "annotations": {"author": "[{\"end\":191,\"start\":124},{\"end\":262,\"start\":192},{\"end\":333,\"start\":263},{\"end\":400,\"start\":334},{\"end\":468,\"start\":401},{\"end\":536,\"start\":469},{\"end\":603,\"start\":537},{\"end\":674,\"start\":604},{\"end\":740,\"start\":675},{\"end\":808,\"start\":741},{\"end\":875,\"start\":809},{\"end\":947,\"start\":876},{\"end\":1018,\"start\":948},{\"end\":1086,\"start\":1019},{\"end\":1157,\"start\":1087},{\"end\":1228,\"start\":1158},{\"end\":1295,\"start\":1229},{\"end\":1363,\"start\":1296},{\"end\":1431,\"start\":1364},{\"end\":1498,\"start\":1432},{\"end\":1569,\"start\":1499},{\"end\":1635,\"start\":1570},{\"end\":1703,\"start\":1636},{\"end\":1770,\"start\":1704},{\"end\":1842,\"start\":1771},{\"end\":1913,\"start\":1843}]", "publisher": null, "author_last_name": "[{\"end\":133,\"start\":131},{\"end\":204,\"start\":200},{\"end\":275,\"start\":271},{\"end\":342,\"start\":340},{\"end\":410,\"start\":408},{\"end\":478,\"start\":476},{\"end\":545,\"start\":542},{\"end\":616,\"start\":612},{\"end\":682,\"start\":679},{\"end\":750,\"start\":746},{\"end\":817,\"start\":813},{\"end\":889,\"start\":885},{\"end\":960,\"start\":956},{\"end\":1028,\"start\":1026},{\"end\":1099,\"start\":1095},{\"end\":1170,\"start\":1166},{\"end\":1237,\"start\":1235},{\"end\":1305,\"start\":1303},{\"end\":1373,\"start\":1371},{\"end\":1440,\"start\":1437},{\"end\":1511,\"start\":1507},{\"end\":1577,\"start\":1574},{\"end\":1645,\"start\":1641},{\"end\":1712,\"start\":1708},{\"end\":1784,\"start\":1780}]", "author_first_name": "[{\"end\":130,\"start\":124},{\"end\":199,\"start\":192},{\"end\":270,\"start\":263},{\"end\":339,\"start\":334},{\"end\":407,\"start\":401},{\"end\":475,\"start\":469},{\"end\":541,\"start\":537},{\"end\":611,\"start\":604},{\"end\":678,\"start\":675},{\"end\":745,\"start\":741},{\"end\":812,\"start\":809},{\"end\":884,\"start\":876},{\"end\":955,\"start\":948},{\"end\":1025,\"start\":1019},{\"end\":1094,\"start\":1087},{\"end\":1165,\"start\":1158},{\"end\":1234,\"start\":1229},{\"end\":1302,\"start\":1296},{\"end\":1370,\"start\":1364},{\"end\":1436,\"start\":1432},{\"end\":1506,\"start\":1499},{\"end\":1573,\"start\":1570},{\"end\":1640,\"start\":1636},{\"end\":1707,\"start\":1704},{\"end\":1779,\"start\":1771},{\"end\":1850,\"start\":1843},{\"end\":1855,\"start\":1851}]", "author_affiliation": "[{\"end\":190,\"start\":135},{\"end\":261,\"start\":206},{\"end\":332,\"start\":277},{\"end\":399,\"start\":344},{\"end\":467,\"start\":412},{\"end\":535,\"start\":480},{\"end\":602,\"start\":547},{\"end\":673,\"start\":618},{\"end\":739,\"start\":684},{\"end\":807,\"start\":752},{\"end\":874,\"start\":819},{\"end\":946,\"start\":891},{\"end\":1017,\"start\":962},{\"end\":1085,\"start\":1030},{\"end\":1156,\"start\":1101},{\"end\":1227,\"start\":1172},{\"end\":1294,\"start\":1239},{\"end\":1362,\"start\":1307},{\"end\":1430,\"start\":1375},{\"end\":1497,\"start\":1442},{\"end\":1568,\"start\":1513},{\"end\":1634,\"start\":1579},{\"end\":1702,\"start\":1647},{\"end\":1769,\"start\":1714},{\"end\":1841,\"start\":1786},{\"end\":1912,\"start\":1857}]", "title": "[{\"end\":98,\"start\":1},{\"end\":2011,\"start\":1914}]", "venue": "[{\"end\":2046,\"start\":2013}]", "abstract": "[{\"end\":3235,\"start\":2328}]", "bib_ref": "[{\"end\":4086,\"start\":4032},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5401,\"start\":5398},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5404,\"start\":5401},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5667,\"start\":5664},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7742,\"start\":7739},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7745,\"start\":7742},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7748,\"start\":7745},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7751,\"start\":7748},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7754,\"start\":7751},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7757,\"start\":7754},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7760,\"start\":7757},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7763,\"start\":7760},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7766,\"start\":7763},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8137,\"start\":8134},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8160,\"start\":8156},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8203,\"start\":8199},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8449,\"start\":8445},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8469,\"start\":8465},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9182,\"start\":9179},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9184,\"start\":9182},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9214,\"start\":9211},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9232,\"start\":9228},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9245,\"start\":9241},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9255,\"start\":9251},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9268,\"start\":9265},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9282,\"start\":9278},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10018,\"start\":10014},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10048,\"start\":10045},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10051,\"start\":10048},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10132,\"start\":10128},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10179,\"start\":10176},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10198,\"start\":10194},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10201,\"start\":10198},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10350,\"start\":10347},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10618,\"start\":10614},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10709,\"start\":10706},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10793,\"start\":10789},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12165,\"start\":12161},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13114,\"start\":13110},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13117,\"start\":13114},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13666,\"start\":13662},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14184,\"start\":14180},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16640,\"start\":16636},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16680,\"start\":16676},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18004,\"start\":18000},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18782,\"start\":18779},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21714,\"start\":21711},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21911,\"start\":21908},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22032,\"start\":22028},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22184,\"start\":22180},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23031,\"start\":23027},{\"end\":25347,\"start\":25340},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26940,\"start\":26936},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29763,\"start\":29759},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31052,\"start\":31048},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31289,\"start\":31286},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31292,\"start\":31289},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31295,\"start\":31292},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":31298,\"start\":31295},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31301,\"start\":31298},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31304,\"start\":31301},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31307,\"start\":31304},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31310,\"start\":31307},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31313,\"start\":31310},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31711,\"start\":31708},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32005,\"start\":32002},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37675,\"start\":37671},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39590,\"start\":39587},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":39593,\"start\":39590},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39831,\"start\":39828},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":39834,\"start\":39831},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":40220,\"start\":40219},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42305,\"start\":42301},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":42308,\"start\":42305},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":44280,\"start\":44279},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46637,\"start\":46634},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46649,\"start\":46646},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46722,\"start\":46719},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46771,\"start\":46768},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46783,\"start\":46780},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46890,\"start\":46887},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46921,\"start\":46918},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46994,\"start\":46991},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":47012,\"start\":47009},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":47022,\"start\":47019},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":47043,\"start\":47040},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":47081,\"start\":47078},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":47148,\"start\":47145},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":50343,\"start\":50339},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":50346,\"start\":50343},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":50349,\"start\":50346},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":51923,\"start\":51919},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":52139,\"start\":52135},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":55910,\"start\":55909}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53249,\"start\":53009},{\"attributes\":{\"id\":\"fig_1\"},\"end\":53518,\"start\":53250},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53649,\"start\":53519},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53843,\"start\":53650},{\"attributes\":{\"id\":\"fig_4\"},\"end\":53865,\"start\":53844},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54168,\"start\":53866},{\"attributes\":{\"id\":\"fig_6\"},\"end\":54390,\"start\":54169},{\"attributes\":{\"id\":\"fig_7\"},\"end\":54630,\"start\":54391},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54920,\"start\":54631},{\"attributes\":{\"id\":\"fig_9\"},\"end\":55571,\"start\":54921},{\"attributes\":{\"id\":\"fig_10\"},\"end\":55626,\"start\":55572},{\"attributes\":{\"id\":\"fig_11\"},\"end\":55661,\"start\":55627},{\"attributes\":{\"id\":\"fig_13\"},\"end\":55717,\"start\":55662},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":55755,\"start\":55718}]", "paragraph": "[{\"end\":3971,\"start\":3251},{\"end\":4302,\"start\":3973},{\"end\":4616,\"start\":4304},{\"end\":4679,\"start\":4618},{\"end\":6099,\"start\":4681},{\"end\":6292,\"start\":6101},{\"end\":6793,\"start\":6294},{\"end\":7050,\"start\":6795},{\"end\":7596,\"start\":7052},{\"end\":8271,\"start\":7614},{\"end\":8995,\"start\":8273},{\"end\":9670,\"start\":8997},{\"end\":10824,\"start\":9672},{\"end\":11272,\"start\":10846},{\"end\":11873,\"start\":11274},{\"end\":12308,\"start\":11916},{\"end\":13414,\"start\":12310},{\"end\":13843,\"start\":13638},{\"end\":13974,\"start\":13908},{\"end\":15220,\"start\":13996},{\"end\":15381,\"start\":15287},{\"end\":16155,\"start\":15497},{\"end\":16552,\"start\":16157},{\"end\":17459,\"start\":16565},{\"end\":17788,\"start\":17461},{\"end\":18700,\"start\":17790},{\"end\":19705,\"start\":18702},{\"end\":19905,\"start\":19736},{\"end\":20244,\"start\":19989},{\"end\":20711,\"start\":20246},{\"end\":21383,\"start\":20725},{\"end\":22258,\"start\":21385},{\"end\":22516,\"start\":22260},{\"end\":22841,\"start\":22559},{\"end\":23151,\"start\":22843},{\"end\":23237,\"start\":23153},{\"end\":24588,\"start\":23436},{\"end\":25355,\"start\":24616},{\"end\":26617,\"start\":25379},{\"end\":28152,\"start\":26619},{\"end\":28290,\"start\":28154},{\"end\":29435,\"start\":28311},{\"end\":29998,\"start\":29437},{\"end\":30848,\"start\":30000},{\"end\":31035,\"start\":30872},{\"end\":31657,\"start\":31037},{\"end\":32103,\"start\":31659},{\"end\":32487,\"start\":32105},{\"end\":33581,\"start\":32489},{\"end\":34616,\"start\":33583},{\"end\":35049,\"start\":34618},{\"end\":35371,\"start\":35051},{\"end\":36341,\"start\":35403},{\"end\":37457,\"start\":36343},{\"end\":37764,\"start\":37459},{\"end\":37897,\"start\":37766},{\"end\":38301,\"start\":37913},{\"end\":38793,\"start\":38303},{\"end\":39463,\"start\":38795},{\"end\":39706,\"start\":39499},{\"end\":41028,\"start\":39757},{\"end\":41679,\"start\":41030},{\"end\":42917,\"start\":41681},{\"end\":43198,\"start\":42957},{\"end\":43821,\"start\":43200},{\"end\":44358,\"start\":43823},{\"end\":44471,\"start\":44360},{\"end\":44800,\"start\":44499},{\"end\":47566,\"start\":44915},{\"end\":47687,\"start\":47568},{\"end\":48077,\"start\":47689},{\"end\":49016,\"start\":48105},{\"end\":49857,\"start\":49018},{\"end\":50521,\"start\":49897},{\"end\":50970,\"start\":50523},{\"end\":51352,\"start\":50972},{\"end\":51751,\"start\":51368},{\"end\":52015,\"start\":51753},{\"end\":52287,\"start\":52017},{\"end\":52627,\"start\":52302},{\"end\":53008,\"start\":52629}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13637,\"start\":13415},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13892,\"start\":13844},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15286,\"start\":15221},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15496,\"start\":15382},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19735,\"start\":19706},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19988,\"start\":19906},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22558,\"start\":22517},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23435,\"start\":23238},{\"attributes\":{\"id\":\"formula_8\"},\"end\":39756,\"start\":39707},{\"attributes\":{\"id\":\"formula_9\"},\"end\":44914,\"start\":44801}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25142,\"start\":25135}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3249,\"start\":3237},{\"attributes\":{\"n\":\"2.2\"},\"end\":7612,\"start\":7599},{\"attributes\":{\"n\":\"2.3\"},\"end\":10844,\"start\":10827},{\"attributes\":{\"n\":\"2.4\"},\"end\":11914,\"start\":11876},{\"attributes\":{\"n\":\"3\"},\"end\":13906,\"start\":13894},{\"attributes\":{\"n\":\"3.1\"},\"end\":13994,\"start\":13977},{\"attributes\":{\"n\":\"3.2\"},\"end\":16563,\"start\":16555},{\"attributes\":{\"n\":\"3.3\"},\"end\":20723,\"start\":20714},{\"attributes\":{\"n\":\"4\"},\"end\":24614,\"start\":24591},{\"attributes\":{\"n\":\"4.2\"},\"end\":25377,\"start\":25358},{\"attributes\":{\"n\":\"4.3\"},\"end\":28309,\"start\":28293},{\"attributes\":{\"n\":\"4.4\"},\"end\":30870,\"start\":30851},{\"attributes\":{\"n\":\"4.5\"},\"end\":35401,\"start\":35374},{\"attributes\":{\"n\":\"4.6\"},\"end\":37911,\"start\":37900},{\"attributes\":{\"n\":\"5\"},\"end\":39497,\"start\":39466},{\"attributes\":{\"n\":\"5.2\"},\"end\":42955,\"start\":42920},{\"attributes\":{\"n\":\"5.3\"},\"end\":44497,\"start\":44474},{\"attributes\":{\"n\":\"5.4\"},\"end\":48103,\"start\":48080},{\"attributes\":{\"n\":\"6\"},\"end\":49895,\"start\":49860},{\"attributes\":{\"n\":\"6.2\"},\"end\":51366,\"start\":51355},{\"attributes\":{\"n\":\"7\"},\"end\":52300,\"start\":52290},{\"end\":53020,\"start\":53010},{\"end\":53271,\"start\":53251},{\"end\":53527,\"start\":53520},{\"end\":53661,\"start\":53651},{\"end\":53855,\"start\":53845},{\"end\":53877,\"start\":53867},{\"end\":54180,\"start\":54170},{\"end\":54414,\"start\":54392},{\"end\":54639,\"start\":54632},{\"end\":54933,\"start\":54922},{\"end\":55584,\"start\":55573},{\"end\":55639,\"start\":55628},{\"end\":55674,\"start\":55663},{\"end\":55728,\"start\":55719}]", "table": null, "figure_caption": "[{\"end\":53249,\"start\":53022},{\"end\":53518,\"start\":53274},{\"end\":53649,\"start\":53529},{\"end\":53843,\"start\":53663},{\"end\":53865,\"start\":53857},{\"end\":54168,\"start\":53879},{\"end\":54390,\"start\":54182},{\"end\":54630,\"start\":54419},{\"end\":54920,\"start\":54641},{\"end\":55571,\"start\":54936},{\"end\":55626,\"start\":55587},{\"end\":55661,\"start\":55642},{\"end\":55717,\"start\":55677},{\"end\":55755,\"start\":55730}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6490,\"start\":6480},{\"end\":11632,\"start\":11624},{\"end\":13104,\"start\":13098},{\"end\":14525,\"start\":14520},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18529,\"start\":18523},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18699,\"start\":18693},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20710,\"start\":20704},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23886,\"start\":23877},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23927,\"start\":23921},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24883,\"start\":24877},{\"end\":25078,\"start\":25053},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25104,\"start\":25098},{\"end\":26422,\"start\":26416},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27472,\"start\":27466},{\"end\":27800,\"start\":27792},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29189,\"start\":29182},{\"end\":30983,\"start\":30975},{\"end\":31874,\"start\":31868},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31917,\"start\":31911},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35537,\"start\":35530},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37896,\"start\":37890},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38313,\"start\":38306},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39365,\"start\":39358},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40096,\"start\":40089},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40408,\"start\":40401},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42539,\"start\":42530},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42649,\"start\":42642},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45303,\"start\":45292},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46028,\"start\":46021},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46224,\"start\":46217},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46587,\"start\":46574},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46620,\"start\":46611},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47161,\"start\":47152},{\"end\":47860,\"start\":47852},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":48204,\"start\":48168},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":48592,\"start\":48580},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":48637,\"start\":48629}]", "bib_author_first_name": "[{\"end\":56365,\"start\":56354},{\"end\":56378,\"start\":56372},{\"end\":56394,\"start\":56390},{\"end\":56874,\"start\":56868},{\"end\":56887,\"start\":56879},{\"end\":57186,\"start\":57173},{\"end\":57378,\"start\":57377},{\"end\":57523,\"start\":57518},{\"end\":57541,\"start\":57534},{\"end\":57557,\"start\":57552},{\"end\":57791,\"start\":57783},{\"end\":57803,\"start\":57798},{\"end\":57819,\"start\":57813},{\"end\":57837,\"start\":57831},{\"end\":58390,\"start\":58389},{\"end\":58408,\"start\":58398},{\"end\":58424,\"start\":58417},{\"end\":58449,\"start\":58438},{\"end\":58840,\"start\":58834},{\"end\":58857,\"start\":58851},{\"end\":58874,\"start\":58867},{\"end\":58888,\"start\":58881},{\"end\":59394,\"start\":59388},{\"end\":59406,\"start\":59399},{\"end\":59417,\"start\":59411},{\"end\":59786,\"start\":59782},{\"end\":60086,\"start\":60066},{\"end\":60278,\"start\":60275},{\"end\":60297,\"start\":60291},{\"end\":60311,\"start\":60306},{\"end\":60449,\"start\":60446},{\"end\":60466,\"start\":60462},{\"end\":60487,\"start\":60482},{\"end\":60499,\"start\":60495},{\"end\":60509,\"start\":60504},{\"end\":60531,\"start\":60524},{\"end\":60544,\"start\":60539},{\"end\":60562,\"start\":60556},{\"end\":60860,\"start\":60852},{\"end\":60873,\"start\":60869},{\"end\":60890,\"start\":60883},{\"end\":60905,\"start\":60899},{\"end\":61207,\"start\":61199},{\"end\":61221,\"start\":61216},{\"end\":61411,\"start\":61410},{\"end\":61425,\"start\":61422},{\"end\":61802,\"start\":61795},{\"end\":61811,\"start\":61810},{\"end\":62086,\"start\":62075},{\"end\":62112,\"start\":62102},{\"end\":62121,\"start\":62118},{\"end\":62132,\"start\":62129},{\"end\":62552,\"start\":62545},{\"end\":62566,\"start\":62562},{\"end\":62587,\"start\":62577},{\"end\":63277,\"start\":63270},{\"end\":63291,\"start\":63287},{\"end\":63312,\"start\":63302},{\"end\":64037,\"start\":64030},{\"end\":64051,\"start\":64046},{\"end\":64065,\"start\":64062},{\"end\":64598,\"start\":64589},{\"end\":64613,\"start\":64606},{\"end\":65050,\"start\":65044},{\"end\":65067,\"start\":65062},{\"end\":65082,\"start\":65077},{\"end\":65488,\"start\":65481},{\"end\":65497,\"start\":65494},{\"end\":65516,\"start\":65503},{\"end\":65536,\"start\":65529},{\"end\":65556,\"start\":65550},{\"end\":65563,\"start\":65561},{\"end\":65574,\"start\":65569},{\"end\":65587,\"start\":65579},{\"end\":65600,\"start\":65595},{\"end\":65959,\"start\":65953},{\"end\":65972,\"start\":65965},{\"end\":65985,\"start\":65979},{\"end\":65998,\"start\":65990},{\"end\":66007,\"start\":66004},{\"end\":66017,\"start\":66013},{\"end\":66030,\"start\":66023},{\"end\":66040,\"start\":66037},{\"end\":66653,\"start\":66650},{\"end\":66661,\"start\":66658},{\"end\":66663,\"start\":66662},{\"end\":66952,\"start\":66948},{\"end\":66968,\"start\":66963},{\"end\":66976,\"start\":66973},{\"end\":66991,\"start\":66983},{\"end\":67002,\"start\":66999},{\"end\":67016,\"start\":67010},{\"end\":67030,\"start\":67025},{\"end\":67044,\"start\":67040},{\"end\":67576,\"start\":67569},{\"end\":67585,\"start\":67583},{\"end\":67595,\"start\":67590},{\"end\":67860,\"start\":67854},{\"end\":67875,\"start\":67870},{\"end\":68195,\"start\":68191},{\"end\":68464,\"start\":68457},{\"end\":68700,\"start\":68694},{\"end\":68724,\"start\":68718},{\"end\":68738,\"start\":68734},{\"end\":69329,\"start\":69323},{\"end\":69348,\"start\":69340},{\"end\":69364,\"start\":69356},{\"end\":69384,\"start\":69374},{\"end\":69957,\"start\":69956},{\"end\":69977,\"start\":69968},{\"end\":70221,\"start\":70213},{\"end\":70238,\"start\":70232},{\"end\":70480,\"start\":70470},{\"end\":70494,\"start\":70488},{\"end\":70508,\"start\":70502},{\"end\":70530,\"start\":70523},{\"end\":70976,\"start\":70975},{\"end\":70978,\"start\":70977},{\"end\":70992,\"start\":70989},{\"end\":70994,\"start\":70993},{\"end\":71007,\"start\":71003},{\"end\":71009,\"start\":71008},{\"end\":71024,\"start\":71017},{\"end\":71040,\"start\":71033},{\"end\":71059,\"start\":71058},{\"end\":71075,\"start\":71069},{\"end\":71077,\"start\":71076},{\"end\":71094,\"start\":71087},{\"end\":71451,\"start\":71447},{\"end\":71468,\"start\":71463},{\"end\":71484,\"start\":71478},{\"end\":71714,\"start\":71711},{\"end\":71728,\"start\":71721},{\"end\":71965,\"start\":71964},{\"end\":71982,\"start\":71981},{\"end\":71984,\"start\":71983},{\"end\":71992,\"start\":71990},{\"end\":72005,\"start\":71998},{\"end\":72423,\"start\":72421},{\"end\":72435,\"start\":72429},{\"end\":72449,\"start\":72444},{\"end\":72463,\"start\":72454},{\"end\":72475,\"start\":72472},{\"end\":72492,\"start\":72484},{\"end\":72512,\"start\":72507},{\"end\":72527,\"start\":72519}]", "bib_author_last_name": "[{\"end\":56370,\"start\":56366},{\"end\":56388,\"start\":56379},{\"end\":56406,\"start\":56395},{\"end\":56877,\"start\":56875},{\"end\":56891,\"start\":56888},{\"end\":57191,\"start\":57187},{\"end\":57390,\"start\":57379},{\"end\":57398,\"start\":57392},{\"end\":57532,\"start\":57524},{\"end\":57550,\"start\":57542},{\"end\":57563,\"start\":57558},{\"end\":57796,\"start\":57792},{\"end\":57811,\"start\":57804},{\"end\":57829,\"start\":57820},{\"end\":57843,\"start\":57838},{\"end\":58396,\"start\":58391},{\"end\":58415,\"start\":58409},{\"end\":58436,\"start\":58425},{\"end\":58462,\"start\":58450},{\"end\":58470,\"start\":58464},{\"end\":58849,\"start\":58841},{\"end\":58865,\"start\":58858},{\"end\":58879,\"start\":58875},{\"end\":58895,\"start\":58889},{\"end\":59397,\"start\":59395},{\"end\":59409,\"start\":59407},{\"end\":59421,\"start\":59418},{\"end\":59793,\"start\":59787},{\"end\":60094,\"start\":60087},{\"end\":60289,\"start\":60279},{\"end\":60304,\"start\":60298},{\"end\":60321,\"start\":60312},{\"end\":60460,\"start\":60450},{\"end\":60480,\"start\":60467},{\"end\":60493,\"start\":60488},{\"end\":60502,\"start\":60500},{\"end\":60522,\"start\":60510},{\"end\":60537,\"start\":60532},{\"end\":60554,\"start\":60545},{\"end\":60569,\"start\":60563},{\"end\":60867,\"start\":60861},{\"end\":60881,\"start\":60874},{\"end\":60897,\"start\":60891},{\"end\":60914,\"start\":60906},{\"end\":61214,\"start\":61208},{\"end\":61224,\"start\":61222},{\"end\":61420,\"start\":61412},{\"end\":61432,\"start\":61426},{\"end\":61441,\"start\":61434},{\"end\":61808,\"start\":61803},{\"end\":61819,\"start\":61812},{\"end\":61826,\"start\":61821},{\"end\":62100,\"start\":62087},{\"end\":62116,\"start\":62113},{\"end\":62127,\"start\":62122},{\"end\":62137,\"start\":62133},{\"end\":62560,\"start\":62553},{\"end\":62575,\"start\":62567},{\"end\":62592,\"start\":62588},{\"end\":63285,\"start\":63278},{\"end\":63300,\"start\":63292},{\"end\":63317,\"start\":63313},{\"end\":64044,\"start\":64038},{\"end\":64060,\"start\":64052},{\"end\":64071,\"start\":64066},{\"end\":64604,\"start\":64599},{\"end\":64619,\"start\":64614},{\"end\":65060,\"start\":65051},{\"end\":65075,\"start\":65068},{\"end\":65092,\"start\":65083},{\"end\":65492,\"start\":65489},{\"end\":65501,\"start\":65498},{\"end\":65527,\"start\":65517},{\"end\":65548,\"start\":65537},{\"end\":65559,\"start\":65557},{\"end\":65567,\"start\":65564},{\"end\":65577,\"start\":65575},{\"end\":65593,\"start\":65588},{\"end\":65609,\"start\":65601},{\"end\":65963,\"start\":65960},{\"end\":65977,\"start\":65973},{\"end\":65988,\"start\":65986},{\"end\":66002,\"start\":65999},{\"end\":66011,\"start\":66008},{\"end\":66021,\"start\":66018},{\"end\":66035,\"start\":66031},{\"end\":66045,\"start\":66041},{\"end\":66656,\"start\":66654},{\"end\":66672,\"start\":66664},{\"end\":66961,\"start\":66953},{\"end\":66971,\"start\":66969},{\"end\":66981,\"start\":66977},{\"end\":66997,\"start\":66992},{\"end\":67008,\"start\":67003},{\"end\":67023,\"start\":67017},{\"end\":67038,\"start\":67031},{\"end\":67053,\"start\":67045},{\"end\":67581,\"start\":67577},{\"end\":67588,\"start\":67586},{\"end\":67601,\"start\":67596},{\"end\":67868,\"start\":67861},{\"end\":67885,\"start\":67876},{\"end\":68216,\"start\":68196},{\"end\":68231,\"start\":68218},{\"end\":68473,\"start\":68465},{\"end\":68716,\"start\":68701},{\"end\":68732,\"start\":68725},{\"end\":68747,\"start\":68739},{\"end\":69338,\"start\":69330},{\"end\":69354,\"start\":69349},{\"end\":69372,\"start\":69365},{\"end\":69389,\"start\":69385},{\"end\":69966,\"start\":69958},{\"end\":69987,\"start\":69978},{\"end\":69998,\"start\":69989},{\"end\":70230,\"start\":70222},{\"end\":70243,\"start\":70239},{\"end\":70486,\"start\":70481},{\"end\":70500,\"start\":70495},{\"end\":70521,\"start\":70509},{\"end\":70538,\"start\":70531},{\"end\":70545,\"start\":70540},{\"end\":70987,\"start\":70979},{\"end\":71001,\"start\":70995},{\"end\":71015,\"start\":71010},{\"end\":71031,\"start\":71025},{\"end\":71047,\"start\":71041},{\"end\":71056,\"start\":71049},{\"end\":71067,\"start\":71060},{\"end\":71085,\"start\":71078},{\"end\":71099,\"start\":71095},{\"end\":71110,\"start\":71101},{\"end\":71461,\"start\":71452},{\"end\":71476,\"start\":71469},{\"end\":71487,\"start\":71485},{\"end\":71719,\"start\":71715},{\"end\":71734,\"start\":71729},{\"end\":71971,\"start\":71966},{\"end\":71979,\"start\":71973},{\"end\":71988,\"start\":71985},{\"end\":71996,\"start\":71993},{\"end\":72016,\"start\":72006},{\"end\":72021,\"start\":72018},{\"end\":72427,\"start\":72424},{\"end\":72442,\"start\":72436},{\"end\":72452,\"start\":72450},{\"end\":72470,\"start\":72464},{\"end\":72482,\"start\":72476},{\"end\":72505,\"start\":72493},{\"end\":72517,\"start\":72513},{\"end\":72533,\"start\":72528}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":10611065},\"end\":56784,\"start\":56274},{\"attributes\":{\"id\":\"b1\"},\"end\":57112,\"start\":56786},{\"attributes\":{\"id\":\"b2\"},\"end\":57333,\"start\":57114},{\"attributes\":{\"id\":\"b3\"},\"end\":57487,\"start\":57335},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207172599},\"end\":57731,\"start\":57489},{\"attributes\":{\"doi\":\"10.1145/2486001.2486035\",\"id\":\"b5\",\"matched_paper_id\":7216027},\"end\":58287,\"start\":57733},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":19002372},\"end\":58726,\"start\":58289},{\"attributes\":{\"doi\":\"10.1145/1921168.1921179\",\"id\":\"b7\",\"matched_paper_id\":7798047},\"end\":59316,\"start\":58728},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15323603},\"end\":59708,\"start\":59318},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":121945365},\"end\":60003,\"start\":59710},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":121518811},\"end\":60258,\"start\":60005},{\"attributes\":{\"id\":\"b11\"},\"end\":60415,\"start\":60260},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1033682},\"end\":60850,\"start\":60417},{\"attributes\":{\"id\":\"b13\"},\"end\":61153,\"start\":60852},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b14\"},\"end\":61375,\"start\":61155},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":216078090},\"end\":61721,\"start\":61377},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17080213},\"end\":62003,\"start\":61723},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10441720},\"end\":62500,\"start\":62005},{\"attributes\":{\"doi\":\"10.1145/1015467.1015492\",\"id\":\"b18\",\"matched_paper_id\":3098086},\"end\":63214,\"start\":62502},{\"attributes\":{\"doi\":\"10.1145/1080091.1080118\",\"id\":\"b19\",\"matched_paper_id\":2028615},\"end\":63952,\"start\":63216},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":207227428},\"end\":64506,\"start\":63954},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6842305},\"end\":64932,\"start\":64508},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13519036},\"end\":65429,\"start\":64934},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":555807},\"end\":65863,\"start\":65431},{\"attributes\":{\"doi\":\"10.1145/2815675.2815679\",\"id\":\"b24\",\"matched_paper_id\":2707750},\"end\":66595,\"start\":65865},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14850912},\"end\":66875,\"start\":66597},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1958648},\"end\":67515,\"start\":66877},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2303041},\"end\":67749,\"start\":67517},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":12041280},\"end\":68150,\"start\":67751},{\"attributes\":{\"id\":\"b29\"},\"end\":68390,\"start\":68152},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15281413},\"end\":68612,\"start\":68392},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":16895865},\"end\":69271,\"start\":68614},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1237823},\"end\":69568,\"start\":69273},{\"attributes\":{\"doi\":\"10.1145/1254882.1254895\",\"id\":\"b33\"},\"end\":69898,\"start\":69570},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12926318},\"end\":70163,\"start\":69900},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":17411204},\"end\":70383,\"start\":70165},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7794175},\"end\":70870,\"start\":70385},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":31568154},\"end\":71393,\"start\":70872},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":7961699},\"end\":71709,\"start\":71395},{\"attributes\":{\"doi\":\"arXiv:1604.01662\",\"id\":\"b39\"},\"end\":71923,\"start\":71711},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":18178967},\"end\":72329,\"start\":71925},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":18303205},\"end\":72834,\"start\":72331}]", "bib_title": "[{\"end\":56352,\"start\":56274},{\"end\":57516,\"start\":57489},{\"end\":57781,\"start\":57733},{\"end\":58387,\"start\":58289},{\"end\":58832,\"start\":58728},{\"end\":59386,\"start\":59318},{\"end\":59780,\"start\":59710},{\"end\":60064,\"start\":60005},{\"end\":60444,\"start\":60417},{\"end\":61408,\"start\":61377},{\"end\":61793,\"start\":61723},{\"end\":62073,\"start\":62005},{\"end\":62543,\"start\":62502},{\"end\":63268,\"start\":63216},{\"end\":64028,\"start\":63954},{\"end\":64587,\"start\":64508},{\"end\":65042,\"start\":64934},{\"end\":65479,\"start\":65431},{\"end\":65951,\"start\":65865},{\"end\":66648,\"start\":66597},{\"end\":66946,\"start\":66877},{\"end\":67567,\"start\":67517},{\"end\":67852,\"start\":67751},{\"end\":68455,\"start\":68392},{\"end\":68692,\"start\":68614},{\"end\":69321,\"start\":69273},{\"end\":69954,\"start\":69900},{\"end\":70211,\"start\":70165},{\"end\":70468,\"start\":70385},{\"end\":70973,\"start\":70872},{\"end\":71445,\"start\":71395},{\"end\":71962,\"start\":71925},{\"end\":72419,\"start\":72331}]", "bib_author": "[{\"end\":56372,\"start\":56354},{\"end\":56390,\"start\":56372},{\"end\":56408,\"start\":56390},{\"end\":56879,\"start\":56868},{\"end\":56893,\"start\":56879},{\"end\":57193,\"start\":57173},{\"end\":57392,\"start\":57377},{\"end\":57400,\"start\":57392},{\"end\":57534,\"start\":57518},{\"end\":57552,\"start\":57534},{\"end\":57565,\"start\":57552},{\"end\":57798,\"start\":57783},{\"end\":57813,\"start\":57798},{\"end\":57831,\"start\":57813},{\"end\":57845,\"start\":57831},{\"end\":58398,\"start\":58389},{\"end\":58417,\"start\":58398},{\"end\":58438,\"start\":58417},{\"end\":58464,\"start\":58438},{\"end\":58472,\"start\":58464},{\"end\":58851,\"start\":58834},{\"end\":58867,\"start\":58851},{\"end\":58881,\"start\":58867},{\"end\":58897,\"start\":58881},{\"end\":59399,\"start\":59388},{\"end\":59411,\"start\":59399},{\"end\":59423,\"start\":59411},{\"end\":59795,\"start\":59782},{\"end\":60096,\"start\":60066},{\"end\":60291,\"start\":60275},{\"end\":60306,\"start\":60291},{\"end\":60323,\"start\":60306},{\"end\":60462,\"start\":60446},{\"end\":60482,\"start\":60462},{\"end\":60495,\"start\":60482},{\"end\":60504,\"start\":60495},{\"end\":60524,\"start\":60504},{\"end\":60539,\"start\":60524},{\"end\":60556,\"start\":60539},{\"end\":60571,\"start\":60556},{\"end\":60869,\"start\":60852},{\"end\":60883,\"start\":60869},{\"end\":60899,\"start\":60883},{\"end\":60916,\"start\":60899},{\"end\":61216,\"start\":61199},{\"end\":61226,\"start\":61216},{\"end\":61422,\"start\":61410},{\"end\":61434,\"start\":61422},{\"end\":61443,\"start\":61434},{\"end\":61810,\"start\":61795},{\"end\":61821,\"start\":61810},{\"end\":61828,\"start\":61821},{\"end\":62102,\"start\":62075},{\"end\":62118,\"start\":62102},{\"end\":62129,\"start\":62118},{\"end\":62139,\"start\":62129},{\"end\":62562,\"start\":62545},{\"end\":62577,\"start\":62562},{\"end\":62594,\"start\":62577},{\"end\":63287,\"start\":63270},{\"end\":63302,\"start\":63287},{\"end\":63319,\"start\":63302},{\"end\":64046,\"start\":64030},{\"end\":64062,\"start\":64046},{\"end\":64073,\"start\":64062},{\"end\":64606,\"start\":64589},{\"end\":64621,\"start\":64606},{\"end\":65062,\"start\":65044},{\"end\":65077,\"start\":65062},{\"end\":65094,\"start\":65077},{\"end\":65494,\"start\":65481},{\"end\":65503,\"start\":65494},{\"end\":65529,\"start\":65503},{\"end\":65550,\"start\":65529},{\"end\":65561,\"start\":65550},{\"end\":65569,\"start\":65561},{\"end\":65579,\"start\":65569},{\"end\":65595,\"start\":65579},{\"end\":65611,\"start\":65595},{\"end\":65965,\"start\":65953},{\"end\":65979,\"start\":65965},{\"end\":65990,\"start\":65979},{\"end\":66004,\"start\":65990},{\"end\":66013,\"start\":66004},{\"end\":66023,\"start\":66013},{\"end\":66037,\"start\":66023},{\"end\":66047,\"start\":66037},{\"end\":66658,\"start\":66650},{\"end\":66674,\"start\":66658},{\"end\":66963,\"start\":66948},{\"end\":66973,\"start\":66963},{\"end\":66983,\"start\":66973},{\"end\":66999,\"start\":66983},{\"end\":67010,\"start\":66999},{\"end\":67025,\"start\":67010},{\"end\":67040,\"start\":67025},{\"end\":67055,\"start\":67040},{\"end\":67583,\"start\":67569},{\"end\":67590,\"start\":67583},{\"end\":67603,\"start\":67590},{\"end\":67870,\"start\":67854},{\"end\":67887,\"start\":67870},{\"end\":68218,\"start\":68191},{\"end\":68233,\"start\":68218},{\"end\":68475,\"start\":68457},{\"end\":68718,\"start\":68694},{\"end\":68734,\"start\":68718},{\"end\":68749,\"start\":68734},{\"end\":69340,\"start\":69323},{\"end\":69356,\"start\":69340},{\"end\":69374,\"start\":69356},{\"end\":69391,\"start\":69374},{\"end\":69968,\"start\":69956},{\"end\":69989,\"start\":69968},{\"end\":70000,\"start\":69989},{\"end\":70232,\"start\":70213},{\"end\":70245,\"start\":70232},{\"end\":70488,\"start\":70470},{\"end\":70502,\"start\":70488},{\"end\":70523,\"start\":70502},{\"end\":70540,\"start\":70523},{\"end\":70547,\"start\":70540},{\"end\":70989,\"start\":70975},{\"end\":71003,\"start\":70989},{\"end\":71017,\"start\":71003},{\"end\":71033,\"start\":71017},{\"end\":71049,\"start\":71033},{\"end\":71058,\"start\":71049},{\"end\":71069,\"start\":71058},{\"end\":71087,\"start\":71069},{\"end\":71101,\"start\":71087},{\"end\":71112,\"start\":71101},{\"end\":71463,\"start\":71447},{\"end\":71478,\"start\":71463},{\"end\":71489,\"start\":71478},{\"end\":71721,\"start\":71711},{\"end\":71736,\"start\":71721},{\"end\":71973,\"start\":71964},{\"end\":71981,\"start\":71973},{\"end\":71990,\"start\":71981},{\"end\":71998,\"start\":71990},{\"end\":72018,\"start\":71998},{\"end\":72023,\"start\":72018},{\"end\":72429,\"start\":72421},{\"end\":72444,\"start\":72429},{\"end\":72454,\"start\":72444},{\"end\":72472,\"start\":72454},{\"end\":72484,\"start\":72472},{\"end\":72507,\"start\":72484},{\"end\":72519,\"start\":72507},{\"end\":72535,\"start\":72519}]", "bib_venue": "[{\"end\":56545,\"start\":56485},{\"end\":58014,\"start\":57941},{\"end\":59001,\"start\":58969},{\"end\":61572,\"start\":61516},{\"end\":62264,\"start\":62210},{\"end\":62893,\"start\":62755},{\"end\":63618,\"start\":63480},{\"end\":64256,\"start\":64173},{\"end\":66240,\"start\":66155},{\"end\":67220,\"start\":67146},{\"end\":68942,\"start\":68847},{\"end\":69414,\"start\":69411},{\"end\":69716,\"start\":69699},{\"end\":56483,\"start\":56408},{\"end\":56866,\"start\":56786},{\"end\":57171,\"start\":57114},{\"end\":57375,\"start\":57335},{\"end\":57593,\"start\":57565},{\"end\":57939,\"start\":57868},{\"end\":58491,\"start\":58472},{\"end\":58967,\"start\":58920},{\"end\":59490,\"start\":59423},{\"end\":59843,\"start\":59795},{\"end\":60115,\"start\":60096},{\"end\":60273,\"start\":60260},{\"end\":60620,\"start\":60571},{\"end\":60989,\"start\":60916},{\"end\":61197,\"start\":61155},{\"end\":61514,\"start\":61443},{\"end\":61845,\"start\":61828},{\"end\":62208,\"start\":62139},{\"end\":62753,\"start\":62617},{\"end\":63478,\"start\":63342},{\"end\":64171,\"start\":64073},{\"end\":64664,\"start\":64621},{\"end\":65169,\"start\":65094},{\"end\":65618,\"start\":65611},{\"end\":66153,\"start\":66070},{\"end\":66722,\"start\":66674},{\"end\":67144,\"start\":67055},{\"end\":67625,\"start\":67603},{\"end\":67929,\"start\":67887},{\"end\":68189,\"start\":68152},{\"end\":68488,\"start\":68475},{\"end\":68845,\"start\":68749},{\"end\":69409,\"start\":69391},{\"end\":69697,\"start\":69593},{\"end\":70015,\"start\":70000},{\"end\":70252,\"start\":70245},{\"end\":70618,\"start\":70547},{\"end\":71115,\"start\":71112},{\"end\":71538,\"start\":71489},{\"end\":71792,\"start\":71752},{\"end\":72116,\"start\":72023},{\"end\":72565,\"start\":72535}]"}}}, "year": 2023, "month": 12, "day": 17}