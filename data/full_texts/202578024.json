{"id": 202578024, "updated": "2023-10-06 23:20:25.327", "metadata": {"title": "Torchmeta: A Meta-Learning library for PyTorch", "authors": "[{\"first\":\"Tristan\",\"last\":\"Deleu\",\"middle\":[]},{\"first\":\"Tobias\",\"last\":\"Wurfl\",\"middle\":[]},{\"first\":\"Mandana\",\"last\":\"Samiei\",\"middle\":[]},{\"first\":\"Joseph\",\"last\":\"Cohen\",\"middle\":[\"Paul\"]},{\"first\":\"Yoshua\",\"last\":\"Bengio\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 9, "day": 14}, "abstract": "The constant introduction of standardized benchmarks in the literature has helped accelerating the recent advances in meta-learning research. They offer a way to get a fair comparison between different algorithms, and the wide range of datasets available allows full control over the complexity of this evaluation. However, for a large majority of code available online, the data pipeline is often specific to one dataset, and testing on another dataset requires significant rework. We introduce Torchmeta, a library built on top of PyTorch that enables seamless and consistent evaluation of meta-learning algorithms on multiple datasets, by providing data-loaders for most of the standard benchmarks in few-shot classification and regression, with a new meta-dataset abstraction. It also features some extensions for PyTorch to simplify the development of models compatible with meta-learning algorithms. The code is available here: https://github.com/tristandeleu/pytorch-meta", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1909.06576", "mag": "2972518327", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1909-06576", "doi": null}}, "content": {"source": {"pdf_hash": "6c72c4ef7c76b4d64fa4c244ec38d70de053d97e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1909.06576v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b6907509924d349f35eafc6ae5afdaed8ad98fe4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6c72c4ef7c76b4d64fa4c244ec38d70de053d97e.txt", "contents": "\nTorchmeta: A Meta-Learning library for PyTorch\n14 Sep 2019\n\nTristan Deleu \nTobias W\u00fcrfl \nMandana Samiei \nJoseph Paul Cohen \nYoshua Bengio \nMila -Montreal \nCanada \nTorchmeta: A Meta-Learning library for PyTorch\n14 Sep 2019\nThe constant introduction of standardized benchmarks in the literature has helped accelerating the recent advances in meta-learning research. They offer a way to get a fair comparison between different algorithms, and the wide range of datasets available allows full control over the complexity of this evaluation. However, for a large majority of code available online, the data pipeline is often specific to one dataset, and testing on another dataset requires significant rework. We introduce Torchmeta, a library built on top of PyTorch that enables seamless and consistent evaluation of meta-learning algorithms on multiple datasets, by providing data-loaders for most of the standard benchmarks in few-shot classification and regression, with a new meta-dataset abstraction. It also features some extensions for PyTorch to simplify the development of models compatible with meta-learning algorithms. The code is available here: https://github.com/tristandeleu/pytorch-meta.\n\nIntroduction\n\nLike for any subfield of machine learning, the existence of standardized benchmarks has played a crucial role in the progress we have observed over the past few years in meta-learning research. They make the evaluation of existing methods easier and fair, which in turn serves as a reference point for the development of new meta-learning algorithms; this creates a virtuous circle, rooted into these well-defined suites of tasks. Unlike existing datasets in supervised learning though, such as MNIST (LeCun et al., 1998) or ImageNet (Russakovsky et al., 2015), the benchmarks in meta-learning consist in datasets of datasets. This adds a layer of complexity to the data pipeline, to the extent that a majority of meta-learning projects implement their own specific data-loading component adapted to their method. The lack of a standard at the input level creates variance in the mechanisms surrounding each meta-learning algorithm, which makes a fair comparison more challenging.\n\nAlthough the implementation might be different from one project to another, the process by which these datasets of datasets are created is generally the same across tasks. In this paper we introduce Torchmeta, a meta-learning library built on top of the PyTorch deep learning framework (Paszke et al., 2017), providing data-loaders for most of the standard datasets for few-shot classification and regression. Torchmeta uses the same interface for all the available benchmarks, making the transition between different datasets as seamless as possible. Inspired by previous efforts to design a unified interface between tasks, such as OpenAI Gym (Brockman et al., 2016) in reinforcement learning, the goal of Torchmeta is to create a framework around which researchers can build their own meta-learning algorithms, rather than adapting the data pipeline to their methods. This new abstraction promotes code reuse, by decoupling meta-datasets from the algorithm itself.\n\nIn addition to these data-loaders, Torchmeta also includes extensions of PyTorch to simplify the creation of models compatible with classic meta-learning algorithms that sometimes require higher-order differentiation (Finn et al., 2017;Finn, 2018;Rusu et al., 2018;Grant et al., 2018). This paper gives an overall overview of the features currently available in Torchmeta, and is organized as follows: Section 2 gives a general presentation of the data-loaders available in the library; in Section 3, we focus on an extension of PyTorch's modules called \"meta-modules\" designed specifically for meta-learning, and we conclude by a discussion in Section 4.\n\n\nData-loaders for few-shot learning\n\nThe library provides a collection of datasets corresponding to classic few-shot classification and regression problems from the meta-learning literature. The interface was created to support modularity between datasets, for both classification and regression, to simplify the process of evaluation on a full suite of benchmarks; we will detail this interface in the following sections. Moreover, the data-loaders from Torchmeta are fully compatible with standard data components of PyTorch, such as Dataset and DataLoader. Before going into the details of the library, we first briefly recall the problem setting.\n\nTo balance the lack of data inherent in few-shot learning, meta-learning algorithms acquire some prior knowledge from a collection of datasets D meta = {D 1 , . . . , D n }, called the meta-training set.\n\nIn the context of few-shot learning, each element D i contains only a few inputs/output pairs (x, y), where y depends on the nature of the problem. For instance, these datasets can contain examples of different tasks performed in the past. Torchmeta offers a solution to automate the creation of each dataset D i , with a minimal amount of problem-specific components.\n\n\nFew-shot regression\n\nA majority of the few-shot regression problems in the literature are simple regression problems between inputs and outputs through different functions, where each function corresponds to a task. These functions are parametrized to allow variability between tasks, while preserving a constant \"theme\" across tasks. For example, these functions can be sine waves of the form f i (x) = a i sin(x + b i ), with a and b varying in some range (Finn et al., 2017). In Torchmeta, the meta-training set inherits from an object called MetaDataset, and each dataset D i (i = 1, . . . , n, with n defined by the user) corresponds to a specific choice of parameters for the function, with all the parameters sampled once at the creation of the meta-training set. Once the parameters of the function are known, we can create the dataset by sampling inputs in a given range, and feeding them to the function.\n\nThe library currently contains 3 toy problems: sine waves (Finn et al., 2017), harmonic function (i.e. sum of two sine waves, Lacoste et al., 2018), and sinusoid and lines . Below is an example of how to instantiate the meta-training set for the sine waves problem: torchmeta.toy.Sinusoid(num_samples_per_task=10, num_tasks=1_000_000, noise_std=None)\n\n\nFew-shot classification\n\nFor few-shot classification problems, the creation of the datasets D i usually follows two steps: first N classes are sampled from a large collection of candidates (corresponding to N in \"N -way classification\"), and then k examples are chosen per class (corresponding to k in \"k-shot learning\"). This two-step process is automated as part of an object called CombinationMetaDataset, inherited from MetaDataset, provided that the user specifies the large collection of class candidates, which is problem-specific. Moreover, to encourage reproducibility in meta-learning, every task is associated to a unique identifier (the N -tuple of class identifiers). Once the task has been chosen, the object returns a dataset D i with all the examples from the corresponding set of classes. In Section 2.3, we will describe how D i can then be further split into training and test datasets, as is common in meta-learning.\n\nThe library currently contains 5 few-shot classification problems: Omniglot (Lake et al., 2015(Lake et al., , 2019, Mini-ImageNet (Vinyals et al., 2016;Ravi and Larochelle, 2017), Tiered-ImageNet (Ren et al., 2018), CIFAR-FS (Bertinetto et al., 2018), and Fewshot-CIFAR100 . Below is an example of how to instantiate the meta-training set for 5-way Mini-ImageNet: torchmeta.datasets.MiniImagenet(\"data\", num_classes_per_task=5, meta_train=True, download=True)\n\nTorchmeta also includes helpful functions to augment the pool of class candidates with variants, such as rotated images (Santoro et al., 2016).\n\n\nTraining & test datasets split\n\nIn meta-learning, it is common to separate each dataset D i in two parts: a training set (or support set) to adapt the model to the task at hand, and a test set (or query set) for evaluation and metaoptimization. It is important to ensure that these two parts do not overlap though: while the task remains the same, no example can be in both the training and test sets. To ensure that, Torchmeta introduces a wrapper over the datasets called a Splitter that is responsible for creating the training and test datasets, as well as optionally shuffling the data. Here is an example of how to instantiate the meta-training set of a 5-way 1-shot classification problem based on Mini-Imagenet: dataset = torchmeta.datasets.MiniImagenet(\"data\", num_classes_per_task=5, meta_train=True, download=True) dataset = torchmeta.transforms.ClassSplitter (dataset,num_train_per_class=1,num_test_per_class=15,shuffle=True) in addition to the meta-training set, most benchmarks also provide a meta-test set for the overall evaluation of the meta-learning algorithm (and possible a meta-validation set as well). These different meta-datasets can be selected when the MetaDataset object is created, with meta_test=True (or meta_val=True) instead of meta_train=True.\n\n\nMeta Data-loaders\n\nThe objects presented in Sections 2. 3 Meta-learning modules Models in PyTorch are created from basic components called modules. Each basic module, equivalent to a layer in a neural network, contains both the computational graph of that layer, as well as its parameters. The modules treat their parameters as an integral part of their computational graph; in standard supervised learning, this is sufficient to train a model with backpropagation. However some meta-learning algorithms require to backpropagate through an update of the parameters (like a gradient update, Finn et al., 2017) for the meta-optimization (or the \"outer-loop\"), hence involving higher-order differentiation. Although high-order differentiation is available in PyTorch as part of its automatic differentiation module (Paszke et al., 2017), replacing one parameter of a basic module with a full computational graph (i.e. the update of the parameter), without altering the way gradients flow, is not obvious.\n\nBackpropagation through an update of the parameters is a key ingredient of gradient-based metalearning methods (Finn et al., 2017;Finn, 2018;Grant et al., 2018;Lee et al., 2019), and various hybrid methods (Rusu et al., 2018;Zintgraf et al., 2019). It is therefore critical to adapt the existing modules in PyTorch so they can handle arbitrary computational graphs as a substitute for these parameters. The approach taken by Torchmeta is to extend these modules, and leave an option to provide new parameters as an additional input. These new objects are called MetaModule, and their default behaviour (i.e. without any extra parameter specified) is equivalent to their PyTorch   this is equivalent to PyTorch's Linear module. Finally, the figure on the right shows how these placeholders can be filled with a complete computational graph, like one step of gradient descent (Finn et al., 2017). In this latter case, the gradient of L outer with respect to W , necessary in the outer-loop update, can correctly flow all the way to the parameter W .\n\u00d7 + W b params: N/A MetaLinear L inner ytrain xtrain W \u2032 = W \u2212 \u03b1\u2207W L & b \u2032 = b \u2212 \u03b1\u2207 b L ytest = model(xtest, params={W \u2032 , b \u2032 }) \u00d7 + \u00d7\u03b1 \u2202L \u2202W \u2212 W b \u00d7\u03b1 \u2202L \u2202b \u2212 b W params: W \u2032 b \u2032 MetaLinear L outer ytest xtest\n\nDiscussion\n\nReproducibility of data pipelines is challenging. It is even more challenging that some early works, even though they were evaluated on benchmarks that now became classic, did not disclose the set of classes available for meta-training and meta-test (and possibly meta-validation) in few-shot classification. For example, while the Mini-ImageNet dataset was introduced in (Vinyals et al., 2016), the split used in (Ravi and Larochelle, 2017) is now widely accepted in the community as the official dataset. The advantage of a library like Torchmeta is to standardize these benchmarks to avoid any confusion.\n\nThe other objective of Torchmeta is to make meta-learning accessible to a larger community. We hope that similar to how OpenAI Gym (Brockman et al., 2016) helped the progress in reinforcement learning, with an access to multiple environments under a unified interface, Torchmeta can have an equal impact on meta-learning research. The full compatibility of the library with both PyTorch and Torchvision, PyTorch's computer vision library, simplifies its integration to existing projects.\n\nEven though Torchmeta already features a number of datasets for both few-shot regression and classification, and covers most of the standard benchmarks in the meta-learning literature, one notable missing dataset in the current version of the library is Meta-Dataset (Triantafillou et al., 2019). Meta-Dataset is a unique and more complex few-shot classification problem, with a varying number of classes per task. And while it could fit the proposed abstraction, Meta-Dataset requires a long initial processing phase, which would make the automatic download and processing feature of the library impractical. Therefore its integration is left as future work. In the meantime, we believe that Torch-meta can provide a structure for the creation of better benchmarks in the future, and is a crucial step forward for reproducible research in meta-learning.\n\nFigure 1 :\n1Illustration of the functionality of the MetaLinear meta-module, the extension of the Linear module. Left: Instantiation of a MetaLinear meta-module. Middle: Default behaviour, equivalent to Linear. Right: Behaviour with extra parameters (a one-step gradient update, Finn et al., 2017). Gradients are represented as dashed arrows, in orange for \u2202/\u2202W and green for \u2202/\u2202b.counterpart. Otherwise, if extra parameters (such as the result of one step of gradient descent) are specified, then the MetaModule treats them as part of the computational graph, and backpropagation works as expected.\n\nFigure 1\n1shows how the extension of the Linear module called MetaLinear works, with and without additional parameters, and the impact on the gradients. The figure on the left shows the instantiation of the meta-module as a container for the parameters W & b, and the computational graph with placeholders for the weight and bias parameters. The figure in the middle shows the default behaviour of the MetaLinear meta-module, where the placeholders are substituted with W & b:\n\n\n1 & 2.2 can be iterated over to generate datasets from the metatraining set; these datasets are PyTorch Dataset objects, and as such can be included as part of any standard data pipeline (combined with DataLoader). Nonetheless, most meta-learning algorithms operate better on batches of tasks. Similar to how examples are batched together with DataLoader in PyTorch, Torchmeta exposes a MetaDalaoader that can produce batches of tasks when iterated over. In particular, such a meta data-loader is able to output a large tensor containing all the examples from the different tasks in the batch. For example:# Helper function, equivalent to Section 2.3 \ndataset = torchmeta.datasets.helpers.miniimagenet(\"data\", shots=1, ways=5, \nmeta_train=True, download=True) \ndataloader = torchmeta.utils.data.BatchMetaDataLoader(dataset, batch_size=16) \n\nfor batch in dataloader: \ntrain_inputs, train_labels = batch[\"train\"] # Size (16, 5, 3, 84, 84) & (16, 5) \n\n\nUniversit\u00e9 de Montr\u00e9al, 2 Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg, 3 Concordia University, 4 CIFAR Senior Fellow, 5 Canada CIFAR AI Chair. Correspondance: tristan.deleu@gmail.com. Preprint. Under review.\nAcknowledgementsWe would like to thank the students at Mila who tested the library, and provided valuable feedback during development. Tristan Deleu is supported by the Antidote scholarship from Druide Informatique.\nMeta-learning with differentiable closed-form solvers. L Bertinetto, J F Henriques, P H Torr, A Vedaldi, International Conference on Learning Representations. Bertinetto, L., Henriques, J. F., Torr, P. H., and Vedaldi, A. (2018). Meta-learning with differentiable closed-form solvers. International Conference on Learning Representations.\n\nOpenAI Gym. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. (2016). OpenAI Gym.\n\nLearning to Learn with Gradients. C Finn, UC BerkeleyPhD thesisFinn, C. (2018). Learning to Learn with Gradients. PhD thesis, UC Berkeley.\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. C Finn, P Abbeel, S Levine, International Conference on Machine Learning (ICML). Finn, C., Abbeel, P., and Levine, S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. International Conference on Machine Learning (ICML).\n\nProbabilistic model-agnostic meta-learning. C Finn, K Xu, S Levine, Advances in Neural Information Processing Systems. Finn, C., Xu, K., and Levine, S. (2018). Probabilistic model-agnostic meta-learning. In Advances in Neural Information Processing Systems.\n\nRecasting Gradient-Based Meta-Learning as Hierarchical Bayes. E Grant, C Finn, S Levine, T Darrell, T L Griffiths, International Conference on Learning RepresentationsGrant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T. L. (2018). Recasting Gradient-Based Meta-Learning as Hierarchical Bayes. International Conference on Learning Representations.\n\nUncertainty in Multitask Transfer Learning. A Lacoste, B Oreshkin, W Chung, T Boquet, N Rostamzadeh, D Krueger, Advances in Neural Information Processing Systems. Lacoste, A., Oreshkin, B., Chung, W., Boquet, T., Rostamzadeh, N., and Krueger, D. (2018). Uncer- tainty in Multitask Transfer Learning. Advances in Neural Information Processing Systems.\n\nHuman-level concept learning through probabilistic program induction. B M Lake, R Salakhutdinov, J B Tenenbaum, ScienceLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science.\n\nThe Omniglot Challenge: A 3-Year Progress Report. B M Lake, R Salakhutdinov, J B Tenenbaum, Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2019). The Omniglot Challenge: A 3-Year Progress Report.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. the IEEE86LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., et al. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86.\n\nK Lee, S Maji, A Ravichandran, S Soatto, Meta-Learning with Differentiable Convex Optimization. Lee, K., Maji, S., Ravichandran, A., and Soatto, S. (2019). Meta-Learning with Differentiable Convex Optimization.\n\nTADAM: Task dependent adaptive metric for improved few-shot learning. B Oreshkin, P R L\u00f3pez, A Lacoste, Advances in Neural Information Processing Systems. Oreshkin, B., L\u00f3pez, P. R., and Lacoste, A. (2018). TADAM: Task dependent adaptive metric for improved few-shot learning. Advances in Neural Information Processing Systems.\n\nAutomatic differentiation in PyTorch. A Paszke, S Gross, S Chintala, G Chanan, E Yang, Z Devito, Z Lin, A Desmaison, L Antiga, A Lerer, NIPS Autodiff Workshop. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. (2017). Automatic differentiation in PyTorch. In NIPS Autodiff Workshop.\n\nOptimization as a model for few-shot learning. S Ravi, H Larochelle, International Conference on Learning Representations. Ravi, S. and Larochelle, H. (2017). Optimization as a model for few-shot learning. International Conference on Learning Representations.\n\nMeta-learning for semi-supervised few-shot classification. M Ren, E Triantafillou, S Ravi, J Snell, K Swersky, J B Tenenbaum, H Larochelle, R S Zemel, International Conference on Learning RepresentationsRen, M., Triantafillou, E., Ravi, S., Snell, J., Swersky, K., Tenenbaum, J. B., Larochelle, H., and Zemel, R. S. (2018). Meta-learning for semi-supervised few-shot classification. International Conference on Learning Representations.\n\nO Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, ImageNet Large Scale Visual Recognition Challenge. 115Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. (2015). ImageNet Large Scale Visual Recognition Challenge. Interna- tional Journal of Computer Vision, 115.\n\nMeta-learning with latent embedding optimization. A A Rusu, D Rao, J Sygnowski, O Vinyals, R Pascanu, S Osindero, R Hadsell, Rusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S., and Hadsell, R. (2018). Meta-learning with latent embedding optimization.\n\nA Santoro, S Bartunov, M Botvinick, D Wierstra, T Lillicrap, Meta-Learning with Memory-Augmented Neural Networks. International Conference on Machine Learning. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. (2016). Meta-Learning with Memory-Augmented Neural Networks. International Conference on Machine Learning.\n\nE Triantafillou, T Zhu, V Dumoulin, P Lamblin, K Xu, R Goroshin, C Gelada, K Swersky, P.-A Manzagol, H Larochelle, Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. International Conference on Machine Learning (ICML). Triantafillou, E., Zhu, T., Dumoulin, V., Lamblin, P., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.-A., and Larochelle, H. (2019). Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. International Conference on Machine Learning (ICML).\n\nMatching Networks for One Shot Learning. O Vinyals, C Blundell, T P Lillicrap, K Kavukcuoglu, D Wierstra, Conference on Neural Information Processing Systems. Vinyals, O., Blundell, C., Lillicrap, T. P., Kavukcuoglu, K., and Wierstra, D. (2016). Matching Networks for One Shot Learning. Conference on Neural Information Processing Systems.\n\nFast Context Adaptation via Meta-Learning. L M Zintgraf, K Shiarlis, V Kurin, K Hofmann, S Whiteson, International Conference on Machine Learning (ICML). Zintgraf, L. M., Shiarlis, K., Kurin, V., Hofmann, K., and Whiteson, S. (2019). Fast Context Adap- tation via Meta-Learning. International Conference on Machine Learning (ICML).\n", "annotations": {"author": "[{\"end\":75,\"start\":61},{\"end\":89,\"start\":76},{\"end\":105,\"start\":90},{\"end\":124,\"start\":106},{\"end\":139,\"start\":125},{\"end\":155,\"start\":140},{\"end\":163,\"start\":156},{\"end\":75,\"start\":61},{\"end\":89,\"start\":76},{\"end\":105,\"start\":90},{\"end\":124,\"start\":106},{\"end\":139,\"start\":125},{\"end\":155,\"start\":140},{\"end\":163,\"start\":156}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":69},{\"end\":88,\"start\":83},{\"end\":104,\"start\":98},{\"end\":123,\"start\":118},{\"end\":138,\"start\":132},{\"end\":74,\"start\":69},{\"end\":88,\"start\":83},{\"end\":104,\"start\":98},{\"end\":123,\"start\":118},{\"end\":138,\"start\":132}]", "author_first_name": "[{\"end\":68,\"start\":61},{\"end\":82,\"start\":76},{\"end\":97,\"start\":90},{\"end\":112,\"start\":106},{\"end\":117,\"start\":113},{\"end\":131,\"start\":125},{\"end\":154,\"start\":140},{\"end\":162,\"start\":156},{\"end\":68,\"start\":61},{\"end\":82,\"start\":76},{\"end\":97,\"start\":90},{\"end\":112,\"start\":106},{\"end\":117,\"start\":113},{\"end\":131,\"start\":125},{\"end\":154,\"start\":140},{\"end\":162,\"start\":156}]", "author_affiliation": null, "title": "[{\"end\":47,\"start\":1},{\"end\":210,\"start\":164},{\"end\":47,\"start\":1},{\"end\":210,\"start\":164}]", "venue": null, "abstract": "[{\"end\":1202,\"start\":223},{\"end\":1202,\"start\":223}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1739,\"start\":1719},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1778,\"start\":1752},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2507,\"start\":2486},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2868,\"start\":2845},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3405,\"start\":3386},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3416,\"start\":3405},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3434,\"start\":3416},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3453,\"start\":3434},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5531,\"start\":5512},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6047,\"start\":6028},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6117,\"start\":6096},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7355,\"start\":7337},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7375,\"start\":7355},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7413,\"start\":7391},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7439,\"start\":7413},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7475,\"start\":7457},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7511,\"start\":7486},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7864,\"start\":7842},{\"end\":8748,\"start\":8739},{\"end\":8770,\"start\":8748},{\"end\":8792,\"start\":8770},{\"end\":8805,\"start\":8792},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9756,\"start\":9738},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9981,\"start\":9960},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10281,\"start\":10262},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10292,\"start\":10281},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10311,\"start\":10292},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10328,\"start\":10311},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10376,\"start\":10357},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10398,\"start\":10376},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11044,\"start\":11025},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11817,\"start\":11795},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11863,\"start\":11837},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12186,\"start\":12163},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12816,\"start\":12788},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1739,\"start\":1719},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1778,\"start\":1752},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2507,\"start\":2486},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2868,\"start\":2845},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3405,\"start\":3386},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3416,\"start\":3405},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3434,\"start\":3416},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3453,\"start\":3434},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5531,\"start\":5512},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6047,\"start\":6028},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6117,\"start\":6096},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7355,\"start\":7337},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7375,\"start\":7355},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7413,\"start\":7391},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7439,\"start\":7413},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7475,\"start\":7457},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7511,\"start\":7486},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7864,\"start\":7842},{\"end\":8748,\"start\":8739},{\"end\":8770,\"start\":8748},{\"end\":8792,\"start\":8770},{\"end\":8805,\"start\":8792},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9756,\"start\":9738},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9981,\"start\":9960},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10281,\"start\":10262},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10292,\"start\":10281},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10311,\"start\":10292},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10328,\"start\":10311},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10376,\"start\":10357},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10398,\"start\":10376},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11044,\"start\":11025},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11817,\"start\":11795},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11863,\"start\":11837},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12186,\"start\":12163},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12816,\"start\":12788}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":13976,\"start\":13376},{\"attributes\":{\"id\":\"fig_1\"},\"end\":14454,\"start\":13977},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":15406,\"start\":14455},{\"attributes\":{\"id\":\"fig_0\"},\"end\":13976,\"start\":13376},{\"attributes\":{\"id\":\"fig_1\"},\"end\":14454,\"start\":13977},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":15406,\"start\":14455}]", "paragraph": "[{\"end\":2198,\"start\":1218},{\"end\":3167,\"start\":2200},{\"end\":3824,\"start\":3169},{\"end\":4476,\"start\":3863},{\"end\":4681,\"start\":4478},{\"end\":5051,\"start\":4683},{\"end\":5968,\"start\":5075},{\"end\":6320,\"start\":5970},{\"end\":7259,\"start\":6348},{\"end\":7720,\"start\":7261},{\"end\":7865,\"start\":7722},{\"end\":9145,\"start\":7900},{\"end\":10149,\"start\":9167},{\"end\":11198,\"start\":10151},{\"end\":12030,\"start\":11423},{\"end\":12519,\"start\":12032},{\"end\":13375,\"start\":12521},{\"end\":2198,\"start\":1218},{\"end\":3167,\"start\":2200},{\"end\":3824,\"start\":3169},{\"end\":4476,\"start\":3863},{\"end\":4681,\"start\":4478},{\"end\":5051,\"start\":4683},{\"end\":5968,\"start\":5075},{\"end\":6320,\"start\":5970},{\"end\":7259,\"start\":6348},{\"end\":7720,\"start\":7261},{\"end\":7865,\"start\":7722},{\"end\":9145,\"start\":7900},{\"end\":10149,\"start\":9167},{\"end\":11198,\"start\":10151},{\"end\":12030,\"start\":11423},{\"end\":12519,\"start\":12032},{\"end\":13375,\"start\":12521}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11409,\"start\":11199},{\"attributes\":{\"id\":\"formula_0\"},\"end\":11409,\"start\":11199}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1216,\"start\":1204},{\"attributes\":{\"n\":\"2\"},\"end\":3861,\"start\":3827},{\"attributes\":{\"n\":\"2.1\"},\"end\":5073,\"start\":5054},{\"attributes\":{\"n\":\"2.2\"},\"end\":6346,\"start\":6323},{\"attributes\":{\"n\":\"2.3\"},\"end\":7898,\"start\":7868},{\"attributes\":{\"n\":\"2.4\"},\"end\":9165,\"start\":9148},{\"attributes\":{\"n\":\"4\"},\"end\":11421,\"start\":11411},{\"end\":13387,\"start\":13377},{\"end\":13986,\"start\":13978},{\"attributes\":{\"n\":\"1\"},\"end\":1216,\"start\":1204},{\"attributes\":{\"n\":\"2\"},\"end\":3861,\"start\":3827},{\"attributes\":{\"n\":\"2.1\"},\"end\":5073,\"start\":5054},{\"attributes\":{\"n\":\"2.2\"},\"end\":6346,\"start\":6323},{\"attributes\":{\"n\":\"2.3\"},\"end\":7898,\"start\":7868},{\"attributes\":{\"n\":\"2.4\"},\"end\":9165,\"start\":9148},{\"attributes\":{\"n\":\"4\"},\"end\":11421,\"start\":11411},{\"end\":13387,\"start\":13377},{\"end\":13986,\"start\":13978}]", "table": "[{\"end\":15406,\"start\":15063},{\"end\":15406,\"start\":15063}]", "figure_caption": "[{\"end\":13976,\"start\":13389},{\"end\":14454,\"start\":13988},{\"end\":15063,\"start\":14457},{\"end\":13976,\"start\":13389},{\"end\":14454,\"start\":13988},{\"end\":15063,\"start\":14457}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":15893,\"start\":15892},{\"end\":15907,\"start\":15906},{\"end\":15909,\"start\":15908},{\"end\":15922,\"start\":15921},{\"end\":15924,\"start\":15923},{\"end\":15932,\"start\":15931},{\"end\":16190,\"start\":16189},{\"end\":16202,\"start\":16201},{\"end\":16212,\"start\":16211},{\"end\":16226,\"start\":16225},{\"end\":16239,\"start\":16238},{\"end\":16251,\"start\":16250},{\"end\":16259,\"start\":16258},{\"end\":16422,\"start\":16421},{\"end\":16595,\"start\":16594},{\"end\":16603,\"start\":16602},{\"end\":16613,\"start\":16612},{\"end\":16886,\"start\":16885},{\"end\":16894,\"start\":16893},{\"end\":16900,\"start\":16899},{\"end\":17163,\"start\":17162},{\"end\":17172,\"start\":17171},{\"end\":17180,\"start\":17179},{\"end\":17190,\"start\":17189},{\"end\":17201,\"start\":17200},{\"end\":17203,\"start\":17202},{\"end\":17504,\"start\":17503},{\"end\":17515,\"start\":17514},{\"end\":17527,\"start\":17526},{\"end\":17536,\"start\":17535},{\"end\":17546,\"start\":17545},{\"end\":17561,\"start\":17560},{\"end\":17882,\"start\":17881},{\"end\":17884,\"start\":17883},{\"end\":17892,\"start\":17891},{\"end\":17909,\"start\":17908},{\"end\":17911,\"start\":17910},{\"end\":18122,\"start\":18121},{\"end\":18124,\"start\":18123},{\"end\":18132,\"start\":18131},{\"end\":18149,\"start\":18148},{\"end\":18151,\"start\":18150},{\"end\":18333,\"start\":18332},{\"end\":18342,\"start\":18341},{\"end\":18352,\"start\":18351},{\"end\":18362,\"start\":18361},{\"end\":18558,\"start\":18557},{\"end\":18565,\"start\":18564},{\"end\":18573,\"start\":18572},{\"end\":18589,\"start\":18588},{\"end\":18840,\"start\":18839},{\"end\":18852,\"start\":18851},{\"end\":18854,\"start\":18853},{\"end\":18863,\"start\":18862},{\"end\":19137,\"start\":19136},{\"end\":19147,\"start\":19146},{\"end\":19156,\"start\":19155},{\"end\":19168,\"start\":19167},{\"end\":19178,\"start\":19177},{\"end\":19186,\"start\":19185},{\"end\":19196,\"start\":19195},{\"end\":19203,\"start\":19202},{\"end\":19216,\"start\":19215},{\"end\":19226,\"start\":19225},{\"end\":19501,\"start\":19500},{\"end\":19509,\"start\":19508},{\"end\":19774,\"start\":19773},{\"end\":19781,\"start\":19780},{\"end\":19798,\"start\":19797},{\"end\":19806,\"start\":19805},{\"end\":19815,\"start\":19814},{\"end\":19826,\"start\":19825},{\"end\":19828,\"start\":19827},{\"end\":19841,\"start\":19840},{\"end\":19855,\"start\":19854},{\"end\":19857,\"start\":19856},{\"end\":20153,\"start\":20152},{\"end\":20168,\"start\":20167},{\"end\":20176,\"start\":20175},{\"end\":20182,\"start\":20181},{\"end\":20192,\"start\":20191},{\"end\":20204,\"start\":20203},{\"end\":20210,\"start\":20209},{\"end\":20219,\"start\":20218},{\"end\":20231,\"start\":20230},{\"end\":20241,\"start\":20240},{\"end\":20595,\"start\":20594},{\"end\":20597,\"start\":20596},{\"end\":20605,\"start\":20604},{\"end\":20612,\"start\":20611},{\"end\":20625,\"start\":20624},{\"end\":20636,\"start\":20635},{\"end\":20647,\"start\":20646},{\"end\":20659,\"start\":20658},{\"end\":20822,\"start\":20821},{\"end\":20833,\"start\":20832},{\"end\":20845,\"start\":20844},{\"end\":20858,\"start\":20857},{\"end\":20870,\"start\":20869},{\"end\":21164,\"start\":21163},{\"end\":21181,\"start\":21180},{\"end\":21188,\"start\":21187},{\"end\":21200,\"start\":21199},{\"end\":21211,\"start\":21210},{\"end\":21217,\"start\":21216},{\"end\":21229,\"start\":21228},{\"end\":21239,\"start\":21238},{\"end\":21253,\"start\":21249},{\"end\":21265,\"start\":21264},{\"end\":21727,\"start\":21726},{\"end\":21738,\"start\":21737},{\"end\":21750,\"start\":21749},{\"end\":21752,\"start\":21751},{\"end\":21765,\"start\":21764},{\"end\":21780,\"start\":21779},{\"end\":22070,\"start\":22069},{\"end\":22072,\"start\":22071},{\"end\":22084,\"start\":22083},{\"end\":22096,\"start\":22095},{\"end\":22105,\"start\":22104},{\"end\":22116,\"start\":22115},{\"end\":15893,\"start\":15892},{\"end\":15907,\"start\":15906},{\"end\":15909,\"start\":15908},{\"end\":15922,\"start\":15921},{\"end\":15924,\"start\":15923},{\"end\":15932,\"start\":15931},{\"end\":16190,\"start\":16189},{\"end\":16202,\"start\":16201},{\"end\":16212,\"start\":16211},{\"end\":16226,\"start\":16225},{\"end\":16239,\"start\":16238},{\"end\":16251,\"start\":16250},{\"end\":16259,\"start\":16258},{\"end\":16422,\"start\":16421},{\"end\":16595,\"start\":16594},{\"end\":16603,\"start\":16602},{\"end\":16613,\"start\":16612},{\"end\":16886,\"start\":16885},{\"end\":16894,\"start\":16893},{\"end\":16900,\"start\":16899},{\"end\":17163,\"start\":17162},{\"end\":17172,\"start\":17171},{\"end\":17180,\"start\":17179},{\"end\":17190,\"start\":17189},{\"end\":17201,\"start\":17200},{\"end\":17203,\"start\":17202},{\"end\":17504,\"start\":17503},{\"end\":17515,\"start\":17514},{\"end\":17527,\"start\":17526},{\"end\":17536,\"start\":17535},{\"end\":17546,\"start\":17545},{\"end\":17561,\"start\":17560},{\"end\":17882,\"start\":17881},{\"end\":17884,\"start\":17883},{\"end\":17892,\"start\":17891},{\"end\":17909,\"start\":17908},{\"end\":17911,\"start\":17910},{\"end\":18122,\"start\":18121},{\"end\":18124,\"start\":18123},{\"end\":18132,\"start\":18131},{\"end\":18149,\"start\":18148},{\"end\":18151,\"start\":18150},{\"end\":18333,\"start\":18332},{\"end\":18342,\"start\":18341},{\"end\":18352,\"start\":18351},{\"end\":18362,\"start\":18361},{\"end\":18558,\"start\":18557},{\"end\":18565,\"start\":18564},{\"end\":18573,\"start\":18572},{\"end\":18589,\"start\":18588},{\"end\":18840,\"start\":18839},{\"end\":18852,\"start\":18851},{\"end\":18854,\"start\":18853},{\"end\":18863,\"start\":18862},{\"end\":19137,\"start\":19136},{\"end\":19147,\"start\":19146},{\"end\":19156,\"start\":19155},{\"end\":19168,\"start\":19167},{\"end\":19178,\"start\":19177},{\"end\":19186,\"start\":19185},{\"end\":19196,\"start\":19195},{\"end\":19203,\"start\":19202},{\"end\":19216,\"start\":19215},{\"end\":19226,\"start\":19225},{\"end\":19501,\"start\":19500},{\"end\":19509,\"start\":19508},{\"end\":19774,\"start\":19773},{\"end\":19781,\"start\":19780},{\"end\":19798,\"start\":19797},{\"end\":19806,\"start\":19805},{\"end\":19815,\"start\":19814},{\"end\":19826,\"start\":19825},{\"end\":19828,\"start\":19827},{\"end\":19841,\"start\":19840},{\"end\":19855,\"start\":19854},{\"end\":19857,\"start\":19856},{\"end\":20153,\"start\":20152},{\"end\":20168,\"start\":20167},{\"end\":20176,\"start\":20175},{\"end\":20182,\"start\":20181},{\"end\":20192,\"start\":20191},{\"end\":20204,\"start\":20203},{\"end\":20210,\"start\":20209},{\"end\":20219,\"start\":20218},{\"end\":20231,\"start\":20230},{\"end\":20241,\"start\":20240},{\"end\":20595,\"start\":20594},{\"end\":20597,\"start\":20596},{\"end\":20605,\"start\":20604},{\"end\":20612,\"start\":20611},{\"end\":20625,\"start\":20624},{\"end\":20636,\"start\":20635},{\"end\":20647,\"start\":20646},{\"end\":20659,\"start\":20658},{\"end\":20822,\"start\":20821},{\"end\":20833,\"start\":20832},{\"end\":20845,\"start\":20844},{\"end\":20858,\"start\":20857},{\"end\":20870,\"start\":20869},{\"end\":21164,\"start\":21163},{\"end\":21181,\"start\":21180},{\"end\":21188,\"start\":21187},{\"end\":21200,\"start\":21199},{\"end\":21211,\"start\":21210},{\"end\":21217,\"start\":21216},{\"end\":21229,\"start\":21228},{\"end\":21239,\"start\":21238},{\"end\":21253,\"start\":21249},{\"end\":21265,\"start\":21264},{\"end\":21727,\"start\":21726},{\"end\":21738,\"start\":21737},{\"end\":21750,\"start\":21749},{\"end\":21752,\"start\":21751},{\"end\":21765,\"start\":21764},{\"end\":21780,\"start\":21779},{\"end\":22070,\"start\":22069},{\"end\":22072,\"start\":22071},{\"end\":22084,\"start\":22083},{\"end\":22096,\"start\":22095},{\"end\":22105,\"start\":22104},{\"end\":22116,\"start\":22115}]", "bib_author_last_name": "[{\"end\":15904,\"start\":15894},{\"end\":15919,\"start\":15910},{\"end\":15929,\"start\":15925},{\"end\":15940,\"start\":15933},{\"end\":16199,\"start\":16191},{\"end\":16209,\"start\":16203},{\"end\":16223,\"start\":16213},{\"end\":16236,\"start\":16227},{\"end\":16248,\"start\":16240},{\"end\":16256,\"start\":16252},{\"end\":16267,\"start\":16260},{\"end\":16427,\"start\":16423},{\"end\":16600,\"start\":16596},{\"end\":16610,\"start\":16604},{\"end\":16620,\"start\":16614},{\"end\":16891,\"start\":16887},{\"end\":16897,\"start\":16895},{\"end\":16907,\"start\":16901},{\"end\":17169,\"start\":17164},{\"end\":17177,\"start\":17173},{\"end\":17187,\"start\":17181},{\"end\":17198,\"start\":17191},{\"end\":17213,\"start\":17204},{\"end\":17512,\"start\":17505},{\"end\":17524,\"start\":17516},{\"end\":17533,\"start\":17528},{\"end\":17543,\"start\":17537},{\"end\":17558,\"start\":17547},{\"end\":17569,\"start\":17562},{\"end\":17889,\"start\":17885},{\"end\":17906,\"start\":17893},{\"end\":17921,\"start\":17912},{\"end\":18129,\"start\":18125},{\"end\":18146,\"start\":18133},{\"end\":18161,\"start\":18152},{\"end\":18339,\"start\":18334},{\"end\":18349,\"start\":18343},{\"end\":18359,\"start\":18353},{\"end\":18370,\"start\":18363},{\"end\":18562,\"start\":18559},{\"end\":18570,\"start\":18566},{\"end\":18586,\"start\":18574},{\"end\":18596,\"start\":18590},{\"end\":18849,\"start\":18841},{\"end\":18860,\"start\":18855},{\"end\":18871,\"start\":18864},{\"end\":19144,\"start\":19138},{\"end\":19153,\"start\":19148},{\"end\":19165,\"start\":19157},{\"end\":19175,\"start\":19169},{\"end\":19183,\"start\":19179},{\"end\":19193,\"start\":19187},{\"end\":19200,\"start\":19197},{\"end\":19213,\"start\":19204},{\"end\":19223,\"start\":19217},{\"end\":19232,\"start\":19227},{\"end\":19506,\"start\":19502},{\"end\":19520,\"start\":19510},{\"end\":19778,\"start\":19775},{\"end\":19795,\"start\":19782},{\"end\":19803,\"start\":19799},{\"end\":19812,\"start\":19807},{\"end\":19823,\"start\":19816},{\"end\":19838,\"start\":19829},{\"end\":19852,\"start\":19842},{\"end\":19863,\"start\":19858},{\"end\":20165,\"start\":20154},{\"end\":20173,\"start\":20169},{\"end\":20179,\"start\":20177},{\"end\":20189,\"start\":20183},{\"end\":20201,\"start\":20193},{\"end\":20207,\"start\":20205},{\"end\":20216,\"start\":20211},{\"end\":20228,\"start\":20220},{\"end\":20238,\"start\":20232},{\"end\":20251,\"start\":20242},{\"end\":20602,\"start\":20598},{\"end\":20609,\"start\":20606},{\"end\":20622,\"start\":20613},{\"end\":20633,\"start\":20626},{\"end\":20644,\"start\":20637},{\"end\":20656,\"start\":20648},{\"end\":20667,\"start\":20660},{\"end\":20830,\"start\":20823},{\"end\":20842,\"start\":20834},{\"end\":20855,\"start\":20846},{\"end\":20867,\"start\":20859},{\"end\":20880,\"start\":20871},{\"end\":21178,\"start\":21165},{\"end\":21185,\"start\":21182},{\"end\":21197,\"start\":21189},{\"end\":21208,\"start\":21201},{\"end\":21214,\"start\":21212},{\"end\":21226,\"start\":21218},{\"end\":21236,\"start\":21230},{\"end\":21247,\"start\":21240},{\"end\":21262,\"start\":21254},{\"end\":21276,\"start\":21266},{\"end\":21735,\"start\":21728},{\"end\":21747,\"start\":21739},{\"end\":21762,\"start\":21753},{\"end\":21777,\"start\":21766},{\"end\":21789,\"start\":21781},{\"end\":22081,\"start\":22073},{\"end\":22093,\"start\":22085},{\"end\":22102,\"start\":22097},{\"end\":22113,\"start\":22106},{\"end\":22125,\"start\":22117},{\"end\":15904,\"start\":15894},{\"end\":15919,\"start\":15910},{\"end\":15929,\"start\":15925},{\"end\":15940,\"start\":15933},{\"end\":16199,\"start\":16191},{\"end\":16209,\"start\":16203},{\"end\":16223,\"start\":16213},{\"end\":16236,\"start\":16227},{\"end\":16248,\"start\":16240},{\"end\":16256,\"start\":16252},{\"end\":16267,\"start\":16260},{\"end\":16427,\"start\":16423},{\"end\":16600,\"start\":16596},{\"end\":16610,\"start\":16604},{\"end\":16620,\"start\":16614},{\"end\":16891,\"start\":16887},{\"end\":16897,\"start\":16895},{\"end\":16907,\"start\":16901},{\"end\":17169,\"start\":17164},{\"end\":17177,\"start\":17173},{\"end\":17187,\"start\":17181},{\"end\":17198,\"start\":17191},{\"end\":17213,\"start\":17204},{\"end\":17512,\"start\":17505},{\"end\":17524,\"start\":17516},{\"end\":17533,\"start\":17528},{\"end\":17543,\"start\":17537},{\"end\":17558,\"start\":17547},{\"end\":17569,\"start\":17562},{\"end\":17889,\"start\":17885},{\"end\":17906,\"start\":17893},{\"end\":17921,\"start\":17912},{\"end\":18129,\"start\":18125},{\"end\":18146,\"start\":18133},{\"end\":18161,\"start\":18152},{\"end\":18339,\"start\":18334},{\"end\":18349,\"start\":18343},{\"end\":18359,\"start\":18353},{\"end\":18370,\"start\":18363},{\"end\":18562,\"start\":18559},{\"end\":18570,\"start\":18566},{\"end\":18586,\"start\":18574},{\"end\":18596,\"start\":18590},{\"end\":18849,\"start\":18841},{\"end\":18860,\"start\":18855},{\"end\":18871,\"start\":18864},{\"end\":19144,\"start\":19138},{\"end\":19153,\"start\":19148},{\"end\":19165,\"start\":19157},{\"end\":19175,\"start\":19169},{\"end\":19183,\"start\":19179},{\"end\":19193,\"start\":19187},{\"end\":19200,\"start\":19197},{\"end\":19213,\"start\":19204},{\"end\":19223,\"start\":19217},{\"end\":19232,\"start\":19227},{\"end\":19506,\"start\":19502},{\"end\":19520,\"start\":19510},{\"end\":19778,\"start\":19775},{\"end\":19795,\"start\":19782},{\"end\":19803,\"start\":19799},{\"end\":19812,\"start\":19807},{\"end\":19823,\"start\":19816},{\"end\":19838,\"start\":19829},{\"end\":19852,\"start\":19842},{\"end\":19863,\"start\":19858},{\"end\":20165,\"start\":20154},{\"end\":20173,\"start\":20169},{\"end\":20179,\"start\":20177},{\"end\":20189,\"start\":20183},{\"end\":20201,\"start\":20193},{\"end\":20207,\"start\":20205},{\"end\":20216,\"start\":20211},{\"end\":20228,\"start\":20220},{\"end\":20238,\"start\":20232},{\"end\":20251,\"start\":20242},{\"end\":20602,\"start\":20598},{\"end\":20609,\"start\":20606},{\"end\":20622,\"start\":20613},{\"end\":20633,\"start\":20626},{\"end\":20644,\"start\":20637},{\"end\":20656,\"start\":20648},{\"end\":20667,\"start\":20660},{\"end\":20830,\"start\":20823},{\"end\":20842,\"start\":20834},{\"end\":20855,\"start\":20846},{\"end\":20867,\"start\":20859},{\"end\":20880,\"start\":20871},{\"end\":21178,\"start\":21165},{\"end\":21185,\"start\":21182},{\"end\":21197,\"start\":21189},{\"end\":21208,\"start\":21201},{\"end\":21214,\"start\":21212},{\"end\":21226,\"start\":21218},{\"end\":21236,\"start\":21230},{\"end\":21247,\"start\":21240},{\"end\":21262,\"start\":21254},{\"end\":21276,\"start\":21266},{\"end\":21735,\"start\":21728},{\"end\":21747,\"start\":21739},{\"end\":21762,\"start\":21753},{\"end\":21777,\"start\":21766},{\"end\":21789,\"start\":21781},{\"end\":22081,\"start\":22073},{\"end\":22093,\"start\":22085},{\"end\":22102,\"start\":22097},{\"end\":22113,\"start\":22106},{\"end\":22125,\"start\":22117}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":29153681},\"end\":16175,\"start\":15837},{\"attributes\":{\"id\":\"b1\"},\"end\":16385,\"start\":16177},{\"attributes\":{\"id\":\"b2\"},\"end\":16525,\"start\":16387},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6719686},\"end\":16839,\"start\":16527},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":46977722},\"end\":17098,\"start\":16841},{\"attributes\":{\"id\":\"b5\"},\"end\":17457,\"start\":17100},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":49321468},\"end\":17809,\"start\":17459},{\"attributes\":{\"id\":\"b7\"},\"end\":18069,\"start\":17811},{\"attributes\":{\"id\":\"b8\"},\"end\":18273,\"start\":18071},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14542261},\"end\":18555,\"start\":18275},{\"attributes\":{\"id\":\"b10\"},\"end\":18767,\"start\":18557},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":44061218},\"end\":19096,\"start\":18769},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":40027675},\"end\":19451,\"start\":19098},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":67413369},\"end\":19712,\"start\":19453},{\"attributes\":{\"id\":\"b14\"},\"end\":20150,\"start\":19714},{\"attributes\":{\"id\":\"b15\"},\"end\":20542,\"start\":20152},{\"attributes\":{\"id\":\"b16\"},\"end\":20819,\"start\":20544},{\"attributes\":{\"id\":\"b17\"},\"end\":21161,\"start\":20821},{\"attributes\":{\"id\":\"b18\"},\"end\":21683,\"start\":21163},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8909022},\"end\":22024,\"start\":21685},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":59413758},\"end\":22357,\"start\":22026},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":29153681},\"end\":16175,\"start\":15837},{\"attributes\":{\"id\":\"b1\"},\"end\":16385,\"start\":16177},{\"attributes\":{\"id\":\"b2\"},\"end\":16525,\"start\":16387},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6719686},\"end\":16839,\"start\":16527},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":46977722},\"end\":17098,\"start\":16841},{\"attributes\":{\"id\":\"b5\"},\"end\":17457,\"start\":17100},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":49321468},\"end\":17809,\"start\":17459},{\"attributes\":{\"id\":\"b7\"},\"end\":18069,\"start\":17811},{\"attributes\":{\"id\":\"b8\"},\"end\":18273,\"start\":18071},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14542261},\"end\":18555,\"start\":18275},{\"attributes\":{\"id\":\"b10\"},\"end\":18767,\"start\":18557},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":44061218},\"end\":19096,\"start\":18769},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":40027675},\"end\":19451,\"start\":19098},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":67413369},\"end\":19712,\"start\":19453},{\"attributes\":{\"id\":\"b14\"},\"end\":20150,\"start\":19714},{\"attributes\":{\"id\":\"b15\"},\"end\":20542,\"start\":20152},{\"attributes\":{\"id\":\"b16\"},\"end\":20819,\"start\":20544},{\"attributes\":{\"id\":\"b17\"},\"end\":21161,\"start\":20821},{\"attributes\":{\"id\":\"b18\"},\"end\":21683,\"start\":21163},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8909022},\"end\":22024,\"start\":21685},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":59413758},\"end\":22357,\"start\":22026}]", "bib_title": "[{\"end\":15890,\"start\":15837},{\"end\":16592,\"start\":16527},{\"end\":16883,\"start\":16841},{\"end\":17501,\"start\":17459},{\"end\":18330,\"start\":18275},{\"end\":18837,\"start\":18769},{\"end\":19134,\"start\":19098},{\"end\":19498,\"start\":19453},{\"end\":21724,\"start\":21685},{\"end\":22067,\"start\":22026},{\"end\":15890,\"start\":15837},{\"end\":16592,\"start\":16527},{\"end\":16883,\"start\":16841},{\"end\":17501,\"start\":17459},{\"end\":18330,\"start\":18275},{\"end\":18837,\"start\":18769},{\"end\":19134,\"start\":19098},{\"end\":19498,\"start\":19453},{\"end\":21724,\"start\":21685},{\"end\":22067,\"start\":22026}]", "bib_author": "[{\"end\":15906,\"start\":15892},{\"end\":15921,\"start\":15906},{\"end\":15931,\"start\":15921},{\"end\":15942,\"start\":15931},{\"end\":16201,\"start\":16189},{\"end\":16211,\"start\":16201},{\"end\":16225,\"start\":16211},{\"end\":16238,\"start\":16225},{\"end\":16250,\"start\":16238},{\"end\":16258,\"start\":16250},{\"end\":16269,\"start\":16258},{\"end\":16429,\"start\":16421},{\"end\":16602,\"start\":16594},{\"end\":16612,\"start\":16602},{\"end\":16622,\"start\":16612},{\"end\":16893,\"start\":16885},{\"end\":16899,\"start\":16893},{\"end\":16909,\"start\":16899},{\"end\":17171,\"start\":17162},{\"end\":17179,\"start\":17171},{\"end\":17189,\"start\":17179},{\"end\":17200,\"start\":17189},{\"end\":17215,\"start\":17200},{\"end\":17514,\"start\":17503},{\"end\":17526,\"start\":17514},{\"end\":17535,\"start\":17526},{\"end\":17545,\"start\":17535},{\"end\":17560,\"start\":17545},{\"end\":17571,\"start\":17560},{\"end\":17891,\"start\":17881},{\"end\":17908,\"start\":17891},{\"end\":17923,\"start\":17908},{\"end\":18131,\"start\":18121},{\"end\":18148,\"start\":18131},{\"end\":18163,\"start\":18148},{\"end\":18341,\"start\":18332},{\"end\":18351,\"start\":18341},{\"end\":18361,\"start\":18351},{\"end\":18372,\"start\":18361},{\"end\":18564,\"start\":18557},{\"end\":18572,\"start\":18564},{\"end\":18588,\"start\":18572},{\"end\":18598,\"start\":18588},{\"end\":18851,\"start\":18839},{\"end\":18862,\"start\":18851},{\"end\":18873,\"start\":18862},{\"end\":19146,\"start\":19136},{\"end\":19155,\"start\":19146},{\"end\":19167,\"start\":19155},{\"end\":19177,\"start\":19167},{\"end\":19185,\"start\":19177},{\"end\":19195,\"start\":19185},{\"end\":19202,\"start\":19195},{\"end\":19215,\"start\":19202},{\"end\":19225,\"start\":19215},{\"end\":19234,\"start\":19225},{\"end\":19508,\"start\":19500},{\"end\":19522,\"start\":19508},{\"end\":19780,\"start\":19773},{\"end\":19797,\"start\":19780},{\"end\":19805,\"start\":19797},{\"end\":19814,\"start\":19805},{\"end\":19825,\"start\":19814},{\"end\":19840,\"start\":19825},{\"end\":19854,\"start\":19840},{\"end\":19865,\"start\":19854},{\"end\":20167,\"start\":20152},{\"end\":20175,\"start\":20167},{\"end\":20181,\"start\":20175},{\"end\":20191,\"start\":20181},{\"end\":20203,\"start\":20191},{\"end\":20209,\"start\":20203},{\"end\":20218,\"start\":20209},{\"end\":20230,\"start\":20218},{\"end\":20240,\"start\":20230},{\"end\":20253,\"start\":20240},{\"end\":20604,\"start\":20594},{\"end\":20611,\"start\":20604},{\"end\":20624,\"start\":20611},{\"end\":20635,\"start\":20624},{\"end\":20646,\"start\":20635},{\"end\":20658,\"start\":20646},{\"end\":20669,\"start\":20658},{\"end\":20832,\"start\":20821},{\"end\":20844,\"start\":20832},{\"end\":20857,\"start\":20844},{\"end\":20869,\"start\":20857},{\"end\":20882,\"start\":20869},{\"end\":21180,\"start\":21163},{\"end\":21187,\"start\":21180},{\"end\":21199,\"start\":21187},{\"end\":21210,\"start\":21199},{\"end\":21216,\"start\":21210},{\"end\":21228,\"start\":21216},{\"end\":21238,\"start\":21228},{\"end\":21249,\"start\":21238},{\"end\":21264,\"start\":21249},{\"end\":21278,\"start\":21264},{\"end\":21737,\"start\":21726},{\"end\":21749,\"start\":21737},{\"end\":21764,\"start\":21749},{\"end\":21779,\"start\":21764},{\"end\":21791,\"start\":21779},{\"end\":22083,\"start\":22069},{\"end\":22095,\"start\":22083},{\"end\":22104,\"start\":22095},{\"end\":22115,\"start\":22104},{\"end\":22127,\"start\":22115},{\"end\":15906,\"start\":15892},{\"end\":15921,\"start\":15906},{\"end\":15931,\"start\":15921},{\"end\":15942,\"start\":15931},{\"end\":16201,\"start\":16189},{\"end\":16211,\"start\":16201},{\"end\":16225,\"start\":16211},{\"end\":16238,\"start\":16225},{\"end\":16250,\"start\":16238},{\"end\":16258,\"start\":16250},{\"end\":16269,\"start\":16258},{\"end\":16429,\"start\":16421},{\"end\":16602,\"start\":16594},{\"end\":16612,\"start\":16602},{\"end\":16622,\"start\":16612},{\"end\":16893,\"start\":16885},{\"end\":16899,\"start\":16893},{\"end\":16909,\"start\":16899},{\"end\":17171,\"start\":17162},{\"end\":17179,\"start\":17171},{\"end\":17189,\"start\":17179},{\"end\":17200,\"start\":17189},{\"end\":17215,\"start\":17200},{\"end\":17514,\"start\":17503},{\"end\":17526,\"start\":17514},{\"end\":17535,\"start\":17526},{\"end\":17545,\"start\":17535},{\"end\":17560,\"start\":17545},{\"end\":17571,\"start\":17560},{\"end\":17891,\"start\":17881},{\"end\":17908,\"start\":17891},{\"end\":17923,\"start\":17908},{\"end\":18131,\"start\":18121},{\"end\":18148,\"start\":18131},{\"end\":18163,\"start\":18148},{\"end\":18341,\"start\":18332},{\"end\":18351,\"start\":18341},{\"end\":18361,\"start\":18351},{\"end\":18372,\"start\":18361},{\"end\":18564,\"start\":18557},{\"end\":18572,\"start\":18564},{\"end\":18588,\"start\":18572},{\"end\":18598,\"start\":18588},{\"end\":18851,\"start\":18839},{\"end\":18862,\"start\":18851},{\"end\":18873,\"start\":18862},{\"end\":19146,\"start\":19136},{\"end\":19155,\"start\":19146},{\"end\":19167,\"start\":19155},{\"end\":19177,\"start\":19167},{\"end\":19185,\"start\":19177},{\"end\":19195,\"start\":19185},{\"end\":19202,\"start\":19195},{\"end\":19215,\"start\":19202},{\"end\":19225,\"start\":19215},{\"end\":19234,\"start\":19225},{\"end\":19508,\"start\":19500},{\"end\":19522,\"start\":19508},{\"end\":19780,\"start\":19773},{\"end\":19797,\"start\":19780},{\"end\":19805,\"start\":19797},{\"end\":19814,\"start\":19805},{\"end\":19825,\"start\":19814},{\"end\":19840,\"start\":19825},{\"end\":19854,\"start\":19840},{\"end\":19865,\"start\":19854},{\"end\":20167,\"start\":20152},{\"end\":20175,\"start\":20167},{\"end\":20181,\"start\":20175},{\"end\":20191,\"start\":20181},{\"end\":20203,\"start\":20191},{\"end\":20209,\"start\":20203},{\"end\":20218,\"start\":20209},{\"end\":20230,\"start\":20218},{\"end\":20240,\"start\":20230},{\"end\":20253,\"start\":20240},{\"end\":20604,\"start\":20594},{\"end\":20611,\"start\":20604},{\"end\":20624,\"start\":20611},{\"end\":20635,\"start\":20624},{\"end\":20646,\"start\":20635},{\"end\":20658,\"start\":20646},{\"end\":20669,\"start\":20658},{\"end\":20832,\"start\":20821},{\"end\":20844,\"start\":20832},{\"end\":20857,\"start\":20844},{\"end\":20869,\"start\":20857},{\"end\":20882,\"start\":20869},{\"end\":21180,\"start\":21163},{\"end\":21187,\"start\":21180},{\"end\":21199,\"start\":21187},{\"end\":21210,\"start\":21199},{\"end\":21216,\"start\":21210},{\"end\":21228,\"start\":21216},{\"end\":21238,\"start\":21228},{\"end\":21249,\"start\":21238},{\"end\":21264,\"start\":21249},{\"end\":21278,\"start\":21264},{\"end\":21737,\"start\":21726},{\"end\":21749,\"start\":21737},{\"end\":21764,\"start\":21749},{\"end\":21779,\"start\":21764},{\"end\":21791,\"start\":21779},{\"end\":22083,\"start\":22069},{\"end\":22095,\"start\":22083},{\"end\":22104,\"start\":22095},{\"end\":22115,\"start\":22104},{\"end\":22127,\"start\":22115}]", "bib_venue": "[{\"end\":15994,\"start\":15942},{\"end\":16187,\"start\":16177},{\"end\":16419,\"start\":16387},{\"end\":16673,\"start\":16622},{\"end\":16958,\"start\":16909},{\"end\":17160,\"start\":17100},{\"end\":17620,\"start\":17571},{\"end\":17879,\"start\":17811},{\"end\":18119,\"start\":18071},{\"end\":18395,\"start\":18372},{\"end\":18651,\"start\":18598},{\"end\":18922,\"start\":18873},{\"end\":19256,\"start\":19234},{\"end\":19574,\"start\":19522},{\"end\":19771,\"start\":19714},{\"end\":20302,\"start\":20253},{\"end\":20592,\"start\":20544},{\"end\":20979,\"start\":20882},{\"end\":21406,\"start\":21278},{\"end\":21842,\"start\":21791},{\"end\":22178,\"start\":22127},{\"end\":15994,\"start\":15942},{\"end\":16187,\"start\":16177},{\"end\":16419,\"start\":16387},{\"end\":16673,\"start\":16622},{\"end\":16958,\"start\":16909},{\"end\":17160,\"start\":17100},{\"end\":17620,\"start\":17571},{\"end\":17879,\"start\":17811},{\"end\":18119,\"start\":18071},{\"end\":18395,\"start\":18372},{\"end\":18651,\"start\":18598},{\"end\":18922,\"start\":18873},{\"end\":19256,\"start\":19234},{\"end\":19574,\"start\":19522},{\"end\":19771,\"start\":19714},{\"end\":20302,\"start\":20253},{\"end\":20592,\"start\":20544},{\"end\":20979,\"start\":20882},{\"end\":21406,\"start\":21278},{\"end\":21842,\"start\":21791},{\"end\":22178,\"start\":22127},{\"end\":18405,\"start\":18397},{\"end\":18405,\"start\":18397}]"}}}, "year": 2023, "month": 12, "day": 17}