{"id": 238216616, "updated": "2023-10-06 10:52:12.373", "metadata": {"title": "Exposure Trajectory Recovery from Motion Blur", "authors": "[{\"first\":\"Youjian\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Chaoyue\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Stephen\",\"last\":\"Maybank\",\"middle\":[\"J.\"]},{\"first\":\"Dacheng\",\"last\":\"Tao\",\"middle\":[]}]", "venue": "IEEE transactions on pattern analysis and machine intelligence", "journal": "IEEE transactions on pattern analysis and machine intelligence", "publication_date": {"year": 2020, "month": 10, "day": 6}, "abstract": "Motion blur in dynamic scenes is an important yet challenging research topic. Recently, deep learning methods have achieved impressive performance for dynamic scene deblurring. However, the motion information contained in a blurry image has yet to be fully explored and accurately formulated because: (i) the ground truth of dynamic motion is difficult to obtain; (ii) the temporal ordering is destroyed during the exposure; and (iii) the motion estimation from a blurry image is highly ill-posed. By revisiting the principle of camera exposure, motion blur can be described by the relative motions of sharp content with respect to each exposed position. In this paper, we define exposure trajectories, which represent the motion information contained in a blurry image and explain the causes of motion blur. A novel motion offset estimation framework is proposed to model pixel-wise displacements of the latent sharp image at multiple timepoints. Under mild constraints, our method can recover dense, (non-)linear exposure trajectories, which significantly reduce temporal disorder and ill-posed problems. Finally, experiments demonstrate that the recovered exposure trajectories not only capture accurate and interpretable motion information from a blurry image, but also benefit motion-aware image deblurring and warping-based video extraction tasks. Codes are available on https://github.com/yjzhang96/Motion-ETR.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.02484", "mag": null, "acl": null, "pubmed": "34582347", "pubmedcentral": null, "dblp": "journals/pami/ZhangWMT22", "doi": "10.1109/tpami.2021.3116135"}}, "content": {"source": {"pdf_hash": "2d3a412ae5b88c61902183a7f39d276414204455", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.02484v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://eprints.bbk.ac.uk/id/eprint/46212/1/ExposureTrajectoryRecovery.pdf", "status": "GREEN"}}, "grobid": {"id": "25d2681b6ff8db941bfcf84c8bf6e22603d04020", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2d3a412ae5b88c61902183a7f39d276414204455.txt", "contents": "\nExposure Trajectory Recovery from Motion Blur\n\n\nYoujian Zhang \nChaoyue Wang \nFellow, IEEEStephen J Maybank \nFellow, IEEEDacheng Tao \nExposure Trajectory Recovery from Motion Blur\n1\nMotion blur in dynamic scenes is an important yet challenging research topic. Recently, deep learning methods have achieved impressive performance for dynamic scene deblurring. However, the motion information contained in a blurry image has yet to be fully explored and accurately formulated because: (i) the ground truth of dynamic motion is difficult to obtain; (ii) the temporal ordering is destroyed during the exposure; and (iii) the motion estimation from a blurry image is highly ill-posed. By revisiting the principle of camera exposure, motion blur can be described by the relative motions of sharp content with respect to each exposed position. In this paper, we define exposure trajectories, which represent the motion information contained in a blurry image and explain the causes of motion blur. A novel motion offset estimation framework is proposed to model pixel-wise displacements of the latent sharp image at multiple timepoints. Under mild constraints, our method can recover dense, (non-)linear exposure trajectories, which significantly reduce temporal disorder and ill-posed problems. Finally, experiments demonstrate that the recovered exposure trajectories not only capture accurate and interpretable motion information from a blurry image, but also benefit motion-aware image deblurring and warping-based video extraction tasks. Codes are available on https://github.com/yjzhang96/Motion-ETR.Index Terms-Motion blur, Exposure trajectory recovery, Motion-aware image deblurring, Video extraction from a single blurry image.\n\nINTRODUCTION\n\nM OTION blur in the dynamic scene caused by camera shake, object motion, or depth variation is one of the commonest image degradations. Estimating motion information and restoring sharp content in dynamic blurry images would benefit many real-world applications including segmentation, detection, and recognition. Benefiting from the powerful fitting ability of deep convolutional neural networks (CNNs), deep learning-based deblurring methods [1], [2], [3], [4] have achieved impressive performance for dynamic motion blur removal. Nevertheless, exploring motion information in a blurry image remains an academic and commercial challenge.\n\nMost conventional blur estimation/removal methods are based on blur kernel optimization [5], [6], [7], [8], [9], [10], [11], which assumes that a blurry area can be represented as a weighted sum of its latent sharp surrounding content. A blur kernel is a weighted matrix that performs convolution on a sharp image patch to synthesize a blurry pixel. Conversely, blur kernel estimation is cast as an energy minimization problem that aims to recover both the blur kernels and the latent sharp image from a blurry image. Such optimizations are highly ill-posed, so most conventional methods are restricted by assumptions of motion types and predefined image priors. For example, [7], [12], [13], [14] only handle blur caused by camera rotations, inplane translations, or forward out-of-plane translations. For more complex dynamic motion blur, identifying a suitably informative and general prior is extremely difficult.\n\nAccompanying the development of deep neural networks, learning-based methods [15], [16] have been proposed to estimate blur kernels directly from blurry images. Compared to optimization-based methods, learning-based methods utilize predefined kernels to synthesize blurry data and then train an estimation network in a supervised manner. A well-trained estimation network is usually more effective and efficient at modeling object motion blur. However, due to the inherent limitations of blurry data synthesis, existing predefined blur kernels only cover limited motion types such as 2D vectors (i.e., linear motions), as in [15]. As a consequence, these methods may not be as effective for complex real-world dynamic scene blur.\n\nTaking advantage of advanced photographic equipment, dynamic scene datasets [1], [17] containing high frame-rate videos have been compiled to further understand dynamic motion blur. A real-world blurry image can be regarded as an accumulation of multiple \"instant\" frames, where a sequence of instant frames implicitly records blur (motion) information during an exposure period. Some methods [18], [19] are trained to directly recover these high frame-rate sharp frames (video extraction) without explicitly depicting dynamic motions. Moreover, in some video deblurring studies [20], [21], optical flow is estimated between adjacent frames as another motion representation. However, since the optical flow between two frames is inherently linear and multiple frames may be misaligned, the estimated optical flow cannot perfectly match the dynamic motion contained in a single blurry image.\n\nAlthough recent years the end-to-end training models can directly recovery the sharp content [2], [3], [22], video sequence [18], [19] or 3D scene [23] from a blurry image, we argue that the motion information carried by a blurry image is important and has not been fully explored due to the aforementioned limitations, e.g. synthetic ground truths, predefined priors, or temporal disorder. In this paper, our main target is to better estimate the motion information of a single dynamic blurry image without using any motion arXiv:2010.02484v2 [cs.CV] 4 Oct 2021 ground truth.\n\nAccording to camera exposure principles, dynamic motion blur is caused by the relative motions of sharp content with respect to each exposed position. Inspired by this principle, we define the trajectories of these relative motions as exposure trajectories. Compared to the convolutional blur kernel, the exposure trajectory can describe a specific physical movement in the temporal order. Through modeling pixel-wise displacements of the latent sharp image at continuous timepoints, exposure trajectories break the linear assumption in existing methods. In addition, we propose a novel motion offsets estimation framework to recover exposure trajectories of a single blurry image. Since the proposed differentiable motion offset module can easily be plug into CNNs for backpropagation, our motion/trajectory estimation framework is trained in a reblurring cycle that only need paired blurry/sharp image pairs. Moreover, to overcome the ill-posed nature of blur estimation and to model complex non-linear motions, we further apply a variety of constraints to ensure that the learned motion offsets form different types of trajectories, e.g., linear, bidirectional linear, or quadratic curves.\n\nBesides recovering accurate and interpretable motion information from a blurry image, we successfully apply the recovered exposure trajectories to two downstream tasks. For image deblurring, we devise a motion-aware deblurring module that takes pixel-wise trajectories to modulate the shape of convolution filters. Experiments show that the proposed motion-aware module enables a more effective deconvolution operation to handle large-scale dynamic motion blur with promising results. In addition, warping-based video extraction from a single blurry image can easily be achieved using the learned exposure trajectories. Compared to existing video extraction models, our solution generates the video with more accurate motions and is capable of interpolating an arbitrary number of middle frames, i.e., deriving slow-motion videos.\n\nIn summary, the contributions of this work are four-fold:\n\n\u2022 We propose a novel framework to model exposure trajectory, which represents the motion information contained in a dynamic blurry image. Compared with existing motion representations, e.g., conventional blur kernels, high-speed video frames (or estimated optical flow), and linear 2D motion vectors, our recovered exposure trajectories are more accurate and easier to interpret. Specifically, the causes of dynamic image blur are the first time modeled as dense, non-linear and continuous trajectories.\n\n\n\u2022\n\nTo recover exposure trajectories from a single blurry image, we proposed a motion offset estimation framework that contains a motion offsets estimation network and a blur creation module. To our best knowledge, it is the first differentiable image (re)blurring module that enables training an end-toend motion estimation network without supervision on ground-truth motion information.\n\n\u2022 To address the ill-posed nature of dynamic motion/trajectory recovery, we propose multiple constraints such that the learned exposure trajectories follow certain patterns. We implement linear, bidirectional linear, and quadratic constraints, and in doing so demonstrate that our motion offsets with non-linear quadratic constraints outperform existing methods in fitting realistic dynamic motion blur.\n\n\u2022 We present extensive experiments and analysis to demonstrate (i) our method can recover accurate and interpretable motion information from a single blurry image (Sec 5.3); (ii) the recovered exposure trajectories further benefit motion blur related tasks. By introducing recovered trajectories and motionaware convolution, we improved the image deblurring performance over the baseline model (Sec 5.4). For extracting videos from a blurry image (Sec 5.5), though our model is only trained on blurry/sharp image pairs, it can synthesize high-quality videos with arbitrary frame rates and further delivers an impressive optical-flow field of the dynamic scene.\n\nThe rest of the paper is organized as follows. After a brief summary of the related works in Section 2, we illustrate our exposure trajectory recovery framework in Section 3. Specifically, we first explain how to model the exposure trajectory, and then introduce the training scheme for exposure trajectory estimation. In Section 4, we attempt to apply the recovered exposure trajectories to image deblurring and video extraction tasks, which justifies the advantages of estimating exposure trajectories. Experiments in Section 5 validate the accuracy of our trajectory estimation, improvements of deblurring performance, and superiority in video extraction, respectively. Limitations and failure cases are discussed in Section 6. Finally, we conclude this paper with some future directions in Section 7.\n\n\nRELATED WORK\n\nSingle image blur estimation and removal have been extensively studied, with many methods proposed to solve different deblurring or blur estimation problems. Here, we focus our discussion on recent motion blur studies, reviewing optimization-and learning-based methods for blur estimation and removal, respectively.\n\n\nOptimization-based Methods\n\nA blur process is conventionally modeled as a convolution operation in which blur kernels are applied to a latent sharp image to generate a blurry output. Given a blurry image, optimization-based methods aim to iteratively recover its deblurred result and the blur kernels that model blur motions. However, this problem is ill-posed, so optimizationbased methods adopt predefined image priors [5], [6], [24], [25], [26], [27], [28], [29], [30] or specific camera motion types [12], [13], [14], [31] to constrain the solution space of the blur kernels. For example, Tai et al. [7] proposed a general projective motion model for cameras undergoing ego motion. Gupta et al. [12] generalized camera motion to 2D translation and in-plane rotation and modeled them as motion density functions. Whyte et al. [13], [32] focused on solving the nonuniform blur caused by camera shake, aiming to recover the 3D rotation of the camera during an exposure process. Zheng et al. [14] attempted to handle another type of motion blur in which the camera moves primarily forwards or backwards by exploring homography associated with different 3D planes. Overall, under predefined priors/assumptions, the ill-posed optimization problem becomes solvable, and these methods have achieved reasonable performance on specific blurry data. However, most of these priors assume that the underlying scene is static and that the blur is caused by camera motion rather than the movement of objects in the captured image.\n\nHowever, it is difficult to identify a suitably informative and general prior for object motion within a dynamic scene. Therefore, some authors [9], [33], [34] have segmented different types of motion blur to overcome this problem. For example, Hyun et al. [9] proposed a novel energy function designed from the weighted sum of multiple blur data models. To handle different types of motion, their method estimated different motion blurs and their associated pixelwise weights. Then, [33] proposed soft-segmentation for object layer estimation. By jointly estimating object segmentation and camera motion, they achieved favorable object motion blur removal performance. Although motion segmentation seems an ideal extension of optimization-based methods, it is hard to estimate an accurate segmentation due to ambiguous pixels between regions. Furthermore, even within a segmented area, existing priors can only handle a limited number of motion types.\n\n\nLearning-based Methods\n\nIn order to overcome the limitations of manually designed image priors or specific camera motions, learning-based methods aim to directly predict blur kernels (or deblurred results) from an input blurry image. Benefiting from the development of CNNs, learning-based models can be trained on a large amount of blurry data and can perform blur estimation (or removal) in an end-to-end manner.\n\nMost learning-based methods were originally proposed to estimate blur causes/representations from blurry images [31], [35], [36], [37]. For example, [35], [36] attempted to identify the type of blur from a restricted set of parametrized blurs. Schuler et al. [37] proposed a CNN module for learning a gradient-like representation and estimated the blur kernels by dividing the learned representation in Fourier space. Similarly, [38] predicted the Fourier coefficients of a deconvolution kernel that modeled blind motions of an image patch. Sun et al. [16] proposed a CNN-based model to predict the probabilistic distribution of motion blur at the patch level. In their method, a well-trained model estimated the direction and length of non-uniform linear motions. Then, [15] developed a fully convolutional framework to achieve pixel-wise prediction of blur kernels. Compared to optimization-based methods, these learning-based methods were more flexible and more efficiently estimated motion blur. However, during training, most learning-based methods required the ground truths of blur representations for supervision. Since the ground truths of real-world blurry data are rarely available, these methods were trained on artificially-generated training examples, limiting the approach to some simple blur types (e.g., linear motion). For more complex real-world dynamic motion, new blur representations and learning schemes are required to improve the estimations.\n\nAccompanying the increased fitting capability of CNNs, many learning-based methods have been proposed to di-rectly restore the latent sharp image from a blurry input [1], [2], [3], [22], [39], [40], [41], [42], [43]. Among these methods, [1] proposed a multi-scale network which performed deblurring in a \"coarse-to-fine\" pipeline. Then, [2], [22] further improved on this strategy by altering the parameter sharing and independent scheme. By combining three CNNs and a recurrent neural network (RNN), Zhang et al. [40] employed the learned variant RNN weights to model spatial-variant blurs. Inspired by [40], many methods [4], [44], [45] have adopted a spatial-variant convolutional module as a substitute for some of the original convolution layers to increase the size of the receptive field in a more compact way. In addition, Kupyn et al. [39] and Ramakrishnan et al. [46] combined deblurring with generative adversarial networks (GANs) to synthesize more realistic sharp images. Overall, the combination of recently established real-world blurry datasets [1] and the powerful learning capability of CNNs have allowed learning-based methods to achieve impressive performance for directly synthesizing deblurred images. Unfortunately, the causes of blur (motions) are generally ignored in these works, preventing the exploration of the rich dynamic information contained in blurry images and introducing training difficulties for related tasks due to a poor understanding of dynamic motion blur. For example, in the absence of motion information, some deblurring and video extraction approaches either require a large receptive field to model large-scale blur [40] or require a complex training scheme and iterative inferences [18], [19], [47]. In this work, we show that improving blur estimation can contribute to overcoming these problems and solving these tasks.\n\n\nEXPOSURE TRAJECTORY RECOVERY\n\n\nMotion Exposure Mechanism\n\nWhen a camera takes a photograph, the exposure time cannot be instant due to technological constraints and physics (i.e., exposure requirements). Therefore, a photograph records a target scene over a period of time. The exposure process can be formulated as:\nB = \u03c4 0 H(L, t) dt,(1)\nwhere L represents the latent content/scene in the photograph, H(L, t) denotes the instant frame at time t, and \u03c4 denotes the camera exposure time. Due to camera shake or the motion or deformation of objects in the scene, H(L, t) may continuously vary with respect to time, leading to dynamic scene blurry image B.\n\nIn this work, we assume the middle instant sharp frame L s records all visual information of latent content/scene L. 1 According to the principle of camera exposure, the function H(L s , t) can be defined as an image wrapping operation that performs a pixel-wise shift over different times, i.e.,\nH(L s , t) = L s (P + \u2206P t ),(2)\nwhere P denotes all pixels in L s , and \u2206p t = (\u2206x t , \u2206y t ) is the shift of pixel (x, y) at time t. Assuming the brightness 1. For most dynamic scene motion blur datasets, the middle instant frame is regarded as a sharp ground truth.  remains constant during exposure, we consider Eq. (1) and (2) and discretize them over multiple time steps N to derive the formation of a blurry pixel p 0 as:\nB(p 0 ) = 1 N N \u22121 n=0 L s (p 0 + \u2206p tn 0 ),(3)\nwhich means a blurry pixel can be represented as the accumulation of pixels in the latent image moved by \u2206p tn . In this work, instead of deriving the blur kernels of a blurry image, we directly focus on the spatial shift \u2206p tn of each pixel. Thus, we propose a new time-dependent blur representation, exposure trajectory which can be sampled by a set of motion offsets ({\u2206P tn } N \u22121 n=0 ). Similar to conventional blur kernels, the proposed exposure trajectory directly act on sharp images and then output blurry results. In contrast, our exposure trajectory models the blur formation as a spatial shift through time.\n\n\nBlur Creation Module\n\nBased on the proposed exposure trajectory and motion exposure mechanism, we devise a blur creation module which takes one sharp image L s and a set of motion offsets as inputs to generate a dynamic blurry image. For each blurry pixel (i.e., exposure location) p, the proposed blur creation module is asked to locate pixels p tn = p + \u2206p tn in a latent sharp image L s (Eq. (3)) and further average them to obtain a blurry pixel. Since a real-world dynamic motion is continuous, we employ the bilinear interpolation to calculate the pixel value of location p tn ,\nL s (p + \u2206p tn ) = L s (p tn ) = q G(q, p tn ) \u00b7 L s (q), (4)\nwhere q enumerates the referenced neighborhood points of the sampling location p tn , and G(\u00b7, \u00b7) is the bilinear interpolation kernel. As illustrated in Fig. 1, our motion offsets are of the same spatial resolution as the input image. Each offset has two channels corresponding to 2D axes. In practice, the blur creation module takes N motion offsets and a sharp image L s as inputs and synthesizes an averaged blurry output. Discussion. Compared to conventional blur kernels, the proposed exposure trajectory (or motion offset) aims to mimic the exposure process of a camera sensor. If we assume the latent content/scene is known, motion offsets encode motion/dynamic information during an exposure period and can further synthesize a blurry image. Mathematically, the proposed motion offsets can be expressed in the formulation of blur kernels. Specifically, in the general blur kernel model, a blurry image B is represented as B = k * L+noise, where k represents a blur kernel. In such a framework, our motion offsets can be regarded as an equivalent blur kernel k(p 0 , t n ) of the location p 0 over time\n{t n } N \u22121 n=0 , k(p 0 , t n ) = \u03b4(p\u2212(p0+\u2206p tn 0 )) N , if p 0 + \u2206p tn 0 \u2208 L s 0, otherwise(5)\nwhere \u03b4(\u00b7) denotes the Dirac delta function. Analyzing the equivalent formulation, the following differences between conventional blur kernels and our proposed motion offsets may exist. First, a time variable t n is introduced as a new and important element to model the exposure process. Under reasonable constraints (discussed in Section 3.4), our time-dependent motion offsets can act as a visualizable and interpretable representation of motion blur. Second, different from the weight matrix in a conventional blur kernel, our motion offsets calculate a uniform average of wrapped frames over a time sequence. We assume each time step is equally discretized; thus, the degree of motion (or blur) in each position is represented by the learned spatial displacements {\u2206p tn }. Since the values of \u2206p tn are continuous, bilinear interpolation (Eq. (4)) is performed to derive the value on each discrete pixel position. Note that the bilinear interpolation operation plays the same role as the weight matrix in blur kernels. Third, benefiting from the spatial shift operation (i.e., \u2206p tn ), our equivalent blur kernel (i.e., motion offset) will not be limited by size, shape, pattern, or resolution, as traditional kernels are. For example, compared to the dense blur kernels estimated in [20], where each kernel size is 33 \u00d7 33, the proposed motion offsets only carry N \u00d7 2 parameters 2 . Finally, since our motion offsets are compact and differentiable, the blur creation module can easily be integrated into deep neural networks and trained in an end-to-end manner.\n\n\nMotion Offset Estimation\n\nThe biggest problem of previous learning-based blur kernel (motion) estimation is the universal absence of ground truth blur kernels for real-world data. Thus, [15], [16] must use synthetic data for training. Based on the motion exposure mechanism, the proposed motion offset could replace conventional blur kernels. Exploiting its compact and differentiable advantages, we devise a training scheme that performs motion offset estimation without any ground truths of the motion information. Specifically, the blur creation module is connected with the motion offset estimation network to form a cyclical pipeline.\n\nGiven a ground truth blurry image B and a sharp reference frame L s , the motion offset estimation network takes B as an input and outputs N motion offsets; then, the blur creation module takes L s and the motion offsets as input to reproduce the estimated blurry imageB. Fig.  1 illustrates this procedure. The motion offset estimation network is based on an encoder-decoder network with skip connections, and the detailed model structure is provided in Section 5.1.\n\nThe loss of this cyclic reconstruction can be written as:\nL circle = L l2 + \u03bb SSIM L SSIM ,(6)\nwhere L l2 and L SSIM denote the 2 loss and SSIM loss respectively. Both are applied to measure the difference between B andB. We elaborate these two terms as follows:\nL 2 = ||B \u2212B|| 2 2 , (7) L SSIM (P ) = 1 \u2212 MS-SSIM(p),(8)\nwherep is the center pixel of patch P , and MS-SSIM denotes the multi-scale SSIM. A more specific definition and implementation can be found in [48]. The reason that we use the SSIM loss is that the 2 loss only weakly penalizes our output because it tends to generate average results, and the blurry image is already averaged. In this case, the SSIM loss more accurately measures the distance between two blurry images.\n\nWe also introduce other losses to regularize motion offsets. First, we apply a regularization loss to encourage offsets that search for nearby pixels as solutions. This benefits the situation in which there is a large smooth region, e.g., the sky or ground, where large displacements (offsets) should be suppressed. Moreover, due to dynamic motion blur usually being continuous along the space, we apply the total variation loss to encourage spatial smoothness within offset maps. These two losses can be formulated as:\nL reg = 1 N wh N n=1 w i=1 h j=1 M n (i, j) 2 ,(9)\n2. As shown our experiments, setting N as 15 already achieves extraordinary performance.\nLtv = 1 N N n=1 1 (w \u2212 1)h w\u22121 i=0 |Mn(i, j) \u2212 Mn(i + 1, j)|+ 1 w(h \u2212 1) h\u22121 j=0 |Mn(i, j) \u2212 Mn(i, j + 1)| ,(10)\nwhere M n (i, j) denotes the location (i, j) in the n th offset map.\n\nIn summary, the final loss function is a weighted sum of the above losses:\nL = L circle + \u03bb reg L reg + \u03bb tv L tv .(11)\n\nDifferent Constraints to Motion Offsets\n\nIf we directly learn all the motion offsets using the framework described above, namely a zero constraint (ZC) model, the results will be as shown in Fig. 2 (a). Though achieving impressive reblurring accuracy, its ill-posed nature creates the following problems: (1) the learned motion offsets are one of several possible solutions of blur formation, and since it is difficult to form them into an explicit trajectory as in real-world blur formation, the learned motion offsets are usually sub-optimal for describing realistic motion; and (2) although there exists the temporal variable t n in our learned offsets, these offsets are disordered due to a lack of spatialtemporal relationship modeling. Therefore, we devise several constraints to reduce the illposed nature of motion estimation and to form the motion offsets into an explainable exposure trajectory: (Bidirectional) linear trajectory constraint. The linear assumption is used to fit motion blur in many methods [10], [15], [16]. We also devise a linear trajectory constraint for motion offsets. Recall our assumption (Section 3.1) that the sharp image L s represents the middle instant frame, i.e. \u2206p t mid = (0, 0). To represent linear motion, the motion offset estimation network only needs to predict another point on the exposure trajectory. Suppose the blurred pixel is caused by uniform linear motion and the predicted offset \u2206p is an endpoint of the exposure trajectory, the other offsets can be derived as:\n\u2206p tn = (1 \u2212 2n N \u2212 1 )\u2206p, n = 0, . . . , N \u2212 1.(12)\nWe attempt to predict the furthest point (endpoint) of the exposure trajectory based on the observation that the blurred edge is easier to capture and estimate. Taking a further step, we can apply a bidirectional linear (b-d linear) constraint to our motion offsets. As shown in Fig. 2 (c), we predict two offsets \u2206p 1 , \u2206p 2 to represent the start and end points of each exposure trajectory. Then, the other offsets can be calculated as:\n\u2206p tn = (1 \u2212 2n N \u22121 )\u2206p 1 , n = 0, . . . , N \u22121 2 , ( 2n N \u22121 \u2212 1)\u2206p 2 . n = N +1 2 , . . . , N \u2212 1.(13)\nAs shown in Fig. 2, this trajectory better fits a curve than the linear one. Quadratic trajectory constraint. Although the bi-directional linear constraint already introduces a certain non-linearity into trajectory learning, the quadratic function can better approximate real-world motion [49], [50]. A quadratic curve can be derived when an object is moving with constant acceleration, a much stronger fitting than the (bi-)linear assumption. Thus, we devise a quadratic trajectory constraint to force a smooth quadratic trajectory on our motion offsets. Unlike previous works, which apply a quadratic trajectory between video frames, we extract this trajectory inside a single blurry frame. Specifically, we still predict two offsets \u2206p 1 , \u2206p 2 as the start and end points of the exposure trajectory, with the other offsets written as:\n\u2206p tn = \u2206p 1 + \u2206p 2 2 ( 2n N \u2212 1 \u2212 1) 2 + \u2206p 2 \u2212 \u2206p 1 2 ( 2n N \u2212 1 \u2212 1), n = 0, . . . , N \u2212 1.(14)\nThus, motion offsets will be formed into a quadratic trajectory ( Fig. 2 (d)). Note that since our motion offsets are modeled in equidistant time, the learned motion offsets not only match a curvilinear exposure trajectory but also reflect the changing velocity. For example, a longer displacement between adjacent time steps corresponds to faster movement.\n\n\nAPPLICATIONS BENEFITING FROM RECOVERED EXPOSURE TRAJECTORIES\n\nIn this section, we apply our recovered exposure trajectory to two motion blur-related downstream tasks, i.e. image deblurring and video extraction from a single blurry image, which further demonstrates the benefit of recovering accurate motions from a blurry image.\n\n\nMotion-aware Image Deblurring\n\nTo handle the challenging problem of dynamic scene deblurring, existing works employ complex network architectures to enlarge the model capacity, such as a multi-scale structure [1], [2], [22]. Some other methods [4], [40], [44] claim that spatially invariant convolution filters, i.e. spatially uniform and limited receptive fields are sub-optimal for modeling dynamic scene blur. With the recovered exposure trajectories, we aim to design a spatial-variant deblurring network, which leads to a more compact and efficient model. As derived in [4], [40], the blurry image deconvolution can be written as an Infinite Impulse Response (IIR) formula, from which they drew two main conclusions: 1) the deblurring process requires a very large receptive field; and 2) for a CNN-based deblurring model, the convolution filters should have a similar direction/shape with the blur kernel. For example, if a blur kernel is linear and horizontal, the latent pixels can be calculated using only horizontal blurry pixels, thus the deconvolution filters should also be pure horizontal. However, the conventional square-shaped convolutional filter cannot meet these requirements. In this work, we propose a motion-aware deblurring network with spatial-variant convolution filters that are shaped by the recovered exposure trajectories.\n\nTo build a spatial-variant convolution module, the deformable convolution unit [51] provides a general solution. In recent works [4], [44], spatial-variant deblurring modules based on deformable convolutions have achieved reasonable performance. However, since the ground truth of the kernel shape is absent, these methods attempt to derive deformation offsets from encoded features of an input blurry image. We propose the motion-aware convolution (MA Conv.), which directly employs the recovered motion offsets to model the aforementioned filter deformation. Our motion-aware convolution can be formulated as:\ny(p 0 ) = N n=0 w(p n ) \u00b7 x(p 0 + \u03b1\u2206p n 0 ),(15)\nwhere x is an input feature map, y is an output feature map, and w is the weight of the convolution filter. The coordinate p 0 + \u03b1\u2206p n 0 denotes the sampling location calculated by the centering coordinate p 0 and an offset \u03b1\u2206p n 0 , which controls the shape and size of the convolution, and w(p n ) is the weight corresponding for the sampling point p 0 + \u03b1\u2206p n 0 . For a square-shaped 3 \u00d7 3 convolutional filter with dilation 1, \u2206p tn 0 \u2208 (\u22121, \u22121), (\u22121, 0), . . . , (0, 1), (1, 1) and \u03b1 = 1. In our motion-aware convolution, \u2206p n 0 are decided by our recovered motion offsets. Specifically, giving the recovered exposure trajectory, we calculate 9 \u2206p n 0 centered by the middle offset \u2206p mid 0 = (0, 0) to modulate the original 3 \u00d7 3 kernel, as shown in Fig. 3. Moreover, we use hyperparameter \u03b1 to scale the size of the convolution, and \u03b1 is set to 0.1 in our experiments. In this way, the proposed motion-aware convolution takes full advantage of the information contained in motion offsets, i.e. both direction  Fig. 3: The proposed motion-aware deblurring network. An encoder-decoder residual architecture for image deblurring is shown on the left, while the schematic of a motion-aware convolution in the motion-aware block is shown on the right. and magnitude, resulting in a more mathematically accurate deconvolution.\n\nHere, we adopts the DMPHN(1-2-4) [3] as the backbone architecture of our motion-aware deblurring network, since it is relatively compact among the state-of-the-art models. As shown in Fig. 3, similar with the most existing image deblurring methods, the encoder-decoder structure is employed. Compare to the vanilla DMPHN(1-2-4), our motionaware deblurring network can be easily derived by replacing the selected convolutional layers with the proposed motion-aware convolution. According to our experiments, adding the motion-aware convolutions in the last stage of the decoder achieved the best performance. In addition, to build a compact deblurring network, we do not employ the stack-DMPHN as Zhang et al [3]. Adding the motion-aware module can already achieve comparable results, and our model largely reduces the memory cost.\n\n\nWarping-based Video Extraction from a Single Blurry Image\n\nDifferent from conventional blur kernels, our motion offsets contain temporal information that could help us to restore time series from a blurry input. As indicated in Eq. (2), frame L tn can be obtained through a transformation H(\u00b7, \u00b7). Now, with the deblurring resultL s and the estimated motion offsetsP tn , we can generate the estimated frameL tn :\nL tn =L s (P + \u2206P tn ).(16)\nUnlike forward optical flow which is a many-to-one mapping, our motion offset is equivalent to backward warping flow. Specifically, the backward warping flow represents the pixel displacement from warped result to the warping input. Therefore, there will be no holes in our warped result. According to Section 3.4, since we have added different trajectory constraints, theoretically we can interpolate arbitrary N offsets into our start and end offsets, which further leads to smooth or even slow-motion video output. To our best knowledge, only two existing works have been capable of restoring a video sequence from a single blurry image. [18] first attempted to generated a video sequence from a single blurry image by training different networks to generate frames at different time t n , only producing limited frames. [19] proposed a recurrent network to address temporal ambiguity, inferring the recurrent state at each time step t n . Unlike these methods, we only need to calculate our motion offsets once, which is more time efficient. Moreover, these previous methods are trained with a series of ground truth sharp frames for supervision, which limit the generated outputs to specific time intervals. Our motion offset estimation module is easy to train and requires fewer annotations. Moreover, during test, our model is more compact and efficient.   \n\n\nEXPERIMENTS\n\nIn this section, we first introduce our training configuration before carrying out quantitative and qualitative comparisons between our method and state-of-the-art methods for motion estimation, image deblurring, and video extraction.\n\n\nImplementation Details\n\nWe provide layer-wise details of our motion offset estimation networks in Table 1. H and W represent the height and width of an input blurry image. For training both the motion estimation network and deblurring network, we use Adam [52] for optimization, with \u03b2 1 = 0.9, \u03b2 2 = 0.999 and = 10 \u22128 . The learning rate is set initially to 10 \u22124 and it is linearly decayed to 0. For motion offset estimation, we set the offset number to N = 15, \u03bb SSIM = 0.1, \u03bb reg = 0.00002, \u03bb tv = 0.0005. All weights are initialized using Xavier [53], and bias is initialized to 0. We first train the motion estimation network using the blurry image and the groundtruth sharp image as the training pair. Then we train the deblurring network with the pre-trained motion estimation network. To train the deblurring network, besides paired training images, the estimated exposure trajectories are utilized as an auxiliary input.\n\n\nDatasets\n\nWe employ two different datasets. The synthetic dataset provides ground truth blur kernels, while the GoPro dataset is synthesized from real-world frame with more challenging dynamic motion blur without ground truth blur kernels.\n\nSynthetic Dataset. We follow the same approach as in [15] to generate blurry/sharp image pairs with pre-defined blur kernels. Specifically, blur kernels are represented by a motion flow map filled with pixel-wise non-uniform motion vectors. Each vector can form a linear blur kernel. Same as [15], we use images from BSD500 [54], which consists of 200 training images and 100 test images, as sharp ground truths. We then generate 50 motion flow maps for each training image and 3 motion flow maps for the test images. Finally, the sharp images are convolved with the corresponding flow maps to generate blurry images.\n\nThe GoPro Dataset [1] addresses the problem that synthetic data are different from real-world blurry images containing more complex dynamic motion. More realistic blurry images are generated by averaging consecutive shortexposure frames from a high frame rate video, e.g., 240fps, taken from a GoPro camera. In this way, [1] collected 3214 blurry/sharp image pairs, and split them into a training set with 2103 pairs and a test set with 1111 pairs. In following experiments, unless stated, the quantitative results are based on the GoPro dataset.\n\n\nEvaluation of Motion Offset Estimation\n\nWe compare the proposed exposure trajectory recovery with one conventional blur kernel estimation method (Xu et al. [55]) and two recent learning-based blur kernel estimation methods (Sun et al. [16] and Gong et al. [15]). Our comparisons are based on both the synthetic and GoPro datasets. Evaluation Metrics. In order to evaluate the accuracy of motion estimation, we calculate the PSNR and SSIM metrics between the input blurry image and reblurred image via estimated blur kernel/motion offsets for both datasets. Specifically, the reblurred results of [16] and [15] can be obtained by convolving a sharp image with the estimated motion flow map. We also apply the MSE metric of motion to evaluate the synthetic data. This metric defines the mean squared error between the ground truth motion and estimated motion [15]. The MSE is easy to calculate in [16] and [15] since their estimated blur kernels share the same form as the ground truth, namely 2D vectors. However, our motion offsets are a set of points, so we calculate the vector of two endpoints as a simplification based on the assumption that the motion is linear. Note that we only provide the kernel visualization results of [55], since its blur kernel cannot be represented as a pixel-wise motion flow map like the others.  Table 2 shows our quantitative comparisons on the synthetic dataset. Our models with different constraints achieve comparable or better performance to [15]. It is noteworthy that [15] is learned in a supervised manner, yet our training scheme need no ground-truth motion as supervision. Although the synthetic blur is linear, we can see the two non-linear constraint models (our b-d linear and quadratic) producing better reblurring PSNR results than the two linear constraint models ( [15] and our linear), we infer that (i) comparing with linear model, the non-linear model has a higher degree of freedom brought by the estimation of two endpoints(only one for linear model), thus a greater fitting ability; (ii) the motion field of synthetic blur changes continuously and form into curves in the space (Fig. 4). Overall, although the blurry kernel on each pixel is linear, the quadratic model has better representation ability and delivers a more accurate approximation for continuous change modeled in the synthetic dataset.\n\nWe can also make some observations from Fig. 4. Xu et al. [55] generates non-trajectory kernels, for which we can only vaguely observe the flow after post-processing. Since Sun et al. [16] performs a patch-level prediction from a blurry input, it is usually misled by the smooth area. Gong et al. [15] shows more continuity across space, but there is also the possibility of when a region of predictions going wrong.     Conversely, ours are more accurate and can perfectly fit into linear motion regardless of the employed constraints.\n\n\nMotion Estimation on the GoPro Dataset.\n\nSince there is no motion ground truth for the GoPro dataset, the methods in [15], [16] cannot train their networks. Here, we employ their models pre-trained on the synthetic dataset and then test them on the GoPro test set. It is unfair to directly compare these results with our own; however, to our best knowledge, no other method is trained without motion ground truths, so their results seem to be a legitimate reference. Table 2 shows that the performance of [15], [16] decreases significantly with more complex dynamic scenes. This decrease in quality can also be observed in the example in Fig. 5 and Fig. 6. We first provide an example of visualized blur kernels estimated from different models in Fig. 5. As we can see, Xu et al. [55] fails to estimate dynamic motion blur, and Sun et al. [16] is obviously inaccurate and tends to generate spatially uniform kernels. The results using Gong et al. [15], although spatially variant, tend to produce many non-blurry regions. Our results, however, show a different flow direction in the background and foreground, e.g., the moving car. To better demonstrate the accuracy and effectiveness of our proposed quadratic trajectory, we show more examples of the estimated motions and the corresponding reblurring regions in Fig. 6. Our models generate more accurate reblurring results compared existing works. Also, the quadratic model is more effective in recovering the quadratic motion trajectory. Note that since the warping through motion offsets is conducted backward rather than forward, the exposure trajectory is center-symmetrical to the object moving trajectory.\n\nAblation Studies. First, we discuss the setting of offset numbers N . As shown in Table 3, the model with N = 15 is notably better than the other models. The visual differences produced by altering the offset numbers are shown in Fig. 7. There is ghosting artifact with the model with 5 offsets, which becomes smoother as the offset number increases from 5 to 15. With 15 offsets, the result is very close to the ground truth image. Since increasing the number of offsets has little effect on performance, we set N = 15, considering the balance between performance and efficiency.    Table 4. The proposed loss combination is better than those without certain losses. The total variation loss which encourage the local uniformity not only improves the motion estimation accuracy, but also solves the ambiguity of the motion direction. The ambiguity of direction always exists since a single blurry image barely contains the direction information, yet the predicted directions under total variance loss will be spatially continuous rather than random. as shown in Fig. 8, we visualize the magnitude and direction of optimized motion offsets by calculating the vector subtraction between the first motion offset and the last motion offset. The model with TV loss generates a smooth color map, while the model w/o TV loss generates the color map with noise dots, which means the directions of these pixels are discontinuous or even opposite to the adjacent pixels. Moreover, although the regulation loss has little influence on the metrics, it prevents the network from estimating large offsets in the smooth area.\n\n\nEvaluation of Dynamic Scene Deblurring\n\nWe quantitatively and qualitatively compare our method with recent state-of-the-art dynamic scene deblurring methods: DeblurGAN(-v2) [39], [56] based on a conditional GAN   to obtain a more realistic texture; Nah et al. [1], Tao et al. [22], and Gao et al. [2] built multi-scale networks but with different parameter sharing and parameter independence schemes; Zhang et al. [3] applies a Deep Multi-Patch Hierarchical Network (DMPHN), which is also the backbone network of our method. We also provide the deblurring results with [15] as representative of conventional MAP optimization. The quantitative results are presented in Table 5.\n\nAs illustrated in Table 5, our motion-aware deblurring network achieves comparable results to current state-of-theart methods with respect to PSNR and achieved slightly better result with respect to SSIM. Note that, our model achieves such performance using a single-stack, which only costs about 30% of the model size compared to the model of the stack(4)-DMPHN. Considering the only difference between our model with DMPHN is the proposed motionaware convolutional layer, it contributes 0.84 and 0.014 increasing in PSNR and SSIM, respectively. Also, as shown in Fig. 9, the visual results of ours are almost the same as stack(4)-DMPHN, while better than the other methods.\n\nBesides verifying that the learned exposure trajectories could contribute to dynamic scene deblurring, we also conduct ablation studies to discuss the effects of different kinds of exposure trajectories. As Table 6 shows, compared to the baseline model, all kinds of exposure trajectories could improve the deblurring performance. The Model Linear and B-d linear perform slightly inferior to the Model Quadratic, owning to the less accurate of the motion estimation. Note that, though the exposure trajectory learned with zeroconstraint (ZC) achieves the best score in above reblurring experiments (Table 2-GoPro), it demonstrates less effect in our deblurring module. A reasonable explanation is that the zero-constraint motion offsets are only one of the ill-posed solutions for reblurring reconstruction, yet it will not be the most accurate trajectory estimation.\n\n\nEvaluation of Video Extraction\n\nTo evaluate the performance of our approach for video extraction, we compare our results with those of Jin et al. [18], Purohit et al. [19] and Zhang et al. [47]. Since the source codes of methods of Purohit et al. [19] and Zhang et al. [47] are not public yet, we can only provide the data recorded in their papers. We first directly compare the accuracy of the extracted frames. In Table 7, the PSNR and SSIM metrics are applied to measure the deblurring performance of the centered frame, and our method achieves the highest scores. Also, we can see from the Fig. 10 that the deblurring result (frame 4) and video extraction results (frames 1 and 7) of [18] both perform poorly when handling large blur. The results of [19] and our own show relatively sharp frames and clear object movements. Then we further validate the accuracy of the motion in extracted videos through comparisons on the optical flow estimated from the synthesized videos. We follow the approach of calculating end-point error (EPE) in [47]. Specifically, we first estimate optical flow from the first generated frame to the last one with PWC-net [57] and vice versa. Then, EPE calculates the errors between estimated flows and the flow estimated from ground truth high-framerate video. The lower EPE value is chosen as the result, since the direction is uncertain. As shown in Table 7, our methods achieve the best EPE. For a fair comparison, we combine the centered frame generated from model of Jin et al. [18] with our motion offsets to warp the other frames, termed Jin et al. [18] + Ours, demonstrating that our improvement on EPE score comes mainly from the more accurate motion estimation rather than a superior deblurring result. Due to the limitation of using optical flow to evaluate video clarity, we obtain similar EPE scores from the generated video of Jin et al. [18] + ours and ours. To summary, the metrics in Table 7 demonstrate that the extracted video from our model is sharper and the encoded motion is more accurate. Qualitative comparison of the visualized optical flow in Fig. 11 also shows that the videos extracted using our method present a more accurate optical flow compared to [18].\n\nBesides the validation of optical flow from first frame to last frame, we also validate the effectiveness of our quadratic trajectory. As shown in Fig. 12, we visualize the trajectory using feature point tracking [58]. Our quadratic motion offsets better fit the curve to the ground truth, especially in the second example.\n\nOur exposure trajectory recovery framework employs blurry/sharp image pairs as training data. It delivers impressive optical flow estimation/trajectory results of dynamic scenes without accessing any motion supervision. Moreover, our motion offsets can be combined with any state-of-the-art deblurring method to generate even sharper video clips, while other methods need to train a whole new model. Finally, our model can generate arbitrary numbers of frames, while [18] can only achieve a fixed number. Although [19], [47] can also generate slow-motion videos, they need to conduct a iterative generation which increase the inference time. However, we only need to interpolate in our trajectory after a single forward prediction. As a result, our network is more compact and faster. The runtimes of [18], [19], and our model are 1.1 s, 0.39 s, and 0.22 s respectively. We also provide an example of video extraction from real images in Fig. 13, demonstrating a good generalization ability of our proposed method. More video results can be found in our supplementary video.\n\n\nLIMITATIONS\n\nThere are two main limitations that existed in our proposed method. First, a more complicated motion may be caused by a large camera shake or a highly dynamic scene, which may need to be modeled by a higher order of exposure trajectory. In these situations, our quadratic constrained exposure trajectory can only act as an approximation of the groundtruth trajectory. However, since the motion captured in the dataset is relatively small, these situations rarely happen in the existing blurry image datasets. Second, similar to most learning-based deep models, our method may have generalization issues, it may fail to handle blur patterns that have a domain gap with the blur in training data. Considering our model still need to be trained on blurry/sharp image pairs, i.e., the GoPro dataset, which is an approximation of real-world blurry images, our well-trained network may fail in recovering the trajectory when encounter unseen realworld blur (Fig. 14). Solving this domain-gap problem is very challenging since an unsupervised training strategy is required for the unpaired real-world data. However, we believe our fully differentiable (re)blurring module can potentially contribute to unsupervised motion estimation/image deblurring, since it can provide a cycle-consistency loss. In the future, we will continue to devote to develop fully unsupervised deblurring and motion estimation methods for motion estimation.\n\n\nCONCLUSION\n\nHere we propose an exposure trajectory recovery scheme to generate motion offsets which are superior to conventional blur kernels in many respects. By imposing different constraints, these offsets can fit into different exposure trajectories. Moreover, we utilize the learned motion offsets for image deblurring and video extraction from a single blurry image. Experiments show that our motion offsets can produce useful information for solving these tasks. However, the learned exposure trajectories are still limited to motion of constant acceleration, and may not perfectly fit real situations. We will further devote to provide more accurate motion estimation and further improve the deblurring and video extraction tasks.\n\nFig. 1 :\n1Illustration of our proposed motion offset estimation method. The figure on the left is our motion offset generation network. It takes blurry images as input and outputs the corresponding motion offsets. Afterwards, the blur creation module (on the right) takes a sharp image and the extracted motion offsets to reconstruct the input blurry image.\n\nFig. 2 :\n2Examples of motion offsets with different constraints. Suppose the green curve is the ground truth exposure trajectory. (a)-(d) simulate the fitting results of motion offsets with no constraint, linear constraint, b-d linear constraint, and quadratic constraint, respectively. Red points are the offsets that output by the estimation network and blue points are calculated by the different constraints.\n\n( d )\ndGong et al. (c) Sun et al. (b) Xu et al.\n\nFig. 4 :\n4Examples of motion estimation on the synthetic dataset. The top row shows the blurry input, ground truth motion, and results of previous methods. The bottom row shows our estimated motion offsets under different constraints.\n\nFig. 5 :\n5Examples of motion estimation on the GoPro dataset. The top row shows the blurry input and results of previous methods. The bottom row shows our estimated motion offsets under different constraints. Sun et al. Gong et al. Ours Sharp GT Blurry GT Fig. 6: Visual comparison of motion estimation on GoPro test set. From left to right shows the groud-truth image patches, the results of Sun et al. [16], Gong et al. [15] and our quadratic model. For each method, we visualize the estimated motion field and the reblurred result. Motion Estimation on the Synthetic Dataset.\n\nFig. 7 :\n7The effect of offset number on blur creation. Left to right show the ground truth blurry image, the result of the model with 5 offsets, the result of the model with 9 offsets, and the result of the model with 15 offsets. It is clear that increasing the number of offsets creates a smoother and more realistic blurry output.\n\nFig. 8 :\n8Visualize the magnitude and direction of motion offsets in a color coded map. Different colors represent different directions. The model with TV loss generates a much more smooth color map than the model w/o TV loss (best view in high resolutions).\n\nFig. 11 :\n11Comparison of optical flow from the extracted videos. The optical flow is calculated from the first and last frame of the extracted videos (Jin et al.[18] and ours) and the ground truth high-frame-rate video (GT).\n\n\nJin et al.    \n\nFig. 12 :\n12Visualized trajectory result of extracted frames (best view in high resolutions).\n\nFig. 14 :\n14A failure case on real-world blurry image. From left to right are the blurry input, the visualized exposure trajectory and color-coded motion offset map. The obvious error exists where the estimated motion is very small or has a wrong direction.\n\n\nConv.Motion offsets \n\nModulation \n\nInput feature map \n\nDeformable \nconvolution \n\nOutput feature map \n\nMA Conv. \n\n! \n! \n\nReLU \n\nResBlock \n\nReLU \n\nMA Block \n\nDeconv. \n\nMotion offsets \n\nMA Conv. \n\nDeformable offsets \n\n! \n\n! \n\n\u00d73 \n\n\n\nTABLE 1 :\n1Detailed architecture of the motion offset estimation network. + denotes that a skip connection concatenates this layer with the corresponding layer in the encoder.Stage \nOutput \nLayer Details \n\nH \n\n2 \u00d7 W \n\n2 \n\nSpace to Depth \nConv1 \n\nH \n\n2 \u00d7 W \n\n2 \n\n5 \u00d7 5, 12, 16, stride 1 \n\nResBlock1 \n\nH \n\n2 \u00d7 W \n\n2 \n\n5 \u00d7 5, 16 \n5 \u00d7 5, 16 \n\u00d7 3 \n\nConv2 \n\nH \n\n4 \u00d7 W \n\n4 \n\n5 \u00d7 5, 16, 32, stride 2 \n\nResBlock2 \n\nH \n\n4 \u00d7 W \n\n4 \n\n5 \u00d7 5, 32 \n5 \u00d7 5, 32 \n\u00d7 3 \n\nConv3 \n\nH \n\n8 \u00d7 W \n\n8 \n\n5 \u00d7 5, 32, 64, stride 2 \n\nResBlock3 \n\nH \n\n8 \u00d7 W \n\n8 \n\n5 \u00d7 5, 64 \n5 \u00d7 5, 64 \n\u00d7 3 \n\nBottleneck1 \n\nH \n\n8 \u00d7 W \n\n8 \n\n1 \u00d7 1, 64, 128 \n3 \u00d7 3, 128, 64 \n\nDconv1 \n\nH \n\n4 \u00d7 W \n\n4 \n\n5 \u00d7 5, 64, 32, stride 2 \n\nBottleneck2 \n\nH \n\n4 \u00d7 W \n\n4 \n\n1 \u00d7 1, 32 + 32, 128 \n3 \u00d7 3, 128, 64 \n\nDconv2 \n\nH \n\n2 \u00d7 W \n\n2 \n\n5 \u00d7 5, 64, 16, stride 2 \n\nBottleneck3 \n\nH \n\n2 \u00d7 W \n\n2 \n\n1 \u00d7 1, 16 + 16, 64 \n3 \u00d7 3, 64, 32 \nDconv3 \nH \u00d7 W \n5 \u00d7 5, 32, 32, stride 2 \nConv4 \nH \u00d7 W \n5 \u00d7 5, 32, 4, stride 1 \n\n\n\nTABLE 2 :\n2Quantitative comparison of motion estimation on both synthetic and the GoPro [1] dataset.Model \nSun et al. [16] Gong et al. [15] Zero constraint (ZC) Linear B-d Linear Quadratic \n\nSynthetic \n\nPSNR \n29.34 \n37.61 \n37.62 \n37.34 \n38.64 \n38.9 \nSSIM \n0.9001 \n0.9818 \n0.9763 \n0.9857 \n0.9872 \n0.9882 \nMSE \n50.12 \n10.05 \n-\n7.42 \n7.16 \n3.27 \n\nGoPro \nPSNR \n29.68 \n30.61 \n35.82 \n33.45 \n33.79 \n34.68 \nSSIM \n0.9282 \n0.9363 \n0.9800 \n0.9669 \n0.9687 \n0.9740 \n\nRuntime(s) \n45.2 \n8.4 \n0.011 \n0.011 \n0.011 \n0.011 \n\n\n\nTABLE 3 :\n3Comparison for the setting of offset numbers N .# of motion offsets \n5 \n9 \n15 \n\nPSNR \n34.09 \n34.52 \n34.68 \nSSIM \n0.9668 0.9727 0.974 \n\n\n\nTABLE 4 :\n4Ablation study for loss function.Proposed w/o SSIM w/o tv w/o reg \n\nPSNR \n34.68 \n34.16 \n33.96 \n34.56 \nSSIM \n0.974 \n0.97 \n0.9672 \n0.9727 \n\n\n\n\nTo demonstrate the effectiveness of the proposed loss function, we trained a model with all the proposed lossesFig. 10: Comparison of video extraction results. In the top-down order, we show ours, result of[19], result of[18].PSNR:25.58 \nSSIM:0.9034 \n\nPSNR:24.45 \nSSIM:0.8925 \n\nPSNR:19.60 \nSSIM:0.7839 \n\nPSNR:23.38 \nSSIM:0.7428 \n\nPSNR:28.13 \nSSIM:0.9081 \n\nPSNR:27.60 \nSSIM:0.8818 \n\nPSNR:29.59 \nSSIM:0.9167 \nPSNR:29.29 \nSSIM:0.9092 \n\nPSNR:23.00 \nSSIM:0.8682 \n\nPSNR:25.51 \nSSIM:0.9076 \n\nFig. 9: Visual comparison with GoPro dataset. From left to right, we show input, deblurring result of DeblurGAN-V2 [56], \nGao et al. [2], DMPHN [3], ours, and stack(4)-DMPHN [3] (best view in high resolutions). \n\nFrame 1 Frame 4 Frame 7 \nFrame 1 Frame 4 Frame 7 \n\n\nTABLE 5 :\n5Quantitative deblurring results on GoPro dataset.Model \nGong [15] \nNah [1] \nTao [22] \nDeblurGAN [39] \nDeblurGAN-V2 [56] \nGao [2] \nDMPHN [3] \nOurs \nStack(4)-DMPHN [3] \n\nPSNR \n26.89 \n29.08 \n30.26 \n28.7 \n29.55 \n30.92 \n30.21 \n31.05 \n31.20 \n\nSSIM \n0.8639 \n0.9135 \n0.9342 \n0.9270 \n0.9340 \n0.9421 \n0.9345 \n0.9485 \n0.9453 \n\nSize(MB) \n54.1 \n303.6 \n33.6 \n35.4 \n244.5 \n46.5 \n21.7 \n26.3 \n86.8 \n\n\n\nTABLE 6 :\n6Ablation study of motion-aware convolution on GoPro dataset.Model Baseline (w/o MA Conv.) Zero constraint (ZC) Linear B-d Linear Quadratic \n\nPSNR \n30.21 \n30.79 \n30.82 \n31.04 \n31.05 \nSSIM \n0.9345 \n0.9459 \n0.9462 \n0.9483 \n0.9485 \n\n(Model Proposed), one without the SSIM loss (Model w/o \nSSIM), one without the total variation loss (Model w/o tv), \nand one without the regulation loss (Model w/o reg). The \nquantitative results are shown in \n\n\nFig. 13: Video extraction results with real images. More video results are provided in our supplementary video.Input \nTrajectory Frame 1 \nFrame 2 Frame 3 \nFrame 4 Frame 5 \nFrame 6 \nFrame 7 \n\n\n\nTABLE 7 :\n7Quantitative comparison of video extraction on GoPro dataset.Method \nPSNR \nSSIM \nEPE \n\nJin et al. [18] \n26.98 \n0.881 \n9.32 \nPurohit et al. [19] \n30.58 \n0.941 \n-\nZhang et al. [47] \n30.64 \n0.942 \n10.03 \nJin et al. + Ours \n26.98 \n0.881 \n6.07 \nOurs \n31.05 \n0.949 \n6.09 \n\n\nACKNOWLEDGMENTSThe authors would like to thank anonymous reviewers and the handlining editor for their constructive comments. This work was supported by Australian Research Council Projects FL-170100117, IH-180100002, IC-190100031.\nDeep multi-scale convolutional neural network for dynamic scene deblurring. S Nah, T Hyun Kim, K. Mu Lee, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitS. Nah, T. Hyun Kim, and K. Mu Lee, \"Deep multi-scale convo- lutional neural network for dynamic scene deblurring,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 3883-3891.\n\nDynamic scene deblurring with parameter selective sharing and nested skip connections. H Gao, X Tao, X Shen, J Jia, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitH. Gao, X. Tao, X. Shen, and J. Jia, \"Dynamic scene deblurring with parameter selective sharing and nested skip connections,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 3848-3856.\n\nDeep stacked hierarchical multi-patch network for image deblurring. H Zhang, Y Dai, H Li, P Koniusz, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitH. Zhang, Y. Dai, H. Li, and P. Koniusz, \"Deep stacked hierarchical multi-patch network for image deblurring,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 5978-5986.\n\nRegion-adaptive dense network for efficient motion deblurring. K Purohit, A Rajagopalan, Assoc. Advanc. Artif. Intell. K. Purohit and A. Rajagopalan, \"Region-adaptive dense network for efficient motion deblurring.\" in Assoc. Advanc. Artif. Intell., 2020, pp. 11 882-11 889.\n\nRemoving camera shake from a single photograph. R Fergus, B Singh, A Hertzmann, S T Roweis, W T Freeman, ACM SIGGRAPH. R. Fergus, B. Singh, A. Hertzmann, S. T. Roweis, and W. T. Freeman, \"Removing camera shake from a single photograph,\" in ACM SIGGRAPH 2006 Papers, 2006, pp. 787-794.\n\nSingle image motion deblurring using transparency. J Jia, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitIEEEJ. Jia, \"Single image motion deblurring using transparency,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE, 2007, pp. 1-8.\n\nRichardson-lucy deblurring for scenes under a projective motion path. Y.-W Tai, P Tan, M S Brown, IEEE Trans. Pattern Anal. Mach Intell. 338Y.-W. Tai, P. Tan, and M. S. Brown, \"Richardson-lucy deblurring for scenes under a projective motion path,\" IEEE Trans. Pattern Anal. Mach Intell., vol. 33, no. 8, pp. 1603-1618, 2010.\n\nEfficient marginal likelihood optimization in blind deconvolution. A Levin, Y Weiss, F Durand, W T Freeman, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitIEEEA. Levin, Y. Weiss, F. Durand, and W. T. Freeman, \"Efficient marginal likelihood optimization in blind deconvolution,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. 2011. IEEE, 2011, pp. 2657-2664.\n\nDynamic scene deblurring. T Hyun Kim, B Ahn, K. Mu Lee, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisT. Hyun Kim, B. Ahn, and K. Mu Lee, \"Dynamic scene deblur- ring,\" in Proc. IEEE Int. Conf. Comput. Vis., 2013, pp. 3160-3167.\n\nSegmentation-free dynamic scene deblurring. T , Hyun Kim, K. Mu Lee, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitT. Hyun Kim and K. Mu Lee, \"Segmentation-free dynamic scene deblurring,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014, pp. 2766-2773.\n\nPhase-only image based kernel estimation for single image blind deblurring. L Pan, R Hartley, M Liu, Y Dai, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitL. Pan, R. Hartley, M. Liu, and Y. Dai, \"Phase-only image based kernel estimation for single image blind deblurring,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 6034-6043.\n\nSingle image deblurring using motion density functions. A Gupta, N Joshi, C L Zitnick, M Cohen, B Curless, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisSpringerA. Gupta, N. Joshi, C. L. Zitnick, M. Cohen, and B. Curless, \"Single image deblurring using motion density functions,\" in Proc. Eur. Conf. Comput. Vis. Springer, 2010, pp. 171-184.\n\nNon-uniform deblurring for shaken images. O Whyte, J Sivic, A Zisserman, J Ponce, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. Vis98O. Whyte, J. Sivic, A. Zisserman, and J. Ponce, \"Non-uniform deblurring for shaken images,\" Proc. IEEE Int. Conf. Comput. Vis., vol. 98, no. 2, pp. 168-186, 2012.\n\nForward motion deblurring. S Zheng, L Xu, J Jia, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisS. Zheng, L. Xu, and J. Jia, \"Forward motion deblurring,\" in Proc. IEEE Int. Conf. Comput. Vis., 2013, pp. 1465-1472.\n\nFrom motion blur to motion flow: a deep learning solution for removing heterogeneous motion blur. D Gong, J Yang, L Liu, Y Zhang, I Reid, C Shen, A Van Den, Q Hengel, Shi, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitD. Gong, J. Yang, L. Liu, Y. Zhang, I. Reid, C. Shen, A. Van Den Hengel, and Q. Shi, \"From motion blur to motion flow: a deep learning solution for removing heterogeneous motion blur,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 2319- 2328.\n\nLearning a convolutional neural network for non-uniform motion blur removal. J Sun, W Cao, Z Xu, J Ponce, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitJ. Sun, W. Cao, Z. Xu, and J. Ponce, \"Learning a convolutional neural network for non-uniform motion blur removal,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 769-777.\n\nDeep video deblurring for hand-held cameras. S Su, M Delbracio, J Wang, G Sapiro, W Heidrich, O Wang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitS. Su, M. Delbracio, J. Wang, G. Sapiro, W. Heidrich, and O. Wang, \"Deep video deblurring for hand-held cameras,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 1279-1288.\n\nLearning to extract a video sequence from a single motion-blurred image. M Jin, G Meishvili, P Favaro, Proc. IEEE Conf. Comput. Vis. IEEE Conf. Comput. VisM. Jin, G. Meishvili, and P. Favaro, \"Learning to extract a video sequence from a single motion-blurred image,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 6334-6342.\n\nBringing alive blurred moments. K Purohit, A Shah, A Rajagopalan, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitK. Purohit, A. Shah, and A. Rajagopalan, \"Bringing alive blurred moments,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 6830-6839.\n\nReblur2deblur: Deblurring videos via self-supervised learning. H Chen, J Gu, O Gallo, M.-Y Liu, A Veeraraghavan, J Kautz, Proc. IEEE Int. Conf. Comput. Photography. IEEE. IEEE Int. Conf. Comput. Photography. IEEEH. Chen, J. Gu, O. Gallo, M.-Y. Liu, A. Veeraraghavan, and J. Kautz, \"Reblur2deblur: Deblurring videos via self-supervised learning,\" in Proc. IEEE Int. Conf. Comput. Photography. IEEE, 2018, pp. 1-9.\n\nSelfsupervised linear motion deblurring. P Liu, J Janai, M Pollefeys, T Sattler, A Geiger, IEEE Trans. Robot. Autom. 52P. Liu, J. Janai, M. Pollefeys, T. Sattler, and A. Geiger, \"Self- supervised linear motion deblurring,\" IEEE Trans. Robot. Autom., vol. 5, no. 2, pp. 2475-2482, 2020.\n\nScale-recurrent network for deep image deblurring. X Tao, H Gao, X Shen, J Wang, J Jia, Proc. IEEE Conf. Comput. Vis. IEEE Conf. Comput. VisX. Tao, H. Gao, X. Shen, J. Wang, and J. Jia, \"Scale-recurrent network for deep image deblurring,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 8174-8182.\n\nWorld from blur. J Qiu, X Wang, S J Maybank, D Tao, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitJ. Qiu, X. Wang, S. J. Maybank, and D. Tao, \"World from blur,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 8493-8504.\n\nBlind image deblurring with local maximum gradient prior. L Chen, F Fang, T Wang, G Zhang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitL. Chen, F. Fang, T. Wang, and G. Zhang, \"Blind image deblurring with local maximum gradient prior,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 1742-1750.\n\nHigh-quality motion deblurring from a single image. Q Shan, J Jia, A Agarwala, ACM Trans. Graph. 273Q. Shan, J. Jia, and A. Agarwala, \"High-quality motion deblurring from a single image,\" ACM Trans. Graph., vol. 27, no. 3, pp. 1-10, 2008.\n\nFast motion deblurring. S Cho, S Lee, ACM Trans. Graph. 285145S. Cho and S. Lee, \"Fast motion deblurring,\" ACM Trans. Graph., vol. 28, no. 5, p. 145, 2009.\n\nTwo-phase kernel estimation for robust motion deblurring. L Xu, J Jia, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisSpringerL. Xu and J. Jia, \"Two-phase kernel estimation for robust motion deblurring,\" in Proc. Eur. Conf. Comput. Vis. Springer, 2010, pp. 157-170.\n\nl 0-regularized intensity and gradient prior for deblurring text images and beyond. J Pan, Z Hu, Z Su, M.-H Yang, IEEE Trans. Pattern Anal. Mach Intell. 392J. Pan, Z. Hu, Z. Su, and M.-H. Yang, \"l 0-regularized intensity and gradient prior for deblurring text images and beyond,\" IEEE Trans. Pattern Anal. Mach Intell., vol. 39, no. 2, pp. 342-355, 2016.\n\nDeblurring images via dark channel prior. J Pan, D Sun, H Pfister, M.-H Yang, IEEE Trans. Pattern Anal. Mach Intell. 4010J. Pan, D. Sun, H. Pfister, and M.-H. Yang, \"Deblurring images via dark channel prior,\" IEEE Trans. Pattern Anal. Mach Intell., vol. 40, no. 10, pp. 2315-2328, 2017.\n\nAppealnet: An efficient and highly-accurate edge/cloud collaborative architecture for dnn inference. M Li, Y Li, Y Tian, L Jiang, Q Xu, arXiv:2105.04104arXiv preprintM. Li, Y. Li, Y. Tian, L. Jiang, and Q. Xu, \"Appealnet: An efficient and highly-accurate edge/cloud collaborative architecture for dnn inference,\" arXiv preprint arXiv:2105.04104, 2021.\n\nMotion-based motion deblurring. S K Nayar, M Ben-Ezra, IEEE Trans. Pattern Anal. Mach Intell. 266S. K. Nayar and M. Ben-Ezra, \"Motion-based motion deblurring,\" IEEE Trans. Pattern Anal. Mach Intell., vol. 26, no. 6, pp. 689-698, 2004.\n\nDeblurring shaken and partially saturated images. O Whyte, J Sivic, A Zisserman, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. Vis110O. Whyte, J. Sivic, and A. Zisserman, \"Deblurring shaken and partially saturated images,\" Proc. IEEE Int. Conf. Comput. Vis., vol. 110, no. 2, pp. 185-201, 2014.\n\nSoft-segmentation guided object motion deblurring. J Pan, Z Hu, Z Su, H.-Y. Lee, M.-H Yang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitJ. Pan, Z. Hu, Z. Su, H.-Y. Lee, and M.-H. Yang, \"Soft-segmentation guided object motion deblurring,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 459-468.\n\nDiscriminative blur detection features. J Shi, L Xu, J Jia, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitJ. Shi, L. Xu, and J. Jia, \"Discriminative blur detection features,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014, pp. 2965-2972.\n\nImage restoration in neural network domain using back propagation network approach. C Khare, K K Nagwanshi, Image. 25C. Khare and K. K. Nagwanshi, \"Image restoration in neural net- work domain using back propagation network approach,\" Image, vol. 2, no. 5, 2011.\n\nBlur identification using neural network for image restoration. I Aizenberg, D Paliy, C Moraga, J Astola, Comput. Intell., Theory Appl. SpringerI. Aizenberg, D. Paliy, C. Moraga, and J. Astola, \"Blur identifi- cation using neural network for image restoration,\" in Comput. Intell., Theory Appl. Springer, 2006, pp. 441-455.\n\nLearning to deblur. C J Schuler, M Hirsch, S Harmeling, B Sch\u00f6lkopf, IEEE Trans. Pattern Anal. Mach Intell. 387C. J. Schuler, M. Hirsch, S. Harmeling, and B. Sch\u00f6lkopf, \"Learning to deblur,\" IEEE Trans. Pattern Anal. Mach Intell., vol. 38, no. 7, pp. 1439-1451, 2015.\n\nA neural approach to blind motion deblurring. A Chakrabarti, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisSpringerA. Chakrabarti, \"A neural approach to blind motion deblurring,\" in Proc. Eur. Conf. Comput. Vis. Springer, 2016, pp. 221-235.\n\nDeblurgan: Blind motion deblurring using conditional adversarial networks. O Kupyn, V Budzan, M Mykhailych, D Mishkin, J Matas, Proc. IEEE Conf. Comput. Vis. IEEE Conf. Comput. VisO. Kupyn, V. Budzan, M. Mykhailych, D. Mishkin, and J. Matas, \"Deblurgan: Blind motion deblurring using conditional adversar- ial networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 8183-8192.\n\nDynamic scene deblurring using spatially variant recurrent neural networks. J Zhang, J Pan, J Ren, Y Song, L Bao, R W Lau, M.-H Yang, Proc. IEEE Conf. Comput. Vis. IEEE Conf. Comput. VisJ. Zhang, J. Pan, J. Ren, Y. Song, L. Bao, R. W. Lau, and M.-H. Yang, \"Dynamic scene deblurring using spatially variant recurrent neural networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2521-2529.\n\nRecurrent neural networks with intra-frame iterations for video deblurring. S Nah, S Son, K M Lee, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitS. Nah, S. Son, and K. M. Lee, \"Recurrent neural networks with intra-frame iterations for video deblurring,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 8102-8111.\n\nEdvr: Video restoration with enhanced deformable convolutional networks. X Wang, K C Chan, K Yu, C Dong, C. Change Loy, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops. IEEE Conf. Comput. Vis. Pattern Recognit. WorkshopsX. Wang, K. C. Chan, K. Yu, C. Dong, and C. Change Loy, \"Edvr: Video restoration with enhanced deformable convolutional networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Work- shops, 2019, pp. 0-0.\n\nEvolutionary generative adversarial networks. C Wang, C Xu, X Yao, D Tao, IEEE Trans. Evol. Comput. 236C. Wang, C. Xu, X. Yao, and D. Tao, \"Evolutionary generative adversarial networks,\" IEEE Trans. Evol. Comput., vol. 23, no. 6, pp. 921-934, 2019.\n\nSpatially-attentive patch-hierarchical network for adaptive motion deblurring. M Suin, K Purohit, A Rajagopalan, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitM. Suin, K. Purohit, and A. Rajagopalan, \"Spatially-attentive patch-hierarchical network for adaptive motion deblurring,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 3606-3615.\n\nEfficient dynamic scene deblurring using spatially variant deconvolution network with optical flow guided training. Y Yuan, W Su, D Ma, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitY. Yuan, W. Su, and D. Ma, \"Efficient dynamic scene deblurring using spatially variant deconvolution network with optical flow guided training,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 3555-3564.\n\nDeep generative filter for motion deblurring. S Ramakrishnan, S Pachori, A Gangopadhyay, S Raman, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisS. Ramakrishnan, S. Pachori, A. Gangopadhyay, and S. Raman, \"Deep generative filter for motion deblurring,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 2993-3000.\n\nEvery moment matters: Detail-aware networks to bring a blurry image alive. K Zhang, W Luo, B Stenger, W Ren, L Ma, H Li, Proc. ACM Int. Conf. Multimed., 2020. ACM Int. Conf. Multimed., 2020K. Zhang, W. Luo, B. Stenger, W. Ren, L. Ma, and H. Li, \"Every moment matters: Detail-aware networks to bring a blurry image alive,\" in Proc. ACM Int. Conf. Multimed., 2020, pp. 384-392.\n\nLoss functions for image restoration with neural networks. H Zhao, O Gallo, I Frosio, J Kautz, IEEE Trans. Comput. Imag. 31H. Zhao, O. Gallo, I. Frosio, and J. Kautz, \"Loss functions for image restoration with neural networks,\" IEEE Trans. Comput. Imag., vol. 3, no. 1, pp. 47-57, 2016.\n\nVideo deblurring via semantic segmentation and pixel-wise non-linear kernel. W Ren, J Pan, X Cao, M.-H Yang, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisW. Ren, J. Pan, X. Cao, and M.-H. Yang, \"Video deblurring via semantic segmentation and pixel-wise non-linear kernel,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 1077-1085.\n\nQuadratic video interpolation. X Xu, L Siyao, W Sun, Q Yin, M.-H Yang, Proc. Advances Neural Inf. Process. Syst. Advances Neural Inf. ess. SystX. Xu, L. Siyao, W. Sun, Q. Yin, and M.-H. Yang, \"Quadratic video interpolation,\" in Proc. Advances Neural Inf. Process. Syst., 2019, pp. 1645-1654.\n\nDeformable convolutional networks. J Dai, H Qi, Y Xiong, Y Li, G Zhang, H Hu, Y Wei, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisJ. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, \"Deformable convolutional networks,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 764-773.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimiza- tion,\" arXiv preprint arXiv:1412.6980, 2014.\n\nUnderstanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio, Proceedings of the thirteenth international conference on artificial intelligence and statistics. the thirteenth international conference on artificial intelligence and statisticsX. Glorot and Y. Bengio, \"Understanding the difficulty of training deep feedforward neural networks,\" in Proceedings of the thirteenth international conference on artificial intelligence and statistics, 2010, pp. 249-256.\n\nContour detection and hierarchical image segmentation. P Arbelaez, M Maire, C Fowlkes, J Malik, IEEE Trans. Pattern Anal. Mach Intell. 335P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, \"Contour de- tection and hierarchical image segmentation,\" IEEE Trans. Pattern Anal. Mach Intell., vol. 33, no. 5, pp. 898-916, 2010.\n\nUnnatural l0 sparse representation for natural image deblurring. L Xu, S Zheng, J Jia, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitL. Xu, S. Zheng, and J. Jia, \"Unnatural l0 sparse representation for natural image deblurring,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2013, pp. 1107-1114.\n\nDeblurgan-v2: Deblurring (orders-of-magnitude) faster and better. O Kupyn, T Martyniuk, J Wu, Z Wang, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisO. Kupyn, T. Martyniuk, J. Wu, and Z. Wang, \"Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better,\" in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 8878-8887.\n\nPwc-net: Cnns for optical flow using pyramid, warping, and cost volume. D Sun, X Yang, M.-Y Liu, J Kautz, Proc. IEEE Conf. Comput. Vis. IEEE Conf. Comput. VisD. Sun, X. Yang, M.-Y. Liu, and J. Kautz, \"Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 8934-8943.\n\nGood features to track. J Shi, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitIEEEJ. Shi et al., \"Good features to track,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE, 1994, pp. 593-600.\n", "annotations": {"author": "[{\"end\":63,\"start\":49},{\"end\":77,\"start\":64},{\"end\":108,\"start\":78},{\"end\":133,\"start\":109}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":57},{\"end\":76,\"start\":72},{\"end\":107,\"start\":100},{\"end\":132,\"start\":129}]", "author_first_name": "[{\"end\":56,\"start\":49},{\"end\":71,\"start\":64},{\"end\":97,\"start\":90},{\"end\":99,\"start\":98},{\"end\":128,\"start\":121}]", "author_affiliation": null, "title": "[{\"end\":46,\"start\":1},{\"end\":179,\"start\":134}]", "venue": null, "abstract": "[{\"end\":1729,\"start\":182}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2192,\"start\":2189},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2197,\"start\":2194},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2202,\"start\":2199},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2207,\"start\":2204},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2477,\"start\":2474},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2482,\"start\":2479},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2487,\"start\":2484},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2492,\"start\":2489},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2497,\"start\":2494},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2503,\"start\":2499},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2509,\"start\":2505},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3065,\"start\":3062},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3071,\"start\":3067},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3077,\"start\":3073},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3083,\"start\":3079},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3386,\"start\":3382},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3392,\"start\":3388},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3934,\"start\":3930},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4115,\"start\":4112},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4121,\"start\":4117},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4433,\"start\":4429},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4439,\"start\":4435},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4619,\"start\":4615},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4625,\"start\":4621},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5024,\"start\":5021},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5029,\"start\":5026},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5035,\"start\":5031},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5056,\"start\":5052},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5062,\"start\":5058},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5079,\"start\":5075},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11116,\"start\":11113},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11121,\"start\":11118},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11127,\"start\":11123},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11133,\"start\":11129},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11139,\"start\":11135},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11145,\"start\":11141},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11151,\"start\":11147},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11157,\"start\":11153},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11163,\"start\":11159},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11200,\"start\":11196},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11206,\"start\":11202},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11212,\"start\":11208},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11218,\"start\":11214},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11299,\"start\":11296},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11395,\"start\":11391},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11525,\"start\":11521},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11531,\"start\":11527},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11688,\"start\":11684},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12360,\"start\":12357},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12366,\"start\":12362},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12372,\"start\":12368},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12473,\"start\":12470},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12701,\"start\":12697},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13700,\"start\":13696},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13706,\"start\":13702},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13712,\"start\":13708},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13718,\"start\":13714},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13737,\"start\":13733},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13743,\"start\":13739},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13847,\"start\":13843},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14017,\"start\":14013},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14140,\"start\":14136},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14359,\"start\":14355},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15222,\"start\":15219},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15227,\"start\":15224},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15232,\"start\":15229},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15238,\"start\":15234},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15244,\"start\":15240},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15250,\"start\":15246},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15256,\"start\":15252},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15262,\"start\":15258},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15268,\"start\":15264},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15294,\"start\":15291},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15394,\"start\":15391},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15400,\"start\":15396},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15572,\"start\":15568},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15662,\"start\":15658},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15680,\"start\":15677},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15686,\"start\":15682},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15692,\"start\":15688},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15902,\"start\":15898},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15931,\"start\":15927},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16118,\"start\":16115},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16722,\"start\":16718},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16789,\"start\":16785},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16795,\"start\":16791},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16801,\"start\":16797},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17701,\"start\":17700},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22127,\"start\":22123},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22595,\"start\":22591},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22601,\"start\":22597},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23984,\"start\":23980},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26242,\"start\":26238},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26248,\"start\":26244},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26254,\"start\":26250},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":27633,\"start\":27629},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":27639,\"start\":27635},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29181,\"start\":29178},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29186,\"start\":29183},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29192,\"start\":29188},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29216,\"start\":29213},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29222,\"start\":29218},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29228,\"start\":29224},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29547,\"start\":29544},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29553,\"start\":29549},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":30406,\"start\":30402},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30455,\"start\":30452},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30461,\"start\":30457},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32349,\"start\":32346},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33024,\"start\":33021},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34233,\"start\":34229},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34416,\"start\":34412},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":35465,\"start\":35461},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":35760,\"start\":35756},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36436,\"start\":36432},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36675,\"start\":36671},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":36707,\"start\":36703},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37019,\"start\":37016},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37322,\"start\":37319},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":37707,\"start\":37703},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37786,\"start\":37782},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37807,\"start\":37803},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38147,\"start\":38143},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38156,\"start\":38152},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38408,\"start\":38404},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38446,\"start\":38442},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38455,\"start\":38451},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":38781,\"start\":38777},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":39032,\"start\":39028},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":39060,\"start\":39056},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":39367,\"start\":39363},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":39969,\"start\":39965},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40095,\"start\":40091},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":40208,\"start\":40204},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":40567,\"start\":40563},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40573,\"start\":40569},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":40955,\"start\":40951},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40961,\"start\":40957},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":41230,\"start\":41226},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41289,\"start\":41285},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41397,\"start\":41393},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":43902,\"start\":43898},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":43908,\"start\":43904},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":43988,\"start\":43985},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":44005,\"start\":44001},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":44025,\"start\":44022},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":44142,\"start\":44139},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":44298,\"start\":44294},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46100,\"start\":46096},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46121,\"start\":46117},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":46143,\"start\":46139},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46201,\"start\":46197},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":46223,\"start\":46219},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46642,\"start\":46638},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46708,\"start\":46704},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":46996,\"start\":46992},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47107,\"start\":47103},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":47469,\"start\":47465},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":47542,\"start\":47538},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":47838,\"start\":47834},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":48167,\"start\":48163},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":48387,\"start\":48383},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":48966,\"start\":48962},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":49013,\"start\":49009},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":49019,\"start\":49015},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":49300,\"start\":49296},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":49306,\"start\":49302},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":54153,\"start\":54149},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":56769,\"start\":56765},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":56784,\"start\":56780}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":52111,\"start\":51753},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52525,\"start\":52112},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52574,\"start\":52526},{\"attributes\":{\"id\":\"fig_5\"},\"end\":52810,\"start\":52575},{\"attributes\":{\"id\":\"fig_6\"},\"end\":53390,\"start\":52811},{\"attributes\":{\"id\":\"fig_7\"},\"end\":53725,\"start\":53391},{\"attributes\":{\"id\":\"fig_8\"},\"end\":53985,\"start\":53726},{\"attributes\":{\"id\":\"fig_9\"},\"end\":54212,\"start\":53986},{\"attributes\":{\"id\":\"fig_10\"},\"end\":54229,\"start\":54213},{\"attributes\":{\"id\":\"fig_11\"},\"end\":54324,\"start\":54230},{\"attributes\":{\"id\":\"fig_12\"},\"end\":54583,\"start\":54325},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54814,\"start\":54584},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":55749,\"start\":54815},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":56257,\"start\":55750},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56405,\"start\":56258},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56556,\"start\":56406},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":57307,\"start\":56557},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":57703,\"start\":57308},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":58154,\"start\":57704},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":58348,\"start\":58155},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":58628,\"start\":58349}]", "paragraph": "[{\"end\":2384,\"start\":1745},{\"end\":3303,\"start\":2386},{\"end\":4034,\"start\":3305},{\"end\":4926,\"start\":4036},{\"end\":5504,\"start\":4928},{\"end\":6698,\"start\":5506},{\"end\":7530,\"start\":6700},{\"end\":7589,\"start\":7532},{\"end\":8094,\"start\":7591},{\"end\":8484,\"start\":8100},{\"end\":8889,\"start\":8486},{\"end\":9551,\"start\":8891},{\"end\":10357,\"start\":9553},{\"end\":10689,\"start\":10374},{\"end\":12211,\"start\":10720},{\"end\":13165,\"start\":12213},{\"end\":13582,\"start\":13192},{\"end\":15051,\"start\":13584},{\"end\":16924,\"start\":15053},{\"end\":17243,\"start\":16985},{\"end\":17581,\"start\":17267},{\"end\":17879,\"start\":17583},{\"end\":18308,\"start\":17913},{\"end\":18976,\"start\":18357},{\"end\":19563,\"start\":19001},{\"end\":20736,\"start\":19626},{\"end\":22402,\"start\":20833},{\"end\":23044,\"start\":22431},{\"end\":23513,\"start\":23046},{\"end\":23572,\"start\":23515},{\"end\":23777,\"start\":23610},{\"end\":24255,\"start\":23836},{\"end\":24776,\"start\":24257},{\"end\":24916,\"start\":24828},{\"end\":25098,\"start\":25030},{\"end\":25174,\"start\":25100},{\"end\":26741,\"start\":25262},{\"end\":27233,\"start\":26795},{\"end\":28178,\"start\":27340},{\"end\":28635,\"start\":28278},{\"end\":28966,\"start\":28700},{\"end\":30321,\"start\":29000},{\"end\":30934,\"start\":30323},{\"end\":32311,\"start\":30984},{\"end\":33143,\"start\":32313},{\"end\":33559,\"start\":33205},{\"end\":34952,\"start\":33588},{\"end\":35202,\"start\":34968},{\"end\":36135,\"start\":35229},{\"end\":36377,\"start\":36148},{\"end\":36996,\"start\":36379},{\"end\":37544,\"start\":36998},{\"end\":39905,\"start\":37587},{\"end\":40443,\"start\":39907},{\"end\":42109,\"start\":40487},{\"end\":43722,\"start\":42111},{\"end\":44401,\"start\":43765},{\"end\":45078,\"start\":44403},{\"end\":45947,\"start\":45080},{\"end\":48168,\"start\":45982},{\"end\":48493,\"start\":48170},{\"end\":49569,\"start\":48495},{\"end\":51011,\"start\":49585},{\"end\":51752,\"start\":51026}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17266,\"start\":17244},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17912,\"start\":17880},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18356,\"start\":18309},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19625,\"start\":19564},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20832,\"start\":20737},{\"attributes\":{\"id\":\"formula_5\"},\"end\":23609,\"start\":23573},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23835,\"start\":23778},{\"attributes\":{\"id\":\"formula_7\"},\"end\":24827,\"start\":24777},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25029,\"start\":24917},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25219,\"start\":25175},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26794,\"start\":26742},{\"attributes\":{\"id\":\"formula_11\"},\"end\":27339,\"start\":27234},{\"attributes\":{\"id\":\"formula_12\"},\"end\":28277,\"start\":28179},{\"attributes\":{\"id\":\"formula_13\"},\"end\":30983,\"start\":30935},{\"attributes\":{\"id\":\"formula_14\"},\"end\":33587,\"start\":33560}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35310,\"start\":35303},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":38884,\"start\":38877},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":40920,\"start\":40913},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":42200,\"start\":42193},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":42702,\"start\":42695},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":44400,\"start\":44393},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":44428,\"start\":44421},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":45294,\"start\":45287},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":45686,\"start\":45678},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":46373,\"start\":46366},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":47341,\"start\":47334},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":47890,\"start\":47883}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1743,\"start\":1731},{\"end\":8098,\"start\":8097},{\"attributes\":{\"n\":\"2\"},\"end\":10372,\"start\":10360},{\"attributes\":{\"n\":\"2.1\"},\"end\":10718,\"start\":10692},{\"attributes\":{\"n\":\"2.2\"},\"end\":13190,\"start\":13168},{\"attributes\":{\"n\":\"3\"},\"end\":16955,\"start\":16927},{\"attributes\":{\"n\":\"3.1\"},\"end\":16983,\"start\":16958},{\"attributes\":{\"n\":\"3.2\"},\"end\":18999,\"start\":18979},{\"attributes\":{\"n\":\"3.3\"},\"end\":22429,\"start\":22405},{\"attributes\":{\"n\":\"3.4\"},\"end\":25260,\"start\":25221},{\"attributes\":{\"n\":\"4\"},\"end\":28698,\"start\":28638},{\"attributes\":{\"n\":\"4.1\"},\"end\":28998,\"start\":28969},{\"attributes\":{\"n\":\"4.2\"},\"end\":33203,\"start\":33146},{\"attributes\":{\"n\":\"5\"},\"end\":34966,\"start\":34955},{\"attributes\":{\"n\":\"5.1\"},\"end\":35227,\"start\":35205},{\"attributes\":{\"n\":\"5.2\"},\"end\":36146,\"start\":36138},{\"attributes\":{\"n\":\"5.3\"},\"end\":37585,\"start\":37547},{\"end\":40485,\"start\":40446},{\"attributes\":{\"n\":\"5.4\"},\"end\":43763,\"start\":43725},{\"attributes\":{\"n\":\"5.5\"},\"end\":45980,\"start\":45950},{\"attributes\":{\"n\":\"6\"},\"end\":49583,\"start\":49572},{\"attributes\":{\"n\":\"7\"},\"end\":51024,\"start\":51014},{\"end\":51762,\"start\":51754},{\"end\":52121,\"start\":52113},{\"end\":52532,\"start\":52527},{\"end\":52584,\"start\":52576},{\"end\":52820,\"start\":52812},{\"end\":53400,\"start\":53392},{\"end\":53735,\"start\":53727},{\"end\":53996,\"start\":53987},{\"end\":54240,\"start\":54231},{\"end\":54335,\"start\":54326},{\"end\":54825,\"start\":54816},{\"end\":55760,\"start\":55751},{\"end\":56268,\"start\":56259},{\"end\":56416,\"start\":56407},{\"end\":57318,\"start\":57309},{\"end\":57714,\"start\":57705},{\"end\":58359,\"start\":58350}]", "table": "[{\"end\":54814,\"start\":54591},{\"end\":55749,\"start\":54991},{\"end\":56257,\"start\":55851},{\"end\":56405,\"start\":56318},{\"end\":56556,\"start\":56451},{\"end\":57307,\"start\":56785},{\"end\":57703,\"start\":57369},{\"end\":58154,\"start\":57776},{\"end\":58348,\"start\":58268},{\"end\":58628,\"start\":58422}]", "figure_caption": "[{\"end\":52111,\"start\":51764},{\"end\":52525,\"start\":52123},{\"end\":52574,\"start\":52534},{\"end\":52810,\"start\":52586},{\"end\":53390,\"start\":52822},{\"end\":53725,\"start\":53402},{\"end\":53985,\"start\":53737},{\"end\":54212,\"start\":53999},{\"end\":54229,\"start\":54215},{\"end\":54324,\"start\":54243},{\"end\":54583,\"start\":54338},{\"end\":54591,\"start\":54586},{\"end\":54991,\"start\":54827},{\"end\":55851,\"start\":55762},{\"end\":56318,\"start\":56270},{\"end\":56451,\"start\":56418},{\"end\":56785,\"start\":56559},{\"end\":57369,\"start\":57320},{\"end\":57776,\"start\":57716},{\"end\":58268,\"start\":58157},{\"end\":58422,\"start\":58361}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19786,\"start\":19780},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23325,\"start\":23318},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25422,\"start\":25412},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27084,\"start\":27074},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27358,\"start\":27352},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28354,\"start\":28344},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31442,\"start\":31435},{\"end\":31746,\"start\":31740},{\"end\":32007,\"start\":32001},{\"end\":32503,\"start\":32497},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39690,\"start\":39682},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39953,\"start\":39947},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41090,\"start\":41084},{\"end\":41101,\"start\":41095},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41199,\"start\":41193},{\"end\":41766,\"start\":41760},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":42347,\"start\":42341},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":43180,\"start\":43174},{\"end\":44974,\"start\":44968},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":46551,\"start\":46544},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48059,\"start\":48052},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48324,\"start\":48317},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":49440,\"start\":49433},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":50545,\"start\":50536}]", "bib_author_first_name": "[{\"end\":58938,\"start\":58937},{\"end\":58945,\"start\":58944},{\"end\":58950,\"start\":58946},{\"end\":58961,\"start\":58956},{\"end\":59333,\"start\":59332},{\"end\":59340,\"start\":59339},{\"end\":59347,\"start\":59346},{\"end\":59355,\"start\":59354},{\"end\":59718,\"start\":59717},{\"end\":59727,\"start\":59726},{\"end\":59734,\"start\":59733},{\"end\":59740,\"start\":59739},{\"end\":60087,\"start\":60086},{\"end\":60098,\"start\":60097},{\"end\":60347,\"start\":60346},{\"end\":60357,\"start\":60356},{\"end\":60366,\"start\":60365},{\"end\":60379,\"start\":60378},{\"end\":60381,\"start\":60380},{\"end\":60391,\"start\":60390},{\"end\":60393,\"start\":60392},{\"end\":60636,\"start\":60635},{\"end\":60942,\"start\":60938},{\"end\":60949,\"start\":60948},{\"end\":60956,\"start\":60955},{\"end\":60958,\"start\":60957},{\"end\":61262,\"start\":61261},{\"end\":61271,\"start\":61270},{\"end\":61280,\"start\":61279},{\"end\":61290,\"start\":61289},{\"end\":61292,\"start\":61291},{\"end\":61625,\"start\":61624},{\"end\":61630,\"start\":61626},{\"end\":61637,\"start\":61636},{\"end\":61648,\"start\":61643},{\"end\":61888,\"start\":61887},{\"end\":61895,\"start\":61891},{\"end\":61906,\"start\":61901},{\"end\":62224,\"start\":62223},{\"end\":62231,\"start\":62230},{\"end\":62242,\"start\":62241},{\"end\":62249,\"start\":62248},{\"end\":62592,\"start\":62591},{\"end\":62601,\"start\":62600},{\"end\":62610,\"start\":62609},{\"end\":62612,\"start\":62611},{\"end\":62623,\"start\":62622},{\"end\":62632,\"start\":62631},{\"end\":62927,\"start\":62926},{\"end\":62936,\"start\":62935},{\"end\":62945,\"start\":62944},{\"end\":62958,\"start\":62957},{\"end\":63222,\"start\":63221},{\"end\":63231,\"start\":63230},{\"end\":63237,\"start\":63236},{\"end\":63523,\"start\":63522},{\"end\":63531,\"start\":63530},{\"end\":63539,\"start\":63538},{\"end\":63546,\"start\":63545},{\"end\":63555,\"start\":63554},{\"end\":63563,\"start\":63562},{\"end\":63571,\"start\":63570},{\"end\":63582,\"start\":63581},{\"end\":64022,\"start\":64021},{\"end\":64029,\"start\":64028},{\"end\":64036,\"start\":64035},{\"end\":64042,\"start\":64041},{\"end\":64372,\"start\":64371},{\"end\":64378,\"start\":64377},{\"end\":64391,\"start\":64390},{\"end\":64399,\"start\":64398},{\"end\":64409,\"start\":64408},{\"end\":64421,\"start\":64420},{\"end\":64778,\"start\":64777},{\"end\":64785,\"start\":64784},{\"end\":64798,\"start\":64797},{\"end\":65078,\"start\":65077},{\"end\":65089,\"start\":65088},{\"end\":65097,\"start\":65096},{\"end\":65412,\"start\":65411},{\"end\":65420,\"start\":65419},{\"end\":65426,\"start\":65425},{\"end\":65438,\"start\":65434},{\"end\":65445,\"start\":65444},{\"end\":65462,\"start\":65461},{\"end\":65804,\"start\":65803},{\"end\":65811,\"start\":65810},{\"end\":65820,\"start\":65819},{\"end\":65833,\"start\":65832},{\"end\":65844,\"start\":65843},{\"end\":66101,\"start\":66100},{\"end\":66108,\"start\":66107},{\"end\":66115,\"start\":66114},{\"end\":66123,\"start\":66122},{\"end\":66131,\"start\":66130},{\"end\":66380,\"start\":66379},{\"end\":66387,\"start\":66386},{\"end\":66395,\"start\":66394},{\"end\":66397,\"start\":66396},{\"end\":66408,\"start\":66407},{\"end\":66698,\"start\":66697},{\"end\":66706,\"start\":66705},{\"end\":66714,\"start\":66713},{\"end\":66722,\"start\":66721},{\"end\":67046,\"start\":67045},{\"end\":67054,\"start\":67053},{\"end\":67061,\"start\":67060},{\"end\":67258,\"start\":67257},{\"end\":67265,\"start\":67264},{\"end\":67449,\"start\":67448},{\"end\":67455,\"start\":67454},{\"end\":67747,\"start\":67746},{\"end\":67754,\"start\":67753},{\"end\":67760,\"start\":67759},{\"end\":67769,\"start\":67765},{\"end\":68061,\"start\":68060},{\"end\":68068,\"start\":68067},{\"end\":68075,\"start\":68074},{\"end\":68089,\"start\":68085},{\"end\":68408,\"start\":68407},{\"end\":68414,\"start\":68413},{\"end\":68420,\"start\":68419},{\"end\":68428,\"start\":68427},{\"end\":68437,\"start\":68436},{\"end\":68692,\"start\":68691},{\"end\":68694,\"start\":68693},{\"end\":68703,\"start\":68702},{\"end\":68946,\"start\":68945},{\"end\":68955,\"start\":68954},{\"end\":68964,\"start\":68963},{\"end\":69256,\"start\":69255},{\"end\":69263,\"start\":69262},{\"end\":69269,\"start\":69268},{\"end\":69279,\"start\":69274},{\"end\":69289,\"start\":69285},{\"end\":69599,\"start\":69598},{\"end\":69606,\"start\":69605},{\"end\":69612,\"start\":69611},{\"end\":69934,\"start\":69933},{\"end\":69943,\"start\":69942},{\"end\":69945,\"start\":69944},{\"end\":70178,\"start\":70177},{\"end\":70191,\"start\":70190},{\"end\":70200,\"start\":70199},{\"end\":70210,\"start\":70209},{\"end\":70459,\"start\":70458},{\"end\":70461,\"start\":70460},{\"end\":70472,\"start\":70471},{\"end\":70482,\"start\":70481},{\"end\":70495,\"start\":70494},{\"end\":70754,\"start\":70753},{\"end\":71031,\"start\":71030},{\"end\":71040,\"start\":71039},{\"end\":71050,\"start\":71049},{\"end\":71064,\"start\":71063},{\"end\":71075,\"start\":71074},{\"end\":71427,\"start\":71426},{\"end\":71436,\"start\":71435},{\"end\":71443,\"start\":71442},{\"end\":71450,\"start\":71449},{\"end\":71458,\"start\":71457},{\"end\":71465,\"start\":71464},{\"end\":71467,\"start\":71466},{\"end\":71477,\"start\":71473},{\"end\":71835,\"start\":71834},{\"end\":71842,\"start\":71841},{\"end\":71849,\"start\":71848},{\"end\":71851,\"start\":71850},{\"end\":72202,\"start\":72201},{\"end\":72210,\"start\":72209},{\"end\":72212,\"start\":72211},{\"end\":72220,\"start\":72219},{\"end\":72226,\"start\":72225},{\"end\":72242,\"start\":72233},{\"end\":72616,\"start\":72615},{\"end\":72624,\"start\":72623},{\"end\":72630,\"start\":72629},{\"end\":72637,\"start\":72636},{\"end\":72899,\"start\":72898},{\"end\":72907,\"start\":72906},{\"end\":72918,\"start\":72917},{\"end\":73333,\"start\":73332},{\"end\":73341,\"start\":73340},{\"end\":73347,\"start\":73346},{\"end\":73706,\"start\":73705},{\"end\":73722,\"start\":73721},{\"end\":73733,\"start\":73732},{\"end\":73749,\"start\":73748},{\"end\":74064,\"start\":74063},{\"end\":74073,\"start\":74072},{\"end\":74080,\"start\":74079},{\"end\":74091,\"start\":74090},{\"end\":74098,\"start\":74097},{\"end\":74104,\"start\":74103},{\"end\":74425,\"start\":74424},{\"end\":74433,\"start\":74432},{\"end\":74442,\"start\":74441},{\"end\":74452,\"start\":74451},{\"end\":74731,\"start\":74730},{\"end\":74738,\"start\":74737},{\"end\":74745,\"start\":74744},{\"end\":74755,\"start\":74751},{\"end\":75036,\"start\":75035},{\"end\":75042,\"start\":75041},{\"end\":75051,\"start\":75050},{\"end\":75058,\"start\":75057},{\"end\":75068,\"start\":75064},{\"end\":75333,\"start\":75332},{\"end\":75340,\"start\":75339},{\"end\":75346,\"start\":75345},{\"end\":75355,\"start\":75354},{\"end\":75361,\"start\":75360},{\"end\":75370,\"start\":75369},{\"end\":75376,\"start\":75375},{\"end\":75646,\"start\":75645},{\"end\":75648,\"start\":75647},{\"end\":75658,\"start\":75657},{\"end\":75879,\"start\":75878},{\"end\":75889,\"start\":75888},{\"end\":76356,\"start\":76355},{\"end\":76368,\"start\":76367},{\"end\":76377,\"start\":76376},{\"end\":76388,\"start\":76387},{\"end\":76688,\"start\":76687},{\"end\":76694,\"start\":76693},{\"end\":76703,\"start\":76702},{\"end\":77034,\"start\":77033},{\"end\":77043,\"start\":77042},{\"end\":77056,\"start\":77055},{\"end\":77062,\"start\":77061},{\"end\":77377,\"start\":77376},{\"end\":77384,\"start\":77383},{\"end\":77395,\"start\":77391},{\"end\":77402,\"start\":77401},{\"end\":77677,\"start\":77676}]", "bib_author_last_name": "[{\"end\":58942,\"start\":58939},{\"end\":58954,\"start\":58951},{\"end\":58965,\"start\":58962},{\"end\":59337,\"start\":59334},{\"end\":59344,\"start\":59341},{\"end\":59352,\"start\":59348},{\"end\":59359,\"start\":59356},{\"end\":59724,\"start\":59719},{\"end\":59731,\"start\":59728},{\"end\":59737,\"start\":59735},{\"end\":59748,\"start\":59741},{\"end\":60095,\"start\":60088},{\"end\":60110,\"start\":60099},{\"end\":60354,\"start\":60348},{\"end\":60363,\"start\":60358},{\"end\":60376,\"start\":60367},{\"end\":60388,\"start\":60382},{\"end\":60401,\"start\":60394},{\"end\":60640,\"start\":60637},{\"end\":60946,\"start\":60943},{\"end\":60953,\"start\":60950},{\"end\":60964,\"start\":60959},{\"end\":61268,\"start\":61263},{\"end\":61277,\"start\":61272},{\"end\":61287,\"start\":61281},{\"end\":61300,\"start\":61293},{\"end\":61634,\"start\":61631},{\"end\":61641,\"start\":61638},{\"end\":61652,\"start\":61649},{\"end\":61899,\"start\":61896},{\"end\":61910,\"start\":61907},{\"end\":62228,\"start\":62225},{\"end\":62239,\"start\":62232},{\"end\":62246,\"start\":62243},{\"end\":62253,\"start\":62250},{\"end\":62598,\"start\":62593},{\"end\":62607,\"start\":62602},{\"end\":62620,\"start\":62613},{\"end\":62629,\"start\":62624},{\"end\":62640,\"start\":62633},{\"end\":62933,\"start\":62928},{\"end\":62942,\"start\":62937},{\"end\":62955,\"start\":62946},{\"end\":62964,\"start\":62959},{\"end\":63228,\"start\":63223},{\"end\":63234,\"start\":63232},{\"end\":63241,\"start\":63238},{\"end\":63528,\"start\":63524},{\"end\":63536,\"start\":63532},{\"end\":63543,\"start\":63540},{\"end\":63552,\"start\":63547},{\"end\":63560,\"start\":63556},{\"end\":63568,\"start\":63564},{\"end\":63579,\"start\":63572},{\"end\":63589,\"start\":63583},{\"end\":63594,\"start\":63591},{\"end\":64026,\"start\":64023},{\"end\":64033,\"start\":64030},{\"end\":64039,\"start\":64037},{\"end\":64048,\"start\":64043},{\"end\":64375,\"start\":64373},{\"end\":64388,\"start\":64379},{\"end\":64396,\"start\":64392},{\"end\":64406,\"start\":64400},{\"end\":64418,\"start\":64410},{\"end\":64426,\"start\":64422},{\"end\":64782,\"start\":64779},{\"end\":64795,\"start\":64786},{\"end\":64805,\"start\":64799},{\"end\":65086,\"start\":65079},{\"end\":65094,\"start\":65090},{\"end\":65109,\"start\":65098},{\"end\":65417,\"start\":65413},{\"end\":65423,\"start\":65421},{\"end\":65432,\"start\":65427},{\"end\":65442,\"start\":65439},{\"end\":65459,\"start\":65446},{\"end\":65468,\"start\":65463},{\"end\":65808,\"start\":65805},{\"end\":65817,\"start\":65812},{\"end\":65830,\"start\":65821},{\"end\":65841,\"start\":65834},{\"end\":65851,\"start\":65845},{\"end\":66105,\"start\":66102},{\"end\":66112,\"start\":66109},{\"end\":66120,\"start\":66116},{\"end\":66128,\"start\":66124},{\"end\":66135,\"start\":66132},{\"end\":66384,\"start\":66381},{\"end\":66392,\"start\":66388},{\"end\":66405,\"start\":66398},{\"end\":66412,\"start\":66409},{\"end\":66703,\"start\":66699},{\"end\":66711,\"start\":66707},{\"end\":66719,\"start\":66715},{\"end\":66728,\"start\":66723},{\"end\":67051,\"start\":67047},{\"end\":67058,\"start\":67055},{\"end\":67070,\"start\":67062},{\"end\":67262,\"start\":67259},{\"end\":67269,\"start\":67266},{\"end\":67452,\"start\":67450},{\"end\":67459,\"start\":67456},{\"end\":67751,\"start\":67748},{\"end\":67757,\"start\":67755},{\"end\":67763,\"start\":67761},{\"end\":67774,\"start\":67770},{\"end\":68065,\"start\":68062},{\"end\":68072,\"start\":68069},{\"end\":68083,\"start\":68076},{\"end\":68094,\"start\":68090},{\"end\":68411,\"start\":68409},{\"end\":68417,\"start\":68415},{\"end\":68425,\"start\":68421},{\"end\":68434,\"start\":68429},{\"end\":68440,\"start\":68438},{\"end\":68700,\"start\":68695},{\"end\":68712,\"start\":68704},{\"end\":68952,\"start\":68947},{\"end\":68961,\"start\":68956},{\"end\":68974,\"start\":68965},{\"end\":69260,\"start\":69257},{\"end\":69266,\"start\":69264},{\"end\":69272,\"start\":69270},{\"end\":69283,\"start\":69280},{\"end\":69294,\"start\":69290},{\"end\":69603,\"start\":69600},{\"end\":69609,\"start\":69607},{\"end\":69616,\"start\":69613},{\"end\":69940,\"start\":69935},{\"end\":69955,\"start\":69946},{\"end\":70188,\"start\":70179},{\"end\":70197,\"start\":70192},{\"end\":70207,\"start\":70201},{\"end\":70217,\"start\":70211},{\"end\":70469,\"start\":70462},{\"end\":70479,\"start\":70473},{\"end\":70492,\"start\":70483},{\"end\":70505,\"start\":70496},{\"end\":70766,\"start\":70755},{\"end\":71037,\"start\":71032},{\"end\":71047,\"start\":71041},{\"end\":71061,\"start\":71051},{\"end\":71072,\"start\":71065},{\"end\":71081,\"start\":71076},{\"end\":71433,\"start\":71428},{\"end\":71440,\"start\":71437},{\"end\":71447,\"start\":71444},{\"end\":71455,\"start\":71451},{\"end\":71462,\"start\":71459},{\"end\":71471,\"start\":71468},{\"end\":71482,\"start\":71478},{\"end\":71839,\"start\":71836},{\"end\":71846,\"start\":71843},{\"end\":71855,\"start\":71852},{\"end\":72207,\"start\":72203},{\"end\":72217,\"start\":72213},{\"end\":72223,\"start\":72221},{\"end\":72231,\"start\":72227},{\"end\":72246,\"start\":72243},{\"end\":72621,\"start\":72617},{\"end\":72627,\"start\":72625},{\"end\":72634,\"start\":72631},{\"end\":72641,\"start\":72638},{\"end\":72904,\"start\":72900},{\"end\":72915,\"start\":72908},{\"end\":72930,\"start\":72919},{\"end\":73338,\"start\":73334},{\"end\":73344,\"start\":73342},{\"end\":73350,\"start\":73348},{\"end\":73719,\"start\":73707},{\"end\":73730,\"start\":73723},{\"end\":73746,\"start\":73734},{\"end\":73755,\"start\":73750},{\"end\":74070,\"start\":74065},{\"end\":74077,\"start\":74074},{\"end\":74088,\"start\":74081},{\"end\":74095,\"start\":74092},{\"end\":74101,\"start\":74099},{\"end\":74107,\"start\":74105},{\"end\":74430,\"start\":74426},{\"end\":74439,\"start\":74434},{\"end\":74449,\"start\":74443},{\"end\":74458,\"start\":74453},{\"end\":74735,\"start\":74732},{\"end\":74742,\"start\":74739},{\"end\":74749,\"start\":74746},{\"end\":74760,\"start\":74756},{\"end\":75039,\"start\":75037},{\"end\":75048,\"start\":75043},{\"end\":75055,\"start\":75052},{\"end\":75062,\"start\":75059},{\"end\":75073,\"start\":75069},{\"end\":75337,\"start\":75334},{\"end\":75343,\"start\":75341},{\"end\":75352,\"start\":75347},{\"end\":75358,\"start\":75356},{\"end\":75367,\"start\":75362},{\"end\":75373,\"start\":75371},{\"end\":75380,\"start\":75377},{\"end\":75655,\"start\":75649},{\"end\":75661,\"start\":75659},{\"end\":75886,\"start\":75880},{\"end\":75896,\"start\":75890},{\"end\":76365,\"start\":76357},{\"end\":76374,\"start\":76369},{\"end\":76385,\"start\":76378},{\"end\":76394,\"start\":76389},{\"end\":76691,\"start\":76689},{\"end\":76700,\"start\":76695},{\"end\":76707,\"start\":76704},{\"end\":77040,\"start\":77035},{\"end\":77053,\"start\":77044},{\"end\":77059,\"start\":77057},{\"end\":77067,\"start\":77063},{\"end\":77381,\"start\":77378},{\"end\":77389,\"start\":77385},{\"end\":77399,\"start\":77396},{\"end\":77408,\"start\":77403},{\"end\":77681,\"start\":77678}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8671030},\"end\":59243,\"start\":58861},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":198185751},\"end\":59647,\"start\":59245},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":102351795},\"end\":60021,\"start\":59649},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":202719293},\"end\":60296,\"start\":60023},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":40060575},\"end\":60582,\"start\":60298},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2103371},\"end\":60866,\"start\":60584},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":591674},\"end\":61192,\"start\":60868},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1212427},\"end\":61596,\"start\":61194},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2667927},\"end\":61841,\"start\":61598},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15749523},\"end\":62145,\"start\":61843},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":53729092},\"end\":62533,\"start\":62147},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":18417477},\"end\":62882,\"start\":62535},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10233982},\"end\":63192,\"start\":62884},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7657380},\"end\":63422,\"start\":63194},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":18123440},\"end\":63942,\"start\":63424},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1485453},\"end\":64324,\"start\":63944},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5872410},\"end\":64702,\"start\":64326},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4784271},\"end\":65043,\"start\":64704},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4708732},\"end\":65346,\"start\":65045},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":33017868},\"end\":65760,\"start\":65348},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":211076323},\"end\":66047,\"start\":65762},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3635375},\"end\":66360,\"start\":66049},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":164905317},\"end\":66637,\"start\":66362},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":198167039},\"end\":66991,\"start\":66639},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":7114206},\"end\":67231,\"start\":66993},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14206347},\"end\":67388,\"start\":67233},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8000561},\"end\":67660,\"start\":67390},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4048268},\"end\":68016,\"start\":67662},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13981820},\"end\":68304,\"start\":68018},{\"attributes\":{\"doi\":\"arXiv:2105.04104\",\"id\":\"b29\"},\"end\":68657,\"start\":68306},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":646197},\"end\":68893,\"start\":68659},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2627888},\"end\":69202,\"start\":68895},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":12311305},\"end\":69556,\"start\":69204},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15311395},\"end\":69847,\"start\":69558},{\"attributes\":{\"id\":\"b34\"},\"end\":70111,\"start\":69849},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":5838887},\"end\":70436,\"start\":70113},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3881811},\"end\":70705,\"start\":70438},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14605354},\"end\":70953,\"start\":70707},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4552226},\"end\":71348,\"start\":70955},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":206596913},\"end\":71756,\"start\":71350},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":195431548},\"end\":72126,\"start\":71758},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":146808596},\"end\":72567,\"start\":72128},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":3662488},\"end\":72817,\"start\":72569},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":215745207},\"end\":73214,\"start\":72819},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":219628624},\"end\":73657,\"start\":73216},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":27027885},\"end\":73986,\"start\":73659},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":222278584},\"end\":74363,\"start\":73988},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":5334482},\"end\":74651,\"start\":74365},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":10109210},\"end\":75002,\"start\":74653},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":202782364},\"end\":75295,\"start\":75004},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":4028864},\"end\":75599,\"start\":75297},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b51\"},\"end\":75801,\"start\":75601},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":5575601},\"end\":76298,\"start\":75803},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206764694},\"end\":76620,\"start\":76300},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":1916689},\"end\":76965,\"start\":76622},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":199543931},\"end\":77302,\"start\":76967},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":30824366},\"end\":77650,\"start\":77304},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":778478},\"end\":77891,\"start\":77652}]", "bib_title": "[{\"end\":58935,\"start\":58861},{\"end\":59330,\"start\":59245},{\"end\":59715,\"start\":59649},{\"end\":60084,\"start\":60023},{\"end\":60344,\"start\":60298},{\"end\":60633,\"start\":60584},{\"end\":60936,\"start\":60868},{\"end\":61259,\"start\":61194},{\"end\":61622,\"start\":61598},{\"end\":61885,\"start\":61843},{\"end\":62221,\"start\":62147},{\"end\":62589,\"start\":62535},{\"end\":62924,\"start\":62884},{\"end\":63219,\"start\":63194},{\"end\":63520,\"start\":63424},{\"end\":64019,\"start\":63944},{\"end\":64369,\"start\":64326},{\"end\":64775,\"start\":64704},{\"end\":65075,\"start\":65045},{\"end\":65409,\"start\":65348},{\"end\":65801,\"start\":65762},{\"end\":66098,\"start\":66049},{\"end\":66377,\"start\":66362},{\"end\":66695,\"start\":66639},{\"end\":67043,\"start\":66993},{\"end\":67255,\"start\":67233},{\"end\":67446,\"start\":67390},{\"end\":67744,\"start\":67662},{\"end\":68058,\"start\":68018},{\"end\":68689,\"start\":68659},{\"end\":68943,\"start\":68895},{\"end\":69253,\"start\":69204},{\"end\":69596,\"start\":69558},{\"end\":69931,\"start\":69849},{\"end\":70175,\"start\":70113},{\"end\":70456,\"start\":70438},{\"end\":70751,\"start\":70707},{\"end\":71028,\"start\":70955},{\"end\":71424,\"start\":71350},{\"end\":71832,\"start\":71758},{\"end\":72199,\"start\":72128},{\"end\":72613,\"start\":72569},{\"end\":72896,\"start\":72819},{\"end\":73330,\"start\":73216},{\"end\":73703,\"start\":73659},{\"end\":74061,\"start\":73988},{\"end\":74422,\"start\":74365},{\"end\":74728,\"start\":74653},{\"end\":75033,\"start\":75004},{\"end\":75330,\"start\":75297},{\"end\":75876,\"start\":75803},{\"end\":76353,\"start\":76300},{\"end\":76685,\"start\":76622},{\"end\":77031,\"start\":76967},{\"end\":77374,\"start\":77304},{\"end\":77674,\"start\":77652}]", "bib_author": "[{\"end\":58944,\"start\":58937},{\"end\":58956,\"start\":58944},{\"end\":58967,\"start\":58956},{\"end\":59339,\"start\":59332},{\"end\":59346,\"start\":59339},{\"end\":59354,\"start\":59346},{\"end\":59361,\"start\":59354},{\"end\":59726,\"start\":59717},{\"end\":59733,\"start\":59726},{\"end\":59739,\"start\":59733},{\"end\":59750,\"start\":59739},{\"end\":60097,\"start\":60086},{\"end\":60112,\"start\":60097},{\"end\":60356,\"start\":60346},{\"end\":60365,\"start\":60356},{\"end\":60378,\"start\":60365},{\"end\":60390,\"start\":60378},{\"end\":60403,\"start\":60390},{\"end\":60642,\"start\":60635},{\"end\":60948,\"start\":60938},{\"end\":60955,\"start\":60948},{\"end\":60966,\"start\":60955},{\"end\":61270,\"start\":61261},{\"end\":61279,\"start\":61270},{\"end\":61289,\"start\":61279},{\"end\":61302,\"start\":61289},{\"end\":61636,\"start\":61624},{\"end\":61643,\"start\":61636},{\"end\":61654,\"start\":61643},{\"end\":61891,\"start\":61887},{\"end\":61901,\"start\":61891},{\"end\":61912,\"start\":61901},{\"end\":62230,\"start\":62223},{\"end\":62241,\"start\":62230},{\"end\":62248,\"start\":62241},{\"end\":62255,\"start\":62248},{\"end\":62600,\"start\":62591},{\"end\":62609,\"start\":62600},{\"end\":62622,\"start\":62609},{\"end\":62631,\"start\":62622},{\"end\":62642,\"start\":62631},{\"end\":62935,\"start\":62926},{\"end\":62944,\"start\":62935},{\"end\":62957,\"start\":62944},{\"end\":62966,\"start\":62957},{\"end\":63230,\"start\":63221},{\"end\":63236,\"start\":63230},{\"end\":63243,\"start\":63236},{\"end\":63530,\"start\":63522},{\"end\":63538,\"start\":63530},{\"end\":63545,\"start\":63538},{\"end\":63554,\"start\":63545},{\"end\":63562,\"start\":63554},{\"end\":63570,\"start\":63562},{\"end\":63581,\"start\":63570},{\"end\":63591,\"start\":63581},{\"end\":63596,\"start\":63591},{\"end\":64028,\"start\":64021},{\"end\":64035,\"start\":64028},{\"end\":64041,\"start\":64035},{\"end\":64050,\"start\":64041},{\"end\":64377,\"start\":64371},{\"end\":64390,\"start\":64377},{\"end\":64398,\"start\":64390},{\"end\":64408,\"start\":64398},{\"end\":64420,\"start\":64408},{\"end\":64428,\"start\":64420},{\"end\":64784,\"start\":64777},{\"end\":64797,\"start\":64784},{\"end\":64807,\"start\":64797},{\"end\":65088,\"start\":65077},{\"end\":65096,\"start\":65088},{\"end\":65111,\"start\":65096},{\"end\":65419,\"start\":65411},{\"end\":65425,\"start\":65419},{\"end\":65434,\"start\":65425},{\"end\":65444,\"start\":65434},{\"end\":65461,\"start\":65444},{\"end\":65470,\"start\":65461},{\"end\":65810,\"start\":65803},{\"end\":65819,\"start\":65810},{\"end\":65832,\"start\":65819},{\"end\":65843,\"start\":65832},{\"end\":65853,\"start\":65843},{\"end\":66107,\"start\":66100},{\"end\":66114,\"start\":66107},{\"end\":66122,\"start\":66114},{\"end\":66130,\"start\":66122},{\"end\":66137,\"start\":66130},{\"end\":66386,\"start\":66379},{\"end\":66394,\"start\":66386},{\"end\":66407,\"start\":66394},{\"end\":66414,\"start\":66407},{\"end\":66705,\"start\":66697},{\"end\":66713,\"start\":66705},{\"end\":66721,\"start\":66713},{\"end\":66730,\"start\":66721},{\"end\":67053,\"start\":67045},{\"end\":67060,\"start\":67053},{\"end\":67072,\"start\":67060},{\"end\":67264,\"start\":67257},{\"end\":67271,\"start\":67264},{\"end\":67454,\"start\":67448},{\"end\":67461,\"start\":67454},{\"end\":67753,\"start\":67746},{\"end\":67759,\"start\":67753},{\"end\":67765,\"start\":67759},{\"end\":67776,\"start\":67765},{\"end\":68067,\"start\":68060},{\"end\":68074,\"start\":68067},{\"end\":68085,\"start\":68074},{\"end\":68096,\"start\":68085},{\"end\":68413,\"start\":68407},{\"end\":68419,\"start\":68413},{\"end\":68427,\"start\":68419},{\"end\":68436,\"start\":68427},{\"end\":68442,\"start\":68436},{\"end\":68702,\"start\":68691},{\"end\":68714,\"start\":68702},{\"end\":68954,\"start\":68945},{\"end\":68963,\"start\":68954},{\"end\":68976,\"start\":68963},{\"end\":69262,\"start\":69255},{\"end\":69268,\"start\":69262},{\"end\":69274,\"start\":69268},{\"end\":69285,\"start\":69274},{\"end\":69296,\"start\":69285},{\"end\":69605,\"start\":69598},{\"end\":69611,\"start\":69605},{\"end\":69618,\"start\":69611},{\"end\":69942,\"start\":69933},{\"end\":69957,\"start\":69942},{\"end\":70190,\"start\":70177},{\"end\":70199,\"start\":70190},{\"end\":70209,\"start\":70199},{\"end\":70219,\"start\":70209},{\"end\":70471,\"start\":70458},{\"end\":70481,\"start\":70471},{\"end\":70494,\"start\":70481},{\"end\":70507,\"start\":70494},{\"end\":70768,\"start\":70753},{\"end\":71039,\"start\":71030},{\"end\":71049,\"start\":71039},{\"end\":71063,\"start\":71049},{\"end\":71074,\"start\":71063},{\"end\":71083,\"start\":71074},{\"end\":71435,\"start\":71426},{\"end\":71442,\"start\":71435},{\"end\":71449,\"start\":71442},{\"end\":71457,\"start\":71449},{\"end\":71464,\"start\":71457},{\"end\":71473,\"start\":71464},{\"end\":71484,\"start\":71473},{\"end\":71841,\"start\":71834},{\"end\":71848,\"start\":71841},{\"end\":71857,\"start\":71848},{\"end\":72209,\"start\":72201},{\"end\":72219,\"start\":72209},{\"end\":72225,\"start\":72219},{\"end\":72233,\"start\":72225},{\"end\":72248,\"start\":72233},{\"end\":72623,\"start\":72615},{\"end\":72629,\"start\":72623},{\"end\":72636,\"start\":72629},{\"end\":72643,\"start\":72636},{\"end\":72906,\"start\":72898},{\"end\":72917,\"start\":72906},{\"end\":72932,\"start\":72917},{\"end\":73340,\"start\":73332},{\"end\":73346,\"start\":73340},{\"end\":73352,\"start\":73346},{\"end\":73721,\"start\":73705},{\"end\":73732,\"start\":73721},{\"end\":73748,\"start\":73732},{\"end\":73757,\"start\":73748},{\"end\":74072,\"start\":74063},{\"end\":74079,\"start\":74072},{\"end\":74090,\"start\":74079},{\"end\":74097,\"start\":74090},{\"end\":74103,\"start\":74097},{\"end\":74109,\"start\":74103},{\"end\":74432,\"start\":74424},{\"end\":74441,\"start\":74432},{\"end\":74451,\"start\":74441},{\"end\":74460,\"start\":74451},{\"end\":74737,\"start\":74730},{\"end\":74744,\"start\":74737},{\"end\":74751,\"start\":74744},{\"end\":74762,\"start\":74751},{\"end\":75041,\"start\":75035},{\"end\":75050,\"start\":75041},{\"end\":75057,\"start\":75050},{\"end\":75064,\"start\":75057},{\"end\":75075,\"start\":75064},{\"end\":75339,\"start\":75332},{\"end\":75345,\"start\":75339},{\"end\":75354,\"start\":75345},{\"end\":75360,\"start\":75354},{\"end\":75369,\"start\":75360},{\"end\":75375,\"start\":75369},{\"end\":75382,\"start\":75375},{\"end\":75657,\"start\":75645},{\"end\":75663,\"start\":75657},{\"end\":75888,\"start\":75878},{\"end\":75898,\"start\":75888},{\"end\":76367,\"start\":76355},{\"end\":76376,\"start\":76367},{\"end\":76387,\"start\":76376},{\"end\":76396,\"start\":76387},{\"end\":76693,\"start\":76687},{\"end\":76702,\"start\":76693},{\"end\":76709,\"start\":76702},{\"end\":77042,\"start\":77033},{\"end\":77055,\"start\":77042},{\"end\":77061,\"start\":77055},{\"end\":77069,\"start\":77061},{\"end\":77383,\"start\":77376},{\"end\":77391,\"start\":77383},{\"end\":77401,\"start\":77391},{\"end\":77410,\"start\":77401},{\"end\":77683,\"start\":77676}]", "bib_venue": "[{\"end\":59055,\"start\":59015},{\"end\":59449,\"start\":59409},{\"end\":59838,\"start\":59798},{\"end\":60730,\"start\":60690},{\"end\":61390,\"start\":61350},{\"end\":61716,\"start\":61689},{\"end\":62000,\"start\":61960},{\"end\":62343,\"start\":62303},{\"end\":62694,\"start\":62672},{\"end\":63028,\"start\":63001},{\"end\":63305,\"start\":63278},{\"end\":63684,\"start\":63644},{\"end\":64138,\"start\":64098},{\"end\":64516,\"start\":64476},{\"end\":64859,\"start\":64837},{\"end\":65199,\"start\":65159},{\"end\":65560,\"start\":65519},{\"end\":66189,\"start\":66167},{\"end\":66502,\"start\":66462},{\"end\":66818,\"start\":66778},{\"end\":67513,\"start\":67491},{\"end\":69038,\"start\":69011},{\"end\":69384,\"start\":69344},{\"end\":69706,\"start\":69666},{\"end\":70820,\"start\":70798},{\"end\":71135,\"start\":71113},{\"end\":71536,\"start\":71514},{\"end\":71945,\"start\":71905},{\"end\":72358,\"start\":72307},{\"end\":73020,\"start\":72980},{\"end\":73440,\"start\":73400},{\"end\":73819,\"start\":73792},{\"end\":74177,\"start\":74147},{\"end\":74824,\"start\":74797},{\"end\":75147,\"start\":75117},{\"end\":75444,\"start\":75417},{\"end\":76077,\"start\":75996},{\"end\":76797,\"start\":76757},{\"end\":77131,\"start\":77104},{\"end\":77462,\"start\":77440},{\"end\":77771,\"start\":77731},{\"end\":59013,\"start\":58967},{\"end\":59407,\"start\":59361},{\"end\":59796,\"start\":59750},{\"end\":60140,\"start\":60112},{\"end\":60415,\"start\":60403},{\"end\":60688,\"start\":60642},{\"end\":61003,\"start\":60966},{\"end\":61348,\"start\":61302},{\"end\":61687,\"start\":61654},{\"end\":61958,\"start\":61912},{\"end\":62301,\"start\":62255},{\"end\":62670,\"start\":62642},{\"end\":62999,\"start\":62966},{\"end\":63276,\"start\":63243},{\"end\":63642,\"start\":63596},{\"end\":64096,\"start\":64050},{\"end\":64474,\"start\":64428},{\"end\":64835,\"start\":64807},{\"end\":65157,\"start\":65111},{\"end\":65517,\"start\":65470},{\"end\":65877,\"start\":65853},{\"end\":66165,\"start\":66137},{\"end\":66460,\"start\":66414},{\"end\":66776,\"start\":66730},{\"end\":67088,\"start\":67072},{\"end\":67287,\"start\":67271},{\"end\":67489,\"start\":67461},{\"end\":67813,\"start\":67776},{\"end\":68133,\"start\":68096},{\"end\":68405,\"start\":68306},{\"end\":68751,\"start\":68714},{\"end\":69009,\"start\":68976},{\"end\":69342,\"start\":69296},{\"end\":69664,\"start\":69618},{\"end\":69962,\"start\":69957},{\"end\":70247,\"start\":70219},{\"end\":70544,\"start\":70507},{\"end\":70796,\"start\":70768},{\"end\":71111,\"start\":71083},{\"end\":71512,\"start\":71484},{\"end\":71903,\"start\":71857},{\"end\":72305,\"start\":72248},{\"end\":72667,\"start\":72643},{\"end\":72978,\"start\":72932},{\"end\":73398,\"start\":73352},{\"end\":73790,\"start\":73757},{\"end\":74145,\"start\":74109},{\"end\":74484,\"start\":74460},{\"end\":74795,\"start\":74762},{\"end\":75115,\"start\":75075},{\"end\":75415,\"start\":75382},{\"end\":75643,\"start\":75601},{\"end\":75994,\"start\":75898},{\"end\":76433,\"start\":76396},{\"end\":76755,\"start\":76709},{\"end\":77102,\"start\":77069},{\"end\":77438,\"start\":77410},{\"end\":77729,\"start\":77683}]"}}}, "year": 2023, "month": 12, "day": 17}