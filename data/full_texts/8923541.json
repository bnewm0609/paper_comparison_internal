{"id": 8923541, "updated": "2023-10-07 11:20:30.506", "metadata": {"title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition", "authors": "[{\"first\":\"Jiankang\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"Jia\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Niannan\",\"last\":\"Xue\",\"middle\":[]},{\"first\":\"Stefanos\",\"last\":\"Zafeiriou\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 6, "day": 1}, "abstract": "One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that can enhance the discriminative power. Centre loss penalises the distance between deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in the angular space and therefore penalises the angles between deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to its exact correspondence to geodesic distance on a hypersphere. We present arguably the most extensive experimental evaluation against all recent state-of-the-art face recognition methods on ten face recognition benchmarks which includes a new large-scale image database with trillions of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state of the art and can be easily implemented with negligible computational overhead. To facilitate future research, the code has been made available.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2969985801", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/DengGXZ19", "doi": "10.1109/cvpr.2019.00482"}}, "content": {"source": {"pdf_hash": "74119d6d0c2a3d89ad0f699857447241f4ba22b3", "pdf_src": "IEEE", "pdf_uri": "[\"https://export.arxiv.org/pdf/1801.07698v4.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1801.07698", "status": "GREEN"}}, "grobid": {"id": "b9dd119e6e94e21f703fc9fd7db2c130d5d47724", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/74119d6d0c2a3d89ad0f699857447241f4ba22b3.txt", "contents": "\nArcFace: Additive Angular Margin Loss for Deep Face Recognition\n\n\nJiankang Deng \nImperial College London\n\n\nInsightFace 3 FaceSoft\n\n\nJia Guo guojia@gmail.com \nInsightFace 3 FaceSoft\n\n\nNiannan Xue \nImperial College London\n\n\nStefanos Zafeiriou s.zafeiriou@imperial.ac.uk \nImperial College London\n\n\nArcFace: Additive Angular Margin Loss for Deep Face Recognition\n10.1109/CVPR.2019.00482\nOne of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for largescale face recognition is the design of appropriate loss functions that can enhance the discriminative power. Centre loss penalises the distance between deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in the angular space and therefore penalises the angles between deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to its exact correspondence to geodesic distance on a hypersphere. We present arguably the most extensive experimental evaluation against all recent state-of-the-art face recognition methods on ten face recognition benchmarks which includes a new large-scale image database with trillions of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state of the art and can be easily implemented with negligible computational overhead. To facilitate future research, code has been made available at: https://github.com/ deepinsight/insightface\n\nIntroduction\n\nFace representation using Deep Convolutional Neural Network (DCNN) embedding is the method of choice for face recognition [30,31,27,22]. DCNNs map the face image, typically after a pose normalisation step [42], into a * Equal contributions. InsightFace is a nonprofit Github project for 2D and 3D face analysis. Figure 1. Based on the centre [15] and feature [35] normalisation, all identities are distributed on a hypersphere. To enhance intraclass compactness and inter-class discrepancy, we consider four kinds of Geodesic Distance (GDis) constraint. (A) Margin-Loss: insert a geodesic distance margin between the sample and centres. (B) Intra-Loss: decrease the geodesic distance between the sample and the corresponding centre. (C) Inter-Loss: increase the geodesic distance between different centres. (D) Triplet-Loss: insert a geodesic distance margin between triplet samples. In this paper, we propose an Additive Angular Margin Loss (ArcFace), which is exactly corresponded to the geodesic distance (Arc) margin penalty in (A), to enhance the discriminative power of face recognition model. Extensive experimental results show that the strategy of (A) is most effective. feature that should have small intra-class and large interclass distance.\n\nThere are two main lines of research to train DCNNs for face recognition. Some train a multi-class classifier which can separate different identities in the training set, such by using a softmax classifier [31,22,3], and the others learn directly an embedding, such as the triplet loss [27]. Based on the large-scale training data and the elaborate DCNN architectures, both the softmax-loss-based methods [3] and the triplet-loss-based methods [27] can obtain excellent performance on face recognition. However, both the softmax loss and the triplet loss have some drawbacks. For the softmax loss: (1) the size of the linear transformation matrix W \u2208 R d\u00d7n increases linearly with the identities number n; (2) the learned features are separable for the closed-set classification problem but not discriminative enough for the open-set face recognition problem. For the triplet loss: (1) there is a combinatorial explosion in the number of face triplets especially for large-scale datasets, leading to a significant increase in the number of iteration steps; (2) semihard sample mining is a quite difficult problem for effective model training.\n\nSeveral variants [36,6,43,15,35,33,4,32,25] have been proposed to enhance the discriminative power of the softmax loss. Wen et al. [36] pioneered the centre loss, the Euclidean distance between each feature vector and its class centre, to obtain intra-class compactness while the interclass dispersion is guaranteed by the joint penalisation of the softmax loss. Nevertheless, updating the actual centres during training is extremely difficult as the number of face classes available for training has recently dramatically increased.\n\nBy observing that the weights from the last fully connected layer of a classification DCNN trained on the softmax loss bear conceptual similarities with the centres of each face class, the works in [15,16] proposed a multiplicative angular margin penalty to enforce extra intra-class compactness and inter-class discrepancy simultaneously, leading to a better discriminative power of the trained model. Even though Sphereface [15] introduced the important idea of angular margin, their loss function required a series of approximations in order to be computed, which resulted in an unstable training of the network. In order to stabilise training, they proposed a hybrid loss function which includes the standard softmax loss. Empirically, the softmax loss dominates the training process, because the integer-based multiplicative angular margin makes the target logit curve very precipitous and thus hinders convergence. CosFace [35,33] directly adds cosine margin penalty to the target logit, which obtains better performance compared to SphereFace but admits much easier implementation and relieves the need for joint supervision from the softmax loss.\n\nIn this paper, we propose an Additive Angular Margin Loss (ArcFace) to further improve the discriminative power of the face recognition model and to stabilise the training process. As illustrated in Figure 2, the dot product between the DCNN feature and the last fully connected layer is equal to the cosine distance after feature and weight normalisation. We utilise the arc-cosine function to calculate the angle between the current feature and the target weight. Afterwards, we add an additive angular margin to the target angle, and we get the target logit back again by the cosine function. Then, we re-scale all logits by a fixed feature norm, and the subsequent steps are exactly the same as in the softmax loss. The advantages of the proposed ArcFace can be summarised as follows: Engaging. ArcFace directly optimises the geodesic distance margin by virtue of the exact correspondence between the angle and arc in the normalised hypersphere. We intuitively illustrate what happens in the 512-D space via analysing the angle statistics between features and weights. Effective. ArcFace achieves state-of-the-art performance on ten face recognition benchmarks including large-scale image and video datasets. Easy. ArcFace only needs several lines of code as given in Algorithm 1 and is extremely easy to implement in the computational-graph-based deep learning frameworks, e.g. MxNet [5], Pytorch [23] and Tensorflow [2]. Furthermore, contrary to the works in [15,16], ArcFace does not need to be combined with other loss functions in order to have stable performance, and can easily converge on any training datasets. Efficient. ArcFace only adds negligible computational complexity during training. Current GPUs can easily support millions of identities for training and the model parallel strategy can easily support many more identities.\n\n\nProposed Approach\n\n\nArcFace\n\nThe most widely used classification loss function, softmax loss, is presented as follows:\nL 1 = \u2212 1 N N i=1 log e W T y i xi+by i n j=1 e W T j xi+bj ,(1)\nwhere x i \u2208 R d denotes the deep feature of the i-th sample, belonging to the y i -th class. The embedding feature dimension d is set to 512 in this paper following [36,43,15,35]. W j \u2208 R d denotes the j-th column of the weight W \u2208 R d\u00d7n and b j \u2208 R n is the bias term. The batch size and the class number are N and n, respectively. Traditional softmax loss is widely used in deep face recognition [22,3]. However, the softmax loss function does not explicitly optimise the feature embedding to enforce higher similarity for intraclass samples and diversity for inter-class samples, which results in a performance gap for deep face recognition under large intra-class appearance variations (e.g. pose variations [28,44] and age gaps [19,45]) and large-scale test scenarios (e.g. million [12,37,18] or trillion pairs [1]). For simplicity, we fix the bias b j = 0 as in [15]. Then, we transform the logit [24] as W T j x i = W j x i cos \u03b8 j , where \u03b8 j is the angle between the weight W j and the feature x i . Following [15,35,34], we fix the individual weight W j = 1 by l 2 normalisation. Following [26,35,34,33], we also fix the embedding feature x i by l 2 normalisation and re-scale it to s. The normalisation step on features and weights makes the predictions only depend on the angle between the feature and the weight. The learned embedding features are thus distributed on a hypersphere with a radius of s.\nL 2 = \u2212 1 N N i=1\nlog e s cos \u03b8y i e s cos \u03b8y i + n j=1,j =yi e s cos \u03b8j . (2) Figure 2. Training a DCNN for face recognition supervised by the ArcFace loss. Based on the feature xi and weight W normalisation, we get the cos \u03b8j (logit) for each class as W T j xi. We calculate the arccos\u03b8y i and get the angle between the feature xi and the ground truth weight Wy i . In fact, Wj provides a kind of centre for each class. Then, we add an angular margin penalty m on the target (ground truth) angle \u03b8y i . After that, we calculate cos(\u03b8y i + m) and multiply all logits by the feature scale s. The logits then go through the softmax function and contribute to the cross entropy loss. 1. x = mx.symbol.L2Normalization (x, mode = 'instance') 2. W = mx.symbol.L2Normalization (W, mode = 'instance') 3. fc7 = mx.sym.FullyConnected (data = x, weight = W, no bias = True, num hidden = n) 4. original target logit = mx.sym.pick (fc7, gt, axis = 1) 5. theta = mx.sym.arccos (original target logit) 6. marginal target logit = mx.sym.cos (theta + m) 7. one hot = mx.sym.one hot (gt, depth = n, on value = 1.0, off value = 0.0) 8. fc7 = fc7 + mx.sym.broadcast mul (one hot, mx.sym.expand dims (marginal target logit -original target logit, 1)) 9. fc7 = fc7 * s Output: Class-wise affinity score fc7.\n\nAs the embedding features are distributed around each feature centre on the hypersphere, we add an additive angular margin penalty m between x i and W yi to simultaneously enhance the intra-class compactness and inter-class discrepancy. Since the proposed additive angular margin penalty is equal to the geodesic distance margin penalty in the normalised hypersphere, we name our method as ArcFace.\nL 3 = \u2212 1 N N i=1\nlog e s(cos(\u03b8y i +m)) e s(cos(\u03b8y i +m)) + n j=1,j =yi e s cos \u03b8j .\n\n(3) We select face images from 8 different identities containing enough samples (around 1,500 images/class) to train 2-D feature embedding networks with the softmax and Ar-cFace loss, respectively. As illustrated in Figure 3, the softmax loss provides roughly separable feature embedding but produces noticeable ambiguity in decision boundaries, while the proposed ArcFace loss can obviously enforce a more evident gap between the nearest classes.\n\n\nComparison with SphereFace and CosFace\n\nNumerical Similarity. In SphereFace [15,16], ArcFace, and CosFace [35,33], three different kinds of margin penalty are proposed, e.g. multiplicative angular margin m 1 , additive angular margin m 2 , and additive cosine mar- gin m 3 , respectively. From the view of numerical analysis, different margin penalties, no matter add on the angle [15] or cosine space [35], all enforce the intra-class compactness and inter-class diversity by penalising the target logit [24]. In Figure 4(b), we plot the target logit curves of SphereFace, ArcFace and CosFace under their best margin settings. We only show these target logit curves within [20 \u2022 , 100 \u2022 ] because the angles between W yi and x i start from around 90 \u2022 (random initialisation) and end at around 30 \u2022 during Arc-Face training as shown in Figure 4 By combining all of the margin penalties, we implement SphereFace, ArcFace and CosFace in an united framework with m 1 , m 2 and m 3 as the hyper-parameters.\nL4 = \u2212 1 N N i=1 log e s(cos(m 1 \u03b8y i +m 2 )\u2212m 3 ) e s(cos(m 1 \u03b8y i +m 2 )\u2212m 3 ) + n j=1,j =y i e s cos \u03b8 j .(4)\nAs shown in Figure 4(b), by combining all of the abovemotioned margins (cos(m 1 \u03b8 + m 2 ) \u2212 m 3 ), we can easily get some other target logit curves which also have high performance. Geometric Difference. Despite the numerical similarity between ArcFace and previous works, the proposed additive angular margin has a better geometric attribute as the angular margin has the exact correspondence to the geodesic distance. As illustrated in Figure 5, we compare the decision boundaries under the binary classification case.\n\nThe proposed ArcFace has a constant linear angular margin throughout the whole interval. By contrast, SphereFace and CosFace only have a nonlinear angular margin. The minor difference in margin designs can have \"butterfly effect\" on the model training. For example, the original SphereFace [15] employs an annealing optimisation strategy. To avoid divergence at the beginning of training, joint supervision from softmax is used in SphereFace to weaken the multiplicative margin penalty. We implement a new version of SphereFace without the integer requirement on the margin by employing the arc-cosine function instead of using the complex double angle formula. In our implementation, we find that m = 1.35 can obtain similar performance compared to the original SphereFace without any convergence difficulty.\n\n\nComparison with Other Losses\n\nOther loss functions can be designed based on the angular representation of features and weight-vectors. For examples, we can design a loss to enforce intra-class compactness and inter-class discrepancy on the hypersphere. As shown in Figure 1, we compare with three other losses in this paper. Intra-Loss is designed to improve the intra-class compactness by decreasing the angle/arc between the sample and the ground truth centre.\nL 5 = L 2 + 1 \u03c0N N i=1 \u03b8 yi .(5)\nInter-Loss targets at enhancing inter-class discrepancy by increasing the angle/arc between different centres.\nL 6 = L 2 \u2212 1 \u03c0N (n \u2212 1) N i=1 n j=1,j =yi arccos(W T yi W j ). (6)\nThe Inter-Loss here is a special case of the Minimum Hyper-spherical Energy (MHE) method [14]. In [14], both hidden layers and output layers are regularised by MHE. In the MHE paper, a special case of loss function was also proposed by combining the SphereFace loss with MHE loss on the last layer of the network. Triplet-loss aims at enlarging the angle/arc margin between triplet samples. In FaceNet [27], Euclidean margin is applied on the normalised features. Here, we employ the triplet-loss by the angular representation of our features as arccos(x pos i x i ) + m \u2264 arccos(x neg i x i ).\n\n\nExperiments\n\n\nImplementation Details\n\nDatasets. As given in Table 1, we separately employ CA-SIA [41], VGGFace2 [3], MS1MV2 and DeepGlint-Face (including MS1M-DeepGlint and Asian-DeepGlint) [1] as our training data in order to conduct fair comparison with other methods. Please note that the proposed MS1MV2 is a semi-automatic refined version of the MS-Celeb-1M dataset [7]. To best of our knowledge, we are the first to employ ethnicity-specific annotators for large-scale face image annotations, as the boundary cases (e.g. hard samples and noisy samples) are very hard to distinguish if the annotator is not familiar with the identity. During training, we explore efficient face verification datasets (e.g. LFW [10], CFP-FP [28], AgeDB-30 [19]) to check the improvement from different settings. Besides the most widely used LFW [10] and YTF [38] datasets, we also report the performance of Ar-cFace on the recent large-pose and large-age datasets(e.g. CPLFW [44] and CALFW [45]). We also extensively test the proposed ArcFace on large-scale image datasets (e.g. MegaFace [12], IJB-B [37], IJB-C [18] and Trillion-Pairs [1]) and video datasets (iQIYI-VID [17]). Experimental Settings. For data prepossessing, we follow the recent papers [15,35]  We follow [35] to set the feature scale s to 64 and choose the angular margin m of ArcFace at 0.5. All experiments in this paper are implemented by MXNet [5]. We set the batch size to 512 and train models on four NVIDIA Tesla P40 (24GB) GPUs. On CASIA, the learning rate starts from 0.1 and is divided by 10 at 20K, 28K iterations. The training process is finished at 32K iterations. On MS1MV2, we divide the learning rate at 100K,160K iterations and finish at 180K iterations. We set momentum to 0.9 and weight decay to 5e \u2212 4. During testing, we only keep the feature embedding network without the fully connected layer (160MB for ResNet50 and 250MB for ResNet100) and extract the 512-D features (8.9 ms/face for ResNet50 and 15.4 ms/face for ResNet100) for each normalised face. To get the embedding features for templates (e.g. IJB-B and IJB-C) or videos (e.g. YTF and iQIYI-VID), we simply calculate the feature centre of all images from the template or all frames from the video. Note that, overlap identities between the training set and the test set are removed for strict evaluations, and we only use a single crop for all testing.\n\n\nAblation Study on Losses\n\nIn Table 2, we first explore the angular margin setting for ArcFace on the CASIA dataset with ResNet50. The best margin observed in our experiments was 0.5. Using the proposed combined margin framework in Eq. 4, it is easier to set the margin of SphereFace and CosFace which we found to have optimal performance when setting at 1.35 and 0.35, respectively. Our implementations for both SphereFace and CosFace can lead to excellent performance without observing any difficulty in convergence. The proposed ArcFace achieves the highest verification accuracy on all three test sets. In addition, we performed extensive experiments with the combined margin framework (some of the best performance was observed for CM1 (1, 0.3, 0.2) and CM2 (0.9, 0.4, 0.15)) guided by the target logit curves in Figure 4(b). The combined margin framework led to better performance than individual SphereFace and CosFace but upper-bounded by the performance of ArcFace.\n\nBesides the comparison with margin-based methods, we conduct a further comparison between ArcFace and other losses which aim at enforcing intra-class compactness (Eq. 5) and inter-class discrepancy (Eq. 6). As the baseline we have chosen the softmax loss and we have observed performance drop on CFP-FP and AgeDB-30 after weight and feature normalisation. By combining the softmax with the intra-class loss, the performance improves on CFP-FP and AgeDB-30. However, combining the softmax with the inter-class loss only slightly improves the accuracy. The fact that Triplet-loss outperforms Norm-Softmax loss indicates the importance of margin in improving the performance. However, employing margin penalty within triplet samples is less effective than inserting margin between samples and centres as in ArcFace. Finally, we incorporate the Intra-loss, Inter-loss and Triplet-loss into ArcFace, but no improvement is observed, which leads us to believe that Ar-cFace is already enforcing intra-class compactness, interclass discrepancy and classification margin.\n\nTo get a better understanding of ArcFace's superiority, we give the detailed angle statistics on training data (CA-SIA) and test data (LFW) under different losses in Table  3. We find that (1) W j is nearly synchronised with embedding feature centre for ArcFace (14.29 \u2022 ), but there is an obvious deviation (44.26 \u2022 ) between W j and the embedding feature centre for Norm-Softmax. Therefore, the angles between W j cannot absolutely represent the interclass discrepancy on training data. Alternatively, the embedding feature centres calculated by the trained network are more representative. (2) Intra-Loss can effectively compress intra-class variations but also brings in smaller interclass angles. (3) Inter-Loss can slightly increase inter-class discrepancy on both W (directly) and the embedding network (indirectly), but also raises intra-class angles. (4) Ar-cFace already has very good intra-class compactness and . Each column denotes one particular loss. \"W-EC\" refers to the mean of angles between Wj and the corresponding embedding feature centre. \"W-Inter\" refers to the mean of minimum angles between Wj's. \"Intra1\" and \"Intra2\" refer to the mean of angles between xi and the embedding feature centre on CASIA and LFW, respectively. \"Inter1\" and \"Inter2\" refer to the mean of minimum angles between embedding feature centres on CASIA and LFW, respectively.\n\ninter-class discrepancy. (5) Triplet-Loss has similar intraclass compactness but inferior inter-class discrepancy compared to ArcFace. In addition, ArcFace has a more distinct margin than Triplet-Loss on the test set as illustrated in Figure 6. \n\n\nEvaluation Results\n\nResults on LFW, YTF, CALFW and CPLFW. LFW [10] and YTF [38] datasets are the most widely used benchmark for unconstrained face verification on images and videos. In this paper, we follow the unrestricted with labelled outside data protocol to report the performance. As reported in Table 4, ArcFace trained on MS1MV2 with ResNet100 beats the baselines (e.g. SphereFace [15] and CosFace [35]) by a significant margin on both LFW and YTF, which shows  that the additive angular margin penalty can notably enhance the discriminative power of deeply learned features, demonstrating the effectiveness of ArcFace. Besides on LFW and YTF datasets, we also report the performance of ArcFace on the recently introduced datasets (e.g. CPLFW [44] and CALFW [45]) which show higher pose and age variations with same identities from LFW. Among all of the open-sourced face recognition models, the ArcFace model is evaluated as the top-ranked face recognition model as shown in Table 5, outperforming counterparts by an obvious margin. In Figure 7, we illustrate the angle distributions (predicted by ArcFace model trained on MS1MV2 with ResNet100) of both positive and negative pairs on LFW, CFP-FP, AgeDB-30, YTF, CPLFW and CALFW. We can clearly find that the intra-variance due to pose and age gaps significantly increases the angles between positive pairs thus making the best threshold for face verification increasing and generating more confusion regions on the histogram. Results on MegaFace. The MegaFace dataset [12] includes 1M images of 690K different individuals as the gallery set and 100K photos of 530 unique individuals from FaceScrub [21] as the probe set. On MegaFace, there are two testing scenarios (identification and verification) under two protocols (large or small training set). The training set is defined as large if it contains more than 0.5M images. For the fair  comparison, we train ArcFace on CAISA and MS1MV2 under the small protocol and large protocol, respectively. In Table 6, ArcFace trained on CASIA achieves the best single-model identification and verification performance, not only surpassing the strong baselines (e.g. SphereFace [15] and CosFace [35]) but also outperforming other published methods [36,14]. As we observed an obvious performance gap between identification and verification, we performed a thorough manual check in the whole MegaFace dataset and found many face images with wrong labels, which significantly affects the performance. Therefore, we manually refined the whole MegaFace dataset and report the correct performance of ArcFace on MegaFace. On the refined MegaFace, ArcFace still clearly outperforms CosFace and achieves the best performance on both verification and identification.\n\nUnder large protocol, ArcFace surpasses FaceNet [27] by a clear margin and obtains comparable results on identification and better results on verification compared to CosFace [35]. Since CosFace employs a private training data, we retrain CosFace on our MS1MV2 dataset with ResNet100. Under fair comparison, ArcFace shows superiority over CosFace and forms an upper envelope of Cos-Face under both identification and verification scenarios as shown in Figure 8. Results on IJB-B and IJB-C. The IJB-B dataset [37] Methods Id (%) Ver (%) Softmax [15] 54.85 65.92 Contrastive Loss [15,30] 65.21 78.86 Triplet [15,27] 64.79 78.32 Center Loss [36] 65.49 80.14 SphereFace [15] 72.729 85.561 CosFace [35] 77.11 89.88 AM-Softmax [33] 72.47 84.44 SphereFace+ [14] 73  Table 6. Face identification and verification evaluation of different methods on MegaFace Challenge1 using FaceScrub as the probe set. \"Id\" refers to the rank-1 face identification accuracy with 1M distractors, and \"Ver\" refers to the face verification TAR at 10 \u22126 FAR. \"R\" refers to data refinement on both probe set and 1M distractors. ArcFace obtains state-of-the-art performance under both small and large protocols.  On the IJB-B and IJB-C datasets, we employ the VGG2 dataset as the training data and the ResNet50 as the embedding network to train ArcFace for the fair comparison with the most recent methods [3,40,39]. In Table 7, we compare the TAR (@FAR=1e-4) of ArcFace with the previous stateof-the-art models [3,40,39]. ArcFace can obviously boost the performance on both IJB-B and IJB-C (about 3 \u223c 5%, which is a significant reduction in the error). Drawing support from more training data (MS1MV2) and deeper neural network (ResNet100), ArcFace can further improve the Method IJB-B IJB-C ResNet50 [3] 0.784 0.825 SENet50 [3] 0.800 0.840 ResNet50+SENet50 [3] 0.800 0.841 MN-v [40] 0.818 0.852 MN-vc [40] 0.831 0.862 ResNet50+DCN(Kpts) [39] 0.850 0.867 ResNet50+DCN(Divs) [39] 0.841 0.880 SENet50+DCN(Kpts) [39] 0.846 0.874 SENet50+DCN(Divs) [39] 0.849 0.885 VGG2, R50, ArcFace 0.898 0.921 MS1MV2, R100, ArcFace 0.942 0.956 TAR (@FAR=1e-4) to 94.2% and 95.6% on IJB-B and IJB-C, respectively. In Figure 9, we show the full ROC curves of the proposed ArcFace on IJB-B and IJB-C , and ArcFace achieves impressive performance even at FAR=1e-6 setting a new baseline. Results on Trillion-Pairs. The Trillion-Pairs dataset [1] provides 1.58M images from Flickr as the gallery set and 274K images from 5.7k LFW [10] identities as the probe set. Every pair between gallery and probe set is used for evaluation (0.4 trillion pairs in total). In Table 8, we compare the performance of ArcFace trained on different datasets. The proposed MS1MV2 dataset obviously boosts the performance compared to CASIA and even slightly outperforms the DeepGlint-Face dataset, which has a double identity number. When combining all identities from MS1MV2 and Asian celebrities from DeepGlint, Arc-Face achieves the best identification performance 84.840% (@FPR=1e-3) and comparable verification performance compared to the most recent submission (CIGIT IRSEC) from the lead-board.\n\nResults on iQIYI-VID. The iQIYI-VID challenge [17] contains  Table 9. MAP of our method on the iQIYI-VID test set. \"MLP\" refers to a three-layer fully connected network trained on the iQIYI-VID training data.\n\niQIYI-VID dataset employs MAP@100 as the evaluation indicator. MAP (Mean Average Precision) refers to the overall average accuracy rate, which is the mean of the average accuracy rate of the corresponding videos of person ID retrieved in the test set for each person ID (as the query) in the training set.\n\nAs shown in Table 9, ArcFace trained on combined MS1MV2 and Asian datasets with ResNet100 sets a high baseline (MAP=(79.80%)). Based on the embedding feature for each training video, we train an additional threelayer fully connected network with a classification loss to get the customised feature descriptor on the iQIYI-VID dataset. The MLP learned on the iQIYI-VID training set significantly boosts the MAP by 6.60%. Drawing support from the model ensemble and context features from the offthe-shelf object and scene classifier [20], our final result surpasses the runner-up by a clear margin ( 0.99%).\n\n\nConclusions\n\nIn this paper, we propose an Additive Angular Margin Loss function, which can effectively enhance the discriminative power of feature embeddings learned via DCNNs for face recognition. We demonstrate that our method consistently outperforms the state of the art through the most comprehensive experiments. Codes with detailed explanations are released to facilitate reproduciblility of the results reported in this paper.\n\nAlgorithm 1\n1The Pseudo-code of ArcFace on MxNet Input: Feature Scale s, Margin Parameter m in Eq. 3, Class Number n, Ground-Truth ID gt.\n\nFigure 3 .\n3Toy examples under the softmax and ArcFace loss on 8 identities with 2D features. Dots indicate samples and lines refer to the centre direction of each identity. Based on the feature normalisation, all face features are pushed to the arc space with a fixed radius. The geodesic distance gap between closest classes becomes evident as the additive angular margin penalty is incorporated.\n\nFigure 4 .\n4(a). Intuitively, there are three factors in the target logit curves that affect the perfor-mance, i.e. the starting point, the end point and the slope. Target logit analysis. (a) \u03b8j distributions from start to end during ArcFace training. (2) Target logit curves for softmax, SphereFace, ArcFace, CosFace and combined margin penalty (cos(m1\u03b8 + m2) \u2212 m3).\n\nFigure 5 .\n5Decision margins of different loss functions under binary classification case. The dashed line represents the decision boundary, and the grey areas are the decision margins.\n\nFigure 6 .\n6Angle distributions of all positive pairs and random negative pairs (\u223c 0.5M) from LFW. Red area indicates positive pairs while blue indicates negative pairs. All angles are represented in degree. ([CASIA, ResNet50, loss*]).\n\nFigure 7 .\n7Angle distributions of both positive and negative pairs on LFW, CFP-FP, AgeDB-30, YTF, CPLFW and CALFW. Red area indicates positive pairs while blue indicates negative pairs. All angles are represented in degree. ([MS1MV2, ResNet100, ArcFace])\n\nFigure 8 .\n8CMC and ROC curves of different models on MegaFace. Results are evaluated on both original and refined MegaFace dataset.\n\nFigure 9 .\n9ROC curves of 1:1 verification protocol on the IJB-B and IJB-C dataset.\n\n\ncontains 1, 845 subjects with 21.8K still images and 55K frames from 7, 011 videos. In total, there are 12, 115 templates with 10, 270 genuine matches and 8M impostor matches. The IJB-C dataset[37] is a further extension of IJB-B, having 3, 531 subjects with 31.3K still images and 117.5K frames from 11, 779 videos. In total, there are 23, 124 templates with 19, 557 genuine matches and 15, 639K impostor matches.\n\nTable 7 .\n71:1 verification TAR (@FAR=1e-4) on the IJB-B and IJB-C dataset.\n\n\n565,372 video clips (training set 219,677, validation set 172,860, and test set 172,835) of 4934 identities from iQIYI variety shows, films and television dramas. The length of each video ranges from 1 to 30 seconds. This dataset supplies multi-modal cues, including face, cloth, voice, gait and subtitles, for character identification. The https://github.com/deepinsight/insightface/tree/master/Evaluation/IJBTable 8. Identification and verification results (%) on the Trillion-Pairs dataset. ([Dataset*, ResNet100, ArcFace])Method \nId (@FPR=1e-3) Ver(@FPR=1e-9) \nCASIA \n26.643 \n21.452 \nMS1MV2 \n80.968 \n78.600 \nDeepGlint-Face \n80.331 \n78.586 \nMS1MV2+Asian \n84.840 (1st) \n80.540 \nCIGIT IRSEC \n84.234 (2nd) \n81.558 (1st) \n\nMethod \nMAP(%) \nMS1MV2+Asian, R100, ArcFace \n79.80 \n+ MLP \n86.40 \n+ Ensemble \n88.26 \n+ Context \n88.65 (1st) \nOther Participant \n87.66 (2nd) \n\n\nAcknowledgements. Jiankang Deng acknowledges financial support from the Imperial President's PhD Scholarship and GPU donations from NVIDIA. Stefanos Zafeiriou acknowledges support from EPSRC Fellowship DEFORM (EP/S010203/1), FACER2VM (EP/N007743/1) and a Google Faculty Fellowship.\nMart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, arXiv:1603.04467Large-scale machine learning on heterogeneous distributed systems. Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv:1603.04467, 2016. 2\n\nVggface2: A dataset for recognising faces across pose and age. Qiong Cao, Li Shen, Weidi Xie, M Omkar, Andrew Parkhi, Zisserman, FG. 7Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and An- drew Zisserman. Vggface2: A dataset for recognising faces across pose and age. In FG, 2018. 1, 2, 4, 5, 7, 8\n\nNoisy softmax: improving the generalization ability of dcnn via postponing the early softmax saturation. Binghui Chen, Weihong Deng, Junping Du, CVPR. Binghui Chen, Weihong Deng, and Junping Du. Noisy soft- max: improving the generalization ability of dcnn via post- poning the early softmax saturation. In CVPR, 2017. 2\n\nMxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, Zheng Zhang, arXiv:1512.0127425Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient ma- chine learning library for heterogeneous distributed systems. arXiv:1512.01274, 2015. 2, 5\n\nMarginal loss for deep face recognition. Jiankang Deng, Yuxiang Zhou, Stefanos Zafeiriou, CVPR Workshop. 26Jiankang Deng, Yuxiang Zhou, and Stefanos Zafeiriou. Marginal loss for deep face recognition. In CVPR Workshop, 2017. 2, 6\n\nMs-celeb-1m: A dataset and benchmark for large-scale face recognition. Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao, ECCV. Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In ECCV, 2016. 4\n\nDeep pyramidal residual networks. Dongyoon Han, Jiwhan Kim, Junmo Kim, arXiv:1610.02915Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyra- midal residual networks. arXiv:1610.02915, 2016. 5\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 5\n\nLabeled faces in the wild: A database for studying face recognition in unconstrained environments. B Gary, Manu Huang, Tamara Ramesh, Erik Berg, Learned-Miller, 6Technical reportGary B Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical report, 2007. 4, 5, 6, 8\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, ICML. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, 2015. 5\n\nThe megaface benchmark: 1 million faces for recognition at scale. Ira Kemelmacher-Shlizerman, M Steven, Daniel Seitz, Evan Miller, Brossard, CVPR. 6Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The megaface benchmark: 1 million faces for recognition at scale. In CVPR, 2016. 2, 5, 6\n\nTargeting ultimate accuracy: Face recognition via deep embedding. Jingtuo Liu, Yafeng Deng, Tao Bai, Zhengping Wei, Chang Huang, arXiv:1506.07310Jingtuo Liu, Yafeng Deng, Tao Bai, Zhengping Wei, and Chang Huang. Targeting ultimate accuracy: Face recogni- tion via deep embedding. arXiv:1506.07310, 2015. 6\n\nLearning towards minimum hyperspherical energy. Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, Le Song, In NeurIPS. 47Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning towards minimum hy- perspherical energy. In NeurIPS, 2018. 4, 6, 7\n\nSphereface: Deep hypersphere embedding for face recognition. Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, Le Song, CVPR. 67Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In CVPR, 2017. 1, 2, 3, 4, 5, 6, 7\n\nLarge-margin softmax loss for convolutional neural networks. Weiyang Liu, Yandong Wen, Zhiding Yu, Meng Yang, ICML. 23Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional neural net- works. In ICML, 2016. 2, 3\n\nYuanliu Liu, Peipei Shi, Bo Peng, He Yan, Yong Zhou, Bing Han, Yi Zheng, Chao Lin, arXiv:1811.07548Jianbin Jiang, and Yin Fan. iqiyivid: A large dataset for multi-modal person identification. 5Yuanliu Liu, Peipei Shi, Bo Peng, He Yan, Yong Zhou, Bing Han, Yi Zheng, Chao Lin, Jianbin Jiang, and Yin Fan. iqiyi- vid: A large dataset for multi-modal person identification. arXiv:1811.07548, 2018. 5, 8\n\nIarpa janus benchmark-c: Face dataset and protocol. Brianna Maze, Jocelyn Adams, A James, Nathan Duncan, Tim Kalka, Charles Miller, Otto, K Anil, Jain, Janet Tyler Niggel, Jordan Anderson, Cheney, In ICB. 25Brianna Maze, Jocelyn Adams, James A Duncan, Nathan Kalka, Tim Miller, Charles Otto, Anil K Jain, W Tyler Niggel, Janet Anderson, and Jordan Cheney. Iarpa janus benchmark-c: Face dataset and protocol. In ICB, 2018. 2, 5\n\nAgedb: The first manually collected in-the-wild age database. Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, Stefanos Zafeiriou, CVPR Workshop. 5Stylianos Moschoglou, Athanasios Papaioannou, Chris- tos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: The first manually collected in-the-wild age database. In CVPR Workshop, 2017. 2, 4, 5\n\nMxNetModels. MxNetModels. http://data.mxnet.io/models/. 8\n\nA data-driven approach to cleaning large face datasets. Hong-Wei Ng, Stefan Winkler, ICIP. Hong-Wei Ng and Stefan Winkler. A data-driven approach to cleaning large face datasets. In ICIP, 2014. 6\n\nDeep face recognition. M Omkar, Andrea Parkhi, Andrew Vedaldi, Zisserman, BMVC. Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition. In BMVC, 2015. 1, 2, 6\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NeurIPS Workshop. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al- ban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NeurIPS Workshop, 2017. 2\n\nRegularizing neural networks by penalizing confident output distributions. Gabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, Geoffrey Hinton, arXiv:1701.0654823Gabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. arXiv:1701.06548, 2017. 2, 3\n\nFace recognition via centralized coordinate learning. Xianbiao Qi, Lei Zhang, arXiv:1801.05678Xianbiao Qi and Lei Zhang. Face recognition via centralized coordinate learning. arXiv:1801.05678, 2018. 2\n\nL2-constrained softmax loss for discriminative face verification. Rajeev Ranjan, D Carlos, Rama Castillo, Chellappa, arXiv:1703.09507Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2- constrained softmax loss for discriminative face verification. arXiv:1703.09507, 2017. 2\n\nFacenet: A unified embedding for face recognition and clustering. Florian Schroff, Dmitry Kalenichenko, James Philbin, CVPR. 67Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clus- tering. In CVPR, 2015. 1, 4, 6, 7\n\nFrontal to profile face verification in the wild. Soumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, M Vishal, Rama Patel, David W Chellappa, Jacobs, WACV. 5Soumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, Vishal M Patel, Rama Chellappa, and David W Jacobs. Frontal to profile face verification in the wild. In WACV, 2016. 2, 4, 5\n\nDropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, JML. 5Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. JML, 2014. 5\n\nDeep learning face representation by joint identificationverification. Yi Sun, Yuheng Chen, Xiaogang Wang, Xiaoou Tang, NeurIPS. 7Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation by joint identification- verification. In NeurIPS, 2014. 1, 6, 7\n\nDeepface: Closing the gap to human-level performance in face verification. Yaniv Taigman, Ming Yang, Marc&apos;aurelio Ranzato, Lior Wolf, CVPR. 16Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-level perfor- mance in face verification. In CVPR, 2014. 1, 6\n\nRethinking feature distribution for loss functions in image classification. Weitao Wan, Yuanyi Zhong, Tianpeng Li, Jiansheng Chen, arXiv:1803.02988Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng Chen. Rethinking feature distribution for loss functions in image classification. arXiv:1803.02988, 2018. 2\n\nAdditive margin softmax for face verification. Feng Wang, Weiyang Liu, Haijun Liu, Jian Cheng, SPL. 27Feng Wang, Weiyang Liu, Haijun Liu, and Jian Cheng. Ad- ditive margin softmax for face verification. SPL, 2018. 2, 3, 7\n\nNormface: l 2 hypersphere embedding for face verification. Feng Wang, Xiang Xiang, Jian Cheng, Alan L Yuille, arXiv:1704.06369Feng Wang, Xiang Xiang, Jian Cheng, and Alan L Yuille. Normface: l 2 hypersphere embedding for face verification. arXiv:1704.06369, 2017. 2\n\nCosface: Large margin cosine loss for deep face recognition. Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li, Dihong Gong, Jingchao Zhou, Wei Liu, CVPR. 67Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li, Dihong Gong, Jingchao Zhou, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In CVPR, 2018. 1, 2, 3, 5, 6, 7\n\nA discriminative feature learning approach for deep face recognition. Yandong Wen, Kaipeng Zhang, Zhifeng Li, Yu Qiao, ECCV. 67Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recog- nition. In ECCV, 2016. 2, 6, 7\n\nIarpa janus benchmark-b face dataset. Cameron Whitelam, Emma Taborsky, Austin Blanton, Brianna Maze, C Jocelyn, Tim Adams, Miller, D Nathan, Kalka, K Anil, James A Jain, Kristen Duncan, Allen, CVPR Workshop. 57Cameron Whitelam, Emma Taborsky, Austin Blanton, Bri- anna Maze, Jocelyn C Adams, Tim Miller, Nathan D Kalka, Anil K Jain, James A Duncan, and Kristen Allen. Iarpa janus benchmark-b face dataset. In CVPR Workshop, 2017. 2, 5, 7\n\nFace recognition in unconstrained videos with matched background similarity. Lior Wolf, Itay Hassner, Maoz, CVPR. 46Lior Wolf, Tal Hassner, and Itay Maoz. Face recognition in unconstrained videos with matched background similarity. In CVPR, 2011. 4, 5, 6\n\nComparator networks. Weidi Xie, Shen Li, Andrew Zisserman, ECCV. 7Weidi Xie, Shen Li, and Andrew Zisserman. Comparator networks. In ECCV, 2018. 7, 8\n\nMulticolumn networks for face recognition. Weidi Xie, Andrew Zisserman, BMVC. 7Weidi Xie and Andrew Zisserman. Multicolumn networks for face recognition. In BMVC, 2018. 7, 8\n\nDong Yi, Zhen Lei, Shengcai Liao, Stan Z Li, arXiv:1411.7923Learning face representation from scratch. 45Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv:1411.7923, 2014. 4, 5\n\nJoint face detection and alignment using multitask cascaded convolutional networks. SPL. Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao, Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. SPL, 2016. 1\n\nRange loss for deep face recognition with long-tail. Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, Yu Qiao, ICCV. 26Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, and Yu Qiao. Range loss for deep face recognition with long-tail. In ICCV, 2017. 2, 6\n\nCross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments. Tianyue Zheng, Weihong Deng, 56Technical ReportTianyue Zheng and Weihong Deng. Cross-pose lfw: A database for studying cross-pose face recognition in uncon- strained environments. Technical Report, 2018. 2, 5, 6\n\nCross-age lfw: A database for studying cross-age face recognition in unconstrained environments. Tianyue Zheng, Weihong Deng, Jiani Hu, arXiv:1708.081976Tianyue Zheng, Weihong Deng, and Jiani Hu. Cross-age lfw: A database for studying cross-age face recognition in unconstrained environments. arXiv:1708.08197, 2017. 2, 5, 6\n", "annotations": {"author": "[{\"end\":132,\"start\":67},{\"end\":183,\"start\":133},{\"end\":222,\"start\":184},{\"end\":295,\"start\":223},{\"end\":132,\"start\":67},{\"end\":183,\"start\":133},{\"end\":222,\"start\":184},{\"end\":295,\"start\":223}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":76},{\"end\":140,\"start\":137},{\"end\":195,\"start\":192},{\"end\":241,\"start\":232},{\"end\":80,\"start\":76},{\"end\":140,\"start\":137},{\"end\":195,\"start\":192},{\"end\":241,\"start\":232}]", "author_first_name": "[{\"end\":75,\"start\":67},{\"end\":136,\"start\":133},{\"end\":191,\"start\":184},{\"end\":231,\"start\":223},{\"end\":75,\"start\":67},{\"end\":136,\"start\":133},{\"end\":191,\"start\":184},{\"end\":231,\"start\":223}]", "author_affiliation": "[{\"end\":106,\"start\":82},{\"end\":131,\"start\":108},{\"end\":182,\"start\":159},{\"end\":221,\"start\":197},{\"end\":294,\"start\":270},{\"end\":106,\"start\":82},{\"end\":131,\"start\":108},{\"end\":182,\"start\":159},{\"end\":221,\"start\":197},{\"end\":294,\"start\":270}]", "title": "[{\"end\":64,\"start\":1},{\"end\":359,\"start\":296},{\"end\":64,\"start\":1},{\"end\":359,\"start\":296}]", "venue": null, "abstract": "[{\"end\":1944,\"start\":384},{\"end\":1944,\"start\":384}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2086,\"start\":2082},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2089,\"start\":2086},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2092,\"start\":2089},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2095,\"start\":2092},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2169,\"start\":2165},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2306,\"start\":2302},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2323,\"start\":2319},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3425,\"start\":3421},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3428,\"start\":3425},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3430,\"start\":3428},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3505,\"start\":3501},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3623,\"start\":3620},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3663,\"start\":3659},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4380,\"start\":4376},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4382,\"start\":4380},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4385,\"start\":4382},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4388,\"start\":4385},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4391,\"start\":4388},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4394,\"start\":4391},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4396,\"start\":4394},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4399,\"start\":4396},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4402,\"start\":4399},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4494,\"start\":4490},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5096,\"start\":5092},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5099,\"start\":5096},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5324,\"start\":5320},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5827,\"start\":5823},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5830,\"start\":5827},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7442,\"start\":7439},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7456,\"start\":7452},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7475,\"start\":7472},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7519,\"start\":7515},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7522,\"start\":7519},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8252,\"start\":8248},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8255,\"start\":8252},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8258,\"start\":8255},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8261,\"start\":8258},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8485,\"start\":8481},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8487,\"start\":8485},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8799,\"start\":8795},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8802,\"start\":8799},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8820,\"start\":8816},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8823,\"start\":8820},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8874,\"start\":8870},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8877,\"start\":8874},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8880,\"start\":8877},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8955,\"start\":8951},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8990,\"start\":8986},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9106,\"start\":9102},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9109,\"start\":9106},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9112,\"start\":9109},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9187,\"start\":9183},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9190,\"start\":9187},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9193,\"start\":9190},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9196,\"start\":9193},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11801,\"start\":11797},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11804,\"start\":11801},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11831,\"start\":11827},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11834,\"start\":11831},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12106,\"start\":12102},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12127,\"start\":12123},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12230,\"start\":12226},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13654,\"start\":13650},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14940,\"start\":14936},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14949,\"start\":14945},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15253,\"start\":15249},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15545,\"start\":15541},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15559,\"start\":15556},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15818,\"start\":15815},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16163,\"start\":16159},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16176,\"start\":16172},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16191,\"start\":16187},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16280,\"start\":16276},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16293,\"start\":16289},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16410,\"start\":16406},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":16425,\"start\":16421},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16523,\"start\":16519},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16535,\"start\":16531},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16547,\"start\":16543},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16606,\"start\":16602},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16688,\"start\":16684},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16691,\"start\":16688},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16707,\"start\":16703},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16850,\"start\":16847},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21562,\"start\":21558},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21575,\"start\":21571},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21889,\"start\":21885},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21906,\"start\":21902},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22251,\"start\":22247},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22266,\"start\":22262},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23028,\"start\":23024},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23158,\"start\":23154},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23679,\"start\":23675},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23696,\"start\":23692},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23749,\"start\":23745},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23752,\"start\":23749},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24434,\"start\":24430},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24767,\"start\":24763},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24803,\"start\":24799},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24837,\"start\":24833},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24840,\"start\":24837},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24865,\"start\":24861},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24868,\"start\":24865},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24897,\"start\":24893},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24925,\"start\":24921},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24952,\"start\":24948},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24980,\"start\":24976},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25009,\"start\":25005},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25633,\"start\":25630},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25636,\"start\":25633},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25639,\"start\":25636},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25739,\"start\":25736},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25742,\"start\":25739},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25745,\"start\":25742},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26029,\"start\":26026},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26053,\"start\":26050},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26086,\"start\":26083},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26108,\"start\":26104},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26131,\"start\":26127},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26167,\"start\":26163},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26203,\"start\":26199},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26238,\"start\":26234},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26273,\"start\":26269},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26736,\"start\":26732},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27434,\"start\":27430},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28436,\"start\":28432},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30951,\"start\":30947},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2086,\"start\":2082},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2089,\"start\":2086},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2092,\"start\":2089},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2095,\"start\":2092},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2169,\"start\":2165},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2306,\"start\":2302},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2323,\"start\":2319},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3425,\"start\":3421},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3428,\"start\":3425},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3430,\"start\":3428},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3505,\"start\":3501},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3623,\"start\":3620},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3663,\"start\":3659},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4380,\"start\":4376},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4382,\"start\":4380},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4385,\"start\":4382},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4388,\"start\":4385},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4391,\"start\":4388},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4394,\"start\":4391},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4396,\"start\":4394},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4399,\"start\":4396},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4402,\"start\":4399},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4494,\"start\":4490},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5096,\"start\":5092},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5099,\"start\":5096},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5324,\"start\":5320},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5827,\"start\":5823},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5830,\"start\":5827},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7442,\"start\":7439},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7456,\"start\":7452},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7475,\"start\":7472},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7519,\"start\":7515},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7522,\"start\":7519},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8252,\"start\":8248},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8255,\"start\":8252},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8258,\"start\":8255},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8261,\"start\":8258},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8485,\"start\":8481},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8487,\"start\":8485},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8799,\"start\":8795},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8802,\"start\":8799},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8820,\"start\":8816},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8823,\"start\":8820},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8874,\"start\":8870},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8877,\"start\":8874},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8880,\"start\":8877},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8955,\"start\":8951},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8990,\"start\":8986},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9106,\"start\":9102},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9109,\"start\":9106},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9112,\"start\":9109},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9187,\"start\":9183},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9190,\"start\":9187},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9193,\"start\":9190},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9196,\"start\":9193},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11801,\"start\":11797},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11804,\"start\":11801},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11831,\"start\":11827},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11834,\"start\":11831},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12106,\"start\":12102},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12127,\"start\":12123},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12230,\"start\":12226},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13654,\"start\":13650},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14940,\"start\":14936},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14949,\"start\":14945},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15253,\"start\":15249},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15545,\"start\":15541},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15559,\"start\":15556},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15818,\"start\":15815},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16163,\"start\":16159},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16176,\"start\":16172},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16191,\"start\":16187},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16280,\"start\":16276},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16293,\"start\":16289},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16410,\"start\":16406},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":16425,\"start\":16421},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16523,\"start\":16519},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16535,\"start\":16531},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16547,\"start\":16543},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16606,\"start\":16602},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16688,\"start\":16684},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16691,\"start\":16688},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16707,\"start\":16703},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16850,\"start\":16847},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21562,\"start\":21558},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21575,\"start\":21571},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21889,\"start\":21885},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21906,\"start\":21902},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22251,\"start\":22247},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22266,\"start\":22262},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23028,\"start\":23024},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23158,\"start\":23154},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23679,\"start\":23675},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23696,\"start\":23692},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23749,\"start\":23745},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23752,\"start\":23749},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24434,\"start\":24430},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24767,\"start\":24763},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24803,\"start\":24799},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24837,\"start\":24833},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24840,\"start\":24837},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24865,\"start\":24861},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24868,\"start\":24865},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24897,\"start\":24893},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24925,\"start\":24921},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24952,\"start\":24948},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24980,\"start\":24976},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25009,\"start\":25005},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25633,\"start\":25630},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25636,\"start\":25633},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25639,\"start\":25636},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25739,\"start\":25736},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25742,\"start\":25739},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25745,\"start\":25742},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26029,\"start\":26026},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26053,\"start\":26050},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26086,\"start\":26083},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26108,\"start\":26104},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26131,\"start\":26127},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26167,\"start\":26163},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26203,\"start\":26199},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26238,\"start\":26234},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26273,\"start\":26269},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26736,\"start\":26732},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27434,\"start\":27430},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28436,\"start\":28432},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30951,\"start\":30947}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29082,\"start\":28944},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29482,\"start\":29083},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29851,\"start\":29483},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30038,\"start\":29852},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30275,\"start\":30039},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30532,\"start\":30276},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30666,\"start\":30533},{\"attributes\":{\"id\":\"fig_7\"},\"end\":30751,\"start\":30667},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31168,\"start\":30752},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":31245,\"start\":31169},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":32112,\"start\":31246},{\"attributes\":{\"id\":\"fig_0\"},\"end\":29082,\"start\":28944},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29482,\"start\":29083},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29851,\"start\":29483},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30038,\"start\":29852},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30275,\"start\":30039},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30532,\"start\":30276},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30666,\"start\":30533},{\"attributes\":{\"id\":\"fig_7\"},\"end\":30751,\"start\":30667},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31168,\"start\":30752},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":31245,\"start\":31169},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":32112,\"start\":31246}]", "paragraph": "[{\"end\":3213,\"start\":1960},{\"end\":4357,\"start\":3215},{\"end\":4892,\"start\":4359},{\"end\":6048,\"start\":4894},{\"end\":7896,\"start\":6050},{\"end\":8017,\"start\":7928},{\"end\":9497,\"start\":8083},{\"end\":10784,\"start\":9516},{\"end\":11184,\"start\":10786},{\"end\":11269,\"start\":11203},{\"end\":11718,\"start\":11271},{\"end\":12724,\"start\":11761},{\"end\":13358,\"start\":12838},{\"end\":14169,\"start\":13360},{\"end\":14634,\"start\":14202},{\"end\":14778,\"start\":14668},{\"end\":15441,\"start\":14847},{\"end\":17833,\"start\":15482},{\"end\":18809,\"start\":17862},{\"end\":19873,\"start\":18811},{\"end\":21246,\"start\":19875},{\"end\":21493,\"start\":21248},{\"end\":24253,\"start\":21516},{\"end\":27382,\"start\":24255},{\"end\":27592,\"start\":27384},{\"end\":27899,\"start\":27594},{\"end\":28506,\"start\":27901},{\"end\":28943,\"start\":28522},{\"end\":3213,\"start\":1960},{\"end\":4357,\"start\":3215},{\"end\":4892,\"start\":4359},{\"end\":6048,\"start\":4894},{\"end\":7896,\"start\":6050},{\"end\":8017,\"start\":7928},{\"end\":9497,\"start\":8083},{\"end\":10784,\"start\":9516},{\"end\":11184,\"start\":10786},{\"end\":11269,\"start\":11203},{\"end\":11718,\"start\":11271},{\"end\":12724,\"start\":11761},{\"end\":13358,\"start\":12838},{\"end\":14169,\"start\":13360},{\"end\":14634,\"start\":14202},{\"end\":14778,\"start\":14668},{\"end\":15441,\"start\":14847},{\"end\":17833,\"start\":15482},{\"end\":18809,\"start\":17862},{\"end\":19873,\"start\":18811},{\"end\":21246,\"start\":19875},{\"end\":21493,\"start\":21248},{\"end\":24253,\"start\":21516},{\"end\":27382,\"start\":24255},{\"end\":27592,\"start\":27384},{\"end\":27899,\"start\":27594},{\"end\":28506,\"start\":27901},{\"end\":28943,\"start\":28522}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8082,\"start\":8018},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9515,\"start\":9498},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11202,\"start\":11185},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12837,\"start\":12725},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14667,\"start\":14635},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14846,\"start\":14779},{\"attributes\":{\"id\":\"formula_0\"},\"end\":8082,\"start\":8018},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9515,\"start\":9498},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11202,\"start\":11185},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12837,\"start\":12725},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14667,\"start\":14635},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14846,\"start\":14779}]", "table_ref": "[{\"end\":15511,\"start\":15504},{\"end\":17872,\"start\":17865},{\"end\":20049,\"start\":20041},{\"end\":21805,\"start\":21798},{\"end\":22487,\"start\":22480},{\"end\":23514,\"start\":23507},{\"end\":25021,\"start\":25014},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":25651,\"start\":25644},{\"end\":26871,\"start\":26864},{\"end\":27452,\"start\":27445},{\"end\":27920,\"start\":27913},{\"end\":15511,\"start\":15504},{\"end\":17872,\"start\":17865},{\"end\":20049,\"start\":20041},{\"end\":21805,\"start\":21798},{\"end\":22487,\"start\":22480},{\"end\":23514,\"start\":23507},{\"end\":25021,\"start\":25014},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":25651,\"start\":25644},{\"end\":26871,\"start\":26864},{\"end\":27452,\"start\":27445},{\"end\":27920,\"start\":27913}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1958,\"start\":1946},{\"attributes\":{\"n\":\"2.\"},\"end\":7916,\"start\":7899},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7926,\"start\":7919},{\"attributes\":{\"n\":\"2.2.\"},\"end\":11759,\"start\":11721},{\"attributes\":{\"n\":\"2.3.\"},\"end\":14200,\"start\":14172},{\"attributes\":{\"n\":\"3.\"},\"end\":15455,\"start\":15444},{\"attributes\":{\"n\":\"3.1.\"},\"end\":15480,\"start\":15458},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17860,\"start\":17836},{\"attributes\":{\"n\":\"3.3.\"},\"end\":21514,\"start\":21496},{\"attributes\":{\"n\":\"4.\"},\"end\":28520,\"start\":28509},{\"end\":28956,\"start\":28945},{\"end\":29094,\"start\":29084},{\"end\":29494,\"start\":29484},{\"end\":29863,\"start\":29853},{\"end\":30050,\"start\":30040},{\"end\":30287,\"start\":30277},{\"end\":30544,\"start\":30534},{\"end\":30678,\"start\":30668},{\"end\":31179,\"start\":31170},{\"attributes\":{\"n\":\"1.\"},\"end\":1958,\"start\":1946},{\"attributes\":{\"n\":\"2.\"},\"end\":7916,\"start\":7899},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7926,\"start\":7919},{\"attributes\":{\"n\":\"2.2.\"},\"end\":11759,\"start\":11721},{\"attributes\":{\"n\":\"2.3.\"},\"end\":14200,\"start\":14172},{\"attributes\":{\"n\":\"3.\"},\"end\":15455,\"start\":15444},{\"attributes\":{\"n\":\"3.1.\"},\"end\":15480,\"start\":15458},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17860,\"start\":17836},{\"attributes\":{\"n\":\"3.3.\"},\"end\":21514,\"start\":21496},{\"attributes\":{\"n\":\"4.\"},\"end\":28520,\"start\":28509},{\"end\":28956,\"start\":28945},{\"end\":29094,\"start\":29084},{\"end\":29494,\"start\":29484},{\"end\":29863,\"start\":29853},{\"end\":30050,\"start\":30040},{\"end\":30287,\"start\":30277},{\"end\":30544,\"start\":30534},{\"end\":30678,\"start\":30668},{\"end\":31179,\"start\":31170}]", "table": "[{\"end\":32112,\"start\":31774},{\"end\":32112,\"start\":31774}]", "figure_caption": "[{\"end\":29082,\"start\":28958},{\"end\":29482,\"start\":29096},{\"end\":29851,\"start\":29496},{\"end\":30038,\"start\":29865},{\"end\":30275,\"start\":30052},{\"end\":30532,\"start\":30289},{\"end\":30666,\"start\":30546},{\"end\":30751,\"start\":30680},{\"end\":31168,\"start\":30754},{\"end\":31245,\"start\":31181},{\"end\":31774,\"start\":31248},{\"end\":29082,\"start\":28958},{\"end\":29482,\"start\":29096},{\"end\":29851,\"start\":29496},{\"end\":30038,\"start\":29865},{\"end\":30275,\"start\":30052},{\"end\":30532,\"start\":30289},{\"end\":30666,\"start\":30546},{\"end\":30751,\"start\":30680},{\"end\":31168,\"start\":30754},{\"end\":31245,\"start\":31181},{\"end\":31774,\"start\":31248}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2280,\"start\":2272},{\"end\":6257,\"start\":6249},{\"end\":9585,\"start\":9577},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11495,\"start\":11487},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12243,\"start\":12235},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12566,\"start\":12558},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12858,\"start\":12850},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13284,\"start\":13276},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14445,\"start\":14437},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18661,\"start\":18653},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21492,\"start\":21483},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22549,\"start\":22541},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24715,\"start\":24707},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26431,\"start\":26423},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2280,\"start\":2272},{\"end\":6257,\"start\":6249},{\"end\":9585,\"start\":9577},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11495,\"start\":11487},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12243,\"start\":12235},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12566,\"start\":12558},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12858,\"start\":12850},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13284,\"start\":13276},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14445,\"start\":14437},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18661,\"start\":18653},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21492,\"start\":21483},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22549,\"start\":22541},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24715,\"start\":24707},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26431,\"start\":26423}]", "bib_author_first_name": "[{\"end\":32401,\"start\":32395},{\"end\":32415,\"start\":32409},{\"end\":32429,\"start\":32425},{\"end\":32444,\"start\":32438},{\"end\":32460,\"start\":32453},{\"end\":32472,\"start\":32467},{\"end\":32484,\"start\":32480},{\"end\":32486,\"start\":32485},{\"end\":32500,\"start\":32496},{\"end\":32515,\"start\":32508},{\"end\":32530,\"start\":32522},{\"end\":32945,\"start\":32940},{\"end\":32953,\"start\":32951},{\"end\":32965,\"start\":32960},{\"end\":32972,\"start\":32971},{\"end\":32986,\"start\":32980},{\"end\":33288,\"start\":33281},{\"end\":33302,\"start\":33295},{\"end\":33316,\"start\":33309},{\"end\":33600,\"start\":33594},{\"end\":33609,\"start\":33607},{\"end\":33620,\"start\":33614},{\"end\":33628,\"start\":33625},{\"end\":33640,\"start\":33634},{\"end\":33653,\"start\":33647},{\"end\":33667,\"start\":33660},{\"end\":33678,\"start\":33674},{\"end\":33690,\"start\":33683},{\"end\":33703,\"start\":33698},{\"end\":34027,\"start\":34019},{\"end\":34041,\"start\":34034},{\"end\":34056,\"start\":34048},{\"end\":34287,\"start\":34280},{\"end\":34296,\"start\":34293},{\"end\":34310,\"start\":34304},{\"end\":34323,\"start\":34315},{\"end\":34336,\"start\":34328},{\"end\":34545,\"start\":34537},{\"end\":34557,\"start\":34551},{\"end\":34568,\"start\":34563},{\"end\":34747,\"start\":34740},{\"end\":34759,\"start\":34752},{\"end\":34775,\"start\":34767},{\"end\":34785,\"start\":34781},{\"end\":35016,\"start\":35015},{\"end\":35027,\"start\":35023},{\"end\":35041,\"start\":35035},{\"end\":35054,\"start\":35050},{\"end\":35394,\"start\":35388},{\"end\":35411,\"start\":35402},{\"end\":35646,\"start\":35643},{\"end\":35672,\"start\":35671},{\"end\":35687,\"start\":35681},{\"end\":35699,\"start\":35695},{\"end\":35966,\"start\":35959},{\"end\":35978,\"start\":35972},{\"end\":35988,\"start\":35985},{\"end\":36003,\"start\":35994},{\"end\":36014,\"start\":36009},{\"end\":36255,\"start\":36248},{\"end\":36268,\"start\":36261},{\"end\":36278,\"start\":36274},{\"end\":36289,\"start\":36284},{\"end\":36302,\"start\":36295},{\"end\":36309,\"start\":36307},{\"end\":36317,\"start\":36315},{\"end\":36563,\"start\":36556},{\"end\":36576,\"start\":36569},{\"end\":36589,\"start\":36582},{\"end\":36598,\"start\":36594},{\"end\":36610,\"start\":36603},{\"end\":36618,\"start\":36616},{\"end\":36871,\"start\":36864},{\"end\":36884,\"start\":36877},{\"end\":36897,\"start\":36890},{\"end\":36906,\"start\":36902},{\"end\":37065,\"start\":37058},{\"end\":37077,\"start\":37071},{\"end\":37085,\"start\":37083},{\"end\":37094,\"start\":37092},{\"end\":37104,\"start\":37100},{\"end\":37115,\"start\":37111},{\"end\":37123,\"start\":37121},{\"end\":37135,\"start\":37131},{\"end\":37518,\"start\":37511},{\"end\":37532,\"start\":37525},{\"end\":37541,\"start\":37540},{\"end\":37555,\"start\":37549},{\"end\":37567,\"start\":37564},{\"end\":37582,\"start\":37575},{\"end\":37598,\"start\":37597},{\"end\":37616,\"start\":37611},{\"end\":37637,\"start\":37631},{\"end\":37958,\"start\":37949},{\"end\":37981,\"start\":37971},{\"end\":38003,\"start\":37995},{\"end\":38021,\"start\":38013},{\"end\":38033,\"start\":38028},{\"end\":38050,\"start\":38042},{\"end\":38415,\"start\":38407},{\"end\":38426,\"start\":38420},{\"end\":38572,\"start\":38571},{\"end\":38586,\"start\":38580},{\"end\":38601,\"start\":38595},{\"end\":38771,\"start\":38767},{\"end\":38783,\"start\":38780},{\"end\":38798,\"start\":38791},{\"end\":38816,\"start\":38809},{\"end\":38831,\"start\":38825},{\"end\":38845,\"start\":38838},{\"end\":38860,\"start\":38854},{\"end\":38871,\"start\":38866},{\"end\":38887,\"start\":38883},{\"end\":38900,\"start\":38896},{\"end\":39223,\"start\":39216},{\"end\":39239,\"start\":39233},{\"end\":39251,\"start\":39248},{\"end\":39269,\"start\":39263},{\"end\":39286,\"start\":39278},{\"end\":39563,\"start\":39555},{\"end\":39571,\"start\":39568},{\"end\":39775,\"start\":39769},{\"end\":39785,\"start\":39784},{\"end\":39798,\"start\":39794},{\"end\":40057,\"start\":40050},{\"end\":40073,\"start\":40067},{\"end\":40093,\"start\":40088},{\"end\":40322,\"start\":40313},{\"end\":40342,\"start\":40333},{\"end\":40355,\"start\":40349},{\"end\":40367,\"start\":40366},{\"end\":40380,\"start\":40376},{\"end\":40393,\"start\":40388},{\"end\":40395,\"start\":40394},{\"end\":40674,\"start\":40668},{\"end\":40695,\"start\":40687},{\"end\":40697,\"start\":40696},{\"end\":40710,\"start\":40706},{\"end\":40727,\"start\":40723},{\"end\":40745,\"start\":40739},{\"end\":41018,\"start\":41016},{\"end\":41030,\"start\":41024},{\"end\":41045,\"start\":41037},{\"end\":41058,\"start\":41052},{\"end\":41308,\"start\":41303},{\"end\":41322,\"start\":41318},{\"end\":41346,\"start\":41329},{\"end\":41360,\"start\":41356},{\"end\":41618,\"start\":41612},{\"end\":41630,\"start\":41624},{\"end\":41646,\"start\":41638},{\"end\":41660,\"start\":41651},{\"end\":41896,\"start\":41892},{\"end\":41910,\"start\":41903},{\"end\":41922,\"start\":41916},{\"end\":41932,\"start\":41928},{\"end\":42131,\"start\":42127},{\"end\":42143,\"start\":42138},{\"end\":42155,\"start\":42151},{\"end\":42167,\"start\":42163},{\"end\":42169,\"start\":42168},{\"end\":42399,\"start\":42396},{\"end\":42412,\"start\":42406},{\"end\":42424,\"start\":42419},{\"end\":42435,\"start\":42431},{\"end\":42447,\"start\":42440},{\"end\":42458,\"start\":42452},{\"end\":42473,\"start\":42465},{\"end\":42483,\"start\":42480},{\"end\":42765,\"start\":42758},{\"end\":42778,\"start\":42771},{\"end\":42793,\"start\":42786},{\"end\":42800,\"start\":42798},{\"end\":43009,\"start\":43002},{\"end\":43024,\"start\":43020},{\"end\":43041,\"start\":43035},{\"end\":43058,\"start\":43051},{\"end\":43066,\"start\":43065},{\"end\":43079,\"start\":43076},{\"end\":43096,\"start\":43095},{\"end\":43113,\"start\":43112},{\"end\":43125,\"start\":43120},{\"end\":43127,\"start\":43126},{\"end\":43141,\"start\":43134},{\"end\":43484,\"start\":43480},{\"end\":43495,\"start\":43491},{\"end\":43685,\"start\":43680},{\"end\":43695,\"start\":43691},{\"end\":43706,\"start\":43700},{\"end\":43857,\"start\":43852},{\"end\":43869,\"start\":43863},{\"end\":43988,\"start\":43984},{\"end\":43997,\"start\":43993},{\"end\":44011,\"start\":44003},{\"end\":44022,\"start\":44018},{\"end\":44024,\"start\":44023},{\"end\":44306,\"start\":44299},{\"end\":44322,\"start\":44314},{\"end\":44337,\"start\":44330},{\"end\":44344,\"start\":44342},{\"end\":44562,\"start\":44558},{\"end\":44577,\"start\":44570},{\"end\":44591,\"start\":44584},{\"end\":44604,\"start\":44597},{\"end\":44611,\"start\":44609},{\"end\":44870,\"start\":44863},{\"end\":44885,\"start\":44878},{\"end\":45180,\"start\":45173},{\"end\":45195,\"start\":45188},{\"end\":45207,\"start\":45202},{\"end\":32401,\"start\":32395},{\"end\":32415,\"start\":32409},{\"end\":32429,\"start\":32425},{\"end\":32444,\"start\":32438},{\"end\":32460,\"start\":32453},{\"end\":32472,\"start\":32467},{\"end\":32484,\"start\":32480},{\"end\":32486,\"start\":32485},{\"end\":32500,\"start\":32496},{\"end\":32515,\"start\":32508},{\"end\":32530,\"start\":32522},{\"end\":32945,\"start\":32940},{\"end\":32953,\"start\":32951},{\"end\":32965,\"start\":32960},{\"end\":32972,\"start\":32971},{\"end\":32986,\"start\":32980},{\"end\":33288,\"start\":33281},{\"end\":33302,\"start\":33295},{\"end\":33316,\"start\":33309},{\"end\":33600,\"start\":33594},{\"end\":33609,\"start\":33607},{\"end\":33620,\"start\":33614},{\"end\":33628,\"start\":33625},{\"end\":33640,\"start\":33634},{\"end\":33653,\"start\":33647},{\"end\":33667,\"start\":33660},{\"end\":33678,\"start\":33674},{\"end\":33690,\"start\":33683},{\"end\":33703,\"start\":33698},{\"end\":34027,\"start\":34019},{\"end\":34041,\"start\":34034},{\"end\":34056,\"start\":34048},{\"end\":34287,\"start\":34280},{\"end\":34296,\"start\":34293},{\"end\":34310,\"start\":34304},{\"end\":34323,\"start\":34315},{\"end\":34336,\"start\":34328},{\"end\":34545,\"start\":34537},{\"end\":34557,\"start\":34551},{\"end\":34568,\"start\":34563},{\"end\":34747,\"start\":34740},{\"end\":34759,\"start\":34752},{\"end\":34775,\"start\":34767},{\"end\":34785,\"start\":34781},{\"end\":35016,\"start\":35015},{\"end\":35027,\"start\":35023},{\"end\":35041,\"start\":35035},{\"end\":35054,\"start\":35050},{\"end\":35394,\"start\":35388},{\"end\":35411,\"start\":35402},{\"end\":35646,\"start\":35643},{\"end\":35672,\"start\":35671},{\"end\":35687,\"start\":35681},{\"end\":35699,\"start\":35695},{\"end\":35966,\"start\":35959},{\"end\":35978,\"start\":35972},{\"end\":35988,\"start\":35985},{\"end\":36003,\"start\":35994},{\"end\":36014,\"start\":36009},{\"end\":36255,\"start\":36248},{\"end\":36268,\"start\":36261},{\"end\":36278,\"start\":36274},{\"end\":36289,\"start\":36284},{\"end\":36302,\"start\":36295},{\"end\":36309,\"start\":36307},{\"end\":36317,\"start\":36315},{\"end\":36563,\"start\":36556},{\"end\":36576,\"start\":36569},{\"end\":36589,\"start\":36582},{\"end\":36598,\"start\":36594},{\"end\":36610,\"start\":36603},{\"end\":36618,\"start\":36616},{\"end\":36871,\"start\":36864},{\"end\":36884,\"start\":36877},{\"end\":36897,\"start\":36890},{\"end\":36906,\"start\":36902},{\"end\":37065,\"start\":37058},{\"end\":37077,\"start\":37071},{\"end\":37085,\"start\":37083},{\"end\":37094,\"start\":37092},{\"end\":37104,\"start\":37100},{\"end\":37115,\"start\":37111},{\"end\":37123,\"start\":37121},{\"end\":37135,\"start\":37131},{\"end\":37518,\"start\":37511},{\"end\":37532,\"start\":37525},{\"end\":37541,\"start\":37540},{\"end\":37555,\"start\":37549},{\"end\":37567,\"start\":37564},{\"end\":37582,\"start\":37575},{\"end\":37598,\"start\":37597},{\"end\":37616,\"start\":37611},{\"end\":37637,\"start\":37631},{\"end\":37958,\"start\":37949},{\"end\":37981,\"start\":37971},{\"end\":38003,\"start\":37995},{\"end\":38021,\"start\":38013},{\"end\":38033,\"start\":38028},{\"end\":38050,\"start\":38042},{\"end\":38415,\"start\":38407},{\"end\":38426,\"start\":38420},{\"end\":38572,\"start\":38571},{\"end\":38586,\"start\":38580},{\"end\":38601,\"start\":38595},{\"end\":38771,\"start\":38767},{\"end\":38783,\"start\":38780},{\"end\":38798,\"start\":38791},{\"end\":38816,\"start\":38809},{\"end\":38831,\"start\":38825},{\"end\":38845,\"start\":38838},{\"end\":38860,\"start\":38854},{\"end\":38871,\"start\":38866},{\"end\":38887,\"start\":38883},{\"end\":38900,\"start\":38896},{\"end\":39223,\"start\":39216},{\"end\":39239,\"start\":39233},{\"end\":39251,\"start\":39248},{\"end\":39269,\"start\":39263},{\"end\":39286,\"start\":39278},{\"end\":39563,\"start\":39555},{\"end\":39571,\"start\":39568},{\"end\":39775,\"start\":39769},{\"end\":39785,\"start\":39784},{\"end\":39798,\"start\":39794},{\"end\":40057,\"start\":40050},{\"end\":40073,\"start\":40067},{\"end\":40093,\"start\":40088},{\"end\":40322,\"start\":40313},{\"end\":40342,\"start\":40333},{\"end\":40355,\"start\":40349},{\"end\":40367,\"start\":40366},{\"end\":40380,\"start\":40376},{\"end\":40393,\"start\":40388},{\"end\":40395,\"start\":40394},{\"end\":40674,\"start\":40668},{\"end\":40695,\"start\":40687},{\"end\":40697,\"start\":40696},{\"end\":40710,\"start\":40706},{\"end\":40727,\"start\":40723},{\"end\":40745,\"start\":40739},{\"end\":41018,\"start\":41016},{\"end\":41030,\"start\":41024},{\"end\":41045,\"start\":41037},{\"end\":41058,\"start\":41052},{\"end\":41308,\"start\":41303},{\"end\":41322,\"start\":41318},{\"end\":41346,\"start\":41329},{\"end\":41360,\"start\":41356},{\"end\":41618,\"start\":41612},{\"end\":41630,\"start\":41624},{\"end\":41646,\"start\":41638},{\"end\":41660,\"start\":41651},{\"end\":41896,\"start\":41892},{\"end\":41910,\"start\":41903},{\"end\":41922,\"start\":41916},{\"end\":41932,\"start\":41928},{\"end\":42131,\"start\":42127},{\"end\":42143,\"start\":42138},{\"end\":42155,\"start\":42151},{\"end\":42167,\"start\":42163},{\"end\":42169,\"start\":42168},{\"end\":42399,\"start\":42396},{\"end\":42412,\"start\":42406},{\"end\":42424,\"start\":42419},{\"end\":42435,\"start\":42431},{\"end\":42447,\"start\":42440},{\"end\":42458,\"start\":42452},{\"end\":42473,\"start\":42465},{\"end\":42483,\"start\":42480},{\"end\":42765,\"start\":42758},{\"end\":42778,\"start\":42771},{\"end\":42793,\"start\":42786},{\"end\":42800,\"start\":42798},{\"end\":43009,\"start\":43002},{\"end\":43024,\"start\":43020},{\"end\":43041,\"start\":43035},{\"end\":43058,\"start\":43051},{\"end\":43066,\"start\":43065},{\"end\":43079,\"start\":43076},{\"end\":43096,\"start\":43095},{\"end\":43113,\"start\":43112},{\"end\":43125,\"start\":43120},{\"end\":43127,\"start\":43126},{\"end\":43141,\"start\":43134},{\"end\":43484,\"start\":43480},{\"end\":43495,\"start\":43491},{\"end\":43685,\"start\":43680},{\"end\":43695,\"start\":43691},{\"end\":43706,\"start\":43700},{\"end\":43857,\"start\":43852},{\"end\":43869,\"start\":43863},{\"end\":43988,\"start\":43984},{\"end\":43997,\"start\":43993},{\"end\":44011,\"start\":44003},{\"end\":44022,\"start\":44018},{\"end\":44024,\"start\":44023},{\"end\":44306,\"start\":44299},{\"end\":44322,\"start\":44314},{\"end\":44337,\"start\":44330},{\"end\":44344,\"start\":44342},{\"end\":44562,\"start\":44558},{\"end\":44577,\"start\":44570},{\"end\":44591,\"start\":44584},{\"end\":44604,\"start\":44597},{\"end\":44611,\"start\":44609},{\"end\":44870,\"start\":44863},{\"end\":44885,\"start\":44878},{\"end\":45180,\"start\":45173},{\"end\":45195,\"start\":45188},{\"end\":45207,\"start\":45202}]", "bib_author_last_name": "[{\"end\":32407,\"start\":32402},{\"end\":32423,\"start\":32416},{\"end\":32436,\"start\":32430},{\"end\":32451,\"start\":32445},{\"end\":32465,\"start\":32461},{\"end\":32478,\"start\":32473},{\"end\":32494,\"start\":32487},{\"end\":32506,\"start\":32501},{\"end\":32520,\"start\":32516},{\"end\":32536,\"start\":32531},{\"end\":32949,\"start\":32946},{\"end\":32958,\"start\":32954},{\"end\":32969,\"start\":32966},{\"end\":32978,\"start\":32973},{\"end\":32993,\"start\":32987},{\"end\":33004,\"start\":32995},{\"end\":33293,\"start\":33289},{\"end\":33307,\"start\":33303},{\"end\":33319,\"start\":33317},{\"end\":33605,\"start\":33601},{\"end\":33612,\"start\":33610},{\"end\":33623,\"start\":33621},{\"end\":33632,\"start\":33629},{\"end\":33645,\"start\":33641},{\"end\":33658,\"start\":33654},{\"end\":33672,\"start\":33668},{\"end\":33681,\"start\":33679},{\"end\":33696,\"start\":33691},{\"end\":33709,\"start\":33704},{\"end\":34032,\"start\":34028},{\"end\":34046,\"start\":34042},{\"end\":34066,\"start\":34057},{\"end\":34291,\"start\":34288},{\"end\":34302,\"start\":34297},{\"end\":34313,\"start\":34311},{\"end\":34326,\"start\":34324},{\"end\":34340,\"start\":34337},{\"end\":34549,\"start\":34546},{\"end\":34561,\"start\":34558},{\"end\":34572,\"start\":34569},{\"end\":34750,\"start\":34748},{\"end\":34765,\"start\":34760},{\"end\":34779,\"start\":34776},{\"end\":34789,\"start\":34786},{\"end\":35021,\"start\":35017},{\"end\":35033,\"start\":35028},{\"end\":35048,\"start\":35042},{\"end\":35059,\"start\":35055},{\"end\":35075,\"start\":35061},{\"end\":35400,\"start\":35395},{\"end\":35419,\"start\":35412},{\"end\":35669,\"start\":35647},{\"end\":35679,\"start\":35673},{\"end\":35693,\"start\":35688},{\"end\":35706,\"start\":35700},{\"end\":35716,\"start\":35708},{\"end\":35970,\"start\":35967},{\"end\":35983,\"start\":35979},{\"end\":35992,\"start\":35989},{\"end\":36007,\"start\":36004},{\"end\":36020,\"start\":36015},{\"end\":36259,\"start\":36256},{\"end\":36272,\"start\":36269},{\"end\":36282,\"start\":36279},{\"end\":36293,\"start\":36290},{\"end\":36305,\"start\":36303},{\"end\":36313,\"start\":36310},{\"end\":36322,\"start\":36318},{\"end\":36567,\"start\":36564},{\"end\":36580,\"start\":36577},{\"end\":36592,\"start\":36590},{\"end\":36601,\"start\":36599},{\"end\":36614,\"start\":36611},{\"end\":36623,\"start\":36619},{\"end\":36875,\"start\":36872},{\"end\":36888,\"start\":36885},{\"end\":36900,\"start\":36898},{\"end\":36911,\"start\":36907},{\"end\":37069,\"start\":37066},{\"end\":37081,\"start\":37078},{\"end\":37090,\"start\":37086},{\"end\":37098,\"start\":37095},{\"end\":37109,\"start\":37105},{\"end\":37119,\"start\":37116},{\"end\":37129,\"start\":37124},{\"end\":37139,\"start\":37136},{\"end\":37523,\"start\":37519},{\"end\":37538,\"start\":37533},{\"end\":37547,\"start\":37542},{\"end\":37562,\"start\":37556},{\"end\":37573,\"start\":37568},{\"end\":37589,\"start\":37583},{\"end\":37595,\"start\":37591},{\"end\":37603,\"start\":37599},{\"end\":37609,\"start\":37605},{\"end\":37629,\"start\":37617},{\"end\":37646,\"start\":37638},{\"end\":37654,\"start\":37648},{\"end\":37969,\"start\":37959},{\"end\":37993,\"start\":37982},{\"end\":38011,\"start\":38004},{\"end\":38026,\"start\":38022},{\"end\":38040,\"start\":38034},{\"end\":38060,\"start\":38051},{\"end\":38418,\"start\":38416},{\"end\":38434,\"start\":38427},{\"end\":38578,\"start\":38573},{\"end\":38593,\"start\":38587},{\"end\":38609,\"start\":38602},{\"end\":38620,\"start\":38611},{\"end\":38778,\"start\":38772},{\"end\":38789,\"start\":38784},{\"end\":38807,\"start\":38799},{\"end\":38823,\"start\":38817},{\"end\":38836,\"start\":38832},{\"end\":38852,\"start\":38846},{\"end\":38864,\"start\":38861},{\"end\":38881,\"start\":38872},{\"end\":38894,\"start\":38888},{\"end\":38906,\"start\":38901},{\"end\":39231,\"start\":39224},{\"end\":39246,\"start\":39240},{\"end\":39261,\"start\":39252},{\"end\":39276,\"start\":39270},{\"end\":39293,\"start\":39287},{\"end\":39566,\"start\":39564},{\"end\":39577,\"start\":39572},{\"end\":39782,\"start\":39776},{\"end\":39792,\"start\":39786},{\"end\":39807,\"start\":39799},{\"end\":39818,\"start\":39809},{\"end\":40065,\"start\":40058},{\"end\":40086,\"start\":40074},{\"end\":40101,\"start\":40094},{\"end\":40331,\"start\":40323},{\"end\":40347,\"start\":40343},{\"end\":40364,\"start\":40356},{\"end\":40374,\"start\":40368},{\"end\":40386,\"start\":40381},{\"end\":40405,\"start\":40396},{\"end\":40413,\"start\":40407},{\"end\":40685,\"start\":40675},{\"end\":40704,\"start\":40698},{\"end\":40721,\"start\":40711},{\"end\":40737,\"start\":40728},{\"end\":40759,\"start\":40746},{\"end\":41022,\"start\":41019},{\"end\":41035,\"start\":41031},{\"end\":41050,\"start\":41046},{\"end\":41063,\"start\":41059},{\"end\":41316,\"start\":41309},{\"end\":41327,\"start\":41323},{\"end\":41354,\"start\":41347},{\"end\":41365,\"start\":41361},{\"end\":41622,\"start\":41619},{\"end\":41636,\"start\":41631},{\"end\":41649,\"start\":41647},{\"end\":41665,\"start\":41661},{\"end\":41901,\"start\":41897},{\"end\":41914,\"start\":41911},{\"end\":41926,\"start\":41923},{\"end\":41938,\"start\":41933},{\"end\":42136,\"start\":42132},{\"end\":42149,\"start\":42144},{\"end\":42161,\"start\":42156},{\"end\":42176,\"start\":42170},{\"end\":42404,\"start\":42400},{\"end\":42417,\"start\":42413},{\"end\":42429,\"start\":42425},{\"end\":42438,\"start\":42436},{\"end\":42450,\"start\":42448},{\"end\":42463,\"start\":42459},{\"end\":42478,\"start\":42474},{\"end\":42487,\"start\":42484},{\"end\":42769,\"start\":42766},{\"end\":42784,\"start\":42779},{\"end\":42796,\"start\":42794},{\"end\":42805,\"start\":42801},{\"end\":43018,\"start\":43010},{\"end\":43033,\"start\":43025},{\"end\":43049,\"start\":43042},{\"end\":43063,\"start\":43059},{\"end\":43074,\"start\":43067},{\"end\":43085,\"start\":43080},{\"end\":43093,\"start\":43087},{\"end\":43103,\"start\":43097},{\"end\":43110,\"start\":43105},{\"end\":43118,\"start\":43114},{\"end\":43132,\"start\":43128},{\"end\":43148,\"start\":43142},{\"end\":43155,\"start\":43150},{\"end\":43489,\"start\":43485},{\"end\":43503,\"start\":43496},{\"end\":43509,\"start\":43505},{\"end\":43689,\"start\":43686},{\"end\":43698,\"start\":43696},{\"end\":43716,\"start\":43707},{\"end\":43861,\"start\":43858},{\"end\":43879,\"start\":43870},{\"end\":43991,\"start\":43989},{\"end\":44001,\"start\":43998},{\"end\":44016,\"start\":44012},{\"end\":44027,\"start\":44025},{\"end\":44312,\"start\":44307},{\"end\":44328,\"start\":44323},{\"end\":44340,\"start\":44338},{\"end\":44349,\"start\":44345},{\"end\":44568,\"start\":44563},{\"end\":44582,\"start\":44578},{\"end\":44595,\"start\":44592},{\"end\":44607,\"start\":44605},{\"end\":44616,\"start\":44612},{\"end\":44876,\"start\":44871},{\"end\":44890,\"start\":44886},{\"end\":45186,\"start\":45181},{\"end\":45200,\"start\":45196},{\"end\":45210,\"start\":45208},{\"end\":32407,\"start\":32402},{\"end\":32423,\"start\":32416},{\"end\":32436,\"start\":32430},{\"end\":32451,\"start\":32445},{\"end\":32465,\"start\":32461},{\"end\":32478,\"start\":32473},{\"end\":32494,\"start\":32487},{\"end\":32506,\"start\":32501},{\"end\":32520,\"start\":32516},{\"end\":32536,\"start\":32531},{\"end\":32949,\"start\":32946},{\"end\":32958,\"start\":32954},{\"end\":32969,\"start\":32966},{\"end\":32978,\"start\":32973},{\"end\":32993,\"start\":32987},{\"end\":33004,\"start\":32995},{\"end\":33293,\"start\":33289},{\"end\":33307,\"start\":33303},{\"end\":33319,\"start\":33317},{\"end\":33605,\"start\":33601},{\"end\":33612,\"start\":33610},{\"end\":33623,\"start\":33621},{\"end\":33632,\"start\":33629},{\"end\":33645,\"start\":33641},{\"end\":33658,\"start\":33654},{\"end\":33672,\"start\":33668},{\"end\":33681,\"start\":33679},{\"end\":33696,\"start\":33691},{\"end\":33709,\"start\":33704},{\"end\":34032,\"start\":34028},{\"end\":34046,\"start\":34042},{\"end\":34066,\"start\":34057},{\"end\":34291,\"start\":34288},{\"end\":34302,\"start\":34297},{\"end\":34313,\"start\":34311},{\"end\":34326,\"start\":34324},{\"end\":34340,\"start\":34337},{\"end\":34549,\"start\":34546},{\"end\":34561,\"start\":34558},{\"end\":34572,\"start\":34569},{\"end\":34750,\"start\":34748},{\"end\":34765,\"start\":34760},{\"end\":34779,\"start\":34776},{\"end\":34789,\"start\":34786},{\"end\":35021,\"start\":35017},{\"end\":35033,\"start\":35028},{\"end\":35048,\"start\":35042},{\"end\":35059,\"start\":35055},{\"end\":35075,\"start\":35061},{\"end\":35400,\"start\":35395},{\"end\":35419,\"start\":35412},{\"end\":35669,\"start\":35647},{\"end\":35679,\"start\":35673},{\"end\":35693,\"start\":35688},{\"end\":35706,\"start\":35700},{\"end\":35716,\"start\":35708},{\"end\":35970,\"start\":35967},{\"end\":35983,\"start\":35979},{\"end\":35992,\"start\":35989},{\"end\":36007,\"start\":36004},{\"end\":36020,\"start\":36015},{\"end\":36259,\"start\":36256},{\"end\":36272,\"start\":36269},{\"end\":36282,\"start\":36279},{\"end\":36293,\"start\":36290},{\"end\":36305,\"start\":36303},{\"end\":36313,\"start\":36310},{\"end\":36322,\"start\":36318},{\"end\":36567,\"start\":36564},{\"end\":36580,\"start\":36577},{\"end\":36592,\"start\":36590},{\"end\":36601,\"start\":36599},{\"end\":36614,\"start\":36611},{\"end\":36623,\"start\":36619},{\"end\":36875,\"start\":36872},{\"end\":36888,\"start\":36885},{\"end\":36900,\"start\":36898},{\"end\":36911,\"start\":36907},{\"end\":37069,\"start\":37066},{\"end\":37081,\"start\":37078},{\"end\":37090,\"start\":37086},{\"end\":37098,\"start\":37095},{\"end\":37109,\"start\":37105},{\"end\":37119,\"start\":37116},{\"end\":37129,\"start\":37124},{\"end\":37139,\"start\":37136},{\"end\":37523,\"start\":37519},{\"end\":37538,\"start\":37533},{\"end\":37547,\"start\":37542},{\"end\":37562,\"start\":37556},{\"end\":37573,\"start\":37568},{\"end\":37589,\"start\":37583},{\"end\":37595,\"start\":37591},{\"end\":37603,\"start\":37599},{\"end\":37609,\"start\":37605},{\"end\":37629,\"start\":37617},{\"end\":37646,\"start\":37638},{\"end\":37654,\"start\":37648},{\"end\":37969,\"start\":37959},{\"end\":37993,\"start\":37982},{\"end\":38011,\"start\":38004},{\"end\":38026,\"start\":38022},{\"end\":38040,\"start\":38034},{\"end\":38060,\"start\":38051},{\"end\":38418,\"start\":38416},{\"end\":38434,\"start\":38427},{\"end\":38578,\"start\":38573},{\"end\":38593,\"start\":38587},{\"end\":38609,\"start\":38602},{\"end\":38620,\"start\":38611},{\"end\":38778,\"start\":38772},{\"end\":38789,\"start\":38784},{\"end\":38807,\"start\":38799},{\"end\":38823,\"start\":38817},{\"end\":38836,\"start\":38832},{\"end\":38852,\"start\":38846},{\"end\":38864,\"start\":38861},{\"end\":38881,\"start\":38872},{\"end\":38894,\"start\":38888},{\"end\":38906,\"start\":38901},{\"end\":39231,\"start\":39224},{\"end\":39246,\"start\":39240},{\"end\":39261,\"start\":39252},{\"end\":39276,\"start\":39270},{\"end\":39293,\"start\":39287},{\"end\":39566,\"start\":39564},{\"end\":39577,\"start\":39572},{\"end\":39782,\"start\":39776},{\"end\":39792,\"start\":39786},{\"end\":39807,\"start\":39799},{\"end\":39818,\"start\":39809},{\"end\":40065,\"start\":40058},{\"end\":40086,\"start\":40074},{\"end\":40101,\"start\":40094},{\"end\":40331,\"start\":40323},{\"end\":40347,\"start\":40343},{\"end\":40364,\"start\":40356},{\"end\":40374,\"start\":40368},{\"end\":40386,\"start\":40381},{\"end\":40405,\"start\":40396},{\"end\":40413,\"start\":40407},{\"end\":40685,\"start\":40675},{\"end\":40704,\"start\":40698},{\"end\":40721,\"start\":40711},{\"end\":40737,\"start\":40728},{\"end\":40759,\"start\":40746},{\"end\":41022,\"start\":41019},{\"end\":41035,\"start\":41031},{\"end\":41050,\"start\":41046},{\"end\":41063,\"start\":41059},{\"end\":41316,\"start\":41309},{\"end\":41327,\"start\":41323},{\"end\":41354,\"start\":41347},{\"end\":41365,\"start\":41361},{\"end\":41622,\"start\":41619},{\"end\":41636,\"start\":41631},{\"end\":41649,\"start\":41647},{\"end\":41665,\"start\":41661},{\"end\":41901,\"start\":41897},{\"end\":41914,\"start\":41911},{\"end\":41926,\"start\":41923},{\"end\":41938,\"start\":41933},{\"end\":42136,\"start\":42132},{\"end\":42149,\"start\":42144},{\"end\":42161,\"start\":42156},{\"end\":42176,\"start\":42170},{\"end\":42404,\"start\":42400},{\"end\":42417,\"start\":42413},{\"end\":42429,\"start\":42425},{\"end\":42438,\"start\":42436},{\"end\":42450,\"start\":42448},{\"end\":42463,\"start\":42459},{\"end\":42478,\"start\":42474},{\"end\":42487,\"start\":42484},{\"end\":42769,\"start\":42766},{\"end\":42784,\"start\":42779},{\"end\":42796,\"start\":42794},{\"end\":42805,\"start\":42801},{\"end\":43018,\"start\":43010},{\"end\":43033,\"start\":43025},{\"end\":43049,\"start\":43042},{\"end\":43063,\"start\":43059},{\"end\":43074,\"start\":43067},{\"end\":43085,\"start\":43080},{\"end\":43093,\"start\":43087},{\"end\":43103,\"start\":43097},{\"end\":43110,\"start\":43105},{\"end\":43118,\"start\":43114},{\"end\":43132,\"start\":43128},{\"end\":43148,\"start\":43142},{\"end\":43155,\"start\":43150},{\"end\":43489,\"start\":43485},{\"end\":43503,\"start\":43496},{\"end\":43509,\"start\":43505},{\"end\":43689,\"start\":43686},{\"end\":43698,\"start\":43696},{\"end\":43716,\"start\":43707},{\"end\":43861,\"start\":43858},{\"end\":43879,\"start\":43870},{\"end\":43991,\"start\":43989},{\"end\":44001,\"start\":43998},{\"end\":44016,\"start\":44012},{\"end\":44027,\"start\":44025},{\"end\":44312,\"start\":44307},{\"end\":44328,\"start\":44323},{\"end\":44340,\"start\":44338},{\"end\":44349,\"start\":44345},{\"end\":44568,\"start\":44563},{\"end\":44582,\"start\":44578},{\"end\":44595,\"start\":44592},{\"end\":44607,\"start\":44605},{\"end\":44616,\"start\":44612},{\"end\":44876,\"start\":44871},{\"end\":44890,\"start\":44886},{\"end\":45186,\"start\":45181},{\"end\":45200,\"start\":45196},{\"end\":45210,\"start\":45208}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b0\"},\"end\":32875,\"start\":32395},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":216009},\"end\":33174,\"start\":32877},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":23578881},\"end\":33496,\"start\":33176},{\"attributes\":{\"doi\":\"arXiv:1512.01274\",\"id\":\"b3\"},\"end\":33976,\"start\":33498},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":29573685},\"end\":34207,\"start\":33978},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2908606},\"end\":34501,\"start\":34209},{\"attributes\":{\"doi\":\"arXiv:1610.02915\",\"id\":\"b6\"},\"end\":34692,\"start\":34503},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206594692},\"end\":34914,\"start\":34694},{\"attributes\":{\"id\":\"b8\"},\"end\":35292,\"start\":34916},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5808102},\"end\":35575,\"start\":35294},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7811489},\"end\":35891,\"start\":35577},{\"attributes\":{\"doi\":\"arXiv:1506.07310\",\"id\":\"b11\"},\"end\":36198,\"start\":35893},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":43921092},\"end\":36493,\"start\":36200},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206596594},\"end\":36801,\"start\":36495},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1829423},\"end\":37056,\"start\":36803},{\"attributes\":{\"doi\":\"arXiv:1811.07548\",\"id\":\"b15\"},\"end\":37457,\"start\":37058},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":28375094},\"end\":37885,\"start\":37459},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1755257},\"end\":38290,\"start\":37887},{\"attributes\":{\"id\":\"b18\"},\"end\":38349,\"start\":38292},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14599182},\"end\":38546,\"start\":38351},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4637184},\"end\":38727,\"start\":38548},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":40027675},\"end\":39139,\"start\":38729},{\"attributes\":{\"doi\":\"arXiv:1701.06548\",\"id\":\"b22\"},\"end\":39499,\"start\":39141},{\"attributes\":{\"doi\":\"arXiv:1801.05678\",\"id\":\"b23\"},\"end\":39701,\"start\":39501},{\"attributes\":{\"doi\":\"arXiv:1703.09507\",\"id\":\"b24\"},\"end\":39982,\"start\":39703},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206592766},\"end\":40261,\"start\":39984},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6544744},\"end\":40599,\"start\":40263},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6844431},\"end\":40943,\"start\":40601},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1395439},\"end\":41226,\"start\":40945},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2814088},\"end\":41534,\"start\":41228},{\"attributes\":{\"doi\":\"arXiv:1803.02988\",\"id\":\"b30\"},\"end\":41843,\"start\":41536},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":9683805},\"end\":42066,\"start\":41845},{\"attributes\":{\"doi\":\"arXiv:1704.06369\",\"id\":\"b32\"},\"end\":42333,\"start\":42068},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":68589},\"end\":42686,\"start\":42335},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4711865},\"end\":42962,\"start\":42688},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2141447},\"end\":43401,\"start\":42964},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9898157},\"end\":43657,\"start\":43403},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":51875547},\"end\":43807,\"start\":43659},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":50768975},\"end\":43982,\"start\":43809},{\"attributes\":{\"doi\":\"arXiv:1411.7923\",\"id\":\"b39\"},\"end\":44208,\"start\":43984},{\"attributes\":{\"id\":\"b40\"},\"end\":44503,\"start\":44210},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14077437},\"end\":44762,\"start\":44505},{\"attributes\":{\"id\":\"b42\"},\"end\":45074,\"start\":44764},{\"attributes\":{\"doi\":\"arXiv:1708.08197\",\"id\":\"b43\"},\"end\":45400,\"start\":45076},{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b0\"},\"end\":32875,\"start\":32395},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":216009},\"end\":33174,\"start\":32877},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":23578881},\"end\":33496,\"start\":33176},{\"attributes\":{\"doi\":\"arXiv:1512.01274\",\"id\":\"b3\"},\"end\":33976,\"start\":33498},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":29573685},\"end\":34207,\"start\":33978},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2908606},\"end\":34501,\"start\":34209},{\"attributes\":{\"doi\":\"arXiv:1610.02915\",\"id\":\"b6\"},\"end\":34692,\"start\":34503},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206594692},\"end\":34914,\"start\":34694},{\"attributes\":{\"id\":\"b8\"},\"end\":35292,\"start\":34916},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5808102},\"end\":35575,\"start\":35294},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7811489},\"end\":35891,\"start\":35577},{\"attributes\":{\"doi\":\"arXiv:1506.07310\",\"id\":\"b11\"},\"end\":36198,\"start\":35893},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":43921092},\"end\":36493,\"start\":36200},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206596594},\"end\":36801,\"start\":36495},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1829423},\"end\":37056,\"start\":36803},{\"attributes\":{\"doi\":\"arXiv:1811.07548\",\"id\":\"b15\"},\"end\":37457,\"start\":37058},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":28375094},\"end\":37885,\"start\":37459},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1755257},\"end\":38290,\"start\":37887},{\"attributes\":{\"id\":\"b18\"},\"end\":38349,\"start\":38292},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14599182},\"end\":38546,\"start\":38351},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4637184},\"end\":38727,\"start\":38548},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":40027675},\"end\":39139,\"start\":38729},{\"attributes\":{\"doi\":\"arXiv:1701.06548\",\"id\":\"b22\"},\"end\":39499,\"start\":39141},{\"attributes\":{\"doi\":\"arXiv:1801.05678\",\"id\":\"b23\"},\"end\":39701,\"start\":39501},{\"attributes\":{\"doi\":\"arXiv:1703.09507\",\"id\":\"b24\"},\"end\":39982,\"start\":39703},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206592766},\"end\":40261,\"start\":39984},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6544744},\"end\":40599,\"start\":40263},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6844431},\"end\":40943,\"start\":40601},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1395439},\"end\":41226,\"start\":40945},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2814088},\"end\":41534,\"start\":41228},{\"attributes\":{\"doi\":\"arXiv:1803.02988\",\"id\":\"b30\"},\"end\":41843,\"start\":41536},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":9683805},\"end\":42066,\"start\":41845},{\"attributes\":{\"doi\":\"arXiv:1704.06369\",\"id\":\"b32\"},\"end\":42333,\"start\":42068},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":68589},\"end\":42686,\"start\":42335},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4711865},\"end\":42962,\"start\":42688},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2141447},\"end\":43401,\"start\":42964},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9898157},\"end\":43657,\"start\":43403},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":51875547},\"end\":43807,\"start\":43659},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":50768975},\"end\":43982,\"start\":43809},{\"attributes\":{\"doi\":\"arXiv:1411.7923\",\"id\":\"b39\"},\"end\":44208,\"start\":43984},{\"attributes\":{\"id\":\"b40\"},\"end\":44503,\"start\":44210},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14077437},\"end\":44762,\"start\":44505},{\"attributes\":{\"id\":\"b42\"},\"end\":45074,\"start\":44764},{\"attributes\":{\"doi\":\"arXiv:1708.08197\",\"id\":\"b43\"},\"end\":45400,\"start\":45076}]", "bib_title": "[{\"end\":32938,\"start\":32877},{\"end\":33279,\"start\":33176},{\"end\":34017,\"start\":33978},{\"end\":34278,\"start\":34209},{\"end\":34738,\"start\":34694},{\"end\":35386,\"start\":35294},{\"end\":35641,\"start\":35577},{\"end\":36246,\"start\":36200},{\"end\":36554,\"start\":36495},{\"end\":36862,\"start\":36803},{\"end\":37509,\"start\":37459},{\"end\":37947,\"start\":37887},{\"end\":38405,\"start\":38351},{\"end\":38569,\"start\":38548},{\"end\":38765,\"start\":38729},{\"end\":40048,\"start\":39984},{\"end\":40311,\"start\":40263},{\"end\":40666,\"start\":40601},{\"end\":41014,\"start\":40945},{\"end\":41301,\"start\":41228},{\"end\":41890,\"start\":41845},{\"end\":42394,\"start\":42335},{\"end\":42756,\"start\":42688},{\"end\":43000,\"start\":42964},{\"end\":43478,\"start\":43403},{\"end\":43678,\"start\":43659},{\"end\":43850,\"start\":43809},{\"end\":44556,\"start\":44505},{\"end\":32938,\"start\":32877},{\"end\":33279,\"start\":33176},{\"end\":34017,\"start\":33978},{\"end\":34278,\"start\":34209},{\"end\":34738,\"start\":34694},{\"end\":35386,\"start\":35294},{\"end\":35641,\"start\":35577},{\"end\":36246,\"start\":36200},{\"end\":36554,\"start\":36495},{\"end\":36862,\"start\":36803},{\"end\":37509,\"start\":37459},{\"end\":37947,\"start\":37887},{\"end\":38405,\"start\":38351},{\"end\":38569,\"start\":38548},{\"end\":38765,\"start\":38729},{\"end\":40048,\"start\":39984},{\"end\":40311,\"start\":40263},{\"end\":40666,\"start\":40601},{\"end\":41014,\"start\":40945},{\"end\":41301,\"start\":41228},{\"end\":41890,\"start\":41845},{\"end\":42394,\"start\":42335},{\"end\":42756,\"start\":42688},{\"end\":43000,\"start\":42964},{\"end\":43478,\"start\":43403},{\"end\":43678,\"start\":43659},{\"end\":43850,\"start\":43809},{\"end\":44556,\"start\":44505}]", "bib_author": "[{\"end\":32409,\"start\":32395},{\"end\":32425,\"start\":32409},{\"end\":32438,\"start\":32425},{\"end\":32453,\"start\":32438},{\"end\":32467,\"start\":32453},{\"end\":32480,\"start\":32467},{\"end\":32496,\"start\":32480},{\"end\":32508,\"start\":32496},{\"end\":32522,\"start\":32508},{\"end\":32538,\"start\":32522},{\"end\":32951,\"start\":32940},{\"end\":32960,\"start\":32951},{\"end\":32971,\"start\":32960},{\"end\":32980,\"start\":32971},{\"end\":32995,\"start\":32980},{\"end\":33006,\"start\":32995},{\"end\":33295,\"start\":33281},{\"end\":33309,\"start\":33295},{\"end\":33321,\"start\":33309},{\"end\":33607,\"start\":33594},{\"end\":33614,\"start\":33607},{\"end\":33625,\"start\":33614},{\"end\":33634,\"start\":33625},{\"end\":33647,\"start\":33634},{\"end\":33660,\"start\":33647},{\"end\":33674,\"start\":33660},{\"end\":33683,\"start\":33674},{\"end\":33698,\"start\":33683},{\"end\":33711,\"start\":33698},{\"end\":34034,\"start\":34019},{\"end\":34048,\"start\":34034},{\"end\":34068,\"start\":34048},{\"end\":34293,\"start\":34280},{\"end\":34304,\"start\":34293},{\"end\":34315,\"start\":34304},{\"end\":34328,\"start\":34315},{\"end\":34342,\"start\":34328},{\"end\":34551,\"start\":34537},{\"end\":34563,\"start\":34551},{\"end\":34574,\"start\":34563},{\"end\":34752,\"start\":34740},{\"end\":34767,\"start\":34752},{\"end\":34781,\"start\":34767},{\"end\":34791,\"start\":34781},{\"end\":35023,\"start\":35015},{\"end\":35035,\"start\":35023},{\"end\":35050,\"start\":35035},{\"end\":35061,\"start\":35050},{\"end\":35077,\"start\":35061},{\"end\":35402,\"start\":35388},{\"end\":35421,\"start\":35402},{\"end\":35671,\"start\":35643},{\"end\":35681,\"start\":35671},{\"end\":35695,\"start\":35681},{\"end\":35708,\"start\":35695},{\"end\":35718,\"start\":35708},{\"end\":35972,\"start\":35959},{\"end\":35985,\"start\":35972},{\"end\":35994,\"start\":35985},{\"end\":36009,\"start\":35994},{\"end\":36022,\"start\":36009},{\"end\":36261,\"start\":36248},{\"end\":36274,\"start\":36261},{\"end\":36284,\"start\":36274},{\"end\":36295,\"start\":36284},{\"end\":36307,\"start\":36295},{\"end\":36315,\"start\":36307},{\"end\":36324,\"start\":36315},{\"end\":36569,\"start\":36556},{\"end\":36582,\"start\":36569},{\"end\":36594,\"start\":36582},{\"end\":36603,\"start\":36594},{\"end\":36616,\"start\":36603},{\"end\":36625,\"start\":36616},{\"end\":36877,\"start\":36864},{\"end\":36890,\"start\":36877},{\"end\":36902,\"start\":36890},{\"end\":36913,\"start\":36902},{\"end\":37071,\"start\":37058},{\"end\":37083,\"start\":37071},{\"end\":37092,\"start\":37083},{\"end\":37100,\"start\":37092},{\"end\":37111,\"start\":37100},{\"end\":37121,\"start\":37111},{\"end\":37131,\"start\":37121},{\"end\":37141,\"start\":37131},{\"end\":37525,\"start\":37511},{\"end\":37540,\"start\":37525},{\"end\":37549,\"start\":37540},{\"end\":37564,\"start\":37549},{\"end\":37575,\"start\":37564},{\"end\":37591,\"start\":37575},{\"end\":37597,\"start\":37591},{\"end\":37605,\"start\":37597},{\"end\":37611,\"start\":37605},{\"end\":37631,\"start\":37611},{\"end\":37648,\"start\":37631},{\"end\":37656,\"start\":37648},{\"end\":37971,\"start\":37949},{\"end\":37995,\"start\":37971},{\"end\":38013,\"start\":37995},{\"end\":38028,\"start\":38013},{\"end\":38042,\"start\":38028},{\"end\":38062,\"start\":38042},{\"end\":38420,\"start\":38407},{\"end\":38436,\"start\":38420},{\"end\":38580,\"start\":38571},{\"end\":38595,\"start\":38580},{\"end\":38611,\"start\":38595},{\"end\":38622,\"start\":38611},{\"end\":38780,\"start\":38767},{\"end\":38791,\"start\":38780},{\"end\":38809,\"start\":38791},{\"end\":38825,\"start\":38809},{\"end\":38838,\"start\":38825},{\"end\":38854,\"start\":38838},{\"end\":38866,\"start\":38854},{\"end\":38883,\"start\":38866},{\"end\":38896,\"start\":38883},{\"end\":38908,\"start\":38896},{\"end\":39233,\"start\":39216},{\"end\":39248,\"start\":39233},{\"end\":39263,\"start\":39248},{\"end\":39278,\"start\":39263},{\"end\":39295,\"start\":39278},{\"end\":39568,\"start\":39555},{\"end\":39579,\"start\":39568},{\"end\":39784,\"start\":39769},{\"end\":39794,\"start\":39784},{\"end\":39809,\"start\":39794},{\"end\":39820,\"start\":39809},{\"end\":40067,\"start\":40050},{\"end\":40088,\"start\":40067},{\"end\":40103,\"start\":40088},{\"end\":40333,\"start\":40313},{\"end\":40349,\"start\":40333},{\"end\":40366,\"start\":40349},{\"end\":40376,\"start\":40366},{\"end\":40388,\"start\":40376},{\"end\":40407,\"start\":40388},{\"end\":40415,\"start\":40407},{\"end\":40687,\"start\":40668},{\"end\":40706,\"start\":40687},{\"end\":40723,\"start\":40706},{\"end\":40739,\"start\":40723},{\"end\":40761,\"start\":40739},{\"end\":41024,\"start\":41016},{\"end\":41037,\"start\":41024},{\"end\":41052,\"start\":41037},{\"end\":41065,\"start\":41052},{\"end\":41318,\"start\":41303},{\"end\":41329,\"start\":41318},{\"end\":41356,\"start\":41329},{\"end\":41367,\"start\":41356},{\"end\":41624,\"start\":41612},{\"end\":41638,\"start\":41624},{\"end\":41651,\"start\":41638},{\"end\":41667,\"start\":41651},{\"end\":41903,\"start\":41892},{\"end\":41916,\"start\":41903},{\"end\":41928,\"start\":41916},{\"end\":41940,\"start\":41928},{\"end\":42138,\"start\":42127},{\"end\":42151,\"start\":42138},{\"end\":42163,\"start\":42151},{\"end\":42178,\"start\":42163},{\"end\":42406,\"start\":42396},{\"end\":42419,\"start\":42406},{\"end\":42431,\"start\":42419},{\"end\":42440,\"start\":42431},{\"end\":42452,\"start\":42440},{\"end\":42465,\"start\":42452},{\"end\":42480,\"start\":42465},{\"end\":42489,\"start\":42480},{\"end\":42771,\"start\":42758},{\"end\":42786,\"start\":42771},{\"end\":42798,\"start\":42786},{\"end\":42807,\"start\":42798},{\"end\":43020,\"start\":43002},{\"end\":43035,\"start\":43020},{\"end\":43051,\"start\":43035},{\"end\":43065,\"start\":43051},{\"end\":43076,\"start\":43065},{\"end\":43087,\"start\":43076},{\"end\":43095,\"start\":43087},{\"end\":43105,\"start\":43095},{\"end\":43112,\"start\":43105},{\"end\":43120,\"start\":43112},{\"end\":43134,\"start\":43120},{\"end\":43150,\"start\":43134},{\"end\":43157,\"start\":43150},{\"end\":43491,\"start\":43480},{\"end\":43505,\"start\":43491},{\"end\":43511,\"start\":43505},{\"end\":43691,\"start\":43680},{\"end\":43700,\"start\":43691},{\"end\":43718,\"start\":43700},{\"end\":43863,\"start\":43852},{\"end\":43881,\"start\":43863},{\"end\":43993,\"start\":43984},{\"end\":44003,\"start\":43993},{\"end\":44018,\"start\":44003},{\"end\":44029,\"start\":44018},{\"end\":44314,\"start\":44299},{\"end\":44330,\"start\":44314},{\"end\":44342,\"start\":44330},{\"end\":44351,\"start\":44342},{\"end\":44570,\"start\":44558},{\"end\":44584,\"start\":44570},{\"end\":44597,\"start\":44584},{\"end\":44609,\"start\":44597},{\"end\":44618,\"start\":44609},{\"end\":44878,\"start\":44863},{\"end\":44892,\"start\":44878},{\"end\":45188,\"start\":45173},{\"end\":45202,\"start\":45188},{\"end\":45212,\"start\":45202},{\"end\":32409,\"start\":32395},{\"end\":32425,\"start\":32409},{\"end\":32438,\"start\":32425},{\"end\":32453,\"start\":32438},{\"end\":32467,\"start\":32453},{\"end\":32480,\"start\":32467},{\"end\":32496,\"start\":32480},{\"end\":32508,\"start\":32496},{\"end\":32522,\"start\":32508},{\"end\":32538,\"start\":32522},{\"end\":32951,\"start\":32940},{\"end\":32960,\"start\":32951},{\"end\":32971,\"start\":32960},{\"end\":32980,\"start\":32971},{\"end\":32995,\"start\":32980},{\"end\":33006,\"start\":32995},{\"end\":33295,\"start\":33281},{\"end\":33309,\"start\":33295},{\"end\":33321,\"start\":33309},{\"end\":33607,\"start\":33594},{\"end\":33614,\"start\":33607},{\"end\":33625,\"start\":33614},{\"end\":33634,\"start\":33625},{\"end\":33647,\"start\":33634},{\"end\":33660,\"start\":33647},{\"end\":33674,\"start\":33660},{\"end\":33683,\"start\":33674},{\"end\":33698,\"start\":33683},{\"end\":33711,\"start\":33698},{\"end\":34034,\"start\":34019},{\"end\":34048,\"start\":34034},{\"end\":34068,\"start\":34048},{\"end\":34293,\"start\":34280},{\"end\":34304,\"start\":34293},{\"end\":34315,\"start\":34304},{\"end\":34328,\"start\":34315},{\"end\":34342,\"start\":34328},{\"end\":34551,\"start\":34537},{\"end\":34563,\"start\":34551},{\"end\":34574,\"start\":34563},{\"end\":34752,\"start\":34740},{\"end\":34767,\"start\":34752},{\"end\":34781,\"start\":34767},{\"end\":34791,\"start\":34781},{\"end\":35023,\"start\":35015},{\"end\":35035,\"start\":35023},{\"end\":35050,\"start\":35035},{\"end\":35061,\"start\":35050},{\"end\":35077,\"start\":35061},{\"end\":35402,\"start\":35388},{\"end\":35421,\"start\":35402},{\"end\":35671,\"start\":35643},{\"end\":35681,\"start\":35671},{\"end\":35695,\"start\":35681},{\"end\":35708,\"start\":35695},{\"end\":35718,\"start\":35708},{\"end\":35972,\"start\":35959},{\"end\":35985,\"start\":35972},{\"end\":35994,\"start\":35985},{\"end\":36009,\"start\":35994},{\"end\":36022,\"start\":36009},{\"end\":36261,\"start\":36248},{\"end\":36274,\"start\":36261},{\"end\":36284,\"start\":36274},{\"end\":36295,\"start\":36284},{\"end\":36307,\"start\":36295},{\"end\":36315,\"start\":36307},{\"end\":36324,\"start\":36315},{\"end\":36569,\"start\":36556},{\"end\":36582,\"start\":36569},{\"end\":36594,\"start\":36582},{\"end\":36603,\"start\":36594},{\"end\":36616,\"start\":36603},{\"end\":36625,\"start\":36616},{\"end\":36877,\"start\":36864},{\"end\":36890,\"start\":36877},{\"end\":36902,\"start\":36890},{\"end\":36913,\"start\":36902},{\"end\":37071,\"start\":37058},{\"end\":37083,\"start\":37071},{\"end\":37092,\"start\":37083},{\"end\":37100,\"start\":37092},{\"end\":37111,\"start\":37100},{\"end\":37121,\"start\":37111},{\"end\":37131,\"start\":37121},{\"end\":37141,\"start\":37131},{\"end\":37525,\"start\":37511},{\"end\":37540,\"start\":37525},{\"end\":37549,\"start\":37540},{\"end\":37564,\"start\":37549},{\"end\":37575,\"start\":37564},{\"end\":37591,\"start\":37575},{\"end\":37597,\"start\":37591},{\"end\":37605,\"start\":37597},{\"end\":37611,\"start\":37605},{\"end\":37631,\"start\":37611},{\"end\":37648,\"start\":37631},{\"end\":37656,\"start\":37648},{\"end\":37971,\"start\":37949},{\"end\":37995,\"start\":37971},{\"end\":38013,\"start\":37995},{\"end\":38028,\"start\":38013},{\"end\":38042,\"start\":38028},{\"end\":38062,\"start\":38042},{\"end\":38420,\"start\":38407},{\"end\":38436,\"start\":38420},{\"end\":38580,\"start\":38571},{\"end\":38595,\"start\":38580},{\"end\":38611,\"start\":38595},{\"end\":38622,\"start\":38611},{\"end\":38780,\"start\":38767},{\"end\":38791,\"start\":38780},{\"end\":38809,\"start\":38791},{\"end\":38825,\"start\":38809},{\"end\":38838,\"start\":38825},{\"end\":38854,\"start\":38838},{\"end\":38866,\"start\":38854},{\"end\":38883,\"start\":38866},{\"end\":38896,\"start\":38883},{\"end\":38908,\"start\":38896},{\"end\":39233,\"start\":39216},{\"end\":39248,\"start\":39233},{\"end\":39263,\"start\":39248},{\"end\":39278,\"start\":39263},{\"end\":39295,\"start\":39278},{\"end\":39568,\"start\":39555},{\"end\":39579,\"start\":39568},{\"end\":39784,\"start\":39769},{\"end\":39794,\"start\":39784},{\"end\":39809,\"start\":39794},{\"end\":39820,\"start\":39809},{\"end\":40067,\"start\":40050},{\"end\":40088,\"start\":40067},{\"end\":40103,\"start\":40088},{\"end\":40333,\"start\":40313},{\"end\":40349,\"start\":40333},{\"end\":40366,\"start\":40349},{\"end\":40376,\"start\":40366},{\"end\":40388,\"start\":40376},{\"end\":40407,\"start\":40388},{\"end\":40415,\"start\":40407},{\"end\":40687,\"start\":40668},{\"end\":40706,\"start\":40687},{\"end\":40723,\"start\":40706},{\"end\":40739,\"start\":40723},{\"end\":40761,\"start\":40739},{\"end\":41024,\"start\":41016},{\"end\":41037,\"start\":41024},{\"end\":41052,\"start\":41037},{\"end\":41065,\"start\":41052},{\"end\":41318,\"start\":41303},{\"end\":41329,\"start\":41318},{\"end\":41356,\"start\":41329},{\"end\":41367,\"start\":41356},{\"end\":41624,\"start\":41612},{\"end\":41638,\"start\":41624},{\"end\":41651,\"start\":41638},{\"end\":41667,\"start\":41651},{\"end\":41903,\"start\":41892},{\"end\":41916,\"start\":41903},{\"end\":41928,\"start\":41916},{\"end\":41940,\"start\":41928},{\"end\":42138,\"start\":42127},{\"end\":42151,\"start\":42138},{\"end\":42163,\"start\":42151},{\"end\":42178,\"start\":42163},{\"end\":42406,\"start\":42396},{\"end\":42419,\"start\":42406},{\"end\":42431,\"start\":42419},{\"end\":42440,\"start\":42431},{\"end\":42452,\"start\":42440},{\"end\":42465,\"start\":42452},{\"end\":42480,\"start\":42465},{\"end\":42489,\"start\":42480},{\"end\":42771,\"start\":42758},{\"end\":42786,\"start\":42771},{\"end\":42798,\"start\":42786},{\"end\":42807,\"start\":42798},{\"end\":43020,\"start\":43002},{\"end\":43035,\"start\":43020},{\"end\":43051,\"start\":43035},{\"end\":43065,\"start\":43051},{\"end\":43076,\"start\":43065},{\"end\":43087,\"start\":43076},{\"end\":43095,\"start\":43087},{\"end\":43105,\"start\":43095},{\"end\":43112,\"start\":43105},{\"end\":43120,\"start\":43112},{\"end\":43134,\"start\":43120},{\"end\":43150,\"start\":43134},{\"end\":43157,\"start\":43150},{\"end\":43491,\"start\":43480},{\"end\":43505,\"start\":43491},{\"end\":43511,\"start\":43505},{\"end\":43691,\"start\":43680},{\"end\":43700,\"start\":43691},{\"end\":43718,\"start\":43700},{\"end\":43863,\"start\":43852},{\"end\":43881,\"start\":43863},{\"end\":43993,\"start\":43984},{\"end\":44003,\"start\":43993},{\"end\":44018,\"start\":44003},{\"end\":44029,\"start\":44018},{\"end\":44314,\"start\":44299},{\"end\":44330,\"start\":44314},{\"end\":44342,\"start\":44330},{\"end\":44351,\"start\":44342},{\"end\":44570,\"start\":44558},{\"end\":44584,\"start\":44570},{\"end\":44597,\"start\":44584},{\"end\":44609,\"start\":44597},{\"end\":44618,\"start\":44609},{\"end\":44878,\"start\":44863},{\"end\":44892,\"start\":44878},{\"end\":45188,\"start\":45173},{\"end\":45202,\"start\":45188},{\"end\":45212,\"start\":45202}]", "bib_venue": "[{\"end\":32619,\"start\":32554},{\"end\":33008,\"start\":33006},{\"end\":33325,\"start\":33321},{\"end\":33592,\"start\":33498},{\"end\":34081,\"start\":34068},{\"end\":34346,\"start\":34342},{\"end\":34535,\"start\":34503},{\"end\":34795,\"start\":34791},{\"end\":35013,\"start\":34916},{\"end\":35425,\"start\":35421},{\"end\":35722,\"start\":35718},{\"end\":35957,\"start\":35893},{\"end\":36334,\"start\":36324},{\"end\":36629,\"start\":36625},{\"end\":36917,\"start\":36913},{\"end\":37248,\"start\":37157},{\"end\":37662,\"start\":37656},{\"end\":38075,\"start\":38062},{\"end\":38303,\"start\":38292},{\"end\":38440,\"start\":38436},{\"end\":38626,\"start\":38622},{\"end\":38924,\"start\":38908},{\"end\":39214,\"start\":39141},{\"end\":39553,\"start\":39501},{\"end\":39767,\"start\":39703},{\"end\":40107,\"start\":40103},{\"end\":40419,\"start\":40415},{\"end\":40764,\"start\":40761},{\"end\":41072,\"start\":41065},{\"end\":41371,\"start\":41367},{\"end\":41610,\"start\":41536},{\"end\":41943,\"start\":41940},{\"end\":42125,\"start\":42068},{\"end\":42493,\"start\":42489},{\"end\":42811,\"start\":42807},{\"end\":43170,\"start\":43157},{\"end\":43515,\"start\":43511},{\"end\":43722,\"start\":43718},{\"end\":43885,\"start\":43881},{\"end\":44085,\"start\":44044},{\"end\":44297,\"start\":44210},{\"end\":44622,\"start\":44618},{\"end\":44861,\"start\":44764},{\"end\":45171,\"start\":45076},{\"end\":32619,\"start\":32554},{\"end\":33008,\"start\":33006},{\"end\":33325,\"start\":33321},{\"end\":33592,\"start\":33498},{\"end\":34081,\"start\":34068},{\"end\":34346,\"start\":34342},{\"end\":34535,\"start\":34503},{\"end\":34795,\"start\":34791},{\"end\":35013,\"start\":34916},{\"end\":35425,\"start\":35421},{\"end\":35722,\"start\":35718},{\"end\":35957,\"start\":35893},{\"end\":36334,\"start\":36324},{\"end\":36629,\"start\":36625},{\"end\":36917,\"start\":36913},{\"end\":37248,\"start\":37157},{\"end\":37662,\"start\":37656},{\"end\":38075,\"start\":38062},{\"end\":38303,\"start\":38292},{\"end\":38440,\"start\":38436},{\"end\":38626,\"start\":38622},{\"end\":38924,\"start\":38908},{\"end\":39214,\"start\":39141},{\"end\":39553,\"start\":39501},{\"end\":39767,\"start\":39703},{\"end\":40107,\"start\":40103},{\"end\":40419,\"start\":40415},{\"end\":40764,\"start\":40761},{\"end\":41072,\"start\":41065},{\"end\":41371,\"start\":41367},{\"end\":41610,\"start\":41536},{\"end\":41943,\"start\":41940},{\"end\":42125,\"start\":42068},{\"end\":42493,\"start\":42489},{\"end\":42811,\"start\":42807},{\"end\":43170,\"start\":43157},{\"end\":43515,\"start\":43511},{\"end\":43722,\"start\":43718},{\"end\":43885,\"start\":43881},{\"end\":44085,\"start\":44044},{\"end\":44297,\"start\":44210},{\"end\":44622,\"start\":44618},{\"end\":44861,\"start\":44764},{\"end\":45171,\"start\":45076}]"}}}, "year": 2023, "month": 12, "day": 17}