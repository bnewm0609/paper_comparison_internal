{"id": 202774172, "updated": "2022-02-04 11:48:25.417", "metadata": {"title": "Face Reconstruction from Voice using Generative Adversarial Networks", "authors": "[{\"middle\":[],\"last\":\"Wen\",\"first\":\"Yandong\"}]", "venue": "NeurIPS", "journal": "5266-5275", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Voice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling reconstructing someone\u2019s face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity? To address this problem, we propose a simple but effective computational framework based on generative adversarial networks (GANs). The network learns to generate faces from voices by matching the identities of generated faces to those of the speakers, on a training set. We evaluate the performance of the network by leveraging a closely related task cross-modal matching. The results show that our model is able to generate faces that match several biometric characteristics of the speaker, and results in matching accuracies that are much better than chance. The code is publicly available in https://github.com/cmu-mlsp/reconstructing_faces_from_voices", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2970903655", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/WenRS19", "doi": null}}, "content": {"source": {"pdf_hash": "8dd85b51f92f53aef218cc2498d7cef887da9ced", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "018aee0b41f7ee5297754d35af8bdce020b864c6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8dd85b51f92f53aef218cc2498d7cef887da9ced.txt", "contents": "\nFace Reconstruction from Voice using Generative Adversarial Networks\n\n\nYandong Wen yandongw@andrew.cmu.edu \nCarnegie Mellon University Pittsburgh\nCarnegie Mellon University Pittsburgh\nCarnegie Mellon University Pittsburgh\n15213, 15213, 15213PA, PA, PA\n\nRita Singh rsingh@cs.cmu.edu \nCarnegie Mellon University Pittsburgh\nCarnegie Mellon University Pittsburgh\nCarnegie Mellon University Pittsburgh\n15213, 15213, 15213PA, PA, PA\n\nBhiksha Raj bhiksha@cs.cmu.edu \nCarnegie Mellon University Pittsburgh\nCarnegie Mellon University Pittsburgh\nCarnegie Mellon University Pittsburgh\n15213, 15213, 15213PA, PA, PA\n\nFace Reconstruction from Voice using Generative Adversarial Networks\n\nVoice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling -reconstructing someone's face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity? To address this problem, we propose a simple but effective computational framework based on generative adversarial networks (GANs). The network learns to generate faces from voices by matching the identities of generated faces to those of the speakers, on a training set. We evaluate the performance of the network by leveraging a closely related task -cross-modal matching. The results show that our model is able to generate faces that match several biometric characteristics of the speaker, and results in matching accuracies that are much better than chance. The code is publicly available in\n\nIntroduction\n\nThe challenge of voice profiling is to infer a person's biophysical parameters, such as gender, age, health conditions, etc. from their speech, and a large body of literature exists on the topic [28,1,21]. In this paper, we extend this challenge and address the problem: is it possible to go beyond merely predicting a person's physical attributes, and actually reconstruct their entire face from their voice? Effectively a new subtask of voice profiling, the task is designed to answer the question: given an unheard audio clip spoken by an unseen person, can we picture a face image that has as many as possible associations with the speaker in terms of identity?\n\nA person's voice is incontrovertibly statistically related to their facial structure. The relationship is, in fact, multi-faceted. Direct relationships include the effect of the underlying skeletal and articulator structure of the face and the tissue covering them, all of which govern the shapes, sizes, and acoustic properties of the vocal tract that produces the voice [22,38]. Less directly, the same genetic, physical and environmental influences that affect the development of the face also affect the voice. Demographic factors, such as gender, age and ethnicity too influence both voice and face (and can in fact be independently inferred from the voice [1,15] or the face [17]), providing additional links between the two.\n\nNeurocognitive studies have shown that human perception implicitly recognizes the association of faces to voices [4]. Studies indicate that neuro-cognitive pathways for voices share a common structure with that for faces [7] -the two may follow parallel pathways within a common recognition framework [4,3]. In empirical studies humans have shown the ability to associate voices of unknown individuals to pictures of their faces [11,19]. They are seen to show improved ability to memorize 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. Figure 1: The proposed GANs-based framework for generating faces from voices. It includes 4 major components: voice embedding network, generator, discriminator, and classifier. and recall voices when previously shown pictures of the speaker's face, but not imposter faces [20,32,31].\n\nGiven these demonstrable dependencies, it is reasonable to hypothesize that it may also be possible to reconstruct face images from a voice signal algorithmically.\n\nOn the other hand, reconstructing the face from voice is a challenging, maybe even impossible task for several reasons [35]. First, it is an ill-posed cross-modal problem : although many face-related factors affect the voice, it may not be possible to entirely disambiguate them from the voice. Even if this were not the case, it is unknown a priori exactly what features of the voice encode information about any given facial feature (although one may take guesses [35]). Moreover, the signatures of the different facial characteristics may lie in different spoken sounds; thus, in order to obtain sufficient evidence, the voice recordings must be long enough to have sufficient coverage of sounds to derive all the necessary information. The information containing in a single audio clip may not be sufficient for constructing a face image.\n\nYet, although prima facie the problem seems extremely hard, recent advances in neural network based generative models have shown that they are able to perform similarly challenging generative tasks in a variety of scenarios, when properly structured and trained. In particular, generative adversarial networks (GANs) have demonstrated the ability to learn to generate highly sophisticated imagery, given only signals about the validity of the generated image, rather than detailed supervision of the content of the image itself [23,30,40]. We use this ability to learn to generate faces from voices.\n\nFor our solution, we propose a simple but effective data-driven framework based on generative adversarial networks (GANs), as illustrated in Fig. 1. The objective of the network is simple: given a voice recording it must generate a face image that plausibly belongs to that voice. The voice recording itself is input to the generator network in the form of a voice embedding vector extracted by a voice embedding network. The generator is trained using a pair of discriminators. The first evaluates if the images it generates are realistic face images. The second discriminator (classifier) verifies that the identity of face image output by the generator does indeed match the actual identity of the speaker.\n\nWe present both qualitative and quantitative evaluations of the results produced by our model. The qualitative results show that our framework is able to map the voice manifold to face manifold. We can observe many identity associations between the generated faces and the input voices. The generated faces are generally age and gender appropriate, frequently matching the real face of the speaker. Additionally, given non-speech input the outputs become unrealistic, showing that the learned mapping is at least somewhat specific, in that the face manifold it learns are derived primarily from the voice manifold and not elsewhere. In addition, for different speech segments from the same person, the generated faces exhibit reasonable intra-class variation.\n\nWe also propose a number of quantitative evaluation metrics to evaluate the output of our network, based on how specific the model is in mapping voices to faces, how well the high-level attributes of the generated face match that of the speaker, and how well the generated faces match the identity (ID) of the speaker itself. For the last metric (ID matching), we leverage the cross-modal matching task [25], wherein, specifically, we need to match a speech segment to one of the two faces, where one is the true face of the speaker, and another is an \"imposter.\" Our tests reveal that the network is highly specific in generating faces in response to voices, produces quantifiably gender-appropriate faces from voices, and that the matching accuracy is much better than chance, or what may be obtained merely by matching gender. We refer the reader to the experiments section for actual numbers.\n\nOverall, our contributions are summarized as follows:\n\n\u2022 We introduce a new task of generating faces from voice in voice profiling. It could be used to explore the relationship between voice and face modalities. \u2022 We propose a simple but effective framework based on generative adversarial networks for this task. Each component in the framework is well motivated. \u2022 We propose to quantitatively evaluate the generated faces by using a cross-modal matching task. Both the qualitative and quantitative results show that our framework is able to generate faces that have identity association with the input voice.\n\n\nRelated Works\n\nThe task of deriving faces from voices relates to several research areas.\n\nVoice profiling. There currently exists a significant body of research on deriving personal profile parameters from a person's voice [35]. Many useful characteristics about the speaker can be inferred from their speech, e.g. age [28,1], emotion [21], identity [6], anthropometric measurement [36], health status [33] etc. These profile parameters may be viewed as providing linkages between a person's voice and their face.\n\nFace generation using GANs. There is a long line of research on generating faces using GANs. [29,12] can be used to generate face images from noise, but it does not consider any conditioning information (like identity) as input. [27,18] use a discrete label as the condition, but it only works on a closed-set scenario. [8,2] achieve identity-preserving face generation in an open-set scenario. However, the problem they focus on is within-modal generation problem, where a reference face image of the target identity to be generated is shown to the model. Our task is more challenging, since the conditioning information provided is actually from a different modality, namely voice.\n\nVoice to face matching. Cross modal matching between voice and face has become an increasingly popular problem in recent years. It aims at matching a probe input from one modality to a gallery of multiple inputs from the other modality. [25] formulates this task as a N -way classification problem. [24,13,39] propose to learn common embeddings for the cross-modal inputs, such that the matching can be performed using the learned embeddings. All of these, however, are essentially selection problems. Our task requires actual generation is naturally more challenging than crossmodal matching. In part, this is also because many possible generated images could be the expected output for an audio input. There is no unique target output as a supervision signal to train our model.\n\nTalking face. Talking face [41, 10] is a recently proposed task. Given a static face image and a speech clip, the goal is to generate a sequence of target face images where the lips are synchronized with the audio. The talking face task is significantly different from our task; in fact the two are actually somewhat orthogonal to each other because talking face focuses on the content of the speech and ignores the identity of the speaker, while our task does the opposite, extracting the identity of speaker and discarding the content of speech.\n\nSpeech to face. A very recent, yet to be presented publication that has been brought to our notice by the authors also reports on the production of faces from voices [37]. While the problem addressed is similar, the approach claimed is different. Since the codebase and much of the necessary detail remain unpublished at the time of this submission 1 , we do not perform comparative assessments.\n\n\nThe Proposed Framework\n\nBefore we begin, we first specify some of the notation we will use. We represent voice recordings by the symbol v, using super or subscripts to identify specific recordings. Similarly, we represent face images by the symbol f . We will represent the identity of a subject who provides voice or face data as y. We will represent the true identity of (the subject of) voice recording v as y v and face f as y f . We represent the function that maps a voice or face recording to its idenitity as ID(), i.e. y v = ID(v) and y f = ID(f ). Additional notation will become apparent as we introduce it.\n\nOur objective is to train a model F (v; \u0398) (with parameter \u0398) that takes as input a voice recording v and produces, as output a face imagef = F (v; \u0398) that belongs to the speaker of v, i.e. such that\nID(f ) = ID(v).\nWe use the framework shown in Figure 1 for our model, which decomposes F (v; \u0398) into a sequence of two components, F e (v; \u03b8 e ) and F g (e; \u03b8 g ). F e (v; \u03b8 e ) : v \u2192 e is a voice embedding function with parameter \u03b8 e that takes in a voice recording v and outputs an embedding vector e that captures all the salient information in v. F g (e; \u03b8 g ) : e \u2192 f is a generator function that takes in an embedding vector and generates a face imagef .\n\nWe must learn \u03b8 e and \u03b8 g such that\nID(F g (F v (v; \u03b8 e ); \u03b8 g ) = ID(v).\n3.1 Training the network\n\n\nData\n\nWe assume the availability of face and voice data from a set of subjects Y = {y 1 , y 2 , \u00b7 \u00b7 \u00b7 , y k }. Correspondingly, we also have a set of voice recordings\nV = {v 1 , v 2 , ..., v N }, with identity labels Y v = {y v 1 , y v 2 , ..., y v N } and a set of faces F = {f 1 , f 2 , ..., f M } with identity labels Y f = {y f 1 , y f 2 , ..., y f M }, such that y v \u2208 Y \u2200y v \u2208 Y v and y f \u2208 Y \u2200y f \u2208 Y f . N may not be equal to M .\nIn addition, we define two sets of labels R = {r 1 , r 2 , ..., r M | \u2200i, r i = 1} andR = {r 1 ,r 2 , ...,r N | \u2200i,r i = 0} corresponding Y f and Y v respectively. R is a set of labels that indicates that all faces in F are \"real.\"R is a set of labels that indicates that any faces generated from any v \u2208 V are synthetic or \"fake.\"\n\n\nGAN framework\n\nIn training the model, we impose two requirements. First, the outputf of the generator in response to any actual voice input v must be a realistic face image. Second, it must belong to the same identity as the voice, i.e. ID(f ) = f v . As explained in Section 1, we will use a GAN framework to train F e (.; \u03b8 e ) and F g (.; \u03b8 g ). This will require the definition of adversary that provide losses that can be used to learn the model parameters.\n\nWe define an adversarial objective. First, the discriminator F d determines if any input image (f orf ) is a genuine picture of a face, or one generated by the generator, i.e. assigns any face image (f orf ) to its real/fake label (r orr). The loss function for F d is defined as L d (F d (f ), r) (or L d (F d (f ),r)). Second, classifier F c learns to assign any real face image f to its identity label y f . Accordingly, the loss function for F c is L c (F c (f ), y f ). Last, the generator F g takes in a voice recording v and attempts to generate any face imagef that can be classified to real label r and identity label y v by F d and F g , respectively. The corresponding loss function for\nF g is L d (F d (F g (v), r) + L c (F c (F g (v)), y v ).\nIn our implementation, we instantiate F e (v; \u03b8 e ), F g (e; \u03b8 g ), F d (f ; \u03b8 d ) and F c (f ; \u03b8 c ) as convolutional neural networks, shown in Fig. 1. F e is the component labeled as Voice Embedding Network. v is the Mel-Spectrographic representations of speech signal. The output of the final convolutional layer is pooled over time, leading to a q-dimensional vector e. F g is labeled as Generator. f andf are RGB images with the same resolution of w \u00d7 h. F d and F c are labeled as Discriminator and Classifier, respectively. The loss functions L d and L c of these two components are the cross-entropy loss.\n\n\nTraining the network\n\nThe training data comprise a set of voice recordings V and a set of face images F. From the voice recordings in V we could obtain the corresponding generated face imagesF = {f = F g (F e (v)) | \u2200v \u2208 V}.\n\nThe framework is trained in an adversarial manner. To simplify, we use a pretrained voice embedding network F e (v; \u03b8 e ) from a speaker recognition task, and freeze the parameter \u03b8 e when training our framework. F d is trained to maximize\nM i=1 L d (F d (f i ), r i ) + N i=1 L d (F d (f i ),r i ) with fixed \u03b8 e , \u03b8 g and \u03b8 c . Similarly, the F c is trained to maximize M i=1 L c (F c (f i ), y i ) with fixed \u03b8 e , \u03b8 g and \u03b8 d . The F g is trained to maximize N i=1 L d (F d (f i ), r i ) + L c (F c (f i ), y v i )\nwith \u03b8 e , \u03b8 d and \u03b8 c fixed, wher\u00ea f i = F g (F e (v i )). The training pipeline is summarized in Algorithm 1.\n\n\nAlgorithm 1 The training algorithm of the proposed framework\n\nInput: A set of voice recordings with identity label (V, Y v ). A set of labeled face images with identity label (F, Y f ). A voice embedding network Fe(v; \u03b8e) trained on V with speaker recognition task. \u03b8e is fixed during the training. Randomly initialized \u03b8g, \u03b8 d , and \u03b8c Output: The parameters \u03b8g.\n\n1: while not converge do 2:\n\nRandomly sample a minibatch of n voice recordings {v1, v2, ..., vn} from V 3:\n\nRandomly sample a minibatch of m face images {f1, f2, ..., fm} from F 4:\n\nUpdate the discriminator F d (f ; \u03b8 d ) by ascending the gradient \n\u2207 \u03b8 d n i=1 log(1 \u2212 F d (fi)) + m i=1 log F d (v i ] + m i=1 log F d (Fg(Fe(vi))) 7: end while Once trained, F d (f ; \u03b8 d ) and F c (f ; \u03b8 c ) can be removed. Only F d (f ; \u03b8 d )\nand F c (f ; \u03b8 c ) are used for face generation from voice in the inference. It is worth noting that the targeted scenario is open-set. In our evaluations, the model is required to work on previously unseen and unheard identities, i.e. y v \u2208 Y in the training phase, while y v / \u2208 Y in the testing phase.\n\n\nExperiments\n\nIn our experiments, the voice recordings are from the Voxceleb [25] dataset and the face images are from the manually filtered version of VGGFace [26] dataset. Both datasets have identity labels. We use the intersection of the two datasets with the common identities, leading to 149,354 voice recordings and 139,572 frontal face images of 1,225 subjects. We follow the train/validation/test split in [25]. The details are shown in Table 1.\n\nSeparated data pre-processing pipelines are employed to audio segments and face images. For audio segments, we use a voice activity detector interface from the WebRTC project to isolate speechbearing regions of the recordings. Subsequently, we extract 64-dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform mean and variance normalization of each mel-frequency bin. We randomly crop an audio clips around 3 to 8 seconds for training, but use the entire recording for testing. For the face data, facial landmarks in all images are detected using [5]. The cropped RGB face images of size 3 \u00d7 64 \u00d7 64 are obtained by similarity transformation. Each pixel in the RGB images is normalized by subtracting 127.5 and then dividing by 127.5. Training. The network architecture is given in Table 2. The parameters in the convolutional layers of discriminator and classifier are shared in our experiments. We basically follow the hyperparameter setting in [29]. We used the Adam optimizer [14] with learning rate of 0.0002. \u03b2 1 and \u03b2 2 are 0.5 and 0.999, respectively. Minibatch size is 128. The training is completed at 100K iterations.\n\n\nQualitative Results\n\nAs a first experiment, we compared the outputs of the network in response to various noise signals to outputs obtained from actual speech recordings. Figure 2 shows outputs generated for four different types of noise. We evaluated noise segments of different durations (1,2,3, 5 and 10 seconds) to observe how the generated faces change with the duration. The generated images are seen to be Conv 3 /2,1 denotes 1D convoluitonal layer with kernel size of 3, where the stride and padding are 2 and 1, respectively. Each convolutional layer is followed by a Batch Normalization (BN) [9] layer and Rectified Linear Units (ReLU) [16]. The output shape is shown accordingly, where ti+1 = (ti \u2212 1)/2 + 1. The final outputs are pooled over time, yielding a 64-dimensional embedding. We use 2D deconvolutional layers with ReLU for the generator and 2D convolutional layers with Leaky ReLU (LReLU) for the discriminator and classifier. The final output is given by fully connected (FC) layer.\n\nVoice Embedding Network Generator Layer Act. Output shape Layer Act. Output shape  blurry, unrecognizable and generally alike, since there is no identity information in noise. With longer noise recordings, the results do not improve. Similar results are obtained over a variety of noises.\nInput - 64 \u00d7 t0 Input - 64 \u00d7 1 \u00d7 1 Conv 3 /2,1 BN + ReLU 256 \u00d7 t1 Deconv 4 \u00d7 4 /1,0 ReLU 1024 \u00d7 4 \u00d7 4 Conv 3 /2,1 BN + ReLU 384 \u00d7 t2 Deconv 3 \u00d7 3 /2,1 ReLU 512 \u00d7 8 \u00d7 8 Conv 3 /2,1 BN + ReLU 576 \u00d7 t3 Deconv 3 \u00d7 3 /2,1 ReLU 256 \u00d7 16 \u00d7 16 Conv 3 /2,1 BN + ReLU 864 \u00d7 t4 Deconv 3 \u00d7 3 /2,1 ReLU 128 \u00d7 32 \u00d7 32 Conv 3 /2,1 BN + ReLU 64 \u00d7 t5 Deconv 3 \u00d7 3 /2,1 ReLU 64 \u00d7 64 \u00d7 64 AvePool 1 \u00d7 t5 - 64\u00d71 Deconv 1 \u00d7 1 /1,0 - 3 \u00d7 64 \u00d7 64 Discriminator Classifier Layer Act. Output shape Layer Act. Output shape Input - 3 \u00d7 64 \u00d7 64 Input - 3 \u00d7 64 \u00d7 64 Conv 1 \u00d7 1 /1,0 LReLU 32 \u00d7 64 \u00d7 64 Conv 1 \u00d7 1 /1,0 LReLU 32 \u00d7 64 \u00d7 64 Conv 3 \u00d7 3 /2,1 LReLU 64 \u00d7 32 \u00d7 32 Conv 3 \u00d7 3 /2,1 LReLU 64 \u00d7 32 \u00d7 32 Conv 3 \u00d7 3 /2,1 LReLU 128 \u00d7 16 \u00d7 16 Conv 3 \u00d7 3 /2,1 LReLU 128 \u00d7 16 \u00d7 16 Conv 3 \u00d7 3 /2,1 LReLU 256 \u00d7 8 \u00d7 8 Conv 3 \u00d7 3 /2,1 LReLU 256 \u00d7 8 \u00d7 8 Conv 3 \u00d7 3 /2,1 LReLU 512 \u00d7 4 \u00d7 4 Conv 3 \u00d7 3 /2,1 LReLU 512 \u00d7 4 \u00d7 4 Conv 4 \u00d7 4 /1,0 LReLU 64 \u00d7 1 \u00d7 1 Conv 4 \u00d7 4 /1,0 LReLU 64 \u00d7 1 \u00d7 1 FC 64 \u00d7 1 Sigmoid 1 FC 64 \u00d7 k Softmax k\nOn the other hand, when we use regular speech recordings with the aforementioned durations as inputs, outputs tend to be realistic faces, as seen in Figure 3. The results indicate that while the generator does learn to produce face-like images, actual faces are produced chiefly in response to actual voice. We infer that while the generator has learned to map the speech manifold to the manifold of faces, it maps other inputs outside of this manifold. Figure 3 also enables us to subjectively evaluate the actual output of the network. These results are typical and not cherry-picked for presentation (several of our reconstructions match the actual speaker closely, but we have chosen not to selectively present those to avoid misrepresenting the actual performance of the system). The generated images are on the left, while the reference images (the actual faces of the speakers) are on the right. To reduce the perceptual bias and better illustration, we show multiple reference face images for each speaker. Although the generated and the reference face indicate different persons, the identity information of these two are matched in some sense (like gender, ethnicity, etc.). With longer speech segments, the generated faces gradually converge to faces associatable with the speaker. Figure 4 shows additional examples demonstrating that the synthesized images are generally age-and gender-matched with the speaker.   In the next experiment, we select 7 different speech segments of each speaker and generate the faces.\n\nEntire recordings are used. The results are shown in Figure 5 (reference images are also provided for comparison). Once again, the results are typical and not cherrypicked. We believe that the images in each row exhibit reasonable variations of the same person (except the fourth image in the first row), indicating that our model is able to build a mapping between the speech group to face group, thus retaining the identity of the face across speech segments from the same speaker.\n\n\nQuantitative Results\n\nWe attempt to quantitatively distinguish between the faces generated in response to noise from those generated from voice using the discriminator F d () itself. Note that F d () is biased, and has explicitly been trained to tag synthesized faces as fake. The mean and standard deviation (obtained from 1000 samples) of its output value in response to actual voices are 0.09 and 0.13 respectively, while for (an identical number of) noise inputs they are 0.06 and 0.07 respectively. Even a biased discriminator is clearly able to distinguish between faces generated from voices, and those obtained from noise.\n\nAs a first test, we attempted to ID (classify) faces derived from test recordings of the speakers in the training set. On a set of 4620 recordings, from 924 identities, we achieved a top-1 accuracy of 61.7% and a top-5 accuracy of 82.3%, showing that the voice-based face reconstructions do actually faithfully capture the identities of known subjects.\n\nFor unknown subjects, we ran a gender classifier on 21,850 generated faces (from the speech segments in test set). The classifier was trained on the face images in the training set using the network architecture of the discriminator, with an accuracy of 98.97% on real face images. The classifier obtained a 96.45% accuracy in matching the gender of the generated faces to the known gender of the speaker, showing that the generated faces are almost always of the correct gender.\n\nFinally, we evaluated our model by leveraging the task of voice to face matching. Here, we are given a voice recording, a face image of the true speaker, and a face image of an imposter. We must match the voice to the face of the true speaker. Ideally, the probe voice could be replaced by the generated face image if they carry the same identity information. So the voice to face matching problem reduces to a typical face verification or face recognition problem. The resulting matching accuracy could be used to quantitatively evaluate the association between the speech segment and the generated face.\n\nWe construct the testing instances (a probe voice recording, a true face image, and an \"imposter\" face) using data in the testing set, leading to 2,353,560 trials. We also compute the matching accuracy on about 50k trials constructed from a small part of the training set to see how well the model fit to the training data. We also perform stratified experiments based on gender where we select the imposter face with the same gender as the true face. In this case, gender information cannot be used for matching anymore, leading to a more fair test.\n\nThe results are shown in Table 3. The high accuracies obtained on the training set for the unstratified and gender stratified tests (96.83% and 93.98% respectively) show that generated faces do carry correct identity information for the training set. The results on the test set on unstratified and gender stratified tests (76.07% and 59.69% respectively) are better than those in DIMNets-G [39], indicating that our model learns more associations than gender. The large drop compared to the results on the training set shows however that considerable room remains to improve generalizability in the model. \n\n\nDiscussion and Conclusion\n\nThe proposed GAN-based framework is seen to achieve reasonable reconstruction results. The generated faces have identity associations with the true speaker. There remains considerable room for improvement. Firstly, there are obvious issues with the GAN-based output: the produced faces have features such as hair that are presumably not predicted by voice, but simply obtained from their co-occurrence with other features. The model may be more appropriately learned through data cleaning that removes obviously unrelated aspects of the facial image, such as hair and background. The proposed model is vanilla in many ways. For instance [34,36] describe several explicit correspondences between speech and face features, e.g. different phonetic units are known to relate to different facial features. We are investigating models that explicitly consider these issues.\n\nFigure 2 :\n2The generated face images from noise input. Each row shows the generated faces using one of the four noise audio segments with different durations. (a) 1 second, (b) 2 seconds, (c) 3 seconds, (d) 5 seconds, (e) 10 seconds.\n\nFigure 3 :\n3(a)-(e) The generated face images from regular speech recordings with different durations. (f) the corresponding reference face images. These 4 speakers (from top to bottom) are Danica McKellar, Cindy Williams, Damian Lewis, and Eva Green.\n\nFigure 4 :\n4(a) Faces from old voices. (b) Faces from male voices. (c) Faces from female voices. For each group, images on the left are the generated images and images on the right are the references.\n\nFigure 5 :\n5The generated face images from different speech segments of the same speaker and their corresponding face images. Each row shows the results from the same speaker.\n\nTable 1 :\n1Statistics of the datasets used in our experimentstrain \nvalidation \ntest \ntotal \n# of speech segments 113,322 \n14,182 \n21,850 149,354 \n# of face images \n106,584 \n12,533 \n20,455 \n139572 \n# of subjects \n924 \n112 \n189 \n1,225 \n\n\n\nTable 2 :\n2The detailed CNNs architectures. For the voice embedding network, we use 1D convolutional layers.\n\nTable 3 :\n3The voice to face matching accuracies. Our results are given by replacing the probe voice embeddings by the embeddings of the generated face. unstratified group (ACC. %) stratified group by gender (ACC. %)(training set / testing set) \n(training set / testing set) \nSVHF [25] \n-/ 81.00 \n-/ 65.20 \nDIMNets-I [39] \n-/ 83.45 \n-/ 70.91 \nDIMNets-G [39] \n-/ 72.90 \n-/ 50.32 \nours \n96.83 / 76.07 \n93.98 / 59.69 \n\n\nA version of the work reported in this paper was tested live by nearly 1000 people at the World Economic Forum in Tianjin, Sep 2018.\n\nAge estimation from telephone speech using i-vectors. Mohamad Hasan Bahari, Mclaren, Da Van Leeuwen, Mohamad Hasan Bahari, ML McLaren, DA Van Leeuwen, et al. Age estimation from telephone speech using i-vectors. 2012.\n\nTowards open-set identity preserving face synthesis. Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, Gang Hua, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua. Towards open-set identity preserving face synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6713-6722, 2018.\n\nUnderstanding voice perception. P Belin, P E G Bestelmeyer, M Latinus, R Watson, British Journal of Psychology. 108P. Belin, P.E.G. Bestelmeyer, M. Latinus, and R. Watson. Understanding voice perception. British Journal of Psychology, 108:711-725, 2011.\n\nThinking the voice: neural correlates of voice perception. Pascal Belin, Shirley Fecteau, Catherine Bedard, Trends in cognitive sciences. 83Pascal Belin, Shirley Fecteau, and Catherine Bedard. Thinking the voice: neural correlates of voice perception. Trends in cognitive sciences, 8(3):129-135, 2004.\n\nHow far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks). Adrian Bulat, Georgios Tzimiropoulos, International Conference on Computer Vision. Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks). In International Conference on Computer Vision, 2017.\n\nFront-end factor analysis for speaker verification. Najim Dehak, J Patrick, R\u00e9da Kenny, Pierre Dehak, Pierre Dumouchel, Ouellet, IEEE Transactions on Audio, Speech, and Language Processing. 194Najim Dehak, Patrick J Kenny, R\u00e9da Dehak, Pierre Dumouchel, and Pierre Ouellet. Front-end fac- tor analysis for speaker verification. IEEE Transactions on Audio, Speech, and Language Processing, 19(4):788-798, 2010.\n\nNeuro-cognitive processing of faces and voices. W Andrew, Ellis, Handbook of research on face processing. ElsevierAndrew W Ellis. Neuro-cognitive processing of faces and voices. In Handbook of research on face processing, pages 207-215. Elsevier, 1989.\n\nBeyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis. Rui Huang, Shu Zhang, Tianyu Li, Ran He, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionRui Huang, Shu Zhang, Tianyu Li, and Ran He. Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis. In Proceedings of the IEEE International Conference on Computer Vision, pages 2439-2448, 2017.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, arXiv:1502.03167arXiv preprintSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n\nYou said that?: Synthesising talking faces from audio. Amir Jamaludin, Joon Son Chung, Andrew Zisserman, International Journal of Computer Vision. Amir Jamaludin, Joon Son Chung, and Andrew Zisserman. You said that?: Synthesising talking faces from audio. International Journal of Computer Vision, pages 1-13, 2019.\n\nPutting the face to the voice': Matching identity across modality. Miyuki Kamachi, Harold Hill, Karen Lander, Eric Vatikiotis-Bateson, Current Biology. 1319Miyuki Kamachi, Harold Hill, Karen Lander, and Eric Vatikiotis-Bateson. Putting the face to the voice': Matching identity across modality. Current Biology, 13(19):1709-1714, 2003.\n\nProgressive growing of gans for improved quality, stability, and variation. Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, arXiv:1710.10196arXiv preprintTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.\n\nChangil Kim, Valentina Hijung, Tae-Hyun Shin, Alexandre Oh, Mohamed Kaspar, Wojciech Elgharib, Matusik, arXiv:1805.05553On learning associations of faces and voices. arXiv preprintChangil Kim, Hijung Valentina Shin, Tae-Hyun Oh, Alexandre Kaspar, Mohamed Elgharib, and Wojciech Matusik. On learning associations of faces and voices. arXiv preprint arXiv:1805.05553, 2018.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nGender classification in two emotional speech databases. Margarita Kotti, Constantine Kotropoulos, 19th International Conference on Pattern Recognition. IEEEMargarita Kotti and Constantine Kotropoulos. Gender classification in two emotional speech databases. In 2008 19th International Conference on Pattern Recognition, pages 1-4. IEEE, 2008.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012.\n\nAttribute and simile classifiers for face verification. Neeraj Kumar, C Alexander, Berg, N Peter, Shree K Belhumeur, Nayar, IEEE 12th International Conference on Computer Vision. IEEENeeraj Kumar, Alexander C Berg, Peter N Belhumeur, and Shree K Nayar. Attribute and simile classifiers for face verification. In 2009 IEEE 12th International Conference on Computer Vision, pages 365-372. IEEE, 2009.\n\nConditional cyclegan for attribute guided face image generation. Yongyi Lu, Yu-Wing Tai, Chi-Keung Tang, arXiv:1705.09966arXiv preprintYongyi Lu, Yu-Wing Tai, and Chi-Keung Tang. Conditional cyclegan for attribute guided face image generation. arXiv preprint arXiv:1705.09966, 2017.\n\nMatching voice and face identity from static images. W Lauren, Elan Mavica, Barenholtz, Journal of Experimental Psychology: Human Perception and Performance. 392307Lauren W Mavica and Elan Barenholtz. Matching voice and face identity from static images. Journal of Experimental Psychology: Human Perception and Performance, 39(2):307, 2013.\n\nWhen eyewitnesses are also earwitnesses: Effects on visual and voice identifications. A Hunter, Mcallister, H I Robert, Dale, J Norman, Allyssa Bregman, C Randy Mccabe, Cotton, Basic and Applied Social Psychology. 142Hunter A McAllister, Robert HI Dale, Norman J Bregman, Allyssa McCabe, and C Randy Cotton. When eyewitnesses are also earwitnesses: Effects on visual and voice identifications. Basic and Applied Social Psychology, 14(2):161-170, 1993.\n\nApproaching automatic recognition of emotion from voice: A rough benchmark. Sin\u00e9ad Mcgilloway, Roddy Cowie, Ellen Douglas-Cowie, Stan Gielen, Machiel Westerdijk, Sybert Stroeve, ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion. Sin\u00e9ad McGilloway, Roddy Cowie, Ellen Douglas-Cowie, Stan Gielen, Machiel Westerdijk, and Sybert Stroeve. Approaching automatic recognition of emotion from voice: A rough benchmark. In ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion, 2000.\n\nDetermination of the vocal-tract shape from measured formant frequencies. Paul Mermelstein, The Journal of the Acoustical Society of America. 415Paul Mermelstein. Determination of the vocal-tract shape from measured formant frequencies. The Journal of the Acoustical Society of America, 41(5):1283-1294, 1967.\n\nMehdi Mirza, Simon Osindero, arXiv:1411.1784Conditional generative adversarial nets. arXiv preprintMehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.\n\nLearnable pins: Cross-modal embeddings for person identity. Arsha Nagrani, Samuel Albanie, Andrew Zisserman, arXiv:1805.00833arXiv preprintArsha Nagrani, Samuel Albanie, and Andrew Zisserman. Learnable pins: Cross-modal embeddings for person identity. arXiv preprint arXiv:1805.00833, 2018.\n\nSeeing voices and hearing faces: Cross-modal biometric matching. Arsha Nagrani, Samuel Albanie, Andrew Zisserman, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionArsha Nagrani, Samuel Albanie, and Andrew Zisserman. Seeing voices and hearing faces: Cross-modal biometric matching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8427-8436, 2018.\n\nDeep face recognition. M Omkar, Andrea Parkhi, Andrew Vedaldi, Zisserman, bmvc. 16Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. Deep face recognition. In bmvc, volume 1, page 6, 2015.\n\nGuim Perarnau, Joost Van De, Bogdan Weijer, Jose M Raducanu, \u00c1lvarez, arXiv:1611.06355Invertible conditional gans for image editing. arXiv preprintGuim Perarnau, Joost Van De Weijer, Bogdan Raducanu, and Jose M \u00c1lvarez. Invertible conditional gans for image editing. arXiv preprint arXiv:1611.06355, 2016.\n\nAge recognition from voice. H Paul, Eric K Ptacek, Sander, Journal of speech and hearing Research. 92Paul H Ptacek and Eric K Sander. Age recognition from voice. Journal of speech and hearing Research, 9(2):273-277, 1966.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. Alec Radford, Luke Metz, Soumith Chintala, arXiv:1511.06434arXiv preprintAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee, arXiv:1605.05396Generative adversarial text to image synthesis. arXiv preprintScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.\n\nHearing facial identities: Brain correlates of face-voice integration in person identification. Nadine Stefan R Schweinberger, David Mc Kloth, Robertson, Cortex. 479Stefan R Schweinberger, Nadine Kloth, and David MC Robertson. Hearing facial identities: Brain correlates of face-voice integration in person identification. Cortex, 47(9):1026-1037, 2011.\n\nHearing facial identities. R Stefan, David Schweinberger, J\u00fcrgen M Robertson, Kaufmann, Quarterly Journal of Experimental Psychology. 6010Stefan R Schweinberger, David Robertson, and J\u00fcrgen M Kaufmann. Hearing facial identities. Quarterly Journal of Experimental Psychology, 60(10):1446-1456, 2007.\n\nHealthline: Speech-based access to health information by low-literate users. Jahanzeb Sherwani, Nosheen Ali, Sarwat Mirza, Anjum Fatma, Yousuf Memon, Mehtab Karim, Rahul Tongia, Roni Rosenfeld, 2007 International Conference on Information and Communication Technologies and Development. IEEEJahanzeb Sherwani, Nosheen Ali, Sarwat Mirza, Anjum Fatma, Yousuf Memon, Mehtab Karim, Rahul Tongia, and Roni Rosenfeld. Healthline: Speech-based access to health information by low-literate users. In 2007 International Conference on Information and Communication Technologies and Development, pages 1-9. IEEE, 2007.\n\nProfiling humans from their voice. Rita Singh, SpringerRita Singh. Profiling humans from their voice. Springer, 2019.\n\nReconstruction of the human persona in 3d, and its reverse. Rita Singh, Profiling Humans from their Voice, chapter 10. springer nature PressRita Singh. Reconstruction of the human persona in 3d, and its reverse. In Profiling Humans from their Voice, chapter 10. springer nature Press, 2020.\n\nForensic anthropometry from voice: an articulatoryphonetic approach. Rita Singh, Bhiksha Raj, Deniz Gencaga, 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO). IEEERita Singh, Bhiksha Raj, and Deniz Gencaga. Forensic anthropometry from voice: an articulatory- phonetic approach. In 2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pages 1375-1380. IEEE, 2016.\n\nLearning the face behind a voice. Proceedings of the IEEE conference on computer vision and pattern recognition. Changil Kim Inbar Mosseri William T. Freeman Michael Rubinstein Wojciech Matusik Tae-Hyun Oh, Tali Dekel*. Speech2facethe IEEE conference on computer vision and pattern recognitionChangil Kim Inbar Mosseri William T. Freeman Michael Rubinstein Wojciech Matusik Tae-Hyun Oh, Tali Dekel*. Speech2face: Learning the face behind a voice. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2019.\n\nEvidence for nonlinear sound production mechanisms in the vocal tract. H M Teager, Teager, Speech production and speech modelling. SpringerHM Teager and SM Teager. Evidence for nonlinear sound production mechanisms in the vocal tract. In Speech production and speech modelling, pages 241-261. Springer, 1990.\n\nDisjoint mapping network for cross-modal matching of voices and faces. Yandong Wen, Mahmoud Al Ismail, Weiyang Liu, Bhiksha Raj, Rita Singh, arXiv:1807.04836arXiv preprintYandong Wen, Mahmoud Al Ismail, Weiyang Liu, Bhiksha Raj, and Rita Singh. Disjoint mapping network for cross-modal matching of voices and faces. arXiv preprint arXiv:1807.04836, 2018.\n\nAuxiliary classifier generative adversarial network with soft labels in imbalanced acoustic event detection. Xianjun Xia, Roberto Togneri, Ferdous Sohel, David Huang, IEEE Transactions on Multimedia. Xianjun Xia, Roberto Togneri, Ferdous Sohel, and David Huang. Auxiliary classifier generative adversarial network with soft labels in imbalanced acoustic event detection. IEEE Transactions on Multimedia, 2018.\n\nTalking face generation by adversarially disentangled audio-visual representation. Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, Xiaogang Wang, arXiv:1807.07860arXiv preprintHang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang Wang. Talking face generation by adversarially disentangled audio-visual representation. arXiv preprint arXiv:1807.07860, 2018.\n", "annotations": {"author": "[{\"start\":\"72\",\"end\":\"253\"},{\"start\":\"254\",\"end\":\"428\"},{\"start\":\"429\",\"end\":\"605\"}]", "publisher": null, "author_last_name": "[{\"start\":\"80\",\"end\":\"83\"},{\"start\":\"259\",\"end\":\"264\"},{\"start\":\"437\",\"end\":\"440\"}]", "author_first_name": "[{\"start\":\"72\",\"end\":\"79\"},{\"start\":\"254\",\"end\":\"258\"},{\"start\":\"429\",\"end\":\"436\"}]", "author_affiliation": "[{\"start\":\"109\",\"end\":\"252\"},{\"start\":\"284\",\"end\":\"427\"},{\"start\":\"461\",\"end\":\"604\"}]", "title": "[{\"start\":\"1\",\"end\":\"69\"},{\"start\":\"606\",\"end\":\"674\"}]", "venue": null, "abstract": "[{\"start\":\"676\",\"end\":\"1717\"}]", "bib_ref": "[{\"start\":\"1928\",\"end\":\"1932\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"1932\",\"end\":\"1934\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"1934\",\"end\":\"1937\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"2772\",\"end\":\"2776\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"2776\",\"end\":\"2779\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"3062\",\"end\":\"3065\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"3065\",\"end\":\"3068\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"3081\",\"end\":\"3085\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"3246\",\"end\":\"3249\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3354\",\"end\":\"3357\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"3434\",\"end\":\"3437\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3437\",\"end\":\"3439\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3562\",\"end\":\"3566\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"3566\",\"end\":\"3569\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"3986\",\"end\":\"3990\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"3990\",\"end\":\"3993\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"3993\",\"end\":\"3996\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"4283\",\"end\":\"4287\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"4630\",\"end\":\"4634\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"5536\",\"end\":\"5540\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"5540\",\"end\":\"5543\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"5543\",\"end\":\"5546\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"7484\",\"end\":\"7488\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"8816\",\"end\":\"8820\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"8912\",\"end\":\"8916\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"8916\",\"end\":\"8918\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"8928\",\"end\":\"8932\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"8943\",\"end\":\"8946\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"8975\",\"end\":\"8979\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"8995\",\"end\":\"8999\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"9201\",\"end\":\"9205\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"9205\",\"end\":\"9208\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"9337\",\"end\":\"9341\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"9341\",\"end\":\"9344\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"9428\",\"end\":\"9431\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"9431\",\"end\":\"9433\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"10030\",\"end\":\"10034\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"10092\",\"end\":\"10096\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"10096\",\"end\":\"10099\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"10099\",\"end\":\"10102\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"11290\",\"end\":\"11294\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"17548\",\"end\":\"17552\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"17631\",\"end\":\"17635\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"17885\",\"end\":\"17889\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"18532\",\"end\":\"18535\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"18932\",\"end\":\"18936\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"18965\",\"end\":\"18969\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"19718\",\"end\":\"19721\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"19762\",\"end\":\"19766\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"26435\",\"end\":\"26439\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"27318\",\"end\":\"27322\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"27322\",\"end\":\"27325\",\"attributes\":{\"ref_id\":\"b35\"}}]", "figure": "[{\"start\":\"27549\",\"end\":\"27784\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"27785\",\"end\":\"28037\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"28038\",\"end\":\"28239\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"28240\",\"end\":\"28416\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"28417\",\"end\":\"28654\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"28655\",\"end\":\"28764\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"28765\",\"end\":\"29182\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1733\",\"end\":\"2398\"},{\"start\":\"2400\",\"end\":\"3131\"},{\"start\":\"3133\",\"end\":\"3997\"},{\"start\":\"3999\",\"end\":\"4162\"},{\"start\":\"4164\",\"end\":\"5006\"},{\"start\":\"5008\",\"end\":\"5607\"},{\"start\":\"5609\",\"end\":\"6318\"},{\"start\":\"6320\",\"end\":\"7079\"},{\"start\":\"7081\",\"end\":\"7977\"},{\"start\":\"7979\",\"end\":\"8032\"},{\"start\":\"8034\",\"end\":\"8590\"},{\"start\":\"8608\",\"end\":\"8681\"},{\"start\":\"8683\",\"end\":\"9106\"},{\"start\":\"9108\",\"end\":\"9791\"},{\"start\":\"9793\",\"end\":\"10573\"},{\"start\":\"10575\",\"end\":\"11122\"},{\"start\":\"11124\",\"end\":\"11519\"},{\"start\":\"11546\",\"end\":\"12140\"},{\"start\":\"12142\",\"end\":\"12341\"},{\"start\":\"12358\",\"end\":\"12802\"},{\"start\":\"12804\",\"end\":\"12839\"},{\"start\":\"12878\",\"end\":\"12902\"},{\"start\":\"12911\",\"end\":\"13071\"},{\"start\":\"13343\",\"end\":\"13674\"},{\"start\":\"13692\",\"end\":\"14139\"},{\"start\":\"14141\",\"end\":\"14838\"},{\"start\":\"14897\",\"end\":\"15510\"},{\"start\":\"15535\",\"end\":\"15737\"},{\"start\":\"15739\",\"end\":\"15978\"},{\"start\":\"16258\",\"end\":\"16369\"},{\"start\":\"16434\",\"end\":\"16735\"},{\"start\":\"16737\",\"end\":\"16764\"},{\"start\":\"16766\",\"end\":\"16843\"},{\"start\":\"16845\",\"end\":\"16917\"},{\"start\":\"16919\",\"end\":\"16985\"},{\"start\":\"17165\",\"end\":\"17469\"},{\"start\":\"17485\",\"end\":\"17924\"},{\"start\":\"17926\",\"end\":\"19113\"},{\"start\":\"19137\",\"end\":\"20120\"},{\"start\":\"20122\",\"end\":\"20410\"},{\"start\":\"21402\",\"end\":\"22930\"},{\"start\":\"22932\",\"end\":\"23415\"},{\"start\":\"23440\",\"end\":\"24048\"},{\"start\":\"24050\",\"end\":\"24402\"},{\"start\":\"24404\",\"end\":\"24883\"},{\"start\":\"24885\",\"end\":\"25490\"},{\"start\":\"25492\",\"end\":\"26042\"},{\"start\":\"26044\",\"end\":\"26651\"},{\"start\":\"26681\",\"end\":\"27548\"}]", "formula": "[{\"start\":\"12342\",\"end\":\"12357\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"12840\",\"end\":\"12877\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"13072\",\"end\":\"13342\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"14839\",\"end\":\"14896\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"15979\",\"end\":\"16257\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"16986\",\"end\":\"17033\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"17033\",\"end\":\"17164\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"20411\",\"end\":\"21401\",\"attributes\":{\"id\":\"formula_7\"}}]", "table_ref": "[{\"start\":\"17916\",\"end\":\"17923\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"18767\",\"end\":\"18774\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"26069\",\"end\":\"26076\",\"attributes\":{\"ref_id\":\"tab_3\"}}]", "section_header": "[{\"start\":\"1719\",\"end\":\"1731\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"8593\",\"end\":\"8606\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"11522\",\"end\":\"11544\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"12905\",\"end\":\"12909\",\"attributes\":{\"n\":\"3.1.1\"}},{\"start\":\"13677\",\"end\":\"13690\",\"attributes\":{\"n\":\"3.1.2\"}},{\"start\":\"15513\",\"end\":\"15533\",\"attributes\":{\"n\":\"3.1.3\"}},{\"start\":\"16372\",\"end\":\"16432\"},{\"start\":\"17472\",\"end\":\"17483\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"19116\",\"end\":\"19135\",\"attributes\":{\"n\":\"4.1\"}},{\"start\":\"23418\",\"end\":\"23438\",\"attributes\":{\"n\":\"4.2\"}},{\"start\":\"26654\",\"end\":\"26679\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"27550\",\"end\":\"27560\"},{\"start\":\"27786\",\"end\":\"27796\"},{\"start\":\"28039\",\"end\":\"28049\"},{\"start\":\"28241\",\"end\":\"28251\"},{\"start\":\"28418\",\"end\":\"28427\"},{\"start\":\"28656\",\"end\":\"28665\"},{\"start\":\"28766\",\"end\":\"28775\"}]", "table": "[{\"start\":\"28479\",\"end\":\"28654\"},{\"start\":\"28982\",\"end\":\"29182\"}]", "figure_caption": "[{\"start\":\"27562\",\"end\":\"27784\"},{\"start\":\"27798\",\"end\":\"28037\"},{\"start\":\"28051\",\"end\":\"28239\"},{\"start\":\"28253\",\"end\":\"28416\"},{\"start\":\"28429\",\"end\":\"28479\"},{\"start\":\"28667\",\"end\":\"28764\"},{\"start\":\"28777\",\"end\":\"28982\"}]", "figure_ref": "[{\"start\":\"3714\",\"end\":\"3722\"},{\"start\":\"5750\",\"end\":\"5756\"},{\"start\":\"12388\",\"end\":\"12396\"},{\"start\":\"15042\",\"end\":\"15048\"},{\"start\":\"19287\",\"end\":\"19295\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"21551\",\"end\":\"21559\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"21856\",\"end\":\"21864\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"22695\",\"end\":\"22703\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"22985\",\"end\":\"22993\",\"attributes\":{\"ref_id\":\"fig_5\"}}]", "bib_author_first_name": "[{\"start\":\"29371\",\"end\":\"29378\"},{\"start\":\"29589\",\"end\":\"29596\"},{\"start\":\"29602\",\"end\":\"29606\"},{\"start\":\"29613\",\"end\":\"29617\"},{\"start\":\"29623\",\"end\":\"29631\"},{\"start\":\"29636\",\"end\":\"29640\"},{\"start\":\"30039\",\"end\":\"30040\"},{\"start\":\"30048\",\"end\":\"30049\"},{\"start\":\"30050\",\"end\":\"30053\"},{\"start\":\"30067\",\"end\":\"30068\"},{\"start\":\"30078\",\"end\":\"30079\"},{\"start\":\"30321\",\"end\":\"30327\"},{\"start\":\"30335\",\"end\":\"30342\"},{\"start\":\"30352\",\"end\":\"30361\"},{\"start\":\"30677\",\"end\":\"30683\"},{\"start\":\"30691\",\"end\":\"30699\"},{\"start\":\"31020\",\"end\":\"31025\"},{\"start\":\"31033\",\"end\":\"31034\"},{\"start\":\"31044\",\"end\":\"31048\"},{\"start\":\"31056\",\"end\":\"31062\"},{\"start\":\"31070\",\"end\":\"31076\"},{\"start\":\"31426\",\"end\":\"31427\"},{\"start\":\"31753\",\"end\":\"31756\"},{\"start\":\"31764\",\"end\":\"31767\"},{\"start\":\"31775\",\"end\":\"31781\"},{\"start\":\"31786\",\"end\":\"31789\"},{\"start\":\"32271\",\"end\":\"32277\"},{\"start\":\"32285\",\"end\":\"32294\"},{\"start\":\"32559\",\"end\":\"32563\"},{\"start\":\"32575\",\"end\":\"32579\"},{\"start\":\"32580\",\"end\":\"32583\"},{\"start\":\"32591\",\"end\":\"32597\"},{\"start\":\"32888\",\"end\":\"32894\"},{\"start\":\"32904\",\"end\":\"32910\"},{\"start\":\"32917\",\"end\":\"32922\"},{\"start\":\"32931\",\"end\":\"32935\"},{\"start\":\"33234\",\"end\":\"33238\"},{\"start\":\"33247\",\"end\":\"33251\"},{\"start\":\"33258\",\"end\":\"33264\"},{\"start\":\"33272\",\"end\":\"33278\"},{\"start\":\"33494\",\"end\":\"33501\"},{\"start\":\"33507\",\"end\":\"33516\"},{\"start\":\"33525\",\"end\":\"33533\"},{\"start\":\"33540\",\"end\":\"33549\"},{\"start\":\"33554\",\"end\":\"33561\"},{\"start\":\"33570\",\"end\":\"33578\"},{\"start\":\"33911\",\"end\":\"33912\"},{\"start\":\"33923\",\"end\":\"33928\"},{\"start\":\"34142\",\"end\":\"34151\"},{\"start\":\"34159\",\"end\":\"34170\"},{\"start\":\"34495\",\"end\":\"34499\"},{\"start\":\"34512\",\"end\":\"34516\"},{\"start\":\"34528\",\"end\":\"34536\"},{\"start\":\"34537\",\"end\":\"34538\"},{\"start\":\"34853\",\"end\":\"34859\"},{\"start\":\"34867\",\"end\":\"34868\"},{\"start\":\"34886\",\"end\":\"34887\"},{\"start\":\"34895\",\"end\":\"34902\"},{\"start\":\"35262\",\"end\":\"35268\"},{\"start\":\"35273\",\"end\":\"35280\"},{\"start\":\"35286\",\"end\":\"35295\"},{\"start\":\"35534\",\"end\":\"35535\"},{\"start\":\"35544\",\"end\":\"35548\"},{\"start\":\"35909\",\"end\":\"35910\"},{\"start\":\"35931\",\"end\":\"35932\"},{\"start\":\"35933\",\"end\":\"35934\"},{\"start\":\"35949\",\"end\":\"35950\"},{\"start\":\"35959\",\"end\":\"35966\"},{\"start\":\"35976\",\"end\":\"35983\"},{\"start\":\"36352\",\"end\":\"36358\"},{\"start\":\"36371\",\"end\":\"36376\"},{\"start\":\"36384\",\"end\":\"36389\"},{\"start\":\"36405\",\"end\":\"36409\"},{\"start\":\"36418\",\"end\":\"36425\"},{\"start\":\"36438\",\"end\":\"36444\"},{\"start\":\"36852\",\"end\":\"36856\"},{\"start\":\"37089\",\"end\":\"37094\"},{\"start\":\"37102\",\"end\":\"37107\"},{\"start\":\"37360\",\"end\":\"37365\"},{\"start\":\"37375\",\"end\":\"37381\"},{\"start\":\"37391\",\"end\":\"37397\"},{\"start\":\"37657\",\"end\":\"37662\"},{\"start\":\"37672\",\"end\":\"37678\"},{\"start\":\"37688\",\"end\":\"37694\"},{\"start\":\"38094\",\"end\":\"38095\"},{\"start\":\"38103\",\"end\":\"38109\"},{\"start\":\"38118\",\"end\":\"38124\"},{\"start\":\"38267\",\"end\":\"38271\"},{\"start\":\"38282\",\"end\":\"38287\"},{\"start\":\"38296\",\"end\":\"38302\"},{\"start\":\"38311\",\"end\":\"38315\"},{\"start\":\"38316\",\"end\":\"38317\"},{\"start\":\"38602\",\"end\":\"38603\"},{\"start\":\"38610\",\"end\":\"38614\"},{\"start\":\"38615\",\"end\":\"38616\"},{\"start\":\"38891\",\"end\":\"38895\"},{\"start\":\"38905\",\"end\":\"38909\"},{\"start\":\"38916\",\"end\":\"38923\"},{\"start\":\"39145\",\"end\":\"39150\"},{\"start\":\"39157\",\"end\":\"39163\"},{\"start\":\"39171\",\"end\":\"39178\"},{\"start\":\"39184\",\"end\":\"39193\"},{\"start\":\"39206\",\"end\":\"39211\"},{\"start\":\"39221\",\"end\":\"39228\"},{\"start\":\"39589\",\"end\":\"39595\"},{\"start\":\"39620\",\"end\":\"39628\"},{\"start\":\"39875\",\"end\":\"39876\"},{\"start\":\"39885\",\"end\":\"39890\"},{\"start\":\"39906\",\"end\":\"39914\"},{\"start\":\"40225\",\"end\":\"40233\"},{\"start\":\"40244\",\"end\":\"40251\"},{\"start\":\"40257\",\"end\":\"40263\"},{\"start\":\"40271\",\"end\":\"40276\"},{\"start\":\"40284\",\"end\":\"40290\"},{\"start\":\"40298\",\"end\":\"40304\"},{\"start\":\"40312\",\"end\":\"40317\"},{\"start\":\"40326\",\"end\":\"40330\"},{\"start\":\"40792\",\"end\":\"40796\"},{\"start\":\"40936\",\"end\":\"40940\"},{\"start\":\"41237\",\"end\":\"41241\"},{\"start\":\"41249\",\"end\":\"41256\"},{\"start\":\"41262\",\"end\":\"41267\"},{\"start\":\"42275\",\"end\":\"42276\"},{\"start\":\"42277\",\"end\":\"42278\"},{\"start\":\"42585\",\"end\":\"42592\"},{\"start\":\"42598\",\"end\":\"42605\"},{\"start\":\"42606\",\"end\":\"42608\"},{\"start\":\"42617\",\"end\":\"42624\"},{\"start\":\"42630\",\"end\":\"42637\"},{\"start\":\"42643\",\"end\":\"42647\"},{\"start\":\"42979\",\"end\":\"42986\"},{\"start\":\"42992\",\"end\":\"42999\"},{\"start\":\"43009\",\"end\":\"43016\"},{\"start\":\"43024\",\"end\":\"43029\"},{\"start\":\"43364\",\"end\":\"43368\"},{\"start\":\"43375\",\"end\":\"43377\"},{\"start\":\"43383\",\"end\":\"43388\"},{\"start\":\"43394\",\"end\":\"43398\"},{\"start\":\"43404\",\"end\":\"43412\"}]", "bib_author_last_name": "[{\"start\":\"29379\",\"end\":\"29391\"},{\"start\":\"29393\",\"end\":\"29400\"},{\"start\":\"29402\",\"end\":\"29416\"},{\"start\":\"29597\",\"end\":\"29600\"},{\"start\":\"29607\",\"end\":\"29611\"},{\"start\":\"29618\",\"end\":\"29621\"},{\"start\":\"29632\",\"end\":\"29634\"},{\"start\":\"29641\",\"end\":\"29644\"},{\"start\":\"30041\",\"end\":\"30046\"},{\"start\":\"30054\",\"end\":\"30065\"},{\"start\":\"30069\",\"end\":\"30076\"},{\"start\":\"30080\",\"end\":\"30086\"},{\"start\":\"30328\",\"end\":\"30333\"},{\"start\":\"30343\",\"end\":\"30350\"},{\"start\":\"30362\",\"end\":\"30368\"},{\"start\":\"30684\",\"end\":\"30689\"},{\"start\":\"30700\",\"end\":\"30713\"},{\"start\":\"31026\",\"end\":\"31031\"},{\"start\":\"31035\",\"end\":\"31042\"},{\"start\":\"31049\",\"end\":\"31054\"},{\"start\":\"31063\",\"end\":\"31068\"},{\"start\":\"31077\",\"end\":\"31086\"},{\"start\":\"31088\",\"end\":\"31095\"},{\"start\":\"31428\",\"end\":\"31434\"},{\"start\":\"31436\",\"end\":\"31441\"},{\"start\":\"31757\",\"end\":\"31762\"},{\"start\":\"31768\",\"end\":\"31773\"},{\"start\":\"31782\",\"end\":\"31784\"},{\"start\":\"31790\",\"end\":\"31792\"},{\"start\":\"32278\",\"end\":\"32283\"},{\"start\":\"32295\",\"end\":\"32302\"},{\"start\":\"32564\",\"end\":\"32573\"},{\"start\":\"32584\",\"end\":\"32589\"},{\"start\":\"32598\",\"end\":\"32607\"},{\"start\":\"32895\",\"end\":\"32902\"},{\"start\":\"32911\",\"end\":\"32915\"},{\"start\":\"32923\",\"end\":\"32929\"},{\"start\":\"32936\",\"end\":\"32954\"},{\"start\":\"33239\",\"end\":\"33245\"},{\"start\":\"33252\",\"end\":\"33256\"},{\"start\":\"33265\",\"end\":\"33270\"},{\"start\":\"33279\",\"end\":\"33287\"},{\"start\":\"33502\",\"end\":\"33505\"},{\"start\":\"33517\",\"end\":\"33523\"},{\"start\":\"33534\",\"end\":\"33538\"},{\"start\":\"33550\",\"end\":\"33552\"},{\"start\":\"33562\",\"end\":\"33568\"},{\"start\":\"33579\",\"end\":\"33587\"},{\"start\":\"33589\",\"end\":\"33596\"},{\"start\":\"33913\",\"end\":\"33921\"},{\"start\":\"33929\",\"end\":\"33935\"},{\"start\":\"33937\",\"end\":\"33939\"},{\"start\":\"34152\",\"end\":\"34157\"},{\"start\":\"34171\",\"end\":\"34182\"},{\"start\":\"34500\",\"end\":\"34510\"},{\"start\":\"34517\",\"end\":\"34526\"},{\"start\":\"34539\",\"end\":\"34545\"},{\"start\":\"34860\",\"end\":\"34865\"},{\"start\":\"34869\",\"end\":\"34878\"},{\"start\":\"34880\",\"end\":\"34884\"},{\"start\":\"34888\",\"end\":\"34893\"},{\"start\":\"34903\",\"end\":\"34912\"},{\"start\":\"34914\",\"end\":\"34919\"},{\"start\":\"35269\",\"end\":\"35271\"},{\"start\":\"35281\",\"end\":\"35284\"},{\"start\":\"35296\",\"end\":\"35300\"},{\"start\":\"35536\",\"end\":\"35542\"},{\"start\":\"35549\",\"end\":\"35555\"},{\"start\":\"35557\",\"end\":\"35567\"},{\"start\":\"35911\",\"end\":\"35917\"},{\"start\":\"35919\",\"end\":\"35929\"},{\"start\":\"35935\",\"end\":\"35941\"},{\"start\":\"35943\",\"end\":\"35947\"},{\"start\":\"35951\",\"end\":\"35957\"},{\"start\":\"35967\",\"end\":\"35974\"},{\"start\":\"35984\",\"end\":\"35990\"},{\"start\":\"35992\",\"end\":\"35998\"},{\"start\":\"36359\",\"end\":\"36369\"},{\"start\":\"36377\",\"end\":\"36382\"},{\"start\":\"36390\",\"end\":\"36403\"},{\"start\":\"36410\",\"end\":\"36416\"},{\"start\":\"36426\",\"end\":\"36436\"},{\"start\":\"36445\",\"end\":\"36452\"},{\"start\":\"36857\",\"end\":\"36868\"},{\"start\":\"37095\",\"end\":\"37100\"},{\"start\":\"37108\",\"end\":\"37116\"},{\"start\":\"37366\",\"end\":\"37373\"},{\"start\":\"37382\",\"end\":\"37389\"},{\"start\":\"37398\",\"end\":\"37407\"},{\"start\":\"37663\",\"end\":\"37670\"},{\"start\":\"37679\",\"end\":\"37686\"},{\"start\":\"37695\",\"end\":\"37704\"},{\"start\":\"38096\",\"end\":\"38101\"},{\"start\":\"38110\",\"end\":\"38116\"},{\"start\":\"38125\",\"end\":\"38132\"},{\"start\":\"38134\",\"end\":\"38143\"},{\"start\":\"38272\",\"end\":\"38280\"},{\"start\":\"38288\",\"end\":\"38294\"},{\"start\":\"38303\",\"end\":\"38309\"},{\"start\":\"38318\",\"end\":\"38326\"},{\"start\":\"38328\",\"end\":\"38335\"},{\"start\":\"38604\",\"end\":\"38608\"},{\"start\":\"38617\",\"end\":\"38623\"},{\"start\":\"38625\",\"end\":\"38631\"},{\"start\":\"38896\",\"end\":\"38903\"},{\"start\":\"38910\",\"end\":\"38914\"},{\"start\":\"38924\",\"end\":\"38932\"},{\"start\":\"39151\",\"end\":\"39155\"},{\"start\":\"39164\",\"end\":\"39169\"},{\"start\":\"39179\",\"end\":\"39182\"},{\"start\":\"39194\",\"end\":\"39204\"},{\"start\":\"39212\",\"end\":\"39219\"},{\"start\":\"39229\",\"end\":\"39232\"},{\"start\":\"39596\",\"end\":\"39618\"},{\"start\":\"39629\",\"end\":\"39634\"},{\"start\":\"39636\",\"end\":\"39645\"},{\"start\":\"39877\",\"end\":\"39883\"},{\"start\":\"39891\",\"end\":\"39904\"},{\"start\":\"39915\",\"end\":\"39924\"},{\"start\":\"39926\",\"end\":\"39934\"},{\"start\":\"40234\",\"end\":\"40242\"},{\"start\":\"40252\",\"end\":\"40255\"},{\"start\":\"40264\",\"end\":\"40269\"},{\"start\":\"40277\",\"end\":\"40282\"},{\"start\":\"40291\",\"end\":\"40296\"},{\"start\":\"40305\",\"end\":\"40310\"},{\"start\":\"40318\",\"end\":\"40324\"},{\"start\":\"40331\",\"end\":\"40340\"},{\"start\":\"40797\",\"end\":\"40802\"},{\"start\":\"40941\",\"end\":\"40946\"},{\"start\":\"41242\",\"end\":\"41247\"},{\"start\":\"41257\",\"end\":\"41260\"},{\"start\":\"41268\",\"end\":\"41275\"},{\"start\":\"42279\",\"end\":\"42285\"},{\"start\":\"42287\",\"end\":\"42293\"},{\"start\":\"42593\",\"end\":\"42596\"},{\"start\":\"42609\",\"end\":\"42615\"},{\"start\":\"42625\",\"end\":\"42628\"},{\"start\":\"42638\",\"end\":\"42641\"},{\"start\":\"42648\",\"end\":\"42653\"},{\"start\":\"42987\",\"end\":\"42990\"},{\"start\":\"43000\",\"end\":\"43007\"},{\"start\":\"43017\",\"end\":\"43022\"},{\"start\":\"43030\",\"end\":\"43035\"},{\"start\":\"43369\",\"end\":\"43373\"},{\"start\":\"43378\",\"end\":\"43381\"},{\"start\":\"43389\",\"end\":\"43392\"},{\"start\":\"43399\",\"end\":\"43402\"},{\"start\":\"43413\",\"end\":\"43417\"}]", "bib_entry": "[{\"start\":\"29317\",\"end\":\"29534\",\"attributes\":{\"id\":\"b0\"}},{\"start\":\"29536\",\"end\":\"30005\",\"attributes\":{\"matched_paper_id\":\"4461076\",\"id\":\"b1\"}},{\"start\":\"30007\",\"end\":\"30260\",\"attributes\":{\"matched_paper_id\":\"2059771\",\"id\":\"b2\"}},{\"start\":\"30262\",\"end\":\"30563\",\"attributes\":{\"matched_paper_id\":\"11756663\",\"id\":\"b3\"}},{\"start\":\"30565\",\"end\":\"30966\",\"attributes\":{\"matched_paper_id\":\"14911023\",\"id\":\"b4\"}},{\"start\":\"30968\",\"end\":\"31376\",\"attributes\":{\"matched_paper_id\":\"41754\",\"id\":\"b5\"}},{\"start\":\"31378\",\"end\":\"31630\",\"attributes\":{\"matched_paper_id\":\"140434936\",\"id\":\"b6\"}},{\"start\":\"31632\",\"end\":\"32175\",\"attributes\":{\"matched_paper_id\":\"13058841\",\"id\":\"b7\"}},{\"start\":\"32177\",\"end\":\"32502\",\"attributes\":{\"id\":\"b8\",\"doi\":\"arXiv:1502.03167\"}},{\"start\":\"32504\",\"end\":\"32819\",\"attributes\":{\"matched_paper_id\":\"61156031\",\"id\":\"b9\"}},{\"start\":\"32821\",\"end\":\"33156\",\"attributes\":{\"matched_paper_id\":\"10147653\",\"id\":\"b10\"}},{\"start\":\"33158\",\"end\":\"33492\",\"attributes\":{\"id\":\"b11\",\"doi\":\"arXiv:1710.10196\"}},{\"start\":\"33494\",\"end\":\"33865\",\"attributes\":{\"id\":\"b12\",\"doi\":\"arXiv:1805.05553\"}},{\"start\":\"33867\",\"end\":\"34083\",\"attributes\":{\"id\":\"b13\",\"doi\":\"arXiv:1412.6980\"}},{\"start\":\"34085\",\"end\":\"34428\",\"attributes\":{\"matched_paper_id\":\"2660030\",\"id\":\"b14\"}},{\"start\":\"34430\",\"end\":\"34795\",\"attributes\":{\"matched_paper_id\":\"195908774\",\"id\":\"b15\"}},{\"start\":\"34797\",\"end\":\"35195\",\"attributes\":{\"matched_paper_id\":\"3136364\",\"id\":\"b16\"}},{\"start\":\"35197\",\"end\":\"35479\",\"attributes\":{\"id\":\"b17\",\"doi\":\"arXiv:1705.09966\"}},{\"start\":\"35481\",\"end\":\"35821\",\"attributes\":{\"matched_paper_id\":\"14366830\",\"id\":\"b18\"}},{\"start\":\"35823\",\"end\":\"36274\",\"attributes\":{\"matched_paper_id\":\"33338099\",\"id\":\"b19\"}},{\"start\":\"36276\",\"end\":\"36776\",\"attributes\":{\"matched_paper_id\":\"9431681\",\"id\":\"b20\"}},{\"start\":\"36778\",\"end\":\"37087\",\"attributes\":{\"matched_paper_id\":\"18879568\",\"id\":\"b21\"}},{\"start\":\"37089\",\"end\":\"37298\",\"attributes\":{\"id\":\"b22\",\"doi\":\"arXiv:1411.1784\"}},{\"start\":\"37300\",\"end\":\"37590\",\"attributes\":{\"id\":\"b23\",\"doi\":\"arXiv:1805.00833\"}},{\"start\":\"37592\",\"end\":\"38069\",\"attributes\":{\"matched_paper_id\":\"4559198\",\"id\":\"b24\"}},{\"start\":\"38071\",\"end\":\"38265\",\"attributes\":{\"matched_paper_id\":\"4637184\",\"id\":\"b25\"}},{\"start\":\"38267\",\"end\":\"38572\",\"attributes\":{\"id\":\"b26\",\"doi\":\"arXiv:1611.06355\"}},{\"start\":\"38574\",\"end\":\"38795\",\"attributes\":{\"matched_paper_id\":\"19566779\",\"id\":\"b27\"}},{\"start\":\"38797\",\"end\":\"39143\",\"attributes\":{\"id\":\"b28\",\"doi\":\"arXiv:1511.06434\"}},{\"start\":\"39145\",\"end\":\"39491\",\"attributes\":{\"id\":\"b29\",\"doi\":\"arXiv:1605.05396\"}},{\"start\":\"39493\",\"end\":\"39846\",\"attributes\":{\"matched_paper_id\":\"42422752\",\"id\":\"b30\"}},{\"start\":\"39848\",\"end\":\"40146\",\"attributes\":{\"matched_paper_id\":\"24405091\",\"id\":\"b31\"}},{\"start\":\"40148\",\"end\":\"40755\",\"attributes\":{\"matched_paper_id\":\"1714813\",\"id\":\"b32\"}},{\"start\":\"40757\",\"end\":\"40874\",\"attributes\":{\"id\":\"b33\"}},{\"start\":\"40876\",\"end\":\"41166\",\"attributes\":{\"id\":\"b34\"}},{\"start\":\"41168\",\"end\":\"41666\",\"attributes\":{\"matched_paper_id\":\"15344343\",\"id\":\"b35\"}},{\"start\":\"41668\",\"end\":\"42202\",\"attributes\":{\"matched_paper_id\":\"162183917\",\"id\":\"b36\"}},{\"start\":\"42204\",\"end\":\"42512\",\"attributes\":{\"matched_paper_id\":\"117482691\",\"id\":\"b37\"}},{\"start\":\"42514\",\"end\":\"42868\",\"attributes\":{\"id\":\"b38\",\"doi\":\"arXiv:1807.04836\"}},{\"start\":\"42870\",\"end\":\"43279\",\"attributes\":{\"matched_paper_id\":\"69784617\",\"id\":\"b39\"}},{\"start\":\"43281\",\"end\":\"43629\",\"attributes\":{\"id\":\"b40\",\"doi\":\"arXiv:1807.07860\"}}]", "bib_title": "[{\"start\":\"29536\",\"end\":\"29587\"},{\"start\":\"30007\",\"end\":\"30037\"},{\"start\":\"30262\",\"end\":\"30319\"},{\"start\":\"30565\",\"end\":\"30675\"},{\"start\":\"30968\",\"end\":\"31018\"},{\"start\":\"31378\",\"end\":\"31424\"},{\"start\":\"31632\",\"end\":\"31751\"},{\"start\":\"32504\",\"end\":\"32557\"},{\"start\":\"32821\",\"end\":\"32886\"},{\"start\":\"34085\",\"end\":\"34140\"},{\"start\":\"34430\",\"end\":\"34493\"},{\"start\":\"34797\",\"end\":\"34851\"},{\"start\":\"35481\",\"end\":\"35532\"},{\"start\":\"35823\",\"end\":\"35907\"},{\"start\":\"36276\",\"end\":\"36350\"},{\"start\":\"36778\",\"end\":\"36850\"},{\"start\":\"37592\",\"end\":\"37655\"},{\"start\":\"38071\",\"end\":\"38092\"},{\"start\":\"38574\",\"end\":\"38600\"},{\"start\":\"39493\",\"end\":\"39587\"},{\"start\":\"39848\",\"end\":\"39873\"},{\"start\":\"40148\",\"end\":\"40223\"},{\"start\":\"40876\",\"end\":\"40934\"},{\"start\":\"41168\",\"end\":\"41235\"},{\"start\":\"41668\",\"end\":\"41700\"},{\"start\":\"42204\",\"end\":\"42273\"},{\"start\":\"42870\",\"end\":\"42977\"}]", "bib_author": "[{\"start\":\"29371\",\"end\":\"29393\"},{\"start\":\"29393\",\"end\":\"29402\"},{\"start\":\"29402\",\"end\":\"29418\"},{\"start\":\"29589\",\"end\":\"29602\"},{\"start\":\"29602\",\"end\":\"29613\"},{\"start\":\"29613\",\"end\":\"29623\"},{\"start\":\"29623\",\"end\":\"29636\"},{\"start\":\"29636\",\"end\":\"29646\"},{\"start\":\"30039\",\"end\":\"30048\"},{\"start\":\"30048\",\"end\":\"30067\"},{\"start\":\"30067\",\"end\":\"30078\"},{\"start\":\"30078\",\"end\":\"30088\"},{\"start\":\"30321\",\"end\":\"30335\"},{\"start\":\"30335\",\"end\":\"30352\"},{\"start\":\"30352\",\"end\":\"30370\"},{\"start\":\"30677\",\"end\":\"30691\"},{\"start\":\"30691\",\"end\":\"30715\"},{\"start\":\"31020\",\"end\":\"31033\"},{\"start\":\"31033\",\"end\":\"31044\"},{\"start\":\"31044\",\"end\":\"31056\"},{\"start\":\"31056\",\"end\":\"31070\"},{\"start\":\"31070\",\"end\":\"31088\"},{\"start\":\"31088\",\"end\":\"31097\"},{\"start\":\"31426\",\"end\":\"31436\"},{\"start\":\"31436\",\"end\":\"31443\"},{\"start\":\"31753\",\"end\":\"31764\"},{\"start\":\"31764\",\"end\":\"31775\"},{\"start\":\"31775\",\"end\":\"31786\"},{\"start\":\"31786\",\"end\":\"31794\"},{\"start\":\"32271\",\"end\":\"32285\"},{\"start\":\"32285\",\"end\":\"32304\"},{\"start\":\"32559\",\"end\":\"32575\"},{\"start\":\"32575\",\"end\":\"32591\"},{\"start\":\"32591\",\"end\":\"32609\"},{\"start\":\"32888\",\"end\":\"32904\"},{\"start\":\"32904\",\"end\":\"32917\"},{\"start\":\"32917\",\"end\":\"32931\"},{\"start\":\"32931\",\"end\":\"32956\"},{\"start\":\"33234\",\"end\":\"33247\"},{\"start\":\"33247\",\"end\":\"33258\"},{\"start\":\"33258\",\"end\":\"33272\"},{\"start\":\"33272\",\"end\":\"33289\"},{\"start\":\"33494\",\"end\":\"33507\"},{\"start\":\"33507\",\"end\":\"33525\"},{\"start\":\"33525\",\"end\":\"33540\"},{\"start\":\"33540\",\"end\":\"33554\"},{\"start\":\"33554\",\"end\":\"33570\"},{\"start\":\"33570\",\"end\":\"33589\"},{\"start\":\"33589\",\"end\":\"33598\"},{\"start\":\"33911\",\"end\":\"33923\"},{\"start\":\"33923\",\"end\":\"33937\"},{\"start\":\"33937\",\"end\":\"33941\"},{\"start\":\"34142\",\"end\":\"34159\"},{\"start\":\"34159\",\"end\":\"34184\"},{\"start\":\"34495\",\"end\":\"34512\"},{\"start\":\"34512\",\"end\":\"34528\"},{\"start\":\"34528\",\"end\":\"34547\"},{\"start\":\"34853\",\"end\":\"34867\"},{\"start\":\"34867\",\"end\":\"34880\"},{\"start\":\"34880\",\"end\":\"34886\"},{\"start\":\"34886\",\"end\":\"34895\"},{\"start\":\"34895\",\"end\":\"34914\"},{\"start\":\"34914\",\"end\":\"34921\"},{\"start\":\"35262\",\"end\":\"35273\"},{\"start\":\"35273\",\"end\":\"35286\"},{\"start\":\"35286\",\"end\":\"35302\"},{\"start\":\"35534\",\"end\":\"35544\"},{\"start\":\"35544\",\"end\":\"35557\"},{\"start\":\"35557\",\"end\":\"35569\"},{\"start\":\"35909\",\"end\":\"35919\"},{\"start\":\"35919\",\"end\":\"35931\"},{\"start\":\"35931\",\"end\":\"35943\"},{\"start\":\"35943\",\"end\":\"35949\"},{\"start\":\"35949\",\"end\":\"35959\"},{\"start\":\"35959\",\"end\":\"35976\"},{\"start\":\"35976\",\"end\":\"35992\"},{\"start\":\"35992\",\"end\":\"36000\"},{\"start\":\"36352\",\"end\":\"36371\"},{\"start\":\"36371\",\"end\":\"36384\"},{\"start\":\"36384\",\"end\":\"36405\"},{\"start\":\"36405\",\"end\":\"36418\"},{\"start\":\"36418\",\"end\":\"36438\"},{\"start\":\"36438\",\"end\":\"36454\"},{\"start\":\"36852\",\"end\":\"36870\"},{\"start\":\"37089\",\"end\":\"37102\"},{\"start\":\"37102\",\"end\":\"37118\"},{\"start\":\"37360\",\"end\":\"37375\"},{\"start\":\"37375\",\"end\":\"37391\"},{\"start\":\"37391\",\"end\":\"37409\"},{\"start\":\"37657\",\"end\":\"37672\"},{\"start\":\"37672\",\"end\":\"37688\"},{\"start\":\"37688\",\"end\":\"37706\"},{\"start\":\"38094\",\"end\":\"38103\"},{\"start\":\"38103\",\"end\":\"38118\"},{\"start\":\"38118\",\"end\":\"38134\"},{\"start\":\"38134\",\"end\":\"38145\"},{\"start\":\"38267\",\"end\":\"38282\"},{\"start\":\"38282\",\"end\":\"38296\"},{\"start\":\"38296\",\"end\":\"38311\"},{\"start\":\"38311\",\"end\":\"38328\"},{\"start\":\"38328\",\"end\":\"38337\"},{\"start\":\"38602\",\"end\":\"38610\"},{\"start\":\"38610\",\"end\":\"38625\"},{\"start\":\"38625\",\"end\":\"38633\"},{\"start\":\"38891\",\"end\":\"38905\"},{\"start\":\"38905\",\"end\":\"38916\"},{\"start\":\"38916\",\"end\":\"38934\"},{\"start\":\"39145\",\"end\":\"39157\"},{\"start\":\"39157\",\"end\":\"39171\"},{\"start\":\"39171\",\"end\":\"39184\"},{\"start\":\"39184\",\"end\":\"39206\"},{\"start\":\"39206\",\"end\":\"39221\"},{\"start\":\"39221\",\"end\":\"39234\"},{\"start\":\"39589\",\"end\":\"39620\"},{\"start\":\"39620\",\"end\":\"39636\"},{\"start\":\"39636\",\"end\":\"39647\"},{\"start\":\"39875\",\"end\":\"39885\"},{\"start\":\"39885\",\"end\":\"39906\"},{\"start\":\"39906\",\"end\":\"39926\"},{\"start\":\"39926\",\"end\":\"39936\"},{\"start\":\"40225\",\"end\":\"40244\"},{\"start\":\"40244\",\"end\":\"40257\"},{\"start\":\"40257\",\"end\":\"40271\"},{\"start\":\"40271\",\"end\":\"40284\"},{\"start\":\"40284\",\"end\":\"40298\"},{\"start\":\"40298\",\"end\":\"40312\"},{\"start\":\"40312\",\"end\":\"40326\"},{\"start\":\"40326\",\"end\":\"40342\"},{\"start\":\"40792\",\"end\":\"40804\"},{\"start\":\"40936\",\"end\":\"40948\"},{\"start\":\"41237\",\"end\":\"41249\"},{\"start\":\"41249\",\"end\":\"41262\"},{\"start\":\"41262\",\"end\":\"41277\"},{\"start\":\"42275\",\"end\":\"42287\"},{\"start\":\"42287\",\"end\":\"42295\"},{\"start\":\"42585\",\"end\":\"42598\"},{\"start\":\"42598\",\"end\":\"42617\"},{\"start\":\"42617\",\"end\":\"42630\"},{\"start\":\"42630\",\"end\":\"42643\"},{\"start\":\"42643\",\"end\":\"42655\"},{\"start\":\"42979\",\"end\":\"42992\"},{\"start\":\"42992\",\"end\":\"43009\"},{\"start\":\"43009\",\"end\":\"43024\"},{\"start\":\"43024\",\"end\":\"43037\"},{\"start\":\"43364\",\"end\":\"43375\"},{\"start\":\"43375\",\"end\":\"43383\"},{\"start\":\"43383\",\"end\":\"43394\"},{\"start\":\"43394\",\"end\":\"43404\"},{\"start\":\"43404\",\"end\":\"43419\"}]", "bib_venue": "[{\"start\":\"29317\",\"end\":\"29369\"},{\"start\":\"29646\",\"end\":\"29723\"},{\"start\":\"30088\",\"end\":\"30117\"},{\"start\":\"30370\",\"end\":\"30398\"},{\"start\":\"30715\",\"end\":\"30758\"},{\"start\":\"31097\",\"end\":\"31156\"},{\"start\":\"31443\",\"end\":\"31482\"},{\"start\":\"31794\",\"end\":\"31861\"},{\"start\":\"32177\",\"end\":\"32269\"},{\"start\":\"32609\",\"end\":\"32649\"},{\"start\":\"32956\",\"end\":\"32971\"},{\"start\":\"33158\",\"end\":\"33232\"},{\"start\":\"33614\",\"end\":\"33658\"},{\"start\":\"33867\",\"end\":\"33909\"},{\"start\":\"34184\",\"end\":\"34236\"},{\"start\":\"34547\",\"end\":\"34596\"},{\"start\":\"34921\",\"end\":\"34974\"},{\"start\":\"35197\",\"end\":\"35260\"},{\"start\":\"35569\",\"end\":\"35637\"},{\"start\":\"36000\",\"end\":\"36035\"},{\"start\":\"36454\",\"end\":\"36518\"},{\"start\":\"36870\",\"end\":\"36918\"},{\"start\":\"37133\",\"end\":\"37172\"},{\"start\":\"37300\",\"end\":\"37358\"},{\"start\":\"37706\",\"end\":\"37783\"},{\"start\":\"38145\",\"end\":\"38149\"},{\"start\":\"38353\",\"end\":\"38398\"},{\"start\":\"38633\",\"end\":\"38671\"},{\"start\":\"38797\",\"end\":\"38889\"},{\"start\":\"39250\",\"end\":\"39296\"},{\"start\":\"39647\",\"end\":\"39653\"},{\"start\":\"39936\",\"end\":\"39980\"},{\"start\":\"40342\",\"end\":\"40433\"},{\"start\":\"40757\",\"end\":\"40790\"},{\"start\":\"40948\",\"end\":\"40993\"},{\"start\":\"41277\",\"end\":\"41392\"},{\"start\":\"41702\",\"end\":\"41779\"},{\"start\":\"42295\",\"end\":\"42333\"},{\"start\":\"42514\",\"end\":\"42583\"},{\"start\":\"43037\",\"end\":\"43068\"},{\"start\":\"43281\",\"end\":\"43362\"},{\"start\":\"29725\",\"end\":\"29787\"},{\"start\":\"31863\",\"end\":\"31915\"},{\"start\":\"37785\",\"end\":\"37847\"},{\"start\":\"41899\",\"end\":\"41961\"}]"}}}, "year": 2023, "month": 12, "day": 17}