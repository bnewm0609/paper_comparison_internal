{"id": 258833251, "updated": "2023-10-09 05:29:31.223", "metadata": {"title": "Training Diffusion Models with Reinforcement Learning", "authors": "[{\"first\":\"Kevin\",\"last\":\"Black\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Janner\",\"middle\":[]},{\"first\":\"Yilun\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Ilya\",\"last\":\"Kostrikov\",\"middle\":[]},{\"first\":\"Sergey\",\"last\":\"Levine\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project's website can be found at http://rl-diffusion.github.io .", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.13301", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-13301", "doi": "10.48550/arxiv.2305.13301"}}, "content": {"source": {"pdf_hash": "d8c78221e4366d6a72a6b3e41e35b706cc45c01d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.13301v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "97bc33c62739709c4d0511b288c06ab43582b7e8", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/d8c78221e4366d6a72a6b3e41e35b706cc45c01d.txt", "contents": "\nTRAINING DIFFUSION MODELS WITH REINFORCEMENT LEARNING\n1 Oct 2023\n\nKevin Black kvablack@berkeley.edu \nUniversity of California\nBerkeley\n\nMichael Janner janner@berkeley.edu \nUniversity of California\nBerkeley\n\nYilun Du yilundu@mit.edu \nMassachusetts Institute of Technology\n\n\nIlya Kostrikov kostrikov@berkeley.edu \nUniversity of California\nBerkeley\n\nSergey Levine sergey.levine@berkeley.edu \nUniversity of California\nBerkeley\n\nTRAINING DIFFUSION MODELS WITH REINFORCEMENT LEARNING\n1 Oct 20234F2CC987FADA768C4A8D48DA5EE819AAarXiv:2305.13301v3[cs.LG]\nDiffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective.However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness.In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives.We describe how posing denoising as a multi-step decisionmaking problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches.Empirically, DDPO can adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality.Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation.The project's website can be found at http://rl-diffusion.github.io.\n\nINTRODUCTION\n\nDiffusion probabilistic models (Sohl-Dickstein et al., 2015) have recently emerged as the de facto standard for generative modeling in continuous domains.Their flexibility in representing complex, high-dimensional distributions has led to the adoption of diffusion models in applications including image and video synthesis (Ramesh et al., 2021;Saharia et al., 2022;Ho et al., 2022), drug and material design (Xu et al., 2021;Xie et al., 2021;Schneuing et al., 2022), and continuous control (Janner et al., 2022;Wang et al., 2022;Hansen-Estruch et al., 2023).The key idea behind diffusion models is to iteratively transform a simple prior distribution into a target distribution by applying a sequential denoising process.This procedure is conventionally motivated as a maximum likelihood estimation problem, with the objective derived as a variational lower bound on the log-likelihood of the training data.\n\nHowever, most use cases of diffusion models are not directly concerned with likelihoods, but instead with downstream objective such as human-perceived image quality or drug effectiveness.In this paper, we consider the problem of training diffusion models to satisfy such objectives directly, as opposed to matching a data distribution.This problem is challenging because exact likelihood computation with diffusion models is intractable, making it difficult to apply many conventional reinforcement learning (RL) algorithms.We instead propose to frame denoising as a multi-step decision-making task, using the exact likelihoods at each denoising step in place of the approximate likelihoods induced by a full denoising process.We present a policy gradient algorithm, which we refer to as denoising diffusion policy optimization (DDPO), that can optimize a diffusion model for downstream tasks using only a black-box reward function.\n\nWe apply our algorithm to the finetuning of large text-to-image diffusion models.Our initial evaluation focuses on tasks that are difficult to specify via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality.However, because many reward functions of interest are difficult to specify programmatically, finetuning procedures often rely on large-scale human labeling efforts to obtain a reward signal (Ouyang et al., 2022).In the case of text-to-image diffusion, we propose a method for replacing such labeling with feedback from a vision-language model (VLM).Similar to RLAIF finetuning for language models (Bai et al., 2022b), the resulting procedure allows for diffusion models to be adapted to reward functions that would otherwise require additional human annotations.We use this procedure to improve prompt-image alignment for unusual subject-setting compositions.\n\nOur contributions are as follows.We first present the derivation and conceptual motivation of DDPO.We then document the design of various reward functions for text-to-image generation, ranging from simple computations to workflows involving large VLMs, and demonstrate the effectiveness of DDPO compared to alternative reward-weighted likelihood methods in these settings.Finally, we demonstrate the generalization ability of our finetuning procedure to unseen prompts.\n\n\nRELATED WORK\n\nDiffusion probabilistic models.Denoising diffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020) have emerged as an effective class of generative models for modalities including images (Ramesh et al., 2021;Saharia et al., 2022), videos (Ho et al., 2022;Singer et al., 2022), 3D shapes (Zhou et al., 2021;Zeng et al., 2022), and robotic trajectories (Janner et al., 2022;Ajay et al., 2022;Chi et al., 2023).While the denoising objective is conventionally derived as an approximation to likelihood, the training of diffusion models typically departs from maximum likelihood in several ways (Ho et al., 2020).Modifying the objective to more strictly optimize likelihood (Nichol & Dhariwal, 2021;Kingma et al., 2021) often leads to worsened image quality, as likelihood is not a faithful proxy for visual quality.In this paper, we show how diffusion models can be optimized directly for downstream objectives.\n\nControllable generation with diffusion models.Recent progress in text-to-image diffusion models (Ramesh et al., 2021;Saharia et al., 2022) has enabled fine-grained high-resolution image synthesis.To further improve the controllability and quality of diffusion models, recent approaches have investigated finetuning on limited user-provided data (Ruiz et al., 2022), optimizing text embeddings for new concepts (Gal et al., 2022), composing models (Du et al., 2023;Liu et al., 2022), adapters for additional input constraints (Zhang & Agrawala, 2023), and inference-time techniques such as classifier (Dhariwal & Nichol, 2021) and classifier-free (Ho & Salimans, 2021) guidance.\n\nReinforcement learning from human feedback.A number of works have studied using human feedback to optimize models in settings such as simulated robotic control (Christiano et al., 2017), game-playing (Knox & Stone, 2008), machine translation (Nguyen et al., 2017), citation retrieval (Menick et al., 2022), browsing-based question-answering (Nakano et al., 2021), summarization (Stiennon et al., 2020;Ziegler et al., 2019), instruction-following (Ouyang et al., 2022), and alignment with specifications (Bai et al., 2022a).Recently, Lee et al. (2023) studied the alignment of text-toimage diffusion models to human preferences using a method based on reward-weighted likelihood maximization.In our comparisons, their method corresponds to one iteration of the RWR method.Our results demonstrate that DDPO significantly outperforms even multiple iterations of weighted likelihood maximization (RWR-style) optimization.\n\nDiffusion models as sequential decision-making processes.Although predating diffusion models, Bachman & Precup (2015) similarly posed data generation as a sequential decision-making problem and used the resulting framework to apply reinforcement learning methods to image generation.More recently, Fan & Lee (2023) introduced a policy gradient method for training diffusion models.However, this paper aimed to improve data distribution matching rather than optimizing downstream objectives, and therefore the only reward function considered was a GAN-like discriminator.In concurrent work to ours, DPOK (Fan et al., 2023) built upon Fan & Lee (2023) and Lee et al. (2023) to better align text-to-image diffusion models to human preferences using a policy gradient algorithm.Like Lee et al. (2023), DPOK only considers a single preference-based reward function (Xu et al., 2023); additionally, their work studies KL-regularization and primarily focuses on training a different diffusion model for each prompt. 1 In contrast, we train on many prompts at once (up to 398) and demonstrate generalization to many more prompts outside of the training set.Furthermore, we study how DDPO can be applied to multiple reward functions beyond those based on human feedback, including how rewards derived automatically from VLMs can improve prompt-image alignment.\n\n\nPRELIMINARIES\n\nIn this section, we provide a brief background on diffusion models and the RL problem formulation.\n\n\nDIFFUSION MODELS\n\nIn this work, we consider conditional diffusion probabilistic models (Sohl-Dickstein et al., 2015;Ho et al., 2020), which represent a distribution p(x 0 |c) over a dataset of samples x 0 and corresponding contexts c.The distribution is modeled as the reverse of a Markovian forward process q(x t | x t\u22121 ), which iteratively adds noise to the data.Reversing the forward process can be accomplished by training a neural network \u00b5 \u03b8 (x t , c, t) with the following objective:\nL DDPM (\u03b8) = E (x0,c)\u223cp(x0,c), t\u223cU {0,T }, xt\u223cq(xt|x0) \u2225 \u03bc(x 0 , t) \u2212 \u00b5 \u03b8 (x t , c, t)\u2225 2 (1)\nwhere \u03bc is the posterior mean of the forward process, a weighted average of x 0 and x t .This objective is justified as maximizing a variational lower bound on the log-likelihood of the data (Ho et al., 2020).\n\nSampling from a diffusion model begins with drawing a random x T \u223c N (0, I) and following the reverse process p \u03b8 (x t\u22121 | x t , c) to produce a trajectory {x T , x T \u22121 , . . ., x 0 } ending with a sample x 0 .The sampling process depends not only on the predictor \u00b5 \u03b8 but also the choice of sampler.Most popular samplers (Ho et al., 2020;Song et al., 2021) use an isotropic Gaussian reverse process with a fixed timestep-dependent variance:\np \u03b8 (x t\u22121 | x t , c) = N (x t\u22121 | \u00b5 \u03b8 (x t , c, t) , \u03c3 2 t I).\n(2)\n\n\nMARKOV DECISION PROCESSES AND REINFORCEMENT LEARNING\n\nA Markov decision process (MDP) is a formalization of sequential decision-making problems.An MDP is defined by a tuple (S, A, \u03c1 0 , P, R), in which S is the state space, A is the action space, \u03c1 0 is the distribution of initial states, P is the transition kernel, and R is the reward function.At each timestep t, the agent observes a state s t \u2208 S, takes an action a t \u2208 A, receives a reward R(s t , a t ), and transitions to a new state s t+1 \u223c P (s t+1 | s t , a t ).An agent acts according to a policy \u03c0(a | s).\n\nAs the agent acts in the MDP, it produces trajectories, which are sequences of states and actions \u03c4 = (s 0 , a 0 , s 1 , a 1 , . . ., s T , a T ).The reinforcement learning (RL) objective is for the agent to maximize J RL (\u03c0), the expected cumulative reward over trajectories sampled from its policy:\nJ RL (\u03c0) = E \u03c4 \u223cp(\u03c4 |\u03c0) T t=0 R(s t , a t ) .\n\nREINFORCEMENT LEARNING TRAINING OF DIFFUSION MODELS\n\nWe now describe how RL algorithms can be used to train diffusion models.We present two classes of methods and show that each corresponds to a different mapping of the denoising process to the MDP framework.\n\n\nPROBLEM STATEMENT\n\nWe assume a pre-existing diffusion model, which may be pretrained or randomly initialized.Assuming a fixed sampler, the diffusion model induces a sample distribution p \u03b8 (x 0 | c).The denoising diffusion RL objective is to maximize a reward signal r defined on the samples and contexts:\nJ DDRL (\u03b8) = E c\u223cp(c), x0\u223cp \u03b8 (x0|c) [r(x 0 , c)]\nfor some context distribution p(c) of our choosing.\n\n\nREWARD-WEIGHTED REGRESSION\n\nTo optimize J DDRL with minimal changes to standard diffusion model training, we can use the denoising loss L DDPM (Equation 1), but with training data x 0 \u223c p \u03b8 (x 0 | c) and an added weighting that depends on the reward r(x 0 , c). Lee et al. ( 2023) describe a single-round version of this procedure for diffusion models, but in general this approach can be performed for multiple rounds of alternating sampling and training, leading to an online RL method.We refer to this general class of algorithms as reward-weighted regression (RWR) (Peters & Schaal, 2007).\n\nA standard weighting scheme uses exponentiated rewards to ensure nonnegativity,\nw RWR (x 0 , c) = 1 Z exp \u03b2r(x 0 , c) ,\nwhere \u03b2 is an inverse temperature and Z is a normalization constant.We also consider a simplified weighting scheme that uses binary weights,\nw sparse (x 0 , c) = 1 r(x 0 , c) \u2265 C ,\nwhere C is a reward threshold determining which samples are used for training.In supervised learning terms, this is equivalent to repeated filtered finetuning on training data coming from the model.\n\nWithin the RL formalism, the RWR procedure corresponds to the following one-step MDP:\ns \u225c c a \u225c x 0 \u03c0(a | s) \u225c p \u03b8 (x 0 | c) \u03c1 0 (s) \u225c p(c) R(s, a) \u225c r(x 0 , c)\nwith a transition kernel P that immediately leads to an absorbing termination state.Therefore, maximizing J DDRL (\u03b8) is equivalent to maximizing the RL objective J RL (\u03c0) in this MDP.\n\nFrom RL literature, weighting a log-likelihood objective by w RWR is known to approximately maximize J RL (\u03c0) subject to a KL divergence constraint on \u03c0 (Nair et al., 2020).However, L DDPM (Equation 1) does not involve an exact log-likelihood -it is instead derived as a variational bound on log p \u03b8 (x 0 | c).Therefore, the RWR procedure applied to diffusion model training is not theoretically justified and only optimizes J DDRL very approximately.\n\n\nDENOISING DIFFUSION POLICY OPTIMIZATION\n\nRWR relies on an approximate log-likelihood because it ignores the sequential nature of the denoising process, only using the final samples x 0 .In this section, we show how the denoising process can be reframed as a multi-step MDP, allowing us to directly optimize J DDRL using policy gradient estimators.This follows the derivation in Fan & Lee (2023), who prove an equivalence between their method and a policy gradient algorithm where the reward is a GAN-like discriminator.We present a general framework with an arbitrary reward function, motivated by our desire to optimize arbitrary downstream objectives (Section 5).We refer to this class of algorithms as denoising diffusion policy optimization (DDPO) and present two variants based on specific gradient estimators.\n\nDenoising as a multi-step MDP.We map the iterative denoising procedure to the following MDP:\ns t \u225c (c, t, x t ) \u03c0(a t | s t ) \u225c p \u03b8 (x t\u22121 | x t , c) P (s t+1 | s t , a t ) \u225c \u03b4 c , \u03b4 t\u22121 , \u03b4 xt\u22121 a t \u225c x t\u22121 \u03c1 0 (s 0 ) \u225c p(c), \u03b4 T , N (0, I) R(s t , a t ) \u225c r(x 0 , c) if t = 0 0 otherwise\nin which \u03b4 y is the Dirac delta distribution with nonzero density only at y. Trajectories consist of T timesteps, after which P leads to a termination state.The cumulative reward of each trajectory is equal to r(x 0 , c), so maximizing J DDRL (\u03b8) is equivalent to maximizing J RL (\u03c0) in this MDP.\n\nThe benefit of this formulation is that if we use a standard sampler with p \u03b8 (x t\u22121 | x t , c) parameterized as in Equation 2, the policy \u03c0 becomes an isotropic Gaussian as opposed to the arbitrarily complicated distribution p \u03b8 (x 0 | c) as it is in the RWR formulation.This simplification allows for the evaluation of exact log-likelihoods and their gradients with respect to the diffusion model parameters.\n\nPolicy gradient estimation.With access to likelihoods and likelihood gradients, we can make direct Monte Carlo estimates of \u2207 \u03b8 J DDRL .Like RWR, DDPO alternates collecting denoising trajectories {x T , x T \u22121 , . . ., x 0 } via sampling and updating parameters via gradient descent.\n\nThe first variant of DDPO, which we call DDPO SF , uses the score function policy gradient estimator, also known as the likelihood ratio method or REINFORCE (Williams, 1992;Mohamed et al., 2020):\n\u2207 \u03b8 J DDRL = E T t=0 \u2207 \u03b8 log p \u03b8 (x t\u22121 | x t , c) r(x 0 , c) (DDPO SF )\nwhere the expectation is taken over denoising trajectories generated by the current parameters \u03b8.\n\nHowever, this estimator only allows for one step of optimization per round of data collection, as the gradient must be computed using data generated by the current parameters.To perform multiple steps of optimization, we may use an importance sampling estimator (Kakade & Langford, 2002):\n\u2207 \u03b8 J DDRL = E T t=0 p \u03b8 (x t\u22121 | x t , c) p \u03b8old (x t\u22121 | x t , c) \u2207 \u03b8 log p \u03b8 (x t\u22121 | x t , c) r(x 0 , c) (DDPO IS )\nwhere the expectation is taken over denoising trajectories generated by the parameters \u03b8 old .This estimator becomes inaccurate if p \u03b8 deviates too far from p \u03b8old , which can be addressed using trust regions (Schulman et al., 2015) to constrain the size of the update.In practice, we implement the trust region via clipping, as in proximal policy optimization (Schulman et al., 2017).\n\n\nREWARD FUNCTIONS FOR TEXT-TO-IMAGE DIFFUSION\n\nIn this work, we evaluate our methods on text-to-image diffusion.Text-to-image diffusion serves as a valuable test environment for reinforcement learning due to the availability of large pretrained models and the versatility of using diverse and visually interesting reward functions.In this section, we outline our selection of reward functions.We study a spectrum of reward functions of varying complexity, ranging from those that are straightforward to specify and evaluate to those that capture the depth of real-world downstream tasks.\n\n\nCOMPRESSIBILITY AND INCOMPRESSIBILITY\n\nThe capabilities of text-to-image diffusion models are limited by the co-occurrences of text and images in their training distribution.For instance, images are rarely captioned with their file size, making it impossible to specify a desired file size via prompting.This limitation makes reward functions based on file size a convenient case study: they are simple to compute, but not controllable through the conventional methods of likelihood maximization and prompt engineering.We fix the resolution of diffusion model samples at 512x512, such that the file size is determined solely by the compressibility of the image.We define two tasks based on file size: compressibility, in which the file size of the image after JPEG compression is minimized, and incompressibility, in which the same measure is maximized.\n\n\nAESTHETIC QUALITY\n\nTo capture a reward function that would be useful to a human user, we define a task based on perceived aesthetic quality.We use the LAION aesthetics predictor (Schuhmann, 2022), which is trained on 176,000 human image ratings.The predictor is implemented as a linear model on top of CLIP embeddings (Radford et al., 2021).Annotations range between 1 and 10, with the highest-rated images mostly containing artwork.Since the aesthetic quality predictor is trained on human judgments, this task constitutes reinforcement learning from human feedback (Ouyang et al., 2022;Christiano et al., 2017;Ziegler et al., 2019).\n\n\nAUTOMATED PROMPT ALIGNMENT WITH VISION-LANGUAGE MODELS\n\nA very general-purpose reward function for training a text-to-image model is prompt-image alignment.However, specifying a reward that captures generic prompt alignment is difficult, conventionally requiring large-scale human labeling efforts.We propose using an existing VLM to replace additional human annotation.This design is inspired by recent work on RLAIF (Bai et al., 2022b), in which language models are improved using feedback from themselves.\n\nWe use LLaVA (Liu et al., 2023), a state-of-the-art VLM, to describe an image.The finetuning reward is the BERTScore (Zhang et al., 2020) recall metric, a measure of semantic similarity, using the prompt as the reference sentence and the VLM description as the candidate sentence.Samples that more faithfully include all of the details of the prompt receive higher rewards, to the extent that those visual details are legible to the VLM.\n\nIn Figure 2, we show one simple question: \"what is happening in this image?\".While this captures the general task of prompt-image alignment, in principle any question could be used to specify complex or hard-to-define reward functions for a particular use case.One could even employ a language model to automatically generate candidate questions and evaluate responses based on the prompt.This framework provides a flexible interface where the complexity of the reward function is only limited by the capabilities of the vision and language models involved.\n\n\nEXPERIMENTAL EVALUATION\n\nThe purpose of our experiments is to evaluate the effectiveness of RL algorithms for finetuning diffusion models to align with a variety of user-specified objectives.After examining the viability of the general approach, we focus on the following questions:\n\n1. How do variants of DDPO compare to RWR and to each other?2. Can VLMs allow for optimizing rewards that are difficult to specify manually?3. Do the effects of RL finetuning generalize to prompts not seen during finetuning?The relative effectiveness of different RL algorithms on three reward functions.We find that the policy gradient variants, denoted DDPO, are more effective optimizers than both RWR variants.\n\n\nALGORITHM COMPARISONS\n\nWe begin by evaluating all methods on the compressibility, incompressibility, and aesthetic quality tasks, as these tasks isolate the effectiveness of the RL approach from considerations relating to the VLM reward function.We use Stable Diffusion v1.4 (Rombach et al., 2022) as the base model for all experiments.Compressibility and incompressibility prompts are sampled uniformly from all 398 animals in the ImageNet-1000 (Deng et al., 2009) categories.Aesthetic quality prompts are sampled uniformly from a smaller set of 45 common animals.\n\nAs shown qualitatively in Figure 3, DDPO is able to effectively adapt a pretrained model with only the specification of a reward function and without any further data curation.The strategies found to optimize each reward are nontrivial; for example, to maximize LAION-predicted aesthetic quality, DDPO transforms a model that produces naturalistic images into one that produces artistic drawings.\n\nTo maximize compressibility, DDPO removes backgrounds and applies smoothing to what remains.\n\nTo maximize incompressibility, DDPO finds artifacts that are difficult for the JPEG compression algorithm to encode, such as high-frequency noise and sharp edges.Samples from RWR are provided in Appendix E for comparison.We provide a quantitative comparison of all methods in Figure 4. We plot the attained reward as a function of the number of queries to the reward function, as reward evaluation becomes the limiting factor in many practical applications.DDPO shows a clear advantage over RWR on all tasks, demonstrating that formulating the denoising process as a multi-step MDP and estimating the policy gradient directly is more effective than optimizing a reward-weighted variational bound on log-likelihood.Within the DDPO class, the importance sampling estimator slightly outperforms the score function estimator, likely due to the increased number of optimization steps.Within the RWR class, the performance of weighting schemes is comparable, making the sparse weighting scheme preferable on these tasks due to its simplicity and reduced resource requirements.\n\n\nAUTOMATED PROMPT ALIGNMENT\n\nWe next evaluate the ability of VLMs, in conjunction with DDPO, to automatically improve the image-prompt alignment of the pretrained model without additional human labels.We focus on DDPO IS for this experiment, as we found it to be the most effective algorithm in Section 6.1.The prompts for this task all have the form \"a(n) [animal] [activity] \", where the animal comes from the same list of 45 common animals used in Section 6.1 and the activity is chosen from a list of 3 activities: \"riding a bike\", \"playing chess\", and \"washing dishes\".\n\nThe progression of finetuning is depicted in Figure 5. Qualitatively, the samples come to depict the prompts much more faithfully throughout the course of training.This trend is also reflected quantitatively, though is less salient as small changes in BERTScore can correspond to large differences in relevance (Zhang et al., 2020).It is important to note that some of the prompts in the finetuning set, such as \"a dolphin riding a bike\", had zero success rate from the pretrained model; if trained in isolation, this prompt would be unlikely to ever improve because there would be no reward signal.It was only via transferrable learning across prompts that these difficult prompts could improve.\n\nNearly all of the samples become more cartoon-like or artistic during finetuning.This was not optimized for directly.We hypothesize that this is a function of the pretraining distribution; though it would be extremely rare to see a photorealistic image of a bear washing dishes, it would be much less unusual to see the scene depicted in a children's book.As a result, in the process of satisfying the content of the prompt, the style of the samples also changes.\n\n\nPretrained\n\nAesthetic Quality (New Animals) Aesthetic Quality (Non-Animals) Prompt Alignment (New Scenarios)\n\n\nFinetuned\n\nFigure 6 (Generalization) Finetuning on a limited set of animals generalizes to both new animals and non-animal everyday objects.The prompts for the rightmost two columns are \"a capybara washing dishes\" and \"a duck taking an exam\".A quantitative analysis is provided in Appendix D, and more samples are provided in Appendix E.\n\n\nGENERALIZATION\n\nRL finetuning on large language models has been shown to produce interesting generalization properties; for example, instruction finetuning almost entirely in English has been shown to improve capabilities in other languages (Ouyang et al., 2022).It is difficult to reconcile this phenomenon with our current understanding of generalization; it would a priori seem more likely for finetuning to have an effect only on the finetuning prompt set or distribution.In order to investigate the same phenomenon with diffusion models, Figure 6 shows a set of DDPO-finetuned model samples corresponding to prompts that were not seen during finetuning.In concordance with instructionfollowing transfer in language modeling, we find that the effects of finetuning do generalize, even with prompt distributions as narrow as 45 animals and 3 activities.We find evidence of generalization to animals outside of the training distribution, to non-animal everyday objects, and in the case of prompt-image alignment, even to novel activities such as \"taking an exam\".\n\n\nDISCUSSION AND LIMITATIONS\n\nWe presented an RL-based framework for training denoising diffusion models to directly optimize a variety of reward functions.By posing the iterative denoising procedure as a multi-step decisionmaking problem, we were able to design a class of policy gradient algorithms that are highly effective at training diffusion models.We found that DDPO was an effective optimizer for tasks that are difficult to specify via prompts, such as image compressibility, and difficult to evaluate programmatically, such as semantic alignment with prompts.To provide an automated way to derive rewards, we also proposed a method for using VLMs to provide feedback on the quality of generated images.While our evaluation considers a variety of prompts, the full range of images in our experiments was constrained (e.g., animals performing activities).Future iterations could expand both the questions posed to the VLM, possibly using language models to propose relevant questions based on the prompt, as well as the diversity of the prompt distribution.We also chose not to study the problem of overoptimization, a common issue with RL finetuning in which the model diverges too far from the original distribution to be useful (see Appendix A); we highlight this as an important area for future work.We hope that this work will provide a step toward more targeted training of large generative models, where optimization via RL can produce models that are effective at achieving user-specified goals rather than simply matching an entire data distribution.\n\nBroader Impacts.Generative models can be valuable productivity aids, but may also pose harm when used for disinformation, impersonation, or phishing.Our work aims to make diffusion models more useful by enabling them to optimize user-specified objectives.This adaptation has beneficial applications, such as the generation of more understandable educational material, but may also be used maliciously, in ways that we do not outline here.Work on the reliable detection of synthetic content remains important to mitigate such harms from generative models.\n\u03b5\u03b8 (x t , t, c) = w\u03f5 \u03b8 (x t , t, c) + (1 \u2212 w)\u03f5 \u03b8 (x t , t)(3)\nwhere \u03f5 \u03b8 is the \u03f5-prediction parameterization of the diffusion model (Ho et al., 2020) and \u03b5\u03b8 is the guided \u03f5-prediction that is used to compute the next denoised sample.\n\nFor reinforcement learning, it does not make sense to train on the unconditional objective since the reward may depend on the context.However, we found that when only training on the conditional objective, performance rapidly deteriorated after the first round of finetuning.We hypothesized that this is due to the guidance weight becoming miscalibrated each time the model is updated, leading to degraded samples, which in turn impair the next round of finetuning, and so on.Our solution was to choose a fixed guidance weight and use the guided \u03f5-prediction during training as well as sampling.\n\nWe call this procedure CFG training.Figure 8 shows the effect of CFG training on RWR sparse ; it has no effect after a single round of finetuning, but becomes essential for subsequent rounds.Samples per Iteration (DDPOIS)\n\n\n256\n\nFigure 9 (RWR interleaving ablation) Ablation over the number of samples collected per iteration for RWR.The number of gradient updates per iteration remains the same throughout.We find that more frequent interleaving is beneficial up to a point, after which it causes performance degradation.However, RWR is still unable to match the asymptotic performance of DDPO at any level of interleaving.\n\n\nC.2 INTERLEAVING\n\nThere are two main differences between DDPO and RWR, as compared in Section 6.1: the objective (DDPO uses the policy gradient) and the data distribution (DDPO is significantly more on-policy, collecting 256 samples per iteration as opposed to 10,000 for RWR).This choice is motivated by standard RL practice, in which policy gradient methods specifically require on-policy data (Sutton et al., 1999), whereas RWR is designed to work in on off-policy data (Nair et al., 2020) and is known to underperform other algorithms in more online settings (Duan et al., 2016).\n\nHowever, we can isolate the effect of the data distribution by varying how interleaved the sampling and training are in RWR.At one extreme is a single-round algorithm (Lee et al., 2023), in which N samples are collected from the pretrained model and used for finetuning.It is also possible to run k rounds of finetuning each on N k samples collected from the most up-to-date model.In Figure 9, we evaluate this hyperparameter and find that increased interleaving does help up to a point, after which it causes performance degradation.However, RWR is still unable to match the asymptotic performance of DDPO at any level of interleaving.\n\n\nAPPENDIX D QUANTITATIVE RESULTS FOR GENERALIZATION\n\nIn Section 6.3, we presented qualitative evidence of both the aesthetic quality model and the imageprompt alignment model generalizing to prompts that were unseen during finetuning.In Figure 10, we provide an additional quantitative analysis of generalization with the aesthetic quality model, where we measure the average reward throughout training for several prompt distributions.In accordance with the qualitative evidence, we see that the model generalizes very well to unseen animals, and everyday objects to a lesser degree.\n\n\nAPPENDIX E MORE SAMPLES\n\nFigure 11 shows qualitative samples from the baseline RWR method.Figure 12 shows more samples on seen prompts from DDPO finetuning with the image-prompt alignment reward function.Figure 13 shows more examples of generalization to unseen animals and everyday objects with the aesthetic quality reward function.Figure 14 shows more examples of generalization to unseen subjects and activities with the image-prompt alignment reward function.\n\n\n\nFigure 1 (Reinforcement learning for diffusion models) We propose a reinforcement learning algorithm, DDPO, for optimizing diffusion models on downstream objectives such as compressibility, aesthetic quality, and prompt-image alignment as determined by vision-language models.Each row shows a progression of samples for the same prompt and random seed over the course of training.\n\n\nFigure 2 (\n2\nFigure 2 (VLM reward function) Illustration of the VLM-based reward function for prompt-image alignment.LLaVA (Liu et al., 2023) provides a short description of a generated image; the reward is the similarity between this description and the original prompt as measured by BERTScore (Zhang et al., 2020).\n\n\nFigure 4 (\n4\nFigure 3 (DDPO samples) Qualitative depiction of the effects of RL finetuning on different reward functions.DDPO transforms naturalistic images into stylized artwork to maximize aesthetic quality, removes background content and applies foreground smoothing to maximize compressibility, and adds high-frequency noise to maximize incompressibility.\n\n\nFigure 5 (\n5\nFigure 5 (Prompt alignment) (L) Progression of samples for the same prompt and random seed over the course of training.The images become significantly more faithful to the prompt.The samples also adopt a cartoon-like style, which we hypothesize is because the prompts are more likely depicted as illustrations than realistic photographs in the pretraining distribution.(R) Quantitative improvement of prompt alignment.Each thick line is the average score for an activity, while the faint lines show average scores for a few randomly selected individual prompts.\n\n\nFigure 8 (\n8\nFigure 8 (CFG training) We run the RWR sparse algorithm while optimizing only the conditional \u03f5prediction (without CFG training), and while optimizing the guided \u03f5-prediction (with CFG training).Each point denotes a diffusion model update.We find that CFG training is essential for methods that do more than one round of interleaved sampling and training.\n\n\nFigure 10 (Figure 14 (\n1014\nFigure10(Quantitative generalization) Reward curves demonstrating the generalization of the aesthetic quality objective to prompts not seen during finetuning.The finetuning prompts are a list of 45 common animals, \"unseen animals\" is a list of 38 additional animals, and \"ordinary objects\" is a list of 50 objects (e.g.toaster, chair, coffee cup, etc.).\n\nDPOK includes one multi-prompt experiment with four prompts, but the authors conclude that it does not work as well as single-prompt training and do not provide qualitative results for that experiment.\nhttps://github.com/kvablack/ddpo-pytorch/issues/3#issuecomment-1634723127\nAPPENDIX A OVEROPTIMIZATION(L) The diffusion model eventually loses all recognizable semantic content and produces noise when optimizing for incompressibility.(R) When optimized for prompts of the form \"n animals\", the diffusion model exploits the VLM with a typographic attack(Goh et al., 2021), writing text that is interpreted as the specified number n instead of generating the correct number of animals.Section 6.1 highlights the optimization problem: given a reward function, how well can an RL algorithm maximize that reward?However, finetuning on a reward function, especially a learned one, has been observed to lead to reward overoptimization or exploitation(Gao et al., 2022)in which the model achieves high reward while moving too far away from the pretraining distribution to be useful.Our setting is no exception, and we provide two examples of reward exploitation in Figure7.When optimizing the incompressibility objective, the model eventually stops producing semantically meaningful content, degenerating into high-frequency noise.Similarly, we observed that LLaVA is susceptible to typographic attacks(Goh et al., 2021).When optimizing for alignment with respect to prompts of the form \"n animals\", DDPO exploited deficiencies in the VLM by instead generating text loosely resembling the specified number: for example, \"sixx ttutttas\" above a picture of eight turtles.There is currently no general-purpose method for preventing overoptimization.One common strategy is to add a KL-regularization term to the reward(Ouyang et al., 2022;Stiennon et al., 2020); we refer the reader to the concurrent work ofFan et al. (2023)for a study of KL-regularization in the context of finetuning text-to-image diffusion models.However,Gao et al. (2022)suggest that existing solutions, including KL-regularization, may be empirically equivalent to early stopping.As a result, in this work, we manually identified the last checkpoint before a model began to deteriorate for each method and used that as the reference for qualitative results.We highlight this problem as an important area for future work.APPENDIX B IMPLEMENTATION DETAILSFor all experiments, we use Stable Diffusion v1.4(Rombach et al., 2022)as the base model and finetune only the UNet weights while keeping the text encoder and autoencoder weights frozen.Note that our large-scale quantitative experiments contained a subtle bug 2 in the implementation of the aesthetic reward, causing the scale of the rewards to be slightly off and the resulting style of images to be different.The quantitative comparisons in Figure4and Figure9use this incorrect version.However, the same reward function is used for all methods and hence these plots faithfully represent the relative ability of each method to optimize the reward function.Although we did not re-run the quantitative comparisons, we expect the relative results to be the same.All of the qualitative results use the fixed reward function.B.1 DDPO IMPLEMENTATIONWe collect 256 samples per training iteration.For DDPO SF , we accumulate gradients across all 256 samples and perform one gradient update.For DDPO IS , we split the samples into 4 minibatches and perform 4 gradient updates.Gradients are always accumulated across all denoising timesteps for a single sample.For DDPO IS , we use the same clipped surrogate objective as in proximal policy optimization(Schulman et al., 2017), but find that we need to use a very small clip range compared to standard RL tasks.We use a clip range of 1e-4 for all experiments.B.2 RWR IMPLEMENTATIONWe compute the weights for a training iteration using the entire dataset of samples collected for that training iteration.For w RWR , the weights are computed using the softmax function.For w sparse , we use a percentile-based threshold, meaning C is dynamically selected such that the bottom p% of a given pool of samples are discarded and the rest are used for training.B.3 REWARD NORMALIZATIONIn practice, rewards are rarely used as-is, but instead are normalized to have zero mean and unit variance.Furthermore, this normalization can depend on the current state; in the policy gradient context, this is analogous to a value function baseline(Sutton et al., 1999), and in the RWR context, this is analogous to advantage-weighted regression(Peng et al., 2019).In our experiments, we normalize the rewards on a per-context basis.For DDPO, this is implemented as normalization by a running mean and standard deviation that is tracked for each prompt independently.For RWR, this is implemented by computing the softmax over rewards for each prompt independently.For RWR sparse , this is implemented by computing the percentile-based threshold C for each prompt independently.B.4 RESOURCE DETAILSRWR experiments were conducted on a v3-128 TPU pod, and took approximately 4 hours to reach 50k samples.DDPO experiments were conducted on a v4-64 TPU pod, and took approximately 4 hours to reach 50k samples.For the VLM-based reward function, LLaVA inference was conducted on a DGX machine with 8 80Gb A100 GPUs.\nPhilip Bachman and Doina Precup. Data generation as sequential decision making. Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, Pulkit Agrawal, arXiv:2211.15657Advances in Neural Information Processing Systems. 2022. 201528arXiv preprintIs conditional generative modeling all you need for decision-making?\n\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mccandlish, Chris Olah, Ben Mann, Jared Kaplan, arXiv:2204.058622022aarXiv preprint\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, ; Samuel, R Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom Brown, Jared Kaplan, arXiv:2212.08073Constitutional AI: Harmlessness from AI feedback. Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston2022barXiv preprint\n\nCheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, Shuran Song, arXiv:2303.04137Diffusion Policy: Visuomotor Policy Learning via Action Diffusion. 2023arXiv preprint\n\nDeep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, Neural Information Processing Systems. 2017\n\nImageNet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Conference on Computer Vision and Pattern Recognition. 2009\n\nDiffusion models beat GANs on image synthesis. Prafulla Dhariwal, Alexander Quinn, Nichol , Advances in Neural Information Processing Systems. 2021\n\nJascha Sohl-Dickstein, Arnaud Doucet, and Will Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, arXiv:2302.115522023arXiv preprint\n\nBenchmarking deep reinforcement learning for continuous control. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, Pieter Abbeel, International conference on machine learning. PMLR2016\n\nOptimizing ddpm sampling with shortcut fine-tuning. Ying Fan, Kangwook Lee, arXiv:2301.133622023arXiv preprint\n\nDpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, Kimin Lee, arXiv:2305.163812023arXiv preprint\n\nAn image is worth one word: Personalizing text-to-image generation using textual inversion. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, H Amit, Gal Bermano, Daniel Chechik, Cohen-Or, arXiv:2208.016182022arXiv preprint\n\nLeo Gao, John Schulman, Jacob Hilton, arXiv:2210.10760Scaling laws for reward model overoptimization. 2022arXiv preprint\n\nMultimodal neurons in artificial neural networks. Gabriel Goh, Nick Cammarata, \u2020 , Chelsea Voss, \u2020 , Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, Chris Olah, Distill. 2021\n\nIDQL: Implicit q-learning as an actor-critic method with diffusion policies. Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, Sergey Levine, arXiv:2304.105732023arXiv preprint\n\nClassifier-free diffusion guidance. Jonathan Ho, Tim Salimans, NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. 2021\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 2020\n\nImagen video: High definition video generation with diffusion models. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, P Diederik, Ben Kingma, Mohammad Poole, David J Norouzi, Tim Fleet, Salimans, arXiv:2210.023032022arXiv preprint\n\nPlanning with diffusion for flexible behavior synthesis. Michael Janner, Yilun Du, Joshua Tenenbaum, Sergey Levine, International Conference on Machine Learning. 2022\n\nApproximately optimal approximate reinforcement learning. Sham Kakade, John Langford, Proceedings of the Nineteenth International Conference on Machine Learning. the Nineteenth International Conference on Machine Learning2002\n\nVariational diffusion models. Tim Diederik P Kingma, Ben Salimans, Jonathan Poole, Ho, Neural Information Processing Systems. 2021\n\nTAMER: Training an Agent Manually via Evaluative Reinforcement. W , Bradley Knox, Peter Stone, International Conference on Development and Learning. 2008\n\nAligning text-to-image models using human feedback. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Shixiang Shane Gu, arXiv:2302.121922023arXiv preprint\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. 2023\n\nCompositional visual generation with composable diffusion models. Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, Joshua B Tenenbaum, arXiv:2206.017142022arXiv preprint\n\nShakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, Nat Mcaleese, arXiv:2203.11147The Journal of Machine Learning Research. 2112022. 2020arXiv preprintTeaching language models to support answers with verified quotes\n\nAccelerating online reinforcement learning with offline datasets. Ashvin Nair, Murtaza Dalal, Abhishek Gupta, Sergey Levine, arXiv:2006.093592020arXiv preprint\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, arXiv:2112.09332Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. 2021arXiv preprint\n\nReinforcement learning for bandit neural machine translation with simulated human feedback. Khanh Nguyen, Hal Daum\u00e9, Iii , Jordan Boyd-Graber, Empirical Methods in Natural Language Processing. 2017\n\nImproved denoising diffusion probabilistic models. Alexander Quinn, Nichol , Prafulla Dhariwal, International Conference on Machine Learning. 2021\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe, arXiv:2203.021552022arXiv preprint\n\nAdvantage-weighted regression: Simple and scalable off-policy reinforcement learning. Xue Bin Peng, Aviral Kumar, Grace Zhang, Sergey Levine, CoRR, abs/1910.001772019\n\nReinforcement learning by reward-weighted regression for operational space control. Jan Peters, Stefan Schaal, International Conference on Machine learning. 2007\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, arXiv:2103.000202021arXiv preprint\n\nZero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Scott Gray, Gabriel Goh, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, arXiv:2102.120922021arXiv preprint\n\nHighresolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer, IEEE Conference on Computer Vision and Pattern Recognition. 2022\n\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman, arXiv:2208.122422022arXiv preprint\n\nPhotorealistic text-to-image diffusion models with deep language understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar, Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi, arXiv:2205.114872022arXiv preprint\n\nArne Schneuing, Yuanqi Du, Arian Jamasb, Charles Harris, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Li\u00f3, Carla Gomes, Michael Bronstein, Max Welling, Bruno Correia, arXiv:2210.02303Structure-based drug design with equivariant diffusion models. 2022arXiv preprint\n\nLaion aesthetics. Chrisoph Schuhmann, Aug 2022\n\nTrust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International Conference on Machine Learning. 2015\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint\n\nMake-a-video: Text-to-video generation without text-video data. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, arXiv:2209.147922022arXiv preprint\n\nDeep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, International Conference on Learning Representations. 2015. 2021Jiaming Song, Chenlin Meng, and Stefano Ermon\n\nLearning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Neural Information Processing Systems. 2020\n\nPolicy gradient methods for reinforcement learning with function approximation. David Richard S Sutton, Satinder Mcallester, Yishay Singh, Mansour, Advances in Neural Information Processing Systems. S Solla, T Leen, K M\u00fcller, MIT Press199912\n\nDiffusion policies as an expressive policy class for offline reinforcement learning. Zhendong Wang, Jonathan J Hunt, Mingyuan Zhou, arXiv:2208.061932022arXiv preprint\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. Williams Ronald, 1992Reinforcement learning\n\nCrystal diffusion variational autoencoder for periodic material generation. Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, Tommi S Jaakkola, International Conference on Learning Representations. 2021\n\nImagereward: Learning and evaluating human preferences for text-to-image generation. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, Yuxiao Dong, arXiv:2304.059772023arXiv preprint\n\nGeoDiff: A geometric diffusion model for molecular conformation generation. Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, Jian Tang, International Conference on Learning Representations. 2021\n\nAdding conditional control to text-to-image diffusion models. Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis, arXiv:2210.06978International Conference on Learning Representations. Tianyi Zhang, Varsha Kishore, * , Felix Wu, Kilian Q Weinberger, Yoav Artzi, 2022. 2023. 2020arXiv preprintBERTScore: Evaluating text generation with BERT\n\n3d shape generation and completion through point-voxel diffusion. Linqi Zhou, Yilun Du, Jiajun Wu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021\n\nM Daniel, Nisan Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint\n", "annotations": {"author": "[{\"end\":136,\"start\":67},{\"end\":207,\"start\":137},{\"end\":273,\"start\":208},{\"end\":347,\"start\":274},{\"end\":424,\"start\":348}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":73},{\"end\":151,\"start\":145},{\"end\":216,\"start\":214},{\"end\":288,\"start\":279},{\"end\":361,\"start\":355}]", "author_first_name": "[{\"end\":72,\"start\":67},{\"end\":144,\"start\":137},{\"end\":213,\"start\":208},{\"end\":278,\"start\":274},{\"end\":354,\"start\":348}]", "author_affiliation": "[{\"end\":135,\"start\":102},{\"end\":206,\"start\":173},{\"end\":272,\"start\":234},{\"end\":346,\"start\":313},{\"end\":423,\"start\":390}]", "title": "[{\"end\":54,\"start\":1},{\"end\":478,\"start\":425}]", "venue": null, "abstract": "[{\"end\":1692,\"start\":547}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b43\"},\"end\":1768,\"start\":1739},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2053,\"start\":2032},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2074,\"start\":2053},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2090,\"start\":2074},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2134,\"start\":2117},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2151,\"start\":2134},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2174,\"start\":2151},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2220,\"start\":2199},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2238,\"start\":2220},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2266,\"start\":2238},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4026,\"start\":4005},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4231,\"start\":4212},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5049,\"start\":5020},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5065,\"start\":5049},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5175,\"start\":5154},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5196,\"start\":5175},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5222,\"start\":5205},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5242,\"start\":5222},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":5273,\"start\":5254},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5291,\"start\":5273},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5339,\"start\":5318},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5357,\"start\":5339},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5374,\"start\":5357},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5574,\"start\":5557},{\"end\":5661,\"start\":5636},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5681,\"start\":5661},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5993,\"start\":5972},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6014,\"start\":5993},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6240,\"start\":6221},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6304,\"start\":6286},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6340,\"start\":6323},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6357,\"start\":6340},{\"end\":6425,\"start\":6401},{\"end\":6501,\"start\":6476},{\"end\":6543,\"start\":6522},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6739,\"start\":6715},{\"end\":6775,\"start\":6739},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6818,\"start\":6797},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6860,\"start\":6839},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6917,\"start\":6896},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6956,\"start\":6933},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6977,\"start\":6956},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7022,\"start\":7001},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7077,\"start\":7058},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7105,\"start\":7088},{\"end\":7591,\"start\":7568},{\"end\":7788,\"start\":7772},{\"end\":8095,\"start\":8077},{\"end\":8123,\"start\":8107},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8145,\"start\":8128},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8270,\"start\":8253},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8351,\"start\":8334},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9060,\"start\":9031},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9076,\"start\":9060},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9738,\"start\":9721},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10081,\"start\":10064},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10099,\"start\":10081},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12436,\"start\":12413},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13458,\"start\":13439},{\"end\":14134,\"start\":14118},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":16015,\"start\":15999},{\"end\":16036,\"start\":16015},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16497,\"start\":16472},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16851,\"start\":16828},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17003,\"start\":16980},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18647,\"start\":18630},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18792,\"start\":18770},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19040,\"start\":19019},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19064,\"start\":19040},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":19085,\"start\":19064},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19526,\"start\":19507},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19630,\"start\":19612},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21596,\"start\":21574},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21764,\"start\":21745},{\"end\":24337,\"start\":24317},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25883,\"start\":25862},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28961,\"start\":28944},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30688,\"start\":30667},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30763,\"start\":30744},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30853,\"start\":30834},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31041,\"start\":31023}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32930,\"start\":32546},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33250,\"start\":32931},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33612,\"start\":33251},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34189,\"start\":33613},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34560,\"start\":34190},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34944,\"start\":34561}]", "paragraph": "[{\"end\":2616,\"start\":1708},{\"end\":3550,\"start\":2618},{\"end\":4474,\"start\":3552},{\"end\":4945,\"start\":4476},{\"end\":5874,\"start\":4962},{\"end\":6553,\"start\":5876},{\"end\":7472,\"start\":6555},{\"end\":8825,\"start\":7474},{\"end\":8941,\"start\":8843},{\"end\":9435,\"start\":8962},{\"end\":9739,\"start\":9530},{\"end\":10183,\"start\":9741},{\"end\":10251,\"start\":10248},{\"end\":10822,\"start\":10308},{\"end\":11124,\"start\":10824},{\"end\":11431,\"start\":11225},{\"end\":11739,\"start\":11453},{\"end\":11841,\"start\":11790},{\"end\":12437,\"start\":11872},{\"end\":12518,\"start\":12439},{\"end\":12699,\"start\":12559},{\"end\":12938,\"start\":12740},{\"end\":13025,\"start\":12940},{\"end\":13284,\"start\":13101},{\"end\":13737,\"start\":13286},{\"end\":14555,\"start\":13781},{\"end\":14649,\"start\":14557},{\"end\":15143,\"start\":14847},{\"end\":15555,\"start\":15145},{\"end\":15840,\"start\":15557},{\"end\":16037,\"start\":15842},{\"end\":16208,\"start\":16111},{\"end\":16498,\"start\":16210},{\"end\":17004,\"start\":16619},{\"end\":17593,\"start\":17053},{\"end\":18449,\"start\":17635},{\"end\":19086,\"start\":18471},{\"end\":19597,\"start\":19145},{\"end\":20036,\"start\":19599},{\"end\":20595,\"start\":20038},{\"end\":20880,\"start\":20623},{\"end\":21296,\"start\":20882},{\"end\":21864,\"start\":21322},{\"end\":22262,\"start\":21866},{\"end\":22356,\"start\":22264},{\"end\":23428,\"start\":22358},{\"end\":24004,\"start\":23459},{\"end\":24702,\"start\":24006},{\"end\":25167,\"start\":24704},{\"end\":25278,\"start\":25182},{\"end\":25618,\"start\":25292},{\"end\":26686,\"start\":25637},{\"end\":28255,\"start\":26717},{\"end\":28811,\"start\":28257},{\"end\":29045,\"start\":28874},{\"end\":29642,\"start\":29047},{\"end\":29865,\"start\":29644},{\"end\":30268,\"start\":29873},{\"end\":30854,\"start\":30289},{\"end\":31492,\"start\":30856},{\"end\":32078,\"start\":31547},{\"end\":32545,\"start\":32106},{\"end\":32929,\"start\":32549},{\"end\":33249,\"start\":32945},{\"end\":33611,\"start\":33265},{\"end\":34188,\"start\":33627},{\"end\":34559,\"start\":34204},{\"end\":34943,\"start\":34590}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9529,\"start\":9436},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10247,\"start\":10184},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11170,\"start\":11125},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11789,\"start\":11740},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12558,\"start\":12519},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12739,\"start\":12700},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13100,\"start\":13026},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14846,\"start\":14650},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16110,\"start\":16038},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16618,\"start\":16499},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28873,\"start\":28812}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1706,\"start\":1694},{\"attributes\":{\"n\":\"2\"},\"end\":4960,\"start\":4948},{\"attributes\":{\"n\":\"3\"},\"end\":8841,\"start\":8828},{\"attributes\":{\"n\":\"3.1\"},\"end\":8960,\"start\":8944},{\"attributes\":{\"n\":\"3.2\"},\"end\":10306,\"start\":10254},{\"attributes\":{\"n\":\"4\"},\"end\":11223,\"start\":11172},{\"attributes\":{\"n\":\"4.1\"},\"end\":11451,\"start\":11434},{\"attributes\":{\"n\":\"4.2\"},\"end\":11870,\"start\":11844},{\"attributes\":{\"n\":\"4.3\"},\"end\":13779,\"start\":13740},{\"attributes\":{\"n\":\"5\"},\"end\":17051,\"start\":17007},{\"attributes\":{\"n\":\"5.1\"},\"end\":17633,\"start\":17596},{\"attributes\":{\"n\":\"5.2\"},\"end\":18469,\"start\":18452},{\"attributes\":{\"n\":\"5.3\"},\"end\":19143,\"start\":19089},{\"attributes\":{\"n\":\"6\"},\"end\":20621,\"start\":20598},{\"attributes\":{\"n\":\"6.1\"},\"end\":21320,\"start\":21299},{\"attributes\":{\"n\":\"6.2\"},\"end\":23457,\"start\":23431},{\"end\":25180,\"start\":25170},{\"end\":25290,\"start\":25281},{\"attributes\":{\"n\":\"6.3\"},\"end\":25635,\"start\":25621},{\"attributes\":{\"n\":\"7\"},\"end\":26715,\"start\":26689},{\"end\":29871,\"start\":29868},{\"end\":30287,\"start\":30271},{\"end\":31545,\"start\":31495},{\"end\":32104,\"start\":32081},{\"end\":32942,\"start\":32932},{\"end\":33262,\"start\":33252},{\"end\":33624,\"start\":33614},{\"end\":34201,\"start\":34191},{\"end\":34584,\"start\":34562}]", "table": null, "figure_caption": "[{\"end\":32930,\"start\":32548},{\"end\":33250,\"start\":32944},{\"end\":33612,\"start\":33264},{\"end\":34189,\"start\":33626},{\"end\":34560,\"start\":34203},{\"end\":34944,\"start\":34589}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20049,\"start\":20048},{\"end\":21900,\"start\":21899},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22642,\"start\":22641},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24059,\"start\":24058},{\"end\":25300,\"start\":25299},{\"end\":26172,\"start\":26171},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29688,\"start\":29687},{\"end\":29881,\"start\":29880},{\"end\":31248,\"start\":31247},{\"end\":31740,\"start\":31738},{\"end\":32115,\"start\":32113},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32180,\"start\":32178},{\"end\":32294,\"start\":32292},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32424,\"start\":32422}]", "bib_author_first_name": "[{\"end\":40376,\"start\":40370},{\"end\":40388,\"start\":40383},{\"end\":40397,\"start\":40393},{\"end\":40411,\"start\":40405},{\"end\":40428,\"start\":40423},{\"end\":40445,\"start\":40439},{\"end\":40715,\"start\":40709},{\"end\":40725,\"start\":40721},{\"end\":40738,\"start\":40733},{\"end\":40754,\"start\":40748},{\"end\":40767,\"start\":40763},{\"end\":40778,\"start\":40774},{\"end\":40793,\"start\":40789},{\"end\":40810,\"start\":40801},{\"end\":40821,\"start\":40817},{\"end\":40834,\"start\":40831},{\"end\":40853,\"start\":40845},{\"end\":40868,\"start\":40862},{\"end\":40886,\"start\":40879},{\"end\":40899,\"start\":40896},{\"end\":40914,\"start\":40909},{\"end\":40931,\"start\":40925},{\"end\":40943,\"start\":40940},{\"end\":40965,\"start\":40960},{\"end\":40984,\"start\":40977},{\"end\":40996,\"start\":40991},{\"end\":41013,\"start\":41007},{\"end\":41027,\"start\":41022},{\"end\":41040,\"start\":41036},{\"end\":41057,\"start\":41048},{\"end\":41071,\"start\":41066},{\"end\":41083,\"start\":41080},{\"end\":41095,\"start\":41091},{\"end\":41106,\"start\":41103},{\"end\":41124,\"start\":41119},{\"end\":41134,\"start\":41131},{\"end\":41146,\"start\":41141},{\"end\":41198,\"start\":41192},{\"end\":41210,\"start\":41204},{\"end\":41229,\"start\":41221},{\"end\":41243,\"start\":41237},{\"end\":41259,\"start\":41252},{\"end\":41273,\"start\":41269},{\"end\":41285,\"start\":41281},{\"end\":41296,\"start\":41292},{\"end\":41311,\"start\":41305},{\"end\":41331,\"start\":41324},{\"end\":41347,\"start\":41342},{\"end\":41363,\"start\":41354},{\"end\":41383,\"start\":41372},{\"end\":41395,\"start\":41390},{\"end\":41411,\"start\":41407},{\"end\":41423,\"start\":41419},{\"end\":41439,\"start\":41433},{\"end\":41447,\"start\":41444},{\"end\":41467,\"start\":41462},{\"end\":41480,\"start\":41475},{\"end\":41492,\"start\":41487},{\"end\":41509,\"start\":41502},{\"end\":41524,\"start\":41518},{\"end\":41538,\"start\":41533},{\"end\":41554,\"start\":41548},{\"end\":41571,\"start\":41566},{\"end\":41587,\"start\":41580},{\"end\":41604,\"start\":41598},{\"end\":41621,\"start\":41613},{\"end\":41637,\"start\":41632},{\"end\":41648,\"start\":41647},{\"end\":41658,\"start\":41657},{\"end\":41670,\"start\":41667},{\"end\":41690,\"start\":41687},{\"end\":41702,\"start\":41697},{\"end\":41719,\"start\":41711},{\"end\":41731,\"start\":41728},{\"end\":41747,\"start\":41744},{\"end\":41760,\"start\":41755},{\"end\":41841,\"start\":41835},{\"end\":41855,\"start\":41850},{\"end\":41858,\"start\":41856},{\"end\":41875,\"start\":41866},{\"end\":41888,\"start\":41882},{\"end\":41904,\"start\":41897},{\"end\":41924,\"start\":41921},{\"end\":41937,\"start\":41934},{\"end\":41955,\"start\":41948},{\"end\":42059,\"start\":42054},{\"end\":42071,\"start\":42065},{\"end\":42083,\"start\":42078},{\"end\":42095,\"start\":42088},{\"end\":42104,\"start\":42100},{\"end\":42124,\"start\":42116},{\"end\":42142,\"start\":42136},{\"end\":42307,\"start\":42304},{\"end\":42330,\"start\":42327},{\"end\":42344,\"start\":42338},{\"end\":42357,\"start\":42352},{\"end\":42371,\"start\":42366},{\"end\":42487,\"start\":42484},{\"end\":42497,\"start\":42494},{\"end\":42511,\"start\":42504},{\"end\":42526,\"start\":42520},{\"end\":42534,\"start\":42531},{\"end\":42541,\"start\":42539},{\"end\":42667,\"start\":42659},{\"end\":42687,\"start\":42678},{\"end\":42701,\"start\":42695},{\"end\":42918,\"start\":42913},{\"end\":42928,\"start\":42923},{\"end\":42942,\"start\":42937},{\"end\":42958,\"start\":42952},{\"end\":42960,\"start\":42959},{\"end\":42978,\"start\":42972},{\"end\":42992,\"start\":42989},{\"end\":43105,\"start\":43102},{\"end\":43114,\"start\":43112},{\"end\":43125,\"start\":43121},{\"end\":43141,\"start\":43137},{\"end\":43158,\"start\":43152},{\"end\":43279,\"start\":43275},{\"end\":43293,\"start\":43285},{\"end\":43416,\"start\":43412},{\"end\":43428,\"start\":43422},{\"end\":43444,\"start\":43438},{\"end\":43452,\"start\":43449},{\"end\":43467,\"start\":43458},{\"end\":43478,\"start\":43473},{\"end\":43496,\"start\":43490},{\"end\":43513,\"start\":43505},{\"end\":43535,\"start\":43527},{\"end\":43546,\"start\":43541},{\"end\":43685,\"start\":43680},{\"end\":43696,\"start\":43691},{\"end\":43710,\"start\":43705},{\"end\":43721,\"start\":43719},{\"end\":43734,\"start\":43733},{\"end\":43744,\"start\":43741},{\"end\":43760,\"start\":43754},{\"end\":43819,\"start\":43816},{\"end\":43829,\"start\":43825},{\"end\":43845,\"start\":43840},{\"end\":43995,\"start\":43988},{\"end\":44005,\"start\":44001},{\"end\":44018,\"start\":44017},{\"end\":44028,\"start\":44021},{\"end\":44036,\"start\":44035},{\"end\":44043,\"start\":44039},{\"end\":44059,\"start\":44052},{\"end\":44074,\"start\":44068},{\"end\":44089,\"start\":44085},{\"end\":44104,\"start\":44099},{\"end\":44211,\"start\":44203},{\"end\":44232,\"start\":44228},{\"end\":44251,\"start\":44244},{\"end\":44265,\"start\":44260},{\"end\":44287,\"start\":44281},{\"end\":44376,\"start\":44368},{\"end\":44384,\"start\":44381},{\"end\":44528,\"start\":44520},{\"end\":44537,\"start\":44533},{\"end\":44550,\"start\":44544},{\"end\":44694,\"start\":44686},{\"end\":44706,\"start\":44699},{\"end\":44720,\"start\":44713},{\"end\":44733,\"start\":44730},{\"end\":44746,\"start\":44741},{\"end\":44758,\"start\":44752},{\"end\":44771,\"start\":44770},{\"end\":44785,\"start\":44782},{\"end\":44802,\"start\":44794},{\"end\":44815,\"start\":44810},{\"end\":44817,\"start\":44816},{\"end\":44830,\"start\":44827},{\"end\":44948,\"start\":44941},{\"end\":44962,\"start\":44957},{\"end\":44973,\"start\":44967},{\"end\":44991,\"start\":44985},{\"end\":45114,\"start\":45110},{\"end\":45127,\"start\":45123},{\"end\":45312,\"start\":45309},{\"end\":45335,\"start\":45332},{\"end\":45354,\"start\":45346},{\"end\":45476,\"start\":45475},{\"end\":45486,\"start\":45479},{\"end\":45498,\"start\":45493},{\"end\":45623,\"start\":45618},{\"end\":45632,\"start\":45629},{\"end\":45647,\"start\":45638},{\"end\":45659,\"start\":45653},{\"end\":45675,\"start\":45669},{\"end\":45685,\"start\":45680},{\"end\":45703,\"start\":45697},{\"end\":45720,\"start\":45712},{\"end\":45742,\"start\":45734},{\"end\":45748,\"start\":45743},{\"end\":45796,\"start\":45789},{\"end\":45810,\"start\":45802},{\"end\":45823,\"start\":45815},{\"end\":45832,\"start\":45828},{\"end\":45836,\"start\":45833},{\"end\":45944,\"start\":45941},{\"end\":45956,\"start\":45950},{\"end\":45966,\"start\":45961},{\"end\":45978,\"start\":45971},{\"end\":45995,\"start\":45989},{\"end\":45997,\"start\":45996},{\"end\":46169,\"start\":46164},{\"end\":46182,\"start\":46178},{\"end\":46200,\"start\":46192},{\"end\":46214,\"start\":46210},{\"end\":46233,\"start\":46226},{\"end\":46246,\"start\":46240},{\"end\":46260,\"start\":46257},{\"end\":46277,\"start\":46269},{\"end\":46289,\"start\":46285},{\"end\":46319,\"start\":46311},{\"end\":46331,\"start\":46328},{\"end\":46565,\"start\":46559},{\"end\":46579,\"start\":46572},{\"end\":46595,\"start\":46587},{\"end\":46609,\"start\":46603},{\"end\":46663,\"start\":46654},{\"end\":46677,\"start\":46672},{\"end\":46692,\"start\":46686},{\"end\":46705,\"start\":46701},{\"end\":46714,\"start\":46710},{\"end\":46732,\"start\":46723},{\"end\":46749,\"start\":46738},{\"end\":46765,\"start\":46757},{\"end\":46778,\"start\":46772},{\"end\":46796,\"start\":46789},{\"end\":46809,\"start\":46807},{\"end\":46821,\"start\":46817},{\"end\":46833,\"start\":46829},{\"end\":46852,\"start\":46844},{\"end\":46867,\"start\":46862},{\"end\":47125,\"start\":47120},{\"end\":47137,\"start\":47134},{\"end\":47148,\"start\":47145},{\"end\":47157,\"start\":47151},{\"end\":47287,\"start\":47278},{\"end\":47301,\"start\":47295},{\"end\":47312,\"start\":47304},{\"end\":47448,\"start\":47444},{\"end\":47461,\"start\":47457},{\"end\":47468,\"start\":47466},{\"end\":47481,\"start\":47476},{\"end\":47498,\"start\":47491},{\"end\":47500,\"start\":47499},{\"end\":47519,\"start\":47513},{\"end\":47534,\"start\":47529},{\"end\":47550,\"start\":47542},{\"end\":47568,\"start\":47560},{\"end\":47580,\"start\":47576},{\"end\":47590,\"start\":47586},{\"end\":47606,\"start\":47601},{\"end\":47621,\"start\":47615},{\"end\":47634,\"start\":47630},{\"end\":47649,\"start\":47643},{\"end\":47664,\"start\":47658},{\"end\":47678,\"start\":47673},{\"end\":47693,\"start\":47689},{\"end\":47709,\"start\":47706},{\"end\":47721,\"start\":47717},{\"end\":47857,\"start\":47850},{\"end\":47870,\"start\":47864},{\"end\":47883,\"start\":47878},{\"end\":47897,\"start\":47891},{\"end\":48019,\"start\":48016},{\"end\":48034,\"start\":48028},{\"end\":48170,\"start\":48166},{\"end\":48184,\"start\":48180},{\"end\":48189,\"start\":48185},{\"end\":48200,\"start\":48195},{\"end\":48216,\"start\":48210},{\"end\":48232,\"start\":48225},{\"end\":48246,\"start\":48238},{\"end\":48262,\"start\":48256},{\"end\":48277,\"start\":48271},{\"end\":48292,\"start\":48286},{\"end\":48306,\"start\":48302},{\"end\":48322,\"start\":48314},{\"end\":48336,\"start\":48332},{\"end\":48426,\"start\":48420},{\"end\":48442,\"start\":48435},{\"end\":48456,\"start\":48451},{\"end\":48470,\"start\":48463},{\"end\":48483,\"start\":48476},{\"end\":48494,\"start\":48490},{\"end\":48508,\"start\":48504},{\"end\":48519,\"start\":48515},{\"end\":48633,\"start\":48628},{\"end\":48650,\"start\":48643},{\"end\":48669,\"start\":48662},{\"end\":48685,\"start\":48678},{\"end\":48698,\"start\":48693},{\"end\":48866,\"start\":48858},{\"end\":48881,\"start\":48873},{\"end\":48891,\"start\":48886},{\"end\":48905,\"start\":48901},{\"end\":48921,\"start\":48914},{\"end\":48938,\"start\":48934},{\"end\":49071,\"start\":49064},{\"end\":49088,\"start\":49081},{\"end\":49102,\"start\":49095},{\"end\":49115,\"start\":49111},{\"end\":49123,\"start\":49120},{\"end\":49136,\"start\":49131},{\"end\":49150,\"start\":49145},{\"end\":49164,\"start\":49159},{\"end\":49183,\"start\":49178},{\"end\":49199,\"start\":49198},{\"end\":49204,\"start\":49200},{\"end\":49219,\"start\":49214},{\"end\":49227,\"start\":49220},{\"end\":49238,\"start\":49235},{\"end\":49257,\"start\":49249},{\"end\":49267,\"start\":49262},{\"end\":49269,\"start\":49268},{\"end\":49285,\"start\":49277},{\"end\":49335,\"start\":49331},{\"end\":49353,\"start\":49347},{\"end\":49363,\"start\":49358},{\"end\":49379,\"start\":49372},{\"end\":49392,\"start\":49388},{\"end\":49408,\"start\":49402},{\"end\":49416,\"start\":49413},{\"end\":49433,\"start\":49427},{\"end\":49444,\"start\":49439},{\"end\":49459,\"start\":49452},{\"end\":49474,\"start\":49471},{\"end\":49489,\"start\":49484},{\"end\":49624,\"start\":49616},{\"end\":49684,\"start\":49680},{\"end\":49701,\"start\":49695},{\"end\":49716,\"start\":49710},{\"end\":49732,\"start\":49725},{\"end\":49748,\"start\":49741},{\"end\":49813,\"start\":49809},{\"end\":49829,\"start\":49824},{\"end\":49846,\"start\":49838},{\"end\":49861,\"start\":49857},{\"end\":49875,\"start\":49871},{\"end\":50030,\"start\":50025},{\"end\":50043,\"start\":50039},{\"end\":50058,\"start\":50052},{\"end\":50068,\"start\":50066},{\"end\":50077,\"start\":50074},{\"end\":50090,\"start\":50082},{\"end\":50104,\"start\":50098},{\"end\":50114,\"start\":50109},{\"end\":50125,\"start\":50121},{\"end\":50138,\"start\":50134},{\"end\":50252,\"start\":50246},{\"end\":50273,\"start\":50269},{\"end\":50285,\"start\":50281},{\"end\":50308,\"start\":50303},{\"end\":50477,\"start\":50472},{\"end\":50492,\"start\":50488},{\"end\":50508,\"start\":50501},{\"end\":50519,\"start\":50513},{\"end\":50533,\"start\":50529},{\"end\":50547,\"start\":50540},{\"end\":50558,\"start\":50554},{\"end\":50573,\"start\":50568},{\"end\":50586,\"start\":50582},{\"end\":50588,\"start\":50587},{\"end\":50731,\"start\":50726},{\"end\":50758,\"start\":50750},{\"end\":50777,\"start\":50771},{\"end\":50846,\"start\":50845},{\"end\":50855,\"start\":50854},{\"end\":50863,\"start\":50862},{\"end\":50982,\"start\":50974},{\"end\":50997,\"start\":50989},{\"end\":50999,\"start\":50998},{\"end\":51014,\"start\":51006},{\"end\":51156,\"start\":51148},{\"end\":51273,\"start\":51269},{\"end\":51284,\"start\":51279},{\"end\":51303,\"start\":51289},{\"end\":51317,\"start\":51311},{\"end\":51333,\"start\":51328},{\"end\":51335,\"start\":51334},{\"end\":51499,\"start\":51491},{\"end\":51508,\"start\":51504},{\"end\":51520,\"start\":51514},{\"end\":51531,\"start\":51525},{\"end\":51544,\"start\":51538},{\"end\":51553,\"start\":51549},{\"end\":51563,\"start\":51560},{\"end\":51576,\"start\":51570},{\"end\":51701,\"start\":51695},{\"end\":51712,\"start\":51706},{\"end\":51721,\"start\":51717},{\"end\":51734,\"start\":51728},{\"end\":51747,\"start\":51740},{\"end\":51759,\"start\":51755},{\"end\":51895,\"start\":51888},{\"end\":51907,\"start\":51902},{\"end\":51923,\"start\":51916},{\"end\":51937,\"start\":51934},{\"end\":51948,\"start\":51946},{\"end\":51962,\"start\":51957},{\"end\":51978,\"start\":51971},{\"end\":52062,\"start\":52056},{\"end\":52076,\"start\":52070},{\"end\":52087,\"start\":52086},{\"end\":52095,\"start\":52090},{\"end\":52106,\"start\":52100},{\"end\":52108,\"start\":52107},{\"end\":52125,\"start\":52121},{\"end\":52283,\"start\":52278},{\"end\":52295,\"start\":52290},{\"end\":52306,\"start\":52300},{\"end\":52447,\"start\":52446},{\"end\":52461,\"start\":52456},{\"end\":52478,\"start\":52471},{\"end\":52492,\"start\":52489},{\"end\":52494,\"start\":52493},{\"end\":52503,\"start\":52499},{\"end\":52516,\"start\":52511},{\"end\":52530,\"start\":52526},{\"end\":52547,\"start\":52539}]", "bib_author_last_name": "[{\"end\":40381,\"start\":40377},{\"end\":40391,\"start\":40389},{\"end\":40403,\"start\":40398},{\"end\":40421,\"start\":40412},{\"end\":40437,\"start\":40429},{\"end\":40453,\"start\":40446},{\"end\":40719,\"start\":40716},{\"end\":40731,\"start\":40726},{\"end\":40746,\"start\":40739},{\"end\":40761,\"start\":40755},{\"end\":40772,\"start\":40768},{\"end\":40787,\"start\":40779},{\"end\":40799,\"start\":40794},{\"end\":40815,\"start\":40811},{\"end\":40829,\"start\":40822},{\"end\":40843,\"start\":40835},{\"end\":40860,\"start\":40854},{\"end\":40877,\"start\":40869},{\"end\":40894,\"start\":40887},{\"end\":40907,\"start\":40900},{\"end\":40923,\"start\":40915},{\"end\":40938,\"start\":40932},{\"end\":40958,\"start\":40944},{\"end\":40975,\"start\":40966},{\"end\":40989,\"start\":40985},{\"end\":41005,\"start\":40997},{\"end\":41020,\"start\":41014},{\"end\":41034,\"start\":41028},{\"end\":41046,\"start\":41041},{\"end\":41064,\"start\":41058},{\"end\":41078,\"start\":41072},{\"end\":41089,\"start\":41084},{\"end\":41101,\"start\":41096},{\"end\":41117,\"start\":41107},{\"end\":41129,\"start\":41125},{\"end\":41139,\"start\":41135},{\"end\":41153,\"start\":41147},{\"end\":41202,\"start\":41199},{\"end\":41219,\"start\":41211},{\"end\":41235,\"start\":41230},{\"end\":41250,\"start\":41244},{\"end\":41267,\"start\":41260},{\"end\":41279,\"start\":41274},{\"end\":41290,\"start\":41286},{\"end\":41303,\"start\":41297},{\"end\":41322,\"start\":41312},{\"end\":41340,\"start\":41332},{\"end\":41352,\"start\":41348},{\"end\":41370,\"start\":41364},{\"end\":41388,\"start\":41384},{\"end\":41405,\"start\":41396},{\"end\":41417,\"start\":41412},{\"end\":41431,\"start\":41424},{\"end\":41442,\"start\":41440},{\"end\":41460,\"start\":41448},{\"end\":41473,\"start\":41468},{\"end\":41485,\"start\":41481},{\"end\":41500,\"start\":41493},{\"end\":41516,\"start\":41510},{\"end\":41531,\"start\":41525},{\"end\":41546,\"start\":41539},{\"end\":41564,\"start\":41555},{\"end\":41578,\"start\":41572},{\"end\":41596,\"start\":41588},{\"end\":41611,\"start\":41605},{\"end\":41630,\"start\":41622},{\"end\":41645,\"start\":41638},{\"end\":41655,\"start\":41649},{\"end\":41665,\"start\":41659},{\"end\":41685,\"start\":41671},{\"end\":41695,\"start\":41691},{\"end\":41709,\"start\":41703},{\"end\":41726,\"start\":41720},{\"end\":41742,\"start\":41732},{\"end\":41753,\"start\":41748},{\"end\":41767,\"start\":41761},{\"end\":41848,\"start\":41842},{\"end\":41864,\"start\":41859},{\"end\":41880,\"start\":41876},{\"end\":41895,\"start\":41889},{\"end\":41919,\"start\":41905},{\"end\":41932,\"start\":41925},{\"end\":41946,\"start\":41938},{\"end\":41960,\"start\":41956},{\"end\":42063,\"start\":42060},{\"end\":42076,\"start\":42072},{\"end\":42086,\"start\":42084},{\"end\":42098,\"start\":42096},{\"end\":42114,\"start\":42105},{\"end\":42134,\"start\":42125},{\"end\":42147,\"start\":42143},{\"end\":42325,\"start\":42308},{\"end\":42336,\"start\":42331},{\"end\":42350,\"start\":42345},{\"end\":42364,\"start\":42358},{\"end\":42376,\"start\":42372},{\"end\":42384,\"start\":42378},{\"end\":42492,\"start\":42488},{\"end\":42502,\"start\":42498},{\"end\":42518,\"start\":42512},{\"end\":42529,\"start\":42527},{\"end\":42537,\"start\":42535},{\"end\":42549,\"start\":42542},{\"end\":42676,\"start\":42668},{\"end\":42693,\"start\":42688},{\"end\":42921,\"start\":42919},{\"end\":42935,\"start\":42929},{\"end\":42950,\"start\":42943},{\"end\":42970,\"start\":42961},{\"end\":42987,\"start\":42979},{\"end\":42999,\"start\":42993},{\"end\":43110,\"start\":43106},{\"end\":43119,\"start\":43115},{\"end\":43135,\"start\":43126},{\"end\":43150,\"start\":43142},{\"end\":43165,\"start\":43159},{\"end\":43283,\"start\":43280},{\"end\":43297,\"start\":43294},{\"end\":43420,\"start\":43417},{\"end\":43436,\"start\":43429},{\"end\":43447,\"start\":43445},{\"end\":43456,\"start\":43453},{\"end\":43471,\"start\":43468},{\"end\":43488,\"start\":43479},{\"end\":43503,\"start\":43497},{\"end\":43525,\"start\":43514},{\"end\":43539,\"start\":43536},{\"end\":43550,\"start\":43547},{\"end\":43689,\"start\":43686},{\"end\":43703,\"start\":43697},{\"end\":43717,\"start\":43711},{\"end\":43731,\"start\":43722},{\"end\":43739,\"start\":43735},{\"end\":43752,\"start\":43745},{\"end\":43768,\"start\":43761},{\"end\":43778,\"start\":43770},{\"end\":43823,\"start\":43820},{\"end\":43838,\"start\":43830},{\"end\":43852,\"start\":43846},{\"end\":43999,\"start\":43996},{\"end\":44015,\"start\":44006},{\"end\":44033,\"start\":44029},{\"end\":44050,\"start\":44044},{\"end\":44066,\"start\":44060},{\"end\":44083,\"start\":44075},{\"end\":44097,\"start\":44090},{\"end\":44109,\"start\":44105},{\"end\":44226,\"start\":44212},{\"end\":44242,\"start\":44233},{\"end\":44258,\"start\":44252},{\"end\":44279,\"start\":44266},{\"end\":44294,\"start\":44288},{\"end\":44379,\"start\":44377},{\"end\":44393,\"start\":44385},{\"end\":44531,\"start\":44529},{\"end\":44542,\"start\":44538},{\"end\":44557,\"start\":44551},{\"end\":44697,\"start\":44695},{\"end\":44711,\"start\":44707},{\"end\":44728,\"start\":44721},{\"end\":44739,\"start\":44734},{\"end\":44750,\"start\":44747},{\"end\":44768,\"start\":44759},{\"end\":44780,\"start\":44772},{\"end\":44792,\"start\":44786},{\"end\":44808,\"start\":44803},{\"end\":44825,\"start\":44818},{\"end\":44836,\"start\":44831},{\"end\":44846,\"start\":44838},{\"end\":44955,\"start\":44949},{\"end\":44965,\"start\":44963},{\"end\":44983,\"start\":44974},{\"end\":44998,\"start\":44992},{\"end\":45121,\"start\":45115},{\"end\":45136,\"start\":45128},{\"end\":45330,\"start\":45313},{\"end\":45344,\"start\":45336},{\"end\":45360,\"start\":45355},{\"end\":45364,\"start\":45362},{\"end\":45491,\"start\":45487},{\"end\":45504,\"start\":45499},{\"end\":45627,\"start\":45624},{\"end\":45636,\"start\":45633},{\"end\":45651,\"start\":45648},{\"end\":45667,\"start\":45660},{\"end\":45678,\"start\":45676},{\"end\":45695,\"start\":45686},{\"end\":45710,\"start\":45704},{\"end\":45732,\"start\":45721},{\"end\":45751,\"start\":45749},{\"end\":45800,\"start\":45797},{\"end\":45813,\"start\":45811},{\"end\":45826,\"start\":45824},{\"end\":45840,\"start\":45837},{\"end\":45948,\"start\":45945},{\"end\":45959,\"start\":45957},{\"end\":45969,\"start\":45967},{\"end\":45987,\"start\":45979},{\"end\":46007,\"start\":45998},{\"end\":46176,\"start\":46170},{\"end\":46190,\"start\":46183},{\"end\":46208,\"start\":46201},{\"end\":46224,\"start\":46215},{\"end\":46238,\"start\":46234},{\"end\":46255,\"start\":46247},{\"end\":46267,\"start\":46261},{\"end\":46283,\"start\":46278},{\"end\":46309,\"start\":46290},{\"end\":46326,\"start\":46320},{\"end\":46340,\"start\":46332},{\"end\":46570,\"start\":46566},{\"end\":46585,\"start\":46580},{\"end\":46601,\"start\":46596},{\"end\":46616,\"start\":46610},{\"end\":46670,\"start\":46664},{\"end\":46684,\"start\":46678},{\"end\":46699,\"start\":46693},{\"end\":46708,\"start\":46706},{\"end\":46721,\"start\":46715},{\"end\":46736,\"start\":46733},{\"end\":46755,\"start\":46750},{\"end\":46770,\"start\":46766},{\"end\":46787,\"start\":46779},{\"end\":46805,\"start\":46797},{\"end\":46815,\"start\":46810},{\"end\":46827,\"start\":46822},{\"end\":46842,\"start\":46834},{\"end\":46860,\"start\":46853},{\"end\":46874,\"start\":46868},{\"end\":47132,\"start\":47126},{\"end\":47143,\"start\":47138},{\"end\":47169,\"start\":47158},{\"end\":47293,\"start\":47288},{\"end\":47321,\"start\":47313},{\"end\":47455,\"start\":47449},{\"end\":47464,\"start\":47462},{\"end\":47474,\"start\":47469},{\"end\":47489,\"start\":47482},{\"end\":47511,\"start\":47501},{\"end\":47527,\"start\":47520},{\"end\":47540,\"start\":47535},{\"end\":47558,\"start\":47551},{\"end\":47574,\"start\":47569},{\"end\":47584,\"start\":47581},{\"end\":47599,\"start\":47591},{\"end\":47613,\"start\":47607},{\"end\":47628,\"start\":47622},{\"end\":47641,\"start\":47635},{\"end\":47656,\"start\":47650},{\"end\":47671,\"start\":47665},{\"end\":47687,\"start\":47679},{\"end\":47704,\"start\":47694},{\"end\":47715,\"start\":47710},{\"end\":47726,\"start\":47722},{\"end\":47862,\"start\":47858},{\"end\":47876,\"start\":47871},{\"end\":47889,\"start\":47884},{\"end\":47904,\"start\":47898},{\"end\":48026,\"start\":48020},{\"end\":48041,\"start\":48035},{\"end\":48178,\"start\":48171},{\"end\":48193,\"start\":48190},{\"end\":48208,\"start\":48201},{\"end\":48223,\"start\":48217},{\"end\":48236,\"start\":48233},{\"end\":48254,\"start\":48247},{\"end\":48269,\"start\":48263},{\"end\":48284,\"start\":48278},{\"end\":48300,\"start\":48293},{\"end\":48312,\"start\":48307},{\"end\":48330,\"start\":48323},{\"end\":48346,\"start\":48337},{\"end\":48433,\"start\":48427},{\"end\":48449,\"start\":48443},{\"end\":48461,\"start\":48457},{\"end\":48474,\"start\":48471},{\"end\":48488,\"start\":48484},{\"end\":48502,\"start\":48495},{\"end\":48513,\"start\":48509},{\"end\":48529,\"start\":48520},{\"end\":48641,\"start\":48634},{\"end\":48660,\"start\":48651},{\"end\":48676,\"start\":48670},{\"end\":48691,\"start\":48686},{\"end\":48704,\"start\":48699},{\"end\":48871,\"start\":48867},{\"end\":48884,\"start\":48882},{\"end\":48899,\"start\":48892},{\"end\":48912,\"start\":48906},{\"end\":48932,\"start\":48922},{\"end\":48946,\"start\":48939},{\"end\":49079,\"start\":49072},{\"end\":49093,\"start\":49089},{\"end\":49109,\"start\":49103},{\"end\":49118,\"start\":49116},{\"end\":49129,\"start\":49124},{\"end\":49143,\"start\":49137},{\"end\":49157,\"start\":49151},{\"end\":49176,\"start\":49165},{\"end\":49196,\"start\":49184},{\"end\":49212,\"start\":49205},{\"end\":49233,\"start\":49228},{\"end\":49247,\"start\":49239},{\"end\":49260,\"start\":49258},{\"end\":49275,\"start\":49270},{\"end\":49293,\"start\":49286},{\"end\":49345,\"start\":49336},{\"end\":49356,\"start\":49354},{\"end\":49370,\"start\":49364},{\"end\":49386,\"start\":49380},{\"end\":49400,\"start\":49393},{\"end\":49411,\"start\":49409},{\"end\":49425,\"start\":49417},{\"end\":49437,\"start\":49434},{\"end\":49450,\"start\":49445},{\"end\":49469,\"start\":49460},{\"end\":49482,\"start\":49475},{\"end\":49497,\"start\":49490},{\"end\":49634,\"start\":49625},{\"end\":49693,\"start\":49685},{\"end\":49708,\"start\":49702},{\"end\":49723,\"start\":49717},{\"end\":49739,\"start\":49733},{\"end\":49755,\"start\":49749},{\"end\":49822,\"start\":49814},{\"end\":49836,\"start\":49830},{\"end\":49855,\"start\":49847},{\"end\":49869,\"start\":49862},{\"end\":49882,\"start\":49876},{\"end\":50037,\"start\":50031},{\"end\":50050,\"start\":50044},{\"end\":50064,\"start\":50059},{\"end\":50072,\"start\":50069},{\"end\":50080,\"start\":50078},{\"end\":50096,\"start\":50091},{\"end\":50107,\"start\":50105},{\"end\":50119,\"start\":50115},{\"end\":50132,\"start\":50126},{\"end\":50144,\"start\":50139},{\"end\":50267,\"start\":50253},{\"end\":50279,\"start\":50274},{\"end\":50301,\"start\":50286},{\"end\":50316,\"start\":50309},{\"end\":50486,\"start\":50478},{\"end\":50499,\"start\":50493},{\"end\":50511,\"start\":50509},{\"end\":50527,\"start\":50520},{\"end\":50538,\"start\":50534},{\"end\":50552,\"start\":50548},{\"end\":50566,\"start\":50559},{\"end\":50580,\"start\":50574},{\"end\":50599,\"start\":50589},{\"end\":50748,\"start\":50732},{\"end\":50769,\"start\":50759},{\"end\":50783,\"start\":50778},{\"end\":50792,\"start\":50785},{\"end\":50852,\"start\":50847},{\"end\":50860,\"start\":50856},{\"end\":50870,\"start\":50864},{\"end\":50987,\"start\":50983},{\"end\":51004,\"start\":51000},{\"end\":51019,\"start\":51015},{\"end\":51163,\"start\":51157},{\"end\":51277,\"start\":51274},{\"end\":51287,\"start\":51285},{\"end\":51309,\"start\":51304},{\"end\":51326,\"start\":51318},{\"end\":51344,\"start\":51336},{\"end\":51502,\"start\":51500},{\"end\":51512,\"start\":51509},{\"end\":51523,\"start\":51521},{\"end\":51536,\"start\":51532},{\"end\":51547,\"start\":51545},{\"end\":51558,\"start\":51554},{\"end\":51568,\"start\":51564},{\"end\":51581,\"start\":51577},{\"end\":51704,\"start\":51702},{\"end\":51715,\"start\":51713},{\"end\":51726,\"start\":51722},{\"end\":51738,\"start\":51735},{\"end\":51753,\"start\":51748},{\"end\":51764,\"start\":51760},{\"end\":51900,\"start\":51896},{\"end\":51914,\"start\":51908},{\"end\":51932,\"start\":51924},{\"end\":51944,\"start\":51938},{\"end\":51955,\"start\":51949},{\"end\":51969,\"start\":51963},{\"end\":51984,\"start\":51979},{\"end\":52068,\"start\":52063},{\"end\":52084,\"start\":52077},{\"end\":52098,\"start\":52096},{\"end\":52119,\"start\":52109},{\"end\":52131,\"start\":52126},{\"end\":52288,\"start\":52284},{\"end\":52298,\"start\":52296},{\"end\":52309,\"start\":52307},{\"end\":52454,\"start\":52448},{\"end\":52469,\"start\":52462},{\"end\":52487,\"start\":52479},{\"end\":52497,\"start\":52495},{\"end\":52509,\"start\":52504},{\"end\":52524,\"start\":52517},{\"end\":52537,\"start\":52531},{\"end\":52558,\"start\":52548},{\"end\":52566,\"start\":52560}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":40616,\"start\":40290},{\"attributes\":{\"id\":\"b1\"},\"end\":41190,\"start\":40618},{\"attributes\":{\"id\":\"b2\"},\"end\":42052,\"start\":41192},{\"attributes\":{\"id\":\"b3\"},\"end\":42250,\"start\":42054},{\"attributes\":{\"id\":\"b4\"},\"end\":42429,\"start\":42252},{\"attributes\":{\"id\":\"b5\"},\"end\":42610,\"start\":42431},{\"attributes\":{\"id\":\"b6\"},\"end\":42759,\"start\":42612},{\"attributes\":{\"id\":\"b7\"},\"end\":43035,\"start\":42761},{\"attributes\":{\"id\":\"b8\"},\"end\":43221,\"start\":43037},{\"attributes\":{\"id\":\"b9\"},\"end\":43333,\"start\":43223},{\"attributes\":{\"id\":\"b10\"},\"end\":43586,\"start\":43335},{\"attributes\":{\"id\":\"b11\"},\"end\":43814,\"start\":43588},{\"attributes\":{\"id\":\"b12\"},\"end\":43936,\"start\":43816},{\"attributes\":{\"id\":\"b13\"},\"end\":44124,\"start\":43938},{\"attributes\":{\"id\":\"b14\"},\"end\":44330,\"start\":44126},{\"attributes\":{\"id\":\"b15\"},\"end\":44476,\"start\":44332},{\"attributes\":{\"id\":\"b16\"},\"end\":44614,\"start\":44478},{\"attributes\":{\"id\":\"b17\"},\"end\":44882,\"start\":44616},{\"attributes\":{\"id\":\"b18\"},\"end\":45050,\"start\":44884},{\"attributes\":{\"id\":\"b19\"},\"end\":45277,\"start\":45052},{\"attributes\":{\"id\":\"b20\"},\"end\":45409,\"start\":45279},{\"attributes\":{\"id\":\"b21\"},\"end\":45564,\"start\":45411},{\"attributes\":{\"id\":\"b22\"},\"end\":45787,\"start\":45566},{\"attributes\":{\"id\":\"b23\"},\"end\":45873,\"start\":45789},{\"attributes\":{\"id\":\"b24\"},\"end\":46043,\"start\":45875},{\"attributes\":{\"id\":\"b25\"},\"end\":46491,\"start\":46045},{\"attributes\":{\"id\":\"b26\"},\"end\":46652,\"start\":46493},{\"attributes\":{\"id\":\"b27\"},\"end\":47026,\"start\":46654},{\"attributes\":{\"id\":\"b28\"},\"end\":47225,\"start\":47028},{\"attributes\":{\"id\":\"b29\"},\"end\":47373,\"start\":47227},{\"attributes\":{\"id\":\"b30\"},\"end\":47762,\"start\":47375},{\"attributes\":{\"id\":\"b31\"},\"end\":47930,\"start\":47764},{\"attributes\":{\"id\":\"b32\"},\"end\":48093,\"start\":47932},{\"attributes\":{\"id\":\"b33\"},\"end\":48382,\"start\":48095},{\"attributes\":{\"id\":\"b34\"},\"end\":48565,\"start\":48384},{\"attributes\":{\"id\":\"b35\"},\"end\":48770,\"start\":48567},{\"attributes\":{\"id\":\"b36\"},\"end\":48982,\"start\":48772},{\"attributes\":{\"id\":\"b37\"},\"end\":49329,\"start\":48984},{\"attributes\":{\"id\":\"b38\"},\"end\":49596,\"start\":49331},{\"attributes\":{\"id\":\"b39\"},\"end\":49644,\"start\":49598},{\"attributes\":{\"id\":\"b40\"},\"end\":49807,\"start\":49646},{\"attributes\":{\"id\":\"b41\"},\"end\":49959,\"start\":49809},{\"attributes\":{\"id\":\"b42\"},\"end\":50180,\"start\":49961},{\"attributes\":{\"id\":\"b43\"},\"end\":50427,\"start\":50182},{\"attributes\":{\"id\":\"b44\"},\"end\":50644,\"start\":50429},{\"attributes\":{\"id\":\"b45\"},\"end\":50887,\"start\":50646},{\"attributes\":{\"id\":\"b46\"},\"end\":51055,\"start\":50889},{\"attributes\":{\"id\":\"b47\"},\"end\":51191,\"start\":51057},{\"attributes\":{\"id\":\"b48\"},\"end\":51404,\"start\":51193},{\"attributes\":{\"id\":\"b49\"},\"end\":51617,\"start\":51406},{\"attributes\":{\"id\":\"b50\"},\"end\":51824,\"start\":51619},{\"attributes\":{\"id\":\"b51\"},\"end\":52210,\"start\":51826},{\"attributes\":{\"id\":\"b52\"},\"end\":52444,\"start\":52212},{\"attributes\":{\"id\":\"b53\"},\"end\":52654,\"start\":52446}]", "bib_title": "[{\"end\":40368,\"start\":40290},{\"end\":42302,\"start\":42252},{\"end\":42482,\"start\":42431},{\"end\":42657,\"start\":42612},{\"end\":43100,\"start\":43037},{\"end\":43986,\"start\":43938},{\"end\":44366,\"start\":44332},{\"end\":44518,\"start\":44478},{\"end\":44939,\"start\":44884},{\"end\":45108,\"start\":45052},{\"end\":45307,\"start\":45279},{\"end\":45473,\"start\":45411},{\"end\":46162,\"start\":46045},{\"end\":47118,\"start\":47028},{\"end\":47276,\"start\":47227},{\"end\":48014,\"start\":47932},{\"end\":48626,\"start\":48567},{\"end\":49678,\"start\":49646},{\"end\":50244,\"start\":50182},{\"end\":50470,\"start\":50429},{\"end\":50724,\"start\":50646},{\"end\":51267,\"start\":51193},{\"end\":51693,\"start\":51619},{\"end\":51886,\"start\":51826},{\"end\":52276,\"start\":52212}]", "bib_author": "[{\"end\":40383,\"start\":40370},{\"end\":40393,\"start\":40383},{\"end\":40405,\"start\":40393},{\"end\":40423,\"start\":40405},{\"end\":40439,\"start\":40423},{\"end\":40455,\"start\":40439},{\"end\":40721,\"start\":40709},{\"end\":40733,\"start\":40721},{\"end\":40748,\"start\":40733},{\"end\":40763,\"start\":40748},{\"end\":40774,\"start\":40763},{\"end\":40789,\"start\":40774},{\"end\":40801,\"start\":40789},{\"end\":40817,\"start\":40801},{\"end\":40831,\"start\":40817},{\"end\":40845,\"start\":40831},{\"end\":40862,\"start\":40845},{\"end\":40879,\"start\":40862},{\"end\":40896,\"start\":40879},{\"end\":40909,\"start\":40896},{\"end\":40925,\"start\":40909},{\"end\":40940,\"start\":40925},{\"end\":40960,\"start\":40940},{\"end\":40977,\"start\":40960},{\"end\":40991,\"start\":40977},{\"end\":41007,\"start\":40991},{\"end\":41022,\"start\":41007},{\"end\":41036,\"start\":41022},{\"end\":41048,\"start\":41036},{\"end\":41066,\"start\":41048},{\"end\":41080,\"start\":41066},{\"end\":41091,\"start\":41080},{\"end\":41103,\"start\":41091},{\"end\":41119,\"start\":41103},{\"end\":41131,\"start\":41119},{\"end\":41141,\"start\":41131},{\"end\":41155,\"start\":41141},{\"end\":41204,\"start\":41192},{\"end\":41221,\"start\":41204},{\"end\":41237,\"start\":41221},{\"end\":41252,\"start\":41237},{\"end\":41269,\"start\":41252},{\"end\":41281,\"start\":41269},{\"end\":41292,\"start\":41281},{\"end\":41305,\"start\":41292},{\"end\":41324,\"start\":41305},{\"end\":41342,\"start\":41324},{\"end\":41354,\"start\":41342},{\"end\":41372,\"start\":41354},{\"end\":41390,\"start\":41372},{\"end\":41407,\"start\":41390},{\"end\":41419,\"start\":41407},{\"end\":41433,\"start\":41419},{\"end\":41444,\"start\":41433},{\"end\":41462,\"start\":41444},{\"end\":41475,\"start\":41462},{\"end\":41487,\"start\":41475},{\"end\":41502,\"start\":41487},{\"end\":41518,\"start\":41502},{\"end\":41533,\"start\":41518},{\"end\":41548,\"start\":41533},{\"end\":41566,\"start\":41548},{\"end\":41580,\"start\":41566},{\"end\":41598,\"start\":41580},{\"end\":41613,\"start\":41598},{\"end\":41632,\"start\":41613},{\"end\":41647,\"start\":41632},{\"end\":41657,\"start\":41647},{\"end\":41667,\"start\":41657},{\"end\":41687,\"start\":41667},{\"end\":41697,\"start\":41687},{\"end\":41711,\"start\":41697},{\"end\":41728,\"start\":41711},{\"end\":41744,\"start\":41728},{\"end\":41755,\"start\":41744},{\"end\":41769,\"start\":41755},{\"end\":42065,\"start\":42054},{\"end\":42078,\"start\":42065},{\"end\":42088,\"start\":42078},{\"end\":42100,\"start\":42088},{\"end\":42116,\"start\":42100},{\"end\":42136,\"start\":42116},{\"end\":42149,\"start\":42136},{\"end\":42327,\"start\":42304},{\"end\":42338,\"start\":42327},{\"end\":42352,\"start\":42338},{\"end\":42366,\"start\":42352},{\"end\":42378,\"start\":42366},{\"end\":42386,\"start\":42378},{\"end\":42494,\"start\":42484},{\"end\":42504,\"start\":42494},{\"end\":42520,\"start\":42504},{\"end\":42531,\"start\":42520},{\"end\":42539,\"start\":42531},{\"end\":42551,\"start\":42539},{\"end\":42678,\"start\":42659},{\"end\":42695,\"start\":42678},{\"end\":42704,\"start\":42695},{\"end\":42923,\"start\":42913},{\"end\":42937,\"start\":42923},{\"end\":42952,\"start\":42937},{\"end\":42972,\"start\":42952},{\"end\":42989,\"start\":42972},{\"end\":43001,\"start\":42989},{\"end\":43112,\"start\":43102},{\"end\":43121,\"start\":43112},{\"end\":43137,\"start\":43121},{\"end\":43152,\"start\":43137},{\"end\":43167,\"start\":43152},{\"end\":43285,\"start\":43275},{\"end\":43299,\"start\":43285},{\"end\":43422,\"start\":43412},{\"end\":43438,\"start\":43422},{\"end\":43449,\"start\":43438},{\"end\":43458,\"start\":43449},{\"end\":43473,\"start\":43458},{\"end\":43490,\"start\":43473},{\"end\":43505,\"start\":43490},{\"end\":43527,\"start\":43505},{\"end\":43541,\"start\":43527},{\"end\":43552,\"start\":43541},{\"end\":43691,\"start\":43680},{\"end\":43705,\"start\":43691},{\"end\":43719,\"start\":43705},{\"end\":43733,\"start\":43719},{\"end\":43741,\"start\":43733},{\"end\":43754,\"start\":43741},{\"end\":43770,\"start\":43754},{\"end\":43780,\"start\":43770},{\"end\":43825,\"start\":43816},{\"end\":43840,\"start\":43825},{\"end\":43854,\"start\":43840},{\"end\":44001,\"start\":43988},{\"end\":44017,\"start\":44001},{\"end\":44021,\"start\":44017},{\"end\":44035,\"start\":44021},{\"end\":44039,\"start\":44035},{\"end\":44052,\"start\":44039},{\"end\":44068,\"start\":44052},{\"end\":44085,\"start\":44068},{\"end\":44099,\"start\":44085},{\"end\":44111,\"start\":44099},{\"end\":44228,\"start\":44203},{\"end\":44244,\"start\":44228},{\"end\":44260,\"start\":44244},{\"end\":44281,\"start\":44260},{\"end\":44296,\"start\":44281},{\"end\":44381,\"start\":44368},{\"end\":44395,\"start\":44381},{\"end\":44533,\"start\":44520},{\"end\":44544,\"start\":44533},{\"end\":44559,\"start\":44544},{\"end\":44699,\"start\":44686},{\"end\":44713,\"start\":44699},{\"end\":44730,\"start\":44713},{\"end\":44741,\"start\":44730},{\"end\":44752,\"start\":44741},{\"end\":44770,\"start\":44752},{\"end\":44782,\"start\":44770},{\"end\":44794,\"start\":44782},{\"end\":44810,\"start\":44794},{\"end\":44827,\"start\":44810},{\"end\":44838,\"start\":44827},{\"end\":44848,\"start\":44838},{\"end\":44957,\"start\":44941},{\"end\":44967,\"start\":44957},{\"end\":44985,\"start\":44967},{\"end\":45000,\"start\":44985},{\"end\":45123,\"start\":45110},{\"end\":45138,\"start\":45123},{\"end\":45332,\"start\":45309},{\"end\":45346,\"start\":45332},{\"end\":45362,\"start\":45346},{\"end\":45366,\"start\":45362},{\"end\":45479,\"start\":45475},{\"end\":45493,\"start\":45479},{\"end\":45506,\"start\":45493},{\"end\":45629,\"start\":45618},{\"end\":45638,\"start\":45629},{\"end\":45653,\"start\":45638},{\"end\":45669,\"start\":45653},{\"end\":45680,\"start\":45669},{\"end\":45697,\"start\":45680},{\"end\":45712,\"start\":45697},{\"end\":45734,\"start\":45712},{\"end\":45753,\"start\":45734},{\"end\":45802,\"start\":45789},{\"end\":45815,\"start\":45802},{\"end\":45828,\"start\":45815},{\"end\":45842,\"start\":45828},{\"end\":45950,\"start\":45941},{\"end\":45961,\"start\":45950},{\"end\":45971,\"start\":45961},{\"end\":45989,\"start\":45971},{\"end\":46009,\"start\":45989},{\"end\":46178,\"start\":46164},{\"end\":46192,\"start\":46178},{\"end\":46210,\"start\":46192},{\"end\":46226,\"start\":46210},{\"end\":46240,\"start\":46226},{\"end\":46257,\"start\":46240},{\"end\":46269,\"start\":46257},{\"end\":46285,\"start\":46269},{\"end\":46311,\"start\":46285},{\"end\":46328,\"start\":46311},{\"end\":46342,\"start\":46328},{\"end\":46572,\"start\":46559},{\"end\":46587,\"start\":46572},{\"end\":46603,\"start\":46587},{\"end\":46618,\"start\":46603},{\"end\":46672,\"start\":46654},{\"end\":46686,\"start\":46672},{\"end\":46701,\"start\":46686},{\"end\":46710,\"start\":46701},{\"end\":46723,\"start\":46710},{\"end\":46738,\"start\":46723},{\"end\":46757,\"start\":46738},{\"end\":46772,\"start\":46757},{\"end\":46789,\"start\":46772},{\"end\":46807,\"start\":46789},{\"end\":46817,\"start\":46807},{\"end\":46829,\"start\":46817},{\"end\":46844,\"start\":46829},{\"end\":46862,\"start\":46844},{\"end\":46876,\"start\":46862},{\"end\":47134,\"start\":47120},{\"end\":47145,\"start\":47134},{\"end\":47151,\"start\":47145},{\"end\":47171,\"start\":47151},{\"end\":47295,\"start\":47278},{\"end\":47304,\"start\":47295},{\"end\":47323,\"start\":47304},{\"end\":47457,\"start\":47444},{\"end\":47466,\"start\":47457},{\"end\":47476,\"start\":47466},{\"end\":47491,\"start\":47476},{\"end\":47513,\"start\":47491},{\"end\":47529,\"start\":47513},{\"end\":47542,\"start\":47529},{\"end\":47560,\"start\":47542},{\"end\":47576,\"start\":47560},{\"end\":47586,\"start\":47576},{\"end\":47601,\"start\":47586},{\"end\":47615,\"start\":47601},{\"end\":47630,\"start\":47615},{\"end\":47643,\"start\":47630},{\"end\":47658,\"start\":47643},{\"end\":47673,\"start\":47658},{\"end\":47689,\"start\":47673},{\"end\":47706,\"start\":47689},{\"end\":47717,\"start\":47706},{\"end\":47728,\"start\":47717},{\"end\":47864,\"start\":47850},{\"end\":47878,\"start\":47864},{\"end\":47891,\"start\":47878},{\"end\":47906,\"start\":47891},{\"end\":48028,\"start\":48016},{\"end\":48043,\"start\":48028},{\"end\":48180,\"start\":48166},{\"end\":48195,\"start\":48180},{\"end\":48210,\"start\":48195},{\"end\":48225,\"start\":48210},{\"end\":48238,\"start\":48225},{\"end\":48256,\"start\":48238},{\"end\":48271,\"start\":48256},{\"end\":48286,\"start\":48271},{\"end\":48302,\"start\":48286},{\"end\":48314,\"start\":48302},{\"end\":48332,\"start\":48314},{\"end\":48348,\"start\":48332},{\"end\":48435,\"start\":48420},{\"end\":48451,\"start\":48435},{\"end\":48463,\"start\":48451},{\"end\":48476,\"start\":48463},{\"end\":48490,\"start\":48476},{\"end\":48504,\"start\":48490},{\"end\":48515,\"start\":48504},{\"end\":48531,\"start\":48515},{\"end\":48643,\"start\":48628},{\"end\":48662,\"start\":48643},{\"end\":48678,\"start\":48662},{\"end\":48693,\"start\":48678},{\"end\":48706,\"start\":48693},{\"end\":48873,\"start\":48858},{\"end\":48886,\"start\":48873},{\"end\":48901,\"start\":48886},{\"end\":48914,\"start\":48901},{\"end\":48934,\"start\":48914},{\"end\":48948,\"start\":48934},{\"end\":49081,\"start\":49064},{\"end\":49095,\"start\":49081},{\"end\":49111,\"start\":49095},{\"end\":49120,\"start\":49111},{\"end\":49131,\"start\":49120},{\"end\":49145,\"start\":49131},{\"end\":49159,\"start\":49145},{\"end\":49178,\"start\":49159},{\"end\":49198,\"start\":49178},{\"end\":49214,\"start\":49198},{\"end\":49235,\"start\":49214},{\"end\":49249,\"start\":49235},{\"end\":49262,\"start\":49249},{\"end\":49277,\"start\":49262},{\"end\":49295,\"start\":49277},{\"end\":49347,\"start\":49331},{\"end\":49358,\"start\":49347},{\"end\":49372,\"start\":49358},{\"end\":49388,\"start\":49372},{\"end\":49402,\"start\":49388},{\"end\":49413,\"start\":49402},{\"end\":49427,\"start\":49413},{\"end\":49439,\"start\":49427},{\"end\":49452,\"start\":49439},{\"end\":49471,\"start\":49452},{\"end\":49484,\"start\":49471},{\"end\":49499,\"start\":49484},{\"end\":49636,\"start\":49616},{\"end\":49695,\"start\":49680},{\"end\":49710,\"start\":49695},{\"end\":49725,\"start\":49710},{\"end\":49741,\"start\":49725},{\"end\":49757,\"start\":49741},{\"end\":49824,\"start\":49809},{\"end\":49838,\"start\":49824},{\"end\":49857,\"start\":49838},{\"end\":49871,\"start\":49857},{\"end\":49884,\"start\":49871},{\"end\":50039,\"start\":50025},{\"end\":50052,\"start\":50039},{\"end\":50066,\"start\":50052},{\"end\":50074,\"start\":50066},{\"end\":50082,\"start\":50074},{\"end\":50098,\"start\":50082},{\"end\":50109,\"start\":50098},{\"end\":50121,\"start\":50109},{\"end\":50134,\"start\":50121},{\"end\":50146,\"start\":50134},{\"end\":50269,\"start\":50246},{\"end\":50281,\"start\":50269},{\"end\":50303,\"start\":50281},{\"end\":50318,\"start\":50303},{\"end\":50488,\"start\":50472},{\"end\":50501,\"start\":50488},{\"end\":50513,\"start\":50501},{\"end\":50529,\"start\":50513},{\"end\":50540,\"start\":50529},{\"end\":50554,\"start\":50540},{\"end\":50568,\"start\":50554},{\"end\":50582,\"start\":50568},{\"end\":50601,\"start\":50582},{\"end\":50750,\"start\":50726},{\"end\":50771,\"start\":50750},{\"end\":50785,\"start\":50771},{\"end\":50794,\"start\":50785},{\"end\":50989,\"start\":50974},{\"end\":51006,\"start\":50989},{\"end\":51021,\"start\":51006},{\"end\":51165,\"start\":51148},{\"end\":51279,\"start\":51269},{\"end\":51289,\"start\":51279},{\"end\":51311,\"start\":51289},{\"end\":51328,\"start\":51311},{\"end\":51346,\"start\":51328},{\"end\":51504,\"start\":51491},{\"end\":51514,\"start\":51504},{\"end\":51525,\"start\":51514},{\"end\":51538,\"start\":51525},{\"end\":51549,\"start\":51538},{\"end\":51560,\"start\":51549},{\"end\":51570,\"start\":51560},{\"end\":51583,\"start\":51570},{\"end\":51706,\"start\":51695},{\"end\":51717,\"start\":51706},{\"end\":51728,\"start\":51717},{\"end\":51740,\"start\":51728},{\"end\":51755,\"start\":51740},{\"end\":51766,\"start\":51755},{\"end\":51902,\"start\":51888},{\"end\":51916,\"start\":51902},{\"end\":51934,\"start\":51916},{\"end\":51946,\"start\":51934},{\"end\":51957,\"start\":51946},{\"end\":51971,\"start\":51957},{\"end\":51986,\"start\":51971},{\"end\":52290,\"start\":52278},{\"end\":52300,\"start\":52290},{\"end\":52311,\"start\":52300},{\"end\":52456,\"start\":52446},{\"end\":52471,\"start\":52456},{\"end\":52489,\"start\":52471},{\"end\":52499,\"start\":52489},{\"end\":52511,\"start\":52499},{\"end\":52526,\"start\":52511},{\"end\":52539,\"start\":52526},{\"end\":52560,\"start\":52539},{\"end\":52568,\"start\":52560}]", "bib_venue": "[{\"end\":42033,\"start\":41962},{\"end\":45273,\"start\":45214},{\"end\":52440,\"start\":52384},{\"end\":40520,\"start\":40471},{\"end\":40707,\"start\":40618},{\"end\":41833,\"start\":41785},{\"end\":42230,\"start\":42165},{\"end\":42423,\"start\":42386},{\"end\":42604,\"start\":42551},{\"end\":42753,\"start\":42704},{\"end\":42911,\"start\":42761},{\"end\":43211,\"start\":43167},{\"end\":43273,\"start\":43223},{\"end\":43410,\"start\":43335},{\"end\":43678,\"start\":43588},{\"end\":43916,\"start\":43870},{\"end\":44118,\"start\":44111},{\"end\":44201,\"start\":44126},{\"end\":44470,\"start\":44395},{\"end\":44608,\"start\":44559},{\"end\":44684,\"start\":44616},{\"end\":45044,\"start\":45000},{\"end\":45212,\"start\":45138},{\"end\":45403,\"start\":45366},{\"end\":45558,\"start\":45506},{\"end\":45616,\"start\":45566},{\"end\":45867,\"start\":45842},{\"end\":45939,\"start\":45875},{\"end\":46398,\"start\":46358},{\"end\":46557,\"start\":46493},{\"end\":47006,\"start\":46892},{\"end\":47219,\"start\":47171},{\"end\":47367,\"start\":47323},{\"end\":47442,\"start\":47375},{\"end\":47848,\"start\":47764},{\"end\":48087,\"start\":48043},{\"end\":48164,\"start\":48095},{\"end\":48418,\"start\":48384},{\"end\":48764,\"start\":48706},{\"end\":48856,\"start\":48772},{\"end\":49062,\"start\":48984},{\"end\":49576,\"start\":49515},{\"end\":49614,\"start\":49598},{\"end\":49801,\"start\":49757},{\"end\":49939,\"start\":49900},{\"end\":50023,\"start\":49961},{\"end\":50370,\"start\":50318},{\"end\":50638,\"start\":50601},{\"end\":50843,\"start\":50794},{\"end\":50972,\"start\":50889},{\"end\":51146,\"start\":51057},{\"end\":51398,\"start\":51346},{\"end\":51489,\"start\":51406},{\"end\":51818,\"start\":51766},{\"end\":52054,\"start\":52002},{\"end\":52382,\"start\":52311},{\"end\":52634,\"start\":52584}]"}}}, "year": 2023, "month": 12, "day": 17}