{"id": 219176558, "updated": "2023-10-06 15:20:32.06", "metadata": {"title": "Evaluation of CNN-based Automatic Music Tagging Models", "authors": "[{\"first\":\"Minz\",\"last\":\"Won\",\"middle\":[]},{\"first\":\"Andres\",\"last\":\"Ferraro\",\"middle\":[]},{\"first\":\"Dmitry\",\"last\":\"Bogdanov\",\"middle\":[]},{\"first\":\"Xavier\",\"last\":\"Serra\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Recent advances in deep learning accelerated the development of content-based automatic music tagging systems. Music information retrieval (MIR) researchers proposed various architecture designs, mainly based on convolutional neural networks (CNNs), that achieve state-of-the-art re-sults in this multi-label binary classi\ufb01cation task. However, due to the differences in experimental setups followed by researchers, such as using different dataset splits and software versions for evaluation, it is dif\ufb01cult to compare the proposed architectures directly with each other. To facilitate further research, in this paper we conduct a consistent evaluation of different music tagging models on three datasets (MagnaTagATune, Million Song Dataset, and MTG-Jamendo) and provide reference results using common evaluation metrics (ROC-AUC and PR-AUC). Furthermore, all the models are evaluated with perturbed inputs to investigate the generalization capabilities concerning time stretch, pitch shift, dynamic range compression, and addition of white noise. For reproducibility, we provide the PyTorch implementations with the pre-trained models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2006.00751", "mag": "3029858316", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2006-00751", "doi": null}}, "content": {"source": {"pdf_hash": "5861b2c22f1e9c00f29f1e20c4993b21ab75ec0a", "pdf_src": "Arxiv", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "43afad51093668af2b342f4d3682ee1a76675e4c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5861b2c22f1e9c00f29f1e20c4993b21ab75ec0a.txt", "contents": "\nEvaluation of CNN-based Automatic Music Tagging Models\n\n\nMinz Won \nMusic Technology Group Universitat Pompeu Fabra\nBarcelona\n\nAndres Ferraro \nMusic Technology Group Universitat Pompeu Fabra\nBarcelona\n\nDmitry Bogdanov \nMusic Technology Group Universitat Pompeu Fabra\nBarcelona\n\nXavier Serra \nMusic Technology Group Universitat Pompeu Fabra\nBarcelona\n\nEvaluation of CNN-based Automatic Music Tagging Models\n\nRecent advances in deep learning accelerated the development of content-based automatic music tagging systems. Music information retrieval (MIR) researchers proposed various architecture designs, mainly based on convolutional neural networks (CNNs), that achieve state-of-the-art results in this multi-label binary classification task. However, due to the differences in experimental setups followed by researchers, such as using different dataset splits and software versions for evaluation, it is difficult to compare the proposed architectures directly with each other. To facilitate further research, in this paper we conduct a consistent evaluation of different music tagging models on three datasets (MagnaTagATune, Million Song Dataset, and MTG-Jamendo) and provide reference results using common evaluation metrics (ROC-AUC and PR-AUC). Furthermore, all the models are evaluated with perturbed inputs to investigate the generalization capabilities concerning time stretch, pitch shift, dynamic range compression, and addition of white noise. For reproducibility, we provide the PyTorch implementations with the pre-trained models.\n\nINTRODUCTION\n\nAutomatic music tagging is a multi-label binary classification task that aims to predict relevant tags for a given song. Typically these tags carry useful semantic music information that can later be used for other applications such as music recommendation or music retrieval. To tackle the problem of music tagging, recent studies in music information retrieval (MIR) adopted deep neural networks, mostly based on convolutional neural networks (CNNs), motivated by their huge success in other domains (e.g., computer vision, natural language processing). The introduction of deep learning helped to break the previous glass ceiling in the performance of music tagging systems and MIR researchers started actively proposing ingenious architecture designs. As a result, the hand-crafted featurebased approaches were replaced by data-driven feature learning approaches in most recent automatic music tagging research.\n\nTo maximize the advantages of CNNs, a deep fully convolutional network (FCN) was proposed for music tagging [1]. It uses a stack of 3 \u00d7 3 rectangular filters followed by a max-pooling layer. As an alternative, the Musicnn [2], also a Mel-spectrogram-based CNN, tried to incorporate domain knowledge into its filter designs so that the model can capture timbral characteristics and temporal patterns using vertical filters and horizontal filters, respectively. Sample-level CNN [3] pursued an assumption-free end-to-end model by applying 1D convolution directly to a raw audio waveform, and the following research [4] improved the performance by adding squeeze-and-excitation blocks [5]. Different from images, however, music is sequential. For this reason, a convolutional recurrent neural network (CRNN) [6] was proposed to extract local features using CNNs and summarize them with recurrent neural networks (RNNs). Another sequence modeling approach [7] adapted the self-attention mechanism [8] to summarize the temporal sequence of the extracted local features by CNNs. Finally, Harmonic CNN [9] used a harmonically stacked trainable representation to preserve spectrotemporal locality in convolution layers.\n\nUnfortunately, due to different experimental setups followed by the authors of these approaches when reporting results (e.g., dataset splits, library versions, computing environments, and optimization methods), it is difficult to compare the proposed architectures directly with each other. In this paper, we address this issue and report experimental results for various state-of-the-art music tagging models using three different datasets (MagnaTa-gATune, Million Song Dataset, and MTG-Jamendo dataset) with a consistent experimental setup. In addition, we conduct experiments to assess the robustness of these architectures against four different types of deformations [10] and determine their generalization abilities. For the reproducibility, we provide PyTorch implementations for all the models considered and their pre-trained models. 1 The paper is organized as follows. Section 2 describes automatic music tagging tasks with details of multiple instance problem, evaluation metrics, and dataset descriptions. Section 3 elaborates on the model designs. We report model performances and their generalization capabilities in Section 4 and Section 5, respectively. Finally, Section 6 concludes the paper.\n\n\nAUTOMATIC MUSIC TAGGING\n\n\nMultiple Instance Problem\n\nVarious semantic information characterizing music, such as genres, subgenres, moods, instruments, decades, and languages, can be expressed in the form of music tags. Automatic content-based music tagging is a task that aims to predict such relevant tags for a given song based on its acoustic characteristics. However, depending on the tag, relevant characteristics are not necessarily predominant in the entire music recording. For example, when a song has a tag female vocal, it does not imply that the female vocal appears in every time segment of the song. In essence, this is a multiple instance problem [11]. A song can have many acoustic characteristics (instances) that describe it, but often only specific characteristics (instances) are responsible for the associated music tag. In most cases, we do not have time precise instance-level annotations because the precise labeling can be laborsome, therefore a music tag associated with a song is simply applied to all music excerpts (instances) of the song during training [12].\n\nThere are two approaches to handle the multiple instance problem in music tagging. One is to train the model on full songs and produce song-level predictions from a songlevel input. From a given song-level input, the model has to predict relevant tags. Another one is to train the model on short audio chunks (instances) and generate chunk-level predictions, which can be later aggregated (e.g., majority vote, average) in the evaluation phase. In this paper, we refer to these two approaches as song-level training and chunk-level training, respectively.\n\n\nEvaluation Metrics\n\nA common evaluation metric of binary decision problems is the area under receiver operating characteristic curve (ROC-AUC). However, the area under precision-recall curve (PR-AUC) can be more informative for evaluation on highly skewed datasets [13]. Hence, we use both macro ROC-AUC and PR-AUC to evaluate all considered music tagging models with both scores being averaged across all the tags the models operate with. For our robustness studies, we report the variance of ROC-AUC and PR-AUC scores obtained on perturbed audio inputs.\n\n\nDatasets\n\nMagnaTagATune (MTAT) [14] is one of the most commonly used datasets for benchmarking automatic music tagging systems. It contains multi-label annotations by genre, mood, and instrumentation for 25,877 audio segments, each 30s long. The audio is in the MP3 format (32 Kbps bitrate and 16 kHz sample rate). Originally the dataset is split into 16 folders, and commonly the first 12 folders are used for training, the 13th for validation, and the last three are used for testing. Only 50 most frequent tags are typically used for the task. The top 50 tags include genre and instrumentation labels, as well as decades (e.g., '80s' and '90s') and moods.\n\nIn our work, we follow the same data split and we use the top 50 tags to be consistent with results reported in the majority of previous studies. 2 However, we have noted that the performances reported in some studies are inconsistent, as their authors discarded audio segments without any associated tags (leading to slightly higher values of performance). Also, some of the previous studies are reusing those reports unintentionally compare incompatible performance values, which is one of the main motivations of our work. Million Song Dataset (MSD) [15] is a dataset of audio features for one million songs, partially expanded by the MIR community with crowdsourced tags from Last.fm as well as a mapping to 30s audio preview segments originally obtained from 7digital. 3 In total, this subset of the dataset contains 241,904 annotated song segments and it is commonly used for benchmarking on a larger scale. The tags cover genre, instrumentation, moods and decades. The audio segments vary in quality, being encoded as MP3s with a bitrate from 64 to 128 Kbps and sample rate of 22 kHz or 44 kHz.\n\nAgain, in our study we follow the dataset split commonly used by researchers 4 [2,3] and use only 50 most frequent tags for consistency with previous studies. This split includes 201,680 songs for training, 11,774 for validation and 28,435 for testing. Unfortunately, similarly to MTAT, some previous studies report inconsistent comparisons of automatic tagging approaches, as there also exists a slightly different split, 5 containing audio segments of shorter duration and on which lower performance values were reported. Note that the tag annotations available for this dataset are inherently noisy as they come from a free-form social tagging application for music enthusiasts and are used without any preprocessing intended to improve the quality of tags [16]. MTG-Jamendo Dataset [17] contains audio for 55,701 full songs and is built using music publicly available on the Jamendo 6 music platform under Creative Commons licenses. The minimum duration of each song is 30s, and they are provided in the MP3 (320 Kbps bitrate). Thus, this dataset contains significantly larger audio segments with higher encoding quality than MTAT and MSD. The tracks in the dataset are annotated by 692 different tags covering genres, instrumentation, moods and themes. All tags were originally provided by the artists submitting music to Jamendo, but they were preprocessed with the goal of tag cleaning by the creators of the dataset.\n\nMultiple splits of the data are provided for training, validation and test. In this work we use the split-0 and 50 most frequent tags. 7 As this dataset has been released only recently, not many studies are reporting the performance of the models using it yet. Yet, it is a useful addition for the evaluation methodologies followed by researchers in order to better assess the generalization of their models.   \n\n\nMODELS\n\nWe here describe the architectures of all models considered in this study. Essentia [18] and Librosa [19] libraries are used for loading audio files and extracting Mel spectrograms, respectively. We re-sampled the audio to 16 kHz sample rate. Table 1 shows different input lengths and the number of Mel bands. Since some models have a smaller number of Mel bands in their original implementations, we further experimented with a larger number of Mel bands for the fair comparison (marked with *). All Mel-spectrogrambased approaches used 512-point FFT with a 50% overlap. For training the models, we used a unified optimization method: a mixture of scheduled ADAM [20] and stochastic gradient descent (SGD), introduced in [7]. The best model is selected based on the validation loss. In the evaluation, chunk-level models average predictions over 16 chunks to perform the final prediction. Note that FCN and CRNN are song-level training models for MTAT and MSD but they perform as chunk-level training models for the MTG-Jamendo dataset since the songs in the dataset are longer than 29.1s which is the size of receptive fields of both models.\n\n\nFully Convolutional Network\n\nA fully convolutional network (FCN) is a variant of CNN that consists of only convolutional layers without any fullyconnected layers. A FCN for music tagging [1] uses Mel spectrogram inputs. In the preprocessing step, a 29.1s audio segment is converted to a 96 \u00d7 1366 Mel spectrogram. It is then used as an input and is passed through 4 convolutional layers. Each convolutional layer uses homogeneous 3 \u00d7 3 2D filters (Figure 1-(a)) followed by a maxpooling layer. Different sizes of strides are used for max-pooling layers ((2, 4), (4,5), (3,8), (4,8)) to increase the size of receptive fields to cover the entire input Mel spectrogram (96 \u00d7 1366). In the original paper, FCN was trained with a song-level training method since the track durations in MTAT correspond to the size of the receptive field. However, this is not the case for MTG-Jamendo containing longer tracks, where chunk-level (29.1s) training is applied.\n\n\nMusicnn\n\nThe Musicnn [2] model also uses Mel spectrograms as its inputs. Different from previously proposed models, the architecture design choices in Musicnn rely on some intuition from the music domain knowledge. The first convolutional layer of Musicnn consists of vertical and horizontal filters. Vertical filters are designed to capture pitchinvariant timbral features (bottom-left of Figure 1-(b)): e.g., 38 \u00d7 7 filter captures sub-band information of short period of time. To enforce the pitch-invariancy, the following max-pooling layer pools the maximum value across the frequency axis. Horizontal filters, on the other hand, capture temporal energy envelope of the audio. After the mean-pooling across the frequency axis of input Mel spectrograms, horizontally long filters (e.g., 1 \u00d7 165) capture the temporal energy patterns (top-right of Figure 1-(b)). The extracted timbral and temporal features are concatenated in the channel, then the following 1D convolutional layers summarize them to predict relevant tags. Different from FCN, the Musicnn only uses short audio excerpts (3s) as its inputs during training, i.e., chunk-level training.\n\n\nSample-level CNN\n\nSample-level CNN [3] tackles the automatic music tagging problem in an end-to-end fashion. It takes raw audio waveforms as its inputs. Sample-level CNN is simpler and deeper than Mel spectrogram-based approaches. It consists of ten 1D convolutional layers with 1 \u00d7 3 filters and 1 \u00d7 3 max-poolings (Figure 1-(c)). Trained front-end filters perform similar to the process of deriving Mel spectrograms and the back-end convolution layers summarize them. We also considered a variation of sample-level CNN [4] with squeeze-and-excitation (SE) [5] blocks. Samplelevel CNN and its variant with SE blocks also use short audio excerpts (3.69s) for the chunk-level training.\n\n\nConvolutional Recurrent Neural Network\n\nConvolutional recurrent neural network (CRNN) [6] uses Mel spectrogram inputs. A CRNN can be described as a combination of CNNs and RNNs. The CNN front end extracts local features and the RNN back end summarizes them temporally. Since RNNs are more flexible than CNNs for summarizing sequential information, it can be beneficial to use RNNs for predicting tags that may be affected by global structures (e.g., moods/themes). Four convolutional layers with 3 \u00d7 3 2D filters are used in the front end and two-layer RNNs with gated recurrent units (GRU) are used in the back end. Long music excerpts (29.1s) are used as inputs of CRNN.\n\n\nSelf-attention\n\nThe self-attention-based music tagging model [7] shares the same intuition as CRNN to extract local features with CNNs and summarize them with sequence models. The only difference is that the self-attention mechanism is used instead of the RNNs for the temporal summarization back end. Motivated by its huge success in natural language processing [8], the authors adapted the Transformer encoder, which is a deep stack of self-attention layers, for automatic music tagging. 15s-long audio excerpts are used for training the self-attention model.\n\n\nHarmonic CNN\n\nHarmonic CNN [9] takes advantage of trainable band-pass filters and harmonically stacked time-frequency representation inputs. Trainable filters (mainly trainable bandwidths) bring more flexibility to the model. And harmonically stacked representation preserves spectro-temporal locality while keeping the harmonic structures through the channel of the input tensor in the first convolution layer (Figure 1-(d)) as introduced in [21]. The number of trainable frequency bands is set to 128 and the number of harmonics considered for stacking is 6. Chunk-level training with 5s audio segments is performed.\n\n\nShort-chunk CNN\n\nAccording to the previous work [9], a simple 2D CNN with 3\u00d73 filters can already claim exceptional results when it is trained with short chunks of audio, i.e., chunk-level training. It is a very prevalent type of CNN (sometimes referred as vgg-like) but, to the best of our knowledge, there are no references for this architecture design in music tagging research. Hence, we implemented a 7-layer CNN with a fully-connected layer and its extension with residual connections [22]. Different from FCN, it uses a smaller size of max-pooling (2 \u00d7 2) because the input segment is way shorter than the song-level inputs (29.1s). We used 128 Mel bins so that 7 max-pooling layers can summarize them into a single dimension (2 7 = 128). It uses 3.69s audio excerpts, hence we call this model \"short-chunk CNN\" in this paper to differentiate it from FCN.\n\n\nRESULTS\n\nWe report ROC-AUC and PR-AUC of all implemented models using three datasets in Table 2. In general, models trained with short audio excerpts (Musicnn, variants of samplelevel CNN, Self-attention, Harmonic CNN, variants of shortchunk CNN) outperform other models trained with relatively longer audio segments (FCN, CRNN). Training with short chunks (instances) is noisier: e.g., an audio excerpt can have a tag guitar if a guitar appears in the song even though the selected excerpt doesn't include guitar sound in it. However, one can expect a much larger number of examples during the training (e.g., 25,877 tracks \u00d7 16 chunks = 414,032 examples). We suspect this brings the performance gain when the model is trained with short chunklevel examples. Furthermore, most of the top 50 tags in the three datasets can be identified only with a short audio excerpt (e.g., instruments, genres). Thus, the model does not need a long sequence of audio to perform its binary classification task. For the top 50 tags in each dataset we experimented with, it is more beneficial to use chunk-level training with short audio excerpts than the song-level training. Short-chunk CNN, short-chunk CNN with residual connections, and Harmonic CNN showed the best results for every dataset. These three models are trained on short audio excerpts (3.69s or 5s) and they use 3 \u00d7 3 convolutional filters followed by 2 \u00d7 2 max-poolings. FCN uses similar filters, but larger max-poolings which increase its size of the receptive field to fit long audio segments (29.1s). We conclude that smaller max-poolings with shorter audio excerpts work better for CNNs with 3 \u00d7 3 filters.\n\nMusicnn shows competitive results in MTAT. However, other models (sample-level + SE, self-attention) outperform Musicnn on larger datasets (MSD and MTG-Jamendo). This confirms an intuition that domain knowledge can be beneficial for relatively small datasets, reported in [2]. However, the design choices for parameters of Musicnn restricts the power of the model when it is trained on larger datasets. For the sequential models, self-attention outperforms the CRNN. Different from self-attention mechanisms, RNNs with long sequence inputs suffer from vanishing gradient problems. Self-attention mechanism alleviates the problems by providing direct paths between all time steps. According to the reported visualizations [7], self-attention performs well for pinpointing relevant short-time acoustic features in the audio sequence, but it was difficult to determine if the model learned long-time characteristics properly. To determine such abilities, some tags related to a global structure have to be cherry-picked and evaluated. Since FCN, Musicnn, and CRNN use Mel spectrogram inputs with 96 Mel bands, there can be relative disadvantages when they are compared with other models using 128 Mel bands. For the fair comparison, we experimented with FCN, Musicnn, and CRNN using 128 Mel bands. A larger number of Mel bands did not show any significant impacts on the performances. Since each architecture design was optimized for a smaller number of Mel bands, simply increasing the size of input Mel bands cannot guarantee the optimized performance of the models.   \n\n\nROBUSTNESS STUDIES\n\n\nInput Deformations\n\nTo further investigate the performance of different state-ofthe-art models, we conducted robustness studies. If a pretrained model has good generalization abilities, the prediction of the model should not be sensitive against small perturbations in the input audio. By applying four different audio deformations to the test set (pitch shift, time stretch, dynamic range compression, and addition of white noise), we intended to determine the generalization abilities of the models. Note that we applied these four deformations only to the test set, which means that the models have never been exposed to the same deformations during training. All employed deformations are based on an existing music data augmentation framework (MUDA) 8 [10]:\n\nPitch shift by n \u2208 {\u22121, 1} semitones.\n\nTime stretch by \u03b3 \u2208 {2 \u22121/2 , 2 1/2 }.\n\nDynamic range compression following speech and music (standard) settings of Dolby E standards [23].\n\nWhite noise addition x mixed = (1 \u2212 \u03b1) \u00b7 x + \u03b1 \u00b7 x noise where \u03b1 \u2208 {0.1, 0.4}. Figure 2 shows performances of each model under various input deformations. Here we tested FCN, Musicnn, sample-level + SE, self-attention, Harmonic CNN, and shortchunk CNN. We followed the original input preprocessing of each model because a larger of Mel bands did not show significant effects in Section 4. CRNN is not included due to its relatively low performance. Dynamic range compression was the least influential and the white noise addition (0.4) was the most critical among the four different perturbations considered. Musicnn is robust against time stretching but it is relatively vulnerable against pitch shift. We suspect the max pooling layer over frequency axis hinders the Musicnn from learning generalized representations. Harmonic CNN and short-chunk CNN were the two best models with original data. However, Harmonic CNN showed better generalization abilities against input deformations except for the white noise addition (0.4). Samplelevel CNN with SE blocks showed good performance with a small amount of noise (0.1), but it could not generalize when this amount was increased (0.4).\n\n\nRobustness results\n\n\nCONCLUSION\n\nIn this paper, we revisit state-of-the-art automatic music tagging models and report their performances with a consistent experimental setup. In general, short-chunk-based approaches showed better results than models trained with larger input segments (FCN, CRNN). The design choices followed by Musicnn could show good performance on a small dataset, but it restricted the model from learning more information on larger datasets. Sequential models (CRNN, self-attention) showed competitive results but could not outperform other models since most of tags in the datasets do not require long sequences for their identification. Interestingly, the best performing model is a simple CNN with 3 \u00d7 3 filters trained on short audio excerpts (short-chunk CNN). Although the original design choice of the CNN is from computer vision, it outperformed other methods except for Harmonic CNN. We further assessed generalization abilities of models by testing perturbed inputs. We could observe a different ranking of the models in terms of their performance on each deformation. This implies that the ROC-AUC and PR-AUC scores are not enough to evaluate music tagging models. In our experiment, Harmonic CNN and short-chunk CNN consistently report better scores than other models. Specifically, Harmonic CNN showed the best generalization abilities against every deformation types except for a heavy white noise addition. We expect the reported results to be a useful reference for further research in automatic music tagging. More detailed exploration of the deformations for robustness tests should be done in the future work. Also, the efficacy of data augmentation in music tagging has to be further investigated.\n\nFigure 1 :\n1Shapes of the first convolution filters and input representations of different models. (a) FCN, CRNN, selfattention, and short-chunk CNN with a Mel spectrogram input (b) Musicnn with a Mel spectrogram input (c) sample-level CNN with a raw audio input (d) Harmonic CNN with a stacked harmonic tensor.\n\nFigure 2 :\n2Evaluations metrics with perturbed audio inputs. Dynamic range compression is shortened as \"drc\" in the plot.\n\nTable 1 :\n1Experimental setups of different models.\n\nTable 2 :\n2Performances of state-of-the-art models.\nhttps://github.com/minzwon/sota-music-tagging-models/ arXiv:2006.00751v1 [eess.AS] 1 Jun 2020\nhttps://github.com/jongpillee/music_dataset_ split/tree/master/MTAT_split 3 https://www.7digital.com 4 https://github.com/jongpillee/music_dataset_ split 5 https://github.com/keunwoochoi/MSD_split_for_ tagging 6 https://jamendo.com 7 https://github.com/MTG/mtg-jamendo-dataset\nhttps://github.com/bmcfee/muda\nAcknowledgmentsThis work was funded by the predoctoral grant MDM-2015-0502-17-2 from the Spanish Ministry of Economy and Competitiveness linked to the Maria de Maeztu Units of Excellence Programme (MDM-2015-0502). Also we acknowledge this research has been supported by Kakao Corp.\nAutomatic tagging using deep convolutional neural networks. K Choi, G Fazekas, M Sandler, Proc. of the 17th International Society for Music Information Retrieval Conference (ISMIR). of the 17th International Society for Music Information Retrieval Conference (ISMIR)K. Choi, G. Fazekas, and M. Sandler, \"Automatic tag- ging using deep convolutional neural networks,\" In Proc. of the 17th International Society for Music In- formation Retrieval Conference (ISMIR), 2016.\n\nEnd-to-end learning for music audio tagging at scale. J Pons, O Nieto, M Prockup, E Schmidt, A Ehmann, X Serra, Proc. of the 19th International Society for Music Information Retrieval Conference (IS-MIR). of the 19th International Society for Music Information Retrieval Conference (IS-MIR)J. Pons, O. Nieto, M. Prockup, E. Schmidt, A. Ehmann, and X. Serra, \"End-to-end learning for music audio tagging at scale,\" In Proc. of the 19th International So- ciety for Music Information Retrieval Conference (IS- MIR), 2018.\n\nSample-level deep convolutional neural networks for music autotagging using raw waveforms. J Lee, SMCJ Park, SMCK L Kim, SMCJ Nam, SMCProc. of the 14th Sound and music computing. of the 14th Sound and music computingJ. Lee, J. Park, K. L. Kim, and J. Nam, \"Sample-level deep convolutional neural networks for music auto- tagging using raw waveforms,\" In Proc. of the 14th Sound and music computing (SMC), 2017.\n\nSample-level cnn architectures for music auto-tagging using raw waveforms. T Kim, J Lee, J Nam, Proc. of International Conference on Acoustics, Speech and Signal Processing. of International Conference on Acoustics, Speech and Signal essingT. Kim, J. Lee, and J. Nam, \"Sample-level cnn ar- chitectures for music auto-tagging using raw wave- forms,\" in Proc. of International Conference on Acous- tics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 366-370.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, Proc. of the IEEE conference on computer vision and pattern recognition (CVPR). of the IEEE conference on computer vision and pattern recognition (CVPR)J. Hu, L. Shen, and G. Sun, \"Squeeze-and-excitation networks,\" in Proc. of the IEEE conference on com- puter vision and pattern recognition (CVPR), 2018, pp. 7132-7141.\n\nConvolutional recurrent neural networks for music classification. K Choi, G Fazekas, M Sandler, K Cho, Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP). of International Conference on Acoustics, Speech and Signal essing (ICASSP)IEEEK. Choi, G. Fazekas, M. Sandler, and K. Cho, \"Convo- lutional recurrent neural networks for music classifica- tion,\" in Proc. of International Conference on Acous- tics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 2392-2396.\n\nToward interpretable music tagging with self-attention. M Won, S Chun, X Serra, arXiv:1906.04972arXiv preprintM. Won, S. Chun, and X. Serra, \"Toward interpretable music tagging with self-attention,\" arXiv preprint arXiv:1906.04972, 2019.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proc. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Long and Short Papers)J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training of deep bidirectional transformers for language understanding,\" Proc. of the 2019 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long and Short Pa- pers), 2018.\n\nDatadriven harmonic filters for audio representation learning. M Won, S Chun, O Nieto, X Serra, Proc. of International Conference on Acoustics, Speech and Signal Processing. of International Conference on Acoustics, Speech and Signal essing2020M. Won, S. Chun, , O. Nieto, and X. Serra, \"Data- driven harmonic filters for audio representation learn- ing,\" In Proc. of International Conference on Acous- tics, Speech and Signal Processing (ICASSP), 2020.\n\nA software framework for musical data augmentation. B Mcfee, E J Humphrey, J P Bello, Proc. of the 16th International Society for Music Information Retrieval Conference (ISMIR). of the 16th International Society for Music Information Retrieval Conference (ISMIR)B. McFee, E. J. Humphrey, and J. P. Bello, \"A software framework for musical data augmentation.\" in Proc. of the 16th International Society for Music Informa- tion Retrieval Conference (ISMIR), vol. 2015, 2015, pp. 248-254.\n\nSolving the multiple instance problem with axisparallel rectangles. T G Dietterich, R H Lathrop, T Lozano-P\u00e9rez, Artificial intelligence. 891-2T. G. Dietterich, R. H. Lathrop, and T. Lozano-P\u00e9rez, \"Solving the multiple instance problem with axis- parallel rectangles,\" Artificial intelligence, vol. 89, no. 1-2, pp. 31-71, 1997.\n\nAdaptive pooling operators for weakly labeled sound event detection. B Mcfee, J Salamon, J P Bello, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 2611B. McFee, J. Salamon, and J. P. Bello, \"Adaptive pool- ing operators for weakly labeled sound event detec- tion,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 11, pp. 2180-2193, 2018.\n\nThe relationship between precision-recall and roc curves. J Davis, M Goadrich, Proc. of the 23rd international conference on Machine learning. of the 23rd international conference on Machine learningJ. Davis and M. Goadrich, \"The relationship between precision-recall and roc curves,\" in Proc. of the 23rd international conference on Machine learning, 2006, pp. 233-240.\n\nEvaluation of algorithms using games: The case of music tagging. E Law, K West, M I Mandel, M Bay, J S Downie, Proc. of International Society for Music Information Retrieval Conference. of International Society for Music Information Retrieval ConferenceE. Law, K. West, M. I. Mandel, M. Bay, and J. S. Downie, \"Evaluation of algorithms using games: The case of music tagging.\" in In Proc. of International So- ciety for Music Information Retrieval Conference (IS- MIR), 2009, pp. 387-392.\n\nThe million song dataset. T Bertin-Mahieux, D P Ellis, B Whitman, P Lamere, Proc. of the 12th International Society for Music Information Retrieval Conference (ISMIR). of the 12th International Society for Music Information Retrieval Conference (ISMIR)T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere, \"The million song dataset,\" In Proc. of the 12th International Society for Music Information Re- trieval Conference (ISMIR), 2011.\n\nThe effects of noisy labels on deep convolutional neural networks for music classification. K Choi, G Fazekas, K Cho, M Sandler, IEEE Transactions on Emerging Topics in Computational Intelligence. K. Choi, G. Fazekas, K. Cho, and M. Sandler, \"The ef- fects of noisy labels on deep convolutional neural net- works for music classification,\" IEEE Transactions on Emerging Topics in Computational Intelligence, 2018.\n\nThe mtg-jamendo dataset for automatic music tagging. D Bogdanov, M Won, P Tovstogan, A Porter, X Serra, Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML). D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and X. Serra, \"The mtg-jamendo dataset for automatic mu- sic tagging,\" Machine Learning for Music Discov- ery Workshop, International Conference on Machine Learning (ICML), 2019.\n\nEssentia: An audio analysis library for music information retrieval. D Bogdanov, N Wack, E Guti\u00e9rrez, S Gulati, H Boyer, O Mayor, G Roma Trepat, J Salamon, J R Zapata Gonz\u00e1lez, X Serra, Proc. of 14th Conference of the International Society for Music Information Retrieval (ISMIR). of 14th Conference of the International Society for Music Information Retrieval (ISMIR)D. Bogdanov, N. Wack, E. G\u00f3mez Guti\u00e9rrez, S. Gu- lati, H. Boyer, O. Mayor, G. Roma Trepat, J. Salamon, J. R. Zapata Gonz\u00e1lez, X. Serra et al., \"Essentia: An audio analysis library for music information retrieval,\" in Proc. of 14th Conference of the International Society for Music Information Retrieval (ISMIR), 2013.\n\nlibrosa: Audio and music signal analysis in python. B Mcfee, C Raffel, D Liang, D P Ellis, M Mcvicar, E Battenberg, O Nieto, Proc. of the 14th python in science conference. of the 14th python in science conference8B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Battenberg, and O. Nieto, \"librosa: Audio and music signal analysis in python,\" in Proc. of the 14th python in science conference, vol. 8, 2015.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, Proc. of the International Conference on Learning Representations (ICLR). of the International Conference on Learning Representations (ICLR)D. P. Kingma and J. Ba, \"Adam: A method for stochas- tic optimization,\" in Proc. of the International Confer- ence on Learning Representations (ICLR), 2015.\n\nDeep salience representations for f0 estimation in polyphonic music. R M Bittner, B Mcfee, J Salamon, P Li, J P Bello, R. M. Bittner, B. McFee, J. Salamon, P. Li, and J. P. Bello, \"Deep salience representations for f0 estimation in polyphonic music.\" in ISMIR, 2017, pp. 63-70.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. of the IEEE conference on computer vision and pattern recognition. of the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. of the IEEE conference on computer vision and pattern recogni- tion, 2016, pp. 770-778.\n\nStandards and practices for authoring dolby digital and dolby e bitstreams. E Dolby, Dolby Labortories, IncE. Dolby, \"Standards and practices for authoring dolby digital and dolby e bitstreams,\" Dolby Labortories, Inc, 2002.\n", "annotations": {"author": "[{\"end\":126,\"start\":58},{\"end\":201,\"start\":127},{\"end\":277,\"start\":202},{\"end\":350,\"start\":278}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":63},{\"end\":141,\"start\":134},{\"end\":217,\"start\":209},{\"end\":290,\"start\":285}]", "author_first_name": "[{\"end\":62,\"start\":58},{\"end\":133,\"start\":127},{\"end\":208,\"start\":202},{\"end\":284,\"start\":278}]", "author_affiliation": "[{\"end\":125,\"start\":68},{\"end\":200,\"start\":143},{\"end\":276,\"start\":219},{\"end\":349,\"start\":292}]", "title": "[{\"end\":55,\"start\":1},{\"end\":405,\"start\":351}]", "venue": null, "abstract": "[{\"end\":1545,\"start\":407}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2589,\"start\":2586},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2703,\"start\":2700},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2958,\"start\":2955},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3094,\"start\":3091},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3163,\"start\":3160},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3286,\"start\":3283},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3433,\"start\":3430},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3474,\"start\":3471},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3576,\"start\":3573},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4367,\"start\":4363},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4535,\"start\":4534},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5570,\"start\":5566},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5992,\"start\":5988},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6822,\"start\":6818},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7146,\"start\":7142},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7918,\"start\":7917},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8328,\"start\":8324},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8546,\"start\":8545},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8956,\"start\":8953},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8958,\"start\":8956},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9638,\"start\":9634},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9664,\"start\":9660},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10436,\"start\":10435},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10810,\"start\":10806},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10827,\"start\":10823},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11390,\"start\":11386},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11447,\"start\":11444},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12058,\"start\":12055},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12433,\"start\":12430},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12435,\"start\":12433},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12440,\"start\":12437},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12442,\"start\":12440},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12447,\"start\":12444},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12449,\"start\":12447},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12846,\"start\":12843},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14016,\"start\":14013},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14502,\"start\":14499},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14539,\"start\":14536},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14754,\"start\":14751},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15404,\"start\":15401},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15706,\"start\":15703},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15934,\"start\":15931},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16351,\"start\":16347},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16576,\"start\":16573},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17020,\"start\":17016},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19328,\"start\":19325},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19777,\"start\":19774},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21406,\"start\":21402},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21586,\"start\":21582}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24829,\"start\":24517},{\"attributes\":{\"id\":\"fig_1\"},\"end\":24952,\"start\":24830},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":25005,\"start\":24953},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":25058,\"start\":25006}]", "paragraph": "[{\"end\":2476,\"start\":1561},{\"end\":3689,\"start\":2478},{\"end\":4901,\"start\":3691},{\"end\":5993,\"start\":4957},{\"end\":6550,\"start\":5995},{\"end\":7108,\"start\":6573},{\"end\":7769,\"start\":7121},{\"end\":8872,\"start\":7771},{\"end\":10298,\"start\":8874},{\"end\":10711,\"start\":10300},{\"end\":11865,\"start\":10722},{\"end\":12819,\"start\":11897},{\"end\":13975,\"start\":12831},{\"end\":14662,\"start\":13996},{\"end\":15337,\"start\":14705},{\"end\":15901,\"start\":15356},{\"end\":16522,\"start\":15918},{\"end\":17387,\"start\":16542},{\"end\":19051,\"start\":17399},{\"end\":20621,\"start\":19053},{\"end\":21407,\"start\":20665},{\"end\":21446,\"start\":21409},{\"end\":21486,\"start\":21448},{\"end\":21587,\"start\":21488},{\"end\":22774,\"start\":21589},{\"end\":24516,\"start\":22810}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":10972,\"start\":10965},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":17485,\"start\":17478}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1559,\"start\":1547},{\"attributes\":{\"n\":\"2.\"},\"end\":4927,\"start\":4904},{\"attributes\":{\"n\":\"2.1\"},\"end\":4955,\"start\":4930},{\"attributes\":{\"n\":\"2.2\"},\"end\":6571,\"start\":6553},{\"attributes\":{\"n\":\"2.3\"},\"end\":7119,\"start\":7111},{\"attributes\":{\"n\":\"3.\"},\"end\":10720,\"start\":10714},{\"attributes\":{\"n\":\"3.1\"},\"end\":11895,\"start\":11868},{\"attributes\":{\"n\":\"3.2\"},\"end\":12829,\"start\":12822},{\"attributes\":{\"n\":\"3.3\"},\"end\":13994,\"start\":13978},{\"attributes\":{\"n\":\"3.4\"},\"end\":14703,\"start\":14665},{\"attributes\":{\"n\":\"3.5\"},\"end\":15354,\"start\":15340},{\"attributes\":{\"n\":\"3.6\"},\"end\":15916,\"start\":15904},{\"attributes\":{\"n\":\"3.7\"},\"end\":16540,\"start\":16525},{\"attributes\":{\"n\":\"4.\"},\"end\":17397,\"start\":17390},{\"attributes\":{\"n\":\"5.\"},\"end\":20642,\"start\":20624},{\"attributes\":{\"n\":\"5.1\"},\"end\":20663,\"start\":20645},{\"attributes\":{\"n\":\"5.2\"},\"end\":22795,\"start\":22777},{\"attributes\":{\"n\":\"6.\"},\"end\":22808,\"start\":22798},{\"end\":24528,\"start\":24518},{\"end\":24841,\"start\":24831},{\"end\":24963,\"start\":24954},{\"end\":25016,\"start\":25007}]", "table": null, "figure_caption": "[{\"end\":24829,\"start\":24530},{\"end\":24952,\"start\":24843},{\"end\":25005,\"start\":24965},{\"end\":25058,\"start\":25018}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12328,\"start\":12315},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13224,\"start\":13212},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13685,\"start\":13673},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14307,\"start\":14294},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16328,\"start\":16315},{\"end\":17718,\"start\":17707},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21676,\"start\":21668},{\"end\":23073,\"start\":23062}]", "bib_author_first_name": "[{\"end\":25804,\"start\":25803},{\"end\":25812,\"start\":25811},{\"end\":25823,\"start\":25822},{\"end\":26269,\"start\":26268},{\"end\":26277,\"start\":26276},{\"end\":26286,\"start\":26285},{\"end\":26297,\"start\":26296},{\"end\":26308,\"start\":26307},{\"end\":26318,\"start\":26317},{\"end\":26826,\"start\":26825},{\"end\":26836,\"start\":26835},{\"end\":26847,\"start\":26846},{\"end\":26849,\"start\":26848},{\"end\":26859,\"start\":26858},{\"end\":27222,\"start\":27221},{\"end\":27229,\"start\":27228},{\"end\":27236,\"start\":27235},{\"end\":27647,\"start\":27646},{\"end\":27653,\"start\":27652},{\"end\":27661,\"start\":27660},{\"end\":28056,\"start\":28055},{\"end\":28064,\"start\":28063},{\"end\":28075,\"start\":28074},{\"end\":28086,\"start\":28085},{\"end\":28552,\"start\":28551},{\"end\":28559,\"start\":28558},{\"end\":28567,\"start\":28566},{\"end\":28817,\"start\":28816},{\"end\":28830,\"start\":28826},{\"end\":28839,\"start\":28838},{\"end\":28846,\"start\":28845},{\"end\":29534,\"start\":29533},{\"end\":29541,\"start\":29540},{\"end\":29549,\"start\":29548},{\"end\":29558,\"start\":29557},{\"end\":29978,\"start\":29977},{\"end\":29987,\"start\":29986},{\"end\":29989,\"start\":29988},{\"end\":30001,\"start\":30000},{\"end\":30003,\"start\":30002},{\"end\":30481,\"start\":30480},{\"end\":30483,\"start\":30482},{\"end\":30497,\"start\":30496},{\"end\":30499,\"start\":30498},{\"end\":30510,\"start\":30509},{\"end\":30812,\"start\":30811},{\"end\":30821,\"start\":30820},{\"end\":30832,\"start\":30831},{\"end\":30834,\"start\":30833},{\"end\":31188,\"start\":31187},{\"end\":31197,\"start\":31196},{\"end\":31567,\"start\":31566},{\"end\":31574,\"start\":31573},{\"end\":31582,\"start\":31581},{\"end\":31584,\"start\":31583},{\"end\":31594,\"start\":31593},{\"end\":31601,\"start\":31600},{\"end\":31603,\"start\":31602},{\"end\":32018,\"start\":32017},{\"end\":32036,\"start\":32035},{\"end\":32038,\"start\":32037},{\"end\":32047,\"start\":32046},{\"end\":32058,\"start\":32057},{\"end\":32527,\"start\":32526},{\"end\":32535,\"start\":32534},{\"end\":32546,\"start\":32545},{\"end\":32553,\"start\":32552},{\"end\":32903,\"start\":32902},{\"end\":32915,\"start\":32914},{\"end\":32922,\"start\":32921},{\"end\":32935,\"start\":32934},{\"end\":32945,\"start\":32944},{\"end\":33349,\"start\":33348},{\"end\":33361,\"start\":33360},{\"end\":33369,\"start\":33368},{\"end\":33382,\"start\":33381},{\"end\":33392,\"start\":33391},{\"end\":33401,\"start\":33400},{\"end\":33410,\"start\":33409},{\"end\":33415,\"start\":33411},{\"end\":33425,\"start\":33424},{\"end\":33436,\"start\":33435},{\"end\":33438,\"start\":33437},{\"end\":33457,\"start\":33456},{\"end\":34019,\"start\":34018},{\"end\":34028,\"start\":34027},{\"end\":34038,\"start\":34037},{\"end\":34047,\"start\":34046},{\"end\":34049,\"start\":34048},{\"end\":34058,\"start\":34057},{\"end\":34069,\"start\":34068},{\"end\":34083,\"start\":34082},{\"end\":34430,\"start\":34429},{\"end\":34432,\"start\":34431},{\"end\":34442,\"start\":34441},{\"end\":34815,\"start\":34814},{\"end\":34817,\"start\":34816},{\"end\":34828,\"start\":34827},{\"end\":34837,\"start\":34836},{\"end\":34848,\"start\":34847},{\"end\":34854,\"start\":34853},{\"end\":34856,\"start\":34855},{\"end\":35071,\"start\":35070},{\"end\":35077,\"start\":35076},{\"end\":35086,\"start\":35085},{\"end\":35093,\"start\":35092},{\"end\":35497,\"start\":35496}]", "bib_author_last_name": "[{\"end\":25809,\"start\":25805},{\"end\":25820,\"start\":25813},{\"end\":25831,\"start\":25824},{\"end\":26274,\"start\":26270},{\"end\":26283,\"start\":26278},{\"end\":26294,\"start\":26287},{\"end\":26305,\"start\":26298},{\"end\":26315,\"start\":26309},{\"end\":26324,\"start\":26319},{\"end\":26830,\"start\":26827},{\"end\":26841,\"start\":26837},{\"end\":26853,\"start\":26850},{\"end\":26863,\"start\":26860},{\"end\":27226,\"start\":27223},{\"end\":27233,\"start\":27230},{\"end\":27240,\"start\":27237},{\"end\":27650,\"start\":27648},{\"end\":27658,\"start\":27654},{\"end\":27665,\"start\":27662},{\"end\":28061,\"start\":28057},{\"end\":28072,\"start\":28065},{\"end\":28083,\"start\":28076},{\"end\":28090,\"start\":28087},{\"end\":28556,\"start\":28553},{\"end\":28564,\"start\":28560},{\"end\":28573,\"start\":28568},{\"end\":28824,\"start\":28818},{\"end\":28836,\"start\":28831},{\"end\":28843,\"start\":28840},{\"end\":28856,\"start\":28847},{\"end\":29538,\"start\":29535},{\"end\":29546,\"start\":29542},{\"end\":29555,\"start\":29550},{\"end\":29564,\"start\":29559},{\"end\":29984,\"start\":29979},{\"end\":29998,\"start\":29990},{\"end\":30009,\"start\":30004},{\"end\":30494,\"start\":30484},{\"end\":30507,\"start\":30500},{\"end\":30523,\"start\":30511},{\"end\":30818,\"start\":30813},{\"end\":30829,\"start\":30822},{\"end\":30840,\"start\":30835},{\"end\":31194,\"start\":31189},{\"end\":31206,\"start\":31198},{\"end\":31571,\"start\":31568},{\"end\":31579,\"start\":31575},{\"end\":31591,\"start\":31585},{\"end\":31598,\"start\":31595},{\"end\":31610,\"start\":31604},{\"end\":32033,\"start\":32019},{\"end\":32044,\"start\":32039},{\"end\":32055,\"start\":32048},{\"end\":32065,\"start\":32059},{\"end\":32532,\"start\":32528},{\"end\":32543,\"start\":32536},{\"end\":32550,\"start\":32547},{\"end\":32561,\"start\":32554},{\"end\":32912,\"start\":32904},{\"end\":32919,\"start\":32916},{\"end\":32932,\"start\":32923},{\"end\":32942,\"start\":32936},{\"end\":32951,\"start\":32946},{\"end\":33358,\"start\":33350},{\"end\":33366,\"start\":33362},{\"end\":33379,\"start\":33370},{\"end\":33389,\"start\":33383},{\"end\":33398,\"start\":33393},{\"end\":33407,\"start\":33402},{\"end\":33422,\"start\":33416},{\"end\":33433,\"start\":33426},{\"end\":33454,\"start\":33439},{\"end\":33463,\"start\":33458},{\"end\":34025,\"start\":34020},{\"end\":34035,\"start\":34029},{\"end\":34044,\"start\":34039},{\"end\":34055,\"start\":34050},{\"end\":34066,\"start\":34059},{\"end\":34080,\"start\":34070},{\"end\":34089,\"start\":34084},{\"end\":34439,\"start\":34433},{\"end\":34445,\"start\":34443},{\"end\":34825,\"start\":34818},{\"end\":34834,\"start\":34829},{\"end\":34845,\"start\":34838},{\"end\":34851,\"start\":34849},{\"end\":34862,\"start\":34857},{\"end\":35074,\"start\":35072},{\"end\":35083,\"start\":35078},{\"end\":35090,\"start\":35087},{\"end\":35097,\"start\":35094},{\"end\":35503,\"start\":35498}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":582314},\"end\":26212,\"start\":25743},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":22756561},\"end\":26732,\"start\":26214},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17589207},\"end\":27144,\"start\":26734},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1906290},\"end\":27611,\"start\":27146},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":140309863},\"end\":27987,\"start\":27613},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":215826700},\"end\":28493,\"start\":27989},{\"attributes\":{\"doi\":\"arXiv:1906.04972\",\"id\":\"b6\"},\"end\":28732,\"start\":28495},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":29468,\"start\":28734},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":216353219},\"end\":29923,\"start\":29470},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":852445},\"end\":30410,\"start\":29925},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7398727},\"end\":30740,\"start\":30412},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13750090},\"end\":31127,\"start\":30742},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":207159665},\"end\":31499,\"start\":31129},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9788545},\"end\":31989,\"start\":31501},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9770532},\"end\":32432,\"start\":31991},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":196065399},\"end\":32847,\"start\":32434},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":196187495},\"end\":33277,\"start\":32849},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11200511},\"end\":33964,\"start\":33279},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":33504},\"end\":34383,\"start\":33966},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6628106},\"end\":34743,\"start\":34385},{\"attributes\":{\"id\":\"b20\"},\"end\":35022,\"start\":34745},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206594692},\"end\":35418,\"start\":35024},{\"attributes\":{\"id\":\"b22\"},\"end\":35644,\"start\":35420}]", "bib_title": "[{\"end\":25801,\"start\":25743},{\"end\":26266,\"start\":26214},{\"end\":26823,\"start\":26734},{\"end\":27219,\"start\":27146},{\"end\":27644,\"start\":27613},{\"end\":28053,\"start\":27989},{\"end\":28814,\"start\":28734},{\"end\":29531,\"start\":29470},{\"end\":29975,\"start\":29925},{\"end\":30478,\"start\":30412},{\"end\":30809,\"start\":30742},{\"end\":31185,\"start\":31129},{\"end\":31564,\"start\":31501},{\"end\":32015,\"start\":31991},{\"end\":32524,\"start\":32434},{\"end\":32900,\"start\":32849},{\"end\":33346,\"start\":33279},{\"end\":34016,\"start\":33966},{\"end\":34427,\"start\":34385},{\"end\":35068,\"start\":35024}]", "bib_author": "[{\"end\":25811,\"start\":25803},{\"end\":25822,\"start\":25811},{\"end\":25833,\"start\":25822},{\"end\":26276,\"start\":26268},{\"end\":26285,\"start\":26276},{\"end\":26296,\"start\":26285},{\"end\":26307,\"start\":26296},{\"end\":26317,\"start\":26307},{\"end\":26326,\"start\":26317},{\"end\":26835,\"start\":26825},{\"end\":26846,\"start\":26835},{\"end\":26858,\"start\":26846},{\"end\":26868,\"start\":26858},{\"end\":27228,\"start\":27221},{\"end\":27235,\"start\":27228},{\"end\":27242,\"start\":27235},{\"end\":27652,\"start\":27646},{\"end\":27660,\"start\":27652},{\"end\":27667,\"start\":27660},{\"end\":28063,\"start\":28055},{\"end\":28074,\"start\":28063},{\"end\":28085,\"start\":28074},{\"end\":28092,\"start\":28085},{\"end\":28558,\"start\":28551},{\"end\":28566,\"start\":28558},{\"end\":28575,\"start\":28566},{\"end\":28826,\"start\":28816},{\"end\":28838,\"start\":28826},{\"end\":28845,\"start\":28838},{\"end\":28858,\"start\":28845},{\"end\":29540,\"start\":29533},{\"end\":29548,\"start\":29540},{\"end\":29557,\"start\":29548},{\"end\":29566,\"start\":29557},{\"end\":29986,\"start\":29977},{\"end\":30000,\"start\":29986},{\"end\":30011,\"start\":30000},{\"end\":30496,\"start\":30480},{\"end\":30509,\"start\":30496},{\"end\":30525,\"start\":30509},{\"end\":30820,\"start\":30811},{\"end\":30831,\"start\":30820},{\"end\":30842,\"start\":30831},{\"end\":31196,\"start\":31187},{\"end\":31208,\"start\":31196},{\"end\":31573,\"start\":31566},{\"end\":31581,\"start\":31573},{\"end\":31593,\"start\":31581},{\"end\":31600,\"start\":31593},{\"end\":31612,\"start\":31600},{\"end\":32035,\"start\":32017},{\"end\":32046,\"start\":32035},{\"end\":32057,\"start\":32046},{\"end\":32067,\"start\":32057},{\"end\":32534,\"start\":32526},{\"end\":32545,\"start\":32534},{\"end\":32552,\"start\":32545},{\"end\":32563,\"start\":32552},{\"end\":32914,\"start\":32902},{\"end\":32921,\"start\":32914},{\"end\":32934,\"start\":32921},{\"end\":32944,\"start\":32934},{\"end\":32953,\"start\":32944},{\"end\":33360,\"start\":33348},{\"end\":33368,\"start\":33360},{\"end\":33381,\"start\":33368},{\"end\":33391,\"start\":33381},{\"end\":33400,\"start\":33391},{\"end\":33409,\"start\":33400},{\"end\":33424,\"start\":33409},{\"end\":33435,\"start\":33424},{\"end\":33456,\"start\":33435},{\"end\":33465,\"start\":33456},{\"end\":34027,\"start\":34018},{\"end\":34037,\"start\":34027},{\"end\":34046,\"start\":34037},{\"end\":34057,\"start\":34046},{\"end\":34068,\"start\":34057},{\"end\":34082,\"start\":34068},{\"end\":34091,\"start\":34082},{\"end\":34441,\"start\":34429},{\"end\":34447,\"start\":34441},{\"end\":34827,\"start\":34814},{\"end\":34836,\"start\":34827},{\"end\":34847,\"start\":34836},{\"end\":34853,\"start\":34847},{\"end\":34864,\"start\":34853},{\"end\":35076,\"start\":35070},{\"end\":35085,\"start\":35076},{\"end\":35092,\"start\":35085},{\"end\":35099,\"start\":35092},{\"end\":35505,\"start\":35496}]", "bib_venue": "[{\"end\":26009,\"start\":25925},{\"end\":26504,\"start\":26419},{\"end\":26950,\"start\":26913},{\"end\":27386,\"start\":27320},{\"end\":27819,\"start\":27747},{\"end\":28254,\"start\":28179},{\"end\":29126,\"start\":28996},{\"end\":29710,\"start\":29644},{\"end\":30187,\"start\":30103},{\"end\":31328,\"start\":31272},{\"end\":31754,\"start\":31687},{\"end\":32243,\"start\":32159},{\"end\":33647,\"start\":33560},{\"end\":34179,\"start\":34139},{\"end\":34587,\"start\":34521},{\"end\":35237,\"start\":35172},{\"end\":25923,\"start\":25833},{\"end\":26417,\"start\":26326},{\"end\":26911,\"start\":26868},{\"end\":27318,\"start\":27242},{\"end\":27745,\"start\":27667},{\"end\":28177,\"start\":28092},{\"end\":28549,\"start\":28495},{\"end\":28994,\"start\":28858},{\"end\":29642,\"start\":29566},{\"end\":30101,\"start\":30011},{\"end\":30548,\"start\":30525},{\"end\":30905,\"start\":30842},{\"end\":31270,\"start\":31208},{\"end\":31685,\"start\":31612},{\"end\":32157,\"start\":32067},{\"end\":32629,\"start\":32563},{\"end\":33051,\"start\":32953},{\"end\":33558,\"start\":33465},{\"end\":34137,\"start\":34091},{\"end\":34519,\"start\":34447},{\"end\":34812,\"start\":34745},{\"end\":35170,\"start\":35099},{\"end\":35494,\"start\":35420}]"}}}, "year": 2023, "month": 12, "day": 17}