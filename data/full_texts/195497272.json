{"id": 195497272, "updated": "2022-11-03 17:33:05.875", "metadata": {"title": "Semi-Supervised Learning With Graph Learning-Convolutional Networks", "authors": "[{\"first\":\"Bo\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Ziyan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Doudou\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Jin\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Luo\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Graph Convolutional Neural Networks (graph CNNs) have been widely used for graph data representation and semi-supervised learning tasks. However, existing graph CNNs generally use a fixed graph which may not be optimal for semi-supervised learning tasks. In this paper, we propose a novel Graph Learning-Convolutional Network (GLCN) for graph data representation and semi-supervised learning. The aim of GLCN is to learn an optimal graph structure that best serves graph CNNs for semi-supervised learning by integrating both graph learning and graph convolution in a unified network architecture. The main advantage is that in GLCN both given labels and the estimated labels are incorporated and thus can provide useful \u2018weakly\u2019 supervised information to refine (or learn) the graph construction and also to facilitate the graph convolution operation for unknown label estimation. Experimental results on seven benchmarks demonstrate that GLCN significantly outperforms the state-of-the-art traditional fixed structure based graph CNNs.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2948729509", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/JiangZLTL19", "doi": "10.1109/cvpr.2019.01157"}}, "content": {"source": {"pdf_hash": "1ba9339a239c6b545b6916dc66986b3ef78c9e99", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "42de759606749b49cd2b0322cdac0c0462205307", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1ba9339a239c6b545b6916dc66986b3ef78c9e99.txt", "contents": "\nSemi-supervised Learning with Graph Learning-Convolutional Networks\n\n\nBo Jiang jiangbo@ahu.edu.cn \nSchool of Computer Science and Technology\nAnhui University\n230601HefeiChina\n\nZiyan Zhang \nSchool of Computer Science and Technology\nAnhui University\n230601HefeiChina\n\nDoudou Lin \nSchool of Computer Science and Technology\nAnhui University\n230601HefeiChina\n\nJin Tang \nSchool of Computer Science and Technology\nAnhui University\n230601HefeiChina\n\nBin Luo luobin@ahu.edu.cn \nSchool of Computer Science and Technology\nAnhui University\n230601HefeiChina\n\nSemi-supervised Learning with Graph Learning-Convolutional Networks\n10.1109/CVPR.2019.01157\nGraph Convolutional Neural Networks (graph CNNs) have been widely used for graph data representation and semi-supervised learning tasks. However, existing graph CNNs generally use a fixed graph which may not be optimal for semi-supervised learning tasks. In this paper, we propose a novel Graph Learning-Convolutional Network (GLCN) for graph data representation and semi-supervised learning. The aim of GLCN is to learn an optimal graph structure that best serves graph CNNs for semi-supervised learning by integrating both graph learning and graph convolution in a unified network architecture. The main advantage is that in GLCN both given labels and the estimated labels are incorporated and thus can provide useful 'weakly' supervised information to refine (or learn) the graph construction and also to facilitate the graph convolution operation for unknown label estimation. Experimental results on seven benchmarks demonstrate that GLCN significantly outperforms the state-of-the-art traditional fixed structure based graph CNNs.\n\nIntroduction\n\nDeep neural networks have been widely used in many computer vision and pattern recognition tasks. Recently, many methods have been proposed to generalize the convolution operation on arbitrary graphs to address graph structure data [5,1,15,11,19,21]. Overall, these methods can be categorized into spatial convolution and spectral convolution methods [22]. For spatial methods, they generally define graph convolution operation directly by defining an operation on node groups of neighbors. For example, Duvenaud et al. [5] propose a convolutional neural network that operates directly on graphs and provide an end-to-end feature learning for graph data. Atwood and Towsley [1] propose Diffusion-Convolutional Neural Networks (DCNNs) by employing a graph diffusion process to incorporate the contextual information of node in graph node classification. * Corresponding author Monti et al. [15] present mixture model CNNs (MoNet) and provide a unified generalization of CNN architectures on graphs. By designing an attention layer, Veli\u010dkovi\u0107 et al. [21] present Graph Attention Networks (GAT) for semisupervised learning. For spectral methods, they generally define graph convolution operation based on spectral representation of graphs. For example, Bruna et al. [3] propose to define graph convolution in the Fourier domain based on eigen-decomposition of graph Laplacian matrix. Defferrard et al. [4] propose to approximate the spectral filters based on Chebyshev expansion of graph Laplacian to avoid the high computational complexity of eigen-decomposition. Kipf et al. [11] propose a more simple Graph Convolutional Network (GCN) for semi-supervised learning.\n\nThe above graph CNNs have been widely used for supervised or semi-supervised learning tasks. In this paper, we focus on semi-supervised learning. One important aspect of graph CNNs is the graph structure representation of data. In general, the data we provide to graph CNNs either has a known intrinsic graph structure, such as social networks, or we construct a human established graph for it, such as k-nearest neighbor graph with Gaussian kernel. However, it is difficult to evaluate whether the graphs obtained from domain knowledge (e.g., social network) or established by human are optimal for semi-supervised learning in graph C-NNs. Henaff et al. [7] propose to learn a supervised graph with a fully connected network. However, the learned graph is obtained from a separate network which is also not guaranteed to best serve the graph CNNs. Li et al. [19] propose optimal graph CNNs, in which the graph is learned adaptively by using a distance metric learning. However, it use an approximate algorithm to estimate graph Laplacian which may lead to weak local optimal solution.\n\nIn this paper, we propose a novel Graph Learning-Convolutional Network (GLCN) for semi-supervised learning problem. The main idea of GLCN is to learn an optimal graph representation that best serves graph CNNs for semisupervised learning by integrating both graph learning and graph convolution simultaneously in a unified network architecture. The main advantages of the proposed GLCN for semi-supervised learning are summarized as follows. \u2022 In GLCN, both given labels and the estimated labels are incorporated and thus can provide useful 'weakly' supervised information to refine (or learn) the graph construction and to facilitate the graph convolution operation in graph CNN for unknown label estimation.\n\n\u2022 GLCN can be trained via a single optimization manner, which can thus be implemented simply.\n\nTo the best of our knowledge, this is the first attempt to build a unified graph learning-convolutional network architecture for semi-supervised learning. Experimental results demonstrate that GLCN outperforms state-of-the-art graph CNNs on semi-supervised learning tasks.\n\n\nRelated Work\n\nHere, we briefly review GCN based semi-supervised learning proposed in work [11]. Let X = (x 1 , x 2 , \u00b7 \u00b7 \u00b7 x n ) \u2208 R n\u00d7p be the collection of n data vectors in p dimension. Let G(X, A) be the graph representation of X with A \u2208 R n\u00d7n encoding the pairwise relationship (such as similarities, neighbors) among data X. GCN contains one input layer, several propagation (hidden) layers and one final perceptron layer [11]. Given an input X (0) = X and graph A, GCN [11] conducts the following layer-wise propagation in hidden layers as,\nX (k+1) = \u03c3(D \u22121/2 AD \u22121/2 X (k) W (k) )(1)\nwhere k = 0, 1, \u00b7 \u00b7 \u00b7 K \u2212 1 and\nD = diag(d 1 , d 2 \u00b7 \u00b7 \u00b7 d n ) is a diagonal matrix with d i = n j=1 A ij . W (k) \u2208 R d k \u00d7d k+1 , d 0 = p\nis a layer-specific weight matrix needing to be trained. \u03c3(\u00b7) denotes an activation function, such as ReLU(\u00b7) = max(0, \u00b7), and X (k+1) \u2208 R n\u00d7d k+1 denotes the output of activations in the k-th layer. For semi-supervised classification, GCN [11] defines the final perceptron layer as\nZ = softmax(D \u22121/2 AD \u22121/2 X (K) W (K) )(2)\nwhere W (K) \u2208 R d K \u00d7c and c denotes the number of classes. The final output Z \u2208 R n\u00d7c denotes the label prediction for all data X in which each row Z i denotes the label prediction for the i-th node. The optimal weight matrices {W (0) , W (1) , \u00b7 \u00b7 \u00b7 W (K) } are trained by minimizing the cross-entropy loss function as,\nL Semi-GCN = \u2212 i\u2208L c j=1 Y ij lnZ ij(3)\nwhere L indicates the set of labeled nodes.\n\n\nGraph Learning-Convolutional Network\n\nOne core aspect of GCN is the graph representation G(X, A) of data X. In some applications, the graph structure of data are available from domain knowledge, such as chemical molecules, social networks etc. In this case, one can use the existing graph directly for GCN based semi-supervised learning. In many other applications, the graph data are not available. One popular way is to construct a human established graph (e.g., k-nearest neighbor graph) [8] for GCN. However, the graphs obtained from domain knowledge or estimated by human are generally independent of GCN (semi-supervised) learning process and thus are not guaranteed to best serve GCN learning. Also, the human established graphs are usually sensitive to the local noise and outliers. To overcome these problems, we propose a novel Graph Learning-Convolution Network (GLCN) which integrates graph learning and graph convolution simultaneously in a unified network architecture and thus can learn an adaptive (or optimal) graph representation for GCN learning. In particular, as shown in Figure 1, GLCN contains one graph learning layer, several convolution layers and one final perceptron layer. In the following, we explain them in detail. \n\n\nGraph learning architecture\n\nGiven an input X = (x 1 , x 2 \u00b7 \u00b7 \u00b7 x n ) \u2208 R n\u00d7p , we aim to seek a nonnegative function S ij = g(x i , x j ) that represents the pairwise relationship between data x i and x j . We implement g(x i , x j ) via a single-layer neural network, which is parameterized by a weight vector a = (a 1 , a 2 , \u00b7 \u00b7 \u00b7 a p ) T \u2208 R p\u00d71 . Formally, we learn a graph S as\nS ij = g(x i , x j ) = exp(ReLU(a T |x i \u2212 x j |)) n j=1 exp(ReLU(a T |x i \u2212 x j |))(4)\nwhere ReLU(\u00b7) = max(0, \u00b7) is an activation function, which guarantees the nonnegativity of S ij . The role of the above softmax operation on each row of S is to guarantee that the learned graph S can satisfy the following property,\nn j=1 S ij = 1, S ij \u2265 0(5)\nWe optimize the optimal weight vector a by minimizing the following loss function,\nL GL = n i,j=1 x i \u2212 x j 2 2 S ij + \u03b3 S 2 F(6)\nThat is, larger distance x i \u2212 x j 2 between data point x i and x j encourages a smaller value S ij . The second term is used to control the sparsity of learned graph S because of simplex property of S (Eq. (5)), as discussed in [17].\n\nRemark. Minimizing the above loss L GL independently may lead to trivial solution, i.e., a = (0, 0 \u00b7 \u00b7 \u00b7 0). We use it as a regularized term in our final loss function, as shown in Eq.(15) in \u00a73.2. For some problems, when an initial graph A is available, we can incorporate it in our graph learning as\nS ij = g(x i , x j ) = A ij exp(ReLU(a T |x i \u2212 x j |)) n j=1 A ij exp(ReLU(a T |x i \u2212 x j |))(7)\nWe can also incorporate the information of A by considering a regularized term in the learning loss function as\nL GL = n i,j=1 x i \u2212 x j 2 2 S ij + \u03b3 S 2 F + \u03b2 S \u2212 A 2 F(8)\nOn the other hand, when the dimension p of the input data X is large, the above computation of g(x i , x j ) may be less effective due to the long weight vector a needing to be trained. Also, the computation of Euclidean distances x i \u2212 x j 2 between data pairs in loss function L GL is complex for large dimension p. To solve this problem, we propose to conduct our graph learning in a lowdimensional subspace. We implement this via a single-layer low-dimensional embedding network, parameterized by a projection matrix P \u2208 R p\u00d7d , d < p. In particular, we conduct our final graph learning as follows,\nx i = x i P, for i = 1, 2 \u00b7 \u00b7 \u00b7 n (9) S ij = g(x i ,x j ) = A ij exp(ReLU(a T |x i \u2212x j |)) n j=1 A ij exp(ReLU(a T |x i \u2212x j |))(10)\nwhere A denotes an initial graph. If it is unavailable, we can set A ij = 1 in the above update rule. The loss function becomes\nL GL = n i,j=1 x i \u2212x j 2 2 S ij + \u03b3 S 2 F(11)\nThe whole architecture of the proposed graph learning network is shown in Figure 2.\n\nRemark. The proposed learned graph S has a desired probability property (Eq.(5)), i.e., the optimal S ij can be regarded a probability that data x j is connected to x i as a neighboring node. That is, the proposed graph learning (GL) architecture can establish the neighborhood structure of data automatically either based on data feature X only or by further incorporating the prior initial graph A with X. The GL architecture indeed provides a kind of nonlinear function S = G GL (X, A; P, a) to predict/compute the neighborhood probabilities between node pairs.\n\n\nGLCN architecture\n\nThe proposed graph learning architecture is general and can be incorporated in any graph CNNs. In this paper, we incorporate it into GCN [11] and propose a unified Graph Learning-Convolutional Network (GLCN) for semisupervised learning problem. Figure 1 shows the overview of GLCN architecture. The aim of GLCN is to learn an optimal graph representation for GCN network and integrate graph learning and convolution simultaneously to boost their respectively performance.\n\nAs shown in Figure 1, GLCN contains one graph learning layer, several graph convolution layers and one final perceptron layer. The graph learning layer aims to provide an optimal adaptive graph representation S for graph convolutional layers. That is, in the convolutional layers, it conducts the layer-wise propagation rule based on the adaptive neighbor graph S returned by graph learning layer, i.e.,\nX (k+1) = \u03c3(D \u22121/2 s SD \u22121/2 s X (k) W (k) )(12)\nwhere\nk = 0, 1 \u00b7 \u00b7 \u00b7 K \u2212 1. D s = diag(d 1 , d 2 , \u00b7 \u00b7 \u00b7 d n ) is a diagonal matrix with diagonal element d i = n j=1 S ij . W (k) \u2208 R d k \u00d7d k+1\nis a layer-specific trainable weight matrix for each convolution layer. \u03c3(\u00b7) denotes an activation function, such as ReLU(\u00b7) = max(0, \u00b7), and X (k+1) \u2208 R n\u00d7d k+1 denotes the output of activations in the k-th layer. Since the learned graph S satisfies j S ij = 1, S ij \u2265 0, thus Eq.(12) can be simplified as\nX (k+1) = \u03c3(SX (k) W (k) )(13)\nFor semi-supervised classification task, we define the final perceptron layer as\nZ = softmax(SX (K) W (K) )(14)\nwhere W (K) \u2208 R d K \u00d7c and c denotes the number of classes. The final output Z \u2208 R n\u00d7c denotes the label prediction of GLCN network, in which each row Z i denotes the label prediction for the i-th node. The whole network parameters \u0398 = {P, a, W (0) , \u00b7 \u00b7 \u00b7 W (K) } are jointly trained by minimizing the following loss function as\nL Semi-GLCN = L Semi-GCN + \u03bbL GL(15)\nwhere L GL and L Semi-GCN are defined in Eq.(11) and Eq. (3), respectively. Parameter \u03bb \u2265 0 is a tradeoff parameter. It is noted that, when \u03bb = 0, the optimal graph S is learned based on labeled data (i.e., cross-entropy loss) only which is also feasible in our GLCN. Demonstration and analysis. There are two main benefits of the proposed GLCN network:\n\n\u2022 In GLCN, both given labels Y and the estimated labels Z are incorporated and thus can provide useful 'weakly' supervised information to refine the graph construction S and thus to facilitate the graph convolution operation in GCN for unknown label estimation. That is, the graph learning and semi-supervised learning are conducted jointly in GLCN and thus can boost their respectively performance.\n\n\u2022 GLCN is a unified network which can be trained via a single optimization manner and thus can be implemented simply. Figure 3 shows the cross-entropy loss values over labeled node L across different epochs. One can note that, GLCN obtains obviously lower cross-entropy value than GCN at convergence, which clearly demonstrates the higher predictive accuracy of GLCN model. Also, the convergence speed of GLCN is just slightly slower than GCN, indicating the efficiency of GLCN. Figure 4 demonstrates 2D t-SNE [14] visualizations of the feature map output by the first convolutional layer of GCN [11] and GLCN, respectively. Different classes are marked by different colors. One can note that, the data of different classes are distributed more clearly and compactly in our GLCN representation, which demonstrates the desired discriminative ability of GLCN on conducting graph node representation and thus semi-supervised classification tasks. \n\n\nExperiments\n\n\nDatasets\n\nTo verify the effectiveness and benefit of the proposed GLCN on semi-supervised learning tasks, we test it on seven benchmark datasets, including three standard citation network benchmark datasets (Citeseer, Cora and Pubmed [20]) and four image datasets (CIFAR10 [12], SVHN [16], MNIST and Scene 15 [9]). The details of these datasets and their usages in our experiments are introduced below. Citeseer. This network data contains 3327 nodes and 4732 edges. The nodes are falling into six classes and each node is represented by a 3703 dimension feature descriptor. Cora. This data contains 2708 nodes and 5429 edges. Each node has a 1433 dimension feature descriptor and all the nodes are falling into six classes.\n\nPubmed. This data contains 19717 nodes and 44338 edges.  . 2D t-SNE [14] visualizations of the feature map output by the first convolutional layer of GCN [11] and GLCN respectively on Scene15 dataset. Different classes are marked by different colors. One can note that, the data of different classes are distributed more clearly and compactly in our GLCN convolutional layer feature representation.\n\nEach node is represented by a 500 dimension feature descriptor and all the nodes are classified into 3 classes.\n\nCIFAR10. This dataset contains 50000 natural images which are falling into 10 classes [12]. Each image in this dataset is a 32 \u00d7 32 RGB color image. In our experiments, we select 1000 images for each class and use 10000 images in all for our evaluation. For each image, we extract a CNN feature descriptor for it. SVHN. It contains 73257 training and 26032 test images [16]. Each image is a 32 \u00d7 32 RGB image which contains multiple number of digits and the task is to recognize the digit in the image center. Similar to CIFAR 10 dataset, in our experiments, we select 1000 images for each class and obtain 10000 images in all for our evaluation. For each image, we extract a CNN feature descriptor for it. MNIST. 1 This dataset consists of images of hand-written digits from '0' to '9'. We randomly select 1000 images from each digit class and obtain 10000 images in all for our evaluation. Similar to other related works, we use gray value directly and convert it to a 784 dimension vector. Scene15. It consists of 4485 scene images with 15 different categories [13,9]. For each image, we use the feature descriptor provided by work [9].\n\n\nExperimental setting\n\nFor Cora, Citeseer and Pubmed datasets, we follow the experimental setup of previous works [11,21]. For image dataset CIFAR10, SVHN and MNIST, we randomly select 1000, 2000 and 3000 images as labeled samples and the remaining data are used as unlabeled samples. For unlabeled samples, we select 1000 images for validation purpose and 1 http://yann.lecun.com/exdb/mnist/ \n\n\nMethond\n\nCiteseer Cora Pubmed ManiReg [2] 60.1% 59.5% 70.7% LP [23] 45.3% 68.0% 63.0% DeepWalk [18] 43.2% 67.2% 65.3% GCN [11] 70.9% 82.9% 77.9% GAT [21] 71.0% 83.2% 78.0% GLCN 72.0% 85.5% 78.3%\n\nuse the remaining 8000, 7000 and 6000 images as test samples, respectively. All the reported results are averaged over ten runs with different splits of training, validation and testing data. For image dataset Scene15 [9], we randomly select 500, 750 and 1000 images as label data and use 500 images for validation, respectively. The remaining samples are used as the unlabeled test samples. The reported results are averaged over ten runs with different data splits. Similar to [11], in our experiments we set the number of convolution layers in our GLCN to 2. The number of units in graph learning layer is set to 70 and it is set to 30 in graph convolution hidden layer. We train GLCN for a maximum of 5000 epochs (training iterations) using an ADAM algorithm [10] with learning rate 0.005, and stop training if the validation loss does not decrease for 100 consecutive epochs, as suggested in work [11]. All the network weights \u0398 are initialized using Glorot initialization [6].\n\n\nComparison with state-of-the-art methods\n\nBaselines. We first compare our GLCN model with baseline GCN [11] which is the most related model with our GLCN. We also compare our method against some oth- er graph neural network based semi-supervised learning approaches which contain i) two traditional graph based semisupervised learning methods including Label Propagation (LP) [23], Manifold Regularization (ManiReg) [2], and ii) three graph neural network methods including DeepWalk [18], Graph Convolutional Network (GCN) [11] and Graph Attention Networks (GAT) [21]. The codes of GCN and GAT have been provided by authors and we use them in our experiments. For fair comparison, the parameter settings of GCN are the same as our GLCN to obtain the average better performance on all datasets. Results. Table 1 summarizes the comparison results on three citation network benchmark datasets (Citeseer, Cora and Pubmed [20]) 2 . Table 2 summarizes the comparison results on four widely used image datasets. The best results are marked as bold in Table 1 and 2. Overall, we can note that (1) GLCN outperforms the baseline method GC-N [11] on all datasets, especially on the four image datasets. This clearly demonstrates the higher predictive accuracy on semi-supervised classification of GLCN by incorporating graph learning architecture. Comparing with GCN, the hidden layer presentations of graph nodes in GLCN become more discriminatively (as shown in Figure 4), which thus facilitates to semi-supervised learning results. (2) GLC-N performs better than recent graph network GAT [21], which indicates the benefit of GLCN on graph data representation and learning. (3) GLCN performs better than other graph based semi-supervised learning methods, such as LP [23], ManiReg [2] and DeepWalk [18], which further demonstrates the effectiveness of GLCN on conducting 2 Note that, GCN results in Table 1 are slightly different from that reported in work [11], because we use a different hidden layer setting to make it consistent with GLCN and also obtain average better results on all the network and image datasets. graph semi-supervised classification tasks.\n\n\nParameter analysis\n\nIn this section, we evaluate the performance of GLCN model with different settings of network parameters. We first investigate the influence of model depth of GLCN (number of convolutional layers) on semi-supervised classification results. Figure 5 shows the performance of our GLCN method across different number of convolutional layers on MNIST dataset. As a baseline, we also list the results of GCN model with the same setting. One can note that GLCN can obtain better performance with different number of layers, which indicates the insensitivity of the GLCN w.r.t. model depth. Also, GLCN always performs better than GCN under different model depths, which further demonstrates the benefit and better performance of GLCN comparing with the baseline method.  Then, we evaluate the performance of two-layer GLCN with different number of hidden units in convolutional layer. Table 3 summarizes the performance of GLCN with different number of hidden units on MNIST dataset. We can note that Both GCN and GLCN are generally insensitive w.r.t. number of units in the hidden layer.\n\nFinally, we investigate the influence of graph learning parameter \u03bb in our GLCN. Table 4 shows the performance of GLCN with different parameter settings. When \u03bb is set to 0, GLCN can also return a reasonable result. Also, one can note that the graph learning regularization term in loss function improves the graph learning and thus semi-supervised classification results.\n\n\nConclusion and Future Works\n\nIn this paper, we propose a novel Graph Learning-Convolutional Network (GLCN) for graph based semisupervised learning problem. GLCN integrates the proposed new graph learning operation and traditional graph convolution architecture together in a unified network, which can learn an optimal graph structure that best serves GCN for semi-supervised learning problem. Experimental results on seven benchmarks demonstrate that GLCN generally outperforms traditional fixed-graph CNNs on various semi-supervised learning tasks.\n\nNote that, GLCN is not limited to deal with semisupervised learning tasks. In the future, we will adapt GLCN on some more pattern recognition tasks, such as graph data clustering, graph link/edge prediction, etc. Also, we can explore GLCN method on some other computer vision tasks, such as visual object detection, image cosegmentation and visual saliency analysis.\n\nFigure 1 .\n1Architecture of the proposed GLCN network for semi-supervised learning.\n\nFigure 2 .\n2Architecture of the proposed graph learning architecture in GLCN.\n\nFigure 3 .\n3Demonstration of cross-entropy loss values across different epochs on MNIST dataset.\n\nFigure 4\n4Figure 4. 2D t-SNE [14] visualizations of the feature map output by the first convolutional layer of GCN [11] and GLCN respectively on Scene15 dataset. Different classes are marked by different colors. One can note that, the data of different classes are distributed more clearly and compactly in our GLCN convolutional layer feature representation.\n\nFigure 5 .\n5Results of GLCN across different convolutional layers on MNIST dataset.\n\nTable 1 .\n1Comparison results of semi-supervised learning on dataset Citeseer, Cora and Pubmed.\n\nTable 2 .\n2Comparison results on dataset SVHN, CIFAR, MNIST and Scene15Dataset \nSVHN \nCIFAR \nNo. of label \n1000 \n2000 \n3000 \n1000 \n2000 \n3000 \nManiReg [2] \n69.44\u00b10.69% 72.73\u00b10.44% 74.63\u00b10.45% \n52.30\u00b10.66% 57.08\u00b10.80% 59.69\u00b10.71% \nLP [23] \n69.68\u00b10.84% 70.35\u00b11.73% 69.47\u00b12.96% \n57.52\u00b10.67% 59.22\u00b10.67% 60.38\u00b10.51% \nDeepWalk [18] \n74.64\u00b10.23% 76.21\u00b10.23% 77.04\u00b10.42% \n56.16\u00b10.54% 59.73\u00b10.35% 61.26\u00b10.32% \nGCN [11] \n71.33\u00b11.48% 73.43\u00b10.46% 73.63\u00b10.52% \n60.43\u00b10.56% 60.91\u00b10.50% 60.99\u00b10.49% \nGAT [21] \n73.87\u00b10.32% 74.85\u00b10.55% 75.17\u00b10.43% \n63.25\u00b10.50% 65.55\u00b10.58% 66.56\u00b10.58% \nGLCN \n79.14\u00b10.38% 80.68\u00b10.22% 81.43\u00b10.34% 66.67\u00b10.24% 69.33\u00b10.54% 70.39\u00b10.54% \nDataset \nMNIST \nScene15 \nNo. of label \n1000 \n2000 \n3000 \n500 \n750 \n1000 \nManiReg [2] \n92.74\u00b10.33% 93.96\u00b10.23% 94.62\u00b10.22% \n81.29\u00b13.35% 86.45\u00b11.91% 89.86\u00b10.71% \nLP [23] \n79.28\u00b10.91% 81.91\u00b10.82% 83.45\u00b10.53% \n89.40\u00b14.74% 92.12\u00b12.87% 92.98\u00b12.45% \nDeepWalk [18] 94.55\u00b10.27% 95.04\u00b10.28% 95.34\u00b10.26% \n95.64\u00b10.24% 96.01\u00b10.24% 96.53\u00b10.37% \nGCN [11] \n90.59\u00b10.26% 90.91\u00b10.19% 91.01\u00b10.23% \n91.42\u00b12.07% 94.41\u00b10.92% 95.44\u00b10.89% \nGAT [21] \n92.11\u00b10.35% 92.64\u00b10.28% 92.81\u00b10.29% \n93.98\u00b10.75% 94.64\u00b10.41% 95.03\u00b10.46% \nGLCN \n94.28\u00b10.28% 95.09\u00b10.17% 95.46\u00b10.20% 96.19\u00b10.38% 96.71\u00b10.40% 96.67\u00b10.37% \n\n\n\nTable 3 .Table 4 .\n34Results of two-layer GLCN across different number of units in convolutional-layer on MNIST dataset. Results of GLCN with different settings of graph learning parameter \u03bb in loss (Eq.(15)) on MNIST and CIFAR10 datasetsGCN-Layers \n50 \n60 \n70 \n80 \n90 \nGCN \n0.9041 0.9075 0.9080 0.9076 0.9070 \nGLCN \n0.9410 0.9396 0.9394 0.9410 0.9389 \n\nParameter \u03bb \n0 \n1e-4 1e-3 1e-2 1e-1 \n1.0 \nCIFAR10 \n0.67 0.69 0.69 0.70 0.69 0.69 \nMNIST \n0.92 0.93 0.93 0.94 0.94 0.93 \n\n\nAcknowledgmentThis work is supported in part by NSFC Key Projects of International (Regional) Cooperation and Exchanges under Grant (61860206004); National Natural Science Foundation of China (61602001, 61872005, 61671018); Natural Science Foundation of Anhui Province (1708085QF139); Natural Science Foundation of Anhui Higher Education Institutions of China (KJ2016A020).\nDiffusionconvolutional neural networks. James Atwood, Don Towsley, Advances in Neural Information Processing Systems. James Atwood and Don Towsley. Diffusion- convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1993-2001, 2016.\n\nManifold regularization: A geometric framework for learning from labeled and unlabeled examples. Mikhail Belkin, Partha Niyogi, Vikas Sindhwani, Journal of machine learning research. 7Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Jour- nal of machine learning research, 7:2399-2434, 2006.\n\nSpectral networks and locally connected networks on graphs. Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann Lecun, International Conference on Learning Representations. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connect- ed networks on graphs. In International Conference on Learning Representations, 2014.\n\nConvolutional neural networks on graphs with fast localized spectral filtering. Micha\u00ebl Defferrard, Xavier Bresson, Pierre Vandergheynst, Advances in Neural Information Processing Systems. Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Van- dergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pages 3844- 3852, 2016.\n\nConvolutional networks on graphs for learning molecular fingerprints. Dougal David K Duvenaud, Jorge Maclaurin, Rafael Iparraguirre, Timothy Bombarell, Al\u00e1n Hirzel, Ryan P Aspuru-Guzik, Adams, Advances in Neural Information Processing Systems. David K Duvenaud, Dougal Maclaurin, Jorge Ipar- raguirre, Rafael Bombarell, Timothy Hirzel, Al\u00e1n Aspuru-Guzik, and Ryan P Adams. Convolution- al networks on graphs for learning molecular finger- prints. In Advances in Neural Information Processing Systems, pages 2224-2232, 2015.\n\nUnderstanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio, International conference on artificial intelligence and statistics. Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural net- works. In International conference on artificial intel- ligence and statistics, pages 249-256, 2010.\n\nDeep convolutional networks on graph-structured data. Mikael Henaff, Joan Bruna, Yann Lecun, arXiv:1506.05163arXiv preprintMikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. arX- iv preprint arXiv:1506.05163, 2015.\n\nGraphlaplacian pca: Closed-form solution and robustness. Bo Jiang, Chris Ding, Bio Luo, Jin Tang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBo Jiang, Chris Ding, Bio Luo, and Jin Tang. Graph- laplacian pca: Closed-form solution and robustness. In Proceedings of the IEEE Conference on Comput- er Vision and Pattern Recognition, pages 3492-3498, 2013.\n\nLabel consistent k-svd: Learning a discriminative dictionary for recognition. Zhuolin Jiang, Zhe Lin, Larry S Davis, 35Zhuolin Jiang, Zhe Lin, and Larry S Davis. Label consistent k-svd: Learning a discriminative dictionary for recognition. IEEE transactions on pattern analysis and machine intelligence, 35(11):2651-2664, 2013.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, International Conference on Learning Representations. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Confer- ence on Learning Representations, 2015.\n\nSemi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.02907arXiv preprintThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arX- iv preprint arXiv:1609.02907, 2016.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, University of TorontoTechnical reportAlex Krizhevsky and Geoffrey Hinton. Learning mul- tiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\nBeyond bags of features: Spatial pyramid matching for recognizing natural scene categories. Svetlana Lazebnik, Cordelia Schmid, Jean Ponce, IEEE Conference on Computer Vision and Pattern Recognition. Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In IEEE Con- ference on Computer Vision and Pattern Recognition, pages 2169-2178, 2006.\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9Laurens van der Maaten and Geoffrey Hinton. Visu- alizing data using t-sne. Journal of machine learning research, 9:2579-2605, 2008.\n\nGeometric deep learning on graphs and manifolds using mixture model cnns. Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, Michael M Bronstein, IEEE Conference on Computer Vision and Pattern Recognition. Federico Monti, Davide Boscaini, Jonathan Masci, E- manuele Rodola, Jan Svoboda, and Michael M Bron- stein. Geometric deep learning on graphs and mani- folds using mixture model cnns. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5423-5434, 2017.\n\nReading digits in natural images with unsupervised feature learning. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y Ng, NIPS workshop on deep learning and unsupervised feature learning. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, 2011.\n\nClustering and projected clustering with adaptive neighbors. Feiping Nie, Xiaoqian Wang, Heng Huang, ACM SIGKDD international conference on Knowledge Discovery and Data Mining. Feiping Nie, Xiaoqian Wang, and Heng Huang. Clus- tering and projected clustering with adaptive neigh- bors. In ACM SIGKDD international conference on Knowledge Discovery and Data Mining, pages 977- 986, 2014.\n\nDeepwalk: Online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, ACM SIGKDD international conference on Knowledge discovery and data mining. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representation- s. In ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701- 710, 2014.\n\nAdaptive graph convolutional neural networks. Feiyun Zhu Junzhou Huang Ruoyu Li, Sheng Wang, AAAI Conference on Artificial Intelligence. Feiyun Zhu Junzhou Huang Ruoyu Li, Sheng Wang. Adaptive graph convolutional neural networks. In AAAI Conference on Artificial Intelligence, pages 3546-3553, 2018.\n\nCollective classification in network data. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, Tina Eliassi-Rad, AI magazine. 293Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Col- lective classification in network data. AI magazine, 29(3):93-93, 2008.\n\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arX- iv:1710.10903Graph attention networks. arXiv preprintPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casano- va, Adriana Romero, Pietro Lio, and Yoshua Ben- gio. Graph attention networks. arXiv preprint arX- iv:1710.10903, 2017.\n\nGraph convolutional networks: Algorithms, applications and open challenges. Si Zhang, Hanghang Tong, Jiejun Xu, Ross Maciejewski, International Conference on Computational Social Networks. Si Zhang, Hanghang Tong, Jiejun Xu, and Ross Ma- ciejewski. Graph convolutional networks: Algorithm- s, applications and open challenges. In International Conference on Computational Social Networks, pages 79-91, 2018.\n\nSemi-supervised learning using gaussian fields and harmonic functions. Xiaojin Zhu, Zoubin Ghahramani, John D Lafferty, Proceedings of the 20th International conference on Machine learning (ICML-03). the 20th International conference on Machine learning (ICML-03)Xiaojin Zhu, Zoubin Ghahramani, and John D Laffer- ty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th Inter- national conference on Machine learning (ICML-03), pages 912-919, 2003.\n", "annotations": {"author": "[{\"end\":176,\"start\":71},{\"end\":266,\"start\":177},{\"end\":355,\"start\":267},{\"end\":442,\"start\":356},{\"end\":546,\"start\":443}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":74},{\"end\":188,\"start\":183},{\"end\":277,\"start\":274},{\"end\":364,\"start\":360},{\"end\":450,\"start\":447}]", "author_first_name": "[{\"end\":73,\"start\":71},{\"end\":182,\"start\":177},{\"end\":273,\"start\":267},{\"end\":359,\"start\":356},{\"end\":446,\"start\":443}]", "author_affiliation": "[{\"end\":175,\"start\":100},{\"end\":265,\"start\":190},{\"end\":354,\"start\":279},{\"end\":441,\"start\":366},{\"end\":545,\"start\":470}]", "title": "[{\"end\":68,\"start\":1},{\"end\":614,\"start\":547}]", "venue": null, "abstract": "[{\"end\":1675,\"start\":639}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1926,\"start\":1923},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1928,\"start\":1926},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1931,\"start\":1928},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1934,\"start\":1931},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1937,\"start\":1934},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1940,\"start\":1937},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2046,\"start\":2042},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2214,\"start\":2211},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2368,\"start\":2365},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2584,\"start\":2580},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2744,\"start\":2740},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2958,\"start\":2955},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3094,\"start\":3091},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3270,\"start\":3266},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4016,\"start\":4013},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4221,\"start\":4217},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5620,\"start\":5616},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5959,\"start\":5955},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6007,\"start\":6003},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6502,\"start\":6498},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6828,\"start\":6825},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7487,\"start\":7484},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9340,\"start\":9336},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11640,\"start\":11636},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14658,\"start\":14654},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14744,\"start\":14740},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15343,\"start\":15339},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15382,\"start\":15378},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15393,\"start\":15389},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15417,\"start\":15414},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15903,\"start\":15899},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15989,\"start\":15985},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16434,\"start\":16430},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16717,\"start\":16713},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17412,\"start\":17408},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17414,\"start\":17412},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17482,\"start\":17479},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17603,\"start\":17599},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17606,\"start\":17603},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17922,\"start\":17919},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17948,\"start\":17944},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17980,\"start\":17976},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18007,\"start\":18003},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18034,\"start\":18030},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18298,\"start\":18295},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18560,\"start\":18556},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18844,\"start\":18840},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18983,\"start\":18979},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19169,\"start\":19165},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19442,\"start\":19438},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19481,\"start\":19478},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19549,\"start\":19545},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19589,\"start\":19585},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19629,\"start\":19625},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19983,\"start\":19979},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20197,\"start\":20193},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20646,\"start\":20642},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20824,\"start\":20820},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20837,\"start\":20834},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20855,\"start\":20851},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20925,\"start\":20924},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21014,\"start\":21010}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23701,\"start\":23617},{\"attributes\":{\"id\":\"fig_1\"},\"end\":23780,\"start\":23702},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23878,\"start\":23781},{\"attributes\":{\"id\":\"fig_4\"},\"end\":24239,\"start\":23879},{\"attributes\":{\"id\":\"fig_5\"},\"end\":24324,\"start\":24240},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":24421,\"start\":24325},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":25651,\"start\":24422},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":26128,\"start\":25652}]", "paragraph": "[{\"end\":3356,\"start\":1691},{\"end\":4443,\"start\":3358},{\"end\":5154,\"start\":4445},{\"end\":5249,\"start\":5156},{\"end\":5523,\"start\":5251},{\"end\":6074,\"start\":5540},{\"end\":6150,\"start\":6119},{\"end\":6540,\"start\":6258},{\"end\":6906,\"start\":6585},{\"end\":6990,\"start\":6947},{\"end\":8240,\"start\":7031},{\"end\":8628,\"start\":8272},{\"end\":8948,\"start\":8717},{\"end\":9059,\"start\":8977},{\"end\":9341,\"start\":9107},{\"end\":9644,\"start\":9343},{\"end\":9854,\"start\":9743},{\"end\":10518,\"start\":9916},{\"end\":10780,\"start\":10653},{\"end\":10911,\"start\":10828},{\"end\":11477,\"start\":10913},{\"end\":11970,\"start\":11499},{\"end\":12375,\"start\":11972},{\"end\":12430,\"start\":12425},{\"end\":12877,\"start\":12571},{\"end\":12989,\"start\":12909},{\"end\":13350,\"start\":13021},{\"end\":13741,\"start\":13388},{\"end\":14142,\"start\":13743},{\"end\":15088,\"start\":14144},{\"end\":15829,\"start\":15115},{\"end\":16229,\"start\":15831},{\"end\":16342,\"start\":16231},{\"end\":17483,\"start\":16344},{\"end\":17878,\"start\":17508},{\"end\":18075,\"start\":17890},{\"end\":19059,\"start\":18077},{\"end\":21217,\"start\":19104},{\"end\":22321,\"start\":21240},{\"end\":22695,\"start\":22323},{\"end\":23248,\"start\":22727},{\"end\":23616,\"start\":23250}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6118,\"start\":6075},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6257,\"start\":6151},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6584,\"start\":6541},{\"attributes\":{\"id\":\"formula_3\"},\"end\":6946,\"start\":6907},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8716,\"start\":8629},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8976,\"start\":8949},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9106,\"start\":9060},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9742,\"start\":9645},{\"attributes\":{\"id\":\"formula_8\"},\"end\":9915,\"start\":9855},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10652,\"start\":10519},{\"attributes\":{\"id\":\"formula_10\"},\"end\":10827,\"start\":10781},{\"attributes\":{\"id\":\"formula_11\"},\"end\":12424,\"start\":12376},{\"attributes\":{\"id\":\"formula_12\"},\"end\":12570,\"start\":12431},{\"attributes\":{\"id\":\"formula_13\"},\"end\":12908,\"start\":12878},{\"attributes\":{\"id\":\"formula_14\"},\"end\":13020,\"start\":12990},{\"attributes\":{\"id\":\"formula_15\"},\"end\":13387,\"start\":13351}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19872,\"start\":19865},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19996,\"start\":19989},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20113,\"start\":20106},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20959,\"start\":20952},{\"end\":22125,\"start\":22118},{\"end\":22411,\"start\":22404}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1689,\"start\":1677},{\"attributes\":{\"n\":\"2.\"},\"end\":5538,\"start\":5526},{\"attributes\":{\"n\":\"3.\"},\"end\":7029,\"start\":6993},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8270,\"start\":8243},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11497,\"start\":11480},{\"attributes\":{\"n\":\"4.\"},\"end\":15102,\"start\":15091},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15113,\"start\":15105},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17506,\"start\":17486},{\"end\":17888,\"start\":17881},{\"attributes\":{\"n\":\"4.3.\"},\"end\":19102,\"start\":19062},{\"attributes\":{\"n\":\"4.4.\"},\"end\":21238,\"start\":21220},{\"attributes\":{\"n\":\"5.\"},\"end\":22725,\"start\":22698},{\"end\":23628,\"start\":23618},{\"end\":23713,\"start\":23703},{\"end\":23792,\"start\":23782},{\"end\":23888,\"start\":23880},{\"end\":24251,\"start\":24241},{\"end\":24335,\"start\":24326},{\"end\":24432,\"start\":24423},{\"end\":25671,\"start\":25653}]", "table": "[{\"end\":25651,\"start\":24494},{\"end\":26128,\"start\":25891}]", "figure_caption": "[{\"end\":23701,\"start\":23630},{\"end\":23780,\"start\":23715},{\"end\":23878,\"start\":23794},{\"end\":24239,\"start\":23890},{\"end\":24324,\"start\":24253},{\"end\":24421,\"start\":24337},{\"end\":24494,\"start\":24434},{\"end\":25891,\"start\":25674}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8094,\"start\":8086},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10910,\"start\":10902},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11752,\"start\":11744},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11992,\"start\":11984},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14270,\"start\":14262},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14631,\"start\":14623},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20523,\"start\":20515},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21488,\"start\":21480}]", "bib_author_first_name": "[{\"end\":26548,\"start\":26543},{\"end\":26560,\"start\":26557},{\"end\":26875,\"start\":26868},{\"end\":26890,\"start\":26884},{\"end\":26904,\"start\":26899},{\"end\":27228,\"start\":27224},{\"end\":27244,\"start\":27236},{\"end\":27260,\"start\":27254},{\"end\":27272,\"start\":27268},{\"end\":27607,\"start\":27600},{\"end\":27626,\"start\":27620},{\"end\":27642,\"start\":27636},{\"end\":28008,\"start\":28002},{\"end\":28032,\"start\":28027},{\"end\":28050,\"start\":28044},{\"end\":28072,\"start\":28065},{\"end\":28088,\"start\":28084},{\"end\":28103,\"start\":28097},{\"end\":28538,\"start\":28532},{\"end\":28553,\"start\":28547},{\"end\":28895,\"start\":28889},{\"end\":28908,\"start\":28904},{\"end\":28920,\"start\":28916},{\"end\":29156,\"start\":29154},{\"end\":29169,\"start\":29164},{\"end\":29179,\"start\":29176},{\"end\":29188,\"start\":29185},{\"end\":29633,\"start\":29626},{\"end\":29644,\"start\":29641},{\"end\":29657,\"start\":29650},{\"end\":29922,\"start\":29921},{\"end\":29938,\"start\":29933},{\"end\":30214,\"start\":30213},{\"end\":30226,\"start\":30223},{\"end\":30470,\"start\":30466},{\"end\":30491,\"start\":30483},{\"end\":30779,\"start\":30771},{\"end\":30798,\"start\":30790},{\"end\":30811,\"start\":30807},{\"end\":31149,\"start\":31142},{\"end\":31174,\"start\":31166},{\"end\":31438,\"start\":31430},{\"end\":31452,\"start\":31446},{\"end\":31471,\"start\":31463},{\"end\":31487,\"start\":31479},{\"end\":31499,\"start\":31496},{\"end\":31516,\"start\":31509},{\"end\":31518,\"start\":31517},{\"end\":31935,\"start\":31930},{\"end\":31947,\"start\":31944},{\"end\":31958,\"start\":31954},{\"end\":31977,\"start\":31967},{\"end\":31990,\"start\":31988},{\"end\":32003,\"start\":31995},{\"end\":32369,\"start\":32362},{\"end\":32383,\"start\":32375},{\"end\":32394,\"start\":32390},{\"end\":32747,\"start\":32742},{\"end\":32761,\"start\":32757},{\"end\":32777,\"start\":32771},{\"end\":33143,\"start\":33113},{\"end\":33153,\"start\":33148},{\"end\":33421,\"start\":33411},{\"end\":33434,\"start\":33427},{\"end\":33450,\"start\":33443},{\"end\":33463,\"start\":33459},{\"end\":33477,\"start\":33472},{\"end\":33493,\"start\":33489},{\"end\":33706,\"start\":33701},{\"end\":33726,\"start\":33719},{\"end\":33744,\"start\":33737},{\"end\":33762,\"start\":33755},{\"end\":33777,\"start\":33771},{\"end\":33789,\"start\":33783},{\"end\":34107,\"start\":34105},{\"end\":34123,\"start\":34115},{\"end\":34136,\"start\":34130},{\"end\":34145,\"start\":34141},{\"end\":34516,\"start\":34509},{\"end\":34528,\"start\":34522},{\"end\":34547,\"start\":34541}]", "bib_author_last_name": "[{\"end\":26555,\"start\":26549},{\"end\":26568,\"start\":26561},{\"end\":26882,\"start\":26876},{\"end\":26897,\"start\":26891},{\"end\":26914,\"start\":26905},{\"end\":27234,\"start\":27229},{\"end\":27252,\"start\":27245},{\"end\":27266,\"start\":27261},{\"end\":27278,\"start\":27273},{\"end\":27618,\"start\":27608},{\"end\":27634,\"start\":27627},{\"end\":27656,\"start\":27643},{\"end\":28025,\"start\":28009},{\"end\":28042,\"start\":28033},{\"end\":28063,\"start\":28051},{\"end\":28082,\"start\":28073},{\"end\":28095,\"start\":28089},{\"end\":28116,\"start\":28104},{\"end\":28123,\"start\":28118},{\"end\":28545,\"start\":28539},{\"end\":28560,\"start\":28554},{\"end\":28902,\"start\":28896},{\"end\":28914,\"start\":28909},{\"end\":28926,\"start\":28921},{\"end\":29162,\"start\":29157},{\"end\":29174,\"start\":29170},{\"end\":29183,\"start\":29180},{\"end\":29193,\"start\":29189},{\"end\":29639,\"start\":29634},{\"end\":29648,\"start\":29645},{\"end\":29663,\"start\":29658},{\"end\":29931,\"start\":29923},{\"end\":29945,\"start\":29939},{\"end\":29949,\"start\":29947},{\"end\":30221,\"start\":30215},{\"end\":30231,\"start\":30227},{\"end\":30240,\"start\":30233},{\"end\":30481,\"start\":30471},{\"end\":30498,\"start\":30492},{\"end\":30788,\"start\":30780},{\"end\":30805,\"start\":30799},{\"end\":30817,\"start\":30812},{\"end\":31164,\"start\":31150},{\"end\":31181,\"start\":31175},{\"end\":31444,\"start\":31439},{\"end\":31461,\"start\":31453},{\"end\":31477,\"start\":31472},{\"end\":31494,\"start\":31488},{\"end\":31507,\"start\":31500},{\"end\":31528,\"start\":31519},{\"end\":31942,\"start\":31936},{\"end\":31952,\"start\":31948},{\"end\":31965,\"start\":31959},{\"end\":31986,\"start\":31978},{\"end\":31993,\"start\":31991},{\"end\":32006,\"start\":32004},{\"end\":32373,\"start\":32370},{\"end\":32388,\"start\":32384},{\"end\":32400,\"start\":32395},{\"end\":32755,\"start\":32748},{\"end\":32769,\"start\":32762},{\"end\":32784,\"start\":32778},{\"end\":33146,\"start\":33144},{\"end\":33158,\"start\":33154},{\"end\":33425,\"start\":33422},{\"end\":33441,\"start\":33435},{\"end\":33457,\"start\":33451},{\"end\":33470,\"start\":33464},{\"end\":33487,\"start\":33478},{\"end\":33505,\"start\":33494},{\"end\":33717,\"start\":33707},{\"end\":33735,\"start\":33727},{\"end\":33753,\"start\":33745},{\"end\":33769,\"start\":33763},{\"end\":33781,\"start\":33778},{\"end\":33796,\"start\":33790},{\"end\":34113,\"start\":34108},{\"end\":34128,\"start\":34124},{\"end\":34139,\"start\":34137},{\"end\":34157,\"start\":34146},{\"end\":34520,\"start\":34517},{\"end\":34539,\"start\":34529},{\"end\":34556,\"start\":34548}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":26769,\"start\":26503},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16902615},\"end\":27162,\"start\":26771},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17682909},\"end\":27518,\"start\":27164},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3016223},\"end\":27930,\"start\":27520},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1690180},\"end\":28455,\"start\":27932},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5575601},\"end\":28833,\"start\":28457},{\"attributes\":{\"doi\":\"arXiv:1506.05163\",\"id\":\"b6\"},\"end\":29095,\"start\":28835},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":148839},\"end\":29546,\"start\":29097},{\"attributes\":{\"id\":\"b8\"},\"end\":29875,\"start\":29548},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6628106},\"end\":30145,\"start\":29877},{\"attributes\":{\"doi\":\"arXiv:1609.02907\",\"id\":\"b10\"},\"end\":30409,\"start\":30147},{\"attributes\":{\"id\":\"b11\"},\"end\":30677,\"start\":30411},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2421251},\"end\":31110,\"start\":30679},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5855042},\"end\":31354,\"start\":31112},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":301319},\"end\":31859,\"start\":31356},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":16852518},\"end\":32299,\"start\":31861},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5226376},\"end\":32687,\"start\":32301},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3051291},\"end\":33065,\"start\":32689},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1415308},\"end\":33366,\"start\":33067},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":62016134},\"end\":33699,\"start\":33368},{\"attributes\":{\"doi\":\"arX- iv:1710.10903\",\"id\":\"b20\"},\"end\":34027,\"start\":33701},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":54526291},\"end\":34436,\"start\":34029},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1052837},\"end\":34932,\"start\":34438}]", "bib_title": "[{\"end\":26541,\"start\":26503},{\"end\":26866,\"start\":26771},{\"end\":27222,\"start\":27164},{\"end\":27598,\"start\":27520},{\"end\":28000,\"start\":27932},{\"end\":28530,\"start\":28457},{\"end\":29152,\"start\":29097},{\"end\":29919,\"start\":29877},{\"end\":30769,\"start\":30679},{\"end\":31140,\"start\":31112},{\"end\":31428,\"start\":31356},{\"end\":31928,\"start\":31861},{\"end\":32360,\"start\":32301},{\"end\":32740,\"start\":32689},{\"end\":33111,\"start\":33067},{\"end\":33409,\"start\":33368},{\"end\":34103,\"start\":34029},{\"end\":34507,\"start\":34438}]", "bib_author": "[{\"end\":26557,\"start\":26543},{\"end\":26570,\"start\":26557},{\"end\":26884,\"start\":26868},{\"end\":26899,\"start\":26884},{\"end\":26916,\"start\":26899},{\"end\":27236,\"start\":27224},{\"end\":27254,\"start\":27236},{\"end\":27268,\"start\":27254},{\"end\":27280,\"start\":27268},{\"end\":27620,\"start\":27600},{\"end\":27636,\"start\":27620},{\"end\":27658,\"start\":27636},{\"end\":28027,\"start\":28002},{\"end\":28044,\"start\":28027},{\"end\":28065,\"start\":28044},{\"end\":28084,\"start\":28065},{\"end\":28097,\"start\":28084},{\"end\":28118,\"start\":28097},{\"end\":28125,\"start\":28118},{\"end\":28547,\"start\":28532},{\"end\":28562,\"start\":28547},{\"end\":28904,\"start\":28889},{\"end\":28916,\"start\":28904},{\"end\":28928,\"start\":28916},{\"end\":29164,\"start\":29154},{\"end\":29176,\"start\":29164},{\"end\":29185,\"start\":29176},{\"end\":29195,\"start\":29185},{\"end\":29641,\"start\":29626},{\"end\":29650,\"start\":29641},{\"end\":29665,\"start\":29650},{\"end\":29933,\"start\":29921},{\"end\":29947,\"start\":29933},{\"end\":29951,\"start\":29947},{\"end\":30223,\"start\":30213},{\"end\":30233,\"start\":30223},{\"end\":30242,\"start\":30233},{\"end\":30483,\"start\":30466},{\"end\":30500,\"start\":30483},{\"end\":30790,\"start\":30771},{\"end\":30807,\"start\":30790},{\"end\":30819,\"start\":30807},{\"end\":31166,\"start\":31142},{\"end\":31183,\"start\":31166},{\"end\":31446,\"start\":31430},{\"end\":31463,\"start\":31446},{\"end\":31479,\"start\":31463},{\"end\":31496,\"start\":31479},{\"end\":31509,\"start\":31496},{\"end\":31530,\"start\":31509},{\"end\":31944,\"start\":31930},{\"end\":31954,\"start\":31944},{\"end\":31967,\"start\":31954},{\"end\":31988,\"start\":31967},{\"end\":31995,\"start\":31988},{\"end\":32008,\"start\":31995},{\"end\":32375,\"start\":32362},{\"end\":32390,\"start\":32375},{\"end\":32402,\"start\":32390},{\"end\":32757,\"start\":32742},{\"end\":32771,\"start\":32757},{\"end\":32786,\"start\":32771},{\"end\":33148,\"start\":33113},{\"end\":33160,\"start\":33148},{\"end\":33427,\"start\":33411},{\"end\":33443,\"start\":33427},{\"end\":33459,\"start\":33443},{\"end\":33472,\"start\":33459},{\"end\":33489,\"start\":33472},{\"end\":33507,\"start\":33489},{\"end\":33719,\"start\":33701},{\"end\":33737,\"start\":33719},{\"end\":33755,\"start\":33737},{\"end\":33771,\"start\":33755},{\"end\":33783,\"start\":33771},{\"end\":33798,\"start\":33783},{\"end\":34115,\"start\":34105},{\"end\":34130,\"start\":34115},{\"end\":34141,\"start\":34130},{\"end\":34159,\"start\":34141},{\"end\":34522,\"start\":34509},{\"end\":34541,\"start\":34522},{\"end\":34558,\"start\":34541}]", "bib_venue": "[{\"end\":29336,\"start\":29274},{\"end\":34701,\"start\":34638},{\"end\":26619,\"start\":26570},{\"end\":26952,\"start\":26916},{\"end\":27332,\"start\":27280},{\"end\":27707,\"start\":27658},{\"end\":28174,\"start\":28125},{\"end\":28628,\"start\":28562},{\"end\":28887,\"start\":28835},{\"end\":29272,\"start\":29195},{\"end\":29624,\"start\":29548},{\"end\":30003,\"start\":29951},{\"end\":30211,\"start\":30147},{\"end\":30464,\"start\":30411},{\"end\":30877,\"start\":30819},{\"end\":31219,\"start\":31183},{\"end\":31588,\"start\":31530},{\"end\":32072,\"start\":32008},{\"end\":32476,\"start\":32402},{\"end\":32860,\"start\":32786},{\"end\":33202,\"start\":33160},{\"end\":33518,\"start\":33507},{\"end\":33840,\"start\":33816},{\"end\":34216,\"start\":34159},{\"end\":34636,\"start\":34558}]"}}}, "year": 2023, "month": 12, "day": 17}