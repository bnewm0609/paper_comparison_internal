{"id": 258455124, "updated": "2023-06-01 15:57:56.677", "metadata": {"title": "Superconducting Hyperdimensional Associative Memory Circuit for Scalable Machine Learning", "authors": "[{\"first\":\"Kylie\",\"last\":\"Huch\",\"middle\":[]},{\"first\":\"Patricia\",\"last\":\"Gonzalez-Guerrero\",\"middle\":[]},{\"first\":\"Darren\",\"last\":\"Lyles\",\"middle\":[]},{\"first\":\"George\",\"last\":\"Michelogiannakis\",\"middle\":[]}]", "venue": "IEEE Transactions on Applied Superconductivity", "journal": "IEEE Transactions on Applied Superconductivity", "publication_date": {"year": 2023, "month": 8, "day": 1}, "abstract": "We propose a generalized architecture for the first rapid-single-flux-quantum (RSFQ) associative memory circuit. The circuit employs hyperdimensional computing (HDC), a machine learning (ML) paradigm utilizing vectors with dimensionality in the thousands to represent information. HDC designs have small memory footprints, simple computations, and simple training algorithms compared to superconducting neural network accelerators (SNNAs), making them a better option for scalable SFQ machine learning (ML) solutions. The proposed superconducting HDC (SHDC) circuit uses entirely on-chip RSFQ memory which is tightly integrated with logic, operates at 33.3 GHz, is applicable to general ML tasks, and is manufacturable at practically useful scales given current SFQ fabrication limits. Tailored to a language recognition task, SHDC consists of $\\sim$2\u201320 M Josephson junctions (JJs) and consumes up to three times less power than an analogous CMOS HDC circuit while achieving 78\u201384% higher throughput. SHDC is capable of outperforming the state of the art RSFQ SNNA, SuperNPU, by 48-99% for all benchmark NN architectures tested while occupying up to 90% less area and consuming up to nine times less power. To the best of the authors' knowledge, SHDC is currently the only superconducting ML approach feasible at practically useful scales for real-world ML tasks and capable of online learning.", "fields_of_study": null, "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1109/tasc.2023.3271951"}}, "content": {"source": {"pdf_hash": "b833350cf3238e0fa770bb38cacab29bb49e42c9", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8991c03f927e11c30e5d552583595e775e85fc3c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b833350cf3238e0fa770bb38cacab29bb49e42c9.txt", "contents": "\nSuperconducting Hyperdimensional Associative Memory Circuit for Scalable Machine Learning\nAUGUST 2023 1801214\n\nKylie Huch \nMember, IEEEPatricia Gonzalez-Guerrero \nMember, IEEEDarren Lyles \nSenior Member, IEEEGeorge Michelogiannakis \nSuperconducting Hyperdimensional Associative Memory Circuit for Scalable Machine Learning\n\nIEEE TRANSACTIONS ON APPLIED SUPERCONDUCTIVITY\n335AUGUST 2023 180121410.1109/TASC.2023.3271951Index Terms-Superconducting digital computinghyper- dimesional computingmachine learninglow energyarea efficiencyneuromorphic computingRSFQ\nWe propose a generalized architecture for the first rapid-single-flux-quantum (RSFQ) associative memory circuit. The circuit employs hyperdimensional computing (HDC), a machine learning (ML) paradigm utilizing vectors with dimensionality in the thousands to represent information. HDC designs have small memory footprints, simple computations, and simple training algorithms compared to superconducting neural network accelerators (SNNAs), making them a better option for scalable SFQ machine learning (ML) solutions. The proposed superconducting HDC (SHDC) circuit uses entirely on-chip RSFQ memory which is tightly integrated with logic, operates at 33.3 GHz, is applicable to general ML tasks, and is manufacturable at practically useful scales given current SFQ fabrication limits. Tailored to a language recognition task, SHDC consists of \u223c2-20 M Josephson junctions (JJs) and consumes up to three times less power than an analogous CMOS HDC circuit while achieving 78-84% higher throughput. SHDC is capable of outperforming the state of the art RSFQ SNNA, SuperNPU, by 48-99% for all benchmark NN architectures tested while occupying up to 90% less area and consuming up to nine times less power. To the best of the authors' knowledge, SHDC is currently the only superconducting ML approach feasible at practically useful scales for real-world ML tasks and capable of online learning.\n\nI. INTRODUCTION\n\nW ITH the slowdown of Moore's law and the end of Dennard scaling, superconducting digital computing offers a promising alternative for future high performance computing (HPC) systems due to its ability to operate at up to \u223c100 GHz with low power dissipation [1], [2]. Of the variety of different superconducting digital logic families based on single-fluxquantum (SFQ) pulses, rapid-single-flux-quantum (RSFQ) logic is the most mature and remains the most common for high-speed circuit applications [3]. As such, we design and simulate a practical HDC circuit based on standard RSFQ logic gates [4].\n\nLimited device density is one of the greatest challenges for superconducting digital computing currently, making area a critical constraint for SFQ circuits [1], [5]. Although RSFQ circuits with about one million Josephson junctions (JJs) [6], [7] and, more recently, close to ten million JJs [8] have been demonstrated, these chips had highly-regular shift register designs. In terms of complex logic and irregular RSFQ circuits, recently demonstrated chips have been limited to around 20-30 thousand JJs [9]. Additionally, cryogenic on-chip memory is widely regarded as a scarce resource [10]. These factors severely hinder the development of SFQ circuits for machine learning (ML) at practical scales due to the computational complexity and large memory footprints typical of these algorithms.\n\n\nA. Background and Related Work\n\nAt a high level, all ML approaches can be broken down into two basic stages: learning, also referred to as training, and inference, also referred to as classification. Typically, ML systems accept a single type of input data-for example: images, text, frequency data, etc.-and map it to some internally-used representation preserving the feature(s) of interest. During learning, the system constructs high-level class representations that capture the distinguishing statistical characteristics of the objects in that class either using explicitly labeled data in what is termed supervised learning, or from naturally-arising similarities and differences among unlabeled data in what is termed unsupervised learning or clustering. There are also other approaches using a combination of the aforementioned methods such as reinforcement learning and semi-supervised learning. Most ML approaches can be used with any of the above methods. During classification, the system calculates the best matching class to unlabeled input data and returns the label of that class as the classification result. For additional details on ML methods and approaches please consult [11].\n\nLearning is significantly more complex and time-consuming than classification. As such, many ML approaches perform training separately from classification in more powerful and less resource-constrained environments to enable deployment of the system on less complex hardware in what is termed offline or batch learning. This approach comes at the expense of the robustness and adaptability of the system; once deployed, they cannot continue to learn, making such systems rigid, highly dependent on the training environment, and often susceptible to noise. Supervised learning methods typically necessitate offline learning. When the system is capable of continued learning after deployment, it is termed online or continuous learning. The architecture used to deploy such systems must be capable of implementing the computations of both learning and classification; however, the resulting system benefits from greatly increased robustness and adaptability. Please consult [12] for additional details.\n\nNeural networks (NNs) are the dominant ML approach currently and are capable of achieving near-human level accuracy for many tasks at the cost of memory, inference computational complexity, and training complexity [13], [14]. NNs consist of multiple layers each containing on the order of thousands of nodes connected in a predefined and fixed manner. Each node has tens to thousands of learnable floating point parameter(s), resulting in millions to hundreds of millions of parameters for standard, modern NN architectures. Input data is mapped to typically float-valued activations of first-layer nodes which then propagate through the network based on the connectivity of the nodes and the values of the learned network parameters. Each node in the output layer represents a class and the one with the highest valued activation is the winner of the classification operation.\n\nClassification in NNs involves on the order of millions of data-intensive matrix multiply-accumulate (MAC) operations per classification operation [13], [15]. Training involves both backwards and forwards passes over the data with the backward pass being far more computationally complex. Following the forward pass (classification), error is back-propagated through the entire network (backwards pass). This requires the repeated computation of extremely large and complex gradients followed by the adjustment of the many millions of learnable network parameters. In order for the network to successfully converge, this must be done over the entire training data set in many thousands of iterations, making NN training exceedingly energy and time intensive [13], [14], [16].\n\nState-of-the-art superconducting NN accelerators (SNNAs) such as SuperNPU [15] use quantized network parameters both to reduce memory requirements and to better match the quantized nature of SFQ pulses. However, quantizing NN parameters to the low bitwidths (\u22648 bits) required for realistic implementation in SFQ technology inevitably introduces quantization error and reduces noise tolerance [17], [18]. Additionally, even with aggressively quantized parameters, the smallest NNs still require on the order of millions of parameters [19], [20], [21], [22], [23], [24], necessitating the use of off-chip, non-SFQ memory as well as converters, resulting in increased latency and area [10]. Perhaps most importantly, due to the extreme complexity of NN training, SNNAs are designed to implement only the forward pass of NNs (classification). This makes SNNA architectures fundamentally incapable of online learning. They must be initialized with pretrained, quantized network parameters and thus are unable to learn following deployment.\n\nThe matrix-based nature of NN computations results in a huge amount of data movement in the form of both network parameters and partial results. As such, NN accelerators require large amounts of on-chip memory in the form of buffers to ensure performance is not unduly limited by memory bandwidth [15]. This is especially important for SNNAs as the maximum operating frequency of the off-chip, non-SFQ memory-DRAM in the case of SuperNPU-is generally much slower than that of the SFQ portion of the circuit [2], [25], [26], [27], [28].\n\nWhile NNs excel in semiconductor-based computing environments, they are not a good match to superconducting digital environments due to the limitations on device density and lack of abundant cryogenic on-chip memory in conjunction with the extreme computational complexity, memory requirements, and training costs of NNs. Conversely, neuromorpic ML systems are uniquely well-suited for implementation in SFQ technology. A major barrier to the implementation of next-generation neuromorphic designs in traditional semiconductor technologies is their vast scales which result in large power consumption and data processing requirements [29]. As such, the implementation of these systems in superconducting environments has garnered increasing attention recently due to the remarkably high speed and low power consumption metrics of this technology, enabling the scaling of such systems beyond what is realistic in semiconductor technologies [29], [30]. Furthermore, the natural spiking behavior of JJs very closely resembles that of biological neurons; JJ critical current is analogous to the threshold potential of neurons, enabling biologically-plausible spike-based computing schemes [29], [30], [31]. [30] and [31] propose potential designs for such next-generation superconducting neuromoprhic designs capitalizing on the aforementioned properties and [29] provides a comprehensive review of cryogenic neuromorphic hardware.\n\nIn terms of currently feasible superconducting ML approaches, the only one the authors are aware of is the SFQ Discrete Hopfield Neural Network presented in [32]. This SFQ circuit is intended for image recognition tasks and is realistically fabricatable given current SFQ technology limits. However, the circuit is only capable of storing two, 8-bit memory patterns, making it unrealistic for use with real-world ML tasks at its current scale.\n\n\nB. Summary\n\nIn this article, we propose hyperdimensional computing (HDC) for efficient ML in superconducting digital computing. HDC is a Turing-complete neuromorphic computational paradigm in which computation is performed with vectors whose dimensionality is in the thousands, termed hypervectors [33], [34], [35], [36], [37], [38]. Data is represented holographically within these hypervectors, meaning that data is distributed across the entire vector; no subset has any particular meaning [37], [39], [40]. Due to their hyperdimensional and holographic nature, the representations of HDC are extremely robust to noise and error from all sources [35], [38], [41]. The power of HDC as an ML approach lies in the topographical properties of its hyperdimensional (HD) representational space which is ideally suited for naturally expressing cognitive operations [33], [35], [38]. As such, the computations of HDC are quite simple, resulting in designs with small hardware footprints dominated Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.  [34], [36]. Additionally, HDC is capable of learning classes from very little training data while achieving accuracy competitive with NNs [38], [40], [42], [43]. Furthermore, learning and classification are performed in the same manner in HDC, making these architectures capable of online learning without added computational overhead [36], [38]. See Table I for a glossary of HDC terms and Section II for additional details on HDC as an ML paradigm.\n\n\nHDC's low computational complexity, small area footprints, simple algorithms, robustness, and fast, online learning capabilities make it ideally suited for implementing ML in SFQ technology.\n\nIn this article, we present a generalized architecture for the first superconducting HDC (SHDC) design and evaluate it against an analogous CMOS HDC design [36] as well as the state-of-the-art SNNA, SuperNPU [15]. As we show, our SHDC architecture operates at 33.3 GHz and is manufacturable at practically useful scales given current SFQ fabrication limits. SHDC consumes up to three times less power than an analogous CMOS HDC circuit while achieving 78-84% higher throughput. SHDC is capable of outperforming SuperNPU by 48-99% for all benchmark NN architectures tested while occupying up to~90% less area and consuming up to nine times less power.\n\nTo the best of the authors' knowledge, SHDC is currently the only superconducting ML approach that is feasible at practically useful scales for real-world ML tasks and capable of online learning. We argue that SHDC is a superior approach for implementing machine learning in superconducting digital computing.\n\n\nII. HYPERDIMENSIONAL COMPUTING (HDC)\n\nIn HDC, objects are represented in hyperdimensional (HD) space as N -dimensional vectors-termed hypervectors-where N typically ranges from 500 to 10 k [33], [35], [38], [40], [44]. Due to its high dimensionality, the topology of the representational space formed by these hypervectors is ideal for naturally expressing the types of cognitive operations required for artificial intelligence (AI), resulting in lightweight but powerful ML architectures [33], [36], [38], [43]. Higher-order class representations are formed by recoverably superimposing multiple object hypervectors of the same class into a single, averaged hypervector representing the class [36], [37]. These class hypervectors are the only things learned and stored in HDC. The natural, highly neuromorphic ability to compute in superposition underlies the remarkable error tolerance, fast learning capabilities, and robustness of solutions learned by HDC [33], [35], [38], [40], [42], [45].\n\nBoth NNs and HDC map data to a HD space to perform learning and classification; however, this happens differently in each paradigm. In NNs, the HD space-the parameter space of all nodes in the network-is used to perform the mapping of inputs to classes, represented by final layer nodes. NNs learn the correct \"slice\" of this HD space such that inputs are mapped to the desired classes. In HDC, the mapping is predefined and the HD representational space-the space of all possible hypervectors under a given representational scheme and dimensionality, N -is not constrained or stored. In this case, it is only the points within the space corresponding to class centroids which are learned and stored. Points are represented as individual hypervectors.\n\nThe training costs and memory footprints of NNs are so high because the entire representational space is learned and stored. By learning and storing only the points in the representational space corresponding to classes, HDC is able to solve the same ML tasks with far simpler hardware, smaller memory footprints, and less training data than NNs [38], [40], [42].\n\nOne highly desirable feature of HDC is one-shot learning: the ability to learn a class from one or very few passes over the training data [38], [40], [42], [46]. For example, an HDC algorithm achieved 97.8% classification accuracy in one pass over 1 3 the training data required by the state-of-the-art support vector machine (SVM) on the same task [47].\n\nDue to the exceptional accuracies of cutting-edge deep neural networks (DNNs) [21], [23], [24], it is extremely challenging to out-perform them in terms of accuracy. This is particularly true for image classification tasks in which feature extraction capabilities are crucial for learning transformation invariance [40], [45], [48]. To overcome these challenges, state-of-the-art HDC approaches for such tasks build feature-extracting kernels into their encoding schemes mapping input data to the HD representational space [49], [50]. HDC is capable of achieving comparable accuracies to DNN models on applications including computer vision, speech detection, robotics, and others. [40], [43], [48], [51], [52], [53], [54].\n\nWe focus on the European language recognition task for our SHDC implementation presented in the following sections. The details of this task are discussed in Section III. The HDC algorithm implemented by our SHDC design achieves 96.7% accuracy on this task compared to the 97.9% accuracy achieved by a histogram-based nearest neighbor baseline classifier [36]. This algorithm makes use of binary hypervectors, the binary  spatter code representational scheme, \u223c1 MB training texts, \u223c1 kB evaluation texts, and generates language profiles based on consecutive groups of three characters (trigrams). It is discussed in detail in Sections III to V.\n\nA high-level overview of a HDC ML architecture for language recognition is shown in Fig. 1. It is composed of two modules, the encode module (EM) and the associative search module (ASM). The EM projects input data to an N-dimensional hypervector in the representational space, termed the profile vector of the input data. During training when the class of the profile vector is known, it is written to one of the M dedicated item memory nodes in the ASM to form a new class. When the class of the profile vector is unknown, it is termed a query vector and searched for across the M classes stored in the ASM in a nearest-neighbor search.\n\nThere are many possible encoding schemes for HD vectors ranging from simple binary and bipolar representations to much more complex representations making use of floating point and imaginary numbers [37], [38], [40]. For simplicity, we use binary hypervectors and the binary spatter code [39] in our implementation following that of [36]. Fig. 2 shows how binary hypervectors are represented as pulse streams in our design. For binary hypervectors, Hamming distance is used as the similarity function. Hamming distance is a simple L1 distance measure consisting of the number of elements at which two vectors differ [33]. The Hamming distance between two vectors A and B is calculated as SUM(XOR(A,B))-the sum of the element-wise XOR, the difference vector, of A and B (6 in Fig. 2). Such computations are exceedingly simple compared to the complex matrix multiplication-accumulation operations of NNs.\n\nThe vector primitives of HDC-multiply, add, permute (abbreviated as MAP)-enable the association of representations in the HD space via superposition while still allowing the individual component representations to be recovered [33], [36], [37]. The representational space is closed under the MAP primitives [38]. For binary hypervectors, the MAP primitives are implemented as element-wise XOR (multiply), element-wise thresholded accumulation (add), and the wrapped shifting of all elements of the vector +1 index (permute) [36]. For binary hypervectors, addition must be thresholded to return the resulting integer-valued vector elements to '1' or '0'. As the permute function is simply a shift in the indexing of vector elements, it is performed solely through wiring, requiring no extra hardware. This is demonstrated in the green portion of Fig. 3 for the letter hypervectors: here the letter 1 hypervector is permuted twice and the letter 2 hypervector is permuted once. See Section IV for details.\n\n\nIII. MACHINE LEARNING BENCHMARK TASK\n\nThe SHDC architecture we present can easily be generalized to almost any ML, pattern recognition, or classification task by varying the length (and thus representational capacity) of the hypervectors, the number of classes stored in the item memory, and the projection function implemented by the encode module.\n\nIn the following sections, we focus on the European language recognition task [36], [55], [56] and tailor our architecture to its specifications. The input data is a text stream consisting of only the Latin alphabet and the space character, and the desired output is the language of the text. There are 21 European languages in the dataset resulting in M = 21 memory classes (one for each language).\n\nAs our dataset is comprised of 27 features known a priori, we implement our projection function as a lookup table mapping each feature of the data (character) to a randomized seed vector which serves as its representation in HD space. This lookup table that maps features of the dataset to hypervectors is termed the seed memory. The details of how the hypervector representing an entire text stream is generated are discussed in detail below.\n\n\nIV. ENCODE MODULE\n\nWe begin by describing the encode module (EM) of our SHDC circuit, shown on the left in Fig. 3. The EM implements the projection function mapping input data to the N-dimensional representational space. The letters of the input text are received one at a time and encoded into hypervectors representing groups of three consecutive letters-termed trigrams-by the EM. Each letter as well as the space character is represented by a predefined N-dimensional seed vector stored in the seed memory of the EM. The seed memory is implemented as a look-up table; its architeture is shown in Fig. 4. All seed vectors are generated randomly and chosen to be approximately orthogonal to each other in the HD space in order to ensure all features of the dataset-characters in our application-have unique representations.\n\nDuring the encode stage, input letters are first mapped to their corresponding seed vectors by the seed memory, then permuted according to their position in the trigram. The first letter hypervector is permuted twice, the second once, and the  third not at all. This is to ensure that different combinations of the same letters have unique trigram hypervectors and thus representations within the HD space. As shown in Fig. 3, permutation is accomplished through the wiring between buffers as letter hypervectors propagate through them in FIFO order.\n\nNext, the permuted letter hypervectors are bound into a single trigram hypervector using two subsequent vector multiplication operations-implemented as XOR for binary vectors. Lastly, all trigram hypervectors of the text stream are accumulated elementwise into a single, integer hypervector such that element i if the integer hypervector is the sum of the ith element of all binary trigram hypervectors. This is accomplished with a vector of N accumulators, one per element. A threshold of t 2 (where t is the total number of trigrams in the text) is then applied to each element of the integer profile vector in order to return it to the binary representational space by converting all elements smaller than t 2 to 0 and all others to 1. The resulting binary hypervector represents the entire text and is termed its profile vector. For scalability, we use inductor-based accumulator cells with temporal result readouts in conjunction with temporal thresholding gates each implemented with a single non-destructive read out (NDRO) cell. Accumulators produce their outputs x cycles after the start of the read operation where x is the number of pulses accumulated. With this temporal formatting, the threshold function is implemented as a temporal inhibit signal sent t 2 cycles after the start of the read operation to block output from any accumulators that have not yet produced it. A DFF array is used to temporally synchronize the elements of the profile vector following the thresholding operation.\n\nHow the EM outputs the profile vector depends on whether the language (class) of the profile vector is known or not. The implementation of profile vector routing at the output of the EM is shown in Fig. 5(a). During training, when the language of the input text is known, its profile vector represents a learned class. In this case the profile vector-termed a memory vector-will be sent to the writelines output of the EM (wls) were it will written to one of the M memory nodes (IMVNs) of the ASM, forming a new class. Each element of the hypervector is represented on its own wire for memory write operations, so the wls signal consists of N , 1-bit writelines. The ASM's sel control signals select the memory node to store the profile vector in, with sel [i] indicating the write should be preformed on IMVN i. When the language of the input text is unknown, its profile vector-termed a query vector-is sent to the ASM for classification. In this case, the profile vector is directed to the query vector output (q_vec) for distribution to all M memory nodes of the ASM to perform the search. Fig. 5(b) shows how the N -bit query vector is formatted as a single pulse stream of length N . This is accomplished using N readlines (rls) to read the query vector out in a serial pattern such that the ith element of the query vector is read in cycle i of the read operation. A merge tree with a fan-in degree of N then merges the N individual query vector element reads into a single, N -cycle pulse stream where the ith element of the vector is represented in the ith cycle of the pulse stream. Memory vector read operations within the IMVNs of the ASM are also performed as shown in Fig. 5(b) to format memory vectors into pulse streams. Read operations in the IMVNs use the same readline control signals as the EM (with added pathbalancing logic) to ensure the memory and query vector pulse streams are temporally synchronized within the IMVNs.\n\n\nV. ASSOCIATIVE SEARCH MODULE (ASM)\n\nIn this section we present the associative search module (ASM) of our SHDC circuit. The ASM consists of M item memory vector nodes (IMVNs) and an M -argument comparator. Each IMVN stores a single class hypervector, calculates its Hamming distance from input query vectors, and outputs said distance as a k-bit binary number where k = ceil(log 2 (N )). The comparator accepts the M , k-bit Hamming distances and selects the minimum, outputting the index of the corresponding class, indicating the best match to the query vector. The implementation and operation of the ASM are discussed below and shown on the right in Fig. 3. The implementation of the IMVNs of the ASM is shown in Fig. 6; the IMVN control signals shown are shared by all M IMVNs of the ASM.\n\n\nA. Training\n\nDuring memory write operations, the profile vector to be written to memory is driven to the ASM by the N writelines (wls) output by the EM. The writelines are projected to all M IMVNs, the M sel control signals of the ASM select which IMVN the profile vector will be written to.\n\nEach IMVN has an N -element NDRO array to store its class vector. An N -element array of coincidence cells is used to gate inputs to the NDRO array to ensure the write operation is only performed to the IMVN indicated by the sel lines. Element i in the coincidence array of IMVN j accepts wls[i] as its data input, sel[j] as its select input, and eow as its reset input, used to signal the end of each write operation. The wls and eow signals are sent to all IMVNs for every write operation but only one of the M sel signals will fire during any given write so only that IMVN will have its memory written to.  (Fig. 6) implement the distance calculations of HDC. During search operations, the query vector is projected to all M IMVNs which perform their distance calculations in parallel. As mentioned previously, the query vector is output from the EM as a single, N -cycle pulse stream. The memory vectors stored in the IMVNs are read out in the same format using the same readline signals rls for temporal synchronization with the query vector read (Fig. 5(b)).\n\n\nB. Classification\n\n\n1) Item Memory Vector Nodes (IMVNs): IMVNs\n\nWith query and memory vectors both represented as N -cycle pulse streams, their difference vector can be calculated using a single XOR gate (see Figs. 2 and 6). The difference vector pulse stream forms the input to the synchronous TFF counter. The counter accumulates all pulses of the difference vector-the Hamming distance between the query and memory vectors-over an accumulation period of N cycles (the temporal length of the difference vector pulse stream) and outputs this distance as a binary number. The maximum Hamming distance possible for vectors of length N is N , thus the bitwidth of the TFF counter is k = ceil(log 2 (N + 1)). A read signal (cntr_rd) is used to gate output from the counter to ensure it only produces output once the accumulation is complete.\n\nAs the counter is synchronous, the output of each bit stage is offset by a single cycle. Therefore, the counter read signal (cntr_rd) must follow the end of the accumulation period by k cycles to allow the LSB value to be fully updated before the read. The cntr_rd signal is shared by all IMVNs in order to synchronize their outputs.\n\n2) Comparator: Each of the M IMVNs generates the Hamming distance between its stored memory vector and the query vector as a k-bit binary number. At the output of the ASM is a M -way k-bit tree comparator which selects the minimum of these Hamming distances and outputs the index of the corresponding IMVN-the index of the best matching class-as a binary  number. To minimize design footprint, all k-bit comparators are implemented with a single 1-bit comparator which is used to compare arguments one bit at a time from MSB to LSB, stopping after an inequality is found or all bits have been compared.\n\n\nVI. METHODOLOGY\n\nWe evaluate our SHDC architecture using a combination of WRspice, PyRTL, and analytical models. We use a cell library derived from the SUNY RSFQ cell library [4] with the exception of the NDRO cell which is from [57]. See Table V for a list of cells. We extracted the parameters for all gates in our library by simulating them at the circuit level in WRspice [58], an opensource SPICE simulator, using the open-source MIT-LL SFQ5ee 10 kA/cm2 process [59]. We used our SPICE models to extract the JJ counts, critical currents, bias currents, propagation delays, hold times, and set-up times for all gates in our cell library.\n\nIn order to simulate and verify our design at scale, we built RTL models of our architecture as well as its individual modules in PyRTL [60], a python library for RTL design, simulation, tracing, and testing built on Verilog. We parameterize our RTL models with the gate parameters extracted from our circuit-level WRspice simulations in order to verify both the functional and timing correctness of our design as well as obtain latency, area, and power results for our full-scale models. We explicitly model all control, signal distribution, and clock tree hardware to ensure our designs are properly path-balanced, there are no timing violations, and our results are accurate.\n\nLatency results for our design are obtained directly from the parameterized, ps-scale RTL simulations of our designs. We extract gate counts from our RTL models and use these in tandem with the gate parameters extracted from our SPICE models to obtain area-in terms of JJ counts-and power consumption results for our designs.\n\nTo obtain mm 2 area estimates for our SHDC architecture and its component modules under the MIT-LL SFQ5ee 10 kA/cm2 manufacturing process, we use the gate count data of our designs in combination with MIT Lincoln Lab (MIT-LL) JJ area data which gives the physical area of JJs based on their critical currents. We calculate the \u00b5m 2 area of all gates in our cell library from the JJ area data and the critical current of each JJ in each gate taken from their circuit-level implementations. We obtain the physical area of our designs using this gate area data in tandem with our module gate count data. We also introduce a 3\u00d7 area overhead to estimate the effect of non-JJ components as well as routing and layout restrictions.\n\nTo obtain power consumption results, we wrote a python library to calculate the dynamic and static power consumption of each cell in our library given the critical currents of its JJs and bias resistances, as well as the bias voltage and operating frequency of the chip. All modules in our design use the same bias voltage of 10 mV. Table V shows the per-gate static and dynamic power consumption values for all cells in our library at an operating frequency of 33.3 GHz and the gate-wise power consumption breakdown of our full SHDC circuit for N = 1 k.\n\nIt is worth noting that static power dissipation dominates the total power consumption of RSFQ circuits due to the large amount of power dissipated by the bias resistors [61], [62]. The static power dissipation of the cells in our library is \u223c 10 2 times higher the dynamic power dissipation at the chip's operating frequency of 33.3 GHz (see Table V). In recent years, energy-efficient rapid single flux quantum (ERSFQ) circuits have emerged as a highly promising alternative to traditional RSFQ circuits. In this technology, bias resistors are replaced with bias JJs, completely eliminating static power dissipation [61], [62]. The timing characteristics and physical area of ERSFQ gates are assumed to be the same as those of their RSFQ counterparts as their gate structures are the same, just the bias current supply lines differ [15], [61].  Following the methodology in [15], to calculate the power consumption of our design's implementation in ERSFQ technology, we estimate that ERSFQ gates dissipate twice the dynamic power of their RSFQ counterparts and zero static power. ERSFQ power consumption results for our SHDC design are shown in Table VI for the language recognition benchmark (M = 21) and Table VIII for the ImageNet benchmark (M = 1 k). All other results are for the RSFQ implementation of our design.\n\nWithout exception, all power calculations given in this paper for our SHDC design assume a worst-case activity factor, meaning each JJ within every gate of our circuit is assumed to be accessed every clock cycle. This is a large over-assumption, however, it effects only the dynamic power consumption. Therefore, the impact to RSFQ total power consumption is minimal but the impact to ERSFQ total power consumption values is substantial since all power dissipation is dynamic in this technology.\n\nFor both RSFQ and ERSFQ implementations, we calculate cooling power consumption as 395 W of cooling power per W of chip power for a helium reliquifier refrigeration system [63].\n\n\nVII. EVALUATION\n\nWe assess the performance of our SHDC architecture for vector lengths in the range N = 1 \u2212 10 k as this spans a typical range of vector lengths used in most HDC applications [33], [38], [40]. As mentioned above, all power numbers reported for our SHDC circuit assume a maximum activity factor, meaning every JJ is assumed to activate every clock cycle. Additionally, control, signal amplification, and clock tree hardware are accounted for in all results. All SHDC results are for the European language recognition task (M = 21) unless otherwise indicated. Power results are for the RSFQ implementation of our circuit and include cooling power unless otherwise indicated. Area percentages are based on JJ count areas as described in the previous section.\n\nThe maximum operating frequency of the SHDC associative memory chip is 33.3 GHz, set by the IMVN. The only module that operates at a different frequency is the comparator at the output of the ASM which uses a self-clocking scheme and operates at 6.67 GHz with worst-case latency. See Section VII-B below for details. The area and power consumption of our SHDC design broken down to its individual modules as well as memory versus logic are shown in Figs. 7 and 8 respectfully. Individual gate contributions to area and power consumption totals for N = 1 k are shown in Table V. The area scaling results for SHDC as well as its component modules in terms of both JJ count and mm 2 are shown in Table IV. Performance and power scaling of our design are shown in Tables II and III respectively. ERSFQ power consumption results for our SHDC circuit are given in Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. \n\n\nA. Encode Module\n\nTwo factors affect the area of the EM, predominantly the length of hypervectors (N ) and to a lesser extent, the length of the input text. As the vast majority of the EM is comprised of gate arrays of length N , its area scales approximately linearly with N . The length of the text being encoded in trigrams (groups of three consecutive characters), t, also has a minor effect on EM area as the accumulators must be adequately sized to hold up to t pulses-one per trigram hypervector.\n\nSized for 1 kB training texts, the EM comprises 47-49% of the area of the full design over a range of N = 1 \u2212 10 k. The seed memory (Fig. 4) comprises 72% of the total EM area over this range. The power consumption of the EM is proportional to its area. Operating at 33.3 GHz, the EM accounts for 42-44% of the power consumption of the full SHDC design over a range of N = 1 \u2212 10 k with the seed memory accounting for 64% of the EM's total power consumption. See Figs. 7 and 8.\n\nDue to the fact that all bits of hypervectors are operated on in parallel in the encode operation implemented by the EM (see Fig. 3), the EM's cycle-based latency is determined solely by the number of trigams in the input text, t. Each trigram hypervector takes four cycles to encode, resulting in 4t cycles to accumulate all trigram hypervectors. The temporal accumulation and thresholding operations-performed simultaneously-take t 2 cycles. Thus, the total latency of the EM-the projection of a t-trigram input text to HD space-takes 4.5t cycles.\n\n\nB. Associative Search Module (ASM)\n\nThe ASM consists of the M IMVNs and the comparator used to select the minimum distance class. The ASM comprises 51-53% of the design footprint and 56-58% of the power consumption for vector lengths ranging from N = 1 \u2212 10 k. The item memory of the ASM, consisting of all in-node memories (Fig. 6) of the M IMVNs, comprises 79-85% of the ASM's total area and 68-73% of its total power consumption over this range. The total area of the IMVNs comprises 83-85% of the ASM footprint and the comparator comprises 3.7-0.5% over the same range of vector lengths. The remainder of the area is signal amplification and control circuitry hardware. As all IMVNs perform their distance calculations in parallel, the total latency of the ASM is the sum of the IMVN and comparator latencies, discussed in detail below.\n\n1) Item Memory Vector Nodes (IMVNs): IMVN area scales approximately linearly with the vector length, N . The area of the TFF counter scales proportionally to log 2 (N ); however, for large values of N , IMVN area becomes dominated by the in-node memory (Fig. 6) which scales proportionally to N . The in-node memory comprises 95-99% of the area and 94-99% of the power consumption of the IMVNs over the range of N = 1 \u2212 10 k. The total area of the IMVNs dominates the area of the ASM at all scales of N .\n\nAs discussed in Section V-B, with both the query and memory vectors formatted into N -cycle pulse streams, it takes N cycles to compute their difference vector using a single XOR gate. The difference vector pulse stream is accumulated by the TFF counter as it is generated. Due to the synchronous nature of the TFF counter, its read must occur k cycles after the completion of the accumulation where k = log 2 (N ) is the bitwidth of the counter. Thus the total latency of the IMVNs is N + k cycles. The IMVN distance calculation latency comprises 78-96% of the total ASM classification latency over vector lengths in the range N = 1 \u2212 10 k.\n\n2) Comparator: The area of the binary tree comparator discussed in Section V-B scales approximately linearly with the number of arguments M and to a lesser extent logarithmically with the argument bit width, k = log 2 (N ). Because k-bit comparators are implemented using 1-bit comparators, the latency of the binary comparator is logarithmically proportional to both the number of arguments, M , and the length of the vectors, N . The latency of the k-bit comparators depends entirely on the bit at which the arguments differ, the worst case being equal arguments which take k + 1 cycles to compare.\n\nIn order to minimize latency, we use a self-clocking scheme for both the k-bit comparator and the M -way tree comparator in which the result of the slowest comparison at each stage is used to initiate the comparison(s) in the next stage. This self-clocking scheme enables us to capitalize on the latency variability of the comparison operations by triggering the next clock cycle as soon as the current cycle's computation is complete. The maximum length of a single comparison cycle is 150 ps corresponding to a minimum operating frequency of 6.67 GHz. With worst-case latency-maximum number of comparison cycles at maximum cycle length-the 21-way k-bit comparator comprises 3.6-21% of the total latency of the ASM over vector lengths in the range N = 1 \u2212 10 k.\n\nAs the comparator is used to select the winning class at the output of the ASM, it is the last module in our SHDC circuit meaning its self-clocking scheme does not cause synchronization issues with the rest of the circuit. We assume worst-case latency for the comparator-6.67 GHz operating frequency-for all results given in this article.\n\n\nC. Full SHDC Design 1) Memory Footprint:\n\nIn HDC designs, only the S seed vectors of the seed memory (Fig. 4) and the M class vectors of the item memory (Fig. 6) must be stored. With binary vectors of length N , this totals to N * (S + M ) bits. For the given task, we have S = 27 and M = 21 for a total memory footprint of only 48 N bits. In both the seed and item memories, each bit of memory is implemented with a single NDRO gate. Over the range N = 1 \u2212 10 k, the memory NDROs comprise 37% of the item memory and 58% of the seed memory with the rest of the area being comprised of read and write control circuitry. Despite the small memory requirements of the SHDC architecture, the combined seed and item memory footprints still comprise 75-78% of the full design footprint due to the simplicity of the computational logic of HDC.\n\n2) Performance: Performance is expressed in terms of millions of classification operations per second (MCO/s). Results are shown in Table II. Each classification consists of an encode operation-performed by the EM-followed by a search operation-performed by the ASM. These two stages can be overlapped, thus the overall throughput of the circuit is set by the throughput of the slowest stage.\n\nThe latency of the EM is proportional to the length of the input text being encoded. With 1 kB training texts, operating at 33.3 GHz, the EM can perform 22.24 million (M) encode operations per second for all values of N . The latency of the ASM is proportional to N so its throughput decreases as vector length increases. The throughput of the ASM becomes the limiting factor on overall throughput for N > 1 k. See Table II.\n\n\nD. SHDC Fabrication Bounds\n\nAs discussed in Section I, current RSFQ fabrication limits depend on the complexity of the circuit in question, lying at \u223c20-30 k JJs for logically complex designs [9] and over 1 M JJs for highly-regular designs [6], [7], [8]. Although much of the SHDC design consists of large logic gate and memory arrays with highly regular structures, its control circuitry and the comparator module are fairly complex. As such, one would expect the actual fabrication limits for SHDC to lie close to but likely below 1 M JJs. Thus, we use \u223c1 M JJs as the upper bounds for the design scales at which SHDC and its component modules are realistically fabricatable.\n\nThe upper bounds (\u223c1 M JJs) of design scales for which SHDC and its component modules are realistically fabricatable with both on-chip memory (total area) and off-chip memory (logic area only) are shown in bold in Table IV. HDC applications are practically useful at vector lengths of N \u2265 500 [33], [38], so as long as the vector length (N ) for which a module is realistic to fabricate lies at or above N = 500, we consider the module fabricatable at practically useful scales. The EM and ASM with both on-and off-chip memory, as well as the full SHDC design with off-chip memory are all realistic to fabricate at practically useful scales of N \u2265 1000. It is possible to fabricate individual IMVNs at practically useful scales even using the lower bounds of 20-30 k JJs. Although it is not shown in the table, the full SHDC architecture with on-chip memory is also realistic to fabricate at a practically useful scale of N = 500 having 995 k JJs, although it lies close to the upper bound.\n\nTo implement SHDC at scales beyond N = 500, one could fabricate the component modules of SHDC on separate chips to create multi-chip modules connected by transmission lines with SFQ pulses. With the EM and ASM each fabricated on their own chip and connected in this manner, SHDC could be implemented for scales of up to N = 1000 with on-chip memory and N = 4000 with off-chip memory.\n\n\nE. CMOS Comparison\n\nTo facilitate a comparison of our SHDC design with an equivalent CMOS circuit, we benchmark our circuit against the 2D CMOS HDC circuit architecture presented in [36] for the European language recognition task [55], [56]. Recall that there are 21 classes in this task (M = 21). This design implements the same algorithm as our SHDC design using the TSMC 65 nm LP CMOS process.\n\n1) Performance: The study of [36] for a CMOS HDC design focuses on the ASM module and in particular the IMVNs. As in our SHDC IMVNs, the CMOS IMVNs presented in [36] use a single XOR gate to compare one element of the query and memory vectors at a time, requiring O(N) cycles to compute the Hamming distance between the two hypervectors. In order to compare against [36], we assume zero latency for the circuitry that is not described in the CMOS design, namely the EM and combinational comparator used in the ASM. We assume that the CMOS distance calculation takes exactly N cycles. Thus, the latency of the CMOS design given here is optimistic even for the distance calculation alone. Our SHDC design's latency includes all components. As such, this is a highly favorable assumption for the CMOS design.\n\nSince performance metrics are not reported in [36], we assume the CMOS design runs at an aggressive operating frequency of 5 GHz and that the entire classification requires N cycles. Even under these conditions, our SHDC design outperforms the CMOS HDC design by 78-84% for vector lengths in the range of N = 1 \u2212 10 k. Even assuming the CMOS HDC design can operate at a frequency of 10 GHz, our design still outperforms it by 55-69% over this range.\n\nGiven the aggressiveness of our latency-based assumptions in favor of the CMOS design, the gains given above are entirely due to the faster operating frequency of SHDC. If we were taking into account the latency introduced by the CMOS comparator, which accounts for 3.6-21% of the SHDC ASM's latency, we would also expect to see cycle-based gains from the self-clocking scheme utilized by the SHDC comparator.\n\n2) Power Consumption: Table VI shows how the power consumption of both RSFQ and ERSFQ implementations of our SHDC design with and without cooling cost compare to that of the CMOS HDC benchmark design over the range N = 1 \u2212 10k. Power consumption results for the CMOS HDC design are taken from Fig. 5 in [36]. \n\n\nF. SuperNPU Comparison\n\nIn this section we evaluate SHDC's performance as a superconducting ML technique against that of the state of the art SNNA: SuperNPU. The NNs used to benchmark SuperNPU were designed for use with the ImageNet dataset [64] which contains images belonging to a total of 1000 classes. Tailoring our SHDC design to this benchmark, the ASM contains M = 1000 IMVNs representing the 1 k classes and a 1 k-way comparator to select the winning class.\n\nAs the input data consists of images for the ImageNet task, it makes more sense to use a pixel-value based projection function to perform the mapping of images to the HD space than a look-up table mapping data features to predefined seed vectors as we do not know the features of the dataset a priori.\n\nUnder this framework, during training input images of a given class would be projected to hypervectors one at a time based on their pixel values and then accumulated into a single profile hypervector representing the class just as with the trigram hypervectors for the language recognition task in what is termed an N-gram encoding [48]. However, for classification, the input is a single image so no accumulation stage is required; the image is mapped to the HD space and sent directly to the ASM for classification.\n\nThe area of the EM scales only with hypervector length (N ) and the number of object hypervectors in a class (t) but not with the number of classes (M ). Therefore, we would expect an EM implementing a simple pixel-based projection function to be roughly the same scale of complexity as the EM for the language recognition task presented above, given that it does not include a seed memory which comprises \u223c 37% of the number of JJs of the language recognition EM. To be conservative, we estimate that the EM for the ImageNet task has the same area (in terms of JJ count) and power consumption of the M = 1 k ImageNet ASM. Recall that the EM is smaller than the ASM for the language recognition task for which M = 27. As M = 1 k for the ImageNet task and the area of the EM does not scale with M while that of the ASM does, this is likely a significant over-estimate.\n\n1) Memory Requirements: The number of parameters for five popular NN architectures used to benchmark SuperNPU are shown in Table VII. All parameters are quantized to 8 bits in SuperNPU; the resulting memory requirements of each network under this quantization scheme are shown in the same table. Network parameters are stored in off-chip DRAM in the SuperNPU architecture. Additionally, SuperNPU uses two buffers of 24 MB each to store partial calculation results and an additional buffer of 128 kB to hold the network parameters currently in use for a total of 48.128 MB of on-chip memory (see Table I in [15]).\n\nWith M = 1000 classes and vectors of length N , our SHDC architecture requires N \u00d7 M = 1000 N bits of memory to store the 1 k binary class vectors in the ASM. As the EM does not contain a seed memory for this application, the item memory of the ASM comprises the memory footprint of the entire design. For vectors in the range N = 1 \u2212 10 k, the total memory requirements of SHDC's architecture for ImageNet would be 1 \u2212 10 Mb. Even at the extreme of N = 10 k, the memory requirements of SHDC are still smaller than those of even the smallest NN architecture, MobileNet (see Table VII).\n\n2) Area: The mm 2 area results given for SuperNPU in [15] (\u223c 299 mm 2 ) assume the JJ device technology used to implement SuperNPU is equivalently scaled to a 28 nm CMOS technology, which was used to benchmark the TPU [15]. To avoid making device technology assumptions for our design, we compare area against SuperNPU in terms of JJ counts.\n\nAs no JJ count numbers are given in the SuperNPU paper [15], we estimate them here given the mircroarchitectural details presented in their paper in combination with their cited multiplier and adder implementations [9], [65]. SuperNPU consists of a 64 \u00d7 256 processing element (PE) array, 48.128 MB of on-chip memory implemented with shift registers, and a data alignment unit (see Figs. 3 & 19 and Table I in [15]). The PEs each contain a 20.3 k JJ 8-bit pipelined multiplier [9], a 3 k JJ adder [65], and 8 shift registers. Ignoring the shift registers, control circuitry, and clock distribution hardware, each PE uses \u223c23 k JJs. Thus, there are 64\u00d7256\u00d723 k JJs = \u223c377 M JJs total in the PE array. Again ignoring the control circuitry and clock distribution hardware, we will assume the shift-register based on-chip memory requires only a single JJ per bit. Under these assumptions, we have 48.128 MB \u00d7 8 b/B = \u223c385 M JJs in the on-chip memory. We ignore the data alignment unit giving an optimistic total of 377 M + 385 M = 762 M JJs for the SuperNPU architecture.\n\nWith M = 1 k for the ImageNet task, our SHDC design uses 96 M JJs at small design scales (N = 1 k) and 892 M JJs at large scales (N = 10 k). SHDC has a similar footprint to SuperNPU at large design scales, however, it makes use of entirely on-chip memory while SuperNPU requires additional off-chip DRAM.\n\nAs the area of SHDC scales proportionally to both the length of hypervectors used (N ) and the number of classes (M ), its area does not scale as favorably as that of SNNAs when the number of classes becomes very large. SNNAs have a fixed size and instead suffer in terms of latency when accelerating very large NN models.\n\n3) Performance: SuperNPU is reported to operate at 52.6 GHz [15]. Its performance in terms of millions of classifications per second (MC/s) for five popular NN architectures is shown in Table VII. The performance numbers given here assume that the SuperNPU architecture performs all computations at its peak performance of 842 TMAC/s. However, the actual performance of the SuperNPU architecture depends on the percent of the PE array being utilized for each MAC operation which depends on the architecture of the network being accelerated [15]. SuperNPU cannot maintain peak performance  for the entirety of the computations for any of the NNs shown here so these performance numbers are optimistic in favor of SuperNPU.\n\nFor classification, input data will consist of only one image at a time, meaning the encode operation will be a single image projection operation without the accumulation and thresholding of multiple images into a single hypervector that is required during training. Thus, the latency of the EM will be minimal compared to that of the ASM even at small vector lengths, meaning the performance of the design will be set by the ASM. Assuming maximum latency for the comparator, the ASM can perform 21.37-3.1 MC/s over vector lengths ranging from N = 1 \u2212 10 k. Fig. 9 shows how the performance of our SHDC architecture compares to that of SuperNPU running the five NN architectures from  [15]. The RSFQ implementation of SHDC consumes approximately nine times less power than SuperNPU at small scales (N = 1k) and about the same amount as SuperNPU at large scales (N = 10k). The ERSFQ implementation of SHDC consumes approximately the same amount of power as SuperNPU at small scales but about fourteen times more at large scales. The discrepancy between RSFQ and ERSFQ energy savings of SHDC compared to SuperNPU is likely due to our assumption of the worst-case activity factor for the SHDC design; recall this has a large impact on ERSFQ power consumption results as all power dissipation is dynamic in this technology.\n\nTo summarize, tailored to the ImageNet ML task with 1 k classes, SHDC at large design scales (N = 10 k) has similar area and power consumption metrics to SuperNPU while achieving 62-99% higher throughput for all but the smallest NN architecture accelerated by SuperNPU, MobileNet. At small design scales (N = 1k), SHDC outperforms SuperNPU in terms of throughput by 48% on MobileNet and over 94% on all other benchmarks while occupying approximately 90% less area and consuming up to nine times less power.\n\n\nVIII. CONCLUSION\n\nGiven the extreme computational complexity, memory requirements, and training costs of NNs, such ML approaches are not ideal for implementation in superconducting digital technologies due to the area limitations arising from limited device density and the lack of scalable cryogenic memory solutions. Additionally, superconducting NN accelerators (SNNAs) are only designed to implement the forward pass of NNs and must be initialized with pretrained, quantized NN parameters, making them fundamentally incapable of online learning.\n\nIn contrast, hyperdimensional computing (HDC) uses simple computations and drastically less memory than SNNAs. Despite the small memory footprint, the computational logic of HDC is so simple that the design footprint is still dominated by memory. Furthermore, training and classification are the same under HDC, the only difference being in whether a vector is written to memory or searched for across the vectors already stored in memory. As such, training is exceedingly simple and can easily be performed online. This allows SHDC to continue learning after deployment in cryogenic environments, greatly increasing its adaptability and robustness.\n\nHere we present the first superconducting HDC (SHDC) design and evaluate it against an analogous CMOS design as well as the state of the art SNNA, SuperNPU. As we have shown, the proposed SHDC circuit uses entirely on-chip RSFQ memory which is tightly integrated with logic, operates at 33.3 GHz, is applicable to general ML tasks, and is manufacturable at practically useful scales given current SFQ fabrication limits. SHDC consumes up to three times less power than an analogous CMOS HDC circuit and achieves 78-84% higher throughput. SHDC is capable of achieving 48-99% higher throughput than SuperNPU while occupying approximately 90% less area and consuming up to nine times less power.\n\nTo the best of the author's knowledge, SHDC is the only superconducting ML approach that is currently feasible at practically useful scales for real-world ML tasks and capable of online learning. Given the much greater maturity of semiconductor digital computing technologies over superconducting ones as well as the restrictions that come with cryogenic computing environments-predominantly the cost of cooling-SHDC is certainly not the best approach for all ML applications; however, we argue that it is a superior option for implementing ML in superconducting digital computing.\n\nSHDC stands poised to make significant impact in any superconducting environment in which ML is required, especially given the fact that these systems are already cryogenic, meaning there is no cooling cost overhead in such environments. The on-chip SFQ memory, extreme noise and error tolerance, and high throughput performance of SHDC are particularly desirable for implementing ML solutions in these environments. Quantum control and error correction are two such key areas of active research.\n\n\nIX. FUTURE WORK\n\nThe binary magnitude comparator implementation used in our SHDC design is rather basic, contributing a significant amount of area and latency to our design. This is particularly true at small design scales where the distance calculation latency is low and signal amplification and clock distribution hardware comprise a smaller proportion of the design. Using a more intelligently designed SFQ comparator could significantly decrease the area and latency of SHDC, especially at small design scales.\n\nUnder temporal logic schemes such as Race Logic [66], values are encoded in the temporal domain and MIN and MAX are firstorder functions with highly simple implementations. This makes the implementation of distance-based computations that are at the heart of HDC extremely efficient. Our SHDC architecture stands to benefit greatly from the use of such a temporal logic scheme for the implementation of the distance calculations and comparisons of HDC.\n\nFig. 1 .\n1Overview of HDC for language recognition.\n\nFig. 2 .\n2Hypervector pulse stream representation.\n\nFig. 3 .\n3HDC language recognition architecture.\n\nFig. 4 .\n4Seed memory implementation. Each element of each seed vector is stored in its own non-distructive read out (NDRO) cell. All elements of the selected seed vector are read out in parallel (simultaneously) to their designated output wire; i.e. element i of the selected seed vector is driven to wire sv[i].\n\nFig. 5 .\n5(a) Encode module (EM) output routing. (b) Vector pulse stream formatting.\n\nFig. 6 .\n6Item memory vector node (IMVN) implementation. In-node memory used to store the memory vector for class i is shown in purple. Distance calculation logic is shown in black.\n\nFig. 7 .\n7Superconducting HDC (SHDC) area scaling.\n\nFig. 8 .\n8RSFQ SHDC power scaling. Includes 395 W/W of cooling power. the CMOS and SuperNPU comparison sections and shown inTables VI and VIII.\n\nFig. 9 .\n9SHDC SuperNPU performance comparison. The performance of our SHDC architecture designed for use with the ImageNet dataset at small (N = 1 k) and large (N = 10 k) design scales is shown in blue. SuperNPU's performance on five NN architectures is shown in orange assuming the peak performance of 842 TMAC/s is maintained throughout the computation.\n\nTABLE I GLOSSARY\nIOF HDC TERMS by memory\n\nTABLE II\nIISHDC PERFORMANCE SCALING \n\nTABLE III \nSHDC POWER CONSUMPTION (W) \n\n\n\nTABLE IV SHDC\nIVAREA SCALING\n\nTABLE V SHDC\nVGATE-WISE POWER CONSUMPTION BREAKDOWN (M=21, N=1 K)\n\nTABLE VI CMOS\nVIPOWER CONSUMPTION COMPARISON (W)\n\nTABLE VII MEMORY\nVIIFOOTPRINT AND PERFORMANCE OF SUPERNPU VERSUS SHDC FOR IMAGENET\n\nTABLE VIII POWER\nVIIICONSUMPTION (W) OF SUPERNPU VERSUS SHDC FOR IMAGENET\n\n\nTable VII at its peak performance of 842 TMAC/s. 4) Power Consumption: The power consumption results for both RSFQ and ERSFQ implementations of the SuperNPU and SHDC architectures with and without cooling power are shown in Table VIII. SuperNPU power consumption results come directly fromTable III of\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\n\nA case for superconducting accelerators. S S Tannu, P Das, M L Lewis, R Krick, D M Carmean, M K Qureshi, Proc. 16th ACM Int. Conf. Comput. Front. 16th ACM Int. Conf. Comput. FrontNew York, NY, USAS. S. Tannu, P. Das, M. L. Lewis, R. Krick, D. M. Carmean, and M. K. Qureshi, \"A case for superconducting accelerators,\" in Proc. 16th ACM Int. Conf. Comput. Front., New York, NY, USA, 2019, pp. 67-75. [Online].\n\n. 10.1145/3310273.3321561Available: https://doi.org/10.1145/3310273.3321561\n\nSuperconducting RSFQ logic: Towards 100 GHz digital electronics. V Michal, E Baggetta, M Aurino, S Bouat, J.-C Villegier, Proc. 21st Int. Conf. Radioelektronika. 21st Int. Conf. RadioelektronikaV. Michal, E. Baggetta, M. Aurino, S. Bouat, and J.-C. Villegier, \"Super- conducting RSFQ logic: Towards 100 GHz digital electronics,\" in Proc. 21st Int. Conf. Radioelektronika, 2011, pp. 1-8.\n\nSuperconductive single flux quantum logic devices and circuits: Status, challenges, and opportunities. M Pedram, Proc. IEEE Int. Electron Devices Meeting. IEEE Int. Electron Devices Meeting25.7.1-25.7.4.M. Pedram, \"Superconductive single flux quantum logic devices and circuits: Status, challenges, and opportunities,\" in Proc. IEEE Int. Electron Devices Meeting, 2020, pp. 25.7.1-25.7.4.\n\nSuny RSFQ cell library. Accessed\"Suny RSFQ cell library,\" Accessed: Mar. 28, 2023. [Online]. Available: http://www.physics.sunysb.edu/Physics/RSFQ/Lib/contents.html\n\nSuperconductor digital electronics: Scalability and energy efficiency issues (review article). S K Tolpygo, Low Temp. Phys. 425S. K. Tolpygo, \"Superconductor digital electronics: Scalability and en- ergy efficiency issues (review article),\" Low Temp. Phys., vol. 42, no. 5, pp. 361-379, May 2016. [Online]. Available: https://doi.org/10.1063% 2F1.4948618\n\nNew AC-powered SFQ digital circuits. V K Semenov, Y A Polyakov, S K Tolpygo, IEEE Trans. Appl. Supercond. 2531301507V. K. Semenov, Y. A. Polyakov, and S. K. Tolpygo, \"New AC-powered SFQ digital circuits,\" IEEE Trans. Appl. Supercond., vol. 25, no. 3, Jun. 2015, Art. no. 1301507.\n\nAC-biased shift registers as fabrication process benchmark circuits and flux trapping diagnostic tool. V K Semenov, Y A Polyakov, S K Tolpygo, no. 1301409IEEE Trans. Appl. Supercond. 274V. K. Semenov, Y. A. Polyakov, and S. K. Tolpygo, \"AC-biased shift registers as fabrication process benchmark circuits and flux trapping diagnostic tool,\" IEEE Trans. Appl. Supercond., vol. 27, no. 4, Jun. 2017, Art, no. 1301409.\n\nIncreasing integration scale of superconductor electronics beyond one million Josephson junctions. S K Tolpygo, V K Semenov, 10.1088/1742-6596/1559/1/012002Art. no. 012002J. Phys.: Conf. Ser. 15591S. K. Tolpygo and V. K. Semenov, \"Increasing integration scale of super- conductor electronics beyond one million Josephson junctions,\" J. Phys.: Conf. Ser., vol. 1559, no. 1, Jun. 2020, Art. no. 012002. [Online]. Available: https://dx.doi.org/10.1088/1742-6596/1559/1/012002\n\nA 48GHz 5.6 mW gate-level-pipelined multiplier using single-flux quantum logic. I Nagaoka, M Tanaka, K Inoue, A Fujimaki, Proc. IEEE Int. Solid-State Circuits Conf. IEEE Int. Solid-State Circuits ConfI. Nagaoka, M. Tanaka, K. Inoue, and A. Fujimaki, \"A 48GHz 5.6 mW gate-level-pipelined multiplier using single-flux quantum logic,\" in Proc. IEEE Int. Solid-State Circuits Conf., 2019, pp. 460-462.\n\nSMART: A heterogeneous scratchpad memory architecture for superconductor SFQ-based systolic CNN accelerators. F Zokaee, L Jiang, 10.1145/3466752.3480041Proc. IEEE/ACM MICRO-54: 54th Annu. Int. Symp. Microarchitecture. IEEE/ACM MICRO-54: 54th Annu. Int. Symp. MicroarchitectureF. Zokaee and L. Jiang, \"SMART: A heterogeneous scratchpad memory architecture for superconductor SFQ-based systolic CNN accelerators,\" in Proc. IEEE/ACM MICRO-54: 54th Annu. Int. Symp. Microarchitecture, Oct. 2021, pp. 912-924. [Online]. Available: http://dx.doi.org/10.1145/ 3466752.3480041\n\nMachine learning: Trends, perspectives, and prospects. M I Jordan, T M Mitchell, Science. 3496245M. I. Jordan and T. M. Mitchell, \"Machine learning: Trends, perspectives, and prospects,\" Science, vol. 349, no. 6245, pp. 255-260, 2015. [Online].\n\n. https:/www.science.org/doi/abs/10.1126/science.aaa8415Available: https://www.science.org/doi/abs/10.1126/science.aaa8415\n\nOnline learning: A comprehensive survey. S C Hoi, D Sahoo, J Lu, P Zhao, Neurocomputing. 459S. C. Hoi, D. Sahoo, J. Lu, and P. Zhao, \"Online learning: A comprehensive survey,\" Neurocomputing, vol. 459, pp. 249-289, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S0925231221006706\n\nA quantitative study of deep learning training on heterogeneous supercomputers. J Han, L Xu, M M Rafique, A R Butt, S.-H Lim, Proc. IEEE Int. Conf. Cluster Comput. IEEE Int. Conf. Cluster ComputJ. Han, L. Xu, M. M. Rafique, A. R. Butt, and S.-H. Lim, \"A quantitative study of deep learning training on heterogeneous supercomputers,\" in Proc. IEEE Int. Conf. Cluster Comput., 2019, pp. 1-12.\n\nPipedream: Generalized pipeline parallelism for dnn training. D Narayanan, 10.1145/3341301.3359646Proc. 27th ACM Symp. Operating Syst. Princ. 27th ACM Symp. Operating Syst. PrincNew York, NY, USAD. Narayanan et al., \"Pipedream: Generalized pipeline parallelism for dnn training,\" in Proc. 27th ACM Symp. Operating Syst. Princ., New York, NY, USA, 2019, pp. 1-15. [Online]. Available: https://doi.org/10.1145/ 3341301.3359646\n\nSupernpu: An extremely fast neural processing unit using superconducting logic devices. K Ishida, Proc. IEEE/ACM 53rd Annu. Int. Symp. Microarchitecture. IEEE/ACM 53rd Annu. Int. Symp. MicroarchitectureK. Ishida et al., \"Supernpu: An extremely fast neural processing unit using superconducting logic devices,\" in Proc. IEEE/ACM 53rd Annu. Int. Symp. Microarchitecture, Oct. 2020, pp. 58-72. [Online]. Available: https://ieeexplore.ieee.org/document/9251979\n\nGradientflow: Optimizing network performance for large-scale distributed DNN training. P Sun, Y Wen, R Han, W Feng, S Yan, IEEE Trans. Big Data. 82P. Sun, Y. Wen, R. Han, W. Feng, and S. Yan, \"Gradientflow: Optimizing network performance for large-scale distributed DNN training,\" IEEE Trans. Big Data, vol. 8, no. 2, pp. 495-507, Apr. 2022.\n\nNeural networks with quantization constraints. I Hounie, J Elenter, A Ribeiro, 10.48550/arXiv.2210.156232022I. Hounie, J. Elenter, and A. Ribeiro, \"Neural networks with quantization constraints,\" 2022, doi: 10.48550/arXiv.2210.15623.\n\nTowards the limit of network quantization. Y Choi, M El-Khamy, J Lee, Proc. 5th Int. Conf. Learn. Representations. 5th Int. Conf. Learn. RepresentationsY. Choi, M. El-Khamy, and J. Lee, \"Towards the limit of network quan- tization,\" in Proc. 5th Int. Conf. Learn. Representations, 2017. [Online]. Available: https://arxiv.org/abs/1612.01543\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, 10.1145/3065386Commun. ACM. 606Association for Computing MachineryA. Krizhevsky, I. Sutskever, and G. E. Hinton, \"ImageNet classification with deep convolutional neural networks,\" Commun. ACM, New York, NY, USA: Association for Computing Machinery, vol. 60, no. 6, pp. 84-90, May 2017, doi: 10.1145/3065386.\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, 10.5555/2969239.2969250Proc. 28th Int. Conf. Neural Inf. 28th Int. Conf. Neural Inf1S. Ren, K. He, R. Girshick, and J. Sun, \"Faster R-CNN: Towards real-time object detection with region proposal networks,\" in Proc. 28th Int. Conf. Neural Inf. Process. Syst., vol. 1, 2015, pp. 91-99, doi: 10.5555/2969239.2969250.\n\nGoing deeper with convolutions. C Szegedy, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitC. Szegedy et al., \"Going deeper with convolutions,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 1-9.\n\nMobilenets: Efficient convolutional neural networks for mobile vision applications. A G Howard, arXiv:1704.04861A. G. Howard et al., \"Mobilenets: Efficient convolutional neural networks for mobile vision applications,\" 2017, arXiv:1704.04861. [Online]. Avail- able: http://arxiv.org/abs/1704.04861\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for im- age recognition,\" 2015. [Online]. Available: https://arxiv.org/abs/1512 .03385\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" 2014. [Online]. Available: https://arxiv. org/abs/1409.1556\n\nSynergy of dynamic frequency scaling and demotion on DRAM power management: Models and optimizations. Y Lu, B He, X Tang, M Guo, IEEE Trans. Comput. 648Y. Lu, B. He, X. Tang, and M. Guo, \"Synergy of dynamic frequency scaling and demotion on DRAM power management: Models and op- timizations,\" IEEE Trans. Comput., vol. 64, no. 8, pp. 2367-2381, Aug. 2015.\n\nA 16Gb/s/pin 8Gb GDDR6 DRAM with bandwidth extension techniques for high-speed applications. K.-D Hwang, Proc. IEEE Int. Solid -State Circuits Conf. IEEE Int. Solid -State Circuits ConfK.-D. Hwang et al., \"A 16Gb/s/pin 8Gb GDDR6 DRAM with bandwidth extension techniques for high-speed applications,\" in Proc. IEEE Int. Solid -State Circuits Conf., 2018, pp. 210-212.\n\nUnderstanding reduced-voltage operation in modern dram devices: Experimental characterization, analysis, and mechanisms. K K Chang, 10.1145/3084447Proc. ACM Meas. Anal. Comput. Syst. 11K. K. Chang et al., \"Understanding reduced-voltage operation in modern dram devices: Experimental characterization, analysis, and mechanisms,\" Proc. ACM Meas. Anal. Comput. Syst., vol. 1, no. 1, pp. 1-42, Jun. 2017. [Online]. Available: https://doi.org/10.1145/3084447\n\n64-GHz datapath demonstration for bit-parallel SFQ microprocessors based on a gate-level-pipeline structure. R Kashima, I Nagaoka, M Tanaka, T Yamashita, A Fujimaki, IEEE Trans. Appl. Supercond. 315Art. no. 1301006R. Kashima, I. Nagaoka, M. Tanaka, T. Yamashita, and A. Fujimaki, \"64- GHz datapath demonstration for bit-parallel SFQ microprocessors based on a gate-level-pipeline structure,\" IEEE Trans. Appl. Supercond., vol. 31, no. 5, Aug. 2021, Art. no. 1301006.\n\nA review of cryogenic neuromorphic hardware. M M Islam, S Alam, M S Hossain, K Roy, A Aziz, 10.1063/5penalty-@M.0133515J. Appl. Phys. 1337M. M. Islam, S. Alam, M. S. Hossain, K. Roy, and A. Aziz, \"A re- view of cryogenic neuromorphic hardware,\" J. Appl. Phys., vol. 133, no. 7, 2023, Art. no. 070701. [Online]. Available: https://doi.org/10.1063/5 .0133515\n\nBrainfreeze: Expanding the capabilities of neuromorphic systems using mixed-signal superconducting electronics. P Tschirhart, K Segall, https:/www.frontiersin.org/articles/10.3389/fnins.2021.750748Front. Neurosci. 15Art. no. 1625. [OnlineP. Tschirhart and K. Segall, \"Brainfreeze: Expanding the capabilities of neuromorphic systems using mixed-signal superconducting electronics,\" Front. Neurosci., vol. 15, 2021, Art. no. 1625. [Online]. Available: https: //www.frontiersin.org/articles/10.3389/fnins.2021.750748\n\nSupermind: A survey of the potential of superconducting electronics for neuromorphic computing. M Schneider, E Toomey, G Rowlands, J Shainline, P Tschirhart, K Segall, 10.1088/1361-6668/ac4cd2Supercond. Sci. Technol. 355M. Schneider, E. Toomey, G. Rowlands, J. Shainline, P. Tschirhart, and K. Segall, \"Supermind: A survey of the potential of superconduct- ing electronics for neuromorphic computing,\" Supercond. Sci. Technol., vol. 35, no. 5, Mar. 2022, Art. no. 053001. [Online]. Available: https: //dx.doi.org/10.1088/1361-6668/ac4cd2\n\nDesign of discrete hopfield neural network using a single flux quantum circuit. H He, Y Yamanashi, N Yoshikawa, IEEE Trans. Appl. Supercond. 324Art. no. 1300604H. He, Y. Yamanashi, and N. Yoshikawa, \"Design of discrete hopfield neural network using a single flux quantum circuit,\" IEEE Trans. Appl. Supercond., vol. 32, no. 4, Jun. 2021, Art. no. 1300604.\n\nP Kanerva, Sparse Distributed Memory. Cambridge, MA, USAMIT PressP. Kanerva, Sparse Distributed Memory. Cambridge, MA, USA: MIT Press, 1988.\n\nSparse distributed memory and related models. P Kanerva, NASA-CR-190553Tech. Rep.P. Kanerva, \"Sparse distributed memory and related models,\" Tech. Rep. NASA-CR-190553, 1992.\n\nSparse distributed memory: Principles and operation. M J Flynn, P Kanerva, N Bhadkamkar, RIACS-TR-89-53Tech. Rep.M. J. Flynn, P. Kanerva, and N. Bhadkamkar, \"Sparse distributed memory: Principles and operation,\" Tech. Rep. RIACS-TR-89-53, 1989.\n\nHigh-dimensional computing as a nanoscalable paradigm. A Rahimi, IEEE Trans. Circuits Syst. I: Reg. Papers. 649A. Rahimi et al., \"High-dimensional computing as a nanoscalable paradigm,\" IEEE Trans. Circuits Syst. I: Reg. Papers, vol. 64, no. 9, pp. 2508-2521, Sep. 2017.\n\nClassification and recall with binary hyperdimensional computing: Tradeoffs in choice of density and mapping characteristics. D Kleyko, A Rahimi, D A Rachkovskij, E Osipov, J M Rabaey, IEEE Trans. Neural Netw. Learn. Syst. 2912D. Kleyko, A. Rahimi, D. A. Rachkovskij, E. Osipov, and J. M. Rabaey, \"Classification and recall with binary hyperdimensional comput- ing: Tradeoffs in choice of density and mapping characteristics,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 29, no. 12, pp. 5880-5898, Dec. 2018.\n\nVector symbolic architectures as a computing framework for nanoscale hardware. D Kleyko, D. Kleyko et al., \"Vector symbolic architectures as a computing framework for nanoscale hardware,\" 2021. [Online]. Available: https://arxiv.org/abs/ 2106.05268\n\nAnalogical mapping and inference with binary spatter codes and sparse distributed memory. B Emruli, R W Gayler, F Sandin, Proc. Int. Joint Conf. Neural Netw. Int. Joint Conf. Neural NetwB. Emruli, R. W. Gayler, and F. Sandin, \"Analogical mapping and inference with binary spatter codes and sparse distributed memory,\" in Proc. Int. Joint Conf. Neural Netw., 2013, pp. 1-8.\n\nA survey on hyperdimensional computing aka vector symbolic architectures, Part I: Models and data transformations. D Kleyko, D A Rachkovskij, E Osipov, A Rahimi, 10.1145/3538531ACM Comput. Surv. 559D. Kleyko, D. A. Rachkovskij, E. Osipov, and A. Rahimi, \"A survey on hy- perdimensional computing aka vector symbolic architectures, Part I: Mod- els and data transformations,\" ACM Comput. Surv., vol. 55, no. 9, pp. 1-52, Dec. 2023. [Online]. Available: https://doi.org/10.1145/3538531\n\nSparse Distributed Memory for Sparse Distributed Data. R Vdovychenko, V Tulchinsky, Intelligent Systems and Applications, K. Arai, Ed. SwitzerlandSpringerChamR. Vdovychenko and V. Tulchinsky, \"Sparse Distributed Memory for Sparse Distributed Data,\" in Intelligent Systems and Applications, K. Arai, Ed. Switzerland, Cham: Springer, 2023, pp. 74-81.\n\nA survey on hyperdimensional computing aka vector symbolic architectures, Part II: Applications, cognitive models, and challenges. D Kleyko, D Rachkovskij, E Osipov, A Rahimi, 10.1145/3558000ACM Comput. Surv. 559D. Kleyko, D. Rachkovskij, E. Osipov, and A. Rahimi, \"A survey on hyperdimensional computing aka vector symbolic architectures, Part II: Applications, cognitive models, and challenges,\" ACM Comput. Surv., vol. 55, no. 9, pp. 1-52, Jan. 2023. [Online]. Available: https://doi.org/ 10.1145/3558000\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, 10.1145/2934583.2934624Proc. Int. Symp. Low Power Electron. Des. Int. Symp. Low Power Electron. DesNew York, NY, USAA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy-efficient classifier using brain-inspired hyperdimensional computing,\" in Proc. Int. Symp. Low Power Electron. Des., New York, NY, USA, 2016, pp. 64-69. [Online]. Available: https://doi.org/10.1145/2934583.2934624\n\nEnergy-efficient and high throughput sparse distributed memory architecture. M Kang, E P Kim, M Keel, N R Shanbhag, Proc. IEEE Int. Symp. Circuits Syst. IEEE Int. Symp. Circuits SystM. Kang, E. P. Kim, M.-s. Keel, and N. R. Shanbhag, \"Energy-efficient and high throughput sparse distributed memory architecture,\" in Proc. IEEE Int. Symp. Circuits Syst., 2015, pp. 2505-2508.\n\nHyper-dimensional computing challenges and opportunities for AI applications. E Hassan, Y Halawani, B Mohammad, H Saleh, IEEE Access. 10E. Hassan, Y. Halawani, B. Mohammad, and H. Saleh, \"Hyper-dimensional computing challenges and opportunities for AI applications,\" IEEE Access, vol. 10, pp. 97651-97664, 2022.\n\nBrain-inspired cognition in next generation racetrack memories. A A Khan, S Ollivier, S Longofono, G Hempel, J Castrillon, A K Jones, 10.1145/3524071ACM Trans. Embedded Comput. Syst. 216A. A. Khan, S. Ollivier, S. Longofono, G. Hempel, J. Castrillon, and A. K. Jones, \"Brain-inspired cognition in next generation racetrack memories,\" ACM Trans. Embedded Comput. Syst., vol. 21, no. 6, pp. 1-28, Mar. 2022. [Online]. Available: https://doi.org/10.1145/3524071\n\nHyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition. A Rahimi, S Benatti, P Kanerva, L Benini, J M Rabaey, Proc. IEEE Int. Conf. Rebooting Comput. IEEE Int. Conf. Rebooting ComputA. Rahimi, S. Benatti, P. Kanerva, L. Benini, and J. M. Rabaey, \"Hy- perdimensional biosignal processing: A case study for EMG-based hand gesture recognition,\" in Proc. IEEE Int. Conf. Rebooting Comput., 2016, pp. 1-8.\n\nClassification using hyperdimensional computing: A review. L Ge, K K Parhi, IEEE Circuits Syst. Mag. 202L. Ge and K. K. Parhi, \"Classification using hyperdimensional computing: A review,\" IEEE Circuits Syst. Mag., vol. 20, no. 2, pp. 30-47, Secondquar- ter 2020.\n\nHyperdimensional computing encoding schemes for improved image classification. V Miranda, O , Proc. IEEE Int. Symp. Technol. Homeland Secur. IEEE Int. Symp. Technol. Homeland SecurV. Miranda and O. d'Aliberti, \"Hyperdimensional computing encoding schemes for improved image classification,\" in Proc. IEEE Int. Symp. Technol. Homeland Secur., 2022, pp. 1-9.\n\nDistrihd: A memory efficient distributed binary hyperdimensional computing architecture for image classification. D Liang, J Shiomi, N Miura, H Awano, Proc. 27th Asia South Pacific Des. Automat. Conf., 2022. 27th Asia South Pacific Des. Automat. Conf., 2022D. Liang, J. Shiomi, N. Miura, and H. Awano, \"Distrihd: A memory efficient distributed binary hyperdimensional computing architecture for image classification,\" in Proc. 27th Asia South Pacific Des. Automat. Conf., 2022, pp. 43-49.\n\nSearchd: A memory-centric hyperdimensional computing with stochastic training. M Imani, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 3910M. Imani et al., \"Searchd: A memory-centric hyperdimensional computing with stochastic training,\" IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., vol. 39, no. 10, pp. 2422-2433, Oct. 2020.\n\nHDPG: Hyperdimensional policy-based reinforcement learning for continuous control. Y Ni, M Issa, D Abraham, M Imani, X Yin, M Imani, 10.1145/3489517.3530668Proc. IEEE/ACM 59th Des. Automat. Conf. IEEE/ACM 59th Des. Automat. ConfNew York, NY, USA, 2022Y. Ni, M. Issa, D. Abraham, M. Imani, X. Yin, and M. Imani, \"HDPG: Hyperdimensional policy-based reinforcement learning for continuous control,\" in Proc. IEEE/ACM 59th Des. Automat. Conf., New York, NY, USA, 2022, pp. 1141-1146. [Online]. Available: https://doi.org/10.1145/ 3489517.3530668\n\nCognitive correlative encoding for genome sequence matching in hyperdimensional system. P Poduval, Z Zou, X Yin, E Sadredini, M Imani, Proc. IEEE/ACM 58th Des. Automat. Conf., 2021. IEEE/ACM 58th Des. Automat. Conf., 2021P. Poduval, Z. Zou, X. Yin, E. Sadredini, and M. Imani, \"Cognitive correlative encoding for genome sequence matching in hyperdimen- sional system,\" in Proc. IEEE/ACM 58th Des. Automat. Conf., 2021, pp. 781-786.\n\nRegHD: Robust and efficient regression in hyper-dimensional learning system. A Hern\u00e1ndez-Cano, C Zhuo, X Yin, M Imani, Proc. IEEE/ACM 58th Des. Automat. Conf., 2021. IEEE/ACM 58th Des. Automat. Conf., 2021A. Hern\u00e1ndez-Cano, C. Zhuo, X. Yin, and M. Imani, \"RegHD: Robust and efficient regression in hyper-dimensional learning system,\" in Proc. IEEE/ACM 58th Des. Automat. Conf., 2021, pp. 7-12.\n\nCorpus portal for search in monolingual corpora. U Quasthoff, M Richter, C Biemann, European Language Resources Association (ELRA). Genoa, ItalyProc. 5th Int. Conf. Lang. Resour. Eval.U. Quasthoff, M. Richter, and C. Biemann, \"Corpus portal for search in monolingual corpora,\" in Proc. 5th Int. Conf. Lang. Resour. Eval., Genoa, Italy: European Language Resources Association (ELRA), May 2006. [Online]. Available: http://www.lrec-conf.org/proceedings/lrec2006/pdf/ 641_pdf.pdf\n\nEuroparl: A parallel corpus for statistical machine translation. P Koehn, Proc. Mach. Transl. Summit X: Papers. Mach. Transl. Summit X: PapersPhuket, ThailandP. Koehn, \"Europarl: A parallel corpus for statistical machine transla- tion,\" in Proc. Mach. Transl. Summit X: Papers, Phuket, Thailand, 2005, pp. 79-86. [Online]. Available: https://aclanthology.org/2005.mtsummit- papers.11\n\nExperimental implementation of SFQ NDRO cells and 8-bit ADC. V Kaplunenko, M Khabipov, D Khokhlov, A Kirichenko, V Koshelets, S Kovtonyuk, IEEE Trans. Appl. Supercond. 31V. Kaplunenko, M. Khabipov, D. Khokhlov, A. Kirichenko, V. Koshelets, and S. Kovtonyuk, \"Experimental implementation of SFQ NDRO cells and 8-bit ADC,\" IEEE Trans. Appl. Supercond., vol. 3, no. 1, pp. 2662-2665, Mar. 1993.\n\nWrspice circuit simulator. \"Wrspice circuit simulator,\" Accessed: Mar. 28, 2023. [Online]. Available: http://www.wrcad.com/wrspice.html\n\nSuperconducting integrated circuits. \"Superconducting integrated circuits,\" Accessed: Mar. 28, 2023. [Online].\n\nA pythonic approach for rapid hardware prototyping and instrumentation. J Clow, G Tzimpragos, D Dangwal, S Guo, J Mcmahan, T Sherwood, Proc. 27th Int. Conf. Field Programmable Log. 27th Int. Conf. Field Programmable LogJ. Clow, G. Tzimpragos, D. Dangwal, S. Guo, J. McMahan, and T. Sherwood, \"A pythonic approach for rapid hardware prototyping and instrumentation,\" in Proc. 27th Int. Conf. Field Programmable Log. Appl., 2017, pp. 1-7. [Online]. Available: https://ieeexplore.ieee.org/abstract/ document/8056860\n\nZero static power dissipation biasing of RSFQ circuits. D Kirichenko, S Sarwana, A Kirichenko, IEEE Trans. Appl. Supercond. 213D. Kirichenko, S. Sarwana, and A. Kirichenko, \"Zero static power dissi- pation biasing of RSFQ circuits,\" IEEE Trans. Appl. Supercond., vol. 21, no. 3, pp. 776-779, Jun. 2011. [Online]. Available: https://ieeexplore.ieee. org/document/5688194\n\nSimulation analysis and energy-saving techniques for ERSFQ circuits. N K Katam, O Mukhanov, M Pedram, IEEE Trans. Appl. Supercond. 2951302607N. K. Katam, O. Mukhanov, and M. Pedram, \"Simulation analysis and energy-saving techniques for ERSFQ circuits,\" IEEE Trans. Appl. Super- cond., vol. 29, no. 5, Aug. 2019, Art. no. 1302607.\n\nEnergyefficient superconducting computing-power budgets and requirements. D S Holmes, A L Ripple, M A Manheimer, IEEE Trans. Appl. Supercond. 233D. S. Holmes, A. L. Ripple, and M. A. Manheimer, \"Energy- efficient superconducting computing-power budgets and requirements,\" IEEE Trans. Appl. Supercond., vol. 23, no. 3, pp. 1701610-1701610, Jun. 2013.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009, pp. 248-255.\n\nA regular layout for parallel adders. R P Brent, H T Kung, IEEE Trans. Comput. 3103R. P. Brent and H. T. Kung, \"A regular layout for parallel adders,\" IEEE Trans. Comput., vol. 31, no. 03, pp. 260-264, Mar. 1982.\n\nA computational temporal logic for superconducting accelerators. G Tzimpragos, Proc. 25th Int. Conf. Architectural Support Program. Lang. Operating Syst. 25th Int. Conf. Architectural Support Program. Lang. Operating SystG. Tzimpragos et al., \"A computational temporal logic for superconducting accelerators,\" in Proc. 25th Int. Conf. Architectural Support Program. Lang. Operating Syst., 2020, pp. 435-448.\n", "annotations": {"author": "[{\"end\":123,\"start\":112},{\"end\":163,\"start\":124},{\"end\":189,\"start\":164},{\"end\":233,\"start\":190}]", "publisher": null, "author_last_name": "[{\"end\":122,\"start\":118},{\"end\":162,\"start\":145},{\"end\":188,\"start\":183},{\"end\":232,\"start\":216}]", "author_first_name": "[{\"end\":117,\"start\":112},{\"end\":144,\"start\":136},{\"end\":182,\"start\":176},{\"end\":215,\"start\":209}]", "author_affiliation": null, "title": "[{\"end\":90,\"start\":1},{\"end\":323,\"start\":234}]", "venue": "[{\"end\":371,\"start\":325}]", "abstract": "[{\"end\":1949,\"start\":559}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2229,\"start\":2226},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2234,\"start\":2231},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2470,\"start\":2467},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2566,\"start\":2563},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2729,\"start\":2726},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2734,\"start\":2731},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2811,\"start\":2808},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2816,\"start\":2813},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2865,\"start\":2862},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3078,\"start\":3075},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3163,\"start\":3159},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4565,\"start\":4561},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5544,\"start\":5540},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5788,\"start\":5784},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5794,\"start\":5790},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6600,\"start\":6596},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6606,\"start\":6602},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7211,\"start\":7207},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7217,\"start\":7213},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7223,\"start\":7219},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7304,\"start\":7300},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7623,\"start\":7619},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7629,\"start\":7625},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7764,\"start\":7760},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7770,\"start\":7766},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7776,\"start\":7772},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7782,\"start\":7778},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7788,\"start\":7784},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7794,\"start\":7790},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7913,\"start\":7909},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8564,\"start\":8560},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8773,\"start\":8770},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8779,\"start\":8775},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8785,\"start\":8781},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8791,\"start\":8787},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8797,\"start\":8793},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9438,\"start\":9434},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9743,\"start\":9739},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9749,\"start\":9745},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9989,\"start\":9985},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9995,\"start\":9991},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10001,\"start\":9997},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10007,\"start\":10003},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10016,\"start\":10012},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10159,\"start\":10155},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10390,\"start\":10386},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10977,\"start\":10973},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10983,\"start\":10979},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10989,\"start\":10985},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10995,\"start\":10991},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11001,\"start\":10997},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11007,\"start\":11003},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11172,\"start\":11168},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11178,\"start\":11174},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11184,\"start\":11180},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11328,\"start\":11324},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11334,\"start\":11330},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11340,\"start\":11336},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11540,\"start\":11536},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11546,\"start\":11542},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11552,\"start\":11548},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11784,\"start\":11780},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11790,\"start\":11786},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11922,\"start\":11918},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11928,\"start\":11924},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11934,\"start\":11930},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11940,\"start\":11936},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12119,\"start\":12115},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12125,\"start\":12121},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12585,\"start\":12581},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12637,\"start\":12633},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13582,\"start\":13578},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13588,\"start\":13584},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13594,\"start\":13590},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13600,\"start\":13596},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13606,\"start\":13602},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13882,\"start\":13878},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13888,\"start\":13884},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13894,\"start\":13890},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13900,\"start\":13896},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14087,\"start\":14083},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14093,\"start\":14089},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14353,\"start\":14349},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14359,\"start\":14355},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14365,\"start\":14361},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14371,\"start\":14367},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14377,\"start\":14373},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14383,\"start\":14379},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15489,\"start\":15485},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15495,\"start\":15491},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15501,\"start\":15497},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15646,\"start\":15642},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15652,\"start\":15648},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15658,\"start\":15654},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15664,\"start\":15660},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15753,\"start\":15752},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":15857,\"start\":15853},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15942,\"start\":15938},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15948,\"start\":15944},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15954,\"start\":15950},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16179,\"start\":16175},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16185,\"start\":16181},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16191,\"start\":16187},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":16387,\"start\":16383},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16393,\"start\":16389},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16546,\"start\":16542},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16552,\"start\":16548},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16558,\"start\":16554},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":16564,\"start\":16560},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16570,\"start\":16566},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16576,\"start\":16572},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16582,\"start\":16578},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16944,\"start\":16940},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18074,\"start\":18070},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18080,\"start\":18076},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18086,\"start\":18082},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18163,\"start\":18159},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18208,\"start\":18204},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18491,\"start\":18487},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19006,\"start\":19002},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19012,\"start\":19008},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19018,\"start\":19014},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19086,\"start\":19082},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19303,\"start\":19299},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20214,\"start\":20210},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":20220,\"start\":20216},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":20226,\"start\":20222},{\"end\":24623,\"start\":24620},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29924,\"start\":29921},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":29979,\"start\":29975},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":30126,\"start\":30122},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":30217,\"start\":30213},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":30529,\"start\":30525},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":32853,\"start\":32849},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":32859,\"start\":32855},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":33301,\"start\":33297},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":33307,\"start\":33303},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33517,\"start\":33513},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":33523,\"start\":33519},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33559,\"start\":33555},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":34675,\"start\":34671},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34874,\"start\":34870},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34880,\"start\":34876},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34886,\"start\":34882},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":43512,\"start\":43509},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":43560,\"start\":43557},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":43565,\"start\":43562},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":43570,\"start\":43567},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":44293,\"start\":44289},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":44299,\"start\":44295},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":45560,\"start\":45556},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":45608,\"start\":45604},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":45614,\"start\":45610},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":45805,\"start\":45801},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":45937,\"start\":45933},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":46142,\"start\":46138},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":46629,\"start\":46625},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":47748,\"start\":47744},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":47998,\"start\":47994},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":48859,\"start\":48855},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":50521,\"start\":50517},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":51169,\"start\":51165},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":51334,\"start\":51330},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":51514,\"start\":51510},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":51673,\"start\":51670},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":51679,\"start\":51675},{\"end\":51853,\"start\":51843},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":51869,\"start\":51865},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":51935,\"start\":51932},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":51956,\"start\":51952},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":53218,\"start\":53214},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":53698,\"start\":53694},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":54566,\"start\":54562},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":59254,\"start\":59250}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":59707,\"start\":59655},{\"attributes\":{\"id\":\"fig_1\"},\"end\":59759,\"start\":59708},{\"attributes\":{\"id\":\"fig_2\"},\"end\":59809,\"start\":59760},{\"attributes\":{\"id\":\"fig_3\"},\"end\":60124,\"start\":59810},{\"attributes\":{\"id\":\"fig_4\"},\"end\":60210,\"start\":60125},{\"attributes\":{\"id\":\"fig_5\"},\"end\":60393,\"start\":60211},{\"attributes\":{\"id\":\"fig_6\"},\"end\":60445,\"start\":60394},{\"attributes\":{\"id\":\"fig_7\"},\"end\":60590,\"start\":60446},{\"attributes\":{\"id\":\"fig_8\"},\"end\":60948,\"start\":60591},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":60990,\"start\":60949},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":61070,\"start\":60991},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":61100,\"start\":61071},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":61167,\"start\":61101},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61217,\"start\":61168},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":61301,\"start\":61218},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":61376,\"start\":61302},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":61680,\"start\":61377}]", "paragraph": "[{\"end\":2567,\"start\":1968},{\"end\":3365,\"start\":2569},{\"end\":4566,\"start\":3400},{\"end\":5568,\"start\":4568},{\"end\":6447,\"start\":5570},{\"end\":7224,\"start\":6449},{\"end\":8261,\"start\":7226},{\"end\":8798,\"start\":8263},{\"end\":10227,\"start\":8800},{\"end\":10672,\"start\":10229},{\"end\":12230,\"start\":10687},{\"end\":13075,\"start\":12425},{\"end\":13386,\"start\":13077},{\"end\":14384,\"start\":13427},{\"end\":15137,\"start\":14386},{\"end\":15502,\"start\":15139},{\"end\":15858,\"start\":15504},{\"end\":16583,\"start\":15860},{\"end\":17230,\"start\":16585},{\"end\":17869,\"start\":17232},{\"end\":18773,\"start\":17871},{\"end\":19778,\"start\":18775},{\"end\":20130,\"start\":19819},{\"end\":20531,\"start\":20132},{\"end\":20976,\"start\":20533},{\"end\":21804,\"start\":20998},{\"end\":22356,\"start\":21806},{\"end\":23861,\"start\":22358},{\"end\":25807,\"start\":23863},{\"end\":26603,\"start\":25846},{\"end\":26897,\"start\":26619},{\"end\":27963,\"start\":26899},{\"end\":28804,\"start\":28030},{\"end\":29139,\"start\":28806},{\"end\":29743,\"start\":29141},{\"end\":30387,\"start\":29763},{\"end\":31067,\"start\":30389},{\"end\":31394,\"start\":31069},{\"end\":32121,\"start\":31396},{\"end\":32677,\"start\":32123},{\"end\":34000,\"start\":32679},{\"end\":34497,\"start\":34002},{\"end\":34676,\"start\":34499},{\"end\":35450,\"start\":34696},{\"end\":36422,\"start\":35452},{\"end\":36928,\"start\":36443},{\"end\":37407,\"start\":36930},{\"end\":37958,\"start\":37409},{\"end\":38801,\"start\":37997},{\"end\":39307,\"start\":38803},{\"end\":39950,\"start\":39309},{\"end\":40552,\"start\":39952},{\"end\":41316,\"start\":40554},{\"end\":41656,\"start\":41318},{\"end\":42494,\"start\":41701},{\"end\":42888,\"start\":42496},{\"end\":43314,\"start\":42890},{\"end\":43994,\"start\":43345},{\"end\":44986,\"start\":43996},{\"end\":45371,\"start\":44988},{\"end\":45770,\"start\":45394},{\"end\":46577,\"start\":45772},{\"end\":47028,\"start\":46579},{\"end\":47439,\"start\":47030},{\"end\":47750,\"start\":47441},{\"end\":48218,\"start\":47777},{\"end\":48521,\"start\":48220},{\"end\":49040,\"start\":48523},{\"end\":49909,\"start\":49042},{\"end\":50523,\"start\":49911},{\"end\":51110,\"start\":50525},{\"end\":51453,\"start\":51112},{\"end\":52522,\"start\":51455},{\"end\":52828,\"start\":52524},{\"end\":53152,\"start\":52830},{\"end\":53875,\"start\":53154},{\"end\":55196,\"start\":53877},{\"end\":55704,\"start\":55198},{\"end\":56256,\"start\":55725},{\"end\":56907,\"start\":56258},{\"end\":57601,\"start\":56909},{\"end\":58184,\"start\":57603},{\"end\":58682,\"start\":58186},{\"end\":59200,\"start\":58702},{\"end\":59654,\"start\":59202}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12138,\"start\":12131},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29992,\"start\":29985},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32463,\"start\":32456},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33029,\"start\":33022},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33834,\"start\":33826},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33897,\"start\":33887},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":36028,\"start\":36021},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36153,\"start\":36145},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":42636,\"start\":42628},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":43313,\"start\":43305},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":44218,\"start\":44210},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":47471,\"start\":47463},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":50043,\"start\":50034},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":50513,\"start\":50506},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":51109,\"start\":51099},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":51861,\"start\":51854},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":53349,\"start\":53340}]", "section_header": "[{\"end\":1966,\"start\":1951},{\"end\":3398,\"start\":3368},{\"end\":10685,\"start\":10675},{\"end\":12423,\"start\":12233},{\"end\":13425,\"start\":13389},{\"end\":19817,\"start\":19781},{\"end\":20996,\"start\":20979},{\"end\":25844,\"start\":25810},{\"end\":26617,\"start\":26606},{\"end\":27983,\"start\":27966},{\"end\":28028,\"start\":27986},{\"end\":29761,\"start\":29746},{\"end\":34694,\"start\":34679},{\"end\":36441,\"start\":36425},{\"end\":37995,\"start\":37961},{\"end\":41699,\"start\":41659},{\"end\":43343,\"start\":43317},{\"end\":45392,\"start\":45374},{\"end\":47775,\"start\":47753},{\"end\":55723,\"start\":55707},{\"end\":58700,\"start\":58685},{\"end\":59664,\"start\":59656},{\"end\":59717,\"start\":59709},{\"end\":59769,\"start\":59761},{\"end\":59819,\"start\":59811},{\"end\":60134,\"start\":60126},{\"end\":60220,\"start\":60212},{\"end\":60403,\"start\":60395},{\"end\":60455,\"start\":60447},{\"end\":60600,\"start\":60592},{\"end\":60966,\"start\":60950},{\"end\":61000,\"start\":60992},{\"end\":61085,\"start\":61072},{\"end\":61114,\"start\":61102},{\"end\":61182,\"start\":61169},{\"end\":61235,\"start\":61219},{\"end\":61319,\"start\":61303}]", "table": "[{\"end\":61070,\"start\":61003}]", "figure_caption": "[{\"end\":59707,\"start\":59666},{\"end\":59759,\"start\":59719},{\"end\":59809,\"start\":59771},{\"end\":60124,\"start\":59821},{\"end\":60210,\"start\":60136},{\"end\":60393,\"start\":60222},{\"end\":60445,\"start\":60405},{\"end\":60590,\"start\":60457},{\"end\":60948,\"start\":60602},{\"end\":60990,\"start\":60968},{\"end\":61100,\"start\":61088},{\"end\":61167,\"start\":61116},{\"end\":61217,\"start\":61185},{\"end\":61301,\"start\":61239},{\"end\":61376,\"start\":61324},{\"end\":61680,\"start\":61379}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17322,\"start\":17316},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18216,\"start\":18210},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18652,\"start\":18646},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19626,\"start\":19620},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21092,\"start\":21086},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21585,\"start\":21579},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22231,\"start\":22225},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24070,\"start\":24061},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24966,\"start\":24957},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25551,\"start\":25545},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26470,\"start\":26464},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26533,\"start\":26527},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27516,\"start\":27509},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27961,\"start\":27951},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":37070,\"start\":37062},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":37540,\"start\":37534},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38293,\"start\":38285},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39063,\"start\":39056},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":41768,\"start\":41760},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":41819,\"start\":41812},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":47740,\"start\":47734},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":54441,\"start\":54435}]", "bib_author_first_name": "[{\"end\":61836,\"start\":61835},{\"end\":61838,\"start\":61837},{\"end\":61847,\"start\":61846},{\"end\":61854,\"start\":61853},{\"end\":61856,\"start\":61855},{\"end\":61865,\"start\":61864},{\"end\":61874,\"start\":61873},{\"end\":61876,\"start\":61875},{\"end\":61887,\"start\":61886},{\"end\":61889,\"start\":61888},{\"end\":62346,\"start\":62345},{\"end\":62356,\"start\":62355},{\"end\":62368,\"start\":62367},{\"end\":62378,\"start\":62377},{\"end\":62390,\"start\":62386},{\"end\":62772,\"start\":62771},{\"end\":63320,\"start\":63319},{\"end\":63322,\"start\":63321},{\"end\":63618,\"start\":63617},{\"end\":63620,\"start\":63619},{\"end\":63631,\"start\":63630},{\"end\":63633,\"start\":63632},{\"end\":63645,\"start\":63644},{\"end\":63647,\"start\":63646},{\"end\":63965,\"start\":63964},{\"end\":63967,\"start\":63966},{\"end\":63978,\"start\":63977},{\"end\":63980,\"start\":63979},{\"end\":63992,\"start\":63991},{\"end\":63994,\"start\":63993},{\"end\":64378,\"start\":64377},{\"end\":64380,\"start\":64379},{\"end\":64391,\"start\":64390},{\"end\":64393,\"start\":64392},{\"end\":64833,\"start\":64832},{\"end\":64844,\"start\":64843},{\"end\":64854,\"start\":64853},{\"end\":64863,\"start\":64862},{\"end\":65262,\"start\":65261},{\"end\":65272,\"start\":65271},{\"end\":65777,\"start\":65776},{\"end\":65779,\"start\":65778},{\"end\":65789,\"start\":65788},{\"end\":65791,\"start\":65790},{\"end\":66133,\"start\":66132},{\"end\":66135,\"start\":66134},{\"end\":66142,\"start\":66141},{\"end\":66151,\"start\":66150},{\"end\":66157,\"start\":66156},{\"end\":66484,\"start\":66483},{\"end\":66491,\"start\":66490},{\"end\":66497,\"start\":66496},{\"end\":66499,\"start\":66498},{\"end\":66510,\"start\":66509},{\"end\":66512,\"start\":66511},{\"end\":66523,\"start\":66519},{\"end\":66858,\"start\":66857},{\"end\":67310,\"start\":67309},{\"end\":67767,\"start\":67766},{\"end\":67774,\"start\":67773},{\"end\":67781,\"start\":67780},{\"end\":67788,\"start\":67787},{\"end\":67796,\"start\":67795},{\"end\":68070,\"start\":68069},{\"end\":68080,\"start\":68079},{\"end\":68091,\"start\":68090},{\"end\":68301,\"start\":68300},{\"end\":68309,\"start\":68308},{\"end\":68321,\"start\":68320},{\"end\":68665,\"start\":68664},{\"end\":68679,\"start\":68678},{\"end\":68692,\"start\":68691},{\"end\":68694,\"start\":68693},{\"end\":69093,\"start\":69092},{\"end\":69100,\"start\":69099},{\"end\":69106,\"start\":69105},{\"end\":69118,\"start\":69117},{\"end\":69472,\"start\":69471},{\"end\":69776,\"start\":69775},{\"end\":69778,\"start\":69777},{\"end\":70037,\"start\":70036},{\"end\":70043,\"start\":70042},{\"end\":70052,\"start\":70051},{\"end\":70059,\"start\":70058},{\"end\":70283,\"start\":70282},{\"end\":70295,\"start\":70294},{\"end\":70571,\"start\":70570},{\"end\":70577,\"start\":70576},{\"end\":70583,\"start\":70582},{\"end\":70591,\"start\":70590},{\"end\":70922,\"start\":70918},{\"end\":71315,\"start\":71314},{\"end\":71317,\"start\":71316},{\"end\":71758,\"start\":71757},{\"end\":71769,\"start\":71768},{\"end\":71780,\"start\":71779},{\"end\":71790,\"start\":71789},{\"end\":71803,\"start\":71802},{\"end\":72162,\"start\":72161},{\"end\":72164,\"start\":72163},{\"end\":72173,\"start\":72172},{\"end\":72181,\"start\":72180},{\"end\":72183,\"start\":72182},{\"end\":72194,\"start\":72193},{\"end\":72201,\"start\":72200},{\"end\":72587,\"start\":72586},{\"end\":72601,\"start\":72600},{\"end\":73086,\"start\":73085},{\"end\":73099,\"start\":73098},{\"end\":73109,\"start\":73108},{\"end\":73121,\"start\":73120},{\"end\":73134,\"start\":73133},{\"end\":73148,\"start\":73147},{\"end\":73609,\"start\":73608},{\"end\":73615,\"start\":73614},{\"end\":73628,\"start\":73627},{\"end\":73886,\"start\":73885},{\"end\":74074,\"start\":74073},{\"end\":74256,\"start\":74255},{\"end\":74258,\"start\":74257},{\"end\":74267,\"start\":74266},{\"end\":74278,\"start\":74277},{\"end\":74504,\"start\":74503},{\"end\":74847,\"start\":74846},{\"end\":74857,\"start\":74856},{\"end\":74867,\"start\":74866},{\"end\":74869,\"start\":74868},{\"end\":74884,\"start\":74883},{\"end\":74894,\"start\":74893},{\"end\":74896,\"start\":74895},{\"end\":75310,\"start\":75309},{\"end\":75571,\"start\":75570},{\"end\":75581,\"start\":75580},{\"end\":75583,\"start\":75582},{\"end\":75593,\"start\":75592},{\"end\":75970,\"start\":75969},{\"end\":75980,\"start\":75979},{\"end\":75982,\"start\":75981},{\"end\":75997,\"start\":75996},{\"end\":76007,\"start\":76006},{\"end\":76395,\"start\":76394},{\"end\":76410,\"start\":76409},{\"end\":76821,\"start\":76820},{\"end\":76831,\"start\":76830},{\"end\":76846,\"start\":76845},{\"end\":76856,\"start\":76855},{\"end\":77289,\"start\":77288},{\"end\":77299,\"start\":77298},{\"end\":77310,\"start\":77309},{\"end\":77312,\"start\":77311},{\"end\":77792,\"start\":77791},{\"end\":77800,\"start\":77799},{\"end\":77802,\"start\":77801},{\"end\":77809,\"start\":77808},{\"end\":77817,\"start\":77816},{\"end\":77819,\"start\":77818},{\"end\":78169,\"start\":78168},{\"end\":78179,\"start\":78178},{\"end\":78191,\"start\":78190},{\"end\":78203,\"start\":78202},{\"end\":78468,\"start\":78467},{\"end\":78470,\"start\":78469},{\"end\":78478,\"start\":78477},{\"end\":78490,\"start\":78489},{\"end\":78503,\"start\":78502},{\"end\":78513,\"start\":78512},{\"end\":78527,\"start\":78526},{\"end\":78529,\"start\":78528},{\"end\":78956,\"start\":78955},{\"end\":78966,\"start\":78965},{\"end\":78977,\"start\":78976},{\"end\":78988,\"start\":78987},{\"end\":78998,\"start\":78997},{\"end\":79000,\"start\":78999},{\"end\":79361,\"start\":79360},{\"end\":79367,\"start\":79366},{\"end\":79369,\"start\":79368},{\"end\":79645,\"start\":79644},{\"end\":79656,\"start\":79655},{\"end\":80038,\"start\":80037},{\"end\":80047,\"start\":80046},{\"end\":80057,\"start\":80056},{\"end\":80066,\"start\":80065},{\"end\":80493,\"start\":80492},{\"end\":80840,\"start\":80839},{\"end\":80846,\"start\":80845},{\"end\":80854,\"start\":80853},{\"end\":80865,\"start\":80864},{\"end\":80874,\"start\":80873},{\"end\":80881,\"start\":80880},{\"end\":81388,\"start\":81387},{\"end\":81399,\"start\":81398},{\"end\":81406,\"start\":81405},{\"end\":81413,\"start\":81412},{\"end\":81426,\"start\":81425},{\"end\":81810,\"start\":81809},{\"end\":81828,\"start\":81827},{\"end\":81836,\"start\":81835},{\"end\":81843,\"start\":81842},{\"end\":82177,\"start\":82176},{\"end\":82190,\"start\":82189},{\"end\":82201,\"start\":82200},{\"end\":82672,\"start\":82671},{\"end\":83053,\"start\":83052},{\"end\":83067,\"start\":83066},{\"end\":83079,\"start\":83078},{\"end\":83091,\"start\":83090},{\"end\":83105,\"start\":83104},{\"end\":83118,\"start\":83117},{\"end\":83706,\"start\":83705},{\"end\":83714,\"start\":83713},{\"end\":83728,\"start\":83727},{\"end\":83739,\"start\":83738},{\"end\":83746,\"start\":83745},{\"end\":83757,\"start\":83756},{\"end\":84204,\"start\":84203},{\"end\":84218,\"start\":84217},{\"end\":84229,\"start\":84228},{\"end\":84588,\"start\":84587},{\"end\":84590,\"start\":84589},{\"end\":84599,\"start\":84598},{\"end\":84611,\"start\":84610},{\"end\":84924,\"start\":84923},{\"end\":84926,\"start\":84925},{\"end\":84936,\"start\":84935},{\"end\":84938,\"start\":84937},{\"end\":84948,\"start\":84947},{\"end\":84950,\"start\":84949},{\"end\":85254,\"start\":85253},{\"end\":85262,\"start\":85261},{\"end\":85270,\"start\":85269},{\"end\":85283,\"start\":85279},{\"end\":85289,\"start\":85288},{\"end\":85295,\"start\":85294},{\"end\":85621,\"start\":85620},{\"end\":85623,\"start\":85622},{\"end\":85632,\"start\":85631},{\"end\":85634,\"start\":85633},{\"end\":85862,\"start\":85861}]", "bib_author_last_name": "[{\"end\":61844,\"start\":61839},{\"end\":61851,\"start\":61848},{\"end\":61862,\"start\":61857},{\"end\":61871,\"start\":61866},{\"end\":61884,\"start\":61877},{\"end\":61897,\"start\":61890},{\"end\":62353,\"start\":62347},{\"end\":62365,\"start\":62357},{\"end\":62375,\"start\":62369},{\"end\":62384,\"start\":62379},{\"end\":62400,\"start\":62391},{\"end\":62779,\"start\":62773},{\"end\":63330,\"start\":63323},{\"end\":63628,\"start\":63621},{\"end\":63642,\"start\":63634},{\"end\":63655,\"start\":63648},{\"end\":63975,\"start\":63968},{\"end\":63989,\"start\":63981},{\"end\":64002,\"start\":63995},{\"end\":64388,\"start\":64381},{\"end\":64401,\"start\":64394},{\"end\":64841,\"start\":64834},{\"end\":64851,\"start\":64845},{\"end\":64860,\"start\":64855},{\"end\":64872,\"start\":64864},{\"end\":65269,\"start\":65263},{\"end\":65278,\"start\":65273},{\"end\":65786,\"start\":65780},{\"end\":65800,\"start\":65792},{\"end\":66139,\"start\":66136},{\"end\":66148,\"start\":66143},{\"end\":66154,\"start\":66152},{\"end\":66162,\"start\":66158},{\"end\":66488,\"start\":66485},{\"end\":66494,\"start\":66492},{\"end\":66507,\"start\":66500},{\"end\":66517,\"start\":66513},{\"end\":66527,\"start\":66524},{\"end\":66868,\"start\":66859},{\"end\":67317,\"start\":67311},{\"end\":67771,\"start\":67768},{\"end\":67778,\"start\":67775},{\"end\":67785,\"start\":67782},{\"end\":67793,\"start\":67789},{\"end\":67800,\"start\":67797},{\"end\":68077,\"start\":68071},{\"end\":68088,\"start\":68081},{\"end\":68099,\"start\":68092},{\"end\":68306,\"start\":68302},{\"end\":68318,\"start\":68310},{\"end\":68325,\"start\":68322},{\"end\":68676,\"start\":68666},{\"end\":68689,\"start\":68680},{\"end\":68701,\"start\":68695},{\"end\":69097,\"start\":69094},{\"end\":69103,\"start\":69101},{\"end\":69115,\"start\":69107},{\"end\":69122,\"start\":69119},{\"end\":69480,\"start\":69473},{\"end\":69785,\"start\":69779},{\"end\":70040,\"start\":70038},{\"end\":70049,\"start\":70044},{\"end\":70056,\"start\":70053},{\"end\":70063,\"start\":70060},{\"end\":70292,\"start\":70284},{\"end\":70305,\"start\":70296},{\"end\":70574,\"start\":70572},{\"end\":70580,\"start\":70578},{\"end\":70588,\"start\":70584},{\"end\":70595,\"start\":70592},{\"end\":70928,\"start\":70923},{\"end\":71323,\"start\":71318},{\"end\":71766,\"start\":71759},{\"end\":71777,\"start\":71770},{\"end\":71787,\"start\":71781},{\"end\":71800,\"start\":71791},{\"end\":71812,\"start\":71804},{\"end\":72170,\"start\":72165},{\"end\":72178,\"start\":72174},{\"end\":72191,\"start\":72184},{\"end\":72198,\"start\":72195},{\"end\":72206,\"start\":72202},{\"end\":72598,\"start\":72588},{\"end\":72608,\"start\":72602},{\"end\":73096,\"start\":73087},{\"end\":73106,\"start\":73100},{\"end\":73118,\"start\":73110},{\"end\":73131,\"start\":73122},{\"end\":73145,\"start\":73135},{\"end\":73155,\"start\":73149},{\"end\":73612,\"start\":73610},{\"end\":73625,\"start\":73616},{\"end\":73638,\"start\":73629},{\"end\":73894,\"start\":73887},{\"end\":74082,\"start\":74075},{\"end\":74264,\"start\":74259},{\"end\":74275,\"start\":74268},{\"end\":74289,\"start\":74279},{\"end\":74511,\"start\":74505},{\"end\":74854,\"start\":74848},{\"end\":74864,\"start\":74858},{\"end\":74881,\"start\":74870},{\"end\":74891,\"start\":74885},{\"end\":74903,\"start\":74897},{\"end\":75317,\"start\":75311},{\"end\":75578,\"start\":75572},{\"end\":75590,\"start\":75584},{\"end\":75600,\"start\":75594},{\"end\":75977,\"start\":75971},{\"end\":75994,\"start\":75983},{\"end\":76004,\"start\":75998},{\"end\":76014,\"start\":76008},{\"end\":76407,\"start\":76396},{\"end\":76421,\"start\":76411},{\"end\":76828,\"start\":76822},{\"end\":76843,\"start\":76832},{\"end\":76853,\"start\":76847},{\"end\":76863,\"start\":76857},{\"end\":77296,\"start\":77290},{\"end\":77307,\"start\":77300},{\"end\":77319,\"start\":77313},{\"end\":77797,\"start\":77793},{\"end\":77806,\"start\":77803},{\"end\":77814,\"start\":77810},{\"end\":77828,\"start\":77820},{\"end\":78176,\"start\":78170},{\"end\":78188,\"start\":78180},{\"end\":78200,\"start\":78192},{\"end\":78209,\"start\":78204},{\"end\":78475,\"start\":78471},{\"end\":78487,\"start\":78479},{\"end\":78500,\"start\":78491},{\"end\":78510,\"start\":78504},{\"end\":78524,\"start\":78514},{\"end\":78535,\"start\":78530},{\"end\":78963,\"start\":78957},{\"end\":78974,\"start\":78967},{\"end\":78985,\"start\":78978},{\"end\":78995,\"start\":78989},{\"end\":79007,\"start\":79001},{\"end\":79364,\"start\":79362},{\"end\":79375,\"start\":79370},{\"end\":79653,\"start\":79646},{\"end\":80044,\"start\":80039},{\"end\":80054,\"start\":80048},{\"end\":80063,\"start\":80058},{\"end\":80072,\"start\":80067},{\"end\":80499,\"start\":80494},{\"end\":80843,\"start\":80841},{\"end\":80851,\"start\":80847},{\"end\":80862,\"start\":80855},{\"end\":80871,\"start\":80866},{\"end\":80878,\"start\":80875},{\"end\":80887,\"start\":80882},{\"end\":81396,\"start\":81389},{\"end\":81403,\"start\":81400},{\"end\":81410,\"start\":81407},{\"end\":81423,\"start\":81414},{\"end\":81432,\"start\":81427},{\"end\":81825,\"start\":81811},{\"end\":81833,\"start\":81829},{\"end\":81840,\"start\":81837},{\"end\":81849,\"start\":81844},{\"end\":82187,\"start\":82178},{\"end\":82198,\"start\":82191},{\"end\":82209,\"start\":82202},{\"end\":82678,\"start\":82673},{\"end\":83064,\"start\":83054},{\"end\":83076,\"start\":83068},{\"end\":83088,\"start\":83080},{\"end\":83102,\"start\":83092},{\"end\":83115,\"start\":83106},{\"end\":83128,\"start\":83119},{\"end\":83711,\"start\":83707},{\"end\":83725,\"start\":83715},{\"end\":83736,\"start\":83729},{\"end\":83743,\"start\":83740},{\"end\":83754,\"start\":83747},{\"end\":83766,\"start\":83758},{\"end\":84215,\"start\":84205},{\"end\":84226,\"start\":84219},{\"end\":84240,\"start\":84230},{\"end\":84596,\"start\":84591},{\"end\":84608,\"start\":84600},{\"end\":84618,\"start\":84612},{\"end\":84933,\"start\":84927},{\"end\":84945,\"start\":84939},{\"end\":84960,\"start\":84951},{\"end\":85259,\"start\":85255},{\"end\":85267,\"start\":85263},{\"end\":85277,\"start\":85271},{\"end\":85286,\"start\":85284},{\"end\":85292,\"start\":85290},{\"end\":85303,\"start\":85296},{\"end\":85629,\"start\":85624},{\"end\":85639,\"start\":85635},{\"end\":85873,\"start\":85863}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":61153561},\"end\":62201,\"start\":61794},{\"attributes\":{\"doi\":\"10.1145/3310273.3321561\",\"id\":\"b1\"},\"end\":62278,\"start\":62203},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":18000518},\"end\":62666,\"start\":62280},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":232265866},\"end\":63056,\"start\":62668},{\"attributes\":{\"id\":\"b4\"},\"end\":63222,\"start\":63058},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":54088375},\"end\":63578,\"start\":63224},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":29766710},\"end\":63859,\"start\":63580},{\"attributes\":{\"doi\":\"no. 1301409\",\"id\":\"b7\",\"matched_paper_id\":5883687},\"end\":64276,\"start\":63861},{\"attributes\":{\"doi\":\"10.1088/1742-6596/1559/1/012002\",\"id\":\"b8\",\"matched_paper_id\":225853238},\"end\":64750,\"start\":64278},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":71150150},\"end\":65149,\"start\":64752},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":237416407},\"end\":65719,\"start\":65151},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":677218},\"end\":65965,\"start\":65721},{\"attributes\":{\"id\":\"b12\"},\"end\":66089,\"start\":65967},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3619626},\"end\":66401,\"start\":66091},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":202547739},\"end\":66793,\"start\":66403},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202488191},\"end\":67219,\"start\":66795},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":222336205},\"end\":67677,\"start\":67221},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":213273077},\"end\":68020,\"start\":67679},{\"attributes\":{\"id\":\"b18\"},\"end\":68255,\"start\":68022},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":17299045},\"end\":68597,\"start\":68257},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195908774},\"end\":69010,\"start\":68599},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10328909},\"end\":69437,\"start\":69012},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206592484},\"end\":69689,\"start\":69439},{\"attributes\":{\"id\":\"b23\"},\"end\":69988,\"start\":69691},{\"attributes\":{\"id\":\"b24\"},\"end\":70212,\"start\":69990},{\"attributes\":{\"id\":\"b25\"},\"end\":70466,\"start\":70214},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8397602},\"end\":70823,\"start\":70468},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3856085},\"end\":71191,\"start\":70825},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53234989},\"end\":71646,\"start\":71193},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":232373124},\"end\":72114,\"start\":71648},{\"attributes\":{\"id\":\"b30\"},\"end\":72472,\"start\":72116},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":245336427},\"end\":72987,\"start\":72474},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":247789751},\"end\":73526,\"start\":72989},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":244950656},\"end\":73883,\"start\":73528},{\"attributes\":{\"id\":\"b34\"},\"end\":74025,\"start\":73885},{\"attributes\":{\"id\":\"b35\"},\"end\":74200,\"start\":74027},{\"attributes\":{\"id\":\"b36\"},\"end\":74446,\"start\":74202},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":10569020},\"end\":74718,\"start\":74448},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":51614496},\"end\":75228,\"start\":74720},{\"attributes\":{\"id\":\"b39\"},\"end\":75478,\"start\":75230},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":15654769},\"end\":75852,\"start\":75480},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":243985859},\"end\":76337,\"start\":75854},{\"attributes\":{\"id\":\"b42\"},\"end\":76687,\"start\":76339},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":245634573},\"end\":77196,\"start\":76689},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":9812826},\"end\":77712,\"start\":77198},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":21313963},\"end\":78088,\"start\":77714},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":233975748},\"end\":78401,\"start\":78090},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":241035584},\"end\":78861,\"start\":78403},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":12008695},\"end\":79299,\"start\":78863},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":216080530},\"end\":79563,\"start\":79301},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":256444632},\"end\":79921,\"start\":79565},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":247031004},\"end\":80411,\"start\":79923},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":209093915},\"end\":80754,\"start\":80413},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":251744377},\"end\":81297,\"start\":80756},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":243896150},\"end\":81730,\"start\":81299},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":243878454},\"end\":82125,\"start\":81732},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":17089484},\"end\":82604,\"start\":82127},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":38407095},\"end\":82989,\"start\":82606},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":31428794},\"end\":83382,\"start\":82991},{\"attributes\":{\"id\":\"b59\"},\"end\":83519,\"start\":83384},{\"attributes\":{\"id\":\"b60\"},\"end\":83631,\"start\":83521},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":11605725},\"end\":84145,\"start\":83633},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":45655702},\"end\":84516,\"start\":84147},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":128140225},\"end\":84847,\"start\":84518},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":20374012},\"end\":85198,\"start\":84849},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":57246310},\"end\":85580,\"start\":85200},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":17348212},\"end\":85794,\"start\":85582},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":212688487},\"end\":86203,\"start\":85796}]", "bib_title": "[{\"end\":61833,\"start\":61794},{\"end\":62343,\"start\":62280},{\"end\":62769,\"start\":62668},{\"end\":63317,\"start\":63224},{\"end\":63615,\"start\":63580},{\"end\":63962,\"start\":63861},{\"end\":64375,\"start\":64278},{\"end\":64830,\"start\":64752},{\"end\":65259,\"start\":65151},{\"end\":65774,\"start\":65721},{\"end\":66130,\"start\":66091},{\"end\":66481,\"start\":66403},{\"end\":66855,\"start\":66795},{\"end\":67307,\"start\":67221},{\"end\":67764,\"start\":67679},{\"end\":68298,\"start\":68257},{\"end\":68662,\"start\":68599},{\"end\":69090,\"start\":69012},{\"end\":69469,\"start\":69439},{\"end\":70568,\"start\":70468},{\"end\":70916,\"start\":70825},{\"end\":71312,\"start\":71193},{\"end\":71755,\"start\":71648},{\"end\":72159,\"start\":72116},{\"end\":72584,\"start\":72474},{\"end\":73083,\"start\":72989},{\"end\":73606,\"start\":73528},{\"end\":74501,\"start\":74448},{\"end\":74844,\"start\":74720},{\"end\":75568,\"start\":75480},{\"end\":75967,\"start\":75854},{\"end\":76818,\"start\":76689},{\"end\":77286,\"start\":77198},{\"end\":77789,\"start\":77714},{\"end\":78166,\"start\":78090},{\"end\":78465,\"start\":78403},{\"end\":78953,\"start\":78863},{\"end\":79358,\"start\":79301},{\"end\":79642,\"start\":79565},{\"end\":80035,\"start\":79923},{\"end\":80490,\"start\":80413},{\"end\":80837,\"start\":80756},{\"end\":81385,\"start\":81299},{\"end\":81807,\"start\":81732},{\"end\":82174,\"start\":82127},{\"end\":82669,\"start\":82606},{\"end\":83050,\"start\":82991},{\"end\":83703,\"start\":83633},{\"end\":84201,\"start\":84147},{\"end\":84585,\"start\":84518},{\"end\":84921,\"start\":84849},{\"end\":85251,\"start\":85200},{\"end\":85618,\"start\":85582},{\"end\":85859,\"start\":85796}]", "bib_author": "[{\"end\":61846,\"start\":61835},{\"end\":61853,\"start\":61846},{\"end\":61864,\"start\":61853},{\"end\":61873,\"start\":61864},{\"end\":61886,\"start\":61873},{\"end\":61899,\"start\":61886},{\"end\":62355,\"start\":62345},{\"end\":62367,\"start\":62355},{\"end\":62377,\"start\":62367},{\"end\":62386,\"start\":62377},{\"end\":62402,\"start\":62386},{\"end\":62781,\"start\":62771},{\"end\":63332,\"start\":63319},{\"end\":63630,\"start\":63617},{\"end\":63644,\"start\":63630},{\"end\":63657,\"start\":63644},{\"end\":63977,\"start\":63964},{\"end\":63991,\"start\":63977},{\"end\":64004,\"start\":63991},{\"end\":64390,\"start\":64377},{\"end\":64403,\"start\":64390},{\"end\":64843,\"start\":64832},{\"end\":64853,\"start\":64843},{\"end\":64862,\"start\":64853},{\"end\":64874,\"start\":64862},{\"end\":65271,\"start\":65261},{\"end\":65280,\"start\":65271},{\"end\":65788,\"start\":65776},{\"end\":65802,\"start\":65788},{\"end\":66141,\"start\":66132},{\"end\":66150,\"start\":66141},{\"end\":66156,\"start\":66150},{\"end\":66164,\"start\":66156},{\"end\":66490,\"start\":66483},{\"end\":66496,\"start\":66490},{\"end\":66509,\"start\":66496},{\"end\":66519,\"start\":66509},{\"end\":66529,\"start\":66519},{\"end\":66870,\"start\":66857},{\"end\":67319,\"start\":67309},{\"end\":67773,\"start\":67766},{\"end\":67780,\"start\":67773},{\"end\":67787,\"start\":67780},{\"end\":67795,\"start\":67787},{\"end\":67802,\"start\":67795},{\"end\":68079,\"start\":68069},{\"end\":68090,\"start\":68079},{\"end\":68101,\"start\":68090},{\"end\":68308,\"start\":68300},{\"end\":68320,\"start\":68308},{\"end\":68327,\"start\":68320},{\"end\":68678,\"start\":68664},{\"end\":68691,\"start\":68678},{\"end\":68703,\"start\":68691},{\"end\":69099,\"start\":69092},{\"end\":69105,\"start\":69099},{\"end\":69117,\"start\":69105},{\"end\":69124,\"start\":69117},{\"end\":69482,\"start\":69471},{\"end\":69787,\"start\":69775},{\"end\":70042,\"start\":70036},{\"end\":70051,\"start\":70042},{\"end\":70058,\"start\":70051},{\"end\":70065,\"start\":70058},{\"end\":70294,\"start\":70282},{\"end\":70307,\"start\":70294},{\"end\":70576,\"start\":70570},{\"end\":70582,\"start\":70576},{\"end\":70590,\"start\":70582},{\"end\":70597,\"start\":70590},{\"end\":70930,\"start\":70918},{\"end\":71325,\"start\":71314},{\"end\":71768,\"start\":71757},{\"end\":71779,\"start\":71768},{\"end\":71789,\"start\":71779},{\"end\":71802,\"start\":71789},{\"end\":71814,\"start\":71802},{\"end\":72172,\"start\":72161},{\"end\":72180,\"start\":72172},{\"end\":72193,\"start\":72180},{\"end\":72200,\"start\":72193},{\"end\":72208,\"start\":72200},{\"end\":72600,\"start\":72586},{\"end\":72610,\"start\":72600},{\"end\":73098,\"start\":73085},{\"end\":73108,\"start\":73098},{\"end\":73120,\"start\":73108},{\"end\":73133,\"start\":73120},{\"end\":73147,\"start\":73133},{\"end\":73157,\"start\":73147},{\"end\":73614,\"start\":73608},{\"end\":73627,\"start\":73614},{\"end\":73640,\"start\":73627},{\"end\":73896,\"start\":73885},{\"end\":74084,\"start\":74073},{\"end\":74266,\"start\":74255},{\"end\":74277,\"start\":74266},{\"end\":74291,\"start\":74277},{\"end\":74513,\"start\":74503},{\"end\":74856,\"start\":74846},{\"end\":74866,\"start\":74856},{\"end\":74883,\"start\":74866},{\"end\":74893,\"start\":74883},{\"end\":74905,\"start\":74893},{\"end\":75319,\"start\":75309},{\"end\":75580,\"start\":75570},{\"end\":75592,\"start\":75580},{\"end\":75602,\"start\":75592},{\"end\":75979,\"start\":75969},{\"end\":75996,\"start\":75979},{\"end\":76006,\"start\":75996},{\"end\":76016,\"start\":76006},{\"end\":76409,\"start\":76394},{\"end\":76423,\"start\":76409},{\"end\":76830,\"start\":76820},{\"end\":76845,\"start\":76830},{\"end\":76855,\"start\":76845},{\"end\":76865,\"start\":76855},{\"end\":77298,\"start\":77288},{\"end\":77309,\"start\":77298},{\"end\":77321,\"start\":77309},{\"end\":77799,\"start\":77791},{\"end\":77808,\"start\":77799},{\"end\":77816,\"start\":77808},{\"end\":77830,\"start\":77816},{\"end\":78178,\"start\":78168},{\"end\":78190,\"start\":78178},{\"end\":78202,\"start\":78190},{\"end\":78211,\"start\":78202},{\"end\":78477,\"start\":78467},{\"end\":78489,\"start\":78477},{\"end\":78502,\"start\":78489},{\"end\":78512,\"start\":78502},{\"end\":78526,\"start\":78512},{\"end\":78537,\"start\":78526},{\"end\":78965,\"start\":78955},{\"end\":78976,\"start\":78965},{\"end\":78987,\"start\":78976},{\"end\":78997,\"start\":78987},{\"end\":79009,\"start\":78997},{\"end\":79366,\"start\":79360},{\"end\":79377,\"start\":79366},{\"end\":79655,\"start\":79644},{\"end\":79659,\"start\":79655},{\"end\":80046,\"start\":80037},{\"end\":80056,\"start\":80046},{\"end\":80065,\"start\":80056},{\"end\":80074,\"start\":80065},{\"end\":80501,\"start\":80492},{\"end\":80845,\"start\":80839},{\"end\":80853,\"start\":80845},{\"end\":80864,\"start\":80853},{\"end\":80873,\"start\":80864},{\"end\":80880,\"start\":80873},{\"end\":80889,\"start\":80880},{\"end\":81398,\"start\":81387},{\"end\":81405,\"start\":81398},{\"end\":81412,\"start\":81405},{\"end\":81425,\"start\":81412},{\"end\":81434,\"start\":81425},{\"end\":81827,\"start\":81809},{\"end\":81835,\"start\":81827},{\"end\":81842,\"start\":81835},{\"end\":81851,\"start\":81842},{\"end\":82189,\"start\":82176},{\"end\":82200,\"start\":82189},{\"end\":82211,\"start\":82200},{\"end\":82680,\"start\":82671},{\"end\":83066,\"start\":83052},{\"end\":83078,\"start\":83066},{\"end\":83090,\"start\":83078},{\"end\":83104,\"start\":83090},{\"end\":83117,\"start\":83104},{\"end\":83130,\"start\":83117},{\"end\":83713,\"start\":83705},{\"end\":83727,\"start\":83713},{\"end\":83738,\"start\":83727},{\"end\":83745,\"start\":83738},{\"end\":83756,\"start\":83745},{\"end\":83768,\"start\":83756},{\"end\":84217,\"start\":84203},{\"end\":84228,\"start\":84217},{\"end\":84242,\"start\":84228},{\"end\":84598,\"start\":84587},{\"end\":84610,\"start\":84598},{\"end\":84620,\"start\":84610},{\"end\":84935,\"start\":84923},{\"end\":84947,\"start\":84935},{\"end\":84962,\"start\":84947},{\"end\":85261,\"start\":85253},{\"end\":85269,\"start\":85261},{\"end\":85279,\"start\":85269},{\"end\":85288,\"start\":85279},{\"end\":85294,\"start\":85288},{\"end\":85305,\"start\":85294},{\"end\":85631,\"start\":85620},{\"end\":85641,\"start\":85631},{\"end\":85875,\"start\":85861}]", "bib_venue": "[{\"end\":61990,\"start\":61940},{\"end\":62474,\"start\":62442},{\"end\":62857,\"start\":62823},{\"end\":64952,\"start\":64917},{\"end\":65427,\"start\":65369},{\"end\":66597,\"start\":66567},{\"end\":66990,\"start\":66937},{\"end\":67423,\"start\":67375},{\"end\":68409,\"start\":68372},{\"end\":69207,\"start\":69181},{\"end\":69570,\"start\":69530},{\"end\":71010,\"start\":70974},{\"end\":73941,\"start\":73923},{\"end\":75666,\"start\":75638},{\"end\":77437,\"start\":77386},{\"end\":77896,\"start\":77867},{\"end\":79081,\"start\":79049},{\"end\":79745,\"start\":79706},{\"end\":80180,\"start\":80131},{\"end\":81007,\"start\":80952},{\"end\":81520,\"start\":81481},{\"end\":81937,\"start\":81898},{\"end\":82271,\"start\":82259},{\"end\":82764,\"start\":82718},{\"end\":83852,\"start\":83814},{\"end\":85393,\"start\":85353},{\"end\":86017,\"start\":85950},{\"end\":61938,\"start\":61899},{\"end\":62440,\"start\":62402},{\"end\":62821,\"start\":62781},{\"end\":63080,\"start\":63058},{\"end\":63346,\"start\":63332},{\"end\":63684,\"start\":63657},{\"end\":64042,\"start\":64015},{\"end\":64468,\"start\":64449},{\"end\":64915,\"start\":64874},{\"end\":65367,\"start\":65303},{\"end\":65809,\"start\":65802},{\"end\":66178,\"start\":66164},{\"end\":66565,\"start\":66529},{\"end\":66935,\"start\":66893},{\"end\":67373,\"start\":67319},{\"end\":67822,\"start\":67802},{\"end\":68067,\"start\":68022},{\"end\":68370,\"start\":68327},{\"end\":68729,\"start\":68718},{\"end\":69179,\"start\":69147},{\"end\":69528,\"start\":69482},{\"end\":69773,\"start\":69691},{\"end\":70034,\"start\":69990},{\"end\":70280,\"start\":70214},{\"end\":70615,\"start\":70597},{\"end\":70972,\"start\":70930},{\"end\":71374,\"start\":71340},{\"end\":71841,\"start\":71814},{\"end\":72248,\"start\":72235},{\"end\":72686,\"start\":72671},{\"end\":73204,\"start\":73181},{\"end\":73667,\"start\":73640},{\"end\":73921,\"start\":73896},{\"end\":74071,\"start\":74027},{\"end\":74253,\"start\":74202},{\"end\":74554,\"start\":74513},{\"end\":74941,\"start\":74905},{\"end\":75307,\"start\":75230},{\"end\":75636,\"start\":75602},{\"end\":76047,\"start\":76031},{\"end\":76392,\"start\":76339},{\"end\":76896,\"start\":76880},{\"end\":77384,\"start\":77344},{\"end\":77865,\"start\":77830},{\"end\":78222,\"start\":78211},{\"end\":78584,\"start\":78552},{\"end\":79047,\"start\":79009},{\"end\":79400,\"start\":79377},{\"end\":79704,\"start\":79659},{\"end\":80129,\"start\":80074},{\"end\":80553,\"start\":80501},{\"end\":80950,\"start\":80912},{\"end\":81479,\"start\":81434},{\"end\":81896,\"start\":81851},{\"end\":82257,\"start\":82211},{\"end\":82716,\"start\":82680},{\"end\":83157,\"start\":83130},{\"end\":83409,\"start\":83384},{\"end\":83556,\"start\":83521},{\"end\":83812,\"start\":83768},{\"end\":84269,\"start\":84242},{\"end\":84647,\"start\":84620},{\"end\":84989,\"start\":84962},{\"end\":85351,\"start\":85305},{\"end\":85659,\"start\":85641},{\"end\":85948,\"start\":85875}]"}}}, "year": 2023, "month": 12, "day": 17}