{"id": 207227372, "updated": "2022-02-02 20:14:08.214", "metadata": {"title": "Deep Graph Kernels", "authors": "[{\"middle\":[],\"last\":\"Yanardag\",\"first\":\"Pinar\"},{\"middle\":[],\"last\":\"Vishwanathan\",\"first\":\"S.V.N.\"}]", "venue": null, "journal": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "publication_date": {"year": 2015, "month": null, "day": null}, "abstract": "In this paper, we present Deep Graph Kernels, a unified framework to learn latent representations of sub-structures for graphs, inspired by latest advancements in language modeling and deep learning. Our framework leverages the dependency information between sub-structures by learning their latent representations. We demonstrate instances of our framework on three popular graph kernels, namely Graphlet kernels, Weisfeiler-Lehman subtree kernels, and Shortest-Path graph kernels. Our experiments on several benchmark datasets show that Deep Graph Kernels achieve significant improvements in classification accuracy over state-of-the-art graph kernels.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2008857988", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/kdd/YanardagV15", "doi": "10.1145/2783258.2783417"}}, "content": {"source": {"pdf_hash": "8ccd0adb1a00358ede79f1d9bdcce472dc1cb8d4", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "51809880b9a8ecf4a27d376d17586715145c8767", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8ccd0adb1a00358ede79f1d9bdcce472dc1cb8d4.txt", "contents": "\nDeep Graph Kernels\n\n\nPinar Yanardag \nDepartment of Computer Science\nDepartment of Computer Science\nPurdue University West Lafayette\n47906INUSA\n\nS V N Vishwanathan \nUniversity of California\n95064Santa CruzCAUSA\n\nDeep Graph Kernels\n10.1145/2783258.2783417H28 [Database Management]: Database Applications -Data Min- ingI26 [Artificial Intelligence]: LearningI51 [Pattern Recog- nition]: Model -Statistical Keywords R-convolution kernels, graph kernels, deep learning, structured data, string kernels, social networks, bioinformatics\nIn this paper, we present Deep Graph Kernels, a unified framework to learn latent representations of sub-structures for graphs, inspired by latest advancements in language modeling and deep learning. Our framework leverages the dependency information between sub-structures by learning their latent representations. We demonstrate instances of our framework on three popular graph kernels, namely Graphlet kernels, Weisfeiler-Lehman subtree kernels, and Shortest-Path graph kernels. Our experiments on several benchmark datasets show that Deep Graph Kernels achieve significant improvements in classification accuracy over state-of-the-art graph kernels.\n\nINTRODUCTION\n\nIn domains such as social networks, bioinformatics, chemoinformatics and robotics, we are often interested in computing similarities between structured objects. Graphs, including sequences and trees as special cases, offer a natural way to represent structureddata. To illustrate one example where graph similarity can be useful, consider the problem of identifying a sub-community (also referred as subreddits) on Reddit 1 . To tackle this problem, one can represent an online discussion thread as a graph where nodes represent users, and edges represent whether two users interact, for instance, by responding to each other's comments (see Figure 1 2 ).\n\nThen, the task is to predict which sub-community a discussion thread belongs to based on its communication graph. Similarly, in bioinformatics, one might be interested in the problem of identifying whether a given protein is an enzyme or not. In this case, the secondary structure of a protein is represented as a graph where nodes correspond to atoms and edges represent the chemical bonds between atoms. If the graph structure of the protein is similar to known enzymes, one can conclude that the given graph is also an enzyme [33]. Therefore, computing semantically meaningful similarities between graphs is an important problem in various domains. Figure 1: A graph of a random post on http://reddit.com/ r/askreddit.\n\nOne of the increasingly popular approaches to measure the similarity between structured objects is to use kernel methods. Roughly speaking, kernel methods measure the similarity between two objects with a kernel function which corresponds to an inner product in reproducing kernel Hilbert space (RKHS) [26]. The challenge for kernel methods is then to find a suitable kernel function that captures the semantics of the structure while being computationally tractable. R-convolution [11] is a general framework for handling discrete objects where the key idea is to recursively decompose structured objects into \"atomic\" sub-structures and define valid local kernels between them. In the case of graphs, given a graph G, let \u03c6 (G) denote a vector which contains counts of atomic substructures, and \u00b7, \u00b7 H denote a dot product in a RKHS H. Then, the kernel between two graphs G and G is given by\nK G, G = \u03c6 (G) , \u03c6 G H .(1)\n(a) Dependency of substructures (b) Exponential growth Figure 2: Dependency schema of a set of graphlets of size k \u2208 {3, 4, 5} where G39 can be derived from G15 (similarly, G15 can be derived from G7) by adding a new node and an edge (a). Exponential growth of feature space in graphlets up to size k = 9 (b).\n\nHowever, this representation does not take a number of important observations into account. First, sub-structures that are used to compute the kernel matrix are not independent. Let us consider an example on graphlets, a popular sub-structure type that is used for decomposing graphs [23,28], which are defined as induced, nonisomorphic sub-graphs of size k (see Figure 4). Graphlets exhibit a strong dependence relationship, that is, size k + 1 graphlets can be derived from size k graphlets by addition of nodes or edges (similarly, size k graphlets can be recovered by deletion of nodes or edges from size k + 1 graphlets). For instance, G39 can be derived from G15 by adding a new node and an edge (see Figure 2 (a)). Second, the dimension of the feature space often grows exponentially. Figure 2 (b) illustrates the growth of the number of unique features in graphlets as graphlet size k increases. Consequently, as the number of features grows, we encounter the sparsity problem: only a few sub-structures will be common across graphs. This leads to diagonal dominance, that is, a given graph is similar to itself but not to any other graph in the dataset. Figure 3 (a) illustrates such a kernel matrix 3 where diagonal dominance is visibly observable using Weisfeiler-Lehman subtree kernel [27]. Ideally, we would like to have a kernel matrix where all entries belonging to a class are similar to each other, and dissimilar to everything else (see Figure 3 (b)). To alleviate this problem, consider an alternative kernel between two graphs G and G such that\nK G, G = \u03c6 (G) T M\u03c6 G(2)\nwhere M represents a |V| \u00d7 |V| positive semi-definite matrix that encodes the relationship between sub-structures and V represents the vocabulary of sub-structures obtained from the training data. Therefore, one can design an M matrix that respects the similarity of the sub-structure space. In cases where there is a strong mathematical relationship between sub-structures, such as edit-distance, one can design an M matrix that respects the geometry of the space. In cases where a clear mathematical relationship between sub-structures might not exist, one can learn the geometry of the space directly from data. In this paper, we propose recipes for designing such M matrices for graph kernels. For our first recipe, we exploit an edit-distance relationship between sub-structures and directly compute an M matrix. In our second recipe, we propose 3 In both cases we observe a block matrix since the first 125 entries of the matrix correspond to graphs which are labeled as +1 and the remaining entries correspond to graphs which are labeled as \u22121. Figure 3: Kernel matrix K for Weisfeiler-Lehman subtree kernel on the MUTAG dataset [7] (a), and its deep variant obtained from our framework (b). Entry Kij encodes the similarity between graph Gi and graph Gj and the color map encodes the degree of the similarity (darker color indicates higher similarity).\n\na framework that computes an M matrix by learning latent representations of sub-structures. Our contributions are as follows:\n\n\u2022 We propose a general framework that learns hidden representations of sub-structures used in graph kernels, inspired by latest advancements in natural language processing and deep learning,\n\n\u2022 We demonstrate our framework on three popular graph kernels, namely Graphlet kernels, Weisfeiler-Lehman subtree kernels, and Shortest-Path kernels and achieve significant improvements on several benchmark datasets,\n\n\u2022 We discuss the connection of our framework to R-convolution kernels and apply our framework to derive deep variants of string kernels,\n\n\u2022 We introduce several new large graph kernel datasets in social network domain associated with novel tasks such as community prediction.\n\nThe rest of this paper is as follows. In Section 2, we review three popular families of graph kernels for which our framework is applicable. In Section 3.1, we design an M matrix for graphlet kernels by exploiting edit-distance relationship between sub-structures. In Section 3.2, we introduce deep graph kernel framework and discuss the connection of our framework to R-convolution kernels. In Section 4, we discuss related work. In Section 5, we compare the classification performance of deep graph kernels to their base variants as well as to other state-of-the-art graph kernels. We report results on classification accuracy on graph benchmark datasets and discuss the run-time cost of our framework. Section 6 concludes the paper.\n\n\nGRAPH KERNELS\n\nWe first introduce basic concepts and notation that will be used throughout the paper. Then, we discuss three major graph kernel families, namely, graph kernels based on limited-sized subgraphs [13,28], graph kernels based on subtree patterns [24,27], and graph kernels based on walks [14,33] and paths [3]. We briefly discuss each of the above kernels, and recap how they can be viewed as instances of the more general R-convolution framework [11]. \n\n\nNotation\n\nLet G = (V, E) represent a graph where V is a set of vertices and E \u2286 (V \u00d7 V ) is a set of edges. Let G represent a set of n graphs where G = {G1, G2, . . . , Gn} and let Y represent a set of labels associated with each graph in G where Y = {yG 1 , yG 2 , . . . , yG n }. Given G = (V, E) and H = (VH , EH ), H is a sub-graph of G if and only if there is an injective mapping \u03b1 : VH \u2192 V such that (v, w) \u2208 EH if and only if (\u03b1(v), \u03b1(w)) \u2208 E. A graph G is called a labeled graph if there is a function l : V \u2192 \u03a3 that assigns labels from an alphabet \u03a3 to vertices in the graph. A graph G is called an unlabeled graph if individual vertices have no distinct identifications other than their inter-connectivity. Throughout the paper, we will refer K(G, G ) as a kernel function that measures the similarity between graphs G and G .\n\nGraph classification task considers the problem of classifying graphs into two or more categories. Given a set of graphs G and a set of class labels Y, the task in graph classification is then to learn a model that maps graphs in G to the label set Y. A popular approach is to first use a graph kernel to compute a kernel matrix K of size n \u00d7 n where Kij represents the similarity between Gi and Gj, and then to plug the computed kernel matrix into a kernelized learning algorithm such as Support Vector Machines (SVM) [12] to perform classification.\n\n\nGraph kernels based on subgraphs\n\nA graphlet G is an induced and non-isomorphic sub-graph of size-k (see Figure 4) [23]. Let V k = {G1, G2, . . . , Gn k } be the set of size-k graphlets where n k denotes the number of unique graphlets of size k. Given two unlabeled graphs G and G , the graphlet kernel is defined as follows [28]:\nKGK (G, G ) = f G , f G ,(3)\nwhere f G and f G are vectors of normalized counts, that is, the i th component of f G (resp. f G ) denotes the frequency of graphlet Gi occurring as a sub-graph of G (resp. G ). Furthermore, \u00b7, \u00b7 denotes the Euclidean dot product.\n\n\nGraph kernels based on subtree patterns\n\nThe second family of graph kernels decomposes a graph into its subtree patterns. The Weisfeiler-Lehman subtree kernel [27] belongs to this family. The key idea here is to iterate over each vertex of a labeled graph and its neighbors in order to create a multiset label. The multiset at every iteration consists of the label of the vertex and the sorted labels of its neighbors. The resultant multiset is given a new label, which is then used for the next iteration. When comparing graphs, we simply count the co-occurrences of labels in both graphs. This procedure is inspired by the Weisfeiler-Lehman test of graph isomorphism, and is equivalent to comparing the number of shared subtrees between two graphs. Formally, given G and G , the Weisfeiler-Lehman subtree kernel is defined as:\nKW L(G, G ) = l G , l G .(4)\nAs before, \u00b7, \u00b7 denotes the Euclidean dot product. If we assume that we perform h iterations of relabeling, then l G consists of h blocks. The i th component in the j th block of l G contains the frequency with which the i th label was assigned to a node in the j th iteration.\n\n\nGraph kernels based on random-walks\n\nThe third family of graph kernels decomposes a graph into randomwalks [14,33] or paths [3] and counts the co-occurrence of randomwalks or paths in two graphs. Let PG represent the set of all shortestpaths in graph G, and pi \u2208 PG denote a triplet (l i s , l i e , n k ) where n k is the length of the path and l i s and l i e are the labels of the starting and ending vertices, respectively. The shortest-path kernel between labeled graphs G and G is defined as [3]:\nKSP (G, G ) = p G , p G ,(5)\nwhere the i th component of p G contains the frequency of the i th triplet occurring in graph G. The vector p G is defined analogously for G .\n\n\nA Unified View using R-convolution\n\nOne can show that all graph kernels summarized above as well as other somewhat sophisticated variants are all instances of the R-convolution framework. In a nutshell, the recipe for defining graph kernels is as follows: First, recursively decompose a graph into its subgraphs. For instance, the graphlet kernel decomposes a graph into graphlets, Weisfeiler-Lehman kernel decomposes a graph into subtrees, and Shortest-Path kernel decomposes a graph into shortest-paths. In next step, the decomposed sub-structures are represented as a vector of frequencies where each item of the vector represents how many times a given sub-structure occurs in the graph. Finally, the Euclidean space or some other domain-specific RKHS is used to define the dot product between the vectors of frequencies.\n\n\nMETHODOLOGY\n\nIn this section, we first discuss how to compute an M matrix by using the edit-distance relationship between sub-structures. Then, we discuss how to compute an M matrix by learning the similarity between sub-structures inspired by latest advancements in language modeling and deep learning.\n\n\nSub-structure similarity via edit-distance\n\nWhen sub-structures exhibit a clear mathematical relationship, one can exploit the underlying similarities between sub-structures to compute an M matrix. For instance, in graphlet kernels, one can derive an edit-distance relationship to encode how similar one graphlet to is another (see Figure 2 (a)). Given a graphlet Gi of size k, and a graphlet Gj of size k + 1, let us build an undirected edit-distance graph G by adding an undirected edge from Gi to Gj if and only if Gi can be obtained from Gj by deleting a node of Gj (or vice versa, if Gj can be obtained from Gi by adding a node to Gi). Given such an undirected graph G, one can simply compute the shortest-path distance between Gi and Gj in order to compute their edit-distance (see Figure 5). While this approach enables us to directly compute an M matrix, the cost of computing the shortest-path distances on G becomes prohibitively expensive as a function of graphlet size k (see Figure 2 (b)). For instance, in order to compute an M matrix at level k = 9, one needs to compute all pairwise shortest-path distances of an undirected graph having 288, 267 nodes. On the other hand, one can observe that while the number of unique graphlets grow exponentially, only a few of them will be observed in a given graph. Therefore, instead of computing a complete M matrix of size |V|\u00d7|V|, one can design an M matrix of size |V | \u00d7 |V | with |V | |V| by only taking the observed sub-structures into account. In next section, we discuss an approach that utilizes this observation.\n\n\nSub-structure similarity via learning\n\nOur second approach is to learn the latent representations of sub-structures by using recently introduced language modeling and deep learning techniques. The learned representations are then utilized to compute an M matrix that respects the similarity between sub-structures. Next, we review the related background in language modeling, and then transform the ideas to learn representations of sub-structures.\n\n\nNeural language models\n\nTraditional language models estimate the likelihood of a sequence of words appearing in a corpus. Given a sequence of training words {w1, w2, . . . , wT }, n-gram based language models aims to maximize the following probability Pr(wt|w1, . . . , wt\u22121).\n\nIn other words, they estimate the likelihood of observing wt given n previous words observed so far. Recent work in language modeling focused on distributed vector representation of words, also referred as word embeddings. These neural language models improve classic n-gram language models by using continuous vector representations for words. Unlike traditional n-gram models, neural language models take advantage of Figure 6: Architecture for the CBOW and Skip-gram method [20]. wt is the current word, while wt+j are the surrounding words where c is the size of the context, and \u2212c \u2264 j \u2264 c. Here, CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. the notion of context where a context is defined as a fixed number of preceding words. In practice, the objective of word embedding models is to maximize the following log-likelihood\nT t=1 log Pr(wt|wt\u2212n+1, . . . , wt\u22121),(7)\nwhere wt\u2212n+1, . . . , wt\u22121 are the context of wt. Several methods are proposed to approximate Equation 7. Next, we discuss two such methods that we utilize in our framework, namely continuous bag-of-words (CBOW) and Skip-gram models [20].\n\n\nContinuous bag-of-words\n\nCBOW model predicts the current word given the surrounding words within a given window. The model architecture is similar to feedforward neural network language model [2] where the nonlinear hidden layer is removed and the projection layer is shared for all words (see Figure 6). Formally, CBOW model aims to maximize the following log-likelihood, \n\nwhere c is the length of the context. The probability Pr(wt|wt\u2212c, . . . , wt+c) is computed using the softmax, defined as\nexp(v v w t ) V w=1 exp(v v w ) .(9)\nHere, vw corresponds to the input vector representation of w and v w t corresponds to the output vector representation of wt. The averaged vector representation from the context is computed as\nv = 1 2c \u2212c\u2264j\u2264c,j =0 vw t+j .(10)\n\nSkip-gram model\n\nThe Skip-gram model maximizes co-occurrence probability among the words that appear within a given window. In other words, instead of predicting the current word based on surrounding words, Figure 7: The learned shortest-path sub-structures on ENZYMES dataset [4] in R 2 . Each node corresponds to a different shortest-path sub-structure and colored by the value of their first dimension (labels on nodes are omitted for clarity). For instance, the shortest path substructures embedded in the cluster marked with * are 2 \u00b7 223, 2 \u00b7 224, 2 \u00b7 225, 2 \u00b7 226, 2 \u00b7 227, 2 \u00b7 228 where first two characters represent the start and end point of the shortest-path and sub-script represents the length of the path). Note that the shortest-paths having the same start and endpoints with similar shortest-path lengths are close to each other in the latent space.\n\nthe main objective of the Skip-gram is to predict the surrounding words given the current word (see Figure 6). More precisely, the objective of the Skip-gram model is to maximize the following loglikelihood, \n\nwhere the probability Pr(wt\u2212c, . . . , wt+c|wt) is computed as\n\u2212c\u2264j\u2264c,j =0 Pr(wt+j|wt).(12)\nHere, the contextual words and the current word are assumed to be independent. Furthermore, Pr(wt+j|wt) is defined as\nexp(v w t v w t+j ) V w=1 exp(v w t v w )(13)\nwhere vw and v w are the input and output vectors of word w. Hierarchical softmax and negative sampling are two efficient algorithms that are used in training the Skip-gram and CBOW models. Hierarchical softmax uses a binary Huffman tree to factorize expensive partition function of the Skip-gram model. An alternative to the Hierarchical softmax is negative sampling, which selects the contexts at random instead of considering all words in the vocabulary. In other words, if a word w appears in the context of another word w , then the vector representation of the word w is closer to the vector representation of word w comparing to any other randomly chosen words. In practice, one should try both the Skip-gram and CBOW models with Hierarchical softmax and negative sampling algorithms in order to decide which pair is more suitable to the application in hand.\n\nAfter the training converges, similar words are mapped to similar positions in the vector space. Moreover, the learned word vectors are empirically shown to preserve semantics. For instance, word vectors can be used to answer analogy questions using simple vector algebra where the result of a vector calculation v(\"Madrid\")\u2212 v(\"Spain\") + v(\"France\") is closer to v(\"Paris\") than any other word vector [20].\n\nThese properties make word vectors attractive for our task since the order independence assumption provides a flexible notion of 'nearness' for sub-structures. A key intuition we utilize in our framework is to view sub-structures in graph kernels as words that are generated from a special language. In other words, different sub-structures compose graphs in a similar way that different words form sentences when used together. With this analogy in mind, one can utilize word embedding models to unveil dimensions of similarity between sub-structures. The main expectation here is that similar sub-structures will be close to each other in the ddimensional latent space. Figure 7 illustrates shortest-path substructures in R 2 learned by our framework. Note that similar substructures are close together in latent space.\n\n\nDeep Graph Kernels\n\nOur framework takes a list of graphs G and decomposes each graph into its sub-structures. The list of decomposed sub-structures for each graph is then treated as a sentence that is generated from a vocabulary V where vocabulary V simply corresponds to the unique set of observed sub-structures in the training data. However, unlike words in a traditional text corpora, sub-structures do not have a linear co-occurrence relationship. Therefore, one needs to build a corpus where the co-occurrence relationship is meaningful. Next, we discuss how to generate corpora where co-occurrence relationship is meaningful on three major graph kernel families.\n\n\u2022 Corpus generation for graphlet kernels: Exhaustive enumeration of all graphlets in a graph G is prohibitively expensive for even moderate sized graphs [23]. Several sampling heuristics are proposed for sampling sub-graphs efficiently, such as random sampling scheme of [28]. In practice, the random sampling of graphlets of size k in a graph G involves placing a randomly generated window of size k \u00d7 k on the adjacency matrix of G and collecting the observed graphlet in that window. This procedure is repeated n times where n being the number of graphlets we would like to sample. However, since this is a random sampling scheme, it does not preserve any notion of co-occurrence relationship which is a desired property for our framework. Therefore, we modify the random sampling scheme to partially preserve the cooccurrence between graphlets by using the notion of neighborhoods. That is, whenever we randomly sample a graphlet G, we also sample its immediate neighbors. The graphlet and its neighbors are then interpreted as co-occurred by our method. Therefore, graphlets which have similar neighborhoods will acquire similar representations. While in this paper we utilized only the immediate neighbors of a graphlet, one can extend the co-occurrence relationship to consider neighborhoods of distance \u2265 1. In order to verify this intuition, we explored the idea of having a similar effect of linguistic regularities in language models to graphs. Word embeddings are known to successfully answer queries such as \"What is the word that is similar to small in the same sense as biggest is similar to big?\" where answer is correctly recovered as smallest [21]. Therefore, we investigate whether a meaningful response with a similar analogy can be recovered, such that \"What is the graphlet that is similar to a square (G27) in the same sense as triangle with a tail (G26) is similar to triangle (G22)?\". We used the multiplicative combination objective proposed by [18]:\nargmax b * \u2208V cos(b * , b) cos(b * , a * ) cos(b * , a) +(14)\nwhere a and b are sub-structures from a vocabulary V, cos is the cosine similarity between sub-structure vectors and = 0.001 is used to prevent division by zero. This objective function amplifies the differences between small quantities and reduces the differences between larger ones. We used two positive (G22, G27) and one negative example (G26), and recovered G36 as the top sub-structure by this arithmetic operation on embedded vectors (see Figure 8).\n\n\u2022 Corpus generation for Shortest-Path graph kernels: Shortestpath graph kernel compares the sorted endpoints and the length of shortest-paths that are common between two graphs. Similar to graphlet kernel, one needs to find a meaningful cooccurrence relationship between shortest-path sub-structures. One can show that all sub-paths of a shortest-path are also shortest-paths with the same source [8]. In other words, whenever we observe a shortest-path sub-structure p of length l, we must also observe all of its sub-paths of length < l as well. Inspired by this property, whenever we generate a shortestpath sub-structure, we also collect all possible shortest-path sub-structures that share the same source node, and treat them as co-occurred. Therefore, shortest-path sub-structures which have similar labels will acquire similar representations (see Figure 7).\n\n\u2022 Corpus generation for Weisfeiler-Lehman kernels: The Weisfeiler-Lehman subtree kernel iterates over each vertex and its neighbors in order to create a multiset label. The resultant multiset is given a new label, which is then used for the next iteration. Therefore, multiset labels that belong a given iteration h can be treated as co-occurred in order to partially preserve a notion of similarity.\n\nAfter generating a corpus where a co-occurrence relationship is partially preserved, we simply build the model by using CBOW or Skip-gram algorithms and train them with Hierarchical softmax or negative sampling 4 . Let s represent an arbitrary sub-structure from a vocabulary V, and \u03a6s represent learned vector representation of s using our framework. Given the vector representations of substructures, we compute a diagonal M matrix such that each entry on the diagonal, Mii computed as \u03a6i, \u03a6i where \u03a6i corresponds to learned d-dimensional hidden of sub-sequence i and Mij = 0 where i = j and 1 \u2264 i \u2264 |V| (resp. j). After computing the M matrix, we simply plug it into Equation 2 in order to compute the kernel between each sub-structure.\n\n\nOther Deep Kernels\n\nIn a similar fashion, we can plug other graph kernels into our framework such as random-walk kernels [10], labeled version of graphlet kernel [28], subtree kernels [6,24], cyclic pattern kernels [13] and p-step random-walk kernel [30]. Moreover, our framework is applicable to any R-convolution kernel where there is a notion of dependency between sub-structures, such as string kernels. String kernels are other popular instances of R-convolution kernels where we are interested in computing a kernel between two sequences. Given an input sequence S over an alphabet V and a number k \u2265 1, k\u2212spectrum of the sequence S is defined as the set of all k-length contiguous sub-sequences S contains [17]. The feature vector \u03c6 (S) is then simply constructed as a frequency vector over sub-sequences in its k-spectrum and the kernel between two sequences are computed via Equation 1. Similar to graph kernels, the co-occurrence relationship between sub-sequences are not taken into account. Similar to graph kernels, we treat all length k sub-sequences of a string as co-occurred and learn the hidden representation of each spectrum using our framework. In case of string kernels, we compute M matrix such that each entry Mij computed as \u03a6i, \u03a6j where \u03a6i corresponds to learned d-dimensional vector of sub-sequence i (resp. \u03a6j).\n\n\nRELATED WORK\n\nThe closest work to our paper is the recently proposed model, DeepWalk by [22]. DeepWalk learns social representations of vertices of graphs by modeling short random-walks. We distance ourselves from DeepWalk in several aspects. First, instead of learning similarities between nodes we are interested in learning similarities between structured objects, such as graphs and strings. In other words, DeepWalk operates on a single graph, while we are interested in the relationship between multiple graphs. Moreover, instead of using random-walks, our framework can be configured to work with any type of sub-structures, including graphlets, shortestpaths, sub-trees and strings.\n\nMany different graph kernels focusing on different types of subgraphs have been defined in the past which can be categorized into three major families: graph kernels based on limited-sized subgraphs [13], [28], graph kernels based on subtree patterns [24], [27] and graph kernels based on walks [14] and paths [3]. Our framework is complementary to existing graph and string kernels where the sub-structures have a similarity relationship between them.\n\n\nEXPERIMENTS\n\nThe aim of our experiments is threefold. First, we want to show that using an M matrix that infers the relationship between substructures improves the classification accuracy. Second, we want to show that our framework is robust to random noise. Third, we want to show that the deep kernels are comparable to or outperform state-of-the-art graph kernels in terms of classification accuracy, while remaining competitive in terms of computational requirements. Next, we discuss bioinformatics and social network datasets that we use in our experiments. \n\n\nDatasets\n\nIn order to test the efficacy of our model, we applied our framework to real-world datasets from bioinformatics and social networks (see Table 1 for summary statistics of these datasets).\n\n\nBioinformatics datasets\n\nWe applied our framework to benchmark graph kernel datasets, namely, MUTAG, PTC, ENZYMES, PROTEINS and NCI1, NCI109. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds [7] with 7 discrete labels. PTC [31] is a dataset of 344 chemical compounds that reports the carcinogenicity for male and female rats and it has 19 discrete labels. NCI1 and NCI109 [34] datasets (4100 and 4127 nodes, respectively), made publicly available by the National Cancer Institute (NCI) are two subsets of balanced datasets of chemical compounds screened for ability to suppress or inhibit the growth of a panel of human tumor cell lines, having 37 and 38 discrete labels respectively. ENZYMES is a balanced dataset of 600 protein tertiary structures obtained from [4] and has 3 discrete labels. PROTEINS is a dataset obtained from [4] where nodes are secondary structure elements (SSEs) and there is an edge between two nodes if they are neighbors in the amino-acid sequence or in 3D space. It has 3 discrete labels, representing helix, sheet or turn.\n\n\nSocial network datasets\n\nIn order to test the efficacy of our framework on social network domain, we derive several unlabeled graph datasets with different tasks as follows.\n\n\u2022 Reddit datasets: REDDIT-BINARY is a balanced dataset where each graph corresponds to an online discussion thread where nodes correspond to users, and there is an edge between two nodes if at least one of them responded to another's comment. We crawled top submissions from four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, atheism. IAmA and AskReddit are two question/answerbased subreddits and TrollXChromosomes and atheism are two discussion-based subreddits. The task is then to identify whether a given graph belongs to a question/answer-based community or a discussion-based community. REDDIT-MULTI-5K is a balanced dataset from five different subreddits, namely, worldnews, videos, AdviceAnimals, aww and mildlyinteresting where we simply label each graph with their correspondent subreddit. REDDIT-MULTI-12K is a larger variant of REDDIT-MULTI-5K, consists of 11 different subreddits, namely, AskReddit, AdviceAnimals, atheism, aww, IAmA, mildlyinteresting, Showerthoughts, videos, todayilearned, worldnews, TrollXChromosomes. The task in both datasets is to predict which subreddit a given discussion graph belongs to.\n\n\u2022 Scientific collaboration dataset: COLLAB is a scientificcollaboration dataset, derived from 3 public collaboration datasets [16], namely, High Energy Physics, Condensed Matter Physics and Astro Physics. Following the approach of [29], we generated ego-networks of different researchers from each field, and labeled each graph as the field of the researcher. The task is then to determine whether the egocollaboration graph of a researcher belongs to High Energy, Condensed Matter or Astro Physics field.\n\n\u2022 Movie collaboration datasets: IMDB-BINARY is a moviecollaboration dataset where we collected actor/actress and genre information of different movies on IMDB. For each graph, nodes represent actors/actresses and there is an edge between them if they appear in the same movie. We generated collaboration graphs on Action and Romance genres and derived ego-networks for each actor/actress. Note that a movie can belong to both genres at the same time, therefore we discarded movies from Romance genre if the movie is already included to the Action genre. Similar to COLLAB dataset, we simply labeled each ego-network with the genre graph it belongs to. The task is then simply to identify which genre an ego-network graph belongs to. IMDB-MULTI is multi-class version of IMDB-BINARY and contains a balanced set of ego-networks derived from Comedy, Romance and Sci-Fi genres.\n\n\nExperimental setup\n\nWe compare our framework against representative instances of major families of graph kernels in the literature. Other than base kernels of our framework, namely, Weisfeiler-Lehman subtree kernel [28], graphlet kernel [28], and shortest-path kernel [3], we also compare our kernels with the random walk kernel [10], the subtree kernel [24], and p-step random-walk kernel [30]. The Random-Walk, p-step Random-Walk and Ramon-G\u00e4rtner kernels are written in Matlab and were obtained from the authors of [28]. All other kernels were coded in Python 5 . In order to ensure a fair comparison, all experiments are performed on the same hardware.\n\nAll kernels are normalized to have a unit length in the feature space. Moreover, we use 10-fold cross validation with a binary C-SVM [5] to test classification performance. The C value for each fold is independently tuned using training data from that fold. In order to exclude random effects of the fold assignments, this experiment is repeated 10 times, and average prediction accuracies with their standard deviations are reported.\n\n\nParameter selection\n\nWe chose parameters for the various kernels as follows: the window size and dimension for deep graph kernels is chosen from {2, 5, 10, 25, 50}, the decay factor for random-walk kernels is chosen from 10 \u22126 , 10 \u22125 , . . . , 10 \u22121 , the p value in the p-step randomwalk kernel is chosen from {1, 2, . . . , 10} and the height parameter in Ramon-G\u00e4rtner subtree kernel is chosen from {1, 2, 3}. For each kernel, we report the results for the parameter which gave the best classification accuracy. For Weisfeiler-Lehman subtree kernel, we experimented with the height parameter h = 2 due to exponentially increasing feature space of the original kernel. For the (a) Edit-distance graphlet kernels (b) Deep graphlet kernels graphlet kernel, we set the size of the graphlets k to be 7 since it exhibits the sparsity problem that we are interested in. We used Nauty [19] to get canonically-labeled isomorphic representations of each graphlet which are then used to construct the feature representation.\n\n\nResults\n\nIn this section, we apply our framework to several benchmark datasets and compare the classification accuracy of our kernels against their base variants.\n\nGraphlet Kernels under noise We have two variants of graphlet kernels, namely, Edit-distance Graphlet Kernel (EGK) introduced in Section 3.1 and Deep Graphlet Kernel (DGK) introduced in Section 3.2. Since graphlet kernels do not exploit label information on the vertices and only compare graphs based on their structural similarity, an interesting experiment is to see how our kernels behave under random noise on the edges. Therefore, we derive noisy variants of the datasets by randomly flipping 10%, 20% and 30% of the edges. Figure 9 (a) shows the comparison between original graphlet kernel and EGK where 0% represents the classification accuracy on the original dataset without noise. As can be seen from the figure, EGK outperforms the base kernel in MUTAG, PTC, PROTEINS, NCI1, NCI109, but outperformed by the original kernel in ENZYMES dataset. We believe this is due to the fact that EGK only uses a mathematical relationship between sub-structures rather than learning a sophisticated relationship. Therefore, we applied our deep kernel framework on graphlet kernels (see Figure  9) (b). As can be seen from the figure, learning latent representations of the graphlets outperforms its base variant significantly in all datasets except PROTEINS.\n\nGraphlet Kernels on social network datasets Next, we test the efficacy of our framework on several social network datasets using graphlet kernels. As can be seen from Table 2, deep graphlet kernels are able to outperform its base variant in all cases.\n\nDeep Graph Kernels on bioinformatics datasets Table 3 shows the classification accuracy between graph kernels and their deep variants using Graphlet kernel, Weisfeiler-Lehman kernel, and Shortest-Path kernel. As can be seen from the table, our method is comparable or outperforms the base variants in all datasets. Comparison against other kernels Table 4 shows the classification accuracy of Ramon & G\u00e4rtner, p-random-walk and randomwalk graph kernels where the first column is constructed by picking the best result of Deep Graph Kernels from Table 3. As can be seen from Table 4, Deep Graph Kernels are able to outperform other graph kernels.\n\n\nComputational Cost\n\nFor Edit-distance Graphlet Kernel, computing an M matrix involves a one-time computation of the undirected graph between 1253 nodes for k = 7 which empirically takes 7 minutes. After that, one needs to compute all-pairs-shortest-path distances on the obtained undirected graph which empirically takes 8 seconds. For deep graph kernels, the overhead of computing an M matrix involves learning latent representations of the observed substructures. The runtime averaged out of all datasets for learning the latent representations is 21.5 seconds for deep graphlet kernel, 4.5 seconds for deep shortest-path graph kernel and 1.75 seconds for deep Weisfeiler-Lehman graph kernel. All runtime experiments use a fixed window size and dimension at 25 and this process is repeated 10 times to eliminate random effects.\n\n\nDeep String Kernels\n\nAs a proof-of-concept, we derive a deep variant of k-spectrum string kernel and perform experiments on benchmark bioinformatics datasets.  \n\n\nDatasets\n\nIn order to test the efficacy of our model, we applied our method to benchmark datasets in string kernels. SCOP (Structural Classification of Proteins) is a manually-curated database that groups proteins together based on their 3-D structures [1]. The task is then to classify protein sequences into 7 distinct super-families. SCOP database has a 4-level structure-based hierarchy of classes where protein sequences are classified into one of the classes, namely, class, fold, super-family and family. Similar to the setting in [32], we tackled family and super-family classification problems where a family contains proteins with clear evolutionary relationship, and super-family contains the same evolutionary origin without being detectable at the level of sequences [15]. In family-classification problem, we considered NAD(P)-binding Rossmann-fold and (trans)glycosidases domains where our main task is to classify proteins in a super-family into their families. NAD(P)-Rossmann dataset has 246 sequences having an average length of 218 with 6 classes where (trans)-glycosidases has 95 sequences having an average length of 375 with 2 classes.\n\nIn super-family classification problem, we used Triose Phosphate Isomerase (TIM) beta/alpha-barrel protein fold where we classify each protein to one of the 7 distinct super-families. TIM beta/alpha dataset has 330 sequences having an average length of 332 with 7 classes. In order to derive the labels of each sequence, we used Astral SCOPe 2.04 genetic domain sequence database [9], based on PDB SEQRES records, with less than 95% identity to each other.\n\nMoreover, we derived a new dataset for string kernels using the transcripts of TED.com talks. We collected the transcript of 385 talks having an average length of 9425 from three categories, namely, Technology, Entertainment, Design. The task is then to predict which of the three category a talk belongs to.\n\n\nResults\n\nThe comparison between original k-spectrum string kernel with k = 3 and our method can be seen from Table 5. As shown in Table  5, deep variant of k-spectrum string kernel is able to outperform the base k-spectrum string kernel in all datasets. \n\n\nCONCLUSION\n\nWe presented a novel framework for graph kernels inspired by latest advancements in natural language processing and deep learning. We applied our framework to three popular graph kernels, namely, graphlet kernel, shortest-path kernel, and Weisfeiler-Lehman subtree kernels. We introduced several large graph kernel datasets in social network domain, and showed that our framework outperforms its base variants in terms of classification accuracy while introducing a negligible overhead.\n\nMoreover, while we mainly restricted ourselves to graph kernels in this paper, we discussed that our framework is rather general, and lends itself to many extensions. For instance, it can be plugged directly into any R-convolution kernel as long as there is a dependency between sub-structures. We demonstrated one such extension on string kernels and achieved significant improvements in classification accuracy.\n\nAn interesting extension of our framework would be applying it to attributed graphs with continuous values. Since a certain divergence between attribute values needs to be tolerated, learning hidden representations of the sub-structures would help to obtain a better classification accuracy.\n\nFigure 4 :\n4Connected, non-isomorphic induced sub-graphs of size k \u2264 5.\n\nFigure 5 :\n5Undirected edit-distance graph G for graphlets of size k \u2264 5 (size and colors of the nodes are based on degree).\n\n\n(wt|wt\u2212c, . . . , wt+c),\n\n\n(wt\u2212c, . . . , wt+c|wt).\n\nFigure 8 :\n8An example of sub-structure regularity in graphlet substructure space.\n\nFigure 9 :\n9Classification accuracy (y-axis) vs. edge noise (x-axis) on different datasets using Graphlet kernel vs. Edit-distance graphlet kernel (a) and Graphlet kernel vs. Deep graphlet kernel (b). Dashed lines represent original graphlet kernel while non-dashed lines represents its corresponding variant derived from our framework.\n\nTable 1 :\n1Properties of the Bioinformatics and Social network datasets used in graph kernel experiments.Dataset \nSize \nClasses Avg.nodes Labels \nMUTAG \n188 \n2 \n17.9 \n7 \nPTC \n344 \n2 \n25.5 \n19 \nENZYMES \n600 \n6 \n32.6 \n3 \nPROTEINS \n1113 \n2 \n39.1 \n3 \nNCI1 \n4110 \n2 \n29.8 \n37 \nNCI109 \n4127 \n2 \n29.6 \n38 \nCOLLAB \n5000 \n3 \n74.49 \n-\nIMDB-BINARY \n1000 \n2 \n19.77 \n-\nIMDB-MULTI \n1500 \n3 \n13 \n-\nREDDIT-BINARY \n2000 \n2 \n429.61 \n-\nREDDIT-MULTI-5K \n5000 \n2 \n508.5 \n-\nREDDIT-MULTI-12K 11929 11 \n391.4 \n-\n\n\n\nTable 2 :\n2Comparison of classification accuracy (\u00b1 standard deviation) of the Graphlet Kernel (GK) and Deep Graphlet Kernel (DGK) on social network datasets. 84 \u00b1 0.28 73.09 \u00b1 0.25 IMDB-BINARY 65.87 \u00b1 0.98 66.96 \u00b1 0.56 IMDB-MULTI 43.89 \u00b1 0.38 44.55 \u00b1 0.52 REDDIT-BINARY 77.34 \u00b1 0.18 78.04 \u00b1 0.39 REDDIT-MULTI-5K 41.01 \u00b1 0.17 41.27 \u00b1 0.18 REDDIT-MULTI-12K 31.82 \u00b1 0.08 32.22 \u00b1 0.10Dataset \nGK \nDGK \nCOLLAB \n72.\n\nTable 3 :\n3Comparison of classification accuracy (\u00b1 standard deviation) of the graphlet kernel (GK), shortest-path kernel (SP), Weisfeiler-Lehman kernel (WL) to their deep variants on bioinformatics datasets. 66 \u00b1 2.11 82.66 \u00b1 1.45 85.22 \u00b1 2.43 87.44 \u00b1 2.72 80.72 \u00b1 3.00 82.94 \u00b1 2.68 PTC 57.26 \u00b1 1.41 57.32 \u00b1 1.13 58.24 \u00b1 2.44 60.08 \u00b1 2.55 56.97 \u00b1 2.01 59.17 \u00b1 1.56 ENZYMES 26.61 \u00b1 0.99 27.08 \u00b1 0.79 40.10 \u00b1 1.50 41.65 \u00b1 1.57 53.15 \u00b1 1.14 53.43 \u00b1 0.91 PROTEINS 71.67 \u00b1 0.55 71.68 \u00b1 0.50 75.07 \u00b1 0.54 75.68 \u00b1 0.54 72.92 \u00b1 0.56 73.30 \u00b1 0.Dataset \nGK \nDeep GK \nSP \nDeep SP \nWL \nDeep WL \nMUTAG \n81.82 \nNCI1 \n62.28 \u00b1 0.29 62.48 \u00b1 0.25 73.00 \u00b1 0.24 73.55 \u00b1 0.51 80.13 \u00b1 0.50 80.31 \u00b1 0.46 \nNCI109 \n62.60 \u00b1 0.19 62.69 \u00b1 0.23 73.00 \u00b1 0.21 73.26 \u00b1 0.26 80.22 \u00b1 0.34 80.32 \u00b1 0.33 \n\n\n\nTable 4 :\n4Comparison of classification accuracy (\u00b1 standard deviation) of Ramon & G\u00e4rtner, p-random-walk and random-walk graph kernels. > 72H indicates that the computation did not finish after 72 hours.Dataset \nDeep Graph Kernels Ramon&G\u00e4rtner p-random-walk Random-walk \nMUTAG \n87.44 \u00b1 2.72 \n84.88 \u00b1 1.86 \n80.05 \u00b1 1.64 \n83.72 \u00b1 1.50 \nPTC \n60.08 \u00b1 2.55 \n58.47 \u00b1 0.90 \n59.38 \u00b1 1.66 \n57.85 \u00b1 1.30 \nENZYMES \n53.43 \u00b1 0.91 \n16.96 \u00b1 1.46 \n30.01 \u00b1 1.01 \n24.16 \u00b1 1.64 \nPROTEINS \n75.68 \u00b1 0.54 \n70.73 \u00b1 0.35 \n71.16 \u00b1 0.35 \n74.22\u00b1 0.42 \nNCI1 \n80.31 \u00b1 0.46 \n56.61 \u00b1 0.53 \n> 72h \n> 72h \nNCI109 \n80.32 \u00b1 0.33 \n54.62 \u00b1 0.23 \n> 72h \n> 72h \n\n\n\nTable 5 :\n5Classification accuracy and error reduction for string kernel experiments where numbers next to the accuracy results represents the standard deviation. Dataset K-Spectrum Deep Spectrum TIM beta/alpha 67.60 \u00b1 1.13 69.03 \u00b1 1.03 (trans)glycosidases 93.88 \u00b1 2.17 95.33 \u00b1 1.02 NAD(P)-Rossmann 69.87 \u00b1 0.78 75.54 \u00b1 0.85 TED 74.31 \u00b1 0.88 77.39 \u00b1 0.97\nWe used Gensim library[25] for all algorithms.\nOur code and datasets are available at http://web.ics. purdue.edu/~ypinar/kdd\nACKNOWLEDGMENTSWe thank to anonymous KDD reviewers for their constructive comments. We also thank to Hyokun Yun, Jiasen Yang, Joon Hee Choi, Marjan Momtazpour, Parameswaran Raman, Sebastian Moreno, Shihao Ji for reviewing our paper. This work is supported by the National Science Foundation under grant No. #1219015.\nScop database in 2004: refinements integrate structure and sequence family data. A Andreeva, D Howorth, S E Brenner, T J Hubbard, C Chothia, A G Murzin, Nucleic acids research. 321supplA. Andreeva, D. Howorth, S. E. Brenner, T. J. Hubbard, C. Chothia, and A. G. Murzin. Scop database in 2004: refinements integrate structure and sequence family data. Nucleic acids research, 32(suppl 1):D226-D229, 2004.\n\nA neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, C Janvin, The Journal of Machine Learning Research. 3Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137-1155, 2003.\n\nShortest-path kernels on graphs. K M Borgwardt, H.-P Kriegel, Proc. Intl. Conf. Data Mining. Intl. Conf. Data MiningK. M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs. In Proc. Intl. Conf. Data Mining, pages 74-81, 2005.\n\nProtein function prediction via graph kernels. K M Borgwardt, C S Ong, S Sch\u00f6nauer, S V N Vishwanathan, A J Smola, H.-P Kriegel, Proceedings of Intelligent Systems in Molecular Biology (ISMB). Intelligent Systems in Molecular Biology (ISMB)Detroit, USAK. M. Borgwardt, C. S. Ong, S. Sch\u00f6nauer, S. V. N. Vishwanathan, A. J. Smola, and H.-P. Kriegel. Protein function prediction via graph kernels. In Proceedings of Intelligent Systems in Molecular Biology (ISMB), Detroit, USA, 2005.\n\nLIBSVM: a library for support vector machines. C Chang, C Lin, C. Chang and C. Lin. LIBSVM: a library for support vector machines, 2001.\n\nFast neighborhood subgraph pairwise distance kernel. F Costa, K De Grave, Proceedings of the 26th International Conference on Machine Learning. the 26th International Conference on Machine LearningF. Costa and K. De Grave. Fast neighborhood subgraph pairwise distance kernel. In Proceedings of the 26th International Conference on Machine Learning, pages 255-262, 2010.\n\nStructure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. A K Debnath, R L Lopez De Compadre, G Debnath, A J Shusterman, C Hansch, J Med Chem. 34A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shusterman, and C. Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. J Med Chem, 34:786-797, 1991.\n\nScalable kernels for graphs with continuous attributes. A Feragen, N Kasenburg, J Petersen, M De Bruijne, K Borgwardt, Advances in Neural Information Processing Systems. A. Feragen, N. Kasenburg, J. Petersen, M. de Bruijne, and K. Borgwardt. Scalable kernels for graphs with continuous attributes. In Advances in Neural Information Processing Systems, pages 216-224, 2013.\n\nScope: Structural classification of proteins-extended, integrating scop and astral data and classification of new structures. N K Fox, S E Brenner, J.-M Chandonia, Nucleic acids research. 42D1N. K. Fox, S. E. Brenner, and J.-M. Chandonia. Scope: Structural classification of proteins-extended, integrating scop and astral data and classification of new structures. Nucleic acids research, 42(D1): D304-D309, 2014.\n\nOn graph kernels: Hardness results and efficient alternatives. T G\u00e4rtner, P Flach, S Wrobel, Proc. Annual Conf. Computational Learning Theory. B. Sch\u00f6lkopf and M. K. WarmuthAnnual Conf. Computational Learning TheorySpringerT. G\u00e4rtner, P. Flach, and S. Wrobel. On graph kernels: Hardness results and efficient alternatives. In B. Sch\u00f6lkopf and M. K. Warmuth, editors, Proc. Annual Conf. Computational Learning Theory, pages 129-143. Springer, 2003.\n\nConvolution kernels on discrete structures. D Haussler, UCS-CRL-99-10UC Santa CruzTechnical ReportD. Haussler. Convolution kernels on discrete structures. Technical Report UCS-CRL-99-10, UC Santa Cruz, 1999.\n\nKernel methods in machine learning. T Hofmann, B Sch\u00f6lkopf, A J Smola, Technical Report. 156Max-Planck-Institut f\u00fcr biologische KybernetikTo appear in the Annals of StatisticsT. Hofmann, B. Sch\u00f6lkopf, and A. J. Smola. Kernel methods in machine learning. Technical Report 156, Max-Planck-Institut f\u00fcr biologische Kybernetik, 2006. To appear in the Annals of Statistics.\n\nCyclic pattern kernels for predictive graph mining. T Horvath, T G\u00e4rtner, S Wrobel, Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD). the International Conference on Knowledge Discovery and Data Mining (KDD)T. Horvath, T. G\u00e4rtner, and S. Wrobel. Cyclic pattern kernels for predictive graph mining. In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD), pages 158-167, 2004.\n\nMarginalized kernels between labeled graphs. H Kashima, K Tsuda, A Inokuchi, Proc. Intl. Conf. Machine Learning. Intl. Conf. Machine LearningSan Francisco, CAMorgan KaufmannH. Kashima, K. Tsuda, and A. Inokuchi. Marginalized kernels between labeled graphs. In Proc. Intl. Conf. Machine Learning, pages 321-328, San Francisco, CA, 2003. Morgan Kaufmann.\n\nProtein classification via kernel matrix completion. T Kin, T Kato, K Tsuda, Kernel Methods in Computational Biology. B. Sch\u00f6lkopf, K. Tsuda, and J.-P. VertCambridge, MA; USAMIT PressT. Kin, T. Kato, and K. Tsuda. Protein classification via kernel matrix completion. In B. Sch\u00f6lkopf, K. Tsuda, and J.-P. Vert, editors, Kernel Methods in Computational Biology, pages 261-274, Cambridge, MA; USA, 2004. MIT Press.\n\nGraphs over time: densification laws, shrinking diameters and possible explanations. J Leskovec, J Kleinberg, C Faloutsos, Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. the eleventh ACM SIGKDD international conference on Knowledge discovery in data miningACMJ. Leskovec, J. Kleinberg, and C. Faloutsos. Graphs over time: densification laws, shrinking diameters and possible explanations. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 177-187. ACM, 2005.\n\nThe spectrum kernel: A string kernel for SVM protein classification. C Leslie, E Eskin, W S Noble, Proceedings of the Pacific Symposium on Biocomputing. the Pacific Symposium on BiocomputingSingaporeWorld Scientific PublishingC. Leslie, E. Eskin, and W. S. Noble. The spectrum kernel: A string kernel for SVM protein classification. In Proceedings of the Pacific Symposium on Biocomputing, pages 564-575, Singapore, 2002. World Scientific Publishing.\n\nNeural word embedding as implicit matrix factorization. O Levy, Y Goldberg, Advances in Neural Information Processing Systems. M. Welling, Z. Ghahramani, C. Cortes, N. Lawrence, and K. Weinberger27O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In M. Welling, Z. Ghahramani, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2177-2185, 2014.\n\nNauty user's guide (version 2.4). B D Mckay, Computer Science Dept., Australian National UniversityB. D. McKay. Nauty user's guide (version 2.4). Computer Science Dept., Australian National University, 2007.\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintT. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G Corrado, J Dean, Advances in Neural Information Processing Systems 26. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. WeinbergerT. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, 2013.\n\nB Perozzi, R Al-Rfou, S Skiena, Deepwalk, arXiv:1403.6652Online learning of social representations. arXiv preprintB. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. arXiv preprint arXiv:1403.6652, 2014.\n\nBiological network comparison using graphlet degree distribution. N Przulj, European Conference on Computational Biology. N. Przulj. Biological network comparison using graphlet degree distribution. In 2006 European Conference on Computational Biology (ECCB), September 2006.\n\nExpressivity versus efficiency of graph kernels. J Ramon, T G\u00e4rtner, First International Workshop on Mining Graphs, Trees and Sequences (held with ECML/PKDD'03). Technical reportJ. Ramon and T. G\u00e4rtner. Expressivity versus efficiency of graph kernels. Technical report, First International Workshop on Mining Graphs, Trees and Sequences (held with ECML/PKDD'03), 2003.\n\nSoftware Framework for Topic Modelling with Large Corpora. R \u0158eh\u016f\u0159ek, P Sojka, Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. the LREC 2010 Workshop on New Challenges for NLP FrameworksValletta, MaltaELRAR.\u0158eh\u016f\u0159ek and P. Sojka. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45-50, Valletta, Malta, May 2010. ELRA.\n\nLearning with Kernels. B Sch\u00f6lkopf, A J Smola, MIT PressCambridge, MAB. Sch\u00f6lkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.\n\nFast subtree kernels on graphs. N Shervashidze, K Borgwardt, Neural Information Processing Systems. N. Shervashidze and K. Borgwardt. Fast subtree kernels on graphs. In Neural Information Processing Systems, 2010.\n\nEfficient graphlet kernels for large graph comparison. N Shervashidze, S V N Vishwanathan, T Petri, K Mehlhorn, K Borgwardt, Proc. Intl. Conference on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics. M. Welling and D. van DykIntl. Conference on Artificial Intelligence and Statistics. Society for Artificial Intelligence and StatisticsN. Shervashidze, S. V. N. Vishwanathan, T. Petri, K. Mehlhorn, and K. Borgwardt. Efficient graphlet kernels for large graph comparison. In M. Welling and D. van Dyk, editors, Proc. Intl. Conference on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics, 2009.\n\nA new space for comparing graphs. A Shrivastava, P Li, Advances in Social Networks Analysis and Mining (ASONAM). A. Shrivastava and P. Li. A new space for comparing graphs. In Advances in Social Networks Analysis and Mining (ASONAM), 2014\n\nIEEE/ACM International Conference on. IEEEIEEE/ACM International Conference on, pages 62-71. IEEE, 2014.\n\nKernels and regularization on graphs. A J Smola, R Kondor, Proc. Annual Conf. Computational Learning Theory. B. Sch\u00f6lkopf and M. K. WarmuthAnnual Conf. Computational Learning TheoryHeidelberg, GermanySpringer-VerlagA. J. Smola and R. Kondor. Kernels and regularization on graphs. In B. Sch\u00f6lkopf and M. K. Warmuth, editors, Proc. Annual Conf. Computational Learning Theory, Lecture Notes in Comput. Sci., pages 144-158, Heidelberg, Germany, 2003. Springer-Verlag.\n\nStatistical evaluation of the predictive toxicology challenge. H Toivonen, A Srinivasan, R D King, S Kramer, C Helma, Bioinformatics. 1910H. Toivonen, A. Srinivasan, R. D. King, S. Kramer, and C. Helma. Statistical evaluation of the predictive toxicology challenge 2000-2001. Bioinformatics, 19(10):1183-1193, July 2003.\n\nKernel extrapolation. S V N Vishwanathan, K M Borgwardt, O Guttman, A J Smola, Neurocomputing. 697-9S. V. N. Vishwanathan, K. M. Borgwardt, O. Guttman, and A. J. Smola. Kernel extrapolation. Neurocomputing, 69(7-9):721-729, 2006.\n\nGraph kernels. S V N Vishwanathan, N N Schraudolph, I R Kondor, K M Borgwardt, Journal of Machine Learning Research. In pressS. V. N. Vishwanathan, N. N. Schraudolph, I. R. Kondor, and K. M. Borgwardt. Graph kernels. Journal of Machine Learning Research, 2010. In press.\n\nComparison of descriptor spaces for chemical compound retrieval and classification. N Wale, I A Watson, G Karypis, Knowledge and Information Systems. 143N. Wale, I. A. Watson, and G. Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. Knowledge and Information Systems, 14(3):347-375, 2008.\n", "annotations": {"author": "[{\"start\":\"22\",\"end\":\"144\"},{\"start\":\"145\",\"end\":\"211\"}]", "publisher": null, "author_last_name": "[{\"start\":\"28\",\"end\":\"36\"},{\"start\":\"151\",\"end\":\"163\"}]", "author_first_name": "[{\"start\":\"22\",\"end\":\"27\"},{\"start\":\"145\",\"end\":\"146\"},{\"start\":\"147\",\"end\":\"150\"}]", "author_affiliation": "[{\"start\":\"38\",\"end\":\"143\"},{\"start\":\"165\",\"end\":\"210\"}]", "title": "[{\"start\":\"1\",\"end\":\"19\"},{\"start\":\"212\",\"end\":\"230\"}]", "venue": null, "abstract": "[{\"start\":\"531\",\"end\":\"1185\"}]", "bib_ref": "[{\"start\":\"2387\",\"end\":\"2391\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"2883\",\"end\":\"2887\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"3063\",\"end\":\"3067\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"4098\",\"end\":\"4102\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"4102\",\"end\":\"4105\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"5111\",\"end\":\"5115\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"5269\",\"end\":\"5281\"},{\"start\":\"6255\",\"end\":\"6256\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"6540\",\"end\":\"6543\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"8527\",\"end\":\"8531\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"8531\",\"end\":\"8534\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"8576\",\"end\":\"8580\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"8580\",\"end\":\"8583\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"8618\",\"end\":\"8622\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"8622\",\"end\":\"8625\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"8636\",\"end\":\"8639\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"8777\",\"end\":\"8781\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"10144\",\"end\":\"10148\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"10293\",\"end\":\"10297\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"10503\",\"end\":\"10507\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"10931\",\"end\":\"10935\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"12017\",\"end\":\"12021\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"12021\",\"end\":\"12024\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"12034\",\"end\":\"12037\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"12408\",\"end\":\"12411\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"16508\",\"end\":\"16512\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"17226\",\"end\":\"17230\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"17426\",\"end\":\"17429\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"18273\",\"end\":\"18276\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"20599\",\"end\":\"20603\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"22254\",\"end\":\"22258\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"22372\",\"end\":\"22376\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"23762\",\"end\":\"23766\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"24072\",\"end\":\"24076\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"24996\",\"end\":\"24999\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"26732\",\"end\":\"26736\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"26773\",\"end\":\"26777\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"26795\",\"end\":\"26798\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"26798\",\"end\":\"26801\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"26826\",\"end\":\"26830\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"26861\",\"end\":\"26865\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"27324\",\"end\":\"27328\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"28041\",\"end\":\"28045\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"28844\",\"end\":\"28848\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"28850\",\"end\":\"28854\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"28896\",\"end\":\"28900\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"28902\",\"end\":\"28906\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"28940\",\"end\":\"28944\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"28955\",\"end\":\"28958\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"30089\",\"end\":\"30092\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"30121\",\"end\":\"30125\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"30270\",\"end\":\"30274\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"30662\",\"end\":\"30665\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"30729\",\"end\":\"30732\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"32402\",\"end\":\"32406\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"32507\",\"end\":\"32511\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"33874\",\"end\":\"33878\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"33896\",\"end\":\"33900\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"33927\",\"end\":\"33930\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"33988\",\"end\":\"33992\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"34013\",\"end\":\"34017\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"34049\",\"end\":\"34053\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"34177\",\"end\":\"34181\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"34450\",\"end\":\"34453\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"35635\",\"end\":\"35639\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"39344\",\"end\":\"39347\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"39629\",\"end\":\"39633\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"39871\",\"end\":\"39875\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"40631\",\"end\":\"40634\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"45841\",\"end\":\"45845\",\"attributes\":{\"ref_id\":\"b24\"}}]", "figure": "[{\"start\":\"42484\",\"end\":\"42556\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"42557\",\"end\":\"42682\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"42683\",\"end\":\"42709\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"42710\",\"end\":\"42736\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"42737\",\"end\":\"42820\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"42821\",\"end\":\"43158\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"43159\",\"end\":\"43649\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"43650\",\"end\":\"44061\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"44062\",\"end\":\"44834\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"44835\",\"end\":\"45462\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"45463\",\"end\":\"45818\",\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1201\",\"end\":\"1856\"},{\"start\":\"1858\",\"end\":\"2579\"},{\"start\":\"2581\",\"end\":\"3474\"},{\"start\":\"3503\",\"end\":\"3812\"},{\"start\":\"3814\",\"end\":\"5378\"},{\"start\":\"5404\",\"end\":\"6764\"},{\"start\":\"6766\",\"end\":\"6891\"},{\"start\":\"6893\",\"end\":\"7083\"},{\"start\":\"7085\",\"end\":\"7301\"},{\"start\":\"7303\",\"end\":\"7439\"},{\"start\":\"7441\",\"end\":\"7578\"},{\"start\":\"7580\",\"end\":\"8315\"},{\"start\":\"8333\",\"end\":\"8783\"},{\"start\":\"8796\",\"end\":\"9623\"},{\"start\":\"9625\",\"end\":\"10175\"},{\"start\":\"10212\",\"end\":\"10508\"},{\"start\":\"10538\",\"end\":\"10769\"},{\"start\":\"10813\",\"end\":\"11600\"},{\"start\":\"11630\",\"end\":\"11907\"},{\"start\":\"11947\",\"end\":\"12412\"},{\"start\":\"12442\",\"end\":\"12584\"},{\"start\":\"12623\",\"end\":\"13412\"},{\"start\":\"13428\",\"end\":\"13718\"},{\"start\":\"13765\",\"end\":\"15299\"},{\"start\":\"15341\",\"end\":\"15750\"},{\"start\":\"15777\",\"end\":\"16029\"},{\"start\":\"16031\",\"end\":\"16950\"},{\"start\":\"16993\",\"end\":\"17231\"},{\"start\":\"17259\",\"end\":\"17607\"},{\"start\":\"17609\",\"end\":\"17730\"},{\"start\":\"17768\",\"end\":\"17960\"},{\"start\":\"18013\",\"end\":\"18862\"},{\"start\":\"18864\",\"end\":\"19072\"},{\"start\":\"19074\",\"end\":\"19136\"},{\"start\":\"19166\",\"end\":\"19283\"},{\"start\":\"19330\",\"end\":\"20195\"},{\"start\":\"20197\",\"end\":\"20604\"},{\"start\":\"20606\",\"end\":\"21427\"},{\"start\":\"21450\",\"end\":\"22099\"},{\"start\":\"22101\",\"end\":\"24077\"},{\"start\":\"24140\",\"end\":\"24597\"},{\"start\":\"24599\",\"end\":\"25465\"},{\"start\":\"25467\",\"end\":\"25867\"},{\"start\":\"25869\",\"end\":\"26608\"},{\"start\":\"26631\",\"end\":\"27950\"},{\"start\":\"27967\",\"end\":\"28643\"},{\"start\":\"28645\",\"end\":\"29097\"},{\"start\":\"29113\",\"end\":\"29664\"},{\"start\":\"29677\",\"end\":\"29864\"},{\"start\":\"29892\",\"end\":\"30949\"},{\"start\":\"30977\",\"end\":\"31125\"},{\"start\":\"31127\",\"end\":\"32274\"},{\"start\":\"32276\",\"end\":\"32781\"},{\"start\":\"32783\",\"end\":\"33656\"},{\"start\":\"33679\",\"end\":\"34315\"},{\"start\":\"34317\",\"end\":\"34751\"},{\"start\":\"34775\",\"end\":\"35771\"},{\"start\":\"35783\",\"end\":\"35936\"},{\"start\":\"35938\",\"end\":\"37193\"},{\"start\":\"37195\",\"end\":\"37446\"},{\"start\":\"37448\",\"end\":\"38093\"},{\"start\":\"38116\",\"end\":\"38925\"},{\"start\":\"38949\",\"end\":\"39088\"},{\"start\":\"39101\",\"end\":\"40249\"},{\"start\":\"40251\",\"end\":\"40707\"},{\"start\":\"40709\",\"end\":\"41017\"},{\"start\":\"41029\",\"end\":\"41274\"},{\"start\":\"41289\",\"end\":\"41775\"},{\"start\":\"41777\",\"end\":\"42190\"},{\"start\":\"42192\",\"end\":\"42483\"}]", "formula": "[{\"start\":\"3475\",\"end\":\"3502\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"5379\",\"end\":\"5403\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"10509\",\"end\":\"10537\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"11601\",\"end\":\"11629\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"12413\",\"end\":\"12441\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"16951\",\"end\":\"16992\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"17731\",\"end\":\"17767\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"17961\",\"end\":\"17994\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"19137\",\"end\":\"19165\",\"attributes\":{\"id\":\"formula_11\"}},{\"start\":\"19284\",\"end\":\"19329\",\"attributes\":{\"id\":\"formula_12\"}},{\"start\":\"24078\",\"end\":\"24139\",\"attributes\":{\"id\":\"formula_13\"}}]", "table_ref": "[{\"start\":\"29814\",\"end\":\"29821\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"37362\",\"end\":\"37369\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"37494\",\"end\":\"37501\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"37796\",\"end\":\"37803\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"37993\",\"end\":\"38000\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"38022\",\"end\":\"38029\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"41129\",\"end\":\"41136\",\"attributes\":{\"ref_id\":\"tab_4\"}},{\"start\":\"41150\",\"end\":\"41158\",\"attributes\":{\"ref_id\":\"tab_4\"}}]", "section_header": "[{\"start\":\"1187\",\"end\":\"1199\",\"attributes\":{\"n\":\"1.\"}},{\"start\":\"8318\",\"end\":\"8331\",\"attributes\":{\"n\":\"2.\"}},{\"start\":\"8786\",\"end\":\"8794\",\"attributes\":{\"n\":\"2.1\"}},{\"start\":\"10178\",\"end\":\"10210\",\"attributes\":{\"n\":\"2.2\"}},{\"start\":\"10772\",\"end\":\"10811\",\"attributes\":{\"n\":\"2.3\"}},{\"start\":\"11910\",\"end\":\"11945\",\"attributes\":{\"n\":\"2.4\"}},{\"start\":\"12587\",\"end\":\"12621\",\"attributes\":{\"n\":\"2.5\"}},{\"start\":\"13415\",\"end\":\"13426\",\"attributes\":{\"n\":\"3.\"}},{\"start\":\"13721\",\"end\":\"13763\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"15302\",\"end\":\"15339\",\"attributes\":{\"n\":\"3.2\"}},{\"start\":\"15753\",\"end\":\"15775\",\"attributes\":{\"n\":\"3.2.1\"}},{\"start\":\"17234\",\"end\":\"17257\",\"attributes\":{\"n\":\"3.2.2\"}},{\"start\":\"17996\",\"end\":\"18011\",\"attributes\":{\"n\":\"3.2.3\"}},{\"start\":\"21430\",\"end\":\"21448\",\"attributes\":{\"n\":\"3.2.4\"}},{\"start\":\"26611\",\"end\":\"26629\",\"attributes\":{\"n\":\"3.2.5\"}},{\"start\":\"27953\",\"end\":\"27965\",\"attributes\":{\"n\":\"4.\"}},{\"start\":\"29100\",\"end\":\"29111\",\"attributes\":{\"n\":\"5.\"}},{\"start\":\"29667\",\"end\":\"29675\",\"attributes\":{\"n\":\"5.1\"}},{\"start\":\"29867\",\"end\":\"29890\",\"attributes\":{\"n\":\"5.1.1\"}},{\"start\":\"30952\",\"end\":\"30975\",\"attributes\":{\"n\":\"5.1.2\"}},{\"start\":\"33659\",\"end\":\"33677\",\"attributes\":{\"n\":\"5.2\"}},{\"start\":\"34754\",\"end\":\"34773\",\"attributes\":{\"n\":\"5.3\"}},{\"start\":\"35774\",\"end\":\"35781\",\"attributes\":{\"n\":\"5.4\"}},{\"start\":\"38096\",\"end\":\"38114\",\"attributes\":{\"n\":\"5.5\"}},{\"start\":\"38928\",\"end\":\"38947\",\"attributes\":{\"n\":\"5.6\"}},{\"start\":\"39091\",\"end\":\"39099\",\"attributes\":{\"n\":\"5.6.1\"}},{\"start\":\"41020\",\"end\":\"41027\",\"attributes\":{\"n\":\"5.6.2\"}},{\"start\":\"41277\",\"end\":\"41287\",\"attributes\":{\"n\":\"6.\"}},{\"start\":\"42485\",\"end\":\"42495\"},{\"start\":\"42558\",\"end\":\"42568\"},{\"start\":\"42738\",\"end\":\"42748\"},{\"start\":\"42822\",\"end\":\"42832\"},{\"start\":\"43160\",\"end\":\"43169\"},{\"start\":\"43651\",\"end\":\"43660\"},{\"start\":\"44063\",\"end\":\"44072\"},{\"start\":\"44836\",\"end\":\"44845\"},{\"start\":\"45464\",\"end\":\"45473\"}]", "table": "[{\"start\":\"43265\",\"end\":\"43649\"},{\"start\":\"44032\",\"end\":\"44061\"},{\"start\":\"44599\",\"end\":\"44834\"},{\"start\":\"45040\",\"end\":\"45462\"}]", "figure_caption": "[{\"start\":\"42497\",\"end\":\"42556\"},{\"start\":\"42570\",\"end\":\"42682\"},{\"start\":\"42685\",\"end\":\"42709\"},{\"start\":\"42712\",\"end\":\"42736\"},{\"start\":\"42750\",\"end\":\"42820\"},{\"start\":\"42834\",\"end\":\"43158\"},{\"start\":\"43171\",\"end\":\"43265\"},{\"start\":\"43662\",\"end\":\"44032\"},{\"start\":\"44074\",\"end\":\"44599\"},{\"start\":\"44847\",\"end\":\"45040\"},{\"start\":\"45475\",\"end\":\"45818\"}]", "figure_ref": "[{\"start\":\"1843\",\"end\":\"1851\"},{\"start\":\"2510\",\"end\":\"2518\"},{\"start\":\"3558\",\"end\":\"3566\"},{\"start\":\"4177\",\"end\":\"4185\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"4521\",\"end\":\"4529\"},{\"start\":\"4606\",\"end\":\"4614\"},{\"start\":\"4977\",\"end\":\"4985\"},{\"start\":\"6456\",\"end\":\"6464\"},{\"start\":\"10283\",\"end\":\"10291\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"14053\",\"end\":\"14061\"},{\"start\":\"14509\",\"end\":\"14517\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"14709\",\"end\":\"14717\"},{\"start\":\"16451\",\"end\":\"16459\"},{\"start\":\"17528\",\"end\":\"17536\"},{\"start\":\"18203\",\"end\":\"18211\"},{\"start\":\"18964\",\"end\":\"18972\"},{\"start\":\"21278\",\"end\":\"21286\"},{\"start\":\"24587\",\"end\":\"24595\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"25455\",\"end\":\"25463\"},{\"start\":\"36467\",\"end\":\"36475\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"37021\",\"end\":\"37030\",\"attributes\":{\"ref_id\":\"fig_5\"}}]", "bib_author_first_name": "[{\"start\":\"46342\",\"end\":\"46343\"},{\"start\":\"46354\",\"end\":\"46355\"},{\"start\":\"46365\",\"end\":\"46366\"},{\"start\":\"46367\",\"end\":\"46368\"},{\"start\":\"46378\",\"end\":\"46379\"},{\"start\":\"46380\",\"end\":\"46381\"},{\"start\":\"46391\",\"end\":\"46392\"},{\"start\":\"46402\",\"end\":\"46403\"},{\"start\":\"46404\",\"end\":\"46405\"},{\"start\":\"46705\",\"end\":\"46706\"},{\"start\":\"46715\",\"end\":\"46716\"},{\"start\":\"46727\",\"end\":\"46728\"},{\"start\":\"46738\",\"end\":\"46739\"},{\"start\":\"46976\",\"end\":\"46977\"},{\"start\":\"46978\",\"end\":\"46979\"},{\"start\":\"46991\",\"end\":\"46995\"},{\"start\":\"47228\",\"end\":\"47229\"},{\"start\":\"47230\",\"end\":\"47231\"},{\"start\":\"47243\",\"end\":\"47244\"},{\"start\":\"47245\",\"end\":\"47246\"},{\"start\":\"47252\",\"end\":\"47253\"},{\"start\":\"47265\",\"end\":\"47266\"},{\"start\":\"47267\",\"end\":\"47270\"},{\"start\":\"47285\",\"end\":\"47286\"},{\"start\":\"47287\",\"end\":\"47288\"},{\"start\":\"47296\",\"end\":\"47300\"},{\"start\":\"47712\",\"end\":\"47713\"},{\"start\":\"47721\",\"end\":\"47722\"},{\"start\":\"47856\",\"end\":\"47857\"},{\"start\":\"47865\",\"end\":\"47866\"},{\"start\":\"48328\",\"end\":\"48329\"},{\"start\":\"48330\",\"end\":\"48331\"},{\"start\":\"48341\",\"end\":\"48342\"},{\"start\":\"48343\",\"end\":\"48344\"},{\"start\":\"48364\",\"end\":\"48365\"},{\"start\":\"48375\",\"end\":\"48376\"},{\"start\":\"48377\",\"end\":\"48378\"},{\"start\":\"48391\",\"end\":\"48392\"},{\"start\":\"48741\",\"end\":\"48742\"},{\"start\":\"48752\",\"end\":\"48753\"},{\"start\":\"48765\",\"end\":\"48766\"},{\"start\":\"48777\",\"end\":\"48778\"},{\"start\":\"48791\",\"end\":\"48792\"},{\"start\":\"49185\",\"end\":\"49186\"},{\"start\":\"49187\",\"end\":\"49188\"},{\"start\":\"49194\",\"end\":\"49195\"},{\"start\":\"49196\",\"end\":\"49197\"},{\"start\":\"49207\",\"end\":\"49211\"},{\"start\":\"49537\",\"end\":\"49538\"},{\"start\":\"49548\",\"end\":\"49549\"},{\"start\":\"49557\",\"end\":\"49558\"},{\"start\":\"49967\",\"end\":\"49968\"},{\"start\":\"50168\",\"end\":\"50169\"},{\"start\":\"50179\",\"end\":\"50180\"},{\"start\":\"50192\",\"end\":\"50193\"},{\"start\":\"50194\",\"end\":\"50195\"},{\"start\":\"50554\",\"end\":\"50555\"},{\"start\":\"50565\",\"end\":\"50566\"},{\"start\":\"50576\",\"end\":\"50577\"},{\"start\":\"51000\",\"end\":\"51001\"},{\"start\":\"51011\",\"end\":\"51012\"},{\"start\":\"51020\",\"end\":\"51021\"},{\"start\":\"51362\",\"end\":\"51363\"},{\"start\":\"51369\",\"end\":\"51370\"},{\"start\":\"51377\",\"end\":\"51378\"},{\"start\":\"51807\",\"end\":\"51808\"},{\"start\":\"51819\",\"end\":\"51820\"},{\"start\":\"51832\",\"end\":\"51833\"},{\"start\":\"52369\",\"end\":\"52370\"},{\"start\":\"52379\",\"end\":\"52380\"},{\"start\":\"52388\",\"end\":\"52389\"},{\"start\":\"52390\",\"end\":\"52391\"},{\"start\":\"52808\",\"end\":\"52809\"},{\"start\":\"52816\",\"end\":\"52817\"},{\"start\":\"53224\",\"end\":\"53225\"},{\"start\":\"53226\",\"end\":\"53227\"},{\"start\":\"53461\",\"end\":\"53462\"},{\"start\":\"53472\",\"end\":\"53473\"},{\"start\":\"53480\",\"end\":\"53481\"},{\"start\":\"53491\",\"end\":\"53492\"},{\"start\":\"53752\",\"end\":\"53753\"},{\"start\":\"53763\",\"end\":\"53764\"},{\"start\":\"53776\",\"end\":\"53777\"},{\"start\":\"53784\",\"end\":\"53785\"},{\"start\":\"53795\",\"end\":\"53796\"},{\"start\":\"54201\",\"end\":\"54202\"},{\"start\":\"54212\",\"end\":\"54213\"},{\"start\":\"54223\",\"end\":\"54224\"},{\"start\":\"54512\",\"end\":\"54513\"},{\"start\":\"54772\",\"end\":\"54773\"},{\"start\":\"54781\",\"end\":\"54782\"},{\"start\":\"55152\",\"end\":\"55153\"},{\"start\":\"55163\",\"end\":\"55164\"},{\"start\":\"55558\",\"end\":\"55559\"},{\"start\":\"55571\",\"end\":\"55572\"},{\"start\":\"55573\",\"end\":\"55574\"},{\"start\":\"55722\",\"end\":\"55723\"},{\"start\":\"55738\",\"end\":\"55739\"},{\"start\":\"55960\",\"end\":\"55961\"},{\"start\":\"55976\",\"end\":\"55977\"},{\"start\":\"55978\",\"end\":\"55981\"},{\"start\":\"55996\",\"end\":\"55997\"},{\"start\":\"56005\",\"end\":\"56006\"},{\"start\":\"56017\",\"end\":\"56018\"},{\"start\":\"56617\",\"end\":\"56618\"},{\"start\":\"56632\",\"end\":\"56633\"},{\"start\":\"56967\",\"end\":\"56968\"},{\"start\":\"56969\",\"end\":\"56970\"},{\"start\":\"56978\",\"end\":\"56979\"},{\"start\":\"57457\",\"end\":\"57458\"},{\"start\":\"57469\",\"end\":\"57470\"},{\"start\":\"57483\",\"end\":\"57484\"},{\"start\":\"57485\",\"end\":\"57486\"},{\"start\":\"57493\",\"end\":\"57494\"},{\"start\":\"57503\",\"end\":\"57504\"},{\"start\":\"57738\",\"end\":\"57739\"},{\"start\":\"57740\",\"end\":\"57743\"},{\"start\":\"57758\",\"end\":\"57759\"},{\"start\":\"57760\",\"end\":\"57761\"},{\"start\":\"57773\",\"end\":\"57774\"},{\"start\":\"57784\",\"end\":\"57785\"},{\"start\":\"57786\",\"end\":\"57787\"},{\"start\":\"57962\",\"end\":\"57963\"},{\"start\":\"57964\",\"end\":\"57967\"},{\"start\":\"57982\",\"end\":\"57983\"},{\"start\":\"57984\",\"end\":\"57985\"},{\"start\":\"57999\",\"end\":\"58000\"},{\"start\":\"58001\",\"end\":\"58002\"},{\"start\":\"58011\",\"end\":\"58012\"},{\"start\":\"58013\",\"end\":\"58014\"},{\"start\":\"58303\",\"end\":\"58304\"},{\"start\":\"58311\",\"end\":\"58312\"},{\"start\":\"58313\",\"end\":\"58314\"},{\"start\":\"58323\",\"end\":\"58324\"}]", "bib_author_last_name": "[{\"start\":\"46344\",\"end\":\"46352\"},{\"start\":\"46356\",\"end\":\"46363\"},{\"start\":\"46369\",\"end\":\"46376\"},{\"start\":\"46382\",\"end\":\"46389\"},{\"start\":\"46393\",\"end\":\"46400\"},{\"start\":\"46406\",\"end\":\"46412\"},{\"start\":\"46707\",\"end\":\"46713\"},{\"start\":\"46717\",\"end\":\"46725\"},{\"start\":\"46729\",\"end\":\"46736\"},{\"start\":\"46740\",\"end\":\"46746\"},{\"start\":\"46980\",\"end\":\"46989\"},{\"start\":\"46996\",\"end\":\"47003\"},{\"start\":\"47232\",\"end\":\"47241\"},{\"start\":\"47247\",\"end\":\"47250\"},{\"start\":\"47254\",\"end\":\"47263\"},{\"start\":\"47271\",\"end\":\"47283\"},{\"start\":\"47289\",\"end\":\"47294\"},{\"start\":\"47301\",\"end\":\"47308\"},{\"start\":\"47714\",\"end\":\"47719\"},{\"start\":\"47723\",\"end\":\"47726\"},{\"start\":\"47858\",\"end\":\"47863\"},{\"start\":\"47867\",\"end\":\"47875\"},{\"start\":\"48332\",\"end\":\"48339\"},{\"start\":\"48345\",\"end\":\"48362\"},{\"start\":\"48366\",\"end\":\"48373\"},{\"start\":\"48379\",\"end\":\"48389\"},{\"start\":\"48393\",\"end\":\"48399\"},{\"start\":\"48743\",\"end\":\"48750\"},{\"start\":\"48754\",\"end\":\"48763\"},{\"start\":\"48767\",\"end\":\"48775\"},{\"start\":\"48779\",\"end\":\"48789\"},{\"start\":\"48793\",\"end\":\"48802\"},{\"start\":\"49189\",\"end\":\"49192\"},{\"start\":\"49198\",\"end\":\"49205\"},{\"start\":\"49212\",\"end\":\"49221\"},{\"start\":\"49539\",\"end\":\"49546\"},{\"start\":\"49550\",\"end\":\"49555\"},{\"start\":\"49559\",\"end\":\"49565\"},{\"start\":\"49969\",\"end\":\"49977\"},{\"start\":\"50170\",\"end\":\"50177\"},{\"start\":\"50181\",\"end\":\"50190\"},{\"start\":\"50196\",\"end\":\"50201\"},{\"start\":\"50556\",\"end\":\"50563\"},{\"start\":\"50567\",\"end\":\"50574\"},{\"start\":\"50578\",\"end\":\"50584\"},{\"start\":\"51002\",\"end\":\"51009\"},{\"start\":\"51013\",\"end\":\"51018\"},{\"start\":\"51022\",\"end\":\"51030\"},{\"start\":\"51364\",\"end\":\"51367\"},{\"start\":\"51371\",\"end\":\"51375\"},{\"start\":\"51379\",\"end\":\"51384\"},{\"start\":\"51809\",\"end\":\"51817\"},{\"start\":\"51821\",\"end\":\"51830\"},{\"start\":\"51834\",\"end\":\"51843\"},{\"start\":\"52371\",\"end\":\"52377\"},{\"start\":\"52381\",\"end\":\"52386\"},{\"start\":\"52392\",\"end\":\"52397\"},{\"start\":\"52810\",\"end\":\"52814\"},{\"start\":\"52818\",\"end\":\"52826\"},{\"start\":\"53228\",\"end\":\"53233\"},{\"start\":\"53463\",\"end\":\"53470\"},{\"start\":\"53474\",\"end\":\"53478\"},{\"start\":\"53482\",\"end\":\"53489\"},{\"start\":\"53493\",\"end\":\"53497\"},{\"start\":\"53754\",\"end\":\"53761\"},{\"start\":\"53765\",\"end\":\"53774\"},{\"start\":\"53778\",\"end\":\"53782\"},{\"start\":\"53786\",\"end\":\"53793\"},{\"start\":\"53797\",\"end\":\"53801\"},{\"start\":\"54203\",\"end\":\"54210\"},{\"start\":\"54214\",\"end\":\"54221\"},{\"start\":\"54225\",\"end\":\"54231\"},{\"start\":\"54233\",\"end\":\"54241\"},{\"start\":\"54514\",\"end\":\"54520\"},{\"start\":\"54774\",\"end\":\"54779\"},{\"start\":\"54783\",\"end\":\"54790\"},{\"start\":\"55154\",\"end\":\"55161\"},{\"start\":\"55165\",\"end\":\"55170\"},{\"start\":\"55560\",\"end\":\"55569\"},{\"start\":\"55575\",\"end\":\"55580\"},{\"start\":\"55724\",\"end\":\"55736\"},{\"start\":\"55740\",\"end\":\"55749\"},{\"start\":\"55962\",\"end\":\"55974\"},{\"start\":\"55982\",\"end\":\"55994\"},{\"start\":\"55998\",\"end\":\"56003\"},{\"start\":\"56007\",\"end\":\"56015\"},{\"start\":\"56019\",\"end\":\"56028\"},{\"start\":\"56619\",\"end\":\"56630\"},{\"start\":\"56634\",\"end\":\"56636\"},{\"start\":\"56971\",\"end\":\"56976\"},{\"start\":\"56980\",\"end\":\"56986\"},{\"start\":\"57459\",\"end\":\"57467\"},{\"start\":\"57471\",\"end\":\"57481\"},{\"start\":\"57487\",\"end\":\"57491\"},{\"start\":\"57495\",\"end\":\"57501\"},{\"start\":\"57505\",\"end\":\"57510\"},{\"start\":\"57744\",\"end\":\"57756\"},{\"start\":\"57762\",\"end\":\"57771\"},{\"start\":\"57775\",\"end\":\"57782\"},{\"start\":\"57788\",\"end\":\"57793\"},{\"start\":\"57968\",\"end\":\"57980\"},{\"start\":\"57986\",\"end\":\"57997\"},{\"start\":\"58003\",\"end\":\"58009\"},{\"start\":\"58015\",\"end\":\"58024\"},{\"start\":\"58305\",\"end\":\"58309\"},{\"start\":\"58315\",\"end\":\"58321\"},{\"start\":\"58325\",\"end\":\"58332\"}]", "bib_entry": "[{\"start\":\"46261\",\"end\":\"46664\",\"attributes\":{\"matched_paper_id\":\"16369816\",\"id\":\"b0\"}},{\"start\":\"46666\",\"end\":\"46941\",\"attributes\":{\"matched_paper_id\":\"221275765\",\"id\":\"b1\"}},{\"start\":\"46943\",\"end\":\"47179\",\"attributes\":{\"matched_paper_id\":\"1550330\",\"id\":\"b2\"}},{\"start\":\"47181\",\"end\":\"47663\",\"attributes\":{\"matched_paper_id\":\"8174592\",\"id\":\"b3\"}},{\"start\":\"47665\",\"end\":\"47801\",\"attributes\":{\"id\":\"b4\"}},{\"start\":\"47803\",\"end\":\"48172\",\"attributes\":{\"matched_paper_id\":\"16262476\",\"id\":\"b5\"}},{\"start\":\"48174\",\"end\":\"48683\",\"attributes\":{\"matched_paper_id\":\"19990980\",\"id\":\"b6\"}},{\"start\":\"48685\",\"end\":\"49057\",\"attributes\":{\"matched_paper_id\":\"218641537\",\"id\":\"b7\"}},{\"start\":\"49059\",\"end\":\"49472\",\"attributes\":{\"matched_paper_id\":\"14864309\",\"id\":\"b8\"}},{\"start\":\"49474\",\"end\":\"49921\",\"attributes\":{\"matched_paper_id\":\"10856944\",\"id\":\"b9\"}},{\"start\":\"49923\",\"end\":\"50130\",\"attributes\":{\"id\":\"b10\",\"doi\":\"UCS-CRL-99-10\"}},{\"start\":\"50132\",\"end\":\"50500\",\"attributes\":{\"matched_paper_id\":\"62273226\",\"id\":\"b11\"}},{\"start\":\"50502\",\"end\":\"50953\",\"attributes\":{\"matched_paper_id\":\"2782489\",\"id\":\"b12\"}},{\"start\":\"50955\",\"end\":\"51307\",\"attributes\":{\"matched_paper_id\":\"5129873\",\"id\":\"b13\"}},{\"start\":\"51309\",\"end\":\"51720\",\"attributes\":{\"matched_paper_id\":\"14460837\",\"id\":\"b14\"}},{\"start\":\"51722\",\"end\":\"52298\",\"attributes\":{\"matched_paper_id\":\"9077635\",\"id\":\"b15\"}},{\"start\":\"52300\",\"end\":\"52750\",\"attributes\":{\"matched_paper_id\":\"9725578\",\"id\":\"b16\"}},{\"start\":\"52752\",\"end\":\"53188\",\"attributes\":{\"matched_paper_id\":\"1190093\",\"id\":\"b17\"}},{\"start\":\"53190\",\"end\":\"53397\",\"attributes\":{\"id\":\"b18\"}},{\"start\":\"53399\",\"end\":\"53673\",\"attributes\":{\"id\":\"b19\",\"doi\":\"arXiv:1301.3781\"}},{\"start\":\"53675\",\"end\":\"54199\",\"attributes\":{\"matched_paper_id\":\"16447573\",\"id\":\"b20\"}},{\"start\":\"54201\",\"end\":\"54444\",\"attributes\":{\"id\":\"b21\",\"doi\":\"arXiv:1403.6652\"}},{\"start\":\"54446\",\"end\":\"54721\",\"attributes\":{\"matched_paper_id\":\"1410793\",\"id\":\"b22\"}},{\"start\":\"54723\",\"end\":\"55091\",\"attributes\":{\"matched_paper_id\":\"12726277\",\"id\":\"b23\"}},{\"start\":\"55093\",\"end\":\"55533\",\"attributes\":{\"matched_paper_id\":\"18593743\",\"id\":\"b24\"}},{\"start\":\"55535\",\"end\":\"55688\",\"attributes\":{\"id\":\"b25\"}},{\"start\":\"55690\",\"end\":\"55903\",\"attributes\":{\"matched_paper_id\":\"1901669\",\"id\":\"b26\"}},{\"start\":\"55905\",\"end\":\"56581\",\"attributes\":{\"matched_paper_id\":\"17557614\",\"id\":\"b27\"}},{\"start\":\"56583\",\"end\":\"56821\",\"attributes\":{\"matched_paper_id\":\"5886174\",\"id\":\"b28\"}},{\"start\":\"56823\",\"end\":\"56927\",\"attributes\":{\"id\":\"b29\"}},{\"start\":\"56929\",\"end\":\"57392\",\"attributes\":{\"matched_paper_id\":\"7326173\",\"id\":\"b30\"}},{\"start\":\"57394\",\"end\":\"57714\",\"attributes\":{\"matched_paper_id\":\"60027311\",\"id\":\"b31\"}},{\"start\":\"57716\",\"end\":\"57945\",\"attributes\":{\"matched_paper_id\":\"11337089\",\"id\":\"b32\"}},{\"start\":\"57947\",\"end\":\"58217\",\"attributes\":{\"matched_paper_id\":\"1729012\",\"id\":\"b33\"}},{\"start\":\"58219\",\"end\":\"58550\",\"attributes\":{\"matched_paper_id\":\"2596211\",\"id\":\"b34\"}}]", "bib_title": "[{\"start\":\"46261\",\"end\":\"46340\"},{\"start\":\"46666\",\"end\":\"46703\"},{\"start\":\"46943\",\"end\":\"46974\"},{\"start\":\"47181\",\"end\":\"47226\"},{\"start\":\"47803\",\"end\":\"47854\"},{\"start\":\"48174\",\"end\":\"48326\"},{\"start\":\"48685\",\"end\":\"48739\"},{\"start\":\"49059\",\"end\":\"49183\"},{\"start\":\"49474\",\"end\":\"49535\"},{\"start\":\"50132\",\"end\":\"50166\"},{\"start\":\"50502\",\"end\":\"50552\"},{\"start\":\"50955\",\"end\":\"50998\"},{\"start\":\"51309\",\"end\":\"51360\"},{\"start\":\"51722\",\"end\":\"51805\"},{\"start\":\"52300\",\"end\":\"52367\"},{\"start\":\"52752\",\"end\":\"52806\"},{\"start\":\"53675\",\"end\":\"53750\"},{\"start\":\"54446\",\"end\":\"54510\"},{\"start\":\"54723\",\"end\":\"54770\"},{\"start\":\"55093\",\"end\":\"55150\"},{\"start\":\"55690\",\"end\":\"55720\"},{\"start\":\"55905\",\"end\":\"55958\"},{\"start\":\"56583\",\"end\":\"56615\"},{\"start\":\"56929\",\"end\":\"56965\"},{\"start\":\"57394\",\"end\":\"57455\"},{\"start\":\"57716\",\"end\":\"57736\"},{\"start\":\"57947\",\"end\":\"57960\"},{\"start\":\"58219\",\"end\":\"58301\"}]", "bib_author": "[{\"start\":\"46342\",\"end\":\"46354\"},{\"start\":\"46354\",\"end\":\"46365\"},{\"start\":\"46365\",\"end\":\"46378\"},{\"start\":\"46378\",\"end\":\"46391\"},{\"start\":\"46391\",\"end\":\"46402\"},{\"start\":\"46402\",\"end\":\"46414\"},{\"start\":\"46705\",\"end\":\"46715\"},{\"start\":\"46715\",\"end\":\"46727\"},{\"start\":\"46727\",\"end\":\"46738\"},{\"start\":\"46738\",\"end\":\"46748\"},{\"start\":\"46976\",\"end\":\"46991\"},{\"start\":\"46991\",\"end\":\"47005\"},{\"start\":\"47228\",\"end\":\"47243\"},{\"start\":\"47243\",\"end\":\"47252\"},{\"start\":\"47252\",\"end\":\"47265\"},{\"start\":\"47265\",\"end\":\"47285\"},{\"start\":\"47285\",\"end\":\"47296\"},{\"start\":\"47296\",\"end\":\"47310\"},{\"start\":\"47712\",\"end\":\"47721\"},{\"start\":\"47721\",\"end\":\"47728\"},{\"start\":\"47856\",\"end\":\"47865\"},{\"start\":\"47865\",\"end\":\"47877\"},{\"start\":\"48328\",\"end\":\"48341\"},{\"start\":\"48341\",\"end\":\"48364\"},{\"start\":\"48364\",\"end\":\"48375\"},{\"start\":\"48375\",\"end\":\"48391\"},{\"start\":\"48391\",\"end\":\"48401\"},{\"start\":\"48741\",\"end\":\"48752\"},{\"start\":\"48752\",\"end\":\"48765\"},{\"start\":\"48765\",\"end\":\"48777\"},{\"start\":\"48777\",\"end\":\"48791\"},{\"start\":\"48791\",\"end\":\"48804\"},{\"start\":\"49185\",\"end\":\"49194\"},{\"start\":\"49194\",\"end\":\"49207\"},{\"start\":\"49207\",\"end\":\"49223\"},{\"start\":\"49537\",\"end\":\"49548\"},{\"start\":\"49548\",\"end\":\"49557\"},{\"start\":\"49557\",\"end\":\"49567\"},{\"start\":\"49967\",\"end\":\"49979\"},{\"start\":\"50168\",\"end\":\"50179\"},{\"start\":\"50179\",\"end\":\"50192\"},{\"start\":\"50192\",\"end\":\"50203\"},{\"start\":\"50554\",\"end\":\"50565\"},{\"start\":\"50565\",\"end\":\"50576\"},{\"start\":\"50576\",\"end\":\"50586\"},{\"start\":\"51000\",\"end\":\"51011\"},{\"start\":\"51011\",\"end\":\"51020\"},{\"start\":\"51020\",\"end\":\"51032\"},{\"start\":\"51362\",\"end\":\"51369\"},{\"start\":\"51369\",\"end\":\"51377\"},{\"start\":\"51377\",\"end\":\"51386\"},{\"start\":\"51807\",\"end\":\"51819\"},{\"start\":\"51819\",\"end\":\"51832\"},{\"start\":\"51832\",\"end\":\"51845\"},{\"start\":\"52369\",\"end\":\"52379\"},{\"start\":\"52379\",\"end\":\"52388\"},{\"start\":\"52388\",\"end\":\"52399\"},{\"start\":\"52808\",\"end\":\"52816\"},{\"start\":\"52816\",\"end\":\"52828\"},{\"start\":\"53224\",\"end\":\"53235\"},{\"start\":\"53461\",\"end\":\"53472\"},{\"start\":\"53472\",\"end\":\"53480\"},{\"start\":\"53480\",\"end\":\"53491\"},{\"start\":\"53491\",\"end\":\"53499\"},{\"start\":\"53752\",\"end\":\"53763\"},{\"start\":\"53763\",\"end\":\"53776\"},{\"start\":\"53776\",\"end\":\"53784\"},{\"start\":\"53784\",\"end\":\"53795\"},{\"start\":\"53795\",\"end\":\"53803\"},{\"start\":\"54201\",\"end\":\"54212\"},{\"start\":\"54212\",\"end\":\"54223\"},{\"start\":\"54223\",\"end\":\"54233\"},{\"start\":\"54233\",\"end\":\"54243\"},{\"start\":\"54512\",\"end\":\"54522\"},{\"start\":\"54772\",\"end\":\"54781\"},{\"start\":\"54781\",\"end\":\"54792\"},{\"start\":\"55152\",\"end\":\"55163\"},{\"start\":\"55163\",\"end\":\"55172\"},{\"start\":\"55558\",\"end\":\"55571\"},{\"start\":\"55571\",\"end\":\"55582\"},{\"start\":\"55722\",\"end\":\"55738\"},{\"start\":\"55738\",\"end\":\"55751\"},{\"start\":\"55960\",\"end\":\"55976\"},{\"start\":\"55976\",\"end\":\"55996\"},{\"start\":\"55996\",\"end\":\"56005\"},{\"start\":\"56005\",\"end\":\"56017\"},{\"start\":\"56017\",\"end\":\"56030\"},{\"start\":\"56617\",\"end\":\"56632\"},{\"start\":\"56632\",\"end\":\"56638\"},{\"start\":\"56967\",\"end\":\"56978\"},{\"start\":\"56978\",\"end\":\"56988\"},{\"start\":\"57457\",\"end\":\"57469\"},{\"start\":\"57469\",\"end\":\"57483\"},{\"start\":\"57483\",\"end\":\"57493\"},{\"start\":\"57493\",\"end\":\"57503\"},{\"start\":\"57503\",\"end\":\"57512\"},{\"start\":\"57738\",\"end\":\"57758\"},{\"start\":\"57758\",\"end\":\"57773\"},{\"start\":\"57773\",\"end\":\"57784\"},{\"start\":\"57784\",\"end\":\"57795\"},{\"start\":\"57962\",\"end\":\"57982\"},{\"start\":\"57982\",\"end\":\"57999\"},{\"start\":\"57999\",\"end\":\"58011\"},{\"start\":\"58011\",\"end\":\"58026\"},{\"start\":\"58303\",\"end\":\"58311\"},{\"start\":\"58311\",\"end\":\"58323\"},{\"start\":\"58323\",\"end\":\"58334\"}]", "bib_venue": "[{\"start\":\"46414\",\"end\":\"46436\"},{\"start\":\"46748\",\"end\":\"46788\"},{\"start\":\"47005\",\"end\":\"47034\"},{\"start\":\"47310\",\"end\":\"47372\"},{\"start\":\"47665\",\"end\":\"47710\"},{\"start\":\"47877\",\"end\":\"47945\"},{\"start\":\"48401\",\"end\":\"48411\"},{\"start\":\"48804\",\"end\":\"48853\"},{\"start\":\"49223\",\"end\":\"49245\"},{\"start\":\"49567\",\"end\":\"49615\"},{\"start\":\"49923\",\"end\":\"49965\"},{\"start\":\"50203\",\"end\":\"50219\"},{\"start\":\"50586\",\"end\":\"50674\"},{\"start\":\"51032\",\"end\":\"51066\"},{\"start\":\"51386\",\"end\":\"51425\"},{\"start\":\"51845\",\"end\":\"51946\"},{\"start\":\"52399\",\"end\":\"52451\"},{\"start\":\"52828\",\"end\":\"52877\"},{\"start\":\"53190\",\"end\":\"53222\"},{\"start\":\"53399\",\"end\":\"53459\"},{\"start\":\"53803\",\"end\":\"53855\"},{\"start\":\"54258\",\"end\":\"54299\"},{\"start\":\"54522\",\"end\":\"54566\"},{\"start\":\"54792\",\"end\":\"54883\"},{\"start\":\"55172\",\"end\":\"55246\"},{\"start\":\"55535\",\"end\":\"55556\"},{\"start\":\"55751\",\"end\":\"55788\"},{\"start\":\"56030\",\"end\":\"56146\"},{\"start\":\"56638\",\"end\":\"56694\"},{\"start\":\"56823\",\"end\":\"56859\"},{\"start\":\"56988\",\"end\":\"57036\"},{\"start\":\"57512\",\"end\":\"57526\"},{\"start\":\"57795\",\"end\":\"57809\"},{\"start\":\"58026\",\"end\":\"58062\"},{\"start\":\"58334\",\"end\":\"58367\"},{\"start\":\"47036\",\"end\":\"47059\"},{\"start\":\"47374\",\"end\":\"47433\"},{\"start\":\"47947\",\"end\":\"48000\"},{\"start\":\"49647\",\"end\":\"49689\"},{\"start\":\"50676\",\"end\":\"50749\"},{\"start\":\"51068\",\"end\":\"51113\"},{\"start\":\"51465\",\"end\":\"51483\"},{\"start\":\"51948\",\"end\":\"52034\"},{\"start\":\"52453\",\"end\":\"52499\"},{\"start\":\"55248\",\"end\":\"55322\"},{\"start\":\"56173\",\"end\":\"56283\"},{\"start\":\"57068\",\"end\":\"57129\"}]"}}}, "year": 2023, "month": 12, "day": 17}