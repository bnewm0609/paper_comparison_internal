{"id": 2683207, "updated": "2023-09-30 08:51:04.75", "metadata": {"title": "Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro", "authors": "[{\"first\":\"Zhedong\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Liang\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "2017 IEEE International Conference on Computer Vision (ICCV)", "journal": "2017 IEEE International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2017, "month": 1, "day": 26}, "abstract": "The main contribution of this paper is a simple semi-supervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/Person-reID_GAN.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1701.07717", "mag": "2949257576", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/ZhengZY17", "doi": "10.1109/iccv.2017.405"}}, "content": {"source": {"pdf_hash": "0d03e842ae9512d1a890b7ee8aea1343e491ffc9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1701.07717v5.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://opus.lib.uts.edu.au/bitstream/10453/118067/4/FF67E427-6528-4081-B0B7-C3EB797E0421.pdf", "status": "GREEN"}}, "grobid": {"id": "3beda2923e9ba3f4515272900b672d3ba8b8166b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0d03e842ae9512d1a890b7ee8aea1343e491ffc9.txt", "contents": "\n\n22 Aug 2017\n\nZhedong Zheng zdzheng12@gmail.com \nCentre for Artificial Intelligence\nUniversity of Technology Sydney\n\n\nLiang Zheng liangzheng06@gmail.com \nCentre for Artificial Intelligence\nUniversity of Technology Sydney\n\n\nYi Yang yee.i.yang@gmail.com \nCentre for Artificial Intelligence\nUniversity of Technology Sydney\n\n22 Aug 2017Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro\nThe main contribution of this paper is a simple semisupervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline.We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and  DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46%   improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/Person-reID_GAN .\n\nIntroduction\n\nUnsupervised learning can serve as an important auxiliary task to supervised tasks [14,29,11,28]. In this work, we propose a semi-supervised pipeline that works on the original training set without an additional data collection process. First, the training set is expanded with unlabeled data using a GAN. Then our model minimizes the sum of the supervised and the unsupervised losses through a new * To whom all correspondence should be addressed. Figure 1. The pipeline of the proposed method. There are two components: a generative adversarial model [27] for unsupervised learning and a convolutional neural network for semi-supervised learning. \"Real Data\" represents the labeled data in the given training set; \"Training data\" includes both the \"Real Data\" and the generated unlabeled data. We aim to learn more discriminative embeddings with the \"Training data\". regularization method. This method is evaluated with person re-ID, which aims to spot the target person in different cameras. This has been recently viewed as an image retrieval problem [50]. This paper addresses three challenges. First, current research in GANs typically considers the quality of the sample generation with and without semi-supervised learning in vivo [24,32,27,7,26,41]. Yet a scientific problem remains unknown: moving the generated samples out of the box and using them in currently available learning frameworks. To this end, this work uses unlabeled data produced by the DCGAN model [27] in conjunction with the labeled training data. As shown in Fig. 1, our pipeline feeds the newly generated samples into another learning machine (i.e. a CNN). Therefore, we use the term \"in vitro\" to differentiate our method from [24,32,27,7]; these methods perform semi-supervised learning in the discriminator of the GANs (in vivo).\n\nSecond, the challenge of performing semi-supervised learning using labeled and unlabeled data in CNN-based methods remains. Usually, the unsupervised data is used as a pre-training step before supervised learning [28,11,14]. Our method uses all the data simultaneously. In [25,18,24,32], the unlabeled/weak-labeled real data are assigned labels according to pre-defined training classes, but our method assumes that the GAN generated data does not belong to any of the existing classes. The proposed LSRO method neither includes unsupervised pre-training nor label assignments for the known classes. We address semisupervised learning from a new perspective. Since the unlabeled samples do not belong to any of the existing classes, they are assigned a uniform label distribution over the training classes. The network is trained not to predict a particular class for the generated data with high confidence.\n\nThird, in person re-ID, data annotation is expensive, because one has to draw a pedestrian bounding box and assign an ID label to it. Recent progress in this field can be attributed to two factors: 1) the availability of large-scale re-ID datasets [49,51,44,19] and 2) the learned embedding of pedestrians using a CNN [8,10]. That being said, the number of images for each identity is still limited, as shown in Fig. 2. There are 17.2 images per identities in Market-1501 [49], 9.6 images in CUHK03 [19], and 23.5 images in DukeMTMC-reID [30] on average. So using additional data is non-trivial to avoid model overfitting. In the literature, pedestrian images used in training are usually provided by the training sets, without being expanded. So it is unknown if a larger training set with unlabeled images would bring any extra benefit. This observation inspired us to resort to the GAN samples to enlarge and enrich the training set. It also motivated us to employ the proposed regularization to implement a semi-supervised system.\n\nIn an attempt to overcome the above-mentioned challenges, this paper 1) adopts GAN in unlabeled data generation, 2) proposes the label smoothing regularization for outliers (LSRO) for unlabeled data integration, and 3) reports improvements over a CNN baseline on three person re-ID datasets. In more details, in the first step, we train DCGAN [27] on the original re-ID training set. We generate new pedestrian images by inputting 100-dim random vectors in which each entry falls within [-1, 1]. Some generated samples are shown in Fig. 3 and Fig. 5. In the second step, these unlabeled GAN-generated data are fed into the ResNet model [13]. The LSRO method regularizes the learning process by integrating the unlabeled data and, thus, reduces the risk of over-fitting. Finally, we evaluate the proposed method on person re-ID and show that the learned embeddings demonstrate a consistent improvement over the strong ResNet baseline.\n\nTo summarize, our contributions are:\n\n\u2022 the introduction of a semi-supervised pipeline that integrates GAN-generated images into the CNN learning machine in vitro;\n\n\u2022 an LSRO method for semi-supervised learning. The integration of unlabeled data regularizes the CNN learning process. We show that the LSRO method is Figure 2. The image distribution per class in the dataset Market-1501 [49], CUHK03 [19] and DukeMTMC-reID [30]. We observe that all these datasets suffer from the limited images per class. Note that there are only a few classes with more than 20 images.\n\nsuperior to the two available strategies for dealing with unlabeled data; and\n\n\u2022 a demonstration that the proposed semi-supervised pipeline has a consistent improvement over the ResNet baseline on three person re-ID datasets and one finegrained recognition dataset.\n\n\nRelated Work\n\nIn this section, we will discuss the relevant works on GANs, semi-supervised learning and person re-ID.\n\n\nGenerative Adversarial Networks\n\nThe generative adversarial networks (GANs) learn two sub-networks: a generator and a discriminator. The discriminator reveals whether a sample is generated or real, while the generator produces samples to cheat the discriminator. The GANs are first proposed by Goodfellow et al. [12] to generate images and gain insights into neural networks. Then, DCGANs [27] provides some techniques to improve the stability of training. The discriminator of DC-GAN can serve as a robust feature extractor. Salimans et al. [32] achieve a state-of-art result in semi-supervised classification and improves the visual quality of GANs. InfoGAN [7] learns interpretable representations by introducing latent codes. On the other hand, GANs also demonstrate potential in generating images for specific fields. Pathak et al. [26] propose an encoder-decoder method for image inpainting, where GANs are used as the image generator. Similarly, Yeh et al. [45] improve the inpainting performance by introducing two loss types. In [41], 3D object images are generated by a 3D-GAN. In this work, we do not focus on investigating more sophisticated sample generation methods. Instead, we use a basic GAN model [27] to generate unlabeled samples from the training data and show that these samples help improve discriminative learning.\n\n\nSemi-supervised Learning\n\nSemi-supervised learning is a sub-class of supervised learning taking unlabeled data into consideration, especially  [27] trained on the Market-1501 training set [49]. (b) The bottom row shows the real samples in training set. Although the generated images in (a) can be easily recognized as fake images by a human, they still serve as an effective regularizer in our experiment.\n\nwhen the volume of annotated data is small. On the one hand, some research treats unsupervised learning as an auxiliary task to supervised learning. For example, in [14], Hinton et al. learn a stack of unsupervised restricted Boltzmann machines to pre-train the model. Ranzato et al. propose to reconstruct the input at every level of a network to get a compact representation [28]. In [29], the auxiliary task of ladder networks is to denoise representations at every level of the model. On the other hand, several works assign labels to the unlabeled data. Papandreou et al. [25] combine strong and weak labels in CNNs using an expectation-maximization (EM) process for image segmentation. In [18], Lee assigns a \"pseudo label\" to the unlabeled data in the class that has the maximum predicted probability. In [24,32], the samples produced by the generator of the GAN are all taken as one class in the discriminator. Departing from previous semi-supervised works, we adopt a different regularization approach by assigning a uniform label distribution to the generated samples.\n\n\nPerson Re-identification\n\nSome pioneering works focus on finding discriminative handcrafted features [22,23,20]. Recent progress in person re-ID mainly consists of advancing CNNs. Yi et al. [46] split a pedestrian image into three horizontal parts and respectively train three part-CNNs to extract features. Similarly, Cheng et al. [8] split the convolutional map into four parts and fuse the part features with the global feature. In [19], Li et al. add a new layer that multiplies the activation of two images in different horizontal stripes. They use this layer to explicitly allow patch matching in the CNN. Later, Ahmed et al. [4] improve the performance by proposing a new patch matching layer that compares the activation of two images in neighboring pixels. In addition, Varior et al. [35] combine the CNN with some gate functions, aiming to adaptively focus on the salient parts of input image pairs, this method is limited by computational inefficiency because the input should be image pairs.\n\nA CNN can be very discriminative by itself without explicit part-matching. Zheng et al. [50,51] directly use a conventional fine-tuning approach (called the IDdiscriminative embedding, or IDE) on the Market-1501 dataset [49] and its performance exceeds many other recent results. Wu et al. [43] combine the CNN embedding with hand-crafted features. In [52], Zheng et al. combine an identification model with a verification model and improve the fine-tuned CNN performance. In this work, we adopt the IDE model [50,51] as a baseline, and show that the GAN samples and LSRO effectively improve its performance. Recently, Barbosa et al. [5] propose synthesizing human images through a photorealistic body generation software. These images are used to pre-train an IDE model before dataset-specific fine-tuning. Our method is different from [5] in both data generation and the training strategy.\n\n\nNetwork Overview\n\nIn this section, we describe the pipeline of the proposed method. As shown in Fig. 1, the real data in the training set is used to train the GAN model. Then, the real training data and the newly generated samples are combined into training input for the CNN. In the following section, we will illustrate the structure of the two components, i.e., the GAN and the CNN, in detail. Note that, our system does not make major changes to the network structures of the GAN or the CNN with one exception -the number of neurons in the last fully-connected layer in the CNN is modified according to the number of training classes.\n\n\nGenerative Adversarial Network\n\nGenerative adversarial networks have two components: a generator and a discriminator. For the generator, we follow the settings in [27]. We start with a 100-dim random vector and enlarge it to 4 \u00d7 4 \u00d7 16 using a linear function. To enlarge the tensor, five deconvolution functions are used with a kernel size of 5 \u00d7 5 and a stride of 2. Every deconvolution is followed by a rectified linear unit and batch normalization. Additionally, one optional deconvolutional layer with a kernel size of 5 \u00d7 5 and a stride of 1, and one tanh function are added to fine-tune the result. A sample that is 128 \u00d7 128 \u00d7 3 in size can then be generated.\n\nThe input of the discriminator network includes the generated images and the real images in the training set. We use five convolutional layers to classify whether the generated image is fake. Similarly, the size of the convolutional filters is 5 \u00d7 5 and their stride is 2. We add a fully-connected layer to perform the binary classification (real or fake).\n\n\nConvolutional Neural Network\n\nThe ResNet-50 [13] model is used in our experiment. We resize the generated images to 256 \u00d7 256 \u00d7 3 using bilinear sampling. The generated images are mixed with the original training set as the input of the CNN. That is, the labeled and unlabeled data are simultaneously trained. These training images are shuffled. Following the conventional fine-tuning strategy [50], we use a model pre-trained on ImageNet [31]. We modify the last fully-connected layer to have K neurons to predict the K-classes, where K is the number of the classes in the original training set (as well as the merged new training set). Unlike [24,32], we do not view the new samples as an extra class but assign a uniform label distribution over the existing classes. So the last fully-connected layer remains K-dimensional. The assigned label distribution of the generated images is discussed in the next section.\n\n\nThe Proposed Regularization Method\n\nIn this section, we first revisit the label smoothing regularization (LSR), which is used for fully-supervised learning. We then extend LSR to the scenario of unlabeled learning, yielding the proposed label smoothing regularization for outliers (LSRO) method.\n\n\nLabel Smoothing Regularization Revisit\n\nLSR was proposed in the 1980s and recently rediscovered by Szegedy et al. [33]. In a nutshell, LSR assigns small values to the non-ground truth classes instead of 0. This strategy discourages the network to be tuned towards the ground truth class and thus reduces the chances of over-fitting. LSR is proposed for use with the cross-entropy loss [33].\n\nFormally, let k \u2208 {1, 2, ..., K} be the pre-defined classes of the training data, where K is the number of classes. The cross-entropy loss can be formulated as:\nl = \u2212 K k=1 log (p(k))q(k),(1)\nwhere p(k) \u2208 [0, 1] is the predicted probability of the input belonging to class k, and can be outputted by CNN. It is derived from the softmax function which normalizes the output of the previous fully-connected layer. q(k) is the ground truth distribution. Let y be the ground truth class label, q(k) can be defined as:\nq(k) = 0 k = y 1 k = y .(2)\nIf we discard the 0 terms in Eq. 1, the cross-entropy loss is equivalent to only considering the ground truth term in Eq. 3. l = \u2212 log (p(y)). Figure 4. The label distributions of a real image and a GANgenerated image in our system. We use a classical label distribution (Eq. 2) for the real image (left). For the generated image (right), we employ the proposed LSRO label distribution (Eq. 6), e.g. a uniform distribution on every training class because the generated image is assumed to belong to none of the training classes. We employ a cross-entropy loss that combines the two types of label distributions as the optimization objective (Eq. 7).\n\nSo, minimizing the cross-entropy loss is equivalent to maximizing the predicted probability of the ground-truth class.\n\nIn [33], the label smoothing regularization (LSR) is introduced to take the distribution of the non-ground truth classes into account. The network is thus encouraged not to be too confident towards the ground truth. In [33], the label distribution q LSR (k) is written as:\nq LSR (k) = \u03b5 K k = y 1 \u2212 \u03b5 + \u03b5 K k = y ,(4)\nwhere \u03b5 \u2208 [0, 1] is a hyperparameter. If \u03b5 is zero, Eq. 4 reduces to Eq. 2. If \u03b5 is too large, the model may fail to predict the ground truth label. So in most cases, \u03b5 is set to 0.1. Szegedy et al. assume that the non-ground truth classes take on a uniform label distribution. Considering Eq. 1 and Eq. 4, the cross-entropy loss evolves to:\nl LSR = \u2212(1 \u2212 \u03b5) log (p(y)) \u2212 \u03b5 K K k=1 log (p(k)).(5)\nCompared with Eq. 3, Eq. 5 pays additional attention to the other classes, rather than only the ground truth class. In this paper, we do not employ LSR on the IDE baseline because it yields a slightly lower performance than using Eq. 2 (see Section 5.3). We re-introduce LSR because it inspires us in designing the LSRO method.\n\n\nLabel Smoothing Regularization for Outliers\n\nThe label smoothing regularization for outliers (LSRO) is used to incorporate the unlabeled images in the network. This extends LSR from the supervised domain to leverage unsupervised data generated by the GAN.\n\nIn LSRO, we propose a virtual label distribution for the unlabeled images. We set the virtual label distribution to be uniform over all classes, due to two inspirations. 1) We assume that the generated samples do not belong to any predefined classes. 2) LSR assumes a uniform distribution over the all classes to address over-fitting. During testing, we expect that the maximum class probability of a generated image will be low, i.e., the network will fail to predict a particular class with high confidence. Formally, for a generated image, its class label distribution, q LSRO (k), is defined as:\nq LSRO (k) = 1 K .(6)\nWe call Eq. 6 the label smoothing regularization for outliers (LSRO). The one-hot distribution defined in Eq. 2 will still be used for the loss computation for the real images in the training set. Combining Eq. 2, Eq. 6 and Eq. 1, we can re-write the cross-entropy loss as:\nl LSRO = \u2212(1 \u2212 Z) log (p(y)) \u2212 Z K K k=1\nlog (p(k)). (7) For a real training image, Z = 0. For a generated training image, Z = 1. So our system actually has two types of losses, one for real images and one for generated images.\n\nAdvantage of LSRO. Using LSRO, we can deal with more training images (outliers) that are located near the real training images in the sample space, and introduce more color, lighting and pose variances to regularize the model. For instance, if we only have one green-clothed identity in the training set, the network may be misled into considering that the color green is a discriminative feature, and this limits the discriminative ability of the model. By adding generated training samples, such as an unlabeled green-clothed person, the classifier will be penalized if it makes the wrong prediction towards the labeled green-clothed person. In this manner, we encourage the network to find more underlying causes and to be less prone to over-fitting. We only use the GAN trained on the original training set to produce outlier images. It would be interesting to further evaluate whether real-world unlabeled images are able to achieve a similar effect (see Table 4).\n\nCompeting methods. We compare LSRO with two alternative methods. Details of both methods are available in existing literature [24,32,18]; breif descriptions follow.\n\n\u2022 All in one. Using [24,32], a new class label is created,\n\ni.e., K + 1, and every generated sample is assigned to this class. CNN training follows in Section 5.2.\n\n\u2022 Pseudo label. Using [18], during network training, each incoming GAN-image is passed forward through the current network and is assigned a pseudo label by taking the maximum value of the probability prediction vector (p(k) in Eq. 1). This GAN-image can be thus trained in the network with this pseudo label. During training, the pseudo label is assigned dynamically, so that the same GAN-image may receive different pseudo labels each time it is fed into the network.\n\nIn our experiments, we begin feeding GAN images and assigning them pseudo labels after 20 epochs. We also set a global weight to the softmax loss of 0.1 to the GAN and 1 to the real images.\n\nOur experimental results show that the two methods also work on the GAN images and that LSRO is superior to \"All in one\" and \"Pseudo label\". Explanations are provided in the Section 5.3.\n\n\nExperiment\n\nWe mainly evaluate the proposed method using the Market-1501 [49] dataset, because it is a large scale and has a fixed training/testing split. We also report results on the CUHK03 dataset [19], but due to the computational cost of 20 training/testing splits, we only use the GAN images generated from the Market-1501 dataset. In addition, we evaluate our method on a recently released pedestrian dataset DukeMTMC-reID [30] and a fine-grained recognition dataset CUB-200-2011 [38].\n\n\nPerson Re-id Datasets\n\nMarket-1501 is a large-scale person re-ID dataset collected from six cameras. It contains 19,732 images for testing and 12,936 images for training. The images are automatically detected by the deformable part model (DPM) [9], so misalignment is common, and the dataset is close to realistic settings. There are 751 identities in the training set and 750 identities in the testing set. There are 17.2 images per identity in the training set. We use all the 12,936 detected images from the training set to train the GAN.\n\nCUHK03 contains 14,097 images of 1,467 identities. Each identity is captured by two cameras on the CUHK campus. This dataset contains two image sets. One is annotated by hand-drawn bounding boxes, and the other is produced by the DPM detector [9]. We use the detected set in this paper. There are 9.6 images per identity in the training set. We report the averaged result after training/testing 20 times. We use the single shot setting.\n\nDukeMTMC-reID is a subset of the newly-released multi-target, multi-camera pedestrian tracking dataset [30]. The original dataset contains eight 85-minute highresolution videos from eight different cameras. Handdrawn pedestrian bounding boxes are available. In this work, we use a subset of [30] for image-based re-ID, in the format of the Market-1501 dataset [49]. We crop pedestrian images from the videos every 120 frames, yielding 36,411 total bounding boxes with IDs annotated by [30]. The DukeMTMC-reID dataset for re-ID has 1,812 identities from eight cameras. There are 1,404 identities appearing in more than two cameras and 408 identities (distractor ID) who appear in only one camera. We randomly select 702 IDs as the training set and the remaining 702 IDs as the testing set. In the testing set, we pick one query image for each ID in each camera and put the remaining images in the gallery. As a result, we get 16,522 training images with 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images. The evaluation protocol is available on our website [2]. Some example re-ID results from the DukeMTMC-reID are shown in Fig. 6.\n\n\nImplementation Details\n\nCNN re-ID baseline. We adopt the CNN re-ID baseline used in [50,51]. Specifically, the Matconvnet [37] package is used. During training, We use the ResNet-50 model [13] and modify the fully-connected layer to have 751, 702 and 1,367 neurons for Market-1501, DukeMTMC-reID and CUHK03, respectively. All the images are resized to 256 \u00d7 256 before being randomly cropped into 224 \u00d7 224 with random horizontal flipping. We insert a dropout layer before the final convolutional layer and set the dropout rate to 0.5 for CUHK03 and 0.75 for Market-1501 and DukeMTMC-reID, respectively. We use stochastic gradient descent with momentum 0.9. The learning rate of the convolution layers is set to 0.002 and decay to 0.0002 after 40 epochs and we stop training after the 50th epochs. During testing, we extract the 2,048-dim CNN embedding in the last convolutional layer for an 224 \u00d7 224 input image. The similarity between two images is calculated by a cosine distance for ranking.\n\nGAN training and testing. We use Tensorflow [3] and the DCGAN package [1] to train the GAN model using the provided data in the original training set without preprocessing (e.g., foreground detection). All the images are resized to 128\u00d7128 and randomly flipped before training. We use Adam [15] with the parameters \u03b2 1 = 0.5, \u03b2 2 = 0.99. We stop training after 30 epochs. During GAN testing, we input a 100-dim random vector in GAN, and the value of each entry ranges in [-1, 1]. The outputted image is resized to 256 \u00d7 256 and then used in CNN training (with LSRO). More GAN images are shown in Fig. 5.\n\n\nEvaluation\n\nThe ResNet baseline. Using the training/testing procedure described in Section 5.2, we report the baseline performance of ResNet in Table 1, Table 5 and Table 3. The rank-1 accuracy is 73.69%, 71.5% and 60.28% on Market-1501, CUHK03 and DukeMTMC-reID respectively. Our baseline results are on par with the those reported in [50,52]. Note that the baseline alone exceeds many previous works  [20,36,47].\n\nThe GAN images improve the baseline. As shown in Table 2, when we add 24, 000 GAN images to the CNN training, our method significantly improves the re-ID performance on Market-1501. We observe improvement of +4.37% (from 73.69% to 78.06%) and +4.75% (from 51.48% to 56.23%) in rank-1 accuracy and mAP, respectively. On CUHK03, we observe improvements of +1.6%, +1.2%, +0.8%, and +1.6% in rank-1, 5, 10 accuracy and mAP, respectively. The improvement on CUHK03 is relatively small compared to that of Market-1501, because the DCGAN model is trained on Market-1501 and the generated images share a more similar distribution with Market-1501 than CUHK03. We also observe improvements of +2.46% and +2.14% in rank-1 and mAP, respectively, on the strong ResNet baseline in the DukeMTMC-reID dataset. These results indicate that the unlabeled images generated by the GAN effectively yield improvements over the baseline using the LSRO method.\n\nThe impact of using different numbers of GAN images during training. We evaluate how the number of GAN images affects the re-ID performance. Since unlabeled data is easy to obtain, we expect the model would learn more general knowledge as the number of unlabeled images increases. The results on Market-1501 are shown in Table 2. We note that the number of real training images in Market-1501 is 12,936. Two observations are made.\n\nFirst, the addition of different numbers of GAN images consistently improves the baseline. Adding approximately 3\u00d7GAN images compared to the real training set still has a +2.38% improvement to rank-1 accuracy.\n\nSecond, the peak performance is achieved when 2\u00d7GAN images are added. When too few GAN sample are incorporated into the system, the regularization ability of the LSRO is inadequate. In contrast, when too many GAN samples are present, the learning machine tends to converge towards assigning uniform prediction probabilities to all the training samples, which is not desirable. Therefore, a trade-off is recommended to avoid poor regularization and over-fitting method Single Query\n\nMulti. Query rank-1 mAP rank-1 mAP BoW+kissme [49] 44. 42 Table 2. Comparison of LSRO, \"All in one\", and \"Pseudo label\" under different numbers of GAN-generated images on Market-1501. We show that LSRO is superior to the other two methods whose best performance is highlighted in blue and red, respectively. Rank-1 accuracy (%) and mAP (%) are shown. method rank-1 mAP BoW+kissme [49] 25. 13 12.17 LOMO+XQDA [20] 30.75 17.04 Basel. [50,52] 65. 22 44.99 Basel. + LSRO 67.68 47.13 Table 3. Comparison of the baseline on DukeMTMC-reID. Rank-1 accuracy (%) and mAP (%) are shown.\n\nof uniform label distributions. GAN images vs. real images in training. To further evaluate the proposed method, we replace the GAN images with the real images from CUHK03 which are viewed as unlabeled in training. Since CUHK03 only contains 14,097 images, we randomly select 12,000 for the fair comparison.\n\nExperimental results are shown in Table 4. We compare the results obtained using the 12,000 CUHK03 images and the 12,000 GAN images. We find the real data from CUHK03 also assists in the regularization and improves the  Table 5. Comparison of the state-of-the-art reports on the CUHK03 dataset. We list the fine-tuned ResNet baseline as well.\n\nThe mAP (%) and rank1 (%) precision are presented. * the respective paper is on ArXiv but not published.\n\nperformance. But the model trained with GAN-generated data is sightly better. In fact, although the images generated from DCGAN are visually imperfect (see Fig. 3), they still possess similar regularization ability as the real images.\n\nComparison with the two competing methods. We compare the LSRO method with the \"All in one\" and \"Pseudo label\" methods implied in [24,32] and [18], respectively. The experimental results on Market-1501 are summarized in Table 2.\n\nWe first observe that both strategies yield improvement over the baseline. The \"All in one\" method treats all the unlabeled samples as a new class, which forces the network to make \"careful\" predictions for the existing K classes. The \"Pseudo label\" method gradually labels the new data, and thus introduces more variance to the network.\n\nNevertheless, we find that LSRO exceeds both strategies by approximately +1% \u223c +2%. We speculate the reason is that the \"All in one\" method makes a coarse label estimation, while the \"Pseudo label\" originally assumes that all the unlabeled data belongs to the existing classes [18] which is not true in person re-ID. While these two methods still use the one-hot label distribution, the LSRO method makes a less stronger assumption (label smoothing) towards the labels of the GAN images. These reasons may explain why LSRO has a superior performance. Comparison with the state-of-the-art methods. We compare our method with the state-of-the-art methods on Market-1501 and CUHK03, listed in Table 1 and Table 5, respectively. On Market-1501, we achieve rank-1 accuracy = 78.06%, mAP = 56.23% when using the single query mode, which is the best result compared to the published papers, and the second best among all the available results including ArXiv papers. On CUHK03, we arrive at rank-1 accuracy = 73.1%, mAP = 77.4% which is also very competitive. The previous best result is produced by combining the identification and verification losses [10,52]. We further investigate whether the LSRO could work on this model. We fine-tuned the publicly available model in [52] with LSRO and achieve state-of-the-art results rank-1 accuracy = 83.97%, mAP = 66.07% on Market-1501. On CUHK03, we also observe a state-of-the art performance rank-1 accuracy = 84.6%, mAP = 87.4%. We, therefore, show that the LSRO method is complementary to previous methods due to the regularization of the GAN data.\n\n\nFine-grained Recognition\n\nFine-grained recognition also faces the problem of a lack of training data and annotations. To further test the effectiveness of our method, we provide results on the CUB-200-2011 dataset [38]. This dataset contains 200 bird classes with 29.97 training images per class on average. Bounding boxes are used in both training and testing. We do not use part annotations. In our implementation, the ResNet baseline has a recognition accuracy of 82.6%, which is slightly higher than the 82.3% reported in [21]. This is the baseline method model annotation top-1 Zhang et al. [48] AlexNet 2\u00d7part 76.7 Zhang et al. [48] VGGNet 2\u00d7part 81.6 Liu et al. [21] ResNet-50 attribute 82.9 Wang et al. [39] 3\u00d7VGGNet \u00d7 83.0 Basel. [21] ResNet-50 \u00d7 82.6 Basel.+LSRO ResNet-50 \u00d7 83.2 Basel.+LSRO 2\u00d7ResNet-50 \u00d7 84.4 Table 6. We show the recognition accuracy (%) on CUB-200-2011.\n\nThe proposed method has a 0.6% improvement over the competitive baseline. The two-model ensemble shows a competitive result.\n\nwe will compare our method with. Using the same pipeline in Fig. 1, we train DCGAN on the 5,994 images in the training set, and then we combine the real images with the generated images (see Fig. 5) to train the CNN. During testing, we adopt the standard 10crop testing [17], which uses 256 \u00d7 256 images as input and the averaged prediction as the classification result. As shown in Table 6, the strong baseline outperforms some recent methods, and the proposed method further yields an improvement of +0.6% (from 82.6% to 83.2%). We also combine the two models generated by our method with different initializations to form an ensemble. This leads to a 84.4% recognition accuracy. In [21], Liu et al. report a 85.5% accuracy with a five-model ensemble using parts and a global scene. We do not include this result because extra annotations are used. We focus on the regularization ability of the GAN, but not on producing a state-of-the-art result.\n\n\nConclusion\n\nIn this paper, we propose an \"in vitro\" usage of the GANs for representation learning, i.e., person re-identification. Using a baseline DCGAN model [27], we show that the imperfect GAN images effectively demonstrate their regularization ability when trained with a ResNet baseline model. Through the proposed LSRO method, we mix the unlabeled GAN images with the labeled real training images for simultaneous semi-supervised learning. Albeit simple, we demonstrate consistent performance improvement over the re-ID and fine-grained recognition baseline systems, which sheds light on the practical use of GAN-generated data.\n\nIn the future, we will continue to investigate on whether GAN images of better visual quality yield superior results when integrated into supervised learning. This paper provides some baseline evaluations using the imperfect GAN images and the future investigation would be intriguing.\n\nFigure 3 .\n3Examples of GAN images and real images. (a) The top two rows show the pedestrian samples generated by DCGAN\n\nFigure 5 .\n5The newly generated images from a DCGAN model trained on DukeMTMC-reID and CUB-200-2011. Through LSRO, they are added to the training sets of DukeMTMC-reID and CUB-200-2011 to regularize the CNN model.\n\nFigure 6 .\n6Sample retrieval results on DukeMTMC-reID using the proposed method. The images in the first column are the query images. The retrieved images are sorted according to the similarity scores from left to right. The correct matches are in the blue rectangles, and the false matching images are in the red rectangles. DukeMTMC-reID is challenging because it contains pedestrians with occlusions and similar appearance.\n\n\nTable 1. Comparison of the state-of-the-art methods reported on the Market-1501 dataset. We also provide results of the fine-tuned ResNet baseline. Rank-1 precision (%) and mAP (%) are listed. * the respective paper is on ArXiv but not published.20.76 \n-\n-\nMR CNN [34] \n45.58 26.11 56.59 32.26 \nFisherNet [42] \n48.15 29.94 \n-\n-\nSL [6] \n51.90 26.35 \n-\n-\nS-LSTM [36] \n-\n-\n61.6 \n35.3 \nDNS [47] \n55.43 29.87 71.56 46.03 \nGate Reid [35] \n65.88 39.55 76.04 48.45 \nSOMAnet [5]* \n73.87 47.89 81.29 56.98 \nVerif.-Identif. [52]* \n79.51 59.87 85.84 70.33 \nDeepTransfer [10]* \n83.7 \n65.5 \n89.6 \n73.8 \nBasel. [50, 52]* \n73.69 51.48 81.47 63.95 \nBasel. + LSRO \n78.06 56.23 85.12 68.52 \nVerif-Identif. + LSRO 83.97 66.07 88.42 76.10 \n\n# GAN Img. \nLSRO \nAll in one \nPseudo label \nrank-1 mAP rank-1 mAP rank-1 mAP \n0 (basel.) \n73.69 51.48 73.69 51.48 73.69 51.48 \n12,000 \n76.81 55.32 75.33 52.82 76.07 53.56 \n18,000 \n77.26 55.55 77.20 55.04 76.34 53.45 \n24,000 \n78.06 56.23 76.63 55.12 75.80 53.03 \n30,000 \n77.38 55.48 75.95 55.18 75.21 52.65 \n36,000 \n76.07 54.59 76.87 55.47 74.67 52.38 \n\n\n\n. Dcgan-Tensorflow Package, DCGAN-tensorflow package. https://github.com/carpedm20/DCGAN-tensorflow.\n\n. Dukemtmc-Reid Dataset, DukeMTMC-reID Dataset. https://github.com/layumi/DukeMTMC-reID_evaluation.\n\nTensorflow: A system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, OSDI. M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensorflow: A system for large-scale machine learning. In OSDI, 2016.\n\nAn improved deep learning architecture for person re-identification. E Ahmed, M Jones, T K Marks, CVPR. E. Ahmed, M. Jones, and T. K. Marks. An improved deep learning architecture for person re-identification. In CVPR, 2015.\n\nI B Barbosa, M Cristani, B Caputo, A Rognhaugen, T Theoharis, arXiv:1701.03153Looking beyond appearances: Synthetic training data for deep cnns in re-identification. I. B. Barbosa, M. Cristani, B. Caputo, A. Rognhaugen, and T. Theo- haris. Looking beyond appearances: Synthetic training data for deep cnns in re-identification. arXiv:1701.03153, 2017.\n\nSimilarity learning with spatial constraints for person re-identification. D Chen, Z Yuan, B Chen, N Zheng, CVPR. D. Chen, Z. Yuan, B. Chen, and N. Zheng. Similarity learning with spatial constraints for person re-identification. In CVPR, 2016.\n\nInfogan: Interpretable representation learning by information maximizing generative adversarial nets. X Chen, Y Duan, R Houthooft, J Schulman, I Sutskever, P Abbeel, NIPS. X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable representation learning by infor- mation maximizing generative adversarial nets. In NIPS, 2016.\n\nPerson reidentification by multi-channel parts-based cnn with improved triplet loss function. D Cheng, Y Gong, S Zhou, J Wang, N Zheng, CVPR. D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng. Person re- identification by multi-channel parts-based cnn with improved triplet loss function. In CVPR, 2016.\n\nObject detection with discriminatively trained part-based models. P F Felzenszwalb, R B Girshick, D Mcallester, D Ramanan, TPAMI. 329P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. TPAMI, 32(9):1627-1645, 2010.\n\nDeep transfer learning for person re-identification. M Geng, Y Wang, T Xiang, Y Tian, arXiv:1611.05244M. Geng, Y. Wang, T. Xiang, and Y. Tian. Deep transfer learning for person re-identification. arXiv:1611.05244, 2016.\n\nMultiprediction deep boltzmann machines. I Goodfellow, M Mirza, A Courville, Y Bengio, NIPS. I. Goodfellow, M. Mirza, A. Courville, and Y. Bengio. Multi- prediction deep boltzmann machines. In NIPS, 2013.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NIPS. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nReducing the dimensionality of data with neural networks. G E Hinton, R R Salakhutdinov, Science. 3135786G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504-507, 2006.\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, arXiv:1412.6980D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\n\nLarge scale metric learning from equivalence constraints. M K\u00f6stinger, M Hirzer, P Wohlhart, P M Roth, H Bischof, CVPR. M. K\u00f6stinger, M. Hirzer, P. Wohlhart, P. M. Roth, and H. Bischof. Large scale metric learning from equivalence constraints. In CVPR, 2012.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classifica- tion with deep convolutional neural networks. In NIPS, 2012.\n\nPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. D.-H Lee, ICML Workshop. D.-H. Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In ICML Workshop, 2013.\n\nDeepreid: Deep filter pairing neural network for person re-identification. W Li, R Zhao, T Xiao, X Wang, CVPR. W. Li, R. Zhao, T. Xiao, and X. Wang. Deepreid: Deep filter pairing neural network for person re-identification. In CVPR, 2014.\n\nPerson re-identification by local maximal occurrence representation and metric learning. S Liao, Y Hu, X Zhu, S Z Li, CVPR. S. Liao, Y. Hu, X. Zhu, and S. Z. Li. Person re-identification by local maximal occurrence representation and metric learning. In CVPR, 2015.\n\nLocalizing by describing: Attribute-guided attention localization for fine-grained recognition. X Liu, J Wang, S Wen, E Ding, Y Lin, arXiv:1605.06217X. Liu, J. Wang, S. Wen, E. Ding, and Y. Lin. Localizing by describ- ing: Attribute-guided attention localization for fine-grained recogni- tion. arXiv:1605.06217, 2016.\n\nBicov: a novel image representation for person re-identification and face verification. B Ma, Y Su, F Jurie, BMVC. B. Ma, Y. Su, and F. Jurie. Bicov: a novel image representation for person re-identification and face verification. In BMVC, 2012.\n\nCovariance descriptor based on bioinspired features for person re-identification and face verification. B Ma, Y Su, F Jurie, Image and Vision Computing. 326B. Ma, Y. Su, and F. Jurie. Covariance descriptor based on bio- inspired features for person re-identification and face verification. Image and Vision Computing, 32(6):379-390, 2014.\n\nSemi-supervised learning with generative adversarial networks. A Odena, arXiv:1606.01583A. Odena. Semi-supervised learning with generative adversarial net- works. arXiv:1606.01583, 2016.\n\nWeaklyand semi-supervised learning of a deep convolutional network for semantic image segmentation. G Papandreou, L.-C Chen, K P Murphy, A L Yuille, ICCV. G. Papandreou, L.-C. Chen, K. P. Murphy, and A. L. Yuille. Weakly- and semi-supervised learning of a deep convolutional network for semantic image segmentation. In ICCV, 2015.\n\nContext encoders: Feature learning by inpainting. D Pathak, P Krahenbuhl, J Donahue, T Darrell, A A Efros, CVPR. D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. ICLR. A Radford, L Metz, S Chintala, A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. ICLR, 2016.\n\nSemi-supervised learning of compact document representations with deep networks. M Ranzato, M Szummer, ICML. M. Ranzato and M. Szummer. Semi-supervised learning of compact document representations with deep networks. In ICML, 2008.\n\nSemi-supervised learning with ladder networks. A Rasmus, M Berglund, M Honkala, H Valpola, T Raiko, NIPS. A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko. Semi-supervised learning with ladder networks. In NIPS, 2015.\n\nPerformance measures and a data set for multi-target, multi-camera tracking. E Ristani, F Solera, R Zou, R Cucchiara, C Tomasi, ECCV Workshop. E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi. Perfor- mance measures and a data set for multi-target, multi-camera track- ing. In ECCV Workshop, 2016.\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, IJCV. 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3):211-252, 2015.\n\nImproved techniques for training gans. T Salimans, I Goodfellow, W Zaremba, V Cheung, A Radford, X Chen, NIPS. T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. In NIPS, 2016.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, CVPR. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Re- thinking the inception architecture for computer vision. In CVPR, 2016.\n\nMultiregion bilinear convolutional neural networks for person re-identification. E Ustinova, Y Ganin, V Lempitsky, arXiv:1512.05300E. Ustinova, Y. Ganin, and V. Lempitsky. Multiregion bi- linear convolutional neural networks for person re-identification. arXiv:1512.05300, 2015.\n\nGated siamese convolutional neural network architecture for human re-identification. R R Varior, M Haloi, G Wang, ECCV. R. R. Varior, M. Haloi, and G. Wang. Gated siamese convolutional neural network architecture for human re-identification. In ECCV, 2016.\n\nA siamese long short-term memory architecture for human re-identification. R R Varior, B Shuai, J Lu, D Xu, G Wang, ECCV. R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang. A siamese long short-term memory architecture for human re-identification. In ECCV, 2016.\n\nMatconvnet -convolutional neural networks for matlab. A Vedaldi, K Lenc, ACMMM. A. Vedaldi and K. Lenc. Matconvnet -convolutional neural networks for matlab. In ACMMM, 2015.\n\nC Wah, S Branson, P Welinder, P Perona, S Belongie, The Caltech-UCSD Birds-200-2011 Dataset. Technical reportC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical report, 2011.\n\nMultiple granularity descriptors for fine-grained categorization. D Wang, Z Shen, J Shao, W Zhang, X Xue, Z Zhang, ICCV. D. Wang, Z. Shen, J. Shao, W. Zhang, X. Xue, and Z. Zhang. Multi- ple granularity descriptors for fine-grained categorization. In ICCV, 2015.\n\nJoint learning of single-image and cross-image representations for person reidentification. F Wang, W Zuo, L Lin, D Zhang, L Zhang, CVPR. F. Wang, W. Zuo, L. Lin, D. Zhang, and L. Zhang. Joint learn- ing of single-image and cross-image representations for person re- identification. In CVPR, 2016.\n\nLearning a probabilistic latent space of object shapes via 3d generativeadversarial modeling. J Wu, C Zhang, T Xue, B Freeman, J Tenenbaum, NIPS. J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum. Learn- ing a probabilistic latent space of object shapes via 3d generative- adversarial modeling. In NIPS, 2016.\n\nDeep linear discriminant analysis on fisher networks: A hybrid architecture for person re-identification. L Wu, C Shen, A Van Den, Hengel, Pattern Recognition. L. Wu, C. Shen, and A. van den Hengel. Deep linear discrimi- nant analysis on fisher networks: A hybrid architecture for person re-identification. Pattern Recognition, 2016.\n\nAn enhanced deep feature representation for person re-identification. S Wu, Y.-C Chen, X Li, A.-C Wu, J.-J You, W.-S Zheng, WACV. S. Wu, Y.-C. Chen, X. Li, A.-C. Wu, J.-J. You, and W.-S. Zheng. An enhanced deep feature representation for person re-identification. In WACV, 2016.\n\nEnd-to-end deep learning for person search. T Xiao, S Li, B Wang, L Lin, X Wang, arXiv:1604.01850T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang. End-to-end deep learning for person search. arXiv:1604.01850, 2016.\n\nR Yeh, C Chen, T Y Lim, M Hasegawa-Johnson, M N Do, arXiv:1607.07539Semantic image inpainting with perceptual and contextual losses. R. Yeh, C. Chen, T. Y. Lim, M. Hasegawa-Johnson, and M. N. Do. Semantic image inpainting with perceptual and contextual losses. arXiv:1607.07539, 2016.\n\nDeep metric learning for practical person re-identification. D Yi, Z Lei, S Z Li, arXiv:1407.4979D. Yi, Z. Lei, and S. Z. Li. Deep metric learning for practical person re-identification. arXiv:1407.4979, 2014.\n\nLearning a discriminative null space for person re-identification. L Zhang, T Xiang, S Gong, arXiv:1603.02139L. Zhang, T. Xiang, and S. Gong. Learning a discriminative null space for person re-identification. arXiv:1603.02139, 2016.\n\nPart-based r-cnns for fine-grained category detection. N Zhang, J Donahue, R Girshick, T Darrell, ECCV. N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-based r-cnns for fine-grained category detection. In ECCV, 2014.\n\nScalable person re-identification: A benchmark. L Zheng, L Shen, L Tian, S Wang, J Wang, Q Tian, ICCV. L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian. Scalable person re-identification: A benchmark. In ICCV, 2015.\n\nPerson re-identification: Past, present and future. L Zheng, Y Yang, A G Hauptmann, arXiv:1610.02984L. Zheng, Y. Yang, and A. G. Hauptmann. Person re-identification: Past, present and future. arXiv:1610.02984, 2016.\n\nL Zheng, H Zhang, S Sun, M Chandraker, Q Tian, arXiv:1604.02531Person re-identification in the wild. L. Zheng, H. Zhang, S. Sun, M. Chandraker, and Q. Tian. Person re-identification in the wild. arXiv:1604.02531, 2016.\n\nA discriminatively learned cnn embedding for person re-identification. Z Zheng, L Zheng, Y Yang, arXiv:1611.05666Z. Zheng, L. Zheng, and Y. Yang. A discriminatively learned cnn embedding for person re-identification. arXiv:1611.05666, 2016.\n", "annotations": {"author": "[{\"end\":118,\"start\":15},{\"end\":223,\"start\":119},{\"end\":322,\"start\":224}]", "publisher": null, "author_last_name": "[{\"end\":28,\"start\":23},{\"end\":130,\"start\":125},{\"end\":231,\"start\":227}]", "author_first_name": "[{\"end\":22,\"start\":15},{\"end\":124,\"start\":119},{\"end\":226,\"start\":224}]", "author_affiliation": "[{\"end\":117,\"start\":50},{\"end\":222,\"start\":155},{\"end\":321,\"start\":254}]", "title": null, "venue": null, "abstract": "[{\"end\":1821,\"start\":423}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1924,\"start\":1920},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1927,\"start\":1924},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1930,\"start\":1927},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1933,\"start\":1930},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2394,\"start\":2390},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2896,\"start\":2892},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3080,\"start\":3076},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3083,\"start\":3080},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3086,\"start\":3083},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3088,\"start\":3086},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3091,\"start\":3088},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3094,\"start\":3091},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3316,\"start\":3312},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3550,\"start\":3546},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3553,\"start\":3550},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3556,\"start\":3553},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3558,\"start\":3556},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3869,\"start\":3865},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3872,\"start\":3869},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3875,\"start\":3872},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3929,\"start\":3925},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3932,\"start\":3929},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3935,\"start\":3932},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3938,\"start\":3935},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4814,\"start\":4810},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4817,\"start\":4814},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4820,\"start\":4817},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4823,\"start\":4820},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4883,\"start\":4880},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4886,\"start\":4883},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5038,\"start\":5034},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5065,\"start\":5061},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5104,\"start\":5100},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5945,\"start\":5941},{\"end\":6092,\"start\":6085},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6238,\"start\":6234},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6923,\"start\":6919},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6936,\"start\":6932},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6959,\"start\":6955},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7808,\"start\":7804},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7885,\"start\":7881},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8038,\"start\":8034},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8155,\"start\":8152},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8333,\"start\":8329},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8460,\"start\":8456},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8534,\"start\":8530},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8711,\"start\":8707},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8980,\"start\":8976},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9025,\"start\":9021},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9409,\"start\":9405},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9621,\"start\":9617},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9630,\"start\":9626},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9821,\"start\":9817},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9939,\"start\":9935},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10056,\"start\":10052},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10059,\"start\":10056},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10426,\"start\":10422},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10429,\"start\":10426},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10432,\"start\":10429},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10515,\"start\":10511},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10656,\"start\":10653},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10760,\"start\":10756},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10956,\"start\":10953},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11118,\"start\":11114},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11418,\"start\":11414},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11421,\"start\":11418},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11550,\"start\":11546},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11620,\"start\":11616},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11682,\"start\":11678},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11840,\"start\":11836},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11843,\"start\":11840},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11963,\"start\":11960},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12166,\"start\":12163},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13028,\"start\":13024},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13937,\"start\":13933},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14287,\"start\":14283},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14332,\"start\":14328},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14538,\"start\":14534},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14541,\"start\":14538},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15224,\"start\":15220},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15495,\"start\":15491},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16818,\"start\":16814},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17034,\"start\":17030},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20339,\"start\":20335},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20342,\"start\":20339},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20345,\"start\":20342},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20399,\"start\":20395},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20402,\"start\":20399},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20566,\"start\":20562},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21468,\"start\":21464},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21595,\"start\":21591},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21825,\"start\":21821},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21882,\"start\":21878},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22133,\"start\":22130},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22675,\"start\":22672},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22974,\"start\":22970},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23162,\"start\":23158},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23231,\"start\":23227},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23356,\"start\":23352},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23965,\"start\":23962},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24128,\"start\":24124},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":24131,\"start\":24128},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24166,\"start\":24162},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24232,\"start\":24228},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25085,\"start\":25082},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25111,\"start\":25108},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25332,\"start\":25328},{\"end\":25516,\"start\":25509},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25984,\"start\":25980},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25987,\"start\":25984},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26051,\"start\":26047},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26054,\"start\":26051},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":26057,\"start\":26054},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28173,\"start\":28169},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28180,\"start\":28178},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28507,\"start\":28503},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28514,\"start\":28512},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28535,\"start\":28531},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":28559,\"start\":28555},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":28562,\"start\":28559},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28569,\"start\":28567},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29829,\"start\":29825},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29832,\"start\":29829},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29841,\"start\":29837},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30545,\"start\":30541},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31414,\"start\":31410},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31417,\"start\":31414},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31535,\"start\":31531},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32075,\"start\":32071},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32387,\"start\":32383},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":32457,\"start\":32453},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":32495,\"start\":32491},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32530,\"start\":32526},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32572,\"start\":32568},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32600,\"start\":32596},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33142,\"start\":33138},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33557,\"start\":33553},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33984,\"start\":33980}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34863,\"start\":34743},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35078,\"start\":34864},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35506,\"start\":35079},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36582,\"start\":35507}]", "paragraph": "[{\"end\":3650,\"start\":1837},{\"end\":4560,\"start\":3652},{\"end\":5596,\"start\":4562},{\"end\":6531,\"start\":5598},{\"end\":6569,\"start\":6533},{\"end\":6696,\"start\":6571},{\"end\":7102,\"start\":6698},{\"end\":7181,\"start\":7104},{\"end\":7369,\"start\":7183},{\"end\":7489,\"start\":7386},{\"end\":8830,\"start\":7525},{\"end\":9238,\"start\":8859},{\"end\":10318,\"start\":9240},{\"end\":11324,\"start\":10347},{\"end\":12217,\"start\":11326},{\"end\":12858,\"start\":12238},{\"end\":13528,\"start\":12893},{\"end\":13886,\"start\":13530},{\"end\":14805,\"start\":13919},{\"end\":15103,\"start\":14844},{\"end\":15496,\"start\":15146},{\"end\":15658,\"start\":15498},{\"end\":16011,\"start\":15690},{\"end\":16689,\"start\":16040},{\"end\":16809,\"start\":16691},{\"end\":17083,\"start\":16811},{\"end\":17470,\"start\":17129},{\"end\":17853,\"start\":17526},{\"end\":18111,\"start\":17901},{\"end\":18712,\"start\":18113},{\"end\":19008,\"start\":18735},{\"end\":19236,\"start\":19050},{\"end\":20207,\"start\":19238},{\"end\":20373,\"start\":20209},{\"end\":20433,\"start\":20375},{\"end\":20538,\"start\":20435},{\"end\":21009,\"start\":20540},{\"end\":21200,\"start\":21011},{\"end\":21388,\"start\":21202},{\"end\":21883,\"start\":21403},{\"end\":22427,\"start\":21909},{\"end\":22865,\"start\":22429},{\"end\":24037,\"start\":22867},{\"end\":25036,\"start\":24064},{\"end\":25641,\"start\":25038},{\"end\":26058,\"start\":25656},{\"end\":26996,\"start\":26060},{\"end\":27428,\"start\":26998},{\"end\":27639,\"start\":27430},{\"end\":28121,\"start\":27641},{\"end\":28698,\"start\":28123},{\"end\":29007,\"start\":28700},{\"end\":29351,\"start\":29009},{\"end\":29457,\"start\":29353},{\"end\":29693,\"start\":29459},{\"end\":29923,\"start\":29695},{\"end\":30262,\"start\":29925},{\"end\":31854,\"start\":30264},{\"end\":32740,\"start\":31883},{\"end\":32866,\"start\":32742},{\"end\":33817,\"start\":32868},{\"end\":34455,\"start\":33832},{\"end\":34742,\"start\":34457}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15689,\"start\":15659},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16039,\"start\":16012},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17128,\"start\":17084},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17525,\"start\":17471},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18734,\"start\":18713},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19049,\"start\":19009}]", "table_ref": "[{\"end\":20205,\"start\":20198},{\"end\":25816,\"start\":25788},{\"end\":26116,\"start\":26109},{\"end\":27326,\"start\":27319},{\"end\":28188,\"start\":28181},{\"end\":28609,\"start\":28602},{\"end\":29050,\"start\":29043},{\"end\":29236,\"start\":29229},{\"end\":29922,\"start\":29915},{\"end\":30973,\"start\":30954},{\"end\":32685,\"start\":32678},{\"end\":33258,\"start\":33251}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1835,\"start\":1823},{\"attributes\":{\"n\":\"2.\"},\"end\":7384,\"start\":7372},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7523,\"start\":7492},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8857,\"start\":8833},{\"attributes\":{\"n\":\"2.3.\"},\"end\":10345,\"start\":10321},{\"attributes\":{\"n\":\"3.\"},\"end\":12236,\"start\":12220},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12891,\"start\":12861},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13917,\"start\":13889},{\"attributes\":{\"n\":\"4.\"},\"end\":14842,\"start\":14808},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15144,\"start\":15106},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17899,\"start\":17856},{\"attributes\":{\"n\":\"5.\"},\"end\":21401,\"start\":21391},{\"attributes\":{\"n\":\"5.1.\"},\"end\":21907,\"start\":21886},{\"attributes\":{\"n\":\"5.2.\"},\"end\":24062,\"start\":24040},{\"attributes\":{\"n\":\"5.3.\"},\"end\":25654,\"start\":25644},{\"attributes\":{\"n\":\"5.4.\"},\"end\":31881,\"start\":31857},{\"attributes\":{\"n\":\"6.\"},\"end\":33830,\"start\":33820},{\"end\":34754,\"start\":34744},{\"end\":34875,\"start\":34865},{\"end\":35090,\"start\":35080}]", "table": "[{\"end\":36582,\"start\":35755}]", "figure_caption": "[{\"end\":34863,\"start\":34756},{\"end\":35078,\"start\":34877},{\"end\":35506,\"start\":35092},{\"end\":35755,\"start\":35509}]", "figure_ref": "[{\"end\":2294,\"start\":2286},{\"end\":3382,\"start\":3376},{\"end\":4980,\"start\":4974},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6136,\"start\":6130},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6147,\"start\":6141},{\"end\":6857,\"start\":6849},{\"end\":12322,\"start\":12316},{\"end\":16191,\"start\":16183},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24036,\"start\":24030},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25640,\"start\":25634},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29621,\"start\":29615},{\"end\":32934,\"start\":32928},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33065,\"start\":33059}]", "bib_author_first_name": "[{\"end\":36701,\"start\":36688},{\"end\":36843,\"start\":36842},{\"end\":36852,\"start\":36851},{\"end\":36862,\"start\":36861},{\"end\":36870,\"start\":36869},{\"end\":36878,\"start\":36877},{\"end\":36887,\"start\":36886},{\"end\":36895,\"start\":36894},{\"end\":36904,\"start\":36903},{\"end\":36916,\"start\":36915},{\"end\":36926,\"start\":36925},{\"end\":37190,\"start\":37189},{\"end\":37199,\"start\":37198},{\"end\":37208,\"start\":37207},{\"end\":37210,\"start\":37209},{\"end\":37347,\"start\":37346},{\"end\":37349,\"start\":37348},{\"end\":37360,\"start\":37359},{\"end\":37372,\"start\":37371},{\"end\":37382,\"start\":37381},{\"end\":37396,\"start\":37395},{\"end\":37775,\"start\":37774},{\"end\":37783,\"start\":37782},{\"end\":37791,\"start\":37790},{\"end\":37799,\"start\":37798},{\"end\":38048,\"start\":38047},{\"end\":38056,\"start\":38055},{\"end\":38064,\"start\":38063},{\"end\":38077,\"start\":38076},{\"end\":38089,\"start\":38088},{\"end\":38102,\"start\":38101},{\"end\":38406,\"start\":38405},{\"end\":38415,\"start\":38414},{\"end\":38423,\"start\":38422},{\"end\":38431,\"start\":38430},{\"end\":38439,\"start\":38438},{\"end\":38683,\"start\":38682},{\"end\":38685,\"start\":38684},{\"end\":38701,\"start\":38700},{\"end\":38703,\"start\":38702},{\"end\":38715,\"start\":38714},{\"end\":38729,\"start\":38728},{\"end\":38967,\"start\":38966},{\"end\":38975,\"start\":38974},{\"end\":38983,\"start\":38982},{\"end\":38992,\"start\":38991},{\"end\":39176,\"start\":39175},{\"end\":39190,\"start\":39189},{\"end\":39199,\"start\":39198},{\"end\":39212,\"start\":39211},{\"end\":39370,\"start\":39369},{\"end\":39384,\"start\":39383},{\"end\":39401,\"start\":39400},{\"end\":39410,\"start\":39409},{\"end\":39416,\"start\":39415},{\"end\":39432,\"start\":39431},{\"end\":39441,\"start\":39440},{\"end\":39454,\"start\":39453},{\"end\":39667,\"start\":39666},{\"end\":39673,\"start\":39672},{\"end\":39682,\"start\":39681},{\"end\":39689,\"start\":39688},{\"end\":39859,\"start\":39858},{\"end\":39861,\"start\":39860},{\"end\":39871,\"start\":39870},{\"end\":39873,\"start\":39872},{\"end\":40081,\"start\":40080},{\"end\":40091,\"start\":40090},{\"end\":40259,\"start\":40258},{\"end\":40272,\"start\":40271},{\"end\":40282,\"start\":40281},{\"end\":40294,\"start\":40293},{\"end\":40296,\"start\":40295},{\"end\":40304,\"start\":40303},{\"end\":40526,\"start\":40525},{\"end\":40540,\"start\":40539},{\"end\":40553,\"start\":40552},{\"end\":40555,\"start\":40554},{\"end\":40801,\"start\":40797},{\"end\":41031,\"start\":41030},{\"end\":41037,\"start\":41036},{\"end\":41045,\"start\":41044},{\"end\":41053,\"start\":41052},{\"end\":41285,\"start\":41284},{\"end\":41293,\"start\":41292},{\"end\":41299,\"start\":41298},{\"end\":41306,\"start\":41305},{\"end\":41308,\"start\":41307},{\"end\":41559,\"start\":41558},{\"end\":41566,\"start\":41565},{\"end\":41574,\"start\":41573},{\"end\":41581,\"start\":41580},{\"end\":41589,\"start\":41588},{\"end\":41871,\"start\":41870},{\"end\":41877,\"start\":41876},{\"end\":41883,\"start\":41882},{\"end\":42134,\"start\":42133},{\"end\":42140,\"start\":42139},{\"end\":42146,\"start\":42145},{\"end\":42433,\"start\":42432},{\"end\":42658,\"start\":42657},{\"end\":42675,\"start\":42671},{\"end\":42683,\"start\":42682},{\"end\":42685,\"start\":42684},{\"end\":42695,\"start\":42694},{\"end\":42697,\"start\":42696},{\"end\":42940,\"start\":42939},{\"end\":42950,\"start\":42949},{\"end\":42964,\"start\":42963},{\"end\":42975,\"start\":42974},{\"end\":42986,\"start\":42985},{\"end\":42988,\"start\":42987},{\"end\":43236,\"start\":43235},{\"end\":43247,\"start\":43246},{\"end\":43255,\"start\":43254},{\"end\":43493,\"start\":43492},{\"end\":43504,\"start\":43503},{\"end\":43692,\"start\":43691},{\"end\":43702,\"start\":43701},{\"end\":43714,\"start\":43713},{\"end\":43725,\"start\":43724},{\"end\":43736,\"start\":43735},{\"end\":43953,\"start\":43952},{\"end\":43964,\"start\":43963},{\"end\":43974,\"start\":43973},{\"end\":43981,\"start\":43980},{\"end\":43994,\"start\":43993},{\"end\":44236,\"start\":44235},{\"end\":44251,\"start\":44250},{\"end\":44259,\"start\":44258},{\"end\":44265,\"start\":44264},{\"end\":44275,\"start\":44274},{\"end\":44287,\"start\":44286},{\"end\":44293,\"start\":44292},{\"end\":44302,\"start\":44301},{\"end\":44314,\"start\":44313},{\"end\":44324,\"start\":44323},{\"end\":44584,\"start\":44583},{\"end\":44596,\"start\":44595},{\"end\":44610,\"start\":44609},{\"end\":44621,\"start\":44620},{\"end\":44631,\"start\":44630},{\"end\":44642,\"start\":44641},{\"end\":44846,\"start\":44845},{\"end\":44857,\"start\":44856},{\"end\":44870,\"start\":44869},{\"end\":44879,\"start\":44878},{\"end\":44889,\"start\":44888},{\"end\":45123,\"start\":45122},{\"end\":45135,\"start\":45134},{\"end\":45144,\"start\":45143},{\"end\":45407,\"start\":45406},{\"end\":45409,\"start\":45408},{\"end\":45419,\"start\":45418},{\"end\":45428,\"start\":45427},{\"end\":45655,\"start\":45654},{\"end\":45657,\"start\":45656},{\"end\":45667,\"start\":45666},{\"end\":45676,\"start\":45675},{\"end\":45682,\"start\":45681},{\"end\":45688,\"start\":45687},{\"end\":45898,\"start\":45897},{\"end\":45909,\"start\":45908},{\"end\":46019,\"start\":46018},{\"end\":46026,\"start\":46025},{\"end\":46037,\"start\":46036},{\"end\":46049,\"start\":46048},{\"end\":46059,\"start\":46058},{\"end\":46321,\"start\":46320},{\"end\":46329,\"start\":46328},{\"end\":46337,\"start\":46336},{\"end\":46345,\"start\":46344},{\"end\":46354,\"start\":46353},{\"end\":46361,\"start\":46360},{\"end\":46611,\"start\":46610},{\"end\":46619,\"start\":46618},{\"end\":46626,\"start\":46625},{\"end\":46633,\"start\":46632},{\"end\":46642,\"start\":46641},{\"end\":46912,\"start\":46911},{\"end\":46918,\"start\":46917},{\"end\":46927,\"start\":46926},{\"end\":46934,\"start\":46933},{\"end\":46945,\"start\":46944},{\"end\":47239,\"start\":47238},{\"end\":47245,\"start\":47244},{\"end\":47253,\"start\":47252},{\"end\":47538,\"start\":47537},{\"end\":47547,\"start\":47543},{\"end\":47555,\"start\":47554},{\"end\":47564,\"start\":47560},{\"end\":47573,\"start\":47569},{\"end\":47583,\"start\":47579},{\"end\":47792,\"start\":47791},{\"end\":47800,\"start\":47799},{\"end\":47806,\"start\":47805},{\"end\":47814,\"start\":47813},{\"end\":47821,\"start\":47820},{\"end\":47960,\"start\":47959},{\"end\":47967,\"start\":47966},{\"end\":47975,\"start\":47974},{\"end\":47977,\"start\":47976},{\"end\":47984,\"start\":47983},{\"end\":48004,\"start\":48003},{\"end\":48006,\"start\":48005},{\"end\":48307,\"start\":48306},{\"end\":48313,\"start\":48312},{\"end\":48320,\"start\":48319},{\"end\":48322,\"start\":48321},{\"end\":48524,\"start\":48523},{\"end\":48533,\"start\":48532},{\"end\":48542,\"start\":48541},{\"end\":48746,\"start\":48745},{\"end\":48755,\"start\":48754},{\"end\":48766,\"start\":48765},{\"end\":48778,\"start\":48777},{\"end\":48965,\"start\":48964},{\"end\":48974,\"start\":48973},{\"end\":48982,\"start\":48981},{\"end\":48990,\"start\":48989},{\"end\":48998,\"start\":48997},{\"end\":49006,\"start\":49005},{\"end\":49195,\"start\":49194},{\"end\":49204,\"start\":49203},{\"end\":49212,\"start\":49211},{\"end\":49214,\"start\":49213},{\"end\":49360,\"start\":49359},{\"end\":49369,\"start\":49368},{\"end\":49378,\"start\":49377},{\"end\":49385,\"start\":49384},{\"end\":49399,\"start\":49398},{\"end\":49651,\"start\":49650},{\"end\":49660,\"start\":49659},{\"end\":49669,\"start\":49668}]", "bib_author_last_name": "[{\"end\":36610,\"start\":36586},{\"end\":36709,\"start\":36702},{\"end\":36849,\"start\":36844},{\"end\":36859,\"start\":36853},{\"end\":36867,\"start\":36863},{\"end\":36875,\"start\":36871},{\"end\":36884,\"start\":36879},{\"end\":36892,\"start\":36888},{\"end\":36901,\"start\":36896},{\"end\":36913,\"start\":36905},{\"end\":36923,\"start\":36917},{\"end\":36932,\"start\":36927},{\"end\":37196,\"start\":37191},{\"end\":37205,\"start\":37200},{\"end\":37216,\"start\":37211},{\"end\":37357,\"start\":37350},{\"end\":37369,\"start\":37361},{\"end\":37379,\"start\":37373},{\"end\":37393,\"start\":37383},{\"end\":37406,\"start\":37397},{\"end\":37780,\"start\":37776},{\"end\":37788,\"start\":37784},{\"end\":37796,\"start\":37792},{\"end\":37805,\"start\":37800},{\"end\":38053,\"start\":38049},{\"end\":38061,\"start\":38057},{\"end\":38074,\"start\":38065},{\"end\":38086,\"start\":38078},{\"end\":38099,\"start\":38090},{\"end\":38109,\"start\":38103},{\"end\":38412,\"start\":38407},{\"end\":38420,\"start\":38416},{\"end\":38428,\"start\":38424},{\"end\":38436,\"start\":38432},{\"end\":38445,\"start\":38440},{\"end\":38698,\"start\":38686},{\"end\":38712,\"start\":38704},{\"end\":38726,\"start\":38716},{\"end\":38737,\"start\":38730},{\"end\":38972,\"start\":38968},{\"end\":38980,\"start\":38976},{\"end\":38989,\"start\":38984},{\"end\":38997,\"start\":38993},{\"end\":39187,\"start\":39177},{\"end\":39196,\"start\":39191},{\"end\":39209,\"start\":39200},{\"end\":39219,\"start\":39213},{\"end\":39381,\"start\":39371},{\"end\":39398,\"start\":39385},{\"end\":39407,\"start\":39402},{\"end\":39413,\"start\":39411},{\"end\":39429,\"start\":39417},{\"end\":39438,\"start\":39433},{\"end\":39451,\"start\":39442},{\"end\":39461,\"start\":39455},{\"end\":39670,\"start\":39668},{\"end\":39679,\"start\":39674},{\"end\":39686,\"start\":39683},{\"end\":39693,\"start\":39690},{\"end\":39868,\"start\":39862},{\"end\":39887,\"start\":39874},{\"end\":40088,\"start\":40082},{\"end\":40094,\"start\":40092},{\"end\":40269,\"start\":40260},{\"end\":40279,\"start\":40273},{\"end\":40291,\"start\":40283},{\"end\":40301,\"start\":40297},{\"end\":40312,\"start\":40305},{\"end\":40537,\"start\":40527},{\"end\":40550,\"start\":40541},{\"end\":40562,\"start\":40556},{\"end\":40805,\"start\":40802},{\"end\":41034,\"start\":41032},{\"end\":41042,\"start\":41038},{\"end\":41050,\"start\":41046},{\"end\":41058,\"start\":41054},{\"end\":41290,\"start\":41286},{\"end\":41296,\"start\":41294},{\"end\":41303,\"start\":41300},{\"end\":41311,\"start\":41309},{\"end\":41563,\"start\":41560},{\"end\":41571,\"start\":41567},{\"end\":41578,\"start\":41575},{\"end\":41586,\"start\":41582},{\"end\":41593,\"start\":41590},{\"end\":41874,\"start\":41872},{\"end\":41880,\"start\":41878},{\"end\":41889,\"start\":41884},{\"end\":42137,\"start\":42135},{\"end\":42143,\"start\":42141},{\"end\":42152,\"start\":42147},{\"end\":42439,\"start\":42434},{\"end\":42669,\"start\":42659},{\"end\":42680,\"start\":42676},{\"end\":42692,\"start\":42686},{\"end\":42704,\"start\":42698},{\"end\":42947,\"start\":42941},{\"end\":42961,\"start\":42951},{\"end\":42972,\"start\":42965},{\"end\":42983,\"start\":42976},{\"end\":42994,\"start\":42989},{\"end\":43244,\"start\":43237},{\"end\":43252,\"start\":43248},{\"end\":43264,\"start\":43256},{\"end\":43501,\"start\":43494},{\"end\":43512,\"start\":43505},{\"end\":43699,\"start\":43693},{\"end\":43711,\"start\":43703},{\"end\":43722,\"start\":43715},{\"end\":43733,\"start\":43726},{\"end\":43742,\"start\":43737},{\"end\":43961,\"start\":43954},{\"end\":43971,\"start\":43965},{\"end\":43978,\"start\":43975},{\"end\":43991,\"start\":43982},{\"end\":44001,\"start\":43995},{\"end\":44248,\"start\":44237},{\"end\":44256,\"start\":44252},{\"end\":44262,\"start\":44260},{\"end\":44272,\"start\":44266},{\"end\":44284,\"start\":44276},{\"end\":44290,\"start\":44288},{\"end\":44299,\"start\":44294},{\"end\":44311,\"start\":44303},{\"end\":44321,\"start\":44315},{\"end\":44334,\"start\":44325},{\"end\":44593,\"start\":44585},{\"end\":44607,\"start\":44597},{\"end\":44618,\"start\":44611},{\"end\":44628,\"start\":44622},{\"end\":44639,\"start\":44632},{\"end\":44647,\"start\":44643},{\"end\":44854,\"start\":44847},{\"end\":44867,\"start\":44858},{\"end\":44876,\"start\":44871},{\"end\":44886,\"start\":44880},{\"end\":44895,\"start\":44890},{\"end\":45132,\"start\":45124},{\"end\":45141,\"start\":45136},{\"end\":45154,\"start\":45145},{\"end\":45416,\"start\":45410},{\"end\":45425,\"start\":45420},{\"end\":45433,\"start\":45429},{\"end\":45664,\"start\":45658},{\"end\":45673,\"start\":45668},{\"end\":45679,\"start\":45677},{\"end\":45685,\"start\":45683},{\"end\":45693,\"start\":45689},{\"end\":45906,\"start\":45899},{\"end\":45914,\"start\":45910},{\"end\":46023,\"start\":46020},{\"end\":46034,\"start\":46027},{\"end\":46046,\"start\":46038},{\"end\":46056,\"start\":46050},{\"end\":46068,\"start\":46060},{\"end\":46326,\"start\":46322},{\"end\":46334,\"start\":46330},{\"end\":46342,\"start\":46338},{\"end\":46351,\"start\":46346},{\"end\":46358,\"start\":46355},{\"end\":46367,\"start\":46362},{\"end\":46616,\"start\":46612},{\"end\":46623,\"start\":46620},{\"end\":46630,\"start\":46627},{\"end\":46639,\"start\":46634},{\"end\":46648,\"start\":46643},{\"end\":46915,\"start\":46913},{\"end\":46924,\"start\":46919},{\"end\":46931,\"start\":46928},{\"end\":46942,\"start\":46935},{\"end\":46955,\"start\":46946},{\"end\":47242,\"start\":47240},{\"end\":47250,\"start\":47246},{\"end\":47261,\"start\":47254},{\"end\":47269,\"start\":47263},{\"end\":47541,\"start\":47539},{\"end\":47552,\"start\":47548},{\"end\":47558,\"start\":47556},{\"end\":47567,\"start\":47565},{\"end\":47577,\"start\":47574},{\"end\":47589,\"start\":47584},{\"end\":47797,\"start\":47793},{\"end\":47803,\"start\":47801},{\"end\":47811,\"start\":47807},{\"end\":47818,\"start\":47815},{\"end\":47826,\"start\":47822},{\"end\":47964,\"start\":47961},{\"end\":47972,\"start\":47968},{\"end\":47981,\"start\":47978},{\"end\":48001,\"start\":47985},{\"end\":48009,\"start\":48007},{\"end\":48310,\"start\":48308},{\"end\":48317,\"start\":48314},{\"end\":48325,\"start\":48323},{\"end\":48530,\"start\":48525},{\"end\":48539,\"start\":48534},{\"end\":48547,\"start\":48543},{\"end\":48752,\"start\":48747},{\"end\":48763,\"start\":48756},{\"end\":48775,\"start\":48767},{\"end\":48786,\"start\":48779},{\"end\":48971,\"start\":48966},{\"end\":48979,\"start\":48975},{\"end\":48987,\"start\":48983},{\"end\":48995,\"start\":48991},{\"end\":49003,\"start\":48999},{\"end\":49011,\"start\":49007},{\"end\":49201,\"start\":49196},{\"end\":49209,\"start\":49205},{\"end\":49224,\"start\":49215},{\"end\":49366,\"start\":49361},{\"end\":49375,\"start\":49370},{\"end\":49382,\"start\":49379},{\"end\":49396,\"start\":49386},{\"end\":49404,\"start\":49400},{\"end\":49657,\"start\":49652},{\"end\":49666,\"start\":49661},{\"end\":49674,\"start\":49670}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":36684,\"start\":36584},{\"attributes\":{\"id\":\"b1\"},\"end\":36785,\"start\":36686},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6287870},\"end\":37118,\"start\":36787},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206593071},\"end\":37344,\"start\":37120},{\"attributes\":{\"doi\":\"arXiv:1701.03153\",\"id\":\"b4\"},\"end\":37697,\"start\":37346},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14811466},\"end\":37943,\"start\":37699},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5002792},\"end\":38309,\"start\":37945},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3332134},\"end\":38614,\"start\":38311},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3198903},\"end\":38911,\"start\":38616},{\"attributes\":{\"doi\":\"arXiv:1611.05244\",\"id\":\"b9\"},\"end\":39132,\"start\":38913},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6442575},\"end\":39338,\"start\":39134},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1033682},\"end\":39618,\"start\":39340},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":39798,\"start\":39620},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1658773},\"end\":40034,\"start\":39800},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b14\"},\"end\":40198,\"start\":40036},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":10070153},\"end\":40458,\"start\":40200},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195908774},\"end\":40698,\"start\":40460},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":18507866},\"end\":40953,\"start\":40700},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":938105},\"end\":41193,\"start\":40955},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14124239},\"end\":41460,\"start\":41195},{\"attributes\":{\"doi\":\"arXiv:1605.06217\",\"id\":\"b20\"},\"end\":41780,\"start\":41462},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5653130},\"end\":42027,\"start\":41782},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":84297},\"end\":42367,\"start\":42029},{\"attributes\":{\"doi\":\"arXiv:1606.01583\",\"id\":\"b23\"},\"end\":42555,\"start\":42369},{\"attributes\":{\"id\":\"b24\"},\"end\":42887,\"start\":42557},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2202933},\"end\":43133,\"start\":42889},{\"attributes\":{\"id\":\"b26\"},\"end\":43409,\"start\":43135},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2151537},\"end\":43642,\"start\":43411},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5855183},\"end\":43873,\"start\":43644},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5584770},\"end\":44182,\"start\":43875},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2930547},\"end\":44542,\"start\":44184},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1687220},\"end\":44784,\"start\":44544},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206593880},\"end\":45039,\"start\":44786},{\"attributes\":{\"doi\":\"arXiv:1512.05300\",\"id\":\"b33\"},\"end\":45319,\"start\":45041},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15172651},\"end\":45577,\"start\":45321},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16756781},\"end\":45841,\"start\":45579},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":207224096},\"end\":46016,\"start\":45843},{\"attributes\":{\"id\":\"b37\"},\"end\":46252,\"start\":46018},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":9267207},\"end\":46516,\"start\":46254},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":2304733},\"end\":46815,\"start\":46518},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3248075},\"end\":47130,\"start\":46817},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":6082175},\"end\":47465,\"start\":47132},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1008985},\"end\":47745,\"start\":47467},{\"attributes\":{\"doi\":\"arXiv:1604.01850\",\"id\":\"b43\"},\"end\":47957,\"start\":47747},{\"attributes\":{\"doi\":\"arXiv:1607.07539\",\"id\":\"b44\"},\"end\":48243,\"start\":47959},{\"attributes\":{\"doi\":\"arXiv:1407.4979\",\"id\":\"b45\"},\"end\":48454,\"start\":48245},{\"attributes\":{\"doi\":\"arXiv:1603.02139\",\"id\":\"b46\"},\"end\":48688,\"start\":48456},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":11710343},\"end\":48914,\"start\":48690},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14991802},\"end\":49140,\"start\":48916},{\"attributes\":{\"doi\":\"arXiv:1610.02984\",\"id\":\"b49\"},\"end\":49357,\"start\":49142},{\"attributes\":{\"doi\":\"arXiv:1604.02531\",\"id\":\"b50\"},\"end\":49577,\"start\":49359},{\"attributes\":{\"doi\":\"arXiv:1611.05666\",\"id\":\"b51\"},\"end\":49819,\"start\":49579}]", "bib_title": "[{\"end\":36840,\"start\":36787},{\"end\":37187,\"start\":37120},{\"end\":37772,\"start\":37699},{\"end\":38045,\"start\":37945},{\"end\":38403,\"start\":38311},{\"end\":38680,\"start\":38616},{\"end\":39173,\"start\":39134},{\"end\":39367,\"start\":39340},{\"end\":39664,\"start\":39620},{\"end\":39856,\"start\":39800},{\"end\":40256,\"start\":40200},{\"end\":40523,\"start\":40460},{\"end\":40795,\"start\":40700},{\"end\":41028,\"start\":40955},{\"end\":41282,\"start\":41195},{\"end\":41868,\"start\":41782},{\"end\":42131,\"start\":42029},{\"end\":42655,\"start\":42557},{\"end\":42937,\"start\":42889},{\"end\":43490,\"start\":43411},{\"end\":43689,\"start\":43644},{\"end\":43950,\"start\":43875},{\"end\":44233,\"start\":44184},{\"end\":44581,\"start\":44544},{\"end\":44843,\"start\":44786},{\"end\":45404,\"start\":45321},{\"end\":45652,\"start\":45579},{\"end\":45895,\"start\":45843},{\"end\":46318,\"start\":46254},{\"end\":46608,\"start\":46518},{\"end\":46909,\"start\":46817},{\"end\":47236,\"start\":47132},{\"end\":47535,\"start\":47467},{\"end\":48743,\"start\":48690},{\"end\":48962,\"start\":48916}]", "bib_author": "[{\"end\":36612,\"start\":36586},{\"end\":36711,\"start\":36688},{\"end\":36851,\"start\":36842},{\"end\":36861,\"start\":36851},{\"end\":36869,\"start\":36861},{\"end\":36877,\"start\":36869},{\"end\":36886,\"start\":36877},{\"end\":36894,\"start\":36886},{\"end\":36903,\"start\":36894},{\"end\":36915,\"start\":36903},{\"end\":36925,\"start\":36915},{\"end\":36934,\"start\":36925},{\"end\":37198,\"start\":37189},{\"end\":37207,\"start\":37198},{\"end\":37218,\"start\":37207},{\"end\":37359,\"start\":37346},{\"end\":37371,\"start\":37359},{\"end\":37381,\"start\":37371},{\"end\":37395,\"start\":37381},{\"end\":37408,\"start\":37395},{\"end\":37782,\"start\":37774},{\"end\":37790,\"start\":37782},{\"end\":37798,\"start\":37790},{\"end\":37807,\"start\":37798},{\"end\":38055,\"start\":38047},{\"end\":38063,\"start\":38055},{\"end\":38076,\"start\":38063},{\"end\":38088,\"start\":38076},{\"end\":38101,\"start\":38088},{\"end\":38111,\"start\":38101},{\"end\":38414,\"start\":38405},{\"end\":38422,\"start\":38414},{\"end\":38430,\"start\":38422},{\"end\":38438,\"start\":38430},{\"end\":38447,\"start\":38438},{\"end\":38700,\"start\":38682},{\"end\":38714,\"start\":38700},{\"end\":38728,\"start\":38714},{\"end\":38739,\"start\":38728},{\"end\":38974,\"start\":38966},{\"end\":38982,\"start\":38974},{\"end\":38991,\"start\":38982},{\"end\":38999,\"start\":38991},{\"end\":39189,\"start\":39175},{\"end\":39198,\"start\":39189},{\"end\":39211,\"start\":39198},{\"end\":39221,\"start\":39211},{\"end\":39383,\"start\":39369},{\"end\":39400,\"start\":39383},{\"end\":39409,\"start\":39400},{\"end\":39415,\"start\":39409},{\"end\":39431,\"start\":39415},{\"end\":39440,\"start\":39431},{\"end\":39453,\"start\":39440},{\"end\":39463,\"start\":39453},{\"end\":39672,\"start\":39666},{\"end\":39681,\"start\":39672},{\"end\":39688,\"start\":39681},{\"end\":39695,\"start\":39688},{\"end\":39870,\"start\":39858},{\"end\":39889,\"start\":39870},{\"end\":40090,\"start\":40080},{\"end\":40096,\"start\":40090},{\"end\":40271,\"start\":40258},{\"end\":40281,\"start\":40271},{\"end\":40293,\"start\":40281},{\"end\":40303,\"start\":40293},{\"end\":40314,\"start\":40303},{\"end\":40539,\"start\":40525},{\"end\":40552,\"start\":40539},{\"end\":40564,\"start\":40552},{\"end\":40807,\"start\":40797},{\"end\":41036,\"start\":41030},{\"end\":41044,\"start\":41036},{\"end\":41052,\"start\":41044},{\"end\":41060,\"start\":41052},{\"end\":41292,\"start\":41284},{\"end\":41298,\"start\":41292},{\"end\":41305,\"start\":41298},{\"end\":41313,\"start\":41305},{\"end\":41565,\"start\":41558},{\"end\":41573,\"start\":41565},{\"end\":41580,\"start\":41573},{\"end\":41588,\"start\":41580},{\"end\":41595,\"start\":41588},{\"end\":41876,\"start\":41870},{\"end\":41882,\"start\":41876},{\"end\":41891,\"start\":41882},{\"end\":42139,\"start\":42133},{\"end\":42145,\"start\":42139},{\"end\":42154,\"start\":42145},{\"end\":42441,\"start\":42432},{\"end\":42671,\"start\":42657},{\"end\":42682,\"start\":42671},{\"end\":42694,\"start\":42682},{\"end\":42706,\"start\":42694},{\"end\":42949,\"start\":42939},{\"end\":42963,\"start\":42949},{\"end\":42974,\"start\":42963},{\"end\":42985,\"start\":42974},{\"end\":42996,\"start\":42985},{\"end\":43246,\"start\":43235},{\"end\":43254,\"start\":43246},{\"end\":43266,\"start\":43254},{\"end\":43503,\"start\":43492},{\"end\":43514,\"start\":43503},{\"end\":43701,\"start\":43691},{\"end\":43713,\"start\":43701},{\"end\":43724,\"start\":43713},{\"end\":43735,\"start\":43724},{\"end\":43744,\"start\":43735},{\"end\":43963,\"start\":43952},{\"end\":43973,\"start\":43963},{\"end\":43980,\"start\":43973},{\"end\":43993,\"start\":43980},{\"end\":44003,\"start\":43993},{\"end\":44250,\"start\":44235},{\"end\":44258,\"start\":44250},{\"end\":44264,\"start\":44258},{\"end\":44274,\"start\":44264},{\"end\":44286,\"start\":44274},{\"end\":44292,\"start\":44286},{\"end\":44301,\"start\":44292},{\"end\":44313,\"start\":44301},{\"end\":44323,\"start\":44313},{\"end\":44336,\"start\":44323},{\"end\":44595,\"start\":44583},{\"end\":44609,\"start\":44595},{\"end\":44620,\"start\":44609},{\"end\":44630,\"start\":44620},{\"end\":44641,\"start\":44630},{\"end\":44649,\"start\":44641},{\"end\":44856,\"start\":44845},{\"end\":44869,\"start\":44856},{\"end\":44878,\"start\":44869},{\"end\":44888,\"start\":44878},{\"end\":44897,\"start\":44888},{\"end\":45134,\"start\":45122},{\"end\":45143,\"start\":45134},{\"end\":45156,\"start\":45143},{\"end\":45418,\"start\":45406},{\"end\":45427,\"start\":45418},{\"end\":45435,\"start\":45427},{\"end\":45666,\"start\":45654},{\"end\":45675,\"start\":45666},{\"end\":45681,\"start\":45675},{\"end\":45687,\"start\":45681},{\"end\":45695,\"start\":45687},{\"end\":45908,\"start\":45897},{\"end\":45916,\"start\":45908},{\"end\":46025,\"start\":46018},{\"end\":46036,\"start\":46025},{\"end\":46048,\"start\":46036},{\"end\":46058,\"start\":46048},{\"end\":46070,\"start\":46058},{\"end\":46328,\"start\":46320},{\"end\":46336,\"start\":46328},{\"end\":46344,\"start\":46336},{\"end\":46353,\"start\":46344},{\"end\":46360,\"start\":46353},{\"end\":46369,\"start\":46360},{\"end\":46618,\"start\":46610},{\"end\":46625,\"start\":46618},{\"end\":46632,\"start\":46625},{\"end\":46641,\"start\":46632},{\"end\":46650,\"start\":46641},{\"end\":46917,\"start\":46911},{\"end\":46926,\"start\":46917},{\"end\":46933,\"start\":46926},{\"end\":46944,\"start\":46933},{\"end\":46957,\"start\":46944},{\"end\":47244,\"start\":47238},{\"end\":47252,\"start\":47244},{\"end\":47263,\"start\":47252},{\"end\":47271,\"start\":47263},{\"end\":47543,\"start\":47537},{\"end\":47554,\"start\":47543},{\"end\":47560,\"start\":47554},{\"end\":47569,\"start\":47560},{\"end\":47579,\"start\":47569},{\"end\":47591,\"start\":47579},{\"end\":47799,\"start\":47791},{\"end\":47805,\"start\":47799},{\"end\":47813,\"start\":47805},{\"end\":47820,\"start\":47813},{\"end\":47828,\"start\":47820},{\"end\":47966,\"start\":47959},{\"end\":47974,\"start\":47966},{\"end\":47983,\"start\":47974},{\"end\":48003,\"start\":47983},{\"end\":48011,\"start\":48003},{\"end\":48312,\"start\":48306},{\"end\":48319,\"start\":48312},{\"end\":48327,\"start\":48319},{\"end\":48532,\"start\":48523},{\"end\":48541,\"start\":48532},{\"end\":48549,\"start\":48541},{\"end\":48754,\"start\":48745},{\"end\":48765,\"start\":48754},{\"end\":48777,\"start\":48765},{\"end\":48788,\"start\":48777},{\"end\":48973,\"start\":48964},{\"end\":48981,\"start\":48973},{\"end\":48989,\"start\":48981},{\"end\":48997,\"start\":48989},{\"end\":49005,\"start\":48997},{\"end\":49013,\"start\":49005},{\"end\":49203,\"start\":49194},{\"end\":49211,\"start\":49203},{\"end\":49226,\"start\":49211},{\"end\":49368,\"start\":49359},{\"end\":49377,\"start\":49368},{\"end\":49384,\"start\":49377},{\"end\":49398,\"start\":49384},{\"end\":49406,\"start\":49398},{\"end\":49659,\"start\":49650},{\"end\":49668,\"start\":49659},{\"end\":49676,\"start\":49668}]", "bib_venue": "[{\"end\":36938,\"start\":36934},{\"end\":37222,\"start\":37218},{\"end\":37510,\"start\":37424},{\"end\":37811,\"start\":37807},{\"end\":38115,\"start\":38111},{\"end\":38451,\"start\":38447},{\"end\":38744,\"start\":38739},{\"end\":38964,\"start\":38913},{\"end\":39225,\"start\":39221},{\"end\":39467,\"start\":39463},{\"end\":39699,\"start\":39695},{\"end\":39896,\"start\":39889},{\"end\":40078,\"start\":40036},{\"end\":40318,\"start\":40314},{\"end\":40568,\"start\":40564},{\"end\":40820,\"start\":40807},{\"end\":41064,\"start\":41060},{\"end\":41317,\"start\":41313},{\"end\":41556,\"start\":41462},{\"end\":41895,\"start\":41891},{\"end\":42180,\"start\":42154},{\"end\":42430,\"start\":42369},{\"end\":42710,\"start\":42706},{\"end\":43000,\"start\":42996},{\"end\":43233,\"start\":43135},{\"end\":43518,\"start\":43514},{\"end\":43748,\"start\":43744},{\"end\":44016,\"start\":44003},{\"end\":44340,\"start\":44336},{\"end\":44653,\"start\":44649},{\"end\":44901,\"start\":44897},{\"end\":45120,\"start\":45041},{\"end\":45439,\"start\":45435},{\"end\":45699,\"start\":45695},{\"end\":45921,\"start\":45916},{\"end\":46109,\"start\":46070},{\"end\":46373,\"start\":46369},{\"end\":46654,\"start\":46650},{\"end\":46961,\"start\":46957},{\"end\":47290,\"start\":47271},{\"end\":47595,\"start\":47591},{\"end\":47789,\"start\":47747},{\"end\":48090,\"start\":48027},{\"end\":48304,\"start\":48245},{\"end\":48521,\"start\":48456},{\"end\":48792,\"start\":48788},{\"end\":49017,\"start\":49013},{\"end\":49192,\"start\":49142},{\"end\":49458,\"start\":49422},{\"end\":49648,\"start\":49579}]"}}}, "year": 2023, "month": 12, "day": 17}