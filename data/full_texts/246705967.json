{"id": 246705967, "updated": "2023-10-05 16:50:20.574", "metadata": {"title": "InPars: Data Augmentation for Information Retrieval using Large Language Models", "authors": "[{\"first\":\"Luiz\",\"last\":\"Bonifacio\",\"middle\":[]},{\"first\":\"Hugo\",\"last\":\"Abonizio\",\"middle\":[]},{\"first\":\"Marzieh\",\"last\":\"Fadaee\",\"middle\":[]},{\"first\":\"Rodrigo\",\"last\":\"Nogueira\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The information retrieval community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our unsupervised dataset outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Furthermore, retrievers finetuned on both supervised and our synthetic data achieve better zero-shot transfer than models finetuned only on supervised data. Code, models, and data are available at https://github.com/zetaalphavector/inpars .", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2202.05144", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2202-05144", "doi": null}}, "content": {"source": {"pdf_hash": "4e36db22808c1d677438137b10979a9279fb6c1f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2202.05144v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "af3cad0efdd9d673a4fbb75bc53c4ddb833b6b39", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4e36db22808c1d677438137b10979a9279fb6c1f.txt", "contents": "\nInPars: Data Augmentation for Information Retrieval using Large Language Models\n\n\nLuiz Bonifacio \nUniversity of Campinas University of Waterloo\n\n\nHugo Abonizio \nUniversity of Campinas University of Waterloo\n\n\nMarzieh Fadaee \nUniversity of Campinas University of Waterloo\n\n\nRodrigo Nogueira \nUniversity of Campinas University of Waterloo\n\n\n\u2020 Zeta \nUniversity of Campinas University of Waterloo\n\n\nAlpha Neuralmind \nUniversity of Campinas University of Waterloo\n\n\nInPars: Data Augmentation for Information Retrieval using Large Language Models\nAll authors contributed equally.\nThe information retrieval community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models (Yu et al., 2021; Sharami et al.,  2022). In this work, we harness the fewshot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our unsupervised dataset outperform strong baselines such as BM25 as well as recently proposed selfsupervised dense retrieval methods. Furthermore, retrievers finetuned on both supervised and our synthetic data achieve better zero-shot transfer than models finetuned only on supervised data. Code, models, and data are available at https: /\n\nIntroduction\n\nLanguage models (LMs) such as GPT-3 (Brown et al., 2020), FLAN (Wei et al., 2022), Gopher (Rae et al., 2021), and T0++ (Sanh et al., 2021) have demonstrated impressive performance on many NLP tasks. Additionally, when sufficient supervised information is not available for a task, they have been shown to be effective and at times yield compelling results (Winata et al., 2021;Schick and Sch\u00fctze, 2021b).\n\nDespite the appealing capabilities of large LMs, multi-billion parameter models are rarely used in information retrieval (IR), with p q p q What are the effects of caffeine during pregnancy? Document: We don't know a lot about the effects of caffeine during pregnancy on you and your baby. So it's best to limit the amount you get each day. Question:\n\n\nFew-shot input: 3 examples + Document d\n\nOutput: Question q and prob p q Relevancy P(R=1|d,q)\n\n\nLanguage\n\nModel G\n\n\nTraining pairs\n\nReranker p q Select top K (q,d) pairs w.r.t. p q Figure 1: Illustration of our few-shot method that generates training data for search tasks. We use a language model G to generate a question q (and its probability p q ) from a document d. The top K pairs (q, d) with respect to p q are used as positive examples to train a reranker whose task it to estimate the relevancy of d to q. some notable exceptions (Nogueira et al., 2020;Pradeep et al., 2021;Neelakantan et al., 2022). One reason is the computationally intensive nature of information retrieval tasks. In a typical reranking task, for instance, we compute the relevancy of 1000 candidate documents for one query, which requires 1000 inference passes on a reranking model. This can be prohibitively expensive when using large models. For example, OpenAI offers a search API that allows one to compute query-document relevancy using their models with billions of parameters. As of February 2022, they charge 0.06 USD per 1000 tokens for their largest model. If each candidate document contains 250 tokens, naively using this API for a reranking task would cost approximately 15 USD per query.\n\nDense retrievers (Karpukhin et al., 2020;Khattab and Zaharia, 2020) avoid this expensive reranking step by precomputing vector representations for each document in the collection prior to retrieval. When a query comes in, only its vector representations are computed, and a fast vector search framework can be used to retrieve the nearest document vectors to the vector representation of the query (Johnson et al., 2019). Despite being computationally cheaper at inference time, dense retrievers need one inference pass to compute the vector representation of each document in the collection, which also makes billion-parameter neural models impracticable to be used as dense retrievers. 1 Another challenge in developing neural models for IR is the lack of domain-specific training data. Manually constructing highquality datasets is difficult as it requires queries from real users. While there are a few general-purpose labeled data available (Nguyen et al., 2016;Kwiatkowski et al., 2019), they are not always effective in generalizing to out-of-domain datasets (Thakur et al., 2021). For this goal, zero-shot and few-shot learning models are in particular promising. However, a cost-effective manner of using large LMs in IR tasks is still an open question.\n\nIn this work, we propose a simple yet effective approach towards efficiently using large LMs in retrieval and obtain improvements across several IR datasets. Rather than using large LMs directly in the retrieval process, we harness them to generate labeled data in a few-shot manner. We then finetune retrieval models on this synthetic data and use them to rerank the the search results of a first-stage retrieval system. We summarize our contributions as follows:\n\n\u2022 We propose a method for adapting large LMs to IR tasks that otherwise are infeasible to be used due to their computational demands. \u2022 In an unsupervised setting, our method largely outperforms recently proposed ones. When combined with supervised finetuning, our method achieves state-ofthe-art results in two of the three transfer learning datasets evaluated in this work.\n\n\nRelated Work\n\nData augmentation methods aim at increasing the amount of data to assist the learn-ing process of data-driven models. To improve the performance of neural models in lowresource settings, small-scale LMs have been used to generate synthetic data in various NLP tasks (Fadaee et al., 2017;Kobayashi, 2018). Recent works show that large pretrained LMs are capable of generating data of reasonable quality (Anaby-Tavor et al., 2020;Papanikolaou and Pierleoni, 2020;Yang et al., 2020;Mohapatra et al., 2021;Kumar et al., 2020;Schick and Sch\u00fctze, 2021a;Meng et al., 2022), sometimes leading to better transfer learning than human generated datasets (Liu et al., 2022). In information retrieval, dense retrievers can achieve comparable results to BM25 in some datasets when solely pretrained on documents without annotations (Ram et al., 2021;Izacard et al., 2021;Neelakantan et al., 2022). These methods rely on extracting pairs of segments of texts that are likely relevant to each other which then used as positive pairs to train the retrieval models.\n\nFocusing on improving the transfer learning effectiveness of dense retrievers, Ma et al. (2021) and Wang et al. (2021) use supervised sequence-to-sequence models to augment the training data. They generate questions from texts from different collections and use these synthetic question-text pairs as positive training examples. Our work differs from existing approaches as we rely exclusively on simple prompts to generate questions from large language models with minimal supervision, i.e., using only a few supervised examples. We were mostly inspired by Han et al. (2021), who uses such models to generate synthetic translation pairs in a zero-shot manner, i.e., without using any parallel corpora.\n\n\nOur Method: InPars\n\nIn this section, we describe the proposed method, dubbed InPars (Inquisitive Parrots for Search), for generating synthetic training datasets for IR tasks. Given a document d and a prefix t consisting of N pairs of questions and their relevant documents, i.e., t = {(q * 1 , d * 1 ), ..., (q * N , d * N )}, our method uses a language model G(t, d) to generate a question q that is likely to be relevant to d. The pair (q, d) forms a positive training example that is later used to finetune our retrieval models.\n\nWe generate thousands of these positive training examples using documents randomly sampled from a collection D. The prefix t is always the same regardless of the input document d, i.e., we can potentially generate millions of synthetic training examples using only N manually annotated examples. This characterizes our method as a few-shot learning approach as long as N is small (in our experiments, we use three examples).\n\nAs a last step to create our training dataset, we select the top K pairs with respect to the following (log) probability:\np q = 1 |q| |q| i=1 log p(q i |t, d, q <i ),(1)\nwhere p(q i |t, d, q <i ) is the probability assigned by G when autoregressively generating the ith token of q. We show in Section 6.3 that this filtering step largely improves IR metrics.\n\nDue to its few-shot nature, our method can be used to adapt retrievers to any collection or IR task, which we later empirically confirm on various collections. This is particularly relevant for IR tasks as gathering data to train retrieval models is an expensive process (Yilmaz et al., 2020), with most highquality collections having less than a few hundred queries (Voorhees and Harman, 1999;Voorhees, 2004).\n\nWe do not perform any pretraining to adapt the model to the target corpus, such as proposed by Gao and Callan (2021). Our method does not require any modifications in the loss function, as is done by Izacard et al. (2021);Neelakantan et al. (2022). This makes In-Pars also suitable for non-neural retrieval algorithms.\n\n\nExperimental Setup\n\nIn this section, we describe the datasets used in this work, the procedure to generate questions from the datasets in an few-shot manner, and finally, how we train retrievers on this synthetic data. Nguyen et al. (2016) is a largescale ranking dataset with more than half million anonymized questions sampled from Bing's search query logs. Its passage ranking version is formed by 8.8M passages and approximately 500k training pairs of queries and relevant documents. On average, there is one relevant passage per query, which were manually annotated. The development and test sets contain approximately 6,900 queries each and the test set is kept hidden from the public.\n\n\nDatasets\n\n\nMS MARCO\n\nTREC-DL Craswell et al. (2021) is a dataset that uses the same collection of passages from MS MARCO, but it contains only 54 queries and a higher number of judged documents per query.\n\nRobust04 (Voorhees, 2005) is an retrieval dataset formed by 249 queries. Its corpus consists of 528k documents from the news wire domain. It has on average 1250.6 judged documents per query.\n\nNatural Questions (Kwiatkowski et al., 2019) is an open domain Question Answering dataset created from Wikipedia documents. The questions are real anonymized queries submitted to Google's search engine. In this work, we use the BEIR's version of NQ, which contains 3,452 queries and 1.21 relevant documents per query in its development set. The corpus consists of 2.6M passages from Wikipedia.\n\nTREC-COVID (Roberts et al., 2020) dataset was created during a challenge organized by NIST whose goal was to study information retrieval methods in response to the pandemic. We use the dataset version provided by the BEIR benchmark. The dataset consists of 50 queries, an average of 1,326 judged documents per query, and a corpus of 171k articles from the COVID-19 scientific literature.\n\n\nTraining Data Generation\n\nOur training set comprises triples of a query, a positive, and a negative document. We first describe how pairs of query and positive documents are generated. We randomly sample 100,000 documents from the collection and generate one question per document using GPT-3's Curie as our language model G.\n\nWe prepend the document text with its title, when it is available. Documents with less than 300 characters are discarded and a new one is sampled. We use a temperature of 0, which defaults to greedy decoding. We tried Figure 2: \"Vanilla\" (left) and \"GBQ\" (right) prompts proposed in this work. The GBQ prompt consists of 3 relevant passages and queries randomly sampled from MS MARCO. The query is used as a bad question and we provide a more descriptive good question for the passage. For the fourth example, we replace {document text} with a sampled document for which the language model is asked to generate a good question.\n\nother decoding algorithms such as sampling (with a temperature of 1.0) and top-p sampling (Holtzman et al., 2019) (with p=0.95) but did not notice significant differences in the final results.\n\nWe experiment with two prompt templates for generating questions which are illustrated in Figure 2. The first called \"Vanilla\" (left), uses N = 3 pairs of document and relevant question randomly chosen from the MS MARCO training dataset. The string {document text} is replaced with the document sampled from the collection and the language model generates a question one token at a time until a termination token \\n is chosen or a maximum number of 64 tokens is reached.\n\nThe second prompt template called \"Guided by Bad Questions\" (GBQ) illustrated in Figure 2 (right) is similar to the \"Vanilla\" template, but we encourage the language model to produce more contextual-aware questions than the ones from MS MARCO. For that, we use MS MARCO questions as examples of \"bad\" questions, and manually create more complicated ones as examples of \"good\" questions.\n\nRather than finding the answer in part of the input document, the full context of the document will contribute to the answer. With this prompt, the model generates a \"good\" and a \"bad\" question for each document and we keep only the good ones to form our positive training pairs. Because the GBQ prompt, by design, generates questions that are different from MS MARCO ones, we use the \"Vanilla\" prompt to generate questions that are used in finetuning retrievers evaluated on MS MARCO and TREC-DL datasets.\n\nThere are four GPT-3 models available to the public via the OpenAI API: Ada, Babbage, Curie and Davinci. Their sizes are not explicitly mentioned, but from the figures reported by Neelakantan et al. (2022) and its accompanying blog post, 2 we infer that Ada, Baggage, Curie and Davinci have 300M, 1.2B, 6B and 175B parameters, respectively. We show in Section 6.2 that questions generated by the more expensive Davinci model lead to marginal improvements in IR metrics. There-fore, throughout this paper, we report results using questions generated by the Curie model.\n\nOf the 100,000 questions generated (and their respective input documents), we use the top K=10,000 pairs w.r.t to p q as positive examples for finetuning our models.\n\nOur retrievers are binary classifiers, so we also need to select documents that are not relevant to q to form negative finetuning examples (q, d \u2212 ). We use a simple method that has been shown to be effective for finetuning rerankers (Pradeep et al., 2021). We use BM25 with q as query to retrieve 1000 documents from the collection D. We randomly select one of these as d \u2212 , and the pair (q, d \u2212 ) forms a negative example.\n\n\nRetrieval Methods\n\nWe use a multi-stage retrieval architecture (Matveeva et al., 2006;Wang et al., 2011;Chen et al., 2017;Liu et al., 2017) comprised of initial retrieval with bag-of-words BM25 (ROBERTSON et al., 1995) followed by a neural reranker.\n\nThe collection is indexed using pyserini (Lin et al., 2021) and 1000 candidate documents for each query are retrieved using BM25. Then we rerank the candidate documents using monoT5, which is an adaption of the T5 model (Raffel et al., 2020) to text ranking proposed by Nogueira et al. (2020). We finetune monoT5 base (220M parameters) and 3B with a constant learning rate of 10 \u22123 and an equal number of positive and negative examples in each batch of size 128. We did not conduct experiments with the 11B version due to its computational cost. To simplify our training procedure (and related hyperparameters) as well as to eliminate the need for convergence checks, we simply train for 156 steps (approximately one epoch). Because the number of training steps is small, we observed a difference of up to two points in the final metric depending on the order in which the training examples are shown to the model. Thus, we report the results that are the average of three seeds. Training T5 base and 3B takes a few minutes on a single Google TPU v3-8. We finetune a model per collection using the synthetic questions generated from that collection. In Section 6.1, we show that having in-domain synthetic sets is beneficial. Never-theless, using only MS MARCO as a source for documents also leads to reasonable results.\n\nWe use a maximum of 512 input tokens and two output tokens (one for the target token and another for the end-of-sequence token). In the experiments with the MS MARCO passage dataset, none of the inputs exceed this length limitation. For NQ and TREC-COVID, documents are often truncated.\n\nRobust04 contains long documents that would require large amounts of memory due to the quadratic cost of the Transformer models. Thus, during finetuning and inference, it is not possible to directly feed the entire text at once to our models. To address this issue, we use a slightly modified version of the MaxP technique of Dai and Callan (2019). We first segment each document into passages by applying a sliding window of 10 sentences with a stride of 5. We then obtain a relevance probability for each passage by classifying it independently. We select the highest probability among these passages as the relevance probability of the document; that is, we do not use the original (BM25) retrieval scores. OpenAI Search API: We also experiment with OpenAI's Search API 3 as a reranker of 100 documents retrieved by BM25. The Search API provides an endpoint to perform semantic text search over a set of documents, where we provide a query and the top 100 documents retrieved by BM25, and the result is a reranked list with the respective scores. OpenAI does not disclose what is under the hood of their Search API, whether it was finetuned on IR datasets, the prompt used during inference, or its relation with cpt-text (Neelakantan et al., 2022). Therefore we compare their Search API, which uses large LMs as rerankers, with our method, which uses the same LMs to train rerankers with lower inference cost.\n\nWe did not rerank more documents due to the high cost of the API (approximately 5.85 USD per 1000 query-document pairs of TREC-DL 2020 using Davinci). Nevertheless, we conducted a single experiment reranking 1000 documents per query using Curie and saw an improvement of almost two nDCG@10 points on TREC-DL 2020 in comparison to -   Table 1 would probably be higher.\n- - - - - 0.6490 (11) ColBERT-v2 (Santhanam et al., 2021) 0.3970 - - - - 0.5620 0.7380 (12) GPL (Wang et al., 2021) - - - - - - 0.7400 (13) miniLM reranker \u2020 0.3901 - - - - \u2021 0.5330 \u2021 0.7570(\n\nResults\n\nThe main results are shown in Table 1. We observe that unsupervised models finetuned using our method (rows 7-8), outperform models of equivalent size available in OpenAI's Search API (rows 4-6). For instance, our monoT5 with 3B parameters (row 8) is significantly better than the larger Curie and Davinci models (rows 5 and 6 We believe different factors contribute to these results: 1) Due to the query-document interaction, cross-encoders (rows 7-8) are more effective (and contextual) than the independent query and document encoding of biencoders (rows 3-6). This is inline with re-sults from Thakur et al. (2021) which shows that bi-encoders and cross-encoders perform similarly when trained and evaluated on the same dataset, but cross-encoders show better zero-shot effectiveness. 2) The synthetic questions generated by InPars have closer resemblance to the queries that the retriever will see at inference time than the sentences extracted from texts used by Contriever and cpt-text. 3) The brand-new queries we generate combined with documents that the model otherwise will not use, provide a completely new training set with high diversity. Diversity in the training data has been shown to lead to improvement in performance of neural models (Qu et al., 2021).\n\nThe last row (16) \n\n\nAblation Study and Analysis\n\nIn this section we take a deeper look into the properties of our proposed approach.\n\n\nPrompt Selection and Source Corpus\n\nFirst, we investigate the impact of using different styles of prompts as well as using indomain or ad hoc collections to generate synthetic data. We compare IR metrics obtained with different prompt templates in Table 2. The column \"Input docs\" refers to the collection used to generate the questions. 5 We do not report PARADE (Li et al., 2020) results, that achieves an nDCG@20 of 0.6127 on Ro-bust04, since it was finetuned on Robust04 data and are are only comparing against zero-shot models on this dataset.\n\nMarco indicates that we sampled documents from the MS MARCO passage collection as input for generating questions since models trained on them have shown great generalization capabilities (Pradeep et al., 2020b;Thakur et al., 2021). In contrast, in-domain signifies that questions were generated from documents sampled from the same collections that the model is evaluated on. This approach can be regarded as zero-shot domain adaptation as we do not use any in-domain labeled data to finetune our retrievers. Since the MS MARCO development set and TREC-DL 2020 use the MS MARCO passage collection, this distinction of in-domain and Marco does not apply to these datasets.\n\nColumn \"Prompt\" in Table 2 refers to the type of prompt fed to the language model during the question generation step. Preliminary experiments demonstrate that GBQ leads to better performance than the Vanilla prompt when used in combination with in-domain input documents. A marginal gain was observed when using the GBQ prompt on MS MARCO documents. Therefore, we combined Marco with the Vanilla prompt and in-domain with the GBQ prompt in our experiments.\n\nIt costs 100-300 USD to generate 100,000 questions using the Curie model regardless of the prompt type. Table 3 shows the difference in the predicted query for both prompts. It is notably observed that GBQ questions tend to be more descriptive and specific to the input document while Vanilla questions are more generic and can be answered by a broader set of documents.\n\nBoth MS MARCO and TREC-DL 2020 achieved the highest scores using the Vanilla prompt for both monoT5 220M and 3B models. However, for the other datasets (Ro-bust04, NQ, and TREC-COVID), we obtained the highest scores with the GBQ prompt and using questions generated from their collec-tions. The 3B models benefited the most from the in-domain examples, consistently outperforming the Marco results. For smaller monoT5 220M, there is no clear winner between the types of prompts and input documents.\n\n\nModel Size Impact on IR Metrics\n\nIn Figure 3 we present the effectiveness on the MS MARCO development set of the monoT5-220M reranker trained on synthetic questions generated using different sizes of GPT-3. As we increase the model size, the IR metric keeps increasing, although very slowly. A hypothesis that explains this gain is that larger models can generate more relevant questions to a given passage, instead of broad and more general questions. Using more specific questions avoids confusing the reranker with questions that are relevant to many passages, which can lead to false-negative examples.\n\nThis relation between GPT-3 model sizes and their IR effectiveness is also observed in the OpenAI Search results presented in Table 1. However, in those results, the Davinci model rivals Curie as the best performing, with the latter delivering a higher score on most experiments.\n\n\nFiltering by the Most Likely Questions\n\nAs mentioned in Section 3, we used only the top K = 10, 000 pairs w.r.t to p q to finetune the rerankers. Finetuning on all 100,000 synthetic examples leads to a 4 MRR@10 points decrease on MS MARCO when compared to the top K filtering approach. Thus, to avoid relying on the test sets and keep the method fully unsupervised, we only evaluated this difference on MS MARCO and applied the filtering approach to all other datasets.\n\n\nWas GPT-3 Trained on Supervised IR Data?\n\nThe answer to this question is not disclosed to the public, but we tried to answer it by measuring the number of questions produced by GPT-3 that match those in the MS MARCO dataset. Of the 93,200 unique questions produced by the largest model (Davinci), 5,285 were also found in the 1M questions from MS MARCO, i.e., less than 5.7%. Most of these  questions contain a single entity (e.g., \"what is freedom of the press\", \"what is the fda\", \"who is gary young\") that are likely to be answered by multiple documents in the collection. Also, the Davinci model produced questions from 6,396 documents marked as relevant in either the training or development sets of MS MARCO. Of those synthetic questions, 725 were found in MS MARCO ground-truth, i.e., less than 12%. Similar numbers were found for smaller models. We argue that these low percentages are evidence that the models were not finetuned on MS MARCO, or at least, they did not memorize it.\n\n\nConclusion and Future Work\n\nIn this work, we presented InPars, a method to generate synthetic training data for IR tasks using large LMs in a few-shot manner. This allows one to harness the information learned by large models in a more efficient and effective way.\n\nOur experiments demonstrated that using large LMs to generate synthetic training data is a promising direction for development of neural retrievers. There are, however, many directions not explored in this work that we leave as future work: 1) Finetuning dense retrievers on our synthetic data; 2) Using \"bad questions\" as negative training examples; 3) Scale up our synthetic datasets to millions of examples; 4) More sophisticated methods to select (question, relevant document) pairs.  Document: John West has resigned as non-executive chairman of Dalgety, the food group, and of Bridon, maker of wire rope and engineered products, having just suffered a stroke. Dalgety has appointed Maurice Warren, chief executive, to succeed West, 65, who became chairman of Dalgety only last September. Richard Clothier, who had been due to become chief executive when Warren turns 60 in June, will now do so on April 1. Warren has agreed to remain chairman for the indefinite future and says the appointment will not affect his plans to become non-executive chairman of the South West Electricity Board in June. Bridon yesterday named Derek Edwards, a non-executive director of the company for the past eight years, as chairman, and Brian Clayton as chief executive. Clayton has been responsible for day-to-day executive decisions since David Allday resigned as chief executive in September. John Hogan has been appointed chief operating officer of Lasmo, the independent oil exploration and production company. He replaces Joe Darby who recently became chief executive after Chris Greentree decided to step down. John Hogan has been managing director of Lasmo's North Sea operations for the past four years. At 39, he is generally regarded as one of the industry's younger generation of pragmatic managers who have to weigh more keenly the financial risks and rewards of oil exploration in a climate of persistently low oil prices. Vanilla question: What is the name of the food group and the wire rope maker? GBQ question: Information on John West's resignation as non-executive chairman of Dalgety and Bridon Document: WIDE PRICE fluctuations in many commodity markets this week resulted from relatively modest buying and selling orders in thin, pre-Christmas conditions. The sharpest movement was gold's Dollars 4.85a-troy-ounce drop over Monday and Tuesday, which traders said was initiated by selling from an individual US trade house. As the price slid towards a support level at Dollars 332 an ounce one dealer told the Reuter news agency that 'under normal circumstances the volume traded would not have moved the market so much'. The gold price was steadier yesterday, gaining 55 cents to Dollars 332.85 an ounce, but was still Dollars 4.30 down on the week so far. The platinum price followed a similar pattern, recovering Dollars 1.10 at yesterday's afternoon fixing to reach Dollars 359.60 an ounce, Dollars 3.90 below last Friday's level, while cash silver was six cents down overall at 370.5 cents an ounce. Most base metals markets at the London Metal Exchange presented a mirror image of the movements in the gold market, slipping back a little yesterday following strong gains earlier in the week. Copper was a case in point. Modest Chinese buying and worries about copper workers joining a Polish general strike were less influential than technical factors in the three months delivery position's Pounds 54.50 rise to Pounds 1,489.25 a tonne over the first two days, dealers said. Mr Ted Arnold, analyst at the Merrill Lynch financial services group, pointed to the profound effect commodity funds, managing about Dollars 26bn, were having on traded metals markets.' These funds tend to work primarily on technical analysis,' he said. Vanilla question: What is the price of gold? GBQ question: What is the effect of commodity funds on the price of metals? Table 4: Examples of questions generated from the TREC-COVID dataset using the Curie model with generated good and bad questions.\n\nDocument: [Ethical principles compromised during the COVID-19 pandemic?]. In the late 1970s, the American bioethicists Tom Beauchamp and James Childress described the four ethical principles that should guide a physician's actions in individual patient care. These principles are: (a) respect for autonomy; (b) doing well (beneficence); (c) not harming (non-maleficence); and (d) justice. In many countries, the global outbreak of SARS-CoV-2 has led to overloaded healthcare systems due to large numbers of COVID-19 patients. In order to provide care to this high volume of patients, far-reaching measures are taken that affect everyone. These measures are not taken from an individual patient's perspective but in the interest of public health; nonetheless, they can directly affect the individual patient's interests. This article examines the extent to which Beauchamp and Childress' ethical principles may be compromised during the COVID-19 pandemic. \n\nFigure 3 :\n3MRR@10 on the MS MARCO development set achieved by InPars using monoT5-220M reranker trained on synthetic questions generated by GPT-3 models of different sizes. Figures for cpt-text are from(Neelakantan et al., 2022). Note the log scale for the x-axis.\n\nTable 1 :\n1Main results. Figures marked with a \u2020 and a \u2021 are from Reimers and Gurevych (2020) and Thakur et al. (2021), respectively. Experiments that are too expensive to run are marked with a $ symbol. For example, evaluating Davinci as a reranker on the MS MARCO development set would cost approximately 6.3k USD. indicates the dataset used for finetuning the model. reranking 100 documents (0.5602 vs 0.5422). Thus, with more candidate documents, the figures reported for the other datasets in\n\n\npresents the results of monoT5-3B finetuned on MS MARCO and further finetuned on our synthetic datasets. Starting from the model from row 15, we finetuned them on questions generated from the in-domain collection. 4 This procedure achieves the state-of-the-art on Robust04 and 4 TREC DL and MS MARCO results in rows 14-16 are from models finetuned for 10 epochs on MS MARCO. For the other datasets, the models were finetuned for one epoch on MS MARCO as this leads to better zero-shot effectiveness (Pradeep et al., 2020a).MARCO \n\nTREC-DL 2020 \nRobust04 \nNQ \nTRECC \nInput docs Prompt MRR@10 \nMAP nDCG@10 \nMAP nDCG@20 nDCG@10 nDCG@10 \n\n(1) monoT5-220M Marco \nVanilla \n0.2585 \n0.3599 \n0.5764 \n0.2242 \n0.4017 \n0.3755 \n0.6727 \n(2) monoT5-220M In-domain \nGBQ \n0.2279 \n0.3354 \n0.5451 \n0.2490 \n0.4268 \n0.3354 \n0.6666 \n(3) monoT5-3B \nMarco \nVanilla \n0.2967 0.4334 \n0.6612 0.3397 \n0.5478 \n0.4870 \n0.7606 \n(4) monoT5-3B \nIn-domain \nGBQ \n0.2819 \n0.4100 \n0.6255 \n0.3180 \n0.5181 \n0.5134 \n0.7835 \n\n\n\nTable 2 :\n2Ablation: Few-shot results when using different input documents and prompts to generate questions. model on 10k synthetic examples that was already trained on 530k manually annotated examples led to a slight decrease in effectiveness. We hypothesize that our synthetic examples are different from MS MARCO ones as we sample passages from the MS MARCO corpus to generate questions that do not often appear in the original relevant passage set (e.g., footnotes, texts about advertisements, passages with multiple topics, etc). Thus, any further training on different examples would be harmful when compared to training and evaluating on data of the same distribution.TREC-COVID datasets, outperforming the \nzero-shot transfer achieved by models trained \nsolely on MS MARCO (rows 9-15) 5 . This \nresult is especially noteworthy for TREC-\nCOVID, in which the subject is relatively re-\ncent and unknown by many pretrained LMs. \nThis indicates that the adaptation to the do-\nmain brought by our generation method led \nto large gains compared to the zero-shot ap-\nproaches. \nAs for MS MARCO and TREC DL, further \nfinetuning a \n\nTable 3 :\n3Examples of questions generated from the Robust04 dataset using the Curie model with Vanilla and GBQ prompts. : South Korean President Kim Yong-sam said Saturday it is imperative that Japan and South Korea work to end North Korea's isolation and halt its suspected nuclear arms program. In a joint press conference with Prime Minister Morihiro Hosokawa after a second round of summit talks, Kim said that South Korea and Japan share a common interest in developing economic cooperation in the Asia-Pacific region. Peace and security on the Korean peninsula are essential to stability in the region, but vestiges of the Cold War still linger on the peninsula, Kim said. Kim stated that it is imperative that neighboring countries work together toward making North Korea discard its isolationist policy. He also said it is imperative that North Korea give up its suspected nuclear arms development program and return to the South-North dialogue. Reporting his agreement with Hosokawa on economic cooperation in the East Asian region, Kim noted that both sides agreed the lopsided bilateral trade imbalance must be resolved.The president ends a three-day visit to Japan Saturday afternoon, leaving for Shanghai for talks with Chinese leaders. Kim said it was the insincerity of North Korea that brought about the referral of the nuclear issue to the United Nations Security Council. Measures should be taken in accordance with the North's attitude, the president said, suggesting a flexible stance toward Pyongyang. Hosokawa stressed the need for a firm and unified response of international society to the issue while leaving open a window for dialogue with North Korea. Vanilla question: What is the president's position on North Korea? GBQ question: What is the South Korean president's position on North Korea?Document\n\n\nBad question: What are Beauchamp and Childress' ethical principles? Good question: How does Beauchamp and Childress' ethical principles apply to the COVID-19 pandemic? Document: Covid-19 Confinement and Changes of Adolescent's Dietary Trends in Italy, Spain, Chile, Colombia and Brazil. Confinement due to the COVID-19 pandemic can influence dietary profiles, especially those of adolescents, who are highly susceptible to acquiring bad eating habits. Adolescents' poor dietary habits increase their subsequent risk of degenerative diseases such as obesity, diabetes, cardiovascular pathologies, etc.Our aim was to study nutritional modifications during COVID-19 confinement in adolescents aged 10 to 19 years, compare them with their usual diet and dietary guidelines, and identify variables that may have influenced changes. Data were collected by an anonymous online questionnaire on food intake among 820 adolescents from Spain, Italy, Brazil, Colombia, and Chile. The results show that COVID-19 confinement did influence their dietary habits. In particular, we recorded modified consumption of fried food, sweet food, legumes, vegetables, and fruits. Moreover, gender, family members at home, watching TV during mealtime, country of residence, and maternal education were diversely correlated with adequate nutrition during COVID-19 confinement. Understanding the adolescents' nutrition behavior during COVID-19 lockdown will help public health authorities reshape future policies on their nutritional recommendations, in preparation for future pandemics. Bad question: What is the COVID-19 pandemic? Good question: What is COVID-19 and how does it affect adolescents?\nFor a more detailed cost estimation, check this blog post.\nhttps://openai.com/blog/ introducing-text-and-code-embeddings/\nhttps://beta.openai.com/docs/guides/search\nAcknowledgmentsThis research was partially funded by a grant from Funda\u00e7\u00e3o de Amparo\u00e0 Pesquisa do Estado de S\u00e3o Paulo (FAPESP) 2020/09753-5. We also would like to thank Google Cloud for credits to support this work.Table 5: Examples of questions generated from the Natural Questions dataset using the Curie model with generated good and bad questions.Document: 1993Miami Dolphins season. The season was marked by Don Shula passing George Halas's record for most wins, against the Philadelphia Eagles. Also, during the Week 5 game against Cleveland, quarterback Dan Marino ruptured his Achilles' tendon and was lost for the remainder of the season. Quarterback Scott Mitchell filled in for Marino, and was Player of the Month for October 1993. Mitchell, too, became injured, leaving the then 900e220ac201c2 team in the hands of Doug Pederson and NFL veteran Steve DeBerg. Bad question: What happened to Dan Marino? Good question: What happened to the 1993 Miami Dolphins? Document: Social construction of gender. On Butler's hypothesis, the socially constructed aspect of gender performativity is perhaps most obvious in drag performance, which offers a rudimentary understanding of gender binaries in its emphasis on gender performance. Butler understands drag cannot be regarded as an example of subjective or singular identity, where\u1e97here is a 'one' who is prior to gender, a one who goes to the wardrobe of gender decides with deliberation which gender it will be today.\nMethod: Data from the Belgian CF Registry (year 200020132010) were collected. Inclusions: Bcc infected patients with entries on lung function in at least 1 y before and 3 y after Bcc acquisition. For each case, we included 2 controls, matched for age at the index year (year of first Bcc infect ion), pancreatic status, sex. Cumulative data up to 2 years before index year were compared to values obtained after infection using Rank sum test. Rate of decline in lu ng function was adjusted for baseline lung function, age, sex. RESULTS: Bcc prevalence in CF. INTRODUCTION: Burkholderi a cepacia complex (Bcc) infection is considered to be associated with worsening of CF lung disease. Patient to patient spread has been reported, however mechanisms of ac quisition of Bcc are not well understood. is low in Belgium (\u00a13%). 183 patients were included: 61 cases, 122 controls. 59% were F508del homozygous. Mean age in cases was 20.9 y (SD 10.5) vs 20.3 y (SD 10.3) in controls. Among the Bcc, 54% were unspecified, 31% were B. multivorans. Mean FEV(1) at index year was 65.2% (SD 24.9) in cases vs 73.1 (SD 26.9) in controls (p = 0.07). FEV(1) decline before index year was significantly higher in cases (20131.7%, SD 0.5) compared to controls (20131.0%, SD 0.3) (p = 0.002Document: 67 CF patients with a declining FEV(1): At risk for acquisition of Burkholderia cepacia complex infection?. INTRODUCTION: Burkholderi a cepacia complex (Bcc) infection is considered to be associated with worsening of CF lung disease. Patient to patient spread has been reported, however mechanisms of ac quisition of Bcc are not well understood. Method: Data from the Belgian CF Registry (year 200020132010) were collected. Inclusions: Bcc infected patients with entries on lung function in at least 1 y before and 3 y after Bcc acquisition. For each case, we included 2 controls, matched for age at the index year (year of first Bcc infect ion), pancreatic status, sex. Cumulative data up to 2 years before index year were compared to values obtained after infection using Rank sum test. Rate of decline in lu ng function was adjusted for baseline lung function, age, sex. RESULTS: Bcc prevalence in CF is low in Belgium (\u00a13%). 183 patients were included: 61 cases, 122 controls. 59% were F508del homozygous. Mean age in cases was 20.9 y (SD 10.5) vs 20.3 y (SD 10.3) in controls. Among the Bcc, 54% were unspecified, 31% were B. multivorans. Mean FEV(1) at index year was 65.2% (SD 24.9) in cases vs 73.1 (SD 26.9) in controls (p = 0.07). FEV(1) decline before index year was significantly higher in cases (20131.7%, SD 0.5) compared to controls (20131.0%, SD 0.3) (p = 0.002).\n\nThis study aims to analyze the efficacy and safety of the 5-mm and 10-mm devices. SUBJECTS AND METHODS Patients who received a laparoscopic or hand-assisted laparoscopic operation for a tumor located in the sigmoid colon or rectum since 2006 were abstracted from a prospectively designed database, and findings were analyzed in two groups based on size of the device used during the procedure. The videotapes of the procedures were watched, and operation reports were read to obtain further information on specific intra-and postoperative complications. Demographics, tumor and operation-related information, and postoperative data were compared. RESULTS Among 215 (128 [59.5%] males; median age, 59.500b113.8 years) patients, data obtained from the 5-mm (n=32) and 10-mm (n=183) groups were identical regarding demographics and data related to tumor (localization and stage) and operation (number of harvested lymph nodes, conversion rates, operation time, intraoperative bleeding, transfusion requirement, reoperation rates, complications, 30-day mortality, and length of hospital stay). However, more patients. Bad question: What is the prevalence of Bcc in CF patients? Good question: What is Burkholderia cepacia complex (Bcc) and how does it affect CF patients? Document: Safety and effectiveness of 5-mm and 10-mm electrothermal bipolar vessel sealers (LigaSure) in laparoscopic resections for sigmoid colon and rectal cancers. underwent an anterior resection in the 10-mm group than in the 5-mm group (31.7% versus 15.6%, P\u00a1.05). Further analyses found device-related bleeding in 8 (3.7%) casesFEV(1) slopes were comparable in the period after index year (20131.1%, SD 0.5, in cases vs20130.99%, SD 0.4, p = 0.24). CONCLUSION: Our results suggest that a declining FEV(1) precedes acquisition of Bcc and may be a risk factor. After acquisition, lung function decline was comparable in Bcc infected and uninfected patients. These results should be interpreted with caution, since registry data are collected retrospectively and bear a risk of incompleteness or inaccuracy. Bad question: What is the prevalence of Bcc in CF patients? Good question: What is Burkholderia cepacia complex (Bcc) and how does it affect CF patients? Document: Safety and effectiveness of 5-mm and 10-mm electrothermal bipolar vessel sealers (LigaSure) in laparoscopic resections for sigmoid colon and rectal cancers.. BACKGROUND LigaSure2122 (Covidien, Mansfield, MA) has been used in cases undergoing laparoscopic colon and rectal resections. This study aims to analyze the efficacy and safety of the 5-mm and 10-mm devices. SUBJECTS AND METHODS Patients who received a laparoscopic or hand-assisted laparoscopic operation for a tumor located in the sigmoid colon or rectum since 2006 were abstracted from a prospectively designed database, and findings were analyzed in two groups based on size of the device used during the procedure. The videotapes of the procedures were watched, and operation reports were read to obtain further information on specific intra-and postoperative complications. Demographics, tumor and operation-related information, and postoperative data were com- pared. RESULTS Among 215 (128 [59.5%] males; median age, 59.500b113.8 years) patients, data obtained from the 5-mm (n=32) and 10-mm (n=183) groups were identical regarding demographics and data related to tumor (localization and stage) and operation (number of harvested lymph nodes, conversion rates, operation time, intraoperative bleeding, transfusion requirement, reoperation rates, complications, 30-day mortality, and length of hospital stay). However, more patients underwent an anterior resection in the 10-mm group than in the 5-mm group (31.7% versus 15.6%, P\u00a1.05). Further analyses found device-related bleeding in 8 (3.7%) cases\n\nCONCLUSIONS The 5-mm and 10-mm LigaSure devices are similarly effective and safe during laparoscopic sigmoid colon and rectal resections. the 5-mm versus 10-mm group, respectively, P\u00bf.05). Severe bleeding from larger vessels may be observed, requiring conversion to open surgery or relaparotomy. question\": \"What is the safety and effectiveness of the LigaSure device? Bad question: What is the LigaSure device? Good question: What is the safety and effectiveness of the LigaSure device3%] versus 6 [3.3%] in the 5-mm versus 10-mm group, respectively, P\u00bf.05), requiring further attempts for hemorrhage control (n=6), conversion to open surgery (n=1), or relaparotomy (n=1). CONCLUSIONS The 5-mm and 10-mm LigaSure devices are similarly effective and safe during laparoscopic sigmoid colon and rectal resections. Severe bleeding from larger vessels may be observed, requiring conversion to open surgery or relaparotomy.\", \"question\": \"What is the safety and effectiveness of the LigaSure device? Bad question: What is the LigaSure device? Good question: What is the safety and effectiveness of the LigaSure device?\n", "annotations": {"author": "[{\"end\":146,\"start\":83},{\"end\":209,\"start\":147},{\"end\":273,\"start\":210},{\"end\":339,\"start\":274},{\"end\":395,\"start\":340},{\"end\":461,\"start\":396}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":88},{\"end\":160,\"start\":152},{\"end\":224,\"start\":218},{\"end\":290,\"start\":282},{\"end\":346,\"start\":342},{\"end\":412,\"start\":402}]", "author_first_name": "[{\"end\":87,\"start\":83},{\"end\":151,\"start\":147},{\"end\":217,\"start\":210},{\"end\":281,\"start\":274},{\"end\":341,\"start\":340},{\"end\":401,\"start\":396}]", "author_affiliation": "[{\"end\":145,\"start\":99},{\"end\":208,\"start\":162},{\"end\":272,\"start\":226},{\"end\":338,\"start\":292},{\"end\":394,\"start\":348},{\"end\":460,\"start\":414}]", "title": "[{\"end\":80,\"start\":1},{\"end\":541,\"start\":462}]", "venue": null, "abstract": "[{\"end\":1649,\"start\":575}]", "bib_ref": "[{\"end\":1721,\"start\":1695},{\"end\":1746,\"start\":1723},{\"end\":1773,\"start\":1748},{\"end\":1803,\"start\":1784},{\"end\":2042,\"start\":2021},{\"end\":2068,\"start\":2042},{\"end\":2986,\"start\":2963},{\"end\":3007,\"start\":2986},{\"end\":3032,\"start\":3007},{\"end\":3748,\"start\":3724},{\"end\":3774,\"start\":3748},{\"end\":4127,\"start\":4105},{\"end\":4396,\"start\":4395},{\"end\":4674,\"start\":4653},{\"end\":4699,\"start\":4674},{\"end\":4794,\"start\":4773},{\"end\":6116,\"start\":6095},{\"end\":6132,\"start\":6116},{\"end\":6257,\"start\":6231},{\"end\":6290,\"start\":6257},{\"end\":6308,\"start\":6290},{\"end\":6331,\"start\":6308},{\"end\":6350,\"start\":6331},{\"end\":6376,\"start\":6350},{\"end\":6394,\"start\":6376},{\"end\":6490,\"start\":6472},{\"end\":6665,\"start\":6647},{\"end\":6686,\"start\":6665},{\"end\":6711,\"start\":6686},{\"end\":6973,\"start\":6957},{\"end\":6996,\"start\":6978},{\"end\":7453,\"start\":7436},{\"end\":9194,\"start\":9173},{\"end\":9296,\"start\":9269},{\"end\":9311,\"start\":9296},{\"end\":9536,\"start\":9514},{\"end\":9561,\"start\":9536},{\"end\":9874,\"start\":9854},{\"end\":10771,\"start\":10735},{\"end\":14235,\"start\":14210},{\"end\":15023,\"start\":15001},{\"end\":15281,\"start\":15258},{\"end\":15299,\"start\":15281},{\"end\":15317,\"start\":15299},{\"end\":15334,\"start\":15317},{\"end\":15413,\"start\":15389},{\"end\":15687,\"start\":15666},{\"end\":15738,\"start\":15716},{\"end\":18306,\"start\":18280},{\"end\":20311,\"start\":20294},{\"end\":20789,\"start\":20788},{\"end\":21210,\"start\":21187},{\"end\":21230,\"start\":21210}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30845,\"start\":30579},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31344,\"start\":30846},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32331,\"start\":31345},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33463,\"start\":32332},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":35295,\"start\":33464},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":36971,\"start\":35296}]", "paragraph": "[{\"end\":2069,\"start\":1665},{\"end\":2421,\"start\":2071},{\"end\":2517,\"start\":2465},{\"end\":2537,\"start\":2530},{\"end\":3705,\"start\":2556},{\"end\":4969,\"start\":3707},{\"end\":5435,\"start\":4971},{\"end\":5812,\"start\":5437},{\"end\":6876,\"start\":5829},{\"end\":7580,\"start\":6878},{\"end\":8114,\"start\":7603},{\"end\":8540,\"start\":8116},{\"end\":8663,\"start\":8542},{\"end\":8900,\"start\":8712},{\"end\":9312,\"start\":8902},{\"end\":9632,\"start\":9314},{\"end\":10326,\"start\":9655},{\"end\":10533,\"start\":10350},{\"end\":10725,\"start\":10535},{\"end\":11120,\"start\":10727},{\"end\":11509,\"start\":11122},{\"end\":11837,\"start\":11538},{\"end\":12466,\"start\":11839},{\"end\":12660,\"start\":12468},{\"end\":13132,\"start\":12662},{\"end\":13520,\"start\":13134},{\"end\":14028,\"start\":13522},{\"end\":14598,\"start\":14030},{\"end\":14765,\"start\":14600},{\"end\":15192,\"start\":14767},{\"end\":15444,\"start\":15214},{\"end\":16766,\"start\":15446},{\"end\":17054,\"start\":16768},{\"end\":18468,\"start\":17056},{\"end\":18837,\"start\":18470},{\"end\":20312,\"start\":19040},{\"end\":20332,\"start\":20314},{\"end\":20447,\"start\":20364},{\"end\":20998,\"start\":20486},{\"end\":21671,\"start\":21000},{\"end\":22130,\"start\":21673},{\"end\":22502,\"start\":22132},{\"end\":23002,\"start\":22504},{\"end\":23611,\"start\":23038},{\"end\":23892,\"start\":23613},{\"end\":24364,\"start\":23935},{\"end\":25356,\"start\":24409},{\"end\":25623,\"start\":25387},{\"end\":29621,\"start\":25625},{\"end\":30578,\"start\":29623}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8711,\"start\":8664},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19029,\"start\":18838}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18811,\"start\":18804},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19077,\"start\":19070},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":21699,\"start\":21692},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":22243,\"start\":22236},{\"end\":29499,\"start\":29492}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1663,\"start\":1651},{\"end\":2463,\"start\":2424},{\"end\":2528,\"start\":2520},{\"end\":2554,\"start\":2540},{\"attributes\":{\"n\":\"2\"},\"end\":5827,\"start\":5815},{\"attributes\":{\"n\":\"3\"},\"end\":7601,\"start\":7583},{\"attributes\":{\"n\":\"4\"},\"end\":9653,\"start\":9635},{\"attributes\":{\"n\":\"4.1\"},\"end\":10337,\"start\":10329},{\"end\":10348,\"start\":10340},{\"attributes\":{\"n\":\"4.2\"},\"end\":11536,\"start\":11512},{\"attributes\":{\"n\":\"4.3\"},\"end\":15212,\"start\":15195},{\"attributes\":{\"n\":\"5\"},\"end\":19038,\"start\":19031},{\"attributes\":{\"n\":\"6\"},\"end\":20362,\"start\":20335},{\"attributes\":{\"n\":\"6.1\"},\"end\":20484,\"start\":20450},{\"attributes\":{\"n\":\"6.2\"},\"end\":23036,\"start\":23005},{\"attributes\":{\"n\":\"6.3\"},\"end\":23933,\"start\":23895},{\"attributes\":{\"n\":\"6.4\"},\"end\":24407,\"start\":24367},{\"attributes\":{\"n\":\"7\"},\"end\":25385,\"start\":25359},{\"end\":30590,\"start\":30580},{\"end\":30856,\"start\":30847},{\"end\":32342,\"start\":32333},{\"end\":33474,\"start\":33465}]", "table": "[{\"end\":32331,\"start\":31870},{\"end\":33463,\"start\":33009},{\"end\":35295,\"start\":35287}]", "figure_caption": "[{\"end\":30845,\"start\":30592},{\"end\":31344,\"start\":30858},{\"end\":31870,\"start\":31347},{\"end\":33009,\"start\":32344},{\"end\":35287,\"start\":33476},{\"end\":36971,\"start\":35298}]", "figure_ref": "[{\"end\":2613,\"start\":2605},{\"end\":12065,\"start\":12057},{\"end\":12760,\"start\":12752},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23049,\"start\":23041}]", "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":41270,\"start\":38611},{\"attributes\":{\"id\":\"b1\"},\"end\":45081,\"start\":41272},{\"attributes\":{\"id\":\"b2\"},\"end\":46196,\"start\":45083}]", "bib_title": "[{\"end\":39168,\"start\":38611},{\"end\":45219,\"start\":45083}]", "bib_author": null, "bib_venue": "[{\"end\":39294,\"start\":39170},{\"end\":42384,\"start\":41272},{\"end\":45270,\"start\":45221}]"}}}, "year": 2023, "month": 12, "day": 17}