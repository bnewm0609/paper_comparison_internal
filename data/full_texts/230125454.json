{"id": 230125454, "updated": "2023-03-20 13:17:07.11", "metadata": {"title": "Multi-Task Knowledge Distillation for Eye Disease Prediction", "authors": "[{\"first\":\"Sahil\",\"last\":\"Chelaramani\",\"middle\":[]},{\"first\":\"Manish\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Vipul\",\"last\":\"Agarwal\",\"middle\":[]},{\"first\":\"Prashant\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Ranya\",\"last\":\"Habash\",\"middle\":[]}]", "venue": "2021 IEEE Winter Conference on Applications of Computer Vision (WACV)", "journal": "2021 IEEE Winter Conference on Applications of Computer Vision (WACV)", "publication_date": {"year": 2021, "month": 1, "day": 1}, "abstract": "While accurate disease prediction from retinal fundus images is critical, collecting large amounts of high quality labeled training data to build such supervised models is difficult. Deep learning classifiers have led to high accuracy results across a wide variety of medical imaging problems, but they need large amounts of labeled data. Given a fundus image, we aim to evaluate various solutions for learning deep neural classifiers using small labeled data for three tasks related to eye disease prediction: (T1) predicting one of the five broad categories \u2013 diabetic retinopathy, age-related macular degeneration, glaucoma, melanoma and normal, (T2) predicting one of the 320 fine-grained disease sub-categories, (T3) generating a textual diagnosis. The problem is challenging because of small data size, need for predictions across multiple tasks, handling image variations, and large number of hyper-parameter choices. Modeling the problem under a multi-task learning (MTL) setup, we investigate the contributions of each of the proposed tasks while dealing with a small amount of labeled data. Further, we suggest a novel MTL-based teacher ensemble method for knowledge distillation. On a dataset of 7212 labeled and 35854 unlabeled images across 3502 patients, our technique obtains ~83% accuracy, ~75% top-5 accuracy and ~48 BLEU for tasks T1, T2 and T3 respectively. Even with 15% training data, our method outperforms baselines by 8.1, 3.2 and 11.2 points for the three tasks respectively.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wacv/ChelaramaniGAGH21", "doi": "10.1109/wacv48630.2021.00403"}}, "content": {"source": {"pdf_hash": "f40527ec9c2a2e0b88d18641cb8eb9befe1de549", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7c2bddc5690d3ed7078a9db9fc5f0302a4dd1c42", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f40527ec9c2a2e0b88d18641cb8eb9befe1de549.txt", "contents": "\nMulti-Task Knowledge Distillation for Eye Disease Prediction\n\n\nSahil Chelaramani \nMicrosoft\n\n\nManish Gupta \nMicrosoft\n\n\nVipul Agarwal \nMicrosoft\n\n\nPrashant Gupta \nRanya Habash \nMicrosoft\n\n\nBascom Palmer Eye Institute\n\n\nMulti-Task Knowledge Distillation for Eye Disease Prediction\n\nWhile accurate disease prediction from retinal fundus images is critical, collecting large amounts of high quality labeled training data to build such supervised models is difficult. Deep learning classifiers have led to high accuracy results across a wide variety of medical imaging problems, but they need large amounts of labeled data. Given a fundus image, we aim to evaluate various solutions for learning deep neural classifiers using small labeled data for three tasks related to eye disease prediction: (T 1 ) predicting one of the five broad categories -diabetic retinopathy, age-related macular degeneration, glaucoma, melanoma and normal, (T 2 ) predicting one of the 320 fine-grained disease sub-categories, (T 3 ) generating a textual diagnosis. The problem is challenging because of small data size, need for predictions across multiple tasks, handling image variations, and large number of hyper-parameter choices. Modeling the problem under a multi-task learning (MTL) setup, we investigate the contributions of each of the proposed tasks while dealing with a small amount of labeled data. Further, we suggest a novel MTL-based teacher ensemble method for knowledge distillation. On a dataset of 7212 labeled and 35854 unlabeled images across 3502 patients, our technique obtains \u223c83% accuracy, \u223c75% top-5 accuracy and \u223c48 BLEU for tasks T 1 , T 2 and T 3 respectively. Even with 15% training data, our method outperforms baselines by 8.1, 3.2 and 11.2 points for the three tasks respectively.\n\nIntroduction\n\nEye diseases significantly impact the quality of life for patients. Four of such diseases are glaucoma, diabetic retinopathy (DR), age-related macular degeneration (AMD), and uveal melanoma. Glaucoma incidence worldwide is 64.3M in 2013 and projected to 76M in 2020 [1]. The disease affects \u223c2M people in USA 1 . In 2015, \u223c415M people were living with diabetes, of which \u223c145M have Each of the seven teachers is trained on a subset of three tasks. KD is then used to distill \"dark knowledge\" from all the teachers to the MTL student using both labeled as well as unlabeled instances making our model robust in small labelled data scenarios. some of DR 2 . Macular degeneration incidence worldwide is 196M in 2017 and projected to 288M by 2040 [2]. Although it is a relatively rare disease, uveal melanoma is the most common primary intraocular tumor in adults with a mean age-adjusted incidence of 5.1 cases per million per year 3 .\n\nEarly diagnosis of these eye diseases can help in effective treatment or at least in avoiding further progression of these diseases. Limited availability of ophthalmologists, lack of awareness and consultation expenses restrict early diagnosis. Hence, automated screening is critical. Recently, there has been a significant focus on applying deep learning techniques for medical imaging. However, deep learning classifiers are known to require large amounts of labeled train-ing data. Gathering labeled data for retinal imaging at a large scale is difficult because of the extensive human labeling effort, especially by qualified ophthalmologists. Hence, we explore different ways of obtaining high accuracy for disease prediction using small amounts of labeled retinal fundus images. Specifically, we explore three critical tasks namely coarse-grained disease classification, fine-grained disease classification, and detailed disease diagnosis generation. Given small labeled dataset L and large unlabeled dataset U of fundus images, we aim to train a classifier such that given a new fundus image, can predict with high accuracy, one of the following classes: DR, AMD, glaucoma, melanoma, normal, One of the 320 fine-grained sub-disease classes and a detailed textual diagnosis similar to that provided by ophthalmologist. Disease prediction given fundus images is very challenging because of the following reasons: (1) Gathering large amount of labeled fundus data is difficult. (2) Images have a lot of heterogeneity because of use of different lighting conditions, different devices, and different kinds of fundus images (like single field of view versus montage images), and the field of view captured. (3) Sometimes, minor changes in images are indicative of particular diseases. (4) Presence of artifacts like specular reflection, periphery haze, dust particle marks, fingerprints, blurred images, incorrectly stitched montage makes modeling complicated. (5) Absence of large corpus to learn robust embeddings for rare diagnosis words. (6) Images vary wrt. macula position -center, nasal, inferior, superior. Some images even have disc/macula cut out.\n\nThe three proposed tasks are very related; hence we model them using a deep learning based multi-task learning (MTL). Further, in presence of small labeled data, we augment the MTL setup with Knowledge Distillation (KD). In KD, a student model is trained to mimic a teacher model. Fig. 1 provides a conceptual illustration of the proposed approach. Given small amounts of labeled data for the three tasks, we train seven teacher models, one each for a subset of the three tasks. Thus, four of these seven teacher models are multi-task in nature. Further, we learn an MTL student model by distilling the \"dark knowledge\" from the seven teachers using both small-labeled as well as large-unlabeled data. Fig. 2 illustrates the detailed architecture of our proposed approach. The MTL teacher model M 1 jointly extracts image description relevant to the three tasks using ImageNetpretrained ResNet-50 [3], a deep convolutional neural network (CNN), fine-tuned on labeled data L. M 1 is trained end-to-end using gradient descent with a linear combination of cross entropy loss across the three tasks. A complex model with small data can lead to overfitting which can be avoided using regularization. Hence, we perform a twostage knowledge distillation (KD) [4]. In the first stage, model M 1 is finetuned to obtain model M 2 such that the cross entropy loss on labeled data is small and KL divergence between M 2 's output distributions and M 1 's output distributions across the three tasks is also minimized. In the second stage, to harness large unlabeled data U , under a semi-supervised setup, we further finetune M 2 to obtain M 3 by minimizing KL divergence between M 2 's output distributions and M 3 's output distributions across the three tasks.\n\nWe extensively experiment with multiple design choices: with and without MTL, with and without KD, with varying temperature for KD, with teacher ensemble and also with varying training data size. We are the only work focusing on modelling multiple eye diseases with the same model, leading to lack of good baselines. Our best method provides an accuracy of 82.52% for task 1, 51.11 (top-1)/75.19 (top-5) for task 2 and 48.1 BLEU for task 3, when 70% of the labeled data is used for training, while demonstrating gains for each task of 8.1, 3.2 and 11.2 points respectively, using only 15% of the labeled data. Our results show the effectiveness of the combined proposed MTL+KD architecture for training retinal disease prediction models from small labeled data.\n\nOverall, we make the following contributions:\n\n\u2022 We explore retinal disease classification task using small labeled training data.\n\n\u2022 We propose a learning algorithm using MTL and multi-teacher KD on different training data sizes. To the best of our knowledge, this is the first work on using KD in MTL scenario for retinal images.\n\n\u2022 Using a dataset of 7212 labeled and 35854 unlabeled fundus images for 3502 patients from 2004 to 2017, we show the effectiveness of the proposed methods. Using only 15% of the labeled data (1082 images), using MTL+KD, we can perform comparably to single task models trained with 70% data.\n\nThe paper is organized as follows. We discuss related work on machine learning for eyecare, deep learning for medical imaging, MTL and deep learning with small data in Section 2. In Section 3, we discuss details of the proposed methods. In Section 4, we present dataset details and insights from analysis of results. We conclude with a summary in Section 5.\n\n\nRelated Work\n\nMachine Learning for Eyecare. Work on applying predictive analytics for eyecare includes the following: prediction of post-operative surgery outcomes [5,6,7], disease prediction (glaucoma [8,9], AMD [10,11], DR [12]), segmentation of eye parts and anomalies (blood vessels [13], retina exudates, microaneurysms, drusen, cotton-wool spots), and predicting if eye tumor will metastasize [14]. We focus on the critical problem of screening patients concerning four most important eye diseases using fundus images.\n\nDeep Learning for Medical Imaging. Motivated by immense success of deep learning techniques in general vision, speech as well as text problems, there has been a lot of focus on applying deep learning for medical imaging recently [15,16]. Specifically, deep learning techniques have been applied to medical image data for neuro [17], retinal [12], pulmonary [18], digital pathology [19], breast [20], cardiac [21], abdominal [22], musculoskeletal [23] areas. Specifically, problem areas include image quality analysis [24], image segmentation [25], image/exam classification [26], object/lesion classification [27], and registration (i.e. spatial alignment) of medical images [28], image enhancement, and image reconstruction [29]. Although another work has recently focused on image classification and caption generation from retinal fundus images [30] with noisy labels, we build models with clean but small labeled data.\n\nMulti-task Learning. MTL has been used successfully across all applications of machine learning, from natural language processing [31] and speech recognition [32] to computer vision [33] and drug discovery [34]. MTL can be done with soft [35] or hard parameter sharing [36]. Further, recently, there has been some work [37,38] on performing knowledge distillation in the context of MTL for Transformer-based architectures on the GLUE [39] which is a set of NLP (Natural Language Processing) tasks. In this paper, we perform hard parameter sharing based MTL across three eye diagnosis tasks.\n\nDeep Learning with Small Labeled Data. Previous works have suggested various ways to handle small labeled data. Pre-training using transfer learning [40] from models trained on similar tasks with rich data, is a common approach. Traditionally, self training [41] has been the most popular semi-supervised approach to handle lack of enough labeled data. But self training requires careful threshold tuning for throttling to avoid label noise. Re-cently, KD [42,43] has been proposed as an approach for model compression but we observe that it can also be used as a semi-supervised technique [44]. In this paper we explore a combination of multitask learning, distillation, and their combined efficacy when applied to small labelled data.\n\n\nApproach\n\nIn this section, we explain the setup of the tasks using deep learning architectures. Next, we describe our proposed MTL architecture. Lastly, we discuss KD with MTL.\n\n\nTask Description\n\nFor each disease, fundus images show symptoms (Table 1). We attempt to diagnose such symptoms using deep learning methods. Early detection is critical to avoid further loss. We model task T 1 as a multi-class classification of with five classes (DR, AMD, melanoma, glaucoma and normal). Task T 2 is modeled as a 320-class classification problem, where the broad five classes have been divided further based on disease sub-types, disease severity, retinal regions and other important symptoms. Tasks T 1 and T 2 are modeled using ResNet-50 architecture. We initialize model weights using ImageNet pre-training to get a 1024D representation of every fundus image. ResNet output is connected to two dense layers with ReLU and softmax activations respectively. Last layer is of size 5 for T 1 and size 320 for T 2 ). We use cross entropy loss for both the tasks.\n\nWe model task T 3 (diagnosis generation) as an image captioning problem, where given a fundus image, we output the detailed disease diagnosis. The diagnosis generation model consists of a single layered LSTM, which takes the features generated by the CNN encoder (projected to a suitable latent representation using a dense layer) along with the \"start-of-sentence (SOS)\" token, and trains a language model conditioned on it. The hidden state of each timestep is projected to a vocabulary sized vector, on which softmax activation is applied. This vector is then sampled to generate the corresponding output word from the vocabulary.  Table 1: Disease description, treatments on early detection and visual symptoms in fundus images [9,11,12,45] .\n\nThe generated word is then fed into the next timestep as an input. We use teacher forcing to prevent training biases with ratio to 0.5. We use the cross entropy loss summed across all generated words.\n\n\nMulti-Task learning (MTL)\n\nA simple approach is to model the three tasks as three independent multi-class classification problems using neural networks. Such an approach would result in the model parameters growing as a factor of the number of tasks, and does not exploit task dependencies. To address these, we apply MTL by sharing hidden layers between all tasks, while keeping several task-specific output layers. Hence, we use a shared convolutional neural network (CNN) to obtain a latent representation for the input image. Few CNN architectures are popular like: AlexNet [46], Inception v3 [47], VGGNet-19 [48] and Resnet-50 [3]. Since ResNet-50 outperformed other architectures for multiple of our early experiments, we choose to use it to produce our shared encoded representation. Task specific layers for each task are conditioned on this output from ResNet-50. A detailed architecture diagram of the MTL model is provided in the supplementary material.\n\nWe use the small labeled data L to train a MTL-based deep ResNet-50 model M 1 (as shown in Fig. 2). Each instance in L consists of an image I along with a label s = (s 1 , s 2 , s 3 ). Note that s actually consists of three labels: broad classification label (s 1 ), fine-grained classification label (s 2 ), and diagnosis (s 3 ). Further, each instance I, s, s is scored against the model M 1 to obtain a prediction vector s = (s 1 , s 2 , s 3 ). s 1 is of size 5. s 2 is of size 320 . s 3 is a prediction matrix of size |s 3 |x|V | (where V is the vocabulary) and stores predictions for each time step of the LSTM. Thus, now, for every image I \u2208 L, we have hard labels s as well as M 1 predicted soft labels s . The final loss for the MTL M 1 model is computed as the weighted combination of the individual losses as shown in Eq. 1.\nL M1 ( I, s, s ) = 3 t=1 \u03bb t 1 CE(s t , s t )(1)\nwhere CE represents cross-entropy loss, and {\u03bb t 1 } 3 t=1 are tunable hyper-parameters for model M 1 .\n\n\nKnowledge Distillation (KD) with MTL\n\nKD (or student-teacher networks [42]) is a model compression method in which a small model (student) is trained to mimic a pre-trained, larger model (teacher), or an ensemble of models. In our paper, the student model has the same capacity as the teacher model. We adapt the KD approach for improved regularization and semi-supervised learning. KD with Labeled Data. KD uses the predicted distributions from the teacher and the student to define a KL divergence loss for training the student. These predicted distributions are obtained after the softmax activation which takes a hyper-parameter called temperature \u03c4 (usually \u03c4 > 1). Our student model M 2 has the same architecture as teacher M 1 . Loss for M 2 is a linear combination of cross entropy loss with respect to hard labels (between s and s ) and KLdivergence loss with respect to soft labels (between s and s ) where s = (s 1 , s 2 , s 3 ) is output prediction from M 2 as shown in Eq. 2.  Learning from Unlabeled Data. Further, we adapt the KD approach for improved semi-supervised learning as follows. Images I from unlabeled set U are scored against the model M 2 to obtain a soft prediction s . These predictions s are then used to further fine-tune model M 2 to obtain model M 3 using the KL-divergence loss between M 3 's predicted class/ diagnosis distributions (s ) and s as shown in Eq. 3.\nL M2 ( I, s, s , s ) = 3 t=1 \u03bb t 2 [CE(s t , s t ) + KL(s t , s t )](2)L M3 ( I, s , s ) = 3 t=1 \u03bb t 3 KL(s t , s t )(3)\nwhere KL represents KL divergence loss, and {\u03bb t 3 } 3 t=1 are tunable hyper-parameters for model M 3 . Given that models M 1 and M 2 have been trained on a relatively small set L, we hope that the addition of soft labels from U , will ensure that model M 3 generalizes better than the prior models.\n\nLearning from Multiple Teachers. In the MTL setup, different tasks may need variable amounts of data as well as number of steps to generalize adequately. Combinations of tasks may improve or hinder the learning process. We train models for every sub-combination of the three tasks, namely {T 1 , T 2 , T 3 , (T 1 , T 2 ), (T 1 , T 3 ), (T 2 , T 3 ), (T 1 , T 2 , T 3 )}. We use these models as the teacher ensemble to guide learning. Specifically, during training, a student model with task T i will distill knowledge (using KLdivergence) from teacher models which have learned task T i . Each teacher captures a different aspect of dynamics between tasks and provides the same as supervision to the student model. As discussed above, teachers are also trained on the labeled set L and fine-tuned on U .\n\n\nExperiments\n\n\nDataset and Experimental Settings\n\nOur dataset consists of 7212 labeled and 35854 unlabeled fundus images corresponding to 3502 patients who visited the hidden-for-review institute from 2004 to 2017. Fundus images have been captured using Topcon 50X with Ophthalmic Imaging Systems (OIS) capture station. The images vary in resolution and field of view. We resized all the images to a standard size of 224x224x3. We also normalized the pixel values to be between \u22121 and 1. The labeled images are assigned to either normal category or to one of the four broad disease categories: DR, AMD, glaucoma or melanoma. Labeled data is almost balanced. Our cleaned dataset contains 320 fine-grained sub categories of diseases. Each image is also labeled with a diagnosis description provided by ophthalmologists, with an average length of 7.69 words and mode of 4 words. We filtered out diagnosis that occur less than 10 times. These diagnosis contain a vocabulary of 237 words. Disease specific diagnosis vocabulary sizes are as follows: DR 130, AMD 59, Glaucoma 71, and Melanoma 15. Fig. 3 shows the diagnosis caption length distribution in words. Of these 237 words, we retained the most frequent 193. Also, we set maximum caption length to 14 and truncate longer captions. Table 2 shows top most popular unigrams, bigrams and trigrams per disease.\n\n\nExperimental Settings.\n\nAll architecture and hyperparameters choices were based on cross-validation. We choose to use Resnet-50 (initialized with pre-trained Ima-geNet weight) for the architecture of our shared image encoder. The architecture is trained with stochastic gradient descent (SGD) with Nesterov momentum. We set the learning rate for SGD to 0.01, with momentum value of 0.9. Our experiments showed that Adam optimizer also performed similar to SGD. We use early stopping, which on average completes training in 30 epochs. All models are trained on an Intel Xeon CPU machine with 32 cores and 2 Titan X GPUs (each with 12GB RAM) with a batch size of 128. For MTL, we set {\u03bb t {1,2,3} } 3 t=1 to 1 so as to give equal importance to each task. We additionally use an L2 weight regularization, whose coefficient we set to 10 \u22126 . For the LSTM decoder, we use embeddings of size 256, hidden layer with 512 units and teacher forcing ratio of 0.5. We learn embeddings dynamically and initialize with random embeddings. We use a fixed 15% of the labeled data for validation and test each. We learn various models by varying the training set percentages as p= [15,30,45,60,70]. All experiments are conducted in PyTorch. We make the code publicly available here 4 .\n\n\nResults\n\nWe attempt to answer the following questions: (1) How does training data size effect the MTL? (2) Does KD help individual tasks? (3) How do we find the optimal KD temperature for each task? (4) Behavior of task combinations with KD, and (5) Does using teacher ensemble for KD help? To answer these, we need to experiment with these design choices (with cardinality in brackets): (1) use KD or not (two), (2) task combinations (seven), (3) training data size (five), (4) temperature for KD (four), and (5) use of teacher ensemble or not for KD (two). Overall, there are a total of 2x7x5x4x2=560 combinations of hyper-parameters. After pruning hyper-parameters (as described in the subsections below), we end up performing 121 experiments to answer these questions. We are the only work focusing on modelling multiple eye diseases with the same model, leading to lack of good baselines. Effect of varying training data size on MTL (M 1 results). We investigate whether MTL helps with varying amount of labeled data without any KD. Table 3 highlights the performance on various tasks after training on a combination of tasks with varying dataset size p. Performance is measured using multi-class classification accuracy for tasks T 1 and T 2 , and BLEU [49] score for T 3 . Note that if we do not include a task in the MTL combination for training, we cannot use it for evaluation; hence some table cells appear empty. Clearly, MTL leads to improved accuracies compared to independently learning for either of the three tasks, across most (dataset size, task) combinations. Comparing with our best MTL model (last row from Table 3), T 3 benefits most with an average of 9.6% gain; while T 1 and T 2 have average gains of 1.4 and 4.8%. Also, benefits from MTL are higher for larger dataset sizes. MTL + KD with Labeled Data (M 2 results). Comparing the last row of Table 3 with row 4 of Table 5, we observe that M 2 improves over the M 1 accuracy by 2.2%, 2.2% and 5.2% across the three tasks resp. For detailed performance of model M 2 across various task combinations, please refer to the supplementary. KD + Temperature Optimization for each task. For each (task, data size) pair, we find the best temperature setting. To manage the burden of extensive computation, we follow a greedy exploration of \u03c4 , i.e., we select \u03c4 based on the best performance on individual tasks rather than a combination. Intuitively, if we had a perfect teacher, we could use an output distribution closer to a one-hot vector; if the teacher is imperfect, we would prefer to use a softer output distribution. This effect can be emulated by experimenting with different temperature values \u03c4 as [0.5, 1, 5, 10]. Small \u03c4 implies sharper distributions, while larger \u03c4 implies softer distributions. Balancing parameter \u03b1 for classifier with hard+soft labels was set to 0.95 (for soft loss). Table 4 shows that across various data fractions and across tasks, we observe better results than in Table 3. Thus, KD seems to be helpful across dataset sizes for retinal disease prediction. The best \u03c4 across tasks is 10 for dataset size p=15% while it is 5 for p=70% for most tasks. This leads to an intuitive observation: when p is small, the task is harder to learn and hence higher \u03c4 is desired. We show the analysis for M 3 only but we followed a similar approach for M 2 as well. MTL + KD with Unlabeled Data + Teacher Ensemble (M 3 results). Table 5 shows results when we combine MTL with KD. (T 1 , T 2 , T 3 ) is the best among all task combinations. Compared to only MTL (Table 3) with all three tasks together, MTL+KD shows massive gains especially with smaller data fractions for each task. Finally, we enable teacher ensemble for KD, rather than learning from a single teacher. Learning from teacher ensemble in an MTL setup implies that a model with tasks (T 1 , T 2 , T 3 ) can distill knowledge from teachers which were trained on task combinations (T 1 ), (T 2 ), (T 3 ), (T 1 , T 2 ), (T 1 , T 3 ), (T 2 , T 3 ), (T 1 , T 2 , T 3 ). Last line in Table 5 shows that MTL+KD with teacher ensemble is the best method, significantly better than the initial baseline. Comparing rows 4 and 6, M 3 improves task accuracies by 1.7%, 3.3%, 14.1% # Test\u2192 15  30  45  60  70  15  30  45  60  70  15  30  45  60 Table 5: Test Accuracy for KD+MTL on different combinations of tasks with varying dataset size p. For each cell, \u03c4 is the best temperature chosen for the (task combination, dataset size) pair from Table 4. Rows with \"+Ensemble\" correspond to using teacher ensemble for distillation. The results of using the combination of all three tasks in M 2 are shown for comparison. The rest of the results are for model M 3 .\nT 1 (Accuracy) T 2 (Accuracy) T 3 (BLEU) MTL Train\u2193 p \u2192\nrespectively over M 2 . Similarly, comparing rows 5 and 7, M 3 improves task accuracies by 2.1%, 6.9%, 17.6% respectively over M 2 . This proves that distillation with unlabeled data is extremely useful.\n\nDetailed Analysis of our Best Model. Table 6 shows confusion matrix for the best result for task T 1 . Interestingly the precision and recall across all classes is between 0.71 and 0.97. Notably, recall for melanoma is as high as 0.97. Fig. 4 shows top-K accuracy values for the fine-grained disease classification task across diseases for our best T 2 classifier. Top-5 accuracy values for task T 2 are \u223c96% for melanoma but on average top-5 accuracy across all diseases is \u223c75%. Further, for task T 3 , BLEU scores across the diseases are   . We achieve high BLEU scores for Melanoma, AMD, moderate BLEU for Glaucoma and relatively low BLEU for DR. Note that DR has the largest vocabulary size of 130 words, making the captions associated with the disease vary more than for other diseases, which had smaller vocabulary sizes. BLEU scores remain similar across diseases even for longer captions. Detailed error analysis can be found in the supplementary.  Table 6: Confusion matrix for broad disease prediction (T 1 ). As shown, we have a high correlation between actual and predicted values, indicating our model is effective.\n\nGrad-CAM Visualization. We used Gradient-weighted Class Activation Mapping (Grad-CAM) [50] to visualize the regions of fundus image that are \"important\" for disease predictions. It captures how intensely the input image activates different channels by computing how impor-tant each channel is with regard to the class. Fig. 5 shows class activation mapping visualizations for four randomly selected images, across the four diseases. The first column shows images with anomaly annotations by an ophthalmologist. The remaining columns show class activation mappings obtained using Grad-CAM for predictions by model M 1 (MTL but no KD) versus the best method M 3 (MTL+KD) for two different dataset sizes (15% and 70%). We show predicted outputs for all the three tasks: T 1 , T 2 and T 3 (Green\u223cCorrect, Yellow\u223cPartially correct, Red\u223cIncorrect). We observe that the Grad-CAM activations highly correlate with expert annotations across all the four images for a dataset size of 70%. However, for small (15%) dataset size, activations generated using KD+MTL have much higher correlation with expert annotations compared to those generated using the basic classifier, and hence lead to more accurate predictions.\n\n\nConclusion\n\nWe proposed the use of MTL and KD methods to improve fine-grained recognition of eye diseases using small labeled dataset of fundus images. Both KD and MTL result in massive boosts in performance across metrics. Tuning softmax temperature is important. Teacher ensemble method is more effective than just one teacher for KD. In the future, we plan to experiment with additional auxiliary tasks and images corresponding to more diseases.\n\nFigure 1 :\n1Multi-Task Learning (MTL) combined with Multi-Teacher Knowledge Distillation (KD) for eye disease prediction.\n\nFigure 2 :\n2Our model's training phases are depicted in the figure. The loss functions used in each phase are shown above the corresponding stages. CE stands for cross-entropy while KL stands for KL-divergence.\n\nFigure 3 :\n3Length distribution of captions (in words).\n\n\n1 ,T 2 ,T 3 +Ensemble(M 2 ) 0.760 0.779 0.759 0.803 0.782 0.397 0.411 0.438 0.480 0.481 0.258 0.333 0.353 0.447 0.421 6 T 1 ,T 2 ,T 3 (M 3 ) 0.751 0.764 0.790 0.795 0.808 0.384 0.436 0.454 0.482 0.503 0.337 0.363 0.400 0.460 0.475 7 T 1 ,T 2 ,T 3 +Ensemble(M 3 ) 0.765 0.782 0.789 0.803 0.825 0.414 0.441 0.487 0.507 0.511 0.346 0.371 0.432 0.473 0.481\n\nFigure 4 :\n4Top-K accuracy achieved by our best model values for fine-grained disease prediction (T 2 ) across diseases.\n\nFigure 5 :\n5Grad-CAM visualization for predictions using the model M 1 versus the proposed MTL+KD model M 3 across different diseases and different training dataset sizes along with their corresponding model outputs for T 1 , T 2 , T 3 . (Green\u223cCorrect, Yellow\u223cPartially correct, Red\u223cIncorrect) as follows: Melanoma (73.84), Glaucoma (50.62), AMD (73.84) and DR (39.86)\n\n\nDisease DescriptionTreatment on early detection Symptoms in fundus images microaneurysms, exudates, cotton wool spots, flame hemorrhages, dot-blot hemorrhages progressive accumulation of drusen in macula; geographic atrophy, increased pigment, and depigmentation Glaucoma damage to the optic nerve and cause vision loss tumors (2.5mm thick) look like pigmented dome shaped mass that extends from the ciliary body or choroid; orange lipofuscin pigmentation or subretinal fluidDR \ndamage to the retina due to dia-\nbetes mellitus \n\nlaser surgery, injection \nof \ncorticosteroids \nor \nanti-VEGF agents \n\nAMD \ndeteriorates the macula; distor-\ntion and loss of central vision \n\nanti-VEGF \nmedications \nand supplements \n\nmedication, laser treat-\nment, or surgery to slow or \nstop the progression \n\nanalyzing cup to disc ratio, Inferior Superior \nNasal Temporal (ISNT) features of cup and disc, \nand Optic Nerve Head atrophy \nMelanoma cancer of the eye involving the \niris, ciliary body, or choroid \n\nradiation therapy; Gamma \nKnife therapy; \nTher-\nmotherapy; \nSurgery \n(resection or enucleation). \n\n\n\nTable 2 :\n2Most popular unigrams, bigrams and trigrams per disease, from the diagnosis provided by ophthalmologists.where CE and KL represent cross-entropy and KL diver-\ngence loss respectively, and {\u03bb t \n2 } 3 \nt=1 are tunable hyper-\nparameters for model M 2 . \n\n\n\nTable 3 :\n3Test Accuracy for MTL on different combinations of tasks with varying dataset size p. No KD.Empty cells represent \n\n\nTable 4 :\n4Test Accuracy for KD on individual tasks (no MTL) with varying dataset size p and varying temperature \u03c4 . These are the results for model M 3 .# Test\u2192 \nT 1 (Accuracy) \nT 2 (Accuracy) \nT 3 (BLEU) \nMTL Train\u2193 p \u2192 \n15 \n30 \n45 \n60 \n70 \n15 \n30 \n45 \n60 \n70 \n15 \n30 \n45 \n60 \n70 \n1 T 1 ,T 2 \nhttps://github.com/SahilC/ mtl-kd-disease-recognition\n\nGlobal prevalence of glaucoma and projections of glaucoma burden through 2040: a systematic review and meta-analysis. Y.-C Tham, X Li, T Y Wong, H A Quigley, T Aung, C.-Y Cheng, ophth. 12111Y.-C. Tham, X. Li, T. Y. Wong, H. A. Quigley, T. Aung, and C.-Y. Cheng, \"Global prevalence of glau- coma and projections of glaucoma burden through 2040: a systematic review and meta-analysis,\" ophth., vol. 121, no. 11, pp. 2081-2090, 2014.\n\nUpdates on the epidemiology of age-related macular degeneration. J B Jonas, C M G Cheung, S Panda-Jonas, The Asia-Pacific Journal of ophth. 66J. B. Jonas, C. M. G. Cheung, and S. Panda-Jonas, \"Updates on the epidemiology of age-related macu- lar degeneration,\" The Asia-Pacific Journal of ophth., vol. 6, no. 6, pp. 493-497, 2017.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in CVPR, pp. 770- 778, 2016.\n\nCross modal distillation for supervision transfer. S Gupta, J Hoffman, J Malik, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionS. Gupta, J. Hoffman, and J. Malik, \"Cross modal dis- tillation for supervision transfer,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2827-2836, 2016.\n\nPredicting post-operative visual acuity for lasik surgeries. M Gupta, P Gupta, P K Vaddavalli, A Fatima, PAKDD. M. Gupta, P. Gupta, P. K. Vaddavalli, and A. Fatima, \"Predicting post-operative visual acuity for lasik surg- eries,\" in PAKDD, pp. 489-501, 2016.\n\nAccuracy Of Ordinary Least Squares And Empirical Bayes Estimates Of Short Term Visual Field Progression Rates To Predict Long Term Outcomes In Glaucoma. H L Rao, U K Addepalli, R K Yadav, N S Choudhari, S Senthil, A K Mandal, C S Garudadri, Investigative ophth. & Visual Science. 5314H. L. Rao, U. K. Addepalli, R. K. Yadav, N. S. Choud- hari, S. Senthil, A. K. Mandal, and C. S. Garudadri, \"Accuracy Of Ordinary Least Squares And Empirical Bayes Estimates Of Short Term Visual Field Progres- sion Rates To Predict Long Term Outcomes In Glau- coma,\" Investigative ophth. & Visual Science, vol. 53, no. 14, pp. 182-182, 2012.\n\nPredictors of Clinical Outcomes after Intrastromal Corneal Ring Segments Implantation. L Torquetti, G Ferrara, P Ferrara, Int J Keratoconus Ectatic Corneal Dis. 1L. Torquetti, G. Ferrara, and P. Ferrara, \"Predictors of Clinical Outcomes after Intrastromal Corneal Ring Segments Implantation,\" Int J Keratoconus Ectatic Corneal Dis, vol. 1, pp. 26-30, 2012.\n\nGlaucomatous Patterns in Frequency Doubling Technology (FDT) Perimetry Data identified by Unsupervised Machine Learning Classifiers. C Bowd, R N Weinreb, M Balasubramanian, I Lee, G Jang, S Yousefi, L M Zangwill, F A Medeiros, C A Girkin, J M Liebmann, PloS one. 9185941C. Bowd, R. N. Weinreb, M. Balasubramanian, I. Lee, G. Jang, S. Yousefi, L. M. Zangwill, F. A. Medeiros, C. A. Girkin, J. M. Liebmann, et al., \"Glauco- matous Patterns in Frequency Doubling Technology (FDT) Perimetry Data identified by Unsupervised Ma- chine Learning Classifiers,\" PloS one, vol. 9, no. 1, p. e85941, 2014.\n\nDisc-aware ensemble network for glaucoma screening from fundus image. H Fu, J Cheng, Y Xu, C Zhang, D W K Wong, J Liu, X Cao, TMI. 3711H. Fu, J. Cheng, Y. Xu, C. Zhang, D. W. K. Wong, J. Liu, and X. Cao, \"Disc-aware ensemble network for glaucoma screening from fundus image,\" TMI, vol. 37, no. 11, pp. 2493-2501, 2018.\n\nCombining Macula Clinical Signs and Patient Characteristics for Age-related Macular Degeneration Diagnosis: A Machine Learning Approach. P Fraccaro, M Nicolo, M Bonetto, M Giacomini, P Weller, C E Traverso, M Prosperi, D Osullivan, BMC ophth. 1511P. Fraccaro, M. Nicolo, M. Bonetto, M. Giacomini, P. Weller, C. E. Traverso, M. Prosperi, and D. OSul- livan, \"Combining Macula Clinical Signs and Patient Characteristics for Age-related Macular Degeneration Diagnosis: A Machine Learning Approach,\" BMC ophth., vol. 15, no. 1, p. 1, 2015.\n\nDeep learning is effective for classifying normal versus age-related macular degeneration oct images. C S Lee, D M Baughman, A Y Lee, ophth. Retina. 14C. S. Lee, D. M. Baughman, and A. Y. Lee, \"Deep learning is effective for classifying normal versus age-related macular degeneration oct images,\" ophth. Retina, vol. 1, no. 4, pp. 322-327, 2017.\n\nDevelopment and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs. V Gulshan, L Peng, M Coram, M C Stumpe, D Wu, A Narayanaswamy, S Venugopalan, K Widner, T Madams, J Cuadros, JAMA. 31622V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy, S. Venugopalan, K. Wid- ner, T. Madams, J. Cuadros, et al., \"Development and Validation of a Deep Learning Algorithm for Detec- tion of Diabetic Retinopathy in Retinal Fundus Pho- tographs,\" JAMA, vol. 316, no. 22, pp. 2402-2410, 2016.\n\nAn Ensemble Classification-based Approach applied to Retinal Blood Vessel Segmentation. M M Fraz, P Remagnino, A Hoppe, B Uyyanonvara, A R Rudnicka, C G Owen, S A , Bio Engg. 599BarmanM. M. Fraz, P. Remagnino, A. Hoppe, B. Uyyanon- vara, A. R. Rudnicka, C. G. Owen, and S. A. Bar- man, \"An Ensemble Classification-based Approach applied to Retinal Blood Vessel Segmentation,\" Bio Engg, vol. 59, no. 9, pp. 2538-2548, 2012.\n\nMolecular Prediction of Time to Metastasis from Ocular Melanoma Fine Needle Aspirates. J Harbour, Clinical Cancer Research. 1219SupplementJ. Harbour, \"Molecular Prediction of Time to Metas- tasis from Ocular Melanoma Fine Needle Aspirates,\" Clinical Cancer Research, vol. 12, no. 19 Supplement, pp. A77-A77, 2006.\n\nGuest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique. H Greenspan, B Van Ginneken, R M Summers, TMI. 355H. Greenspan, B. Van Ginneken, and R. M. Summers, \"Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new tech- nique,\" TMI, vol. 35, no. 5, pp. 1153-1159, 2016.\n\nA survey on deep learning in medical image analysis. G Litjens, T Kooi, B E Bejnordi, A A A Setio, F Ciompi, M Ghafoorian, J A Van Der Laak, B Van Ginneken, C I S\u00e1nchez, Medical image analysis. 42G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Se- tio, F. Ciompi, M. Ghafoorian, J. A. Van Der Laak, B. Van Ginneken, and C. I. S\u00e1nchez, \"A survey on deep learning in medical image analysis,\" Medical im- age analysis, vol. 42, pp. 60-88, 2017.\n\n3d deep learning for multi-modal imaging-guided survival time prediction of brain tumor patients. D Nie, H Zhang, E Adeli, L Liu, D Shen, MIC-CAID. Nie, H. Zhang, E. Adeli, L. Liu, and D. Shen, \"3d deep learning for multi-modal imaging-guided sur- vival time prediction of brain tumor patients,\" in MIC- CAI, pp. 212-220, 2016.\n\nDeep learning at chest radiography: automated classification of pulmonary tuberculosis by using cnns. P Lakhani, B Sundaram, Radiology. 2842P. Lakhani and B. Sundaram, \"Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using cnns,\" Radiology, vol. 284, no. 2, pp. 574-582, 2017.\n\nDeep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases. A Janowczyk, A Madabhushi, J. Path. informatics. 7A. Janowczyk and A. Madabhushi, \"Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases,\" J. Path. informatics, vol. 7, 2016.\n\nStacked sparse autoencoder (ssae) for nuclei detection on breast cancer histopathology images. J Xu, L Xiang, Q Liu, H Gilmore, J Wu, J Tang, A Madabhushi, TMI. 351J. Xu, L. Xiang, Q. Liu, H. Gilmore, J. Wu, J. Tang, and A. Madabhushi, \"Stacked sparse au- toencoder (ssae) for nuclei detection on breast cancer histopathology images,\" TMI, vol. 35, no. 1, pp. 119- 130, 2015.\n\nA combined deep-learning and deformable-model approach to fully automatic segmentation of the left ventricle in cardiac mri. M Avendi, A Kheradvar, H Jafarkhani, Medical Image Analysis. 30M. Avendi, A. Kheradvar, and H. Jafarkhani, \"A com- bined deep-learning and deformable-model approach to fully automatic segmentation of the left ventri- cle in cardiac mri,\" Medical Image Analysis, vol. 30, pp. 108-119, 2016.\n\nTransfer learning with cnns for classification of abdominal ultrasound images. P M Cheng, H S Malhi, J. digital imaging. 302P. M. Cheng and H. S. Malhi, \"Transfer learning with cnns for classification of abdominal ultrasound im- ages,\" J. digital imaging, vol. 30, no. 2, pp. 234-243, 2017.\n\nDeep cnn and 3d deformable approach for tissue segmentation in musculoskeletal magnetic resonance imaging. F Liu, Z Zhou, H Jang, A Samsonov, G Zhao, R Kijowski, Magnetic resonance in medicine. 794F. Liu, Z. Zhou, H. Jang, A. Samsonov, G. Zhao, and R. Kijowski, \"Deep cnn and 3d deformable approach for tissue segmentation in musculoskeletal magnetic resonance imaging,\" Magnetic resonance in medicine, vol. 79, no. 4, pp. 2379-2391, 2018.\n\nAutomatic visual quality assessment in optical fundus images. M Lalonde, L Gagnon, M.-C Boucher, Vision Interface. 32M. Lalonde, L. Gagnon, M.-C. Boucher, et al., \"Auto- matic visual quality assessment in optical fundus im- ages,\" in Vision Interface, vol. 32, pp. 259-264, 2001.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, MICCAI. O. Ronneberger, P. Fischer, and T. Brox, \"U-net: Con- volutional networks for biomedical image segmenta- tion,\" in MICCAI, pp. 234-241, 2015.\n\nDeep learning of feature representation with multiple instance learning for medical image analysis. Y Xu, T Mo, Q Feng, P Zhong, M Lai, I Eric, C Chang, ICASSP. Y. Xu, T. Mo, Q. Feng, P. Zhong, M. Lai, I. Eric, and C. Chang, \"Deep learning of feature representa- tion with multiple instance learning for medical image analysis,\" in ICASSP, pp. 1626-1630, 2014.\n\nDermatologistlevel classification of skin cancer with deep neural networks. A Esteva, B Kuprel, R A Novoa, J Ko, S M Swetter, H M Blau, S Thrun, Nature. 5427639115A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, and S. Thrun, \"Dermatologist- level classification of skin cancer with deep neural net- works,\" Nature, vol. 542, no. 7639, p. 115, 2017.\n\nEnd-to-end unsupervised deformable image registration with a cnn. B D De Vos, F F Berendsen, M A Viergever, M Staring, I I\u0161gum, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. B. D. de Vos, F. F. Berendsen, M. A. Viergever, M. Staring, and I. I\u0161gum, \"End-to-end unsupervised deformable image registration with a cnn,\" in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 204-212, 2017.\n\nLearning a variational network for reconstruction of accelerated mri data. K Hammernik, T Klatzer, E Kobler, M P Recht, D K Sodickson, T Pock, F Knoll, Magnetic resonance in medicine. 796K. Hammernik, T. Klatzer, E. Kobler, M. P. Recht, D. K. Sodickson, T. Pock, and F. Knoll, \"Learning a variational network for reconstruction of accelerated mri data,\" Magnetic resonance in medicine, vol. 79, no. 6, pp. 3055-3071, 2018.\n\nMulti-task learning for eye disease prediction. S Chelaramani, M Gupta, V Agarwal, P Gupta, R Habash, ACPRS. Chelaramani, M. Gupta, V. Agarwal, P. Gupta, and R. Habash, \"Multi-task learning for eye disease pre- diction,\" in ACPR, 2019.\n\nA unified architecture for natural language processing: Deep neural networks with multitask learning. R Collobert, J Weston, ICML. R. Collobert and J. Weston, \"A unified architecture for natural language processing: Deep neural networks with multitask learning,\" in ICML, pp. 160-167, 2008.\n\nNew types of dnn learning for speech recognition and related applications: An overview. L Deng, G Hinton, B Kingsbury, ICASSP. L. Deng, G. Hinton, and B. Kingsbury, \"New types of dnn learning for speech recognition and related ap- plications: An overview,\" in ICASSP, pp. 8599-8603, 2013.\n\nFast r-cnn. R Girshick, ICCV. R. Girshick, \"Fast r-cnn,\" in ICCV, pp. 1440-1448, 2015.\n\nMassively multitask networks for drug discovery. B Ramsundar, S Kearnes, P Riley, D Webster, D Konerding, V Pande, arXivB. Ramsundar, S. Kearnes, P. Riley, D. Webster, D. Konerding, and V. Pande, \"Massively multitask networks for drug discovery,\" arXiv, 2015.\n\nLow resource dependency parsing: Cross-lingual parameter sharing in a nn parser. L Duong, T Cohn, S Bird, P Cook, IJCNLP. L. Duong, T. Cohn, S. Bird, and P. Cook, \"Low re- source dependency parsing: Cross-lingual parameter sharing in a nn parser,\" in IJCNLP, pp. 845-850, 2015.\n\nMultitask learning: A knowledge-based source of inductive bias. R Caruana, ICML. R. Caruana, \"Multitask learning: A knowledge-based source of inductive bias,\" in ICML, pp. 41-48, 1993.\n\nAttentive student meets multi-task teacher: Improved knowledge distillation for pretrained models. L Liu, H Wang, J Lin, R Socher, C Xiong, arXiv:1911.03588arXiv preprintL. Liu, H. Wang, J. Lin, R. Socher, and C. Xiong, \"Attentive student meets multi-task teacher: Improved knowledge distillation for pretrained models,\" arXiv preprint arXiv:1911.03588, 2019.\n\nImproving multitask deep neural networks via knowledge distillation for natural language understanding. X Liu, P He, W Chen, J Gao, arXiv:1904.09482arXiv preprintX. Liu, P. He, W. Chen, and J. Gao, \"Improving multi- task deep neural networks via knowledge distillation for natural language understanding,\" arXiv preprint arXiv:1904.09482, 2019.\n\nGlue: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, 7th International Conference on Learning Representations. A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, \"Glue: A multi-task benchmark and analysis platform for natural language understanding,\" in 7th International Conference on Learning Repre- sentations, ICLR 2019, 2019.\n\nHow transferable are features in deep neural networks?. J Yosinski, J Clune, Y Bengio, H Lipson, NIPS. J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, \"How transferable are features in deep neural networks?,\" in NIPS, pp. 3320-3328, 2014.\n\nSemisupervised learning (chapelle. O Chapelle, B Scholkopf, A Zien, IEEE Trans. on Neural Networks. o. et al.203book reviewsO. Chapelle, B. Scholkopf, and A. Zien, \"Semi- supervised learning (chapelle, o. et al., eds.; 2006)[book reviews],\" IEEE Trans. on Neural Net- works, vol. 20, no. 3, pp. 542-542, 2009.\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, arXiv:1503.02531arXiv preprintG. Hinton, O. Vinyals, and J. Dean, \"Distilling the knowledge in a neural network,\" arXiv preprint arXiv:1503.02531, 2015.\n\nImproved knowledge distillation via teacher assistant: Bridging the gap between student and teacher. S.-I Mirzadeh, M Farajtabar, A Li, H Ghasemzadeh, arXiv:1902.03393arXiv preprintS.-I. Mirzadeh, M. Farajtabar, A. Li, and H. Ghasemzadeh, \"Improved knowledge distillation via teacher assistant: Bridging the gap between stu- dent and teacher,\" arXiv preprint arXiv:1902.03393, 2019.\n\nSelftraining with noisy student improves imagenet classification. Q Xie, M.-T Luong, E Hovy, Q V Le, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionQ. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, \"Self- training with noisy student improves imagenet clas- sification,\" in Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pp. 10687-10698, 2020.\n\nUveal melanoma: trends in incidence, treatment, and survival. A D Singh, M E Turell, A K Topham, Ophthalmology. 1189A. D. Singh, M. E. Turell, and A. K. Topham, \"Uveal melanoma: trends in incidence, treatment, and sur- vival,\" Ophthalmology, vol. 118, no. 9, pp. 1881- 1885, 2011.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Im- agenet classification with deep convolutional neural networks,\" in NIPS, pp. 1097-1105, 2012.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, CVPR. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking the inception architecture for computer vision,\" in CVPR, pp. 2818-2826, 2016.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXivK. Simonyan and A. Zisserman, \"Very deep convo- lutional networks for large-scale image recognition,\" arXiv, 2014.\n\nBleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, ACL. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \"Bleu: a method for automatic evaluation of machine translation,\" in ACL, pp. 311-318, 2002.\n\nGrad-cam: Visual explanations from deep networks via gradient-based localization. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, ICCV. R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, \"Grad-cam: Visual explana- tions from deep networks via gradient-based localiza- tion,\" in ICCV, pp. 618-626, 2017.\n", "annotations": {"author": "[{\"end\":94,\"start\":64},{\"end\":120,\"start\":95},{\"end\":147,\"start\":121},{\"end\":163,\"start\":148},{\"end\":219,\"start\":164}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":70},{\"end\":107,\"start\":102},{\"end\":134,\"start\":127},{\"end\":162,\"start\":157},{\"end\":176,\"start\":170}]", "author_first_name": "[{\"end\":69,\"start\":64},{\"end\":101,\"start\":95},{\"end\":126,\"start\":121},{\"end\":156,\"start\":148},{\"end\":169,\"start\":164}]", "author_affiliation": "[{\"end\":93,\"start\":83},{\"end\":119,\"start\":109},{\"end\":146,\"start\":136},{\"end\":188,\"start\":178},{\"end\":218,\"start\":190}]", "title": "[{\"end\":61,\"start\":1},{\"end\":280,\"start\":220}]", "venue": null, "abstract": "[{\"end\":1791,\"start\":282}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2076,\"start\":2073},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2117,\"start\":2116},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2553,\"start\":2550},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2737,\"start\":2736},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4226,\"start\":4223},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5802,\"start\":5799},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6157,\"start\":6154},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8570,\"start\":8567},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8572,\"start\":8570},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8574,\"start\":8572},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8608,\"start\":8605},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8610,\"start\":8608},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8620,\"start\":8616},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8623,\"start\":8620},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8632,\"start\":8628},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8694,\"start\":8690},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8806,\"start\":8802},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9162,\"start\":9158},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9165,\"start\":9162},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9260,\"start\":9256},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9274,\"start\":9270},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9290,\"start\":9286},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9314,\"start\":9310},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9327,\"start\":9323},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9341,\"start\":9337},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9357,\"start\":9353},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9379,\"start\":9375},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9450,\"start\":9446},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9475,\"start\":9471},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9507,\"start\":9503},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9542,\"start\":9538},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9608,\"start\":9604},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9658,\"start\":9654},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9781,\"start\":9777},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9987,\"start\":9983},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10015,\"start\":10011},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10039,\"start\":10035},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10063,\"start\":10059},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10095,\"start\":10091},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10126,\"start\":10122},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10176,\"start\":10172},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10179,\"start\":10176},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10291,\"start\":10287},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10598,\"start\":10594},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10707,\"start\":10703},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10905,\"start\":10901},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10908,\"start\":10905},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11039,\"start\":11035},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12976,\"start\":12973},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12979,\"start\":12976},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12982,\"start\":12979},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12985,\"start\":12982},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13774,\"start\":13770},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13793,\"start\":13789},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13809,\"start\":13805},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13827,\"start\":13824},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15222,\"start\":15218},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20300,\"start\":20296},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20303,\"start\":20300},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20306,\"start\":20303},{\"end\":20309,\"start\":20306},{\"end\":20312,\"start\":20309},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21666,\"start\":21662},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":26591,\"start\":26587}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28281,\"start\":28159},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28493,\"start\":28282},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28550,\"start\":28494},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28905,\"start\":28551},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29027,\"start\":28906},{\"attributes\":{\"id\":\"fig_7\"},\"end\":29398,\"start\":29028},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30492,\"start\":29399},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30758,\"start\":30493},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30886,\"start\":30759},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31182,\"start\":30887}]", "paragraph": "[{\"end\":2739,\"start\":1807},{\"end\":4900,\"start\":2741},{\"end\":6653,\"start\":4902},{\"end\":7416,\"start\":6655},{\"end\":7463,\"start\":7418},{\"end\":7548,\"start\":7465},{\"end\":7749,\"start\":7550},{\"end\":8041,\"start\":7751},{\"end\":8400,\"start\":8043},{\"end\":8927,\"start\":8417},{\"end\":9851,\"start\":8929},{\"end\":10443,\"start\":9853},{\"end\":11181,\"start\":10445},{\"end\":11360,\"start\":11194},{\"end\":12239,\"start\":11381},{\"end\":12987,\"start\":12241},{\"end\":13189,\"start\":12989},{\"end\":14156,\"start\":13219},{\"end\":14992,\"start\":14158},{\"end\":15145,\"start\":15042},{\"end\":16546,\"start\":15186},{\"end\":16967,\"start\":16668},{\"end\":17772,\"start\":16969},{\"end\":19130,\"start\":17824},{\"end\":20400,\"start\":19157},{\"end\":25108,\"start\":20412},{\"end\":25368,\"start\":25165},{\"end\":26499,\"start\":25370},{\"end\":27707,\"start\":26501},{\"end\":28158,\"start\":27722}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15041,\"start\":14993},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16618,\"start\":16547},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16667,\"start\":16618},{\"attributes\":{\"id\":\"formula_3\"},\"end\":25164,\"start\":25109}]", "table_ref": "[{\"end\":12883,\"start\":12876},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19063,\"start\":19056},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21448,\"start\":21441},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22040,\"start\":22032},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22280,\"start\":22273},{\"end\":22302,\"start\":22295},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23282,\"start\":23275},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23383,\"start\":23376},{\"end\":23832,\"start\":23825},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23965,\"start\":23957},{\"end\":24447,\"start\":24440},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24692,\"start\":24638},{\"end\":24700,\"start\":24693},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24897,\"start\":24890},{\"end\":25414,\"start\":25407},{\"end\":26335,\"start\":26328}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1805,\"start\":1793},{\"attributes\":{\"n\":\"2.\"},\"end\":8415,\"start\":8403},{\"attributes\":{\"n\":\"3.\"},\"end\":11192,\"start\":11184},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11379,\"start\":11363},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13217,\"start\":13192},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15184,\"start\":15148},{\"attributes\":{\"n\":\"4.\"},\"end\":17786,\"start\":17775},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17822,\"start\":17789},{\"end\":19155,\"start\":19133},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20410,\"start\":20403},{\"attributes\":{\"n\":\"5.\"},\"end\":27720,\"start\":27710},{\"end\":28170,\"start\":28160},{\"end\":28293,\"start\":28283},{\"end\":28505,\"start\":28495},{\"end\":28917,\"start\":28907},{\"end\":29039,\"start\":29029},{\"end\":30503,\"start\":30494},{\"end\":30769,\"start\":30760},{\"end\":30897,\"start\":30888}]", "table": "[{\"end\":30492,\"start\":29876},{\"end\":30758,\"start\":30610},{\"end\":30886,\"start\":30863},{\"end\":31182,\"start\":31042}]", "figure_caption": "[{\"end\":28281,\"start\":28172},{\"end\":28493,\"start\":28295},{\"end\":28550,\"start\":28507},{\"end\":28905,\"start\":28553},{\"end\":29027,\"start\":28919},{\"end\":29398,\"start\":29041},{\"end\":29876,\"start\":29401},{\"end\":30610,\"start\":30505},{\"end\":30863,\"start\":30771},{\"end\":31042,\"start\":30899}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5189,\"start\":5183},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5610,\"start\":5604},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14256,\"start\":14249},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18870,\"start\":18864},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25612,\"start\":25606},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26826,\"start\":26820}]", "bib_author_first_name": "[{\"end\":31360,\"start\":31356},{\"end\":31368,\"start\":31367},{\"end\":31374,\"start\":31373},{\"end\":31376,\"start\":31375},{\"end\":31384,\"start\":31383},{\"end\":31386,\"start\":31385},{\"end\":31397,\"start\":31396},{\"end\":31408,\"start\":31404},{\"end\":31736,\"start\":31735},{\"end\":31738,\"start\":31737},{\"end\":31747,\"start\":31746},{\"end\":31751,\"start\":31748},{\"end\":31761,\"start\":31760},{\"end\":32049,\"start\":32048},{\"end\":32055,\"start\":32054},{\"end\":32064,\"start\":32063},{\"end\":32071,\"start\":32070},{\"end\":32250,\"start\":32249},{\"end\":32259,\"start\":32258},{\"end\":32270,\"start\":32269},{\"end\":32676,\"start\":32675},{\"end\":32685,\"start\":32684},{\"end\":32694,\"start\":32693},{\"end\":32696,\"start\":32695},{\"end\":32710,\"start\":32709},{\"end\":33028,\"start\":33027},{\"end\":33030,\"start\":33029},{\"end\":33037,\"start\":33036},{\"end\":33039,\"start\":33038},{\"end\":33052,\"start\":33051},{\"end\":33054,\"start\":33053},{\"end\":33063,\"start\":33062},{\"end\":33065,\"start\":33064},{\"end\":33078,\"start\":33077},{\"end\":33089,\"start\":33088},{\"end\":33091,\"start\":33090},{\"end\":33101,\"start\":33100},{\"end\":33103,\"start\":33102},{\"end\":33588,\"start\":33587},{\"end\":33601,\"start\":33600},{\"end\":33612,\"start\":33611},{\"end\":33992,\"start\":33991},{\"end\":34000,\"start\":33999},{\"end\":34002,\"start\":34001},{\"end\":34013,\"start\":34012},{\"end\":34032,\"start\":34031},{\"end\":34039,\"start\":34038},{\"end\":34047,\"start\":34046},{\"end\":34058,\"start\":34057},{\"end\":34060,\"start\":34059},{\"end\":34072,\"start\":34071},{\"end\":34074,\"start\":34073},{\"end\":34086,\"start\":34085},{\"end\":34088,\"start\":34087},{\"end\":34098,\"start\":34097},{\"end\":34100,\"start\":34099},{\"end\":34524,\"start\":34523},{\"end\":34530,\"start\":34529},{\"end\":34539,\"start\":34538},{\"end\":34545,\"start\":34544},{\"end\":34554,\"start\":34553},{\"end\":34558,\"start\":34555},{\"end\":34566,\"start\":34565},{\"end\":34573,\"start\":34572},{\"end\":34911,\"start\":34910},{\"end\":34923,\"start\":34922},{\"end\":34933,\"start\":34932},{\"end\":34944,\"start\":34943},{\"end\":34957,\"start\":34956},{\"end\":34967,\"start\":34966},{\"end\":34969,\"start\":34968},{\"end\":34981,\"start\":34980},{\"end\":34993,\"start\":34992},{\"end\":35413,\"start\":35412},{\"end\":35415,\"start\":35414},{\"end\":35422,\"start\":35421},{\"end\":35424,\"start\":35423},{\"end\":35436,\"start\":35435},{\"end\":35438,\"start\":35437},{\"end\":35783,\"start\":35782},{\"end\":35794,\"start\":35793},{\"end\":35802,\"start\":35801},{\"end\":35811,\"start\":35810},{\"end\":35813,\"start\":35812},{\"end\":35823,\"start\":35822},{\"end\":35829,\"start\":35828},{\"end\":35846,\"start\":35845},{\"end\":35861,\"start\":35860},{\"end\":35871,\"start\":35870},{\"end\":35881,\"start\":35880},{\"end\":36298,\"start\":36297},{\"end\":36300,\"start\":36299},{\"end\":36308,\"start\":36307},{\"end\":36321,\"start\":36320},{\"end\":36330,\"start\":36329},{\"end\":36345,\"start\":36344},{\"end\":36347,\"start\":36346},{\"end\":36359,\"start\":36358},{\"end\":36361,\"start\":36360},{\"end\":36369,\"start\":36368},{\"end\":36371,\"start\":36370},{\"end\":36721,\"start\":36720},{\"end\":37057,\"start\":37056},{\"end\":37070,\"start\":37069},{\"end\":37086,\"start\":37085},{\"end\":37088,\"start\":37087},{\"end\":37365,\"start\":37364},{\"end\":37376,\"start\":37375},{\"end\":37384,\"start\":37383},{\"end\":37386,\"start\":37385},{\"end\":37398,\"start\":37397},{\"end\":37402,\"start\":37399},{\"end\":37411,\"start\":37410},{\"end\":37421,\"start\":37420},{\"end\":37435,\"start\":37434},{\"end\":37437,\"start\":37436},{\"end\":37453,\"start\":37452},{\"end\":37469,\"start\":37468},{\"end\":37471,\"start\":37470},{\"end\":37851,\"start\":37850},{\"end\":37858,\"start\":37857},{\"end\":37867,\"start\":37866},{\"end\":37876,\"start\":37875},{\"end\":37883,\"start\":37882},{\"end\":38184,\"start\":38183},{\"end\":38195,\"start\":38194},{\"end\":38504,\"start\":38503},{\"end\":38517,\"start\":38516},{\"end\":38822,\"start\":38821},{\"end\":38828,\"start\":38827},{\"end\":38837,\"start\":38836},{\"end\":38844,\"start\":38843},{\"end\":38855,\"start\":38854},{\"end\":38861,\"start\":38860},{\"end\":38869,\"start\":38868},{\"end\":39229,\"start\":39228},{\"end\":39239,\"start\":39238},{\"end\":39252,\"start\":39251},{\"end\":39599,\"start\":39598},{\"end\":39601,\"start\":39600},{\"end\":39610,\"start\":39609},{\"end\":39612,\"start\":39611},{\"end\":39919,\"start\":39918},{\"end\":39926,\"start\":39925},{\"end\":39934,\"start\":39933},{\"end\":39942,\"start\":39941},{\"end\":39954,\"start\":39953},{\"end\":39962,\"start\":39961},{\"end\":40315,\"start\":40314},{\"end\":40326,\"start\":40325},{\"end\":40339,\"start\":40335},{\"end\":40599,\"start\":40598},{\"end\":40614,\"start\":40613},{\"end\":40625,\"start\":40624},{\"end\":40884,\"start\":40883},{\"end\":40890,\"start\":40889},{\"end\":40896,\"start\":40895},{\"end\":40904,\"start\":40903},{\"end\":40913,\"start\":40912},{\"end\":40920,\"start\":40919},{\"end\":40928,\"start\":40927},{\"end\":41222,\"start\":41221},{\"end\":41232,\"start\":41231},{\"end\":41242,\"start\":41241},{\"end\":41244,\"start\":41243},{\"end\":41253,\"start\":41252},{\"end\":41259,\"start\":41258},{\"end\":41261,\"start\":41260},{\"end\":41272,\"start\":41271},{\"end\":41274,\"start\":41273},{\"end\":41282,\"start\":41281},{\"end\":41583,\"start\":41582},{\"end\":41585,\"start\":41584},{\"end\":41595,\"start\":41594},{\"end\":41597,\"start\":41596},{\"end\":41610,\"start\":41609},{\"end\":41612,\"start\":41611},{\"end\":41625,\"start\":41624},{\"end\":41636,\"start\":41635},{\"end\":42075,\"start\":42074},{\"end\":42088,\"start\":42087},{\"end\":42099,\"start\":42098},{\"end\":42109,\"start\":42108},{\"end\":42111,\"start\":42110},{\"end\":42120,\"start\":42119},{\"end\":42122,\"start\":42121},{\"end\":42135,\"start\":42134},{\"end\":42143,\"start\":42142},{\"end\":42472,\"start\":42471},{\"end\":42487,\"start\":42486},{\"end\":42496,\"start\":42495},{\"end\":42507,\"start\":42506},{\"end\":42516,\"start\":42515},{\"end\":42763,\"start\":42762},{\"end\":42776,\"start\":42775},{\"end\":43041,\"start\":43040},{\"end\":43049,\"start\":43048},{\"end\":43059,\"start\":43058},{\"end\":43255,\"start\":43254},{\"end\":43380,\"start\":43379},{\"end\":43393,\"start\":43392},{\"end\":43404,\"start\":43403},{\"end\":43413,\"start\":43412},{\"end\":43424,\"start\":43423},{\"end\":43437,\"start\":43436},{\"end\":43673,\"start\":43672},{\"end\":43682,\"start\":43681},{\"end\":43690,\"start\":43689},{\"end\":43698,\"start\":43697},{\"end\":43935,\"start\":43934},{\"end\":44156,\"start\":44155},{\"end\":44163,\"start\":44162},{\"end\":44171,\"start\":44170},{\"end\":44178,\"start\":44177},{\"end\":44188,\"start\":44187},{\"end\":44522,\"start\":44521},{\"end\":44529,\"start\":44528},{\"end\":44535,\"start\":44534},{\"end\":44543,\"start\":44542},{\"end\":44851,\"start\":44850},{\"end\":44859,\"start\":44858},{\"end\":44868,\"start\":44867},{\"end\":44879,\"start\":44878},{\"end\":44887,\"start\":44886},{\"end\":44895,\"start\":44894},{\"end\":44897,\"start\":44896},{\"end\":45258,\"start\":45257},{\"end\":45270,\"start\":45269},{\"end\":45279,\"start\":45278},{\"end\":45289,\"start\":45288},{\"end\":45478,\"start\":45477},{\"end\":45490,\"start\":45489},{\"end\":45503,\"start\":45502},{\"end\":45800,\"start\":45799},{\"end\":45810,\"start\":45809},{\"end\":45821,\"start\":45820},{\"end\":46087,\"start\":46083},{\"end\":46099,\"start\":46098},{\"end\":46113,\"start\":46112},{\"end\":46119,\"start\":46118},{\"end\":46433,\"start\":46432},{\"end\":46443,\"start\":46439},{\"end\":46452,\"start\":46451},{\"end\":46460,\"start\":46459},{\"end\":46462,\"start\":46461},{\"end\":46907,\"start\":46906},{\"end\":46909,\"start\":46908},{\"end\":46918,\"start\":46917},{\"end\":46920,\"start\":46919},{\"end\":46930,\"start\":46929},{\"end\":46932,\"start\":46931},{\"end\":47192,\"start\":47191},{\"end\":47206,\"start\":47205},{\"end\":47219,\"start\":47218},{\"end\":47221,\"start\":47220},{\"end\":47443,\"start\":47442},{\"end\":47454,\"start\":47453},{\"end\":47467,\"start\":47466},{\"end\":47476,\"start\":47475},{\"end\":47486,\"start\":47485},{\"end\":47722,\"start\":47721},{\"end\":47734,\"start\":47733},{\"end\":47932,\"start\":47931},{\"end\":47944,\"start\":47943},{\"end\":47954,\"start\":47953},{\"end\":47965,\"start\":47961},{\"end\":48201,\"start\":48200},{\"end\":48203,\"start\":48202},{\"end\":48216,\"start\":48215},{\"end\":48228,\"start\":48227},{\"end\":48235,\"start\":48234},{\"end\":48247,\"start\":48246},{\"end\":48257,\"start\":48256}]", "bib_author_last_name": "[{\"end\":31365,\"start\":31361},{\"end\":31371,\"start\":31369},{\"end\":31381,\"start\":31377},{\"end\":31394,\"start\":31387},{\"end\":31402,\"start\":31398},{\"end\":31414,\"start\":31409},{\"end\":31744,\"start\":31739},{\"end\":31758,\"start\":31752},{\"end\":31773,\"start\":31762},{\"end\":32052,\"start\":32050},{\"end\":32061,\"start\":32056},{\"end\":32068,\"start\":32065},{\"end\":32075,\"start\":32072},{\"end\":32256,\"start\":32251},{\"end\":32267,\"start\":32260},{\"end\":32276,\"start\":32271},{\"end\":32682,\"start\":32677},{\"end\":32691,\"start\":32686},{\"end\":32707,\"start\":32697},{\"end\":32717,\"start\":32711},{\"end\":33034,\"start\":33031},{\"end\":33049,\"start\":33040},{\"end\":33060,\"start\":33055},{\"end\":33075,\"start\":33066},{\"end\":33086,\"start\":33079},{\"end\":33098,\"start\":33092},{\"end\":33113,\"start\":33104},{\"end\":33598,\"start\":33589},{\"end\":33609,\"start\":33602},{\"end\":33620,\"start\":33613},{\"end\":33997,\"start\":33993},{\"end\":34010,\"start\":34003},{\"end\":34029,\"start\":34014},{\"end\":34036,\"start\":34033},{\"end\":34044,\"start\":34040},{\"end\":34055,\"start\":34048},{\"end\":34069,\"start\":34061},{\"end\":34083,\"start\":34075},{\"end\":34095,\"start\":34089},{\"end\":34109,\"start\":34101},{\"end\":34527,\"start\":34525},{\"end\":34536,\"start\":34531},{\"end\":34542,\"start\":34540},{\"end\":34551,\"start\":34546},{\"end\":34563,\"start\":34559},{\"end\":34570,\"start\":34567},{\"end\":34577,\"start\":34574},{\"end\":34920,\"start\":34912},{\"end\":34930,\"start\":34924},{\"end\":34941,\"start\":34934},{\"end\":34954,\"start\":34945},{\"end\":34964,\"start\":34958},{\"end\":34978,\"start\":34970},{\"end\":34990,\"start\":34982},{\"end\":35003,\"start\":34994},{\"end\":35419,\"start\":35416},{\"end\":35433,\"start\":35425},{\"end\":35442,\"start\":35439},{\"end\":35791,\"start\":35784},{\"end\":35799,\"start\":35795},{\"end\":35808,\"start\":35803},{\"end\":35820,\"start\":35814},{\"end\":35826,\"start\":35824},{\"end\":35843,\"start\":35830},{\"end\":35858,\"start\":35847},{\"end\":35868,\"start\":35862},{\"end\":35878,\"start\":35872},{\"end\":35889,\"start\":35882},{\"end\":36305,\"start\":36301},{\"end\":36318,\"start\":36309},{\"end\":36327,\"start\":36322},{\"end\":36342,\"start\":36331},{\"end\":36356,\"start\":36348},{\"end\":36366,\"start\":36362},{\"end\":36729,\"start\":36722},{\"end\":37067,\"start\":37058},{\"end\":37083,\"start\":37071},{\"end\":37096,\"start\":37089},{\"end\":37373,\"start\":37366},{\"end\":37381,\"start\":37377},{\"end\":37395,\"start\":37387},{\"end\":37408,\"start\":37403},{\"end\":37418,\"start\":37412},{\"end\":37432,\"start\":37422},{\"end\":37450,\"start\":37438},{\"end\":37466,\"start\":37454},{\"end\":37479,\"start\":37472},{\"end\":37855,\"start\":37852},{\"end\":37864,\"start\":37859},{\"end\":37873,\"start\":37868},{\"end\":37880,\"start\":37877},{\"end\":37888,\"start\":37884},{\"end\":38192,\"start\":38185},{\"end\":38204,\"start\":38196},{\"end\":38514,\"start\":38505},{\"end\":38528,\"start\":38518},{\"end\":38825,\"start\":38823},{\"end\":38834,\"start\":38829},{\"end\":38841,\"start\":38838},{\"end\":38852,\"start\":38845},{\"end\":38858,\"start\":38856},{\"end\":38866,\"start\":38862},{\"end\":38880,\"start\":38870},{\"end\":39236,\"start\":39230},{\"end\":39249,\"start\":39240},{\"end\":39263,\"start\":39253},{\"end\":39607,\"start\":39602},{\"end\":39618,\"start\":39613},{\"end\":39923,\"start\":39920},{\"end\":39931,\"start\":39927},{\"end\":39939,\"start\":39935},{\"end\":39951,\"start\":39943},{\"end\":39959,\"start\":39955},{\"end\":39971,\"start\":39963},{\"end\":40323,\"start\":40316},{\"end\":40333,\"start\":40327},{\"end\":40347,\"start\":40340},{\"end\":40611,\"start\":40600},{\"end\":40622,\"start\":40615},{\"end\":40630,\"start\":40626},{\"end\":40887,\"start\":40885},{\"end\":40893,\"start\":40891},{\"end\":40901,\"start\":40897},{\"end\":40910,\"start\":40905},{\"end\":40917,\"start\":40914},{\"end\":40925,\"start\":40921},{\"end\":40934,\"start\":40929},{\"end\":41229,\"start\":41223},{\"end\":41239,\"start\":41233},{\"end\":41250,\"start\":41245},{\"end\":41256,\"start\":41254},{\"end\":41269,\"start\":41262},{\"end\":41279,\"start\":41275},{\"end\":41288,\"start\":41283},{\"end\":41592,\"start\":41586},{\"end\":41607,\"start\":41598},{\"end\":41622,\"start\":41613},{\"end\":41633,\"start\":41626},{\"end\":41642,\"start\":41637},{\"end\":42085,\"start\":42076},{\"end\":42096,\"start\":42089},{\"end\":42106,\"start\":42100},{\"end\":42117,\"start\":42112},{\"end\":42132,\"start\":42123},{\"end\":42140,\"start\":42136},{\"end\":42149,\"start\":42144},{\"end\":42484,\"start\":42473},{\"end\":42493,\"start\":42488},{\"end\":42504,\"start\":42497},{\"end\":42513,\"start\":42508},{\"end\":42523,\"start\":42517},{\"end\":42773,\"start\":42764},{\"end\":42783,\"start\":42777},{\"end\":43046,\"start\":43042},{\"end\":43056,\"start\":43050},{\"end\":43069,\"start\":43060},{\"end\":43264,\"start\":43256},{\"end\":43390,\"start\":43381},{\"end\":43401,\"start\":43394},{\"end\":43410,\"start\":43405},{\"end\":43421,\"start\":43414},{\"end\":43434,\"start\":43425},{\"end\":43443,\"start\":43438},{\"end\":43679,\"start\":43674},{\"end\":43687,\"start\":43683},{\"end\":43695,\"start\":43691},{\"end\":43703,\"start\":43699},{\"end\":43943,\"start\":43936},{\"end\":44160,\"start\":44157},{\"end\":44168,\"start\":44164},{\"end\":44175,\"start\":44172},{\"end\":44185,\"start\":44179},{\"end\":44194,\"start\":44189},{\"end\":44526,\"start\":44523},{\"end\":44532,\"start\":44530},{\"end\":44540,\"start\":44536},{\"end\":44547,\"start\":44544},{\"end\":44856,\"start\":44852},{\"end\":44865,\"start\":44860},{\"end\":44876,\"start\":44869},{\"end\":44884,\"start\":44880},{\"end\":44892,\"start\":44888},{\"end\":44904,\"start\":44898},{\"end\":45267,\"start\":45259},{\"end\":45276,\"start\":45271},{\"end\":45286,\"start\":45280},{\"end\":45296,\"start\":45290},{\"end\":45487,\"start\":45479},{\"end\":45500,\"start\":45491},{\"end\":45508,\"start\":45504},{\"end\":45807,\"start\":45801},{\"end\":45818,\"start\":45811},{\"end\":45826,\"start\":45822},{\"end\":46096,\"start\":46088},{\"end\":46110,\"start\":46100},{\"end\":46116,\"start\":46114},{\"end\":46131,\"start\":46120},{\"end\":46437,\"start\":46434},{\"end\":46449,\"start\":46444},{\"end\":46457,\"start\":46453},{\"end\":46465,\"start\":46463},{\"end\":46915,\"start\":46910},{\"end\":46927,\"start\":46921},{\"end\":46939,\"start\":46933},{\"end\":47203,\"start\":47193},{\"end\":47216,\"start\":47207},{\"end\":47228,\"start\":47222},{\"end\":47451,\"start\":47444},{\"end\":47464,\"start\":47455},{\"end\":47473,\"start\":47468},{\"end\":47483,\"start\":47477},{\"end\":47492,\"start\":47487},{\"end\":47731,\"start\":47723},{\"end\":47744,\"start\":47735},{\"end\":47941,\"start\":47933},{\"end\":47951,\"start\":47945},{\"end\":47959,\"start\":47955},{\"end\":47969,\"start\":47966},{\"end\":48213,\"start\":48204},{\"end\":48225,\"start\":48217},{\"end\":48232,\"start\":48229},{\"end\":48244,\"start\":48236},{\"end\":48254,\"start\":48248},{\"end\":48263,\"start\":48258}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8298120},\"end\":31668,\"start\":31238},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":19755043},\"end\":32000,\"start\":31670},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206594692},\"end\":32196,\"start\":32002},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6832420},\"end\":32612,\"start\":32198},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":18852982},\"end\":32872,\"start\":32614},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":82783636},\"end\":33498,\"start\":32874},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":54089234},\"end\":33856,\"start\":33500},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14550161},\"end\":34451,\"start\":33858},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":29153456},\"end\":34771,\"start\":34453},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14521078},\"end\":35308,\"start\":34773},{\"attributes\":{\"id\":\"b10\"},\"end\":35655,\"start\":35310},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":26657811},\"end\":36207,\"start\":35657},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7515294},\"end\":36631,\"start\":36209},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":73373285},\"end\":36946,\"start\":36633},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5501470},\"end\":37309,\"start\":36948},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2088679},\"end\":37750,\"start\":37311},{\"attributes\":{\"id\":\"b16\"},\"end\":38079,\"start\":37752},{\"attributes\":{\"id\":\"b17\"},\"end\":38399,\"start\":38081},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":27391290},\"end\":38724,\"start\":38401},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2612052},\"end\":39101,\"start\":38726},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3776155},\"end\":39517,\"start\":39103},{\"attributes\":{\"id\":\"b21\"},\"end\":39809,\"start\":39519},{\"attributes\":{\"id\":\"b22\"},\"end\":40250,\"start\":39811},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17779853},\"end\":40531,\"start\":40252},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3719281},\"end\":40781,\"start\":40533},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10875300},\"end\":41143,\"start\":40783},{\"attributes\":{\"id\":\"b26\"},\"end\":41514,\"start\":41145},{\"attributes\":{\"id\":\"b27\"},\"end\":41997,\"start\":41516},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3815411},\"end\":42421,\"start\":41999},{\"attributes\":{\"id\":\"b29\"},\"end\":42658,\"start\":42423},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2617020},\"end\":42950,\"start\":42660},{\"attributes\":{\"id\":\"b31\"},\"end\":43240,\"start\":42952},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206770307},\"end\":43328,\"start\":43242},{\"attributes\":{\"id\":\"b33\"},\"end\":43589,\"start\":43330},{\"attributes\":{\"id\":\"b34\"},\"end\":43868,\"start\":43591},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":18522085},\"end\":44054,\"start\":43870},{\"attributes\":{\"doi\":\"arXiv:1911.03588\",\"id\":\"b36\"},\"end\":44415,\"start\":44056},{\"attributes\":{\"doi\":\"arXiv:1904.09482\",\"id\":\"b37\"},\"end\":44761,\"start\":44417},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5034059},\"end\":45199,\"start\":44763},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":362467},\"end\":45440,\"start\":45201},{\"attributes\":{\"id\":\"b40\"},\"end\":45751,\"start\":45442},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b41\"},\"end\":45980,\"start\":45753},{\"attributes\":{\"doi\":\"arXiv:1902.03393\",\"id\":\"b42\"},\"end\":46364,\"start\":45982},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":207853355},\"end\":46842,\"start\":46366},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":2264183},\"end\":47124,\"start\":46844},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":195908774},\"end\":47381,\"start\":47126},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":206593880},\"end\":47651,\"start\":47383},{\"attributes\":{\"id\":\"b47\"},\"end\":47865,\"start\":47653},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":11080756},\"end\":48116,\"start\":47867},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":15019293},\"end\":48462,\"start\":48118}]", "bib_title": "[{\"end\":31354,\"start\":31238},{\"end\":31733,\"start\":31670},{\"end\":32046,\"start\":32002},{\"end\":32247,\"start\":32198},{\"end\":32673,\"start\":32614},{\"end\":33025,\"start\":32874},{\"end\":33585,\"start\":33500},{\"end\":33989,\"start\":33858},{\"end\":34521,\"start\":34453},{\"end\":34908,\"start\":34773},{\"end\":35410,\"start\":35310},{\"end\":35780,\"start\":35657},{\"end\":36295,\"start\":36209},{\"end\":36718,\"start\":36633},{\"end\":37054,\"start\":36948},{\"end\":37362,\"start\":37311},{\"end\":38181,\"start\":38081},{\"end\":38501,\"start\":38401},{\"end\":38819,\"start\":38726},{\"end\":39226,\"start\":39103},{\"end\":39596,\"start\":39519},{\"end\":39916,\"start\":39811},{\"end\":40312,\"start\":40252},{\"end\":40596,\"start\":40533},{\"end\":40881,\"start\":40783},{\"end\":41219,\"start\":41145},{\"end\":41580,\"start\":41516},{\"end\":42072,\"start\":41999},{\"end\":42760,\"start\":42660},{\"end\":43038,\"start\":42952},{\"end\":43252,\"start\":43242},{\"end\":43670,\"start\":43591},{\"end\":43932,\"start\":43870},{\"end\":44848,\"start\":44763},{\"end\":45255,\"start\":45201},{\"end\":45475,\"start\":45442},{\"end\":46430,\"start\":46366},{\"end\":46904,\"start\":46844},{\"end\":47189,\"start\":47126},{\"end\":47440,\"start\":47383},{\"end\":47929,\"start\":47867},{\"end\":48198,\"start\":48118}]", "bib_author": "[{\"end\":31367,\"start\":31356},{\"end\":31373,\"start\":31367},{\"end\":31383,\"start\":31373},{\"end\":31396,\"start\":31383},{\"end\":31404,\"start\":31396},{\"end\":31416,\"start\":31404},{\"end\":31746,\"start\":31735},{\"end\":31760,\"start\":31746},{\"end\":31775,\"start\":31760},{\"end\":32054,\"start\":32048},{\"end\":32063,\"start\":32054},{\"end\":32070,\"start\":32063},{\"end\":32077,\"start\":32070},{\"end\":32258,\"start\":32249},{\"end\":32269,\"start\":32258},{\"end\":32278,\"start\":32269},{\"end\":32684,\"start\":32675},{\"end\":32693,\"start\":32684},{\"end\":32709,\"start\":32693},{\"end\":32719,\"start\":32709},{\"end\":33036,\"start\":33027},{\"end\":33051,\"start\":33036},{\"end\":33062,\"start\":33051},{\"end\":33077,\"start\":33062},{\"end\":33088,\"start\":33077},{\"end\":33100,\"start\":33088},{\"end\":33115,\"start\":33100},{\"end\":33600,\"start\":33587},{\"end\":33611,\"start\":33600},{\"end\":33622,\"start\":33611},{\"end\":33999,\"start\":33991},{\"end\":34012,\"start\":33999},{\"end\":34031,\"start\":34012},{\"end\":34038,\"start\":34031},{\"end\":34046,\"start\":34038},{\"end\":34057,\"start\":34046},{\"end\":34071,\"start\":34057},{\"end\":34085,\"start\":34071},{\"end\":34097,\"start\":34085},{\"end\":34111,\"start\":34097},{\"end\":34529,\"start\":34523},{\"end\":34538,\"start\":34529},{\"end\":34544,\"start\":34538},{\"end\":34553,\"start\":34544},{\"end\":34565,\"start\":34553},{\"end\":34572,\"start\":34565},{\"end\":34579,\"start\":34572},{\"end\":34922,\"start\":34910},{\"end\":34932,\"start\":34922},{\"end\":34943,\"start\":34932},{\"end\":34956,\"start\":34943},{\"end\":34966,\"start\":34956},{\"end\":34980,\"start\":34966},{\"end\":34992,\"start\":34980},{\"end\":35005,\"start\":34992},{\"end\":35421,\"start\":35412},{\"end\":35435,\"start\":35421},{\"end\":35444,\"start\":35435},{\"end\":35793,\"start\":35782},{\"end\":35801,\"start\":35793},{\"end\":35810,\"start\":35801},{\"end\":35822,\"start\":35810},{\"end\":35828,\"start\":35822},{\"end\":35845,\"start\":35828},{\"end\":35860,\"start\":35845},{\"end\":35870,\"start\":35860},{\"end\":35880,\"start\":35870},{\"end\":35891,\"start\":35880},{\"end\":36307,\"start\":36297},{\"end\":36320,\"start\":36307},{\"end\":36329,\"start\":36320},{\"end\":36344,\"start\":36329},{\"end\":36358,\"start\":36344},{\"end\":36368,\"start\":36358},{\"end\":36374,\"start\":36368},{\"end\":36731,\"start\":36720},{\"end\":37069,\"start\":37056},{\"end\":37085,\"start\":37069},{\"end\":37098,\"start\":37085},{\"end\":37375,\"start\":37364},{\"end\":37383,\"start\":37375},{\"end\":37397,\"start\":37383},{\"end\":37410,\"start\":37397},{\"end\":37420,\"start\":37410},{\"end\":37434,\"start\":37420},{\"end\":37452,\"start\":37434},{\"end\":37468,\"start\":37452},{\"end\":37481,\"start\":37468},{\"end\":37857,\"start\":37850},{\"end\":37866,\"start\":37857},{\"end\":37875,\"start\":37866},{\"end\":37882,\"start\":37875},{\"end\":37890,\"start\":37882},{\"end\":38194,\"start\":38183},{\"end\":38206,\"start\":38194},{\"end\":38516,\"start\":38503},{\"end\":38530,\"start\":38516},{\"end\":38827,\"start\":38821},{\"end\":38836,\"start\":38827},{\"end\":38843,\"start\":38836},{\"end\":38854,\"start\":38843},{\"end\":38860,\"start\":38854},{\"end\":38868,\"start\":38860},{\"end\":38882,\"start\":38868},{\"end\":39238,\"start\":39228},{\"end\":39251,\"start\":39238},{\"end\":39265,\"start\":39251},{\"end\":39609,\"start\":39598},{\"end\":39620,\"start\":39609},{\"end\":39925,\"start\":39918},{\"end\":39933,\"start\":39925},{\"end\":39941,\"start\":39933},{\"end\":39953,\"start\":39941},{\"end\":39961,\"start\":39953},{\"end\":39973,\"start\":39961},{\"end\":40325,\"start\":40314},{\"end\":40335,\"start\":40325},{\"end\":40349,\"start\":40335},{\"end\":40613,\"start\":40598},{\"end\":40624,\"start\":40613},{\"end\":40632,\"start\":40624},{\"end\":40889,\"start\":40883},{\"end\":40895,\"start\":40889},{\"end\":40903,\"start\":40895},{\"end\":40912,\"start\":40903},{\"end\":40919,\"start\":40912},{\"end\":40927,\"start\":40919},{\"end\":40936,\"start\":40927},{\"end\":41231,\"start\":41221},{\"end\":41241,\"start\":41231},{\"end\":41252,\"start\":41241},{\"end\":41258,\"start\":41252},{\"end\":41271,\"start\":41258},{\"end\":41281,\"start\":41271},{\"end\":41290,\"start\":41281},{\"end\":41594,\"start\":41582},{\"end\":41609,\"start\":41594},{\"end\":41624,\"start\":41609},{\"end\":41635,\"start\":41624},{\"end\":41644,\"start\":41635},{\"end\":42087,\"start\":42074},{\"end\":42098,\"start\":42087},{\"end\":42108,\"start\":42098},{\"end\":42119,\"start\":42108},{\"end\":42134,\"start\":42119},{\"end\":42142,\"start\":42134},{\"end\":42151,\"start\":42142},{\"end\":42486,\"start\":42471},{\"end\":42495,\"start\":42486},{\"end\":42506,\"start\":42495},{\"end\":42515,\"start\":42506},{\"end\":42525,\"start\":42515},{\"end\":42775,\"start\":42762},{\"end\":42785,\"start\":42775},{\"end\":43048,\"start\":43040},{\"end\":43058,\"start\":43048},{\"end\":43071,\"start\":43058},{\"end\":43266,\"start\":43254},{\"end\":43392,\"start\":43379},{\"end\":43403,\"start\":43392},{\"end\":43412,\"start\":43403},{\"end\":43423,\"start\":43412},{\"end\":43436,\"start\":43423},{\"end\":43445,\"start\":43436},{\"end\":43681,\"start\":43672},{\"end\":43689,\"start\":43681},{\"end\":43697,\"start\":43689},{\"end\":43705,\"start\":43697},{\"end\":43945,\"start\":43934},{\"end\":44162,\"start\":44155},{\"end\":44170,\"start\":44162},{\"end\":44177,\"start\":44170},{\"end\":44187,\"start\":44177},{\"end\":44196,\"start\":44187},{\"end\":44528,\"start\":44521},{\"end\":44534,\"start\":44528},{\"end\":44542,\"start\":44534},{\"end\":44549,\"start\":44542},{\"end\":44858,\"start\":44850},{\"end\":44867,\"start\":44858},{\"end\":44878,\"start\":44867},{\"end\":44886,\"start\":44878},{\"end\":44894,\"start\":44886},{\"end\":44906,\"start\":44894},{\"end\":45269,\"start\":45257},{\"end\":45278,\"start\":45269},{\"end\":45288,\"start\":45278},{\"end\":45298,\"start\":45288},{\"end\":45489,\"start\":45477},{\"end\":45502,\"start\":45489},{\"end\":45510,\"start\":45502},{\"end\":45809,\"start\":45799},{\"end\":45820,\"start\":45809},{\"end\":45828,\"start\":45820},{\"end\":46098,\"start\":46083},{\"end\":46112,\"start\":46098},{\"end\":46118,\"start\":46112},{\"end\":46133,\"start\":46118},{\"end\":46439,\"start\":46432},{\"end\":46451,\"start\":46439},{\"end\":46459,\"start\":46451},{\"end\":46467,\"start\":46459},{\"end\":46917,\"start\":46906},{\"end\":46929,\"start\":46917},{\"end\":46941,\"start\":46929},{\"end\":47205,\"start\":47191},{\"end\":47218,\"start\":47205},{\"end\":47230,\"start\":47218},{\"end\":47453,\"start\":47442},{\"end\":47466,\"start\":47453},{\"end\":47475,\"start\":47466},{\"end\":47485,\"start\":47475},{\"end\":47494,\"start\":47485},{\"end\":47733,\"start\":47721},{\"end\":47746,\"start\":47733},{\"end\":47943,\"start\":47931},{\"end\":47953,\"start\":47943},{\"end\":47961,\"start\":47953},{\"end\":47971,\"start\":47961},{\"end\":48215,\"start\":48200},{\"end\":48227,\"start\":48215},{\"end\":48234,\"start\":48227},{\"end\":48246,\"start\":48234},{\"end\":48256,\"start\":48246},{\"end\":48265,\"start\":48256}]", "bib_venue": "[{\"end\":31421,\"start\":31416},{\"end\":31808,\"start\":31775},{\"end\":32081,\"start\":32077},{\"end\":32355,\"start\":32278},{\"end\":32724,\"start\":32719},{\"end\":33152,\"start\":33115},{\"end\":33659,\"start\":33622},{\"end\":34119,\"start\":34111},{\"end\":34582,\"start\":34579},{\"end\":35014,\"start\":35005},{\"end\":35457,\"start\":35444},{\"end\":35895,\"start\":35891},{\"end\":36382,\"start\":36374},{\"end\":36755,\"start\":36731},{\"end\":37101,\"start\":37098},{\"end\":37503,\"start\":37481},{\"end\":37848,\"start\":37752},{\"end\":38215,\"start\":38206},{\"end\":38550,\"start\":38530},{\"end\":38885,\"start\":38882},{\"end\":39287,\"start\":39265},{\"end\":39638,\"start\":39620},{\"end\":40003,\"start\":39973},{\"end\":40365,\"start\":40349},{\"end\":40638,\"start\":40632},{\"end\":40942,\"start\":40936},{\"end\":41296,\"start\":41290},{\"end\":41737,\"start\":41644},{\"end\":42181,\"start\":42151},{\"end\":42469,\"start\":42423},{\"end\":42789,\"start\":42785},{\"end\":43077,\"start\":43071},{\"end\":43270,\"start\":43266},{\"end\":43377,\"start\":43330},{\"end\":43711,\"start\":43705},{\"end\":43949,\"start\":43945},{\"end\":44153,\"start\":44056},{\"end\":44519,\"start\":44417},{\"end\":44962,\"start\":44906},{\"end\":45302,\"start\":45298},{\"end\":45540,\"start\":45510},{\"end\":45797,\"start\":45753},{\"end\":46081,\"start\":45982},{\"end\":46548,\"start\":46467},{\"end\":46954,\"start\":46941},{\"end\":47234,\"start\":47230},{\"end\":47498,\"start\":47494},{\"end\":47719,\"start\":47653},{\"end\":47974,\"start\":47971},{\"end\":48269,\"start\":48265},{\"end\":32419,\"start\":32357},{\"end\":46616,\"start\":46550}]"}}}, "year": 2023, "month": 12, "day": 17}