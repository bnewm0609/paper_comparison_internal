{"id": 37961921, "updated": "2023-10-01 09:25:28.292", "metadata": {"title": "An Unsupervised Learning Model for Deformable Medical Image Registration", "authors": "[{\"first\":\"Guha\",\"last\":\"Balakrishnan\",\"middle\":[]},{\"first\":\"Amy\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Mert\",\"last\":\"Sabuncu\",\"middle\":[\"R.\"]},{\"first\":\"John\",\"last\":\"Guttag\",\"middle\":[]},{\"first\":\"Adrian\",\"last\":\"Dalca\",\"middle\":[\"V.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "We present a fast learning-based algorithm for deformable, pairwise 3D medical image registration. Current registration methods optimize an objective function independently for each pair of images, which can be time-consuming for large data. We define registration as a parametric function, and optimize its parameters given a set of images from a collection of interest. Given a new pair of scans, we can quickly compute a registration field by directly evaluating the function using the learned parameters. We model this function using a convolutional neural network (CNN), and use a spatial transform layer to reconstruct one image from another while imposing smoothness constraints on the registration field. The proposed method does not require supervised information such as ground truth registration fields or anatomical landmarks. We demonstrate registration accuracy comparable to state-of-the-art 3D image registration, while operating orders of magnitude faster in practice. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is available at https://github.com/balakg/voxelmorph .", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1802.02604", "mag": "3098269293", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/BalakrishnanZSG18", "doi": "10.1109/cvpr.2018.00964"}}, "content": {"source": {"pdf_hash": "28eaf1ebe3689eba6841cecbaa73a09fc136fab8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1802.02604v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1802.02604", "status": "GREEN"}}, "grobid": {"id": "9f2c2a9a98cc5d5f95a777d74cb96940181c47f7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/28eaf1ebe3689eba6841cecbaa73a09fc136fab8.txt", "contents": "\nAn Unsupervised Learning Model for Deformable Medical Image Registration\n\n\nGuha Balakrishnan \nCornell University\n\n\nMit \nCornell University\n\n\nAmy Zhao xamyzhao@mit.edu \nCornell University\n\n\nMit \nCornell University\n\n\nMert R Sabuncu msabuncu@cornell.edu \nCornell University\n\n\nMITJohn Guttag guttag@mit.edu \nCornell University\n\n\nMITAdrian V Dalca adalca@mit.edu \nCornell University\n\n\nMgh \nCornell University\n\n\nAn Unsupervised Learning Model for Deformable Medical Image Registration\n\nWe present a fast learning-based algorithm for deformable, pairwise 3D medical image registration. Current registration methods optimize an objective function independently for each pair of images, which can be timeconsuming for large data. We define registration as a parametric function, and optimize its parameters given a set of images from a collection of interest. Given a new pair of scans, we can quickly compute a registration field by directly evaluating the function using the learned parameters. We model this function using a convolutional neural network (CNN), and use a spatial transform layer to reconstruct one image from another while imposing smoothness constraints on the registration field. The proposed method does not require supervised information such as ground truth registration fields or anatomical landmarks. We demonstrate registration accuracy comparable to stateof-the-art 3D image registration, while operating orders of magnitude faster in practice. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learningbased registration and its applications. Our code is available at https://github.com/balakg/voxelmorph.\n\nIntroduction\n\nDeformable registration is a fundamental task in a variety of medical imaging studies, and has been a topic of active research for decades. In deformable registration, a dense, non-linear correspondence is established between a pair of n-D image volumes, such as 3D MR brain scans, depicting similar structures. Most registration methods solve an optimization problem for each volume pair that aligns voxels with similar appearance while enforcing smoothness constraints on the registration mapping. Solving this optimization is computationally intensive, and therefore extremely slow in practice.\n\nIn contrast, we propose a novel registration method that learns a parametrized registration function from a collection of volumes. We implement the function using a convolutional neural network (CNN), that takes two n-D input volumes and outputs a mapping of all voxels of one volume to another volume. The parameters of the network, i.e., the convolutional kernel weights, are optimized using a training set of volume pairs from the dataset of interest. By sharing the same parameters for a collection of volumes, the procedure learns a common representation which can align any new pair of volumes from the same distribution. In essence, we replace a costly optimization of traditional registration algorithms for each test image pair with one global function optimization during a training phase. Registration between a new test scan pair is achieved by simply evaluating the learned function on the given volumes, resulting in rapid registration.\n\nThe novelty of this work is that:\n\n\u2022 we present a learning-based solution requiring no supervised information such as ground truth correspondences or anatomical landmarks during training,\n\n\u2022 we propose a CNN function with parameters shared across a population, enabling registration to be achieved through a function evaluation, and\n\n\u2022 our method enables parameter optimization for a variety of cost functions, which can be adapted to various tasks.\n\nThroughout this paper, we use the example of registering 3D MR brain scans. However, our method is broadly applicable to registration tasks, both within and beyond the medical imaging domain. We evaluate our method on a multi-study dataset of over 7,000 scans containing images of healthy and diseased brains from a variety of age groups. Results show that our method achieves comparable accuracy to a state-of-the-art registration package, while taking orders of magnitude less time. Scans that used to take two hours to register can now be registered within one or two minutes using a CPU, and under a second with a GPU. This is of significant practical importance for many medical image analysis tasks.\n\n\nBackground\n\nIn the typical volume registration formulation, one (moving or source) volume is warped to align with a second (fixed or target) volume. Deformable registration strategies separate an initial affine transformation for global alignment from a typically much slower deformable transformation with higher degrees of freedom. We concentrate on the latter step, in which we compute a dense, nonlinear correspondence for all voxels. Fig. 1 shows sample 2D coronal slices taken from 3D MRI volumes, with boundaries of several anatomical structures outlined. There is significant variability across subjects, caused by differences in health state and natural anatomical variations in healthy brains. Deformable registration enables comparison of structures across scans and population analyses. Such analyses are useful for understanding variability across populations or the evolution of brain anatomy over time for individuals with disease.\n\nMost existing registration algorithms iteratively optimize a transformation based on an energy function. Let F, M denote the fixed and moving images, respectively, and let \u03c6 be the registration field. The optimization problem is typically written as:\u03c6\n= arg min \u03c6 L(F, M, \u03c6),(1)\nwhere\nL(F, M, \u03c6) = L sim (F, M (\u03c6)) + \u03bbL smooth (\u03c6),(2)\nM (\u03c6) is M warped by \u03c6, function L sim (\u00b7, \u00b7) measures image similarity between M (\u03c6) and F , L smooth (\u00b7) imposes regularization on \u03c6, and \u03bb is the regularization parameter. There are several common formulations for \u03c6, L sim and L smooth . Often, \u03c6 is a displacement vector field, specifying the vector offset from F to M for each voxel. Diffeomorphic transforms are a popular alternative that model \u03c6 as the integral of a velocity vector field. As a result, they are able to preserve topology and enforce invertibility on \u03c6. Common metrics used for L sim include mean squared voxel difference, mutual information, and cross-correlation. The latter two are particularly useful when volumes have varying intensity distributions and contrasts. L smooth enforces  : Example coronal slices from the 3D MRI brain dataset, after affine alignment. Each column is a different scan (subject) and each row is a different coronal slice. Several significant anatomical regions are outlined using different colors: L/R white matter in light/dark blue, L/R ventricles in yellow/red, and L/R hippocampi in purple/green. There are significant structural differences across scans, necessitating a deformable registration step to analyze interscan variations.\n\na spatially smooth deformation, often modeled as a linear operator on spatial gradients of \u03c6. In our work, we optimize function parameters to minimize the expected energy of the form of (1) using a dataset of volume pairs, instead of doing it for each pair independently.\n\n\nRelated Work\n\n\nMedical Image Registration (Non-learningbased)\n\nThere is extensive work in 3D medical image registration [2,4,6,7,13,18,42]. 1 Several studies optimize within the space of displacement vector fields. These include elastic-type models [6,38], statistical parametric mapping [3], free-form deformations with b-splines, [37] and Demons [42]. Our model also assumes displacement vector fields. Diffeomorphic transforms, which are topology-preserving, have shown remarkable success in various computational anatomy studies. Popular formulations include Large Diffeomorphic Distance Metric Mapping (LDDMM) [7], DARTEL [2] and standard symmetric normalization (SyN) [4].\n\n\nMedical Image Registration (Learning-based)\n\nThere are several recent papers proposing neural networks to learn a function for medical image registration. Most of these rely on ground truth warp fields or segmentations [26,35,39,45], a significant drawback compared to our method, which does not require either. Two recent works [14,27] present unsupervised methods that are closer to our approach. Both propose a neural network consisting of a CNN and spatial transformation function [23] that warps images to one another. Unfortunately, these methods are preliminary and have significant drawbacks: they are only demonstrated on limited subsets of volumes, such as 3D subregions or 2D slices, and support only small transformations. Others [14] employ regularization only implicitly determined by interpolation methods. In contrast, our generalizable method is applicable to entire 3D volumes, handles large deformations, and enables any differentiable cost function. We present a rigorous analysis of our method, and demonstrate results on full MR volumes.\n\n\n2D Image Alignment\n\nOptical flow estimation is an analogous problem to 3D volume registration for 2D images. Optical flow algorithms return a dense displacement vector field depicting small displacements between a 2D image pair. Traditional optical flow approaches typically solve an optimization problem similar to (1) using variational methods [8,21,41]. Extensions that better handle large displacements or dramatic changes in appearance include feature-based matching [9,28] and nearest neighbor fields [10].\n\nSeveral learning-based approaches to dense 2D image alignment have been proposed. One study learns a lowdimensional basis for optical flow in natural images using PCA [44]. Other recent studies in optical flow learn a parametric function using convolutional neural networks [16,43]. Unfortunately, these methods require ground truth registrations during training. The spatial transform layer enables neural networks to perform global parametric 2D image alignment without requiring supervised labels [23]. The layer has since been used for dense spatial transformations as well [34,46]. We extend the spatial transformer to the 3D setting in our work.\n\n\nMethod\n\nLet F, M be two image volumes defined over a n-D spatial domain \u2126 \u2282 R n . For the rest of this paper, we focus on the case n = 3. For simplicity we assume that F and M contain single-channel, grayscale data. We also assume that F and M are affinely aligned as a preprocessing step, so that the only source of misalignment between the volumes is nonlinear. Many packages are available for rapid affine alignment.\n\nWe model a function g \u03b8 (F, M ) = \u03c6 using a convolutional neural network (CNN), where \u03c6 is a registration field and \u03b8 are learnable parameters of g. For each voxel p \u2208 \u2126, \u03c6(p) is a location such that F (p) and M (\u03c6(p)) define similar anatomical locations. Fig. 2 presents an overview of our method. Our network takes M and F as input, and computes \u03c6 based on a set of parameters \u03b8, the kernels of the convolutional layers. We warp M (p) to M (\u03c6(p)) using a spatial transformation function, enabling the model to evaluate the similarity of M (\u03c6) and F and update \u03b8.\n\nWe use stochastic gradient descent to find optimal parameters\u03b8 by minimizing an expected loss function L(\u00b7, \u00b7, \u00b7), similar to (2), using a training dataset: (3) where D is the dataset distribution. We learn\u03b8 by aligning volume pairs sampled from D. Importantly, we do not require supervised information such as ground truth registration fields or anatomical landmarks. Given an unseen M and F during test time, we obtain a registration field by evaluating g. We describe our model, which we call Voxel-Morph, in the next sections.\n\u03b8 = arg min \u03b8 E (F,M )\u223cD [L (F, M, g \u03b8 (F, M ))] ,\n\nVoxelMorph CNN Architecture\n\nThe parametrization of g is based on a convolutional neural network architecture similar to UNet [22,36]. The network consists of an encoder-decoder with skip connections that is responsible for generating \u03c6 given M and F . Fig. 3 depicts two variants of the proposed architectures that tradeoff between registration accuracy and computation time. Both take a single input formed by concatenating M and F into a 2-channel 3D image. In our experiments, the input is of size 160 \u00d7 192 \u00d7 224 \u00d7 2. We apply 3D convolutions followed by Leaky ReLU activations in both the encoder and decoder stages, with a convolutional kernel size of 3 \u00d7 3 \u00d7 3. The convolutional layers capture hierarchical features of the input image pair necessary to estimate the correspondence \u03c6. In the encoder, we use strided convolutions to reduce the spatial dimensions in half until the smallest layer is reached. Successive layers of the encoder operate over coarser representations of the input, similar to the image pyramid used in traditional image registration work.\n\nThe receptive fields of the convolutional kernels of the smallest layer should be at least as large as the maximum expected displacement between corresponding voxels in M and F . The smallest layer applies convolutions over a volume (1/16) 3 the size of the input images. In the decoding stage, we alternate between upsampling, convolutions (followed by Leaky ReLU activations) and concatenating skip Moving 3D Image ( )\nMoved ( ( )) Registration Field ( ) & ,\n\nLoss Function (\u2112)\n\nFixed 3D Image ( ) \u2026 Spatial Transform  Figure 3: Proposed convolutional architectures implementing g \u03b8 (F, M ). Each rectangle represents a 3D volume. The number of channels is shown inside the rectangle, and the spatial resolution with respect to the input volume is printed underneath. VoxelMorph-2 uses a larger architecture, using one extra convolutional layer at the output resolution, and more channels for later layers.\n\nconnections. Skip connections propagate features learned during the encoding stages directly to layers generating the registration. The output of the decoder, \u03c6, is of size 160 \u00d7 192 \u00d7 224 \u00d7 3 in our experiments.\n\nSuccessive layers of the decoder operate on finer spatial scales, enabling precise anatomical alignment. However, these convolutions are applied to the largest image volumes, which is computationally expensive. We explore this tradeoff using two architectures, VoxelMorph-1 and VoxelMorph-2, that differ in size at the end of the decoder (see Fig. 3). VoxelMorph-1 uses one less layer at the final resolution and fewer channels over its last three layers.\n\n\nSpatial Transformation Function\n\nThe proposed method learns optimal parameter values in part by minimizing differences between M (\u03c6) and F . In order to use standard gradient-based methods, we construct a differentiable operation based on spatial transformer networks to compute M (\u03c6) [23].\n\nFor each voxel p, we compute a (subpixel) voxel location \u03c6(p) in M . Because image values are only defined at integer locations, we linearly interpolate the values at the eight neighboring voxels. That is, we perform:\nM (\u03c6(p)) = q\u2208Z(\u03c6(p)) M (q) d\u2208{x,y,z} (1 \u2212 |\u03c6 d (p) \u2212 q d |),(4)\nwhere Z(\u03c6(p)) are the voxel neighbors of \u03c6(p). Because the operations are differentiable almost everywhere, we can backpropagate errors during optimization.\n\n\nLoss Function\n\nThe proposed method works with any differentiable loss. In this section, we formulate an example of a popular loss function L of the form (2), consisting of two components: L sim that penalizes differences in appearance, and L smooth that penalizes local spatial variations in \u03c6. In our experiments, we set L sim to the negative local cross-correlation of M (\u03c6) and F , a popular metric that is robust to intensity variations often found across scans and datasets.\n\nLetF (p) andM (\u03c6(p)) denote images with local mean intensities subtracted out. We compute local means over a n 3 volume, with n = 9 in our experiments. We write the local cross-correlation of F and M (\u03c6), as:\nCC(F, M (\u03c6)) = p\u2208\u2126 p i (F (p i ) \u2212F (p))(M (\u03c6(p i )) \u2212M (\u03c6(p))) 2 p i (F (p i ) \u2212F (p)) p i (M (\u03c6(p i )) \u2212M (\u03c6(p)))\n, (5) where p i iterates over a n 3 volume around p. A higher CC indicates a better alignment, yielding the loss function: L sim (F, M, \u03c6) = \u2212CC(F, M (\u03c6)). We compute CC efficiently using only convolutional operations over M (\u03c6) and F .\n\nMinimizing L sim will encourage M (\u03c6) to approximate F , but may generate a discontinuous \u03c6. We encourage a smooth \u03c6 using a diffusion regularizer on its spatial gradients:\nL smooth (\u03c6) = p\u2208\u2126 \u2207\u03c6(p) 2 .(6)\nWe approximate spatial gradients using differences between neighboring voxels. The complete loss is therefore:\nL(F, M, \u03c6) = \u2212CC(F, M (\u03c6)) + \u03bb p\u2208\u2126 \u2207\u03c6(p) 2 ,(7)\nwhere \u03bb is a regularization parameter.\n\n\nExperiments\n\n\nDataset\n\nWe demonstrate our method on the task of brain MRI registration. We use a large-scale, multi-site, multi-study dataset of 7829 T1weighted brain MRI scans from eight publicly available datasets: ADNI [33], OASIS [29], ABIDE [31], ADHD200 [32], MCIC [19], PPMI [30], HABS [12], and Harvard GSP [20]. Acquisition details, subject age ranges and health conditions are different for each dataset. All scans were resampled to a 256\u00d7256\u00d7256 grid with 1mm isotropic voxels. We carry out standard preprocessing steps, including affine spatial normalization and brain extraction for each scan using FreeSurfer [17], and crop the resulting images to 160 \u00d7 192 \u00d7 224. All MRIs were also anatomically segmented with FreeSurfer, and we applied quality control (QC) using visual inspection to catch gross errors in segmentation results. We use the resulting segmentation maps in evaluating our registration as described below. We split our dataset into 7329, 250, and 250 volumes for train, validation, and test sets respectively, although we highlight that we do not use any supervised information at any stage.\n\nWe focus on atlas-based registration, in which we compute a registration field between an atlas, or reference volume, and each volume in our dataset. Atlas-based registration is a common formulation in population analysis, where inter-subject registration is a core problem. The atlas represents a reference, or average volume, and is usually constructed by jointly and repeatedly aligning a dataset of brain MR volumes and averaging them together. We use an atlas computed using an external dataset [17,40]. Each input volume pair consists of the atlas (image F ) and a random volume from the dataset (image M ). Columns 1-2 of Fig. 4 show example image pairs from the dataset using the same fixed atlas for all examples. All figures that depict brains in this paper show 2D coronal slices for visualization purposes only. All registration is done in 3D.\n\n\nDice Score\n\nObtaining dense ground truth registration for these data is not well-defined since many registration fields can yield similar looking warped images. We evaluate our method using volume overlap of anatomical segmentations. We include any anatomical structures that are at least 100 voxels in volume for all test subjects, resulting in 29 structures. If a registration field \u03c6 represents accurate anatomical correspondences, we expect the regions in F and M (\u03c6) corresponding to the same anatomical structure to overlap well (see Fig. 4 for examples). Let S k F , S k M (\u03c6) be the set of voxels of structure k for F and M (\u03c6), respectively. We measure the accuracy of our method using the Dice score [15], which quantifies the volume overlap between two structures:\nDice(S k M (\u03c6) , S k F ) = 2 * S k M (\u03c6) \u2229 S k F |S k M (\u03c6) | + |S k F | .(8)\nA Dice score of 1 indicates that the structures are identical, and a score of 0 indicates that there is no overlap.\n\n\nBaseline Methods\n\nWe compare our approach to Symmetric Normalization (SyN) [4], the top-performing registration algorithm in a comparative study [25]. We use the SyN implementation in the publicly available ANTs software package [5], with a cross-correlation similarity measure. Throughout our work VoxelMorph-2 VoxelMorph-1 Figure 4: Example MR coronal slices extracted from input pairs (columns 1-2), and resulting M (\u03c6) for VoxelMorph-1 and VoxelMorph-2, with overlaid boundaries of the ventricles (yellow, orange) and hippocampi (red, green). A good registration will cause structures in M (\u03c6) to look similar to structures in F . Our networks handle large changes in shapes, such as the ventricles in row 2 and the left hippocampi in rows 3-4. with medical images, we found the default ANTs smoothness parameters to be sub-optimal for our purposes. We obtained improved parameters using a wide parameter sweep across a multiple of datasets, and use those in these experiments.\n\n\nImplementation\n\nWe implement our networks using Keras [11] with a Tensorflow backend [1]. We use the ADAM optimizer [24] with a learning rate of 1e \u22124 . To reduce memory usage, each training batch consists of one pair of volumes. We train separate networks with different \u03bb values until convergence. We select the network that optimizes Dice score on our validation set, and report results on our held-out test set. Our code and model parameters are available online at https://github.com/balakg/voxelmorph.  Table 1 shows average Dice scores over all subjects and structures for ANTs, the proposed VoxelMorph architectures, and a baseline of only global affine alignment. VoxelMorph models perform comparably to ANTs, and VoxelMorph-2 performs slightly better than VoxelMorph-1. All three improve significantly on affine alignment. We visualize the distribution of Dice scores for each structure as boxplots in Fig. 5. For visualization purposes, we combine same structures from the two hemispheres, such as the left and right white matter. The VoxelMorph models achieve comparable Dice measures to ANTs for all structures, performing slightly better than ANTs on some structures such as cerebral white matter, and worse on others such as the hippocampi. Table 1 presents runtime results using an Intel Xeon (E5-2680) CPU, and a NVIDIA TitanX GPU. We report the elapsed time for computations following the affine alignment preprocessing step, which all of the presented methods share, and requires just a few minutes on a CPU. ANTs requires roughly two or more hours of CPU time.\n\n\nResults\n\n\nAccuracy\n\n\nRuntime\n\nVoxelMorph-1 and VoxelMorph-2 are 60+ and 150+ times faster on average using the CPU. ANTs runtimes vary widely, because its convergence depends on the difficulty of the alignment task. When using the GPU, our networks compute a registration in under a second. To our knowledge, there is no publicly available ANTs implementation for GPUs.\n\n\nTraining and Testing on a Sub-population\n\nThe results in the previous sections combine multiple datasets consisting of different population types, resulting in a trained model that generalizes well to a range of subjects. In this section, we model parameters specific to a subpopulation, demonstrating the ability of tailoring our approach to particular tasks. We train using ABIDE subject scans, and evaluate test performance on unseen ABIDE scans. ABIDE contains scans of subjects with autism and controls, and includes a wide age range, with a median age of 15 years. In Table 2 we compare the results to those of the models trained on all datasets, presented in the previous section. The dataset-specific networks achieve a 1.5% Dice score improvement.\n\n\nRegularization Analysis\n\nFig. 6a presents average Dice scores for the validation set for different values of the smoothing parameter \u03bb. As a baseline, we display Dice score of the affinely aligned scans. The optimal Dice scores occur when \u03bb = 1 for VoxelMorph-1 and \u03bb = 1.5 for VoxelMorph-2. However, the results vary slowly over a large range of \u03bb values, showing that our model is robust to choice of \u03bb. Interestingly, even setting \u03bb = 0, which enforces no regularization, results in a significant improvement over affine registration. This is likely due to the fact that the optimal network parameters\u03b8 need to register all pairs in the training set well, giving an implicit regularization. Fig. 6b shows example registration fields at a coronal slice with different regularization values. For low \u03bb, the field can change dramatically across edges and structural boundaries.\n\n\nDiscussion\n\nOur model is able to perform on par with the state-ofthe-art ANTs registration package while requiring far less computation time to register test volume pairs. While our method learns general features about the data necessary for registration, it can adapt these parameters to specific subpopulations. When training on the ABIDE dataset only, we obtain improved Dice scores on test ABIDE scans compared to training on a dataset from several sources exhibiting different health conditions and variations in acquisition. This result shows that some of our model's parameters are learning properties specific to the training images.\n\nWe present two models which trade off in accuracy and computation time. The smaller architecture, VoxelMorph-1, runs significantly faster on the CPU and is less than 1 Dice point worse than VoxelMorph-2. This enables an application-specific decision. An advantage of our model is that it is easy to explore this tradeoff by changing the number of convolutional layers and channels of the network, which can be considered as hyperparameters. We selected these hyperparameters by experimenting on training and validation data, and they could be adapted to other Each row is a different scan. We clip the x, y, z displacements to [\u221210, 10], rescale them to [0, 1], and place them in RGB channels. As \u03bb increases, the registration field becomes smoother across structural boundaries.\n\n\ntasks.\n\nWe quantify accuracy in this study using Dice score, which acts as a proxy measure of registration accuracy. While our models achieve comparable Dice scores, ANTs produces diffeomorphic registrations, which are not guaranteed by our models. Diffeomorphic fields have attractive properties like invertibility and topology-preservation that are useful in some analyses. This presents an exciting area of future work for learning-based registration.\n\nOur method replaces a costly optimization problem for each test image pair, with one function optimization aggregated over a dataset during a training phase. This idea is applicable to a wide variety of problems traditionally relying on complex, non-learning-based optimization algorithms for each input. Our network implementations needed a one-time training period of a few days on a single NVIDIA TI-TANX GPU, but less than a second to register a test pair of images. Given the growing availability of image data, our solution is preferable to a non-learning-based approach, and sorely-needed to facilitate fast medical image analyses.\n\n\nConclusion\n\nThis paper presents an unsupervised learning-based approach to medical image registration, that requires no supervised information such as ground truth registration fields or anatomical landmarks. The approach obtains similar registration accuracy to state-of-the-art 3D image registration on a large-scale, multi-study MR brain dataset, while operating orders of magnitude faster. Model analysis shows that our model is robust to regularization parameter, can be tailored to different data populations, and can be easily modified to explore accuracy and runtime tradeoffs. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration.\n\nFigure 1\n1Figure 1: Example coronal slices from the 3D MRI brain dataset, after affine alignment. Each column is a different scan (subject) and each row is a different coronal slice. Several significant anatomical regions are outlined using different colors: L/R white matter in light/dark blue, L/R ventricles in yellow/red, and L/R hippocampi in purple/green. There are significant structural differences across scans, necessitating a deformable registration step to analyze interscan variations.\n\nFigure 2 :\n2Overview of our method. We learn parameters for a function g that registers one 3D volume (M ) to a second, fixed volume (F ). During training, we warp M with \u03c6 using a spatial transformer function. Our loss compares M \u03c6 and F and enforces smoothness of \u03c6.\n\nFigure 6 :\n6(a) Effect of varying the regularization parameter \u03bb on Dice score. The best results occur when \u03bb = 1 for VoxelMorph-1 and \u03bb = 1.5 for VoxelMorph-2. Also shown are Dice scores when applying only affine registration. (b) Examples of VoxelMorph-2 registration fields for a 2D coronal slice, for different values of \u03bb.\n\nTable 1 :\n1Average Dice scores and runtime results for affine alignment, ANTs, VoxelMorph-1, VoxelMorph-2. Standard deviations are in parentheses. The average Dice score is computed over all structures and subjects. Timing is computed after preprocessing. Our networks yield comparable results to ANTs in Dice score, while operating orders of magnitude faster during testing. To our knowledge, ANTs does not have a GPU implementation.Method \nAvg. Dice \nGPU sec \nCPU sec \nAffine only \n0.567 (0.157) \n0 \n0 \nANTs \n0.749 (0.135) \n-\n9059 (2023) \nVoxelMorph-1 0.742 (0.139) 0.365 (0.012) \n57(1) \nVoxelMorph-2 0.750 (0.137) 0.554 (0.017) \n144 (1) \n\n\n\nTable 2 :\n2Average Dice scores on ABIDE scans, when trained on all datasets (column 2) and ABIDE scans only (column 3). We achieve roughly 1.5% better scores when training on ABIDE only.Avg. Dice \nAvg. Dice \nMethod \n(Train on All) (Train on ABIDE) \nVoxelMorph-1 0.715(0.140) \n0.729(0.142) \nVoxelMorph-2 0.718(0.141) \n0.734(0.140) \n\n\nin medical imaging literature, the volumes produced by 3D imaging techniques are often referred to as images\n\nTensorflow: Large-scale machine learning on heterogeneous distributed systems. M Abadi, arXiv:1603.04467arXiv preprintM. Abadi et al. Tensorflow: Large-scale machine learn- ing on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. 6\n\nA fast diffeomorphic image registration algorithm. J Ashburner, Neuroimage. 381J. Ashburner. A fast diffeomorphic image registration algo- rithm. Neuroimage, 38(1):95-113, 2007. 2\n\nVoxel-based morphometry-the methods. J Ashburner, K Friston, Neuroimage. 112J. Ashburner and K. Friston. Voxel-based morphometry-the methods. Neuroimage, 11:805-821, 2000. 2\n\nSymmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain. B B Avants, Medical image analysis. 1215B. B. Avants et al. Symmetric diffeomorphic image registra- tion with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain. Medical image analy- sis, 12(1):26-41, 2008. 2, 3, 5\n\nA reproducible evaluation of ants similarity metric performance in brain image registration. B B Avants, Neuroimage. 543B. B. Avants et al. A reproducible evaluation of ants simi- larity metric performance in brain image registration. Neu- roimage, 54(3):2033-2044, 2011. 5\n\nMultiresolution elastic matching. R Bajcsy, S Kovacic, Computer Vision, Graphics, and Image Processing. 46R. Bajcsy and S. Kovacic. Multiresolution elastic matching. Computer Vision, Graphics, and Image Processing, 46:1-21, 1989. 2\n\nComputing large deformation metric mappings via geodesic flows of diffeomorphisms. M F Beg, Int. J. Comput. Vision. 612M. F. Beg et al. Computing large deformation metric map- pings via geodesic flows of diffeomorphisms. Int. J. Comput. Vision, 61:139-157, 2005. 2\n\nT Brox, High accuracy optical flow estimation based on a theory for warping. European Conference on Computer Vision (ECCV). T. Brox et al. High accuracy optical flow estimation based on a theory for warping. European Conference on Computer Vision (ECCV), pages 25-36, 2004. 3\n\nLarge displacement optical flow: Descriptor matching in variational motion estimation. T Brox, J Malik, IEEE Trans. Pattern Anal. Mach. Intell. 333T. Brox and J. Malik. Large displacement optical flow: De- scriptor matching in variational motion estimation. IEEE Trans. Pattern Anal. Mach. Intell., 33(3):500-513, 2011. 3\n\nLarge displacement optical flow from nearest neighbor fields. Z Chen, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Z. Chen et al. Large displacement optical flow from nearest neighbor fields. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2443-2450, 2013. 3\n\n. F Chollet, F. Chollet et al. Keras. https://github.com/ fchollet/keras, 2015. 6\n\nHarvard aging brain study: dataset and accessibility. A Dagley, NeuroImage. 5A. Dagley et al. Harvard aging brain study: dataset and ac- cessibility. NeuroImage, 2015. 5\n\nPatch-based discrete registration of clinical brain images. A V Dalca, International Workshop on Patch-based Techniques in Medical Imaging. SpringerA. V. Dalca et al. Patch-based discrete registration of clini- cal brain images. In International Workshop on Patch-based Techniques in Medical Imaging, pages 60-67. Springer, 2016. 2\n\nEnd-to-end unsupervised deformable image registration with a convolutional neural network. B De Vos, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. B. de Vos et al. End-to-end unsupervised deformable image registration with a convolutional neural network. In Deep Learning in Medical Image Analysis and Multimodal Learn- ing for Clinical Decision Support, pages 204-212. 2017. 3\n\nMeasures of the amount of ecologic association between species. L R Dice, Ecology. 263L. R. Dice. Measures of the amount of ecologic association between species. Ecology, 26(3):297-302, 1945. 5\n\nLearning optical flow with convolutional networks. A Dosovitskiy, IEEE International Conference on Computer Vision (ICCV). A. Dosovitskiy et al. Flownet: Learning optical flow with convolutional networks. In IEEE International Conference on Computer Vision (ICCV), pages 2758-2766, 2015. 3\n\n. B Fischl, Freesurfer, Neuroimage. 622B. Fischl. Freesurfer. Neuroimage, 62(2):774-781, 2012. 5\n\nDense image registration through mrfs and efficient linear programming. B Glocker, Medical image analysis. 126B. Glocker et al. Dense image registration through mrfs and efficient linear programming. Medical image analysis, 12(6):731-741, 2008. 2\n\nThe mcic collection: a shared repository of multi-modal, multi-site brain image data from a clinical investigation of schizophrenia. R L Gollub, Neuroinformatics. 113R. L. Gollub et al. The mcic collection: a shared repos- itory of multi-modal, multi-site brain image data from a clinical investigation of schizophrenia. Neuroinformatics, 11(3):367-388, 2013. 5\n\nBrain genomics superstruct project initial data release with structural, functional, and behavioral measures. A J Holmes, Scientific data. 25A. J. Holmes et al. Brain genomics superstruct project ini- tial data release with structural, functional, and behavioral measures. Scientific data, 2, 2015. 5\n\nDetermining optical flow. B K Horn, B G Schunck, B. K. Horn and B. G. Schunck. Determining optical flow. 1980. 3\n\nImage-to-image translation with conditional adversarial networks. P Isola, arXiv preprintP. Isola et al. Image-to-image translation with conditional adversarial networks. arXiv preprint, 2017. 3\n\nSpatial transformer networks. M Jaderberg, Advances in neural information processing systems. 34M. Jaderberg et al. Spatial transformer networks. In Advances in neural information processing systems, pages 2017-2025, 2015. 3, 4\n\nADAM: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba. ADAM: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6\n\nEvaluation of 14 nonlinear deformation algorithms applied to human brain mri registration. A Klein, Neuroimage. 463A. Klein et al. Evaluation of 14 nonlinear deformation algo- rithms applied to human brain mri registration. Neuroimage, 46(3):786-802, 2009. 5\n\nRobust non-rigid registration through agentbased action learning. J Krebs, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). SpringerJ. Krebs et al. Robust non-rigid registration through agent- based action learning. In International Conference on Med- ical Image Computing and Computer-Assisted Intervention (MICCAI), pages 344-352. Springer, 2017. 3\n\nNon-rigid image registration using fully convolutional networks with deep self-supervision. H Li, Y Fan, arXiv:1709.00799arXiv preprintH. Li and Y. Fan. Non-rigid image registration using fully convolutional networks with deep self-supervision. arXiv preprint arXiv:1709.00799, 2017. 3\n\nSIFT flow: Dense correspondence across scenes and its applications. C Liu, IEEE Trans. Pattern Anal. Mach. Intell. 335C. Liu et al. SIFT flow: Dense correspondence across scenes and its applications. IEEE Trans. Pattern Anal. Mach. Intell., 33(5):978-994, 2011. 3\n\nOpen access series of imaging studies (oasis): cross-sectional mri data in young, middle aged, nondemented, and demented older adults. D S Marcus, Journal of cognitive neuroscience. 199D. S. Marcus et al. Open access series of imaging studies (oasis): cross-sectional mri data in young, middle aged, non- demented, and demented older adults. Journal of cognitive neuroscience, 19(9):1498-1507, 2007. 5\n\nThe parkinson progression marker initiative. K Marek, Progress in neurobiology. 954K. Marek et al. The parkinson progression marker initiative. Progress in neurobiology, 95(4):629-635, 2011. 5\n\nThe autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism. A D Martino, Molecular psychiatry. 196A. D. Martino et al. The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain ar- chitecture in autism. Molecular psychiatry, 19(6):659-667, 2014. 5\n\nThe ADHD-200 consortium: a model to advance the translational potential of neuroimaging in clinical neuroscience. Frontiers in systems neuroscience. M P Milham, 662M. P. Milham et al. The ADHD-200 consortium: a model to advance the translational potential of neuroimaging in clin- ical neuroscience. Frontiers in systems neuroscience, 6:62, 2012. 5\n\nWays toward an early diagnosis in alzheimer's disease: the alzheimer's disease neuroimaging initiative (adni). S G Mueller, 1Alzheimer's & DementiaS. G. Mueller et al. Ways toward an early diagnosis in alzheimer's disease: the alzheimer's disease neuroimaging initiative (adni). Alzheimer's & Dementia, 1(1):55-66, 2005. 5\n\nTransformation-grounded image generation network for novel 3D view synthesis. E Park, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). E. Park et al. Transformation-grounded image generation network for novel 3D view synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 702-711, 2017. 3\n\nSvf-net: Learning deformable image registration using shape matching. M.-M Roh\u00e9, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). SpringerM.-M. Roh\u00e9 et al. Svf-net: Learning deformable image reg- istration using shape matching. In International Conference on Medical Image Computing and Computer-Assisted Inter- vention (MICCAI), pages 266-274. Springer, 2017. 3\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). SpringerO. Ronneberger et al. U-net: Convolutional networks for biomedical image segmentation. In International Confer- ence on Medical Image Computing and Computer-Assisted Intervention (MICCAI), pages 234-241. Springer, 2015. 3\n\nNonrigid registration using free-form deformation: Application to breast mr images. D Rueckert, IEEE Transactions on Medical Imaging. 188D. Rueckert et al. Nonrigid registration using free-form de- formation: Application to breast mr images. IEEE Transac- tions on Medical Imaging, 18(8):712-721, 1999. 2\n\nHammer: Hierarchical attribute matching mechanism for elastic registration. D Shen, C Davatzikos, IEEE Transactions on Medical Imaging. 2111D. Shen and C. Davatzikos. Hammer: Hierarchical attribute matching mechanism for elastic registration. IEEE Transac- tions on Medical Imaging, 21(11):1421-1439, 2002. 2\n\nNonrigid image registration using multiscale 3d convolutional neural networks. H Sokooti, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). SpringerH. Sokooti et al. Nonrigid image registration using multi- scale 3d convolutional neural networks. In International Conference on Medical Image Computing and Computer- Assisted Intervention (MICCAI), pages 232-239. Springer, 2017. 3\n\nQuantification and analysis of large multimodal clinical image studies: Application to stroke. R Sridharan, International Workshop on Multimodal Brain Image Analysis. SpringerR. Sridharan et al. Quantification and analysis of large mul- timodal clinical image studies: Application to stroke. In In- ternational Workshop on Multimodal Brain Image Analysis, pages 18-30. Springer, 2013. 5\n\nSecrets of optical flow estimation and their principles. D Sun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 3D. Sun et al. Secrets of optical flow estimation and their principles. IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 2432-2439, 2010. 3\n\nImage matching as a diffusion process: an analogy with maxwell's demons. J Thirion, Medical Image Analysis. 23J. Thirion. Image matching as a diffusion process: an analogy with maxwell's demons. Medical Image Analysis, 2(3):243-260, 1998. 2\n\nLarge displacement optical flow with deep matching. P , IEEE International Conference on Computer Vision (ICCV). P. Weinzaepfel et al. Deepflow: Large displacement optical flow with deep matching. In IEEE International Conference on Computer Vision (ICCV), pages 1385-1392, 2013. 3\n\nEfficient sparse-to-dense optical flow estimation using a learned basis and layers. J Wulff, M J Black, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Wulff and M. J. Black. Efficient sparse-to-dense opti- cal flow estimation using a learned basis and layers. In IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), pages 120-130, 2015. 3\n\nFast predictive image registration-a deep learning approach. X Yang, NeuroImage. 1583X. Yang et al. Quicksilver: Fast predictive im- age registration-a deep learning approach. NeuroImage, 158:378-396, 2017. 3\n\nView synthesis by appearance flow. T Zhou, European Conference on Computer Vision (ECCV). T. Zhou et al. View synthesis by appearance flow. Euro- pean Conference on Computer Vision (ECCV), pages 286- 301, 2016. 3\n", "annotations": {"author": "[{\"end\":115,\"start\":76},{\"end\":141,\"start\":116},{\"end\":189,\"start\":142},{\"end\":215,\"start\":190},{\"end\":273,\"start\":216},{\"end\":325,\"start\":274},{\"end\":380,\"start\":326},{\"end\":406,\"start\":381}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":81},{\"end\":150,\"start\":146},{\"end\":230,\"start\":223},{\"end\":288,\"start\":282},{\"end\":343,\"start\":338}]", "author_first_name": "[{\"end\":80,\"start\":76},{\"end\":119,\"start\":116},{\"end\":145,\"start\":142},{\"end\":193,\"start\":190},{\"end\":220,\"start\":216},{\"end\":222,\"start\":221},{\"end\":281,\"start\":277},{\"end\":335,\"start\":329},{\"end\":337,\"start\":336},{\"end\":384,\"start\":381}]", "author_affiliation": "[{\"end\":114,\"start\":95},{\"end\":140,\"start\":121},{\"end\":188,\"start\":169},{\"end\":214,\"start\":195},{\"end\":272,\"start\":253},{\"end\":324,\"start\":305},{\"end\":379,\"start\":360},{\"end\":405,\"start\":386}]", "title": "[{\"end\":73,\"start\":1},{\"end\":479,\"start\":407}]", "venue": null, "abstract": "[{\"end\":1710,\"start\":481}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7360,\"start\":7357},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7362,\"start\":7360},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7364,\"start\":7362},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7366,\"start\":7364},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7369,\"start\":7366},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7372,\"start\":7369},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7375,\"start\":7372},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7378,\"start\":7377},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7489,\"start\":7486},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7492,\"start\":7489},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7528,\"start\":7525},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7573,\"start\":7569},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7589,\"start\":7585},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7855,\"start\":7852},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7867,\"start\":7864},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7914,\"start\":7911},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8141,\"start\":8137},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8144,\"start\":8141},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8147,\"start\":8144},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8150,\"start\":8147},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8251,\"start\":8247},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8254,\"start\":8251},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8407,\"start\":8403},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8664,\"start\":8660},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9329,\"start\":9326},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9332,\"start\":9329},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9335,\"start\":9332},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9455,\"start\":9452},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9458,\"start\":9455},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9491,\"start\":9487},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9665,\"start\":9661},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9772,\"start\":9768},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9775,\"start\":9772},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9998,\"start\":9994},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10076,\"start\":10072},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10079,\"start\":10076},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11848,\"start\":11844},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11851,\"start\":11848},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14663,\"start\":14659},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15918,\"start\":15915},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16782,\"start\":16778},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16794,\"start\":16790},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16806,\"start\":16802},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16820,\"start\":16816},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16831,\"start\":16827},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16842,\"start\":16838},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16853,\"start\":16849},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16875,\"start\":16871},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17183,\"start\":17179},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18182,\"start\":18178},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18185,\"start\":18182},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19250,\"start\":19246},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19586,\"start\":19583},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19657,\"start\":19653},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19740,\"start\":19737},{\"end\":20256,\"start\":20252},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20550,\"start\":20546},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20580,\"start\":20577},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20612,\"start\":20608}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":27856,\"start\":27357},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28126,\"start\":27857},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28455,\"start\":28127},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29099,\"start\":28456},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29433,\"start\":29100}]", "paragraph": "[{\"end\":2323,\"start\":1726},{\"end\":3275,\"start\":2325},{\"end\":3310,\"start\":3277},{\"end\":3464,\"start\":3312},{\"end\":3609,\"start\":3466},{\"end\":3726,\"start\":3611},{\"end\":4433,\"start\":3728},{\"end\":5382,\"start\":4448},{\"end\":5635,\"start\":5384},{\"end\":5668,\"start\":5663},{\"end\":6961,\"start\":5719},{\"end\":7234,\"start\":6963},{\"end\":7915,\"start\":7300},{\"end\":8977,\"start\":7963},{\"end\":9492,\"start\":9000},{\"end\":10145,\"start\":9494},{\"end\":10567,\"start\":10156},{\"end\":11133,\"start\":10569},{\"end\":11665,\"start\":11135},{\"end\":12790,\"start\":11747},{\"end\":13212,\"start\":12792},{\"end\":13700,\"start\":13273},{\"end\":13914,\"start\":13702},{\"end\":14371,\"start\":13916},{\"end\":14664,\"start\":14407},{\"end\":14883,\"start\":14666},{\"end\":15104,\"start\":14948},{\"end\":15586,\"start\":15122},{\"end\":15796,\"start\":15588},{\"end\":16149,\"start\":15913},{\"end\":16323,\"start\":16151},{\"end\":16466,\"start\":16356},{\"end\":16553,\"start\":16515},{\"end\":17676,\"start\":16579},{\"end\":18533,\"start\":17678},{\"end\":19311,\"start\":18548},{\"end\":19505,\"start\":19390},{\"end\":20489,\"start\":19526},{\"end\":22072,\"start\":20508},{\"end\":22444,\"start\":22105},{\"end\":23203,\"start\":22489},{\"end\":24083,\"start\":23231},{\"end\":24727,\"start\":24098},{\"end\":25508,\"start\":24729},{\"end\":25965,\"start\":25519},{\"end\":26605,\"start\":25967},{\"end\":27356,\"start\":26620}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5662,\"start\":5636},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5718,\"start\":5669},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11716,\"start\":11666},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13252,\"start\":13213},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14947,\"start\":14884},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15912,\"start\":15797},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16355,\"start\":16324},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16514,\"start\":16467},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19389,\"start\":19312}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21008,\"start\":21001},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21755,\"start\":21748},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23028,\"start\":23021}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1724,\"start\":1712},{\"attributes\":{\"n\":\"2.\"},\"end\":4446,\"start\":4436},{\"attributes\":{\"n\":\"3.\"},\"end\":7249,\"start\":7237},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7298,\"start\":7252},{\"attributes\":{\"n\":\"3.2.\"},\"end\":7961,\"start\":7918},{\"attributes\":{\"n\":\"3.3.\"},\"end\":8998,\"start\":8980},{\"attributes\":{\"n\":\"4.\"},\"end\":10154,\"start\":10148},{\"attributes\":{\"n\":\"4.1.\"},\"end\":11745,\"start\":11718},{\"end\":13271,\"start\":13254},{\"attributes\":{\"n\":\"4.2.\"},\"end\":14405,\"start\":14374},{\"attributes\":{\"n\":\"4.3.\"},\"end\":15120,\"start\":15107},{\"attributes\":{\"n\":\"5.\"},\"end\":16567,\"start\":16556},{\"attributes\":{\"n\":\"5.1.\"},\"end\":16577,\"start\":16570},{\"attributes\":{\"n\":\"5.2.\"},\"end\":18546,\"start\":18536},{\"attributes\":{\"n\":\"5.3.\"},\"end\":19524,\"start\":19508},{\"attributes\":{\"n\":\"5.4.\"},\"end\":20506,\"start\":20492},{\"attributes\":{\"n\":\"5.5.\"},\"end\":22082,\"start\":22075},{\"attributes\":{\"n\":\"5.5.1\"},\"end\":22093,\"start\":22085},{\"attributes\":{\"n\":\"5.5.2\"},\"end\":22103,\"start\":22096},{\"attributes\":{\"n\":\"5.5.3\"},\"end\":22487,\"start\":22447},{\"attributes\":{\"n\":\"5.5.4\"},\"end\":23229,\"start\":23206},{\"attributes\":{\"n\":\"6.\"},\"end\":24096,\"start\":24086},{\"end\":25517,\"start\":25511},{\"attributes\":{\"n\":\"7.\"},\"end\":26618,\"start\":26608},{\"end\":27366,\"start\":27358},{\"end\":27868,\"start\":27858},{\"end\":28138,\"start\":28128},{\"end\":28466,\"start\":28457},{\"end\":29110,\"start\":29101}]", "table": "[{\"end\":29099,\"start\":28891},{\"end\":29433,\"start\":29287}]", "figure_caption": "[{\"end\":27856,\"start\":27368},{\"end\":28126,\"start\":27870},{\"end\":28455,\"start\":28140},{\"end\":28891,\"start\":28468},{\"end\":29287,\"start\":29112}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4881,\"start\":4875},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10831,\"start\":10825},{\"end\":11977,\"start\":11971},{\"end\":13321,\"start\":13313},{\"end\":14265,\"start\":14259},{\"end\":18313,\"start\":18307},{\"end\":19082,\"start\":19076},{\"end\":19841,\"start\":19833},{\"end\":21410,\"start\":21404},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23907,\"start\":23900}]", "bib_author_first_name": "[{\"end\":29624,\"start\":29623},{\"end\":29853,\"start\":29852},{\"end\":30020,\"start\":30019},{\"end\":30033,\"start\":30032},{\"end\":30295,\"start\":30294},{\"end\":30297,\"start\":30296},{\"end\":30641,\"start\":30640},{\"end\":30643,\"start\":30642},{\"end\":30857,\"start\":30856},{\"end\":30867,\"start\":30866},{\"end\":31139,\"start\":31138},{\"end\":31141,\"start\":31140},{\"end\":31322,\"start\":31321},{\"end\":31686,\"start\":31685},{\"end\":31694,\"start\":31693},{\"end\":31984,\"start\":31983},{\"end\":32231,\"start\":32230},{\"end\":32366,\"start\":32365},{\"end\":32543,\"start\":32542},{\"end\":32545,\"start\":32544},{\"end\":32907,\"start\":32906},{\"end\":33308,\"start\":33307},{\"end\":33310,\"start\":33309},{\"end\":33490,\"start\":33489},{\"end\":33732,\"start\":33731},{\"end\":33900,\"start\":33899},{\"end\":34209,\"start\":34208},{\"end\":34211,\"start\":34210},{\"end\":34549,\"start\":34548},{\"end\":34551,\"start\":34550},{\"end\":34767,\"start\":34766},{\"end\":34769,\"start\":34768},{\"end\":34777,\"start\":34776},{\"end\":34779,\"start\":34778},{\"end\":34921,\"start\":34920},{\"end\":35081,\"start\":35080},{\"end\":35324,\"start\":35323},{\"end\":35326,\"start\":35325},{\"end\":35336,\"start\":35335},{\"end\":35571,\"start\":35570},{\"end\":35806,\"start\":35805},{\"end\":36232,\"start\":36231},{\"end\":36238,\"start\":36237},{\"end\":36495,\"start\":36494},{\"end\":36827,\"start\":36826},{\"end\":36829,\"start\":36828},{\"end\":37140,\"start\":37139},{\"end\":37409,\"start\":37408},{\"end\":37411,\"start\":37410},{\"end\":37785,\"start\":37784},{\"end\":37787,\"start\":37786},{\"end\":38097,\"start\":38096},{\"end\":38099,\"start\":38098},{\"end\":38388,\"start\":38387},{\"end\":38723,\"start\":38719},{\"end\":39127,\"start\":39126},{\"end\":39554,\"start\":39553},{\"end\":39852,\"start\":39851},{\"end\":39860,\"start\":39859},{\"end\":40165,\"start\":40164},{\"end\":40610,\"start\":40609},{\"end\":40960,\"start\":40959},{\"end\":41274,\"start\":41273},{\"end\":41495,\"start\":41494},{\"end\":41810,\"start\":41809},{\"end\":41819,\"start\":41818},{\"end\":41821,\"start\":41820},{\"end\":42166,\"start\":42165},{\"end\":42350,\"start\":42349}]", "bib_author_last_name": "[{\"end\":29630,\"start\":29625},{\"end\":29863,\"start\":29854},{\"end\":30030,\"start\":30021},{\"end\":30041,\"start\":30034},{\"end\":30304,\"start\":30298},{\"end\":30650,\"start\":30644},{\"end\":30864,\"start\":30858},{\"end\":30875,\"start\":30868},{\"end\":31145,\"start\":31142},{\"end\":31327,\"start\":31323},{\"end\":31691,\"start\":31687},{\"end\":31700,\"start\":31695},{\"end\":31989,\"start\":31985},{\"end\":32239,\"start\":32232},{\"end\":32373,\"start\":32367},{\"end\":32551,\"start\":32546},{\"end\":32914,\"start\":32908},{\"end\":33315,\"start\":33311},{\"end\":33502,\"start\":33491},{\"end\":33739,\"start\":33733},{\"end\":33751,\"start\":33741},{\"end\":33908,\"start\":33901},{\"end\":34218,\"start\":34212},{\"end\":34558,\"start\":34552},{\"end\":34774,\"start\":34770},{\"end\":34787,\"start\":34780},{\"end\":34927,\"start\":34922},{\"end\":35091,\"start\":35082},{\"end\":35333,\"start\":35327},{\"end\":35339,\"start\":35337},{\"end\":35577,\"start\":35572},{\"end\":35812,\"start\":35807},{\"end\":36235,\"start\":36233},{\"end\":36242,\"start\":36239},{\"end\":36499,\"start\":36496},{\"end\":36836,\"start\":36830},{\"end\":37146,\"start\":37141},{\"end\":37419,\"start\":37412},{\"end\":37794,\"start\":37788},{\"end\":38107,\"start\":38100},{\"end\":38393,\"start\":38389},{\"end\":38728,\"start\":38724},{\"end\":39139,\"start\":39128},{\"end\":39563,\"start\":39555},{\"end\":39857,\"start\":39853},{\"end\":39871,\"start\":39861},{\"end\":40173,\"start\":40166},{\"end\":40620,\"start\":40611},{\"end\":40964,\"start\":40961},{\"end\":41282,\"start\":41275},{\"end\":41816,\"start\":41811},{\"end\":41827,\"start\":41822},{\"end\":42171,\"start\":42167},{\"end\":42355,\"start\":42351}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b0\"},\"end\":29799,\"start\":29544},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":545830},\"end\":29980,\"start\":29801},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16777465},\"end\":30155,\"start\":29982},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":18468567},\"end\":30545,\"start\":30157},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7253493},\"end\":30820,\"start\":30547},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":19718946},\"end\":31053,\"start\":30822},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":17772076},\"end\":31319,\"start\":31055},{\"attributes\":{\"id\":\"b7\"},\"end\":31596,\"start\":31321},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4129821},\"end\":31919,\"start\":31598},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3912058},\"end\":32226,\"start\":31921},{\"attributes\":{\"id\":\"b10\"},\"end\":32309,\"start\":32228},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":207193527},\"end\":32480,\"start\":32311},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10080108},\"end\":32813,\"start\":32482},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2637577},\"end\":33241,\"start\":32815},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":53335638},\"end\":33436,\"start\":33243},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":12552176},\"end\":33727,\"start\":33438},{\"attributes\":{\"id\":\"b16\"},\"end\":33825,\"start\":33729},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5404982},\"end\":34073,\"start\":33827},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7529201},\"end\":34436,\"start\":34075},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":428300},\"end\":34738,\"start\":34438},{\"attributes\":{\"id\":\"b20\"},\"end\":34852,\"start\":34740},{\"attributes\":{\"id\":\"b21\"},\"end\":35048,\"start\":34854},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6099034},\"end\":35277,\"start\":35050},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b23\"},\"end\":35477,\"start\":35279},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":220973703},\"end\":35737,\"start\":35479},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2118083},\"end\":36137,\"start\":35739},{\"attributes\":{\"doi\":\"arXiv:1709.00799\",\"id\":\"b26\"},\"end\":36424,\"start\":36139},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":10458500},\"end\":36689,\"start\":36426},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10840552},\"end\":37092,\"start\":36691},{\"attributes\":{\"id\":\"b29\"},\"end\":37286,\"start\":37094},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13785515},\"end\":37633,\"start\":37288},{\"attributes\":{\"id\":\"b31\"},\"end\":37983,\"start\":37635},{\"attributes\":{\"id\":\"b32\"},\"end\":38307,\"start\":37985},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1536163},\"end\":38647,\"start\":38309},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":40104153},\"end\":39059,\"start\":38649},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3719281},\"end\":39467,\"start\":39061},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":330039},\"end\":39773,\"start\":39469},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":5586563},\"end\":40083,\"start\":39775},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2790786},\"end\":40512,\"start\":40085},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11527814},\"end\":40900,\"start\":40514},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":206591220},\"end\":41198,\"start\":40902},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":1224379},\"end\":41440,\"start\":41200},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":206769904},\"end\":41723,\"start\":41442},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2551813},\"end\":42102,\"start\":41725},{\"attributes\":{\"id\":\"b44\"},\"end\":42312,\"start\":42104},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":6002134},\"end\":42526,\"start\":42314}]", "bib_title": "[{\"end\":29850,\"start\":29801},{\"end\":30017,\"start\":29982},{\"end\":30292,\"start\":30157},{\"end\":30638,\"start\":30547},{\"end\":30854,\"start\":30822},{\"end\":31136,\"start\":31055},{\"end\":31683,\"start\":31598},{\"end\":31981,\"start\":31921},{\"end\":32363,\"start\":32311},{\"end\":32540,\"start\":32482},{\"end\":32904,\"start\":32815},{\"end\":33305,\"start\":33243},{\"end\":33487,\"start\":33438},{\"end\":33897,\"start\":33827},{\"end\":34206,\"start\":34075},{\"end\":34546,\"start\":34438},{\"end\":35078,\"start\":35050},{\"end\":35568,\"start\":35479},{\"end\":35803,\"start\":35739},{\"end\":36492,\"start\":36426},{\"end\":36824,\"start\":36691},{\"end\":37137,\"start\":37094},{\"end\":37406,\"start\":37288},{\"end\":38385,\"start\":38309},{\"end\":38717,\"start\":38649},{\"end\":39124,\"start\":39061},{\"end\":39551,\"start\":39469},{\"end\":39849,\"start\":39775},{\"end\":40162,\"start\":40085},{\"end\":40607,\"start\":40514},{\"end\":40957,\"start\":40902},{\"end\":41271,\"start\":41200},{\"end\":41492,\"start\":41442},{\"end\":41807,\"start\":41725},{\"end\":42163,\"start\":42104},{\"end\":42347,\"start\":42314}]", "bib_author": "[{\"end\":29632,\"start\":29623},{\"end\":29865,\"start\":29852},{\"end\":30032,\"start\":30019},{\"end\":30043,\"start\":30032},{\"end\":30306,\"start\":30294},{\"end\":30652,\"start\":30640},{\"end\":30866,\"start\":30856},{\"end\":30877,\"start\":30866},{\"end\":31147,\"start\":31138},{\"end\":31329,\"start\":31321},{\"end\":31693,\"start\":31685},{\"end\":31702,\"start\":31693},{\"end\":31991,\"start\":31983},{\"end\":32241,\"start\":32230},{\"end\":32375,\"start\":32365},{\"end\":32553,\"start\":32542},{\"end\":32916,\"start\":32906},{\"end\":33317,\"start\":33307},{\"end\":33504,\"start\":33489},{\"end\":33741,\"start\":33731},{\"end\":33753,\"start\":33741},{\"end\":33910,\"start\":33899},{\"end\":34220,\"start\":34208},{\"end\":34560,\"start\":34548},{\"end\":34776,\"start\":34766},{\"end\":34789,\"start\":34776},{\"end\":34929,\"start\":34920},{\"end\":35093,\"start\":35080},{\"end\":35335,\"start\":35323},{\"end\":35341,\"start\":35335},{\"end\":35579,\"start\":35570},{\"end\":35814,\"start\":35805},{\"end\":36237,\"start\":36231},{\"end\":36244,\"start\":36237},{\"end\":36501,\"start\":36494},{\"end\":36838,\"start\":36826},{\"end\":37148,\"start\":37139},{\"end\":37421,\"start\":37408},{\"end\":37796,\"start\":37784},{\"end\":38109,\"start\":38096},{\"end\":38395,\"start\":38387},{\"end\":38730,\"start\":38719},{\"end\":39141,\"start\":39126},{\"end\":39565,\"start\":39553},{\"end\":39859,\"start\":39851},{\"end\":39873,\"start\":39859},{\"end\":40175,\"start\":40164},{\"end\":40622,\"start\":40609},{\"end\":40966,\"start\":40959},{\"end\":41284,\"start\":41273},{\"end\":41498,\"start\":41494},{\"end\":41818,\"start\":41809},{\"end\":41829,\"start\":41818},{\"end\":42173,\"start\":42165},{\"end\":42357,\"start\":42349}]", "bib_venue": "[{\"end\":29621,\"start\":29544},{\"end\":29875,\"start\":29865},{\"end\":30053,\"start\":30043},{\"end\":30328,\"start\":30306},{\"end\":30662,\"start\":30652},{\"end\":30924,\"start\":30877},{\"end\":31169,\"start\":31147},{\"end\":31443,\"start\":31329},{\"end\":31740,\"start\":31702},{\"end\":32056,\"start\":31991},{\"end\":32385,\"start\":32375},{\"end\":32620,\"start\":32553},{\"end\":33009,\"start\":32916},{\"end\":33324,\"start\":33317},{\"end\":33559,\"start\":33504},{\"end\":33763,\"start\":33753},{\"end\":33932,\"start\":33910},{\"end\":34236,\"start\":34220},{\"end\":34575,\"start\":34560},{\"end\":34764,\"start\":34740},{\"end\":34918,\"start\":34854},{\"end\":35142,\"start\":35093},{\"end\":35321,\"start\":35279},{\"end\":35589,\"start\":35579},{\"end\":35909,\"start\":35814},{\"end\":36229,\"start\":36139},{\"end\":36539,\"start\":36501},{\"end\":36871,\"start\":36838},{\"end\":37172,\"start\":37148},{\"end\":37441,\"start\":37421},{\"end\":37782,\"start\":37635},{\"end\":38094,\"start\":37985},{\"end\":38460,\"start\":38395},{\"end\":38825,\"start\":38730},{\"end\":39236,\"start\":39141},{\"end\":39601,\"start\":39565},{\"end\":39909,\"start\":39873},{\"end\":40270,\"start\":40175},{\"end\":40679,\"start\":40622},{\"end\":41031,\"start\":40966},{\"end\":41306,\"start\":41284},{\"end\":41553,\"start\":41498},{\"end\":41894,\"start\":41829},{\"end\":42183,\"start\":42173},{\"end\":42402,\"start\":42357}]"}}}, "year": 2023, "month": 12, "day": 17}