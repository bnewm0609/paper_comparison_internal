{"id": 233476465, "updated": "2023-10-06 04:22:12.702", "metadata": {"title": "Post-training deep neural network pruning via layer-wise calibration", "authors": "[{\"first\":\"Ivan\",\"last\":\"Lazarevich\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Kozlov\",\"middle\":[]},{\"first\":\"Nikita\",\"last\":\"Malinin\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "journal": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "publication_date": {"year": 2021, "month": 4, "day": 30}, "abstract": "We present a post-training weight pruning method for deep neural networks that achieves accuracy levels tolerable for the production setting and that is sufficiently fast to be run on commodity hardware such as desktop CPUs or edge devices. We propose a data-free extension of the approach for computer vision models based on automatically-generated synthetic fractal images. We obtain state-of-the-art results for data-free neural network pruning, with ~1.5% top@1 accuracy drop for a ResNet50 on ImageNet at 50% sparsity rate. When using real data, we are able to get a ResNet50 model on ImageNet with 65% sparsity rate in 8-bit precision in a post-training setting with a ~1% top@1 accuracy drop. We release the code as a part of the OpenVINO(TM) Post-Training Optimization tool.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.15023", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccvw/LazarevichKM21", "doi": "10.1109/iccvw54120.2021.00094"}}, "content": {"source": {"pdf_hash": "07432dd1f73938a2439dfe74c55bcaabf775aa77", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.15023v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2104.15023", "status": "GREEN"}}, "grobid": {"id": "aa3ebbe4a40758b64d2cad3bccdf559ab685a1cb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/07432dd1f73938a2439dfe74c55bcaabf775aa77.txt", "contents": "\nPost-training deep neural network pruning via layer-wise calibration\n\n\nIvan Lazarevich ivan.lazarevich@intel.com \nIntel Corporation\nIntel Corporation\nIntel Corporation\n\n\nAlexander Kozlov alexander.kozlov@intel.com \nIntel Corporation\nIntel Corporation\nIntel Corporation\n\n\nNikita Malinin nikita.malinin@intel.com \nIntel Corporation\nIntel Corporation\nIntel Corporation\n\n\nPost-training deep neural network pruning via layer-wise calibration\n\nWe present a post-training weight pruning method for deep neural networks that achieves accuracy levels tolerable for the production setting and that is sufficiently fast to be run on commodity hardware such as desktop CPUs or edge devices. We propose a data-free extension of the approach for computer vision models based on automatically-generated synthetic fractal images. We obtain state-of-the-art results for data-free neural network pruning, with \u223c1.5% top@1 accuracy drop for a ResNet50 on ImageNet at 50% sparsity rate. When using real data, we are able to get a ResNet50 model on ImageNet with 65% sparsity rate in 8-bit precision in a post-training setting with a \u223c1% top@1 accuracy drop. We release the code as a part of the OpenVINO TM Post-Training Optimization tool 1 .\n\nIntroduction\n\nDeep neural network (DNN) models have achieved unprecedented accuracy in several crucial domains such as computer vision and natural language processing. Despite the success of DNN models, an unreasonably large amount of computations and memory required for their inference limits their deployment on edge devices, such as smart cameras equipped with low-power CPUs, GPUs or ASIC accelerators. Significant efforts in recent years have been devoted to both hardware design and algorithmic approaches to DNN model compression to enable inference speedups for various model architectures and use cases. Some of the DNN compression methods, such as 8-bit quantization, were adapted to the post-training setting where the original DNN model to be compressed could come from any software framework and no access to the original training pipeline and the training dataset is given. One of the promising approaches to reduce the memory footprint and inference latency of DNNs is weight pruning [2,4], 1 https://docs.openvinotoolkit.org/latest/pot README.html which results in models with sparse weight matrices. Recently, a lot of research and development has been aimed at leveraging weight sparsity to achieve inference speedups on a range of hardware platforms [3,6]. However, relatively little effort was devoted to providing accurate sparse DNN models in the post-training scenario.\n\nIn this work, we propose a recipe for fast posttraining pruning of DNNs that produces models with significant sparsity rates (e.g. 50%) but negligible accuracy drops. Furthermore, if combined with weight quantization techniques, the proposed method could reduce the model memory footprint by a factor of 6-8x [16]. We propose a fast data-free extension of our weight pruning pipeline which allows getting state-of-the-art accuracy levels for a range of computer vision models. To streamline the deployment process of sparse quantized DNNs on hardware, we have implemented the proposed method as a part of the OpenVINO TM Post-Training Optimization tool.\n\nWe summarize our contributions as follows:\n\n\u2022 A recipe for post-training weight pruning with demonstrated results on a wide range of models and datasets.\n\n\u2022 State-of-the-art results for data-free weight pruning of computer vision models using synthetic fractal images for model compression.\n\n\u2022 An ablation study of the proposed post-training weight pruning pipeline demonstrating the effects of particular components such as per-layer sparsity rate selection criteria, bias correction and layerwise fine-tuning settings.\n\n\nRelated work\n\nNeural network weight pruning is a technique used to produce lightweight models by removing (zeroing out) a certain percentage of unimportant weights. In  this work, we focus on unstructured pruning (weight sparsification) whereby no structural constraints on the sparsity pattern are imposed and a subset of weights determined to have the lowest importance score values is removed regardless of position in weight tensors. Various definitions of weight importance functions have been proposed in the literature [4,23], the simplest baseline being magnitude-based weight pruning, as well as various heuristics to determine per-layer sparsity rates [13,7]. Magnitude-based sparsification via a global importance threshold was found to be a strong baseline in the compression-aware training regime for a range of models [4,20]. These weight pruning approaches typically imply compression-aware model retraining, which means existing access to the training code, the training dataset and appropriate compute resources. Other DNN compression techniques, such as 8-bit quantization, however, have been successfully applied in a less restrictive setting -in the post-training or data-free regimes [18]. The post-training compression regime is favorable from a practical perspective, since model compression could be ultimately implemented via a single API call rather than via the mod-ification of the original model training code. There, however, have been few attempts to implement posttraining or data-free weight pruning of DNNs, primarily due to the large accuracy drop incurred during sparsification [8,21]. Recently, there have also been developed layer-wise gradient optimization-based methods for post-training compression [9,15,17,8] with applications to low-bitwidth quantization and weight pruning. These methods are promising because they allow restoring compressed model accuracy in the posttraining setting in many cases. Nevertheless accuracy degradation was still found to be significant for sparsity rates above 40%. In this work, we propose a posttraining sparsification recipe that allows insignificant accuracy drops on a range of DNN models at sparsity rates of 50% and higher. We also suggest a straightforward and fast extension of the method for the data-free compression of computer vision models, using synthetic fractal image data, that allows getting state-of-the-art accuracy on a range of natural image datasets.  Figure 1: Flowchart of the proposed post-training sparsity pipeline. The process begins with a small initial global sparsity value, the model is fine-tuned in a layerwise manner, the sparsity level is increased and the fine-tuning is repeated. This iterative process is carried out until the target sparsity level or the maximal allowed accuracy drop is reached (either of these parameters is set in advance).\n\n\nPost-training sparsity pipeline\n\nThe proposed post-training sparsity pipeline consists of three basic steps: (i) layer-wise sparsity rate selection given a global sparsity constraint, (ii) bias & variance correction steps, and (iii) layer-wise finetuning using auxiliary knowledge distillation losses. We introduce a progressively increasing sparsity schedule for each layer whereby these three steps are performed iteratively and the global sparsity rate in the model is increased on each iteration (see the flowchart in Fig. 1). The global sparsity rate for the model on the t th iteration of the pipeline is determined via the following polynomial (cubic) sparsity schedule [23]:\ns t = s f + (s i \u2212 s f ) 1 \u2212 t T 3\nwhere s i and s f are the initial and final global sparsity rates of the model, respectively, and T is the total number of iterations of the pipeline. After the original floating-point precision model with the target global sparsity rate is obtained, the standard procedure of post-training quantization is performed to prepare the model to be executed in 8-bit precision. We found that performing post-training quantization on  Fig. 2 for results on ResNet18/50 on Im-ageNet), probably due to reduced quantization noise of sparse weight matrices. We are using the following quantization configuration throughout the paper: symmetric per-tensor quantization of activations (except for specific per-channel cases like e.g. depthwise convolutions) and symmetric per-channel quantization of weights. We further provide details on all the steps performed on every iteration of the pruning pipeline in the corresponding sections below.\n\nLayer-wise sparsity rate selection procedure\n\nThe problem of selecting an optimal (in terms of model accuracy) layer-wise sparsity rate configuration given a certain global sparsity constraint is a widely discussed problem in the literature [13]. The proposed approaches range from simple heuristics (e.g. pruning uniformly except for the first and the last layers in the network [4]) to making per-layer sparsity rates learnable [12] or searching for the best configuration via global non-gradient optimization or reinforcement learning [7]. The heuristic approaches also include finding a global threshold for weight importance scores and pruning all the weights with importances below this threshold. This naturally leads to a non-uniform pattern of per-layer sparsity rates. The importance score function in this case might be the absolute weight magnitude or a normalized version thereof (like e.g. the LAMP score [13]). We compared several variations of importance score functions in the global threshold approach, namely (i) the absolute weight magnitude, (ii) the absolute weight magnitude normalized by the L2 norm of the corresponding layer, (iii) the LAMP score (Table 3). We initally observed that the global magnitude criterion led to much worse accuracy comprared to the normalized criteria (Table 3). This effect was found to be caused by the fusing of the BatchNorm layers into preceding convolutions, which was performed in the model prior to compression. BatchNorm fusing resulted in different layer-wise weight scales compared to the original model, an effect easily counteracted by per-layer normalization of weight magnitudes. In the case where the normalization layers were not fused into convolutions, however, we found that the vanilla global magnitude criterion performed the best compared to LAMP and L2-normalized magnitude (Table 3 and Figure 3). We further assumed that the fusing could generally occur prior to model compression and the original normalization layer parameters might be unknown, hence we picked the per-layer L2-normalized magnitude criterion as our sparsity rate selection heuristic. It performed better than LAMP in our post-training scenario on most of the models with BatchNorm fusing. The weight importance criterion for the i th weight in the l th layer w l i we use in our pipeline thus reads\nI(w l i ) = |w l i | j\u2208l |w l j | 2\nWe pool the importance scores from all the layers and find the threshold value corresponding to the set sparsity rate. The weights with importance values below the threshold are pruned.\n\n\nWeight and activation bias correction\n\nOnce the layer-wise pruning rates have been determined, the weights are zeroed out based on the intralayer absolute magnitudes. This pruning operation distorts the weight distribution, introducing bias and scale shifts. It is beneficial to carry out a bias correction procedure on the weights in order to restore the original mean and variance values in all of the convolutional layer filters and fully-connected layer weight matrices [1]. We perform the following affine transformation on all of the pruned weight tensors in a per-channel/perfeature fashion:\nW s corr = \u03bbW s + E(W dense ) \u2212 E(\u03bbW s ) \u03bb = \u03c3(W dense ) \u03c3(W s ) + where W s\ncorr is the weight tensor after the correction procedure, and W s and W dense are the weight tensors of sparse and original dense models, respectively, and E and \u03c3 are the mean and standard deviation operators, = 10 \u22129 is a small constant added for numerical stability. The resulting sparse weight tensor has ImageNet top@1 accuracy drop, % magnitude L2-normalized magnitude LAMP Figure 3: Absolute top@1 accuracy drops for a range of models from the torchvision package pruned with a 50% global sparsity rate depending on the per-layer compression level selection criterion. BatchNorm fusing was not performed in the networks. Pruning is done in the post-training regime (without any fine-tuning), BatchNorm adaptation is performed after weight pruning. Global magnitude criterion is optimal in most cases in this setting except for lightweight models such as MobileNets, SqueezeNets and ShuffleNets. BatchNorm adaptation is a procedure analogous to bias correction whereby the BN statistics are recollected after the model has been compressed [14]. Accuracy drops are measured relative to the original pre-trained weights using PyTorch on a GPU. the same mean and variance values as original dense model weights for each output kernel/feature, since this correction is applied to every output feature independently. Output activations at each pruned layer are also suffering from a bias introduced by the zeroed weights, which can be compensated by altering the bias parameters of the convolutional and fully-connected layers. Nagel et al. [18] proposed to perform this operation to mitigate biases introduced by quantization in an iterative fashion, correcting the first layer and then calculating the bias shift factors for the second layer using this corrected model. We found that a one-shot version of the bias correction procedure was sufficient for post-training sparsity, whereby we perform a forward pass of the original model and calculate the input activation tensors X dense for each layer. The corrected bias parameters are then determined as\nb corr = b dense + E(f (W dense , X dense ))\u2212 E(f (W s\ncorr , X dense )) where f (W, X) is the convolutional or matrix-multiply operation of the layer acting on inputs X with weights W , b dense are the original bias values in the layer, X dense is the set of input activation tensors for the corresponding layer in the original dense model. In other words, we are using the input tensors from the original model to calculate bias shifts, not from the iteratively corrected compressed model. We found no significant difference in the resulting accuracy between the two approaches, with the one-shot one being faster since it requires a single forward pass of the model. Results of the weight & activation bias correction procedures are shown in Table 4 for a ResNet18 model at 50% sparsity rate. Both procedures cumulatively improve the pruned model accuracy and top@1 accuracy drops are not exceeding several percent for many ImageNet models at the sparsity rate of 50% just after layer-wise sparsity selection and bias correction. Accuracy can be further improved by local layer-wise fine-tuning using auxiliary knowledge-distillation losses, which is described in more detail below.  \n\n\nLocal layer-wise fine-tuning with auxiliary losses\n\nGradient-based fine-tuning with auxiliary loss functions was previously successfully applied for posttraining quantization [17,15,9] and, to a smaller extent, to post-training weight pruning [8] and even filter pruning in convolutional networks [5]. In this work, we follow a similar approach whereby we define a local knowledge distillation loss for every pruned layer (see Figure 4). These losses serve as a measure of how close the output activation feature maps of the original (unpruned) and pruned layers are. Suppose the pruned layer weights and biases are W s and b s , then the knowledge distillation mean-squared error loss for that layer is defined as\nL = i\u2208batch (Y i dense \u2212 f (W s M s , X i dense ) \u2212 b s ) 2 Y i dense = f (W dense , X i dense )\nwhere f (X, W ) is the convolutional/matrix-multiply operation represented by the layer with weights W acting on inputs X. The input tensors X dense used to calculate the output activations above are constructed by running a forward pass of the original, unpruned model. M s is the binary mask layer which is equal to one if the corresponding weight is not pruned and zero otherwise. We fix the sparse binary mask and run a gradient descent of the loss functions defined above for each layer independently to find the optimal weights and biases W s , b s .\n\n\nAblation study\n\nLayer-wise fine-tuning settings. We run several ablation experiments to establish the best optimization settings for the layer-wise fine-tuning procedure, since it is inherently different from full model training via backpropagation. We used a batch size of 50 samples in our experiments, and found the optimal learning rate values across different models to be 10 \u22125 for weights and 10 \u22124 for bias parameters. We found that using the Adam optimizer outperforms alternatives, such as SGD with momentum or Adadelta, and techniques for better generalization in the vicinity of local minima, like Lookahead [22] and Stochastic Weight Averaging [10] were also not found to be beneficial in the layerwise fine-tuning case. The MSE loss function also was found to be a better choice than e.g. L1 loss, Huber loss or cosine similarity between feature maps. We did not observe significant over-fitting present in layer-wise optimization (we discuss this phenomenon in more details below) and in particular we found that even low values of weight decay/L2 weight regularization strength such as 10 \u22126 could hurt the resulting model accuracy (see Table 5). Thus, we set the weight decay strength to 0 in all our experiments. Increasing model sparsity rate using a cubic schedule throughout the pruning pipeline also turned out to improve accuracy for most models compared to the constant sparsity baseline (Table 6). Overall, we were able to prune and quantize a wide range of models with resulting sparsity rates ranging from 40% to 70% and an absolute accuracy drop not exceeding or close to 1% with our layer-wise fine-tuning recipe using images from the respective models' training datasets (Table 1 and Figure 2).\n\nSource of input data used for fine-tuning. Throughout experiments, we noticed that the set of input sam- ples to be used for fine-tuning does not have to be necessarily large (we used a pool of randomly selected several hundred samples in our experiments) and can come either from the training or the validation dataset, with no significant accuracy difference between the two (see Table 2). These results suggest that this finetuning regime is not as prone to over-fitting compared to full model training, a fact that was also previously reported for layer-wise tuning of a quantized model [9]. We observed no difference between fine-tuning on a batch of training or validation samples not only for the ImageNet dataset but also for object detection models trained on the Pascal VOC, COCO and WIDER FACE datasets. This lack of over-fitting is not surprising since no annotation is used during fine-tuning and all the layers are optimized independently, which reduces the amount of tuned parameters per single optimization problem. The amount of supervision signal is also high because the difference between whole activation tensors produced by a set of input samples is used as a loss function. We further verified whether we could utilize arbitrary input data for bias correction and layer-wise fine-tuning, not related to the original dataset that the model has been trained and tested on. The intuition behind this is that once the output activation feature maps produced by these arbitrary data are similar to the ones produced by running model inference on its original dataset, the layer-wise fine-tuning procedure could produce a model that is sufficiently accurate on the validation data. In particular, we tested several computer vision models trained on different datasets (ImageNet, Pascal VOC, COCO, WIDER FACE; see Table 2); the models were pruned using our posttraining pipeline, but the input images used to calculate activation statistics and feature maps for finetuning consisted of synthetically generated white noise (each pixel value in every color channel is independently sampled from a uniform distribution from 0 to 255). We observed certain accuracy degradation when fine-tuning on white noise images compared to tuning on original data, but typically not exceeding several percent. We further tested whether these results could be improved by using synthetic images producing activation distributions closer to those generated by natural images in corresponding datasets. We took images from the FractalDB-1k dataset [11], which is comprised of automatically-generated grayscale images of fractals. These images and their generated annotation were used to pre-train strong backbones for computer vision, including Vision Transformers [11,19]. We found that using these fractal images as input samples to computer vision models during post-training weight pruning significantly improves the resulting model accuracy compared to the white noise baseline (Table 2). We took the original 512x512 images from FractalDB-1k, randomly colored them by performing random shift-scale operations on the color channels and used the same pre-processing strategy as for the original datasets that the models were trained on. Overall, we were able to achieve an accuracy degradation of absolute 1-3% at 50% sparsity rates in the data-free pruning regime by using synthetically-generated fractal images as model inputs. The results generalized beyond ImageNet to other natural image datasets like Pascal VOC, COCO and WIDER FACE. Random colorization of FractalDB images consistently yielded better accuracy compared to using the (original) grayscale images ( Table 7). The proposed data-free pruning approach leads to better accuracy values compared to the existing state-of-the-art [8] and is also fast and less restrictive since it does not include a resource-consuming data distillation process that relies on backpropagation through the model graph.\n\n\nConclusion\n\nIn this work, we have presented a novel post-training pruning recipe for deep neural networks that allows zeroing out a significant proportion of model weights without significant accuracy drops. We demonstrated Table 7: Accuracy of models pruned in the post-training regime using different types of synthetic data: randomlycolored FractalDB1k images, (original) grayscale FractalDB-1k, and generated white noise images. Note that colorization of FractalDB images leads to increased resulting accuracy in most models. efficiency of the proposed pipeline on ImageNet models, object detection models on Pascal VOC and COCO datasets as well as deep NLP and recommendation models. We proposed a data-free formulation of the method by using synthetic fractal images to compress computer vision models, which led to state-of-the-art results in data-free weight pruning. We demonstrated that the proposed pruning method can also be safely combined with post-training quantizaton, further increasing its applicability in production settings.\n\nFigure 2 :\n2Accuracy drop/sparsity rate curves for a ResNet18 (top) and a ResNet50 (bottom) model obtained with our post-training pruning and quantization pipeline. The horizontal dashed red line indicates the level of 1% absolute accuracy drop. Note that posttraining quantization of the pruned model does not lead to a huge accuracy drop increase for both models at different sparsity rate levels.\n\nFigure 4 :\n4Schematic description of the layerwise finetuning approach for post-training sparse model calibration. Input and dense model output tensors are precomputed and stored in memory for each tuned layer. The red arrows depict the local flow of gradients during weight and bias optimization. Red pixels indicate sparsity masks in the compressed layers (sparsity levels are individually selected for each layer).\n\nTable 1 :\n1Accuracy values of the sparse 8-bit quantized DNN models obtained with the proposed post-training method. Metric values were measured on a CPU with OpenVINO TM 2021.3 as an inference engine. The same is for other accuracy values reported in the paper unless specified otherwise.Model \nDataset (acc. metric) \nSparsity rate, % Compressed model acc. Absolute acc. drop \nResNet50 \nImageNet (top@1 acc.) \n65 \n75.09 \n1.04 \nResNet18 \nImageNet (top@1 acc.) \n50 \n68.93 \n0.81 \nGoogleNetV4 \nImageNet (top@1 acc.) \n50 \n78.96 \n0.94 \nMobileNetV2 \nImageNet (top@1 acc.) \n40 \n70.29 \n1.51 \nMobileNetV1-SSD \nVOC07 (mAP) \n50 \n71.53 \n0.98 \nTinyYOLOv2 \nCOCO (AP) \n50 \n28.29 \n0.83 \nNCF \nMovieLens 20M (hit ratio) \n70 \n64.67 \n0.93 \nBERT-base \nMRPC (acc.) \n50 \n82.50 \n0.63 \n\n\n\nTable 2 :\n2Accuracy of sparse computer vision models obtained with layer-wise fine-tuning on different input data. Note that accuracy levels are very similar when fine-tuning on original validation or training datasets, suggesting the absence of overfitting during layer-wise fine-tuning. \"FractalDB-1k(c)\" denotes the colored FractalDB-1k dataset.Model (sparsity rate, dataset/acc. metric) \nOrig. \nmodel \nacc. \n\nVal. data Training \ndata \n\nFractalDB \n1k(c) \n\nWhite \nnoise \n\nResNet18 (50%; ImageNet top@1) \n69.75 \n68.94 \n68.92 \n68.27 \n66.90 \nResNet50 (50%; ImageNet top@1) \n76.13 \n75.51 \n75.57 \n74.50 \n73.89 \nMobileNetV2 (40%; ImageNet top@1) \n71.81 \n70.04 \n70.12 \n68.94 \n66.84 \nMobileNetV1-SSD (50%; VOC07 mAP) \n72.51 \n71.37 \n71.53 \n71.13 \n69.52 \nTinyYOLOv2 (50%; COCO AP) \n29.12 \n28.06 \n28.29 \n28.18 \n27.10 \nRetinaFace-ResNet50 (50%; WIDER FACE mAP) 87.29 \n87.40 \n87.41 \n87.45 \n86.87 \n\n\n\nTable 3 :\n3Impact of per-layer sparsity selection criteria \non a pre-trained ResNet50 model on ImageNet with \n50% of weights pruned. \n\nSparsity selection criterion BN-\nfusing \n\nTop@1 \naccuracy, \n% \n\nResNet50 50% sparsity \nOriginal model \n76.13 \nMagnitude \nYes \n0.3814 \nL2-normalized magnitude \nYes \n72.544 \nLAMP \nYes \n72.328 \nMagnitude \nNo \n72.831 \nL2-normalized magnitude \nNo \n72.244 \nResNet18 50% sparsity \nOriginal model \n69.75 \nMagnitude \nYes \n0.418 \nL2-normalized magnitude \nYes \n64.856 \nLAMP \nYes \n64.866 \nMobileNetV2 30% sparsity \n(with bias correction) \nOriginal model \n71.81 \nMagnitude \nYes \n18.386 \nL2-normalized magnitude \nYes \n69.528 \nLAMP \nYes \n69.524 \n\nthe pruned model does not incur significant accuracy \ndegradation compared to the original-precision sparse \nmodel (see \n\nTable 4 :\n4Impact of bias & variance correction for \nweights and activations on a ResNet18 model on Ima-\ngeNet with 50% of the weights pruned. \n\nResNet18 50% sparse \nTop@1 \naccu-\nracy \n\nTop@5 \naccu-\nracy \n\nL2-normalized magnitude \n64.856 86.126 \nL2-normalized magnitude \n(+ act. bias correction) \n66.852 87.438 \nL2-normalized magnitude \n(+ act. & weight bias correction) 67.46 87.794 \n\n\n\nTable 5 :\n5Impact of L2 weight regularization on the fine-tuned model accuracy for a ResNet18 model at 50% sparsity on ImageNet. Even small weight decay values result in significant accuracy loss compared to the baseline with no regularization in place.L2-reg. strength Top@1 / Top@5 accuracy, % \n\n\u03bb = 0.0 \n68.97 / 88.77 \n\u03bb = 1e-6 \n67.78 / 87.96 \n\u03bb = 1e-5 \n38.14 / 64.49 \n\n\n\nTable 6 :\n6Impact of the sparsity schedule vs. constant sparsity throughout fine-tuning. All metrics are reported at 50% sparsity rates. The cubic schedule was initialized at 10% sparsity rate which was increased in 10 iterations. The same number of optimizer steps was used for both fine-tuning modes.Model \nTop@1 accuracy \nw/o schedule \n\nTop@1 accuracy \nwith schedule \n\nResNet18 \n68.76 \n68.91 \nResNet50 \n75.43 \n75.60 \nGoogleNetV4 79.37 \n78.10 \n\n\n\n\nModel (sparsity rate, dataset/acc. metric) FractalDB-1k(c)FractalDB-1k \nWhite noise \n\nResNet18 (50%; ImageNet top@1) \n68.27 \n67.94 \n66.90 \nResNet50 (50%; ImageNet top@1) \n74.50 \n74.46 \n73.89 \nMobileNetV2 (40%; ImageNet top@1) \n68.94 \n68.47 \n66.84 \nMobileNetV1-SSD (50%; VOC07 mAP) \n71.13 \n70.79 \n69.52 \nTinyYOLOv2 (50%; COCO AP) \n28.18 \n28.28 \n27.10 \n\n\n\nPost-training 4-bit quantization of convolution networks for rapid-deployment. R Banner, Y Nahshan, E Hoffer, D Soudry, arXiv:1810.05723arXiv preprintR. Banner, Y. Nahshan, E. Hoffer, and D. Soudry. Post-training 4-bit quantization of convolution networks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018.\n\nD Blalock, J J G Ortiz, J Frankle, J Guttag, arXiv:2003.03033What is the state of neural network pruning. arXiv preprintD. Blalock, J. J. G. Ortiz, J. Frankle, and J. Guttag. What is the state of neural network pruning? arXiv preprint arXiv:2003.03033, 2020.\n\nFast sparse convnets. E Elsen, M Dukhan, T Gale, K Simonyan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionE. Elsen, M. Dukhan, T. Gale, and K. Simonyan. Fast sparse convnets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pages 14629-14638, 2020.\n\nT Gale, E Elsen, S Hooker, arXiv:1902.09574The state of sparsity in deep neural networks. arXiv preprintT. Gale, E. Elsen, and S. Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.\n\nWootz: A compilerbased framework for fast cnn pruning via composability. H Guan, X Shen, S.-H Lim, Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation. the 40th ACM SIGPLAN Conference on Programming Language Design and ImplementationH. Guan, X. Shen, and S.-H. Lim. Wootz: A compiler- based framework for fast cnn pruning via composabil- ity. In Proceedings of the 40th ACM SIGPLAN Con- ference on Programming Language Design and Imple- mentation, pages 717-730, 2019.\n\nAccelerating sparse dnn models without hardware-support via tile-wise sparsity. C Guo, B Y Hsueh, J Leng, Y Qiu, Y Guan, Z Wang, X Jia, X Li, M Guo, Y Zhu, arXiv:2008.13006arXiv preprintC. Guo, B. Y. Hsueh, J. Leng, Y. Qiu, Y. Guan, Z. Wang, X. Jia, X. Li, M. Guo, and Y. Zhu. Accel- erating sparse dnn models without hardware-support via tile-wise sparsity. arXiv preprint arXiv:2008.13006, 2020.\n\nAmc: Automl for model compression and acceleration on mobile devices. Y He, J Lin, Z Liu, H Wang, L.-J Li, S Han, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Con- ference on Computer Vision (ECCV), pages 784-800, 2018.\n\nLayer-wise data-free cnn compression. M Horton, Y Jin, A Farhadi, M Rastegari, arXiv:2011.09058arXiv preprintM. Horton, Y. Jin, A. Farhadi, and M. Rastegari. Layer-wise data-free cnn compression. arXiv preprint arXiv:2011.09058, 2020.\n\nImproving post training neural quantization: Layer-wise calibration and integer programming. I Hubara, Y Nahshan, Y Hanani, R Banner, D Soudry, arXiv:2006.10518arXiv preprintI. Hubara, Y. Nahshan, Y. Hanani, R. Banner, and D. Soudry. Improving post training neural quantiza- tion: Layer-wise calibration and integer programming. arXiv preprint arXiv:2006.10518, 2020.\n\nAveraging weights leads to wider optima and better generalization. P Izmailov, D Podoprikhin, T Garipov, D Vetrov, A G Wilson, arXiv:1803.05407arXiv preprintP. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018.\n\nPre-training without natural images. H Kataoka, K Okayasu, A Matsumoto, E Yamagata, R Yamada, N Inoue, A Nakamura, Y Satoh, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer VisionH. Kataoka, K. Okayasu, A. Matsumoto, E. Yamagata, R. Yamada, N. Inoue, A. Nakamura, and Y. Satoh. Pre-training without natural images. In Proceedings of the Asian Conference on Computer Vision, 2020.\n\nSoft threshold weight reparameterization for learnable sparsity. A Kusupati, V Ramanujan, R Somani, M Wortsman, P Jain, S Kakade, A Farhadi, International Conference on Machine Learning. PMLRA. Kusupati, V. Ramanujan, R. Somani, M. Worts- man, P. Jain, S. Kakade, and A. Farhadi. Soft thresh- old weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pages 5544-5555. PMLR, 2020.\n\nA deeper look at the layerwise sparsity of magnitudebased pruning. J Lee, S Park, S Mo, S Ahn, J Shin, arXiv:2010.07611arXiv preprintJ. Lee, S. Park, S. Mo, S. Ahn, and J. Shin. A deeper look at the layerwise sparsity of magnitude- based pruning. arXiv preprint arXiv:2010.07611, 2020.\n\nEagleeye: Fast subnet evaluation for efficient neural network pruning. B Li, B Wu, J Su, G Wang, European Conference on Computer Vision. SpringerB. Li, B. Wu, J. Su, and G. Wang. Eagleeye: Fast sub- net evaluation for efficient neural network pruning. In European Conference on Computer Vision, pages 639- 654. Springer, 2020.\n\nBrecq: Pushing the limit of post-training quantization by block reconstruction. Y Li, R Gong, X Tan, Y Yang, P Hu, Q Zhang, F Yu, W Wang, S Gu, arXiv:2102.05426arXiv preprintY. Li, R. Gong, X. Tan, Y. Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021.\n\nLayerwise sparse coding for pruned deep neural networks with extreme compression ratio. X Liu, W Li, J Huo, L Yao, Y Gao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34X. Liu, W. Li, J. Huo, L. Yao, and Y. Gao. Layerwise sparse coding for pruned deep neural networks with extreme compression ratio. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 4900-4907, 2020.\n\nUp or down? adaptive rounding for post-training quantization. M Nagel, R A Amjad, M Van Baalen, C Louizos, T Blankevoort, International Conference on Machine Learning. PMLRM. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort. Up or down? adaptive round- ing for post-training quantization. In International Conference on Machine Learning, pages 7197-7206. PMLR, 2020.\n\nData-free quantization through weight equalization and bias correction. M Nagel, M V Baalen, T Blankevoort, M Welling, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionM. Nagel, M. v. Baalen, T. Blankevoort, and M. Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325-1334, 2019.\n\nCan vision transformers learn without natural images?. K Nakashima, H Kataoka, A Matsumoto, K Iwata, N Inoue, arXiv:2103.13023arXiv preprintK. Nakashima, H. Kataoka, A. Matsumoto, K. Iwata, and N. Inoue. Can vision transformers learn with- out natural images? arXiv preprint arXiv:2103.13023, 2021.\n\nWoodfisher: Efficient second-order approximation for neural network compression. S P Singh, D Alistarh, Advances in Neural Information Processing Systems. 33S. P. Singh and D. Alistarh. Woodfisher: Efficient second-order approximation for neural network com- pression. Advances in Neural Information Processing Systems, 33, 2020.\n\nS Srinivas, R V Babu, arXiv:1507.06149Data-free parameter pruning for deep neural networks. arXiv preprintS. Srinivas and R. V. Babu. Data-free parameter pruning for deep neural networks. arXiv preprint arXiv:1507.06149, 2015.\n\nM R Zhang, J Lucas, G Hinton, J Ba, arXiv:1907.08610Lookahead optimizer: k steps forward, 1 step back. arXiv preprintM. R. Zhang, J. Lucas, G. Hinton, and J. Ba. Looka- head optimizer: k steps forward, 1 step back. arXiv preprint arXiv:1907.08610, 2019.\n\nTo prune, or not to prune: exploring the efficacy of pruning for model compression. M Zhu, S Gupta, arXiv:1710.01878arXiv preprintM. Zhu and S. Gupta. To prune, or not to prune: ex- ploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017.\n", "annotations": {"author": "[{\"end\":170,\"start\":72},{\"end\":271,\"start\":171},{\"end\":368,\"start\":272}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":77},{\"end\":187,\"start\":181},{\"end\":286,\"start\":279}]", "author_first_name": "[{\"end\":76,\"start\":72},{\"end\":180,\"start\":171},{\"end\":278,\"start\":272}]", "author_affiliation": "[{\"end\":169,\"start\":115},{\"end\":270,\"start\":216},{\"end\":367,\"start\":313}]", "title": "[{\"end\":69,\"start\":1},{\"end\":437,\"start\":369}]", "venue": null, "abstract": "[{\"end\":1223,\"start\":439}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2228,\"start\":2225},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2230,\"start\":2228},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2233,\"start\":2232},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2498,\"start\":2495},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2500,\"start\":2498},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2933,\"start\":2929},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4327,\"start\":4324},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4330,\"start\":4327},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4464,\"start\":4460},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4466,\"start\":4464},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4633,\"start\":4630},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4636,\"start\":4633},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5007,\"start\":5003},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5415,\"start\":5412},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5418,\"start\":5415},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5541,\"start\":5538},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5544,\"start\":5541},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5547,\"start\":5544},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5549,\"start\":5547},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7344,\"start\":7340},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8558,\"start\":8554},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8696,\"start\":8693},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8747,\"start\":8743},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8854,\"start\":8851},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9236,\"start\":9232},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11360,\"start\":11357},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12608,\"start\":12604},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13105,\"start\":13101},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14986,\"start\":14982},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14989,\"start\":14986},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14991,\"start\":14989},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15053,\"start\":15050},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15107,\"start\":15104},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16802,\"start\":16798},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16839,\"start\":16835},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18498,\"start\":18495},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20453,\"start\":20449},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20670,\"start\":20666},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20673,\"start\":20670},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21701,\"start\":21698}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23317,\"start\":22917},{\"attributes\":{\"id\":\"fig_3\"},\"end\":23736,\"start\":23318},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":24500,\"start\":23737},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":25389,\"start\":24501},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":26178,\"start\":25390},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":26566,\"start\":26179},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":26941,\"start\":26567},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":27390,\"start\":26942},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":27745,\"start\":27391}]", "paragraph": "[{\"end\":2618,\"start\":1239},{\"end\":3273,\"start\":2620},{\"end\":3317,\"start\":3275},{\"end\":3428,\"start\":3319},{\"end\":3565,\"start\":3430},{\"end\":3795,\"start\":3567},{\"end\":6660,\"start\":3812},{\"end\":7345,\"start\":6696},{\"end\":8311,\"start\":7381},{\"end\":8357,\"start\":8313},{\"end\":10658,\"start\":8359},{\"end\":10880,\"start\":10695},{\"end\":11481,\"start\":10922},{\"end\":13616,\"start\":11559},{\"end\":14804,\"start\":13672},{\"end\":15521,\"start\":14859},{\"end\":16175,\"start\":15619},{\"end\":17902,\"start\":16194},{\"end\":21868,\"start\":17904},{\"end\":22916,\"start\":21883}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7380,\"start\":7346},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10694,\"start\":10659},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11558,\"start\":11482},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13671,\"start\":13617},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15618,\"start\":15522}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":9495,\"start\":9486},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":9627,\"start\":9618},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":10172,\"start\":10164},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":14369,\"start\":14362},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17338,\"start\":17331},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":17598,\"start\":17590},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17887,\"start\":17879},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18293,\"start\":18286},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19741,\"start\":19734},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20893,\"start\":20884},{\"end\":21582,\"start\":21574},{\"end\":22102,\"start\":22095}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1237,\"start\":1225},{\"attributes\":{\"n\":\"2.\"},\"end\":3810,\"start\":3798},{\"attributes\":{\"n\":\"3.\"},\"end\":6694,\"start\":6663},{\"end\":10920,\"start\":10883},{\"end\":14857,\"start\":14807},{\"end\":16192,\"start\":16178},{\"attributes\":{\"n\":\"4.\"},\"end\":21881,\"start\":21871},{\"end\":22928,\"start\":22918},{\"end\":23329,\"start\":23319},{\"end\":23747,\"start\":23738},{\"end\":24511,\"start\":24502},{\"end\":25400,\"start\":25391},{\"end\":26189,\"start\":26180},{\"end\":26577,\"start\":26568},{\"end\":26952,\"start\":26943}]", "table": "[{\"end\":24500,\"start\":24027},{\"end\":25389,\"start\":24850},{\"end\":26178,\"start\":25402},{\"end\":26566,\"start\":26191},{\"end\":26941,\"start\":26821},{\"end\":27390,\"start\":27245},{\"end\":27745,\"start\":27451}]", "figure_caption": "[{\"end\":23317,\"start\":22930},{\"end\":23736,\"start\":23331},{\"end\":24027,\"start\":23749},{\"end\":24850,\"start\":24513},{\"end\":26821,\"start\":26579},{\"end\":27245,\"start\":26954},{\"end\":27451,\"start\":27393}]", "figure_ref": "[{\"end\":6259,\"start\":6251},{\"end\":7191,\"start\":7185},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7816,\"start\":7810},{\"end\":10186,\"start\":10177},{\"end\":11947,\"start\":11939},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15242,\"start\":15234},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17900,\"start\":17892}]", "bib_author_first_name": "[{\"end\":27827,\"start\":27826},{\"end\":27837,\"start\":27836},{\"end\":27848,\"start\":27847},{\"end\":27858,\"start\":27857},{\"end\":28066,\"start\":28065},{\"end\":28077,\"start\":28076},{\"end\":28081,\"start\":28078},{\"end\":28090,\"start\":28089},{\"end\":28101,\"start\":28100},{\"end\":28348,\"start\":28347},{\"end\":28357,\"start\":28356},{\"end\":28367,\"start\":28366},{\"end\":28375,\"start\":28374},{\"end\":28719,\"start\":28718},{\"end\":28727,\"start\":28726},{\"end\":28736,\"start\":28735},{\"end\":29017,\"start\":29016},{\"end\":29025,\"start\":29024},{\"end\":29036,\"start\":29032},{\"end\":29539,\"start\":29538},{\"end\":29546,\"start\":29545},{\"end\":29548,\"start\":29547},{\"end\":29557,\"start\":29556},{\"end\":29565,\"start\":29564},{\"end\":29572,\"start\":29571},{\"end\":29580,\"start\":29579},{\"end\":29588,\"start\":29587},{\"end\":29595,\"start\":29594},{\"end\":29601,\"start\":29600},{\"end\":29608,\"start\":29607},{\"end\":29928,\"start\":29927},{\"end\":29934,\"start\":29933},{\"end\":29941,\"start\":29940},{\"end\":29948,\"start\":29947},{\"end\":29959,\"start\":29955},{\"end\":29965,\"start\":29964},{\"end\":30342,\"start\":30341},{\"end\":30352,\"start\":30351},{\"end\":30359,\"start\":30358},{\"end\":30370,\"start\":30369},{\"end\":30633,\"start\":30632},{\"end\":30643,\"start\":30642},{\"end\":30654,\"start\":30653},{\"end\":30664,\"start\":30663},{\"end\":30674,\"start\":30673},{\"end\":30976,\"start\":30975},{\"end\":30988,\"start\":30987},{\"end\":31003,\"start\":31002},{\"end\":31014,\"start\":31013},{\"end\":31024,\"start\":31023},{\"end\":31026,\"start\":31025},{\"end\":31280,\"start\":31279},{\"end\":31291,\"start\":31290},{\"end\":31302,\"start\":31301},{\"end\":31315,\"start\":31314},{\"end\":31327,\"start\":31326},{\"end\":31337,\"start\":31336},{\"end\":31346,\"start\":31345},{\"end\":31358,\"start\":31357},{\"end\":31729,\"start\":31728},{\"end\":31741,\"start\":31740},{\"end\":31754,\"start\":31753},{\"end\":31764,\"start\":31763},{\"end\":31776,\"start\":31775},{\"end\":31784,\"start\":31783},{\"end\":31794,\"start\":31793},{\"end\":32157,\"start\":32156},{\"end\":32164,\"start\":32163},{\"end\":32172,\"start\":32171},{\"end\":32178,\"start\":32177},{\"end\":32185,\"start\":32184},{\"end\":32448,\"start\":32447},{\"end\":32454,\"start\":32453},{\"end\":32460,\"start\":32459},{\"end\":32466,\"start\":32465},{\"end\":32785,\"start\":32784},{\"end\":32791,\"start\":32790},{\"end\":32799,\"start\":32798},{\"end\":32806,\"start\":32805},{\"end\":32814,\"start\":32813},{\"end\":32820,\"start\":32819},{\"end\":32829,\"start\":32828},{\"end\":32835,\"start\":32834},{\"end\":32843,\"start\":32842},{\"end\":33164,\"start\":33163},{\"end\":33171,\"start\":33170},{\"end\":33177,\"start\":33176},{\"end\":33184,\"start\":33183},{\"end\":33191,\"start\":33190},{\"end\":33603,\"start\":33602},{\"end\":33612,\"start\":33611},{\"end\":33614,\"start\":33613},{\"end\":33623,\"start\":33622},{\"end\":33637,\"start\":33636},{\"end\":33648,\"start\":33647},{\"end\":33998,\"start\":33997},{\"end\":34007,\"start\":34006},{\"end\":34009,\"start\":34008},{\"end\":34019,\"start\":34018},{\"end\":34034,\"start\":34033},{\"end\":34457,\"start\":34456},{\"end\":34470,\"start\":34469},{\"end\":34481,\"start\":34480},{\"end\":34494,\"start\":34493},{\"end\":34503,\"start\":34502},{\"end\":34783,\"start\":34782},{\"end\":34785,\"start\":34784},{\"end\":34794,\"start\":34793},{\"end\":35033,\"start\":35032},{\"end\":35045,\"start\":35044},{\"end\":35047,\"start\":35046},{\"end\":35261,\"start\":35260},{\"end\":35263,\"start\":35262},{\"end\":35272,\"start\":35271},{\"end\":35281,\"start\":35280},{\"end\":35291,\"start\":35290},{\"end\":35600,\"start\":35599},{\"end\":35607,\"start\":35606}]", "bib_author_last_name": "[{\"end\":27834,\"start\":27828},{\"end\":27845,\"start\":27838},{\"end\":27855,\"start\":27849},{\"end\":27865,\"start\":27859},{\"end\":28074,\"start\":28067},{\"end\":28087,\"start\":28082},{\"end\":28098,\"start\":28091},{\"end\":28108,\"start\":28102},{\"end\":28354,\"start\":28349},{\"end\":28364,\"start\":28358},{\"end\":28372,\"start\":28368},{\"end\":28384,\"start\":28376},{\"end\":28724,\"start\":28720},{\"end\":28733,\"start\":28728},{\"end\":28743,\"start\":28737},{\"end\":29022,\"start\":29018},{\"end\":29030,\"start\":29026},{\"end\":29040,\"start\":29037},{\"end\":29543,\"start\":29540},{\"end\":29554,\"start\":29549},{\"end\":29562,\"start\":29558},{\"end\":29569,\"start\":29566},{\"end\":29577,\"start\":29573},{\"end\":29585,\"start\":29581},{\"end\":29592,\"start\":29589},{\"end\":29598,\"start\":29596},{\"end\":29605,\"start\":29602},{\"end\":29612,\"start\":29609},{\"end\":29931,\"start\":29929},{\"end\":29938,\"start\":29935},{\"end\":29945,\"start\":29942},{\"end\":29953,\"start\":29949},{\"end\":29962,\"start\":29960},{\"end\":29969,\"start\":29966},{\"end\":30349,\"start\":30343},{\"end\":30356,\"start\":30353},{\"end\":30367,\"start\":30360},{\"end\":30380,\"start\":30371},{\"end\":30640,\"start\":30634},{\"end\":30651,\"start\":30644},{\"end\":30661,\"start\":30655},{\"end\":30671,\"start\":30665},{\"end\":30681,\"start\":30675},{\"end\":30985,\"start\":30977},{\"end\":31000,\"start\":30989},{\"end\":31011,\"start\":31004},{\"end\":31021,\"start\":31015},{\"end\":31033,\"start\":31027},{\"end\":31288,\"start\":31281},{\"end\":31299,\"start\":31292},{\"end\":31312,\"start\":31303},{\"end\":31324,\"start\":31316},{\"end\":31334,\"start\":31328},{\"end\":31343,\"start\":31338},{\"end\":31355,\"start\":31347},{\"end\":31364,\"start\":31359},{\"end\":31738,\"start\":31730},{\"end\":31751,\"start\":31742},{\"end\":31761,\"start\":31755},{\"end\":31773,\"start\":31765},{\"end\":31781,\"start\":31777},{\"end\":31791,\"start\":31785},{\"end\":31802,\"start\":31795},{\"end\":32161,\"start\":32158},{\"end\":32169,\"start\":32165},{\"end\":32175,\"start\":32173},{\"end\":32182,\"start\":32179},{\"end\":32190,\"start\":32186},{\"end\":32451,\"start\":32449},{\"end\":32457,\"start\":32455},{\"end\":32463,\"start\":32461},{\"end\":32471,\"start\":32467},{\"end\":32788,\"start\":32786},{\"end\":32796,\"start\":32792},{\"end\":32803,\"start\":32800},{\"end\":32811,\"start\":32807},{\"end\":32817,\"start\":32815},{\"end\":32826,\"start\":32821},{\"end\":32832,\"start\":32830},{\"end\":32840,\"start\":32836},{\"end\":32846,\"start\":32844},{\"end\":33168,\"start\":33165},{\"end\":33174,\"start\":33172},{\"end\":33181,\"start\":33178},{\"end\":33188,\"start\":33185},{\"end\":33195,\"start\":33192},{\"end\":33609,\"start\":33604},{\"end\":33620,\"start\":33615},{\"end\":33634,\"start\":33624},{\"end\":33645,\"start\":33638},{\"end\":33660,\"start\":33649},{\"end\":34004,\"start\":33999},{\"end\":34016,\"start\":34010},{\"end\":34031,\"start\":34020},{\"end\":34042,\"start\":34035},{\"end\":34467,\"start\":34458},{\"end\":34478,\"start\":34471},{\"end\":34491,\"start\":34482},{\"end\":34500,\"start\":34495},{\"end\":34509,\"start\":34504},{\"end\":34791,\"start\":34786},{\"end\":34803,\"start\":34795},{\"end\":35042,\"start\":35034},{\"end\":35052,\"start\":35048},{\"end\":35269,\"start\":35264},{\"end\":35278,\"start\":35273},{\"end\":35288,\"start\":35282},{\"end\":35294,\"start\":35292},{\"end\":35604,\"start\":35601},{\"end\":35613,\"start\":35608}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1810.05723\",\"id\":\"b0\"},\"end\":28063,\"start\":27747},{\"attributes\":{\"doi\":\"arXiv:2003.03033\",\"id\":\"b1\"},\"end\":28323,\"start\":28065},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":208248161},\"end\":28716,\"start\":28325},{\"attributes\":{\"doi\":\"arXiv:1902.09574\",\"id\":\"b3\"},\"end\":28941,\"start\":28718},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":131764428},\"end\":29456,\"start\":28943},{\"attributes\":{\"doi\":\"arXiv:2008.13006\",\"id\":\"b5\"},\"end\":29855,\"start\":29458},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52048008},\"end\":30301,\"start\":29857},{\"attributes\":{\"doi\":\"arXiv:2011.09058\",\"id\":\"b7\"},\"end\":30537,\"start\":30303},{\"attributes\":{\"doi\":\"arXiv:2006.10518\",\"id\":\"b8\"},\"end\":30906,\"start\":30539},{\"attributes\":{\"doi\":\"arXiv:1803.05407\",\"id\":\"b9\"},\"end\":31240,\"start\":30908},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":229321892},\"end\":31661,\"start\":31242},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":211069143},\"end\":32087,\"start\":31663},{\"attributes\":{\"doi\":\"arXiv:2010.07611\",\"id\":\"b12\"},\"end\":32374,\"start\":32089},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":220363814},\"end\":32702,\"start\":32376},{\"attributes\":{\"doi\":\"arXiv:2102.05426\",\"id\":\"b14\"},\"end\":33073,\"start\":32704},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":213355401},\"end\":33538,\"start\":33075},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":216056295},\"end\":33923,\"start\":33540},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":184487878},\"end\":34399,\"start\":33925},{\"attributes\":{\"doi\":\"arXiv:2103.13023\",\"id\":\"b18\"},\"end\":34699,\"start\":34401},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220364055},\"end\":35030,\"start\":34701},{\"attributes\":{\"doi\":\"arXiv:1507.06149\",\"id\":\"b20\"},\"end\":35258,\"start\":35032},{\"attributes\":{\"doi\":\"arXiv:1907.08610\",\"id\":\"b21\"},\"end\":35513,\"start\":35260},{\"attributes\":{\"doi\":\"arXiv:1710.01878\",\"id\":\"b22\"},\"end\":35790,\"start\":35515}]", "bib_title": "[{\"end\":28345,\"start\":28325},{\"end\":29014,\"start\":28943},{\"end\":29925,\"start\":29857},{\"end\":31277,\"start\":31242},{\"end\":31726,\"start\":31663},{\"end\":32445,\"start\":32376},{\"end\":33161,\"start\":33075},{\"end\":33600,\"start\":33540},{\"end\":33995,\"start\":33925},{\"end\":34780,\"start\":34701}]", "bib_author": "[{\"end\":27836,\"start\":27826},{\"end\":27847,\"start\":27836},{\"end\":27857,\"start\":27847},{\"end\":27867,\"start\":27857},{\"end\":28076,\"start\":28065},{\"end\":28089,\"start\":28076},{\"end\":28100,\"start\":28089},{\"end\":28110,\"start\":28100},{\"end\":28356,\"start\":28347},{\"end\":28366,\"start\":28356},{\"end\":28374,\"start\":28366},{\"end\":28386,\"start\":28374},{\"end\":28726,\"start\":28718},{\"end\":28735,\"start\":28726},{\"end\":28745,\"start\":28735},{\"end\":29024,\"start\":29016},{\"end\":29032,\"start\":29024},{\"end\":29042,\"start\":29032},{\"end\":29545,\"start\":29538},{\"end\":29556,\"start\":29545},{\"end\":29564,\"start\":29556},{\"end\":29571,\"start\":29564},{\"end\":29579,\"start\":29571},{\"end\":29587,\"start\":29579},{\"end\":29594,\"start\":29587},{\"end\":29600,\"start\":29594},{\"end\":29607,\"start\":29600},{\"end\":29614,\"start\":29607},{\"end\":29933,\"start\":29927},{\"end\":29940,\"start\":29933},{\"end\":29947,\"start\":29940},{\"end\":29955,\"start\":29947},{\"end\":29964,\"start\":29955},{\"end\":29971,\"start\":29964},{\"end\":30351,\"start\":30341},{\"end\":30358,\"start\":30351},{\"end\":30369,\"start\":30358},{\"end\":30382,\"start\":30369},{\"end\":30642,\"start\":30632},{\"end\":30653,\"start\":30642},{\"end\":30663,\"start\":30653},{\"end\":30673,\"start\":30663},{\"end\":30683,\"start\":30673},{\"end\":30987,\"start\":30975},{\"end\":31002,\"start\":30987},{\"end\":31013,\"start\":31002},{\"end\":31023,\"start\":31013},{\"end\":31035,\"start\":31023},{\"end\":31290,\"start\":31279},{\"end\":31301,\"start\":31290},{\"end\":31314,\"start\":31301},{\"end\":31326,\"start\":31314},{\"end\":31336,\"start\":31326},{\"end\":31345,\"start\":31336},{\"end\":31357,\"start\":31345},{\"end\":31366,\"start\":31357},{\"end\":31740,\"start\":31728},{\"end\":31753,\"start\":31740},{\"end\":31763,\"start\":31753},{\"end\":31775,\"start\":31763},{\"end\":31783,\"start\":31775},{\"end\":31793,\"start\":31783},{\"end\":31804,\"start\":31793},{\"end\":32163,\"start\":32156},{\"end\":32171,\"start\":32163},{\"end\":32177,\"start\":32171},{\"end\":32184,\"start\":32177},{\"end\":32192,\"start\":32184},{\"end\":32453,\"start\":32447},{\"end\":32459,\"start\":32453},{\"end\":32465,\"start\":32459},{\"end\":32473,\"start\":32465},{\"end\":32790,\"start\":32784},{\"end\":32798,\"start\":32790},{\"end\":32805,\"start\":32798},{\"end\":32813,\"start\":32805},{\"end\":32819,\"start\":32813},{\"end\":32828,\"start\":32819},{\"end\":32834,\"start\":32828},{\"end\":32842,\"start\":32834},{\"end\":32848,\"start\":32842},{\"end\":33170,\"start\":33163},{\"end\":33176,\"start\":33170},{\"end\":33183,\"start\":33176},{\"end\":33190,\"start\":33183},{\"end\":33197,\"start\":33190},{\"end\":33611,\"start\":33602},{\"end\":33622,\"start\":33611},{\"end\":33636,\"start\":33622},{\"end\":33647,\"start\":33636},{\"end\":33662,\"start\":33647},{\"end\":34006,\"start\":33997},{\"end\":34018,\"start\":34006},{\"end\":34033,\"start\":34018},{\"end\":34044,\"start\":34033},{\"end\":34469,\"start\":34456},{\"end\":34480,\"start\":34469},{\"end\":34493,\"start\":34480},{\"end\":34502,\"start\":34493},{\"end\":34511,\"start\":34502},{\"end\":34793,\"start\":34782},{\"end\":34805,\"start\":34793},{\"end\":35044,\"start\":35032},{\"end\":35054,\"start\":35044},{\"end\":35271,\"start\":35260},{\"end\":35280,\"start\":35271},{\"end\":35290,\"start\":35280},{\"end\":35296,\"start\":35290},{\"end\":35606,\"start\":35599},{\"end\":35615,\"start\":35606}]", "bib_venue": "[{\"end\":27824,\"start\":27747},{\"end\":28169,\"start\":28126},{\"end\":28467,\"start\":28386},{\"end\":28806,\"start\":28761},{\"end\":29138,\"start\":29042},{\"end\":29536,\"start\":29458},{\"end\":30035,\"start\":29971},{\"end\":30339,\"start\":30303},{\"end\":30630,\"start\":30539},{\"end\":30973,\"start\":30908},{\"end\":31420,\"start\":31366},{\"end\":31848,\"start\":31804},{\"end\":32154,\"start\":32089},{\"end\":32511,\"start\":32473},{\"end\":32782,\"start\":32704},{\"end\":33258,\"start\":33197},{\"end\":33706,\"start\":33662},{\"end\":34115,\"start\":34044},{\"end\":34454,\"start\":34401},{\"end\":34854,\"start\":34805},{\"end\":35122,\"start\":35070},{\"end\":35361,\"start\":35312},{\"end\":35597,\"start\":35515},{\"end\":28535,\"start\":28469},{\"end\":29221,\"start\":29140},{\"end\":30086,\"start\":30037},{\"end\":31461,\"start\":31422},{\"end\":33306,\"start\":33260},{\"end\":34173,\"start\":34117}]"}}}, "year": 2023, "month": 12, "day": 17}