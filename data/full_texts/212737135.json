{"id": 212737135, "updated": "2023-10-06 17:45:09.437", "metadata": {"title": "Feedback Graph Convolutional Network for Skeleton-based Action Recognition", "authors": "[{\"first\":\"Hao\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Yan\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Dong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"YunDa\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"ShaoDi\",\"last\":\"You\",\"middle\":[]},{\"first\":\"Stephen\",\"last\":\"Maybank\",\"middle\":[\"J.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 3, "day": 17}, "abstract": "Skeleton-based action recognition has attracted considerable attention in computer vision since skeleton data is more robust to the dynamic circumstance and complicated background than other modalities. Recently, many researchers have used the Graph Convolutional Network (GCN) to model spatial-temporal features of skeleton sequences by an end-to-end optimization. However, conventional GCNs are feedforward networks which are impossible for low-level layers to access semantic information in the high-level layers. In this paper, we propose a novel network, named Feedback Graph Convolutional Network (FGCN). This is the first work that introduces the feedback mechanism into GCNs and action recognition. Compared with conventional GCNs, FGCN has the following advantages: (1) a multi-stage temporal sampling strategy is designed to extract spatial-temporal features for action recognition in a coarse-to-fine progressive process; (2) A dense connections based Feedback Graph Convolutional Block (FGCB) is proposed to introduce feedback connections into the GCNs. It transmits the high-level semantic features to the low-level layers and flows temporal information stage by stage to progressively model global spatial-temporal features for action recognition; (3) The FGCN model provides early predictions. In the early stages, the model receives partial information about actions. Naturally, its predictions are relatively coarse. The coarse predictions are treated as the prior to guide the feature learning of later stages for a accurate prediction. Extensive experiments on the datasets, NTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA, demonstrate that the proposed FGCN is effective for action recognition. It achieves the state-of-the-art performance on the three datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2003.07564", "mag": "3011934170", "acl": null, "pubmed": "34818190", "pubmedcentral": null, "dblp": "journals/tip/YangYZSLM22", "doi": "10.1109/tip.2021.3129117"}}, "content": {"source": {"pdf_hash": "24ea00bd9e737c7dd0d66f1e2706ad97841eb9c8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.07564v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://eprints.bbk.ac.uk/id/eprint/46540/1/FeedBackGraph.pdf", "status": "GREEN"}}, "grobid": {"id": "b83ac3d230e8b09995c43b8065f45cd93ed87a45", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/24ea00bd9e737c7dd0d66f1e2706ad97841eb9c8.txt", "contents": "\nFeedback Graph Convolutional Network for Skeleton-based Action Recognition\n17 Mar 2020\n\nHao Yang yanghao1@nuctech.com \nR&D Center of Artificial Intelligent\nNUCTECH Company Limited\nBeijingChina\n\nIntroduction\n\n\nDan Yan yandan@nuctech.com \nLi Zhang \nR&D Center of Artificial Intelligent\nNUCTECH Company Limited\nBeijingChina\n\nDepartment of Engineering Physics\nTsinghua University\nBeijingChina\n\nIntroduction\n\n\nDong Li li.dong@nuctech.com \nR&D Center of Artificial Intelligent\nNUCTECH Company Limited\nBeijingChina\n\nIntroduction\n\n\nYunda Sun sunyunda@nuctech.com \nR&D Center of Artificial Intelligent\nNUCTECH Company Limited\nBeijingChina\n\nDepartment of Engineering Physics\nTsinghua University\nBeijingChina\n\nIntroduction\n\n\nShaodi You s.you@uva.nl \nR&D Center of Artificial Intelligent\nNUCTECH Company Limited\nBeijingChina\n\nInformatics Institute\nUniversity of Amsterdam. Amsterdam\nNetherlands\n\nIntroduction\n\n\nStephen J Maybank sjmaybank@dcs.bbk.ac.uk \nDepartment of Engineering Physics\nTsinghua University\nBeijingChina\n\nDepartment of Computer Science and Information Systems\nBirkbeck College\nLondonUnited Kingdom\n\nH Yang \nD Yan \nL Zhang \nD Li \nY Sun \nS You \nS Maybank \nFeedback Graph Convolutional Network for Skeleton-based Action Recognition\n17 Mar 2020FeedbackGraph Convolutional NetworkSkeletonAction Recognition\nSkeleton-based action recognition has attracted considerable attention in computer vision since skeleton data is more robust to the dynamic circumstance and complicated background than other modalities. Recently, many researchers have used the Graph Convolutional Network (GCN) to model spatial-temporal features of skeleton sequences by an end-to-end optimization. However, conventional GCNs are feedforward networks which are impossible for low-level layers to access semantic information in the high-level layers. In this paper, we propose a novel network, named Feedback Graph Convolutional Network (FGCN). This is the first work that introduces the feedback mechanism into GCNs and action recognition. Compared with conventional GCNs, FGCN has the following advantages: (1) a multi-stage temporal sampling strategy is designed to extract spatial-temporal features for action recognition in a coarse-to-fine progressive process; (2) A dense connections based Feedback Graph Convolutional Block (FGCB) is proposed to introduce feedback connections into the GCNs. It transmits the high-level semantic features to the low-level layers and flows temporal information stage by stage to progressively model global spatial-temporal features for action recognition; (3) The FGCN model provides early predictions. In the early stages, the model receives partial information about actions. Naturally, its predictions are relatively coarse. The coarse predictions are treated as the prior to guide the feature learning of later stages for a accurate prediction. Extensive experiments on the datasets, NTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA, demonstrate that the proposed FGCN is effective for action recognition. It achieves the state-of-the-art performance on the three datasets.\n\nIntroduction\n\nIn recent years, the quantity of videos uploaded from various terminals has exploded. This has driven imperious demands for human action analysis automatically based on the content of videos. In particular, human action recognition using skeleton has attracted many computer vision researchers because of its strong adaptability to the effects of dynamic circumstance and complicated background, as compared with other modalities such as RGB [16] and optical flow [47]. Early deep learning methods using skeletons for action recognition usually represent the skeleton data as a sequence of joint-coordinate vectors [6,48,58,24] or a pseudo-image [17,32,27] which is then modeled by a RNN or CNN respectively. However, these methods do not explicitly exploit the spatial dependencies among correlated joints, even though the spatial dependencies are informative for understanding human actions. More recently, some methods [56,44,43,23] construct spatial temporal graphs based on the natural connections of joints and temporal edges of consecutive frames. They then exploit a GCN to model spatial-temporal features. However, the conventional GCNs are all single-pass feedforward networks that are fed with the entire skeleton sequence. It is difficult for these methods to extract effective spatial-temporal features, because the useful information is usually buried in the motion-irrelevant or undiscriminating clips when they are fed with entire skeleton sequence. For example, in the action \"kicking something\", most clips are \"standing upright\", and in the action \"wear a shoe\", most clips are a subject sitting on a chair. Then the singlepass feedforward networks can not access the high-level semantic information for the low-level layers. Meanwhile, inputting the entire skeleton sequence increases computational complexity of the model.\n\nMotivated by this, we propose a novel neural network, named Feedback Graph Convolutional Network (FGCN), to extract effective spatial-temporal features from skeleton data in a coarse-to-fine progressive process for action recognition. The FGCN is the first work that introduces feedback mechanism into GCNs and action recognition. Compared with conventional GCNs, the FGCN has a multi-stage temporal sampling strategy which divides input skeleton sequences into multiple stages in the temporal domain and sparsely samples input skeleton clips from temporal stages to avoid feeding with the entire skeleton sequence. Each sampled clip is input into graph convolutional layers to extract local spatial-temporal features for each stage. A Feedback Graph Convolutional Block (FGCB) is proposed to model global spatial-temporal features by fusing the local features. The FGCB is a local dense graph convolutional network with lateral connections from each stage to the next stage and it introduces feedback connections into conventional GCNs. From a semantic point of view it works in a top down manner, which makes it possible for low-level convolutional layers to access semantic information in the high-level layers at each stage. From the temporal domain, the feedback mechanism in FGCB works with a sequence of cause-and-effect and the output of the previous stage flows into the next stage to modulate its input.\n\nAnother advantage of the FGCN is that it provides early predictions of the output in a fraction of the total inference time. This is valuable in many applications such as robotics or autonomous driving, in which latency time is very crucial. The early predictions are a result of the proposed multi-stage coarseto-fine progressively optimization. In the early stages, FGCN is only fed with a part of skeleton sequence and the information about the action is limited, so the inferences of it are relatively coarse. These inferences are treated as a prior to guide the feature learning in later stages. In later stages, the model receives more complete information about the action and the guider of former inferences, thus it outputs more accurate inferences. Several temporal fusion strategies are proposed to fuse the local predictions in temporal stages for a video-level prediction. The strategies enable the network to be optimized in a progressive process.\n\nThe main contributions of this paper are summarized as follows:\n\n-We propose a novel Feedback Graph Convolutional Network (FGCN) for action recognition from skeleton sequences. It models spatial-temporal features by a multi-stage progressive process. To our knowledge, this is the first work that introduces the feedback mechanism into GCNs and action recognition. -We propose a dense connections based Feedback Graph Convolutional Block (FGCB) which is a local network with lateral connections between two temporal stages. Functionally, it transmits high-level semantic features as priors to module its features in low-level layers. -The FGCN model provides early predictions, which benefits from the multistage coarse-to-fine progressive optimization. The proposed model is extensively evaluated on three datasets, NTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA, and it achieves state-of-the-art performance on the three datasets.\n\n\nRelated Works\n\n\nSkeleton based Action Recognition\n\nAs the depth sensor technologies (i.e. kinect [59]) and pose estimation algorithms [52,4] matured, it becomes possible to capture skeleton data in real time by locating the key joints. The skeleton data is robust to illumination change, scene variation, and complex background. These facilitate the data-driven method's development of skeleton-based action recognition. Conventional action recognition methods usually extract hand-crafted features from skeleton sequences. Some traditional methods [8,53,33,40] design several view-invariant features of actions. Examples of these features are body part-based skeletal quads [8,53], group sparsity based class-specific dictionary coding [33], and canonical view transformed features [40]. Other traditional methods integrate the information from different modalities that are always available in 3D action datasets. Some works [13,35,39,54] combine the depth information with the skeleton to improve performance. The depth information is represented by HOG features [13,35] and Fourier Temporal Pyramids [54], or it is modeled by random decision forests [39]. The recent success of deep learning has led to a surge of deep network based skeleton modeling methods. The widely used models are RNNs and CNNs. RNN-based methods [6,48,58,24] usually concatenate all of the joint-coordinates (2D or 3D) in each frame as a vector and then model the features of actions by a RNN fed with a sequence of the coordinate vectors. CNN-based methods [17,32,27] stack the sequence of coordinate vectors to obtain a pseudo-image, and then reduce the action recognition using skeleton sequences to an image classification task. The two-stream based model [60] combines RNN and CNN, operating on coordinate vectors of skeletons and RGB images respectively, to improve performance from a single network. However, these methods do not explicitly model the spatial dependence between correlated joints which is crucial for understanding human actions.\n\n\nGCN based Action Recognition\n\nThe Graph Convolutional Networks (GCNs) [2,34,7,12,19] generalize the convolutional operation to deal with the data with graph construction. There are two main ways of constructing GCNs: spatial perspective and spectral perspective. Spatial perspective methods [2,34] directly perform the convolution filters on the graph vertexes and their neighbors. In contrast, spectral perspective methods [7,12,19] consider the graph convolution as a form of spectral analysis by utilizing the eigenvalues and eigenvectors of the graph Laplacian matrices. This work follows the spatial perspective based methods [56,44,43,23]. The ST-GCN model [56] is proposed to move beyond the limitations of hand-crafted parts and traversal rules used in previous methods. It operates on a spatial temporal graph to model the structured information about the joints along both the spatial and temporal dimensions. Based on ST-GCN, the 2s-AGCN model [44] proposes a two-stream adaptive graph convolutional network, which exploits the second-order information of the skeleton to improve the performance of action recognition. The DGNN model [43] represents the skeleton data as a directed acyclic graph based on the kinematic dependency between the joints and bones.\n\nThe AS-GCN model [23] proposes an actional-structural graph convolution network by generating the skeleton graph with actional links and structural links. However, conventional GCNs are all feedforward networks in which it is impossible for low-level layers to access the semantic information in high-level layers.\n\n\nFeedback Network\n\nFeedback mechanism exists in the human visual cortex [15,9], and it has been a focus of research in psychology [1] and control theory [20,37]. In recent years, feedback mechanism has been introduced into deep neural networks in computer vision [49,57,26,11,10,5], because it allows the network to carry the information of output to correct previous states. In object recognition, the dasNet model [49] exploits the feedback structure by dynamically altering its convolutional filter sensitivities during classification and iteratively focusing its internal attention on some of its convolutional filters. Feedback Network [57] firstly introduces the feedback mechanism into the convolutional recurrent neural network, which transfers the hidden state with high-level information to the input layer. In super resolution, several efforts [26,11,10] are made to take advantage of the feedback mechanism. The DBPN model [11] proposes a deep back-projection network which exploits iterative up-projection and down-projection units to achieve error feedback. The DSRN model [10] proposes a dual-state RNN and transmits the information between two recurrent states via a delayed feedback. The SRFBN model [26] designs a feedback block to handle the feedback connections and refines low-level representations with high-level information. In human pose estimation, [5] proposes an iterative error feedback (IEF) by iteratively estimating and applying a self-correction to the current estimation.\n\n\nThe Method\n\n\nGraph Convolutional Network\n\nGCNs generalize the convolution operation to learn effective representations from graph structured data. In action recognition, the skeleton of a body is defined as an undirected graph in which each joint of the skeleton is defined as a vertex of the graph and the natural connections in the human body are defined as edges of the graph. In this paper, the skeleton in the frame t is denoted as a\ngraph G t = {V t , E t },\nwhere V t is the set of joints in the frame and E t is the set of bones in the skeleton. For 3D skeleton data, the joint set is denoted as\nV t = {v ti } N i=1 , where v ti = (x ti , y ti , z ti ).\nGiven two joints v ti = (x ti , y ti , z ti ) and v tj = (x tj , y tj , z tj ), a bone of the skeleton is defined as a vector e vti,vtj = (x tj \u2212 x ti , y tj \u2212 y ti , z tj \u2212 z ti ), (i, j) \u2208 Q, where Q is the set of naturally connected human body joints. The skeleton sequence with len frames is denoted as S = {G 1 , G 2 , . . . , G len }.\n\nThe graph convolution is defined operating on each vertex and its neighbors. For a vertex v ti in the graph, its neighbor set is denoted as\nN (v ti ) = {v tj |d(v ti , v tj ) \u2264 D}, where d(v ti , v tj )\nis the length of the shortest path from v tj to v ti . We set D = 1 for the 1-distance neighbor set in this paper. The graph convolution operating on the neighbor set of vertex v ti is formulated as:\nf out (v ti ) = vtj \u2208N (vti) 1 Z[l(v tj )] f in (v tj )W [l(v tj )],(1)\nwhere f in and f out denote the input and output feature maps of this convolutional layer. l(v tj ) is the label function which allocates a label from 1 to K for the vertex in N (v ti ). In our experiments, we set K = 3 empirically to divide N (v ti ) into 3 subsets. W (\u00b7) is the weighting function which provides a weight vector according to the label l(v tj ). Similarly, Z[l(v tj )] denotes the number of vertexes corresponding to the subset of l(v tj ).\n\nIn implementation, the connections of a graph are recorded in an N \u00d7 N adjacency matrix A k . With the adjacency matrix, Eqn. 1 can be formulated as: where denotes the dot product and \u039b ii k = j A ij k is a diagonal matrix. W k is the weight vector of the convolution operation, which corresponds to the weighting function W (\u00b7) in Eqn. 1. In practice, A k is allocated with a learnable weight matrix M k which is an N \u00d7 N attention map that indicates the importance of each vertex. It is initialized as an all-one matrix.\nf out = K k=1 W k (\u039b \u2212 1 2 k A k \u039b \u2212 1 2 k f in ) (M k ),(2)\n\nFeedback Graph Convolutional Network\n\nTraditional action recognition methods [56,44,43,23] based on GCNs are all fed with the entire skeleton sequence in a feedforward network. However, the useful information is usually buried in the motion-irrelevant and undiscriminating clips when fed with entire skeleton sequence. And single-pass feedforward networks can not access semantic information at low-level layers. To tackle these problems, we propose a Feedback Graph Convolutional Network (FGCN) which extracts spatial-temporal features by a multi-stage progressive process, as shown in Fig. 1. Specifically, in the FGCN a multi-stage temporal sampling strategy is designed to sparsely sample a sequence of input clips from the skeleton data, instead of operating on the entire skeleton sequence directly. These clips are first fed into graph convolutional layers to extract the local spatial-temporal features. Then, a Feedback Graph Convolutional Block (FGCB) is proposed to fuse the local spatial-temporal features from multiple temporal stages by transmitting the high-level information in the previous stage to the next stage to modulate its input. Finally, several temporal fusion strategies are proposed to fuse the local predictions from all temporal stages to give a video-level prediction.\n\nFormally, given a skeleton sequence S, the multi-stage temporal sampling strategy first divides it into T temporal stages with equal time interval, denoted as S = {s 1 , s 2 , . . . , s T }. In each temporal stage, a skeleton clip is sampled randomly as an input of the deep model, denoted as {c 1 , c 2 , . . . , c T }, where c t is the input clip sampled from the corresponding stage s t . Each sampled clip c t is input to the stacked multiple graph convolutional layers to extract the local spatial-temporal features in the corresponding temporal stage, formulated as:\nF t = f GConvs (c t ),(3)\nwhere t = 1, 2, . . . , T , and F t is the local spatial-temporal features extracted by graph convolutional layers which are denoted as GConvs in Fig. 1.\n\nThe local features extracted from all temporal stages flow into the feedback block FGCB to learn global spatial-temporal features for action recognition. As shown in Fig. 2, FGCB receives two inputs at the stage t: one is the hidden state from the previous stage t \u2212 1, denoted as H t\u22121 ; the other is the local features from the current stage, denoted as F t . Particularly, the input feature at the first stage F 1 is regarded as the initial hidden state H 0 . Based on these two inputs, the feedback process of FGCB is formulated as:\nH t = f F GCB (H t\u22121 , F t ),(4)\nwhere H t is the output of FGCB at stage t, and the function f F GCB (\u00b7) represents the operations of the feedback block FGCB. More details about FGCB can be found in Section 3.3.\n\nFollowing the FGCB, a fully connected layer and a softmax loss layer are used at each stage to predict actions. The prediction process from the output H t of FGCB is formulated as:\nP t = f pred (H t ),(5)\nwhere P t \u2208 R C denotes the local prediction at stage t and C is the number of actions. The function f pred (\u00b7) represents the operations of the fully connected layer and the softmax layer. After operating on T temporal stages, we will obtain totally T local predictions {P 1 , P 2 , . . . , P T }. Several temporal fusion strategies are proposed to fuse these local predictions corresponding to multiple stages for a video-level prediction P S which is computed as:\nP S = f tf (P 1 , P 2 , . . . , P T ),(6)\nwhere f tf is the operations of a temporal fusion strategy. In this paper, we propose three temporal fusion strategies, i.e. last-win-all fusion, average fusion and weighting fusion. The FGCN model is trained end-to-end with the crossentropy loss as follows:\nL(y, P S ) = \u2212 C i=1 y i log(P i S ),(7)\nwhere y is the action label of the skeleton S, if y = i, y i is set as 1, otherwise it is set as 0.\n\n\nGconv GConv Gconv\n\nInput: \nOutput \u22121 1 2 \u22121 Gconv \u2026 , ,, , , , 2 , ,\n\nFeedback Graph Convolutional Block\n\nThe feedback block FGCB is the core component of the FGCN model. On the one hand, the FGCB transmits the high-level semantic information back to lowlevel layers to refine their encoded features. On the other hand, the output at the previous stage flows into the next stage to modulate its input. To enable the FGCB to effectively transmit information from high-level to low-level and from the previous stage to the next stage, we propose a dense connected local graph convolutional network which adds shortcut connections from each layer to all subsequent layers. At a temporal stage t, the FGCB receives the high-level information from the output H t\u22121 of the previous stage to modulate the lowlevel input F t of the current stage. In our model, the FGCB consists of L spatial temporal graph convolutional layers. The spatial temporal graph convolutional layer is denoted as GConv(k s , k t , m) in Fig. 2, where k s and k t are the kernel size in the spatial and temporal domains respectively, and m denotes output channels of the graph convolutional layer. As shown in Fig. 2, the first convolutional layer in FGCB receives two inputs F t and H t\u22121 . It compresses and fuses the features from the concatenation of the two inputs [F t , H t\u22121 ]. The output of this layer is formulated as:\nh 1 t = f 1 F GCB ([F t , H t\u22121 ]),(8)\nwhere f 1 F GCB (\u00b7) denotes the operations in the first convolutional graph layer of FGCB, and h 1 t denotes the output feature maps of the first layer. Following the first layer, the l th layer receives the output feature maps from all preceding layers, h 1 t , h 2 t , . . . , h l\u22121 t , as input:\nh l t = f l F GCB ([h 1 t , h 2 t , . . . , h l\u22121 t ]),(9)\nwhere l = 1, 2, . . . , L and [h 1 t , h 2 t , . . . , h l\u22121 t ] refers to the concatenated feature maps in preceding layers. Similar to the first layer, the final layer in FGCB compresses and fuses the feature maps from the outputs of all preceding layers to produce the output of FGCB:\nH t = h L t = f L F GCB ([h 1 t , h 2 t , . . . , h L\u22121 t ]),(10)\n\nTwo-stream Framework of FGCN\n\nThe joints and bones of a skeleton only contain spatial information of actions, However, many actions are difficult to recognize from the spatial information alone, for example \"wear a shoe\" versus \"take off a shoe\", \"wear on glasses\" versus \"take off glasses\" and etc. Inspired by [43], we model the spatial-temporal features not only exploiting spatial information but the temporal movement information of skeleton sequences. As in Section 3.1, the joint and bone of skeleton are denoted as a vector of coordinates. The movement of a joint or bone is defined as the difference of the vectors for the same joint or bone in consecutive frames along the temporal dimension. Given the joints and bones from two consecutive frames, denoted as v ti , v (t+1)i and e vti,vtj , e v (t+1)i ,v (t+1)j respectively, the movement of joints is defined as mv ti = v (t+1)i \u2212 v ti . Similarly, the movement of bones is defined as me ij t = e v (t+1)i ,v (t+1)j \u2212 e vti,vtj . As the spatial information modeling, the motion information is formulated as a sequence of graphs\nS m = {G m 1 , G m 2 , . . . , G m len }, where G m t = {V m t , E m t }, V m t = {mv ti } N i=1 and E m t = {me ij t } (i,j)\u2208Q .\nIn this paper, the spatial graph S and the motion graph S m are fed into two separate FGCN models to predict action labels. The model fed with spatial graphs S is denoted as FGCN-spatial, the other fed with temporal graphs S m is denoted as FGCN-motion. The two models are finally fused by weighting the output scores of the softmax layers, as shown in Fig. 3.\n\n\nExperiments\n\nIn this section, we evaluate the proposed FGCN method by conducting extensive experiments on three 3D skeleton action datasets, NTU-RGB+D, NTU-RGB+D120, and Northwestern-UCLA. [41] is a widely used dataset for skeleton-based action recognition. The dataset contains more than 56,000 skeleton sequences categorized into 60 action classes. It provides 25 major body joints with 3D coordinates for every human in each frame. Two benchmark evaluations are recommended: crosssubject and cross-view. For cross-subject, both training and test sets consist of 20 subjects, and have 40,320 and 16,560 sequences respectively. The cross-view setup divides the data according to camera views. The training set has 37,920 sequences captured from the front and two side views, while the test set has 18,960 sequences captured from left and right 45 degree views.\n\n\nDatasets\n\n\nNTU-RGB+D\n\nNTU-RGB+D120 [28] is currently the largest in-door captured 3D skeleton dataset. It is an extension of NTU-RGB+D with 120 action classes and more than 114,000 video samples. The newly added action classes make the action recognition more challenging. For example, different actions may have similar body motions but different subjects. There may be fine-grained hand or finger motions and so on. The dataset has 106 subjects and 32 setup IDs. Crosssubject and cross-setup benchmarks are defined. For cross-subject, 53 subjects constitute the training set, and the remaining 53 subjects constitute the test set. Analogously, the 32 setup IDs are also divided equally into two parts for training and testing in cross-setup.\n\nNorthwestern-UCLA [55] is a multi-view 3D event dataset captured simultaneously by three Kinect cameras from different viewpoints. This dataset includes 1494 video sequences covering 10 action categories performed by 10 subjects from 1 to 6 times. It provides 3D spatial coordinates of 20 major body joints. As reported in [55], we pick all samples from the first two cameras for training. The samples from the remaining cameras are for testing.\n\n\nImplementation Details\n\nAll experiments are implemented with PyTorch deep learning framework. A stochastic gradient descent (SGD) optimizer is used during training with the batch size as 32, the momentum as 0.9, and the initial learning rate as 0.1. The learning rate is divided by 10 at the 40th and 60th epoch. The training process ends at the 80th epoch. In our experiments, the input video is divided into five stages temporally and 64 consecutive frames are sampled randomly from each stage to form an input clip. Ten graph convolutional layers are stacked at the front of the feedback block FGCB and these layers have the same configuration as the graph convolutional layers in ST-GCN [56]. The FGCB has four graph convolutional layers (i.e. L = 4). The spatial temporal kernel sizes and output channels of them are set as k s = 3, k t = 3 and m = 256 respectively. \n\n\nAblation Study\n\nIn this section, we design four ablation experiments to evaluate the influence of different hyper-parameters, architecture and inputs on the performance of our FGCN model. These ablation experiments are all conducted on the challenging skeleton dataset NTU-RGB+D.\n\nIn the first experiment, we evaluate the influence of two key hyper-parameters on the performance of our FGCN model, i.e., the number of stages and the length of the input clip in each stage. In Fig. 4(a), the performances of FGCN with different numbers of temporal stages are reported. The FGCN model achieves the best performance when the input video is divided into 6 stages with equal duration. In the subsequent experiments, we set the number of temporal stages at 5, to balance performance against computational cost. Similar performances are obtained with 6 temporal stages. In Fig. 4(b), we evaluate the performance of FGCN fed with different numbers of frames at each stage. Based on the similar model selection strategy in the last experiment, we set the frame length as 64 in the subsequent experiments to balance performance against computational cost.  In the second experiment, we evaluate the effectiveness of different temporal fusion strategies in the FGCN model, i.e. last-win-all fusion, average fusion, and weight fusion. The experiment results are listed in Tab. 1. Among these three fusion strategies, the average fusion strategy achieves the best performance. Based on the results, we use the average fusion strategy to fuse the local predictions for the video-level prediction in the subsequent experiments.\n\nIn the third experiment, we evaluate the effectiveness of the proposed FGCN model fed with joints and bones. We first compare the proposed FGCN model with its baseline ST-GCN model. The two models have the same architecture and configuration of convolutional layers. As shown in the upper part of Tab. 2, the FGCN model fed with joint sequences of skeletons (FGCN-joint) outperforms the baseline model ST-GCN-joint by 5.54% and 5.27% on the cross-subject and cross-view benchmarks respectively. The confusion matrices for the former 30 classes are shown in Fig. 5, and the complete confusion matrices are shown in the supplementary materials. The improvements indicate that introducing feedback mechanism into GCNs is very effective for action recognition. Moreover, we fuse the softmax scores of two FGCN models, where one model is FGCN-joint, the other is FGCN-bone which is fed with the bone sequences. The fusion model FGCN-joint+FGCN-bone achieves a clear improvement, compared with FGCNjoint and FGCN-bone.\n\nIn the fourth experiment, we evaluate the effectiveness of FGCN model fed with the spatial information and the motion information, i.e. FGCN-spatial and FGCN-motion, on the NTU-RGB+D dataset. The experiment results of these two models and their fusion are reported in the under part of Tab. 2. Firstly, the FGCN-spatial model fed with spatial information (joints and bones) achieves 88.32% on cross-subject and 94.82% on cross-view. It is comparable with the performance of the FGCN-joint+FGCN-bone model that fuses the softmax scores of two models. Then, the FGCN-motion fed with the movement of joints and bones achieves 85.96% on cross-subject and 93.57% on cross-view. Finally, we fuse the softmax scores of FGCN-spatial and FGCN-motion. The FGCN-spatial+FGCN-motion achieves 90.22% on cross-subject and 96.25% on cross-view, and it achieves a clear improvement from both of FGCN-spatial and FGCN-motion. Table 3. Comparisons with the state-of-the-art methods on NTU-RGB+D.\n\n\nModels\n\nCross-subject(%) Cross-view(%) ResNet152-3S (ICMEW 2017) [22] 85.0 92.3 ST-GCN (AAAI 2018) [56] 81.5 88.3 DPRL+GCNN (CVPR 2018) [50] 83.5 89.8 SR-TSL (ECCV 2018) [46] 84.8 92.4 PB-GCN (BMVC 2018) [51] 87.5 93.2 Bayesian GC-LSTM (ICCV 2019) [61] 81.8 89.0 AS-GCN (CVPR 2019) [23] 86.8 94.2 AGC-LSTM (CVPR 2019) [45] 89.2 95.0 2s-AGCN (CVPR 2019) [44] 88. \n\n\nComparison with State-of-the-art\n\nIn this section, we compare the performance of the FGCN model with the recent state-of-the-art methods on the NTU-RGB+D dataset, the NTU-RGB+D120 dataset, and the Northwestern-UCLA dataset.\n\nFor the NTU-RGB+D dataset, we display the accuracy of skeleton based action recognition methods, such as CNN-based methods [22], RNN-based methods [46,45,61] and GCN based methods [23,43,44,56]. As shown in Tab. 3, the proposed FGCN model achieves 8.7% and 8.0% improvements on the cross-subject and cross-view benchmarks respectively over the most comparable method ST-GCN [56]. These improvements show the effectiveness of the proposed feedback framework in action recognition. Moreover, the FGCN model outperforms other recent state-of-the-art methods, such as AS-GCN [45], 2s-AGCN [44], and DGNN [43]. Our FGCN model achieves state-of-the-art performance on both cross-subject and cross-view benchmarks of the NTU-RGB+D dataset.  Table 5. Comparisons with the state-of-the-art methods on Northwestern-UCLA.\n\n\nModels\n\nAccuracy(%) Actionlet ensemble (T-PAMI 2013) [54] 76.0 Lie group (CVPR 2014 ) [53] 74.2 HBRNN-L(CVPR 2015) [6] 78.5 Skeleton Visualization (PR 2017) [32] 86.1 Ensemble TS-LSTM (ICCV 2017) [21] 89.2 AGC-LSTM (CVPR 2019) [45] 93.3 JS+JM+BS+BM (ICME 2019) [25] 91.3 HiGCN (ICIG 2019) [14] 88.9 MSNN (CSVT 2020) [42] 89.4 FGCN (ours) 95.3\n\nFor the UTU-RGB+D120 dataset, the results on cross-subject and crosssetup benchmarks of the recent state-of-the-art methods are listed in Tab. 4. The proposed FGCN model achieves 85.4% on cross-subject and 87.4% on crosssetup and it outperforms the most comparable ST-GCN model [56] by 13.0% and 16.1% on the cross-subject and cross-setup benchmarks respectively. The FGCN model outperforms other state-of-the-art methods with much lager margins. For example, the FGCN model outperforms Two-Stream Attention LSTM [31] by over 24% on both cross-subject and cross-setup benchmarks, and outperforms the most recent work FSNet [29] by over 25% on both of cross-subject and cross-setup benchmarks.\n\nFor the typical 3D action recognition dataset Northwestern-UCLA, we compare the proposed FGCN model with the state-of-the-art methods in recent years. The results of these models are reported in Tab. 5. The FGCN model outperforms the part-based hierarchical recurrent neural network HBRNN-L [6] by 16.8%. The recent method AGC-LSTM proposes an attention enhanced graph convolutional LSTM network to capture discriminative features from the cooccurrence relationship between spatial configuration and temporal dynamics. The FGCN model outperforms it by 2%. Moreover, the FGCN model outperforms the most recent methods, such as JS+JM+BS+BM [25], HiGCN [14] and MSNN [42]. The proposed FGCN model achieves state-of-the-art performance on the Northwestern-UCLA dataset.\n\n\nConclusion\n\nIn this paper, we propose a novel FGCN model to extract effective spatialtemporal features of actions in a coarse-to-fine progressive process. Firstly, we propose a multi-stage temporal sampling strategy to sample sparse skeleton clips in multiple temporal stages and exploit graph convolutional layers to extract local spatial-temporal features for each stage. Then, we introduce the feedback mechanism into conventional GCNs by proposing the FGCB which is a local graph convolutional dense network. The FGCB transmits the semantic information from high-level layers to low-level layers and from the former stages to the later stages. Moreover, the FGCN provides early predictions which help agents in many applications to make timely decisions on-the-fly. The proposed FGCN model is extensively evaluated on the NTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA datasets, indicating that the FGCN is effective for action recognition. It has achieved state-of-the-art performance on the three datasets.\n\nFig. 1 .\n1Comparison of the conventional GCNs (left) and the proposed FGCN (right). Red arrows represent the feedback connections of the feedback block (FGCB).\n\nFig. 3 .\n3The prediction scores of FGCN-spatial and FGCN-motion are fused for final action prediction.\n\nFig. 4 .\n4Evaluating the influence of two key factors on NTU-RGB+D, (a) influence of the number of stages, (b) influence of frame length in each stage.\n\nFig. 5 .\n5The confusion matrices of ST-GCN-joint and FGCN-joint on NTU-RGB+D.\n\n2\nFig. 2. The detailed architecture of the proposed FGCB local network.\n\nTable 1 .\n1Evaluating different temporal fusion strategies on NTU-RGB+D.Temporal Fusion Strategies \nWeights \nCross-view(%) \nw1 \nw2 \nw3 \nw4 \nw5 \nLast-win-all fusion \n0 \n0 \n0 \n0 \n1 \n89.88 \nWeight fusion-1 \n0.05 0.05 0.1 \n0.2 \n0.6 \n93.09 \nWeight fusion-2 \n0.1 \n0.15 0.2 0.25 0.3 \n93.05 \nAverage fusion \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n93.57 \n\n\n\nTable 2 .\n2Evaluating the effectiveness of the FGCN model fed with different inputs on the NTU-RGB+D dataset.Models \nCross-subject(%) Cross-view(%) \nST-GCN-joint [56] \n81.5 \n88.3 \nFGCN-joint \n87.04 \n93.57 \nFGCN-bone \n86.96 \n93.22 \nFGCN-joint+FGCN-bone \n89.24 \n95.28 \nFGCN-spatial \n88.32 \n94.82 \nFGCN-motion \n85.96 \n93.57 \nFGCN-spatial+FGCN-motion \n90.22 \n96.25 \n\n\n\nTable 4 .\n4Comparisons with the state-of-the-art methods on NTU-RGB+D120.Models \nCross-subject(%) Cross-setup(%) \nInternal Feature Fusion (T-PAMI 2017) [30] \n58.2 \n60.9 \nMulti-Task Learning Network (CVPR 2017) [17] \n58.4 \n57.9 \nSkeleton Visualization (PR 2017) [32] \n60.3 \n63.2 \nTwo-Stream Attention LSTM (TIP 2017) [31] \n61.2 \n63.3 \nMulti-Task CNN with RotClips (TIP 2018) [18] \n62.2 \n61.8 \nST-GCN (AAAI 2018) (reported in [36]) \n72.4 \n71.3 \nAS-GCN (CVPR 2019) (reported in [36]) \n77.7 \n78.9 \nFSNet (T-PAMI 2019) [29] \n59.9 \n62.4 \nTSRJI (SIBGRAPI 2019) [3] \n67.9 \n62.8 \nLSTM-IRN (arXiv 2019) [38] \n77.7 \n79.6 \nGVFE + AS-GCN (arXiv 2019) [36] \n78.3 \n79.8 \nFGCN (ours) \n85.4 \n87.4 \n\n\nFeedback as an individual resource: Personal strategies of creating information. S J Ashford, L L Cummings, Organizational Behavior and Human Performance. 323Ashford, S.J., Cummings, L.L.: Feedback as an individual resource: Personal strate- gies of creating information. Organizational Behavior and Human Performance 32(3), 370-398 (1983)\n\nSpectral networks and locally connected networks on graphs. J Bruna, W Zaremba, A Szlam, Y Lecun, International Conference on Learning Representations. Bruna, J., Zaremba, W., Szlam, A., Lecun, Y.: Spectral networks and locally con- nected networks on graphs. In: International Conference on Learning Representa- tions (2014)\n\nSkeleton image representation for 3D action recognition based on tree structure and reference joints. C Caetano, F Br\u00e9mond, W R Schwartz, SIBGRAPI Conference on Graphics, Patterns and Images. IEEECaetano, C., Br\u00e9mond, F., Schwartz, W.R.: Skeleton image representation for 3D action recognition based on tree structure and reference joints. In: SIBGRAPI Conference on Graphics, Patterns and Images. pp. 16-23. IEEE (2019)\n\nRealtime multi-person 2D pose estimation using part affinity fields. Z Cao, T Simon, S E Wei, Y Sheikh, IEEE Conference on Computer Vision and Pattern Recognition. Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2D pose esti- mation using part affinity fields. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 7291-7299 (2017)\n\nHuman pose estimation with iterative error feedback. J Carreira, P Agrawal, K Fragkiadaki, J Malik, IEEE Conference on Computer Vision and Pattern Recognition. Carreira, J., Agrawal, P., Fragkiadaki, K., Malik, J.: Human pose estimation with iterative error feedback. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 4733-4742 (2016)\n\nHierarchical recurrent neural network for skeleton based action recognition. Y Du, W Wang, L Wang, IEEE Conference on Computer Vision and Pattern Recognition. Du, Y., Wang, W., Wang, L.: Hierarchical recurrent neural network for skeleton based action recognition. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 1110-1118 (2015)\n\nConvolutional networks on graphs for learning molecular fingerprints. D K Duvenaud, D Maclaurin, J Iparraguirre, R Bombarell, T Hirzel, A Aspuru-Guzik, R P Adams, Advances in Neural Information Processing Systems. Duvenaud, D.K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru- Guzik, A., Adams, R.P.: Convolutional networks on graphs for learning molecular fingerprints. In: Advances in Neural Information Processing Systems. pp. 2224- 2232 (2015)\n\nSkeletal quads: Human action recognition using joint quadruples. G Evangelidis, G Singh, R Horaud, IEEE International Conference on Pattern Recognition. Evangelidis, G., Singh, G., Horaud, R.: Skeletal quads: Human action recognition using joint quadruples. In: IEEE International Conference on Pattern Recognition. pp. 4513-4518 (2014)\n\nBrain states: top-down influences in sensory processing. C D Gilbert, M Sigman, Neuron. 545Gilbert, C.D., Sigman, M.: Brain states: top-down influences in sensory processing. Neuron 54(5), 677-696 (2007)\n\nImage superresolution via dual-state recurrent networks. W Han, S Chang, D Liu, M Yu, M Witbrock, T S Huang, IEEE Conference on Computer Vision and Pattern Recognition. Han, W., Chang, S., Liu, D., Yu, M., Witbrock, M., Huang, T.S.: Image super- resolution via dual-state recurrent networks. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 1654-1663 (2018)\n\nDeep back-projection networks for superresolution. M Haris, G Shakhnarovich, N Ukita, IEEE Conference on Computer Vision and Pattern Recognition. Haris, M., Shakhnarovich, G., Ukita, N.: Deep back-projection networks for super- resolution. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 1664-1673 (2018)\n\nDeep convolutional networks on graph-structured data. M Henaff, J Bruna, Y Lecun, Computer Science. Henaff, M., Bruna, J., Lecun, Y.: Deep convolutional networks on graph-structured data. Computer Science (2015)\n\nJointly learning heterogeneous features for RGB-D activity recognition. J F Hu, W S Zheng, J Lai, J Zhang, IEEE Conference on Computer Vision and Pattern Recognition. Hu, J.F., Zheng, W.S., Lai, J., Zhang, J.: Jointly learning heterogeneous features for RGB-D activity recognition. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 5344-5352 (2015)\n\nHierarchical graph convolutional network for skeleton-based action recognition. L Huang, Y Huang, W Ouyang, L Wang, Springer International Conference on Image and Graphics. Huang, L., Huang, Y., Ouyang, W., Wang, L.: Hierarchical graph convolutional network for skeleton-based action recognition. In: Springer International Confer- ence on Image and Graphics. pp. 93-102 (2019)\n\nCortical feedback improves discrimination between figure and background by v1, v2 and v3 neurons. J Hup\u00e9, A James, B Payne, S Lomber, P Girard, J Bullier, Nature. 3946695Hup\u00e9, J., James, A., Payne, B., Lomber, S., Girard, P., Bullier, J.: Cortical feedback improves discrimination between figure and background by v1, v2 and v3 neurons. Nature 394(6695), 784-787 (1998)\n\nLargescale video classification with convolutional neural networks. A Karpathy, G Toderici, S Shetty, T Leung, R Sukthankar, L Fei-Fei, IEEE Conference on Computer Vision and Pattern Recognition. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large- scale video classification with convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 1725-1732 (2014)\n\nA new representation of skeleton sequences for 3D action recognition. Q Ke, M Bennamoun, S An, F Sohel, F Boussaid, IEEE Conference on Computer Vision and Pattern Recognition. Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F.: A new representation of skeleton sequences for 3D action recognition. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 3288-3297 (2017)\n\nLearning clip representations for skeleton-based 3D action recognition. Q Ke, M Bennamoun, S An, F Sohel, F Boussaid, IEEE Transactions on Image Processing. 276Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F.: Learning clip representa- tions for skeleton-based 3D action recognition. IEEE Transactions on Image Pro- cessing 27(6), 2842-2855 (2018)\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, M Welling, International Conference on Learning Representations. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks. In: International Conference on Learning Representations (2017)\n\nFoundations of optimal control theory. E B Lee, L Markus, Minnesota Univ Minneapolis Center For Control SciencesTech. rep.Lee, E.B., Markus, L.: Foundations of optimal control theory. Tech. rep., Minnesota Univ Minneapolis Center For Control Sciences (1967)\n\nEnsemble deep learning for skeleton-based action recognition using temporal sliding LSTM networks. I Lee, D Kim, S Kang, S Lee, IEEE International Conference on Computer Vision. Lee, I., Kim, D., Kang, S., Lee, S.: Ensemble deep learning for skeleton-based action recognition using temporal sliding LSTM networks. In: IEEE International Conference on Computer Vision. pp. 1012-1020 (2017)\n\nSkeleton based action recognition using translation-scale invariant image mapping and multi-scale deep CNN. B Li, Y Dai, X Cheng, H Chen, Y Lin, M He, IEEE International Conference on Multimedia and Expo Workshops. Li, B., Dai, Y., Cheng, X., Chen, H., Lin, Y., He, M.: Skeleton based action recog- nition using translation-scale invariant image mapping and multi-scale deep CNN. In: IEEE International Conference on Multimedia and Expo Workshops. pp. 601- 604 (2017)\n\nActional-structural graph convolutional networks for skeleton-based action recognition. M Li, S Chen, X Chen, Y Zhang, Y Wang, Q Tian, IEEE Conference on Computer Vision and Pattern Recognition. Li, M., Chen, S., Chen, X., Zhang, Y., Wang, Y., Tian, Q.: Actional-structural graph convolutional networks for skeleton-based action recognition. In: IEEE Con- ference on Computer Vision and Pattern Recognition. pp. 3595-3603 (2019)\n\nIndependently recurrent neural network (indrnn): Building a longer and deeper RNN. S Li, W Li, C Cook, C Zhu, Y Gao, IEEE Conference on Computer Vision and Pattern Recognition. Li, S., Li, W., Cook, C., Zhu, C., Gao, Y.: Independently recurrent neural network (indrnn): Building a longer and deeper RNN. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 5457-5466 (2018)\n\nLearning shape-motion representations from geometric algebra spatio-temporal model for skeleton-based action recognition. Y Li, R Xia, X Liu, Q Huang, IEEE International Conference on Multimedia and Expo. Li, Y., Xia, R., Liu, X., Huang, Q.: Learning shape-motion representations from geometric algebra spatio-temporal model for skeleton-based action recognition. In: IEEE International Conference on Multimedia and Expo. pp. 1066-1071 (2019)\n\nFeedback network for image super-resolution. Z Li, J Yang, Z Liu, X Yang, G Jeon, W Wu, IEEE Conference on Computer Vision and Pattern Recognition. Li, Z., Yang, J., Liu, Z., Yang, X., Jeon, G., Wu, W.: Feedback network for image super-resolution. In: IEEE Conference on Computer Vision and Pattern Recogni- tion. pp. 3867-3876 (2019)\n\nTwo-stream 3D convolutional neural network for skeletonbased action recognition. H Liu, J Tu, M Liu, arXiv:1705.08106arXiv preprintLiu, H., Tu, J., Liu, M.: Two-stream 3D convolutional neural network for skeleton- based action recognition. arXiv preprint arXiv:1705.08106 (2017)\n\nNTU RGB+D 120: A large-scale benchmark for 3D human activity understanding. J Liu, A Shahroudy, M L Perez, G Wang, L Y Duan, A K Chichung, IEEE Transactions on Pattern Analysis and Machine Intelligence. Liu, J., Shahroudy, A., Perez, M.L., Wang, G., Duan, L.Y., Chichung, A.K.: NTU RGB+D 120: A large-scale benchmark for 3D human activity understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence (2019)\n\nSkeleton-based online action prediction using scale selection network. J Liu, A Shahroudy, G Wang, L Y Duan, A K Chichung, IEEE Transactions on Pattern Analysis and Machine Intelligence. Liu, J., Shahroudy, A., Wang, G., Duan, L.Y., Chichung, A.K.: Skeleton-based on- line action prediction using scale selection network. IEEE Transactions on Pattern Analysis and Machine Intelligence (2019)\n\nSkeleton-based action recognition using spatio-temporal LSTM network with trust gates. J Liu, A Shahroudy, D Xu, A C Kot, G Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4012Liu, J., Shahroudy, A., Xu, D., Kot, A.C., Wang, G.: Skeleton-based action recog- nition using spatio-temporal LSTM network with trust gates. IEEE Transactions on Pattern Analysis and Machine Intelligence 40(12), 3007-3021 (2017)\n\nSkeleton-based human action recognition with global context-aware attention LSTM networks. J Liu, G Wang, L Y Duan, K Abdiyeva, A C Kot, IEEE Transactions on Image Processing. 274Liu, J., Wang, G., Duan, L.Y., Abdiyeva, K., Kot, A.C.: Skeleton-based human ac- tion recognition with global context-aware attention LSTM networks. IEEE Trans- actions on Image Processing 27(4), 1586-1599 (2017)\n\nEnhanced skeleton visualization for view invariant human action recognition. M Liu, H Liu, C Chen, Pattern Recognition. 68ElsevierLiu, M., Liu, H., Chen, C.: Enhanced skeleton visualization for view invariant human action recognition. Elsevier Pattern Recognition 68, 346-362 (2017)\n\nGroup sparsity and geometry constrained dictionary learning for action recognition from depth maps. J Luo, W Wang, H Qi, IEEE International Conference on Computer Vision. Luo, J., Wang, W., Qi, H.: Group sparsity and geometry constrained dictionary learning for action recognition from depth maps. In: IEEE International Conference on Computer Vision. pp. 1809-1816 (2013)\n\nLearning convolutional neural networks for graphs. M Niepert, M Ahmed, K Kutzkov, IEEE International Conference on Machine Learning. Niepert, M., Ahmed, M., Kutzkov, K.: Learning convolutional neural networks for graphs. In: IEEE International Conference on Machine Learning. pp. 2014-2023 (2016)\n\nJoint angles similarities and HOG2 for action recognition. E Ohn-Bar, M Trivedi, IEEE Conference on Computer Vision and Pattern Recognition Workshops. Ohn-Bar, E., Trivedi, M.: Joint angles similarities and HOG2 for action recognition. In: IEEE Conference on Computer Vision and Pattern Recognition Workshops. pp. 465-470 (2013)\n\nVertex feature encoding and hierarchical temporal modeling in a spatial-temporal graph convolutional network for action recognition. K Papadopoulos, E Ghorbel, D Aouada, B Ottersten, arXiv:1912.09745arXiv preprintPapadopoulos, K., Ghorbel, E., Aouada, D., Ottersten, B.: Vertex feature encod- ing and hierarchical temporal modeling in a spatial-temporal graph convolutional network for action recognition. arXiv preprint arXiv:1912.09745 (2019)\n\nApplication of the recurrent multilayer perceptron in modeling complex process dynamics. A G Parlos, K T Chong, A F Atiya, IEEE Transactions on Neural Networks. 52Parlos, A.G., Chong, K.T., Atiya, A.F.: Application of the recurrent multilayer perceptron in modeling complex process dynamics. IEEE Transactions on Neural Networks 5(2), 255-266 (1994)\n\nInteraction relational network for mutual action recognition. M Perez, J Liu, A C Kot, arXiv:1910.04963arXiv preprintPerez, M., Liu, J., Kot, A.C.: Interaction relational network for mutual action recognition. arXiv preprint arXiv:1910.04963 (2019)\n\nReal time action recognition using histograms of depth gradients and random decision forests. H Rahmani, A Mahmood, D Q Huynh, A Mian, IEEE Winter Conference on Applications of Computer Vision. Rahmani, H., Mahmood, A., Huynh, D.Q., Mian, A.: Real time action recognition using histograms of depth gradients and random decision forests. In: IEEE Winter Conference on Applications of Computer Vision. pp. 626-633 (2014)\n\nLearning a non-linear knowledge transfer model for crossview action recognition. H Rahmani, A Mian, IEEE Conference on Computer Vision and Pattern Recognition. Rahmani, H., Mian, A.: Learning a non-linear knowledge transfer model for cross- view action recognition. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 2458-2466 (2015)\n\nNTU RGB+D: A large scale dataset for 3D human activity analysis. A Shahroudy, J Liu, T T Ng, G Wang, IEEE Conference on Computer Vision and Pattern Recognition. Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: NTU RGB+D: A large scale dataset for 3D human activity analysis. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 1010-1019 (2016)\n\nLearning representations from skeletal self-similarities for cross-view action recognition. Z Shao, Y Li, H Zhang, IEEE Transactions on Circuits and Systems for Video Technology. Shao, Z., Li, Y., Zhang, H.: Learning representations from skeletal self-similarities for cross-view action recognition. IEEE Transactions on Circuits and Systems for Video Technology (2020)\n\nSkeleton-based action recognition with directed graph neural networks. L Shi, Y Zhang, J Cheng, H Lu, IEEE Conference on Computer Vision and Pattern Recognition. Shi, L., Zhang, Y., Cheng, J., Lu, H.: Skeleton-based action recognition with di- rected graph neural networks. In: IEEE Conference on Computer Vision and Pat- tern Recognition. pp. 7912-7921 (2019)\n\nTwo-stream adaptive graph convolutional networks for skeleton-based action recognition. L Shi, Y Zhang, J Cheng, H Lu, IEEE Conference on Computer Vision and Pattern Recognition. Shi, L., Zhang, Y., Cheng, J., Lu, H.: Two-stream adaptive graph convolutional networks for skeleton-based action recognition. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 12026-12035 (2019)\n\nAn attention enhanced graph convolutional LSTM network for skeleton-based action recognition. C Si, W Chen, W Wang, L Wang, T Tan, IEEE Conference on Computer Vision and Pattern Recognition. Si, C., Chen, W., Wang, W., Wang, L., Tan, T.: An attention enhanced graph convolutional LSTM network for skeleton-based action recognition. In: IEEE Con- ference on Computer Vision and Pattern Recognition. pp. 1227-1236 (2019)\n\nSkeleton-based action recognition with spatial reasoning and temporal stack learning. C Si, Y Jing, W Wang, L Wang, T Tan, Springer European Conference on Computer Vision. Si, C., Jing, Y., Wang, W., Wang, L., Tan, T.: Skeleton-based action recognition with spatial reasoning and temporal stack learning. In: Springer European Confer- ence on Computer Vision. pp. 103-118 (2018)\n\nTwo-stream convolutional networks for action recognition in videos. K Simonyan, A Zisserman, Advances in Neural Information Processing Systems. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog- nition in videos. In: Advances in Neural Information Processing Systems. pp. 568- 576 (2014)\n\nAn end-to-end spatio-temporal attention model for human action recognition from skeleton data. S Song, C Lan, J Xing, W Zeng, J Liu, AAAI Conference on Artificial Intelligence. Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: An end-to-end spatio-temporal atten- tion model for human action recognition from skeleton data. In: AAAI Conference on Artificial Intelligence. pp. 4263-4270 (2017)\n\nDeep networks with internal selective attention through feedback connections. M F Stollenga, J Masci, F Gomez, J Schmidhuber, Advances in Neural Information Processing Systems. Stollenga, M.F., Masci, J., Gomez, F., Schmidhuber, J.: Deep networks with in- ternal selective attention through feedback connections. In: Advances in Neural Information Processing Systems. pp. 3545-3553 (2014)\n\nDeep progressive reinforcement learning for skeleton-based action recognition. Y Tang, Y Tian, J Lu, P Li, J Zhou, IEEE Conference on Computer Vision and Pattern Recognition. Tang, Y., Tian, Y., Lu, J., Li, P., Zhou, J.: Deep progressive reinforcement learning for skeleton-based action recognition. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 5323-5332 (2018)\n\nPart-based graph convolutional network for action recognition. K Thakkar, P Narayanan, British Machine Vision Conference. Thakkar, K., Narayanan, P.: Part-based graph convolutional network for action recognition. In: British Machine Vision Conference. pp. 1-13 (2018)\n\nDeeppose: Human pose estimation via deep neural networks. A Toshev, C Szegedy, IEEE Conference on Computer Vision and Pattern Recognition. Toshev, A., Szegedy, C.: Deeppose: Human pose estimation via deep neural net- works. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 1653-1660 (2014)\n\nHuman action recognition by representing 3D skeletons as points in a lie group. R Vemulapalli, F Arrate, R Chellappa, IEEE Conference on Computer Vision and Pattern Recognition. Vemulapalli, R., Arrate, F., Chellappa, R.: Human action recognition by repre- senting 3D skeletons as points in a lie group. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 588-595 (2014)\n\nLearning actionlet ensemble for 3D human action recognition. J Wang, Z Liu, Y Wu, J Yuan, IEEE Transactions on Pattern Analysis and Machine Intelligence. 365Wang, J., Liu, Z., Wu, Y., Yuan, J.: Learning actionlet ensemble for 3D human ac- tion recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 36(5), 914-927 (2013)\n\nCross-view action modeling, learning and recognition. J Wang, X Nie, Y Xia, Y Wu, S C Zhu, IEEE Conference on Computer Vision and Pattern Recognition. Wang, J., Nie, X., Xia, Y., Wu, Y., Zhu, S.C.: Cross-view action modeling, learning and recognition. In: IEEE Conference on Computer Vision and Pattern Recogni- tion. pp. 2649-2656 (2014)\n\nSpatial temporal graph convolutional networks for skeleton-based action recognition. S Yan, Y Xiong, D Lin, AAAI Conference on Artificial Intelligence. Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional networks for skeleton-based action recognition. In: AAAI Conference on Artificial Intelligence. pp. 7444-7452 (2018)\n\nFeedback networks. A R Zamir, T L Wu, L Sun, W B Shen, B E Shi, J Malik, S Savarese, IEEE Conference on Computer Vision and Pattern Recognition. Zamir, A.R., Wu, T.L., Sun, L., Shen, W.B., Shi, B.E., Malik, J., Savarese, S.: Feedback networks. In: IEEE Conference on Computer Vision and Pattern Recog- nition. pp. 1308-1317 (2017)\n\nView adaptive recurrent neural networks for high performance human action recognition from skeleton data. P Zhang, C Lan, J Xing, W Zeng, J Xue, N Zheng, IEEE International Conference on Computer Vision. Zhang, P., Lan, C., Xing, J., Zeng, W., Xue, J., Zheng, N.: View adaptive recurrent neural networks for high performance human action recognition from skeleton data. In: IEEE International Conference on Computer Vision. pp. 2117-2126 (2017)\n\nMicrosoft kinect sensor and its effect. Z Zhang, IEEE Multimedia. 192Zhang, Z.: Microsoft kinect sensor and its effect. IEEE Multimedia 19(2), 4-10 (2012)\n\nTwo-stream RNN/CNN for action recognition in 3D videos. R Zhao, H Ali, P Van Der Smagt, IEEE International Conference on Intelligent Robots and Systems. Zhao, R., Ali, H., Van der Smagt, P.: Two-stream RNN/CNN for action recognition in 3D videos. In: IEEE International Conference on Intelligent Robots and Systems. pp. 4260-4267 (2017)\n\nBayesian graph convolution LSTM for skeleton based action recognition. R Zhao, K Wang, H Su, Q Ji, IEEE International Conference on Computer Vision. Zhao, R., Wang, K., Su, H., Ji, Q.: Bayesian graph convolution LSTM for skeleton based action recognition. In: IEEE International Conference on Computer Vision. pp. 6882-6892 (2019)\n", "annotations": {"author": "[{\"end\":209,\"start\":89},{\"end\":237,\"start\":210},{\"end\":405,\"start\":238},{\"end\":524,\"start\":406},{\"end\":714,\"start\":525},{\"end\":899,\"start\":715},{\"end\":1104,\"start\":900},{\"end\":1112,\"start\":1105},{\"end\":1119,\"start\":1113},{\"end\":1128,\"start\":1120},{\"end\":1134,\"start\":1129},{\"end\":1141,\"start\":1135},{\"end\":1148,\"start\":1142},{\"end\":1159,\"start\":1149}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":93},{\"end\":217,\"start\":214},{\"end\":246,\"start\":241},{\"end\":413,\"start\":411},{\"end\":534,\"start\":531},{\"end\":725,\"start\":722},{\"end\":917,\"start\":910},{\"end\":1111,\"start\":1107},{\"end\":1118,\"start\":1115},{\"end\":1127,\"start\":1122},{\"end\":1133,\"start\":1131},{\"end\":1140,\"start\":1137},{\"end\":1147,\"start\":1144},{\"end\":1158,\"start\":1151}]", "author_first_name": "[{\"end\":92,\"start\":89},{\"end\":213,\"start\":210},{\"end\":240,\"start\":238},{\"end\":410,\"start\":406},{\"end\":530,\"start\":525},{\"end\":721,\"start\":715},{\"end\":907,\"start\":900},{\"end\":909,\"start\":908},{\"end\":1106,\"start\":1105},{\"end\":1114,\"start\":1113},{\"end\":1121,\"start\":1120},{\"end\":1130,\"start\":1129},{\"end\":1136,\"start\":1135},{\"end\":1143,\"start\":1142},{\"end\":1150,\"start\":1149}]", "author_affiliation": "[{\"end\":193,\"start\":120},{\"end\":208,\"start\":195},{\"end\":321,\"start\":248},{\"end\":389,\"start\":323},{\"end\":404,\"start\":391},{\"end\":508,\"start\":435},{\"end\":523,\"start\":510},{\"end\":630,\"start\":557},{\"end\":698,\"start\":632},{\"end\":713,\"start\":700},{\"end\":813,\"start\":740},{\"end\":883,\"start\":815},{\"end\":898,\"start\":885},{\"end\":1009,\"start\":943},{\"end\":1103,\"start\":1011}]", "title": "[{\"end\":75,\"start\":1},{\"end\":1234,\"start\":1160}]", "venue": null, "abstract": "[{\"end\":3088,\"start\":1308}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3550,\"start\":3546},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3572,\"start\":3568},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3722,\"start\":3719},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3725,\"start\":3722},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":3728,\"start\":3725},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3731,\"start\":3728},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3754,\"start\":3750},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3757,\"start\":3754},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3760,\"start\":3757},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4030,\"start\":4026},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4033,\"start\":4030},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4036,\"start\":4033},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4039,\"start\":4036},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8362,\"start\":8358},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8399,\"start\":8395},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8401,\"start\":8399},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8813,\"start\":8810},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8816,\"start\":8813},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8819,\"start\":8816},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8822,\"start\":8819},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8939,\"start\":8936},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8942,\"start\":8939},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9002,\"start\":8998},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9048,\"start\":9044},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9192,\"start\":9188},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9195,\"start\":9192},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9198,\"start\":9195},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9201,\"start\":9198},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9331,\"start\":9327},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9334,\"start\":9331},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9369,\"start\":9365},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9419,\"start\":9415},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9588,\"start\":9585},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9591,\"start\":9588},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9594,\"start\":9591},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9597,\"start\":9594},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9801,\"start\":9797},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9804,\"start\":9801},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9807,\"start\":9804},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10003,\"start\":9999},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10367,\"start\":10364},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10370,\"start\":10367},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10372,\"start\":10370},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10375,\"start\":10372},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10378,\"start\":10375},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10588,\"start\":10585},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10591,\"start\":10588},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10721,\"start\":10718},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10724,\"start\":10721},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10727,\"start\":10724},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10929,\"start\":10925},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10932,\"start\":10929},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10935,\"start\":10932},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10938,\"start\":10935},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10961,\"start\":10957},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11253,\"start\":11249},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11443,\"start\":11439},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11587,\"start\":11583},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11958,\"start\":11954},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11960,\"start\":11958},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12015,\"start\":12012},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12039,\"start\":12035},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12042,\"start\":12039},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12149,\"start\":12145},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12152,\"start\":12149},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12155,\"start\":12152},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12158,\"start\":12155},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12161,\"start\":12158},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12163,\"start\":12161},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12302,\"start\":12298},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12527,\"start\":12523},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12741,\"start\":12737},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12744,\"start\":12741},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12747,\"start\":12744},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12821,\"start\":12817},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12973,\"start\":12969},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13103,\"start\":13099},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13260,\"start\":13257},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":15995,\"start\":15991},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15998,\"start\":15995},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16001,\"start\":15998},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16004,\"start\":16001},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22301,\"start\":22297},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23761,\"start\":23757},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24471,\"start\":24467},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25199,\"start\":25195},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25504,\"start\":25500},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":26320,\"start\":26316},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30177,\"start\":30173},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":30211,\"start\":30207},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":30248,\"start\":30244},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30282,\"start\":30278},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":30316,\"start\":30312},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":30360,\"start\":30356},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30394,\"start\":30390},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":30430,\"start\":30426},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30465,\"start\":30461},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30825,\"start\":30821},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30849,\"start\":30845},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":30852,\"start\":30849},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":30855,\"start\":30852},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30882,\"start\":30878},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30885,\"start\":30882},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30888,\"start\":30885},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":30891,\"start\":30888},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":31076,\"start\":31072},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":31273,\"start\":31269},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":31287,\"start\":31283},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31302,\"start\":31298},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":31568,\"start\":31564},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":31601,\"start\":31597},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31629,\"start\":31626},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31672,\"start\":31668},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31711,\"start\":31707},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":31742,\"start\":31738},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31776,\"start\":31772},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31804,\"start\":31800},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31831,\"start\":31827},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31999,\"start\":31998},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":32137,\"start\":32133},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32372,\"start\":32368},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32482,\"start\":32478},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33191,\"start\":33187},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33203,\"start\":33199},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":33217,\"start\":33213}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34489,\"start\":34329},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34593,\"start\":34490},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34746,\"start\":34594},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34825,\"start\":34747},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34898,\"start\":34826},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35233,\"start\":34899},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35598,\"start\":35234},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36281,\"start\":35599}]", "paragraph": "[{\"end\":4947,\"start\":3104},{\"end\":6362,\"start\":4949},{\"end\":7325,\"start\":6364},{\"end\":7390,\"start\":7327},{\"end\":8258,\"start\":7392},{\"end\":10291,\"start\":8312},{\"end\":11564,\"start\":10324},{\"end\":11880,\"start\":11566},{\"end\":13387,\"start\":11901},{\"end\":13828,\"start\":13432},{\"end\":13993,\"start\":13855},{\"end\":14392,\"start\":14052},{\"end\":14533,\"start\":14394},{\"end\":14796,\"start\":14597},{\"end\":15327,\"start\":14869},{\"end\":15851,\"start\":15329},{\"end\":17213,\"start\":15952},{\"end\":17787,\"start\":17215},{\"end\":17967,\"start\":17814},{\"end\":18505,\"start\":17969},{\"end\":18718,\"start\":18539},{\"end\":18900,\"start\":18720},{\"end\":19391,\"start\":18925},{\"end\":19692,\"start\":19434},{\"end\":19833,\"start\":19734},{\"end\":19862,\"start\":19855},{\"end\":21232,\"start\":19942},{\"end\":21570,\"start\":21272},{\"end\":21917,\"start\":21630},{\"end\":23074,\"start\":22015},{\"end\":23565,\"start\":23205},{\"end\":24429,\"start\":23581},{\"end\":25175,\"start\":24454},{\"end\":25622,\"start\":25177},{\"end\":26497,\"start\":25649},{\"end\":26779,\"start\":26516},{\"end\":28112,\"start\":26781},{\"end\":29126,\"start\":28114},{\"end\":30105,\"start\":29128},{\"end\":30470,\"start\":30116},{\"end\":30696,\"start\":30507},{\"end\":31508,\"start\":30698},{\"end\":31853,\"start\":31519},{\"end\":32547,\"start\":31855},{\"end\":33314,\"start\":32549},{\"end\":34328,\"start\":33329}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13854,\"start\":13829},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14051,\"start\":13994},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14596,\"start\":14534},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14868,\"start\":14797},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15912,\"start\":15852},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17813,\"start\":17788},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18538,\"start\":18506},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18924,\"start\":18901},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19433,\"start\":19392},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19733,\"start\":19693},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19904,\"start\":19863},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21271,\"start\":21233},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21629,\"start\":21571},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21983,\"start\":21918},{\"attributes\":{\"id\":\"formula_14\"},\"end\":23204,\"start\":23075}]", "table_ref": "[{\"end\":30044,\"start\":30037},{\"end\":31439,\"start\":31432}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3102,\"start\":3090},{\"attributes\":{\"n\":\"2\"},\"end\":8274,\"start\":8261},{\"attributes\":{\"n\":\"2.1\"},\"end\":8310,\"start\":8277},{\"attributes\":{\"n\":\"2.2\"},\"end\":10322,\"start\":10294},{\"attributes\":{\"n\":\"2.3\"},\"end\":11899,\"start\":11883},{\"attributes\":{\"n\":\"3\"},\"end\":13400,\"start\":13390},{\"attributes\":{\"n\":\"3.1\"},\"end\":13430,\"start\":13403},{\"attributes\":{\"n\":\"3.2\"},\"end\":15950,\"start\":15914},{\"end\":19853,\"start\":19836},{\"attributes\":{\"n\":\"3.3\"},\"end\":19940,\"start\":19906},{\"attributes\":{\"n\":\"3.4\"},\"end\":22013,\"start\":21985},{\"attributes\":{\"n\":\"4\"},\"end\":23579,\"start\":23568},{\"attributes\":{\"n\":\"4.1\"},\"end\":24440,\"start\":24432},{\"end\":24452,\"start\":24443},{\"attributes\":{\"n\":\"4.2\"},\"end\":25647,\"start\":25625},{\"attributes\":{\"n\":\"4.3\"},\"end\":26514,\"start\":26500},{\"end\":30114,\"start\":30108},{\"attributes\":{\"n\":\"4.4\"},\"end\":30505,\"start\":30473},{\"end\":31517,\"start\":31511},{\"attributes\":{\"n\":\"5\"},\"end\":33327,\"start\":33317},{\"end\":34338,\"start\":34330},{\"end\":34499,\"start\":34491},{\"end\":34603,\"start\":34595},{\"end\":34756,\"start\":34748},{\"end\":34828,\"start\":34827},{\"end\":34909,\"start\":34900},{\"end\":35244,\"start\":35235},{\"end\":35609,\"start\":35600}]", "table": "[{\"end\":35233,\"start\":34972},{\"end\":35598,\"start\":35344},{\"end\":36281,\"start\":35673}]", "figure_caption": "[{\"end\":34489,\"start\":34340},{\"end\":34593,\"start\":34501},{\"end\":34746,\"start\":34605},{\"end\":34825,\"start\":34758},{\"end\":34898,\"start\":34829},{\"end\":34972,\"start\":34911},{\"end\":35344,\"start\":35246},{\"end\":35673,\"start\":35611}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16507,\"start\":16501},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17966,\"start\":17960},{\"end\":18141,\"start\":18135},{\"end\":20848,\"start\":20842},{\"end\":21020,\"start\":21014},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23564,\"start\":23558},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26985,\"start\":26976},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27375,\"start\":27366},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28677,\"start\":28671}]", "bib_author_first_name": "[{\"end\":36365,\"start\":36364},{\"end\":36367,\"start\":36366},{\"end\":36378,\"start\":36377},{\"end\":36380,\"start\":36379},{\"end\":36685,\"start\":36684},{\"end\":36694,\"start\":36693},{\"end\":36705,\"start\":36704},{\"end\":36714,\"start\":36713},{\"end\":37054,\"start\":37053},{\"end\":37065,\"start\":37064},{\"end\":37076,\"start\":37075},{\"end\":37078,\"start\":37077},{\"end\":37443,\"start\":37442},{\"end\":37450,\"start\":37449},{\"end\":37459,\"start\":37458},{\"end\":37461,\"start\":37460},{\"end\":37468,\"start\":37467},{\"end\":37791,\"start\":37790},{\"end\":37803,\"start\":37802},{\"end\":37814,\"start\":37813},{\"end\":37829,\"start\":37828},{\"end\":38169,\"start\":38168},{\"end\":38175,\"start\":38174},{\"end\":38183,\"start\":38182},{\"end\":38512,\"start\":38511},{\"end\":38514,\"start\":38513},{\"end\":38526,\"start\":38525},{\"end\":38539,\"start\":38538},{\"end\":38555,\"start\":38554},{\"end\":38568,\"start\":38567},{\"end\":38578,\"start\":38577},{\"end\":38594,\"start\":38593},{\"end\":38596,\"start\":38595},{\"end\":38977,\"start\":38976},{\"end\":38992,\"start\":38991},{\"end\":39001,\"start\":39000},{\"end\":39307,\"start\":39306},{\"end\":39309,\"start\":39308},{\"end\":39320,\"start\":39319},{\"end\":39512,\"start\":39511},{\"end\":39519,\"start\":39518},{\"end\":39528,\"start\":39527},{\"end\":39535,\"start\":39534},{\"end\":39541,\"start\":39540},{\"end\":39553,\"start\":39552},{\"end\":39555,\"start\":39554},{\"end\":39884,\"start\":39883},{\"end\":39893,\"start\":39892},{\"end\":39910,\"start\":39909},{\"end\":40213,\"start\":40212},{\"end\":40223,\"start\":40222},{\"end\":40232,\"start\":40231},{\"end\":40444,\"start\":40443},{\"end\":40446,\"start\":40445},{\"end\":40452,\"start\":40451},{\"end\":40454,\"start\":40453},{\"end\":40463,\"start\":40462},{\"end\":40470,\"start\":40469},{\"end\":40820,\"start\":40819},{\"end\":40829,\"start\":40828},{\"end\":40838,\"start\":40837},{\"end\":40848,\"start\":40847},{\"end\":41217,\"start\":41216},{\"end\":41225,\"start\":41224},{\"end\":41234,\"start\":41233},{\"end\":41243,\"start\":41242},{\"end\":41253,\"start\":41252},{\"end\":41263,\"start\":41262},{\"end\":41558,\"start\":41557},{\"end\":41570,\"start\":41569},{\"end\":41582,\"start\":41581},{\"end\":41592,\"start\":41591},{\"end\":41601,\"start\":41600},{\"end\":41615,\"start\":41614},{\"end\":41992,\"start\":41991},{\"end\":41998,\"start\":41997},{\"end\":42011,\"start\":42010},{\"end\":42017,\"start\":42016},{\"end\":42026,\"start\":42025},{\"end\":42382,\"start\":42381},{\"end\":42388,\"start\":42387},{\"end\":42401,\"start\":42400},{\"end\":42407,\"start\":42406},{\"end\":42416,\"start\":42415},{\"end\":42731,\"start\":42730},{\"end\":42733,\"start\":42732},{\"end\":42741,\"start\":42740},{\"end\":43001,\"start\":43000},{\"end\":43003,\"start\":43002},{\"end\":43010,\"start\":43009},{\"end\":43320,\"start\":43319},{\"end\":43327,\"start\":43326},{\"end\":43334,\"start\":43333},{\"end\":43342,\"start\":43341},{\"end\":43719,\"start\":43718},{\"end\":43725,\"start\":43724},{\"end\":43732,\"start\":43731},{\"end\":43741,\"start\":43740},{\"end\":43749,\"start\":43748},{\"end\":43756,\"start\":43755},{\"end\":44168,\"start\":44167},{\"end\":44174,\"start\":44173},{\"end\":44182,\"start\":44181},{\"end\":44190,\"start\":44189},{\"end\":44199,\"start\":44198},{\"end\":44207,\"start\":44206},{\"end\":44593,\"start\":44592},{\"end\":44599,\"start\":44598},{\"end\":44605,\"start\":44604},{\"end\":44613,\"start\":44612},{\"end\":44620,\"start\":44619},{\"end\":45022,\"start\":45021},{\"end\":45028,\"start\":45027},{\"end\":45035,\"start\":45034},{\"end\":45042,\"start\":45041},{\"end\":45389,\"start\":45388},{\"end\":45395,\"start\":45394},{\"end\":45403,\"start\":45402},{\"end\":45410,\"start\":45409},{\"end\":45418,\"start\":45417},{\"end\":45426,\"start\":45425},{\"end\":45761,\"start\":45760},{\"end\":45768,\"start\":45767},{\"end\":45774,\"start\":45773},{\"end\":46036,\"start\":46035},{\"end\":46043,\"start\":46042},{\"end\":46056,\"start\":46055},{\"end\":46058,\"start\":46057},{\"end\":46067,\"start\":46066},{\"end\":46075,\"start\":46074},{\"end\":46077,\"start\":46076},{\"end\":46085,\"start\":46084},{\"end\":46087,\"start\":46086},{\"end\":46456,\"start\":46455},{\"end\":46463,\"start\":46462},{\"end\":46476,\"start\":46475},{\"end\":46484,\"start\":46483},{\"end\":46486,\"start\":46485},{\"end\":46494,\"start\":46493},{\"end\":46496,\"start\":46495},{\"end\":46865,\"start\":46864},{\"end\":46872,\"start\":46871},{\"end\":46885,\"start\":46884},{\"end\":46891,\"start\":46890},{\"end\":46893,\"start\":46892},{\"end\":46900,\"start\":46899},{\"end\":47298,\"start\":47297},{\"end\":47305,\"start\":47304},{\"end\":47313,\"start\":47312},{\"end\":47315,\"start\":47314},{\"end\":47323,\"start\":47322},{\"end\":47335,\"start\":47334},{\"end\":47337,\"start\":47336},{\"end\":47677,\"start\":47676},{\"end\":47684,\"start\":47683},{\"end\":47691,\"start\":47690},{\"end\":47984,\"start\":47983},{\"end\":47991,\"start\":47990},{\"end\":47999,\"start\":47998},{\"end\":48309,\"start\":48308},{\"end\":48320,\"start\":48319},{\"end\":48329,\"start\":48328},{\"end\":48615,\"start\":48614},{\"end\":48626,\"start\":48625},{\"end\":49019,\"start\":49018},{\"end\":49035,\"start\":49034},{\"end\":49046,\"start\":49045},{\"end\":49056,\"start\":49055},{\"end\":49421,\"start\":49420},{\"end\":49423,\"start\":49422},{\"end\":49433,\"start\":49432},{\"end\":49435,\"start\":49434},{\"end\":49444,\"start\":49443},{\"end\":49446,\"start\":49445},{\"end\":49745,\"start\":49744},{\"end\":49754,\"start\":49753},{\"end\":49761,\"start\":49760},{\"end\":49763,\"start\":49762},{\"end\":50027,\"start\":50026},{\"end\":50038,\"start\":50037},{\"end\":50049,\"start\":50048},{\"end\":50051,\"start\":50050},{\"end\":50060,\"start\":50059},{\"end\":50434,\"start\":50433},{\"end\":50445,\"start\":50444},{\"end\":50770,\"start\":50769},{\"end\":50783,\"start\":50782},{\"end\":50790,\"start\":50789},{\"end\":50792,\"start\":50791},{\"end\":50798,\"start\":50797},{\"end\":51153,\"start\":51152},{\"end\":51161,\"start\":51160},{\"end\":51167,\"start\":51166},{\"end\":51503,\"start\":51502},{\"end\":51510,\"start\":51509},{\"end\":51519,\"start\":51518},{\"end\":51528,\"start\":51527},{\"end\":51882,\"start\":51881},{\"end\":51889,\"start\":51888},{\"end\":51898,\"start\":51897},{\"end\":51907,\"start\":51906},{\"end\":52282,\"start\":52281},{\"end\":52288,\"start\":52287},{\"end\":52296,\"start\":52295},{\"end\":52304,\"start\":52303},{\"end\":52312,\"start\":52311},{\"end\":52694,\"start\":52693},{\"end\":52700,\"start\":52699},{\"end\":52708,\"start\":52707},{\"end\":52716,\"start\":52715},{\"end\":52724,\"start\":52723},{\"end\":53056,\"start\":53055},{\"end\":53068,\"start\":53067},{\"end\":53402,\"start\":53401},{\"end\":53410,\"start\":53409},{\"end\":53417,\"start\":53416},{\"end\":53425,\"start\":53424},{\"end\":53433,\"start\":53432},{\"end\":53777,\"start\":53776},{\"end\":53779,\"start\":53778},{\"end\":53792,\"start\":53791},{\"end\":53801,\"start\":53800},{\"end\":53810,\"start\":53809},{\"end\":54168,\"start\":54167},{\"end\":54176,\"start\":54175},{\"end\":54184,\"start\":54183},{\"end\":54190,\"start\":54189},{\"end\":54196,\"start\":54195},{\"end\":54538,\"start\":54537},{\"end\":54549,\"start\":54548},{\"end\":54802,\"start\":54801},{\"end\":54812,\"start\":54811},{\"end\":55134,\"start\":55133},{\"end\":55149,\"start\":55148},{\"end\":55159,\"start\":55158},{\"end\":55503,\"start\":55502},{\"end\":55511,\"start\":55510},{\"end\":55518,\"start\":55517},{\"end\":55524,\"start\":55523},{\"end\":55839,\"start\":55838},{\"end\":55847,\"start\":55846},{\"end\":55854,\"start\":55853},{\"end\":55861,\"start\":55860},{\"end\":55867,\"start\":55866},{\"end\":55869,\"start\":55868},{\"end\":56210,\"start\":56209},{\"end\":56217,\"start\":56216},{\"end\":56226,\"start\":56225},{\"end\":56480,\"start\":56479},{\"end\":56482,\"start\":56481},{\"end\":56491,\"start\":56490},{\"end\":56493,\"start\":56492},{\"end\":56499,\"start\":56498},{\"end\":56506,\"start\":56505},{\"end\":56508,\"start\":56507},{\"end\":56516,\"start\":56515},{\"end\":56518,\"start\":56517},{\"end\":56525,\"start\":56524},{\"end\":56534,\"start\":56533},{\"end\":56899,\"start\":56898},{\"end\":56908,\"start\":56907},{\"end\":56915,\"start\":56914},{\"end\":56923,\"start\":56922},{\"end\":56931,\"start\":56930},{\"end\":56938,\"start\":56937},{\"end\":57279,\"start\":57278},{\"end\":57451,\"start\":57450},{\"end\":57459,\"start\":57458},{\"end\":57466,\"start\":57465},{\"end\":57804,\"start\":57803},{\"end\":57812,\"start\":57811},{\"end\":57820,\"start\":57819},{\"end\":57826,\"start\":57825}]", "bib_author_last_name": "[{\"end\":36375,\"start\":36368},{\"end\":36389,\"start\":36381},{\"end\":36691,\"start\":36686},{\"end\":36702,\"start\":36695},{\"end\":36711,\"start\":36706},{\"end\":36720,\"start\":36715},{\"end\":37062,\"start\":37055},{\"end\":37073,\"start\":37066},{\"end\":37087,\"start\":37079},{\"end\":37447,\"start\":37444},{\"end\":37456,\"start\":37451},{\"end\":37465,\"start\":37462},{\"end\":37475,\"start\":37469},{\"end\":37800,\"start\":37792},{\"end\":37811,\"start\":37804},{\"end\":37826,\"start\":37815},{\"end\":37835,\"start\":37830},{\"end\":38172,\"start\":38170},{\"end\":38180,\"start\":38176},{\"end\":38188,\"start\":38184},{\"end\":38523,\"start\":38515},{\"end\":38536,\"start\":38527},{\"end\":38552,\"start\":38540},{\"end\":38565,\"start\":38556},{\"end\":38575,\"start\":38569},{\"end\":38591,\"start\":38579},{\"end\":38602,\"start\":38597},{\"end\":38989,\"start\":38978},{\"end\":38998,\"start\":38993},{\"end\":39008,\"start\":39002},{\"end\":39317,\"start\":39310},{\"end\":39327,\"start\":39321},{\"end\":39516,\"start\":39513},{\"end\":39525,\"start\":39520},{\"end\":39532,\"start\":39529},{\"end\":39538,\"start\":39536},{\"end\":39550,\"start\":39542},{\"end\":39561,\"start\":39556},{\"end\":39890,\"start\":39885},{\"end\":39907,\"start\":39894},{\"end\":39916,\"start\":39911},{\"end\":40220,\"start\":40214},{\"end\":40229,\"start\":40224},{\"end\":40238,\"start\":40233},{\"end\":40449,\"start\":40447},{\"end\":40460,\"start\":40455},{\"end\":40467,\"start\":40464},{\"end\":40476,\"start\":40471},{\"end\":40826,\"start\":40821},{\"end\":40835,\"start\":40830},{\"end\":40845,\"start\":40839},{\"end\":40853,\"start\":40849},{\"end\":41222,\"start\":41218},{\"end\":41231,\"start\":41226},{\"end\":41240,\"start\":41235},{\"end\":41250,\"start\":41244},{\"end\":41260,\"start\":41254},{\"end\":41271,\"start\":41264},{\"end\":41567,\"start\":41559},{\"end\":41579,\"start\":41571},{\"end\":41589,\"start\":41583},{\"end\":41598,\"start\":41593},{\"end\":41612,\"start\":41602},{\"end\":41623,\"start\":41616},{\"end\":41995,\"start\":41993},{\"end\":42008,\"start\":41999},{\"end\":42014,\"start\":42012},{\"end\":42023,\"start\":42018},{\"end\":42035,\"start\":42027},{\"end\":42385,\"start\":42383},{\"end\":42398,\"start\":42389},{\"end\":42404,\"start\":42402},{\"end\":42413,\"start\":42408},{\"end\":42425,\"start\":42417},{\"end\":42738,\"start\":42734},{\"end\":42749,\"start\":42742},{\"end\":43007,\"start\":43004},{\"end\":43017,\"start\":43011},{\"end\":43324,\"start\":43321},{\"end\":43331,\"start\":43328},{\"end\":43339,\"start\":43335},{\"end\":43346,\"start\":43343},{\"end\":43722,\"start\":43720},{\"end\":43729,\"start\":43726},{\"end\":43738,\"start\":43733},{\"end\":43746,\"start\":43742},{\"end\":43753,\"start\":43750},{\"end\":43759,\"start\":43757},{\"end\":44171,\"start\":44169},{\"end\":44179,\"start\":44175},{\"end\":44187,\"start\":44183},{\"end\":44196,\"start\":44191},{\"end\":44204,\"start\":44200},{\"end\":44212,\"start\":44208},{\"end\":44596,\"start\":44594},{\"end\":44602,\"start\":44600},{\"end\":44610,\"start\":44606},{\"end\":44617,\"start\":44614},{\"end\":44624,\"start\":44621},{\"end\":45025,\"start\":45023},{\"end\":45032,\"start\":45029},{\"end\":45039,\"start\":45036},{\"end\":45048,\"start\":45043},{\"end\":45392,\"start\":45390},{\"end\":45400,\"start\":45396},{\"end\":45407,\"start\":45404},{\"end\":45415,\"start\":45411},{\"end\":45423,\"start\":45419},{\"end\":45429,\"start\":45427},{\"end\":45765,\"start\":45762},{\"end\":45771,\"start\":45769},{\"end\":45778,\"start\":45775},{\"end\":46040,\"start\":46037},{\"end\":46053,\"start\":46044},{\"end\":46064,\"start\":46059},{\"end\":46072,\"start\":46068},{\"end\":46082,\"start\":46078},{\"end\":46096,\"start\":46088},{\"end\":46460,\"start\":46457},{\"end\":46473,\"start\":46464},{\"end\":46481,\"start\":46477},{\"end\":46491,\"start\":46487},{\"end\":46505,\"start\":46497},{\"end\":46869,\"start\":46866},{\"end\":46882,\"start\":46873},{\"end\":46888,\"start\":46886},{\"end\":46897,\"start\":46894},{\"end\":46905,\"start\":46901},{\"end\":47302,\"start\":47299},{\"end\":47310,\"start\":47306},{\"end\":47320,\"start\":47316},{\"end\":47332,\"start\":47324},{\"end\":47341,\"start\":47338},{\"end\":47681,\"start\":47678},{\"end\":47688,\"start\":47685},{\"end\":47696,\"start\":47692},{\"end\":47988,\"start\":47985},{\"end\":47996,\"start\":47992},{\"end\":48002,\"start\":48000},{\"end\":48317,\"start\":48310},{\"end\":48326,\"start\":48321},{\"end\":48337,\"start\":48330},{\"end\":48623,\"start\":48616},{\"end\":48634,\"start\":48627},{\"end\":49032,\"start\":49020},{\"end\":49043,\"start\":49036},{\"end\":49053,\"start\":49047},{\"end\":49066,\"start\":49057},{\"end\":49430,\"start\":49424},{\"end\":49441,\"start\":49436},{\"end\":49452,\"start\":49447},{\"end\":49751,\"start\":49746},{\"end\":49758,\"start\":49755},{\"end\":49767,\"start\":49764},{\"end\":50035,\"start\":50028},{\"end\":50046,\"start\":50039},{\"end\":50057,\"start\":50052},{\"end\":50065,\"start\":50061},{\"end\":50442,\"start\":50435},{\"end\":50450,\"start\":50446},{\"end\":50780,\"start\":50771},{\"end\":50787,\"start\":50784},{\"end\":50795,\"start\":50793},{\"end\":50803,\"start\":50799},{\"end\":51158,\"start\":51154},{\"end\":51164,\"start\":51162},{\"end\":51173,\"start\":51168},{\"end\":51507,\"start\":51504},{\"end\":51516,\"start\":51511},{\"end\":51525,\"start\":51520},{\"end\":51531,\"start\":51529},{\"end\":51886,\"start\":51883},{\"end\":51895,\"start\":51890},{\"end\":51904,\"start\":51899},{\"end\":51910,\"start\":51908},{\"end\":52285,\"start\":52283},{\"end\":52293,\"start\":52289},{\"end\":52301,\"start\":52297},{\"end\":52309,\"start\":52305},{\"end\":52316,\"start\":52313},{\"end\":52697,\"start\":52695},{\"end\":52705,\"start\":52701},{\"end\":52713,\"start\":52709},{\"end\":52721,\"start\":52717},{\"end\":52728,\"start\":52725},{\"end\":53065,\"start\":53057},{\"end\":53078,\"start\":53069},{\"end\":53407,\"start\":53403},{\"end\":53414,\"start\":53411},{\"end\":53422,\"start\":53418},{\"end\":53430,\"start\":53426},{\"end\":53437,\"start\":53434},{\"end\":53789,\"start\":53780},{\"end\":53798,\"start\":53793},{\"end\":53807,\"start\":53802},{\"end\":53822,\"start\":53811},{\"end\":54173,\"start\":54169},{\"end\":54181,\"start\":54177},{\"end\":54187,\"start\":54185},{\"end\":54193,\"start\":54191},{\"end\":54201,\"start\":54197},{\"end\":54546,\"start\":54539},{\"end\":54559,\"start\":54550},{\"end\":54809,\"start\":54803},{\"end\":54820,\"start\":54813},{\"end\":55146,\"start\":55135},{\"end\":55156,\"start\":55150},{\"end\":55169,\"start\":55160},{\"end\":55508,\"start\":55504},{\"end\":55515,\"start\":55512},{\"end\":55521,\"start\":55519},{\"end\":55529,\"start\":55525},{\"end\":55844,\"start\":55840},{\"end\":55851,\"start\":55848},{\"end\":55858,\"start\":55855},{\"end\":55864,\"start\":55862},{\"end\":55873,\"start\":55870},{\"end\":56214,\"start\":56211},{\"end\":56223,\"start\":56218},{\"end\":56230,\"start\":56227},{\"end\":56488,\"start\":56483},{\"end\":56496,\"start\":56494},{\"end\":56503,\"start\":56500},{\"end\":56513,\"start\":56509},{\"end\":56522,\"start\":56519},{\"end\":56531,\"start\":56526},{\"end\":56543,\"start\":56535},{\"end\":56905,\"start\":56900},{\"end\":56912,\"start\":56909},{\"end\":56920,\"start\":56916},{\"end\":56928,\"start\":56924},{\"end\":56935,\"start\":56932},{\"end\":56944,\"start\":56939},{\"end\":57285,\"start\":57280},{\"end\":57456,\"start\":57452},{\"end\":57463,\"start\":57460},{\"end\":57480,\"start\":57467},{\"end\":57809,\"start\":57805},{\"end\":57817,\"start\":57813},{\"end\":57823,\"start\":57821},{\"end\":57829,\"start\":57827}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":143934627},\"end\":36622,\"start\":36283},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17682909},\"end\":36949,\"start\":36624},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":202565715},\"end\":37371,\"start\":36951},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16224674},\"end\":37735,\"start\":37373},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10111903},\"end\":38089,\"start\":37737},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8040013},\"end\":38439,\"start\":38091},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1690180},\"end\":38909,\"start\":38441},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":215776139},\"end\":39247,\"start\":38911},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7662993},\"end\":39452,\"start\":39249},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":21656084},\"end\":39830,\"start\":39454},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3739626},\"end\":40156,\"start\":39832},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10443309},\"end\":40369,\"start\":40158},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5652420},\"end\":40737,\"start\":40371},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":208355755},\"end\":41116,\"start\":40739},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":204999845},\"end\":41487,\"start\":41118},{\"attributes\":{\"id\":\"b15\"},\"end\":41919,\"start\":41489},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9085113},\"end\":42307,\"start\":41921},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4087603},\"end\":42662,\"start\":42309},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3144218},\"end\":42959,\"start\":42664},{\"attributes\":{\"id\":\"b19\"},\"end\":43218,\"start\":42961},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":12965906},\"end\":43608,\"start\":43220},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1709967},\"end\":44077,\"start\":43610},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":139101198},\"end\":44507,\"start\":44079},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3880365},\"end\":44897,\"start\":44509},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":199488215},\"end\":45341,\"start\":44899},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":85496615},\"end\":45677,\"start\":45343},{\"attributes\":{\"doi\":\"arXiv:1705.08106\",\"id\":\"b26\"},\"end\":45957,\"start\":45679},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":152282878},\"end\":46382,\"start\":45959},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":59843103},\"end\":46775,\"start\":46384},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":20813768},\"end\":47204,\"start\":46777},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3225725},\"end\":47597,\"start\":47206},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":43823483},\"end\":47881,\"start\":47599},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10527367},\"end\":48255,\"start\":47883},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1430801},\"end\":48553,\"start\":48257},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13951521},\"end\":48883,\"start\":48555},{\"attributes\":{\"doi\":\"arXiv:1912.09745\",\"id\":\"b35\"},\"end\":49329,\"start\":48885},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":195309},\"end\":49680,\"start\":49331},{\"attributes\":{\"doi\":\"arXiv:1910.04963\",\"id\":\"b37\"},\"end\":49930,\"start\":49682},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":7405026},\"end\":50350,\"start\":49932},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":9118111},\"end\":50702,\"start\":50352},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":15928602},\"end\":51058,\"start\":50704},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":214434833},\"end\":51429,\"start\":51060},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":195446390},\"end\":51791,\"start\":51431},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":195440283},\"end\":52185,\"start\":51793},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":67856333},\"end\":52605,\"start\":52187},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":19170594},\"end\":52985,\"start\":52607},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":11797475},\"end\":53304,\"start\":52987},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":14298099},\"end\":53696,\"start\":53306},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":7875983},\"end\":54086,\"start\":53698},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52245597},\"end\":54472,\"start\":54088},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":52271360},\"end\":54741,\"start\":54474},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":206592152},\"end\":55051,\"start\":54743},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1732632},\"end\":55439,\"start\":55053},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206765283},\"end\":55782,\"start\":55441},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":2239612},\"end\":56122,\"start\":55784},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":19167105},\"end\":56458,\"start\":56124},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":1022460},\"end\":56790,\"start\":56460},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":12699455},\"end\":57236,\"start\":56792},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":8629444},\"end\":57392,\"start\":57238},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":2098173},\"end\":57730,\"start\":57394},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":207978618},\"end\":58062,\"start\":57732}]", "bib_title": "[{\"end\":36362,\"start\":36283},{\"end\":36682,\"start\":36624},{\"end\":37051,\"start\":36951},{\"end\":37440,\"start\":37373},{\"end\":37788,\"start\":37737},{\"end\":38166,\"start\":38091},{\"end\":38509,\"start\":38441},{\"end\":38974,\"start\":38911},{\"end\":39304,\"start\":39249},{\"end\":39509,\"start\":39454},{\"end\":39881,\"start\":39832},{\"end\":40210,\"start\":40158},{\"end\":40441,\"start\":40371},{\"end\":40817,\"start\":40739},{\"end\":41214,\"start\":41118},{\"end\":41555,\"start\":41489},{\"end\":41989,\"start\":41921},{\"end\":42379,\"start\":42309},{\"end\":42728,\"start\":42664},{\"end\":43317,\"start\":43220},{\"end\":43716,\"start\":43610},{\"end\":44165,\"start\":44079},{\"end\":44590,\"start\":44509},{\"end\":45019,\"start\":44899},{\"end\":45386,\"start\":45343},{\"end\":46033,\"start\":45959},{\"end\":46453,\"start\":46384},{\"end\":46862,\"start\":46777},{\"end\":47295,\"start\":47206},{\"end\":47674,\"start\":47599},{\"end\":47981,\"start\":47883},{\"end\":48306,\"start\":48257},{\"end\":48612,\"start\":48555},{\"end\":49418,\"start\":49331},{\"end\":50024,\"start\":49932},{\"end\":50431,\"start\":50352},{\"end\":50767,\"start\":50704},{\"end\":51150,\"start\":51060},{\"end\":51500,\"start\":51431},{\"end\":51879,\"start\":51793},{\"end\":52279,\"start\":52187},{\"end\":52691,\"start\":52607},{\"end\":53053,\"start\":52987},{\"end\":53399,\"start\":53306},{\"end\":53774,\"start\":53698},{\"end\":54165,\"start\":54088},{\"end\":54535,\"start\":54474},{\"end\":54799,\"start\":54743},{\"end\":55131,\"start\":55053},{\"end\":55500,\"start\":55441},{\"end\":55836,\"start\":55784},{\"end\":56207,\"start\":56124},{\"end\":56477,\"start\":56460},{\"end\":56896,\"start\":56792},{\"end\":57276,\"start\":57238},{\"end\":57448,\"start\":57394},{\"end\":57801,\"start\":57732}]", "bib_author": "[{\"end\":36377,\"start\":36364},{\"end\":36391,\"start\":36377},{\"end\":36693,\"start\":36684},{\"end\":36704,\"start\":36693},{\"end\":36713,\"start\":36704},{\"end\":36722,\"start\":36713},{\"end\":37064,\"start\":37053},{\"end\":37075,\"start\":37064},{\"end\":37089,\"start\":37075},{\"end\":37449,\"start\":37442},{\"end\":37458,\"start\":37449},{\"end\":37467,\"start\":37458},{\"end\":37477,\"start\":37467},{\"end\":37802,\"start\":37790},{\"end\":37813,\"start\":37802},{\"end\":37828,\"start\":37813},{\"end\":37837,\"start\":37828},{\"end\":38174,\"start\":38168},{\"end\":38182,\"start\":38174},{\"end\":38190,\"start\":38182},{\"end\":38525,\"start\":38511},{\"end\":38538,\"start\":38525},{\"end\":38554,\"start\":38538},{\"end\":38567,\"start\":38554},{\"end\":38577,\"start\":38567},{\"end\":38593,\"start\":38577},{\"end\":38604,\"start\":38593},{\"end\":38991,\"start\":38976},{\"end\":39000,\"start\":38991},{\"end\":39010,\"start\":39000},{\"end\":39319,\"start\":39306},{\"end\":39329,\"start\":39319},{\"end\":39518,\"start\":39511},{\"end\":39527,\"start\":39518},{\"end\":39534,\"start\":39527},{\"end\":39540,\"start\":39534},{\"end\":39552,\"start\":39540},{\"end\":39563,\"start\":39552},{\"end\":39892,\"start\":39883},{\"end\":39909,\"start\":39892},{\"end\":39918,\"start\":39909},{\"end\":40222,\"start\":40212},{\"end\":40231,\"start\":40222},{\"end\":40240,\"start\":40231},{\"end\":40451,\"start\":40443},{\"end\":40462,\"start\":40451},{\"end\":40469,\"start\":40462},{\"end\":40478,\"start\":40469},{\"end\":40828,\"start\":40819},{\"end\":40837,\"start\":40828},{\"end\":40847,\"start\":40837},{\"end\":40855,\"start\":40847},{\"end\":41224,\"start\":41216},{\"end\":41233,\"start\":41224},{\"end\":41242,\"start\":41233},{\"end\":41252,\"start\":41242},{\"end\":41262,\"start\":41252},{\"end\":41273,\"start\":41262},{\"end\":41569,\"start\":41557},{\"end\":41581,\"start\":41569},{\"end\":41591,\"start\":41581},{\"end\":41600,\"start\":41591},{\"end\":41614,\"start\":41600},{\"end\":41625,\"start\":41614},{\"end\":41997,\"start\":41991},{\"end\":42010,\"start\":41997},{\"end\":42016,\"start\":42010},{\"end\":42025,\"start\":42016},{\"end\":42037,\"start\":42025},{\"end\":42387,\"start\":42381},{\"end\":42400,\"start\":42387},{\"end\":42406,\"start\":42400},{\"end\":42415,\"start\":42406},{\"end\":42427,\"start\":42415},{\"end\":42740,\"start\":42730},{\"end\":42751,\"start\":42740},{\"end\":43009,\"start\":43000},{\"end\":43019,\"start\":43009},{\"end\":43326,\"start\":43319},{\"end\":43333,\"start\":43326},{\"end\":43341,\"start\":43333},{\"end\":43348,\"start\":43341},{\"end\":43724,\"start\":43718},{\"end\":43731,\"start\":43724},{\"end\":43740,\"start\":43731},{\"end\":43748,\"start\":43740},{\"end\":43755,\"start\":43748},{\"end\":43761,\"start\":43755},{\"end\":44173,\"start\":44167},{\"end\":44181,\"start\":44173},{\"end\":44189,\"start\":44181},{\"end\":44198,\"start\":44189},{\"end\":44206,\"start\":44198},{\"end\":44214,\"start\":44206},{\"end\":44598,\"start\":44592},{\"end\":44604,\"start\":44598},{\"end\":44612,\"start\":44604},{\"end\":44619,\"start\":44612},{\"end\":44626,\"start\":44619},{\"end\":45027,\"start\":45021},{\"end\":45034,\"start\":45027},{\"end\":45041,\"start\":45034},{\"end\":45050,\"start\":45041},{\"end\":45394,\"start\":45388},{\"end\":45402,\"start\":45394},{\"end\":45409,\"start\":45402},{\"end\":45417,\"start\":45409},{\"end\":45425,\"start\":45417},{\"end\":45431,\"start\":45425},{\"end\":45767,\"start\":45760},{\"end\":45773,\"start\":45767},{\"end\":45780,\"start\":45773},{\"end\":46042,\"start\":46035},{\"end\":46055,\"start\":46042},{\"end\":46066,\"start\":46055},{\"end\":46074,\"start\":46066},{\"end\":46084,\"start\":46074},{\"end\":46098,\"start\":46084},{\"end\":46462,\"start\":46455},{\"end\":46475,\"start\":46462},{\"end\":46483,\"start\":46475},{\"end\":46493,\"start\":46483},{\"end\":46507,\"start\":46493},{\"end\":46871,\"start\":46864},{\"end\":46884,\"start\":46871},{\"end\":46890,\"start\":46884},{\"end\":46899,\"start\":46890},{\"end\":46907,\"start\":46899},{\"end\":47304,\"start\":47297},{\"end\":47312,\"start\":47304},{\"end\":47322,\"start\":47312},{\"end\":47334,\"start\":47322},{\"end\":47343,\"start\":47334},{\"end\":47683,\"start\":47676},{\"end\":47690,\"start\":47683},{\"end\":47698,\"start\":47690},{\"end\":47990,\"start\":47983},{\"end\":47998,\"start\":47990},{\"end\":48004,\"start\":47998},{\"end\":48319,\"start\":48308},{\"end\":48328,\"start\":48319},{\"end\":48339,\"start\":48328},{\"end\":48625,\"start\":48614},{\"end\":48636,\"start\":48625},{\"end\":49034,\"start\":49018},{\"end\":49045,\"start\":49034},{\"end\":49055,\"start\":49045},{\"end\":49068,\"start\":49055},{\"end\":49432,\"start\":49420},{\"end\":49443,\"start\":49432},{\"end\":49454,\"start\":49443},{\"end\":49753,\"start\":49744},{\"end\":49760,\"start\":49753},{\"end\":49769,\"start\":49760},{\"end\":50037,\"start\":50026},{\"end\":50048,\"start\":50037},{\"end\":50059,\"start\":50048},{\"end\":50067,\"start\":50059},{\"end\":50444,\"start\":50433},{\"end\":50452,\"start\":50444},{\"end\":50782,\"start\":50769},{\"end\":50789,\"start\":50782},{\"end\":50797,\"start\":50789},{\"end\":50805,\"start\":50797},{\"end\":51160,\"start\":51152},{\"end\":51166,\"start\":51160},{\"end\":51175,\"start\":51166},{\"end\":51509,\"start\":51502},{\"end\":51518,\"start\":51509},{\"end\":51527,\"start\":51518},{\"end\":51533,\"start\":51527},{\"end\":51888,\"start\":51881},{\"end\":51897,\"start\":51888},{\"end\":51906,\"start\":51897},{\"end\":51912,\"start\":51906},{\"end\":52287,\"start\":52281},{\"end\":52295,\"start\":52287},{\"end\":52303,\"start\":52295},{\"end\":52311,\"start\":52303},{\"end\":52318,\"start\":52311},{\"end\":52699,\"start\":52693},{\"end\":52707,\"start\":52699},{\"end\":52715,\"start\":52707},{\"end\":52723,\"start\":52715},{\"end\":52730,\"start\":52723},{\"end\":53067,\"start\":53055},{\"end\":53080,\"start\":53067},{\"end\":53409,\"start\":53401},{\"end\":53416,\"start\":53409},{\"end\":53424,\"start\":53416},{\"end\":53432,\"start\":53424},{\"end\":53439,\"start\":53432},{\"end\":53791,\"start\":53776},{\"end\":53800,\"start\":53791},{\"end\":53809,\"start\":53800},{\"end\":53824,\"start\":53809},{\"end\":54175,\"start\":54167},{\"end\":54183,\"start\":54175},{\"end\":54189,\"start\":54183},{\"end\":54195,\"start\":54189},{\"end\":54203,\"start\":54195},{\"end\":54548,\"start\":54537},{\"end\":54561,\"start\":54548},{\"end\":54811,\"start\":54801},{\"end\":54822,\"start\":54811},{\"end\":55148,\"start\":55133},{\"end\":55158,\"start\":55148},{\"end\":55171,\"start\":55158},{\"end\":55510,\"start\":55502},{\"end\":55517,\"start\":55510},{\"end\":55523,\"start\":55517},{\"end\":55531,\"start\":55523},{\"end\":55846,\"start\":55838},{\"end\":55853,\"start\":55846},{\"end\":55860,\"start\":55853},{\"end\":55866,\"start\":55860},{\"end\":55875,\"start\":55866},{\"end\":56216,\"start\":56209},{\"end\":56225,\"start\":56216},{\"end\":56232,\"start\":56225},{\"end\":56490,\"start\":56479},{\"end\":56498,\"start\":56490},{\"end\":56505,\"start\":56498},{\"end\":56515,\"start\":56505},{\"end\":56524,\"start\":56515},{\"end\":56533,\"start\":56524},{\"end\":56545,\"start\":56533},{\"end\":56907,\"start\":56898},{\"end\":56914,\"start\":56907},{\"end\":56922,\"start\":56914},{\"end\":56930,\"start\":56922},{\"end\":56937,\"start\":56930},{\"end\":56946,\"start\":56937},{\"end\":57287,\"start\":57278},{\"end\":57458,\"start\":57450},{\"end\":57465,\"start\":57458},{\"end\":57482,\"start\":57465},{\"end\":57811,\"start\":57803},{\"end\":57819,\"start\":57811},{\"end\":57825,\"start\":57819},{\"end\":57831,\"start\":57825}]", "bib_venue": "[{\"end\":36436,\"start\":36391},{\"end\":36774,\"start\":36722},{\"end\":37141,\"start\":37089},{\"end\":37535,\"start\":37477},{\"end\":37895,\"start\":37837},{\"end\":38248,\"start\":38190},{\"end\":38653,\"start\":38604},{\"end\":39062,\"start\":39010},{\"end\":39335,\"start\":39329},{\"end\":39621,\"start\":39563},{\"end\":39976,\"start\":39918},{\"end\":40256,\"start\":40240},{\"end\":40536,\"start\":40478},{\"end\":40910,\"start\":40855},{\"end\":41279,\"start\":41273},{\"end\":41683,\"start\":41625},{\"end\":42095,\"start\":42037},{\"end\":42464,\"start\":42427},{\"end\":42803,\"start\":42751},{\"end\":42998,\"start\":42961},{\"end\":43396,\"start\":43348},{\"end\":43823,\"start\":43761},{\"end\":44272,\"start\":44214},{\"end\":44684,\"start\":44626},{\"end\":45102,\"start\":45050},{\"end\":45489,\"start\":45431},{\"end\":45758,\"start\":45679},{\"end\":46160,\"start\":46098},{\"end\":46569,\"start\":46507},{\"end\":46969,\"start\":46907},{\"end\":47380,\"start\":47343},{\"end\":47717,\"start\":47698},{\"end\":48052,\"start\":48004},{\"end\":48388,\"start\":48339},{\"end\":48704,\"start\":48636},{\"end\":49016,\"start\":48885},{\"end\":49490,\"start\":49454},{\"end\":49742,\"start\":49682},{\"end\":50124,\"start\":50067},{\"end\":50510,\"start\":50452},{\"end\":50863,\"start\":50805},{\"end\":51237,\"start\":51175},{\"end\":51591,\"start\":51533},{\"end\":51970,\"start\":51912},{\"end\":52376,\"start\":52318},{\"end\":52777,\"start\":52730},{\"end\":53129,\"start\":53080},{\"end\":53481,\"start\":53439},{\"end\":53873,\"start\":53824},{\"end\":54261,\"start\":54203},{\"end\":54594,\"start\":54561},{\"end\":54880,\"start\":54822},{\"end\":55229,\"start\":55171},{\"end\":55593,\"start\":55531},{\"end\":55933,\"start\":55875},{\"end\":56274,\"start\":56232},{\"end\":56603,\"start\":56545},{\"end\":56994,\"start\":56946},{\"end\":57302,\"start\":57287},{\"end\":57545,\"start\":57482},{\"end\":57879,\"start\":57831}]"}}}, "year": 2023, "month": 12, "day": 17}