{"id": 257079212, "updated": "2023-10-05 03:41:00.021", "metadata": {"title": "K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging", "authors": "[{\"first\":\"Chaoyi\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Xiaoman\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yanfeng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Ya\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Weidi\",\"last\":\"Xie\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "In this paper, we consider the problem of disease diagnosis. Unlike the conventional learning paradigm that treats labels independently, we propose a knowledge-enhanced framework, that enables training visual representation with the guidance of medical domain knowledge. In particular, we make the following contributions: First, to explicitly incorporate experts' knowledge, we propose to learn a neural representation for the medical knowledge graph via contrastive learning, implicitly establishing relations between different medical concepts. Second, while training the visual encoder, we keep the parameters of the knowledge encoder frozen and propose to learn a set of prompt vectors for efficient adaptation. Third, we adopt a Transformer-based disease-query module for cross-model fusion, which naturally enables explainable diagnosis results via cross attention. To validate the effectiveness of our proposed framework, we conduct thorough experiments on three x-ray imaging datasets across different anatomy structures, showing our model is able to exploit the implicit relations between diseases/findings, thus is beneficial to the commonly encountered problem in the medical domain, namely, long-tailed and zero-shot recognition, which conventional methods either struggle or completely fail to realize.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2302.11557", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2302-11557", "doi": "10.48550/arxiv.2302.11557"}}, "content": {"source": {"pdf_hash": "08c58af3974ee731354062a3b5a81fad67b44424", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.11557v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "35e562cd8c3b3c08490e5a7d7d1c48ee3e753bef", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/08c58af3974ee731354062a3b5a81fad67b44424.txt", "contents": "\nK-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging\n\n\nChaoyi Wu \nCooperative Medianet Innovation Center\nShanghai Jiao Tong University\nShanghaiChina\n\nShanghai AI Laboratory\nShanghaiChina\n\nXiaoman Zhang \nCooperative Medianet Innovation Center\nShanghai Jiao Tong University\nShanghaiChina\n\nShanghai AI Laboratory\nShanghaiChina\n\nYanfeng Wang \nCooperative Medianet Innovation Center\nShanghai Jiao Tong University\nShanghaiChina\n\nShanghai AI Laboratory\nShanghaiChina\n\nYa Zhang \nCooperative Medianet Innovation Center\nShanghai Jiao Tong University\nShanghaiChina\n\nShanghai AI Laboratory\nShanghaiChina\n\nWeidi Xie \nCooperative Medianet Innovation Center\nShanghai Jiao Tong University\nShanghaiChina\n\nShanghai AI Laboratory\nShanghaiChina\n\nK-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging\n\nIn this paper, we consider the problem of disease diagnosis. Unlike the conventional learning paradigm that treats labels independently, we propose a knowledge-enhanced framework, that enables training visual representation with the guidance of medical domain knowledge. In particular, we make the following contributions: First, to explicitly incorporate experts' knowledge, we propose to learn a neural representation of medical knowledge graph via contrastive learning, implicitly establishing relations between different medical concepts. Second, while training the visual encoder, we keep the parameters of the knowledge encoder frozen and propose to learn a set of prompt vectors for efficient adaptation. Third, we adopt a Transformer-based disease-query module for cross-model fusion, which naturally enables explainable diagnosis results via cross attention. To validate the effectiveness of our proposed framework, we conduct thorough experiments on three x-ray imaging datasets across different anatomy structures, showing our model can exploit the implicit relations between diseases/findings, thus is beneficial to the commonly encountered problem in the medical domain, namely, long-tailed and zero-shot recognition, which conventional methods either struggle or completely fail to realize.\n\nIntroduction\n\nThe application of artificial intelligence (AI) has delivered impressive results in diagnosing diseases from medical scans [23]. A commonly adopted framework is to train vision models by supervised learning with discrete labels and predict pathology categories within a fixed-size vocabulary at inference time [6,34]. However, such a learning paradigm suffers from two limitations: first, the model is unable to generalize toward previously unseen categories; second, the labels are converted into one-hot vectors as illustrated in Fig. 1, that are orthogonal in the embedding space, leaving the intrinsic relations between different pathologies or diseases unexploited.\n\nIn the recent literature, jointly training visual-language models has shown promising progress in computer vision [30,31], often called Foundation Models. For example, CLIP [31] and ALIGN [17] have demonstrated remarkable \"zero-shot\" generalization for various downstream tasks by learning the joint representation of image and text with simple noise contrastive learning. Crucially, the data used to train these powerful foundation models can simply be crawled from the Internet without laborious manual annotation. However, as commonly known, collecting training data at scale in the medical domain is often impractical [6,2], due to its safety-critical nature. In this paper, we, therefore, explore an alternative by injecting medical expert knowledge into the visual representation learning procedure. We present a novel knowledge-enhanced classification framework, as shown in the upper right of Fig. 1, first, to explicitly incorporate experts' knowledge, we build a neural representation for the medical knowledge graph via contrastive learning, acting as a \"knowledge encoder\" that explicitly encodes the relations between medical concepts in the embedding space; second, while training the visual encoder, we keep the parameters of the knowledge encoder frozen, and only learn a set of prompt vectors for efficient adaptation; third, we adopt a Transformer-based disease-query module for text-image cross-modality fusion, where the disease names act as queries that cross-attend the visual features and infer the likelihood of the disease existence, naturally, it enables explainable diagnosis results via the cross attentions.\n\nTo demonstrate the effectiveness of the proposed knowledge-enhanced classification framework, we conduct thorough experiments to analyze from three perspectives: first, we experiment on three disease diagnosis tasks across different anatomy structures, to show our method is efficient across various image distributions; second, we train on a combination of 11 public chest x-ray datasets, showing our model can better exploit the potential of various public datasets regardless of their annotation granularity, which traditional training paradigm can slightly benefit from; third, we perform zero-shot disease diagnosis, i.e., evaluating unseen classes on PadChest [4], achieving an AUC of at least 0.600 on 79 out of 106 unseen radiographic findings, Note that, such a task is completely unachievable in conventional supervised training.\n\n\nMethod\n\nIn this section, we start by describing the considered problem scenario in Sec. 2.1, followed by the procedure of condensing the medical domain knowledge into a text encoder in Sec. 2.2. In Sec. 2.3, we detail the proposed knowledge-enhanced classification model, including the visual encoder, knowledge encoder, learnable prompt module, and disease-query module, and describe the training procedure.\n\n\nProblem Scenario\n\nGiven a dataset with sample pairs, i.e.,\nD train = {( 1 , 1 ), ( 2 , 2 ), . . . , ( , )}, where\n\u2208 R \u00d7 \u00d73 refers to the input image, and \u2208 T = { 1 . . . . , } denotes the ground truth annotation from a pool of | | candidate diseases. Unlike conventional supervised learning that often converts the labels to one-hot vectors, our goal is to train a classification model that leverages the semantics encapsulated in the disease category texts, specifically,\n= \u03a6 query (\u03a6 visual ( ), \u03a6 prompt (\u03a6 knowledge (T ))),(1)\nwhere \u2208 R refers to the inferred likelihood of the patient having any disease in T . \u03a6 knowledge (\u00b7), \u03a6 prompt (\u00b7), \u03a6 visual (\u00b7), \u03a6 query (\u00b7) refer to the trainable modules in our proposed knowledge-enhanced classification framework, that will be detailed in the following sections.\n\n\nKnowledge Encoder\n\nTo explicitly incorporate experts' knowledge, we propose to inject the medical domain knowledge into a text encoder (\u03a6 knowledge ), by implicitly modeling the relations between medical entities in the textural embedding space. Specifically, we employ an off-theshelf knowledge graph in the medical community, namely, Unified Medical Language System (UMLS) [3], to fine-tune a pre-trained BERT language model. In the following section, we detail the training procedure, as shown in Fig. 2.\nNotation. Let D UMLS = {( , ) } | | | |\n=1 denote a concept dictionary for UMLS in text form, each concept ( ) is associated with one corresponding definition ( ), for example, the concept \"pulmonary infiltrate\" is defined as \"A finding indicating the presence of an inflammatory or neoplastic cellular infiltrate in the lung parenchyma\".\n\nTraining. Here, we train the text encoder by maximizing the similarities between positive concept-definition pairs, i.e., pull the distance of language description and its corresponding concept in the textual embedding space. Given randomly sampled concepts and definitions, we pass them through a standard BERT architecture [8], and take the average-pooled features as their textual embeddings. i.e., the concept \u2208 R \u00d7 , [  definition \u2208 R \u00d7 . At training time, each mini-batch can be expressed as {( , )} =1 , and the model can be trained via contrastive learning [28]:\nL contrastive = \u2212 1 2 \u2211\ufe01 =1 (log ( , / ) =1 ( , / ) + log ( , / ) =1 ( , / )\n).\n\nwhere \u2208 R + is a scalar temperature parameter. Once this is trained, the text encoder effectively becomes a \"knowledge encoder\" with domain experts' knowledge injected.\n\n\nKnowledge-enhanced Classification Model\n\nAfter injecting the domain knowledge into the text encoder, here, we describe the procedure to guide the visual representation learning with the knowledge encoder. Specifically, the classification model consists of four core modules, namely, the visual encoder, frozen knowledge encoder, prompt module, and disease-query module.\n\nVisual Encoder. Given an image scan \u2208 R \u00d7 \u00d73 , we compute the features with a visual backbone, which can be either ResNet [13] or Vision Transformer [9], = \u03a6 visual ( ) \u2208 R \u210e\u00d7 \u00d7 , where refers to the feature dimension, and \u210e, denote the size of the output feature map, feature dimension is set to 256.\n\nFrozen Knowledge Encoder. Given the disease categories T , we compute the features with the pre-trained knowledge encoder: = \u03a6 knowledge (T ) \u2208 R \u00d7 , where refers to the feature dimension, and refers to the category number. As the number of classes in downstream medical diagnosis datasets is usually extremely limited, e.g., 10 diseases on VinDr-Mammo [25], to prevent the knowledge encoder from over-fitting on certain training classes, deviating from the originally embedded knowledge graph, we keep its parameters frozen, and use it to guide the learning of visual encoder, effectively, such a training procedure resembles knowledge injection into visual representation learning.\n\n\nLearnable Prompt Module.\n\nTo bring more flexibility, we also introduce a learnable prompt module (\u03a6 prompt ) for efficient knowledge adaptation. Specifically, as shown in the lower-right of Fig. 2, it consists of a set of learnable vectors, i.e., \u2208 R \u00d7 , where denotes the feature numbers and is the embedding dimension of each feature. Given the disease embeddings ( \u2208 R \u00d7 ), we first use an MLP to project it into a probability distribution over the learnable prompt vectors, = SoftMax(MLP( )) \u2208 R \u00d7 . Then, the output of the Prompt Module can be calculated as the matrix multiplication between and , i.e., that can be formulated as\n= \u03a6 prompt ( ) = ( \u00b7 ) \u2208 R \u00d7 .\nDisease-Query Module. We use a 4-layer transformer decoder with an MLP to get the final prediction. Given the disease categories T , we have converted them into a set of disease embeddings ( ) with the pre-trained knowledge encoder and learnable prompt module. As inputs to the Transformer decoders, disease embeddings are treated as queries, and the encoded features ( ) act as key and value of the disease-query module: = \u03a6 query ( , ) \u2208 R \u00d7 , where represents the class number and is set as 2, since the diagnosis tasks are all binary classification. Cross-entropy loss is used as the optimization function.\n\n\nExperiments\n\n\nDatasets\n\nIn this paper, we conduct experiments on datasets of X-ray images, due to there exists sufficient data across anatomy structures and annotated pathology categories in this field, supporting thorough evaluation.\n\n\nVinDr-PCXR [27]\n\nis a new pediatric CXR dataset of 9,125 studies, which was officially divided into a training set and a test set of 7,728 and 1,397 studies respectively. Each scan in the training set was manually annotated for the presence of 15 diseases by a pediatric radiologist who has more than ten years of experience, while in the official test set, there are 11 diseases. Additionally, for fair and robust evaluation, we further merged the rare diseases (positive samples less than 5) into \"other diseases\", resulting in only 6 classes in the test set, including no finding, bronchitis, broncho-pneumonia, other disease, bronchiolitis, and pneumonia.\n\n\nVinDr-Mammo [25]\n\nis a full-field digital mammography dataset comprising 20,000 images (5,000 four-view scans). Each scan was manually annotated for no finding or the presence of 10 mammography findings, including mass, calcification, asymmetry, focal asymmetry, global asymmetry, architectural distortion, and suspicious lymph node, skin thickening, skin retraction, nipple retraction. The dataset was official divided into a training set and a test set with 4,000 and 1,000 exams respectively.\n\n\nVinDr-SpineXr [26]\n\nis a spine X-ray dataset comprising 10,468 spine X-ray images from 5,000 studies. Each image was manually annotated by an experienced radiologist with no finding or abnormal findings in 7 categories, including osteophytes, foraminal stenosis, vertebral collapse, disc space narrowing, spondylolysthesis, surgical implant, and other lesions. The dataset was official divided into a training set a test set of 4,000 and 1,000 studies respectively.\n\n\nPadChest [4]\n\nis a chest X-ray dataset with 160,868 chest X-ray images labeled with 174 different radiographic findings, 19 differential diagnoses, only 27% of the labels (totaling 39,053 examples) come from board-certified radiologists, and the rest are obtained by using a recurrent neural network with attention trained on the radiology reports. For evaluation purposes, we only test on samples annotated by board-certified radiologists, and report the zero-shot test results.\n\n\nCXR-Mix.\n\nIn this paper, we also construct a dataset by assembling 11 public datasets, including ChestXray-14 [ [19], termed as CXR-Mix. We refer the readers to [6] for more details of these datasets. For the datasets [33,15] with official train/val/test splits, we use them directly, for those [22,24,29,14] with official train/test splits, we random split the train split with 0.8/0.2 for train/val, in other cases, we randomly split the datasets [32,11,16,7,19] with 0.7/0.1/0.2 for train/val/test. As a result, our constructed CXR-Mix ends up with 763, 520 chest X-rays for training, 28, 925 for val, and 28, 448 for test, spanning across a total of 38 classes in the CXR-Mix. Note that, each of these datasets was originally collected to serve different purposes, the annotations are often partially available, for example, the images from the pneumonia dataset lack labels for pneumothorax, we use \u22121 to denote the label missing and will not calculate the final CE loss on them.\n\n\nImplementation and Training Details\n\nKnowledge-Enhanced Text Encoder. To construct the knowledge encoder, we initialize the text encoder from ClinicalBERT [1], and finetune it for 100K training steps.\n\nIn each mini-batch, 64 concept-definition pairs are used for training. We set the maximal sequence length as 256, though the definition could be long sometimes. We use AdamW [21] as the optimizer with = 1 \u00d7 10 \u22124 and warm up = 1 \u00d7 10 \u22125 .\n\n\nKnowledge-Enhanced Classification Framework.\n\nWe freeze the knowledge encoder and set other parts to be learnable, i.e., visual encoder, prompt module, disease-query module. We train the model for 100 epochs with batch size 128, and use AdamW [21] is adopted as the optimizer with = 1 \u00d7 10 \u22124 and warm up = 1 \u00d7 10 \u22125 .\n\n\nResults\n\nHere, we conduct experiments to validate the effectiveness of our knowledge-enhanced classification model. In Sec. 4.1, we first compare knowledge-enhanced training with standard training using discrete labels across different architectures and then perform a thorough ablation study of proposed modules. In Sec. 4.2, we conduct an analysis on the proposed knowledge encoder by replacing it with other pre-trained language models. In Sec. 4.3, we experiment on CXR-Mix, to show that our model can effectively exploit knowledge across datasets with varying annotation granularity, we conduct analysis from three aspects: (i) the ability to combine various partial-labeled datasets, (ii) to leverage implicit relations between diseases, (iii) to diagnose diseases that are unseen at training time, resembling an open-set recognition scenario. \n\n\nAnalysis of Knowledge-Enhanced Classification Model\n\nComparison to Conventional Training Scheme. As baselines, we adopt the widely used ResNet-50 [13] and ViT-16 [10], and train with conventional learning scheme, i.e., using discrete labels. The results are summarized in Tab. 1, we refer the reader to detailed results for each category presented in the supplementary material (Tab. 4, 6 and 5). Our proposed knowledge-enhanced model achieves a higher average AUC on all three datasets across different architectures.\n\n\nAblation Study.\n\nWe conduct a thorough ablation study of the proposed model by removing individual modules and varying the hyper-parameters, as shown in Tab. 1. Specifically, the performance of ResNet-50 is improved to 71.39%, 83.23%, and 87.76% for the three different tasks equipped with the proposed knowledge encoder to guide the visual representation learning. While combining with the learnable prompt (LP) module, which potentially offers more flexibility for knowledge adaptation, we observe a significant performance gain, up to 2.58% on average AUC scores on VinDr-PCXR. The same conclusion can be drawn for the ViT-16 visual backbone.\n\nTo analyze the effect of prompts, we experiment with different numbers of prompts. As shown, the optimal number of prompts varies from task to task, but in general, adding the LP module benefits all the downstream tasks. The only exception is for the VinDR-Mammo task with ResNet as a backbone, which is mainly caused by the extremely small test samples in some categories (Tab. 5), e.g., skin retraction and skin thickening.\n\n\nQualitative Visualisation.\n\nTo provide a visualization that can potentially be used for clinicians to discover and understand the evidence that AI algorithm bases its predictions on, the disease-query module in our proposed architecture enables detailed visualization for each of the queries with positive output. Specifically, we average the cross-attention map in each transformer layer in the disease query module, and visualize the results in Fig. 3. The model's attention well matches radiologists' diagnoses of different diseases, i.e. red boxes labeled by board-certified radiologists. \n\n\nAnalysis of the Knowledge-Enhanced Text Encoder\n\nHere we investigate another way of incorporating prior knowledge, that is, to guide the visual representation with a text encoder pre-trained on the large medical corpus, such as the electronic health records MIMIC III [18] or scientific publications PubMed, to guide the classification task. As shown in Tab. 2, while comparing with the models that adopts ClinicalBERT [1] or PubMedBERT [12] as knowledge encoder, we can make two observations: (i) guiding visual representation learning with domain knowledge generally works better, e.g., results of using ClinicalBERT or PubMedBERT outperform conventional training with discrete labels, (ii) our proposed knowledge-enhanced text encoder consistently demonstrates superior results, that can be attributed to the explicitly injected domain knowledge, rather than implicitly learning it from the document corpus. Table 2. Ablation study on knowledge encoder with ResNet as a backbone, mean and standard deviation for AUC scores is reported with three different seeds. we use the optimal prompt numbers according to the ablation study, i.e., 32 for VinDr-PCXR, 128 for VinDr-Mammo, and 64 for VinDr-SpineXr.\n\n\nModel\n\nKnowledge Encoder VinDr-PCXR VinDr-Mammo VinDr-SpineXr \n\n\nAnalysis on the CXR-Mix\n\nIn this section, we experiment on the assembled dataset CXR-Mix, to demonstrate the effectiveness of our proposed framework in exploiting knowledge across datasets and annotation granularity.\n\n\nThe Ability to Combine Various Partial-labeled Datasets:\n\nUnlike the traditional approach, which requires to carefully merging the label space from different datasets [6,5,20] to benefit from them, our formulation of embedding the 'disease name' with a knowledge encoder naturally enables us to train models on the mixture of multiple datasets, handling different granularities of diagnosis targets and inconsistent pathology expression. As shown in Tab. 3, we compare to TorchXRayVision [6] that merges the label space and trains a baseline model with discrete labels, our knowledge-enhanced framework improves the performance from 82.60% to 85.13% and form 77.39% to 79.54% under the two commonly-used backbones, ResNet and ViT, respectively. The Ability to Leverage Class Diversity: In this part, we further show that our framework can significantly improve the performance of each dataset, by training on data from other categories. Specifically, we consider each dataset separately, i.e., measuring the performance on their own test splits. We propose to decouple the data increment into two dimensions, \"Diversity\" and \"Amount\". \"Diversity\" refers to only adding the cases beyond the target classes and keeping the amount of data of target classes constant, while \"Amount\" refers to increasing the target class cases. As shown in Fig. 4, based on our structure, adding \"Diversity\" can improve the results on all 11 datasets. In particular, for some relatively small datasets, the gain is more significant, e.g., GoogleNIH, SIIM-ACR, and OpenI. Such an experiment has validated that a knowledge-enhanced model enables to leverage of the shared information between classes, and can be greatly beneficial for dealing with long-tailed diseases, which are seldom annotated in common datasets, by leveraging the publicly available data. Fig. 4. Analyse the performance gain on the assembling dataset. \"Seperation\" refers to using a single dataset to train our framework. \"+Diversity\" refers to adding the cases beyond the target classes, increasing the class diversity, and keeping the data amount of the target classes constant. \"+Diversity+Amount\" means directly mixing the 11 datasets and for most datasets, the data amount of the target classes will further increase.\n\n\nThe Ability to Diagnose Open-set Unseen Diseases:\n\nConventional models can only handle a close-set classification, while, with knowledge-enhanced design, our model enables to predict open-set diseases that never appear in the training set. This is meaningful for clinical practical usage, as some rare or new diseases can hardly find an off-shelf dataset to re-train models. We test our model on the PadChest testset [4] and dismiss the classes that exist in the assembling dataset and those having very few cases ( <= 50), that can hardly have statistically convincing test results. During testing, to get the embedding, we simply input the names of unseen classes into the knowledge encoder, and continue the standard evaluation procedure, i.e., learnable prompt module, disease-query module. As shown in Fig. 5, without any example in the training set, our model can directly achieve an AUC of at least 0.800 on 14 findings, at least 0.700 on 46 findings and at least 0.600 on 79 findings (unseen at training time) out of 106 radiographic findings. This demonstrates our model can break the limits of the labeling classes and be adapted to more practical medical scenarios. \n\n\nConclusion\n\nIn this paper, we propose a novel knowledge-enhanced classification model, that enables learning visual representation by exploiting the relationship between different medical concepts in the knowledge graph. While conducting thorough experiments on x-ray image datasets across different anatomy structures, we show that injecting medical prior knowledge is beneficial for tackling (i) long-tailed recognition, (ii) zero-shot recognition.\n\nAs for future work, we plan to generalize the idea towards self-supervised learning on pairs of image and text reports and more diverse modalities. Table 5. Compare with Baseline Models on VinDr-Mammo Task. For each disease, AUC score is reported. We report the mean and standard deviation of three different seeds. We use the combine of the first two letters of each word to represent the disease with two words, and first four letters to represent the disease with only one word.  Table 6. Compare with Baseline Models on VinDr-SpineXR Task. For each disease, AUC score is reported. We report the mean and standard deviation of three different seeds. We use the combine of the first two letters of each word to represent the disease with two words, and first four letters to represent the disease with only one word. \n\nFig. 1 .\n1of lung tissue caused by a bacterial, or fungal infection in o lungs accompanied by inflammation of the alveoli. a type of pneumonia that causes inflammation in the alveoli.infection of the main airways of the lungs (bronchi), causing them irritated and inflamed. In the conventional training scheme (left), manual annotations are often converted into discrete one-hot vectors, that are orthogonal in the embedding space, thus ignoring the implicit relations between labels. While in our proposed knowledge-enhanced classification framework (right), the labels are transformed into continuous vectors in a knowledge embedding space, to capture the implicit relations, and further used to supervise visual representation learning.\n\nFig. 2 .\n2Overview of the knowledge-enhanced disease diagnosis workflow. The knowledge encoder (left) is first trained to learn a neural representation of the medical knowledge graph via contrastive learning, and then used to guide the visual representation learning in our knowledgeenhanced classification model (right).\n\nFig. 3 .\n3Sample visualization of randomly chosen samples from VinDr-SpinXr, we present both the original image (left) and an attention map generated from our proposed model with ResNet-50 as the backbone (right).\n\nFig. 5 .\n5AUC and 95% CI are shown on the unseen classes under the zero-shot setting. represents the number of related cases. The top 46 classes are plotted in the figure to show what classes our model can achieve AUC > 0.700 on. Generally, our method achieves an AUC of at least 0.800 on 14 findings and at least 0.600 on 79 findings out of 106 radiographic findings where > 50 in the PadChest test dataset ( = 39, 053).\n\nTable 1 .\n1Compare with Baseline Models with ResNet-50[13] and ViT-16[9] as backbone on disease classification tasks. KE indicates the proposed knowledge encoder, LP indicates the proposed learnable prompt module, and the number denotes the prompt number. AUC scores averaged across different diseases are reported. We report the mean and standard deviation of three different seeds.Model \nBackbone KE LP VinDr-PCXR VinDr-Mammo VinDr-SpineXr \n\nRes [13] \nResNet-50 \n\n70.53 \u00b1 1.00 \n82.54 \u00b1 1.79 \n87.35 \u00b1 0.38 \nRes+KE \nResNet-50 \n\n71.39 \u00b1 0.66 \n83.23 \u00b1 1.98 \n87.76 \u00b1 0.41 \nRes+KE+LP ResNet-50 \n32 \n73.97 \u00b1 0.28 \n80.59 \u00b1 3.11 \n88.46 \u00b1 0.22 \nRes+KE+LP ResNet-50 \n64 \n72.88 \u00b1 0.19 \n81.27\u00b1 1.83 \n88.90 \u00b1 0.04 \nRes+KE+LP ResNet-50 128 73.70 \u00b1 0.41 \n84.80 \u00b1 1.04 \n88.22 \u00b1 0.53 \n\nViT [9] \nViT-16 \n\n\n69.06\u00b1 0.74 \n80.50 \u00b1 2.47 \n85.56 \u00b1 0.97 \nViT+KE \nViT-16 \n\n\n71.69 \u00b1 1.64 \n83.89 \u00b1 0.38 \n85.75 \u00b1 0.44 \nViT+KE+LP ViT-16 \n\n32 \n72.90 \u00b1 0.97 \n83.67 \u00b1 2.34 \n86.55 \u00b1 0.48 \nViT+KE+LP ViT-16 \n\n64 \n71.07 \u00b1 3.28 \n84.33 \u00b1 1.54 \n86.83 \u00b1 0.81 \nViT+KE+LP ViT-16 \n128 72.47 \u00b1 0.78 \n84.46 \u00b1 0.84 \n86.33 \u00b1 0.23 \n\n\n\nTable 3 .\n3Compare with Baseline Models on disease classification tasks on the assembling dataset.AvgAUC refers to the AUC score averaged across different diseases. The first line refers to the use \nof the training flow proposed by TorchXrayVision [6] and use ResNet or ViT as the backbone. \n\nMethods \nPrompt \nAvgAUC \nMethods \nPrompt \nAvgAUC \n\nRes [6] \n-\n82.60 \u00b1 1.27 ViT [6] \n-\n77.39 \u00b1 0.71 \nRes+KE \n-\n83.11 \u00b1 0.27 ViT+KE \n-\n78.30 \u00b1 0.93 \nRes+KE+LP \n32 \n84.45 \u00b1 1.06 ViT+KE+LP \n32 \n79.25 \u00b1 1.05 \nRes+KE+LP \n64 \n85.13 \u00b1 0.78 ViT+KE+LP \n64 \n79.54 \u00b1 0.60 \nRes+KE+LP \n128 \n83.38 \u00b1 0.18 ViT+KE+LP \n128 \n78.42 \u00b1 0.71 \n\n\n\n\nModelPrompt Mass Asym NiRe SkTh FoAs SkRe ArDi SuCa SuLN NoFi GlAs MeanResNet [13] \n-\n78.52 \n\u00b1 2.32 \n\nA Detail Results of Knowledge-Enhanced Classification ModelWe show detailed results on the VinDr-PCXR dataset in Tab 4, VinDr-Mammo dataset in Tab 5, and VinDr-SpineXr dataset in Tab 6. The proposed knowledge-enhanced classification model exceed the conventional classification model on most diseases. Note that, our model significantly boosts the performance from 73.29% to 80.07% on Brocho-pneumonia in VinDr-PCXR, demonstrating the benefits of injecting relations between different diseases (the relation and definition of bronchilits, pneumonia and broncho-pneumonia are shown in theFig 1).Table 4. Compare with Baseline Models on VinDr-PCXR Task. For each disease, AUC score is reported. We report the mean and standard deviation of three different seeds. We use the combine of the first two letters of each word to represent the disease with two words, and first four letters to represent the disease with only one word. \u00b1 1.04  Train number  792  69  25  40  200  13  85  293  38  13143  17  Test number  219  20  7  12  52  2  24  105  10  3643  6  \u00b1 0.53   Train number  4260  3575  271  157  333  602  257  257  Test number  1070  878  60  52  80  148  62  64\nE Alsentzer, J R Murphy, W Boag, W H Weng, D Jin, T Naumann, M Mcdermott, arXiv:1904.03323Publicly available clinical bert embeddings. arXiv preprintAlsentzer, E., Murphy, J.R., Boag, W., Weng, W.H., Jin, D., Naumann, T., McDermott, M.: Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323 (2019)\n\nDeep learning and medical diagnosis: A review of literature. M Bakator, D Radosav, Multimodal Technologies and Interaction. 2347Bakator, M., Radosav, D.: Deep learning and medical diagnosis: A review of literature. Multimodal Technologies and Interaction 2(3), 47 (2018)\n\nThe unified medical language system (umls): integrating biomedical terminology. O Bodenreider, Nucleic acids research. 32suppl_1Bodenreider, O.: The unified medical language system (umls): integrating biomedical termi- nology. Nucleic acids research 32(suppl_1), D267-D270 (2004)\n\nPadchest: A large chest x-ray image dataset with multi-label annotated reports. A Bustos, A Pertusa, J M Salinas, M De La Iglesia-Vay\u00e1, Medical image analysis. 66101797Bustos, A., Pertusa, A., Salinas, J.M., de la Iglesia-Vay\u00e1, M.: Padchest: A large chest x-ray image dataset with multi-label annotated reports. Medical image analysis 66, 101797 (2020)\n\nOn the limits of cross-domain generalization in automated x-ray prediction. J P Cohen, M Hashir, R Brooks, H Bertrand, Medical Imaging with Deep Learning. PMLRCohen, J.P., Hashir, M., Brooks, R., Bertrand, H.: On the limits of cross-domain generalization in automated x-ray prediction. In: Medical Imaging with Deep Learning. pp. 136-155. PMLR (2020)\n\nTorchxrayvision: A library of chest x-ray datasets and models. J P Cohen, J D Viviano, P Bertin, P Morrison, P Torabian, M Guarrera, M P Lungren, A Chaudhari, R Brooks, M Hashir, H Bertrand, Proceedings of The 5th International Conference on Medical Imaging with Deep Learning. Proceedings of Machine Learning Research. The 5th International Conference on Medical Imaging with Deep Learning. Machine Learning Research172Cohen, J.P., Viviano, J.D., Bertin, P., Morrison, P., Torabian, P., Guarrera, M., Lungren, M.P., Chaudhari, A., Brooks, R., Hashir, M., Bertrand, H.: Torchxrayvision: A library of chest x-ray datasets and models. In: Proceedings of The 5th International Conference on Medical Imaging with Deep Learning. Proceedings of Machine Learning Research, vol. 172, pp. 231-249 (06-08 Jul 2022)\n\nPreparing a collection of radiology examinations for distribution and retrieval. D Demner-Fushman, M D Kohli, M B Rosenman, S E Shooshan, L Rodriguez, S Antani, G R Thoma, C J Mcdonald, Journal of the American Medical Informatics Association. 232Demner-Fushman, D., Kohli, M.D., Rosenman, M.B., Shooshan, S.E., Rodriguez, L., Antani, S., Thoma, G.R., McDonald, C.J.: Preparing a collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association 23(2), 304-310 (2016)\n\nJ Devlin, M W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)\n\nA Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale. ICLR. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., De- hghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. ICLR (2021)\n\nA Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, arXiv:2010.11929An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., De- hghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n\nCrowdsourcing pneumothorax annotations using machine learning annotations on the nih chest x-ray dataset. R W Filice, A Stein, C C Wu, V A Arteaga, S Borstelmann, R Gaddikeri, M Galperin-Aizenberg, R R Gill, M C Godoy, S B Hobbs, Journal of digital imaging. 33Filice, R.W., Stein, A., Wu, C.C., Arteaga, V.A., Borstelmann, S., Gaddikeri, R., Galperin- Aizenberg, M., Gill, R.R., Godoy, M.C., Hobbs, S.B., et al.: Crowdsourcing pneumothorax annotations using machine learning annotations on the nih chest x-ray dataset. Journal of digital imaging 33, 490-496 (2020)\n\nDomain-specific language model pretraining for biomedical natural language processing. Y Gu, R Tinn, H Cheng, M Lucas, N Usuyama, X Liu, T Naumann, J Gao, H Poon, ACM Transactions on Computing for Healthcare (HEALTH). 31Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., Poon, H.: Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH) 3(1), 1-23 (2021)\n\nDeep residual learning for image recognition. K He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., et al.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n\nObject-cxr -automatic detection of foreign objects on chest x-rays. J Healthcare, Healthcare, J.: Object-cxr -automatic detection of foreign objects on chest x-rays (2020), https://web.archive.org/web/20201127235812/https://jfhealthcare. github.io/object-CXR/\n\nChexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. J Irvin, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence33Irvin, J., et al.: Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In: Proceedings of the AAAI conference on artificial intelligence. vol. 33, pp. 590-597 (2019)\n\nTwo public chest x-ray datasets for computer-aided screening of pulmonary diseases. S Jaeger, S Candemir, S Antani, Y X J W\u00e1ng, P X Lu, G Thoma, Quantitative imaging in medicine and surgery. 46475Jaeger, S., Candemir, S., Antani, S., W\u00e1ng, Y.X.J., Lu, P.X., Thoma, G.: Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery 4(6), 475 (2014)\n\nScaling up visual and vision-language representation learning with noisy text supervision. C Jia, Y Yang, Y Xia, Y T Chen, Z Parekh, H Pham, Q Le, Y H Sung, Z Li, T Duerig, International Conference on Machine Learning. PMLRJia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy text supervision. In: International Conference on Machine Learning. pp. 4904-4916. PMLR (2021)\n\nMimic-iii, a freely accessible critical care database. A E W Johnson, T J Pollard, L Shen, H Wei, L Lehman, M Feng, M M Ghassemi, B Moody, P Szolovits, L A Celi, R G Mark, Scientific Data. 3Johnson, A.E.W., Pollard, T.J., Shen, L., wei H. Lehman, L., Feng, M., Ghassemi, M.M., Moody, B., Szolovits, P., Celi, L.A., Mark, R.G.: Mimic-iii, a freely accessible critical care database. Scientific Data 3 (2016)\n\nMimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. A E Johnson, T J Pollard, S J Berkowitz, N R Greenbaum, M P Lungren, C Y Deng, R G Mark, S Horng, Scientific data. 61317Johnson, A.E., Pollard, T.J., Berkowitz, S.J., Greenbaum, N.R., Lungren, M.P., Deng, C.y., Mark, R.G., Horng, S.: Mimic-cxr, a de-identified publicly available database of chest radio- graphs with free-text reports. Scientific data 6(1), 317 (2019)\n\nMseg: A composite dataset for multidomain semantic segmentation. J Lambert, Z Liu, O Sener, J Hays, V Koltun, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionLambert, J., Liu, Z., Sener, O., Hays, J., Koltun, V.: Mseg: A composite dataset for multi- domain semantic segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2879-2888 (2020)\n\nI Loshchilov, F Hutter, arXiv:1711.05101Decoupled weight decay regularization. arXiv preprintLoshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)\n\nChest radiograph interpretation with deep learning models: assessment with radiologist-adjudicated reference standards and population-adjusted evaluation. A Majkowska, S Mittal, D F Steiner, J J Reicher, S M Mckinney, G E Duggan, K Eswaran, P H Cameron Chen, Y Liu, S R Kalidindi, Radiology. 2942Majkowska, A., Mittal, S., Steiner, D.F., Reicher, J.J., McKinney, S.M., Duggan, G.E., Eswaran, K., Cameron Chen, P.H., Liu, Y., Kalidindi, S.R., et al.: Chest radiograph in- terpretation with deep learning models: assessment with radiologist-adjudicated reference standards and population-adjusted evaluation. Radiology 294(2), 421-431 (2020)\n\nArtificial intelligence in disease diagnostics: A critical review and classification on the current state of research guiding future direction. M Mirbabaie, S Stieglitz, N R J Frick, Health and Technology. 114Mirbabaie, M., Stieglitz, S., Frick, N.R.J.: Artificial intelligence in disease diagnostics: A critical review and classification on the current state of research guiding future direction. Health and Technology 11(4), 693-731 (2021)\n\nVindr-cxr: An open dataset of chest x-rays with radiologist's annotations. H Q Nguyen, K Lam, L T Le, H H Pham, D Q Tran, D B Nguyen, D D Le, C M Pham, H T Tong, D H Dinh, Scientific Data. 91429Nguyen, H.Q., Lam, K., Le, L.T., Pham, H.H., Tran, D.Q., Nguyen, D.B., Le, D.D., Pham, C.M., Tong, H.T., Dinh, D.H., et al.: Vindr-cxr: An open dataset of chest x-rays with radiol- ogist's annotations. Scientific Data 9(1), 429 (2022)\n\nVindrmammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. H T Nguyen, H Q Nguyen, H H Pham, K Lam, L T Le, M Dao, V H Vu, PhysioNet. Nguyen, H.T., Nguyen, H.Q., Pham, H.H., Lam, K., Le, L.T., Dao, M., Vu, V.H.: Vindr- mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. PhysioNet (2022)\n\nVindr-spinexr: A deep learning framework for spinal lesions detection and classification from radiographs. H T Nguyen, H Pham, N T Nguyen, H Q Nguyen, T Q Huynh, M Dao, V H Vu, ArXiv abs/2106.12930Nguyen, H.T., Pham, H., Nguyen, N.T., Nguyen, H.Q., Huynh, T.Q., Dao, M., Vu, V.H.: Vindr-spinexr: A deep learning framework for spinal lesions detection and classification from radiographs. ArXiv abs/2106.12930 (2021)\n\nVindr-pcxr: An open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children. N H Nguyen, H Pham, T T Tran, T N M Nguyen, H Q Nguyen, PhysioNet. Nguyen, N.H., Pham, H., Tran, T.T., Nguyen, T.N.M., Nguyen, H.Q.: Vindr-pcxr: An open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children. PhysioNet (2022)\n\nA Oord, Y Li, O Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintOord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018)\n\nCovid-net cxr-2: An enhanced deep convolutional neural network design for detection of covid-19 cases from chest x-ray images. M Pavlova, Frontiers in Medicine. 9Pavlova, M., et al.: Covid-net cxr-2: An enhanced deep convolutional neural network design for detection of covid-19 cases from chest x-ray images. Frontiers in Medicine 9 (2022)\n\nWhat does a platypus look like? generating customized prompts for zero-shot image classification. S Pratt, R Liu, A Farhadi, 2209Pratt, S., Liu, R., Farhadi, A.: What does a platypus look like? generating customized prompts for zero-shot image classification. arXiv e-prints pp. arXiv-2209 (2022)\n\nLearning transferable visual models from natural language supervision. A Radford, International Conference on Machine Learning. PMLRRadford, A., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning. pp. 8748-8763. PMLR (2021)\n\nAugmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia. G Shih, C C Wu, S S Halabi, M D Kohli, L M Prevedello, T S Cook, A Sharma, J K Amorosa, V Arteaga, M Galperin-Aizenberg, Radiology: Artificial Intelligence. 11180041Shih, G., Wu, C.C., Halabi, S.S., Kohli, M.D., Prevedello, L.M., Cook, T.S., Sharma, A., Amorosa, J.K., Arteaga, V., Galperin-Aizenberg, M., et al.: Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia. Radiology: Artificial Intelligence 1(1), e180041 (2019)\n\nChestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. X Wang, Y Peng, L Lu, Z Lu, M Bagheri, R M Summers, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionWang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Summers, R.M.: Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2097-2106 (2017)\n\nLong-tailed multi-label retinal diseases recognition via relational learning and knowledge distillation. Q Zhou, H Zou, Z Wang, Medical Image Computing and Computer Assisted Intervention. Zhou, Q., Zou, H., Wang, Z.: Long-tailed multi-label retinal diseases recognition via relational learning and knowledge distillation. In: Medical Image Computing and Computer Assisted Intervention. p. 709-718 (2022)\n\n. 64 75.34\u00b10.20 69.91\u00b11.72 78.94\u00b10.68 66.13\u00b11.31 69.91\u00b11.36 77.07\u00b11.49 72.88\u00b10.19Res+KE+LP. Res+KE+LP 64 75.34\u00b10.20 69.91\u00b11.72 78.94\u00b10.68 66.13\u00b11.31 69.91\u00b11.36 77.07\u00b11.49 72.88\u00b10.19\n\n. 128 75.59\u00b10.59 70.62\u00b1 0.54 80.07\u00b10.58 67.23\u00b10.32 70.71\u00b11.88 77.98\u00b11.45 73.70\u00b10.41Res+KE+LP. Res+KE+LP 128 75.59\u00b10.59 70.62\u00b1 0.54 80.07\u00b10.58 67.23\u00b10.32 70.71\u00b11.88 77.98\u00b11.45 73.70\u00b10.41\n", "annotations": {"author": "[{\"end\":204,\"start\":72},{\"end\":341,\"start\":205},{\"end\":477,\"start\":342},{\"end\":609,\"start\":478},{\"end\":742,\"start\":610}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":79},{\"end\":218,\"start\":213},{\"end\":354,\"start\":350},{\"end\":486,\"start\":481},{\"end\":619,\"start\":616}]", "author_first_name": "[{\"end\":78,\"start\":72},{\"end\":212,\"start\":205},{\"end\":349,\"start\":342},{\"end\":480,\"start\":478},{\"end\":615,\"start\":610}]", "author_affiliation": "[{\"end\":165,\"start\":83},{\"end\":203,\"start\":167},{\"end\":302,\"start\":220},{\"end\":340,\"start\":304},{\"end\":438,\"start\":356},{\"end\":476,\"start\":440},{\"end\":570,\"start\":488},{\"end\":608,\"start\":572},{\"end\":703,\"start\":621},{\"end\":741,\"start\":705}]", "title": "[{\"end\":69,\"start\":1},{\"end\":811,\"start\":743}]", "venue": null, "abstract": "[{\"end\":2117,\"start\":813}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2260,\"start\":2256},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2446,\"start\":2443},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2449,\"start\":2446},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2923,\"start\":2919},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2926,\"start\":2923},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2982,\"start\":2978},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2997,\"start\":2993},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3430,\"start\":3427},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3432,\"start\":3430},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5112,\"start\":5109},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6890,\"start\":6887},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7688,\"start\":7685},{\"end\":7783,\"start\":7782},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7929,\"start\":7925},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8680,\"start\":8676},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8706,\"start\":8703},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9214,\"start\":9210},{\"end\":13280,\"start\":13279},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13285,\"start\":13281},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13333,\"start\":13330},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13391,\"start\":13387},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13394,\"start\":13391},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13468,\"start\":13464},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13471,\"start\":13468},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13474,\"start\":13471},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13477,\"start\":13474},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13622,\"start\":13618},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13625,\"start\":13622},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13628,\"start\":13625},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13630,\"start\":13628},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13633,\"start\":13630},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14314,\"start\":14311},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14536,\"start\":14532},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14846,\"start\":14842},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15923,\"start\":15919},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15939,\"start\":15935},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18237,\"start\":18233},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18387,\"start\":18384},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18406,\"start\":18402},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19626,\"start\":19623},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19628,\"start\":19626},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19631,\"start\":19628},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19947,\"start\":19944},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22150,\"start\":22147},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25943,\"start\":25939},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25957,\"start\":25954}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24922,\"start\":24182},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25245,\"start\":24923},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25460,\"start\":25246},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25883,\"start\":25461},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":26970,\"start\":25884},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":27586,\"start\":26971},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":27690,\"start\":27587}]", "paragraph": "[{\"end\":2803,\"start\":2133},{\"end\":4441,\"start\":2805},{\"end\":5282,\"start\":4443},{\"end\":5693,\"start\":5293},{\"end\":5754,\"start\":5714},{\"end\":6168,\"start\":5810},{\"end\":6509,\"start\":6227},{\"end\":7019,\"start\":6531},{\"end\":7358,\"start\":7060},{\"end\":7930,\"start\":7360},{\"end\":8010,\"start\":8008},{\"end\":8180,\"start\":8012},{\"end\":8552,\"start\":8224},{\"end\":8855,\"start\":8554},{\"end\":9540,\"start\":8857},{\"end\":10177,\"start\":9569},{\"end\":10819,\"start\":10209},{\"end\":11056,\"start\":10846},{\"end\":11718,\"start\":11076},{\"end\":12216,\"start\":11739},{\"end\":12684,\"start\":12239},{\"end\":13166,\"start\":12701},{\"end\":14153,\"start\":13179},{\"end\":14356,\"start\":14193},{\"end\":14596,\"start\":14358},{\"end\":14917,\"start\":14645},{\"end\":15770,\"start\":14929},{\"end\":16291,\"start\":15826},{\"end\":16939,\"start\":16311},{\"end\":17366,\"start\":16941},{\"end\":17962,\"start\":17397},{\"end\":19169,\"start\":18014},{\"end\":19234,\"start\":19179},{\"end\":19453,\"start\":19262},{\"end\":21727,\"start\":19514},{\"end\":22907,\"start\":21781},{\"end\":23360,\"start\":22922},{\"end\":24181,\"start\":23362}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5809,\"start\":5755},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6226,\"start\":6169},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7059,\"start\":7020},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8007,\"start\":7931},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10208,\"start\":10178}]", "table_ref": "[{\"end\":18883,\"start\":18876},{\"end\":23517,\"start\":23510},{\"end\":23852,\"start\":23845}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2131,\"start\":2119},{\"attributes\":{\"n\":\"2\"},\"end\":5291,\"start\":5285},{\"attributes\":{\"n\":\"2.1\"},\"end\":5712,\"start\":5696},{\"attributes\":{\"n\":\"2.2\"},\"end\":6529,\"start\":6512},{\"attributes\":{\"n\":\"2.3\"},\"end\":8222,\"start\":8183},{\"end\":9567,\"start\":9543},{\"attributes\":{\"n\":\"3\"},\"end\":10833,\"start\":10822},{\"attributes\":{\"n\":\"3.1\"},\"end\":10844,\"start\":10836},{\"end\":11074,\"start\":11059},{\"end\":11737,\"start\":11721},{\"end\":12237,\"start\":12219},{\"end\":12699,\"start\":12687},{\"end\":13177,\"start\":13169},{\"attributes\":{\"n\":\"3.2\"},\"end\":14191,\"start\":14156},{\"end\":14643,\"start\":14599},{\"attributes\":{\"n\":\"4\"},\"end\":14927,\"start\":14920},{\"attributes\":{\"n\":\"4.1\"},\"end\":15824,\"start\":15773},{\"end\":16309,\"start\":16294},{\"end\":17395,\"start\":17369},{\"attributes\":{\"n\":\"4.2\"},\"end\":18012,\"start\":17965},{\"end\":19177,\"start\":19172},{\"attributes\":{\"n\":\"4.3\"},\"end\":19260,\"start\":19237},{\"end\":19512,\"start\":19456},{\"end\":21779,\"start\":21730},{\"attributes\":{\"n\":\"5\"},\"end\":22920,\"start\":22910},{\"end\":24191,\"start\":24183},{\"end\":24932,\"start\":24924},{\"end\":25255,\"start\":25247},{\"end\":25470,\"start\":25462},{\"end\":25894,\"start\":25885},{\"end\":26981,\"start\":26972}]", "table": "[{\"end\":26970,\"start\":26268},{\"end\":27586,\"start\":27070},{\"end\":27690,\"start\":27660}]", "figure_caption": "[{\"end\":24922,\"start\":24193},{\"end\":25245,\"start\":24934},{\"end\":25460,\"start\":25257},{\"end\":25883,\"start\":25472},{\"end\":26268,\"start\":25896},{\"end\":27070,\"start\":26983},{\"end\":27660,\"start\":27589}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2671,\"start\":2665},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3712,\"start\":3706},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7018,\"start\":7012},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9739,\"start\":9733},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17822,\"start\":17816},{\"end\":20798,\"start\":20792},{\"end\":21299,\"start\":21293},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22543,\"start\":22537}]", "bib_author_first_name": "[{\"end\":28862,\"start\":28861},{\"end\":28875,\"start\":28874},{\"end\":28877,\"start\":28876},{\"end\":28887,\"start\":28886},{\"end\":28895,\"start\":28894},{\"end\":28897,\"start\":28896},{\"end\":28905,\"start\":28904},{\"end\":28912,\"start\":28911},{\"end\":28923,\"start\":28922},{\"end\":29245,\"start\":29244},{\"end\":29256,\"start\":29255},{\"end\":29536,\"start\":29535},{\"end\":29817,\"start\":29816},{\"end\":29827,\"start\":29826},{\"end\":29838,\"start\":29837},{\"end\":29840,\"start\":29839},{\"end\":29851,\"start\":29850},{\"end\":30167,\"start\":30166},{\"end\":30169,\"start\":30168},{\"end\":30178,\"start\":30177},{\"end\":30188,\"start\":30187},{\"end\":30198,\"start\":30197},{\"end\":30506,\"start\":30505},{\"end\":30508,\"start\":30507},{\"end\":30517,\"start\":30516},{\"end\":30519,\"start\":30518},{\"end\":30530,\"start\":30529},{\"end\":30540,\"start\":30539},{\"end\":30552,\"start\":30551},{\"end\":30564,\"start\":30563},{\"end\":30576,\"start\":30575},{\"end\":30578,\"start\":30577},{\"end\":30589,\"start\":30588},{\"end\":30602,\"start\":30601},{\"end\":30612,\"start\":30611},{\"end\":30622,\"start\":30621},{\"end\":31330,\"start\":31329},{\"end\":31348,\"start\":31347},{\"end\":31350,\"start\":31349},{\"end\":31359,\"start\":31358},{\"end\":31361,\"start\":31360},{\"end\":31373,\"start\":31372},{\"end\":31375,\"start\":31374},{\"end\":31387,\"start\":31386},{\"end\":31400,\"start\":31399},{\"end\":31410,\"start\":31409},{\"end\":31412,\"start\":31411},{\"end\":31421,\"start\":31420},{\"end\":31423,\"start\":31422},{\"end\":31776,\"start\":31775},{\"end\":31786,\"start\":31785},{\"end\":31788,\"start\":31787},{\"end\":31797,\"start\":31796},{\"end\":31804,\"start\":31803},{\"end\":32100,\"start\":32099},{\"end\":32115,\"start\":32114},{\"end\":32124,\"start\":32123},{\"end\":32138,\"start\":32137},{\"end\":32153,\"start\":32152},{\"end\":32161,\"start\":32160},{\"end\":32176,\"start\":32175},{\"end\":32188,\"start\":32187},{\"end\":32200,\"start\":32199},{\"end\":32211,\"start\":32210},{\"end\":32220,\"start\":32219},{\"end\":32233,\"start\":32232},{\"end\":32585,\"start\":32584},{\"end\":32600,\"start\":32599},{\"end\":32609,\"start\":32608},{\"end\":32623,\"start\":32622},{\"end\":32638,\"start\":32637},{\"end\":32646,\"start\":32645},{\"end\":32661,\"start\":32660},{\"end\":32673,\"start\":32672},{\"end\":32685,\"start\":32684},{\"end\":32696,\"start\":32695},{\"end\":33183,\"start\":33182},{\"end\":33185,\"start\":33184},{\"end\":33195,\"start\":33194},{\"end\":33204,\"start\":33203},{\"end\":33206,\"start\":33205},{\"end\":33212,\"start\":33211},{\"end\":33214,\"start\":33213},{\"end\":33225,\"start\":33224},{\"end\":33240,\"start\":33239},{\"end\":33253,\"start\":33252},{\"end\":33275,\"start\":33274},{\"end\":33277,\"start\":33276},{\"end\":33285,\"start\":33284},{\"end\":33287,\"start\":33286},{\"end\":33296,\"start\":33295},{\"end\":33298,\"start\":33297},{\"end\":33730,\"start\":33729},{\"end\":33736,\"start\":33735},{\"end\":33744,\"start\":33743},{\"end\":33753,\"start\":33752},{\"end\":33762,\"start\":33761},{\"end\":33773,\"start\":33772},{\"end\":33780,\"start\":33779},{\"end\":33791,\"start\":33790},{\"end\":33798,\"start\":33797},{\"end\":34163,\"start\":34162},{\"end\":34543,\"start\":34542},{\"end\":34826,\"start\":34825},{\"end\":35235,\"start\":35234},{\"end\":35245,\"start\":35244},{\"end\":35257,\"start\":35256},{\"end\":35267,\"start\":35266},{\"end\":35271,\"start\":35268},{\"end\":35279,\"start\":35278},{\"end\":35281,\"start\":35280},{\"end\":35287,\"start\":35286},{\"end\":35658,\"start\":35657},{\"end\":35665,\"start\":35664},{\"end\":35673,\"start\":35672},{\"end\":35680,\"start\":35679},{\"end\":35682,\"start\":35681},{\"end\":35690,\"start\":35689},{\"end\":35700,\"start\":35699},{\"end\":35708,\"start\":35707},{\"end\":35714,\"start\":35713},{\"end\":35716,\"start\":35715},{\"end\":35724,\"start\":35723},{\"end\":35730,\"start\":35729},{\"end\":36116,\"start\":36115},{\"end\":36120,\"start\":36117},{\"end\":36131,\"start\":36130},{\"end\":36133,\"start\":36132},{\"end\":36144,\"start\":36143},{\"end\":36152,\"start\":36151},{\"end\":36159,\"start\":36158},{\"end\":36169,\"start\":36168},{\"end\":36177,\"start\":36176},{\"end\":36179,\"start\":36178},{\"end\":36191,\"start\":36190},{\"end\":36200,\"start\":36199},{\"end\":36213,\"start\":36212},{\"end\":36215,\"start\":36214},{\"end\":36223,\"start\":36222},{\"end\":36225,\"start\":36224},{\"end\":36569,\"start\":36568},{\"end\":36571,\"start\":36570},{\"end\":36582,\"start\":36581},{\"end\":36584,\"start\":36583},{\"end\":36595,\"start\":36594},{\"end\":36597,\"start\":36596},{\"end\":36610,\"start\":36609},{\"end\":36612,\"start\":36611},{\"end\":36625,\"start\":36624},{\"end\":36627,\"start\":36626},{\"end\":36638,\"start\":36637},{\"end\":36640,\"start\":36639},{\"end\":36648,\"start\":36647},{\"end\":36650,\"start\":36649},{\"end\":36658,\"start\":36657},{\"end\":37004,\"start\":37003},{\"end\":37015,\"start\":37014},{\"end\":37022,\"start\":37021},{\"end\":37031,\"start\":37030},{\"end\":37039,\"start\":37038},{\"end\":37429,\"start\":37428},{\"end\":37443,\"start\":37442},{\"end\":37784,\"start\":37783},{\"end\":37797,\"start\":37796},{\"end\":37807,\"start\":37806},{\"end\":37809,\"start\":37808},{\"end\":37820,\"start\":37819},{\"end\":37822,\"start\":37821},{\"end\":37833,\"start\":37832},{\"end\":37835,\"start\":37834},{\"end\":37847,\"start\":37846},{\"end\":37849,\"start\":37848},{\"end\":37859,\"start\":37858},{\"end\":37870,\"start\":37869},{\"end\":37872,\"start\":37871},{\"end\":37888,\"start\":37887},{\"end\":37895,\"start\":37894},{\"end\":37897,\"start\":37896},{\"end\":38414,\"start\":38413},{\"end\":38427,\"start\":38426},{\"end\":38440,\"start\":38439},{\"end\":38444,\"start\":38441},{\"end\":38788,\"start\":38787},{\"end\":38790,\"start\":38789},{\"end\":38800,\"start\":38799},{\"end\":38807,\"start\":38806},{\"end\":38809,\"start\":38808},{\"end\":38815,\"start\":38814},{\"end\":38817,\"start\":38816},{\"end\":38825,\"start\":38824},{\"end\":38827,\"start\":38826},{\"end\":38835,\"start\":38834},{\"end\":38837,\"start\":38836},{\"end\":38847,\"start\":38846},{\"end\":38849,\"start\":38848},{\"end\":38855,\"start\":38854},{\"end\":38857,\"start\":38856},{\"end\":38865,\"start\":38864},{\"end\":38867,\"start\":38866},{\"end\":38875,\"start\":38874},{\"end\":38877,\"start\":38876},{\"end\":39251,\"start\":39250},{\"end\":39253,\"start\":39252},{\"end\":39263,\"start\":39262},{\"end\":39265,\"start\":39264},{\"end\":39275,\"start\":39274},{\"end\":39277,\"start\":39276},{\"end\":39285,\"start\":39284},{\"end\":39292,\"start\":39291},{\"end\":39294,\"start\":39293},{\"end\":39300,\"start\":39299},{\"end\":39307,\"start\":39306},{\"end\":39309,\"start\":39308},{\"end\":39639,\"start\":39638},{\"end\":39641,\"start\":39640},{\"end\":39651,\"start\":39650},{\"end\":39659,\"start\":39658},{\"end\":39661,\"start\":39660},{\"end\":39671,\"start\":39670},{\"end\":39673,\"start\":39672},{\"end\":39683,\"start\":39682},{\"end\":39685,\"start\":39684},{\"end\":39694,\"start\":39693},{\"end\":39701,\"start\":39700},{\"end\":39703,\"start\":39702},{\"end\":40067,\"start\":40066},{\"end\":40069,\"start\":40068},{\"end\":40079,\"start\":40078},{\"end\":40087,\"start\":40086},{\"end\":40089,\"start\":40088},{\"end\":40097,\"start\":40096},{\"end\":40101,\"start\":40098},{\"end\":40111,\"start\":40110},{\"end\":40113,\"start\":40112},{\"end\":40336,\"start\":40335},{\"end\":40344,\"start\":40343},{\"end\":40350,\"start\":40349},{\"end\":40713,\"start\":40712},{\"end\":41026,\"start\":41025},{\"end\":41035,\"start\":41034},{\"end\":41042,\"start\":41041},{\"end\":41297,\"start\":41296},{\"end\":41645,\"start\":41644},{\"end\":41653,\"start\":41652},{\"end\":41655,\"start\":41654},{\"end\":41661,\"start\":41660},{\"end\":41663,\"start\":41662},{\"end\":41673,\"start\":41672},{\"end\":41675,\"start\":41674},{\"end\":41684,\"start\":41683},{\"end\":41686,\"start\":41685},{\"end\":41700,\"start\":41699},{\"end\":41702,\"start\":41701},{\"end\":41710,\"start\":41709},{\"end\":41720,\"start\":41719},{\"end\":41722,\"start\":41721},{\"end\":41733,\"start\":41732},{\"end\":41744,\"start\":41743},{\"end\":42277,\"start\":42276},{\"end\":42285,\"start\":42284},{\"end\":42293,\"start\":42292},{\"end\":42299,\"start\":42298},{\"end\":42305,\"start\":42304},{\"end\":42316,\"start\":42315},{\"end\":42318,\"start\":42317},{\"end\":42888,\"start\":42887},{\"end\":42896,\"start\":42895},{\"end\":42903,\"start\":42902}]", "bib_author_last_name": "[{\"end\":28872,\"start\":28863},{\"end\":28884,\"start\":28878},{\"end\":28892,\"start\":28888},{\"end\":28902,\"start\":28898},{\"end\":28909,\"start\":28906},{\"end\":28920,\"start\":28913},{\"end\":28933,\"start\":28924},{\"end\":29253,\"start\":29246},{\"end\":29264,\"start\":29257},{\"end\":29548,\"start\":29537},{\"end\":29824,\"start\":29818},{\"end\":29835,\"start\":29828},{\"end\":29848,\"start\":29841},{\"end\":29870,\"start\":29852},{\"end\":30175,\"start\":30170},{\"end\":30185,\"start\":30179},{\"end\":30195,\"start\":30189},{\"end\":30207,\"start\":30199},{\"end\":30514,\"start\":30509},{\"end\":30527,\"start\":30520},{\"end\":30537,\"start\":30531},{\"end\":30549,\"start\":30541},{\"end\":30561,\"start\":30553},{\"end\":30573,\"start\":30565},{\"end\":30586,\"start\":30579},{\"end\":30599,\"start\":30590},{\"end\":30609,\"start\":30603},{\"end\":30619,\"start\":30613},{\"end\":30631,\"start\":30623},{\"end\":31345,\"start\":31331},{\"end\":31356,\"start\":31351},{\"end\":31370,\"start\":31362},{\"end\":31384,\"start\":31376},{\"end\":31397,\"start\":31388},{\"end\":31407,\"start\":31401},{\"end\":31418,\"start\":31413},{\"end\":31432,\"start\":31424},{\"end\":31783,\"start\":31777},{\"end\":31794,\"start\":31789},{\"end\":31801,\"start\":31798},{\"end\":31814,\"start\":31805},{\"end\":32112,\"start\":32101},{\"end\":32121,\"start\":32116},{\"end\":32135,\"start\":32125},{\"end\":32150,\"start\":32139},{\"end\":32158,\"start\":32154},{\"end\":32173,\"start\":32162},{\"end\":32185,\"start\":32177},{\"end\":32197,\"start\":32189},{\"end\":32208,\"start\":32201},{\"end\":32217,\"start\":32212},{\"end\":32230,\"start\":32221},{\"end\":32241,\"start\":32234},{\"end\":32597,\"start\":32586},{\"end\":32606,\"start\":32601},{\"end\":32620,\"start\":32610},{\"end\":32635,\"start\":32624},{\"end\":32643,\"start\":32639},{\"end\":32658,\"start\":32647},{\"end\":32670,\"start\":32662},{\"end\":32682,\"start\":32674},{\"end\":32693,\"start\":32686},{\"end\":32702,\"start\":32697},{\"end\":33192,\"start\":33186},{\"end\":33201,\"start\":33196},{\"end\":33209,\"start\":33207},{\"end\":33222,\"start\":33215},{\"end\":33237,\"start\":33226},{\"end\":33250,\"start\":33241},{\"end\":33272,\"start\":33254},{\"end\":33282,\"start\":33278},{\"end\":33293,\"start\":33288},{\"end\":33304,\"start\":33299},{\"end\":33733,\"start\":33731},{\"end\":33741,\"start\":33737},{\"end\":33750,\"start\":33745},{\"end\":33759,\"start\":33754},{\"end\":33770,\"start\":33763},{\"end\":33777,\"start\":33774},{\"end\":33788,\"start\":33781},{\"end\":33795,\"start\":33792},{\"end\":33803,\"start\":33799},{\"end\":34166,\"start\":34164},{\"end\":34554,\"start\":34544},{\"end\":34832,\"start\":34827},{\"end\":35242,\"start\":35236},{\"end\":35254,\"start\":35246},{\"end\":35264,\"start\":35258},{\"end\":35276,\"start\":35272},{\"end\":35284,\"start\":35282},{\"end\":35293,\"start\":35288},{\"end\":35662,\"start\":35659},{\"end\":35670,\"start\":35666},{\"end\":35677,\"start\":35674},{\"end\":35687,\"start\":35683},{\"end\":35697,\"start\":35691},{\"end\":35705,\"start\":35701},{\"end\":35711,\"start\":35709},{\"end\":35721,\"start\":35717},{\"end\":35727,\"start\":35725},{\"end\":35737,\"start\":35731},{\"end\":36128,\"start\":36121},{\"end\":36141,\"start\":36134},{\"end\":36149,\"start\":36145},{\"end\":36156,\"start\":36153},{\"end\":36166,\"start\":36160},{\"end\":36174,\"start\":36170},{\"end\":36188,\"start\":36180},{\"end\":36197,\"start\":36192},{\"end\":36210,\"start\":36201},{\"end\":36220,\"start\":36216},{\"end\":36230,\"start\":36226},{\"end\":36579,\"start\":36572},{\"end\":36592,\"start\":36585},{\"end\":36607,\"start\":36598},{\"end\":36622,\"start\":36613},{\"end\":36635,\"start\":36628},{\"end\":36645,\"start\":36641},{\"end\":36655,\"start\":36651},{\"end\":36664,\"start\":36659},{\"end\":37012,\"start\":37005},{\"end\":37019,\"start\":37016},{\"end\":37028,\"start\":37023},{\"end\":37036,\"start\":37032},{\"end\":37046,\"start\":37040},{\"end\":37440,\"start\":37430},{\"end\":37450,\"start\":37444},{\"end\":37794,\"start\":37785},{\"end\":37804,\"start\":37798},{\"end\":37817,\"start\":37810},{\"end\":37830,\"start\":37823},{\"end\":37844,\"start\":37836},{\"end\":37856,\"start\":37850},{\"end\":37867,\"start\":37860},{\"end\":37885,\"start\":37873},{\"end\":37892,\"start\":37889},{\"end\":37907,\"start\":37898},{\"end\":38424,\"start\":38415},{\"end\":38437,\"start\":38428},{\"end\":38450,\"start\":38445},{\"end\":38797,\"start\":38791},{\"end\":38804,\"start\":38801},{\"end\":38812,\"start\":38810},{\"end\":38822,\"start\":38818},{\"end\":38832,\"start\":38828},{\"end\":38844,\"start\":38838},{\"end\":38852,\"start\":38850},{\"end\":38862,\"start\":38858},{\"end\":38872,\"start\":38868},{\"end\":38882,\"start\":38878},{\"end\":39260,\"start\":39254},{\"end\":39272,\"start\":39266},{\"end\":39282,\"start\":39278},{\"end\":39289,\"start\":39286},{\"end\":39297,\"start\":39295},{\"end\":39304,\"start\":39301},{\"end\":39312,\"start\":39310},{\"end\":39648,\"start\":39642},{\"end\":39656,\"start\":39652},{\"end\":39668,\"start\":39662},{\"end\":39680,\"start\":39674},{\"end\":39691,\"start\":39686},{\"end\":39698,\"start\":39695},{\"end\":39706,\"start\":39704},{\"end\":40076,\"start\":40070},{\"end\":40084,\"start\":40080},{\"end\":40094,\"start\":40090},{\"end\":40108,\"start\":40102},{\"end\":40120,\"start\":40114},{\"end\":40341,\"start\":40337},{\"end\":40347,\"start\":40345},{\"end\":40358,\"start\":40351},{\"end\":40721,\"start\":40714},{\"end\":41032,\"start\":41027},{\"end\":41039,\"start\":41036},{\"end\":41050,\"start\":41043},{\"end\":41305,\"start\":41298},{\"end\":41650,\"start\":41646},{\"end\":41658,\"start\":41656},{\"end\":41670,\"start\":41664},{\"end\":41681,\"start\":41676},{\"end\":41697,\"start\":41687},{\"end\":41707,\"start\":41703},{\"end\":41717,\"start\":41711},{\"end\":41730,\"start\":41723},{\"end\":41741,\"start\":41734},{\"end\":41763,\"start\":41745},{\"end\":42282,\"start\":42278},{\"end\":42290,\"start\":42286},{\"end\":42296,\"start\":42294},{\"end\":42302,\"start\":42300},{\"end\":42313,\"start\":42306},{\"end\":42326,\"start\":42319},{\"end\":42893,\"start\":42889},{\"end\":42900,\"start\":42897},{\"end\":42908,\"start\":42904}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1904.03323\",\"id\":\"b0\"},\"end\":29181,\"start\":28861},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52243686},\"end\":29453,\"start\":29183},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":205228801},\"end\":29734,\"start\":29455},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":58981612},\"end\":30088,\"start\":29736},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":211066570},\"end\":30440,\"start\":30090},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":240353861},\"end\":31246,\"start\":30442},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":16941525},\"end\":31773,\"start\":31248},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b7\"},\"end\":32097,\"start\":31775},{\"attributes\":{\"id\":\"b8\"},\"end\":32582,\"start\":32099},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b9\"},\"end\":33074,\"start\":32584},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":208278918},\"end\":33640,\"start\":33076},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":220919723},\"end\":34114,\"start\":33642},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":34472,\"start\":34116},{\"attributes\":{\"id\":\"b13\"},\"end\":34733,\"start\":34474},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":58981871},\"end\":35148,\"start\":34735},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":24215976},\"end\":35564,\"start\":35150},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":231879586},\"end\":36058,\"start\":35566},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":33285731},\"end\":36466,\"start\":36060},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":209342303},\"end\":36936,\"start\":36468},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":215541815},\"end\":37426,\"start\":36938},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b20\"},\"end\":37626,\"start\":37428},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":208611383},\"end\":38267,\"start\":37628},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":236541033},\"end\":38710,\"start\":38269},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":229923729},\"end\":39140,\"start\":38712},{\"attributes\":{\"id\":\"b24\"},\"end\":39529,\"start\":39142},{\"attributes\":{\"doi\":\"ArXiv abs/2106.12930\",\"id\":\"b25\"},\"end\":39946,\"start\":39531},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":247595359},\"end\":40333,\"start\":39948},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b27\"},\"end\":40583,\"start\":40335},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":234681365},\"end\":40925,\"start\":40585},{\"attributes\":{\"id\":\"b29\"},\"end\":41223,\"start\":40927},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":231591445},\"end\":41525,\"start\":41225},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":86605240},\"end\":42130,\"start\":41527},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":8945673},\"end\":42780,\"start\":42132},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":252369280},\"end\":43185,\"start\":42782},{\"attributes\":{\"doi\":\"64 75.34\u00b10.20 69.91\u00b11.72 78.94\u00b10.68 66.13\u00b11.31 69.91\u00b11.36 77.07\u00b11.49 72.88\u00b10.19\",\"id\":\"b34\"},\"end\":43368,\"start\":43187},{\"attributes\":{\"doi\":\"128 75.59\u00b10.59 70.62\u00b1 0.54 80.07\u00b10.58 67.23\u00b10.32 70.71\u00b11.88 77.98\u00b11.45 73.70\u00b10.41\",\"id\":\"b35\"},\"end\":43555,\"start\":43370}]", "bib_title": "[{\"end\":29242,\"start\":29183},{\"end\":29533,\"start\":29455},{\"end\":29814,\"start\":29736},{\"end\":30164,\"start\":30090},{\"end\":30503,\"start\":30442},{\"end\":31327,\"start\":31248},{\"end\":33180,\"start\":33076},{\"end\":33727,\"start\":33642},{\"end\":34160,\"start\":34116},{\"end\":34823,\"start\":34735},{\"end\":35232,\"start\":35150},{\"end\":35655,\"start\":35566},{\"end\":36113,\"start\":36060},{\"end\":36566,\"start\":36468},{\"end\":37001,\"start\":36938},{\"end\":37781,\"start\":37628},{\"end\":38411,\"start\":38269},{\"end\":38785,\"start\":38712},{\"end\":39248,\"start\":39142},{\"end\":40064,\"start\":39948},{\"end\":40710,\"start\":40585},{\"end\":41294,\"start\":41225},{\"end\":41642,\"start\":41527},{\"end\":42274,\"start\":42132},{\"end\":42885,\"start\":42782}]", "bib_author": "[{\"end\":28874,\"start\":28861},{\"end\":28886,\"start\":28874},{\"end\":28894,\"start\":28886},{\"end\":28904,\"start\":28894},{\"end\":28911,\"start\":28904},{\"end\":28922,\"start\":28911},{\"end\":28935,\"start\":28922},{\"end\":29255,\"start\":29244},{\"end\":29266,\"start\":29255},{\"end\":29550,\"start\":29535},{\"end\":29826,\"start\":29816},{\"end\":29837,\"start\":29826},{\"end\":29850,\"start\":29837},{\"end\":29872,\"start\":29850},{\"end\":30177,\"start\":30166},{\"end\":30187,\"start\":30177},{\"end\":30197,\"start\":30187},{\"end\":30209,\"start\":30197},{\"end\":30516,\"start\":30505},{\"end\":30529,\"start\":30516},{\"end\":30539,\"start\":30529},{\"end\":30551,\"start\":30539},{\"end\":30563,\"start\":30551},{\"end\":30575,\"start\":30563},{\"end\":30588,\"start\":30575},{\"end\":30601,\"start\":30588},{\"end\":30611,\"start\":30601},{\"end\":30621,\"start\":30611},{\"end\":30633,\"start\":30621},{\"end\":31347,\"start\":31329},{\"end\":31358,\"start\":31347},{\"end\":31372,\"start\":31358},{\"end\":31386,\"start\":31372},{\"end\":31399,\"start\":31386},{\"end\":31409,\"start\":31399},{\"end\":31420,\"start\":31409},{\"end\":31434,\"start\":31420},{\"end\":31785,\"start\":31775},{\"end\":31796,\"start\":31785},{\"end\":31803,\"start\":31796},{\"end\":31816,\"start\":31803},{\"end\":32114,\"start\":32099},{\"end\":32123,\"start\":32114},{\"end\":32137,\"start\":32123},{\"end\":32152,\"start\":32137},{\"end\":32160,\"start\":32152},{\"end\":32175,\"start\":32160},{\"end\":32187,\"start\":32175},{\"end\":32199,\"start\":32187},{\"end\":32210,\"start\":32199},{\"end\":32219,\"start\":32210},{\"end\":32232,\"start\":32219},{\"end\":32243,\"start\":32232},{\"end\":32599,\"start\":32584},{\"end\":32608,\"start\":32599},{\"end\":32622,\"start\":32608},{\"end\":32637,\"start\":32622},{\"end\":32645,\"start\":32637},{\"end\":32660,\"start\":32645},{\"end\":32672,\"start\":32660},{\"end\":32684,\"start\":32672},{\"end\":32695,\"start\":32684},{\"end\":32704,\"start\":32695},{\"end\":33194,\"start\":33182},{\"end\":33203,\"start\":33194},{\"end\":33211,\"start\":33203},{\"end\":33224,\"start\":33211},{\"end\":33239,\"start\":33224},{\"end\":33252,\"start\":33239},{\"end\":33274,\"start\":33252},{\"end\":33284,\"start\":33274},{\"end\":33295,\"start\":33284},{\"end\":33306,\"start\":33295},{\"end\":33735,\"start\":33729},{\"end\":33743,\"start\":33735},{\"end\":33752,\"start\":33743},{\"end\":33761,\"start\":33752},{\"end\":33772,\"start\":33761},{\"end\":33779,\"start\":33772},{\"end\":33790,\"start\":33779},{\"end\":33797,\"start\":33790},{\"end\":33805,\"start\":33797},{\"end\":34168,\"start\":34162},{\"end\":34556,\"start\":34542},{\"end\":34834,\"start\":34825},{\"end\":35244,\"start\":35234},{\"end\":35256,\"start\":35244},{\"end\":35266,\"start\":35256},{\"end\":35278,\"start\":35266},{\"end\":35286,\"start\":35278},{\"end\":35295,\"start\":35286},{\"end\":35664,\"start\":35657},{\"end\":35672,\"start\":35664},{\"end\":35679,\"start\":35672},{\"end\":35689,\"start\":35679},{\"end\":35699,\"start\":35689},{\"end\":35707,\"start\":35699},{\"end\":35713,\"start\":35707},{\"end\":35723,\"start\":35713},{\"end\":35729,\"start\":35723},{\"end\":35739,\"start\":35729},{\"end\":36130,\"start\":36115},{\"end\":36143,\"start\":36130},{\"end\":36151,\"start\":36143},{\"end\":36158,\"start\":36151},{\"end\":36168,\"start\":36158},{\"end\":36176,\"start\":36168},{\"end\":36190,\"start\":36176},{\"end\":36199,\"start\":36190},{\"end\":36212,\"start\":36199},{\"end\":36222,\"start\":36212},{\"end\":36232,\"start\":36222},{\"end\":36581,\"start\":36568},{\"end\":36594,\"start\":36581},{\"end\":36609,\"start\":36594},{\"end\":36624,\"start\":36609},{\"end\":36637,\"start\":36624},{\"end\":36647,\"start\":36637},{\"end\":36657,\"start\":36647},{\"end\":36666,\"start\":36657},{\"end\":37014,\"start\":37003},{\"end\":37021,\"start\":37014},{\"end\":37030,\"start\":37021},{\"end\":37038,\"start\":37030},{\"end\":37048,\"start\":37038},{\"end\":37442,\"start\":37428},{\"end\":37452,\"start\":37442},{\"end\":37796,\"start\":37783},{\"end\":37806,\"start\":37796},{\"end\":37819,\"start\":37806},{\"end\":37832,\"start\":37819},{\"end\":37846,\"start\":37832},{\"end\":37858,\"start\":37846},{\"end\":37869,\"start\":37858},{\"end\":37887,\"start\":37869},{\"end\":37894,\"start\":37887},{\"end\":37909,\"start\":37894},{\"end\":38426,\"start\":38413},{\"end\":38439,\"start\":38426},{\"end\":38452,\"start\":38439},{\"end\":38799,\"start\":38787},{\"end\":38806,\"start\":38799},{\"end\":38814,\"start\":38806},{\"end\":38824,\"start\":38814},{\"end\":38834,\"start\":38824},{\"end\":38846,\"start\":38834},{\"end\":38854,\"start\":38846},{\"end\":38864,\"start\":38854},{\"end\":38874,\"start\":38864},{\"end\":38884,\"start\":38874},{\"end\":39262,\"start\":39250},{\"end\":39274,\"start\":39262},{\"end\":39284,\"start\":39274},{\"end\":39291,\"start\":39284},{\"end\":39299,\"start\":39291},{\"end\":39306,\"start\":39299},{\"end\":39314,\"start\":39306},{\"end\":39650,\"start\":39638},{\"end\":39658,\"start\":39650},{\"end\":39670,\"start\":39658},{\"end\":39682,\"start\":39670},{\"end\":39693,\"start\":39682},{\"end\":39700,\"start\":39693},{\"end\":39708,\"start\":39700},{\"end\":40078,\"start\":40066},{\"end\":40086,\"start\":40078},{\"end\":40096,\"start\":40086},{\"end\":40110,\"start\":40096},{\"end\":40122,\"start\":40110},{\"end\":40343,\"start\":40335},{\"end\":40349,\"start\":40343},{\"end\":40360,\"start\":40349},{\"end\":40723,\"start\":40712},{\"end\":41034,\"start\":41025},{\"end\":41041,\"start\":41034},{\"end\":41052,\"start\":41041},{\"end\":41307,\"start\":41296},{\"end\":41652,\"start\":41644},{\"end\":41660,\"start\":41652},{\"end\":41672,\"start\":41660},{\"end\":41683,\"start\":41672},{\"end\":41699,\"start\":41683},{\"end\":41709,\"start\":41699},{\"end\":41719,\"start\":41709},{\"end\":41732,\"start\":41719},{\"end\":41743,\"start\":41732},{\"end\":41765,\"start\":41743},{\"end\":42284,\"start\":42276},{\"end\":42292,\"start\":42284},{\"end\":42298,\"start\":42292},{\"end\":42304,\"start\":42298},{\"end\":42315,\"start\":42304},{\"end\":42328,\"start\":42315},{\"end\":42895,\"start\":42887},{\"end\":42902,\"start\":42895},{\"end\":42910,\"start\":42902}]", "bib_venue": "[{\"end\":30859,\"start\":30762},{\"end\":34309,\"start\":34247},{\"end\":34943,\"start\":34897},{\"end\":37197,\"start\":37131},{\"end\":42469,\"start\":42407},{\"end\":28994,\"start\":28951},{\"end\":29305,\"start\":29266},{\"end\":29572,\"start\":29550},{\"end\":29894,\"start\":29872},{\"end\":30243,\"start\":30209},{\"end\":30760,\"start\":30633},{\"end\":31489,\"start\":31434},{\"end\":31912,\"start\":31832},{\"end\":32323,\"start\":32243},{\"end\":32794,\"start\":32720},{\"end\":33332,\"start\":33306},{\"end\":33858,\"start\":33805},{\"end\":34245,\"start\":34168},{\"end\":34540,\"start\":34474},{\"end\":34895,\"start\":34834},{\"end\":35339,\"start\":35295},{\"end\":35783,\"start\":35739},{\"end\":36247,\"start\":36232},{\"end\":36681,\"start\":36666},{\"end\":37129,\"start\":37048},{\"end\":37505,\"start\":37468},{\"end\":37918,\"start\":37909},{\"end\":38473,\"start\":38452},{\"end\":38899,\"start\":38884},{\"end\":39323,\"start\":39314},{\"end\":39636,\"start\":39531},{\"end\":40131,\"start\":40122},{\"end\":40434,\"start\":40376},{\"end\":40744,\"start\":40723},{\"end\":41023,\"start\":40927},{\"end\":41351,\"start\":41307},{\"end\":41799,\"start\":41765},{\"end\":42405,\"start\":42328},{\"end\":42968,\"start\":42910},{\"end\":43277,\"start\":43268},{\"end\":43462,\"start\":43453}]"}}}, "year": 2023, "month": 12, "day": 17}