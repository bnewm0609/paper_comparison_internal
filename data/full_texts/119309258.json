{"id": 119309258, "updated": "2023-10-01 18:26:50.554", "metadata": {"title": "YouTube UGC Dataset for Video Compression Research", "authors": "[{\"first\":\"Yilin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Sasi\",\"last\":\"Inguva\",\"middle\":[]},{\"first\":\"Balu\",\"last\":\"Adsumilli\",\"middle\":[]}]", "venue": "2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)", "journal": null, "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Non-professional video, commonly known as User Generated Content (UGC) has become very popular in today's video sharing applications. However, traditional metrics used in compression and quality assessment, like BD-Rate and PSNR, are designed for pristine originals. Thus, their accuracy drops significantly when being applied on non-pristine originals (the majority of UGC). Understanding difficulties for compression and quality assessment in the scenario of UGC is important, but there are few public UGC datasets available for research. This paper introduces a large scale UGC dataset (1500 20 sec video clips) sampled from millions of YouTube videos. The dataset covers popular categories like Gaming, Sports, and new features like High Dynamic Range (HDR). Besides a novel sampling method based on features extracted from encoding, challenges for UGC compression and quality evaluation are also discussed. Shortcomings of traditional reference-based metrics on UGC are addressed. We demonstrate a promising way to evaluate UGC quality by no-reference objective quality metrics, and evaluate the current dataset with three no-reference metrics (Noise, Banding, and SLEEQ).", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "1904.06457", "mag": "3103745061", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mmsp/WangIA19", "doi": "10.1109/mmsp.2019.8901772"}}, "content": {"source": {"pdf_hash": "152df6278a0e2a145dcbd249708b5aa5c82d5713", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/1904.06457v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1904.06457", "status": "GREEN"}}, "grobid": {"id": "bb957f2d93be77b662de5901e17bcbf0d9dd1417", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/152df6278a0e2a145dcbd249708b5aa5c82d5713.txt", "contents": "\nYouTube UGC Dataset for Video Compression Research\n\n\nYilin Wang \nGoogle Inc. Mountain View\nCAUSA\n\nSasi Inguva \nGoogle Inc. Mountain View\nCAUSA\n\nBalu Adsumilli badsumilli@google.com \nGoogle Inc. Mountain View\nCAUSA\n\nYouTube UGC Dataset for Video Compression Research\nIndex Terms-User Generated ContentVideo CompressionVideo Quality Assessment\nNon-professional video, commonly known as User Generated Content (UGC) has become very popular in todays video sharing applications. However, traditional metrics used in compression and quality assessment, like BD-Rate and PSNR, are designed for pristine originals. Thus, their accuracy drops significantly when being applied on non-pristine originals (the majority of UGC). Understanding difficulties for compression and quality assessment in the scenario of UGC is important, but there are few public UGC datasets available for research. This paper introduces a large scale UGC dataset (1500 20 sec video clips) sampled from millions of YouTube videos. The dataset covers popular categories like Gaming, Sports, and new features like High Dynamic Range (HDR). Besides a novel sampling method based on features extracted from encoding, challenges for UGC compression and quality evaluation are also discussed. Shortcomings of traditional reference-based metrics on UGC are addressed. We demonstrate a promising way to evaluate UGC quality by no-reference objective quality metrics, and evaluate the current dataset with three no-reference metrics (Noise, Banding, and SLEEQ).\n\nI. INTRODUCTION\n\nVideo makes up the majority of todays Internet traffic. Consequently, this motivates video service companies (e.g. YouTube) to spend substantial effort to control bandwidth usage [1]. The main remedy deployed is typically video bitrate reduction. However, aggressive bitrate reduction may hurt perceptual visual quality at the same time as both creators and viewers have increasing expectations for streaming media quality. The evolution of new codec technology (HEVC, VP9, AV1) continues to address this bitrate/quality tradeoff.\n\nTo measure the quality degradation, numerous quality metrics have been proposed in the last few decades. Some reference-based metrics (like PSNR, SSIM [2] and VMAF [3]) have been widely used in the industry. A common assumption held by most video quality and compression research is that the original video is perfect, and any operation on the original (processing, compression etc.) makes it worse. Most research measures how good the resulting video is by comparing it to the original. However, such an assumption does not hold for most of User Generated Content (UGC) due to the following reasons:\n\n\u2022 Non-pristine original When there are visual artifacts present in the original, it is not clear whether an encoder should be putting in efforts to accurately represent those artifacts. It is necessary to consider the effect that the encoding has on those undesired artifacts, but it is also necessary to consider the effect that the artifacts have on the ability to encode the video effectively. \u2022 Mismatched absolute and reference quality Using the original as a reference does not always make sense when the original isnt perfect. Quality improvement may be affected by pre/post processing before/after transcoding, but reference-based metrics (e.g. PSNR, SSIM) cannot fairly evaluate the impact of these tools in a compression chain. We created this large scale UGC dataset in order to encourage and facilitate research that considers the practical and realistic needs of video compression and quality assessment in video processing infrastructure.\n\nA major contribution of this work is the analysis of the enormous content in YouTube in a way that illustrates the breadth of visual quality in media worldwide. That analysis leads to the creation of a statistically representative set that is more amenable to academic research and computational resources. We built a large scale UGC dataset(Section III), and propose (Section IV) a novel sampling scheme based on features extracted from encoding logs, which achieves high coverage over millions of YouTube videos. Shortcomings of traditional reference-based metrics on UGC are discussed (Section V), and we evaluate the dataset with three noreference metrics: Noise [4], Banding [5], and Self-reference based LEarning-free Evaluator of Quality (SLEEQ) [6].\n\nThe dataset can be previewed and downloaded from https://media.withyoutube.com/ugc-dataset.\n\n\nII. RELATED WORK\n\nSome large-scale datasets have already been released for UGC videos, like YouTube-8M [7] and AVA [8]. However, they only provide extracted features instead of raw pixel data, which makes them less useful for compression research.\n\nXiph.org Video Test Media [9] is a popular dataset for video compression and it contains around 120 individual video clips (including both pristine and UGC samples). These videos have various resolutions (e.g. SD, HD, and 4K) and multiple content categories (e.g. movie and gaming).\n\nLIVE datasets [10]- [12] are also quite popular. All of them contain less than 30 individual pristine clips, along with about 978-1-7281-1817-8/19/$31.00 \u00a9 2019 IEEE 150 distorted versions. The goal of the LIVE datasets is subjective quality assessment. Each video clip in the dataset was assessed by 35 to 55 human subjects.\n\nVideoSet [13] contains 220 5 sec clips extracted from 11 videos. The target here is also quality assessment, but instead of providing Mean Opinion Score (MOS) like the LIVE datasets, it provides the first three Just-Noticeable-Difference (JND) scores collected from more than 30 subjects.\n\nCrowdsourced Video Quality Dataset [14] contains 585 10 sec video clips, captured by 80 inexpert videographers. The dataset has 18 different resolutions and a wide range of quality owing to the intrinsic nature of real-world distortions. Subjective opinions were collected from thousands of participants using Amazon Mechanical Turk.\n\nKoNViD-1k [15] is another large scale dataset which contains 1200 clips with corresponding subjective scores. All videos are in landscape layout and have resolution higher than 960 \u00d7 540. They started from a collection of 150K videos, grouping them by multiple attributes like blur, colorfulness etc. The final set was created by a \"fair-sampling\" strategy. The subjective mean opinion scores were gathered through crowdsourcing.\n\n\nIII. YOUTUBE UGC DATASET OVERVIEW\n\nOur dataset is sampled from YouTube videos with the Creative Commons license. We selected an initial set of 1.5 millions videos belonging to 15 categories annotated by Knowledge Graph [16] (as shown in Fig. 1): Animation, Cover Song, Gaming, HDR, How To, Lecture, Live Music, Lyric Video, Music Video, News Clip, Sports, Television Clip, Vertical Video, Vlog, and VR. The video category is an important feature of our dataset, which allows users to easily explore characteristics of different kinds of videos. For example, Gaming videos usually contain lots of fast motion, while many Lyric videos have still backgrounds. Compression algorithms can be optimized in different ways based on such category information.\n\nVideos within each category are further divided into subgroups based on their resolutions. Resolution is an important feature revealing the diversity of user preferences, as well as the different behaviors of various devices and platforms. So it would be helpful to treat resolution as an independent dimension. In our dataset, we provided 360P, 480P, 720P, 1080P for all categories (except for HDR and VR) and 4K for HDR, Gaming, Sports, Vertical Video, Vlog, and VR.\n\nThe final dataset contains 1500 video clips, each of 20 sec duration. All clips are in Raw YUV 4:2:0 format with constant framerate. Details of the sampling scheme are discussed in the next section.\n\n\nIV. DATASET SAMPLING\n\nSelecting representative samples from millions of videos is challenging. Not only is the sheer number of videos a challenge to process and generate features, but also the long duration of some videos (which can be in hours) makes it that much more difficult. Compared with another large Large scale video compression/transcoding pipelines typically divide long videos into chunks and encode them in parallel. Maintaining the quality consistency among chunk boundaries becomes an issue in practice. Thus, besides the three common attributes (spatial, temporal, and color) suggested in [17], we propose the variation of complexity across the video as the fourth attribute that reflects inter-chunk quality consistency. We made the length of video clips in our dataset 20 sec, which is long enough to involve multiple complexity variations. These 20 sec clips could be taken from any segment of a video. For the 5 million hours of videos therefore, there were 1.8 billion putative 20 sec clips.\n\nWe used Google's Borg system [18] to encode each video in the initial collection with FFmpeg H.264 encoder with PSNR stats enabled. The detailed compression settings we used are constant QP 20, fixed GOP size 14 frames with no B frames. Other reasonable settings will also work and bring similar features. The encoder reports on statistics from processing on a per frame basis. That diagnostic output was used to collect the following features over 20 sec clip stepped by 1 sec throughout each video:\n\n\u2022 Spatial In general spatial detail in a frame is correlated with the bits used to encode that frame, when encoded as an Intra frame. Over a 20 sec chunk therefore, we calculate our spatial complexity feature as the average I frame bitrate normalized by the frame area. Fig. 2 (a) and Fig. 2 (b) are frames for low and high spatial complexity respectively. \u2022 Color We define color complexity metric as the ratio between the average of mean Sum of Squared Error (SSE) in U and V channels to the mean SSE in Y channel (obtained from PSNR measurements). A high score means complex color variations in the frame (Fig. 2 (d)), and a low score usually means a gray image with only luminance changes (Fig. 2 (c)). \u2022 Temporal The number of bits used to encode a P frame is proportional to the temporal complexity of a sequence. However visual material with high spatial complexity (large I frames) tends to have large P frames because small motion compensation errors lead to large residuals.\n\nTo decouple this effect, we normalize the P frame bits by taking a ratio with the I frame bits, as a fair indicator of temporal complexity. Fig. 3.(a) and Fig. 3.(b) show row sum map of videos with low and high temporal complexity, where the ith column is the row sum of the ith frame. Frequent column color changes in Fig. 3.(b) imply fast motion among frames, while small changes in temporal domain leads to homogeneous regions in Fig. 3.(a). \u2022 Chunk Variation To explore quality variation within the video, we measured the standard deviation of compressed bitrates among all 1 sec chunks (also normalized by the frame size). If the scene is static or has a smooth motion, the chunk variation score should be close to 0, and the row sum map has no sudden changes (Fig. 3 (c)). Multiple scene changes within a video (common in Gaming videos) will lead to multiple different regions on the row sum map (Fig. 3 (d)). Each original video and time offset combination forms a candidate clip for the dataset. We sample from this sparse 4-D feature space as described below:\n\n1) Normalize the feature space F by subtracting min(F ) and dividing with (percentile(F, 99) \u2212 min(F )). 2) For each feature, divide the normalized range [0, 1] uniformly into N (= 3) bins, where the last bin can go \u2022 The Euclidean distance of feature scores, between this clip and each of the already selected clips is greater than a certain threshold (0.3). \u2022 The original video that this clip belongs to, doesn't already have another clip in the dataset. 5) Move to the next bin and repeat step 4 until desired number of samples are added. We manually remove mislabeled clips from the 50 selected clips, leaving with 15 to 25 selected clips from each (category, resolution) subgroup. We design the sampling scheme to comprehensively sample along all four feature spaces. As the complexity distribution (normalized) shown in Fig. 4, the feature scores in our sampled set are less spiky than that of the initial set. The sample distribution in color space is left skewed, because many videos with high complexity in other feature spaces are uncolorful. To evaluate the coverage of the sampled set, we divide each pairwise space into 10 \u00d7 10 grid. The percentage of grids covered by the sampled set is shown in Table I, where the average coverage rate is 89%. As mentioned in Section I, most UGC videos are nonpristine, which may confuse traditional reference-based quality metrics. Fig. 5 and Fig. 6 show two clips (id: Vlog 720P-343d and Vlog 720P-670d) from our UGC dataset, as well as their compressed versions (by H.264 with CRF 32). We can see that the corresponding PSNR, SSIM, and VMAF scores are bad, however the original and compressed versions are similar The low reference quality scores are mainly caused by not correctly accounting for artifacts in originals.\n\nAlthough no existing quality metric can perfectly evaluate quality degradation between the original and compressed versions, a possible way is to identify quality issues separately. For example, since we can tell the major quality issue for videos in Fig. 5 is distortions in natural scene, we can compute SLEEQ [6] scores on the original and compressed versions independently, and use their differences to evaluate the quality degradation. In this case, SLEEQ scores for the original and compressed versions are 0.21 and 0.18, respectively, which implies that compression didn't introduce noticeable quality degradation. We can get the same conclusion for videos in Fig. 6 by applying a noise detector [4].\n\nWe analyze perceptual quality of our UGC dataset based on existing no-reference metrics. Current evaluation includes three no-reference metrics: Noise, Banding [5], and SLEEQ. The first two metrics are artifact-oriented, which can be interpreted as meaningful extents of specific compression issues. The third metric (SLEEQ) is designed to measure compression artifacts on natural scenes. All these metrics have good correlations with human ratings, and outperform other related no-reference metrics. Fig. 7 shows the distribution for the three quality metric scores (normalized to [0, 1] where lower score is better) on our UGC dataset. We can see that heavy artifacts are not detected in most videos, which in some sense tells us that the overall quality of videos uploaded to YouTube is good. Fig. 8 shows some quality issues within individual categories. For example, Animation videos seem to have more banding artifacts than others, and nature scenes in Vlog tend to contain more artifacts than other categories.\n\n\nVI. CONCLUSION\n\nThis paper introduced a large scale dataset for UGC videos, which highly represents the videos uploaded to YouTube. A novel sampling scheme was proposed to extract features from millions of video samples, and we investigated complexity distribution as well as quality distribution for videos across 15 categories. The difficulties of UGC quality assessment were cited. An important open question remains regarding how to evaluate quality degradation caused by compression for UGC (non-pristine reference). We hope this dataset can inspire and facilitate research on practical requirements of video compression and quality assessment.\n\nFig. 2 .Fig. 3 .\n23Various complexity in spatial and color spaces. Various complexity in temporal and chunk variation spaces, where the ith column is the row sum of the ith frame.\n\nFig. 4 .\n4Complexity distributions (normalized) for initial and sampled datasets. over 1 and extend up to the maximum value. 3) Permutate all non-empty bins uniformly at random. 4) For current bin, randomly select one candidate clip and add it to the dataset if and only if:\n\nFig. 6 .\n6Misleading reference scores caused by noise artifacts, where PSNR, SSIM, and VMAF are 30.28, 0.63, and 31.43.visually. Both the original and compressed versions inFig. 5contain significant artifacts, while the compressed version inFig. 6actually has less noise artifacts than the original one.\n\nFig. 7 .Fig. 8 .\n78Distribution of no-reference quality metrics. Quality comparison for various categories.\n\n\nFig. 1. All categories in the YouTube UGC dataset.scale dataset, Konvid-1k, this set has videos sampled from a collection that is 10 times larger (1.5M vs. 150K). A video in our dataset can be sampled at any time offset from its original content, instead of taking only the first 30 sec of the original like Konvid-1k. This makes the search space for our datasetAnimation \nGaming \n\nVlog \n\nLecture \n\nCover Song \nHow To \n\nNews Clip \nLyric Video \nMusic Video \n\nTelevision Clip \n\nLive Music \n\nVertical Video \nVR \n\nHDR \n\nSports \n\n200 times bigger (an average video being 600s long). Due to \nthis huge search space, computing third party metrics (like \nBlur metric used in Konvid-1k) is resource heavy or even \ninfeasible. \n\n\nTABLE I COVERAGE\nIRATES FOR PAIRWISE FEATURE SPACES.Temporal \nColor Chunk Variation \nSpatial \n93% \n90% \n88% \nTemporal \n92% \n88% \nColor \n83% \n\nV. CHALLENGES ON UGC QUALITY ASSESSMENT \n\n\n\nOptimized transcoding for large scale adaptive streaming using playback statistics. Chao Chen, Yao-Chung Lin, Steve Benting, Anil Kokaram, 2018 IEEE International Conference on Image Processing. Chao Chen, Yao-Chung Lin, Steve Benting, and Anil Kokaram, \"Op- timized transcoding for large scale adaptive streaming using playback statistics,\" in 2018 IEEE International Conference on Image Processing, 2018.\n\nImage quality assessment: From error visibility to structural similarity. Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, IEEE Transactions on Image Processing. 134Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli, \"Image quality assessment: From error visibility to structural similarity,\" IEEE Transactions on Image Processing, vol. 13, no. 4, 2004.\n\nToward a practical perceptual video quality metric. Zhi Li, Anne Aaron, Ioannis Katsavounidis, Anush Moorthy, Megha Manohara, Blog, Netflix Technology. Zhi Li, Anne Aaron, Ioannis Katsavounidis, Anush Moorthy, and Megha Manohara, \"Toward a practical perceptual video quality metric,\" Blog, Netflix Technology, 2016.\n\nA no-reference perceptual quality metric for videos distorted by spatially correlated noise. Chao Chen, Mohammad Izadi, Anil Kokaram, ACM MultimediaChao Chen, Mohammad Izadi, , and Anil Kokaram, \"A no-reference perceptual quality metric for videos distorted by spatially correlated noise,\" ACM Multimedia, 2016.\n\nA perceptual visibility metric for banding artifacts. Yilin Wang, Sang-Uok Kum, Chao Chen, Anil Kokaram, IEEE International Conference on Image Processing. Yilin Wang, Sang-Uok Kum, Chao Chen, and Anil Kokaram, \"A perceptual visibility metric for banding artifacts,\" IEEE International Conference on Image Processing, 2016.\n\nA noreference video quality predictor for compression and scaling artifacts. Deepti Ghadiyaram, Chao Chen, Sasi Inguva, Anil Kokaram, IEEE International Conference on Image Processing. Deepti Ghadiyaram, Chao Chen, Sasi Inguva, and Anil Kokaram, \"A no- reference video quality predictor for compression and scaling artifacts,\" IEEE International Conference on Image Processing, 2017.\n\nYoutube-8m: A large-scale video classification benchmark. Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan, arXiv:1609.08675arXiv preprintSami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan, \"Youtube-8m: A large-scale video classification benchmark,\" arXiv preprint arXiv:1609.08675, 2016.\n\nAva: A video dataset of spatio-temporally localized atomic visual actions. Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, Jitendra Malik, Proceedings of the Conference on Computer Vision and Pattern Recognition. the Conference on Computer Vision and Pattern RecognitionChunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, and Jitendra Malik, \"Ava: A video dataset of spatio-temporally localized atomic visual actions,\" Proceedings of the Conference on Computer Vision and Pattern Recognition, 2018.\n\nVideo test media. Xiph, Org, Xiph.org, \"Video test media,\" https://media.xiph.org/video/derf/.\n\nStudy of subjective and objective quality assessment of video. K Seshadrinathan, R Soundararajan, A C Bovik, L K Cormack, IEEE Transactions on Image Processing. 196K. Seshadrinathan, R. Soundararajan, A. C. Bovik, and L. K. Cormack, \"Study of subjective and objective quality assessment of video,\" IEEE Transactions on Image Processing, vol. 19, no. 6, 2010.\n\nLive netflix video quality of experience database. C G Bampis, Z Li, A K Moorthy, I Katsavounidis, A Aaron, A C Bovik, C. G. Bampis, Z. Li, A. K. Moorthy, I. Katsavounidis, A. Aaron, and A. C. Bovik, \"Live netflix video quality of experience database,\" http: //live.ece.utexas.edu/research/LIVE NFLXStudy/index.html, 2016.\n\nLive mobile stall video database-ii. D Ghadiyaram, J Pan, A C Bovik, D. Ghadiyaram, J. Pan, and A.C. Bovik, \"Live mobile stall video database-ii,\" http://live.ece.utexas.edu/research/LIVEStallStudy/ index.html, 2017.\n\n. Haiqiang Wang, Ioannis Katsavounidis, Xin Zhou, Jiwu Huang, Man-On Pun, Xin Jin, Ronggang Wang, Xu Wang, Yun Zhang, Jeonghoon Park, Jiantong Zhou, Shawmin Lei, Sam Kwong, C.-C. Jay Kuo, 10.21227/H2H01CVideoset. Haiqiang Wang, Ioannis Katsavounidis, Xin Zhou, Jiwu Huang, Man- On Pun, Xin Jin, Ronggang Wang, Xu Wang, Yun Zhang, Jeonghoon Park, Jiantong Zhou, Shawmin Lei, Sam Kwong, and C.-C. Jay Kuo, \"Videoset,\" http://dx.doi.org/10.21227/H2H01C, 2016.\n\nLarge scale subjective video quality study. Zeina Sinno, Alan C Bovik, IEEE International Conference on Image Processing. Zeina Sinno and Alan C. Bovik, \"Large scale subjective video quality study,\" IEEE International Conference on Image Processing, 2018.\n\nVlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, Hui Men, Tam\u00e1s Szir\u00e1nyi, Shujun Li, Dietmar Saupe, The konstanz natural video database. Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, Hui Men, Tam\u00e1s Szir\u00e1nyi, Shujun Li, and Dietmar Saupe, \"The konstanz natural video database,\" http://database.mmsp-kn.de, 2017.\n\nIntroducing the knowledge graph: Things, not strings. Amit Singhal, Amit Singhal, \"Introducing the knowledge graph: Things, not strings,\" 2016.\n\nAnalysis of public image and video databases for quality assessment. Stefan Winkler, IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING. 66Stefan Winkler, \"Analysis of public image and video databases for quality assessment,\" IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, vol. 6, no. 6, 2012.\n\nLarge-scale cluster management at Google with Borg. Abhishek Verma, Luis Pedrosa, R Madhukar, David Korupolu, Eric Oppenheimer, John Tune, Wilkes, Proceedings of the European Conference on Computer Systems (EuroSys). the European Conference on Computer Systems (EuroSys)Bordeaux, FranceAbhishek Verma, Luis Pedrosa, Madhukar R. Korupolu, David Oppen- heimer, Eric Tune, and John Wilkes, \"Large-scale cluster management at Google with Borg,\" in Proceedings of the European Conference on Computer Systems (EuroSys), Bordeaux, France, 2015.\n", "annotations": {"author": "[{\"end\":98,\"start\":54},{\"end\":144,\"start\":99},{\"end\":215,\"start\":145}]", "publisher": null, "author_last_name": "[{\"end\":64,\"start\":60},{\"end\":110,\"start\":104},{\"end\":159,\"start\":150}]", "author_first_name": "[{\"end\":59,\"start\":54},{\"end\":103,\"start\":99},{\"end\":149,\"start\":145}]", "author_affiliation": "[{\"end\":97,\"start\":66},{\"end\":143,\"start\":112},{\"end\":214,\"start\":183}]", "title": "[{\"end\":51,\"start\":1},{\"end\":266,\"start\":216}]", "venue": null, "abstract": "[{\"end\":1519,\"start\":343}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1720,\"start\":1717},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2224,\"start\":2221},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2237,\"start\":2234},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4296,\"start\":4293},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4309,\"start\":4306},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4382,\"start\":4379},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4585,\"start\":4582},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4597,\"start\":4594},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4757,\"start\":4754},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5030,\"start\":5026},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5036,\"start\":5032},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5352,\"start\":5348},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5668,\"start\":5664},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5978,\"start\":5974},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6619,\"start\":6615},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8429,\"start\":8425},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8867,\"start\":8863},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11552,\"start\":11550},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13482,\"start\":13479},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13873,\"start\":13870},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14039,\"start\":14036}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":15726,\"start\":15546},{\"attributes\":{\"id\":\"fig_1\"},\"end\":16002,\"start\":15727},{\"attributes\":{\"id\":\"fig_2\"},\"end\":16307,\"start\":16003},{\"attributes\":{\"id\":\"fig_3\"},\"end\":16416,\"start\":16308},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":17137,\"start\":16417},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":17323,\"start\":17138}]", "paragraph": "[{\"end\":2068,\"start\":1538},{\"end\":2670,\"start\":2070},{\"end\":3624,\"start\":2672},{\"end\":4383,\"start\":3626},{\"end\":4476,\"start\":4385},{\"end\":4726,\"start\":4497},{\"end\":5010,\"start\":4728},{\"end\":5337,\"start\":5012},{\"end\":5627,\"start\":5339},{\"end\":5962,\"start\":5629},{\"end\":6393,\"start\":5964},{\"end\":7146,\"start\":6431},{\"end\":7616,\"start\":7148},{\"end\":7816,\"start\":7618},{\"end\":8832,\"start\":7841},{\"end\":9334,\"start\":8834},{\"end\":10320,\"start\":9336},{\"end\":11390,\"start\":10322},{\"end\":13165,\"start\":11392},{\"end\":13874,\"start\":13167},{\"end\":14893,\"start\":13876},{\"end\":15545,\"start\":14912}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12610,\"start\":12603}]", "section_header": "[{\"end\":1536,\"start\":1521},{\"end\":4495,\"start\":4479},{\"end\":6429,\"start\":6396},{\"end\":7839,\"start\":7819},{\"end\":14910,\"start\":14896},{\"end\":15563,\"start\":15547},{\"end\":15736,\"start\":15728},{\"end\":16012,\"start\":16004},{\"end\":16325,\"start\":16309},{\"end\":17155,\"start\":17139}]", "table": "[{\"end\":17137,\"start\":16781},{\"end\":17323,\"start\":17191}]", "figure_caption": "[{\"end\":15726,\"start\":15566},{\"end\":16002,\"start\":15738},{\"end\":16307,\"start\":16014},{\"end\":16416,\"start\":16328},{\"end\":16781,\"start\":16419},{\"end\":17191,\"start\":17157}]", "figure_ref": "[{\"end\":6639,\"start\":6633},{\"end\":9616,\"start\":9606},{\"end\":9627,\"start\":9621},{\"end\":9955,\"start\":9944},{\"end\":10040,\"start\":10029},{\"end\":10468,\"start\":10462},{\"end\":10487,\"start\":10477},{\"end\":10648,\"start\":10641},{\"end\":10761,\"start\":10755},{\"end\":11098,\"start\":11087},{\"end\":11235,\"start\":11224},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12225,\"start\":12219},{\"end\":12781,\"start\":12775},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12792,\"start\":12786},{\"end\":13424,\"start\":13418},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13840,\"start\":13834},{\"end\":14383,\"start\":14377},{\"end\":14678,\"start\":14672}]", "bib_author_first_name": "[{\"end\":17413,\"start\":17409},{\"end\":17429,\"start\":17420},{\"end\":17440,\"start\":17435},{\"end\":17454,\"start\":17450},{\"end\":17811,\"start\":17807},{\"end\":17822,\"start\":17818},{\"end\":17824,\"start\":17823},{\"end\":17837,\"start\":17832},{\"end\":17839,\"start\":17838},{\"end\":17852,\"start\":17848},{\"end\":17854,\"start\":17853},{\"end\":18169,\"start\":18166},{\"end\":18178,\"start\":18174},{\"end\":18193,\"start\":18186},{\"end\":18214,\"start\":18209},{\"end\":18229,\"start\":18224},{\"end\":18528,\"start\":18524},{\"end\":18543,\"start\":18535},{\"end\":18555,\"start\":18551},{\"end\":18803,\"start\":18798},{\"end\":18818,\"start\":18810},{\"end\":18828,\"start\":18824},{\"end\":18839,\"start\":18835},{\"end\":19152,\"start\":19146},{\"end\":19169,\"start\":19165},{\"end\":19180,\"start\":19176},{\"end\":19193,\"start\":19189},{\"end\":19516,\"start\":19512},{\"end\":19537,\"start\":19531},{\"end\":19555,\"start\":19547},{\"end\":19565,\"start\":19561},{\"end\":19580,\"start\":19574},{\"end\":19603,\"start\":19591},{\"end\":19627,\"start\":19617},{\"end\":19996,\"start\":19989},{\"end\":20005,\"start\":20001},{\"end\":20016,\"start\":20011},{\"end\":20018,\"start\":20017},{\"end\":20029,\"start\":20025},{\"end\":20048,\"start\":20040},{\"end\":20066,\"start\":20060},{\"end\":20081,\"start\":20071},{\"end\":20106,\"start\":20100},{\"end\":20124,\"start\":20117},{\"end\":20137,\"start\":20132},{\"end\":20158,\"start\":20150},{\"end\":20175,\"start\":20167},{\"end\":20831,\"start\":20830},{\"end\":20849,\"start\":20848},{\"end\":20866,\"start\":20865},{\"end\":20868,\"start\":20867},{\"end\":20877,\"start\":20876},{\"end\":20879,\"start\":20878},{\"end\":21179,\"start\":21178},{\"end\":21181,\"start\":21180},{\"end\":21191,\"start\":21190},{\"end\":21197,\"start\":21196},{\"end\":21199,\"start\":21198},{\"end\":21210,\"start\":21209},{\"end\":21227,\"start\":21226},{\"end\":21236,\"start\":21235},{\"end\":21238,\"start\":21237},{\"end\":21489,\"start\":21488},{\"end\":21503,\"start\":21502},{\"end\":21510,\"start\":21509},{\"end\":21512,\"start\":21511},{\"end\":21679,\"start\":21671},{\"end\":21693,\"start\":21686},{\"end\":21712,\"start\":21709},{\"end\":21723,\"start\":21719},{\"end\":21737,\"start\":21731},{\"end\":21746,\"start\":21743},{\"end\":21760,\"start\":21752},{\"end\":21769,\"start\":21767},{\"end\":21779,\"start\":21776},{\"end\":21796,\"start\":21787},{\"end\":21811,\"start\":21803},{\"end\":21825,\"start\":21818},{\"end\":21834,\"start\":21831},{\"end\":21851,\"start\":21842},{\"end\":22176,\"start\":22171},{\"end\":22188,\"start\":22184},{\"end\":22190,\"start\":22189},{\"end\":22388,\"start\":22384},{\"end\":22400,\"start\":22395},{\"end\":22413,\"start\":22407},{\"end\":22430,\"start\":22425},{\"end\":22439,\"start\":22436},{\"end\":22450,\"start\":22445},{\"end\":22467,\"start\":22461},{\"end\":22479,\"start\":22472},{\"end\":22763,\"start\":22759},{\"end\":22925,\"start\":22919},{\"end\":23214,\"start\":23206},{\"end\":23226,\"start\":23222},{\"end\":23237,\"start\":23236},{\"end\":23253,\"start\":23248},{\"end\":23268,\"start\":23264},{\"end\":23286,\"start\":23282}]", "bib_author_last_name": "[{\"end\":17418,\"start\":17414},{\"end\":17433,\"start\":17430},{\"end\":17448,\"start\":17441},{\"end\":17462,\"start\":17455},{\"end\":17816,\"start\":17812},{\"end\":17830,\"start\":17825},{\"end\":17846,\"start\":17840},{\"end\":17865,\"start\":17855},{\"end\":18172,\"start\":18170},{\"end\":18184,\"start\":18179},{\"end\":18207,\"start\":18194},{\"end\":18222,\"start\":18215},{\"end\":18238,\"start\":18230},{\"end\":18533,\"start\":18529},{\"end\":18549,\"start\":18544},{\"end\":18563,\"start\":18556},{\"end\":18808,\"start\":18804},{\"end\":18822,\"start\":18819},{\"end\":18833,\"start\":18829},{\"end\":18847,\"start\":18840},{\"end\":19163,\"start\":19153},{\"end\":19174,\"start\":19170},{\"end\":19187,\"start\":19181},{\"end\":19201,\"start\":19194},{\"end\":19529,\"start\":19517},{\"end\":19545,\"start\":19538},{\"end\":19559,\"start\":19556},{\"end\":19572,\"start\":19566},{\"end\":19589,\"start\":19581},{\"end\":19615,\"start\":19604},{\"end\":19644,\"start\":19628},{\"end\":19999,\"start\":19997},{\"end\":20009,\"start\":20006},{\"end\":20023,\"start\":20019},{\"end\":20038,\"start\":20030},{\"end\":20058,\"start\":20049},{\"end\":20069,\"start\":20067},{\"end\":20098,\"start\":20082},{\"end\":20115,\"start\":20107},{\"end\":20130,\"start\":20125},{\"end\":20148,\"start\":20138},{\"end\":20165,\"start\":20159},{\"end\":20181,\"start\":20176},{\"end\":20693,\"start\":20689},{\"end\":20698,\"start\":20695},{\"end\":20846,\"start\":20832},{\"end\":20863,\"start\":20850},{\"end\":20874,\"start\":20869},{\"end\":20887,\"start\":20880},{\"end\":21188,\"start\":21182},{\"end\":21194,\"start\":21192},{\"end\":21207,\"start\":21200},{\"end\":21224,\"start\":21211},{\"end\":21233,\"start\":21228},{\"end\":21244,\"start\":21239},{\"end\":21500,\"start\":21490},{\"end\":21507,\"start\":21504},{\"end\":21518,\"start\":21513},{\"end\":21684,\"start\":21680},{\"end\":21707,\"start\":21694},{\"end\":21717,\"start\":21713},{\"end\":21729,\"start\":21724},{\"end\":21741,\"start\":21738},{\"end\":21750,\"start\":21747},{\"end\":21765,\"start\":21761},{\"end\":21774,\"start\":21770},{\"end\":21785,\"start\":21780},{\"end\":21801,\"start\":21797},{\"end\":21816,\"start\":21812},{\"end\":21829,\"start\":21826},{\"end\":21840,\"start\":21835},{\"end\":21855,\"start\":21852},{\"end\":22182,\"start\":22177},{\"end\":22196,\"start\":22191},{\"end\":22393,\"start\":22389},{\"end\":22405,\"start\":22401},{\"end\":22423,\"start\":22414},{\"end\":22434,\"start\":22431},{\"end\":22443,\"start\":22440},{\"end\":22459,\"start\":22451},{\"end\":22470,\"start\":22468},{\"end\":22485,\"start\":22480},{\"end\":22771,\"start\":22764},{\"end\":22933,\"start\":22926},{\"end\":23220,\"start\":23215},{\"end\":23234,\"start\":23227},{\"end\":23246,\"start\":23238},{\"end\":23262,\"start\":23254},{\"end\":23280,\"start\":23269},{\"end\":23291,\"start\":23287},{\"end\":23299,\"start\":23293}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52191467},\"end\":17731,\"start\":17325},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207761262},\"end\":18112,\"start\":17733},{\"attributes\":{\"id\":\"b2\"},\"end\":18429,\"start\":18114},{\"attributes\":{\"id\":\"b3\"},\"end\":18742,\"start\":18431},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15395746},\"end\":19067,\"start\":18744},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3463873},\"end\":19452,\"start\":19069},{\"attributes\":{\"doi\":\"arXiv:1609.08675\",\"id\":\"b6\"},\"end\":19912,\"start\":19454},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":688013},\"end\":20669,\"start\":19914},{\"attributes\":{\"id\":\"b8\"},\"end\":20765,\"start\":20671},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206724285},\"end\":21125,\"start\":20767},{\"attributes\":{\"id\":\"b10\"},\"end\":21449,\"start\":21127},{\"attributes\":{\"id\":\"b11\"},\"end\":21667,\"start\":21451},{\"attributes\":{\"doi\":\"10.21227/H2H01C\",\"id\":\"b12\"},\"end\":22125,\"start\":21669},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":51995218},\"end\":22382,\"start\":22127},{\"attributes\":{\"id\":\"b14\"},\"end\":22703,\"start\":22384},{\"attributes\":{\"id\":\"b15\"},\"end\":22848,\"start\":22705},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11352985},\"end\":23152,\"start\":22850},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1149996},\"end\":23691,\"start\":23154}]", "bib_title": "[{\"end\":17407,\"start\":17325},{\"end\":17805,\"start\":17733},{\"end\":18164,\"start\":18114},{\"end\":18796,\"start\":18744},{\"end\":19144,\"start\":19069},{\"end\":19987,\"start\":19914},{\"end\":20828,\"start\":20767},{\"end\":22169,\"start\":22127},{\"end\":22917,\"start\":22850},{\"end\":23204,\"start\":23154}]", "bib_author": "[{\"end\":17420,\"start\":17409},{\"end\":17435,\"start\":17420},{\"end\":17450,\"start\":17435},{\"end\":17464,\"start\":17450},{\"end\":17818,\"start\":17807},{\"end\":17832,\"start\":17818},{\"end\":17848,\"start\":17832},{\"end\":17867,\"start\":17848},{\"end\":18174,\"start\":18166},{\"end\":18186,\"start\":18174},{\"end\":18209,\"start\":18186},{\"end\":18224,\"start\":18209},{\"end\":18240,\"start\":18224},{\"end\":18535,\"start\":18524},{\"end\":18551,\"start\":18535},{\"end\":18565,\"start\":18551},{\"end\":18810,\"start\":18798},{\"end\":18824,\"start\":18810},{\"end\":18835,\"start\":18824},{\"end\":18849,\"start\":18835},{\"end\":19165,\"start\":19146},{\"end\":19176,\"start\":19165},{\"end\":19189,\"start\":19176},{\"end\":19203,\"start\":19189},{\"end\":19531,\"start\":19512},{\"end\":19547,\"start\":19531},{\"end\":19561,\"start\":19547},{\"end\":19574,\"start\":19561},{\"end\":19591,\"start\":19574},{\"end\":19617,\"start\":19591},{\"end\":19646,\"start\":19617},{\"end\":20001,\"start\":19989},{\"end\":20011,\"start\":20001},{\"end\":20025,\"start\":20011},{\"end\":20040,\"start\":20025},{\"end\":20060,\"start\":20040},{\"end\":20071,\"start\":20060},{\"end\":20100,\"start\":20071},{\"end\":20117,\"start\":20100},{\"end\":20132,\"start\":20117},{\"end\":20150,\"start\":20132},{\"end\":20167,\"start\":20150},{\"end\":20183,\"start\":20167},{\"end\":20695,\"start\":20689},{\"end\":20700,\"start\":20695},{\"end\":20848,\"start\":20830},{\"end\":20865,\"start\":20848},{\"end\":20876,\"start\":20865},{\"end\":20889,\"start\":20876},{\"end\":21190,\"start\":21178},{\"end\":21196,\"start\":21190},{\"end\":21209,\"start\":21196},{\"end\":21226,\"start\":21209},{\"end\":21235,\"start\":21226},{\"end\":21246,\"start\":21235},{\"end\":21502,\"start\":21488},{\"end\":21509,\"start\":21502},{\"end\":21520,\"start\":21509},{\"end\":21686,\"start\":21671},{\"end\":21709,\"start\":21686},{\"end\":21719,\"start\":21709},{\"end\":21731,\"start\":21719},{\"end\":21743,\"start\":21731},{\"end\":21752,\"start\":21743},{\"end\":21767,\"start\":21752},{\"end\":21776,\"start\":21767},{\"end\":21787,\"start\":21776},{\"end\":21803,\"start\":21787},{\"end\":21818,\"start\":21803},{\"end\":21831,\"start\":21818},{\"end\":21842,\"start\":21831},{\"end\":21857,\"start\":21842},{\"end\":22184,\"start\":22171},{\"end\":22198,\"start\":22184},{\"end\":22395,\"start\":22384},{\"end\":22407,\"start\":22395},{\"end\":22425,\"start\":22407},{\"end\":22436,\"start\":22425},{\"end\":22445,\"start\":22436},{\"end\":22461,\"start\":22445},{\"end\":22472,\"start\":22461},{\"end\":22487,\"start\":22472},{\"end\":22773,\"start\":22759},{\"end\":22935,\"start\":22919},{\"end\":23222,\"start\":23206},{\"end\":23236,\"start\":23222},{\"end\":23248,\"start\":23236},{\"end\":23264,\"start\":23248},{\"end\":23282,\"start\":23264},{\"end\":23293,\"start\":23282},{\"end\":23301,\"start\":23293}]", "bib_venue": "[{\"end\":17518,\"start\":17464},{\"end\":17904,\"start\":17867},{\"end\":18264,\"start\":18240},{\"end\":18522,\"start\":18431},{\"end\":18898,\"start\":18849},{\"end\":19252,\"start\":19203},{\"end\":19510,\"start\":19454},{\"end\":20255,\"start\":20183},{\"end\":20687,\"start\":20671},{\"end\":20926,\"start\":20889},{\"end\":21176,\"start\":21127},{\"end\":21486,\"start\":21451},{\"end\":21880,\"start\":21872},{\"end\":22247,\"start\":22198},{\"end\":22522,\"start\":22487},{\"end\":22757,\"start\":22705},{\"end\":22987,\"start\":22935},{\"end\":23369,\"start\":23301},{\"end\":20314,\"start\":20257},{\"end\":23440,\"start\":23371}]"}}}, "year": 2023, "month": 12, "day": 17}