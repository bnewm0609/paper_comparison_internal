{"id": 5275290, "updated": "2023-10-08 22:25:38.017", "metadata": {"title": "Machine learning for decoding listeners\u2019 attention from electroencephalography evoked by continuous speech", "authors": "[{\"first\":\"Tobias\",\"last\":\"Taillez\",\"middle\":[]},{\"first\":\"Birger\",\"last\":\"Kollmeier\",\"middle\":[]},{\"first\":\"Bernd T.\",\"last\":\"Meyer\",\"middle\":[]}]", "venue": "The European journal of neuroscience", "journal": "The European journal of neuroscience", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Previous research has shown that it is possible to predict which speaker is attended in a multispeaker scene by analyzing a listener's electroencephalography (EEG) activity. In this study, existing linear models that learn the mapping from neural activity to an attended speech envelope are replaced by a non\u2010linear neural network (NN). The proposed architecture takes into account the temporal context of the estimated envelope and is evaluated using EEG data obtained from 20 normal\u2010hearing listeners who focused on one speaker in a two\u2010speaker setting. The network is optimized with respect to the frequency range and the temporal segmentation of the EEG input, as well as the cost function used to estimate the model parameters. To identify the salient cues involved in auditory attention, a relevance algorithm is applied that highlights the electrode signals most important for attention decoding. In contrast to linear approaches, the NN profits from a wider EEG frequency range (1\u201332 Hz) and achieves a performance seven times higher than the linear baseline. Relevant EEG activations following the speech stimulus after 170 ms at physiologically plausible locations were found. This was not observed when the model was trained on the unattended speaker. Our findings therefore indicate that non\u2010linear NNs can provide insight into physiological processes by analyzing EEG activity.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2771075519", "acl": null, "pubmed": "29205588", "pubmedcentral": null, "dblp": null, "doi": "10.1111/ejn.13790"}}, "content": {"source": {"pdf_hash": "399bcbae9a8b67fa873fd80a1f2d250892124897", "pdf_src": "Wiley", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a6499e62661a596f4fb4faeef770b772d2159881", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/399bcbae9a8b67fa873fd80a1f2d250892124897.txt", "contents": "\nMachine learning for decoding listeners' attention from electroencephalography evoked by continuous speech\n\n\nTobias De Taillez \nMedizinische Physik and Cluster of Excellence Hearing4all\nCarl von Ossietzky Universit\u20ac at26129OldenburgGermany\n\nBirger Kollmeier \nMedizinische Physik and Cluster of Excellence Hearing4all\nCarl von Ossietzky Universit\u20ac at26129OldenburgGermany\n\nBernd T Meyer \nMedizinische Physik and Cluster of Excellence Hearing4all\nCarl von Ossietzky Universit\u20ac at26129OldenburgGermany\n\nMachine learning for decoding listeners' attention from electroencephalography evoked by continuous speech\n10.1111/ejn.13790auditoryauditory processinghearingneural networkssignaling pathways\nPrevious research has shown that it is possible to predict which speaker is attended in a multispeaker scene by analyzing a listener's electroencephalography (EEG) activity. In this study, existing linear models that learn the mapping from neural activity to an attended speech envelope are replaced by a non-linear neural network (NN). The proposed architecture takes into account the temporal context of the estimated envelope and is evaluated using EEG data obtained from 20 normal-hearing listeners who focused on one speaker in a two-speaker setting. The network is optimized with respect to the frequency range and the temporal segmentation of the EEG input, as well as the cost function used to estimate the model parameters. To identify the salient cues involved in auditory attention, a relevance algorithm is applied that highlights the electrode signals most important for attention decoding. In contrast to linear approaches, the NN profits from a wider EEG frequency range (1-32 Hz) and achieves a performance seven times higher than the linear baseline. Relevant EEG activations following the speech stimulus after 170 ms at physiologically plausible locations were found. This was not observed when the model was trained on the unattended speaker. Our findings therefore indicate that non-linear NNs can provide insight into physiological processes by analyzing EEG activity.\n\nIntroduction\n\nNormal-hearing listeners are able to focus on one speaker in a multispeaker scenario and to effectively suppress concurring speakers (Mesgarani & Chang, 2012). This ability can be affected in hearingimpaired listeners (Shinn-Cunningham & Best, 2008), which can partially be compensated by spatial filtering in multichannel hearing aids, for example, by beamforming for enhancing speech from one specific speaker (Haykin & Liu, 2010). A passive system that automatically identifies and enhances the attended speaker is highly desirable in this application scenario. Attentional effects on auditory stimuli are detectable by analyzing rhythm-correlated brain activity like auditory steady-state response (Ding & Simon, 2012) or the p300 responses (Polich, 1986;Spencer & Polich, 1999). However, these techniques fail for continuous speech stimuli (Ding & Simon, 2012;Horton et al., 2014). Another approach is to track the lateralization of alpha power bands for decoding spatial attention. The effect is strongest when shifting attention, while it is less pronounced when a sound source is continuously attended and is therefore also not suitable for continuous attention decoding (Kerlin et al., 2010). The current state-of-the-art technique for detecting auditory attention for continuous speech streams was proposed by Aiken & Picton (2008): this approach is based on the hypothesis that speech features of the attended speech stream such as the envelope are represented in neural brain activity. A source can be determined by measurement of this activity and subsequent correlation with individual speech streams of the speech sources. This was shown for electroencephalography (EEG) (Aiken & Picton, 2008;Di Liberto et al., 2015;Mirkovic et al., 2015;Biesmans et al., 2017), magnetoencephalography (Akram et al., 2016) and electrocorticography; Mesgarani & Chang, 2012).\n\nPrevious studies reached accuracies of 88% with a 60 s analysis window using EEG data (Mirkovic et al., 2015) for decoding attention in a two-speaker paradigm. EEG offers high temporal resolution, is not invasive and is available as mobile system (Debener et al., 2012).\n\nThe previously mentioned studies (Aiken & Picton, 2008;Mesgarani & Chang, 2012;O'sullivan et al., 2014;Mirkovic et al., 2015Mirkovic et al., , 2016Biesmans et al., 2017) successfully applied a linear superposition of measured brain activity for stimulus reconstruction or attention decoding. In these studies, the authors point out two potential shortcomings of this approach. First, the auditory processing can be assumed to be non-linear due to dynamic compression, loss of fine structure information by integration in the auditory nerve or non-linear neuronal processes. Second, the mapping of this process is probably not invertible by a linear approach. The fairly low correlation values between the reconstructed and the attended envelope (r = 0.054; O'sullivan et al., 2014) also support this assumption. Therefore, in this study, we investigate non-linear machine learning methods with the aim of a better decoding of listeners' attention. Non-linear models for speech modeling have been used in automatic speech recognition (ASR) for many decades, but network topologies such as deep neural networks (NNs; Hinton et al., 2012) or long short-term memory networks (Graves et al., 2013) recently had a large impact on improving ASR systems. We propose to apply an artificial NN with a novel net architecture to replace the linear regression used in previous studies (O'sullivan et al., 2014;Mirkovic et al., 2015) as an attention decoder. The network non-linearity reduces the error in the inverse mapping between EEG and audio stimulus to some extent. This approach also offers a tunable amount of parameters for a more sensitive and accurate reconstruction of the stimulus' envelope. A training paradigm is suggested in which consecutive output values of the net are considered for adjustment of model parameters, which contrasts with the standard procedure of using temporal context of input values. With this network, we investigate whether the non-linear model can profit from a wider EEG frequency range than previous models (O'sullivan et al., 2014;Mirkovic et al., 2015), from a temporal segmentation of data, and whether the accuracy for efficient decoding can be increased.\n\nA better understanding of how a machine learning algorithm inverts the brain's audio processing could also contribute to understanding physiological processes. While statistical models such as NNs are often considered to be black boxes, several methods exist to analyze the salient cues for classification learned by the net. An algorithm to analyze the relevance of the input features for producing the output (also referred to as heat mapping) was recently proposed (Bach et al., 2015;Sturm et al., 2016). In this study, the algorithm is used to identify where and when neural activity occurs that is relevant for decoding of auditory attention. This includes the location and time of electrode activity that later results in correct speaker decoding, which enables an analysis of important cues in auditory processing that arise from physiological processes, such as the stimulus-response delay time, the skull region associated with high relevance and an interaction of these two.\n\nIn summary, three main goals are addressed in this study: first, we investigate whether current machine learning methods can contribute to improve the decoding of listeners' attention. Second, different input representations and network architectures are analyzed to optimize model parameters. Third, the spatial and temporal cues for reconstructing the attended speech envelope are explored by analyzing model parameters and the input-output relation of EEG data.\n\n\nMaterials and methods\n\n\nAcoustic stimuli\n\nThe data set gathered by Mirkovic et al. (2016) was made available for our study and contains synchronized data streams of audio and EEG signals. Excerpts from German audiobooks were used as speech material ('A drama in the air' by Jules Verne and 'Two brothers' by Grimm brothers) read by two different professional male speakers. The speakers were virtually placed at +45\u00b0and \u00c045\u00b0a zimuth by calculating the convolution of the stimuli with headrelated transfer functions that were recorded in an anechoic chamber (Kayser et al., 2009). Both stories were played back continuously over tube in-ear headphones (E-A-RTONE 3A) with an external electromagnetically shielded sound generation box to minimize the interference of the headphones with EEG electrodes. The audio presentation was paused every 10 minutes, and participants were asked to answer content-related questionnaires subsequently. This procedure was repeated five times, which resulted in 50 min of data per subject.\n\n\nParticipants\n\nTwenty healthy, normal-hearing listeners participated in the experiments (mean age 25, eight male, one left-handed). They were randomly assigned to attend exclusively to the left or the right audiobook while keeping the number of participants per group even (10 for both sides). All participants were paid for their participation equally.\n\nEEG recording procedure and preprocessing EEG data were collected with a BrainAmp EEG amplifier system (BrainProducts, Gilching, Germany). A cap with a 96 Ag/AgCl electrode layout, equidistantly placed with a central frontopolar site as ground and a nose-tip reference, was used (Easycap, Herrsching, Germany) with 500-Hz sampling frequency. These electrodes do not follow the naming convention and/or locations of the 10/20-system (Jasper, 1958). Therefore, references to electrode positions are given with a corresponding 10/20-position. From the available 96 electrode locations, 12 were not used due to the presence of an additional ear-centered EEG devices called 'cEEGrid' (six electrodes on each side). To obtain a homogeneous data set, the cEE-Grid data were not used in this study so that 84 channels were available. Divergent from the initial approach (Aiken & Picton, 2008), the frequency band used was widened due to findings from Di Liberto et al. (2015) who found that for phoneme level decoding, information up to 45 Hz can be useful. The preprocessing was performed off-line using customized Matlab/Python scripts and consisted of band-pass filtering between 1 and 32 Hz (2-8 Hz for resembling the linear regression approach used in earlier studies (O'sullivan et al., 2014;Mirkovic et al., 2015) for comparison) as well as a subsequently down-sampling to 64 Hz. Data were rereferenced to a common average reference. No further artifact correction was conducted. The clean speech material was transformed to their respective absolute envelope by a Hilbert transformation, low-pass filtered with 8 or 32 Hz (see above) and down sampled to 64 Hz to match the EEG data. EEG data channels and audio envelopes were normalized to ensure zero mean and unit variance. For each participant, the five 10-min blocks of EEG recording and corresponding audio streams were concatenated so that a 50-min long data block was created. During post-processing, the data blocks of four participants were excluded: two data sets from the group that attended Story 1 (one due to a low score in the questionnaire and one due to technical problems) and two randomly selected sets from the group attending Story 2 (to reestablish a quantitatively evenly distributed data set). This resulted in the final data set with data from 16 subjects.\n\n\nNeural network structure\n\nThe linear regression used in previous studies (O'sullivan et al., 2014;Mirkovic et al., 2015) is nearly equivalent to a NN using a linear activation function and no hidden layers. For comparability, this linear mapping was therefore defined as the starting point for experiments with NNs and is also tested in the experiments to compare the previous linear approach with the proposed NN structure. Subsequently, more features of deep learning were added ranging from typical non-linear 'tanh' activation functions over additional hidden layers to methods for avoiding overfitting to the data such as dropout (Srivastava et al., 2014). For attention decoding, we introduce a novel training scheme that performs a sample-wise prediction of the envelope but is able to include temporal context of the output (i.e., many consecutive samples) during training. While the inclusion of temporal context for the input is standard procedure, that is, in ASR (Hinton et al., 2012), it has not been considered for output values in the context of physiological data. The resulting network was implemented in Keras (Chollet, 2015)/Ten-sorFlow (Abadi et al., 2016). The NN-based processing is illustrated in Fig. 1: for each point in time t 0 , a NN is trained to predict the current envelope value\u00ea 0 , taking into account a temporal input context of M = 27 frames (corresponding to 420 ms; compare (Mesgarani & Chang, 2012)). During training, we use a training prediction window of length L, which will produce L adjacent envelope predictions using the same NN weights. The resulting time series\u00ea n \u00f0 \u00de is compared to the original (attended) envelope e a n \u00f0 \u00de. The match between e a n \u00f0 \u00de and\u00ea n \u00f0 \u00de is measured with the cost function [mean-squared error (MSE) or correlation, respectively], which in turn is used to update the weights of the NN. This sample-wise approach allows to analyze long time series with a moderate amount of parameters. A smaller number of parameters allow successful training on comparatively small amounts of data (such as the EEG data in this study, which amounts to 50 min per participant). The NN consisted of an adjustable number of hidden layers with neurons of the non-linear activation and one output neuron with a linear activation. This modular setup provided the opportunity to investigate a range of neural net parameter settings. In speech processing, an intentional omission of training samples was found to be beneficial for generalization of classifiers. This technique is referred to as dropout. A dropout layer was included after each layer (except the output layer) with dropout strength of a = 0.25, so that with a chance of 25%, the activation of a certain neuron will be set to zero. Further, two cost functions were used to measure the distance between the estimated and the actual attended envelope: first, the MSE was used as a cost function C MSE . This is a widely used approach in deep learning (Bengio et al., 2007;Bengio, 2012;Sturm et al., 2016). With this cost function and the simple neural net, a linear regression is resembled by\nC MSE\u00eaa ; e a \u00f0 \u00de\u00bc\u00bde a \u00c0\u00ea a 2 ;\nwith the attended e a and predicted\u00ea a envelope. Second, a correlation-based cost function was employed that aims to maximize the correlation of prediction and stimulus:\nC corr \u00f0\u00ea a ; e a \u00de \u00bc 1 \u00c0 cov\u00f0\u00ea a ; e a \u00de std\u00f0\u00ea a \u00de \u00c1 std\u00f0e a \u00de \u00fe e :\nA cost of zero implies a perfect correlation between attended envelope and predicted envelope, while cost values of 1 correspond to no correlation. Values between 1 and 2 indicate negative correlation. \u025b is a random number in the machine precision range (e.g., 10 \u00c030 ) to avoid division by zero in rare cases.\n\n\nEvaluation cycle\n\nIn the following, we describe the training and testing procedure of the neural net. First, 20% consecutive data samples were chosen from a randomly selected participant and starting point. The remaining 80% of the data were used for training as described in Section Neural network structure. The 20% were split into two 10% blocks, one for cross-validation (CV set) and one for evaluation (test set). The training algorithm uses the Nadam optimization criterion, which effectively prevents to get stuck in local minima in the error distribution by adaptively changing the learning rate. Training was performed according to standard deep learning procedure, that is, 'early stopping' was used as a method to prevent overfitting (Caruana et al., 2001). Training was continued until no loss reduction was achieved on the CV set for five epochs. For evaluation, the NN weights are fixed and each envelope value of the test set is predicted with the same temporal input context as during training (27 samples). To determine how well this prediction matches the attended stimulus envelope, time windows of\u00ea n \u00f0 \u00de were correlated with e a n \u00f0 \u00de and the unattended envelope e u n \u00f0 \u00de, respectively. The higher correlation value was counted as decision and rated against the groundtruth. The time windows had an overlap of at least 80% to ensure systematic testing. The resulting accuracy vector was averaged over time. Various analysis time window lengths were evaluated by this process. In a last step, the average accuracy P of a cycle is transformed into the information transfer rate (i.e., the effective bitrate) (Wolpaw et al., 1998).\nR \u00bc log 2 N \u00fe P \u00c1 log 2 P \u00fe 1 \u00c0 P \u00f0 \u00de\u00c1log 2 1\u00c0P N\u00c01 :\nN denotes the number of classes (in this study: N = 2 for attended and unattended streams). For comparability, the bitrate is scaled with the time window length to yield the number of correct decisions per minute (bit/min). To cover all participants and data points in test and training, the procedure is repeated 350 times; one iteration is referred to as evaluation cycle. Due to the random weight initialization during training, the repetition and the later averaging result in convergence and stable results. The random weights for each cycle also enforce intra-subject training. \n\n\nRelevance of electrodes for attention decoding\n\nThe use of NNs for envelope reconstruction enables the application of recently developed algorithms that focus on a better understanding of processing principles in NNs. Bach et al. (2015) introduced an algorithm that estimates the contribution (or relevance) of NN input values to a correct classification result. This is achieved by propagating the relevance of a NN output to the input neurons. We use this approach to investigate the relevance of electrodes (and implicitly electrode positions) for attention decoding. This should separate input components with a small contribution (channels that do not encode the speech envelope and/or are dominated by noise) and important electrodes. The relevance algorithm allows an analysis of the complete input matrix that extends over time. Therefore, the relevance of electrode activity relative to an acoustic event (e.g., a word or syllable onset) can be studied. The portion of the output neuron that is explained by the specific input neuron is given by\nr l \u00f0 \u00de i \u00bc P j zij P i 0 z i 0 j r l\u00fe1 \u00f0 \u00de j with z ij \u00bc x l \u00f0 \u00de i w l;l\u00fe1 \u00f0 \u00de ij ;\nwhere the relevance r i of neuron i in layer l is calculated recursively from all upper layer relevances r j . r i is weighted with the activation z ij between two neurons i and j. x i represents the neuron activation, and w ij , the weighting parameter. The relevance of the output neuron is defined as the match (correlation or MSE) between predicted and attended envelope. This match is calculated with a moving, overlapping time window of 16 samples (250 ms) and therefore provides results on this relatively short time scale. To identify cues that are relevant for a correct decoding, only time samples with a positive correlation were evaluated with the relevance algorithm. For each evaluated sample of the test set, an array of input relevance values was calculated, which are subsequently averaged over time.\n\n\nResults\n\n\nDecoding performance\n\nIn this section, we report the performance for decoding listeners' attention in dependence of four important parameters that are expected to affect the score. These are (a) the EEG bandwidth, (b) the length of the training prediction window, (c) the type of cost function and (d) the length of the analysis window (cf. Fig. 1). Also, the NN depth was varied with the best results were achieved with one hidden layer (data not shown). This presumably arises from the limited amount of training data, as deep NNs (i.e., with several hidden layers) are prone to overfitting when a larger number of weights are used for a rather small training set as it was performed here. The parameters (a-c) were optimized sequentially as a complete search of the four-dimensional search space was computationally not feasible. Our intuition is that a complete search is not required either, as for instance, the type of cost function should be optimal both for wideband as well as narrowband EEG data, that is, the factors presumably do not interact with each other. However, the full range of analysis window duration was analyzed for each configuration.\n\nThe results for identifying the attended speaker in terms of bits/ minute are shown in Fig. 2. Note that the first tested condition in Panel (a) corresponds to the linear regression from previous studies (O'sullivan et al., 2014;Mirkovic et al., 2015): a fully connected two-layer net without activation function is identical to a linear mapping. Using a narrowband EEG and envelope bandwidth, a training prediction window of one sample, and the MSE cost function, the procedure used in Mirkovic et al. study (2016) is recreated, which serves as the linear baseline. It can be shown that a strong overall impact is achieved by increasing the bandwidth from 2-8 Hz to 1-32 Hz. When using broadband information, the transmitted bitrate is increased to 355% in comparison with the 2-8 Hz frequency range. These results were obtained with the C MSE cost function and a training prediction window of one sample, which corresponds to the linear regression approach from previous studies (O'sullivan et al., 2014;Mirkovic et al., 2015). Statistical significance of differences was tested with a one-sided Wilcoxon signed-rank test as the data are not normally distributed; the result of the significance test is denoted in the plot. Figure 2b shows the effect of the training prediction window, that is, the number of consecutive predicted samples of the net used to calculate correlation or MSE error during training. The bitrate increases with longer prediction windows and saturates for durations of 12 and 16 samples. Due to this saturation, and as the computational cost for training increases exponentially with longer windows, higher values for the window duration were not considered. For the training prediction window of 16 samples (250 ms), both cost functions were evaluated (Fig. 2c): maximizing the correlation between predicted and real envelope significantly increases the bitrate to 110% compared to the traditional MSE criterion.\n\nOn single frame level, a long analysis window provides the best accuracy with a classification rate of 0.976 for a 60 s window. However, the analysis of rather long segments introduces a significant delay. Further, a segmentation into smaller chunks and subsequent integration could provide an increased performance. Figure 2d shows how different durations of analysis windows affect the decoding performance. In the linear case, the analysis window duration is not a crucial factor. For non-linear neural nets, however, decoding performance is greatly increased when moving from 60 s to the optimum 2 s analysis duration. Choosing a window of two-seconds therefore constitutes the best trade-off between frame-wise accuracy (P = 0.678 on average) and subsequent integration of single decisions. Overall the evaluation over 350 cycles for each condition shows a maximum in decisions per minute for the correlation-based network including a bandwidth of 1-32 Hz and 16 samples filter length. In comparison with the linear regression from previous studies (P = 0.88 for t = 60 s corresponding to 0.47 bit min , compare (Mirkovic et al., 2015)), this is a factor of 7.5 or a bit rate of 3.6 bit min (compare Fig. 2d, black/diamond line). In terms of decisions per time window, this bit rate corresponds to a correct decision every 16.7 s. This is also reflected in the average correlation coefficient between prediction and attended envelope. For the linear regression, an average correlation coefficient of corr LR = 0.030 was achieved whereas the highest performance condition yields corr Best = 0.131.\n\n\nRelevance of electrodes and time lags\n\nThe configuration that yields the highest decoding performance (cf. Section Decoding performance) uses an EEG frequency range of 1-32 Hz, a 16 sample training prediction window, and correlation as a training cost function. The EEG data and the corresponding envelopes for this configuration are used to analyze the relevance of time lags between EEG and audio and of electrode channels (and therefore implicitly electrode positions). The relevance matrix was calculated by averaging over 350 evaluation cycles and sorted by energy per channel. The assumption behind this is that a high relevance only could appear at distinct points in time which would result in high root-mean-squared (RMS) values for an electrode in the relevance analysis. Therefore, the channels with high RMS presumably carry more information. Figure 3a shows the averaged relevance matrix with sorted channels. A high relevance can be observed at around 47 and 172 ms. Figure 3b shows the RMS per channel and a threshold for which the channel RMS starts to increase considerably. Channels above this threshold will be evaluated for Fig. 4. In Fig. 3c, topographic representations are displayed for these two distinct time points and in grand average over all time points. Clear relevance clusters can be distinguished above both ears for each time point. Additionally, an occipital lateral relevance cluster on the left hemisphere is observed for the early time point. The relevance of time points averaged over all channels is depicted in Fig. 3d. The electrodes of the 10/20-system close to the relevance clusters would be TP9/TP7/T7 as well as TP10/TP8/T8. If only the above-threshold channels are considered, the two peaks at 47 and 172 ms are clearly visible (compare Fig. 3e). These findings correspond to the case for which the reconstructor was trained on the attended stimulus.\n\nIn an additional experiment, we investigate the relation of EEG data with the envelope of the unattended speaker. This is achieved using the envelope of unattended speech as the training target. If the importance of EEG activity is related to the speaker attention task (and not a mere product of bottom-up processing), a clear difference between the mapping learned by the neural net should emerge. The results contrasting a training with unattended and with attended speech are shown in Fig. 4. The main peak for the attended case (120-300 ms) cannot be observed for unattended speech, which means that EEG data provide little information for reconstruction of the unattended speaker (which is in line with the good decoding results from the attended speaker shown in Fig. 2.\n\n\nDiscussion\n\n\nDecoding performance\n\nThe aim of this study was to provide a method for decoding listeners' attention based on machine learning methods and to improve results obtained in earlier studies (O'sullivan et al., 2014;Mirkovic et al., 2015) that exploited linear solutions for the decoding task (Mirkovic et al., 2016). Compared to these approaches, decoding performance was increased by a factor of seven using neural nets, which can be attributed to four main factors: (i) an increased bandwidth of EEG data compared to the narrowband representation applied in linear approaches, (ii) the use of temporal context during training through a training prediction window, (iii) optimization of the cost function that measures the difference between the current predicted and the target envelope and (iv) temporal segmentation of data into relatively short observation chunks. These factors are discussed in the following. (i) Exploiting broadband EEG information: The performance increase depicted in Fig. 2a shows that an extension from narrowband (2-8 Hz) to broadband (1-32 Hz) EEG data boosts performance of envelope-related decoding. This is in line with findings from Di Liberto et al. (2015) who found that for speech reconstruction on phoneme level, EEG frequency bands up to 45 Hz is useful. Although the authors state that temporal speech modulations predominantly occur in the range of 2-8 Hz, it seems that cues useful for discrimination in a twospeaker scenario extend to much higher modulations. A possible explanation is that the difference between two envelopes on time scales as short as 31 ms (which corresponds to 32 Hz) is represented in the EEG data, which could be exploited for onset detection (which in turn would be helpful for attributing the EEG data to the attended speaker). A frequency-specific analysis could help to disentangle the contributions of low-and highfrequency modulations for this decoding task, which will be subject of future research. (ii) Usage of time prediction windows: It could be also shown that the usage of training prediction windows is useful to improve performance. While in machine learning tasks such as ASR, usually a temporal context is provided for the input (as it is also performed in this study), the extension of providing temporal context during training for the output has not been used before for time series predicted by neural nets. The inclusion of temporal context of the output during training enabled the use of cost functions that require consecutive time samples such as correlation of time signals, which resulted in further improvements. (iii) The improvement achieved using a correlation-based cost function instead of MSE (compare Fig. 2b) might result from the invariance of correlation with respect to linear signal compression and to offsets between predicted and measured data. Such mismatches can occur as the predicted output cannot be normalized (which is the case when predicting short time segments with a neural net), but is compared to the normalized envelope. MSE on the other hand penalizes offsets and differences through linear scaling, while the selection criterion in the final selection step is also invariant to both factors. (iv) The length of the analysis window was analyzed for a range of combinations of the preceding three factors. When using a configuration similar to the linear approach applied earlier (O'sullivan et al., 2014;Mirkovic et al., 2015; red curve in Fig. 2d), a separation into smaller time segments only had minor effects, that is, temporal separation seems to be a negligible factor for linear decoding. For all other conditions that use either broadband data or longer training prediction windows, we observed a trade-off between the length of the analysis window and the number of binary decisions that can be made in a given (here: 1 min) time segment. Best decoding performance was obtained with 2-s windows, which is consistent over all test conditions. This duration therefore seems to be long enough to capture syllables reflected in the envelope, yet short enough to ignore  unrelated speech events, that is, temporally distant phonetic events do not influence each other. For decoding with a non-linear neural net, this factor increased overall decoding performance by a factor of seven.\n\n\nRelevance of time lags and suppression of interfering speaker\n\nThe relevance analysis has shown two electrode clusters to be of special importance for envelope reconstruction. These clusters are located in an occipital/lateral position near to the auditory cortex. Both show high relevance at two distinct points in time (47 and 172 ms, respectively). The second relevance peak is in line with results for linear decoding from Mirkovic et al. (2015), and the relevant activations arise from regions that are physiologically plausible. This indicates that neural nets linked with relevance analysis can confirm earlier results from physiological studies (Power et al., 2012). Moreover, the existence of an additional peak (47 ms, which was not reported in Mirkovic et al. study (2015)) hints at improved exploitation of the EEG input data for decoding, that is, it suggests that additional information can be derived with non-linear neural nets. When the network is trained on the unattended stimulus, the second relevance peak vanishes (Fig. 4), that is, the second maximum after 120 ms helps to discriminate between unattended and attended speakers, which could be caused by attentional top-down effects for suppressing interfering talkers. A first maximum at 47 ms has a high relevance for envelope reconstruction for the attended and unattended case and therefore does not contribute for attention decoding.\n\n\nConclusion\n\nIn this study, a NN was proposed to map listeners' EEG signals to the envelope of the attended speaker's signal in a spatial twospeaker scenario. The proposed network was trained using the temporal context of the output, which enabled the evaluation of time-dependent cost functions such as correlation. We found the duration of the analysis window to be an important parameter when using a non-linear neural net: When segmenting the data into short blocks and subsequent combination of single decisions, the decoding performance was strongly increased compared to 1min segments. We also found thatin contrast to other reconstruction schemes -NNs profit from broadband EEG input (1-32 Hz). The resulting approach outperforms previous systems (Di Liberto et al., 2015;Mirkovic et al., 2015) by a factor of seven in terms of binary decisions per minute. A relevance analysis of envelope reconstruction provided insight about temporal relations between stimulus and EEG activation (and implicitly location) and therefore indirectly about the temporal and spatial processing encoded by neural activity. Specifically, EEG activity around 170 ms was found to be relevant for reconstructing the envelope of the attended speaker. A back tracing of relevant activity to the electrode position showed relevant activity in physiologically plausible locations. This indicates that a relevance analysis of neural net models can provide insight into physiological processes involved in auditory attention.\n\nFig. 1 .\n1Illustration of the neural network (NN) training process. From the electroencephalography (EEG) training set (blue), a time series of predicted envelope samples is obtained by shifting the NN (green) over the training prediction window (red arrows). For each t 0 , a temporal context (typically 27 frames) is included. The cost function [mean-squared error (MSE) or correlation] is calculated and used to update the weights of the NN. This process is repeated until convergence.\n\nFig. 2 .\n2Decoding performance in terms of bits/minute, obtained from 350 evaluation cycles for the respective condition. Significant differences P < 0.001 are indicated by three and P < 0.05 by one asterisk, respectively. (a) Performance for narrowband-filtered (2-8 Hz) and broadband-filtered (1-32 Hz) data, respectively. (b) Performance for different training prediction window lengths with mean-squared error (MSE) cost function and broadband data. (c) Difference in performance between MSE and correlation cost function, respectively. Length of training prediction window is 16 samples with broadband filtering. (d) Comparison in performance of all unique conditions from (a), (b) and (c) over evaluation window length. The red line corresponds to the linear regression also used in previous studies(O'sullivan et al., 2014;Mirkovic et al., 2015).\n\nFig. 3 .\n3(a) Relevance matrix averaged over 350 evaluation cycles trained for the attended speaker with correlation as loss function. Channels sorted after averaging for ascending root-mean-square (RMS). Blue indicates low and red high relevance. (b) RMS calculated for each channel. Threshold defines the point of the 'elbow' where channels begin to carry distinct relevance. (c) Topographic plots of electrode relevance distribution from the two columns of (a) at 47 and 172 ms (first two topographic plots on the left) and averaged over all time samples of (a) (plot on the right). Electrode locations of the 10/20-system in the vicinity of the relevance clusters: TP9/TP7/T7 and TP10/TP8/T8. (d) Relevance of (a) averaged over all channels. (e) Relevance of (a) averaged over just the 12 channels with highest RMS. Number of channels is defined by b).\n\nFig. 4 .\n4Time lag relevance averaged over the 12 channels with highest RMS from the relevance matrix for the correlation loss function. Attended/ unattended refers to training on the attended/unattended envelope. For both conditions, correlation was used as cost function together with 16 samples training prediction window length.\n\u00a9 2017 Federation of European Neuroscience Societies and John Wiley & Sons Ltd European Journal of Neuroscience, 51, 1234-1241\nAcknowledgementsThis work was funded by the DFG (SFB/TRR 31 'The Active Auditory System', Research Unit FOR 1732 'Individualized Hearing Acoustics', Cluster of Excellence 1077/1 'Hearing4all'). The authors want to thank Bojana Mirkovic and Stefan Debener emphatically for sharing their data and for fruitful discussions.Conflict of interestNo potential conflict of interest was reported by the authors.Author contributionsTobias de Taillez and Bernd T. Meyer designed the study, developed the methodology and wrote the manuscript. Tobias de Taillez performed the experiments and did the data analysis. The study was supervised by Bernd T. Meyer and Birger Kollmeier. Birger Kollmeier contributed ideas both early in the study (general concept) as well as regarding experimental details such as the temporal segmentation of the EEG data.Data accessibilityDue to copyright issues, the underlying data set of EEG and audio data cannot be made available for public. The code for the NN is accessible on the following github repository: https://github.com/ tdetaillez/neural_networks_auditory_attention_decoding Abbreviations ECoG, electrocorticography; EEG, electroencephalography; MEG, magnetoencephalography; MSE, mean-squared error; NN, neural network; RMS, root-mean-squared.\nTensorflow: large-scale machine learning on heterogeneous distributed systems. M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, G S Corrado, A Davis, arXiv:1603.04467arXiv preprintAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Cor- rado, G.S., Davis, A. et al. (2016) Tensorflow: large-scale machine learn- ing on heterogeneous distributed systems. arXiv preprint arXiv:1603. 04467.\n\nHuman cortical responses to the speech envelope. S J Aiken, T W Picton, Ear Hearing. 29Aiken, S.J. & Picton, T.W. (2008) Human cortical responses to the speech envelope. Ear Hearing, 29, 139-157.\n\nRobust decoding of selective auditory attention from MEG in a competingspeaker environment via state-space modeling. S Akram, A Presacco, J Z Simon, S A Shamma, B Babadi, NeuroImage. 124Akram, S., Presacco, A., Simon, J.Z., Shamma, S.A. & Babadi, B. (2016) Robust decoding of selective auditory attention from MEG in a competing- speaker environment via state-space modeling. NeuroImage, 124, 906-917.\n\nOn pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. S Bach, A Binder, G Montavon, F Klauschen, K.-R M\u20ac Uller, W Samek, PLoS One. 10130140Bach, S., Binder, A., Montavon, G., Klauschen, F., M\u20ac uller, K.-R. & Samek, W. (2015) On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PLoS One, 10, e0130140.\n\nPractical recommendations for gradient-based training of deep architectures. Y Bengio, Neural Networks: Tricks of the Trade. Berlin, HeidelbergSpringerBengio, Y. (2012). Practical recommendations for gradient-based training of deep architectures. Neural Networks: Tricks of the Trade. Springer, Berlin, Heidelberg, pp. 437-478.\n\nGreedy layer-wise training of deep networks. Y Bengio, P Lamblin, D Popovici, H Larochelle, Advances in Neural Information Processing Systems. Cambridge, MAThe MIT PressBengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. (2007). Greedy layer-wise training of deep networks. Advances in Neural Information Pro- cessing Systems. The MIT Press, Cambridge, MA, pp. 153-160.\n\nAuditory-inspired speech envelope extraction methods for improved EEG-based auditory attention detection in a cocktail party scenario. W Biesmans, N Das, T Francart, A Bertrand, IEEE T. Neur. Sys. Reh. 25Biesmans, W., Das, N., Francart, T. & Bertrand, A. (2017) Auditory-inspired speech envelope extraction methods for improved EEG-based auditory attention detection in a cocktail party scenario. IEEE T. Neur. Sys. Reh., 25, 402-412.\n\nOverfitting in neural nets: backpropagation, conjugate gradient, and early stopping. R Caruana, S Lawrence, C L Giles, The MIT PressCambridge, MAAdvances in Neural Information Processing SystemsCaruana, R., Lawrence, S. & Giles, C.L. (2001). Overfitting in neural nets: back- propagation, conjugate gradient, and early stopping. Advances in Neural Infor- mation Processing Systems. The MIT Press, Cambridge, MA, pp. 402-408.\n\n. F Chollet, Github Keras, Chollet, F. (2015) Keras, GitHub, https://github.com/fchollet/keras\n\nHow about taking a low-cost, small, and wireless EEG for a walk? Psychophysiology. S Debener, F Minow, R Emkes, K Gandras, M Vos, 49Debener, S., Minow, F., Emkes, R., Gandras, K. & Vos, M. (2012) How about taking a low-cost, small, and wireless EEG for a walk? Psychophysi- ology, 49, 1617-1621.\n\nLow-frequency cortical entrainment to speech reflects phoneme-level processing. G M Di Liberto, J A O&apos;sullivan, E C Lalor, Curr. Biol. 25Di Liberto, G.M., O'Sullivan, J.A. & Lalor, E.C. (2015) Low-frequency cor- tical entrainment to speech reflects phoneme-level processing. Curr. Biol., 25, 2457-2465.\n\nNeural coding of continuous speech in auditory cortex during monaural and dichotic listening. N Ding, J Z Simon, J. Neurophysiol. 107Ding, N. & Simon, J.Z. (2012) Neural coding of continuous speech in audi- tory cortex during monaural and dichotic listening. J. Neurophysiol., 107, 78-89.\n\nHybrid speech recognition with deep bidirectional LSTM. Automatic Speech Recognition and Understanding (ASRU). A Graves, N Jaitly, A R Mohamed, IEEE Workshop. Graves, A., Jaitly, N. & Mohamed, A.R. (2013). Hybrid speech recognition with deep bidirectional LSTM. Automatic Speech Recognition and Under- standing (ASRU), 2013 IEEE Workshop on. IEEE, pp. 273-278.\n\nHandbook on Array Processing and Sensor Networks. S Haykin, K R Liu, John Wiley & SonsHoboken, NJHaykin, S. & Liu, K.R. (2010) Handbook on Array Processing and Sensor Networks. John Wiley & Sons, Hoboken, NJ.\n\nDeep neural networks for acoustic modeling in speech recognition: the shared views of four research groups. G Hinton, L Deng, D Yu, G E Dahl, A.-R Mohamed, N Jaitly, A Senior, V Vanhoucke, IEEE Signal Proc. Mag. 29Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-R., Jaitly, N., Senior, A., Vanhoucke, V. et al. (2012) Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups. IEEE Signal Proc. Mag., 29, 82-97.\n\nEnvelope responses in single-trial EEG indicate attended speaker in a 'cocktail party. C Horton, R Srinivasan, M D&apos;zmura, J. Neural Eng. 1146015Horton, C., Srinivasan, R. & D'Zmura, M. (2014) Envelope responses in sin- gle-trial EEG indicate attended speaker in a 'cocktail party'. J. Neural Eng., 11, 046015.\n\nThe ten twenty electrode system of the international federation. H H Jasper, Electroen. Clin. Neuro. 10Jasper, H.H. (1958) The ten twenty electrode system of the international fed- eration. Electroen. Clin. Neuro., 10, 371-375.\n\nDatabase of multichannel in-ear and behind-the-ear head-related and binaural room impulse responses. H Kayser, S D Ewert, J Anem\u20ac Uller, T Rohdenburg, V Hohmann, B Kollmeier, EURASIP J. Adv. Sig. Pr. 6Kayser, H., Ewert, S.D., Anem\u20ac uller, J., Rohdenburg, T., Hohmann, V. & Kollmeier, B. (2009) Database of multichannel in-ear and behind-the-ear head-related and binaural room impulse responses. EURASIP J. Adv. Sig. Pr., 2009, 6.\n\nAttentional gain control of ongoing cortical speech representations in a \"cocktail party. J R Kerlin, A J Shahin, L M Miller, J. Neurosci. 30Kerlin, J.R., Shahin, A.J. & Miller, L.M. (2010) Attentional gain control of ongoing cortical speech representations in a \"cocktail party\". J. Neurosci., 30, 620-628.\n\nSelective cortical representation of attended speaker in multi-talker speech perception. N Mesgarani, E F Chang, Nature. 485Mesgarani, N. & Chang, E.F. (2012) Selective cortical representation of attended speaker in multi-talker speech perception. Nature, 485, 233- 236.\n\nDecoding the attended speech stream with multi-channel EEG: implications for online, daily-life applications. B Mirkovic, S Debener, M Jaeger, M De Vos, J. Neural Eng. 1246007Mirkovic, B., Debener, S., Jaeger, M. & De Vos, M. (2015) Decoding the attended speech stream with multi-channel EEG: implications for online, daily-life applications. J. Neural Eng., 12, 046007.\n\nTarget speaker detection with concealed EEG around the ear. B Mirkovic, M G Bleichner, M De Vos, S Debener, Front. Neurosci. Switz. 10349Mirkovic, B., Bleichner, M.G., De Vos, M. & Debener, S. (2016) Target speaker detection with concealed EEG around the ear. Front. Neurosci. Switz., 10, 349.\n\nAttentional selection in a cocktail party environment can be decoded from single-trial EEG. J A O&apos;sullivan, A J Power, N Mesgarani, S Rajaram, J J Foxe, B G Shinn-Cunningham, M Slaney, S A Shamma, Cereb. Cortex. 25O'sullivan, J.A., Power, A.J., Mesgarani, N., Rajaram, S., Foxe, J.J., Shinn- Cunningham, B.G., Slaney, M., Shamma, S.A. et al. (2014) Attentional selection in a cocktail party environment can be decoded from single-trial EEG. Cereb. Cortex, 25, 1697-1706.\n\nAttention, probability, and task demands as determinants of P300 latency from auditory stimuli. J Polich, Electroen. Clin. Neuro. 63Polich, J. (1986) Attention, probability, and task demands as determinants of P300 latency from auditory stimuli. Electroen. Clin. Neuro., 63, 251-259.\n\nAt what time is the cocktail party? A late locus of selective attention to natural speech. A J Power, J J Foxe, E J Forde, R B Reilly, E C Lalor, Eur. J. Neurosci. 35Power, A.J., Foxe, J.J., Forde, E.J., Reilly, R.B. & Lalor, E.C. (2012) At what time is the cocktail party? A late locus of selective attention to natu- ral speech. Eur. J. Neurosci., 35, 1497-1503.\n\nSelective attention in normal and impaired hearing. B G Shinn-Cunningham, V Best, Trends Amplif. 12Shinn-Cunningham, B.G. & Best, V. (2008) Selective attention in normal and impaired hearing. Trends Amplif., 12, 283-299.\n\nPoststimulus EEG spectral analysis and P300: attention, task, and probability. K M Spencer, J Polich, Psychophysiology. 36Spencer, K.M. & Polich, J. (1999) Poststimulus EEG spectral analysis and P300: attention, task, and probability. Psychophysiology, 36, 220-232.\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G E Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, J. Mach. Learn. Res. 15Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. (2014) Dropout: a simple way to prevent neural networks from overfit- ting. J. Mach. Learn. Res., 15, 1929-1958.\n\nInterpretable deep neural networks for single-trial EEG classification. I Sturm, S Lapuschkin, W Samek, K.-R M\u20ac Uller, J. Neurosci. Meth. 274Sturm, I., Lapuschkin, S., Samek, W. & M\u20ac uller, K.-R. (2016) Interpretable deep neural networks for single-trial EEG classification. J. Neurosci. Meth., 274, 141-145.\n\nEEG-based communication: improved accuracy by response verification. J R Wolpaw, H Ramoser, D J Mcfarland, G Pfurtscheller, IEEE T. Rehabil. Eng. 6Wolpaw, J.R., Ramoser, H., McFarland, D.J. & Pfurtscheller, G. (1998) EEG-based communication: improved accuracy by response verification. IEEE T. Rehabil. Eng., 6, 326-333.\n", "annotations": {"author": "[{\"end\":241,\"start\":110},{\"end\":372,\"start\":242},{\"end\":500,\"start\":373}]", "publisher": null, "author_last_name": "[{\"end\":127,\"start\":117},{\"end\":258,\"start\":249},{\"end\":386,\"start\":381}]", "author_first_name": "[{\"end\":116,\"start\":110},{\"end\":248,\"start\":242},{\"end\":378,\"start\":373},{\"end\":380,\"start\":379}]", "author_affiliation": "[{\"end\":240,\"start\":129},{\"end\":371,\"start\":260},{\"end\":499,\"start\":388}]", "title": "[{\"end\":107,\"start\":1},{\"end\":607,\"start\":501}]", "venue": null, "abstract": "[{\"end\":2083,\"start\":693}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2257,\"start\":2232},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2348,\"start\":2317},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2531,\"start\":2511},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2821,\"start\":2801},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2858,\"start\":2844},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2881,\"start\":2858},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2964,\"start\":2944},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2984,\"start\":2964},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3299,\"start\":3278},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3440,\"start\":3419},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3807,\"start\":3785},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3831,\"start\":3807},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3853,\"start\":3831},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3875,\"start\":3853},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3920,\"start\":3900},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3971,\"start\":3947},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4083,\"start\":4060},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4243,\"start\":4221},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4301,\"start\":4279},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4325,\"start\":4301},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4349,\"start\":4325},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4370,\"start\":4349},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4393,\"start\":4370},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4415,\"start\":4393},{\"end\":5027,\"start\":5003},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5381,\"start\":5361},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5438,\"start\":5417},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5643,\"start\":5618},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5665,\"start\":5643},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6308,\"start\":6283},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6330,\"start\":6308},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6924,\"start\":6905},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6943,\"start\":6924},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7979,\"start\":7957},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8468,\"start\":8447},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9714,\"start\":9700},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10152,\"start\":10130},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10558,\"start\":10533},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10580,\"start\":10558},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11700,\"start\":11675},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11722,\"start\":11700},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12262,\"start\":12237},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12598,\"start\":12577},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12744,\"start\":12730},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12778,\"start\":12758},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13039,\"start\":13014},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14588,\"start\":14567},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14601,\"start\":14588},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14620,\"start\":14601},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16061,\"start\":16039},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16943,\"start\":16922},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17822,\"start\":17804},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20948,\"start\":20923},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20970,\"start\":20948},{\"end\":21234,\"start\":21206},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21725,\"start\":21700},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21747,\"start\":21725},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23801,\"start\":23778},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27170,\"start\":27145},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27192,\"start\":27170},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27270,\"start\":27247},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30386,\"start\":30361},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30407,\"start\":30386},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31722,\"start\":31700},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31946,\"start\":31926},{\"end\":32056,\"start\":32028},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33465,\"start\":33440},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33487,\"start\":33465},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":35511,\"start\":35486},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":35533,\"start\":35511}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34679,\"start\":34190},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35534,\"start\":34680},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36392,\"start\":35535},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36726,\"start\":36393}]", "paragraph": "[{\"end\":3972,\"start\":2099},{\"end\":4244,\"start\":3974},{\"end\":6435,\"start\":4246},{\"end\":7421,\"start\":6437},{\"end\":7887,\"start\":7423},{\"end\":8911,\"start\":7932},{\"end\":9266,\"start\":8928},{\"end\":11599,\"start\":9268},{\"end\":14708,\"start\":11628},{\"end\":14910,\"start\":14741},{\"end\":15291,\"start\":14981},{\"end\":16944,\"start\":15312},{\"end\":17583,\"start\":16999},{\"end\":18640,\"start\":17634},{\"end\":19543,\"start\":18726},{\"end\":20717,\"start\":19578},{\"end\":22659,\"start\":20719},{\"end\":24262,\"start\":22661},{\"end\":26163,\"start\":24304},{\"end\":26942,\"start\":26165},{\"end\":31270,\"start\":26980},{\"end\":32683,\"start\":31336},{\"end\":34189,\"start\":32698}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14740,\"start\":14709},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14980,\"start\":14911},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16998,\"start\":16945},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18725,\"start\":18641}]", "table_ref": null, "section_header": "[{\"end\":2097,\"start\":2085},{\"end\":7911,\"start\":7890},{\"end\":7930,\"start\":7914},{\"end\":8926,\"start\":8914},{\"end\":11626,\"start\":11602},{\"end\":15310,\"start\":15294},{\"end\":17632,\"start\":17586},{\"end\":19553,\"start\":19546},{\"end\":19576,\"start\":19556},{\"end\":24302,\"start\":24265},{\"end\":26955,\"start\":26945},{\"end\":26978,\"start\":26958},{\"end\":31334,\"start\":31273},{\"end\":32696,\"start\":32686},{\"end\":34199,\"start\":34191},{\"end\":34689,\"start\":34681},{\"end\":35544,\"start\":35536},{\"end\":36402,\"start\":36394}]", "table": null, "figure_caption": "[{\"end\":34679,\"start\":34201},{\"end\":35534,\"start\":34691},{\"end\":36392,\"start\":35546},{\"end\":36726,\"start\":36404}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12828,\"start\":12822},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19904,\"start\":19897},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20812,\"start\":20806},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21954,\"start\":21945},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22508,\"start\":22499},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22987,\"start\":22978},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23873,\"start\":23866},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25129,\"start\":25120},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25255,\"start\":25246},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25415,\"start\":25409},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25427,\"start\":25420},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25824,\"start\":25817},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26057,\"start\":26050},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26660,\"start\":26654},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26941,\"start\":26935},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27957,\"start\":27950},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29668,\"start\":29661},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30430,\"start\":30422},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32317,\"start\":32309}]", "bib_author_first_name": "[{\"end\":38210,\"start\":38209},{\"end\":38219,\"start\":38218},{\"end\":38230,\"start\":38229},{\"end\":38240,\"start\":38239},{\"end\":38250,\"start\":38249},{\"end\":38258,\"start\":38257},{\"end\":38267,\"start\":38266},{\"end\":38269,\"start\":38268},{\"end\":38280,\"start\":38279},{\"end\":38594,\"start\":38593},{\"end\":38596,\"start\":38595},{\"end\":38605,\"start\":38604},{\"end\":38607,\"start\":38606},{\"end\":38859,\"start\":38858},{\"end\":38868,\"start\":38867},{\"end\":38880,\"start\":38879},{\"end\":38882,\"start\":38881},{\"end\":38891,\"start\":38890},{\"end\":38893,\"start\":38892},{\"end\":38903,\"start\":38902},{\"end\":39245,\"start\":39244},{\"end\":39253,\"start\":39252},{\"end\":39263,\"start\":39262},{\"end\":39275,\"start\":39274},{\"end\":39291,\"start\":39287},{\"end\":39303,\"start\":39302},{\"end\":39618,\"start\":39617},{\"end\":39915,\"start\":39914},{\"end\":39925,\"start\":39924},{\"end\":39936,\"start\":39935},{\"end\":39948,\"start\":39947},{\"end\":40379,\"start\":40378},{\"end\":40391,\"start\":40390},{\"end\":40398,\"start\":40397},{\"end\":40410,\"start\":40409},{\"end\":40765,\"start\":40764},{\"end\":40776,\"start\":40775},{\"end\":40788,\"start\":40787},{\"end\":40790,\"start\":40789},{\"end\":41108,\"start\":41107},{\"end\":41124,\"start\":41118},{\"end\":41285,\"start\":41284},{\"end\":41296,\"start\":41295},{\"end\":41305,\"start\":41304},{\"end\":41314,\"start\":41313},{\"end\":41325,\"start\":41324},{\"end\":41579,\"start\":41578},{\"end\":41581,\"start\":41580},{\"end\":41595,\"start\":41594},{\"end\":41597,\"start\":41596},{\"end\":41616,\"start\":41615},{\"end\":41618,\"start\":41617},{\"end\":41902,\"start\":41901},{\"end\":41910,\"start\":41909},{\"end\":41912,\"start\":41911},{\"end\":42209,\"start\":42208},{\"end\":42219,\"start\":42218},{\"end\":42229,\"start\":42228},{\"end\":42231,\"start\":42230},{\"end\":42510,\"start\":42509},{\"end\":42520,\"start\":42519},{\"end\":42522,\"start\":42521},{\"end\":42778,\"start\":42777},{\"end\":42788,\"start\":42787},{\"end\":42796,\"start\":42795},{\"end\":42802,\"start\":42801},{\"end\":42804,\"start\":42803},{\"end\":42815,\"start\":42811},{\"end\":42826,\"start\":42825},{\"end\":42836,\"start\":42835},{\"end\":42846,\"start\":42845},{\"end\":43225,\"start\":43224},{\"end\":43235,\"start\":43234},{\"end\":43249,\"start\":43248},{\"end\":43519,\"start\":43518},{\"end\":43521,\"start\":43520},{\"end\":43784,\"start\":43783},{\"end\":43794,\"start\":43793},{\"end\":43796,\"start\":43795},{\"end\":43805,\"start\":43804},{\"end\":43820,\"start\":43819},{\"end\":43834,\"start\":43833},{\"end\":43845,\"start\":43844},{\"end\":44204,\"start\":44203},{\"end\":44206,\"start\":44205},{\"end\":44216,\"start\":44215},{\"end\":44218,\"start\":44217},{\"end\":44228,\"start\":44227},{\"end\":44230,\"start\":44229},{\"end\":44512,\"start\":44511},{\"end\":44525,\"start\":44524},{\"end\":44527,\"start\":44526},{\"end\":44805,\"start\":44804},{\"end\":44817,\"start\":44816},{\"end\":44828,\"start\":44827},{\"end\":44838,\"start\":44837},{\"end\":45127,\"start\":45126},{\"end\":45139,\"start\":45138},{\"end\":45141,\"start\":45140},{\"end\":45154,\"start\":45153},{\"end\":45164,\"start\":45163},{\"end\":45454,\"start\":45453},{\"end\":45456,\"start\":45455},{\"end\":45475,\"start\":45474},{\"end\":45477,\"start\":45476},{\"end\":45486,\"start\":45485},{\"end\":45499,\"start\":45498},{\"end\":45510,\"start\":45509},{\"end\":45512,\"start\":45511},{\"end\":45520,\"start\":45519},{\"end\":45522,\"start\":45521},{\"end\":45542,\"start\":45541},{\"end\":45552,\"start\":45551},{\"end\":45554,\"start\":45553},{\"end\":45935,\"start\":45934},{\"end\":46215,\"start\":46214},{\"end\":46217,\"start\":46216},{\"end\":46226,\"start\":46225},{\"end\":46228,\"start\":46227},{\"end\":46236,\"start\":46235},{\"end\":46238,\"start\":46237},{\"end\":46247,\"start\":46246},{\"end\":46249,\"start\":46248},{\"end\":46259,\"start\":46258},{\"end\":46261,\"start\":46260},{\"end\":46542,\"start\":46541},{\"end\":46544,\"start\":46543},{\"end\":46564,\"start\":46563},{\"end\":46791,\"start\":46790},{\"end\":46793,\"start\":46792},{\"end\":46804,\"start\":46803},{\"end\":47046,\"start\":47045},{\"end\":47060,\"start\":47059},{\"end\":47062,\"start\":47061},{\"end\":47072,\"start\":47071},{\"end\":47086,\"start\":47085},{\"end\":47099,\"start\":47098},{\"end\":47405,\"start\":47404},{\"end\":47414,\"start\":47413},{\"end\":47428,\"start\":47427},{\"end\":47440,\"start\":47436},{\"end\":47712,\"start\":47711},{\"end\":47714,\"start\":47713},{\"end\":47724,\"start\":47723},{\"end\":47735,\"start\":47734},{\"end\":47737,\"start\":47736},{\"end\":47750,\"start\":47749}]", "bib_author_last_name": "[{\"end\":38216,\"start\":38211},{\"end\":38227,\"start\":38220},{\"end\":38237,\"start\":38231},{\"end\":38247,\"start\":38241},{\"end\":38255,\"start\":38251},{\"end\":38264,\"start\":38259},{\"end\":38277,\"start\":38270},{\"end\":38286,\"start\":38281},{\"end\":38602,\"start\":38597},{\"end\":38614,\"start\":38608},{\"end\":38865,\"start\":38860},{\"end\":38877,\"start\":38869},{\"end\":38888,\"start\":38883},{\"end\":38900,\"start\":38894},{\"end\":38910,\"start\":38904},{\"end\":39250,\"start\":39246},{\"end\":39260,\"start\":39254},{\"end\":39272,\"start\":39264},{\"end\":39285,\"start\":39276},{\"end\":39300,\"start\":39292},{\"end\":39309,\"start\":39304},{\"end\":39625,\"start\":39619},{\"end\":39922,\"start\":39916},{\"end\":39933,\"start\":39926},{\"end\":39945,\"start\":39937},{\"end\":39959,\"start\":39949},{\"end\":40388,\"start\":40380},{\"end\":40395,\"start\":40392},{\"end\":40407,\"start\":40399},{\"end\":40419,\"start\":40411},{\"end\":40773,\"start\":40766},{\"end\":40785,\"start\":40777},{\"end\":40796,\"start\":40791},{\"end\":41116,\"start\":41109},{\"end\":41130,\"start\":41125},{\"end\":41293,\"start\":41286},{\"end\":41302,\"start\":41297},{\"end\":41311,\"start\":41306},{\"end\":41322,\"start\":41315},{\"end\":41329,\"start\":41326},{\"end\":41592,\"start\":41582},{\"end\":41613,\"start\":41598},{\"end\":41624,\"start\":41619},{\"end\":41907,\"start\":41903},{\"end\":41918,\"start\":41913},{\"end\":42216,\"start\":42210},{\"end\":42226,\"start\":42220},{\"end\":42239,\"start\":42232},{\"end\":42517,\"start\":42511},{\"end\":42526,\"start\":42523},{\"end\":42785,\"start\":42779},{\"end\":42793,\"start\":42789},{\"end\":42799,\"start\":42797},{\"end\":42809,\"start\":42805},{\"end\":42823,\"start\":42816},{\"end\":42833,\"start\":42827},{\"end\":42843,\"start\":42837},{\"end\":42856,\"start\":42847},{\"end\":43232,\"start\":43226},{\"end\":43246,\"start\":43236},{\"end\":43262,\"start\":43250},{\"end\":43528,\"start\":43522},{\"end\":43791,\"start\":43785},{\"end\":43802,\"start\":43797},{\"end\":43817,\"start\":43806},{\"end\":43831,\"start\":43821},{\"end\":43842,\"start\":43835},{\"end\":43855,\"start\":43846},{\"end\":44213,\"start\":44207},{\"end\":44225,\"start\":44219},{\"end\":44237,\"start\":44231},{\"end\":44522,\"start\":44513},{\"end\":44533,\"start\":44528},{\"end\":44814,\"start\":44806},{\"end\":44825,\"start\":44818},{\"end\":44835,\"start\":44829},{\"end\":44845,\"start\":44839},{\"end\":45136,\"start\":45128},{\"end\":45151,\"start\":45142},{\"end\":45161,\"start\":45155},{\"end\":45172,\"start\":45165},{\"end\":45472,\"start\":45457},{\"end\":45483,\"start\":45478},{\"end\":45496,\"start\":45487},{\"end\":45507,\"start\":45500},{\"end\":45517,\"start\":45513},{\"end\":45539,\"start\":45523},{\"end\":45549,\"start\":45543},{\"end\":45561,\"start\":45555},{\"end\":45942,\"start\":45936},{\"end\":46223,\"start\":46218},{\"end\":46233,\"start\":46229},{\"end\":46244,\"start\":46239},{\"end\":46256,\"start\":46250},{\"end\":46267,\"start\":46262},{\"end\":46561,\"start\":46545},{\"end\":46569,\"start\":46565},{\"end\":46801,\"start\":46794},{\"end\":46811,\"start\":46805},{\"end\":47057,\"start\":47047},{\"end\":47069,\"start\":47063},{\"end\":47083,\"start\":47073},{\"end\":47096,\"start\":47087},{\"end\":47113,\"start\":47100},{\"end\":47411,\"start\":47406},{\"end\":47425,\"start\":47415},{\"end\":47434,\"start\":47429},{\"end\":47449,\"start\":47441},{\"end\":47721,\"start\":47715},{\"end\":47732,\"start\":47725},{\"end\":47747,\"start\":47738},{\"end\":47764,\"start\":47751}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b0\"},\"end\":38542,\"start\":38130},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":43037341},\"end\":38739,\"start\":38544},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14106822},\"end\":39142,\"start\":38741},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9327892},\"end\":39538,\"start\":39144},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10808461},\"end\":39867,\"start\":39540},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14201947},\"end\":40241,\"start\":39869},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":27835175},\"end\":40677,\"start\":40243},{\"attributes\":{\"id\":\"b7\"},\"end\":41103,\"start\":40679},{\"attributes\":{\"id\":\"b8\"},\"end\":41199,\"start\":41105},{\"attributes\":{\"id\":\"b9\"},\"end\":41496,\"start\":41201},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":15888974},\"end\":41805,\"start\":41498},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":682727},\"end\":42095,\"start\":41807},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3338763},\"end\":42457,\"start\":42097},{\"attributes\":{\"id\":\"b13\"},\"end\":42667,\"start\":42459},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206485943},\"end\":43135,\"start\":42669},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6940411},\"end\":43451,\"start\":43137},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":27778550},\"end\":43680,\"start\":43453},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10173983},\"end\":44111,\"start\":43682},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":24404227},\"end\":44420,\"start\":44113},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4320045},\"end\":44692,\"start\":44422},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":22076362},\"end\":45064,\"start\":44694},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5261720},\"end\":45359,\"start\":45066},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2934551},\"end\":45836,\"start\":45361},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4551693},\"end\":46121,\"start\":45838},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":16649688},\"end\":46487,\"start\":46123},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6543400},\"end\":46709,\"start\":46489},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":45624025},\"end\":46976,\"start\":46711},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6844431},\"end\":47330,\"start\":46978},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2707281},\"end\":47640,\"start\":47332},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":30017646},\"end\":47962,\"start\":47642}]", "bib_title": "[{\"end\":38591,\"start\":38544},{\"end\":38856,\"start\":38741},{\"end\":39242,\"start\":39144},{\"end\":39615,\"start\":39540},{\"end\":39912,\"start\":39869},{\"end\":40376,\"start\":40243},{\"end\":41576,\"start\":41498},{\"end\":41899,\"start\":41807},{\"end\":42206,\"start\":42097},{\"end\":42775,\"start\":42669},{\"end\":43222,\"start\":43137},{\"end\":43516,\"start\":43453},{\"end\":43781,\"start\":43682},{\"end\":44201,\"start\":44113},{\"end\":44509,\"start\":44422},{\"end\":44802,\"start\":44694},{\"end\":45124,\"start\":45066},{\"end\":45451,\"start\":45361},{\"end\":45932,\"start\":45838},{\"end\":46212,\"start\":46123},{\"end\":46539,\"start\":46489},{\"end\":46788,\"start\":46711},{\"end\":47043,\"start\":46978},{\"end\":47402,\"start\":47332},{\"end\":47709,\"start\":47642}]", "bib_author": "[{\"end\":38218,\"start\":38209},{\"end\":38229,\"start\":38218},{\"end\":38239,\"start\":38229},{\"end\":38249,\"start\":38239},{\"end\":38257,\"start\":38249},{\"end\":38266,\"start\":38257},{\"end\":38279,\"start\":38266},{\"end\":38288,\"start\":38279},{\"end\":38604,\"start\":38593},{\"end\":38616,\"start\":38604},{\"end\":38867,\"start\":38858},{\"end\":38879,\"start\":38867},{\"end\":38890,\"start\":38879},{\"end\":38902,\"start\":38890},{\"end\":38912,\"start\":38902},{\"end\":39252,\"start\":39244},{\"end\":39262,\"start\":39252},{\"end\":39274,\"start\":39262},{\"end\":39287,\"start\":39274},{\"end\":39302,\"start\":39287},{\"end\":39311,\"start\":39302},{\"end\":39627,\"start\":39617},{\"end\":39924,\"start\":39914},{\"end\":39935,\"start\":39924},{\"end\":39947,\"start\":39935},{\"end\":39961,\"start\":39947},{\"end\":40390,\"start\":40378},{\"end\":40397,\"start\":40390},{\"end\":40409,\"start\":40397},{\"end\":40421,\"start\":40409},{\"end\":40775,\"start\":40764},{\"end\":40787,\"start\":40775},{\"end\":40798,\"start\":40787},{\"end\":41118,\"start\":41107},{\"end\":41132,\"start\":41118},{\"end\":41295,\"start\":41284},{\"end\":41304,\"start\":41295},{\"end\":41313,\"start\":41304},{\"end\":41324,\"start\":41313},{\"end\":41331,\"start\":41324},{\"end\":41594,\"start\":41578},{\"end\":41615,\"start\":41594},{\"end\":41626,\"start\":41615},{\"end\":41909,\"start\":41901},{\"end\":41920,\"start\":41909},{\"end\":42218,\"start\":42208},{\"end\":42228,\"start\":42218},{\"end\":42241,\"start\":42228},{\"end\":42519,\"start\":42509},{\"end\":42528,\"start\":42519},{\"end\":42787,\"start\":42777},{\"end\":42795,\"start\":42787},{\"end\":42801,\"start\":42795},{\"end\":42811,\"start\":42801},{\"end\":42825,\"start\":42811},{\"end\":42835,\"start\":42825},{\"end\":42845,\"start\":42835},{\"end\":42858,\"start\":42845},{\"end\":43234,\"start\":43224},{\"end\":43248,\"start\":43234},{\"end\":43264,\"start\":43248},{\"end\":43530,\"start\":43518},{\"end\":43793,\"start\":43783},{\"end\":43804,\"start\":43793},{\"end\":43819,\"start\":43804},{\"end\":43833,\"start\":43819},{\"end\":43844,\"start\":43833},{\"end\":43857,\"start\":43844},{\"end\":44215,\"start\":44203},{\"end\":44227,\"start\":44215},{\"end\":44239,\"start\":44227},{\"end\":44524,\"start\":44511},{\"end\":44535,\"start\":44524},{\"end\":44816,\"start\":44804},{\"end\":44827,\"start\":44816},{\"end\":44837,\"start\":44827},{\"end\":44847,\"start\":44837},{\"end\":45138,\"start\":45126},{\"end\":45153,\"start\":45138},{\"end\":45163,\"start\":45153},{\"end\":45174,\"start\":45163},{\"end\":45474,\"start\":45453},{\"end\":45485,\"start\":45474},{\"end\":45498,\"start\":45485},{\"end\":45509,\"start\":45498},{\"end\":45519,\"start\":45509},{\"end\":45541,\"start\":45519},{\"end\":45551,\"start\":45541},{\"end\":45563,\"start\":45551},{\"end\":45944,\"start\":45934},{\"end\":46225,\"start\":46214},{\"end\":46235,\"start\":46225},{\"end\":46246,\"start\":46235},{\"end\":46258,\"start\":46246},{\"end\":46269,\"start\":46258},{\"end\":46563,\"start\":46541},{\"end\":46571,\"start\":46563},{\"end\":46803,\"start\":46790},{\"end\":46813,\"start\":46803},{\"end\":47059,\"start\":47045},{\"end\":47071,\"start\":47059},{\"end\":47085,\"start\":47071},{\"end\":47098,\"start\":47085},{\"end\":47115,\"start\":47098},{\"end\":47413,\"start\":47404},{\"end\":47427,\"start\":47413},{\"end\":47436,\"start\":47427},{\"end\":47451,\"start\":47436},{\"end\":47723,\"start\":47711},{\"end\":47734,\"start\":47723},{\"end\":47749,\"start\":47734},{\"end\":47766,\"start\":47749}]", "bib_venue": "[{\"end\":38207,\"start\":38130},{\"end\":38627,\"start\":38616},{\"end\":38922,\"start\":38912},{\"end\":39319,\"start\":39311},{\"end\":39663,\"start\":39627},{\"end\":40010,\"start\":39961},{\"end\":40443,\"start\":40421},{\"end\":40762,\"start\":40679},{\"end\":41282,\"start\":41201},{\"end\":41636,\"start\":41626},{\"end\":41935,\"start\":41920},{\"end\":42254,\"start\":42241},{\"end\":42507,\"start\":42459},{\"end\":42879,\"start\":42858},{\"end\":43277,\"start\":43264},{\"end\":43552,\"start\":43530},{\"end\":43880,\"start\":43857},{\"end\":44250,\"start\":44239},{\"end\":44541,\"start\":44535},{\"end\":44860,\"start\":44847},{\"end\":45196,\"start\":45174},{\"end\":45576,\"start\":45563},{\"end\":45966,\"start\":45944},{\"end\":46285,\"start\":46269},{\"end\":46584,\"start\":46571},{\"end\":46829,\"start\":46813},{\"end\":47134,\"start\":47115},{\"end\":47468,\"start\":47451},{\"end\":47786,\"start\":47766},{\"end\":39683,\"start\":39665},{\"end\":40025,\"start\":40012}]"}}}, "year": 2023, "month": 12, "day": 17}