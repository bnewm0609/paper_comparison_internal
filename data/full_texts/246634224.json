{"id": 246634224, "updated": "2023-10-05 17:26:38.688", "metadata": {"title": "Causal Disentanglement for Semantics-Aware Intent Learning in Recommendation", "authors": "[{\"first\":\"Xiangmeng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Qian\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Dianer\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Peng\",\"last\":\"Cui\",\"middle\":[]},{\"first\":\"Zhichao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Guandong\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Traditional recommendation models trained on observational interaction data have generated large impacts in a wide range of applications, it faces bias problems that cover users' true intent and thus deteriorate the recommendation effectiveness. Existing methods tracks this problem as eliminating bias for the robust recommendation, e.g., by re-weighting training samples or learning disentangled representation. The disentangled representation methods as the state-of-the-art eliminate bias through revealing cause-effect of the bias generation. However, how to design the semantics-aware and unbiased representation for users true intents is largely unexplored. To bridge the gap, we are the first to propose an unbiased and semantics-aware disentanglement learning called CaDSI (Causal Disentanglement for Semantics-Aware Intent Learning) from a causal perspective. Particularly, CaDSI explicitly models the causal relations underlying recommendation task, and thus produces semantics-aware representations via disentangling users true intents aware of specific item context. Moreover, the causal intervention mechanism is designed to eliminate confounding bias stemmed from context information, which further to align the semantics-aware representation with users true intent. Extensive experiments and case studies both validate the robustness and interpretability of our proposed model.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2202.02576", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tkde/WangLYCWX23", "doi": "10.1109/tkde.2022.3159802"}}, "content": {"source": {"pdf_hash": "0d34b563c7e8688f7d777eead4a2451ba383d674", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2202.02576v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8c232db714a9bf32508d11b5e716192aa567b8ed", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0d34b563c7e8688f7d777eead4a2451ba383d674.txt", "contents": "\nCausal Disentanglement for Semantics-Aware Intent Learning in Recommendation\nAUGUST 2015 1\n\nJournal Of L A T E X Class \nFiles \nCausal Disentanglement for Semantics-Aware Intent Learning in Recommendation\n148AUGUST 2015 1Index Terms-Causal Disentanglement LearningSemantics-aware RepresentationCausal Intervention\nTraditional recommendation models trained on observational interaction data have generated large impacts in a wide range of applications, it faces bias problems that cover users' true intent and thus deteriorate the recommendation effectiveness. Existing methods tracks this problem as eliminating bias for the robust recommendation, e.g., by re-weighting training samples or learning disentangled representation. The disentangled representation methods as the state-of-the-art eliminate bias through revealing cause-effect of the bias generation. However, how to design the semantics-aware and unbiased representation for users true intents is largely unexplored. To bridge the gap, we are the first to propose an unbiased and semantics-aware disentanglement learning called CaDSI (Causal Disentanglement for Semantics-Aware Intent Learning) from a causal perspective. Particularly, CaDSI explicitly models the causal relations underlying recommendation task, and thus produces semantics-aware representations via disentangling users true intents aware of specific item context. Moreover, the causal intervention mechanism is designed to eliminate confounding bias stemmed from context information, which further to align the semantics-aware representation with users true intent. Extensive experiments and case studies both validate the robustness and interpretability of our proposed model.\n\nINTRODUCTION\n\nRecommender system (RS) has become a panacea for any scenario requiring personalized recommendations, to help users discover users' interested products from overwhelming alternatives. Early works mainly adopt collaborative filtering (CF) methods [1], [2] to model user preference on items, based on historical user-item interactions (e.g., ratings, clicks). However, such user-item interaction usually exhibits bias that is entangled with users' real interests, ignoring degrading the recommendation performance ultimately. For instance, in movie recommendation, users are more likely to watch movies that are watched by many people, which however is due to users' conformity to other people, rather than stemming from users' real interests [3], [4]. Therefore, it is essential to capture users' pure interests that are independent of the bias and thus can be leveraged to build high-quality recommender models.\n\nMost existing works on bias-aware recommendation can be attributed into two categories. The first category adopts a re-weighting strategies on the observed interaction samples, with the aim of imitating the scenario that samples are evenly distributed without bias [5], [6], [7], [8]. One major limitation of these methods is that they merely mitigate bias at the data level, but fail to answer the fundamental question: what are the root causes for bias amplification. Another category of approaches that aim to disentangle user true-interest via inspecting cause-effect of the bias generation for the robust recommendation, have recently gained much attention [4], [9], [10]. Particularly, these works usually design a specific causal graph attributing the bias to a confounder. For example, social network is in fact a confounder for exposure bias, since it influences both users' choice of movie watching and their ratings [9]. Apparently, the confounder introduces pseudo-intents, the ignorance of which definitely misguides the learning of user's true intent. A widely used solution of these works is to learn the user representation that is forced to be independent of the confounder, with the aim of uncovering the user's true intents for final downstream recommendations. Particularly, these works design a regularizer for the independence constraint via statistical measures like L 1 -inv, L 2 -inv [11] and distance correlation [8]. By explicitly disentangling the cause from a confounder, the representation uncovering user true intent can be learned for final recommendations.\n\nAlthough promising improvements have been observed, existing approaches on disentangling user intent from bias still suffer two limitations: First, most of them merely focus on user-item relationships, however the interaction data faces sparse issue in practical [12], [13], [14], leading to the arXiv:2202.02576v1 [cs.IR] 5 Feb 2022 difficulty of learning effective user or item representations. Moreover, existing disentangled learning methods for user intents merely treat one user-item interaction record as an independent instance and neglect its rich context information.\n\nWe claim that the rich context information in the form of heterogeneous information can help to disentangle and interpret semantics-aware intents of users for the robust recommendation. For instance, higher-order graph structure like a meta path User-Movie-Actor-Movie-User encodes the semantics interpretation of \"movies starring the same actor rated by the users\". In other words, without considering rich context information, disentangled learning fails in offering fine-grained interpretability in terms of item attributes for recommendation.\n\nTherefore, in this work, we propose to enhance user intent disentangled learning with heterogeneous information, which however is not trivial. The heterogeneous information is complicated and consists of various types of data, e.g., item attributes. The complexity in heterogeneous information, e.g., the fact that items grouped by attributes (e.g., brand) are frequently with skewed distributions, can bias the user preference and prediction score. The skewed distribution is attributed to missing values of aspects, i.e., the number of non-missing aspects is not evenly distributed in observational dataset. An empirical study conducted on Douban Movie dataset can validate this claim by Figure 1: unobserved Director aspect accounts for 19.7% of items compared to Actor accounting for 7.6%. That is, the skewed distribution of context aspect can easily bias the prediction model towards the majority group, even though their items have the same matching level (see example in Figure 1). Fig. 1: An example of bias: movie HP contains only one aspect Director, however, Actor and Type are missing. The high rating of user on movie HP trains prediction model towards the user's preference on the director Steve Kloves. In contrast, we observed that movie RWM with the same director received very low ratings from the same user.\n\nIn this work, we attempt to tackle the challenge from a novel causal perspective, with the aim of developing a unbiased and interpretable disentangled approach on heterogeneous information, named CaDSI (Causal Disentanglement for Semantics-Aware Intent Learning). To make users' intents semantics-aware, we propose a pre-trained model as a first stage to leverage multiple item facets of heterogeneous information. As a second stage, besides considering the items directly interacted with the user, the higher-order interacted items via meta paths are exploited to disentangle user intents in a robust manner. Finally, the pre-trained model together with disentangled learning is subsequently fine-tuned by the causal intervention. Thanks to the development of causal inference, the second module is designed to adopt causal intervention mechanism to eliminate the bias introduced by heterogeneous information. With these two stages, our method can guide the unbiased and semantics-aware representations disentangling user intents for the robust recommendation. Overall, the key contributions of this work are fourfold:\n\n\u2022 Fundamentally different from previous works, CaDSI is the first method that can disentangle the unbiased user's intents from a causal perspective, in the meanwhile endow each user intent with specific semantics under the disentanglement learning task. \u2022 We design a novel causal graph for the qualitative analysis of causal relationships in recommendation, based on which a pre-trained model is designed. With heterogeneous information, the pretrained model can semantically account for the item context influence towards the user intent.\n\n\u2022 To eliminate confounding bias stemmed from heterogeneous information, we perform the causal intervention on user representation and refine the pretrained model for unbiased user intent learning. \u2022 We conduct extensive experiments to show that our CaDSI method outperforms state-of-the-art methods. The interpretability of our CaDSI is also validated by our empirical study.\n\n\nPRELIMINARY AND PROBLEM FORMULATION\n\nIn this section, we will first present causal relations underlying the data generation mechanism of recommendation with access to the heterogeneous information. Following this, we prove the existence of confounder in the heterogeneous information and discuss the consequences for ignoring the context bias bought by the confounder. We then introduce the causal intervention and discuss how intervention via back-door adjustment can control the context bias from a causal perspective. Finally, we introduce the important concepts and notations used in our approach and give the formal definition of our problem to be solved.\n\n\nProblem definition\n\nWe formulate our task as disentangling and interpreting users' intent based on Heterogeneous Information Network (HIN). The important concepts of HIN and the formal definition of our problem are given as follows.\n\nDefinition 1 (Heterogeneous Information Network). A Heterogeneous Information Network (HIN) is denoted as G = (V, E) with a node type mapping function: \u03c6 : V \u2192 A and an edge type mapping function: \u03c8 : E \u2192 R, where A and R are the node type set and edge type set of G, respectively. Each node v \u2208 V and edge e \u2208 E in a HIN belongs to one particular type in node/edge type sets V/E with \u03c6(v) \u2208 A and \u03c8(e) \u2208 R, where |A| + |R| > 2.\n\nDefinition 2 (Meta Path). Meta path p is a path defined on the network schema T G = (A, R), and is denoted in the form of\np (A 1 R1 \u2192 A 2 R2 \u2192 ... R l \u2192 A o+1 )\nwhich defines a composite R = R 1 R 2 ...R o between type A 1 and A o+1 . For simplicity, we use node type names to denote the meta path if no multiple relations exist between type pairs, as p = (A 1 A 2 ...A o+1 ). Commonly, a HIN contains multiple meta paths, the meta path set is defined as P where each p \u2208 P.\n\nBased on these important concepts, we collect the key notations in Table 1 and formulate the problem to be solved as follows.\n\nDefinition 3 (Problem Definition). Given user and item sets, we define an interaction matrix y \u2208 R m\u00d7n where entry y ui = 1 indicates a user u in user set interacts with an item i in item set, otherwise y ui = 0. We also have additional contextual information about users and items, e.g., social relationships between users or item brands and categories, absorbing in G of Definition 1. Thus, we aim to learn the prediction function P parameterized by \u0398, such that\u0177 ui = P (u, i|y, G; \u0398), where\u0177 ui denotes the probability that user u will engage with item i conditional on the given y and G. We apply backdoor adjustment to remove the effect of confounder C for U , as indicated by the red cross.\n\n\nA Causal View on Recommendation\n\n\nStructural Causal Model\n\nTo illustrate the recommendation data generation mechanism seriously and soberly, we consider the structural causal model (SCM) [15] based on causality to reveal the true causal relations in recommendation. The basic idea of our proposed approach is to disentangle and interpret users' intent based on heterogeneous information. From a causal perspective, Figure 2 (a) demonstrates the illustrative causal graph that offers an interpretable representation for disentangled recommender system, which consists of four variables including {U, C, E, Y }. In particular, as a directed acyclic graph, it can describe the generation mechanism of recommendation results and guide the design of recommendation methods. In the following, we explain the rationality of this causal graph at a higher-level.\n\n\u2022 C as a confounder is the item aspects (e.g., movie genre) acquired from the heterogeneous information network. The representation of C can be learned by a pre-trained model of context information, which retains semantics information of item aspects.\n\n\u2022 U denotes user representation which essentially reveals k user intents. U is presented in the form of k chunked intent representation, where each chunk of representation reveals a piece of user intent, such as the user's special taste towards items' brand.\n\n\u2022 I is item representations and each I denotes the embedding of one item attribute (e.g. Genre).\n\n\u2022 E is the semantics-aware intent representation generated by the context information from C and the user representation U . E retains the information of the user intent towards different item aspects.\n\n\u2022 Y \u2208 [0, 1] is the recommendation probability for the user-item pair.\n\nThe directed edge represents the causal relation between two variables, in particular, the rationality of causal relations can be explained as follows.\n\n\u2022 C \u2192 U : The prior knowledge C of item aspects affect user representation U , which is reflected by the fact that users prefer the items who have particular attributes (e.g., brand).\n\n\u2022 (C, U ) \u2192 E: Item context C and user representation U consist of the semantics-aware user intent representation.\n\n\u2022 I \u2192 Y : item representation by I affects the recommendation probability Y .\n\n\u2022 U \u2192 Y : user's preference represented by U affects the recommendation probability Y .\n\n\u2022 U \u2192 E \u2192 Y : the recommendation probability of item could be high if the user shows interest in the context of the item, e.g., the item type rather than the item. For example, items whose type is \"lipstick\" are more likely to be purchased by the user u whose gender is female.\n\n\nAdjusting Confounding Bias via Intervention\n\nFrom this causal graph, the semantic knowledge C is a confounder between user representation U and recommendation outcome Y , since C is the common cause of U and Y by definitions in causal theory. The presence of confounder C leads to the spurious correlation between U and Y if we ignore to account its causal effect into modeling, which is equal to the estimation of P (Y | U ). In semantic knowledgeaware recommendation, the confounder C (i.e., semantic knowledge) leads directly to the misguided recommendation probability P (Y | U ) that is biased towards items that have dominant item attributes. For example, as illustrated in Figure 1, we expect that the rating prediction of RWM is caused by both of the three item attributes, but not only the dominant item attribute director which has a majority attribute popularity (i.e., the majority group). In the language of causal inference, the conventional correlation P (Y | U ) fails to capture the true causality between U and Y , because the prediction likelihood of Y is conditional on not only U , but also the spurious correlation via (1) C \u2192 U \u2192 Y , i.e., prior knowledge C determines the prediction likelihood through user representation U . For example, the undesirable and low-quality items in the specific attribute group will not attract users' intent, degrading recommendation accuracy.\n\n(2) C \u2192 E \u2192 Y . i.e., the semantic-ware user intent representation E derived from C affects the prediction Y . Once users' future interest in item attribute groups changes (i.e., user interest drift), the recommendations will be biased.\n\nTo pursue the true causality between U and Y , we should propose a causal intervention method P (Y | do(U )) to remove the confounding bias from C. The do(\u00b7) operation [15] is to forcibly and externally assign a certain value to the variable U , which can be intuitively seen as    a meta path in P y \u2208 R m\u00d7n user item interaction matrix y ui predicted interaction likelihood of user u and item i cu semantics-aware embedding for user u c i semantics-aware embedding for item i ca context information embedding for aspect a k user intent number l iteration number of graph disentangling module L L-th layer of graph disentangling module S k (u, i) intent score of u and i on intent k u u intent-aware embedding for u i i embedding for item i removing the edge C \u2192 U and blocking the effect of C on U (as shown in Figure 2 (b)). As the result, the prediction likelihood can be independent of its causes, so as to generate better recommendation performance that is free from the confounding bias.\n\n\nOUR METHOD\n\nIn this section, we first introduce motivation and the overall architecture of the proposed model, which includes Causal Disentanglement Model and Causal Intervention as shown in Figure 3. We then present the details of each component and how they are applied to top-N recommendation.\n\n\nPre-trained Model for Learning Context Information\n\nThe pre-trained model for learning context representation C is a key component in the causal disentanglement model, which aims to leverage side-information in the given HIN and construct expressive representations for users, items and aspects directly. Specifically, Given a HIN G = (V, E) and its corresponding meta paths set P, we aim to learn semantics-aware representation (a.k.a., embedding) c u for each user node (i.e, User type) that represents the semanticaware embedding for user u \u2208 V, c i for each item node (i.e, Item type) that represents the semantic-aware embedding for item i \u2208 V and c a for each aspect node a \u2208 V that represents context information representation of a specific type of aspect a (e.g., Director). Towards this, a Heterogeneous Skip-Gram with Meta Path Based Random Walks is designed to output a set of multinomial distributions, while each distribution corresponding to one type of node (i.e, User, Item and Aspect type); the Meta Path Based Random Walks is used to generate node sequences that capture the complex semantics reflected in a Heterogeneous Information Network (HIN), while Heterogeneous Skip-Gram takes the generated node sequences as inputs and catches the heterogeneous neighborhood of a node for outputting the semantics-aware embeddings. Finally, the semantics-aware embeddings for users, items and aspects are given by aggregating every node representation under different meta paths by an Embedding Fusion operation.\n\n\nMeta Path Based Random Walks\n\nTo generate node sequences that are able to capture both the semantics and structural correlations between different types of nodes. The Meta Path Based Random Walks [16] is proposed to generate the node sequences traversed by random walkers over a HIN. The basic idea is to put random walkers [17] in a HIN to generate paths that constitute multiple types of nodes. Specifically, given G = (V, E, A, R, \u03c6, \u03c8), the node sequence n p = {v 1 , \u00b7 \u00b7 \u00b7 , v i+1 } under a specific meta path p is generated according to the following distribution:\nP (v i+1 | v i , p) = \uf8f1 \uf8f2 \uf8f3 1 N (A o+1 ) v i , (v i , v i+1 ) \u2208 E and \u03c6(v i+1 ) = A o+1 0, otherwise (1) where N (Ao+1) vi\nis the first-order neighbor set for node v i whose type is A o+1 ; v i+1 is the i + 1-th node whose type is A o+1 , and v i is the i-th node in the walk which belongs to type A o . By regulating v i \u2208 A o while v i+1 \u2208 A o+1 , the node types sampled by random walkers is conditioned on the pre-defined meta path p.\n\nFollowing the pre-defined meta paths in Table 3, by performing the Meta Path Based Random Walks on each meta path p \u2208 P, we can obtain node sequences set for all meta paths as n P = {n 1 , \u00b7 \u00b7 \u00b7 , n |P| }. As we care about sematicaware embeddings for users, items and aspects, we then select meta paths p starting with user, item or Aspect type and reorganize its corresponding node sequences n p into user type-specific set n(U ) as n(U ) = {n 1 , \u00b7 \u00b7 \u00b7 , n m }, item type-specific set n(I) as n(I) = {n 1 , \u00b7 \u00b7 \u00b7 , n n } and aspect type-specific set n(A) = {n 1 , \u00b7 \u00b7 \u00b7 , n h }. We then use Heterogeneous Skip-Gram to generate semantics-aware embeddings of node sequences in n(U ), n(I) and n(A).\n\n\nHeterogeneous Skip-Gram\n\nBased on node sequences in set n(U ), n(I) and n(A), we aim to leverage Heterogeneous Skip-Gram [16] to learn node representations c u and c i and c a , which represent semantics-aware user, item and aspect representations of a specific node sequence n i in n(U ), n(I) and n(A), respectively. For concise purpose, we only present the learning process of the node representation c u , and analogously, we can obtain the representations of c i and c a .\n\nSpecifically, a Heterogeneous Skip-Gram is designed to learn node representations by aggregating the heterogeneous neighborhood of the node in node sequence, while optimized through a node type-specific negative sampling [16]. Given each node sequence n i in n(U ) generated from Eq. (1), the Skip-Gram model learns the semanticsaware embedding c u of n i by maximizing the probability of having the heterogeneous context N u given a node u as follows:\nL \u03b8 = u\u2208V uc\u2208N A i u A i \u2208A \u03c3 cu T cu c W w=1 \u03c3 cu T cw ; \u03b8(2)\nwhere N Ai u denotes u's neighborhood whose type is A i , u c is one node in the neighborhood set N u of u, c u and c uc are latent vectors that correspond to the target node and context node representations of u and u c , and \u03c3(x) = 1/1 + exp(\u2212x). W is a parameter that determines the number of negative examples to be drawn per a positive example, c w is the sampled node's representation within W negative samples and \u03b8 is the model parameters of Heterogeneous Skip-Gram. Finally, c u for each node sequence n i in n(U ) are estimated by applying gradient descent algorithm [18] with respect to the objective in Eq. (2).\n\n\nEmbedding Fusion\n\nSince we have multiple node sequences in n(U ), n(I) and n(A), while each learned representation c u , c i and c a is the semantic-aware embedding of each node sequence n i in n(U ), n(I) and n(A) respectively, we therefore perform embedding fusion to aggregate every representations c u , c i and c a into an uniform manner categorized by their node types so as to guide the recommendation task. The reason why we fuse the individual embedding of each node sequence is quite straightforward. Firstly, in recommendation system, the optimization goal is to learn effective representations for users and items. Hence, it requires a principled fusion way to transform node embeddings w.r.t. different meta paths relating user type or item type into a more suitable form for later recommendation tasks. Secondly, the context information across meta paths starting with an aspect type should be further arranged into an uniform embedding space, representing one piece of semantics meaning, e.g., the aspect of the object been \"Director\".\n\nThe embedding fusion is implemented as a liner combination function defined as follows:\nc u \u2190 1 |c u (U )| |cu(U )| j=1 M \u00b7 c j u + b c i \u2190 1 |c i (I)| |ci(I)| j=1 M \u00b7 c j i + b c a \u2190 1 |c a (A)| |ca(A)| j=1 M \u00b7 c j a + b(3)\nwhere c u (U ) is the user node representation set which absorb the node representations of user u and c u (U ) = {c 1 u , \u00b7 \u00b7 \u00b7 , c m u }. Correspondingly, the item and aspect node representation sets c i (I) = {c 1 i , \u00b7 \u00b7 \u00b7 , c n i } and c a (A) = {c 1 a , \u00b7 \u00b7 \u00b7 , c h a } are established for item i and aspect a. M is a linear combination transformation matrix [19] and b is the error term. Through Eq. (3), c u , c i and c a can be learned as final semantics-aware representations for a user u and an item i and context information representation for aspect a, respectively.\n\n\nDisentanglement Learning for User Intent\n\nInspired by recent achievements on GNNs [12], [13], [20], [21], we propose a GNN-based disentangling module to learn user representations that can essentially reveal k user intents. Specifically, the L-layer disentangling module exploits the high-order connectivities among user-item interaction graph and initializes intent-ware embeddings by separating each user/item embedding into k chunks. Then, an interactive update rule that computes the importance scores of intent-aware user-item interactions is designed to refine intent-aware embeddings, so as to disentangle the holistic interaction graph into k intent-ware sub-graphs. Thereafter, each intent-aware embedding chunk are stacked by embedding propagation in the current layer, serving as the holistic intent-aware embedding u u and i i for user u and item i, where each intent-aware embedding u u and i i is composed of k independent chunks:\nu u = [u u u u k = u u k (1) + \u00b7 \u00b7 \u00b7 + u u k (L), i i k = i i k (1) + \u00b7 \u00b7 \u00b7 + i i k (L) (5)\nThe detailed operations are given as follows.\n\n\nInitialization\n\nAs ID embedding captures intrinsic characteristics of users, we separate the ID embeddings of user u into k chunks and associate each chunk with a latent intent, serving as the initialization of the intent-aware embeddings u u in Eq. (4):\nx = [x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x k ](6)\nThereafter, we initialize the importance score S k (u, i). The S k (u, i) is the importance score of the interaction between u and i with respect to the k-th intent. Such S k (u, i) can be seen as the indicator of whether u should interact with i under a specific intent k. Thus, by learn S k (u, i) for aspect k \u2208 {1, \u00b7 \u00b7 \u00b7 , k}, we can construct an intent-aware sub-graph at intent k. Such that, the embedding propagation can output intent-aware representation at intent k for all users based on the corresponding intent-aware subgraph. We uniformly initialize importance score S k (u, i) as S k (u, i) = 1 k which presumes the equal contributions of intents at the start of modeling.\n\n\nIterative Update Rule\n\nAn iterative update rule is then designed to update the importance score S k (u, i) of user-item connections under aspect k within l iterations, so as to disentangle interaction sub-graph at intent k to refine each intent-aware embedding chunk x k in Eq. (6). Note that x k \u2208 x in Eq. (6) serves as the initialized representation chunk of u u k \u2208 u u in Eq. (4) and is used to memorize the update value during iteration, the final x k is assigned to each u u k as the final intent-aware representation chunk of user u.\n\nIn particular, we set l iterations in the interactive update. At each iteration, for the target interaction (u, i), we firstly normalize the score vector S k (u, i) | \u2200k \u2208 {1, \u00b7 \u00b7 \u00b7 , k}} over all intents intoS k through a softmax function:\nS l k (u, i) = exp S l k (u, i) k k =1 exp S l k (u, i)(7)\nwhich is capable of illustrating which intents should get more attention to explain each user behavior (u, i). We then perform embedding propagation over individual intentaware graphs whose representation is denoted by x k , i i k and its adjacency matrix is denoted byS k in Eq. (7), such that the information of all individual intent-aware graphs are encoded into the learned representations. The weighted sum aggregator is defined as:\nx l k = i\u2208NuS l k (u, i) D l k (u) \u00b7 D l k (i) \u00b7 i i k (8) where D l k (u) = i \u2208NuS l k (u, i ) and D l k (i) = u \u2208NiS l k (u , i)\nare the degrees of user u and item i, respectively.\n\nThen we interactively update the intent-aware graphs. Intuitively, historical items of users driven by the same intent tend to have the similar chunked representations, such goal can be achieved by encouraging users and items among the same intent to have stronger relationships. We hence iteratively update the interaction importance score S l k (u, i) in order to strengthen the degree between the centroid u and its neighbor i under intent k, as follows:\nS l+1 k (u, i) = S l k (u, i) + x l k tanh i i k (9)\nwhere x l k tanh i i k considers the affinity between x l k and i i k , and tanh [22] is a nonlinear activation function to increase the representation ability of model.\n\nAfter l iterations, u u (1) = x l (1) for user u is obtained in the current layer, where each chuncked representation u u k (1) = x l k (1) denotes the chunked embedding of u u (1) on the intent k corresponds to the k-th dimension of Eq. (4).\n\n\nLayer Combination\n\nTo explore high-order connectivity between users and items, we recursively formulate the representation of L-th layer as:\nu u k (L) = g x l k (L \u2212 1), i i k (L \u2212 1) | i \u2208 N u(10)\nwhere u u k (L) and i i k (L) are the representations of user u and item i on the k-th intent at layer L,\nx l k (L \u2212 1) is the chunked embedding of k-th intent at L \u2212 1-th layer of u u k (L \u2212 1). Note that g(\u00b7)\nis a fully connection layer that memorizes the information propagated from the (L \u2212 1)-order neighbors of u.\n\nFinally, after L layers, we sum up intent-aware representations at different layers as the final representation of the k-th chunk of Eq. (4), as u u k = u u k (1) + \u00b7 \u00b7 \u00b7 + u u k (L), where u u k donates the intent-aware representation for user u at intent k. Analogously, we can establish the final intentaware embedding chunk i i k follow the definition in Eq. (5).\n\n\nSemantics-aware Intent Learning\n\nHaving obtained the intent-aware embeddings u u , i i from disentanglement learning for User Intent, to provide semantics information to the recommendation task, our method takes semantics factors c u and c i from Pre-trained Model for Context Information as the auxiliary input. To facilitate the usage of semantics factors, we design an operator to instantiate a semantics-aware intent representation e, which denotes the user intent towards different aspects. By learning e, the effect of semantics factors towards user intent can be incorporated into the final updated user representation u u , such that u u can be easily plugged into the backdoor adjustment to alleviate bias. Specifically, a second-order Factorization Machine (FM) [23] module is used to instantiate e:\ne = d a=1 d b=1 u u a c ia c u b i i b(11)\nwhere denotes the element-wise product between each latent vector, such that the learned e captures the interactions between the intent-aware representation u u /i i and semantics factors in c u /c i .\n\nNext, the semantics-aware intent representation e can be incorporated into recommender models as one additional user representation. Formally, we use the collaborative filtering to calculate the prediction score\u0177 ui given user and item ID representations, as follows:\ny ui = f (u, i, e) = \u03b4u i + (1 \u2212 \u03b4)e i(12)\nwhere u and i are the ID embeddings given by id mapping techniques such as Multi-OneHot [24], and \u03b4 is the coefficient that describes how much each component contributes to the prediction score. Then we use the pairwise BPR loss [25] to optimize the model parameters \u0398. Specifically, BPR loss encourages the prediction of a user's historical items to be higher than those of unobserved items:\nL BPR = (u,i,j)\u2208O \u2212 ln \u03c3 \u0177 ui \u2212\u0177 uj + \u03bb \u0398 2 2(13)\nwhere u, i and j are the ID embeddings of user u, item i and \nitem j, O = {(u, i, j) | (u, i) \u2208 O + , (u, j) \u2208 O \u2212 } denotes\n\nCausal Intervention for Debiasing\n\nThe context information as a confounder tends to introduce bad effect on both user representation and prediction score.\n\nTo make context information beneficial for semantics-aware intent learning, we resort to causal technique, backdoor adjustment [15] to adjust the representation mechanism in disentangled causal model. By doing this, we aim to produce unbiased user representation and apply the modified one to recommendation task.\n\n\nBackdoor Adjustment\n\nNote that previous recommendation methods build a predictive model P (Y | U ) from the passively collected interaction dataset, which neglects the effect of confounder C, thus leading to a spurious correlation between users and items. The spurious correlation is harmful to most users because the items in the majority group are likely to dominate the recommendation list and narrow down the user interests.\n\nAccording to the theory of backdoor adjustment [15], the target of our method is to remove the bad effect of context information C on user representation U . Instead of P (Y | U ), we formulate the predictive model as P (Y |do(U = u)) to account for the effect of confounder. Based on the graph in Figure. 2, we first need to formulate the causal effect between variables by causal intervention, which is denoted as the do(\u00b7) operation [15]. do(U = u) is to forcibly and externally assign a certain value to the variable U , which can be intuitively seen as removing the edge C \u2192 U and blocking the effect of C on U , making its value independent of its causes (cf. Figure. 2). By applying do operation, we can estimate the effect of C on Y as\nP (Y |do(U = u)) \u2212 P (Y |do(U = 0))(14)\nwhere P (Y |do(U = 0)) denotes the null intervention, e.g., the baseline compared to U = u. In the physical world, P (Y |do(U = u)) corresponds to actively manipulating the aspects or attributes in the item.\n\nOur implementation is inspired from two inherent properties of any Heterogeneous Network Embedding method (e.g., metapath2vec++ [16]). First, by passing a meta-path into the pre-trained context model from Eq. (3), we have the context embedding for an aspect, denoted as c a . Aggregating context embeddings for all aspects into the aspect representation set C, we can obtain the unified aspect representation C, where each element of C, i.e., c a , representing one semantic aspect (e.g., \"Director\") is computed by Eq. (3). As C is no longer correlated with u u by removing the edge C \u2192 U , the causal intervention makes u u have a fair opportunity to incorporate every context c a into the prediction of\u0177 ui , subject to a prior P (C = c a ). Second, prevailing pre-trained models use a specific task (in our method is the semantics-aware intent learning in Section 3.3) as the objective, the representations trained from it can be considered as the distilled information u u that waits to adjust by do(U = u u C).\n\nBy far, we have the context information C for all aspects, where each c a \u2208 C represents context embedding of one aspect. We also have the refined intent representation u u that waits to be adjusted from Eq. (13) as U . Next, we will detail the proposed causal intervention by providing implementation for Eq. (14).\n\nThe overall backdoor adjustment is achieved through:\nP (Y | U, do(U = u)) \u2212 P (Y | do(U = 0)) = C (P (Y | do (U = u u C)) \u2212 P (Y | do (U = 0))) P (C = ca) = 1 N N i=1 (P (\u0177 ui | u u C) \u2212 P (\u0177 ui | u u ))(15)\nwhere each component in Eq. (15) is designed by:\n\n\u2022 C = c a indicates the confounder C is set as one context embedding c a .\n\n\u2022 P (Y |U, do(U = u)) = P (\u0177 ui | u u C). The selected context embedding C is concatenated with u u by the element-wise product , which serves as the adjustment value for the prediction of\u0177 ui .\n\n\u2022 P (C = c a ) is the prior distribution of different item aspect, by defining P (C = c a ) = 1/N , we can assume a uniform prior for the adjusted features, where N is the total number of aspect type.\n\nWe then utilize an inference strategy to adaptively fuse the prediction scores from the P (\u0177 ui | u u C) and P (\u0177 ui | u u ). Specifically, we first train the recommender model by P (Y | do (\u0177 ui = u u C)) and P (Y | do (\u0177 ui = 0)) and obtain\u0177 C and\u0177, respectively. Then, the adjusted prediction scores\u0177 C and unadjusted prediction score\u0177 are automatically fused to regulate the impact of backdoor adjustment. We define a indicator function I a that determines whether to include c a into the user intent u u or not.\nI a := c a if tanh (\u0177 C \u2212\u0177) > 0 1 if tanh (\u0177 C \u2212\u0177) < 0(16)\nwhere tanh(\u0177 C \u2212\u0177) > 0 indicates that the backdoor adjustment leads a positive impact on recommendation result by considering the aspect c a . Otherwise, c a leads a negative impact, which should be removed from user intent representation. Based on Eq. (16), the semantic-aware user intent representation can be refined as follows:\ne a = u u I a(17)\nTo define the unbiased loss function for observation\u0177 ui , we aim to maximize the discrepancy between the final adjusted and the unadjusted representation u u under the guidance of e a , that is,\nL d = arg min \u03b8 (u,i,y ui )\u2208O (y ui , f (u, i, N a ea))(18)\nwhere u, i are the ID embedding for user u and item i, f (\u00b7) is defined in Eq. (12), and y ui is the ground-truth for user u and item i.\n\n\nOptimization\n\nOur model ultimately has three loss functions, i.e., L d of unbiased loss function for preference score estimation given in Eq. (18), L \u03b8 of Heterogeneous Skip-Gram model given in Eq. (2), and the BPR loss for preference score prediction given in Eq. (13). To this end, the objective function of our CaDSI method could be derived as:\nL = \u03bb d L d + \u03bb \u03b8 L \u03b8 + \u03bb z L BP R + R(\u2126)(19)\nwhere \u2126 represents the trainable parameters and R(\u00b7) is a squared L 2 norm regularization term on \u2126 to alleviate the overfitting problem. \u03bb d , \u03bb \u03b8 , \u03bb z are trade-off hyperparameters of the three separate loss functions respectively. During the training, we optimize the objective function in Eq. (19) using gradient descent algorithm such as Adam [26].\n\n\nEXPERIMENTS\n\nTo more thoroughly evaluate the proposed methd, experiments are conducted to answer the following research questions:\n\n\u2022 (RQ1) How confoundeing bias caused by the context information is manifested in real-world recommendation datasets?\n\n\u2022 (RQ2) How does our model perform compared with state-of-the-art models for top-N recommendation?\n\n\u2022 (RQ3) How does key components in our model impact the recommendation performance? (i.e., disentanglement learning task, causal intervention)? How do hyper-parameters in our model impact recommendation performance?\n\n\u2022 (RQ4) How does our model interprets user intents for recommendation?\n\nWe first present the experimental settings for good reproducibility, followed by answering the above four research questions.   \n\n\nExperimental Settings\n\n\nDatasets\n\nWe evaluate our model on three public accessible datasets for top-N recommendation. The statistics of the datasets are summarized in Table 2 and the selected meta paths for all data sets are shown in Table 3. To ensure the quality of all the datasets, we use the core settings, i.e., we transform explicit ratings into implicit data, where each interaction between the user and item is marked as 0 or 1 indicating whether the user has rated the item or not; we retaining users and items with at least five interactions and each user has at least five friends for both of the datasets. In the training phase, each observed user-item interaction is treated as a positive instance, while we use negative sampling to randomly sample an unobserved item and pair it with the user as a negative instance.\n\n\nBaselines\n\nTo demonstrate the effectiveness, we compare our model with four classes of methods: (I) conventional entangled CF methods; (II) graph-based entangled recommendation methods; (III) HIN enhanced entangled recommendation methods, which model user-item interaction with rich context information as HIN; (IV) disentangled recommendation methods, which disentangle user intents or item aspects with different mechanisms; (V) causal-based recommendation methods. Network (GCN) encoder to generate representations based on first-order connectivity.\n\n\u2022 NGCF [21] (II): This adopts three Graph Neural Network(GNN) layers to model at most third-order connectivity on the user-item interaction graph.\n\n\u2022 LightGCN [13] (II): This is a state-of-the-art graph-based recommendation method that learns user/item embeddings by linearly propagating them with neighborhood aggregation in the GCN component.\n\n\u2022 IF-BPR [28] (III): This method leverages meta path based social relations derived from a HIN, and proposes a social recommendation method that can capture the similarity of users for top-N recommendation.\n\n\u2022 MCRec [29] (III): This method leverages meta path based context with co-attention mechanism for top-N recommendation in HIN.\n\n\u2022 NeuACF [30] (IV): This method disentangles multiple aspects of users and items with a deep neural network for recommendation in HIN.\n\n\u2022 MacridVAE [8] (IV): This method disentangle user intents behind user behaviors, assuming that the coexistence of macro and micro latent factors affects user behaviors.\n\n\u2022 DGCF [31] (IV): This is a state-of-the-art CF-based disentangled recommendation method, which disentangles latent factors of user intents by the neighbor routing and embedding propagation.\n\n\u2022 DICE [11] (V): This is a state-of-the-art causal-based recommendation method, which aims at disentangling users' interest by controlling the conformity bias using causal embedding.\n\n\nEvaluation Metrics\n\nWe adopt two popular metrics: Recall@K and Normalized Discounted Cumulative Gain(NDCG)@K to evaluate the top-K recommendation performance of our model. K is set as 20 by default. In the inference phase, we view the historical items of a user in the test set as the positive, and evaluate how well these items are ranked higher than all unobserved ones. The average results w.r.t. the metrics over all users are reported.\n\n\nParameter Settings\n\nWe implement all baseline models and our proposed CaDSI model on a Linux server with Tesla P100 PCI-E 16GB GPU. For a fair comparison, datasets for implementing all models are split as train/test/validate set with a proportion of 80%/10%/10% of the dataset, while we optimize all models with Adam [26]. For a fair comparison, a grid search is conducted to choose the optimal parameter settings, e.g., dimension of user/item latent vector k M F for matrix factorization-based models and dimension of embedding vector d for neural network-based models. The embedding size is initialized with the Xavier [32] and \n\n\nUnderstanding Confounders (RQ1)\n\nWe initially conduct an experiment to understand to what extent the confounding bias exists in meta paths of realworld recommendation datasets. To this end, we aim to investigate the distribution of nodes among the same meta path. Intuitively, an unbiased HIN-based recommendation method should expect that, for a specific attribute, each user/item should hold an equal number of this attribute (i.e, interactions between nodes and attributes are likely to be evenly distributed). Thus, we investigate the confounding bias by analyzing the statistics of node-attribute interactions of meta paths in Douban Book. We randomly sample n = 100 books from Douban Book and extract their Author, Publisher, and Year attributes. By counting the connections between books and their attributes whose type belongs to Author, Publisher, and Year, respectively, we have the statistical results shown in Figure 4. The connected graphs in the left part of Figure 4 depict whether the book i connected with the selected attribute. The figures in the right part of Figure 4 shows the distributions of connected book numbers by a certain attribute, where the y-axis denotes the total amount of the connected books. Apparently, attributes and books exhibit an unevenly distribution regarding their interactions: the attributes in dataset are partially observed, leaving a larger number of attributes to be unobserved. For example, for Book-Author meta path, there are a lot of books that do not connect with any node whose type is Author, which means lots of author attributes of books are missing. In conventional recommendation methods, the missing pattern of such attributes is ignored by either regarding them as outliers and padding them with random values, or treating the missing attributes as negative feedback. Such measure would preserve the confoundings bought by node attributes, degrading the recommendation ultimately.\n\nAnother finding is that, the distribution for bookattribute connection numbers is significantly skewed, it displays a long-tail phenomenon: the green vertical line separates the top 50% of connection numbers by popularity -these connections outweigh another 50% long tail connections to the right. For instance, in Figure 4 (a), authors in the Book-Author relation cumulatively connect with 90% more books than the long tail authors to the right. For Book-Publisher meta path, ideally, Book-Publisher has the one-toone relation from book to the publisher, while every publisher has published an equal number of books. However, some publishers have published at most 510 books, while more than 90% of publishers only published fewer than 10 books. Such long-tail distribution can bias the users' interest on item aspects. i.e., recommendation methods tend to recommend those items that have the most frequent attribute, while users can only be exposed to those that are recommended.  \n\n\nPerformance Comparison (RQ2)\n\nWe compare the top-K recommendation performance of CaDSI with ten recommendation baselines on three datasets: MovieLens-HetRec, Douban Book and Douban Movie. Table 4 demonstrates the performance comparison and we have the following observations:\n\n\u2022 Our CaDSI consistently yields the best performance among all methods on three datasets. In particular, CaDSI improves over the strongest baselines w.r.t. Recall@20 by 23.5%, 22.7%, 13.6% , NDCG@20 by 11.9%, 18.8%, 3.8%, Recall@40 by 3.4%, 78.8%, 13.2% and NDCG@40 by 3.8%, 49.4%, 0.8% on MovieLens-HetRec, Douban Book and Douban Movie respectively. CaDSI outperforms all baseline methods on top-K recommendation task, which validates that the semantics-aware user intents representation can enhance the recommendation performance.\n\n\u2022 In virtue of user-item interaction graph and meta paths, GNN-based (GC-MC, NGCF and LightGCN) and HIN-based (IF-BPR, MCRec) recommendation methods can achieve better performance than conventional MF methods (NeuMF) in most cases. However, they ignore controlling the bias existing in the context information. On the contrary, our CaDSI adoptes a principled causal inference way to easing such confounding bias. So it outperforms GNN-based and HIN-based recommendation method on both of the datasets. For instance, our CaDSI outperforms the most competitive HIN-based recommender IF-BPR w.r.t. Recall@20/Recall@40 by 24.2%/5.2% and NDCG@20/NDCG@40 by 29.2%/6.8% on MovieLens-HetRec.\n\n\u2022 By performing unbiased disentanglement via semantics context, our CaDSI can infer user's potential interests of items. However, those user interests could not be well inferred from other disentangled recommendation methods (NeuACF, MacridVAE and DGCF).\n\n\u2022 Among the GNN-based (GC-MC, NGCF and LightGCN) and HIN-based recommenders (IF-BPR, MCRec ) recommenders and disentangled recommenders (NeuACF, MacridVAE and DGCF), causalbased disentangled method (DICE) serves as the strongest baseline in most cases. This justifies the effectiveness of easing the counfounding bias in context information when estimating disentangled users' interests. However, DICE performs worse than our CaDSI, as it ignores rich semantics information in HIN, and fails to ingest semantics aspects when disentangling user interests.\n\n\u2022 From movie recommendation datasets, we can find that the improvements on MovieLens-HetRec is bigger than that on Douban Movie. This is reasonable since Douban Movie is much more sparser than MovieLens-HetRec with sparsity rate 0.63% vs. 4.0%, respectively. However, our CaDSI has better performance than all baselines on Douban Movie, because it achieves unbiased evaluation on highorder connectivity and rich semantics. This indicates that CaDSI is robust to the very sparse dataset.\n\n\nStudy of CaDSI (RQ3)\n\nAblation studies on CaDSI are also conducted to investigate the rationality and effectiveness. Specifically, we first attempt to exploit how the disentangled learning and causal intervention affect our performance. Moreover, the stability of our approach's performance on top-K recommendation is validated as well.\n\nWe have one fixed parameter n = 140 (cf. Eq. (15)) which denotes the total number of causal intervention times. Three important hyperparameters k (cf. Eq. (4)), L (cf. Eq. (10)) and K (cf. Section 4.1.3) correspond to: the number of latent factors of user intents, the number of graph disentangling layers and the number of items in top-K recommendation list, respectively. Based on the hyperparameter setup in Section 4.1.4, for all questions listed above, we vary the value of one parameter while keeping the others unchanged.\n\n\nEffect of Disentanglement Learning\n\nThe intent number k controls the total amount of user intents considered in our model, larger k stands for more fine-grained disentangled user intents. To study the influence, we vary k in the range of {1, 2, 4, 8, 16} and show the corresponding performance comparison on MovieLens-HetRec Douban Book, Douban Movie in Figure 5. We have several observations. (a) (a) Recall@20 on MovieLens-HetRec, Douban Book and Douban Movie.\n\n(b) (b) NDCG@20 on MovieLens-HetRec, Douban Book and Douban Movie. \u2022 Increasing the intent number from 1 to 16 can significantly enhances the performance, while CaDSI performs the worst when k = 1. This indicates learning the disentanglement of user intents is effective to capture the real user preferences towards items instead of coupling all preference together.\n\n\n\u2022\n\nThe variations diverse across different datasets. For MovieLens-HetRec and Douban Movie, the performance of CaDSI increase steadily as the K value increases from 1 to 16, while the performance drops when k is set from 2 to 4 on Douban Book. One possible reason is that CaDSI should balance between too fine-grained disentangled intents and the adjustment from causal intervention, such balancing learning is more obvious when dataset size is lager.\n\n\nEffect of Causal Intervention\n\nTo investigate whether CaDSI can get benefit from causal intervention, we study the performance of CaDSI by varying the iterations of causal intervention. Figure 6 summarizes the experimental results w.r.t. MovieLens-HetRec Douban Book, Douban Movie and we have the following observations:\n\n\u2022 Clearly, the causal intervention mechanism renders our CaDSI a better recommendation performance:\n\n(a) (a) MovieLens-HetRec.\n(b) (b) Douban Book.\n(c) (c) Douban Movie. more iterations of causal intervention lead to the better recommendation performance before saturation on all datasets, e.g., the Recall@20 and NDCG@20 values generally increase along with training iterations in Figure 6.\n\n\u2022 When training iterations reach to 130, 70 and 110 for MovieLens-HetRec, Douban Book and Douban Movie, respectively, the performance becomes relatively stable. Moreover, Douban Book requires less iteration times than the other two datasets. Intuitively, purchasing books is a much more simple behavior than choosing movies. Thus the user intents on book aspects are less diverse, leading to a quick convergence to the optimal interventional representations.\n\n\u2022 Some fluctuations appear in the iteration process, especially on MovieLens-HetRec dataset. The size of MovieLens-HetRec is much smaller than the other two datasets, thus leading to an instability to intervention process due to the data sparsity. However, when carrying more iterations, small-size datasets such as MovieLens-HetRec can also yield satisfying results.\n\n\nEffect of Multi-order Connectivity\n\nSince CaDSI is benefited from the higher-order connectivity between complex interactions and context information, we investigate how connectivity degrees affect CaDSI. Specifically, we search the graph disentangling layer number L in the range of {1, 2, 3}, which correspond to first-order connectivity, second-order connectivity and third-order connectivity, respectively. We show the performance comparison in Table 5 and below are our observations. More graph disentangling layers will collect more information form multi-hop neighbors from a holistic user-item interaction graph. Clearly, the performance of our CaDSI with layer number L = 2 is better than that with L = 1, since the second-order connectivity can capture significant collaborative signals with respect to users and items.\n\n\u2022 When stacking more than 2 layers, the influence of multi-hop neighbors is small and the recommendation performance is degraded. This is reasonable since the informative signals of user-item interactions might introduce additional noises to the representation learning. This again emphasizes the importance of controlling the bias bought by context information.\n\n\nTop-K Recommendation Performance\n\nBased on the evaluation on Recall@K and NDCG@K, Figure 7 shows that CaDSI achieves the stable performance on top-K recommendation when K (i.e., the length of ranking list) varies from 10 to 80. This indicates that our CaDSI performs stably on top-K recommendation task and can recommend more relevant items within top-K positions when the ranking list length increases.\n\n(a) (a) MovieLens-HetRec.\n(b) (b) Douban Book.\n(c) (c) Douban Movie. \n\n\nCase Study and Visualization(RQ4)\n\nWe conduct experiments to get deep insights into the disentangled representations w.r.t. the disentanglement of the semantics of user intents, the representability and interpretability of the learned disentangled embedding. The case studies towards the disentanglement of the semantics of user intents are shown in Appendix A, the visualization result of the learned disentangled embedding is shown in Appendix B.\n\n\nRELATED WORK\n\nIn this section, we will introduce previous works related to ours from the following three aspects, including HIN enhanced representation, disentangled representation and causal inference for recommendation.\n\n\nHIN Enhanced Representation\n\nAs a newly emerging direction, heterogeneous information network [14] is proved to be effective in modeling complex objects and providing rich semantics information to recommender systems [14], [33]. Many HIN-based recommendation methods achieve the-state-of-the-art performance [14]. For example, HeteMF [34] utilizes meta path based similarities as regularization terms in the MF model. HeteRec [35] learns meta path based latent features based on different types of entity relationships and proposes an enhanced personalized recommendation framework. SemRec [36] proposes a weighted HIN and designs a meta path based CF model to flexibly integrate heterogeneous information for a personalized recommendation. The effectiveness of HIN has been proved by a vast amount of HIN-based recommendation methods [14], thus, in our work, we value HIN in providing rich semantics information of user and item types. Despite the effectiveness, neither the enhanced graph-based nor HIN-based representations can disentangle users' intents by just presuming a uniform entangled embeddings behind behaviors. This can result in the poor interpretability of the developed recommendation methods. Thus, disentangle representation learning, which aims to learn factorized representations that separate and uncover latent explanatory factors behind the data [37], has recently received much attention in recommendation systems.\n\n\nDisentangled Representation\n\nPrevious study has demonstrated that disentangled representations are more robust, i.e., counfounding bias are less likely to be preserved by uncovering latent factors. Ma et al. [8] propose to differentiate latent factors of learned user/item embeddings into macro and micro ones, thus the developed recommendation methods are less likely to mistakenly preserve the confounding of the factors. Moreover, the disentangle representation can provide rich semantics of users' preference, involving items' aspect information [8], [38] to users' behavior type information [39]. Thus, several works are proposed using disentangle representation learning to improve recommendation, for instance, DisHAN [40] learns disentangled aspect-aware user/item representations based on different meta path types in a HIN, these aspectaware embeddings are then used to guide the top-N recommendation. Unfortunately, these aspect-aware disentangled embeddings only captured users' general taste on item aspects, however, failed to combine the specified item aspects with the real user intents. Parallelly, several works are conducted on modeling disentangled representations of users' intents, such as MacridVAE [8], DICE [11] and DGCF [31], the drawback is also distinct that they failed to combine the learned user intents with real-world item aspects, i.e., the user intents are predefined manually, such as \"passing the time\", short of providing meaningful information in a recommendation method. To sum up, the current studies on disentangle representation learning-enhanced recommendation either target at learning items' aspect-level representation [30], [40] or users' intents-level representation [8], [11], [31]. However, aforementioned approaches ignore bias stemmed from semantics information. To our knowledge, our approach is the first attempt to achieve interpretable and unbiased recommendation with disentangled embeddings for user intent.\n\n\nCausal Methods for Debiasing\n\nTo the best of our knowledge, existing causal methods for recommendations aim at mitigating the effects of different bias rather than improving interpretability as in our work. Most existing works claim that the observational rating data suffers from selection bias [9], [41], exposure bias [6], [7], [42], [43] or popularity bias [3], [4], [10]. Following this paradigm, dominant approaches adopt two main strategies such as propensity-score [5], [6], [42] or causal embedding [7], [41], [43], to disentangle user interests from different types of bias. For instance, the method in [6] uses propensity score to re-weight the observational click data, with the aim of imitating the scenario that item is randomly exposed and alleviating the exposure bias. The work in [7] learns a uniform unbiased embeddings from partially observed user-item interactions via their decounfonded model. More recently, the work in [41] resorts to balance learning with a Middle-point Distance Minimization (MPDM) strategy to learn causal embeddings that are free from selection bias. Facing user conformity issue in recommendation, [3] relates such issue with popularity bias, and proposes to alleviate the popularity bias by learning disentangled embeddings of user interest. A few state-of-the-art works [4], [9], [10] inspect cause-effect of the bias generation and design a specific causal graph attributing the exposure bias to a confounder. For example, Li et al. [9] prove the social network to be a confounder that affects the user's rating and the exposure policy of the item to the user.\n\n\nCONCLUSION AND FUTURE WORK\n\nIn this paper, we have researched the confounding bias issue stemming from different aspects, and propose an unbiased and robust Causal Disentanglement Semantics-Aware Intent Learning (CaDSI) for recommendation. Our CaDSI is capable of providing semantics to fine-grained representations for disentangling user intents, meanwhile easing the bias stemming from unevenly distributed item aspects. We evaluate our CaDSI on three real-world recommendation datasets, with extensive experiments and visualizations demonstrate the robustness and interpretability of our semantics-aware user intent representation. In future work, we will explore the effect of different auxiliary information on the recommendation system using the intervention analysis in causal inference. We first conduct an experiment to understand the disentanglement of user intents by our CaDSI, then explore whether such intent related to real-world item semantics. We select an user u2972 from Douban Book and learn its interaction scores S(u, i) (cf. Eq. (9)) with his/her historical interacted items under our CaDSI. The user intents factor k = 4 indicates four distinct user intents. Thereafter, we randomly select four items from the interaction score matrices. For each interaction under different k, we mark the interaction scores with the highest confidence with solid lines and couple the certain item attributes below them. Figure 8 shows the visualization results and we have the following findings:\n\n\u2022 Jointly analyzing intent-aware user-item interaction graphs, we can see user preference differs across each graphs, reflected by different interaction scores in each intent-aware graph. For example, u2972 interacts with i12047 with a preference score of 1.43 under intent k 1 , while the score changes to 1.79 under intent k 2 . This demonstrates the importance of disentangling user intents in recommendation scenario. \u2022 We thereafter couple item attributes to investigate whether user intents are related to item semantics. It can be seen that different fine-grained user intents are highly consistent with high-level item semantics. For instance, intent k 3 contributes mostly to interactions (u2972, i14892) and (u2972, i18), which suggests its high confidence as being the intents behind these behaviors. When switch to item attributes of i12047 and i18, one highlighted item attribute Author with the same id can be found, reflecting the reason why u2972 chose to interact with i12047 and i18. This demonstrated that our CaDSI, which aims at disentangling user intents meanwhile assign specific item semantics to the learned intents, is effective in the disentanglement of user intents towards item aspects. We randomly select 100 users from Douban Book dataset, and implement CaDSI on the dataset to output the 128-dimensional semantics-aware user intent embedding e. For visualization purposes, we use t-SNE [44] to map highdimensional user intent representation e to 2-dimensional vectors. Following the parameter settings in Section 4.1.4, we set the user intent factors k = 4. Figure 9 shows the visualization result.\n\n\nAPPENDIX B VISUALIZATION\n\nWe notice that the projections are capable of distinguishing four discernible clusters of users, and the cluster number is consistent with our pre-defined latent user intent factors k. This indicates that CaDSI is able to group users of the same intent closely based on the distances among users' embeddings. Meanwhile, each cluster is well-separated from others, further demonstrating the robust representation of CaDSI.\n\nFurthermore, we extract two users termed 1377903 and G.F rankenstein and show their historical interaction abstracts in the right of Figure 9. Analyzing these abstracts, we can see that our CaDSI is also capable of grouping each user whose interests are on the same item attributes. Such as 1377903 and G.F rankenstein, both users arrange items with similar attributes close to each other and dissimilar ones distant from each other. In summary, the visualizations intuitively demonstrate CaDSI's novel capability to discover, model, and capture the underlying semantics and structural relationships between multiple item aspects and user intents in heterogeneous networks.\n\nFig. 2 :\n2Causal disentanglement model for recommendation.\n\nFig. 3 :\n3Overview of the proposed CaDSI. CaDSI takes HIN as the input, and passes the causal disentanglement model for learning intent-aware representations (cf. Section 3.3); then use the causal intervention (cf. Section 3.4) for controlling the counfounding bias.\n\n\nthe training dataset involving the observed interactions O + and unobserved counterparts O \u2212 ; \u03c3(\u00b7) is sigmoid function; \u03bb is the coefficient controlling regularization.\n\n\u2022\nNeuMF[27] (I): This method combines deep neural networks with Matrix Factorization (MF) method for modeling the user-item interactions.\u2022 GC-MC[20] (II): The method organizes user behaviors as a graph, and employs one Graph Convolution\n\n\nof Book \u2212 Author. (b) (b) Distribution of Book \u2212 P ublisher. (c) (c) Distribution of Book \u2212 Y ear.\n\nFig. 4 :\n4The distributions of Book \u2212 Author, Book \u2212 P ublisher and Book \u2212 Y ear of Douban Book dataset.\n\nFig. 5 :\n5The recommendation performance comparison under different latent user intent factors.\n\nFig. 6 :\n6Impact of causal intervention on the recommendation performance of our CaDSI along with iterations.\n\nFig. 7 :\n7Performance of CaDSI in terms of Recall@K and NDCG@K under different K.\n\nFig. 8 :\n8Visualization of the disentangled user intent graphs based on score matrices. user-item interactions with highest scores are marked in solidlines; item attributes with the same values are highlighted in red.\n\nFig. 9 :\n92-dimensional t-SNE projections of the 128dimensional embeddings of 100 users from Douban Book dataset.\n\nTABLE 1 :\n1Key notations and descriptions.Notation \nDescription \nG \nHeterogeneous Information Network (HIN) \nV \nnode set of HIN \nA \nnode type set of HIN \nP \nmeta paths set of HIN \np \n\n\nTABLE 2 :\n2Statistics of three Datasets. Density of dataset \nis #Interactions/(#U sers \u00b7 #Items), Avg.Degree of A is \n#Relation/#A, Avg.Degree of B is #Relation/#B. \n\n\n\nTABLE 3 :\n3The selected meta paths for three datasets in our work.Dataset \nMeta path Schemes \n\nMovieLens-HetRec \nUMU, UMAMU, UMDMU, UMCMU, UMGMU \nMUM, MAM, MDM, MCM, MGM \nDouban Book \nUBoU, UBoAuBoU, UBoPBoU, UBoYBoU, UBoAuBoU \nBoUBo, BoPBo, BoYBo, BoAuBo \nDouban Movie \nUMU, UMDMU, UMAMU, UMTMU \nMUM, MAM, MDM, MTM \n\n\n\nTABLE 4 :\n4Overall Performance Comparison: bold numbers are the improvement percentages; the best results are marked with * , strongest baselines are marked with underline.MovieLens-HetRec \nDouban Book \nDouban Movi \nRecall@20 NDCG@20 Recall@40 NDCG@40 Recall@20 NDCG@20 Recall@40 NDCG@40 Recall@20 NDCG@20 Reca \nNeuMF \n0.0434 \n0.0557 \n0.0665 \n0.0709 \n0.0339 \n0.0391 \n0.0641 \n0.0682 \n0.0460 \n0.0417 \n0.0 \nGC-MC \n0.0336 \n0.0404 \n0.0653 \n0.0584 \n0.0458 \n0.0402 \n0.0675 \n0.0643 \n0.0448 \n0.0461 \n0.0 \nNGCF \n0.0365 \n0.0508 \n0.0699 \n0.0615 \n0.0252 \n0.0301 \n0.0707 \n0.0691 \n0.0475 \n0.0498 \n0.0 \nLightGCN \n0.0466 \n0.0155 \n0.0615 \n0.0498 \n0.0201 \n0.0225 \n0.0531 \n0.0568 \n0.0294 \n0.0331 \n0.0 \nIF-BPR \n0.0546 \n0.0510 \n0.0727 \n0.0689 \n0.0396 \n0.0463 \n0.0628 \n0.0601 \n0.0483 \n0.0501 \n0.0 \nMCRec \n0.0352 \n0.0195 \n0.0680 \n0.0677 \n0.0165 \n0.0294 \n0.0481 \n0.0507 \n0.0281 \n0.0336 \n0.0 \nNeuACF \n0.0236 \n0.0308 \n0.0556 \n0.0684 \n0.0298 \n0.0201 \n0.0601 \n0.0579 \n0.0351 \n0.0438 \n0.0 \nMacridVAE \n0.0454 \n0.0290 \n0.0661 \n0.0592 \n0.0309 \n0.0425 \n0.0691 \n0.0645 \n0.0489 \n0.0441 \n0.0 \nDGCF \n0.0229 \n0.0589 \n0.0532 \n0.0708 \n0.0431 \n0.0502 \n0.0649 \n0.0663 \n0.0416 \n0.0527 \n0.0 \nDICE \n0.0549 \n0.0499 \n0.0740 \n0.0703 \n0.0577 \n0.0608 \n0.0820 \n0.0799 \n0.0513 \n0.0389 \n0.0 \nOur model \n0.0678  *  \n0.0659  *  \n0.0765  *  \n0.0736  *  \n0.0708  *  \n0.0722  *  \n0.1466  *  \n0.1194  *  \n0.0583  *  \n0.0547  *  \n0.09 \n%improv. \n23.5% \n11.9% \n3.4% \n3.8% \n22.7% \n18.8% \n78.8% \n49.4% \n13.6% \n3.8% \n13. \n\n\n\nTABLE 5 :\n5Impact of multi-order connectivity (i.e., graph propagation layer number L) on MovieLens-HetRec, Douban Book and Douban Movie.MovieLens-HetRec Douban Book \nDouban Movie \nLayer number Recall \nNDCG \nRecall \nNDCG Recall \nNDCG \n1 \n0.0505 0.0521 \n0.0651 0.0684 \n0.0583 0.0546 \n2 \n0.0672 0.0683 \n0.0712 0.0736 \n0.0596 0.0570 \n3 \n0.0611 0.0624 \n0.0682 0.0701 \n0.0562 0.0573 \n\n\u2022 \n\n\n, \u00b7 \u00b7 \u00b7 , u u k ],i i = [i i 1 , \u00b7 \u00b7 \u00b7 , i i k ](4)Each chunked representation u u k \u2208 R d k and i i k \u2208 R d k is built upon the intent-aware interactions between user u and its preferred items under intent i. We ultimately sum up the intent-aware representations at each intent k of all L layers, the final layer outputs the k-th chunked intent-aware representations u u k and i i k :\nXiangmeng Wang has been a Ph.D. student at the School of Computer Science, Faculty of Engineering and Information Technology, University of Technology Sydney (UTS). She received her MSc degree in Computer Application Technology from Shanghai University. Her general research interests lie primarily in explainable artificial intelligence, data analysis, and causal machine learning. Her papers have been published in the top-tier conferences and journals in the field of machine learning.\nMatrix factorization techniques for recommender systems. Y Koren, R Bell, C Volinsky, Computer. 428Y. Koren, R. Bell, and C. Volinsky, \"Matrix factorization techniques for recommender systems,\" Computer, vol. 42, no. 8, pp. 30-37, 2009.\n\nRestricted boltzmann machines for collaborative filtering. R Salakhutdinov, A Mnih, G Hinton, Proceedings of the 24th international conference on Machine learning. the 24th international conference on Machine learningR. Salakhutdinov, A. Mnih, and G. Hinton, \"Restricted boltzmann machines for collaborative filtering,\" in Proceedings of the 24th international conference on Machine learning, 2007, pp. 791-798.\n\nDisentangling user interest and conformity for recommendation with causal embedding. Y Zheng, C Gao, X Li, X He, Y Li, D Jin, Proceedings of the Web Conference. the Web ConferenceY. Zheng, C. Gao, X. Li, X. He, Y. Li, and D. Jin, \"Disentangling user interest and conformity for recommendation with causal embedding,\" in Proceedings of the Web Conference 2021, 2021, pp. 2980-2991.\n\nDeconfounded recommendation for alleviating bias amplification. W Wang, F Feng, X He, X Wang, T.-S Chua, arXiv:2105.10648arXiv preprintW. Wang, F. Feng, X. He, X. Wang, and T.-S. Chua, \"Deconfounded recommendation for alleviating bias amplification,\" arXiv preprint arXiv:2105.10648, 2021.\n\nOffline evaluation to make decisions about playlistrecommendation algorithms. A Gruson, P Chandar, C Charbuillet, J Mcinerney, S Hansen, D Tardieu, B Carterette, Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining. the Twelfth ACM International Conference on Web Search and Data MiningA. Gruson, P. Chandar, C. Charbuillet, J. McInerney, S. Hansen, D. Tardieu, and B. Carterette, \"Offline evaluation to make deci- sions about playlistrecommendation algorithms,\" in Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, 2019, pp. 420-428.\n\nRecommendations as treatments: Debiasing learning and evaluation. T Schnabel, A Swaminathan, A Singh, N Chandak, T Joachims, international conference on machine learning. PMLRT. Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims, \"Recommendations as treatments: Debiasing learning and evaluation,\" in international conference on machine learning. PMLR, 2016, pp. 1670-1679.\n\nThe deconfounded recommender: A causal inference approach to recommendation. Y Wang, D Liang, L Charlin, D Blei, 08Y. Wang, D. Liang, L. Charlin, and D. Blei, \"The deconfounded recommender: A causal inference approach to recommendation,\" 08 2018.\n\nLearning disentangled representations for recommendation. J Ma, C Zhou, P Cui, H Yang, W Zhu, NeurIPS. J. Ma, C. Zhou, P. Cui, H. Yang, and W. Zhu, \"Learning disentan- gled representations for recommendation,\" NeurIPS, 2019.\n\nBe causal: De-biasing social network confounding in recommendation. Q Li, X Wang, G Xu, arXiv:2105.07775arXiv preprintQ. Li, X. Wang, and G. Xu, \"Be causal: De-biasing social network confounding in recommendation,\" arXiv preprint arXiv:2105.07775, 2021.\n\nCausal intervention for leveraging popularity bias in recommendation. Y Zhang, F Feng, X He, T Wei, C Song, G Ling, Y Zhang, arXiv:2105.06067arXiv preprintY. Zhang, F. Feng, X. He, T. Wei, C. Song, G. Ling, and Y. Zhang, \"Causal intervention for leveraging popularity bias in recommen- dation,\" arXiv preprint arXiv:2105.06067, 2021.\n\nDisentangling user interest and conformity for recommendation with causal embedding. Y Zheng, C Gao, X Li, X He, Y Li, D Jin, 10.1145/3442381.3449788Proceedings of the Web Conference 2021, ser. WWW '21. the Web Conference 2021, ser. WWW '21New York, NY, USAAssociation for Computing MachineryY. Zheng, C. Gao, X. Li, X. He, Y. Li, and D. Jin, \"Disentangling user interest and conformity for recommendation with causal embedding,\" in Proceedings of the Web Conference 2021, ser. WWW '21. New York, NY, USA: Association for Computing Machinery, 2021, p. 2980-2991. [Online]. Available: https://doi.org/10.1145/3442381.3449788\n\nGraph convolutional neural networks for web-scale recommender systems. R Ying, R He, K Chen, P Eksombatchai, W L Hamilton, J Leskovec, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningR. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec, \"Graph convolutional neural networks for web-scale recommender systems,\" in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018, pp. 974-983.\n\nLightgcn: Simplifying and powering graph convolution network for recommendation. X He, K Deng, X Wang, Y Li, Y Zhang, M Wang, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information RetrievalX. He, K. Deng, X. Wang, Y. Li, Y. Zhang, and M. Wang, \"Lightgcn: Simplifying and powering graph convolution network for recom- mendation,\" in Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2020, pp. 639-648.\n\nA survey of heterogeneous information network analysis. C Shi, Y Li, J Zhang, Y Sun, S Y Philip, IEEE Transactions on Knowledge and Data Engineering. 291C. Shi, Y. Li, J. Zhang, Y. Sun, and S. Y. Philip, \"A survey of heterogeneous information network analysis,\" IEEE Transactions on Knowledge and Data Engineering, vol. 29, no. 1, pp. 17-37, 2016.\n\n. J Pearl, Causality , Cambridge university pressJ. Pearl, Causality. Cambridge university press, 2009.\n\nmetapath2vec: Scalable representation learning for heterogeneous networks. Y Dong, N V Chawla, A Swami, Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. the 23rd ACM SIGKDD international conference on knowledge discovery and data miningY. Dong, N. V. Chawla, and A. Swami, \"metapath2vec: Scalable representation learning for heterogeneous networks,\" in Proceed- ings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 135-144.\n\nnode2vec: Scalable feature learning for networks. A Grover, J Leskovec, Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on Knowledge discovery and data miningA. Grover and J. Leskovec, \"node2vec: Scalable feature learning for networks,\" in Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp. 855-864.\n\nStochastic gradient descent tricks. L Bottou, Neural networks: Tricks of the trade. SpringerL. Bottou, \"Stochastic gradient descent tricks,\" in Neural networks: Tricks of the trade. Springer, 2012, pp. 421-436.\n\nApplied linear regression. S Weisberg, John Wiley & Sons528S. Weisberg, Applied linear regression. John Wiley & Sons, 2005, vol. 528.\n\nGraph convolutional matrix completion. R V Berg, T N Kipf, M Welling, R. v. d. Berg, T. N. Kipf, and M. Welling, \"Graph convolutional matrix completion,\" KDD, 2017.\n\nNeural graph collaborative filtering. X Wang, X He, M Wang, F Feng, T.-S Chua, Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval. the 42nd international ACM SIGIR conference on Research and development in Information RetrievalX. Wang, X. He, M. Wang, F. Feng, and T.-S. Chua, \"Neural graph collaborative filtering,\" in Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval, 2019, pp. 165-174.\n\nActivation functions in neural networks. S Sharma, S Sharma, Towards Data Science. 612S. Sharma and S. Sharma, \"Activation functions in neural net- works,\" Towards Data Science, vol. 6, no. 12, pp. 310-316, 2017.\n\nFactorization machines. S Rendle, 2010 IEEE International Conference on Data Mining. S. Rendle, \"Factorization machines,\" in 2010 IEEE International Conference on Data Mining, 2010, pp. 995-1000.\n\nStylistic scene enhancement gan: mixed stylistic enhancement generation for 3d indoor scenes. S Zhang, Z Han, Y.-K Lai, M Zwicker, H Zhang, The Visual Computer. 356S. Zhang, Z. Han, Y.-K. Lai, M. Zwicker, and H. Zhang, \"Stylistic scene enhancement gan: mixed stylistic enhancement generation for 3d indoor scenes,\" The Visual Computer, vol. 35, no. 6, pp. 1157- 1169, 2019.\n\nPersonalized ranking with importance sampling. D Lian, Q Liu, E Chen, Proceedings of The Web Conference 2020. The Web Conference 2020D. Lian, Q. Liu, and E. Chen, \"Personalized ranking with impor- tance sampling,\" in Proceedings of The Web Conference 2020, 2020, pp. 1093-1103.\n\nAdam-latest trends in deep learning optimization. V Bushaev, Towards Data Science, Listopad. V. Bushaev, \"Adam-latest trends in deep learning optimization,\" Towards Data Science, Listopad, 2018.\n\nNeural collaborative filtering. X He, L Liao, H Zhang, L Nie, X Hu, T.-S Chua, Proceedings of the 26th international conference on world wide. the 26th international conference on world wideX. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua, \"Neu- ral collaborative filtering,\" in Proceedings of the 26th international conference on world wide web, 2017, pp. 173-182.\n\nAdaptive implicit friends identification over heterogeneous network for social recommendation. J Yu, M Gao, J Li, H Yin, H Liu, Proceedings of the 27th ACM international conference on information and knowledge management. the 27th ACM international conference on information and knowledge managementJ. Yu, M. Gao, J. Li, H. Yin, and H. Liu, \"Adaptive implicit friends identification over heterogeneous network for social recommen- dation,\" in Proceedings of the 27th ACM international conference on information and knowledge management, 2018, pp. 357-366.\n\nLeveraging metapath based context for top-n recommendation with a neural coattention model. B Hu, C Shi, W X Zhao, P S Yu, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningB. Hu, C. Shi, W. X. Zhao, and P. S. Yu, \"Leveraging meta- path based context for top-n recommendation with a neural co- attention model,\" in Proceedings of the 24th ACM SIGKDD Interna- tional Conference on Knowledge Discovery & Data Mining, 2018, pp. 1531-1540.\n\nAspectlevel deep collaborative filtering via heterogeneous information networks. X Han, C Shi, S Wang, S Y Philip, L Song, in IJCAI. X. Han, C. Shi, S. Wang, S. Y. Philip, and L. Song, \"Aspect- level deep collaborative filtering via heterogeneous information networks.\" in IJCAI, 2018, pp. 3393-3399.\n\nDisentangled graph collaborative filtering. X Wang, H Jin, A Zhang, X He, T Xu, T.-S Chua, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information RetrievalX. Wang, H. Jin, A. Zhang, X. He, T. Xu, and T.-S. Chua, \"Dis- entangled graph collaborative filtering,\" in Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2020, pp. 1001-1010.\n\nUnderstanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio, Proceedings of the thirteenth international conference on artificial intelligence and statistics. the thirteenth international conference on artificial intelligence and statisticsX. Glorot and Y. Bengio, \"Understanding the difficulty of training deep feedforward neural networks,\" in Proceedings of the thirteenth international conference on artificial intelligence and statistics, 2010, pp. 249-256.\n\nMining heterogeneous information networks: a structural analysis approach. Y Sun, J Han, Acm Sigkdd Explorations Newsletter. 142Y. Sun and J. Han, \"Mining heterogeneous information networks: a structural analysis approach,\" Acm Sigkdd Explorations Newsletter, vol. 14, no. 2, pp. 20-28, 2013.\n\nCollaborative filtering with entity similarity regularization in heterogeneous information networks. X Yu, X Ren, Q Gu, Y Sun, J Han, IJCAI HINA. 27X. Yu, X. Ren, Q. Gu, Y. Sun, and J. Han, \"Collaborative filtering with entity similarity regularization in heterogeneous information networks,\" IJCAI HINA, vol. 27, 2013.\n\nPersonalized entity recommendation: A heterogeneous information network approach. X Yu, X Ren, Y Sun, Q Gu, B Sturt, U Khandelwal, B Norick, J Han, Proceedings of the 7th ACM international conference on Web search and data mining. the 7th ACM international conference on Web search and data miningX. Yu, X. Ren, Y. Sun, Q. Gu, B. Sturt, U. Khandelwal, B. Norick, and J. Han, \"Personalized entity recommendation: A heteroge- neous information network approach,\" in Proceedings of the 7th ACM international conference on Web search and data mining, 2014, pp. 283-292.\n\nSemantic path based personalized recommendation on weighted heterogeneous information networks. C Shi, Z Zhang, P Luo, P S Yu, Y Yue, B Wu, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. the 24th ACM International on Conference on Information and Knowledge ManagementC. Shi, Z. Zhang, P. Luo, P. S. Yu, Y. Yue, and B. Wu, \"Semantic path based personalized recommendation on weighted heterogeneous information networks,\" in Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, 2015, pp. 453-462.\n\nRepresentation learning: A review and new perspectives. Y Bengio, A Courville, P Vincent, IEEE transactions on pattern analysis and machine intelligence. 35Y. Bengio, A. Courville, and P. Vincent, \"Representation learning: A review and new perspectives,\" IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 8, pp. 1798-1828, 2013.\n\nExplicit factor models for explainable recommendation based on phraselevel sentiment analysis. Y Zhang, G Lai, M Zhang, Y Zhang, Y Liu, S Ma, Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. the 37th international ACM SIGIR conference on Research & development in information retrievalY. Zhang, G. Lai, M. Zhang, Y. Zhang, Y. Liu, and S. Ma, \"Explicit factor models for explainable recommendation based on phrase- level sentiment analysis,\" in Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval, 2014, pp. 83-92.\n\nAdreveal: Improving transparency into online targeted advertising. B Liu, A Sheth, U Weinsberg, J Chandrashekar, R Govindan, Proceedings of the Twelfth ACM Workshop on Hot Topics in Networks. the Twelfth ACM Workshop on Hot Topics in NetworksB. Liu, A. Sheth, U. Weinsberg, J. Chandrashekar, and R. Govin- dan, \"Adreveal: Improving transparency into online targeted ad- vertising,\" in Proceedings of the Twelfth ACM Workshop on Hot Topics in Networks, 2013, pp. 1-7.\n\nDisenhan: Disentangled heterogeneous graph attention network for recommendation. Y Wang, S Tang, Y Lei, W Song, S Wang, M Zhang, Proceedings of the 29th ACM International Conference on Information & Knowledge Management. the 29th ACM International Conference on Information & Knowledge ManagementY. Wang, S. Tang, Y. Lei, W. Song, S. Wang, and M. Zhang, \"Disenhan: Disentangled heterogeneous graph attention network for recommendation,\" in Proceedings of the 29th ACM International Conference on Information & Knowledge Management, 2020, pp. 1605- 1614.\n\nRepresentation learning for treatment effect estimation from observational data. L Yao, S Li, Y Li, M Huai, J Gao, A Zhang, Advances in Neural Information Processing Systems. 31L. Yao, S. Li, Y. Li, M. Huai, J. Gao, and A. Zhang, \"Representation learning for treatment effect estimation from observational data,\" Advances in Neural Information Processing Systems, vol. 31, 2018.\n\nCausal inference for recommendation. D Liang, L Charlin, D M Blei, Causation: Foundation to Application, Workshop at UAI. AUAI. D. Liang, L. Charlin, and D. M. Blei, \"Causal inference for rec- ommendation,\" in Causation: Foundation to Application, Workshop at UAI. AUAI, 2016.\n\nCausal embeddings for recommendation. S Bonner, F Vasile, Proceedings of the 12th ACM Conference on Recommender Systems. the 12th ACM Conference on Recommender SystemsS. Bonner and F. Vasile, \"Causal embeddings for recommenda- tion,\" in Proceedings of the 12th ACM Conference on Recommender Systems, 2018, pp. 104-112.\n\nVisualizing data using t-sne. L Van Der Maaten, G Hinton, Journal of machine learning research. 911L. Van der Maaten and G. Hinton, \"Visualizing data using t-sne.\" Journal of machine learning research, vol. 9, no. 11, 2008.\n\nAttribute for (U 2972, I 14892 ) and (U 2972, I 18 ) Book 14892 -Publisher :274; Year. 32Attribute for (U 2972, I 14892 ) and (U 2972, I 18 ) Book 14892 -Publisher :274; Year: 32.\n\nBook 18 -Publisher :828; Year: 63. Attribute for (U 2972, I 12047 ) and (U 2972, I 18 ) Book 1523 -Publisher :828. Year: 4. Book 18 -Publisher :828; Year: 63Book 18 -Publisher :828; Year: 63. Attribute for (U 2972, I 12047 ) and (U 2972, I 18 ) Book 1523 -Publisher :828; Year: 4. Book 18 -Publisher :828; Year: 63.\n\n. Author, 793Author: 793\n", "annotations": {"author": "[{\"end\":120,\"start\":93},{\"end\":127,\"start\":121}]", "publisher": null, "author_last_name": "[{\"end\":119,\"start\":93},{\"end\":126,\"start\":121}]", "author_first_name": null, "author_affiliation": null, "title": "[{\"end\":77,\"start\":1},{\"end\":204,\"start\":128}]", "venue": null, "abstract": "[{\"end\":1707,\"start\":314}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1972,\"start\":1969},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1977,\"start\":1974},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2467,\"start\":2464},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2472,\"start\":2469},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2904,\"start\":2901},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2909,\"start\":2906},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2914,\"start\":2911},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2919,\"start\":2916},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3301,\"start\":3298},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3306,\"start\":3303},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3312,\"start\":3308},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3566,\"start\":3563},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4049,\"start\":4045},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4078,\"start\":4075},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4494,\"start\":4490},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4500,\"start\":4496},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4506,\"start\":4502},{\"end\":8059,\"start\":8058},{\"end\":8544,\"start\":8543},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11545,\"start\":11541},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15808,\"start\":15804},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18658,\"start\":18654},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18786,\"start\":18782},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20294,\"start\":20290},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20873,\"start\":20869},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21745,\"start\":21741},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23436,\"start\":23432},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23735,\"start\":23731},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23741,\"start\":23737},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23747,\"start\":23743},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23753,\"start\":23749},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25993,\"start\":25990},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27773,\"start\":27769},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29769,\"start\":29765},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30452,\"start\":30448},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30593,\"start\":30589},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31216,\"start\":31212},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31882,\"start\":31878},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32271,\"start\":32267},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32956,\"start\":32952},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34156,\"start\":34152},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36156,\"start\":36152},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36481,\"start\":36477},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36908,\"start\":36904},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":36959,\"start\":36955},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":39132,\"start\":39128},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":39284,\"start\":39280},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":39480,\"start\":39476},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":39687,\"start\":39683},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39816,\"start\":39812},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39954,\"start\":39951},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40121,\"start\":40117},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":40313,\"start\":40309},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":41251,\"start\":41247},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":41555,\"start\":41551},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":53439,\"start\":53435},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":53562,\"start\":53558},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":53568,\"start\":53564},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":53653,\"start\":53649},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":53679,\"start\":53675},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":53771,\"start\":53767},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":53935,\"start\":53931},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":54180,\"start\":54176},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":54715,\"start\":54711},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54994,\"start\":54991},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":55336,\"start\":55333},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":55342,\"start\":55338},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":55383,\"start\":55379},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":55512,\"start\":55508},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":56008,\"start\":56005},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":56019,\"start\":56015},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":56033,\"start\":56029},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":56453,\"start\":56449},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":56459,\"start\":56455},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":56502,\"start\":56499},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":56508,\"start\":56504},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":56514,\"start\":56510},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":57051,\"start\":57048},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":57057,\"start\":57053},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":57076,\"start\":57073},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":57081,\"start\":57078},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":57087,\"start\":57083},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":57093,\"start\":57089},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":57116,\"start\":57113},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":57121,\"start\":57118},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":57127,\"start\":57123},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":57228,\"start\":57225},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":57233,\"start\":57230},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":57239,\"start\":57235},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":57263,\"start\":57260},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":57269,\"start\":57265},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":57275,\"start\":57271},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":57368,\"start\":57365},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":57553,\"start\":57550},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":57699,\"start\":57695},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":57899,\"start\":57896},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":58073,\"start\":58070},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":58078,\"start\":58075},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":58084,\"start\":58080},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":58237,\"start\":58234},{\"end\":60294,\"start\":60293},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":61293,\"start\":61289},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":63139,\"start\":63135},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":63276,\"start\":63272}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":62686,\"start\":62627},{\"attributes\":{\"id\":\"fig_2\"},\"end\":62954,\"start\":62687},{\"attributes\":{\"id\":\"fig_3\"},\"end\":63126,\"start\":62955},{\"attributes\":{\"id\":\"fig_5\"},\"end\":63364,\"start\":63127},{\"attributes\":{\"id\":\"fig_6\"},\"end\":63465,\"start\":63365},{\"attributes\":{\"id\":\"fig_7\"},\"end\":63571,\"start\":63466},{\"attributes\":{\"id\":\"fig_8\"},\"end\":63668,\"start\":63572},{\"attributes\":{\"id\":\"fig_9\"},\"end\":63779,\"start\":63669},{\"attributes\":{\"id\":\"fig_10\"},\"end\":63862,\"start\":63780},{\"attributes\":{\"id\":\"fig_11\"},\"end\":64081,\"start\":63863},{\"attributes\":{\"id\":\"fig_12\"},\"end\":64196,\"start\":64082},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":64381,\"start\":64197},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":64550,\"start\":64382},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":64870,\"start\":64551},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":66330,\"start\":64871},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":66716,\"start\":66331}]", "paragraph": "[{\"end\":2634,\"start\":1723},{\"end\":4225,\"start\":2636},{\"end\":4804,\"start\":4227},{\"end\":5352,\"start\":4806},{\"end\":6681,\"start\":5354},{\"end\":7802,\"start\":6683},{\"end\":8344,\"start\":7804},{\"end\":8721,\"start\":8346},{\"end\":9384,\"start\":8761},{\"end\":9619,\"start\":9407},{\"end\":10049,\"start\":9621},{\"end\":10172,\"start\":10051},{\"end\":10525,\"start\":10212},{\"end\":10652,\"start\":10527},{\"end\":11351,\"start\":10654},{\"end\":12207,\"start\":11413},{\"end\":12460,\"start\":12209},{\"end\":12720,\"start\":12462},{\"end\":12818,\"start\":12722},{\"end\":13021,\"start\":12820},{\"end\":13093,\"start\":13023},{\"end\":13246,\"start\":13095},{\"end\":13431,\"start\":13248},{\"end\":13547,\"start\":13433},{\"end\":13626,\"start\":13549},{\"end\":13715,\"start\":13628},{\"end\":13994,\"start\":13717},{\"end\":15396,\"start\":14042},{\"end\":15634,\"start\":15398},{\"end\":16630,\"start\":15636},{\"end\":16929,\"start\":16645},{\"end\":18455,\"start\":16984},{\"end\":19028,\"start\":18488},{\"end\":19466,\"start\":19152},{\"end\":20166,\"start\":19468},{\"end\":20646,\"start\":20194},{\"end\":21100,\"start\":20648},{\"end\":21787,\"start\":21164},{\"end\":22840,\"start\":21808},{\"end\":22929,\"start\":22842},{\"end\":23646,\"start\":23067},{\"end\":24593,\"start\":23691},{\"end\":24731,\"start\":24686},{\"end\":24988,\"start\":24750},{\"end\":25709,\"start\":25023},{\"end\":26253,\"start\":25735},{\"end\":26495,\"start\":26255},{\"end\":26992,\"start\":26555},{\"end\":27175,\"start\":27124},{\"end\":27634,\"start\":27177},{\"end\":27857,\"start\":27688},{\"end\":28101,\"start\":27859},{\"end\":28244,\"start\":28123},{\"end\":28407,\"start\":28302},{\"end\":28621,\"start\":28513},{\"end\":28990,\"start\":28623},{\"end\":29802,\"start\":29026},{\"end\":30047,\"start\":29846},{\"end\":30316,\"start\":30049},{\"end\":30752,\"start\":30360},{\"end\":30864,\"start\":30803},{\"end\":31083,\"start\":30964},{\"end\":31398,\"start\":31085},{\"end\":31829,\"start\":31422},{\"end\":32574,\"start\":31831},{\"end\":32822,\"start\":32615},{\"end\":33840,\"start\":32824},{\"end\":34157,\"start\":33842},{\"end\":34211,\"start\":34159},{\"end\":34415,\"start\":34367},{\"end\":34491,\"start\":34417},{\"end\":34687,\"start\":34493},{\"end\":34889,\"start\":34689},{\"end\":35407,\"start\":34891},{\"end\":35798,\"start\":35467},{\"end\":36012,\"start\":35817},{\"end\":36209,\"start\":36073},{\"end\":36559,\"start\":36226},{\"end\":36960,\"start\":36606},{\"end\":37093,\"start\":36976},{\"end\":37211,\"start\":37095},{\"end\":37311,\"start\":37213},{\"end\":37528,\"start\":37313},{\"end\":37600,\"start\":37530},{\"end\":37730,\"start\":37602},{\"end\":38564,\"start\":37767},{\"end\":39119,\"start\":38578},{\"end\":39267,\"start\":39121},{\"end\":39465,\"start\":39269},{\"end\":39673,\"start\":39467},{\"end\":39801,\"start\":39675},{\"end\":39937,\"start\":39803},{\"end\":40108,\"start\":39939},{\"end\":40300,\"start\":40110},{\"end\":40484,\"start\":40302},{\"end\":40927,\"start\":40507},{\"end\":41560,\"start\":40950},{\"end\":43508,\"start\":41596},{\"end\":44493,\"start\":43510},{\"end\":44771,\"start\":44526},{\"end\":45305,\"start\":44773},{\"end\":45990,\"start\":45307},{\"end\":46246,\"start\":45992},{\"end\":46802,\"start\":46248},{\"end\":47290,\"start\":46804},{\"end\":47629,\"start\":47315},{\"end\":48159,\"start\":47631},{\"end\":48624,\"start\":48198},{\"end\":48992,\"start\":48626},{\"end\":49446,\"start\":48998},{\"end\":49769,\"start\":49480},{\"end\":49870,\"start\":49771},{\"end\":49897,\"start\":49872},{\"end\":50162,\"start\":49919},{\"end\":50622,\"start\":50164},{\"end\":50991,\"start\":50624},{\"end\":51822,\"start\":51030},{\"end\":52186,\"start\":51824},{\"end\":52592,\"start\":52223},{\"end\":52619,\"start\":52594},{\"end\":52663,\"start\":52641},{\"end\":53114,\"start\":52701},{\"end\":53338,\"start\":53131},{\"end\":54780,\"start\":53370},{\"end\":56749,\"start\":54812},{\"end\":58361,\"start\":56782},{\"end\":59869,\"start\":58392},{\"end\":61501,\"start\":59871},{\"end\":61951,\"start\":61530},{\"end\":62626,\"start\":61953}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10211,\"start\":10173},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19151,\"start\":19029},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21163,\"start\":21101},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23066,\"start\":22930},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24685,\"start\":24594},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25022,\"start\":24989},{\"attributes\":{\"id\":\"formula_6\"},\"end\":26554,\"start\":26496},{\"attributes\":{\"id\":\"formula_7\"},\"end\":27123,\"start\":26993},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27687,\"start\":27635},{\"attributes\":{\"id\":\"formula_9\"},\"end\":28301,\"start\":28245},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28512,\"start\":28408},{\"attributes\":{\"id\":\"formula_11\"},\"end\":29845,\"start\":29803},{\"attributes\":{\"id\":\"formula_12\"},\"end\":30359,\"start\":30317},{\"attributes\":{\"id\":\"formula_13\"},\"end\":30802,\"start\":30753},{\"attributes\":{\"id\":\"formula_14\"},\"end\":30927,\"start\":30865},{\"attributes\":{\"id\":\"formula_15\"},\"end\":32614,\"start\":32575},{\"attributes\":{\"id\":\"formula_16\"},\"end\":34366,\"start\":34212},{\"attributes\":{\"id\":\"formula_17\"},\"end\":35466,\"start\":35408},{\"attributes\":{\"id\":\"formula_18\"},\"end\":35816,\"start\":35799},{\"attributes\":{\"id\":\"formula_19\"},\"end\":36072,\"start\":36013},{\"attributes\":{\"id\":\"formula_20\"},\"end\":36605,\"start\":36560},{\"attributes\":{\"id\":\"formula_21\"},\"end\":49918,\"start\":49898},{\"attributes\":{\"id\":\"formula_22\"},\"end\":52640,\"start\":52620}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":10601,\"start\":10594},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19515,\"start\":19508},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":37907,\"start\":37900},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":37974,\"start\":37967},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":44691,\"start\":44684},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":51449,\"start\":51442}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1721,\"start\":1709},{\"attributes\":{\"n\":\"2\"},\"end\":8759,\"start\":8724},{\"attributes\":{\"n\":\"2.1\"},\"end\":9405,\"start\":9387},{\"attributes\":{\"n\":\"2.2\"},\"end\":11385,\"start\":11354},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":11411,\"start\":11388},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":14040,\"start\":13997},{\"attributes\":{\"n\":\"3\"},\"end\":16643,\"start\":16633},{\"attributes\":{\"n\":\"3.1\"},\"end\":16982,\"start\":16932},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":18486,\"start\":18458},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":20192,\"start\":20169},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":21806,\"start\":21790},{\"attributes\":{\"n\":\"3.2\"},\"end\":23689,\"start\":23649},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":24748,\"start\":24734},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":25733,\"start\":25712},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":28121,\"start\":28104},{\"attributes\":{\"n\":\"3.3\"},\"end\":29024,\"start\":28993},{\"attributes\":{\"n\":\"3.4\"},\"end\":30962,\"start\":30929},{\"attributes\":{\"n\":\"3.4.1\"},\"end\":31420,\"start\":31401},{\"attributes\":{\"n\":\"3.5\"},\"end\":36224,\"start\":36212},{\"attributes\":{\"n\":\"4\"},\"end\":36974,\"start\":36963},{\"attributes\":{\"n\":\"4.1\"},\"end\":37754,\"start\":37733},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":37765,\"start\":37757},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":38576,\"start\":38567},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":40505,\"start\":40487},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":40948,\"start\":40930},{\"attributes\":{\"n\":\"4.2\"},\"end\":41594,\"start\":41563},{\"attributes\":{\"n\":\"4.3\"},\"end\":44524,\"start\":44496},{\"attributes\":{\"n\":\"4.4\"},\"end\":47313,\"start\":47293},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":48196,\"start\":48162},{\"end\":48996,\"start\":48995},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":49478,\"start\":49449},{\"attributes\":{\"n\":\"4.4.3\"},\"end\":51028,\"start\":50994},{\"attributes\":{\"n\":\"4.4.4\"},\"end\":52221,\"start\":52189},{\"attributes\":{\"n\":\"4.5\"},\"end\":52699,\"start\":52666},{\"attributes\":{\"n\":\"5\"},\"end\":53129,\"start\":53117},{\"attributes\":{\"n\":\"5.1\"},\"end\":53368,\"start\":53341},{\"attributes\":{\"n\":\"5.2\"},\"end\":54810,\"start\":54783},{\"attributes\":{\"n\":\"5.3\"},\"end\":56780,\"start\":56752},{\"attributes\":{\"n\":\"6\"},\"end\":58390,\"start\":58364},{\"end\":61528,\"start\":61504},{\"end\":62636,\"start\":62628},{\"end\":62696,\"start\":62688},{\"end\":63129,\"start\":63128},{\"end\":63475,\"start\":63467},{\"end\":63581,\"start\":63573},{\"end\":63678,\"start\":63670},{\"end\":63789,\"start\":63781},{\"end\":63872,\"start\":63864},{\"end\":64091,\"start\":64083},{\"end\":64207,\"start\":64198},{\"end\":64392,\"start\":64383},{\"end\":64561,\"start\":64552},{\"end\":64881,\"start\":64872},{\"end\":66341,\"start\":66332}]", "table": "[{\"end\":64381,\"start\":64240},{\"end\":64550,\"start\":64394},{\"end\":64870,\"start\":64618},{\"end\":66330,\"start\":65044},{\"end\":66716,\"start\":66469}]", "figure_caption": "[{\"end\":62686,\"start\":62638},{\"end\":62954,\"start\":62698},{\"end\":63126,\"start\":62957},{\"end\":63364,\"start\":63130},{\"end\":63465,\"start\":63367},{\"end\":63571,\"start\":63477},{\"end\":63668,\"start\":63583},{\"end\":63779,\"start\":63680},{\"end\":63862,\"start\":63791},{\"end\":64081,\"start\":63874},{\"end\":64196,\"start\":64093},{\"end\":64240,\"start\":64209},{\"end\":64618,\"start\":64563},{\"end\":65044,\"start\":64883},{\"end\":66469,\"start\":66343}]", "figure_ref": "[{\"end\":6052,\"start\":6044},{\"end\":6341,\"start\":6333},{\"end\":6350,\"start\":6344},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11777,\"start\":11769},{\"end\":14685,\"start\":14677},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16457,\"start\":16449},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16832,\"start\":16824},{\"end\":32136,\"start\":32129},{\"end\":32504,\"start\":32497},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33347,\"start\":33344},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":42493,\"start\":42485},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":42544,\"start\":42536},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":42651,\"start\":42643},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":43837,\"start\":43825},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":48524,\"start\":48516},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":49643,\"start\":49635},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":50161,\"start\":50153},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":52279,\"start\":52271},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":59801,\"start\":59793},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":61469,\"start\":61461},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":62094,\"start\":62086}]", "bib_author_first_name": "[{\"end\":67650,\"start\":67649},{\"end\":67659,\"start\":67658},{\"end\":67667,\"start\":67666},{\"end\":67890,\"start\":67889},{\"end\":67907,\"start\":67906},{\"end\":67915,\"start\":67914},{\"end\":68329,\"start\":68328},{\"end\":68338,\"start\":68337},{\"end\":68345,\"start\":68344},{\"end\":68351,\"start\":68350},{\"end\":68357,\"start\":68356},{\"end\":68363,\"start\":68362},{\"end\":68690,\"start\":68689},{\"end\":68698,\"start\":68697},{\"end\":68706,\"start\":68705},{\"end\":68712,\"start\":68711},{\"end\":68723,\"start\":68719},{\"end\":68995,\"start\":68994},{\"end\":69005,\"start\":69004},{\"end\":69016,\"start\":69015},{\"end\":69031,\"start\":69030},{\"end\":69044,\"start\":69043},{\"end\":69054,\"start\":69053},{\"end\":69065,\"start\":69064},{\"end\":69589,\"start\":69588},{\"end\":69601,\"start\":69600},{\"end\":69616,\"start\":69615},{\"end\":69625,\"start\":69624},{\"end\":69636,\"start\":69635},{\"end\":69988,\"start\":69987},{\"end\":69996,\"start\":69995},{\"end\":70005,\"start\":70004},{\"end\":70016,\"start\":70015},{\"end\":70217,\"start\":70216},{\"end\":70223,\"start\":70222},{\"end\":70231,\"start\":70230},{\"end\":70238,\"start\":70237},{\"end\":70246,\"start\":70245},{\"end\":70453,\"start\":70452},{\"end\":70459,\"start\":70458},{\"end\":70467,\"start\":70466},{\"end\":70710,\"start\":70709},{\"end\":70719,\"start\":70718},{\"end\":70727,\"start\":70726},{\"end\":70733,\"start\":70732},{\"end\":70740,\"start\":70739},{\"end\":70748,\"start\":70747},{\"end\":70756,\"start\":70755},{\"end\":71060,\"start\":71059},{\"end\":71069,\"start\":71068},{\"end\":71076,\"start\":71075},{\"end\":71082,\"start\":71081},{\"end\":71088,\"start\":71087},{\"end\":71094,\"start\":71093},{\"end\":71671,\"start\":71670},{\"end\":71679,\"start\":71678},{\"end\":71685,\"start\":71684},{\"end\":71693,\"start\":71692},{\"end\":71709,\"start\":71708},{\"end\":71711,\"start\":71710},{\"end\":71723,\"start\":71722},{\"end\":72264,\"start\":72263},{\"end\":72270,\"start\":72269},{\"end\":72278,\"start\":72277},{\"end\":72286,\"start\":72285},{\"end\":72292,\"start\":72291},{\"end\":72301,\"start\":72300},{\"end\":72850,\"start\":72849},{\"end\":72857,\"start\":72856},{\"end\":72863,\"start\":72862},{\"end\":72872,\"start\":72871},{\"end\":72879,\"start\":72878},{\"end\":72881,\"start\":72880},{\"end\":73145,\"start\":73144},{\"end\":73162,\"start\":73153},{\"end\":73323,\"start\":73322},{\"end\":73331,\"start\":73330},{\"end\":73333,\"start\":73332},{\"end\":73343,\"start\":73342},{\"end\":73824,\"start\":73823},{\"end\":73834,\"start\":73833},{\"end\":74267,\"start\":74266},{\"end\":74470,\"start\":74469},{\"end\":74617,\"start\":74616},{\"end\":74619,\"start\":74618},{\"end\":74627,\"start\":74626},{\"end\":74629,\"start\":74628},{\"end\":74637,\"start\":74636},{\"end\":74782,\"start\":74781},{\"end\":74790,\"start\":74789},{\"end\":74796,\"start\":74795},{\"end\":74804,\"start\":74803},{\"end\":74815,\"start\":74811},{\"end\":75299,\"start\":75298},{\"end\":75309,\"start\":75308},{\"end\":75496,\"start\":75495},{\"end\":75763,\"start\":75762},{\"end\":75772,\"start\":75771},{\"end\":75782,\"start\":75778},{\"end\":75789,\"start\":75788},{\"end\":75800,\"start\":75799},{\"end\":76091,\"start\":76090},{\"end\":76099,\"start\":76098},{\"end\":76106,\"start\":76105},{\"end\":76373,\"start\":76372},{\"end\":76551,\"start\":76550},{\"end\":76557,\"start\":76556},{\"end\":76565,\"start\":76564},{\"end\":76574,\"start\":76573},{\"end\":76581,\"start\":76580},{\"end\":76590,\"start\":76586},{\"end\":76988,\"start\":76987},{\"end\":76994,\"start\":76993},{\"end\":77001,\"start\":77000},{\"end\":77007,\"start\":77006},{\"end\":77014,\"start\":77013},{\"end\":77542,\"start\":77541},{\"end\":77548,\"start\":77547},{\"end\":77555,\"start\":77554},{\"end\":77557,\"start\":77556},{\"end\":77565,\"start\":77564},{\"end\":77567,\"start\":77566},{\"end\":78097,\"start\":78096},{\"end\":78104,\"start\":78103},{\"end\":78111,\"start\":78110},{\"end\":78119,\"start\":78118},{\"end\":78121,\"start\":78120},{\"end\":78131,\"start\":78130},{\"end\":78362,\"start\":78361},{\"end\":78370,\"start\":78369},{\"end\":78377,\"start\":78376},{\"end\":78386,\"start\":78385},{\"end\":78392,\"start\":78391},{\"end\":78401,\"start\":78397},{\"end\":78936,\"start\":78935},{\"end\":78946,\"start\":78945},{\"end\":79433,\"start\":79432},{\"end\":79440,\"start\":79439},{\"end\":79753,\"start\":79752},{\"end\":79759,\"start\":79758},{\"end\":79766,\"start\":79765},{\"end\":79772,\"start\":79771},{\"end\":79779,\"start\":79778},{\"end\":80055,\"start\":80054},{\"end\":80061,\"start\":80060},{\"end\":80068,\"start\":80067},{\"end\":80075,\"start\":80074},{\"end\":80081,\"start\":80080},{\"end\":80090,\"start\":80089},{\"end\":80104,\"start\":80103},{\"end\":80114,\"start\":80113},{\"end\":80636,\"start\":80635},{\"end\":80643,\"start\":80642},{\"end\":80652,\"start\":80651},{\"end\":80659,\"start\":80658},{\"end\":80661,\"start\":80660},{\"end\":80667,\"start\":80666},{\"end\":80674,\"start\":80673},{\"end\":81186,\"start\":81185},{\"end\":81196,\"start\":81195},{\"end\":81209,\"start\":81208},{\"end\":81582,\"start\":81581},{\"end\":81591,\"start\":81590},{\"end\":81598,\"start\":81597},{\"end\":81607,\"start\":81606},{\"end\":81616,\"start\":81615},{\"end\":81623,\"start\":81622},{\"end\":82189,\"start\":82188},{\"end\":82196,\"start\":82195},{\"end\":82205,\"start\":82204},{\"end\":82218,\"start\":82217},{\"end\":82235,\"start\":82234},{\"end\":82671,\"start\":82670},{\"end\":82679,\"start\":82678},{\"end\":82687,\"start\":82686},{\"end\":82694,\"start\":82693},{\"end\":82702,\"start\":82701},{\"end\":82710,\"start\":82709},{\"end\":83226,\"start\":83225},{\"end\":83233,\"start\":83232},{\"end\":83239,\"start\":83238},{\"end\":83245,\"start\":83244},{\"end\":83253,\"start\":83252},{\"end\":83260,\"start\":83259},{\"end\":83562,\"start\":83561},{\"end\":83571,\"start\":83570},{\"end\":83582,\"start\":83581},{\"end\":83584,\"start\":83583},{\"end\":83841,\"start\":83840},{\"end\":83851,\"start\":83850},{\"end\":84153,\"start\":84152},{\"end\":84171,\"start\":84170}]", "bib_author_last_name": "[{\"end\":67656,\"start\":67651},{\"end\":67664,\"start\":67660},{\"end\":67676,\"start\":67668},{\"end\":67904,\"start\":67891},{\"end\":67912,\"start\":67908},{\"end\":67922,\"start\":67916},{\"end\":68335,\"start\":68330},{\"end\":68342,\"start\":68339},{\"end\":68348,\"start\":68346},{\"end\":68354,\"start\":68352},{\"end\":68360,\"start\":68358},{\"end\":68367,\"start\":68364},{\"end\":68695,\"start\":68691},{\"end\":68703,\"start\":68699},{\"end\":68709,\"start\":68707},{\"end\":68717,\"start\":68713},{\"end\":68728,\"start\":68724},{\"end\":69002,\"start\":68996},{\"end\":69013,\"start\":69006},{\"end\":69028,\"start\":69017},{\"end\":69041,\"start\":69032},{\"end\":69051,\"start\":69045},{\"end\":69062,\"start\":69055},{\"end\":69076,\"start\":69066},{\"end\":69598,\"start\":69590},{\"end\":69613,\"start\":69602},{\"end\":69622,\"start\":69617},{\"end\":69633,\"start\":69626},{\"end\":69645,\"start\":69637},{\"end\":69993,\"start\":69989},{\"end\":70002,\"start\":69997},{\"end\":70013,\"start\":70006},{\"end\":70021,\"start\":70017},{\"end\":70220,\"start\":70218},{\"end\":70228,\"start\":70224},{\"end\":70235,\"start\":70232},{\"end\":70243,\"start\":70239},{\"end\":70250,\"start\":70247},{\"end\":70456,\"start\":70454},{\"end\":70464,\"start\":70460},{\"end\":70470,\"start\":70468},{\"end\":70716,\"start\":70711},{\"end\":70724,\"start\":70720},{\"end\":70730,\"start\":70728},{\"end\":70737,\"start\":70734},{\"end\":70745,\"start\":70741},{\"end\":70753,\"start\":70749},{\"end\":70762,\"start\":70757},{\"end\":71066,\"start\":71061},{\"end\":71073,\"start\":71070},{\"end\":71079,\"start\":71077},{\"end\":71085,\"start\":71083},{\"end\":71091,\"start\":71089},{\"end\":71098,\"start\":71095},{\"end\":71676,\"start\":71672},{\"end\":71682,\"start\":71680},{\"end\":71690,\"start\":71686},{\"end\":71706,\"start\":71694},{\"end\":71720,\"start\":71712},{\"end\":71732,\"start\":71724},{\"end\":72267,\"start\":72265},{\"end\":72275,\"start\":72271},{\"end\":72283,\"start\":72279},{\"end\":72289,\"start\":72287},{\"end\":72298,\"start\":72293},{\"end\":72306,\"start\":72302},{\"end\":72854,\"start\":72851},{\"end\":72860,\"start\":72858},{\"end\":72869,\"start\":72864},{\"end\":72876,\"start\":72873},{\"end\":72888,\"start\":72882},{\"end\":73151,\"start\":73146},{\"end\":73328,\"start\":73324},{\"end\":73340,\"start\":73334},{\"end\":73349,\"start\":73344},{\"end\":73831,\"start\":73825},{\"end\":73843,\"start\":73835},{\"end\":74274,\"start\":74268},{\"end\":74479,\"start\":74471},{\"end\":74624,\"start\":74620},{\"end\":74634,\"start\":74630},{\"end\":74645,\"start\":74638},{\"end\":74787,\"start\":74783},{\"end\":74793,\"start\":74791},{\"end\":74801,\"start\":74797},{\"end\":74809,\"start\":74805},{\"end\":74820,\"start\":74816},{\"end\":75306,\"start\":75300},{\"end\":75316,\"start\":75310},{\"end\":75503,\"start\":75497},{\"end\":75769,\"start\":75764},{\"end\":75776,\"start\":75773},{\"end\":75786,\"start\":75783},{\"end\":75797,\"start\":75790},{\"end\":75806,\"start\":75801},{\"end\":76096,\"start\":76092},{\"end\":76103,\"start\":76100},{\"end\":76111,\"start\":76107},{\"end\":76381,\"start\":76374},{\"end\":76554,\"start\":76552},{\"end\":76562,\"start\":76558},{\"end\":76571,\"start\":76566},{\"end\":76578,\"start\":76575},{\"end\":76584,\"start\":76582},{\"end\":76595,\"start\":76591},{\"end\":76991,\"start\":76989},{\"end\":76998,\"start\":76995},{\"end\":77004,\"start\":77002},{\"end\":77011,\"start\":77008},{\"end\":77018,\"start\":77015},{\"end\":77545,\"start\":77543},{\"end\":77552,\"start\":77549},{\"end\":77562,\"start\":77558},{\"end\":77570,\"start\":77568},{\"end\":78101,\"start\":78098},{\"end\":78108,\"start\":78105},{\"end\":78116,\"start\":78112},{\"end\":78128,\"start\":78122},{\"end\":78136,\"start\":78132},{\"end\":78367,\"start\":78363},{\"end\":78374,\"start\":78371},{\"end\":78383,\"start\":78378},{\"end\":78389,\"start\":78387},{\"end\":78395,\"start\":78393},{\"end\":78406,\"start\":78402},{\"end\":78943,\"start\":78937},{\"end\":78953,\"start\":78947},{\"end\":79437,\"start\":79434},{\"end\":79444,\"start\":79441},{\"end\":79756,\"start\":79754},{\"end\":79763,\"start\":79760},{\"end\":79769,\"start\":79767},{\"end\":79776,\"start\":79773},{\"end\":79783,\"start\":79780},{\"end\":80058,\"start\":80056},{\"end\":80065,\"start\":80062},{\"end\":80072,\"start\":80069},{\"end\":80078,\"start\":80076},{\"end\":80087,\"start\":80082},{\"end\":80101,\"start\":80091},{\"end\":80111,\"start\":80105},{\"end\":80118,\"start\":80115},{\"end\":80640,\"start\":80637},{\"end\":80649,\"start\":80644},{\"end\":80656,\"start\":80653},{\"end\":80664,\"start\":80662},{\"end\":80671,\"start\":80668},{\"end\":80677,\"start\":80675},{\"end\":81193,\"start\":81187},{\"end\":81206,\"start\":81197},{\"end\":81217,\"start\":81210},{\"end\":81588,\"start\":81583},{\"end\":81595,\"start\":81592},{\"end\":81604,\"start\":81599},{\"end\":81613,\"start\":81608},{\"end\":81620,\"start\":81617},{\"end\":81626,\"start\":81624},{\"end\":82193,\"start\":82190},{\"end\":82202,\"start\":82197},{\"end\":82215,\"start\":82206},{\"end\":82232,\"start\":82219},{\"end\":82244,\"start\":82236},{\"end\":82676,\"start\":82672},{\"end\":82684,\"start\":82680},{\"end\":82691,\"start\":82688},{\"end\":82699,\"start\":82695},{\"end\":82707,\"start\":82703},{\"end\":82716,\"start\":82711},{\"end\":83230,\"start\":83227},{\"end\":83236,\"start\":83234},{\"end\":83242,\"start\":83240},{\"end\":83250,\"start\":83246},{\"end\":83257,\"start\":83254},{\"end\":83266,\"start\":83261},{\"end\":83568,\"start\":83563},{\"end\":83579,\"start\":83572},{\"end\":83589,\"start\":83585},{\"end\":83848,\"start\":83842},{\"end\":83858,\"start\":83852},{\"end\":84168,\"start\":84154},{\"end\":84178,\"start\":84172},{\"end\":84853,\"start\":84847}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":58370896},\"end\":67828,\"start\":67592},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7285098},\"end\":68241,\"start\":67830},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":231984619},\"end\":68623,\"start\":68243},{\"attributes\":{\"doi\":\"arXiv:2105.10648\",\"id\":\"b3\"},\"end\":68914,\"start\":68625},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":59528301},\"end\":69520,\"start\":68916},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":929260},\"end\":69908,\"start\":69522},{\"attributes\":{\"id\":\"b6\"},\"end\":70156,\"start\":69910},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":202789109},\"end\":70382,\"start\":70158},{\"attributes\":{\"doi\":\"arXiv:2105.07775\",\"id\":\"b8\"},\"end\":70637,\"start\":70384},{\"attributes\":{\"doi\":\"arXiv:2105.06067\",\"id\":\"b9\"},\"end\":70972,\"start\":70639},{\"attributes\":{\"doi\":\"10.1145/3442381.3449788\",\"id\":\"b10\",\"matched_paper_id\":231984619},\"end\":71597,\"start\":70974},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":46949657},\"end\":72180,\"start\":71599},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":211043589},\"end\":72791,\"start\":72182},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7885409},\"end\":73140,\"start\":72793},{\"attributes\":{\"id\":\"b14\"},\"end\":73245,\"start\":73142},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3919301},\"end\":73771,\"start\":73247},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207238980},\"end\":74228,\"start\":73773},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":121049},\"end\":74440,\"start\":74230},{\"attributes\":{\"id\":\"b18\"},\"end\":74575,\"start\":74442},{\"attributes\":{\"id\":\"b19\"},\"end\":74741,\"start\":74577},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":150380651},\"end\":75255,\"start\":74743},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":225922639},\"end\":75469,\"start\":75257},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":17265929},\"end\":75666,\"start\":75471},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":117890116},\"end\":76041,\"start\":75668},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":215881974},\"end\":76320,\"start\":76043},{\"attributes\":{\"id\":\"b25\"},\"end\":76516,\"start\":76322},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13907106},\"end\":76890,\"start\":76518},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52228661},\"end\":77447,\"start\":76892},{\"attributes\":{\"id\":\"b28\"},\"end\":78013,\"start\":77449},{\"attributes\":{\"id\":\"b29\"},\"end\":78315,\"start\":78015},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":220347145},\"end\":78858,\"start\":78317},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":5575601},\"end\":79355,\"start\":78860},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":207204065},\"end\":79649,\"start\":79357},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14914495},\"end\":79970,\"start\":79651},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":207209998},\"end\":80537,\"start\":79972},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3473607},\"end\":81127,\"start\":80539},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":393948},\"end\":81484,\"start\":81129},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3331952},\"end\":82119,\"start\":81486},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4527045},\"end\":82587,\"start\":82121},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":224281017},\"end\":83142,\"start\":82589},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":54178962},\"end\":83522,\"start\":83144},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":7067369},\"end\":83800,\"start\":83524},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":38027149},\"end\":84120,\"start\":83802},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":5855042},\"end\":84345,\"start\":84122},{\"attributes\":{\"id\":\"b44\"},\"end\":84526,\"start\":84347},{\"attributes\":{\"id\":\"b45\"},\"end\":84843,\"start\":84528},{\"attributes\":{\"id\":\"b46\"},\"end\":84869,\"start\":84845}]", "bib_title": "[{\"end\":67647,\"start\":67592},{\"end\":67887,\"start\":67830},{\"end\":68326,\"start\":68243},{\"end\":68992,\"start\":68916},{\"end\":69586,\"start\":69522},{\"end\":70214,\"start\":70158},{\"end\":71057,\"start\":70974},{\"end\":71668,\"start\":71599},{\"end\":72261,\"start\":72182},{\"end\":72847,\"start\":72793},{\"end\":73320,\"start\":73247},{\"end\":73821,\"start\":73773},{\"end\":74264,\"start\":74230},{\"end\":74779,\"start\":74743},{\"end\":75296,\"start\":75257},{\"end\":75493,\"start\":75471},{\"end\":75760,\"start\":75668},{\"end\":76088,\"start\":76043},{\"end\":76370,\"start\":76322},{\"end\":76548,\"start\":76518},{\"end\":76985,\"start\":76892},{\"end\":77539,\"start\":77449},{\"end\":78094,\"start\":78015},{\"end\":78359,\"start\":78317},{\"end\":78933,\"start\":78860},{\"end\":79430,\"start\":79357},{\"end\":79750,\"start\":79651},{\"end\":80052,\"start\":79972},{\"end\":80633,\"start\":80539},{\"end\":81183,\"start\":81129},{\"end\":81579,\"start\":81486},{\"end\":82186,\"start\":82121},{\"end\":82668,\"start\":82589},{\"end\":83223,\"start\":83144},{\"end\":83559,\"start\":83524},{\"end\":83838,\"start\":83802},{\"end\":84150,\"start\":84122}]", "bib_author": "[{\"end\":67658,\"start\":67649},{\"end\":67666,\"start\":67658},{\"end\":67678,\"start\":67666},{\"end\":67906,\"start\":67889},{\"end\":67914,\"start\":67906},{\"end\":67924,\"start\":67914},{\"end\":68337,\"start\":68328},{\"end\":68344,\"start\":68337},{\"end\":68350,\"start\":68344},{\"end\":68356,\"start\":68350},{\"end\":68362,\"start\":68356},{\"end\":68369,\"start\":68362},{\"end\":68697,\"start\":68689},{\"end\":68705,\"start\":68697},{\"end\":68711,\"start\":68705},{\"end\":68719,\"start\":68711},{\"end\":68730,\"start\":68719},{\"end\":69004,\"start\":68994},{\"end\":69015,\"start\":69004},{\"end\":69030,\"start\":69015},{\"end\":69043,\"start\":69030},{\"end\":69053,\"start\":69043},{\"end\":69064,\"start\":69053},{\"end\":69078,\"start\":69064},{\"end\":69600,\"start\":69588},{\"end\":69615,\"start\":69600},{\"end\":69624,\"start\":69615},{\"end\":69635,\"start\":69624},{\"end\":69647,\"start\":69635},{\"end\":69995,\"start\":69987},{\"end\":70004,\"start\":69995},{\"end\":70015,\"start\":70004},{\"end\":70023,\"start\":70015},{\"end\":70222,\"start\":70216},{\"end\":70230,\"start\":70222},{\"end\":70237,\"start\":70230},{\"end\":70245,\"start\":70237},{\"end\":70252,\"start\":70245},{\"end\":70458,\"start\":70452},{\"end\":70466,\"start\":70458},{\"end\":70472,\"start\":70466},{\"end\":70718,\"start\":70709},{\"end\":70726,\"start\":70718},{\"end\":70732,\"start\":70726},{\"end\":70739,\"start\":70732},{\"end\":70747,\"start\":70739},{\"end\":70755,\"start\":70747},{\"end\":70764,\"start\":70755},{\"end\":71068,\"start\":71059},{\"end\":71075,\"start\":71068},{\"end\":71081,\"start\":71075},{\"end\":71087,\"start\":71081},{\"end\":71093,\"start\":71087},{\"end\":71100,\"start\":71093},{\"end\":71678,\"start\":71670},{\"end\":71684,\"start\":71678},{\"end\":71692,\"start\":71684},{\"end\":71708,\"start\":71692},{\"end\":71722,\"start\":71708},{\"end\":71734,\"start\":71722},{\"end\":72269,\"start\":72263},{\"end\":72277,\"start\":72269},{\"end\":72285,\"start\":72277},{\"end\":72291,\"start\":72285},{\"end\":72300,\"start\":72291},{\"end\":72308,\"start\":72300},{\"end\":72856,\"start\":72849},{\"end\":72862,\"start\":72856},{\"end\":72871,\"start\":72862},{\"end\":72878,\"start\":72871},{\"end\":72890,\"start\":72878},{\"end\":73153,\"start\":73144},{\"end\":73165,\"start\":73153},{\"end\":73330,\"start\":73322},{\"end\":73342,\"start\":73330},{\"end\":73351,\"start\":73342},{\"end\":73833,\"start\":73823},{\"end\":73845,\"start\":73833},{\"end\":74276,\"start\":74266},{\"end\":74481,\"start\":74469},{\"end\":74626,\"start\":74616},{\"end\":74636,\"start\":74626},{\"end\":74647,\"start\":74636},{\"end\":74789,\"start\":74781},{\"end\":74795,\"start\":74789},{\"end\":74803,\"start\":74795},{\"end\":74811,\"start\":74803},{\"end\":74822,\"start\":74811},{\"end\":75308,\"start\":75298},{\"end\":75318,\"start\":75308},{\"end\":75505,\"start\":75495},{\"end\":75771,\"start\":75762},{\"end\":75778,\"start\":75771},{\"end\":75788,\"start\":75778},{\"end\":75799,\"start\":75788},{\"end\":75808,\"start\":75799},{\"end\":76098,\"start\":76090},{\"end\":76105,\"start\":76098},{\"end\":76113,\"start\":76105},{\"end\":76383,\"start\":76372},{\"end\":76556,\"start\":76550},{\"end\":76564,\"start\":76556},{\"end\":76573,\"start\":76564},{\"end\":76580,\"start\":76573},{\"end\":76586,\"start\":76580},{\"end\":76597,\"start\":76586},{\"end\":76993,\"start\":76987},{\"end\":77000,\"start\":76993},{\"end\":77006,\"start\":77000},{\"end\":77013,\"start\":77006},{\"end\":77020,\"start\":77013},{\"end\":77547,\"start\":77541},{\"end\":77554,\"start\":77547},{\"end\":77564,\"start\":77554},{\"end\":77572,\"start\":77564},{\"end\":78103,\"start\":78096},{\"end\":78110,\"start\":78103},{\"end\":78118,\"start\":78110},{\"end\":78130,\"start\":78118},{\"end\":78138,\"start\":78130},{\"end\":78369,\"start\":78361},{\"end\":78376,\"start\":78369},{\"end\":78385,\"start\":78376},{\"end\":78391,\"start\":78385},{\"end\":78397,\"start\":78391},{\"end\":78408,\"start\":78397},{\"end\":78945,\"start\":78935},{\"end\":78955,\"start\":78945},{\"end\":79439,\"start\":79432},{\"end\":79446,\"start\":79439},{\"end\":79758,\"start\":79752},{\"end\":79765,\"start\":79758},{\"end\":79771,\"start\":79765},{\"end\":79778,\"start\":79771},{\"end\":79785,\"start\":79778},{\"end\":80060,\"start\":80054},{\"end\":80067,\"start\":80060},{\"end\":80074,\"start\":80067},{\"end\":80080,\"start\":80074},{\"end\":80089,\"start\":80080},{\"end\":80103,\"start\":80089},{\"end\":80113,\"start\":80103},{\"end\":80120,\"start\":80113},{\"end\":80642,\"start\":80635},{\"end\":80651,\"start\":80642},{\"end\":80658,\"start\":80651},{\"end\":80666,\"start\":80658},{\"end\":80673,\"start\":80666},{\"end\":80679,\"start\":80673},{\"end\":81195,\"start\":81185},{\"end\":81208,\"start\":81195},{\"end\":81219,\"start\":81208},{\"end\":81590,\"start\":81581},{\"end\":81597,\"start\":81590},{\"end\":81606,\"start\":81597},{\"end\":81615,\"start\":81606},{\"end\":81622,\"start\":81615},{\"end\":81628,\"start\":81622},{\"end\":82195,\"start\":82188},{\"end\":82204,\"start\":82195},{\"end\":82217,\"start\":82204},{\"end\":82234,\"start\":82217},{\"end\":82246,\"start\":82234},{\"end\":82678,\"start\":82670},{\"end\":82686,\"start\":82678},{\"end\":82693,\"start\":82686},{\"end\":82701,\"start\":82693},{\"end\":82709,\"start\":82701},{\"end\":82718,\"start\":82709},{\"end\":83232,\"start\":83225},{\"end\":83238,\"start\":83232},{\"end\":83244,\"start\":83238},{\"end\":83252,\"start\":83244},{\"end\":83259,\"start\":83252},{\"end\":83268,\"start\":83259},{\"end\":83570,\"start\":83561},{\"end\":83581,\"start\":83570},{\"end\":83591,\"start\":83581},{\"end\":83850,\"start\":83840},{\"end\":83860,\"start\":83850},{\"end\":84170,\"start\":84152},{\"end\":84180,\"start\":84170},{\"end\":84855,\"start\":84847}]", "bib_venue": "[{\"end\":67686,\"start\":67678},{\"end\":67992,\"start\":67924},{\"end\":68402,\"start\":68369},{\"end\":68687,\"start\":68625},{\"end\":69163,\"start\":69078},{\"end\":69691,\"start\":69647},{\"end\":69985,\"start\":69910},{\"end\":70259,\"start\":70252},{\"end\":70450,\"start\":70384},{\"end\":70707,\"start\":70639},{\"end\":71175,\"start\":71123},{\"end\":71830,\"start\":71734},{\"end\":72419,\"start\":72308},{\"end\":72941,\"start\":72890},{\"end\":73449,\"start\":73351},{\"end\":73943,\"start\":73845},{\"end\":74312,\"start\":74276},{\"end\":74467,\"start\":74442},{\"end\":74614,\"start\":74577},{\"end\":74933,\"start\":74822},{\"end\":75338,\"start\":75318},{\"end\":75554,\"start\":75505},{\"end\":75827,\"start\":75808},{\"end\":76151,\"start\":76113},{\"end\":76413,\"start\":76383},{\"end\":76659,\"start\":76597},{\"end\":77112,\"start\":77020},{\"end\":77668,\"start\":77572},{\"end\":78146,\"start\":78138},{\"end\":78519,\"start\":78408},{\"end\":79051,\"start\":78955},{\"end\":79480,\"start\":79446},{\"end\":79795,\"start\":79785},{\"end\":80201,\"start\":80120},{\"end\":80774,\"start\":80679},{\"end\":81281,\"start\":81219},{\"end\":81737,\"start\":81628},{\"end\":82311,\"start\":82246},{\"end\":82808,\"start\":82718},{\"end\":83317,\"start\":83268},{\"end\":83650,\"start\":83591},{\"end\":83921,\"start\":83860},{\"end\":84216,\"start\":84180},{\"end\":84432,\"start\":84347},{\"end\":84641,\"start\":84528},{\"end\":68047,\"start\":67994},{\"end\":68422,\"start\":68404},{\"end\":69235,\"start\":69165},{\"end\":71231,\"start\":71177},{\"end\":71913,\"start\":71832},{\"end\":72517,\"start\":72421},{\"end\":73534,\"start\":73451},{\"end\":74028,\"start\":73945},{\"end\":75031,\"start\":74935},{\"end\":76176,\"start\":76153},{\"end\":76708,\"start\":76661},{\"end\":77191,\"start\":77114},{\"end\":77751,\"start\":77670},{\"end\":78617,\"start\":78521},{\"end\":79134,\"start\":79053},{\"end\":80269,\"start\":80203},{\"end\":80856,\"start\":80776},{\"end\":81833,\"start\":81739},{\"end\":82363,\"start\":82313},{\"end\":82885,\"start\":82810},{\"end\":83969,\"start\":83923}]"}}}, "year": 2023, "month": 12, "day": 17}