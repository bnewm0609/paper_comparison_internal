{"id": 258762826, "updated": "2023-10-05 01:10:56.179", "metadata": {"title": "OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding", "authors": "[{\"first\":\"Minghua\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Ruoxi\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Kaiming\",\"last\":\"Kuang\",\"middle\":[]},{\"first\":\"Yinhao\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Xuanlin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Shizhong\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Hong\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Fatih\",\"last\":\"Porikli\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Su\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We introduce OpenShape, a method for learning multi-modal joint representations of text, image, and point clouds. We adopt the commonly used multi-modal contrastive learning framework for representation alignment, but with a specific focus on scaling up 3D representations to enable open-world 3D shape understanding. To achieve this, we scale up training data by ensembling multiple 3D datasets and propose several strategies to automatically filter and enrich noisy text descriptions. We also explore and compare strategies for scaling 3D backbone networks and introduce a novel hard negative mining module for more efficient training. We evaluate OpenShape on zero-shot 3D classification benchmarks and demonstrate its superior capabilities for open-world recognition. Specifically, OpenShape achieves a zero-shot accuracy of 46.8% on the 1,156-category Objaverse-LVIS benchmark, compared to less than 10% for existing methods. OpenShape also achieves an accuracy of 85.3% on ModelNet40, outperforming previous zero-shot baseline methods by 20% and performing on par with some fully-supervised methods. Furthermore, we show that our learned embeddings encode a wide range of visual and semantic concepts (e.g., subcategories, color, shape, style) and facilitate fine-grained text-3D and image-3D interactions. Due to their alignment with CLIP embeddings, our learned shape representations can also be integrated with off-the-shelf CLIP-based models for various applications, such as point cloud captioning and point cloud-conditioned image generation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.10764", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-10764", "doi": "10.48550/arxiv.2305.10764"}}, "content": {"source": {"pdf_hash": "432df08dc388bb08ab31847328b62b4632e5b7b7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.10764v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f40a49487466ff6396bb79454d9c2f7f717fc20f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/432df08dc388bb08ab31847328b62b4632e5b7b7.txt", "contents": "\nOpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding\n\n\nMinghua Liu \nUC San Diego\n\n\nRuoxi Shi \nShanghai Jiao Tong University\n\n\nKaiming Kuang \nUC San Diego\n\n\nYinhao Zhu \nQualcomm AI Research \u2020\n\n\nXuanlin Li \nUC San Diego\n\n\nShizhong Han \nQualcomm AI Research \u2020\n\n\nHong Cai \nQualcomm AI Research \u2020\n\n\nFatih Porikli \nQualcomm AI Research \u2020\n\n\nHao Su \nUC San Diego\n\n\nOpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding\n\nWe introduce OpenShape, a method for learning multi-modal joint representations of text, image, and point clouds. We adopt the commonly used multi-modal contrastive learning framework for representation alignment, but with a specific focus on scaling up 3D representations to enable open-world 3D shape understanding. To achieve this, we scale up training data by ensembling multiple 3D datasets and propose several strategies to automatically filter and enrich noisy text descriptions. We also explore and compare strategies for scaling 3D backbone networks and introduce a novel hard negative mining module for more efficient training. We evaluate OpenShape on zero-shot 3D classification benchmarks and demonstrate its superior capabilities for open-world recognition. Specifically, OpenShape achieves a zero-shot accuracy of 46.8% on the 1,156-category Objaverse-LVIS benchmark, compared to less than 10% for existing methods. OpenShape also achieves an accuracy of 85.3% on ModelNet40, outperforming previous zero-shot baseline methods by 20% and performing on par with some fully-supervised methods. Furthermore, we show that our learned embeddings encode a wide range of visual and semantic concepts (e.g., subcategories, color, shape, style) and facilitate fine-grained text-3D and image-3D interactions. Due to their alignment with CLIP embeddings, our learned shape representations can also be integrated with off-the-shelf CLIPbased models for various applications, such as point cloud captioning and point cloud-conditioned image generation.\n\nIntroduction\n\n3D shape understanding has recently garnered a surge of interest driven by the growing demands in real-world applications, such as augmented/virtual reality, autonomous driving, and robotics. Despite significant advancements in 3D recognition and analysis, existing data-driven approaches are still greatly limited by the scale of 3D training datasets and tend to exhibit poor generalization when facing unseen shape categories, hindering the deployment of existing models in real-world applications.\n\nNote that 3D shapes and 2D images can be easily linked through rendering, and the dataset scale issue of 2D images has been remarkably addressed, as shown in recent works such as CLIP [53]. Therefore, many recent studies aim to utilize pre-trained 2D image-language models [53,57] to assist 3D tasks, such as 3D generation [22,26,43,61,33,7] and 3D scene-level segmentation [18,27,14,76,39,47]. Regarding 3D shape-level understanding, a straightforward idea is to project 3D data to the 2D + + + + Figure 1: Left: Zero-shot shape classification on the Objaverse-LVIS (1,156 categories) and Model-Net40 datasets. OpenShape outperforms previous methods by a large margin. We exclude shapes in Objaverse-LVIS during training, and we also retrain ULIP [75] on our ensembled training shapes for fair comparison. Right: Our shape representations encode a broad range of semantic and visual concepts. We input two 3D shapes and use their shape embeddings to retrieve the top three shapes whose embeddings are simultaneously closest to both inputs. See Section. 4.4 for more details.\n\ndomain through rendering and use CLIP to analyze the 2D images, thereby enabling zero-shot 3D shape classification [82,84]. However, these methods suffer from occlusion and information loss during projection, and unnecessary latency due to point cloud rendering and multiple CLIP inferences.\n\nTo overcome the limitations caused by projection, it is necessary to train a 3D-native model by distilling knowledge from pretrained 2D models. However, training a 3D-native model requires a set of 3D shapes, and the amount of knowledge that can be distilled is determined by the size of the 3D dataset. For example, ULIP [75] aims to learn a joint representation space between language, 2D images, and 3D shapes, but uses a small-scale 3D dataset ShapeNetCore [8] for knowledge distillation. Specifically, ULIP fixes the 2D CLIP text and image encoders and trains a dedicated 3D-native point cloud encoder to extract 3D shape representations. The 3D encoder strives to align the 3D shape embedding space with the CLIP image and language embedding spaces by utilizing contrastive learning across all three modalities. However, since ULIP is only trained on 52K shapes of 55 object categories, it still struggles with out-of-distribution shape categories and fails to demonstrate an impressive open-world understanding of 3D shapes.\n\nIn this work, we propose a novel method called OpenShape, which follows a similar paradigm as ULIP but aims to achieve a more generalized and scalable joint representation space encompassing language, 2D images, and 3D shapes. Our focus mainly lies on scaling up representation learning and addressing corresponding challenges. In OpenShape, we emphasize four key factors during the training process: (a) data scale: we significantly increase the scale of 3D training data by combining four public 3D shape datasets, resulting in 876k 3D shapes covering much more diverse categories; (b) text quality: the 3D shapes from our main dataset, Objaverse [12], is dominated with inaccurate or uninformative text descriptions. Given the data scale, we propose three strategies to automatically filter and enrich the text descriptions; (c) 3D backbone scaling: since most existing 3D backbones target small datasets, we find that it's important but non-trivial to scale up the 3D backbones; and (d) data resampling: since the ensembled dataset is highly unbalanced, we utilize hard negative mining to improve the model's discriminative ability.\n\nWe first evaluate OpenShape on the zero-shot 3D shape classification task. As shown in Figure 1, OpenShape outperforms previous zero-shot approaches on the ModelNet40 dataset by at least 20%. Moreover, OpenShape excels at handling long-tail categories. On the challenging Objaverse-LVIS dataset, which contains 1,156 categories, OpenShape achieves a 46.8% accuracy, significantly surpassing previous methods. Notably, this performance gap remains even when ULIP is retrained on our ensembled datasets, highlighting the superiority of our text enrichment and training strategies. Besides zero-shot classification, we present demos that showcase the wide range of visual and semantic concepts learned by OpenShape. For example, in Figure 1-right, we take two 3D shapes as input and use their OpenShape embeddings to retrieve the top three shapes whose embeddings are simultaneously closest to both inputs from our ensembled dataset. The retrieved shapes exhibit an interesting combination of the semantic and geometric elements from both input shapes. Furthermore, since we align our 3D shape embedding space with the CLIP language and image embedding space, we demonstrate that OpenShape embeddings can be easily integrated with other CLIP-based models to perform cross-modality tasks such as point cloud captioning and point cloud-conditioned image generation.\n\n2 Related Work 2.1 CLIP for 3D Learning Image-language models like CLIP have achieved remarkable performance through large-scale imagetext pretraining [53,29,35,80,4,54,59]. As these models excel at capturing rich visual concepts and possess impressive zero-shot capabilities, they have been applied to various 3D vision tasks. For instance, numerous recent works utilize CLIP to facilitate zero-shot text-to-3D generation [22,26,43,61,33,7,32,5,28,74,38], typically through CLIP-guided per-scene optimization. From a recognition perspective, some works focus on scene-level representation, aiming to leverage CLIP priors for zero-shot 3D segmentation or detection in both indoor [18,27,14,76,39,47,79,23,58,81,31] and outdoor scenes [9,21]. Meanwhile, another line of work focuses on shape-level understanding, targeting zero-shot shape classification [82,84,51,75,19] and part segmentation [37,1]. There are two primary working paradigms for these methods. The first [82,84,24] involves using images as a medium representation, projecting 3D point clouds into 2D and employing 2D CLIP for inference. However, these methods typically suffer from occlusion and information loss during projection, along with unnecessary latency due to point cloud rendering and multiple 2D CLIP inferences. The second paradigm involves training a 3D-native encoder attempting to distill or fuse CLIP features into 3D representations. Our paper follows this paradigm.\n\n\n3D Shape Representation Learning\n\nVarious works have studied self-supervised pretraining for point clouds by designing pretext tasks [15,66,48,2,64] such as self-reconstruction [55,13,3,69], masked auto-encoding [46,77,20], distortion reconstruction [62,42,65], normal estimation [55], and contrastive learning [83,60,73]. These tasks enhance models' shape representations and improve their performance on downstream applications, although they do not involve multimodal semantic alignments during pretraining.\n\nRecently, some works [51,75,19], exemplified by ULIP [75], have explored learning multimodal joint representations for 3D shapes. They train 3D-native shape encoders by aligning 3D shape embeddings with CLIP's language and/or image embeddings through multimodal contrastive learning. Works like ReCon [51] further combines cross-modal contrastive learning with masked auto-encoding for added enhancement. While these methods allow for zero-shot 3D classification through the computation of 3D-text similarity, the amount of distilled knowledge and their model capability are heavily limited by the small-scale training datasets used. Our work follows this paradigm but aims to learn more generalizable and scalable representations to enable open-world 3D shape understanding.\n\n\nMethod\n\nWe propose a novel method, OpenShape, for learning generalizable and scalable multi-modal joint representation between language, 2D images, and 3D shapes, as shown in Figure 2. We first introduce the multi-modal contrastive learning framework we used for aligning representations of three modalities in Section 3.1. We then elaborate how we create our training sets and enrich our text data in Sections 3.2 and 3.3. In Section 3.4, we present how we scale up our 3D backbone models. Finally, we propose a hard negative mining strategy to enhance contrastive learning in Section 3.5.\n\n\nMulti-Modal Representation Alignment\n\nWe aim to learn 3D shape representations that are aligned with pretrained CLIP embedding spaces of language and image. As shown in Figure 2 (c), we train a 3D native encoder f P that takes a 3D point cloud as input and extracts 3D shape feature. Following previous works [51,75,19], such as ULIP [75], we utilize multi-modal contrastive learning for representation alignment. Since CLIP is pretrained on a much larger scale data, we freeze both its text encoder f T and its image encoder f I  Figure 2: (a) We ensemble four public 3D shape datasets, resulting in 876k shapes that encompass diverse categories and concepts. (b) We propose three strategies to automatically filter and enrich the noisy texts in the original datasets. (c) We train a 3D point cloud encoder to align the 3D shape embedding space with the CLIP's text and image embedding spaces. We perform cross-modal contrastive learning with scaled 3D backbones and hard negative mining. (d) OpenShape embeddings can be easily integrated with other CLIP-based models, enabling various cross-modality tasks.\n\nduring feature alignment to preserve CLIP's feature priors and avoid model collapse. Specifically, given a sampled batch of triplets {(P i , T i , I i )}, where P i denotes a point cloud of a 3D shape, T i and I i denote corresponding text and image, the contrastive loss is calculated as:\n\u2212 1 4n i log exp(h P i \u00b7 h T i /\u03c4 ) j exp(h P i \u00b7 h T j /\u03c4 ) + log exp(h T i \u00b7 h P i /\u03c4 ) j exp(h T i \u00b7 h P j /\u03c4 ) + log exp(h P i \u00b7 h I i /\u03c4 ) j exp(h P i \u00b7 h I j /\u03c4 ) + log exp(h I i \u00b7 h P i /\u03c4 ) j exp(h I i \u00b7 h P j /\u03c4 )(1)\nwhere n is the number of shapes in a batch; \u03c4 is a learnable temperature;\nh P i = f P (P i )/|f P (P i )|, h T i = g T (f T (T i ))/|g T (f T (T i ))|, and h I i = g I (f I (I i ))/|g I (f I (I i )\n)| denote normalized projected features of P i , T i , and I i , where g T and g I are two learnable linear projections. Since f T and f I are frozen, we extract all f T (T i ) and f I (I i ) before training and cache them for acceleration. In most of our experiments, we utilize OpenCLIP ViT-bigG-14 [25] as the pretrained CLIP model.\n\n\nEnsembling 3D Datasets\n\nSince the scale and diversity of training triplets play a crucial role in learning scalable shape representations, we ensemble four currently-largest public 3D datasets for training as shown in Figure 2 (a), resulting in 876k training shapes. Among these four datasets, ShapeNetCore [8], 3D-FUTURE [16] and ABO [11] are three popular datasets used by prior works. They contain human-verified high-quality 3D shapes, but only cover a limited number of shapes and dozens of categories. The Objaverse [12] dataset is a more recent dataset, containing many more 3D shapes and covering significantly more diverse categories. However, shapes in Objaverse are mainly uploaded by web users and not verified by experts, and thus have uneven quality and exhibit highly unbalanced distributions, necessitating further processing.\n\nTo create triplets for training, for each shape, we sample 10,000 points from the mesh surface and interpolate the point colors according to the mesh textures. We also render 12 color images from the preset camera poses that uniformly cover the whole shape. For datasets providing thumbnails, we include them as part of image candidates, since they typically capture the shape from a better camera view. For the Objaverse dataset, we use the model name as the raw text for each shape. For other datasets, we utilize provided metadata to create raw texts (see supplementary for details). During each pretraining iteration, we randomly sample one rendered image or thumbnail for each shape, and apply standard augmentation to the point clouds [75].\n\n\nText Filtering and Enrichment\n\nWe find that only applying contrastive learning between 3D shapes and 2D images is insufficient to fuel zero-shot 3D classification, even when training on large-scale datasets. We conjecture that this is caused by the inherent domain gap in CLIP's language and image embedding spaces, which is also observed by previous studies [36,67]. Consequently, 3D-text alignment is not guaranteed even if we obtain good 3D-image alignments via contrastive learning. Therefore, we need to explicitly align 3D shapes with text. Along this process, to facilitate better 3D-text alignment, we introduce 3 techniques to improve the text quality: filtering, captioning, and image retrieval, as shown in Figure 2 (b).\n\nFiltering. As shown in Figure 3, the 3D shapes from our main dataset, Objaverse, is dominated with noisy text descriptions (\"names\") uploaded by web users. Many of the problematic texts can be identified from the text itself without seeing the corresponding 3D shape. We thus leverage a powerful   Figure 4: Accuracy on Objaverse-LVIS [12] when scaling up the parameters of different models.\n\nlarge language model, GPT-4 [45], to filter out inaccurate or uninformative text descriptions. We find that GPT-4 excels at recognizing irrelevant contents, such as timestamps, pure model numbers, incomprehensible descriptions, random filenames (e.g., new project), and random characters. Through GPT-4, we filter out about 30% of raw user texts. Note that we only filter the texts, and still keep all shapes for training. More details, such as the prompts we used, are presented in the supplementary.\n\nCaptioning. We utilize BLIP [34] and the Azure cognition services to caption the 2D thumbnails (if present, or images rendered from a fixed frontal view) of the 3D models, obtaining two texts for each shape. As shown in Figure 3, the captioning models can usually produce meaningful and descriptive captions that either enhance user-uploaded texts or replace low-quality ones. We also notice that the two caption models complement each other, leading to better performance.\n\nImage Retrieval. In addition to image captioning, we also perform image retrieval to obtain additional descriptions of 3D models. We retrieve k-NN images of shape renderings from the LAION-5B dataset [63] using the CLIP ViT-L retrieval index [6]. We then take the captions of the k-NN images as the retrieved texts for our 3D models. Compared with captioning model generations, retrieved texts cover a wider range of text styles. They can also include more fine-grained semantics than both the user texts and the generated captions (e.g., \"Labrador\" in Figure 3).\n\nIn each iteration of pretraining, for each shape, we first randomly sample a text source category among the raw text (if unfiltered), the captions, and the retrieved texts. We then select a text candidate from the selected category. We also apply the template-based prompt engineering technique used in ULIP [75] to both training texts and test-time category names. Specifically, we extend a word or a phrase to a collection of templated simple sentences and take their average embedding.\n\n\nScaling Up 3D Point Cloud Backbones\n\nPrevious works on 3D point cloud learning have primarily focused on smaller-scale datasets like ShapeNet. These techniques may not be directly applicable to our larger-scale ensembled dataset and need to be scaled up accordingly. We find that different 3D backbones may exhibit distinct behavior and scalability when trained on datasets with varying sizes. Specifically, we compare six popular backbones trained on ShapeNet or our ensembled dataset by evaluating their zero-shot classification performance on ModelNet40 [72] and Objaverse-LVIS datasets (for now, these backbones are trained with their original configurations and without scaling up model sizes). Objaverse-LVIS is a subset of Objaverse dataset with human-verified category labels. With 1,156 categories, it serves as a suitable dataset for evaluating zero-shot long-tail classification, and we exclude all shapes of Objaverse-LVIS from this experiment. Results are shown in Table 1. We find that when trained on ShapeNet, all backbones share similar performances. However, when trained on our ensembled dataset, the performance gap between backbones increases significantly. This suggests that while the original versions of these backbones share a similar number of parameters, some may have been saturated when trained on small datasets, while others do not.\n\nWe also explore the performance and scalability of these backbones when scaling up the model sizes and training on our ensembled dataset. Please refer to the supplementary for details on how we scale up each model. As shown in Figure 4, we observe that all 3D backbones benefit significantly from model scaling. However, traditional backbones without a shrinking hierarchical structure, such as DGCNN and PointNet, require operating completely on dense points or modeling the relationships (e.g., through kNN) between dense points. As a result, they become more time-consuming and memory-intensive when scaled up compared to more modern backbones. We therefore select PointBERT [78] (Transformer-based) and SparseConv [10] (convolution-based) as our 3D backbones for the remaining experiments, as they exhibit strong performance and scalability.\n\n\nHard Negative Mining\n\nOur ensembled dataset exhibits a high degree of class imbalance. Certain common categories, such as building, may occupy tens of thousands of shapes, while many other categories, such as walrus and wallet, are underrepresented with only a few dozen or even fewer shapes. Consequently, when randomly constructing batches, it is unlikely that shapes from two confusing categories (e.g., apples and cherries) will be contrasted within the same batch. Inspired by some previous works [56,30], we propose an offline hard negative mining strategy for improving the training efficiency and performance. Specifically, in the first round of training, we train our model with random batches until it is about to converge. We then compute the kNN for each shape in the learned 3D embedding space. In the second round of training, for each iteration, we randomly select s seed shapes and then obtain m neighbors from the kNN results of each seed shape, resulting s \u00d7 m shapes per batch. In this way, confusing pairs are more likely to be selected in a single batch. However, this may also introduce false negative pairs (e.g., two apples) into contrastive learning. To mitigate this issue, we leverage image and text embeddings to filter out pairs sharing similar texts when calculating the contrastive loss. Specifically, for two shapes i and j selected from the same seed shape, if h T j \u00b7 h I i + \u03b4 > h T i \u00b7 h I i , where h T and h I are text and image embeddings, and \u03b4 is a small threshold, we believe that the text embeddings of i and j are very close to each other, and we remove j from i's negative examples when calculating contrastive loss. By employing this strategy to construct batches, we observe faster and better model learning.\n\n\nExperiments\n\n\nZero-Shot Shape Classification\n\nWe evaluate the zero-shot classification performances of our models on three benchmarks: the traditional ModelNet40 [72] and ScanObjectNN [68], as well as a new benchmark, Objaverse-LVIS [12]. ModelNet40 and ScanObjacetNN consist of 40 and 15 common categories, respectively. Objaverse-LVIS is an annotated subset of Objaverse [12] and comprises 46,832 shapes among 1,156 LVIS [17] categories. With a much larger base of classes than other benchmarks, Objaverse-LVIS presents a challenging long-tailed distribution, making it a better reflection on models' performance in open-world scenarios. We compare OpenShape with existing zero-shot approaches, including PointCLIP [82], PointCLIPv2 [84], ReCon [51], CG3D [19], CLIP2Point [24], and ULIP [75]. Among them, PointCLIP [82] and PointCLIPv2 [84] project point clouds into 2D images and directly utilize 2D CLIP for inference, while other methods leverage the CLIP embedding spaces for   Figure 5: Few-shot linear probing on Objaverse-LVIS [12], ModelNet40 [72], and ScanOb-jectNN [67]. We report the average performance over 10 random seeds. alignment and require 3D shapes for training. We report results on these baselines using their released checkpoints. To better analyze the source of our performance gains, we also retrain the baseline ULIP [75] on our ensembled shape dataset, but we use the original texts in the four constituent datasets along with the official codebase without backbone scaling. We train OpenShape and ULIP on three different sets of training shapes: \"Ensembled\" denotes using all shapes from the four datasets; \"Ensembled (no LVIS)\" is the same but excludes all shapes from the Objavserse-LVIS subset; \"ShapeNet\" only includes shapes from the ShapeNet [8] dataset. Note that even when LVIS shapes are included in the training shapes (i.e., the \"Ensembled\" dataset), their test-time category labels are probably not included in their training texts. Please refer to the supplementary for more training and evaluation details. Table 2 shows the results. We observe that OpenShape consistently outperforms prior approaches, even when trained only on ShapeNet. When models are trained on our larger-scale ensembled dataset, they receive a significant performance boost. In this case, OpenShape still surpasses retrained ULIP by a significant margin, demonstrating the advantages of our text enrichment, backbone scaling, and other training strategies. Specifically, OpenShape greatly improves the classification accuracy on the long tail categories in Objaverse-LVIS from a dull < 10% to 46.8%, outperforming the retrained ULIP by about 20 points and reaching a decent top-5 accuracy of 77.0%. These results demonstrate OpenShape's capability to recognize open-world objects effectively. As for ModelNet40, OpenShape achieves a 85.3% accuracy, surpassing previous methods by a substantial margin of at least 20 percent. OpenShape also achieves impressive top-3 and top-5 accuracies of 96.5% and 98.0%. To the best of our knowledge, this is the first time zero-shot methods have matched the performance of a fullysupervised 3D learning method on ModelNet40, where OpenShape outperforms fully-supervised 3D ShapeNets [72] and VoxNet [41]. In addition, on ScanObjectNN, which contains challenging real scans with noise and occlusion, OpenShape exhibits decent sim-to-real transfer capabilities. To contextualize, OpenShape-SparseConv achieves 56.7% zero-shot accuracy on ScanObjectNN without specific sim-to-real training, which surpasses 52.7% reported by SKPConv [71], a recent method specially designed for sim-to-real transfer in point cloud classification tasks.  Figure 7: Ablation study on different text enrichment strategies. \n\n\nFew-Shot Linear Probing\n\nIn the literature, linear probing is a common way to assess the representation learning capabilities of a model. To perform linear probing, we gather and freeze the representation vectors from all samples in a dataset. Subsequently, we train a linear classifier using these fixed vectors and few-shot class labels. We evaluate the accuracy of the linear classifier on three benchmarks: Objaverse-LVIS [12], ModelNet40 [72], and ScanObjectNN [68]. Figure 5 summarizes the performance of OpenShape in comparison with ULIP [75] (official release and our retrained versions) and PointCLIPv2 [84].\n\nOn the most challenging Objaverse-LVIS benchmark, OpenShape outperforms all other methods by a large margin. Notably, zero-shot OpenShape beats few-shot linear probes of other methods. On ModelNet40 and ScanObjectNN, we do not see a large performance margin between OpenShape and retrained ULIP. We hypothesize that for few-shot ModelNet40, the error is dominated by in-category sample bias rather than the representation quality; while for ScanObjectNN, the domain gap plays a major role. Since both OpenShape and retrained ULIP are exposed to the same source domain of training objects, their few-shot out-of-domain generalization performances tend to be similar.\n\n\nAblation Study\n\nWe perform various ablations by training a scaled version of SparseConv [10] on the ensembled dataset and then evaluate it on the Objaverse-LVIS [12] and ModelNet40 [72] zero-shot classification benchmarks, unless otherwise specified. The results are shown in Table 3 and Figures 6 and 7.\n\nData and Model Scaling. We investigate the impact of training data by ablating (1) without or with only Objaverse shapes (Tab. 3) and (2) with different ratios of our ensembled dataset (Fig. 6). We observe that training with 1% of our ensembled dataset (about 8.8k shapes) achieves similar or better zero-shot performance than training without Objaverse shapes (about 77.1k shapes), indicating that the diversity of training data is sometimes more crucial than the scale. In addition, we compare the performances between scaled-up and non-scaled-up backbones. From Tab. 3, we demonstrate that model scaling plays an essential role when training on our large-scale ensembled dataset (also Fig. 4).\n\nText Filtering and Enrichment. As shown in Tab. 3, both text filtering and text enrichment are beneficial for performance. We also investigate the specific text enrichment strategies to use for the SparseConv and PointBERT backbones. In Fig. 7, we observe that both image captioning and text retrieval are helpful, and including both yield the best results. Notably, PointBERT improves more than 10 points from text enrichment, highlighting the significance of enhancing text quality.\n\nOther Aspects. We also conduct additional ablation studies on color information, contrastive loss components, and our hard-negative mining strategy in Tab. 3. We observe that OpenShape performs well with only xyz coordinates as input and no RGB color. While 3D-image contrastive loss is also helpful, we observe that 3D shape-text alignment plays a very essential role for model zero-shot generalization, which necessitates our text filtering and text enrichment strategies that significantly Figure 9: Text-input 3D shape retrieval. In each row, we show input texts on the left and two retrieved shapes for each text on the right. OpenShape embedding encodes a wide range of visual and semantic concepts and enables (a) retrieval of fine-grained subcategories (first two rows), and (b) control of attributes (e.g., color, shape, style) and their combinations (last two rows).\n\n\n+ \"in a large desert\"\n\n+ \"in the woods\"\n1. 2.\n3.\n\n\n4.\n\nCap\ufffdons:\n\n1. The chair in the style of the 1920s.  enhance text quality. Lastly, by employing our hard negative mining strategy, OpenShape effectively addresses the issue of unbalanced data distribution, leading to further improvements in performance.\n\n\nCross-Modal Applications\n\nMulti-modal 3D Shape Retrieval. Through OpenShape multi-modal representations, we can index and retrieve 3D shapes from images, texts, or point clouds. In this section, we retrieve 3D shapes from our ensembled dataset by calculating the cosine similarity between input embedding(s) and 3D shape embeddings and performing kNN. As shown in Figure 8, OpenShape is capable of retrieving visually or semantically similar shapes from a single image or point cloud input. OpenShape embeddings encode a wide range of visual and semantic concepts. In Figure 9, we show that OpenShape supports retrieving 3D shapes from detailed text descriptions, which include fine-grained subcategories, attributes, and their combinations. Note that these input texts are typically not present in the raw texts of the retrieved shapes, indicating that OpenShape effectively learns generalizable concepts across shapes. In Figure 1, we provide a demo which takes two 3D shapes as inputs and retrieves the shapes that are simultaneously closest to both inputs. This is achieved by finding arg max i min(h P i \u00b7 h P a , h P i \u00b7 h P b ), where h P a and h P b denote normalized shape embeddings of the two input shapes. We can see that the retrieved shapes integrate visual or semantic elements in an interesting manner, highlighting the rich concepts and priors encoded in OpenShape embeddings.\n\nShape-Conditioned Multimodal Generation. As OpenShape's 3D shape representations are aligned with CLIP's image and text embedding spaces, they can serve as inputs into other CLIP-based models to facilitate various multimodal generation applications. For example, we show that by feeding our 3D shape embeddings into ClipCap [44], an off-the-shelf image captioning model, along with Stable unCLIP [54], a text-to-image diffusion model, we can perform point cloud captioning and point cloud-conditioned image generation (optional text prompt supported) without extra training or finetuning. Qualitative results are shown in Figure 10. Please refer to the supplementary for more results and details.\n\n\nDiscussion and Conclusion\n\nWe introduce OpenShape, a novel approach for learning scalable and generalizable multi-modal joint representations for 3D shapes. OpenShape representations effectively capture a wide range of semantic and visual concepts, enabling superior capabilities for open-world 3D shape recognition. By aligning OpenShape with CLIP's embedding space, our shape embeddings can be integrated with off-the-shelf CLIP-based models for various cross-modality applications. Moving forward, there are several directions worth further exploration: (a) More 3D data. While we utilized 876k 3D shapes during training, this is still quite limited compared to the 2D counterparts. We hope that our work inspires future investments in more resources to build even more powerful 3D representations. (b) Part-level information. Our current shape representations mainly focus on global semantic and visual features, and it would be beneficial to add more part-level supervision during training. (c) Sim-to-real domain gap. Our model is mainly trained on synthetic data, and it's challenging but crucial to explore explicit designs for reducing the domain gap with real-world shapes.\n\n\nAppendix\n\n\nMore Examples of Multi-Modal 3D Shape Retrieval\n\nIn Figures 11 and 12, we showcase more examples of multi-modal 3D shape retrieval. Figure 11: Image-input 3D shape retrieval. In each triplet, we present the input image and two 3D shapes retrieved using OpenShape embeddings from the Objaverse [12] dataset. Input images are from unsplash.com. Figure 12: Point cloud-input 3D shape retrieval. In each triplet, we present the input point cloud and two 3D shapes retrieved using OpenShape embeddings from the Objaverse [12] dataset.\n\n\nMore Examples of Shape-Conditioned Multimodal Generation\n\nIn Figure 13 and Figure \n\n\nDetails on Raw Text Generation and Filtering\n\n\nRaw Text Generation\n\nWe leverage the metadata from the four datasets to generate the raw texts. Although the original datasets may contain numerous attributes for each shape, we carefully choose the most informative ones to compose the text, ensuring its quality and relevance.\n\nObjaverse:We utilize the name associated with each shape to serve as the text.\n\nShapeNetCore: For each shape, we generate three types of texts: (a) the name, (b) the category name (with a total of 55 categories), and (c) the concatenation of the sub-category names (with a total of 336 sub-categories), separated by commas.\n\n3DFuture: For each shape, we generate two types of texts: (a) the category, and (b) the concatenation of category, style, theme, and material, separated by commas.\n\nABO: For each shape, we generate two types of texts: (a) the item_name, and (b) the product_type.\n\nIn this way, we generate one or more raw texts for each shape.\n\n\nRaw Text Filtering\n\nWe employ GPT-4 [45] to filter out uninformative raw texts. To accomplish this, we divide all the raw texts into batches, each containing 256 entries, and process each batch independently using GPT-4.\n\nHere is an example illustrating the prompt we used and the corresponding response generated by GPT-4.\n\nI am analyzing a 3D dataset with various text descriptions for the 3D models. However, many of these texts are inaccurate or uninformative, and therefore, not suitable as descriptions for 3D models. I need your help to identify such incorrect texts. Specifically, if a text primarily consists of irrelevant or uninformative content, such as timestamps, model numbers, incomprehensible descriptions, random filenames (e.g., \"my project\"), random characters, etc., please respond with \"N\". If a text contains a clear noun (or noun phrase) that could potentially describe a 3D object, please respond with \"Y\". You will find a list of texts below, and each line contains a three-digit ID and associated text. For each text, please respond with \"Y\" or \"N\", following the ID number (e.g., \"001 Y\" or \"002 N\" \n\n\nDetails on the Backbone Scaling Experiment\n\nIn Figure 4 of the main paper, we investigate the performance and scalability of various backbones when scaling up their model sizes. For this experiment, we employ a default resolution of 10,000 points for input point clouds, a batch size of 200, and conduct the experiment on a single A100 GPU. In general, if instructions are given in the original paper of a backbone, we scale up the model as instructed. Otherwise, we scale up the model by expanding width or depth (i.e., stacking blocks or layers). Specifically, we scale up each backbone as follow:\n\nPointBERT [78] The scaling parameters are shown in Table 4. We scaled PointBERT to 72.1M parameters beyond the 32.3M version reported in Figure 4 of the main paper. However, at this scale, the model dramatically overfits on the training data and performs worse on all benchmarks than the 32.3M version.  Table 5 for the specific scaling parameters. PointNeXt [52] PointNeXt is proposed as a scalable version of PointNet++ [50], and includes S/B/L/XL variants in the original paper. We simply adopt these official configurations.\n\nDGCNN [70] and PointNet [49] For these two backbones without a hierarchical structure, we increase the width of each layer proportionally to scale up to 4xPointNet and 2xDGCNN before we hit the GPU memory limit. As the models operate completely on dense points, it is impractical to use the default 10k-point resolution. We thus reduce the input resolution for the two backbones, resulting in 1k points for DGCNN and 4k points for PointNet.\n\n\nDetails on Training and Evaluation\n\nTraining Details We freeze the CLIP text and image encoders and train the 3D encoder and two projection heads on our ensembled dataset using the cross-modal contrastive loss. We train the model on a single A100 GPU with a batch size of 200. Since we precache the text and image CLIP embeddings of all shapes, the training is greatly accelerated and takes about 300 A100 hours for convergence. We utilize an exponential learning rate schedule, and employ an range test to find the initial learning rate. For 32.3M version of PointBERT, we utilize a learning rate of 5e \u2212 4; for 72.1M version of PointBERT, we utilize a learning rate of 4e \u2212 4; and for other models, we utilize a learning rate of 1e \u2212 3. For hard-negative mining, the number of seed shapes s is set to 40, and the number of neighbors m is set to 5 per shape, and the threshold \u03b4 is set to 0.1.\n\nFine-tuning CLIP Text and Image Encoders? After training OpenShape-PointBERT, we conducted experiments to unfreeze and finetune the CLIP text encoder for a single epoch. However, the results obtained did not demonstrate any noticeable improvement on the benchmarks. Moreover, we observed that finetuning the CLIP text encoder could potentially undermine the generalization capabilities of CLIP and hinder the integration of OpenShape embeddings into existing CLIP-based models. As a result, we choose to freeze the CLIP encoders throughout the entire training process.\n\nEvaluation Details We evaluated all baselines using their publicly released pretrained checkpoints. Additionally, we retrained ULIP [75] on our ensembled training shapes using their official code base and backbone networks. Note that the retrained ULIP model utilized the original raw texts from the four datasets during training (prompt engineering is also applied), rather than our filtered and enriched texts. For ModelNet40 [72], the evaluation is conducted on the test split with 2,468 shapes. Regarding ScanObjectNN [68], we follow ULIP [75] to evaluate on the OBJ_ONLY version, which contains 581 test shapes. For Objaverse-LVIS [12], the input is 10,000 sampled points with point colors. For ModelNet40 [72], the input is 10,000 sampled points without color. For ScanObjectNN [68], we utilize the official 2,048 points without color as input. All methods use the same input during evaluation. The forward inference time on an A100 GPU for a 10,000-point point cloud is approximately 0.9ms for OpenShape-SparseConv and 3.8ms for OpenShape-PointBERT.\n\n\nDetails on Shape-Conditioned Multimodal Generation\n\nPoint Cloud Captioning CLIPCap [44] utilizes a 10-token prefix generated from CLIP image embeddings to enable GPT-2 for captioning. In order to align with the off-the-shelf CLIPCap model, we trained a variant of OpenShape-PointBERT that employs CLIP ViT-B/32 embeddings instead of OpenCLIP ViT-G/14 used in other experiments. Consequently, we directly input the point cloud encoding, without normalization, into CLIPCap for captioning.\n\n\nPoint Cloud Conditioned Image Generation\n\nWe take the Stable Diffusion v2.1 unCLIP model [54] for image generation and replace the CLIP image condition encoder with our OpenShape encoder to perform image generation conditioned on point clouds (and optionally text prompts). The unCLIP model takes CLIP ViT-L/14 embeddings without normalization as input. To match the embedding space, we trained a variant of OpenShape-PointBERT with CLIP ViT-L/14 embeddings. Additionally, we noticed a significant mismatching of scales (L 2 -norm of embedding vectors) between ViT-L/14 image embeddings and OpenShape embeddings. To mitigate this issue, we perform a re-normalization on OpenShape embeddings to a L 2 -norm of 1 2 \u221a 768, which is our observed mean L 2 -norm of ViT-L/14 image embeddings. We use 50 diffusion steps. The guidance scale can be tuned freely.\n\n\"\"Figure 3 :\n3Steampunk Goggles by MonoFlow on \u2026\" \"Steampunk Goggles Made from hand ...\" azure: \"a pair of steampunk goggles\" blip: \"steampunk goggles 3d model\" Black Labrador in front of a white \u2026\" \"Black Labrador puppy Vinyl Wall Mural\" azure: \"a black dog sitting on a blue...\" blip: \"a black dog sitting on a blue...\" name: \"untitled\" GPT4: Remove \"Nike AirMax 1 (Red/White)\" azure: \"a close up of a shoe\" blip: \"nike air max 1 -white / red\" \"nike air max red and white\" Text Filtering & Enrichment Examples In each example, the left section features the thumbnail, model name, and GPT-4 filtering results. The upper right section shows image captions from two captioning models, while the lower right section displays retrieved images and their corresponding texts.\n\nFigure 8 :\n83D shape retrieval from image (left, mid) and point cloud (right).\n\n2 .\n2The ladder to the second floor. 3. Goldfish in the sea -photo #. 4. The car of the day.\n\nFigure 10 :\n10(a) Point cloud captioning. (b) Point cloud-conditioned image generation. Our learned 3D shape embeddings can be integrated with off-the-shelf pretrained CLIP-based models (e.g., captioning and image generation models) to support various cross-modal applications.\n\n\n14, we showcase more examples of point cloud captioning and point cloud-conditioned image generation.\n\nFigure 13 :\n13Point cloud captioning. In each row, we show the input point clouds on the left and the generated captions on the right.\n\nFigure 14 :\n14Point cloud-conditioned image generation. Each row shows three examples (input point clouds and generated images).\n\nTable 1 :\n1Comparison of different 3D backbones before scaling up their parameters. Models are trained on ShapeNet[8] or our ensembled dataset excluding Objaverse-LVIS[12]. Zero-shot classification performance are evaluated on Model-Net40[72] and Objaverse-LVIS[12].Train on ShapeNet [8] Train on Ens-no-LVIS MNet40 O-LVIS MNet40 O-LVISModel \n#Param. \n\nPointNet [49] \n1.3M \n67.0 \n9.3 \n74.9 \n24.4 \nDGCNN [70] \n2.3M \n67.8 \n9.0 \n74.2 \n24.8 \nPointMLP [40] 9.3M \n73.5 \n12.9 \n82.9 \n36.6 \nPointNeXt [52] 2.8M \n72.6 \n12.2 \n81.6 \n33.8 \nPointBERT [78] 5.1M \n70.3 \n10.8 \n84.5 \n37.0 \nSparseConv [10] 5.3M \n70.7 \n10.6 \n78.8 \n31.7 \n\nstd. dev. \n2.3 \n1.4 \n3.9 \n5.1 \n\n0 \n0 \n0 \n0 \n\n3DUDPHWHUV \n\n2/9,6$FF \n\n'*&11 \n3RLQW1HW \n3RLQW1H;W \n3RLQW%(57 \n6SDUVH&RQY \n\n\n\nTable 2 :\n2Zero-shot classification on Objaverse-LVIS[12], ModelNet40[72], and ScanObjectNN[67].Method \n\ntraining shape \nObjaverse-LVIS [12] \nModelNet40 [72] \nScanObjectNN [68] \n\nsource \nTop1 Top3 Top5 Top1 Top3 Top5 Top1 Top3 Top5 \n\nPointCLIP [82] \n2D inferences, \nno 3D Training \n\n1.9 \n4.1 \n5.8 \n19.3 \n28.6 \n34.8 \n10.5 \n20.8 \n30.6 \nPointCLIP v2 [84] \n4.7 \n9.5 \n12.9 \n63.6 \n77.9 \n85.0 \n42.2 \n63.3 \n74.5 \n\nReCon [51] \n\nShapeNet \n\n1.1 \n2.7 \n3.7 \n61.2 \n73.9 \n78.1 \n42.3 \n62.5 \n75.6 \nCG3D [19] \n5.0 \n9.5 \n11.6 \n48.7 \n60.7 \n66.5 \n42.5 \n57.3 \n60.8 \nCLIP2Point [24] \n2.7 \n5.8 \n7.9 \n49.5 \n71.3 \n81.2 \n25.5 \n44.6 \n59.4 \nULIP-PointBERT (Official) [75] \n6.2 \n13.6 \n17.9 \n60.4 \n79.0 \n84.4 \n51.5 \n71.1 \n80.2 \nOpenShape-SparseConv \n11.6 \n21.8 \n27.1 \n72.9 \n87.2 \n93.0 \n52.7 \n72.7 \n83.6 \nOpenShape-PointBERT \n10.8 \n20.2 \n25.0 \n70.3 \n86.9 \n91.3 \n51.3 \n69.4 \n78.4 \n\nULIP-PointBERT (Retrained) \nEnsembled \n21.4 \n38.1 \n46.0 \n71.4 \n84.4 \n89.2 \n46.0 \n66.1 \n76.4 \nOpenShape-SparseConv \n(no LVIS) \n37.0 \n58.4 \n66.9 \n82.6 \n95.0 \n97.5 \n54.9 \n76.8 \n87.0 \nOpenShape-PointBERT \n39.1 \n60.8 \n68.9 \n85.3 \n96.2 \n97.4 \n47.2 \n72.4 \n84.7 \n\nULIP-PointBERT (Retrained) \nEnsembled \n\n26.8 \n44.8 \n52.6 \n75.1 \n88.1 \n93.2 \n51.6 \n72.5 \n82.3 \nOpenShape-SparseConv \n43.4 \n64.8 \n72.4 \n83.4 \n95.6 \n97.8 \n56.7 \n78.9 \n88.6 \nOpenShape-PointBERT \n46.8 \n69.1 \n77.0 \n84.4 \n96.5 \n98.0 \n52.2 \n79.7 \n88.7 \n\nObjaverse-LVIS \nModelNet40 \nScanObjectNN \n\n\n\nTable 3 :\n3Ablation study. Top 1 zeroshot accuracies on ModelNet40[72] and Objaverse-LVIS[12] are shown.Variant \nO-LVIS MNet40 \n\nNo Objaverse shapes \n13.9 \n75.5 \nOnly Objaverse shapes \n41.6 \n79.2 \nNo backbone scale up \n31.7 \n78.7 \n\nNo caption & retrieval \n37.0 \n82.9 \nNo text filtering \n41.4 \n82.9 \n\nNo point rgb, only xyz \n39.6 \n83.6 \nNo text contras. learning \n23.3 \n67.4 \nNo image contras. learning \n41.0 \n81.0 \n\nFull \n42.0 \n83.1 \nFull + hard mining \n43.4 \n83.4 \n\n\n\n\n\n\n\n2/9,6$FF \n\n0RGHO1HW$FF \n\nFigure 6: Ablation study on \nusing different ratios of train-\ning data. \n\n%DVH &DS 5HWU )XOO \n\n\n2/9,6$FF \n\n3RLQW%(57 \n6SDUVH&RQY \n\n\n\n\n). Please evaluate all 256 texts.Afterwards, we combine all the responses to create the final filtering results, effectively removing approximately 30% of the raw texts.000 New project ( 19 ) \n001 3December -Chemestry \n002 Fake Brand Soda Can \n003 Spartan Shild \n004 Apple3d \n005 Landmine \n006 FaunveinB-S \n007 FIGURA 5 \n008 Sphero Blue \n009 Sofa \n010 Maddox \n011 A3 Complete \n012 Suspension Bridge \n013 Maung \n014 Captain-americas-shield \n015 sphorb4 \n...... \n\n000 N \n001 Y \n002 Y \n003 Y \n004 Y \n005 Y \n006 N \n007 N \n008 Y \n009 Y \n010 N \n011 N \n012 Y \n013 N \n014 Y \n015 N \n...... \n\n\n\nTable 4 :\n4Hyperparameters for scaling up PointBERT[78].# Parameters # Layers Width # Heads MLP Dim # Patches Patch Embed Dim SparseConv [10] The smallest version (5.3M parameters) of the model is adapted from the MinkowskiFCNN model by adjusting the width of the final convolution and linear layers. The remaining three models are adaptations of MinkowskiResNet, each varying in the number of basic ResNet blocks used. See5.1M \n6 \n256 \n4 \n1024 \n64 \n96 \n13.3M \n6 \n512 \n8 \n1024 \n64 \n128 \n32.3M \n12 \n512 \n8 \n1536 \n384 \n256 \n72.1M \n12 \n768 \n12 \n2304 \n512 \n256 \n\n\n\nTable 5 :\n5Hyperparameters for scaling up SparseConv[10].# Parameters # Convolution Layers # Linear Layers \n\n5.3M \n7 \n4 \n29.0M \n18 \n3 \n33.7M \n26 \n3 \n41.3M \n42 \n3 \n\n\n\nSatr: Zero-shot semantic segmentation of 3d shapes. Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, Peter Wonka, arXiv:2304.04909arXiv preprintAhmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, and Peter Wonka. Satr: Zero-shot semantic segmentation of 3d shapes. arXiv preprint arXiv:2304.04909, 2023.\n\nSelf-supervised learning for domain adaptation on point clouds. Haggai Idan Achituve, Gal Maron, Chechik, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer visionIdan Achituve, Haggai Maron, and Gal Chechik. Self-supervised learning for domain adaptation on point clouds. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 123-133, 2021.\n\nLearning representations and generative models for 3d point clouds. Panos Achlioptas, Olga Diamanti, International conference on machine learning. PMLRIoannis Mitliagkas, and Leonidas GuibasPanos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning repre- sentations and generative models for 3d point clouds. In International conference on machine learning, pages 40-49. PMLR, 2018.\n\nFlamingo: a visual language model for few-shot learning. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katie Mensch, Malcolm Millican, Reynolds, arXiv:2204.14198arXiv preprintJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\n\nClipface: Text-guided editing of textured 3d morphable models. Shivangi Aneja, Justus Thies, Angela Dai, Matthias Nie\u00dfner, arXiv:2212.01406arXiv preprintShivangi Aneja, Justus Thies, Angela Dai, and Matthias Nie\u00dfner. Clipface: Text-guided editing of textured 3d morphable models. arXiv preprint arXiv:2212.01406, 2022.\n\nClip retrieval: Easily compute clip embeddings and build a clip retrieval system with them. Romain Beaumont, Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them. https://github.com/rom1504/clip-retrieval, 2022.\n\nText and image guided 3d avatar generation and manipulation. Zehranaz Canfes, Alara Furkan Atasoy, Pinar Dirik, Yanardag, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionZehranaz Canfes, M Furkan Atasoy, Alara Dirik, and Pinar Yanardag. Text and image guided 3d avatar generation and manipulation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 4421-4431, 2023.\n\nX Angel, Thomas Chang, Leonidas Funkhouser, Pat Guibas, Qixing Hanrahan, Zimo Huang, Silvio Li, Manolis Savarese, Shuran Savva, Hao Song, Su, arXiv:1512.03012An information-rich 3d model repository. arXiv preprintAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n\nRunnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, Wenping Wang, arXiv:2301.04926Clip2scene: Towards label-efficient 3d scene understanding by clip. arXiv preprintRunnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wenping Wang. Clip2scene: Towards label-efficient 3d scene understanding by clip. arXiv preprint arXiv:2301.04926, 2023.\n\n4d spatio-temporal convnets: Minkowski convolutional neural networks. Christopher Choy, Junyoung Gwak, Silvio Savarese, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChristopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3075-3084, 2019.\n\nDataset and benchmarks for real-world 3d object understanding. Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21126-21136, 2022.\n\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli Vanderbilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi, arXiv:2212.08051Objaverse: A universe of annotated 3d objects. arXiv preprintMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022.\n\nPpf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors. Haowen Deng, Tolga Birdal, Slobodan Ilic, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors. In Proceedings of the European conference on computer vision (ECCV), pages 602-618, 2018.\n\nLanguagedriven open-vocabulary 3d scene understanding. Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, Xiaojuan Qi, arXiv:2211.16312arXiv preprintRunyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Language- driven open-vocabulary 3d scene understanding. arXiv preprint arXiv:2211.16312, 2022.\n\nSelf-supervised learning on 3d point clouds by learning discrete generative models. Benjamin Eckart, Wentao Yuan, Chao Liu, Jan Kautz, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionBenjamin Eckart, Wentao Yuan, Chao Liu, and Jan Kautz. Self-supervised learning on 3d point clouds by learning discrete generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8248-8257, 2021.\n\n3d-future: 3d furniture shape with texture. Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, Dacheng Tao, International Journal of Computer Vision. 129Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision, 129:3313-3337, 2021.\n\nLvis: A dataset for large vocabulary instance segmentation. Agrim Gupta, Piotr Dollar, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionAgrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356-5364, 2019.\n\nSemantic abstraction: Open-world 3d scene understanding from 2d vision-language models. Huy Ha, Shuran Song, Conference on Robot Learning. Huy Ha and Shuran Song. Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models. In Conference on Robot Learning, 2022.\n\nDeepti Hegde, Jeya Maria Jose Valanarasu, Patel, arXiv:2303.11313Clip goes 3d: Leveraging prompt tuning for language grounded 3d recognition. arXiv preprintDeepti Hegde, Jeya Maria Jose Valanarasu, and Vishal M Patel. Clip goes 3d: Leveraging prompt tuning for language grounded 3d recognition. arXiv preprint arXiv:2303.11313, 2023.\n\nMasked autoencoder for self-supervised pre-training on lidar point clouds. Georg Hess, Johan Jaxing, Elias Svensson, David Hagerman, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionChristoffer Petersson, and Lennart SvenssonGeorg Hess, Johan Jaxing, Elias Svensson, David Hagerman, Christoffer Petersson, and Lennart Svensson. Masked autoencoder for self-supervised pre-training on lidar point clouds. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 350-359, 2023.\n\nLidarclip or: How i learned to talk to point clouds. Georg Hess, Adam Tonderski, Christoffer Petersson, Lennart Svensson, Kalle \u00c5str\u00f6m, arXiv:2212.06858arXiv preprintGeorg Hess, Adam Tonderski, Christoffer Petersson, Lennart Svensson, and Kalle \u00c5str\u00f6m. Lidarclip or: How i learned to talk to point clouds. arXiv preprint arXiv:2212.06858, 2022.\n\nAvatarclip: Zero-shot text-driven generation and animation of 3d avatars. Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, Ziwei Liu, arXiv:2205.08535arXiv preprintFangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-driven generation and animation of 3d avatars. arXiv preprint arXiv:2205.08535, 2022.\n\nJoint representation learning for text and 3d point cloud. Rui Huang, Xuran Pan, Henry Zheng, Haojun Jiang, Zhifeng Xie, Shiji Song, Gao Huang, arXiv:2301.07584arXiv preprintRui Huang, Xuran Pan, Henry Zheng, Haojun Jiang, Zhifeng Xie, Shiji Song, and Gao Huang. Joint representation learning for text and 3d point cloud. arXiv preprint arXiv:2301.07584, 2023.\n\nClip2point: Transfer clip to point cloud classification with image-depth pre-training. Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, W H Rynson, Wanli Lau, Wangmeng Ouyang, Zuo, arXiv:2210.01055arXiv preprintTianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, Rynson WH Lau, Wanli Ouyang, and Wangmeng Zuo. Clip2point: Transfer clip to point cloud classification with image-depth pre-training. arXiv preprint arXiv:2210.01055, 2022.\n\n. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, Ludwig Schmidt, Openclip, If you use this software, please cite it as belowGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. If you use this software, please cite it as below.\n\nZero-shot text-guided object generation with dream fields. Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, Ben Poole, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAjay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 867-876, 2022.\n\nKrishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, arXiv:2302.07241Open-set multimodal 3d mapping. arXiv preprintKrishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Conceptfusion: Open-set multimodal 3d mapping. arXiv preprint arXiv:2302.07241, 2023.\n\nClipmatrix: Text-controlled creation of 3d textured meshes. Nikolay Jetchev, arXiv:2109.12922arXiv preprintNikolay Jetchev. Clipmatrix: Text-controlled creation of 3d textured meshes. arXiv preprint arXiv:2109.12922, 2021.\n\nScaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, International Conference on Machine Learning. PMLRChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun- Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904-4916. PMLR, 2021.\n\nHard negative mixing for contrastive learning. Yannis Kalantidis, Bulent Mert, Noe Sariyildiz, Philippe Pion, Diane Weinzaepfel, Larlus, Advances in Neural Information Processing Systems. 33Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. Advances in Neural Information Processing Systems, 33:21798-21809, 2020.\n\nJustin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, Matthew Tancik, arXiv:2303.09553Lerf: Language embedded radiance fields. arXiv preprintJustin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. arXiv preprint arXiv:2303.09553, 2023.\n\nText to mesh without 3d supervision using limit subdivision. Nasir Khalid, Tianhao Xie, Eugene Belilovsky, Tiberiu Popa, arXiv:2203.13333arXiv preprintNasir Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Text to mesh without 3d supervision using limit subdivision. arXiv preprint arXiv:2203.13333, 2022.\n\nUnderstanding pure clip guidance for voxel grid nerf models. Han-Hung Lee, Chang, arXiv:2209.15172arXiv preprintHan-Hung Lee and Angel X Chang. Understanding pure clip guidance for voxel grid nerf models. arXiv preprint arXiv:2209.15172, 2022.\n\nBlip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International Conference on Machine Learning. PMLRJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language- image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888-12900. PMLR, 2022.\n\nGrounded language-image pre-training. Pengchuan Liunian Harold Li, Haotian Zhang, Jianwei Zhang, Chunyuan Yang, Yiwu Li, Lijuan Zhong, Lu Wang, Lei Yuan, Jenq-Neng Zhang, Hwang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLiunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965-10975, 2022.\n\nMind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Yuhui Victor Weixin Liang, Yongchan Zhang, Serena Kwon, James Y Yeung, Zou, Advances in Neural Information Processing Systems. 35Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 35:17612-17625, 2022.\n\nPartslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models. Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, Hao Su, arXiv:2212.01558arXiv preprintMinghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su. Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models. arXiv preprint arXiv:2212.01558, 2022.\n\nIss: Image as stetting stone for text-guided 3d shape generation. Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, Chi-Wing Fu, arXiv:2209.04145arXiv preprintZhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, and Chi-Wing Fu. Iss: Image as stetting stone for text-guided 3d shape generation. arXiv preprint arXiv:2209.04145, 2022.\n\nOpen-vocabulary point-cloud object detection without 3d annotation. Yuheng Lu, Chenfeng Xu, Xiaobao Wei, Xiaodong Xie, Masayoshi Tomizuka, Kurt Keutzer, Shanghang Zhang, arXiv:2304.00788arXiv preprintYuheng Lu, Chenfeng Xu, Xiaobao Wei, Xiaodong Xie, Masayoshi Tomizuka, Kurt Keutzer, and Shanghang Zhang. Open-vocabulary point-cloud object detection without 3d annotation. arXiv preprint arXiv:2304.00788, 2023.\n\nRethinking network design and local geometry in point cloud: A simple residual mlp framework. Xu Ma, Can Qin, Haoxuan You, Yun Haoxi Ran, Fu, arXiv:2202.07123arXiv preprintXu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local geometry in point cloud: A simple residual mlp framework. arXiv preprint arXiv:2202.07123, 2022.\n\nVoxnet: A 3d convolutional neural network for realtime object recognition. Daniel Maturana, Sebastian Scherer, 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEEDaniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real- time object recognition. In 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 922-928. IEEE, 2015.\n\nSelf-supervised point cloud prediction using 3d spatio-temporal convolutional networks. Benedikt Mersch, Xieyuanli Chen, Jens Behley, Cyrill Stachniss, Conference on Robot Learning. PMLRBenedikt Mersch, Xieyuanli Chen, Jens Behley, and Cyrill Stachniss. Self-supervised point cloud prediction using 3d spatio-temporal convolutional networks. In Conference on Robot Learning, pages 1444-1454. PMLR, 2022.\n\nText2mesh: Textdriven neural stylization for meshes. Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, Rana Hanocka, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text- driven neural stylization for meshes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13492-13502, 2022.\n\nRon Mokady, Amir Hertz, Amit Bermano, Clipcap, arXiv:2111.09734Clip prefix for image captioning. arXiv preprintRon Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021.\n\n. Openai, Gpt-4 technical reportOpenAI. Gpt-4 technical report, 2023.\n\nMasked autoencoders for point cloud self-supervised learning. Yatian Pang, Wenxiao Wang, E H Francis, Wei Tay, Yonghong Liu, Li Tian, Yuan, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerProceedings, Part IIYatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part II, pages 604-621. Springer, 2022.\n\nSongyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, arXiv:2211.156543d scene understanding with open vocabularies. arXiv preprintSongyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. arXiv preprint arXiv:2211.15654, 2022.\n\nSelf-supervised learning of point clouds via orientation estimation. Omid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, Vladimir G Kim, 2020 International Conference on 3D Vision (3DV). IEEEOmid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, and Vladimir G Kim. Self-supervised learning of point clouds via orientation estimation. In 2020 International Conference on 3D Vision (3DV), pages 1018-1028. IEEE, 2020.\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652-660, 2017.\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. Li Charles Ruizhongtai Qi, Hao Yi, Leonidas J Su, Guibas, Advances in neural information processing systems. 30Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017.\n\nZekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, Li Yi, arXiv:2302.02318Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining. arXiv preprintZekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining. arXiv preprint arXiv:2302.02318, 2023.\n\nGuocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, Bernard Ghanem, arXiv:2206.04670Pointnext: Revisiting pointnet++ with improved training and scaling strategies. Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. arXiv:2206.04670, 2022.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International Conference on Machine Learning. PMLRAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021.\n\nHierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.06125arXiv preprintAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nGlobal-local bidirectional reasoning for unsupervised representation learning of 3d point clouds. Yongming Rao, Jiwen Lu, Jie Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYongming Rao, Jiwen Lu, and Jie Zhou. Global-local bidirectional reasoning for unsupervised representation learning of 3d point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5376-5385, 2020.\n\nJoshua Robinson, Ching-Yao Chuang, Suvrit Sra, Stefanie Jegelka, arXiv:2010.04592Contrastive learning with hard negative samples. arXiv preprintJoshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples. arXiv preprint arXiv:2010.04592, 2020.\n\nHighresolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High- resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022.\n\nLanguage-grounded indoor 3d semantic segmentation in the wild. David Rozenberszki, Or Litany, Angela Dai, arXiv:2204.07761arXiv preprintDavid Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d semantic segmentation in the wild. arXiv preprint arXiv:2204.07761, 2022.\n\nPhotorealistic text-to-image diffusion models with deep language understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, ; S Sara Mahdavi, Rapha Gontijo Lopes, arXiv:2205.11487Burcu Karagol Ayan. arXiv preprintSeyed Kamyar Seyed GhasemipourChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\n\nInfo3d: Representation learning on 3d objects using mutual information maximization and contrastive learning. Aditya Sanghi, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKProceedings, Part XXIX 16Aditya Sanghi. Info3d: Representation learning on 3d objects using mutual information maximization and contrastive learning. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIX 16, pages 626-642.\n\n. Springer, Springer, 2020.\n\nClip-forge: Towards zero-shot text-to-shape generation. Aditya Sanghi, Hang Chu, Ye Joseph G Lambourne, Chin-Yi Wang, Marco Cheng, Kamal Rahimi Fumero, Malekshan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18603-18613, 2022.\n\nSelf-supervised deep learning on point clouds by reconstructing space. Jonathan Sauder, Bjarne Sievers, Advances in Neural Information Processing Systems. 32Jonathan Sauder and Bjarne Sievers. Self-supervised deep learning on point clouds by recon- structing space. Advances in Neural Information Processing Systems, 32, 2019.\n\nLaion-5b: An open large-scale dataset for training next generation image-text models. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, arXiv:2210.08402arXiv preprintChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion- 5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n\nSelf-supervised few-shot learning on point clouds. Charu Sharma, Manohar Kaul, Advances in Neural Information Processing Systems. 33Charu Sharma and Manohar Kaul. Self-supervised few-shot learning on point clouds. Advances in Neural Information Processing Systems, 33:7212-7221, 2020.\n\nPoint cloud pre-training by mixing and disentangling. Chao Sun, Zhedong Zheng, Xiaohan Wang, Mingliang Xu, Yi Yang, 2109arXiv e-printsChao Sun, Zhedong Zheng, Xiaohan Wang, Mingliang Xu, and Yi Yang. Point cloud pre-training by mixing and disentangling. arXiv e-prints, pages arXiv-2109, 2021.\n\nSelf-supervised learning of local features in 3d point clouds. Ali Thabet, Humam Alwassel, Bernard Ghanem, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. the IEEE/CVF conference on computer vision and pattern recognition workshopsAli Thabet, Humam Alwassel, and Bernard Ghanem. Self-supervised learning of local features in 3d point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 938-939, 2020.\n\nUnderstanding and fixing the modality gap in vision-language models. Vishaal Udandarao, University of CambridgeMaster's thesisVishaal Udandarao. Understanding and fixing the modality gap in vision-language models. Master's thesis, University of Cambridge, 2022.\n\nRevisiting point cloud classification: A new benchmark dataset and classification model on real-world data. Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, Sai-Kit Yeung, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionMikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1588-1597, 2019.\n\nUnsupervised point cloud pre-training via occlusion completion. Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, Matt J Kusner, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionHanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and Matt J Kusner. Unsupervised point cloud pre-training via occlusion completion. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9782-9792, 2021.\n\nDynamic graph cnn for learning on point clouds. Yue Wang, Yongbin Sun, Ziwei Liu, E Sanjay, Sarma, Justin M Michael M Bronstein, Solomon, Acm Transactions On Graphics (tog). 385Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):1-12, 2019.\n\nSim2real 3d object classification using spherical kernel point convolution and a deep center voting scheme. Jean-Baptiste Weibel, Timothy Patten, Markus Vincze, arXiv:2103.06134arXiv preprintJean-Baptiste Weibel, Timothy Patten, and Markus Vincze. Sim2real 3d object classification using spherical kernel point convolution and a deep center voting scheme. arXiv preprint arXiv:2103.06134, 2021.\n\n3d shapenets: A deep representation for volumetric shapes. Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912-1920, 2015.\n\nPointcontrast: Unsupervised pre-training for 3d point cloud understanding. Saining Xie, Jiatao Gu, Demi Guo, Leonidas Charles R Qi, Or Guibas, Litany, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerProceedings, Part III 16Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16, pages 574-591. Springer, 2020.\n\nDream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion models. Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Shenghua Gao, arXiv:2212.14704arXiv preprintJiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion models. arXiv preprint arXiv:2212.14704, 2022.\n\nUlip: Learning unified representation of language, image and point cloud for 3d understanding. Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese, arXiv:2212.05171arXiv preprintLe Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, image and point cloud for 3d understanding. arXiv preprint arXiv:2212.05171, 2022.\n\nRegionplc: Regional point-language contrastive learning for open-world 3d scene understanding. Jihan Yang, Runyu Ding, Zhe Wang, Xiaojuan Qi, arXiv:2304.00962arXiv preprintJihan Yang, Runyu Ding, Zhe Wang, and Xiaojuan Qi. Regionplc: Regional point-language contrastive learning for open-world 3d scene understanding. arXiv preprint arXiv:2304.00962, 2023.\n\nPoint-bert: Pre-training 3d point cloud transformers with masked point modeling. Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, Jiwen Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19313-19322, 2022.\n\nPoint-bert: Pre-training 3d point cloud transformers with masked point modeling. Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, Jiwen Lu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2022Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n\nClip\u02c62: Contrastive language-image-point pretraining from real-world point cloud data. Yihan Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han, Chaoqiang Ye, Qingqiu Huang, Dit-Yan Yeung, Zhen Yang, Xiaodan Liang, Hang Xu, arXiv:2303.12417arXiv preprintYihan Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han, Chaoqiang Ye, Qingqiu Huang, Dit-Yan Yeung, Zhen Yang, Xiaodan Liang, and Hang Xu. Clip\u02c62: Contrastive language-image-point pretraining from real-world point cloud data. arXiv preprint arXiv:2303.12417, 2023.\n\nHaotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, arXiv:2206.05836Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. arXiv preprintHaotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. arXiv preprint arXiv:2206.05836, 2022.\n\nJunbo Zhang, Runpei Dong, Kaisheng Ma, arXiv:2303.04748Clip-fo3d: Learning free open-world 3d scene representations from 2d dense clip. arXiv preprintJunbo Zhang, Runpei Dong, and Kaisheng Ma. Clip-fo3d: Learning free open-world 3d scene representations from 2d dense clip. arXiv preprint arXiv:2303.04748, 2023.\n\nPointclip: Point cloud understanding by clip. Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, Hongsheng Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionRenrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8552-8562, 2022.\n\nSelf-supervised pretraining of 3d features on any point-cloud. Zaiwei Zhang, Rohit Girdhar, Armand Joulin, Ishan Misra, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan Misra. Self-supervised pretraining of 3d features on any point-cloud. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10252-10263, 2021.\n\nPointclip v2: Adapting clip for powerful 3d open-world learning. Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyao Zeng, Shanghang Zhang, Peng Gao, arXiv:2211.11682arXiv preprintXiangyang Zhu, Renrui Zhang, Bowei He, Ziyao Zeng, Shanghang Zhang, and Peng Gao. Point- clip v2: Adapting clip for powerful 3d open-world learning. arXiv preprint arXiv:2211.11682, 2022.\n", "annotations": {"author": "[{\"end\":109,\"start\":82},{\"end\":152,\"start\":110},{\"end\":182,\"start\":153},{\"end\":219,\"start\":183},{\"end\":246,\"start\":220},{\"end\":285,\"start\":247},{\"end\":320,\"start\":286},{\"end\":360,\"start\":321},{\"end\":383,\"start\":361}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":90},{\"end\":119,\"start\":116},{\"end\":166,\"start\":161},{\"end\":193,\"start\":190},{\"end\":230,\"start\":228},{\"end\":259,\"start\":256},{\"end\":294,\"start\":291},{\"end\":334,\"start\":327},{\"end\":367,\"start\":365}]", "author_first_name": "[{\"end\":89,\"start\":82},{\"end\":115,\"start\":110},{\"end\":160,\"start\":153},{\"end\":189,\"start\":183},{\"end\":227,\"start\":220},{\"end\":255,\"start\":247},{\"end\":290,\"start\":286},{\"end\":326,\"start\":321},{\"end\":364,\"start\":361}]", "author_affiliation": "[{\"end\":108,\"start\":95},{\"end\":151,\"start\":121},{\"end\":181,\"start\":168},{\"end\":218,\"start\":195},{\"end\":245,\"start\":232},{\"end\":284,\"start\":261},{\"end\":319,\"start\":296},{\"end\":359,\"start\":336},{\"end\":382,\"start\":369}]", "title": "[{\"end\":79,\"start\":1},{\"end\":462,\"start\":384}]", "venue": null, "abstract": "[{\"end\":2017,\"start\":464}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2723,\"start\":2719},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2812,\"start\":2808},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2815,\"start\":2812},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2862,\"start\":2858},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2865,\"start\":2862},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2868,\"start\":2865},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":2871,\"start\":2868},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2874,\"start\":2871},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2876,\"start\":2874},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2913,\"start\":2909},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2916,\"start\":2913},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2919,\"start\":2916},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":2922,\"start\":2919},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2925,\"start\":2922},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2927,\"start\":2925},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":3287,\"start\":3283},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":3731,\"start\":3727},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":3734,\"start\":3731},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":4231,\"start\":4227},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4369,\"start\":4366},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5591,\"start\":5587},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7593,\"start\":7589},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7596,\"start\":7593},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7599,\"start\":7596},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":7602,\"start\":7599},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7604,\"start\":7602},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7607,\"start\":7604},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7610,\"start\":7607},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7865,\"start\":7861},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7868,\"start\":7865},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7871,\"start\":7868},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7874,\"start\":7871},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7877,\"start\":7874},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7879,\"start\":7877},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7882,\"start\":7879},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7884,\"start\":7882},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7887,\"start\":7884},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":7890,\"start\":7887},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7893,\"start\":7890},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8122,\"start\":8118},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8125,\"start\":8122},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8128,\"start\":8125},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":8131,\"start\":8128},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8134,\"start\":8131},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8137,\"start\":8134},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":8140,\"start\":8137},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8143,\"start\":8140},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8146,\"start\":8143},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":8149,\"start\":8146},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8152,\"start\":8149},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8175,\"start\":8172},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8178,\"start\":8175},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":8295,\"start\":8291},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":8298,\"start\":8295},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8301,\"start\":8298},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":8304,\"start\":8301},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8307,\"start\":8304},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8334,\"start\":8330},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8336,\"start\":8334},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":8411,\"start\":8407},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":8414,\"start\":8411},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8417,\"start\":8414},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9027,\"start\":9023},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":9030,\"start\":9027},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9033,\"start\":9030},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9035,\"start\":9033},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":9038,\"start\":9035},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9071,\"start\":9067},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9074,\"start\":9071},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9076,\"start\":9074},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":9079,\"start\":9076},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9106,\"start\":9102},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":9109,\"start\":9106},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9112,\"start\":9109},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9144,\"start\":9140},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9147,\"start\":9144},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":9150,\"start\":9147},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9174,\"start\":9170},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":9205,\"start\":9201},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9208,\"start\":9205},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":9211,\"start\":9208},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9427,\"start\":9423},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":9430,\"start\":9427},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9433,\"start\":9430},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":9459,\"start\":9455},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9707,\"start\":9703},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11086,\"start\":11082},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":11089,\"start\":11086},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11092,\"start\":11089},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":11111,\"start\":11107},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12902,\"start\":12898},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13245,\"start\":13242},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13261,\"start\":13257},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13274,\"start\":13270},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13461,\"start\":13457},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":14524,\"start\":14520},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14891,\"start\":14887},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":14894,\"start\":14891},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15600,\"start\":15596},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15686,\"start\":15682},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16189,\"start\":16185},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":16836,\"start\":16832},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16877,\"start\":16874},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":17509,\"start\":17505},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":18249,\"start\":18245},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":19736,\"start\":19732},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19776,\"start\":19772},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":20408,\"start\":20404},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20411,\"start\":20408},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":21826,\"start\":21822},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":21848,\"start\":21844},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21897,\"start\":21893},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22037,\"start\":22033},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22087,\"start\":22083},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":22381,\"start\":22377},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":22399,\"start\":22395},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":22411,\"start\":22407},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22422,\"start\":22418},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22439,\"start\":22435},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":22454,\"start\":22450},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":22482,\"start\":22478},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":22503,\"start\":22499},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22701,\"start\":22697},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":22718,\"start\":22714},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":22742,\"start\":22738},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23442,\"start\":23439},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":24902,\"start\":24898},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24918,\"start\":24914},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":25249,\"start\":25245},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25848,\"start\":25844},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":25865,\"start\":25861},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":25888,\"start\":25884},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":25967,\"start\":25963},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":26034,\"start\":26030},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26797,\"start\":26793},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26870,\"start\":26866},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":26890,\"start\":26886},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":31106,\"start\":31102},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":31178,\"start\":31174},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32971,\"start\":32967},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33194,\"start\":33190},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":34311,\"start\":34307},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":36016,\"start\":36012},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":36365,\"start\":36361},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":36428,\"start\":36424},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":36542,\"start\":36538},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":36560,\"start\":36556},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":38577,\"start\":38573},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":38873,\"start\":38869},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":38967,\"start\":38963},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":38988,\"start\":38984},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":39081,\"start\":39077},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":39156,\"start\":39152},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":39229,\"start\":39225},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":39587,\"start\":39583},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":40083,\"start\":40079},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":42557,\"start\":42554},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":42611,\"start\":42607},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":42682,\"start\":42678},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":42705,\"start\":42701},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":43239,\"start\":43235},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":43255,\"start\":43251},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":43277,\"start\":43273},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":44648,\"start\":44644},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":44671,\"start\":44667},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":45850,\"start\":45846},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":46412,\"start\":46408}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41615,\"start\":40844},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41695,\"start\":41616},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41789,\"start\":41696},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42068,\"start\":41790},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42172,\"start\":42069},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42308,\"start\":42173},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42438,\"start\":42309},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43180,\"start\":42439},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44576,\"start\":43181},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45207,\"start\":44577},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":45793,\"start\":45208},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":46354,\"start\":45794},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":46520,\"start\":46355}]", "paragraph": "[{\"end\":2533,\"start\":2033},{\"end\":3610,\"start\":2535},{\"end\":3903,\"start\":3612},{\"end\":4936,\"start\":3905},{\"end\":6074,\"start\":4938},{\"end\":7436,\"start\":6076},{\"end\":8887,\"start\":7438},{\"end\":9400,\"start\":8924},{\"end\":10177,\"start\":9402},{\"end\":10770,\"start\":10188},{\"end\":11881,\"start\":10811},{\"end\":12172,\"start\":11883},{\"end\":12472,\"start\":12399},{\"end\":12932,\"start\":12597},{\"end\":13777,\"start\":12959},{\"end\":14525,\"start\":13779},{\"end\":15259,\"start\":14559},{\"end\":15652,\"start\":15261},{\"end\":16155,\"start\":15654},{\"end\":16630,\"start\":16157},{\"end\":17195,\"start\":16632},{\"end\":17685,\"start\":17197},{\"end\":19052,\"start\":17725},{\"end\":19899,\"start\":19054},{\"end\":21657,\"start\":19924},{\"end\":25415,\"start\":21706},{\"end\":26035,\"start\":25443},{\"end\":26702,\"start\":26037},{\"end\":27009,\"start\":26721},{\"end\":27707,\"start\":27011},{\"end\":28193,\"start\":27709},{\"end\":29071,\"start\":28195},{\"end\":29113,\"start\":29097},{\"end\":29122,\"start\":29120},{\"end\":29137,\"start\":29129},{\"end\":29380,\"start\":29139},{\"end\":30776,\"start\":29409},{\"end\":31474,\"start\":30778},{\"end\":32660,\"start\":31504},{\"end\":33203,\"start\":32723},{\"end\":33288,\"start\":33264},{\"end\":33615,\"start\":33359},{\"end\":33695,\"start\":33617},{\"end\":33940,\"start\":33697},{\"end\":34105,\"start\":33942},{\"end\":34204,\"start\":34107},{\"end\":34268,\"start\":34206},{\"end\":34491,\"start\":34291},{\"end\":34594,\"start\":34493},{\"end\":35398,\"start\":34596},{\"end\":36000,\"start\":35445},{\"end\":36530,\"start\":36002},{\"end\":36972,\"start\":36532},{\"end\":37869,\"start\":37011},{\"end\":38439,\"start\":37871},{\"end\":39497,\"start\":38441},{\"end\":39987,\"start\":39552},{\"end\":40843,\"start\":40032}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12398,\"start\":12173},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12596,\"start\":12473},{\"attributes\":{\"id\":\"formula_2\"},\"end\":29119,\"start\":29114}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18673,\"start\":18666},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23719,\"start\":23712},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26988,\"start\":26981},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36060,\"start\":36053},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36313,\"start\":36306}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2031,\"start\":2019},{\"attributes\":{\"n\":\"2.2\"},\"end\":8922,\"start\":8890},{\"attributes\":{\"n\":\"3\"},\"end\":10186,\"start\":10180},{\"attributes\":{\"n\":\"3.1\"},\"end\":10809,\"start\":10773},{\"attributes\":{\"n\":\"3.2\"},\"end\":12957,\"start\":12935},{\"attributes\":{\"n\":\"3.3\"},\"end\":14557,\"start\":14528},{\"attributes\":{\"n\":\"3.4\"},\"end\":17723,\"start\":17688},{\"attributes\":{\"n\":\"3.5\"},\"end\":19922,\"start\":19902},{\"attributes\":{\"n\":\"4\"},\"end\":21671,\"start\":21660},{\"attributes\":{\"n\":\"4.1\"},\"end\":21704,\"start\":21674},{\"attributes\":{\"n\":\"4.2\"},\"end\":25441,\"start\":25418},{\"attributes\":{\"n\":\"4.3\"},\"end\":26719,\"start\":26705},{\"end\":29095,\"start\":29074},{\"end\":29127,\"start\":29125},{\"attributes\":{\"n\":\"4.4\"},\"end\":29407,\"start\":29383},{\"attributes\":{\"n\":\"5\"},\"end\":31502,\"start\":31477},{\"attributes\":{\"n\":\"6\"},\"end\":32671,\"start\":32663},{\"attributes\":{\"n\":\"6.1\"},\"end\":32721,\"start\":32674},{\"attributes\":{\"n\":\"6.2\"},\"end\":33262,\"start\":33206},{\"attributes\":{\"n\":\"6.3\"},\"end\":33335,\"start\":33291},{\"attributes\":{\"n\":\"6.3.1\"},\"end\":33357,\"start\":33338},{\"attributes\":{\"n\":\"6.3.2\"},\"end\":34289,\"start\":34271},{\"attributes\":{\"n\":\"6.4\"},\"end\":35443,\"start\":35401},{\"attributes\":{\"n\":\"6.5\"},\"end\":37009,\"start\":36975},{\"attributes\":{\"n\":\"6.6\"},\"end\":39550,\"start\":39500},{\"end\":40030,\"start\":39990},{\"end\":40857,\"start\":40845},{\"end\":41627,\"start\":41617},{\"end\":41700,\"start\":41697},{\"end\":41802,\"start\":41791},{\"end\":42185,\"start\":42174},{\"end\":42321,\"start\":42310},{\"end\":42449,\"start\":42440},{\"end\":43191,\"start\":43182},{\"end\":44587,\"start\":44578},{\"end\":45804,\"start\":45795},{\"end\":46365,\"start\":46356}]", "table": "[{\"end\":43180,\"start\":42776},{\"end\":44576,\"start\":43278},{\"end\":45207,\"start\":44682},{\"end\":45793,\"start\":45379},{\"end\":46354,\"start\":46218},{\"end\":46520,\"start\":46413}]", "figure_caption": "[{\"end\":41615,\"start\":40859},{\"end\":41695,\"start\":41629},{\"end\":41789,\"start\":41702},{\"end\":42068,\"start\":41805},{\"end\":42172,\"start\":42071},{\"end\":42308,\"start\":42188},{\"end\":42438,\"start\":42324},{\"end\":42776,\"start\":42451},{\"end\":43278,\"start\":43193},{\"end\":44682,\"start\":44589},{\"end\":45379,\"start\":45210},{\"end\":46218,\"start\":45806},{\"end\":46413,\"start\":46367}]", "figure_ref": "[{\"end\":3041,\"start\":3033},{\"end\":6171,\"start\":6163},{\"end\":6813,\"start\":6805},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10363,\"start\":10355},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10950,\"start\":10942},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11312,\"start\":11304},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13161,\"start\":13153},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15254,\"start\":15246},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15292,\"start\":15284},{\"end\":15567,\"start\":15559},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16385,\"start\":16377},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17193,\"start\":17185},{\"end\":19289,\"start\":19281},{\"end\":22653,\"start\":22645},{\"end\":25357,\"start\":25349},{\"end\":25898,\"start\":25890},{\"end\":27008,\"start\":26993},{\"end\":27204,\"start\":27196},{\"end\":27706,\"start\":27699},{\"end\":27952,\"start\":27946},{\"end\":28696,\"start\":28688},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29755,\"start\":29747},{\"end\":29959,\"start\":29951},{\"end\":30315,\"start\":30307},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31409,\"start\":31400},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32743,\"start\":32726},{\"end\":32815,\"start\":32806},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33026,\"start\":33017},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33276,\"start\":33267},{\"end\":33287,\"start\":33281},{\"end\":35456,\"start\":35448},{\"end\":36147,\"start\":36139}]", "bib_author_first_name": "[{\"end\":46579,\"start\":46574},{\"end\":46597,\"start\":46593},{\"end\":46615,\"start\":46611},{\"end\":46633,\"start\":46628},{\"end\":46904,\"start\":46898},{\"end\":46923,\"start\":46920},{\"end\":47377,\"start\":47372},{\"end\":47394,\"start\":47390},{\"end\":47774,\"start\":47770},{\"end\":47805,\"start\":47798},{\"end\":47822,\"start\":47815},{\"end\":47832,\"start\":47828},{\"end\":47844,\"start\":47840},{\"end\":47856,\"start\":47851},{\"end\":47871,\"start\":47865},{\"end\":47883,\"start\":47878},{\"end\":47899,\"start\":47892},{\"end\":48275,\"start\":48267},{\"end\":48289,\"start\":48283},{\"end\":48303,\"start\":48297},{\"end\":48317,\"start\":48309},{\"end\":48622,\"start\":48616},{\"end\":48861,\"start\":48853},{\"end\":48875,\"start\":48870},{\"end\":48896,\"start\":48891},{\"end\":49299,\"start\":49298},{\"end\":49313,\"start\":49307},{\"end\":49329,\"start\":49321},{\"end\":49345,\"start\":49342},{\"end\":49360,\"start\":49354},{\"end\":49375,\"start\":49371},{\"end\":49389,\"start\":49383},{\"end\":49401,\"start\":49394},{\"end\":49418,\"start\":49412},{\"end\":49429,\"start\":49426},{\"end\":49756,\"start\":49750},{\"end\":49770,\"start\":49763},{\"end\":49784,\"start\":49776},{\"end\":49796,\"start\":49791},{\"end\":49808,\"start\":49802},{\"end\":49819,\"start\":49813},{\"end\":49830,\"start\":49824},{\"end\":49838,\"start\":49836},{\"end\":49852,\"start\":49845},{\"end\":50259,\"start\":50248},{\"end\":50274,\"start\":50266},{\"end\":50287,\"start\":50281},{\"end\":50739,\"start\":50732},{\"end\":50756,\"start\":50749},{\"end\":50768,\"start\":50763},{\"end\":50785,\"start\":50775},{\"end\":50798,\"start\":50794},{\"end\":50808,\"start\":50803},{\"end\":50821,\"start\":50819},{\"end\":50841,\"start\":50829},{\"end\":50857,\"start\":50851},{\"end\":50878,\"start\":50870},{\"end\":51380,\"start\":51376},{\"end\":51395,\"start\":51389},{\"end\":51410,\"start\":51405},{\"end\":51425,\"start\":51421},{\"end\":51438,\"start\":51433},{\"end\":51450,\"start\":51447},{\"end\":51469,\"start\":51463},{\"end\":51484,\"start\":51479},{\"end\":51502,\"start\":51493},{\"end\":51516,\"start\":51513},{\"end\":51929,\"start\":51923},{\"end\":51941,\"start\":51936},{\"end\":51958,\"start\":51950},{\"end\":52356,\"start\":52351},{\"end\":52368,\"start\":52363},{\"end\":52381,\"start\":52375},{\"end\":52394,\"start\":52387},{\"end\":52406,\"start\":52402},{\"end\":52420,\"start\":52412},{\"end\":52722,\"start\":52714},{\"end\":52737,\"start\":52731},{\"end\":52748,\"start\":52744},{\"end\":52757,\"start\":52754},{\"end\":53211,\"start\":53207},{\"end\":53223,\"start\":53216},{\"end\":53232,\"start\":53229},{\"end\":53246,\"start\":53238},{\"end\":53261,\"start\":53253},{\"end\":53273,\"start\":53268},{\"end\":53290,\"start\":53283},{\"end\":53607,\"start\":53602},{\"end\":53620,\"start\":53615},{\"end\":53633,\"start\":53629},{\"end\":54100,\"start\":54097},{\"end\":54111,\"start\":54105},{\"end\":54306,\"start\":54300},{\"end\":54329,\"start\":54314},{\"end\":54715,\"start\":54710},{\"end\":54727,\"start\":54722},{\"end\":54741,\"start\":54736},{\"end\":54757,\"start\":54752},{\"end\":55301,\"start\":55296},{\"end\":55312,\"start\":55308},{\"end\":55335,\"start\":55324},{\"end\":55354,\"start\":55347},{\"end\":55370,\"start\":55365},{\"end\":55671,\"start\":55663},{\"end\":55686,\"start\":55678},{\"end\":55699,\"start\":55694},{\"end\":55713,\"start\":55705},{\"end\":55722,\"start\":55719},{\"end\":55734,\"start\":55729},{\"end\":56027,\"start\":56024},{\"end\":56040,\"start\":56035},{\"end\":56051,\"start\":56046},{\"end\":56065,\"start\":56059},{\"end\":56080,\"start\":56073},{\"end\":56091,\"start\":56086},{\"end\":56101,\"start\":56098},{\"end\":56420,\"start\":56414},{\"end\":56433,\"start\":56428},{\"end\":56446,\"start\":56440},{\"end\":56461,\"start\":56453},{\"end\":56470,\"start\":56469},{\"end\":56472,\"start\":56471},{\"end\":56486,\"start\":56481},{\"end\":56500,\"start\":56492},{\"end\":56782,\"start\":56775},{\"end\":56800,\"start\":56792},{\"end\":56815,\"start\":56811},{\"end\":56830,\"start\":56826},{\"end\":56847,\"start\":56839},{\"end\":56862,\"start\":56857},{\"end\":56875,\"start\":56870},{\"end\":56890,\"start\":56882},{\"end\":56908,\"start\":56900},{\"end\":56923,\"start\":56919},{\"end\":56940,\"start\":56932},{\"end\":56956,\"start\":56953},{\"end\":56972,\"start\":56966},{\"end\":57388,\"start\":57384},{\"end\":57398,\"start\":57395},{\"end\":57419,\"start\":57411},{\"end\":57421,\"start\":57420},{\"end\":57436,\"start\":57430},{\"end\":57448,\"start\":57445},{\"end\":57855,\"start\":57848},{\"end\":57888,\"start\":57879},{\"end\":57906,\"start\":57902},{\"end\":57915,\"start\":57911},{\"end\":57926,\"start\":57923},{\"end\":57939,\"start\":57933},{\"end\":57950,\"start\":57944},{\"end\":57964,\"start\":57957},{\"end\":57981,\"start\":57975},{\"end\":57995,\"start\":57990},{\"end\":58383,\"start\":58376},{\"end\":58635,\"start\":58631},{\"end\":58647,\"start\":58641},{\"end\":58656,\"start\":58654},{\"end\":58669,\"start\":58662},{\"end\":58682,\"start\":58676},{\"end\":58695,\"start\":58691},{\"end\":58706,\"start\":58702},{\"end\":58720,\"start\":58711},{\"end\":58731,\"start\":58727},{\"end\":58739,\"start\":58736},{\"end\":59143,\"start\":59137},{\"end\":59162,\"start\":59156},{\"end\":59172,\"start\":59169},{\"end\":59193,\"start\":59185},{\"end\":59205,\"start\":59200},{\"end\":59500,\"start\":59494},{\"end\":59512,\"start\":59507},{\"end\":59516,\"start\":59513},{\"end\":59525,\"start\":59522},{\"end\":59542,\"start\":59536},{\"end\":59560,\"start\":59553},{\"end\":59866,\"start\":59861},{\"end\":59882,\"start\":59875},{\"end\":59894,\"start\":59888},{\"end\":59914,\"start\":59907},{\"end\":60185,\"start\":60177},{\"end\":60472,\"start\":60466},{\"end\":60483,\"start\":60477},{\"end\":60495,\"start\":60488},{\"end\":60509,\"start\":60503},{\"end\":60853,\"start\":60844},{\"end\":60880,\"start\":60873},{\"end\":60895,\"start\":60888},{\"end\":60911,\"start\":60903},{\"end\":60922,\"start\":60918},{\"end\":60933,\"start\":60927},{\"end\":60943,\"start\":60941},{\"end\":60953,\"start\":60950},{\"end\":60969,\"start\":60960},{\"end\":61532,\"start\":61527},{\"end\":61562,\"start\":61554},{\"end\":61576,\"start\":61570},{\"end\":61590,\"start\":61583},{\"end\":62009,\"start\":62002},{\"end\":62021,\"start\":62015},{\"end\":62031,\"start\":62027},{\"end\":62045,\"start\":62037},{\"end\":62055,\"start\":62051},{\"end\":62067,\"start\":62062},{\"end\":62080,\"start\":62077},{\"end\":62411,\"start\":62403},{\"end\":62421,\"start\":62417},{\"end\":62433,\"start\":62427},{\"end\":62446,\"start\":62438},{\"end\":62459,\"start\":62451},{\"end\":62739,\"start\":62733},{\"end\":62752,\"start\":62744},{\"end\":62764,\"start\":62757},{\"end\":62778,\"start\":62770},{\"end\":62793,\"start\":62784},{\"end\":62808,\"start\":62804},{\"end\":62827,\"start\":62818},{\"end\":63175,\"start\":63173},{\"end\":63183,\"start\":63180},{\"end\":63196,\"start\":63189},{\"end\":63205,\"start\":63202},{\"end\":63518,\"start\":63512},{\"end\":63538,\"start\":63529},{\"end\":63957,\"start\":63949},{\"end\":63975,\"start\":63966},{\"end\":63986,\"start\":63982},{\"end\":64001,\"start\":63995},{\"end\":64324,\"start\":64319},{\"end\":64336,\"start\":64333},{\"end\":64352,\"start\":64345},{\"end\":64363,\"start\":64358},{\"end\":64376,\"start\":64372},{\"end\":64776,\"start\":64773},{\"end\":64789,\"start\":64785},{\"end\":64801,\"start\":64797},{\"end\":65150,\"start\":65144},{\"end\":65164,\"start\":65157},{\"end\":65172,\"start\":65171},{\"end\":65174,\"start\":65173},{\"end\":65187,\"start\":65184},{\"end\":65201,\"start\":65193},{\"end\":65209,\"start\":65207},{\"end\":65617,\"start\":65610},{\"end\":65628,\"start\":65624},{\"end\":65642,\"start\":65637},{\"end\":65656,\"start\":65650},{\"end\":65675,\"start\":65671},{\"end\":65693,\"start\":65687},{\"end\":66057,\"start\":66053},{\"end\":66077,\"start\":66069},{\"end\":66088,\"start\":66085},{\"end\":66100,\"start\":66095},{\"end\":66113,\"start\":66105},{\"end\":66115,\"start\":66114},{\"end\":66480,\"start\":66477},{\"end\":66502,\"start\":66495},{\"end\":66515,\"start\":66507},{\"end\":66517,\"start\":66516},{\"end\":66992,\"start\":66990},{\"end\":67020,\"start\":67017},{\"end\":67033,\"start\":67025},{\"end\":67035,\"start\":67034},{\"end\":67310,\"start\":67305},{\"end\":67321,\"start\":67315},{\"end\":67334,\"start\":67328},{\"end\":67345,\"start\":67340},{\"end\":67357,\"start\":67350},{\"end\":67373,\"start\":67365},{\"end\":67380,\"start\":67378},{\"end\":67747,\"start\":67739},{\"end\":67760,\"start\":67754},{\"end\":67771,\"start\":67765},{\"end\":67784,\"start\":67778},{\"end\":67795,\"start\":67790},{\"end\":67812,\"start\":67805},{\"end\":67831,\"start\":67824},{\"end\":68221,\"start\":68217},{\"end\":68235,\"start\":68231},{\"end\":68240,\"start\":68236},{\"end\":68251,\"start\":68246},{\"end\":68267,\"start\":68261},{\"end\":68283,\"start\":68276},{\"end\":68297,\"start\":68289},{\"end\":68313,\"start\":68307},{\"end\":68328,\"start\":68322},{\"end\":68343,\"start\":68337},{\"end\":68357,\"start\":68353},{\"end\":68792,\"start\":68786},{\"end\":68809,\"start\":68801},{\"end\":68824,\"start\":68820},{\"end\":68838,\"start\":68833},{\"end\":68848,\"start\":68844},{\"end\":69170,\"start\":69162},{\"end\":69181,\"start\":69176},{\"end\":69189,\"start\":69186},{\"end\":69597,\"start\":69591},{\"end\":69617,\"start\":69608},{\"end\":69632,\"start\":69626},{\"end\":69646,\"start\":69638},{\"end\":69959,\"start\":69954},{\"end\":69976,\"start\":69969},{\"end\":69995,\"start\":69988},{\"end\":70011,\"start\":70004},{\"end\":70024,\"start\":70019},{\"end\":70506,\"start\":70501},{\"end\":70523,\"start\":70521},{\"end\":70538,\"start\":70532},{\"end\":70811,\"start\":70804},{\"end\":70828,\"start\":70821},{\"end\":70842,\"start\":70835},{\"end\":70855,\"start\":70851},{\"end\":70863,\"start\":70860},{\"end\":70876,\"start\":70871},{\"end\":70893,\"start\":70885},{\"end\":70916,\"start\":70903},{\"end\":71417,\"start\":71411},{\"end\":71863,\"start\":71857},{\"end\":71876,\"start\":71872},{\"end\":71884,\"start\":71882},{\"end\":71912,\"start\":71905},{\"end\":71924,\"start\":71919},{\"end\":71944,\"start\":71932},{\"end\":72471,\"start\":72463},{\"end\":72486,\"start\":72480},{\"end\":72815,\"start\":72806},{\"end\":72833,\"start\":72827},{\"end\":72851,\"start\":72844},{\"end\":72863,\"start\":72859},{\"end\":72876,\"start\":72872},{\"end\":72892,\"start\":72887},{\"end\":72905,\"start\":72901},{\"end\":72921,\"start\":72915},{\"end\":72936,\"start\":72929},{\"end\":72953,\"start\":72945},{\"end\":73342,\"start\":73337},{\"end\":73358,\"start\":73351},{\"end\":73630,\"start\":73626},{\"end\":73643,\"start\":73636},{\"end\":73658,\"start\":73651},{\"end\":73674,\"start\":73665},{\"end\":73681,\"start\":73679},{\"end\":73933,\"start\":73930},{\"end\":73947,\"start\":73942},{\"end\":73965,\"start\":73958},{\"end\":74448,\"start\":74441},{\"end\":74750,\"start\":74743},{\"end\":74759,\"start\":74751},{\"end\":74774,\"start\":74764},{\"end\":74789,\"start\":74781},{\"end\":74800,\"start\":74795},{\"end\":74816,\"start\":74809},{\"end\":75317,\"start\":75310},{\"end\":75326,\"start\":75324},{\"end\":75339,\"start\":75332},{\"end\":75349,\"start\":75345},{\"end\":75363,\"start\":75359},{\"end\":75365,\"start\":75364},{\"end\":75786,\"start\":75783},{\"end\":75800,\"start\":75793},{\"end\":75811,\"start\":75806},{\"end\":75818,\"start\":75817},{\"end\":75840,\"start\":75834},{\"end\":75842,\"start\":75841},{\"end\":76229,\"start\":76216},{\"end\":76245,\"start\":76238},{\"end\":76260,\"start\":76254},{\"end\":76570,\"start\":76563},{\"end\":76581,\"start\":76575},{\"end\":76594,\"start\":76588},{\"end\":76609,\"start\":76603},{\"end\":76622,\"start\":76614},{\"end\":76636,\"start\":76630},{\"end\":76652,\"start\":76643},{\"end\":77147,\"start\":77140},{\"end\":77159,\"start\":77153},{\"end\":77168,\"start\":77164},{\"end\":77182,\"start\":77174},{\"end\":77199,\"start\":77197},{\"end\":77716,\"start\":77711},{\"end\":77727,\"start\":77721},{\"end\":77740,\"start\":77734},{\"end\":77755,\"start\":77748},{\"end\":77765,\"start\":77761},{\"end\":77778,\"start\":77772},{\"end\":77792,\"start\":77784},{\"end\":78153,\"start\":78151},{\"end\":78166,\"start\":78159},{\"end\":78176,\"start\":78172},{\"end\":78190,\"start\":78183},{\"end\":78212,\"start\":78206},{\"end\":78224,\"start\":78217},{\"end\":78235,\"start\":78232},{\"end\":78244,\"start\":78240},{\"end\":78251,\"start\":78245},{\"end\":78267,\"start\":78261},{\"end\":78674,\"start\":78669},{\"end\":78686,\"start\":78681},{\"end\":78696,\"start\":78693},{\"end\":78711,\"start\":78703},{\"end\":79018,\"start\":79013},{\"end\":79027,\"start\":79023},{\"end\":79042,\"start\":79034},{\"end\":79054,\"start\":79048},{\"end\":79065,\"start\":79062},{\"end\":79077,\"start\":79072},{\"end\":79583,\"start\":79578},{\"end\":79592,\"start\":79588},{\"end\":79607,\"start\":79599},{\"end\":79619,\"start\":79613},{\"end\":79630,\"start\":79627},{\"end\":79642,\"start\":79637},{\"end\":80148,\"start\":80143},{\"end\":80162,\"start\":80155},{\"end\":80177,\"start\":80170},{\"end\":80190,\"start\":80183},{\"end\":80205,\"start\":80196},{\"end\":80217,\"start\":80210},{\"end\":80232,\"start\":80225},{\"end\":80244,\"start\":80240},{\"end\":80258,\"start\":80251},{\"end\":80270,\"start\":80266},{\"end\":80575,\"start\":80568},{\"end\":80592,\"start\":80583},{\"end\":80607,\"start\":80600},{\"end\":80620,\"start\":80612},{\"end\":80634,\"start\":80627},{\"end\":80641,\"start\":80635},{\"end\":80652,\"start\":80646},{\"end\":80664,\"start\":80658},{\"end\":80673,\"start\":80671},{\"end\":81067,\"start\":81062},{\"end\":81081,\"start\":81075},{\"end\":81096,\"start\":81088},{\"end\":81428,\"start\":81422},{\"end\":81440,\"start\":81436},{\"end\":81449,\"start\":81446},{\"end\":81465,\"start\":81457},{\"end\":81476,\"start\":81470},{\"end\":81486,\"start\":81483},{\"end\":81494,\"start\":81492},{\"end\":81505,\"start\":81501},{\"end\":81520,\"start\":81511},{\"end\":82006,\"start\":82000},{\"end\":82019,\"start\":82014},{\"end\":82035,\"start\":82029},{\"end\":82049,\"start\":82044},{\"end\":82486,\"start\":82477},{\"end\":82498,\"start\":82492},{\"end\":82511,\"start\":82506},{\"end\":82521,\"start\":82516},{\"end\":82537,\"start\":82528},{\"end\":82549,\"start\":82545}]", "bib_author_last_name": "[{\"end\":46591,\"start\":46580},{\"end\":46609,\"start\":46598},{\"end\":46626,\"start\":46616},{\"end\":46639,\"start\":46634},{\"end\":46918,\"start\":46905},{\"end\":46929,\"start\":46924},{\"end\":46938,\"start\":46931},{\"end\":47388,\"start\":47378},{\"end\":47403,\"start\":47395},{\"end\":47796,\"start\":47775},{\"end\":47813,\"start\":47806},{\"end\":47826,\"start\":47823},{\"end\":47838,\"start\":47833},{\"end\":47849,\"start\":47845},{\"end\":47863,\"start\":47857},{\"end\":47876,\"start\":47872},{\"end\":47890,\"start\":47884},{\"end\":47908,\"start\":47900},{\"end\":47918,\"start\":47910},{\"end\":48281,\"start\":48276},{\"end\":48295,\"start\":48290},{\"end\":48307,\"start\":48304},{\"end\":48325,\"start\":48318},{\"end\":48631,\"start\":48623},{\"end\":48868,\"start\":48862},{\"end\":48889,\"start\":48876},{\"end\":48902,\"start\":48897},{\"end\":48912,\"start\":48904},{\"end\":49305,\"start\":49300},{\"end\":49319,\"start\":49314},{\"end\":49340,\"start\":49330},{\"end\":49352,\"start\":49346},{\"end\":49369,\"start\":49361},{\"end\":49381,\"start\":49376},{\"end\":49392,\"start\":49390},{\"end\":49410,\"start\":49402},{\"end\":49424,\"start\":49419},{\"end\":49434,\"start\":49430},{\"end\":49438,\"start\":49436},{\"end\":49761,\"start\":49757},{\"end\":49774,\"start\":49771},{\"end\":49789,\"start\":49785},{\"end\":49800,\"start\":49797},{\"end\":49811,\"start\":49809},{\"end\":49822,\"start\":49820},{\"end\":49834,\"start\":49831},{\"end\":49843,\"start\":49839},{\"end\":49857,\"start\":49853},{\"end\":50264,\"start\":50260},{\"end\":50279,\"start\":50275},{\"end\":50296,\"start\":50288},{\"end\":50747,\"start\":50740},{\"end\":50761,\"start\":50757},{\"end\":50773,\"start\":50769},{\"end\":50792,\"start\":50786},{\"end\":50801,\"start\":50799},{\"end\":50817,\"start\":50809},{\"end\":50827,\"start\":50822},{\"end\":50849,\"start\":50842},{\"end\":50868,\"start\":50858},{\"end\":50884,\"start\":50879},{\"end\":51387,\"start\":51381},{\"end\":51403,\"start\":51396},{\"end\":51419,\"start\":51411},{\"end\":51431,\"start\":51426},{\"end\":51445,\"start\":51439},{\"end\":51461,\"start\":51451},{\"end\":51477,\"start\":51470},{\"end\":51491,\"start\":51485},{\"end\":51511,\"start\":51503},{\"end\":51524,\"start\":51517},{\"end\":51934,\"start\":51930},{\"end\":51948,\"start\":51942},{\"end\":51963,\"start\":51959},{\"end\":52361,\"start\":52357},{\"end\":52373,\"start\":52369},{\"end\":52385,\"start\":52382},{\"end\":52400,\"start\":52395},{\"end\":52410,\"start\":52407},{\"end\":52423,\"start\":52421},{\"end\":52729,\"start\":52723},{\"end\":52742,\"start\":52738},{\"end\":52752,\"start\":52749},{\"end\":52763,\"start\":52758},{\"end\":53214,\"start\":53212},{\"end\":53227,\"start\":53224},{\"end\":53236,\"start\":53233},{\"end\":53251,\"start\":53247},{\"end\":53266,\"start\":53262},{\"end\":53281,\"start\":53274},{\"end\":53294,\"start\":53291},{\"end\":53613,\"start\":53608},{\"end\":53627,\"start\":53621},{\"end\":53642,\"start\":53634},{\"end\":54103,\"start\":54101},{\"end\":54116,\"start\":54112},{\"end\":54312,\"start\":54307},{\"end\":54340,\"start\":54330},{\"end\":54347,\"start\":54342},{\"end\":54720,\"start\":54716},{\"end\":54734,\"start\":54728},{\"end\":54750,\"start\":54742},{\"end\":54766,\"start\":54758},{\"end\":55306,\"start\":55302},{\"end\":55322,\"start\":55313},{\"end\":55345,\"start\":55336},{\"end\":55363,\"start\":55355},{\"end\":55377,\"start\":55371},{\"end\":55676,\"start\":55672},{\"end\":55692,\"start\":55687},{\"end\":55703,\"start\":55700},{\"end\":55717,\"start\":55714},{\"end\":55727,\"start\":55723},{\"end\":55738,\"start\":55735},{\"end\":56033,\"start\":56028},{\"end\":56044,\"start\":56041},{\"end\":56057,\"start\":56052},{\"end\":56071,\"start\":56066},{\"end\":56084,\"start\":56081},{\"end\":56096,\"start\":56092},{\"end\":56107,\"start\":56102},{\"end\":56426,\"start\":56421},{\"end\":56438,\"start\":56434},{\"end\":56451,\"start\":56447},{\"end\":56467,\"start\":56462},{\"end\":56479,\"start\":56473},{\"end\":56490,\"start\":56487},{\"end\":56507,\"start\":56501},{\"end\":56512,\"start\":56509},{\"end\":56790,\"start\":56783},{\"end\":56809,\"start\":56801},{\"end\":56824,\"start\":56816},{\"end\":56837,\"start\":56831},{\"end\":56855,\"start\":56848},{\"end\":56868,\"start\":56863},{\"end\":56880,\"start\":56876},{\"end\":56898,\"start\":56891},{\"end\":56917,\"start\":56909},{\"end\":56930,\"start\":56924},{\"end\":56951,\"start\":56941},{\"end\":56964,\"start\":56957},{\"end\":56980,\"start\":56973},{\"end\":56990,\"start\":56982},{\"end\":57393,\"start\":57389},{\"end\":57409,\"start\":57399},{\"end\":57428,\"start\":57422},{\"end\":57443,\"start\":57437},{\"end\":57454,\"start\":57449},{\"end\":57877,\"start\":57856},{\"end\":57900,\"start\":57889},{\"end\":57909,\"start\":57907},{\"end\":57921,\"start\":57916},{\"end\":57931,\"start\":57927},{\"end\":57942,\"start\":57940},{\"end\":57955,\"start\":57951},{\"end\":57973,\"start\":57965},{\"end\":57988,\"start\":57982},{\"end\":58002,\"start\":57996},{\"end\":58391,\"start\":58384},{\"end\":58639,\"start\":58636},{\"end\":58652,\"start\":58648},{\"end\":58660,\"start\":58657},{\"end\":58674,\"start\":58670},{\"end\":58689,\"start\":58683},{\"end\":58700,\"start\":58696},{\"end\":58709,\"start\":58707},{\"end\":58725,\"start\":58721},{\"end\":58734,\"start\":58732},{\"end\":58746,\"start\":58740},{\"end\":59154,\"start\":59144},{\"end\":59167,\"start\":59163},{\"end\":59183,\"start\":59173},{\"end\":59198,\"start\":59194},{\"end\":59217,\"start\":59206},{\"end\":59225,\"start\":59219},{\"end\":59505,\"start\":59501},{\"end\":59520,\"start\":59517},{\"end\":59534,\"start\":59526},{\"end\":59551,\"start\":59543},{\"end\":59567,\"start\":59561},{\"end\":59873,\"start\":59867},{\"end\":59886,\"start\":59883},{\"end\":59905,\"start\":59895},{\"end\":59919,\"start\":59915},{\"end\":60189,\"start\":60186},{\"end\":60196,\"start\":60191},{\"end\":60475,\"start\":60473},{\"end\":60486,\"start\":60484},{\"end\":60501,\"start\":60496},{\"end\":60513,\"start\":60510},{\"end\":60871,\"start\":60854},{\"end\":60886,\"start\":60881},{\"end\":60901,\"start\":60896},{\"end\":60916,\"start\":60912},{\"end\":60925,\"start\":60923},{\"end\":60939,\"start\":60934},{\"end\":60948,\"start\":60944},{\"end\":60958,\"start\":60954},{\"end\":60975,\"start\":60970},{\"end\":60982,\"start\":60977},{\"end\":61552,\"start\":61533},{\"end\":61568,\"start\":61563},{\"end\":61581,\"start\":61577},{\"end\":61596,\"start\":61591},{\"end\":61601,\"start\":61598},{\"end\":62013,\"start\":62010},{\"end\":62025,\"start\":62022},{\"end\":62035,\"start\":62032},{\"end\":62049,\"start\":62046},{\"end\":62060,\"start\":62056},{\"end\":62075,\"start\":62068},{\"end\":62083,\"start\":62081},{\"end\":62415,\"start\":62412},{\"end\":62425,\"start\":62422},{\"end\":62436,\"start\":62434},{\"end\":62449,\"start\":62447},{\"end\":62462,\"start\":62460},{\"end\":62742,\"start\":62740},{\"end\":62755,\"start\":62753},{\"end\":62768,\"start\":62765},{\"end\":62782,\"start\":62779},{\"end\":62802,\"start\":62794},{\"end\":62816,\"start\":62809},{\"end\":62833,\"start\":62828},{\"end\":63178,\"start\":63176},{\"end\":63187,\"start\":63184},{\"end\":63200,\"start\":63197},{\"end\":63215,\"start\":63206},{\"end\":63219,\"start\":63217},{\"end\":63527,\"start\":63519},{\"end\":63546,\"start\":63539},{\"end\":63964,\"start\":63958},{\"end\":63980,\"start\":63976},{\"end\":63993,\"start\":63987},{\"end\":64011,\"start\":64002},{\"end\":64331,\"start\":64325},{\"end\":64343,\"start\":64337},{\"end\":64356,\"start\":64353},{\"end\":64370,\"start\":64364},{\"end\":64384,\"start\":64377},{\"end\":64783,\"start\":64777},{\"end\":64795,\"start\":64790},{\"end\":64809,\"start\":64802},{\"end\":64818,\"start\":64811},{\"end\":65019,\"start\":65013},{\"end\":65155,\"start\":65151},{\"end\":65169,\"start\":65165},{\"end\":65182,\"start\":65175},{\"end\":65191,\"start\":65188},{\"end\":65205,\"start\":65202},{\"end\":65214,\"start\":65210},{\"end\":65220,\"start\":65216},{\"end\":65622,\"start\":65618},{\"end\":65635,\"start\":65629},{\"end\":65648,\"start\":65643},{\"end\":65669,\"start\":65657},{\"end\":65685,\"start\":65676},{\"end\":65704,\"start\":65694},{\"end\":66067,\"start\":66058},{\"end\":66083,\"start\":66078},{\"end\":66093,\"start\":66089},{\"end\":66103,\"start\":66101},{\"end\":66119,\"start\":66116},{\"end\":66493,\"start\":66481},{\"end\":66505,\"start\":66503},{\"end\":66520,\"start\":66518},{\"end\":66528,\"start\":66522},{\"end\":67015,\"start\":66993},{\"end\":67023,\"start\":67021},{\"end\":67038,\"start\":67036},{\"end\":67046,\"start\":67040},{\"end\":67313,\"start\":67311},{\"end\":67326,\"start\":67322},{\"end\":67338,\"start\":67335},{\"end\":67348,\"start\":67346},{\"end\":67363,\"start\":67358},{\"end\":67376,\"start\":67374},{\"end\":67383,\"start\":67381},{\"end\":67752,\"start\":67748},{\"end\":67763,\"start\":67761},{\"end\":67776,\"start\":67772},{\"end\":67788,\"start\":67785},{\"end\":67803,\"start\":67796},{\"end\":67822,\"start\":67813},{\"end\":67838,\"start\":67832},{\"end\":68229,\"start\":68222},{\"end\":68244,\"start\":68241},{\"end\":68259,\"start\":68252},{\"end\":68274,\"start\":68268},{\"end\":68287,\"start\":68284},{\"end\":68305,\"start\":68298},{\"end\":68320,\"start\":68314},{\"end\":68335,\"start\":68329},{\"end\":68351,\"start\":68344},{\"end\":68363,\"start\":68358},{\"end\":68799,\"start\":68793},{\"end\":68818,\"start\":68810},{\"end\":68831,\"start\":68825},{\"end\":68842,\"start\":68839},{\"end\":68853,\"start\":68849},{\"end\":69174,\"start\":69171},{\"end\":69184,\"start\":69182},{\"end\":69194,\"start\":69190},{\"end\":69606,\"start\":69598},{\"end\":69624,\"start\":69618},{\"end\":69636,\"start\":69633},{\"end\":69654,\"start\":69647},{\"end\":69967,\"start\":69960},{\"end\":69986,\"start\":69977},{\"end\":70002,\"start\":69996},{\"end\":70017,\"start\":70012},{\"end\":70030,\"start\":70025},{\"end\":70519,\"start\":70507},{\"end\":70530,\"start\":70524},{\"end\":70542,\"start\":70539},{\"end\":70819,\"start\":70812},{\"end\":70833,\"start\":70829},{\"end\":70849,\"start\":70843},{\"end\":70858,\"start\":70856},{\"end\":70869,\"start\":70864},{\"end\":70883,\"start\":70877},{\"end\":70901,\"start\":70894},{\"end\":70922,\"start\":70917},{\"end\":71424,\"start\":71418},{\"end\":71782,\"start\":71774},{\"end\":71870,\"start\":71864},{\"end\":71880,\"start\":71877},{\"end\":71903,\"start\":71885},{\"end\":71917,\"start\":71913},{\"end\":71930,\"start\":71925},{\"end\":71951,\"start\":71945},{\"end\":71962,\"start\":71953},{\"end\":72478,\"start\":72472},{\"end\":72494,\"start\":72487},{\"end\":72825,\"start\":72816},{\"end\":72842,\"start\":72834},{\"end\":72857,\"start\":72852},{\"end\":72870,\"start\":72864},{\"end\":72885,\"start\":72877},{\"end\":72899,\"start\":72893},{\"end\":72913,\"start\":72906},{\"end\":72927,\"start\":72922},{\"end\":72943,\"start\":72937},{\"end\":72962,\"start\":72954},{\"end\":73349,\"start\":73343},{\"end\":73363,\"start\":73359},{\"end\":73634,\"start\":73631},{\"end\":73649,\"start\":73644},{\"end\":73663,\"start\":73659},{\"end\":73677,\"start\":73675},{\"end\":73686,\"start\":73682},{\"end\":73940,\"start\":73934},{\"end\":73956,\"start\":73948},{\"end\":73972,\"start\":73966},{\"end\":74458,\"start\":74449},{\"end\":74762,\"start\":74760},{\"end\":74779,\"start\":74775},{\"end\":74793,\"start\":74790},{\"end\":74807,\"start\":74801},{\"end\":74822,\"start\":74817},{\"end\":75322,\"start\":75318},{\"end\":75330,\"start\":75327},{\"end\":75343,\"start\":75340},{\"end\":75357,\"start\":75350},{\"end\":75372,\"start\":75366},{\"end\":75791,\"start\":75787},{\"end\":75804,\"start\":75801},{\"end\":75815,\"start\":75812},{\"end\":75825,\"start\":75819},{\"end\":75832,\"start\":75827},{\"end\":75862,\"start\":75843},{\"end\":75871,\"start\":75864},{\"end\":76236,\"start\":76230},{\"end\":76252,\"start\":76246},{\"end\":76267,\"start\":76261},{\"end\":76573,\"start\":76571},{\"end\":76586,\"start\":76582},{\"end\":76601,\"start\":76595},{\"end\":76612,\"start\":76610},{\"end\":76628,\"start\":76623},{\"end\":76641,\"start\":76637},{\"end\":76657,\"start\":76653},{\"end\":77151,\"start\":77148},{\"end\":77162,\"start\":77160},{\"end\":77172,\"start\":77169},{\"end\":77195,\"start\":77183},{\"end\":77206,\"start\":77200},{\"end\":77214,\"start\":77208},{\"end\":77719,\"start\":77717},{\"end\":77732,\"start\":77728},{\"end\":77746,\"start\":77741},{\"end\":77759,\"start\":77756},{\"end\":77770,\"start\":77766},{\"end\":77782,\"start\":77779},{\"end\":77796,\"start\":77793},{\"end\":78157,\"start\":78154},{\"end\":78170,\"start\":78167},{\"end\":78181,\"start\":78177},{\"end\":78204,\"start\":78191},{\"end\":78215,\"start\":78213},{\"end\":78230,\"start\":78225},{\"end\":78238,\"start\":78236},{\"end\":78259,\"start\":78252},{\"end\":78276,\"start\":78268},{\"end\":78679,\"start\":78675},{\"end\":78691,\"start\":78687},{\"end\":78701,\"start\":78697},{\"end\":78714,\"start\":78712},{\"end\":79021,\"start\":79019},{\"end\":79032,\"start\":79028},{\"end\":79046,\"start\":79043},{\"end\":79060,\"start\":79055},{\"end\":79070,\"start\":79066},{\"end\":79080,\"start\":79078},{\"end\":79586,\"start\":79584},{\"end\":79597,\"start\":79593},{\"end\":79611,\"start\":79608},{\"end\":79625,\"start\":79620},{\"end\":79635,\"start\":79631},{\"end\":79645,\"start\":79643},{\"end\":80153,\"start\":80149},{\"end\":80168,\"start\":80163},{\"end\":80181,\"start\":80178},{\"end\":80194,\"start\":80191},{\"end\":80208,\"start\":80206},{\"end\":80223,\"start\":80218},{\"end\":80238,\"start\":80233},{\"end\":80249,\"start\":80245},{\"end\":80264,\"start\":80259},{\"end\":80273,\"start\":80271},{\"end\":80581,\"start\":80576},{\"end\":80598,\"start\":80593},{\"end\":80610,\"start\":80608},{\"end\":80625,\"start\":80621},{\"end\":80644,\"start\":80642},{\"end\":80656,\"start\":80653},{\"end\":80669,\"start\":80665},{\"end\":80678,\"start\":80674},{\"end\":81073,\"start\":81068},{\"end\":81086,\"start\":81082},{\"end\":81099,\"start\":81097},{\"end\":81434,\"start\":81429},{\"end\":81444,\"start\":81441},{\"end\":81455,\"start\":81450},{\"end\":81468,\"start\":81466},{\"end\":81481,\"start\":81477},{\"end\":81490,\"start\":81487},{\"end\":81499,\"start\":81495},{\"end\":81509,\"start\":81506},{\"end\":81523,\"start\":81521},{\"end\":82012,\"start\":82007},{\"end\":82027,\"start\":82020},{\"end\":82042,\"start\":82036},{\"end\":82055,\"start\":82050},{\"end\":82490,\"start\":82487},{\"end\":82504,\"start\":82499},{\"end\":82514,\"start\":82512},{\"end\":82526,\"start\":82522},{\"end\":82543,\"start\":82538},{\"end\":82553,\"start\":82550}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2304.04909\",\"id\":\"b0\"},\"end\":46832,\"start\":46522},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":214714283},\"end\":47302,\"start\":46834},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":23102425},\"end\":47711,\"start\":47304},{\"attributes\":{\"doi\":\"arXiv:2204.14198\",\"id\":\"b3\"},\"end\":48202,\"start\":47713},{\"attributes\":{\"doi\":\"arXiv:2212.01406\",\"id\":\"b4\"},\"end\":48522,\"start\":48204},{\"attributes\":{\"id\":\"b5\"},\"end\":48790,\"start\":48524},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":246823836},\"end\":49296,\"start\":48792},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b7\"},\"end\":49748,\"start\":49298},{\"attributes\":{\"doi\":\"arXiv:2301.04926\",\"id\":\"b8\"},\"end\":50176,\"start\":49750},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":121123422},\"end\":50667,\"start\":50178},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":238634701},\"end\":51374,\"start\":50669},{\"attributes\":{\"doi\":\"arXiv:2212.08051\",\"id\":\"b11\"},\"end\":51842,\"start\":51376},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":52131369},\"end\":52294,\"start\":51844},{\"attributes\":{\"doi\":\"arXiv:2211.16312\",\"id\":\"b13\"},\"end\":52628,\"start\":52296},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":235719240},\"end\":53161,\"start\":52630},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":221819358},\"end\":53540,\"start\":53163},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195441339},\"end\":54007,\"start\":53542},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":251040433},\"end\":54298,\"start\":54009},{\"attributes\":{\"doi\":\"arXiv:2303.11313\",\"id\":\"b18\"},\"end\":54633,\"start\":54300},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":253098267},\"end\":55241,\"start\":54635},{\"attributes\":{\"doi\":\"arXiv:2212.06858\",\"id\":\"b20\"},\"end\":55587,\"start\":55243},{\"attributes\":{\"doi\":\"arXiv:2205.08535\",\"id\":\"b21\"},\"end\":55963,\"start\":55589},{\"attributes\":{\"doi\":\"arXiv:2301.07584\",\"id\":\"b22\"},\"end\":56325,\"start\":55965},{\"attributes\":{\"doi\":\"arXiv:2210.01055\",\"id\":\"b23\"},\"end\":56771,\"start\":56327},{\"attributes\":{\"id\":\"b24\"},\"end\":57323,\"start\":56773},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":244799255},\"end\":57846,\"start\":57325},{\"attributes\":{\"doi\":\"arXiv:2302.07241\",\"id\":\"b26\"},\"end\":58314,\"start\":57848},{\"attributes\":{\"doi\":\"arXiv:2109.12922\",\"id\":\"b27\"},\"end\":58538,\"start\":58316},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":231879586},\"end\":59088,\"start\":58540},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":222124996},\"end\":59492,\"start\":59090},{\"attributes\":{\"doi\":\"arXiv:2303.09553\",\"id\":\"b30\"},\"end\":59798,\"start\":59494},{\"attributes\":{\"doi\":\"arXiv:2203.13333\",\"id\":\"b31\"},\"end\":60114,\"start\":59800},{\"attributes\":{\"doi\":\"arXiv:2209.15172\",\"id\":\"b32\"},\"end\":60359,\"start\":60116},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":246411402},\"end\":60804,\"start\":60361},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":244920947},\"end\":61428,\"start\":60806},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":247244904},\"end\":61905,\"start\":61430},{\"attributes\":{\"doi\":\"arXiv:2212.01558\",\"id\":\"b36\"},\"end\":62335,\"start\":61907},{\"attributes\":{\"doi\":\"arXiv:2209.04145\",\"id\":\"b37\"},\"end\":62663,\"start\":62337},{\"attributes\":{\"doi\":\"arXiv:2304.00788\",\"id\":\"b38\"},\"end\":63077,\"start\":62665},{\"attributes\":{\"doi\":\"arXiv:2202.07123\",\"id\":\"b39\"},\"end\":63435,\"start\":63079},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":14620252},\"end\":63859,\"start\":63437},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":237951544},\"end\":64264,\"start\":63861},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":244908764},\"end\":64771,\"start\":64266},{\"attributes\":{\"doi\":\"arXiv:2111.09734\",\"id\":\"b43\"},\"end\":65009,\"start\":64773},{\"attributes\":{\"id\":\"b44\"},\"end\":65080,\"start\":65011},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":247447528},\"end\":65608,\"start\":65082},{\"attributes\":{\"doi\":\"arXiv:2211.15654\",\"id\":\"b46\"},\"end\":65982,\"start\":65610},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":220935803},\"end\":66397,\"start\":65984},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":5115938},\"end\":66908,\"start\":66399},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1745976},\"end\":67303,\"start\":66910},{\"attributes\":{\"doi\":\"arXiv:2302.02318\",\"id\":\"b50\"},\"end\":67737,\"start\":67305},{\"attributes\":{\"doi\":\"arXiv:2206.04670\",\"id\":\"b51\"},\"end\":68144,\"start\":67739},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":231591445},\"end\":68718,\"start\":68146},{\"attributes\":{\"doi\":\"arXiv:2204.06125\",\"id\":\"b53\"},\"end\":69062,\"start\":68720},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":214713787},\"end\":69589,\"start\":69064},{\"attributes\":{\"doi\":\"arXiv:2010.04592\",\"id\":\"b55\"},\"end\":69891,\"start\":69591},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":245335280},\"end\":70436,\"start\":69893},{\"attributes\":{\"doi\":\"arXiv:2204.07761\",\"id\":\"b57\"},\"end\":70722,\"start\":70438},{\"attributes\":{\"doi\":\"arXiv:2205.11487\",\"id\":\"b58\",\"matched_paper_id\":248986576},\"end\":71299,\"start\":70724},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":219304526},\"end\":71770,\"start\":71301},{\"attributes\":{\"id\":\"b60\"},\"end\":71799,\"start\":71772},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":238408362},\"end\":72390,\"start\":71801},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":173990644},\"end\":72718,\"start\":72392},{\"attributes\":{\"doi\":\"arXiv:2210.08402\",\"id\":\"b63\"},\"end\":73284,\"start\":72720},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":221995690},\"end\":73570,\"start\":73286},{\"attributes\":{\"id\":\"b65\"},\"end\":73865,\"start\":73572},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":220892094},\"end\":74370,\"start\":73867},{\"attributes\":{\"id\":\"b67\"},\"end\":74633,\"start\":74372},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":199552106},\"end\":75244,\"start\":74635},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":238857344},\"end\":75733,\"start\":75246},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":94822},\"end\":76106,\"start\":75735},{\"attributes\":{\"doi\":\"arXiv:2103.06134\",\"id\":\"b71\"},\"end\":76502,\"start\":76108},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":206592833},\"end\":77063,\"start\":76504},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":220666215},\"end\":77612,\"start\":77065},{\"attributes\":{\"doi\":\"arXiv:2212.14704\",\"id\":\"b74\"},\"end\":78054,\"start\":77614},{\"attributes\":{\"doi\":\"arXiv:2212.05171\",\"id\":\"b75\"},\"end\":78572,\"start\":78056},{\"attributes\":{\"doi\":\"arXiv:2304.00962\",\"id\":\"b76\"},\"end\":78930,\"start\":78574},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":244714512},\"end\":79495,\"start\":78932},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":244714512},\"end\":80054,\"start\":79497},{\"attributes\":{\"doi\":\"arXiv:2303.12417\",\"id\":\"b79\"},\"end\":80566,\"start\":80056},{\"attributes\":{\"doi\":\"arXiv:2206.05836\",\"id\":\"b80\"},\"end\":81060,\"start\":80568},{\"attributes\":{\"doi\":\"arXiv:2303.04748\",\"id\":\"b81\"},\"end\":81374,\"start\":81062},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":244909021},\"end\":81935,\"start\":81376},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":230799166},\"end\":82410,\"start\":81937},{\"attributes\":{\"doi\":\"arXiv:2211.11682\",\"id\":\"b84\"},\"end\":82772,\"start\":82412}]", "bib_title": "[{\"end\":46896,\"start\":46834},{\"end\":47370,\"start\":47304},{\"end\":48851,\"start\":48792},{\"end\":50246,\"start\":50178},{\"end\":50730,\"start\":50669},{\"end\":51921,\"start\":51844},{\"end\":52712,\"start\":52630},{\"end\":53205,\"start\":53163},{\"end\":53600,\"start\":53542},{\"end\":54095,\"start\":54009},{\"end\":54708,\"start\":54635},{\"end\":57382,\"start\":57325},{\"end\":58629,\"start\":58540},{\"end\":59135,\"start\":59090},{\"end\":60464,\"start\":60361},{\"end\":60842,\"start\":60806},{\"end\":61525,\"start\":61430},{\"end\":63510,\"start\":63437},{\"end\":63947,\"start\":63861},{\"end\":64317,\"start\":64266},{\"end\":65142,\"start\":65082},{\"end\":66051,\"start\":65984},{\"end\":66475,\"start\":66399},{\"end\":66988,\"start\":66910},{\"end\":68215,\"start\":68146},{\"end\":69160,\"start\":69064},{\"end\":69952,\"start\":69893},{\"end\":70802,\"start\":70724},{\"end\":71409,\"start\":71301},{\"end\":71855,\"start\":71801},{\"end\":72461,\"start\":72392},{\"end\":73335,\"start\":73286},{\"end\":73928,\"start\":73867},{\"end\":74741,\"start\":74635},{\"end\":75308,\"start\":75246},{\"end\":75781,\"start\":75735},{\"end\":76561,\"start\":76504},{\"end\":77138,\"start\":77065},{\"end\":79011,\"start\":78932},{\"end\":79576,\"start\":79497},{\"end\":81420,\"start\":81376},{\"end\":81998,\"start\":81937}]", "bib_author": "[{\"end\":46593,\"start\":46574},{\"end\":46611,\"start\":46593},{\"end\":46628,\"start\":46611},{\"end\":46641,\"start\":46628},{\"end\":46920,\"start\":46898},{\"end\":46931,\"start\":46920},{\"end\":46940,\"start\":46931},{\"end\":47390,\"start\":47372},{\"end\":47405,\"start\":47390},{\"end\":47798,\"start\":47770},{\"end\":47815,\"start\":47798},{\"end\":47828,\"start\":47815},{\"end\":47840,\"start\":47828},{\"end\":47851,\"start\":47840},{\"end\":47865,\"start\":47851},{\"end\":47878,\"start\":47865},{\"end\":47892,\"start\":47878},{\"end\":47910,\"start\":47892},{\"end\":47920,\"start\":47910},{\"end\":48283,\"start\":48267},{\"end\":48297,\"start\":48283},{\"end\":48309,\"start\":48297},{\"end\":48327,\"start\":48309},{\"end\":48633,\"start\":48616},{\"end\":48870,\"start\":48853},{\"end\":48891,\"start\":48870},{\"end\":48904,\"start\":48891},{\"end\":48914,\"start\":48904},{\"end\":49307,\"start\":49298},{\"end\":49321,\"start\":49307},{\"end\":49342,\"start\":49321},{\"end\":49354,\"start\":49342},{\"end\":49371,\"start\":49354},{\"end\":49383,\"start\":49371},{\"end\":49394,\"start\":49383},{\"end\":49412,\"start\":49394},{\"end\":49426,\"start\":49412},{\"end\":49436,\"start\":49426},{\"end\":49440,\"start\":49436},{\"end\":49763,\"start\":49750},{\"end\":49776,\"start\":49763},{\"end\":49791,\"start\":49776},{\"end\":49802,\"start\":49791},{\"end\":49813,\"start\":49802},{\"end\":49824,\"start\":49813},{\"end\":49836,\"start\":49824},{\"end\":49845,\"start\":49836},{\"end\":49859,\"start\":49845},{\"end\":50266,\"start\":50248},{\"end\":50281,\"start\":50266},{\"end\":50298,\"start\":50281},{\"end\":50749,\"start\":50732},{\"end\":50763,\"start\":50749},{\"end\":50775,\"start\":50763},{\"end\":50794,\"start\":50775},{\"end\":50803,\"start\":50794},{\"end\":50819,\"start\":50803},{\"end\":50829,\"start\":50819},{\"end\":50851,\"start\":50829},{\"end\":50870,\"start\":50851},{\"end\":50886,\"start\":50870},{\"end\":51389,\"start\":51376},{\"end\":51405,\"start\":51389},{\"end\":51421,\"start\":51405},{\"end\":51433,\"start\":51421},{\"end\":51447,\"start\":51433},{\"end\":51463,\"start\":51447},{\"end\":51479,\"start\":51463},{\"end\":51493,\"start\":51479},{\"end\":51513,\"start\":51493},{\"end\":51526,\"start\":51513},{\"end\":51936,\"start\":51923},{\"end\":51950,\"start\":51936},{\"end\":51965,\"start\":51950},{\"end\":52363,\"start\":52351},{\"end\":52375,\"start\":52363},{\"end\":52387,\"start\":52375},{\"end\":52402,\"start\":52387},{\"end\":52412,\"start\":52402},{\"end\":52425,\"start\":52412},{\"end\":52731,\"start\":52714},{\"end\":52744,\"start\":52731},{\"end\":52754,\"start\":52744},{\"end\":52765,\"start\":52754},{\"end\":53216,\"start\":53207},{\"end\":53229,\"start\":53216},{\"end\":53238,\"start\":53229},{\"end\":53253,\"start\":53238},{\"end\":53268,\"start\":53253},{\"end\":53283,\"start\":53268},{\"end\":53296,\"start\":53283},{\"end\":53615,\"start\":53602},{\"end\":53629,\"start\":53615},{\"end\":53644,\"start\":53629},{\"end\":54105,\"start\":54097},{\"end\":54118,\"start\":54105},{\"end\":54314,\"start\":54300},{\"end\":54342,\"start\":54314},{\"end\":54349,\"start\":54342},{\"end\":54722,\"start\":54710},{\"end\":54736,\"start\":54722},{\"end\":54752,\"start\":54736},{\"end\":54768,\"start\":54752},{\"end\":55308,\"start\":55296},{\"end\":55324,\"start\":55308},{\"end\":55347,\"start\":55324},{\"end\":55365,\"start\":55347},{\"end\":55379,\"start\":55365},{\"end\":55678,\"start\":55663},{\"end\":55694,\"start\":55678},{\"end\":55705,\"start\":55694},{\"end\":55719,\"start\":55705},{\"end\":55729,\"start\":55719},{\"end\":55740,\"start\":55729},{\"end\":56035,\"start\":56024},{\"end\":56046,\"start\":56035},{\"end\":56059,\"start\":56046},{\"end\":56073,\"start\":56059},{\"end\":56086,\"start\":56073},{\"end\":56098,\"start\":56086},{\"end\":56109,\"start\":56098},{\"end\":56428,\"start\":56414},{\"end\":56440,\"start\":56428},{\"end\":56453,\"start\":56440},{\"end\":56469,\"start\":56453},{\"end\":56481,\"start\":56469},{\"end\":56492,\"start\":56481},{\"end\":56509,\"start\":56492},{\"end\":56514,\"start\":56509},{\"end\":56792,\"start\":56775},{\"end\":56811,\"start\":56792},{\"end\":56826,\"start\":56811},{\"end\":56839,\"start\":56826},{\"end\":56857,\"start\":56839},{\"end\":56870,\"start\":56857},{\"end\":56882,\"start\":56870},{\"end\":56900,\"start\":56882},{\"end\":56919,\"start\":56900},{\"end\":56932,\"start\":56919},{\"end\":56953,\"start\":56932},{\"end\":56966,\"start\":56953},{\"end\":56982,\"start\":56966},{\"end\":56992,\"start\":56982},{\"end\":57395,\"start\":57384},{\"end\":57411,\"start\":57395},{\"end\":57430,\"start\":57411},{\"end\":57445,\"start\":57430},{\"end\":57456,\"start\":57445},{\"end\":57879,\"start\":57848},{\"end\":57902,\"start\":57879},{\"end\":57911,\"start\":57902},{\"end\":57923,\"start\":57911},{\"end\":57933,\"start\":57923},{\"end\":57944,\"start\":57933},{\"end\":57957,\"start\":57944},{\"end\":57975,\"start\":57957},{\"end\":57990,\"start\":57975},{\"end\":58004,\"start\":57990},{\"end\":58393,\"start\":58376},{\"end\":58641,\"start\":58631},{\"end\":58654,\"start\":58641},{\"end\":58662,\"start\":58654},{\"end\":58676,\"start\":58662},{\"end\":58691,\"start\":58676},{\"end\":58702,\"start\":58691},{\"end\":58711,\"start\":58702},{\"end\":58727,\"start\":58711},{\"end\":58736,\"start\":58727},{\"end\":58748,\"start\":58736},{\"end\":59156,\"start\":59137},{\"end\":59169,\"start\":59156},{\"end\":59185,\"start\":59169},{\"end\":59200,\"start\":59185},{\"end\":59219,\"start\":59200},{\"end\":59227,\"start\":59219},{\"end\":59507,\"start\":59494},{\"end\":59522,\"start\":59507},{\"end\":59536,\"start\":59522},{\"end\":59553,\"start\":59536},{\"end\":59569,\"start\":59553},{\"end\":59875,\"start\":59861},{\"end\":59888,\"start\":59875},{\"end\":59907,\"start\":59888},{\"end\":59921,\"start\":59907},{\"end\":60191,\"start\":60177},{\"end\":60198,\"start\":60191},{\"end\":60477,\"start\":60466},{\"end\":60488,\"start\":60477},{\"end\":60503,\"start\":60488},{\"end\":60515,\"start\":60503},{\"end\":60873,\"start\":60844},{\"end\":60888,\"start\":60873},{\"end\":60903,\"start\":60888},{\"end\":60918,\"start\":60903},{\"end\":60927,\"start\":60918},{\"end\":60941,\"start\":60927},{\"end\":60950,\"start\":60941},{\"end\":60960,\"start\":60950},{\"end\":60977,\"start\":60960},{\"end\":60984,\"start\":60977},{\"end\":61554,\"start\":61527},{\"end\":61570,\"start\":61554},{\"end\":61583,\"start\":61570},{\"end\":61598,\"start\":61583},{\"end\":61603,\"start\":61598},{\"end\":62015,\"start\":62002},{\"end\":62027,\"start\":62015},{\"end\":62037,\"start\":62027},{\"end\":62051,\"start\":62037},{\"end\":62062,\"start\":62051},{\"end\":62077,\"start\":62062},{\"end\":62085,\"start\":62077},{\"end\":62417,\"start\":62403},{\"end\":62427,\"start\":62417},{\"end\":62438,\"start\":62427},{\"end\":62451,\"start\":62438},{\"end\":62464,\"start\":62451},{\"end\":62744,\"start\":62733},{\"end\":62757,\"start\":62744},{\"end\":62770,\"start\":62757},{\"end\":62784,\"start\":62770},{\"end\":62804,\"start\":62784},{\"end\":62818,\"start\":62804},{\"end\":62835,\"start\":62818},{\"end\":63180,\"start\":63173},{\"end\":63189,\"start\":63180},{\"end\":63202,\"start\":63189},{\"end\":63217,\"start\":63202},{\"end\":63221,\"start\":63217},{\"end\":63529,\"start\":63512},{\"end\":63548,\"start\":63529},{\"end\":63966,\"start\":63949},{\"end\":63982,\"start\":63966},{\"end\":63995,\"start\":63982},{\"end\":64013,\"start\":63995},{\"end\":64333,\"start\":64319},{\"end\":64345,\"start\":64333},{\"end\":64358,\"start\":64345},{\"end\":64372,\"start\":64358},{\"end\":64386,\"start\":64372},{\"end\":64785,\"start\":64773},{\"end\":64797,\"start\":64785},{\"end\":64811,\"start\":64797},{\"end\":64820,\"start\":64811},{\"end\":65021,\"start\":65013},{\"end\":65157,\"start\":65144},{\"end\":65171,\"start\":65157},{\"end\":65184,\"start\":65171},{\"end\":65193,\"start\":65184},{\"end\":65207,\"start\":65193},{\"end\":65216,\"start\":65207},{\"end\":65222,\"start\":65216},{\"end\":65624,\"start\":65610},{\"end\":65637,\"start\":65624},{\"end\":65650,\"start\":65637},{\"end\":65671,\"start\":65650},{\"end\":65687,\"start\":65671},{\"end\":65706,\"start\":65687},{\"end\":66069,\"start\":66053},{\"end\":66085,\"start\":66069},{\"end\":66095,\"start\":66085},{\"end\":66105,\"start\":66095},{\"end\":66121,\"start\":66105},{\"end\":66495,\"start\":66477},{\"end\":66507,\"start\":66495},{\"end\":66522,\"start\":66507},{\"end\":66530,\"start\":66522},{\"end\":67017,\"start\":66990},{\"end\":67025,\"start\":67017},{\"end\":67040,\"start\":67025},{\"end\":67048,\"start\":67040},{\"end\":67315,\"start\":67305},{\"end\":67328,\"start\":67315},{\"end\":67340,\"start\":67328},{\"end\":67350,\"start\":67340},{\"end\":67365,\"start\":67350},{\"end\":67378,\"start\":67365},{\"end\":67385,\"start\":67378},{\"end\":67754,\"start\":67739},{\"end\":67765,\"start\":67754},{\"end\":67778,\"start\":67765},{\"end\":67790,\"start\":67778},{\"end\":67805,\"start\":67790},{\"end\":67824,\"start\":67805},{\"end\":67840,\"start\":67824},{\"end\":68231,\"start\":68217},{\"end\":68246,\"start\":68231},{\"end\":68261,\"start\":68246},{\"end\":68276,\"start\":68261},{\"end\":68289,\"start\":68276},{\"end\":68307,\"start\":68289},{\"end\":68322,\"start\":68307},{\"end\":68337,\"start\":68322},{\"end\":68353,\"start\":68337},{\"end\":68365,\"start\":68353},{\"end\":68801,\"start\":68786},{\"end\":68820,\"start\":68801},{\"end\":68833,\"start\":68820},{\"end\":68844,\"start\":68833},{\"end\":68855,\"start\":68844},{\"end\":69176,\"start\":69162},{\"end\":69186,\"start\":69176},{\"end\":69196,\"start\":69186},{\"end\":69608,\"start\":69591},{\"end\":69626,\"start\":69608},{\"end\":69638,\"start\":69626},{\"end\":69656,\"start\":69638},{\"end\":69969,\"start\":69954},{\"end\":69988,\"start\":69969},{\"end\":70004,\"start\":69988},{\"end\":70019,\"start\":70004},{\"end\":70032,\"start\":70019},{\"end\":70521,\"start\":70501},{\"end\":70532,\"start\":70521},{\"end\":70544,\"start\":70532},{\"end\":70821,\"start\":70804},{\"end\":70835,\"start\":70821},{\"end\":70851,\"start\":70835},{\"end\":70860,\"start\":70851},{\"end\":70871,\"start\":70860},{\"end\":70885,\"start\":70871},{\"end\":70903,\"start\":70885},{\"end\":70924,\"start\":70903},{\"end\":71426,\"start\":71411},{\"end\":71784,\"start\":71774},{\"end\":71872,\"start\":71857},{\"end\":71882,\"start\":71872},{\"end\":71905,\"start\":71882},{\"end\":71919,\"start\":71905},{\"end\":71932,\"start\":71919},{\"end\":71953,\"start\":71932},{\"end\":71964,\"start\":71953},{\"end\":72480,\"start\":72463},{\"end\":72496,\"start\":72480},{\"end\":72827,\"start\":72806},{\"end\":72844,\"start\":72827},{\"end\":72859,\"start\":72844},{\"end\":72872,\"start\":72859},{\"end\":72887,\"start\":72872},{\"end\":72901,\"start\":72887},{\"end\":72915,\"start\":72901},{\"end\":72929,\"start\":72915},{\"end\":72945,\"start\":72929},{\"end\":72964,\"start\":72945},{\"end\":73351,\"start\":73337},{\"end\":73365,\"start\":73351},{\"end\":73636,\"start\":73626},{\"end\":73651,\"start\":73636},{\"end\":73665,\"start\":73651},{\"end\":73679,\"start\":73665},{\"end\":73688,\"start\":73679},{\"end\":73942,\"start\":73930},{\"end\":73958,\"start\":73942},{\"end\":73974,\"start\":73958},{\"end\":74460,\"start\":74441},{\"end\":74764,\"start\":74743},{\"end\":74781,\"start\":74764},{\"end\":74795,\"start\":74781},{\"end\":74809,\"start\":74795},{\"end\":74824,\"start\":74809},{\"end\":75324,\"start\":75310},{\"end\":75332,\"start\":75324},{\"end\":75345,\"start\":75332},{\"end\":75359,\"start\":75345},{\"end\":75374,\"start\":75359},{\"end\":75793,\"start\":75783},{\"end\":75806,\"start\":75793},{\"end\":75817,\"start\":75806},{\"end\":75827,\"start\":75817},{\"end\":75834,\"start\":75827},{\"end\":75864,\"start\":75834},{\"end\":75873,\"start\":75864},{\"end\":76238,\"start\":76216},{\"end\":76254,\"start\":76238},{\"end\":76269,\"start\":76254},{\"end\":76575,\"start\":76563},{\"end\":76588,\"start\":76575},{\"end\":76603,\"start\":76588},{\"end\":76614,\"start\":76603},{\"end\":76630,\"start\":76614},{\"end\":76643,\"start\":76630},{\"end\":76659,\"start\":76643},{\"end\":77153,\"start\":77140},{\"end\":77164,\"start\":77153},{\"end\":77174,\"start\":77164},{\"end\":77197,\"start\":77174},{\"end\":77208,\"start\":77197},{\"end\":77216,\"start\":77208},{\"end\":77721,\"start\":77711},{\"end\":77734,\"start\":77721},{\"end\":77748,\"start\":77734},{\"end\":77761,\"start\":77748},{\"end\":77772,\"start\":77761},{\"end\":77784,\"start\":77772},{\"end\":77798,\"start\":77784},{\"end\":78159,\"start\":78151},{\"end\":78172,\"start\":78159},{\"end\":78183,\"start\":78172},{\"end\":78206,\"start\":78183},{\"end\":78217,\"start\":78206},{\"end\":78232,\"start\":78217},{\"end\":78240,\"start\":78232},{\"end\":78261,\"start\":78240},{\"end\":78278,\"start\":78261},{\"end\":78681,\"start\":78669},{\"end\":78693,\"start\":78681},{\"end\":78703,\"start\":78693},{\"end\":78716,\"start\":78703},{\"end\":79023,\"start\":79013},{\"end\":79034,\"start\":79023},{\"end\":79048,\"start\":79034},{\"end\":79062,\"start\":79048},{\"end\":79072,\"start\":79062},{\"end\":79082,\"start\":79072},{\"end\":79588,\"start\":79578},{\"end\":79599,\"start\":79588},{\"end\":79613,\"start\":79599},{\"end\":79627,\"start\":79613},{\"end\":79637,\"start\":79627},{\"end\":79647,\"start\":79637},{\"end\":80155,\"start\":80143},{\"end\":80170,\"start\":80155},{\"end\":80183,\"start\":80170},{\"end\":80196,\"start\":80183},{\"end\":80210,\"start\":80196},{\"end\":80225,\"start\":80210},{\"end\":80240,\"start\":80225},{\"end\":80251,\"start\":80240},{\"end\":80266,\"start\":80251},{\"end\":80275,\"start\":80266},{\"end\":80583,\"start\":80568},{\"end\":80600,\"start\":80583},{\"end\":80612,\"start\":80600},{\"end\":80627,\"start\":80612},{\"end\":80646,\"start\":80627},{\"end\":80658,\"start\":80646},{\"end\":80671,\"start\":80658},{\"end\":80680,\"start\":80671},{\"end\":81075,\"start\":81062},{\"end\":81088,\"start\":81075},{\"end\":81101,\"start\":81088},{\"end\":81436,\"start\":81422},{\"end\":81446,\"start\":81436},{\"end\":81457,\"start\":81446},{\"end\":81470,\"start\":81457},{\"end\":81483,\"start\":81470},{\"end\":81492,\"start\":81483},{\"end\":81501,\"start\":81492},{\"end\":81511,\"start\":81501},{\"end\":81525,\"start\":81511},{\"end\":82014,\"start\":82000},{\"end\":82029,\"start\":82014},{\"end\":82044,\"start\":82029},{\"end\":82057,\"start\":82044},{\"end\":82492,\"start\":82477},{\"end\":82506,\"start\":82492},{\"end\":82516,\"start\":82506},{\"end\":82528,\"start\":82516},{\"end\":82545,\"start\":82528},{\"end\":82555,\"start\":82545}]", "bib_venue": "[{\"end\":47087,\"start\":47022},{\"end\":49061,\"start\":48996},{\"end\":50439,\"start\":50377},{\"end\":51035,\"start\":50969},{\"end\":52080,\"start\":52031},{\"end\":52914,\"start\":52848},{\"end\":53793,\"start\":53727},{\"end\":54915,\"start\":54850},{\"end\":57605,\"start\":57539},{\"end\":61133,\"start\":61067},{\"end\":64535,\"start\":64469},{\"end\":65291,\"start\":65275},{\"end\":66671,\"start\":66609},{\"end\":69345,\"start\":69279},{\"end\":70181,\"start\":70115},{\"end\":71490,\"start\":71479},{\"end\":72113,\"start\":72047},{\"end\":74143,\"start\":74067},{\"end\":74953,\"start\":74897},{\"end\":75503,\"start\":75447},{\"end\":76800,\"start\":76738},{\"end\":77280,\"start\":77269},{\"end\":79231,\"start\":79165},{\"end\":79802,\"start\":79733},{\"end\":81674,\"start\":81608},{\"end\":82186,\"start\":82130},{\"end\":46572,\"start\":46522},{\"end\":47020,\"start\":46940},{\"end\":47449,\"start\":47405},{\"end\":47768,\"start\":47713},{\"end\":48265,\"start\":48204},{\"end\":48614,\"start\":48524},{\"end\":48994,\"start\":48914},{\"end\":49495,\"start\":49456},{\"end\":49941,\"start\":49875},{\"end\":50375,\"start\":50298},{\"end\":50967,\"start\":50886},{\"end\":51587,\"start\":51542},{\"end\":52029,\"start\":51965},{\"end\":52349,\"start\":52296},{\"end\":52846,\"start\":52765},{\"end\":53336,\"start\":53296},{\"end\":53725,\"start\":53644},{\"end\":54146,\"start\":54118},{\"end\":54440,\"start\":54365},{\"end\":54848,\"start\":54768},{\"end\":55294,\"start\":55243},{\"end\":55661,\"start\":55589},{\"end\":56022,\"start\":55965},{\"end\":56412,\"start\":56327},{\"end\":57537,\"start\":57456},{\"end\":58050,\"start\":58020},{\"end\":58374,\"start\":58316},{\"end\":58792,\"start\":58748},{\"end\":59276,\"start\":59227},{\"end\":59624,\"start\":59585},{\"end\":59859,\"start\":59800},{\"end\":60175,\"start\":60116},{\"end\":60559,\"start\":60515},{\"end\":61065,\"start\":60984},{\"end\":61652,\"start\":61603},{\"end\":62000,\"start\":61907},{\"end\":62401,\"start\":62337},{\"end\":62731,\"start\":62665},{\"end\":63171,\"start\":63079},{\"end\":63627,\"start\":63548},{\"end\":64041,\"start\":64013},{\"end\":64467,\"start\":64386},{\"end\":64868,\"start\":64836},{\"end\":65273,\"start\":65222},{\"end\":65767,\"start\":65722},{\"end\":66169,\"start\":66121},{\"end\":66607,\"start\":66530},{\"end\":67097,\"start\":67048},{\"end\":67499,\"start\":67401},{\"end\":67934,\"start\":67856},{\"end\":68409,\"start\":68365},{\"end\":68784,\"start\":68720},{\"end\":69277,\"start\":69196},{\"end\":69719,\"start\":69672},{\"end\":70113,\"start\":70032},{\"end\":70499,\"start\":70438},{\"end\":70958,\"start\":70940},{\"end\":71477,\"start\":71426},{\"end\":72045,\"start\":71964},{\"end\":72545,\"start\":72496},{\"end\":72804,\"start\":72720},{\"end\":73414,\"start\":73365},{\"end\":73624,\"start\":73572},{\"end\":74065,\"start\":73974},{\"end\":74439,\"start\":74372},{\"end\":74895,\"start\":74824},{\"end\":75445,\"start\":75374},{\"end\":75907,\"start\":75873},{\"end\":76214,\"start\":76108},{\"end\":76736,\"start\":76659},{\"end\":77267,\"start\":77216},{\"end\":77709,\"start\":77614},{\"end\":78149,\"start\":78056},{\"end\":78667,\"start\":78574},{\"end\":79163,\"start\":79082},{\"end\":79731,\"start\":79647},{\"end\":80141,\"start\":80056},{\"end\":80794,\"start\":80696},{\"end\":81196,\"start\":81117},{\"end\":81606,\"start\":81525},{\"end\":82128,\"start\":82057},{\"end\":82475,\"start\":82412}]"}}}, "year": 2023, "month": 12, "day": 17}