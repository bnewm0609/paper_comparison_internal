{"id": 201698107, "updated": "2023-10-06 23:39:24.294", "metadata": {"title": "Bin-wise Temperature Scaling (BTS): Improvement in Confidence Calibration Performance through Simple Scaling Techniques", "authors": "[{\"first\":\"Byeongmoon\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"Hyemin\",\"last\":\"Jung\",\"middle\":[]},{\"first\":\"Jihyeun\",\"last\":\"Yoon\",\"middle\":[]},{\"first\":\"Kyungyul\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Younghak\",\"last\":\"Shin\",\"middle\":[]}]", "venue": "ICCV 2019 Workshop on Interpreting and Explaining Visual Artificial Intelligence Models", "journal": null, "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "The prediction reliability of neural networks is important in many applications. Specifically, in safety-critical domains, such as cancer prediction or autonomous driving, a reliable confidence of model's prediction is critical for the interpretation of the results. Modern deep neural networks have achieved a significant improvement in performance for many different image classification tasks. However, these networks tend to be poorly calibrated in terms of output confidence. Temperature scaling is an efficient post-processing-based calibration scheme and obtains well calibrated results. In this study, we leverage the concept of temperature scaling to build a sophisticated bin-wise scaling. Furthermore, we adopt augmentation of validation samples for elaborated scaling. The proposed methods consistently improve calibration performance with various datasets and deep convolutional neural network models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1908.11528", "mag": "3011367903", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccvw/JiJYKS19", "doi": "10.1109/iccvw.2019.00515"}}, "content": {"source": {"pdf_hash": "0a70e27b2c6f663da83fd7448b30c8c9966c5cac", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1908.11528v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1908.11528", "status": "GREEN"}}, "grobid": {"id": "aef1ead670e3964d1a7f0202e0e1ca55a118b0bb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0a70e27b2c6f663da83fd7448b30c8c9966c5cac.txt", "contents": "\nBin-wise Temperature Scaling (BTS): Improvement in Confidence Calibration Performance through Simple Scaling Techniques\n\n\nByeongmoon Ji \nLG CNS Seoul\nKorea\n\nHyemin Jung \nLG CNS Seoul\nKorea\n\nJihyeun Yoon jihyeun.yoon@lgcns.com \nLG CNS Seoul\nKorea\n\nKyungyul Kim kyungyul.kim@lgcns.com \nLG CNS Seoul\nKorea\n\nYounghak Shin younghak.shin@lgcns.com \nLG CNS Seoul\nKorea\n\nBin-wise Temperature Scaling (BTS): Improvement in Confidence Calibration Performance through Simple Scaling Techniques\n\nThe prediction reliability of neural networks is important in many applications. Specifically, in safety-critical domains, such as cancer prediction or autonomous driving, a reliable confidence of models prediction is critical for the interpretation of the results. Modern deep neural networks have achieved a significant improvement in performance for many different image classification tasks. However, these networks tend to be poorly calibrated in terms of output confidence. Temperature scaling is an efficient postprocessing-based calibration scheme and obtains well calibrated results. In this study, we leverage the concept of temperature scaling to build a sophisticated bin-wise scaling. Furthermore, we adopt augmentation of validation samples for elaborated scaling. The proposed methods consistently improve calibration performance with various datasets and deep convolutional neural network models.\n\nIntroduction\n\nThe recent progress made in deep convolutional neural networks (CNNs) has significantly improved their performance in various computer vision tasks, including image classification. For a typical ImageNet classification task [13], recently developed deep CNNs are already performing better than humans [4] [5]. However, as shown in [3], there is a problem in that as the layers of recent networks become deeper, those networks' confidence values (i.e., probabilities for predicted class) become increasingly over-confident. That is, although the accuracy of the latest deep models has been improved, their confidence is much higher than their accuracy.\n\nThis can pose significant challenges to interpretation of model's output. For example, in medical applications such as cancer prediction, there is a tremendous difference in the explanation of two diagnoses (each with an incorrect prediction) when one model outputs an overestimated 90% confidence and the other model outputs a 55% confidence. Therefore, calibrating a model's output confidence is a critical issue in terms of model reliability and interpretation.\n\nVarious confidence calibration methods have recently been proposed in the field of deep learning to overcome the over-confidence issue. Calibration studies can be divided into two main types. The first is a post-processing-based calibration method using an already trained model [3], and the second method performs calibration while training the model [12] [14] [9].\n\nThe most simple and effective calibration method is a post-processing technique called temperature scaling (TS) [3]. The authors showed that the predicted confidence of recent CNNs is not calibrated, and they proposed simple post-processing-based scaling using the validation dataset. This method calibrates predicted confidence well for various image datasets. The advantage of this method is its ability to calibrate confidence without affecting the test error rate.\n\nIn [12], the authors proposed several methods to control confidence during model training. They showed that directly penalizing confidence using an entropy measure during training is an effective technique in terms of generalization performance. However, calibration performance was not evaluated in their study. A variance-weighted confidence-integrated loss function was proposed for calibrating confidence during model training in [14]. Similarly, in [9], the authors proposed a method for conducting calibration during training. They included a direct term for minimizing the calibration error into the training loss. Both methods [14] [9] have the advantage that a separate validation dataset is not needed. However, compared to temperature scaling, calibration performance in [14] and [9] was not consistently improved.\n\nBecause simple post-processing-based calibration strategies are powerful tools for calibrating confidence without diminishing the test error rate, we focus on post-processing- based calibration techniques in this study. We propose the use of bin-wise temperature scaling (BTS), which aims to find multiple temperatures for different confidence ranges. In addition, we suggest augmentation-based bin-wise temperature scaling (ABTS) for finding stable bin temperatures. We evaluate the proposed methods using several recent deep models, including ResNet [4] and DenseNet [5], and obtain outstanding calibration performance on various image classification datasets, such as CIFAR-10/100 [8], Caltech Birds [17], and Stanford Cars [7].\n\n\nMethods\n\nConsider the multi-class (C > 2) classification problem with inputs X = {x 1 , x 2 , . . . , x i } and corresponding class labels Y = {y 1 , y 2 , . . . , y i }. For an instance x i (with corresponding label y i ), the model \u03c6 predicts the label y i = argmax \u03c6(y|x i ). The confidence scorep i is defined asp\ni = max c \u03c3 SM (z i ) c ,(1)\nwhere z i is the logits vector for input x i , and \u03c3 SM represents the softmax function. A well-calibrated confidencep should reflect a true probability. For example, if we collect ten samples, each having an identical confidence score of 0.8, we then expect an 80% classification accuracy for the ten samples.\n\nIn Figure 1, we visualize the baseline TS concept and the two proposed approaches, BTS and ABTS, using a reliability diagram [3] [1]. We describe in detail the procedure for each technique in the following subsections.\n\n\nTemperature Scaling\n\nWith TS [3], a scalar temperature t is determined using a validation set, and it is applied to all test samples for confidence calibration. Formally, the calibrated confidence scor\u00ea q is defined asq\ni = max c \u03c3 SM (z i /t) c .(2)\nHere, t is a positive scalar parameter (temperature) that aims to soften softmax in the case of overconfidence with t > 1.\n\nThe temperature t is optimized by minimizing the negative log likelihood (NLL) loss on the validation set [3]. With TS, a single t is computed and applied to all test samples. However, normally most samples have a high confidence as shown in Figure 1; therefore, temperature t could easily be biased toward high-confidence samples, and might not be optimal for low-confidence samples. Motivated by this, we propose the use of bin-wise temperature scaling methods.\n\n\nBin-wise Temperature Scaling\n\nIn BTS, we divide the samples into multiple bins. Then, using the validation samples of each bin, bin-wise temperatures are computed. In each test sample, the corresponding temperature is applied for scaling based on the test confidence as follows:q\ni = max c \u03c3 SM (z j i /t j ) c ,(3)\nwhere j is the bin number corresponding to the test confidence.\n\nConfidence-interval-based binning is a simple and intuitive binning method. In this case, the confidence range [0, 1] is partitioned into N equal-sized bins. This simple binning method has a problem that temperatures with a low confidence are computed using an extremely small number of validation samples, as shown in Figure 1. Therefore, this method tends to be unstable for the corresponding test samples.\n\nTo overcome this issue, we propose a binning method based on the number of samples. With this method, we collect the same number of validation samples in each bin except for high-confidence samples. If we had collected the same number of samples from the crowded high-confidence area, then extremely high-confidence samples (confidence greater than 0.999) with small differences (under 0.0001) would have been placed into many different bins, resulting in redundant temperatures across test samples with almost the same confidence. Therefore, we utilize a 0.999 threshold for collecting high-confidence samples. Thus, if validation samples have a confidence greater than 0.999, we collect them in a single bin, regardless of the number of samples.\n\n\nAugmentation based Bin-wise Temperature Scaling\n\nAs mentioned in Section 2.2, temperatures for lowconfidence parts may be unstable when applying a confidence-interval-based binning method. The proposed binning method (based on the number of samples) has a similar weak point. Because a small number of validation samples are positioned in low-confidence areas, the range of confidence of such areas increases if we collect the same number of samples in each bin.\n\nTo overcome this issue, we adopt an image augmentation technique. Image augmentation (e.g., image rotation, image shifting, and brightness adjustment) is widely used in the training phase of deep neural networks. It is known that such augmentation is efficient when the number of training samples is limited. In recent studies [16] [2], simple augmentation techniques have been adopted to test samples and improve model performance during the testing period.\n\nWe expect that augmenting the validation samples will help identify stable temperatures for low-confidence bins and further improve the performance of BTS. In the ABTS method, as shown in Figure 1 (right), we first apply an image augmentation technique to bins that have a small number of validation samples. The optimal number of bins to augment can differ by dataset and model. Instead of optimizing this number, we choose eight bins with confidence less than 0.8 for all datasets and models in this study.\n\nAfter augmentation, we perform the same procedure with BTS. We evaluate widely used image augmentation techniques (such as image shifting, brightness adjustment, contrast control, and adding blur) with ABTS in our experiment. We apply each augmentation technique to the validation samples of eight bins; therefore, the number of samples is doubled after each augmentation.\n\n\nExperiments\n\nIn this study, we compare the calibration performance of the proposed BTS and ABTS methods with conventional TS using a prevalent calibration measure, expected calibration error (ECE) [3] [10]. In addition, we evaluate four image datasets using five network architectures.\n\n\nDatasets\n\n1. CIFAR-10/100 [8]: Color images (32\u00d732 pixel resolution) from 10/100 classes were applied: 45,000, 5,000, and 10,000 images for the training, validation, and testing sets, respectively. The original dataset included 50,000 and 10,000 training and testing images, respectively. Therefore, we randomly divide the training set into training and validation sets.\n\n2. Caltech-UCSD Birds [17]: Color images of 200 bird species drawn from ImageNet were applied: 5,994, 2,897, and 2,897 images for the training, validation, and testing sets, respectively.\n\n3. Stanford Cars [7]: Color images of 196 classes of car by make, model, and year were applied: 8,041, 4,020, and 4,020 images for the training, validation, and testing sets, respectively.\n\n\nModels\n\nFor CIFAR-10/100, we use four well-known CNNs: ResNet-50/110 [4], Wide ResNet 28-10 [18], DenseNet 100 [5], and VGG 16 [15]. For the Caltech-UCSD Birds and Stanford Cars datasets, we use two CNNs: DenseNet and ResNet-101 [4]. To train the classification models, we normalize all images using the pixel mean and standard deviation, and we apply the stochastic gradient decent method with a momentum of 0.9 for training. We utilize other hyperparameters, such as the initial learning rate and learning rate schedule, as described in the above studies, for training each model. We train CIFAR-10 and CIFAR-100 from scratch and use the ImageNet pretrained weights for the Caltech-UCSD Birds and Stanford Cars datasets.\n\n\nEvaluation Metrics\n\nAs shown in Figures 1 and 2  same bin. The reliability diagram then shows the accuracy and average confidence of each bin. If the model is perfectly calibrated, the accuracy and average confidence should be the same.\n\nTo numerically evaluate calibration performance, ECE is the most widely used evaluation metric [3] [15]. Here, B = {B 1 , B 2 , . . . , B N } represents N different bins based on confidence range, and ACC(B j ) and CON F (B j ) are the accuracy and average confidence of each bin B j , respectively. ECE is the weighted average of the difference between accuracy and average confidence for each bin.\nECE = 1 n N j=1 B j \u00b7 ACC(B j ) \u2212 CON F (B j ) . (4)\n\nResults\n\nAs shown in Table 1, we evaluate the calibration performance of BTS and ABTS by comparing the test ECE (%) with an uncalibrated model (as a baseline) and TS across various image datasets and models. With BTS and ABTS, the results of two different binning methods (confidence interval and number of samples) are represented. We use a fixed number of bins (50) for both BTS and ABTS, as shown in Table 1. This number can be changed to obtain better calibration results for each dataset and model (see Table 3).\n\nAs shown in Table 1, the simple BTS method achieves better calibration results compared to TS for most of the models and datasets; that is, for the binning methods based on the confidence interval and number of samples, BTS shows improved calibration performance compared to TS in 10 of 12 and 11 of 12 cases, respectively. In addition, the best and second-best (in bold and italic, respectively) calibration performances are generally obtained using ABTS based on the number of samples and the confidence interval, respectively. Specifically, ABTS based on the number of samples (last column) outperforms TS for all experiment cases listed in Table 1. Note that TS, BTS, and ABTS are all post-processing-based scaling techniques; therefore, their test error rates are the same as that of the baseline model.\n\nIn Figure 2, we aim to analyze the effect of the proposed method by comparing the reliability diagrams of the uncalibrated baseline model (left), TS (center), and the proposed ABTS binning method based on the number of samples (right). For this comparison, we use the results of Wide ResNet on the CIFAR-100 dataset, as shown in Table 1. For ABTS (right), image shifting was applied for augmenting the validation samples.\n\nWith the uncalibrated baseline model (left), samples with a lower confidence (under 0.4) are under-confident, and samples with a higher confidence (over 0.4) are overconfident. In this case, for ideal calibration, a lower confidence should be increased, and a higher confidence should be decreased. However, when applying temperature scaling using a single temperature for all the samples, it is difficult to adopt two different scaling strategies. After temperature scaling (center), a large number of over-confident samples led to a decreased overall confidence and a slightly improved ECE, but low-confidence samples are more underconfident than the baseline. By contrast, because we apply bin-wise temperatures that adjust the scaling used for each bin, after applying ABTS (right), the overall confidence is well-calibrated; therefore, a much lower ECE is obtained   Figure 3 shows four example pictures of CIFAR-10 test set, each with its actual and predicted class labels. The predicted confidence (i.e., maximum confidence) values from the baseline VGG 16 model along with those obtained using TS and ABTS based on the number of samples are listed. These four pictures are incorrectly classified by the model and also appear ambiguous to human eyes. However, the confidence values of the baseline model are unnecessarily high for the incorrect labels. This may result in an unreliable interpretation of the its output. For example, the third image's actual class label is Automobile, but the baseline model incorrectly classified it as Truck with a very high confidence (0.98). For the same image, after confidence calibration, TS and ABTS exhibit a reduced confidence of 0.83 and 0.68, respectively. The proposed ABTS method not only shows the lowest ECE as listed in Table 1 (CIFAR-10 and VGG 16) but also exhibits the lowest confidence (i.e. most uncertainty) when incorrectly predicting the ambigu- ous images as shown in Figure 3.\n\nIn the results of Table 1, we use one type of augmentation, image shifting, for the proposed ABTS methods. Typically, a variety of image augmentation techniques are adopted to augment the training samples in image classification tasks. We aim to evaluate the calibration performance of the proposed ABTS methods for different augmentation techniques, as shown in Table 2. We adopt image shifting, brightness adjustment, contrast control, and blur for augmenting the validation samples, and we evaluate these on the CIFAR-100 dataset.\n\nFor our image-shifting augmentation, we shift the input image along the x-axis using a random value ranging from -4 to -8. For brightness control, a value ranging from -150 to 150 is randomly selected and added to all pixels. We use linear contrast with a strength parameter of 0.5 for con-trast control. For blur augmentation, we apply Gaussian blur with its sigma parameter randomly set between 0 and 1. To implement these augmentations, we use an open-source Python library [6].\n\nAs the results in Table 2 indicate, the proposed ABTS methods based on the confidence interval and number of samples show better calibration performance than TS for the four different augmentation techniques. There is no clear leader among the different augmentation techniques, and the difference in performance for each augmentation is marginal in each model. We note that strong data augmentation can change the distribution between the validation and test sets, thereby diminishing the effectiveness of ABTS. For the proposed BTS and ABTS methods shown in Tables 1 and 2, we fix the number of bins at fifty, although the optimal number of bins might differ for different datasets and models. In Table 3, we compare the ECE for different numbers of bins (e.g., 5, 10, 20, and 50) for BTS and ABTS (based on the number of samples) on CIFAR-100. The exact number of bins may be smaller owing to the use of a threshold for high-confidence samples in number-ofsamples-based binning (see Section 2.2). The results indicate that, for all models, the ECE of the proposed BTS and ABTS methods is not sensitive to differing numbers of bins. Thus, regardless of the number of bins used, the proposed methods outperform the baseline TS method.\n\n\nConclusion\n\nCalibrating the output confidence of a neural network is essential for reliably interpreting its results in many appli-cations. Post-processing-based TS was shown to be the simplest and most effective calibration scheme, and this scheme does not diminish test error performance. However, TS relies on a single temperature to soften softmax for all samples. In this study, we aimed to propose a more sophisticated post-processing based scaling method. We proposed the use of bin-wise temperature scaling (BTS), which adopts a binning method based on either the confidence interval or the number of samples. In addition, to further improve calibration performance, we suggested the use of an augmentation-based BTS method, which augmented the validation samples to find a stable temperature for each bin. We evaluated these ideas using various deep CNNs and image datasets. The experimental results indicate that the proposed, simple idea outperforms the baseline temperature scaling approach.\n\nFigure 1 .\n1Concepts of TS, BTS, and ABTS using reliability diagrams with the number of samples for each bin. (Left) TS finds a scalar temperature using a validation set and applies it to all test sets. (Center) In BTS, bin-wise temperatures are computed, and each temperature is applied to the corresponding bin-wise test samples. (Right) In ABTS, the validation set is augmented, and the same BTS procedure is conducted.\n\n\nECE (%) results of the baseline model, TS, and the proposed methods (BTS and ABTS) on different datasets and models. The best and second-best ECE values are represented in bold and italics, respectively. For augmentation with ABTS, we applied image shifting along the x-axis within a range of [-4, -8] on CIFAR-10 and 100 and [-28, -56] on Caltech-UCSD Birds and Stanford Cars dataset based on the size of the original images.\n\nFigure 2 .\n2Reliability diagrams and ECE (%) of (left) baseline uncalibrated model, (center) TS, and (right) proposed ABTS (based on the number of samples binned) for Wide ResNet on CIFAR-100. compared to TS.\n\nFigure 3 .\n3Example pictures from CIFAR-10 test set with actual class labels and those predicted by the VGG 16 model. Confidence values are listed for baseline, TS, and ABTS using the number of samples. For ABTS, image shifting was applied for augmenting the validation samples.\n\n\n, we utilize a reliability diagram[3] [1][11] for visualizing calibration performance. A reliability diagram is the most intuitive and readable way to visualize calibration performance. In a reliability diagram, test samples are binned based on the range of confidence; i.e., samples with a similar confidence are collected into theDataset \n\nModel \nTest error \n(%) \n\nECE (%) \n\nBaseline \n(uncalibrated) \nTS \n\nBTS \nABTS \nConfidence \nInterval \n\nNumber of \nSamples \n\nConfidence \nInterval \n\nNumber of \nSamples \n\nCIFAR-10 \nVGG 16 \n7.10 \n4.99 \n1.45 \n1.26 \n0.90 \n0.73 \n0.37 \nCIFAR-10 \nResNet 110 \n6.23 \n4.37 \n0.98 \n1.12 \n0.79 \n0.58 \n0.36 \nCIFAR-10 \nDenseNet 100(k=12) \n5.37 \n3.09 \n1.05 \n0.93 \n0.87 \n0.53 \n0.69 \nCIFAR-10 \nWide ResNet 28-10 \n4.80 \n3.03 \n0.73 \n0.78 \n0.55 \n0.64 \n0.44 \nCIFAR-100 \nVGG 16 \n26.56 \n14.13 \n5.20 \n4.92 \n4.98 \n3.41 \n2.53 \nCIFAR-100 \nResNet 110 \n28.51 \n15.79 \n2.54 \n2.32 \n2.28 \n0.99 \n1.29 \nCIFAR-100 DenseNet 100 (k=12) \n25.11 \n10.29 \n3.55 \n2.53 \n2.37 \n1.55 \n1.64 \nCIFAR-100 \nWide ResNet 28-10 \n21.05 \n7.16 \n5.01 \n3.08 \n2.97 \n1.96 \n1.74 \nBirds \nResNet 101 \n21.44 \n2.13 \n2.27 \n1.33 \n1.40 \n1.28 \n1.21 \nBirds \nDenseNet 100 (k=12) \n19.92 \n3.22 \n1.44 \n1.42 \n1.57 \n1.70 \n1.40 \nCars \nResNet 101 \n12.16 \n2.06 \n2.05 \n1.24 \n1.36 \n0.96 \n1.16 \nCars \nDenseNet 100 (k=12) \n9.43 \n2.31 \n2.20 \n1.24 \n1.16 \n0.89 \n1.09 \nTable 1. \n\n\nTable 3. ECE (%) comparison of different numbers of bins in BTS and ABTS on CIFAR-100. Image-shifting augmentation is used for ABTS.Model \nBaseline \nTS \n# of \nbins \n\nBTS \n(CI) \n\nABTS \n(# of \nsamples) \n\nVGG 16 \n14.13 \n5.20 \n\n5 \n4.92 \n2.76 \n10 \n4.90 \n2.52 \n20 \n4.90 \n2.51 \n50 \n4.98 \n2.53 \n\nResNet 110 \n15.79 \n2.54 \n\n5 \n2.37 \n1.13 \n10 \n2.26 \n1.10 \n20 \n2.24 \n1.15 \n50 \n2.28 \n1.20 \n\nDenseNet \n100 (k=12) \n10.29 \n3.55 \n\n5 \n2.28 \n1.36 \n10 \n2.32 \n1.25 \n20 \n2.15 \n1.50 \n50 \n2.37 \n1.64 \n\nWide ResNet \n28-10 \n7.16 \n5.01 \n\n5 \n3.42 \n2.06 \n10 \n2.69 \n1.74 \n20 \n2.96 \n1.74 \n50 \n2.97 \n1.74 \n\n\n\nThe comparison and evaluation of forecasters. M H Degroot, S E Fienberg, The Statistician: Journal of the Institute of Statisticians. 32M. H. DeGroot and S. E. Fienberg. The comparison and evaluation of forecasters. The Statistician: Journal of the Institute of Statisticians, 32:12-22, 1983.\n\nI Golan, R El-Yaniv, Deep anomaly detection using geometric transformations. CoRR, abs/1805.10917. I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. CoRR, abs/1805.10917, 2018.\n\nOn calibration of modern neural networks. C Guo, G Pleiss, Y Sun, K Q Weinberger, abs/1706.04599CoRRC. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On cali- bration of modern neural networks. CoRR, abs/1706.04599, 2017.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, abs/1512.03385CoRRK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.\n\n. G Huang, Z Liu, K Q Weinberger, abs/1608.06993G. Huang, Z. Liu, and K. Q. Weinberger. Densely connected convolutional networks. CoRR, abs/1608.06993, 2016.\n\n. A B Jung, 30A. B. Jung. imgaug. https://github.com/aleju/ imgaug, 2018. [Online; accessed 30-Oct-2018].\n\n3d object representations for fine-grained categorization. J Krause, M Stark, J Deng, L Fei-Fei, 4th International IEEE Workshop on 3D Representation and Recognition. Sydney, Australia3dRR-13J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep- resentations for fine-grained categorization. In 4th Interna- tional IEEE Workshop on 3D Representation and Recogni- tion (3dRR-13), Sydney, Australia, 2013.\n\nCifar-100 (canadian institute for advanced research). A Krizhevsky, V Nair, G Hinton, A. Krizhevsky, V. Nair, and G. Hinton. Cifar-100 (canadian institute for advanced research).\n\nTrainable calibration measures for neural networks from kernel mean embeddings. A Kumar, S Sarawagi, U Jain, PMLRProceedings of the 35th International Conference on Machine Learning. J. Dy and A. Krausethe 35th International Conference on Machine LearningStockholmsmssan, Stockholm Sweden80A. Kumar, S. Sarawagi, and U. Jain. Trainable calibra- tion measures for neural networks from kernel mean em- beddings. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2805-2814, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR.\n\nObtaining well calibrated probabilities using bayesian binning. M P Naeini, G F Cooper, M Hauskrecht, Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI'15. the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI'15AAAI PressM. P. Naeini, G. F. Cooper, and M. Hauskrecht. Obtain- ing well calibrated probabilities using bayesian binning. In Proceedings of the Twenty-Ninth AAAI Conference on Artifi- cial Intelligence, AAAI'15, pages 2901-2907. AAAI Press, 2015.\n\nPredicting good probabilities with supervised learning. A Niculescu-Mizil, R Caruana, Proceedings of the 22Nd International Conference on Machine Learning, ICML '05. the 22Nd International Conference on Machine Learning, ICML '05New York, NY, USAACMA. Niculescu-Mizil and R. Caruana. Predicting good proba- bilities with supervised learning. In Proceedings of the 22Nd International Conference on Machine Learning, ICML '05, pages 625-632, New York, NY, USA, 2005. ACM.\n\nRegularizing neural networks by penalizing confident output distributions. G Pereyra, G Tucker, J Chorowski, L Kaiser, G E Hinton, abs/1701.06548CoRRG. Pereyra, G. Tucker, J. Chorowski, L. Kaiser, and G. E. Hinton. Regularizing neural networks by penalizing confi- dent output distributions. CoRR, abs/1701.06548, 2017.\n\nImageNet Large Scale Visual Recognition Challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, International Journal of Computer Vision (IJCV). 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.\n\nConfidence calibration in deep neural networks through stochastic inferences. S Seo, P H Seo, B Han, abs/1809.10877CoRRS. Seo, P. H. Seo, and B. Han. Confidence calibration in deep neural networks through stochastic inferences. CoRR, abs/1809.10877, 2018.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, International Conference on Learning Representations. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015.\n\nSeven ways to improve example-based single image super resolution. R Timofte, R Rothe, L V Gool, abs/1511.02228CoRRR. Timofte, R. Rothe, and L. V. Gool. Seven ways to im- prove example-based single image super resolution. CoRR, abs/1511.02228, 2015.\n\nThe Caltech-UCSD Birds-200-2011 Dataset. C Wah, S Branson, P Welinder, P Perona, S Belongie, - port CNS-TR-2011-001California Institute of TechnologyTechnical ReC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Re- port CNS-TR-2011-001, California Institute of Technology, 2011.\n\nS Zagoruyko, N Komodakis, Wide residual networks. CoRR, abs/1605.07146. S. Zagoruyko and N. Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.\n", "annotations": {"author": "[{\"end\":157,\"start\":123},{\"end\":190,\"start\":158},{\"end\":247,\"start\":191},{\"end\":304,\"start\":248},{\"end\":363,\"start\":305}]", "publisher": null, "author_last_name": "[{\"end\":136,\"start\":134},{\"end\":169,\"start\":165},{\"end\":203,\"start\":199},{\"end\":260,\"start\":257},{\"end\":318,\"start\":314}]", "author_first_name": "[{\"end\":133,\"start\":123},{\"end\":164,\"start\":158},{\"end\":198,\"start\":191},{\"end\":256,\"start\":248},{\"end\":313,\"start\":305}]", "author_affiliation": "[{\"end\":156,\"start\":138},{\"end\":189,\"start\":171},{\"end\":246,\"start\":228},{\"end\":303,\"start\":285},{\"end\":362,\"start\":344}]", "title": "[{\"end\":120,\"start\":1},{\"end\":483,\"start\":364}]", "venue": null, "abstract": "[{\"end\":1397,\"start\":485}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1641,\"start\":1637},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1717,\"start\":1714},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1721,\"start\":1718},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1747,\"start\":1744},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2814,\"start\":2811},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2888,\"start\":2884},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2897,\"start\":2894},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3015,\"start\":3012},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3377,\"start\":3373},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3808,\"start\":3804},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3827,\"start\":3824},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4009,\"start\":4005},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4156,\"start\":4152},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4164,\"start\":4161},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4752,\"start\":4749},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4769,\"start\":4766},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4884,\"start\":4881},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4904,\"start\":4900},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4927,\"start\":4924},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5718,\"start\":5715},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5722,\"start\":5719},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5843,\"start\":5840},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6295,\"start\":6292},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8988,\"start\":8984},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10202,\"start\":10199},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10207,\"start\":10203},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10319,\"start\":10316},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10688,\"start\":10684},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10871,\"start\":10868},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11114,\"start\":11111},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11138,\"start\":11134},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11156,\"start\":11153},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11173,\"start\":11169},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11274,\"start\":11271},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12103,\"start\":12100},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12108,\"start\":12104},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17171,\"start\":17168},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20798,\"start\":20795},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20806,\"start\":20802}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19839,\"start\":19416},{\"attributes\":{\"id\":\"fig_1\"},\"end\":20268,\"start\":19840},{\"attributes\":{\"id\":\"fig_2\"},\"end\":20478,\"start\":20269},{\"attributes\":{\"id\":\"fig_3\"},\"end\":20758,\"start\":20479},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":22085,\"start\":20759},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":22663,\"start\":22086}]", "paragraph": "[{\"end\":2064,\"start\":1413},{\"end\":2530,\"start\":2066},{\"end\":2898,\"start\":2532},{\"end\":3368,\"start\":2900},{\"end\":4195,\"start\":3370},{\"end\":4928,\"start\":4197},{\"end\":5248,\"start\":4940},{\"end\":5588,\"start\":5278},{\"end\":5808,\"start\":5590},{\"end\":6030,\"start\":5832},{\"end\":6184,\"start\":6062},{\"end\":6649,\"start\":6186},{\"end\":6931,\"start\":6682},{\"end\":7031,\"start\":6968},{\"end\":7441,\"start\":7033},{\"end\":8190,\"start\":7443},{\"end\":8655,\"start\":8242},{\"end\":9115,\"start\":8657},{\"end\":9625,\"start\":9117},{\"end\":9999,\"start\":9627},{\"end\":10287,\"start\":10015},{\"end\":10660,\"start\":10300},{\"end\":10849,\"start\":10662},{\"end\":11039,\"start\":10851},{\"end\":11764,\"start\":11050},{\"end\":12003,\"start\":11787},{\"end\":12404,\"start\":12005},{\"end\":12976,\"start\":12468},{\"end\":13786,\"start\":12978},{\"end\":14209,\"start\":13788},{\"end\":16154,\"start\":14211},{\"end\":16689,\"start\":16156},{\"end\":17172,\"start\":16691},{\"end\":18409,\"start\":17174},{\"end\":19415,\"start\":18424}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5277,\"start\":5249},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6061,\"start\":6031},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6967,\"start\":6932},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12457,\"start\":12405}]", "table_ref": "[{\"end\":12487,\"start\":12480},{\"end\":12869,\"start\":12862},{\"end\":12974,\"start\":12967},{\"end\":12997,\"start\":12990},{\"end\":13629,\"start\":13622},{\"end\":14124,\"start\":14117},{\"end\":15995,\"start\":15988},{\"end\":16181,\"start\":16174},{\"end\":16526,\"start\":16519},{\"end\":17199,\"start\":17192},{\"end\":17880,\"start\":17873}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1411,\"start\":1399},{\"attributes\":{\"n\":\"2.\"},\"end\":4938,\"start\":4931},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5830,\"start\":5811},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6680,\"start\":6652},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8240,\"start\":8193},{\"attributes\":{\"n\":\"3.\"},\"end\":10013,\"start\":10002},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10298,\"start\":10290},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11048,\"start\":11042},{\"attributes\":{\"n\":\"3.3.\"},\"end\":11785,\"start\":11767},{\"attributes\":{\"n\":\"4.\"},\"end\":12466,\"start\":12459},{\"attributes\":{\"n\":\"5.\"},\"end\":18422,\"start\":18412},{\"end\":19427,\"start\":19417},{\"end\":20280,\"start\":20270},{\"end\":20490,\"start\":20480}]", "table": "[{\"end\":22085,\"start\":21093},{\"end\":22663,\"start\":22220}]", "figure_caption": "[{\"end\":19839,\"start\":19429},{\"end\":20268,\"start\":19842},{\"end\":20478,\"start\":20282},{\"end\":20758,\"start\":20492},{\"end\":21093,\"start\":20761},{\"end\":22220,\"start\":22088}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5601,\"start\":5593},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6436,\"start\":6428},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7360,\"start\":7352},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9313,\"start\":9305},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11814,\"start\":11799},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13799,\"start\":13791},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15091,\"start\":15083},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16153,\"start\":16145}]", "bib_author_first_name": "[{\"end\":22712,\"start\":22711},{\"end\":22714,\"start\":22713},{\"end\":22725,\"start\":22724},{\"end\":22727,\"start\":22726},{\"end\":22960,\"start\":22959},{\"end\":22969,\"start\":22968},{\"end\":23212,\"start\":23211},{\"end\":23219,\"start\":23218},{\"end\":23229,\"start\":23228},{\"end\":23236,\"start\":23235},{\"end\":23238,\"start\":23237},{\"end\":23438,\"start\":23437},{\"end\":23444,\"start\":23443},{\"end\":23453,\"start\":23452},{\"end\":23460,\"start\":23459},{\"end\":23599,\"start\":23598},{\"end\":23608,\"start\":23607},{\"end\":23615,\"start\":23614},{\"end\":23617,\"start\":23616},{\"end\":23758,\"start\":23757},{\"end\":23760,\"start\":23759},{\"end\":23922,\"start\":23921},{\"end\":23932,\"start\":23931},{\"end\":23941,\"start\":23940},{\"end\":23949,\"start\":23948},{\"end\":24328,\"start\":24327},{\"end\":24342,\"start\":24341},{\"end\":24350,\"start\":24349},{\"end\":24534,\"start\":24533},{\"end\":24543,\"start\":24542},{\"end\":24555,\"start\":24554},{\"end\":25161,\"start\":25160},{\"end\":25163,\"start\":25162},{\"end\":25173,\"start\":25172},{\"end\":25175,\"start\":25174},{\"end\":25185,\"start\":25184},{\"end\":25657,\"start\":25656},{\"end\":25676,\"start\":25675},{\"end\":26147,\"start\":26146},{\"end\":26158,\"start\":26157},{\"end\":26168,\"start\":26167},{\"end\":26181,\"start\":26180},{\"end\":26191,\"start\":26190},{\"end\":26193,\"start\":26192},{\"end\":26444,\"start\":26443},{\"end\":26459,\"start\":26458},{\"end\":26467,\"start\":26466},{\"end\":26473,\"start\":26472},{\"end\":26483,\"start\":26482},{\"end\":26495,\"start\":26494},{\"end\":26501,\"start\":26500},{\"end\":26510,\"start\":26509},{\"end\":26522,\"start\":26521},{\"end\":26532,\"start\":26531},{\"end\":26545,\"start\":26544},{\"end\":26547,\"start\":26546},{\"end\":26555,\"start\":26554},{\"end\":26959,\"start\":26958},{\"end\":26966,\"start\":26965},{\"end\":26968,\"start\":26967},{\"end\":26975,\"start\":26974},{\"end\":27206,\"start\":27205},{\"end\":27218,\"start\":27217},{\"end\":27514,\"start\":27513},{\"end\":27525,\"start\":27524},{\"end\":27534,\"start\":27533},{\"end\":27536,\"start\":27535},{\"end\":27739,\"start\":27738},{\"end\":27746,\"start\":27745},{\"end\":27757,\"start\":27756},{\"end\":27769,\"start\":27768},{\"end\":27779,\"start\":27778},{\"end\":28040,\"start\":28039},{\"end\":28053,\"start\":28052}]", "bib_author_last_name": "[{\"end\":22722,\"start\":22715},{\"end\":22736,\"start\":22728},{\"end\":22966,\"start\":22961},{\"end\":22978,\"start\":22970},{\"end\":23216,\"start\":23213},{\"end\":23226,\"start\":23220},{\"end\":23233,\"start\":23230},{\"end\":23249,\"start\":23239},{\"end\":23441,\"start\":23439},{\"end\":23450,\"start\":23445},{\"end\":23457,\"start\":23454},{\"end\":23464,\"start\":23461},{\"end\":23605,\"start\":23600},{\"end\":23612,\"start\":23609},{\"end\":23628,\"start\":23618},{\"end\":23765,\"start\":23761},{\"end\":23929,\"start\":23923},{\"end\":23938,\"start\":23933},{\"end\":23946,\"start\":23942},{\"end\":23957,\"start\":23950},{\"end\":24339,\"start\":24329},{\"end\":24347,\"start\":24343},{\"end\":24357,\"start\":24351},{\"end\":24540,\"start\":24535},{\"end\":24552,\"start\":24544},{\"end\":24560,\"start\":24556},{\"end\":25170,\"start\":25164},{\"end\":25182,\"start\":25176},{\"end\":25196,\"start\":25186},{\"end\":25673,\"start\":25658},{\"end\":25684,\"start\":25677},{\"end\":26155,\"start\":26148},{\"end\":26165,\"start\":26159},{\"end\":26178,\"start\":26169},{\"end\":26188,\"start\":26182},{\"end\":26200,\"start\":26194},{\"end\":26456,\"start\":26445},{\"end\":26464,\"start\":26460},{\"end\":26470,\"start\":26468},{\"end\":26480,\"start\":26474},{\"end\":26492,\"start\":26484},{\"end\":26498,\"start\":26496},{\"end\":26507,\"start\":26502},{\"end\":26519,\"start\":26511},{\"end\":26529,\"start\":26523},{\"end\":26542,\"start\":26533},{\"end\":26552,\"start\":26548},{\"end\":26563,\"start\":26556},{\"end\":26963,\"start\":26960},{\"end\":26972,\"start\":26969},{\"end\":26979,\"start\":26976},{\"end\":27215,\"start\":27207},{\"end\":27228,\"start\":27219},{\"end\":27522,\"start\":27515},{\"end\":27531,\"start\":27526},{\"end\":27541,\"start\":27537},{\"end\":27743,\"start\":27740},{\"end\":27754,\"start\":27747},{\"end\":27766,\"start\":27758},{\"end\":27776,\"start\":27770},{\"end\":27788,\"start\":27780},{\"end\":28050,\"start\":28041},{\"end\":28063,\"start\":28054}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":109884250},\"end\":22957,\"start\":22665},{\"attributes\":{\"id\":\"b1\"},\"end\":23167,\"start\":22959},{\"attributes\":{\"doi\":\"abs/1706.04599\",\"id\":\"b2\"},\"end\":23389,\"start\":23169},{\"attributes\":{\"doi\":\"abs/1512.03385\",\"id\":\"b3\"},\"end\":23594,\"start\":23391},{\"attributes\":{\"doi\":\"abs/1608.06993\",\"id\":\"b4\"},\"end\":23753,\"start\":23596},{\"attributes\":{\"id\":\"b5\"},\"end\":23860,\"start\":23755},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14342571},\"end\":24271,\"start\":23862},{\"attributes\":{\"id\":\"b7\"},\"end\":24451,\"start\":24273},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b8\",\"matched_paper_id\":49314079},\"end\":25094,\"start\":24453},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6292807},\"end\":25598,\"start\":25096},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207158152},\"end\":26069,\"start\":25600},{\"attributes\":{\"doi\":\"abs/1701.06548\",\"id\":\"b11\"},\"end\":26390,\"start\":26071},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2930547},\"end\":26878,\"start\":26392},{\"attributes\":{\"doi\":\"abs/1809.10877\",\"id\":\"b13\"},\"end\":27135,\"start\":26880},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14124313},\"end\":27444,\"start\":27137},{\"attributes\":{\"doi\":\"abs/1511.02228\",\"id\":\"b15\"},\"end\":27695,\"start\":27446},{\"attributes\":{\"doi\":\"- port CNS-TR-2011-001\",\"id\":\"b16\"},\"end\":28037,\"start\":27697},{\"attributes\":{\"id\":\"b17\"},\"end\":28193,\"start\":28039}]", "bib_title": "[{\"end\":22709,\"start\":22665},{\"end\":23919,\"start\":23862},{\"end\":24531,\"start\":24453},{\"end\":25158,\"start\":25096},{\"end\":25654,\"start\":25600},{\"end\":26441,\"start\":26392},{\"end\":27203,\"start\":27137}]", "bib_author": "[{\"end\":22724,\"start\":22711},{\"end\":22738,\"start\":22724},{\"end\":22968,\"start\":22959},{\"end\":22980,\"start\":22968},{\"end\":23218,\"start\":23211},{\"end\":23228,\"start\":23218},{\"end\":23235,\"start\":23228},{\"end\":23251,\"start\":23235},{\"end\":23443,\"start\":23437},{\"end\":23452,\"start\":23443},{\"end\":23459,\"start\":23452},{\"end\":23466,\"start\":23459},{\"end\":23607,\"start\":23598},{\"end\":23614,\"start\":23607},{\"end\":23630,\"start\":23614},{\"end\":23767,\"start\":23757},{\"end\":23931,\"start\":23921},{\"end\":23940,\"start\":23931},{\"end\":23948,\"start\":23940},{\"end\":23959,\"start\":23948},{\"end\":24341,\"start\":24327},{\"end\":24349,\"start\":24341},{\"end\":24359,\"start\":24349},{\"end\":24542,\"start\":24533},{\"end\":24554,\"start\":24542},{\"end\":24562,\"start\":24554},{\"end\":25172,\"start\":25160},{\"end\":25184,\"start\":25172},{\"end\":25198,\"start\":25184},{\"end\":25675,\"start\":25656},{\"end\":25686,\"start\":25675},{\"end\":26157,\"start\":26146},{\"end\":26167,\"start\":26157},{\"end\":26180,\"start\":26167},{\"end\":26190,\"start\":26180},{\"end\":26202,\"start\":26190},{\"end\":26458,\"start\":26443},{\"end\":26466,\"start\":26458},{\"end\":26472,\"start\":26466},{\"end\":26482,\"start\":26472},{\"end\":26494,\"start\":26482},{\"end\":26500,\"start\":26494},{\"end\":26509,\"start\":26500},{\"end\":26521,\"start\":26509},{\"end\":26531,\"start\":26521},{\"end\":26544,\"start\":26531},{\"end\":26554,\"start\":26544},{\"end\":26565,\"start\":26554},{\"end\":26965,\"start\":26958},{\"end\":26974,\"start\":26965},{\"end\":26981,\"start\":26974},{\"end\":27217,\"start\":27205},{\"end\":27230,\"start\":27217},{\"end\":27524,\"start\":27513},{\"end\":27533,\"start\":27524},{\"end\":27543,\"start\":27533},{\"end\":27745,\"start\":27738},{\"end\":27756,\"start\":27745},{\"end\":27768,\"start\":27756},{\"end\":27778,\"start\":27768},{\"end\":27790,\"start\":27778},{\"end\":28052,\"start\":28039},{\"end\":28065,\"start\":28052}]", "bib_venue": "[{\"end\":22797,\"start\":22738},{\"end\":23056,\"start\":22980},{\"end\":23209,\"start\":23169},{\"end\":23435,\"start\":23391},{\"end\":24027,\"start\":23959},{\"end\":24325,\"start\":24273},{\"end\":24634,\"start\":24566},{\"end\":25281,\"start\":25198},{\"end\":25764,\"start\":25686},{\"end\":26144,\"start\":26071},{\"end\":26612,\"start\":26565},{\"end\":26956,\"start\":26880},{\"end\":27282,\"start\":27230},{\"end\":27511,\"start\":27446},{\"end\":27736,\"start\":27697},{\"end\":28109,\"start\":28065},{\"end\":24046,\"start\":24029},{\"end\":24741,\"start\":24655},{\"end\":25351,\"start\":25283},{\"end\":25846,\"start\":25766}]"}}}, "year": 2023, "month": 12, "day": 17}