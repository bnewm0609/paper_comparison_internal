{"id": 228083838, "updated": "2023-10-06 07:48:03.116", "metadata": {"title": "Robust Consistent Video Depth Estimation", "authors": "[{\"first\":\"Johannes\",\"last\":\"Kopf\",\"middle\":[]},{\"first\":\"Xuejian\",\"last\":\"Rong\",\"middle\":[]},{\"first\":\"Jia-Bin\",\"last\":\"Huang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 12, "day": 10}, "abstract": "We present an algorithm for estimating consistent dense depth maps and camera poses from a monocular video. We integrate a learning-based depth prior, in the form of a convolutional neural network trained for single-image depth estimation, with geometric optimization, to estimate a smooth camera trajectory as well as detailed and stable depth reconstruction. Our algorithm combines two complementary techniques: (1) flexible deformation-splines for low-frequency large-scale alignment and (2) geometry-aware depth filtering for high-frequency alignment of fine depth details. In contrast to prior approaches, our method does not require camera poses as input and achieves robust reconstruction for challenging hand-held cell phone captures containing a significant amount of noise, shake, motion blur, and rolling shutter deformations. Our method quantitatively outperforms state-of-the-arts on the Sintel benchmark for both depth and pose estimations and attains favorable qualitative results across diverse wild datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2012.05901", "mag": "3112163773", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/KopfRH21", "doi": "10.1109/cvpr46437.2021.00166"}}, "content": {"source": {"pdf_hash": "d0b2a0f2ae00887f91a7aa508871c0d522ad312a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2012.05901v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2012.05901", "status": "GREEN"}}, "grobid": {"id": "099847b78bed5a05e2b79d8bc9ed81bab641888a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d0b2a0f2ae00887f91a7aa508871c0d522ad312a.txt", "contents": "\nRobust Consistent Video Depth Estimation\n\n\nJohannes Kopf \nFacebook Xuejian \nRong Facebook \nJia-Bin Huang \nVirginia Tech \nRobust Consistent Video Depth Estimation\n\nFigure 1: Robust consistent video depth estimation of dynamic scenes. Our method estimates a smooth camera trajectory and detailed and stable dense depth map on challenging hand-held cellphone videos. Our method supports both still (static) and dynamic camera motion.AbstractWe present an algorithm for estimating consistent dense depth maps and camera poses from a monocular video. We integrate a learning-based depth prior, in the form of a convolutional neural network trained for single-image depth estimation, with geometric optimization, to estimate a smooth camera trajectory as well as detailed and stable depth reconstruction. Our algorithm combines two complementary techniques: (1) flexible deformation-splines for low-frequency large-scale alignment and (2) geometryaware depth filtering for high-frequency alignment of fine depth details. In contrast to prior approaches, our method does not require camera poses as input and achieves robust reconstruction for challenging hand-held cell phone captures containing a significant amount of noise, shake, motion blur, and rolling shutter deformations. Our method quantitatively outperforms state-of-the-arts on the Sintel benchmark for both depth and pose estimations and attains favorable qualitative results across diverse wild datasets.\n\nIntroduction\n\nDense per-frame depth is an important intermediate representation that is useful for many video-based applications, such as 3D video stabilization [37], augmented reality (AR) and special video effects [59,39], and for converting videos for virtual reality (VR) viewing [22]. However, estimating accurate and consistent depth maps for casually captured videos still remains very challenging. Cell phones contain small image sensors that may produce noisy images, especially in low lighting situations. They use a rolling shutter that may result in wobbly image deformations. Handheld captured casual videos often contain camera shake and motion blur, and dynamic objects, such as people, animals, and vehicles. In addition to all these degradations, there exist well-known problems for 3D reconstruction that are not specific to video, including poorly textured image regions, repetitive patterns, and occlusions.\n\nTraditional algorithms for dense reconstruction that combine Structure from Motion (SFM) and Multi-view Stereo (MVS) have difficulties dealing with these challenges. The SFM step suffers from the limitations of accuracy and availability of correspondence and often fails entirely, as explained below, preventing further processing. Even when SFM succeeds, the MVS reconstructions typically contain a significant amount of holes and noises.\n\nLearning-based algorithms [35,34,48] are better equipped to handle with this situation. Instead of matching points across frames and geometric triangulation they employ priors learned from diverse training datasets. This enables them to handle many of the challenging situations aforementioned. However, the estimated depth is only defined up to scale, and, while plausible, is not necessarily accurate, i.e., it lacks geometric consistency.\n\nHybrid algorithms [39,36,70,56] achieve desirable characteristics of both approaches by combining learnt priors with geometric reasoning. These methods often assume precise per-frame camera poses as auxiliary inputs, which are typically estimated with SFM. However, SFM algorithms are not robust to the issues described above. In such situations, SFM might fail to register all frames or produce outlier poses with large errors. As a consequence, hybrid algorithms work well when the pose estimation succeeds and fail catastrophically when it does not. This problem of robustness makes these algorithms unsuitable for many real-world applications, as they might fail in unpredictable ways. Recently, a hybrid approach proposed in the DeepV2D work [56] attempts to interleave pose and depth estimations in inference for an ideal convergence, which performs reasonably well on static scenes but still does not prove the capability of tackling dynamic scenes.\n\nWe present a new algorithm that is more robust and does not require poses as input. Similar to Luo et al. [39], we leverage a convolutional neural network trained for singleimage depth estimation as a depth prior and optimize the alignment of the depth maps. However, their test-time finetuning formulation requires a pre-established geometric relationship between matched pixels across frames, which, in turn, requires precisely calibrated camera poses and perframe depth scale factors. In contrast, we jointly optimize extrinsic and intrinsic camera pose parameters as well as 3D alignment of the estimated depth maps using continuous optimization. Na\u00efve alignment using rigid-scale transformations does not result in accurate poses because the independently estimated per-frame depth maps usually contain random inaccuracies. These further lead to misalignment, which inevitably imposes noisy errors onto the estimated camera trajectory. We resolve it by turning to a more flexible deformation model, using spatially-varying splines. They provide a more exact alignment, which, in succession, results in smoother and more accurate trajectories.\n\nThe spline-based deformation achieves accurate lowfrequency alignment. To further improve high-frequency details and remove residual jitter, we use a geometry-aware depth filter. This filter is capable of bringing out fine depth details, rather than blurring them because of the precise alignment from the previous stage.\n\nAs shown in previous work, the learning-based prior is resilient to moderate amounts of dynamic motion. We make our method even more robust to large dynamic motion by incorporating automatic segmentation-based masks to relax the geometric alignment requirements in regions containing people, vehicles, and animals.\n\nWe evaluate our method qualitatively (visually) by processing all 90 sequences from the DAVIS dataset (originally designed for dynamic video object segmentation) [46] and comparing against previous methods (of which many fail). We further, evaluate quantitatively on the 23 sequences from the Sintel dataset [4], for which ground truth pose and depth are available.\n\n\nRelated Work\n\nMulti-view stereo.\n\nMulti-view stereo (MVS) algorithms estimate depth from a collection of images captured from different viewpoints [53,17]. Geometry-based MVS systems (e.g., COLMAP [52]) follow the incremental Structure-from-Motion (SFM) pipeline (correspondence estimation, pose estimation, triangulation, and bundle adjustment). Several learning-based methods further improve the reconstruction quality by fusing classic MVS techniques (e.g., cost aggregation and plane-sweep volume) and datadriven priors [58,24,68,25,28]. In contrast to MVS algorithms that assume a static scene, our work aims to reconstruct fully dense depth from a dynamic scene video.\n\n\nSingle-image depth estimation.\n\nIn recent years we have witnessed rapid progresses on supervised learningbased single-image depth estimation [13,12,30,38,16]. As diverse training images with the corresponding ground truth depth maps are difficult to obtain, existing work explores training models using synthetic datasets [40], crowdsourced human annotations of relative depth [7] or 3D surfaces [9], pseudo ground truth depth maps from internet images/videos [35,34,8], or 3D movies [48,61]. Another research line focuses on self-supervised approaches for learning single-image depth estimation models. Specific examples include learning from stereo pairs [18,21,20,64] or monocular videos [73,60,11,75,69,47,50]. Most selfsupervised learning methods minimize photometric reprojection errors (computed from the estimated depth and pose) and do not account for dynamic objects in videos. Several methods alleviate this problem by masking out dynamic objects [75], modeling the motion of individual objects [5] or estimating dense 3D translation field [32]. We use the state-of-the-art single-image depth estimation method [48] to obtain an initial dense depth map for each video frame. While these depth maps are plausible when viewed individually, they are not geometrically consistent across different frames. Our work aims to produce geometrically consistent camera poses and dense depth for a video.\n\nVideo-based depth estimation. Several methods integrate camera motion estimation and multi-view reconstruction from a pair of frames [58] or multiple frames [72,2,59]. However, these methods work well only on a static scene. To account for moving objects, a line of work use motion segmentation [26,49] or semantic instance segmentation [5] to help constrain the depth of moving objects. State-ofthe-art learning-based video depth estimation approaches can be grouped into two tracks: (1) MVS-based methods and (2) hybrid methods. MVS-based methods improve the conventional SFM and MVS workflow using differentiable pose/depth modules [56] or explicit modeling depth estimation uncertainty [36]. Both methods [36,56] estimate depth based on the cost volume constructed by warping nearby frames to a reference viewpoint. These methods may produce erroneous depth estimation and fail to generate accurate camera trajectories for dynamic scenes. Hybrid methods integrate single-view depth estimation models and multi-view stereo for achieving geometrically consistent video depth, either through fusing depth predictions from single-view and multi-view [70] or fine-tuning singleview depth estimation model to satisfy geometric consistency [39]. While impressive results have been shown, the methods [70,39] assume that precise camera poses are available as input and thus are not applicable for challenging sequences where existing SFM/MVS systems fail. Our method also leverages a pretrained single-view depth estimation model. Unlike [70,39], we jointly optimize camera poses and 3D deformable alignment of the depth maps and thus can handle a broader range of challenging videos of highly dynamic scenes.\n\nVisual odometry. Visual Odometry (VO) or Simultaneously Localization And Mapping (SLAM) aim to estimate the relative camera poses from image sequences [43,51]. Conventional geometry-based methods [15,14,41,49,42] can be grouped into the four categories depending on using direct (feature-less) vs. indirect (feature-based) methods and dense or sparse reconstruction. While significant progress has been made, applying VO and SLAM for generic scenes remains challenging [67]. Recent years, numerous learning-based approaches have been proposed to tackle these challenges either via supervised [58,62,72,27] or self-supervised learning [73,33,71,19,54,65,74,66]. Similar to the existing VO/SLAM methods, our work also estimates both camera poses from a monocular video. Unlike prior work, our primary focus lies in estimating geometrically consistent dense depth reconstruction for dynamic scenes.\n\nTemporal consistency. Per-frame processing often leads to temporal flickering results. Enforcing temporal consistency of a output video has been explored in many different applications, including style transfer [6], video completion [23], video synthesis [63], or as post-processing Figure 2: Overview. Our algorithm only takes a monocular color video as input. We first estimate per-frame depth maps using an existing single-frame CNN. We jointly optimize camera poses as well as flexible deformations to align the depth maps in 3D and resolve any large-scale misalignments. Finally, we resolve fine-scale details using a geometry-aware depth filter. Green frames: inputs; yellow frames: outputs.\n\nstep [31,3,29]. For video depth estimation, temporal consistency can either be explicitly constrained by optical flow [26] or implicitly applied using recurrent neural networks [44]. Our 3D depth filter is similar to that of [31] because we also filter the depth maps across time along the flow trajectory. Our approach differs in that our method is geometry-aware.\n\n\nOverview\n\nOur approach builds on the formulation established in the Consistent Video Depth Estimation (CVD) paper by Luo et al. [39], so let us start by recapping it, first. They iteratively fine-tune the weights of a CNN trained for singleimage depth estimation until it learns to satisfy the geometry of a particular scene in question. To assess the progress against this goal, they relate pairs of images geometrically using known camera parameters (extrinsic and intrinsic, as well as per-frame depth scale factors). More precisely, their algorithm compares the 3D reprojection of pixels from one image to the other with the corresponding image-space motion, computed by an optical flow method. The reprojection error is back-propagated to the network weights, so that it reduces over the course of the fine-tuning (see details in Section 4.2). This results in a very detailed and temporally consistent, i.e., flicker-free, depth video.\n\nHowever, one key limitation of their approach is that precise camera parameters are needed as input, which are computed with SFM in their case. Unfortunately, SFM for video is a challenging problem in itself, and it frequently fails; for example, when a video does not contain sufficient camera motion, or in the presence of dynamic object motion, or in numerous other situations, as we explained earlier. In such cases, it might either fail entirely to produce an output, or fail to register a subset of the frames, or it might produce erroneous (outlier) camera poses. Inaccurate poses have a strong degrading effect on the CVD optimization, as shown in their paper [39]. The key contribution of our paper is to remove this limitation, by replacing the test-time finetuning with joint optimization of the camera parameters and depth alignment.\n\nWe show in Section 4.3 that the same formulation can optimize the camera poses as in CVD. However, one complication is that the pose optimization only works well when we have precise depth, similarly to how depth fine-tuning only works when the poses are accurate (Section 4.4). If the depth is not accurate, which is the case in our setting, misalignments in the depth impose themselves as noisy errors onto the resulting camera pose trajectory.\n\nWe resolve this problem by improving the ability of the camera optimization to align the depth estimates, despite their initial inaccuracy (Section 4.5). Specifically, we achieve this by replacing the per-frame camera scale with a more flexible spatially-varying transformation, i.e., a bilinear spline. The improved alignment of the depth estimates enables computing smoother and more accurate pose trajectories.\n\nThe joint pose estimation and deformation resolves lowfrequency inconsistencies in the depth maps. We further improve high-frequency alignment using a geometry-aware depth filter (Section 4.6). This filter low-pass filters the reprojected depth along flow trajectories. Because the input to the filter is well-aligned, due to the deformation, the filter resolves fine details, rather than blurring them.\n\n\nMethod\n\n\nPre-processing\n\nWe share some of the preprocessing steps with CVD [39]. However, importantly, we do not need to compute COLMAP [52], which considerably improves the robustness of our method.\n\nIn order to lower the overall amount of computation in the pairwise optimization below, we subsample a set of frame pairs spanning temporally near and distant frames:\nP = (i, j) |i \u2212 j| = k, i mod k = 0, k = 1, 2, 4, . . . . (1)\nFor each frame pair (i, j) \u2208 P we compute a dense optical flow field f i\u2192 j (mapping a pixel in frame i to its corresponding location in frame j) using RAFT [57]), as well as a binary mask m flow i\u2192 j that indicates forward-backward consistent flow pixels. Please refer to [39] for details about these preprocessing steps. We also compute a binary segmentation mask m dyn i using Mask R-CNN that indicates likelydynamic pixels (belonging to the categories \"person\", \"animal\", or \"vehicle\").\n\n\nDepth Estimation\n\nIn this section, we establish CVD [39] from a technical point of view and some notation, and in the subsequent sections, we will then present our method.\n\nLet p be a 2D pixel coordinate. We can lift it into a 3D coordinate c i (p) in frame i's 3D camera coordinate system:\nc i (p) = s i d i (p)p.(2)\nHere, s i is the per-frame scale coefficient, and d i is the CNN-estimated depth map, andp is the homogeneousaugmented pixel coordinate, i.e., [p x , p y , 1] . We can also project this 3D point into the camera coordinate system of another frame j:\nc i\u2192 j (p) = K j R j R i K \u22121 i c i (p) + t i \u2212 t j(3)\nHere, R i , R j and t i ,t j and K i , K j are the rotation, translation, and intrinsics of frames i and j, respectively. The objective that CVD optimizes is a reprojection loss for every pixel (with valid flow) in every frame pair:\nargmin \u03b8 depth \u2211 (i, j)\u2208P \u2211 p\u2208m flow i\u2192 j L repro i\u2192 j (p), s.t. fixed \u03b8 cam (4)\nThe optimization variables, \u03b8 depth , are the network weights of the depth CNN, and the fixed camera parameters are\n\u03b8 cam = {R i ,t i , K i , s i }.\nThe reprojection loss is defined as follows:\nL repro i\u2192 j (p) = L sim j c i\u2192 j (p) 3D-reprojection , c j f i\u2192 j (p) Flow-reprojection ,(5)\ni.e., it reprojects the pixel into the other frame's 3D camera coordinate system using (1) 3D geometry and (2) optical flow and measures the similarity of the two resulting 3D points. The exact form of the reprojection similarity loss L sim is not critical for the overall understanding of the algorithm, so we will defer its definition to below.\n\n\nPose Optimization\n\nAs mentioned before, the camera parameters \u03b8 cam are needed for the geometric reprojection mechanics, and it is critically important that they are precise. Otherwise, the depth optimization converges to poor results (Figure 3d). It would be desirable to have a more reliable way to obtain poses for our application than with SFM.\n\nWhen examining Eq. 4 we notice that we can actually use this equation to compute the camera parameters if we reverse the role of \u03b8 depth and \u03b8 cam , i.e., fixing \u03b8 depth (assuming that we know them) and optimizing \u03b8 cam . This modified equation resembles the triangulation in bundle adjustment, but it can be more robustly solved since the depth of  matched image points does not need to be estimated since it is known (up to scale). However, this time it is the depth that needs to be known precisely for this to work well. In the depth maps do not agree with each other, these misalignment errors will manifest as noisy errors in the estimated camera parameters (Figure 3c).\n\n\nPose and Depth Optimization with Fine-tuning\n\nWhat about optimizing both quantities, \u03b8 cam and \u03b8 depth , jointly? One problem is that these two quantities are best optimized with different kinds of machinery. \u03b8 depth is best optimized using standard training algorithms for CNNs, i.e., SGD. For \u03b8 cam , however, that is not a good fit, since changes to one parameters have far-reaching influence, as the poses are chained in a trajectory. Global continuous optimization is a better solution for \u03b8 cam and converges faster and more stably. We can optimize both quantities by alternating between optimizing depth and pose, each with their respective best optimization algorithm while keeping the other quantity fixed.\n\nHowever, another significant problem is the sensitivity to the accuracy of the particular fixed parameters, as explained before. Starting with the initially inaccurate depth estimate will result in noisy poses (Figure 3c), because the misalignment errors will \"push\" the camera pose variables in erratic ways. The jittery poess will, in the next step, degrade the depth estimate further. The algorithm does not converge to a good solution (Figure 3d).\n\n\nPose and Depth with Flexible Deformation\n\nOur solution to this apparent dilemma is to improve the depth alignment in the pose estimation. We achieve this by injecting a smooth and flexible spatially-varying depth deformation model into the alignment. More precisely, we replace the depth scale coefficients s i in Eq. 4 with a spatiallyvarying bilinear spline:\n\u03d5 i (p) = \u2211 k b k (p) s k i .(6)\nHere, the s k i are scale factors that are defined on a regular grid of \"handles\" across the image. b k (p) are bilinear coefficients, such that within a grid cell the four surrounding handles of a pixel p are bilinearly interpolated.\n\nAfter this change the depth maps align better and will not impose jittery errors onto the estimated camera trajectory anymore (Figure 3e). In addition, this algorithm is considerably faster since we do not need to iterate between pose optimization and fine-tuning.\n\n\nGeometry-aware Depth Filtering\n\nThe flexible deformation \u03d5 i achieves a low-frequency alignment of the depth map, i.e., removing any large-scale Input Initial depth Filtered depth misalignments. But what about fine depth details? We tried using alternating pose-depth optimization, as described in Section 4.4, with flexible deformation. This works and improves both the depth and poses slightly, but it does not reach the same level of quality that CVD achieves when using precise SFM pose as input. Instead, it converges quickly to a configuration where both depth and pose alignment are well-satisfied, but depth details smooth out considerably.\n\nWe solve this problem instead with a spatio-temporal depth filter that follows flow trajectories. Importantly, the filter is geometry-aware in the sense that it transforms the depths from other frames using the reprojection mechanics in Equations 2-3:\nd final i (p) = \u2211 q\u2208N(p) i+\u03c4 \u2211 j=i\u2212\u03c4 z j\u2192i f i\u2192 j (q) w i\u2192 j (q).(7)\nHere, N(p) refers to a 3 \u00d7 3 neighborhood centered around the pixel p, \u03c4 = 4, z j\u2192i is the scalar z-component of c j\u2192i (i.e., the reprojected depth), andf is the flow between far frames obtained by chaining flow maps between consecutive frames. The weights w i\u2192 j (q) make the filter edgepreserving by reducing the influence os samples across depth edges:\nw i\u2192 j (q) = exp \u2212 \u03bb f L ratio c i (p)\n, c j\u2192i f i\u2192 j (q) . (8) \u03bb f = 3 adjusts the strength of the filter, and L ratio measures the similarity of the reference and the reprojected pixel in camera coordinates (the definition is given in the implementation details below). The improved provided by the depth filter can be seen in Figure 3f, and more results in Figure 4.\n\n\nImplementation Details\n\nPrecomputation. For MiDaS we downscale the image to 384 pixels on the long side (the default resolution in their code). For RAFT we downscale the image to 1024 pixels on the long side, to lower memory requirements. In both cases we adjust the short image side according to image aspect ratio, while rounding to the nearest multiple of 16 pixels which is the alignment requirement of the respective CNNs. We use the pretrained raft-things.pth from the RAFT project page [57], which was trained on FlyingChairs and FlyingThings. Pose optimization. We use the SPARSE NORMAL CHO-LESKY solver from the Ceres library [1] to solve the camera pose estimation problem. In order to reduce the computational complexity, we do not include every pixel in the optimization but instead, subsample a set of pairwise matches from the flow fields (so that there is a minimum distance of at least 10 pixels between any pair of matches). Since Eq. 4 assumes a static scene, we exclude any points matches within the m dyn mask.\n\nSince the objective is non-convex optimization, it is somewhat sensitive to local minima in the objective. We alleviate that problem by first optimizing a 1 \u00d7 1 grid (similar to the original s i scalar coefficients), and then subdividing it in four steps until a grid resolution of 17 \u00d7 10 is reached (always using 17 for the long image dimension, and adjusting the short image dimension according to the aspect ratio). After every step, we optimize until convergence and use the result as initialization for the next step.\n\nSince the scale of the depth maps estimated by the CNN is arbitrary, we initialize the scale of the first frame so that the median depth is 1, and use the same scale for all other frames. Pose regularization. To encourage smoothness in the deformation field, we add a loss that penalizes large differences in neighboring grid values:\nL deform = \u2211 i \u2211 (k,r)\u2208N s k i \u2212 s r i 2 2 max(w k i , w r i ),(9)\nwhere N refers to the set of all vertically and horizontally neighboring vertices. The weights are set to encourage more smoothness in parts of the image that are masked by m dyn , since there are no point matches in these regions and they are unconstrained otherwise.\nw k i = \u03bb 1 + \u03bb 2 \u2211 p m dyn i (p) b k (p)(10)\n\u03bb 1 = 0.1, \u03bb 2 = 10 are balancing coefficients. The \u03bb 2 term computes the fraction of dynamic pixels in the handle's influence region. We use the following form for the intrinsic matrices:\nK i = \uf8ee \uf8f0 u i u i 1 \uf8f9 \uf8fb ,(11)\ni.e., the only degree of freedom is the focal length u i . We, further, add a small bias,\nL focal = \u2211 i (u i \u2212\u00fb) 2 ,(12)\nwhere\u00fb = 0.35 (corresponds to \u223c40 \u2022 field of view).\n\nReprojection loss. In Section 4.2 we omitted the definition of the reprojection similarity loss L sim . A na\u00efve way would be to simple measure the Euclidean distance\nL euclidean (a, b) = a \u2212 b 2 2 .(13)\nHowever, this biases the solution toward small depths. Shrinking the whole scene to a point would achieve a minimum.\n\nTo prevent this, Luo et al. [39] use a split loss where they measure the spatial component L spatial in image space\nL spatial (a, b) = a xy a z \u2212 b xy b z 2 2 ,(14)\nand the depth L depth component in disparity space.\nL disparity (a, b) = 1 a z \u2212 1 b z 2 2 .(15)\nThe disparity loss actually has the opposite bias of Euclidean loss: it is minimized when scene scale grows very large (so that the disparities vanish). This is not a problem for Luo et al., since they use fixed poses. However, it affects our results negatively.\n\nTo alleviate this, we propose a new loss that measures the ratio of depth values:\nL ratio (a, b) = max(a z , b z ) min(a z , b z ) \u2212 1.(16)\nThis loss does not suffer from any depth bias; it does neither encourage growing nor shrinking the scene scale. The measure L ratio is also used to compute the depth-similarity of samples in the depth filter in Eq. 8. In summary, we define the reprojection similarity loss as follows:\nL sim (a, b) = L spatial (a, b) + L ratio (a, b)(17)\n\nExperimental Results\n\n\nExperimental setup\n\nDatasets. We validate the effectiveness of the proposed method on three main video datasets, covering a wide range of challenging indoor and outdoor scenes. Here we focus on reporting results on the Sintel [4] dataset. The MPI Sintel dataset consists of 23 synthetic sequences of highly dynamic scenes. Each sequence comes with ground truth depth measured in meters as well as ground truth camera poses. We use both the clean and final versions of the dataset. The ground truth annotations allow us to conduct a quantitative comparison on both the estimated depth and pose. We refer the readers to our qualitative results on DAVIS [45] and Cellphone videos in the project page. Note that we do not choose datasets focusing on closeddomain applications such as driving scenes (e.g., KITTI depth/odometry dataset or office (e.g., TUM RGB-D [55]) or fully static scenes (e.g., ScanNet [10]). Compared methods. We compare our results with several state-of-the-art depth estimation algorithms.\n\n\u2022 COLMAP [52]: Traditional SFM/MVS algorithm for 3D reconstruction. As COLMAP reconstruction is very sensitive to dynamic objects in the scene, we use the same m dyn dynamic masks using by our algorithm (computed automatically Mask R-CNN) to exclude feature extraction/matching from those areas.\n\n\u2022 DeepV2D [56]: Video depth estimation algorithm using differentable motion estimation and depth estimation.\n\n\u2022 CVD [39]: A hybrid method that combine multi-view and single-view depth estimation methods for producing consistent video depth.\n\n\u2022 MiDaS-v2 [48]: State-of-the-art single-view depth estimation model.\n\n\nEvaluation on MPI Sintel dataset [4]\n\nDepth evaluation. As depth estimation from all the methods is up to an unknown scene scale, we follow the standard depth evaluation protocol to align the predicted depth and the ground truth depth using median scaling. We exclude depth values that are larger than 80 meters. Table 1 shows the quantitative comparisons with the state-of-the-art on various metrics. Note that COLMAP [52] fails to estimate the camera pose for 11 in 23 sequences. CVD [39], which uses COLMAP poses as input, thus, also does not produce depth estimation results for these scenes. Furthermore, COLMAP discards single-pixel depth estimates deemed unreliable. As a result, we cannot evaluate these methods using standard averaged metrics. Instead, we store all the pixel-wise error metrics across all pixels, frames, and videos. We then Pose evaluation. We follow the standard evaluation protocol of visual odometry for pose evaluation and compare our methods against state-of-the-arts in terms of absolute trajectory error (ATE) and relative pose error (RPE). ATE measures the root-mean-square error between predicted camera poses [x, y, z] and ground truth. RPE measures frameto-frame relative pose error between frame pairs, including translation error (RPE-T) and rotational error (RPE-R). Still, since the scene scale is unknown, we scale and align the predictions to the ground truth associated poses during evaluation by minimizing ATE for fair comparisons. We conduct pose evaluations on Sintel with ground truth pose annotations, and the quantitative results are presented in Table 1 and Figure 5. As demonstrated in Table 1, our proposed method outperforms MiDaS-v2 on all metrics (mean of ATE, RPE-T, and RPE-R) with a noticeable margin and significantly outperforms DeepV2D, on both the Clean and Final categories of Sintel. Note that since COLMAP and CVD fail on a significant portion of the dynamic scenes of Sintel, it is not directly comparable in terms of mean errors as in Table 1. For a fair comparison, we store RPE-T and RPE-R errors from all plausible pose predictions between frame pairs from all Sintel sequences, then sort them and plot the curves in Figure 5. The results show our proposed method consistently achieves more accurate pose predictions than completing methods.\n\n\nLimitations\n\nThe main limitation that we observe is a kind of residual wobble of the aligned depth maps. It is apparent in most results we provide on the project page. We think that it can be addressed by better deformation models, in particular, replacing the spline-based deformation with a pixel-based de- Pose Error (RPE) on Sintel. All the frame pair-wise errors are stored and sorted for plotting the distributions. Note that COLMAP and CVD (which relies on COLMAP) fail on many Sintel sequences, resulting in partial data points. formation field with appropriate regularization. However, this would require denser pairwise constraints, which our current formulation using continuous global optimization does not support, i.e., when we densify the constraints the Ceres Solver memory usage blows up and the performance goes down drastically. This is due to the global nature of the optimization problem. Finding a better formulation to resolve this problem is a great avenue for future work.\n\n\nConclusions\n\nWe presented a general optimization algorithm for consistent depth estimation on monocular videos, requiring neither input poses nor inference-time fine-tuning. Our method attains robust reconstruction for challenging dynamic videos casually captured by hand-held devices, and achieves better performances on diverse test beds.\n\nFigure 3 :\n3Joint depth and pose optimization. Various configurations of our algorithm: (a-b) Ground truth depth with ground truth and estimated poses, respectively. (c) Misalignments in estimated depth impose jittery errors on the optimized camera trajectories. (d) CVD-style fine-tuning fails in the absence of precise poses. (e) Our flexible deformations resolve depth misalignments, which results in smoother camera trajectories. (f) Using geometry-aware depth filtering we resolve fine depth details (our final result).\n\nFigure 4 :\n4Geometry-aware depth filter. Top: frames from the input video. Middle: initial per-frame depth estimates (after flexible alignment). Bottom: the geometry-aware depth filter resolves fine-scale details in the depth maps.\n\nFigure 5 :\n5Evaluation of translational and rotational Relative\n\nTable 1 :\n1Quantitative evaluations of depth and pose on the MPI Sintel benchmark (Top: Sintel Clean, Bottom: Sintel Final). For depth evaluation, we present per-frame evaluations on standard error and accuracy metrics. For pose evaluation, we present per-sequence evaluations on translational and rotational error metrics. Depth -Error metric \u2193 Depth -Accuracy metric \u2191 Pose -Error metric \u2193 Method Abs Rel Sq Rel RMSE log RMSE \u03b4 < 1.25 \u03b4 < 1.25 2 \u03b4 < 1.25 3 ATE (m)\u2193 RPE Trans (m)\u2193 RPE Rot (deg)\u2193 sort these errors and plot the curves. Please refer to the supplemental material.The plots capture both the accuracy and completeness of each method.DeepV2D [56] \n0.526 \n3.629 \n6.493 \n0.683 \n0.487 \n0.671 \n0.761 \n0.9526 \n0.3819 \n0.1869 \nOurs -Single-scale pose (aligned MiDaS) \n0.380 \n2.617 \n5.773 \n0.533 \n0.562 \n0.736 \n0.832 \n0.1883 \n0.0806 \n0.0262 \nOurs -Single-scale pose + depth fine-tuning \n0.472 \n3.444 \n6.340 \n0.635 \n0.534 \n0.694 \n0.790 \n0.1686 \n0.0724 \n0.0139 \nOurs -Single-scale pose + depth filter \n0.375 \n2.546 \n5.763 \n0.530 \n0.569 \n0.738 \n0.835 \n0.1882 \n0.0806 \n0.0262 \nOurs -Flexible pose \n0.379 \n2.702 \n5.795 \n0.533 \n0.565 \n0.744 \n0.836 \n0.1843 \n0.0723 \n0.0095 \nOurs -Flexible pose + depth fine-tuning \n0.439 \n3.100 \n6.213 \n0.614 \n0.524 \n0.698 \n0.796 \n0.1656 \n0.0651 \n0.0070 \nOurs -Flexible pose + depth filter \n0.377 \n2.657 \n5.786 \n0.531 \n0.568 \n0.745 \n0.837 \n0.1843 \n0.0723 \n0.0095 \n\nDeepV2D \n0.526 \n3.620 \n6.470 \n0.670 \n0.486 \n0.674 \n0.760 \n0.9192 \n0.5834 \n0.2506 \nOurs -Single-scale pose (aligned MiDaS) \n0.425 \n2.640 \n5.858 \n0.559 \n0.529 \n0.726 \n0.828 \n0.2210 \n0.0827 \n0.0258 \nOurs -Single-scale pose + depth fine-tuning \n0.473 \n3.215 \n6.298 \n0.639 \n0.527 \n0.684 \n0.782 \n0.1620 \n0.0727 \n0.0116 \nOurs -Single-scale pose + depth filter \n0.421 \n2.616 \n5.850 \n0.556 \n0.533 \n0.728 \n0.830 \n0.1803 \n0.0827 \n0.0258 \nOurs -Flexible pose \n0.421 \n2.660 \n5.906 \n0.559 \n0.523 \n0.730 \n0.832 \n0.1831 \n0.0713 \n0.0088 \nOurs -Flexible pose + depth fine-tuning \n0.438 \n3.053 \n6.300 \n0.605 \n0.525 \n0.705 \n0.807 \n0.1594 \n0.0652 \n0.0073 \nOurs -Flexible pose + depth filter \n0.419 \n2.628 \n5.896 \n0.558 \n0.526 \n0.730 \n0.833 \n0.1831 \n0.0714 \n0.0088 \n\n\n\n. Sameer Agarwal, Keir Mierle, Others Ceres, Sameer Agarwal, Keir Mierle, and Others. Ceres solver. http://ceres-solver.org. 6\n\nCodeslam-learning a compact, optimisable representation for dense visual slam. Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, Andrew J Davison, In CVPR. 3Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J Davison. Codeslam-learning a compact, optimisable representation for dense visual slam. In CVPR, 2018. 3\n\nBlind video temporal consistency. Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, Deqing Sun, Sylvain Paris, Hanspeter Pfister, ACM Transactions on Graphics (TOG). 346196Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, De- qing Sun, Sylvain Paris, and Hanspeter Pfister. Blind video temporal consistency. ACM Transactions on Graph- ics (TOG), 34(6):196, 2015. 3\n\nA naturalistic open source movie for optical flow evaluation (Sintel movie \u00a9 copyright Blender Foundation, www.sintel. org). D J Butler, J Wulff, G B Stanley, M J Black, ECCV. 27D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A nat- uralistic open source movie for optical flow evaluation (Sin- tel movie \u00a9 copyright Blender Foundation, www.sintel. org). In ECCV, 2012. 2, 7\n\nDepth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. Vincent Casser, Soeren Pirk, Reza Mahjourian, Anelia Angelova, AAAI. 23Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia Angelova. Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. In AAAI, 2019. 2, 3\n\nCoherent online video style transfer. Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, Gang Hua, ICCV. Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang Hua. Coherent online video style transfer. In ICCV, 2017. 3\n\nSingleimage depth perception in the wild. Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng, NeurIPS. Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single- image depth perception in the wild. In NeurIPS, 2016. 2\n\nLearning singleimage depth from videos using quality assessment networks. Weifeng Chen, Shengyi Qian, Jia Deng, CVPR. Weifeng Chen, Shengyi Qian, and Jia Deng. Learning single- image depth from videos using quality assessment networks. In CVPR, 2019. 2\n\nOasis: A large-scale dataset for single image 3d in the wild. Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, Jia Deng, CVPR. 2020Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, and Jia Deng. Oasis: A large-scale dataset for single image 3d in the wild. In CVPR, 2020. 2\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, X Angel, Manolis Chang, Maciej Savva, Thomas Halber, Matthias Funkhouser, Nie\u00dfner, CVPR. Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 7\n\nSelf-supervised object motion and depth estimation from video. Qi Dai, Vaishakh Patil, Simon Hecker, Dengxin Dai, Luc Van Gool, Konrad Schindler, arXiv:1912.04250arXiv preprintQi Dai, Vaishakh Patil, Simon Hecker, Dengxin Dai, Luc Van Gool, and Konrad Schindler. Self-supervised object motion and depth estimation from video. arXiv preprint arXiv:1912.04250, 2019. 2\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. David Eigen, Rob Fergus, ICCV. David Eigen and Rob Fergus. Predicting depth, surface nor- mals and semantic labels with a common multi-scale convo- lutional architecture. In ICCV, 2015. 2\n\nDepth map prediction from a single image using a multi-scale deep network. David Eigen, Christian Puhrsch, Rob Fergus, NeurIPS. David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep net- work. In NeurIPS, 2014. 2\n\nDirect sparse odometry. Jakob Engel, Vladlen Koltun, Daniel Cremers, IEEE transactions on pattern analysis and machine intelligence. 40Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3):611-625, 2017. 3\n\nLsdslam: Large-scale direct monocular slam. Jakob Engel, Thomas Sch\u00f6ps, Daniel Cremers, ECCV. Jakob Engel, Thomas Sch\u00f6ps, and Daniel Cremers. Lsd- slam: Large-scale direct monocular slam. In ECCV, 2014. 3\n\nDeep ordinal regression network for monocular depth estimation. Huan Fu, Mingming Gong, Chaohui Wang, CVPR. Kayhan Batmanghelich, and Dacheng TaoHuan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat- manghelich, and Dacheng Tao. Deep ordinal regression net- work for monocular depth estimation. In CVPR, 2018. 2\n\nMulti-view stereo: A tutorial. Foundations and Trends\u00ae in Computer Graphics and Vision. Yasutaka Furukawa, Carlos Hern\u00e1ndez, 9Yasutaka Furukawa, Carlos Hern\u00e1ndez, et al. Multi-view stereo: A tutorial. Foundations and Trends\u00ae in Computer Graphics and Vision, 9(1-2):1-148, 2015. 2\n\nUnsupervised monocular depth estimation with leftright consistency. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J Brostow, CVPR. Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Bros- tow. Unsupervised monocular depth estimation with left- right consistency. In CVPR, 2017. 2\n\nDigging into self-supervised monocular depth estimation. Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, Gabriel J Brostow, ICCV. Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular depth estimation. In ICCV, 2019. 3\n\nDepth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. Ariel Gordon, Hanhan Li, Rico Jonschkowski, Anelia Angelova, ICCV. Ariel Gordon, Hanhan Li, Rico Jonschkowski, and Anelia Angelova. Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. In ICCV, 2019. 2\n\nLearning monocular depth by distilling cross-domain stereo networks. Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, Xiaogang Wang, In ECCV. 2Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, and Xiaogang Wang. Learning monocular depth by distilling cross-domain stereo networks. In ECCV, 2018. 2\n\nDuygu Ceylan, and Hailin Jin. 6-dof vr videos with a single 360-camera. Jingwei Huang, Zhili Chen, 2017. 1IEEE Virtual Reality. Jingwei Huang, Zhili Chen, Duygu Ceylan, and Hailin Jin. 6-dof vr videos with a single 360-camera. In 2017 IEEE Virtual Reality (VR), 2017. 1\n\nTemporally coherent completion of dynamic video. Jia-Bin Huang, Bing Sing, Narendra Kang, Johannes Ahuja, Kopf, ACM TOG (Proc. SIGGRAPH Asia). 35Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Jo- hannes Kopf. Temporally coherent completion of dynamic video. ACM TOG (Proc. SIGGRAPH Asia), 35(6):1-11, 2016. 3\n\nDeepMVS: Learning multi-view stereopsis. Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, Jia-Bin Huang, In CVPR. 2Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. DeepMVS: Learning multi-view stereopsis. In CVPR, 2018. 2\n\nDPSNet: end-to-end deep plane sweep stereo. Sunghoon Im, Hae-Gon Jeon, Stephen Lin, In So Kweon, ICLR. Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. DPSNet: end-to-end deep plane sweep stereo. In ICLR, 2019. 2\n\nDepth transfer: Depth extraction from video using non-parametric sampling. Kevin Karsch, Ce Liu, Sing Bing Kang, TPAMI. 3611Kevin Karsch, Ce Liu, and Sing Bing Kang. Depth transfer: Depth extraction from video using non-parametric sampling. TPAMI, 36(11):2144-2158, 2014. 3\n\ngradslam: Dense slam meets automatic differentiation. Soroush Jatavallabhula Krishna Murthy, Ganesh Saryazdi, Liam Iyer, Paull, arXiv, 2020. 3Jatavallabhula Krishna Murthy, Soroush Saryazdi, Ganesh Iyer, and Liam Paull. gradslam: Dense slam meets automatic differentiation. arXiv, 2020. 3\n\nNormal assisted stereo depth estimation. Uday Kusupati, Shuo Cheng, Rui Chen, Hao Su, CVPR. Uday Kusupati, Shuo Cheng, Rui Chen, and Hao Su. Normal assisted stereo depth estimation. In CVPR, 2020. 2\n\nLearning blind video temporal consistency. Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, Ming-Hsuan Yang, In ECCV. 3Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. Learning blind video temporal consistency. In ECCV, 2018. 3\n\nVasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. Iro Laina, Christian Rupprecht, 3D Vision (3DV). Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed- erico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In 3D Vision (3DV), 2016. 2\n\nPractical temporal consistency for imagebased graphics applications. Manuel Lang, Oliver Wang, Tunc Aydin, Aljoscha Smolic, Markus Gross, ACM TOG (Proc. SIGGRAPH). 31Manuel Lang, Oliver Wang, Tunc Aydin, Aljoscha Smolic, and Markus Gross. Practical temporal consistency for image- based graphics applications. ACM TOG (Proc. SIGGRAPH), 31(4):1-8, 2012. 3\n\nUnsupervised monocular depth learning in dynamic scenes. Hanhan Li, Ariel Gordon, Hang Zhao, Vincent Casser, Anelia Angelova, CoLR. 2020Hanhan Li, Ariel Gordon, Hang Zhao, Vincent Casser, and Anelia Angelova. Unsupervised monocular depth learning in dynamic scenes. In CoLR, 2020. 2\n\nUndeepvo: Monocular visual odometry through unsupervised deep learning. Ruihao Li, Sen Wang, Zhiqiang Long, Dongbing Gu, In ICRA. 3Ruihao Li, Sen Wang, Zhiqiang Long, and Dongbing Gu. Undeepvo: Monocular visual odometry through unsuper- vised deep learning. In ICRA, 2018. 3\n\nLearning the depths of moving people by watching frozen people. Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, William T Freeman, CVPR. Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, and William T Freeman. Learning the depths of moving people by watching frozen people. In CVPR, 2019. 2\n\nMegadepth: Learning singleview depth prediction from internet photos. Zhengqi Li, Noah Snavely, CVPR. Zhengqi Li and Noah Snavely. Megadepth: Learning single- view depth prediction from internet photos. In CVPR, 2018. 2\n\nNeural rgb (r) d sensing: Depth and uncertainty from a video camera. Chao Liu, Jinwei Gu, Kihwan Kim, G Srinivasa, Jan Narasimhan, Kautz, CVPR. 23Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa G Narasimhan, and Jan Kautz. Neural rgb (r) d sensing: Depth and uncer- tainty from a video camera. In CVPR, 2019. 2, 3\n\nHailin Jin, and Aseem Agarwala. Content-preserving warps for 3d video stabilization. Feng Liu, Michael Gleicher, ACM TOG (Proc. SIGGRAPH). 28Feng Liu, Michael Gleicher, Hailin Jin, and Aseem Agar- wala. Content-preserving warps for 3d video stabilization. ACM TOG (Proc. SIGGRAPH), 28(3), 2009. 1\n\nLearning depth from single monocular images using deep convolutional neural fields. Fayao Liu, Chunhua Shen, Guosheng Lin, Ian Reid, TPAMI. 3810Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning depth from single monocular images using deep convolutional neural fields. TPAMI, 38(10):2024-2039, 2015. 2\n\nConsistent video depth estimation. Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, Johannes Kopf, ACM TOG (Proc. SIGGRAPH). 39Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM TOG (Proc. SIGGRAPH), 39(4), 2020. 1, 2, 3, 4, 7\n\nA large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox, CVPR. Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In CVPR, 2016. 2\n\nOrb-slam2: An opensource slam system for monocular, stereo, and rgb-d cameras. Raul Mur, -Artal , Juan D Tard\u00f3s, IEEE Transactions on Robotics. 335Raul Mur-Artal and Juan D Tard\u00f3s. Orb-slam2: An open- source slam system for monocular, stereo, and rgb-d cam- eras. IEEE Transactions on Robotics, 33(5):1255-1262, 2017. 3\n\nDtam: Dense tracking and mapping in real-time. A Richard, Newcombe, J Steven, Andrew J Lovegrove, Davison, ICCV. Richard A Newcombe, Steven J Lovegrove, and Andrew J Davison. Dtam: Dense tracking and mapping in real-time. In ICCV, 2011. 3\n\nVisual odometry. David Nist\u00e9r, Oleg Naroditsky, James Bergen, CVPR. David Nist\u00e9r, Oleg Naroditsky, and James Bergen. Visual odometry. In CVPR, 2004. 3\n\nDon't forget the past: Recurrent depth estimation from monocular video. Vaishakh Patil, Dengxin Wouter Van Gansbeke, Luc Dai, Van Gool, arXiv:2001.02613arXiv preprintVaishakh Patil, Wouter Van Gansbeke, Dengxin Dai, and Luc Van Gool. Don't forget the past: Recurrent depth estimation from monocular video. arXiv preprint arXiv:2001.02613, 2020. 3\n\nLuc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. Federico Perazzi, Jordi Pont-Tuset, Brian Mcwilliams, CVPR. Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In CVPR, 2016. 7\n\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alex Sorkine-Hornung, Luc Van Gool, arXiv:1704.00675The 2017 davis challenge on video object segmentation. arXiv preprintJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar- bel\u00e1ez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 2\n\nGeonet: Geometric neural network for joint depth and surface normal estimation. Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, Jiaya Jia, CVPR. Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia. Geonet: Geometric neural network for joint depth and surface normal estimation. In CVPR, 2018. 2\n\nTowards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. Ren\u00e9 Ranftl, Katrin Lasinger, Konrad Schindler, Vladlen Koltun, TPAMI. 7Ren\u00e9 Ranftl, Katrin Lasinger, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. TPAMI, 2020. 2, 5, 7\n\nDense monocular depth estimation in complex dynamic scenes. Rene Ranftl, Vibhav Vineet, Qifeng Chen, Vladlen Koltun, CVPR. Rene Ranftl, Vibhav Vineet, Qifeng Chen, and Vladlen Koltun. Dense monocular depth estimation in complex dy- namic scenes. In CVPR, 2016. 3\n\nCompetitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun, Jonas Wulff, Michael J Black, CVPR. Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun, Jonas Wulff, and Michael J Black. Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. In CVPR, 2019. 2\n\nVisual odometry [tutorial]. IEEE robotics & automation magazine. Davide Scaramuzza, Friedrich Fraundorfer, 18Davide Scaramuzza and Friedrich Fraundorfer. Visual odom- etry [tutorial]. IEEE robotics & automation magazine, 18(4):80-92, 2011. 3\n\nStructurefrom-motion revisited. L Johannes, Jan-Michael Schonberger, Frahm, CVPR. 7Johannes L Schonberger and Jan-Michael Frahm. Structure- from-motion revisited. In CVPR, 2016. 2, 4, 7\n\nA comparison and evaluation of multi-view stereo reconstruction algorithms. M Steven, Brian Seitz, James Curless, Daniel Diebel, Richard Scharstein, Szeliski, CVPR. Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. A comparison and eval- uation of multi-view stereo reconstruction algorithms. In CVPR, 2006. 2\n\nUnsupervised collaborative learning of keyframe detection and visual odometry towards monocular deep slam. Lu Sheng, Dan Xu, Wanli Ouyang, Xiaogang Wang, ICCV. Lu Sheng, Dan Xu, Wanli Ouyang, and Xiaogang Wang. Un- supervised collaborative learning of keyframe detection and visual odometry towards monocular deep slam. In ICCV, 2019. 3\n\nA benchmark for the evaluation of rgb-d slam systems. J Sturm, N Engelhard, F Endres, W Burgard, D Cremers, International Conference on Intelligent Robot Systems (IROS). J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre- mers. A benchmark for the evaluation of rgb-d slam sys- tems. In International Conference on Intelligent Robot Sys- tems (IROS), 2012. 7\n\nDeepV2D: Video to depth with differentiable structure from motion. Zachary Teed, Jia Deng, ICLR. Zachary Teed and Jia Deng. DeepV2D: Video to depth with differentiable structure from motion. In ICLR, 2020. 2, 3, 7, 8\n\nRAFT: Recurrent all-pairs field transforms for optical flow. Zachary Teed, Jia Deng, ECCV, 2020. 46Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020. 4, 6\n\nDemon: Depth and motion network for learning monocular stereo. Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox, CVPR. 23Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Niko- laus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In CVPR, 2017. 2, 3\n\nDepth from motion for smartphone ar. Julien Valentin, Adarsh Kowdle, Jonathan T Barron, Neal Wadhwa, Max Dzitsiuk, Michael Schoenberg, Vivek Verma, Ambrus Csaszar, Eric Turner, Ivan Dryanovski, ACM Transactions on Graphics (TOG). 3763Julien Valentin, Adarsh Kowdle, Jonathan T Barron, Neal Wadhwa, Max Dzitsiuk, Michael Schoenberg, Vivek Verma, Ambrus Csaszar, Eric Turner, Ivan Dryanovski, et al. Depth from motion for smartphone ar. ACM Transactions on Graphics (TOG), 37(6):1-19, 2018. 1, 3\n\nSudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, Katerina Fragkiadaki, arXiv:1704.07804Sfmnet: Learning of structure and motion from video. arXiv preprintSudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, and Katerina Fragkiadaki. Sfm- net: Learning of structure and motion from video. arXiv preprint arXiv:1704.07804, 2017. 2\n\nWeb stereo video supervision for depth prediction from dynamic scenes. Chaoyang Wang, Simon Lucey, Federico Perazzi, Oliver Wang, 3Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver Wang. Web stereo video supervision for depth prediction from dynamic scenes. In 3DV, 2019. 2\n\nDeepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. Sen Wang, Ronald Clark, Hongkai Wen, Niki Trigoni, In ICRA. 3Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. Deepvo: Towards end-to-end visual odometry with deep re- current convolutional neural networks. In ICRA, 2017. 3\n\nVideo-tovideo synthesis. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, Bryan Catanzaro, In NeurIPS. 3Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to- video synthesis. In NeurIPS, 2018. 3\n\nSelf-supervised monocular depth hints. Jamie Watson, Michael Firman, J Gabriel, Daniyar Brostow, Turmukhambetov, ICCV. Jamie Watson, Michael Firman, Gabriel J Brostow, and Daniyar Turmukhambetov. Self-supervised monocular depth hints. In ICCV, 2019. 2\n\nBeyond tracking: Selecting memory and refining poses for deep visual odometry. Fei Xue, Xin Wang, Shunkai Li, Qiuyuan Wang, Junqiu Wang, Hongbin Zha, CVPR. Fei Xue, Xin Wang, Shunkai Li, Qiuyuan Wang, Junqiu Wang, and Hongbin Zha. Beyond tracking: Selecting mem- ory and refining poses for deep visual odometry. In CVPR, 2019. 3\n\nD3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. Nan Yang, Rui Lukas Von Stumberg, Daniel Wang, Cremers, CVPR. 2020Nan Yang, Lukas von Stumberg, Rui Wang, and Daniel Cre- mers. D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. In CVPR, 2020. 3\n\nChallenges in monocular visual odometry: Photometric calibration, motion bias, and rolling shutter effect. Nan Yang, Rui Wang, Xiang Gao, Daniel Cremers, IEEE Robotics and Automation Letters. 34Nan Yang, Rui Wang, Xiang Gao, and Daniel Cremers. Chal- lenges in monocular visual odometry: Photometric calibra- tion, motion bias, and rolling shutter effect. IEEE Robotics and Automation Letters, 3(4):2878-2885, 2018. 3\n\nMVSNet: Depth inference for unstructured multiview stereo. Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, Long Quan, In ECCV. 2Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. MVSNet: Depth inference for unstructured multi- view stereo. In ECCV, 2018. 2\n\nGeonet: Unsupervised learning of dense depth, optical flow and camera pose. Zhichao Yin, Jianping Shi, CVPR. Zhichao Yin and Jianping Shi. Geonet: Unsupervised learn- ing of dense depth, optical flow and camera pose. In CVPR, 2018. 2\n\nNovel view synthesis of dynamic scenes with globally coherent depths from a monocular camera. Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, Jan Kautz, CVPR. 23Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera. In CVPR, 2020. 2, 3\n\nUnsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction. Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li, Harsh Agarwal, Ian Reid, CVPR. Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li, Harsh Agarwal, and Ian Reid. Unsupervised learn- ing of monocular depth estimation and visual odometry with deep feature reconstruction. In CVPR, 2018. 3\n\nDeeptam: Deep tracking and mapping. Huizhong Zhou, Benjamin Ummenhofer, Thomas Brox, ECCV. Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. Deeptam: Deep tracking and mapping. In ECCV, 2018. 3\n\nUnsupervised learning of depth and ego-motion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, CVPR. 23Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In CVPR, 2017. 2, 3\n\nLearning monocular visual odometry via self-supervised long-term modeling. Yuliang Zou, Pan Ji, Quoc-Huy Tran, Jia-Bin Huang, Manmohan Chandraker, ECCV. 2020Yuliang Zou, Pan Ji, Quoc-Huy Tran, Jia-Bin Huang, and Manmohan Chandraker. Learning monocular visual odome- try via self-supervised long-term modeling. In ECCV, 2020. 3\n\nDF-net: Unsupervised joint learning of depth and flow using cross-task consistency. Yuliang Zou, Zelun Luo, Jia-Bin Huang, In ECCV. 2Yuliang Zou, Zelun Luo, and Jia-Bin Huang. DF-net: Un- supervised joint learning of depth and flow using cross-task consistency. In ECCV, 2018. 2\n", "annotations": {"author": "[{\"end\":58,\"start\":44},{\"end\":76,\"start\":59},{\"end\":91,\"start\":77},{\"end\":106,\"start\":92},{\"end\":121,\"start\":107}]", "publisher": null, "author_last_name": "[{\"end\":57,\"start\":53},{\"end\":75,\"start\":68},{\"end\":90,\"start\":82},{\"end\":105,\"start\":100},{\"end\":120,\"start\":116}]", "author_first_name": "[{\"end\":52,\"start\":44},{\"end\":67,\"start\":59},{\"end\":81,\"start\":77},{\"end\":99,\"start\":92},{\"end\":115,\"start\":107}]", "author_affiliation": null, "title": "[{\"end\":41,\"start\":1},{\"end\":162,\"start\":122}]", "venue": null, "abstract": "[{\"end\":1463,\"start\":164}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1630,\"start\":1626},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":1685,\"start\":1681},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1688,\"start\":1685},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1753,\"start\":1749},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2865,\"start\":2861},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2868,\"start\":2865},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2871,\"start\":2868},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3300,\"start\":3296},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3303,\"start\":3300},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":3306,\"start\":3303},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3309,\"start\":3306},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4029,\"start\":4025},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4346,\"start\":4342},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6190,\"start\":6186},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6335,\"start\":6332},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6543,\"start\":6539},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6546,\"start\":6543},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6593,\"start\":6589},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6920,\"start\":6916},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6923,\"start\":6920},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":6926,\"start\":6923},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6929,\"start\":6926},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6932,\"start\":6929},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7214,\"start\":7210},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7217,\"start\":7214},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7220,\"start\":7217},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7223,\"start\":7220},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7226,\"start\":7223},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7395,\"start\":7391},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7449,\"start\":7446},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7468,\"start\":7465},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7533,\"start\":7529},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7536,\"start\":7533},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7538,\"start\":7536},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7557,\"start\":7553},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7560,\"start\":7557},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7730,\"start\":7726},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7733,\"start\":7730},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7736,\"start\":7733},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":7739,\"start\":7736},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":7764,\"start\":7760},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7767,\"start\":7764},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7770,\"start\":7767},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":7773,\"start\":7770},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7776,\"start\":7773},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7779,\"start\":7776},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7782,\"start\":7779},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":8031,\"start\":8027},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8078,\"start\":8075},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8124,\"start\":8120},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8195,\"start\":8191},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8611,\"start\":8607},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":8635,\"start\":8631},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8637,\"start\":8635},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8640,\"start\":8637},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8773,\"start\":8769},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8776,\"start\":8773},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8814,\"start\":8811},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9113,\"start\":9109},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9168,\"start\":9164},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9187,\"start\":9183},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9190,\"start\":9187},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":9628,\"start\":9624},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9715,\"start\":9711},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":9775,\"start\":9771},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9778,\"start\":9775},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":10012,\"start\":10008},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10015,\"start\":10012},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10336,\"start\":10332},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10339,\"start\":10336},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10381,\"start\":10377},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10384,\"start\":10381},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10387,\"start\":10384},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10390,\"start\":10387},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10393,\"start\":10390},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10654,\"start\":10650},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10777,\"start\":10773},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10780,\"start\":10777},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":10783,\"start\":10780},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10786,\"start\":10783},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":10819,\"start\":10815},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10822,\"start\":10819},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":10825,\"start\":10822},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10828,\"start\":10825},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10831,\"start\":10828},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":10834,\"start\":10831},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":10837,\"start\":10834},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":10840,\"start\":10837},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11292,\"start\":11289},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11315,\"start\":11311},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11337,\"start\":11333},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11786,\"start\":11782},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11788,\"start\":11786},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11791,\"start\":11788},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11899,\"start\":11895},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11958,\"start\":11954},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12006,\"start\":12002},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12277,\"start\":12273},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13759,\"start\":13755},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15282,\"start\":15278},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15343,\"start\":15339},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":15794,\"start\":15790},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15910,\"start\":15906},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16182,\"start\":16178},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":22992,\"start\":22988},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23133,\"start\":23130},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25514,\"start\":25510},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26739,\"start\":26736},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27165,\"start\":27161},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":27372,\"start\":27368},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27416,\"start\":27412},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":27533,\"start\":27529},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":27831,\"start\":27827},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27937,\"start\":27933},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":28074,\"start\":28070},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":28554,\"start\":28550},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28621,\"start\":28617}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32314,\"start\":31789},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32547,\"start\":32315},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32612,\"start\":32548},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34756,\"start\":32613}]", "paragraph": "[{\"end\":2392,\"start\":1479},{\"end\":2833,\"start\":2394},{\"end\":3276,\"start\":2835},{\"end\":4234,\"start\":3278},{\"end\":5383,\"start\":4236},{\"end\":5706,\"start\":5385},{\"end\":6022,\"start\":5708},{\"end\":6389,\"start\":6024},{\"end\":6424,\"start\":6406},{\"end\":7066,\"start\":6426},{\"end\":8472,\"start\":7101},{\"end\":10179,\"start\":8474},{\"end\":11076,\"start\":10181},{\"end\":11775,\"start\":11078},{\"end\":12142,\"start\":11777},{\"end\":13085,\"start\":12155},{\"end\":13932,\"start\":13087},{\"end\":14380,\"start\":13934},{\"end\":14795,\"start\":14382},{\"end\":15200,\"start\":14797},{\"end\":15402,\"start\":15228},{\"end\":15570,\"start\":15404},{\"end\":16123,\"start\":15633},{\"end\":16297,\"start\":16144},{\"end\":16416,\"start\":16299},{\"end\":16692,\"start\":16444},{\"end\":16980,\"start\":16748},{\"end\":17177,\"start\":17062},{\"end\":17255,\"start\":17211},{\"end\":17696,\"start\":17350},{\"end\":18047,\"start\":17718},{\"end\":18725,\"start\":18049},{\"end\":19443,\"start\":18774},{\"end\":19896,\"start\":19445},{\"end\":20259,\"start\":19941},{\"end\":20527,\"start\":20293},{\"end\":20793,\"start\":20529},{\"end\":21444,\"start\":20828},{\"end\":21697,\"start\":21446},{\"end\":22122,\"start\":21767},{\"end\":22492,\"start\":22162},{\"end\":23525,\"start\":22519},{\"end\":24050,\"start\":23527},{\"end\":24385,\"start\":24052},{\"end\":24721,\"start\":24453},{\"end\":24956,\"start\":24768},{\"end\":25076,\"start\":24987},{\"end\":25159,\"start\":25108},{\"end\":25326,\"start\":25161},{\"end\":25480,\"start\":25364},{\"end\":25597,\"start\":25482},{\"end\":25698,\"start\":25647},{\"end\":26006,\"start\":25744},{\"end\":26089,\"start\":26008},{\"end\":26432,\"start\":26148},{\"end\":27518,\"start\":26530},{\"end\":27815,\"start\":27520},{\"end\":27925,\"start\":27817},{\"end\":28057,\"start\":27927},{\"end\":28128,\"start\":28059},{\"end\":30445,\"start\":28169},{\"end\":31445,\"start\":30461},{\"end\":31788,\"start\":31461}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15632,\"start\":15571},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16443,\"start\":16417},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16747,\"start\":16693},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17061,\"start\":16981},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17210,\"start\":17178},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17349,\"start\":17256},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20292,\"start\":20260},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21766,\"start\":21698},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22161,\"start\":22123},{\"attributes\":{\"id\":\"formula_9\"},\"end\":24452,\"start\":24386},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24767,\"start\":24722},{\"attributes\":{\"id\":\"formula_11\"},\"end\":24986,\"start\":24957},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25107,\"start\":25077},{\"attributes\":{\"id\":\"formula_13\"},\"end\":25363,\"start\":25327},{\"attributes\":{\"id\":\"formula_14\"},\"end\":25646,\"start\":25598},{\"attributes\":{\"id\":\"formula_15\"},\"end\":25743,\"start\":25699},{\"attributes\":{\"id\":\"formula_16\"},\"end\":26147,\"start\":26090},{\"attributes\":{\"id\":\"formula_17\"},\"end\":26485,\"start\":26433}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28451,\"start\":28444},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29737,\"start\":29730},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29778,\"start\":29771},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30143,\"start\":30136}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1477,\"start\":1465},{\"attributes\":{\"n\":\"2.\"},\"end\":6404,\"start\":6392},{\"end\":7099,\"start\":7069},{\"attributes\":{\"n\":\"3.\"},\"end\":12153,\"start\":12145},{\"attributes\":{\"n\":\"4.\"},\"end\":15209,\"start\":15203},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15226,\"start\":15212},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16142,\"start\":16126},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17716,\"start\":17699},{\"attributes\":{\"n\":\"4.4.\"},\"end\":18772,\"start\":18728},{\"attributes\":{\"n\":\"4.5.\"},\"end\":19939,\"start\":19899},{\"attributes\":{\"n\":\"4.6.\"},\"end\":20826,\"start\":20796},{\"attributes\":{\"n\":\"4.7.\"},\"end\":22517,\"start\":22495},{\"attributes\":{\"n\":\"5.\"},\"end\":26507,\"start\":26487},{\"attributes\":{\"n\":\"5.1.\"},\"end\":26528,\"start\":26510},{\"attributes\":{\"n\":\"5.2.\"},\"end\":28167,\"start\":28131},{\"attributes\":{\"n\":\"5.3.\"},\"end\":30459,\"start\":30448},{\"attributes\":{\"n\":\"6.\"},\"end\":31459,\"start\":31448},{\"end\":31800,\"start\":31790},{\"end\":32326,\"start\":32316},{\"end\":32559,\"start\":32549},{\"end\":32623,\"start\":32614}]", "table": "[{\"end\":34756,\"start\":33261}]", "figure_caption": "[{\"end\":32314,\"start\":31802},{\"end\":32547,\"start\":32328},{\"end\":32612,\"start\":32561},{\"end\":33261,\"start\":32625}]", "figure_ref": "[{\"end\":11369,\"start\":11361},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17944,\"start\":17934},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18723,\"start\":18713},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19666,\"start\":19655},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19894,\"start\":19884},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20665,\"start\":20655},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22461,\"start\":22452},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22491,\"start\":22483},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29750,\"start\":29742},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30329,\"start\":30321}]", "bib_author_first_name": "[{\"end\":34766,\"start\":34760},{\"end\":34780,\"start\":34776},{\"end\":34795,\"start\":34789},{\"end\":34972,\"start\":34965},{\"end\":34985,\"start\":34982},{\"end\":35004,\"start\":34998},{\"end\":35018,\"start\":35012},{\"end\":35040,\"start\":35032},{\"end\":35287,\"start\":35280},{\"end\":35302,\"start\":35297},{\"end\":35318,\"start\":35312},{\"end\":35337,\"start\":35331},{\"end\":35350,\"start\":35343},{\"end\":35367,\"start\":35358},{\"end\":35740,\"start\":35739},{\"end\":35742,\"start\":35741},{\"end\":35752,\"start\":35751},{\"end\":35761,\"start\":35760},{\"end\":35763,\"start\":35762},{\"end\":35774,\"start\":35773},{\"end\":35776,\"start\":35775},{\"end\":36113,\"start\":36106},{\"end\":36128,\"start\":36122},{\"end\":36139,\"start\":36135},{\"end\":36158,\"start\":36152},{\"end\":36419,\"start\":36411},{\"end\":36430,\"start\":36426},{\"end\":36439,\"start\":36437},{\"end\":36453,\"start\":36446},{\"end\":36462,\"start\":36458},{\"end\":36640,\"start\":36633},{\"end\":36651,\"start\":36647},{\"end\":36661,\"start\":36656},{\"end\":36671,\"start\":36668},{\"end\":36882,\"start\":36875},{\"end\":36896,\"start\":36889},{\"end\":36906,\"start\":36903},{\"end\":37124,\"start\":37117},{\"end\":37138,\"start\":37131},{\"end\":37150,\"start\":37145},{\"end\":37164,\"start\":37156},{\"end\":37176,\"start\":37173},{\"end\":37190,\"start\":37187},{\"end\":37440,\"start\":37434},{\"end\":37447,\"start\":37446},{\"end\":37462,\"start\":37455},{\"end\":37476,\"start\":37470},{\"end\":37490,\"start\":37484},{\"end\":37507,\"start\":37499},{\"end\":37781,\"start\":37779},{\"end\":37795,\"start\":37787},{\"end\":37808,\"start\":37803},{\"end\":37824,\"start\":37817},{\"end\":37833,\"start\":37830},{\"end\":37850,\"start\":37844},{\"end\":38197,\"start\":38192},{\"end\":38208,\"start\":38205},{\"end\":38461,\"start\":38456},{\"end\":38478,\"start\":38469},{\"end\":38491,\"start\":38488},{\"end\":38684,\"start\":38679},{\"end\":38699,\"start\":38692},{\"end\":38714,\"start\":38708},{\"end\":39000,\"start\":38995},{\"end\":39014,\"start\":39008},{\"end\":39029,\"start\":39023},{\"end\":39225,\"start\":39221},{\"end\":39238,\"start\":39230},{\"end\":39252,\"start\":39245},{\"end\":39561,\"start\":39553},{\"end\":39578,\"start\":39572},{\"end\":39821,\"start\":39814},{\"end\":39835,\"start\":39830},{\"end\":39854,\"start\":39847},{\"end\":39856,\"start\":39855},{\"end\":40082,\"start\":40075},{\"end\":40096,\"start\":40091},{\"end\":40100,\"start\":40097},{\"end\":40115,\"start\":40108},{\"end\":40131,\"start\":40124},{\"end\":40133,\"start\":40132},{\"end\":40392,\"start\":40387},{\"end\":40407,\"start\":40401},{\"end\":40416,\"start\":40412},{\"end\":40437,\"start\":40431},{\"end\":40705,\"start\":40697},{\"end\":40720,\"start\":40711},{\"end\":40730,\"start\":40725},{\"end\":40740,\"start\":40735},{\"end\":40754,\"start\":40746},{\"end\":41005,\"start\":40998},{\"end\":41018,\"start\":41013},{\"end\":41253,\"start\":41246},{\"end\":41265,\"start\":41261},{\"end\":41280,\"start\":41272},{\"end\":41295,\"start\":41287},{\"end\":41558,\"start\":41552},{\"end\":41571,\"start\":41566},{\"end\":41588,\"start\":41580},{\"end\":41603,\"start\":41595},{\"end\":41618,\"start\":41611},{\"end\":41825,\"start\":41817},{\"end\":41837,\"start\":41830},{\"end\":41851,\"start\":41844},{\"end\":41859,\"start\":41857},{\"end\":41862,\"start\":41860},{\"end\":42075,\"start\":42070},{\"end\":42086,\"start\":42084},{\"end\":42101,\"start\":42092},{\"end\":42331,\"start\":42324},{\"end\":42369,\"start\":42363},{\"end\":42384,\"start\":42380},{\"end\":42605,\"start\":42601},{\"end\":42620,\"start\":42616},{\"end\":42631,\"start\":42628},{\"end\":42641,\"start\":42638},{\"end\":42812,\"start\":42803},{\"end\":42825,\"start\":42818},{\"end\":42839,\"start\":42833},{\"end\":42849,\"start\":42846},{\"end\":42866,\"start\":42861},{\"end\":42884,\"start\":42874},{\"end\":43184,\"start\":43181},{\"end\":43201,\"start\":43192},{\"end\":43495,\"start\":43489},{\"end\":43508,\"start\":43502},{\"end\":43519,\"start\":43515},{\"end\":43535,\"start\":43527},{\"end\":43550,\"start\":43544},{\"end\":43839,\"start\":43833},{\"end\":43849,\"start\":43844},{\"end\":43862,\"start\":43858},{\"end\":43876,\"start\":43869},{\"end\":43891,\"start\":43885},{\"end\":44138,\"start\":44132},{\"end\":44146,\"start\":44143},{\"end\":44161,\"start\":44153},{\"end\":44176,\"start\":44168},{\"end\":44407,\"start\":44400},{\"end\":44416,\"start\":44412},{\"end\":44433,\"start\":44424},{\"end\":44447,\"start\":44440},{\"end\":44460,\"start\":44456},{\"end\":44472,\"start\":44470},{\"end\":44487,\"start\":44478},{\"end\":44763,\"start\":44756},{\"end\":44772,\"start\":44768},{\"end\":44980,\"start\":44976},{\"end\":44992,\"start\":44986},{\"end\":45003,\"start\":44997},{\"end\":45010,\"start\":45009},{\"end\":45025,\"start\":45022},{\"end\":45306,\"start\":45302},{\"end\":45319,\"start\":45312},{\"end\":45604,\"start\":45599},{\"end\":45617,\"start\":45610},{\"end\":45632,\"start\":45624},{\"end\":45641,\"start\":45638},{\"end\":45869,\"start\":45865},{\"end\":45882,\"start\":45875},{\"end\":45897,\"start\":45890},{\"end\":45913,\"start\":45908},{\"end\":45930,\"start\":45922},{\"end\":46242,\"start\":46234},{\"end\":46254,\"start\":46250},{\"end\":46266,\"start\":46260},{\"end\":46283,\"start\":46276},{\"end\":46299,\"start\":46293},{\"end\":46315,\"start\":46309},{\"end\":46335,\"start\":46329},{\"end\":46665,\"start\":46661},{\"end\":46677,\"start\":46671},{\"end\":46684,\"start\":46680},{\"end\":46686,\"start\":46685},{\"end\":46951,\"start\":46950},{\"end\":46972,\"start\":46971},{\"end\":46989,\"start\":46981},{\"end\":47165,\"start\":47160},{\"end\":47178,\"start\":47174},{\"end\":47196,\"start\":47191},{\"end\":47375,\"start\":47367},{\"end\":47390,\"start\":47383},{\"end\":47415,\"start\":47412},{\"end\":47788,\"start\":47780},{\"end\":47803,\"start\":47798},{\"end\":47821,\"start\":47816},{\"end\":48054,\"start\":48049},{\"end\":48075,\"start\":48067},{\"end\":48090,\"start\":48085},{\"end\":48105,\"start\":48100},{\"end\":48120,\"start\":48116},{\"end\":48141,\"start\":48138},{\"end\":48531,\"start\":48523},{\"end\":48542,\"start\":48536},{\"end\":48557,\"start\":48549},{\"end\":48569,\"start\":48563},{\"end\":48584,\"start\":48579},{\"end\":48866,\"start\":48862},{\"end\":48881,\"start\":48875},{\"end\":48898,\"start\":48892},{\"end\":48917,\"start\":48910},{\"end\":49185,\"start\":49181},{\"end\":49200,\"start\":49194},{\"end\":49215,\"start\":49209},{\"end\":49229,\"start\":49222},{\"end\":49509,\"start\":49503},{\"end\":49523,\"start\":49518},{\"end\":49538,\"start\":49533},{\"end\":49553,\"start\":49547},{\"end\":49565,\"start\":49559},{\"end\":49576,\"start\":49571},{\"end\":49593,\"start\":49584},{\"end\":49916,\"start\":49910},{\"end\":49938,\"start\":49929},{\"end\":50121,\"start\":50120},{\"end\":50143,\"start\":50132},{\"end\":50352,\"start\":50351},{\"end\":50366,\"start\":50361},{\"end\":50379,\"start\":50374},{\"end\":50395,\"start\":50389},{\"end\":50411,\"start\":50404},{\"end\":50731,\"start\":50729},{\"end\":50742,\"start\":50739},{\"end\":50752,\"start\":50747},{\"end\":50769,\"start\":50761},{\"end\":51015,\"start\":51014},{\"end\":51024,\"start\":51023},{\"end\":51037,\"start\":51036},{\"end\":51047,\"start\":51046},{\"end\":51058,\"start\":51057},{\"end\":51401,\"start\":51394},{\"end\":51411,\"start\":51408},{\"end\":51613,\"start\":51606},{\"end\":51623,\"start\":51620},{\"end\":51824,\"start\":51816},{\"end\":51845,\"start\":51837},{\"end\":51857,\"start\":51852},{\"end\":51873,\"start\":51865},{\"end\":51885,\"start\":51881},{\"end\":51897,\"start\":51891},{\"end\":51917,\"start\":51911},{\"end\":52173,\"start\":52167},{\"end\":52190,\"start\":52184},{\"end\":52207,\"start\":52199},{\"end\":52209,\"start\":52208},{\"end\":52222,\"start\":52218},{\"end\":52234,\"start\":52231},{\"end\":52252,\"start\":52245},{\"end\":52270,\"start\":52265},{\"end\":52284,\"start\":52278},{\"end\":52298,\"start\":52294},{\"end\":52311,\"start\":52307},{\"end\":52635,\"start\":52625},{\"end\":52661,\"start\":52654},{\"end\":52677,\"start\":52669},{\"end\":52691,\"start\":52686},{\"end\":52712,\"start\":52704},{\"end\":53090,\"start\":53082},{\"end\":53102,\"start\":53097},{\"end\":53118,\"start\":53110},{\"end\":53134,\"start\":53128},{\"end\":53390,\"start\":53387},{\"end\":53403,\"start\":53397},{\"end\":53418,\"start\":53411},{\"end\":53428,\"start\":53424},{\"end\":53651,\"start\":53642},{\"end\":53665,\"start\":53658},{\"end\":53678,\"start\":53671},{\"end\":53690,\"start\":53684},{\"end\":53702,\"start\":53696},{\"end\":53711,\"start\":53708},{\"end\":53724,\"start\":53719},{\"end\":53939,\"start\":53934},{\"end\":53955,\"start\":53948},{\"end\":53965,\"start\":53964},{\"end\":53982,\"start\":53975},{\"end\":54230,\"start\":54227},{\"end\":54239,\"start\":54236},{\"end\":54253,\"start\":54246},{\"end\":54265,\"start\":54258},{\"end\":54278,\"start\":54272},{\"end\":54292,\"start\":54285},{\"end\":54561,\"start\":54558},{\"end\":54571,\"start\":54568},{\"end\":54598,\"start\":54592},{\"end\":54894,\"start\":54891},{\"end\":54904,\"start\":54901},{\"end\":54916,\"start\":54911},{\"end\":54928,\"start\":54922},{\"end\":55265,\"start\":55262},{\"end\":55276,\"start\":55271},{\"end\":55288,\"start\":55282},{\"end\":55297,\"start\":55293},{\"end\":55308,\"start\":55304},{\"end\":55544,\"start\":55537},{\"end\":55558,\"start\":55550},{\"end\":55793,\"start\":55790},{\"end\":55811,\"start\":55805},{\"end\":55823,\"start\":55817},{\"end\":55835,\"start\":55831},{\"end\":55839,\"start\":55836},{\"end\":55849,\"start\":55846},{\"end\":56166,\"start\":56157},{\"end\":56177,\"start\":56173},{\"end\":56191,\"start\":56184},{\"end\":56216,\"start\":56211},{\"end\":56226,\"start\":56221},{\"end\":56239,\"start\":56236},{\"end\":56515,\"start\":56507},{\"end\":56530,\"start\":56522},{\"end\":56549,\"start\":56543},{\"end\":56734,\"start\":56727},{\"end\":56748,\"start\":56741},{\"end\":56760,\"start\":56756},{\"end\":56777,\"start\":56770},{\"end\":57014,\"start\":57007},{\"end\":57023,\"start\":57020},{\"end\":57036,\"start\":57028},{\"end\":57050,\"start\":57043},{\"end\":57066,\"start\":57058},{\"end\":57351,\"start\":57344},{\"end\":57362,\"start\":57357},{\"end\":57375,\"start\":57368}]", "bib_author_last_name": "[{\"end\":34774,\"start\":34767},{\"end\":34787,\"start\":34781},{\"end\":34801,\"start\":34796},{\"end\":34980,\"start\":34973},{\"end\":34996,\"start\":34986},{\"end\":35010,\"start\":35005},{\"end\":35030,\"start\":35019},{\"end\":35048,\"start\":35041},{\"end\":35295,\"start\":35288},{\"end\":35310,\"start\":35303},{\"end\":35329,\"start\":35319},{\"end\":35341,\"start\":35338},{\"end\":35356,\"start\":35351},{\"end\":35375,\"start\":35368},{\"end\":35749,\"start\":35743},{\"end\":35758,\"start\":35753},{\"end\":35771,\"start\":35764},{\"end\":35782,\"start\":35777},{\"end\":36120,\"start\":36114},{\"end\":36133,\"start\":36129},{\"end\":36150,\"start\":36140},{\"end\":36167,\"start\":36159},{\"end\":36424,\"start\":36420},{\"end\":36435,\"start\":36431},{\"end\":36444,\"start\":36440},{\"end\":36456,\"start\":36454},{\"end\":36466,\"start\":36463},{\"end\":36645,\"start\":36641},{\"end\":36654,\"start\":36652},{\"end\":36666,\"start\":36662},{\"end\":36676,\"start\":36672},{\"end\":36887,\"start\":36883},{\"end\":36901,\"start\":36897},{\"end\":36911,\"start\":36907},{\"end\":37129,\"start\":37125},{\"end\":37143,\"start\":37139},{\"end\":37154,\"start\":37151},{\"end\":37171,\"start\":37165},{\"end\":37185,\"start\":37177},{\"end\":37195,\"start\":37191},{\"end\":37444,\"start\":37441},{\"end\":37453,\"start\":37448},{\"end\":37468,\"start\":37463},{\"end\":37482,\"start\":37477},{\"end\":37497,\"start\":37491},{\"end\":37518,\"start\":37508},{\"end\":37527,\"start\":37520},{\"end\":37785,\"start\":37782},{\"end\":37801,\"start\":37796},{\"end\":37815,\"start\":37809},{\"end\":37828,\"start\":37825},{\"end\":37842,\"start\":37834},{\"end\":37860,\"start\":37851},{\"end\":38203,\"start\":38198},{\"end\":38215,\"start\":38209},{\"end\":38467,\"start\":38462},{\"end\":38486,\"start\":38479},{\"end\":38498,\"start\":38492},{\"end\":38690,\"start\":38685},{\"end\":38706,\"start\":38700},{\"end\":38722,\"start\":38715},{\"end\":39006,\"start\":39001},{\"end\":39021,\"start\":39015},{\"end\":39037,\"start\":39030},{\"end\":39228,\"start\":39226},{\"end\":39243,\"start\":39239},{\"end\":39257,\"start\":39253},{\"end\":39570,\"start\":39562},{\"end\":39588,\"start\":39579},{\"end\":39828,\"start\":39822},{\"end\":39845,\"start\":39836},{\"end\":39864,\"start\":39857},{\"end\":40089,\"start\":40083},{\"end\":40106,\"start\":40101},{\"end\":40122,\"start\":40116},{\"end\":40141,\"start\":40134},{\"end\":40399,\"start\":40393},{\"end\":40410,\"start\":40408},{\"end\":40429,\"start\":40417},{\"end\":40446,\"start\":40438},{\"end\":40709,\"start\":40706},{\"end\":40723,\"start\":40721},{\"end\":40733,\"start\":40731},{\"end\":40744,\"start\":40741},{\"end\":40759,\"start\":40755},{\"end\":41011,\"start\":41006},{\"end\":41023,\"start\":41019},{\"end\":41259,\"start\":41254},{\"end\":41270,\"start\":41266},{\"end\":41285,\"start\":41281},{\"end\":41301,\"start\":41296},{\"end\":41307,\"start\":41303},{\"end\":41564,\"start\":41559},{\"end\":41578,\"start\":41572},{\"end\":41593,\"start\":41589},{\"end\":41609,\"start\":41604},{\"end\":41624,\"start\":41619},{\"end\":41828,\"start\":41826},{\"end\":41842,\"start\":41838},{\"end\":41855,\"start\":41852},{\"end\":41868,\"start\":41863},{\"end\":42082,\"start\":42076},{\"end\":42090,\"start\":42087},{\"end\":42106,\"start\":42102},{\"end\":42361,\"start\":42332},{\"end\":42378,\"start\":42370},{\"end\":42389,\"start\":42385},{\"end\":42396,\"start\":42391},{\"end\":42614,\"start\":42606},{\"end\":42626,\"start\":42621},{\"end\":42636,\"start\":42632},{\"end\":42644,\"start\":42642},{\"end\":42816,\"start\":42813},{\"end\":42831,\"start\":42826},{\"end\":42844,\"start\":42840},{\"end\":42859,\"start\":42850},{\"end\":42872,\"start\":42867},{\"end\":42889,\"start\":42885},{\"end\":43190,\"start\":43185},{\"end\":43211,\"start\":43202},{\"end\":43500,\"start\":43496},{\"end\":43513,\"start\":43509},{\"end\":43525,\"start\":43520},{\"end\":43542,\"start\":43536},{\"end\":43556,\"start\":43551},{\"end\":43842,\"start\":43840},{\"end\":43856,\"start\":43850},{\"end\":43867,\"start\":43863},{\"end\":43883,\"start\":43877},{\"end\":43900,\"start\":43892},{\"end\":44141,\"start\":44139},{\"end\":44151,\"start\":44147},{\"end\":44166,\"start\":44162},{\"end\":44179,\"start\":44177},{\"end\":44410,\"start\":44408},{\"end\":44422,\"start\":44417},{\"end\":44438,\"start\":44434},{\"end\":44454,\"start\":44448},{\"end\":44468,\"start\":44461},{\"end\":44476,\"start\":44473},{\"end\":44495,\"start\":44488},{\"end\":44766,\"start\":44764},{\"end\":44780,\"start\":44773},{\"end\":44984,\"start\":44981},{\"end\":44995,\"start\":44993},{\"end\":45007,\"start\":45004},{\"end\":45020,\"start\":45011},{\"end\":45036,\"start\":45026},{\"end\":45043,\"start\":45038},{\"end\":45310,\"start\":45307},{\"end\":45328,\"start\":45320},{\"end\":45608,\"start\":45605},{\"end\":45622,\"start\":45618},{\"end\":45636,\"start\":45633},{\"end\":45646,\"start\":45642},{\"end\":45873,\"start\":45870},{\"end\":45888,\"start\":45883},{\"end\":45906,\"start\":45898},{\"end\":45920,\"start\":45914},{\"end\":45935,\"start\":45931},{\"end\":46248,\"start\":46243},{\"end\":46258,\"start\":46255},{\"end\":46274,\"start\":46267},{\"end\":46291,\"start\":46284},{\"end\":46307,\"start\":46300},{\"end\":46327,\"start\":46316},{\"end\":46340,\"start\":46336},{\"end\":46669,\"start\":46666},{\"end\":46693,\"start\":46687},{\"end\":46959,\"start\":46952},{\"end\":46969,\"start\":46961},{\"end\":46979,\"start\":46973},{\"end\":46999,\"start\":46990},{\"end\":47008,\"start\":47001},{\"end\":47172,\"start\":47166},{\"end\":47189,\"start\":47179},{\"end\":47203,\"start\":47197},{\"end\":47381,\"start\":47376},{\"end\":47410,\"start\":47391},{\"end\":47419,\"start\":47416},{\"end\":47429,\"start\":47421},{\"end\":47796,\"start\":47789},{\"end\":47814,\"start\":47804},{\"end\":47832,\"start\":47822},{\"end\":48065,\"start\":48055},{\"end\":48083,\"start\":48076},{\"end\":48098,\"start\":48091},{\"end\":48114,\"start\":48106},{\"end\":48136,\"start\":48121},{\"end\":48150,\"start\":48142},{\"end\":48534,\"start\":48532},{\"end\":48547,\"start\":48543},{\"end\":48561,\"start\":48558},{\"end\":48577,\"start\":48570},{\"end\":48588,\"start\":48585},{\"end\":48873,\"start\":48867},{\"end\":48890,\"start\":48882},{\"end\":48908,\"start\":48899},{\"end\":48924,\"start\":48918},{\"end\":49192,\"start\":49186},{\"end\":49207,\"start\":49201},{\"end\":49220,\"start\":49216},{\"end\":49236,\"start\":49230},{\"end\":49516,\"start\":49510},{\"end\":49531,\"start\":49524},{\"end\":49545,\"start\":49539},{\"end\":49557,\"start\":49554},{\"end\":49569,\"start\":49566},{\"end\":49582,\"start\":49577},{\"end\":49599,\"start\":49594},{\"end\":49927,\"start\":49917},{\"end\":49950,\"start\":49939},{\"end\":50130,\"start\":50122},{\"end\":50155,\"start\":50144},{\"end\":50162,\"start\":50157},{\"end\":50359,\"start\":50353},{\"end\":50372,\"start\":50367},{\"end\":50387,\"start\":50380},{\"end\":50402,\"start\":50396},{\"end\":50422,\"start\":50412},{\"end\":50432,\"start\":50424},{\"end\":50737,\"start\":50732},{\"end\":50745,\"start\":50743},{\"end\":50759,\"start\":50753},{\"end\":50774,\"start\":50770},{\"end\":51021,\"start\":51016},{\"end\":51034,\"start\":51025},{\"end\":51044,\"start\":51038},{\"end\":51055,\"start\":51048},{\"end\":51066,\"start\":51059},{\"end\":51406,\"start\":51402},{\"end\":51416,\"start\":51412},{\"end\":51618,\"start\":51614},{\"end\":51628,\"start\":51624},{\"end\":51835,\"start\":51825},{\"end\":51850,\"start\":51846},{\"end\":51863,\"start\":51858},{\"end\":51879,\"start\":51874},{\"end\":51889,\"start\":51886},{\"end\":51909,\"start\":51898},{\"end\":51922,\"start\":51918},{\"end\":52182,\"start\":52174},{\"end\":52197,\"start\":52191},{\"end\":52216,\"start\":52210},{\"end\":52229,\"start\":52223},{\"end\":52243,\"start\":52235},{\"end\":52263,\"start\":52253},{\"end\":52276,\"start\":52271},{\"end\":52292,\"start\":52285},{\"end\":52305,\"start\":52299},{\"end\":52322,\"start\":52312},{\"end\":52652,\"start\":52636},{\"end\":52667,\"start\":52662},{\"end\":52684,\"start\":52678},{\"end\":52702,\"start\":52692},{\"end\":52724,\"start\":52713},{\"end\":53095,\"start\":53091},{\"end\":53108,\"start\":53103},{\"end\":53126,\"start\":53119},{\"end\":53139,\"start\":53135},{\"end\":53395,\"start\":53391},{\"end\":53409,\"start\":53404},{\"end\":53422,\"start\":53419},{\"end\":53436,\"start\":53429},{\"end\":53656,\"start\":53652},{\"end\":53669,\"start\":53666},{\"end\":53682,\"start\":53679},{\"end\":53694,\"start\":53691},{\"end\":53706,\"start\":53703},{\"end\":53717,\"start\":53712},{\"end\":53734,\"start\":53725},{\"end\":53946,\"start\":53940},{\"end\":53962,\"start\":53956},{\"end\":53973,\"start\":53966},{\"end\":53990,\"start\":53983},{\"end\":54006,\"start\":53992},{\"end\":54234,\"start\":54231},{\"end\":54244,\"start\":54240},{\"end\":54256,\"start\":54254},{\"end\":54270,\"start\":54266},{\"end\":54283,\"start\":54279},{\"end\":54296,\"start\":54293},{\"end\":54566,\"start\":54562},{\"end\":54590,\"start\":54572},{\"end\":54603,\"start\":54599},{\"end\":54612,\"start\":54605},{\"end\":54899,\"start\":54895},{\"end\":54909,\"start\":54905},{\"end\":54920,\"start\":54917},{\"end\":54936,\"start\":54929},{\"end\":55269,\"start\":55266},{\"end\":55280,\"start\":55277},{\"end\":55291,\"start\":55289},{\"end\":55302,\"start\":55298},{\"end\":55313,\"start\":55309},{\"end\":55548,\"start\":55545},{\"end\":55562,\"start\":55559},{\"end\":55803,\"start\":55794},{\"end\":55815,\"start\":55812},{\"end\":55829,\"start\":55824},{\"end\":55844,\"start\":55840},{\"end\":55855,\"start\":55850},{\"end\":56171,\"start\":56167},{\"end\":56182,\"start\":56178},{\"end\":56209,\"start\":56192},{\"end\":56219,\"start\":56217},{\"end\":56234,\"start\":56227},{\"end\":56244,\"start\":56240},{\"end\":56520,\"start\":56516},{\"end\":56541,\"start\":56531},{\"end\":56554,\"start\":56550},{\"end\":56739,\"start\":56735},{\"end\":56754,\"start\":56749},{\"end\":56768,\"start\":56761},{\"end\":56782,\"start\":56778},{\"end\":57018,\"start\":57015},{\"end\":57026,\"start\":57024},{\"end\":57041,\"start\":57037},{\"end\":57056,\"start\":57051},{\"end\":57077,\"start\":57067},{\"end\":57355,\"start\":57352},{\"end\":57366,\"start\":57363},{\"end\":57381,\"start\":57376}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":34884,\"start\":34758},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4624670},\"end\":35244,\"start\":34886},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":215748287},\"end\":35612,\"start\":35246},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4637111},\"end\":35996,\"start\":35614},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":53437459},\"end\":36371,\"start\":35998},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1497291},\"end\":36589,\"start\":36373},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14395783},\"end\":36799,\"start\":36591},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":49416362},\"end\":37053,\"start\":36801},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":219617574},\"end\":37369,\"start\":37055},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7684883},\"end\":37714,\"start\":37371},{\"attributes\":{\"doi\":\"arXiv:1912.04250\",\"id\":\"b10\"},\"end\":38082,\"start\":37716},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":102496818},\"end\":38379,\"start\":38084},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2255738},\"end\":38653,\"start\":38381},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3299195},\"end\":38949,\"start\":38655},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14547347},\"end\":39155,\"start\":38951},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":46968214},\"end\":39463,\"start\":39157},{\"attributes\":{\"id\":\"b16\"},\"end\":39744,\"start\":39465},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206596513},\"end\":40016,\"start\":39746},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":46936649},\"end\":40294,\"start\":40018},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":131774103},\"end\":40626,\"start\":40296},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":52051348},\"end\":40924,\"start\":40628},{\"attributes\":{\"doi\":\"2017. 1\",\"id\":\"b21\",\"matched_paper_id\":206845618},\"end\":41195,\"start\":40926},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14055300},\"end\":41509,\"start\":41197},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4559916},\"end\":41771,\"start\":41511},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53073405},\"end\":41993,\"start\":41773},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12230426},\"end\":42268,\"start\":41995},{\"attributes\":{\"doi\":\"arXiv, 2020. 3\",\"id\":\"b26\"},\"end\":42558,\"start\":42270},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":208267850},\"end\":42758,\"start\":42560},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":51892815},\"end\":43052,\"start\":42760},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":11091110},\"end\":43418,\"start\":43054},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":207194152},\"end\":43774,\"start\":43420},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":221698979},\"end\":44058,\"start\":43776},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206853077},\"end\":44334,\"start\":44060},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":131775632},\"end\":44684,\"start\":44336},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4572038},\"end\":44905,\"start\":44686},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":57759271},\"end\":45215,\"start\":44907},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":8264664},\"end\":45513,\"start\":45217},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":15774646},\"end\":45828,\"start\":45515},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":216915079},\"end\":46128,\"start\":45830},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":206594275},\"end\":46580,\"start\":46130},{\"attributes\":{\"id\":\"b40\"},\"end\":46901,\"start\":46582},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":1336659},\"end\":47141,\"start\":46903},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":9224113},\"end\":47293,\"start\":47143},{\"attributes\":{\"doi\":\"arXiv:2001.02613\",\"id\":\"b43\"},\"end\":47641,\"start\":47295},{\"attributes\":{\"id\":\"b44\"},\"end\":48047,\"start\":47643},{\"attributes\":{\"doi\":\"arXiv:1704.00675\",\"id\":\"b45\"},\"end\":48441,\"start\":48049},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4797810},\"end\":48763,\"start\":48443},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":195776274},\"end\":49119,\"start\":48765},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":6385934},\"end\":49383,\"start\":49121},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52936213},\"end\":49843,\"start\":49385},{\"attributes\":{\"id\":\"b50\"},\"end\":50086,\"start\":49845},{\"attributes\":{\"id\":\"b51\"},\"end\":50273,\"start\":50088},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":206590743},\"end\":50620,\"start\":50275},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":204956409},\"end\":50958,\"start\":50622},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":206942855},\"end\":51325,\"start\":50960},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":54482591},\"end\":51543,\"start\":51327},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":214667893},\"end\":51751,\"start\":51545},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":6159584},\"end\":52128,\"start\":51753},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":54115081},\"end\":52623,\"start\":52130},{\"attributes\":{\"doi\":\"arXiv:1704.07804\",\"id\":\"b59\"},\"end\":53009,\"start\":52625},{\"attributes\":{\"id\":\"b60\"},\"end\":53291,\"start\":53011},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":9114952},\"end\":53615,\"start\":53293},{\"attributes\":{\"id\":\"b62\"},\"end\":53893,\"start\":53617},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":202676783},\"end\":54146,\"start\":53895},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":102351155},\"end\":54476,\"start\":54148},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":211677466},\"end\":54782,\"start\":54478},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":6761996},\"end\":55201,\"start\":54784},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":4712004},\"end\":55459,\"start\":55203},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":3714620},\"end\":55694,\"start\":55461},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":214795169},\"end\":56049,\"start\":55696},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":4578162},\"end\":56469,\"start\":56051},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":51929808},\"end\":56667,\"start\":56471},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":11977588},\"end\":56930,\"start\":56669},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":220665428},\"end\":57258,\"start\":56932},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":52158437},\"end\":57538,\"start\":57260}]", "bib_title": "[{\"end\":34963,\"start\":34886},{\"end\":35278,\"start\":35246},{\"end\":35737,\"start\":35614},{\"end\":36104,\"start\":35998},{\"end\":36409,\"start\":36373},{\"end\":36631,\"start\":36591},{\"end\":36873,\"start\":36801},{\"end\":37115,\"start\":37055},{\"end\":37432,\"start\":37371},{\"end\":38190,\"start\":38084},{\"end\":38454,\"start\":38381},{\"end\":38677,\"start\":38655},{\"end\":38993,\"start\":38951},{\"end\":39219,\"start\":39157},{\"end\":39812,\"start\":39746},{\"end\":40073,\"start\":40018},{\"end\":40385,\"start\":40296},{\"end\":40695,\"start\":40628},{\"end\":40996,\"start\":40926},{\"end\":41244,\"start\":41197},{\"end\":41550,\"start\":41511},{\"end\":41815,\"start\":41773},{\"end\":42068,\"start\":41995},{\"end\":42599,\"start\":42560},{\"end\":42801,\"start\":42760},{\"end\":43179,\"start\":43054},{\"end\":43487,\"start\":43420},{\"end\":43831,\"start\":43776},{\"end\":44130,\"start\":44060},{\"end\":44398,\"start\":44336},{\"end\":44754,\"start\":44686},{\"end\":44974,\"start\":44907},{\"end\":45300,\"start\":45217},{\"end\":45597,\"start\":45515},{\"end\":45863,\"start\":45830},{\"end\":46232,\"start\":46130},{\"end\":46659,\"start\":46582},{\"end\":46948,\"start\":46903},{\"end\":47158,\"start\":47143},{\"end\":47778,\"start\":47643},{\"end\":48521,\"start\":48443},{\"end\":48860,\"start\":48765},{\"end\":49179,\"start\":49121},{\"end\":49501,\"start\":49385},{\"end\":50118,\"start\":50088},{\"end\":50349,\"start\":50275},{\"end\":50727,\"start\":50622},{\"end\":51012,\"start\":50960},{\"end\":51392,\"start\":51327},{\"end\":51604,\"start\":51545},{\"end\":51814,\"start\":51753},{\"end\":52165,\"start\":52130},{\"end\":53385,\"start\":53293},{\"end\":53640,\"start\":53617},{\"end\":53932,\"start\":53895},{\"end\":54225,\"start\":54148},{\"end\":54556,\"start\":54478},{\"end\":54889,\"start\":54784},{\"end\":55260,\"start\":55203},{\"end\":55535,\"start\":55461},{\"end\":55788,\"start\":55696},{\"end\":56155,\"start\":56051},{\"end\":56505,\"start\":56471},{\"end\":56725,\"start\":56669},{\"end\":57005,\"start\":56932},{\"end\":57342,\"start\":57260}]", "bib_author": "[{\"end\":34776,\"start\":34760},{\"end\":34789,\"start\":34776},{\"end\":34803,\"start\":34789},{\"end\":34982,\"start\":34965},{\"end\":34998,\"start\":34982},{\"end\":35012,\"start\":34998},{\"end\":35032,\"start\":35012},{\"end\":35050,\"start\":35032},{\"end\":35297,\"start\":35280},{\"end\":35312,\"start\":35297},{\"end\":35331,\"start\":35312},{\"end\":35343,\"start\":35331},{\"end\":35358,\"start\":35343},{\"end\":35377,\"start\":35358},{\"end\":35751,\"start\":35739},{\"end\":35760,\"start\":35751},{\"end\":35773,\"start\":35760},{\"end\":35784,\"start\":35773},{\"end\":36122,\"start\":36106},{\"end\":36135,\"start\":36122},{\"end\":36152,\"start\":36135},{\"end\":36169,\"start\":36152},{\"end\":36426,\"start\":36411},{\"end\":36437,\"start\":36426},{\"end\":36446,\"start\":36437},{\"end\":36458,\"start\":36446},{\"end\":36468,\"start\":36458},{\"end\":36647,\"start\":36633},{\"end\":36656,\"start\":36647},{\"end\":36668,\"start\":36656},{\"end\":36678,\"start\":36668},{\"end\":36889,\"start\":36875},{\"end\":36903,\"start\":36889},{\"end\":36913,\"start\":36903},{\"end\":37131,\"start\":37117},{\"end\":37145,\"start\":37131},{\"end\":37156,\"start\":37145},{\"end\":37173,\"start\":37156},{\"end\":37187,\"start\":37173},{\"end\":37197,\"start\":37187},{\"end\":37446,\"start\":37434},{\"end\":37455,\"start\":37446},{\"end\":37470,\"start\":37455},{\"end\":37484,\"start\":37470},{\"end\":37499,\"start\":37484},{\"end\":37520,\"start\":37499},{\"end\":37529,\"start\":37520},{\"end\":37787,\"start\":37779},{\"end\":37803,\"start\":37787},{\"end\":37817,\"start\":37803},{\"end\":37830,\"start\":37817},{\"end\":37844,\"start\":37830},{\"end\":37862,\"start\":37844},{\"end\":38205,\"start\":38192},{\"end\":38217,\"start\":38205},{\"end\":38469,\"start\":38456},{\"end\":38488,\"start\":38469},{\"end\":38500,\"start\":38488},{\"end\":38692,\"start\":38679},{\"end\":38708,\"start\":38692},{\"end\":38724,\"start\":38708},{\"end\":39008,\"start\":38995},{\"end\":39023,\"start\":39008},{\"end\":39039,\"start\":39023},{\"end\":39230,\"start\":39221},{\"end\":39245,\"start\":39230},{\"end\":39259,\"start\":39245},{\"end\":39572,\"start\":39553},{\"end\":39590,\"start\":39572},{\"end\":39830,\"start\":39814},{\"end\":39847,\"start\":39830},{\"end\":39866,\"start\":39847},{\"end\":40091,\"start\":40075},{\"end\":40108,\"start\":40091},{\"end\":40124,\"start\":40108},{\"end\":40143,\"start\":40124},{\"end\":40401,\"start\":40387},{\"end\":40412,\"start\":40401},{\"end\":40431,\"start\":40412},{\"end\":40448,\"start\":40431},{\"end\":40711,\"start\":40697},{\"end\":40725,\"start\":40711},{\"end\":40735,\"start\":40725},{\"end\":40746,\"start\":40735},{\"end\":40761,\"start\":40746},{\"end\":41013,\"start\":40998},{\"end\":41025,\"start\":41013},{\"end\":41261,\"start\":41246},{\"end\":41272,\"start\":41261},{\"end\":41287,\"start\":41272},{\"end\":41303,\"start\":41287},{\"end\":41309,\"start\":41303},{\"end\":41566,\"start\":41552},{\"end\":41580,\"start\":41566},{\"end\":41595,\"start\":41580},{\"end\":41611,\"start\":41595},{\"end\":41626,\"start\":41611},{\"end\":41830,\"start\":41817},{\"end\":41844,\"start\":41830},{\"end\":41857,\"start\":41844},{\"end\":41870,\"start\":41857},{\"end\":42084,\"start\":42070},{\"end\":42092,\"start\":42084},{\"end\":42108,\"start\":42092},{\"end\":42363,\"start\":42324},{\"end\":42380,\"start\":42363},{\"end\":42391,\"start\":42380},{\"end\":42398,\"start\":42391},{\"end\":42616,\"start\":42601},{\"end\":42628,\"start\":42616},{\"end\":42638,\"start\":42628},{\"end\":42646,\"start\":42638},{\"end\":42818,\"start\":42803},{\"end\":42833,\"start\":42818},{\"end\":42846,\"start\":42833},{\"end\":42861,\"start\":42846},{\"end\":42874,\"start\":42861},{\"end\":42891,\"start\":42874},{\"end\":43192,\"start\":43181},{\"end\":43213,\"start\":43192},{\"end\":43502,\"start\":43489},{\"end\":43515,\"start\":43502},{\"end\":43527,\"start\":43515},{\"end\":43544,\"start\":43527},{\"end\":43558,\"start\":43544},{\"end\":43844,\"start\":43833},{\"end\":43858,\"start\":43844},{\"end\":43869,\"start\":43858},{\"end\":43885,\"start\":43869},{\"end\":43902,\"start\":43885},{\"end\":44143,\"start\":44132},{\"end\":44153,\"start\":44143},{\"end\":44168,\"start\":44153},{\"end\":44181,\"start\":44168},{\"end\":44412,\"start\":44400},{\"end\":44424,\"start\":44412},{\"end\":44440,\"start\":44424},{\"end\":44456,\"start\":44440},{\"end\":44470,\"start\":44456},{\"end\":44478,\"start\":44470},{\"end\":44497,\"start\":44478},{\"end\":44768,\"start\":44756},{\"end\":44782,\"start\":44768},{\"end\":44986,\"start\":44976},{\"end\":44997,\"start\":44986},{\"end\":45009,\"start\":44997},{\"end\":45022,\"start\":45009},{\"end\":45038,\"start\":45022},{\"end\":45045,\"start\":45038},{\"end\":45312,\"start\":45302},{\"end\":45330,\"start\":45312},{\"end\":45610,\"start\":45599},{\"end\":45624,\"start\":45610},{\"end\":45638,\"start\":45624},{\"end\":45648,\"start\":45638},{\"end\":45875,\"start\":45865},{\"end\":45890,\"start\":45875},{\"end\":45908,\"start\":45890},{\"end\":45922,\"start\":45908},{\"end\":45937,\"start\":45922},{\"end\":46250,\"start\":46234},{\"end\":46260,\"start\":46250},{\"end\":46276,\"start\":46260},{\"end\":46293,\"start\":46276},{\"end\":46309,\"start\":46293},{\"end\":46329,\"start\":46309},{\"end\":46342,\"start\":46329},{\"end\":46671,\"start\":46661},{\"end\":46680,\"start\":46671},{\"end\":46695,\"start\":46680},{\"end\":46961,\"start\":46950},{\"end\":46971,\"start\":46961},{\"end\":46981,\"start\":46971},{\"end\":47001,\"start\":46981},{\"end\":47010,\"start\":47001},{\"end\":47174,\"start\":47160},{\"end\":47191,\"start\":47174},{\"end\":47205,\"start\":47191},{\"end\":47383,\"start\":47367},{\"end\":47412,\"start\":47383},{\"end\":47421,\"start\":47412},{\"end\":47431,\"start\":47421},{\"end\":47798,\"start\":47780},{\"end\":47816,\"start\":47798},{\"end\":47834,\"start\":47816},{\"end\":48067,\"start\":48049},{\"end\":48085,\"start\":48067},{\"end\":48100,\"start\":48085},{\"end\":48116,\"start\":48100},{\"end\":48138,\"start\":48116},{\"end\":48152,\"start\":48138},{\"end\":48536,\"start\":48523},{\"end\":48549,\"start\":48536},{\"end\":48563,\"start\":48549},{\"end\":48579,\"start\":48563},{\"end\":48590,\"start\":48579},{\"end\":48875,\"start\":48862},{\"end\":48892,\"start\":48875},{\"end\":48910,\"start\":48892},{\"end\":48926,\"start\":48910},{\"end\":49194,\"start\":49181},{\"end\":49209,\"start\":49194},{\"end\":49222,\"start\":49209},{\"end\":49238,\"start\":49222},{\"end\":49518,\"start\":49503},{\"end\":49533,\"start\":49518},{\"end\":49547,\"start\":49533},{\"end\":49559,\"start\":49547},{\"end\":49571,\"start\":49559},{\"end\":49584,\"start\":49571},{\"end\":49601,\"start\":49584},{\"end\":49929,\"start\":49910},{\"end\":49952,\"start\":49929},{\"end\":50132,\"start\":50120},{\"end\":50157,\"start\":50132},{\"end\":50164,\"start\":50157},{\"end\":50361,\"start\":50351},{\"end\":50374,\"start\":50361},{\"end\":50389,\"start\":50374},{\"end\":50404,\"start\":50389},{\"end\":50424,\"start\":50404},{\"end\":50434,\"start\":50424},{\"end\":50739,\"start\":50729},{\"end\":50747,\"start\":50739},{\"end\":50761,\"start\":50747},{\"end\":50776,\"start\":50761},{\"end\":51023,\"start\":51014},{\"end\":51036,\"start\":51023},{\"end\":51046,\"start\":51036},{\"end\":51057,\"start\":51046},{\"end\":51068,\"start\":51057},{\"end\":51408,\"start\":51394},{\"end\":51418,\"start\":51408},{\"end\":51620,\"start\":51606},{\"end\":51630,\"start\":51620},{\"end\":51837,\"start\":51816},{\"end\":51852,\"start\":51837},{\"end\":51865,\"start\":51852},{\"end\":51881,\"start\":51865},{\"end\":51891,\"start\":51881},{\"end\":51911,\"start\":51891},{\"end\":51924,\"start\":51911},{\"end\":52184,\"start\":52167},{\"end\":52199,\"start\":52184},{\"end\":52218,\"start\":52199},{\"end\":52231,\"start\":52218},{\"end\":52245,\"start\":52231},{\"end\":52265,\"start\":52245},{\"end\":52278,\"start\":52265},{\"end\":52294,\"start\":52278},{\"end\":52307,\"start\":52294},{\"end\":52324,\"start\":52307},{\"end\":52654,\"start\":52625},{\"end\":52669,\"start\":52654},{\"end\":52686,\"start\":52669},{\"end\":52704,\"start\":52686},{\"end\":52726,\"start\":52704},{\"end\":53097,\"start\":53082},{\"end\":53110,\"start\":53097},{\"end\":53128,\"start\":53110},{\"end\":53141,\"start\":53128},{\"end\":53397,\"start\":53387},{\"end\":53411,\"start\":53397},{\"end\":53424,\"start\":53411},{\"end\":53438,\"start\":53424},{\"end\":53658,\"start\":53642},{\"end\":53671,\"start\":53658},{\"end\":53684,\"start\":53671},{\"end\":53696,\"start\":53684},{\"end\":53708,\"start\":53696},{\"end\":53719,\"start\":53708},{\"end\":53736,\"start\":53719},{\"end\":53948,\"start\":53934},{\"end\":53964,\"start\":53948},{\"end\":53975,\"start\":53964},{\"end\":53992,\"start\":53975},{\"end\":54008,\"start\":53992},{\"end\":54236,\"start\":54227},{\"end\":54246,\"start\":54236},{\"end\":54258,\"start\":54246},{\"end\":54272,\"start\":54258},{\"end\":54285,\"start\":54272},{\"end\":54298,\"start\":54285},{\"end\":54568,\"start\":54558},{\"end\":54592,\"start\":54568},{\"end\":54605,\"start\":54592},{\"end\":54614,\"start\":54605},{\"end\":54901,\"start\":54891},{\"end\":54911,\"start\":54901},{\"end\":54922,\"start\":54911},{\"end\":54938,\"start\":54922},{\"end\":55271,\"start\":55262},{\"end\":55282,\"start\":55271},{\"end\":55293,\"start\":55282},{\"end\":55304,\"start\":55293},{\"end\":55315,\"start\":55304},{\"end\":55550,\"start\":55537},{\"end\":55564,\"start\":55550},{\"end\":55805,\"start\":55790},{\"end\":55817,\"start\":55805},{\"end\":55831,\"start\":55817},{\"end\":55846,\"start\":55831},{\"end\":55857,\"start\":55846},{\"end\":56173,\"start\":56157},{\"end\":56184,\"start\":56173},{\"end\":56211,\"start\":56184},{\"end\":56221,\"start\":56211},{\"end\":56236,\"start\":56221},{\"end\":56246,\"start\":56236},{\"end\":56522,\"start\":56507},{\"end\":56543,\"start\":56522},{\"end\":56556,\"start\":56543},{\"end\":56741,\"start\":56727},{\"end\":56756,\"start\":56741},{\"end\":56770,\"start\":56756},{\"end\":56784,\"start\":56770},{\"end\":57020,\"start\":57007},{\"end\":57028,\"start\":57020},{\"end\":57043,\"start\":57028},{\"end\":57058,\"start\":57043},{\"end\":57079,\"start\":57058},{\"end\":57357,\"start\":57344},{\"end\":57368,\"start\":57357},{\"end\":57383,\"start\":57368}]", "bib_venue": "[{\"end\":35057,\"start\":35050},{\"end\":35411,\"start\":35377},{\"end\":35788,\"start\":35784},{\"end\":36173,\"start\":36169},{\"end\":36472,\"start\":36468},{\"end\":36685,\"start\":36678},{\"end\":36917,\"start\":36913},{\"end\":37201,\"start\":37197},{\"end\":37533,\"start\":37529},{\"end\":37777,\"start\":37716},{\"end\":38221,\"start\":38217},{\"end\":38507,\"start\":38500},{\"end\":38786,\"start\":38724},{\"end\":39043,\"start\":39039},{\"end\":39263,\"start\":39259},{\"end\":39551,\"start\":39465},{\"end\":39870,\"start\":39866},{\"end\":40147,\"start\":40143},{\"end\":40452,\"start\":40448},{\"end\":40768,\"start\":40761},{\"end\":41052,\"start\":41032},{\"end\":41338,\"start\":41309},{\"end\":41633,\"start\":41626},{\"end\":41874,\"start\":41870},{\"end\":42113,\"start\":42108},{\"end\":42322,\"start\":42270},{\"end\":42650,\"start\":42646},{\"end\":42898,\"start\":42891},{\"end\":43228,\"start\":43213},{\"end\":43582,\"start\":43558},{\"end\":43906,\"start\":43902},{\"end\":44188,\"start\":44181},{\"end\":44501,\"start\":44497},{\"end\":44786,\"start\":44782},{\"end\":45049,\"start\":45045},{\"end\":45354,\"start\":45330},{\"end\":45653,\"start\":45648},{\"end\":45961,\"start\":45937},{\"end\":46346,\"start\":46342},{\"end\":46724,\"start\":46695},{\"end\":47014,\"start\":47010},{\"end\":47209,\"start\":47205},{\"end\":47365,\"start\":47295},{\"end\":47838,\"start\":47834},{\"end\":48221,\"start\":48168},{\"end\":48594,\"start\":48590},{\"end\":48931,\"start\":48926},{\"end\":49242,\"start\":49238},{\"end\":49605,\"start\":49601},{\"end\":49908,\"start\":49845},{\"end\":50168,\"start\":50164},{\"end\":50438,\"start\":50434},{\"end\":50780,\"start\":50776},{\"end\":51128,\"start\":51068},{\"end\":51422,\"start\":51418},{\"end\":51640,\"start\":51630},{\"end\":51928,\"start\":51924},{\"end\":52358,\"start\":52324},{\"end\":52793,\"start\":52742},{\"end\":53080,\"start\":53011},{\"end\":53445,\"start\":53438},{\"end\":53746,\"start\":53736},{\"end\":54012,\"start\":54008},{\"end\":54302,\"start\":54298},{\"end\":54618,\"start\":54614},{\"end\":54974,\"start\":54938},{\"end\":55322,\"start\":55315},{\"end\":55568,\"start\":55564},{\"end\":55861,\"start\":55857},{\"end\":56250,\"start\":56246},{\"end\":56560,\"start\":56556},{\"end\":56788,\"start\":56784},{\"end\":57083,\"start\":57079},{\"end\":57390,\"start\":57383}]"}}}, "year": 2023, "month": 12, "day": 17}