{"id": 261031087, "updated": "2023-10-04 21:20:38.943", "metadata": {"title": "StableVideo: Text-driven Consistency-aware Diffusion Video Editing", "authors": "[{\"first\":\"Wenhao\",\"last\":\"Chai\",\"middle\":[]},{\"first\":\"Xun\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Gaoang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yan\",\"last\":\"Lu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Diffusion-based methods can generate realistic images and videos, but they struggle to edit existing objects in a video while preserving their appearance over time. This prevents diffusion models from being applied to natural video editing in practical scenarios. In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects. Specifically, we develop a novel inter-frame propagation mechanism for diffusion video editing, which leverages the concept of layered representations to propagate the appearance information from one frame to the next. We then build up a text-driven video editing framework based on this mechanism, namely StableVideo, which can achieve consistency-aware video editing. Extensive experiments demonstrate the strong editing capability of our approach. Compared with state-of-the-art video editing methods, our approach shows superior qualitative and quantitative results. Our code is available at \\href{https://github.com/rese1f/StableVideo}{this https URL}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2308.09592", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2308-09592", "doi": "10.48550/arxiv.2308.09592"}}, "content": {"source": {"pdf_hash": "05cbac9a5101f47a6fabad72398616506572c9fa", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.09592v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c68ecf42005041c6102bb0dc9fe96bdd57d98b4e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/05cbac9a5101f47a6fabad72398616506572c9fa.txt", "contents": "\nStableVideo: Text-driven Consistency-aware Diffusion Video Editing\n\n\nWenhao Chai \nZhejiang University\n\n\nXun Guo \nMicrosoft Research Asia\n\n\nGaoang Wang gaoangwang@intl.zju.edu.cn \nZhejiang University\n\n\nYan Lu yanlu@microsoft.com \nMicrosoft Research Asia\n\n\nStableVideo: Text-driven Consistency-aware Diffusion Video Editing\n\nFigure 1: Editing results of StableVideo. The input video (top row) contains long range motion and viewpoint changing. Our approach performs stable editing according to the prompt of \"Orange SUV in sunny snow winter\" on foreground and background. In the edited video (bottom row), the \"orange SUV\" maintains high geometric and temporal consistency, although the viewpoints keep changing.AbstractDiffusion-based methods can generate realistic images and videos, but they struggle to edit existing objects in a video while preserving their appearance over time. This prevents diffusion models from being applied to natural video editing in practical scenarios. In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects. Specifically, we develop a novel inter-frame propagation mechanism for diffusion video editing, which leverages the concept of layered representations to propagate the appearance information from one frame to the next. We then build up a text-driven video editing framework based on this mechanism, namely StableVideo, which can achieve consistencyaware video editing. Extensive experiments demonstrate the strong editing capability of our approach. Compared with state-of-the-art video editing methods, our approach shows superior qualitative and quantitative results. Our code is available at this https URL.\n\nIntroduction\n\nRecent years have witnessed significant progress in extensive computer vision tasks taken by deep learning. Nev-* The work was done when the author was with MSRA as an intern. ertheless, natural video editing, which aims at manipulating the appearance of target objects and scenes, still faces two essential challenges that are deterministic to the editing quality: the generator equipped with rich prior knowledge that consistently produces high-fidelity edited contents adhering faithfully to the original geometry of the target objects, and the propagator that disseminates the edited contents throughout the entire video while keeping highly temporal consistency.\n\nThe flourish of text-driven generative diffusion models pre-trained on large-scale image and language data [34,14,53,16,41,5] provides impressive generation quality. Several diffusion-based methods achieve good performance in image editing [2,31], but few methods have tried to apply diffusion models in video editing, since it is challenging to modify existing objects while preserving their appearance over the entire video [49,12,26]. Dreamix [27] proposes a solution to generate consistent video according to input image/video and prompts. However, it focuses more on generating smooth motions, e.g., pose and camera movements, rather than maintaining geometric consistency of the objects across time. Moreover, such video diffusion models often suffer from huge computing complexity which is not friendly for practical applications.\n\nNeural layered atlas (NLA) [24,23] tries to tackle the temporal continuity problem by decomposing the video into a set of atlas layers, each of which describes one target object to be edited. For each atlas layer, the positions of the video are mapped into the corresponding 2D positions in it, so that semantically correspondent pixels over the whole video can be represented by the same atlas position. Instead of frame-by-frame editing, NLA edits atlas layers to ensure that the modifications can be precisely mapped back to video frames for temporal smoothness. Text2LIVE [1] provides a text-driven appearance manipulation solution of adding additional edit layers on atlases, in which a specific generator for the edit layers is trained. Although it achieves good results with strict structure preserved, it is not able to apply thorough editing. Moreover, the specifically trained generator also limits the richness of the generated contents.\n\nThis brings up the question: Could text-driven diffusion video editing achieve high temporal consistency? Intuitively, employing text-driven diffusion models to edit the atlases corresponding to the target objects could reach such goal. However, this gives rise to drawbacks rather than benefits. Being the summary of the whole video, atlases always have distorted appearance due to the viewpoint and camera movement, which are required to be specifically pretrained and generated as in [1]. Diffusion models may fail in generating satisfied atlas pixels in many cases, so that the corresponding edited frames will also be contaminated. To answer the question, we present two concepts for utilizing diffusion models in video editing. Firstly, instead of editing the atlases directly, we propose to update the atlases via editing key video frames. Secondly, we introduce temporal dependency constraints for diffusion models to generate objects with consistent appearance across time.\n\nBased on analysis above, we present a novel diffusion video editing approach, StableVideo, to perform consistency-aware video editing. In specific, we propose two effective technologies for this purpose. Firstly, to edit the objects with consistent appearance, we design an interframe propagation mechanism on top of the existing diffusion model [55], which can generate new objects with coherent geometry across time. Secondly, to achieve temporal consistency by leveraging NLA, we design an aggregation network to generate the edited atlases from the key frames. We then build up a text-driven diffusion-based framework, which provides high-quality natural video editing. We conduct extensive qualitative and quantitative experiments to demonstrate the capability of our approach. Compared with state-of-the-art methods, our approach achieves superior results with much lower complexity.\n\nIn summary, we present the following contributions:\n\n\u2022 To our best knowledge, we are the first to solve the consistency problem of diffusion video editing by considering the concept of layered atlas approaches, which provides an efficient and effective way for this topic.\n\n\u2022 We present a new video editing framework which can manipulate the appearance of the objects with high ge-ometry and appearance consistency across time. Our method can be easily applied to other text-driven diffusion models.\n\n\u2022 We conduct extensive experiments on a variety of natural videos, which shows superior editing performance compared with state-of-the-art methods.\n\n\nRelated Work\n\n\nDiffusion for Image Editing\n\nThe editing of natural images is an important task in the field of computer vision that has been widely studied. Prior to the emergence of diffusion models [42,15], many GANbased approaches [11,10,30,32,48] have achieved good results. There are also some works focus on low-level editing [52,6,51]. The advent of diffusion models has made it possible to achieve even higher quality and more diverse edited contents. SDEdit [26] adds noise and corruptions to an input image and uses diffusion models to reverse the process for image editing, while suffering from the loss of fidelity. Prompt-to-Prompt [12] and Plug-and-Play [45] perform semantic editing by mixing activations from original and target text prompts. InstructPix2Pix [2] applies semantic editing at test time and personalizes the model through finetuning and optimization to learn a special token describing the content. UniTune [46] and Imagic [21] finetune on a single image for better editability while maintaining good fidelity. There are also some works exploring the controllability [55,18,39,22,4,3] and personalization [9,38,29] of diffusion-based generation. Our proposed video editing method leverages existing image editing methods that can preserve the structures, such as [55,31,45] and [28].\n\n\nDiffusion for Video Editing\n\nCompared to image editing, video editing is more challenging for diffusion-based methods for geometric and temporal consistency. Tune-a-Video [49] inflates a text-toimage model for video editing. However, since temporal correlations are not fully considered, the editing results suffer from inconsistency of geometry and motion. Dreamix [27] develops a text-to-video backbone for motion editing while maintaining temporal consistency. There are also some works based on video generation like [8,40,13,54,56] and [17]. Unlike these approaches, our purpose is to enable diffusion models to perform appearance editing with both geometric and temporal consistency.\n\n\nTemporal Propagation in Video Editing\n\nTemporal propagation plays an important role in natural video process, since it is the essential factor for temporal consistency. Some methods rely on key frames [19,44,50] or optical flow [37] to propagate contents between frames. Inter-frame Propagation Figure 2: Framework of the proposed StableVideo. The input video is first fed into NLA [24] to generate foreground and background atlases using the pre-trained model. G b is the diffusion model used to edit background atlas, and G f is used to edit the foreground key frames. Note that G b and G f share the same weights, but accept different conditions. We employ depth information, extracted by MiDaS [35], for G b to maintain the consistency between the foreground motion and the environment, while structure guidance is used for G f to keep geometric consistency between the new generated foreground and the old one. After being edited, the foreground and background are blended together to reconstruct the edited frames.\n\nAnother bunch of methods are to achieve consistent interframe editing by forming a compressed representation of a video. Omnimattes [24,25] estimate RGBA layers for target subject and scene effects for each frame independently, but cannot achieve consistent propagation of contents along temporal direction. Atlas [1,20] tackles this problem by decomposing the video into unified 2D atlas layers for each target. This approach allows contents to be applied to the global summarized 2D atlases and mapped back to the video, achieving temporal consistency with minimal effort. Inspired by the concept of atlas approach, we employ the pre-trained neural layered atlas model to solve the inconsistency problem in diffusion video editing, thereby achieving high-quality editing results with temporal coherence. We utilize NLA [20] as the propagator for consistent video editing. Specifically, we conduct foreground and background editing separately. For foreground editing, we adopt key frame editing to generate atlas layers with high quality and the inter-frame propagation module to ensure better geometric and temporal consistency. The edited key frames are then mapped to partial atlases and aggregated by the aggregation network to produce the edited foreground atlas. It is noteworthy that our approach can also handle more than one foreground layers.\n\n\nMethod\n\n\nProblem Formulation\n\nWe employ the pre-trained NLA model [24] to propagate the edited contents to ensure that the target objects and scenes can maintain homogeneous appearances and motions across the entire video. The concept of NLA is to decompose the input video into layered representations, namely foreground atlas and background atlas, which globally summarize the correlated pixels for the foreground and the background, respectively. Three mapping networks, i.e., M b (\u00b7), M f (\u00b7) and M \u03b1 (\u00b7), are provided for this purpose. Given an input video I, for each frame I i , we obtain the mapping relationships of the atlas in the background and the foreground with respect to the pixel coordinate system, named as UV b (\u00b7) and UV f (\u00b7), as well as the foreground opacity \u03b1 i on the pixel coordinate system, formulated as:\nUV b i (\u00b7) = M b (I i ), UV f i (\u00b7) = M f (I i ), \u03b1 i = M \u03b1 (I i ). (1) \u2744 \u2744 \u2218 (\n\"an orange SUV\"\n\nText Prompt Structure Guidance Figure 3: Inter-frame propagation for foreground editing. We use two edited key frames, E i\u22121 and E i , to illustrate the process more clearly. The structure guidance and the text prompt is added into the denoising UNet via the concatenation and cross-attention mechanism respectively.\n\nAfter that, we formulate the mappings from the atlas representation of the background A b and the foreground A f , to the pixel coordinate systems of B i and F i :\nB i = UV b i (A b ), F i = UV f i (A f ).(2)\nOur method achieves geometrical consistent editing by fixing the mappings of UV b and UV f , and generating the edited atlases of A b and A f . We adopt a pre-trained latent diffusion model [36] with guided conditions as our generator, namely G b (\u00b7) and G f (\u00b7). Note that we are not simply editing the foreground and the background atlases directly. We apply inter-frame propagation mechanism on the editing process of foreground atlas. More details are explained in Sec. 3.2. After that, the entire video I can be reconstructed frame by frame as the following equation:\nI i = \u03b1 i \u2022 UV f i (G f (A f )) + (1 \u2212 \u03b1 i ) \u2022 UV b i (G b (A b )),(3)\nwhere \u2022 denotes pixel-wise product.\n\n\nInter-frame Propagation\n\nIn this section, we further elaborate on how inter-frame propagation mechanism helps consistent foreground editing. One of the major challenges for diffusion models is to generate video contents with temporal consistency. Existing state-of-the-art text-driven diffusion methods [55,28] can maintain the similar geometry between the target objects and the generated ones for image editing by adding structure conditions. However, the situation is different for  Figure 4: Training process of aggregation network. We employ a simple 3D network to aggregate the partial atlases generated from the edited key frames. For each training iteration, the sampled key frames are edited and mapped to partial atlases by U V f i . The partial atlases are then fed into the Aggregation Network to generate the edit atlas, which is mapped back by (U V f i ) \u22121 to generate the reconstructed frames. A L 1 loss is used to guarantee the aggregation consistency between the edited key frames and the reconstructed frames.\n\nvideos. Generating temporally consistent geometry needs to handle some uncertain changes across time, e.g., motion and deformation, which can not be supported by them. We tackle this problem by introducing a conditional denoising process to enable the diffusion models to consider both the structure of the current frame and the appearance information from previous frame, thereby sequentially generating new objects with geometric consistency across time. In specific, we employ canny edge as structure guidance, which is also adopted by existing diffusion methods [55]. Another important question is how to propagate the information of one object across frames to achieve consistent appearance. With the help of NLA, we can transfer the appearance features of the overlapping parts of previous frame to the next frame. Inspired by SDEdit [26] and ILVR [7], we further use a process of adding noise and denoising to obtain a more complete output. We illustrate this generation process in Fig. 3. The inter-frame propagation method is only applied on the foreground objects.\n\nSpecifically, we first select N foreground key frames from the original video I, ensuring that 1) there is significant overlap between the adjacent frames, and 2) these key frames capture the appearance of all faces of the object. Given a generator G f (\u00b7) and a text prompt T , we edit the first frame F 0 in pixel coordinate system with its structure condition C 0 as the extra guidance:\nE 0 = G f (T, C 0 ),(4)\nwhere E 0 represents the editing result. Then for the remaining key frames, we propagate the editing result from the previous key frame E i\u22121 to obtain the one of the current key frame E i . To be specific, we map E i\u22121 from the pixel Foreground: a car with graffiti; Background: Miami city.\n\nForeground: a polar bear; Background: north pole. Figure 5: Compositing editing. We demonstrate two editing examples with non-rigid and rigid foreground objects. Our approach can well preserve the geometry of \"bear\" and \"car\" across frames.\n\ncoordinate to atlas A E i\u22121 and then map it back to the pixel coordinate in the current frame i as\u00ca i with multiplying the opacity \u03b1 i , formulated by:\nA f i\u22121 = (UV f i\u22121 ) \u22121 (E i\u22121 ), E i = \u03b1 i \u2022 UV f i (A f i\u22121 ).(5)\nIt is noteworthy that the entire video shares the same foreground atlas A f for each target object, where the subscript i represents an incomplete partial atlas in frame i. We then use the atlas to propagate the pixel values from previous frame to their corresponding positions in the current frame to obtain an incomplete partial appearance\u00ca i . Given the partial appearance\u00ca i , we first encode it with VQ-VAE [47] to get the latent representation\u1e90 i , and then add noise to it with Variance Preserving Stochastic Differential Equation (VP-SDE), formulated as:\nZ i (t 0 ) = \u03b1(t 0 )\u1e90 t (0) + \u03c3(t 0 )z, z \u223c N (0, I) (6)\nwhere \u03c3(t 0 ) and \u03b1(t 0 ) are two scalar functions that satisfy \u03b1 2 (t) + \u03c3 2 (t) = 1, \u2200t \u2208 (0, 1], and t 0 \u2208 [0, 1] is a hyper parameter of the noise strength. Then we apply denoising process\u1e90 i (t 0 ) under the condition guidance from both text prompt T and structure guidance C i to get the latent representation Z i . Finally, we decode the latent representation to Z i propagate editing result E i . Our experiments further demonstrate that this mechanism can achieve good propagation results without training or fine-tuning the model.\n\n\nAggregation Network\n\nDifferent from [1] and [24], our approach edits video frames rather than atlases, which have the chance to achieve more information of different viewpoints. This brings two advantages. Firstly, the geometries and pixels from different viewpoints provide more details of the target objects, allowing the diffusion model to generate the edited content with higher fidelity. Secondly, this alleviates the risk of failure editing due to the potential wrong mapping from the atlas to the video frames. We then aggregate the edited key frames by using a simple yet effective two-layer 2D convolution network with skip connection as shown in Fig. 4. Our goal is to guarantee that the aggregated atlas is highly aligned with the original one, in terms of locations, so that appearance edit will not affect the geometric consistency and the temporal continuity. Reconstruction loss, L rec , between the edited and reconstructed key frames is employed in the training process as:\nL rec = N i=1 ||E i \u2212 UV f i (A f )|| 1 .(7)\nwhere N is the number of key frames.\n\n\nExperiments\n\n\nExperimental Settings\n\nIn practice, we implement our approach over Stable Diffusion [36]. Despite there are several image-based meth-Background: desert scene, pyramid.\n\nBackground: magma scene, crack road. ods [31,45,28] can perform structure-preserving editing, we choose the canny condition branch from [55] as the structure guidance for the proposed inter-frame propagation in our method. We apply our method on several videos from DAVIS [33], with each video containing a moving object in 50 \u223c 70 frames. The image resolution is set to 768 \u00d7 432, and the resolution of foreground atlas is set to 2000 \u00d7 2000. We employ DDIM [43] sampler with 20 steps. In this case, our method requires only \u223c10 GB GPU memory and takes \u223c30 seconds for each video in a single NVIDIA A40 GPU.\n\n\nEditing Results\n\nWe tested our method for various editing types among several videos. Here we demonstrate several scenarios: Compositing editing. Our method can edit foreground and background separately to achieve high-quality and semantically matched editing results as shown in Fig. 5. Background replacement. Due to the separate editing capabilities enabled by NLA, as well as the diverse and highquality editing brought by the diffusion model, our method achieves high-quality video background replacement while maintaining geometric consistency of depth and temporal continuity of perspective, as shown in Fig. 6. Style transfer. Our method accomplishes a wide range of style transfers, while simultaneously ensuring temporal consistency, as shown in Fig. 7. \n\n\nComparison to Prior Arts\n\nIn this section, we compare our editing results with stateof-the-art atlas-based method Text2LIVE [1] and diffusion-based method Tune-A-Video [49]. Comparison to Text2LIVE [1]. As shown in Fig. 8, given the text prompt \"an orange SUV\", Text2LIVE shows incomplete editing, while our method demonstrates a much more holistic result. It is because our method employs key frame editing, while Text2LIVE creates a new layer to edit the atlas somehow directly. The potential failure editing in atlas will lead to the error propagation to the entire video. In addition, compared to Text2LIVE, we can achieve higher quality and richer content with faster inference speed. Comparison to Tune-A-Video [49]. As shown in Fig. 9, vanilla video diffusion models like Tune-A-Video often fail in video editing. While it adeptly captures the semantic information of text prompts, it struggles to preserve consistency in video layout and object geometry. Consistency analysis. To the best of our knowledge, there is currently no widely accepted metric for evaluating the geometric and temporal consistency of videos. In this paper, we employ motion consistency of dense optical flow and deviation consistency of the edited video frames for this metric. For motion consistency, we employ the Farneback algorithm in OpenCV to calculate the average L2 distance of dense optical flow between the edited and original videos. We select the \"car-turn\" video from DAVIS and apply the same prompt to all methods. The experiment is repeated several times to obtain the average number. Our method shows better stability than Tune-A-Video as shown in Tab  deviation between adjacent frames, as shown in Tab. 2. Our method achieves comparable CLIP score and much lower deviations, which shows the effectiveness and stability.\n\n\nAblation Study\n\nTo verify the necessity of the key frame editing, we apply editing in atlas layer directly for the foreground as a simple baseline. The atlas might not be so deformed for human perception, but it significantly affects the diffusion models. Fig. 10 shows an example, where obvious deformation and\n\n\nTune-A-Video\n\nStableVideo (ours) original video Figure 9: Comparison to Tune-A-Video. Prompt: an orange SUV in sunny snowy winter, cabins. Our method achieves much more consistent editing results.\n\n\nEditing atlas\n\nEditing key frames Figure 10: Ablation study on directly editing the atlas. The deformation in atlas affects the diffusion models.\n\ninconsistency exist.\n\nWe also conduct extensive ablation study on inter-frame propagation module. The objective of this module is to random generation fix initial latent sequential generation\n\nInter-frame Propagation (ours) Figure 11: Ablation study on inter-frame propagation module. Foreground prompt: an orange SUV. Our method achieves excellent consistency in key frame appearance. maintain the geometry of the foreground when editing key frames. Firstly, we consider four different settings for editing key frames as shown in Fig. 11. Random generation. Each key frame only shares the same text prompt with the others. In this case, there are significant differences among the generated key frames. Fix initial latent. Unlike starting from random noise every time we edit, we start generating each key frame from the same latent noise and share the text prompt. In this case, there is higher similarity in the content generated for each frame, but the consistency is still not satisfactory. Sequential generation. Furthermore, we concatenate the latent noise between frames. Specifically, we apply imageto-image translation between frames. This method still cannot guarantee consistency since the appearances of the objects between the two frames do not match.\n\nInter-frame propagation (ours). Our final approach is to employ partial atlas to geometrically align the appearances between two frames, followed by a process of adding noise and then apply denoising process.\n\nIn addition, We also conducted experiments on the selection of the hyper parameter t 0 in this module as shown in  Prompt: an orange suv in the winter. Figure 13: Failure case. Videos with non-rigid deformation may lead to failure editing, since the movement of the object is more difficult to be well captured in this case. Fig. 12. We observe that as t 0 gradually increases, the generated results become more realistic but gradually lose their match with the appearance of the previous frame. Using binary search, we find out that a reasonable trade-off between fidelity and realism for t 0 lies around 0.8.\n\n\nLimitations and Future Works\n\nFirstly, our method is constrained by NLA. Learning atlas layers may fail for non-rigid objects with significant structural deformation as shown in Fig. 13. While we can mitigate this by dividing long videos into short clips where the objects can be considered to be rigid, it is still not feasible to address every single case. Secondly, our method is constrained by the capabilities of the diffusion models, which may struggle with specific scenarios such as human or animals. Besides, it may be better to optimize the diffusion model with the objective of aligning the generated contents to the reconstructed ones.\n\n\nConclusion\n\nWe have proposed a text-driven diffusion video editing approach. To solve the consistency problem for diffusion models in foreground object editing, we propose a interframe propagation mechanism and an atlas aggregation network. We conducted extensive experiments and demonstrated the superior qualitative and quantitative results of our method compared to state-of-the-art approaches.\n\n\nSupplement Material\n\n\nA. Implementation Details\n\nIn our experiments, we choose key frames for foreground editing by evenly sampling the input frames, i.e., every 20 frames. We train the aggregation network for 500 epochs with initial learning rate of 0.003 and momentum of 0.9. The network consists of two convolution layers with a ReLU in between, for which the training process is very fast. At inference stage, we conduct the training once for each edit. We set the lower and upper thresholds of Canny edges as 100 and 200 respectively, which can make the edges better represent the structure of the foreground. The numbers in Tab. 1 are the optical flow differences between the videos before and after editing (lower is better). We use cv2.calcOpticalFlowFarneback with default parameters. More detailed setting could be found in our code that will be released soon.\n\n\nB. Failure Cases\n\nSince our approach edits the key frames by using existing pre-trained diffusion models, some failure cases will occur due to the ineffective diffusion control. For example, our inter-frame propagation can well preserve the structure of the target objects across time, but cannot guarantee the quality of partial editing, as shown in Fig. A. This problem could be handled by using the masks provided by the users in practical applications, which would be our future work. As we discussed in the manuscript, NLA [24] may fail to build the foreground atlas due to the complex motion or occlusion. In this case, our editing will also fail. However, since our approach edits directly on key frames and generates corresponding partial atlases, such failure can be alleviated.\n\n\nC. Complexity Analysis\n\nSince inference is also an essential factor for video editing, we provide the comparison of our approach to exist-Figure A: An example of failure editing. Our method generates the edited contents by leveraging existing diffusion models [55,36]. In the case of partial editing, e.g., changing the color of the skirt, the diffusion models may generate the whole person instead.\n\n\nMethod\n\nVideo Training Edit Training Edit Inference Text2LIVE [1] \u223c 10 hr \u223c 1 hours \u223c 10 sec Tune-A-Video [49] \u223c -30 min \u223c 4 min StableVideo (ours) \u223c 10 hr -\u223c 30 sec ing state-of-the-art methods, i.e., Tune-A-Video [49] and Text2LIVE [1] as shown in Tab. A. Our approach only needs to perform lightweight training for atlas aggregation at inference stage, thereby being more efficient in pratical application compared to Text2LIVE and Tune-A-Video.\n\n\nD. More Editing Results\n\nWe provide more editing results to demonstrate the effectiveness of our approach. Fig. B shows the foreground editing for the video of \"boat\". We can see that the temporal consistency is well preserved. Fig. C shows the composite edit of our approach. Since the foreground and background are generated by the same diffusion model, they are highly semantically consistent. Besides, the geometry is also be well preserved across time.  \n\nFig. 2\n2shows the pipeline of our proposed StableVideo.\n\nFigure 6 :\n6Background replacement. Since our approach can effectively maintain the geometry of the foreground, it can perform background replacement while maintaining geometric consistency of depth and temporal continuity of perspective.\n\n. 1 .\n1For deviation consistency, we conduct experiments on 1) CLIP score: target text faithfulness 2) LPIPS-P: deviation from the original video frames and 3) LPIPS-T: Both: in the style of Vincent Willem van Gogh's Starry Night. Foreground: a panda; Both: in the style of Chinese painting.\n\nFigure 7 :Figure 8 :\n78Style transfer. Our approach achieves diverse video style transfer while ensuring high temporal consistency. Comparison to Text2LIVE. Foreground prompt: an orange SUV. Our method achieves more holistic editing results on the foreground.\n\nFigure 12 :\n12Binary search on hyper parameter t 0 in interframe propagation module. The appearance of previous frame is shown on the right side. The sequence on the left shows the generated results of the current frame as a function of t 0 . The value of t 0 around 0.8 is a good sweet spot.\n\nFigure B :\nBThe editing results of foreground. The ship in this video has relatively complex geometry. Our approach can well preserve the temporal consistency.\n\nFigure C :\nCThe results of composite editing. We separately edit the foreground and the background with semantically correlated prompts.\n\nTable 2 :\n2Quantitative results. Original video: car-turn.\n\nTable A :\nAThe inference speed of three methods. Video Training: training once for each video. Edit Training: training once for each edit. Edit Inference: inference time. The approximated cost time is tested under the video with 768 \u00d7 432 resolution and 70 frames in a single NVIDIA A40. For StableVideo, we pick three key frames for foreground editing.\n\nText2live: Text-driven layered image and video editing. Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, Tali Dekel, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringer812Proceedings, Part XVOmer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas- ten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV, pages 707-723. Springer, 2022. 2, 3, 5, 6, 8, 12\n\nTim Brooks, Aleksander Holynski, Alexei A Efros, arXiv:2211.09800-structpix2pix: Learning to follow image editing instructions. 1arXiv preprintTim Brooks, Aleksander Holynski, and Alexei A Efros. In- structpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. 1, 2\n\nImage reference-guided fashion design with structure-aware transfer by diffusion models. Shidong Cao, Wenhao Chai, Shengyu Hao, Gaoang Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionShidong Cao, Wenhao Chai, Shengyu Hao, and Gaoang Wang. Image reference-guided fashion design with structure-aware transfer by diffusion models. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3524-3528, 2023. 2\n\nDifffashion: Referencebased fashion design with structure-aware transfer by diffusion models. Shidong Cao, Wenhao Chai, Shengyu Hao, Yanting Zhang, Hangyue Chen, Gaoang Wang, arXiv:2302.06826arXiv preprintShidong Cao, Wenhao Chai, Shengyu Hao, Yanting Zhang, Hangyue Chen, and Gaoang Wang. Difffashion: Reference- based fashion design with structure-aware transfer by diffu- sion models. arXiv preprint arXiv:2302.06826, 2023. 2\n\nDeep vision multimodal learning: Methodology, benchmark, and trend. Wenhao Chai, Gaoang Wang, Applied Sciences. 1213Wenhao Chai and Gaoang Wang. Deep vision multimodal learning: Methodology, benchmark, and trend. Applied Sci- ences, 12(13):6588, 2022. 1\n\nSnowformer: Scale-aware transformer via context interaction for single image desnowing. Sixiang Chen, Tian Ye, Yun Liu, Erkang Chen, Jun Shi, Jingchun Zhou, arXiv:2208.09703arXiv preprintSixiang Chen, Tian Ye, Yun Liu, Erkang Chen, Jun Shi, and Jingchun Zhou. Snowformer: Scale-aware transformer via context interaction for single image desnowing. arXiv preprint arXiv:2208.09703, 2022. 2\n\nIlvr: Conditioning method for denoising diffusion probabilistic models. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon, arXiv:2108.02938arXiv preprintJooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938, 2021. 4\n\nStructure and content-guided video synthesis with diffusion models. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis, arXiv:2302.03011arXiv preprintPatrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011, 2023. 2\n\nOr Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-toimage generation using textual inversion. Rinon Gal, Yuval Alaluf, Yuval Atzmon, arXiv:2208.01618arXiv preprintRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash- nik, Amit H Bermano, Gal Chechik, and Daniel Cohen- Or. An image is worth one word: Personalizing text-to- image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2\n\nStylegan-nada: Clipguided domain adaptation of image generators. Rinon Gal, Or Patashnik, Haggai Maron, H Amit, Gal Bermano, Daniel Chechik, Cohen-Or, ACM Transactions on Graphics (TOG). 414Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip- guided domain adaptation of image generators. ACM Trans- actions on Graphics (TOG), 41(4):1-13, 2022. 2\n\n. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative adversarial networks. Communications of the ACM. 6311Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Commu- nications of the ACM, 63(11):139-144, 2020. 2\n\nPrompt-to-prompt image editing with cross attention control. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or, arXiv:2208.016261arXiv preprintAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im- age editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 1, 2\n\nImagen video: High definition video generation with diffusion models. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, P Diederik, Ben Kingma, Mohammad Poole, David J Norouzi, Fleet, arXiv:2210.02303arXiv preprintJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion mod- els. arXiv preprint arXiv:2210.02303, 2022. 2\n\nFleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, P Diederik, Ben Kingma, Mohammad Poole, Norouzi, J David, arXiv:210.02303arXiv preprintJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Sali- mans. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:210.02303, 2022. 1\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 33Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020. 2\n\nVideo diffusion models. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet, arXiv:2204.034582022arXiv preprintJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffu- sion models. arXiv preprint arXiv:2204.03458, 2022. 1\n\nVideo diffusion models. Jonathan Ho, Tim Salimans, A Alexey, William Gritsenko, Mohammad Chan, David J Norouzi, Fleet, Advances in Neural Information Processing Systems. Jonathan Ho, Tim Salimans, Alexey A Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffu- sion models. In Advances in Neural Information Processing Systems. 2\n\nLianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, Jingren Zhou, arXiv:2302.09778Composer: Creative and controllable image synthesis with composable conditions. arXiv preprintLianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable im- age synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. 2\n\nStylizing video by example. Ond\u0159ej Jamri\u0161ka, \u0160\u00e1rka Sochorov\u00e1, Ond\u0159ej Texler, Michal Luk\u00e1\u010d, Jakub Fi\u0161er, Jingwan Lu, Eli Shechtman, Daniel S\u1ef3kora, ACM Transactions on Graphics (TOG). 384Ond\u0159ej Jamri\u0161ka,\u0160\u00e1rka Sochorov\u00e1, Ond\u0159ej Texler, Michal Luk\u00e1\u010d, Jakub Fi\u0161er, Jingwan Lu, Eli Shechtman, and Daniel S\u1ef3kora. Stylizing video by example. ACM Transactions on Graphics (TOG), 38(4):1-11, 2019. 2\n\nLayered neural atlases for consistent video editing. Yoni Kasten, Dolev Ofri, Oliver Wang, Tali Dekel, ACM Transactions on Graphics (TOG). 406Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Lay- ered neural atlases for consistent video editing. ACM Trans- actions on Graphics (TOG), 40(6):1-12, 2021. 3\n\nImagic: Text-based real image editing with diffusion models. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani, arXiv:2210.09276arXiv preprintBahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276, 2022. 2\n\nGligen: Open-set grounded text-to-image generation. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, Yong Jae Lee, arXiv:2301.07093arXiv preprintYuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian- wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint arXiv:2301.07093, 2023. 2\n\nMinimal neural atlas: Parameterizing complex surfaces with minimal charts and distortion. Fei Weng, Gim Hee Low, Lee, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerProceedings, Part IIWeng Fei Low and Gim Hee Lee. Minimal neural atlas: Pa- rameterizing complex surfaces with minimal charts and dis- tortion. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceed- ings, Part II, pages 465-481. Springer, 2022. 1\n\nLayered neural rendering for retiming people in video. Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, T William, Michael Freeman, Rubinstein, arXiv:2009.07833512arXiv preprintErika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William T Freeman, and Michael Rubinstein. Layered neural rendering for retiming people in video. arXiv preprint arXiv:2009.07833, 2020. 1, 3, 5, 12\n\nAssociating objects and their effects in video through coordination games. Erika Lu, Forrester Cole, Weidi Xie, Tali Dekel, T William, Andrew Freeman, Michael Zisserman, Rubinstein, Advances in Neural Information Processing Systems. Erika Lu, Forrester Cole, Weidi Xie, Tali Dekel, William T Freeman, Andrew Zisserman, and Michael Rubinstein. As- sociating objects and their effects in video through coordina- tion games. In Advances in Neural Information Processing Systems. 3\n\nSdedit: Guided image synthesis and editing with stochastic differential equations. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon, International Conference on Learning Representations. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia- jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equa- tions. In International Conference on Learning Representa- tions, 2021. 1, 2, 4\n\nDreamix: Video diffusion models are general video editors. Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, Yedid Hoshen, arXiv:2302.013291arXiv preprintEyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023. 1, 2\n\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie, arXiv:2302.084536arXiv preprintChong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon- gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. 2, 4, 6\n\nSinfusion: Training diffusion models on a single image or video. Yaniv Nikankin, Niv Haim, Michal Irani, arXiv:2211.117432022arXiv preprintYaniv Nikankin, Niv Haim, and Michal Irani. Sinfusion: Training diffusion models on a single image or video. arXiv preprint arXiv:2211.11743, 2022. 2\n\nSemantic image synthesis with spatially-adaptive normalization. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive nor- malization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2337-2346, 2019. 2\n\nZero-shot image-to-image translation. Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, Jun-Yan Zhu, arXiv:2302.030276arXiv preprintGaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. arXiv preprint arXiv:2302.03027, 2023. 1, 2, 6\n\nStyleclip: Text-driven manipulation of stylegan imagery. Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 2085-2094, 2021. 2\n\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alex Sorkine-Hornung, Luc Van Gool, arXiv:1704.00675The 2017 davis challenge on video object segmentation. arXiv preprintJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar- bel\u00e1ez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 6\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLR, 2021. 1International conference on machine learning. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 1\n\nTowards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun, 2022. 3IEEE Transactions on Pattern Analysis and Machine Intelligence. 443Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, 44(3), 2022. 3\n\nHigh-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition412Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022. 4, 5, 12\n\nArtistic style transfer for videos. Manuel Ruder, Alexey Dosovitskiy, Thomas Brox, Pattern Recognition: 38th German Conference. Hannover, GermanySpringer38Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox. Artistic style transfer for videos. In Pattern Recognition: 38th German Conference, GCPR 2016, Hannover, Ger- many, September 12-15, 2016, Proceedings 38, pages 26-36. Springer, 2016. 2\n\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman, arXiv:2208.12242arXiv preprintNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. 2\n\nPalette: Image-to-image diffusion models. Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, Mohammad Norouzi, ACM SIGGRAPH 2022 Conference Proceedings. 2022Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1- 10, 2022. 2\n\n. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\n\nMake-a-video: Text-to-video generation without text-video data. Oran Gafni, arXiv:2209.147922022arXiv preprintOran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2\n\nMake-a-video: Text-to-video generation without text-video data. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman, arXiv:2209.14792arXiv preprintUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 1\n\nDenoising diffusion implicit models. Jiaming Song, Chenlin Meng, Stefano Ermon, International Conference on Learning Representations. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois- ing diffusion implicit models. In International Conference on Learning Representations. 2\n\n. Jiaming Song, Chenlin Meng, Stefano Ermon, arXiv:2010.02502Denoising diffusion implicit models. arXiv preprintJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 6\n\nInteractive video stylization using fewshot patch-based training. Ond\u0159ej Texler, David Futschik, Michal Ku\u010dera, Ond\u0159ej Jamri\u0161ka, \u0160\u00e1rka Sochorov\u00e1, Menclei Chai, Sergey Tulyakov, Daniel S\u1ef3kora, ACM Transactions on Graphics (TOG). 394Ond\u0159ej Texler, David Futschik, Michal Ku\u010dera, Ond\u0159ej Jamri\u0161ka,\u0160\u00e1rka Sochorov\u00e1, Menclei Chai, Sergey Tulyakov, and Daniel S\u1ef3kora. Interactive video stylization using few- shot patch-based training. ACM Transactions on Graphics (TOG), 39(4):73-1, 2020. 2\n\nPlug-and-play diffusion features for textdriven image-to-image translation. Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel, arXiv:2211.1257226arXiv preprintNarek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text- driven image-to-image translation. arXiv preprint arXiv:2211.12572, 2022. 2, 6\n\nUnitune: Text-driven image editing by fine tuning an image generation model on a single image. Dani Valevski, Matan Kalman, Yossi Matias, Yaniv Leviathan, arXiv:2210.09477arXiv preprintDani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. Unitune: Text-driven image editing by fine tuning an image generation model on a single image. arXiv preprint arXiv:2210.09477, 2022. 2\n\nNeural discrete representation learning. Advances in neural information processing systems. Aaron Van Den, Oriol Oord, Vinyals, 30Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information pro- cessing systems, 30, 2017. 5\n\nHigh-resolution image synthesis and semantic manipulation with conditional gans. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image syn- thesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8798-8807, 2018. 2\n\nTune-a-video: One-shot tuning of image diffusion models for text-to-video generation. Yixiao Jay Zhangjie Wu, Xintao Ge, Weixian Wang, Yuchao Lei, Wynne Gu, Ying Hsu, Xiaohu Shan, Mike Zheng Qie, Shou, arXiv:2212.11565812arXiv preprintJay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. 1, 2, 6, 8, 12\n\nTemporally consistent semantic video editing. Yiran Xu, Badour Albahar, Jia-Bin Huang, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerProceedings, Part XVYiran Xu, Badour AlBahar, and Jia-Bin Huang. Tempo- rally consistent semantic video editing. In Computer Vision- ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV, pages 357-374. Springer, 2022. 2\n\nAim 2022 challenge on super-resolution of compressed image and video: Dataset, methods and results. Ren Yang, Radu Timofte, Xin Li, Qi Zhang, Lin Zhang, Fanglong Liu, Dongliang He, Fu Li, He Zheng, Weihang Yuan, European Conference on Computer Vision. SpringerRen Yang, Radu Timofte, Xin Li, Qi Zhang, Lin Zhang, Fan- glong Liu, Dongliang He, Fu Li, He Zheng, Weihang Yuan, et al. Aim 2022 challenge on super-resolution of compressed image and video: Dataset, methods and results. In European Conference on Computer Vision, pages 174-202. Springer, 2022. 2\n\nPerceiving and modeling density for image dehazing. Tian Ye, Yunchen Zhang, Mingchao Jiang, Liang Chen, Yun Liu, Sixiang Chen, Erkang Chen, European Conference on Computer Vision. SpringerTian Ye, Yunchen Zhang, Mingchao Jiang, Liang Chen, Yun Liu, Sixiang Chen, and Erkang Chen. Perceiving and mod- eling density for image dehazing. In European Conference on Computer Vision, pages 130-145. Springer, 2022. 2\n\nLijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00e9 Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, Lu Jiang, arXiv:2212.05199Magvit: Masked generative video transformer. arXiv preprintLijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00e9 Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, Ming- Hsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. Magvit: Masked generative video transformer. arXiv preprint arXiv:2212.05199, 2022. 1\n\nVideo probabilistic diffusion models in projected latent space. Sihyun Yu, Kihyuk Sohn, Subin Kim, Jinwoo Shin, arXiv:2302.07685arXiv preprintSihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. arXiv preprint arXiv:2302.07685, 2023. 2\n\nAdding conditional control to text-to-image diffusion models. Lvmin Zhang, Maneesh Agrawala, arXiv:2302.05543612arXiv preprintLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. 2, 4, 6, 12\n\nMagicvideo: Efficient video generation with latent diffusion models. Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, Jiashi Feng, arXiv:2211.110182022arXiv preprintDaquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. 2\n", "annotations": {"author": "[{\"end\":104,\"start\":70},{\"end\":139,\"start\":105},{\"end\":201,\"start\":140},{\"end\":255,\"start\":202}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":77},{\"end\":112,\"start\":109},{\"end\":151,\"start\":147},{\"end\":208,\"start\":206}]", "author_first_name": "[{\"end\":76,\"start\":70},{\"end\":108,\"start\":105},{\"end\":146,\"start\":140},{\"end\":205,\"start\":202}]", "author_affiliation": "[{\"end\":103,\"start\":83},{\"end\":138,\"start\":114},{\"end\":200,\"start\":180},{\"end\":254,\"start\":230}]", "title": "[{\"end\":67,\"start\":1},{\"end\":322,\"start\":256}]", "venue": null, "abstract": "[{\"end\":1784,\"start\":324}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2580,\"start\":2576},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2583,\"start\":2580},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2586,\"start\":2583},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2589,\"start\":2586},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2592,\"start\":2589},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2594,\"start\":2592},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2712,\"start\":2709},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2715,\"start\":2712},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2899,\"start\":2895},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2902,\"start\":2899},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2905,\"start\":2902},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2919,\"start\":2915},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3339,\"start\":3335},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3342,\"start\":3339},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3887,\"start\":3884},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4748,\"start\":4745},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":5592,\"start\":5588},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6988,\"start\":6984},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6991,\"start\":6988},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7022,\"start\":7018},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7025,\"start\":7022},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7028,\"start\":7025},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7031,\"start\":7028},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7034,\"start\":7031},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7120,\"start\":7116},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7122,\"start\":7120},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7125,\"start\":7122},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7255,\"start\":7251},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7433,\"start\":7429},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7456,\"start\":7452},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7562,\"start\":7559},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7725,\"start\":7721},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7741,\"start\":7737},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7885,\"start\":7881},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7888,\"start\":7885},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7891,\"start\":7888},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7894,\"start\":7891},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7896,\"start\":7894},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7898,\"start\":7896},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7922,\"start\":7919},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7925,\"start\":7922},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7928,\"start\":7925},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8081,\"start\":8077},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8084,\"start\":8081},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8087,\"start\":8084},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8096,\"start\":8092},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8275,\"start\":8271},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8470,\"start\":8466},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8624,\"start\":8621},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8627,\"start\":8624},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8630,\"start\":8627},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8633,\"start\":8630},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8636,\"start\":8633},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8645,\"start\":8641},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8997,\"start\":8993},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9000,\"start\":8997},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9003,\"start\":9000},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9024,\"start\":9020},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9178,\"start\":9174},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9494,\"start\":9490},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9950,\"start\":9946},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9953,\"start\":9950},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10131,\"start\":10128},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10134,\"start\":10131},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10639,\"start\":10635},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11240,\"start\":11236},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12822,\"start\":12818},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13617,\"start\":13613},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13620,\"start\":13617},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":14911,\"start\":14907},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15185,\"start\":15181},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15198,\"start\":15195},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17003,\"start\":16999},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17789,\"start\":17786},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17798,\"start\":17794},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18927,\"start\":18923},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19053,\"start\":19049},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19056,\"start\":19053},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19059,\"start\":19056},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":19148,\"start\":19144},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19284,\"start\":19280},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19471,\"start\":19467},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20513,\"start\":20510},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20558,\"start\":20554},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20587,\"start\":20584},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21107,\"start\":21103},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27414,\"start\":27410},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":27936,\"start\":27932},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27939,\"start\":27936},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28139,\"start\":28136},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":28184,\"start\":28180},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":28293,\"start\":28289},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28311,\"start\":28308}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":29041,\"start\":28985},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29281,\"start\":29042},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29574,\"start\":29282},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29835,\"start\":29575},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30129,\"start\":29836},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30290,\"start\":30130},{\"attributes\":{\"id\":\"fig_7\"},\"end\":30428,\"start\":30291},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30488,\"start\":30429},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30843,\"start\":30489}]", "paragraph": "[{\"end\":2467,\"start\":1800},{\"end\":3306,\"start\":2469},{\"end\":4256,\"start\":3308},{\"end\":5240,\"start\":4258},{\"end\":6131,\"start\":5242},{\"end\":6184,\"start\":6133},{\"end\":6405,\"start\":6186},{\"end\":6632,\"start\":6407},{\"end\":6781,\"start\":6634},{\"end\":8097,\"start\":6828},{\"end\":8789,\"start\":8129},{\"end\":9812,\"start\":8831},{\"end\":11167,\"start\":9814},{\"end\":12003,\"start\":11200},{\"end\":12099,\"start\":12084},{\"end\":12417,\"start\":12101},{\"end\":12582,\"start\":12419},{\"end\":13200,\"start\":12628},{\"end\":13307,\"start\":13272},{\"end\":14339,\"start\":13335},{\"end\":15415,\"start\":14341},{\"end\":15806,\"start\":15417},{\"end\":16122,\"start\":15831},{\"end\":16364,\"start\":16124},{\"end\":16517,\"start\":16366},{\"end\":17149,\"start\":16587},{\"end\":17747,\"start\":17207},{\"end\":18740,\"start\":17771},{\"end\":18822,\"start\":18786},{\"end\":19006,\"start\":18862},{\"end\":19616,\"start\":19008},{\"end\":20383,\"start\":19636},{\"end\":22206,\"start\":20412},{\"end\":22520,\"start\":22225},{\"end\":22719,\"start\":22537},{\"end\":22867,\"start\":22737},{\"end\":22889,\"start\":22869},{\"end\":23060,\"start\":22891},{\"end\":24134,\"start\":23062},{\"end\":24344,\"start\":24136},{\"end\":24956,\"start\":24346},{\"end\":25606,\"start\":24989},{\"end\":26006,\"start\":25621},{\"end\":26879,\"start\":26058},{\"end\":27669,\"start\":26900},{\"end\":28071,\"start\":27696},{\"end\":28522,\"start\":28082},{\"end\":28984,\"start\":28550}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12083,\"start\":12004},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12627,\"start\":12583},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13271,\"start\":13201},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15830,\"start\":15807},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16586,\"start\":16518},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17206,\"start\":17150},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18785,\"start\":18741}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22036,\"start\":22033}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1798,\"start\":1786},{\"attributes\":{\"n\":\"2.\"},\"end\":6796,\"start\":6784},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6826,\"start\":6799},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8127,\"start\":8100},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8829,\"start\":8792},{\"attributes\":{\"n\":\"3.\"},\"end\":11176,\"start\":11170},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11198,\"start\":11179},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13333,\"start\":13310},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17769,\"start\":17750},{\"attributes\":{\"n\":\"4.\"},\"end\":18836,\"start\":18825},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18860,\"start\":18839},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19634,\"start\":19619},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20410,\"start\":20386},{\"attributes\":{\"n\":\"4.4.\"},\"end\":22223,\"start\":22209},{\"end\":22535,\"start\":22523},{\"end\":22735,\"start\":22722},{\"attributes\":{\"n\":\"5.\"},\"end\":24987,\"start\":24959},{\"attributes\":{\"n\":\"6.\"},\"end\":25619,\"start\":25609},{\"end\":26028,\"start\":26009},{\"end\":26056,\"start\":26031},{\"end\":26898,\"start\":26882},{\"end\":27694,\"start\":27672},{\"end\":28080,\"start\":28074},{\"end\":28548,\"start\":28525},{\"end\":28992,\"start\":28986},{\"end\":29053,\"start\":29043},{\"end\":29288,\"start\":29283},{\"end\":29596,\"start\":29576},{\"end\":29848,\"start\":29837},{\"end\":30141,\"start\":30131},{\"end\":30302,\"start\":30292},{\"end\":30439,\"start\":30430},{\"end\":30499,\"start\":30490}]", "table": null, "figure_caption": "[{\"end\":29041,\"start\":28994},{\"end\":29281,\"start\":29055},{\"end\":29574,\"start\":29290},{\"end\":29835,\"start\":29599},{\"end\":30129,\"start\":29851},{\"end\":30290,\"start\":30143},{\"end\":30428,\"start\":30304},{\"end\":30488,\"start\":30441},{\"end\":30843,\"start\":30501}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9095,\"start\":9087},{\"end\":12140,\"start\":12132},{\"end\":13804,\"start\":13796},{\"end\":15336,\"start\":15330},{\"end\":16182,\"start\":16174},{\"end\":18412,\"start\":18406},{\"end\":19905,\"start\":19899},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20236,\"start\":20230},{\"end\":20382,\"start\":20375},{\"end\":20607,\"start\":20601},{\"end\":21127,\"start\":21121},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22472,\"start\":22465},{\"end\":22579,\"start\":22571},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22765,\"start\":22756},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23102,\"start\":23093},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23407,\"start\":23400},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24507,\"start\":24498},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24678,\"start\":24671},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25144,\"start\":25137},{\"end\":27239,\"start\":27233},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28638,\"start\":28632},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28759,\"start\":28753}]", "bib_author_first_name": "[{\"end\":30905,\"start\":30901},{\"end\":30920,\"start\":30915},{\"end\":30938,\"start\":30932},{\"end\":30952,\"start\":30948},{\"end\":30965,\"start\":30961},{\"end\":31377,\"start\":31374},{\"end\":31396,\"start\":31386},{\"end\":31413,\"start\":31407},{\"end\":31415,\"start\":31414},{\"end\":31777,\"start\":31770},{\"end\":31789,\"start\":31783},{\"end\":31803,\"start\":31796},{\"end\":31815,\"start\":31809},{\"end\":32331,\"start\":32324},{\"end\":32343,\"start\":32337},{\"end\":32357,\"start\":32350},{\"end\":32370,\"start\":32363},{\"end\":32385,\"start\":32378},{\"end\":32398,\"start\":32392},{\"end\":32734,\"start\":32728},{\"end\":32747,\"start\":32741},{\"end\":33010,\"start\":33003},{\"end\":33021,\"start\":33017},{\"end\":33029,\"start\":33026},{\"end\":33041,\"start\":33035},{\"end\":33051,\"start\":33048},{\"end\":33065,\"start\":33057},{\"end\":33385,\"start\":33377},{\"end\":33399,\"start\":33392},{\"end\":33413,\"start\":33405},{\"end\":33430,\"start\":33421},{\"end\":33444,\"start\":33437},{\"end\":33748,\"start\":33741},{\"end\":33765,\"start\":33756},{\"end\":33779,\"start\":33772},{\"end\":33802,\"start\":33794},{\"end\":33822,\"start\":33813},{\"end\":34233,\"start\":34228},{\"end\":34244,\"start\":34239},{\"end\":34258,\"start\":34253},{\"end\":34608,\"start\":34603},{\"end\":34616,\"start\":34614},{\"end\":34634,\"start\":34628},{\"end\":34643,\"start\":34642},{\"end\":34653,\"start\":34650},{\"end\":34669,\"start\":34663},{\"end\":34948,\"start\":34945},{\"end\":34965,\"start\":34961},{\"end\":34986,\"start\":34981},{\"end\":34998,\"start\":34994},{\"end\":35008,\"start\":35003},{\"end\":35030,\"start\":35023},{\"end\":35043,\"start\":35038},{\"end\":35061,\"start\":35055},{\"end\":35415,\"start\":35411},{\"end\":35426,\"start\":35423},{\"end\":35438,\"start\":35435},{\"end\":35454,\"start\":35450},{\"end\":35468,\"start\":35464},{\"end\":35483,\"start\":35477},{\"end\":35798,\"start\":35790},{\"end\":35810,\"start\":35803},{\"end\":35824,\"start\":35817},{\"end\":35837,\"start\":35834},{\"end\":35850,\"start\":35845},{\"end\":35862,\"start\":35856},{\"end\":35875,\"start\":35874},{\"end\":35889,\"start\":35886},{\"end\":35906,\"start\":35898},{\"end\":35919,\"start\":35914},{\"end\":35921,\"start\":35920},{\"end\":36339,\"start\":36331},{\"end\":36351,\"start\":36344},{\"end\":36365,\"start\":36358},{\"end\":36378,\"start\":36375},{\"end\":36391,\"start\":36386},{\"end\":36403,\"start\":36397},{\"end\":36416,\"start\":36415},{\"end\":36430,\"start\":36427},{\"end\":36447,\"start\":36439},{\"end\":36465,\"start\":36464},{\"end\":36832,\"start\":36824},{\"end\":36841,\"start\":36837},{\"end\":36854,\"start\":36848},{\"end\":37109,\"start\":37101},{\"end\":37117,\"start\":37114},{\"end\":37134,\"start\":37128},{\"end\":37153,\"start\":37146},{\"end\":37168,\"start\":37160},{\"end\":37183,\"start\":37178},{\"end\":37185,\"start\":37184},{\"end\":37424,\"start\":37416},{\"end\":37432,\"start\":37429},{\"end\":37444,\"start\":37443},{\"end\":37460,\"start\":37453},{\"end\":37480,\"start\":37472},{\"end\":37492,\"start\":37487},{\"end\":37494,\"start\":37493},{\"end\":37751,\"start\":37743},{\"end\":37761,\"start\":37759},{\"end\":37770,\"start\":37768},{\"end\":37781,\"start\":37776},{\"end\":37792,\"start\":37788},{\"end\":37806,\"start\":37799},{\"end\":38155,\"start\":38149},{\"end\":38171,\"start\":38166},{\"end\":38189,\"start\":38183},{\"end\":38204,\"start\":38198},{\"end\":38217,\"start\":38212},{\"end\":38232,\"start\":38225},{\"end\":38240,\"start\":38237},{\"end\":38258,\"start\":38252},{\"end\":38569,\"start\":38565},{\"end\":38583,\"start\":38578},{\"end\":38596,\"start\":38590},{\"end\":38607,\"start\":38603},{\"end\":38889,\"start\":38883},{\"end\":38903,\"start\":38897},{\"end\":38914,\"start\":38910},{\"end\":38925,\"start\":38921},{\"end\":38937,\"start\":38931},{\"end\":38949,\"start\":38945},{\"end\":38962,\"start\":38957},{\"end\":38978,\"start\":38972},{\"end\":39284,\"start\":39278},{\"end\":39296,\"start\":39289},{\"end\":39310,\"start\":39302},{\"end\":39323,\"start\":39315},{\"end\":39335,\"start\":39328},{\"end\":39350,\"start\":39342},{\"end\":39364,\"start\":39356},{\"end\":39373,\"start\":39369},{\"end\":39377,\"start\":39374},{\"end\":39711,\"start\":39708},{\"end\":39725,\"start\":39718},{\"end\":40170,\"start\":40165},{\"end\":40184,\"start\":40175},{\"end\":40195,\"start\":40191},{\"end\":40208,\"start\":40203},{\"end\":40220,\"start\":40214},{\"end\":40237,\"start\":40232},{\"end\":40248,\"start\":40247},{\"end\":40265,\"start\":40258},{\"end\":40632,\"start\":40627},{\"end\":40646,\"start\":40637},{\"end\":40658,\"start\":40653},{\"end\":40668,\"start\":40664},{\"end\":40677,\"start\":40676},{\"end\":40693,\"start\":40687},{\"end\":40710,\"start\":40703},{\"end\":41121,\"start\":41114},{\"end\":41134,\"start\":41128},{\"end\":41143,\"start\":41139},{\"end\":41157,\"start\":41150},{\"end\":41170,\"start\":41164},{\"end\":41182,\"start\":41175},{\"end\":41195,\"start\":41188},{\"end\":41574,\"start\":41570},{\"end\":41588,\"start\":41582},{\"end\":41602,\"start\":41598},{\"end\":41617,\"start\":41613},{\"end\":41621,\"start\":41618},{\"end\":41633,\"start\":41628},{\"end\":41646,\"start\":41642},{\"end\":41660,\"start\":41655},{\"end\":41677,\"start\":41672},{\"end\":42050,\"start\":42045},{\"end\":42062,\"start\":42056},{\"end\":42077,\"start\":42069},{\"end\":42087,\"start\":42083},{\"end\":42103,\"start\":42095},{\"end\":42112,\"start\":42108},{\"end\":42125,\"start\":42119},{\"end\":42476,\"start\":42471},{\"end\":42490,\"start\":42487},{\"end\":42503,\"start\":42497},{\"end\":42767,\"start\":42760},{\"end\":42781,\"start\":42774},{\"end\":42796,\"start\":42787},{\"end\":42810,\"start\":42803},{\"end\":43247,\"start\":43241},{\"end\":43263,\"start\":43256},{\"end\":43269,\"start\":43264},{\"end\":43284,\"start\":43277},{\"end\":43297,\"start\":43292},{\"end\":43309,\"start\":43302},{\"end\":43321,\"start\":43314},{\"end\":43593,\"start\":43591},{\"end\":43611,\"start\":43605},{\"end\":43619,\"start\":43616},{\"end\":43637,\"start\":43631},{\"end\":43652,\"start\":43648},{\"end\":44038,\"start\":44033},{\"end\":44059,\"start\":44051},{\"end\":44074,\"start\":44069},{\"end\":44089,\"start\":44084},{\"end\":44104,\"start\":44100},{\"end\":44125,\"start\":44122},{\"end\":44502,\"start\":44498},{\"end\":44516,\"start\":44512},{\"end\":44521,\"start\":44517},{\"end\":44532,\"start\":44527},{\"end\":44548,\"start\":44542},{\"end\":44564,\"start\":44557},{\"end\":44578,\"start\":44570},{\"end\":44594,\"start\":44588},{\"end\":44609,\"start\":44603},{\"end\":44624,\"start\":44618},{\"end\":44638,\"start\":44634},{\"end\":45115,\"start\":45111},{\"end\":45130,\"start\":45124},{\"end\":45146,\"start\":45141},{\"end\":45161,\"start\":45155},{\"end\":45180,\"start\":45173},{\"end\":45591,\"start\":45586},{\"end\":45608,\"start\":45601},{\"end\":45627,\"start\":45620},{\"end\":45643,\"start\":45636},{\"end\":45656,\"start\":45651},{\"end\":46123,\"start\":46117},{\"end\":46137,\"start\":46131},{\"end\":46157,\"start\":46151},{\"end\":46568,\"start\":46560},{\"end\":46583,\"start\":46575},{\"end\":46593,\"start\":46588},{\"end\":46607,\"start\":46603},{\"end\":46623,\"start\":46616},{\"end\":46640,\"start\":46636},{\"end\":46951,\"start\":46944},{\"end\":46968,\"start\":46961},{\"end\":46981,\"start\":46975},{\"end\":46994,\"start\":46989},{\"end\":47008,\"start\":47000},{\"end\":47016,\"start\":47013},{\"end\":47032,\"start\":47027},{\"end\":47048,\"start\":47040},{\"end\":47338,\"start\":47333},{\"end\":47351,\"start\":47347},{\"end\":47366,\"start\":47360},{\"end\":47376,\"start\":47374},{\"end\":47385,\"start\":47382},{\"end\":47398,\"start\":47390},{\"end\":47412,\"start\":47406},{\"end\":47422,\"start\":47417},{\"end\":47620,\"start\":47616},{\"end\":47856,\"start\":47851},{\"end\":47869,\"start\":47865},{\"end\":47884,\"start\":47878},{\"end\":47894,\"start\":47892},{\"end\":47903,\"start\":47900},{\"end\":47916,\"start\":47908},{\"end\":47930,\"start\":47924},{\"end\":47940,\"start\":47935},{\"end\":47951,\"start\":47947},{\"end\":47964,\"start\":47960},{\"end\":47976,\"start\":47972},{\"end\":47990,\"start\":47985},{\"end\":48003,\"start\":47998},{\"end\":48359,\"start\":48352},{\"end\":48373,\"start\":48366},{\"end\":48387,\"start\":48380},{\"end\":48604,\"start\":48597},{\"end\":48618,\"start\":48611},{\"end\":48632,\"start\":48625},{\"end\":48905,\"start\":48899},{\"end\":48919,\"start\":48914},{\"end\":48936,\"start\":48930},{\"end\":48951,\"start\":48945},{\"end\":48967,\"start\":48962},{\"end\":48986,\"start\":48979},{\"end\":48999,\"start\":48993},{\"end\":49016,\"start\":49010},{\"end\":49399,\"start\":49394},{\"end\":49416,\"start\":49410},{\"end\":49428,\"start\":49424},{\"end\":49440,\"start\":49436},{\"end\":49760,\"start\":49756},{\"end\":49776,\"start\":49771},{\"end\":49790,\"start\":49785},{\"end\":49804,\"start\":49799},{\"end\":50144,\"start\":50139},{\"end\":50159,\"start\":50154},{\"end\":50416,\"start\":50407},{\"end\":50430,\"start\":50423},{\"end\":50443,\"start\":50436},{\"end\":50455,\"start\":50449},{\"end\":50464,\"start\":50461},{\"end\":50477,\"start\":50472},{\"end\":50999,\"start\":50993},{\"end\":51023,\"start\":51017},{\"end\":51035,\"start\":51028},{\"end\":51048,\"start\":51042},{\"end\":51059,\"start\":51054},{\"end\":51068,\"start\":51064},{\"end\":51080,\"start\":51074},{\"end\":51091,\"start\":51087},{\"end\":51097,\"start\":51092},{\"end\":51454,\"start\":51449},{\"end\":51465,\"start\":51459},{\"end\":51482,\"start\":51475},{\"end\":51935,\"start\":51932},{\"end\":51946,\"start\":51942},{\"end\":51959,\"start\":51956},{\"end\":51966,\"start\":51964},{\"end\":51977,\"start\":51974},{\"end\":51993,\"start\":51985},{\"end\":52008,\"start\":51999},{\"end\":52015,\"start\":52013},{\"end\":52022,\"start\":52020},{\"end\":52037,\"start\":52030},{\"end\":52446,\"start\":52442},{\"end\":52458,\"start\":52451},{\"end\":52474,\"start\":52466},{\"end\":52487,\"start\":52482},{\"end\":52497,\"start\":52494},{\"end\":52510,\"start\":52503},{\"end\":52523,\"start\":52517},{\"end\":52806,\"start\":52801},{\"end\":52815,\"start\":52811},{\"end\":52829,\"start\":52823},{\"end\":52840,\"start\":52836},{\"end\":52852,\"start\":52849},{\"end\":52866,\"start\":52860},{\"end\":52883,\"start\":52874},{\"end\":52885,\"start\":52884},{\"end\":52907,\"start\":52897},{\"end\":52918,\"start\":52914},{\"end\":52929,\"start\":52924},{\"end\":52938,\"start\":52936},{\"end\":53329,\"start\":53323},{\"end\":53340,\"start\":53334},{\"end\":53352,\"start\":53347},{\"end\":53364,\"start\":53358},{\"end\":53626,\"start\":53621},{\"end\":53641,\"start\":53634},{\"end\":53908,\"start\":53902},{\"end\":53921,\"start\":53915},{\"end\":53934,\"start\":53928},{\"end\":53946,\"start\":53940},{\"end\":53956,\"start\":53951},{\"end\":53968,\"start\":53962}]", "bib_author_last_name": "[{\"end\":30913,\"start\":30906},{\"end\":30930,\"start\":30921},{\"end\":30946,\"start\":30939},{\"end\":30959,\"start\":30953},{\"end\":30971,\"start\":30966},{\"end\":31384,\"start\":31378},{\"end\":31405,\"start\":31397},{\"end\":31421,\"start\":31416},{\"end\":31781,\"start\":31778},{\"end\":31794,\"start\":31790},{\"end\":31807,\"start\":31804},{\"end\":31820,\"start\":31816},{\"end\":32335,\"start\":32332},{\"end\":32348,\"start\":32344},{\"end\":32361,\"start\":32358},{\"end\":32376,\"start\":32371},{\"end\":32390,\"start\":32386},{\"end\":32403,\"start\":32399},{\"end\":32739,\"start\":32735},{\"end\":32752,\"start\":32748},{\"end\":33015,\"start\":33011},{\"end\":33024,\"start\":33022},{\"end\":33033,\"start\":33030},{\"end\":33046,\"start\":33042},{\"end\":33055,\"start\":33052},{\"end\":33070,\"start\":33066},{\"end\":33390,\"start\":33386},{\"end\":33403,\"start\":33400},{\"end\":33419,\"start\":33414},{\"end\":33435,\"start\":33431},{\"end\":33449,\"start\":33445},{\"end\":33754,\"start\":33749},{\"end\":33770,\"start\":33766},{\"end\":33792,\"start\":33780},{\"end\":33811,\"start\":33803},{\"end\":33833,\"start\":33823},{\"end\":34237,\"start\":34234},{\"end\":34251,\"start\":34245},{\"end\":34265,\"start\":34259},{\"end\":34612,\"start\":34609},{\"end\":34626,\"start\":34617},{\"end\":34640,\"start\":34635},{\"end\":34648,\"start\":34644},{\"end\":34661,\"start\":34654},{\"end\":34677,\"start\":34670},{\"end\":34687,\"start\":34679},{\"end\":34959,\"start\":34949},{\"end\":34979,\"start\":34966},{\"end\":34992,\"start\":34987},{\"end\":35001,\"start\":34999},{\"end\":35021,\"start\":35009},{\"end\":35036,\"start\":35031},{\"end\":35053,\"start\":35044},{\"end\":35068,\"start\":35062},{\"end\":35421,\"start\":35416},{\"end\":35433,\"start\":35427},{\"end\":35448,\"start\":35439},{\"end\":35462,\"start\":35455},{\"end\":35475,\"start\":35469},{\"end\":35492,\"start\":35484},{\"end\":35801,\"start\":35799},{\"end\":35815,\"start\":35811},{\"end\":35832,\"start\":35825},{\"end\":35843,\"start\":35838},{\"end\":35854,\"start\":35851},{\"end\":35872,\"start\":35863},{\"end\":35884,\"start\":35876},{\"end\":35896,\"start\":35890},{\"end\":35912,\"start\":35907},{\"end\":35929,\"start\":35922},{\"end\":35936,\"start\":35931},{\"end\":36342,\"start\":36340},{\"end\":36356,\"start\":36352},{\"end\":36373,\"start\":36366},{\"end\":36384,\"start\":36379},{\"end\":36395,\"start\":36392},{\"end\":36413,\"start\":36404},{\"end\":36425,\"start\":36417},{\"end\":36437,\"start\":36431},{\"end\":36453,\"start\":36448},{\"end\":36462,\"start\":36455},{\"end\":36471,\"start\":36466},{\"end\":36835,\"start\":36833},{\"end\":36846,\"start\":36842},{\"end\":36861,\"start\":36855},{\"end\":37112,\"start\":37110},{\"end\":37126,\"start\":37118},{\"end\":37144,\"start\":37135},{\"end\":37158,\"start\":37154},{\"end\":37176,\"start\":37169},{\"end\":37191,\"start\":37186},{\"end\":37427,\"start\":37425},{\"end\":37441,\"start\":37433},{\"end\":37451,\"start\":37445},{\"end\":37470,\"start\":37461},{\"end\":37485,\"start\":37481},{\"end\":37502,\"start\":37495},{\"end\":37509,\"start\":37504},{\"end\":37757,\"start\":37752},{\"end\":37766,\"start\":37762},{\"end\":37774,\"start\":37771},{\"end\":37786,\"start\":37782},{\"end\":37797,\"start\":37793},{\"end\":37811,\"start\":37807},{\"end\":38164,\"start\":38156},{\"end\":38181,\"start\":38172},{\"end\":38196,\"start\":38190},{\"end\":38210,\"start\":38205},{\"end\":38223,\"start\":38218},{\"end\":38235,\"start\":38233},{\"end\":38250,\"start\":38241},{\"end\":38265,\"start\":38259},{\"end\":38576,\"start\":38570},{\"end\":38588,\"start\":38584},{\"end\":38601,\"start\":38597},{\"end\":38613,\"start\":38608},{\"end\":38895,\"start\":38890},{\"end\":38908,\"start\":38904},{\"end\":38919,\"start\":38915},{\"end\":38929,\"start\":38926},{\"end\":38943,\"start\":38938},{\"end\":38955,\"start\":38950},{\"end\":38970,\"start\":38963},{\"end\":38984,\"start\":38979},{\"end\":39287,\"start\":39285},{\"end\":39300,\"start\":39297},{\"end\":39313,\"start\":39311},{\"end\":39326,\"start\":39324},{\"end\":39340,\"start\":39336},{\"end\":39354,\"start\":39351},{\"end\":39367,\"start\":39365},{\"end\":39381,\"start\":39378},{\"end\":39716,\"start\":39712},{\"end\":39729,\"start\":39726},{\"end\":39734,\"start\":39731},{\"end\":40173,\"start\":40171},{\"end\":40189,\"start\":40185},{\"end\":40201,\"start\":40196},{\"end\":40212,\"start\":40209},{\"end\":40230,\"start\":40221},{\"end\":40245,\"start\":40238},{\"end\":40256,\"start\":40249},{\"end\":40273,\"start\":40266},{\"end\":40285,\"start\":40275},{\"end\":40635,\"start\":40633},{\"end\":40651,\"start\":40647},{\"end\":40662,\"start\":40659},{\"end\":40674,\"start\":40669},{\"end\":40685,\"start\":40678},{\"end\":40701,\"start\":40694},{\"end\":40720,\"start\":40711},{\"end\":40732,\"start\":40722},{\"end\":41126,\"start\":41122},{\"end\":41137,\"start\":41135},{\"end\":41148,\"start\":41144},{\"end\":41162,\"start\":41158},{\"end\":41173,\"start\":41171},{\"end\":41186,\"start\":41183},{\"end\":41201,\"start\":41196},{\"end\":41580,\"start\":41575},{\"end\":41596,\"start\":41589},{\"end\":41611,\"start\":41603},{\"end\":41626,\"start\":41622},{\"end\":41640,\"start\":41634},{\"end\":41653,\"start\":41647},{\"end\":41670,\"start\":41661},{\"end\":41684,\"start\":41678},{\"end\":42054,\"start\":42051},{\"end\":42067,\"start\":42063},{\"end\":42081,\"start\":42078},{\"end\":42093,\"start\":42088},{\"end\":42106,\"start\":42104},{\"end\":42117,\"start\":42113},{\"end\":42129,\"start\":42126},{\"end\":42485,\"start\":42477},{\"end\":42495,\"start\":42491},{\"end\":42509,\"start\":42504},{\"end\":42772,\"start\":42768},{\"end\":42785,\"start\":42782},{\"end\":42801,\"start\":42797},{\"end\":42814,\"start\":42811},{\"end\":43254,\"start\":43248},{\"end\":43275,\"start\":43270},{\"end\":43290,\"start\":43285},{\"end\":43300,\"start\":43298},{\"end\":43312,\"start\":43310},{\"end\":43325,\"start\":43322},{\"end\":43603,\"start\":43594},{\"end\":43614,\"start\":43612},{\"end\":43629,\"start\":43620},{\"end\":43646,\"start\":43638},{\"end\":43663,\"start\":43653},{\"end\":44049,\"start\":44039},{\"end\":44067,\"start\":44060},{\"end\":44082,\"start\":44075},{\"end\":44098,\"start\":44090},{\"end\":44120,\"start\":44105},{\"end\":44134,\"start\":44126},{\"end\":44510,\"start\":44503},{\"end\":44525,\"start\":44522},{\"end\":44540,\"start\":44533},{\"end\":44555,\"start\":44549},{\"end\":44568,\"start\":44565},{\"end\":44586,\"start\":44579},{\"end\":44601,\"start\":44595},{\"end\":44616,\"start\":44610},{\"end\":44632,\"start\":44625},{\"end\":44644,\"start\":44639},{\"end\":45122,\"start\":45116},{\"end\":45139,\"start\":45131},{\"end\":45153,\"start\":45147},{\"end\":45171,\"start\":45162},{\"end\":45187,\"start\":45181},{\"end\":45599,\"start\":45592},{\"end\":45618,\"start\":45609},{\"end\":45634,\"start\":45628},{\"end\":45649,\"start\":45644},{\"end\":45662,\"start\":45657},{\"end\":46129,\"start\":46124},{\"end\":46149,\"start\":46138},{\"end\":46162,\"start\":46158},{\"end\":46573,\"start\":46569},{\"end\":46586,\"start\":46584},{\"end\":46601,\"start\":46594},{\"end\":46614,\"start\":46608},{\"end\":46634,\"start\":46624},{\"end\":46648,\"start\":46641},{\"end\":46959,\"start\":46952},{\"end\":46973,\"start\":46969},{\"end\":46987,\"start\":46982},{\"end\":46998,\"start\":46995},{\"end\":47011,\"start\":47009},{\"end\":47025,\"start\":47017},{\"end\":47038,\"start\":47033},{\"end\":47056,\"start\":47049},{\"end\":47345,\"start\":47339},{\"end\":47358,\"start\":47352},{\"end\":47372,\"start\":47367},{\"end\":47380,\"start\":47377},{\"end\":47388,\"start\":47386},{\"end\":47404,\"start\":47399},{\"end\":47415,\"start\":47413},{\"end\":47427,\"start\":47423},{\"end\":47626,\"start\":47621},{\"end\":47863,\"start\":47857},{\"end\":47876,\"start\":47870},{\"end\":47890,\"start\":47885},{\"end\":47898,\"start\":47895},{\"end\":47906,\"start\":47904},{\"end\":47922,\"start\":47917},{\"end\":47933,\"start\":47931},{\"end\":47945,\"start\":47941},{\"end\":47958,\"start\":47952},{\"end\":47970,\"start\":47965},{\"end\":47983,\"start\":47977},{\"end\":47996,\"start\":47991},{\"end\":48011,\"start\":48004},{\"end\":48364,\"start\":48360},{\"end\":48378,\"start\":48374},{\"end\":48393,\"start\":48388},{\"end\":48609,\"start\":48605},{\"end\":48623,\"start\":48619},{\"end\":48638,\"start\":48633},{\"end\":48912,\"start\":48906},{\"end\":48928,\"start\":48920},{\"end\":48943,\"start\":48937},{\"end\":48960,\"start\":48952},{\"end\":48977,\"start\":48968},{\"end\":48991,\"start\":48987},{\"end\":49008,\"start\":49000},{\"end\":49023,\"start\":49017},{\"end\":49408,\"start\":49400},{\"end\":49422,\"start\":49417},{\"end\":49434,\"start\":49429},{\"end\":49446,\"start\":49441},{\"end\":49769,\"start\":49761},{\"end\":49783,\"start\":49777},{\"end\":49797,\"start\":49791},{\"end\":49814,\"start\":49805},{\"end\":50152,\"start\":50145},{\"end\":50164,\"start\":50160},{\"end\":50173,\"start\":50166},{\"end\":50421,\"start\":50417},{\"end\":50434,\"start\":50431},{\"end\":50447,\"start\":50444},{\"end\":50459,\"start\":50456},{\"end\":50470,\"start\":50465},{\"end\":50487,\"start\":50478},{\"end\":51015,\"start\":51000},{\"end\":51026,\"start\":51024},{\"end\":51040,\"start\":51036},{\"end\":51052,\"start\":51049},{\"end\":51062,\"start\":51060},{\"end\":51072,\"start\":51069},{\"end\":51085,\"start\":51081},{\"end\":51101,\"start\":51098},{\"end\":51107,\"start\":51103},{\"end\":51457,\"start\":51455},{\"end\":51473,\"start\":51466},{\"end\":51488,\"start\":51483},{\"end\":51940,\"start\":51936},{\"end\":51954,\"start\":51947},{\"end\":51962,\"start\":51960},{\"end\":51972,\"start\":51967},{\"end\":51983,\"start\":51978},{\"end\":51997,\"start\":51994},{\"end\":52011,\"start\":52009},{\"end\":52018,\"start\":52016},{\"end\":52028,\"start\":52023},{\"end\":52042,\"start\":52038},{\"end\":52449,\"start\":52447},{\"end\":52464,\"start\":52459},{\"end\":52480,\"start\":52475},{\"end\":52492,\"start\":52488},{\"end\":52501,\"start\":52498},{\"end\":52515,\"start\":52511},{\"end\":52528,\"start\":52524},{\"end\":52809,\"start\":52807},{\"end\":52821,\"start\":52816},{\"end\":52834,\"start\":52830},{\"end\":52847,\"start\":52841},{\"end\":52858,\"start\":52853},{\"end\":52872,\"start\":52867},{\"end\":52895,\"start\":52886},{\"end\":52912,\"start\":52908},{\"end\":52922,\"start\":52919},{\"end\":52934,\"start\":52930},{\"end\":52944,\"start\":52939},{\"end\":53332,\"start\":53330},{\"end\":53345,\"start\":53341},{\"end\":53356,\"start\":53353},{\"end\":53369,\"start\":53365},{\"end\":53632,\"start\":53627},{\"end\":53650,\"start\":53642},{\"end\":53913,\"start\":53909},{\"end\":53926,\"start\":53922},{\"end\":53938,\"start\":53935},{\"end\":53949,\"start\":53947},{\"end\":53960,\"start\":53957},{\"end\":53973,\"start\":53969}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":247996703},\"end\":31372,\"start\":30845},{\"attributes\":{\"doi\":\"arXiv:2211.09800\",\"id\":\"b1\"},\"end\":31679,\"start\":31374},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":260913881},\"end\":32228,\"start\":31681},{\"attributes\":{\"doi\":\"arXiv:2302.06826\",\"id\":\"b3\"},\"end\":32658,\"start\":32230},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":250192188},\"end\":32913,\"start\":32660},{\"attributes\":{\"doi\":\"arXiv:2208.09703\",\"id\":\"b5\"},\"end\":33303,\"start\":32915},{\"attributes\":{\"doi\":\"arXiv:2108.02938\",\"id\":\"b6\"},\"end\":33671,\"start\":33305},{\"attributes\":{\"doi\":\"arXiv:2302.03011\",\"id\":\"b7\"},\"end\":34071,\"start\":33673},{\"attributes\":{\"doi\":\"arXiv:2208.01618\",\"id\":\"b8\"},\"end\":34536,\"start\":34073},{\"attributes\":{\"id\":\"b9\"},\"end\":34941,\"start\":34538},{\"attributes\":{\"id\":\"b10\"},\"end\":35348,\"start\":34943},{\"attributes\":{\"doi\":\"arXiv:2208.01626\",\"id\":\"b11\"},\"end\":35718,\"start\":35350},{\"attributes\":{\"doi\":\"arXiv:2210.02303\",\"id\":\"b12\"},\"end\":36234,\"start\":35720},{\"attributes\":{\"doi\":\"arXiv:210.02303\",\"id\":\"b13\"},\"end\":36780,\"start\":36236},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":219955663},\"end\":37075,\"start\":36782},{\"attributes\":{\"doi\":\"arXiv:2204.03458\",\"id\":\"b15\"},\"end\":37390,\"start\":37077},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":248006185},\"end\":37741,\"start\":37392},{\"attributes\":{\"doi\":\"arXiv:2302.09778\",\"id\":\"b17\"},\"end\":38119,\"start\":37743},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":196834800},\"end\":38510,\"start\":38121},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":237605410},\"end\":38820,\"start\":38512},{\"attributes\":{\"doi\":\"arXiv:2210.09276\",\"id\":\"b20\"},\"end\":39224,\"start\":38822},{\"attributes\":{\"doi\":\"arXiv:2301.07093\",\"id\":\"b21\"},\"end\":39616,\"start\":39226},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":251196768},\"end\":40108,\"start\":39618},{\"attributes\":{\"doi\":\"arXiv:2009.07833\",\"id\":\"b23\"},\"end\":40550,\"start\":40110},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":254629118},\"end\":41029,\"start\":40552},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":245704504},\"end\":41509,\"start\":41031},{\"attributes\":{\"doi\":\"arXiv:2302.01329\",\"id\":\"b26\"},\"end\":41939,\"start\":41511},{\"attributes\":{\"doi\":\"arXiv:2302.08453\",\"id\":\"b27\"},\"end\":42404,\"start\":41941},{\"attributes\":{\"doi\":\"arXiv:2211.11743\",\"id\":\"b28\"},\"end\":42694,\"start\":42406},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":81981856},\"end\":43201,\"start\":42696},{\"attributes\":{\"doi\":\"arXiv:2302.03027\",\"id\":\"b30\"},\"end\":43532,\"start\":43203},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":232428282},\"end\":44031,\"start\":43534},{\"attributes\":{\"doi\":\"arXiv:1704.00675\",\"id\":\"b32\"},\"end\":44425,\"start\":44033},{\"attributes\":{\"doi\":\"PMLR, 2021. 1\",\"id\":\"b33\",\"matched_paper_id\":231591445},\"end\":45012,\"start\":44427},{\"attributes\":{\"doi\":\"2022. 3\",\"id\":\"b34\",\"matched_paper_id\":195776274},\"end\":45522,\"start\":45014},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":245335280},\"end\":46079,\"start\":45524},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":47476652},\"end\":46472,\"start\":46081},{\"attributes\":{\"doi\":\"arXiv:2208.12242\",\"id\":\"b37\"},\"end\":46900,\"start\":46474},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":243938678},\"end\":47329,\"start\":46902},{\"attributes\":{\"id\":\"b39\"},\"end\":47550,\"start\":47331},{\"attributes\":{\"doi\":\"arXiv:2209.14792\",\"id\":\"b40\"},\"end\":47785,\"start\":47552},{\"attributes\":{\"doi\":\"arXiv:2209.14792\",\"id\":\"b41\"},\"end\":48313,\"start\":47787},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":222140788},\"end\":48593,\"start\":48315},{\"attributes\":{\"doi\":\"arXiv:2010.02502\",\"id\":\"b43\"},\"end\":48831,\"start\":48595},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":216868355},\"end\":49316,\"start\":48833},{\"attributes\":{\"doi\":\"arXiv:2211.12572\",\"id\":\"b45\"},\"end\":49659,\"start\":49318},{\"attributes\":{\"doi\":\"arXiv:2210.09477\",\"id\":\"b46\"},\"end\":50045,\"start\":49661},{\"attributes\":{\"id\":\"b47\"},\"end\":50324,\"start\":50047},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":41805341},\"end\":50905,\"start\":50326},{\"attributes\":{\"doi\":\"arXiv:2212.11565\",\"id\":\"b49\"},\"end\":51401,\"start\":50907},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":249889696},\"end\":51830,\"start\":51403},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":251765127},\"end\":52388,\"start\":51832},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":253523895},\"end\":52799,\"start\":52390},{\"attributes\":{\"doi\":\"arXiv:2212.05199\",\"id\":\"b53\"},\"end\":53257,\"start\":52801},{\"attributes\":{\"doi\":\"arXiv:2302.07685\",\"id\":\"b54\"},\"end\":53557,\"start\":53259},{\"attributes\":{\"doi\":\"arXiv:2302.05543\",\"id\":\"b55\"},\"end\":53831,\"start\":53559},{\"attributes\":{\"doi\":\"arXiv:2211.11018\",\"id\":\"b56\"},\"end\":54195,\"start\":53833}]", "bib_title": "[{\"end\":30899,\"start\":30845},{\"end\":31768,\"start\":31681},{\"end\":32726,\"start\":32660},{\"end\":34601,\"start\":34538},{\"end\":36822,\"start\":36782},{\"end\":37414,\"start\":37392},{\"end\":38147,\"start\":38121},{\"end\":38563,\"start\":38512},{\"end\":39706,\"start\":39618},{\"end\":40625,\"start\":40552},{\"end\":41112,\"start\":41031},{\"end\":42758,\"start\":42696},{\"end\":43589,\"start\":43534},{\"end\":44496,\"start\":44427},{\"end\":45109,\"start\":45014},{\"end\":45584,\"start\":45524},{\"end\":46115,\"start\":46081},{\"end\":46942,\"start\":46902},{\"end\":48350,\"start\":48315},{\"end\":48897,\"start\":48833},{\"end\":50405,\"start\":50326},{\"end\":51447,\"start\":51403},{\"end\":51930,\"start\":51832},{\"end\":52440,\"start\":52390}]", "bib_author": "[{\"end\":30915,\"start\":30901},{\"end\":30932,\"start\":30915},{\"end\":30948,\"start\":30932},{\"end\":30961,\"start\":30948},{\"end\":30973,\"start\":30961},{\"end\":31386,\"start\":31374},{\"end\":31407,\"start\":31386},{\"end\":31423,\"start\":31407},{\"end\":31783,\"start\":31770},{\"end\":31796,\"start\":31783},{\"end\":31809,\"start\":31796},{\"end\":31822,\"start\":31809},{\"end\":32337,\"start\":32324},{\"end\":32350,\"start\":32337},{\"end\":32363,\"start\":32350},{\"end\":32378,\"start\":32363},{\"end\":32392,\"start\":32378},{\"end\":32405,\"start\":32392},{\"end\":32741,\"start\":32728},{\"end\":32754,\"start\":32741},{\"end\":33017,\"start\":33003},{\"end\":33026,\"start\":33017},{\"end\":33035,\"start\":33026},{\"end\":33048,\"start\":33035},{\"end\":33057,\"start\":33048},{\"end\":33072,\"start\":33057},{\"end\":33392,\"start\":33377},{\"end\":33405,\"start\":33392},{\"end\":33421,\"start\":33405},{\"end\":33437,\"start\":33421},{\"end\":33451,\"start\":33437},{\"end\":33756,\"start\":33741},{\"end\":33772,\"start\":33756},{\"end\":33794,\"start\":33772},{\"end\":33813,\"start\":33794},{\"end\":33835,\"start\":33813},{\"end\":34239,\"start\":34228},{\"end\":34253,\"start\":34239},{\"end\":34267,\"start\":34253},{\"end\":34614,\"start\":34603},{\"end\":34628,\"start\":34614},{\"end\":34642,\"start\":34628},{\"end\":34650,\"start\":34642},{\"end\":34663,\"start\":34650},{\"end\":34679,\"start\":34663},{\"end\":34689,\"start\":34679},{\"end\":34961,\"start\":34945},{\"end\":34981,\"start\":34961},{\"end\":34994,\"start\":34981},{\"end\":35003,\"start\":34994},{\"end\":35023,\"start\":35003},{\"end\":35038,\"start\":35023},{\"end\":35055,\"start\":35038},{\"end\":35070,\"start\":35055},{\"end\":35423,\"start\":35411},{\"end\":35435,\"start\":35423},{\"end\":35450,\"start\":35435},{\"end\":35464,\"start\":35450},{\"end\":35477,\"start\":35464},{\"end\":35494,\"start\":35477},{\"end\":35803,\"start\":35790},{\"end\":35817,\"start\":35803},{\"end\":35834,\"start\":35817},{\"end\":35845,\"start\":35834},{\"end\":35856,\"start\":35845},{\"end\":35874,\"start\":35856},{\"end\":35886,\"start\":35874},{\"end\":35898,\"start\":35886},{\"end\":35914,\"start\":35898},{\"end\":35931,\"start\":35914},{\"end\":35938,\"start\":35931},{\"end\":36344,\"start\":36331},{\"end\":36358,\"start\":36344},{\"end\":36375,\"start\":36358},{\"end\":36386,\"start\":36375},{\"end\":36397,\"start\":36386},{\"end\":36415,\"start\":36397},{\"end\":36427,\"start\":36415},{\"end\":36439,\"start\":36427},{\"end\":36455,\"start\":36439},{\"end\":36464,\"start\":36455},{\"end\":36473,\"start\":36464},{\"end\":36837,\"start\":36824},{\"end\":36848,\"start\":36837},{\"end\":36863,\"start\":36848},{\"end\":37114,\"start\":37101},{\"end\":37128,\"start\":37114},{\"end\":37146,\"start\":37128},{\"end\":37160,\"start\":37146},{\"end\":37178,\"start\":37160},{\"end\":37193,\"start\":37178},{\"end\":37429,\"start\":37416},{\"end\":37443,\"start\":37429},{\"end\":37453,\"start\":37443},{\"end\":37472,\"start\":37453},{\"end\":37487,\"start\":37472},{\"end\":37504,\"start\":37487},{\"end\":37511,\"start\":37504},{\"end\":37759,\"start\":37743},{\"end\":37768,\"start\":37759},{\"end\":37776,\"start\":37768},{\"end\":37788,\"start\":37776},{\"end\":37799,\"start\":37788},{\"end\":37813,\"start\":37799},{\"end\":38166,\"start\":38149},{\"end\":38183,\"start\":38166},{\"end\":38198,\"start\":38183},{\"end\":38212,\"start\":38198},{\"end\":38225,\"start\":38212},{\"end\":38237,\"start\":38225},{\"end\":38252,\"start\":38237},{\"end\":38267,\"start\":38252},{\"end\":38578,\"start\":38565},{\"end\":38590,\"start\":38578},{\"end\":38603,\"start\":38590},{\"end\":38615,\"start\":38603},{\"end\":38897,\"start\":38883},{\"end\":38910,\"start\":38897},{\"end\":38921,\"start\":38910},{\"end\":38931,\"start\":38921},{\"end\":38945,\"start\":38931},{\"end\":38957,\"start\":38945},{\"end\":38972,\"start\":38957},{\"end\":38986,\"start\":38972},{\"end\":39289,\"start\":39278},{\"end\":39302,\"start\":39289},{\"end\":39315,\"start\":39302},{\"end\":39328,\"start\":39315},{\"end\":39342,\"start\":39328},{\"end\":39356,\"start\":39342},{\"end\":39369,\"start\":39356},{\"end\":39383,\"start\":39369},{\"end\":39718,\"start\":39708},{\"end\":39731,\"start\":39718},{\"end\":39736,\"start\":39731},{\"end\":40175,\"start\":40165},{\"end\":40191,\"start\":40175},{\"end\":40203,\"start\":40191},{\"end\":40214,\"start\":40203},{\"end\":40232,\"start\":40214},{\"end\":40247,\"start\":40232},{\"end\":40258,\"start\":40247},{\"end\":40275,\"start\":40258},{\"end\":40287,\"start\":40275},{\"end\":40637,\"start\":40627},{\"end\":40653,\"start\":40637},{\"end\":40664,\"start\":40653},{\"end\":40676,\"start\":40664},{\"end\":40687,\"start\":40676},{\"end\":40703,\"start\":40687},{\"end\":40722,\"start\":40703},{\"end\":40734,\"start\":40722},{\"end\":41128,\"start\":41114},{\"end\":41139,\"start\":41128},{\"end\":41150,\"start\":41139},{\"end\":41164,\"start\":41150},{\"end\":41175,\"start\":41164},{\"end\":41188,\"start\":41175},{\"end\":41203,\"start\":41188},{\"end\":41582,\"start\":41570},{\"end\":41598,\"start\":41582},{\"end\":41613,\"start\":41598},{\"end\":41628,\"start\":41613},{\"end\":41642,\"start\":41628},{\"end\":41655,\"start\":41642},{\"end\":41672,\"start\":41655},{\"end\":41686,\"start\":41672},{\"end\":42056,\"start\":42045},{\"end\":42069,\"start\":42056},{\"end\":42083,\"start\":42069},{\"end\":42095,\"start\":42083},{\"end\":42108,\"start\":42095},{\"end\":42119,\"start\":42108},{\"end\":42131,\"start\":42119},{\"end\":42487,\"start\":42471},{\"end\":42497,\"start\":42487},{\"end\":42511,\"start\":42497},{\"end\":42774,\"start\":42760},{\"end\":42787,\"start\":42774},{\"end\":42803,\"start\":42787},{\"end\":42816,\"start\":42803},{\"end\":43256,\"start\":43241},{\"end\":43277,\"start\":43256},{\"end\":43292,\"start\":43277},{\"end\":43302,\"start\":43292},{\"end\":43314,\"start\":43302},{\"end\":43327,\"start\":43314},{\"end\":43605,\"start\":43591},{\"end\":43616,\"start\":43605},{\"end\":43631,\"start\":43616},{\"end\":43648,\"start\":43631},{\"end\":43665,\"start\":43648},{\"end\":44051,\"start\":44033},{\"end\":44069,\"start\":44051},{\"end\":44084,\"start\":44069},{\"end\":44100,\"start\":44084},{\"end\":44122,\"start\":44100},{\"end\":44136,\"start\":44122},{\"end\":44512,\"start\":44498},{\"end\":44527,\"start\":44512},{\"end\":44542,\"start\":44527},{\"end\":44557,\"start\":44542},{\"end\":44570,\"start\":44557},{\"end\":44588,\"start\":44570},{\"end\":44603,\"start\":44588},{\"end\":44618,\"start\":44603},{\"end\":44634,\"start\":44618},{\"end\":44646,\"start\":44634},{\"end\":45124,\"start\":45111},{\"end\":45141,\"start\":45124},{\"end\":45155,\"start\":45141},{\"end\":45173,\"start\":45155},{\"end\":45189,\"start\":45173},{\"end\":45601,\"start\":45586},{\"end\":45620,\"start\":45601},{\"end\":45636,\"start\":45620},{\"end\":45651,\"start\":45636},{\"end\":45664,\"start\":45651},{\"end\":46131,\"start\":46117},{\"end\":46151,\"start\":46131},{\"end\":46164,\"start\":46151},{\"end\":46575,\"start\":46560},{\"end\":46588,\"start\":46575},{\"end\":46603,\"start\":46588},{\"end\":46616,\"start\":46603},{\"end\":46636,\"start\":46616},{\"end\":46650,\"start\":46636},{\"end\":46961,\"start\":46944},{\"end\":46975,\"start\":46961},{\"end\":46989,\"start\":46975},{\"end\":47000,\"start\":46989},{\"end\":47013,\"start\":47000},{\"end\":47027,\"start\":47013},{\"end\":47040,\"start\":47027},{\"end\":47058,\"start\":47040},{\"end\":47347,\"start\":47333},{\"end\":47360,\"start\":47347},{\"end\":47374,\"start\":47360},{\"end\":47382,\"start\":47374},{\"end\":47390,\"start\":47382},{\"end\":47406,\"start\":47390},{\"end\":47417,\"start\":47406},{\"end\":47429,\"start\":47417},{\"end\":47628,\"start\":47616},{\"end\":47865,\"start\":47851},{\"end\":47878,\"start\":47865},{\"end\":47892,\"start\":47878},{\"end\":47900,\"start\":47892},{\"end\":47908,\"start\":47900},{\"end\":47924,\"start\":47908},{\"end\":47935,\"start\":47924},{\"end\":47947,\"start\":47935},{\"end\":47960,\"start\":47947},{\"end\":47972,\"start\":47960},{\"end\":47985,\"start\":47972},{\"end\":47998,\"start\":47985},{\"end\":48013,\"start\":47998},{\"end\":48366,\"start\":48352},{\"end\":48380,\"start\":48366},{\"end\":48395,\"start\":48380},{\"end\":48611,\"start\":48597},{\"end\":48625,\"start\":48611},{\"end\":48640,\"start\":48625},{\"end\":48914,\"start\":48899},{\"end\":48930,\"start\":48914},{\"end\":48945,\"start\":48930},{\"end\":48962,\"start\":48945},{\"end\":48979,\"start\":48962},{\"end\":48993,\"start\":48979},{\"end\":49010,\"start\":48993},{\"end\":49025,\"start\":49010},{\"end\":49410,\"start\":49394},{\"end\":49424,\"start\":49410},{\"end\":49436,\"start\":49424},{\"end\":49448,\"start\":49436},{\"end\":49771,\"start\":49756},{\"end\":49785,\"start\":49771},{\"end\":49799,\"start\":49785},{\"end\":49816,\"start\":49799},{\"end\":50154,\"start\":50139},{\"end\":50166,\"start\":50154},{\"end\":50175,\"start\":50166},{\"end\":50423,\"start\":50407},{\"end\":50436,\"start\":50423},{\"end\":50449,\"start\":50436},{\"end\":50461,\"start\":50449},{\"end\":50472,\"start\":50461},{\"end\":50489,\"start\":50472},{\"end\":51017,\"start\":50993},{\"end\":51028,\"start\":51017},{\"end\":51042,\"start\":51028},{\"end\":51054,\"start\":51042},{\"end\":51064,\"start\":51054},{\"end\":51074,\"start\":51064},{\"end\":51087,\"start\":51074},{\"end\":51103,\"start\":51087},{\"end\":51109,\"start\":51103},{\"end\":51459,\"start\":51449},{\"end\":51475,\"start\":51459},{\"end\":51490,\"start\":51475},{\"end\":51942,\"start\":51932},{\"end\":51956,\"start\":51942},{\"end\":51964,\"start\":51956},{\"end\":51974,\"start\":51964},{\"end\":51985,\"start\":51974},{\"end\":51999,\"start\":51985},{\"end\":52013,\"start\":51999},{\"end\":52020,\"start\":52013},{\"end\":52030,\"start\":52020},{\"end\":52044,\"start\":52030},{\"end\":52451,\"start\":52442},{\"end\":52466,\"start\":52451},{\"end\":52482,\"start\":52466},{\"end\":52494,\"start\":52482},{\"end\":52503,\"start\":52494},{\"end\":52517,\"start\":52503},{\"end\":52530,\"start\":52517},{\"end\":52811,\"start\":52801},{\"end\":52823,\"start\":52811},{\"end\":52836,\"start\":52823},{\"end\":52849,\"start\":52836},{\"end\":52860,\"start\":52849},{\"end\":52874,\"start\":52860},{\"end\":52897,\"start\":52874},{\"end\":52914,\"start\":52897},{\"end\":52924,\"start\":52914},{\"end\":52936,\"start\":52924},{\"end\":52946,\"start\":52936},{\"end\":53334,\"start\":53323},{\"end\":53347,\"start\":53334},{\"end\":53358,\"start\":53347},{\"end\":53371,\"start\":53358},{\"end\":53634,\"start\":53621},{\"end\":53652,\"start\":53634},{\"end\":53915,\"start\":53902},{\"end\":53928,\"start\":53915},{\"end\":53940,\"start\":53928},{\"end\":53951,\"start\":53940},{\"end\":53962,\"start\":53951},{\"end\":53975,\"start\":53962}]", "bib_venue": "[{\"end\":31042,\"start\":31026},{\"end\":31971,\"start\":31905},{\"end\":39805,\"start\":39789},{\"end\":42965,\"start\":42899},{\"end\":43794,\"start\":43738},{\"end\":45813,\"start\":45747},{\"end\":46226,\"start\":46209},{\"end\":50630,\"start\":50568},{\"end\":51559,\"start\":51543},{\"end\":31024,\"start\":30973},{\"end\":31500,\"start\":31439},{\"end\":31903,\"start\":31822},{\"end\":32322,\"start\":32230},{\"end\":32770,\"start\":32754},{\"end\":33001,\"start\":32915},{\"end\":33375,\"start\":33305},{\"end\":33739,\"start\":33673},{\"end\":34226,\"start\":34073},{\"end\":34723,\"start\":34689},{\"end\":35128,\"start\":35070},{\"end\":35409,\"start\":35350},{\"end\":35788,\"start\":35720},{\"end\":36329,\"start\":36236},{\"end\":36912,\"start\":36863},{\"end\":37099,\"start\":37077},{\"end\":37560,\"start\":37511},{\"end\":37907,\"start\":37829},{\"end\":38301,\"start\":38267},{\"end\":38649,\"start\":38615},{\"end\":38881,\"start\":38822},{\"end\":39276,\"start\":39226},{\"end\":39787,\"start\":39736},{\"end\":40163,\"start\":40110},{\"end\":40783,\"start\":40734},{\"end\":41255,\"start\":41203},{\"end\":41568,\"start\":41511},{\"end\":42043,\"start\":41941},{\"end\":42469,\"start\":42406},{\"end\":42897,\"start\":42816},{\"end\":43239,\"start\":43203},{\"end\":43736,\"start\":43665},{\"end\":44205,\"start\":44152},{\"end\":44703,\"start\":44659},{\"end\":45258,\"start\":45196},{\"end\":45745,\"start\":45664},{\"end\":46207,\"start\":46164},{\"end\":46558,\"start\":46474},{\"end\":47098,\"start\":47058},{\"end\":47440,\"start\":47429},{\"end\":47614,\"start\":47552},{\"end\":47849,\"start\":47787},{\"end\":48447,\"start\":48395},{\"end\":49059,\"start\":49025},{\"end\":49392,\"start\":49318},{\"end\":49754,\"start\":49661},{\"end\":50137,\"start\":50047},{\"end\":50566,\"start\":50489},{\"end\":50991,\"start\":50907},{\"end\":51541,\"start\":51490},{\"end\":52082,\"start\":52044},{\"end\":52568,\"start\":52530},{\"end\":53005,\"start\":52962},{\"end\":53321,\"start\":53259},{\"end\":53619,\"start\":53559},{\"end\":53900,\"start\":53833}]"}}}, "year": 2023, "month": 12, "day": 17}