{"id": 219619080, "updated": "2022-11-17 01:29:16.319", "metadata": {"title": "Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image", "authors": "[{\"first\":\"Yuhui\",\"last\":\"Quan\",\"middle\":[]},{\"first\":\"Mingqin\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Tongyao\",\"last\":\"Pang\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Ji\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "In last few years, supervised deep learning has emerged as one powerful tool for image denoising, which trains a denoising network over an external dataset of noisy/clean image pairs. However, the requirement on a high-quality training dataset limits the broad applicability of the denoising networks. Recently, there have been a few works that allow training a denoising network on the set of external noisy images only. Taking one step further, this paper proposes a self-supervised learning method which only uses the input noisy image itself for training. In the proposed method, the network is trained with dropout on the pairs of Bernoulli-sampled instances of the input image, and the result is estimated by averaging the predictions generated from multiple instances of the trained model with dropout. The experiments show that the proposed method not only significantly outperforms existing single-image learning or non-learning methods, but also is competitive to the denoising networks trained on external datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3035542568", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/QuanCPJ20", "doi": "10.1109/cvpr42600.2020.00196"}}, "content": {"source": {"pdf_hash": "21922f18444dea15e307777215cf3fb6988cd256", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c254231d0b8b261741917f4f767ea23031e3b170", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/21922f18444dea15e307777215cf3fb6988cd256.txt", "contents": "\nSelf2Self With Dropout: Learning Self-Supervised Denoising From Single Image\n\n\nYuhui Quan csyhquan@scut.edu.cn \nSchool of Computer Science and Engineering\nSouth China University of Technology\n510006GuangzhouChina\n\nMingqin Chen csmingqinchen@mail.scut.edu.cn \nSchool of Computer Science and Engineering\nSouth China University of Technology\n510006GuangzhouChina\n\nTongyao Pang \nDepartment of Mathematics\nNational University of Singapore\n119076Singapore\n\nHui Ji \nDepartment of Mathematics\nNational University of Singapore\n119076Singapore\n\nSelf2Self With Dropout: Learning Self-Supervised Denoising From Single Image\n\nIn last few years, supervised deep learning has emerged as one powerful tool for image denoising, which trains a denoising network over an external dataset of noisy/clean image pairs. However, the requirement on a high-quality training dataset limits the broad applicability of the denoising networks. Recently, there have been a few works that allow training a denoising network on the set of external noisy images only. Taking one step further, this paper proposes a self-supervised learning method which only uses the input noisy image itself for training. In the proposed method, the network is trained with dropout on the pairs of Bernoulli-sampled instances of the input image, and the result is estimated by averaging the predictions generated from multiple instances of the trained model with dropout. The experiments show that the proposed method not only significantly outperforms existing single-image learning or non-learning methods, but also is competitive to the denoising networks trained on external datasets.\n\nIntroduction\n\nImage denoising is the process to remove measurement noises from noisy images. It not only has great practical value, but also serves as a core module in many image recovery tasks. A noisy image y is usually modeled as\ny = x + n,(1)\nwhere x denotes the clean image (ground truth), and n denotes the measurement noise often assumed to be random. In recent years, deep learning has become a prominent approach for image denoising, which uses a set of training samples to train a deep neural network (NN), denoted by F \u03b8 (\u00b7) with the parameter vector \u03b8, that maps a noisy image to its clean counterpart. Most existing deep-learningbased denoising methods (e.g. [26,31,32]) use many pairs of clean/noisy images, denoted by {x (i) , y (i) } i , as the train-ing samples, and the training is done by solving\nmin \u03b8 i L(F \u03b8 (x (i) ), y (i) ),(2)\nwhere L(\u00b7, \u00b7) measures the distance between two images. The availability of a large number of training samples is one key factor contributing to the performance of these methods. Sometimes, it can be expensive and difficult to collect a large dataset of useful clean/noisy image pairs. Recently, there are some studies on training denoising NNs with only external noisy images. The Noise2Noise (N2N) method [19] showed that a denoising NN model can be trained using many pairs of two noisy images of the same scene. Using a self-prediction loss, together with a so-called blind-spot strategy to avoid learning an identity mapping, the Noise2Void (N2V) method [15] and the Noise2Self (N2S) method [3] showed the possibility to learn a denoising NN with good performance on a set of unorganized external noisy images. Yet, to achieve good performance, the external images used for training should be highly related to the noisy image being processed, in terms of image content and noise statistics. The collection of such external images can be costly or challenging in practice.\n\nIt is of great value to develop a powerful denoising NN that has no prerequisite on training samples. That is, the denoising NN is learned only on the input image itself. So far, there has been very little work along this line. Based on the deep image prior (DIP), Ulyanov et al. [25] proposed a single-image deep learning model for image recovery. The aforementioned dataset-based N2V and N2S methods can also be trained using only a noisy image. However, the performance of these methods is not competitive to existing non-local methods, e.g. BM3D [10]. To summarize, there is no satisfactory solution on how to train a denoising NN with good performance, given only the input noisy image.\n\n\nAim and Basic Idea\n\nMotivated by its practical value and the lack of good solutions, this paper aims at developing an NN-based denoiser, which has good performance and yet can be trained on only the given noisy image. In other words, this paper studies how to train a denoising NN\nF \u03b8 (\u00b7) : y \u2192 x,(3)\nusing only the input noisy image y itself. Compared to supervised deep learning, the single-imagebased self-supervised learning is much more challenging. The over-fitting is much more severe when training an NN on a single image. A denoising NN can be interpreted as a Bayes estimator with its prediction accuracy measured by the mean squared error (MSE):\nMSE = bias 2 + variance,(4)\nThe variance will dramatically increase when the number of training samples decreases from many to one. The blindspot technique [15,3] can overcome one phenomenon of overfitting, i.e. the model converges to an identity mapping. However, it is not effective on reducing the large variance caused by a single training sample. As a result, existing blind-spot-based NNs, e.g. N2V and N2S, do not perform well when being trained on a single image. In short, variance reduction is the key for the self-supervised learning on a single image.\n\nTo reduce the variance of an NN-based Bayes estimator, our solution is the dropout-based ensemble. Dropout [24] is a widely-used regularization technique for deep NNs. It refers to randomly dropping out nodes when training an NN, which can be viewed as using a single NN to approximate a large number of different NNs. In other words, dropout provides a computationally-efficient way to train and maintain multiple NN models for prediction. Owing to model uncertainty introduced by dropout [12], the predictions from these models are likely to have certain degree of statistical independence, and thus the average of these predictions will reduce the variance of the result.\n\nIndeed, dropout is closely related to the blind-spot strategy used in N2V for avoiding the convergence to an identity mapping. Note that the blind-spot strategy synthesizes multiple noisy versions of the noisy image y by randomly sampling y with replacement, and the loss for training is measured on those replaced samples. Thus, it can be viewed as some form of dropout in the first and the last layer of the NN with specific connectivity.\n\nBased on the discussion above, we propose a dropoutbased scheme for the single-image self-supervised learning of denoising NNs. Our scheme uses a self-prediction loss defined on the pairs of Bernoulli sampled instances of the input image. A Bernoulli sampled instance y of an image y with probability p is defined by\ny[k] = y[k]\n, with probability p; 0, with probability 1 \u2212 p.\n\nConsider two sets { y m } m , { y n } n of independent Bernoulli sampled instances of y. Two main components of the proposed scheme are outlined as follows.\n\n\u2022 Training. Train the NN by minimizing the following loss function with Bernoulli dropout:\nmin \u03b8 m L(F \u03b8 ( y m ), y \u2212 y m ).\n\u2022 Test. Feed each y n to the trained model with Bernoulli dropout to generate a prediction x n . Then output the average of all predictions { x n } n as the result.\n\nRemark 1. Dropout is often seen when training a classification NN. Most NNs for image recovery are trained without dropout. Also, it is very rare to see the usage of dropout during test in image recovery. This paper shows that using dropout in both training and test is very effective on boosting the performance when training a denoising NN on only an input noisy image. The main reason is that it can effectively reduce the variance of the prediction.\n\n\nContributions and Significance\n\nIn this paper, we present a self-supervised dropout NN, called Self2Self (S2S), for image denoising, which allows being trained on a single noisy image. See the following for the summary of our technical contributions.\n\n\u2022 Training a denoising NN using Bernoulli sampled instances, with a partial-convolution-based implementation. Given only a noisy image without ground truth, we propose to use its Bernoulli sampled instances for training the NN with mathematical justification. Also, the partial convolution is used to replace the standard one for re-normalization on sampled pixels, which further improves the performance. \u2022 Using Bernoulli dropout in both training and test for variance reduction. Interpreting a denoising NN as a Bayes estimator, the variance reduction is the key for single-image self-supervised training. Built upon the model uncertainty introduced by dropout, we propose to use Bernoulli dropout in both the training and test stages for reducing the variance of the prediction. \u2022 Solid performance improvement over existing solutions. Extensive experiments on blind denoising under different scenarios show that the proposed approach outperforms existing single-image methods by a large margin. More importantly, its performance is even competitive to the denoising NNs trained on external image datasets, e.g. N2N.\n\nThe work in this paper has significance for both research and applications. The deep denoising NN has been a very basic tool in recent development of image recovery methods. However, most existing NN-based methods have the prerequisite on a large amount of training data relevant to the target images, which limits their broader applicability. The issue on data collection remains, even though some methods only need noisy/noisy image pairs (e.g. N2N) or unorganized noisy images (e.g. N2V, N2S). An image denoising NN without prerequisite on training data is very welcomed in practice owing to its convenience.\n\nDespite the importance of single-image self-supervised learning for image denoising NNs, there are few solutions and their performance is not competitive to those datasetbased learning methods. This paper shows that it is possible to train a denoising NN with competitive performance, using a single noisy image itself. The results presented in this paper on self-supervised learning for single image not only provide an NN-based image denoiser that is attractive in practice, but also can inspire further investigations on selfsupervised learning to other image restoration problems.\n\n\nRelated Work\n\nThere is abundant literature on image denoising. The following review is focused more on the learning-based approaches closely related to our work. Non-learning based image denoisers. A large number of image denoisers are non-learning-based and they impose some pre-defined image priors on the ground truth image to guide the denoising. One widely-used prior in image denoising is the sparsity prior of image gradients, which leads to various p -norm relating regularization methods, e.g. total variation denoising [6]. Another prominent one is the patch recurrence prior employed by the non-local methods. Among those, BM3D [10] is one of the top performers, which applies collaborative filtering to similar patches. Image denoisers learned on clean/noisy image pairs. In recent years, many supervised learning methods are developed for image denoising, which learn the denoiser on a set of clean/noisy image pairs. Some of them learn the parameters of unfolded denoising processes; e.g. [23,9,30]. The more prominent ones train deep NNs as the denoisers; see e.g. [26,31,32,8,18,13,14]. Among them, the DnCNN [31] that uses residual learning for blind denoising is a common benchmark for NN-based image denoisers. Deep image denoisers trained with multiple noisy images. Instead of using the pairs of clean/noisy images for training, the aforementioned N2N method [19] successfully trains a denoising NN using the pairs of two noisy images of the same scene. Its performance is close to that of NNs trained using clean/noisy pairs. Indeed, as long as the noise of the noisy/noisy pair is independent, the expectation of MSE of such a pair is the same as that of the clear/noisy pair. Nevertheless, the collection of many image pairs can still be difficult. Cha et al. [5] alleviated this problem by synthesizing noisy image pairs using GAN.\n\nInstead of using organized noisy image pairs, some approaches [15,16,3,17] use only unorganized noisy images for NN training, which is done by defining an effective selfprediction loss. Given a set of noisy images {y i } i , training the NN using the standard loss function, i L(F \u03b8 (y i ), y i ), can lead to severe overfitting such that F \u03b8 converges to an identity mapping. Avoiding the convergence to an identity mapping has been one main concern of self-supervised learning in image denoising.\n\nThe auto-encoder-based denoising NN [27] addresses such a concern using the architecture which excludes identity mappings, yet its performance is unsatisfactory. The blind-spot mechanism proposed in N2V [15] avoids learning an identity mapping by only allowing the NN to predict each pixel by its neighboring pixels. The implementation is done by randomly choosing image pixels of a noisy image and replacing the value of each chosen pixel by the value of a randomly-chosen neighboring pixel, and the loss is only computed on the image pixels with replaced values. Similar schemes are used in a parallel work N2S [3] and N2V's probabilistic extension [16]. Laine et al. [17] built the blindspot mechanism into its NN architecture by excluding the center pixel in its receptive field. Image denoisers learned from only a single noisy image. A learning-based image denoiser without any prerequisite on training samples is the most flexible to employ in practice. The sparse-coding-based denoisers learn a dictionary [11,1,2,22] or a wavelet tight frame [4] from the noisy image, and the denoising result is defined as a sparse approximation to the input over the learned system.\n\nThere are few studies on training denoising NNs using only one single noisy image. One is the DIP method [25]. It assumes that, when learning an NN to approximate a degraded image, meaningful image patterns are learned with the priority over random patterns such as noise. Thus, it trains a generative NN that maps a random input to the given degraded image, which is regularized by early stopping. Despite its simplicity, the performance of DIP is not satisfactory and may be sensitive to the iteration number whose optimal value is hard to determine. By defining the training data as only a single noisy image, the aforementioned N2V and N2S can be extended to the case of single-image learning. However, their performance is not competitive either.\n\n\nMain Body\n\nThis section starts with the introduction of the architecture of our Self2Self NN, followed by a detailed discussion on the schemes for self-supervised training and denoising.\n\n\nNN Architecture\n\nThe diagram of the proposed Sefl2Self NN is shown in Fig. 1. Briefly, it is an encoder-decoder NN. Given an input noisy image of the size H \u00d7 W \u00d7 C, the encoder first maps the image to an H \u00d7 W \u00d7 48 feature cube with a partial convolutional (PConv) layer [20], which is then processed by the following six encoder's blocks (EBs). Each of the first five EBs sequentially connects a PConv layer, a leaky rectified linear unit (LReLU), and a max pooling layer with 2\u00d72 receptive fields and with a stride of 2. The last EB contains a PConv layer and an LReLU. The number of channels is fixed to 48 across all EBs. The output of the encoder is then a feature cube of size H/32 \u00d7 W/32 \u00d7 48. The decoder contains five decoder's blocks (DBs). Each of the first four DBs sequentially connects an up-sampling layer with a scaling factor of 2, a concatenation (Concate) operation, and two standard convolutional (Conv) layers with LReLUs. All the Conv layers in DBs are configured with dropout. The Concate operation in a DB stacks the feature cube from the upsampling layer and the one output by the LReLU in the corresponding EB. All Conv layers in the first four DBs have 96 output channels. The last DB contains three dropout Conv layers with LReLUs for mapping the feature cube back to an image of size H \u00d7 W \u00d7 C, and the numbers of output channels of these Conv layers are 64, 32, C respectively.\nInput DB5 H x W EB1 H x W DB4 (\u00bdH) x (\u00bdW) EB2 (\u00bdH) x (\u00bdW) EB5 ( \u2044 H) x ( \u2044 W\nThe architecture of our NN shares similarity with the ones used in some existing methods such as N2N [19]. The key differences are as follows. Firstly, we introduce dropout to the Conv layers in the decoder. In a dropout Conv layer, each weight entry is set to zero with a probability, and those untouched entries will be scaled for energy maintaining. Secondly, we use partial convolutions instead of the standard ones in the encoder, which further improves the effectiveness and efficiency of the NN training. See supplementary materials for more details of the partial convolution.\n\n\nTraining Scheme\n\nAs the NN is trained only on a single noisy image y, we need to generate multiple image pairs from y, which are different from y yet cover most of its information. With this goal, we generate a set of Bernoulli sampled instances of y, denoted by { y m } M m=1 . Recall that for y, its Bernoulli sampled instance can be expressed as\ny := b y,(5)\nwhere denotes the element-wise multiplication and b denotes one instance of binary Bernoulli vector whose entries are independently sampled from a Bernoulli distribution with probability p \u2208 (0, 1). Then, a set of image pairs {( y m , y m )} M m=1 is defined as: for each m,\ny m := b m y; y m := (1 \u2212 b m ) y.(6)\nGiven such a set of image pairs, the NN F \u03b8 (\u00b7), is trained by minimizing the following loss function:\nmin \u03b8 M m=1 F \u03b8 ( y m ) \u2212 y m 2 bm ,(7)\nwhere\n\u00b7 2 b = (1 \u2212 b) \u00b7 2 2 .\nIt can be seen that the loss of each pair is measured only on those pixels that are masked by b m . As the masked pixels are randomly selected using a Bernoulli process, the summation of the loss over all pairs measures the difference over all image pixels.\n\nClearly, the Bernoulli sampling we adopt can avoid the convergence of the NN to an identity mapping. Furthermore, training with the pairs of Bernoulli sampled instances { y m , y m } is very related to training with the pairs of a Bernoulli sampled instance y m and the ground truth x, especially when many such pairs are used for training. See the following proposition. Proposition 1. Assume the noise components are independent and of zero mean. The expectation of the loss function (7) with respect to noise is the same as that of\nM m=1 F \u03b8 ( y m ) \u2212 x 2 bm + M m=1 \u03c3 2 bm ,(8)\nfor arbitrary F \u03b8 , where \u03c3(i) denotes the standard deviation of n(i).\n\nProof. See supplementary materials for the details.\n\nSince Bernoulli sampling can be viewed as an input layer with dropout, the use of Bernoulli sampled instances of noisy images can also be viewed as learning with dropout on single image. In the computation, we do not need to create the whole dataset of Bernoulli sampled instances in advance but just enable dropout without energy scaling on the input layer and pass the copies of the input noisy images to the NN at each iteration. For further improvement, data augmentation is also used in the implementation by flipping the input image horizontally, vertically and diagonally. Thus, we have totally four versions of y for training.\n\n\nDenoising Scheme\n\nAn NN trained with dropout provides a set of NNs whose certain weights follow independent Bernoulli distributions. The often-seen scheme for testing an NN with dropout is using the NN whose weights are scaled by their associated Bernoulli probability. As in our case, dropout is used for reducing the variance of the prediction, we propose to generate multiple NNs from the trained NN so as to have multiple estimators with likely certain degree of independence.\n\nFor denoising, multiple NNs F \u03b81 , \u00b7 \u00b7 \u00b7 , F \u03b8 N are formed by running dropout on the configured layers of the trained NN F \u03b8 * . Then, multiple recovered imagesx 1 , \u00b7 \u00b7 \u00b7 ,x N are generated by feeding a Bernoulli sampled instance of y to each of the newly-formed NNs. The recovered images are then averaged to the obtain the final result x * :\nx * = 1 N N n=1x n = 1 N N n=1 F \u03b8n (b M +n y).(9)\nIn implementation, the above process can be done by simply collecting the results of stochastic forward passes through the trained model F \u03b8 * . Furthermore, such forward passes can be done concurrently [12], resulting in constant running time identical to that of the standard dropout.\n\n\nExperiments\n\nThe proposed method is evaluated on several denoising tasks: including blind Gaussian denoising, real-world noisy image denoising and salt-and-pepper noise removal. Due to space limitation, we only show partial results in this section. More results can be found in our supplementary materials.\n\n\nImplementation Details\n\nThroughout the experiments, all the PConv layers and Conv layers are with kernel sizes of 3 \u00d7 3, strides of 1, and zero padding of length 2. The hyper-parameter of each LReLU is set to 0.1. All the dropouts are conducted element-wisely with the dropout probability set to 0.3. The probability of Bernoulli sampling is also set to 0.3. The Adam optimizer is used for training. The learning rate is initialized to 10 \u22125 with 4.5\u00d710 5 training steps. During test, we use dropout 50 times to generate the final result. With parallel computation enabled on processing multiple images simultaneously, our implementation takes around 1.2 hours to process an image of size 256 \u00d7 256 on average using an RTX 2080Ti GPU. Our code will be released on GitHub.\n\n\nBlind Gaussian Denoising\n\nTwo datasets are used for the performance evaluation in the case of additive white Gaussian noise (AWGN), including Set9 used in [25] with 9 color images and BSD68 used in [15] with 68 gray-scale images. Our experiments follow [25,15] with more trials on high noise levels. Images are corrupted by the AWGN with noise levels: \u03c3 = 25, 50, 75, 100 for Set9 and \u03c3 = 25, 50 for BSD68.\n\nComparison to single-image-based methods. Several representative single-image-based denoising methods with published codes are selected for comparison: KSVD [11], PALM-DL [2], (C)BM3D [10] and DIP [25]. (C)BM3D is a well-known non-local method, KSVD and PALM-DL are two dictionary-learning-based methods, and DIP is an unsupervised deep-learning-based method. (C)BM3D, KSVD and PALM-DL are non-blind to the noise level, while DIP is blind if stopped with a universal maximal iteration number. However, we found that DIP's performance is sensitive to the iteration number for different noise levels and it becomes much better if the iteration is stopped once the residual matches the given noise level. Thus, we use such a non-blind version of DIP, denoted by DIP*, for comparison. Also, our method is compared to the single-image extension of N2V and N2S, denoted by N2V(1) and N2S(1), using their codes from the papers' GitHub sites. Note that N2V(1), N2S(1) and ours are blind to the noise level.\n\nSee Table 1 and Fig. 2 for the comparison. (a) Not surprisingly, our method outperforms KSVD and PALM-DL with a large margin, which is attributed to the advantage of deep learning over dictionary learning. (b) In comparison to the single-image-learning based denoising NNs, including DIP*, N2V(1) and N2S(1), ours also performs much better on all noise levels. This shows the effectiveness of using our dropout-based ensemble in test. (c) In comparison to one top performer in non-learning methods, (C)BM3D, our method performs better on all other noise levels.\n\nComparison to dataset-based deep learning methods. Our method is also compared to several recent dataset-based deep learning methods with published training codes, including N2V [15], N2S [3], N2N [19] and (C)DnCNN [31]. Recall that N2V and N2S are trained on unorganized noisy images, N2N is trained on paired noisy images, and (C)DnCNN is trained on clean/noisy image pairs. Following N2V's setting and its noisy data generation scheme, we train N2V and N2S on CBSD300 [15] and CBSD300's gray-scale version for color/gray-scale image denoising respectively. Regarding N2N, we use its published model trained on color images with the noise level range L = [0, 50] for the test on Set9 with \u03c3 = 25, 50. For other settings, we train N2N's model using CBSD300 with N2N's noisy image pair generation scheme. For (C)DnCNN, we use its pre-trained model on L = [0, 55] for the test with Table 1. Average PSNR(dB)/SSIM(1.00E-1) of AWGN removal results on Set9 and BSD68. The best results in all approaches are marked in bold, and the best ones in single-image-based approaches or dataset-based deep approaches are underlined.   Table 1 for the comparison. (a) As expected, deep learning benefits a lot from sufficient high-quality training data with noisy/clean image pairs, and (C)DnCNN is the top performer in BSD68. (b) It is surprising that our method performs much better than N2V and N2S which are trained with unorganized training samples. One reason might be that the unorganized training samples do not provide accurate information of the truth for the noisy image being processed. Oppositely, as the training data varies with different patterns and different noise levels, it might introduce misleading unrelated features into the NN. In contrast, our method avoids such an issue, as the training is on the noisy image being processed. (c) Very surprisingly, our method even outperforms N2N and DnCNN in many scenarios, despite the fact they are trained over the dataset with paired samples, and ours is trained with only a single noisy image.\n\n\nRemoving Real-World Image Noise\n\nThe performance evaluation on real-world noisy image denoising is conducted on the PolyU dataset [28] with 100 real clean/noisy color image pairs. Our method is compared with CBM3D, TWSC [29], DIP, N2V, N2S and CDnCNN. We randomly select 70 images for training N2V, N2S and DnCNN, and the remaining images are used for test. These NNs are trained using their published codes with our effort on parameter tuning-up. The noise level is estimated by the method [7] for CBM3D.  Table 2 for the quantitative evaluation. Our method performs better than the non-learning methods including BM3D and TWSC, which shows the power of deep learning. Furthermore, except CDnCNN, our method noticeably outperforms other deep-learning-based methods, either the single-image-based or dataset-based ones. The reason for our superior results might be that the content of training samples is quite diverse, and thus the training samples and target images are not strongly correlated. Such a weak cor-  relation between training data and test images might mislead the NNs. Note that our quantitative results are very close to that of CDnCNN, and on some images our results are even better. See Fig. 3 for some visual comparison.\n\n\nRemoving Salt-and-Pepper Noise and Beyond\n\nRemoving salt-and-pepper noise (impulse noise) from images can be cast as inpainting randomly-missing image pixels. Following DIP, we use the Set11 dataset [25] for the performance evaluation on inpainting (i.e. non-blindly removing salt-and-pepper noise). As pixel values are completely erased by the salt-and-pepper noise, we only use uncorrupted pixels to train the NN. That is, only running sampling on un-corrupted pixels for generating the Bernoulli sampled instances, and the loss is not measured on corrupted image pixels. To generate the corrupted images for evaluation, we randomly drop the pixels of each image with ratios 50%, 70% and 90% respectively. Besides DIP, we use CSC [22], a dictionary-learning-based inpainting method, for comparison. See Table 3 for the quantitative comparison. Our method is much better than DIP and CSC. See also Fig. 4 for the visual comparison on three images. Image inpainting. Our method is also tested on inpainting missing image regions. See Fig. 5 for two demos. It can be seen that the image quality of our results is better than that of DIP. For instance, DIP produced faint text imprints around the nose, which is not the case in our result.    \n\n\nAblation Study\n\nTo evaluate the effectiveness of its individual components, the following ablation studies on our method are conducted on the Set9 dataset with \u03c3 = 25.  \n\n\nMore Analysis\n\nBehavior of dropout-based ensemble. In Fig. 6, we show how the prediction times, the value of N in (9), impact the denoising performance on two sample images. We can see that the PSNR value increases as more predictions are used for averaging during test. The performance gain saturates when sufficient predictions are used. Thus, our trained NN with dropout in test can produce quite independent results such that their average is capable of effectively reducing the variance of prediction. Stability over iterations. As mentioned previously, DIP's performance is sensitive to the iteration number. It can be seen from Fig. 7 that DIP has its optimal performance happening at different steps (gray points) for different images, and its performance may have noticeable drop after passing the optimal step. In contrast, Fig. 7 shows that the performance of our method keeps unaffected after sufficient training steps. Such a feature is attractive for practical use as it requires little manual intervention. \n\n\nConclusion\n\nWe proposed Self2Self, a self-supervised deep learning method for image denoising, which only uses the input noisy image itself for training and thus has no prerequisite on the training data collection. Our recipe for reducing the variance of prediction when training a denoising NN on a single noisy image is a dropout-based scheme. Dropouts are used during training as well as test, in terms of both dropping nodes in the NN and dropping pixels (Bernoulli sampling) in the input noisy image. This brings about different estimates of the ground truth image, which are averaged to yield the final output with reduced variance of prediction. Extensive experiments showed that, the performance of our denoising NN trained by the proposed Self2Self scheme is much better than that of other non-learning-based denoisers and single-image-learning denoisers. It is even close to that of those dataset-based deep learning methods. The results presented in this paper can inspire further investigations on self-supervised learning techniques in image recovery.\n\nFigure 2 .\n2Visual results of blind AWGN denoising on image 'F16' of Set9 with noise level \u03c3 = 25. \u03c3 = 25, 50 and retrain its model on L = [55, 110] using CBSD500 [21] for \u03c3 = 75, 100.See\n\nFigure 3 .\n3Denoising results on a real-world noisy image.\n\nFigure 4 .\n4Visualization of removing (inpainting) pepper noise .\n\nFigure 5 .\n5Visual results of text/scribble inpainting.\n\nFigure 6 .\n6PSNR versus prediction times. Blue bars denote the PSNR results of individual inferences, and red curves denote the cumulative average PSNR.\n\nFigure 7 .\n7PSNR versus number of training iterations.\n\nTable 2 .\n2Average PSNR(dB)/SSIM results on PolyU.Metric \nCBM3D TWSC DIP N2V N2S CDnCNN Ours \n\nPSNR \n36.98 \n36.10 36.95 34.08 35.46 \n37.55 \n37.52 \nSSIM \n0.977 \n0.963 0.975 0.954 0.965 \n0.983 \n0.980 \n\nSee \n\nTable 3 .\n3Average PSNR(dB)/SSIM of inpainting results on Set11.Dropping Ratio \nCSC \nDIP \nOurs \n\n50% \n32.97/0.912 33.48/0.930 \n35.14/0.954 \n70% \n28.44/0.855 28.50/0.848 \n31.06/0.897 \n90% \n24.34/0.712 24.24/0.727 \n25.91/0.792 \n\nInput (17.09dB) \nDIP (38.81dB) \nOurs (43.22dB) \n\nInput (13.56dB) \nDIP (32.95dB) \nOurs (34.00dB) \n\n\n\n\n(a) w/o dropout: disabling dropout on all layers during training and test; (b) w/o ensemble: using the trained dropout NN directly without dropout-based ensemble in test; (c) w/o sampling: using the original input image without Bernoulli sampling; (d) w/o PConv: replacing all PConv layers with Conv layers .SeeTable 4for the comparison, which leads to the following conclusions. (a) The comparison of 'Ours' v.s. 'w/o dropout', shows the important role of dropout in our method, as it causes significant PSNR drop, around 7.3dB, if no dropout is involved in either training or test. (b) Training with dropout itself is critical when training an NN with a single image, as the comparison of 'w/o ensemble' vs. 'w/o dropout', shows that only using dropout in training also leads to significant improvement. It justifies that dropout greatly helps overcoming the overfitting problem in our setting. (c) The comparison of 'Ours' vs. 'w/o ensemble', shows that running dropout in test is important, as it brings around 1.7dB gain in PSNR. It justifies the effectiveness of dropout-based ensemble in variance reduction. (d) The results of 'w/o sampling' show the importance of using Bernoulli sampling instances for training samples, which is consistent to what is observed in N2V and N2S. (e) The results of 'w/o PConv' show that partial convolution has a minor but worthwhile contribution to the performance.\n\nTable 4 .\n4Results of ablation studies on Set9 with \u03c3 = 25.Ablation (w/o) \ndropout ensemble sampling PConv \nOurs \n\nPSNR(dB) \n23.88 \n29.92 \n23.12 \n31.26 \n31.74 \nSSIM \n0.658 \n0.932 \n0.744 \n0.938 \n0.956 \n\n\n\nFast sparsitybased orthogonal dictionary learning for image restoration. Chenglong Bao, Jian-Feng Cai, Hui Ji, Proc. ICCV. ICCVChenglong Bao, Jian-Feng Cai, and Hui Ji. Fast sparsity- based orthogonal dictionary learning for image restoration. In Proc. ICCV, pages 3384-3391, 2013. 3\n\nDictionary learning for sparse coding: Algorithms and convergence analysis. Chenglong Bao, Hui Ji, Yuhui Quan, Zuowei Shen, IEEE Trans. Pattern Anal. Mach. Intell. 3875Chenglong Bao, Hui Ji, Yuhui Quan, and Zuowei Shen. Dic- tionary learning for sparse coding: Algorithms and conver- gence analysis. IEEE Trans. Pattern Anal. Mach. Intell., 38(7):1356-1369, 2015. 3, 5\n\nNoise2self: Blind denoising by self-supervision. Joshua Batson, Loic Royer, Proc. ICML. ICMLJoshua Batson and Loic Royer. Noise2self: Blind denoising by self-supervision. Proc. ICML, 2019. 1, 2, 3, 5\n\nData-driven tight frame construction and image denoising. Jian-Feng Cai, Hui Ji, Zuowei Shen, Gui-Bo Ye, Appl.Comput. Harmonic Anal. 371Jian-Feng Cai, Hui Ji, Zuowei Shen, and Gui-Bo Ye. Data-driven tight frame construction and image denoising. Appl.Comput. Harmonic Anal., 37(1):89-105, 2014. 3\n\nGan2gan: Generative noise learning for blind image denoising with single noisy images. Sungmin Cha, Taeeon Park, Taesup Moon, arXiv:1803.04189arXiv preprintSungmin Cha, Taeeon Park, and Taesup Moon. Gan2gan: Generative noise learning for blind image denoising with sin- gle noisy images. arXiv preprint arXiv:1803.04189, 2019. 3\n\nAn algorithm for total variation minimization and applications. Antonin Chambolle, J. Math. Imaging Vision. 201-2Antonin Chambolle. An algorithm for total variation min- imization and applications. J. Math. Imaging Vision, 20(1- 2):89-97, 2004. 3\n\nAn efficient statistical method for image noise level estimation. Guangyong Chen, Fengyuan Zhu, Pheng Ann Heng, Proc. ICCV. ICCVGuangyong Chen, Fengyuan Zhu, and Pheng Ann Heng. An efficient statistical method for image noise level estimation. In Proc. ICCV, pages 477-485, 2015. 6\n\nImage blind denoising with generative adversarial network based noise modeling. Jingwen Chen, Jiawei Chen, Hongyang Chao, Ming Yang, Proc. CVPR. CVPRJingwen Chen, Jiawei Chen, Hongyang Chao, and Ming Yang. Image blind denoising with generative adversarial net- work based noise modeling. In Proc. CVPR, pages 3155- 3164, 2018. 3\n\nTrainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration. Yunjin Chen, Thomas Pock, IEEE Pattern Anal. Mach. Intell. 396Yunjin Chen and Thomas Pock. Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration. IEEE Pattern Anal. Mach. Intell., 39(6):1256- 1272, 2016. 3\n\nImage denoising by sparse 3-d transformdomain collaborative filtering. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, IEEE Trans. Image Process. 1685Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-d transform- domain collaborative filtering. IEEE Trans. Image Process., 16(8):2080-2095, 2007. 1, 3, 5\n\nImage denoising via sparse and redundant representations over learned dictionaries. Michael Elad, Michal Aharon, IEEE Trans. Image Process. 15125Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over learned dictionar- ies. IEEE Trans. Image Process., 15(12):3736-3745, 2006. 3, 5\n\nDropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, Proc. ICML. ICML25Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Proc. ICML, pages 1050-1059, 2016. 2, 5\n\nToward convolutional blind denoising of real photographs. Zifei Shi Guo, Kai Yan, Wangmeng Zhang, Lei Zuo, Zhang, Proc. CVPR. CVPRShi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei Zhang. Toward convolutional blind denoising of real pho- tographs. In Proc. CVPR, June 2019. 3\n\nFocnet: A fractional optimal control network for image denoising. Xixi Jia, Sanyang Liu, Xiangchu Feng, Lei Zhang, Proc. CVPR. CVPRXixi Jia, Sanyang Liu, Xiangchu Feng, and Lei Zhang. Foc- net: A fractional optimal control network for image denois- ing. In Proc. CVPR, pages 6054-6063, 2019. 3\n\nNoise2void-learning denoising from single noisy images. Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Proc. CVPR. CVPRAlexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images. In Proc. CVPR, pages 2129-2137, 2019. 1, 2, 3, 5\n\nProbabilistic noise2void: Unsupervised content-aware denoising. Alexander Krull, Tomas Vicar, Florian Jug, arXiv:1906.00651arXiv preprintAlexander Krull, Tomas Vicar, and Florian Jug. Probabilistic noise2void: Unsupervised content-aware denoising. arXiv preprint arXiv:1906.00651, 2019. 3\n\nHigh-quality self-supervised deep image denoising. Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila, Proc. NIPS. NIPSSamuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising. In Proc. NIPS, pages 6968-6978, 2019. 3\n\nUniversal denoising networks: a novel cnn architecture for image denoising. Stamatios Lefkimmiatis, Proc. CVPR. CVPRStamatios Lefkimmiatis. Universal denoising networks: a novel cnn architecture for image denoising. In Proc. CVPR, pages 3204-3213, 2018. 3\n\nNoise2noise: Learning image restoration without clean data. Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila, Proc. ICML. ICMLJaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. Proc. ICML, 2018. 1, 3, 4, 5\n\nImage inpainting for irregular holes using partial convolutions. Guilin Liu, A Fitsum, Kevin J Reda, Ting-Chun Shih, Andrew Wang, Bryan Tao, Catanzaro, Proc. ECCV. ECCVGuilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In Proc. ECCV, pages 85-100, 2018. 4\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik, David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik, et al. A database of human segmented natural images and its application to evaluating segmentation algorithms and mea- suring ecological statistics. Iccv Vancouver:, 2001. 6\n\nConvolutional dictionary learning via local processing. Yaniv Vardan Papyan, Jeremias Romano, Michael Sulam, Elad, Proc. ICCV. ICCV37Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. Convolutional dictionary learning via local processing. In Proc. ICCV, pages 5296-5304, 2017. 3, 7\n\nShrinkage fields for effective image restoration. Uwe Schmidt, Stefan Roth, Proc. CVPR. CVPRUwe Schmidt and Stefan Roth. Shrinkage fields for effective image restoration. In Proc. CVPR, pages 2774-2781, 2014. 3\n\nDropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, J. Mach. Learning Research. 151Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learning Research, 15(1):1929-1958, 2014. 2\n\nDeep image prior. Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky, Proc. CVPR. CVPR57Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proc. CVPR, pages 9446-9454, 2018. 1, 3, 5, 7\n\nDeep gaussian conditional random field network: A model-based deep network for discriminative denoising. Raviteja Vemulapalli, Oncel Tuzel, Ming-Yu Liu, Proc. CVPR. CVPR13Raviteja Vemulapalli, Oncel Tuzel, and Ming-Yu Liu. Deep gaussian conditional random field network: A model-based deep network for discriminative denoising. In Proc. CVPR, pages 4801-4809, 2016. 1, 3\n\nImage denoising and inpainting with deep neural networks. Junyuan Xie, Linli Xu, Enhong Chen, Proc. NIPS. NIPSJunyuan Xie, Linli Xu, and Enhong Chen. Image denoising and inpainting with deep neural networks. In Proc. NIPS, pages 341-349, 2012. 3\n\nJun Xu, Hui Li, Zhetong Liang, David Zhang, Lei Zhang, arXiv:1804.02603Real-world noisy image denoising: A new benchmark. arXiv preprintJun Xu, Hui Li, Zhetong Liang, David Zhang, and Lei Zhang. Real-world noisy image denoising: A new bench- mark. arXiv preprint arXiv:1804.02603, 2018. 6\n\nA trilateral weighted sparse coding scheme for real-world image denoising. Jun Xu, Lei Zhang, David Zhang, Proc. ECCV. ECCVJun Xu, Lei Zhang, and David Zhang. A trilateral weighted sparse coding scheme for real-world image denoising. In Proc. ECCV, pages 20-36, 2018. 6\n\nImage denoising via sequential ensemble learning. Xuhui Yang, Yong Xu, Yuhui Quan, Hui Ji, IEEE Trans. Image Process. 2912Xuhui Yang, Yong Xu, Yuhui Quan, and Hui Ji. Image de- noising via sequential ensemble learning. IEEE Trans. Image Process., 29(12):5038-5049, 2020. 3\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, IEEE Trans. Image Process. 2675Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE Trans. Image Process., 26(7):3142-3155, 2017. 1, 3, 5\n\nFfdnet: Toward a fast and flexible solution for cnn-based image denoising. Kai Zhang, Wangmeng Zuo, Lei Zhang, IEEE Trans. Image Process. 2793Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. IEEE Trans. Image Process., 27(9):4608-4622, 2018. 1, 3\n", "annotations": {"author": "[{\"end\":214,\"start\":80},{\"end\":361,\"start\":215},{\"end\":451,\"start\":362},{\"end\":535,\"start\":452}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":86},{\"end\":227,\"start\":223},{\"end\":374,\"start\":370},{\"end\":458,\"start\":456}]", "author_first_name": "[{\"end\":85,\"start\":80},{\"end\":222,\"start\":215},{\"end\":369,\"start\":362},{\"end\":455,\"start\":452}]", "author_affiliation": "[{\"end\":213,\"start\":113},{\"end\":360,\"start\":260},{\"end\":450,\"start\":376},{\"end\":534,\"start\":460}]", "title": "[{\"end\":77,\"start\":1},{\"end\":612,\"start\":536}]", "venue": null, "abstract": "[{\"end\":1640,\"start\":614}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2318,\"start\":2314},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2321,\"start\":2318},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2324,\"start\":2321},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2905,\"start\":2901},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3157,\"start\":3153},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3193,\"start\":3190},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3857,\"start\":3853},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4127,\"start\":4123},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5084,\"start\":5080},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5086,\"start\":5084},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5600,\"start\":5596},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5983,\"start\":5979},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10997,\"start\":10994},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11108,\"start\":11104},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11472,\"start\":11468},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11474,\"start\":11472},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11477,\"start\":11474},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11549,\"start\":11545},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11552,\"start\":11549},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11555,\"start\":11552},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11557,\"start\":11555},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11560,\"start\":11557},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11563,\"start\":11560},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11566,\"start\":11563},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11594,\"start\":11590},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11849,\"start\":11845},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12252,\"start\":12249},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12389,\"start\":12385},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12392,\"start\":12389},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12394,\"start\":12392},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12397,\"start\":12394},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12863,\"start\":12859},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13030,\"start\":13026},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13439,\"start\":13436},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13478,\"start\":13474},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13497,\"start\":13493},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13841,\"start\":13837},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13843,\"start\":13841},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13845,\"start\":13843},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13848,\"start\":13845},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13877,\"start\":13874},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14110,\"start\":14106},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15220,\"start\":15216},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16535,\"start\":16531},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20554,\"start\":20550},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21878,\"start\":21874},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21921,\"start\":21917},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21976,\"start\":21972},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21979,\"start\":21976},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22288,\"start\":22284},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22301,\"start\":22298},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22315,\"start\":22311},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22328,\"start\":22324},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23872,\"start\":23868},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23881,\"start\":23878},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23891,\"start\":23887},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23909,\"start\":23905},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24165,\"start\":24161},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25873,\"start\":25869},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25963,\"start\":25959},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26233,\"start\":26230},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27185,\"start\":27181},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27718,\"start\":27714}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30676,\"start\":30488},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30736,\"start\":30677},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30803,\"start\":30737},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30860,\"start\":30804},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31014,\"start\":30861},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31070,\"start\":31015},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31276,\"start\":31071},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":31603,\"start\":31277},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33011,\"start\":31604},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":33215,\"start\":33012}]", "paragraph": "[{\"end\":1874,\"start\":1656},{\"end\":2457,\"start\":1889},{\"end\":3571,\"start\":2494},{\"end\":4264,\"start\":3573},{\"end\":4547,\"start\":4287},{\"end\":4923,\"start\":4568},{\"end\":5487,\"start\":4952},{\"end\":6163,\"start\":5489},{\"end\":6605,\"start\":6165},{\"end\":6923,\"start\":6607},{\"end\":6984,\"start\":6936},{\"end\":7142,\"start\":6986},{\"end\":7234,\"start\":7144},{\"end\":7433,\"start\":7269},{\"end\":7888,\"start\":7435},{\"end\":8141,\"start\":7923},{\"end\":9263,\"start\":8143},{\"end\":9876,\"start\":9265},{\"end\":10462,\"start\":9878},{\"end\":12321,\"start\":10479},{\"end\":12821,\"start\":12323},{\"end\":13999,\"start\":12823},{\"end\":14752,\"start\":14001},{\"end\":14941,\"start\":14766},{\"end\":16352,\"start\":14961},{\"end\":17014,\"start\":16430},{\"end\":17365,\"start\":17034},{\"end\":17653,\"start\":17379},{\"end\":17794,\"start\":17692},{\"end\":17840,\"start\":17835},{\"end\":18122,\"start\":17865},{\"end\":18658,\"start\":18124},{\"end\":18776,\"start\":18706},{\"end\":18829,\"start\":18778},{\"end\":19465,\"start\":18831},{\"end\":19948,\"start\":19486},{\"end\":20295,\"start\":19950},{\"end\":20633,\"start\":20347},{\"end\":20942,\"start\":20649},{\"end\":21716,\"start\":20969},{\"end\":22125,\"start\":21745},{\"end\":23125,\"start\":22127},{\"end\":23688,\"start\":23127},{\"end\":25736,\"start\":23690},{\"end\":26979,\"start\":25772},{\"end\":28223,\"start\":27025},{\"end\":28395,\"start\":28242},{\"end\":29420,\"start\":28413},{\"end\":30487,\"start\":29435}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":1888,\"start\":1875},{\"attributes\":{\"id\":\"formula_1\"},\"end\":2493,\"start\":2458},{\"attributes\":{\"id\":\"formula_2\"},\"end\":4567,\"start\":4548},{\"attributes\":{\"id\":\"formula_3\"},\"end\":4951,\"start\":4924},{\"attributes\":{\"id\":\"formula_4\"},\"end\":6935,\"start\":6924},{\"attributes\":{\"id\":\"formula_5\"},\"end\":7268,\"start\":7235},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16429,\"start\":16353},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17378,\"start\":17366},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17691,\"start\":17654},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17834,\"start\":17795},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17864,\"start\":17841},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18705,\"start\":18659},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20346,\"start\":20296}]", "table_ref": "[{\"end\":23138,\"start\":23131},{\"end\":24578,\"start\":24571},{\"end\":24818,\"start\":24811},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26253,\"start\":26246},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27794,\"start\":27787}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1654,\"start\":1642},{\"attributes\":{\"n\":\"1.1.\"},\"end\":4285,\"start\":4267},{\"attributes\":{\"n\":\"1.2.\"},\"end\":7921,\"start\":7891},{\"attributes\":{\"n\":\"2.\"},\"end\":10477,\"start\":10465},{\"attributes\":{\"n\":\"3.\"},\"end\":14764,\"start\":14755},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14959,\"start\":14944},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17032,\"start\":17017},{\"attributes\":{\"n\":\"3.3.\"},\"end\":19484,\"start\":19468},{\"attributes\":{\"n\":\"4.\"},\"end\":20647,\"start\":20636},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20967,\"start\":20945},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21743,\"start\":21719},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25770,\"start\":25739},{\"attributes\":{\"n\":\"4.4.\"},\"end\":27023,\"start\":26982},{\"attributes\":{\"n\":\"4.5.\"},\"end\":28240,\"start\":28226},{\"attributes\":{\"n\":\"4.6.\"},\"end\":28411,\"start\":28398},{\"attributes\":{\"n\":\"5.\"},\"end\":29433,\"start\":29423},{\"end\":30499,\"start\":30489},{\"end\":30688,\"start\":30678},{\"end\":30748,\"start\":30738},{\"end\":30815,\"start\":30805},{\"end\":30872,\"start\":30862},{\"end\":31026,\"start\":31016},{\"end\":31081,\"start\":31072},{\"end\":31287,\"start\":31278},{\"end\":33022,\"start\":33013}]", "table": "[{\"end\":31276,\"start\":31122},{\"end\":31603,\"start\":31342},{\"end\":33215,\"start\":33072}]", "figure_caption": "[{\"end\":30676,\"start\":30501},{\"end\":30736,\"start\":30690},{\"end\":30803,\"start\":30750},{\"end\":30860,\"start\":30817},{\"end\":31014,\"start\":30874},{\"end\":31070,\"start\":31028},{\"end\":31122,\"start\":31083},{\"end\":31342,\"start\":31289},{\"end\":33011,\"start\":31606},{\"end\":33072,\"start\":33024}]", "figure_ref": "[{\"end\":15020,\"start\":15014},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23149,\"start\":23143},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26951,\"start\":26945},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27887,\"start\":27881},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28022,\"start\":28016},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28458,\"start\":28452},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29039,\"start\":29033},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29238,\"start\":29232}]", "bib_author_first_name": "[{\"end\":33299,\"start\":33290},{\"end\":33314,\"start\":33305},{\"end\":33323,\"start\":33320},{\"end\":33587,\"start\":33578},{\"end\":33596,\"start\":33593},{\"end\":33606,\"start\":33601},{\"end\":33619,\"start\":33613},{\"end\":33927,\"start\":33921},{\"end\":33940,\"start\":33936},{\"end\":34140,\"start\":34131},{\"end\":34149,\"start\":34146},{\"end\":34160,\"start\":34154},{\"end\":34173,\"start\":34167},{\"end\":34464,\"start\":34457},{\"end\":34476,\"start\":34470},{\"end\":34489,\"start\":34483},{\"end\":34771,\"start\":34764},{\"end\":35023,\"start\":35014},{\"end\":35038,\"start\":35030},{\"end\":35053,\"start\":35044},{\"end\":35318,\"start\":35311},{\"end\":35331,\"start\":35325},{\"end\":35346,\"start\":35338},{\"end\":35357,\"start\":35353},{\"end\":35670,\"start\":35664},{\"end\":35683,\"start\":35677},{\"end\":35998,\"start\":35990},{\"end\":36016,\"start\":36006},{\"end\":36030,\"start\":36022},{\"end\":36047,\"start\":36042},{\"end\":36389,\"start\":36382},{\"end\":36402,\"start\":36396},{\"end\":36710,\"start\":36705},{\"end\":36722,\"start\":36716},{\"end\":36979,\"start\":36974},{\"end\":36992,\"start\":36989},{\"end\":37006,\"start\":36998},{\"end\":37017,\"start\":37014},{\"end\":37265,\"start\":37261},{\"end\":37278,\"start\":37271},{\"end\":37292,\"start\":37284},{\"end\":37302,\"start\":37299},{\"end\":37555,\"start\":37546},{\"end\":37573,\"start\":37563},{\"end\":37591,\"start\":37584},{\"end\":37847,\"start\":37838},{\"end\":37860,\"start\":37855},{\"end\":37875,\"start\":37868},{\"end\":38121,\"start\":38115},{\"end\":38133,\"start\":38129},{\"end\":38148,\"start\":38142},{\"end\":38163,\"start\":38159},{\"end\":38422,\"start\":38413},{\"end\":38660,\"start\":38654},{\"end\":38676,\"start\":38671},{\"end\":38690,\"start\":38687},{\"end\":38709,\"start\":38703},{\"end\":38721,\"start\":38717},{\"end\":38735,\"start\":38730},{\"end\":38749,\"start\":38745},{\"end\":39039,\"start\":39033},{\"end\":39046,\"start\":39045},{\"end\":39060,\"start\":39055},{\"end\":39062,\"start\":39061},{\"end\":39078,\"start\":39069},{\"end\":39091,\"start\":39085},{\"end\":39103,\"start\":39098},{\"end\":39474,\"start\":39469},{\"end\":39491,\"start\":39483},{\"end\":39506,\"start\":39501},{\"end\":39520,\"start\":39512},{\"end\":39823,\"start\":39818},{\"end\":39847,\"start\":39839},{\"end\":39863,\"start\":39856},{\"end\":40111,\"start\":40108},{\"end\":40127,\"start\":40121},{\"end\":40343,\"start\":40337},{\"end\":40364,\"start\":40356},{\"end\":40377,\"start\":40373},{\"end\":40394,\"start\":40390},{\"end\":40412,\"start\":40406},{\"end\":40699,\"start\":40693},{\"end\":40715,\"start\":40709},{\"end\":40731,\"start\":40725},{\"end\":40996,\"start\":40988},{\"end\":41015,\"start\":41010},{\"end\":41030,\"start\":41023},{\"end\":41320,\"start\":41313},{\"end\":41331,\"start\":41326},{\"end\":41342,\"start\":41336},{\"end\":41505,\"start\":41502},{\"end\":41513,\"start\":41510},{\"end\":41525,\"start\":41518},{\"end\":41538,\"start\":41533},{\"end\":41549,\"start\":41546},{\"end\":41870,\"start\":41867},{\"end\":41878,\"start\":41875},{\"end\":41891,\"start\":41886},{\"end\":42118,\"start\":42113},{\"end\":42129,\"start\":42125},{\"end\":42139,\"start\":42134},{\"end\":42149,\"start\":42146},{\"end\":42419,\"start\":42416},{\"end\":42435,\"start\":42427},{\"end\":42447,\"start\":42441},{\"end\":42458,\"start\":42454},{\"end\":42468,\"start\":42465},{\"end\":42788,\"start\":42785},{\"end\":42804,\"start\":42796},{\"end\":42813,\"start\":42810}]", "bib_author_last_name": "[{\"end\":33303,\"start\":33300},{\"end\":33318,\"start\":33315},{\"end\":33326,\"start\":33324},{\"end\":33591,\"start\":33588},{\"end\":33599,\"start\":33597},{\"end\":33611,\"start\":33607},{\"end\":33624,\"start\":33620},{\"end\":33934,\"start\":33928},{\"end\":33946,\"start\":33941},{\"end\":34144,\"start\":34141},{\"end\":34152,\"start\":34150},{\"end\":34165,\"start\":34161},{\"end\":34176,\"start\":34174},{\"end\":34468,\"start\":34465},{\"end\":34481,\"start\":34477},{\"end\":34494,\"start\":34490},{\"end\":34781,\"start\":34772},{\"end\":35028,\"start\":35024},{\"end\":35042,\"start\":35039},{\"end\":35058,\"start\":35054},{\"end\":35323,\"start\":35319},{\"end\":35336,\"start\":35332},{\"end\":35351,\"start\":35347},{\"end\":35362,\"start\":35358},{\"end\":35675,\"start\":35671},{\"end\":35688,\"start\":35684},{\"end\":36004,\"start\":35999},{\"end\":36020,\"start\":36017},{\"end\":36040,\"start\":36031},{\"end\":36058,\"start\":36048},{\"end\":36394,\"start\":36390},{\"end\":36409,\"start\":36403},{\"end\":36714,\"start\":36711},{\"end\":36733,\"start\":36723},{\"end\":36987,\"start\":36980},{\"end\":36996,\"start\":36993},{\"end\":37012,\"start\":37007},{\"end\":37021,\"start\":37018},{\"end\":37028,\"start\":37023},{\"end\":37269,\"start\":37266},{\"end\":37282,\"start\":37279},{\"end\":37297,\"start\":37293},{\"end\":37308,\"start\":37303},{\"end\":37561,\"start\":37556},{\"end\":37582,\"start\":37574},{\"end\":37595,\"start\":37592},{\"end\":37853,\"start\":37848},{\"end\":37866,\"start\":37861},{\"end\":37879,\"start\":37876},{\"end\":38127,\"start\":38122},{\"end\":38140,\"start\":38134},{\"end\":38157,\"start\":38149},{\"end\":38168,\"start\":38164},{\"end\":38435,\"start\":38423},{\"end\":38669,\"start\":38661},{\"end\":38685,\"start\":38677},{\"end\":38701,\"start\":38691},{\"end\":38715,\"start\":38710},{\"end\":38728,\"start\":38722},{\"end\":38743,\"start\":38736},{\"end\":38754,\"start\":38750},{\"end\":39043,\"start\":39040},{\"end\":39053,\"start\":39047},{\"end\":39067,\"start\":39063},{\"end\":39083,\"start\":39079},{\"end\":39096,\"start\":39092},{\"end\":39107,\"start\":39104},{\"end\":39118,\"start\":39109},{\"end\":39481,\"start\":39475},{\"end\":39499,\"start\":39492},{\"end\":39510,\"start\":39507},{\"end\":39526,\"start\":39521},{\"end\":39837,\"start\":39824},{\"end\":39854,\"start\":39848},{\"end\":39869,\"start\":39864},{\"end\":39875,\"start\":39871},{\"end\":40119,\"start\":40112},{\"end\":40132,\"start\":40128},{\"end\":40354,\"start\":40344},{\"end\":40371,\"start\":40365},{\"end\":40388,\"start\":40378},{\"end\":40404,\"start\":40395},{\"end\":40426,\"start\":40413},{\"end\":40707,\"start\":40700},{\"end\":40723,\"start\":40716},{\"end\":40741,\"start\":40732},{\"end\":41008,\"start\":40997},{\"end\":41021,\"start\":41016},{\"end\":41034,\"start\":41031},{\"end\":41324,\"start\":41321},{\"end\":41334,\"start\":41332},{\"end\":41347,\"start\":41343},{\"end\":41508,\"start\":41506},{\"end\":41516,\"start\":41514},{\"end\":41531,\"start\":41526},{\"end\":41544,\"start\":41539},{\"end\":41555,\"start\":41550},{\"end\":41873,\"start\":41871},{\"end\":41884,\"start\":41879},{\"end\":41897,\"start\":41892},{\"end\":42123,\"start\":42119},{\"end\":42132,\"start\":42130},{\"end\":42144,\"start\":42140},{\"end\":42152,\"start\":42150},{\"end\":42425,\"start\":42420},{\"end\":42439,\"start\":42436},{\"end\":42452,\"start\":42448},{\"end\":42463,\"start\":42459},{\"end\":42474,\"start\":42469},{\"end\":42794,\"start\":42789},{\"end\":42808,\"start\":42805},{\"end\":42819,\"start\":42814}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16399150},\"end\":33500,\"start\":33217},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13899021},\"end\":33870,\"start\":33502},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":59523708},\"end\":34071,\"start\":33872},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16777223},\"end\":34368,\"start\":34073},{\"attributes\":{\"doi\":\"arXiv:1803.04189\",\"id\":\"b4\"},\"end\":34698,\"start\":34370},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":119912563},\"end\":34946,\"start\":34700},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206770723},\"end\":35229,\"start\":34948},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":51989956},\"end\":35559,\"start\":35231},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15799108},\"end\":35917,\"start\":35561},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1475121},\"end\":36296,\"start\":35919},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6888534},\"end\":36617,\"start\":36298},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":160705},\"end\":36914,\"start\":36619},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49672261},\"end\":37193,\"start\":36916},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":195803430},\"end\":37488,\"start\":37195},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":53751136},\"end\":37772,\"start\":37490},{\"attributes\":{\"doi\":\"arXiv:1906.00651\",\"id\":\"b15\"},\"end\":38062,\"start\":37774},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":173990648},\"end\":38335,\"start\":38064},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4350405},\"end\":38592,\"start\":38337},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3846544},\"end\":38966,\"start\":38594},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5035107},\"end\":39327,\"start\":38968},{\"attributes\":{\"id\":\"b20\"},\"end\":39760,\"start\":39329},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":22007313},\"end\":40056,\"start\":39762},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3612623},\"end\":40268,\"start\":40058},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6844431},\"end\":40673,\"start\":40270},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4531078},\"end\":40881,\"start\":40675},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":583422},\"end\":41253,\"start\":40883},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13852540},\"end\":41500,\"start\":41255},{\"attributes\":{\"doi\":\"arXiv:1804.02603\",\"id\":\"b27\"},\"end\":41790,\"start\":41502},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":49665770},\"end\":42061,\"start\":41792},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":212708613},\"end\":42335,\"start\":42063},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":996788},\"end\":42708,\"start\":42337},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":10514149},\"end\":43022,\"start\":42710}]", "bib_title": "[{\"end\":33288,\"start\":33217},{\"end\":33576,\"start\":33502},{\"end\":33919,\"start\":33872},{\"end\":34129,\"start\":34073},{\"end\":34762,\"start\":34700},{\"end\":35012,\"start\":34948},{\"end\":35309,\"start\":35231},{\"end\":35662,\"start\":35561},{\"end\":35988,\"start\":35919},{\"end\":36380,\"start\":36298},{\"end\":36703,\"start\":36619},{\"end\":36972,\"start\":36916},{\"end\":37259,\"start\":37195},{\"end\":37544,\"start\":37490},{\"end\":38113,\"start\":38064},{\"end\":38411,\"start\":38337},{\"end\":38652,\"start\":38594},{\"end\":39031,\"start\":38968},{\"end\":39816,\"start\":39762},{\"end\":40106,\"start\":40058},{\"end\":40335,\"start\":40270},{\"end\":40691,\"start\":40675},{\"end\":40986,\"start\":40883},{\"end\":41311,\"start\":41255},{\"end\":41865,\"start\":41792},{\"end\":42111,\"start\":42063},{\"end\":42414,\"start\":42337},{\"end\":42783,\"start\":42710}]", "bib_author": "[{\"end\":33305,\"start\":33290},{\"end\":33320,\"start\":33305},{\"end\":33328,\"start\":33320},{\"end\":33593,\"start\":33578},{\"end\":33601,\"start\":33593},{\"end\":33613,\"start\":33601},{\"end\":33626,\"start\":33613},{\"end\":33936,\"start\":33921},{\"end\":33948,\"start\":33936},{\"end\":34146,\"start\":34131},{\"end\":34154,\"start\":34146},{\"end\":34167,\"start\":34154},{\"end\":34178,\"start\":34167},{\"end\":34470,\"start\":34457},{\"end\":34483,\"start\":34470},{\"end\":34496,\"start\":34483},{\"end\":34783,\"start\":34764},{\"end\":35030,\"start\":35014},{\"end\":35044,\"start\":35030},{\"end\":35060,\"start\":35044},{\"end\":35325,\"start\":35311},{\"end\":35338,\"start\":35325},{\"end\":35353,\"start\":35338},{\"end\":35364,\"start\":35353},{\"end\":35677,\"start\":35664},{\"end\":35690,\"start\":35677},{\"end\":36006,\"start\":35990},{\"end\":36022,\"start\":36006},{\"end\":36042,\"start\":36022},{\"end\":36060,\"start\":36042},{\"end\":36396,\"start\":36382},{\"end\":36411,\"start\":36396},{\"end\":36716,\"start\":36705},{\"end\":36735,\"start\":36716},{\"end\":36989,\"start\":36974},{\"end\":36998,\"start\":36989},{\"end\":37014,\"start\":36998},{\"end\":37023,\"start\":37014},{\"end\":37030,\"start\":37023},{\"end\":37271,\"start\":37261},{\"end\":37284,\"start\":37271},{\"end\":37299,\"start\":37284},{\"end\":37310,\"start\":37299},{\"end\":37563,\"start\":37546},{\"end\":37584,\"start\":37563},{\"end\":37597,\"start\":37584},{\"end\":37855,\"start\":37838},{\"end\":37868,\"start\":37855},{\"end\":37881,\"start\":37868},{\"end\":38129,\"start\":38115},{\"end\":38142,\"start\":38129},{\"end\":38159,\"start\":38142},{\"end\":38170,\"start\":38159},{\"end\":38437,\"start\":38413},{\"end\":38671,\"start\":38654},{\"end\":38687,\"start\":38671},{\"end\":38703,\"start\":38687},{\"end\":38717,\"start\":38703},{\"end\":38730,\"start\":38717},{\"end\":38745,\"start\":38730},{\"end\":38756,\"start\":38745},{\"end\":39045,\"start\":39033},{\"end\":39055,\"start\":39045},{\"end\":39069,\"start\":39055},{\"end\":39085,\"start\":39069},{\"end\":39098,\"start\":39085},{\"end\":39109,\"start\":39098},{\"end\":39120,\"start\":39109},{\"end\":39483,\"start\":39469},{\"end\":39501,\"start\":39483},{\"end\":39512,\"start\":39501},{\"end\":39528,\"start\":39512},{\"end\":39839,\"start\":39818},{\"end\":39856,\"start\":39839},{\"end\":39871,\"start\":39856},{\"end\":39877,\"start\":39871},{\"end\":40121,\"start\":40108},{\"end\":40134,\"start\":40121},{\"end\":40356,\"start\":40337},{\"end\":40373,\"start\":40356},{\"end\":40390,\"start\":40373},{\"end\":40406,\"start\":40390},{\"end\":40428,\"start\":40406},{\"end\":40709,\"start\":40693},{\"end\":40725,\"start\":40709},{\"end\":40743,\"start\":40725},{\"end\":41010,\"start\":40988},{\"end\":41023,\"start\":41010},{\"end\":41036,\"start\":41023},{\"end\":41326,\"start\":41313},{\"end\":41336,\"start\":41326},{\"end\":41349,\"start\":41336},{\"end\":41510,\"start\":41502},{\"end\":41518,\"start\":41510},{\"end\":41533,\"start\":41518},{\"end\":41546,\"start\":41533},{\"end\":41557,\"start\":41546},{\"end\":41875,\"start\":41867},{\"end\":41886,\"start\":41875},{\"end\":41899,\"start\":41886},{\"end\":42125,\"start\":42113},{\"end\":42134,\"start\":42125},{\"end\":42146,\"start\":42134},{\"end\":42154,\"start\":42146},{\"end\":42427,\"start\":42416},{\"end\":42441,\"start\":42427},{\"end\":42454,\"start\":42441},{\"end\":42465,\"start\":42454},{\"end\":42476,\"start\":42465},{\"end\":42796,\"start\":42785},{\"end\":42810,\"start\":42796},{\"end\":42821,\"start\":42810}]", "bib_venue": "[{\"end\":33344,\"start\":33340},{\"end\":33964,\"start\":33960},{\"end\":35076,\"start\":35072},{\"end\":35380,\"start\":35376},{\"end\":36751,\"start\":36747},{\"end\":37046,\"start\":37042},{\"end\":37326,\"start\":37322},{\"end\":37613,\"start\":37609},{\"end\":38186,\"start\":38182},{\"end\":38453,\"start\":38449},{\"end\":38772,\"start\":38768},{\"end\":39136,\"start\":39132},{\"end\":39893,\"start\":39889},{\"end\":40150,\"start\":40146},{\"end\":40759,\"start\":40755},{\"end\":41052,\"start\":41048},{\"end\":41365,\"start\":41361},{\"end\":41915,\"start\":41911},{\"end\":33338,\"start\":33328},{\"end\":33664,\"start\":33626},{\"end\":33958,\"start\":33948},{\"end\":34204,\"start\":34178},{\"end\":34455,\"start\":34370},{\"end\":34806,\"start\":34783},{\"end\":35070,\"start\":35060},{\"end\":35374,\"start\":35364},{\"end\":35721,\"start\":35690},{\"end\":36085,\"start\":36060},{\"end\":36436,\"start\":36411},{\"end\":36745,\"start\":36735},{\"end\":37040,\"start\":37030},{\"end\":37320,\"start\":37310},{\"end\":37607,\"start\":37597},{\"end\":37836,\"start\":37774},{\"end\":38180,\"start\":38170},{\"end\":38447,\"start\":38437},{\"end\":38766,\"start\":38756},{\"end\":39130,\"start\":39120},{\"end\":39467,\"start\":39329},{\"end\":39887,\"start\":39877},{\"end\":40144,\"start\":40134},{\"end\":40454,\"start\":40428},{\"end\":40753,\"start\":40743},{\"end\":41046,\"start\":41036},{\"end\":41359,\"start\":41349},{\"end\":41622,\"start\":41573},{\"end\":41909,\"start\":41899},{\"end\":42179,\"start\":42154},{\"end\":42501,\"start\":42476},{\"end\":42846,\"start\":42821}]"}}}, "year": 2023, "month": 12, "day": 17}