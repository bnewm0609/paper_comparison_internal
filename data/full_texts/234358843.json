{"id": 234358843, "updated": "2023-10-06 10:37:17.335", "metadata": {"title": "Layer-adaptive sparsity for the Magnitude-based Pruning", "authors": "[{\"first\":\"Jaeho\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Sejun\",\"last\":\"Park\",\"middle\":[]},{\"first\":\"Sangwoo\",\"last\":\"Mo\",\"middle\":[]},{\"first\":\"Sungsoo\",\"last\":\"Ahn\",\"middle\":[]},{\"first\":\"Jinwoo\",\"last\":\"Shin\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on\"how to choose,\"the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation. Under various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection. Furthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.07611", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/LeePMAS21", "doi": null}}, "content": {"source": {"pdf_hash": "9227d5897abbf297a34d447e94a802a714b8eab2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2010.07611v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "27a4d5f2e2e5b7a237f0c22ed4d417b9400cc806", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9227d5897abbf297a34d447e94a802a714b8eab2.txt", "contents": "\nLAYER-ADAPTIVE SPARSITY FOR THE MAGNITUDE-BASED PRUNING\n\n\nJaeho Lee \nE Sejun \nPark A \nSangwoo Mo \nE Sungsoo \nAhn M \nJinwoo Shin jinwoos@kaist.ac.kr \nAe E Kaist \nEe A Kaist \nAi M Mbzuai \nLAYER-ADAPTIVE SPARSITY FOR THE MAGNITUDE-BASED PRUNING\nPublished as a conference paper at ICLR 2021Code: https://githubcom/jaeho-lee/layer-adaptive-sparsity\nRecent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on \"how to choose,\" the layerwise sparsities are mostly selected algorithm-byalgorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level 2 distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation. Under various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection. Furthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity Recent discoveries (Gale et al., 2019; Evci et al., 2020) demonstrate that, given an appropriate choice of layerwise sparsity, simply pruning on the basis of weight magnitude yields a surprisingly powerful unstructured pruning scheme. For instance, Gale et al. (2019) evaluates the performance of magnitudebased pruning (MP; Han et al. (2015); Zhu & Gupta(2018)) with an extensive hyperparameter tuning, and shows that MP achieves comparable or better performance than state-of-the-art pruning algorithms that use more complicated importance scores. To arrive at such a performance level, the authors introduce the following handcrafted heuristic: Leave the first convolutional layer fully dense, and prune up to only 80% of weights from the last fully-connected layer; the heuristic is motivated by the sparsity pattern from other state-of-the-art algorithms(Molchanov et al., 2017)and additional experimental/architectural observations. Unfortunately, there is an apparent lack of consensus on \"how to choose the layerwise sparsity\" for the magnitude-based pruning. Instead, the layerwise sparsity is selected mostly on an algorithm-byalgorithm basis. One common method is the global MP criteria (see, e.g., Morcos et al.(2019)), * Work done at KAIST 1 i.e., simultaneously training and pruning arXiv:2010.07611v2 [cs.LG]\n\nINTRODUCTION\n\nNeural network pruning is an art of removing \"unimportant weights\" from a model, with an intention to meet practical constraints (Han et al., 2015), mitigate overfitting (Hanson & Pratt, 1988), enhance interpretability (Mozer & Smolensky, 1988), or deepen our understanding on neural network training (Frankle & Carbin, 2019). Yet, the importance of weight is still a vaguely defined notion, and thus a wide range of pruning algorithms based on various importance scores has been proposed. One popular approach is to estimate the loss increment from removing the target weight to use as an importance score, e.g., Hessian-based approximations (LeCun et al., 1989;Hassibi & Stork, 1993;Dong et al., 2017), coreset-based estimates (Baykal et al., 2019;Mussay et al., 2020), convex optimization (Aghasi et al., 2017), and operator distortion (Park et al., 2020). Other approaches include on-the-fly 1 regularization (Louizos et al., 2018;Xiao et al., 2019), Bayesian methods (Molchanov et al., 2017;Louizos et al., 2017;Dai et al., 2018), and reinforcement learning (Lin et al., 2017). Figure 1: The LAMP score is a squared weight magnitude, normalized by the sum of all \"surviving weights\" in the layer. Global pruning by LAMP is equivalent to the layerwise magnitude-based pruning with an automatically chosen layerwise sparsity.\n\nwhere the layerwise sparsity is automatically determined by using a single global threshold on weight magnitude. Lin et al. (2020) propose a magnitude-based pruning algorithm using a feedback signal, using a heuristic rule of keeping the last fully connected layer dense. A recent work by Evci et al. (2020) proposes a magnitude-based dynamic sparse training method, adopting layerwise sparsity inspired from the network science approach toward neural network pruning (Mocanu et al., 2018).\n\nContributions. In search of a \"go-to\" layerwise sparsity for MP, we take a model-level distortion minimization perspective towards MP. We build on the observation of Dong et al. (2017); Park et al. (2020) that each neural network layer can be viewed as an operator, and MP is a choice that incurs minimum 2 distortion to the operator output (given a worst-case input signal). We bring the perspective further to examine the \"model-level\" distortion incurred by pruning a layer; preceding layers scale the input signal to the target layer, and succeeding layers scale the output distortion.\n\nBased on the distortion minimization framework, we propose a novel importance score for global pruning, coined LAMP (Layer-Adaptive Magnitude-based Pruning). The LAMP score is a rescaled weight magnitude, approximating the model-level distortion from pruning. Importantly, the LAMP score is designed to approximate the distortion on the model being pruned, i.e., all connections with a smaller LAMP score than the target weight is already pruned. Global pruning 2 with the LAMP score is equivalent to the MP with an automatically determined layerwise sparsity. At the same time, pruning with LAMP keeps the benefits of MP intact; the LAMP score is efficiently computable, hyperparameter-free, and does not rely on any model-specific knowledge.\n\nWe validate the effectiveness of LAMP under a diverse experimental setup, encompassing various convolutional neural network architectures  and various image datasets (CIFAR-10/100, SVHN, Restricted ImageNet). In all considered setups, LAMP consistently outperforms the baseline layerwise sparsity selection schemes. We also perform additional ablation studies with one-shot pruning and weight-rewinding setup to confirm that LAMP performs reliably well under a wider range of scenarios.\n\nOrganization. In Section 2, we briefly describe existing methods to choose the layerwise sparsity for magnitude-based pruning. In Section 3, we formally introduce LAMP and describe how the 2 distortion minimization perspective motivates the LAMP score. In Section 4, we empirically validate the effectiveness and versatility of LAMP. In Section 5, we take a closer look at the layerwise sparsity discovered by LAMP and compare with baseline methods and previously proposed handcrafted heuristics. In Section 6, we summarize our findings and discuss future directions. Appendices include the experimental details (Appendix A), complexity analysis (Appendix B), derivation of the LAMP score (Appendix C), additional experiments on Transformer (Appendix D), and detailed experimental results with standard deviations (Appendix E).\n\n\nRELATED WORK\n\nThis section gives a (necessarily non-exhaustive) survey of various layerwise sparsity selection schemes used for magnitude-based pruning algorithms. Magnitude-based pruning of neural networks dates back to the early works of Janowsky (1989); LeCun et al. (1989), and has been actively studied again under the context of model compression since the work of Han et al. (2015). In Han et al. (2015), the authors propose an iterative pruning scheme where the layerwise pruning threshold is determined by the standard-deviation-based heuristic. Zhu & Gupta (2018) propose a uniform pruning algorithm with a carefully tuned gradual pruning schedule combined with weight re-growths. Gale et al. (2019) refine the algorithm by adding a heuristic constraint of keeping the first convolutional layer fully dense and keeping at least 20% of the weights surviving in the last fully-connected layer.\n\nMP has also been widely used in the context of \"pruning at initialization.\" Frankle & Carbin (2019) combine MP with weight rewinding to discover efficiently trainable subnetworks: for small nets, the authors employ uniform layerwise sparsity, but use different rates for convolutional layers and fully-connected layers (with an added heuristic on the last fully-connected layer); for larger nets, authors use global MP. Morcos et al. (2019) consider transferring the \"winning ticket\" initializations, using the global MP. Evci et al. (2020) proposes a training scheme for sparsely initialized neural networks, where the layerwise sparsity is given by the Erd\u0151s-R\u00e9nyi kernel method; the method generalizes the scheme initially proposed by Mocanu et al. (2018) to convolutional neural networks.\n\nWe note that there is a line of results on the trainable layerwise sparsity; we refer the interested readers to the recent work of Kusupati et al. (2020) for a concise survey. However, we do not make direct comparisons to these methods, as our primary purpose is to deliver an easy-to-use layerwise sparsity selection scheme without requiring the modification of training objective, or an extensive hyperparameter tuning.\n\nWe also note that we focus on the unstructured sparsity. While such unstructured pruning techniques have been considered less practical (compared to structured pruning), several recent breakthroughs provide promising methods to bridge this gap; see ; .\n\n\nLAYER-ADAPTIVE MAGNITUDE-BASED PRUNING (LAMP)\n\nWe now formally introduce the Layer-Adaptive Magnitude-based Pruning (LAMP) score. Consider a depth-d feedforward neural network with weight tensors W (1) , . . . , W (d) associated with each fully-connected/convolutional layer. For fully-connected layers, corresponding weight tensors are twodimensional matrices, and for 2d convolutional layers, corresponding tensors are four-dimensional. To give a unified definition of the LAMP score for both fully-connected and convolutional layers, we assume that each weight tensor is unrolled (or flattened) to a one-dimensional vector. For each of these unrolled vectors, we assume (without loss of generality) that the weights are sorted in an ascending order according to the given index map, i.e., |W [u]| \u2264 |W [v]| holds whenever u < v, where W [u] denote the entry of W mapped by the index u. 3\n\nThe LAMP score for the u-th index of the weight tensor W is then defined as\nscore(u; W ) := (W [u]) 2 v\u2265u (W [v]) 2 .(1)\nInformally, the LAMP score (Eq. 1) measures the relative importance of the target connection among all surviving connections belonging to the same layer, where the connections with a smaller weight magnitude (in the same layer) have already been pruned. As a consequence, two connections with identical weight magnitudes have different LAMP scores, depending on the index map being used.\n\nOnce the LAMP score is computed, we globally prune the connections with smallest LAMP scores until the desired global sparsity constraint is met; the procedure is equivalent to performing MP with an automatically selected layerwise sparsity. To see this, it suffices to check that\n(W [u]) 2 > (W [v]) 2 \u21d2 score(u; W ) > score(v; W )(2)\nholds for any weight tensor W and a pair of indices u, v. From the definition of the LAMP score (Eq. 1), it is easy to see that the logical relation (2) holds. Indeed, for the connection with a larger weight magnitude, the denominator of Eq. 1 is smaller, while the numerator is larger.\n\nWe note that the global pruning with respect to the LAMP score is not identical to the global pruning with respect to the magnitude score |W [u]| (or (W [u]) 2 , equivalently). Indeed, in each layer, there exists exactly one connection with the LAMP score of 1, which is the maximum LAMP score possible. In other words, LAMP keeps at least one surviving connection in each layer. The same does not hold for the global pruning with respect to the weight magnitude score.\n\nWe also note that the LAMP score is easy-to-use. Similar to the vanilla MP, the LAMP score does not have any hyperparameter to be tuned, and is easily implementable via elementary tensor operations. Furthermore, the LAMP score can be computed with only a minimal computational overhead; the sorting of squared weight magnitudes required to compute the denominator in Eq. 1 is already a part of typical vanilla MP algorithms. For more discussions, see Appendix B.\n\n\nDESIGN MOTIVATION: MINIMIZING OUTPUT 2 DISTORTION\n\nThe LAMP score (Eq. 1) is motivated by the following observation: The layerwise MP is the solution of the layerwise minimization of Frobenius distortion incurred by pruning, which can be viewed as a relaxation of the output 2 distortion minimization with respect to the worst-case input. This observation leads us to the speculation \"Reducing the pruning-incurred 2 distortion of the model output with respect to the worst-case output may be beneficial to the performance of the retrained model (and perhaps that is why MP works well in practice).\" This speculation is not entirely new; the optimal brain damage (OBD; (LeCun et al., 1989)) is also designed around a similar philosophy of loss minimization, without a complete understanding on how the benefit of loss minimization seems to pertain after retraining.\n\nNevertheless, we use this speculation as a guideline to design LAMP as a natural extension of layerwise MP to a global pruning scheme with an automatically determined layerwise sparsity. To make arguments a bit more formal, consider a depth-d fully-connected 4 neural net, whose output given the input x is\nf (x; W (1:d) ) = W (d) \u03c3 W (d\u22121) \u03c3 \u00b7 \u00b7 \u00b7 W (2) \u03c3 W (1) x \u00b7 \u00b7 \u00b7 ,(3)\nwhere \u03c3 denotes the ReLU activation and W i denotes the weight matrix for the i-th layer, and W (1:d) = (W (1) , . . . , W (d) ) denotes the set of weight matrices.\n\nViewing MP as a relaxed layerwise 2 distortion minimization. We first focus on a single fullyconnected layer (instead of a full model), and consider the problem of minimizing the pruning-incurred 2 distortion in the layer output, given the worst-case input signal. We then observe that the problem can be relaxed to the minimization of Frobenius distortion in the weight tensor, whose solution coincides with the layerwise MP. Formally, let \u03be \u2208 R n be an input vector to a fully-connected layer with the weight tensor W \u2208 R m\u00d7n . We want to prune the tensor to W := M W , where M is an m \u00d7 n binary matrix (i.e., having only 0s and 1s as its entries) satisfying some predefined sparsity constraint M 0 \u2264 \u03ba imposed by the operational constraints (e.g., model size requirements). We wish to find the pruning mask M that incurs the minimum 2 distortion in the output given the worst-case 2 -bounded input, i.e.,\nmin M binary M 0 \u2264\u03ba sup \u03be 2\u22641 W \u03be \u2212 (M W )\u03be 2 .(4)\nThe minimax distortion (4) upper-bounds the minimum expected 2 distortion for any distribution of \u03be supported on the unit ball, and thus can be viewed as a data-oblivious version of the pruning algorithms designed for loss minimization (using squared loss). By the definition of the spectral norm, 5 Eq. 4 is equivalent to\nmin M binary M 0 \u2264\u03ba W \u2212 M W ,(5)\nwhere \u00b7 denotes the spectral norm. Using the fact that A \u2264 A F holds for any matrix A 6 (where \u00b7 F denotes the Frobenius norm), the optimization (5) can be relaxed to the Frobenius 4 An extension to convolutional neural network is straightforward; see, e.g., (Sedghi et al., 2019). 5 which is the operator norm with respect to the 2 norms, i.e., sup \u03be =0 W \u03be 2 \u03be 2 , 6 The inequality is a simple consequence of the Cauchy-Schwarz inequality:\nAx 2 2 = i ( j Aijxj) 2 \u2264 i ( j A 2 ij ) \u00b7 ( j x 2 j ) = A 2 F x 2 2 , where subscripts denote weight indices. distortion minimization min M binary M 0 \u2264\u03ba W \u2212 M W F = min M binary M 0 \u2264\u03ba i\u2208{1,...,m} j\u2208{1,...,n} (1 \u2212 M ij )W 2 ij ,(6)\nwhere W ij , M ij denote (i, j)-th entries of W, M , respectively. From the right-hand side of Eq. 6, we see that the layerwise MP, i.e., setting M ij = 1 for (i, j) pairs with top-\u03ba largest W ij , is the optimal choice to minimize the Frobenius distortion incurred by pruning. This observation motivates us to view the layerwise MP as the (approximate) solution of the output 2 distortion minimization procedure, and speculate the connection between the small output 2 distortion and the favorable performance of the pruned-retrained subnetwork (given the unreasonable effectiveness of seeminglyna\u00efve MP as demonstrated by Gale et al. (2019)).\n\nLAMP: greedy, relaxed minimization of model output distortion. Building on this speculation, we now ask the following question: \"How can we select the layerwise sparsity of MP to have small model-level output distortion?\" To formalize, we consider the minimization\nmin d i=1 M (i) 0\u2264\u03ba sup x 2\u22641 f (x; W (1:d) ) \u2212 f (x; W (1:d) ) 2 ,(7)\nwhere \u03ba denotes the model-level sparsity constraint imposed by the operational requirements and W (i) := M (i) W (i) denotes the pruned version of the i-th layer weight matrix.\n\nDue to the nonlinearities from the activation functions, it is difficult to solve Eq. 7 exactly. Instead, we consider the following greedy procedure: At each step, we (a) approximate the distortion incurred by pruning a single connection, (b) remove the connection with the smallest score, and then (c) go back to step (a) and re-compute the scores based on the pruned model.\n\nOnce we assume that only one connection is pruned at a single iteration of the step (a), we can use the following upper bound of the model output distortion to relax the optimization (7):\nWith W i denoting a pruned version of W i , we have sup x 2\u22641 f (x; W (1:d) ) \u2212 f (x; W (1:i\u22121) , W (i) , W (i+1:d) ) 2 \u2264 W (i) \u2212 W (i) F W (i) F \uf8eb \uf8ed d j=1 W (j) F \uf8f6 \uf8f8(8)\n(see Appendix C for a derivation). Despite the sub-optimalities from the relaxation, considering the right-hand side of Eq. 8 provides two favorable properties. First, the right-hand side is free of any activation function, and is equivalent to the layerwise MP. Second, the score can be computed in advance, i.e., does not require re-computing after pruning each connection. In particular, the product term d j=1 W (j) F does not affect the pruning decision, and the denominator can be pre-computed with the cumulative sum v\u2265u (W (i) [v]) 2 for each index u for W (i) . This computational trick leads us to the LAMP score (1).\n\n\nEXPERIMENTS & ANALYSES\n\nTo empirically validate the effectiveness of the proposed method, we compare LAMP with following layerwise sparsity selection schemes for magnitude-based pruning:\n\n\u2022 Global. A global threshold on the weight magnitudes is imposed on every layer to meet the global sparsity constraint, and the layerwise sparsity is automatically determined according to this threshold; see, e.g., Morcos et al. (2019).\n\n\u2022 Uniform. Every layer is pruned to have identical layerwise sparsity levels, which is in turn equal to the global sparsity constraint; see, e.g., Zhu & Gupta (2018).\n\n\u2022 Uniform+. Same as Uniform, but we impose two additional constraints: (1) we keep the first convolutional layer unpruned, and (2)  \u2022 Erd\u0151s-R\u00e9nyi kernel. An extension of Erd\u0151s-R\u00e9nyi method (originally given by Mocanu et al. (2018)) accounting for convolutional layers, as proposed by Evci et al. (2020). The numbers of nonzero parameters of sparse convolutional layers are scaled proportional to 1 \u2212 n l\u22121 +n l +w l +h l n l\u22121 \u00b7n l \u00b7w l \u00b7h l , where n l denotes the number of neurons at layer l, and w l , h l denotes the width and height of the lth layer convolutional kernel.\n\nAs a default setup, we perform five independent trials for each baseline method, where in each trial we use iterative pruning-and-retraining (Han et al., 2015): we prune 20% of the surviving weights at each iteration. For the Restricted-ImageNet experiment, we provide the result from four trials. For a clear presentation, we only report the average on the figures appearing in the main text. Standard deviations for five-seed results in Section 4.1 will be given in Appendix E. Also, in Appendix D, we report additional experimental results for language modeling tasks (Penn Treebank and WT-2) on Transformers (Vaswani et al., 2017).\n\nSummary of observations. From the experimental results (Figs. 2 to 4), we observe that LAMP consistently outperforms all other baselines, in terms of the sparsity-accuracy tradeoff. The performance gap between LAMP and baseline methods seems be more pronounced with modern network architectures, e.g., EfficientNet-B0. We also observe that LAMP performs well under weight rewinding setup, while the strongest baseline (Erd\u0151s-R\u00e9nyi kernel) seems to be sensitive to such rewinding.\n\n\nMAIN RESULTS\n\nOur main experimental results are on image classification models. We explore a diverse set of model architectures and datasets, with a base setup of VGG-16 (Simonyan & Zisserman, 2015) trained on CIFAR-10 (Krizhevsky & Hinton, 2009) dataset. In particular, our experiments cover the following models and datasets. Other details. Detailed experimental setup is given in Appendix A.\n\nIn Fig. 2, we provide sparsity-accuracy tradeoff curves for four different model architectures trained on CIFAR-10 dataset. The first observation is that LAMP achieves the best tradeoff; in all four models, LAMP consistently outperforms baseline methods. We also observe that Erd\u0151s-R\u00e9nyi kernel method also outperforms other baselines in VGG-16, ResNet-20, and EfficientNet-B0, but fails to do so on DenseNet-121. Furthermore, the gap between LAMP and the Erd\u0151s-R\u00e9nyi kernel method seems to widen as the model architecture gets more complicated; the gap between two methods is notable especially in EfficientNet-B0, where mobile inverted bottleneck convolutional layers replace traditional convolutional modules. In particular, LAMP achieves 88.1% test accuracy when only 1.44% of all weights survive, while Erd\u0151s-R\u00e9nyi kernel achieves 77.8%. Lastly, we observe that the heuristic of Gale et al. (2019) seems to provide an improvement over the Uniform MP.\n\nIn Fig. 3, we present the tradeoff curves for three additional datasets: SVHN, CIFAR-100, and Restricted ImageNet. Similar to Fig. 2, we observe that LAMP outperforms all other baselines and Erd\u0151s-R\u00e9nyi kernel remains to be the most competitive baseline.\n\n\nABLATIONS: ONE-SHOT PRUNING, WEIGHT REWINDING, AND SNIP\n\nModern magnitude-based pruning algorithms are often used in combination with customized pruning schedules (e.g., Zhu & Gupta (2018)) or weight rewinding (e.g., Frankle & Carbin (2019); Renda et al. (2020)). As a sanity check that LAMP perform reliably well along with such techniques, we conduct following additional experiments.\n\n\u2022 One-shot pruning. As an extreme case of pruning schedule, we test the scheme where we only run a single training-pruning-retraining cycle, instead of iterating multiple cycles. We test the one-shot pruning on VGG-16 trained with CIFAR-10 dataset. \u2022 Weight rewinding. After pruning, we rewind the remaining weights to their values in the early epoch, as in the \"lottery ticket\" experiments by Frankle & Carbin (2019). We perform iterative magnitude-based pruning (IMP) on VGG-16, using the warm-up step and the training schedule described in . \u2022 SNIP. As an additional experiment, we test whether LAMP can provide a general-purpose layerwise sparsity for pruning schemes other than MP. We test under the \"pruning at initialization\" setup with SNIP scores (Lee et al., 2019). Baselines are similarly modified to use the SNIP score. We use Conv-6 model on CIFAR-10 dataset (see Frankle & Carbin (2019) for more details of the model) with a batch size of 128. Results for all three experiments are given in Fig. 4. On one-shot pruning, we confirm that LAMP comfortably leads other baselines, as in the iterative pruning case. We note that the gap between one-shot pruning and iterative pruning is quite small for LAMP; when 1.15% of all prunable weights survive, iterative LAMP brings only 1.09% accuracy gain over one-shot LAMP. By contrast, the gain from iteration for Uniform MP is 41.62% at the same sparsity level.\n\nOn the weight-rewinding experiment, we observe that LAMP remains beneficial over the baseline methods. We also remark that the Global baseline tend to perform well in this case, even outperforming the Erd\u0151s-R\u00e9nyi kernel method in the low-sparsity regime. This phenomenon seems to be connected to the observation of Zhou et al. (2019) that the initial weights and final weights of a model are highly correlated; global MP may help preserving connections with larger initial magnitudes, which play important roles in terms of signal propagation at initialization .\n\nOn the SNIP experiment, we observe that LAMP achieves a similar performance to the Global SNIP.\n\nRecalling that the SNIP score is designed for global pruning (Lee et al., 2019), such high performance of LAMP is unexpected. We suspect that this is because LAMP is also designed for \"output distortion minimization,\" which shares a similar spirit with the \"gradient distortion minimization.\"\n\n\nLAYERWISE SPARSITY: GLOBAL MP, ERD\u0150S-R\u00c9NYI KERNEL, AND LAMP\n\nWith the effectiveness of LAMP confirmed, we take a closer look at the layerwise sparsity discovered by LAMP. We focus on answering two questions: Q1. Does layerwise sparsity distilled from LAMP behave similarly to the heuristics constructed from experiences, e.g. the one given by Gale et al.\n\n(2019)? Q2. Is there any other defining characteristic of LAMP sparsity patterns, which may help us guide the design of (sparse) network architectures?\n\nIn Fig. 5, we plot the layerwise survival rates and the number of nonzero weights discovered by iteratively pruning VGG-16 (trained on CIFAR-10), by Global MP, Erd\u0151s-R\u00e9nyi kernel, and LAMP. Layerwise survival rates are given for the global survival rates of {51.2%, 26.2%, 13.4%, 6.87%, 3.52%} (from lighter to darker). Number of nonzero weights are plotted for the pruned models with total {3.52%, 1.80%, 0.92%, 0.47%, 0.24%} fraction of all weights surviving.\n\nWe observe that LAMP sparsities share a similar tendency to sparsity levels given by the Erd\u0151s-R\u00e9nyi kernel method. In particular, both methods tend to keep the first and layer layers of the neural network relatively dense; this property is reminiscent of the handcrafted heuristic given in Gale et al. (2019): keep the first convolutional layer unpruned, and prune at most 80% from the last fully-connected layer. While Global MP also keeps a large fraction of the last fully-connected layer unpruned, the first convolutional layer gets pruned quickly. LAMP sparsities differ from Erd\u0151s-R\u00e9nyi kernel sparsities in two senses.\n\n\u2022 Although LAMP demonstrates its tendency to keep the first and the last layers relatively unpruned, this tendency is softer. When 3.52% of weights survive, LAMP keeps \u223c 79% and \u223c 62% of weights unpruned from the first and last layers (respectively), while Erd\u0151s-R\u00e9nyi kernel does not prune any weight from those two layers.  \u2022 LAMP tends to keep the number of nonzero weights relatively uniform throughout the layers at extreme sparsity levels (indeed, the first observation can be understood as a consequence of the second observation). In contrast, Erd\u0151s-R\u00e9nyi kernel method keeps the relative ratio constant, regardless of the global sparsity level.\n\nFollowing the second observation, we conjecture that having a similar number of nonzero connections in each layer may be an essential condition to guarantee maximal memory capacity (see, e.g., Yun et al. (2019)), given a global sparsity constraint on the neural network. Theoretical investigations of the approximability by such sparse neural networks may be an interesting future research direction, potentially leading to a more principled and robust pruning algorithms.\n\nAs an additional remark, we note that the layerwise sparsity discovered by LAMP behaves similarly to that of AMC (He et al., 2018), which uses a reinforcement learning agent to search over the space of all available layerwise sparsity. We provide further details in Appendix F.\n\n\nCONCLUSION\n\nIn this paper, we investigate an under-explored problem on the layerwise sparsity for magnitude-based pruning scheme. The proposed method, coined LAMP (Layer-Adaptive Magnitude-based Pruning), is motivated from the 2 distortion minimization perspective on magnitude-based pruning, and provides a consistent performance gain on a wide range of models and datasets. Furthermore, LAMP performs reliably well when combined with one-shot pruning schedule or weight rewinding, which makes it an attractive candidate as a \"go-to\" layerwise sparsity for the magnitude-based pruning. Taking a deeper look at LAMP-discovered layerwise sparsities, we observe that LAMP automatically recovers the handcrafted rules for the layerwise sparsity. Furthermore, we observe that LAMP tend to keep the number of nonzero weights relatively uniform throughout the layers, as we consider more extreme sparsity levels. We conjecture that such uniformity in the number of unpruned weights may be an essential condition for a maximal expressive power of sparsified neural networks. \n\n\nACKNOWLEDGMENTS\n\n\nA EXPERIMENTAL SETUPS\n\nFor any implementational details not given in this section, we refer to the code at:\n\nhttps://github.com/jaeho-lee/layer-adaptive-sparsity\n\nOptimizer. With an exception of the weight rewinding experiment, we use AdamW (Loshchilov & Hutter, 2019) with learning rate 0.0003; we use vanilla Adam with learning rate 0.0003 for the weight rewinding experiment, following the setup of Frankle & Carbin (2019). For other hyperparameters, we follow the PyTorch default setup: \u03b2 = (0.9, 0.999), wd = 0.01, \u03b5 = 10 \u22128 .\n\nPre-processing. CIFAR-10/100 dataset is augmented with random crops with a padding of 4 and random horizontal flips. We normalize both training and test datasets with constants SVHN training dataset is augmented with random crops with a padding of 2. We normalize both training and test datasets with constants (0.4377, 0.4438, 0.4728), (0.198, 0.201, 0.197).\n\nRestricted ImageNet training dataset is augmented with Random resized crop to size 224 and random horizontal flips. Restricted ImageNet test dataset is resized to 256 and then center-cropped to size 224. We normalize both training and test datasets with constants Models. Some of the models used for CIFAR-10 datasets are originally developed for ImageNet: VGG-16, DenseNet-121, EfficientNet-B0. The models are adapted for CIFAR-10/100 datasets by modifying only the average pooling and the (first) fully-connected layer (and not convolutional layers) to fit the 32 \u00d7 32 resolution of the input image. \n\n\nB COMPUTATIONAL AND IMPLEMENTATIONAL ASPECTS OF LAMP\n\nIn this section, we discuss the computational complexity of LAMP and describe how LAMP can be implemented. We consider a depth-d neural network, with n i number of connections in i-th layer. We also let n = d i=1 n i . The global MP is comprised of two steps:\n\n\u2022 Step 1. Pool and sort the weight magnitudes of all connections, taking O(n log n) operations.\n\n\u2022 Step 2. Assign 0s to connections with smallest weights until the desired sparsity level is achieved, taking O(n) computations.\n\nThe total computational cost is of order O(n log n). Similarly, we see that performing MP with any pre-determined sparsity levels incurs O( d i=1 n i log n i ) computations. LAMP, on the other hand, can be done in four steps.\n\n\u2022 Step 1. Weight magnitudes are squared and sorted for each layer, which can be done with a computation cost of O(\nd i=1 n i log n i ). \u2022 Step 2. LAMP score denominators v\u2265u W 2 i [v]\nfor each layer is computed by summing-andstoring the squared weight magnitudes according to a descending order; takes O(n) computations.\n\n\u2022 Step 3. The LAMP score is computed by dividing squared weight magnitudes by the denominators, using O(n) steps.\n\n\u2022\n\nStep 4. We sort and prune as in global MP, taking O(n log n) steps. 7\n\nWe observe that step 4 is the dominant term, which is shared by the global MP. The first three steps can be easily implemented in PyTorch as follows.  (8) In this section, we prove the following inequality.\nsup x 2\u22641 f (x; W (1:d) ) \u2212 f (x; W (1:i\u22121) , W (i) , W (i+1:d) ) 2 \u2264 W (i) \u2212 W (i) F \u00b7 j =i W (j) F .(9)\nThis inequality is a simplified and modified version of what is popularly known as \"peeling\" procedure in the generalization literature (e.g., (Neyshabur et al., 2015)), and we present the proof only for the sake of completeness. We begin by peeling the outermost layer as\nf (x; W (1:d) ) \u2212 f (x; W (1:i\u22121) , W (i) , W (i+1:d) ) 2 (10) = W (d) \u03c3(f (x; W (1:d\u22121) ) \u2212 \u03c3(f (x; W (1:i\u22121) , W (i) , W (i+1:d\u22121) )) 2 (11) \u2264 W (d) F \u00b7 \u03c3(f (x; W (1:d\u22121) ) \u2212 \u03c3(f (x; W (1:i\u22121) , W (i) , W (i+1:d\u22121) )) 2 (12) \u2264 W (d) F \u00b7 f (x; W (1:d\u22121) ) \u2212 f (x; W (1:i\u22121) , W (i) , W (i+1:d\u22121) ) 2 ,(13)\nwhere we have used Cauchy-Schwarz inequality for the first inequality, and the 1-Lipschitzness of ReLU activation with respect to 2 norm. We keep on peeling until we get\nf (x; W (1:d) ) \u2212 f (x; W (1:i\u22121) , W (i) , W (i+1:d) ) 2 (14) \u2264 j>i W (j) F \u00b7 f (x; W (1:i) ) \u2212 f (x; W (1:i\u22121) , W (i) ) 2 .(15)\nThe second multiplicative term on the right-hand side requires a slightly different treatment. Via Cauchy-Schwarz, we get\nf (x; W (1:i) ) \u2212 f (x; W (1:i\u22121) , W (i) ) 2 = W (i) \u2212 W (i) \u03c3(f (x; W (1:i\u22121) )) 2 (16) \u2264 W (i) \u2212W (i) F \u00b7 \u03c3(f (x; W (1:i\u22121) )) 2 .(17)\nThe activation functions from this point require zero-in-zero-out (ZIZO; \u03c3(0) = 0) property to be peeled, in addition to the Lipschitzness. Indeed, we can proceed as\n\u03c3(f (x; W (1:i\u22121) )) 2 = \u03c3(f (x; W (1:i\u22121) )) \u2212 \u03c3(0) 2 \u2264 f (x; W (1:i\u22121) ) \u2212 0 2 (18) = f (x; W (1:i\u22121) ) 2 .(19)\nIteratively applying the Cauchy-Schwarz inequality and the step Eq. (19), we arrive at Eq. (9).\n\n\nD EXPERIMENTAL RESULTS ON LANGUAGE MODELING\n\nIn this section, we apply the proposed LAMP and baseline layerwise sparsities on language modeling tasks. We note, however, that it is still unclear whether magnitude-based pruning approaches achieve state-of-the-art results for pruning natural language processing tasks (see, e.g., Sanh et al. (2020), for more discussions). In particular, NLP model architectures (e.g., embedding layers) and learning pipelines (e.g., more prevalent use of pre-trained models) add more complications to model pruning, obscuring how pruning algorithms should be fairly compared. We also note that we do not test Uniform+ baseline for this task, as the heuristic is proposed explicitly for image classification models, not language models (Gale et al., 2019).\n\nModel. We consider the simplified version of Transformer-XL (Dai et al., 2018) architecture, which has six self-attention layers (original has 16) and removed some regularizing heuristics, such as exponential moving average and cosine annealing. Similar to the setting of Sanh et al. (2020), we focus on pruning self-attention layers of the model (approximately 7.57M parameters).\n\nOptimizer. We use AdamW (Loshchilov & Hutter, 2019) with learning rate 0.0003, following the PyTorch default setup for other hyperparameters: \u03b2 = (0.9, 0.999), wd = 0.01, \u03b5 = 10 \u22128 . We use batch size 20 and maximum sequence length 70. We train the initial unpruned model for 200,000 iterations and re-train the model after pruning for 50,000 iterations.\n\nDatasets. We use Penn Treebank (Marcus et al., 1993) and WikiText-2 (Merity et al., 2016) datasets.\n\nWe provide the experimental results in Tables 2 and 3. We observe that LAMP consistently achieves the near-best performance among all considered methods. We note, however, that the gain is marginal. We suspect that this marginal-ity is due to the fact that widths of the Transformer layers are relatively uniform compared to image classification models. We do not observe any notable pattern among the baseline methods.     \n\n\nF PEAKS AND CRESTS OF LAMP SPARSITY\n\nIn this section, we compare the layerwise sparsity discovered by LAMP (and other non-uniform baselines) to the sparsity discovered by AMC (He et al., 2018), which uses a reinforcement learning agent to explicitly search for the optimal layerwise sparsity. In particular, we focus on whether the \"peaks and crests\" phenomenon observed for AMC-discovered layerwise sparsity also takes place in the layerwise sparsity induced by LAMP: He et al. (2018) observe that the layerwise sparsity of ResNet-50 (trained on ImageNet) and pruned by AMC exhibits what they call peaks and crests, i.e., the sparsity oscillates between high and low periodically; see Figure 3 of He et al. (2018). The authors speculate that such phenomenon takes place, because AMC automatically learns that 3 \u00d7 3 convolution has more redundancy than 1 \u00d7 1 convolution and can be pruned more.\n\nTo see if LAMP also automatically discovers such \"peaks and crests,\" we prune the ImageNetpretrained ResNet-50 model with one-shot LAMP, global MP, and Erd\u00f6s-R\u00e9nyi kernel. The layerwise survival ratios are reported in Fig. 6. From the figure, we observe that LAMP also discovers different sparsity ratio for 1 \u00d7 1 convolution and 3 \u00d7 3 convolution layers; 3 \u00d7 3 convolutional filters are pruned more. Such pattern is more noticeable in the later layers, where more weights are pruned. In other words, LAMP discovers a layerwise sparsity similar to that of AMC, even without training a reinforcement learning agent. We note, however, a slight discrepancy in the setting: AMC reports the sparsity from an iterative pruning setup, and the results in Fig. 6 are from a one-shot pruning setup.\n\nIn the model pruned with global MP, such pattern does not stand out. In the model pruned with Erd\u00f6s-R\u00e9nyi kernel method, peaks and crests are extremely evident, even more than those discovered by AMC. \n\n\nG LAMP WITH AN EXTENSIVE TRAINING SCHEDULE\n\nIn this section, we depart from the standardized training setup in the main text which uses Adam with weight decay Loshchilov & Hutter (2019), without any learning rate decay. Instead, we validate the effectiveness of LAMP using the optimizer and hyperparameters extensively tuned for the maximum performance of the model. In particular, we use configurations from Liu et al. (2019) for training VGG-16 on the CIFAR-10 dataset. The experimental result is reported in Fig. 7. Similar to the experiments appearing in the main text, we observe that LAMP continues to outperform or match the performances of the baselines.\n\nFigure 2 :\n2retain at least 20% of connections alive in the last fullyconnected layer; this heuristic rule is proposed byGale et al. (2019). Sparsity-accuracy tradeoff curves of VGG-16, ResNet-18, DenseNet-121, and EfficientNet-B0. All models are iteratively pruned and retrained with CIFAR-10 dataset.\n\nFigure 3 :\n3Sparsity-accuracy tradeoff curves of pruned models trained for SVHN and CIFAR-100 (on VGG-16) and Restricted ImageNet (on ResNet-34). Models. We consider four network architectures for image classification experiments: (1) VGG-16 (Simonyan & Zisserman, 2015) adapted for CIFAR-10 to have batch normalization layers and one fully-connected layer (as used in Liu et al. (2019); Frankle & Carbin (2019)); (2) ResNet-20/34 (He et al., 2016); (3) DenseNet-121 (Huang et al., 2017); (4) EfficientNet-B0 (Tan & Le, 2019). For all four models, we prune the weight tensors for fully-connected and convolutional units. Biases and batch normalization layers are kept unpruned.Datasets. We consider the following datasets; CIFAR-10/100 (Krizhevsky & Hinton, 2009), SVHN(Netzer et al., 2011), and Restricted ImageNet (Tsipras et al., 2019). All datasets except for Restricted ImageNet are used for training VGG-16; Restricted ImageNet is used for training ResNet-34.\n\nFigure 4 :\n4Sparsity-accuracy tradeoff curves under one-shot pruning, weight rewinding, and the SNIP setup. One-shot pruning and the weight-rewinding experiments are done on VGG-16 trained on CIFAR-10 dataset. SNIP experiment is performed on Conv-6 trained on CIFAR-10.Again, other experimental details are given at the Appendix A.\n\nFigure 5 :\n5Layerwise statistics of VGG-16 iteratively pruned on CIFAR-10.\n\n\nThis work was supported in part by Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)) in part by Samsung Advanced Institute of Technology (SAIT), and in part by the Defense Challengeable Future Technology Program of the Agency for Defense Development, Republic of Korea. M. Tan and Q. V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In Proceedings of International Conference on Machine Learning, 2019. D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations, 2019. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017. X. Xiao, Z. Wang, and S. Rajasekaran. AutoPrune: Automatic network pruning by regularizing auxiliary parameters. In Advances in Neural Information Processing Systems, 2019. C. Yun, S. Sra, and A. Jadbabaie. Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity. In Advances in Neural Information Processing Systems, 2019. H. Zhou, J. Lan, R. Liu, and J. Yosinki. Deconstructing lottery tickets: Zeros, signs, and supermasks. In Advances in Neural Information Processing Systems, 2019. M. Zhu and S. Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. In ICLR Workshop track, 2018.\n\nFigure 6 :\n6Layerwise survival rates of one-shot pruned ResNet-50 trained on ImageNet dataset. The global survival rates are {80.0%, 41.0%, 21.0%} (from lighter to darker).\n\nFigure 7 :\n7Performance of LAMP and baselines on VGG-16 trained on CIFAR-10 dataset, using the training schedules fromLiu et al. (2019).\n\nTable 1 :\n1Optimization details.Dataset \nModel \nInitial training iter. Re-training iter. Batch size \n\nSVHN \nVGG-16 \n40000 \n30000 \n100 \nCIFAR-10 \n{VGG-16, EfficientNet-B0} \n50000 \n40000 \n100 \nCIFAR-10 \nDenseNet-121 \n80000 \n60000 \n100 \nCIFAR-100 \nVGG-16 \n60000 \n50000 \n100 \nRestricted ImageNet ResNet-34 \n80000 \n80000 \n128 \n\nCIFAR-10 \nConv-6 (SNIP) \n50000 \n40000 \n128 \n\n\nTable 2 :\n2Test perplexities of Transformer-XL iteratively pruned and trained on Penn Treebank. Bold denotes the pruning scheme with accuracy within one standard deviation from the highest. 38\u00b10.39 71.78\u00b10.34 73.23\u00b10.38 73.67\u00b10.10 76.23\u00b10.43 78.21\u00b10.31 80.92\u00b10.10 83.98\u00b10.36 88.62\u00b10.25 92.30\u00b10.54 Uniform 71.57\u00b10.46 71.74\u00b10.39 73.27\u00b10.36 74.18\u00b10.34 76.98\u00b10.41 79.25\u00b10.29 81.79\u00b10.13 84.95\u00b10.35 89.41\u00b10.39 92.44\u00b10.69 Erd\u0151s-R\u00e9nyi ker. 71.53\u00b10.33 71.94\u00b10.20 73.56\u00b10.31 74.43\u00b10.20 77.15\u00b10.33 79.38\u00b10.26 82.03\u00b10.13 85.01\u00b10.38 89.34\u00b10.31 92.75\u00b10.55 LAMP (Ours) 71.51\u00b10.38 71.79\u00b10.24 73.11\u00b10.37 73.74\u00b10.21 76.23\u00b10.27 78.27\u00b10.18 80.73\u00b10.09 83.60\u00b10.35 88.06\u00b10.24 91.24\u00b10.38Surv. weights \n26.2% \n21.0% \n16,8% \n13.4% \n10.8% \n8.59% \n6.87% \n5.50% \n4.40% \n3.52% \n\nGlobal \n71.\n\nTable 3 :\n3Test perplexities of Transformer-XL iteratively pruned and trained on WT-2. Bold denotes the pruning scheme with accuracy within one standard deviation from the highest. 62\u00b10.28 90.43\u00b10.58 91.35\u00b10.21 94.65\u00b10.30 96.34\u00b10.35 100.07\u00b10.48 103.56\u00b10.48 107.95\u00b10.74 112.20\u00b10.34 117.03\u00b10.63 Uniform 87.88\u00b10.26 90.46\u00b10.69 91.58\u00b10.29 95.01\u00b10.21 96.72\u00b10.79 100.83\u00b10.32 104.25\u00b10.28 108.31\u00b10.53 112.71\u00b10.25 117.01\u00b10.50 Erd\u0151s-R\u00e9nyi ker. 88.01\u00b10.23 90.62\u00b10.64 91.99\u00b10.28 95.40\u00b10.36 97.13\u00b10.72 100.83\u00b10.38 104.49\u00b10.51 108.51\u00b10.90 112.56\u00b10.47 116.69\u00b10.34 LAMP (Ours) 87.73\u00b10.26 90.76\u00b10.54 91.47\u00b10.33 94.58\u00b10.30 96.10\u00b10.53 99.62\u00b10.19 103.01\u00b10.22 106.60\u00b10.46 110.87\u00b10.63 115.31\u00b10.64 E DETAILED EXPERIMENTAL RESULTS (WITH STANDARD DEVIATIONS)Surv. weights \n26.2% \n21.0% \n16,8% \n13.4% \n10.8% \n8.59% \n6.87% \n5.50% \n4.40% \n3.52% \n\nGlobal \n87.\n\nTable 4 :\n4Test accuracies of VGG-16 iteratively pruned and trained on CIFAR-10. Bold denotes the pruning scheme with accuracy within one standard deviation from the highest. Erd\u0151s-R\u00e9nyi ker. 92.34\u00b10.25 91.99\u00b10.14 91.66\u00b10.29 91.15\u00b10.24Surv. weights \n6.87% \n4.40% \n2.81% \n1.80% \n1.15% \n0.74% \n0.47% \n0.30% \n0.19% \n0.12% \n\nGlobal \n91.30\u00b10.23 90.80\u00b10.23 89.28\u00b10.30 85.55\u00b12.38 \n81.56\u00b13.73 54.58\u00b122.07 41.91\u00b118.63 31.93\u00b121.24 21.87\u00b121.21 \n11.72\u00b12.29 \nUniform \n91.47\u00b10.19 90.78\u00b10.12 88.61\u00b11.09 84.17\u00b14.46 55.68\u00b112.20 38.51\u00b124.83 \n26.41\u00b18.83 \n16.75\u00b16.62 \n11.58\u00b12.79 \n9.95\u00b10.28 \nUniform+ \n91.54\u00b10.19 91.20\u00b10.28 90.16\u00b10.12 89.44\u00b10.15 \n87.85\u00b10.26 \n86.53\u00b10.05 \n84.84\u00b10.41 \n82.41\u00b10.44 \n74.54\u00b15.41 24.46\u00b114.98 \n90.55\u00b10.19 \n89.51\u00b10.50 \n88.21\u00b10.38 \n86.73\u00b10.25 \n84.85\u00b10.28 \n81.50\u00b10.42 \n\nLAMP (Ours) \n92.24\u00b10.24 92.06\u00b10.21 91.71\u00b10.30 91.66\u00b10.21 \n91.07\u00b10.40 \n90.49\u00b10.21 \n89.64\u00b10.32 \n88.75\u00b10.29 \n87.07\u00b10.37 \n84.90\u00b10.59 \n\n\n\nTable 5 :\n5Test accuracies of ResNet-20 iteratively pruned and trained on CIFAR-10. Bold denotes the pruning scheme with accuracy within one standard deviation from the highest.\ni.e., using a global threshold for LAMP score\nThis \"order\" among weights is required to handle the weights with equal magnitude.\nThis cost can be further reduced, recalling that the LAMP scores in each layer are already sorted. All that remains is a merging step.\n\nNet-trim: Convex pruning of deep neural networks with performance guarantee. A Aghasi, A Abdi, N Nguyen, J Romberg, Advances in Neural Information Processing Systems. A. Aghasi, A. Abdi, N. Nguyen, and J. Romberg. Net-trim: Convex pruning of deep neural networks with performance guarantee. In Advances in Neural Information Processing Systems, 2017.\n\nData-dependent coresets for compressing neural networks with applications to generalization bounds. C Baykal, L Liebenwein, I Gilitschenski, D Feldman, D Rus, International Conference on Learning Representations. C. Baykal, L. Liebenwein, I. Gilitschenski, D. Feldman, and D. Rus. Data-dependent coresets for compressing neural networks with applications to generalization bounds. In International Conference on Learning Representations, 2019.\n\nCompressing neural networks using the variational information bottleneck. B Dai, C Zhu, B Guo, D Wipf, Proceedings of International Conference on Machine Learning. International Conference on Machine LearningB. Dai, C. Zhu, B. Guo, and D. Wipf. Compressing neural networks using the variational information bottleneck. In Proceedings of International Conference on Machine Learning, 2018.\n\nLearning to prune deep neural networks via layer-wise optimal brain surgeon. X Dong, S Chen, S J Pan, Advances in Neural Information Processing Systems. X. Dong, S. Chen, and S. J. Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. In Advances in Neural Information Processing Systems, 2017.\n\nFast sparse ConvNets. E Elsen, M Dukhan, T Gale, K Simonyan, IEEE Conference on Computer Vision and Pattern Recognition. E. Elsen, M. Dukhan, T. Gale, and K. Simonyan. Fast sparse ConvNets. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.\n\nRigging the lottery: Making all tickets winners. U Evci, T Gale, J Menick, P S Castro, E Elsen, Proceedings of International Conference on Machine Learning. International Conference on Machine LearningU. Evci, T. Gale, J. Menick, P. S. Castro, and E. Elsen. Rigging the lottery: Making all tickets winners. In Proceedings of International Conference on Machine Learning, 2020.\n\nThe lottery ticket hypothesis: Finding sparse, trainable neural networks. J Frankle, M Carbin, International Conference on Learning Representations. J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019.\n\nLinear mode connectivity and the lottery ticket hypothesis. J Frankle, G K Dziugaite, D M Roy, M Carbin, Proceedings of International Conference on Machine Learning. International Conference on Machine LearningJ. Frankle, G. K. Dziugaite, D. M. Roy, and M. Carbin. Linear mode connectivity and the lottery ticket hypothesis. In Proceedings of International Conference on Machine Learning, 2020.\n\nA signal propagation perspective for pruning neural networks at initialization. N Lee, T Ajanthan, S Gould, P H S Torr, International Conference on Learning Representations. N. Lee, T. Ajanthan, S. Gould, and P. H. S. Torr. A signal propagation perspective for pruning neural networks at initialization. In International Conference on Learning Representations, 2020.\n\nRuntime neural pruning. J Lin, Y Rao, J Lu, J Zhou, Advances in Neural Information Processing Systems. J. Lin, Y. Rao, J. Lu, and J. Zhou. Runtime neural pruning. In Advances in Neural Information Processing Systems, 2017.\n\nDynamic model pruning with feedback. T Lin, S U Stich, L Barba, D Dmitriev, M Jaggi, International Conference on Learning Representations. T. Lin, S. U. Stich, L. Barba, D. Dmitriev, and M. Jaggi. Dynamic model pruning with feedback. In International Conference on Learning Representations, 2020.\n\nRethinking the value of network pruning. Z Liu, M Sun, T Zhou, G Huang, T Darrell, International Conference on Learning Representations. Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell. Rethinking the value of network pruning. In International Conference on Learning Representations, 2019.\n\nDecoupled weight decay regularization. I Loshchilov, F Hutter, International Conference on Learning Representations. I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.\n\nBayesian compression for deep learning. C Louizos, K Ullrich, M Welling, Advances in Neural Information Processing Systems. C. Louizos, K. Ullrich, and M. Welling. Bayesian compression for deep learning. In Advances in Neural Information Processing Systems, 2017.\n\nLearning sparse neural networks through l 0 regularization. C Louizos, M Welling, D P Kingma, International Conference on Learning Representations. C. Louizos, M. Welling, and D. P. Kingma. Learning sparse neural networks through l 0 regularization. In International Conference on Learning Representations, 2018.\n\nBuilding a larger annotated corpus of English: The Penn treebank. M Marcus, B Santorini, M Marcinkiewicz, Computational Linguistics. M. Marcus, B. Santorini, and M. Marcinkiewicz. Building a larger annotated corpus of English: The Penn treebank. Computational Linguistics, 1993.\n\nPointer sentinel mixture models. S Merity, C Xiong, J Bradbury, R Socher, International Conference on Learning Representations. S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2016.\n\nScalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science. D C Mocanu, E Mocanu, P Stone, P H Nguyen, M Gibescu, A Liotta, Nature Communications. D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu, and A. Liotta. Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science. Nature Communications, 2018.\n\nVariational dropout sparsifies deep neural networks. D Molchanov, A Ashukha, D Vetrov, Proceedings of International Conference on Machine Learning. International Conference on Machine LearningD. Molchanov, A. Ashukha, and D. Vetrov. Variational dropout sparsifies deep neural networks. In Proceedings of International Conference on Machine Learning, 2017.\n\nOne ticket to win them all: Generalizing lottery ticket initializations across datasets and optimizers. A S Morcos, H Yi, M Paganini, Y Tian, Advances in Neural Information Processing Systems. A. S. Morcos, H. Yi, M. Paganini, and Y. Tian. One ticket to win them all: Generalizing lottery ticket initializations across datasets and optimizers. In Advances in Neural Information Processing Systems, 2019.\n\nSkeletonization: A technique for trimming the fat from a network via relevance assessment. M C Mozer, P Smolensky, Advances in Neural Information Processing Systems. M. C. Mozer and P. Smolensky. Skeletonization: A technique for trimming the fat from a network via relevance assessment. In Advances in Neural Information Processing Systems, 1988.\n\nData-indenpendent neural pruning via coresets. B Mussay, M Osadchy, V Braverman, S Zhou, D Feldman, International Conference on Learning Representations. B. Mussay, M. Osadchy, V. Braverman, S. Zhou, and D. Feldman. Data-indenpendent neural pruning via coresets. In International Conference on Learning Representations, 2020.\n\nReading digits in natural images with unsupervised feature learning. Y Netzer, T Wang, A Coates, A Bissacco, B Wu, A Y Ng, NeurIPS Deep Learning and Unsupervised Feature Learning Workshop. Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In NeurIPS Deep Learning and Unsupervised Feature Learning Workshop, 2011.\n\nNorm-based capacity control in neural networks. B Neyshabur, R Tomioka, N Srebro, Conference on Learning Theory. B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, 2015.\n\nLookahead: A far-sighted alternative of magnitude-based pruning. S Park, J Lee, S Mo, J Shin, International Conference on Learning Representations. S. Park, J. Lee, S. Mo, and J. Shin. Lookahead: A far-sighted alternative of magnitude-based pruning. In International Conference on Learning Representations, 2020.\n\nComparing fine-tuning and rewinding in neural network pruning. A Renda, J Frankle, M Carbin, International Conference on Learning Representations. A. Renda, J. Frankle, and M. Carbin. Comparing fine-tuning and rewinding in neural network pruning. In International Conference on Learning Representations, 2020.\n\nMovement pruning: Adaptive sparsity by fine-tuning. V Sanh, T Wolf, A M Rush, arXiv preprintV. Sanh, T. Wolf, and A. M. Rush. Movement pruning: Adaptive sparsity by fine-tuning. arXiv preprint 2005.07683, 2020.\n\nThe singular values of convolutional layers. H Sedghi, V Gupta, P M Long, International Conference on Learning Representations. H. Sedghi, V. Gupta, and P. M. Long. The singular values of convolutional layers. In International Conference on Learning Representations, 2019.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, International Conference on Learning Representations. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015.\n\nUniform+. 87.30\u00b10.29 87.00\u00b10.23 86.27\u00b10.13 85.00\u00b10.15 83.23\u00b10.19 80.40\u00b10.31 76.40\u00b10.45 69.31\u00b11.27 52.06\u00b12.53 20.19\u00b18.44Uniform+ 87.30\u00b10.29 87.00\u00b10.23 86.27\u00b10.13 85.00\u00b10.15 83.23\u00b10.19 80.40\u00b10.31 76.40\u00b10.45 69.31\u00b11.27 52.06\u00b12.53 20.19\u00b18.44\n\n. Erd\u0151s-R\u00e9nyi Ker, 63\u00b10.10 87.49\u00b10.31 86.83\u00b10.27 85.84\u00b10.23 84.08\u00b10.34 81.76\u00b10.37 78.70\u00b10.25 74.40\u00b10.52 66.42\u00b11.46 50.90\u00b13.2887Erd\u0151s-R\u00e9nyi ker. 87.63\u00b10.10 87.49\u00b10.31 86.83\u00b10.27 85.84\u00b10.23 84.08\u00b10.34 81.76\u00b10.37 78.70\u00b10.25 74.40\u00b10.52 66.42\u00b11.46 50.90\u00b13.28\n\n. 87.54\u00b10.37 87.12\u00b10.11 86.56\u00b10.21 85.64\u00b10.26 84.18\u00b10.23 81.56\u00b10.26 78.63\u00b10.50 74.20\u00b10.81 67.01\u00b10.92 51.24\u00b18.48LAMP. LAMP (Ours) 87.54\u00b10.37 87.12\u00b10.11 86.56\u00b10.21 85.64\u00b10.26 84.18\u00b10.23 81.56\u00b10.26 78.63\u00b10.50 74.20\u00b10.81 67.01\u00b10.92 51.24\u00b18.48\n\nTest accuracies of DenseNet-121 iteratively pruned and trained on CIFAR-10. Bold denotes the pruning scheme with accuracy within one standard deviation from the highest. 6Table 6: Test accuracies of DenseNet-121 iteratively pruned and trained on CIFAR-10. Bold denotes the pruning scheme with accuracy within one standard deviation from the highest.\n\n. Erd\u0151s-R\u00e9nyi, ker. 90.21\u00b10.29 89.79\u00b10.29 88.92\u00b10.07 88.20\u00b10.23 87.25\u00b10.36 86.22\u00b10.30 84.11\u00b10.50 81.82\u00b10.90 59.06\u00b125.61 59.07\u00b125.70Erd\u0151s-R\u00e9nyi ker. 90.21\u00b10.29 89.79\u00b10.29 88.92\u00b10.07 88.20\u00b10.23 87.25\u00b10.36 86.22\u00b10.30 84.11\u00b10.50 81.82\u00b10.90 59.06\u00b125.61 59.07\u00b125.70\n\n. Ours) 90.89\u00b10.17 90.11\u00b10.13 89.72\u00b10.26 89.12\u00b10.35 88.39\u00b10.26 87.75\u00b10.18 86.53\u00b10.33 85.13\u00b10.31 82.92\u00b10.66 79.23\u00b11.29LAMP. LAMP (Ours) 90.89\u00b10.17 90.11\u00b10.13 89.72\u00b10.26 89.12\u00b10.35 88.39\u00b10.26 87.75\u00b10.18 86.53\u00b10.33 85.13\u00b10.31 82.92\u00b10.66 79.23\u00b11.29\n\nTest accuracies of EfficientNet-B0 iteratively pruned and trained on CIFAR-10. Bold denotes the pruning scheme with accuracy within one standard deviation from the highest. 78Surv. weights 41.0% 26.2% 16Table 7: Test accuracies of EfficientNet-B0 iteratively pruned and trained on CIFAR-10. Bold denotes the pruning scheme with accuracy within one standard deviation from the highest. Surv. weights 41.0% 26.2% 16.8%\n\n. Erd\u0151s-R\u00e9nyi, ker. 89.54\u00b10.17 90.09\u00b10.09 90.01\u00b10.31 89.62\u00b10.26 88.82\u00b10.39 87.08\u00b10.32 84.72\u00b10.24 81.53\u00b10.44 51.31\u00b125.77 13.40\u00b13.04Erd\u0151s-R\u00e9nyi ker. 89.54\u00b10.17 90.09\u00b10.09 90.01\u00b10.31 89.62\u00b10.26 88.82\u00b10.39 87.08\u00b10.32 84.72\u00b10.24 81.53\u00b10.44 51.31\u00b125.77 13.40\u00b13.04\n\n. 89.52\u00b10.51 89.95\u00b10.20 89.97\u00b10.23 90.21\u00b10.28 89.91\u00b10.14 89.79\u00b10.33 89.30\u00b10.08 88.51\u00b10.37 86.79\u00b10.73 65.76\u00b120.07LAMP. LAMP (Ours) 89.52\u00b10.51 89.95\u00b10.20 89.97\u00b10.23 90.21\u00b10.28 89.91\u00b10.14 89.79\u00b10.33 89.30\u00b10.08 88.51\u00b10.37 86.79\u00b10.73 65.76\u00b120.07\n\nTest accuracies of VGG-16 iteratively pruned and trained on SVHN. 8Bold denotes theTable 8: Test accuracies of VGG-16 iteratively pruned and trained on SVHN. Bold denotes the\n\n49.51\u00b124.23 26.83\u00b119.39 15. 37\u00b15.95 7.79\u00b11.60 14.72\u00b13.65 Uniform 95.78\u00b10.16 95.52\u00b10.10 95.35\u00b10.14 94.98\u00b10.19 93.26\u00b11.42 64.32\u00b115.03 29.54\u00b110.21 19.63\u00b17.54 13.73\u00b16.97 17.94\u00b14.85Global 95.75\u00b10.17 95.46\u00b10.10 94.91\u00b10.48 61.06\u00b130.93 65.06\u00b111.01 49.51\u00b124.23 26.83\u00b119.39 15.37\u00b15.95 7.79\u00b11.60 14.72\u00b13.65 Uniform 95.78\u00b10.16 95.52\u00b10.10 95.35\u00b10.14 94.98\u00b10.19 93.26\u00b11.42 64.32\u00b115.03 29.54\u00b110.21 19.63\u00b17.54 13.73\u00b16.97 17.94\u00b14.85\n\nUniform+. 95.70\u00b10.13 95.68\u00b10.17 95.42\u00b10.11 95.05\u00b10.15 94.57\u00b10.16 93.87\u00b10.10 93.07\u00b10.16 92.25\u00b10.25 74.88\u00b125.75 17.67\u00b114.33Uniform+ 95.70\u00b10.13 95.68\u00b10.17 95.42\u00b10.11 95.05\u00b10.15 94.57\u00b10.16 93.87\u00b10.10 93.07\u00b10.16 92.25\u00b10.25 74.88\u00b125.75 17.67\u00b114.33\n\n. 95.98\u00b10.15 95.87\u00b10.13 95.74\u00b10.10 95.58\u00b10.07 95.35\u00b10.18 95.16\u00b10.08 94.71\u00b10.18 94.24\u00b10.17 94.05\u00b10.15 93.97\u00b10.19LAMP. LAMP (Ours) 95.98\u00b10.15 95.87\u00b10.13 95.74\u00b10.10 95.58\u00b10.07 95.35\u00b10.18 95.16\u00b10.08 94.71\u00b10.18 94.24\u00b10.17 94.05\u00b10.15 93.97\u00b10.19\n\nTest accuracies of VGG-16 iteratively pruned and trained on CIFAR-100. 9Table 9: Test accuracies of VGG-16 iteratively pruned and trained on CIFAR-100\n\nUniform+. 67.94\u00b10.32 67.41\u00b10.16 66.37\u00b10.09 65.51\u00b10.33 63.43\u00b10.35 60.76\u00b10.29 57.93\u00b10.39 55.40\u00b10.22 51.90\u00b11.30 33.35\u00b113.08Uniform+ 67.94\u00b10.32 67.41\u00b10.16 66.37\u00b10.09 65.51\u00b10.33 63.43\u00b10.35 60.76\u00b10.29 57.93\u00b10.39 55.40\u00b10.22 51.90\u00b11.30 33.35\u00b113.08\n\n. Erd\u0151s-R\u00e9nyi, ker. 68.07\u00b10.42 67.92\u00b10.36 67.45\u00b10.22 67.09\u00b10.31 65.86\u00b10.18 64.15\u00b10.31 62.04\u00b10.37 60.07\u00b10.35 58.58\u00b10.58 55.88\u00b10.51Erd\u0151s-R\u00e9nyi ker. 68.07\u00b10.42 67.92\u00b10.36 67.45\u00b10.22 67.09\u00b10.31 65.86\u00b10.18 64.15\u00b10.31 62.04\u00b10.37 60.07\u00b10.35 58.58\u00b10.58 55.88\u00b10.51\n\n. 68.07\u00b10.58 68.22\u00b10.27 67.84\u00b10.31 67.37\u00b10.28 66.35\u00b10.53 65.25\u00b10.23 63.46\u00b10.28 62.37\u00b10.28 60.77\u00b10.42 58.05\u00b10.42LAMP. LAMP (Ours) 68.07\u00b10.58 68.22\u00b10.27 67.84\u00b10.31 67.37\u00b10.28 66.35\u00b10.53 65.25\u00b10.23 63.46\u00b10.28 62.37\u00b10.28 60.77\u00b10.42 58.05\u00b10.42\n", "annotations": {"author": "[{\"end\":69,\"start\":59},{\"end\":78,\"start\":70},{\"end\":86,\"start\":79},{\"end\":98,\"start\":87},{\"end\":109,\"start\":99},{\"end\":116,\"start\":110},{\"end\":149,\"start\":117},{\"end\":161,\"start\":150},{\"end\":173,\"start\":162},{\"end\":186,\"start\":174}]", "publisher": null, "author_last_name": "[{\"end\":68,\"start\":65},{\"end\":77,\"start\":72},{\"end\":97,\"start\":95},{\"end\":108,\"start\":101},{\"end\":128,\"start\":124},{\"end\":160,\"start\":155},{\"end\":172,\"start\":167},{\"end\":185,\"start\":179}]", "author_first_name": "[{\"end\":64,\"start\":59},{\"end\":71,\"start\":70},{\"end\":83,\"start\":79},{\"end\":85,\"start\":84},{\"end\":94,\"start\":87},{\"end\":100,\"start\":99},{\"end\":113,\"start\":110},{\"end\":115,\"start\":114},{\"end\":123,\"start\":117},{\"end\":152,\"start\":150},{\"end\":154,\"start\":153},{\"end\":164,\"start\":162},{\"end\":166,\"start\":165},{\"end\":176,\"start\":174},{\"end\":178,\"start\":177}]", "author_affiliation": null, "title": "[{\"end\":56,\"start\":1},{\"end\":242,\"start\":187}]", "venue": null, "abstract": "[{\"end\":2849,\"start\":345}]", "bib_ref": "[{\"end\":3012,\"start\":2994},{\"end\":3057,\"start\":3035},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3109,\"start\":3084},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3189,\"start\":3166},{\"end\":3528,\"start\":3508},{\"end\":3550,\"start\":3528},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3568,\"start\":3550},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3615,\"start\":3594},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3635,\"start\":3615},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3678,\"start\":3657},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3723,\"start\":3704},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3800,\"start\":3778},{\"end\":3818,\"start\":3800},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3861,\"start\":3837},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3882,\"start\":3861},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3899,\"start\":3882},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3946,\"start\":3928},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4325,\"start\":4308},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4502,\"start\":4484},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4684,\"start\":4663},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4871,\"start\":4853},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4891,\"start\":4873},{\"end\":7617,\"start\":7598},{\"end\":7729,\"start\":7712},{\"end\":7751,\"start\":7734},{\"end\":7914,\"start\":7896},{\"end\":8050,\"start\":8032},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8343,\"start\":8320},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8684,\"start\":8664},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8784,\"start\":8766},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9002,\"start\":8982},{\"end\":9191,\"start\":9169},{\"end\":13367,\"start\":13347},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15683,\"start\":15662},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19028,\"start\":19008},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19429,\"start\":19409},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19501,\"start\":19483},{\"end\":19937,\"start\":19919},{\"end\":20412,\"start\":20390},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21095,\"start\":21067},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23312,\"start\":23289},{\"end\":23669,\"start\":23651},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23795,\"start\":23772},{\"end\":25054,\"start\":25036},{\"end\":26551,\"start\":26533},{\"end\":28129,\"start\":28112},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29636,\"start\":29609},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29793,\"start\":29770},{\"end\":30259,\"start\":30238},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32627,\"start\":32603},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34325,\"start\":34307},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34845,\"start\":34828},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35201,\"start\":35174},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35558,\"start\":35537},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35595,\"start\":35574},{\"end\":36226,\"start\":36205},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38109,\"start\":38083},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":38350,\"start\":38333},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":39687,\"start\":39661},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":42175,\"start\":42158}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38890,\"start\":38587},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39857,\"start\":38891},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40190,\"start\":39858},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40266,\"start\":40191},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41864,\"start\":40267},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42038,\"start\":41865},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42176,\"start\":42039},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42545,\"start\":42177},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43307,\"start\":42546},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44138,\"start\":43308},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":45042,\"start\":44139},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":45221,\"start\":45043}]", "paragraph": "[{\"end\":4193,\"start\":2865},{\"end\":4685,\"start\":4195},{\"end\":5276,\"start\":4687},{\"end\":6021,\"start\":5278},{\"end\":6509,\"start\":6023},{\"end\":7338,\"start\":6511},{\"end\":8242,\"start\":7355},{\"end\":9036,\"start\":8244},{\"end\":9459,\"start\":9038},{\"end\":9713,\"start\":9461},{\"end\":10606,\"start\":9763},{\"end\":10683,\"start\":10608},{\"end\":11116,\"start\":10729},{\"end\":11398,\"start\":11118},{\"end\":11740,\"start\":11454},{\"end\":12211,\"start\":11742},{\"end\":12675,\"start\":12213},{\"end\":13543,\"start\":12729},{\"end\":13851,\"start\":13545},{\"end\":14085,\"start\":13921},{\"end\":14995,\"start\":14087},{\"end\":15369,\"start\":15047},{\"end\":15844,\"start\":15403},{\"end\":16723,\"start\":16079},{\"end\":16989,\"start\":16725},{\"end\":17237,\"start\":17061},{\"end\":17614,\"start\":17239},{\"end\":17803,\"start\":17616},{\"end\":18602,\"start\":17975},{\"end\":18791,\"start\":18629},{\"end\":19029,\"start\":18793},{\"end\":19197,\"start\":19031},{\"end\":19776,\"start\":19199},{\"end\":20413,\"start\":19778},{\"end\":20894,\"start\":20415},{\"end\":21291,\"start\":20911},{\"end\":22248,\"start\":21293},{\"end\":22504,\"start\":22250},{\"end\":22893,\"start\":22564},{\"end\":24312,\"start\":22895},{\"end\":24876,\"start\":24314},{\"end\":24973,\"start\":24878},{\"end\":25267,\"start\":24975},{\"end\":25624,\"start\":25331},{\"end\":25777,\"start\":25626},{\"end\":26240,\"start\":25779},{\"end\":26868,\"start\":26242},{\"end\":27523,\"start\":26870},{\"end\":27997,\"start\":27525},{\"end\":28276,\"start\":27999},{\"end\":29347,\"start\":28291},{\"end\":29475,\"start\":29391},{\"end\":29529,\"start\":29477},{\"end\":29899,\"start\":29531},{\"end\":30260,\"start\":29901},{\"end\":30864,\"start\":30262},{\"end\":31180,\"start\":30921},{\"end\":31277,\"start\":31182},{\"end\":31407,\"start\":31279},{\"end\":31634,\"start\":31409},{\"end\":31750,\"start\":31636},{\"end\":31956,\"start\":31820},{\"end\":32071,\"start\":31958},{\"end\":32074,\"start\":32073},{\"end\":32145,\"start\":32076},{\"end\":32353,\"start\":32147},{\"end\":32732,\"start\":32460},{\"end\":33209,\"start\":33040},{\"end\":33462,\"start\":33341},{\"end\":33766,\"start\":33601},{\"end\":33976,\"start\":33881},{\"end\":34766,\"start\":34024},{\"end\":35148,\"start\":34768},{\"end\":35504,\"start\":35150},{\"end\":35605,\"start\":35506},{\"end\":36031,\"start\":35607},{\"end\":36928,\"start\":36071},{\"end\":37718,\"start\":36930},{\"end\":37921,\"start\":37720},{\"end\":38586,\"start\":37968}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10728,\"start\":10684},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11453,\"start\":11399},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13920,\"start\":13852},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15046,\"start\":14996},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15402,\"start\":15370},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16078,\"start\":15845},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17060,\"start\":16990},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17974,\"start\":17804},{\"attributes\":{\"id\":\"formula_8\"},\"end\":31819,\"start\":31751},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32459,\"start\":32354},{\"attributes\":{\"id\":\"formula_10\"},\"end\":33039,\"start\":32733},{\"attributes\":{\"id\":\"formula_11\"},\"end\":33340,\"start\":33210},{\"attributes\":{\"id\":\"formula_12\"},\"end\":33600,\"start\":33463},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33880,\"start\":33767}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2863,\"start\":2851},{\"attributes\":{\"n\":\"2\"},\"end\":7353,\"start\":7341},{\"attributes\":{\"n\":\"3\"},\"end\":9761,\"start\":9716},{\"attributes\":{\"n\":\"3.1\"},\"end\":12727,\"start\":12678},{\"attributes\":{\"n\":\"4\"},\"end\":18627,\"start\":18605},{\"attributes\":{\"n\":\"4.1\"},\"end\":20909,\"start\":20897},{\"attributes\":{\"n\":\"4.2\"},\"end\":22562,\"start\":22507},{\"attributes\":{\"n\":\"5\"},\"end\":25329,\"start\":25270},{\"attributes\":{\"n\":\"6\"},\"end\":28289,\"start\":28279},{\"end\":29365,\"start\":29350},{\"end\":29389,\"start\":29368},{\"end\":30919,\"start\":30867},{\"end\":34022,\"start\":33979},{\"end\":36069,\"start\":36034},{\"end\":37966,\"start\":37924},{\"end\":38598,\"start\":38588},{\"end\":38902,\"start\":38892},{\"end\":39869,\"start\":39859},{\"end\":40202,\"start\":40192},{\"end\":41876,\"start\":41866},{\"end\":42050,\"start\":42040},{\"end\":42187,\"start\":42178},{\"end\":42556,\"start\":42547},{\"end\":43318,\"start\":43309},{\"end\":44149,\"start\":44140},{\"end\":45053,\"start\":45044}]", "table": "[{\"end\":42545,\"start\":42210},{\"end\":43307,\"start\":43210},{\"end\":44138,\"start\":44041},{\"end\":45042,\"start\":44375}]", "figure_caption": "[{\"end\":38890,\"start\":38600},{\"end\":39857,\"start\":38904},{\"end\":40190,\"start\":39871},{\"end\":40266,\"start\":40204},{\"end\":41864,\"start\":40269},{\"end\":42038,\"start\":41878},{\"end\":42176,\"start\":42052},{\"end\":42210,\"start\":42189},{\"end\":43210,\"start\":42558},{\"end\":44041,\"start\":43320},{\"end\":44375,\"start\":44151},{\"end\":45221,\"start\":45055}]", "figure_ref": "[{\"end\":3956,\"start\":3948},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21302,\"start\":21296},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22259,\"start\":22253},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22382,\"start\":22376},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23906,\"start\":23900},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25788,\"start\":25782},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36728,\"start\":36720},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37154,\"start\":37148},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37683,\"start\":37677},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":38441,\"start\":38435}]", "bib_author_first_name": "[{\"end\":45565,\"start\":45564},{\"end\":45575,\"start\":45574},{\"end\":45583,\"start\":45582},{\"end\":45593,\"start\":45592},{\"end\":45940,\"start\":45939},{\"end\":45950,\"start\":45949},{\"end\":45964,\"start\":45963},{\"end\":45981,\"start\":45980},{\"end\":45992,\"start\":45991},{\"end\":46359,\"start\":46358},{\"end\":46366,\"start\":46365},{\"end\":46373,\"start\":46372},{\"end\":46380,\"start\":46379},{\"end\":46752,\"start\":46751},{\"end\":46760,\"start\":46759},{\"end\":46768,\"start\":46767},{\"end\":46770,\"start\":46769},{\"end\":47021,\"start\":47020},{\"end\":47030,\"start\":47029},{\"end\":47040,\"start\":47039},{\"end\":47048,\"start\":47047},{\"end\":47308,\"start\":47307},{\"end\":47316,\"start\":47315},{\"end\":47324,\"start\":47323},{\"end\":47334,\"start\":47333},{\"end\":47336,\"start\":47335},{\"end\":47346,\"start\":47345},{\"end\":47711,\"start\":47710},{\"end\":47722,\"start\":47721},{\"end\":48010,\"start\":48009},{\"end\":48021,\"start\":48020},{\"end\":48023,\"start\":48022},{\"end\":48036,\"start\":48035},{\"end\":48038,\"start\":48037},{\"end\":48045,\"start\":48044},{\"end\":48426,\"start\":48425},{\"end\":48433,\"start\":48432},{\"end\":48445,\"start\":48444},{\"end\":48454,\"start\":48453},{\"end\":48458,\"start\":48455},{\"end\":48738,\"start\":48737},{\"end\":48745,\"start\":48744},{\"end\":48752,\"start\":48751},{\"end\":48758,\"start\":48757},{\"end\":48975,\"start\":48974},{\"end\":48982,\"start\":48981},{\"end\":48984,\"start\":48983},{\"end\":48993,\"start\":48992},{\"end\":49002,\"start\":49001},{\"end\":49014,\"start\":49013},{\"end\":49277,\"start\":49276},{\"end\":49284,\"start\":49283},{\"end\":49291,\"start\":49290},{\"end\":49299,\"start\":49298},{\"end\":49308,\"start\":49307},{\"end\":49568,\"start\":49567},{\"end\":49582,\"start\":49581},{\"end\":49818,\"start\":49817},{\"end\":49829,\"start\":49828},{\"end\":49840,\"start\":49839},{\"end\":50103,\"start\":50102},{\"end\":50114,\"start\":50113},{\"end\":50125,\"start\":50124},{\"end\":50127,\"start\":50126},{\"end\":50423,\"start\":50422},{\"end\":50433,\"start\":50432},{\"end\":50446,\"start\":50445},{\"end\":50670,\"start\":50669},{\"end\":50680,\"start\":50679},{\"end\":50689,\"start\":50688},{\"end\":50701,\"start\":50700},{\"end\":51023,\"start\":51022},{\"end\":51025,\"start\":51024},{\"end\":51035,\"start\":51034},{\"end\":51045,\"start\":51044},{\"end\":51054,\"start\":51053},{\"end\":51056,\"start\":51055},{\"end\":51066,\"start\":51065},{\"end\":51077,\"start\":51076},{\"end\":51381,\"start\":51380},{\"end\":51394,\"start\":51393},{\"end\":51405,\"start\":51404},{\"end\":51789,\"start\":51788},{\"end\":51791,\"start\":51790},{\"end\":51801,\"start\":51800},{\"end\":51807,\"start\":51806},{\"end\":51819,\"start\":51818},{\"end\":52181,\"start\":52180},{\"end\":52183,\"start\":52182},{\"end\":52192,\"start\":52191},{\"end\":52485,\"start\":52484},{\"end\":52495,\"start\":52494},{\"end\":52506,\"start\":52505},{\"end\":52519,\"start\":52518},{\"end\":52527,\"start\":52526},{\"end\":52834,\"start\":52833},{\"end\":52844,\"start\":52843},{\"end\":52852,\"start\":52851},{\"end\":52862,\"start\":52861},{\"end\":52874,\"start\":52873},{\"end\":52880,\"start\":52879},{\"end\":52882,\"start\":52881},{\"end\":53212,\"start\":53211},{\"end\":53225,\"start\":53224},{\"end\":53236,\"start\":53235},{\"end\":53472,\"start\":53471},{\"end\":53480,\"start\":53479},{\"end\":53487,\"start\":53486},{\"end\":53493,\"start\":53492},{\"end\":53784,\"start\":53783},{\"end\":53793,\"start\":53792},{\"end\":53804,\"start\":53803},{\"end\":54084,\"start\":54083},{\"end\":54092,\"start\":54091},{\"end\":54100,\"start\":54099},{\"end\":54102,\"start\":54101},{\"end\":54289,\"start\":54288},{\"end\":54299,\"start\":54298},{\"end\":54308,\"start\":54307},{\"end\":54310,\"start\":54309},{\"end\":54586,\"start\":54585},{\"end\":54598,\"start\":54597},{\"end\":55078,\"start\":55067}]", "bib_author_last_name": "[{\"end\":45572,\"start\":45566},{\"end\":45580,\"start\":45576},{\"end\":45590,\"start\":45584},{\"end\":45601,\"start\":45594},{\"end\":45947,\"start\":45941},{\"end\":45961,\"start\":45951},{\"end\":45978,\"start\":45965},{\"end\":45989,\"start\":45982},{\"end\":45996,\"start\":45993},{\"end\":46363,\"start\":46360},{\"end\":46370,\"start\":46367},{\"end\":46377,\"start\":46374},{\"end\":46385,\"start\":46381},{\"end\":46757,\"start\":46753},{\"end\":46765,\"start\":46761},{\"end\":46774,\"start\":46771},{\"end\":47027,\"start\":47022},{\"end\":47037,\"start\":47031},{\"end\":47045,\"start\":47041},{\"end\":47057,\"start\":47049},{\"end\":47313,\"start\":47309},{\"end\":47321,\"start\":47317},{\"end\":47331,\"start\":47325},{\"end\":47343,\"start\":47337},{\"end\":47352,\"start\":47347},{\"end\":47719,\"start\":47712},{\"end\":47729,\"start\":47723},{\"end\":48018,\"start\":48011},{\"end\":48033,\"start\":48024},{\"end\":48042,\"start\":48039},{\"end\":48052,\"start\":48046},{\"end\":48430,\"start\":48427},{\"end\":48442,\"start\":48434},{\"end\":48451,\"start\":48446},{\"end\":48463,\"start\":48459},{\"end\":48742,\"start\":48739},{\"end\":48749,\"start\":48746},{\"end\":48755,\"start\":48753},{\"end\":48763,\"start\":48759},{\"end\":48979,\"start\":48976},{\"end\":48990,\"start\":48985},{\"end\":48999,\"start\":48994},{\"end\":49011,\"start\":49003},{\"end\":49020,\"start\":49015},{\"end\":49281,\"start\":49278},{\"end\":49288,\"start\":49285},{\"end\":49296,\"start\":49292},{\"end\":49305,\"start\":49300},{\"end\":49316,\"start\":49309},{\"end\":49579,\"start\":49569},{\"end\":49589,\"start\":49583},{\"end\":49826,\"start\":49819},{\"end\":49837,\"start\":49830},{\"end\":49848,\"start\":49841},{\"end\":50111,\"start\":50104},{\"end\":50122,\"start\":50115},{\"end\":50134,\"start\":50128},{\"end\":50430,\"start\":50424},{\"end\":50443,\"start\":50434},{\"end\":50460,\"start\":50447},{\"end\":50677,\"start\":50671},{\"end\":50686,\"start\":50681},{\"end\":50698,\"start\":50690},{\"end\":50708,\"start\":50702},{\"end\":51032,\"start\":51026},{\"end\":51042,\"start\":51036},{\"end\":51051,\"start\":51046},{\"end\":51063,\"start\":51057},{\"end\":51074,\"start\":51067},{\"end\":51084,\"start\":51078},{\"end\":51391,\"start\":51382},{\"end\":51402,\"start\":51395},{\"end\":51412,\"start\":51406},{\"end\":51798,\"start\":51792},{\"end\":51804,\"start\":51802},{\"end\":51816,\"start\":51808},{\"end\":51824,\"start\":51820},{\"end\":52189,\"start\":52184},{\"end\":52202,\"start\":52193},{\"end\":52492,\"start\":52486},{\"end\":52503,\"start\":52496},{\"end\":52516,\"start\":52507},{\"end\":52524,\"start\":52520},{\"end\":52535,\"start\":52528},{\"end\":52841,\"start\":52835},{\"end\":52849,\"start\":52845},{\"end\":52859,\"start\":52853},{\"end\":52871,\"start\":52863},{\"end\":52877,\"start\":52875},{\"end\":52885,\"start\":52883},{\"end\":53222,\"start\":53213},{\"end\":53233,\"start\":53226},{\"end\":53243,\"start\":53237},{\"end\":53477,\"start\":53473},{\"end\":53484,\"start\":53481},{\"end\":53490,\"start\":53488},{\"end\":53498,\"start\":53494},{\"end\":53790,\"start\":53785},{\"end\":53801,\"start\":53794},{\"end\":53811,\"start\":53805},{\"end\":54089,\"start\":54085},{\"end\":54097,\"start\":54093},{\"end\":54107,\"start\":54103},{\"end\":54296,\"start\":54290},{\"end\":54305,\"start\":54300},{\"end\":54315,\"start\":54311},{\"end\":54595,\"start\":54587},{\"end\":54608,\"start\":54599},{\"end\":55082,\"start\":55079},{\"end\":55924,\"start\":55913},{\"end\":56849,\"start\":56838},{\"end\":58819,\"start\":58808}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":46563096},\"end\":45837,\"start\":45487},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4885767},\"end\":46282,\"start\":45839},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3574227},\"end\":46672,\"start\":46284},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5750817},\"end\":46996,\"start\":46674},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":208248161},\"end\":47256,\"start\":46998},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":208267757},\"end\":47634,\"start\":47258},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":53388625},\"end\":47947,\"start\":47636},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":209324341},\"end\":48343,\"start\":47949},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":189898449},\"end\":48711,\"start\":48345},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":38486148},\"end\":48935,\"start\":48713},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":213729382},\"end\":49233,\"start\":48937},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":52978527},\"end\":49526,\"start\":49235},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":53592270},\"end\":49775,\"start\":49528},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9328854},\"end\":50040,\"start\":49777},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":30535508},\"end\":50354,\"start\":50042},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":252796},\"end\":50634,\"start\":50356},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16299141},\"end\":50908,\"start\":50636},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":49310977},\"end\":51325,\"start\":50910},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":18201582},\"end\":51682,\"start\":51327},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":174801046},\"end\":52087,\"start\":51684},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":17651092},\"end\":52435,\"start\":52089},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":203593945},\"end\":52762,\"start\":52437},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16852518},\"end\":53161,\"start\":52764},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14338058},\"end\":53404,\"start\":53163},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":211082491},\"end\":53718,\"start\":53406},{\"attributes\":{\"id\":\"b25\"},\"end\":54029,\"start\":53720},{\"attributes\":{\"id\":\"b26\"},\"end\":54241,\"start\":54031},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":44084312},\"end\":54515,\"start\":54243},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14124313},\"end\":54824,\"start\":54517},{\"attributes\":{\"doi\":\"87.30\u00b10.29 87.00\u00b10.23 86.27\u00b10.13 85.00\u00b10.15 83.23\u00b10.19 80.40\u00b10.31 76.40\u00b10.45 69.31\u00b11.27 52.06\u00b12.53 20.19\u00b18.44\",\"id\":\"b29\"},\"end\":55063,\"start\":54826},{\"attributes\":{\"doi\":\"63\u00b10.10 87.49\u00b10.31 86.83\u00b10.27 85.84\u00b10.23 84.08\u00b10.34 81.76\u00b10.37 78.70\u00b10.25 74.40\u00b10.52 66.42\u00b11.46 50.90\u00b13.28\",\"id\":\"b30\"},\"end\":55318,\"start\":55065},{\"attributes\":{\"doi\":\"87.54\u00b10.37 87.12\u00b10.11 86.56\u00b10.21 85.64\u00b10.26 84.18\u00b10.23 81.56\u00b10.26 78.63\u00b10.50 74.20\u00b10.81 67.01\u00b10.92 51.24\u00b18.48\",\"id\":\"b31\"},\"end\":55558,\"start\":55320},{\"attributes\":{\"id\":\"b32\"},\"end\":55909,\"start\":55560},{\"attributes\":{\"doi\":\"ker. 90.21\u00b10.29 89.79\u00b10.29 88.92\u00b10.07 88.20\u00b10.23 87.25\u00b10.36 86.22\u00b10.30 84.11\u00b10.50 81.82\u00b10.90 59.06\u00b125.61 59.07\u00b125.70\",\"id\":\"b33\"},\"end\":56170,\"start\":55911},{\"attributes\":{\"doi\":\"Ours) 90.89\u00b10.17 90.11\u00b10.13 89.72\u00b10.26 89.12\u00b10.35 88.39\u00b10.26 87.75\u00b10.18 86.53\u00b10.33 85.13\u00b10.31 82.92\u00b10.66 79.23\u00b11.29\",\"id\":\"b34\"},\"end\":56416,\"start\":56172},{\"attributes\":{\"id\":\"b35\"},\"end\":56834,\"start\":56418},{\"attributes\":{\"doi\":\"ker. 89.54\u00b10.17 90.09\u00b10.09 90.01\u00b10.31 89.62\u00b10.26 88.82\u00b10.39 87.08\u00b10.32 84.72\u00b10.24 81.53\u00b10.44 51.31\u00b125.77 13.40\u00b13.04\",\"id\":\"b36\"},\"end\":57093,\"start\":56836},{\"attributes\":{\"doi\":\"89.52\u00b10.51 89.95\u00b10.20 89.97\u00b10.23 90.21\u00b10.28 89.91\u00b10.14 89.79\u00b10.33 89.30\u00b10.08 88.51\u00b10.37 86.79\u00b10.73 65.76\u00b120.07\",\"id\":\"b37\"},\"end\":57335,\"start\":57095},{\"attributes\":{\"id\":\"b38\"},\"end\":57511,\"start\":57337},{\"attributes\":{\"doi\":\"37\u00b15.95 7.79\u00b11.60 14.72\u00b13.65 Uniform 95.78\u00b10.16 95.52\u00b10.10 95.35\u00b10.14 94.98\u00b10.19 93.26\u00b11.42 64.32\u00b115.03 29.54\u00b110.21 19.63\u00b17.54 13.73\u00b16.97 17.94\u00b14.85\",\"id\":\"b39\"},\"end\":57928,\"start\":57513},{\"attributes\":{\"doi\":\"95.70\u00b10.13 95.68\u00b10.17 95.42\u00b10.11 95.05\u00b10.15 94.57\u00b10.16 93.87\u00b10.10 93.07\u00b10.16 92.25\u00b10.25 74.88\u00b125.75 17.67\u00b114.33\",\"id\":\"b40\"},\"end\":58171,\"start\":57930},{\"attributes\":{\"doi\":\"95.98\u00b10.15 95.87\u00b10.13 95.74\u00b10.10 95.58\u00b10.07 95.35\u00b10.18 95.16\u00b10.08 94.71\u00b10.18 94.24\u00b10.17 94.05\u00b10.15 93.97\u00b10.19\",\"id\":\"b41\"},\"end\":58411,\"start\":58173},{\"attributes\":{\"id\":\"b42\"},\"end\":58563,\"start\":58413},{\"attributes\":{\"doi\":\"67.94\u00b10.32 67.41\u00b10.16 66.37\u00b10.09 65.51\u00b10.33 63.43\u00b10.35 60.76\u00b10.29 57.93\u00b10.39 55.40\u00b10.22 51.90\u00b11.30 33.35\u00b113.08\",\"id\":\"b43\"},\"end\":58804,\"start\":58565},{\"attributes\":{\"doi\":\"ker. 68.07\u00b10.42 67.92\u00b10.36 67.45\u00b10.22 67.09\u00b10.31 65.86\u00b10.18 64.15\u00b10.31 62.04\u00b10.37 60.07\u00b10.35 58.58\u00b10.58 55.88\u00b10.51\",\"id\":\"b44\"},\"end\":59061,\"start\":58806},{\"attributes\":{\"doi\":\"68.07\u00b10.58 68.22\u00b10.27 67.84\u00b10.31 67.37\u00b10.28 66.35\u00b10.53 65.25\u00b10.23 63.46\u00b10.28 62.37\u00b10.28 60.77\u00b10.42 58.05\u00b10.42\",\"id\":\"b45\"},\"end\":59301,\"start\":59063}]", "bib_title": "[{\"end\":45562,\"start\":45487},{\"end\":45937,\"start\":45839},{\"end\":46356,\"start\":46284},{\"end\":46749,\"start\":46674},{\"end\":47018,\"start\":46998},{\"end\":47305,\"start\":47258},{\"end\":47708,\"start\":47636},{\"end\":48007,\"start\":47949},{\"end\":48423,\"start\":48345},{\"end\":48735,\"start\":48713},{\"end\":48972,\"start\":48937},{\"end\":49274,\"start\":49235},{\"end\":49565,\"start\":49528},{\"end\":49815,\"start\":49777},{\"end\":50100,\"start\":50042},{\"end\":50420,\"start\":50356},{\"end\":50667,\"start\":50636},{\"end\":51020,\"start\":50910},{\"end\":51378,\"start\":51327},{\"end\":51786,\"start\":51684},{\"end\":52178,\"start\":52089},{\"end\":52482,\"start\":52437},{\"end\":52831,\"start\":52764},{\"end\":53209,\"start\":53163},{\"end\":53469,\"start\":53406},{\"end\":53781,\"start\":53720},{\"end\":54286,\"start\":54243},{\"end\":54583,\"start\":54517}]", "bib_author": "[{\"end\":45574,\"start\":45564},{\"end\":45582,\"start\":45574},{\"end\":45592,\"start\":45582},{\"end\":45603,\"start\":45592},{\"end\":45949,\"start\":45939},{\"end\":45963,\"start\":45949},{\"end\":45980,\"start\":45963},{\"end\":45991,\"start\":45980},{\"end\":45998,\"start\":45991},{\"end\":46365,\"start\":46358},{\"end\":46372,\"start\":46365},{\"end\":46379,\"start\":46372},{\"end\":46387,\"start\":46379},{\"end\":46759,\"start\":46751},{\"end\":46767,\"start\":46759},{\"end\":46776,\"start\":46767},{\"end\":47029,\"start\":47020},{\"end\":47039,\"start\":47029},{\"end\":47047,\"start\":47039},{\"end\":47059,\"start\":47047},{\"end\":47315,\"start\":47307},{\"end\":47323,\"start\":47315},{\"end\":47333,\"start\":47323},{\"end\":47345,\"start\":47333},{\"end\":47354,\"start\":47345},{\"end\":47721,\"start\":47710},{\"end\":47731,\"start\":47721},{\"end\":48020,\"start\":48009},{\"end\":48035,\"start\":48020},{\"end\":48044,\"start\":48035},{\"end\":48054,\"start\":48044},{\"end\":48432,\"start\":48425},{\"end\":48444,\"start\":48432},{\"end\":48453,\"start\":48444},{\"end\":48465,\"start\":48453},{\"end\":48744,\"start\":48737},{\"end\":48751,\"start\":48744},{\"end\":48757,\"start\":48751},{\"end\":48765,\"start\":48757},{\"end\":48981,\"start\":48974},{\"end\":48992,\"start\":48981},{\"end\":49001,\"start\":48992},{\"end\":49013,\"start\":49001},{\"end\":49022,\"start\":49013},{\"end\":49283,\"start\":49276},{\"end\":49290,\"start\":49283},{\"end\":49298,\"start\":49290},{\"end\":49307,\"start\":49298},{\"end\":49318,\"start\":49307},{\"end\":49581,\"start\":49567},{\"end\":49591,\"start\":49581},{\"end\":49828,\"start\":49817},{\"end\":49839,\"start\":49828},{\"end\":49850,\"start\":49839},{\"end\":50113,\"start\":50102},{\"end\":50124,\"start\":50113},{\"end\":50136,\"start\":50124},{\"end\":50432,\"start\":50422},{\"end\":50445,\"start\":50432},{\"end\":50462,\"start\":50445},{\"end\":50679,\"start\":50669},{\"end\":50688,\"start\":50679},{\"end\":50700,\"start\":50688},{\"end\":50710,\"start\":50700},{\"end\":51034,\"start\":51022},{\"end\":51044,\"start\":51034},{\"end\":51053,\"start\":51044},{\"end\":51065,\"start\":51053},{\"end\":51076,\"start\":51065},{\"end\":51086,\"start\":51076},{\"end\":51393,\"start\":51380},{\"end\":51404,\"start\":51393},{\"end\":51414,\"start\":51404},{\"end\":51800,\"start\":51788},{\"end\":51806,\"start\":51800},{\"end\":51818,\"start\":51806},{\"end\":51826,\"start\":51818},{\"end\":52191,\"start\":52180},{\"end\":52204,\"start\":52191},{\"end\":52494,\"start\":52484},{\"end\":52505,\"start\":52494},{\"end\":52518,\"start\":52505},{\"end\":52526,\"start\":52518},{\"end\":52537,\"start\":52526},{\"end\":52843,\"start\":52833},{\"end\":52851,\"start\":52843},{\"end\":52861,\"start\":52851},{\"end\":52873,\"start\":52861},{\"end\":52879,\"start\":52873},{\"end\":52887,\"start\":52879},{\"end\":53224,\"start\":53211},{\"end\":53235,\"start\":53224},{\"end\":53245,\"start\":53235},{\"end\":53479,\"start\":53471},{\"end\":53486,\"start\":53479},{\"end\":53492,\"start\":53486},{\"end\":53500,\"start\":53492},{\"end\":53792,\"start\":53783},{\"end\":53803,\"start\":53792},{\"end\":53813,\"start\":53803},{\"end\":54091,\"start\":54083},{\"end\":54099,\"start\":54091},{\"end\":54109,\"start\":54099},{\"end\":54298,\"start\":54288},{\"end\":54307,\"start\":54298},{\"end\":54317,\"start\":54307},{\"end\":54597,\"start\":54585},{\"end\":54610,\"start\":54597},{\"end\":55084,\"start\":55067},{\"end\":55926,\"start\":55913},{\"end\":56851,\"start\":56838},{\"end\":58821,\"start\":58808}]", "bib_venue": "[{\"end\":46492,\"start\":46448},{\"end\":47459,\"start\":47415},{\"end\":48159,\"start\":48115},{\"end\":51519,\"start\":51475},{\"end\":45652,\"start\":45603},{\"end\":46050,\"start\":45998},{\"end\":46446,\"start\":46387},{\"end\":46825,\"start\":46776},{\"end\":47117,\"start\":47059},{\"end\":47413,\"start\":47354},{\"end\":47783,\"start\":47731},{\"end\":48113,\"start\":48054},{\"end\":48517,\"start\":48465},{\"end\":48814,\"start\":48765},{\"end\":49074,\"start\":49022},{\"end\":49370,\"start\":49318},{\"end\":49643,\"start\":49591},{\"end\":49899,\"start\":49850},{\"end\":50188,\"start\":50136},{\"end\":50487,\"start\":50462},{\"end\":50762,\"start\":50710},{\"end\":51107,\"start\":51086},{\"end\":51473,\"start\":51414},{\"end\":51875,\"start\":51826},{\"end\":52253,\"start\":52204},{\"end\":52589,\"start\":52537},{\"end\":52951,\"start\":52887},{\"end\":53274,\"start\":53245},{\"end\":53552,\"start\":53500},{\"end\":53865,\"start\":53813},{\"end\":54081,\"start\":54031},{\"end\":54369,\"start\":54317},{\"end\":54662,\"start\":54610},{\"end\":54834,\"start\":54826},{\"end\":55435,\"start\":55431},{\"end\":55728,\"start\":55560},{\"end\":56293,\"start\":56289},{\"end\":56589,\"start\":56418},{\"end\":57211,\"start\":57207},{\"end\":57401,\"start\":57337},{\"end\":57539,\"start\":57513},{\"end\":57938,\"start\":57930},{\"end\":58288,\"start\":58284},{\"end\":58482,\"start\":58413},{\"end\":58573,\"start\":58565},{\"end\":59178,\"start\":59174}]"}}}, "year": 2023, "month": 12, "day": 17}