{"id": 221912722, "updated": "2022-03-27 10:13:15.12", "metadata": {"title": "Genome\u2010wide association study\u2010based deep learning for survival prediction", "authors": "[{\"first\":\"Tao\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Yue\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Ding\",\"middle\":[]}]", "venue": "Statistics in medicine", "journal": "Statistics in Medicine", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Informative and accurate survival prediction with individualized dynamic risk profiles over time is critical for personalized disease prevention and clinical management. The massive genetic data, such as SNPs from genome\u2010wide association studies (GWAS), together with well\u2010characterized time\u2010to\u2010event phenotypes provide unprecedented opportunities for developing effective survival prediction models. Recent advances in deep learning have made extraordinary achievements in establishing powerful prediction models in the biomedical field. However, the applications of deep learning approaches in survival prediction are limited, especially with utilizing the wealthy GWAS data. Motivated by developing powerful prediction models for the progression of an eye disease, age\u2010related macular degeneration (AMD), we develop and implement a multilayer deep neural network (DNN) survival model to effectively extract features and make accurate and interpretable predictions. Various simulation studies are performed to compare the prediction performance of the DNN survival model with several other machine learning\u2010based survival models. Finally, using the GWAS data from two large\u2010scale randomized clinical trials in AMD with over 7800 observations, we show that the DNN survival model not only outperforms several existing survival prediction models in terms of prediction accuracy (eg, c\u2010index =0.76), but also successfully detects clinically meaningful risk subgroups by effectively learning the complex structures among genetic variants. Moreover, we obtain a subject\u2010specific importance measure for each predictor from the DNN survival model, which provides valuable insights into the personalized early prevention and clinical management for this disease.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "3088475214", "acl": null, "pubmed": "32974946", "pubmedcentral": null, "dblp": null, "doi": "10.1002/sim.8743"}}, "content": {"source": {"pdf_hash": "a87928fa8e628930024d0dbfb6b80d05a77590a9", "pdf_src": "Wiley", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8056253", "status": "GREEN"}}, "grobid": {"id": "d6d15aba48414840554983f77df72687de8ecfea", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a87928fa8e628930024d0dbfb6b80d05a77590a9.txt", "contents": "\nGenome-wide association study-based deep learning for survival prediction\n\n\nTao Sun \nDepartment of Biostatistics\nUniversity of Pittsburgh\nPittsburghPennsylvaniaUSA\n\nCenter for Applied Statistics and School of Statistics\nRenmin University of China\nBeijingChina\n\nYue Wei \nDepartment of Biostatistics\nUniversity of Pittsburgh\nPittsburghPennsylvaniaUSA\n\nWei Chen \nDepartment of Biostatistics\nUniversity of Pittsburgh\nPittsburghPennsylvaniaUSA\n\nDepartment of Pediatrics\nCorrespondence Ying Ding\nDepartment of Biostatistics\nUniversity of Pittsburgh\nPittsburghPennsylvaniaUSA\n\nUniversity of Pittsburgh\n130 De Soto Street15261PittsburghPAUSA\n\nYing Ding yingding@pitt.edu \nDepartment of Biostatistics\nUniversity of Pittsburgh\nPittsburghPennsylvaniaUSA\n\nGenome-wide association study-based deep learning for survival prediction\n10.1002/sim.8743Received: 16 October 2019 Revised: 25 May 2020 Accepted: 15 August 2020R E S E A R C H A R T I C L E\nInformative and accurate survival prediction with individualized dynamic risk profiles over time is critical for personalized disease prevention and clinical management. The massive genetic data, such as SNPs from genome-wide association studies (GWAS), together with well-characterized time-to-event phenotypes provide unprecedented opportunities for developing effective survival prediction models. Recent advances in deep learning have made extraordinary achievements in establishing powerful prediction models in the biomedical field. However, the applications of deep learning approaches in survival prediction are limited, especially with utilizing the wealthy GWAS data. Motivated by developing powerful prediction models for the progression of an eye disease, age-related macular degeneration (AMD), we develop and implement a multilayer deep neural network (DNN) survival model to effectively extract features and make accurate and interpretable predictions. Various simulation studies are performed to compare the prediction performance of the DNN survival model with several other machine learning-based survival models.Finally, using the GWAS data from two large-scale randomized clinical trials in AMD with over 7800 observations, we show that the DNN survival model not only outperforms several existing survival prediction models in terms of prediction accuracy (eg, c-index =0.76), but also successfully detects clinically meaningful risk subgroups by effectively learning the complex structures among genetic variants. Moreover, we obtain a subject-specific importance measure for each predictor from the DNN survival model, which provides valuable insights into the personalized early prevention and clinical management for this disease.\n\nregarding the disease progression pattern in the future and shapes the physician's decision making for the treatment or clinical management strategy. Note that the survival prediction is fundamentally different from typical prediction models that predict a future event (whether occurs or not) by fixing the time of interest through a binary classification. 3,4 Despite its essential role in precision medicine, the survival prediction remains a challenging task, [5][6][7] largely due to the complex nature of diseases and the heterogeneity between patients. Therefore, there is an urgent need for developing accurate and personalized survival prediction models with improved capacity in learning the complex structures and interplays among predictors. Recent advances in high-throughput technologies have generated large volumes of molecular profiling data for each patient, which provides unprecedented opportunities in identifying potential biomarkers and further establishing accurate survival prediction models. [8][9][10] In particular, several national-wide large-scale longitudinal studies, such as the trans-omics for precision medicine and All of Us, are underway using whole-genome sequencing and other omics technologies, with the ultimate goal of accelerating precision medicine. However, how to effectively utilize the wealthy amount of data is challenging. The first challenge comes from how to connect high-dimensional predictors with the outcome of interest. This problem is particularly difficult in survival prediction because the events of interest are sometimes censored due to either a short study period or loss of follow-up during the study. The second challenge is how to model the complex structure among numerous biomarkers, where the specific structure is largely unknown. The third challenge is that given the heterogeneity of patients, how to interpret the importance of each predictor for each patient and further how to identify patient subgroups to provide personalized prevention or treatment strategy.\n\nThe recent advances in multilayer deep neural network (DNN) models have made extraordinary achievements in providing new effective risk prediction models from complex and high-dimensional biomedical data, such as omics and biomedical imaging. [11][12][13][14] However, the application of deep learning in survival prediction is still limited. Faraggi and Simon 15 proposed a single-layer neural network based on the Cox proportional hazards (PH) model. However, its performance did not exceed the regular Cox model in a prostate cancer survival dataset with 475 patents and only four clinical predictors. More recently, multiple efforts have been devoted to evaluating Cox-based neural network survival models using larger datasets with omic biomarkers. For example, Katzman et al 16 demonstrated that a single hidden layer neural network survival model performed marginally better than the Cox model and random survival forest (RSF) model in a breast cancer survival dataset with 1980 patients and nine predictors. In another study, Ching et al 17 applied a single hidden layer neural network survival model to 10 TCGA cancer survival datasets (sample sizes range from 302 to 1077) with high-throughput gene expression biomarkers, from which the neural network survival models resulted in comparable or better performance than the Cox model, the penalized Cox models such as Cox-LASSO and the RSF model. In another study 18 that also used TCGA cancer survival datasets (sample sizes range from 194 to 1092 with up to 17 000 gene expression biomarkers), the neural network survival models yielded comparable performance to the penalized Cox model and better performance than the RSF model. Hao et al 19 developed a pathway-based neural network survival model and applied it to a TCGA cancer dataset (sample size 522 with 860 pathways and 5567 genes). However, all these studies have limited sample sizes, particular in the presence of tens of thousands of predictors, and thus may lead to severe model overfitting problem. Moreover, the patient-specific predictor importance was not considered in those studies. The scenario of tied events, which is commonly seen in practice, especially when the sample size is large, was not carefully considered in these studies.\n\nIn this article, we propose and evaluate a multi-hidden-layer Cox-based DNN survival model to predict the progression of a progressive eye disease, namely, age-related macular degeneration (AMD). The genome-wide association study (GWAS) of AMD is the first and most successful GWAS research, where the massive GWAS data provide unprecedented opportunities to study disease risk and progression. Although some attempts have been tried to predict AMD progression risks using genetic information such as SNPs, most statistical models focus on the structured regression framework, which typically only accounts for (generalized) linear effects of the SNPs and thus have considerable limitations. To the best of our knowledge, there has no existing work on survival prediction using deep learning to effectively extract features from the GWAS data. Therefore, we develop and apply the DNN survival model to build an accurate and interpretable prediction model for the AMD progression.\n\nThe rest of the article is organized as follows. Section 2 describes the deep learning survival methods and prediction evaluation procedures. We assess the performance of three machine/deep learning survival prediction models (DNN, LASSO, RSF) through extensive simulation studies in Section 3 and apply them to the GWAS data from two large-scale clinical studies of AMD in Section 4. Discussion and conclusion are presented in Section 5.\n\n\nMETHODS\n\nFirst, we define notation for survival observations. For each subject i \u2208 {1, \u2026 , n}, the observations are\n{Y i , i , Z i }, where Y i = min (T i , C i )\nis the minimum of survival time T i and censoring time C i ; i = I(T i \u2264 C i ) is the (right-)censoring indicator; Z i is the vector of covariates.\n\n\nCox-based DNN survival model\n\nThe Cox PH model is the most popular regression model for censored survival data. It assumes that the hazard function of survival time T takes the form h(\nt|Z i ) = h 0 (t) exp(Z T i ),\nwhere h 0 (t) is the unspecified baseline hazard function at time t and is a vector of covariate effects. The term Z T i is called the linear predictor or prognostic index. On the other hand, the DNN model is well known for its capacity in learning complex covariate structures (ie, nonlinearity, interactions). 20 By the universal approximation theorem, 21,22 for any continuous function g(Z; ), it is guaranteed to exist a neural network that approximates this function. Moreover, this theorem holds even if we restrict the neural networks to have just one single hidden layer. Therefore, even very simple neural network architecture can be extremely powerful. The synergy of the powerful DNN and the popular Cox model leads us to build the Cox-based DNN survival model and apply it to AMD progression prediction.\n\n\nAssumption and loss function of DNN survival model\n\nThe DNN survival model we consider here can be written as h(t|Z i ) = h 0 (t)e g(Z i ; ) . The major difference between this DNN model and the regular Cox model is that DNN takes the prognostic index g(Z i ; ) as an unknown function with parameters , instead of assuming a simple linear relationship. In this way, the DNN model can approximate various nonlinear covariate structures by estimating g(Z i ; ). We will employ a feedforward DNN with multiple hidden layers to estimate the unspecified g(Z; ), as shown in Sections 2.1.2 and 2.1.3. In fact, one can regard the regular Cox model as a special case of DNN when g(Z i ; ) = Z T i . In large-scale clinical and observational studies, it is quite common that more than one observations develop events at the same time. Such events are called tied events. To handle this scenario, we approximate the partial likelihood via Efron's approach. 23 Moreover, to deal with high-dimensional covariates, we introduce the L 1 penalty to the DNN loss function \u2212l( ; Z) + || || 1 , where l( ; Z) is the Efron approximation of log partial likelihood:\nl( ; Z) = 1 N D \u2211 j\u2208D \u23a7 \u23aa \u23a8 \u23aa \u23a9 \u2211 i\u2208H j g(Z i ; ) \u2212 m j \u22121 \u2211 l=0 log \u239b \u239c \u239c \u239d \u2211 i\u2208R j e g(Z i ; ) \u2212 l m j \u2211 i\u2208H j e g(Z i ; ) \u239e \u239f \u239f \u23a0 \u23ab \u23aa \u23ac \u23aa \u23ad ,(1)\nwhere D is the set of all events with size N D and {t j } is the set of unique event times; H j is the set of subjects {i} such that Y i = t j and i = 1 and m j is the size of H j ; and R j is the risk set satisfying Y i \u2265 t j .\n\n\nDNN architecture\n\nFirst, we introduce the general form of an L-hidden-layer feedforward DNN, which is composed of one input layer, L hidden layers and one output layer (with one node in our case). For each subject, DNN inputs the vector of covariates Z into its input layer and output a scalar prognostic index g(Z; ). For each hidden layer l \u2208 {1, \u2026 , L} with n l number of nodes, it takes the input n l \u2212 1 -dimensional a (l \u2212 1) from the (l \u2212 1)th layer and outputs n l -dimensional a (l) through a n l -dimensional activation function f l . Mathematically, the lth hidden layer model can be written as a (l) = f (l) (W 0 (l) + W (l) a (l\u22121) ), where W 0 (l) is the bias vector with length n l ; W (l) is an n l \u00d7 n l \u2212 1 weight matrix. f (l) (\u22c5) is a vector of activation functions f (l) (\u22c5).\n\nOften a common f (l) (\u22c5) function is assumed for all the nodes in the lth hidden layer and it is usually a nonlinear function, such as the sigmoid 22 f (x) = 1 1+e \u2212x , the tangent f (x) = e x \u2212e \u2212x e x +e \u2212x , the rectified linear unit (ReLU) 24 f (x) = max (0, x), and the scaled exponential linear units (SeLU) 25 f (x) = \u00d7 ReLU(x) + I(x < 0) (e x \u2212 1), where and are constants. The final or output layer also has weights and an output function f out , which is an identity function. Take a simple one-hidden layer neural network, for example. We have p-dimensional input covariates z i from the ith subject, n 1 number of hidden nodes with k = 1, \u2026 , n 1 and one single output node. For the kth hidden node, we have a (1) (1) k by assuming f out is an identity function. Typically we have o i = g(z i ; ). The full parameter set is composed of {w (1) k0 , k = 1, \u2026 , n 1 }, {w (1) kj , k = 1, \u2026 , n 1 , j = 1, \u2026 , p}, w (2) 0 , and {w (2) k , k = 1, \u2026 , n 1 }.\nk = f (1) k (w (1) k0 + \u2211 p j=1 w (1) kj z ij ). Similarly, the output node is o i = f out (w (2) 0 + \u2211 n 1 k=1 w (2) k a (1) k ) = w (2) 0 + \u2211 n 1 k=1 w (2) k a\n\nDNN optimization and survival prediction\n\nTo solve for\u0302, we use the mini-batch stochastic gradient descent algorithm 26 to minimize the loss function in Equation (1).\n\nComparing with the standard stochastic gradient descent that uses all samples for each iteration, the mini-batch algorithm is much faster. Specifically, we randomly divide all observations into mini-batches with size N B and update\u0302by adding the gradient contributed by each mini-batch. In particular, the loss function for the rth batch is\n\u2212l r ( ; Z) + || || 1 = \u2212 1 N r D \u2211 j\u2208D r \u23a7 \u23aa \u23a8 \u23aa \u23a9 \u2211 i\u2208H r j g(Z i ; ) \u2212 m r j \u22121 \u2211 l=0 log \u239b \u239c \u239c \u239d \u2211 i\u2208R r j e g(Z i ; ) \u2212 l m r j \u2211 i\u2208H r j e g(Z i ; ) \u239e \u239f \u239f \u23a0 \u23ab \u23aa \u23ac \u23aa \u23ad + || || 1 ,\nwhere N r D , D r , H r j , m r j , and R r j are the corresponding terms for the rth batch similar to those defined in Equation (1). Then we update by adding the gradient contributed by the rth batch through:\n\u0394 r = \u2212\u25bd l r ( ; Z) + \u25bd || || 1 \u2190 \u2212 \u0394 r ,\nwhere is the learning rate (also called step size). This process will be repeated for N E times (also called epochs) before convergence. We employ the Glorot uniform initializer 27 to randomly select initial values. Once we get\u011d(Z i ;\u0302), we can obtain the predicted survival probability for subject i at time t through\u015c(t|Z i ) = exp{\u2212\u0124 0 (t)e\u0302g (Z i ;\u0302) }.\n\n\nDNN hyperparameters\n\nTo perform the survival prediction based on the DNN survival model, we need to select the DNN hyperparameters. The main hyperparameters include the number of hidden layers, number of nodes per hidden layer, choice of activation function, the L 1 penalty parameter, batch size, epoch size, and learning rate. Based on our limited experience, we found that selecting hyperparameters in a sequential manner helps to understand how each parameter influences the model prediction performance. We select hyperparameters in the following ordering: number of hidden layers, number of nodes per layer, activation function, learning rate, L 1 penalty, epoch size, and batch size. In this work, we perform cross-validations in the training data and select the combination of hyperparameters that lead to the most optimal prediction performance on the validation data. Specific hyperparameter choices are presented in Section 3 for simulation studies and in Section 4 for real data analysis.\n\n\nDNN interpretation\n\nIt is important to understand and interpret the fitted neural network prediction model. One way is to export feature (ie, predictor) importance measures that decide the top important features in a prediction model. The local interpretable model-agnostic explanation (LIME) method 28 provides prediction importance of each predictor for each subject in the model by perturbing the feature values and evaluating how the prediction results change. To perform LIME, we first perturb the value of one feature of one individual sample by adding some random noise (to get a new data point), and then obtain a new prognostic index\u011d from the DNN model. We repeat this perturbation (eg, for 1000 times). Next, we fit a simpler model (ie, a linear regression) between the 1000 pairs of perturbed feature values and their corresponding estimated prognostic index\u011d(Z;\u0302) values, and obtain the regression coefficient. We do this for all features across all samples. Finally, the most important features will be identified by the rank of absolute coefficient values. Therefore, the magnitude of the individualized feature importance reflects the estimated effect size on the prognostic index by increasing one unit value in this feature for each individual sample. LIME has been widely applied to neural network models with continuous or categorical outcomes, but not with censored survival outcomes yet. In this article, we apply the LIME method to the neural network survival model and produce subject-specific predictor importance measures with meaningful interpretations.\n\n\nEvaluation metrics for survival prediction performance\n\nWe calculate the Harrell's concordance index (c-index) 29 to measure the proportion of concordance pairs (ie, the predicted and observed outcomes are concordant) among all comparable pairs (ie, the true progression statuses can be ordered for two observations within one pair). Pairs are not comparable if both are censored, or one is censored at time c 1 and the other event occurs at time t 2 with t 2 > c 1 . The c-index is between 0 and 1 with a larger value indicating a better prediction model, which can be estimated by\u0108 =\n\u2211 n i=1 \u2211 n j=1 i I(Y i <Y j )I(\u011d(Z i ;\u0302)>\u011d(Z j ;\u0302))+0.5 * I(\u011d(Z i ;\u0302)=\u011d(Z j ;\u0302)) \u2211 n i=1 \u2211 n j=1 i I(Y i <Y j )+I(\u011d(Z i ;\u0302)=\u011d(Z j ;\u0302))\n.\n\nWe also obtain the time-dependent Brier score. 30,31 At a specific time point t, the Brier score measures the mean squared error between the observed progression status at time t (ie, Y i (t) = I(Y i \u2265 t)) and the predicted survival probability (ie,\u015c(t|Z i )). A lower Brier score indicates a better prediction model. A Brier score of 33% corresponds to predicting the risk by a random number drawn from Uniform [0, 1] and 25% corresponds to predicting 50% risk for every observation. The estimated Brier score is expressed asBS(t, 31 We also obtain the time-dependent ROC curve and its associated area under the curve (AUC). 32 The AUC measures the discrimination capability of\u011d(Z;\u0302). It ranges between 0 and 1, with higher AUC indicating better discrimination ability. Specifically, we first derive the time-dependent sensitivity and specificity\n\u015c) = 1 M \u2211 i\u2208D M\u0174 i (t){Y i (t) \u2212\u015c(t|Z i )} 2 , where D M is the test dataset with size M,\u015c(t|Z i ) is estimated using the training data, and\u0174 i (t) = (1\u2212Y i (t)) \u00ee G(Y i \u2212) + Y i (t) G(t) is the inverse probability of censoring weights with\u011c(t) =P(C > t).sensitivity(c, t) = P{\u011d(Z;\u0302) > c|T \u2264 t}, specificity(c, t) = P{\u011d(Z;\u0302) \u2264 c|T > t},\nwhere c is some arbitrary cut-off. For a given t, sensitivity(c, t), and specificity(c, t) determine the ROC curve profile and its associated AUC at time t.\n\n\nK-fold cross-validations\n\nOverfitting is a common issue for all machine learning models. One way to alleviate the issue is to perform K-fold cross-validation. Specifically, the original data D N are split into K subsets D k , k = 1, \u2026 , K, accounting for the censoring proportions. For the kth cross-validation, models are trained in the samples D N \u2216 D k (original data without the kth subset) and then validated in the test samples D k . Finally, the K-fold cross-validation estimates (ie, c-index and Brier score) are calculated by averaging over the test data results, as shown belo\u0175\nCvBS(t,\u015c) = 1 K K \u2211 k=1 1 M k \u2211 i\u2208D k\u0174 i (t){Y i (t) \u2212\u015c k (t|Z i )} 2 , CvC = 1 K K \u2211 k=1 1 M K \u2211 i\u2208D k \u2211 j\u2208D k i I(Y i < Y j )I(\u011d k (Z i ;\u0302k) >\u011d k (Z j ;\u0302k)) + 0.5I(\u011d k (Z i ;\u0302k) =\u011d k (Z j ;\u0302k)) \u2211 i\u2208D k \u2211 j\u2208D k i I(Y i < Y j ) + I(\u011d k (Z i ;\u0302k) =\u011d k (Z j ;\u0302k)) ,\nwhere M k is the sample size of the kth subset.\n\n\nImplementation\n\nOur DNN survival model is built with Keras 33 and Tensorflow 34 to ensure computational stability and efficiency. Keras is a deep learning framework that provides a convenient way to define and train deep learning models. It provides high-level building blocks for deep learning models. 35 For example, one can define a neural network model with a few lines of codes in Keras. We use Tensorflow for low-level operations such as differentiation, which serves as the backend engine of Keras. Via Keras and Tensorflow, our DNN survival model is compatible with both GPUs and CPUs.\n\n\nSIMULATION STUDIES\n\nWe use simulations to evaluate the prediction performance of DNN and compare it with Cox-LASSO (abbreviated as LASSO) 36 and RSF. 37,38 Two main simulation settings are considered. In the first setting, data are generated with sparse signals (ie, only a few predictors with nonzero effects on the survival outcome). In the second setting, all predictors have nonzero but weak signals, which is common in settings with genetics or genomics predictors. Within each simulation setting, we generate multiple scenarios with different structures in predictors' effects. For each scenario, we train the models in a training dataset, and then test them in an independent test dataset and summarize the results across 200 replications. The sample sizes for both training and test datasets are 1000. All three models involve the selection of tuning parameters. For LASSO, we use fivefold cross-validation to select the tuning parameter in the L 1 penalty using the training data. After the tuning parameter is determined, we then train the LASSO model using the entire training data and finally validate the model in the test data. For RSF, we train the model by utilizing the default setting of 1000 trees and \u221a p number of randomly selected predictors at each split. In the case of DNN, it is widely known for its exhaustive process in selecting optimal tuning parameters since there are many tuning parameters to consider. The tuning process is even more time consuming given that we have multiple simulation scenarios. Therefore, for all simulation scenarios, we use the sequential tuning strategy as described in Section 2.1.4 and choose one common set of hyperparameters as follows: two hidden layers, 30 nodes per hidden layer, activation function SeLU, L 1 penalty =0.1, batch size N B = 50, epoch size N E = 1000, and learning rate = 0.01 (for sparse signals) or = 0.0001 (for weak signals).\n\n\nSimulation I: Survival data with sparse signals\n\nWe consider five scenarios of predictor effects following Mi et al, 39 which includes linear effects only (scenario 1) and linear effects together with nonlinear effects (scenario 2) or with interactions (scenario 3) or with both nonlinear and interaction effects (scenario 4) or with nonlinear, interaction and threshold effects (scenario 5). The total number of predictors is set at p = 10, 50, 100, 500, respectively. The true models for these five scenarios are illustrated as follows:\nScenario 1 \u2236 h(t|Z i ) = h 0 (t) exp ( 5 \u2211 j=1 Z ij ) , Scenario 2 \u2236 h(t|Z i ) = h 0 (t) exp ( 5 \u2211 j=1 Z ij + Z 2 i6 + Z 2 i7 ) , Scenario 3 \u2236 h(t|Z i ) = h 0 (t) exp ( 5 \u2211 j=1 Z ij + Z i6 + Z i7 + 5Z i6 Z i7 ) , Scenario 4 \u2236 h(t|Z i ) = h 0 (t) exp ( 5 \u2211 j=1 Z ij + Z i6 + Z i7 + 5Z i6 Z i7 + Z 2 i8 + Z 2 i9 ) , Scenario 5 \u2236 h(t|Z i ) = h 0 (t) exp ( 5 \u2211 j=1 Z ij + Z i6 + Z i7 + 5Z i6 Z i7 + I(Z i8 < \u22120.5 \u222a Z i9 < \u22120.5) \u2212 I(Z i8 \u2265 \u22120.5 \u2229 Z i9 \u2265 \u22120.5) ) ,\nwhere h 0 (t) = k k t k\u22121 is the baseline Weibull hazard function with = 0.1, k = 2. For Z i = (Z i1 , \u2026 , Z ip ), we first generate Z i from MVN(0, \u03a3) with \u03a3 = { jj\u2032 = e \u2212|j\u2212j\u2032| , 1 \u2264 j, j\u2032 \u2264 p} and then transform Z i4 into a binary predictor through I(Z i4 > 0) and Z i5 into a multinomial predictor through I(Z i5 > \u22120.5) + I(Z i5 > 0.5). The right censoring rates are set at 50%. In Table 1, we compare the prediction accuracy of the DNN, RSF, LASSO under the five simulation scenarios in terms of c-index. We also present the c-index from fitting the true model as the bench mark. LASSO performs the best in scenario 1 where all predictor effects are linear, but its performance declines in all the other four scenarios. RSF generally has higher c-index than LASSO in nonlinear scenarios. For our proposed method, its performance is worse than LASSO as expected in scenario 1 but is better than RSF, while it outperforms both LASSO and RSF in all nonlinear scenarios. Moreover, it can be seen that when p is small, DNN produces c-index values that are very close to the truth for all five scenarios. In this sparse simulation setting, all three methods' performance (in terms of c-index) declines as p increases. LASSO is most robust to the increase of p among all three methods. The performance of DNN seems to be mostly affected when p increases. However, it still achieves the highest c-index for the complex nonlinear scenarios (3, 4, and 5) compared with the other two methods across all p's.\n\n\nSimulation II: Survival data with weak signals\n\nIn genetics and genomics data, we often observe that many predictors have (nonzero) weak effects due to correlations among SNPs or genes. Moreover, there are various types of omics predictors, such as gene expressions (ie, continuous), mutations (ie, binary), and SNPs (ie, multinomial). Therefore, we generate data that include various types of predictors with weak effects. The total number of predictors is set as p = 20, 50, 100, 500 and we consider the following five scenarios:\nScenario 1 \u2236 h(t|Z i ) = h 0 (t) exp ( p \u2211 j=1 j Z ij ) , Scenario 2 \u2236 h(t|Z i ) = h 0 (t) exp ( p \u2211 j=1 j Z ij + Z 2 i1 + Z 2 i2 ) , Scenario 3 \u2236 h(t|Z i ) = h 0 (t) exp ( p \u2211 j=1 j Z ij + Z i3 Z i4 ) , Scenario 4 \u2236 h(t|Z i ) = h 0 (t) exp ( p \u2211 j=1 j Z ij + Z 2 i1 + Z 2 i2 + Z i3 Z i4 ) , Scenario 5 \u2236 h(t|Z i ) = h 0 (t) exp ( p \u2211 j=1 j Z ij + I(Z i1 < \u22120.5 \u222a Z i2 < \u22120.5) \u2212 I(Z i1 \u2265 \u22120.5 \u2229 Z i2 \u2265 \u22120.5) + Z i3 Z i4 ) ,\nwhere h 0 (t) is the baseline Weibull hazard function with = 0.01, k = 10. Similarly to the first simulation setting, we first generate Z i from a multivariate normal distribution MVN(0, \u03a3) with \u03a3 = { jj \u2032 = e \u2212|j\u2212j \u2032 | , 1 \u2264 j, j \u2032 \u2264 p}. Then the first 20% Z ij remain continuous, the second 20% Z ij are transformed into binary predictors through I(Z ij > 0) and the rest 60% Z ij are transformed into multinomial predictors through I(Z ij > \u22120.5) + I(Z ij > 0.5). For predictor effects, we set j = 0.2 for continuous and binary predictors. For multinomial predictors, we mimic the linkage disequilibrium effect in SNP data by generating j from MVN(0.2, 0.01 \u00d7 \u03a3) with the same \u03a3. The right censoring rates are 50%. Table 2 summarizes the prediction performance results (in terms of c-index) under the five simulation scenarios. As the size of p increases, our proposal method improves in all scenarios. In particular, when p is large (eg, p = 500), our proposed method outperforms the other two models significantly in all simulation settings. The c-index of LASSO also increases as p gets larger, but remains unchanged or even slightly decreases when p increases from 100 to 500. RSF also improves with larger p but its performance is generally lower than the other two methods. \n\n\nSimulation III: Sample size effect on prediction performance\n\nWe also evaluate the effect of sample sizes on the prediction performance of the DNN survival model in the presence of large-dimensional predictors, given that is usually when DNN models show advantages. We choose the scenarios 4 and 5 with p = 100 or 500 under the sparse signal setting from Section 3.1. Table 3 presents the c-index values for each scenario.\n\nOverall, for both scenarios, the c-index increases as the sample size increases, and the increment is more dramatic between smaller sample sizes such as from n = 200 to 500 or n = 500 to 1000. This demonstrates that the DNN survival model requires a moderately large sample size (ie, n = 1000 at least) to achieve reasonable prediction performance when the number of predictors is relatively large (p = 100 or more).\n\n\nAPPLICATION TO AREDS DATA\n\n\nStudy population\n\nWe apply the three machine learning models for predicting AMD progression using genetic and clinical variables. Data are from the age-related eye disease studies (AREDS), which is composed of the first study AREDS 40 and the subsequent study AREDS2 41 (with independent participants), designed to assess risk factors and effects of various supplements on AMD development and progression. Both studies collected DNA samples of consenting participants. 42 The two studies are combined for the following model development and analysis.\n\n\nSurvival outcome and baseline predictors\n\nTo measure the disease progression, a severity score, scaled from 1 to 12 (with larger value indicating more severe AMD), is determined for each eye at every examination during study follow-up. In this article, our outcome of interest is time-to-late-AMD, where \"late-AMD\" is defined as the stage with severity score \u22659. There are 30% of subjects progressed to late-AMD before the study ends. We develop prediction models on the individual eye level. There are a total of 7803 eyes free of late-AMD at baseline. We include a list of potential predictors, including age at baseline, smoking status (never, former, or current smoker), education status (\u2264 or > high school), and top common SNPs (MAF >5%) that have been reported to be associated with AMD progression (identified from the GWAS study of AMD progression in Yan et al 43 with various p-value cut-offs). Table 4 summarizes the baseline characteristics of the study samples. We also preprocess the continuous predictors, for example, dividing age by 100 to scale it within (0, 1) and dividing SNP data (originally coded between [0, 2]) by 2 to make them within [0, 1], as we find such a scaling procedure enhances the prediction performance in survival machine learning models.\n\n\nModel development and evaluation\n\nWe perform 10-fold cross-validation in the combined AREDS and AREDS2 data. The splitting is stratified based on the censoring status and study cohort. For LASSO and RSF, we use the same tuning procedure as in the simulations. For \n\n\n4) 77\n\nNote: The last column shows the DNN's computing time for running on the entire dataset once. Abbreviations: AREDS, age-related eye disease studies; DNN, deep neural network; GRS, genetic risk score; RSF, random survival forest. a GRS is invariant to the choice of p-value cut-offs as it does not use individual SNPs but rather a composite score.\n\nDNN, we first perform a grid search for tuning parameters and select the set of hyperparameters that gives the best average prediction performance (ie, c-index) across the 10 test validations. The final choice of DNN hyperparameters is given as follows: two hidden layers, 300 nodes per hidden layer, activation function SeLU, L 1 penalty =0.01, batch size N B = 50, epoch size N E = 1000, and learning rate = 0.00001. We also include Ridge (a Cox PH model with L 2 penalty) and a benchmark genetic risk score (GRS) model, which is a regular Cox PH model using age, smoking status, education status, and an AMD GRS from Ding et al 44 for comparisons. We first examine the prediction performance, measured by c-index (\u00d7100), employing various numbers of top genetic variants across different models. Specifically, we choose four different p-value cut-offs from the first AMD progression GWAS article 43 (ie, p < 10 \u22127 , 10 \u22126 , 10 \u22125 , 10 \u22124 ) to obtain different numbers of top variants, as shown in Table 5. The prediction performance becomes relatively stable for all methods when the p-value cut-off reaches 10 \u22125 , which corresponds to 663 SNPs (and three nongenetic predictors). We also include in the last column of Table 5 the DNN model computing time for fitting the entire data once. It can be seen that the computing time increases only moderately (slower than the linear trend) as the number of predictors increases. On average, it takes about 1 hour in the presence of 8000 observations and 1000 predictors.\n\nThen, we choose the result from p = 666 as our main result and report in Table 6 the c-index, 10-year AUC, and 10-year Brier score (a predictive error measurement) from all four models. DNN achieves higher c-index (76.1) and AUC (81.8) as well as lower Brier score (0.136) than all the other models including LASSO, Ridge, RSF, and the benchmark GRS model. The LASSO and Ridge produce very similar performance results in terms of all metrics. Figure 1 presents the time-dependent Brier scores for the test data (all 10 CV test datasets combined) under each prediction model. The Brier score profile from our DNN survival model is consistently lower than all the other models across most time points, demonstrating its better performance than the other models. Similarly, Figure 2 presents the TA B L E 6 The 10-fold cross-validation c-index (\u00d7100), 10-year AUC (\u00d7100), and 10-year Brier score from five survival models (GRS, LASSO, Ridge, RSF, DNN) in the AREDS and AREDS2 data \n\n\nDNN interpretation and subgroup identification\n\nTo interpret the DNN-based prediction, we obtain the prediction importance measure for the test data samples using the LIME method under our DNN survival model. We use ninefolds data to train a DNN model and then interpret the model in the rest onefold test data. One big advantage of the LIME method is that it provides a subject-specific interpretation of the predictor importance. Figure 3 illustrates the top clinical and genetic predictors (named by their corresponding gene names). Among the top predictors, (older) age and smoking are harmful (colored in red) to AMD progression, whereas genetic variants (carrying minor alleles) can be either harmful (red) or protective (green). For example, the minor allele of locus rs10922098 in the CFH gene region shows a protective effect for AMD progression; while the minor allele of locus rs12987936 in the CROCC2 gene region shows a harmful effect for AMD progression. Moreover, we notice that one predictor could be important for some subjects but may not be crucial for others (visualized by different vertical color bands within each predictor), which suggests there are possible heterogeneous subgroups in this population. Motivated by the heterogeneity across subjects shown in Figure 3, we further identify two distinct subgroups of AMD patients by performing the Gaussian mixture model on the predicted prognostic risk factors\u011d (output from the DNN model), as illustrated in the histogram of Figure 4. The corresponding Kaplan-Meier plot on progression-free probability indicates significantly different progression profiles between the two subgroups (namely, the low-risk and high-risk subgroups), with a very significant log-rank test result (p = 4.1 \u00d7 10 \u2212166 ). Furthermore, we find significant differences between the two subgroups in terms of age, smoking status, education level, and most top genetic variants in Figure 3. The  Table 7. On average, the high-risk individuals are older, with more smokers and lower education level compared with the low-risk individuals. The high-risk individuals also carry more AMD risk alleles compared with the low-risk individuals (eg, GRS is 1.07 vs 0.94). Moreover, as shown in Figure 5, the separate LIME plots for the two subgroups also demonstrate that the individual predictors' importance measures are different between the two subgroups. In particular, the harmful predictors generally have stronger impacts (darker in red) on the high-risk subgroup than in the low-risk subgroup; whereas the protective predictors show stronger impacts (darker in green) on the low-risk subgroup than the high-risk subgroup. These results provide potentially useful insights for the early prevention and tailored clinical management for the AMD patients.\n\n\nDISCUSSION AND CONCLUSION\n\nIn this work, we develop a multilayer DNN survival model and successfully apply it on a real study with both large n and large p to examine and evaluate its effectiveness in making accurate dynamic survival predictions and detecting clinically meaningful subgroups. To open up the \"black-box\" of DNN, a novel LIME method is implemented to calculate the individualized importance measure of each predictor. Moreover, our work demonstrates the power of DNN in the presence of various types of complex nonlinear structures in the predictors through extensive simulation studies. As we did not perform hyperparameter tuning separately for each scenario, further enhanced performance of DNN would be expected if separate tuning was performed. Some existing tools that are compatible with Keras and Tensorflow to facilitate such a hyperparameter searching process may be considered, for example, the Auto-Keras. 45 Our work presents the first deep-learning-based survival prediction model for AMD progression and the model framework can be readily applied to other progressive disorders where large GWAS or omics data are collected. We evaluate survival models based on the pooled dataset of AREDS and AREDS2, whereas Ding et al 44 used AREDS as the training data and AREDS2 as the test data. However, as noted by Ding et al, 44 AREDS and AREDS2 populations are different in multiple aspects such as disease severity and age (at enrollment). As a result, the top significant SNPs identified by GWAS are largely nonoverlapping between the two studies. 43 As expected, in Ding et al, 44 the GRS-based Cox model trained in AREDS achieves a c-index of 0.75 in AREDS but drops to 0.63 in AREDS2. To establish a prediction model that is generalizable to a broader AMD population, we pooled them together. Unsurprisingly, the benchmark GRS model performance in the pooled data improves to 0.73 in terms of a 10-fold CV-based c-index, as shown in Table 6. Abbreviations: AREDS, age-related eye disease studies; GRS, genetic risk score.\n\n\nTA B L E 7 Comparison\n\nbetween the low-risk (n = 2516) and high-risk (n = 5287) subgroups in AREDS and AREDS2 data\n\nOne potential limitation of our DNN survival model is that it involves tuning of multiple hyperparameters, which is usually computationally expensive. According to our real data analysis and simulations, we could heuristically start from a two-hidden-layer DNN and perform a grid search for the other tuning parameters such as the optimal node size. In general, the DNN model size should be moderate to avoid overfitting. Moreover, the utilization of GPUs could significantly boost the computing speed of our DNN survival model. To further improve the DNN survival model, there are multiple future directions. For example, one may first obtain low-dimensional signals by performing unsupervised feature extraction such as autoencoder 46 and then use the extracted signals as predictors. In this way, the noises in the original data can be reduced. Another possible extension is to build a DNN survival model based on the Bayesian approach, 47 which could perform variable selection to identify relevant predictors under the high-dimensional nonlinear setting. Finally, we predict disease progression on the eye level by assuming that the two eyes are independent of each other in one individual. Ideally one should take the correlation into account when constructing the prediction model. One possible extension includes using a copula model to account for the dependence between the two eyes from the same subject 48,49 and predicting the joint progression profiles of the two eyes through a DNN. We are investigating some of these extensions.\n\n\nDATA AVAILABILITY STATEMENT\n\nBoth phenotype and genotype data of AREDS and AREDS2 are available from the online repository dbGap (accession: phs000001.v3.p1, and phs001039.v1.p1, respectively). The core program codes and a well-written R Markdown tutorial can be found on GitHub (yingding99/DNNSurv). In the tutorial, we demonstrate in details how to use our codes to make predictions and generate performance metrics in the example datasets.\n\nAbbreviation: AREDS, age-related eye disease studies.\n\n\nORCID\n\nYing Ding https://orcid.org/0000-0003-1352-1000\n\n\narea under the curve; DNN, deep neural network; GRS, genetic risk score; RSF, random survival forest F I G U R E 3 The representation of individualized importance measures for the top predictors in one split of the test dataset from the LIME method. Each row represents one predictor and each vertical column represents one sample. The unit of age is 10-year. LIME, local interpretable model-agnostic explanation [Colour figure can be viewed at wileyonlinelibrary.com] comparison results are summarized in\n\n\nF I G U R E 4 The Kaplan-Meier estimatedprogression-free profiles for the two identified risk subgroups in the AREDS and AREDS2 test data. The gray curve represents the low-risk subgroup and the black curve represents the high-risk subgroup. The histogram shows the predicted diagnostic index values of all cross-validation results, with two subgroups identified by the Gaussian mixture model. AREDS, age-related eye disease studies F I G U R E 5 The representation of individualized importance measures for the top predictors in the low-risk, A and high-risk, B, subgroups, respectively. Each row represents one predictor and each vertical column represents one sample. The unit of age is 10-year [Colour figure can be viewed at wileyonlinelibrary.com]\n\n\nLow-risk subgroup Mean (SD) or n (%) or risk allele frequency High-risk subgroup Mean (SD) or n (%) or risk allele frequency p-values Smoke <2.2 \u00d7 10 \u221216Top predictors \n\nAge \n66.1 (5.4) \n71.1 (5.9) \n<2.2 \u00d7 10 \u221216 \n\nNever \n1343 (53%) \n2280 (43%) \n\nFormer \n1088 (43%) \n2664 (50%) \n\nCurrent \n85 (3%) \n343 (6%) \n\nEducation \n\n\u2264 high school \n625 (25%) \n1744 (33%) \n3.2 \u00d7 10 \u221213 \n\n> high school \n1891 (75%) \n3543 (67%) \n\nrs10922098 (CFH) \n0.34 \n0.61 \n<2.2 \u00d7 10 \u221216 \n\nrs11200638 (HTRA1) \n0.17 \n0.39 \n<2.2 \u00d7 10 \u221216 \n\nrs12987936 (CROCC2) \n0.18 \n0.18 \n0.35 \n\nrs147518956 (ADAMTS12) \n0.27 \n0.32 \n1.8 \u00d7 10 \u221213 \n\nrs200880300 (SV2C) \n0.04 \n0.06 \n1.0 \u00d7 10 \u22123 \n\nrs2186849 (LOC105371956) \n0.47 \n0.50 \n1.0 \u00d7 10 \u22123 \n\nrs3750847 (ARMS2) \n0.17 \n0.40 \n<2.2 \u00d7 10 \u221216 \n\nrs4044578 (CFHR4) \n0.33 \n0.62 \n<2.2 \u00d7 10 \u221216 \n\nOther characteristics \n\nGRS \n0.94 (0.13) \n1.07 (0.13) \n<2.2 \u00d7 10 \u221216 \n\nGender \n\nFemale \n1439 (57) \n3027 (57) \n0.98 \n\nMale \n1077 (43) \n2260 (43) \n\nBaseline severity \n3.0 (2.3) \n4.7 (2.4) \n<2.2 \u00d7 10 \u221216 \n\n\nACKNOWLEDGEMENTSWe thank the participants in the AREDS and AREDS2 studies and the International AMD Genomics Consortium for generating the genetic data and performing the quality check. This project was supported by the National Institutes of Health through Grant Number UL1TR001857.\nCancer genomics: from discovery science to personalized medicine. L Chin, J N Andersen, P A Futreal, Nature Medic. 173297Chin L, Andersen JN, Futreal PA. Cancer genomics: from discovery science to personalized medicine. Nature Medic. 2011;17(3):297.\n\nPrecision medicine core: progress in prognostication-populations to patients. C Compton, Ann Surg Oncol. 252Compton C. Precision medicine core: progress in prognostication-populations to patients. Ann Surg Oncol. 2018;25(2):349-350.\n\nApplication of artificial neural network-based survival analysis on two breast cancer datasets. C L Chi, W N Street, W H Wolberg, Proceedings of the 2007 of AMIA Annual Symposium Proceedings. the 2007 of AMIA Annual Symposium ProceedingsChicago130Paper presented atChi CL, Street WN, Wolberg WH. Application of artificial neural network-based survival analysis on two breast cancer datasets. Paper presented at: Proceedings of the 2007 of AMIA Annual Symposium Proceedings. Chicago: American Medical Informatics Association; 2007:130.\n\nA clinical index to define risk of asthma in young children with recurrent wheezing. J A Castro-Rodriguez, C J Holberg, A L Wright, F D Martinez, Am J Respirat Crit Care Medic. 1624Castro-Rodriguez JA, Holberg CJ, Wright AL, Martinez FD. A clinical index to define risk of asthma in young children with recurrent wheezing. Am J Respirat Crit Care Medic. 2000;162(4):1403-1406.\n\nPrognostic factor studies. M Schumacher, N Hollander, G Schwarzer, H Binder, W Sauerbrei, Handbook of Statistics in Clinical Oncology. Crowley J, Hoering ABoca Raton, FLChapman and Hall/CRC Press20123rd edSchumacher M, Hollander N, Schwarzer G, Binder H, Sauerbrei W. Prognostic factor studies. In: Crowley J, Hoering A, eds. Handbook of Statistics in Clinical Oncology. 3rd ed. Boca Raton, FL: Chapman and Hall/CRC Press; 2012:415-470.\n\nComputational Systems Biology of Cancer. E Barillot, L Calzone, A Zinovyev, P Hupe, J P Vert, CRC PressBoca Raton, FLBarillot E, Calzone L, Zinovyev A, Hupe P, Vert JP. Computational Systems Biology of Cancer. Boca Raton, FL: CRC Press; 2012.\n\nNational cancer institute's precision medicine initiatives for the new national clinical trials network. J Abrams, B Conley, M Mooney, Proceedings of the American Society of Clinical Oncology educational book. the American Society of Clinical Oncology educational bookChicago, USAAmerican SocietyPaper presented atAbrams J, Conley B, Mooney M, et al. National cancer institute's precision medicine initiatives for the new national clinical trials network. Paper presented at: Proceedings of the American Society of Clinical Oncology educational book. Chicago, USA: American Society of Clinical Oncology Annual Meeting; 2014:71-76.\n\nA new initiative on precision medicine. F S Collins, H Varmus, New Engl J Medic. 3729Collins FS, Varmus H. A new initiative on precision medicine. New Engl J Medic. 2015;372(9):793-795.\n\nWhole genome sequence analyses of brain imaging measures in the Framingham study. C Sarnowski, C L Satizabal, C Decarli, Neurology. 903Sarnowski C, Satizabal CL, DeCarli C, et al. Whole genome sequence analyses of brain imaging measures in the Framingham study. Neurology. 2018;90(3):e188-e196.\n\nEfficient variant set mixed model association tests for continuous and binary traits in large-scale whole-genome sequencing studies. H Chen, J E Huffman, J A Brody, Am J Human Genet. 1042Chen H, Huffman JE, Brody JA, et al. Efficient variant set mixed model association tests for continuous and binary traits in large-scale whole-genome sequencing studies. Am J Human Genet. 2019;104(2):260-274.\n\nDeep learning in bioinformatics. S Min, B Lee, S Yoon, Brief Bioinform. 185Min S, Lee B, Yoon S. Deep learning in bioinformatics. Brief Bioinform. 2016;18(5):851-869.\n\nDeep learning for healthcare: review, opportunities and challenges. R Miotto, F Wang, S Wang, X Jiang, J T Dudley, Brief Bioinform. 196Miotto R, Wang F, Wang S, Jiang X, Dudley JT. Deep learning for healthcare: review, opportunities and challenges. Brief Bioinform. 2017;19(6):1236-1246.\n\nPrediction of cardiovascular risk factors from retinal fundus photographs via deep learning. R Poplin, A V Varadarajan, K Blumer, Nature Biomed Eng. 23158Poplin R, Varadarajan AV, Blumer K, et al. Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning. Nature Biomed Eng. 2018;2(3):158.\n\nA deep learning algorithm for prediction of age-related eye disease study severity scale for age-related macular degeneration from color fundus photography. F Grassmann, J Mengelkamp, C Brandl, Ophthalmology. 1259Grassmann F, Mengelkamp J, Brandl C, et al. A deep learning algorithm for prediction of age-related eye disease study severity scale for age-related macular degeneration from color fundus photography. Ophthalmology. 2018;125(9):1410-1420.\n\nA neural network model for survival data. D Faraggi, R Simon, Stat Medic. 141Faraggi D, Simon R. A neural network model for survival data. Stat Medic. 1995;14(1):73-82.\n\nDeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. J L Katzman, U Shaham, A Cloninger, J Bates, T Jiang, Y Kluger, BMC Med Res Methodol. 18124Katzman JL, Shaham U, Cloninger A, Bates J, Jiang T, Kluger Y. DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC Med Res Methodol. 2018;18(1):24.\n\nCox-nnet: an artificial neural network method for prognosis prediction of high-throughput omics data. T Ching, X Zhu, L X Garmire, PLoS Comput Biol. 1441006076Ching T, Zhu X, Garmire LX. Cox-nnet: an artificial neural network method for prognosis prediction of high-throughput omics data. PLoS Comput Biol. 2018;14(4):e1006076.\n\nPredicting clinical outcomes from large scale cancer genomic profiles with deep survival models. S Yousefi, F Amrollahi, M Amgad, Sci Rep. 7111707Yousefi S, Amrollahi F, Amgad M, et al. Predicting clinical outcomes from large scale cancer genomic profiles with deep survival models. Sci Rep. 2017;7(1):11707.\n\nCox-PASNet: pathway-based sparse deep neural network for survival analysis. J Hao, Y Kim, T Mallavarapu, J H Oh, M Kang, Proceedings of the 2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). the 2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)Madrid, SpainIEEE2018Paper presented atHao J, Kim Y, Mallavarapu T, Oh JH, Kang M. Cox-PASNet: pathway-based sparse deep neural network for survival analysis. Paper presented at: Proceedings of the 2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). Madrid, Spain; 2018:381-386; IEEE.\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5217553436LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436.\n\nApproximation by superpositions of a sigmoidal function. G Cybenko, Math Control Sign Syst. 24Cybenko G. Approximation by superpositions of a sigmoidal function. Math Control Sign Syst. 1989;2(4):303-314.\n\nMultilayer feedforward networks are universal approximators. K Hornik, M Stinchcombe, H White, Neural Netw. 25Hornik K, Stinchcombe M, White H. Multilayer feedforward networks are universal approximators. Neural Netw. 1989;2(5):359-366.\n\nThe efficiency of Cox's likelihood function for censored data. B Efron, J Am Stat Assoc. 72359Efron B. The efficiency of Cox's likelihood function for censored data. J Am Stat Assoc. 1977;72(359):557-565.\n\nOn the importance of initialization and momentum in deep learning. I Sutskever, J Martens, G Dahl, G Hinton, PMLRProceedings of the International Conference on Machine Learning. the International Conference on Machine LearningAtlanta, USA2013Paper presented atSutskever I, Martens J, Dahl G, Hinton G. On the importance of initialization and momentum in deep learning. Paper presented at: Proceedings of the International Conference on Machine Learning. Atlanta, USA: PMLR; 2013:1139-1147.\n\nSelf-normalizing neural networks. G Klambauer, T Unterthiner, A Mayr, S Hochreiter, Advances in Neural Information Processing Systems. 10010 N Torrey Pines Rd. La Jolla, California, USA2017Klambauer G, Unterthiner T, Mayr A, Hochreiter S. Self-normalizing neural networks. Advances in Neural Information Processing Systems. 10010 N Torrey Pines Rd, La Jolla, California, USA: NIPS; 2017:971-980.\n\nNeural networks for machine learning lecture 6a overview of mini-batch gradient descent; CSE 250C Machine Learning Theory Lecture: University of California. G Hinton, N Srivastava, K Swersky, 14San DiegoHinton G, Srivastava N, Swersky K. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent; CSE 250C Machine Learning Theory Lecture: University of California, San Diego; 2012:14.\n\nUnderstanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio, Proceedings of the 13th International Conference on Artificial Intelligence and Statistics. the 13th International Conference on Artificial Intelligence and StatisticsSardinia, ItalyJMLR2010Paper presented atGlorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. Paper presented at: Proceedings of the 13th International Conference on Artificial Intelligence and Statistics. Sardinia, Italy: JMLR; 2010:249-256.\n\nWhy should i trust you? Explaining the predictions of any classifier. M T Ribeiro, S Singh, C Guestrin, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningSan FranciscoACM2016Paper presented atRibeiro MT, Singh S, Guestrin C. Why should i trust you? Explaining the predictions of any classifier. Paper presented at: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. San Francisco; 2016:1135-1144; ACM.\n\nMultivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors. F E Harrell, K L Lee, D B Mark, Stat Medic. 154Harrell FE, Lee KL, Mark DB. Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors. Stat Medic. 1996;15(4):361-387.\n\nAssessment and comparison of prognostic classification schemes for survival data. E Graf, C Schmoor, W Sauerbrei, M Schumacher, Stat Medic. 18Graf E, Schmoor C, Sauerbrei W, Schumacher M. Assessment and comparison of prognostic classification schemes for survival data. Stat Medic. 1999;18(17-18):2529-2545.\n\nConsistent estimation of the expected Brier score in general survival models with right-censored event times. T A Gerds, M Schumacher, Biometr J. 486Gerds TA, Schumacher M. Consistent estimation of the expected Brier score in general survival models with right-censored event times. Biometr J. 2006;48(6):1029-1040.\n\nTime-dependent ROC curves for censored survival data and a diagnostic marker. P J Heagerty, T Lumley, M S Pepe, Biometrics. 562Heagerty PJ, Lumley T, Pepe MS. Time-dependent ROC curves for censored survival data and a diagnostic marker. Biometrics. 2000;56(2):337-344.\n\n. F Chollet, KerasChollet F, 2015; Keras, GitHub. https://github.com/fchollet/keras.\n\nTensorflow: a system for large-scale machine learning. M Abadi, P Barham, J Chen, Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation. the 12th USENIX Symposium on Operating Systems Design and ImplementationSavannah: USENIX2016Paper presented atAbadi M, Barham P, Chen J, et al. Tensorflow: a system for large-scale machine learning. Paper presented at: Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation.Savannah: USENIX; 2016:265-283.\n\nDeep Learning with R. F Chollet, J J Allaire, Manning Publications CoGreenwich, CT1st ed.Chollet F, Allaire JJ. Deep Learning with R. 1st ed. Greenwich, CT: Manning Publications Co; 2018.\n\nRegression shrinkage and selection via the lasso. R Tibshirani, J Royal Stat Soc Ser B (Methodol). 581Tibshirani R. Regression shrinkage and selection via the lasso. J Royal Stat Soc Ser B (Methodol). 1996;58(1):267-288.\n\nRandom survival forests. H Ishwaran, U B Kogalur, E H Blackstone, M S Lauer, Ann Appl Stat. 23Ishwaran H, Kogalur UB, Blackstone EH, Lauer MS. Random survival forests. Ann Appl Stat. 2008;2(3):841-860.\n\nRandom survival forests for R. R News. H Ishwaran, U Kogalur, 7Ishwaran H, Kogalur U. Random survival forests for R. R News. 2007;7(2):25-31.\n\nBagging and deep learning in optimal individualized treatment rules. X Mi, F Zou, R Zhu, Biometrics. 752Mi X, Zou F, Zhu R. Bagging and deep learning in optimal individualized treatment rules. Biometrics. 2018;75(2):674-684.\n\nThe age-related eye disease study (AREDS): design implications. Areds Group, Controll Clin Trials. 206AREDS Group. The age-related eye disease study (AREDS): design implications. Controll Clin Trials. 1999;20(6):573-600.\n\nThe age-related eye disease study 2 (AREDS2): study design and baseline characteristics (AREDS2 report number 1). E Y Chew, T Clemons, J P Sangiovanni, Ophthalmology. 11911Chew EY, Clemons T, SanGiovanni JP, et al. The age-related eye disease study 2 (AREDS2): study design and baseline characteristics (AREDS2 report number 1). Ophthalmology. 2012;119(11):2282-2289.\n\nA large genome-wide association study of age-related macular degeneration highlights contributions of rare and common variants. L G Fritsche, W Igi, J N Bailey, Nature Genet. 482Fritsche LG, IgI W, Bailey JN. A large genome-wide association study of age-related macular degeneration highlights contributions of rare and common variants. Nature Genet. 2016;48(2):134-143.\n\nGenome-wide analysis of disease progression in age-related macular degeneration. Q Yan, Y Ding, Y Liu, Human Molecul Genet. 275Yan Q, Ding Y, Liu Y, et al. Genome-wide analysis of disease progression in age-related macular degeneration. Human Molecul Genet. 2018;27(5):929-940.\n\nBivariate analysis of age-related macular degeneration progression using genetic risk scores. Y Ding, Y Liu, Q Yan, Genetics. 2061Ding Y, Liu Y, Yan Q, et al. Bivariate analysis of age-related macular degeneration progression using genetic risk scores. Genetics. 2017;206(1):119-133.\n\nAuto-keras: an efficient neural architecture search system. H Jin, Q Song, X Hu, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningAnchorageAssociation for Computing Machinery2019Paper presented atJin H, Song Q, Hu X. Auto-keras: an efficient neural architecture search system. Paper presented at: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. Anchorage: Association for Computing Machinery; 2019:1946-1956.\n\nStacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion. P Vincent, H Larochelle, I Lajoie, Y Bengio, P A Manzagol, J Mach Learn Res. 11Vincent P, Larochelle H, Lajoie I, Bengio Y, Manzagol PA. Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion. J Mach Learn Res. 2010;11(Dec):3371-3408.\n\nBayesian neural networks for selection of drug sensitive genes. F Liang, Q Li, L Zhou, J Am Stat Assoc. 113523Liang F, Li Q, Zhou L. Bayesian neural networks for selection of drug sensitive genes. J Am Stat Assoc. 2018;113(523):955-972.\n\nCopula-based score test for bivariate time-to-event data, with application to a genetic study of AMD progression. T Sun, Y Liu, R J Cook, W Chen, Y Ding, Lifetime Data Anal. 253Sun T, Liu Y, Cook RJ, Chen W, Ding Y. Copula-based score test for bivariate time-to-event data, with application to a genetic study of AMD progression. Lifetime Data Anal. 2019;25(3):546-568.\n\nCopula-based semiparametric regression method for bivariate data under general interval censoring. T Sun, Y ; Ding, T Sun, Y Wei, W Chen, Y Ding, 10.1093/biostatistics/kxz032Statistics in Medicine. 39Biostatistics.Sun T, Ding Y. Copula-based semiparametric regression method for bivariate data under general interval censoring. Biostatistics. 2019. https://doi.org/10.1093/biostatistics/kxz032. How to cite this article: Sun T, Wei Y, Chen W, Ding Y. Genome-wide association study-based deep learning for survival prediction. Statistics in Medicine. 2020;39:4605-4620. https://doi.org/10.1002/sim.8743\n", "annotations": {"author": "[{\"end\":261,\"start\":77},{\"end\":350,\"start\":262},{\"end\":635,\"start\":351},{\"end\":744,\"start\":636}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":81},{\"end\":269,\"start\":266},{\"end\":359,\"start\":355},{\"end\":645,\"start\":641}]", "author_first_name": "[{\"end\":80,\"start\":77},{\"end\":265,\"start\":262},{\"end\":354,\"start\":351},{\"end\":640,\"start\":636}]", "author_affiliation": "[{\"end\":164,\"start\":86},{\"end\":260,\"start\":166},{\"end\":349,\"start\":271},{\"end\":439,\"start\":361},{\"end\":569,\"start\":441},{\"end\":634,\"start\":571},{\"end\":743,\"start\":665}]", "title": "[{\"end\":74,\"start\":1},{\"end\":818,\"start\":745}]", "venue": null, "abstract": "[{\"end\":2691,\"start\":936}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3053,\"start\":3051},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3054,\"start\":3053},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3160,\"start\":3157},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3163,\"start\":3160},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3166,\"start\":3163},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3714,\"start\":3711},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3717,\"start\":3714},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3721,\"start\":3717},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4979,\"start\":4975},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4983,\"start\":4979},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4987,\"start\":4983},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4991,\"start\":4987},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5095,\"start\":5093},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5515,\"start\":5513},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6156,\"start\":6154},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6434,\"start\":6432},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9264,\"start\":9262},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9308,\"start\":9305},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9310,\"start\":9308},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10717,\"start\":10715},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12336,\"start\":12334},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12406,\"start\":12404},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12815,\"start\":12812},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12819,\"start\":12816},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12944,\"start\":12941},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12974,\"start\":12971},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13032,\"start\":13029},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13337,\"start\":13335},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15829,\"start\":15827},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17223,\"start\":17221},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18369,\"start\":18367},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18463,\"start\":18461},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20387,\"start\":20385},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20818,\"start\":20816},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20831,\"start\":20828},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20833,\"start\":20831},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22710,\"start\":22708},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28678,\"start\":28676},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36898,\"start\":36896},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37312,\"start\":37310},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":37537,\"start\":37535},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37568,\"start\":37566},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":38866,\"start\":38864},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":39072,\"start\":39070},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":39548,\"start\":39545},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":39550,\"start\":39548}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40739,\"start\":40232},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41495,\"start\":40740},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":42492,\"start\":41496}]", "paragraph": "[{\"end\":4730,\"start\":2693},{\"end\":6997,\"start\":4732},{\"end\":7978,\"start\":6999},{\"end\":8418,\"start\":7980},{\"end\":8536,\"start\":8430},{\"end\":8731,\"start\":8584},{\"end\":8918,\"start\":8764},{\"end\":9765,\"start\":8950},{\"end\":10912,\"start\":9820},{\"end\":11289,\"start\":11061},{\"end\":12088,\"start\":11310},{\"end\":13054,\"start\":12090},{\"end\":13384,\"start\":13260},{\"end\":13726,\"start\":13386},{\"end\":14121,\"start\":13912},{\"end\":14521,\"start\":14164},{\"end\":15524,\"start\":14545},{\"end\":17107,\"start\":15547},{\"end\":17695,\"start\":17166},{\"end\":17833,\"start\":17832},{\"end\":18682,\"start\":17835},{\"end\":19177,\"start\":19021},{\"end\":19767,\"start\":19206},{\"end\":20079,\"start\":20032},{\"end\":20675,\"start\":20098},{\"end\":22588,\"start\":20698},{\"end\":23129,\"start\":22640},{\"end\":25091,\"start\":23589},{\"end\":25625,\"start\":25142},{\"end\":27333,\"start\":26050},{\"end\":27758,\"start\":27398},{\"end\":28176,\"start\":27760},{\"end\":28757,\"start\":28225},{\"end\":30037,\"start\":28802},{\"end\":30304,\"start\":30074},{\"end\":30659,\"start\":30314},{\"end\":32180,\"start\":30661},{\"end\":33160,\"start\":32182},{\"end\":35960,\"start\":33211},{\"end\":38011,\"start\":35990},{\"end\":38128,\"start\":38037},{\"end\":39674,\"start\":38130},{\"end\":40119,\"start\":39706},{\"end\":40174,\"start\":40121},{\"end\":40231,\"start\":40184}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8583,\"start\":8537},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8949,\"start\":8919},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11060,\"start\":10913},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13216,\"start\":13055},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13911,\"start\":13727},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14163,\"start\":14122},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17831,\"start\":17696},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18939,\"start\":18683},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19020,\"start\":18939},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20031,\"start\":19768},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23588,\"start\":23130},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26049,\"start\":25626}]", "table_ref": "[{\"end\":23983,\"start\":23976},{\"end\":26775,\"start\":26768},{\"end\":27711,\"start\":27704},{\"end\":29672,\"start\":29665},{\"end\":31668,\"start\":31661},{\"end\":31890,\"start\":31883},{\"end\":32262,\"start\":32255},{\"end\":35112,\"start\":35105},{\"end\":37930,\"start\":37923}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":8428,\"start\":8421},{\"attributes\":{\"n\":\"2.1\"},\"end\":8762,\"start\":8734},{\"attributes\":{\"n\":\"2.1.1\"},\"end\":9818,\"start\":9768},{\"attributes\":{\"n\":\"2.1.2\"},\"end\":11308,\"start\":11292},{\"attributes\":{\"n\":\"2.1.3\"},\"end\":13258,\"start\":13218},{\"attributes\":{\"n\":\"2.1.4\"},\"end\":14543,\"start\":14524},{\"attributes\":{\"n\":\"2.1.5\"},\"end\":15545,\"start\":15527},{\"attributes\":{\"n\":\"2.2\"},\"end\":17164,\"start\":17110},{\"attributes\":{\"n\":\"2.3\"},\"end\":19204,\"start\":19180},{\"attributes\":{\"n\":\"2.4\"},\"end\":20096,\"start\":20082},{\"attributes\":{\"n\":\"3\"},\"end\":20696,\"start\":20678},{\"attributes\":{\"n\":\"3.1\"},\"end\":22638,\"start\":22591},{\"attributes\":{\"n\":\"3.2\"},\"end\":25140,\"start\":25094},{\"attributes\":{\"n\":\"3.3\"},\"end\":27396,\"start\":27336},{\"attributes\":{\"n\":\"4\"},\"end\":28204,\"start\":28179},{\"attributes\":{\"n\":\"4.1\"},\"end\":28223,\"start\":28207},{\"attributes\":{\"n\":\"4.2\"},\"end\":28800,\"start\":28760},{\"attributes\":{\"n\":\"4.3\"},\"end\":30072,\"start\":30040},{\"end\":30312,\"start\":30307},{\"attributes\":{\"n\":\"4.4\"},\"end\":33209,\"start\":33163},{\"attributes\":{\"n\":\"5\"},\"end\":35988,\"start\":35963},{\"end\":38035,\"start\":38014},{\"end\":39704,\"start\":39677},{\"end\":40182,\"start\":40177}]", "table": "[{\"end\":42492,\"start\":41651}]", "figure_caption": "[{\"end\":40739,\"start\":40234},{\"end\":41495,\"start\":40742},{\"end\":41651,\"start\":41498}]", "figure_ref": "[{\"end\":32633,\"start\":32625},{\"end\":32961,\"start\":32953},{\"end\":33603,\"start\":33595},{\"end\":34454,\"start\":34446},{\"end\":34670,\"start\":34662},{\"end\":35098,\"start\":35090},{\"end\":35402,\"start\":35394},{\"end\":39869,\"start\":39818}]", "bib_author_first_name": "[{\"end\":42844,\"start\":42843},{\"end\":42852,\"start\":42851},{\"end\":42854,\"start\":42853},{\"end\":42866,\"start\":42865},{\"end\":42868,\"start\":42867},{\"end\":43107,\"start\":43106},{\"end\":43359,\"start\":43358},{\"end\":43361,\"start\":43360},{\"end\":43368,\"start\":43367},{\"end\":43370,\"start\":43369},{\"end\":43380,\"start\":43379},{\"end\":43382,\"start\":43381},{\"end\":43884,\"start\":43883},{\"end\":43886,\"start\":43885},{\"end\":43906,\"start\":43905},{\"end\":43908,\"start\":43907},{\"end\":43919,\"start\":43918},{\"end\":43921,\"start\":43920},{\"end\":43931,\"start\":43930},{\"end\":43933,\"start\":43932},{\"end\":44204,\"start\":44203},{\"end\":44218,\"start\":44217},{\"end\":44231,\"start\":44230},{\"end\":44244,\"start\":44243},{\"end\":44254,\"start\":44253},{\"end\":44656,\"start\":44655},{\"end\":44668,\"start\":44667},{\"end\":44679,\"start\":44678},{\"end\":44691,\"start\":44690},{\"end\":44699,\"start\":44698},{\"end\":44701,\"start\":44700},{\"end\":44964,\"start\":44963},{\"end\":44974,\"start\":44973},{\"end\":44984,\"start\":44983},{\"end\":45531,\"start\":45530},{\"end\":45533,\"start\":45532},{\"end\":45544,\"start\":45543},{\"end\":45760,\"start\":45759},{\"end\":45773,\"start\":45772},{\"end\":45775,\"start\":45774},{\"end\":45788,\"start\":45787},{\"end\":46107,\"start\":46106},{\"end\":46115,\"start\":46114},{\"end\":46117,\"start\":46116},{\"end\":46128,\"start\":46127},{\"end\":46130,\"start\":46129},{\"end\":46404,\"start\":46403},{\"end\":46411,\"start\":46410},{\"end\":46418,\"start\":46417},{\"end\":46607,\"start\":46606},{\"end\":46617,\"start\":46616},{\"end\":46625,\"start\":46624},{\"end\":46633,\"start\":46632},{\"end\":46642,\"start\":46641},{\"end\":46644,\"start\":46643},{\"end\":46921,\"start\":46920},{\"end\":46931,\"start\":46930},{\"end\":46933,\"start\":46932},{\"end\":46948,\"start\":46947},{\"end\":47310,\"start\":47309},{\"end\":47323,\"start\":47322},{\"end\":47337,\"start\":47336},{\"end\":47648,\"start\":47647},{\"end\":47659,\"start\":47658},{\"end\":47882,\"start\":47881},{\"end\":47884,\"start\":47883},{\"end\":47895,\"start\":47894},{\"end\":47905,\"start\":47904},{\"end\":47918,\"start\":47917},{\"end\":47927,\"start\":47926},{\"end\":47936,\"start\":47935},{\"end\":48282,\"start\":48281},{\"end\":48291,\"start\":48290},{\"end\":48298,\"start\":48297},{\"end\":48300,\"start\":48299},{\"end\":48606,\"start\":48605},{\"end\":48617,\"start\":48616},{\"end\":48630,\"start\":48629},{\"end\":48895,\"start\":48894},{\"end\":48902,\"start\":48901},{\"end\":48909,\"start\":48908},{\"end\":48924,\"start\":48923},{\"end\":48926,\"start\":48925},{\"end\":48932,\"start\":48931},{\"end\":49441,\"start\":49440},{\"end\":49450,\"start\":49449},{\"end\":49460,\"start\":49459},{\"end\":49618,\"start\":49617},{\"end\":49828,\"start\":49827},{\"end\":49838,\"start\":49837},{\"end\":49853,\"start\":49852},{\"end\":50068,\"start\":50067},{\"end\":50278,\"start\":50277},{\"end\":50291,\"start\":50290},{\"end\":50302,\"start\":50301},{\"end\":50310,\"start\":50309},{\"end\":50736,\"start\":50735},{\"end\":50749,\"start\":50748},{\"end\":50764,\"start\":50763},{\"end\":50772,\"start\":50771},{\"end\":51256,\"start\":51255},{\"end\":51266,\"start\":51265},{\"end\":51280,\"start\":51279},{\"end\":51590,\"start\":51589},{\"end\":51600,\"start\":51599},{\"end\":52133,\"start\":52132},{\"end\":52135,\"start\":52134},{\"end\":52146,\"start\":52145},{\"end\":52155,\"start\":52154},{\"end\":52782,\"start\":52781},{\"end\":52784,\"start\":52783},{\"end\":52795,\"start\":52794},{\"end\":52797,\"start\":52796},{\"end\":52804,\"start\":52803},{\"end\":52806,\"start\":52805},{\"end\":53107,\"start\":53106},{\"end\":53115,\"start\":53114},{\"end\":53126,\"start\":53125},{\"end\":53139,\"start\":53138},{\"end\":53444,\"start\":53443},{\"end\":53446,\"start\":53445},{\"end\":53455,\"start\":53454},{\"end\":53729,\"start\":53728},{\"end\":53731,\"start\":53730},{\"end\":53743,\"start\":53742},{\"end\":53753,\"start\":53752},{\"end\":53755,\"start\":53754},{\"end\":53923,\"start\":53922},{\"end\":54062,\"start\":54061},{\"end\":54071,\"start\":54070},{\"end\":54081,\"start\":54080},{\"end\":54540,\"start\":54539},{\"end\":54551,\"start\":54550},{\"end\":54553,\"start\":54552},{\"end\":54757,\"start\":54756},{\"end\":54954,\"start\":54953},{\"end\":54966,\"start\":54965},{\"end\":54968,\"start\":54967},{\"end\":54979,\"start\":54978},{\"end\":54981,\"start\":54980},{\"end\":54995,\"start\":54994},{\"end\":54997,\"start\":54996},{\"end\":55171,\"start\":55170},{\"end\":55183,\"start\":55182},{\"end\":55344,\"start\":55343},{\"end\":55350,\"start\":55349},{\"end\":55357,\"start\":55356},{\"end\":55837,\"start\":55836},{\"end\":55839,\"start\":55838},{\"end\":55847,\"start\":55846},{\"end\":55858,\"start\":55857},{\"end\":55860,\"start\":55859},{\"end\":56220,\"start\":56219},{\"end\":56222,\"start\":56221},{\"end\":56234,\"start\":56233},{\"end\":56241,\"start\":56240},{\"end\":56243,\"start\":56242},{\"end\":56545,\"start\":56544},{\"end\":56552,\"start\":56551},{\"end\":56560,\"start\":56559},{\"end\":56837,\"start\":56836},{\"end\":56845,\"start\":56844},{\"end\":56852,\"start\":56851},{\"end\":57088,\"start\":57087},{\"end\":57095,\"start\":57094},{\"end\":57103,\"start\":57102},{\"end\":57734,\"start\":57733},{\"end\":57745,\"start\":57744},{\"end\":57759,\"start\":57758},{\"end\":57769,\"start\":57768},{\"end\":57779,\"start\":57778},{\"end\":57781,\"start\":57780},{\"end\":58094,\"start\":58093},{\"end\":58103,\"start\":58102},{\"end\":58109,\"start\":58108},{\"end\":58382,\"start\":58381},{\"end\":58389,\"start\":58388},{\"end\":58396,\"start\":58395},{\"end\":58398,\"start\":58397},{\"end\":58406,\"start\":58405},{\"end\":58414,\"start\":58413},{\"end\":58738,\"start\":58737},{\"end\":58747,\"start\":58744},{\"end\":58755,\"start\":58754},{\"end\":58762,\"start\":58761},{\"end\":58769,\"start\":58768},{\"end\":58777,\"start\":58776}]", "bib_author_last_name": "[{\"end\":42849,\"start\":42845},{\"end\":42863,\"start\":42855},{\"end\":42876,\"start\":42869},{\"end\":43115,\"start\":43108},{\"end\":43365,\"start\":43362},{\"end\":43377,\"start\":43371},{\"end\":43390,\"start\":43383},{\"end\":43903,\"start\":43887},{\"end\":43916,\"start\":43909},{\"end\":43928,\"start\":43922},{\"end\":43942,\"start\":43934},{\"end\":44215,\"start\":44205},{\"end\":44228,\"start\":44219},{\"end\":44241,\"start\":44232},{\"end\":44251,\"start\":44245},{\"end\":44264,\"start\":44255},{\"end\":44665,\"start\":44657},{\"end\":44676,\"start\":44669},{\"end\":44688,\"start\":44680},{\"end\":44696,\"start\":44692},{\"end\":44706,\"start\":44702},{\"end\":44971,\"start\":44965},{\"end\":44981,\"start\":44975},{\"end\":44991,\"start\":44985},{\"end\":45541,\"start\":45534},{\"end\":45551,\"start\":45545},{\"end\":45770,\"start\":45761},{\"end\":45785,\"start\":45776},{\"end\":45796,\"start\":45789},{\"end\":46112,\"start\":46108},{\"end\":46125,\"start\":46118},{\"end\":46136,\"start\":46131},{\"end\":46408,\"start\":46405},{\"end\":46415,\"start\":46412},{\"end\":46423,\"start\":46419},{\"end\":46614,\"start\":46608},{\"end\":46622,\"start\":46618},{\"end\":46630,\"start\":46626},{\"end\":46639,\"start\":46634},{\"end\":46651,\"start\":46645},{\"end\":46928,\"start\":46922},{\"end\":46945,\"start\":46934},{\"end\":46955,\"start\":46949},{\"end\":47320,\"start\":47311},{\"end\":47334,\"start\":47324},{\"end\":47344,\"start\":47338},{\"end\":47656,\"start\":47649},{\"end\":47665,\"start\":47660},{\"end\":47892,\"start\":47885},{\"end\":47902,\"start\":47896},{\"end\":47915,\"start\":47906},{\"end\":47924,\"start\":47919},{\"end\":47933,\"start\":47928},{\"end\":47943,\"start\":47937},{\"end\":48288,\"start\":48283},{\"end\":48295,\"start\":48292},{\"end\":48308,\"start\":48301},{\"end\":48614,\"start\":48607},{\"end\":48627,\"start\":48618},{\"end\":48636,\"start\":48631},{\"end\":48899,\"start\":48896},{\"end\":48906,\"start\":48903},{\"end\":48921,\"start\":48910},{\"end\":48929,\"start\":48927},{\"end\":48937,\"start\":48933},{\"end\":49447,\"start\":49442},{\"end\":49457,\"start\":49451},{\"end\":49467,\"start\":49461},{\"end\":49626,\"start\":49619},{\"end\":49835,\"start\":49829},{\"end\":49850,\"start\":49839},{\"end\":49859,\"start\":49854},{\"end\":50074,\"start\":50069},{\"end\":50288,\"start\":50279},{\"end\":50299,\"start\":50292},{\"end\":50307,\"start\":50303},{\"end\":50317,\"start\":50311},{\"end\":50746,\"start\":50737},{\"end\":50761,\"start\":50750},{\"end\":50769,\"start\":50765},{\"end\":50783,\"start\":50773},{\"end\":51263,\"start\":51257},{\"end\":51277,\"start\":51267},{\"end\":51288,\"start\":51281},{\"end\":51597,\"start\":51591},{\"end\":51607,\"start\":51601},{\"end\":52143,\"start\":52136},{\"end\":52152,\"start\":52147},{\"end\":52164,\"start\":52156},{\"end\":52792,\"start\":52785},{\"end\":52801,\"start\":52798},{\"end\":52811,\"start\":52807},{\"end\":53112,\"start\":53108},{\"end\":53123,\"start\":53116},{\"end\":53136,\"start\":53127},{\"end\":53150,\"start\":53140},{\"end\":53452,\"start\":53447},{\"end\":53466,\"start\":53456},{\"end\":53740,\"start\":53732},{\"end\":53750,\"start\":53744},{\"end\":53760,\"start\":53756},{\"end\":53931,\"start\":53924},{\"end\":54068,\"start\":54063},{\"end\":54078,\"start\":54072},{\"end\":54086,\"start\":54082},{\"end\":54548,\"start\":54541},{\"end\":54561,\"start\":54554},{\"end\":54768,\"start\":54758},{\"end\":54963,\"start\":54955},{\"end\":54976,\"start\":54969},{\"end\":54992,\"start\":54982},{\"end\":55003,\"start\":54998},{\"end\":55180,\"start\":55172},{\"end\":55191,\"start\":55184},{\"end\":55347,\"start\":55345},{\"end\":55354,\"start\":55351},{\"end\":55361,\"start\":55358},{\"end\":55575,\"start\":55564},{\"end\":55844,\"start\":55840},{\"end\":55855,\"start\":55848},{\"end\":55872,\"start\":55861},{\"end\":56231,\"start\":56223},{\"end\":56238,\"start\":56235},{\"end\":56250,\"start\":56244},{\"end\":56549,\"start\":56546},{\"end\":56557,\"start\":56553},{\"end\":56564,\"start\":56561},{\"end\":56842,\"start\":56838},{\"end\":56849,\"start\":56846},{\"end\":56856,\"start\":56853},{\"end\":57092,\"start\":57089},{\"end\":57100,\"start\":57096},{\"end\":57106,\"start\":57104},{\"end\":57742,\"start\":57735},{\"end\":57756,\"start\":57746},{\"end\":57766,\"start\":57760},{\"end\":57776,\"start\":57770},{\"end\":57790,\"start\":57782},{\"end\":58100,\"start\":58095},{\"end\":58106,\"start\":58104},{\"end\":58114,\"start\":58110},{\"end\":58386,\"start\":58383},{\"end\":58393,\"start\":58390},{\"end\":58403,\"start\":58399},{\"end\":58411,\"start\":58407},{\"end\":58419,\"start\":58415},{\"end\":58742,\"start\":58739},{\"end\":58752,\"start\":58748},{\"end\":58759,\"start\":58756},{\"end\":58766,\"start\":58763},{\"end\":58774,\"start\":58770},{\"end\":58782,\"start\":58778}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6421289},\"end\":43026,\"start\":42777},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":42083138},\"end\":43260,\"start\":43028},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7194952},\"end\":43796,\"start\":43262},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13818965},\"end\":44174,\"start\":43798},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":117034861},\"end\":44612,\"start\":44176},{\"attributes\":{\"id\":\"b5\"},\"end\":44856,\"start\":44614},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3200288},\"end\":45488,\"start\":44858},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3194567},\"end\":45675,\"start\":45490},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":19529517},\"end\":45971,\"start\":45677},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":58644554},\"end\":46368,\"start\":45973},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2958085},\"end\":46536,\"start\":46370},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2740197},\"end\":46825,\"start\":46538},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3386122},\"end\":47150,\"start\":46827},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4893541},\"end\":47603,\"start\":47152},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9188241},\"end\":47773,\"start\":47605},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3548380},\"end\":48177,\"start\":47775},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5010592},\"end\":48506,\"start\":48179},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7313725},\"end\":48816,\"start\":48508},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":59236368},\"end\":49423,\"start\":48818},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1779661},\"end\":49558,\"start\":49425},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3958369},\"end\":49764,\"start\":49560},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2757547},\"end\":50002,\"start\":49766},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":119570145},\"end\":50208,\"start\":50004},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b23\",\"matched_paper_id\":10940950},\"end\":50699,\"start\":50210},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13713980},\"end\":51096,\"start\":50701},{\"attributes\":{\"id\":\"b25\"},\"end\":51512,\"start\":51098},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":5575601},\"end\":52060,\"start\":51514},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":13029170},\"end\":52645,\"start\":52062},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1630353},\"end\":53022,\"start\":52647},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":19900204},\"end\":53331,\"start\":53024},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":37795154},\"end\":53648,\"start\":53333},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8822160},\"end\":53918,\"start\":53650},{\"attributes\":{\"id\":\"b32\"},\"end\":54004,\"start\":53920},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6287870},\"end\":54515,\"start\":54006},{\"attributes\":{\"id\":\"b34\"},\"end\":54704,\"start\":54517},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16162039},\"end\":54926,\"start\":54706},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2003897},\"end\":55129,\"start\":54928},{\"attributes\":{\"id\":\"b37\"},\"end\":55272,\"start\":55131},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":53093384},\"end\":55498,\"start\":55274},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":25563527},\"end\":55720,\"start\":55500},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13793043},\"end\":56089,\"start\":55722},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":892100},\"end\":56461,\"start\":56091},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":3395350},\"end\":56740,\"start\":56463},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":207158330},\"end\":57025,\"start\":56742},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":85517610},\"end\":57615,\"start\":57027},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":17804904},\"end\":58027,\"start\":57617},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":126030015},\"end\":58265,\"start\":58029},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":56176661},\"end\":58636,\"start\":58267},{\"attributes\":{\"doi\":\"10.1093/biostatistics/kxz032\",\"id\":\"b48\",\"matched_paper_id\":201103824},\"end\":59239,\"start\":58638}]", "bib_title": "[{\"end\":42841,\"start\":42777},{\"end\":43104,\"start\":43028},{\"end\":43356,\"start\":43262},{\"end\":43881,\"start\":43798},{\"end\":44201,\"start\":44176},{\"end\":44961,\"start\":44858},{\"end\":45528,\"start\":45490},{\"end\":45757,\"start\":45677},{\"end\":46104,\"start\":45973},{\"end\":46401,\"start\":46370},{\"end\":46604,\"start\":46538},{\"end\":46918,\"start\":46827},{\"end\":47307,\"start\":47152},{\"end\":47645,\"start\":47605},{\"end\":47879,\"start\":47775},{\"end\":48279,\"start\":48179},{\"end\":48603,\"start\":48508},{\"end\":48892,\"start\":48818},{\"end\":49438,\"start\":49425},{\"end\":49615,\"start\":49560},{\"end\":49825,\"start\":49766},{\"end\":50065,\"start\":50004},{\"end\":50275,\"start\":50210},{\"end\":50733,\"start\":50701},{\"end\":51587,\"start\":51514},{\"end\":52130,\"start\":52062},{\"end\":52779,\"start\":52647},{\"end\":53104,\"start\":53024},{\"end\":53441,\"start\":53333},{\"end\":53726,\"start\":53650},{\"end\":54059,\"start\":54006},{\"end\":54754,\"start\":54706},{\"end\":54951,\"start\":54928},{\"end\":55341,\"start\":55274},{\"end\":55562,\"start\":55500},{\"end\":55834,\"start\":55722},{\"end\":56217,\"start\":56091},{\"end\":56542,\"start\":56463},{\"end\":56834,\"start\":56742},{\"end\":57085,\"start\":57027},{\"end\":57731,\"start\":57617},{\"end\":58091,\"start\":58029},{\"end\":58379,\"start\":58267},{\"end\":58735,\"start\":58638}]", "bib_author": "[{\"end\":42851,\"start\":42843},{\"end\":42865,\"start\":42851},{\"end\":42878,\"start\":42865},{\"end\":43117,\"start\":43106},{\"end\":43367,\"start\":43358},{\"end\":43379,\"start\":43367},{\"end\":43392,\"start\":43379},{\"end\":43905,\"start\":43883},{\"end\":43918,\"start\":43905},{\"end\":43930,\"start\":43918},{\"end\":43944,\"start\":43930},{\"end\":44217,\"start\":44203},{\"end\":44230,\"start\":44217},{\"end\":44243,\"start\":44230},{\"end\":44253,\"start\":44243},{\"end\":44266,\"start\":44253},{\"end\":44667,\"start\":44655},{\"end\":44678,\"start\":44667},{\"end\":44690,\"start\":44678},{\"end\":44698,\"start\":44690},{\"end\":44708,\"start\":44698},{\"end\":44973,\"start\":44963},{\"end\":44983,\"start\":44973},{\"end\":44993,\"start\":44983},{\"end\":45543,\"start\":45530},{\"end\":45553,\"start\":45543},{\"end\":45772,\"start\":45759},{\"end\":45787,\"start\":45772},{\"end\":45798,\"start\":45787},{\"end\":46114,\"start\":46106},{\"end\":46127,\"start\":46114},{\"end\":46138,\"start\":46127},{\"end\":46410,\"start\":46403},{\"end\":46417,\"start\":46410},{\"end\":46425,\"start\":46417},{\"end\":46616,\"start\":46606},{\"end\":46624,\"start\":46616},{\"end\":46632,\"start\":46624},{\"end\":46641,\"start\":46632},{\"end\":46653,\"start\":46641},{\"end\":46930,\"start\":46920},{\"end\":46947,\"start\":46930},{\"end\":46957,\"start\":46947},{\"end\":47322,\"start\":47309},{\"end\":47336,\"start\":47322},{\"end\":47346,\"start\":47336},{\"end\":47658,\"start\":47647},{\"end\":47667,\"start\":47658},{\"end\":47894,\"start\":47881},{\"end\":47904,\"start\":47894},{\"end\":47917,\"start\":47904},{\"end\":47926,\"start\":47917},{\"end\":47935,\"start\":47926},{\"end\":47945,\"start\":47935},{\"end\":48290,\"start\":48281},{\"end\":48297,\"start\":48290},{\"end\":48310,\"start\":48297},{\"end\":48616,\"start\":48605},{\"end\":48629,\"start\":48616},{\"end\":48638,\"start\":48629},{\"end\":48901,\"start\":48894},{\"end\":48908,\"start\":48901},{\"end\":48923,\"start\":48908},{\"end\":48931,\"start\":48923},{\"end\":48939,\"start\":48931},{\"end\":49449,\"start\":49440},{\"end\":49459,\"start\":49449},{\"end\":49469,\"start\":49459},{\"end\":49628,\"start\":49617},{\"end\":49837,\"start\":49827},{\"end\":49852,\"start\":49837},{\"end\":49861,\"start\":49852},{\"end\":50076,\"start\":50067},{\"end\":50290,\"start\":50277},{\"end\":50301,\"start\":50290},{\"end\":50309,\"start\":50301},{\"end\":50319,\"start\":50309},{\"end\":50748,\"start\":50735},{\"end\":50763,\"start\":50748},{\"end\":50771,\"start\":50763},{\"end\":50785,\"start\":50771},{\"end\":51265,\"start\":51255},{\"end\":51279,\"start\":51265},{\"end\":51290,\"start\":51279},{\"end\":51599,\"start\":51589},{\"end\":51609,\"start\":51599},{\"end\":52145,\"start\":52132},{\"end\":52154,\"start\":52145},{\"end\":52166,\"start\":52154},{\"end\":52794,\"start\":52781},{\"end\":52803,\"start\":52794},{\"end\":52813,\"start\":52803},{\"end\":53114,\"start\":53106},{\"end\":53125,\"start\":53114},{\"end\":53138,\"start\":53125},{\"end\":53152,\"start\":53138},{\"end\":53454,\"start\":53443},{\"end\":53468,\"start\":53454},{\"end\":53742,\"start\":53728},{\"end\":53752,\"start\":53742},{\"end\":53762,\"start\":53752},{\"end\":53933,\"start\":53922},{\"end\":54070,\"start\":54061},{\"end\":54080,\"start\":54070},{\"end\":54088,\"start\":54080},{\"end\":54550,\"start\":54539},{\"end\":54563,\"start\":54550},{\"end\":54770,\"start\":54756},{\"end\":54965,\"start\":54953},{\"end\":54978,\"start\":54965},{\"end\":54994,\"start\":54978},{\"end\":55005,\"start\":54994},{\"end\":55182,\"start\":55170},{\"end\":55193,\"start\":55182},{\"end\":55349,\"start\":55343},{\"end\":55356,\"start\":55349},{\"end\":55363,\"start\":55356},{\"end\":55577,\"start\":55564},{\"end\":55846,\"start\":55836},{\"end\":55857,\"start\":55846},{\"end\":55874,\"start\":55857},{\"end\":56233,\"start\":56219},{\"end\":56240,\"start\":56233},{\"end\":56252,\"start\":56240},{\"end\":56551,\"start\":56544},{\"end\":56559,\"start\":56551},{\"end\":56566,\"start\":56559},{\"end\":56844,\"start\":56836},{\"end\":56851,\"start\":56844},{\"end\":56858,\"start\":56851},{\"end\":57094,\"start\":57087},{\"end\":57102,\"start\":57094},{\"end\":57108,\"start\":57102},{\"end\":57744,\"start\":57733},{\"end\":57758,\"start\":57744},{\"end\":57768,\"start\":57758},{\"end\":57778,\"start\":57768},{\"end\":57792,\"start\":57778},{\"end\":58102,\"start\":58093},{\"end\":58108,\"start\":58102},{\"end\":58116,\"start\":58108},{\"end\":58388,\"start\":58381},{\"end\":58395,\"start\":58388},{\"end\":58405,\"start\":58395},{\"end\":58413,\"start\":58405},{\"end\":58421,\"start\":58413},{\"end\":58744,\"start\":58737},{\"end\":58754,\"start\":58744},{\"end\":58761,\"start\":58754},{\"end\":58768,\"start\":58761},{\"end\":58776,\"start\":58768},{\"end\":58784,\"start\":58776}]", "bib_venue": "[{\"end\":42890,\"start\":42878},{\"end\":43131,\"start\":43117},{\"end\":43452,\"start\":43392},{\"end\":43973,\"start\":43944},{\"end\":44309,\"start\":44266},{\"end\":44653,\"start\":44614},{\"end\":45066,\"start\":44993},{\"end\":45569,\"start\":45553},{\"end\":45807,\"start\":45798},{\"end\":46154,\"start\":46138},{\"end\":46440,\"start\":46425},{\"end\":46668,\"start\":46653},{\"end\":46974,\"start\":46957},{\"end\":47359,\"start\":47346},{\"end\":47677,\"start\":47667},{\"end\":47965,\"start\":47945},{\"end\":48326,\"start\":48310},{\"end\":48645,\"start\":48638},{\"end\":49033,\"start\":48939},{\"end\":49475,\"start\":49469},{\"end\":49650,\"start\":49628},{\"end\":49872,\"start\":49861},{\"end\":50091,\"start\":50076},{\"end\":50386,\"start\":50323},{\"end\":50859,\"start\":50785},{\"end\":51253,\"start\":51098},{\"end\":51699,\"start\":51609},{\"end\":52264,\"start\":52166},{\"end\":52823,\"start\":52813},{\"end\":53162,\"start\":53152},{\"end\":53477,\"start\":53468},{\"end\":53772,\"start\":53762},{\"end\":54175,\"start\":54088},{\"end\":54537,\"start\":54517},{\"end\":54803,\"start\":54770},{\"end\":55018,\"start\":55005},{\"end\":55168,\"start\":55131},{\"end\":55373,\"start\":55363},{\"end\":55597,\"start\":55577},{\"end\":55887,\"start\":55874},{\"end\":56264,\"start\":56252},{\"end\":56585,\"start\":56566},{\"end\":56866,\"start\":56858},{\"end\":57204,\"start\":57108},{\"end\":57808,\"start\":57792},{\"end\":58131,\"start\":58116},{\"end\":58439,\"start\":58421},{\"end\":58834,\"start\":58812},{\"end\":43506,\"start\":43454},{\"end\":44345,\"start\":44331},{\"end\":45138,\"start\":45068},{\"end\":49127,\"start\":49035},{\"end\":50448,\"start\":50388},{\"end\":50886,\"start\":50861},{\"end\":51791,\"start\":51701},{\"end\":52362,\"start\":52266},{\"end\":54265,\"start\":54177},{\"end\":57296,\"start\":57206}]"}}}, "year": 2023, "month": 12, "day": 17}