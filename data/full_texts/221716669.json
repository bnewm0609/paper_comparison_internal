{"id": 221716669, "updated": "2023-10-06 11:43:14.582", "metadata": {"title": "Fair Meta-Learning For Few-Shot Classification", "authors": "[{\"first\":\"Chen\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Changbin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jincheng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Feng\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "2020 IEEE International Conference on Knowledge Graph (ICKG)", "journal": "2020 IEEE International Conference on Knowledge Graph (ICKG)", "publication_date": {"year": 2020, "month": 9, "day": 23}, "abstract": "Artificial intelligence nowadays plays an increasingly prominent role in our life since decisions that were once made by humans are now delegated to automated systems. A machine learning algorithm trained based on biased data, however, tends to make unfair predictions. Developing classification algorithms that are fair with respect to protected attributes of the data thus becomes an important problem. Motivated by concerns surrounding the fairness effects of sharing and few-shot machine learning tools, such as the Model Agnostic Meta-Learning framework, we propose a novel fair fast-adapted few-shot meta-learning approach that efficiently mitigates biases during meta-train by ensuring controlling the decision boundary covariance that between the protected variable and the signed distance from the feature vectors to the decision boundary. Through extensive experiments on two real-world image benchmarks over three state-of-the-art meta-learning algorithms, we empirically demonstrate that our proposed approach efficiently mitigates biases on model output and generalizes both accuracy and fairness to unseen tasks with a limited amount of training samples.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2009.13516", "mag": "3086867014", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icbk/ZhaoLLC20", "doi": "10.1109/icbk50248.2020.00047"}}, "content": {"source": {"pdf_hash": "ce353fd00d3428ffda16ae0fafa551b8fcf4fa8d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2009.13516v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2009.13516", "status": "GREEN"}}, "grobid": {"id": "c792a827874d8f09f3ed871fed50945ccf82c6d9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ce353fd00d3428ffda16ae0fafa551b8fcf4fa8d.txt", "contents": "\nFair Meta-Learning For Few-Shot Classification\n\n\nChen Zhao chen.zhao@utdallas.edu \nDepartment of Computer Science\nThe University of Texas at Dallas Richardson Texas\nUSA\n\nChangbin Li changbin.li@utdallas.edu \nDepartment of Computer Science\nThe University of Texas at Dallas Richardson Texas\nUSA\n\nJincheng Li jincheng.li@utdallas.edu \nDepartment of Computer Science\nThe University of Texas at Dallas Richardson Texas\nUSA\n\nFeng Chen feng.chen@utdallas.edu \nDepartment of Computer Science\nThe University of Texas at Dallas Richardson Texas\nUSA\n\nFair Meta-Learning For Few-Shot Classification\nIndex Terms-decision boundary covariancestatistical parityfew-shotmeta-learning\nArtificial intelligence nowadays plays an increasingly prominent role in our life since decisions that were once made by humans are now delegated to automated systems. A machine learning algorithm trained based on biased data, however, tends to make unfair predictions. Developing classification algorithms that are fair with respect to protected attributes of the data thus becomes an important problem. Motivated by concerns surrounding the fairness effects of sharing and few-shot machine learning tools, such as the Model Agnostic Meta-Learning [1] framework, we propose a novel fair fast-adapted few-shot metalearning approach that efficiently mitigates biases during metatrain by ensuring controlling the decision boundary covariance that between the protected variable and the signed distance from the feature vectors to the decision boundary. Through extensive experiments on two real-world image benchmarks over three state-of-the-art meta-learning algorithms, we empirically demonstrate that our proposed approach efficiently mitigates biases on model output and generalizes both accuracy and fairness to unseen tasks with a limited amount of training samples.\n\nI. INTRODUCTION\n\nIn data mining and machine learning, the information system is becoming increasingly reliant on statistical inference and learning to give automated prediction and decision-making to solve regression and classification problems. Biased historical data or data containing biases, however, are often learned and thus lead to results with undesirability, inaccuracy, and even illegality. In recent years, there are increasing numbers of news reported that human bias is revealed in an artificial intelligence system applied by high-tech companies. [2] reported that a picture of two African Americans was automatically tagged as \"Gorillas\" by Google Photos. A 2016 study [3] found that the data-driven system developed by Amazon that used to determine the neighborhoods in which to offer free same-day delivery is highly biased and unfair to African American communities due to the stark disparities in the demographic makeup of neighborhoods: white residents were more than twice as likely as African American residents to live in one of the qualifying neighborhoods. Critics have voiced that human bias potentially has an influence on nowadays technology, which leads to outcomes with unfairness. Another example for biased image classification problem is shown in Figure 1: a dog classifier is trained with images of dogs lying on the grass. The training process goes through a feature A dog classifier is trained using images of dogs lying on the grass. Features learned from the neural network, however, are not fully concentrated on the target objects (i.e. dogs). As a consequence, the biased learner leads to unfair or uncertain decision-makings: (a) a dog on the grass; (b) a dog running on the beach; (c) a dog on the stone step; (d) a goat on the grass.\n\nextractor, but the captured features used for classifier training are not totally concentrated on target objects (i.e. dogs). As a consequence, the decision-making accuracy for testing images does not turn out well. To investigate the reason, we deduce that there is a non-negligible relationship between the predicted outcome and the protected feature (i.e. grass in this example), which leads to an unfair result.\n\nTo ameliorate this unfairness problem, one may attempt to make the automated decision-maker blind to the protected attributes [4]. This however, is difficult, as many attributes may be correlated with the protected one [5]. With biased input, the main goal of training an unbiased model is to make the output fair. In other words, the predicted outcomes are statistically independent on protected variables. Statistical parity, also known as group fairness, ensures that the overall proportion of members in a protected group receiving predictions (i.e. positive/negative classification) are identical to the proportion of the population as a whole.\n\nTo the best of our knowledge, unfortunately, the majority of existing fairness-aware machine learning algorithms are under the assumption of giving abundant training examples. Learning quickly, however, is another significant hallmark of Fig. 2: An overview of our proposed bias controlling approach in few-shot meta-learning. (Left) The meta-learning framework include two processes: meta-train and meta-test, where each includes multiple tasks. (Right) Taking 5-way-5-shot classification problem as an example, \u03b3(\u03c6) refers to a discrimination measure function, and c is a predefined small threshold to account for a degree of randomness in the decision making process and sampling. The constraint here is considered to ensure no discrimination on the prediction model for each task.\n\nhuman intelligence. In meta-learning, also known as learning to learn, the goal of trained model is to quickly learn a new task from a small amount of new data (i.e. few-shot), and the model is trained by the meta-leaner to be able to learn on a large number of different tasks [1]. In contrast to traditional machine learning algorithms, such as multi-task learning [6]- [9] and transfer learning [10]- [13], meta-learning framework has advantages: (1) it learns across tasks where each task takes one or few samples as input; (2) it therefore efficiently speeds up model adaptation (3) and generalizes accuracy to unseen tasks.\n\nThe overall idea of existing methods of meta-learning, however, is to train a model which is capability of generalizing accuracy, rather than fairness, to unseen data tasks. But techniques for unfairness prevention and bias control in the fewshot meta-learning study are challenging and rarely touched. To ensure prediction without biases, the main contribution to this paper is that we feed each support set of a task with unified group fairness constraints and minimize meta-loss overall episodes. Specifically, we mitigate biases in each episode during meta-training by controlling the decision boundary covariance [14] which is defined as the covariance between the protected attribute and the signed distance from the feature vectors to the decision boundary. A value of zero signifying no dependency or attribute effect. Our experimental results demonstrate our approach is capability of controlling bias and decreasing loss as well as generalizing both to unseen tasks. In the context of classification, for example, as shown in Figure  2, each support set of a task used for training contains images sampled from 5 different classes (N = 5 ways) and each class includes 5 images (K = 5 shots). By giving an unified meta-initialization for each task, a task specific local model parameter is learned through one or few steps gradient update of the loss function that is constrained by fairness condition. To update the meta-parameter, the generalization error, i.e. the summation of the query loss across all tasks, is minimized. In summary, the main contributions of this paper are listed:\n\n\u2022 For the first time the issue of bias control in meta-learning multi-class classification problem is applied to image data sets. We mitigate biases by controlling the decision boundary covariance. \u2022 We develop a novel algorithm to solve this constrained classification problem under the Model Agnostic Meta-Learning (MAML) few-shot framework. \u2022 We validate the performance of our proposed approach of controlling biases on three state-of-the-art meta-learning techniques through extensive experiments based on realworld data sets. Our results demonstrate the proposed approach is capability of mitigating biases and generalizing both accuracy and fairness to unseen tasks, with the input training data is minimal. In Section 2, some related works are referred. In Section 3, we see how unfairness is important in a machine learning model by introducing a simple causal based knowledge graph and how a statistical parity constraint, i.e. decision boundary covariance, is able to be used for bias-control in a single task. In Section 4, the fair few-shot meta-learning problem is formulated and how to solve it by applying the Model-Agnostics Meta-Learning framework is presented in detail. In Section 5, to validate the proposed approach, we conduct experiments by using two real-world benchmarks and three cutting-edge techniques, and we conclude this paper in Section 6.\n\n\nII. RELATED WORK\n\nIn recent years, researches involving processing biased data became increasingly significant. Fairness-aware in data mining is classified into unfairness discovery and prevention. Based on the taxonomy by tasks, it can be further categorized to classification [14]- [22], regression [9], [23]- [26], clustering [27], [28], recommendation [29]- [31] and dimension reduction [32].\n\n\nA. Unfairness Prevention in Classification\n\nMajority of works in unfairness prevention is concentrated on data classification. According to approaches studied in fairness, bias-prevention in classification is subcategorized into pre-processing [15], in-processing [14], [16] and postprocessing [17]- [19]. Recent works [20] and [21] developed new approaches resulting in increasing the binary classification accuracy through reduction of fair classification to a sequence of cost-sensitive problems and applications of multi-task techniques with convex fairness constraints, respectively.\n\nNon-discrimination (unfairness-free) can be defined as follows: (1) people that are similar in terms of non-sensitive characteristics should receive similar predictions, and (2) differences in predictions across groups of people can only be as large as justified by non-sensitive characteristics [33]. The first condition is related to direct discrimination. The second condition ensures that there is no indirect discrimination, also referred to as redlining. These types of discrimination (direct and indirect) are supported by two legal frameworks applied in large bodies of cases, disparate treatment and disparate impact [34]. The disparate treatment framework enforces procedural fairness, namely, the equality of treatments that prohibits the use of the protected attribute in the decision process. The disparate impact framework guarantees outcome fairness, namely, the equality of outcomes among protected groups [35]. Many of the prior studies, however, suffer from one or more of the following limitations: (i) they are restricted to a narrow range of classifiers, (ii) they only accommodate a single, binary sensitive attribute, and (iii) they cannot eliminate disparate treatment and disparate impact simultaneously. To overcome such limitations, in this paper, we consider the measure of decision boundary fairness [14], which enables us to ensure fairness with respect to one or more sensitive attributes, in terms of both disparate treatment and disparate impact.\n\n\nB. Few-shot Meta-learning\n\nTo the best of our knowledge, the majority of existing fairness-aware machine learning algorithms are under the assumption of giving abundant training examples. Learning quickly, however, is another significant hallmark of human intelligence. Much efforts have been devoted to overcome the data efficiency issue. One popular category of few-shot learning techniques is distance metric learning based method, which addresses the few-shot classification problem by \"learning to compare. The intuition is that if a model can determine the similarity of two images, it can classify an unseen input image with the labeled instances. [36] introduced Matching Networks which employed ideas from k-nearest neighbors algorithm and metric learning based on a bidirectional Long-Short Term Memory (LSTM) to encode in the context of the support set. Prototypical networks [37] learn a metric space in which classification is able to be performed by computing Euclidean distances to prototype representations of each class. In addition, gradient descent based algorithms, such as [1], [38]- [42], aim to learn good model initialization so that the meta-loss is minimum.\n\nThe overall idea of these state-of-the-art is to train a metalearning model which is capability of generalizing accuracy, Fig. 3: A simple diagram demonstrates the causal relationship in fairness learning. X, S, and Y represent input feature, the protected attribute and target outcome, respectively. but less attention on fairness generalization to unseen data tasks. In this paper, our proposed approach makes up for this regret of unfairness prevention using few-shot meta-learning techniques in multi-class classification problems.\n\n\nIII. MODELING OF FAIRNESS BASED ON CAUSAL KNOWLEDGE GRAPH\n\nIn this section, we first present how unfairness/bias affects decision-making by introducing a simple causal knowledge graph and then explain the mechanism of mitigating bias in a single task using the decision boundary covariance.\n\n\nA. Causation in Fairness Learning\n\nTo understand how past decisions may bias a prediction model, we must first understand how the protected attribute may have affected the outcome by answering such questions: What would this outcome have been under different protected values? How would the outcome change if the protected attribute were changed, all else being equal? These questions are core to the mission of learning fair systems which aim to inform decision-making.\n\nUnfairness can be broadly partitioned into two types: direct and indirect. The directed bias is concerned with settings where individuals received less favorable treatments on the basis of the protected attribute. The indirect one is concerned with individuals who receive treatments on the basis of inadequately justified factors that are somewhat related with the protected attribute [35].\n\nFor simplicity, we consider one binary protected attribute (e.g. white and black) in this work. However, our ideas can be easily extended to many protected attributes with multiple levels. Let Z = X \u00d7 S \u00d7 Y be the data space, where X \u2282 R n is an input space, S = {0, 1} is a protected space, and Y = {1, 2, ..., N } is an output space for multi-class classification where N is the number of classes. We consider a single task\ndata D = {(x i , y i , s i )} h i=1 , i = 1, ...h, where x i \u2208 R n\ndenotes the i-th observation, y i denotes the corresponding output, s i \u2208 {s + , s \u2212 } represents the binary protected attribute, and h is the number of observations in each task. A practical definition of fair causality is: Definition 1 (Fair Causality and Causal Effect). X causes Y if and only if changing X leads to a change in Y , while keeping everything else (i.e. S) constant. Causal effect is defined as the magnitude by which Y is changed by a unit change in X.\n\nTherefore, a fair prediction, shown in Figure 3, indicates there is no either direct (S \u2192 Y ) or indirect (S \u2192 X \u2192 Y ) dependency effect of outcome on the protected attribute. These types of discrimination (direct and indirect) are supported by two legal frameworks applied in large bodies of cases throughout the disparate treatment and disparate impact [34]. The disparate treatment framework enforces procedural fairness, namely, the equality of treatments that prohibits the use of the protected attribute in the decision process. The disparate impact framework guarantees outcome fairness, namely, the equality of outcomes among protected groups [35].\n\nTo comply with disparate treatment criterion we specify that sensitive attributes are not used in decision making,\ni.e. {x i } h i=1 and {s i } h i=1\nconsist of disjoint feature sets. As discussed in [14], our definition of disparate impact leverages the 80%-rule [43]. A decision boundary satisfies the 80%-rule if the ratio between the percentage of users with a particular protected attribute value having d \u03b1 (x) \u2265 0, where \u03b1 is the decision boundary parameter, and the percentage of users without that value having d \u03b1 (x) \u2265 0 is no less than 0.8 [14].\nmin P (d \u03b1 (x) \u2265 0|s = 1) P (d \u03b1 (x) \u2265 0|s = 0) , P (d \u03b1 (x) \u2265 0|s = 0) P (d \u03b1 (x) \u2265 0|s = 1) \u2265 0.8(1)\n\nB. Decision Boundary Covariance in Statistical Parity\n\nIn this section, we introduce a measure of decision boundary fairness, which enables us to ensure fairness with respect to one or more protected attributes, in terms of both disparate treatment and disparate impact. The decision boundary covariance (DBC) which measures the decision boundary (un)fairness is defined as Definition 2 (Decision Boundary Covariance [14]). The covariance between the protected variables s = {s i } h i=1 and the signed distance from the feature vectors to the decision boundary,\nd \u03b1 (x) = {d \u03b1 (x i )} h i=1 , where \u03b1 is the decision boundary parameter. DBC(s, d \u03b1 (x)) = E[(s \u2212s)d \u03b1 (x)] \u2212 E[s \u2212s]d \u03b1 (x) \u2248 1 h h i=1 (s i \u2212s)d \u03b1 (x)(2)\nwhere E[s \u2212s]d \u03b1 (x) is cancels out since E[s \u2212s] = 0. Taking linear model as an example, the decision boundary is simply the hyperplane defined by \u03b1 T x = 0. Then the DBC reduces to \nP (d \u03b1 (x) \u2265 0|s = 1) = P (d \u03b1 (x) \u2265 0|s = 0)\nthen the empirical covariance will be approximately zero for a sufficiently large training set.\n\n\nIV. FAIR META-LEARNING\n\nMeta-learning for few-shot learning aims to train a metalearner which is able to learn on a large number of various tasks from a small amount of data. MAML (Model-Agnostic Meta-Learning) proposed by [1] is one of the popular gradient based meta-learning frameworks, which leads to state-ofthe-art performance and fast adaptation to unseen tasks. To generalize fairness in a classification problem with minimal samples, we propose a novel approach by modifying MAML in which we uniformly control DBC for each task. The goal of the proposed approach is to estimate a good meta-parameter such that the summation of empirical risks for each task is minimized and meanwhile each task is fair.\n\n\nA. Settings\n\nIn this work, we consider a collection of supervised learning tasks T = {(D S j , D Q j )} T j=1 which distributions over Z and T is denoted as the number of tasks. T is often referred to as a meta-training set as well as an episode (D S j , D Q j ) explicitly contains a pair of a support D S j and a query D Q j data sets. For each task j \u2208 {1, 2, ..., T }, we let {x j,i , y j,i , s j,i } m i=1 \u2208 (X \u00d7 Y \u00d7 S) be the corresponding task data and m is the number of datapoints in the support set. For example, standard few-shot learning benchmarks evaluate model in N -way Kshot classification tasks and thus m = N \u00d7 K indicates, in the support set of the j-th task, it contains N categories and each consists of K datapoints. We emphasize that we need to sample without replacement, i.e., D S j \u2229 D Q j = \u2205.\n\nIn a general meta-learning setting, it consists of meta-train and meta-test partitions where each contains a number of minibatches of episodes (see Figure 2). We consider a distribution over tasks p(T ) that we want our model to be able to adapt to. In a N -way-K-shot learning setting, a task T j is sampled from p(T ), where the subscript j represents the j-th task of a mini-batch. In the supervised learning setting, supposing the meta-model is a parameterized function f \u03c6 with parameters \u03c6. In a general meta-learning model, the goal is to learn an optimized \u03c6 so that the summation of query losses l Tj (f \u03c6 ) over all meta-training tasks is minimum. During meta-training, \u03c6 is updated iteratively.\n\u03c6 * = arg min \u03c6 E T \u223cp(T ) l T (f \u03c6 )(3)\n\nB. Model-Agnostic Meta-Learning with convex constraints\n\nMeta-learning approaches for few-shot learning are often assumed that the support and query sets of a task are sampled from the same distribution. In our work, for each single task, the objective is to minimize the predictive error L inner (D S j , \u03c6) such that it is constrained by a function g j (\u03c6).\nmin \u03c6j L inner (D S j , \u03c6) (4) subject to g j (\u03c6) \u2264 0\nwhere L inner : R n \u2192 R is a loss function, such as crossentropy for classification, and g : R n \u2192 R is an appropriate complexity function ensuring the existence and the uniqueness of the above minimizer. A point \u03c6 in the domain of the problem is feasible if it satisfies the constraint g j (\u03c6) \u2264 0. Specifically, g j (\u03c6) is defined by the definition of decision boundary covariance in Eq.(2), i.e.\ng j (\u03c6) = 1 N \u00d7 K si,xi\u223cTj (s i \u2212s)d \u03b1 (x i ) \u2212 c (5)\nwhere c is a small positive fairness relaxation, d \u03b1 (x i ) \u2248 max{p n \u2208 [0, 1] N } and p denotes the class probabilities of x i . Here, for a N -way-K-shot classification task, we include N \u00d7 K data points in the support set D S of each task T j . To solve the optimization problem, we thus introduce an unified Lagrange multiplier \u03bb \u2265 0 for all tasks and the Lagrange function L Tj (\u03c6, \u03bb) for each task is defined by\nL Tj (\u03c6, \u03bb) = L inner (\u03c6) + \u03bb(g j (\u03c6))(6)\nTherefore the original problem can be finally seen by minimizing L Tj (\u03c6, \u03bb) for each task and thus mitigates dependency of prediction on the protected attribute. The goal of training a single task is to output a local parameter \u03c6 j given the metaparameter \u03c6 such that it minimizes the task loss L inner subject to the task constraint g j (\u03c6) \u2264 0. Next, to update the metaparameter, we minimize the generalization error L meta using query sets across every task in the batch such that the query constraints are satisfied. Formally, the learning objective across all tasks is\nmin \u03c6 L meta ( T j=1 D Q j , \u03c6) = T j=1 L inner (D Q j , \u03c6 j )(7)\nwhere \u03c6 j = arg min \u03c6j ,gj (\u03c6)\u22640 L inner is the local optimum for each task. A step-by-step learning algorithm for unfairness prevention in few-shot regression is proposed in Algorithm 1. \n\n\nAlgorithm 1 Unfairness Prevention in\nfor all T j = {D S j , D Q j } do 5: Sample N -way-K-shot datapoints from D S j = {x j , y j , s j } 6: \u03c6 j \u2190 \u03c6 7:\nfor q = 1, 2, ... do 8: Evaluate \u2207 \u03c6j L Tj (\u03c6 j , \u03bb) using D S j 9:\n\nCompute adapted local parameter \u03c6 j \u2190 \u03c6 j \u2212 \u03b1\u2207 \u03c6j L Tj (\u03c6 j , \u03bb) 10: end for 11: Sample datapoints from D Q j = {x j , y j , s j }\n\n\n12:\n\nEvaluate query loss l Tj (\u03c6 j ) and query fairness g Tj (\u03c6 j ) using D Q Update \u03c6 \u2190 \u03c6 \u2212 \u03b2\u2207 \u03c6 Tj \u223cp(T ) l Tj (\u03c6 j ) 15: Evaluate training fairness mean(g Tj (\u03c6 j )) 16: end while\n\n\nC. Algorithm Analysis\n\nSince the proposed Algorithm 1 is modified following [1], the convergence is guaranteed and detailed analysis is stated in [44]. Accessing to sufficient samples, the running time of the algorithm is O(n \u00b7 b \u00b7 q) , where n is the number of outer iterations, b is batch-size, and q is gradient steps of inner loop. For a N -way-K-shot learning, the best accuracy is achieved\nwhen ||\u2207\u03c6|| \u2264 O(\u03c3/ \u221a N K), where \u03c6 = E T \u223cp(T ) l T (f \u03c6 )\n, \u03c3 is a bound on the standard deviation of \u2207L Tj (\u03c6 j , \u03bb) from its mean \u2207L T (\u03c6, \u03bb), and\u03c3 is a bound on the standard deviation of estimating \u2207L Tj (\u03c6 j , \u03bb) using a single data point.\n\n\nV. EXPERIMENTS\n\nTo validate our approach of unfairness prevention in fewshot meta-learning models, we conduct experiments with two real-world image data sets.\n\n\nA. Data\n\nOmniglot [45] is a data set of 1623 handwritten characters collected from 50 alphabets. To avoid overfitting, data augmentation is used on images in the form of rotations of 90 degrees increments, i.e. 90, 180, and 270 degrees. Rotated class samples are considered new classes and thus we have 1623\u00d74 classes in total. We shuffle all character classes and randomly split the data set into three sets, 1150\u00d74 for the training set, 50\u00d74 for validation, and 423\u00d74 for testing. We follow the procedure of [36] by resizing the grayscale images to 28\u00d728. In order to study fairness, an arbitrary probability p(x|s = 1) is assigned to each class and thus each image sample is given a protected attribute s \u2208 {0, 1}. mini-ImageNet is originally proposed by [36]. It consists of 60,000 color images scaled down to 84\u00d784 divided into 100 classes with 600 examples each. We use the split proposed in [38], which consists of 64 classes for training, 12 classes for validation and 24 classes for testing. The protected attribute is randomly added following the same procedure stated in our Omniglot settings.\n\n\nB. Parameter Tuning\n\nIn order to provide a fair comparison for all methods, our embedding architecture mirrors that used by [36]. It consists of four modules and each comprises a 64-filter 3\u00d73 convolution, batch normalization layer [46], a ReLU nonlinearity and a 2\u00d72 max-pooling layer. For Omniglot, since each image is resized to 28 \u00d7 28, it results in a 64-dimensional output space. Due to the increased size of images in mini-ImageNet, the resulting feature map is 1600-dimensional. All models are trained with Adam optimizer.\n\nFor N -way, K-shot classification, each gradient is computed using a batch size of N \u00d7 K examples. For Omniglot, the 5way convolutional model is trained with 1 gradient step with step size of \u03b1 = 0.4, and a meta batch-size of 32 tasks. The network is evaluated using 3 gradient steps with the same step size \u03b1 = 0.4. The 20-way convolutional model is trained and evaluated with 5 gradient steps with step size of \u03b1 = 0.1. During training, the meta batch-size is set to 16 tasks. For MiniImagenet, the model is trained using 5 gradient steps of size \u03b1 = 0.01, and evaluated using 10 gradient steps at test time. Following [38], 15 examples per class are used for evaluating the post-update meta-gradient. We used a meta batch-size of 4 and 2 tasks for 1-shot and 5-shot training respectively. All model are trained for 60000 iterations.\n\nBesides, to comply with disparate treatment criterion, we specify that protected attributes are not used in the decision making, i.e. protected and explanatory attributes consist of disjoint feature sets. Key characteristics for all data set are listed in Table I.\n\n\nC. Baseline Methods\n\nWe compared our work with three well known metalearning state-of-the-arts, MAML [1], Matching Networks [36], and Prototypical Networks [37]. These methods apply the same meta-learning framework but differ in their strategies to make predictions conditioned on the support set. MAML is a gradient based meta-learning algorithm, where each support set is used to adapt a uniformed initialized model parameters using one or few gradient steps. After several updates, the meta-loss reaches the minimum along with each episode loss reaching its local minimum.\n\nFor both Matching and Prototypical Networks (i.e. Match-ingNet and ProtoNet), the prediction of the samples in a query set is based on comparing the distance between embedded query feature and support feature within each class. Match-ingNet was the first to both train and test on N -way-K-shot tasks. The appeal of this is training and evaluating on the same tasks lets us optimise for the target task in an end-toend fashion. Different from earlier approaches such as siamese networks [47], MatchingNet combines both embedding and classification to form an end-to-end differentiable nearest neighbours classifier. Specifically, it applied a bidirectional Long-Short Term Memory (LSTM) to encode in the context of the support set and compares cosine distance between the query feature and each support feature.\n\nIn Prototypical Networks, the authors apply a compelling inductive bias in the form of class prototypes to achieve impressive few-shot performance that exceeds Matching Networks without the complication of full context embeddings or FCE for short. The key assumption is that there exists an embedding in which samples from each class cluster around a single prototypical representation which is simply the mean of the individual samples. This idea streamlines K-shot classification in the case of K > 1 as classification is simply performed by taking the label of the closest class prototype.\n\nIn order to output fair predictions, fairness constraints are applied. Experimental results shown with prefix \"Fair-\" in the front indicate models adjusted using our proposed approach.  Table II showcases results experimented through three cutting-edge meta-learning methods and those associated with our proposed unfairness prevention approach (noted with \"Fair-\"), which examined with two real-world image data sets, i.e. Omniglot [45] and mini-ImageNet [36]. The problem of N -way classification is set up as follows: select N unseen classes, provide the model with K different instances of each of the N classes, and evaluate the models ability to classify new instances within the N classes.\n\nIn order to check the generalized fairness of these stateof-the-arts on unseen tasks, we produce local replications labeled with superscript \u2021 that are used to compare with the results reported in the original paper labeled with * . Note that for local replication methods, input images are slightly different, where in this paper we additionally consider the protected attribute as one of the input features. Besides, in the proposed unfairness prevention approach (labeled with the prefix \"Fair-\" in Table II), cross-entropy losses are calculated with using images without the protected attribute, as the fairness constraint is applied to control the covariance between the protected variable and the signed distance from the feature vectors to the decision boundary.\n\nBesides, our approaches (Fair-MAML, Fair-MatchingNet, and Fair-ProtoNet) are outperformed than the original methods and the gap between all methods is narrowing as the number of training data increases (i.e. from 1-shot to 5-shot). However, in Omniglot data, as more image classes are considered in the experiment, accuracy decreases. Even in the 20-way classification problem, our approach is able to control the decision boundary covariance efficiently.\n\nOur proposed approach is empirically shown to reduce DBC and thus improve outcome fairness in multi-class decisionmaking for all selected meta-learning methods. Another notable observation is that decreasing unfairness is brought at the sacrifice of classification accuracy in a bit. This comes down to the trade-off between accuracy and fairness. In summary, our approach significantly controls biases for few-shot metalearning models and generalizes to unseen tasks.\n\n\nVI. CONCLUSION AND FUTURE WORK\n\nIn this paper, for the first time we develop deep into the few-shot supervised meta-learning classification model and propose a bias-control approach by adding statistical parity constraint, namely decision boundary covariance, which significantly mitigates dependence of prediction on the protected variable in each task and generalize both accuracy and fairness to unseen tasks. Experimental results on two real-world image data sets indicate the proposed approach works for three cutting-edge few-shot meta-learning models with multi-class classification problems. Further research in this area can make multitask parameters a standard ingredient in deep fairness learning.\n\nFig. 1 :\n1An example of biased image classification problem.\n\n0 Fig. 4 :\n04s i \u2212s)\u03b1 T x.An example of fair binary classification with a linear decision boundary is given inFigure 4. Red markers represent the protected group (i.e. s = 1) and blue ones are the unprotected group (i.e. s = 0). In the left ofFigure 4, we calculate P (d \u03b1 (x) \u2265 0|s = 1) = 1/4 = 0.25, where d \u03b1 (x) \u2265 An example of fair binary classification (unfair (left) and fair (right) classifier) by controlling the decision boundary covariance between the protected variable and the signed distance from from user's feature to the decision boundary. indicates the triangle class and P (d \u03b1 (x) \u2265 0|s = 0) = 7/(16 \u2212 4) = 0.583. By applying the 80%-rule indicated in Eq.(1), the disparate impact value of the left classifier is 0.25/0.583 = 0.43, which is lower than the threshold of 0.8 and returns an unfair classification prediction. Similarly, in the right case of Figure 4, however, P (d \u03b1 (x) \u2265 0|s = 1) = P (d \u03b1 (x) \u2265 0|s = 0) = 0.5 and thus the disparate impact is 1.0. Note that, if a decision boundary satisfies the 100%-rule, i.e.\n\n\nFew-Shot Classification. Require: p(T ): distribution over tasks. Require: \u03b1, \u03b2: step size hyperparameters. Require: q: inner gradient update steps.\n\nTABLE I :\nIKey Characteristics of Experimental DataOmniglot \nmini-ImageNet \nimages \ngrey \ncolor \naugmentation \nyes \nno \nscale \n28 \u00d7 28 \n84 \u00d7 84 \nclasses for training \n1150 \u00d7 4 \n64 \nclasses for validation \n50 \u00d7 4 \n12 \nclasses for test \n423 \u00d7 4 \n24 \nN -way \n5 \n20 \n5 \nlocal gradient step(s) \nfor training \n\n1 \n5 \n5 \n\nlocal gradient step(s) \nfor evaluation \n\n3 \n5 \n10 \n\nlocal learning rate \nfor training \n\n0.4 0.1 \n0.01 \n\nlocal learning rate \nfor evaluation \n\n0.4 0.1 \n0.01 \n\nmeta batch-size \n32 \n16 \n4 (1-shot), 2 (5-shot) \n\n\n\nTABLE II :\nIIConsolidated overall result for few-shot classification accuracies and fairness. Methods with superscript * and \u2021 respectively indicates results reported by the original paper and our local replication with addition of protected attributes.98.1\u00b10.5% 0.39\u00b10.01 98.1\u00b10.1% 0.59\u00b10.09 96.2\u00b10.6% 0.50 \u00b10.10 98.7\u00b10.4% 0.59\u00b10.03 Fair-MAML 89.4\u00b10.5% 0.18\u00b10.01 92.2\u00b10.4% 0.19\u00b10.01 85.4\u00b10.3% 0.21\u00b10.03 90.4\u00b10.4% 0.27 \u00b10.01 0\u00b10.4% 0.45\u00b10.02 98.6\u00b10.3% 0.45\u00b10.01 95.2\u00b10.5% 0.47\u00b10.02 97.9\u00b10.3% 0.48\u00b10.03 Fair-MatchingNet 96.5\u00b10.5% 0.14\u00b10.01 98.0\u00b10.5% 0.15\u00b10.01 94.1\u00b10.5% 0.13\u00b10.03 98.5\u00b10.5% 0.13\u00b10.01 3\u00b12.9% 0.09\u00b10.03 64.2\u00b11.0% 0.14\u00b10.01 Fair-ProtoNet 39.9\u00b11.0% 0.05\u00b10.02 59.9\u00b12.0% 0.06\u00b10.05 D. Experimental ResultsOmniglot \n5-way \n20-way \n1-shot \n5-shot \n1-shot \n5-shot \nApproach \nAccuracy \nDBC \nAccuracy \nDBC \nAccuracy \nDBC \nAccuracy \nDBC \nMAML  *  \n98.7\u00b10.4% \n-\n99.9\u00b10.1% \n-\n95.8\u00b10.3% \n-\n98.9\u00b10.2% \n-\nMAML  \u2021 \nMatchingNet  *  \n98.1% \n-\n98.9% \n-\n93.8% \n-\n98.7% \n-\nMatchingNet  \u2021 \n98.ProtoNet  *  \n98.8% \n-\n99.7 % \n-\n96.0% \n-\n98.9% \n-\nProtoNet  \u2021 \n98.5\u00b11.8% 0.44\u00b10.01 99.1\u00b10.3% 0.44\u00b10.03 96.6\u00b11.7% 0.46\u00b10.01 98.6\u00b10.1% 0.48\u00b10.01 \nFair-ProtoNet \n86.4\u00b12.0% 0.10\u00b10.07 94.3\u00b10.5% 0.10\u00b10.02 88.7\u00b11.5% 0.03\u00b10.02 89.4\u00b10.5% 0.02\u00b10.01 \n\nmini-ImageNet 5-way \n1-shot \n5-shot \nApproach \nAccuracy \nDBC \nAccuracy \nDBC \nMAML  *  \n48.7\u00b11.8% \n-\n63.1\u00b10.9% \n-\nMAML  \u2021 \n44.2\u00b11.1% 0.91\u00b10.01 61.1\u00b10.8% 0.89\u00b10.01 \nFair-MAML \n35.5\u00b10.9% 0.61\u00b10.05 54.5\u00b10.8% 0.58\u00b10.07 \nMatchingNet  *  \n43.6\u00b10.8% \n-\n55.3\u00b10.7% \n-\nMatchingNet  \u2021 \n44.8\u00b10.1% 0.10\u00b10.01 60.9\u00b11.0% 0.13\u00b10.03 \nFair-MatchingNet \n37.1\u00b12.0% 0.06\u00b10.05 56.5\u00b12.0% 0.07\u00b10.05 \nProtoNet  *  \n49.4\u00b10.8% \n-\n68.2\u00b10.7% \n-\nProtoNet  \u2021 \n43.\nACKNOWLEDGEMENT This work is supported by the National Science Foundation (NSF) under Grant No #1815696 and #1750911.\nModel-agnostic meta-learning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, Proceedings of the 34th International Conference on Machine Learning (ICML). the 34th International Conference on Machine Learning (ICML)C. Finn, P. Abbeel, and S. Levine, \"Model-agnostic meta-learning for fast adaptation of deep networks,\" Proceedings of the 34th International Conference on Machine Learning (ICML), 2017.\n\nGoogle mistakenly tags black people as gorillas, showing limits of algorithms. A Barr, The Wall Street Journal. A. Barr, \"Google mistakenly tags black people as gorillas, showing limits of algorithms,\" The Wall Street Journal, 2015.\n\nAmazon doesnt consider the race of its customers. should it. D Ingold, S Soper, Bloomberg. D. Ingold and S. Soper, \"Amazon doesnt consider the race of its customers. should it?\" Bloomberg, 2016.\n\nLikelihood ratios for outof-distribution detection. J Ren, P J Liu, E Fertig, J Snoek, R Poplin, M A Depristo, J V Dillon, B Lakshminarayanan, the 33rd Conference on Neural Information Processing Systems (NeurIPS). J. Ren, P. J. Liu, E. Fertig, J. Snoek, R. Poplin, M. A. DePristo, J. V. Dillon, and B. Lakshminarayanan, \"Likelihood ratios for out- of-distribution detection.\" the 33rd Conference on Neural Information Processing Systems (NeurIPS), 2019.\n\nLearning fair representations. R Zemel, Y Wu, K Swersky, T Pitassi, C Dwork, International Conference on Machine Learning (ICML). R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, \"Learning fair representations,\" International Conference on Machine Learning (ICML), 2013.\n\nMulti-task feature learning via efficient l21-norm minimization. J Liu, S Ji, J Ye, Conference on Uncertainty in Artificial Intelligence (UAI). J. Liu, S. Ji, and J. Ye, \"Multi-task feature learning via efficient l21- norm minimization.\" Conference on Uncertainty in Artificial Intelligence (UAI), 2009.\n\nAn overview of multi-task learning in deep neural networks. S Ruder, arXiv:1706.05098arXiv preprintS. Ruder, \"An overview of multi-task learning in deep neural networks.\" arXiv preprint arXiv:1706.05098, 2017.\n\nAn overview of multi-task learning. Y Zhang, Q Yang, National Science Review. Y. Zhang and Q. Yang, \"An overview of multi-task learning.\" National Science Review, 2018.\n\nRank-based multi-task learning for fair regression. C Zhao, F Chen, Proceedings of the IEEE International Conference on Data Mining (ICDM). the IEEE International Conference on Data Mining (ICDM)C. Zhao and F. Chen, \"Rank-based multi-task learning for fair regres- sion,\" In Proceedings of the IEEE International Conference on Data Mining (ICDM), pp. 916-925, 2019.\n\nDomain adaption in one-shot learning. N Dong, E P Xing, Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD). N. Dong and E. P. Xing, \"Domain adaption in one-shot learning.\" In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD), 2018.\n\nLearning to cluster in order to transfer across domains and tasks. Y.-C Hsu, Z Lv, Z Kira, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)Y.-C. Hsu, Z. Lv, , and Z. Kira, \"Learning to cluster in order to transfer across domains and tasks.\" In Proceedings of the International Conference on Learning Representations (ICLR), 2018.\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)Y. Ganin and V. Lempitsky, \"Unsupervised domain adaptation by backpropagation.\" In Proceedings of the International Conference on Machine Learning (ICML), 2015.\n\nA survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on Knowledge and Data Engineering (TKDE). S. J. Pan and Q. Yang, \"A survey on transfer learning.\" IEEE Transac- tions on Knowledge and Data Engineering (TKDE), 2010.\n\nFairness constraints: Mechanisms for fair classification. M B Zafar, I Valera, M G Rodriguez, K P Gummadi, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics. the 20th International Conference on Artificial Intelligence and StatisticsAISTATSM. B. Zafar, I. Valera, M. G. Rodriguez, and K. P. Gummadi, \"Fairness constraints: Mechanisms for fair classification,\" Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.\n\nCertifying and removing disparate impact. M Feldman, S Friedler, J Moeller, C Scheidegger, S Venkatasubramanian, Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)M. Feldman, S. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasub- ramanian, \"Certifying and removing disparate impact.\" In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2015.\n\nA confidence-based approach for balancing fairness and accuracy. B Fish, J Kun, Dm D Lelkes, arXivpreprint:1601.05764B. Fish, J. Kun, and dm D. Lelkes, \"A confidence-based approach for balancing fairness and accuracy.\" arXivpreprint:1601.05764, 2015.\n\nEquality of opportunity in supervised learning. M Hardt, E Price, N Srebro, 30th Conference on Neural Information Processing Systems (NIPS). M. Hardt, E. Price, and N. Srebro, \"Equality of opportunity in supervised learning.\" 30th Conference on Neural Information Processing Systems (NIPS), 2016.\n\nModel-based and actual independence for fairness-aware classification. T Kamishima, S Akaho, H Asoh, J Sakuma, Data Mining and Knowledge Discovery. T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma, \"Model-based and actual independence for fairness-aware classification.\" Data Mining and Knowledge Discovery, 2018.\n\nDiscrimination-and privacy-aware patterns. S Hajian, J Domingo-Ferrer, A Monreale, D Pedreschi, F Giannotti, Data Mining and Knowledge Discovery. S. Hajian, J. Domingo-Ferrer, A. Monreale, D. Pedreschi, and F. Gi- annotti, \"Discrimination-and privacy-aware patterns.\" Data Mining and Knowledge Discovery, 2015.\n\nA reductions approach to fair classification. A Agarwal, A Beygelzimer, M Dudk, J Langford, H Wallach, International Conference on Machine Learning (ICML). 03A. Agarwal, A. Beygelzimer, M. Dudk, J. Langford, and H. Wallach, \"A reductions approach to fair classification,\" International Conference on Machine Learning (ICML), 03 2018.\n\nTaking advantage of multitask learning for fair classification. L Oneto, M Doninini, A Elders, M A Pontil, the 2019 AAAI/ACM Conference (AIES). L. Oneto, M. Doninini, A. Elders, and M. A. Pontil, \"Taking advan- tage of multitask learning for fair classification.\" the 2019 AAAI/ACM Conference (AIES), 2019.\n\nCorepresentation learning framework for the open-set data classification. Z Wang, B Dong, Y Lin, Y Wang, M S Islam, L Khan, 2019 IEEE International Conference on Big Data (Big Data). IEEEZ. Wang, B. Dong, Y. Lin, Y. Wang, M. S. Islam, and L. Khan, \"Co- representation learning framework for the open-set data classification,\" in 2019 IEEE International Conference on Big Data (Big Data). IEEE, 2019, pp. 239-244.\n\nControlling attribute effect in linear regression. T Calders, A Karim, F Kamiran, W Ali, X Zhang, IEEE International Conference on Data Mining (ICDM). T. Calders, A. Karim, F. Kamiran, W. Ali, and X. Zhang, \"Controlling attribute effect in linear regression,\" IEEE International Conference on Data Mining (ICDM), 2013.\n\nA convex framework for fair regression. R Berk, H Heidari, S Jabbari, M Joseph, M Kearns, J Morgenstern, S Neel, A Roth, Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT). the Conference on Fairness, Accountability, and Transparency (FAT)R. Berk, H. Heidari, S. Jabbari, M. Joseph, M. Kearns, J. Morgenstern, S. Neel, and A. Roth, \"A convex framework for fair regression.\" In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT), 2015.\n\nNonconvex optimization for regression with fairness constraints. J Komiyama, A Takeda, J Honda, H Shimao, Proceedings of the 35th International Conference on Machine Learning (ICML). the 35th International Conference on Machine Learning (ICML)J. Komiyama, A. Takeda, J. Honda, and H. Shimao, \"Nonconvex optimization for regression with fairness constraints.\" In Proceedings of the 35th International Conference on Machine Learning (ICML), 2018.\n\nFair kernel learning. A Prez-Suay, V Laparra, G Mateo-Garca, J Muoz-Mar, L Gmez-Chova, G Camps-Valls, Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD. A. Prez-Suay, V. Laparra, G. Mateo-Garca, J. Muoz-Mar, L. Gmez- Chova, and G. Camps-Valls, \"Fair kernel learning.\" In Joint Euro- pean Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD), 2017.\n\nNon-redundant data clustering. D Gondek, T Hofmann, Proceedings of the IEEE International Conference on Data Mining (ICDM). the IEEE International Conference on Data Mining (ICDM)D. Gondek and T. Hofmann, \"Non-redundant data clustering.\" In Pro- ceedings of the IEEE International Conference on Data Mining (ICDM), 2004.\n\nNon-redundant clustering with conditional ensembles. D Gondek, T Hofman, Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)D. Gondek and T. Hofman, \"Non-redundant clustering with conditional ensembles.\" In Proceedings of the ACM SIGKDD Conference on Knowl- edge Discovery and Data Mining (KDD), 2005.\n\nModel-based approaches for independence-enhanced recommendation. T Kamishima, S Akaho, H Asoh, I Sato, The IEEE 16th International Conference on Data Mining Workshops. ICDMWT. Kamishima, S. Akaho, H. Asoh, and I. Sato, \"Model-based ap- proaches for independence-enhanced recommendation.\" The IEEE 16th International Conference on Data Mining Workshops (ICDMW), 2017.\n\nConsiderations on recommendation independence for a find-good-items task. T Kamishima, S Akaho, Workshop on Responsible Recommendation. T. Kamishima and S. Akaho, \"Considerations on recommendation independence for a find-good-items task.\" In Workshop on Responsible Recommendation, 2017.\n\nFairness of exposure in rankings. A Singh, T Joachims, Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)A. Singh and T. Joachims, \"Fairness of exposure in rankings.\" In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2018.\n\nMan is to computer programmer as woman is to homemaker? debiasing word embeddings. T Bolukbasi, K.-W Chang, J Zou, V Saligrama, A Kalai, arXiv:1607.06520cs.CLT. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, and A. Kalai, \"Man is to computer programmer as woman is to homemaker? debiasing word embeddings.\" arXiv:1607.06520 [cs.CL], 2016.\n\nA survey on measuring indirect discrimination in machine learning. I Zliobaite, arXiv:1511.00148arXiv preprintI. Zliobaite, \"A survey on measuring indirect discrimination in machine learning.\" arXiv preprint arXiv:1511.00148, 2015.\n\nBig data's disparate impact. S Barocas, A D Selbst, California Law Review. 104S. Barocas and A. D. Selbst, \"Big data's disparate impact.\" California Law Review 104, 2016.\n\nFairness in decision-making the causal explanation formula. J Zhang, E Bareinboim, Association for the Advancement of Artificial Intelligence (AAAI). J. Zhang and E. Bareinboim, \"Fairness in decision-making the causal explanation formula.\" Association for the Advancement of Artificial Intelligence (AAAI), 2018.\n\nMatching networks for one shot learning. O Vinyals, C Blundell, T Lillicrap, K Kavukcuoglu, D Wierstra, 30th Conference on Neural Information Processing Systems (NIPS). O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra, \"Matching networks for one shot learning,\" In 30th Conference on Neural Information Processing Systems (NIPS), 2016.\n\nPrototypical networks for few-shot learning. J Snell, K Swersky, R Zemel, 31th Conference on Neural Information Processing Systems (NIPS). J. Snell, K. Swersky, and R. Zemel, \"Prototypical networks for few-shot learning,\" In 31th Conference on Neural Information Processing Systems (NIPS), 2017.\n\nOptimization as a model for few-shot learning. S Ravi, H Larochelle, International Conference for Learning Representations (ICLR. S. Ravi and H. Larochelle, \"Optimization as a model for few-shot learn- ing,\" International Conference for Learning Representations (ICLR), 2017.\n\nProbabilistic model-agnostic metalearning. C Finn, K Xu, S Levine, Advances in Neural Information Processing Systems (NIPS). C. Finn, K. Xu, , and S. Levine, \"Probabilistic model-agnostic meta- learning,\" In Advances in Neural Information Processing Systems (NIPS), 2018.\n\nReptile: a scalable metalearning algorithm. A Nichol, J Schulman, arXiv:1803.02999arXiv preprintA. Nichol and J. Schulman, \"Reptile: a scalable metalearning algorithm,\" arXiv preprint arXiv:1803.02999, 2018.\n\nMeta-learning with latent embedding optimization. A A Rusu, D Rao, J Sygnowski, O Vinyals, R Pascanu, S Osindero, R Hadsell, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero, and R. Hadsell, \"Meta-learning with latent embedding optimization,\" In Proceedings of the International Conference on Learning Representa- tions (ICLR), 2019.\n\nHow to train your maml. A Antoniou, H Edwards, A Storkey, International Conference for Learning Representations (ICLR). A. Antoniou, H. Edwards, and A. Storkey, \"How to train your maml,\" International Conference for Learning Representations (ICLR), 2019.\n\nAdverse impact and test validation: A practitioner's guide to valid and defensible employment testing. D Biddle, GowerD. Biddle, \"Adverse impact and test validation: A practitioner's guide to valid and defensible employment testing.\" Gower, 2005.\n\nOn the convergence theory of gradient-based model-agnostic meta-learning algorithms. A Fallah, A Mokhtari, A Ozdaglar, the proceedings of the 23rd International Conference of Artificial Intelligence and Statistics (AISTATS). 2020A. Fallah, A. Mokhtari, and A. Ozdaglar, \"On the convergence theory of gradient-based model-agnostic meta-learning algorithms.\" In the pro- ceedings of the 23rd International Conference of Artificial Intelligence and Statistics (AISTATS), 2020.\n\nOne shot learning of simple visual concepts. B M Lake, R Salakhutdinov, J Gross, J B Tenenbaum, The 41st Annual Meeting of the Cognitive Science Society (CogSci). B. M. Lake, R. Salakhutdinov, J. Gross, and J. B. Tenenbaum, \"One shot learning of simple visual concepts,\" The 41st Annual Meeting of the Cognitive Science Society (CogSci), 2011.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, arXiv:1502.03167arXiv preprintS. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" arXiv preprint arXiv:1502.03167, 2015.\n\nSiamese neural networks for one-shot image recognition. G Koch, R Zemel, R Salakhutdinov, Proceedings of the 32nd International Conference on Machine Learning (ICML). the 32nd International Conference on Machine Learning (ICML)G. Koch, R. Zemel, and R. Salakhutdinov, \"Siamese neural networks for one-shot image recognition.\" Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\n", "annotations": {"author": "[{\"end\":170,\"start\":50},{\"end\":295,\"start\":171},{\"end\":420,\"start\":296},{\"end\":541,\"start\":421}]", "publisher": null, "author_last_name": "[{\"end\":59,\"start\":55},{\"end\":182,\"start\":180},{\"end\":307,\"start\":305},{\"end\":430,\"start\":426}]", "author_first_name": "[{\"end\":54,\"start\":50},{\"end\":179,\"start\":171},{\"end\":304,\"start\":296},{\"end\":425,\"start\":421}]", "author_affiliation": "[{\"end\":169,\"start\":84},{\"end\":294,\"start\":209},{\"end\":419,\"start\":334},{\"end\":540,\"start\":455}]", "title": "[{\"end\":47,\"start\":1},{\"end\":588,\"start\":542}]", "venue": null, "abstract": "[{\"end\":1839,\"start\":669}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2406,\"start\":2403},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2529,\"start\":2526},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4167,\"start\":4164},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4260,\"start\":4257},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5756,\"start\":5753},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5845,\"start\":5842},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5850,\"start\":5847},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5877,\"start\":5873},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5883,\"start\":5879},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6006,\"start\":6003},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6062,\"start\":6059},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6728,\"start\":6724},{\"end\":7904,\"start\":7903},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9362,\"start\":9358},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9368,\"start\":9364},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9384,\"start\":9381},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9390,\"start\":9386},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9396,\"start\":9392},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9413,\"start\":9409},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9419,\"start\":9415},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9440,\"start\":9436},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9446,\"start\":9442},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9475,\"start\":9471},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9727,\"start\":9723},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9747,\"start\":9743},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9753,\"start\":9749},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9777,\"start\":9773},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9783,\"start\":9779},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9802,\"start\":9798},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9811,\"start\":9807},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10136,\"start\":10133},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10369,\"start\":10365},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10699,\"start\":10695},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10995,\"start\":10991},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11402,\"start\":11398},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12210,\"start\":12206},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12442,\"start\":12438},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12648,\"start\":12645},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12654,\"start\":12650},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12660,\"start\":12656},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14429,\"start\":14425},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15757,\"start\":15753},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16053,\"start\":16049},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16260,\"start\":16256},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16324,\"start\":16320},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16612,\"start\":16608},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17139,\"start\":17135},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17993,\"start\":17990},{\"end\":22386,\"start\":22384},{\"end\":22500,\"start\":22497},{\"end\":22512,\"start\":22509},{\"end\":22688,\"start\":22685},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22736,\"start\":22734},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22829,\"start\":22826},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22900,\"start\":22896},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23576,\"start\":23572},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24068,\"start\":24064},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24316,\"start\":24312},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24456,\"start\":24452},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24789,\"start\":24785},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24897,\"start\":24893},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25818,\"start\":25814},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26401,\"start\":26398},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26425,\"start\":26421},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26457,\"start\":26453},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27365,\"start\":27361},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28718,\"start\":28714},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28741,\"start\":28737}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31448,\"start\":31387},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32496,\"start\":31449},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32647,\"start\":32497},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33172,\"start\":32648},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34829,\"start\":33173}]", "paragraph": "[{\"end\":3619,\"start\":1858},{\"end\":4036,\"start\":3621},{\"end\":4687,\"start\":4038},{\"end\":5473,\"start\":4689},{\"end\":6104,\"start\":5475},{\"end\":7703,\"start\":6106},{\"end\":9077,\"start\":7705},{\"end\":9476,\"start\":9098},{\"end\":10067,\"start\":9523},{\"end\":11548,\"start\":10069},{\"end\":12734,\"start\":11578},{\"end\":13271,\"start\":12736},{\"end\":13564,\"start\":13333},{\"end\":14037,\"start\":13602},{\"end\":14430,\"start\":14039},{\"end\":14857,\"start\":14432},{\"end\":15396,\"start\":14925},{\"end\":16054,\"start\":15398},{\"end\":16170,\"start\":16056},{\"end\":16613,\"start\":16206},{\"end\":17280,\"start\":16773},{\"end\":17622,\"start\":17439},{\"end\":17764,\"start\":17669},{\"end\":18478,\"start\":17791},{\"end\":19302,\"start\":18494},{\"end\":20009,\"start\":19304},{\"end\":20411,\"start\":20109},{\"end\":20864,\"start\":20466},{\"end\":21336,\"start\":20919},{\"end\":21953,\"start\":21379},{\"end\":22208,\"start\":22020},{\"end\":22430,\"start\":22363},{\"end\":22562,\"start\":22432},{\"end\":22747,\"start\":22570},{\"end\":23145,\"start\":22773},{\"end\":23390,\"start\":23205},{\"end\":23551,\"start\":23409},{\"end\":24658,\"start\":23563},{\"end\":25191,\"start\":24682},{\"end\":26028,\"start\":25193},{\"end\":26294,\"start\":26030},{\"end\":26872,\"start\":26318},{\"end\":27685,\"start\":26874},{\"end\":28279,\"start\":27687},{\"end\":28977,\"start\":28281},{\"end\":29748,\"start\":28979},{\"end\":30205,\"start\":29750},{\"end\":30675,\"start\":30207},{\"end\":31386,\"start\":30710}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14924,\"start\":14858},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16205,\"start\":16171},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16716,\"start\":16614},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17438,\"start\":17281},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17668,\"start\":17623},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20050,\"start\":20010},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20465,\"start\":20412},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20918,\"start\":20865},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21378,\"start\":21337},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22019,\"start\":21954},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22362,\"start\":22248},{\"attributes\":{\"id\":\"formula_11\"},\"end\":23204,\"start\":23146}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26293,\"start\":26286},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28475,\"start\":28467},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":29490,\"start\":29481}]", "section_header": "[{\"end\":1856,\"start\":1841},{\"end\":9096,\"start\":9080},{\"end\":9521,\"start\":9479},{\"end\":11576,\"start\":11551},{\"end\":13331,\"start\":13274},{\"end\":13600,\"start\":13567},{\"end\":16771,\"start\":16718},{\"end\":17789,\"start\":17767},{\"end\":18492,\"start\":18481},{\"end\":20107,\"start\":20052},{\"end\":22247,\"start\":22211},{\"end\":22568,\"start\":22565},{\"end\":22771,\"start\":22750},{\"end\":23407,\"start\":23393},{\"end\":23561,\"start\":23554},{\"end\":24680,\"start\":24661},{\"end\":26316,\"start\":26297},{\"end\":30708,\"start\":30678},{\"end\":31396,\"start\":31388},{\"end\":31460,\"start\":31450},{\"end\":32658,\"start\":32649},{\"end\":33184,\"start\":33174}]", "table": "[{\"end\":33172,\"start\":32700},{\"end\":34829,\"start\":33887}]", "figure_caption": "[{\"end\":31448,\"start\":31398},{\"end\":32496,\"start\":31463},{\"end\":32647,\"start\":32499},{\"end\":32700,\"start\":32660},{\"end\":33887,\"start\":33187}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3130,\"start\":3122},{\"end\":4933,\"start\":4927},{\"end\":7151,\"start\":7142},{\"end\":12864,\"start\":12858},{\"end\":15445,\"start\":15437},{\"end\":19460,\"start\":19452}]", "bib_author_first_name": "[{\"end\":35016,\"start\":35015},{\"end\":35024,\"start\":35023},{\"end\":35034,\"start\":35033},{\"end\":35448,\"start\":35447},{\"end\":35664,\"start\":35663},{\"end\":35674,\"start\":35673},{\"end\":35851,\"start\":35850},{\"end\":35858,\"start\":35857},{\"end\":35860,\"start\":35859},{\"end\":35867,\"start\":35866},{\"end\":35877,\"start\":35876},{\"end\":35886,\"start\":35885},{\"end\":35896,\"start\":35895},{\"end\":35898,\"start\":35897},{\"end\":35910,\"start\":35909},{\"end\":35912,\"start\":35911},{\"end\":35922,\"start\":35921},{\"end\":36286,\"start\":36285},{\"end\":36295,\"start\":36294},{\"end\":36301,\"start\":36300},{\"end\":36312,\"start\":36311},{\"end\":36323,\"start\":36322},{\"end\":36598,\"start\":36597},{\"end\":36605,\"start\":36604},{\"end\":36611,\"start\":36610},{\"end\":36898,\"start\":36897},{\"end\":37085,\"start\":37084},{\"end\":37094,\"start\":37093},{\"end\":37271,\"start\":37270},{\"end\":37279,\"start\":37278},{\"end\":37624,\"start\":37623},{\"end\":37632,\"start\":37631},{\"end\":37634,\"start\":37633},{\"end\":37978,\"start\":37974},{\"end\":37985,\"start\":37984},{\"end\":37991,\"start\":37990},{\"end\":38385,\"start\":38384},{\"end\":38394,\"start\":38393},{\"end\":38727,\"start\":38726},{\"end\":38729,\"start\":38728},{\"end\":38736,\"start\":38735},{\"end\":38987,\"start\":38986},{\"end\":38989,\"start\":38988},{\"end\":38998,\"start\":38997},{\"end\":39008,\"start\":39007},{\"end\":39010,\"start\":39009},{\"end\":39023,\"start\":39022},{\"end\":39025,\"start\":39024},{\"end\":39481,\"start\":39480},{\"end\":39492,\"start\":39491},{\"end\":39504,\"start\":39503},{\"end\":39515,\"start\":39514},{\"end\":39530,\"start\":39529},{\"end\":39997,\"start\":39996},{\"end\":40005,\"start\":40004},{\"end\":40013,\"start\":40011},{\"end\":40015,\"start\":40014},{\"end\":40232,\"start\":40231},{\"end\":40241,\"start\":40240},{\"end\":40250,\"start\":40249},{\"end\":40553,\"start\":40552},{\"end\":40566,\"start\":40565},{\"end\":40575,\"start\":40574},{\"end\":40583,\"start\":40582},{\"end\":40838,\"start\":40837},{\"end\":40848,\"start\":40847},{\"end\":40866,\"start\":40865},{\"end\":40878,\"start\":40877},{\"end\":40891,\"start\":40890},{\"end\":41153,\"start\":41152},{\"end\":41164,\"start\":41163},{\"end\":41179,\"start\":41178},{\"end\":41187,\"start\":41186},{\"end\":41199,\"start\":41198},{\"end\":41506,\"start\":41505},{\"end\":41515,\"start\":41514},{\"end\":41527,\"start\":41526},{\"end\":41537,\"start\":41536},{\"end\":41539,\"start\":41538},{\"end\":41824,\"start\":41823},{\"end\":41832,\"start\":41831},{\"end\":41840,\"start\":41839},{\"end\":41847,\"start\":41846},{\"end\":41855,\"start\":41854},{\"end\":41857,\"start\":41856},{\"end\":41866,\"start\":41865},{\"end\":42215,\"start\":42214},{\"end\":42226,\"start\":42225},{\"end\":42235,\"start\":42234},{\"end\":42246,\"start\":42245},{\"end\":42253,\"start\":42252},{\"end\":42524,\"start\":42523},{\"end\":42532,\"start\":42531},{\"end\":42543,\"start\":42542},{\"end\":42554,\"start\":42553},{\"end\":42564,\"start\":42563},{\"end\":42574,\"start\":42573},{\"end\":42589,\"start\":42588},{\"end\":42597,\"start\":42596},{\"end\":43047,\"start\":43046},{\"end\":43059,\"start\":43058},{\"end\":43069,\"start\":43068},{\"end\":43078,\"start\":43077},{\"end\":43450,\"start\":43449},{\"end\":43463,\"start\":43462},{\"end\":43474,\"start\":43473},{\"end\":43489,\"start\":43488},{\"end\":43501,\"start\":43500},{\"end\":43515,\"start\":43514},{\"end\":43879,\"start\":43878},{\"end\":43889,\"start\":43888},{\"end\":44223,\"start\":44222},{\"end\":44233,\"start\":44232},{\"end\":44644,\"start\":44643},{\"end\":44657,\"start\":44656},{\"end\":44666,\"start\":44665},{\"end\":44674,\"start\":44673},{\"end\":45021,\"start\":45020},{\"end\":45034,\"start\":45033},{\"end\":45270,\"start\":45269},{\"end\":45279,\"start\":45278},{\"end\":45690,\"start\":45689},{\"end\":45706,\"start\":45702},{\"end\":45715,\"start\":45714},{\"end\":45722,\"start\":45721},{\"end\":45735,\"start\":45734},{\"end\":46013,\"start\":46012},{\"end\":46208,\"start\":46207},{\"end\":46219,\"start\":46218},{\"end\":46221,\"start\":46220},{\"end\":46411,\"start\":46410},{\"end\":46420,\"start\":46419},{\"end\":46706,\"start\":46705},{\"end\":46717,\"start\":46716},{\"end\":46729,\"start\":46728},{\"end\":46742,\"start\":46741},{\"end\":46757,\"start\":46756},{\"end\":47069,\"start\":47068},{\"end\":47078,\"start\":47077},{\"end\":47089,\"start\":47088},{\"end\":47368,\"start\":47367},{\"end\":47376,\"start\":47375},{\"end\":47641,\"start\":47640},{\"end\":47649,\"start\":47648},{\"end\":47655,\"start\":47654},{\"end\":47915,\"start\":47914},{\"end\":47925,\"start\":47924},{\"end\":48130,\"start\":48129},{\"end\":48132,\"start\":48131},{\"end\":48140,\"start\":48139},{\"end\":48147,\"start\":48146},{\"end\":48160,\"start\":48159},{\"end\":48171,\"start\":48170},{\"end\":48182,\"start\":48181},{\"end\":48194,\"start\":48193},{\"end\":48603,\"start\":48602},{\"end\":48615,\"start\":48614},{\"end\":48626,\"start\":48625},{\"end\":48938,\"start\":48937},{\"end\":49168,\"start\":49167},{\"end\":49178,\"start\":49177},{\"end\":49190,\"start\":49189},{\"end\":49603,\"start\":49602},{\"end\":49605,\"start\":49604},{\"end\":49613,\"start\":49612},{\"end\":49630,\"start\":49629},{\"end\":49639,\"start\":49638},{\"end\":49641,\"start\":49640},{\"end\":49997,\"start\":49996},{\"end\":50006,\"start\":50005},{\"end\":50264,\"start\":50263},{\"end\":50272,\"start\":50271},{\"end\":50281,\"start\":50280}]", "bib_author_last_name": "[{\"end\":35021,\"start\":35017},{\"end\":35031,\"start\":35025},{\"end\":35041,\"start\":35035},{\"end\":35453,\"start\":35449},{\"end\":35671,\"start\":35665},{\"end\":35680,\"start\":35675},{\"end\":35855,\"start\":35852},{\"end\":35864,\"start\":35861},{\"end\":35874,\"start\":35868},{\"end\":35883,\"start\":35878},{\"end\":35893,\"start\":35887},{\"end\":35907,\"start\":35899},{\"end\":35919,\"start\":35913},{\"end\":35939,\"start\":35923},{\"end\":36292,\"start\":36287},{\"end\":36298,\"start\":36296},{\"end\":36309,\"start\":36302},{\"end\":36320,\"start\":36313},{\"end\":36329,\"start\":36324},{\"end\":36602,\"start\":36599},{\"end\":36608,\"start\":36606},{\"end\":36614,\"start\":36612},{\"end\":36904,\"start\":36899},{\"end\":37091,\"start\":37086},{\"end\":37099,\"start\":37095},{\"end\":37276,\"start\":37272},{\"end\":37284,\"start\":37280},{\"end\":37629,\"start\":37625},{\"end\":37639,\"start\":37635},{\"end\":37982,\"start\":37979},{\"end\":37988,\"start\":37986},{\"end\":37996,\"start\":37992},{\"end\":38391,\"start\":38386},{\"end\":38404,\"start\":38395},{\"end\":38733,\"start\":38730},{\"end\":38741,\"start\":38737},{\"end\":38995,\"start\":38990},{\"end\":39005,\"start\":38999},{\"end\":39020,\"start\":39011},{\"end\":39033,\"start\":39026},{\"end\":39489,\"start\":39482},{\"end\":39501,\"start\":39493},{\"end\":39512,\"start\":39505},{\"end\":39527,\"start\":39516},{\"end\":39549,\"start\":39531},{\"end\":40002,\"start\":39998},{\"end\":40009,\"start\":40006},{\"end\":40022,\"start\":40016},{\"end\":40238,\"start\":40233},{\"end\":40247,\"start\":40242},{\"end\":40257,\"start\":40251},{\"end\":40563,\"start\":40554},{\"end\":40572,\"start\":40567},{\"end\":40580,\"start\":40576},{\"end\":40590,\"start\":40584},{\"end\":40845,\"start\":40839},{\"end\":40863,\"start\":40849},{\"end\":40875,\"start\":40867},{\"end\":40888,\"start\":40879},{\"end\":40901,\"start\":40892},{\"end\":41161,\"start\":41154},{\"end\":41176,\"start\":41165},{\"end\":41184,\"start\":41180},{\"end\":41196,\"start\":41188},{\"end\":41207,\"start\":41200},{\"end\":41512,\"start\":41507},{\"end\":41524,\"start\":41516},{\"end\":41534,\"start\":41528},{\"end\":41546,\"start\":41540},{\"end\":41829,\"start\":41825},{\"end\":41837,\"start\":41833},{\"end\":41844,\"start\":41841},{\"end\":41852,\"start\":41848},{\"end\":41863,\"start\":41858},{\"end\":41871,\"start\":41867},{\"end\":42223,\"start\":42216},{\"end\":42232,\"start\":42227},{\"end\":42243,\"start\":42236},{\"end\":42250,\"start\":42247},{\"end\":42259,\"start\":42254},{\"end\":42529,\"start\":42525},{\"end\":42540,\"start\":42533},{\"end\":42551,\"start\":42544},{\"end\":42561,\"start\":42555},{\"end\":42571,\"start\":42565},{\"end\":42586,\"start\":42575},{\"end\":42594,\"start\":42590},{\"end\":42602,\"start\":42598},{\"end\":43056,\"start\":43048},{\"end\":43066,\"start\":43060},{\"end\":43075,\"start\":43070},{\"end\":43085,\"start\":43079},{\"end\":43460,\"start\":43451},{\"end\":43471,\"start\":43464},{\"end\":43486,\"start\":43475},{\"end\":43498,\"start\":43490},{\"end\":43512,\"start\":43502},{\"end\":43527,\"start\":43516},{\"end\":43886,\"start\":43880},{\"end\":43897,\"start\":43890},{\"end\":44230,\"start\":44224},{\"end\":44240,\"start\":44234},{\"end\":44654,\"start\":44645},{\"end\":44663,\"start\":44658},{\"end\":44671,\"start\":44667},{\"end\":44679,\"start\":44675},{\"end\":45031,\"start\":45022},{\"end\":45040,\"start\":45035},{\"end\":45276,\"start\":45271},{\"end\":45288,\"start\":45280},{\"end\":45700,\"start\":45691},{\"end\":45712,\"start\":45707},{\"end\":45719,\"start\":45716},{\"end\":45732,\"start\":45723},{\"end\":45741,\"start\":45736},{\"end\":46023,\"start\":46014},{\"end\":46216,\"start\":46209},{\"end\":46228,\"start\":46222},{\"end\":46417,\"start\":46412},{\"end\":46431,\"start\":46421},{\"end\":46714,\"start\":46707},{\"end\":46726,\"start\":46718},{\"end\":46739,\"start\":46730},{\"end\":46754,\"start\":46743},{\"end\":46766,\"start\":46758},{\"end\":47075,\"start\":47070},{\"end\":47086,\"start\":47079},{\"end\":47095,\"start\":47090},{\"end\":47373,\"start\":47369},{\"end\":47387,\"start\":47377},{\"end\":47646,\"start\":47642},{\"end\":47652,\"start\":47650},{\"end\":47662,\"start\":47656},{\"end\":47922,\"start\":47916},{\"end\":47934,\"start\":47926},{\"end\":48137,\"start\":48133},{\"end\":48144,\"start\":48141},{\"end\":48157,\"start\":48148},{\"end\":48168,\"start\":48161},{\"end\":48179,\"start\":48172},{\"end\":48191,\"start\":48183},{\"end\":48202,\"start\":48195},{\"end\":48612,\"start\":48604},{\"end\":48623,\"start\":48616},{\"end\":48634,\"start\":48627},{\"end\":48945,\"start\":48939},{\"end\":49175,\"start\":49169},{\"end\":49187,\"start\":49179},{\"end\":49199,\"start\":49191},{\"end\":49610,\"start\":49606},{\"end\":49627,\"start\":49614},{\"end\":49636,\"start\":49631},{\"end\":49651,\"start\":49642},{\"end\":50003,\"start\":49998},{\"end\":50014,\"start\":50007},{\"end\":50269,\"start\":50265},{\"end\":50278,\"start\":50273},{\"end\":50295,\"start\":50282}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6719686},\"end\":35366,\"start\":34948},{\"attributes\":{\"id\":\"b1\"},\"end\":35600,\"start\":35368},{\"attributes\":{\"id\":\"b2\"},\"end\":35796,\"start\":35602},{\"attributes\":{\"id\":\"b3\"},\"end\":36252,\"start\":35798},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":490669},\"end\":36530,\"start\":36254},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51985230},\"end\":36835,\"start\":36532},{\"attributes\":{\"doi\":\"arXiv:1706.05098\",\"id\":\"b6\"},\"end\":37046,\"start\":36837},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":90063862},\"end\":37216,\"start\":37048},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":210848927},\"end\":37583,\"start\":37218},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":58015193},\"end\":37905,\"start\":37585},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3527879},\"end\":38331,\"start\":37907},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6755881},\"end\":38693,\"start\":38333},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":740063},\"end\":38926,\"start\":38695},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8529258},\"end\":39436,\"start\":38928},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2077168},\"end\":39929,\"start\":39438},{\"attributes\":{\"doi\":\"arXivpreprint:1601.05764\",\"id\":\"b15\"},\"end\":40181,\"start\":39931},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7567061},\"end\":40479,\"start\":40183},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1909781},\"end\":40792,\"start\":40481},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14410788},\"end\":41104,\"start\":40794},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4725675},\"end\":41439,\"start\":41106},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":53047413},\"end\":41747,\"start\":41441},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":211298631},\"end\":42161,\"start\":41749},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16541789},\"end\":42481,\"start\":42163},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12641090},\"end\":42979,\"start\":42483},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":51801548},\"end\":43425,\"start\":42981},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":33280631},\"end\":43845,\"start\":43427},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14170409},\"end\":44167,\"start\":43847},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5641274},\"end\":44576,\"start\":44169},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":16772240},\"end\":44944,\"start\":44578},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":37511334},\"end\":45233,\"start\":44946},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3387187},\"end\":45604,\"start\":45235},{\"attributes\":{\"doi\":\"arXiv:1607.06520\",\"id\":\"b31\"},\"end\":45943,\"start\":45606},{\"attributes\":{\"doi\":\"arXiv:1511.00148\",\"id\":\"b32\"},\"end\":46176,\"start\":45945},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":143133374},\"end\":46348,\"start\":46178},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":19117312},\"end\":46662,\"start\":46350},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8909022},\"end\":47021,\"start\":46664},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":309759},\"end\":47318,\"start\":47023},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":67413369},\"end\":47595,\"start\":47320},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":46977722},\"end\":47868,\"start\":47597},{\"attributes\":{\"doi\":\"arXiv:1803.02999\",\"id\":\"b39\"},\"end\":48077,\"start\":47870},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":49868626},\"end\":48576,\"start\":48079},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":53036488},\"end\":48832,\"start\":48578},{\"attributes\":{\"id\":\"b42\"},\"end\":49080,\"start\":48834},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":201651077},\"end\":49555,\"start\":49082},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":15373038},\"end\":49900,\"start\":49557},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b45\"},\"end\":50205,\"start\":49902},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":13874643},\"end\":50615,\"start\":50207}]", "bib_title": "[{\"end\":35013,\"start\":34948},{\"end\":35445,\"start\":35368},{\"end\":35661,\"start\":35602},{\"end\":35848,\"start\":35798},{\"end\":36283,\"start\":36254},{\"end\":36595,\"start\":36532},{\"end\":37082,\"start\":37048},{\"end\":37268,\"start\":37218},{\"end\":37621,\"start\":37585},{\"end\":37972,\"start\":37907},{\"end\":38382,\"start\":38333},{\"end\":38724,\"start\":38695},{\"end\":38984,\"start\":38928},{\"end\":39478,\"start\":39438},{\"end\":40229,\"start\":40183},{\"end\":40550,\"start\":40481},{\"end\":40835,\"start\":40794},{\"end\":41150,\"start\":41106},{\"end\":41503,\"start\":41441},{\"end\":41821,\"start\":41749},{\"end\":42212,\"start\":42163},{\"end\":42521,\"start\":42483},{\"end\":43044,\"start\":42981},{\"end\":43447,\"start\":43427},{\"end\":43876,\"start\":43847},{\"end\":44220,\"start\":44169},{\"end\":44641,\"start\":44578},{\"end\":45018,\"start\":44946},{\"end\":45267,\"start\":45235},{\"end\":46205,\"start\":46178},{\"end\":46408,\"start\":46350},{\"end\":46703,\"start\":46664},{\"end\":47066,\"start\":47023},{\"end\":47365,\"start\":47320},{\"end\":47638,\"start\":47597},{\"end\":48127,\"start\":48079},{\"end\":48600,\"start\":48578},{\"end\":49165,\"start\":49082},{\"end\":49600,\"start\":49557},{\"end\":50261,\"start\":50207}]", "bib_author": "[{\"end\":35023,\"start\":35015},{\"end\":35033,\"start\":35023},{\"end\":35043,\"start\":35033},{\"end\":35455,\"start\":35447},{\"end\":35673,\"start\":35663},{\"end\":35682,\"start\":35673},{\"end\":35857,\"start\":35850},{\"end\":35866,\"start\":35857},{\"end\":35876,\"start\":35866},{\"end\":35885,\"start\":35876},{\"end\":35895,\"start\":35885},{\"end\":35909,\"start\":35895},{\"end\":35921,\"start\":35909},{\"end\":35941,\"start\":35921},{\"end\":36294,\"start\":36285},{\"end\":36300,\"start\":36294},{\"end\":36311,\"start\":36300},{\"end\":36322,\"start\":36311},{\"end\":36331,\"start\":36322},{\"end\":36604,\"start\":36597},{\"end\":36610,\"start\":36604},{\"end\":36616,\"start\":36610},{\"end\":36906,\"start\":36897},{\"end\":37093,\"start\":37084},{\"end\":37101,\"start\":37093},{\"end\":37278,\"start\":37270},{\"end\":37286,\"start\":37278},{\"end\":37631,\"start\":37623},{\"end\":37641,\"start\":37631},{\"end\":37984,\"start\":37974},{\"end\":37990,\"start\":37984},{\"end\":37998,\"start\":37990},{\"end\":38393,\"start\":38384},{\"end\":38406,\"start\":38393},{\"end\":38735,\"start\":38726},{\"end\":38743,\"start\":38735},{\"end\":38997,\"start\":38986},{\"end\":39007,\"start\":38997},{\"end\":39022,\"start\":39007},{\"end\":39035,\"start\":39022},{\"end\":39491,\"start\":39480},{\"end\":39503,\"start\":39491},{\"end\":39514,\"start\":39503},{\"end\":39529,\"start\":39514},{\"end\":39551,\"start\":39529},{\"end\":40004,\"start\":39996},{\"end\":40011,\"start\":40004},{\"end\":40024,\"start\":40011},{\"end\":40240,\"start\":40231},{\"end\":40249,\"start\":40240},{\"end\":40259,\"start\":40249},{\"end\":40565,\"start\":40552},{\"end\":40574,\"start\":40565},{\"end\":40582,\"start\":40574},{\"end\":40592,\"start\":40582},{\"end\":40847,\"start\":40837},{\"end\":40865,\"start\":40847},{\"end\":40877,\"start\":40865},{\"end\":40890,\"start\":40877},{\"end\":40903,\"start\":40890},{\"end\":41163,\"start\":41152},{\"end\":41178,\"start\":41163},{\"end\":41186,\"start\":41178},{\"end\":41198,\"start\":41186},{\"end\":41209,\"start\":41198},{\"end\":41514,\"start\":41505},{\"end\":41526,\"start\":41514},{\"end\":41536,\"start\":41526},{\"end\":41548,\"start\":41536},{\"end\":41831,\"start\":41823},{\"end\":41839,\"start\":41831},{\"end\":41846,\"start\":41839},{\"end\":41854,\"start\":41846},{\"end\":41865,\"start\":41854},{\"end\":41873,\"start\":41865},{\"end\":42225,\"start\":42214},{\"end\":42234,\"start\":42225},{\"end\":42245,\"start\":42234},{\"end\":42252,\"start\":42245},{\"end\":42261,\"start\":42252},{\"end\":42531,\"start\":42523},{\"end\":42542,\"start\":42531},{\"end\":42553,\"start\":42542},{\"end\":42563,\"start\":42553},{\"end\":42573,\"start\":42563},{\"end\":42588,\"start\":42573},{\"end\":42596,\"start\":42588},{\"end\":42604,\"start\":42596},{\"end\":43058,\"start\":43046},{\"end\":43068,\"start\":43058},{\"end\":43077,\"start\":43068},{\"end\":43087,\"start\":43077},{\"end\":43462,\"start\":43449},{\"end\":43473,\"start\":43462},{\"end\":43488,\"start\":43473},{\"end\":43500,\"start\":43488},{\"end\":43514,\"start\":43500},{\"end\":43529,\"start\":43514},{\"end\":43888,\"start\":43878},{\"end\":43899,\"start\":43888},{\"end\":44232,\"start\":44222},{\"end\":44242,\"start\":44232},{\"end\":44656,\"start\":44643},{\"end\":44665,\"start\":44656},{\"end\":44673,\"start\":44665},{\"end\":44681,\"start\":44673},{\"end\":45033,\"start\":45020},{\"end\":45042,\"start\":45033},{\"end\":45278,\"start\":45269},{\"end\":45290,\"start\":45278},{\"end\":45702,\"start\":45689},{\"end\":45714,\"start\":45702},{\"end\":45721,\"start\":45714},{\"end\":45734,\"start\":45721},{\"end\":45743,\"start\":45734},{\"end\":46025,\"start\":46012},{\"end\":46218,\"start\":46207},{\"end\":46230,\"start\":46218},{\"end\":46419,\"start\":46410},{\"end\":46433,\"start\":46419},{\"end\":46716,\"start\":46705},{\"end\":46728,\"start\":46716},{\"end\":46741,\"start\":46728},{\"end\":46756,\"start\":46741},{\"end\":46768,\"start\":46756},{\"end\":47077,\"start\":47068},{\"end\":47088,\"start\":47077},{\"end\":47097,\"start\":47088},{\"end\":47375,\"start\":47367},{\"end\":47389,\"start\":47375},{\"end\":47648,\"start\":47640},{\"end\":47654,\"start\":47648},{\"end\":47664,\"start\":47654},{\"end\":47924,\"start\":47914},{\"end\":47936,\"start\":47924},{\"end\":48139,\"start\":48129},{\"end\":48146,\"start\":48139},{\"end\":48159,\"start\":48146},{\"end\":48170,\"start\":48159},{\"end\":48181,\"start\":48170},{\"end\":48193,\"start\":48181},{\"end\":48204,\"start\":48193},{\"end\":48614,\"start\":48602},{\"end\":48625,\"start\":48614},{\"end\":48636,\"start\":48625},{\"end\":48947,\"start\":48937},{\"end\":49177,\"start\":49167},{\"end\":49189,\"start\":49177},{\"end\":49201,\"start\":49189},{\"end\":49612,\"start\":49602},{\"end\":49629,\"start\":49612},{\"end\":49638,\"start\":49629},{\"end\":49653,\"start\":49638},{\"end\":50005,\"start\":49996},{\"end\":50016,\"start\":50005},{\"end\":50271,\"start\":50263},{\"end\":50280,\"start\":50271},{\"end\":50297,\"start\":50280}]", "bib_venue": "[{\"end\":35180,\"start\":35120},{\"end\":37413,\"start\":37358},{\"end\":38141,\"start\":38078},{\"end\":38533,\"start\":38478},{\"end\":39202,\"start\":39127},{\"end\":39708,\"start\":39638},{\"end\":42753,\"start\":42687},{\"end\":43224,\"start\":43164},{\"end\":44026,\"start\":43971},{\"end\":44399,\"start\":44329},{\"end\":45447,\"start\":45377},{\"end\":48347,\"start\":48284},{\"end\":50434,\"start\":50374},{\"end\":35118,\"start\":35043},{\"end\":35478,\"start\":35455},{\"end\":35691,\"start\":35682},{\"end\":36011,\"start\":35941},{\"end\":36382,\"start\":36331},{\"end\":36674,\"start\":36616},{\"end\":36895,\"start\":36837},{\"end\":37124,\"start\":37101},{\"end\":37356,\"start\":37286},{\"end\":37735,\"start\":37641},{\"end\":38076,\"start\":37998},{\"end\":38476,\"start\":38406},{\"end\":38801,\"start\":38743},{\"end\":39125,\"start\":39035},{\"end\":39636,\"start\":39551},{\"end\":39994,\"start\":39931},{\"end\":40322,\"start\":40259},{\"end\":40627,\"start\":40592},{\"end\":40938,\"start\":40903},{\"end\":41260,\"start\":41209},{\"end\":41583,\"start\":41548},{\"end\":41930,\"start\":41873},{\"end\":42312,\"start\":42261},{\"end\":42685,\"start\":42604},{\"end\":43162,\"start\":43087},{\"end\":43622,\"start\":43529},{\"end\":43969,\"start\":43899},{\"end\":44327,\"start\":44242},{\"end\":44744,\"start\":44681},{\"end\":45080,\"start\":45042},{\"end\":45375,\"start\":45290},{\"end\":45687,\"start\":45606},{\"end\":46010,\"start\":45945},{\"end\":46251,\"start\":46230},{\"end\":46498,\"start\":46433},{\"end\":46831,\"start\":46768},{\"end\":47160,\"start\":47097},{\"end\":47448,\"start\":47389},{\"end\":47720,\"start\":47664},{\"end\":47912,\"start\":47870},{\"end\":48282,\"start\":48204},{\"end\":48696,\"start\":48636},{\"end\":48935,\"start\":48834},{\"end\":49305,\"start\":49201},{\"end\":49718,\"start\":49653},{\"end\":49994,\"start\":49902},{\"end\":50372,\"start\":50297}]"}}}, "year": 2023, "month": 12, "day": 17}