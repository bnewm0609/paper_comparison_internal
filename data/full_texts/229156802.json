{"id": 229156802, "updated": "2023-10-06 07:44:22.43", "metadata": {"title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting", "authors": "[{\"first\":\"Haoyi\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Shanghang\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Jieqi\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Shuai\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Jianxin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Xiong\",\"middle\":[]},{\"first\":\"Wancai\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 12, "day": 14}, "abstract": "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, such as quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a $ProbSparse$ Self-attention mechanism, which achieves $O(L \\log L)$ in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2012.07436", "mag": "3111507638", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/ZhouZPZLXZ21", "doi": "10.1609/aaai.v35i12.17325"}}, "content": {"source": {"pdf_hash": "5b9d8bcc46b766b47389c912a8e026f81b91b0d8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2012.07436v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "58520a49c3123a270ccc51d75cf3994d3b8a80f4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5b9d8bcc46b766b47389c912a8e026f81b91b0d8.txt", "contents": "\nInformer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\n\n\nHaoyi Zhou \nBeihang University\n\n\nShanghang Zhang \nBerkeley\n\nJieqi Peng \nBeihang University\n\n\nShuai Zhang \nBeihang University\n\n\nJianxin Li \nBeihang University\n\n\nHui Xiong xionghui@gmail.com \nRutgers University\n\n\nWancai Zhang zhangwancai@sdee.sgcc.com.cn \nBeijing Guowang Fuda Science & Technology Development Company\n\n\nInformer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\n\nMany real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, such as quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse Self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.\n\nIntroduction\n\nTime-series forecasting is a critical ingredient across many domains, such as sensor network monitoring (Papadimitriou and Yu 2006), energy and smart grid management, economics and finance (Zhu and Shasha 2002), and disease propagation analysis (Matsubara et al. 2014). In these scenarios, we can leverage a substantial amount of time-series data on past behavior to make a forecast in the long run, namely long sequence time-series forecasting (LSTF). However, existing methods are designed under limited problem setting, like predicting 48 points or less (Hochreiter and Schmidhuber 1997;Li et al. 2018;Liu et al. 2019;Qin et al. 2017;Wen et al. 2017). The increasingly long sequences strain the models' prediction capacity to the point where some argue that this trend is holding the LSTF research. As an empirical example, Fig.(1) shows the forecasting results on a real dataset, where the LSTM net-  The prediction capacity of existing methods limits the long sequence's performance, i.e., starting from length=48, the MSE rises unacceptably high, and the inference speed drops rapidly.\n\nwork predicts the hourly temperature of an electrical transformer station from the short-term period (12 points, 0.5 days) to the long-term period (480 points, 20 days). The overall performance gap is substantial when the prediction length is greater than 48 points (the solid star in Fig.(1(c)). The MSE score rises to unsatisfactory performance, the inference speed gets sharp drop, and the LSTM model fails. The major challenge for LSTF is enhancing the prediction capacity to meet the increasingly long sequences demand, which requires (a) extraordinary long-range alignment ability and (b) efficient operations on long sequence inputs and outputs. Recently, Transformer models show superior performance in capturing long-range dependency than RNN models. The self-attention mechanism can reduce the maximum length of network signals traveling paths into the theoretical shortest O(1) and avoids the recurrent structure, whereby Transformer shows great potential for LSTF problem. But on the other hand, the self-attention mechanism violates requirement (b) due to its L-quadratic computation and memory consumption on L length inputs/outputs. Some large-scale Transformer models pour resources and yield impressive results on NLP tasks (Brown et al. 2020), but the training on dozens of GPUs and expensive deploying cost make theses models unaffordable on real-world LSTF problem. The efficiency of the self-attention mechanism and Transformer framework becomes the bottleneck of applying them to LSTF problem. Thus, in this paper, we seek to an-swer the question: can Transformer models be improved to be computation, memory, and architecture efficient, as well as maintain higher prediction capacity?\n\nVanilla Transformer (Vaswani et al. 2017) has three significant limitations when solving LSTF:\n\n1. The quadratic computation of self-attention. The atom operation of self-attention mechanism, namely canonical dot-product, causes the time complexity and memory usage per layer to be O(L 2 ). 2. The memory bottleneck in stacking layers for long inputs. The stack of J encoder/decoder layer makes total memory usage to be O(J \u00b7 L 2 ), which limits the model scalability on receiving long sequence inputs. 3. The speed plunge in predicting long outputs. The dynamic decoding of vanilla Transformer makes the step-by-step inference as slow as RNN-based model, suggested in Fig.(1c). There are some prior works on improving the efficiency of self-attention. The Sparse Transformer (Child et al. 2019), LogSparse Transformer (Li et al. 2019), and Longformer (Beltagy, Peters, and Cohan 2020) all use a heuristic method to tackle limitation 1 and reduce the complexity of selfattention mechanism to O(L log L), where their efficiency gain is limited (Qiu et al. 2019). Reformer (Kitaev, Kaiser, and Levskaya 2019) also achieves O(L log L) by locallysensitive hashing self-attention, but it only works on extremely long sequences. More recently, Linformer (Wang et al. 2020) claims a linear complexity O(L), but the project matrix can not be fixed for real-world long sequence input, which may have the risk of degradation to O(L 2 ). Transformer-XL (Dai et al. 2019) and Compressive Transformer (Rae et al. 2019) use auxiliary hidden states to capture long-range dependency, which could amplify limitation 1 and be adverse to break the efficiency bottleneck. All the works mainly focus on limitation 1, and the limitation 2&3 remains in the LSTF problem. To enhance the prediction capacity, we will tackle all of them and achieve improvement beyond efficiency in the proposed Informer.\n\nTo this end, our work delves explicitly into these three issues. We investigate the sparsity in the self-attention mechanism, make improvements of network components, and conduct extensive experiments. The contributions of this paper are summarized as follows:\n\n\u2022 We propose Informer to successfully enhance the prediction capacity in the LSTF problem, which validates the Transformer-like model's potential value to capture individual long-range dependency between long sequence time-series outputs and inputs. \u2022 We propose ProbSparse Self-attention mechanism to efficiently replace the canonical self-attention and it achieves the O(L log L) time complexity and O(L log L) memory usage. \u2022 We propose Self-attention Distilling operation privileges dominating attention scores in J-stacking layers and sharply reduce the total space complexity to be O((2 \u2212 )L log L). \u2022 We propose Generative Style Decoder to acquire long sequence output with only one forward step needed, simultaneously avoiding cumulative error spreading \n\n\nPreliminary\n\nWe first provide the problem definition. Under the rolling forecasting setting with a fixed size window, we have the input X t = {x t 1 , . . . , x t Lx | x t i \u2208 R dx } at time t, and output is to predict corresponding sequence Y t = {y t 1 , . . . , y t Ly | y t i \u2208 R dy }. LSTF problem encourages a longer output's length L y than previous works Sutskever, Vinyals, and Le 2014) and the feature dimension is not limited to univariate case (d y \u2265 1).\n\nEncoder-decoder architecture Many popular models are devised to \"encode\" the input representations X t into a hidden state representations H t and \"decode\" an output rep-\nresentations Y t from H t = {h t 1 , . . . , h t L h }.\nThe inference involves a step-by-step process named \"dynamic decoding\", where the decoder computes a new hidden state h t k+1 from the previous state h t k and other necessary outputs from k-th step then predicts the (k + 1)-th sequence y t k+1 . Input Representation A uniform input representation is given to enhance the global positional context and local temporal context of time-series inputs. To avoid trivializing description, we put the details in Appendix B.\n\n\nMethodology\n\nExisting methods for time-series forecasting can be roughly grouped into two categories 1 . Classical time-series models serve as a reliable workhorse for time-series forecasting (Box et al. 2015;Ray 1990;Seeger et al. 2017;Seeger, Salinas, and Flunkert 2016), and deep learning techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants (Hochreiter and Schmidhuber 1997;Li et al. 2018;. Our proposed Informer holds the encoder-decoder architecture while targeting the LSTF problem. Please refer to Fig.(2) for an overview and the following sections for details.\n\n\nEfficient Self-attention Mechanism\n\nThe canonical self-attention in (Vaswani et al. 2017) is defined on receiving the tuple input (query, key, value) and performs the scaled dot-product as A(Q,\nK, V) = Softmax( QK \u221a d )V, where Q \u2208 R L Q \u00d7d , K \u2208 R L K \u00d7d , V \u2208 R L V \u00d7d and d is the input dimension.\nTo further discuss the self-attention mechanism, let q i , k i , v i stand for the i-th row in Q, K, V respectively. Following the formulation in (Tsai et al. 2019), the i-th query's attention is defined as a kernel smoother in a probability form:\nA(q i , K, V) = j k(q i , k j ) l k(q i , k l ) v j = E p(kj |qi) [v j ] , (1) where p(k j |q i ) = k(qi,kj ) l k(qi,k l ) and k(q i , k j ) selects the asymmetric exponential kernel exp( qik j \u221a d ).\nThe selfattention combines the values and acquires outputs based on computing the probability p(k j |q i ). It requires the quadratic times dot-product computation and O(L Q L K ) memory usage, which is the major drawback in enhancing prediction capacity.\n\nSome previous attempts have revealed that the distribution of self-attention probability has potential sparsity, and they have designed some \"selective\" counting strategies on all p(k j |q i ) without significantly affecting performance. The Sparse Transformer (Child et al. 2019) incorporates both the row outputs and column inputs, in which the sparsity arises from the separated spatial correlation. The LogSparse Transformer (Li et al. 2019) notices the cyclical pattern in self-attention and forces each cell to attend to its previous one by an exponential step size. The Longformer (Beltagy, Peters, and Cohan 2020) extend previous two works to more complicated sparse configuration. However, they are limited to theoretical analysis from following heuristic methods and tackle each multi-head self-attention with the same strategy, which narrows its further improvement.\n\nTo motivate our approach, we first perform a qualitative assessment on the learned attention patterns of the canonical self-attention. The \"sparsity\" self-attention score forms a long tail distribution (see Appendix C for details), i.e., a few dot-product pairs contribute to the major attention, and others can be ignored. Then, the next question is how to distinguish them?\n\nQuery Sparsity Measurement From Eq.(1), the i-th query's attention on all the keys are defined as a probability p(k j |q i ) and the output is its composition with values v. The dominant dot-product pairs encourage the corresponding query's attention probability distribution away from the uniform distribution. If p(k j |q i ) is close to a uniform dis-tribution q(k j |q i ) = 1 L K , the self-attention becomes a trivial sum of values V and is redundant to the residential input. Naturally, the \"likeness\" between distribution p and q can be used to distinguish the \"important\" queries. We measure the \"likeness\" through Kullback-Leibler divergence KL(q||p) = ln\nL K l=1 e qik l / \u221a d \u2212 1 L K L K j=1 q i k j / \u221a d \u2212 ln L K .\nDropping the constant, we define the i-th query's sparsity measurement as\nM (q i , K) = ln L K j=1 e q i k j \u221a d \u2212 1 L K L K j=1 q i k j \u221a d ,(2)\nwhere the first term is the Log-Sum-Exp (LSE) of q i on all the keys, and the second term is the arithmetic mean on them. If the i-th query gains a larger M (q i , K), its attention probability p is more \"diverse\" and has a high chance to contain the dominate dot-product pairs in the header field of the long tail self-attention distribution.\n\nProbSparse Self-attention Based on the proposed measurement, we have the ProbSparse Self-attention by allowing each key only to attend to the u dominant queries\nA(Q, K, V) = Softmax( QK \u221a d )V ,(3)\nwhere Q is a sparse matrix of the same size of q and it only contains the Top-u queries under the sparsity measurement M (q, K). Controlled by a constant sampling factor c, we set u = c \u00b7 ln L Q , which makes the ProbSparse selfattention only need to calculate O(ln L Q ) dot-product for each query-key lookup and the layer memory usage maintains O(L K ln L Q ). However, the traversing of all queries for the measurement M (q i , K) requires calculating each dot-product pairs, i.e. quadratically O(L Q L K ), and the LSE operation has the potential numerical stability issue. Motivated by this, we proposed an approximation to the query sparsity measurement. Lemma 1. For each query q i \u2208 R d and k j \u2208 R d in the keys set K, we have the bounds as ln\nL K \u2264 M (q i , K) \u2264 max j { qik j \u221a d } \u2212 1 L K L K j=1 { qik j \u221a d } + ln L K . When q i \u2208 K,\nit also holds.\n\nFrom the Lemma 1 (proof is given in Appendix D.1), we propose the max-mean measurement as\nM (q i , K) = max j { q i k j \u221a d } \u2212 1 L K L K j=1 q i k j \u221a d .(4)\nThe order of Top-u holds in the boundary relaxation with Proposition 1 (refers proof in Appendix D.2). Under the long tail distribution, we only need randomly sample U = L Q ln L K dot-product pairs to calculate the M (q i , K), i.e. filling other pairs with zero. We select sparse Top-u from them as Q. The max-operator in M (q i , K) is less sensitive to zero values and is numerical stable. In practice, the input length of queries and keys are typically equivalent, i.e L Q = L K = L such that the total ProbSparse self-attention time complexity and space complexity are O(L ln L). (2) the upper stack is the main stack, which receives the whole input sequence, while the second stacks take half slices of the input; (3) the red layers are dot product matrixes of self-attention mechanism, and it gets cascade decrease by applying self-attention distilling on each layer; (4) concate the 2 stack's feature map as the encoder's output.\nProposition 1. Assuming that k j \u223c N (\u00b5, \u03a3) and we let qk i denote set {(q i k j )/ \u221a d | j = 1, . . . , L K }, then \u2200M m = max i M (q i , K) there exist \u03ba > 0 such that: in the interval \u2200q 1 , q 2 \u2208 {q|M (q, K) \u2208 [M m , M m \u2212 \u03ba)}, if M (q 1 , K) > M (q 2 , K) and Var(qk 1 ) > Var(qk 2 ), we have high prob- ability that M (q 1 , K) > M (q 2 , K).\nTo be simplify, an estimation of the probability is given in the proof.\n\n\nEncoder: Allowing for processing longer sequential inputs under the memory usage limitation\n\nThe encoder is designed to extract the robust long-range dependency of long sequential inputs. After the input representation, the t-th sequence input X t has been shaped into a matrix X t feed en \u2208 R Lx\u00d7dmodel . We give a sketch of the encoder in Fig.(3) for clarity.\n\nSelf-attention Distilling As the natural consequence of the ProbSparse Self-attention mechanism, the encoder's feature map have redundant combinations of value V. We use the distilling operation to privilege the superior ones with dominating features and make a focused self-attention feature map in the next layer. It trims the input's time dimension sharply, seeing the n-heads weights matrix (overlapping red squares) of Attention blocks in Fig.(3). Inspired by the dilated convolution (Yu, Koltun, and Funkhouser 2017;Gupta and Rush 2017), our \"distilling\" procedure forwards from j-th layer into (j + 1)-th layer as\nX t j+1 = MaxPool ELU( Conv1d( [X t j ] AB ) ) ,(5)\nwhere [\u00b7] AB contains the Multi-head ProbSparse selfattention and the essential operations in attention block, and Conv1d(\u00b7) performs an 1-D convolutional filters (kernel width=3) on time dimension with the ELU(\u00b7) activation function (Clevert, Unterthiner, and Hochreiter 2016). We add a max-pooling layer with stride 2 and down-sample X t into its half slice after stacking a layer, which reduces the whole memory usage to be O((2 \u2212 )L log L), where is a small number. To enhance the robustness of the distilling operation, we build halving replicas of the main stack and progressively decrease the number of self-attention distilling layers by dropping one layer at a time, like a pyramid in Fig.(3), such that their output dimension is aligned. Thus, we concatenate all the stacks' outputs and have the final hidden representation of encoder.\n\nDecoder: Generating long sequential outputs through one forward procedure\n\nWe use a standard decoder structure (Vaswani et al. 2017) in Fig.(2), and it is composed of a stack of 2 identical multihead attention layers. However, the generative inference is employed to alleviate the speed plunge in long prediction. We feed the decoder with following vectors as\nX t feed de = Concat(X t token , X t 0 ) \u2208 R (Ltoken+Ly)\u00d7dmodel ,(6)\nwhere X t token \u2208 R Ltoken\u00d7dmodel is the start token, X t 0 \u2208 R Ly\u00d7dmodel is a placeholder for the target sequence (set scalar as 0). Masked multi-head attention is applied in the ProbSparse self-attention computing by setting masked dotproducts to \u2212\u221e. It prevents each position from attending to coming positions, which avoids auto-regressive. A fully connected layer acquires the final output, and its outsize d y depends on whether we are performing a univariate forecasting or a multivariate one.\n\nGenerative Inference Start token is an efficient technique in NLP's \"dynamic decoding\" (Devlin et al. 2018), and we extend it into a generative way. Instead of choosing a specific flag as the token, we sample a L token long sequence in the input sequence, which is an earlier slice before the output sequence. Take predicting 168 points as an example (7-day temperature prediction) in Fig.(2(b)), we will take the known 5 days before the target sequence as \"starttoken\", and feed the generative-style inference decoder with X feed de = {X 5d , X 0 }. The X 0 contains target sequence's time stamp, i.e. the context at the target week. Note that our proposed decoder predicts all the outputs by one forward procedure and is free from the time consuming \"dynamic decoding\" transaction in the trivial encoder-decoder architecture. A detailed performance comparison is given in the computation efficiency section.\n\nLoss function We choose the MSE loss function on prediction w.r.t the target sequences, and the loss is propagated back from the decoder's outputs across the entire model.\n\n\nExperiment Datasets\n\nWe empirically perform experiments on four datasets, including 2 collected real-world datasets for LSTF and 2 public benchmark datasets.\n\nETT (Electricity Transformer Temperature) 2 : The ETT is a crucial indicator in the electric power long-term deployment. We collected 2 years data from two separated counties in China. To explorer the granularity on the LSTF problem, we create separate datasets as {ETTh 1 , ETTh 2 } for 1-hourlevel and ETTm 1 for 15-minutes-level. Each data point consists of the target value \"oil temperature\" and 6 power load features. The train/val/test is 12/4/4 months.\n\nECL (Electricity Consuming Load) 3 : It collects the electricity consumption (Kwh) of 321 clients. Due to the missing data (Li et al. 2019), we convert the dataset into hourly consumption of 2 years and set 'MT 320' as the target value. The train/val/test is 15/3/4 months.\n\nWeather 4 : This dataset contains local climatological data for nearly 1,600 U.S. locations, 4 years from 2010 to 2013, where data points are collected every 1 hour. Each data point consists of the target value \"wet bulb\" and 11 climate features. The train/val/test is 28/10/10 months.\n\n\nExperimental Details\n\nWe briefly summarize basics, and more information on network components and setups are given in Appendix E.\n\nBaselines: The details of network components are given in Appendix E.1. We have selected 5 time-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet (Taylor and Letham 2018), LSTMa (Bahdanau, Cho, and Bengio 2015) and LSTnet (Lai et al. 2018) and DeepAR . To better explore the ProbSparse selfattention's performance in our proposed Informer, we incorporate the canonical self-attention variant (Informer \u2020 ), the efficient variant Reformer (Kitaev, Kaiser, and Levskaya 1 n n i=1 (y \u2212\u0177) 2 and MAE = 1 n n i=1 |y \u2212\u0177| on each prediction window (averaging for multivariate prediction), and rolling the whole set with stride = 1. Platform: All models were training/testing on a single Nvidia V100 32GB GPU. Table 1 and Table 2 summarize the univariate/multivariate evaluation results of all the methods on 4 datasets. We gradually prolong the prediction horizon as a higher requirement of prediction capacity. To claim a fair comparison, we have precisely controlled the problem setting to make LSTF is tractable on one single GPU for every method. The best results are highlighted in boldface.\n\n\nResults and Analysis\n\nUnivariate Time-series Forecasting Under this setting, each method attains predictions in a single variable over time. From Table 1, we observe that: (1) The proposed model Informer greatly improves the inference performance (wining-counts in the last column) across all datasets, and their predict error rises smoothly and slowly within the growing prediction horizon. That demonstrates the success of Informer in enhancing the prediction capacity in the LSTF problem.\n\n(2) The Informer beats its canonical degradation Informer \u2020 mostly in wining-counts, i.e., 28>14, which supports the query sparsity assumption in providing a comparable attention feature map. Our proposed method also outperforms the most related work LogTrans and Reformer. We note that the Reformer keeps dynamic decoding and performs poorly in LSTF, while other methods benefit from the generative style decoder as nonautoregressive predictors.\n\n(3) The Informer model shows significantly better results than recurrent neural networks LSTMa. Our method has a MSE decrease of 41.5% (at 168), 60.7% (at 336) and 60.7% (at 720). This reveals a shorter network path in the self-attention mechanism acquires better prediction capacity than the RNN-based models. (4) Our proposed method achieves better results than DeepAR, ARIMA and Prophet on MSE by decreasing 20.9% (at 168), 61.2% (at 336), and 51.3% (at 720) in average. On the ECL dataset, DeepAR performs better on shorter horizons (\u2264 336), and our method surpasses on longer horizons. We attribute this to a specific example, in which the effectiveness of prediction capacity is reflected with the problem scalability.\n\nMultivariate Time-series Forecasting Within this setting, some univariate methods are inappropriate, and LSTnet   \n\n\nParameter Sensitivity\n\nWe perform the sensitivity analysis of the proposed Informer model on ETTh1 under the univariate setting. Input Length: In Fig.(4(a)), when predicting short sequences (like 48), initially increasing input length of encoder/decoder degrades performance, but further increasing causes the MSE to drop because it brings repeat short-term patterns. However, the MSE gets lower with longer inputs in predicting  long sequences (like 168). Because the longer encoder input may contain more dependency, and the longer decoder token has rich local information. Sampling Factor: The sampling factor controls the information bandwidth of ProbSparse self-attention in Eq.(3). We start from the small factor (=3) to large ones, and the general performance increases a little and stabilizes at last in Fig.(4(b)). It verifies our query sparsity assumption that there are redundant dot-product pairs in the self-attention mechanism. We set the sample factor c = 5 (the red line) in practice. The Number of Layer Stacking:\n\nThe replica of Layers is complementary for the self-attention distilling, and we investigate each stack {L, L/2, L/4}'s behavior in Fig.(4(c)). The longer stack is more sensitive to inputs, partly due to receiving more long-term information. Our method's selection (the red line), i.e., combining L and L/4, is the most robust strategy.   \n\n\nAblation Study: How Informer works?\n\nWe also conducted additional experiments on ETTh 1 with ablation consideration. The performance of ProbSparse self-attention mechanism In the overall results Table 1 & 2, we limited the problem setting to make the memory usage feasible for the canonical self-attention. In this study, we compare our methods with LogTrans and Reformer, and thoroughly explore their extreme performance. To isolate the memory efficient problem, we first reduce settings as {batch size=8, heads=8, dim=64}, and maintain other setups in the univariate case. In Table 3, the ProbSparse self-attention shows better performance than the counterparts. The LogTrans gets OOM in extreme cases for its public implementation is the mask of the full-attention, which still has O(L 2 ) memory usage. Our proposed ProbSparse self-attention avoids this from the simplicity brought by the query sparsity assumption in Eq.(4), referring to the pseudo-code in Appendix E.2, and reaches smaller memory usage.\n\nThe performance of self-attention distilling In this study, we use Informer \u2020 as the benchmark to eliminate additional effects of ProbSparse self-attention. The other experimental setup is aligned with the settings of univariate Time-series. From the Table 4, Informer \u2020 has fulfilled all experiments and achieves better performance after taking advantage of long sequence inputs. The comparison method Informer \u2021 removes the distilling operation and reaches OOM with longer inputs (> 720). Regarding the benefits of long sequence inputs in the LSTF problem, we  Figure 5: The total runtime of training/testing phase. \nO(L log L) O(L log L) 1 Transformer O(L 2 ) O(L 2 ) L LogTrans O(L log L) O(L 2 ) 1 Reformer O(L log L) O(L log L) L LSTM O(L) O(L) L 1\nThe LSTnet is hard to have a closed form.\n\nconclude that the self-attention distilling is worth adopting, especially when a longer prediction is required.\n\nThe performance of generative style decoder In this study, we testify the potential value of our decoder in acquiring a \"generative\" results. Unlike the existing methods, the labels and outputs are forced to be aligned in the training and inference, our proposed decoder's predicting relies solely on the time stamp, which can predict with offsets. From Table 5, we can see that the general prediction performance of Informer \u2021 resists with the offset increasing, while the counterpart fails for the dynamic decoding. It proves the decoder's ability to capture individual long-range dependency between arbitrary outputs and avoids error accumulation in the inference.\n\n\nComputation Efficiency\n\nWith the multivariate setting and each method's current finest implement, we perform a rigorous runtime comparison in Fig.(5). During the training phase, the Informer (red line) achieves the best training efficiency among Transformer-based methods. During the testing phase, our methods are much faster than others with the generative style decoding. The comparisons of theoretical time complexity and memory usage are summarized in Table 6, the performance of Informer is aligned with runtime experiments. Note that the LogTrans focus on the self-attention mechanism, and we apply our proposed decoder in LogTrans for a fair comparison (the in Table 6).\n\n\nConclusion\n\nIn this paper, we studied the long-sequence time-series forecasting problem and proposed Informer to predict long sequences. Specifically, we designed the ProbSparse selfattention mechanism and distilling operation to handle the challenges of quadratic time complexity and quadratic memory usage in vanilla Transformer. Also, the carefully designed generative decoder alleviates the limitation of traditional encoder-decoder architecture. The experiments on real-world data demonstrated the effectiveness of Informer for enhancing the prediction capacity in LSTF problem.\n\n\nAppendices Appendix A Related Work\n\nWe provide a literature review of the long sequence timeseries forecasting (LSTF) problem below.\n\nTime-series Forecasting Existing methods for timeseries forecasting can be roughly grouped into two categories: classical models and deep learning based methods. Classical time-series models serve as a reliable workhorse for time-series forecasting, with appealing properties such as interpretability and theoretical guarantees (Box et al. 2015;Ray 1990). Modern extensions include the support for missing data (Seeger et al. 2017) and multiple data types (Seeger, Salinas, and Flunkert 2016). Deep learning based methods mainly develop sequence to sequence prediction paradigm by using RNN and their variants, achieving ground-breaking performance (Hochreiter and Schmidhuber 1997;Li et al. 2018;. Despite the substantial progress, existing algorithms still fail to predict long sequence time series with satisfying accuracy. Typical state-of-the-art approaches (Seeger et al. 2017;Seeger, Salinas, and Flunkert 2016), especially deep-learning methods Qin et al. 2017;Flunkert, Salinas, and Gasthaus 2017;Mukherjee et al. 2018;Wen et al. 2017), remain as a sequence to sequence prediction paradigm with step-by-step process, which have the following limitations: (i) Even though they may achieve accurate prediction for one step forward, they often suffer from accumulated error from the dynamic decoding, resulting in the large errors for LSTF problem (Liu et al. 2019;Qin et al. 2017). The prediction accuracy decays along with the increase of the predicted sequence length. (ii) Due to the problem of vanishing gradient and memory constraint (Sutskever, Vinyals, and Le 2014), most existing methods cannot learn from the past behavior of the whole history of the time-series. In our work, the Informer is designed to address this two limitations.\n\nLong sequence input problem From the above discussion, we refer to the second limitation as to the long sequence time-series input (LSTI) problem. We will explore related works and draw a comparison between our LSTF problem. The researchers truncate / summarize / sample the input sequence to handle a very long sequence in practice, but valuable data may be lost in making accurate predictions. Instead of modifying inputs, Truncated BPTT (Aicher, Foti, and Fox 2019) only uses last time steps to estimate the gradients in weight updates, and Auxiliary Losses (Trinh et al. 2018) enhance the gradients flow by adding auxiliary gradients. Other attempts includes Recurrent Highway Networks (Zilly et al. 2017) and Bootstrapping Regularizer (Cao and Xu 2019). Theses methods try to improve the gradient flows in the recurrent network's long path, but the performance is limited with the sequence length growing in the LSTI problem. CNN-based methods (Stoller et al. 2019;Bai, Kolter, and Koltun 2018) use the convolutional filter to capture the long term dependency, and their receptive fields grow exponentially with the stacking of layers, which hurts the sequence alignment. In the LSTI problem, the main task is to enhance the model's capacity of receiving long sequence in-puts and extract the long-range dependency from these inputs. But the LSTF problem seeks to enhance the model's prediction capacity of forecasting long sequence outputs, which requires establishing the long-range dependency between outputs and inputs. Thus, the above methods are not feasible for LSTF directly.\n\nAttention model Bahdanau et al. firstly proposed the addictive attention (Bahdanau, Cho, and Bengio 2015) to improve the word alignment of the encoder-decoder architecture in the translation task. Then, its variant (Luong, Pham, and Manning 2015) has proposed the widely used location, general, and dot-product attention. The popular selfattention based Transformer (Vaswani et al. 2017) has recently been proposed as new thinking of sequence modeling and has achieved great success, especially in the NLP field. The ability of better sequence alignment has been validated by applying it to translation, speech, music, and image generation. In our work, the Informer takes advantage of its sequence alignment ability and makes it amenable to the LSTF problem.\n\nTransformer-based time-series model The most related works (Song et al. 2018;Ma et al. 2019;Li et al. 2019) all start from a trail on applying Transformer in time-series data and fail in LSTF forecasting as they use the vanilla Transformer. And some other works (Child et al. 2019;Li et al. 2019) noticed the sparsity in self-attention mechanism and we have discussed them in the main context.\n\n\nAppendix B The Uniform Input Representation\n\nThe RNN models (Schuster and Paliwal 1997;Hochreiter and Schmidhuber 1997;Chung et al. 2014;Sutskever, Vinyals, and Le 2014;Qin et al. 2017;Chang et al. 2018) capture the time-series pattern by the recurrent structure itself and barely relies on time stamps. The vanilla transformer (Vaswani et al. 2017;Devlin et al. 2018) uses pointwise self-attention mechanism and the time stamps serve as local positional context. However, in the LSTF problem, the ability to capture long-range independence requires global information like hierarchical time stamps (week, month and year) and agnostic time stamps (holidays, events). These are hardly leveraged in canonical self-attention and consequent query-key mismatches between the encoder and decoder bring underlying degradation on the forecasting performance. We propose a uniform input representation to mitigate the issue, the Fig.(6) gives an intuitive overview. Assuming we have t-th sequence input X t and p types of global time stamps and the feature dimension after input representation is d model . We firstly preserve the local context by using a fixed position embedding, i.e. PE (pos,2j) = sin(pos/(2L x ) 2j/d model ), PE (pos,2j+1) = cos(pos/(2L x ) 2j/d model ), where j \u2208 {1, . . . , d model /2 }. Each global time stamp is employed by a learnable stamp embeddings SE (pos) with limited vocab size (up to 60, namely taking minutes as the finest granularity). That is, the self-attention's similarity computation can have access to global context and the computation consuming is affordable on long inputs. To align the dimension, we project the scalar context x t i into d model -dim vector u t i with 1-D convo-lutional filters (kernel width=3, stride=1). Thus, we have the feeding vector (7) where i \u2208 {1, . . . , L x }, and \u03b1 is the factor balancing the magnitude between the scalar projection and local/global embeddings. We recommend \u03b1 = 1 if the sequence input has been normalized.\nX t feed[i] = \u03b1u t i +PE (Lx\u00d7(t\u22121)+i, ) + p [SE (Lx\u00d7(t\u22121)+i) ]p ,\n\nPosition Embeddings\nP E0 P E1 P E2 P E3 P E4 P E5 P E6 P E7\nProjection u 0 u 1 u 2 u 3 u 4 u 5 u 6 u 7\n\n\nWeek Embeddings\n\nWeek E0\n\nWeek E1\n\n\nMonth Embeddings\n\nMonth E0\n\n\nHoliday Embeddings\n\nWeek E2\n\nWeek E3 \n\n\nE0\n\n\nAppendix C The long tail distribution in self-attention feature map\n\nWe have performed the vanilla Transformer on the ETTh 1 dataset to investigate the distribution of self-attention feature map. We select the attention score of {Head1,Head7} @ Layer1. The blue line in Fig.(7) forms a long tail distribution, i.e. a few dot-product pairs contribute to the major attention and others can be ignored. Proof. For the individual q i , we can relax the discrete keys into the continuous d-dimensional variable, i.e. vec-tor k j . The query sparsity measurement is defined as the M (q i , K) = ln\nL K j=1 e qik j / \u221a d \u2212 1 L K L K j=1 (q i k j / \u221a d)\n. Firstly, we look into the left part of the inequality. For each query q i , the first term of the M (q i , K) becomes the log-sum-exp of the inner-product of a fixed query q i and all the keys , and we can define f i (K) = ln\nL K j=1 e qik j / \u221a d .\nFrom the Eq.(2) in the Log-sum-exp network (Calafiore, Gaubert, and Possieri 2018) and the further analysis, the function f i (K) is convex. Moreover, f i (K) add a linear combination of k j makes the M (q i , K) to be the convex function for a fixed query. Then we can take the derivation of the measurement with respect to the individual vector k j as \u2202M (qi,K)\n\u2202kj = e q i k j / \u221a d L K j=1 e q i k j / \u221a d \u00b7 qi \u221a d \u2212 1 L K \u00b7 qi \u221a d .\nTo reach the minimum value, we let \u2207M (q i ) = 0 and the following condition is acquired as q i k 1 + ln L K = \u00b7 \u00b7 \u00b7 = q i k j + ln L K = \u00b7 \u00b7 \u00b7 = ln L K j=1 e qik j . Naturally, it requires k 1 = k 2 = \u00b7 \u00b7 \u00b7 = k L K , and we have the measurement's minimum as ln L K , i.e.\nM (q i , K) \u2265 ln L K .(8)\nSecondly, we look into the right part of the inequality. If we select the largest inner-product max\nj {q i k j / \u221a d}, it is easy that M (qi, K) = ln L K j=1 e q i k j \u221a d \u2212 1 LK L K j=1 ( qik j \u221a d ) \u2264 ln(LK \u00b7max j { qik j \u221a d })\u2212 1 LK L K j=1 ( qik j \u221a d ) = ln LK +max j { qik j \u221a d }\u2212 1 LK L K j=1 ( qik j \u221a d )\n. (9) Combine the Eq.(14) and Eq.(15), we have the results of Lemma 1. When the key set is the same with the query set, the above discussion also holds.\n\n\nProof of Proposition 1\n\nProof. To make the further discussion simplify, we can note a i,j = q i k T j / \u221a d, thus define the array A i = [a i,1 , \u00b7 \u00b7 \u00b7 , a i,L k ]. Moreover, we denote\n1 L K L K j=1 (q i k j / \u221a d) = mean(A i ), then we can denot\u0113 M (q i , K) = max(A i ) \u2212 mean(A i ), i = 1, 2.\nAs for M (q i , K), we denote each component a i,j = mean(A i ) + \u2206a i,j , j = 1, \u00b7 \u00b7 \u00b7 , L k , then we have the following:\nM (q i , K) = ln L K j=1 e qik j / \u221a d \u2212 1 L K L K j=1 (q i k j / \u221a d)\n= ln(\u03a3 L k j=1 e mean(Ai) e \u2206ai,j ) \u2212 mean(A i ) = ln(e mean(Ai) \u03a3 L k j=1 e \u2206ai,j ) \u2212 mean(A i ) = ln(\u03a3 L k j=1 e \u2206ai,j )\n\n, and it is easy to find \u03a3 L k j=1 \u2206a i,j = 0. We define the function ES(A i ) = \u03a3 L k j=1 exp(\u2206a i,j ), equivalently defines A i = [\u2206a i,1 , \u00b7 \u00b7 \u00b7 , \u2206a i,L k ], and immediately our proposition can be written as the equivalent form:\n\nFor\n\u2200A 1 , A 2 , if 1. max(A 1 ) \u2212 mean(A 1 ) \u2265 max(A 2 ) \u2212 mean(A 2 ) 2. Var(A 1 ) > Var(A 2 )\nThen we rephrase the original conclusion into more general form that ES(A 1 ) > ES(A 2 ) with high probability, and the probability have positive correlation with Var(A 1 ) \u2212 Var(A 2 ).\n\nFurthermore, we consider a fine case,\n\u2200M m = max i M (q i , K) there exist \u03ba > 0 such that in that interval \u2200q i , q j \u2208 {q|M (q, K) \u2208 [M m , M m \u2212 \u03ba)} if max(A 1 ) \u2212 mean(A 1 ) \u2265 max(A 2 ) \u2212 mean(A 2 ) and Var(A 1 ) > Var(A 2 ), we have high probability that M (q 1 , K) > M (q 2 , K),which is equivalent to ES(A 1 ) > ES(A 2 ).\nIn the original proposition, k j \u223c N (\u00b5, \u03a3) follows multivariate Gaussian distribution, which means that k 1 , \u00b7 \u00b7 \u00b7 , k n are I.I.D Gaussian distribution, thus defined by the Wienerkhinchin law of large Numbers, a i,j = q i k T j / \u221a d is onedimension Gaussian distribution with the expectation of 0 if n \u2192 \u221e. So back to our definition, \u2206a 1,m \u223c N (0, \u03c3 2 1 ), \u2206a 2,m \u223c N (0, \u03c3 2 2 ), \u2200m \u2208 1, \u00b7 \u00b7 \u00b7 , L k , and our proposition is equivalent to a lognormal-distribution sum problem.\n\nA lognormal-distribution sum problem is equivalent to approximating the distribution of ES(A 1 ) accurately, whose history is well-introduced in the articles (Dufresne 2008), (Vargasguzman 2005). Approximating lognormality of sums of lognormals is a well-known rule of thumb, and no general PDF function can be given for the sums of lognormals. However, (Romeo, Da Costa, and Bardou 2003) and (Hcine and Bouallegue 2015) pointed out that in most cases, sums of lognormals is still a lognormal distribution, and by applying central limits theorem in (Beaulieu 2011), we can have a good approximation that ES(A 1 ) is a lognormal distribution, and we have E(ES(A 1 )) = ne \u03c3 2 1 2 , Var(ES(A 1 )) = ne \u03c3 2 1 (e \u03c3 2 1 \u2212 1). Equally, E(ES(A 2 )) = ne \u03c3 2 2 2 , Var(ES(A 2 )) = ne \u03c3 2 2 (e \u03c3 2 2 \u2212 1). We denote B 1 = ES(A 1 ), B 2 = ES(A 2 ), and the probability P r(B 1 \u2212 B 2 > 0) is the final result of our proposition in general conditions, with \u03c3 2 1 > \u03c3 2 2 WLOG. The difference of lognormals is still a hard problem to solve.\n\nBy using the theorem given in (Lo 2012), which gives a general approximation of the probability distribution on the sums and difference for the lognormal distribution. Namely S 1 and S 2 are two lognormal stochastic variables obeying the stochastic differential equations dSi Si = \u03c3 i dZ i , i = 1, 2, in which dZ 1,2 presents a standard Weiner process associated with S 1,2 respectively, and \u03c3 2 i = Var (ln S i ), S \u00b1 \u2261 S 1 \u00b1 S 2 ,S \u00b1 0 \u2261 S 10 \u00b1 S 20 . As for the joint probability distribution function P (S 1 , S 2 , t; S 10 , S 20 , t 0 ), the value of S 1 and S 2 at time t > t 0 are provided by their initial value S 10 and S 20 at initial time t 0 . The Weiner process above is equivalent to the lognormal distribution (Weiner and Solbrig 1984), and the conclusion below is written in general form containing both the sum and difference of lognormal distribution approximation denoting \u00b1 for sum + and difference \u2212 respectively.\n\nIn boundary condition P \u00b1 S \u00b1 , t; S 10 , S 20 , t 0 \u2212\u2192 t = \u03b4 S 10 \u00b1 S 20 \u2212 S \u00b1 , their closed-form probability distribution functions are given by\nf LN S \u00b1 , t;S \u00b1 0 , t 0 = 1 S \u00b1 2\u03c0\u03c3 2 \u00b1 (t \u2212 t 0 ) \u00b7 exp \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u2212 ln S + /S + 0 + (1/2)\u03c3 2 \u00b1 (t \u2212 t 0 ) 2 2\u03c3 2 \u00b1 (t \u2212 t 0 ) \uf8fc \uf8f4 \uf8fd \uf8f4 \uf8fe .\nIt is an approximately normal distribution, andS + ,S \u2212 are lognormal random variables,S \u00b1 0 are initial condition in t 0 defined by Weiner process above. (Noticed that\u03c3 2 \u00b1 (t \u2212 t 0 ) should be small to make this approximation valid.In our simulation experiment, we set t \u2212 t 0 = 1 WLOG.) Since\nS \u2212 0 = (S 10 \u2212 S 20 ) + \u03c3 2 \u2212 \u03c3 2 1 \u2212 \u03c3 2 2 (S 10 + S 20 ), and\u03c3 \u2212 = \u03c3 2 1 \u2212 \u03c3 2 2 / (2\u03c3 \u2212 ) \u03c3 \u2212 = \u03c3 2 1 + \u03c3 2 2\nNoticed that E(B 1 ) > E(B 2 ), Var(B 1 ) > Var(B 2 ), the mean value and the variance of the approximate normal distribution shows positive correlation with \u03c3 2 1 \u2212 \u03c3 2 2 .Besides, the closed-form PDF f LN S \u00b1 , t;S \u00b1 0 , t 0 also show positive correlation with \u03c3 2 1 \u2212 \u03c3 2 2 . Due to the limitation of \u03c3 2 \u00b1 (t \u2212 t 0 ) should be small enough, such positive correlation is not significant in our illustrative numerical experiment.\n\nBy using Lie-Trotter Operator Splitting Method in (Lo 2012), we can give illustrative numeral examples for the distribution of B 1 \u2212 B 2 ,in which the parameters are well chosen to fit for our top-u approximation in actual LLLT experiments. Figure shows that it is of high probability that when \u03c3 2 1 > \u03c3 2 2 , the inequality holds that B 1 > B 2 ,\nES(A 1 ) > ES(A 2 ).\nFinishing prooving our proposition in general conditions, we can consider a more specific condition that if q 1 , q 2 \u2208 {q|M (q, K) \u2208 [M m , M m \u2212 \u03ba)},the proposition still holds with high probability.\n\nFirst, we have M (q 1 , k) = ln(B 1 ) > (M m \u2212 \u03ba) holds for \u2200q 1 , q 2 in this interval. Since we have proved that E(B 1 )) = ne \u03c3 2 1 2 , we can conclude that \u2200q i in the given interval,\u2203\u03b1, \u03c3 2 i > \u03b1, i = 1, 2. Since we have S \u2212 0 = (S 10 \u2212 S 20 ) + positive correlation with \u03c3 2 1 + \u03c3 2 2 > 2\u03b1, and positive correlation with \u03c3 2 1 \u2212 \u03c3 2 2 . So due to the nature of the approximate normal distribution PDF, if \u03c3 2 1 > \u03c3 2 2 WLOG, P r(M (q 1 , k) > M (q 2 , k)) \u2248 \u03a6( S \u2212 0 \u03c3\u2212 ) also shows positive correlation with \u03c3 2 1 + \u03c3 2 2 > 2\u03b1. We give an illustrative numerical examples of the approximation above in Fig.(8). In our actual LTTnet experiment, we choose Top-k of A 1 , A 2 , not the whole set.Actually, we can make a naive assumption that in choosing top \u2212 1 4 L k variables of A 1 , A 2 denoted as A 1 , A 2 ,the variation \u03c3 1 , \u03c3 2 don't change significantly, but the expectation E(A 1 ), E(A 2 ) ascends obviously, which leads to initial condition S 10 , S 20 ascends significantly, since the initial condition will be sampled from top \u2212 1 4 L k variables, not the whole set.\n\nIn our actual LTTnet experiment, we set U , namely choosing around top \u2212 1 4 L k of A 1 and A 2 , it is guaranteed that with over 99% probability that in the [M m , M m \u2212 \u03ba) interval, as shown in the black curve of Fig.(8). Typically the condition 2 can be relaxed, and we can believe that if q 1 , q 2 fits the condition 1 in our proposition, we have M (q 1 , K) > M (q 2 , K).\n\n\nAppendix E Reproducibility\n\n\nDetails of the experiments\n\nThe details of proposed Informer model is summarized in Table 7. For the ProbSparse self-attention mechanism, we let d=32, n=16 and add residual connections, a positionwise feed-forward network layer (inner-layer dimension is 2048) and a dropout layer (p = 0.1) likewise. Note that we preserves 10% validation data for each dataset, so all the experiments are conducted over 5 random train/val shifting selection along time and the results are averaged over the 5 runs. All the datasets are performed standardization such that the mean of variable is 0 and the standard deviation is 1.\n\n\nImplement of the ProbSparse self-attention\n\nWe have implemented the ProbSparse self-attention in Python 3.6 with Pytorch 1.0. The pseudo-code is given in Algo.\n\n(1). The source code is available at https://github.com/ zhouhaoyi/Informer2020. All the procedure can be highly efficient vector operation and maintains logarithmic total memory usage. The masked version can be achieved by applying positional mask on step 6 and using cmusum(\u00b7) in mean(\u00b7) of step 7.\n\n\nAlgorithm 1 ProbSparse self-attention\n\nInput: Tensor Q \u2208 R m\u00d7d , K \u2208 R n\u00d7d , V \u2208 R n\u00d7d 1: initialize: set hyperparameter c, u = c ln m and U = m ln n 2: randomly select U dot-product pairs from K asK 3: set the sample scoreS = QK 4: compute the measurement M = max(S) \u2212 mean(S) by row 5: set Top-u queries under M asQ 6: set S1 = softmax(QK / \u221a d) \u00b7 V 7: set S0 = mean(V) 8: set S = {S1, S0} by their original rows accordingly Output: self-attention feature map S.  dataset, and {24, 48, 96, 192, 288, 480, 672} for the ETTm dataset. In the experiment, the decoder's start token is a segment truncated from the encoder's input sequence, so the length of decoder's start token must be less than the length of encoder's input. The RNN-based methods perform a dynamic decoding with left shifting on the prediction windows. Our proposed methods Informer-series and LogTrans (our decoder) perform non-dynamic decoding.\n\n\nThe hyperparameter tuning range\n\nFigure 1 :\n1(a) Short sequence predictions only reveal the near future. (b) Long sequence time-series forecasting can cover an extended period for better policy-planning and investment-protecting. (c)\n\nFigure 3 :\n3The architecture of Informer's encoder. (1) each horizontal stack stands for an individual one of the encoder replicas inFig.(2);\n\nFigure 4 :\n4The parameters' sensitivity of Informer.\n\nFigure 6 :\n6The input representation of Informer. The inputs's embedding consists of three separate parts, a scalar projection, the local time stamp (Position) and global time stamp embeddings (Minutes, Hours, Week, Month, Holiday etc.).\n\nFigure 7 :\n7The Softmax scores in the self-attention from a 4-layer canonical Transformer trained on ETTh 1 dataset. Appendix D Details of the proof Proof of Lemma 1\n\n(Figure 8 :\n8S 10 + S 20 ), which also shows Probability Density verses S 1 \u2212 S 2 for the approximate shifted lognormal\n\nTable 1 :\n1Univariate long sequence time-series forecasting results on four datasets (five cases)Methods Metric \nETTh1 \nETTh2 \nETTm1 \nWeather \nECL \ncount \n24 \n48 \n168 336 720 \n24 \n48 \n168 336 720 \n24 \n48 \n96 \n288 672 \n24 \n48 \n168 336 720 \n48 \n168 336 720 \n960 \n\nInformer \nMSE 0.062 0.108 0.146 0.208 0.193 0.079 0.103 0.143 0.171 0.184 0.051 0.092 0.119 0.181 0.204 0.107 0.164 0.226 0.241 0.259 0.335 0.408 0.451 0.466 0.470 28 \nMAE 0.178 0.245 0.294 0.363 0.365 0.206 0.240 0.296 0.327 0.339 0.153 0.217 0.249 0.320 0.345 0.223 0.282 0.338 0.352 0.367 0.423 0.466 0.488 0.499 0.520 \n\n\n\nTable 2 :\n2Multivariate long sequence time-series forecasting results on four datasets (five cases) and such phenomena can be caused by the anisotropy in feature dimensions' prediction capacity. It beyonds this paper's scope, and we explore it in future work.LSTF with Granularity Consideration We perform an additional comparison trying to explore the performance with various granularities. The sequences {96, 288, 672} of ETTm 1 (minutes-level) are aligned with {24, 48, 168} of ETTh 1 (hour-level). The proposed Informer outperforms other baselines even if the sequences are at different granularity levels.Methods Metric \nETTh1 \nETTh2 \nETTm1 \nWeather \nECL \ncount \n24 \n48 \n168 336 720 \n24 \n48 \n168 336 720 \n24 \n48 \n96 \n288 672 \n24 \n48 \n168 336 720 \n48 \n168 336 720 960 \n\nInformer \nMSE 0.509 0.551 0.878 0.884 0.941 0.446 0.934 1.512 1.665 2.340 0.325 0.472 0.642 1.219 1.651 0.353 0.464 0.592 0.623 0.685 0.269 0.300 0.311 0.308 0.328 32 \nMAE 0.523 0.563 0.722 0.753 0.768 0.523 0.733 0.996 1.035 1.209 0.440 0.537 0.626 0.871 1.002 0.381 0.455 0.531 0.546 0.575 0.351 0.376 0.385 0.385 0.406 \n\nInformer  \u2020 MSE 0.550 0.602 0.893 0.836 0.981 0.683 0.977 1.873 1.374 2.493 0.324 0.446 0.651 1.342 1.661 0.355 0.471 0.613 0.626 0.680 0.269 0.281 0.309 0.314 0.356 12 \nMAE 0.551 0.581 0.733 0.729 0.779 0.637 0.793 1.094 0.935 1.253 0.440 0.508 0.616 0.927 1.001 0.383 0.456 0.544 0.548 0.569 0.351 0.366 0.383 0.388 0.394 \n\nLogTrans \nMSE 0.656 0.670 0.888 0.942 1.109 0.726 1.728 3.944 3.711 2.817 0.341 0.495 0.674 1.728 1.865 0.365 0.496 0.649 0.666 0.741 0.267 0.290 0.305 0.311 0.333 2 \nMAE 0.600 0.611 0.766 0.766 0.843 0.638 0.944 1.573 1.587 1.356 0.495 0.527 0.674 1.656 1.721 0.405 0.485 0.573 0.584 0.611 0.366 0.382 0.395 0.397 0.413 \n\nReformer \nMSE 0.887 1.159 1.686 1.919 2.177 1.381 1.715 4.484 3.798 5.111 0.598 0.952 1.267 1.632 1.943 0.583 0.633 1.228 1.770 2.548 1.312 1.453 1.507 1.883 1.973 0 \nMAE 0.630 0.750 0.996 1.090 1.218 1.475 1.585 1.650 1.508 1.793 0.489 0.645 0.795 0.886 1.006 0.497 0.556 0.763 0.997 1.407 0.911 0.975 0.978 1.002 1.185 \n\nLSTMa \nMSE 0.536 0.616 1.058 1.152 1.682 1.049 1.331 3.987 3.276 3.711 0.511 1.280 1.195 1.598 2.530 0.476 0.763 0.948 1.497 1.314 0.388 0.492 0.778 1.528 1.343 0 \nMAE 0.528 0.577 0.725 0.794 1.018 0.689 0.805 1.560 1.375 1.520 0.517 0.819 0.785 0.952 1.259 0.464 0.589 0.713 0.889 0.875 0.444 0.498 0.629 0.945 0.886 \n\nLSTnet \nMSE 1.175 1.344 1.865 2.477 1.925 2.632 3.487 1.442 1.372 2.403 1.856 1.909 2.654 1.009 1.681 0.575 0.622 0.676 0.714 0.773 0.279 0.318 0.357 0.442 0.473 4 \nMAE 0.793 0.864 1.092 1.193 1.084 1.337 1.577 2.389 2.429 3.403 1.058 1.085 1.378 1.902 2.701 0.507 0.553 0.585 0.607 0.643 0.337 0.368 0.391 0.433 0.443 \n\nis the state-of-art baseline. On the contrary, our proposed In-\nformer is easy to change from univariate prediction to mul-\ntivariate one by adjusting the final FCN layer. From Table 2, \nwe observe that: (1) The proposed model Informer greatly \noutperforms other methods and the findings 1 & 2 in the uni-\nvariate settings still hold for the multivariate time-series. (2) \nThe Informer model shows better results than RNN-based \nLSTMa and CNN-based LSTnet, and the MSE decreases \n9.5% (at 168), 2.1% (at 336), 13.8% (at 720) in average. \nCompared with the univariate results, the overwhelming per-\nformance is reduced, \n\nTable 3 :\n3Ablation of ProbSparse mechanismPrediction length \n336 \n720 \nEncoder's input \n336 \n720 \n1440 \n720 \n1440 \n2880 \n\nInformer \nMSE \n0.243 \n0.225 \n0.212 \n0.258 \n0.238 \n0.224 \nMAE \n0.487 \n0.404 \n0.381 \n0.503 \n0.399 \n0.387 \n\nInformer  \u2020 \nMSE \n0.214 \n0.205 \n-\n0.235 \n-\n-\nMAE \n0.369 \n0.364 \n-\n0.401 \n-\n-\n\nLogTrans \nMSE \n0.256 \n0.233 \n-\n0.264 \n-\n-\nMAE \n0.496 \n0.412 \n-\n0.523 \n-\n-\n\nReformer \nMSE \n1.848 \n1.832 \n1.817 \n2.094 \n2.055 \n2.032 \nMAE \n1.054 \n1.027 \n1.010 \n1.363 \n1.306 \n1.334 \n\n1 Informer  \u2020 uses the canonical self-attention mechanism. \n2 The '-' indicates failure for out-of-memory. \n\n\n\nTable 4 :\n4Ablation of Self-attention DistillingPrediction length \n336 \n480 \nEncoder's input \n336 \n480 \n720 \n960 1200 336 \n480 \n720 \n960 1200 \n\nInformer  \u2020 MSE 0.201 0.175 0.215 0.185 0.172 0.136 0.213 0.178 0.146 0.121 \nMAE 0.360 0.335 0.366 0.355 0.321 0.282 0.382 0.345 0.296 0.272 \n\nInformer  \u2021 MSE 0.187 0.182 0.177 \n-\n-\n0.208 0.182 0.168 \n-\n-\nMAE 0.330 0.341 0.329 \n-\n-\n0.384 0.337 0.304 \n-\n-\n\n1 Informer  \u2021 removes the self-attention distilling from Informer  \u2020 . \n2 The '-' indicates failure for out-of-memory. \n\n\n\nTable 5 :\n5Ablation of Generative Style Decoder Informer \u00a7 replaces our decoder with dynamic decoding one in Informer \u2021 . 2 The '-' indicates failure for the unacceptable metric results.Prediction length \n336 \n480 \nPrediction offset \n+0 \n+12 \n+24 \n+48 \n+0 \n+48 \n+96 \n+168 \n\nInformer  \u2021 \nMSE 0.101 0.102 0.103 0.103 0.155 0.158 0.160 0.165 \nMAE 0.215 0.218 0.223 0.227 0.317 0.397 0.399 0.406 \n\nInformer  \u00a7 \nMSE 0.152 \n-\n-\n-\n0.462 \n-\n-\n-\nMAE 0.294 \n-\n-\n-\n0.595 \n-\n-\n-\n\n1 \n\nTable 6 :\n6L-related computation statics of each layerMethods \nTraining \nTesting \nTime Complexity \nMemory Usage \nSteps \nInformer \n\n\nTable 7 :\n7The Informer network components in details d = 32)    Add, LayerNorm, Dropout (p = 0.1) Pos-wise FFN (dinner = 2048), GELU Add, LayerNorm, Dropout (p = 0.1)Encoder: \nN \nInputs \n1x3 Conv1d \nEmbedding (d = 512) \n\n4 \n\nProbSparse \nSelf-attention \nBlock \n\nDistilling \n1x3 conv1d, ELU \nMax pooling (stride = 2) \nDecoder: \nN \nInputs \n1x3 Conv1d \nEmbedding (d = 512) \n\n2 \n\nMasked PSB \nadd Mask on Attention Block \n\nSelf-attention \nBlock \n\nMulti-head Attention (h = 8, d = 64) \nAdd, LayerNorm, Dropout (p = 0.1) \nPos-wise FFN (dinner = 2048), GELU \nAdd, LayerNorm, Dropout (p = 0.1) \nFinal: \nOutputs \nFCN (d = dout) \n\n\n\n\nFor all methods, the input length of recurrent component is chosen from{24, 48, 96, 168, 336, 720} for the ETTh1, ETTh2, Weather and Electricity dataset, and chosen from{24, 48, 96, 192, 288, 672} for the ETTm dataset. For LSTMa and DeepAR, the size of hidden states is chosen from {32, 64, 128, 256}. For LSTnet, the hidden dimension of the Recurrent layer and Convolutional layer is chosen from {64, 128, 256} and {32, 64, 128} for Recurrentskip layer, and the skip-length of Recurrent-skip layer is set as 24 for the ETTh1, ETTh2, Weather and ECL dataset, and set as 96 for the ETTm dataset. For Informer, the layer of encoder is chosen from {6, 4, 3, 2} and the layer of decoder is set as 2. The head number of multi-head attention is chosen from {8, 16}, and the dimension of multi-head attention's output is set as 512. The length of encoder's input sequence and decoder's start token is chosen from{24, 48, 96, 168,  336, 480, 720}  for the ETTh1, ETTh2, Weather and ECLFigure 9: The predicts (len=336) of Informer, Informer \u2020 , LogTrans, Reformer, DeepAR, LSTMa, ARIMA and Prophet on the ETTm dataset. The red / blue curves stand for slices of the prediction / ground truth.0 \n\n50 \n100 \n150 \n200 \n250 \n300 \n\n\u22121.75 \n\n\u22121.50 \n\n\u22121.25 \n\n\u22121.00 \n\n\u22120.75 \n\n\u22120.50 \n\n\u22120.25 \n\n0.00 \nETTm1 Gro ndTr th \nInformer \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n\nETTm1 Gro ndTr th \nInformer  \u2020 \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n\nETTm1 Gro ndTr th \nLogTrans \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n\nETTm1 Gro ndTr th \nReformer \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n\nETTm1 Gro ndTr th \nDeepAR \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n\nETTm1 Gro ndTr th \nLSTMa \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n\nETTm1 Gro ndTr th \nARIMA \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n\nETTm1 Gro ndTr th \nProphet \n\n\nDue to the space limitation, a complete related work survey is provided in Appendix A.\nWe collected the ETT dataset and published it at https:// github.com/zhouhaoyi/ETDataset. 3 ECL dataset was acquired at https://archive.ics.uci.edu/ml/ datasets/ElectricityLoadDiagrams20112014 4 Weather dataset was acquired at https://www.ncdc.noaa.gov/ orders/qclcd/ 2019) and the most related work LogSparse self-attention(Li et al. 2019) in the experiments.Hyper-parameter tuning: We conduct grid search over the hyper-parameters and detail ranges are given in Appendix E.3. Informer contains a 3-layer stack and a 2-layer stack (1/4 input) in encoder, 2-layer decoder. Our proposed methods are optimized with Adam optimizer and its learning rate starts from 1e \u22124 , decaying 10 times smaller every 2 epochs and total epochs is 10. We set comparison methods as recommended and the batch size is 32. Setup: The input of each dataset is zero-mean normalized. Under the LSTF settings, we prolong the prediction windows size L y progressively, i.e. {1d, 2d, 7d, 14d, 30d, 40d} in {ETTh, ECL, Weather}, {6h, 12h, 24h, 72h, 168h} in ETTm. Metrics: We used two evaluation metrics, including MSE =\nAppendix F Extra experimental resultsFig.(9) presents a slice of the predicts of 8 models. The most realted work LogTrans and Reformer shows acceptable results. The LSTMa model is not amenable for the long sequence prediction task. The ARIMA and DeepAR can capture the long trend of the long sequences. And the Prophet detects the changing point and fits it with a smooth curve better than the ARIMA and DeepAR. Our proposed model Informer and Informer \u2020 show significantly better results than above methods.Appendix G Computing InfrastructureAll the experiments are conducted on Nvidia Tesla V100 SXM2 GPUs (32GB memory). Other configuration includes 2 * Intel Xeon Gold 6148 CPU, 384GB DDR4 RAM and 2 * 240GB M.2 SSD, which is sufficient for all the baselines.\nAdaptively Truncating Backpropagation Through Time to Control Gradient Bias. C Aicher, N J Foti, E B Fox, arXiv:1905.07473Aicher, C.; Foti, N. J.; and Fox, E. B. 2019. Adaptively Trun- cating Backpropagation Through Time to Control Gradient Bias. arXiv:1905.07473 .\n\nStock price prediction using the ARIMA model. A A Ariyo, A O Adewumi, C K Ayo, The 16th International Conference on Computer Modelling and Simulation. IEEEAriyo, A. A.; Adewumi, A. O.; and Ayo, C. K. 2014. Stock price prediction using the ARIMA model. In The 16th In- ternational Conference on Computer Modelling and Simu- lation, 106-112. IEEE.\n\nNeural Machine Translation by Jointly Learning to Align and Translate. D Bahdanau, K Cho, Y Bengio, Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural Ma- chine Translation by Jointly Learning to Align and Trans- late. In ICLR 2015.\n\nConvolutional sequence modeling revisited. S Bai, J Z Kolter, V Koltun, ICLRBai, S.; Kolter, J. Z.; and Koltun, V. 2018. Convolutional sequence modeling revisited. ICLR .\n\nAn extended limit theorem for correlated lognormal sums. N C Beaulieu, IEEE transactions on communications. 601Beaulieu, N. C. 2011. An extended limit theorem for cor- related lognormal sums. IEEE transactions on communica- tions 60(1): 23-26.\n\nLongformer: The Long-Document Transformer. I Beltagy, M E Peters, A Cohan, CoRR abs/2004.05150Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer: The Long-Document Transformer. CoRR abs/2004.05150.\n\nTime series analysis: forecasting and control. G E Box, G M Jenkins, G C Reinsel, G M Ljung, John Wiley & SonsBox, G. E.; Jenkins, G. M.; Reinsel, G. C.; and Ljung, G. M. 2015. Time series analysis: forecasting and control. John Wiley & Sons.\n\n. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. CoRR abs/2005.14165Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. CoRR abs/2005.14165.\n\nLogsum-exp neural networks and posynomial models for convex and log-log-convex data. G C Calafiore, S Gaubert, C Possieri, CoRR abs/1806.07850Calafiore, G. C.; Gaubert, S.; and Possieri, C. 2018. Log- sum-exp neural networks and posynomial models for convex and log-log-convex data. CoRR abs/1806.07850. URL http: //arxiv.org/abs/1806.07850.\n\nBetter Long-Range Dependency By Bootstrapping A Mutual Information Regularizer. Y Cao, P Xu, arXiv:1905.11978Cao, Y.; and Xu, P. 2019. Better Long-Range Depen- dency By Bootstrapping A Mutual Information Regularizer. arXiv:1905.11978 .\n\nA Memory-Network Based Solution for Multivariate Time-Series Forecasting. Y.-Y Chang, F.-Y Sun, Y.-H Wu, S.-D Lin, arXiv:1809.02105Chang, Y.-Y.; Sun, F.-Y.; Wu, Y.-H.; and Lin, S.-D. 2018. A Memory-Network Based Solution for Multivariate Time- Series Forecasting. arXiv:1809.02105 .\n\nR Child, S Gray, A Radford, I Sutskever, arXiv:1904.10509Generating Long Sequences with Sparse Transformers. Child, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019. Generating Long Sequences with Sparse Transformers. arXiv:1904.10509 .\n\nOn the Properties of Neural Machine Translation: Encoder-Decoder Approaches. K Cho, B Van Merrienboer, D Bahdanau, Y Bengio, Proceedings of SSST@EMNLP 2014. SSST@EMNLP 2014Cho, K.; van Merrienboer, B.; Bahdanau, D.; and Bengio, Y. 2014. On the Properties of Neural Machine Trans- lation: Encoder-Decoder Approaches. In Proceedings of SSST@EMNLP 2014, 103-111.\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, C Gulcehre, K Cho, Y Bengio, arXiv:1412.3555Chung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y. 2014. Em- pirical evaluation of gated recurrent neural networks on se- quence modeling. arXiv:1412.3555 .\n\nFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). D Clevert, T Unterthiner, S Hochreiter, Clevert, D.; Unterthiner, T.; and Hochreiter, S. 2016. Fast and Accurate Deep Network Learning by Exponential Lin- ear Units (ELUs). In ICLR 2016.\n\nTransformer-xl: Attentive language models beyond a fixed-length context. Z Dai, Z Yang, Y Yang, J Carbonell, Q V Le, R Salakhutdinov, arXiv:1901.02860Dai, Z.; Yang, Z.; Yang, Y.; Carbonell, J.; Le, Q. V.; and Salakhutdinov, R. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv:1901.02860 .\n\nJ Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv:1810.04805 .\n\nSums of lognormals. D Dufresne, Actuarial Research Conference. Dufresne, D. 2008. Sums of lognormals. In Actuarial Re- search Conference, 1-6.\n\nV Flunkert, D Salinas, J Gasthaus, arXiv:1704.04110Probabilistic forecasting with autoregressive recurrent networks. Flunkert, V.; Salinas, D.; and Gasthaus, J. 2017. DeepAR: Probabilistic forecasting with autoregressive recurrent net- works. arXiv:1704.04110 .\n\nDilated convolutions for modeling long-distance genomic dependencies. A Gupta, A M Rush, arXiv:1710.01278Gupta, A.; and Rush, A. M. 2017. Dilated convolu- tions for modeling long-distance genomic dependencies. arXiv:1710.01278 .\n\nOn the approximation of the sum of lognormals by a log skew normal distribution. M B Hcine, R Bouallegue, arXiv:1502.03619arXiv preprintHcine, M. B.; and Bouallegue, R. 2015. On the approxima- tion of the sum of lognormals by a log skew normal distri- bution. arXiv preprint arXiv:1502.03619 .\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 98Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term memory. Neural computation 9(8): 1735-1780.\n\nReformer: The Efficient Transformer. N Kitaev, L Kaiser, A Levskaya, ICLR. Kitaev, N.; Kaiser, L.; and Levskaya, A. 2019. Reformer: The Efficient Transformer. In ICLR.\n\nModeling long-and short-term temporal patterns with deep neural networks. G Lai, W.-C Chang, Y Yang, H Liu, In ACM. ACMLai, G.; Chang, W.-C.; Yang, Y.; and Liu, H. 2018. Model- ing long-and short-term temporal patterns with deep neural networks. In ACM SIGIR 2018, 95-104. ACM.\n\nS Li, X Jin, Y Xuan, X Zhou, W Chen, Y.-X Wang, Yan , X , arXiv:1907.00235Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting. Li, S.; Jin, X.; Xuan, Y.; Zhou, X.; Chen, W.; Wang, Y.-X.; and Yan, X. 2019. Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Fore- casting. arXiv:1907.00235 .\n\nDiffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting. Y Li, R Yu, C Shahabi, Y Liu, Li, Y.; Yu, R.; Shahabi, C.; and Liu, Y. 2018. Diffusion Con- volutional Recurrent Neural Network: Data-Driven Traffic Forecasting. In ICLR 2018.\n\nDSTP-RNN: a dual-stage two-phase attention-based recurrent neural networks for long-term and multivariate time series prediction. Y Liu, C Gong, L Yang, Y Chen, CoRR abs/1904.07464Liu, Y.; Gong, C.; Yang, L.; and Chen, Y. 2019. DSTP-RNN: a dual-stage two-phase attention-based recurrent neural net- works for long-term and multivariate time series prediction. CoRR abs/1904.07464.\n\nThe sum and difference of two lognormal random variables. C.-F Lo, Journal of Applied Mathematics. Lo, C.-F. 2012. The sum and difference of two lognormal random variables. Journal of Applied Mathematics 2012.\n\nEffective Approaches to Attention-based Neural Machine Translation. T Luong, H Pham, C D Manning, L M\u00e0rquez, C Callison-Burch, J Su, D Pighin, Y Marton, 10.18653/v1/d15-1166The Association for Computational LinguisticsLuong, T.; Pham, H.; and Manning, C. D. 2015. Effective Approaches to Attention-based Neural Machine Transla- tion. In M\u00e0rquez, L.; Callison-Burch, C.; Su, J.; Pighin, D.; and Marton, Y., eds., EMNLP, 1412-1421. The Association for Computational Linguistics. doi:10.18653/v1/d15-1166. URL https://doi.org/10.18653/v1/d15-1166.\n\nCDSA: Cross-Dimensional Self-Attention for Multivariate. J Ma, Z Shou, A Zareian, H Mansour, A Vetro, S.-F Chang, arXiv:1905.09904Geo-tagged Time Series Imputation. Ma, J.; Shou, Z.; Zareian, A.; Mansour, H.; Vetro, A.; and Chang, S.-F. 2019. CDSA: Cross-Dimensional Self- Attention for Multivariate, Geo-tagged Time Series Impu- tation. arXiv:1905.09904 .\n\nFUNNEL: automatic mining of spatially coevolving epidemics. Y Matsubara, Y Sakurai, W G Van Panhuis, C Faloutsos, ACM SIGKDD 2014. Matsubara, Y.; Sakurai, Y.; van Panhuis, W. G.; and Falout- sos, C. 2014. FUNNEL: automatic mining of spatially coe- volving epidemics. In ACM SIGKDD 2014, 105-114.\n\nArmdn: Associative and recurrent mixture density networks for eretail demand forecasting. S Mukherjee, D Shankar, A Ghosh, N Tathawadekar, P Kompalli, S Sarawagi, K Chaudhury, arXiv:1803.03800Mukherjee, S.; Shankar, D.; Ghosh, A.; Tathawadekar, N.; Kompalli, P.; Sarawagi, S.; and Chaudhury, K. 2018. Ar- mdn: Associative and recurrent mixture density networks for eretail demand forecasting. arXiv:1803.03800 .\n\nOptimal multi-scale patterns in time series streams. S Papadimitriou, P Yu, ACM SIGMOD 2006. ACMPapadimitriou, S.; and Yu, P. 2006. Optimal multi-scale pat- terns in time series streams. In ACM SIGMOD 2006, 647- 658. ACM.\n\nA Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction. Y Qin, D Song, H Chen, W Cheng, G Jiang, G W Cottrell, Qin, Y.; Song, D.; Chen, H.; Cheng, W.; Jiang, G.; and Cot- trell, G. W. 2017. A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction. In IJCAI 2017, 2627-2633.\n\nBlockwise Self-Attention for Long Document Understanding. J Qiu, H Ma, O Levy, S W Yih, S Wang, J Tang, arXiv:1911.02972Qiu, J.; Ma, H.; Levy, O.; Yih, S. W.-t.; Wang, S.; and Tang, J. 2019. Blockwise Self-Attention for Long Document Un- derstanding. arXiv:1911.02972 .\n\nCompressive transformers for long-range sequence modelling. J W Rae, A Potapenko, S M Jayakumar, T P Lillicrap, arXiv:1911.05507Rae, J. W.; Potapenko, A.; Jayakumar, S. M.; and Lillicrap, T. P. 2019. Compressive transformers for long-range se- quence modelling. arXiv:1911.05507 .\n\nTime series: theory and methods. W Ray, Journal of the Royal Statistical Society: Series A (Statistics in Society). 1533Ray, W. 1990. Time series: theory and methods. Journal of the Royal Statistical Society: Series A (Statistics in Society) 153(3): 400-400.\n\nBroad distribution effects in sums of lognormal random variables. M Romeo, V Da Costa, F Bardou, The European Physical Journal B-Condensed Matter and Complex Systems. 324Romeo, M.; Da Costa, V.; and Bardou, F. 2003. Broad distri- bution effects in sums of lognormal random variables. The European Physical Journal B-Condensed Matter and Com- plex Systems 32(4): 513-525.\n\nBidirectional recurrent neural networks. M Schuster, K K Paliwal, IEEE Transactions on Signal Processing. 4511Schuster, M.; and Paliwal, K. K. 1997. Bidirectional recur- rent neural networks. IEEE Transactions on Signal Process- ing 45(11): 2673-2681.\n\nApproximate bayesian inference in linear state space models for intermittent demand forecasting at scale. M Seeger, S Rangapuram, Y Wang, D Salinas, J Gasthaus, T Januschowski, V Flunkert, arXiv:1709.07638Seeger, M.; Rangapuram, S.; Wang, Y.; Salinas, D.; Gasthaus, J.; Januschowski, T.; and Flunkert, V. 2017. Approximate bayesian inference in linear state space models for intermittent demand forecasting at scale. arXiv:1709.07638 .\n\nBayesian intermittent demand forecasting for large inventories. M W Seeger, D Salinas, V Flunkert, NIPS. Seeger, M. W.; Salinas, D.; and Flunkert, V. 2016. Bayesian intermittent demand forecasting for large inventories. In NIPS, 4646-4654.\n\nAttend and diagnose: Clinical time series analysis using attention models. H Song, D Rajan, J J Thiagarajan, A Spanias, AAAI. Song, H.; Rajan, D.; Thiagarajan, J. J.; and Spanias, A. 2018. Attend and diagnose: Clinical time series analysis using at- tention models. In AAAI 2018.\n\nSeq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence Modelling. D Stoller, M Tian, S Ewert, S Dixon, arXiv:1911.06393Stoller, D.; Tian, M.; Ewert, S.; and Dixon, S. 2019. Seq- U-Net: A One-Dimensional Causal U-Net for Efficient Se- quence Modelling. arXiv:1911.06393 .\n\nSequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, NIPS. Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to sequence learning with neural networks. In NIPS, 3104- 3112.\n\nForecasting at scale. S J Taylor, B Letham, The American Statistician. 721Taylor, S. J.; and Letham, B. 2018. Forecasting at scale. The American Statistician 72(1): 37-45.\n\nT H Trinh, A M Dai, M.-T Luong, Q V Le, arXiv:1803.00144Learning longer-term dependencies in rnns with auxiliary losses. arXiv preprintTrinh, T. H.; Dai, A. M.; Luong, M.-T.; and Le, Q. V. 2018. Learning longer-term dependencies in rnns with auxiliary losses. arXiv preprint arXiv:1803.00144 .\n\nTransformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel. Y.-H H Tsai, S Bai, M Yamada, L.-P Morency, R Salakhutdinov, ACL 2019. Tsai, Y.-H. H.; Bai, S.; Yamada, M.; Morency, L.-P.; and Salakhutdinov, R. 2019. Transformer Dissection: An Uni- fied Understanding for Transformer's Attention via the Lens of Kernel. In ACL 2019, 4335-4344.\n\nChange of Support of Transformations: Conservation of Lognormality Revisited. J A Vargasguzman, Mathematical Geosciences. 376Vargasguzman, J. A. 2005. Change of Support of Transfor- mations: Conservation of Lognormality Revisited. Mathe- matical Geosciences 37(6): 551-567.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, NIPS. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. At- tention is all you need. In NIPS, 5998-6008.\n\nS Wang, B Li, M Khabsa, H Fang, H Ma, arXiv:2006.04768Linformer: Self-Attention with Linear Complexity. Wang, S.; Li, B.; Khabsa, M.; Fang, H.; and Ma, H. 2020. Linformer: Self-Attention with Linear Complexity. arXiv:2006.04768 .\n\nThe meaning and measurement of size hierarchies in plant populations. J Weiner, O T Solbrig, Oecologia. 613Weiner, J.; and Solbrig, O. T. 1984. The meaning and mea- surement of size hierarchies in plant populations. Oecologia 61(3): 334-336.\n\nR Wen, K Torkkola, B Narayanaswamy, D Madeka, arXiv:1711.11053A multi-horizon quantile recurrent forecaster. Wen, R.; Torkkola, K.; Narayanaswamy, B.; and Madeka, D. 2017. A multi-horizon quantile recurrent forecaster. arXiv:1711.11053 .\n\nDilated residual networks. F Yu, V Koltun, T Funkhouser, CVPR. Yu, F.; Koltun, V.; and Funkhouser, T. 2017. Dilated residual networks. In CVPR, 472-480.\n\nR Yu, S Zheng, A Anandkumar, Y Yue, arXiv:1711.00073Longterm forecasting using tensor-train rnns. Yu, R.; Zheng, S.; Anandkumar, A.; and Yue, Y. 2017. Long- term forecasting using tensor-train rnns. arXiv:1711.00073 .\n\nStatStream: Statistical Monitoring of Thousands of Data Streams in Real Time. Y Zhu, D E Shasha, VLDB 2002. Zhu, Y.; and Shasha, D. E. 2002. StatStream: Statistical Monitoring of Thousands of Data Streams in Real Time. In VLDB 2002, 358-369.\n\nRecurrent highway networks. J G Zilly, R K Srivastava, J Koutn\u00edk, J Schmidhuber, ICML. Zilly, J. G.; Srivastava, R. K.; Koutn\u00edk, J.; and Schmidhuber, J. 2017. Recurrent highway networks. In ICML, 4189-4198.\n", "annotations": {"author": "[{\"end\":116,\"start\":84},{\"end\":143,\"start\":117},{\"end\":176,\"start\":144},{\"end\":210,\"start\":177},{\"end\":243,\"start\":211},{\"end\":294,\"start\":244},{\"end\":401,\"start\":295}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":90},{\"end\":132,\"start\":127},{\"end\":154,\"start\":150},{\"end\":188,\"start\":183},{\"end\":221,\"start\":219},{\"end\":253,\"start\":248},{\"end\":307,\"start\":302}]", "author_first_name": "[{\"end\":89,\"start\":84},{\"end\":126,\"start\":117},{\"end\":149,\"start\":144},{\"end\":182,\"start\":177},{\"end\":218,\"start\":211},{\"end\":247,\"start\":244},{\"end\":301,\"start\":295}]", "author_affiliation": "[{\"end\":115,\"start\":96},{\"end\":142,\"start\":134},{\"end\":175,\"start\":156},{\"end\":209,\"start\":190},{\"end\":242,\"start\":223},{\"end\":293,\"start\":274},{\"end\":400,\"start\":338}]", "title": "[{\"end\":81,\"start\":1},{\"end\":482,\"start\":402}]", "venue": null, "abstract": "[{\"end\":2003,\"start\":484}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2150,\"start\":2123},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2229,\"start\":2208},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2287,\"start\":2264},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2609,\"start\":2576},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2624,\"start\":2609},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2640,\"start\":2624},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2656,\"start\":2640},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2672,\"start\":2656},{\"end\":4371,\"start\":4353},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4861,\"start\":4841},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5616,\"start\":5597},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5656,\"start\":5640},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5706,\"start\":5673},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5881,\"start\":5864},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5927,\"start\":5892},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6086,\"start\":6069},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6280,\"start\":6263},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6325,\"start\":6309},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8123,\"start\":8091},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9102,\"start\":9085},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9111,\"start\":9102},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9130,\"start\":9111},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9165,\"start\":9130},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9315,\"start\":9282},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9330,\"start\":9315},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9597,\"start\":9577},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9974,\"start\":9956},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10796,\"start\":10777},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10960,\"start\":10945},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":16460,\"start\":16427},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16480,\"start\":16460},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16888,\"start\":16845},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":17590,\"start\":17569},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18496,\"start\":18476},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20233,\"start\":20217},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21006,\"start\":20982},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21046,\"start\":21014},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21075,\"start\":21058},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29423,\"start\":29406},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":29432,\"start\":29423},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29509,\"start\":29489},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29570,\"start\":29534},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29760,\"start\":29727},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29775,\"start\":29760},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29961,\"start\":29941},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29996,\"start\":29961},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30047,\"start\":30031},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30084,\"start\":30047},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30106,\"start\":30084},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":30122,\"start\":30106},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30449,\"start\":30432},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30465,\"start\":30449},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30657,\"start\":30624},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":31409,\"start\":31391},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":31539,\"start\":31520},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31587,\"start\":31570},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31800,\"start\":31779},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31829,\"start\":31800},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32525,\"start\":32493},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32666,\"start\":32635},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":32806,\"start\":32786},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":33258,\"start\":33240},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33273,\"start\":33258},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33287,\"start\":33273},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33462,\"start\":33443},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33477,\"start\":33462},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33664,\"start\":33637},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33696,\"start\":33664},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33714,\"start\":33696},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":33746,\"start\":33714},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33762,\"start\":33746},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33780,\"start\":33762},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":33926,\"start\":33905},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33945,\"start\":33926},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36833,\"start\":36794},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40078,\"start\":40063},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":40099,\"start\":40080},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":40293,\"start\":40259},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":40325,\"start\":40298},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":40973,\"start\":40964},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":41686,\"start\":41661},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":43062,\"start\":43053},{\"end\":46657,\"start\":46618},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":56466,\"start\":56450}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":47301,\"start\":47100},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47444,\"start\":47302},{\"attributes\":{\"id\":\"fig_5\"},\"end\":47498,\"start\":47445},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47737,\"start\":47499},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47904,\"start\":47738},{\"attributes\":{\"id\":\"fig_8\"},\"end\":48025,\"start\":47905},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48613,\"start\":48026},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":51946,\"start\":48614},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":52543,\"start\":51947},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":53066,\"start\":52544},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":53538,\"start\":53067},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":53670,\"start\":53539},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":54292,\"start\":53671},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":56038,\"start\":54293}]", "paragraph": "[{\"end\":3110,\"start\":2019},{\"end\":4819,\"start\":3112},{\"end\":4915,\"start\":4821},{\"end\":6699,\"start\":4917},{\"end\":6961,\"start\":6701},{\"end\":7725,\"start\":6963},{\"end\":8194,\"start\":7741},{\"end\":8366,\"start\":8196},{\"end\":8890,\"start\":8423},{\"end\":9506,\"start\":8906},{\"end\":9702,\"start\":9545},{\"end\":10057,\"start\":9810},{\"end\":10514,\"start\":10259},{\"end\":11393,\"start\":10516},{\"end\":11770,\"start\":11395},{\"end\":12437,\"start\":11772},{\"end\":12574,\"start\":12501},{\"end\":12990,\"start\":12647},{\"end\":13152,\"start\":12992},{\"end\":13942,\"start\":13190},{\"end\":14052,\"start\":14038},{\"end\":14143,\"start\":14054},{\"end\":15151,\"start\":14213},{\"end\":15572,\"start\":15501},{\"end\":15936,\"start\":15668},{\"end\":16558,\"start\":15938},{\"end\":17456,\"start\":16611},{\"end\":17531,\"start\":17458},{\"end\":17817,\"start\":17533},{\"end\":18387,\"start\":17887},{\"end\":19298,\"start\":18389},{\"end\":19471,\"start\":19300},{\"end\":19631,\"start\":19495},{\"end\":20092,\"start\":19633},{\"end\":20367,\"start\":20094},{\"end\":20654,\"start\":20369},{\"end\":20786,\"start\":20679},{\"end\":21924,\"start\":20788},{\"end\":22418,\"start\":21949},{\"end\":22866,\"start\":22420},{\"end\":23592,\"start\":22868},{\"end\":23708,\"start\":23594},{\"end\":24741,\"start\":23734},{\"end\":25082,\"start\":24743},{\"end\":26094,\"start\":25122},{\"end\":26714,\"start\":26096},{\"end\":26892,\"start\":26851},{\"end\":27005,\"start\":26894},{\"end\":27674,\"start\":27007},{\"end\":28355,\"start\":27701},{\"end\":28941,\"start\":28370},{\"end\":29076,\"start\":28980},{\"end\":30828,\"start\":29078},{\"end\":32418,\"start\":30830},{\"end\":33179,\"start\":32420},{\"end\":33574,\"start\":33181},{\"end\":35570,\"start\":33622},{\"end\":35740,\"start\":35698},{\"end\":35767,\"start\":35760},{\"end\":35776,\"start\":35769},{\"end\":35805,\"start\":35797},{\"end\":35835,\"start\":35828},{\"end\":35845,\"start\":35837},{\"end\":36444,\"start\":35922},{\"end\":36726,\"start\":36499},{\"end\":37114,\"start\":36751},{\"end\":37461,\"start\":37189},{\"end\":37587,\"start\":37488},{\"end\":37956,\"start\":37804},{\"end\":38143,\"start\":37983},{\"end\":38378,\"start\":38255},{\"end\":38572,\"start\":38450},{\"end\":38806,\"start\":38574},{\"end\":38811,\"start\":38808},{\"end\":39089,\"start\":38904},{\"end\":39128,\"start\":39091},{\"end\":39903,\"start\":39421},{\"end\":40932,\"start\":39905},{\"end\":41870,\"start\":40934},{\"end\":42019,\"start\":41872},{\"end\":42455,\"start\":42160},{\"end\":43001,\"start\":42570},{\"end\":43351,\"start\":43003},{\"end\":43574,\"start\":43373},{\"end\":44660,\"start\":43576},{\"end\":45040,\"start\":44662},{\"end\":45685,\"start\":45100},{\"end\":45847,\"start\":45732},{\"end\":46149,\"start\":45849},{\"end\":47065,\"start\":46191}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8422,\"start\":8367},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9809,\"start\":9703},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10258,\"start\":10058},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12500,\"start\":12438},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12646,\"start\":12575},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13189,\"start\":13153},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14037,\"start\":13943},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14212,\"start\":14144},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15500,\"start\":15152},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16610,\"start\":16559},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17886,\"start\":17818},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26850,\"start\":26715},{\"attributes\":{\"id\":\"formula_12\"},\"end\":35636,\"start\":35571},{\"attributes\":{\"id\":\"formula_13\"},\"end\":35697,\"start\":35658},{\"attributes\":{\"id\":\"formula_14\"},\"end\":36498,\"start\":36445},{\"attributes\":{\"id\":\"formula_15\"},\"end\":36750,\"start\":36727},{\"attributes\":{\"id\":\"formula_16\"},\"end\":37188,\"start\":37115},{\"attributes\":{\"id\":\"formula_17\"},\"end\":37487,\"start\":37462},{\"attributes\":{\"id\":\"formula_18\"},\"end\":37803,\"start\":37588},{\"attributes\":{\"id\":\"formula_19\"},\"end\":38254,\"start\":38144},{\"attributes\":{\"id\":\"formula_20\"},\"end\":38449,\"start\":38379},{\"attributes\":{\"id\":\"formula_21\"},\"end\":38903,\"start\":38812},{\"attributes\":{\"id\":\"formula_22\"},\"end\":39420,\"start\":39129},{\"attributes\":{\"id\":\"formula_23\"},\"end\":42159,\"start\":42020},{\"attributes\":{\"id\":\"formula_24\"},\"end\":42569,\"start\":42456},{\"attributes\":{\"id\":\"formula_25\"},\"end\":43372,\"start\":43352}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21544,\"start\":21537},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21556,\"start\":21549},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22080,\"start\":22073},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25287,\"start\":25280},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25670,\"start\":25663},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26354,\"start\":26347},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":28141,\"start\":28134},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":28353,\"start\":28346},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":45163,\"start\":45156}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2017,\"start\":2005},{\"attributes\":{\"n\":\"2\"},\"end\":7739,\"start\":7728},{\"attributes\":{\"n\":\"3\"},\"end\":8904,\"start\":8893},{\"end\":9543,\"start\":9509},{\"end\":15666,\"start\":15575},{\"attributes\":{\"n\":\"4\"},\"end\":19493,\"start\":19474},{\"end\":20677,\"start\":20657},{\"end\":21947,\"start\":21927},{\"end\":23732,\"start\":23711},{\"end\":25120,\"start\":25085},{\"end\":27699,\"start\":27677},{\"attributes\":{\"n\":\"5\"},\"end\":28368,\"start\":28358},{\"end\":28978,\"start\":28944},{\"end\":33620,\"start\":33577},{\"end\":35657,\"start\":35638},{\"end\":35758,\"start\":35743},{\"end\":35795,\"start\":35779},{\"end\":35826,\"start\":35808},{\"end\":35850,\"start\":35848},{\"end\":35920,\"start\":35853},{\"end\":37981,\"start\":37959},{\"end\":45069,\"start\":45043},{\"end\":45098,\"start\":45072},{\"end\":45730,\"start\":45688},{\"end\":46189,\"start\":46152},{\"end\":47099,\"start\":47068},{\"end\":47111,\"start\":47101},{\"end\":47313,\"start\":47303},{\"end\":47456,\"start\":47446},{\"end\":47510,\"start\":47500},{\"end\":47749,\"start\":47739},{\"end\":47917,\"start\":47906},{\"end\":48036,\"start\":48027},{\"end\":48624,\"start\":48615},{\"end\":51957,\"start\":51948},{\"end\":52554,\"start\":52545},{\"end\":53077,\"start\":53068},{\"end\":53549,\"start\":53540},{\"end\":53681,\"start\":53672}]", "table": "[{\"end\":48613,\"start\":48124},{\"end\":51946,\"start\":49226},{\"end\":52543,\"start\":51991},{\"end\":53066,\"start\":52593},{\"end\":53538,\"start\":53254},{\"end\":53670,\"start\":53594},{\"end\":54292,\"start\":53839},{\"end\":56038,\"start\":55477}]", "figure_caption": "[{\"end\":47301,\"start\":47113},{\"end\":47444,\"start\":47315},{\"end\":47498,\"start\":47458},{\"end\":47737,\"start\":47512},{\"end\":47904,\"start\":47751},{\"end\":48025,\"start\":47919},{\"end\":48124,\"start\":48038},{\"end\":49226,\"start\":48626},{\"end\":51991,\"start\":51959},{\"end\":52593,\"start\":52556},{\"end\":53254,\"start\":53079},{\"end\":53594,\"start\":53551},{\"end\":53839,\"start\":53683},{\"end\":55477,\"start\":54295}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":2853,\"start\":2846},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3406,\"start\":3397},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5498,\"start\":5490},{\"end\":9450,\"start\":9443},{\"end\":15921,\"start\":15916},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16389,\"start\":16382},{\"end\":17310,\"start\":17305},{\"end\":17598,\"start\":17594},{\"end\":18783,\"start\":18774},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23866,\"start\":23857},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24532,\"start\":24523},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24884,\"start\":24875},{\"end\":26667,\"start\":26659},{\"end\":27826,\"start\":27819},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34504,\"start\":34497},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":36130,\"start\":36123},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":44191,\"start\":44184},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":44884,\"start\":44877}]", "bib_author_first_name": "[{\"end\":58060,\"start\":58059},{\"end\":58070,\"start\":58069},{\"end\":58072,\"start\":58071},{\"end\":58080,\"start\":58079},{\"end\":58082,\"start\":58081},{\"end\":58296,\"start\":58295},{\"end\":58298,\"start\":58297},{\"end\":58307,\"start\":58306},{\"end\":58309,\"start\":58308},{\"end\":58320,\"start\":58319},{\"end\":58322,\"start\":58321},{\"end\":58668,\"start\":58667},{\"end\":58680,\"start\":58679},{\"end\":58687,\"start\":58686},{\"end\":58874,\"start\":58873},{\"end\":58881,\"start\":58880},{\"end\":58883,\"start\":58882},{\"end\":58893,\"start\":58892},{\"end\":59060,\"start\":59059},{\"end\":59062,\"start\":59061},{\"end\":59291,\"start\":59290},{\"end\":59302,\"start\":59301},{\"end\":59304,\"start\":59303},{\"end\":59314,\"start\":59313},{\"end\":59502,\"start\":59501},{\"end\":59504,\"start\":59503},{\"end\":59511,\"start\":59510},{\"end\":59513,\"start\":59512},{\"end\":59524,\"start\":59523},{\"end\":59526,\"start\":59525},{\"end\":59537,\"start\":59536},{\"end\":59539,\"start\":59538},{\"end\":59701,\"start\":59700},{\"end\":59703,\"start\":59702},{\"end\":59712,\"start\":59711},{\"end\":59720,\"start\":59719},{\"end\":59729,\"start\":59728},{\"end\":59740,\"start\":59739},{\"end\":59750,\"start\":59749},{\"end\":59762,\"start\":59761},{\"end\":59777,\"start\":59776},{\"end\":59786,\"start\":59785},{\"end\":59796,\"start\":59795},{\"end\":59806,\"start\":59805},{\"end\":59817,\"start\":59816},{\"end\":59833,\"start\":59832},{\"end\":59844,\"start\":59843},{\"end\":59856,\"start\":59855},{\"end\":59865,\"start\":59864},{\"end\":59875,\"start\":59874},{\"end\":59877,\"start\":59876},{\"end\":59888,\"start\":59887},{\"end\":59894,\"start\":59893},{\"end\":59904,\"start\":59903},{\"end\":59913,\"start\":59912},{\"end\":59921,\"start\":59920},{\"end\":59931,\"start\":59930},{\"end\":59941,\"start\":59940},{\"end\":59949,\"start\":59948},{\"end\":59958,\"start\":59957},{\"end\":59967,\"start\":59966},{\"end\":59977,\"start\":59976},{\"end\":59991,\"start\":59990},{\"end\":60639,\"start\":60638},{\"end\":60641,\"start\":60640},{\"end\":60654,\"start\":60653},{\"end\":60665,\"start\":60664},{\"end\":60977,\"start\":60976},{\"end\":60984,\"start\":60983},{\"end\":61211,\"start\":61207},{\"end\":61223,\"start\":61219},{\"end\":61233,\"start\":61229},{\"end\":61242,\"start\":61238},{\"end\":61418,\"start\":61417},{\"end\":61427,\"start\":61426},{\"end\":61435,\"start\":61434},{\"end\":61446,\"start\":61445},{\"end\":61734,\"start\":61733},{\"end\":61741,\"start\":61740},{\"end\":61760,\"start\":61759},{\"end\":61772,\"start\":61771},{\"end\":62096,\"start\":62095},{\"end\":62105,\"start\":62104},{\"end\":62117,\"start\":62116},{\"end\":62124,\"start\":62123},{\"end\":62381,\"start\":62380},{\"end\":62392,\"start\":62391},{\"end\":62407,\"start\":62406},{\"end\":62642,\"start\":62641},{\"end\":62649,\"start\":62648},{\"end\":62657,\"start\":62656},{\"end\":62665,\"start\":62664},{\"end\":62678,\"start\":62677},{\"end\":62680,\"start\":62679},{\"end\":62686,\"start\":62685},{\"end\":62895,\"start\":62894},{\"end\":62908,\"start\":62904},{\"end\":62917,\"start\":62916},{\"end\":62924,\"start\":62923},{\"end\":63218,\"start\":63217},{\"end\":63342,\"start\":63341},{\"end\":63354,\"start\":63353},{\"end\":63365,\"start\":63364},{\"end\":63675,\"start\":63674},{\"end\":63684,\"start\":63683},{\"end\":63686,\"start\":63685},{\"end\":63916,\"start\":63915},{\"end\":63918,\"start\":63917},{\"end\":63927,\"start\":63926},{\"end\":64154,\"start\":64153},{\"end\":64168,\"start\":64167},{\"end\":64345,\"start\":64344},{\"end\":64355,\"start\":64354},{\"end\":64365,\"start\":64364},{\"end\":64551,\"start\":64550},{\"end\":64561,\"start\":64557},{\"end\":64570,\"start\":64569},{\"end\":64578,\"start\":64577},{\"end\":64756,\"start\":64755},{\"end\":64762,\"start\":64761},{\"end\":64769,\"start\":64768},{\"end\":64777,\"start\":64776},{\"end\":64785,\"start\":64784},{\"end\":64796,\"start\":64792},{\"end\":64806,\"start\":64803},{\"end\":64810,\"start\":64809},{\"end\":65215,\"start\":65214},{\"end\":65221,\"start\":65220},{\"end\":65227,\"start\":65226},{\"end\":65238,\"start\":65237},{\"end\":65522,\"start\":65521},{\"end\":65529,\"start\":65528},{\"end\":65537,\"start\":65536},{\"end\":65545,\"start\":65544},{\"end\":65835,\"start\":65831},{\"end\":66053,\"start\":66052},{\"end\":66062,\"start\":66061},{\"end\":66070,\"start\":66069},{\"end\":66072,\"start\":66071},{\"end\":66083,\"start\":66082},{\"end\":66094,\"start\":66093},{\"end\":66112,\"start\":66111},{\"end\":66118,\"start\":66117},{\"end\":66128,\"start\":66127},{\"end\":66588,\"start\":66587},{\"end\":66594,\"start\":66593},{\"end\":66602,\"start\":66601},{\"end\":66613,\"start\":66612},{\"end\":66624,\"start\":66623},{\"end\":66636,\"start\":66632},{\"end\":66949,\"start\":66948},{\"end\":66962,\"start\":66961},{\"end\":66973,\"start\":66972},{\"end\":66975,\"start\":66974},{\"end\":66990,\"start\":66989},{\"end\":67276,\"start\":67275},{\"end\":67289,\"start\":67288},{\"end\":67300,\"start\":67299},{\"end\":67309,\"start\":67308},{\"end\":67325,\"start\":67324},{\"end\":67337,\"start\":67336},{\"end\":67349,\"start\":67348},{\"end\":67652,\"start\":67651},{\"end\":67669,\"start\":67668},{\"end\":67904,\"start\":67903},{\"end\":67911,\"start\":67910},{\"end\":67919,\"start\":67918},{\"end\":67927,\"start\":67926},{\"end\":67936,\"start\":67935},{\"end\":67945,\"start\":67944},{\"end\":67947,\"start\":67946},{\"end\":68205,\"start\":68204},{\"end\":68212,\"start\":68211},{\"end\":68218,\"start\":68217},{\"end\":68226,\"start\":68225},{\"end\":68228,\"start\":68227},{\"end\":68235,\"start\":68234},{\"end\":68243,\"start\":68242},{\"end\":68478,\"start\":68477},{\"end\":68480,\"start\":68479},{\"end\":68487,\"start\":68486},{\"end\":68500,\"start\":68499},{\"end\":68502,\"start\":68501},{\"end\":68515,\"start\":68514},{\"end\":68517,\"start\":68516},{\"end\":68733,\"start\":68732},{\"end\":69026,\"start\":69025},{\"end\":69035,\"start\":69034},{\"end\":69047,\"start\":69046},{\"end\":69373,\"start\":69372},{\"end\":69385,\"start\":69384},{\"end\":69387,\"start\":69386},{\"end\":69691,\"start\":69690},{\"end\":69701,\"start\":69700},{\"end\":69715,\"start\":69714},{\"end\":69723,\"start\":69722},{\"end\":69734,\"start\":69733},{\"end\":69746,\"start\":69745},{\"end\":69762,\"start\":69761},{\"end\":70086,\"start\":70085},{\"end\":70088,\"start\":70087},{\"end\":70098,\"start\":70097},{\"end\":70109,\"start\":70108},{\"end\":70338,\"start\":70337},{\"end\":70346,\"start\":70345},{\"end\":70355,\"start\":70354},{\"end\":70357,\"start\":70356},{\"end\":70372,\"start\":70371},{\"end\":70620,\"start\":70619},{\"end\":70631,\"start\":70630},{\"end\":70639,\"start\":70638},{\"end\":70648,\"start\":70647},{\"end\":70878,\"start\":70877},{\"end\":70891,\"start\":70890},{\"end\":70902,\"start\":70901},{\"end\":70904,\"start\":70903},{\"end\":71060,\"start\":71059},{\"end\":71062,\"start\":71061},{\"end\":71072,\"start\":71071},{\"end\":71211,\"start\":71210},{\"end\":71213,\"start\":71212},{\"end\":71222,\"start\":71221},{\"end\":71224,\"start\":71223},{\"end\":71234,\"start\":71230},{\"end\":71243,\"start\":71242},{\"end\":71245,\"start\":71244},{\"end\":71610,\"start\":71606},{\"end\":71612,\"start\":71611},{\"end\":71620,\"start\":71619},{\"end\":71627,\"start\":71626},{\"end\":71640,\"start\":71636},{\"end\":71651,\"start\":71650},{\"end\":71965,\"start\":71964},{\"end\":71967,\"start\":71966},{\"end\":72189,\"start\":72188},{\"end\":72200,\"start\":72199},{\"end\":72211,\"start\":72210},{\"end\":72221,\"start\":72220},{\"end\":72234,\"start\":72233},{\"end\":72243,\"start\":72242},{\"end\":72245,\"start\":72244},{\"end\":72254,\"start\":72253},{\"end\":72264,\"start\":72263},{\"end\":72449,\"start\":72448},{\"end\":72457,\"start\":72456},{\"end\":72463,\"start\":72462},{\"end\":72473,\"start\":72472},{\"end\":72481,\"start\":72480},{\"end\":72750,\"start\":72749},{\"end\":72760,\"start\":72759},{\"end\":72762,\"start\":72761},{\"end\":72923,\"start\":72922},{\"end\":72930,\"start\":72929},{\"end\":72942,\"start\":72941},{\"end\":72959,\"start\":72958},{\"end\":73189,\"start\":73188},{\"end\":73195,\"start\":73194},{\"end\":73205,\"start\":73204},{\"end\":73316,\"start\":73315},{\"end\":73322,\"start\":73321},{\"end\":73331,\"start\":73330},{\"end\":73345,\"start\":73344},{\"end\":73613,\"start\":73612},{\"end\":73620,\"start\":73619},{\"end\":73622,\"start\":73621},{\"end\":73806,\"start\":73805},{\"end\":73808,\"start\":73807},{\"end\":73817,\"start\":73816},{\"end\":73819,\"start\":73818},{\"end\":73833,\"start\":73832},{\"end\":73844,\"start\":73843}]", "bib_author_last_name": "[{\"end\":58067,\"start\":58061},{\"end\":58077,\"start\":58073},{\"end\":58086,\"start\":58083},{\"end\":58304,\"start\":58299},{\"end\":58317,\"start\":58310},{\"end\":58326,\"start\":58323},{\"end\":58677,\"start\":58669},{\"end\":58684,\"start\":58681},{\"end\":58694,\"start\":58688},{\"end\":58878,\"start\":58875},{\"end\":58890,\"start\":58884},{\"end\":58900,\"start\":58894},{\"end\":59071,\"start\":59063},{\"end\":59299,\"start\":59292},{\"end\":59311,\"start\":59305},{\"end\":59320,\"start\":59315},{\"end\":59508,\"start\":59505},{\"end\":59521,\"start\":59514},{\"end\":59534,\"start\":59527},{\"end\":59545,\"start\":59540},{\"end\":59709,\"start\":59704},{\"end\":59717,\"start\":59713},{\"end\":59726,\"start\":59721},{\"end\":59737,\"start\":59730},{\"end\":59747,\"start\":59741},{\"end\":59759,\"start\":59751},{\"end\":59774,\"start\":59763},{\"end\":59783,\"start\":59778},{\"end\":59793,\"start\":59787},{\"end\":59803,\"start\":59797},{\"end\":59814,\"start\":59807},{\"end\":59830,\"start\":59818},{\"end\":59841,\"start\":59834},{\"end\":59853,\"start\":59845},{\"end\":59862,\"start\":59857},{\"end\":59872,\"start\":59866},{\"end\":59885,\"start\":59878},{\"end\":59891,\"start\":59889},{\"end\":59901,\"start\":59895},{\"end\":59910,\"start\":59905},{\"end\":59918,\"start\":59914},{\"end\":59928,\"start\":59922},{\"end\":59938,\"start\":59932},{\"end\":59946,\"start\":59942},{\"end\":59955,\"start\":59950},{\"end\":59964,\"start\":59959},{\"end\":59974,\"start\":59968},{\"end\":59988,\"start\":59978},{\"end\":59999,\"start\":59992},{\"end\":60651,\"start\":60642},{\"end\":60662,\"start\":60655},{\"end\":60674,\"start\":60666},{\"end\":60981,\"start\":60978},{\"end\":60987,\"start\":60985},{\"end\":61217,\"start\":61212},{\"end\":61227,\"start\":61224},{\"end\":61236,\"start\":61234},{\"end\":61246,\"start\":61243},{\"end\":61424,\"start\":61419},{\"end\":61432,\"start\":61428},{\"end\":61443,\"start\":61436},{\"end\":61456,\"start\":61447},{\"end\":61738,\"start\":61735},{\"end\":61757,\"start\":61742},{\"end\":61769,\"start\":61761},{\"end\":61779,\"start\":61773},{\"end\":62102,\"start\":62097},{\"end\":62114,\"start\":62106},{\"end\":62121,\"start\":62118},{\"end\":62131,\"start\":62125},{\"end\":62389,\"start\":62382},{\"end\":62404,\"start\":62393},{\"end\":62418,\"start\":62408},{\"end\":62646,\"start\":62643},{\"end\":62654,\"start\":62650},{\"end\":62662,\"start\":62658},{\"end\":62675,\"start\":62666},{\"end\":62683,\"start\":62681},{\"end\":62700,\"start\":62687},{\"end\":62902,\"start\":62896},{\"end\":62914,\"start\":62909},{\"end\":62921,\"start\":62918},{\"end\":62934,\"start\":62925},{\"end\":63227,\"start\":63219},{\"end\":63351,\"start\":63343},{\"end\":63362,\"start\":63355},{\"end\":63374,\"start\":63366},{\"end\":63681,\"start\":63676},{\"end\":63691,\"start\":63687},{\"end\":63924,\"start\":63919},{\"end\":63938,\"start\":63928},{\"end\":64165,\"start\":64155},{\"end\":64180,\"start\":64169},{\"end\":64352,\"start\":64346},{\"end\":64362,\"start\":64356},{\"end\":64374,\"start\":64366},{\"end\":64555,\"start\":64552},{\"end\":64567,\"start\":64562},{\"end\":64575,\"start\":64571},{\"end\":64582,\"start\":64579},{\"end\":64759,\"start\":64757},{\"end\":64766,\"start\":64763},{\"end\":64774,\"start\":64770},{\"end\":64782,\"start\":64778},{\"end\":64790,\"start\":64786},{\"end\":64801,\"start\":64797},{\"end\":65218,\"start\":65216},{\"end\":65224,\"start\":65222},{\"end\":65235,\"start\":65228},{\"end\":65242,\"start\":65239},{\"end\":65526,\"start\":65523},{\"end\":65534,\"start\":65530},{\"end\":65542,\"start\":65538},{\"end\":65550,\"start\":65546},{\"end\":65838,\"start\":65836},{\"end\":66059,\"start\":66054},{\"end\":66067,\"start\":66063},{\"end\":66080,\"start\":66073},{\"end\":66091,\"start\":66084},{\"end\":66109,\"start\":66095},{\"end\":66115,\"start\":66113},{\"end\":66125,\"start\":66119},{\"end\":66135,\"start\":66129},{\"end\":66591,\"start\":66589},{\"end\":66599,\"start\":66595},{\"end\":66610,\"start\":66603},{\"end\":66621,\"start\":66614},{\"end\":66630,\"start\":66625},{\"end\":66642,\"start\":66637},{\"end\":66959,\"start\":66950},{\"end\":66970,\"start\":66963},{\"end\":66987,\"start\":66976},{\"end\":67000,\"start\":66991},{\"end\":67286,\"start\":67277},{\"end\":67297,\"start\":67290},{\"end\":67306,\"start\":67301},{\"end\":67322,\"start\":67310},{\"end\":67334,\"start\":67326},{\"end\":67346,\"start\":67338},{\"end\":67359,\"start\":67350},{\"end\":67666,\"start\":67653},{\"end\":67672,\"start\":67670},{\"end\":67908,\"start\":67905},{\"end\":67916,\"start\":67912},{\"end\":67924,\"start\":67920},{\"end\":67933,\"start\":67928},{\"end\":67942,\"start\":67937},{\"end\":67956,\"start\":67948},{\"end\":68209,\"start\":68206},{\"end\":68215,\"start\":68213},{\"end\":68223,\"start\":68219},{\"end\":68232,\"start\":68229},{\"end\":68240,\"start\":68236},{\"end\":68248,\"start\":68244},{\"end\":68484,\"start\":68481},{\"end\":68497,\"start\":68488},{\"end\":68512,\"start\":68503},{\"end\":68527,\"start\":68518},{\"end\":68737,\"start\":68734},{\"end\":69032,\"start\":69027},{\"end\":69044,\"start\":69036},{\"end\":69054,\"start\":69048},{\"end\":69382,\"start\":69374},{\"end\":69395,\"start\":69388},{\"end\":69698,\"start\":69692},{\"end\":69712,\"start\":69702},{\"end\":69720,\"start\":69716},{\"end\":69731,\"start\":69724},{\"end\":69743,\"start\":69735},{\"end\":69759,\"start\":69747},{\"end\":69771,\"start\":69763},{\"end\":70095,\"start\":70089},{\"end\":70106,\"start\":70099},{\"end\":70118,\"start\":70110},{\"end\":70343,\"start\":70339},{\"end\":70352,\"start\":70347},{\"end\":70369,\"start\":70358},{\"end\":70380,\"start\":70373},{\"end\":70628,\"start\":70621},{\"end\":70636,\"start\":70632},{\"end\":70645,\"start\":70640},{\"end\":70654,\"start\":70649},{\"end\":70888,\"start\":70879},{\"end\":70899,\"start\":70892},{\"end\":70907,\"start\":70905},{\"end\":71069,\"start\":71063},{\"end\":71079,\"start\":71073},{\"end\":71219,\"start\":71214},{\"end\":71228,\"start\":71225},{\"end\":71240,\"start\":71235},{\"end\":71248,\"start\":71246},{\"end\":71617,\"start\":71613},{\"end\":71624,\"start\":71621},{\"end\":71634,\"start\":71628},{\"end\":71648,\"start\":71641},{\"end\":71665,\"start\":71652},{\"end\":71980,\"start\":71968},{\"end\":72197,\"start\":72190},{\"end\":72208,\"start\":72201},{\"end\":72218,\"start\":72212},{\"end\":72231,\"start\":72222},{\"end\":72240,\"start\":72235},{\"end\":72251,\"start\":72246},{\"end\":72261,\"start\":72255},{\"end\":72275,\"start\":72265},{\"end\":72454,\"start\":72450},{\"end\":72460,\"start\":72458},{\"end\":72470,\"start\":72464},{\"end\":72478,\"start\":72474},{\"end\":72484,\"start\":72482},{\"end\":72757,\"start\":72751},{\"end\":72770,\"start\":72763},{\"end\":72927,\"start\":72924},{\"end\":72939,\"start\":72931},{\"end\":72956,\"start\":72943},{\"end\":72966,\"start\":72960},{\"end\":73192,\"start\":73190},{\"end\":73202,\"start\":73196},{\"end\":73216,\"start\":73206},{\"end\":73319,\"start\":73317},{\"end\":73328,\"start\":73323},{\"end\":73342,\"start\":73332},{\"end\":73349,\"start\":73346},{\"end\":73617,\"start\":73614},{\"end\":73629,\"start\":73623},{\"end\":73814,\"start\":73809},{\"end\":73830,\"start\":73820},{\"end\":73841,\"start\":73834},{\"end\":73856,\"start\":73845}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1905.07473\",\"id\":\"b0\"},\"end\":58247,\"start\":57982},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6767688},\"end\":58594,\"start\":58249},{\"attributes\":{\"id\":\"b2\"},\"end\":58828,\"start\":58596},{\"attributes\":{\"id\":\"b3\"},\"end\":59000,\"start\":58830},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":35617924},\"end\":59245,\"start\":59002},{\"attributes\":{\"doi\":\"CoRR abs/2004.05150\",\"id\":\"b5\"},\"end\":59452,\"start\":59247},{\"attributes\":{\"id\":\"b6\"},\"end\":59696,\"start\":59454},{\"attributes\":{\"id\":\"b7\"},\"end\":60551,\"start\":59698},{\"attributes\":{\"doi\":\"CoRR abs/1806.07850\",\"id\":\"b8\"},\"end\":60894,\"start\":60553},{\"attributes\":{\"doi\":\"arXiv:1905.11978\",\"id\":\"b9\"},\"end\":61131,\"start\":60896},{\"attributes\":{\"doi\":\"arXiv:1809.02105\",\"id\":\"b10\"},\"end\":61415,\"start\":61133},{\"attributes\":{\"doi\":\"arXiv:1904.10509\",\"id\":\"b11\"},\"end\":61654,\"start\":61417},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11336213},\"end\":62015,\"start\":61656},{\"attributes\":{\"doi\":\"arXiv:1412.3555\",\"id\":\"b13\"},\"end\":62302,\"start\":62017},{\"attributes\":{\"id\":\"b14\"},\"end\":62566,\"start\":62304},{\"attributes\":{\"doi\":\"arXiv:1901.02860\",\"id\":\"b15\"},\"end\":62892,\"start\":62568},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b16\"},\"end\":63195,\"start\":62894},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17158880},\"end\":63339,\"start\":63197},{\"attributes\":{\"doi\":\"arXiv:1704.04110\",\"id\":\"b18\"},\"end\":63602,\"start\":63341},{\"attributes\":{\"doi\":\"arXiv:1710.01278\",\"id\":\"b19\"},\"end\":63832,\"start\":63604},{\"attributes\":{\"doi\":\"arXiv:1502.03619\",\"id\":\"b20\"},\"end\":64127,\"start\":63834},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1915014},\"end\":64305,\"start\":64129},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":209315300},\"end\":64474,\"start\":64307},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4922476},\"end\":64753,\"start\":64476},{\"attributes\":{\"doi\":\"arXiv:1907.00235\",\"id\":\"b24\"},\"end\":65129,\"start\":64755},{\"attributes\":{\"id\":\"b25\"},\"end\":65389,\"start\":65131},{\"attributes\":{\"id\":\"b26\"},\"end\":65771,\"start\":65391},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":19793761},\"end\":65982,\"start\":65773},{\"attributes\":{\"doi\":\"10.18653/v1/d15-1166\",\"id\":\"b28\"},\"end\":66528,\"start\":65984},{\"attributes\":{\"doi\":\"arXiv:1905.09904\",\"id\":\"b29\",\"matched_paper_id\":165163982},\"end\":66886,\"start\":66530},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":12777998},\"end\":67183,\"start\":66888},{\"attributes\":{\"doi\":\"arXiv:1803.03800\",\"id\":\"b31\"},\"end\":67596,\"start\":67185},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":195904},\"end\":67819,\"start\":67598},{\"attributes\":{\"id\":\"b33\"},\"end\":68144,\"start\":67821},{\"attributes\":{\"doi\":\"arXiv:1911.02972\",\"id\":\"b34\"},\"end\":68415,\"start\":68146},{\"attributes\":{\"doi\":\"arXiv:1911.05507\",\"id\":\"b35\"},\"end\":68697,\"start\":68417},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":121901473},\"end\":68957,\"start\":68699},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":55675868},\"end\":69329,\"start\":68959},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":18375389},\"end\":69582,\"start\":69331},{\"attributes\":{\"doi\":\"arXiv:1709.07638\",\"id\":\"b39\"},\"end\":70019,\"start\":69584},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":12956435},\"end\":70260,\"start\":70021},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5985448},\"end\":70541,\"start\":70262},{\"attributes\":{\"doi\":\"arXiv:1911.06393\",\"id\":\"b42\"},\"end\":70823,\"start\":70543},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":7961699},\"end\":71035,\"start\":70825},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":12515984},\"end\":71208,\"start\":71037},{\"attributes\":{\"doi\":\"arXiv:1803.00144\",\"id\":\"b45\"},\"end\":71503,\"start\":71210},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":201698358},\"end\":71884,\"start\":71505},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":120711900},\"end\":72159,\"start\":71886},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":13756489},\"end\":72446,\"start\":72161},{\"attributes\":{\"doi\":\"arXiv:2006.04768\",\"id\":\"b49\"},\"end\":72677,\"start\":72448},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":30816022},\"end\":72920,\"start\":72679},{\"attributes\":{\"doi\":\"arXiv:1711.11053\",\"id\":\"b51\"},\"end\":73159,\"start\":72922},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":6592393},\"end\":73313,\"start\":73161},{\"attributes\":{\"doi\":\"arXiv:1711.00073\",\"id\":\"b53\"},\"end\":73532,\"start\":73315},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":846272},\"end\":73775,\"start\":73534},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":1101453},\"end\":73983,\"start\":73777}]", "bib_title": "[{\"end\":58293,\"start\":58249},{\"end\":59057,\"start\":59002},{\"end\":61731,\"start\":61656},{\"end\":63215,\"start\":63197},{\"end\":64151,\"start\":64129},{\"end\":64342,\"start\":64307},{\"end\":64548,\"start\":64476},{\"end\":65829,\"start\":65773},{\"end\":66585,\"start\":66530},{\"end\":66946,\"start\":66888},{\"end\":67649,\"start\":67598},{\"end\":68730,\"start\":68699},{\"end\":69023,\"start\":68959},{\"end\":69370,\"start\":69331},{\"end\":70083,\"start\":70021},{\"end\":70335,\"start\":70262},{\"end\":70875,\"start\":70825},{\"end\":71057,\"start\":71037},{\"end\":71604,\"start\":71505},{\"end\":71962,\"start\":71886},{\"end\":72186,\"start\":72161},{\"end\":72747,\"start\":72679},{\"end\":73186,\"start\":73161},{\"end\":73610,\"start\":73534},{\"end\":73803,\"start\":73777}]", "bib_author": "[{\"end\":58069,\"start\":58059},{\"end\":58079,\"start\":58069},{\"end\":58088,\"start\":58079},{\"end\":58306,\"start\":58295},{\"end\":58319,\"start\":58306},{\"end\":58328,\"start\":58319},{\"end\":58679,\"start\":58667},{\"end\":58686,\"start\":58679},{\"end\":58696,\"start\":58686},{\"end\":58880,\"start\":58873},{\"end\":58892,\"start\":58880},{\"end\":58902,\"start\":58892},{\"end\":59073,\"start\":59059},{\"end\":59301,\"start\":59290},{\"end\":59313,\"start\":59301},{\"end\":59322,\"start\":59313},{\"end\":59510,\"start\":59501},{\"end\":59523,\"start\":59510},{\"end\":59536,\"start\":59523},{\"end\":59547,\"start\":59536},{\"end\":59711,\"start\":59700},{\"end\":59719,\"start\":59711},{\"end\":59728,\"start\":59719},{\"end\":59739,\"start\":59728},{\"end\":59749,\"start\":59739},{\"end\":59761,\"start\":59749},{\"end\":59776,\"start\":59761},{\"end\":59785,\"start\":59776},{\"end\":59795,\"start\":59785},{\"end\":59805,\"start\":59795},{\"end\":59816,\"start\":59805},{\"end\":59832,\"start\":59816},{\"end\":59843,\"start\":59832},{\"end\":59855,\"start\":59843},{\"end\":59864,\"start\":59855},{\"end\":59874,\"start\":59864},{\"end\":59887,\"start\":59874},{\"end\":59893,\"start\":59887},{\"end\":59903,\"start\":59893},{\"end\":59912,\"start\":59903},{\"end\":59920,\"start\":59912},{\"end\":59930,\"start\":59920},{\"end\":59940,\"start\":59930},{\"end\":59948,\"start\":59940},{\"end\":59957,\"start\":59948},{\"end\":59966,\"start\":59957},{\"end\":59976,\"start\":59966},{\"end\":59990,\"start\":59976},{\"end\":60001,\"start\":59990},{\"end\":60653,\"start\":60638},{\"end\":60664,\"start\":60653},{\"end\":60676,\"start\":60664},{\"end\":60983,\"start\":60976},{\"end\":60989,\"start\":60983},{\"end\":61219,\"start\":61207},{\"end\":61229,\"start\":61219},{\"end\":61238,\"start\":61229},{\"end\":61248,\"start\":61238},{\"end\":61426,\"start\":61417},{\"end\":61434,\"start\":61426},{\"end\":61445,\"start\":61434},{\"end\":61458,\"start\":61445},{\"end\":61740,\"start\":61733},{\"end\":61759,\"start\":61740},{\"end\":61771,\"start\":61759},{\"end\":61781,\"start\":61771},{\"end\":62104,\"start\":62095},{\"end\":62116,\"start\":62104},{\"end\":62123,\"start\":62116},{\"end\":62133,\"start\":62123},{\"end\":62391,\"start\":62380},{\"end\":62406,\"start\":62391},{\"end\":62420,\"start\":62406},{\"end\":62648,\"start\":62641},{\"end\":62656,\"start\":62648},{\"end\":62664,\"start\":62656},{\"end\":62677,\"start\":62664},{\"end\":62685,\"start\":62677},{\"end\":62702,\"start\":62685},{\"end\":62904,\"start\":62894},{\"end\":62916,\"start\":62904},{\"end\":62923,\"start\":62916},{\"end\":62936,\"start\":62923},{\"end\":63229,\"start\":63217},{\"end\":63353,\"start\":63341},{\"end\":63364,\"start\":63353},{\"end\":63376,\"start\":63364},{\"end\":63683,\"start\":63674},{\"end\":63693,\"start\":63683},{\"end\":63926,\"start\":63915},{\"end\":63940,\"start\":63926},{\"end\":64167,\"start\":64153},{\"end\":64182,\"start\":64167},{\"end\":64354,\"start\":64344},{\"end\":64364,\"start\":64354},{\"end\":64376,\"start\":64364},{\"end\":64557,\"start\":64550},{\"end\":64569,\"start\":64557},{\"end\":64577,\"start\":64569},{\"end\":64584,\"start\":64577},{\"end\":64761,\"start\":64755},{\"end\":64768,\"start\":64761},{\"end\":64776,\"start\":64768},{\"end\":64784,\"start\":64776},{\"end\":64792,\"start\":64784},{\"end\":64803,\"start\":64792},{\"end\":64809,\"start\":64803},{\"end\":64813,\"start\":64809},{\"end\":65220,\"start\":65214},{\"end\":65226,\"start\":65220},{\"end\":65237,\"start\":65226},{\"end\":65244,\"start\":65237},{\"end\":65528,\"start\":65521},{\"end\":65536,\"start\":65528},{\"end\":65544,\"start\":65536},{\"end\":65552,\"start\":65544},{\"end\":65840,\"start\":65831},{\"end\":66061,\"start\":66052},{\"end\":66069,\"start\":66061},{\"end\":66082,\"start\":66069},{\"end\":66093,\"start\":66082},{\"end\":66111,\"start\":66093},{\"end\":66117,\"start\":66111},{\"end\":66127,\"start\":66117},{\"end\":66137,\"start\":66127},{\"end\":66593,\"start\":66587},{\"end\":66601,\"start\":66593},{\"end\":66612,\"start\":66601},{\"end\":66623,\"start\":66612},{\"end\":66632,\"start\":66623},{\"end\":66644,\"start\":66632},{\"end\":66961,\"start\":66948},{\"end\":66972,\"start\":66961},{\"end\":66989,\"start\":66972},{\"end\":67002,\"start\":66989},{\"end\":67288,\"start\":67275},{\"end\":67299,\"start\":67288},{\"end\":67308,\"start\":67299},{\"end\":67324,\"start\":67308},{\"end\":67336,\"start\":67324},{\"end\":67348,\"start\":67336},{\"end\":67361,\"start\":67348},{\"end\":67668,\"start\":67651},{\"end\":67674,\"start\":67668},{\"end\":67910,\"start\":67903},{\"end\":67918,\"start\":67910},{\"end\":67926,\"start\":67918},{\"end\":67935,\"start\":67926},{\"end\":67944,\"start\":67935},{\"end\":67958,\"start\":67944},{\"end\":68211,\"start\":68204},{\"end\":68217,\"start\":68211},{\"end\":68225,\"start\":68217},{\"end\":68234,\"start\":68225},{\"end\":68242,\"start\":68234},{\"end\":68250,\"start\":68242},{\"end\":68486,\"start\":68477},{\"end\":68499,\"start\":68486},{\"end\":68514,\"start\":68499},{\"end\":68529,\"start\":68514},{\"end\":68739,\"start\":68732},{\"end\":69034,\"start\":69025},{\"end\":69046,\"start\":69034},{\"end\":69056,\"start\":69046},{\"end\":69384,\"start\":69372},{\"end\":69397,\"start\":69384},{\"end\":69700,\"start\":69690},{\"end\":69714,\"start\":69700},{\"end\":69722,\"start\":69714},{\"end\":69733,\"start\":69722},{\"end\":69745,\"start\":69733},{\"end\":69761,\"start\":69745},{\"end\":69773,\"start\":69761},{\"end\":70097,\"start\":70085},{\"end\":70108,\"start\":70097},{\"end\":70120,\"start\":70108},{\"end\":70345,\"start\":70337},{\"end\":70354,\"start\":70345},{\"end\":70371,\"start\":70354},{\"end\":70382,\"start\":70371},{\"end\":70630,\"start\":70619},{\"end\":70638,\"start\":70630},{\"end\":70647,\"start\":70638},{\"end\":70656,\"start\":70647},{\"end\":70890,\"start\":70877},{\"end\":70901,\"start\":70890},{\"end\":70909,\"start\":70901},{\"end\":71071,\"start\":71059},{\"end\":71081,\"start\":71071},{\"end\":71221,\"start\":71210},{\"end\":71230,\"start\":71221},{\"end\":71242,\"start\":71230},{\"end\":71250,\"start\":71242},{\"end\":71619,\"start\":71606},{\"end\":71626,\"start\":71619},{\"end\":71636,\"start\":71626},{\"end\":71650,\"start\":71636},{\"end\":71667,\"start\":71650},{\"end\":71982,\"start\":71964},{\"end\":72199,\"start\":72188},{\"end\":72210,\"start\":72199},{\"end\":72220,\"start\":72210},{\"end\":72233,\"start\":72220},{\"end\":72242,\"start\":72233},{\"end\":72253,\"start\":72242},{\"end\":72263,\"start\":72253},{\"end\":72277,\"start\":72263},{\"end\":72456,\"start\":72448},{\"end\":72462,\"start\":72456},{\"end\":72472,\"start\":72462},{\"end\":72480,\"start\":72472},{\"end\":72486,\"start\":72480},{\"end\":72759,\"start\":72749},{\"end\":72772,\"start\":72759},{\"end\":72929,\"start\":72922},{\"end\":72941,\"start\":72929},{\"end\":72958,\"start\":72941},{\"end\":72968,\"start\":72958},{\"end\":73194,\"start\":73188},{\"end\":73204,\"start\":73194},{\"end\":73218,\"start\":73204},{\"end\":73321,\"start\":73315},{\"end\":73330,\"start\":73321},{\"end\":73344,\"start\":73330},{\"end\":73351,\"start\":73344},{\"end\":73619,\"start\":73612},{\"end\":73631,\"start\":73619},{\"end\":73816,\"start\":73805},{\"end\":73832,\"start\":73816},{\"end\":73843,\"start\":73832},{\"end\":73858,\"start\":73843}]", "bib_venue": "[{\"end\":58057,\"start\":57982},{\"end\":58398,\"start\":58328},{\"end\":58665,\"start\":58596},{\"end\":58871,\"start\":58830},{\"end\":59108,\"start\":59073},{\"end\":59288,\"start\":59247},{\"end\":59499,\"start\":59454},{\"end\":60636,\"start\":60553},{\"end\":60974,\"start\":60896},{\"end\":61205,\"start\":61133},{\"end\":61524,\"start\":61474},{\"end\":61811,\"start\":61781},{\"end\":62093,\"start\":62017},{\"end\":62378,\"start\":62304},{\"end\":62639,\"start\":62568},{\"end\":63032,\"start\":62952},{\"end\":63258,\"start\":63229},{\"end\":63456,\"start\":63392},{\"end\":63672,\"start\":63604},{\"end\":63913,\"start\":63834},{\"end\":64200,\"start\":64182},{\"end\":64380,\"start\":64376},{\"end\":64590,\"start\":64584},{\"end\":64928,\"start\":64829},{\"end\":65212,\"start\":65131},{\"end\":65519,\"start\":65391},{\"end\":65870,\"start\":65840},{\"end\":66050,\"start\":65984},{\"end\":66693,\"start\":66660},{\"end\":67017,\"start\":67002},{\"end\":67273,\"start\":67185},{\"end\":67689,\"start\":67674},{\"end\":67901,\"start\":67821},{\"end\":68202,\"start\":68146},{\"end\":68475,\"start\":68417},{\"end\":68813,\"start\":68739},{\"end\":69124,\"start\":69056},{\"end\":69435,\"start\":69397},{\"end\":69688,\"start\":69584},{\"end\":70124,\"start\":70120},{\"end\":70386,\"start\":70382},{\"end\":70617,\"start\":70543},{\"end\":70913,\"start\":70909},{\"end\":71106,\"start\":71081},{\"end\":71329,\"start\":71266},{\"end\":71675,\"start\":71667},{\"end\":72006,\"start\":71982},{\"end\":72281,\"start\":72277},{\"end\":72550,\"start\":72502},{\"end\":72781,\"start\":72772},{\"end\":73029,\"start\":72984},{\"end\":73222,\"start\":73218},{\"end\":73411,\"start\":73367},{\"end\":73640,\"start\":73631},{\"end\":73862,\"start\":73858},{\"end\":61828,\"start\":61813}]"}}}, "year": 2023, "month": 12, "day": 17}