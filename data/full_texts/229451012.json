{"id": 229451012, "updated": "2023-11-08 00:08:18.18", "metadata": {"title": "RIM-ONE DL: A UNIFIED RETINAL IMAGE DATABASE FOR ASSESSING GLAUCOMA USING DEEP LEARNING", "authors": "[{\"first\":\"F\",\"last\":\"UMERO\",\"middle\":[\"RANCISCO\",\"F\"]},{\"first\":\"T\",\"last\":\"LEMAN\",\"middle\":[\"INGUARO\",\"D\",\"IAZ\",\"-A\"]},{\"first\":\"J\",\"last\":\"IGUT\",\"middle\":[\"OSE\",\"S\"]},{\"first\":\"S\",\"last\":\"LAYON\",\"middle\":[\"ILVIA\",\"A\"]},{\"first\":\"R\",\"last\":\"RNAY\",\"middle\":[\"AFAEL\",\"A\"]},{\"first\":\"D\",\"last\":\"EREIRA\",\"middle\":[\"ENISSE\",\"A\",\"NGEL\",\"-P\"]}]", "venue": "Image Analysis & Stereology", "journal": "Image Analysis & Stereology", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "The \ufb01rst version of the R etinal IM age database for O ptic N erve E valuation (RIM-ONE) was published in 2011. This was followed by two more, turning it into one of the most cited public retinography databases for evaluating glaucoma. Although it was initially intended to be a database with reference images for segmenting the optic disc, in recent years we have observed that its use has been more oriented toward training and testing deep learning models. The recent REFUGE challenge laid out some criteria that a set of images of these characteristics must satisfy to be used as a standard reference for validating deep learning methods that rely on the use of these data. This, combined with the certain confusion and even improper use observed in some cases of the three versions published, led us to consider revising and combining them into a new, publicly available version called RIM-ONE DL (RIM-ONE for D eep L earning). This paper describes this set of images, consisting of 313 retinographies from normal subjects and 172 retinographies from patients with glaucoma. All of these images have been assessed by two experts and include a manual segmentation of the disc and cup. It also describes an evaluation benchmark with different models of well-known convolutional neural networks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3108542203", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.5566/ias.2346"}}, "content": {"source": {"pdf_hash": "11f7990a46e3f0906091bb5cedff16d69ebf8189", "pdf_src": "Anansi", "pdf_uri": "[\"https://www.ias-iss.org/ojs/IAS/article/download/2346/1128\"]", "oa_url_match": true, "oa_info": {"license": "CCBYNC", "open_access_url": "https://www.ias-iss.org/ojs/IAS/article/download/2346/1128", "status": "GOLD"}}, "grobid": {"id": "06e4615ec1fb4e5fa9feaa5c16baafd8f8eb60db", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/11f7990a46e3f0906091bb5cedff16d69ebf8189.txt", "contents": "\nRIM-ONE DL: A UNIFIED RETINAL IMAGE DATABASE FOR ASSESSING GLAUCOMA USING DEEP LEARNING\n2020\n\nFrancisco Fumero ffumerob@ull.edu.es \nDepartment of Computer Engineering and Systems\nUniversity of La Laguna\nCampus de Anchieta38200TenerifeSpain\n\nTinguaro Diaz-Aleman \nServicio de Oftalmolog\u00eda\nHospital Universitario de Canarias\n38320TenerifeSpain\n\nJose Sigut jfsigut@ull.edu.es \nDepartment of Computer Engineering and Systems\nUniversity of La Laguna\nCampus de Anchieta38200TenerifeSpain\n\nSilvia Alayon salayon@ull.edu.es \nDepartment of Computer Engineering and Systems\nUniversity of La Laguna\nCampus de Anchieta38200TenerifeSpain\n\nRafael Arnay rarnay@ull.edu.es \nDepartment of Computer Engineering and Systems\nUniversity of La Laguna\nCampus de Anchieta38200TenerifeSpain\n\nDenisse Angel-Pereira \nServicio de Oftalmolog\u00eda\nHospital Universitario de Canarias\n38320TenerifeSpain\n\nRIM-ONE DL: A UNIFIED RETINAL IMAGE DATABASE FOR ASSESSING GLAUCOMA USING DEEP LEARNING\n\nImage Anal Stereol\n39202010.5566/ias.2346(Received February 11, 2020; revised October 5, 2020; accepted October 5, 2020)Original Research PaperConvolutional Neural NetworksDeep LearningGlaucoma AssessmentRIM-ONE\nThe first version of the Retinal IMage database for Optic Nerve Evaluation (RIM-ONE) was published in 2011. This was followed by two more, turning it into one of the most cited public retinography databases for evaluating glaucoma. Although it was initially intended to be a database with reference images for segmenting the optic disc, in recent years we have observed that its use has been more oriented toward training and testing deep learning models. The recent REFUGE challenge laid out some criteria that a set of images of these characteristics must satisfy to be used as a standard reference for validating deep learning methods that rely on the use of these data. This, combined with the certain confusion and even improper use observed in some cases of the three versions published, led us to consider revising and combining them into a new, publicly available version called RIM-ONE DL (RIM-ONE for Deep Learning). This paper describes this set of images, consisting of 313 retinographies from normal subjects and 172 retinographies from patients with glaucoma. All of these images have been assessed by two experts and include a manual segmentation of the disc and cup. It also describes an evaluation benchmark with different models of well-known convolutional neural networks.\n\nINTRODUCTION\n\nThe term glaucoma refers to a group of pathologies that affect the optic nerve and involve the loss of retinal ganglion cells, which is frequently associated with an increase in intraocular pressure. It is one of the leading causes of blindness in the world (Tham et al., 2014) and, since the progression of the disease is typically asymptomatic, early detection is quite difficult. There are different techniques for viewing the retina that help in making this diagnosis. One of them is a retinography, which is a color photograph of the fundus of the eye. Fig. 1 shows an example of a retinography, where the most relevant parts for diagnosing glaucoma have been highlighted: the optic disc, the cup and the neuroretinal rim. In fact, since this is the most important part of the retinography, it is typical to cut around it and discard the rest. As in other fields, automated learning-based diagnoses, and more specifically the technique known as deep learning, have taken on great significance. The key to the proper functioning of these methods is the availability of a sufficient amount of data with which to train and test the system. In addition, validating these methods requires a reference standard that can be used for comparison. In the case at hand, this involves having public retinography databases that satisfy a series of requirements, which must also be clearly defined. In the very recent paper on the Retinal Fundus Glaucoma Challenge (REFUGE) (Orlando et al., 2020), written by researchers from 20 institutions, a very important step is taken in this direction by suggesting certain criteria that can be used to compare these methods in terms of both classifying the glaucoma and segmenting the disc and cup:\n\n1. Availability of publicly-accessible sets of images, labelled by several experts, sufficient enough in number that they can be used in deep-learning methods.\n\n2. Clear separation between training and test sets. As noted in (Trucco et al., 2013), a comparison of the results may be unreliable without said separation.\n\n3. Presence of diversity in the set of images, with diversity meaning having images captured by various devices involving different patient ethnicities, and images taken in different lighting, contrast, noise and other conditions.\n\n4. In addition to having a preliminary diagnosis, include also manual reference segmentations of the disc and cup.\n\n5. Provide an evaluation framework that includes the metrics used and the format for presenting the results.\n\nThe first open version of the set of fundus images called Retinal IMage database for Optic Nerve Evaluation (RIM-ONE) was published in 2011 (Fumero et al., 2011). Two other versions followed, in 2014 and 2015. They will be referred to in this paper as RIM-ONE v1, v2 and v3, respectively. Since its publication, it has been cited 179 times, becoming, based on our data, the most cited public set of retinographies for evaluating glaucoma. Although it was initially intended to be used as a database of reference images for segmenting the optic disc, in recent years we have observed that its use has been more oriented toward training and testing deep learning models. This, combined with the certain confusion and even improper use observed in some cases of the three versions published, led us to consider revising and combining them into a new version called RIM-ONE DL (RIM-ONE for Deep Learning), optimized for a deep-learning context in keeping with the specifications explained earlier. Furthermore, for benchmarking purposes, we studied the performance of various convolutional neural network models that are very popular, due to their widespread use for the classification and semantic segmentation of natural images. As a result, we hope to lay the foundations to have RIM-ONE DL become a reference for evaluating glaucoma, like the previous versions were. The rest of the paper is structured as follows. The section on \"Related Work\" describes other sets of public images for evaluating glaucoma. The \"RIM-ONE database\" section presents the different versions of this imaging database and analyzes the feedback received over its nearly ten years of use. The \"Materials and Methods\" section describes RIM-ONE DL, underscoring the changes made with respect to previous versions and explaining the neural network models and the metrics used in the benchmark. We conclude with the \"Results and Discussion\" section.\n\n\nRELATED WORK\n\nThere are not too many public sets of images of the fundus of the eye for evaluating glaucoma, whether for classification or for segmenting the optic disc and cup. ORIGA (Zhang et al., 2010) is composed of 168 images from patients with glaucoma and 482 from healthy patients. The data also include the disc and cup segmentation. The problem with this database is that even though it appears to have been public at one point, as far as we can tell, it stopped being public quite some time ago. DRISHTI-GS (Sivaswamy et al., 2014) consists of 70 images of glaucoma and 31 normal images. It also includes the disc and cup segmentation. DR HAGIS (Holm et al., 2017), HRF (Odstrcilik et al., 2013), and LES-AV (Orlando et al., 2018) are small sets with 39, 45 and 22 images, respectively, with no disc and cup segmentation. The ACRIMA set (Diaz-Pinto et al., 2019) contains 396 images from patients with glaucoma and 309 images from healthy patients. It also does not include segmentation of the disc or cup. Finally, the recent REFUGE database (Orlando et al., 2020) contains 120 images from patients with glaucoma and 1080 images from healthy patients with disc and cup segmentation.\n\nIt is important to note that of all the sets mentioned, only REFUGE satisfies the additional requirements of offering images from different cameras, as well as a clear division of the training and test data. There seems to be a clear need, then, to expand the number of public image databases available that comply with these requirements.\n\n\nTHE RIM-ONE DATABASE\n\nThe images in the three versions of RIM-ONE include healthy and glaucomatous eyes, and were taken in various Spanish hospitals: Hospital Universitario de Canarias (HUC), in Tenerife, Hospital Universitario Miguel Servet (HUMS), in Zaragoza, and Hospital Cl\u00ednico Universitario San Carlos (HCSC), in Madrid. The repository of retinographies is accessible through the website of the research group at https://medimrg.webs.ull.es.\n\n\nCRITICAL REVISION OF THE THREE VERSIONS OF RIM-ONE\n\nRIM-ONE v1 was presented in 2011 at the 24th International Symposium on Computer-Based Medical Systems (CBMS) (Fumero et al., 2011). The main goal of this work was to provide an open database of retinographies from 118 healthy subjects and 51 patients with various stages of glaucoma. In addition to a diagnosis, it included a manual segmentation of the optic disc, carried out by five experts in the field. This was the main goal of its publication, to provide a reference for assessing methods to segment the disc. The images were taken at the three hospitals mentioned above using a Nidek AFC-210 non-mydriatic fundus camera with a 21.1-megapixel Canon EOS 5D Mark II body, with a vertical and horizontal field of view of 45 \u2022 .\n\nRIM-ONE v2 was published in 2014. It contains 255 images from healthy subjects and 200 images from patients with glaucoma that are manually segmented by a medical specialist. In this case, the images were taken at HUC and the Hospital Universitario Miguel Servet using the same camera as in v1. It is important to note that this version was designed as an extension of the first; as a result, some images are duplicated. It also includes some test-retest cases that give rise to images that are practically identical.\n\nRIM-ONE v3 was published in 2015 and contains 85 images of healthy subjects and 74 images of patients with glaucoma. The main difference between this version and the two previous ones is that the images were captured only in the HUC with a nonmydriatic Kowa WX 3D stereo fundus camera. The images are centered on the ONH using a field angle of 34 \u2022 , giving a final stereo image with a horizontal field of view of 20 \u2022 and a vertical field of view of 27 \u2022 , with a total resolution of 2144 x 1424 pixels (1072 x 1424 pixels per image in the stereo pair). Having stereo images available resulted in a manual segmentation of not only the optic disc, but of the cup as well. Two specialists carried out this task with help from the freely distributed DCSeg tool. More details on this version and on the tool are available at (Fumero et al., 2015). It should be noted that some of the subjects whose retinographies are contained in v2 are also present in v3.\n\nAs was stated in the introduction, a critical review of these three versions is necessary in order to determine how well they comply with the criteria in place for using them in methods based on deep learning:\n\n1. Although it may be tempting to combine the three versions for use in deep-learning problems, as indicated earlier, indiscriminately combining the images could result in their inappropriate use. It should also be noted that there is no consistency between the three versions in terms of the labelling of the images, as this was not always done by the same experts.\n\n2. Since RIM-ONE was not originally designed for deep learning, a clear division between training and test images was never established.\n\n3. The RIM-ONE images were taken in different hospitals with different cameras, but only one camera was used in each version.\n\n4. In this area there is also little consistency between versions, since in the first two only the disc is segmented, while the cup is also segmented in v3.\n\nAs happened with the diagnosis, the specialists involved in the manual segmentation were not the same in every case.\n\n5. Although version 1 does propose a criterion for evaluating the quality of the disc segmentation, no details are given on the metrics for evaluating the diagnosis, since that was not the initial idea.\n\nThe \"Materials and Methods\" section details the process used to attempt to resolve these problems with RIM-ONE DL.\n\n\nFEEDBACK ON THE EXPERIENCE WITH RIM-ONE\n\nUsing a procedure similar to that presented in (Decenci\u00e8re et al., 2014) for the well-known Messidor database for segmenting the optic disc in diabetic retinography images, in this section we will focus on the feedback received over the nearly 10 years that RIM-ONE has been publicly available. To provide a quantitative idea of the impact that its publication has had, we used as a reference the total number of citations, which is 179. Moreover, Fig. 2 shows the recorded trend in terms of the primary purpose for which RIM-ONE has been used. In its early years, a large majority of the uses were centered on segmentation tasks. However, the last two years have seen a very significant increase in the number of publications in which its use was associated with deeplearning problems, a use that in 2019 even outpaced the number of works involving segmentation. This reinforces the need to have a revised and updated version of RIM-ONE that can satisfy this new trend. \n\n\nMATERIALS AND METHODS\n\n\nRIM-ONE DL\n\nIn this section, we describe RIM-ONE DL, which results from combining the three previous versions. This new version does away with the duplicate images contained in v1 and v2, and it also eliminates the testretest images in v2. The images of the same patient in v2 and v3 were also deleted, with only the left image from v3 being retained. The result is a single image per patient and eye. Moreover, all the images were cropped squarely around the head of the optic nerve using the same proportionality criterion, something that was not done in the previous versions. Table 1 shows the minimum and maximum size of these cropped images per version and the hospital where the images were taken. Furthermore, the format of all the images in this new version is PNG, and the file names are prefixed with \"r1\", \"r2\" or \"r3\" according to the RIM-ONE version they were extracted from. As concerns its use in deep-learning problems, and as discussed in previous sections, we note the following:\n\n1. The final set of images consists of 313 images from healthy subjects and 172 images from patients with glaucoma. In order to standardize the criterion of experts for classifying glaucoma, two experts again reviewed all the images and re-labelled them after a visual inspection. In the event of a disagreement between them, a third specialist with 20 years of experience was consulted, who made the final decision.\n\n\nA clear division is established between the training\n\nand test sets, with two variants. In one, the test set is built randomly, while in the other, the samples taken in the HUC are used for training and the samples taken in the two other hospitals (in Madrid and Zaragoza) are used for testing.\n\n3. This combined version exhibits great diversity in terms of the cameras and hospitals.\n\n4. In addition to the ground truth for classification, the set includes the manual segmentation of the disc and cup performed by one of the specialists.\n\n5. The sub-section below details the evaluation framework for classifying the glaucoma disease. Fig. 3 shows some examples of the images contained in RIM-ONE DL, indicating the hospital they were taken in. This database is publicly available at the following location: https://github.com/ miag-ull/rim-one-dl\n\n\nEVALUATION FRAMEWORK\n\nThe evaluation framework proposed contains four main elements: definition of the training and test sets, neural network models used, training and testing strategy employed, and the metrics considered in the evaluation.\n\nAs concerns the training and test sets used, as noted in the preceding sub-section, two variants are considered. In the first variant, the set of images was divided at random into training and testing images using a 70:30 ratio, respectively. In the second variant, the images taken at the HUC were used for training (195 normal and 116 glaucoma), and the images taken at the two other hospitals were used for testing (118 normal and 56 glaucoma). The only processing done to the images involved re-scaling them in intensity in the 0-1 range and resizing them to 224x224x3.\n\nIn terms of the neural network models used, most of the architectures contained in the Keras Deep Learning Framework were tested: Xception, VGG16, VGG19, ResNet50, InceptionV3, InceptionResNetV2, MobileNet, DenseNet121, NASNetMobile and MobileNetV2. In every case, the size of the input layer was set to 224x224x3, and a GlobalAveragePooling2D layer was added to the convolutional base, followed by a fully-connected output layer with two outputs,  using SoftMax to distinguish between the Normal and Glaucoma classes.\n\nThe training strategy was the same with both variants of the data sets. We started with the pretrained networks using the weight values of ImageNet provided by Keras and fine tuned all the layers. Diaz-Pinto et al. (2019) found that this yielded the best results. To avoid overfitting, data augmentation consisting of random rotations (-30 \u2022 , 30 \u2022 ), vertical and horizontal flip, and zoom (0.8, 1.2) was used.\n\nThe steps carried out to fine tune the networks were as follows:\n\n1. Freeze the convolutional base network.\n\n2. Train the part that was added.\n\n3. Unfreeze all the layers in the base network.\n\n\nJointly train all the layers in the network.\n\nTo increase the reliability of the experiments, a 5fold cross-validation was applied, with a proportion of 80% for the training set and 20% for the validation set in each fold. For each fold, the steps listed were followed in order to determine the most suitable number of epochs in 2 and 4. For the training in step 2, a batch size of 32 was used, along with an RMSprop optimizer with a learning rate of 2e-5, and categorical cross-entropy as a loss function. For the training in step 4, the learning rate was set to 1e-5. Once the validation phase was complete, the final model for each network was trained using the whole training data (no folds) and following the same four steps as before for the number of epochs that maximized the average validation accuracy across folds in steps 2 and 4. This final model was used to evaluate each network in the test set.\n\nAs concerns the metrics used, the outline proposed in (Orlando et al., 2020) was used, in which the area under the curve (AUC) is used as a reference evaluation measure. This measure was complemented with the sensitivity value (Se = T p/(T p + Fn)) at a specificity of 0.85 (Sp = T n/(T n + F p)), where T p, F p, T n and Fn are the number of true positives, false positives, true negatives and false negatives, respectively. This allows for an assessment of the performance of the various networks when a low rate of false positives is imposed. The third measure included is accuracy, which is fairly standard in this type of problem, although it is well-known that it can exhibit some bias in data sets whose classes are not properly balanced. Tables 2 and 3 and figures 4 and 5 show the results of the glaucoma classifications obtained under the conditions described in the preceding section. In the case of the random test sample, the results are highly satisfactory. The VGG19 network model not only provided the highest AUC, but its sensitivity also equalled 1, the highest possible. The other network model with similar characteristics, VGG16, also yielded good results. Although a direct comparison with the results of the REFUGE challenge is not possible, it is interesting to note that the winning team (Son et al., 2018) attained an AUC of 0.9885 with a sensitivity of 0.9752 for a test sample consisting of 360 images from healthy subjects and 40 images from patients with glaucoma. In the case of the test sample from the hospitals in Madrid and Zaragoza, there is a significant drop in all the metrics, with the best response again being obtained by networks VGG19 and VGG16. This drop could be explained by the fact that a set of test images was used whose visual appearance was rather different from that of the images used during training. It is important to keep in mind that the images were captured in different hospitals under different circumstances, which seems to have affected the networks. The lack of robustness of this type of system to distortions that can affect the images, such as noise, contrast or lighting, has been analyzed by various authors (Borkar and Karam, 2019). Again, it is difficult to cite any work to compare against in this regard. The closest would be (Diaz-Pinto et al., 2019), in which the Xception network was trained using images from some public databases, and it was trained with different databases whose images were taken under varying conditions. Its results are in keeping with those stemming from our experiments, with the exception that in our case, the experts who performed the reference diagnoses were the same for the training and test groups, unlike in the aforementioned work. This leads us to think that even though this factor could have some influence, the fact that the images were taken in different conditions is likely to be more relevant.   \n\n\nRESULTS AND DISCUSSION\n\nFig. 1 :\n1Sample retinography with the most relevant regions for diagnosing glaucoma.\n\nFig. 2 :\n2Number of citations of RIM-ONE. Blue: RIM-ONE for segmentation, Red: RIM-ONE for Deep Learning.\n\nFig. 3 :\n3Examples of images included in RIM-ONE DL, indicating in which hospital they were taken.\n\nFig. 5 :\n5ROC Curves for all the networks using the test set from Madrid and Zaragoza.\n\nTable 1 :\n1Minimum and maximum size of the cropped images of RIM-ONE DL per version, indicating the hospital where the images were taken.Version \nHospital Min. Size Max. Size \nv1 \nHCSC, HUMS \n316 \n708 \nv2 \nHUC, HUMS \n274 \n793 \nv3 \nHUC \n318 \n626 \n\n\n\nTable 2 :\n2Evaluation of the different networks using the random test set.Network \nAUC \nSe \nAcc. \nVGG19 \n0.9867 1.0000 0.9315 \nVGG16 \n0.9834 0.9615 0.9247 \nXception \n0.9771 0.9808 0.9178 \nResNet50 \n0.9755 0.9808 0.9110 \nMobileNetV2 \n0.9738 0.9423 0.9041 \nDenseNet \n0.9726 0.9615 0.9041 \nMobileNet \n0.9712 0.9615 0.9315 \nInceptionResNetV2 0.9685 0.9808 0.9110 \nInceptionV3 \n0.9597 0.9423 0.8904 \nNASNetMobile \n0.9290 0.9231 0.7534 \n\n\n\nTable 3 :\n3Evaluation of the different networks using the test set from Madrid and Zaragoza.Fig. 4: ROC Curves for all the networks using the random test set.Network \nAUC \nSe \nAcc. \nVGG19 \n0.9272 0.8750 0.8563 \nVGG16 \n0.9177 0.8214 0.8506 \nInceptionV3 \n0.9015 0.7500 0.8046 \nXception \n0.8982 0.7500 0.7989 \nDenseNet \n0.8919 0.7143 0.7816 \nMobileNet \n0.8912 0.7500 0.8276 \nResNet50 \n0.8855 0.7321 0.8333 \nInceptionResNetV2 0.8396 \n0.625 0.7644 \nNASNetMobile \n0.7969 0.6071 0.7989 \nMobileNetV2 \n0.7765 0.4464 0.5287 \n\n\n\nDeepCorrect: Correcting DNN Models Against Image Distortions. T S Borkar, L J Karam, IEEE T Image Process. 28Borkar TS, Karam LJ (2019). DeepCorrect: Correcting DNN Models Against Image Distortions. IEEE T Image Process 28:6022-34.\n\nFeedback on a publicly distributed database: the Messidor database. E Decenci\u00e8re, X Zhang, G Cazuguel, B Lay, B Cochener, C Trone, P Gain, R Ordonez, P Massin, A Erginay, B Charton, J C Klein, Image Anal Stereol. 33Decenci\u00e8re E, Zhang X, Cazuguel G, Lay B, Cochener B, Trone C, Gain P, Ordonez R, Massin P, Erginay A, Charton B, Klein JC (2014). Feedback on a publicly distributed database: the Messidor database. Image Anal Stereol 33:231-4.\n\nCNNs for automatic glaucoma assessment using fundus images: an extensive validation. A Diaz-Pinto, S Morales, V Naranjo, T K\u00f6hler, J M Mossi, A Navea, Biomed Eng Online. 18Diaz-Pinto A, Morales S, Naranjo V, K\u00f6hler T, Mossi JM, Navea A (2019). CNNs for automatic glaucoma assessment using fundus images: an extensive validation. Biomed Eng Online 18:1-19.\n\nRIM-ONE: An open retinal image database for optic nerve evaluation. F Fumero, S Alayon, J L Sanchez, J Sigut, M Gonzalez-Hernandez, 2011 24th International Symposium on Computer-Based Medical Systems (CBMS). Fumero F, Alayon S, Sanchez JL, Sigut J, Gonzalez- Hernandez M (2011). RIM-ONE: An open retinal image database for optic nerve evaluation. In: 2011 24th International Symposium on Computer- Based Medical Systems (CBMS).\n\nInteractive tool and database for optic disc and cup segmentation of stereo and monocular retinal fundus images. F Fumero, J Sigut, S Alayon, M Gonzalez-Hernandez, Gonzalez De La Rosa, M , Short communications proceedings. Plzen, Czech Republic; Skala -UNION AgencyV\u00e1clavFumero F, Sigut J, Alayon S, Gonzalez-Hernandez M, Gonzalez de la Rosa M (2015). Interactive tool and database for optic disc and cup segmentation of stereo and monocular retinal fundus images. In: Short communications proceedings. Plzen, Czech Republic: V\u00e1clav Skala -UNION Agency.\n\nDR HAGIS -a fundus image database for the automatic extraction of retinal surface vessels from diabetic patients. S Holm, G Russell, V Nourrit, N Mcloughlin, J Med Imaging Bellingham. 4Holm S, Russell G, Nourrit V, McLoughlin N (2017). DR HAGIS -a fundus image database for the automatic extraction of retinal surface vessels from diabetic patients. J Med Imaging Bellingham 4:1- 11.\n\nRetinal vessel segmentation by improved matched filtering: evaluation on a new high-resolution fundus image database. J Odstrcilik, R Kolar, A Budai, J Hornegger, Jan J Gazarek, J Kubena, T Cernosek, P Svoboda, O Angelopoulou, E , IET Image Process. 7Odstrcilik J, Kolar R, Budai A, Hornegger J, Jan J, Gazarek J, Kubena T, Cernosek P, Svoboda O, Angelopoulou E (2013). Retinal vessel segmentation by improved matched filtering: evaluation on a new high-resolution fundus image database. IET Image Process 7:373-83.\n\nREFUGE Challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs. J I Orlando, H Fu, J Barbosa Breda, K Van Keer, D R Bathula, A Diaz-Pinto, R Fang, P A Heng, J Kim, J Lee, J Lee, X Li, P Liu, S Lu, B Murugesan, V Naranjo, Ssr Phaye, S M Shankaranarayana, A Sikka, J Son, A Van Den Hengel, S Wang, J Wu, Z Wu, G Xu, Y Xu, P Yin, F Li, X Zhang, Y Xu, H Bogunovi\u0107, Med Image Anal. 59101570Orlando JI, Fu H, Barbosa Breda J, van Keer K, Bathula DR, Diaz-Pinto A, Fang R, Heng PA, Kim J, Lee J, Lee J, Li X, Liu P, Lu S, Murugesan B, Naranjo V, Phaye SSR, Shankaranarayana SM, Sikka A, Son J, van den Hengel A, Wang S, Wu J, Wu Z, Xu G, Xu Y, Yin P, Li F, Zhang X, Xu Y, Bogunovi\u0107 H (2020). REFUGE Challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs. Med Image Anal 59:101570.\n\nTowards a glaucoma risk index based on simulated hemodynamics from fundus images. J I Orlando, K Van Keer, Barbosa Breda, J Blaschko, M B Blanco, P Bulant, C A , Medical Image Computing and Computer Assisted Intervention -MICCAI. Frangi AF, Schnabel JA, Davatzikos C, Alberola-Lopez C, Fichtinger GSpringer International PublishingLecture Notes in Computer ScienceOrlando JI, van Keer K, Barbosa Breda J, Blaschko MB, Blanco P, Bulant CA (2018). Towards a glaucoma risk index based on simulated hemodynamics from fundus images. In: Frangi AF, Schnabel JA, Davatzikos C, Alberola-Lopez C, Fichtinger G, eds., Medical Image Computing and Computer Assisted Intervention -MICCAI 2018, Lecture Notes in Computer Science. Springer International Publishing, 65-73.\n\nDrishti-GS: Retinal image dataset for optic nerve head (ONH) segmentation. J Sivaswamy, S Krishnadas, Datt Joshi, G Jain, M , 2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI). Sivaswamy J, Krishnadas S, Datt Joshi G, Jain M, Syed Tabish A (2014). Drishti-GS: Retinal image dataset for optic nerve head (ONH) segmentation. In: 2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI).\n\nClassification of Findings with Localized Lesions in Fundoscopic Images Using a Regionally Guided CNN. J Son, W Bae, S Kim, S J Park, K H Jung, D Stoyanov, Z Taylor, F Ciompi, Y Xu, A Martel, L Maier-Hein, N Rajpoot, J Van Der Laak, M Veta, S Mckenna, D Snead, E Trucco, M K Garvin, X J Chen, Computational Pathology and Ophthalmic Medical Image Analysis. Bogunovic HSpringer International PublishingSon J, Bae W, Kim S, Park SJ, Jung KH (2018). Classification of Findings with Localized Lesions in Fundoscopic Images Using a Regionally Guided CNN. In: Stoyanov D, Taylor Z, Ciompi F, Xu Y, Martel A, Maier-Hein L, Rajpoot N, van der Laak J, Veta M, McKenna S, Snead D, Trucco E, Garvin MK, Chen XJ, Bogunovic H, eds., Computational Pathology and Ophthalmic Medical Image Analysis, Lecture Notes in Computer Science. Springer International Publishing, 176-84.\n\nGlobal prevalence of glaucoma and projections of glaucoma burden through 2040: a systematic review and metaanalysis. Y C Tham, X Li, T Y Wong, H A Quigley, T Aung, C Y Cheng, Ophthalmology. 121Tham YC, Li X, Wong TY, Quigley HA, Aung T, Cheng CY (2014). Global prevalence of glaucoma and projections of glaucoma burden through 2040: a systematic review and meta- analysis. Ophthalmology 121:2081-90.\n\nValidating retinal fundus image analysis algorithms: issues and a proposal. E Trucco, A Ruggeri, T Karnowski, L Giancardo, E Chaum, J P Hubschman, B Al-Diri, C Y Cheung, D Wong, M Abr\u00e0moff, G Lim, D Kumar, P Burlina, N M Bressler, H F Jelinek, F Meriaudeau, G Quellec, T Macgillivray, B Dhillon, Invest Ophth Vis Sci. 54Trucco E, Ruggeri A, Karnowski T, Giancardo L, Chaum E, Hubschman JP, Al-Diri B, Cheung CY, Wong D, Abr\u00e0moff M, Lim G, Kumar D, Burlina P, Bressler NM, Jelinek HF, Meriaudeau F, Quellec G, Macgillivray T, Dhillon B (2013). Validating retinal fundus image analysis algorithms: issues and a proposal. Invest Ophth Vis Sci 54:3546-59.\n\nORIGAlight: An online retinal fundus image database for glaucoma analysis and research. Z Zhang, F S Yin, J Liu, W K Wong, N M Tan, B H Lee, J Cheng, T Y Wong, 2010 Annual International Conference of the IEEE Engineering in Medicine and Biology. Zhang Z, Yin FS, Liu J, Wong WK, Tan NM, Lee BH, Cheng J, Wong TY (2010). ORIGA- light: An online retinal fundus image database for glaucoma analysis and research. In: 2010 Annual International Conference of the IEEE Engineering in Medicine and Biology.\n", "annotations": {"author": "[{\"end\":241,\"start\":95},{\"end\":343,\"start\":242},{\"end\":483,\"start\":344},{\"end\":626,\"start\":484},{\"end\":767,\"start\":627},{\"end\":870,\"start\":768}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":105},{\"end\":262,\"start\":251},{\"end\":354,\"start\":349},{\"end\":497,\"start\":491},{\"end\":639,\"start\":634},{\"end\":789,\"start\":776}]", "author_first_name": "[{\"end\":104,\"start\":95},{\"end\":250,\"start\":242},{\"end\":348,\"start\":344},{\"end\":490,\"start\":484},{\"end\":633,\"start\":627},{\"end\":775,\"start\":768}]", "author_affiliation": "[{\"end\":240,\"start\":133},{\"end\":342,\"start\":264},{\"end\":482,\"start\":375},{\"end\":625,\"start\":518},{\"end\":766,\"start\":659},{\"end\":869,\"start\":791}]", "title": "[{\"end\":88,\"start\":1},{\"end\":958,\"start\":871}]", "venue": "[{\"end\":978,\"start\":960}]", "abstract": "[{\"end\":2463,\"start\":1172}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2756,\"start\":2737},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3966,\"start\":3944},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4457,\"start\":4436},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5150,\"start\":5129},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7117,\"start\":7097},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7455,\"start\":7431},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7588,\"start\":7569},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7619,\"start\":7594},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7654,\"start\":7632},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7989,\"start\":7967},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9085,\"start\":9064},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11049,\"start\":11028},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12716,\"start\":12691},{\"end\":16500,\"start\":16471},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17470,\"start\":17446},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18844,\"start\":18822},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20099,\"start\":20081},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20971,\"start\":20947},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21094,\"start\":21069}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":21796,\"start\":21710},{\"attributes\":{\"id\":\"fig_1\"},\"end\":21903,\"start\":21797},{\"attributes\":{\"id\":\"fig_3\"},\"end\":22003,\"start\":21904},{\"attributes\":{\"id\":\"fig_4\"},\"end\":22091,\"start\":22004},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":22340,\"start\":22092},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":22774,\"start\":22341},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":23292,\"start\":22775}]", "paragraph": "[{\"end\":4209,\"start\":2479},{\"end\":4370,\"start\":4211},{\"end\":4529,\"start\":4372},{\"end\":4761,\"start\":4531},{\"end\":4877,\"start\":4763},{\"end\":4987,\"start\":4879},{\"end\":6910,\"start\":4989},{\"end\":8107,\"start\":6927},{\"end\":8448,\"start\":8109},{\"end\":8899,\"start\":8473},{\"end\":9685,\"start\":8954},{\"end\":10204,\"start\":9687},{\"end\":11160,\"start\":10206},{\"end\":11371,\"start\":11162},{\"end\":11739,\"start\":11373},{\"end\":11877,\"start\":11741},{\"end\":12004,\"start\":11879},{\"end\":12162,\"start\":12006},{\"end\":12280,\"start\":12164},{\"end\":12484,\"start\":12282},{\"end\":12600,\"start\":12486},{\"end\":13615,\"start\":12644},{\"end\":14640,\"start\":13654},{\"end\":15058,\"start\":14642},{\"end\":15355,\"start\":15115},{\"end\":15445,\"start\":15357},{\"end\":15599,\"start\":15447},{\"end\":15909,\"start\":15601},{\"end\":16152,\"start\":15934},{\"end\":16727,\"start\":16154},{\"end\":17247,\"start\":16729},{\"end\":17660,\"start\":17249},{\"end\":17726,\"start\":17662},{\"end\":17769,\"start\":17728},{\"end\":17804,\"start\":17771},{\"end\":17853,\"start\":17806},{\"end\":18766,\"start\":17902},{\"end\":21684,\"start\":18768}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14229,\"start\":14222},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19528,\"start\":19514}]", "section_header": "[{\"end\":2477,\"start\":2465},{\"end\":6925,\"start\":6913},{\"end\":8471,\"start\":8451},{\"end\":8952,\"start\":8902},{\"end\":12642,\"start\":12603},{\"end\":13639,\"start\":13618},{\"end\":13652,\"start\":13642},{\"attributes\":{\"n\":\"2.\"},\"end\":15113,\"start\":15061},{\"end\":15932,\"start\":15912},{\"attributes\":{\"n\":\"4.\"},\"end\":17900,\"start\":17856},{\"end\":21709,\"start\":21687},{\"end\":21719,\"start\":21711},{\"end\":21806,\"start\":21798},{\"end\":21913,\"start\":21905},{\"end\":22013,\"start\":22005},{\"end\":22102,\"start\":22093},{\"end\":22351,\"start\":22342},{\"end\":22785,\"start\":22776}]", "table": "[{\"end\":22340,\"start\":22230},{\"end\":22774,\"start\":22416},{\"end\":23292,\"start\":22934}]", "figure_caption": "[{\"end\":21796,\"start\":21721},{\"end\":21903,\"start\":21808},{\"end\":22003,\"start\":21915},{\"end\":22091,\"start\":22015},{\"end\":22230,\"start\":22104},{\"end\":22416,\"start\":22353},{\"end\":22934,\"start\":22787}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3043,\"start\":3037},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13098,\"start\":13092},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15703,\"start\":15697}]", "bib_author_first_name": "[{\"end\":23357,\"start\":23356},{\"end\":23359,\"start\":23358},{\"end\":23369,\"start\":23368},{\"end\":23371,\"start\":23370},{\"end\":23596,\"start\":23595},{\"end\":23610,\"start\":23609},{\"end\":23619,\"start\":23618},{\"end\":23631,\"start\":23630},{\"end\":23638,\"start\":23637},{\"end\":23650,\"start\":23649},{\"end\":23659,\"start\":23658},{\"end\":23667,\"start\":23666},{\"end\":23678,\"start\":23677},{\"end\":23688,\"start\":23687},{\"end\":23699,\"start\":23698},{\"end\":23710,\"start\":23709},{\"end\":23712,\"start\":23711},{\"end\":24057,\"start\":24056},{\"end\":24071,\"start\":24070},{\"end\":24082,\"start\":24081},{\"end\":24093,\"start\":24092},{\"end\":24103,\"start\":24102},{\"end\":24105,\"start\":24104},{\"end\":24114,\"start\":24113},{\"end\":24397,\"start\":24396},{\"end\":24407,\"start\":24406},{\"end\":24417,\"start\":24416},{\"end\":24419,\"start\":24418},{\"end\":24430,\"start\":24429},{\"end\":24439,\"start\":24438},{\"end\":24871,\"start\":24870},{\"end\":24881,\"start\":24880},{\"end\":24890,\"start\":24889},{\"end\":24900,\"start\":24899},{\"end\":24929,\"start\":24921},{\"end\":24943,\"start\":24942},{\"end\":25427,\"start\":25426},{\"end\":25435,\"start\":25434},{\"end\":25446,\"start\":25445},{\"end\":25457,\"start\":25456},{\"end\":25816,\"start\":25815},{\"end\":25830,\"start\":25829},{\"end\":25839,\"start\":25838},{\"end\":25848,\"start\":25847},{\"end\":25863,\"start\":25860},{\"end\":25865,\"start\":25864},{\"end\":25876,\"start\":25875},{\"end\":25886,\"start\":25885},{\"end\":25898,\"start\":25897},{\"end\":25909,\"start\":25908},{\"end\":25925,\"start\":25924},{\"end\":26335,\"start\":26334},{\"end\":26337,\"start\":26336},{\"end\":26348,\"start\":26347},{\"end\":26354,\"start\":26353},{\"end\":26371,\"start\":26370},{\"end\":26383,\"start\":26382},{\"end\":26385,\"start\":26384},{\"end\":26396,\"start\":26395},{\"end\":26410,\"start\":26409},{\"end\":26418,\"start\":26417},{\"end\":26420,\"start\":26419},{\"end\":26428,\"start\":26427},{\"end\":26435,\"start\":26434},{\"end\":26442,\"start\":26441},{\"end\":26449,\"start\":26448},{\"end\":26455,\"start\":26454},{\"end\":26462,\"start\":26461},{\"end\":26468,\"start\":26467},{\"end\":26481,\"start\":26480},{\"end\":26494,\"start\":26491},{\"end\":26503,\"start\":26502},{\"end\":26505,\"start\":26504},{\"end\":26525,\"start\":26524},{\"end\":26534,\"start\":26533},{\"end\":26541,\"start\":26540},{\"end\":26559,\"start\":26558},{\"end\":26567,\"start\":26566},{\"end\":26573,\"start\":26572},{\"end\":26579,\"start\":26578},{\"end\":26585,\"start\":26584},{\"end\":26591,\"start\":26590},{\"end\":26598,\"start\":26597},{\"end\":26604,\"start\":26603},{\"end\":26613,\"start\":26612},{\"end\":26619,\"start\":26618},{\"end\":27185,\"start\":27184},{\"end\":27187,\"start\":27186},{\"end\":27198,\"start\":27197},{\"end\":27216,\"start\":27209},{\"end\":27225,\"start\":27224},{\"end\":27237,\"start\":27236},{\"end\":27239,\"start\":27238},{\"end\":27249,\"start\":27248},{\"end\":27259,\"start\":27258},{\"end\":27261,\"start\":27260},{\"end\":27937,\"start\":27936},{\"end\":27950,\"start\":27949},{\"end\":27967,\"start\":27963},{\"end\":27976,\"start\":27975},{\"end\":27984,\"start\":27983},{\"end\":28380,\"start\":28379},{\"end\":28387,\"start\":28386},{\"end\":28394,\"start\":28393},{\"end\":28401,\"start\":28400},{\"end\":28403,\"start\":28402},{\"end\":28411,\"start\":28410},{\"end\":28413,\"start\":28412},{\"end\":28421,\"start\":28420},{\"end\":28433,\"start\":28432},{\"end\":28443,\"start\":28442},{\"end\":28453,\"start\":28452},{\"end\":28459,\"start\":28458},{\"end\":28469,\"start\":28468},{\"end\":28483,\"start\":28482},{\"end\":28494,\"start\":28493},{\"end\":28510,\"start\":28509},{\"end\":28518,\"start\":28517},{\"end\":28529,\"start\":28528},{\"end\":28538,\"start\":28537},{\"end\":28548,\"start\":28547},{\"end\":28550,\"start\":28549},{\"end\":28560,\"start\":28559},{\"end\":28562,\"start\":28561},{\"end\":29255,\"start\":29254},{\"end\":29257,\"start\":29256},{\"end\":29265,\"start\":29264},{\"end\":29271,\"start\":29270},{\"end\":29273,\"start\":29272},{\"end\":29281,\"start\":29280},{\"end\":29283,\"start\":29282},{\"end\":29294,\"start\":29293},{\"end\":29302,\"start\":29301},{\"end\":29304,\"start\":29303},{\"end\":29615,\"start\":29614},{\"end\":29625,\"start\":29624},{\"end\":29636,\"start\":29635},{\"end\":29649,\"start\":29648},{\"end\":29662,\"start\":29661},{\"end\":29671,\"start\":29670},{\"end\":29673,\"start\":29672},{\"end\":29686,\"start\":29685},{\"end\":29697,\"start\":29696},{\"end\":29699,\"start\":29698},{\"end\":29709,\"start\":29708},{\"end\":29717,\"start\":29716},{\"end\":29729,\"start\":29728},{\"end\":29736,\"start\":29735},{\"end\":29745,\"start\":29744},{\"end\":29756,\"start\":29755},{\"end\":29758,\"start\":29757},{\"end\":29770,\"start\":29769},{\"end\":29772,\"start\":29771},{\"end\":29783,\"start\":29782},{\"end\":29797,\"start\":29796},{\"end\":29808,\"start\":29807},{\"end\":29824,\"start\":29823},{\"end\":30280,\"start\":30279},{\"end\":30289,\"start\":30288},{\"end\":30291,\"start\":30290},{\"end\":30298,\"start\":30297},{\"end\":30305,\"start\":30304},{\"end\":30307,\"start\":30306},{\"end\":30315,\"start\":30314},{\"end\":30317,\"start\":30316},{\"end\":30324,\"start\":30323},{\"end\":30326,\"start\":30325},{\"end\":30333,\"start\":30332},{\"end\":30342,\"start\":30341},{\"end\":30344,\"start\":30343}]", "bib_author_last_name": "[{\"end\":23366,\"start\":23360},{\"end\":23377,\"start\":23372},{\"end\":23607,\"start\":23597},{\"end\":23616,\"start\":23611},{\"end\":23628,\"start\":23620},{\"end\":23635,\"start\":23632},{\"end\":23647,\"start\":23639},{\"end\":23656,\"start\":23651},{\"end\":23664,\"start\":23660},{\"end\":23675,\"start\":23668},{\"end\":23685,\"start\":23679},{\"end\":23696,\"start\":23689},{\"end\":23707,\"start\":23700},{\"end\":23718,\"start\":23713},{\"end\":24068,\"start\":24058},{\"end\":24079,\"start\":24072},{\"end\":24090,\"start\":24083},{\"end\":24100,\"start\":24094},{\"end\":24111,\"start\":24106},{\"end\":24120,\"start\":24115},{\"end\":24404,\"start\":24398},{\"end\":24414,\"start\":24408},{\"end\":24427,\"start\":24420},{\"end\":24436,\"start\":24431},{\"end\":24458,\"start\":24440},{\"end\":24878,\"start\":24872},{\"end\":24887,\"start\":24882},{\"end\":24897,\"start\":24891},{\"end\":24919,\"start\":24901},{\"end\":24940,\"start\":24930},{\"end\":25432,\"start\":25428},{\"end\":25443,\"start\":25436},{\"end\":25454,\"start\":25447},{\"end\":25468,\"start\":25458},{\"end\":25827,\"start\":25817},{\"end\":25836,\"start\":25831},{\"end\":25845,\"start\":25840},{\"end\":25858,\"start\":25849},{\"end\":25873,\"start\":25866},{\"end\":25883,\"start\":25877},{\"end\":25895,\"start\":25887},{\"end\":25906,\"start\":25899},{\"end\":25922,\"start\":25910},{\"end\":26345,\"start\":26338},{\"end\":26351,\"start\":26349},{\"end\":26368,\"start\":26355},{\"end\":26380,\"start\":26372},{\"end\":26393,\"start\":26386},{\"end\":26407,\"start\":26397},{\"end\":26415,\"start\":26411},{\"end\":26425,\"start\":26421},{\"end\":26432,\"start\":26429},{\"end\":26439,\"start\":26436},{\"end\":26446,\"start\":26443},{\"end\":26452,\"start\":26450},{\"end\":26459,\"start\":26456},{\"end\":26465,\"start\":26463},{\"end\":26478,\"start\":26469},{\"end\":26489,\"start\":26482},{\"end\":26500,\"start\":26495},{\"end\":26522,\"start\":26506},{\"end\":26531,\"start\":26526},{\"end\":26538,\"start\":26535},{\"end\":26556,\"start\":26542},{\"end\":26564,\"start\":26560},{\"end\":26570,\"start\":26568},{\"end\":26576,\"start\":26574},{\"end\":26582,\"start\":26580},{\"end\":26588,\"start\":26586},{\"end\":26595,\"start\":26592},{\"end\":26601,\"start\":26599},{\"end\":26610,\"start\":26605},{\"end\":26616,\"start\":26614},{\"end\":26629,\"start\":26620},{\"end\":27195,\"start\":27188},{\"end\":27207,\"start\":27199},{\"end\":27222,\"start\":27217},{\"end\":27234,\"start\":27226},{\"end\":27246,\"start\":27240},{\"end\":27256,\"start\":27250},{\"end\":27947,\"start\":27938},{\"end\":27961,\"start\":27951},{\"end\":27973,\"start\":27968},{\"end\":27981,\"start\":27977},{\"end\":28384,\"start\":28381},{\"end\":28391,\"start\":28388},{\"end\":28398,\"start\":28395},{\"end\":28408,\"start\":28404},{\"end\":28418,\"start\":28414},{\"end\":28430,\"start\":28422},{\"end\":28440,\"start\":28434},{\"end\":28450,\"start\":28444},{\"end\":28456,\"start\":28454},{\"end\":28466,\"start\":28460},{\"end\":28480,\"start\":28470},{\"end\":28491,\"start\":28484},{\"end\":28507,\"start\":28495},{\"end\":28515,\"start\":28511},{\"end\":28526,\"start\":28519},{\"end\":28535,\"start\":28530},{\"end\":28545,\"start\":28539},{\"end\":28557,\"start\":28551},{\"end\":28567,\"start\":28563},{\"end\":29262,\"start\":29258},{\"end\":29268,\"start\":29266},{\"end\":29278,\"start\":29274},{\"end\":29291,\"start\":29284},{\"end\":29299,\"start\":29295},{\"end\":29310,\"start\":29305},{\"end\":29622,\"start\":29616},{\"end\":29633,\"start\":29626},{\"end\":29646,\"start\":29637},{\"end\":29659,\"start\":29650},{\"end\":29668,\"start\":29663},{\"end\":29683,\"start\":29674},{\"end\":29694,\"start\":29687},{\"end\":29706,\"start\":29700},{\"end\":29714,\"start\":29710},{\"end\":29726,\"start\":29718},{\"end\":29733,\"start\":29730},{\"end\":29742,\"start\":29737},{\"end\":29753,\"start\":29746},{\"end\":29767,\"start\":29759},{\"end\":29780,\"start\":29773},{\"end\":29794,\"start\":29784},{\"end\":29805,\"start\":29798},{\"end\":29821,\"start\":29809},{\"end\":29832,\"start\":29825},{\"end\":30286,\"start\":30281},{\"end\":30295,\"start\":30292},{\"end\":30302,\"start\":30299},{\"end\":30312,\"start\":30308},{\"end\":30321,\"start\":30318},{\"end\":30330,\"start\":30327},{\"end\":30339,\"start\":30334},{\"end\":30349,\"start\":30345}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7422295},\"end\":23525,\"start\":23294},{\"attributes\":{\"id\":\"b1\"},\"end\":23969,\"start\":23527},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":84841586},\"end\":24326,\"start\":23971},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14779650},\"end\":24755,\"start\":24328},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":220454861},\"end\":25310,\"start\":24757},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":206445429},\"end\":25695,\"start\":25312},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":33036339},\"end\":26212,\"start\":25697},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":203952960},\"end\":27100,\"start\":26214},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":44061096},\"end\":27859,\"start\":27102},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18432155},\"end\":28274,\"start\":27861},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52276002},\"end\":29135,\"start\":28276},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8298120},\"end\":29536,\"start\":29137},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9289936},\"end\":30189,\"start\":29538},{\"attributes\":{\"id\":\"b13\"},\"end\":30690,\"start\":30191}]", "bib_title": "[{\"end\":23354,\"start\":23294},{\"end\":23593,\"start\":23527},{\"end\":24054,\"start\":23971},{\"end\":24394,\"start\":24328},{\"end\":24868,\"start\":24757},{\"end\":25424,\"start\":25312},{\"end\":25813,\"start\":25697},{\"end\":26332,\"start\":26214},{\"end\":27182,\"start\":27102},{\"end\":27934,\"start\":27861},{\"end\":28377,\"start\":28276},{\"end\":29252,\"start\":29137},{\"end\":29612,\"start\":29538},{\"end\":30277,\"start\":30191}]", "bib_author": "[{\"end\":23368,\"start\":23356},{\"end\":23379,\"start\":23368},{\"end\":23609,\"start\":23595},{\"end\":23618,\"start\":23609},{\"end\":23630,\"start\":23618},{\"end\":23637,\"start\":23630},{\"end\":23649,\"start\":23637},{\"end\":23658,\"start\":23649},{\"end\":23666,\"start\":23658},{\"end\":23677,\"start\":23666},{\"end\":23687,\"start\":23677},{\"end\":23698,\"start\":23687},{\"end\":23709,\"start\":23698},{\"end\":23720,\"start\":23709},{\"end\":24070,\"start\":24056},{\"end\":24081,\"start\":24070},{\"end\":24092,\"start\":24081},{\"end\":24102,\"start\":24092},{\"end\":24113,\"start\":24102},{\"end\":24122,\"start\":24113},{\"end\":24406,\"start\":24396},{\"end\":24416,\"start\":24406},{\"end\":24429,\"start\":24416},{\"end\":24438,\"start\":24429},{\"end\":24460,\"start\":24438},{\"end\":24880,\"start\":24870},{\"end\":24889,\"start\":24880},{\"end\":24899,\"start\":24889},{\"end\":24921,\"start\":24899},{\"end\":24942,\"start\":24921},{\"end\":24946,\"start\":24942},{\"end\":25434,\"start\":25426},{\"end\":25445,\"start\":25434},{\"end\":25456,\"start\":25445},{\"end\":25470,\"start\":25456},{\"end\":25829,\"start\":25815},{\"end\":25838,\"start\":25829},{\"end\":25847,\"start\":25838},{\"end\":25860,\"start\":25847},{\"end\":25875,\"start\":25860},{\"end\":25885,\"start\":25875},{\"end\":25897,\"start\":25885},{\"end\":25908,\"start\":25897},{\"end\":25924,\"start\":25908},{\"end\":25928,\"start\":25924},{\"end\":26347,\"start\":26334},{\"end\":26353,\"start\":26347},{\"end\":26370,\"start\":26353},{\"end\":26382,\"start\":26370},{\"end\":26395,\"start\":26382},{\"end\":26409,\"start\":26395},{\"end\":26417,\"start\":26409},{\"end\":26427,\"start\":26417},{\"end\":26434,\"start\":26427},{\"end\":26441,\"start\":26434},{\"end\":26448,\"start\":26441},{\"end\":26454,\"start\":26448},{\"end\":26461,\"start\":26454},{\"end\":26467,\"start\":26461},{\"end\":26480,\"start\":26467},{\"end\":26491,\"start\":26480},{\"end\":26502,\"start\":26491},{\"end\":26524,\"start\":26502},{\"end\":26533,\"start\":26524},{\"end\":26540,\"start\":26533},{\"end\":26558,\"start\":26540},{\"end\":26566,\"start\":26558},{\"end\":26572,\"start\":26566},{\"end\":26578,\"start\":26572},{\"end\":26584,\"start\":26578},{\"end\":26590,\"start\":26584},{\"end\":26597,\"start\":26590},{\"end\":26603,\"start\":26597},{\"end\":26612,\"start\":26603},{\"end\":26618,\"start\":26612},{\"end\":26631,\"start\":26618},{\"end\":27197,\"start\":27184},{\"end\":27209,\"start\":27197},{\"end\":27224,\"start\":27209},{\"end\":27236,\"start\":27224},{\"end\":27248,\"start\":27236},{\"end\":27258,\"start\":27248},{\"end\":27264,\"start\":27258},{\"end\":27949,\"start\":27936},{\"end\":27963,\"start\":27949},{\"end\":27975,\"start\":27963},{\"end\":27983,\"start\":27975},{\"end\":27987,\"start\":27983},{\"end\":28386,\"start\":28379},{\"end\":28393,\"start\":28386},{\"end\":28400,\"start\":28393},{\"end\":28410,\"start\":28400},{\"end\":28420,\"start\":28410},{\"end\":28432,\"start\":28420},{\"end\":28442,\"start\":28432},{\"end\":28452,\"start\":28442},{\"end\":28458,\"start\":28452},{\"end\":28468,\"start\":28458},{\"end\":28482,\"start\":28468},{\"end\":28493,\"start\":28482},{\"end\":28509,\"start\":28493},{\"end\":28517,\"start\":28509},{\"end\":28528,\"start\":28517},{\"end\":28537,\"start\":28528},{\"end\":28547,\"start\":28537},{\"end\":28559,\"start\":28547},{\"end\":28569,\"start\":28559},{\"end\":29264,\"start\":29254},{\"end\":29270,\"start\":29264},{\"end\":29280,\"start\":29270},{\"end\":29293,\"start\":29280},{\"end\":29301,\"start\":29293},{\"end\":29312,\"start\":29301},{\"end\":29624,\"start\":29614},{\"end\":29635,\"start\":29624},{\"end\":29648,\"start\":29635},{\"end\":29661,\"start\":29648},{\"end\":29670,\"start\":29661},{\"end\":29685,\"start\":29670},{\"end\":29696,\"start\":29685},{\"end\":29708,\"start\":29696},{\"end\":29716,\"start\":29708},{\"end\":29728,\"start\":29716},{\"end\":29735,\"start\":29728},{\"end\":29744,\"start\":29735},{\"end\":29755,\"start\":29744},{\"end\":29769,\"start\":29755},{\"end\":29782,\"start\":29769},{\"end\":29796,\"start\":29782},{\"end\":29807,\"start\":29796},{\"end\":29823,\"start\":29807},{\"end\":29834,\"start\":29823},{\"end\":30288,\"start\":30279},{\"end\":30297,\"start\":30288},{\"end\":30304,\"start\":30297},{\"end\":30314,\"start\":30304},{\"end\":30323,\"start\":30314},{\"end\":30332,\"start\":30323},{\"end\":30341,\"start\":30332},{\"end\":30351,\"start\":30341}]", "bib_venue": "[{\"end\":23399,\"start\":23379},{\"end\":23738,\"start\":23720},{\"end\":24139,\"start\":24122},{\"end\":24534,\"start\":24460},{\"end\":24978,\"start\":24946},{\"end\":25494,\"start\":25470},{\"end\":25945,\"start\":25928},{\"end\":26645,\"start\":26631},{\"end\":27330,\"start\":27264},{\"end\":28054,\"start\":27987},{\"end\":28630,\"start\":28569},{\"end\":29325,\"start\":29312},{\"end\":29854,\"start\":29834},{\"end\":30435,\"start\":30351},{\"end\":25022,\"start\":24980},{\"end\":28643,\"start\":28632}]"}}}, "year": 2023, "month": 12, "day": 17}