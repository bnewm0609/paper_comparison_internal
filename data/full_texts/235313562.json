{"id": 235313562, "updated": "2023-10-06 02:11:35.618", "metadata": {"title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification", "authors": "[{\"first\":\"Yongming\",\"last\":\"Rao\",\"middle\":[]},{\"first\":\"Wenliang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Benlin\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Jiwen\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Cho-Jui\",\"last\":\"Hsieh\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "13937-13949", "publication_date": {"year": 2021, "month": 6, "day": 3}, "abstract": "Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31%~37% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. Equipped with the dynamic token sparsification framework, DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/raoyongming/DynamicViT", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.02034", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/RaoZLLZH21", "doi": null}}, "content": {"source": {"pdf_hash": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.02034v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1497bc523975838e0470acae17802a2842527430", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/dbdcabd0444ad50b68ee09e30f39b66e9068f5d2.txt", "contents": "\nDynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification\n\n\nYongming Rao \nTsinghua University\n\n\nWenliang Zhao \nTsinghua University\n\n\nBenlin Liu \nUCLA\n\n\nUniversity of Washington\n\n\nJiwen Lu \nTsinghua University\n\n\nJie Zhou \nTsinghua University\n\n\nCho-Jui Hsieh \nUCLA\n\n\nDynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification\n\nAttention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31% \u223c 37% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. Equipped with the dynamic token sparsification framework, DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/ raoyongming/DynamicViT.\n\nIntroduction\n\nThese years have witnessed the great progress in computer vision brought by the evolution of CNNtype architectures [12,18]. Some recent works start to replace CNN by using transformer for many vision tasks, like object detection [36,20] and classification [25]. Just like what has been done to the CNN-type architectures in the past few years, it is also desirable to accelerate the transformer-like models to make them more suitable for real-time applications.\n\nOne common practice for the acceleration of CNN-type networks is to prune the filters that are of less importance. The way input is processed by the vision transformer and its variants, i.e. splitting the input image into multiple independent patches, provides us another orthogonal way to introduce the sparsity for the acceleration. That is, we can prune the tokens of less importance in the input instance, given the fact that many tokens contribute very little to the final prediction. This is only possible for the transformer-like models where the self-attention module can take the token sequence of variable length as input, and the unstructured pruned input will not affect the self-attention module, while dropping a certain part of the pixels can not really accelerate the convolution operation since the unstructured neighborhood used by convolution would make it difficult to accelerate through parallel computing. Since the hierarchical architecture of CNNs with structural downsampling has improved model efficiency in various vision tasks, we hope to explore the unstructured and data-dependent  Figure 1: Illustration of our main idea. CNN models usually leverage the structural downsampling strategy to build hierarchical architectures as shown in (a). unstructured and data-dependent downsampling method in (b) can better exploit the sparsity in the input data. Thanks to the nature of the self-attention operation, the unstructured token set is also easy to accelerate through parallel computing. (c) visualizes the impact of each spatial location on the final prediction in the DeiT-S model [25] using the visualization method proposed in [3]. These results demonstrate the final prediction in vision transformers is only based on a subset of most informative tokens, which suggests a large proportion of tokens can be removed without hurting the performance.\n\ndownsampling strategy for vision transformers to further leverage the advantages of self-attention (our experiments also show unstructured sparsification can lead to better performance for vision transformers compared to structural downsampling). The basic idea of our method is illustrated in Figure 1.\n\nIn this work, we propose to employ a lightweight prediction module to determine which tokens to be pruned in a dynamic way, dubbed as DynamicViT. In particular, for each input instance, the prediction module produces a customized binary decision mask to decide which tokens are uninformative and need to be abandoned. This module is added to multiple layers of the vision transformer, such that the sparsification can be performed in a hierarchical way as we gradually increase the amount of pruned tokens after each prediction module. Once a token is pruned after a certain layer, it will not be ever used in the feed-forward procedure. The additional computational overhead introduced by this lightweight module is quite small, especially considering the computational overhead saved by eliminating the uninformative tokens.\n\nThis prediction module can be optimized jointly in an end-to-end manner together with the vision transformer backbone. To this end, two specialized strategies are adopted. The first one is to adopt Gumbel-Softmax [15] to overcome the non-differentiable problem of sampling from a distribution so that it is possible to perform the end-to-end training. The second one is about how to apply this learned binary decision mask to prune the unnecessary tokens. Considering the number of zero elements in the binary decision mask is different for each instance, directly eliminating the uninformative tokens for each input instance during training will make parallel computing impossible. Moreover, this would also hinder the back-propagation for the prediction module, which needs to calculate the probability distribution of whether to keep the token even if it is finally eliminated. Besides, directly setting the abandoned tokens as zero vectors is also not a wise idea since zero vectors will still affect the calculation of the attention matrix. Therefore, we propose a strategy called attention masking where we drop the connection from abandoned tokens to all other tokens in the attention matrix based on the binary decision mask. By doing so, we can overcome the difficulties described above. We also modify the original training objective of the vision transformer by adding a term to constrain the proportion of pruned tokens after a certain layer. During the inference phase, we can directly abandon a fixed amount of tokens after certain layers for each input instance as we no longer need to consider whether the operation is differentiable, and this will greatly accelerate the inference.\n\nWe illustrate the effectiveness of our method on ImageNet using DeiT [25] and LV-ViT [16] as backbone. The experimental results demonstrate the competitive trade-off between speed and accuracy. In particular, by hierarchically pruning 66% of the input tokens, we can greatly reduce 31% \u223c 37% GFLOPs and improve the throughput by over 40% while the drop of accuracy is within 0.5% for all different vision transformers. Our DynamicViT demonstrates the possibility of exploiting the sparsity in space for the acceleration of transformer-like model. We expect our attempt to open a new path for future work on the acceleration of transformer-like models.\n\n\nRelated Work\n\nVision transformers. Transformer model is first widely studied in NLP community [26]. It proves the possibility to use self-attention to replace the recurrent neural networks and their variants. Recent progress has demonstrated the variants of transformers can also be a competitive alternative to CNNs and achieve promising results on different vision tasks including image classification [8,25,20,35,23], object detection [2], semantic segmentation [34,5] and 3D analysis [31,33]. DETR [2] is the first work to apply the transformer model to vision tasks. It formulates the object detection task as a set prediction problem and follows the encoder-decoder design in the transformer to generate a sequence of bounding boxes. ViT [8] is the first work to directly apply transformer architecture on non-overlapping image patches for the image classification task, and the whole framework contains no convolution operation. Compared to CNN-type models, ViT can achieve better performance with large-scale pre-training. It is really preferred if the architecture can achieve the state-of-the-art without any pre-training. DeiT [25] proposes many training techniques so that we can train the convolutionfree transformer only on ImageNet1K [7] and achieve better performance than ViT. LV-ViT [16] further improves the performance by introducing a new training objective called token labeling. Both ViT and its follow-ups split the input image into multiple independent image patches and transform these image patches into tokens for further process. This makes it feasible to incorporate the sparsity in space dimension for these transformer-like models.\n\nModel acceleration. Model acceleration techniques are important for the deployment of deep models on edge devices. There are many techniques can be used to accelerate the inference speed of deep model, including quantization [9,27], pruning [13,22], low-rank factorization [30], knowledge distillation [14,19] and so on. There are also many works aims at accelerating the inference speed of transformer models. For example, TinyBERT [17] proposes a distillation method to accelerate the inference of transformer. Star-Transformer [10] reduces quadratic space and time complexity to linear by replacing the fully connected structure with a star-shaped topology. However, all these works focus on NLP tasks, and few works explore the possibility of making use of the characteristic of vision tasks to accelerate vision transformer. Furthermore, the difference between the characteristics of Transformer and CNN also makes it possible to adopt another way for acceleration rather than the methods used for CNN acceleration like filter pruning [13], which removes non-critical or redundant neurons from a deep model. Our method aims at pruning the tokens of less importance instead of the neurons by exploiting the sparsity of informative image patches.\n\n\nDynamic Vision Transformers\n\n\nOverview\n\nThe overall framework of our DynamicViT is illustrated in Figure 2. Our DynamicViT consists of a normal vision transformer as the backbone and several prediction modules. The backbone network can be implemented as a wide range of vision transformer (e.g., ViT [8], DeiT [25], LV-ViT [16]). The prediction modules are responsible for generating the probabilities of dropping/keeping the tokens. The token sparsification is performed hierarchically through the whole network at certain locations. For example, given a 12-layer transformer, we can conduct token sparsification before the 4th, 7th, and 10th blocks. During training, the prediction modules and the backbone network can be optimized in an end-to-end manner thanks to our newly devised attention masking strategy. During inference, we only need to select the most informative tokens according to a predefined pruning ratio and the scores computed by the prediction modules. The overall framework of the proposed approach. The proposed prediction module is inserted between the transformer blocks to selectively prune less informative token conditioned on features produced by the previous layer. By doing so, less tokens are processed in the followed layers.\n\n\nHierarchical Token Sparsification with Prediction Modules\n\nAn important characteristic of our DynamicViT is that the token sparsification is performed hierarchically, i.e., we gradually drop the uninformative tokens as the computation proceeds. To achieve this, we maintain a binary decision maskD \u2208 {0, 1} N to indicate whether to drop or keep each token, where N = HW is the number of patch embeddings 2 . We initialize all elements in the decision mask to 1 and update the mask progressively. The prediction modules take the current decisionD and the tokens x \u2208 R N \u00d7C as input. We first project the tokens using an MLP:\nz local = MLP(x) \u2208 R N \u00d7C ,(1)\nwhere C can be a smaller dimension and we use C = C/2 in our implementation. Similarly, we can compute a global feature by:\nz global = Agg(MLP(x),D) \u2208 R C ,(2)\nwhere Agg is the function which aggregate the information all the existing tokens and can be simply implemented as an average pooling:\nAgg(u,D) = N i=1D i u i N i=1D i , u \u2208 R N \u00d7C .(3)\nThe local feature encodes the information of a certain token while the global feature contains the context of the whole image, thus both of them are informative. Therefore, we combine both the local and global features to obtain local-global embeddings and feed them to another MLP to predict the probabilities to drop/keep the tokens:\nz i = [z local i , z global i ], 1 \u2264 i \u2264 N,(4)\u03c0 = Softmax(MLP(z)) \u2208 R N \u00d72 ,(5)\nwhere \u03c0 i,0 denotes the probability of dropping the i-th token and \u03c0 i,1 is the probability of keeping it.\n\nWe can then generate current decision D by sampling from \u03c0 and updateD b\u0177\nD \u2190D D,(6)\nwhere is the Hadamard product, indicating that once a token is dropped, it will never be used.\n\n\nEnd-to-end Optimization with Attention Masking\n\nAlthough our target is to perform token sparsification, we find it non-trivial to implement in practice during training. First, the sampling from \u03c0 to get binary decision mask D is is non-differentiable, which impedes the end-to-end training. To overcome this, we apply the Gumbel-Softmax technique [15] to sample from the probabilities \u03c0:\nD = Gumbel-Softmax(\u03c0) * ,1 \u2208 {0, 1} N ,(7)\nwhere we use the index \"1\" because D represents the mask of the kept tokens. The output of Gumbel-Softmax is a one-hot tensor, of which the expectation equals \u03c0 exactly. Meanwhile, Gumbel-Softmax is differentiable thus makes it possible for end-to-end training.\n\nThe second obstacle comes when we try to prune the tokens during training. The decision maskD is usually unstructured and the masks for different samples contain various numbers of 1's. Therefore, simply discarding the tokens whereD i = 0 would result in a non-uniform number of tokens for samples within a batch, which makes it hard to parallelize the computation. Thus, we must keep the number of tokens unchanged, while cut down the interactions between the pruned tokens and other tokens. We also find that merely zero-out the tokens to be dropped using the binary maskD is not feasible, because in the calculation of self-attention matrix [26] \nA = Softmax QK T \u221a C(8)\nthe zeroed tokens will still influence other tokens through the Softmax operation. To this end, we devise a strategy called attention masking which can totally eliminate the effects of the dropped tokens. Specifically, we compute the attention matrix by:\nP = QK T / \u221a C \u2208 R N \u00d7N ,(9)G ij = 1, i = j, D j , i = j. 1 \u2264 i, j \u2264 N,(10)A ij = exp(P ij )G ij N k=1 exp(P ik )G ik , 1 \u2264 i, j \u2264 N.(11)\nBy Equation (10) we construct a graph where G ij = 1 means the j-th token will contribute to the update of the i-th token. Note that we explicitly add a self-loop to each token to improve numerically stability. It is also easy to show the self-loop does not influence the results: ifD j = 0, the j-th token will not contribute to any tokens other than itself. Equation (11) computes the masked attention matrix\u00c3, which is equivalent to the attention matrix calculated by considering only the kept tokens but has a constant shape N \u00d7 N during training.\n\n\nTraining and Inference\n\nWe now describe the training objectives of our DynamicViT. The training of DynamicViT includes training the prediction modules such that they can produce favorable decisions and fine-tuning the backbone to make it adapt to token sparsification. Assuming we are dealing with a minibatch of B samples, we adopt the standard cross-entropy loss:\nL cls = CrossEntropy(y,\u0233),(12)\nwhere y is the prediction of the DynamicViT (after softmax) and\u0233 is the ground truth.\n\nTo minimize the influence on performance caused by our token sparsification, we use the original backbone network as a teacher model and hope the behavior of our DynamicViT as close to the teacher model as possible. Specifically, we consider this constraint from two aspects. First, we make the finally remaining tokens of the DynamicViT close to the ones of the teacher model, which can be viewed as a kind of self-distillation:  where t i and t i denotes the i-th token after the last block of the DynamicViT and the teacher model, respectively.D b,s is the decision mask for the b-th sample at the s-th sparsification stage. Second, we minimize the difference of the predictions between our DynamicViT and its teacher via the KL divergence:\nL distill = 1 B b=1 N i=1D b,S i B b=1 N i=1D b,S i (t i \u2212 t i ) 2 ,(13)L KL = KL (y y ) ,(14)\nwhere y is the prediction of the teacher model.\n\nFinally, we want to constrain the ratio of the kept tokens to a predefined value. Given a set of target ratios for S stages \u03c1 = [\u03c1 (1) , . . . , \u03c1 (S) ], we utilize an MSE loss to supervise the prediction module:\nL ratio = 1 BS B b=1 S s=1 \u03c1 (s) \u2212 1 N N i=1D b,s i 2 .(15)\nThe full training objective is a combination of the above objectives:\nL = L cls + \u03bb KL L KL + \u03bb distill L distill + \u03bb ratio L ratio ,(16)\nwhere we set \u03bb KL = 0.5, \u03bb distill = 0.5, \u03bb ratio = 2 in all our experiments.\n\nDuring inference, given the target ratio \u03c1, we can directly discard the less informative tokens via the probabilities produced by the prediction modules such that only exact m s = \u03c1 s N tokens are kept at the s-th stage. Formally, for the s-th stage, let\nI s = argsort(\u03c0 * ,1 )(17)\nbe the indices sorted by the keeping probabilities \u03c0 * ,1 , we can then keep the tokens of which the indices lie in I s 1:m s while discarding the others. In this way, our DynamicViT prunes less informative tokens dynamically at runtime, thus can reduce the computational costs during inference.\n\n\nExperimental Results\n\nIn this section, we will demonstrate the superiority of the proposed DynamicViT through extensive experiments. In all of our experiments, we fix the number of sparsification stages S = 3 and apply the target keeping ratio \u03c1 as a geometric sequence [\u03c1, \u03c1 2 , \u03c1 3 ] where \u03c1 ranges from (0, 1). During training DynamicViT models, we follow most of the training techniques used in DeiT [25]. We use the pre-trained vision transformer models to initialize the backbone models and jointly train the whole model for 30 epochs. We set the learning rate of the prediction module to batch size 1024 \u00d7 0.001 and use 0.01\u00d7 learning rate for the backbone model. We fix the weights of the backbone models in the first 5 epochs. All of our models are trained on a single machine with 8 GPUs. Other training setups and details can be found in the supplementary material.  \n\n\nMain results\n\nOne of the most advantages of the DynamicViT is that it can be applied to a wide range of vision transformer architectures to reduce the computational complexity with minor loss of performance. In Table 1, we summarize the main results on ImageNet [7] where we evaluate our DynamicViT used three base models (DeiT-S [25], LV-ViT-S [16] and LV-ViT-M [16]). We report the top-1 accuracy, FLOPs, and the throughput under different keeping ratios \u03c1. Note that our token sparsification is performed hierarchically in three stages, there are only N \u03c1 3 tokens left after the last stage. The throughput is measured on a single NVIDIA RTX 3090 GPU with batch size fixed to 32. We demonstrate that our DynamicViT can reduce the computational costs by 31% \u223c 37% and accelerate the inference at runtime by 43% \u223c 54%, with the neglectable influence of performance (\u22120.2% \u223c \u22120.5%).\n\n\nComparisons with the-state-of-the-arts\n\nIn Table 2, we compare the DynamicViT with the state-of-the-art models in image classification, including convolutional networks and transformer-like architectures. We use the DynamicViT with LV-ViT [16] as the base model and use the \"/\u03c1\" to indicate the keeping ratio. We observe that our DynamicViT exhibits favorable complexity/accuracy trade-offs at all three complexity levels. Notably, we find our DynamicViT-LV-M/0.7 beats the EfficientNet-B5 [24] and NFNet-F0 [1], which are two of the current state-of-the-arts CNN architectures. This can also be shown clearer in Figure 3, where we plot the FLOPS-accuracy curve of DynamicViT series (where we use DyViT for short), along with other state-of-the-art models. We can also observe that DynamicViT can achieve better trade-offs than LV-ViT series, which strongly demonstrates the effectiveness of our method.\n\n\nAnalysis\n\nDynamicViT for model scaling. The success of EfficientNet [24] shows that we can obtain a model with better complexity/accuracy tradeoffs by scaling the model along different dimensions. While in vision transformers, the most commonly used method to scale the model is to change the number of channels, our DynamicViT provides another powerful tool to perform token sparsification. We analysis this nice property of DynamicViT in Figure 4. First, we train several DeiT [25] models with the embedding dimension varying from 192 (DeiT-Ti) to 384 (DeiT-S). Second, we train our DynamicViT based on those models with the keeping ratio \u03c1 = 0.7. We find that after performing token sparsification, the complexity of the model is reduced to be similar to its variant with a smaller embedding dimension. Specifically, we observe that by applying our DynamicViT to DeiT-256, we Table 2: Comparisons with the state-of-the-arts on ImageNet. We compare our DynamicViT models with state-of-the-art image classifciation models with comparable FLOPs and number of parameters. We use the DynamicViT with LV-ViT [16] as the base model and use the \"/\u03c1\" to indicate the keeping ratio. We also include the results of LV-ViT models as references.\n\nModel obtain a model that has a comparable computational complexity to DeiT-Ti, but enjoys around 4.3% higher ImageNet top-1 accuracy.\n\nVisualizations. To further investigate the behavior of DynamicViT, we visualize the sparsification procedure in Figure 5. We show the original input image and the sparsification results after the three stages, where the masks represent the corresponding tokens are discarded. We find that through the hierarchically token sparsification, our DynamicViT can gradually drop the uninformative tokens and finally focus on the objects in the images. This phenomenon also suggests that the DynamicViT leads to better interpretability, i.e., it can locate the important parts in the image which contribute most to the classification step-by-step.\n\nBesides the sample-wise visualization we have shown above, we are also interested in the statistical characteristics of the sparsification decisions, i.e., what kind of general patterns does the DynamicViT learn from the dataset? We then use the DynamicViT to generate the decisions for all the images in the ImageNet validation set and compute the keep probability of each token in all three stages, as shown in Figure 6. We average pool the probability maps into 7 \u00d7 7 such that they can be visualized more easily. Unsurprisingly, we find the tokens in the middle of the image tend to be kept, which is reasonable because in most images the objects are located in the center. We can also find that the later stage generally has lower probabilities to be kept, mainly because that the keeping ratio at the s stage is \u03c1 s , which decreases exponentially as s increases.  Figure 5: Visualization of the progressively sparsified tokens. We show the original input image and the sparsification results after the three stages, where the masks represent the corresponding tokens are discarded. We see our method can gradually focus on the most representative regions in the image. This phenomenon suggests that the DynamicViT has better interpretability.  Effects of different losses. We show the effects of different losses in Table 3. We see the improvement brought by the distillation loss and the KL loss is not very significant, but it can consistently further boost the performance of various models.\n\nComparisons of different sparsification strategies. As illustrated in Figure 2, the dynamic token sparsification is unstructured. To discuss whether the dynamic sparsification is better than other strategies, we perform ablation experiments and the results are shown in Table 4. For the structural downsampling, we perform an average pooling with kernel size 2 \u00d7 2 after the sixth block of the baseline DeiT-S [25] model, which has similar FLOPs to our DynamicViT. The static token sparsification means that the sparsification decisions are not conditioned on the input tokens. We also compare our method with other token removal methods like randomly removing tokens or removing Table 4: Comparisons of different sparsification strategies. We investigate different methods to select redundant tokens based on the DeiT-S model. We report the top-1 accuracy on ImageNet for different methods. We fix the complexity of the accelerated models to 2.9G FLOPs for fair comparisons. tokens based the attention score of the class token. We find through the experiments that although other strategies have similar computational complexities, the proposed dynamic token sparsification method achieves the best accuracy. We also show that the progressive sparsification method is significantly better than one-stage sparsification.\n\nAccelerating larger models. To show the effectiveness of our method on larger models, we apply our method to the model with larger width (i.e., DeiT-B) and models with larger input size (i.e., DeiT-S with 384 \u00d7 384 input). The results are presented in Table 5. We see our method also works well on the larger DeiT model. The accuracy drop become less significant when we apply our method to the model with larger feature maps. Notably, we can reduce the complexity of the DeiT-S model with 384 \u00d7 384 input by over 50% with only 1.3% accuracy drop.\n\n\nConclusion\n\nIn this work, we open a new path to accelerate vision transformer by exploiting the sparsity of informative patches in the input image. For each input instance, our DynamicViT model prunes the tokens of less importance in a dynamic way according to the customized binary decision mask output from the lightweight prediction module, which fuses the local and global information containing in the tokens. The prediction module is added to multiple layers such that the token pruning is performed in a hierarchical way. Gumbel-Softmax and attention masking techniques are also incorporated for the end-to-end training of the transformer model together with the prediction module. During the inference phase, our approach can greatly improves the efficiency by gradually pruning 66% of the input tokens, while the drop of accuracy is less than 0.5% for different transformer backbone. In this paper, we focus on the image classification task. Extending our method to other scenarios like video classification and dense prediction tasks can be interesting directions.\n\n\nA Implementation Details\n\nWe conduct our experiments on the ImageNet (also known as ILSVRC2012) [7] dataset. ImageNet is a commonly used benchmark for image classification. We train our models on the training set, which consists of 1.28M images. The top-1 accuracy is measured on the 50k validation images following common practice [12,25]. To fairly compare with previous methods, we report the single crop results.\n\nWe fix the number of sparsification stages S = 3 in all of our experiments, since this setting can lead to a decent trade-off between complexity and performance. For the sake of simplicity, we set the target keeping ratio \u03c1 as a geometric sequence [\u03c1, \u03c1 2 , \u03c1 3 ], where \u03c1 is the keeping ratio after each sparsifcation ranging from (0, 1). For the prediction module, we use the identical architecture for different stages. We use two LayerNorm \u2192 Linear(C, C/2) \u2192 GELU block to produce z local and z global respectively. We employ a Linear(C, C/2) \u2192 GELU \u2192 Linear(C/2, C/4) \u2192 GELU \u2192 Linear(C/4, 2) \u2192 Softmax block to predict the probabilities.\n\nDuring training our DynamicViT models, we follow most of the training techniques used in DeiT [25]. We use the pre-trained vision transformer models to initialize the backbone models and jointly train the backbone model as well as the prediction modules for 30 epochs. We set the learning rate of the prediction module to batch size 1024 \u00d7 0.001 and use 0.01\u00d7 learning rate for the backbone model. The batch size is adjusted adaptively for different models according to the GPU memory. We fix the weights of the backbone models in the first 5 epochs. All of our models can be trained on a single machine with 8 NVIDIA GTX 1080Ti GPUs.\n\n\nB More Analysis\n\nIn this section, we provide more analysis of our method. We investigate the effects of progressive sparsification, distillation loss, ratio loss, and keeping ratio. We also include more visualization results. The following describes the details of the experiments, results and analysis.\n\nProgressive sparsification. To verify the effectiveness of the progressive sparsification strategy, we test different sparsification methods that result in similar overall complexity. Here we provide more detailed results and more analysis. We find that progressive sparsification is much better than single-shot sparsification. Increasing the number of stages will lead to better performance. Since further increasing the number of stages (> 3) will not lead to significantly better performance but add computation, we use a 3-stage progressive sparsification strategy in our main experiments.\n\n\nTop-1 accuracy (%) GFLOPs\n\nDeiT-S [25] 79. Ablation on the distillation loss and ratio loss. The weights of the distillation losses and ratio loss are the key hyper-parameters in our method. Since the token-wise distillation loss and the KL divergence loss play similar roles in our method, we set \u03bb KL = \u03bb distill in all of our experiments for the sake of simplicity. In this experiment, we fix the keeping ratio \u03c1 to be 0.7. We find our method is not sensitive to these hyper-parameters in general. The proposed ratio loss can encourage the model to reach the desired acceleration rate. Distillation losses can improve the performance after sparsification. We directly apply the best hyper-parameters searched on DeiT-S for all models.\n\nSmaller keeping ratio. We have also tried applying a smaller keeping ratio (larger acceleration rate). The results based on DeiT-S [25] and LV-ViT-S [16] models are presented in the following tables. We see that using \u03c1 < 0.7 will lead to a significant accuracy drop while reducing fewer FLOPs. Since only 22% and 13% tokens are remaining in the last stage when we set \u03c1 to 0.6 and 0.5 respectively, small \u03c1 may cause a significant information loss. Therefore, we use \u03c1 \u2265 0.7 in our main experiments. Jointly scaling \u03c1 and the model width can be a better solution to achieve a large acceleration rate as shown in Figure 4 in the paper. input stage1 stage2 stage3 input stage1 stage2 stage3 input stage1 stage2 stage3 Figure 7: More visual results. The input images are randomly sampled from the validation set of ImageNet. We see our method works well for different images from various categories.\n\nFigure 2 :\n2Figure 2: The overall framework of the proposed approach. The proposed prediction module is inserted between the transformer blocks to selectively prune less informative token conditioned on features produced by the previous layer. By doing so, less tokens are processed in the followed layers.\n\n1 :\n1Main results on ImageNet. We apply our method on three representative vision transformers: DeiT-S, LV-ViT-S and LV-ViT-M. DeiT-S[25] is a widely used vision transformer with the simple architecture. LV-ViT-S and LV-ViT-M[16] are the state-of-the-art vision transformers. We report the top-1 classification accuracy, theoretical complexity in FLOPs and throughput for different ratio \u03c1. The throughput is measured on a single NVIDIA RTX 3090 GPU with batch size fixed to 32.\n\nFigure 3 :\n3Model complexity (FLOPs) and top-1 accuracy trade-offs on ImageNet. We compare Dynam-icViT with the state-of-the-art image classification models. Our models achieve better trade-offs compared to the various vision transformers as well as carefully designed CNN models.\n\nFigure 4 :\n4Comparison of our dynamic token sparsification method with model width scaling. We train our DynamicViT based on DeiT models with embedding dimension varying from 192 to 384 and fix ratio \u03c1 = 0.7. We see dynamic token sparsification is more efficient than commonly used model width scaling.\n\nFigure 6 :\n6The keep probabilities of the tokens at each stage.\n\n:\nResults on larger models. We apply our method to the model with larger width (i.e., DeiT-B) and the model with larger input size (i.e., DeiT-S with 384 \u00d7 384 input).\n\nTable\n\n\nTable 3 :\n3Effects of different losses. We pro-vide the results after removing the distillation \nloss and the KL loss. \n\n\nWe omit the class token for simplicity, while in practice we always keep the class token (i.e., the decision for class token is always \"1\").\n\nHigh-performance largescale image recognition without normalization. Andrew Brock, Soham De, L Samuel, Karen Smith, Simonyan, arXiv:2102.061717arXiv preprintAndrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large- scale image recognition without normalization. arXiv preprint arXiv:2102.06171, 2021. 7, 8\n\nEnd-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, ECCV. 2020Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, pages 213-229, 2020. 3\n\nTransformer interpretability beyond attention visualization. Hila Chefer, Shir Gur, Lior Wolf, arXiv:2012.09838arXiv preprintHila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. arXiv preprint arXiv:2012.09838, 2020. 2\n\nCrossvit: Cross-attention multi-scale vision transformer for image classification. Chun-Fu Chen, Quanfu Fan, Rameswar Panda, arXiv:2103.14899arXiv preprintChun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. arXiv preprint arXiv:2103.14899, 2021. 8\n\nPer-pixel classification is not all you need for semantic segmentation. Bowen Cheng, Alexander G Schwing, Alexander Kirillov, NeurIPS. 3Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. NeurIPS, 2021. 3\n\nConditional positional encodings for vision transformers. Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, Chunhua Shen, arXiv:2102.10882arXiv preprintXiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021. 8\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. 713Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248-255, 2009. 3, 7, 13\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, arXiv:2010.11929An image is worth 16x16 words: Transformers for image recognition at scale. 3arXiv preprintAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3, 8\n\nCompressing deep convolutional networks using vector quantization. Yunchao Gong, Liu Liu, Ming Yang, Lubomir Bourdev, arXiv:1412.6115arXiv preprintYunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014. 3\n\n. Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, Zheng Zhang, arXiv:1902.09113Star-transformer. arXiv preprintQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. Star-transformer. arXiv preprint arXiv:1902.09113, 2019. 3\n\nTransformer in transformer. Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang, arXiv:2103.00112arXiv preprintKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021. 8\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 113Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016. 1, 13\n\nChannel pruning for accelerating very deep neural networks. Yihui He, Xiangyu Zhang, Jian Sun, ICCV. Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In ICCV, pages 1389-1397, 2017. 3\n\nDistilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 3\n\nCategorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, ICLR. 25Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017. 2, 5\n\nToken labeling: Training a 85. Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, Jiashi Feng, arXiv:2104.108585% top-1 accuracy vision transformer with 56m parameters on imagenet. 1314arXiv preprintZihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, and Jiashi Feng. Token labeling: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on imagenet. arXiv preprint arXiv:2104.10858, 2021. 3, 6, 7, 8, 13, 14\n\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Tinybert, arXiv:1909.10351Distilling bert for natural language understanding. arXiv preprintXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019. 3\n\n. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Imagenet classification with deep convolutional neural networks. NeurIPS. 251Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. NeurIPS, 25:1097-1105, 2012. 1\n\nMetadistiller: Network self-boosting via meta-learned top-down distillation. Benlin Liu, Yongming Rao, Jiwen Lu, Jie Zhou, Cho-Jui Hsieh, European Conference on Computer Vision. SpringerBenlin Liu, Yongming Rao, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Metadistiller: Network self-boosting via meta-learned top-down distillation. In European Conference on Computer Vision, pages 694-709. Springer, 2020. 3\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, arXiv:2103.14030arXiv preprintZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021. 1, 3, 8\n\nKaiming He, and Piotr Doll\u00e1r. Designing network design spaces. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, CVPR. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Design- ing network design spaces. In CVPR, pages 10428-10436, 2020. 8\n\nRuntime network routing for efficient image classification. Yongming Rao, Jiwen Lu, Ji Lin, Jie Zhou, IEEE transactions on pattern analysis and machine intelligence. 41Yongming Rao, Jiwen Lu, Ji Lin, and Jie Zhou. Runtime network routing for efficient image classification. IEEE transactions on pattern analysis and machine intelligence, 41(10):2291- 2304, 2018. 3\n\nGlobal filter networks for image classification. Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, Jie Zhou, NeurIPS. Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global filter networks for image classification. In NeurIPS, 2021. 3\n\nEfficientnet: Rethinking model scaling for convolutional neural networks. Mingxing Tan, Quoc Le, PMLRICML. 7Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, pages 6105-6114. PMLR, 2019. 7, 8\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou, arXiv:2012.128771314arXiv preprintHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 3, 6, 7, 8, 9, 13, 14\n\n. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv:1706.0376235arXiv preprintAttention is all you needAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. 3, 5\n\nHaq: Hardware-aware automated quantization with mixed precision. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, Song Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quantization with mixed precision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8612-8620, 2019. 3\n\nPyramid vision transformer: A versatile backbone for dense prediction without convolutions. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao, arXiv:2102.12122arXiv preprintWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021. 8\n\nCo-scale conv-attentional image transformers. Weijian Xu, Yifan Xu, Tyler Chang, Zhuowen Tu, arXiv:2104.06399arXiv preprintWeijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. arXiv preprint arXiv:2104.06399, 2021. 8\n\nOn compressing deep models by low rank and sparse decomposition. Xiyu Yu, Tongliang Liu, Xinchao Wang, Dacheng Tao, CVPR. Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank and sparse decomposition. In CVPR, pages 7370-7379, 2017. 3\n\nPointr: Diverse point cloud completion with geometry-aware transformers. Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, Jie Zhou, ICCV. Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou. Pointr: Diverse point cloud completion with geometry-aware transformers. In ICCV, 2021. 3\n\nTokens-to-token vit: Training vision transformers from scratch on imagenet. Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, E H Francis, Jiashi Tay, Shuicheng Feng, Yan, arXiv:2101.11986arXiv preprintLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021. 8\n\nPoint transformer. Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun, ICCV. Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer. In ICCV, 2021. 3\n\nRethinking semantic segmentation from a sequence-to-sequence perspective with transformers. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, H S Philip, Li Torr, Zhang, CVPR. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In CVPR, 2021. 3\n\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, arXiv:2103.11886Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprintDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021. 3\n\nDeformable detr: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, arXiv:2010.04159arXiv preprintXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. 1\n\nTop-1 acc. (%) GFLOPs. Top-1 acc. (%) GFLOPs\n\nWe provide more visual results in Figure 7. The input images are randomly sampled from the validation set of ImageNet. We see our method works well for different images from various categories. More visual resultsMore visual results. We provide more visual results in Figure 7. The input images are randomly sampled from the validation set of ImageNet. We see our method works well for different images from various categories.\n", "annotations": {"author": "[{\"end\":114,\"start\":79},{\"end\":151,\"start\":115},{\"end\":197,\"start\":152},{\"end\":229,\"start\":198},{\"end\":261,\"start\":230},{\"end\":283,\"start\":262}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":88},{\"end\":128,\"start\":124},{\"end\":162,\"start\":159},{\"end\":206,\"start\":204},{\"end\":238,\"start\":234},{\"end\":275,\"start\":270}]", "author_first_name": "[{\"end\":87,\"start\":79},{\"end\":123,\"start\":115},{\"end\":158,\"start\":152},{\"end\":203,\"start\":198},{\"end\":233,\"start\":230},{\"end\":269,\"start\":262}]", "author_affiliation": "[{\"end\":113,\"start\":93},{\"end\":150,\"start\":130},{\"end\":169,\"start\":164},{\"end\":196,\"start\":171},{\"end\":228,\"start\":208},{\"end\":260,\"start\":240},{\"end\":282,\"start\":277}]", "title": "[{\"end\":76,\"start\":1},{\"end\":359,\"start\":284}]", "venue": null, "abstract": "[{\"end\":1759,\"start\":361}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1894,\"start\":1890},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1897,\"start\":1894},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2008,\"start\":2004},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2011,\"start\":2008},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2035,\"start\":2031},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3854,\"start\":3850},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3901,\"start\":3898},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5470,\"start\":5466},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7026,\"start\":7022},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7042,\"start\":7038},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7705,\"start\":7701},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8014,\"start\":8011},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8017,\"start\":8014},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8020,\"start\":8017},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8023,\"start\":8020},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8026,\"start\":8023},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8048,\"start\":8045},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8076,\"start\":8072},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8078,\"start\":8076},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8099,\"start\":8095},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8102,\"start\":8099},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8112,\"start\":8109},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8354,\"start\":8351},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8749,\"start\":8745},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8859,\"start\":8856},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8912,\"start\":8908},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9500,\"start\":9497},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9503,\"start\":9500},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9517,\"start\":9513},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9520,\"start\":9517},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9549,\"start\":9545},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9578,\"start\":9574},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9581,\"start\":9578},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9709,\"start\":9705},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9806,\"start\":9802},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10316,\"start\":10312},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10827,\"start\":10824},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10838,\"start\":10834},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10851,\"start\":10847},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13843,\"start\":13839},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14834,\"start\":14830},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17313,\"start\":17310},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18657,\"start\":18653},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19395,\"start\":19392},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19464,\"start\":19460},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19479,\"start\":19475},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19497,\"start\":19493},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20258,\"start\":20254},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20509,\"start\":20505},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20526,\"start\":20523},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20993,\"start\":20989},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21404,\"start\":21400},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22030,\"start\":22026},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24852,\"start\":24848},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27486,\"start\":27483},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27723,\"start\":27719},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27726,\"start\":27723},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28547,\"start\":28543},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30026,\"start\":30022},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30862,\"start\":30858},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30880,\"start\":30876},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32071,\"start\":32067},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32163,\"start\":32159}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":31932,\"start\":31625},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32412,\"start\":31933},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32694,\"start\":32413},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32998,\"start\":32695},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33063,\"start\":32999},{\"attributes\":{\"id\":\"fig_6\"},\"end\":33232,\"start\":33064},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33240,\"start\":33233},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33363,\"start\":33241}]", "paragraph": "[{\"end\":2236,\"start\":1775},{\"end\":4118,\"start\":2238},{\"end\":4423,\"start\":4120},{\"end\":5251,\"start\":4425},{\"end\":6951,\"start\":5253},{\"end\":7604,\"start\":6953},{\"end\":9270,\"start\":7621},{\"end\":10521,\"start\":9272},{\"end\":11782,\"start\":10564},{\"end\":12408,\"start\":11844},{\"end\":12563,\"start\":12440},{\"end\":12734,\"start\":12600},{\"end\":13121,\"start\":12786},{\"end\":13308,\"start\":13202},{\"end\":13383,\"start\":13310},{\"end\":13489,\"start\":13395},{\"end\":13879,\"start\":13540},{\"end\":14184,\"start\":13923},{\"end\":14835,\"start\":14186},{\"end\":15114,\"start\":14860},{\"end\":15804,\"start\":15253},{\"end\":16172,\"start\":15831},{\"end\":16289,\"start\":16204},{\"end\":17034,\"start\":16291},{\"end\":17177,\"start\":17130},{\"end\":17391,\"start\":17179},{\"end\":17521,\"start\":17452},{\"end\":17667,\"start\":17590},{\"end\":17923,\"start\":17669},{\"end\":18246,\"start\":17951},{\"end\":19127,\"start\":18271},{\"end\":20012,\"start\":19144},{\"end\":20918,\"start\":20055},{\"end\":22156,\"start\":20931},{\"end\":22292,\"start\":22158},{\"end\":22933,\"start\":22294},{\"end\":24436,\"start\":22935},{\"end\":25758,\"start\":24438},{\"end\":26307,\"start\":25760},{\"end\":27384,\"start\":26322},{\"end\":27803,\"start\":27413},{\"end\":28447,\"start\":27805},{\"end\":29083,\"start\":28449},{\"end\":29389,\"start\":29103},{\"end\":29985,\"start\":29391},{\"end\":30725,\"start\":30015},{\"end\":31624,\"start\":30727}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12439,\"start\":12409},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12599,\"start\":12564},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12785,\"start\":12735},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13168,\"start\":13122},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13201,\"start\":13168},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13394,\"start\":13384},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13922,\"start\":13880},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14859,\"start\":14836},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15143,\"start\":15115},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15190,\"start\":15143},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15252,\"start\":15190},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16203,\"start\":16173},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17107,\"start\":17035},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17129,\"start\":17107},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17451,\"start\":17392},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17589,\"start\":17522},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17950,\"start\":17924}]", "table_ref": "[{\"end\":19348,\"start\":19341},{\"end\":20065,\"start\":20058},{\"end\":21807,\"start\":21800},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24265,\"start\":24258},{\"end\":24715,\"start\":24708},{\"end\":25125,\"start\":25118},{\"end\":26019,\"start\":26012}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1773,\"start\":1761},{\"attributes\":{\"n\":\"2\"},\"end\":7619,\"start\":7607},{\"attributes\":{\"n\":\"3\"},\"end\":10551,\"start\":10524},{\"attributes\":{\"n\":\"3.1\"},\"end\":10562,\"start\":10554},{\"attributes\":{\"n\":\"3.2\"},\"end\":11842,\"start\":11785},{\"attributes\":{\"n\":\"3.3\"},\"end\":13538,\"start\":13492},{\"attributes\":{\"n\":\"3.4\"},\"end\":15829,\"start\":15807},{\"attributes\":{\"n\":\"4\"},\"end\":18269,\"start\":18249},{\"attributes\":{\"n\":\"4.1\"},\"end\":19142,\"start\":19130},{\"attributes\":{\"n\":\"4.2\"},\"end\":20053,\"start\":20015},{\"attributes\":{\"n\":\"4.3\"},\"end\":20929,\"start\":20921},{\"attributes\":{\"n\":\"5\"},\"end\":26320,\"start\":26310},{\"end\":27411,\"start\":27387},{\"end\":29101,\"start\":29086},{\"end\":30013,\"start\":29988},{\"end\":31636,\"start\":31626},{\"end\":31937,\"start\":31934},{\"end\":32424,\"start\":32414},{\"end\":32706,\"start\":32696},{\"end\":33010,\"start\":33000},{\"end\":33066,\"start\":33065},{\"end\":33239,\"start\":33234},{\"end\":33251,\"start\":33242}]", "table": "[{\"end\":33363,\"start\":33289}]", "figure_caption": "[{\"end\":31932,\"start\":31638},{\"end\":32412,\"start\":31939},{\"end\":32694,\"start\":32426},{\"end\":32998,\"start\":32708},{\"end\":33063,\"start\":33012},{\"end\":33232,\"start\":33067},{\"end\":33289,\"start\":33253}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":3358,\"start\":3350},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":4422,\"start\":4414},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10630,\"start\":10622},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20636,\"start\":20628},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21369,\"start\":21361},{\"end\":22414,\"start\":22406},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23356,\"start\":23348},{\"end\":23814,\"start\":23806},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24516,\"start\":24508},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31348,\"start\":31340},{\"end\":31452,\"start\":31444}]", "bib_author_first_name": "[{\"end\":33581,\"start\":33575},{\"end\":33594,\"start\":33589},{\"end\":33600,\"start\":33599},{\"end\":33614,\"start\":33609},{\"end\":33893,\"start\":33886},{\"end\":33911,\"start\":33902},{\"end\":33926,\"start\":33919},{\"end\":33944,\"start\":33937},{\"end\":33963,\"start\":33954},{\"end\":33980,\"start\":33974},{\"end\":34257,\"start\":34253},{\"end\":34270,\"start\":34266},{\"end\":34280,\"start\":34276},{\"end\":34548,\"start\":34541},{\"end\":34561,\"start\":34555},{\"end\":34575,\"start\":34567},{\"end\":34861,\"start\":34856},{\"end\":34878,\"start\":34869},{\"end\":34880,\"start\":34879},{\"end\":34899,\"start\":34890},{\"end\":35137,\"start\":35127},{\"end\":35146,\"start\":35143},{\"end\":35155,\"start\":35153},{\"end\":35170,\"start\":35163},{\"end\":35184,\"start\":35177},{\"end\":35196,\"start\":35190},{\"end\":35209,\"start\":35202},{\"end\":35495,\"start\":35492},{\"end\":35505,\"start\":35502},{\"end\":35519,\"start\":35512},{\"end\":35534,\"start\":35528},{\"end\":35542,\"start\":35539},{\"end\":35549,\"start\":35547},{\"end\":35738,\"start\":35732},{\"end\":35757,\"start\":35752},{\"end\":35774,\"start\":35765},{\"end\":35791,\"start\":35787},{\"end\":35812,\"start\":35805},{\"end\":35825,\"start\":35819},{\"end\":35846,\"start\":35839},{\"end\":35865,\"start\":35857},{\"end\":35881,\"start\":35876},{\"end\":35898,\"start\":35891},{\"end\":35911,\"start\":35906},{\"end\":35927,\"start\":35923},{\"end\":36448,\"start\":36441},{\"end\":36458,\"start\":36455},{\"end\":36468,\"start\":36464},{\"end\":36482,\"start\":36475},{\"end\":36692,\"start\":36686},{\"end\":36704,\"start\":36698},{\"end\":36717,\"start\":36710},{\"end\":36729,\"start\":36723},{\"end\":36745,\"start\":36736},{\"end\":36756,\"start\":36751},{\"end\":36985,\"start\":36982},{\"end\":36993,\"start\":36991},{\"end\":37005,\"start\":37000},{\"end\":37018,\"start\":37010},{\"end\":37032,\"start\":37024},{\"end\":37042,\"start\":37037},{\"end\":37273,\"start\":37266},{\"end\":37285,\"start\":37278},{\"end\":37301,\"start\":37293},{\"end\":37311,\"start\":37307},{\"end\":37529,\"start\":37524},{\"end\":37541,\"start\":37534},{\"end\":37553,\"start\":37549},{\"end\":37753,\"start\":37745},{\"end\":37767,\"start\":37762},{\"end\":37781,\"start\":37777},{\"end\":38009,\"start\":38005},{\"end\":38024,\"start\":38016},{\"end\":38032,\"start\":38029},{\"end\":38197,\"start\":38191},{\"end\":38210,\"start\":38205},{\"end\":38218,\"start\":38216},{\"end\":38231,\"start\":38225},{\"end\":38245,\"start\":38238},{\"end\":38256,\"start\":38251},{\"end\":38269,\"start\":38263},{\"end\":38634,\"start\":38628},{\"end\":38647,\"start\":38641},{\"end\":38659,\"start\":38653},{\"end\":38670,\"start\":38667},{\"end\":38682,\"start\":38678},{\"end\":38695,\"start\":38689},{\"end\":38704,\"start\":38700},{\"end\":38714,\"start\":38711},{\"end\":39018,\"start\":39014},{\"end\":39035,\"start\":39031},{\"end\":39055,\"start\":39047},{\"end\":39057,\"start\":39056},{\"end\":39379,\"start\":39373},{\"end\":39393,\"start\":39385},{\"end\":39404,\"start\":39399},{\"end\":39412,\"start\":39409},{\"end\":39426,\"start\":39419},{\"end\":39776,\"start\":39774},{\"end\":39788,\"start\":39782},{\"end\":39797,\"start\":39794},{\"end\":39806,\"start\":39803},{\"end\":39817,\"start\":39811},{\"end\":39828,\"start\":39823},{\"end\":39843,\"start\":39836},{\"end\":39856,\"start\":39849},{\"end\":40173,\"start\":40168},{\"end\":40190,\"start\":40187},{\"end\":40198,\"start\":40191},{\"end\":40213,\"start\":40209},{\"end\":40456,\"start\":40448},{\"end\":40467,\"start\":40462},{\"end\":40474,\"start\":40472},{\"end\":40483,\"start\":40480},{\"end\":40811,\"start\":40803},{\"end\":40825,\"start\":40817},{\"end\":40837,\"start\":40832},{\"end\":40848,\"start\":40843},{\"end\":40856,\"start\":40853},{\"end\":41088,\"start\":41080},{\"end\":41098,\"start\":41094},{\"end\":41339,\"start\":41335},{\"end\":41357,\"start\":41349},{\"end\":41372,\"start\":41364},{\"end\":41389,\"start\":41380},{\"end\":41406,\"start\":41397},{\"end\":41426,\"start\":41421},{\"end\":41724,\"start\":41718},{\"end\":41738,\"start\":41734},{\"end\":41752,\"start\":41748},{\"end\":41766,\"start\":41761},{\"end\":41783,\"start\":41778},{\"end\":41796,\"start\":41791},{\"end\":41798,\"start\":41797},{\"end\":41812,\"start\":41806},{\"end\":41826,\"start\":41821},{\"end\":42162,\"start\":42158},{\"end\":42176,\"start\":42169},{\"end\":42187,\"start\":42182},{\"end\":42195,\"start\":42193},{\"end\":42205,\"start\":42201},{\"end\":42692,\"start\":42686},{\"end\":42703,\"start\":42699},{\"end\":42714,\"start\":42709},{\"end\":42728,\"start\":42719},{\"end\":42740,\"start\":42734},{\"end\":42751,\"start\":42747},{\"end\":42763,\"start\":42759},{\"end\":42772,\"start\":42768},{\"end\":42782,\"start\":42778},{\"end\":43113,\"start\":43106},{\"end\":43123,\"start\":43118},{\"end\":43133,\"start\":43128},{\"end\":43148,\"start\":43141},{\"end\":43391,\"start\":43387},{\"end\":43405,\"start\":43396},{\"end\":43418,\"start\":43411},{\"end\":43432,\"start\":43425},{\"end\":43677,\"start\":43672},{\"end\":43690,\"start\":43682},{\"end\":43700,\"start\":43696},{\"end\":43712,\"start\":43707},{\"end\":43723,\"start\":43718},{\"end\":43731,\"start\":43728},{\"end\":43983,\"start\":43981},{\"end\":43997,\"start\":43990},{\"end\":44007,\"start\":44004},{\"end\":44020,\"start\":44014},{\"end\":44030,\"start\":44025},{\"end\":44042,\"start\":44036},{\"end\":44051,\"start\":44050},{\"end\":44053,\"start\":44052},{\"end\":44069,\"start\":44063},{\"end\":44084,\"start\":44075},{\"end\":44390,\"start\":44380},{\"end\":44399,\"start\":44397},{\"end\":44412,\"start\":44407},{\"end\":44424,\"start\":44418},{\"end\":44438,\"start\":44431},{\"end\":44659,\"start\":44653},{\"end\":44674,\"start\":44667},{\"end\":44689,\"start\":44679},{\"end\":44703,\"start\":44696},{\"end\":44714,\"start\":44709},{\"end\":44726,\"start\":44720},{\"end\":44739,\"start\":44733},{\"end\":44752,\"start\":44744},{\"end\":44762,\"start\":44759},{\"end\":44771,\"start\":44770},{\"end\":44773,\"start\":44772},{\"end\":44784,\"start\":44782},{\"end\":45069,\"start\":45063},{\"end\":45082,\"start\":45076},{\"end\":45096,\"start\":45089},{\"end\":45108,\"start\":45102},{\"end\":45123,\"start\":45115},{\"end\":45493,\"start\":45487},{\"end\":45505,\"start\":45499},{\"end\":45515,\"start\":45510},{\"end\":45523,\"start\":45520},{\"end\":45536,\"start\":45528},{\"end\":45549,\"start\":45543}]", "bib_author_last_name": "[{\"end\":33587,\"start\":33582},{\"end\":33597,\"start\":33595},{\"end\":33607,\"start\":33601},{\"end\":33620,\"start\":33615},{\"end\":33630,\"start\":33622},{\"end\":33900,\"start\":33894},{\"end\":33917,\"start\":33912},{\"end\":33935,\"start\":33927},{\"end\":33952,\"start\":33945},{\"end\":33972,\"start\":33964},{\"end\":33990,\"start\":33981},{\"end\":34264,\"start\":34258},{\"end\":34274,\"start\":34271},{\"end\":34285,\"start\":34281},{\"end\":34553,\"start\":34549},{\"end\":34565,\"start\":34562},{\"end\":34581,\"start\":34576},{\"end\":34867,\"start\":34862},{\"end\":34888,\"start\":34881},{\"end\":34908,\"start\":34900},{\"end\":35141,\"start\":35138},{\"end\":35151,\"start\":35147},{\"end\":35161,\"start\":35156},{\"end\":35175,\"start\":35171},{\"end\":35188,\"start\":35185},{\"end\":35200,\"start\":35197},{\"end\":35214,\"start\":35210},{\"end\":35500,\"start\":35496},{\"end\":35510,\"start\":35506},{\"end\":35526,\"start\":35520},{\"end\":35537,\"start\":35535},{\"end\":35545,\"start\":35543},{\"end\":35557,\"start\":35550},{\"end\":35750,\"start\":35739},{\"end\":35763,\"start\":35758},{\"end\":35785,\"start\":35775},{\"end\":35803,\"start\":35792},{\"end\":35817,\"start\":35813},{\"end\":35837,\"start\":35826},{\"end\":35855,\"start\":35847},{\"end\":35874,\"start\":35866},{\"end\":35889,\"start\":35882},{\"end\":35904,\"start\":35899},{\"end\":35921,\"start\":35912},{\"end\":35935,\"start\":35928},{\"end\":36453,\"start\":36449},{\"end\":36462,\"start\":36459},{\"end\":36473,\"start\":36469},{\"end\":36490,\"start\":36483},{\"end\":36696,\"start\":36693},{\"end\":36708,\"start\":36705},{\"end\":36721,\"start\":36718},{\"end\":36734,\"start\":36730},{\"end\":36749,\"start\":36746},{\"end\":36762,\"start\":36757},{\"end\":36989,\"start\":36986},{\"end\":36998,\"start\":36994},{\"end\":37008,\"start\":37006},{\"end\":37022,\"start\":37019},{\"end\":37035,\"start\":37033},{\"end\":37047,\"start\":37043},{\"end\":37276,\"start\":37274},{\"end\":37291,\"start\":37286},{\"end\":37305,\"start\":37302},{\"end\":37315,\"start\":37312},{\"end\":37532,\"start\":37530},{\"end\":37547,\"start\":37542},{\"end\":37557,\"start\":37554},{\"end\":37760,\"start\":37754},{\"end\":37775,\"start\":37768},{\"end\":37786,\"start\":37782},{\"end\":38014,\"start\":38010},{\"end\":38027,\"start\":38025},{\"end\":38038,\"start\":38033},{\"end\":38203,\"start\":38198},{\"end\":38214,\"start\":38211},{\"end\":38223,\"start\":38219},{\"end\":38236,\"start\":38232},{\"end\":38249,\"start\":38246},{\"end\":38261,\"start\":38257},{\"end\":38274,\"start\":38270},{\"end\":38639,\"start\":38635},{\"end\":38651,\"start\":38648},{\"end\":38665,\"start\":38660},{\"end\":38676,\"start\":38671},{\"end\":38687,\"start\":38683},{\"end\":38698,\"start\":38696},{\"end\":38709,\"start\":38705},{\"end\":38718,\"start\":38715},{\"end\":38728,\"start\":38720},{\"end\":39029,\"start\":39019},{\"end\":39045,\"start\":39036},{\"end\":39064,\"start\":39058},{\"end\":39383,\"start\":39380},{\"end\":39397,\"start\":39394},{\"end\":39407,\"start\":39405},{\"end\":39417,\"start\":39413},{\"end\":39432,\"start\":39427},{\"end\":39780,\"start\":39777},{\"end\":39792,\"start\":39789},{\"end\":39801,\"start\":39798},{\"end\":39809,\"start\":39807},{\"end\":39821,\"start\":39818},{\"end\":39834,\"start\":39829},{\"end\":39847,\"start\":39844},{\"end\":39860,\"start\":39857},{\"end\":40185,\"start\":40174},{\"end\":40207,\"start\":40199},{\"end\":40222,\"start\":40214},{\"end\":40460,\"start\":40457},{\"end\":40470,\"start\":40468},{\"end\":40478,\"start\":40475},{\"end\":40488,\"start\":40484},{\"end\":40815,\"start\":40812},{\"end\":40830,\"start\":40826},{\"end\":40841,\"start\":40838},{\"end\":40851,\"start\":40849},{\"end\":40861,\"start\":40857},{\"end\":41092,\"start\":41089},{\"end\":41101,\"start\":41099},{\"end\":41347,\"start\":41340},{\"end\":41362,\"start\":41358},{\"end\":41378,\"start\":41373},{\"end\":41395,\"start\":41390},{\"end\":41419,\"start\":41407},{\"end\":41432,\"start\":41427},{\"end\":41732,\"start\":41725},{\"end\":41746,\"start\":41739},{\"end\":41759,\"start\":41753},{\"end\":41776,\"start\":41767},{\"end\":41789,\"start\":41784},{\"end\":41804,\"start\":41799},{\"end\":41819,\"start\":41813},{\"end\":41837,\"start\":41827},{\"end\":42167,\"start\":42163},{\"end\":42180,\"start\":42177},{\"end\":42191,\"start\":42188},{\"end\":42199,\"start\":42196},{\"end\":42209,\"start\":42206},{\"end\":42697,\"start\":42693},{\"end\":42707,\"start\":42704},{\"end\":42717,\"start\":42715},{\"end\":42732,\"start\":42729},{\"end\":42745,\"start\":42741},{\"end\":42757,\"start\":42752},{\"end\":42766,\"start\":42764},{\"end\":42776,\"start\":42773},{\"end\":42787,\"start\":42783},{\"end\":43116,\"start\":43114},{\"end\":43126,\"start\":43124},{\"end\":43139,\"start\":43134},{\"end\":43151,\"start\":43149},{\"end\":43394,\"start\":43392},{\"end\":43409,\"start\":43406},{\"end\":43423,\"start\":43419},{\"end\":43436,\"start\":43433},{\"end\":43680,\"start\":43678},{\"end\":43694,\"start\":43691},{\"end\":43705,\"start\":43701},{\"end\":43716,\"start\":43713},{\"end\":43726,\"start\":43724},{\"end\":43736,\"start\":43732},{\"end\":43988,\"start\":43984},{\"end\":44002,\"start\":43998},{\"end\":44012,\"start\":44008},{\"end\":44023,\"start\":44021},{\"end\":44034,\"start\":44031},{\"end\":44048,\"start\":44043},{\"end\":44061,\"start\":44054},{\"end\":44073,\"start\":44070},{\"end\":44089,\"start\":44085},{\"end\":44094,\"start\":44091},{\"end\":44395,\"start\":44391},{\"end\":44405,\"start\":44400},{\"end\":44416,\"start\":44413},{\"end\":44429,\"start\":44425},{\"end\":44445,\"start\":44439},{\"end\":44665,\"start\":44660},{\"end\":44677,\"start\":44675},{\"end\":44694,\"start\":44690},{\"end\":44707,\"start\":44704},{\"end\":44718,\"start\":44715},{\"end\":44731,\"start\":44727},{\"end\":44742,\"start\":44740},{\"end\":44757,\"start\":44753},{\"end\":44768,\"start\":44763},{\"end\":44780,\"start\":44774},{\"end\":44789,\"start\":44785},{\"end\":44796,\"start\":44791},{\"end\":45074,\"start\":45070},{\"end\":45087,\"start\":45083},{\"end\":45100,\"start\":45097},{\"end\":45113,\"start\":45109},{\"end\":45128,\"start\":45124},{\"end\":45497,\"start\":45494},{\"end\":45508,\"start\":45506},{\"end\":45518,\"start\":45516},{\"end\":45526,\"start\":45524},{\"end\":45541,\"start\":45537},{\"end\":45553,\"start\":45550}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2102.06171\",\"id\":\"b0\"},\"end\":33837,\"start\":33506},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218889832},\"end\":34190,\"start\":33839},{\"attributes\":{\"doi\":\"arXiv:2012.09838\",\"id\":\"b2\"},\"end\":34456,\"start\":34192},{\"attributes\":{\"doi\":\"arXiv:2103.14899\",\"id\":\"b3\"},\"end\":34782,\"start\":34458},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":235829267},\"end\":35067,\"start\":34784},{\"attributes\":{\"doi\":\"arXiv:2102.10882\",\"id\":\"b5\"},\"end\":35437,\"start\":35069},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":57246310},\"end\":35730,\"start\":35439},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b7\"},\"end\":36372,\"start\":35732},{\"attributes\":{\"doi\":\"arXiv:1412.6115\",\"id\":\"b8\"},\"end\":36682,\"start\":36374},{\"attributes\":{\"doi\":\"arXiv:1902.09113\",\"id\":\"b9\"},\"end\":36952,\"start\":36684},{\"attributes\":{\"doi\":\"arXiv:2103.00112\",\"id\":\"b10\"},\"end\":37218,\"start\":36954},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":37462,\"start\":37220},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":20157893},\"end\":37697,\"start\":37464},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b13\"},\"end\":37951,\"start\":37699},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2428314},\"end\":38158,\"start\":37953},{\"attributes\":{\"doi\":\"arXiv:2104.10858\",\"id\":\"b15\"},\"end\":38626,\"start\":38160},{\"attributes\":{\"doi\":\"arXiv:1909.10351\",\"id\":\"b16\"},\"end\":39010,\"start\":38628},{\"attributes\":{\"id\":\"b17\"},\"end\":39294,\"start\":39012},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":221340727},\"end\":39699,\"start\":39296},{\"attributes\":{\"doi\":\"arXiv:2103.14030\",\"id\":\"b19\"},\"end\":40103,\"start\":39701},{\"attributes\":{\"id\":\"b20\"},\"end\":40386,\"start\":40105},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":53105063},\"end\":40752,\"start\":40388},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":235694359},\"end\":41004,\"start\":40754},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b23\",\"matched_paper_id\":167217261},\"end\":41256,\"start\":41006},{\"attributes\":{\"doi\":\"arXiv:2012.12877\",\"id\":\"b24\"},\"end\":41714,\"start\":41258},{\"attributes\":{\"doi\":\"arXiv:1706.03762\",\"id\":\"b25\"},\"end\":42091,\"start\":41716},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":102350477},\"end\":42592,\"start\":42093},{\"attributes\":{\"doi\":\"arXiv:2102.12122\",\"id\":\"b27\"},\"end\":43058,\"start\":42594},{\"attributes\":{\"doi\":\"arXiv:2104.06399\",\"id\":\"b28\"},\"end\":43320,\"start\":43060},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":24553488},\"end\":43597,\"start\":43322},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":237213435},\"end\":43903,\"start\":43599},{\"attributes\":{\"doi\":\"arXiv:2101.11986\",\"id\":\"b31\"},\"end\":44359,\"start\":43905},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":226227046},\"end\":44559,\"start\":44361},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":229924195},\"end\":45061,\"start\":44561},{\"attributes\":{\"doi\":\"arXiv:2103.11886\",\"id\":\"b34\"},\"end\":45411,\"start\":45063},{\"attributes\":{\"doi\":\"arXiv:2010.04159\",\"id\":\"b35\"},\"end\":45771,\"start\":45413},{\"attributes\":{\"id\":\"b36\"},\"end\":45817,\"start\":45773},{\"attributes\":{\"id\":\"b37\"},\"end\":46246,\"start\":45819}]", "bib_title": "[{\"end\":33884,\"start\":33839},{\"end\":34854,\"start\":34784},{\"end\":35490,\"start\":35439},{\"end\":37264,\"start\":37220},{\"end\":37522,\"start\":37464},{\"end\":38003,\"start\":37953},{\"end\":38189,\"start\":38160},{\"end\":39371,\"start\":39296},{\"end\":40166,\"start\":40105},{\"end\":40446,\"start\":40388},{\"end\":40801,\"start\":40754},{\"end\":41078,\"start\":41006},{\"end\":42156,\"start\":42093},{\"end\":43385,\"start\":43322},{\"end\":43670,\"start\":43599},{\"end\":44378,\"start\":44361},{\"end\":44651,\"start\":44561}]", "bib_author": "[{\"end\":33589,\"start\":33575},{\"end\":33599,\"start\":33589},{\"end\":33609,\"start\":33599},{\"end\":33622,\"start\":33609},{\"end\":33632,\"start\":33622},{\"end\":33902,\"start\":33886},{\"end\":33919,\"start\":33902},{\"end\":33937,\"start\":33919},{\"end\":33954,\"start\":33937},{\"end\":33974,\"start\":33954},{\"end\":33992,\"start\":33974},{\"end\":34266,\"start\":34253},{\"end\":34276,\"start\":34266},{\"end\":34287,\"start\":34276},{\"end\":34555,\"start\":34541},{\"end\":34567,\"start\":34555},{\"end\":34583,\"start\":34567},{\"end\":34869,\"start\":34856},{\"end\":34890,\"start\":34869},{\"end\":34910,\"start\":34890},{\"end\":35143,\"start\":35127},{\"end\":35153,\"start\":35143},{\"end\":35163,\"start\":35153},{\"end\":35177,\"start\":35163},{\"end\":35190,\"start\":35177},{\"end\":35202,\"start\":35190},{\"end\":35216,\"start\":35202},{\"end\":35502,\"start\":35492},{\"end\":35512,\"start\":35502},{\"end\":35528,\"start\":35512},{\"end\":35539,\"start\":35528},{\"end\":35547,\"start\":35539},{\"end\":35559,\"start\":35547},{\"end\":35752,\"start\":35732},{\"end\":35765,\"start\":35752},{\"end\":35787,\"start\":35765},{\"end\":35805,\"start\":35787},{\"end\":35819,\"start\":35805},{\"end\":35839,\"start\":35819},{\"end\":35857,\"start\":35839},{\"end\":35876,\"start\":35857},{\"end\":35891,\"start\":35876},{\"end\":35906,\"start\":35891},{\"end\":35923,\"start\":35906},{\"end\":35937,\"start\":35923},{\"end\":36455,\"start\":36441},{\"end\":36464,\"start\":36455},{\"end\":36475,\"start\":36464},{\"end\":36492,\"start\":36475},{\"end\":36698,\"start\":36686},{\"end\":36710,\"start\":36698},{\"end\":36723,\"start\":36710},{\"end\":36736,\"start\":36723},{\"end\":36751,\"start\":36736},{\"end\":36764,\"start\":36751},{\"end\":36991,\"start\":36982},{\"end\":37000,\"start\":36991},{\"end\":37010,\"start\":37000},{\"end\":37024,\"start\":37010},{\"end\":37037,\"start\":37024},{\"end\":37049,\"start\":37037},{\"end\":37278,\"start\":37266},{\"end\":37293,\"start\":37278},{\"end\":37307,\"start\":37293},{\"end\":37317,\"start\":37307},{\"end\":37534,\"start\":37524},{\"end\":37549,\"start\":37534},{\"end\":37559,\"start\":37549},{\"end\":37762,\"start\":37745},{\"end\":37777,\"start\":37762},{\"end\":37788,\"start\":37777},{\"end\":38016,\"start\":38005},{\"end\":38029,\"start\":38016},{\"end\":38040,\"start\":38029},{\"end\":38205,\"start\":38191},{\"end\":38216,\"start\":38205},{\"end\":38225,\"start\":38216},{\"end\":38238,\"start\":38225},{\"end\":38251,\"start\":38238},{\"end\":38263,\"start\":38251},{\"end\":38276,\"start\":38263},{\"end\":38641,\"start\":38628},{\"end\":38653,\"start\":38641},{\"end\":38667,\"start\":38653},{\"end\":38678,\"start\":38667},{\"end\":38689,\"start\":38678},{\"end\":38700,\"start\":38689},{\"end\":38711,\"start\":38700},{\"end\":38720,\"start\":38711},{\"end\":38730,\"start\":38720},{\"end\":39031,\"start\":39014},{\"end\":39047,\"start\":39031},{\"end\":39066,\"start\":39047},{\"end\":39385,\"start\":39373},{\"end\":39399,\"start\":39385},{\"end\":39409,\"start\":39399},{\"end\":39419,\"start\":39409},{\"end\":39434,\"start\":39419},{\"end\":39782,\"start\":39774},{\"end\":39794,\"start\":39782},{\"end\":39803,\"start\":39794},{\"end\":39811,\"start\":39803},{\"end\":39823,\"start\":39811},{\"end\":39836,\"start\":39823},{\"end\":39849,\"start\":39836},{\"end\":39862,\"start\":39849},{\"end\":40187,\"start\":40168},{\"end\":40209,\"start\":40187},{\"end\":40224,\"start\":40209},{\"end\":40462,\"start\":40448},{\"end\":40472,\"start\":40462},{\"end\":40480,\"start\":40472},{\"end\":40490,\"start\":40480},{\"end\":40817,\"start\":40803},{\"end\":40832,\"start\":40817},{\"end\":40843,\"start\":40832},{\"end\":40853,\"start\":40843},{\"end\":40863,\"start\":40853},{\"end\":41094,\"start\":41080},{\"end\":41103,\"start\":41094},{\"end\":41349,\"start\":41335},{\"end\":41364,\"start\":41349},{\"end\":41380,\"start\":41364},{\"end\":41397,\"start\":41380},{\"end\":41421,\"start\":41397},{\"end\":41434,\"start\":41421},{\"end\":41734,\"start\":41718},{\"end\":41748,\"start\":41734},{\"end\":41761,\"start\":41748},{\"end\":41778,\"start\":41761},{\"end\":41791,\"start\":41778},{\"end\":41806,\"start\":41791},{\"end\":41821,\"start\":41806},{\"end\":41839,\"start\":41821},{\"end\":42169,\"start\":42158},{\"end\":42182,\"start\":42169},{\"end\":42193,\"start\":42182},{\"end\":42201,\"start\":42193},{\"end\":42211,\"start\":42201},{\"end\":42699,\"start\":42686},{\"end\":42709,\"start\":42699},{\"end\":42719,\"start\":42709},{\"end\":42734,\"start\":42719},{\"end\":42747,\"start\":42734},{\"end\":42759,\"start\":42747},{\"end\":42768,\"start\":42759},{\"end\":42778,\"start\":42768},{\"end\":42789,\"start\":42778},{\"end\":43118,\"start\":43106},{\"end\":43128,\"start\":43118},{\"end\":43141,\"start\":43128},{\"end\":43153,\"start\":43141},{\"end\":43396,\"start\":43387},{\"end\":43411,\"start\":43396},{\"end\":43425,\"start\":43411},{\"end\":43438,\"start\":43425},{\"end\":43682,\"start\":43672},{\"end\":43696,\"start\":43682},{\"end\":43707,\"start\":43696},{\"end\":43718,\"start\":43707},{\"end\":43728,\"start\":43718},{\"end\":43738,\"start\":43728},{\"end\":43990,\"start\":43981},{\"end\":44004,\"start\":43990},{\"end\":44014,\"start\":44004},{\"end\":44025,\"start\":44014},{\"end\":44036,\"start\":44025},{\"end\":44050,\"start\":44036},{\"end\":44063,\"start\":44050},{\"end\":44075,\"start\":44063},{\"end\":44091,\"start\":44075},{\"end\":44096,\"start\":44091},{\"end\":44397,\"start\":44380},{\"end\":44407,\"start\":44397},{\"end\":44418,\"start\":44407},{\"end\":44431,\"start\":44418},{\"end\":44447,\"start\":44431},{\"end\":44667,\"start\":44653},{\"end\":44679,\"start\":44667},{\"end\":44696,\"start\":44679},{\"end\":44709,\"start\":44696},{\"end\":44720,\"start\":44709},{\"end\":44733,\"start\":44720},{\"end\":44744,\"start\":44733},{\"end\":44759,\"start\":44744},{\"end\":44770,\"start\":44759},{\"end\":44782,\"start\":44770},{\"end\":44791,\"start\":44782},{\"end\":44798,\"start\":44791},{\"end\":45076,\"start\":45063},{\"end\":45089,\"start\":45076},{\"end\":45102,\"start\":45089},{\"end\":45115,\"start\":45102},{\"end\":45130,\"start\":45115},{\"end\":45499,\"start\":45487},{\"end\":45510,\"start\":45499},{\"end\":45520,\"start\":45510},{\"end\":45528,\"start\":45520},{\"end\":45543,\"start\":45528},{\"end\":45555,\"start\":45543}]", "bib_venue": "[{\"end\":42360,\"start\":42294},{\"end\":33573,\"start\":33506},{\"end\":33996,\"start\":33992},{\"end\":34251,\"start\":34192},{\"end\":34539,\"start\":34458},{\"end\":34917,\"start\":34910},{\"end\":35125,\"start\":35069},{\"end\":35563,\"start\":35559},{\"end\":36027,\"start\":35953},{\"end\":36439,\"start\":36374},{\"end\":36980,\"start\":36954},{\"end\":37321,\"start\":37317},{\"end\":37563,\"start\":37559},{\"end\":37743,\"start\":37699},{\"end\":38044,\"start\":38040},{\"end\":38360,\"start\":38292},{\"end\":38796,\"start\":38746},{\"end\":39138,\"start\":39066},{\"end\":39472,\"start\":39434},{\"end\":39772,\"start\":39701},{\"end\":40228,\"start\":40224},{\"end\":40552,\"start\":40490},{\"end\":40870,\"start\":40863},{\"end\":41111,\"start\":41107},{\"end\":41333,\"start\":41258},{\"end\":42292,\"start\":42211},{\"end\":42684,\"start\":42594},{\"end\":43104,\"start\":43060},{\"end\":43442,\"start\":43438},{\"end\":43742,\"start\":43738},{\"end\":43979,\"start\":43905},{\"end\":44451,\"start\":44447},{\"end\":44802,\"start\":44798},{\"end\":45216,\"start\":45146},{\"end\":45485,\"start\":45413},{\"end\":45794,\"start\":45773},{\"end\":46011,\"start\":45819}]"}}}, "year": 2023, "month": 12, "day": 17}