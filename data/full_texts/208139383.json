{"id": 208139383, "updated": "2023-10-06 21:41:35.049", "metadata": {"title": "Graph-Revised Convolutional Network", "authors": "[{\"first\":\"Donghan\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Ruohong\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Zhengbao\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Yuexin\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Yiming\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 11, "day": 17}, "abstract": "Graph Convolutional Networks (GCNs) have received increasing attention in the machine learning community for effectively leveraging both the content features of nodes and the linkage patterns across graphs in various applications. As real-world graphs are often incomplete and noisy, treating them as ground-truth information, which is a common practice in most GCNs, unavoidably leads to sub-optimal solutions. Existing efforts for addressing this problem either involve an over-parameterized model which is difficult to scale, or simply re-weight observed edges without dealing with the missing-edge issue. This paper proposes a novel framework called Graph-Revised Convolutional Network (GRCN), which avoids both extremes. Specifically, a GCN-based graph revision module is introduced for predicting missing edges and revising edge weights w.r.t. downstream tasks via joint optimization. A theoretical analysis reveals the connection between GRCN and previous work on multigraph belief propagation. Experiments on six benchmark datasets show that GRCN consistently outperforms strong baseline methods by a large margin, especially when the original graphs are severely incomplete or the labeled instances for model training are highly sparse.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1911.07123", "mag": "2985015573", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/pkdd/YuZJWY20", "doi": "10.1007/978-3-030-67664-3_23"}}, "content": {"source": {"pdf_hash": "6aa391ed57430e911839a9eca33d34a02e63efd2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1911.07123v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1911.07123", "status": "GREEN"}}, "grobid": {"id": "f7808cc33d3a9c77379dfc25268a34b7251ee16c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6aa391ed57430e911839a9eca33d34a02e63efd2.txt", "contents": "\nGraph-Revised Convolutional Network\n\n\nDonghan Yu \nCarnegie Mellon University\n\n\nRuohong Zhang ruohongz@cs.cmu.edu \nCarnegie Mellon University\n\n\nZhengbao Jiang zhengbaj@cs.cmu.edu \nCarnegie Mellon University\n\n\nYuexin Wu yuexinw@cs.cmu.edu \nCarnegie Mellon University\n\n\nYiming Yang yiming@cs.cmu.edu \nCarnegie Mellon University\n\n\nGraph-Revised Convolutional Network\n\nGraph Convolutional Networks (GCNs) have received increasing attention in the machine learning community for effectively leveraging both the content features of nodes and the linkage patterns across graphs in various applications. As real-world graphs are often incomplete and noisy, treating them as ground-truth information, which is a common practice in most GCNs, unavoidably leads to sub-optimal solutions. Existing efforts for addressing this problem either involve an over-parameterized model which is difficult to scale, or simply re-weight observed edges without dealing with the missing-edge issue. This paper proposes a novel framework called Graph-Revised Convolutional Network (GRCN), which avoids both extremes. Specifically, a GCN-based graph revision module is introduced for predicting missing edges and revising edge weights w.r.t. downstream tasks via joint optimization. A theoretical analysis reveals the connection between GRCN and previous work on multigraph belief propagation. Experiments on six benchmark datasets show that GRCN consistently outperforms strong baseline methods by a large margin, especially when the original graphs are severely incomplete or the labeled instances for model training are highly sparse.\n\nIntroduction\n\nGraph Convolutional Networks (GCNs) have received increasing attention in recent years as they are highly effective in graph-based node feature induction and belief propagation, and widely applicable to many real-world problems, including computer vision (Wang et al. 2018;Landrieu and Simonovsky 2018), natural language processing (Kipf and Welling 2016;Marcheggiani and Titov 2017), recommender systems (Monti et al. 2017;Ying et al. 2018), epidemiological forecasting (Wu et al. 2018), and more.\n\nHowever, the power of GCNs has not been fully exploited as most of the models assume that the given graph perfectly depicts the ground-truth of the relationship between nodes. Such assumptions are bound to yield sub-optimal results as real-world graphs are usually highly noisy, incomplete (with many missing edges), and not necessarily ideal for different downstream tasks. Ignoring these issues is a fundamental weakness of many existing GCN methods.\n\nRecent methods that attempt to modify the original graph can be split into two major streams: 1) Edge reweighting: GAT (Veli\u010dkovi\u0107 et al. 2017) and GLCN (Jiang et al. 2019) use attention mechanism or feature similarity to reweight the existing edges of the given graph. Since the topological structure of the graph is not changed, the model is prone to be affected by noisy data when edges are sparse. 2) Full graph parameterization: LDS (Franceschi et al. 2019), on the other hand, allows every possible node pairs in a graph to be parameterized. Although this design is more flexible, the memory cost is intractable for large datasets, since the number of parameters increases quadratically with the number of nodes. Therefore, finding a balance between model expressiveness and memory consumption remains an open challenge.\n\nTo enable flexible edge editing while maintaining scalability, we develop a GCN-based graph revision module that performs edge addition and edge reweighting. In each iteration, we calculate an adjacency matrix via GCN-based node embeddings, and select the edges with high confidence to be added. Our method permits a gradient-based training of an end-to-end neural model that can predict unseen edges. Our theoretical analysis demonstrates the effectiveness of our model from the perspective of multigraph (Balakrishnan 1997), which allows more than one edges from different sources between a pair of vertices. To the best of our knowledge, we are the first to reveal the connection between graph convolutional networks and multigraph propagation. Our contributions can be summarized as follows:\n\n\u2022 We introduce a novel structure that simultaneously learns both graph revision and node classification through different GCN modules.\n\n\u2022 Through theoretical analysis, we show our model's advantages in the view of multigraph propagation.\n\n\u2022 Comprehensive experiments on six benchmark datasets from different domains show that our proposed model achieves the best or highly competitive results, especially under the scenarios of highly incomplete graphs or sparse training labels. arXiv:1911.07123v1 [cs.\n\nLG] 17 Nov 2019\n\n\nBackground\n\nWe first introduce some basics of graph theory. An undirected graph G can be represented as (V, E) where V denotes the set of vertices and E denotes the set of edges. Let N and M be the number of vertices and edges, respectively. Each graph can also be represented by an adjacency matrix A of size N \u00d7 N where A ij = 1 if there is an edge between v i and v j , and A ij = 0 otherwise. We use A i to denote the i-th row of the adjacency matrix. A graph with adjacency matrix A is denoted as G A . Usually each node i has its own feature x i \u2208 R F where F is the feature dimension (for example, if nodes represent documents, the feature can be a bag-of-words vector). The node feature matrix of the whole graph is denoted as X \u2208 R N \u00d7F . Graph convolutional networks generalize the convolution operation on images to graph structure data, performing layer-wise propagation of node features. Suppose we are given a graph with adjacency matrix A and node features H (0) = X. An L-layer Graph Convolution Network (GCN) (Kipf and Welling 2016) conducts the following inductive layer-wise propagation:\nH (l+1) = \u03c3 D \u2212 1 2 A D \u2212 1 2 H (l) W (l) ,(1)\nwhere l = 0, 1, \u00b7 \u00b7 \u00b7 , L \u2212 1,\u00c3 = A + I and D is a diagonal matrix with D ii = j A ij . {W (0) , \u00b7 \u00b7 \u00b7 , W (L\u22121) } are the model parameters and \u03c3(\u00b7) is the activation function. The node embedding H (L) can be used for downsteam tasks. For semi-supervised node classification, GCN defines the final output as:\nY = softmax H (L) W (L) .(2)\nwhere Y \u2208 R N \u00d7C and C denotes the number of classes. We note that in the GCN computation, A is directly used as the underlining graph without any modification. Additionally, in each layer, GCN only updates node representations as a degree-normalized aggregation of neighbor nodes.\n\nTo allow for an adaptive aggregation paradigm, GLCN (Jiang et al. 2019) learns to reweight the existing edges by node feature embeddings. The reweighted adjacancy matrix A is calculated by:\nA ij = A ij exp ReLU a T |x i P \u2212 x j P | n k=1 A ik exp (ReLU (a T |x i P \u2212 x k P |)) ,(3)\nwhere x i denotes the feature vector of node i and a, P are model parameters. Another model GAT (Veli\u010dkovi\u0107 et al. 2017) reweights edges by a layer-wise self-attention across node-neighbor pairs to compute hidden representations. For each layer l, the reweighted edge is computed by:\nA (l) ij = A ij exp a(W (l) H (l) i , W (l) H (l) j ) n k=1 A ik exp a(W (l) H (l) i , W (l) H (l) k )(4)\nwhere a(\u00b7, \u00b7) is a shared attention function to compute the attention coefficients. Compared with GLCN, GAT uses different layer-wise maskings to allow for more flexible representation. However, neither of the methods has the ability to add edges since the revised edge A ij or A (l) ij = 0 only if the original edge A ij = 0.\n\nIn order to add new edges into the original graph, LDS (Franceschi et al. 2019) makes the entire adjacency matrix parameterizable. Then it jointly learns the graph structure \u03b8 and the GCN parameters W by approximately solving a bilevel program as follows:\nmin \u03b8\u2208H N E A\u223cBer(\u03b8) [\u03b6 val (W \u03b8 , A)] , such that W \u03b8 = arg min W E A\u223cBer(\u03b8) [\u03b6 train (W, A)],(5)\nwhere A \u223c Ber(\u03b8) means sampling adjacency matrix A \u2208 R N \u00d7N from Bernoulli distribution under parameter \u03b8 \u2208 R N \u00d7N . H N is the convex hull of the set of all adjecency matrices for N nodes. \u03b6 train and \u03b6 val denote the node classification loss on training and validation data respectively. However, this method can hardly scale to large graphs since the parameter size of \u03b8 is N 2 where N is the number of nodes. In the next section, we'll present our method which resolves the issues in previous work.\n\n\nProposed Method\n\n\nGraph-Revised Convolutional Network\n\nOur Graph-Revised Convolutional Network (GRCN) contains two modules: a graph revision module and a node classification module. The graph revision module adjusts the original graph by adding or reweighting edges, and the node classification module performs classification using the revised graph. Specifically, in our graph revision module, we choose to use a GCN to combine the node features and the original graph input, as GCNs are effective at fusing data from different sources. We first learn the node embedding Z \u2208 R N \u00d7D as follows:\nZ = GCN g (A, X)(6)\nwhere GCN g denotes the graph convolutional network for graph revision, A is the original graph adjacency matrix and X is node feature. Then we calculate a similarity graph S based on node embedding using certain kernel function k :\nR D \u00d7 R D \u2192 R: S ij = k(z i , z j ).(7)\nThe revised adjacency matrix is formed by an elementwise summation of the original adjacency matrix and the calculated similarity matrix: A = A + S. Compared with the graph revision in GAT and GLCN which use entrywise product, we instead adopt the entrywise addition operator \"+\" in order for new edges to be considered. In this process, the original graph A is revised by the similarity graph S, which can insert new edges to A and potentially reweight or delete existing edges in A. In practice, we apply a sparsification technique on dense matrix S to reduce computational cost and memory usage, which will be introduced in the next section. Then the predicted labels are calculated by:\nY = GCN c ( A, X)(8)\nwhere GCN c denotes the graph convolutional network for the downstream node classification task. Figure 1 provides an illustration of our model. Finally, we use cross-entropy loss as our objective function:\n\u03b6 = \u2212 i\u2208Y L C j=1 Y ij ln Y ij (9)\nwhere Y L is the set of node indices that have labels Y and C is the number of classes. It's worth emphasizing that our model does not need other loss functions to guide the graph revision process. Overall, our model can be formulated as:\nGRCN (A, X) = GCN c ( A, X), A = A + K(GCN g (A, X)),(10)\nwhere K(\u00b7) is the kernel matrix computed from the node embeddings in Equation (6). In our implementation, we use dot product as kernel function for simplicity, and we use a two-layer GCN (Kipf and Welling 2016) in both modules. Applying the two-layer GCN for graph revision is a design choice, but our framework is highly flexible, and thus can be adapted to other graph convolutional networks.\n\n\nSparsification\n\nSince the adjacency matrix S of similarity graph is dense, directly applying it in the classification module is inefficient. Besides, we only want those edges with higher confidence to avoid introducing too much noise. Thus we conduct a Knearest-neighbour (KNN) sparsification on the dense graph: for each node, we keep the edges with top-K prediction scores. The adjacancy matrix of the KNN-sparse graph, denoted as S (K) , is computed as:\nS (K) ij = S ij , S ij \u2208 topK(S i ), 0, S ij / \u2208 topK(S i ).(11)\nwhere topK(S i ) is the set of top-K values of vector S i . Finally, in order to keep the symmetric property, the output sparse graph S is calculated by:\nS ij = max(S (K) ij , S (K) ji ), S (K) ij , S (K) ij \u2265 0 min(S (K) ij , S (K) ji ), S (K) ij , S (K) ij \u2264 0(12)\nNow since both original graph A and similarity graph S are sparse, efficient matrix multiplication can be applied on both GCNs as in the training time, gradients will only backpropagate through the top-K values.\n\n\nTheoretical Analysis\n\nIn this section, we show the effectiveness of our model in the view of Multigraph (Balakrishnan 1997) propagation. The major observation is that for existing methods, the learned function from GCNs can be regarded as a linear combination of limited pre-defined kernels where the flexibility of kernels have a large influence on the final prediction accuracy.\n\nWe consider the simplified graph convolution neural network GCN s for the ease of analysis. That is, we remove feature transformation parameter W and non-linear activation function \u03c3(\u00b7) as:\nGCN s (A, X) = A k X(13)\nwhere k is the number of GCN layers. For simplicity we denote A as the adjacency matrix with self-loop after normalization. The final output can be acquired by applying a linear or logistic regression function f (\u00b7) on the node embeddings above:\nY = f (GCN s (A, X)) = f (A k X)(14)\nwhere Y denotes the predicted labels of nodes. Then the following theorem shows that under certain conditions, the optimal function f * can be expressed as a linear combination of kernel functions defined on training samples.\n\nRepresenter Theorem. (Sch\u00f6lkopf, Herbrich, and Smola 2001) Consider a non-empty set P and a positive-definite real-valued kernel: k : P \u00d7P \u2192 R with a corresponding reproducing kernel Hilbert space H k . If given: a. a set of training samples {(p i , y i ) \u2208 P \u00d7 R|i = 1, \u00b7 \u00b7 \u00b7 , n}; b. a strictly monotonically increasing real-valued function g : [0, \u221e) \u2192 R; and c. an error function E : (P \u00d7 R 2 ) n \u2192 R \u222a {\u221e}, which together define the following regularized empirical risk functional on H k :\n\nf \u2192 E ((p 1 , y 1 , f (p 1 )) , . . . , (p n , y n , f (p n ))) + g( f )\n\nThen, any minimizer of the empirical risk admits a representation of the form:\nf * (\u00b7) = n i=1 \u03b1 i k (\u00b7, p i )\nwhere a i \u2208 R \u2200i = 1, \u00b7 \u00b7 \u00b7 , n. In our case, p i \u2208 R D is the embedding of node i. As shown in the theorem, the final optimized output is the linear combination of certain kernels on node embeddings. We assume the kernel function to be dot product for simplicity, which means k(p i , p j ) = p T i p j . The corresponding kernel matrix can be written as:\nK(GCN s (A, X)) = A k XX T A k = A k BA k(15)\nwhere B = XX T is the adjacency matrix of graph induced by node features. Now we have two graphs based on the same node set: original graph G A (associated with adjacency matrix A) and feature graph G B (associated with adjacency matrix B). They form a multigraph (Balakrishnan 1997) where multiple edges is permitted between the same end nodes. Then the random-walk-like matrix A k BA k can be regarded as one way to perform graph label/feature propagation on the multigraph. Its limitation is obvious: the propagation only happens once on the feature graph G B , which lacks flexibility. However, for our method, we have:\nGRCN (A, X) =(A + K(GCN s (A, X))) k X =(A + A m XX T A m ) k X =(A + A m BA m ) k X, K(GRCN (A, X)) =(A + A m BA m ) k B (A + A m BA m ) k ,(16)\nwhere labels/features can propagate multiple times on the feature graph G B . Thus our model is more flexible and more effective especially when the original graph G A is not reliable or cannot provide enough information for downstream tasks. In Equation (16)   We can further extend our model to a layer-wise version for comparison to GAT (Veli\u010dkovi\u0107 et al. 2017). More specifically, for the l-th layer, we denote the input as X l . The output X l+1 is then calculated by:\nX l+1 =(A + K(GCN s (A, X l )))X l =(A + A m X l X T l A m )X l =(A + A m B l A m )X l ,(17)\nwhere B l = X l X T l . Similar to the analysis mentioned before, if we consider the special case of GRCN that m = 0 and change the edge combination operator from entrywise sum \"+\" to entrywise product \"\u2022\", we have X l+1 = (A \u2022 B l )X l , which is the key idea behind GAT (Veli\u010dkovi\u0107 et al. 2017). Due to the property of entrywise product, the combined edges of both GAT and GLCN are only the reweighted edges of A, which becomes ineffective when the original graph G A is highly sparse. Through the analysis above, we see that our model is more general in combining different edges by varying the value of m, and also has more robust combination operator \"+\" compared to previous methods.\n\n\nExperiments\n\nWe evaluate the proposed GRCN model on semi-supervised node classification tasks, and conduct extensive experimental analysis in the following sections.\n\n\nDataset\n\nWe use six benchmark datasets for semi-supervised node classification evaluation. Among them, Cora, CiteSeer (Sen et al. 2008) and PubMed (Namata et al. 2012) are three commonly used datasets. For a more robust comparison of the model performance, we conduct 10 random splits while keeping the same number of labels for training, validation and testing as previous work (Yang, Cohen, and Salakhutdinov 2016). To further test the scalability of our model, we utilize three other datasets: Cora-Full (Bojchevski and G\u00fcnnemann 2018), Amazon-Computers and Coauthor CS (Shchur et al. 2018). The first is an extended version of Cora, while the second and the third are copurchase and co-authorship graphs respectively. On these three datasets, we follow the previous work (Shchur et al. 2018) and take 20 labels of each classes for training, 30 for validation, and the rest for testing. We also delete the classes with less than 50 labels to make sure each class contains enough instances. The data statistics are shown in Table 1 \n\n\nBaselines\n\nWe compare the effectiveness of our GRCN model with several baselines, where the first two models are vanilla graph convolutional networks without any graph revision: \n\n\nImplementation Details\n\nTransductive setting is used for node classification on all the datasets. We train GRCN for 300 epochs using Adam (Kingma and Ba 2014) and select the model with highest validation accuracy for test. We set learning rate as 1e\u22123 for graph refinement module and 5e\u22123 for label prediction module. Weight decay and sparsification parameter K are tuned by grid search on validation set, with the search space [1e\u22124, 5e\u22124, 1e\u22123, 5e\u22123, 1e\u22122, 5e\u22122] and [5,10,20,30,50,100,200] Table 2 shows the mean accuracy and the corresponding standard deviation for all models across the 6 datasets averaged over 10 different runs. We see that our proposed model achieves the best or highly competitive results for all the datasets. The effectiveness of our model over the other baselines demonstrates that taking the original graph as input for GCN is not optimal for graph propagation in semisupervised classification.\n\n\nMain Results\n\nTo further test the superiority of our model, we consider the edge-sparse scenario when a certain fraction of edges in the given graph is randomly removed. Given an edge retaining ratio, we randomly sample the retained edges 10 times and report the mean classification accuracy and standard deviation. Figure 2 shows the results under different ratios of retained edges. There are several observations from this figure. First, our model GRCN achieves notable improvement on almost all the datasets, especially when edge retaining ratio is low. For instance, when edge retaining ratio is 10%, our model outperforms the second best model by 6.5%, 2.5%, 1.1%, 11.0%, 4.6%, 2.3% on each dataset. Second, the GAT and GLCN models which reweight the existing edges do not perform well, indicating that such a reweighting mechanism is not enough when the original graph is highly incomplete. Third, our method also outperforms the over-parameterized model LDS in Cora and CiteSeer because of our restrained edge editing procedure. Though LDS achieves better performances than other baseline methods in these two datasets, its inability to scale prevents us from testing it on four of the larger datasets.\n\n\nRobustness on Training Labels\n\nWe also show that the gains achieved by our model are very robust to the reduction in the number of training labels for each class, denoted by T . We compare all the models on the Cora-Full, Amazon Computers and Coauthor CS datasets and fix the edge sampling ratio to 20%. We reduce T from 15 to 5 and report the results in Table 3. While containing more parameters than vanilla GCN, our model still outperforms others. Moreover, it wins by a larger margin when T is smaller. This demonstrates our model's capability to handle tasks with sparse training labels.\n\n\nHyperparameter Analysis\n\nWe investigate the influence of the hyperparameter K in this section. After calculating the similarity graph in GRCN , we use a K-nearest-neighbour to generate a sparse graph out of the dense graph. This is not only benificial to efficiency, but also important for effectiveness. Figure 3 shows the results of classification accuracy vs. sampling ratio on Cora dataset, where we vary the edge sampling ratio from 10% to 100%   and change K from 5 to 200. From this figure, increasing the value of K helps improve the classification accuracy at the initial stage. However, after reaching a peak, further increasing K lowers the model performance. We conjecture that this is because a larger K will introduce too much noise and thus lower the quality of the revised graph.\n\n\nAblation Study\n\nTo further examine the effectiveness of our GCN-based graph revision module, we conduct an ablation study by testing three different simplifications of the graph revision module:\n\n\u2022 Feature-Only (FO): A = K(X)\n\n\u2022 Feature plus Graph (FG): A = A + K(X) \u2022 Random Walk Feature plus Graph (RWFG):\nA = A + K(A 2 X)\nNote that FO is the simplest method and only uses the node features to construct the graph, without any information of the original graph. This is followed by the FG method, which adds the original graph to the feature similarity graph used in FO. Our model is most closely related to the third method, RWFG, which constructs the feature graph with similarity of node features via graph propagation, but without feature learning.\n\nWe conduct the ablation experiment on Cora dataset with different edge retaining ratios and report the results in Figure  4. The comparison between FO and FG shows that adding original graph as residual links is helpful for all edge retaing ratios, especially when there are more known edges in the graph. Examining the results of FG and RWFG, we can also observe a large improvement brought by graph propagation on features. Finally, the performance of our model and RWFG underscores the importance of feature learning, especially in the cases of low edge retraining ratio. \n\n\nRelated Work Graph Convolutional Network\n\nGraph Convolution Networks (GCNs) were first introduced in the work by (Bruna et al. 2013), with subsequent development and improvements from (Henaff, Bruna, and Le-Cun 2015). Overall, GCNs can be categorized into two cat-egories: spectral convolution and spatial convolution. The spectral convolution operates on the spectral representation of graphs defined in the Fourier domain by the eigendecomposition of graph Laplacian (Defferrard, Bresson, and Vandergheynst 2016;Kipf and Welling 2016). The spatial convolution operates directly on the graph to aggregate groups of spatially close neighbors (Atwood and Towsley 2016;. Besides those methods that are directly applied to an existing graph, GAT (Veli\u010dkovi\u0107 et al. 2017), GLCN (Jiang et al. 2019) use attention mechanism or feature similarity to reweight the original graph for better GCN performance, while LDS (Franceschi et al. 2019) reconstructs the entire graph via a bilevel optimization. Although our work is related to these methods, we develop a different strategy for graph revision that maintains both efficiency and high flexibility.\n\n\nLink prediction\n\nLink prediction aims at identifying missing links, or links that are likely to be formed in a given network. It is widely applicable to many tasks, including prediction of friendship in social network (Dong et al. 2012) and affinities between users and items in recommender systems (Berg, Kipf, and Welling 2017). Previous line of work uses heuristic methods based on local neighborhood structure of nodes, including first-order heuristics by common neighbors and preferential attachment (Barab\u00e1si and Albert 1999), second-order heuristics by Adamic-Adar and resource allocation (Zhou, L\u00fc, and Zhang 2009), or high-order heuristics by PageRank (Brin and Page 1998). To loose the strong assumptions of heuristic method, a number of neural network based methods (Grover and Leskovec 2016; Zhang and Chen 2018) are proposed, which are capable to learn general structure features. The problem we study in this paper is related to link prediction since we try to revise the graph by adding or reweighting edges. However, instead of treating link prediction as an objective, our work focus on improving node classification by feeding the revised graph into GCNs.\n\n\nConclusion\n\nThis paper presents Graph-Revised Convolutional Network, a novel framework for incorporating graph revision into graph convolution networks. We show both theoretically and experimentally that the proposed way of graph revision can significantly enhance the prediction accuracy for downstream tasks, such as semi-supervised node classification. GRCN overcomes two main drawbacks in previous approaches to graph revision, which either employ overparameterized models and consequently face scaling issues, or fail to consider missing edges. In our experiments with node classification tasks, the performance of GRCN stands out in particular when the input graphs are highly incomplete or if the labeled training instances are very sparse. Additionally, as a key advantage, GRCN is also highly scalable to large graphs.\n\nIn the future, we plan to explore GRCN in a broader range of prediction tasks, such as knowledge base completion, epidemiological forecasting and aircraft anomaly detection based on sensor network data.\n\nFigure 1 :\n1Architecture of the proposed GRCN model for semi-supervised node classification. The node classification GCN is enhanced with a revised graph constructed by the graph revision GCN module.\n\n\n, A + A m BA m can be regarded as a combination of different edges in the multigraph. To reveal the connection between GRCN and GLCN (Jiang et al. 2019), we first consider the special case of our model that m = 0: GRCN (A, X) = (A + B) k X. The operator \"+\" is analogous to the operator OR which incorporates information from both graph A and B. While GLCN (Jiang et al. 2019) takes another combination denoted as A \u2022 B using Hadamard (entrywise) product \"\u2022\", which can be analogous to AN D operation.\n\n\n\u2022 GCN(Kipf and Welling 2016): one of the earlier models which performs a linear approximation to spectral graph convolution.\u2022 SGC (Wu et al. 2019) removes the nonlinearities and collapse weight matrices between consecutive layers, and thus can increase number of layers without introducing more model parameters. \u2022 GAT (Veli\u010dkovi\u0107 et al. 2017) uses an attention mechanism for edge reweighting during the feature aggregation step. \u2022 LDS (Franceschi et al. 2019) jointly learn the graph structure and parameters of graph convolution networks by solving a bilevel program. \u2022 GLCN (Jiang et al. 2019) integrates both graph learningand graph convolution in a unified network architecture, which is most related to our model.\n\n\nrespectively. Our code is based on Pytorch (Paszke et al. 2017) and one geometric deep learning extension library (Fey and Lenssen 2019), which provides implementation for GCN (Kipf and Welling 2016), SGC (Wu et al. 2019) and GAT (Veli\u010dkovi\u0107 et al. 2017). For LDS (Franceschi et al. 2019), the results were obtained using the publicly available code. Since an implementation for GLCN (Jiang et al. 2019) was not available, we report the results based on our own implementation of the original paper.\n\nFigure 2 :\n2Mean test classification accuracy on all the datasets under different ratios of retained edges over 10 different runs.\n\nFigure 3 :\n3Results of GRCN under different settings of sparsification parameter K on Cora dataset, with different edge retaining ratios.\n\nFigure 4 :\n4Results of our model and its simplified versions on Cora dataset with different ratios of retained edges\n\nTable 1 :\n1Data statistics\n\n\n.Cora \n(rand. splits) \n\nCiteSeer \n(rand. splits) \n\nPubMed \n(rand. splits) \nGCN \n81.2 \u00b1 1.9 \n69.8 \u00b1 1.9 \n77.7 \u00b1 2.9 \nSGC \n81.0 \u00b1 1.7 \n68.9 \u00b1 2.0 \n75.8 \u00b1 3.0 \nGAT \n81.7 \u00b1 1.9 \n68.8 \u00b1 1.8 \n77.7 \u00b1 3.2 \nLDS \n81.6 \u00b1 1.0 \n71.0 \u00b1 0.9 \nN/A \nGLCN \n81.4 \u00b1 1.9 \n69.8 \u00b1 1.8 \n77.2 \u00b1 3.2 \nGRCN 83.9 \u00b1 1.7 \n72.6 \u00b1 1.3 \n77.9 \u00b1 3.2 \n\nCora-Full \nAmazon \nComputers \n\nCoauthor \nCS \nGCN \n60.3 \u00b1 0.7 \n81.9 \u00b1 1.7 \n91.3 \u00b1 0.3 \nSGC \n59.1 \u00b1 0.7 \n81.8 \u00b1 2.3 \n91.3 \u00b1 0.2 \nGAT \n59.9 \u00b1 0.6 \n81.8 \u00b1 2.0 \n89.5 \u00b1 0.5 \nLDS \nN/A \nN/A \nN/A \nGLCN \n59.1 \u00b1 0.7 \n80.4 \u00b1 1.9 \n90.1 \u00b1 0.5 \nGRCN 60.3 \u00b1 0.3 \n83.3 \u00b1 1.6 \n91.3 \u00b1 0.4 \n\n\n\nTable 2 :\n2Mean test classification accuracy and standard devi-\nation in percent averaged for all models and all datasets. For \neach dataset, the highest accuracy score is marked in bold. \nN/A stands for the datasets that couldn't be processed by the \nfull-batch version because of GPU RAM limitations. \n\n\n\nTable 3 :\n3Mean test classification accuracy and standard de-\nviation on Cora-Full, Amazon Computers and Coauthor CS \ndatasets under different number of training labels for each \nclass. The edge retaining ratio is 20% for all the results. For \neach dataset, the highest accuracy score is marked in bold. \n\n\n\nDiffusion-convolutional neural networks. J Atwood, D Towsley, V Balakrishnan, Advances in Neural Information Processing Systems. and Towsley. Graph theory (schaums outlineand Towsley 2016] Atwood, J., and Towsley, D. 2016. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems, 1993-2001. [Balakrishnan 1997] Balakrishnan, V. 1997. Graph theory (schaums outline).\n\nEmergence of scaling in random networks. A.-L Barab\u00e1si, Albert , R , science. 2865439[Barab\u00e1si and Albert 1999] Barab\u00e1si, A.-L., and Albert, R. 1999. Emergence of scaling in random networks. science 286(5439):509-512.\n\nDeep gaussian embedding of graphs: Unsupervised inductive learning via ranking. Kipf Berg, Welling, R V D Berg, T N Kipf, M Welling, A Bojchevski, S G\u00fcnnemann, arXiv:1706.02263International Conference on Learning Representations. arXiv preprintGraph convolutional matrix completionBerg, Kipf, and Welling 2017] Berg, R. v. d.; Kipf, T. N.; and Welling, M. 2017. Graph convolutional matrix com- pletion. arXiv preprint arXiv:1706.02263. [Bojchevski and G\u00fcnnemann 2018] Bojchevski, A., and G\u00fcnnemann, S. 2018. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. In International Conference on Learning Representations, 1-13.\n\nConvolutional neural networks on graphs with fast localized spectral filtering. S Brin, L Page, J Bruna, W Zaremba, A Szlam, Y Lecun, M Defferrard, X Bresson, P Vandergheynst, arXiv:1312.6203Spectral networks and locally connected networks on graphs. 30arXiv preprintAdvances in neural information processing systemsand Page 1998] Brin, S., and Page, L. 1998. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems 30(1-7):107-117. [Bruna et al. 2013] Bruna, J.; Zaremba, W.; Szlam, A.; and LeCun, Y. 2013. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203. [Defferrard, Bresson, and Vandergheynst 2016] Defferrard, M.; Bresson, X.; and Vandergheynst, P. 2016. Convolu- tional neural networks on graphs with fast localized spectral filtering. In Advances in neural information processing systems, 3844-3852.\n\nLink prediction and recommendation across heterogeneous social networks. Dong, 2012 IEEE 12th International conference on data mining. IEEE[Dong et al. 2012] Dong, Y.; Tang, J.; Wu, S.; Tian, J.; Chawla, N. V.; Rao, J.; and Cao, H. 2012. Link prediction and recommendation across heterogeneous social networks. In 2012 IEEE 12th International conference on data mining, 181-190. IEEE.\n\nLearning discrete structures for graph neural networks. M Fey, J E Lenssen, Franceschi, arXiv:1903.02428arXiv:1903.11960Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on Knowledge discovery and data miningLeskovec; Hamilton, Ying, and LeskovecACMarXiv preprintAdvances in Neural Information Processing Systemsand Lenssen 2019] Fey, M., and Lenssen, J. E. 2019. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428. [Franceschi et al. 2019] Franceschi, L.; Niepert, M.; Pontil, M.; and He, X. 2019. Learning discrete structures for graph neural networks. arXiv preprint arXiv:1903.11960. [Grover and Leskovec 2016] Grover, A., and Leskovec, J. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international con- ference on Knowledge discovery and data mining, 855-864. ACM. [Hamilton, Ying, and Leskovec 2017] Hamilton, W.; Ying, Z.; and Leskovec, J. 2017. Inductive representation learning on large graphs. In Advances in Neural Information Pro- cessing Systems, 1024-1034.\n\nSemi-supervised learning with graph learning-convolutional networks. Bruna Henaff, M Henaff, J Bruna, Y Lecun, B Jiang, Z Zhang, D Lin, J Tang, B Luo, arXiv:1506.05163Jiang et al. 2019Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionarXiv preprintDeep convolutional networks on graphstructured dataHenaff, Bruna, and LeCun 2015] Henaff, M.; Bruna, J.; and LeCun, Y. 2015. Deep convolutional networks on graph- structured data. arXiv preprint arXiv:1506.05163. [Jiang et al. 2019] Jiang, B.; Zhang, Z.; Lin, D.; Tang, J.; and Luo, B. 2019. Semi-supervised learning with graph learning-convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, 11313-11320.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprint[Kingma and Ba 2014] Kingma, D. P., and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n\nLarge-scale point cloud semantic segmentation with superpoint graphs. T N Kipf, M Welling, L Landrieu, M Simonovsky, arXiv:1609.02907Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionarXiv preprintSemi-supervised classification with graph convolutional networksand Welling 2016] Kipf, T. N., and Welling, M. 2016. Semi-supervised classification with graph convolutional net- works. arXiv preprint arXiv:1609.02907. [Landrieu and Simonovsky 2018] Landrieu, L., and Si- monovsky, M. 2018. Large-scale point cloud semantic segmentation with superpoint graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4558-4567.\n\nEncoding sentences with graph convolutional networks for semantic role labeling. D Marcheggiani, I Titov, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing[Marcheggiani and Titov 2017] Marcheggiani, D., and Titov, I. 2017. Encoding sentences with graph convolutional net- works for semantic role labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing, 1506-1515.\n\nGeometric deep learning on graphs and manifolds using mixture model cnns. [ Monti, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition8NIPS Autodiff Workshop[Monti et al. 2017] Monti, F.; Boscaini, D.; Masci, J.; Rodola, E.; Svoboda, J.; and Bronstein, M. M. 2017. Ge- ometric deep learning on graphs and manifolds using mix- ture model cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5115-5124. [Namata et al. 2012] Namata, G.; London, B.; Getoor, L.; Huang, B.; and EDU, U. 2012. Query-driven active sur- veying for collective classification. In 10th International Workshop on Mining and Learning with Graphs, 8. [Paszke et al. 2017] Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.; DeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A. 2017. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop.\n\nRevisiting semisupervised learning with graph embeddings. Herbrich Sch\u00f6lkopf, B Sch\u00f6lkopf, R Herbrich, A J Smola, Sen, arXiv:1811.05868arXiv:1603.08861Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningACM29arXiv preprintThe 41st International ACM SIGIR Conference on Research & Development in Information RetrievalSch\u00f6lkopf, Herbrich, and Smola 2001] Sch\u00f6lkopf, B.; Her- brich, R.; and Smola, A. J. 2001. A generalized represen- ter theorem. In International conference on computational learning theory, 416-426. Springer. [Sen et al. 2008] Sen, P.; Namata, G.; Bilgic, M.; Getoor, L.; Galligher, B.; and Eliassi-Rad, T. 2008. Collective classifi- cation in network data. AI magazine 29(3):93-93. [Shchur et al. 2018] Shchur, O.; Mumme, M.; Bojchevski, A.; and G\u00fcnnemann, S. 2018. Pitfalls of graph neural net- work evaluation. arXiv preprint arXiv:1811.05868. [Veli\u010dkovi\u0107 et al. 2017] Veli\u010dkovi\u0107, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903. [Wang et al. 2018] Wang, Y.; Sun, Y.; Liu, Z.; Sarma, S. E.; Bronstein, M. M.; and Solomon, J. M. 2018. Dynamic graph cnn for learning on point clouds. arXiv preprint arXiv:1801.07829. [Wu et al. 2018] Wu, Y.; Yang, Y.; Nishiura, H.; and Saitoh, M. 2018. Deep learning for epidemiological predictions. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, 1085-1088. ACM. [Wu et al. 2019] Wu, F.; Zhang, T.; Souza Jr, A. H. d.; Fifty, C.; Yu, T.; and Weinberger, K. Q. 2019. Simplifying graph convolutional networks. arXiv preprint arXiv:1902.07153. [Yang, Cohen, and Salakhutdinov 2016] Yang, Z.; Cohen, W. W.; and Salakhutdinov, R. 2016. Revisiting semi- supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861. [Ying et al. 2018] Ying, R.; He, R.; Chen, K.; Eksombatchai, P.; Hamilton, W. L.; and Leskovec, J. 2018. Graph con- volutional neural networks for web-scale recommender sys- tems. In Proceedings of the 24th ACM SIGKDD Interna- tional Conference on Knowledge Discovery & Data Mining, 974-983. ACM.\n\nLink prediction based on graph neural networks. M Zhang, Chen , Y , Advances in Neural Information Processing Systems. [Zhang and Chen 2018] Zhang, M., and Chen, Y. 2018. Link prediction based on graph neural networks. In Advances in Neural Information Processing Systems, 5165-5175.\n\nPredicting missing links via local information. L\u00fc Zhou, Zhang, T Zhou, L L\u00fc, Y.-C Zhang, The European Physical Journal B. 714Zhou, L\u00fc, and Zhang 2009] Zhou, T.; L\u00fc, L.; and Zhang, Y.- C. 2009. Predicting missing links via local information. The European Physical Journal B 71(4):623-630.\n", "annotations": {"author": "[{\"end\":79,\"start\":39},{\"end\":143,\"start\":80},{\"end\":208,\"start\":144},{\"end\":267,\"start\":209},{\"end\":327,\"start\":268}]", "publisher": null, "author_last_name": "[{\"end\":49,\"start\":47},{\"end\":93,\"start\":88},{\"end\":158,\"start\":153},{\"end\":218,\"start\":216},{\"end\":279,\"start\":275}]", "author_first_name": "[{\"end\":46,\"start\":39},{\"end\":87,\"start\":80},{\"end\":152,\"start\":144},{\"end\":215,\"start\":209},{\"end\":274,\"start\":268}]", "author_affiliation": "[{\"end\":78,\"start\":51},{\"end\":142,\"start\":115},{\"end\":207,\"start\":180},{\"end\":266,\"start\":239},{\"end\":326,\"start\":299}]", "title": "[{\"end\":36,\"start\":1},{\"end\":363,\"start\":328}]", "venue": null, "abstract": "[{\"end\":1610,\"start\":365}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1899,\"start\":1881},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1928,\"start\":1899},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1981,\"start\":1958},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2009,\"start\":1981},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2050,\"start\":2031},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2067,\"start\":2050},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2113,\"start\":2097},{\"end\":2723,\"start\":2695},{\"end\":2752,\"start\":2728},{\"end\":3041,\"start\":3014},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3933,\"start\":3914},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5777,\"start\":5754},{\"end\":6574,\"start\":6550},{\"end\":6904,\"start\":6877},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10711,\"start\":10688},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13066,\"start\":13029},{\"end\":15224,\"start\":15197},{\"end\":15723,\"start\":15696},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16704,\"start\":16667},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17082,\"start\":17063},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17664,\"start\":17644},{\"end\":17978,\"start\":17975},{\"end\":17981,\"start\":17978},{\"end\":17984,\"start\":17981},{\"end\":17987,\"start\":17984},{\"end\":17990,\"start\":17987},{\"end\":17994,\"start\":17990},{\"end\":17998,\"start\":17994},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22505,\"start\":22486},{\"end\":22589,\"start\":22557},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22887,\"start\":22842},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22909,\"start\":22887},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23040,\"start\":23015},{\"end\":23140,\"start\":23112},{\"end\":23165,\"start\":23142},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23754,\"start\":23736},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23847,\"start\":23817},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24049,\"start\":24023},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24139,\"start\":24114},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24199,\"start\":24179},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26461,\"start\":26438}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25926,\"start\":25726},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26430,\"start\":25927},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27152,\"start\":26431},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27654,\"start\":27153},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27786,\"start\":27655},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27925,\"start\":27787},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28043,\"start\":27926},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28071,\"start\":28044},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28662,\"start\":28072},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":28969,\"start\":28663},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":29277,\"start\":28970}]", "paragraph": "[{\"end\":2124,\"start\":1626},{\"end\":2578,\"start\":2126},{\"end\":3406,\"start\":2580},{\"end\":4203,\"start\":3408},{\"end\":4339,\"start\":4205},{\"end\":4442,\"start\":4341},{\"end\":4708,\"start\":4444},{\"end\":4725,\"start\":4710},{\"end\":5834,\"start\":4740},{\"end\":6190,\"start\":5882},{\"end\":6501,\"start\":6220},{\"end\":6692,\"start\":6503},{\"end\":7068,\"start\":6785},{\"end\":7501,\"start\":7175},{\"end\":7758,\"start\":7503},{\"end\":8360,\"start\":7858},{\"end\":8957,\"start\":8418},{\"end\":9210,\"start\":8978},{\"end\":9940,\"start\":9251},{\"end\":10168,\"start\":9962},{\"end\":10442,\"start\":10204},{\"end\":10895,\"start\":10501},{\"end\":11354,\"start\":10914},{\"end\":11573,\"start\":11420},{\"end\":11898,\"start\":11687},{\"end\":12281,\"start\":11923},{\"end\":12472,\"start\":12283},{\"end\":12743,\"start\":12498},{\"end\":13006,\"start\":12781},{\"end\":13502,\"start\":13008},{\"end\":13576,\"start\":13504},{\"end\":13656,\"start\":13578},{\"end\":14044,\"start\":13689},{\"end\":14714,\"start\":14091},{\"end\":15334,\"start\":14861},{\"end\":16117,\"start\":15428},{\"end\":16285,\"start\":16133},{\"end\":17322,\"start\":16297},{\"end\":17503,\"start\":17336},{\"end\":18430,\"start\":17530},{\"end\":19643,\"start\":18447},{\"end\":20238,\"start\":19677},{\"end\":21036,\"start\":20266},{\"end\":21233,\"start\":21055},{\"end\":21264,\"start\":21235},{\"end\":21346,\"start\":21266},{\"end\":21793,\"start\":21364},{\"end\":22370,\"start\":21795},{\"end\":23515,\"start\":22415},{\"end\":24691,\"start\":23535},{\"end\":25521,\"start\":24706},{\"end\":25725,\"start\":25523}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5881,\"start\":5835},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6219,\"start\":6191},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6784,\"start\":6693},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7174,\"start\":7069},{\"attributes\":{\"id\":\"formula_4\"},\"end\":7857,\"start\":7759},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8977,\"start\":8958},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9250,\"start\":9211},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9961,\"start\":9941},{\"attributes\":{\"id\":\"formula_8\"},\"end\":10203,\"start\":10169},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10500,\"start\":10443},{\"attributes\":{\"id\":\"formula_10\"},\"end\":11419,\"start\":11355},{\"attributes\":{\"id\":\"formula_11\"},\"end\":11686,\"start\":11574},{\"attributes\":{\"id\":\"formula_12\"},\"end\":12497,\"start\":12473},{\"attributes\":{\"id\":\"formula_13\"},\"end\":12780,\"start\":12744},{\"attributes\":{\"id\":\"formula_14\"},\"end\":13688,\"start\":13657},{\"attributes\":{\"id\":\"formula_15\"},\"end\":14090,\"start\":14045},{\"attributes\":{\"id\":\"formula_16\"},\"end\":14860,\"start\":14715},{\"attributes\":{\"id\":\"formula_17\"},\"end\":15427,\"start\":15335},{\"attributes\":{\"id\":\"formula_18\"},\"end\":21363,\"start\":21347}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17321,\"start\":17314},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18006,\"start\":17999},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":20008,\"start\":20001}]", "section_header": "[{\"end\":1624,\"start\":1612},{\"end\":4738,\"start\":4728},{\"end\":8378,\"start\":8363},{\"end\":8416,\"start\":8381},{\"end\":10912,\"start\":10898},{\"end\":11921,\"start\":11901},{\"end\":16131,\"start\":16120},{\"end\":16295,\"start\":16288},{\"end\":17334,\"start\":17325},{\"end\":17528,\"start\":17506},{\"end\":18445,\"start\":18433},{\"end\":19675,\"start\":19646},{\"end\":20264,\"start\":20241},{\"end\":21053,\"start\":21039},{\"end\":22413,\"start\":22373},{\"end\":23533,\"start\":23518},{\"end\":24704,\"start\":24694},{\"end\":25737,\"start\":25727},{\"end\":27666,\"start\":27656},{\"end\":27798,\"start\":27788},{\"end\":27937,\"start\":27927},{\"end\":28054,\"start\":28045},{\"end\":28673,\"start\":28664},{\"end\":28980,\"start\":28971}]", "table": "[{\"end\":28662,\"start\":28075},{\"end\":28969,\"start\":28675},{\"end\":29277,\"start\":28982}]", "figure_caption": "[{\"end\":25926,\"start\":25739},{\"end\":26430,\"start\":25929},{\"end\":27152,\"start\":26433},{\"end\":27654,\"start\":27155},{\"end\":27786,\"start\":27668},{\"end\":27925,\"start\":27800},{\"end\":28043,\"start\":27939},{\"end\":28071,\"start\":28056},{\"end\":28075,\"start\":28074}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10067,\"start\":10059},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18757,\"start\":18749},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20554,\"start\":20546},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21918,\"start\":21909}]", "bib_author_first_name": "[{\"end\":29321,\"start\":29320},{\"end\":29331,\"start\":29330},{\"end\":29342,\"start\":29341},{\"end\":29729,\"start\":29725},{\"end\":29746,\"start\":29740},{\"end\":29750,\"start\":29749},{\"end\":29987,\"start\":29983},{\"end\":30004,\"start\":30003},{\"end\":30008,\"start\":30005},{\"end\":30016,\"start\":30015},{\"end\":30018,\"start\":30017},{\"end\":30026,\"start\":30025},{\"end\":30037,\"start\":30036},{\"end\":30051,\"start\":30050},{\"end\":30636,\"start\":30635},{\"end\":30644,\"start\":30643},{\"end\":30652,\"start\":30651},{\"end\":30661,\"start\":30660},{\"end\":30672,\"start\":30671},{\"end\":30681,\"start\":30680},{\"end\":30690,\"start\":30689},{\"end\":30704,\"start\":30703},{\"end\":30715,\"start\":30714},{\"end\":31889,\"start\":31888},{\"end\":31896,\"start\":31895},{\"end\":31898,\"start\":31897},{\"end\":33062,\"start\":33057},{\"end\":33072,\"start\":33071},{\"end\":33082,\"start\":33081},{\"end\":33091,\"start\":33090},{\"end\":33100,\"start\":33099},{\"end\":33109,\"start\":33108},{\"end\":33118,\"start\":33117},{\"end\":33125,\"start\":33124},{\"end\":33133,\"start\":33132},{\"end\":33831,\"start\":33830},{\"end\":33833,\"start\":33832},{\"end\":33843,\"start\":33842},{\"end\":34078,\"start\":34077},{\"end\":34080,\"start\":34079},{\"end\":34088,\"start\":34087},{\"end\":34099,\"start\":34098},{\"end\":34111,\"start\":34110},{\"end\":34831,\"start\":34830},{\"end\":34847,\"start\":34846},{\"end\":35345,\"start\":35344},{\"end\":36295,\"start\":36287},{\"end\":36308,\"start\":36307},{\"end\":36321,\"start\":36320},{\"end\":36333,\"start\":36332},{\"end\":36335,\"start\":36334},{\"end\":38514,\"start\":38513},{\"end\":38526,\"start\":38522},{\"end\":38530,\"start\":38529},{\"end\":38800,\"start\":38798},{\"end\":38815,\"start\":38814},{\"end\":38823,\"start\":38822},{\"end\":38832,\"start\":38828}]", "bib_author_last_name": "[{\"end\":29328,\"start\":29322},{\"end\":29339,\"start\":29332},{\"end\":29355,\"start\":29343},{\"end\":29738,\"start\":29730},{\"end\":29992,\"start\":29988},{\"end\":30001,\"start\":29994},{\"end\":30013,\"start\":30009},{\"end\":30023,\"start\":30019},{\"end\":30034,\"start\":30027},{\"end\":30048,\"start\":30038},{\"end\":30061,\"start\":30052},{\"end\":30641,\"start\":30637},{\"end\":30649,\"start\":30645},{\"end\":30658,\"start\":30653},{\"end\":30669,\"start\":30662},{\"end\":30678,\"start\":30673},{\"end\":30687,\"start\":30682},{\"end\":30701,\"start\":30691},{\"end\":30712,\"start\":30705},{\"end\":30729,\"start\":30716},{\"end\":31523,\"start\":31519},{\"end\":31893,\"start\":31890},{\"end\":31906,\"start\":31899},{\"end\":31918,\"start\":31908},{\"end\":33069,\"start\":33063},{\"end\":33079,\"start\":33073},{\"end\":33088,\"start\":33083},{\"end\":33097,\"start\":33092},{\"end\":33106,\"start\":33101},{\"end\":33115,\"start\":33110},{\"end\":33122,\"start\":33119},{\"end\":33130,\"start\":33126},{\"end\":33137,\"start\":33134},{\"end\":33840,\"start\":33834},{\"end\":33846,\"start\":33844},{\"end\":34085,\"start\":34081},{\"end\":34096,\"start\":34089},{\"end\":34108,\"start\":34100},{\"end\":34122,\"start\":34112},{\"end\":34844,\"start\":34832},{\"end\":34853,\"start\":34848},{\"end\":35351,\"start\":35346},{\"end\":36305,\"start\":36296},{\"end\":36318,\"start\":36309},{\"end\":36330,\"start\":36322},{\"end\":36341,\"start\":36336},{\"end\":36346,\"start\":36343},{\"end\":38520,\"start\":38515},{\"end\":38805,\"start\":38801},{\"end\":38812,\"start\":38807},{\"end\":38820,\"start\":38816},{\"end\":38826,\"start\":38824},{\"end\":38838,\"start\":38833}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15483870},\"end\":29682,\"start\":29279},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":524106},\"end\":29901,\"start\":29684},{\"attributes\":{\"doi\":\"arXiv:1706.02263\",\"id\":\"b2\",\"matched_paper_id\":4630420},\"end\":30553,\"start\":29903},{\"attributes\":{\"doi\":\"arXiv:1312.6203\",\"id\":\"b3\",\"matched_paper_id\":3016223},\"end\":31444,\"start\":30555},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1547016},\"end\":31830,\"start\":31446},{\"attributes\":{\"doi\":\"arXiv:1903.02428\",\"id\":\"b5\",\"matched_paper_id\":85543335},\"end\":32986,\"start\":31832},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":195497272},\"end\":33784,\"start\":32988},{\"attributes\":{\"id\":\"b7\"},\"end\":34005,\"start\":33786},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4396837},\"end\":34747,\"start\":34007},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16839291},\"end\":35268,\"start\":34749},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":301319},\"end\":36227,\"start\":35270},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7008752},\"end\":38463,\"start\":36229},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3573161},\"end\":38748,\"start\":38465},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":14353767},\"end\":39038,\"start\":38750}]", "bib_title": "[{\"end\":29318,\"start\":29279},{\"end\":29723,\"start\":29684},{\"end\":29981,\"start\":29903},{\"end\":30633,\"start\":30555},{\"end\":31517,\"start\":31446},{\"end\":31886,\"start\":31832},{\"end\":33055,\"start\":32988},{\"end\":34075,\"start\":34007},{\"end\":34828,\"start\":34749},{\"end\":35342,\"start\":35270},{\"end\":36285,\"start\":36229},{\"end\":38511,\"start\":38465},{\"end\":38796,\"start\":38750}]", "bib_author": "[{\"end\":29330,\"start\":29320},{\"end\":29341,\"start\":29330},{\"end\":29357,\"start\":29341},{\"end\":29740,\"start\":29725},{\"end\":29749,\"start\":29740},{\"end\":29753,\"start\":29749},{\"end\":29994,\"start\":29983},{\"end\":30003,\"start\":29994},{\"end\":30015,\"start\":30003},{\"end\":30025,\"start\":30015},{\"end\":30036,\"start\":30025},{\"end\":30050,\"start\":30036},{\"end\":30063,\"start\":30050},{\"end\":30643,\"start\":30635},{\"end\":30651,\"start\":30643},{\"end\":30660,\"start\":30651},{\"end\":30671,\"start\":30660},{\"end\":30680,\"start\":30671},{\"end\":30689,\"start\":30680},{\"end\":30703,\"start\":30689},{\"end\":30714,\"start\":30703},{\"end\":30731,\"start\":30714},{\"end\":31525,\"start\":31519},{\"end\":31895,\"start\":31888},{\"end\":31908,\"start\":31895},{\"end\":31920,\"start\":31908},{\"end\":33071,\"start\":33057},{\"end\":33081,\"start\":33071},{\"end\":33090,\"start\":33081},{\"end\":33099,\"start\":33090},{\"end\":33108,\"start\":33099},{\"end\":33117,\"start\":33108},{\"end\":33124,\"start\":33117},{\"end\":33132,\"start\":33124},{\"end\":33139,\"start\":33132},{\"end\":33842,\"start\":33830},{\"end\":33848,\"start\":33842},{\"end\":34087,\"start\":34077},{\"end\":34098,\"start\":34087},{\"end\":34110,\"start\":34098},{\"end\":34124,\"start\":34110},{\"end\":34846,\"start\":34830},{\"end\":34855,\"start\":34846},{\"end\":35353,\"start\":35344},{\"end\":36307,\"start\":36287},{\"end\":36320,\"start\":36307},{\"end\":36332,\"start\":36320},{\"end\":36343,\"start\":36332},{\"end\":36348,\"start\":36343},{\"end\":38522,\"start\":38513},{\"end\":38529,\"start\":38522},{\"end\":38533,\"start\":38529},{\"end\":38807,\"start\":38798},{\"end\":38814,\"start\":38807},{\"end\":38822,\"start\":38814},{\"end\":38828,\"start\":38822},{\"end\":38840,\"start\":38828}]", "bib_venue": "[{\"end\":32173,\"start\":32052},{\"end\":33313,\"start\":33251},{\"end\":34281,\"start\":34219},{\"end\":35014,\"start\":34943},{\"end\":35494,\"start\":35432},{\"end\":36559,\"start\":36478},{\"end\":29406,\"start\":29357},{\"end\":29760,\"start\":29753},{\"end\":30131,\"start\":30079},{\"end\":30804,\"start\":30746},{\"end\":31579,\"start\":31525},{\"end\":32050,\"start\":31952},{\"end\":33249,\"start\":33172},{\"end\":33828,\"start\":33786},{\"end\":34217,\"start\":34140},{\"end\":34941,\"start\":34855},{\"end\":35430,\"start\":35353},{\"end\":36476,\"start\":36380},{\"end\":38582,\"start\":38533},{\"end\":38871,\"start\":38840}]"}}}, "year": 2023, "month": 12, "day": 17}