{"id": 258714822, "updated": "2023-11-06 16:09:03.658", "metadata": {"title": "Large Language Models are Built-in Autoregressive Search Engines", "authors": "[{\"first\":\"Noah\",\"last\":\"Ziems\",\"middle\":[]},{\"first\":\"Wenhao\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Zhihan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Meng\",\"last\":\"Jiang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.09612", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/Ziems0Z023", "doi": "10.18653/v1/2023.findings-acl.167"}}, "content": {"source": {"pdf_hash": "a19193ef534619654d3252d4034ef154407562dc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.09612v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7d4b30be9bb8d5a6f40b4c6b2862c01e99a51870", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a19193ef534619654d3252d4034ef154407562dc.txt", "contents": "\nLarge Language Models are Built-in Autoregressive Search Engines\n\n\nNoah Ziems nziems2@nd.edu \nUniversity of Notre Dame\n\n\nWenhao Yu \nUniversity of Notre Dame\n\n\nZhihan Zhang zzhang23@nd.edu \nUniversity of Notre Dame\n\n\nMeng Jiang mjiang2@nd.edu \nUniversity of Notre Dame\n\n\nLarge Language Models are Built-in Autoregressive Search Engines\n\nDocument retrieval is a key stage of standard Web search engines. Existing dualencoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dualencoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval.Surprisingly, when providing a few Query-URL pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as builtin search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at https: //github.com/Ziems/llm-url.\n\nIntroduction\n\nAlong with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;. For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020).\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022). To reduce the high training cost of autoregressive search engine, a smaller model size is preferred. However, the results of our pilot study in Figure 1 show smaller language models are significantly worse at mapping passages to document identifiers than larger ones. Moreover, different retrieval tasks can have unique retrieval requirements. One task may require a model to retrieve factual evidence to support or refute a claim (i.e., fact checking) (Onoe et al., 2021) while another may require a model to retrieve specific trivia information about an entity (i.e., entity linking) (Petroni et al., 2021;. It would be better if the retriever was capable of generalizing to new retrieval tasks with only a few examples.\n\nIn this work, we explore the use of in-context demonstrations to prompt LLMs to directly generate web URLs for document retrieval, namely LLM-URL. Surprisingly, we find that by providing a few (query, URL) pairs as contextual demonstrations, large language models (e.g. GPT-3) generate Web URLs where nearly 90% of the corresponding documents contain answers to opendomain questions. In this way, LLMs can be thought of as built-in search engines, as they have not been explicitly trained to map questions or documents to identifiers. Instead of using newlycreated document identifiers, LLM-URL leverages existing and widely used document identifiers directly, i.e., URLs. We compare our approach to existing document retrieval methods on three different open-domain question answering (QA) datasets: WebQ (Berant et al., 2013), NQ (Kwiatkowski et al., 2019), and TriviaQA (Joshi et al., 2017). Further, to avoid exceeding the limit on the number of input tokens of LLMs, we employ an unsupervised passage filtering module to remove irrelevant portions of supporting documents. To summarize, our main contributions are as follows:\n\n1. We reveal that LLMs are built-in autoregressive search engines capable of document re-trieval by directly generating Web page URLs under both zero and few-shot settings.\n\n2. We show retrieving documents by generating URLs with LLMs significantly outperforms existing methods for document retrieval, as measured by Recall@K. Further, we show that breaking the retrieved documents into passages then using a ranker to filter the passages significantly reduces the number of supporting passages while maintaining high recall.\n\n3. We show the retrieved documents improve downstream QA performance as measured by EM when compared to baseline methods.\n\n2 Related Work\n\n\nTraditional Document Retrievers\n\nTraditional methods such as TF-IDF and BM25 explore sparse retrieval strategies by matching the overlapping contents between questions and passages (Robertson and Zaragoza, 2009;Chen et al., 2017;Yang et al., 2019). DPR (Karpukhin et al., 2020) revolutionized the field by utilizing dense contextualized vectors for passage indexing. It is first initialized as a pretrained BERT model, then trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25. Recent research has improved DPR via better training strategies (Xiong et al., 2020;Qu et al., 2021;Zhang et al., 2023a) and passage reranking (Mao et al., 2021;Yu et al., 2021;Ju et al., 2022). However, representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021).\n\n\nAutoregressive Search Engines\n\nRecent works have investigated the use of autoregressive language models to generate identifier strings for documents as an intermediate target for retrieval , such as Wikipedia page titles (De Cao et al., 2020), root-to-leaf paths in a hierarchical cluster tree (Tay et al., 2022), or distinctive n-grams that can be mapped to full passages (Bevilacqua et al., 2022). Since the series of work was carried out almost simultaneously by different research groups, they are often referred to multiple different names in the literature, such as autoregressive search engine, differential search  index (DSI), and neural document indexers (NDI). Compared to traditional dense document retrievers, these methods leverage a generation model to produce the document indexes. By forcing the generation model to explain every token in the question and document using cross-attention, the generation abilities of the model significantly improve. Our work is closely related to these works, showing experimentally that properly prompting pretrained large language models can achieve better performance than traditional dense retrieval models (Ouyang et al., 2022;Yu et al., 2023) .\n\n\nProposed Method\n\nIn this section we describe a new method, which we refer to as LLM-URL, that employs a large language model (LLM) to perform effective and efficient web document retrieval for knowledgeintensive NLP tasks such as open-domain question answering (ODQA). ODQA is a two step process consisting of a retriever and a reader. Given a question q, the goal of the retriever is to find the top-n passages P n relevant to answering q. Given q and the top-n relevant passages P n , the goal of the reader is to use internal knowledge along with P n to generate a correct answer a to question q. The passage retriever plays an essential role in this process. When P n contains more passages that have the correct answer, the reader has a higher chance of finding it. Instead of heavily training a dedicated retriever, our LLM-URL solves the problem in a different way as shown in Figure 2.\n\nGiven a question q, our LLM-URL should find a set of relevant passages to P n and give it to the reader. First, it prompts a LLM (e.g., GPT-3) to directly generate m URLs for q. By default, it uses \"Which m Wikipedia URLs would have the answer?\" as the instruction which is appended to each input question as the prompt. We also append the beginning of the Wikipedia URL (https://en.wikipedia.org/wiki) to the end of the prompt to encourage the generation of URLs and restrict generation to the Wikipedia article URL format. As LLM has the ability of in-context learning, we take this advantage to enable the fewshot setting in the prompt. The prompt described above also includes a series of in-context demonstrations. Each demonstration contains a question sampled from the training set following the prompt described above. At the end of each demonstration, m URLs which point to gold-labeled documents are listed. In the zero-shot setting, the original prompt is used without any demonstrations. In the few-shot setting, the original prompt appended to a series of d demonstrations (d=10 in this work).\n\nGiven the prompt, the LLM returns a generated sequence of tokens. Ideally these tokens would construct a sequence of m separated URLs. In practice, the generated sequence often has extra information such as a proposed answer that is unreliable and needs to be filtered. We use a regular expression to extract all URLs from the sequence and discard all extra information. This also filters out many URLs that are improperly formatted. After extraction, GET requests are made using the extracted URLs and the contents of each retrieval is used to create a set of fetched documents D f . Often, |D f | < m because some of the generated URLs do not follow a correct format or do not point to real web pages on the Internet. The set of fetched documents D f can be passed directly to a reader if m is a small value or the reader being used can handle many large documents. However, this is usually not the case. Often, D f needs to be filtered such that only a small number of the most relevant passages are given to the reader. To do this, our LLM-URL first breaks each document d \u2208 D f into a set of small passages. The passages from each document are collected into a new set, P f . A scoring function is used to quantify the relevance of each passage with respect to the question q, with high values indicating high relevance with respect to q and low scores indicating low relevance. A simple scoring function such as BM25 can be used or a more complex one such as DPR (Karpukhin et al., 2020) can. The passages in P f are then sorted from highest to lowest and the top n are kept as P n . Finally, P n are given to a reader along with q to generate an answer.\n\nAdvantages of LLM-URL : Existing autoregressive retrieval methods such as DSI and SEAL use a pre-trained large language model then fine tune it to take questions as input and generate relevant document identifiers as output (Tay et al., 2022;Bevilacqua et al., 2022). Both DSI and SEAL do extensive experiments on a variety of document identifiers which are generated by a heavily trained language model. Examples of these identifiers include unstructured atomic identifiers, naively structured string identifiers, hierarchical document clustering, and others. LLM-URL instead uses pre-existing document identifiers that exist on the internet: URLs. Using URLs instead of the aforementioned identifiers has multiple advantages. URLs often contain words related to the information they link to, allowing for strong association of topics with their URLs. For example, the title of each Wikipedia page is used in its URL, allowing the LLM is able to directly generate the URL by leveraging semantic information from the question. To validate the importance of URLs themselves, we also experiment with prompting the LLM to generate Wikipedia titles instead of URLs and find Recall@1 significantly reduces compared to prompting for URL generation. We believe this is because the URL format itself helps prompt the model for specific information in a specific format. Further, the use of URLs allows us to simply obtain the evidence document via a HTTP request without any need of training a model or building an index to find the mapping between identifiers and documents.\n\n\nExperiments\n\nIn this section, we present and discuss results from our experiments to \"directly\" demonstrate that our LLM-URL is a strong retriever and \"indirectly\" show that it achieves competitive performance on the ODQA task against state-of-the-art solutions.\n\nLarge Language Model: Following Figure 1, the large language model we use to generate URLs for our experiments is GPT-3 text-davinci-003 with greedy decoding and a temperature of 0. A variety of different prompts are tested for generating URLs, but little difference in performance is observed, so we simply use the best performing prompt which is discussed in Section 3.\n\n\nDatasets:\n\nWe use three ODQA datasets including Web Questions, Natural Questions, and Trivia QA. We use them to perform evaluation on both the task of document or passage retrieval and ODQA itself.\n\n\nRetrieval\n\nWe expect retrievers to find the most relevant documents and/or passages. We conduct experiments on both document retrieval and passage retrieval.  Evaluation metrics. Recall@k (k=1, 10, 100) is calculated by measuring the percentage of documents or passages in the top-k which contain one of the gold labeled answers while exact match is calculated by the percentage of predicted answers which match one of the gold labeled answers. While LLM-URL is not constrained by which URLs can be generated for document retrieval, we restrict all generations to Wikipedia URLs only for fair comparison, as discussed in Section 3 All baseline models also use Wikipedia for retrieval, with some fetching documents in real time and others fetching from an offline corpus.\n\n\nDocument Retrieval\n\nBaselines: Contriever (Izacard et al., 2021) and BM25 (Robertson and Zaragoza, 2009) are usually used for passage retrieval. Contriever is a dual encoder which uses a dot product between dense representations of a question and passage to calculate relevance. BM25 is a sparse retriever which uses the overlapping contents between question and passage to calculate relevance. Because we use the same passage size to chunk Wikipedia documents, we were able to map their retrieved passages back to the original documents. We use Google API (Brin and Page, 1998) restricted to Wikipedia as a third baseline to retrieve relevant documents given a question. Existing works such as DSI and SEAL have investigated the use of autoregressive language mod-els to generate identifier strings for documents as an intermediate target for retrieval. DSI is a Transformer which has been trained to map directly from question to document identifiers by memorizing the contents of the entire corpus (Tay et al., 2022). SEAL is a variant of DSI which uses ngrams as document ids to improve retrieval performance (Bevilacqua et al., 2022). Neither DSI nor SEAL report retrieval results on full documents and do not have publicly available implementations, so they are left out and discussed in Table 3 and Section 4.1.2 on passage retrieval.\n\nUnlike the baselines, our LLM-URL employs an LLM. It has two settings: zero-shot and fewshot. In the zero-shot setting, no in-context demonstrations are given whereas in the few-shot setting a few demonstrations are appended to the prompt.\n\n\nResults:\n\nThe results of our document retrieval experiments are shown in Table 1. In this setting Recall@k is calculated directly after the documents are retrieved with no intermediary steps. LLM-URL significantly outperforms baseline methods on all datasets for both Recall@1 and Recall@10. Specifically, zero-shot LLM-URL improves document Recall@1 relatively by 20.4%, 11.2%, and 13.2% over the strongest baseline on WebQ, NQ, and TriviaQA, respectively. Few-shot LLM-URL further expands the improvement to 24.9%, 12.8%, and 16.7%, respectively. URLs can be extracted from the large-scale parameters of LLMs, and these   URLs can lead to more accurate documents than what existing methods can retrieve. Both the LLM parameters and in-context demonstrations are significantly useful in document retrieval. Figure 3 shows Recall scores converge when the number of generated URLs m increases. Due to the diminishing returns from increasing m, our experiments do not explore values of m greater than 10.\n\nAre the generated URLs valid? It is worth noting that the generated URLs are not always valid. Some generated URLs do not have valid URL syntax and some point to Wikipedia pages that do not exist. Rarely, URLs will be generated for domains aside from Wikipedia. For fair comparison, all of these faulty URLs are discarded and only documents coming from valid Wikipedia articles are kept.\n\nFurther analysis is done to measure the ratio of valid Wikipedia URLs while the total number of generated URLs m increases from 1 to 10, shown in Figure 4. The number of valid URL generations remains surprisingly high (i.e., higher than 68%) as m increases from 1 to 10. However, the rate of  valid generations appears to fall off as m increases, indicating there are diminishing returns from each marginal increase of m.\n\n\nPassage Retrieval\n\nBaselines: Four methods, including Contriever, BM25, DSI (Tay et al., 2022), and SEAL (Bevilacqua et al., 2022), were introduced in Section 4.1.1. Google API was used for document retrieval and not applied to passages.\n\n\nResults:\n\nThe results of our passage retrieval experiments are shown in Table 2. In this setting Recall@k is calculated on the top-k passages ranked by the ranker instead of just on the raw documents shown in Table 1. LLM-URL performs slightly better than baseline methods for Recall@1 and Recall@10 and as well as baseline methods for Recall@100. In the zero-shot setting, LLM-URL improves relative Recall@1 by 16.2%, 5.3%, and 1.1% with respect to the strongest baseline on WebQ, NQ, and TriviaA respectively. The few-shot setting of LLM-URL expands the improvement to 16.8%, 11.8%, and 6.3%, respectively. For Re-call@10, similar improvements can be seen.\n\nFor Recall@100, performance is better relative to baseline models for all datasets except NQ. In the zero-shot setting, LLM-URL improves the relative Recall@100 by 5.0% for WebQ and performs slightly worse than the best baseline method on NQ and TriviaQA by 1.7% and 0.4% respectively. The few-shot setting of LLM-URL for Recall@100 shows a slight improvement on WebQ and Trivi-aQA, but performs slightly worse than the strongest baseline on NQ.\n\nDespite being limited to only the passages from 10 documents, LLM-URL performs better than baseline methods for smaller k and performs as well as baseline methods for higher values of k.\n\nThe comparison between LLM-URL and existing document identifier-based methods such as DSI and SEAL are shown in Table 3. For Recall@1, zero-shot LLM-URL performs slightly worse than the best baseline by 8.8%. This performance gap is slightly smaller in the few-shot setting with LLM-URL performing 3.1% worse than the best baseline. For Recall@10, zero-shot LLM-URL performs worse than the best baseline by 18.7%. Few-shot LLM-URL performs only slightly better than the zero-shot setting, performing worse than the best baseline by 18.4%.\n\n\nOpen-Domain Question Answering\n\nEvaluation metric: We use exact match (EM), which is short for exact string match with the correct answer, because the goal of ODQA is to find an exact answer to any question using Wikipedia articles.\n\nResults: Here we discuss the downstream QA performance of LLM-URL. In this setting, an answer only has an exact match if the normalized generated text is within the list of acceptable answers to a question. When combined with InstructGPT as a reader, LLM-URL performs significantly better on WebQ and slightly better on TriviaQA when compared with the best performing baseline methods. On NQ, LLM-URL+InstructGPT performs worse than baseline NDIs and only slightly worse than the best remaining baseline. In the zero-shot setting, LLM-URL+InstructGPT improves upon the best baseline method by 13.3% and 1.3% on WebQ and TriviaQA respectively. LLM-URL +Instruct-GPT performs worse than the best baseline method by 39.5% on NQ. In the few-shot setting, LLM-URL+InstructGPT performs better than the best baseline method by 16.9% and 2.3% on WebQ and TriviaQA respectively. LLM-URL+InstructGPT performs worse than the best baseline method by 37.4% on NQ.\n\nDespite not being explicitly trained for retrieval, LLM-URL+InstructGPT performs significantly better than baseline methods for WebQ, achieves on-par performance with existing methods for Triv-iaQA, and performs slightly worse than existing methods for NQ.\n\nOur results indicate LLM-URL could be a promising solution to retrieval for a wide range of knowledge intensive tasks with little to no training data required.\n\n\nDiscussions\n\n\nTime Sensitive Queries\n\nThere are a number of additional qualitative benefits that LLM-URL has over existing methods. One large advantage of LLM-URL is that the documents are retrieved in real time from the source. So long as the source stays up to date without the URL itself changing, our proposed method is capable of answering time sensitive queries without any extra modifications.\n\nIn contrast, existing dual encoder approaches such as Contriever require a document to be reencoded each time it changes. Existing methods such as SEAL (Bevilacqua et al., 2022) and DSI are also tricky to keep up to date for time sensitive queries as the LLM would have to be retrained to learn the new content of the updated documents.\n\n\nFrequent Entities Analysis\n\nFollowing (Mallen et al., 2022), we analyze the retrieval performance of LLM-URL when the goldlabeled answer entity is common versus when it is not. For each question-answer pair in a given dataset we check to see if the labeled entity exists within the top one-million common entities from Wikipedia. Using this, we split our dataset into two distinct subsets: question-answer pairs that contain a common entity and those that do not. In measuring the performance of our model on these two distinct sets across Web Questions, Natural Questions, and TriviaQA, we find LLM-URL performs significantly better on common en-\n\n\nLLM-URL\n\n\nExists Answer Contriever\n\nAnswer BM25 Table 5: Case study of retrieved documents from the question \"A 'smack' is a collective noun for a group of which sea creatures?\". \"Exists\" means whether the URL points to a valid Wiki page. \"Answer\" means whether the document contains the answer. We omit the prefix of generated URLs for brevity (https://en.wikipedia.org/). For BM25 and Contriever, we show the document titles of the top-10 retrieved passages, respectively. The correct answer is \"jellyfish.\"\n\ntity question-answer pairs. The results of our analysis are shown in Figure 5. Across all three datasets, the recall of common-entity question-answer pairs is many times greater than the recall from the rest of the dataset. Previous work has shown LLMs in the closedbook setting, where the model must rely solely on the information contained within its weights, perform much better on common-entities versus uncommon ones (Mallen et al., 2022). Our results show this problem extends beyond the closed-book setting and also applies to retrieval when using LLM-URL. This also could explain the high word count from documents we found when evaluating LLM-URL. The average Wikipedia article is 644 words, but the average word count from Wikipedia documents retrieved via LLM-URL was 10k. We believe this discrepancy is caused by common entities having much more detail in their Wikipedia articles and in turn having much higher word count.\n\n\nCase Study\n\nIn Table 5, we show a case study comparing LLM-URL with two baseline retrieval methods, BM25 and Contriever, on the question \"A 'smack' is a collective noun for a group of which sea creatures?\" which is in the TriviaQA test set. The gold-labeled answer to this question is \"jellyfish\".\n\nIn the closed-book setting, InstructGPT mistakenly predicts \"dolphins\" as the answer. When using Contriever to retrieve 10 passages from Wikipedia given the query, none of the passages contains the gold answer. For instance, Contriever retrieves passages about \"smack\", a kind of fishing vessel, along with other passages about sperm whales, plankton, and other unrelated topics. Similar results are found while using BM25 as the retriever.\n\nIn contrast, LLM-URL performs much better in this scenario, retrieving 7 documents which contain the answer. The top retrieved document is exactly about the gold answer \"jellyfish\". The fourth to the tenth documents all talk about different types of jellyfish. After being chunked into passages then sorted by the ranker, the top 10 passages are concatenated. Among them, it contains \"A group of jellyfish is called a smack,\" which contains the answer to the question and comes directly from the first retrieved document, titled \"jellyfish.\" When In-structGPT is then prompted with these 10 passages along with the question, the gold answer \"jellyfish\" is correctly generated.\n\nThis case study highlights multiple advantages of LLM-URL . First, LLM-URL finds documents related to both the question and the answer. It directly locates documents that talks about \"jellyfish\" instead while BM25 and Contriever locate documents related to the question only-not the answer. Second, LLM-URL is more precise than BM25 or Contriever. In this case, 7 out of 10 generated URLs from LLM-URL point to a Wikipedia document that contains the answer. However, both BM25 and Contriever fail to retrieve any documents containing the answer. Third, the set of documents retrieved by LLM-URL are complementary to each other, while in BM25 or contriever, each document in the top-10 is selected independently. This is because the LLM is able to refer to previous generated URLs before it generates the next one, allowing each newly generated URL to be conditioned on all the previous URLs. This leads to a more informative evidence context in open-domain question answering.\n\n\nConclusion and Future Work\n\nIn this paper, we explored whether large language models can generate URLs prompted by human instructions for document retrieval. Surprisingly, we found that by providing a few (query, URL) pairs as in-context demonstrations, large language models (e.g. GPT-3) generated Web URLs where near 90% of the corresponding documents contain correct answers to open-domain questions in WebQ. Furthermore, by breaking the retrieved documents into passages then ranking them with BM25, we showed a significant number of unnecessary passages could be filtered out while retaining high recall, which outperformed baseline methods by a significant margin.\n\nThere are numerous exciting directions for future work. While a number of broad spectrum retrieval benchmarks such as BIER (Thakur et al., 2021) exist, it remains to be seen whether the few-shot demonstrations shown in this work can be further tuned for specific retrieval tasks. Promptagator (Dai et al., 2022) shows significant performance improvements can be achieved by tuning prompts in a similar way.\n\nFurther, it remains to be seen whether fine tuning the prompt for each individual question can further improve the retrieval performance. As with Promptagator, prior work has shown using clustering to select diverse demonstrations for any given question further improves retrieval performance as well as downstream QA performance.\n\n\nLimitations\n\nDespite the strong performance on the presented datasets, our approach is limited in its ability to update knowledge state and adapt to new domains. A major feature of retrieve-then-read is the ability to swap in new documents when new information is learned, such as temporally more recent documents, or adding in documents from a new domain to quickly adapt to a new downstream task. Our approach relies on a large language model to contain all this knowledge and adding new knowledge would likely require some retraining. In addition, large generation models still suffer from hallucina-tion errors, resulting in incorrect predictions. When tasked with generating 10 URLs, LLM-URL may only generate 6 or 7 which link to valid documents. Finally, our approach involves very large language models, slow web requests, and document processing which may make it cumbersome to use in practice.\n\nFigure 1 :\n1Successful URL reconstructions by different size of GPT-3 as URL generators. The models are prompted with the first 100 words of a Wikipedia page then tasked with generating the URL of the page they came from. Tested on 10k Wikipedia pages sampled from the top 100k most frequent Wikipedia entities.\n\nFigure 2 :\n2The overall pipeline of our proposed LLM-URL. Given a question, LLM-URL first generates a set of URLs which are extracted from the generated text. The URLs are retrieved from the Internet then broken into passages which are ranked and filtered such that only the most relevant are kept. Finally, these passages are given as input to a reader model along with the original question to generate a final answer.\n\nFigure 3 :\n3We prompt LLM-URL to generate m documents and measure the recall as m increases. Significant recall improvements are seen when m is small but as it increases the marginal benefit decreases.\n\nFigure 4 :\n4The percentage of valid URLs generated from LLM-URL as the total number of generated URLs m increases from 1 to 10. As m increases, invalid URL generations become more frequent.\n\nFigure 5 :\n5Common vs Uncommon entity recall@1. Common is defined as a question containing entities in the top-1 million most common entities on Wikipedia. LLM-URL performs much better when retrieving information about common entities.\n\nTable 2 :\n2Passage retrieval as measured by Recall@1, Recall@10 and Recall@100. Here LLM-URL is equipped with BM25 to perform the ranking task.\n\nTable 3 :\n3Passage retrieval as measured by Recall@1 and Recall@10. LLM-URL is equipped with BM25 for passage ranking. Other datasets are left out due to not being reported in either paper and no public implementations.\n\nTable 4 :\n4Zero-shot open-domain QA performance as measured by exact match (EM). All LLM-URL models use InstructGPT as the reader unless otherwise stated.\nAcknowledgementsThis work was supported by NSF IIS-2119531, IIS-2137396, IIS-2142827, CCF-1901059, and ONR N00014-22-1-2507. Wenhao Yu was partly supported by the Bloomberg Data Science Fellowship.\nSemantic parsing on freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, EMNLP. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In EMNLP, pages 1533- 1544.\n\nAutoregressive search engines: Generating substrings as document identifiers. Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-Tau Yih, Sebastian Riedel, Fabio Petroni, arXiv:2204.10628arXiv preprintMichele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive search engines: Gen- erating substrings as document identifiers. arXiv preprint arXiv:2204.10628.\n\nThe anatomy of a large-scale hypertextual web search engine. Sergey Brin, Lawrence Page, Proceedings of the Seventh International Conference on World Wide Web 7, WWW7. the Seventh International Conference on World Wide Web 7, WWW7Elsevier Science Publishers B. VNLDSergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. In Proceedings of the Seventh International Confer- ence on World Wide Web 7, WWW7, page 107-117, NLD. Elsevier Science Publishers B. V.\n\nReading wikipedia to answer opendomain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, Procs. of ACL. s. of ACLDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer open- domain questions. In Procs. of ACL.\n\nZhuyun Dai, Y Vincent, Ji Zhao, Yi Ma, Jianmo Luan, Jing Ni, Anton Lu, Kelvin Bakalov, Keith B Guu, Ming-Wei Hall, Chang, arXiv:2209.117552022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprintZhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755.\n\nAutoregressive entity retrieval. Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni, International Conference on Learning Representations. Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval. In International Conference on Learning Represen- tations.\n\n. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, 10.48550/ARXIV.2112.09118and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learningGautier Izacard, Mathilde Caron, Lucas Hosseini, Se- bastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense in- formation retrieval with contrastive learning.\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, S Daniel, Luke Weld, Zettlemoyer, ACL. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In ACL, pages 1601-1611.\n\nGrape: Knowledge graph enhanced passage reader for open-domain question answering. Mingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang, Yanfang Ye, arXiv:2210.02933arXiv preprintMingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang, and Yanfang Ye. 2022. Grape: Knowledge graph enhanced passage reader for open-domain question answering. arXiv preprint arXiv:2210.02933.\n\nDense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingEMNLPVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natu- ral Language Processing (EMNLP).\n\nRelevance-guided supervision for openqa with colbert. Omar Khattab, Christopher Potts, Matei Zaharia, Transactions of the Association for Computational Linguistics. 9Omar Khattab, Christopher Potts, and Matei Zaharia. 2021. Relevance-guided supervision for openqa with colbert. Transactions of the Association for Computational Linguistics, 9:929-944.\n\nColbert: Efficient and effective passage search via contextualized late interaction over bert. Omar Khattab, Matei Zaharia, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. the 43rd International ACM SIGIR conference on research and development in Information RetrievalOmar Khattab and Matei Zaharia. 2020. Colbert: Ef- ficient and effective passage search via contextual- ized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39-48.\n\nNatural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, TACL. et al. 2019.Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: A bench- mark for question answering research. TACL, pages 452-466.\n\nWhen not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, Daniel Khashabi, ArXiv:2212.10511arXiv preprintAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: In- vestigating effectiveness and limitations of paramet- ric and non-parametric memories. arXiv preprint ArXiv:2212.10511.\n\nReader-guided passage reranking for opendomain question answering. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen, Findings of ACL-IJCNLP. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2021. Reader-guided passage reranking for open- domain question answering. In Findings of ACL- IJCNLP.\n\nCREAK: A dataset for commonsense reasoning over entity knowledge. Yasumasa Onoe, J Q Michael, Eunsol Zhang, Greg Choi, Durrett, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1. the Neural Information Processing Systems Track on Datasets and Benchmarks 1NeurIPSYasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg Durrett. 2021. CREAK: A dataset for com- monsense reasoning over entity knowledge. In Pro- ceedings of the Neural Information Processing Sys- tems Track on Datasets and Benchmarks 1, NeurIPS.\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, L Carroll, Pamela Wainwright, Chong Mishkin, Sandhini Zhang, Katarina Agarwal, Alex Slama, Ray, arXiv:2203.02155Training language models to follow instructions with human feedback. arXiv preprintLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car- roll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow in- structions with human feedback. arXiv preprint arXiv:2203.02155.\n\nKilt: a benchmark for knowledge intensive language tasks. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2021. Kilt: a benchmark for knowl- edge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 2523-2544.\n\nRocketqa: An optimized training approach to dense passage retrieval for opendomain question answering. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, Haifeng Wang, Procs. of NAACL. s. of NAACLYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. Rocketqa: An optimized train- ing approach to dense passage retrieval for open- domain question answering. In Procs. of NAACL.\n\nThe probabilistic relevance framework: BM25 and beyond. E Stephen, Hugo Robertson, Zaragoza, Foundations and Trends\u00ae in Information Retrieval. 34Stephen E. Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and be- yond. Foundations and Trends\u00ae in Information Re- trieval, 3(4):333-389.\n\nTransformer memory as a differentiable search index. Yi Tay, Q Vinh, Mostafa Tran, Jianmo Dehghani, Dara Ni, Harsh Bahri, Zhen Mehta, Kai Qin, Zhe Hui, Jai Zhao, Gupta, arXiv:2202.06991arXiv preprintYi Tay, Vinh Q Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. 2022. Transformer mem- ory as a differentiable search index. arXiv preprint arXiv:2202.06991.\n\nBEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Abhishek Srivastava, and Iryna Gurevych. Round 2Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab- hishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogeneous benchmark for zero-shot evalua- tion of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Sys- tems Datasets and Benchmarks Track (Round 2).\n\n2022. A neural corpus indexer for document retrieval. Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Sun, Weiwei Deng, Qi Zhang, Mao Yang, Advances in Neural Information Processing Systems. Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A neural corpus indexer for document retrieval. In Advances in Neural Information Pro- cessing Systems.\n\nApproximate nearest neighbor negative contrastive learning for dense text retrieval. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, N Paul, Junaid Bennett, Arnold Ahmed, Overwijk, International Conference on Learning Representations. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neigh- bor negative contrastive learning for dense text re- trieval. In International Conference on Learning Representations.\n\nEnd-to-end open-domain question answering with bertserini. Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, Jimmy Lin, NAACL 2019 (demo). Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-end open-domain question answering with bertserini. In NAACL 2019 (demo).\n\nKg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering. Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, Michael Zeng, arXiv:2110.04330arXiv preprintDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim- ing Yang, and Michael Zeng. 2021. Kg-fid: In- fusing knowledge graph in fusion-in-decoder for open-domain question answering. arXiv preprint arXiv:2110.04330.\n\nGenerate rather than retrieve: Large language models are strong context generators. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang, International Conference for Learning Representation. ICLRWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are strong context generators. International Conference for Learning Representation (ICLR).\n\n2022. A survey of knowledge-enhanced text generation. Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Ji Heng, Meng Jiang, ACM Computing Surveys (CSUR). Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang. 2022. A survey of knowledge-enhanced text generation. ACM Computing Surveys (CSUR).\n\nKnowledge-aware procedural text understanding with multi-stage training. Zhihan Zhang, Xiubo Geng, Tao Qin, Yunfang Wu, Daxin Jiang, WWW '21: The Web Conference. Zhihan Zhang, Xiubo Geng, Tao Qin, Yunfang Wu, and Daxin Jiang. 2021. Knowledge-aware procedu- ral text understanding with multi-stage training. In WWW '21: The Web Conference 2021.\n\nExploring contrast consistency of open-domain question answering systems on minimally edited questions. Zhihan Zhang, Wenhao Yu, Zheng Ning, Mingxuan Ju, Meng Jiang, Trans. Assoc. Comput. Linguistics. Zhihan Zhang, Wenhao Yu, Zheng Ning, Mingxuan Ju, and Meng Jiang. 2023a. Exploring contrast consis- tency of open-domain question answering systems on minimally edited questions. Trans. Assoc. Com- put. Linguistics.\n\nA survey of multi-task learning in natural language processing: Regarding task relatedness and training methods. Zhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, Meng Jiang, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023. the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023Zhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, and Meng Jiang. 2023b. A survey of multi-task learning in natural language processing: Regarding task relatedness and training methods. In Proceed- ings of the 17th Conference of the European Chap- ter of the Association for Computational Linguistics, EACL 2023.\n\nA unified encoder-decoder framework with entity memory. Zhihan Zhang, Wenhao Yu, Chenguang Zhu, Meng Jiang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022Zhihan Zhang, Wenhao Yu, Chenguang Zhu, and Meng Jiang. 2022. A unified encoder-decoder framework with entity memory. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan- guage Processing, EMNLP 2022.\n\nDense text retrieval based on pretrained language models: A survey. Jing Wayne Xin Zhao, Ruiyang Liu, Ji-Rong Ren, Wen, arXiv:2211.14876arXiv preprintWayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji- Rong Wen. 2022. Dense text retrieval based on pre- trained language models: A survey. arXiv preprint arXiv:2211.14876.\n\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, arXiv:2101.00774Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering. arXiv preprintFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774.\n", "annotations": {"author": "[{\"end\":121,\"start\":68},{\"end\":159,\"start\":122},{\"end\":216,\"start\":160},{\"end\":270,\"start\":217}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":73},{\"end\":131,\"start\":129},{\"end\":172,\"start\":167},{\"end\":227,\"start\":222}]", "author_first_name": "[{\"end\":72,\"start\":68},{\"end\":128,\"start\":122},{\"end\":166,\"start\":160},{\"end\":221,\"start\":217}]", "author_affiliation": "[{\"end\":120,\"start\":95},{\"end\":158,\"start\":133},{\"end\":215,\"start\":190},{\"end\":269,\"start\":244}]", "title": "[{\"end\":65,\"start\":1},{\"end\":335,\"start\":271}]", "venue": null, "abstract": "[{\"end\":1679,\"start\":337}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1833,\"start\":1815},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1876,\"start\":1852},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2280,\"start\":2256},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2356,\"start\":2334},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2587,\"start\":2560},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2928,\"start\":2910},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2952,\"start\":2928},{\"end\":2970,\"start\":2952},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3435,\"start\":3417},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3744,\"start\":3726},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4218,\"start\":4199},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4354,\"start\":4332},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5297,\"start\":5276},{\"end\":5328,\"start\":5302},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5363,\"start\":5343},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6480,\"start\":6450},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6498,\"start\":6480},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6516,\"start\":6498},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6546,\"start\":6522},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6880,\"start\":6860},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6896,\"start\":6880},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6916,\"start\":6896},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6957,\"start\":6939},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6973,\"start\":6957},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6989,\"start\":6973},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7150,\"start\":7126},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7169,\"start\":7150},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7245,\"start\":7223},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7491,\"start\":7470},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7561,\"start\":7543},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7647,\"start\":7622},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8431,\"start\":8410},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8447,\"start\":8431},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11948,\"start\":11924},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12359,\"start\":12341},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12383,\"start\":12359},{\"end\":15362,\"start\":15340},{\"end\":15401,\"start\":15367},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15876,\"start\":15855},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16317,\"start\":16299},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16436,\"start\":16411},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18794,\"start\":18776},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18830,\"start\":18805},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22961,\"start\":22936},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23182,\"start\":23161},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24727,\"start\":24706},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28436,\"start\":28415},{\"end\":28603,\"start\":28585}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30249,\"start\":29937},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30671,\"start\":30250},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30874,\"start\":30672},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31065,\"start\":30875},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31302,\"start\":31066},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31447,\"start\":31303},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31668,\"start\":31448},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31824,\"start\":31669}]", "paragraph": "[{\"end\":2588,\"start\":1695},{\"end\":4468,\"start\":2590},{\"end\":5600,\"start\":4470},{\"end\":5774,\"start\":5602},{\"end\":6127,\"start\":5776},{\"end\":6250,\"start\":6129},{\"end\":6266,\"start\":6252},{\"end\":7246,\"start\":6302},{\"end\":8449,\"start\":7280},{\"end\":9345,\"start\":8469},{\"end\":10453,\"start\":9347},{\"end\":12115,\"start\":10455},{\"end\":13684,\"start\":12117},{\"end\":13949,\"start\":13700},{\"end\":14322,\"start\":13951},{\"end\":14522,\"start\":14336},{\"end\":15295,\"start\":14536},{\"end\":16639,\"start\":15318},{\"end\":16880,\"start\":16641},{\"end\":17885,\"start\":16893},{\"end\":18274,\"start\":17887},{\"end\":18697,\"start\":18276},{\"end\":18937,\"start\":18719},{\"end\":19598,\"start\":18950},{\"end\":20045,\"start\":19600},{\"end\":20233,\"start\":20047},{\"end\":20773,\"start\":20235},{\"end\":21008,\"start\":20808},{\"end\":21960,\"start\":21010},{\"end\":22218,\"start\":21962},{\"end\":22379,\"start\":22220},{\"end\":22782,\"start\":22420},{\"end\":23120,\"start\":22784},{\"end\":23770,\"start\":23151},{\"end\":24282,\"start\":23809},{\"end\":25219,\"start\":24284},{\"end\":25519,\"start\":25234},{\"end\":25961,\"start\":25521},{\"end\":26639,\"start\":25963},{\"end\":27617,\"start\":26641},{\"end\":28290,\"start\":27648},{\"end\":28698,\"start\":28292},{\"end\":29030,\"start\":28700},{\"end\":29936,\"start\":29046}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16599,\"start\":16592},{\"end\":16963,\"start\":16956},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19019,\"start\":19012},{\"end\":19156,\"start\":19149},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20354,\"start\":20347},{\"end\":23828,\"start\":23821},{\"end\":25244,\"start\":25237}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1693,\"start\":1681},{\"attributes\":{\"n\":\"2.1\"},\"end\":6300,\"start\":6269},{\"attributes\":{\"n\":\"2.2\"},\"end\":7278,\"start\":7249},{\"attributes\":{\"n\":\"3\"},\"end\":8467,\"start\":8452},{\"attributes\":{\"n\":\"4\"},\"end\":13698,\"start\":13687},{\"end\":14334,\"start\":14325},{\"attributes\":{\"n\":\"4.1\"},\"end\":14534,\"start\":14525},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":15316,\"start\":15298},{\"end\":16891,\"start\":16883},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":18717,\"start\":18700},{\"end\":18948,\"start\":18940},{\"attributes\":{\"n\":\"4.2\"},\"end\":20806,\"start\":20776},{\"attributes\":{\"n\":\"4.3\"},\"end\":22393,\"start\":22382},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":22418,\"start\":22396},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":23149,\"start\":23123},{\"end\":23780,\"start\":23773},{\"end\":23807,\"start\":23783},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":25232,\"start\":25222},{\"attributes\":{\"n\":\"5\"},\"end\":27646,\"start\":27620},{\"end\":29044,\"start\":29033},{\"end\":29948,\"start\":29938},{\"end\":30261,\"start\":30251},{\"end\":30683,\"start\":30673},{\"end\":30886,\"start\":30876},{\"end\":31077,\"start\":31067},{\"end\":31313,\"start\":31304},{\"end\":31458,\"start\":31449},{\"end\":31679,\"start\":31670}]", "table": null, "figure_caption": "[{\"end\":30249,\"start\":29950},{\"end\":30671,\"start\":30263},{\"end\":30874,\"start\":30685},{\"end\":31065,\"start\":30888},{\"end\":31302,\"start\":31079},{\"end\":31447,\"start\":31315},{\"end\":31668,\"start\":31460},{\"end\":31824,\"start\":31681}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3898,\"start\":3890},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9344,\"start\":9336},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13991,\"start\":13983},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17699,\"start\":17691},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18430,\"start\":18422},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24361,\"start\":24353}]", "bib_author_first_name": "[{\"end\":32088,\"start\":32080},{\"end\":32103,\"start\":32097},{\"end\":32113,\"start\":32110},{\"end\":32128,\"start\":32123},{\"end\":32380,\"start\":32373},{\"end\":32401,\"start\":32393},{\"end\":32420,\"start\":32413},{\"end\":32435,\"start\":32428},{\"end\":32450,\"start\":32441},{\"end\":32464,\"start\":32459},{\"end\":32796,\"start\":32790},{\"end\":32811,\"start\":32803},{\"end\":33285,\"start\":33280},{\"end\":33296,\"start\":33292},{\"end\":33309,\"start\":33304},{\"end\":33325,\"start\":33318},{\"end\":33499,\"start\":33493},{\"end\":33506,\"start\":33505},{\"end\":33518,\"start\":33516},{\"end\":33527,\"start\":33525},{\"end\":33538,\"start\":33532},{\"end\":33549,\"start\":33545},{\"end\":33559,\"start\":33554},{\"end\":33570,\"start\":33564},{\"end\":33585,\"start\":33580},{\"end\":33587,\"start\":33586},{\"end\":33601,\"start\":33593},{\"end\":33969,\"start\":33963},{\"end\":33985,\"start\":33978},{\"end\":34004,\"start\":33995},{\"end\":34018,\"start\":34013},{\"end\":34259,\"start\":34252},{\"end\":34277,\"start\":34269},{\"end\":34290,\"start\":34285},{\"end\":34310,\"start\":34301},{\"end\":34324,\"start\":34319},{\"end\":34343,\"start\":34337},{\"end\":34762,\"start\":34756},{\"end\":34776,\"start\":34770},{\"end\":34784,\"start\":34783},{\"end\":34797,\"start\":34793},{\"end\":35101,\"start\":35093},{\"end\":35112,\"start\":35106},{\"end\":35121,\"start\":35117},{\"end\":35133,\"start\":35128},{\"end\":35148,\"start\":35141},{\"end\":35438,\"start\":35430},{\"end\":35456,\"start\":35450},{\"end\":35468,\"start\":35463},{\"end\":35481,\"start\":35474},{\"end\":35495,\"start\":35489},{\"end\":35506,\"start\":35500},{\"end\":35520,\"start\":35515},{\"end\":35534,\"start\":35527},{\"end\":36044,\"start\":36040},{\"end\":36065,\"start\":36054},{\"end\":36078,\"start\":36073},{\"end\":36438,\"start\":36434},{\"end\":36453,\"start\":36448},{\"end\":37006,\"start\":37003},{\"end\":37030,\"start\":37020},{\"end\":37047,\"start\":37041},{\"end\":37065,\"start\":37058},{\"end\":37080,\"start\":37075},{\"end\":37094,\"start\":37089},{\"end\":37112,\"start\":37104},{\"end\":37127,\"start\":37122},{\"end\":37145,\"start\":37140},{\"end\":37564,\"start\":37560},{\"end\":37578,\"start\":37573},{\"end\":37591,\"start\":37585},{\"end\":37607,\"start\":37599},{\"end\":37621,\"start\":37613},{\"end\":37640,\"start\":37634},{\"end\":38015,\"start\":38009},{\"end\":38030,\"start\":38021},{\"end\":38043,\"start\":38035},{\"end\":38055,\"start\":38049},{\"end\":38070,\"start\":38062},{\"end\":38082,\"start\":38076},{\"end\":38094,\"start\":38088},{\"end\":38399,\"start\":38391},{\"end\":38407,\"start\":38406},{\"end\":38409,\"start\":38408},{\"end\":38425,\"start\":38419},{\"end\":38437,\"start\":38433},{\"end\":38884,\"start\":38880},{\"end\":38897,\"start\":38893},{\"end\":38904,\"start\":38902},{\"end\":38917,\"start\":38912},{\"end\":38928,\"start\":38927},{\"end\":38944,\"start\":38938},{\"end\":38962,\"start\":38957},{\"end\":38980,\"start\":38972},{\"end\":38996,\"start\":38988},{\"end\":39010,\"start\":39006},{\"end\":39447,\"start\":39442},{\"end\":39467,\"start\":39457},{\"end\":39482,\"start\":39476},{\"end\":39495,\"start\":39488},{\"end\":39508,\"start\":39503},{\"end\":39524,\"start\":39518},{\"end\":39527,\"start\":39525},{\"end\":39538,\"start\":39533},{\"end\":39553,\"start\":39547},{\"end\":39571,\"start\":39563},{\"end\":39587,\"start\":39583},{\"end\":40374,\"start\":40368},{\"end\":40385,\"start\":40379},{\"end\":40396,\"start\":40392},{\"end\":40405,\"start\":40402},{\"end\":40418,\"start\":40411},{\"end\":40429,\"start\":40424},{\"end\":40433,\"start\":40430},{\"end\":40447,\"start\":40440},{\"end\":40457,\"start\":40454},{\"end\":40469,\"start\":40462},{\"end\":40807,\"start\":40806},{\"end\":40821,\"start\":40817},{\"end\":41121,\"start\":41119},{\"end\":41128,\"start\":41127},{\"end\":41142,\"start\":41135},{\"end\":41155,\"start\":41149},{\"end\":41170,\"start\":41166},{\"end\":41180,\"start\":41175},{\"end\":41192,\"start\":41188},{\"end\":41203,\"start\":41200},{\"end\":41212,\"start\":41209},{\"end\":41221,\"start\":41218},{\"end\":41578,\"start\":41572},{\"end\":41591,\"start\":41587},{\"end\":41608,\"start\":41601},{\"end\":42119,\"start\":42113},{\"end\":42133,\"start\":42126},{\"end\":42145,\"start\":42139},{\"end\":42158,\"start\":42152},{\"end\":42171,\"start\":42165},{\"end\":42179,\"start\":42176},{\"end\":42187,\"start\":42185},{\"end\":42200,\"start\":42194},{\"end\":42214,\"start\":42206},{\"end\":42228,\"start\":42220},{\"end\":42240,\"start\":42235},{\"end\":42250,\"start\":42246},{\"end\":42259,\"start\":42256},{\"end\":42271,\"start\":42265},{\"end\":42280,\"start\":42278},{\"end\":42291,\"start\":42288},{\"end\":42737,\"start\":42734},{\"end\":42752,\"start\":42745},{\"end\":42762,\"start\":42760},{\"end\":42776,\"start\":42767},{\"end\":42789,\"start\":42783},{\"end\":42796,\"start\":42795},{\"end\":42809,\"start\":42803},{\"end\":42825,\"start\":42819},{\"end\":43224,\"start\":43221},{\"end\":43237,\"start\":43231},{\"end\":43249,\"start\":43243},{\"end\":43261,\"start\":43255},{\"end\":43272,\"start\":43266},{\"end\":43281,\"start\":43278},{\"end\":43293,\"start\":43289},{\"end\":43303,\"start\":43298},{\"end\":43605,\"start\":43598},{\"end\":43619,\"start\":43610},{\"end\":43630,\"start\":43625},{\"end\":43643,\"start\":43637},{\"end\":43656,\"start\":43648},{\"end\":43670,\"start\":43663},{\"end\":43680,\"start\":43675},{\"end\":43692,\"start\":43686},{\"end\":43706,\"start\":43699},{\"end\":44086,\"start\":44080},{\"end\":44094,\"start\":44091},{\"end\":44109,\"start\":44101},{\"end\":44123,\"start\":44116},{\"end\":44136,\"start\":44128},{\"end\":44147,\"start\":44141},{\"end\":44165,\"start\":44156},{\"end\":44178,\"start\":44171},{\"end\":44189,\"start\":44185},{\"end\":44588,\"start\":44582},{\"end\":44602,\"start\":44593},{\"end\":44615,\"start\":44608},{\"end\":44627,\"start\":44620},{\"end\":44639,\"start\":44632},{\"end\":44648,\"start\":44646},{\"end\":44659,\"start\":44655},{\"end\":44950,\"start\":44944},{\"end\":44963,\"start\":44958},{\"end\":44973,\"start\":44970},{\"end\":44986,\"start\":44979},{\"end\":44996,\"start\":44991},{\"end\":45326,\"start\":45320},{\"end\":45340,\"start\":45334},{\"end\":45350,\"start\":45345},{\"end\":45365,\"start\":45357},{\"end\":45374,\"start\":45370},{\"end\":45753,\"start\":45747},{\"end\":45767,\"start\":45761},{\"end\":45779,\"start\":45772},{\"end\":45791,\"start\":45784},{\"end\":45801,\"start\":45797},{\"end\":46408,\"start\":46402},{\"end\":46422,\"start\":46416},{\"end\":46436,\"start\":46427},{\"end\":46446,\"start\":46442},{\"end\":46913,\"start\":46909},{\"end\":46937,\"start\":46930},{\"end\":46950,\"start\":46943},{\"end\":47165,\"start\":47158},{\"end\":47179,\"start\":47171},{\"end\":47189,\"start\":47185},{\"end\":47204,\"start\":47196}]", "bib_author_last_name": "[{\"end\":32095,\"start\":32089},{\"end\":32108,\"start\":32104},{\"end\":32121,\"start\":32114},{\"end\":32134,\"start\":32129},{\"end\":32391,\"start\":32381},{\"end\":32411,\"start\":32402},{\"end\":32426,\"start\":32421},{\"end\":32439,\"start\":32436},{\"end\":32457,\"start\":32451},{\"end\":32472,\"start\":32465},{\"end\":32801,\"start\":32797},{\"end\":32816,\"start\":32812},{\"end\":33290,\"start\":33286},{\"end\":33302,\"start\":33297},{\"end\":33316,\"start\":33310},{\"end\":33332,\"start\":33326},{\"end\":33503,\"start\":33500},{\"end\":33514,\"start\":33507},{\"end\":33523,\"start\":33519},{\"end\":33530,\"start\":33528},{\"end\":33543,\"start\":33539},{\"end\":33552,\"start\":33550},{\"end\":33562,\"start\":33560},{\"end\":33578,\"start\":33571},{\"end\":33591,\"start\":33588},{\"end\":33606,\"start\":33602},{\"end\":33613,\"start\":33608},{\"end\":33976,\"start\":33970},{\"end\":33993,\"start\":33986},{\"end\":34011,\"start\":34005},{\"end\":34026,\"start\":34019},{\"end\":34267,\"start\":34260},{\"end\":34283,\"start\":34278},{\"end\":34299,\"start\":34291},{\"end\":34317,\"start\":34311},{\"end\":34335,\"start\":34325},{\"end\":34350,\"start\":34344},{\"end\":34768,\"start\":34763},{\"end\":34781,\"start\":34777},{\"end\":34791,\"start\":34785},{\"end\":34802,\"start\":34798},{\"end\":34815,\"start\":34804},{\"end\":35104,\"start\":35102},{\"end\":35115,\"start\":35113},{\"end\":35126,\"start\":35122},{\"end\":35139,\"start\":35134},{\"end\":35151,\"start\":35149},{\"end\":35448,\"start\":35439},{\"end\":35461,\"start\":35457},{\"end\":35472,\"start\":35469},{\"end\":35487,\"start\":35482},{\"end\":35498,\"start\":35496},{\"end\":35513,\"start\":35507},{\"end\":35525,\"start\":35521},{\"end\":35538,\"start\":35535},{\"end\":36052,\"start\":36045},{\"end\":36071,\"start\":36066},{\"end\":36086,\"start\":36079},{\"end\":36446,\"start\":36439},{\"end\":36461,\"start\":36454},{\"end\":37018,\"start\":37007},{\"end\":37039,\"start\":37031},{\"end\":37056,\"start\":37048},{\"end\":37073,\"start\":37066},{\"end\":37087,\"start\":37081},{\"end\":37102,\"start\":37095},{\"end\":37120,\"start\":37113},{\"end\":37138,\"start\":37128},{\"end\":37152,\"start\":37146},{\"end\":37571,\"start\":37565},{\"end\":37583,\"start\":37579},{\"end\":37597,\"start\":37592},{\"end\":37611,\"start\":37608},{\"end\":37632,\"start\":37622},{\"end\":37649,\"start\":37641},{\"end\":38019,\"start\":38016},{\"end\":38033,\"start\":38031},{\"end\":38047,\"start\":38044},{\"end\":38060,\"start\":38056},{\"end\":38074,\"start\":38071},{\"end\":38086,\"start\":38083},{\"end\":38099,\"start\":38095},{\"end\":38404,\"start\":38400},{\"end\":38417,\"start\":38410},{\"end\":38431,\"start\":38426},{\"end\":38442,\"start\":38438},{\"end\":38451,\"start\":38444},{\"end\":38891,\"start\":38885},{\"end\":38900,\"start\":38898},{\"end\":38910,\"start\":38905},{\"end\":38925,\"start\":38918},{\"end\":38936,\"start\":38929},{\"end\":38955,\"start\":38945},{\"end\":38970,\"start\":38963},{\"end\":38986,\"start\":38981},{\"end\":39004,\"start\":38997},{\"end\":39016,\"start\":39011},{\"end\":39021,\"start\":39018},{\"end\":39455,\"start\":39448},{\"end\":39474,\"start\":39468},{\"end\":39486,\"start\":39483},{\"end\":39501,\"start\":39496},{\"end\":39516,\"start\":39509},{\"end\":39531,\"start\":39528},{\"end\":39545,\"start\":39539},{\"end\":39561,\"start\":39554},{\"end\":39581,\"start\":39572},{\"end\":39596,\"start\":39588},{\"end\":40377,\"start\":40375},{\"end\":40390,\"start\":40386},{\"end\":40400,\"start\":40397},{\"end\":40409,\"start\":40406},{\"end\":40422,\"start\":40419},{\"end\":40438,\"start\":40434},{\"end\":40452,\"start\":40448},{\"end\":40460,\"start\":40458},{\"end\":40474,\"start\":40470},{\"end\":40815,\"start\":40808},{\"end\":40831,\"start\":40822},{\"end\":40841,\"start\":40833},{\"end\":41125,\"start\":41122},{\"end\":41133,\"start\":41129},{\"end\":41147,\"start\":41143},{\"end\":41164,\"start\":41156},{\"end\":41173,\"start\":41171},{\"end\":41186,\"start\":41181},{\"end\":41198,\"start\":41193},{\"end\":41207,\"start\":41204},{\"end\":41216,\"start\":41213},{\"end\":41226,\"start\":41222},{\"end\":41233,\"start\":41228},{\"end\":41585,\"start\":41579},{\"end\":41599,\"start\":41592},{\"end\":41615,\"start\":41609},{\"end\":42124,\"start\":42120},{\"end\":42137,\"start\":42134},{\"end\":42150,\"start\":42146},{\"end\":42163,\"start\":42159},{\"end\":42174,\"start\":42172},{\"end\":42183,\"start\":42180},{\"end\":42192,\"start\":42188},{\"end\":42204,\"start\":42201},{\"end\":42218,\"start\":42215},{\"end\":42233,\"start\":42229},{\"end\":42244,\"start\":42241},{\"end\":42254,\"start\":42251},{\"end\":42263,\"start\":42260},{\"end\":42276,\"start\":42272},{\"end\":42286,\"start\":42281},{\"end\":42296,\"start\":42292},{\"end\":42743,\"start\":42738},{\"end\":42758,\"start\":42753},{\"end\":42765,\"start\":42763},{\"end\":42781,\"start\":42777},{\"end\":42793,\"start\":42790},{\"end\":42801,\"start\":42797},{\"end\":42817,\"start\":42810},{\"end\":42831,\"start\":42826},{\"end\":42841,\"start\":42833},{\"end\":43229,\"start\":43225},{\"end\":43241,\"start\":43238},{\"end\":43253,\"start\":43250},{\"end\":43264,\"start\":43262},{\"end\":43276,\"start\":43273},{\"end\":43287,\"start\":43282},{\"end\":43296,\"start\":43294},{\"end\":43307,\"start\":43304},{\"end\":43608,\"start\":43606},{\"end\":43623,\"start\":43620},{\"end\":43635,\"start\":43631},{\"end\":43646,\"start\":43644},{\"end\":43661,\"start\":43657},{\"end\":43673,\"start\":43671},{\"end\":43684,\"start\":43681},{\"end\":43697,\"start\":43693},{\"end\":43711,\"start\":43707},{\"end\":44089,\"start\":44087},{\"end\":44099,\"start\":44095},{\"end\":44114,\"start\":44110},{\"end\":44126,\"start\":44124},{\"end\":44139,\"start\":44137},{\"end\":44154,\"start\":44148},{\"end\":44169,\"start\":44166},{\"end\":44183,\"start\":44179},{\"end\":44195,\"start\":44190},{\"end\":44591,\"start\":44589},{\"end\":44606,\"start\":44603},{\"end\":44618,\"start\":44616},{\"end\":44630,\"start\":44628},{\"end\":44644,\"start\":44640},{\"end\":44653,\"start\":44649},{\"end\":44665,\"start\":44660},{\"end\":44956,\"start\":44951},{\"end\":44968,\"start\":44964},{\"end\":44977,\"start\":44974},{\"end\":44989,\"start\":44987},{\"end\":45002,\"start\":44997},{\"end\":45332,\"start\":45327},{\"end\":45343,\"start\":45341},{\"end\":45355,\"start\":45351},{\"end\":45368,\"start\":45366},{\"end\":45380,\"start\":45375},{\"end\":45759,\"start\":45754},{\"end\":45770,\"start\":45768},{\"end\":45782,\"start\":45780},{\"end\":45795,\"start\":45792},{\"end\":45807,\"start\":45802},{\"end\":46414,\"start\":46409},{\"end\":46425,\"start\":46423},{\"end\":46440,\"start\":46437},{\"end\":46452,\"start\":46447},{\"end\":46928,\"start\":46914},{\"end\":46941,\"start\":46938},{\"end\":46954,\"start\":46951},{\"end\":46959,\"start\":46956},{\"end\":47169,\"start\":47166},{\"end\":47183,\"start\":47180},{\"end\":47194,\"start\":47190},{\"end\":47210,\"start\":47205}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6401679},\"end\":32293,\"start\":32023},{\"attributes\":{\"doi\":\"arXiv:2204.10628\",\"id\":\"b1\"},\"end\":32727,\"start\":32295},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7587743},\"end\":33228,\"start\":32729},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3618568},\"end\":33491,\"start\":33230},{\"attributes\":{\"doi\":\"arXiv:2209.11755\",\"id\":\"b4\"},\"end\":33928,\"start\":33493},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":222125277},\"end\":34248,\"start\":33930},{\"attributes\":{\"doi\":\"10.48550/ARXIV.2112.09118\",\"id\":\"b6\"},\"end\":34664,\"start\":34250},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":26501419},\"end\":35008,\"start\":34666},{\"attributes\":{\"doi\":\"arXiv:2210.02933\",\"id\":\"b8\"},\"end\":35368,\"start\":35010},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":215737187},\"end\":35984,\"start\":35370},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220302658},\"end\":36337,\"start\":35986},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":216553223},\"end\":36937,\"start\":36339},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":86611921},\"end\":37436,\"start\":36939},{\"attributes\":{\"doi\":\"ArXiv:2212.10511\",\"id\":\"b13\"},\"end\":37940,\"start\":37438},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":230435683},\"end\":38323,\"start\":37942},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":237417284},\"end\":38878,\"start\":38325},{\"attributes\":{\"doi\":\"arXiv:2203.02155\",\"id\":\"b16\"},\"end\":39382,\"start\":38880},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":221507798},\"end\":40263,\"start\":39384},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":231815627},\"end\":40748,\"start\":40265},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":207178704},\"end\":41064,\"start\":40750},{\"attributes\":{\"doi\":\"arXiv:2202.06991\",\"id\":\"b20\"},\"end\":41480,\"start\":41066},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":233296016},\"end\":42057,\"start\":41482},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":249395549},\"end\":42647,\"start\":42059},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":220302524},\"end\":43160,\"start\":42649},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":59604492},\"end\":43506,\"start\":43162},{\"attributes\":{\"doi\":\"arXiv:2110.04330\",\"id\":\"b25\"},\"end\":43994,\"start\":43508},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":252408513},\"end\":44526,\"start\":43996},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":222272210},\"end\":44869,\"start\":44528},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":221969993},\"end\":45214,\"start\":44871},{\"attributes\":{\"id\":\"b29\"},\"end\":45632,\"start\":45216},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":248005954},\"end\":46344,\"start\":45634},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":252762485},\"end\":46839,\"start\":46346},{\"attributes\":{\"doi\":\"arXiv:2211.14876\",\"id\":\"b32\"},\"end\":47156,\"start\":46841},{\"attributes\":{\"doi\":\"arXiv:2101.00774\",\"id\":\"b33\"},\"end\":47574,\"start\":47158}]", "bib_title": "[{\"end\":32078,\"start\":32023},{\"end\":32788,\"start\":32729},{\"end\":33278,\"start\":33230},{\"end\":33961,\"start\":33930},{\"end\":34754,\"start\":34666},{\"end\":35428,\"start\":35370},{\"end\":36038,\"start\":35986},{\"end\":36432,\"start\":36339},{\"end\":37001,\"start\":36939},{\"end\":38007,\"start\":37942},{\"end\":38389,\"start\":38325},{\"end\":39440,\"start\":39384},{\"end\":40366,\"start\":40265},{\"end\":40804,\"start\":40750},{\"end\":41570,\"start\":41482},{\"end\":42111,\"start\":42059},{\"end\":42732,\"start\":42649},{\"end\":43219,\"start\":43162},{\"end\":44078,\"start\":43996},{\"end\":44580,\"start\":44528},{\"end\":44942,\"start\":44871},{\"end\":45318,\"start\":45216},{\"end\":45745,\"start\":45634},{\"end\":46400,\"start\":46346}]", "bib_author": "[{\"end\":32097,\"start\":32080},{\"end\":32110,\"start\":32097},{\"end\":32123,\"start\":32110},{\"end\":32136,\"start\":32123},{\"end\":32393,\"start\":32373},{\"end\":32413,\"start\":32393},{\"end\":32428,\"start\":32413},{\"end\":32441,\"start\":32428},{\"end\":32459,\"start\":32441},{\"end\":32474,\"start\":32459},{\"end\":32803,\"start\":32790},{\"end\":32818,\"start\":32803},{\"end\":33292,\"start\":33280},{\"end\":33304,\"start\":33292},{\"end\":33318,\"start\":33304},{\"end\":33334,\"start\":33318},{\"end\":33505,\"start\":33493},{\"end\":33516,\"start\":33505},{\"end\":33525,\"start\":33516},{\"end\":33532,\"start\":33525},{\"end\":33545,\"start\":33532},{\"end\":33554,\"start\":33545},{\"end\":33564,\"start\":33554},{\"end\":33580,\"start\":33564},{\"end\":33593,\"start\":33580},{\"end\":33608,\"start\":33593},{\"end\":33615,\"start\":33608},{\"end\":33978,\"start\":33963},{\"end\":33995,\"start\":33978},{\"end\":34013,\"start\":33995},{\"end\":34028,\"start\":34013},{\"end\":34269,\"start\":34252},{\"end\":34285,\"start\":34269},{\"end\":34301,\"start\":34285},{\"end\":34319,\"start\":34301},{\"end\":34337,\"start\":34319},{\"end\":34352,\"start\":34337},{\"end\":34770,\"start\":34756},{\"end\":34783,\"start\":34770},{\"end\":34793,\"start\":34783},{\"end\":34804,\"start\":34793},{\"end\":34817,\"start\":34804},{\"end\":35106,\"start\":35093},{\"end\":35117,\"start\":35106},{\"end\":35128,\"start\":35117},{\"end\":35141,\"start\":35128},{\"end\":35153,\"start\":35141},{\"end\":35450,\"start\":35430},{\"end\":35463,\"start\":35450},{\"end\":35474,\"start\":35463},{\"end\":35489,\"start\":35474},{\"end\":35500,\"start\":35489},{\"end\":35515,\"start\":35500},{\"end\":35527,\"start\":35515},{\"end\":35540,\"start\":35527},{\"end\":36054,\"start\":36040},{\"end\":36073,\"start\":36054},{\"end\":36088,\"start\":36073},{\"end\":36448,\"start\":36434},{\"end\":36463,\"start\":36448},{\"end\":37020,\"start\":37003},{\"end\":37041,\"start\":37020},{\"end\":37058,\"start\":37041},{\"end\":37075,\"start\":37058},{\"end\":37089,\"start\":37075},{\"end\":37104,\"start\":37089},{\"end\":37122,\"start\":37104},{\"end\":37140,\"start\":37122},{\"end\":37154,\"start\":37140},{\"end\":37573,\"start\":37560},{\"end\":37585,\"start\":37573},{\"end\":37599,\"start\":37585},{\"end\":37613,\"start\":37599},{\"end\":37634,\"start\":37613},{\"end\":37651,\"start\":37634},{\"end\":38021,\"start\":38009},{\"end\":38035,\"start\":38021},{\"end\":38049,\"start\":38035},{\"end\":38062,\"start\":38049},{\"end\":38076,\"start\":38062},{\"end\":38088,\"start\":38076},{\"end\":38101,\"start\":38088},{\"end\":38406,\"start\":38391},{\"end\":38419,\"start\":38406},{\"end\":38433,\"start\":38419},{\"end\":38444,\"start\":38433},{\"end\":38453,\"start\":38444},{\"end\":38893,\"start\":38880},{\"end\":38902,\"start\":38893},{\"end\":38912,\"start\":38902},{\"end\":38927,\"start\":38912},{\"end\":38938,\"start\":38927},{\"end\":38957,\"start\":38938},{\"end\":38972,\"start\":38957},{\"end\":38988,\"start\":38972},{\"end\":39006,\"start\":38988},{\"end\":39018,\"start\":39006},{\"end\":39023,\"start\":39018},{\"end\":39457,\"start\":39442},{\"end\":39476,\"start\":39457},{\"end\":39488,\"start\":39476},{\"end\":39503,\"start\":39488},{\"end\":39518,\"start\":39503},{\"end\":39533,\"start\":39518},{\"end\":39547,\"start\":39533},{\"end\":39563,\"start\":39547},{\"end\":39583,\"start\":39563},{\"end\":39598,\"start\":39583},{\"end\":40379,\"start\":40368},{\"end\":40392,\"start\":40379},{\"end\":40402,\"start\":40392},{\"end\":40411,\"start\":40402},{\"end\":40424,\"start\":40411},{\"end\":40440,\"start\":40424},{\"end\":40454,\"start\":40440},{\"end\":40462,\"start\":40454},{\"end\":40476,\"start\":40462},{\"end\":40817,\"start\":40806},{\"end\":40833,\"start\":40817},{\"end\":40843,\"start\":40833},{\"end\":41127,\"start\":41119},{\"end\":41135,\"start\":41127},{\"end\":41149,\"start\":41135},{\"end\":41166,\"start\":41149},{\"end\":41175,\"start\":41166},{\"end\":41188,\"start\":41175},{\"end\":41200,\"start\":41188},{\"end\":41209,\"start\":41200},{\"end\":41218,\"start\":41209},{\"end\":41228,\"start\":41218},{\"end\":41235,\"start\":41228},{\"end\":41587,\"start\":41572},{\"end\":41601,\"start\":41587},{\"end\":41617,\"start\":41601},{\"end\":42126,\"start\":42113},{\"end\":42139,\"start\":42126},{\"end\":42152,\"start\":42139},{\"end\":42165,\"start\":42152},{\"end\":42176,\"start\":42165},{\"end\":42185,\"start\":42176},{\"end\":42194,\"start\":42185},{\"end\":42206,\"start\":42194},{\"end\":42220,\"start\":42206},{\"end\":42235,\"start\":42220},{\"end\":42246,\"start\":42235},{\"end\":42256,\"start\":42246},{\"end\":42265,\"start\":42256},{\"end\":42278,\"start\":42265},{\"end\":42288,\"start\":42278},{\"end\":42298,\"start\":42288},{\"end\":42745,\"start\":42734},{\"end\":42760,\"start\":42745},{\"end\":42767,\"start\":42760},{\"end\":42783,\"start\":42767},{\"end\":42795,\"start\":42783},{\"end\":42803,\"start\":42795},{\"end\":42819,\"start\":42803},{\"end\":42833,\"start\":42819},{\"end\":42843,\"start\":42833},{\"end\":43231,\"start\":43221},{\"end\":43243,\"start\":43231},{\"end\":43255,\"start\":43243},{\"end\":43266,\"start\":43255},{\"end\":43278,\"start\":43266},{\"end\":43289,\"start\":43278},{\"end\":43298,\"start\":43289},{\"end\":43309,\"start\":43298},{\"end\":43610,\"start\":43598},{\"end\":43625,\"start\":43610},{\"end\":43637,\"start\":43625},{\"end\":43648,\"start\":43637},{\"end\":43663,\"start\":43648},{\"end\":43675,\"start\":43663},{\"end\":43686,\"start\":43675},{\"end\":43699,\"start\":43686},{\"end\":43713,\"start\":43699},{\"end\":44091,\"start\":44080},{\"end\":44101,\"start\":44091},{\"end\":44116,\"start\":44101},{\"end\":44128,\"start\":44116},{\"end\":44141,\"start\":44128},{\"end\":44156,\"start\":44141},{\"end\":44171,\"start\":44156},{\"end\":44185,\"start\":44171},{\"end\":44197,\"start\":44185},{\"end\":44593,\"start\":44582},{\"end\":44608,\"start\":44593},{\"end\":44620,\"start\":44608},{\"end\":44632,\"start\":44620},{\"end\":44646,\"start\":44632},{\"end\":44655,\"start\":44646},{\"end\":44667,\"start\":44655},{\"end\":44958,\"start\":44944},{\"end\":44970,\"start\":44958},{\"end\":44979,\"start\":44970},{\"end\":44991,\"start\":44979},{\"end\":45004,\"start\":44991},{\"end\":45334,\"start\":45320},{\"end\":45345,\"start\":45334},{\"end\":45357,\"start\":45345},{\"end\":45370,\"start\":45357},{\"end\":45382,\"start\":45370},{\"end\":45761,\"start\":45747},{\"end\":45772,\"start\":45761},{\"end\":45784,\"start\":45772},{\"end\":45797,\"start\":45784},{\"end\":45809,\"start\":45797},{\"end\":46416,\"start\":46402},{\"end\":46427,\"start\":46416},{\"end\":46442,\"start\":46427},{\"end\":46454,\"start\":46442},{\"end\":46930,\"start\":46909},{\"end\":46943,\"start\":46930},{\"end\":46956,\"start\":46943},{\"end\":46961,\"start\":46956},{\"end\":47171,\"start\":47158},{\"end\":47185,\"start\":47171},{\"end\":47196,\"start\":47185},{\"end\":47212,\"start\":47196}]", "bib_venue": "[{\"end\":32959,\"start\":32897},{\"end\":33358,\"start\":33349},{\"end\":35699,\"start\":35628},{\"end\":36672,\"start\":36576},{\"end\":38629,\"start\":38546},{\"end\":39869,\"start\":39742},{\"end\":40504,\"start\":40493},{\"end\":46032,\"start\":45929},{\"end\":46613,\"start\":46542},{\"end\":32141,\"start\":32136},{\"end\":32371,\"start\":32295},{\"end\":32895,\"start\":32818},{\"end\":33347,\"start\":33334},{\"end\":33691,\"start\":33631},{\"end\":34080,\"start\":34028},{\"end\":34820,\"start\":34817},{\"end\":35091,\"start\":35010},{\"end\":35626,\"start\":35540},{\"end\":36149,\"start\":36088},{\"end\":36574,\"start\":36463},{\"end\":37158,\"start\":37154},{\"end\":37558,\"start\":37438},{\"end\":38123,\"start\":38101},{\"end\":38544,\"start\":38453},{\"end\":39106,\"start\":39039},{\"end\":39740,\"start\":39598},{\"end\":40491,\"start\":40476},{\"end\":40891,\"start\":40843},{\"end\":41117,\"start\":41066},{\"end\":41711,\"start\":41617},{\"end\":42347,\"start\":42298},{\"end\":42895,\"start\":42843},{\"end\":43326,\"start\":43309},{\"end\":43596,\"start\":43508},{\"end\":44249,\"start\":44197},{\"end\":44695,\"start\":44667},{\"end\":45031,\"start\":45004},{\"end\":45415,\"start\":45382},{\"end\":45927,\"start\":45809},{\"end\":46540,\"start\":46454},{\"end\":46907,\"start\":46841},{\"end\":47349,\"start\":47228}]"}}}, "year": 2023, "month": 12, "day": 17}