{
    "id": 57955,
    "updated": "2023-10-01 16:49:02.244",
    "metadata": {
        "title": "OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings",
        "authors": "[{\"first\":\"Jelani\",\"last\":\"Nelson\",\"middle\":[]},{\"first\":\"Huy\",\"last\":\"Nguyen\",\"middle\":[\"L.\"]}]",
        "venue": "ArXiv",
        "journal": "ArXiv",
        "publication_date": {
            "year": 2012,
            "month": 11,
            "day": 5
        },
        "abstract": "An\"oblivious subspace embedding (OSE)\"given some parameters eps,d is a distribution D over matrices B in R^{m x n} such that for any linear subspace W in R^n with dim(W) = d it holds that Pr_{B ~ D}(forall x in W ||B x||_2 in (1 +/- eps)||x||_2)>2/3 We show an OSE exists with m = O(d^2/eps^2) and where every B in the support of D has exactly s=1 non-zero entries per column. This improves previously best known bound in [Clarkson-Woodruff, arXiv:1207.6365]. Our quadratic dependence on d is optimal for any OSE with s=1 [Nelson-Nguyen, 2012]. We also give two OSE's, which we call Oblivious Sparse Norm-Approximating Projections (OSNAPs), that both allow the parameter settings m = \\~O(d/eps^2) and s = polylog(d)/eps, or m = O(d^{1+gamma}/eps^2) and s=O(1/eps) for any constant gamma>0. This m is nearly optimal since m>= d is required simply to no non-zero vector of W lands in the kernel of B. These are the first constructions with m=o(d^2) to have s=o(d). In fact, our OSNAPs are nothing more than the sparse Johnson-Lindenstrauss matrices of [Kane-Nelson, SODA 2012]. Our analyses all yield OSE's that are sampled using either O(1)-wise or O(log d)-wise independent hash functions, which provides some efficiency advantages over previous work for turnstile streaming applications. Our main result is essentially a Bai-Yin type theorem in random matrix theory and is likely to be of independent interest: i.e. we show that for any U in R^{n x d} with orthonormal columns and random sparse B, all singular values of BU lie in [1-eps, 1+eps] with good probability. Plugging OSNAPs into known algorithms for numerical linear algebra problems such as approximate least squares regression, low rank approximation, and approximating leverage scores implies faster algorithms for all these problems.",
        "fields_of_study": "[\"Computer Science\",\"Mathematics\"]",
        "external_ids": {
            "arxiv": "1211.1002",
            "mag": "2949809202",
            "acl": null,
            "pubmed": null,
            "pubmedcentral": null,
            "dblp": "conf/focs/NelsonN13",
            "doi": "10.1109/focs.2013.21"
        }
    },
    "content": {
        "source": {
            "pdf_hash": "73963c53dcfb4586bc86f328709b004192195d49",
            "pdf_src": "Arxiv",
            "pdf_uri": "[\"https://arxiv.org/pdf/1211.1002v1.pdf\"]",
            "oa_url_match": true,
            "oa_info": {
                "license": null,
                "open_access_url": "http://arxiv.org/pdf/1211.1002",
                "status": "GREEN"
            }
        },
        "grobid": {
            "id": "7cd89f8b49da38089e3bc1556f153d82c3d09df9",
            "type": "plain-text",
            "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/73963c53dcfb4586bc86f328709b004192195d49.txt",
            "contents": "\nOSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings\n5 Nov 2012 May 5, 2014\n\nJelani Nelson \nHuy L Nguy\u1ebdn \nOSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings\n5 Nov 2012 May 5, 2014\nAn oblivious subspace embedding (OSE) given some parameters \u03b5, d is a distribution D over matrices \u03a0 \u2208 R m\u00d7n such that for any linear subspace W \u2286 R n with dim(W ) = d it holds thatWe show an OSE exists with m = O(d 2 /\u03b5 2 ) and where every \u03a0 in the support of D has exactly s = 1 non-zero entries per column. This improves previously best known bound in [Clarkson-Woodruff, arXiv abs/1207.6365]. Our quadratic dependence on d is optimal for any OSE with s = 1[Nelson-Nguy\u1ebdn, 2012]. We also give two OSE's, which we call Oblivious Sparse Norm-Approximating Projections (OSNAPs), that both allow the parameter settings m =\u00d5(d/\u03b5 2 ) and s = polylog(d)/\u03b5, or m = O(d 1+\u03b3 /\u03b5 2 ) and s = O(1/\u03b5) for any constant \u03b3 > 0. 1 This m is nearly optimal since m \u2265 d is required simply to no non-zero vector of W lands in the kernel of \u03a0. These are the first constructions with m = o(d 2 ) to have s = o(d). In fact, our OSNAPs are nothing more than the sparse Johnson-Lindenstrauss matrices of [Kane-Nelson, SODA 2012]. Our analyses all yield OSE's that are sampled using either O(1)-wise or O(log d)wise independent hash functions, which provides some efficiency advantages over previous work for turnstile streaming applications. Our main result is essentially a Bai-Yin type theorem in random matrix theory and is likely to be of independent interest: i.e. we show that for any U \u2208 R n\u00d7d with orthonormal columns and random sparse \u03a0, all singular values of \u03a0U lie in [1 \u2212 \u03b5, 1 + \u03b5] with good probability.Plugging OSNAPs into known algorithms for numerical linear algebra problems such as approximate least squares regression, low rank approximation, and approximating leverage scores implies faster algorithms for all these problems. For example, for the approximate least squares regression problem of computing x that minimizes Ax \u2212 b 2 up to a constant factor, our embeddings imply a running time of\u00d5(nnz(A) + r \u03c9 ), which is essentially the best bound one could hope for (up to logarithmic factors). Here r = rank(A), nnz(\u00b7) counts non-zero entries, and \u03c9 is the exponent of matrix multiplication. Previous algorithms had a worse dependence on r. * Institute for Advanced Study. minilek@ias.edu.\n\nIntroduction\n\nThere has been much recent work on applications of dimensionality reduction to handling large datasets. Typically special features of the data such as low \"intrinsic\" dimensionality, or sparsity, are exploited to reduce the volume of data before processing, thus speeding up analysis time. One success story of this approach is the applications of fast algorithms for the Johnson-Lindenstrauss lemma [JL84], which allows one to reduce the dimension of a set of vectors while preserving all pairwise distances. There have been two popular lines of work in this area: one focusing on fast embeddings for all vectors [AC09, AL09, AL11, HV11, KMR12, KW11,Vyb11], and one focusing on fast embeddings specifically for sparse vectors [Ach03,BOR10,DKS10,KN10,KN12].\n\nIn this work we focus on the problem of constructing an oblivious subspace embedding (OSE) [Sar06] and on applications of these embeddings. Roughly speaking, the problem is to design a data-independent distribution over linear mappings such that when data come from an unknown low-dimensional subspace, they are reduced to roughly their true dimension while their structure (all distances in the subspace in this case) is preserved at the same time. It can be seen as a continuation of the approach based on the Johnson-Lindenstrauss lemma to subspaces. Here we focus on the setting of sparse inputs, where it is important that the algorithms take time proportional to the input sparsity. These embeddings have found applications in numerical linear algebra problems such as least squares regression, low rank approximation, and approximating leverage scores [CW09,CW12,DMIMW12,NDT09,Sar06,Tro11]. We refer the interested reader to the surveys [HMT11,Mah11] for an overview of this area.\n\nThroughout this document we use \u00b7 to denote \u2113 2 norm in the case of vector arguments, and \u2113 2\u21922 operator norm in the case of matrix arguments. Recall the definition of the OSE problem.\n\nDefinition 1. The oblivious subspace embedding problem is to design a distribution over m \u00d7 n matrices \u03a0 such that for any d-dimensional subspace W \u2282 R n , with probability at least 2/3 over the choice of \u03a0 \u223c D, the following inequalities hold for all x \u2208 W simultaneously:\n\n(1 \u2212 \u03b5) x \u2264 \u03a0x \u2264 (1 + \u03b5) x .\n\nHere n, d, \u03b5, \u03b4 are given parameters of the problem and we would like m as small as possible.\n\nOSE's were first introduced in [Sar06] as a means to obtain fast randomized algorithms for several numerical linear algebra problems. To see the connection, consider for example the least squares regression problem of computing argmin x\u2208R d Ax \u2212 b for some A \u2208 R n\u00d7d . Suppose \u03a0 \u2208 R m\u00d7n preserves the \u2113 2 norm up to 1 + \u03b5 of all vectors in the subspace spanned by b and the columns of A. Then computing argmin x \u03a0Ax \u2212 \u03a0b instead gives a solution that is within 1 + \u03b5 of optimal. Since the subspace being preserved has dimension at most r + 1 \u2264 d + 1, where r = rank(A), one only needs m = f (r + 1, \u03b5) for whatever function f is achievable in some OSE construction. Thus the running time for approximate n \u00d7 d regression becomes that for f (r, \u03b5) \u00d7 d regression, plus an additive term for the time required to compute \u03a0A, \u03a0b. Even if A has full column rank and r = d this is still a gain for instances with n \u226b d. Also note that the 2/3 success probability guaranteed by Definition 1 can be amplified to 1 \u2212 \u03b4 by running this procedure O(log(1/\u03b4)) times with independent randomness and taking the best x found in any run.\n\nNaively there is no gain from the above approach since the time to compute \u03a0A could be as large as matrix multiplication between an m \u00d7 n and n \u00d7 d matrix. Since m \u2265 d in any OSE, this is O(nd \u03c9\u22121 ) time where \u03c9 < 2.373 . . . [Wil12] is the exponent of square matrix multiplication, and exact least squares regression can already be computed in this time bound. The work of [Sar06] overcame this barrier by choosing \u03a0 to be a special structured matrix, with the property that \u03a0A can be computed in time O(nd log n) (see also [Tro11]). This matrix \u03a0 was the Fast Johnson-Lindenstrauss Transform of [AC09], which has the property that \u03a0x can be computed in roughly O(n log n) time for any x \u2208 R n . Thus, multiplying \u03a0A by iterating over columns of A gives the desired speedup.\n\nThe O(nd log n) running time of the above scheme to compute \u03a0A seems almost linear, and thus nearly optimal, since the input size is already nd to describe A. While this is true for dense A, in many practical instances one often expects the input matrix A to be sparse, in which case linear time in the input description actually means O(nnz(A)), where nnz(\u00b7) is the number of non-zero entries. For example consider the case of A being the Netflix matrix, where A i,j is user i's score for movie j: A is very sparse since most users do not watch, let alone score, most movies [ZWSP08].\n\nIn a recent beautiful and surprising work, [CW12] showed that there exist OSE's with m = poly(d/\u03b5), and where every matrix \u03a0 in the support of the distribution is very sparse: even with only s = 1 non-zero entries per column! Thus one can transform, for example, an n \u00d7 d least squares regression problem into a poly(d/\u03b5) \u00d7 d regression problem in nnz(A) time. They gave two sparse OSE constructions: one with m =\u00d5(d 4 /\u03b5 4 ), s = 1, and another with m =\u00d5(d 2 /\u03b5 4 ), s = O((log d)/\u03b5). 2 The second construction is advantageous when d is larger as a function of n and one is willing to slightly worsen the nnz(A) term in the running time for a gain in the input size of the final regression problem.\n\nWe also remark that the analyses given of both constructions in [CW12] require \u2126(d)-wise independent hash functions, so that from the O(d)-wise independent seed used to generate \u03a0 naively one needs an additive \u2126(d) time to identify the non-zero entries in each column just to evaluate the hash function. In streaming applications this can be improved to additive\u00d5(log 2 d) time using fast multipoint evaluation of polynomials (see [KNPW11,Remark 16]), though ideally if s = 1 one could hope for a construction that allows one to find, for any column, the non-zero entry in that column in constant time given only a short seed that specifies \u03a0 (i.e. without writing down \u03a0 explicitly in memory, which could be prohibitively expensive for n large in applications such as streaming and out-of-core numerical linear algebra). Recall that in the entry-wise turnstile streaming model, A receives entry-wise updates of the form ((i, j), v), which cause the change A i,j \u2190 A i,j + v. Updating the embedding thus amounts to adding v times the jth row of \u03a0 to \u03a0A, which should ideally take O(s) time and not O(s) +\u00d5(log 2 d).\n\nIn the following paragraph we let S \u03a0 be the space required to store \u03a0 implicitly (e.g. store the seed to some hash function that specifies \u03a0). We let t c be the running time required by an algorithm which, given a column index and the length-S \u03a0 seed specifying \u03a0, returns the list of all non-zeroes in that column in \u03a0.\n\nOur Main Contribution: We give an improved analysis of the s = 1 OSE in [CW12] and show that it actually achieves m = O(d 2 /\u03b5 2 ), s = 1. Our analysis is near-optimal since m = \u2126(d 2 ) is required for any OSE with s = 1 [NN12]. Furthermore, for this construction we show t c = O(1), S \u03a0 = O(log(nd)). We also show that the two sparse Johnson-Lindenstrauss constructions of [KN12] both reference regression leverage scores low rank approximation [CW12] O(nnz(A)) +\u00d5(d 5 )\nO(nnz(A)) +\u00d5(nk 5 ) O(nnz(A) log n) +\u00d5(r 3 )\u00d5(nnz(A) + r 3 ) O(nnz(A) log k) +\u00d5(nk 2 ) this work O(nnz(A) + d 3 log d) O(nnz(A)) +\u00d5(nk 2 ) O(nnz(A) + r \u03c9 )\u00d5(nnz(A) + r \u03c9 ) O(nnz(A) log O(1) k) +\u00d5(nk \u03c9\u22121 )\nO(nnz(A)) +\u00d5(nk \u03c9\u22121+\u03b3 ) Figure 1: The improvement gained in running times by using our OSE's. Dependence on \u03b5 suppressed for readability; see Section 3 for dependence.\n\nyield OSE's that allow for the parameter\nsettings m =\u00d5(d/\u03b5 2 ), s = polylog(d)/\u03b5, t c =\u00d5(s), S \u03a0 = O(log d log(nd)) or m = O(d 1+\u03b3 /\u03b5 2 ), s = O \u03b3 (1/\u03b5), t c = O((log d)/\u03b5), S \u03a0 = O(log d log(nd)\n) for any desired constant \u03b3 > 0. This m is nearly optimal since m \u2265 d is required simply to ensure that no non-zero vector in the subspace lands in the kernel of \u03a0. Plugging our improved OSE's into previous work implies faster algorithms for several numerical linear algebra problems, such as approximate least squares regression, low rank approximation, and approximating leverage scores. We remark that both of the OSE's in this work and [CW12] with s \u226b 1 have the added benefit of preserving any subspace with 1/ poly(d), and not just constant, failure probability.\n\n\nProblem Statements and Bounds\n\nWe now formally define all numerical linear algebra problems we consider. Plugging our new OSE's into previous algorithms for the above problems yields the bounds in Figure 1; the value r used in bounds denotes rank(A).\n\nApproximating Leverage Scores: A d-dimensional subspace W \u2286 R n can be written as W = {x : \u2203y \u2208 R d , x = U y} for some U \u2208 R n\u00d7d with orthonormal columns. The squared Euclidean norms of rows of U are unique up to permutation, i.e. they depend only on A, and are known as the leverage scores of A. Given A, we would like to output a list of its leverage scores up to 1 \u00b1 \u03b5.\n\n\nLeast Squares Regression: Given\nA \u2208 R n\u00d7d , b \u2208 R n , computex \u2208 R d so that Ax \u2212 b \u2264 (1 + \u03b5) \u00b7 min x\u2208R d Ax \u2212 b .\nLow Rank Approximation: Given A \u2208 R n\u00d7d and integer k > 0, compute\u00c3 k \u2208 R n\u00d7d with rank\n(\u00c3) \u2264 k so that A \u2212\u00c3 k F \u2264 (1 + \u03b5) \u00b7 min rank(A k )\u2264k A \u2212 A k F , where \u00b7 F is Frobenius norm.\n\nOur Construction and Techniques\n\nThe s = 1 construction is simply the TZ sketch [TZ12]. This matrix \u03a0 is specified by a random hash function h : [d] \u2192 [n] and a random \u03c3 \u2208 {\u22121, 1} d . For each i \u2208 [d] we set \u03a0 h(i),i = \u03c3 i , and every other entry in \u03a0 is set to zero. Observe any d-dimensional subspace W \u2286 R n can be written as W = {x : \u2203y \u2208 R d , x = U y} for some U \u2208 R n\u00d7d with orthonormal columns. The analysis of the s = 1 construction in [CW12] worked roughly as follows: let I \u2282 [n] denote the set of \"heavy\" rows, i.e. those rows u i of U where u i is \"large\". We write x = x I + x [n]\\I , where x S for a set S denotes x with all coordinates in [n]\\S zeroed out. Then x 2 = x I 2 + x [n]\\I 2 + 2 x I , x [n]\\I . The argument in [CW12] conditioned on I being perfectly hashed by h so that x I 2 is preserved exactly.\n\nUsing an approach in [KN10,KN12] based on the Hanson-Wright inequality [HW71] together with a net argument, it was argued that x [n]\\I 2 is preserved simultaneously for all x \u2208 W ; this step required \u2126(d)-wise independence to union bound over the net. A simpler concentration argument was used to handle the x I , x [n]\\I term. The construction in [CW12] with smaller m and larger s followed a similar but more complicated analysis; that construction involving hashing into buckets and using the sparse Johnson-Lindenstrauss matrices of [KN12] in each bucket.\n\nOur analysis is completely different. First, just as in the TZ sketch's application to \u2113 2 estimation in data streams, we only require h to be pairwise independent and \u03c3 to be 4-wise independent. Our observation is simple: a matrix \u03a0 preserving the Euclidean norm of all vectors x \u2208 W up to 1 \u00b1 \u03b5 is equivalent to the statement \u03a0U y = (1 \u00b1 \u03b5) y simultaneously for all y \u2208 R d . This is equivalent to all singular values of \u03a0U lying in the interval [1 \u2212 \u03b5, 1 + \u03b5]. 3 Write S = (\u03a0U ) * \u03a0U , so that we want to show all eigenvalues values of S lie in [(1 \u2212 \u03b5) 2 , (1 + \u03b5) 2 ]. We can trivially write S = I + (S \u2212 I), and thus by Weyl's inequality (see a statement in Section 2) all eigenvalues of S are 1 \u00b1 S \u2212 I . We thus show that S \u2212 I is small with good probability. By Markov's inequality\nP( S \u2212 I \u2265 t) = P( S \u2212 I 2 \u2265 t 2 ) \u2264 t \u22122 \u00b7 E S \u2212 I 2 \u2264 t \u22122 \u00b7 E S \u2212 I 2 F .\nBounding this latter quantity is a simple calculation and fits in under a page (Theorem 3). The two constructions with smaller m \u2248 d/\u03b5 2 are the sparse Johnson-Lindenstrauss matrices of [KN12]. In particular, the only properties we need from our OSE in our analyses are the following. Let each matrix in the support of the OSE have entries in {0, 1/ \u221a s, \u22121/ \u221a s}. For a randomly drawn \u03a0, let \u03b4 i,j be an indicator random variable for the event \u03a0 i,j = 0, and write\n\u03a0 i,j = \u03b4 i,j \u03c3 i,j / \u221a s,\nwhere the \u03c3 i,j are random signs. Then the properties we need are\n\u2022 For any j \u2208 [n], m i=1 \u03b4 i,j = s with probability 1. \u2022 For any S \u2286 [m] \u00d7 [n], E (i,j)\u2208S \u03b4 i,j \u2264 (s/m) |S| .\nThe second property says the \u03b4 i,j are negatively correlated. We call any matrix drawn from an OSE with the above properties an oblivious sparse norm-approximating projection (OSNAP). The work of [KN12] gave two OSNAP distributions, either of which suffice for our current OSE problem. In the first construction, each column is chosen to have exactly s non-zero entries in random locations, each equal to \u00b11/ \u221a s uniformly at random. For our purposes the signs \u03c3 i,j need only be O(log d)-wise independent, and each column can be specified by a O(log d)-wise independent permutation, and the seeds specifying the permutations in different columns need only be O(log d)-wise independent. In the second construction we pick hash functions h :\n[d] \u00d7 [s] \u2192 [m/s], \u03c3 : [d] \u00d7 [s] \u2192 {\u22121, 1}, both O(log d)-wise independent, and thus each representable using O(log d log nd) random bits. For each (i, j) \u2208 [d] \u00d7 [s] we set \u03a0 (j\u22121)s+h(i,j),i = \u03c3(i, j)/ \u221a s, and all\nother entries in \u03a0 are set to zero. Note also that the TZ sketch is itself an OSNAP with s = 1. Just as in the TZ sketch, it suffices to show some tail bound: that P( S \u2212 I > \u03b5 \u2032 ) is small for some \u03b5 \u2032 = O(\u03b5), where S = (\u03a0U ) * \u03a0U . Note that if the eigenvalues of S \u2212 I are \u03bb 1 , . . . , \u03bb d , then the eigenvalues of (S \u2212 I) \u2113 are \u03bb \u2113 1 , . . . , \u03bb \u2113 d . Thus for \u2113 even, tr((S \u2212 I) \u2113 ) = d i=1 \u03bb \u2113 i is an upper bound on S \u2212 I \u2113 . Thus by Markov's inequality with \u2113 even,\nP( S \u2212 I \u2265 t) = P( S \u2212 I \u2113 \u2265 t \u2113 ) \u2264 t \u2212\u2113 \u00b7 E S \u2212 I \u2113 \u2264 t \u2212\u2113 \u00b7 Etr((S \u2212 I) \u2113 ).\n(1)\n\nOur proof works by expanding the expression tr((S \u2212 I) \u2113 ) and computing its expectation. This expression is a sum of exponentially many monomials, each involving a product of \u2113 terms. Without delving into technical details at this point, each such monomial can be thought of as being in correspondence with some undirected multigraph (see the dot product multigraphs in the proof of Theorem 9). We group monomials corresponding to the same graph, bound the contribution from each graph separately, then sum over all graphs. Multigraphs whose edges all have even multiplicity turn out to be easier to handle (Lemma 10). However most graphs G do not have this property. Informally speaking, the contribution of a graph turns out to be related to the product over its edges of the contribution of that edge. Let us informally call this \"contribution\" F (G).\nThus if E \u2032 \u2282 E is a subset of the edges of G, we can write F (G) \u2264 F ((G| E \u2032 ) 2 )/2 + F ((G| E\\E \u2032 ) 2 )/2 by AM-GM,\nwhere squaring a multigraph means duplicating every edge, and G| E \u2032 is G with all edges in E\\E \u2032 removed. This reduces back to the case of even edge multiplicities, but unfortunately the bound we desire on F (G) depends exponentially on the number of connected components of G. Thus this step is bad, since if G is connected, then one of G| E \u2032 , G| E \u2032 \\E can have many connected components for any choice of E \u2032 . For example if G is a cycle on N vertices, for E \u2032 a single edge almost every vertex in G E \u2032 is in its own connected component, and even if E \u2032 is every odd-indexed edge then the number of components blows up to N/2. Our method to overcome this is to show that any F (G) is bounded by some F (G \u2032 ) with the property that every connected component of G \u2032 has two edge-disjoint spanning trees. We then put one such spanning tree into E \u2032 for each component, so that G| E\\E \u2032 and G| E \u2032 both have the same number of connected components as G.\n\nOur approach follows the classical moment method in random matrix theory; see [Tao12, Section 2] or [Ver12] introductions to this area. In particular, our approach is inspired by one taken by Bai and Yin [BY93], who in our notation were concerned with the case n = d, U = I, \u03a0 dense. Most of the complications in our proof arise because U is not the identity matrix, so that rows of U are not orthogonal. For example, in the case of U having orthogonal rows all graphs G in the last paragraph have no edges other than self-loops and are trivial to analyze.\n\n\nAnalysis\n\nIn this section let the orthonormal columns of U \u2208 R n\u00d7d be denoted u 1 , . . . , u d . Recall our goal is to show that all singular values of \u03a0U lie in the interval [1 \u2212 \u03b5, 1 + \u03b5] with probability 1 \u2212 \u03b4 over the choice of \u03a0 as long as s, m are sufficiently large. We assume \u03a0 is an OSNAP with sparsity s. As in [BY93] we make use of Weyl's inequality (see a proof in [Tao12, Section 1.3]).\n\nTheorem 2 (Weyl's inequality). Let M, H, P be n\u00d7n Hermitian matrices where M has eigenvalues \u00b5 1 \u2265 . . . \u2265 \u00b5 n , H has eigenvalues \u03bd 1 \u2265 . . . \u2265 \u03bd n , and P has eigenvalues \u03c1 1 \u2265 . . . \u2265 \u03c1 n . Then\n\u2200 1 \u2264 i \u2264 n, it holds that \u03bd i + \u03c1 n \u2264 \u00b5 i \u2264 \u03bd i + \u03c1 1 .\nLet S = (\u03a0U ) * \u03a0U . Letting I be the d \u00d7 d identity matrix, Weyl's inequality with M = S, H = (1 + \u03b5 2 )I, and P = S \u2212 (1 + \u03b5 2 )I implies that all the eigenvalues of S lie in the range [1 + \u03b5 2 + \u03bb min (P ), 1 + \u03b5 2 + \u03bb max (P )] \u2286 [1 + \u03b5 2 \u2212 P , 1 + \u03b5 2 + P ], where \u03bb min (M ) (resp. \u03bb max (M )) is the smallest (resp. largest) eigenvalue of M . Since P \u2264 \u03b5 2 + S \u2212 I , it thus suffices to show\nP( S \u2212 I > 2\u03b5 \u2212 \u03b5 2 ) < \u03b4,(2)since P \u2264 2\u03b5 implies that all eigenvalues of S lie in [(1 \u2212 \u03b5) 2 , (1 + \u03b5) 2 ].\nBefore proceeding with our proofs below, observe that for all k, k \u2032\nS k,k \u2032 = 1 s m r=1 n i=1 \u03b4 r,i \u03c3 r,i u k i n i=1 \u03b4 r,i \u03c3 r,i u k \u2032 i = 1 s n i=1 u k i u k \u2032 i \u00b7 m r=1 \u03b4 r,i + 1 s m r=1 i =j \u03b4 r,i \u03b4 r,j \u03c3 r,i \u03c3 r,j u k i u k \u2032 j = u k , u k \u2032 + 1 s m r=1 i =j \u03b4 r,i \u03b4 r,j \u03c3 r,i \u03c3 r,j u k i u k \u2032 j Noting u k , u k = u k 2 = 1 and u k , u k \u2032 = 0 for k = k \u2032 , we have for all k, k \u2032 (S \u2212 I) k,k \u2032 = m r=1 i =j \u03b4 r,i \u03b4 r,j \u03c3 r,i \u03c3 r,j u k i u k \u2032 j .(3)\nTheorem 3. For \u03a0 an OSNAP with s = 1 and \u03b5 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 all singular values of \u03a0U are 1 \u00b1 \u03b5 as long as\nm \u2265 \u03b4 \u22121 (d 2 + d)/(2\u03b5 \u2212 \u03b5 2 ) 2 , \u03c3 is 4-wise independent, and h is pairwise independent.\nProof. We show Eq.\n\n(2). Our approach is to bound E S \u2212 I 2 then use Markov's inequality. Since\nP( S\u2212I > 2\u03b5\u2212\u03b5 2 ) = P( S\u2212I 2 > (2\u03b5\u2212\u03b5 2 ) 2 ) \u2264 (2\u03b5\u2212\u03b5 2 ) \u22122 \u00b7E S\u2212I 2 \u2264 (2\u03b5\u2212\u03b5 2 ) \u22122 \u00b7E S\u2212I 2 F , (4) we can bound E S \u2212 I 2 F to show Eq.\n(2). Here \u00b7 F denotes Frobenius norm. Now we bound E S \u2212 I 2 F . We first deal with the diagonal terms of S \u2212 I. By Eq.\n\n(3),\nE(S \u2212 I) 2 k,k = m r=1 i =j 2 m 2 (u k i ) 2 (u k j ) 2 \u2264 2 m \u00b7 u i 4 = 2 m ,\nand thus the diagonal terms in total contribute at most 2d/m to E S \u2212 I 2 F . We now focus on the off-diagonal terms. By Eq.\n(3), E(S \u2212 I) 2 k,k \u2032 is equal to 1 m 2 m r=1 i =j (u k i ) 2 (u k \u2032 j ) 2 + u k i u k \u2032 i u k j u k \u2032 j = 1 m i =j (u k i ) 2 (u k \u2032 j ) 2 + u k i u k \u2032 i u k j u k \u2032 j . Noting 0 = u k , u k \u2032 2 = n k=1 (u k i ) 2 (u k \u2032 i ) 2 + i =j u k i u k \u2032 i u k j u k \u2032 j we have that i =j u k i u k \u2032 i u k j u k \u2032 j \u2264 0, so E(S \u2212 I) 2 k,k \u2032 \u2264 1 m i =j (u k i ) 2 (u k \u2032 j ) 2 \u2264 1 m u i 2 \u00b7 u j 2 = 1 m .\nThus summing over i = j, the total contribution from off-diagonal terms to E S \u2212 I 2 F is at most d(d \u2212 1)/m. Thus in total E S \u2212 I 2 F \u2264 (d 2 + d)/m, and so Eq. (4) and our setting of m gives\nP S \u2212 I > 2\u03b5 \u2212 \u03b5 2 < 1 (2\u03b5 \u2212 \u03b5 2 ) 2 \u00b7 d 2 + d m \u2264 \u03b4.\nBefore proving the next theorem, it is helpful to state a few facts that we will repeatedly use. Recall that u i denotes the ith column of U , and we will let u i denote the ith row of U .\nLemma 4. n k=1 u k u * k = I. Proof. n k=1 u k u * k i,j = e * i n k=1 u k u * k e j = n k=1 (u k ) i (u k ) j = u i , u j ,\nand this inner product is 1 for i = j and 0 otherwise.\nLemma 5. For all i \u2208 [n], u i \u2264 1. Proof. We can extend U to some orthogonal matrix U \u2032 \u2208 R n\u00d7n by appending n \u2212 d columns. For the rows u \u2032 i of U \u2032 we then have u i \u2264 u \u2032 i = 1. Theorem 6 ( [NW61, Tut61]). A multigraph G has k edge-disjoint spanning trees iff |E P (G)| \u2265 k(|P | \u2212 1)\nfor every partition P of the vertex set of G, where E P (G) is the set of edges of G crossing between two different partitions in P .\n\nThe following corollary is standard, and we will later only need it for the case k = 2.\n\nCorollary 7. Let G be a multigraph formed by removing at most k edges from a multigraph G \u2032 that has edge-connectivity at least 2k. Then G must have at least k edge-disjoint spanning trees.\n\nProof. For any partition P of the vertex set, each partition must have at least 2k edges leaving it in G \u2032 . Thus the number of edges crossing partitions must be at least k|P | in G \u2032 , and thus at least k|P | \u2212 k in G. Theorem 6 thus implies that G has k edge-disjoint spanning trees.\nFact 8. For any matrix B \u2208 C d\u00d7d , B = sup x , y =1 x * By. Proof. We have sup x , y =1 x * By \u2264 B since x * By \u2264 x \u00b7 B \u00b7 y .\nTo show that unit norm x, y exist which achieve B , let B = U \u03a3V * be the singular value decomposition of B. That is, U, V are unitary and \u03a3 is diagonal with entries \u03c3 1 \u2265 \u03c3 2 \u2265 . . . \u03c3 d \u2265 0 so that B = \u03c3 1 . We can then achieve x * By = \u03c3 1 by letting x be the first column of U and y be the first column of V .\n\nTheorem 9. For \u03a0 an OSNAP with s = \u0398(log 3 (d/\u03b4)/\u03b5) and \u03b5 \u2208 (0, 1), with probability at least 1\u2212\u03b4, all singular values of \u03a0U are 1\u00b1\u03b5 as long as m = \u2126(d log 8 (d/\u03b4)/\u03b5 2 ) and \u03c3, h are \u2126(log(d/\u03b4))wise independent.\n\nProof. We will again show Eq.\n\n(2). Recall that by Eq. (1) we have\nP( S \u2212 I \u2265 t) \u2264 t \u2212\u2113 \u00b7 Etr((S \u2212 I) \u2113 )(5)\nfor \u2113 any even integer. We thus proceed by bounding Etr((S \u2212 I) \u2113 ) then applying Eq. (5).\n\nIt is easy to verify by induction on \u2113 that for any B \u2208 R n\u00d7n and \u2113 \u2265 1,\n(B \u2113 ) i,j = t 1 ,...,t \u2113+1 \u2208[n] t 1 =i,t \u2113+1 =j \u2113 k=1 B t k ,t k+1 , and thus tr(B \u2113 ) = t 1 ,...,t \u2113+1 \u2208[n] t 1 =t \u2113+1 \u2113 k=1 B t k ,t k+1 .\nApplying this identity to B = S \u2212 I yields\nEtr((S \u2212 I) \u2113 ) = 1 s \u2113 \u00b7 E k 1 ,k 2 ,...,k \u2113+1 k 1 =k \u2113+1 i 1 =j 1 ,...,i \u2113 =j \u2113 r 1 ,...,r \u2113 \u2113 t=1 \u03b4 rt,it \u03b4 rt,jt \u03c3 rt,it \u03c3 rt,jt u kt it u k t+1 jt .(6)\nThe general strategy to bound the above summation is the following. Let \u03a8 be the set of all monomials appearing on the right hand side of Eq. (6). For \u03c8 \u2208 \u03a8 define K(\u03c8) = (k 1 , . . . , k \u2113 ) as the ordered tuple of k t values in \u03c8, and similarly define P (\u03c8) = ((i 1 , j 1 ), . . . , (i \u2113 , j \u2113 )) and W (\u03c8) = (r 1 , . . . , r \u2113 ). For each \u03c8 \u2208 \u03a8 we associate a three-layered undirected multigraph G \u03c8 with labeled edges and unlabeled vertices. We call these three layers the left, middle, and right layers, and we refer to vertices in the left layer as left vertices, and similarly for vertices in the other layers. Define M (\u03c8) to be the set {i 1 , . . . , i \u2113 , j 1 , . . . , j \u2113 } and define R(\u03c8) = {r 1 , . . . , r \u2113 }. We define y = |M (\u03c8)| and z = |R(\u03c8)|. Note it can happen that y < 2\u2113 if some i t = i t \u2032 , j t = j t \u2032 , or i t = j t \u2032 , and similarly we may also have z < \u2113. The graph G \u03c8 has x = \u2113 left vertices, y middle vertices corresponding to the distinct i t , j t in \u03c8, and z right vertices corresponding to the distinct r t . For the sake of brevity, often we refer to the vertex corresponding to i t (resp. j t , r t ) as simply i t (resp. j t , r t ). Thus note that when we refer to for example some vertex i t , it may happen that some other i t \u2032 or j t \u2032 is also the same vertex. We now describe the edges of G \u03c8 . For \u03c8 = \u2113 t=1 \u03b4 rt,it \u03b4 rt,jt \u03c3 rt,it \u03c3 rt,jt u kt it u k t+1 jt we draw 4\u2113 labeled edges in G \u03c8 with distinct labels in [4\u2113]. For each t \u2208 [\u2113] we draw an edge from the tth left vertex to i t with label 4(t \u2212 1) + 1, from i t to r t with label 4(t \u2212 1) + 2, from r t to j t with label 4(t \u2212 1) + 3, and from j t to the (t + 1)st left vertex with label 4(t \u2212 1) + 4. Observe that many different monomials \u03c8 will map to the same graph G \u03c8 ; in particular the graph maintains no information concerning equalities amongst the k t , and the y middle vertices may map to any y distinct values in [n] (and similarly the right vertices may map to any z distinct values in [m]). We handle the right hand side of Eq. (6) by grouping monomials \u03c8 that map to the same graph, bound the total contribution of a given graph G in terms of its graph structure when summing over all \u03c8 with G \u03c8 = G, then sum the contributions from all such graphs G combined.\n\nBefore continuing further we introduce some more notation then make a few observations. For a graph G as above, recall G has 4\u2113 edges, and we refer to the distinct edges (ignoring labels) as bonds. We let E(G) denote the edge multiset of a multigraph G and B(G) denote the bond set. We refer to the number of bonds a vertex is incident upon as its bond-degree, and the number of edges as its edge-degree. We do not count self-loops for calculating bond-degree, and we count them twice for edge-degree. We let LM (G) be the induced multigraph on the left and middle vertices of G, and M R(G) be the induced multigraph on the middle and right vertices. We let w = w(G) be the number of connected components in M R(G). We let b = b(G) denote the number of bonds in M R(G) (note M R(G) has 2\u2113 edges, but it may happen that b < 2\u2113 since G is a multigraph). Given G we define the undirected dot product multigraph G with vertex set M (\u03c8). Note every left vertex of G has edge-degree 2. For each t \u2208 [\u2113] an edge (i, j) is drawn in G between the two middle vertices that the tth left vertex is adjacent to (we draw a self-loop on i if i = j). We do not label the edges of G, but we label the vertices with distinct labels in [y] in increasing order of when each vertex was first visited by the natural tour of G (by following edges in increasing label order). We name G the dot product multigraph since if some left vertex t has its two edges connecting to vertices i, j \u2208 [n], then summing over k t \u2208 [d] produces the dot product u i , u j . Now we make some observations. Due to the random signs \u03c3 r,i , a monomial \u03c8 has expectation zero unless every bond in M R(G) has even multiplicity, in which case the product of random signs in \u03c8 is 1. Also, note the expectation of the product of the \u03b4 r,i terms in \u03c8 is at most (s/m) b by OSNAP properties. Thus letting G be the set of all such graphs G with even bond multiplicity in M R(G) that arise from some monomial \u03c8 appearing in Eq. (6), we have\nEtr((S \u2212 I) \u2113 ) \u2264 1 s \u2113 \u00b7 G\u2208G s m b \u00b7 \u03c8:G \u03c8 =G \u2113 t=1 u kt it u k t+1 jt = 1 s \u2113 \u00b7 G\u2208G s m b m z \u00b7 \u03c8:G \u03c8 =G R(\u03c8)=[z] k 1 ,...,k \u2113 \u2113 t=1 u kt it u k t+1 jt = 1 s \u2113 \u00b7 G\u2208G s m b m z \u00b7 a 1 ,...,ay\u2208[n] \u2200i =j a i =a j e\u2208E( G) e=(i,j) u a i , u a j(7)\nBefore continuing further it will be convenient to introduce a notion we will use in our analysis called a generalized dot product multigraph. Such a graph G is just as in the case of a dot product multigraph, except that each edge e = (i, j) is associated with some matrix M e . We call M e the edge-matrix of e. Also since G is undirected, we can think of an edge e = (i, j) with edge-matrix M e also as an edge (j, i), in which case we say its associated edge-matrix is M * e . We then associate with G the product\ne\u2208 G e=(i,j) u a i , M e u a j .\nNote that a dot product multigraph is simply a generalized dot product multigraph in which M e = I for all e. Also, in such a generalized dot product multigraph, we treat multiedges as representing the same bond iff the associated edge-matrices are also equal (in general multiedges may have different edge-matrices). \n\u00b7 \u00b7 \u00b7 n a N =1 e\u2208E(H) e=(i,j) v a i , M e v a j ,\nwhere v a i = u a i for i = 1, and v a 1 equals some fixed vector c with c \u2264 1. Then f (H) \u2264 c 2 .\n\nProof. Let \u03c0 be some permutation of {2, . . . , N }. For a bond q = (i, j) \u2208 B(H), let 2\u03b1 q denote the multiplicity of q in H. Then by ordering the assignments of the a t in the summation\na 2 ,...,a N \u2208[n] e\u2208E(H) e=(i,j) v a i , M e v a j\naccording to \u03c0, we obtain the exactly equal expression n a \u03c0(N) =1 q\u2208B(H) q=(\u03c0(N ),j)\nN \u2264\u03c0 \u22121 (j) v a \u03c0(N) , M q v a j 2\u03b1q \u00b7 \u00b7 \u00b7 n a \u03c0(2) =1 q\u2208B(H) q=(\u03c0(1),j) 2\u2264\u03c0 \u22121 (j) v a \u03c0(2) , M q v a j 2\u03b1q .(8)\nHere we have taken the product over t \u2264 \u03c0 \u22121 (j) as opposed to t < \u03c0 \u22121 (j) since there may be selfloops. By Lemma 5 and the fact that c \u2264 1 we have that for any i, j, v i , v j 2 \u2264 v i 2 \u00b7 v j 2 \u2264 1, so we obtain an upper bound on Eq. (8) by replacing each v a \u03c0(t) , v a j 2\u03b1v term with v a \u03c0(t) , v a j 2 . We can thus obtain the sum\nn a \u03c0(N) =1 q\u2208B(H) q=(\u03c0(N ),j) q\u2264\u03c0 \u22121 (j) v a \u03c0(N) , M q v a j 2 \u00b7 \u00b7 \u00b7 n a \u03c0(2) =1 q\u2208B(H) q=(\u03c0(2),j) 2\u2264\u03c0 \u22121 (j) v a \u03c0(2) , M q v a j 2 ,(9)\nwhich upper bounds Eq. (8). Now note for 2 \u2264 t \u2264 N that for any nonnegative integer \u03b2 t and for {q \u2208 B(H) : q = (\u03c0(t), j), t < \u03c0 \u22121 (j)} non-empty (note the strict inequality t < \u03c0 \u22121 (j)),\nn a \u03c0(t) =1 v a \u03c0(t) 2\u03b2t \u00b7 q\u2208B(H) q=(\u03c0(t),j) t\u2264\u03c0 \u22121 (j) v a \u03c0(t) , M q v a j 2 \u2264 n a \u03c0(t) =1 q\u2208B(H) q=(\u03c0(t),j) t\u2264\u03c0 \u22121 (j) v a \u03c0(t) , M q v a j 2 (10) \u2264 q\u2208B(H) q=(\u03c0(t),j) t<\u03c0 \u22121 (j) \uf8eb \uf8ed n a \u03c0(t) =1 v a \u03c0(t) , M q v a j 2 \uf8f6 \uf8f8 = q\u2208B(H) q=(\u03c0(t),j) t<\u03c0 \u22121 (j) \uf8eb \uf8ed n a \u03c0(t) =1 v * a j M * q v a \u03c0(t) v * a \u03c0(t) M q v a j \uf8f6 \uf8f8 = q\u2208B(H) q=(\u03c0(t),j) t<\u03c0 \u22121 (j) (M q v a j ) * n i=1 u i u * i M q v a j = q\u2208B(H) q=(\u03c0(t),j) t<\u03c0 \u22121 (j) M q v a j 2 (11) \u2264 q\u2208B(H) q=(\u03c0(t),j) t<\u03c0 \u22121 (j) v a j 2 ,(12)\nwhere Eq. (10) used Lemma 5, Eq. (11) used Lemma 4, and Eq. (12) used that M q \u2264 1. Now consider processing the alternating sum-product in Eq. (9) from right to left. We say that a bond (i, j) \u2208 B(H) is assigned to i if \u03c0 \u22121 (i) < \u03c0 \u22121 (j). When arriving at the tth sum-product and using the upper bound Eq. (11) on the previous t \u2212 1 sum-products, we will have a sum over v a \u03c0(t) 2 raised to some nonnegative power (specifically the number of bonds incident upon \u03c0(t) but not assigned to \u03c0(t), plus one if \u03c0(t) has a self-loop) multiplied by a product of v a \u03c0(t) , v a j 2 over all bonds (\u03c0(t), j) assigned to \u03c0(t). There are two cases. In the first case \u03c0(t) has no bonds assigned to it. We will ignore this case since we will show that we can choose \u03c0 to avoid it.\n\nThe other case is that \u03c0(t) has at least one bond assigned to it. In this case we are in the scenario of Eq. (11) and thus summing over a \u03c0(t) yields a non-empty product of v a j 2 for the j for which (\u03c0(t), j) is a bond assigned to \u03c0(t). Thus in our final sum, as long as we choose \u03c0 to avoid the first case, we are left with an upper bound of c raised to some power equal to the edge-degree of vertex 1 in H, which is at least 2. The lemma would then follow since c j \u2264 c 2 for j \u2265 2.\n\nIt now remains to show that we can choose \u03c0 to avoid the first case where some t \u2208 {2, . . . , N } is such that \u03c0(t) has no bonds assigned to it. Let T be a spanning tree in H rooted at vertex 1. We then choose any \u03c0 with the property that for any i < j, \u03c0(i) is not an ancestor of \u03c0(j) in T . This can be achieved, for example, by assigning \u03c0 values in reverse breadth first search order.\n\nLemma 11. Let G be any dot product graph as in Eq. (7). Then\na 1 ,...,ay\u2208[n] \u2200i =j a i =a j e\u2208 G e=(i,j) u a i , u a j \u2264 y! \u00b7 d y\u2212w+1 .\nProof. We first note that we have the inequality\na 1 ,...,ay\u2208[n] \u2200i =j a i =a j e\u2208E( G) e=(i,j) u a i , u a j = a 1 ,...,a y\u22121 \u2208[n] \u2200i =j\u2208[y\u22121] a i =a j \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed n ay=1 e\u2208E( G) e=(i,j) u a i , u a j \u2212 y\u22121 t=1 ay=at e\u2208E( G) e=(i,j) u a i , u a j \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u2264 a 1 ,...,a y\u22121 \u2208[n] \u2200i =j\u2208[y\u22121] a i =a j n ay=1 e\u2208E( G) e=(i,j) u a i , u a j + y\u22121 t=1 a 1 ,...,a y\u22121 \u2208[n] \u2200i =j\u2208[y\u22121] a i =a j ay=at e\u2208E( G) e=(i,j) u a i , u a j\nWe can view the sum over t on the right hand side of the above as creating t \u2212 1 new dot product multigraphs, each with one fewer vertex where we eliminated vertex y and associated it with vertex applying this inequality to each of the resulting t summations, we bound\na 1 ,...,ay\u2208[n] \u2200i =j a i =a j e\u2208E( G) e=(i,j) u a i , u a j\nby a sum of contributions from y! dot product multigraphs where in none of these multigraphs do we have the constraint that a i = a j for i = j. We will show that each one of these resulting multigraphs contributes at most d y\u2212w+1 , from which the lemma follows. Let G \u2032 be one of the dot product multigraphs at a leaf of the above recursion so that we now wish to bound\nF (G \u2032 ) def = n a 1 ,...,ay=1 e\u2208E( G \u2032 ) e=(i,j) u a i , M e u a j(13)\nwhere M e = I for all e for G \u2032 . Before proceeding, we first claim that every connected component of G \u2032 is Eulerian. To see this, observe G has an Eulerian tour, by following the edges of G in increasing order of label, and thus all middle vertices have even edge-degree in G. However they also have even edge-degree in M R(G), and thus the edge-degree of a middle vertex in LM (G) must be even as well. Thus, every vertex in G has even edge-degree, and thus every vertex in each of the recursively created leaf graphs also has even edge-degree since at every step when we eliminate a vertex, some other vertex's degree increases by the eliminated vertex's degree which was even. Thus every connected component of G \u2032 is Eulerian as desired.\n\nWe now upper bound F (G \u2032 ). Let the connected components of G \u2032 be C 1 , . . . , C CC(G \u2032 ) , where CC(\u00b7) counts connected components. An observation we repeatedly use later is that for any generalized dot product multigraph H with components C 1 , . . . , C CC(H) ,\nF (H) = CC(H) i=1 F (C i ).(14)\nWe treat G \u2032 as a generalized dot product multigraph so that each edge e has an associated matrix M e (though in fact M e = I for all e). Define an undirected multigraph to be good if all its connected components have two edge-disjoint spanning trees. We will show that F (G \u2032 ) \u2264 F (G \u2032\u2032 ) for some generalized dot product multigraph G \u2032\u2032 that is good then will show F (G \u2032\u2032 ) \u2264 d y\u2212w+1 . If G \u2032 itself is good then we can set G \u2032\u2032 = G \u2032 . Otherwise, we will show F (G \u2032 ) = F (H 0 ) = . . . = F (H \u03c4 ) for smaller and smaller generalized dot product multigraphs H t (i.e. with successively fewer vertices) whilst maintaining the invariant that each H t has Eulerian connected components and has M e \u2264 1 for all e. We stop when some H \u03c4 is good and we can set G \u2032\u2032 = H \u03c4 . Let us now focus on constructing this sequence of H t in the case that G \u2032 is not good. Let H 0 = G \u2032 . Suppose we have constructed H 0 , . . . , H t\u22121 for i \u2265 1 none of which are good, and now we want to construct H t . Since H t\u22121 is not good it cannot be 4-edge-connected by Corollary 7, so there is some connected component C j * of H t\u22121 with some cut S V (C j * ) with 2 edges crossing the cut (S, V (C j * )\\S) (note that since C j * is Eulerian, any cut has an even number of edges crossing it). Choose such an S V (C j * ) with |S| minimum amongst all such cuts. Let the two edges crossing the cut be (g, h), (g \u2032 , h \u2032 ) with h, h \u2032 \u2208 S (note that it may be the case that g = g \u2032 and/or h = h \u2032 ). Note that F (C j * ) equals the magnitude of\na V (C j * )\\S \u2208[n] |V (C j * )\\S| \uf8eb \uf8ec \uf8ec \uf8ed e\u2208E(V (C j * )\\S) e=(i,j) ua i , Meua j \uf8f6 \uf8f7 \uf8f7 \uf8f8 u * ag M (g,h) \uf8eb \uf8ec \uf8ec \uf8ed a S \u2208[n] |S| ua h \uf8eb \uf8ec \uf8ec \uf8ed e\u2208E(C j * (S)) e=(i,j) ua i , Meua j \uf8f6 \uf8f7 \uf8f7 \uf8f8 u * a h \u2032 \uf8f6 \uf8f7 \uf8f7 \uf8f8 M (h \u2032 ,g \u2032 ) M ua g \u2032 .(15)\nWe define H t to be H t\u22121 but where in the j * th component we replace C j * with C * j (V (C * j )\\S) and add an additional edge from g to g \u2032 which we assign edge-matrix M . We thus have that F (H t\u22121 ) = F (H t ) by Eq. (14). Furthermore each component of H t is still Eulerian since every vertex in H t\u22121 has either been eliminated, or its edge-degree has been preserved and thus all edge-degrees are even. It remains to show that M \u2264 1.\n\nWe first claim that C j * (S) has two edge-disjoint spanning trees. Define C \u2032 to be the graph C j * (S) with an edge from h to h \u2032 added. We show that C \u2032 (S) is 4-edge-connected so that C j * (S) has two edge-disjoint spanning trees by Corollary 7. Now to see this, consider some S \u2032 S. Consider the cut (S \u2032 , V (C \u2032 )\\S \u2032 ). C \u2032 is Eulerian, so the number of edges crossing this cut is either 2 or at least 4. If it 2, then since |S \u2032 | < |S| this is a contradiction since S was chosen amongst such cuts to have |S| minimum. Thus it is at least 4, and we claim that the number of edges crossing the cut (S \u2032 , S\\S \u2032 ) in C \u2032 (S) must also be at least 4. If not, then it is 2 since C \u2032 (S) is Eulerian. However since the number of edges leaving S \u2032 in C \u2032 is at least 4, it must then be that h, h \u2032 \u2208 S \u2032 . But then the cut (S\\S \u2032 , V (C \u2032 )\\(S\\S \u2032 )) has 2 edges crossing it so that S\\S \u2032 is a smaller cut than S with 2 edges leaving it in C \u2032 , violating the minimality of |S|, a contradiction. Thus C \u2032 (S) is 4-edge-connected, implying C j * (S) has two edge-disjoint spanning trees T 1 , T 2 as desired. Now to show M \u2264 1, by Fact 8 we have M = sup x , x \u2032 =1 x * M x \u2032 . We have that\nx * M x \u2032 = a S \u2208[n] |S| x, M (g,h) u a h \u00b7 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed e\u2208E(C j * (S)) e=(i,j) u a i , M e u a j \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u00b7 u a h \u2032 , M (h \u2032 ,g \u2032 ) x \u2032 = a S \u2208[n] |S| \uf8eb \uf8ec \uf8ec \uf8ed x, M (g,h) u a h \u00b7 e\u2208T 1 e=(i,j) u a i , M e u a j \uf8f6 \uf8f7 \uf8f7 \uf8f8 \u00b7 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed u a h \u2032 , M (h \u2032 ,g \u2032 ) x \u2032 \u00b7 e\u2208E(C j * (S))\\T 1 e=(i,j) u a i , M e u a j \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u2264 1 2 \u00b7 \uf8ee \uf8ef \uf8ef \uf8f0 a S \u2208[n] |S| \uf8eb \uf8ec \uf8ec \uf8ed x, M (g,h) u a h 2 \u00b7 e\u2208T 1 e=(i,j) u a i , M e u a j 2 \uf8f6 \uf8f7 \uf8f7 \uf8f8 + a S \u2208[n] |S| \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed u a h \u2032 , M (h \u2032 ,g \u2032 ) x \u2032 2 \u00b7 e\u2208E(C j * (S))\\T 1 e=(i,j) u a i , M e u a j 2 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb (16) \u2264 1 2 x 2 + x \u2032 2 (17) = 1,\nwhere Eq. (16) used the AM-GM inequality, and Eq. (17) used Lemma 10 (note the graph with vertex set S \u222a{g \u2032 } and edge set E(C j * (S))\\T 1 \u222a{(g \u2032 , h \u2032 )} is connected since T 2 \u2286 E(C j * (S))\\T 1 ). Thus we have shown that H t satisfies the desired properties. Now notice that the sequence H 0 , . . . , H 1 , . . . must eventually terminate since the number of vertices is strictly decreasing in this sequence and any Eulerian graph on 2 vertices is good. Therefore we have that H \u03c4 is eventually good for some \u03c4 > 0 and we can set G \u2032\u2032 = H \u03c4 . It remains to show that for our final good G \u2032\u2032 we have F (G \u2032\u2032 ) \u2264 d y\u2212w+1 . We will show this in two parts by showing that both CC(G \u2032\u2032 ) \u2264 d y\u2212w+1 and F (G \u2032\u2032 ) \u2264 d CC(G \u2032\u2032 ) . For the first claim, note that CC(G \u2032\u2032 ) \u2264 CC( G) since every H t has the same number of connected components as G \u2032 , and CC(G \u2032 ) \u2264 CC( G). This latter inequality holds since in each level of recursion used to eventually obtain G \u2032 from G, we repeatedly identified two vertices as equal and merged them, which can only decrease the number of connected components. Now, all middle vertices in G lie in one connected component (since G is connected) and M R(G) has w connected components. Thus the at least w \u2212 1 edges connecting these components in G must come from LM (G), implying that LM (G) (and thus G) has at most y \u2212 w + 1 connected components, which thus must also be true for G \u2032\u2032 as argued above.\n\nIt only remains to show F (G \u2032\u2032 ) \u2264 d CC(G \u2032\u2032 ) . Let G \u2032\u2032 have connected components C 1 , . . . , C CC(G \u2032\u2032 ) with each C j having 2 edge-disjoint spanning trees T j 1 , T j 2 . We then have\nF (G \u2032\u2032 ) = CC(G \u2032\u2032 ) t=1 F (C t ) = CC(G \u2032\u2032 ) t=1 n a 1 ,...,a |V (C t )| =1 e\u2208E(Ct) e=(i,j) u a i , M e u a j = CC(G \u2032\u2032 ) t=1 n a 1 ,...,a |V (C t )| =1 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed e\u2208T t 1 e=(i,j) u a i , M e u a j \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u00b7 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed e\u2208E(Ct)\\T t 1 e=(i,j) u a i , M e u a j \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u2264 CC(G \u2032\u2032 ) t=1 1 2 \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 n a 1 =1 n a 2 ,...,a |V (C t )| =1 e\u2208T t 1 e=(i,j) u a i , M e u a j 2 + n a 1 =1 n a 2 ,...,a |V (C t )| =1 e\u2208E(Ct)\\T t 1 e=(i,j) u a i , M e u a j 2 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb (18) \u2264 CC(G \u2032\u2032 ) t=1 n a 1 =1 u a 1 2 (19) = CC(G \u2032\u2032 ) t= U 2 F = d CC(G \u2032\u2032 )\nwhere Eq. (18) used the AM-GM inequality, and Eq. (19) used Lemma 10, which applies since V (C t ) with edge set T t 1 is connected, and V (C t ) with edge set E(C t )\\T t 1 is connected (since T t 2 \u2286 E(C t )\\T t 1 ). Now, for any G \u2208 G we have y + z \u2264 b + w since for any graph the number of edges plus the number of connected components is at least the number of vertices. We also have b \u2265 2z since every right vertex of G is incident upon at least two distinct bonds (since i t = j t for all t). We also have y \u2264 b \u2264 \u2113 since M R(G) has exactly 2\u2113 edges with no isolated vertices, and every bond has even multiplicity. Finally, a crude bound on the number of different G \u2208 G with a given b, y, z is (zy 2 ) \u2113 \u2264 (b 3 ) \u2113 . This is because when drawing the graph edges in increasing order of edge label, when at a left vertex, we draw edges from the left to the middle, then to the right, then to the middle, and then back to the left again, giving y 2 z choices. This is done \u2113 times. Thus by Lemma 11 and Eq. (7), and using that t! \u2264 e \u221a t(t/e) t for all t \u2265 1,\nEtr((S \u2212 I) \u2113 ) \u2264 d \u00b7 1 s \u2113 b,y,z,w G\u2208G b(G)=b,y(G)=y w(G)=w,z(G)=z y! \u00b7 s b \u00b7 m z\u2212b \u00b7 d y\u2212w \u2264 ed \u221a \u2113 \u00b7 1 s \u2113 b,y,z,w (b/e) b s b G\u2208G b(G)=b,y(G)=y w(G)=w,z(G)=z d m b\u2212z \u2264 ed \u221a \u2113 \u00b7 1 s \u2113 b,y,z,w b 3\u2113 (b/e) b s b \u00b7 d m b\u2212z \u2264 ed \u221a \u2113 \u00b7 1 s \u2113 b,y,z,w b 3\u2113 (sb/e) d m b \u2264 ed\u2113 4 \u221a \u2113 \u00b7 max 2\u2264b\u2264\u2113 b 3 s \u2113\u2212b (b 4 /e) d m b(20)\nDefine \u01eb = 2\u03b5 \u2212 \u03b5 2 . For \u2113 \u2265 ln(ed\u2113 9/2 /\u03b4) = O(ln(d/\u03b4)), s \u2265 e\u2113 3 /\u01eb = O(log(d/\u03b4) 3 /\u03b5), and m \u2265 d\u2113 8 /\u01eb 2 = O(d log(d/\u03b4) 8 /\u03b5 2 ), the above expression is at most \u03b4\u01eb \u2113 . Thus as in Eq. (2), by Eq. (5) we have\nP ( S \u2212 I > \u01eb) < 1 \u01eb \u2113 \u00b7 Etr((S \u2212 I) \u2113 ) \u2264 \u03b4.\nThe proof of Theorem 9 reveals that for \u03b4 = 1/poly(d) one could also set m = O(d 1+\u03b3 /\u03b5 2 ) and s = O \u03b3 (1/\u03b5) for any fixed constant \u03b3 > 0 and arrive at the same conclusion. Indeed, let \u03b3 \u2032 < \u03b3 be any positive constant. Let \u2113 in the proof of Theorem 9 be taken as O(log(d/\u03b4)) = O(log d). It suffices to ensure max 2\u2264b\u2264\u2113 (b 3 /s) \u2113\u2212b \u00b7 ((b 4 /e) d/m) b \u2264 \u03b5 \u2113 \u03b4/(ed\u2113 9/2 ) by Eq. (20). Note d \u03b3 \u2032 > b 3\u2113 as long as b/ ln b > 3\u03b3 \u22121 \u2113/ ln d = O(1/\u03b3 \u2032 ), so d \u03b3 \u2032 > b 3\u2113 for b > b * for some b * = \u0398(\u03b3 \u22121 / log(1/\u03b3)). We choose s \u2265 e(b * ) 3 /\u03b5 and m = d 1+\u03b3 /\u03b5 2 , which is at least d 1+\u03b3 \u2032 \u2113 8 /\u03b5 2 for d larger than some fixed constant. Thus the max above is always as small as desired, which can be seen by looking at b \u2264 b * and b > b * separately (in the former case b 3 /s < 1/e, and in the latter case (b 3 /s) \u2113\u2212b \u00b7 ((b 4 /e) d/m) b < (\u03b5/e) \u2113 b 3\u2113 d \u2212\u03b3 \u2032 b = (\u03b5/e) \u2113 e 3\u2113 ln b\u2212\u03b3 \u2032 b ln d < (\u03b5/e) \u2113 is as small as desired). This observation yields:\n\nTheorem 12. Let \u03b1, \u03b3 > 0 be arbitrary constants. For \u03a0 an OSNAP with s = \u0398(1/\u03b5) and \u03b5 \u2208 (0, 1), with probability at least 1 \u2212 1/d \u03b1 , all singular values of \u03a0U are 1 \u00b1 \u03b5 for m = \u2126(d 1+\u03b3 /\u03b5 2 ) and \u03c3, h being \u2126(log d)-wise independent. The constants in the big-\u0398 and big-\u2126 depend on \u03b1, \u03b3.\n\nRemark 13. Section 1 stated the time to list all non-zeroes in a column in Theorem 9 is t c = O(s). For \u03b4 = 1/poly(d), naively one would actually achieve t c = O(s \u00b7 log d) since one needs to evaluate an O(log d)-wise independent hash function s times. This can be improved to\u00d5(s) using fast multipoint evaluation of hash functions; see for example the last paragraph of Remark 16 of [KNPW11].\n\n\nApplications\n\nWe use the fact that many matrix problems have the same time complexity as matrix multiplication including computing the matrix inverse [BH74] [Har08, Appendix A], and QR decomposition [Sch73]. In this paper we only consider the real RAM model and state the running time in terms of the number of field operations. The algorithms for solving linear systems, computing inverse, QR decomposition, and approximating SVD based on fast matrix multiplication can be implemented with precision comparable to that of conventional algorithms to achieve the same error bound (with a suitable notion of approximation/stability). We refer readers to [DDH07] for details. Notice that it is possible that both algorithms based on fast matrix multiplication and conventional counterparts are unstable, see e.g. [AV97] for an example of a pathological matrix with very high condition number.\n\nIn this section we describe some applications of our subspace embeddings to problems in numerical linear algebra. All applications follow from a straightforward replacement of previously used embeddings with our new ones as most proofs go through verbatim. In the statement of our bounds we implicitly assume nnz(A) \u2265 n, since otherwise fully zero rows of A can be ignored without affecting the problem solution.\n\n\nApproximate Leverage Scores\n\nThis section describes the application of our subspace embedding from Theorem 9 or Theorem 12 to approximating the leverage scores. Consider a matrix A of size n\u00d7d and rank r. Let U be a n\u00d7r matrix whose columns form an orthonormal basis of the column space of A. The leverage scores of A are the squared lengths of the rows of U . The algorithm for approximating the leverage scores and the analysis are the same as those of [CW12], which itself uses essentially the same algorithm outline as Algorithm 1 of [DMIMW12]. The improved bound is stated below (cf. [CW12, Theorem 21]).\n\nTheorem 14. For any constant \u03b5 > 0, there is an algorithm that with probability at least 2/3, approximates all leverage scores of a n \u00d7 d matrix A in time\u00d5(nnz(A)/\u03b5 2 + r \u03c9 \u03b5 \u22122\u03c9 ).\n\nProof. As in [CW12], this follows by replacing the Fast Johnson-Lindenstrauss embedding used in [DMIMW12] with our sparse subspace embeddings. The only difference is in the parameters of our OSNAPs. We essentially repeat the argument verbatim just to illustrate where our new OSE parameters fit in; nothing in this proof is new. Now, we first use [yCKL12] so that we can assume A has only r = rank(A) columns and is of full column rank. Then, we take an OSNAP \u03a0 with m =\u00d5(r/\u03b5 2 ), s = (polylog r)/\u03b5 and compute \u03a0A. We then find R \u22121 so that \u03a0AR \u22121 has orthonormal columns. The analysis of [DMIMW12] shows that the \u2113 2 2 of the rows of AR \u22121 are 1 \u00b1 \u03b5 times the leverage scores of A. Take \u03a0 \u2032 \u2208 R r\u00d7t to be a JL matrix that preserves the \u2113 2 norms of the n rows of AR \u22121 up to 1 \u00b1 \u03b5. Finally, compute R \u22121 \u03a0 \u2032 then A(R \u22121 \u03a0 \u2032 ) and output the squared row norms of AR\u03a0 \u2032 . Now we bound the running time. The time to reduce A to having r linearly independent columns is O((nnz(A) + r \u03c9 ) log n). \u03a0A can be computed in time O(nnz(A) \u00b7 (polylog r)/\u03b5). Computing R \u2208 R r\u00d7r from the QR decomposition takes time\u00d5(m \u03c9 ) =\u00d5(r \u03c9 /\u03b5 2\u03c9 ), and then R can be inverted in time\u00d5(r \u03c9 ); note \u03a0AR \u22121 has orthonormal columns. Computing R \u22121 \u03a0 \u2032 column by column takes time O(r 2 log r) using the FJLT of [AL11,KW11] with t = O(\u03b5 \u22122 log n(log log n) 4 ). We then multiply the matrix A by the r \u00d7 t matrix R \u22121 \u03a0 \u2032 , which takes time O(t \u00b7 nnz(A)) =\u00d5(nnz(A)/\u03b5 2 ).\n\n\nLeast Squares Regression\n\nIn this section, we describe the application of our subspace embeddings to the problem of least squares regression. Here given a matrix A of size n \u00d7 d and a vector b \u2208 R n , the objective is to find x \u2208 R d minimizing Ax \u2212 b 2 . The reduction to subspace embedding is similar to those of [CW12,Sar06]. The proof is included for completeness.\n\nTheorem 15. There is an algorithm for least squares regression running in time O(nnz(A) + d 3 log(d/\u03b5)/\u03b5 2 ) and succeeding with probability at least 2/3.\n\n\nProof.\n\nApplying Theorem 3 to the subspace spanned by columns of A and b, we get a distribution over matrices \u03a0 of size O(d 2 /\u03b5 2 ) \u00d7 n such that \u03a0 preserves lengths of vectors in the subspace up to a factor 1 \u00b1 \u03b5 with probability at least 5/6. Thus, we only need to find argmin x \u03a0Ax \u2212 \u03a0b 2 . Note that \u03a0A has size O(d 2 /\u03b5 2 ) \u00d7 d. By Theorem 12 of [Sar06], there is an algorithm that with probability at least 5/6, finds a 1 \u00b1 \u03b5 approximate solution for least squares regression for the smaller input of \u03a0A and \u03a0b and runs in time O(d 3 log(d/\u03b5)/\u03b5 2 ).\n\nThe following theorem follows from using the embedding of Theorem 9 and the same argument as [CW12, Theorem 32].\n\nTheorem 16. Let r be the rank of A. There is an algorithm for least squares regression running in time O(nnz(A)((log r) O(1) + log(n/\u03b5)) + r \u03c9 (log r) O(1) + r 2 log(1/\u03b5)) and succeeding with probability at least 2/3.\n\n\nLow Rank Approximation\n\nIn this section, we describe the application of our subspace embeddings to low rank approximation. Here given a matrix A, one wants to find a rank k matrix A k minimizing A \u2212 A k F . Let \u2206 k be the minimum A \u2212 A k F over all rank k matrices A k . Notice that our matrices are of the same form as sparse JL matrices considered by [KN12] so the following property holds for matrices constructed in Theorem 9 (cf. [CW12, Lemma 24]).\n\nTheorem 17. [KN12, Theorem 19] Fix \u03b5, \u03b4 > 0. Let D be the distribution over matrices given in Theorem 9 with n columns. For any matrices A, B with n rows,\nP S\u223cD [ A T S T SB \u2212 A T B F > 3\u03b5/2 A F B F ] < \u03b4\nThe matrices of Theorem 3 are the same as those of [CW12] so the above property holds for them as well. Therefore, the same algorithm and analysis as in [CW12] work. We state the improved bounds using the embedding of Theorem 3 and Theorem 9 below (cf. [CW12, Theorem 36 and 38]).\n\nTheorem 18. Given a matrix A of size n \u00d7 n, there are 2 algorithms that, with probability at least 3/5, find 3 matrices U, \u03a3, V where U is of size n \u00d7 k, \u03a3 is of size k \u00d7 k, V is of size n \u00d7 k, U T U = V T V = I k , \u03a3 is a diagonal matrix, and\nA \u2212 U \u03a3V * F \u2264 (1 + \u03b5)\u2206 k\nThe first algorithm runs in time O(nnz(A))+\u00d5(nk 2 +nk \u03c9\u22121 \u03b5 \u22121\u2212\u03c9 +k \u03c9 \u03b5 \u22122\u2212\u03c9 ). The second algorithm runs in time O(nnz(A) log O(1) k) +\u00d5(nk \u03c9\u22121 \u03b5 \u22121\u2212\u03c9 + k \u03c9 \u03b5 \u22122\u2212\u03c9 ).\n\nProof. The proof is essentially the same as that of [CW12] so we only mention the difference. We \n\nLemma 10 .\n10Let H be a connected generalized dot product multigraph on vertex set [N ] with E(H) = \u2205 and where every bond has even multiplicity. Also suppose that for all e \u2208 E(H), M e \u2264 1. Define f (H) = n a 2 =1\n\n\nuse 2 bounds for the running time: multiplying an a \u00d7 b matrix and a b \u00d7 c matrix with c > a takes O(a \u03c9\u22122 bc) time (simply dividing the matrices into a \u00d7 a blocks), and approximating SVD for an a \u00d7 b matrix M with a > b takes O(ab \u03c9\u22121 ) time (time to compute M T M , approximate SVD of M T M = QDQ T in O(b \u03c9 ) time [DDH07], and compute M Q to complete the SVD of M ).\nRecently after sharing the statement of our bounds with the authors of[CW12], independently of our methods they have been able to push their own methods further to obtain m = O((d 2 /\u03b5 2 ) log 6 (d/\u03b5)) with s = 1, nearly matching our bound, though only for the s = 1 case. This improves the two bounds in the topmost row ofFigure 1under the[CW12] reference to come within polylog d or polylog k factors of the two bounds in our topmost row.\nRecall that the singular values of a (possibly rectangular) matrix B are the square roots of the eigenvalues of B * B, where (\u00b7) * denotes conjugate transpose.\nt for some t, and for each edge (y, a) we effectively replaced it with (t, a). Also in first sum where we sum over all n values of a y , we have eliminated the constraints a y = a i for i = y. By recursively\nAcknowledgmentsWe thank Andrew Drucker for suggesting the SNAP acronym for the OSE's considered in this work, to which we added the \"oblivious\" descriptor.\nThe Fast Johnson-Lindenstrauss transform and approximate nearest neighbors. Nir Ailon, Bernard Chazelle, SIAM J. Comput. 391Nir Ailon and Bernard Chazelle. The Fast Johnson-Lindenstrauss transform and approximate nearest neighbors. SIAM J. Comput., 39(1):302-322, 2009.\n\nDatabase-friendly random projections: Johnson-Lindenstrauss with binary coins. Dimitris Achlioptas, J. Comput. Syst. Sci. 664Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. J. Comput. Syst. Sci., 66(4):671-687, 2003.\n\nFast dimension reduction using Rademacher series on dual BCH codes. Nir Ailon, Edo Liberty, Discrete Comput. Geom. 424Nir Ailon and Edo Liberty. Fast dimension reduction using Rademacher series on dual BCH codes. Discrete Comput. Geom., 42(4):615-630, 2009.\n\nAlmost optimal unrestricted fast Johnson-Lindenstrauss transform. Nir Ailon, Edo Liberty, Proceedings of the 22nd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). the 22nd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)Nir Ailon and Edo Liberty. Almost optimal unrestricted fast Johnson-Lindenstrauss transform. In Proceedings of the 22nd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 185-191, 2011.\n\nAnti-Hadamard matrices, coin weighing, threshold gates, and indecomposable hypergraphs. Noga Alon, H Van, Vu, J. Comb. Theory, Ser. A. 791Noga Alon and Van H. Vu. Anti-Hadamard matrices, coin weighing, threshold gates, and indecomposable hypergraphs. J. Comb. Theory, Ser. A, 79(1):133-160, 1997.\n\nTriangular factorization and inversion by fast matrix multiplication. James R Bunch, John E Hopcroft, Math. Comp. 28James R. Bunch and John E. Hopcroft. Triangular factorization and inversion by fast matrix multiplication. Math. Comp., 28:231-236, 1974.\n\nRademacher chaos, random Eulerian graphs and the sparse Johnson-Lindenstrauss transform. Vladimir Braverman, Rafail Ostrovsky, Yuval Rabani, abs/1011.2590CoRRVladimir Braverman, Rafail Ostrovsky, and Yuval Rabani. Rademacher chaos, random Eulerian graphs and the sparse Johnson-Lindenstrauss transform. CoRR, abs/1011.2590, 2010.\n\nLimit of the smallest eigenvalue of a large dimensional sample covariance matrix. Z D Bai, Y Q Yin, Ann. Probab. 213Z.D. Bai and Y.Q. Yin. Limit of the smallest eigenvalue of a large dimensional sample covariance matrix. Ann. Probab., 21(3):1275-1294, 1993.\n\nNumerical linear algebra in the streaming model. L Kenneth, David P Clarkson, Woodruff, Proceedings of the 41st ACM Symposium on Theory of Computing (STOC). the 41st ACM Symposium on Theory of Computing (STOC)Kenneth L. Clarkson and David P. Woodruff. Numerical linear algebra in the stream- ing model. In Proceedings of the 41st ACM Symposium on Theory of Computing (STOC), pages 205-214, 2009.\n\nLow rank approximation and regression in input sparsity time. CoRR, abs/1207.6365v2. L Kenneth, David P Clarkson, Woodruff, Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input sparsity time. CoRR, abs/1207.6365v2, 2012.\n\nFast linear algebra is stable. James Demmel, Ioana Dumitriu, Olga Holtz, Numer. Math. 1081James Demmel, Ioana Dumitriu, and Olga Holtz. Fast linear algebra is stable. Numer. Math., 108(1):59-91, October 2007.\n\nA sparse Johnson-Lindenstrauss transform. Anirban Dasgupta, Ravi Kumar, Tam\u00e1s Sarl\u00f3s, Proceedings of the 42nd ACM Symposium on Theory of Computing (STOC). the 42nd ACM Symposium on Theory of Computing (STOC)Anirban Dasgupta, Ravi Kumar, and Tam\u00e1s Sarl\u00f3s. A sparse Johnson-Lindenstrauss transform. In Proceedings of the 42nd ACM Symposium on Theory of Computing (STOC), pages 341-350, 2010.\n\nFast approximation of matrix coherence and statistical leverage. Petros Drineas, Malik Magdon-Ismail, Michael Mahoney, David Woodruff, Proceedings of the 29th International Conference on Machine Learning (ICML). the 29th International Conference on Machine Learning (ICML)Petros Drineas, Malik Magdon-Ismail, Michael Mahoney, and David Woodruff. Fast approximation of matrix coherence and statistical leverage. In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.\n\nMatchings, Matroids and Submodular Functions. J A Nicholas, Harvey, Massachusetts Institute of TechnologyPhD thesisNicholas J. A. Harvey. Matchings, Matroids and Submodular Functions. PhD thesis, Massachusetts Institute of Technology, 2008.\n\nFinding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev., Survey and Review section. Nathan Halko, Joel A Per-Gunnar Martinsson, Tropp, 53Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompo- sitions. SIAM Rev., Survey and Review section, 53(2):217-288, 2011.\n\nJohnson-lindenstrauss lemma for circulant matrices. Aicke Hinrichs, Jan Vyb\u00edral, Random Struct. Algorithms. 393Aicke Hinrichs and Jan Vyb\u00edral. Johnson-lindenstrauss lemma for circulant matrices. Random Struct. Algorithms, 39(3):391-398, 2011.\n\nA bound on tail probabilities for quadratic forms in independent random variables. David Lee Hanson, Farroll Tim Wright, Ann. Math. Statist. 423David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables. Ann. Math. Statist., 42(3):1079-1083, 1971.\n\nExtensions of Lipschitz mappings into a Hilbert space. B William, Joram Johnson, Lindenstrauss, Contemporary Mathematics. 26William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemporary Mathematics, 26:189-206, 1984.\n\nFelix Krahmer, Shahar Mendelson, Holger Rauhut, abs/1207.0235Suprema of chaos processes and the restricted isometry property. arXiv. Felix Krahmer, Shahar Mendelson, and Holger Rauhut. Suprema of chaos processes and the restricted isometry property. arXiv, abs/1207.0235, 2012.\n\nA derandomized sparse Johnson-Lindenstrauss transform. CoRR, abs/1006. M Daniel, Jelani Kane, Nelson, 3585Daniel M. Kane and Jelani Nelson. A derandomized sparse Johnson-Lindenstrauss transform. CoRR, abs/1006.3585, 2010.\n\nSparser Johnson-Lindenstrauss transforms. M Daniel, Jelani Kane, Nelson, SODA. Daniel M. Kane and Jelani Nelson. Sparser Johnson-Lindenstrauss transforms. In SODA, pages 1195-1206, 2012.\n\nFast moment estimation in data streams in optimal space. Daniel M Kane, Jelani Nelson, Ely Porat, David P Woodruff, Proceedings of the 43rd ACM Symposium on Theory of Computing (STOC). the 43rd ACM Symposium on Theory of Computing (STOC)Daniel M. Kane, Jelani Nelson, Ely Porat, and David P. Woodruff. Fast moment esti- mation in data streams in optimal space. In Proceedings of the 43rd ACM Symposium on Theory of Computing (STOC), pages 745-754, 2011.\n\nNew and improved Johnson-Lindenstrauss embeddings via the Restricted Isometry Property. Felix Krahmer, Rachel Ward, SIAM J. Math. Anal. 433Felix Krahmer and Rachel Ward. New and improved Johnson-Lindenstrauss embed- dings via the Restricted Isometry Property. SIAM J. Math. Anal., 43(3):1269-1281, 2011.\n\nRandomized algorithms for matrices and data. Foundations and Trends in Machine Learning. Michael W Mahoney, 3Michael W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends in Machine Learning, 3(2):123-224, 2011.\n\nA fast and efficient algorithm for low-rank approximation of a matrix. Nam H Nguyen, Thong T Do, Trac D Tran, Proceedings of the 41st ACM Symposium on Theory of Computing (STOC). the 41st ACM Symposium on Theory of Computing (STOC)Nam H. Nguyen, Thong T. Do, and Trac D. Tran. A fast and efficient algorithm for low-rank approximation of a matrix. In Proceedings of the 41st ACM Symposium on Theory of Computing (STOC), pages 215-224, 2009.\n\nSparsity lower bounds for dimensionality-reducing maps. Jelani Nelson, Huy L Nguy\u1ebdn, ManuscriptJelani Nelson and Huy L. Nguy\u1ebdn. Sparsity lower bounds for dimensionality-reducing maps. Manuscript, 2012.\n\nEdge-disjoint spanning trees of finite graphs. Crispin St, John Alvah, Nash-Williams, J. London Math. Soc. 36Crispin St. John Alvah Nash-Williams. Edge-disjoint spanning trees of finite graphs. J. London Math. Soc., 36:445-450, 1961.\n\nImproved approximation algorithms for large matrices via random projections. Tam\u00e1s Sarl\u00f3s, Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS). the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)Tam\u00e1s Sarl\u00f3s. Improved approximation algorithms for large matrices via random projections. In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 143-152, 2006.\n\nUnit\u00e4re transformationen gro\u00dfer matrizen. Arnold Sch\u00f6nhage, Numer. Math. 20Arnold Sch\u00f6nhage. Unit\u00e4re transformationen gro\u00dfer matrizen. Numer. Math., 20:409- 417, 1973.\n\nTopics in random matrix theory. Terence Tao, Graduate Studies in Mathematics. 132American Mathematical SocietyTerence Tao. Topics in random matrix theory, volume 132 of Graduate Studies in Mathematics. American Mathematical Society, 2012.\n\nImproved analysis of the subsampled randomized Hadamard transform. Joel A Tropp, Adv. Adapt. Data Anal., Special Issue on Sparse Representation of Data and Images. 31-2Joel A. Tropp. Improved analysis of the subsampled randomized Hadamard trans- form. Adv. Adapt. Data Anal., Special Issue on Sparse Representation of Data and Images, 3(1-2):115-126, 2011.\n\nOn the problem of decomposing a graph into n connected factors. William Thomas Tutte, J. London Math. Soc. 142William Thomas Tutte. On the problem of decomposing a graph into n connected factors. J. London Math. Soc., 142:221-230, 1961.\n\nTabulation-based 5-independent hashing with applications to linear probing and second moment estimation. Mikkel Thorup, Yin Zhang, SIAM J. Comput. 412Mikkel Thorup and Yin Zhang. Tabulation-based 5-independent hashing with ap- plications to linear probing and second moment estimation. SIAM J. Comput., 41(2):293-331, 2012.\n\nIntroduction to the non-asymptotic analysis of random matrices. Roman Vershynin, Compressed Sensing. Y. Eldar and G. KutyniokCambridge University PressRoman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and G. Kutyniok, editors, Compressed Sensing, Theory and Applications, chapter 5, pages 210-268. Cambridge University Press, 2012.\n\nA variant of the Johnson-Lindenstrauss lemma for circulant matrices. Jan Vyb\u00edral, J. Funct. Anal. 2604Jan Vyb\u00edral. A variant of the Johnson-Lindenstrauss lemma for circulant matrices. J. Funct. Anal., 260(4):1096-1105, 2011.\n\nMultiplying matrices faster than Coppersmith-Winograd. Virginia Vassilevska, Williams , STOC. Virginia Vassilevska Williams. Multiplying matrices faster than Coppersmith- Winograd. In STOC, pages 887-898, 2012.\n\nFast matrix rank algorithms and applications. Tsz Ho Yee Cheung, Lap Chi Chiu Kwok, Lau, Proceedings of the 44th Symposium on Theory of Computing (STOC). the 44th Symposium on Theory of Computing (STOC)Ho yee Cheung, Tsz Chiu Kwok, and Lap Chi Lau. Fast matrix rank algorithms and applications. In Proceedings of the 44th Symposium on Theory of Computing (STOC), pages 549-562, 2012.\n\nLargescale parallel collaborative filtering for the netflix prize. Yunhong Zhou, Dennis M Wilkinson, Robert Schreiber, Rong Pan, Proceedings of the 4th International Conference on Algorithmic Aspects in Information and Management (AAIM). the 4th International Conference on Algorithmic Aspects in Information and Management (AAIM)Yunhong Zhou, Dennis M. Wilkinson, Robert Schreiber, and Rong Pan. Large- scale parallel collaborative filtering for the netflix prize. In Proceedings of the 4th International Conference on Algorithmic Aspects in Information and Management (AAIM), pages 337-348, 2008.\n",
            "annotations": {
                "author": "[{\"end\":121,\"start\":107},{\"end\":135,\"start\":122}]",
                "publisher": null,
                "author_last_name": "[{\"end\":120,\"start\":114},{\"end\":134,\"start\":128}]",
                "author_first_name": "[{\"end\":113,\"start\":107},{\"end\":125,\"start\":122},{\"end\":127,\"start\":126}]",
                "author_affiliation": null,
                "title": "[{\"end\":82,\"start\":1},{\"end\":217,\"start\":136}]",
                "venue": null,
                "abstract": "[{\"end\":2430,\"start\":241}]",
                "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2852,\"start\":2846},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3097,\"start\":3092},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3103,\"start\":3097},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3180,\"start\":3173},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3186,\"start\":3180},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3192,\"start\":3186},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3197,\"start\":3192},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3202,\"start\":3197},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3303,\"start\":3296},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4070,\"start\":4064},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4075,\"start\":4070},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4083,\"start\":4075},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4089,\"start\":4083},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4095,\"start\":4089},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4101,\"start\":4095},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4156,\"start\":4149},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4162,\"start\":4156},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4818,\"start\":4811},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6136,\"start\":6129},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6284,\"start\":6277},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6435,\"start\":6428},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6506,\"start\":6500},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7264,\"start\":7256},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7316,\"start\":7310},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8038,\"start\":8032},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8407,\"start\":8399},{\"end\":8417,\"start\":8407},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9486,\"start\":9480},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9635,\"start\":9629},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9788,\"start\":9782},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9860,\"start\":9854},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10897,\"start\":10891},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12035,\"start\":12029},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12400,\"start\":12394},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12693,\"start\":12687},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12803,\"start\":12797},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12808,\"start\":12803},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12853,\"start\":12847},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13130,\"start\":13124},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13319,\"start\":13313},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14397,\"start\":14391},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15076,\"start\":15070},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18435,\"start\":18428},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18538,\"start\":18532},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19215,\"start\":19209},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":45133,\"start\":45125},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45293,\"start\":45287},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":45343,\"start\":45336},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":45796,\"start\":45789},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":45953,\"start\":45947},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":46904,\"start\":46898},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46990,\"start\":46981},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":47256,\"start\":47250},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":47342,\"start\":47333},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":47592,\"start\":47584},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":47835,\"start\":47826},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":48528,\"start\":48522},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":48533,\"start\":48528},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49004,\"start\":48998},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":49010,\"start\":49004},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":49569,\"start\":49562},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":50461,\"start\":50455},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":50819,\"start\":50813},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":50921,\"start\":50915},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":51541,\"start\":51535},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":52245,\"start\":52239},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":52515,\"start\":52509}]",
                "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51796,\"start\":51581},{\"attributes\":{\"id\":\"fig_1\"},\"end\":52168,\"start\":51797}]",
                "paragraph": "[{\"end\":3203,\"start\":2446},{\"end\":4192,\"start\":3205},{\"end\":4378,\"start\":4194},{\"end\":4653,\"start\":4380},{\"end\":4683,\"start\":4655},{\"end\":4778,\"start\":4685},{\"end\":5901,\"start\":4780},{\"end\":6678,\"start\":5903},{\"end\":7265,\"start\":6680},{\"end\":7966,\"start\":7267},{\"end\":9083,\"start\":7968},{\"end\":9406,\"start\":9085},{\"end\":9879,\"start\":9408},{\"end\":10252,\"start\":10085},{\"end\":10294,\"start\":10254},{\"end\":11019,\"start\":10450},{\"end\":11272,\"start\":11053},{\"end\":11647,\"start\":11274},{\"end\":11852,\"start\":11765},{\"end\":12774,\"start\":11982},{\"end\":13335,\"start\":12776},{\"end\":14127,\"start\":13337},{\"end\":14670,\"start\":14205},{\"end\":14763,\"start\":14698},{\"end\":15614,\"start\":14874},{\"end\":16306,\"start\":15831},{\"end\":16390,\"start\":16387},{\"end\":17247,\"start\":16392},{\"end\":18326,\"start\":17368},{\"end\":18884,\"start\":18328},{\"end\":19287,\"start\":18897},{\"end\":19486,\"start\":19289},{\"end\":19942,\"start\":19544},{\"end\":20120,\"start\":20052},{\"end\":20642,\"start\":20511},{\"end\":20752,\"start\":20734},{\"end\":20829,\"start\":20754},{\"end\":21087,\"start\":20968},{\"end\":21093,\"start\":21089},{\"end\":21296,\"start\":21172},{\"end\":21887,\"start\":21695},{\"end\":22130,\"start\":21942},{\"end\":22310,\"start\":22256},{\"end\":22730,\"start\":22597},{\"end\":22819,\"start\":22732},{\"end\":23010,\"start\":22821},{\"end\":23297,\"start\":23012},{\"end\":23737,\"start\":23424},{\"end\":23950,\"start\":23739},{\"end\":23981,\"start\":23952},{\"end\":24018,\"start\":23983},{\"end\":24151,\"start\":24061},{\"end\":24225,\"start\":24153},{\"end\":24410,\"start\":24368},{\"end\":26849,\"start\":24568},{\"end\":28839,\"start\":26851},{\"end\":29601,\"start\":29084},{\"end\":29953,\"start\":29635},{\"end\":30102,\"start\":30004},{\"end\":30291,\"start\":30104},{\"end\":30428,\"start\":30343},{\"end\":30879,\"start\":30543},{\"end\":31209,\"start\":31020},{\"end\":32463,\"start\":31694},{\"end\":32951,\"start\":32465},{\"end\":33342,\"start\":32953},{\"end\":33404,\"start\":33344},{\"end\":33528,\"start\":33480},{\"end\":34171,\"start\":33903},{\"end\":34603,\"start\":34233},{\"end\":35419,\"start\":34676},{\"end\":35688,\"start\":35421},{\"end\":37247,\"start\":35721},{\"end\":37921,\"start\":37480},{\"end\":39115,\"start\":37923},{\"end\":41123,\"start\":39687},{\"end\":41316,\"start\":41125},{\"end\":42922,\"start\":41858},{\"end\":43452,\"start\":43241},{\"end\":44450,\"start\":43499},{\"end\":44739,\"start\":44452},{\"end\":45134,\"start\":44741},{\"end\":46026,\"start\":45151},{\"end\":46440,\"start\":46028},{\"end\":47052,\"start\":46472},{\"end\":47235,\"start\":47054},{\"end\":48680,\"start\":47237},{\"end\":49051,\"start\":48709},{\"end\":49207,\"start\":49053},{\"end\":49766,\"start\":49218},{\"end\":49880,\"start\":49768},{\"end\":50099,\"start\":49882},{\"end\":50555,\"start\":50126},{\"end\":50711,\"start\":50557},{\"end\":51042,\"start\":50762},{\"end\":51287,\"start\":51044},{\"end\":51481,\"start\":51314},{\"end\":51580,\"start\":51483}]",
                "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10084,\"start\":9880},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10449,\"start\":10295},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11764,\"start\":11682},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11947,\"start\":11853},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14204,\"start\":14128},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14697,\"start\":14671},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14873,\"start\":14764},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15830,\"start\":15615},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16386,\"start\":16307},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17367,\"start\":17248},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19543,\"start\":19487},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19972,\"start\":19943},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20051,\"start\":19972},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20510,\"start\":20121},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20733,\"start\":20643},{\"attributes\":{\"id\":\"formula_15\"},\"end\":20967,\"start\":20830},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21171,\"start\":21094},{\"attributes\":{\"id\":\"formula_17\"},\"end\":21694,\"start\":21297},{\"attributes\":{\"id\":\"formula_18\"},\"end\":21941,\"start\":21888},{\"attributes\":{\"id\":\"formula_19\"},\"end\":22255,\"start\":22131},{\"attributes\":{\"id\":\"formula_20\"},\"end\":22596,\"start\":22311},{\"attributes\":{\"id\":\"formula_21\"},\"end\":23423,\"start\":23298},{\"attributes\":{\"id\":\"formula_22\"},\"end\":24060,\"start\":24019},{\"attributes\":{\"id\":\"formula_23\"},\"end\":24367,\"start\":24226},{\"attributes\":{\"id\":\"formula_24\"},\"end\":24567,\"start\":24411},{\"attributes\":{\"id\":\"formula_25\"},\"end\":29083,\"start\":28840},{\"attributes\":{\"id\":\"formula_26\"},\"end\":29634,\"start\":29602},{\"attributes\":{\"id\":\"formula_27\"},\"end\":30003,\"start\":29954},{\"attributes\":{\"id\":\"formula_28\"},\"end\":30342,\"start\":30292},{\"attributes\":{\"id\":\"formula_29\"},\"end\":30542,\"start\":30429},{\"attributes\":{\"id\":\"formula_30\"},\"end\":31019,\"start\":30880},{\"attributes\":{\"id\":\"formula_31\"},\"end\":31693,\"start\":31210},{\"attributes\":{\"id\":\"formula_32\"},\"end\":33479,\"start\":33405},{\"attributes\":{\"id\":\"formula_33\"},\"end\":33902,\"start\":33529},{\"attributes\":{\"id\":\"formula_34\"},\"end\":34232,\"start\":34172},{\"attributes\":{\"id\":\"formula_35\"},\"end\":34675,\"start\":34604},{\"attributes\":{\"id\":\"formula_36\"},\"end\":35720,\"start\":35689},{\"attributes\":{\"id\":\"formula_37\"},\"end\":37479,\"start\":37248},{\"attributes\":{\"id\":\"formula_38\"},\"end\":39686,\"start\":39116},{\"attributes\":{\"id\":\"formula_39\"},\"end\":41857,\"start\":41317},{\"attributes\":{\"id\":\"formula_40\"},\"end\":43240,\"start\":42923},{\"attributes\":{\"id\":\"formula_41\"},\"end\":43498,\"start\":43453},{\"attributes\":{\"id\":\"formula_42\"},\"end\":50761,\"start\":50712},{\"attributes\":{\"id\":\"formula_43\"},\"end\":51313,\"start\":51288}]",
                "table_ref": null,
                "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2444,\"start\":2432},{\"attributes\":{\"n\":\"1.1\"},\"end\":11051,\"start\":11022},{\"end\":11681,\"start\":11650},{\"attributes\":{\"n\":\"1.2\"},\"end\":11980,\"start\":11949},{\"attributes\":{\"n\":\"2\"},\"end\":18895,\"start\":18887},{\"attributes\":{\"n\":\"3\"},\"end\":45149,\"start\":45137},{\"attributes\":{\"n\":\"3.1\"},\"end\":46470,\"start\":46443},{\"attributes\":{\"n\":\"3.2\"},\"end\":48707,\"start\":48683},{\"end\":49216,\"start\":49210},{\"attributes\":{\"n\":\"3.3\"},\"end\":50124,\"start\":50102},{\"end\":51592,\"start\":51582}]",
                "table": null,
                "figure_caption": "[{\"end\":51796,\"start\":51595},{\"end\":52168,\"start\":51799}]",
                "figure_ref": "[{\"end\":10117,\"start\":10109},{\"end\":11227,\"start\":11219},{\"end\":37707,\"start\":37703}]",
                "bib_author_first_name": "[{\"end\":53213,\"start\":53210},{\"end\":53228,\"start\":53221},{\"end\":53492,\"start\":53484},{\"end\":53746,\"start\":53743},{\"end\":53757,\"start\":53754},{\"end\":54003,\"start\":54000},{\"end\":54014,\"start\":54011},{\"end\":54460,\"start\":54456},{\"end\":54468,\"start\":54467},{\"end\":54741,\"start\":54736},{\"end\":54743,\"start\":54742},{\"end\":54755,\"start\":54751},{\"end\":54757,\"start\":54756},{\"end\":55018,\"start\":55010},{\"end\":55036,\"start\":55030},{\"end\":55053,\"start\":55048},{\"end\":55335,\"start\":55334},{\"end\":55337,\"start\":55336},{\"end\":55344,\"start\":55343},{\"end\":55346,\"start\":55345},{\"end\":55561,\"start\":55560},{\"end\":55576,\"start\":55571},{\"end\":55578,\"start\":55577},{\"end\":55994,\"start\":55993},{\"end\":56009,\"start\":56004},{\"end\":56011,\"start\":56010},{\"end\":56203,\"start\":56198},{\"end\":56217,\"start\":56212},{\"end\":56232,\"start\":56228},{\"end\":56426,\"start\":56419},{\"end\":56441,\"start\":56437},{\"end\":56454,\"start\":56449},{\"end\":56839,\"start\":56833},{\"end\":56854,\"start\":56849},{\"end\":56877,\"start\":56870},{\"end\":56892,\"start\":56887},{\"end\":57313,\"start\":57312},{\"end\":57315,\"start\":57314},{\"end\":57664,\"start\":57658},{\"end\":57676,\"start\":57672},{\"end\":57678,\"start\":57677},{\"end\":57998,\"start\":57993},{\"end\":58012,\"start\":58009},{\"end\":58273,\"start\":58268},{\"end\":58277,\"start\":58274},{\"end\":58297,\"start\":58286},{\"end\":58554,\"start\":58553},{\"end\":58569,\"start\":58564},{\"end\":58771,\"start\":58766},{\"end\":58787,\"start\":58781},{\"end\":58805,\"start\":58799},{\"end\":59117,\"start\":59116},{\"end\":59132,\"start\":59126},{\"end\":59311,\"start\":59310},{\"end\":59326,\"start\":59320},{\"end\":59519,\"start\":59513},{\"end\":59521,\"start\":59520},{\"end\":59534,\"start\":59528},{\"end\":59546,\"start\":59543},{\"end\":59559,\"start\":59554},{\"end\":59561,\"start\":59560},{\"end\":60004,\"start\":59999},{\"end\":60020,\"start\":60014},{\"end\":60312,\"start\":60305},{\"end\":60314,\"start\":60313},{\"end\":60529,\"start\":60526},{\"end\":60531,\"start\":60530},{\"end\":60545,\"start\":60540},{\"end\":60547,\"start\":60546},{\"end\":60556,\"start\":60552},{\"end\":60558,\"start\":60557},{\"end\":60959,\"start\":60953},{\"end\":60971,\"start\":60968},{\"end\":60973,\"start\":60972},{\"end\":61154,\"start\":61147},{\"end\":61417,\"start\":61412},{\"end\":61840,\"start\":61834},{\"end\":62000,\"start\":61993},{\"end\":62272,\"start\":62268},{\"end\":62274,\"start\":62273},{\"end\":62630,\"start\":62623},{\"end\":62908,\"start\":62902},{\"end\":62920,\"start\":62917},{\"end\":63191,\"start\":63186},{\"end\":63570,\"start\":63567},{\"end\":63787,\"start\":63779},{\"end\":63809,\"start\":63801},{\"end\":63985,\"start\":63982},{\"end\":64008,\"start\":64001},{\"end\":64395,\"start\":64388},{\"end\":64408,\"start\":64402},{\"end\":64410,\"start\":64409},{\"end\":64428,\"start\":64422},{\"end\":64444,\"start\":64440}]",
                "bib_author_last_name": "[{\"end\":53219,\"start\":53214},{\"end\":53237,\"start\":53229},{\"end\":53503,\"start\":53493},{\"end\":53752,\"start\":53747},{\"end\":53765,\"start\":53758},{\"end\":54009,\"start\":54004},{\"end\":54022,\"start\":54015},{\"end\":54465,\"start\":54461},{\"end\":54472,\"start\":54469},{\"end\":54476,\"start\":54474},{\"end\":54749,\"start\":54744},{\"end\":54766,\"start\":54758},{\"end\":55028,\"start\":55019},{\"end\":55046,\"start\":55037},{\"end\":55060,\"start\":55054},{\"end\":55341,\"start\":55338},{\"end\":55350,\"start\":55347},{\"end\":55569,\"start\":55562},{\"end\":55587,\"start\":55579},{\"end\":55597,\"start\":55589},{\"end\":56002,\"start\":55995},{\"end\":56020,\"start\":56012},{\"end\":56030,\"start\":56022},{\"end\":56210,\"start\":56204},{\"end\":56226,\"start\":56218},{\"end\":56238,\"start\":56233},{\"end\":56435,\"start\":56427},{\"end\":56447,\"start\":56442},{\"end\":56461,\"start\":56455},{\"end\":56847,\"start\":56840},{\"end\":56868,\"start\":56855},{\"end\":56885,\"start\":56878},{\"end\":56901,\"start\":56893},{\"end\":57324,\"start\":57316},{\"end\":57332,\"start\":57326},{\"end\":57670,\"start\":57665},{\"end\":57700,\"start\":57679},{\"end\":57707,\"start\":57702},{\"end\":58007,\"start\":57999},{\"end\":58020,\"start\":58013},{\"end\":58284,\"start\":58278},{\"end\":58304,\"start\":58298},{\"end\":58562,\"start\":58555},{\"end\":58577,\"start\":58570},{\"end\":58592,\"start\":58579},{\"end\":58779,\"start\":58772},{\"end\":58797,\"start\":58788},{\"end\":58812,\"start\":58806},{\"end\":59124,\"start\":59118},{\"end\":59137,\"start\":59133},{\"end\":59145,\"start\":59139},{\"end\":59318,\"start\":59312},{\"end\":59331,\"start\":59327},{\"end\":59339,\"start\":59333},{\"end\":59526,\"start\":59522},{\"end\":59541,\"start\":59535},{\"end\":59552,\"start\":59547},{\"end\":59570,\"start\":59562},{\"end\":60012,\"start\":60005},{\"end\":60025,\"start\":60021},{\"end\":60322,\"start\":60315},{\"end\":60538,\"start\":60532},{\"end\":60550,\"start\":60548},{\"end\":60563,\"start\":60559},{\"end\":60966,\"start\":60960},{\"end\":60980,\"start\":60974},{\"end\":61157,\"start\":61155},{\"end\":61169,\"start\":61159},{\"end\":61184,\"start\":61171},{\"end\":61424,\"start\":61418},{\"end\":61850,\"start\":61841},{\"end\":62004,\"start\":62001},{\"end\":62280,\"start\":62275},{\"end\":62643,\"start\":62631},{\"end\":62915,\"start\":62909},{\"end\":62926,\"start\":62921},{\"end\":63201,\"start\":63192},{\"end\":63578,\"start\":63571},{\"end\":63799,\"start\":63788},{\"end\":63999,\"start\":63986},{\"end\":64018,\"start\":64009},{\"end\":64023,\"start\":64020},{\"end\":64400,\"start\":64396},{\"end\":64420,\"start\":64411},{\"end\":64438,\"start\":64429},{\"end\":64448,\"start\":64445}]",
                "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17443637},\"end\":53403,\"start\":53134},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":18931062},\"end\":53673,\"start\":53405},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":313102},\"end\":53932,\"start\":53675},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1900},\"end\":54366,\"start\":53934},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2346827},\"end\":54664,\"start\":54368},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9768783},\"end\":54919,\"start\":54666},{\"attributes\":{\"doi\":\"abs/1011.2590\",\"id\":\"b6\"},\"end\":55250,\"start\":54921},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":120161816},\"end\":55509,\"start\":55252},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13988984},\"end\":55906,\"start\":55511},{\"attributes\":{\"id\":\"b9\"},\"end\":56165,\"start\":55908},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6057731},\"end\":56375,\"start\":56167},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13492200},\"end\":56766,\"start\":56377},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":129692},\"end\":57264,\"start\":56768},{\"attributes\":{\"id\":\"b13\"},\"end\":57506,\"start\":57266},{\"attributes\":{\"id\":\"b14\"},\"end\":57939,\"start\":57508},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1812057},\"end\":58183,\"start\":57941},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":123145182},\"end\":58496,\"start\":58185},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":117819162},\"end\":58764,\"start\":58498},{\"attributes\":{\"doi\":\"abs/1207.0235\",\"id\":\"b18\"},\"end\":59043,\"start\":58766},{\"attributes\":{\"id\":\"b19\"},\"end\":59266,\"start\":59045},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7821848},\"end\":59454,\"start\":59268},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2474485},\"end\":59909,\"start\":59456},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5692906},\"end\":60214,\"start\":59911},{\"attributes\":{\"id\":\"b23\"},\"end\":60453,\"start\":60216},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":12902805},\"end\":60895,\"start\":60455},{\"attributes\":{\"id\":\"b25\"},\"end\":61098,\"start\":60897},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":121193791},\"end\":61333,\"start\":61100},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1299951},\"end\":61790,\"start\":61335},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":123918447},\"end\":61959,\"start\":61792},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":118771550},\"end\":62199,\"start\":61961},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5783804},\"end\":62557,\"start\":62201},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":121586383},\"end\":62795,\"start\":62559},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2627459},\"end\":63120,\"start\":62797},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":133440},\"end\":63496,\"start\":63122},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14043932},\"end\":63722,\"start\":63498},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14350287},\"end\":63934,\"start\":63724},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5707170},\"end\":64319,\"start\":63936},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":8574004},\"end\":64919,\"start\":64321}]",
                "bib_title": "[{\"end\":53208,\"start\":53134},{\"end\":53482,\"start\":53405},{\"end\":53741,\"start\":53675},{\"end\":53998,\"start\":53934},{\"end\":54454,\"start\":54368},{\"end\":54734,\"start\":54666},{\"end\":55332,\"start\":55252},{\"end\":55558,\"start\":55511},{\"end\":56196,\"start\":56167},{\"end\":56417,\"start\":56377},{\"end\":56831,\"start\":56768},{\"end\":57991,\"start\":57941},{\"end\":58266,\"start\":58185},{\"end\":58551,\"start\":58498},{\"end\":59308,\"start\":59268},{\"end\":59511,\"start\":59456},{\"end\":59997,\"start\":59911},{\"end\":60524,\"start\":60455},{\"end\":61145,\"start\":61100},{\"end\":61410,\"start\":61335},{\"end\":61832,\"start\":61792},{\"end\":61991,\"start\":61961},{\"end\":62266,\"start\":62201},{\"end\":62621,\"start\":62559},{\"end\":62900,\"start\":62797},{\"end\":63184,\"start\":63122},{\"end\":63565,\"start\":63498},{\"end\":63777,\"start\":63724},{\"end\":63980,\"start\":63936},{\"end\":64386,\"start\":64321}]",
                "bib_author": "[{\"end\":53221,\"start\":53210},{\"end\":53239,\"start\":53221},{\"end\":53505,\"start\":53484},{\"end\":53754,\"start\":53743},{\"end\":53767,\"start\":53754},{\"end\":54011,\"start\":54000},{\"end\":54024,\"start\":54011},{\"end\":54467,\"start\":54456},{\"end\":54474,\"start\":54467},{\"end\":54478,\"start\":54474},{\"end\":54751,\"start\":54736},{\"end\":54768,\"start\":54751},{\"end\":55030,\"start\":55010},{\"end\":55048,\"start\":55030},{\"end\":55062,\"start\":55048},{\"end\":55343,\"start\":55334},{\"end\":55352,\"start\":55343},{\"end\":55571,\"start\":55560},{\"end\":55589,\"start\":55571},{\"end\":55599,\"start\":55589},{\"end\":56004,\"start\":55993},{\"end\":56022,\"start\":56004},{\"end\":56032,\"start\":56022},{\"end\":56212,\"start\":56198},{\"end\":56228,\"start\":56212},{\"end\":56240,\"start\":56228},{\"end\":56437,\"start\":56419},{\"end\":56449,\"start\":56437},{\"end\":56463,\"start\":56449},{\"end\":56849,\"start\":56833},{\"end\":56870,\"start\":56849},{\"end\":56887,\"start\":56870},{\"end\":56903,\"start\":56887},{\"end\":57326,\"start\":57312},{\"end\":57334,\"start\":57326},{\"end\":57672,\"start\":57658},{\"end\":57702,\"start\":57672},{\"end\":57709,\"start\":57702},{\"end\":58009,\"start\":57993},{\"end\":58022,\"start\":58009},{\"end\":58286,\"start\":58268},{\"end\":58306,\"start\":58286},{\"end\":58564,\"start\":58553},{\"end\":58579,\"start\":58564},{\"end\":58594,\"start\":58579},{\"end\":58781,\"start\":58766},{\"end\":58799,\"start\":58781},{\"end\":58814,\"start\":58799},{\"end\":59126,\"start\":59116},{\"end\":59139,\"start\":59126},{\"end\":59147,\"start\":59139},{\"end\":59320,\"start\":59310},{\"end\":59333,\"start\":59320},{\"end\":59341,\"start\":59333},{\"end\":59528,\"start\":59513},{\"end\":59543,\"start\":59528},{\"end\":59554,\"start\":59543},{\"end\":59572,\"start\":59554},{\"end\":60014,\"start\":59999},{\"end\":60027,\"start\":60014},{\"end\":60324,\"start\":60305},{\"end\":60540,\"start\":60526},{\"end\":60552,\"start\":60540},{\"end\":60565,\"start\":60552},{\"end\":60968,\"start\":60953},{\"end\":60982,\"start\":60968},{\"end\":61159,\"start\":61147},{\"end\":61171,\"start\":61159},{\"end\":61186,\"start\":61171},{\"end\":61426,\"start\":61412},{\"end\":61852,\"start\":61834},{\"end\":62006,\"start\":61993},{\"end\":62282,\"start\":62268},{\"end\":62645,\"start\":62623},{\"end\":62917,\"start\":62902},{\"end\":62928,\"start\":62917},{\"end\":63203,\"start\":63186},{\"end\":63580,\"start\":63567},{\"end\":63801,\"start\":63779},{\"end\":63812,\"start\":63801},{\"end\":64001,\"start\":63982},{\"end\":64020,\"start\":64001},{\"end\":64025,\"start\":64020},{\"end\":64402,\"start\":64388},{\"end\":64422,\"start\":64402},{\"end\":64440,\"start\":64422},{\"end\":64450,\"start\":64440}]",
                "bib_venue": "[{\"end\":54169,\"start\":54105},{\"end\":55720,\"start\":55668},{\"end\":56584,\"start\":56532},{\"end\":57040,\"start\":56980},{\"end\":59693,\"start\":59641},{\"end\":60686,\"start\":60634},{\"end\":61587,\"start\":61515},{\"end\":64138,\"start\":64090},{\"end\":64651,\"start\":64559},{\"end\":53253,\"start\":53239},{\"end\":53525,\"start\":53505},{\"end\":53788,\"start\":53767},{\"end\":54103,\"start\":54024},{\"end\":54501,\"start\":54478},{\"end\":54778,\"start\":54768},{\"end\":55008,\"start\":54921},{\"end\":55363,\"start\":55352},{\"end\":55666,\"start\":55599},{\"end\":55991,\"start\":55908},{\"end\":56251,\"start\":56240},{\"end\":56530,\"start\":56463},{\"end\":56978,\"start\":56903},{\"end\":57310,\"start\":57266},{\"end\":57656,\"start\":57508},{\"end\":58047,\"start\":58022},{\"end\":58324,\"start\":58306},{\"end\":58618,\"start\":58594},{\"end\":58897,\"start\":58827},{\"end\":59114,\"start\":59045},{\"end\":59345,\"start\":59341},{\"end\":59639,\"start\":59572},{\"end\":60045,\"start\":60027},{\"end\":60303,\"start\":60216},{\"end\":60632,\"start\":60565},{\"end\":60951,\"start\":60897},{\"end\":61205,\"start\":61186},{\"end\":61513,\"start\":61426},{\"end\":61863,\"start\":61852},{\"end\":62037,\"start\":62006},{\"end\":62363,\"start\":62282},{\"end\":62664,\"start\":62645},{\"end\":62942,\"start\":62928},{\"end\":63221,\"start\":63203},{\"end\":63594,\"start\":63580},{\"end\":63816,\"start\":63812},{\"end\":64088,\"start\":64025},{\"end\":64557,\"start\":64450}]"
            }
        }
    },
    "year": 2023,
    "month": 12,
    "day": 17
}