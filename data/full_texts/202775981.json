{"id": 202775981, "updated": "2023-10-06 22:24:51.353", "metadata": {"title": "CondConv: Conditionally Parameterized Convolutions for Efficient Inference", "authors": "[{\"first\":\"Brandon\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Gabriel\",\"last\":\"Bender\",\"middle\":[]},{\"first\":\"Quoc\",\"last\":\"Le\",\"middle\":[\"V.\"]},{\"first\":\"Jiquan\",\"last\":\"Ngiam\",\"middle\":[]}]", "venue": "NeurIPS 2019", "journal": null, "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks. On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-of-the-art performance of 78.3% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorflow layer and CondConv-EfficientNet models are available at: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1904.04971", "mag": "2982101047", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/YangBLN19", "doi": null}}, "content": {"source": {"pdf_hash": "200f95ff278760dcb4c1b22293898f58e8c6dd52", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.04971v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2eb60c6f7269e19167ae8ec4bbccc28aa40afda3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/200f95ff278760dcb4c1b22293898f58e8c6dd52.txt", "contents": "\nCondConv: Conditionally Parameterized Convolutions for Efficient Inference\n\n\nBrandon Yang bcyang@google.com \nGoogle Brain \nGabriel Bender gbender@google.com \nGoogle Brain \nQuoc V Le Google Brain \nJiquan Ngiam jngiam@google.com \nGoogle Brain \nCondConv: Conditionally Parameterized Convolutions for Efficient Inference\n\nConvolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks. On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-ofthe-art performance of 78.3% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorflow layer and CondConv-EfficientNet models are available at: https://github.com/tensorflow/tpu/tree/master/ models/official/efficientnet/condconv.IntroductionDeep convolutional neural networks (CNNs) have achieved state-of-the-art performance on many tasks in computer vision[23,22]. Improvements in performance have largely come from increasing model size and capacity to scale to larger and larger datasets[29,17,33]. However, current approaches to increasing model capacity are computationally expensive. Deploying the best-performing models for inference can consume significant datacenter capacity[19]and are often not feasible for applications with strict latency constraints.One fundamental assumption in the design of convolutional layers is that the same convolutional kernels are applied to every example in a dataset. To increase the capacity of a model, model developers usually add more convolutional layers or increase the size of existing convolutions (kernel height/width, number of input/output channels). In either case, the computational cost of additional capacity increases proportionally to the size of the input to the convolution, which can be large.Due to this assumption and focus on mobile deployment, current computationally efficient models have very few parameters[15,36,41]. However, there is a growing class of computer vision applications that are not constrained by parameter count, but have strict latency requirements at inference, such as real-time server-side video processing and perception for self-driving cars. In this paper, we aim to design models to better serve these applications.\n\n(a) CondConv: (\u03b11W1 + . . . + \u03b1nWn) * x (b) Mixture of Experts: \u03b11(W1 * x)+. . .+\u03b1n(Wn * x) Figure 1: (a) Our CondConv layer architecture with n = 3 kernels vs. (b) a mixture of experts approach. By parameterizing the convolutional kernel conditionally on the input, CondConv is mathematically equivalent to the mixture of experts approach, but requires only 1 convolution.\n\nWe propose conditionally parameterized convolutions (CondConv), which challenge the paradigm of static convolutional kernels by computing convolutional kernels as a function of the input. In particular, we parameterize the convolutional kernels in a CondConv layer as a linear combination of n experts (\u03b1 1 W 1 + . . . + \u03b1 n W n ) * x, where \u03b1 1 , . . . , \u03b1 n are functions of the input learned through gradient descent. To efficiently increase the capacity of a CondConv layer, model developers can increase the number of experts. This is much more computationally efficient than increasing the size of the convolutional kernel itself, because the convolutional kernel is applied at many different positions within the input, while the experts are combined only once per input. This allows model developers to increase model capacity and performance while maintaining efficient inference.\n\nCondConv can be used as a drop-in replacement for existing convolutional layers in CNN architectures. We demonstrate that replacing convolutional layers with CondConv improves model capacity and performance on several CNN architectures on ImageNet classification and COCO object detection, while maintaining efficient inference. In our analysis, we find that CondConv layers learn semantically meaningful relationships across examples to compute the conditional convolutional kernels.\n\n\nRelated Work\n\nConditional computation. Similar to CondConv, conditional computation aims to increase model capacity without a proportional increase in computation cost. In conditional computation models, this is achieved by activating only a portion of the entire network for each example [3,8,5,2]. However, conditional computation models are often challenging to train, since they require learning discrete routing decisions from individual examples to different experts. Unlike these approaches, CondConv does not require discrete routing of examples, so can be easily optimized with gradient descent.\n\nOne approach to conditional computation uses reinforcement learning or evolutionary methods to learn discrete routing functions [34,30,25,9]. BlockDrop [47] and SkipNet [45] use reinforcement learning to learn the subset of blocks needed to process a given input. Another approach uses unsupervised clustering methods to partition examples into sub-networks. Gross et al. [12] use a two stage training and clustering pipeline to train a hard mixture of experts model. Mullapudi et al. [31] use clusters as labels to train a routing function between branches in a deep CNN model. Finally, Shazeer et al. [37] proposed the sparsely-gated mixture-of-experts layer, which which achieves significant success on large language modeling using noisy top-k gating.\n\nPrior work in computation demonstrates the potential of designing large models that process different sets of examples with different sub-networks. Our work on CondConv pushes the boundaries of this paradigm, by enabling each individual example to be processed with different weights.\n\nWeight generating networks. Ha et al. [13] propose the use of a small network to generate weights for a larger network. Unlike CondConv, for CNNs, these weights are the same for every example in the dataset. This enables greater weight-sharing, which achieves lower parameter count but worse performance than the original network. In neural machine translation, Platanios et al. [32] generate weights to translate between different language pairs, but use the same weights for every example within each language pair.\n\nMulti-branch convolutional networks. Multi-branch architectures like Inception [40] and ResNext [48] have shown success on a variety of computer vision tasks. In these architectures, a layer consists of multiple convolutional branches, which are aggregated to compute the final output. A CondConv layer is mathematically equivalent to a multi-branch convolutional layer where each branch is a single convolution and outputs are aggregated by a weighted sum, but only requires the computation of one convolution.\n\nExample dependent activation scaling. Some recent work proposes to adapt the activations of neural networks conditionally on the input. Squeeze-and-Excitation networks [16] learn to scale the activations of every layer output. GaterNet [4] uses a separate network to select a binary mask for filters for a larger backbone network. Attention-based methods [28,1,44] scale previous layer inputs based on learned attention weights. Scaling activations has similar motivations as CondConv, but is restricted to modulating activations in the base network.\n\nInput-dependent convolutional layers. In language modeling, Wu et al. [46] use input-dependent convolutional kernels as a form of local attention. In vision, Brabandere et al. [18] generate small input-dependent convolutional filters to transform images for next frame and stereo prediction. Rather than learning input-dependent weights, Dai et al. [7] propose to learn different convolutional offsets for each example. Finally, in recent work, SplineNets [20] apply input-dependent convolutional weights, modeled as 1-dimensional B-splines, to implement continuous neural decision graphs.\n\n\nConditionally Parameterized Convolutions\n\nIn a regular convolutional layer, the same convolutional kernel is used for all input examples. In a CondConv layer, the convolutional kernel is computed as a function of the input example (Fig 1a). Specifically, we parameterize the convolutional kernels in CondConv by:\nOutput(x) = \u03c3((\u03b1 1 \u00b7 W 1 + . . . + \u03b1 n \u00b7 W n ) * x)\nwhere each \u03b1 i = r i (x) is an example-dependent scalar weight computed using a routing function with learned parameters, n is the number of experts, and \u03c3 is an activation function. When we adapt a convolutional layer to use CondConv, each kernel W i has the same dimensions as the kernel in the original convolution.\n\nWe typically increase the capacity of a regular convolutional layer by increasing the kernel height/width or number of input/output channels. However, each additional parameter in a convolution requires additional multiply-adds proportional to the number of pixels in the input feature map, which can be large. In a CondConv layer, we compute a convolutional kernel for each example as a linear combination of n experts before applying the convolution. Crucially, each convolutional kernel only needs to be computed once but is applied at many different positions in the input image. This means that by increasing n, we can increase the capacity of the network with only a small increase in inference cost; each additional parameter requires only 1 additional multiply-add.\n\nA CondConv layer is mathematically equivalent to a more expensive linear mixture of experts formulation, where each expert corresponds to a static convolution (Fig 1b):\n\u03c3((\u03b1 1 \u00b7 W 1 + . . . + \u03b1 n \u00b7 W n ) * x) = \u03c3(\u03b1 1 \u00b7 (W 1 * x) + . . . + \u03b1 n \u00b7 (W n * x))\nThus, CondConv has the same capacity as a linear mixture of experts formulation with n experts, but is computationally efficient since it requires computing only one expensive convolution. This formulation gives insight into the properties of CondConv and relates it to prior work on conditional computation and mixture of experts. The per-example routing function is crucial to CondConv performance: if the learned routing function is constant for all examples, a CondConv layer has the same capacity as a static convolutional layer.\n\nWe wish to design a per-example routing function that is computationally efficient, able to meaningfully differentiate between input examples, and is easily interpretable. We compute the exampledependent routing weights \u03b1 i = r i (x) from the layer input in three steps: global average pooling, fully-connected layer, Sigmoid activation. where R is a matrix of learned routing weights mapping the pooled inputs to n expert weights. A normal convolution operation operates only over local receptive fields, so our routing function allows adaptation of local operations using global context.\nr(x) = Sigmoid(GlobalAveragePool(x) R)\nThe CondConv layer can be used in place of any convolutional layer in a network. The same approach can easily be extended to other linear functions like those in depth-wise convolutions and fully-connected layers.\n\n\nExperiments\n\nWe evaluate CondConv on ImageNet classification and COCO object detection by scaling up the MobileNetV1 [15], MobileNetV2 [36], ResNet-50 [14], MnasNet [41], and EfficientNet [42] architectures. In practice, we have two options to train CondConv models, which are mathematically equivalent. We can either first compute the kernel for each example and apply convolutions with a batch size of one ( Fig. 1a), or we can use the linear mixture of experts formulation ( Fig. 1b) to perform batch convolutions on each branch and sum the outputs. Current accelerators are optimized to train on large batch convolutions, and it is difficult to fully utilize them for small batch sizes. Thus, with small numbers of experts (<=4), we found it to be more efficient to train CondConv layers with the linear mixture of experts formulation and large batch convolutions, then use our efficient CondConv approach for inference. With larger numbers of experts (>4), training CondConv layers directly with batch size one is more efficient.\n\n\nImageNet Classification\n\nWe evaluate our approach on the ImageNet 2012 classification dataset [35]. The ImageNet dataset consists of 1.28 million training images and 50K validation images from 1000 classes. We train all models on the entire training set and compare the single-crop top-1 validation set accuracy with input image resolution 224x224. For MobileNetV1, MobileNetV2, and ResNet-50, we use the same training hyperparameters for all models on ImageNet, following [21], except we use BatchNorm momentum of 0.9 and disable exponential moving average on weights. For MnasNet [41] and EfficientNet [42], we use the same training hyperparameters as the original papers, with the batch size, learning rate, and training steps scaled appropriately for our hardware configuration. For fair comparison, we retrain all of our baseline models with the same hyperparameters and regularization search space as the CondConv models. We used accuracy on the validation set to determine early stopping. We measure performance as ImageNet top-1 accuracy relative to computational cost in multiply-adds (MADDs). For each baseline architecture, we evaluate CondConv by replacing convolutional layers with Cond-Conv layers, and increasing the number of experts per layer. We share routing weights between layers in a block (a residual block, inverted bottleneck block, or separable convolution). Additionally, for some models, we replace the fully-connected classification layer with a 1x1 CondConv layer. For the exact architectural details, refer to Appendix A. Our ablation experiments in Table 3 and Table 4 suggest CondConv improves performance across a wide range of layer and routing architectures.\n\nWe use two general regularization techniques for models with large capacity. First, we use Dropout [39] on the input to the fully-connected layer preceding the logits, with keep probability between 0.6 and 1.0. Second, we also add data augmentation using the AutoAugment [6] ImageNet policy and Mixup [49] with \u03b1 = 0.2. To address overfitting in the large ResNet models, we additionally introduce a new data augmentation technique for CondConv based on Shake-Shake [10] by randomly dropping out experts during training.\n\nOn MobileNetV1, we find that increasing the number of CondConv experts improves accuracy relative to inference cost compared to the performance frontier with static convolutional scaling techniques using the channel width and input size ( Figure 2). Moreover, we find that increasing the number of CondConv experts leads to monotonically increasing performance with sufficient regularization.\n\nWe further find that CondConv improves performance relative to inference cost on a wide range of architectures (Table 1). This includes architectures that take advantage of architecture search [42,41], Squeeze-and-Excitation [16], and large architectures with ordinary convolutions not optimized for inference time [14]. For more in depth comparisons, see Appendix A.\n\nOur CondConv-EfficientNet-B0 model achieves state-of-the-art performance of 78.3% accuracy with 413M multiply-adds, when compared to the MixNet frontier [43]. To directly compare our CondConv scaling approach to the compound scaling coefficient proposed by Tan et al. [42], we additionally scale the CondConv-EfficientNet-B0 model with a depth multiplier of 1.1x, which we call CondConv-EfficientNet-B0-depth. Our CondConv-EfficientNet-B0-depth model achieves 79.5% accuracy with only 614M multiply-adds. When trained with the same hyperparameters and regularization search space, the EfficientNet-B1 model, which is scaled from the EfficientNet-B0 model using the compound coefficient, achieves 79.2% accuracy with 700M multiply-adds. In this regime, CondConv scaling outperforms static convolutional scaling with the compound coefficient.\n\n\nCOCO Object Detection\n\nWe next evaluate the effectiveness of CondConv on a different task and dataset with the COCO object detection dataset [24]. Our experiments use the MobileNetV1 feature extractor and the Single Shot Detector [26] with 300x300 input resolution (SSD300).\n\nFollowing Howard et al. [15], we train on the combined COCO training and validation sets excluding 8,000 minival images, which we evaluate our networks on. We train our models using a batch size 2 Our re-implementation of the baseline models and our CondConv models use the same hyperparameters and regularization search space for fair comparison. For reference, published results for baselines are: MobileNetV1 (1.0x): 70.6% [15]. MobileNetV2 (1.0x): 72.0% [36]. MnasNet-A1: 75.2% [41]. ResNet-50: 76.4% [11]. EfficientNet-B0: 76.3% [42].   of 1024 for 20,000 steps. For the learning rate, we use linear warmup from 0.3 to 0.9 over 1,000 steps, followed by cosine decay [27] from 0.9. We use the data augmentation scheme proposed by Liu et al. [26]. We use the same convolutional feature layer dimensions, SSD hyperparameters, and training hyperparameters across all models. We measure performance as COCO minival mean average precision (mAP) relative to computational cost in multiply-adds (MADDs).\n\nWe use our CondConv-MobileNetV1 models with depth multipliers {0.50, 0.75, 1.0} as the feature extractors for object detection. We further replace the additional convolutional feature extractor layers in SSD with CondConv layers.\n\nCondConv with 8 experts improves object detection performance at all model sizes (  4 We choose this setup for ease of training and large effect of CondConv.  \n\n\nRouting function\n\nWe investigate different choices for the routing function in Table 3. The baseline model computes new routing weights for each layer. Single computes the routing weights only once at CondConv 7 (the 7th separable convolutional block), and uses the same routing weights in all subsequent layers. Partially-shared shares the routing weights between every other layer. Both the baseline model and Partially-shared significantly outperform Single, which suggests that routing at multiple depths in the network improve quality. Partially-shared performs slightly outperforms the baseline, suggesting that sharing routing functions among nearby layers can improve quality.\n\nWe then experiment with more complex routing functions, by introducing a hidden layer with ReLU activation after the global average pooling step. We vary the hidden layer size to be input_dim/8\n\nfor Hidden (small), input_dim for Hidden (medium), and input_dim \u00b7 8 for Hidden (large). Adding a non-linear hidden layer of appropriate size can slightly improve performance. Large hidden layer sizes are prone to over-fitting, even with the same number of experts.\n\nNext, we experiment with Hierarchical routing functions, by concatenating the routing weights of the previous layer to the output of the global average pooling layer in the routing function. This adds a dependency between CondConv routing weights, which we find is also prone to overfitting.\n\nFinally, we experiment with the Softmax activation function to compute routing weights. The baseline's Sigmoid significantly outperforms Softmax, which suggests that multiple experts are often useful for a single example.\n\n\nCondConv Layer Depth\n\nWe analyze the effect of CondConv layers at different depths in the CondConv-MobileNetV1 (0.25x) model (Table 4). We use CondConv layers in the begin layer, and all subsequent layers. We further perform ablation studies specific to the final fully-connected classification layer. We find CondConv layers improve performance when applied at every layer in the network. Additionally, we find that additionally applying CondConv before layer 7 in the network has only small effects on performance.  For image classification with the CondConv-MobileNetV1 (0.25x) model, CondConv in the final classification layer accounts for a significant fraction of the additional inference cost. Using a normal final classification layer results in smaller performance gains, but is more efficient.\n\n\nAnalysis\n\nIn this section, we aim to gain a better understanding of the learned kernels and routing functions in our CondConv-MobileNetV1 architecture. We study our CondConv-MobileNetV1 (0.50x) architecture with 32 experts per layer trained on ImageNet with Mixup and AutoAugment, which achieves 71.6% top-1 validation accuracy. We evaluate our CondConv-MobileNetV1 (0.5x) model on the 50,000\n\nImageNet validation examples, and compute the routing-weights at CondConv layers in the network.\n\nWe first study inter-class variation between the routing weights at different layers in the network. We visualize the average routing weight for four different classes (cliff, pug, goldfish, and plane, as suggested by Hu et al. [16] for semantic and appearance diversity) at three different depths in the network (Layer 12, Layer 26, and the final fully-connected layer). The distribution of the routing weights is very similar across classes at early layers in the network, and become more and more class specific at later layers ( Figure 3). This suggests an explanation for why replacing additional convolutional layers with CondConv layers near the input of the network does not significantly improve performance.\n\nWe next analyze the distribution of the routing weights of the final fully-connected layer in Figure  4. The routing weights follow a bi-modal distribution, with most experts receiving a routing weight close to 0 or 1. This suggests that the experts are sparsely activated, even without regularization, and further suggests the specialization of the experts.\n\nWe then study intra-class variation between the routing weights in the final CondConv layer ( Figure  5). Within one class, some kernels are activated with high weight and small variance for all examples. However, even within one class, there can be big variation in the routing weights between examples.\n\nFinally, to better understand experts in the final CondConv layer, we visualize top 10 classes with highest mean routing weight for four difference experts on the ImageNet validation set ( Figure 6). We show the exemplar image with highest routing weight within each class. CondConv layers learn to specialize in semantically and visually meaningful ways.\n\n\nConclusion\n\nIn this paper, we proposed conditionally parameterized convolutions (CondConv  [36]), but requires 585M multiply-adds.\n\nCondConv-MnasNet-A1. We replace the convolutional layers in the final 3 block groups of the baseline MnasNet-A1 architecture with CondConv layers. We share routing weights between convolutional layers in each inverted bottleneck block within a block group. The baseline MnasNet-A1 model achieves 74.9% accuracy with 312M multiply-adds. Our CondConv-MnasNet-A1 model achieves 76.2% accuracy with 329M multiply-adds. A larger model from the same search space using static convolutional layers, MnasNet-A2, achieves 75.6% accuracy with 340M multiply-adds [41].\n\nCondConv-ResNet-50. We replace the convolutional layers in the final 3 residual blocks and the final fully-connected classification layer of the baseline ResNet-50 architecture with CondConv layers. The baseline ResNet-50 model achieves 77.7% accuracy at 4096M multiply-adds. Our CondConv-ResNet-50 architecture achieves 78.6% accuracy at 4213 multiply-adds. With sufficient regularization, CondConv improves the performance of even large model architectures with ordinary convolutions that are not optimized for inference time.\n\nCondConv-EfficientNet-B0. We replace the convolutional layers in the final 3 block groups of the baseline EfficientNet-B0 architecture with CondConv layers. We share routing weights between convolutional layers in each inverted bottleneck block within a block group. The baseline EfficientNet-B0 model achieves 77.2% accuracy with 391M multiply-adds. Our CondConv-EfficientNet-B0 model achieves 78.3% accuracy with 413M multiply-adds.\n\nTo directly compare our CondConv scaling approach to the compound scaling coefficient proposed by Tan et al. [42], we additionally scale the CondConv-EfficientNet-B0 model with a depth multiplier of 1.1x, which we call CondConv-EfficientNet-B0-depth. Our CondConv-EfficientNet-B0-depth model achieves 79.5% accuracy with only 614M multiply-adds. When trained with the same hyperparameters and regularization search space, the EfficientNet-B1 model, which is scaled from the EfficientNet-B0 model using the compound coefficient, achieves 79.2% accuracy with 700M multiply-adds. In this regime, CondConv scaling outperforms static convolutional scaling with the compound coefficient.\n\nFigure 2 :\n2On ImageNet validation, increasing the number of experts per layer of our CondConv-MobileNetV1 models improves performance relative to inference cost compared to the MobileNetV1 frontier[38] across a spectrum of model sizes. Models with more experts per layer achieve monotonically higher accuracy. We train CondConv models with {1, 2, 4, 8, 16, 32} experts at width multipliers {0.25, 0.50, 0.75, 1.0}.\n\nFigure 3 :\n3Mean routing weights for four classes averaged across the ImageNet validation set at three different depths in our CondConv-MobileNetV1 (0.5x) model. CondConv routing weights are more class-specific at greater depth.\n\nFigure 4 :\n4Distribution of routing weights in the final CondConv layer of our CondConv-MobileNetV1 (0.5x) model when evaluated on all images in the ImageNet validation set. Routing weights follow a bi-modal distribution.\n\nFigure 5 :\n5Routing weights in the final CondConv layer in our CondConv-MobileNetV1 (0.5x) model for 2 classes averaged across the ImageNet validation set. Error bars indicate one standard deviation.\n\nFigure 6 :\n6Top 10 classes with highest mean routing weight for 4 different experts in the final CondConv layer in our CondConv-MobileNetV1 (0.5x) model, as measured across the ImageNet validation set. Expert 1 is most activated for wheeled vehicles; expert 2 is most activated for rectangular structures; expert 3 is most activated for cylindrical household objects; expert 4 is most activated for brown and black dog breeds.\n\nTable 1 :\n1ImageNet validation accuracy and inference cost for our CondConv models on several baseline model architectures. All models use 8 experts per CondConv layer. CondConv improves the accuracy of all baseline architectures with small relative increase in inference cost (<10%).Baseline \nCondConv \nMADDs (\u00d710 6 ) Top-1 (%) \nMADDs (\u00d710 6 ) Top-1 (%) \n\nMobileNetV1 (1.0x) \n567 \n71.9 \n600 \n73.7 \nMobileNetV2 (1.0x) \n301 \n71.6 \n329 \n74.6 \nMnasNet-A1 \n312 \n74.9 \n325 \n76.2 \nResNet-50 \n4093 \n77.7 \n4213 \n78.6 \nEfficientNet-B0 \n391 \n77.2 \n413 \n78.3 \n\n\n\nTable 2 :\n2COCO object detection minival performance of our CondConv-MobileNetV1 SSD 300 architecture with 8 experts per layer. Mean average precision (mAP) reported with COCO primary challenge metric (AP at IoU=0.50:0.05:0.95). CondConv improves mAP at all model sizes with small relative increase in inference cost (<5%).Baseline \nCondConv \nMADDs (\u00d710 6 ) mAP \nMADDs (\u00d710 6 ) mAP \n\nMobileNetV1 (0.5x) \n352 \n14.4 \n363 \n18.0 \nMobileNetV1 (0.75x) \n730 \n18.2 \n755 \n21.0 \nMobileNetV1(1.0x) \n1230 \n20.3 \n1280 \n22.4 \n\n\n\nTable 3 :\n3Different routing architectures. Our \nbaseline CondConv(CC)-MobileNetV1 uses a \none-layer, fully-connected routing function with \nSigmoid activation for each CondConv block. \n\nRouting Fn \nMADDs \nValid \n(\u00d710 6 ) Top-1 (%) \n\nCC-MobileNetV1 (0.25x) \n55.7 \n62.0 \n\nSingle \n55.5 \n56.5 \nPartially-shared \n55.6 \n62.5 \n\nHidden (small) \n55.6 \n57.7 \nHidden (medium) \n55.9 \n62.2 \nHidden (large) \n57.8 \n54.1 \n\nHierarchical \n55.7 \n60.3 \n\nSoftmax \n55.7 \n60.5 \n\n\n\nTable 4 :\n4CondConv at different layers in our \nCondConv(CC)-MobileNetV1 (0.25x) model. \nFC refers to the final classification layer. Cond-\nConv improves performance at every layer. \n\nCondConv Begin \nMADDs \nValid \nLayer \n(\u00d710 6 ) Top-1 (%) \n\nCC-MobileNetV1 (0.25x) \n55.7 \n62.0 \nMobileNetV1 (0.25x) \n41.2 \n50.0 \n\n1 \n56.3 \n62.5 \n5 \n56.0 \n62.0 \n7 \n55.7 \n62.0 \n13 \n52.5 \n59.5 \n15 (FC Only) \n49.3 \n54.2 \n\n7 (No FC) \n47.6 \n60.2 \n\n\n\nTable 2 )\n2. Our \n\n\n\n). CondConv challenges the assumption that convolutional kernels should be shared across all input examples. This introduces a new direction for increasing model capacity while maintaining efficient inference: increase the size and complexity of the kernel-generating function. Since the kernel is computed only once, then convolved across the input, increasing the complexity of the kernel-generating function can be much more efficient than adding additional convolutions or expanding existing convolutions. CondConv also highlights an important research question in the trend towards larger datasets on how to best uncover, represent, and leverage the relationship between examples to improve model performance. In the future, we hope to further explore the design space and limitations of CondConv with larger datasets, more complex kernel-generating functions, and architecture search to design better base architectures.In this section, we provide detailed descriptions of the specific CondConv architectures we used for each baseline model, and elaborate on individual results. All CondConv results are reported with 8 experts per layer. For fair comparison, all results reported use the same training hyperparameters and regularization search space as the CondConv model they are compared against.CondConv-MobileNetV1. We replace the convolutional layers starting from the sixth separable convolutional block and the final fully-connected classification layer of the baseline MobileNetV1 model with CondConv layers. We share routing weights between depthwise and pointwise layers with a separable convolution block. Our CondConv-MobileNetV1 (0.5x) model with 32 experts per CondConv layer achieves 71.6% accuracy at 190M multiply-adds, comparable to the MobileNetV1 (1.0x) model at 71.7% at 571M multiply-adds.CondConv-MobileNetV2. We replace the convolutional layers in the final 6 inverted residual blocks and the final fully-connected classification layer of the baseline MobileNetV2 architecture with CondConv layers. We share routing weights between convolutional layers in each inverted bottleneck block. Our CondConv-MobileNetV2 (1.0x) model achieves 74.6% accuracy at 329M multiply-adds. The MobileNetV2 (1.4x) architecture with static convolutions scaled by width multiplier achieves similar accuracy of 74.5% in our implementation (74.7% inAppendices \n\nA ImageNet Architectures \n\n\nOur re-implementation of the baseline models and our CondConv models use the same hyperparameters for fair comparison. As published reference, Howard et al.[15] report mAP of 19.3 for MobileNetV1 (1.0x).4 Our implementation. Howard et al.[15] report a top-1 accuracy of 50.0% with different hyperparameters.\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015.\n\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, Doina Precup, arXiv:1511.06297Conditional computation in neural networks for faster models. arXiv preprintEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computa- tion in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.\n\nEstimating or propagating gradients through stochastic neurons for conditional computation. Yoshua Bengio, Nicholas L\u00e9onard, Aaron Courville, arXiv:1308.3432arXiv preprintYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n\nGaternet: Dynamic filter selection in convolutional neural network via a dedicated global gating network. Zhourong Chen, Yang Li, Samy Bengio, Si Si, arXiv:1811.11205arXiv preprintZhourong Chen, Yang Li, Samy Bengio, and Si Si. Gaternet: Dynamic filter selection in convo- lutional neural network via a dedicated global gating network. arXiv preprint arXiv:1811.11205, 2018.\n\nExponentially increasing the capacity-to-computation ratio for conditional computation in deep learning. Kyunghyun Cho, Yoshua Bengio, arXiv:1406.7362arXiv preprintKyunghyun Cho and Yoshua Bengio. Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning. arXiv preprint arXiv:1406.7362, 2014.\n\nAutoaugment: Learning augmentation policies from data. Barret Ekin D Cubuk, Dandelion Zoph, Vijay Mane, Quoc V Vasudevan, Le, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nDeformable convolutional networks. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionJifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 764-773, 2017.\n\nLow-rank approximations for conditional feedforward computation in deep neural networks. Andrew Davis, Itamar Arel, arXiv:1312.4461arXiv preprintAndrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward compu- tation in deep neural networks. arXiv preprint arXiv:1312.4461, 2013.\n\nChrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, Daan Wierstra, Pathnet, arXiv:1701.08734Evolution channels gradient descent in super neural networks. arXiv preprintChrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.\n\nXavier Gastaldi, arXiv:1705.07485Shake-shake regularization. arXiv preprintXavier Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485, 2017.\n\nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, arXiv:1706.02677Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprintPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\nHard mixtures of experts for large scale weakly supervised vision. Sam Gross, Marc&apos;aurelio Ranzato, Arthur Szlam, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSam Gross, Marc'Aurelio Ranzato, and Arthur Szlam. Hard mixtures of experts for large scale weakly supervised vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6865-6873, 2017.\n\nDavid Ha, Andrew Dai, Quoc V Le, Hypernetworks, International Conference on Learning Representations. David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. In International Conference on Learning Representations, 2017.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.\n\nMobilenets: Efficient convolutional neural networks for mobile vision applications. G Andrew, Menglong Howard, Bo Zhu, Dmitry Chen, Weijun Kalenichenko, Tobias Wang, Marco Weyand, Hartwig Andreetto, Adam, arXiv:1704.04861arXiv preprintAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n\nSqueeze-and-excitation networks. Jie Hu, Li Shen, Gang Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7132-7141, 2018.\n\nYanping Huang, Yonglong Cheng, Dehao Chen, Hyoukjoong Lee, Jiquan Ngiam, Quoc V Le, Zhifeng Chen, Gpipe, arXiv:1808.07233Efficient training of giant neural networks using pipeline parallelism. arXiv preprintYanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng Chen. GPipe: Efficient training of giant neural networks using pipeline parallelism. arXiv preprint arXiv:1808.07233, 2018.\n\nDynamic filter networks. Xu Jia, Bert De Brabandere, Tinne Tuytelaars, Luc V Gool, Advances in Neural Information Processing Systems. Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. In Advances in Neural Information Processing Systems, pages 667-675, 2016.\n\nIn-datacenter performance analysis of a tensor processing unit. P Norman, Cliff Jouppi, Nishant Young, David Patil, Gaurav Patterson, Raminder Agrawal, Sarah Bajwa, Suresh Bates, Nan Bhatia, Al Boden, Borchers, 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA). IEEENorman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA), pages 1-12. IEEE, 2017.\n\nSplinenets: Continuous neural decision graphs. Cem Keskin, Shahram Izadi, Advances in Neural Processing Systems. Cem Keskin and Shahram Izadi. Splinenets: Continuous neural decision graphs. In Advances in Neural Processing Systems, 2018.\n\nDo better imagenet models transfer better?. Simon Kornblith, Jonathon Shlens, Quoc V Le, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSimon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097-1105, 2012.\n\nHandwritten digit recognition with a backpropagation network. Yann Lecun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, Lawrence D Jackel, Advances in Neural Information Processing Systems. Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back- propagation network. In Advances in Neural Information Processing Systems, pages 396-404, 1990.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European Conference on Computer Vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740-755. Springer, 2014.\n\nDynamic deep neural networks: Optimizing accuracy-efficiency trade-offs by selective execution. Lanlan Liu, Jia Deng, Thirty-Second AAAI Conference on Artificial Intelligence. Lanlan Liu and Jia Deng. Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offs by selective execution. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\nSsd: Single shot multibox detector. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C Berg, European Conference on Computer Vision. SpringerWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European Conference on Computer Vision, pages 21-37. Springer, 2016.\n\nSgdr: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. 2016.\n\nEffective approaches to attentionbased neural machine translation. Minh-Thang Luong, Hieu Pham, Christopher D Manning, Emperical Methods in Natural Language Processing. Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. In Emperical Methods in Natural Language Processing, 2015.\n\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, arXiv:1805.00932arXiv preprintDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. arXiv preprint arXiv:1805.00932, 2018.\n\nDeciding how to decide: Dynamic routing in artificial neural networks. Mason Mcgill, Pietro Perona, International Conference on Machine Learning. Mason McGill and Pietro Perona. Deciding how to decide: Dynamic routing in artificial neural networks. In International Conference on Machine Learning, 2017.\n\nHydranets: Specialized dynamic architectures for efficient inference. William R Ravi Teja Mullapudi, Noam Mark, Kayvon Shazeer, Fatahalian, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionRavi Teja Mullapudi, William R.Mark, Noam Shazeer, and Kayvon Fatahalian. Hydranets: Specialized dynamic architectures for efficient inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\nContextual parameter generation for universal neural machine translation. Mrinmaya Emmanouil Antonios Platanios, Graham Sachan, Tom Neubig, Mitchell, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingEmmanouil Antonios Platanios, Mrinmaya Sachan, Graham Neubig, and Tom Mitchell. Contex- tual parameter generation for universal neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 425-435, 2018.\n\nRegularized evolution for image classifier architecture search. Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le, Thirty-Third AAAI Conference on Artificial Intelligence. Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In Thirty-Third AAAI Conference on Artificial Intelligence, 2019.\n\nRouting networks: Adaptive selection of non-linear functions for multi-task learning. Clemens Rosenbaum, Tim Klinger, Matthew Riemer, International Conference on Learning Representations. Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. In International Conference on Learning Representations, 2018.\n\nImagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International Journal of Computer Vision. 1153Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.\n\nMobilenetv2: Inverted residuals and linear bottlenecks. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4510-4520, 2018.\n\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean, International Conference on Learning Representations. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017.\n\nTensorflow-slim image classification model library. N Silberman, S Guadarrama, N. Silberman and S. Guadarrama. Tensorflow-slim image classification model library, 2016.\n\nDropout: A simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Journal of Machine Learning Research. 15Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929-1958, 2014.\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1-9, 2015.\n\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Quoc V Le, Mnasnet, arXiv:1807.11626Platformaware neural architecture search for mobile. arXiv preprintMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V Le. Mnasnet: Platform- aware neural architecture search for mobile. arXiv preprint arXiv:1807.11626, 2018.\n\nMingxing Tan, V Quoc, Le, Efficientnet, arXiv:1905.11946Rethinking model scaling for convolutional neural networks. arXiv preprintMingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n\nMingxing Tan, V Quoc, Le, Mixnet, arXiv:1907.09595Mixed depthwise convolutional kernels. arXiv preprintMingxing Tan and Quoc V Le. Mixnet: Mixed depthwise convolutional kernels. arXiv preprint arXiv:1907.09595, 2019.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, pages 5998-6008, 2017.\n\nSkipnet: Learning dynamic routing in convolutional networks. Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, Joseph E Gonzalez, European Conference on Computer Vision. SpringerXin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In European Conference on Computer Vision, pages 420-436. Springer, 2018.\n\nPay less attention with lightweight and dynamic convolutions. Felix Wu, Angela Fan, Alexei Baevski, Michael Yann N Dauphin, Auli, International Conference on Learning Representations. Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less atten- tion with lightweight and dynamic convolutions. In International Conference on Learning Representations, 2019.\n\nBlockdrop: Dynamic inference paths in residual networks. Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, S Larry, Kristen Davis, Rogerio Grauman, Feris, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis, Kristen Grauman, and Rogerio Feris. Blockdrop: Dynamic inference paths in residual networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492-1500, 2017.\n\nmixup: Beyond empirical risk minimization. Hongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, International Conference on Learning Representations. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2017.\n", "annotations": {"author": "[{\"end\":109,\"start\":78},{\"end\":123,\"start\":110},{\"end\":158,\"start\":124},{\"end\":172,\"start\":159},{\"end\":196,\"start\":173},{\"end\":228,\"start\":197},{\"end\":242,\"start\":229}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":86},{\"end\":122,\"start\":117},{\"end\":138,\"start\":132},{\"end\":171,\"start\":166},{\"end\":195,\"start\":180},{\"end\":209,\"start\":204},{\"end\":241,\"start\":236}]", "author_first_name": "[{\"end\":85,\"start\":78},{\"end\":116,\"start\":110},{\"end\":131,\"start\":124},{\"end\":165,\"start\":159},{\"end\":177,\"start\":173},{\"end\":179,\"start\":178},{\"end\":203,\"start\":197},{\"end\":235,\"start\":229}]", "author_affiliation": null, "title": "[{\"end\":75,\"start\":1},{\"end\":317,\"start\":243}]", "venue": null, "abstract": "[{\"end\":2830,\"start\":319}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4877,\"start\":4874},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4879,\"start\":4877},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4881,\"start\":4879},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4883,\"start\":4881},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5323,\"start\":5319},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5326,\"start\":5323},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5329,\"start\":5326},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5331,\"start\":5329},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5347,\"start\":5343},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5364,\"start\":5360},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5567,\"start\":5563},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5680,\"start\":5676},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5798,\"start\":5794},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6276,\"start\":6272},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6617,\"start\":6613},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6836,\"start\":6832},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6853,\"start\":6849},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7438,\"start\":7434},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7505,\"start\":7502},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7625,\"start\":7621},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7627,\"start\":7625},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7630,\"start\":7627},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7892,\"start\":7888},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7998,\"start\":7994},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8170,\"start\":8167},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8278,\"start\":8274},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11628,\"start\":11624},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11646,\"start\":11642},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11662,\"start\":11658},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11676,\"start\":11672},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11699,\"start\":11695},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12642,\"start\":12638},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13021,\"start\":13017},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13130,\"start\":13126},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13152,\"start\":13148},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14343,\"start\":14339},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14514,\"start\":14511},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14545,\"start\":14541},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14709,\"start\":14705},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15352,\"start\":15348},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15355,\"start\":15352},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15384,\"start\":15380},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15474,\"start\":15470},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15681,\"start\":15677},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15796,\"start\":15792},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16512,\"start\":16508},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16601,\"start\":16597},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16671,\"start\":16667},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16839,\"start\":16838},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17073,\"start\":17069},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17105,\"start\":17101},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17129,\"start\":17125},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17152,\"start\":17148},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17181,\"start\":17177},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17318,\"start\":17314},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17392,\"start\":17388},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17961,\"start\":17960},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21233,\"start\":21229},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22839,\"start\":22835},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23432,\"start\":23428},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24514,\"start\":24510},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25286,\"start\":25282},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31115,\"start\":31111},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31159,\"start\":31158},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31197,\"start\":31193}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25499,\"start\":25083},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25729,\"start\":25500},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25952,\"start\":25730},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26153,\"start\":25953},{\"attributes\":{\"id\":\"fig_4\"},\"end\":26581,\"start\":26154},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27133,\"start\":26582},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27648,\"start\":27134},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28107,\"start\":27649},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":28533,\"start\":28108},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":28553,\"start\":28534},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30954,\"start\":28554}]", "paragraph": "[{\"end\":3205,\"start\":2832},{\"end\":4096,\"start\":3207},{\"end\":4582,\"start\":4098},{\"end\":5189,\"start\":4599},{\"end\":5946,\"start\":5191},{\"end\":6232,\"start\":5948},{\"end\":6751,\"start\":6234},{\"end\":7264,\"start\":6753},{\"end\":7816,\"start\":7266},{\"end\":8407,\"start\":7818},{\"end\":8722,\"start\":8452},{\"end\":9093,\"start\":8775},{\"end\":9868,\"start\":9095},{\"end\":10038,\"start\":9870},{\"end\":10660,\"start\":10126},{\"end\":11251,\"start\":10662},{\"end\":11504,\"start\":11291},{\"end\":12541,\"start\":11520},{\"end\":14238,\"start\":12569},{\"end\":14759,\"start\":14240},{\"end\":15153,\"start\":14761},{\"end\":15522,\"start\":15155},{\"end\":16364,\"start\":15524},{\"end\":16641,\"start\":16390},{\"end\":17643,\"start\":16643},{\"end\":17874,\"start\":17645},{\"end\":18035,\"start\":17876},{\"end\":18722,\"start\":18056},{\"end\":18917,\"start\":18724},{\"end\":19184,\"start\":18919},{\"end\":19477,\"start\":19186},{\"end\":19700,\"start\":19479},{\"end\":20506,\"start\":19725},{\"end\":20901,\"start\":20519},{\"end\":20999,\"start\":20903},{\"end\":21718,\"start\":21001},{\"end\":22078,\"start\":21720},{\"end\":22384,\"start\":22080},{\"end\":22741,\"start\":22386},{\"end\":22874,\"start\":22756},{\"end\":23433,\"start\":22876},{\"end\":23963,\"start\":23435},{\"end\":24399,\"start\":23965},{\"end\":25082,\"start\":24401}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8774,\"start\":8723},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10125,\"start\":10039},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11290,\"start\":11252}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14132,\"start\":14125},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":14144,\"start\":14137},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15274,\"start\":15266},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18124,\"start\":18117},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19837,\"start\":19828}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":4597,\"start\":4585},{\"attributes\":{\"n\":\"3\"},\"end\":8450,\"start\":8410},{\"attributes\":{\"n\":\"4\"},\"end\":11518,\"start\":11507},{\"attributes\":{\"n\":\"4.1\"},\"end\":12567,\"start\":12544},{\"attributes\":{\"n\":\"4.2\"},\"end\":16388,\"start\":16367},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":18054,\"start\":18038},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":19723,\"start\":19703},{\"attributes\":{\"n\":\"5\"},\"end\":20517,\"start\":20509},{\"attributes\":{\"n\":\"6\"},\"end\":22754,\"start\":22744},{\"end\":25094,\"start\":25084},{\"end\":25511,\"start\":25501},{\"end\":25741,\"start\":25731},{\"end\":25964,\"start\":25954},{\"end\":26165,\"start\":26155},{\"end\":26592,\"start\":26583},{\"end\":27144,\"start\":27135},{\"end\":27659,\"start\":27650},{\"end\":28118,\"start\":28109},{\"end\":28544,\"start\":28535}]", "table": "[{\"end\":27133,\"start\":26867},{\"end\":27648,\"start\":27458},{\"end\":28107,\"start\":27661},{\"end\":28533,\"start\":28120},{\"end\":28553,\"start\":28546},{\"end\":30954,\"start\":30914}]", "figure_caption": "[{\"end\":25499,\"start\":25096},{\"end\":25729,\"start\":25513},{\"end\":25952,\"start\":25743},{\"end\":26153,\"start\":25966},{\"end\":26581,\"start\":26167},{\"end\":26867,\"start\":26594},{\"end\":27458,\"start\":27146},{\"end\":30914,\"start\":28556}]", "figure_ref": "[{\"end\":2932,\"start\":2924},{\"end\":8649,\"start\":8641},{\"end\":10037,\"start\":10029},{\"end\":11924,\"start\":11917},{\"end\":11993,\"start\":11985},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15008,\"start\":15000},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21542,\"start\":21534},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21823,\"start\":21814},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22183,\"start\":22174},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22583,\"start\":22575}]", "bib_author_first_name": "[{\"end\":31342,\"start\":31335},{\"end\":31362,\"start\":31353},{\"end\":31374,\"start\":31368},{\"end\":31632,\"start\":31624},{\"end\":31651,\"start\":31641},{\"end\":31665,\"start\":31659},{\"end\":31679,\"start\":31674},{\"end\":32050,\"start\":32044},{\"end\":32067,\"start\":32059},{\"end\":32082,\"start\":32077},{\"end\":32422,\"start\":32414},{\"end\":32433,\"start\":32429},{\"end\":32442,\"start\":32438},{\"end\":32453,\"start\":32451},{\"end\":32798,\"start\":32789},{\"end\":32810,\"start\":32804},{\"end\":33086,\"start\":33080},{\"end\":33110,\"start\":33101},{\"end\":33122,\"start\":33117},{\"end\":33135,\"start\":33129},{\"end\":33552,\"start\":33546},{\"end\":33564,\"start\":33558},{\"end\":33574,\"start\":33569},{\"end\":33584,\"start\":33582},{\"end\":33596,\"start\":33589},{\"end\":33607,\"start\":33604},{\"end\":33618,\"start\":33612},{\"end\":34051,\"start\":34045},{\"end\":34065,\"start\":34059},{\"end\":34271,\"start\":34261},{\"end\":34287,\"start\":34282},{\"end\":34304,\"start\":34297},{\"end\":34319,\"start\":34315},{\"end\":34332,\"start\":34327},{\"end\":34343,\"start\":34337},{\"end\":34345,\"start\":34344},{\"end\":34361,\"start\":34352},{\"end\":34375,\"start\":34371},{\"end\":34733,\"start\":34727},{\"end\":34892,\"start\":34887},{\"end\":34905,\"start\":34900},{\"end\":34918,\"start\":34914},{\"end\":34935,\"start\":34929},{\"end\":34953,\"start\":34947},{\"end\":34970,\"start\":34966},{\"end\":34985,\"start\":34979},{\"end\":35423,\"start\":35420},{\"end\":35448,\"start\":35431},{\"end\":35464,\"start\":35458},{\"end\":35842,\"start\":35837},{\"end\":35853,\"start\":35847},{\"end\":35865,\"start\":35859},{\"end\":36108,\"start\":36101},{\"end\":36120,\"start\":36113},{\"end\":36136,\"start\":36128},{\"end\":36146,\"start\":36142},{\"end\":36585,\"start\":36584},{\"end\":36602,\"start\":36594},{\"end\":36613,\"start\":36611},{\"end\":36625,\"start\":36619},{\"end\":36638,\"start\":36632},{\"end\":36659,\"start\":36653},{\"end\":36671,\"start\":36666},{\"end\":36687,\"start\":36680},{\"end\":37019,\"start\":37016},{\"end\":37026,\"start\":37024},{\"end\":37037,\"start\":37033},{\"end\":37361,\"start\":37354},{\"end\":37377,\"start\":37369},{\"end\":37390,\"start\":37385},{\"end\":37407,\"start\":37397},{\"end\":37419,\"start\":37413},{\"end\":37431,\"start\":37427},{\"end\":37433,\"start\":37432},{\"end\":37445,\"start\":37438},{\"end\":37810,\"start\":37808},{\"end\":37820,\"start\":37816},{\"end\":37841,\"start\":37836},{\"end\":37859,\"start\":37854},{\"end\":38145,\"start\":38144},{\"end\":38159,\"start\":38154},{\"end\":38175,\"start\":38168},{\"end\":38188,\"start\":38183},{\"end\":38202,\"start\":38196},{\"end\":38222,\"start\":38214},{\"end\":38237,\"start\":38232},{\"end\":38251,\"start\":38245},{\"end\":38262,\"start\":38259},{\"end\":38273,\"start\":38271},{\"end\":38756,\"start\":38753},{\"end\":38772,\"start\":38765},{\"end\":38994,\"start\":38989},{\"end\":39014,\"start\":39006},{\"end\":39029,\"start\":39023},{\"end\":39425,\"start\":39421},{\"end\":39442,\"start\":39438},{\"end\":39462,\"start\":39454},{\"end\":39464,\"start\":39463},{\"end\":39789,\"start\":39785},{\"end\":39805,\"start\":39797},{\"end\":39807,\"start\":39806},{\"end\":39819,\"start\":39815},{\"end\":39821,\"start\":39820},{\"end\":39836,\"start\":39830},{\"end\":39855,\"start\":39848},{\"end\":39857,\"start\":39856},{\"end\":39871,\"start\":39866},{\"end\":39873,\"start\":39872},{\"end\":39891,\"start\":39883},{\"end\":39893,\"start\":39892},{\"end\":40265,\"start\":40257},{\"end\":40278,\"start\":40271},{\"end\":40291,\"start\":40286},{\"end\":40307,\"start\":40302},{\"end\":40320,\"start\":40314},{\"end\":40333,\"start\":40329},{\"end\":40348,\"start\":40343},{\"end\":40367,\"start\":40357},{\"end\":40769,\"start\":40763},{\"end\":40778,\"start\":40775},{\"end\":41071,\"start\":41068},{\"end\":41085,\"start\":41077},{\"end\":41103,\"start\":41096},{\"end\":41120,\"start\":41111},{\"end\":41135,\"start\":41130},{\"end\":41152,\"start\":41142},{\"end\":41168,\"start\":41157},{\"end\":41501,\"start\":41497},{\"end\":41519,\"start\":41514},{\"end\":41700,\"start\":41690},{\"end\":41712,\"start\":41708},{\"end\":41732,\"start\":41719},{\"end\":42082,\"start\":42077},{\"end\":42096,\"start\":42092},{\"end\":42114,\"start\":42107},{\"end\":42134,\"start\":42127},{\"end\":42146,\"start\":42139},{\"end\":42161,\"start\":42155},{\"end\":42501,\"start\":42496},{\"end\":42516,\"start\":42510},{\"end\":42807,\"start\":42800},{\"end\":42809,\"start\":42808},{\"end\":42835,\"start\":42831},{\"end\":42848,\"start\":42842},{\"end\":43326,\"start\":43318},{\"end\":43363,\"start\":43357},{\"end\":43375,\"start\":43372},{\"end\":43893,\"start\":43886},{\"end\":43904,\"start\":43900},{\"end\":43922,\"start\":43915},{\"end\":43936,\"start\":43930},{\"end\":44281,\"start\":44274},{\"end\":44296,\"start\":44293},{\"end\":44313,\"start\":44306},{\"end\":44633,\"start\":44629},{\"end\":44650,\"start\":44647},{\"end\":44660,\"start\":44657},{\"end\":44673,\"start\":44665},{\"end\":44689,\"start\":44682},{\"end\":44704,\"start\":44700},{\"end\":44716,\"start\":44709},{\"end\":44730,\"start\":44724},{\"end\":44747,\"start\":44741},{\"end\":44763,\"start\":44756},{\"end\":45150,\"start\":45146},{\"end\":45166,\"start\":45160},{\"end\":45183,\"start\":45175},{\"end\":45195,\"start\":45189},{\"end\":45218,\"start\":45207},{\"end\":45696,\"start\":45692},{\"end\":45712,\"start\":45706},{\"end\":45734,\"start\":45725},{\"end\":45748,\"start\":45744},{\"end\":45760,\"start\":45756},{\"end\":45773,\"start\":45765},{\"end\":45786,\"start\":45782},{\"end\":46150,\"start\":46149},{\"end\":46163,\"start\":46162},{\"end\":46340,\"start\":46334},{\"end\":46361,\"start\":46353},{\"end\":46374,\"start\":46370},{\"end\":46391,\"start\":46387},{\"end\":46409,\"start\":46403},{\"end\":46727,\"start\":46718},{\"end\":46740,\"start\":46737},{\"end\":46754,\"start\":46746},{\"end\":46766,\"start\":46760},{\"end\":46782,\"start\":46777},{\"end\":46797,\"start\":46789},{\"end\":46815,\"start\":46808},{\"end\":46830,\"start\":46823},{\"end\":46848,\"start\":46842},{\"end\":47289,\"start\":47281},{\"end\":47297,\"start\":47295},{\"end\":47311,\"start\":47304},{\"end\":47323,\"start\":47318},{\"end\":47341,\"start\":47335},{\"end\":47619,\"start\":47611},{\"end\":47626,\"start\":47625},{\"end\":47891,\"start\":47883},{\"end\":47898,\"start\":47897},{\"end\":48134,\"start\":48128},{\"end\":48148,\"start\":48144},{\"end\":48162,\"start\":48158},{\"end\":48176,\"start\":48171},{\"end\":48193,\"start\":48188},{\"end\":48206,\"start\":48201},{\"end\":48208,\"start\":48207},{\"end\":48222,\"start\":48216},{\"end\":48236,\"start\":48231},{\"end\":48596,\"start\":48593},{\"end\":48609,\"start\":48603},{\"end\":48619,\"start\":48614},{\"end\":48631,\"start\":48625},{\"end\":48647,\"start\":48641},{\"end\":48649,\"start\":48648},{\"end\":48982,\"start\":48977},{\"end\":48993,\"start\":48987},{\"end\":49005,\"start\":48999},{\"end\":49022,\"start\":49015},{\"end\":49362,\"start\":49356},{\"end\":49373,\"start\":49367},{\"end\":49393,\"start\":49385},{\"end\":49407,\"start\":49401},{\"end\":49417,\"start\":49416},{\"end\":49432,\"start\":49425},{\"end\":49447,\"start\":49440},{\"end\":49931,\"start\":49924},{\"end\":49941,\"start\":49937},{\"end\":49957,\"start\":49952},{\"end\":49973,\"start\":49966},{\"end\":49985,\"start\":49978},{\"end\":50418,\"start\":50412},{\"end\":50435,\"start\":50426},{\"end\":50448,\"start\":50443}]", "bib_author_last_name": "[{\"end\":31351,\"start\":31343},{\"end\":31366,\"start\":31363},{\"end\":31381,\"start\":31375},{\"end\":31639,\"start\":31633},{\"end\":31657,\"start\":31652},{\"end\":31672,\"start\":31666},{\"end\":31686,\"start\":31680},{\"end\":32057,\"start\":32051},{\"end\":32075,\"start\":32068},{\"end\":32092,\"start\":32083},{\"end\":32427,\"start\":32423},{\"end\":32436,\"start\":32434},{\"end\":32449,\"start\":32443},{\"end\":32456,\"start\":32454},{\"end\":32802,\"start\":32799},{\"end\":32817,\"start\":32811},{\"end\":33099,\"start\":33087},{\"end\":33115,\"start\":33111},{\"end\":33127,\"start\":33123},{\"end\":33145,\"start\":33136},{\"end\":33149,\"start\":33147},{\"end\":33556,\"start\":33553},{\"end\":33567,\"start\":33565},{\"end\":33580,\"start\":33575},{\"end\":33587,\"start\":33585},{\"end\":33602,\"start\":33597},{\"end\":33610,\"start\":33608},{\"end\":33622,\"start\":33619},{\"end\":34057,\"start\":34052},{\"end\":34070,\"start\":34066},{\"end\":34280,\"start\":34272},{\"end\":34295,\"start\":34288},{\"end\":34313,\"start\":34305},{\"end\":34325,\"start\":34320},{\"end\":34335,\"start\":34333},{\"end\":34350,\"start\":34346},{\"end\":34369,\"start\":34362},{\"end\":34384,\"start\":34376},{\"end\":34393,\"start\":34386},{\"end\":34742,\"start\":34734},{\"end\":34898,\"start\":34893},{\"end\":34912,\"start\":34906},{\"end\":34927,\"start\":34919},{\"end\":34945,\"start\":34936},{\"end\":34964,\"start\":34954},{\"end\":34977,\"start\":34971},{\"end\":34993,\"start\":34986},{\"end\":35429,\"start\":35424},{\"end\":35456,\"start\":35449},{\"end\":35470,\"start\":35465},{\"end\":35845,\"start\":35843},{\"end\":35857,\"start\":35854},{\"end\":35868,\"start\":35866},{\"end\":35883,\"start\":35870},{\"end\":36111,\"start\":36109},{\"end\":36126,\"start\":36121},{\"end\":36140,\"start\":36137},{\"end\":36150,\"start\":36147},{\"end\":36592,\"start\":36586},{\"end\":36609,\"start\":36603},{\"end\":36617,\"start\":36614},{\"end\":36630,\"start\":36626},{\"end\":36651,\"start\":36639},{\"end\":36664,\"start\":36660},{\"end\":36678,\"start\":36672},{\"end\":36697,\"start\":36688},{\"end\":36703,\"start\":36699},{\"end\":37022,\"start\":37020},{\"end\":37031,\"start\":37027},{\"end\":37041,\"start\":37038},{\"end\":37367,\"start\":37362},{\"end\":37383,\"start\":37378},{\"end\":37395,\"start\":37391},{\"end\":37411,\"start\":37408},{\"end\":37425,\"start\":37420},{\"end\":37436,\"start\":37434},{\"end\":37450,\"start\":37446},{\"end\":37457,\"start\":37452},{\"end\":37814,\"start\":37811},{\"end\":37834,\"start\":37821},{\"end\":37852,\"start\":37842},{\"end\":37864,\"start\":37860},{\"end\":38152,\"start\":38146},{\"end\":38166,\"start\":38160},{\"end\":38181,\"start\":38176},{\"end\":38194,\"start\":38189},{\"end\":38212,\"start\":38203},{\"end\":38230,\"start\":38223},{\"end\":38243,\"start\":38238},{\"end\":38257,\"start\":38252},{\"end\":38269,\"start\":38263},{\"end\":38279,\"start\":38274},{\"end\":38289,\"start\":38281},{\"end\":38763,\"start\":38757},{\"end\":38778,\"start\":38773},{\"end\":39004,\"start\":38995},{\"end\":39021,\"start\":39015},{\"end\":39032,\"start\":39030},{\"end\":39436,\"start\":39426},{\"end\":39452,\"start\":39443},{\"end\":39471,\"start\":39465},{\"end\":39795,\"start\":39790},{\"end\":39813,\"start\":39808},{\"end\":39828,\"start\":39822},{\"end\":39846,\"start\":39837},{\"end\":39864,\"start\":39858},{\"end\":39881,\"start\":39874},{\"end\":39900,\"start\":39894},{\"end\":40269,\"start\":40266},{\"end\":40284,\"start\":40279},{\"end\":40300,\"start\":40292},{\"end\":40312,\"start\":40308},{\"end\":40327,\"start\":40321},{\"end\":40341,\"start\":40334},{\"end\":40355,\"start\":40349},{\"end\":40375,\"start\":40368},{\"end\":40773,\"start\":40770},{\"end\":40783,\"start\":40779},{\"end\":41075,\"start\":41072},{\"end\":41094,\"start\":41086},{\"end\":41109,\"start\":41104},{\"end\":41128,\"start\":41121},{\"end\":41140,\"start\":41136},{\"end\":41155,\"start\":41153},{\"end\":41173,\"start\":41169},{\"end\":41512,\"start\":41502},{\"end\":41526,\"start\":41520},{\"end\":41706,\"start\":41701},{\"end\":41717,\"start\":41713},{\"end\":41740,\"start\":41733},{\"end\":42090,\"start\":42083},{\"end\":42105,\"start\":42097},{\"end\":42125,\"start\":42115},{\"end\":42137,\"start\":42135},{\"end\":42153,\"start\":42147},{\"end\":42164,\"start\":42162},{\"end\":42508,\"start\":42502},{\"end\":42523,\"start\":42517},{\"end\":42829,\"start\":42810},{\"end\":42840,\"start\":42836},{\"end\":42856,\"start\":42849},{\"end\":42868,\"start\":42858},{\"end\":43355,\"start\":43327},{\"end\":43370,\"start\":43364},{\"end\":43382,\"start\":43376},{\"end\":43392,\"start\":43384},{\"end\":43898,\"start\":43894},{\"end\":43913,\"start\":43905},{\"end\":43928,\"start\":43923},{\"end\":43939,\"start\":43937},{\"end\":44291,\"start\":44282},{\"end\":44304,\"start\":44297},{\"end\":44320,\"start\":44314},{\"end\":44645,\"start\":44634},{\"end\":44655,\"start\":44651},{\"end\":44663,\"start\":44661},{\"end\":44680,\"start\":44674},{\"end\":44698,\"start\":44690},{\"end\":44707,\"start\":44705},{\"end\":44722,\"start\":44717},{\"end\":44739,\"start\":44731},{\"end\":44754,\"start\":44748},{\"end\":44773,\"start\":44764},{\"end\":45158,\"start\":45151},{\"end\":45173,\"start\":45167},{\"end\":45187,\"start\":45184},{\"end\":45205,\"start\":45196},{\"end\":45223,\"start\":45219},{\"end\":45704,\"start\":45697},{\"end\":45723,\"start\":45713},{\"end\":45742,\"start\":45735},{\"end\":45754,\"start\":45749},{\"end\":45763,\"start\":45761},{\"end\":45780,\"start\":45774},{\"end\":45791,\"start\":45787},{\"end\":46160,\"start\":46151},{\"end\":46174,\"start\":46164},{\"end\":46351,\"start\":46341},{\"end\":46368,\"start\":46362},{\"end\":46385,\"start\":46375},{\"end\":46401,\"start\":46392},{\"end\":46423,\"start\":46410},{\"end\":46735,\"start\":46728},{\"end\":46744,\"start\":46741},{\"end\":46758,\"start\":46755},{\"end\":46775,\"start\":46767},{\"end\":46787,\"start\":46783},{\"end\":46806,\"start\":46798},{\"end\":46821,\"start\":46816},{\"end\":46840,\"start\":46831},{\"end\":46859,\"start\":46849},{\"end\":47293,\"start\":47290},{\"end\":47302,\"start\":47298},{\"end\":47316,\"start\":47312},{\"end\":47333,\"start\":47324},{\"end\":47344,\"start\":47342},{\"end\":47353,\"start\":47346},{\"end\":47623,\"start\":47620},{\"end\":47631,\"start\":47627},{\"end\":47635,\"start\":47633},{\"end\":47649,\"start\":47637},{\"end\":47895,\"start\":47892},{\"end\":47903,\"start\":47899},{\"end\":47907,\"start\":47905},{\"end\":47915,\"start\":47909},{\"end\":48142,\"start\":48135},{\"end\":48156,\"start\":48149},{\"end\":48169,\"start\":48163},{\"end\":48186,\"start\":48177},{\"end\":48199,\"start\":48194},{\"end\":48214,\"start\":48209},{\"end\":48229,\"start\":48223},{\"end\":48247,\"start\":48237},{\"end\":48601,\"start\":48597},{\"end\":48612,\"start\":48610},{\"end\":48623,\"start\":48620},{\"end\":48639,\"start\":48632},{\"end\":48658,\"start\":48650},{\"end\":48985,\"start\":48983},{\"end\":48997,\"start\":48994},{\"end\":49013,\"start\":49006},{\"end\":49037,\"start\":49023},{\"end\":49043,\"start\":49039},{\"end\":49365,\"start\":49363},{\"end\":49383,\"start\":49374},{\"end\":49399,\"start\":49394},{\"end\":49414,\"start\":49408},{\"end\":49423,\"start\":49418},{\"end\":49438,\"start\":49433},{\"end\":49455,\"start\":49448},{\"end\":49462,\"start\":49457},{\"end\":49935,\"start\":49932},{\"end\":49950,\"start\":49942},{\"end\":49964,\"start\":49958},{\"end\":49976,\"start\":49974},{\"end\":49988,\"start\":49986},{\"end\":50424,\"start\":50419},{\"end\":50441,\"start\":50436},{\"end\":50463,\"start\":50449},{\"end\":50474,\"start\":50465}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11212020},\"end\":31622,\"start\":31264},{\"attributes\":{\"doi\":\"arXiv:1511.06297\",\"id\":\"b1\"},\"end\":31950,\"start\":31624},{\"attributes\":{\"doi\":\"arXiv:1308.3432\",\"id\":\"b2\"},\"end\":32306,\"start\":31952},{\"attributes\":{\"doi\":\"arXiv:1811.11205\",\"id\":\"b3\"},\"end\":32682,\"start\":32308},{\"attributes\":{\"doi\":\"arXiv:1406.7362\",\"id\":\"b4\"},\"end\":33023,\"start\":32684},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":43928340},\"end\":33509,\"start\":33025},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4028864},\"end\":33954,\"start\":33511},{\"attributes\":{\"doi\":\"arXiv:1312.4461\",\"id\":\"b7\"},\"end\":34259,\"start\":33956},{\"attributes\":{\"doi\":\"arXiv:1701.08734\",\"id\":\"b8\"},\"end\":34725,\"start\":34261},{\"attributes\":{\"doi\":\"arXiv:1705.07485\",\"id\":\"b9\"},\"end\":34885,\"start\":34727},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b10\"},\"end\":35351,\"start\":34887},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6869636},\"end\":35835,\"start\":35353},{\"attributes\":{\"id\":\"b12\"},\"end\":36053,\"start\":35837},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206594692},\"end\":36498,\"start\":36055},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b14\"},\"end\":36981,\"start\":36500},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":140309863},\"end\":37352,\"start\":36983},{\"attributes\":{\"doi\":\"arXiv:1808.07233\",\"id\":\"b16\"},\"end\":37781,\"start\":37354},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2097418},\"end\":38078,\"start\":37783},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4202768},\"end\":38704,\"start\":38080},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":53109222},\"end\":38943,\"start\":38706},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":43928547},\"end\":39354,\"start\":38945},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":195908774},\"end\":39721,\"start\":39356},{\"attributes\":{\"id\":\"b22\"},\"end\":40212,\"start\":39723},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14113767},\"end\":40665,\"start\":40214},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3662396},\"end\":41030,\"start\":40667},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2141740},\"end\":41441,\"start\":41032},{\"attributes\":{\"id\":\"b26\"},\"end\":41621,\"start\":41443},{\"attributes\":{\"id\":\"b27\"},\"end\":41975,\"start\":41623},{\"attributes\":{\"doi\":\"arXiv:1805.00932\",\"id\":\"b28\"},\"end\":42423,\"start\":41977},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15136849},\"end\":42728,\"start\":42425},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":20640456},\"end\":43242,\"start\":42730},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":52100117},\"end\":43820,\"start\":43244},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3640974},\"end\":44186,\"start\":43822},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":22014305},\"end\":44576,\"start\":44188},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2930547},\"end\":45088,\"start\":44578},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4555207},\"end\":45609,\"start\":45090},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":12462234},\"end\":46095,\"start\":45611},{\"attributes\":{\"id\":\"b37\"},\"end\":46265,\"start\":46097},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6844431},\"end\":46684,\"start\":46267},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":206592484},\"end\":47279,\"start\":46686},{\"attributes\":{\"doi\":\"arXiv:1807.11626\",\"id\":\"b40\"},\"end\":47609,\"start\":47281},{\"attributes\":{\"doi\":\"arXiv:1905.11946\",\"id\":\"b41\"},\"end\":47881,\"start\":47611},{\"attributes\":{\"doi\":\"arXiv:1907.09595\",\"id\":\"b42\"},\"end\":48099,\"start\":47883},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":13756489},\"end\":48530,\"start\":48101},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":21110409},\"end\":48913,\"start\":48532},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":59310641},\"end\":49297,\"start\":48915},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":13739860},\"end\":49860,\"start\":49299},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":8485068},\"end\":50367,\"start\":49862},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":3162051},\"end\":50703,\"start\":50369}]", "bib_title": "[{\"end\":31333,\"start\":31264},{\"end\":33078,\"start\":33025},{\"end\":33544,\"start\":33511},{\"end\":35418,\"start\":35353},{\"end\":36099,\"start\":36055},{\"end\":37014,\"start\":36983},{\"end\":37806,\"start\":37783},{\"end\":38142,\"start\":38080},{\"end\":38751,\"start\":38706},{\"end\":38987,\"start\":38945},{\"end\":39419,\"start\":39356},{\"end\":39783,\"start\":39723},{\"end\":40255,\"start\":40214},{\"end\":40761,\"start\":40667},{\"end\":41066,\"start\":41032},{\"end\":41688,\"start\":41623},{\"end\":42494,\"start\":42425},{\"end\":42798,\"start\":42730},{\"end\":43316,\"start\":43244},{\"end\":43884,\"start\":43822},{\"end\":44272,\"start\":44188},{\"end\":44627,\"start\":44578},{\"end\":45144,\"start\":45090},{\"end\":45690,\"start\":45611},{\"end\":46332,\"start\":46267},{\"end\":46716,\"start\":46686},{\"end\":48126,\"start\":48101},{\"end\":48591,\"start\":48532},{\"end\":48975,\"start\":48915},{\"end\":49354,\"start\":49299},{\"end\":49922,\"start\":49862},{\"end\":50410,\"start\":50369}]", "bib_author": "[{\"end\":31353,\"start\":31335},{\"end\":31368,\"start\":31353},{\"end\":31383,\"start\":31368},{\"end\":31641,\"start\":31624},{\"end\":31659,\"start\":31641},{\"end\":31674,\"start\":31659},{\"end\":31688,\"start\":31674},{\"end\":32059,\"start\":32044},{\"end\":32077,\"start\":32059},{\"end\":32094,\"start\":32077},{\"end\":32429,\"start\":32414},{\"end\":32438,\"start\":32429},{\"end\":32451,\"start\":32438},{\"end\":32458,\"start\":32451},{\"end\":32804,\"start\":32789},{\"end\":32819,\"start\":32804},{\"end\":33101,\"start\":33080},{\"end\":33117,\"start\":33101},{\"end\":33129,\"start\":33117},{\"end\":33147,\"start\":33129},{\"end\":33151,\"start\":33147},{\"end\":33558,\"start\":33546},{\"end\":33569,\"start\":33558},{\"end\":33582,\"start\":33569},{\"end\":33589,\"start\":33582},{\"end\":33604,\"start\":33589},{\"end\":33612,\"start\":33604},{\"end\":33624,\"start\":33612},{\"end\":34059,\"start\":34045},{\"end\":34072,\"start\":34059},{\"end\":34282,\"start\":34261},{\"end\":34297,\"start\":34282},{\"end\":34315,\"start\":34297},{\"end\":34327,\"start\":34315},{\"end\":34337,\"start\":34327},{\"end\":34352,\"start\":34337},{\"end\":34371,\"start\":34352},{\"end\":34386,\"start\":34371},{\"end\":34395,\"start\":34386},{\"end\":34744,\"start\":34727},{\"end\":34900,\"start\":34887},{\"end\":34914,\"start\":34900},{\"end\":34929,\"start\":34914},{\"end\":34947,\"start\":34929},{\"end\":34966,\"start\":34947},{\"end\":34979,\"start\":34966},{\"end\":34995,\"start\":34979},{\"end\":35431,\"start\":35420},{\"end\":35458,\"start\":35431},{\"end\":35472,\"start\":35458},{\"end\":35847,\"start\":35837},{\"end\":35859,\"start\":35847},{\"end\":35870,\"start\":35859},{\"end\":35885,\"start\":35870},{\"end\":36113,\"start\":36101},{\"end\":36128,\"start\":36113},{\"end\":36142,\"start\":36128},{\"end\":36152,\"start\":36142},{\"end\":36594,\"start\":36584},{\"end\":36611,\"start\":36594},{\"end\":36619,\"start\":36611},{\"end\":36632,\"start\":36619},{\"end\":36653,\"start\":36632},{\"end\":36666,\"start\":36653},{\"end\":36680,\"start\":36666},{\"end\":36699,\"start\":36680},{\"end\":36705,\"start\":36699},{\"end\":37024,\"start\":37016},{\"end\":37033,\"start\":37024},{\"end\":37043,\"start\":37033},{\"end\":37369,\"start\":37354},{\"end\":37385,\"start\":37369},{\"end\":37397,\"start\":37385},{\"end\":37413,\"start\":37397},{\"end\":37427,\"start\":37413},{\"end\":37438,\"start\":37427},{\"end\":37452,\"start\":37438},{\"end\":37459,\"start\":37452},{\"end\":37816,\"start\":37808},{\"end\":37836,\"start\":37816},{\"end\":37854,\"start\":37836},{\"end\":37866,\"start\":37854},{\"end\":38154,\"start\":38144},{\"end\":38168,\"start\":38154},{\"end\":38183,\"start\":38168},{\"end\":38196,\"start\":38183},{\"end\":38214,\"start\":38196},{\"end\":38232,\"start\":38214},{\"end\":38245,\"start\":38232},{\"end\":38259,\"start\":38245},{\"end\":38271,\"start\":38259},{\"end\":38281,\"start\":38271},{\"end\":38291,\"start\":38281},{\"end\":38765,\"start\":38753},{\"end\":38780,\"start\":38765},{\"end\":39006,\"start\":38989},{\"end\":39023,\"start\":39006},{\"end\":39034,\"start\":39023},{\"end\":39438,\"start\":39421},{\"end\":39454,\"start\":39438},{\"end\":39473,\"start\":39454},{\"end\":39797,\"start\":39785},{\"end\":39815,\"start\":39797},{\"end\":39830,\"start\":39815},{\"end\":39848,\"start\":39830},{\"end\":39866,\"start\":39848},{\"end\":39883,\"start\":39866},{\"end\":39902,\"start\":39883},{\"end\":40271,\"start\":40257},{\"end\":40286,\"start\":40271},{\"end\":40302,\"start\":40286},{\"end\":40314,\"start\":40302},{\"end\":40329,\"start\":40314},{\"end\":40343,\"start\":40329},{\"end\":40357,\"start\":40343},{\"end\":40377,\"start\":40357},{\"end\":40775,\"start\":40763},{\"end\":40785,\"start\":40775},{\"end\":41077,\"start\":41068},{\"end\":41096,\"start\":41077},{\"end\":41111,\"start\":41096},{\"end\":41130,\"start\":41111},{\"end\":41142,\"start\":41130},{\"end\":41157,\"start\":41142},{\"end\":41175,\"start\":41157},{\"end\":41514,\"start\":41497},{\"end\":41528,\"start\":41514},{\"end\":41708,\"start\":41690},{\"end\":41719,\"start\":41708},{\"end\":41742,\"start\":41719},{\"end\":42092,\"start\":42077},{\"end\":42107,\"start\":42092},{\"end\":42127,\"start\":42107},{\"end\":42139,\"start\":42127},{\"end\":42155,\"start\":42139},{\"end\":42166,\"start\":42155},{\"end\":42510,\"start\":42496},{\"end\":42525,\"start\":42510},{\"end\":42831,\"start\":42800},{\"end\":42842,\"start\":42831},{\"end\":42858,\"start\":42842},{\"end\":42870,\"start\":42858},{\"end\":43357,\"start\":43318},{\"end\":43372,\"start\":43357},{\"end\":43384,\"start\":43372},{\"end\":43394,\"start\":43384},{\"end\":43900,\"start\":43886},{\"end\":43915,\"start\":43900},{\"end\":43930,\"start\":43915},{\"end\":43941,\"start\":43930},{\"end\":44293,\"start\":44274},{\"end\":44306,\"start\":44293},{\"end\":44322,\"start\":44306},{\"end\":44647,\"start\":44629},{\"end\":44657,\"start\":44647},{\"end\":44665,\"start\":44657},{\"end\":44682,\"start\":44665},{\"end\":44700,\"start\":44682},{\"end\":44709,\"start\":44700},{\"end\":44724,\"start\":44709},{\"end\":44741,\"start\":44724},{\"end\":44756,\"start\":44741},{\"end\":44775,\"start\":44756},{\"end\":45160,\"start\":45146},{\"end\":45175,\"start\":45160},{\"end\":45189,\"start\":45175},{\"end\":45207,\"start\":45189},{\"end\":45225,\"start\":45207},{\"end\":45706,\"start\":45692},{\"end\":45725,\"start\":45706},{\"end\":45744,\"start\":45725},{\"end\":45756,\"start\":45744},{\"end\":45765,\"start\":45756},{\"end\":45782,\"start\":45765},{\"end\":45793,\"start\":45782},{\"end\":46162,\"start\":46149},{\"end\":46176,\"start\":46162},{\"end\":46353,\"start\":46334},{\"end\":46370,\"start\":46353},{\"end\":46387,\"start\":46370},{\"end\":46403,\"start\":46387},{\"end\":46425,\"start\":46403},{\"end\":46737,\"start\":46718},{\"end\":46746,\"start\":46737},{\"end\":46760,\"start\":46746},{\"end\":46777,\"start\":46760},{\"end\":46789,\"start\":46777},{\"end\":46808,\"start\":46789},{\"end\":46823,\"start\":46808},{\"end\":46842,\"start\":46823},{\"end\":46861,\"start\":46842},{\"end\":47295,\"start\":47281},{\"end\":47304,\"start\":47295},{\"end\":47318,\"start\":47304},{\"end\":47335,\"start\":47318},{\"end\":47346,\"start\":47335},{\"end\":47355,\"start\":47346},{\"end\":47625,\"start\":47611},{\"end\":47633,\"start\":47625},{\"end\":47637,\"start\":47633},{\"end\":47651,\"start\":47637},{\"end\":47897,\"start\":47883},{\"end\":47905,\"start\":47897},{\"end\":47909,\"start\":47905},{\"end\":47917,\"start\":47909},{\"end\":48144,\"start\":48128},{\"end\":48158,\"start\":48144},{\"end\":48171,\"start\":48158},{\"end\":48188,\"start\":48171},{\"end\":48201,\"start\":48188},{\"end\":48216,\"start\":48201},{\"end\":48231,\"start\":48216},{\"end\":48249,\"start\":48231},{\"end\":48603,\"start\":48593},{\"end\":48614,\"start\":48603},{\"end\":48625,\"start\":48614},{\"end\":48641,\"start\":48625},{\"end\":48660,\"start\":48641},{\"end\":48987,\"start\":48977},{\"end\":48999,\"start\":48987},{\"end\":49015,\"start\":48999},{\"end\":49039,\"start\":49015},{\"end\":49045,\"start\":49039},{\"end\":49367,\"start\":49356},{\"end\":49385,\"start\":49367},{\"end\":49401,\"start\":49385},{\"end\":49416,\"start\":49401},{\"end\":49425,\"start\":49416},{\"end\":49440,\"start\":49425},{\"end\":49457,\"start\":49440},{\"end\":49464,\"start\":49457},{\"end\":49937,\"start\":49924},{\"end\":49952,\"start\":49937},{\"end\":49966,\"start\":49952},{\"end\":49978,\"start\":49966},{\"end\":49990,\"start\":49978},{\"end\":50426,\"start\":50412},{\"end\":50443,\"start\":50426},{\"end\":50465,\"start\":50443},{\"end\":50476,\"start\":50465}]", "bib_venue": "[{\"end\":33292,\"start\":33230},{\"end\":33745,\"start\":33693},{\"end\":35613,\"start\":35551},{\"end\":36293,\"start\":36231},{\"end\":37184,\"start\":37122},{\"end\":39175,\"start\":39113},{\"end\":43011,\"start\":42949},{\"end\":43553,\"start\":43482},{\"end\":45366,\"start\":45304},{\"end\":47002,\"start\":46940},{\"end\":49605,\"start\":49543},{\"end\":50131,\"start\":50069},{\"end\":31435,\"start\":31383},{\"end\":31764,\"start\":31704},{\"end\":32042,\"start\":31952},{\"end\":32412,\"start\":32308},{\"end\":32787,\"start\":32684},{\"end\":33228,\"start\":33151},{\"end\":33691,\"start\":33624},{\"end\":34043,\"start\":33956},{\"end\":34471,\"start\":34411},{\"end\":34786,\"start\":34760},{\"end\":35099,\"start\":35011},{\"end\":35549,\"start\":35472},{\"end\":35937,\"start\":35885},{\"end\":36229,\"start\":36152},{\"end\":36582,\"start\":36500},{\"end\":37120,\"start\":37043},{\"end\":37545,\"start\":37475},{\"end\":37915,\"start\":37866},{\"end\":38372,\"start\":38291},{\"end\":38817,\"start\":38780},{\"end\":39111,\"start\":39034},{\"end\":39522,\"start\":39473},{\"end\":39951,\"start\":39902},{\"end\":40415,\"start\":40377},{\"end\":40841,\"start\":40785},{\"end\":41213,\"start\":41175},{\"end\":41495,\"start\":41443},{\"end\":41790,\"start\":41742},{\"end\":42075,\"start\":41977},{\"end\":42569,\"start\":42525},{\"end\":42947,\"start\":42870},{\"end\":43480,\"start\":43394},{\"end\":43996,\"start\":43941},{\"end\":44374,\"start\":44322},{\"end\":44815,\"start\":44775},{\"end\":45302,\"start\":45225},{\"end\":45845,\"start\":45793},{\"end\":46147,\"start\":46097},{\"end\":46461,\"start\":46425},{\"end\":46938,\"start\":46861},{\"end\":47422,\"start\":47371},{\"end\":47725,\"start\":47667},{\"end\":47970,\"start\":47933},{\"end\":48298,\"start\":48249},{\"end\":48698,\"start\":48660},{\"end\":49097,\"start\":49045},{\"end\":49541,\"start\":49464},{\"end\":50067,\"start\":49990},{\"end\":50528,\"start\":50476}]"}}}, "year": 2023, "month": 12, "day": 17}