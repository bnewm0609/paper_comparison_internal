{"id": 261242415, "updated": "2023-10-24 13:07:15.447", "metadata": {"title": "SuperUDF: Self-supervised UDF Estimation for Surface Reconstruction", "authors": "[{\"first\":\"Hui\",\"last\":\"Tian\",\"middle\":[]},{\"first\":\"Chenyang\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Yifei\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Learning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces. We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling. The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP). The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF. Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently. A novel regularization loss is proposed to make SuperUDF robust to sparse sampling. Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs. Extensive evaluations demonstrate that SuperUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency. Code url is https://github.com/THHHomas/SuperUDF.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2308.14371", "mag": null, "acl": null, "pubmed": "37738188", "pubmedcentral": null, "dblp": "journals/corr/abs-2308-14371", "doi": "10.1109/tvcg.2023.3318085"}}, "content": {"source": {"pdf_hash": "f0f8471873132d02c18d030aaf72cd21988e657e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.14371v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2308.14371", "status": "GREEN"}}, "grobid": {"id": "c3c5b6c788fb887b163b5fdbb16951c29f11e4e8", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/f0f8471873132d02c18d030aaf72cd21988e657e.txt", "contents": "\nSuperUDF: Self-supervised UDF Estimation for Surface Reconstruction\n22 Oct 2023\n\nHui Tian \nChenyang Zhu \nYifei Shi \nKai Xu \nSuperUDF: Self-supervised UDF Estimation for Surface Reconstruction\n22 Oct 20235C9986848630B10E71C39D0E8A0774D3arXiv:2308.14371v2[cs.CV]UDFPoint Cloud ReconstructionImplicit Surface\nLearning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces.We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling.The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP).The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF.Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently.A novel regularization loss is proposed to make SuperUDF robust to sparse sampling.Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs.Extensive evaluations demonstrate that Su-perUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency.Code url is https://github.com/THHHomas/SuperUDF.\n\nI. INTRODUCTION\n\nSurface reconstruction from 3D point clouds has been a long-standing problem in graphics and vision.Since the seminal work of Poisson surface reconstruction [1], there have been a large body of literature [2].Albeit relying on normal as input, the idea of reconstruction based on implicit field has been inspiring many deep learning methods [3], [4], [5], [6], [7].Signed Distance Function (SDF) is a typical implicit representation of 3D shapes of arbitrary geometry and topology.Many deep learning models have been proposed to predict the SDF of a 3D point cloud for high-quality reconstruction [3], [4], [6], [8].However, a major drawback of SDF is that it can represent only closed and watertight surfaces due to its nature of inside/outside discrimination, so SDF-based methods find difficulty in handling open surfaces such as garments or incomplete scans.\n\nUnsigned Distance Function (UDF) is suited for representing open surfaces since it does not differentiate between inside and outside.Compared to SDF, UDF is easier to learn since it concerns only distance information and ignores the sign.The pioneering work of [9] proposes a direct rendering method based on UDFs.However, both this work and its followups [10], [11] require strong supervision of ground-truth UDFs.Moreover, surface extraction from UDFs is difficult due to the absence of signs which is critical to marching cube [12].[13] convert UDFs to meshes via elevating a UDF to an SDF which brings back the limitation of SDFs.\n\n1, National University of Defense Technology PSR NDF SuperUDF GT Input CAP Fig. 1.A scene from ScanNet [14] reconstructed with PSR [1], NDF [9] , CAP [15] and our SuperUDF.\n\nRecently, [15] introduces a self-supervised pipeline to learn smooth UDFs directly from raw point clouds.Their network is trained in a shape-specific manner, requiring \u223c20 minutes for overfitting one shape.Furthermore, the method is sensitive to the sampling density of the input point cloud; the prediction of field gradients is inaccurate for sparsely sampled point clouds.Inspired by recent works [5], [4] demonstrating that geometry priors learned from large datasets benefit efficient reconstruction, we propose SuperUDF, a self-supervised learning method for fast UDF estimation with learned priors.To obtain robustness under sparse sampling, we introduce a novel regularization loss to train SuperUDF.We also introduce a learning-based mesh extraction directly from the estimated UDFs.\n\nThe core idea of our self-supervised UDF estimation draws inspiration from the seminal surface approximation operator of Locally Optimal Projection (LOP) [16].In particular, we first upsample the input point cloud via per-point duplication and random perturbation.The key insight behind our design is that if the UDF is estimated correctly, the upsampled points should be locally projected onto the underlying (ground-truth) surface following the gradient of the UDF.Since the ground-truth surface is unknown a priori, we instead require the projected points to approximate the input point cloud.\n\nTo do so, we impose a direct constraint on the estimated UDF for a better data approximation by minimizing the mean UDF value of all points of the input point cloud.However, imposing the approximation constraints alone may lead to noisy UDFs where the monotonicity of UDF at one side of the zero-level-set may be violated.To this end, we devise a regularization loss to ensure UDF monotonicity.In particular, for each upsampled point, denoted by p, we compute its offset point q by moving p along UDF gradient for a distance of half of p's UDF value.Here, q is expected to reside at the same side as p about the surface.We impose that the UDF gradients at the two points are the same and the UDF value at q should be half of that of p.This design also makes the points projected more uniformly.\n\nA difficulty in the method, however, is that the computation of UDF gradients in the losses above is time-consuming, making the training intractable.We thus opt for the reverse.We instead estimate a projection vector for each upsampled point.This results in a projection flow as an approximation of the UDF gradients.The projection flow network can also be trained in a self-supervised manner using the constraints above.The resulting projection flow is a strong geometry prior learned from a shape dataset which greatly improves training efficiency.The UDF can be easily obtained from the projection flow based on their duality.\n\nTo learn mesh extraction from UDFs, we partition the 3D space into a regular grid and train a 3D CNN for each grid cell to estimate the signs of its 8 corners.Note that we do not care about the absolute signs being positive or negative, but only concern with their relative signs.In practice, we fix the sign of one corner and let the network predict the rest.Since the network is trained for local sign prediction, it is easy to train and generalizes well across different shapes.\n\nExtensive evaluations demonstrate that SuperUDF outperforms the state of the arts on ShapeNet, MGN and Scannet in terms of both quality and efficiency.The main contributions of our work are:\n\n\u2022 A self-supervised UDF estimation network inspired by the classical surface approximation operator of Locally Optimal Projection (LOP).\u2022 A number of inductive biases on UDF geometry and a pre-learned geometry prior for efficient learning.\u2022 A novel regularization loss to make SuperUDF robust to sparse sampling.\u2022 A learning-based mesh extraction method that generalizes well across shapes.\n\nII. RELATED WORK a) Traditional point cloud surface reconstruction: Point cloud reconstruction has been a long-standing task in graphics and vision.The most important traditional method is Poisson surface Reconstruction [1] and ball-pivoting reconstruction [17].The former method classifies the query point according to the Poisson indicator function.The latter constructs the continuous surface by making up a ball rolling on the points.Those two methods can reconstruct pretty good surfaces, however, the performance can be improved more.\n\nb) SDF-based implicit surface reconstruction: Deep methods based on SDF always classify the occupancy of query points or directly regress the SDF value, which can be divided into local methods, global methods, and a combination.The global method, as the name implies, when giving a query point, classifies the query according to the whole shape information.The local method classifies the query point according to its neighbor points of it.Representative global methods are DeepSDF [3], BSP-Net [18].The routine of those methods is extracting the feature code of the whole shape and then recovering the surface from the code.Representative local methods are ConvOccNet [4], SSR-Net [19], DeepMLS [3] and POCO [8].ConvOccNet [4] first converts the point cloud feature into voxels and then applies volume convolution to enhance the feature of every voxel.SSR-Net [19] firstly extracts point feature, then maps the neighborhood points feature to octants, and finally classifies the octants.DeepMLS [20] tries to predict the normal and radius of every point, then classify the query points according to the moving least-squares equation.Furthermore, except for local methods and global methods, there are some methods that try to combine global and local information.The most representative method is Points2Surf [6].It tries to regress the absolute SDF value according to local information and classifies the sign according to global information.Another implicit surface reconstruction methods based on SDF are SAL [21], SALD [22] and On-Surface Prior [23].This kind of method aims to convert explicit representation, such as point cloud and 3D triangle soup, to implicit SDF representation.Thus, every 3D model needs a unique training process and unique network parameters.SAL [21] uses MLP to predict the SDF of shapes but adopts the UDF-based metric to supervise the network training, SALD [22] follows the SAL and adds derivative regularization term.On-surface Prior [23] use a pre-trained UDF-based network to help the main network to predict better SDF.\n\nThe SDF methods mentioned above achieve excellent progress in point cloud reconstruction.However, SDF representation has its weakness.It is hard to represent an open surface or partial scan.Furthermore, all the deep methods mentioned above needs 3D ground truth as the training label, while 3D ground truth is expensive for closed shape and real scan.\n\nc) UDF-based implicit surface reconstruction: UDF can express more general surfaces, such as open surfaces and partial scans.Lots of works have focus on the UDF representation.NDF [9] uses UDF to represent the surface, then they propose a point cloud up-sample method while reconstructing the mesh not directly from UDF but up-sampled point cloud via Ball Pivoting [17] method.DUDE [24] represent the shape with a volumetric UDF encoded in a neural network.Those works bring UDF to deep implicit surface reconstruction area and achieve good results.However, they need the 3D ground truth as the training label.What's more, they remain an open problem on how to directly extract iso-surface from UDF. UWED [25] makes use of MLS ( Moving Least Squares ) to convert the UDF to dense point cloud.d) Self-supervised method for surface reconstruction: Besides the supervised methods mentioned above, there are a few self-supervised approaches.For example, Neural Pulling [7] proposes a pipeline to train a network such that the network can predict SDF in the whole space without any extra supervision.However, the method has several disadvantages.First, it needs a dense and complete point cloud as supervision, limiting its scalability in real-world scenarios.Second, the method is only capable of reconstructing objects with closed surfaces.Thus, CAP [15]  predicting UDF rather than SDF so it can represent open surface.But it also suffers from the requirements of dense point cloud and long inference time.e) Iso-surface extraction: For SDF, the most common method for iso-surface extraction is Marching Cube [12].It is a template matching method based on the sign of eight grid corners.For iso-surface extraction on UDF, there are only a few works.MeshUDF [13] uses a neural network to vote the sign of the grid corner while bringing some sign conflict.The two methods try to convert the UDF to SDF.But SDF has difficulty representing the open surface.They are meaningful exploitation of iso-surface extraction on UDF.While sharing the same drawback of SDF and weak generalization ability.\n\n\nIII. METHOD\n\n\nA. Overview\n\nSuperUDF reconstructs the surface by estimating the mesh from a point cloud.The whole pipeline contains two main parts: UDF estimation and iso-surface extraction.In sec.III-B, we describe the self-supervised UDF estimation network based on the inductive biases on UDF geometry.This part is shown in Fig. 2 In sec.III-C, we introduce a learning-based mesh extraction from the estimated UDFs.This part is illustrated in Fig. 5.\n\n\nB. Self-supervised UDF Estimation\n\nWe propose a method to estimate the UDF from a point cloud P. As shown in Fig. 2, the method starts by upsampling the input point cloud P via per-point duplication and perturbation, resulting in an upsampled point cloud P upsample .The network then estimates the UDF in an implicit manner: it takes the coordinate of an upsampled point as input and outputs its UDF value, as well as its UDF gradient.The details of the UDF estimation network is illustrated in Fig. 3\n\n\n\ud835\udc5e\n\nPer-point UDF flow estimation Fig. 3.The UDF estimation network.Given the input point cloud P and the upsampled point cloud P upsample , the network estimates the per-point UDF projection flow in two stages with the guidance of the predicted approaching vector and the point-wise feature, respectively.perceptive point transformer consists of four Point Transformer layers [26], which is shallow and has a perceptive field.For a point cloud P = {p i } N 1 , we adopt the local perceptive point transformer to extract the point-wise features:\nf (l) i = pj \u2208N (pi) PointTransformer(f (l\u22121) j , f (l\u22121) i ),(1)\nwhere PointTransformer(\u2022) is the Point Transformer layer [26],\nf (l) i\nis the point feature of p i at the l-th layer, N (p i ) is the neighboring point set of p i .We compute the neighboring point set by using a KNN with the number of neighboring points being 36.\n\nb) UDF projection flow estimation: A straightforward solution to estimate the point-wise UDF is to aggregate features at each upsampled point and output its UDF with a neural network.For any point, the UDF gradient can then be computed by estimating the dense point-wise UDFs in the neighboring region of that point.This process is computationally time-consuming, making network training infeasible.To solve the problem, We propose to directly estimate a projection vector for each upsampled point.This results in a projection flow as an approximation of the UDF gradients.The UDF can be obtained from the projection flow based on their duality.\n\nThe projection vector which displaces the upsampled point to the surface is estimated in two stages (Fig. 4 a).\n\nFirst, for each upsampled point q, we compute a coarse projection vector by aggregating the estimated approaching vector of the neighboring input points:\ns 1 = 1 k px\u2208N (q) < p x \u2212 q, a x > a x ,(2)\nwhere N (q) is the neighboring input points to q, k is the number of neighboring points, < \u2022 > is the inner-product operation.a x is an approaching vector of input point p x which is estimated by:\na x = MLP a (f (4) x ),(3)\nwhere MLP a (\u2022) is the multi-layer perceptron, f\n\nis the extracted feature of p x .Although we did not provide any direct supervision, we found the learned approaching vector is very similar to the normal vector.Please refer to the supplemental materials for the visualization.The above process displaces point q such that the displaced position q = q + s 1 is close to the underlying surface.\n\nSecond, to further optimize the projection flow, we estimate a refining projection vector with a neural network.To achieve this, we first fetch the features of the neighboring input points N (q) at the displaced position q and interpolate its feature:\nf q = px\u2208N (q) [MLP f (p x \u2212 q) \u2022 f (4) x ],(4)\nwhere MLP f (\u2022) is the multi-layer perceptron, \u2022 is the elementwise multiplication.The refining displacement vector can be estimated by:\ns 2 = \u03b2Tanh[MLP d (f q )](5)\nwhere Tanh(\u2022) is the hyperbolic tangent function, MLP d (\u2022) is the multi-layer perceptron, \u03b2 is a scale factor being 0.01.As a result, the projection flow at point q is s 1 + s 2 , and the estimated projection of point q on the underlying scene surface is q = q + s 1 + s 2 .c) Self-supervised network training: If the UDF projection flow is estimated correctly, the upsampled points should be locally projected onto the underlying surface following the flow.Since the underlying surface is unknown a priori, we require the projected points P project to approximate the input point cloud P instead.A chamfer distance loss between P and P project is utilized:\nL chamfer = ChamferDistance(P, P project ).(6)\nMeanwhile, we devise a direct constraint on UDF for an intra-consistency by minimizing the mean UDF value of all points of the input point cloud:\nL intra = p\u2208P UDF(p),(7)\nwhere UDF(\u2022) is the UDF value, it is computed by taking the L2 norm of the estimated UDF projection flow.\n\nImposing the above constraints would lead to noisy UDFs where the monotonicity of UDF at one side of the zero-value surface may be broken.To this end, we introduce an extra loss to ensure the monotonicity of the UDF (Fig. 4 b).For each upsampled point q \u2208 P upsample , we compute its offset point q \u2032 by moving q along UDF gradient for a distance of half of q's UDF value.q \u2032 is expected to reside at the same side as q about the surface.The UDF gradient at the two points should be the same and the UDF value at q \u2032 should be half of that of q.We feed q and q \u2032 into the UDF projection flow estimation network, respectively.The network is trained in a siamese fashion.It optimizes the inter-consistency between the two estimated UDFs:\nL inter = q\u2208Pupsample (|0.5 \u2022 UDF(q) \u2212 UDF(q \u2032 )| +\u2225\u2206UDF(q) \u2212 \u2206UDF(q \u2032 )\u2225 2 ),(8)\nwhere \u2206UDF(\u2022) is the UDF projection flow.To sum up, the total loss is: L = w chamfer L chamfer + w inter L inter + w intra L intra , where w chamfer , w intra , and w inter are the pre-defined weights.We set w chamfer = 10, w intra = 1, and w intra = 1,\n\n\nC. Learning-based Mesh Extraction\n\nNext, we describe how to extract the mesh surface based on the estimated UDF.The marching cube algorithm has proven to be extremely effective in extracting iso-surface from signed distance fields (SDFs).However, since no sign is provided to distinguish the inner and outer of the surface by UDFs, directly applying the conventional marching cube algorithm is infeasible.Therefore, we propose a learning-based UDF sign estimation method to solve this problem.CAP [15] also provide a method to reconstruct mesh, they have the sign conflict problem in theory.\n\nThe method starts by dividing the 3D space into H 3 voxels and calculating the UDF value and UDF gradient direction of every vertex.For each h 3 (h = 5) regular grid of overlapping local regions, we propose a local sign estimation network to predict the sign of the vertices.To be specific, as Fig. 5 shown, the network backbone is stacked by several 3D Convolution layers for UDF-sensitive feature extraction.By feeding the estimated UDF and positional encoding [27] of the grid vertices into the network, we make binary classifications for each vertex, indicating the sign of UDFs.Note that only the relative sign of the UDFs is needed.As a result, we train the network using a modified binary cross-entropy loss:\nL = c\u2208C min( v\u2208V BC(y c v , \u1ef9c v ), v\u2208V BC(\u2212y c v , \u1ef9c v )), (9)\nwhere C is the grid set, V is vertex set.BC is the binary crossentropy loss function, y c v is the predicted sign at vertex v of grid c, and \u1ef9 \u2208 {0, 1} is the ground-truth sign.In practice, we fix the sign of one corner and let the network predict the rest.\n\nNote that, since the network is trained for local sign prediction, it is easy to train and generalizes well across different shapes.We show the cross-category and cross-dataset generality of our method in the ablation study.\n\n\nD. Discussion\n\na) Approaching Vector Explanation: First, we need to explain why we need approaching vector and why we can learn the approaching vector.The mechanism of approaching vector is shown in Fig. 11 (a).Let's imagine a local plane P with ground-truth normal n.On the one hand, if the approaching vector is on the line of the ground-truth normal, we can project the points around the surface, according to Eq. ( 2), to the plane P .On the other hand, if the approaching vector is not on the line of the ground-truth normal, the points around the surface will not be projected to the surface and the chamfer loss will be large.Thus, if we project the points around the surface according to Eq. ( 2), the chamfer loss will push the approaching vector close to the point normal.According to Eq. ( 2), if we flip the orientation of the approaching vector, the result shift s 1 do not change.Because s 1 is the even function of the approaching vector.Thus, it is possible that our approaching vector can be close to the normal.We also provide experiments validation in Sec.IV-E.\n\n\nE. Difference between CAP and Neural Pull\n\nIn the field (SDF or UDF) prediction stage, the two previous methods do not need the label of query point.It is Neural Pull [7] and CAP [15].The former is the pioneer work designed for SDF prediction.The latter is designed for UDF prediction.Our method is based on UDF and do not need label of query point as well.Therefore, we compare the difference between ours and the two previous works.\n\nFirst, our method and CAP [15] can represent open surface and close surface.While Neural Pull [7]  UDF while Neural Pull is based on SDF.Second, Neural Pull and CAP have to train the network for every shape which is time-consuming.While ours can predict the UDF fast at inference stage which is much faster.The reason is that we learn a prior with input as condition, similar to Occupancy Network [5].While CAP and Neural Pull adopt a pipeline similar to SAL [21].The pipeline of CAP [15], Neural Pull [21] and SAL [21] is easier to obtain continuous SDF or UDF, because neural network is prone to be a continuous function with the cost of longer time at inference stage .Third, due to the existence of regularization Eq. ( 8), we can make the UDF near surface more reasonable, thus ours can work better on relatively sparse point cloud.While Neural Pull and CAP need dense point as input.This is shown in Tab.I.The total time consumption can be found in Appendix.\n\nF. Details of of UDF Estimation Network a) Network: The network is composed of 4 Point Transformer layers [26], the output width of each layer is 32, 128, 256 and 256.The number of k-nearest points is 36.Through the 4 layers, we can obtain per-point feature.Then we can use a MLP to predict the approaching vector and calculate the shifted points with s 1 according to Eq. ( 2).Next, we calculate the refined shift s 2 with a learning based interpolation method in Eq. ( 4) followed by MLP.\n\nb) Implementation Details: During training, we adopt cosine learning rate adjustment [28], the initial learning rate is 0.001.We use Adam [29] optimizer to learn the parameter.The batch-size is 2. The query point for training is obtained by randomly sampling point around the original point with the uniform distribution U (\u22120.03, 0.03).During testing, to obtain the UDF in the whole space, we first partition the space into 256 3 voxels.We use the network to predict the UDF of the voxel center.\n\n\nIV. EXPERIMENTS\n\nA. Experiments Settings a) Benchmarks: We evaluate our method on 3 widely used open-source datasets.The first is ShapeNet [30], we random choose 1300 shapes from 13 commonly used categories, every categories with 100 shapes.The second is the MGN dataset, including pants and shirts.The third ScanNet [14], a real scene from scanning, we choose 100 scenes from it.To comprehensively compare our results with other methods, we make use of the most commonly used metrics in surface reconstruction for comparison.CD 1 is Chamfer-L 1 , a set operation which measures distance of two point sets.In short,  we show how we make use of chamfer distance to constrain the training.CD 1 equation is shown as the Eq.( 10),\nCD 1 = 1 2N x Nx i=1 \u2225x i \u2212 S y (x i )\u2225 1 + 1 2N y Ny i=1 \u2225y i \u2212 S x (y i )\u2225 1 , (10){x i , i = 0 \u2022 \u2022 \u2022 N x } is the points set sampled from ground- truth mesh. {y i , i = 0 \u2022 \u2022 \u2022 N y }\nis the point set sampled from reconstructed mesh.S y (x i ) means the nearest point to x i in reconstructed point set.S x (y i ) means the nearest point to y i in ground-truth point set.\u2225 \u2022 \u2225 1 means L1 \u2212 distance.\n\n\nB. Close Surface Reconstruction\n\nThis part is a standard implicit surface reconstruction task, lots of previous works [3], [4], [5], [9] have adopted the same setting.We first evaluate our method by close shape reconstruction on ShapeNet [30].In detail, we uniformly sample 3000 points on the watertight mesh as input.Our method is self-supervised method, as well as Neural Pulling [7] and CAP [15] in Fig. 6 and Fig. 10.Others are supervised methods, which need ground-truth label of query point.As for PSR [1], we provide ground-truth normal as input.We can see that our method achieves state-of-the-art performance in almost all the 13 classes.SuperUDF can also outperform the supervised methods since our method can extract more implicit geometry information on less supervision, such as the UDF projection flow, for the surface reconstruction learning.\n\n\nC. Open Surface Reconstruction\n\nSince our pipeline adopts UDF to represent the implicit surface, SuperUDF can reconstruct open surface as well.We uniformly sample 3000 points on each MGN mesh, and reconstructing the implicit surface.SDF-based methods cannot represent the garment correctly because it is open surface.The quantitative result is shown in Tab.III, we can see that our method outperforms the supervised method NDF [9], PSR [1], CAP [15] and GIFS [31].The visualization result is shown in #points CAP [15] NDF [9] GIFS [31]   Fig. 7.We can see that our reconstructed mesh is closer to the ground-truth and retain more fine-grained details.\n\n\nD. Real Scene Reconstruction\n\nReal scene reconstruction is more challenging since the input point cloud is usually open and incomplete, while there is no reconstruction supervision for training.Our self-supervised method can train the network with the raw point cloud directly and does not rely on the synthetic dataset for supervision.We evaluate our method on ScanNet [14].For each scene, we sample 3000 or 10000 points as input.The visualization result is shown in Fig. 8.By comparing with the reconstructed ground truth, we can see that our method can achieve good result with fine-grained details.As for quantitative result, we sample 100000 points on our mesh and ground truth mesh, then calculate the CD 1 as the metric.The quantative result is shown in Tab.IV.\n\nWe provide some reasons of artefacts and an improvement direction.The artefacts at least comes from two aspects, the local discontinuities of the UDF gradient near the surface and the noise near the edge of open surface.For the first problem, it is due to the sudden UDF gradient direction changing near the surface, which is tightly connected with the UDF representation way.Therefore, the artefacts might be more than SDF-based method.However, UDF representation is still a very promising direction and the artefacts can be removed with the UDF accuracy increase.As for the second problem, the UDF gradient from two sides of the surface is not completely opposite to each other on the edge of the surface, it will lead to some noise during mesh extraction.To solve the second problem, we can design a half plane rather than a complete plane in Eq(2) to fit the edge of the open surface.\n\n\nE. Validation Experiments\n\na) The Explanation of 2 stage Learning Method: In fact, only the stage 1 and stage 2 can predict the UDF flow alone.Here, we analyze why we need the 2-stage pipeline.Let's consider if the local plane is a curve surface in Fig. 11 (b).Then the shift s 1 according to Eq. (2) will have a systematical error.The points around the surface will not be projected to the surface.Thus, we need a refinement stage to push the points near the surface to the surface exactly.Next, we explain distance to gt GIFS(S) Fig. 6.Visualized results of our method and state-of-the-art alternatives.Note that we provide the ground-truth normal as input for PSR as it requires.The color map in the figure is the error distribution of the reconstruction.Blue indicates low reconstruction error while yellow indicates higher error values.(S) mean supervised method and (U) means unsupervised method.Methods are ConvOcc [4], Onet [5], NP [7], CAP [15], GIFS [31].Max dist is 0.017.The number of input point cloud is 3000.\n\n\nGT Input PSR(w/ normal) NDF(S) Ours(U) CAP(U) GIFS(S)\n\nFig. 7. Visualized results of our method, traditional reconstruction methods PSR [1] and 3 deep method NDF [9], CAP [15], GIFS [31] on garment dataset.Note that we provide the ground-truth normal as input for PSR as it requires.\n\nThe number of input point cloud is 3000.\n\nwhy we do not use the second stage multiple times.In theory, neural network can fit any curve thus can move the query point to any curve.However, according to our experiments, the neural network is prone to move the query point to the input point cloud rather than evenly scattering the query point to the surface, as shown in second column in Fig. 12.If the query point moves to the position of input point cloud only, the network is over-fitting and the predicted UDF is obviously wrong.In summary, we cannot use one stage multiple times.In the meanwhile, two-stage method can help to reduce the systematic error in Fig. 2 (b) and avoid the query point moving to the position of input point cloud only.It is shown in Fig. 12.We can see that better upsampling result can be reached with 2 stages together.\n\nWe provide the ablation study results in Tab.V. We can see that stage-1 + stage-2 can outperform both stage-1 and stage-1+stage-2.In experiments, it proves the necessity of the two stage method.\n\nb) Approaching Vector Explanation: What's more, we provide the comparison result of approaching vector and ground truth normal visualization in Fig. 13.we can see that the visualization result is close to the ground-truth normal.What's more, we provide the quantitative results as well.The result is shown in Tab.VI. we can see that our approaching .Visualized results of our method and NDF [9], CAP [15] and GIFS [31].The number of input point cloud is 10000.\n\n\nUpsampled point cloud\n\n\nMesh\n\n\nGT UDF Slice\n\nWith inter-loss Without inter-loss  vector is close to the ground-truth normal.\n\n\nF. Robust Analysis of k in Stage-1\n\nIn the UDF projection flow estimation part, we propose a 2-stage way to predict the query point shift.In Eq. ( 2), k is an important parameter.Here, we provide the robust analysis of k in Tab.VII.We can see that when k varies, the result is robust.\n\n\nG. Ablation Study of the Regularization Term\n\nWe verify the effectiveness of the proposed inter-consistency loss in Fig. 9.We show the visualization result of up-sampled point cloud w/o the regularization term and 2D slice of UDF.By comparing to result w/o the regularization, we can see that our result is obviously better with the regularization.\n\nDue to the inter-loss which can regularize the UDF distribution around the surface, our self-supervised method can work on rather sparse input, otherwise the query point will shrink to the input point cloud, as shown in Fig. 9, thus the UDF will have lots of errors.\n\nHere we provide the result w/ and w/o inter-loss with different number of point cloud as input.By comparison, we can see that method with regularization term perform much better than method without regularization term.Meanwhile, we also compare ours with other methods with different number of point cloud as input, shown in Tab.X.We can see that our method with regularization can work well on both sparse and dense point cloud.The two comparisons show the effectiveness of the regularization term.\n\nAs for the reason why our method without regularization term is worse than the CAP [15] and Neural Pull [7], because the predicted UDF in our pipeline is based on local feature, while NP and CAP model one shape with one neural network, thus ours are harder to obtain continuous UDF or SDF without regularization.However the cost of their pipeline is more time at inference stage.\n\n\nH. Mesh Extraction Comparison\n\nIn order to compare the performance of our mesh extraction part and the MeshUDF [13], we design two experiments, mesh distance to gt GIFS(S) Fig. 10.Visualized results of our method and state-of-the-art alternatives.Note that we provide the ground-truth normal as input for PSR as it requires.The color map in the figure is the error distribution of the reconstruction.Blue indicates low reconstruction error while yellow indicates higher error values.(S) mean supervised method and (U) means unsupervised method.Methods are ConvOcc [4], Onet [5], NP [7], CAP [15], GIFS [31].Approaching Vector GT Fig. 13.The visualization result of approaching vector.The color is mapped from approaching vector.We can see that the approaching vector is close to the ground-truth normal.\n\nextraction on ground truth UDF (clean) and mesh extraction on predicted UDF (noisy).The UDF is predicted by our UDF prediction network.In order to show that the good performance is not only on one dataset, we show the result on 2 datasets, ShapeNet, MGN.Specifically in Tab.VIII, we can   [13].\n\nSHAPENET(V) MEANS VESSEL CLASS OF SHAPENET.\n\n\nMeshUDF\n\nOurs GT see that our method show better performance than MeshUDF on both datasets and on both GT and Predicted UDF.What's more, we provide the visualization result of the mesh extracted my ours and MeshUDF in Fig. 14.It is consistent with the result on Tab.VIII.All experiments is with resolution 256 3 .\n\n\nI. Generalization of Iso-Surface Extraction\n\nWe define the metric acc for evaluating the generalization ability.Detail definition is in Appendix.We only choose the cubes near the surface for mesh extraction.In detail, the distance from the center of the cube to the nearest point in the point cloud should be less than 0.03.\n\nWe demonstrate that our method can be trained and tested on different datasets as shown in Tab.IX.We can see that the test accuracy will not decrease much if the network is trained on a different small-scale dataset.The reason is the repeatability of micro structures as discussed.\n\n\nJ. More Result of Surface Reconstruction\n\nWe also provide the result on FAMOUS dataset released by Points2Surf [6].The result is shown in Fig. 15.We can see that our method can reconstruct the surface with more geometry details.\n\n\nV. CONCLUSION AND FUTURE WORK\n\nIn this work, we propose the self-supervised pipeline to reconstruct the implicit surface based on UDF with a regularization term.Through this pipeline, we can reconstruct good UDF field.In order to reconstruct mesh directly from UDF, we propose a learning-based iso-surface extraction and achieve good result.However, one weakness is the edge of the open surface, our method usually generates some noise.Because the UDF distribution on the open surface edge is hard to handle.Our next work will focus on improving reconstruction quality of the surface edge.Currently, almost all the learning methods are supervised methods.However the 3D ground truth is very expensive to acquire.Further more, the experiment can only be done on synthesis dataset, such as ShapeNet [30], because real world dataset usually do not contain perfect ground truth surface.To apply implicit surface reconstruction from point cloud to real world dataset.One way [19] is training the network on synthesis dataset and generalize the method to real world dataset.However, this idea is also restricted when point cloud density and local shape is much dissimilar to the training data, the performance will degrade as well.Another idea is training the implicit surface reconstruction network in an unsupervised manner.Then, the method will not be disturbed by above problems.Because we can train the network on target dataset even if the target dataset do not have 3D ground truth surface.When we want to modify the original supervised pipeline to unsupervised one, in Fig. 16 (a), we have to overcome two problem.Marching Cube and point cloud sampling is not differentiable.Although we can follow the method of deep marching cube [32], we cannot find a light differentiable sampling method on mesh.However, if we consider the two non-differentiable methods together, we can design a method directly sampling point cloud on UDF implicit surface.That is shifting the query point along the UDF gradient direction and forwarding the distance of UDF value.Then the query point will reach the zero-level set of the UDF field.The pipeline is shown in Fig. 16 (b).Thus, we can train the network with chamfer loss.\n\n\nB. Evaluation Metric\n\nWe also provide the quantitative result with modified normal consistency as in Eq. (11),\nNC = 1 2N x Nx i=1 | < n (x i ), n(S y (x i )) > |+ 1 2N y Ny i=1 | < n (y i ), n(S x (y i )) > |.(11)\nwhere {x i , i = 0 \u2022 \u2022 \u2022 N x } is the points set sampled from ground-truth mesh.{y i , i = 0 \u2022 \u2022 \u2022 N y } is the point set sampled from reconstructed mesh.S y (x i ) means the nearest point to x i in reconstructed point set.S x (y i ) means the nearest point to y i in ground-truth point set.| \u2022 | means absolute value.n(x) means the normal of point x.<, > means inner-product.\n\n\nC. Effectiveness Comparison\n\nCAP [15] and Neural Pull [7] directly predict the distance function with coordinate of query point as input and point cloud is only used as loss function during training, thus they need training the network even in test time.Ours first calculate point-wise feature, then for every query point, we calculate its feature and predict the distance function.Thus, ours do not need train the network during test time, that's the reason why ours is faster than CAP-UDF and Neural Pull in distance prediction.However, during mesh extraction stage, because\n\n\nD. Slice Visualization\n\nIn order to show what the network indeed learn, we visualize the direct output of the network, UDF slices along x, y and z axis.We visualize the UDF in 2D slices in Fig. 17.We can see that in all the 3 slices from x-axis, y-aixs or z-aixs, the UDF is close to 0 near the surface, gradually increases when the distance to surface is larger.The relative error between our prediction and the ground truth is only 3% which demonstrate that the network indeed learns correct UDF distribution around the surface.\n\n\nE. Learning-based Iso-surface Extraction\n\n1) The Motivation of Learning based Iso-surface Extraction: First, we introduce a key observation of mesh reconstruction.Let's take a look at Marching Cube.It is a template match method.For every cube, according to the sign of the eight corners, it can generate a corresponding mesh in the cube.However, if we flip the sign of the eight corners at the same time, the only change of the generated mesh is that the normal of the triangulation is flipped.In most occasions, we do not need global consistent triangulation orientation if our aim is to recover the shape of the implicit surface.Thus, we can make a minor sacrifice of the reconstructed mesh, giving up the globally consistent triangulation orientation, to make the sign prediction much easier.\n\nNext, we introduce the evolution of our learning based iso-surface extraction idea.Inspired by [33], we want to expand their 1D quadratic function on the edge to 3D quadratic function in the space to overcome the sign conflict problem in [33].However, due to the noise, it is not a very stable solution.\n\nThus, we want to treat the sign prediction problem as a classification problem by giving up the curve as a middle result.Luckily, the UDF has already been voxelized and can be easily processed by standard 3D convolution network.Therefore, we just make use of 3D convolution to extract the feature of 8 corner vertices in the cube.We can not only make use of the function value, but also make use of the partial derivatives to make the curve fitting more reliable.Thus, inspired by the curve fitting process, the network input not only include the UDF value, but also the gradient.Further more, we can add more information by adding sample points in the cube to improve the input feature quality.Thus, our final input for network is a 4 \u00d7 5 \u00d7 5 \u00d7 5 cube.The first channel include 1-dimension UDF value and 3-dimension UDF gradient.\n\nIf we adopt a learning method, we can treat the sign predict problem as classification problem.Considering the key observation mentioned above, we have to handle the relative sign prediction problem in learning method pipeline.In detail, if the network predicts the eight corner's sign which is the same with the ground-truth or is the flip of the ground-truth sign, the loss should be 0. In order to handle this problem, we design 3 methods, all of them can achieve rather good result.\n\n2) Alternatives of Sign Prediction in Learning Based Isosurface Extraction: In learning based iso-surface extraction part, we need to handle the relative sign problem.We also propose 3 ways to handle it.First, we use a modified binary cross entropy loss to predict the relative sign of the UDF in the corner of cube.In detail, we adopt the following\nL 1 = c\u2208C min( v\u2208V BC(\u1ef9 c j , y c j ), 7 j=0 BC(\u1ef9 c j , \u00acy c j )),(12)\nwhere C is the cube set, c is the cube index, j is vertex index in the local cube.BC is binary cross entropy, \u1ef9 is the predicted sign, y is the ground-truth label, y \u2208 {0, 1}, \u00acy is the negative y.In this way, we choose a label or negative label which is close to the prediction.In testing, we only need to obtain one prediction same with label or negative label.Obviously, from the UDF and its gradient in the local cube, it is impossible to infer the global sign.On the contrary, predicting relative sign is possible.In fact, predicting the relative sign of vertex is learning the relation between vertices.Thus, we can give up the above way where we treat the eight vertices as a whole.Every time, we only handle two vertices and predict their relation.We can write the loss as\nL 2 = c\u2208C 7 i,j=0 BC(\u1ef9 c ij , y i \u2295 y j ),(13)\nwhere \u1ef9ij is the relation of i \u2212 th vertex and j \u2212 th vertex.\u2295 is xor.Thirdly, we can go further from the first idea, treating the cube as a whole.we do not predict the sign the each vertex but predict the whole status of the cube.In this way, we can totally have 128 cube status.Thus, the sign prediction problem becomes a multi-classification problem.The loss is where y \u2208 {0, 1, \u2022 \u2022 \u2022 , 127}, CE is cross-entropy.It shall be explained here why the number of status is 128.Totally, we have eight vertex and varies in {0, 1}, therefore, there are 2 8 = 256 status.Because we regard the flip of all the vertices and the original status as the same, thus the number of status is 256/2 = 128.\nL 3 = c\u2208C CE( \u1ef9c , y c )(14)\nWe define the acc here.For every the cube near the surface, c i , i = 1, .., N , if the network predicts the right sign of all the 8 corners or the negative sign of all the 8 corners, the indicator function I(c i ) = 1, otherwise, I(c i ) = 0. Then the metric is the accuracy defined as\nacc = N i I(c i ) N .(15)\nWe compare the three methods for predict the cube status and the results do not vary too much.The comparison result is shown in Tab.XII.We random choose 20 shapes from the all the 3 datasets for training.\n\n3) Sign Conflict: The term sign conflict has two different meanings in different situations.One situation is that the two corners of two consecutive cubes have different sign by prediction in SDF reconstruction, but the two corners are the same point in fact, they should have the same sign.Another is in UDF reconstruction, the sign of 8 corners is conflict with each other.For simplicity, taking a sqaure rather than cube for example, as shown in Fig. 18.A and B have opposite UDF gradient so they have the opposite sign.Let's suppose A is positive and B is negative.Because A and C have the opposite sign as well, there C is negative.C and D have the same-direction gradient, therefore they have the same sign, D is negative.While, B and D have opposite gradient, they have different sign, D is positive.D is positive and negative at the same time, there is conflict.When conflict arises, we cannot map the sign of cube corner to the Marching Cube table, therefore we cannot generate mesh as well.Here, the sign conflict means the latter one.The mesh extraction method of CAP [15] has a risk of sign conflict.There are 8 corners of every cube, so there are 2 8 = 256 possible situations of the corner sign composition.Meanwhile if we flip the sign of 8 corners at the same time, the mesh without orientation does not change.Therefore, there are totally 256/2 = 128 corner sign composition.While it extract mesh via predicting whether the two consecutive corners have the same sign separately.There are 12 edges of the cube so that there are 2 12 = 2048 sign composition.There are 2048 \u2212 128 = 1920 edge sign composition generating sign conflict.However, if adopting our method to reconstruct mesh, we directly output the sign of the 8 corners by considering the cube as a whole rather than considering the pair of corners separately.Although the  sign conflict does not always appear in practice, our method for mesh extraction is complete in theory.\n\n4) Details of Learning based Iso-surface Extraction: In order to improve the generalization ability of our network further, We design three ways to augment the input data.First, we adopt random flip.In every training iteration, we randomly choose a axis and give a probability of whether flip or not.Second, we adopt random rotation.we randomly choose a axis and give a probability of rotation angle 0 \u2022 , 90 \u2022 , 180 \u2022 , 270 \u2022 .Third, we adopt random scale.We randomly scale the UDF value in the cube, the scale parameter varies in [0.5, 1.5].\n\nThe network is backbone is composed of 4 3D convolution layers.The width of the layers is 8, 64, 128 and 128.After extracting the feature, we use a 3D positional encoding [27] to encode the coordinates of 8 corners, then concatenating the feature of the cube and 8 corners.During training, we adopt cosine learning rate adjustment [28], the initial learning rate is 0.001.We use Adam [29] optimizer to learn the parameter.The batch-size is 2048.\n\nIn our experiment, we train the network on ShapeNet vessel class with different training shapes and test on other classes of ShapeNet.The result is shown in Tab.XIII.We can conclude that only 5 shapes for training is good enough and the network can generalize to different classes.\n\nFig. 4 .\n4\nFig. 4. (a): The two-stage UDF projection flow prediction.The projection vectors s 1 and s 2 are predicted respectively by aggregating features at different scales.(b): The inter-consistency loss that ensures the monotonicity of the UDF.\n\n\nFig. 5 .\n5\nFig. 5.The architecture of the local UDF sign estimation network.\n\n\nFig. 8\n8\nFig.8.Visualized results of our method and NDF[9], CAP[15] and GIFS[31].The number of input point cloud is 10000.\n\n\nFig. 9 .\n9\nFig. 9. Visualized results of the ablation study on the regularization term.The proposed inter-consistency loss can significantly improve the up-sample and reconstruction quality.Dataset ShapeNet MGN ScanNet NC 0.9023 0.8875 0.8934\n\n\nFig. 11 .\n11\nFig. 11.The mechanism of s 1 is in (a).The systematic error of s 1 is in (b).\n\n\nFig. 14 .\n14\nFig. 14.The visualization of comparison between our mesh extraction method and MeshUDF on predicted UDF.\n\n\nFig. 16 .\n16\nFig. 16.Motivation of UDF prediction method.(a) is pipeline based in SDF.(b) is our pipeline.\n\n\nFig. 17\n17\nFig. 17.The 2D UDF value slices along x, y and z axis.The color red is large and blue is small.\n\n\n\n\nFig. 17.The 2D UDF value slices along x, y and z axis.The color red is large and blue is small.\n\n\nFig. 18 .\n18\nFig. 18.The sign conflict of mesh extraction.The orange arrow is the UDF gradient of every point.\n\n\n\n\ntries to improve this method by\nUDFUpsamplingestimation networkProjectionIntra-consistency lossInter-consistencyloss\ud835\udcab\ud835\udcab upsampleUDF projection flow\ud835\udcab projectChamfer distance lossFig. 2. The pipeline of our UDF estimation method. Given a point cloud P, it first performs an upsampling via per-point duplication and perturbation. Theupsampled point cloud P upsample is fed into the UDF estimation network. By performing a per-point projection based on the estimated UDF projection flow.The projected points are optimized in a self-supervised manner with several dedicated loss functions.\n\n\n\ncan only represent close surface.The reason is that ours and CAP is based on\nMethodOpen surface Inference Time Sparse InputNP [7]No29min22sHardCAP [15] Yes21min56sHardOursYes9sEasyTABLE ITHE UDF ESTIMATION COMPARISON BETWEEN NP [7], CAP [15] ANDOURS.\n\nTABLE II CD 1\nII1\nCOMPARISON ON SHAPENETWITHIN 13 CLASSES\n\n\nTABLE V ABLATION\nV\nSTUDY OF SECOND-STAGE PREDICTION.\n\n\nTABLE VI THE\nVI\nNORMAL CONSISTENCY BETWEEN APPROACHING VECTOR AND GROUND-TRUTH NORMAL.\n\n\nTABLE VII THE\nVII\nCD 1 ON 3 DATASETS WHEN k IN EQ. (2) VARIES.\n\n\nTABLE VIII THE\nVIII\nCOMPARISON BETWEEN OUR METHOD AND MESHUDF\n\n\nTABLE X CD1\nX\nFig. 15.Visualized results of our method and PSR[1] on FAMOUS dataset.COMPARISON RESULTS WITH DIFFERENT SPARSE LEVEL ON SHAPENET, MGN AND SCANNET.\nInputPSROursGT#points100030001000020000ShapeNetNPull CAP0.0075 0.00720.0060 0.0047 0.0027 0.0048 0.0026 0.0023Ours w/o inter-loss0.01220.0082 0.0052 0.0028Ours w inter-loss0.00390.0028 0.0024 0.0023CAP0.00470.0035 0.00220.0018MGNOurs w/o inter-loss0.00760.0052 0.0026 0.0023Ours w inter-loss0.00360.0024 0.00210.0019CAP-0.0075 0.00440.0035ScanNetOurs w/o inter-loss-0.0097 0.0074 0.0056Ours w inter-loss-0.0057 0.00390.0036train dataset test dataset #train shape accuracyShapeNetScanNet50.95ShapeNetScanNet200.96ScanNetShapeNet50.97ScanNetShapeNet200.98TABLE IXTHE ACCURACY TRAINING ON DIFFERENT NUMBER OF SHAPES AND TESTON DIFFERENT DATASET.\nVI. ACKNOWLEDGEMENTSThis research is funded by the following projects.National Key Research and Development Program of China(2018AAA0102200).National Natural Science Foundation of China(62002375, 62002376, 62132021, 62325211, 62372457, 62002379).National Science Foundation of Hunan Province of China(2021JJ40696, 2021RC3071, 2022RC1104, 2023JJ20051).NUDT Research Grants(ZK22-52).The Science and Technology Innovation Program of Hunan Province(2023RC3011).\nPoisson surface reconstruction. M Kazhdan, M Bolitho, H Hoppe, 2013The Japan Institute of Energy67\n\nA survey of surface reconstruction from point clouds. M Berger, A Tagliasacchi, L M Seversky, P Alliez, G Guennebaud, J A Levine, A Sharf, C T Silva, Comput. Graph. Forum. 3612017Wiley Online Library\n\nDeepsdf: Learning continuous signed distance functions for shape representation. J Park, P Florence, J Straub, R Newcombe, S Lovegrove, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019\n\nS Peng, M Niemeyer, L Mescheder, M Pollefeys, A Geiger, Convolutional Occupancy Networks. 2020\n\nOccupancy networks: Learning 3d reconstruction in function space. L Mescheder, M Oechsle, M Niemeyer, S Nowozin, A Geiger, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019\n\nPoints2surf: Learning implicit surfaces from point cloud patches. P Erler, P Guerrero, S Ohrhallinger, M Wimmer, N J Mitra, 2020\n\nNeural-pull: Learning signed distance functions from point clouds by learning to pull space onto surfaces. B Ma, Z Han, Y S Liu, M Zwicker, 2020\n\nPoco: Point convolution for surface reconstruction. A Boulch, R Marlet, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022\n\nNeural unsigned distance fields for implicit function learning. J Chibane, A Mir, G Pons-Moll, 2020\n\nDeep implicit surface point prediction networks. R Venkatesh, T Karmali, S Sharma, A Ghosh, R V Babu, L A Jeni, M Singh, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision202112662\n\nLearning anchored unsigned distance functions with gradient direction alignment for single-view garment reconstruction. F Zhao, W Wang, S Liao, L Shao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision202112683\n\nMarching cubes: A high resolution 3d surface construction algorithm. W E Lorensen, H E Cline, ACM siggraph computer graphics. 2141987\n\nMeshudf: Fast and differentiable meshing of unsigned distance field networks. B Guillard, F Stella, P Fua, 2021arXiv e-prints\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nie\u00dfner, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)IEEE2017\n\nLearning consistency-aware unsigned distance functions progressively from raw point clouds. J Zhou, B Ma, L Yu-Shen, F Yi, H Zhizhong, Advances in Neural Information Processing Systems (NeurIPS). 2022\n\nParameterizationfree projection for geometry reconstruction. Y Lipman, D Cohen-Or, D Levin, H Tal-Ezer, ACM Transactions on Graphics (TOG). 2632007\n\nThe ball-pivoting algorithm for surface reconstruction. F Bernardini, J Mittleman, H Rushmeier, C Silva, G Taubin, IEEE Transactions on. 1999Visualization and Computer Graphics\n\nBsp-net: Generating compact meshes via binary space partitioning. Z Chen, A Tagliasacchi, H Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020\n\nSsrnet: Scalable 3d surface reconstruction network. Z Mi, Y Luo, W Tao, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020\n\nDeep implicit moving least-squares functions for 3d reconstruction. S.-L Liu, H.-X Guo, H Pan, P.-S Wang, X Tong, Y Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021\n\nSAL: Sign Agnostic Learning of Shapes From Raw Data. M Atzmon, Y Lipman, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Seattle, WA, USAIEEEJun. 2020\n\nSALD: Sign Agnostic Learning with Derivatives. Oct. 2020\n\nReconstructing Surfaces for Sparse Point Clouds With On-Surface Priors. B Ma, Y.-S Liu, Z Han, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11\n\nDUDE: Deep Unsigned Distance Embeddings for Hi-Fidelity Representation of Complex 3D Surfaces. R Venkatesh, S Sharma, A Ghosh, L Jeni, M Singh, Dec. 2020\n\nUWED: Unsigned Distance Field for Accurate 3D Scene Representation and Completion. J P Richa, J.-E Deschaud, F Goulette, N Dalmasso, 20\n\nPoint transformer. H Zhao, L Jiang, J Jia, P H Torr, V Koltun, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision202116268\n\nFourier features let networks learn high frequency functions in low dimensional domains. M Tancik, P P Srinivasan, B Mildenhall, S Fridovich-Keil, N Raghavan, U Singhal, R Ramamoorthi, J T Barron, R Ng, 2020\n\nSgdr: Stochastic gradient descent with restarts. I Loshchilov, F Hutter, 2016\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.69802014arXiv preprint\n\nA X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, arXiv:1512.03012Shapenet: An informationrich 3d model repository. 2015arXiv preprint\n\nGIFS: Neural Implicit Function for General Shape Representation. J Ye, Y Chen, N Wang, X Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202211\n\nDeep marching cubes: Learning explicit surface representations. Y Liao, S Donn\u00e9, A Geiger, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2018\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, International conference on machine learning. PMLR2015\n", "annotations": {"author": "[{\"end\":91,\"start\":82},{\"end\":105,\"start\":92},{\"end\":116,\"start\":106},{\"end\":124,\"start\":117}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":86},{\"end\":104,\"start\":101},{\"end\":115,\"start\":112},{\"end\":123,\"start\":121}]", "author_first_name": "[{\"end\":85,\"start\":82},{\"end\":100,\"start\":92},{\"end\":111,\"start\":106},{\"end\":120,\"start\":117}]", "author_affiliation": null, "title": "[{\"end\":68,\"start\":1},{\"end\":192,\"start\":125}]", "venue": null, "abstract": "[{\"end\":1420,\"start\":307}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1599,\"start\":1596},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1647,\"start\":1644},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1783,\"start\":1780},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1788,\"start\":1785},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1793,\"start\":1790},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1798,\"start\":1795},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1803,\"start\":1800},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2039,\"start\":2036},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2044,\"start\":2041},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2049,\"start\":2046},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2054,\"start\":2051},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2567,\"start\":2564},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2663,\"start\":2659},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2669,\"start\":2665},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2837,\"start\":2833},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2842,\"start\":2838},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3046,\"start\":3042},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3073,\"start\":3070},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3082,\"start\":3079},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3093,\"start\":3089},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3127,\"start\":3123},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3516,\"start\":3513},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3521,\"start\":3518},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4065,\"start\":4061},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7222,\"start\":7219},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7260,\"start\":7256},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8026,\"start\":8023},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8040,\"start\":8036},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8213,\"start\":8210},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8227,\"start\":8223},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8240,\"start\":8237},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8253,\"start\":8250},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8268,\"start\":8265},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8406,\"start\":8402},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8540,\"start\":8536},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8853,\"start\":8850},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9057,\"start\":9053},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9068,\"start\":9064},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9094,\"start\":9090},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9320,\"start\":9316},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9435,\"start\":9431},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9513,\"start\":9509},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10135,\"start\":10132},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10321,\"start\":10317},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10338,\"start\":10334},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10661,\"start\":10657},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10920,\"start\":10917},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11303,\"start\":11299},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11563,\"start\":11559},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11711,\"start\":11707},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13382,\"start\":13378},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13674,\"start\":13670},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18481,\"start\":18477},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19040,\"start\":19036},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21093,\"start\":21090},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21106,\"start\":21102},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21389,\"start\":21385},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21456,\"start\":21453},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21759,\"start\":21756},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21822,\"start\":21818},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21847,\"start\":21843},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21865,\"start\":21861},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21878,\"start\":21874},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22435,\"start\":22431},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22906,\"start\":22902},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22959,\"start\":22955},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23459,\"start\":23455},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23637,\"start\":23633},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24567,\"start\":24564},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24572,\"start\":24569},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24577,\"start\":24574},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24582,\"start\":24579},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24688,\"start\":24684},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24831,\"start\":24828},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24844,\"start\":24840},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24957,\"start\":24954},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25736,\"start\":25733},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25745,\"start\":25742},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25755,\"start\":25751},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25769,\"start\":25765},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25823,\"start\":25819},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25831,\"start\":25828},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25841,\"start\":25837},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26334,\"start\":26330},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28546,\"start\":28543},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28556,\"start\":28553},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28564,\"start\":28561},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28574,\"start\":28570},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28585,\"start\":28581},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28786,\"start\":28783},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28812,\"start\":28809},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28822,\"start\":28818},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28833,\"start\":28829},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30372,\"start\":30369},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30382,\"start\":30378},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30396,\"start\":30392},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32061,\"start\":32057},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32081,\"start\":32078},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32471,\"start\":32467},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32923,\"start\":32920},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32933,\"start\":32930},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32941,\"start\":32938},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32951,\"start\":32947},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32962,\"start\":32958},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33454,\"start\":33450},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34543,\"start\":34540},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":35461,\"start\":35457},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":35634,\"start\":35630},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":36397,\"start\":36393},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":36980,\"start\":36976},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37501,\"start\":37497},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37521,\"start\":37518},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39472,\"start\":39468},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39615,\"start\":39611},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":44569,\"start\":44565},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":46161,\"start\":46157},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":46321,\"start\":46317},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":46374,\"start\":46370},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":47104,\"start\":47101},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":47113,\"start\":47109},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":47126,\"start\":47122}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46965,\"start\":46715},{\"attributes\":{\"id\":\"fig_1\"},\"end\":47044,\"start\":46966},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47169,\"start\":47045},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47414,\"start\":47170},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47507,\"start\":47415},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47627,\"start\":47508},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47736,\"start\":47628},{\"attributes\":{\"id\":\"fig_9\"},\"end\":47845,\"start\":47737},{\"attributes\":{\"id\":\"fig_10\"},\"end\":47945,\"start\":47846},{\"attributes\":{\"id\":\"fig_11\"},\"end\":48058,\"start\":47946},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":48645,\"start\":48059},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48899,\"start\":48646},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":48959,\"start\":48900},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":49014,\"start\":48960},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":49103,\"start\":49015},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":49168,\"start\":49104},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":49232,\"start\":49169},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":50037,\"start\":49233}]", "paragraph": "[{\"end\":2301,\"start\":1439},{\"end\":2937,\"start\":2303},{\"end\":3111,\"start\":2939},{\"end\":3905,\"start\":3113},{\"end\":4503,\"start\":3907},{\"end\":5299,\"start\":4505},{\"end\":5930,\"start\":5301},{\"end\":6413,\"start\":5932},{\"end\":6605,\"start\":6415},{\"end\":6997,\"start\":6607},{\"end\":7539,\"start\":6999},{\"end\":9597,\"start\":7541},{\"end\":9950,\"start\":9599},{\"end\":12040,\"start\":9952},{\"end\":12495,\"start\":12070},{\"end\":12999,\"start\":12533},{\"end\":13546,\"start\":13005},{\"end\":13675,\"start\":13613},{\"end\":13876,\"start\":13684},{\"end\":14523,\"start\":13878},{\"end\":14636,\"start\":14525},{\"end\":14791,\"start\":14638},{\"end\":15033,\"start\":14837},{\"end\":15109,\"start\":15061},{\"end\":15454,\"start\":15111},{\"end\":15707,\"start\":15456},{\"end\":15892,\"start\":15756},{\"end\":16580,\"start\":15922},{\"end\":16773,\"start\":16628},{\"end\":16904,\"start\":16799},{\"end\":17641,\"start\":16906},{\"end\":17977,\"start\":17724},{\"end\":18571,\"start\":18015},{\"end\":19288,\"start\":18573},{\"end\":19611,\"start\":19354},{\"end\":19837,\"start\":19613},{\"end\":20920,\"start\":19855},{\"end\":21357,\"start\":20966},{\"end\":22323,\"start\":21359},{\"end\":22815,\"start\":22325},{\"end\":23313,\"start\":22817},{\"end\":24042,\"start\":23333},{\"end\":24443,\"start\":24229},{\"end\":25303,\"start\":24479},{\"end\":25957,\"start\":25338},{\"end\":26728,\"start\":25990},{\"end\":27618,\"start\":26730},{\"end\":28644,\"start\":27648},{\"end\":28930,\"start\":28702},{\"end\":28972,\"start\":28932},{\"end\":29780,\"start\":28974},{\"end\":29976,\"start\":29782},{\"end\":30438,\"start\":29978},{\"end\":30565,\"start\":30486},{\"end\":30852,\"start\":30604},{\"end\":31203,\"start\":30901},{\"end\":31471,\"start\":31205},{\"end\":31972,\"start\":31473},{\"end\":32353,\"start\":31974},{\"end\":33159,\"start\":32387},{\"end\":33455,\"start\":33161},{\"end\":33500,\"start\":33457},{\"end\":33816,\"start\":33512},{\"end\":34143,\"start\":33864},{\"end\":34426,\"start\":34145},{\"end\":34657,\"start\":34471},{\"end\":36868,\"start\":34691},{\"end\":36981,\"start\":36893},{\"end\":37461,\"start\":37085},{\"end\":38040,\"start\":37493},{\"end\":38573,\"start\":38067},{\"end\":39371,\"start\":38618},{\"end\":39676,\"start\":39373},{\"end\":40508,\"start\":39678},{\"end\":40996,\"start\":40510},{\"end\":41347,\"start\":40998},{\"end\":42199,\"start\":41419},{\"end\":42937,\"start\":42247},{\"end\":43253,\"start\":42967},{\"end\":43484,\"start\":43280},{\"end\":45439,\"start\":43486},{\"end\":45984,\"start\":45441},{\"end\":46431,\"start\":45986},{\"end\":46714,\"start\":46433},{\"end\":46964,\"start\":46727},{\"end\":47043,\"start\":46978},{\"end\":47168,\"start\":47055},{\"end\":47413,\"start\":47182},{\"end\":47506,\"start\":47429},{\"end\":47626,\"start\":47522},{\"end\":47735,\"start\":47642},{\"end\":47844,\"start\":47749},{\"end\":47944,\"start\":47849},{\"end\":48057,\"start\":47960},{\"end\":48093,\"start\":48062},{\"end\":48725,\"start\":48649},{\"end\":48958,\"start\":48919},{\"end\":49013,\"start\":48980},{\"end\":49102,\"start\":49032},{\"end\":49167,\"start\":49123},{\"end\":49231,\"start\":49190},{\"end\":49394,\"start\":49248}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13612,\"start\":13547},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13683,\"start\":13676},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14836,\"start\":14792},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15060,\"start\":15034},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15755,\"start\":15708},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15921,\"start\":15893},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16627,\"start\":16581},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16798,\"start\":16774},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17723,\"start\":17642},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19352,\"start\":19289},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19353,\"start\":19352},{\"attributes\":{\"id\":\"formula_12\"},\"end\":24127,\"start\":24043},{\"attributes\":{\"id\":\"formula_13\"},\"end\":24128,\"start\":24127},{\"attributes\":{\"id\":\"formula_14\"},\"end\":24228,\"start\":24128},{\"attributes\":{\"id\":\"formula_15\"},\"end\":37084,\"start\":36982},{\"attributes\":{\"id\":\"formula_16\"},\"end\":41418,\"start\":41348},{\"attributes\":{\"id\":\"formula_17\"},\"end\":42246,\"start\":42200},{\"attributes\":{\"id\":\"formula_18\"},\"end\":42966,\"start\":42938},{\"attributes\":{\"id\":\"formula_19\"},\"end\":43279,\"start\":43254}]", "table_ref": null, "section_header": "[{\"end\":1437,\"start\":1422},{\"end\":12054,\"start\":12043},{\"end\":12068,\"start\":12057},{\"end\":12531,\"start\":12498},{\"end\":13003,\"start\":13002},{\"end\":18013,\"start\":17980},{\"end\":19853,\"start\":19840},{\"end\":20964,\"start\":20923},{\"end\":23331,\"start\":23316},{\"end\":24477,\"start\":24446},{\"end\":25336,\"start\":25306},{\"end\":25988,\"start\":25960},{\"end\":27646,\"start\":27621},{\"end\":28700,\"start\":28647},{\"end\":30462,\"start\":30441},{\"end\":30469,\"start\":30465},{\"end\":30484,\"start\":30472},{\"end\":30602,\"start\":30568},{\"end\":30899,\"start\":30855},{\"end\":32385,\"start\":32356},{\"end\":33510,\"start\":33503},{\"end\":33862,\"start\":33819},{\"end\":34469,\"start\":34429},{\"end\":34689,\"start\":34660},{\"end\":36891,\"start\":36871},{\"end\":37491,\"start\":37464},{\"end\":38065,\"start\":38043},{\"end\":38616,\"start\":38576},{\"end\":46724,\"start\":46716},{\"end\":46975,\"start\":46967},{\"end\":47052,\"start\":47046},{\"end\":47179,\"start\":47171},{\"end\":47425,\"start\":47416},{\"end\":47518,\"start\":47509},{\"end\":47638,\"start\":47629},{\"end\":47745,\"start\":47738},{\"end\":47956,\"start\":47947},{\"end\":48914,\"start\":48901},{\"end\":48977,\"start\":48961},{\"end\":49028,\"start\":49016},{\"end\":49118,\"start\":49105},{\"end\":49184,\"start\":49170},{\"end\":49245,\"start\":49234}]", "table": "[{\"end\":48645,\"start\":48094},{\"end\":48899,\"start\":48726},{\"end\":50037,\"start\":49395}]", "figure_caption": "[{\"end\":46965,\"start\":46726},{\"end\":47044,\"start\":46977},{\"end\":47169,\"start\":47054},{\"end\":47414,\"start\":47181},{\"end\":47507,\"start\":47428},{\"end\":47627,\"start\":47521},{\"end\":47736,\"start\":47641},{\"end\":47845,\"start\":47748},{\"end\":47945,\"start\":47848},{\"end\":48058,\"start\":47959},{\"end\":48094,\"start\":48061},{\"end\":48726,\"start\":48648},{\"end\":48959,\"start\":48918},{\"end\":49014,\"start\":48979},{\"end\":49103,\"start\":49031},{\"end\":49168,\"start\":49122},{\"end\":49232,\"start\":49189},{\"end\":49395,\"start\":49247}]", "figure_ref": "[{\"end\":3020,\"start\":3019},{\"end\":12375,\"start\":12374},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12494,\"start\":12493},{\"end\":12613,\"start\":12612},{\"end\":12999,\"start\":12998},{\"end\":13041,\"start\":13040},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14634,\"start\":14631},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17131,\"start\":17128},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18873,\"start\":18872},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20049,\"start\":20044},{\"end\":24854,\"start\":24853},{\"end\":24866,\"start\":24864},{\"end\":25850,\"start\":25849},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26434,\"start\":26433},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27881,\"start\":27875},{\"end\":28158,\"start\":28157},{\"end\":28708,\"start\":28707},{\"end\":29325,\"start\":29323},{\"end\":29601,\"start\":29597},{\"end\":29700,\"start\":29698},{\"end\":30129,\"start\":30127},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30977,\"start\":30976},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31431,\"start\":31430},{\"end\":32535,\"start\":32533},{\"end\":32992,\"start\":32990},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33728,\"start\":33726},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34574,\"start\":34572},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":36238,\"start\":36236},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":36817,\"start\":36812},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":38239,\"start\":38237},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":43942,\"start\":43940}]", "bib_author_first_name": "[{\"end\":50529,\"start\":50528},{\"end\":50540,\"start\":50539},{\"end\":50551,\"start\":50550},{\"end\":50651,\"start\":50650},{\"end\":50661,\"start\":50660},{\"end\":50677,\"start\":50676},{\"end\":50679,\"start\":50678},{\"end\":50691,\"start\":50690},{\"end\":50701,\"start\":50700},{\"end\":50715,\"start\":50714},{\"end\":50717,\"start\":50716},{\"end\":50727,\"start\":50726},{\"end\":50736,\"start\":50735},{\"end\":50738,\"start\":50737},{\"end\":50879,\"start\":50878},{\"end\":50887,\"start\":50886},{\"end\":50899,\"start\":50898},{\"end\":50909,\"start\":50908},{\"end\":50921,\"start\":50920},{\"end\":51016,\"start\":51015},{\"end\":51024,\"start\":51023},{\"end\":51036,\"start\":51035},{\"end\":51049,\"start\":51048},{\"end\":51062,\"start\":51061},{\"end\":51178,\"start\":51177},{\"end\":51191,\"start\":51190},{\"end\":51202,\"start\":51201},{\"end\":51214,\"start\":51213},{\"end\":51225,\"start\":51224},{\"end\":51383,\"start\":51382},{\"end\":51392,\"start\":51391},{\"end\":51404,\"start\":51403},{\"end\":51420,\"start\":51419},{\"end\":51430,\"start\":51429},{\"end\":51432,\"start\":51431},{\"end\":51554,\"start\":51553},{\"end\":51560,\"start\":51559},{\"end\":51567,\"start\":51566},{\"end\":51569,\"start\":51568},{\"end\":51576,\"start\":51575},{\"end\":51645,\"start\":51644},{\"end\":51655,\"start\":51654},{\"end\":51884,\"start\":51883},{\"end\":51895,\"start\":51894},{\"end\":51902,\"start\":51901},{\"end\":51970,\"start\":51969},{\"end\":51983,\"start\":51982},{\"end\":51994,\"start\":51993},{\"end\":52004,\"start\":52003},{\"end\":52013,\"start\":52012},{\"end\":52015,\"start\":52014},{\"end\":52023,\"start\":52022},{\"end\":52025,\"start\":52024},{\"end\":52033,\"start\":52032},{\"end\":52302,\"start\":52301},{\"end\":52310,\"start\":52309},{\"end\":52318,\"start\":52317},{\"end\":52326,\"start\":52325},{\"end\":52543,\"start\":52542},{\"end\":52545,\"start\":52544},{\"end\":52557,\"start\":52556},{\"end\":52559,\"start\":52558},{\"end\":52687,\"start\":52686},{\"end\":52699,\"start\":52698},{\"end\":52709,\"start\":52708},{\"end\":52799,\"start\":52798},{\"end\":52806,\"start\":52805},{\"end\":52808,\"start\":52807},{\"end\":52817,\"start\":52816},{\"end\":52826,\"start\":52825},{\"end\":52836,\"start\":52835},{\"end\":52850,\"start\":52849},{\"end\":53063,\"start\":53062},{\"end\":53071,\"start\":53070},{\"end\":53077,\"start\":53076},{\"end\":53088,\"start\":53087},{\"end\":53094,\"start\":53093},{\"end\":53234,\"start\":53233},{\"end\":53244,\"start\":53243},{\"end\":53256,\"start\":53255},{\"end\":53265,\"start\":53264},{\"end\":53378,\"start\":53377},{\"end\":53392,\"start\":53391},{\"end\":53405,\"start\":53404},{\"end\":53418,\"start\":53417},{\"end\":53427,\"start\":53426},{\"end\":53566,\"start\":53565},{\"end\":53574,\"start\":53573},{\"end\":53590,\"start\":53589},{\"end\":53806,\"start\":53805},{\"end\":53812,\"start\":53811},{\"end\":53819,\"start\":53818},{\"end\":53979,\"start\":53975},{\"end\":53989,\"start\":53985},{\"end\":53996,\"start\":53995},{\"end\":54006,\"start\":54002},{\"end\":54014,\"start\":54013},{\"end\":54022,\"start\":54021},{\"end\":54237,\"start\":54236},{\"end\":54247,\"start\":54246},{\"end\":54494,\"start\":54493},{\"end\":54503,\"start\":54499},{\"end\":54510,\"start\":54509},{\"end\":54685,\"start\":54684},{\"end\":54698,\"start\":54697},{\"end\":54708,\"start\":54707},{\"end\":54717,\"start\":54716},{\"end\":54725,\"start\":54724},{\"end\":54828,\"start\":54827},{\"end\":54830,\"start\":54829},{\"end\":54842,\"start\":54838},{\"end\":54854,\"start\":54853},{\"end\":54866,\"start\":54865},{\"end\":54901,\"start\":54900},{\"end\":54909,\"start\":54908},{\"end\":54918,\"start\":54917},{\"end\":54925,\"start\":54924},{\"end\":54927,\"start\":54926},{\"end\":54935,\"start\":54934},{\"end\":55174,\"start\":55173},{\"end\":55184,\"start\":55183},{\"end\":55186,\"start\":55185},{\"end\":55200,\"start\":55199},{\"end\":55214,\"start\":55213},{\"end\":55232,\"start\":55231},{\"end\":55244,\"start\":55243},{\"end\":55255,\"start\":55254},{\"end\":55270,\"start\":55269},{\"end\":55272,\"start\":55271},{\"end\":55282,\"start\":55281},{\"end\":55343,\"start\":55342},{\"end\":55357,\"start\":55356},{\"end\":55417,\"start\":55416},{\"end\":55419,\"start\":55418},{\"end\":55429,\"start\":55428},{\"end\":55470,\"start\":55469},{\"end\":55472,\"start\":55471},{\"end\":55481,\"start\":55480},{\"end\":55495,\"start\":55494},{\"end\":55505,\"start\":55504},{\"end\":55517,\"start\":55516},{\"end\":55526,\"start\":55525},{\"end\":55532,\"start\":55531},{\"end\":55544,\"start\":55543},{\"end\":55553,\"start\":55552},{\"end\":55561,\"start\":55560},{\"end\":55718,\"start\":55717},{\"end\":55724,\"start\":55723},{\"end\":55732,\"start\":55731},{\"end\":55740,\"start\":55739},{\"end\":55969,\"start\":55968},{\"end\":55977,\"start\":55976},{\"end\":55986,\"start\":55985},{\"end\":56165,\"start\":56164},{\"end\":56174,\"start\":56173}]", "bib_author_last_name": "[{\"end\":50537,\"start\":50530},{\"end\":50548,\"start\":50541},{\"end\":50557,\"start\":50552},{\"end\":50658,\"start\":50652},{\"end\":50674,\"start\":50662},{\"end\":50688,\"start\":50680},{\"end\":50698,\"start\":50692},{\"end\":50712,\"start\":50702},{\"end\":50724,\"start\":50718},{\"end\":50733,\"start\":50728},{\"end\":50744,\"start\":50739},{\"end\":50884,\"start\":50880},{\"end\":50896,\"start\":50888},{\"end\":50906,\"start\":50900},{\"end\":50918,\"start\":50910},{\"end\":50931,\"start\":50922},{\"end\":51021,\"start\":51017},{\"end\":51033,\"start\":51025},{\"end\":51046,\"start\":51037},{\"end\":51059,\"start\":51050},{\"end\":51069,\"start\":51063},{\"end\":51188,\"start\":51179},{\"end\":51199,\"start\":51192},{\"end\":51211,\"start\":51203},{\"end\":51222,\"start\":51215},{\"end\":51232,\"start\":51226},{\"end\":51389,\"start\":51384},{\"end\":51401,\"start\":51393},{\"end\":51417,\"start\":51405},{\"end\":51427,\"start\":51421},{\"end\":51438,\"start\":51433},{\"end\":51557,\"start\":51555},{\"end\":51564,\"start\":51561},{\"end\":51573,\"start\":51570},{\"end\":51584,\"start\":51577},{\"end\":51652,\"start\":51646},{\"end\":51662,\"start\":51656},{\"end\":51892,\"start\":51885},{\"end\":51899,\"start\":51896},{\"end\":51912,\"start\":51903},{\"end\":51980,\"start\":51971},{\"end\":51991,\"start\":51984},{\"end\":52001,\"start\":51995},{\"end\":52010,\"start\":52005},{\"end\":52020,\"start\":52016},{\"end\":52030,\"start\":52026},{\"end\":52039,\"start\":52034},{\"end\":52307,\"start\":52303},{\"end\":52315,\"start\":52311},{\"end\":52323,\"start\":52319},{\"end\":52331,\"start\":52327},{\"end\":52554,\"start\":52546},{\"end\":52565,\"start\":52560},{\"end\":52696,\"start\":52688},{\"end\":52706,\"start\":52700},{\"end\":52713,\"start\":52710},{\"end\":52803,\"start\":52800},{\"end\":52814,\"start\":52809},{\"end\":52823,\"start\":52818},{\"end\":52833,\"start\":52827},{\"end\":52847,\"start\":52837},{\"end\":52858,\"start\":52851},{\"end\":53068,\"start\":53064},{\"end\":53074,\"start\":53072},{\"end\":53085,\"start\":53078},{\"end\":53091,\"start\":53089},{\"end\":53103,\"start\":53095},{\"end\":53241,\"start\":53235},{\"end\":53253,\"start\":53245},{\"end\":53262,\"start\":53257},{\"end\":53274,\"start\":53266},{\"end\":53389,\"start\":53379},{\"end\":53402,\"start\":53393},{\"end\":53415,\"start\":53406},{\"end\":53424,\"start\":53419},{\"end\":53434,\"start\":53428},{\"end\":53571,\"start\":53567},{\"end\":53587,\"start\":53575},{\"end\":53596,\"start\":53591},{\"end\":53809,\"start\":53807},{\"end\":53816,\"start\":53813},{\"end\":53823,\"start\":53820},{\"end\":53983,\"start\":53980},{\"end\":53993,\"start\":53990},{\"end\":54000,\"start\":53997},{\"end\":54011,\"start\":54007},{\"end\":54019,\"start\":54015},{\"end\":54026,\"start\":54023},{\"end\":54244,\"start\":54238},{\"end\":54254,\"start\":54248},{\"end\":54497,\"start\":54495},{\"end\":54507,\"start\":54504},{\"end\":54514,\"start\":54511},{\"end\":54695,\"start\":54686},{\"end\":54705,\"start\":54699},{\"end\":54714,\"start\":54709},{\"end\":54722,\"start\":54718},{\"end\":54731,\"start\":54726},{\"end\":54836,\"start\":54831},{\"end\":54851,\"start\":54843},{\"end\":54863,\"start\":54855},{\"end\":54875,\"start\":54867},{\"end\":54906,\"start\":54902},{\"end\":54915,\"start\":54910},{\"end\":54922,\"start\":54919},{\"end\":54932,\"start\":54928},{\"end\":54942,\"start\":54936},{\"end\":55181,\"start\":55175},{\"end\":55197,\"start\":55187},{\"end\":55211,\"start\":55201},{\"end\":55229,\"start\":55215},{\"end\":55241,\"start\":55233},{\"end\":55252,\"start\":55245},{\"end\":55267,\"start\":55256},{\"end\":55279,\"start\":55273},{\"end\":55285,\"start\":55283},{\"end\":55354,\"start\":55344},{\"end\":55364,\"start\":55358},{\"end\":55426,\"start\":55420},{\"end\":55432,\"start\":55430},{\"end\":55478,\"start\":55473},{\"end\":55492,\"start\":55482},{\"end\":55502,\"start\":55496},{\"end\":55514,\"start\":55506},{\"end\":55523,\"start\":55518},{\"end\":55529,\"start\":55527},{\"end\":55541,\"start\":55533},{\"end\":55550,\"start\":55545},{\"end\":55558,\"start\":55554},{\"end\":55564,\"start\":55562},{\"end\":55721,\"start\":55719},{\"end\":55729,\"start\":55725},{\"end\":55737,\"start\":55733},{\"end\":55745,\"start\":55741},{\"end\":55974,\"start\":55970},{\"end\":55983,\"start\":55978},{\"end\":55993,\"start\":55987},{\"end\":56171,\"start\":56166},{\"end\":56182,\"start\":56175}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":50594,\"start\":50496},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":5902682},\"end\":50795,\"start\":50596},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":58007025},\"end\":51013,\"start\":50797},{\"attributes\":{\"id\":\"b3\"},\"end\":51109,\"start\":51015},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":54465161},\"end\":51314,\"start\":51111},{\"attributes\":{\"id\":\"b5\"},\"end\":51444,\"start\":51316},{\"attributes\":{\"id\":\"b6\"},\"end\":51590,\"start\":51446},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":245769792},\"end\":51817,\"start\":51592},{\"attributes\":{\"id\":\"b8\"},\"end\":51918,\"start\":51819},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":235390948},\"end\":52179,\"start\":51920},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":237213621},\"end\":52471,\"start\":52181},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15545924},\"end\":52606,\"start\":52473},{\"attributes\":{\"id\":\"b12\"},\"end\":52733,\"start\":52608},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7684883},\"end\":52968,\"start\":52735},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":252735231},\"end\":53170,\"start\":52970},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6874260},\"end\":53319,\"start\":53172},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11096669},\"end\":53497,\"start\":53321},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":208139574},\"end\":53751,\"start\":53499},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":215754836},\"end\":53905,\"start\":53753},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":232320737},\"end\":54181,\"start\":53907},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":208267630},\"end\":54361,\"start\":54183},{\"attributes\":{\"id\":\"b21\"},\"end\":54419,\"start\":54363},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":248366252},\"end\":54587,\"start\":54421},{\"attributes\":{\"id\":\"b23\"},\"end\":54742,\"start\":54589},{\"attributes\":{\"id\":\"b24\"},\"end\":54879,\"start\":54744},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":226227046},\"end\":55082,\"start\":54881},{\"attributes\":{\"id\":\"b26\"},\"end\":55291,\"start\":55084},{\"attributes\":{\"id\":\"b27\"},\"end\":55370,\"start\":55293},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b28\"},\"end\":55467,\"start\":55372},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b29\"},\"end\":55650,\"start\":55469},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":248177781},\"end\":55902,\"start\":55652},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":46900294},\"end\":56068,\"start\":55904},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":5808102},\"end\":56238,\"start\":56070}]", "bib_title": "[{\"end\":50648,\"start\":50596},{\"end\":50876,\"start\":50797},{\"end\":51175,\"start\":51111},{\"end\":51642,\"start\":51592},{\"end\":51967,\"start\":51920},{\"end\":52299,\"start\":52181},{\"end\":52540,\"start\":52473},{\"end\":52796,\"start\":52735},{\"end\":53060,\"start\":52970},{\"end\":53231,\"start\":53172},{\"end\":53375,\"start\":53321},{\"end\":53563,\"start\":53499},{\"end\":53803,\"start\":53753},{\"end\":53973,\"start\":53907},{\"end\":54234,\"start\":54183},{\"end\":54491,\"start\":54421},{\"end\":54898,\"start\":54881},{\"end\":55715,\"start\":55652},{\"end\":55966,\"start\":55904},{\"end\":56162,\"start\":56070}]", "bib_author": "[{\"end\":50539,\"start\":50528},{\"end\":50550,\"start\":50539},{\"end\":50559,\"start\":50550},{\"end\":50660,\"start\":50650},{\"end\":50676,\"start\":50660},{\"end\":50690,\"start\":50676},{\"end\":50700,\"start\":50690},{\"end\":50714,\"start\":50700},{\"end\":50726,\"start\":50714},{\"end\":50735,\"start\":50726},{\"end\":50746,\"start\":50735},{\"end\":50886,\"start\":50878},{\"end\":50898,\"start\":50886},{\"end\":50908,\"start\":50898},{\"end\":50920,\"start\":50908},{\"end\":50933,\"start\":50920},{\"end\":51023,\"start\":51015},{\"end\":51035,\"start\":51023},{\"end\":51048,\"start\":51035},{\"end\":51061,\"start\":51048},{\"end\":51071,\"start\":51061},{\"end\":51190,\"start\":51177},{\"end\":51201,\"start\":51190},{\"end\":51213,\"start\":51201},{\"end\":51224,\"start\":51213},{\"end\":51234,\"start\":51224},{\"end\":51391,\"start\":51382},{\"end\":51403,\"start\":51391},{\"end\":51419,\"start\":51403},{\"end\":51429,\"start\":51419},{\"end\":51440,\"start\":51429},{\"end\":51559,\"start\":51553},{\"end\":51566,\"start\":51559},{\"end\":51575,\"start\":51566},{\"end\":51586,\"start\":51575},{\"end\":51654,\"start\":51644},{\"end\":51664,\"start\":51654},{\"end\":51894,\"start\":51883},{\"end\":51901,\"start\":51894},{\"end\":51914,\"start\":51901},{\"end\":51982,\"start\":51969},{\"end\":51993,\"start\":51982},{\"end\":52003,\"start\":51993},{\"end\":52012,\"start\":52003},{\"end\":52022,\"start\":52012},{\"end\":52032,\"start\":52022},{\"end\":52041,\"start\":52032},{\"end\":52309,\"start\":52301},{\"end\":52317,\"start\":52309},{\"end\":52325,\"start\":52317},{\"end\":52333,\"start\":52325},{\"end\":52556,\"start\":52542},{\"end\":52567,\"start\":52556},{\"end\":52698,\"start\":52686},{\"end\":52708,\"start\":52698},{\"end\":52715,\"start\":52708},{\"end\":52805,\"start\":52798},{\"end\":52816,\"start\":52805},{\"end\":52825,\"start\":52816},{\"end\":52835,\"start\":52825},{\"end\":52849,\"start\":52835},{\"end\":52860,\"start\":52849},{\"end\":53070,\"start\":53062},{\"end\":53076,\"start\":53070},{\"end\":53087,\"start\":53076},{\"end\":53093,\"start\":53087},{\"end\":53105,\"start\":53093},{\"end\":53243,\"start\":53233},{\"end\":53255,\"start\":53243},{\"end\":53264,\"start\":53255},{\"end\":53276,\"start\":53264},{\"end\":53391,\"start\":53377},{\"end\":53404,\"start\":53391},{\"end\":53417,\"start\":53404},{\"end\":53426,\"start\":53417},{\"end\":53436,\"start\":53426},{\"end\":53573,\"start\":53565},{\"end\":53589,\"start\":53573},{\"end\":53598,\"start\":53589},{\"end\":53811,\"start\":53805},{\"end\":53818,\"start\":53811},{\"end\":53825,\"start\":53818},{\"end\":53985,\"start\":53975},{\"end\":53995,\"start\":53985},{\"end\":54002,\"start\":53995},{\"end\":54013,\"start\":54002},{\"end\":54021,\"start\":54013},{\"end\":54028,\"start\":54021},{\"end\":54246,\"start\":54236},{\"end\":54256,\"start\":54246},{\"end\":54499,\"start\":54493},{\"end\":54509,\"start\":54499},{\"end\":54516,\"start\":54509},{\"end\":54697,\"start\":54684},{\"end\":54707,\"start\":54697},{\"end\":54716,\"start\":54707},{\"end\":54724,\"start\":54716},{\"end\":54733,\"start\":54724},{\"end\":54838,\"start\":54827},{\"end\":54853,\"start\":54838},{\"end\":54865,\"start\":54853},{\"end\":54877,\"start\":54865},{\"end\":54908,\"start\":54900},{\"end\":54917,\"start\":54908},{\"end\":54924,\"start\":54917},{\"end\":54934,\"start\":54924},{\"end\":54944,\"start\":54934},{\"end\":55183,\"start\":55173},{\"end\":55199,\"start\":55183},{\"end\":55213,\"start\":55199},{\"end\":55231,\"start\":55213},{\"end\":55243,\"start\":55231},{\"end\":55254,\"start\":55243},{\"end\":55269,\"start\":55254},{\"end\":55281,\"start\":55269},{\"end\":55287,\"start\":55281},{\"end\":55356,\"start\":55342},{\"end\":55366,\"start\":55356},{\"end\":55428,\"start\":55416},{\"end\":55434,\"start\":55428},{\"end\":55480,\"start\":55469},{\"end\":55494,\"start\":55480},{\"end\":55504,\"start\":55494},{\"end\":55516,\"start\":55504},{\"end\":55525,\"start\":55516},{\"end\":55531,\"start\":55525},{\"end\":55543,\"start\":55531},{\"end\":55552,\"start\":55543},{\"end\":55560,\"start\":55552},{\"end\":55566,\"start\":55560},{\"end\":55723,\"start\":55717},{\"end\":55731,\"start\":55723},{\"end\":55739,\"start\":55731},{\"end\":55747,\"start\":55739},{\"end\":55976,\"start\":55968},{\"end\":55985,\"start\":55976},{\"end\":55995,\"start\":55985},{\"end\":56173,\"start\":56164},{\"end\":56184,\"start\":56173}]", "bib_venue": "[{\"end\":51813,\"start\":51747},{\"end\":52170,\"start\":52114},{\"end\":52462,\"start\":52406},{\"end\":52960,\"start\":52914},{\"end\":53747,\"start\":53681},{\"end\":54177,\"start\":54111},{\"end\":54348,\"start\":54332},{\"end\":55073,\"start\":55017},{\"end\":55896,\"start\":55830},{\"end\":50526,\"start\":50496},{\"end\":50766,\"start\":50746},{\"end\":51007,\"start\":50933},{\"end\":51103,\"start\":51071},{\"end\":51308,\"start\":51234},{\"end\":51380,\"start\":51316},{\"end\":51551,\"start\":51446},{\"end\":51745,\"start\":51664},{\"end\":51881,\"start\":51819},{\"end\":52112,\"start\":52041},{\"end\":52404,\"start\":52333},{\"end\":52597,\"start\":52567},{\"end\":52684,\"start\":52608},{\"end\":52912,\"start\":52860},{\"end\":53164,\"start\":53105},{\"end\":53310,\"start\":53276},{\"end\":53456,\"start\":53436},{\"end\":53679,\"start\":53598},{\"end\":53899,\"start\":53825},{\"end\":54109,\"start\":54028},{\"end\":54330,\"start\":54256},{\"end\":54408,\"start\":54363},{\"end\":54583,\"start\":54516},{\"end\":54682,\"start\":54589},{\"end\":54825,\"start\":54744},{\"end\":55015,\"start\":54944},{\"end\":55171,\"start\":55084},{\"end\":55340,\"start\":55293},{\"end\":55414,\"start\":55372},{\"end\":55630,\"start\":55582},{\"end\":55828,\"start\":55747},{\"end\":56062,\"start\":55995},{\"end\":56228,\"start\":56184}]"}}}, "year": 2023, "month": 12, "day": 17}