{"id": 247593925, "updated": "2023-10-05 16:12:06.696", "metadata": {"title": "Robust Visual Tracking by Segmentation", "authors": "[{\"first\":\"Matthieu\",\"last\":\"Paul\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Danelljan\",\"middle\":[]},{\"first\":\"Christoph\",\"last\":\"Mayer\",\"middle\":[]},{\"first\":\"Luc\",\"last\":\"Gool\",\"middle\":[\"Van\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Estimating the target extent poses a fundamental challenge in visual object tracking. Typically, trackers are box-centric and fully rely on a bounding box to define the target in the scene. In practice, objects often have complex shapes and are not aligned with the image axis. In these cases, bounding boxes do not provide an accurate description of the target and often contain a majority of background pixels. We propose a segmentation-centric tracking pipeline that not only produces a highly accurate segmentation mask, but also internally works with segmentation masks instead of bounding boxes. Thus, our tracker is able to better learn a target representation that clearly differentiates the target in the scene from background content. In order to achieve the necessary robustness for the challenging tracking scenario, we propose a separate instance localization component that is used to condition the segmentation decoder when producing the output mask. We infer a bounding box from the segmentation mask, validate our tracker on challenging tracking datasets and achieve the new state of the art on LaSOT with a success AUC score of 69.7%. Since most tracking datasets do not contain mask annotations, we cannot use them to evaluate predicted segmentation masks. Instead, we validate our segmentation quality on two popular video object segmentation datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.11191", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/PaulDMG22", "doi": "10.48550/arxiv.2203.11191"}}, "content": {"source": {"pdf_hash": "9286efaa3dba58837b628f61f4940a09b3eeb85c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2203.11191v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1e32fae04dec9e31e32b01399b791e022e4927a8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9286efaa3dba58837b628f61f4940a09b3eeb85c.txt", "contents": "\nRobust Visual Tracking by Segmentation\n\n\nMatthieu Paul paulma@vision.ee.ethz.ch \nComputer Vision Lab\nETH Z\u00fcrich\nSwitzerland\n\nMartin Danelljan \nComputer Vision Lab\nETH Z\u00fcrich\nSwitzerland\n\nChristoph Mayer chmayer@vision.ee.ethz.ch \nComputer Vision Lab\nETH Z\u00fcrich\nSwitzerland\n\nLuc Van Gool \nComputer Vision Lab\nETH Z\u00fcrich\nSwitzerland\n\nRobust Visual Tracking by Segmentation\n\nEstimating the target extent poses a fundamental challenge in visual object tracking. Typically, trackers are box-centric and fully rely on a bounding box to define the target in the scene. In practice, objects often have complex shapes and are not aligned with the image axis. In these cases, bounding boxes do not provide an accurate description of the target and often contain a majority of background pixels. We propose a segmentation-centric tracking pipeline that not only produces a highly accurate segmentation mask, but also internally works with segmentation masks instead of bounding boxes. Thus, our tracker is able to better learn a target representation that clearly differentiates the target in the scene from background content. In order to achieve the necessary robustness for the challenging tracking scenario, we propose a separate instance localization component that is used to condition the segmentation decoder when producing the output mask. We infer a bounding box from the segmentation mask, validate our tracker on challenging tracking datasets and achieve the new state of the art on LaSOT with a success AUC score of 69.7%. Since most tracking datasets do not contain mask annotations, we cannot use them to evaluate predicted segmentation masks. Instead, we validate our segmentation quality on two popular video object segmentation datasets. The code and trained models are available at https://github.com/visionml/pytracking.\n\nIntroduction\n\nVisual object tracking is the task of estimating the state of a target object for each frame in a video sequence. The target is solely characterized by its initial state in the video. Current approaches predominately characterize the state itself with a bounding box. However, this only gives a very coarse representation of the target in the image. In practice, objects often have complex shapes, undergo substantial deformations. Often, targets do not align well with the image axes, while most benchmarks use axis-aligned bounding boxes. In such cases, the majority of the image content inside the target's bounding box often consists of background regions which provide limited information about the object itself. In contrast, a segmentation mask precisely indicates the object's extent in the image (see Fig. 1 frames #1600 and #3200). Such information is vital in a variety of applications, including video analysis, video editing, and robotics. In this work, we therefore develop an approach for accurate and robust target object segmentation, even in the highly challenging tracking datasets [16,36]. , the VOS method LWL [5] and our proposed method on two tracking sequences from the LaSOT [16] dataset. The ground-truth annotation ( ) is shown in each frame for reference. Our approach is more robust and predicts a more accurate target representation.\n\nWhile severely limiting the information about the target's state in the video, the aforementioned issues with the bounding box representation can itself lead to inaccurate bounding box predictions, or even tracking failure. To illustrate this, Fig. 1 shows two typical tracking sequences. The tracking method STARK [55] (first row) fails to regress bounding boxes that contain the entire object (#1600, #1400) or even starts tracking the wrong object (#0700). Conversely, segmentation masks are a better fit to differentiate pixels in the scene that belong to the background and the target. Therefore, a segmentation-centric tracking architecture designed to work internally with a segmentation mask of the target instead of a bounding box has the potential to learn better target representations, because it can clearly differentiate background from foreground regions in the scene.\n\nA few recent tracking methods [47,54] have recognized the advantage of producing segmentation masks instead of bounding boxes as final output. However, these trackers are typically bounding-box-centric and the final segmentation mask is obtained by a separate box-to-mask post-processing network. These methods do not leverage the accurate target definition of segmentation masks to learn a more accurate and robust internal representation of the target.\n\nIn contrast, most Video Object Segmentation (VOS) methods [38,5] follow a segmentation-centric paradigm. However, these methods are not designed for the challenging tracking scenarios. Typical VOS sequences consist only of a few hundred frames [41] whereas multiple sequences of more than ten thousand frames exist in tracking datasets [16]. Due to this setup, VOS methods focus on producing highly accurate segmentation masks but are sensitive to distractors, substantial deformations and occlusions of the target object. Fig. 1 shows two typical tracking sequences where the VOS method LWL [5] (second row) produces a fine-grained segmentation mask of the wrong object (#3200) or is unable to detect only the target within a crowd (#0700, #1400).\n\nWe propose Robust Visual Tracking by Segmentation (RTS), a unified tracking architecture capable of predicting accurate segmentation masks. To design a segmentation-centric approach, we take inspiration from the aforementioned LWL [5] method. However, to achieve robust and accurate segmentation on Visual Object Tracking (VOT) datasets, we introduce several new components. In particular, we propose an instance localization branch trained to predict a target appearance model, which allows occlusion detection and target identification even in cluttered scenes. The output of the instance localization branch is further used to condition the high-dimensional mask encoding. This allows the segmentation decoder to focus on the localized target, leading to a more robust mask prediction. Since our proposed method contains a segmentation and instance memory that need to be updated with previous tracking results, we design a memory management module. This module first assesses the prediction quality, decides whether the sample should enter the memory and, when necessary, triggers the model update. Contributions Our contributions are the following: (i) We propose a unified tracking architecture capable of predicting robust classification scores and accurate segmentation masks. We design separate feature spaces and memories to ensure optimal receptive fields and update rates for segmentation and instance localization. (ii) To produce a segmentation mask which agrees with the instance prediction, we design a fusion mechanism that conditions the segmentation decoder on the instance localization output and leads to more robust tracking performance. (iii) We introduce an effective inference procedure capable of fusing the instance localization output and mask encoding to ensure both robust and accurate tracking. (iv) We perform comprehensive evaluation and ablation studies of the proposed tracking pipeline on multiple popular tracking benchmarks. Our approach achieves the new state of the art on LaSOT with an area-under-the-curve (AUC) score of 69.7%.\n\n\nRelated Work\n\n\nVisual Object Tracking\n\nOver the years, research in the field of visual tracking has been accelerated by the introduction of new and challenging benchmarks, such as LaSOT [16], , and TrackingNet [37]. This led to the introduction of new paradigms in visual object tracking, based on Discriminative Correlation Filters (DCFs), Siamese networks and Transformers.\n\nOne of the most popular type of approaches, DCF-based visual trackers [6,22,15,32,12,48,61,3,14] essentially solve an optimization problem to estimate the weights of the DCF that allow to distinguish foreground from background regions. The DCF is often referred to as the target appearance model and allows to localize the target in the video frame. More recent DCF approaches [3,14] enable end-to-end training by unrolling a fixed number of the optimization iterations during offline training.\n\nSiamese tracking methods have gained in popularity due to their simplicity, speed and end-to-end trainability [44,2,43,62,20,50,21,29,28]. These trackers learn a similarity metric using only the initial video frame and its annotation that allows to clearly identify the target offline. Since no online learning component is involved, these trackers achieve high frame rates at the cost of limited online adaptability to changes of the target's appearance. Nonetheless, several methods have been proposed to overcome these issues [44,2,29,28].\n\nVery recently, Transformer-based trackers have achieved state-of-the-art performance on many datasets, often outperforming their rivals. This group of trackers typically uses a Transformer component in order to fuse information extracted from training and test frames. This produces discriminative features that allow to accurately localize and estimate the target in the scene [8,56,55,49,34]. Video Object Segmentation Semi-supervised VOS is the task of classifying all pixels belonging to the target in each video frame, given only the segmentation mask of the target in the initial frame. The cost of annotating accurate segmentation masks is limiting the sequence length and number of videos contained in available VOS datasets. Despite the relatively small size of VOS datasets compared to other computer vision problems, new benchmarks such as Youtube-VOS [53] and DAVIS [41] accelerated the research progress in the last years.\n\nSome methods rely on a learnt target detector [7,46,33], others learn how to propagate the segmentation mask across frames [52,40,30,25]. Another group of methods uses feature matching techniques across one or multiple frames with or without using an explicit spatio-temporal memory [9,23,45,38]. Recently, Bhat et al. [5] employed meta-learning approach, introducing an end-to-end trainable VOS architecture. In this approach, a few-shot learner predicts a learnable labels encoding. It generates and updates online the parameters of a segmentation target model that produces the mask encoding used to generate the final segmentation mask. Joint Visual Tracking and Segmentation A group of tracking methods have already identified the advantages of predicting a segmentation mask instead of a bounding box [54,60,47,31,51,42]. Siam-RCNN is a box-centric tracker that uses a pretrained box2seg network to predict the segmentation mask given a bounding box prediction. In contrast, AlphaRefine represents a novel box2seg method that has been evaluated with many recent trackers such as SuperDiMP [14] and SiamRPN++ [28]. Further, Zhao et al.\n\n[60] focus on generating segmentation masks from bounding box annotations in videos using a spatio-temporal aggregation module to mine consistencies of the scene across multiple frames. Conversely, SiamMask [51] and D3S [31] are segmentation-centric trackers that produce a segmentation mask directly, without employing a box2seg module. In particular, SiamMask [51] is a fully-convolutional Siamese network with a separate branch which predicts binary segmentation masks supervised by a segmentation loss.\n\nFrom a high-level view, the single-shot segmentation tracker D3S [31] is most related to our proposed method. Both methods employ two dedicated modules or branches; one for localization and one for segmentation. D3S adopts the target classification component of ATOM [12], requiring online optimization of weights in a two-layer CNN. In contrast, we learn online the weights of a DCF similar  to DiMP [3]. For segmentation, D3S [31] propose a feature matching technique that matches test frame features with background and foreground features corresponding to the initial frame. In contrast, we adopt the few-shot learning based model prediction proposed in LWL [5] to produce accurate segmentation masks. Furthermore, D3S proposes to simply concatenate the outputs of both modules whereas we learn a localization encoding to condition the segmentation mask decoding based on the localization information. Compared to D3S, we update not only the instance localization but also the segmentation models and memories. Hence, our method integrates specific memory management components.\n\n\nMethod\n\n\nOverview\n\nVideo object segmentation methods can produce high quality segmentation masks but are typically not robust enough for video object tracking. Robustness becomes vital for medium and long sequences, which are most prevalent in tracking datasets [16,36]. In such scenarios, the target object frequently undergoes substantial appearance changes. Occlusions and similarly looking objects are common. Hence, we propose to adapt a typical VOS approach with tracking components to increase its robustness. In particular, we base our approach on the Learning What to Learn (LWL) [5] method and design a novel and segmentationcentric tracking pipeline that estimates accurate object masks instead of bounding boxes. During inference, a segmentation mask is typically not provided in visual object tracking. Hence, we use STA [60] to generate a segmentation mask from the provided initial bounding box. An overview of our RTS method is shown in Fig. 2. Our pipeline consists of a backbone network, a segmentation branch, an instance localization branch and a segmentation decoder. For each video frame, the backbone first extracts a feature map x b . These features are further processed into segmentation features x s and classification features x c to serve as input for their respective branch. The segmentation branch is designed to capture the details of the object with a high dimensional mask encoding, whereas the instance localization branch aims at providing a coarser but robust score map representing the target location. Both branches contain components learned online, trained on memories (D s and D c ) that store features and predictions of past frames. The instance localization branch has two purposes. The first is to control models and memories updating. The second is used to condition the segmentation mask decoder. To do so, we add instance localization information with a learnt score encoding produced by H \u03b8 . The obtained segmentation scores and the raw instance model score map are then used to generate the final segmentation mask output.\n\n\nSegmentation Branch\n\nThe architecture of the segmentation branch is adopted from LWL [5], and we briefly review it here. It consists of a segmentation sample memory D s , a label generator E \u03b8 , a weight predictor W \u03b8 , a few-shot learner A \u03b8 and a segmentation model T \u03c4 . The goal of the few-shot learner A \u03b8 is producing the parameters \u03c4 of the segmentation model T \u03c4 such that the obtained mask encoding x m contains the information needed to compute the final segmentation mask of the target object. The label mask encodings used by the few-shot learner are predicted by the label generator E \u03b8 . The few-shot learner is formulated through the following optimization problem, which is unrolled through steepest descent iterations in the network L s (\u03c4 ) = 1 2 (xs,ys)\u2208Ds\nW \u03b8 (y s ) \u00b7 T \u03c4 (x s ) \u2212 E \u03b8 (y s ) 2 + \u03bb s 2 \u2225\u03c4 \u2225 2 ,(1)\nwhere D s corresponds to the segmentation memory, x s denotes the segmentation features, y s the segmentation masks and \u03bb s is a learnable scalar regularization parameter. The weight predictor W \u03b8 produces sample confidence weights for each spatial location in each memory sample. Applying the optimized model parameters \u03c4 * within the segmentation model produces the mask encoding x m = T \u03c4 * (x s ) for the segmentation features x s . LWL [5] feeds the mask encoding directly into the segmentation decoder to produce the segmentation mask. For long and challenging tracking sequences, only relying on the mask encoding may lead to an accurate segmentation mask, but often for the wrong object in the scene (see Fig 1). Since LWL [5] is only able to identify the target to a certain degree in challenging tracking sequences, we propose to condition the mask encoding based on an instance localization representation, described next.\n\n\nInstance Localization Branch\n\nThe segmentation branch can produce accurate masks but typically lacks the necessary robustness for tracking in medium or long-term sequences. Especially challenging are sequences where objects similar to the target appear, where the target object is occluded or vanishes from the scene for a short time. Therefore, we propose a dedicated branch for target instance localization, in order to robustly identify the target among distractors or to detect occlusions. A powerful tracking paradigm that learns a target-specific appearance model on both foreground and background information are discriminative correlation filters (DCF) [6,22,13,3]. These methods learn the weights of a filter that differentiates foreground from background pixels represented by a score map, where the maximal value corresponds to the target's center.\n\nSimilar to the segmentation branch, we propose an instance localization branch that consists of a sample memory D c and a model predictor P \u03b8 . The latter predicts the parameters \u03ba of the instance model T \u03ba . The instance model is trained online to produce the target score map used to localize the target object. To obtain the instance model parameters \u03ba we minimize the following loss function\nL c (\u03ba) = (xc,yc)\u2208Dc R T \u03ba (x c ), y c 2 + \u03bb c 2 \u2225\u03ba\u2225 2 ,(2)\nwhere D c corresponds to the instance memory containing the classification features x c and the Gaussian labels y c . R denotes the robust hinge-like loss [3] and \u03bb c is a fixed regularization parameter. To solve the optimization problem we apply the method from [3], which unrolls steepest descent iterations of the Gauss-Newton approximation of (2) to obtain the final model parameters \u03ba * . The score map can then be obtained with s c = T \u03ba * (x c ) by evaluating the target model on the classification features x c .\n\n\nInstance-Conditional Segmentation Decoder\n\nIn video object segmentation the produced mask encoding is directly fed into the segmentation decoder to generate the segmentation mask. However, solely relying on the mask encoding is not robust enough for the challenging tracking scenario, see Fig 1. Thus, we propose to integrate the instance localization information into the segmentation decoding procedure. In particular, we condition the mask encoding on a learned encoding of the instance localization score map. First, we encode the raw score maps using a multi-layer Convolutional Neural Network (CNN) to learn a suitable representation. Secondly, we condition the mask encoding with the learned representation using element-wise addition. The entire conditioning procedure can be defined as\nx f = x m + H \u03b8 (s c ),\nwhere H \u03b8 denotes the CNN encoding the scores s c , and x m the mask encoding. The resulting features are then fed into the segmentation decoder that produces the segmentation scores of the target object.\n\n\nJointly Learning Instance Localization and Segmentation\n\nIn this section, we describe our general training strategy and parameters. In particular, we further detail the segmentation and classification losses that we use for offline training. Segmentation Loss First, we randomly sample J frames from an annotated video sequence and sort them according to their frame IDs in increasing order to construct the training sequence\nV = {(x j b , y j s , y j c )} J\u22121 j=0 , where x j b = B \u03b8 (I j )\nare the extracted features of the video frame I j using the backbone B \u03b8 , y j s is the corresponding segmentation mask and y j c denotes the Gaussian label at the target's center location. We start with entry v 0 \u2208 V and store it in the segmentation D s and instance memory D c and obtain parameters \u03c4 0 and \u03ba 0 of the segmentation and instance model. We use these parameters to compute the segmentation loss for v 1 \u2208 V. Using the predicted segmentation mask, we update the segmentation model parameters to \u03c4 1 but keep the instance model parameters fixed. Segmentation parameters typically need to be updated frequently to enable accurate segmentation. Conversely, we train the model predictor on a single frame only. The resulting instance model generalizes to multiple unseen future frames, ensuring robust target localization. The resulting segmentation loss for the entire sequence V can thus be described as follows\nL seq s (\u03b8; V) = J\u22121 j=1 L s D \u03b8 T \u03c4 j\u22121 (x j s ) + H \u03b8 T \u03ba 0 (x j c ) , y j s ,(3)\nwhere\nx s = F \u03b8 (x b ) and x c = G \u03b8 (x b )\nand L s is the Lovasz segmentation loss [1]. Classification Loss Instead of training our tracker only with the segmentation loss, we add an auxiliary loss to ensure that the instance module produces score maps localizing the target via a Gaussian distribution. These score maps are essential to update the segmentation and instance memories and to generate the final output. As explained before, we use only the first training v 0 \u2208 V to optimize the instance model parameters. To encourage fast convergence, we use not only the parameters corresponding to the final iteration N iter of the optimization method \u03ba 0 (Niter) explained in Sec. 3.3, but also all the intermediate parameters \u03ba 0 (i) of loss computation. The final target classification loss for the whole sequence V is defined as follows\nL seq c (\u03b8; V) = J\u22121 j=1 1 N iter Niter i=0 L c T \u03ba 0 (i) (x j c ), y j c ,(4)\nwhere L c is the hinge loss defined in [3]. To train our tracker we combine the segmentation and classification losses using the scalar weight \u03b7 and minimize both losses jointly optimizer with a learning rate decay of 0.2 at epochs 25, 115 and 160. We weigh the losses such that the segmentation loss is predominant but in the same range as the classification loss. We empirically choose \u03b7 = 10. Further details about training and the network architecture are given in the appendix.\nL seq tot (\u03b8; V) = L seq s (\u03b8; V) + \u03b7 \u00b7 L seq c (\u03b8; V).(5)\n\nInference\n\nMemory Management and Model Updating Our tracker consists of two different memory modules. A segmentation memory that stores segmentation features and predicted segmentation masks of previous frames. In contrast, an instance memory contains classification features and Gaussian labels marking the center location of the target in the predicted segmentation mask of the previous video frame. The quality of the predicted labels directly influences the localization and segmentation quality in future video frames. Hence, it is crucial to avoid contaminating the memory modules with predictions that do not correspond to the actual target. We propose the following strategy to keep the memory as clean as possible. (a) If the instance model is able to clearly localize the target (maximum value in the score map larger than t sc = 0.3) and the segmentation model constructs a valid segmentation mask (at least one pixel above t ss = 0.5) we update both memories with the current predictions and features. (b) If either the instance localization or segmentation fail to identify the target we omit updating the segmentation memory. (c) If only the segmentation mask fails to represent the target but the instance model can localize it, we update the instance memory only. (d) If instance localization fails we do not update either memory. Further, we trigger the few-shot learner and model predictor after 20 frames have passed, but only if the corresponding memory has been updated. Final Mask Output Generation We obtain the final segmentation mask by thresholding the segmentation decoder output. To obtain the bounding box required for standard tracking benchmarks, we report the smallest axis-aligned box that contains the entire estimated object mask. Inference Details We set the input image resolution such that the segmentation learner features have a resolution of 52 \u00d7 30 (stride 16), while the instance learner operates on features of size 26 \u00d7 15 (stride 32). The learning rate is set to 0.1 and 0.01 for the segmentation and instance learner respectively. We use a maximum buffer of 32 frames for the segmentation memory and 50 frames for the instance memory. We keep the samples corresponding to the initial frame in both memories and replace the oldest entries if the memory is full. We update both memories for the first 100 video frames and afterwards only after every 20 th frame. We randomly augment the sample corresponding to the initial frame with vertical flip, random translation and blurring.\n\n\nEvaluation\n\nOur approach is developed within the PyTracking [11] framework. The implementation is done with PyTorch 1.9 with CUDA 11.1. Our model is evaluated on a single Nvidia GTX 2080Ti GPU. Our method achieves an average speed of 30 FPS on LaSOT [16]. Each number corresponds to the average of five runs with different random seeds.\n\n\nBranch Ablation Study\n\nFor the ablation study, we analyze the impact of the instance branch on three datasets and present the results in Tab. 1. First, we report the performance of LWL [5] since we build upon it to design our final tracking pipeline. We use the network weights provided by Bhat et al. [5] and the corresponding inference settings. We input the same segmentation masks obtained from the initial bounding box for LWL as used for our method. We observe that LWL is not robust enough for challenging tracking scenarios. The second row in Tab. 1 corresponds to our method but we omit the proposed instance branch. Hence, we use the proposed inference components and settings and train the tracker as explained in Sec. 3.5, but with conditioning removed. We observe that even without the instance localization branch our tracker can achieve competitive performance on all three datasets (e.g. +5.6% on LaSOT). Fully integrating the instance localization branch increases the performance even more (e.g. +4.4 on LaSOT). Thus, we conclude that adapting the baseline method to the tracking domain improves the tracking performance. To boost the performance and achieve state-of-theart results, an additional component able to increase the tracking robustness is required.\n\n\nInference Parameters\n\nIn this part, we ablate two key aspects of our inference strategy. First, we study the effect of relying on the instance branch if the segmentation decoder is unable to localize the target (max(s s ) < t ss ). Second, we study different values for t sc that determines whether the target is detected by the instance model, see Tab. 2.\n\nIf the segmentation branch cannot identify the target, using the instance branch improves tracking performance on all datasets (e.g. +1.3% on UAV123).  Furthermore, Tab. 2 shows that our tracking pipeline achieves the best performance when setting t sc = 0.3 whereas smaller or larger values for t sc decrease the tracking accuracy. Hence, it is important to find a suitable trade-off between frequently updating the model and memory to quickly adapt to appearance changes and updating only rarely to avoid contaminating the memory and model based on wrong predictions.\n\n\nComparison to the state of the art\n\nAssessing segmentation accuracy on tracking datasets is not possible since only bounding box annotations are provided. Therefore, we compare our approach on six VOT benchmarks and validate the segmentation masks quality on two VOS datasets.\n\nLaSOT [16] We evaluate our method on the test set of the LaSOT dataset, consisting of 280 sequences with 2500 frames on average. Thus, the benchmark challenges the long term adaptability and robustness of trackers. Fig. 3 shows the success plot reporting the overlap precision OP with respect to the overlap threshold T . Trackers are ranked by AUC score. In addition, Tab. 3 reports the precision and normalized precision for all compared methods. Our method outperforms the state-of-the-art ToMP-50 [34] and ToMP-101 [34] by large margins (+1.2% and +2.1% AUC respectively). Our method is not only as robust as KeepTrack (see the success plot for T < 0.2) but also estimates far more accurate bounding boxes than any tracker (0.8 < T < 1.0).      We use the validation set which consist of 507 sequences. They contain 91 object categories out of which 26 are unseen in the training set. The results presented in Tab. 8 were generated by an online server after uploading the raw results. On this benchmark, we want to validate the quality of the produced segmentation masks rather than to achieve the best accuracy possible. Hence, we use the same model weight as for VOT without further fine tuning. When using the provided segmentation masks for initialization, we observe that our method performs slightly worse than LWL [5] and STA [60] (-1.3 G, -0.9 G) but still outperforms the VOS method STM [38] (+0.5 G). We conclude that \n\n\nConclusion\n\nWe introduced RTS, a robust, end-to-end trainable, segmentation-driven tracking method that is able to generate accurate segmentation masks. Compared to the traditional bounding box outputs of classical visual object trackers, segmentation masks enable a more accurate representation of the target's shape and extent. The proposed instance localization branch helps increasing the robustness of our tracker to enable reliable tracking even for long sequences of thousands of frames. Our method outperforms previous segmentation-driven tracking methods by a large margin, and it is competitive on several VOT benchmarks. In particular, we set a new state of the art on the challenging LaSOT [16] dataset with a success AUC of 69.7%. Competitive results on two VOS datasets confirm the high quality of the generated segmentation masks.\n\nAcknowledgements This work was partly supported by uniqFEED AG and the ETH Future Computing Laboratory (EFCL) financed by a gift from Huawei Technologies. at the current target location and 6 times larger than the current estimated target size, when it does not exceed the size of the image. The estimation of the target state (position and size) is therefore crucial to ensure an optimal crop. In most situations, the segmentation output is used to determine the target state since it has a high accuracy. The target center is computed as the center of mass of the predicted per-pixel segmentation probability scores. The target size is computed as the variance of the segmentation probability scores.\n\nIf the segmentation branch cannot find the target (as described in the main paper), but the instance branch still outputs a high enough confidence score, we use it to update the target position. This is particularly important in sequences where the target is becoming too small for some time, but we can still track the target position.\n\nWhen both branch cannot find the target, the internal state of the tracker is not updated. We upscale the search area based on the previous 60 valid predicted scales. This is helpful in situations where the size of the object shrinks although its size does not change. This typically happens during occlusions, or if the target goes out of the frame partially or completely.\n\n\nC Additional Ablations\n\nIn this section, we provide additional ablation studies related to our method, first on the weighting of the segmentation and classification losses used for training, Weighting segmentation and classification losses For this ablation, we study the weighting of the segmentation loss L s and the instance localization loss L c in the total loss L tot . It used to train our model and its influence on the overall performance during tracking. We recall that Table A1 shows the results when training the tracker with three different values of \u03b7 on five VOT datasets. First, we examine the case where we omit the auxiliary instance localization loss (\u03b7 = 0.0). This means that the whole pipeline is trained for segmentation and the instance branch is not trained to produce specifically accurate localization scores. We observe that this setting leads to the lowest performance on all tested datasets, often by a large margin. Secondly, we test a dominant segmentation loss (\u03b7 = 0.4), since the segmentation branch needs to be trained for a more complex task than the instance branch. We see a performance gain for almost all datasets. Thus, employing the auxiliary loss to train the instance localization branch helps to improve the tracking performance. We observe that using the auxiliary loss leads to localization scores generated during inference that are sharper, cleaner and localize the center of the target more accurately. Finally, we put an even higher weight on the classification term (\u03b7 = 10). This setup leads to an even more accurate localization, and leads to the best results on average. Thus, we set \u03b7 = 10 to train our tracking pipeline. Fine-tuning on Youtube-VOS [16] In this section, we analyze whether we can gear our pipeline towards VOS benchmarks. To do that, we take our model  Precision plot Fig. A2. Success, precision and normalized precision plots on LaSOT [16]. Our approach outperforms all other methods by a large margin in AUC, reported in the legend.\nL tot = L s + \u03b7 \u00b7 L c .(6)\nand inference parameters, and modify them slightly. On the one hand, the model is fined-tuned for 50 epochs using Youtube-VOS [53] only for both training and validation. We also increase the initialization phase from 100 to 200 frames, and remove the relative target scale change limit from one frame to the next. In our standard model, we limit that scale change to 20% for increased robustness.\n\nThe results are presented in Table A2 for Youtube-VOS [53] and Davis [41]. We observe that the performances between both of our models stay very close for Davis, but that the fine-tuned model is getting closer to the baseline LWL [5] for Youtube-VOS. The more frequent updates seem to help, and not restricting the scale change of objects from frame to frames seems to play a role, since we get an improvement of 0.6 in G score.\n\n\nD Additional Evaluation results\n\nIn this section we provide additional plots of our approach on different benchmarks, and a attribute analysis on LaSOT [16]. Success plots for LaSOT [16], NFS [19] and UAV123 [36] We provide in Figure A2 all the plots for the metrics we report for LaSOT [16] in the paper:  Success, Normalized Precision and Precision plots. For completeness, we provide the success plots for NFS [19] and UAV123 [36] in Figure A3. Attribute analysis on LaSOT [16] In this section, we focus on the dataset sequences attributes. We compare our approach to numerous other trackers, and provide the detailed results in Table A3. Furthermore, we highlight the strength of our approach in Figure A4 by focusing the comparison only to the two current state-of-the-art methods ToMP-101 and ToMP-50 [34]. There are 14 attributes provided for LaSOT [16] sequences, representing different kind of challenges the tracker has to deal with in different situations. Compared to existing trackers, our method achieves better AUC scores in 11 out of 14 attributes. In particular, we outperform ToMP-50 [34] and ToMP-101 [34] by a large margin for the following attributes: Camera Motion (+4.2% and +2.7%), Scale Variation (+1.8% and 0.9%), Deformation (+3.1% and +2.2%), Motion Blur (+3.1% and +2.5%) and Aspect Ratio Change (+1.7% and +1.0%). Our method is only outperformed on three attributes by KeepTrack [35] and ToMP [34] for Fast Motion (-2.3% to -4.1%) and for Illumination Variation (-0.3% to -1.4%). For Background Clutter, RTS outperforms ToMP-50 [34]  E Additional Content Figure A5 shows additional visual results compared to other state-of-the-art trackers on 6 different sequences of LaSOT [16]. For more content, we refer the reader to: https://github.com/visionml/pytracking. Qualitative results on LaSOT [16] of our approach compared to the previous state-of-the-art methods KeepTrack [35] and STARK-ST101 [55]. As they do not produce segmentation masks, we represent ours as a red overlay and print for all methods the predicted bounding boxes with the following color code: KeepTrack STARK-ST101 RTS\n\nFig. 1 .\n1Comparison between the VOT method Stark [55]\n\nFig. 2 .\n2Overview of our entire online tracking pipeline used for inference, see Sec 3.1.\n\nFig. 3 .\n3Success (left) and Precision (right) plots on LaSOT[16] with other state-ofthe-art methods. The AUCs for all methods are ordered and reported in the legend. Our method outperforms all existing approaches, both in Overlap Precision (left) and Distance Precision (right).\n\nFig. A1 .\nA1Fan, H., Ling, H.: Cract: Cascaded regression-align-classification for robust visual tracking. arXiv preprint arXiv:2011.12483 (2020) 13 18. Fu, Z., Liu, Q., Fu, Z., Wang, Y.: Stmtrack: Template-free visual tracking with space-time memory networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2021) 11, 12, 13, 23 19. Galoogahi, H.K., Fagg, A., Huang, C., Ramanan, D., Lucey, S.: Need for speed: A benchmark for higher frame rate object tracking. In: ICCV (2017) 10, 11, 13, 21, 22, 23 20. Guo, Q., Feng, W., Zhou, C., Huang, R., Wan, L., Wang, S.: Learning dynamic siamese network for visual object tracking. In: ICCV (2017) 3 21. He, A., Luo, C., Tian, X., Zeng, W.: Towards a better match in siamese network based visual object tracker. In: ECCV workshop (2018) 3 22. Henriques, J.F., Caseiro, R., Martins, P., Batista, J.: High-speed tracking with kernelized correlation filters. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 37(3), 583-596 (2015) 3, 7 23. Hu, Y.T., Huang, J.B., Schwing, A.G.: Videomatch: Matching based video object segmentation. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 54-70 (2018) 4 24. Huang, L., Zhao, X., Huang, K.: Got-10k: A large high-diversity benchmark for generic object tracking in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 43(5), 1562-1577 (2021) 3, 8, 12, 21 25. Khoreva, A., Benenson, R., Ilg, E., Brox, T., Schiele, B.: Lucid data dreaming for object tracking. In: The DAVIS Challenge on Video Object Segmentation (2017) 4 26. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Proceedings of the International Conference on Learning Representations (ICLR) (2014) 9 27. Kristan, M., Leonardis, A., Matas, J., Felsberg, M., Pflugfelder, R., K\u00e4m\u00e4r\u00e4inen, J.K., Danelljan, M., Zajc, L.\u010c., Luke\u017ei\u010d, A., Drbohlav, O., He, L., Zhang, Y., Yan, S., Yang, J., Fern\u00e1ndez, G., et al: The eighth visual object tracking vot2020 challenge results. In: Proceedings of the European Conference on Computer Vision Workshops (ECCVW) (August 2020) 13 28. Li, B., Wu, W., Wang, Q., Zhang, F., Xing, J., Yan, J.: Siamrpn++: Evolution of siamese visual tracking with very deep networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019) 3, 4, 12 29. Li, B., Yan, J., Wu, W., Zhu, Z., Hu, X.: High performance visual tracking with siamese region proposal network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2018) 3, 4 30. Li, X., Change Loy, C.: Video object segmentation with joint re-identification and attention-aware mask propagation. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 90-105 (2018) 4 31. Lukezic, A., Matas, J., Kristan, M.: D3s -a discriminative single shot segmentation tracker. In: CVPR (2020) 4, 5, 11, 12, 13, 14 32. Lukezic, A., Voj\u00edr, T., Zajc, L.C., Matas, J., Kristan, M.: Discriminative correlation filter tracker with channel and spatial reliability. International Journal of Computer Vision (IJCV) 126(7), 671-688 (2018) 3 33. Maninis, K.K., Caelles, S., Chen, Y., Pont-Tuset, J., Leal-Taix\u00e9, L., Cremers, D., Van Gool, L.: Video object segmentation without temporal information. IEEE 49. Wang, N., Zhou, W., Wang, J., Li, H.: Transformer meets tracker: Exploiting temporal context for robust visual tracking. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2021) 4, 11, 12, 13, 23 50. Wang, Q., Teng, Z., Xing, J., Gao, J., Hu, W., Maybank, S.J.: Learning attentions: Residual attentional siamese network for high performance online visual tracking. In: CVPR (2018) 3 51. Wang, Q., Zhang, L., Bertinetto, L., Hu, W., Torr, P.H.: Fast online object tracking and segmentation: A unifying approach. In: Proceedings of the IEEE conference on computer vision and pattern recognition (2019) 4, 14 52. Wug Oh, S., Lee, J.Y., Sunkavalli, K., Joo Kim, S.: Fast video object segmentation by reference-guided mask propagation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7376-7385 (2018) 4 53. Xu, N., Yang, L., Fan, Y., Yue, D., Liang, Y., Yang, J., Huang, T.: Youtube-vos: A large-scale video object segmentation benchmark (2018) 4, 8, 9, 13, 14, 21, 22 54. Yan, B., Wang, D., Lu, H., Yang, X.: Alpha-refine: Boosting tracking performance by precise bounding box estimation. In: CVPR (2021) 2, 4, 11, 12, 13, 23 55. Yan, B., Peng, H., Fu, J., Wang, D., Lu, H.: Learning spatio-temporal transformer for visual tracking. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 10448-10457 (October 2021) 2, 4, 11, 12, 13, 23, 24 56. Yu, B., Tang, M., Zheng, L., Zhu, G., Wang, J., Feng, H., Feng, X., Lu, H.: High-performance discriminative tracking with transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 9856-9865 (October 2021) 4, 12 57. Yu, Y., Xiong, Y., Huang, W., Scott, M.R.: Deformable siamese attention networks for visual object tracking. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020) 13 58. Zhang, Z., Peng, H., Fu, J., Li, B., Hu, W.: Ocean: Object-aware anchor-free tracking. In: Proceedings of the European Conference on Computer Vision (ECCV) (August 2020) 11 59. Zhang, Z., Zhong, B., Zhang, S., Tang, Z., Liu, X., Zhang, Z.: Distractor-aware fast tracking via dynamic convolutions and mot philosophy. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2021) 11 60. Zhao, B., Bhat, G., Danelljan, M., Van Gool, L., Timofte, R.: Generating masks from boxes by mining spatio-temporal consistencies in videos. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 13556-13566 (October 2021) 4, 5, 8, 9, 12, 13, 14, 21 61. Zheng, L., Tang, M., Chen, Y., Wang, J., Lu, H.: Learning feature embeddings for discriminant model based tracking. In: Proceedings of the European Conference on Computer Vision (ECCV) (August 2020) 3 62. Zhu, Z., Wang, Q., Bo, L., Wu, W., Yan, J., Hu, W.: Distractor-aware siamese networks for visual object tracking. In: ECCV (Classification Scores Encoder H \u03b8 .\n\nFig. A5 .\nA5Fig. A5. Qualitative results on LaSOT [16] of our approach compared to the previous state-of-the-art methods KeepTrack [35] and STARK-ST101 [55]. As they do not produce segmentation masks, we represent ours as a red overlay and print for all methods the predicted bounding boxes with the following color code: KeepTrack STARK-ST101 RTS\n\n\nTraining Details We use the train sets of LaSOT[16],, Youtube-VOS [53] and DAVIS[41]. For VOT datasets that only provide annotated bounding boxes, we use these boxes and STA [60] to generate segmentation masks and treat them as ground truth annotations during training. STA [60] is trained separately on YouTube-VOS 2019 [53] and DAVIS 2017[39]. For our model, we use ResNet-50 with pre-trained MaskRCNN weights as our backbone and initialize the segmentation model and decoder weights with the ones available from LWL[5]. We train for 200 epochs and sample 15'000 videos per epoch, which takes 96 hours to train on a single Nvidia A100 GPU. We use the ADAM[26]   \n\nTable 1 .\n1Comparison between our segmentation network baseline LWL and our pipeline, with and without Instance conditioning on different VOT datasets.Method \nSeg. \nInst. Branch \nLaSOT [16] \nNFS [19] \nUAV123 [36] \nBranch Conditioning \nAUC \nP \nNP \nAUC \nP \nNP \nAUC \nP \nNP \n\nLWL [5] \n\u2713 \n-\n59.7 60.6 63.3 \n61.5 75.1 76.9 \n59.7 78.8 71.4 \nRTS \n\u2713 \n\u2717 \n65.3 68.5 71.5 \n65.8 84.0 85.0 \n65.2 85.6 78.8 \nRTS \n\u2713 \n\u2713 \n69.7 73.7 76.2 \n65.4 82.8 84.0 \n67.6 89.4 81.6 \n\n\nTable 2 .\n2Ablation on inference strategies. The first column analyzes the effect of using the instance branch as fallback for target localization if the segmentation branch is unable to detect the target (max(ss) < ts s ). The second column shows the impact of different confidence thresholds ts c .Table 3. Comparison to the state of the art on the LaSOT[16] test set in terms of AUC score. The methods are ordered by AUC score.Inst. Branch ts c \nLaSOT [16] \nNFS [19] \nUAV123 [36] \nFallback \nAUC \nP \nNP \nAUC \nP \nNP \nAUC \nP \nNP \n\n\u2717 \n0.30 \n69.3 73.1 75.9 \n65.3 82.7 84.0 \n66.3 87.2 80.4 \n\u2713 \n0.30 \n69.7 73.7 76.2 \n65.4 82.8 84.0 \n67.6 89.4 81.6 \n\n\u2713 \n0.20 \n68.6 72.3 75.0 \n65.3 82.7 83.9 \n67.0 88.7 80.7 \n\u2713 \n0.30 \n69.7 73.7 76.2 \n65.4 82.8 84.0 \n67.6 89.4 81.6 \n\u2713 \n0.40 \n69.1 72.7 75.6 \n63.3 79.7 81.7 \n67.1 89.1 80.7 \n\nToMP ToMP Keep STARK Alpha \nSiam \nTr Super STM Pr \nDM \nRTS 101 \n50 Track ST-101 Refine TransT R-CNN DiMP DiMP Track DiMP LWL Track LTMU DiMP Ocean D3S \n[34] \n[34] \n[35] \n[55] \n[54] \n[8] \n[47] \n[49] \n[11] \n[18] \n[14] \n[5] \n[59] \n[10] \n[3] \n[58] \n[31] \n\nPrecision \n73.7 73.5 72.2 70.2 \n72.2 \n68.8 \n69.0 \n68.4 \n66.3 65.3 63.3 60.8 60.6 59.7 \n57.2 \n56.7 56.6 49.4 \nNorm. Prec \n76.2 79.2 78.0 77.2 \n76.9 \n73.8 \n73.8 \n72.2 \n73.0 72.2 69.3 68.8 63.3 66.9 \n66.2 \n65.0 65.1 53.9 \nSuccess (AUC) 69.7 68.5 67.6 67.1 \n67.1 \n65.9 \n64.9 \n64.8 \n63.9 63.1 60.6 59.8 59.7 58.4 \n57.2 \n56.9 56.0 49.2 \n\n\u2206 AUC to Ours -\n\u21911.2 \u21912.1 \u21912.6 \u21912.6 \n\u21913.8 \u21914.8 \n\u21914.9 \u21915.8 \u21916.6 \u21919.1 \u21919.9 \u219110.0 \u219111.3 \u219112.5 \u219112.8 \u219113.7 \u219120.5 \n\n\n\nTable 4 .\n4Resultson the GOT-10k validation set [24] in terms of Average Overlap \n(AO) and Success Rates (SR) for overlap thresholds of 0.5 and 0.75. \n\nRTS STA LWL PrDiMP-50 DiMP-50 SiamRPN++ \n[60] [5] \n[14] \n[3] \n[28] \n\nSR0.50(%) 94.5 95.1 92.4 \n89.6 \n88.7 \n82.8 \nSR0.75(%) 82.6 85.2 82.2 \n72.8 \n68.8 \n-\nAO(%) \n85.2 86.7 84.6 \n77.8 \n75.3 \n73.0 \n\n\n\nTable 5 .\n5Comparison to the state of the art on the TrackingNet[37] test set in terms of AUC scores, Precision and Normalized Precision.GOT-10k [24]The large-scale GOT-10k dataset contains over 10.000 shorter sequences. Since we train our method on several datasets instead of only GOT-10k train, we evaluate it on the val set only, which consists of 180 short videos.ToMP ToMP Keep STARK STARK \nSiam Alpha STM \nTr Super Pr \nRTS 101 \n50 Track ST101 ST50 STA LWL TransT R-CNN Refine Track DTT DiMP DiMP DiMP D3S \n[34] \n[34] \n[35] \n[55] \n[55] \n[60] [5] \n[8] \n[47] \n[54] \n[18] [56] [49] \n[11] \n[14] [31] \n\nPrecision \n79.4 78.9 78.6 73.8 \n-\n-\n79.1 78.4 80.3 \n80.0 \n78.3 76.7 78.9 73.1 73.3 70.4 66.4 \nNorm. Prec \n86.0 86.4 86.2 83.5 \n86.9 \n86.1 \n84.7 84.4 86.7 \n85.4 \n85.6 85.1 85.0 83.3 83.5 81.6 76.8 \nSuccess (AUC) 81.6 81.5 81.2 78.1 \n82.0 \n81.3 \n81.2 80.7 81.4 \n81.2 \n80.5 80.3 79.6 78.4 78.1 75.8 72.8 \n\n\u2206 AUC to Ours -\n\u21910.1 \u21910.4 \u21913.5 \u21930.4 \n\u21910.3 \u21910.4 \u21910.9 \u21910.2 \n\u21910.4 \n\u21911.1 \u21911.3 \u21912.0 \u21913.2 \u21913.5 \u21915.8 \u21918.8 \n\nWe compile the results in Tab. 4. Our method ranks second for all metrics, \nfalling between two VOS-oriented methods, +0.6% over LWL [5] and \u22121.5% \nbehind STA [60]. Our tracker outperforms other trackers by a large margin. \nTrackingNet [37] We compare our approach on the test set of the Track-\ningNet dataset, consisting of 511 sequences. Tab. 5 shows the results obtained \nfrom the online evaluation server. Our method outperforms most of the existing \napproaches and ranks second in terms of AUC, close behind STARK-ST101 [55] \nwhich is based on a ResNet-101 backbone. Note that we outperform STARK-\nST50 [55] that uses a ResNet-50 as backbone. Also, we achieve a higher preci-\nsion score than other methods that produce a segmentation mask output such \nas LWL [5], STA [60], Alpha-Refine [54] and D3S [31]. \n\n\nTable 6 .\n6Comparison with state-of-the-art on the UAV123[36] and NFS[19]  datasets in terms of AUC score.UAV123[36] The UAV dataset consists of 123 test videos that contain small objects, target occlusion, and distractors. Small objects are particularly challenging in a segmentation setup. Tab. 6 shows the achieved results in terms of success AUC. Our method achieves competitive results on UAV123, close to TrDiMP[49]  or SuperDiMP[11]. It outperforms LWL[5] by a large margin. Finally, we evaluate our method on the VOT2020 short-term challenge. It consists of 60 videos and provides segmentation mask annotations. For the challenge, the multi-start protocol is used and the tracking performance is assessed based on accuracy and robustness. We compare with the top methods on the leader board and include more recent methods in Tab. 7. In this setup, our method ranks 2 nd in Robustness, thus outperforming most of the other methods. In particular, we achieve a higher EAO score than STARK [ToMP ToMP Keep \nSTARK \nSTARK Super Pr STM Siam Siam \nRTS 101 \n50 Track CRACT ST101 TrDiMP TransT ST50 DiMP DiMP Track AttN R-CNN KYS DiMP LWL \n[34] \n[34] \n[35] \n[17] \n[55] \n[49] \n[8] \n[55] \n[11] \n[14] \n[18] [57] \n[47] \n[4] \n[3] \n[5] \n\nUAV123 67.6 66.9 69.0 69.7 \n66.4 \n68.2 \n67.5 \n69.1 \n69.1 \n67.7 68.0 64.7 65.0 \n64.9 \n-\n65.3 59.7 \nNFS \n65.4 66.7 66.9 66.4 \n62.5 \n66.2 \n66.2 \n65.7 \n65.2 \n64.8 63.5 \n-\n-\n63.9 63.5 62.0 61.5 \n\nTable 7. Results on the VOT2020-ST [27] challenge in terms of Expected Average \nOverlap (EAO), Accuracy and Robustness. \n\nSTARK STARK-\nST-50 ST-101-\nOcean Fast Alpha \nRTS +AR \n+AR \nLWL \nSTA \nPlus Ocean Refine RPT AFOD D3S \nSTM \n[55] \n[55] \n[27] \n[60] \n[27] \n[27] \n[27] \n[27] \n[27] \n[27] \n[27] \n\nRobustness \n0.845 0.817 \n0.789 \n0.798 0.824 0.842 0.803 0.777 0.869 0.795 0.769 0.574 \nAccuracy \n0.710 0.759 \n0.763 \n0.719 0.732 0.685 0.693 0.754 0.700 0.713 0.699 0.751 \nEAO \n0.506 0.505 \n0.497 \n0.463 0.510 0.491 0.461 0.482 0.530 0.472 0.439 0.308 \n\n\u2206 EAO to Ours \n-\n\u21910.001 \u21910.009 \u21910.043 \u21930.004 \u21910.015 \u21910.045 \u21910.024 \u21930.024 \u21910.034 \u21910.067 \u21910.198 \n\nNFS [19] The NFS dataset (30FPS version) contains 100 test videos with fast \nmotions and challenging sequences with distractors. Our method achieves an \nAUC score that is only 1% below the current best method KeepTrack [35] while \noutperforming numerous other trackers, including STARK-ST50 [55] (+0.2) Su-\nperDiMP [3] (+0.6) and PrDiMP [14] (+1.9). \nVOT 2020 [27] 55], LWL [5], \nAlphaRefine [54] and D3S [31]. \nYouTube-VOS 2019 [53] \n\nTable 8 .\n8Results on the Youtube-VOS 2019[53]  and DAVIS 2017[41] datasets. The table is split in two parts to separate methods using bounding box initialization or segmentation masks initialization, in order to enable a fair comparison. Similarly, we compare our method on the validation set of DAVIS 2017[41], which contains 30 sequences. We do not fine tune the model for this benchmark. The results are shown in Tab. 8 and confirm the observation made above that RTS is able to generate accurate segmentation masks. Our method is competitive in the mask-initialization setup.Method \nYouTube-VOS 2019 [53] \nDAVIS 2017 [41] \nG \nJseen \nJunseen \nFseen \nFunseen \nJ &F \nJ \nF \n\nRTS \n79.7 \n77.9 \n75.4 \n82.0 \n83.3 \n80.2 \n77.9 \n82.6 \nLWL [5] \n81.0 \n79.6 \n76.4 \n83.8 \n84.2 \n81.6 \n79.1 \n84.1 \nSTA [60] \n80.6 \n-\n-\n-\n-\n-\n-\n-\nSTM [38] \n79.2 \n79.6 \n73.0 \n83.6 \n80.6 \n81.8 \n79.2 \n84.3 \n\nRTS (Box) \n70.8 \n71.1 \n65.2 \n74.0 \n72.8 \n72.6 \n69.4 \n75.8 \nLWL (Box) [5] \n-\n-\n-\n-\n-\n70.6 \n67.9 \n73.3 \nSiam-RCNN [47] \n67.3 \n68.1 \n61.5 \n70.8 \n68.8 \n70.6 \n66.1 \n75.0 \nD3S [51] \n-\n-\n-\n-\n-\n60.8 \n57.8 \n63.8 \nSiamMask [31] \n52.8 \n60.2 \n45.1 \n58.2 \n47.7 \n56.4 \n54.3 \n58.5 \n\nour method can generate accurate segmentation masks. When using bounding \nboxes to predict both the initialization and segmentation masks, we outperform \nall other methods by a large margin. This confirms that even with our bounding-\nbox initialization strategy, RTS produces accurate segmentation masks. \nDAVIS 2017 [41] In the box-initialization \nsetup however, our approach outperforms all other methods in J &F, in particu-\nlar the segmentation trackers like SiamMask [51] (+16.2) and D3S [31] (+11.8). \n\n\n\nTable A1 .\nA1Ablation on the classification vs. segmentation loss weighting on different datasets in terms of AUC (area-under-the-curve) and AO (average overlap) second on the parameters that might make a difference specifically for VOS benchmarks like Youtube-VOS [53].LaSOT [16] \nGOT-10k [24] \nTrackingNet [37] \nNFS [19] \nUAV123 [36] \n\u03b7 \nAUC \nAO \nAUC \nAUC \nAUC \n\n0.0 \n67.7 \n84.0 \n81.2 \n63.7 \n64.7 \n0.4 \n69.8 \n84.0 \n81.4 \n66.2 \n67.4 \n10 \n69.7 \n85.2 \n81.6 \n65.4 \n67.6 \n\nTable A2. Results on the Youtube-VOS 2019 [53] and DAVIS 2017 [41] datasets with \na fined tuned model and inference parameters refered as RTS (YT-FT). \n\nYouTube-VOS 2019 [53] \nDAVIS 2017 [41] \nMethod \nG \nJseen \nJunseen \nFseen \nFunseen \nJ &F \nJ \nF \n\nRTS \n79.7 \n77.9 \n75.4 \n82.0 \n83.3 \n80.2 \n77.9 \n82.6 \nRTS (YT-FT) \n80.3 \n78.8 \n76.2 \n82.9 \n83.5 \n80.3 \n77.7 \n82.9 \nLWL [5] \n81.0 \n79.6 \n76.4 \n83.8 \n84.2 \n81.6 \n79.1 \n84.1 \nSTA [60] \n80.6 \n-\n-\n-\n-\n-\n-\n-\nSTM [38] \n79.2 \n79.6 \n73.0 \n83.6 \n80.6 \n81.8 \n79.2 \n84.3 \n\n\n\n\nFig. A3. Success plots on the UAV123[36] (left) and NFS [19] (right) datasets in terms of overall AUC score, reported in the legend.0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n\nOverlap threshold \n\n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n100 \n\nOverlap Precision [%] \n\nSuccess plot \n\nKeepTrack [69.7] \nTransT [69.1] \nToMP-50 [69.0] \nPrDiMP50 [68.0] \nSuper DiMP [67.7] \n\nRTS [67.6] \n\nTrDiMP [67.5] \nToMP-101 [66.9] \nSTMTrack [65.7] \nDiMP50 [65.3] \nSiamRPN++ [65.2] \nATOM [64.2] \nDaSiamRPN [57.7] \nUPDT [54.5] \nECO [53.2] \nCCOT [51.3] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n\nOverlap threshold \n\n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n100 \n\nOverlap Precision [%] \n\nSuccess plot \n\nToMP-50 [66.9] \nToMP-101 [66.7] \nKeepTrack [66.4] \nTrDiMP [66.2] \n\nRTS [65.4] \n\nTransT [65.3] \nSuper DiMP [64.8] \nPrDiMP50 [63.5] \nDiMP50 [61.9] \nATOM [58.4] \nUPDT [53.6] \nCCOT [48.8] \nECO [46.6] \n\n\n\nTable A3 .\nA3LaSOT[16] attribute-based analysis. Each column corresponds to the results computed on all sequences in the dataset with the corresponding attribute. Our method outperforms all others in 12 out of 14 attributes.Illumination Partial Deformation \nMotion Camera Rotation \nBackground Viewpoint Scale \nFull \nFast Out-of-View \nLow \nAspect \nTotal \nVariation Occlusion \nBlur Motion \nClutter \nChange Variation Occlusion Motion \nResolution Ratio Change \n\nLTMU [10] \n56.5 \n54.0 \n57.2 \n55.8 \n61.6 \n55.1 \n49.9 \n56.7 \n57.1 \n49.9 \n44.0 \n52.7 \n51.4 \n55.1 \n57.2 \nLWL [5] \n65.3 \n56.4 \n61.6 \n59.1 \n64.7 \n57.4 \n53.1 \n58.1 \n59.3 \n48.7 \n46.5 \n51.5 \n48.7 \n57.9 \n59.7 \nPrDiMP50 [14] \n63.7 \n56.9 \n60.8 \n57.9 \n64.2 \n58.1 \n54.3 \n59.2 \n59.4 \n51.3 \n48.4 \n55.3 \n53.5 \n58.6 \n59.8 \nSTMTrack [18] \n65.2 \n57.1 \n64.0 \n55.3 \n63.3 \n60.1 \n54.1 \n58.2 \n60.6 \n47.8 \n42.4 \n51.9 \n50.3 \n58.8 \n60.6 \nSuperDiMP [3] \n67.8 \n59.7 \n63.4 \n62.0 \n68.0 \n61.4 \n57.3 \n63.4 \n62.9 \n54.1 \n50.7 \n59.0 \n56.4 \n61.6 \n63.1 \nTrDiMP [49] \n67.5 \n61.1 \n64.4 \n62.4 \n68.1 \n62.4 \n58.9 \n62.8 \n63.4 \n56.4 \n53.0 \n60.7 \n58.1 \n62.3 \n63.9 \nSiam R-CNN [47] \n64.6 \n62.2 \n65.2 \n63.1 \n68.2 \n64.1 \n54.2 \n65.3 \n64.5 \n55.3 \n51.5 \n62.2 \n57.1 \n63.4 \n64.8 \nTransT [8] \n65.2 \n62.0 \n67.0 \n63.0 \n67.2 \n64.3 \n57.9 \n61.7 \n64.6 \n55.3 \n51.0 \n58.2 \n56.4 \n63.2 \n64.9 \nAlphaRefine [54] \n69.4 \n62.3 \n66.3 \n65.2 \n70.0 \n63.9 \n58.8 \n63.1 \n65.4 \n57.4 \n53.6 \n61.1 \n58.6 \n64.1 \n65.3 \nKeepTrack Fast [35] \n70.1 \n63.8 \n66.2 \n65.0 \n70.7 \n65.1 \n60.1 \n67.6 \n66.6 \n59.2 \n57.1 \n63.4 \n62.0 \n65.6 \n66.8 \nKeepTrack [35] \n69.7 \n64.1 \n67.0 \n66.7 \n71.0 \n65.3 \n61.2 \n66.9 \n66.8 \n60.1 \n57.7 \n64.1 \n62.0 \n65.9 \n67.1 \nSTARK-ST101 [55] \n67.5 \n65.1 \n68.3 \n64.5 \n69.5 \n66.6 \n57.4 \n68.8 \n66.8 \n58.9 \n54.2 \n63.3 \n59.6 \n65.6 \n67.1 \nToMP-50 [34] \n66.8 \n64.9 \n68.5 \n64.6 \n70.2 \n67.3 \n59.1 \n67.2 \n67.5 \n59.3 \n56.1 \n63.7 \n61.1 \n66.5 \n67.6 \nToMP-101 [34] \n69.0 \n65.3 \n69.4 \n65.2 \n71.7 \n67.8 \n61.5 \n69.2 \n68.4 \n59.1 \n57.9 \n64.1 \n62.5 \n67.2 \n68.5 \nRTS \n68.7 \n66.9 \n71.6 \n67.7 \n74.4 \n67.9 \n61.4 \n69.7 \n69.3 \n60.5 \n53.8 \n66.3 \n62.7 \n68.2 \n69.7 \n\n\n\n\nby 2.3% and fall just behind ToMP-101 [34] (-0.1%).Fig. A4. Attributes comparison on LaSOT [16].Illumination \nVariation \n\nPartial \nOcclusion \n\nDeformation \n\nMotion \nBlur \nCamera \nMotion \nRotation \n\nBackground \nClutter \n\nViewpoint \nChange \n\nScale \nVariation \n\nFull \nOcclusion Fast \nMotion Out-of-View \n\nLow \nResolution \n\nAspect \nRatio \nChange \n\n50 \n\n60 \n\n70 \n\nRTS \nKeepTrack \nSTARK-ST101 \n\n\nAppendixIn this Appendix, we provide further details on various aspects of our tracking pipeline. First, we provide additional architectural and inference details in Sections A and B. Second, we provide additional ablation studies, in particular on the loss weighting parameter \u03b7 on different benchmarks to show the importance of the auxiliary instance localization loss in Section C. Then, we provide success plots for different VOT benchmarks as well as a detailed analysis of our results on LaSOT[16]by comparing our approach against the other state-of-the-art methods for all the dataset attributes in Section D. Finally, we provide some additional visual comparison to other trackers in Section E.A Additional Architecture detailsClassification Scores Encoder H \u03b8 First, we describe inFigure A1the architecture of the Classification Scores Encoder H \u03b8 . It takes as input the H \u00d7 Wdimensional scores predicted by the Instance Localization (Classification) branch and outputs a 16 channels deep representation of those scores. The score encoder consists of a convolutional layer followed by a max-pool layer with stride one and two residual blocks. The output of the residual blocks has 64 channels. Thus, the final convolutional layer reduces the number of channels of the output to 16 to match the encoded scores with the mask encoding. All the convolutional layers use (3 \u00d7 3) kernels with a stride of one to preserve the spatial size of the input classification scores. Segmentation Decoder D \u03b8 The segmentation decoder has the same structure has in LWL[5]. Together with the backbone, it shows a U-Net structure and mainly consists of four decoder blocks. It takes as input the extracted ResNet-50 backbone features and the combined encoding x f from both the instance localization branch (H \u03b8 (s c )) and the segmentation branch (x m ), with x f = x m + H \u03b8 (s c ). Since the encoded instance localization scores have a lower spatial resolution than the mask encoding x m , we upscale the encoded instance localization scores using a bilinear interpolation before adding it with the mask encoding x m . We refer the reader to[5]for more details about the decoder structure. Segmentation Branch We use the same architectures for the feature extractor F \u03b8 , the label encoder E \u03b8 , the weight predictor W \u03b8 , the few-shot learner A \u03b8 and the segmentation model T \u03c4 as proposed in LWL[5]. Hence, we refer the reader to[5]for more details. Instance Localization Branch We use the same architectures for the feature extractor G \u03b8 , the model predictor P \u03b8 and the instance model T \u03ba as proposed in DiMP[3]. Hence, we refer the reader to[3]for more details.B Additional Inference detailsSearch region selection The backbone does not extract features on the full image. Instead, we sample a smaller image patch for extraction, which is centered\nThe lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks. M Berman, A R Triki, M B Blaschko, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR. the IEEE Conference on Computer Vision and Pattern Recognition (CVPRBerman, M., Triki, A.R., Blaschko, M.B.: The lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2018) 8\n\nFullyconvolutional siamese networks for object tracking. L Bertinetto, J Valmadre, J F Henriques, A Vedaldi, P H Torr, Proceedings of the European Conference on Computer Vision Workshops. the European Conference on Computer Vision WorkshopsECCVW34Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr, P.H.: Fully- convolutional siamese networks for object tracking. In: Proceedings of the Eu- ropean Conference on Computer Vision Workshops (ECCVW) (October 2016) 3, 4\n\nLearning discriminative model prediction for tracking. G Bhat, M Danelljan, L V Gool, R Timofte, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)1923Bhat, G., Danelljan, M., Gool, L.V., Timofte, R.: Learning discriminative model prediction for tracking. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (October 2019) 3, 5, 7, 8, 11, 12, 13, 19, 23\n\nKnow your surroundings: Exploiting scene information for object tracking. G Bhat, M Danelljan, L Van Gool, R Timofte, Proceedings of the European Conference on Computer Vision (ECCV. the European Conference on Computer Vision (ECCV13Bhat, G., Danelljan, M., Van Gool, L., Timofte, R.: Know your surroundings: Exploiting scene information for object tracking. In: Proceedings of the European Conference on Computer Vision (ECCV) (August 2020) 13\n\nLearning what to learn for video object segmentation. G Bhat, F J Lawin, M Danelljan, A Robinson, M Felsberg, L V Gool, R Timofte, European Conference on Computer Vision ECCV. 2223Bhat, G., Lawin, F.J., Danelljan, M., Robinson, A., Felsberg, M., Gool, L.V., Timofte, R.: Learning what to learn for video object segmentation. In: European Conference on Computer Vision ECCV (2020) 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 19, 21, 22, 23\n\nVisual object tracking using adaptive correlation filters. D S Bolme, J R Beveridge, B A Draper, Y M Lui, CVPR37Bolme, D.S., Beveridge, J.R., Draper, B.A., Lui, Y.M.: Visual object tracking using adaptive correlation filters. In: CVPR (2010) 3, 7\n\nOne-shot video object segmentation. S Caelles, K K Maninis, J Pont-Tuset, L Leal-Taix\u00e9, D Cremers, L Van Gool, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition4Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taix\u00e9, L., Cremers, D., Van Gool, L.: One-shot video object segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 221-230 (2017) 4\n\nTransformer tracking. X Chen, B Yan, J Zhu, D Wang, X Yang, H Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)423Chen, X., Yan, B., Zhu, J., Wang, D., Yang, X., Lu, H.: Transformer tracking. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2021) 4, 11, 12, 13, 23\n\nBlazingly fast video object segmentation with pixel-wise metric learning. Y Chen, J Pont-Tuset, A Montes, L Van Gool, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition4Chen, Y., Pont-Tuset, J., Montes, A., Van Gool, L.: Blazingly fast video object seg- mentation with pixel-wise metric learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1189-1198 (2018) 4\n\nHigh-performance longterm tracking with meta-updater. K Dai, Y Zhang, D Wang, J Li, H Lu, X Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)1123Dai, K., Zhang, Y., Wang, D., Li, J., Lu, H., Yang, X.: High-performance long- term tracking with meta-updater. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020) 11, 23\n\nPyTracking: Visual tracking library based on PyTorch. M Danelljan, G Bhat, 1013Danelljan, M., Bhat, G.: PyTracking: Visual tracking library based on PyTorch. https://github.com/visionml/pytracking (2019) 10, 11, 12, 13\n\nATOM: Accurate tracking by overlap maximization. M Danelljan, G Bhat, F S Khan, M Felsberg, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)34Danelljan, M., Bhat, G., Khan, F.S., Felsberg, M.: ATOM: Accurate tracking by overlap maximization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019) 3, 4\n\nECO: efficient convolution operators for tracking. M Danelljan, G Bhat, F Shahbaz Khan, M Felsberg, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR. the IEEE Conference on Computer Vision and Pattern Recognition (CVPR7Danelljan, M., Bhat, G., Shahbaz Khan, F., Felsberg, M.: ECO: efficient convolu- tion operators for tracking. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2017) 7\n\nProbabilistic regression for visual tracking. M Danelljan, L V Gool, R Timofte, CVPR (2020) 3, 4. 1223Danelljan, M., Gool, L.V., Timofte, R.: Probabilistic regression for visual tracking. In: CVPR (2020) 3, 4, 11, 12, 13, 23\n\nBeyond correlation filters: Learning continuous convolution operators for visual tracking. M Danelljan, A Robinson, F Shahbaz Khan, M Felsberg, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)3Danelljan, M., Robinson, A., Shahbaz Khan, F., Felsberg, M.: Beyond correlation filters: Learning continuous convolution operators for visual tracking. In: Proceed- ings of the European Conference on Computer Vision (ECCV) (October 2016) 3\n\nLasot: A high-quality benchmark for large-scale single object tracking. H Fan, L Lin, F Yang, P Chu, G Deng, S Yu, H Bai, Y Xu, C Liao, H Ling, Transactions on Pattern Analysis and Machine Intelligence. 4164Fan, H., Lin, L., Yang, F., Chu, P., Deng, G., Yu, S., Bai, H., Xu, Y., Liao, C., Ling, H.: Lasot: A high-quality benchmark for large-scale single object tracking. Transactions on Pattern Analysis and Machine Intelligence 41(6), 1515-1530 (2018) 4\n\nTransforming model prediction for tracking. C Mayer, M Danelljan, G Bhat, M Paul, D P Paudel, F Yu, L Van Gool, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)423Mayer, C., Danelljan, M., Bhat, G., Paul, M., Paudel, D.P., Yu, F., Van Gool, L.: Transforming model prediction for tracking. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8731-8740 (June 2022) 4, 11, 12, 13, 23\n\nLearning target candidate association to keep track of what not to track. C Mayer, M Danelljan, D P Paudel, L Van Gool, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)1124Mayer, C., Danelljan, M., Paudel, D.P., Van Gool, L.: Learning target candidate association to keep track of what not to track. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 13444-13454 (October 2021) 11, 12, 13, 23, 24\n\nA benchmark and simulator for uav tracking. M Mueller, N Smith, B Ghanem, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2223Mueller, M., Smith, N., Ghanem, B.: A benchmark and simulator for uav tracking. In: Proceedings of the European Conference on Computer Vision (ECCV) (October 2016) 1, 5, 10, 11, 13, 21, 22, 23\n\nTrackingnet: A large-scale dataset and benchmark for object tracking in the wild. M M\u00fcller, A Bibi, S Giancola, S Al-Subaihi, B Ghanem, In: ECCV. 321M\u00fcller, M., Bibi, A., Giancola, S., Al-Subaihi, S., Ghanem, B.: Trackingnet: A large-scale dataset and benchmark for object tracking in the wild. In: ECCV (2018) 3, 12, 21\n\nVideo object segmentation using space-time memory networks. S W Oh, J Y Lee, N Xu, S J Kim, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)1421Oh, S.W., Lee, J.Y., Xu, N., Kim, S.J.: Video object segmentation using space-time memory networks. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (October 2019) 2, 4, 13, 14, 21\n\nA benchmark dataset and evaluation methodology for video object segmentation. F Perazzi, J Pont-Tuset, B Mcwilliams, L Van Gool, M Gross, A Sorkine-Hornung, Computer Vision and Pattern Recognition. 9Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine- Hornung, A.: A benchmark dataset and evaluation methodology for video object segmentation. In: Computer Vision and Pattern Recognition (2016) 9\n\nLearning video object segmentation from static images. F Perazzi, A Khoreva, R Benenson, B Schiele, A Sorkine-Hornung, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition4Perazzi, F., Khoreva, A., Benenson, R., Schiele, B., Sorkine-Hornung, A.: Learn- ing video object segmentation from static images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2663-2672 (2017) 4\n\nJ Pont-Tuset, F Perazzi, S Caelles, P Arbel\u00e1ez, A Sorkine-Hornung, L Van Gool, arXiv:1704.00675The 2017 davis challenge on video object segmentation. 2122Pont-Tuset, J., Perazzi, F., Caelles, S., Arbel\u00e1ez, P., Sorkine-Hornung, A., Van Gool, L.: The 2017 davis challenge on video object segmentation. arXiv:1704.00675 (2017) 2, 4, 8, 14, 21, 22\n\nTracking-by-segmentation with online gradient boosting decision tree. J Son, I Jung, K Park, B Han, Proceedings of the IEEE International Conference on Computer Vision (ICCV. the IEEE International Conference on Computer Vision (ICCV4Son, J., Jung, I., Park, K., Han, B.: Tracking-by-segmentation with online gradient boosting decision tree. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV) (December 2015) 4\n\nSiamese instance search for tracking. R Tao, E Gavves, A W M Smeulders, CVPR3Tao, R., Gavves, E., Smeulders, A.W.M.: Siamese instance search for tracking. In: CVPR (2016) 3\n\nEnd-to-end representation learning for correlation filter based tracking. J Valmadre, L Bertinetto, J Henriques, A Vedaldi, P H S Torr, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)34Valmadre, J., Bertinetto, L., Henriques, J., Vedaldi, A., Torr, P.H.S.: End-to-end representation learning for correlation filter based tracking. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (July 2017) 3, 4\n\nFeelvos: Fast end-to-end embedding learning for video object segmentation. P Voigtlaender, Y Chai, F Schroff, H Adam, B Leibe, L C Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition4Voigtlaender, P., Chai, Y., Schroff, F., Adam, H., Leibe, B., Chen, L.C.: Feelvos: Fast end-to-end embedding learning for video object segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 9481- 9490 (2019) 4\n\nOnline adaptation of convolutional neural networks for video object segmentation. P Voigtlaender, B Leibe, BMVC4Voigtlaender, P., Leibe, B.: Online adaptation of convolutional neural networks for video object segmentation. In: BMVC (2017) 4\n\nSiam R-CNN: Visual tracking by re-detection. P Voigtlaender, J Luiten, P H Torr, B Leibe, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1423Voigtlaender, P., Luiten, J., Torr, P.H., Leibe, B.: Siam R-CNN: Visual track- ing by re-detection. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020) 2, 4, 11, 12, 13, 14, 23\n\nTracking by instance detection: A meta-learning approach. G Wang, C Luo, X Sun, Z Xiong, W Zeng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR3Wang, G., Luo, C., Sun, X., Xiong, Z., Zeng, W.: Tracking by instance detec- tion: A meta-learning approach. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020) 3\n", "annotations": {"author": "[{\"end\":125,\"start\":42},{\"end\":187,\"start\":126},{\"end\":274,\"start\":188},{\"end\":332,\"start\":275}]", "publisher": null, "author_last_name": "[{\"end\":55,\"start\":51},{\"end\":142,\"start\":133},{\"end\":203,\"start\":198},{\"end\":287,\"start\":283}]", "author_first_name": "[{\"end\":50,\"start\":42},{\"end\":132,\"start\":126},{\"end\":197,\"start\":188},{\"end\":278,\"start\":275},{\"end\":282,\"start\":279}]", "author_affiliation": "[{\"end\":124,\"start\":82},{\"end\":186,\"start\":144},{\"end\":273,\"start\":231},{\"end\":331,\"start\":289}]", "title": "[{\"end\":39,\"start\":1},{\"end\":371,\"start\":333}]", "venue": null, "abstract": "[{\"end\":1830,\"start\":373}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2951,\"start\":2947},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2954,\"start\":2951},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2980,\"start\":2977},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3050,\"start\":3046},{\"end\":3530,\"start\":3526},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4130,\"start\":4126},{\"end\":4133,\"start\":4130},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4614,\"start\":4610},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4616,\"start\":4614},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4800,\"start\":4796},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4892,\"start\":4888},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5147,\"start\":5144},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5536,\"start\":5533},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7564,\"start\":7560},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7588,\"start\":7584},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7824,\"start\":7821},{\"end\":7827,\"start\":7824},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7830,\"start\":7827},{\"end\":7833,\"start\":7830},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7836,\"start\":7833},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7839,\"start\":7836},{\"end\":7842,\"start\":7839},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7844,\"start\":7842},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7847,\"start\":7844},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8131,\"start\":8128},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8134,\"start\":8131},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8361,\"start\":8357},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8363,\"start\":8361},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8366,\"start\":8363},{\"end\":8369,\"start\":8366},{\"end\":8372,\"start\":8369},{\"end\":8375,\"start\":8372},{\"end\":8378,\"start\":8375},{\"end\":8381,\"start\":8378},{\"end\":8384,\"start\":8381},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8780,\"start\":8776},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8782,\"start\":8780},{\"end\":8785,\"start\":8782},{\"end\":8788,\"start\":8785},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9172,\"start\":9169},{\"end\":9175,\"start\":9172},{\"end\":9178,\"start\":9175},{\"end\":9181,\"start\":9178},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9184,\"start\":9181},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9673,\"start\":9669},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9777,\"start\":9774},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9780,\"start\":9777},{\"end\":9783,\"start\":9780},{\"end\":9855,\"start\":9851},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9858,\"start\":9855},{\"end\":9861,\"start\":9858},{\"end\":9864,\"start\":9861},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10014,\"start\":10011},{\"end\":10017,\"start\":10014},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10020,\"start\":10017},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10023,\"start\":10020},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10050,\"start\":10047},{\"end\":10539,\"start\":10535},{\"end\":10542,\"start\":10539},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10545,\"start\":10542},{\"end\":10548,\"start\":10545},{\"end\":10551,\"start\":10548},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10554,\"start\":10551},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10827,\"start\":10823},{\"end\":10846,\"start\":10842},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11649,\"start\":11645},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11782,\"start\":11779},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12043,\"start\":12040},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12729,\"start\":12725},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12732,\"start\":12729},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13055,\"start\":13052},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14629,\"start\":14626},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15820,\"start\":15817},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16110,\"start\":16107},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16976,\"start\":16973},{\"end\":16979,\"start\":16976},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16982,\"start\":16979},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16984,\"start\":16982},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17787,\"start\":17784},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17895,\"start\":17892},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20765,\"start\":20762},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21643,\"start\":21640},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24737,\"start\":24733},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24927,\"start\":24923},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25200,\"start\":25197},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25317,\"start\":25314},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27512,\"start\":27508},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28007,\"start\":28003},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28025,\"start\":28021},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28830,\"start\":28827},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28906,\"start\":28902},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29643,\"start\":29639},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32913,\"start\":32909},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33117,\"start\":33113},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33710,\"start\":33706},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33870,\"start\":33867},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34224,\"start\":34220},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34254,\"start\":34250},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34280,\"start\":34276},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34359,\"start\":34355},{\"end\":34485,\"start\":34481},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34501,\"start\":34497},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34548,\"start\":34544},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34879,\"start\":34875},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34928,\"start\":34924},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35174,\"start\":35170},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35192,\"start\":35188},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35481,\"start\":35477},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35495,\"start\":35491},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35630,\"start\":35626},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35777,\"start\":35773},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35894,\"start\":35890},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35975,\"start\":35971},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36402,\"start\":36398},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":43395,\"start\":43391},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":43428,\"start\":43424},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":43688,\"start\":43684},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":43865,\"start\":43862},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":44824,\"start\":44820},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":46396,\"start\":46392},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":48211,\"start\":48207},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":48266,\"start\":48262},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":48589,\"start\":48585},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":48612,\"start\":48609},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":50719,\"start\":50715},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":50964,\"start\":50960},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":53329,\"start\":53325},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":54126,\"start\":54122}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36243,\"start\":36188},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36335,\"start\":36244},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36616,\"start\":36336},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42992,\"start\":36617},{\"attributes\":{\"id\":\"fig_6\"},\"end\":43341,\"start\":42993},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44008,\"start\":43342},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44462,\"start\":44009},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45977,\"start\":44463},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":46326,\"start\":45978},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":48148,\"start\":46327},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":50651,\"start\":48149},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":52305,\"start\":50652},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":53286,\"start\":52306},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":54102,\"start\":53287},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":56127,\"start\":54103},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":56519,\"start\":56128}]", "paragraph": "[{\"end\":3209,\"start\":1846},{\"end\":4094,\"start\":3211},{\"end\":4550,\"start\":4096},{\"end\":5300,\"start\":4552},{\"end\":7371,\"start\":5302},{\"end\":7749,\"start\":7413},{\"end\":8245,\"start\":7751},{\"end\":8789,\"start\":8247},{\"end\":9726,\"start\":8791},{\"end\":10868,\"start\":9728},{\"end\":11376,\"start\":10870},{\"end\":12460,\"start\":11378},{\"end\":14538,\"start\":12482},{\"end\":15316,\"start\":14562},{\"end\":16309,\"start\":15376},{\"end\":17171,\"start\":16342},{\"end\":17568,\"start\":17173},{\"end\":18149,\"start\":17629},{\"end\":18946,\"start\":18195},{\"end\":19175,\"start\":18971},{\"end\":19603,\"start\":19235},{\"end\":20593,\"start\":19670},{\"end\":20683,\"start\":20678},{\"end\":21521,\"start\":20722},{\"end\":22083,\"start\":21601},{\"end\":24670,\"start\":22155},{\"end\":25009,\"start\":24685},{\"end\":26291,\"start\":25035},{\"end\":26650,\"start\":26316},{\"end\":27221,\"start\":26652},{\"end\":27500,\"start\":27260},{\"end\":28934,\"start\":27502},{\"end\":29782,\"start\":28949},{\"end\":30486,\"start\":29784},{\"end\":30824,\"start\":30488},{\"end\":31200,\"start\":30826},{\"end\":33211,\"start\":31227},{\"end\":33635,\"start\":33239},{\"end\":34065,\"start\":33637},{\"end\":36187,\"start\":34101}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15375,\"start\":15317},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17628,\"start\":17569},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18970,\"start\":18947},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19669,\"start\":19604},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20677,\"start\":20594},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20721,\"start\":20684},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21600,\"start\":21522},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22142,\"start\":22084},{\"attributes\":{\"id\":\"formula_8\"},\"end\":33238,\"start\":33212}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31691,\"start\":31683},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33674,\"start\":33666},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":34708,\"start\":34700}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1844,\"start\":1832},{\"attributes\":{\"n\":\"2\"},\"end\":7386,\"start\":7374},{\"end\":7411,\"start\":7389},{\"attributes\":{\"n\":\"3\"},\"end\":12469,\"start\":12463},{\"attributes\":{\"n\":\"3.1\"},\"end\":12480,\"start\":12472},{\"attributes\":{\"n\":\"3.2\"},\"end\":14560,\"start\":14541},{\"attributes\":{\"n\":\"3.3\"},\"end\":16340,\"start\":16312},{\"attributes\":{\"n\":\"3.4\"},\"end\":18193,\"start\":18152},{\"attributes\":{\"n\":\"3.5\"},\"end\":19233,\"start\":19178},{\"attributes\":{\"n\":\"3.6\"},\"end\":22153,\"start\":22144},{\"attributes\":{\"n\":\"4\"},\"end\":24683,\"start\":24673},{\"attributes\":{\"n\":\"4.1\"},\"end\":25033,\"start\":25012},{\"attributes\":{\"n\":\"4.2\"},\"end\":26314,\"start\":26294},{\"attributes\":{\"n\":\"4.3\"},\"end\":27258,\"start\":27224},{\"attributes\":{\"n\":\"5\"},\"end\":28947,\"start\":28937},{\"end\":31225,\"start\":31203},{\"end\":34099,\"start\":34068},{\"end\":36197,\"start\":36189},{\"end\":36253,\"start\":36245},{\"end\":36345,\"start\":36337},{\"end\":36627,\"start\":36618},{\"end\":43003,\"start\":42994},{\"end\":44019,\"start\":44010},{\"end\":44473,\"start\":44464},{\"end\":45988,\"start\":45979},{\"end\":46337,\"start\":46328},{\"end\":48159,\"start\":48150},{\"end\":50662,\"start\":50653},{\"end\":52317,\"start\":52307},{\"end\":54114,\"start\":54104}]", "table": "[{\"end\":44462,\"start\":44161},{\"end\":45977,\"start\":44894},{\"end\":46326,\"start\":45997},{\"end\":48148,\"start\":46697},{\"end\":50651,\"start\":49147},{\"end\":52305,\"start\":51233},{\"end\":53286,\"start\":52577},{\"end\":54102,\"start\":53421},{\"end\":56127,\"start\":54328},{\"end\":56519,\"start\":56226}]", "figure_caption": "[{\"end\":36243,\"start\":36199},{\"end\":36335,\"start\":36255},{\"end\":36616,\"start\":36347},{\"end\":42992,\"start\":36630},{\"end\":43341,\"start\":43006},{\"end\":44008,\"start\":43344},{\"end\":44161,\"start\":44021},{\"end\":44894,\"start\":44475},{\"end\":45997,\"start\":45990},{\"end\":46697,\"start\":46339},{\"end\":49147,\"start\":48161},{\"end\":51233,\"start\":50664},{\"end\":52577,\"start\":52320},{\"end\":53421,\"start\":53289},{\"end\":54328,\"start\":54117},{\"end\":56226,\"start\":56130}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2662,\"start\":2656},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3461,\"start\":3455},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5081,\"start\":5075},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13422,\"start\":13416},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16095,\"start\":16089},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18447,\"start\":18441},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27723,\"start\":27717},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33052,\"start\":33045},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34304,\"start\":34295},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34514,\"start\":34505},{\"end\":34777,\"start\":34768},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35662,\"start\":35653}]", "bib_author_first_name": "[{\"end\":59496,\"start\":59495},{\"end\":59506,\"start\":59505},{\"end\":59508,\"start\":59507},{\"end\":59517,\"start\":59516},{\"end\":59519,\"start\":59518},{\"end\":60013,\"start\":60012},{\"end\":60027,\"start\":60026},{\"end\":60039,\"start\":60038},{\"end\":60041,\"start\":60040},{\"end\":60054,\"start\":60053},{\"end\":60065,\"start\":60064},{\"end\":60067,\"start\":60066},{\"end\":60492,\"start\":60491},{\"end\":60500,\"start\":60499},{\"end\":60513,\"start\":60512},{\"end\":60515,\"start\":60514},{\"end\":60523,\"start\":60522},{\"end\":60990,\"start\":60989},{\"end\":60998,\"start\":60997},{\"end\":61011,\"start\":61010},{\"end\":61023,\"start\":61022},{\"end\":61416,\"start\":61415},{\"end\":61424,\"start\":61423},{\"end\":61426,\"start\":61425},{\"end\":61435,\"start\":61434},{\"end\":61448,\"start\":61447},{\"end\":61460,\"start\":61459},{\"end\":61472,\"start\":61471},{\"end\":61474,\"start\":61473},{\"end\":61482,\"start\":61481},{\"end\":61855,\"start\":61854},{\"end\":61857,\"start\":61856},{\"end\":61866,\"start\":61865},{\"end\":61868,\"start\":61867},{\"end\":61881,\"start\":61880},{\"end\":61883,\"start\":61882},{\"end\":61893,\"start\":61892},{\"end\":61895,\"start\":61894},{\"end\":62080,\"start\":62079},{\"end\":62091,\"start\":62090},{\"end\":62093,\"start\":62092},{\"end\":62104,\"start\":62103},{\"end\":62118,\"start\":62117},{\"end\":62132,\"start\":62131},{\"end\":62143,\"start\":62142},{\"end\":62547,\"start\":62546},{\"end\":62555,\"start\":62554},{\"end\":62562,\"start\":62561},{\"end\":62569,\"start\":62568},{\"end\":62577,\"start\":62576},{\"end\":62585,\"start\":62584},{\"end\":63033,\"start\":63032},{\"end\":63041,\"start\":63040},{\"end\":63055,\"start\":63054},{\"end\":63065,\"start\":63064},{\"end\":63508,\"start\":63507},{\"end\":63515,\"start\":63514},{\"end\":63524,\"start\":63523},{\"end\":63532,\"start\":63531},{\"end\":63538,\"start\":63537},{\"end\":63544,\"start\":63543},{\"end\":63998,\"start\":63997},{\"end\":64011,\"start\":64010},{\"end\":64213,\"start\":64212},{\"end\":64226,\"start\":64225},{\"end\":64234,\"start\":64233},{\"end\":64236,\"start\":64235},{\"end\":64244,\"start\":64243},{\"end\":64683,\"start\":64682},{\"end\":64696,\"start\":64695},{\"end\":64704,\"start\":64703},{\"end\":64720,\"start\":64719},{\"end\":65146,\"start\":65145},{\"end\":65159,\"start\":65158},{\"end\":65161,\"start\":65160},{\"end\":65169,\"start\":65168},{\"end\":65417,\"start\":65416},{\"end\":65430,\"start\":65429},{\"end\":65442,\"start\":65441},{\"end\":65458,\"start\":65457},{\"end\":65899,\"start\":65898},{\"end\":65906,\"start\":65905},{\"end\":65913,\"start\":65912},{\"end\":65921,\"start\":65920},{\"end\":65928,\"start\":65927},{\"end\":65936,\"start\":65935},{\"end\":65942,\"start\":65941},{\"end\":65949,\"start\":65948},{\"end\":65955,\"start\":65954},{\"end\":65963,\"start\":65962},{\"end\":66327,\"start\":66326},{\"end\":66336,\"start\":66335},{\"end\":66349,\"start\":66348},{\"end\":66357,\"start\":66356},{\"end\":66365,\"start\":66364},{\"end\":66367,\"start\":66366},{\"end\":66377,\"start\":66376},{\"end\":66383,\"start\":66382},{\"end\":66900,\"start\":66899},{\"end\":66909,\"start\":66908},{\"end\":66922,\"start\":66921},{\"end\":66924,\"start\":66923},{\"end\":66934,\"start\":66933},{\"end\":67400,\"start\":67399},{\"end\":67411,\"start\":67410},{\"end\":67420,\"start\":67419},{\"end\":67825,\"start\":67824},{\"end\":67835,\"start\":67834},{\"end\":67843,\"start\":67842},{\"end\":67855,\"start\":67854},{\"end\":67869,\"start\":67868},{\"end\":68125,\"start\":68124},{\"end\":68127,\"start\":68126},{\"end\":68133,\"start\":68132},{\"end\":68135,\"start\":68134},{\"end\":68142,\"start\":68141},{\"end\":68148,\"start\":68147},{\"end\":68150,\"start\":68149},{\"end\":68598,\"start\":68597},{\"end\":68609,\"start\":68608},{\"end\":68623,\"start\":68622},{\"end\":68637,\"start\":68636},{\"end\":68649,\"start\":68648},{\"end\":68658,\"start\":68657},{\"end\":68998,\"start\":68997},{\"end\":69009,\"start\":69008},{\"end\":69020,\"start\":69019},{\"end\":69032,\"start\":69031},{\"end\":69043,\"start\":69042},{\"end\":69442,\"start\":69441},{\"end\":69456,\"start\":69455},{\"end\":69467,\"start\":69466},{\"end\":69478,\"start\":69477},{\"end\":69490,\"start\":69489},{\"end\":69509,\"start\":69508},{\"end\":69857,\"start\":69856},{\"end\":69864,\"start\":69863},{\"end\":69872,\"start\":69871},{\"end\":69880,\"start\":69879},{\"end\":70265,\"start\":70264},{\"end\":70272,\"start\":70271},{\"end\":70282,\"start\":70281},{\"end\":70286,\"start\":70283},{\"end\":70475,\"start\":70474},{\"end\":70487,\"start\":70486},{\"end\":70501,\"start\":70500},{\"end\":70514,\"start\":70513},{\"end\":70525,\"start\":70524},{\"end\":70529,\"start\":70526},{\"end\":71034,\"start\":71033},{\"end\":71050,\"start\":71049},{\"end\":71058,\"start\":71057},{\"end\":71069,\"start\":71068},{\"end\":71077,\"start\":71076},{\"end\":71086,\"start\":71085},{\"end\":71088,\"start\":71087},{\"end\":71577,\"start\":71576},{\"end\":71593,\"start\":71592},{\"end\":71782,\"start\":71781},{\"end\":71798,\"start\":71797},{\"end\":71808,\"start\":71807},{\"end\":71810,\"start\":71809},{\"end\":71818,\"start\":71817},{\"end\":72172,\"start\":72171},{\"end\":72180,\"start\":72179},{\"end\":72187,\"start\":72186},{\"end\":72194,\"start\":72193},{\"end\":72203,\"start\":72202}]", "bib_author_last_name": "[{\"end\":59503,\"start\":59497},{\"end\":59514,\"start\":59509},{\"end\":59528,\"start\":59520},{\"end\":60024,\"start\":60014},{\"end\":60036,\"start\":60028},{\"end\":60051,\"start\":60042},{\"end\":60062,\"start\":60055},{\"end\":60072,\"start\":60068},{\"end\":60497,\"start\":60493},{\"end\":60510,\"start\":60501},{\"end\":60520,\"start\":60516},{\"end\":60531,\"start\":60524},{\"end\":60995,\"start\":60991},{\"end\":61008,\"start\":60999},{\"end\":61020,\"start\":61012},{\"end\":61031,\"start\":61024},{\"end\":61421,\"start\":61417},{\"end\":61432,\"start\":61427},{\"end\":61445,\"start\":61436},{\"end\":61457,\"start\":61449},{\"end\":61469,\"start\":61461},{\"end\":61479,\"start\":61475},{\"end\":61490,\"start\":61483},{\"end\":61863,\"start\":61858},{\"end\":61878,\"start\":61869},{\"end\":61890,\"start\":61884},{\"end\":61899,\"start\":61896},{\"end\":62088,\"start\":62081},{\"end\":62101,\"start\":62094},{\"end\":62115,\"start\":62105},{\"end\":62129,\"start\":62119},{\"end\":62140,\"start\":62133},{\"end\":62152,\"start\":62144},{\"end\":62552,\"start\":62548},{\"end\":62559,\"start\":62556},{\"end\":62566,\"start\":62563},{\"end\":62574,\"start\":62570},{\"end\":62582,\"start\":62578},{\"end\":62588,\"start\":62586},{\"end\":63038,\"start\":63034},{\"end\":63052,\"start\":63042},{\"end\":63062,\"start\":63056},{\"end\":63074,\"start\":63066},{\"end\":63512,\"start\":63509},{\"end\":63521,\"start\":63516},{\"end\":63529,\"start\":63525},{\"end\":63535,\"start\":63533},{\"end\":63541,\"start\":63539},{\"end\":63549,\"start\":63545},{\"end\":64008,\"start\":63999},{\"end\":64016,\"start\":64012},{\"end\":64223,\"start\":64214},{\"end\":64231,\"start\":64227},{\"end\":64241,\"start\":64237},{\"end\":64253,\"start\":64245},{\"end\":64693,\"start\":64684},{\"end\":64701,\"start\":64697},{\"end\":64717,\"start\":64705},{\"end\":64729,\"start\":64721},{\"end\":65156,\"start\":65147},{\"end\":65166,\"start\":65162},{\"end\":65177,\"start\":65170},{\"end\":65427,\"start\":65418},{\"end\":65439,\"start\":65431},{\"end\":65455,\"start\":65443},{\"end\":65467,\"start\":65459},{\"end\":65903,\"start\":65900},{\"end\":65910,\"start\":65907},{\"end\":65918,\"start\":65914},{\"end\":65925,\"start\":65922},{\"end\":65933,\"start\":65929},{\"end\":65939,\"start\":65937},{\"end\":65946,\"start\":65943},{\"end\":65952,\"start\":65950},{\"end\":65960,\"start\":65956},{\"end\":65968,\"start\":65964},{\"end\":66333,\"start\":66328},{\"end\":66346,\"start\":66337},{\"end\":66354,\"start\":66350},{\"end\":66362,\"start\":66358},{\"end\":66374,\"start\":66368},{\"end\":66380,\"start\":66378},{\"end\":66392,\"start\":66384},{\"end\":66906,\"start\":66901},{\"end\":66919,\"start\":66910},{\"end\":66931,\"start\":66925},{\"end\":66943,\"start\":66935},{\"end\":67408,\"start\":67401},{\"end\":67417,\"start\":67412},{\"end\":67427,\"start\":67421},{\"end\":67832,\"start\":67826},{\"end\":67840,\"start\":67836},{\"end\":67852,\"start\":67844},{\"end\":67866,\"start\":67856},{\"end\":67876,\"start\":67870},{\"end\":68130,\"start\":68128},{\"end\":68139,\"start\":68136},{\"end\":68145,\"start\":68143},{\"end\":68154,\"start\":68151},{\"end\":68606,\"start\":68599},{\"end\":68620,\"start\":68610},{\"end\":68634,\"start\":68624},{\"end\":68646,\"start\":68638},{\"end\":68655,\"start\":68650},{\"end\":68674,\"start\":68659},{\"end\":69006,\"start\":68999},{\"end\":69017,\"start\":69010},{\"end\":69029,\"start\":69021},{\"end\":69040,\"start\":69033},{\"end\":69059,\"start\":69044},{\"end\":69453,\"start\":69443},{\"end\":69464,\"start\":69457},{\"end\":69475,\"start\":69468},{\"end\":69487,\"start\":69479},{\"end\":69506,\"start\":69491},{\"end\":69518,\"start\":69510},{\"end\":69861,\"start\":69858},{\"end\":69869,\"start\":69865},{\"end\":69877,\"start\":69873},{\"end\":69884,\"start\":69881},{\"end\":70269,\"start\":70266},{\"end\":70279,\"start\":70273},{\"end\":70296,\"start\":70287},{\"end\":70484,\"start\":70476},{\"end\":70498,\"start\":70488},{\"end\":70511,\"start\":70502},{\"end\":70522,\"start\":70515},{\"end\":70534,\"start\":70530},{\"end\":71047,\"start\":71035},{\"end\":71055,\"start\":71051},{\"end\":71066,\"start\":71059},{\"end\":71074,\"start\":71070},{\"end\":71083,\"start\":71078},{\"end\":71093,\"start\":71089},{\"end\":71590,\"start\":71578},{\"end\":71599,\"start\":71594},{\"end\":71795,\"start\":71783},{\"end\":71805,\"start\":71799},{\"end\":71815,\"start\":71811},{\"end\":71824,\"start\":71819},{\"end\":72177,\"start\":72173},{\"end\":72184,\"start\":72181},{\"end\":72191,\"start\":72188},{\"end\":72200,\"start\":72195},{\"end\":72208,\"start\":72204}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4716955},\"end\":59953,\"start\":59368},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14309034},\"end\":60434,\"start\":59955},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":118637813},\"end\":60913,\"start\":60436},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":214623106},\"end\":61359,\"start\":60915},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":214640989},\"end\":61793,\"start\":61361},{\"attributes\":{\"id\":\"b5\"},\"end\":62041,\"start\":61795},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5381099},\"end\":62522,\"start\":62043},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":232404168},\"end\":62956,\"start\":62524},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4710115},\"end\":63451,\"start\":62958},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":214743010},\"end\":63941,\"start\":63453},{\"attributes\":{\"id\":\"b10\"},\"end\":64161,\"start\":63943},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":53712235},\"end\":64629,\"start\":64163},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14958161},\"end\":65097,\"start\":64631},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":214693026},\"end\":65323,\"start\":65099},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5650694},\"end\":65824,\"start\":65325},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52350875},\"end\":66280,\"start\":65826},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":247593790},\"end\":66823,\"start\":66282},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":232417647},\"end\":67353,\"start\":66825},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10184155},\"end\":67740,\"start\":67355},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4455970},\"end\":68062,\"start\":67742},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":90262243},\"end\":68517,\"start\":68064},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1949934},\"end\":68940,\"start\":68519},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3155322},\"end\":69439,\"start\":68942},{\"attributes\":{\"doi\":\"arXiv:1704.00675\",\"id\":\"b23\"},\"end\":69784,\"start\":69441},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":18063910},\"end\":70224,\"start\":69786},{\"attributes\":{\"id\":\"b25\"},\"end\":70398,\"start\":70226},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10520310},\"end\":70956,\"start\":70400},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":67856723},\"end\":71492,\"start\":70958},{\"attributes\":{\"id\":\"b28\"},\"end\":71734,\"start\":71494},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":208512936},\"end\":72111,\"start\":71736},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":214774780},\"end\":72587,\"start\":72113}]", "bib_title": "[{\"end\":59493,\"start\":59368},{\"end\":60010,\"start\":59955},{\"end\":60489,\"start\":60436},{\"end\":60987,\"start\":60915},{\"end\":61413,\"start\":61361},{\"end\":62077,\"start\":62043},{\"end\":62544,\"start\":62524},{\"end\":63030,\"start\":62958},{\"end\":63505,\"start\":63453},{\"end\":64210,\"start\":64163},{\"end\":64680,\"start\":64631},{\"end\":65143,\"start\":65099},{\"end\":65414,\"start\":65325},{\"end\":65896,\"start\":65826},{\"end\":66324,\"start\":66282},{\"end\":66897,\"start\":66825},{\"end\":67397,\"start\":67355},{\"end\":67822,\"start\":67742},{\"end\":68122,\"start\":68064},{\"end\":68595,\"start\":68519},{\"end\":68995,\"start\":68942},{\"end\":69854,\"start\":69786},{\"end\":70472,\"start\":70400},{\"end\":71031,\"start\":70958},{\"end\":71779,\"start\":71736},{\"end\":72169,\"start\":72113}]", "bib_author": "[{\"end\":59505,\"start\":59495},{\"end\":59516,\"start\":59505},{\"end\":59530,\"start\":59516},{\"end\":60026,\"start\":60012},{\"end\":60038,\"start\":60026},{\"end\":60053,\"start\":60038},{\"end\":60064,\"start\":60053},{\"end\":60074,\"start\":60064},{\"end\":60499,\"start\":60491},{\"end\":60512,\"start\":60499},{\"end\":60522,\"start\":60512},{\"end\":60533,\"start\":60522},{\"end\":60997,\"start\":60989},{\"end\":61010,\"start\":60997},{\"end\":61022,\"start\":61010},{\"end\":61033,\"start\":61022},{\"end\":61423,\"start\":61415},{\"end\":61434,\"start\":61423},{\"end\":61447,\"start\":61434},{\"end\":61459,\"start\":61447},{\"end\":61471,\"start\":61459},{\"end\":61481,\"start\":61471},{\"end\":61492,\"start\":61481},{\"end\":61865,\"start\":61854},{\"end\":61880,\"start\":61865},{\"end\":61892,\"start\":61880},{\"end\":61901,\"start\":61892},{\"end\":62090,\"start\":62079},{\"end\":62103,\"start\":62090},{\"end\":62117,\"start\":62103},{\"end\":62131,\"start\":62117},{\"end\":62142,\"start\":62131},{\"end\":62154,\"start\":62142},{\"end\":62554,\"start\":62546},{\"end\":62561,\"start\":62554},{\"end\":62568,\"start\":62561},{\"end\":62576,\"start\":62568},{\"end\":62584,\"start\":62576},{\"end\":62590,\"start\":62584},{\"end\":63040,\"start\":63032},{\"end\":63054,\"start\":63040},{\"end\":63064,\"start\":63054},{\"end\":63076,\"start\":63064},{\"end\":63514,\"start\":63507},{\"end\":63523,\"start\":63514},{\"end\":63531,\"start\":63523},{\"end\":63537,\"start\":63531},{\"end\":63543,\"start\":63537},{\"end\":63551,\"start\":63543},{\"end\":64010,\"start\":63997},{\"end\":64018,\"start\":64010},{\"end\":64225,\"start\":64212},{\"end\":64233,\"start\":64225},{\"end\":64243,\"start\":64233},{\"end\":64255,\"start\":64243},{\"end\":64695,\"start\":64682},{\"end\":64703,\"start\":64695},{\"end\":64719,\"start\":64703},{\"end\":64731,\"start\":64719},{\"end\":65158,\"start\":65145},{\"end\":65168,\"start\":65158},{\"end\":65179,\"start\":65168},{\"end\":65429,\"start\":65416},{\"end\":65441,\"start\":65429},{\"end\":65457,\"start\":65441},{\"end\":65469,\"start\":65457},{\"end\":65905,\"start\":65898},{\"end\":65912,\"start\":65905},{\"end\":65920,\"start\":65912},{\"end\":65927,\"start\":65920},{\"end\":65935,\"start\":65927},{\"end\":65941,\"start\":65935},{\"end\":65948,\"start\":65941},{\"end\":65954,\"start\":65948},{\"end\":65962,\"start\":65954},{\"end\":65970,\"start\":65962},{\"end\":66335,\"start\":66326},{\"end\":66348,\"start\":66335},{\"end\":66356,\"start\":66348},{\"end\":66364,\"start\":66356},{\"end\":66376,\"start\":66364},{\"end\":66382,\"start\":66376},{\"end\":66394,\"start\":66382},{\"end\":66908,\"start\":66899},{\"end\":66921,\"start\":66908},{\"end\":66933,\"start\":66921},{\"end\":66945,\"start\":66933},{\"end\":67410,\"start\":67399},{\"end\":67419,\"start\":67410},{\"end\":67429,\"start\":67419},{\"end\":67834,\"start\":67824},{\"end\":67842,\"start\":67834},{\"end\":67854,\"start\":67842},{\"end\":67868,\"start\":67854},{\"end\":67878,\"start\":67868},{\"end\":68132,\"start\":68124},{\"end\":68141,\"start\":68132},{\"end\":68147,\"start\":68141},{\"end\":68156,\"start\":68147},{\"end\":68608,\"start\":68597},{\"end\":68622,\"start\":68608},{\"end\":68636,\"start\":68622},{\"end\":68648,\"start\":68636},{\"end\":68657,\"start\":68648},{\"end\":68676,\"start\":68657},{\"end\":69008,\"start\":68997},{\"end\":69019,\"start\":69008},{\"end\":69031,\"start\":69019},{\"end\":69042,\"start\":69031},{\"end\":69061,\"start\":69042},{\"end\":69455,\"start\":69441},{\"end\":69466,\"start\":69455},{\"end\":69477,\"start\":69466},{\"end\":69489,\"start\":69477},{\"end\":69508,\"start\":69489},{\"end\":69520,\"start\":69508},{\"end\":69863,\"start\":69856},{\"end\":69871,\"start\":69863},{\"end\":69879,\"start\":69871},{\"end\":69886,\"start\":69879},{\"end\":70271,\"start\":70264},{\"end\":70281,\"start\":70271},{\"end\":70298,\"start\":70281},{\"end\":70486,\"start\":70474},{\"end\":70500,\"start\":70486},{\"end\":70513,\"start\":70500},{\"end\":70524,\"start\":70513},{\"end\":70536,\"start\":70524},{\"end\":71049,\"start\":71033},{\"end\":71057,\"start\":71049},{\"end\":71068,\"start\":71057},{\"end\":71076,\"start\":71068},{\"end\":71085,\"start\":71076},{\"end\":71095,\"start\":71085},{\"end\":71592,\"start\":71576},{\"end\":71601,\"start\":71592},{\"end\":71797,\"start\":71781},{\"end\":71807,\"start\":71797},{\"end\":71817,\"start\":71807},{\"end\":71826,\"start\":71817},{\"end\":72179,\"start\":72171},{\"end\":72186,\"start\":72179},{\"end\":72193,\"start\":72186},{\"end\":72202,\"start\":72193},{\"end\":72210,\"start\":72202}]", "bib_venue": "[{\"end\":59613,\"start\":59530},{\"end\":60141,\"start\":60074},{\"end\":60611,\"start\":60533},{\"end\":61096,\"start\":61033},{\"end\":61535,\"start\":61492},{\"end\":61852,\"start\":61795},{\"end\":62231,\"start\":62154},{\"end\":62678,\"start\":62590},{\"end\":63153,\"start\":63076},{\"end\":63639,\"start\":63551},{\"end\":63995,\"start\":63943},{\"end\":64343,\"start\":64255},{\"end\":64814,\"start\":64731},{\"end\":65195,\"start\":65179},{\"end\":65533,\"start\":65469},{\"end\":66027,\"start\":65970},{\"end\":66482,\"start\":66394},{\"end\":67023,\"start\":66945},{\"end\":67493,\"start\":67429},{\"end\":67886,\"start\":67878},{\"end\":68234,\"start\":68156},{\"end\":68715,\"start\":68676},{\"end\":69138,\"start\":69061},{\"end\":69589,\"start\":69536},{\"end\":69959,\"start\":69886},{\"end\":70262,\"start\":70226},{\"end\":70624,\"start\":70536},{\"end\":71172,\"start\":71095},{\"end\":71574,\"start\":71494},{\"end\":71895,\"start\":71826},{\"end\":72297,\"start\":72210},{\"end\":59683,\"start\":59615},{\"end\":60195,\"start\":60143},{\"end\":60676,\"start\":60613},{\"end\":61146,\"start\":61098},{\"end\":62295,\"start\":62233},{\"end\":62753,\"start\":62680},{\"end\":63217,\"start\":63155},{\"end\":63714,\"start\":63641},{\"end\":64418,\"start\":64345},{\"end\":64884,\"start\":64816},{\"end\":65584,\"start\":65535},{\"end\":66557,\"start\":66484},{\"end\":67088,\"start\":67025},{\"end\":67544,\"start\":67495},{\"end\":68299,\"start\":68236},{\"end\":69202,\"start\":69140},{\"end\":70019,\"start\":69961},{\"end\":70699,\"start\":70626},{\"end\":71236,\"start\":71174},{\"end\":72371,\"start\":72299}]"}}}, "year": 2023, "month": 12, "day": 17}