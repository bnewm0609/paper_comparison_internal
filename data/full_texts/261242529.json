{"id": 261242529, "updated": "2023-11-21 22:23:02.941", "metadata": {"title": "TextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models", "authors": "[{\"first\":\"Shengpeng\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"Jialong\",\"last\":\"Zuo\",\"middle\":[]},{\"first\":\"Minghui\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Ziyue\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Feiyang\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xinyu\",\"last\":\"Duan\",\"middle\":[]},{\"first\":\"Baoxing\",\"last\":\"Huai\",\"middle\":[]},{\"first\":\"Zhou\",\"last\":\"Zhao\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,220 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available at https://sall-e.github.io/", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2308-14430", "doi": "10.48550/arxiv.2308.14430"}}, "content": {"source": {"pdf_hash": "9f432d2500758cd1182fe47fb09c2065bf7b9123", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.14430v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7c1f07ea097748203d855aca90ce34c88fe48832", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9f432d2500758cd1182fe47fb09c2065bf7b9123.txt", "contents": "\nTEXTROLSPEECH: A TEXT STYLE CONTROL SPEECH CORPUS WITH CODEC LANGUAGE TEXT-TO-SPEECH MODELS\n\n\nShengpeng Ji shengpengji@zju.edu.cn \nZhejiang University\nHuawei Cloud\n\nJialong Zuo jialongzuo@zju.edu.cn \nZhejiang University\nHuawei Cloud\n\nMinghui Fang minghuifang@zju.edu.cn \nZhejiang University\nHuawei Cloud\n\nZiyue Jiang \nZhejiang University\nHuawei Cloud\n\nFeiyang Chen \nZhejiang University\nHuawei Cloud\n\nXinyu Duan \nZhejiang University\nHuawei Cloud\n\nBaoxing Huai \nZhejiang University\nHuawei Cloud\n\nZhou Zhao zhaozhou@zju.edu.cn \nZhejiang University\nHuawei Cloud\n\nTEXTROLSPEECH: A TEXT STYLE CONTROL SPEECH CORPUS WITH CODEC LANGUAGE TEXT-TO-SPEECH MODELS\nIndex Terms-DatasetText-to-SpeechStyle Control\nRecently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of highquality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,220 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available on the demo page https://sall-e.github.io/.\n\nINTRODUCTION\n\nIn recent years, significant advancements have been made in the field of speech synthesis [1,2,3], with an increasing focus on a more challenging task known as controllable Text-to-Speech (TTS). Previous studies in controllable TTS have predominantly employed either reference audio for style transfer [4] or employed different style factors such as speaking rate [5], pitch [6], and prosody [7] for speech control. However, these approaches necessitate users to provide specific \u2020 Corresponding Author: Zhou Zhao, zhaozhou@zju.edu.cn values for style factors which requiring professional acoustic knowledge [5,6,7], or select reference speech [4] that satisfies the desired criteria. These methods are time-consuming and lack user-friendliness. Moreover, the style information derived from the reference speech lacks intuitiveness and interpretability. The effect over these styles is often constrained to the training set of the reference audio, resulting in weak generalization capabilities for unseen styles.\n\nBased on the aforementioned limitations: PromptTTS [8] proposes that it is a preferable choice to achieve style control using a natural language text description. We believe that utilizing natural text descriptions for controlling style in speech is the direction for future development of controllable TTS systems, due to its user-friendliness, generalizability, and interpretability. However, to the best of our knowledge, there is currently no high-quality, large-scale open-source text style prompt speech dataset available for advanced textcontrollable TTS models. In this work, we introduce a novel 330-hour clean text style prompt speech emotion dataset called TextrolSpeech. Each style encompasses 5 style factors and 500 distinct natural language text descriptions. Given the increased demands of controllable TTS systems for speech diversity, we get inspiration from [2] and propose Salle, which employs discrete tokens [9] based Residual Vector Quantization (RVQ) instead of conventional mel spectrograms. The tokens in Salle exhibit a hierarchical structure, where tokens from previous quantizers capture acoustic properties such as speaker identity, while consecutive quantizers learn finegrained acoustic details. Building upon this characteristic, we directly utilize the text style tokens in an autoregressive manner to prompt the generation of the first layer of acoustic tokens. Our contributions can be summarized as follows:\n\n\u2022 We have released TextrolSpeech, an open-source speech emotion dataset that is large-scale, multi-speaker, and enriched with diverse and natural text descriptions. This dataset aims to drive the development of text controllable TTS systems.\n\n\u2022 We provide a detailed account of the creation process of TextrolSpeech, through our experiments, we have devised an efficient prompt programming methodology \u2022 We propose Salle, a multi-stage discrete style tokenguided control framework for TTS language models, which exhibits powerful in-context capabilities.\n\n\nTEXTROLSPEECH DATASET\n\n\nOverview of the TextrolSpeech\n\nGiven the absence of large-scale and high-quality TTS datasets with text style prompts, we have created and made available a dataset called TextrolSpeech, which consists of speech samples paired with corresponding style prompts. As shown in Table 1, to the best of our knowledge, there have been some previous attempts to construct similar datasets. However, these datasets are often either proprietary [10,11] or only a small portion of the data is publicly released [8] . In previous works, dataset sizes were also limited to a few tens of hours or several thousand corresponding text prompts, greatly restricting the performance of the models. In contrast, TextrolSpeech provides an open-source dataset consisting of 330 hours of speech data and 236,220 naturally occurring text style descriptions. The speech samples and their corresponding prompts describe the same style and content, and the audio waveforms are resampled to a codec format with a sampling rate of 24kHz. Each speech sample in TextrolSpeech includes five different style factors: gender, pitch, speaking speed, volume, and emotion. The emotion factor comprises eight categories 1 , while the gender factor has two categories (male/female). The remaining factors, pitch, speaking speed, and volume, consist of three categories (high/low/normal). We randomly set aside 200 samples as the validation set, another 200 samples as the test set, and the remaining data for training purposes.\n\n\nData Collection and Text preprocessing\n\nBased on the previous methodology used in PromptTTS [8] for dataset creation, we integrated the clean portion of the LibriTTS dataset [12] and the VCTK dataset [13] to form a part of the audio corpus in TextrolSpeech. Considering the limited emotional content in these two datasets, we collected and curated a separate set of emotion datasets, namely ESD 1 angry/contempt/disgusted/fear/happy/sad/surprised/neutral\n(a) example1 (b) example2\nFig. 1: Two sets of word cloud exemplars [14], TESS [15], MEAD [16], SAVEE 2 and MESS [17].\n\nIn total, we obtained 42,909 instances of emotional speech data. For those instances lacking emotional content, we labeled their style factors as \"neutral.\" To extract acoustic features, we employed librosa to analyze energy levels and a world vocoder [18] to extract pitch information. Additionally, we aligned the text and speech data to obtain duration information, which was subsequently averaged. Finally, the energy levels, pitch, and speech rate were categorized into three levels (high/low/normal) based on the overall distribution of the data.\n\n\nPrompt Programming\n\nIt is worth noting that we have greatly expanded the diversity of text style prompts through prompt programming. As illustrated in Figure 1, we found that previous work had only 5 types of descriptions for each style, which makes it difficult to cover all situations in real-world scenarios and increases the risk of the model learning biases. Through our continuous experimentation, we have designed an efficient multi-stage prompt programming approach. After manual selection, we obtained 500 naturally described prompts for each style group. We utilize GPT-3.5-TURBO to generate style prompts. it is beneficial to consider prompt programming from the perspective of constraint behavior [19]. We have categorized the prompts provided to GPT into four stages.  Fig. 2: The overall architecture of Salle style based on four keywords: \"male\", \"high pitch\", \"fast speaking speed\", \"normal energy\". Increasing diversity We hope that the style prompts generated by GPT will contain the content of the keywords while ensuring diversity, so we allow GPT to use the keywords themselves or their synonyms, or even to generate style prompts based on their own experience. E.g. you can use them themselves or replace them with their synonyms, and even use some adjectives to describe them. For example, you can use \"tone\", \"key\" or \"volume\" to describe \"pitch\". Reducing irrelevant descriptions GPT trained on web-scale corpora are very powerful in natural language understanding and reasoning , so it is necessary to limit the GPT to prevent the generation of too much content unrelated to keywords from affecting the quality of style prompts. E.g. Please remember do not include scene descriptions such as \"churches\" in the sentences. Few-shot templates The GPT can be conditioned with a few-shot prompts to fully utilize the knowledge it already has [20]. Based on this experience, we have used hand-crafted templates as part of the prompts to enhance the quality of the final generated style prompts. E.g. I provide you with one correct template based on four keywords that you can refer to and generate diverse sentences. The template is \"The rapid, loud and high-keyed voice belongs to the girl\". Figure 1 shows two sample word clouds for stylistic descriptions. On our demo website, we showcase a collection of 500 distinct descriptions for a group of styles.\n\n\nSALLE\n\nBased on [9], Salle (denoted as \u03b8) leverages style prompt autoregressive codec LM and a non-autoregressive textto-speech codec LM to generate acoustic tokens at different granularities. Given the dataset D = {s i , x i , y i },\n\nwhere (x, y) represents a paired set of text and speech, S = {s 1 , s 2 , s 3 , \u00b7 \u00b7 \u00b7 , s L } denotes the corresponding text style description, we first get a discrete representation A T * n of each speech y:\nA T * n = encodec(y)(1)\nwhere A represents the two-dimensional acoustic code matrix, T is the downsampled utterance length and n represents the quantity codes for frame t. In all of our experiments, the hyperparameter n is set to 8. Taking into consideration the hierarchical structure of the acoustic code, where the first layer codeblock often contains primary speaker information, while the consecutive n \u2212 1 layers of quantizers learn fine acoustic details. We divide the model into two parts: for the generation of the first layer codeblock A (1:T,1) of speech, we design an autoregressive structure. As shown in Figure 1, the overall codec language \u03b8 SAR is a decoder-only architecture that does not require textspeech alignment. It generates acoustic tokens based on the text style prompt S and corresponding text token embeddings x, and stops the generation of speech codec by predicting the < eos > token. This process is formulated as: (2) To expedite the overall speed of the Salle, in the second stage, we employ a parallelized language model \u03b8 SN AR to predict the discrete tokens from the second to the last n quantizers, denoted as A (:,2:n) . In this stage, \u03b8 SN AR follows a conventional speech synthesis model, where we exclude the inclusion of text style prompts. Instead, the generation of each subsequent layer of acoustic codeblocks relies solely on the text information x and the acoustic tokens generated by the previous layers, formulated as: During the training process of \u03b8 SN AR , to mitigate the learned biases of the model, we randomly select A (:,i) as the ground truth in each batch. Both the \u03b8 SAR model and the \u03b8 SN AR model share the same transformer architecture, consisting of 6 layers, 16 attention heads, an embedding dimension of 512, a feed-forward layer dimension of 2048, and a dropout rate of 0.1. We calculate the cross entropy loss only at A (1:T,n) . Overall, the prediction of A can be modeled as: \n\n4. EXPERIMENTS\n\n\nExperiment setup\n\nSalle is trained on 4 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustic tokens per GPU for 200k steps.\n\nWe optimize the models with the AdamW optimizer, warm up the initial learning rate 1 \u00d7 10 \u22127 for the first 32k updates to a peak of 5 \u00d7 10 \u22124 , and then linear decay it. We attempt to reproduce the current SOTA model PromptTTS, and train it on the TextrolSpeech dataset as a baseline for comparison. In order to assess the model's performance, we train two separate classification models for gender and emotion, achieving classification accuracies of 99.2% and 85%, respectively. For pitch, speech rate, and energy, we obtain labels using a similar method employed in TextrolSpeech. We analyze the Mean Opinion Score (MOS) ratings from 1 to 5 on two dimensions: MOS-Q (Quality, including attributes such as clarity, highfrequency, and prosody) and MOS-S (Speaker similarity with the style prompt). Table 2 presents the accuracy results of PromptTTS and Salle models on five style factors in the TextrolSpeech test set. Based on the findings, we can draw the following conclusions: In gender, pitch, speech, volume, and emotion, Salle surpasses PromptTTS comprehensively, achieving an average accuracy rate of 87.6%. We attribute this improvement to the architecture based on discrete codecs, which enhances the model's diversity. Additionally, we directly guide the acoustic tokens by utilizing text and style prompts in an autoregressive manner. This approach avoids some information loss that occurs when extracting CLS tokens. Furthermore, we observe that the model easily learns gender-related information, while the classification accuracy for emotions tends to be lower. We attribute this to factors such as limited data, interference from neutral emotions, and the inherent difficulty of emotion classification.  \n\n\nResults\n\n\nMain Results\n\n\nSpeech Quality\n\nTo assess the perceptual quality and similarity, we conducted MOS-Q and MOS-S analysis on four different conditions: 1) GT (Ground Truth), representing the original unaltered audio; 2) GT-codec, where the original audio was encoded and subsequently decoded for reconstruction; 3) PromptTTS, the current state-of-the-art model; and 4) Salle, we proposed model. By referring to Table 3, we can observe the following findings: In subjective MOS experiments, it is evident that Salle outperforms PromptTTS in terms of audio quality and audio similarity. Particularly, the improvement in audio similarity is by 0.14. However, we note that the overall sound quality is not exceptionally high. We attribute this to factors such as limited data and the inherent difficulty of the text stylecontrolled speech synthesis task itself. We have extensively demonstrated the diversity of results achieved by Salle in our demo.\n\n\nCONCLUSION\n\nIn this paper, we release TextrolSpeech, a large-scale and high-quality text style prompt speech emotion corpus in the field of controllable TTS for the first time. Additionally, we propose a straightforward codec language model named Salle, which is built upon TextrolSpeech. We believe that there is still significant space for improvement in text style control, particularly in enhancing both audio quality and robustness in style control, especially in the domain of emotional effects. We anticipate that TextrolSpeech and Salle can serve as future baselines in this regard.\n\n)\n|A (<t,1) , S, x; \u03b8 SAR )\n\n\n|A (:,<i) , x; \u03b8 SN AR ) (3)\n\n\np(A (1:T,n) |S, x; \u03b8) = p(A (1:T,1) |S, x; \u03b8 SAR )\u00b7 n i=2 p(A (1:T,i) |A (1:T,<i) , x, \u03b8 SN AR )\n\nTable 1 :\n1Comparison between Text style prompt TTS datasets.Dataset \nOpen source Hours Text items Prompt diversity Speaker Emotion \nStylePrompt [10] \nno \n12 \n-\n-\n8 \n-\nNLSpeech [11] \nno \n44 \n32000 \n-\n7 \nyes \nPromptSpeech [8] \nyes (part) \n-\n27893 \n5 \n-\nno \nTextrolSpeech \nyes \n330 \n236220 \n500 \n1324 \nyes \n\nthat significantly expands the number of text descrip-\ntions for each style group. \n\n\n\nTable 2 :\n2The Acc(%) of Prompttts and Salle on style factors.Models \nGender Pitch Speed Volume Emotion Mean \nPromptTTS \n92.5 \n82.5 \n83 \n82 \n71.5 \n82.3 \nSalle \n95.5 \n90.5 \n85 \n86 \n81 \n87.6 \n\n\n\nTable 3 :\n3The results of MOS-Q and MOS-S with 95% confidence intervals.Models \nMOS-Q \nMOS-S \nGT \n4.21 \u00b1 0.09 4.25 \u00b1 0.08 \nGT-codec \n4.10 \u00b1 0.11 4.13 \u00b1 0.09 \nPromptTTS 3.76 \u00b1 0.12 3.74 \u00b1 0.09 \nSalle \n3.78 \u00b1 0.09 3.88 \u00b1 0.10 \n\n\nhttp://kahlan.eps.surrey.ac.uk/savee\n\nFastspeech 2: Fast and high-quality end-to-end text to speech. Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu, arXiv:2006.04558arXiv preprintYi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu, \"Fastspeech 2: Fast and high-quality end-to-end text to speech,\" arXiv preprint arXiv:2006.04558, 2020.\n\nNeural codec language models are zero-shot text to speech synthesizers. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, arXiv:2301.02111arXiv preprintChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al., \"Neural codec lan- guage models are zero-shot text to speech synthesizers,\" arXiv preprint arXiv:2301.02111, 2023.\n\nMega-tts: Zero-shot text-to-speech at scale with intrinsic inductive bias. Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin, arXiv:2306.03509arXiv preprintZiyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin, et al., \"Mega-tts: Zero-shot text-to-speech at scale with intrinsic inductive bias,\" arXiv preprint arXiv:2306.03509, 2023.\n\nNorespeech: Knowledge distillation based conditional diffusion model for noise-robust expressive tts. Dongchao Yang, Songxiang Liu, Jianwei Yu, Helin Wang, Chao Weng, Yuexian Zou, arXiv:2211.02448arXiv preprintDongchao Yang, Songxiang Liu, Jianwei Yu, He- lin Wang, Chao Weng, and Yuexian Zou, \"Nore- speech: Knowledge distillation based conditional dif- fusion model for noise-robust expressive tts,\" arXiv preprint arXiv:2211.02448, 2022.\n\nSpeaking speed control of end-to-end speech synthesis using sentence-level conditioning. Jae-Sung Bae, Hanbin Bae, Young-Sun Joo, Junmo Lee, Gyeong-Hoon Lee, Hoon-Young Cho, arXiv:2007.15281arXiv preprintJae-Sung Bae, Hanbin Bae, Young-Sun Joo, Junmo Lee, Gyeong-Hoon Lee, and Hoon-Young Cho, \"Speak- ing speed control of end-to-end speech synthesis us- ing sentence-level conditioning,\" arXiv preprint arXiv:2007.15281, 2020.\n\nFastpitchformant: Sourcefilter based decomposed modeling for speech synthesis. Taejun Bak, Jae-Sung Bae, Hanbin Bae, Young-Ik Kim, Hoon-Young Cho, arXiv:2106.15123arXiv preprintTaejun Bak, Jae-Sung Bae, Hanbin Bae, Young-Ik Kim, and Hoon-Young Cho, \"Fastpitchformant: Source- filter based decomposed modeling for speech synthesis,\" arXiv preprint arXiv:2106.15123, 2021.\n\nTowards end-to-end prosody transfer for expressive speech synthesis with tacotron. R J Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron Weiss, Rob Clark, Saurous, international conference on machine learning. PMLRRJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron Weiss, Rob Clark, and Rif A Saurous, \"Towards end-to-end prosody trans- fer for expressive speech synthesis with tacotron,\" in international conference on machine learning. PMLR, 2018, pp. 4693-4702.\n\nPrompttts: Controllable text-to-speech with text descriptions. Zhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, Xu Tan, ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEZhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and Xu Tan, \"Prompttts: Controllable text-to-speech with text descriptions,\" in ICASSP 2023-2023 IEEE In- ternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1-5.\n\nHigh fidelity neural audio compression. Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi, arXiv:2210.13438arXiv preprintAlexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi, \"High fidelity neural audio compression,\" arXiv preprint arXiv:2210.13438, 2022.\n\nPromptstyle: Controllable style transfer for text-to-speech with natural language descriptions. Guanghou Liu, Yongmao Zhang, Yi Lei, Yunlin Chen, Rui Wang, Zhifei Li, Lei Xie, arXiv:2305.19522arXiv preprintGuanghou Liu, Yongmao Zhang, Yi Lei, Yunlin Chen, Rui Wang, Zhifei Li, and Lei Xie, \"Prompt- style: Controllable style transfer for text-to-speech with natural language descriptions,\" arXiv preprint arXiv:2305.19522, 2023.\n\nInstructtts: Modelling expressive tts in discrete latent space with natural language style prompt. Dongchao Yang, Songxiang Liu, Rongjie Huang, Guangzhi Lei, Chao Weng, Helen Meng, Dong Yu, arXiv:2301.13662arXiv preprintDongchao Yang, Songxiang Liu, Rongjie Huang, Guangzhi Lei, Chao Weng, Helen Meng, and Dong Yu, \"Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt,\" arXiv preprint arXiv:2301.13662, 2023.\n\nLibritts: A corpus derived from librispeech for text-tospeech. Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, Yonghui Wu, Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu, \"Lib- ritts: A corpus derived from librispeech for text-to- speech,\" Interspeech 2019, 2019.\n\nCstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit. Christophe Veaux, Junichi Yamagishi, Kirsten Macdonald, The Centre for Speech Technology Research (CSTR). 615University of EdinburghChristophe Veaux, Junichi Yamagishi, Kirsten MacDon- ald, et al., \"Cstr vctk corpus: English multi-speaker cor- pus for cstr voice cloning toolkit,\" University of Ed- inburgh. The Centre for Speech Technology Research (CSTR), vol. 6, pp. 15, 2017.\n\nSeen and unseen emotional style transfer for voice conversion with a new emotional speech dataset. Kun Zhou, Berrak Sisman, Rui Liu, Haizhou Li, ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing. ICASSPKun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li, \"Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset,\" in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\n\n. IEEE. IEEE, 2021, pp. 920-924.\n\nToronto emotional speech set (tess)-younger talker happy. Kate Dupuis, M Kathleen Pichora-Fuller , Kate Dupuis and M Kathleen Pichora-Fuller, \"Toronto emotional speech set (tess)-younger talker happy,\" 2010.\n\nMead: A large-scale audio-visual dataset for emotional talking-face generation. Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, Chen Change Loy, European Conference on Computer Vision. SpringerKaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy, \"Mead: A large-scale audio-visual dataset for emotional talking-face generation,\" in Euro- pean Conference on Computer Vision. Springer, 2020, pp. 700-717.\n\nCategorical and dimensional ratings of emotional speech: Behavioral findings from the morgan emotional speech set. D Shae, Morgan, Journal of Speech, Language, and Hearing Research. 6211Shae D Morgan, \"Categorical and dimensional ratings of emotional speech: Behavioral findings from the mor- gan emotional speech set,\" Journal of Speech, Lan- guage, and Hearing Research, vol. 62, no. 11, pp. 4015- 4029, 2019.\n\nWorld: a vocoder-based high-quality speech synthesis system for real-time applications. Masanori Morise, Fumiya Yokomori, Kenji Ozawa, IEICE TRANSAC-TIONS on Information and Systems. 997Masanori Morise, Fumiya Yokomori, and Kenji Ozawa, \"World: a vocoder-based high-quality speech synthesis system for real-time applications,\" IEICE TRANSAC- TIONS on Information and Systems, vol. 99, no. 7, pp. 1877-1884, 2016.\n\nPrompt programming for large language models: Beyond the few-shot paradigm. Laria Reynolds, Kyle Mcdonell, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Laria Reynolds and Kyle McDonell, \"Prompt program- ming for large language models: Beyond the few-shot paradigm,\" in Extended Abstracts of the 2021 CHI Con- ference on Human Factors in Computing Systems, 2021, pp. 1-7.\n\nRon Mokady, Amir Hertz, Amit H Bermano, arXiv:2111.09734Clipcap: Clip prefix for image captioning. arXiv preprintRon Mokady, Amir Hertz, and Amit H Bermano, \"Clip- cap: Clip prefix for image captioning,\" arXiv preprint arXiv:2111.09734, 2021.\n", "annotations": {"author": "[{\"end\":165,\"start\":95},{\"end\":234,\"start\":166},{\"end\":305,\"start\":235},{\"end\":352,\"start\":306},{\"end\":400,\"start\":353},{\"end\":446,\"start\":401},{\"end\":494,\"start\":447},{\"end\":559,\"start\":495}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":105},{\"end\":177,\"start\":174},{\"end\":247,\"start\":243},{\"end\":317,\"start\":312},{\"end\":365,\"start\":361},{\"end\":411,\"start\":407},{\"end\":459,\"start\":455},{\"end\":504,\"start\":500}]", "author_first_name": "[{\"end\":104,\"start\":95},{\"end\":173,\"start\":166},{\"end\":242,\"start\":235},{\"end\":311,\"start\":306},{\"end\":360,\"start\":353},{\"end\":406,\"start\":401},{\"end\":454,\"start\":447},{\"end\":499,\"start\":495}]", "author_affiliation": "[{\"end\":164,\"start\":132},{\"end\":233,\"start\":201},{\"end\":304,\"start\":272},{\"end\":351,\"start\":319},{\"end\":399,\"start\":367},{\"end\":445,\"start\":413},{\"end\":493,\"start\":461},{\"end\":558,\"start\":526}]", "title": "[{\"end\":92,\"start\":1},{\"end\":651,\"start\":560}]", "venue": null, "abstract": "[{\"end\":2223,\"start\":699}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2332,\"start\":2329},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2334,\"start\":2332},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2336,\"start\":2334},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2544,\"start\":2541},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2606,\"start\":2603},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2617,\"start\":2614},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2634,\"start\":2631},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2850,\"start\":2847},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2852,\"start\":2850},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2854,\"start\":2852},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2886,\"start\":2883},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3307,\"start\":3304},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4133,\"start\":4130},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4186,\"start\":4183},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5718,\"start\":5714},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5721,\"start\":5718},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5782,\"start\":5779},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6865,\"start\":6862},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6948,\"start\":6944},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6974,\"start\":6970},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7296,\"start\":7292},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7307,\"start\":7303},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7318,\"start\":7314},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7341,\"start\":7337},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7600,\"start\":7596},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8612,\"start\":8608},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9766,\"start\":9762},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10297,\"start\":10294}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16118,\"start\":16090},{\"attributes\":{\"id\":\"fig_1\"},\"end\":16149,\"start\":16119},{\"attributes\":{\"id\":\"fig_2\"},\"end\":16248,\"start\":16150},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":16641,\"start\":16249},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":16834,\"start\":16642},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":17062,\"start\":16835}]", "paragraph": "[{\"end\":3251,\"start\":2239},{\"end\":4697,\"start\":3253},{\"end\":4940,\"start\":4699},{\"end\":5253,\"start\":4942},{\"end\":6767,\"start\":5311},{\"end\":7224,\"start\":6810},{\"end\":7342,\"start\":7251},{\"end\":7896,\"start\":7344},{\"end\":10275,\"start\":7919},{\"end\":10512,\"start\":10285},{\"end\":10722,\"start\":10514},{\"end\":12669,\"start\":10747},{\"end\":12685,\"start\":12671},{\"end\":12819,\"start\":12706},{\"end\":14541,\"start\":12821},{\"end\":15496,\"start\":14585},{\"end\":16089,\"start\":15511}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7250,\"start\":7225},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10746,\"start\":10723}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5559,\"start\":5552},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13626,\"start\":13619},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":14968,\"start\":14961}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2237,\"start\":2225},{\"attributes\":{\"n\":\"2.\"},\"end\":5277,\"start\":5256},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5309,\"start\":5280},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6808,\"start\":6770},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7917,\"start\":7899},{\"attributes\":{\"n\":\"3.\"},\"end\":10283,\"start\":10278},{\"attributes\":{\"n\":\"4.1.\"},\"end\":12704,\"start\":12688},{\"attributes\":{\"n\":\"4.2.\"},\"end\":14551,\"start\":14544},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":14566,\"start\":14554},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":14583,\"start\":14569},{\"attributes\":{\"n\":\"5.\"},\"end\":15509,\"start\":15499},{\"end\":16092,\"start\":16091},{\"end\":16259,\"start\":16250},{\"end\":16652,\"start\":16643},{\"end\":16845,\"start\":16836}]", "table": "[{\"end\":16641,\"start\":16311},{\"end\":16834,\"start\":16705},{\"end\":17062,\"start\":16908}]", "figure_caption": "[{\"end\":16118,\"start\":16093},{\"end\":16149,\"start\":16121},{\"end\":16248,\"start\":16152},{\"end\":16311,\"start\":16261},{\"end\":16705,\"start\":16654},{\"end\":16908,\"start\":16847}]", "figure_ref": "[{\"end\":8058,\"start\":8050},{\"end\":8687,\"start\":8681},{\"end\":10120,\"start\":10112},{\"end\":11349,\"start\":11341},{\"end\":12618,\"start\":12611}]", "bib_author_first_name": "[{\"end\":17166,\"start\":17164},{\"end\":17178,\"start\":17172},{\"end\":17185,\"start\":17183},{\"end\":17194,\"start\":17191},{\"end\":17205,\"start\":17200},{\"end\":17216,\"start\":17212},{\"end\":17230,\"start\":17223},{\"end\":17526,\"start\":17519},{\"end\":17540,\"start\":17533},{\"end\":17549,\"start\":17547},{\"end\":17561,\"start\":17554},{\"end\":17573,\"start\":17569},{\"end\":17586,\"start\":17580},{\"end\":17596,\"start\":17592},{\"end\":17610,\"start\":17603},{\"end\":17623,\"start\":17616},{\"end\":17635,\"start\":17630},{\"end\":17995,\"start\":17990},{\"end\":18005,\"start\":18003},{\"end\":18018,\"start\":18011},{\"end\":18030,\"start\":18023},{\"end\":18040,\"start\":18036},{\"end\":18052,\"start\":18048},{\"end\":18068,\"start\":18059},{\"end\":18080,\"start\":18073},{\"end\":18096,\"start\":18088},{\"end\":18108,\"start\":18103},{\"end\":18503,\"start\":18495},{\"end\":18519,\"start\":18510},{\"end\":18532,\"start\":18525},{\"end\":18542,\"start\":18537},{\"end\":18553,\"start\":18549},{\"end\":18567,\"start\":18560},{\"end\":18932,\"start\":18924},{\"end\":18944,\"start\":18938},{\"end\":18959,\"start\":18950},{\"end\":18970,\"start\":18965},{\"end\":18987,\"start\":18976},{\"end\":19003,\"start\":18993},{\"end\":19348,\"start\":19342},{\"end\":19362,\"start\":19354},{\"end\":19374,\"start\":19368},{\"end\":19388,\"start\":19380},{\"end\":19404,\"start\":19394},{\"end\":19719,\"start\":19718},{\"end\":19721,\"start\":19720},{\"end\":19739,\"start\":19735},{\"end\":19756,\"start\":19752},{\"end\":19769,\"start\":19763},{\"end\":19781,\"start\":19776},{\"end\":19795,\"start\":19791},{\"end\":19805,\"start\":19802},{\"end\":19816,\"start\":19813},{\"end\":20241,\"start\":20234},{\"end\":20254,\"start\":20247},{\"end\":20266,\"start\":20261},{\"end\":20276,\"start\":20271},{\"end\":20285,\"start\":20283},{\"end\":20697,\"start\":20688},{\"end\":20712,\"start\":20708},{\"end\":20727,\"start\":20720},{\"end\":20743,\"start\":20738},{\"end\":21030,\"start\":21022},{\"end\":21043,\"start\":21036},{\"end\":21053,\"start\":21051},{\"end\":21065,\"start\":21059},{\"end\":21075,\"start\":21072},{\"end\":21088,\"start\":21082},{\"end\":21096,\"start\":21093},{\"end\":21463,\"start\":21455},{\"end\":21479,\"start\":21470},{\"end\":21492,\"start\":21485},{\"end\":21508,\"start\":21500},{\"end\":21518,\"start\":21514},{\"end\":21530,\"start\":21525},{\"end\":21541,\"start\":21537},{\"end\":21880,\"start\":21875},{\"end\":21890,\"start\":21886},{\"end\":21900,\"start\":21897},{\"end\":21910,\"start\":21908},{\"end\":21921,\"start\":21918},{\"end\":21923,\"start\":21922},{\"end\":21933,\"start\":21931},{\"end\":21946,\"start\":21939},{\"end\":21960,\"start\":21953},{\"end\":22242,\"start\":22232},{\"end\":22257,\"start\":22250},{\"end\":22276,\"start\":22269},{\"end\":22715,\"start\":22712},{\"end\":22728,\"start\":22722},{\"end\":22740,\"start\":22737},{\"end\":22753,\"start\":22746},{\"end\":23206,\"start\":23202},{\"end\":23240,\"start\":23215},{\"end\":23442,\"start\":23433},{\"end\":23455,\"start\":23449},{\"end\":23466,\"start\":23460},{\"end\":23481,\"start\":23473},{\"end\":23493,\"start\":23488},{\"end\":23502,\"start\":23498},{\"end\":23512,\"start\":23509},{\"end\":23519,\"start\":23517},{\"end\":23537,\"start\":23526},{\"end\":23978,\"start\":23977},{\"end\":24371,\"start\":24363},{\"end\":24386,\"start\":24380},{\"end\":24402,\"start\":24397},{\"end\":24770,\"start\":24765},{\"end\":24785,\"start\":24781},{\"end\":25104,\"start\":25101},{\"end\":25117,\"start\":25113},{\"end\":25131,\"start\":25125}]", "bib_author_last_name": "[{\"end\":17170,\"start\":17167},{\"end\":17181,\"start\":17179},{\"end\":17189,\"start\":17186},{\"end\":17198,\"start\":17195},{\"end\":17210,\"start\":17206},{\"end\":17221,\"start\":17217},{\"end\":17234,\"start\":17231},{\"end\":17531,\"start\":17527},{\"end\":17545,\"start\":17541},{\"end\":17552,\"start\":17550},{\"end\":17567,\"start\":17562},{\"end\":17578,\"start\":17574},{\"end\":17590,\"start\":17587},{\"end\":17601,\"start\":17597},{\"end\":17614,\"start\":17611},{\"end\":17628,\"start\":17624},{\"end\":17638,\"start\":17636},{\"end\":18001,\"start\":17996},{\"end\":18009,\"start\":18006},{\"end\":18021,\"start\":18019},{\"end\":18034,\"start\":18031},{\"end\":18046,\"start\":18041},{\"end\":18057,\"start\":18053},{\"end\":18071,\"start\":18069},{\"end\":18086,\"start\":18081},{\"end\":18101,\"start\":18097},{\"end\":18112,\"start\":18109},{\"end\":18508,\"start\":18504},{\"end\":18523,\"start\":18520},{\"end\":18535,\"start\":18533},{\"end\":18547,\"start\":18543},{\"end\":18558,\"start\":18554},{\"end\":18571,\"start\":18568},{\"end\":18936,\"start\":18933},{\"end\":18948,\"start\":18945},{\"end\":18963,\"start\":18960},{\"end\":18974,\"start\":18971},{\"end\":18991,\"start\":18988},{\"end\":19007,\"start\":19004},{\"end\":19352,\"start\":19349},{\"end\":19366,\"start\":19363},{\"end\":19378,\"start\":19375},{\"end\":19392,\"start\":19389},{\"end\":19408,\"start\":19405},{\"end\":19733,\"start\":19722},{\"end\":19750,\"start\":19740},{\"end\":19761,\"start\":19757},{\"end\":19774,\"start\":19770},{\"end\":19789,\"start\":19782},{\"end\":19800,\"start\":19796},{\"end\":19811,\"start\":19806},{\"end\":19822,\"start\":19817},{\"end\":19831,\"start\":19824},{\"end\":20245,\"start\":20242},{\"end\":20259,\"start\":20255},{\"end\":20269,\"start\":20267},{\"end\":20281,\"start\":20277},{\"end\":20289,\"start\":20286},{\"end\":20706,\"start\":20698},{\"end\":20718,\"start\":20713},{\"end\":20736,\"start\":20728},{\"end\":20747,\"start\":20744},{\"end\":21034,\"start\":21031},{\"end\":21049,\"start\":21044},{\"end\":21057,\"start\":21054},{\"end\":21070,\"start\":21066},{\"end\":21080,\"start\":21076},{\"end\":21091,\"start\":21089},{\"end\":21100,\"start\":21097},{\"end\":21468,\"start\":21464},{\"end\":21483,\"start\":21480},{\"end\":21498,\"start\":21493},{\"end\":21512,\"start\":21509},{\"end\":21523,\"start\":21519},{\"end\":21535,\"start\":21531},{\"end\":21544,\"start\":21542},{\"end\":21884,\"start\":21881},{\"end\":21895,\"start\":21891},{\"end\":21906,\"start\":21901},{\"end\":21916,\"start\":21911},{\"end\":21929,\"start\":21924},{\"end\":21937,\"start\":21934},{\"end\":21951,\"start\":21947},{\"end\":21963,\"start\":21961},{\"end\":22248,\"start\":22243},{\"end\":22267,\"start\":22258},{\"end\":22286,\"start\":22277},{\"end\":22720,\"start\":22716},{\"end\":22735,\"start\":22729},{\"end\":22744,\"start\":22741},{\"end\":22756,\"start\":22754},{\"end\":23213,\"start\":23207},{\"end\":23447,\"start\":23443},{\"end\":23458,\"start\":23456},{\"end\":23471,\"start\":23467},{\"end\":23486,\"start\":23482},{\"end\":23496,\"start\":23494},{\"end\":23507,\"start\":23503},{\"end\":23515,\"start\":23513},{\"end\":23524,\"start\":23520},{\"end\":23541,\"start\":23538},{\"end\":23983,\"start\":23979},{\"end\":23991,\"start\":23985},{\"end\":24378,\"start\":24372},{\"end\":24395,\"start\":24387},{\"end\":24408,\"start\":24403},{\"end\":24779,\"start\":24771},{\"end\":24794,\"start\":24786},{\"end\":25111,\"start\":25105},{\"end\":25123,\"start\":25118},{\"end\":25139,\"start\":25132}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2006.04558\",\"id\":\"b0\"},\"end\":17445,\"start\":17101},{\"attributes\":{\"doi\":\"arXiv:2301.02111\",\"id\":\"b1\"},\"end\":17913,\"start\":17447},{\"attributes\":{\"doi\":\"arXiv:2306.03509\",\"id\":\"b2\"},\"end\":18391,\"start\":17915},{\"attributes\":{\"doi\":\"arXiv:2211.02448\",\"id\":\"b3\"},\"end\":18833,\"start\":18393},{\"attributes\":{\"doi\":\"arXiv:2007.15281\",\"id\":\"b4\"},\"end\":19261,\"start\":18835},{\"attributes\":{\"doi\":\"arXiv:2106.15123\",\"id\":\"b5\"},\"end\":19633,\"start\":19263},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4425995},\"end\":20169,\"start\":19635},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":253761189},\"end\":20646,\"start\":20171},{\"attributes\":{\"doi\":\"arXiv:2210.13438\",\"id\":\"b8\"},\"end\":20924,\"start\":20648},{\"attributes\":{\"doi\":\"arXiv:2305.19522\",\"id\":\"b9\"},\"end\":21354,\"start\":20926},{\"attributes\":{\"doi\":\"arXiv:2301.13662\",\"id\":\"b10\"},\"end\":21810,\"start\":21356},{\"attributes\":{\"id\":\"b11\"},\"end\":22151,\"start\":21812},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":64303572},\"end\":22611,\"start\":22153},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":225094190},\"end\":23108,\"start\":22613},{\"attributes\":{\"id\":\"b14\"},\"end\":23142,\"start\":23110},{\"attributes\":{\"id\":\"b15\"},\"end\":23351,\"start\":23144},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":221727985},\"end\":23860,\"start\":23353},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":204908241},\"end\":24273,\"start\":23862},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3570465},\"end\":24687,\"start\":24275},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":231925131},\"end\":25099,\"start\":24689},{\"attributes\":{\"doi\":\"arXiv:2111.09734\",\"id\":\"b20\"},\"end\":25343,\"start\":25101}]", "bib_title": "[{\"end\":19716,\"start\":19635},{\"end\":20232,\"start\":20171},{\"end\":22230,\"start\":22153},{\"end\":22710,\"start\":22613},{\"end\":23431,\"start\":23353},{\"end\":23975,\"start\":23862},{\"end\":24361,\"start\":24275},{\"end\":24763,\"start\":24689}]", "bib_author": "[{\"end\":17172,\"start\":17164},{\"end\":17183,\"start\":17172},{\"end\":17191,\"start\":17183},{\"end\":17200,\"start\":17191},{\"end\":17212,\"start\":17200},{\"end\":17223,\"start\":17212},{\"end\":17236,\"start\":17223},{\"end\":17533,\"start\":17519},{\"end\":17547,\"start\":17533},{\"end\":17554,\"start\":17547},{\"end\":17569,\"start\":17554},{\"end\":17580,\"start\":17569},{\"end\":17592,\"start\":17580},{\"end\":17603,\"start\":17592},{\"end\":17616,\"start\":17603},{\"end\":17630,\"start\":17616},{\"end\":17640,\"start\":17630},{\"end\":18003,\"start\":17990},{\"end\":18011,\"start\":18003},{\"end\":18023,\"start\":18011},{\"end\":18036,\"start\":18023},{\"end\":18048,\"start\":18036},{\"end\":18059,\"start\":18048},{\"end\":18073,\"start\":18059},{\"end\":18088,\"start\":18073},{\"end\":18103,\"start\":18088},{\"end\":18114,\"start\":18103},{\"end\":18510,\"start\":18495},{\"end\":18525,\"start\":18510},{\"end\":18537,\"start\":18525},{\"end\":18549,\"start\":18537},{\"end\":18560,\"start\":18549},{\"end\":18573,\"start\":18560},{\"end\":18938,\"start\":18924},{\"end\":18950,\"start\":18938},{\"end\":18965,\"start\":18950},{\"end\":18976,\"start\":18965},{\"end\":18993,\"start\":18976},{\"end\":19009,\"start\":18993},{\"end\":19354,\"start\":19342},{\"end\":19368,\"start\":19354},{\"end\":19380,\"start\":19368},{\"end\":19394,\"start\":19380},{\"end\":19410,\"start\":19394},{\"end\":19735,\"start\":19718},{\"end\":19752,\"start\":19735},{\"end\":19763,\"start\":19752},{\"end\":19776,\"start\":19763},{\"end\":19791,\"start\":19776},{\"end\":19802,\"start\":19791},{\"end\":19813,\"start\":19802},{\"end\":19824,\"start\":19813},{\"end\":19833,\"start\":19824},{\"end\":20247,\"start\":20234},{\"end\":20261,\"start\":20247},{\"end\":20271,\"start\":20261},{\"end\":20283,\"start\":20271},{\"end\":20291,\"start\":20283},{\"end\":20708,\"start\":20688},{\"end\":20720,\"start\":20708},{\"end\":20738,\"start\":20720},{\"end\":20749,\"start\":20738},{\"end\":21036,\"start\":21022},{\"end\":21051,\"start\":21036},{\"end\":21059,\"start\":21051},{\"end\":21072,\"start\":21059},{\"end\":21082,\"start\":21072},{\"end\":21093,\"start\":21082},{\"end\":21102,\"start\":21093},{\"end\":21470,\"start\":21455},{\"end\":21485,\"start\":21470},{\"end\":21500,\"start\":21485},{\"end\":21514,\"start\":21500},{\"end\":21525,\"start\":21514},{\"end\":21537,\"start\":21525},{\"end\":21546,\"start\":21537},{\"end\":21886,\"start\":21875},{\"end\":21897,\"start\":21886},{\"end\":21908,\"start\":21897},{\"end\":21918,\"start\":21908},{\"end\":21931,\"start\":21918},{\"end\":21939,\"start\":21931},{\"end\":21953,\"start\":21939},{\"end\":21965,\"start\":21953},{\"end\":22250,\"start\":22232},{\"end\":22269,\"start\":22250},{\"end\":22288,\"start\":22269},{\"end\":22722,\"start\":22712},{\"end\":22737,\"start\":22722},{\"end\":22746,\"start\":22737},{\"end\":22758,\"start\":22746},{\"end\":23215,\"start\":23202},{\"end\":23243,\"start\":23215},{\"end\":23449,\"start\":23433},{\"end\":23460,\"start\":23449},{\"end\":23473,\"start\":23460},{\"end\":23488,\"start\":23473},{\"end\":23498,\"start\":23488},{\"end\":23509,\"start\":23498},{\"end\":23517,\"start\":23509},{\"end\":23526,\"start\":23517},{\"end\":23543,\"start\":23526},{\"end\":23985,\"start\":23977},{\"end\":23993,\"start\":23985},{\"end\":24380,\"start\":24363},{\"end\":24397,\"start\":24380},{\"end\":24410,\"start\":24397},{\"end\":24781,\"start\":24765},{\"end\":24796,\"start\":24781},{\"end\":25113,\"start\":25101},{\"end\":25125,\"start\":25113},{\"end\":25141,\"start\":25125}]", "bib_venue": "[{\"end\":17162,\"start\":17101},{\"end\":17517,\"start\":17447},{\"end\":17988,\"start\":17915},{\"end\":18493,\"start\":18393},{\"end\":18922,\"start\":18835},{\"end\":19340,\"start\":19263},{\"end\":19877,\"start\":19833},{\"end\":20389,\"start\":20291},{\"end\":20686,\"start\":20648},{\"end\":21020,\"start\":20926},{\"end\":21453,\"start\":21356},{\"end\":21873,\"start\":21812},{\"end\":22336,\"start\":22288},{\"end\":22847,\"start\":22758},{\"end\":23116,\"start\":23112},{\"end\":23200,\"start\":23144},{\"end\":23581,\"start\":23543},{\"end\":24042,\"start\":23993},{\"end\":24456,\"start\":24410},{\"end\":24879,\"start\":24796},{\"end\":25198,\"start\":25157}]"}}}, "year": 2023, "month": 12, "day": 17}