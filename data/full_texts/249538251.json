{"id": 249538251, "updated": "2023-10-31 14:19:25.337", "metadata": {"title": "Byzantine-Resilient Decentralized Stochastic Optimization with Robust Aggregation Rules", "authors": "[{\"first\":\"Zhaoxian\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Tianyi\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Qing\",\"last\":\"Ling\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This paper focuses on decentralized stochastic optimization in the presence of Byzantine attacks. During the optimization process, an unknown number of malfunctioning or malicious workers, termed as Byzantine workers, disobey the algorithmic protocol and send arbitrarily wrong messages to their neighbors. Even though various Byzantine-resilient algorithms have been developed for distributed stochastic optimization with a central server, we show that there are two major issues in the existing robust aggregation rules when being applied to the decentralized scenario: disagreement and non-doubly stochastic virtual mixing matrix. This paper provides comprehensive analysis that discloses the negative effects of these two issues, and gives guidelines of designing favorable Byzantine-resilient decentralized stochastic optimization algorithms. Under these guidelines, we propose iterative outlier scissor (IOS), an iterative filtering-based robust aggregation rule with provable performance guarantees. Numerical experiments demonstrate the effectiveness of IOS. The code of simulation implementation is available at github.com/Zhaoxian-Wu/IOS.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2206.04568", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tsp/WuCL23", "doi": "10.1109/tsp.2023.3300629"}}, "content": {"source": {"pdf_hash": "8f809f2c2936157b96f989b327cf6fa5a7a5619c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2206.04568v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9d9de3d18b49be1cac678a13da104de8177f1c40", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8f809f2c2936157b96f989b327cf6fa5a7a5619c.txt", "contents": "\nByzantine-Resilient Decentralized Stochastic Optimization with Robust Aggregation Rules\n\n\nZhaoxian Wu \nTianyi Chen \nQing Ling \nByzantine-Resilient Decentralized Stochastic Optimization with Robust Aggregation Rules\n1Index Terms-Decentralized networkstochastic optimizationByzantine attacksrobust aggregation rule\nThis paper focuses on decentralized stochastic optimization in the presence of Byzantine attacks. During the optimization process, an unknown number of malfunctioning or malicious workers, termed as Byzantine workers, disobey the algorithmic protocol and send arbitrarily wrong messages to their neighbors. Even though various Byzantine-resilient algorithms have been developed for distributed stochastic optimization with a central server, we show that there are two major issues in the existing robust aggregation rules when being applied to the decentralized scenario: disagreement and non-doubly stochastic virtual mixing matrix. This paper provides comprehensive analysis that discloses the negative effects of these two issues, and gives guidelines of designing favorable Byzantineresilient decentralized stochastic optimization algorithms. Under these guidelines, we propose iterative outlier scissor (IOS), an iterative filtering-based robust aggregation rule with provable performance guarantees. Numerical experiments demonstrate the effectiveness of IOS. The code of simulation implementation is available at github.com/Zhaoxian-Wu/IOS.\n\nByzantine-Resilient Decentralized Stochastic Optimization with Robust Aggregation Rules\n\nZhaoxian Wu * , Tianyi Chen \u2020 , and Qing Ling * \u2021 Abstract-This paper focuses on decentralized stochastic optimization in the presence of Byzantine attacks. During the optimization process, an unknown number of malfunctioning or malicious workers, termed as Byzantine workers, disobey the algorithmic protocol and send arbitrarily wrong messages to their neighbors. Even though various Byzantine-resilient algorithms have been developed for distributed stochastic optimization with a central server, we show that there are two major issues in the existing robust aggregation rules when being applied to the decentralized scenario: disagreement and non-doubly stochastic virtual mixing matrix. This paper provides comprehensive analysis that discloses the negative effects of these two issues, and gives guidelines of designing favorable Byzantineresilient decentralized stochastic optimization algorithms. Under these guidelines, we propose iterative outlier scissor (IOS), an iterative filtering-based robust aggregation rule with provable performance guarantees. Numerical experiments demonstrate the effectiveness of IOS. The code of simulation implementation is available at github.com/Zhaoxian-Wu/IOS. Index Terms-Decentralized network, stochastic optimization, Byzantine attacks, robust aggregation rule.\n\n\nI. INTRODUCTION\n\nTraining large machine learning models relies on vast amounts of data to achieve accurate predictions. However, data is often dispersed among geographically distributed devices, or workers, and is subject to growing privacy concerns. To address this issue, distributed or decentralized stochastic optimization has been proposed as a means of privacy-preserving model training [1]- [3]. Distributed stochastic optimization involves a central server that exchanges messages, such as stochastic gradients or intermediate models. In contrast, in the decentralized scenario, workers exchange messages in a peer-to-peer manner. Decentralized topology offers better scalability and avoids communication bottlenecks that can occur in the distributed counterpart, as there is no dependence on a central server.\n\nHowever, distributed or decentralized stochastic optimization faces potential robustness issues due to the involvement of a vast number of workers. Data corruption, device malfunctioning, or malicious attacks can cause some workers to deviate from the training protocol, which we refer to as Byzantine attacks. The abnormal workers, called as Byzantine workers, are assumed to be omniscient and arbitrarily malicious, while their number and identities are unknown to the central server and the honest workers [4], [5]. Although Byzantine-resilient distributed stochastic optimization has been extensively studied in literature, much less attention has been paid to the decentralized scenario. The aim of this paper is to highlight challenges of Byzantine-resilient decentralized stochastic optimization, provide algorithmic guidelines, and propose an effective approach. Below we briefly review the literature.\n\nByzantine-resilient distributed stochastic optimization. To defend against Byzantine attacks in distributed stochastic optimization, most existing algorithms substitute the vulnerable mean aggregation rule in distributed stochastic gradient descent (SGD) with robust aggregation rules. Examples of such aggregation rules include coordinate-wise median [6], geometric median [7], [8], trimmed mean [9], Krum [10], Bulyan [11], FABA [12], centered clipping (CC) [13]. The key idea of these robust aggregation rules is to find a point that is close to the mean of the stochastic gradients transmitted from the honest workers. When the stochastic gradients from the honest workers are i.i.d. (independent and identically distributed) but subject to large noise, finding such a point is difficult. This fact motivates the applications of variance reduction techniques [14], [15] and momentum [16] to alleviate the stochastic gradient noise, and consequently, boost the Byzantine-resilience. Although these methods have been shown effective in the distributed scenario, directly extending them to the decentralized scenario does not yield similar results, as we will discuss in Section IV.\n\nWhen the data across the honest workers is non-i.i.d., the stochastic gradients from the honest workers are non-i.i.d. as well, making approximating their mean more challenging [17]. To overcome this heterogeneity issue, several techniques have been developed, such as robust stochastic model aggregation (RSA) [18] and resampling/bucketing techniques [19], [20].\n\nBesides those based on robust aggregation rules, some other algorithms have been proposed to identify Byzantine workers during the distributed training process [21], [22], followed by eliminating their influence.\n\nByzantine-resilient decentralized (stochastic) optimization. One of the most popular decentralized stochastic optimization algorithms is decentralized SGD [23], [24]. Different to distributed SGD in which the central server uses mean to aggregate stochastic gradients from all workers, in decentralized SGD, each worker uses weighted mean to aggregate optimization variables (models) from its neighbors, followed by a local SGD step. However, decentralized SGD fails even when one Byzantine worker exists. The Byzantine worker can disturb the training processes of its honest neighbors, and further affect those of all honest workers across the entire network, through the diffusion of polluted messages.\n\nBased on trimmed mean aggregation, the works of [25]- [28] study Byzantine-resilient decentralized deterministic optimization. Byzantine-resilient decentralized stochastic optimization is relatively less investigated. In [29], the equivalence between Byzantine-resilient agreement and stochastic optimization is highlighted, but the investigated network topology is confined to be complete. The work of [30] extends RSA from the distributed scenario to decentralized, and [31] extends distributed CC to decentralized self centered clipping (SCC). They require to set task-dependent hyper-parameters, which are hard to tune. The work of [32] proposes a two-stage method to filter Byzantine attacks, and [33] proposes a similarity-based method to aggregate neighboring messages. In addition, trimmed mean can be implemented in the stochastic scenario too. However, most of these methods fail to display favorable Byzantineresilience under certain attacks as we will show with numerical experiments. More importantly, principled guidelines to develop effective Byzantine-resilient decentralized stochastic optimization algorithms are still lacking.\n\nThe contributions of this paper are summarized as follows. C1) We show a wide class of existing robust aggregation rules in the distributed scenario fail to reach consensus in the decentralized scenario even when no Byzantine workers are present. In addition, many of them lead to nondoubly stochastic virtual mixing matrices (see Definition 1). We theoretically demonstrate that both issues enlarge the asymptotic learning error of a Byzantine-resilient decentralized stochastic optimization algorithm. C2) Leveraging the theoretical analysis, we provide guidelines to design a Byzantine-resilient decentralized stochastic optimization algorithm; that is the robust aggregation rules of honest workers should satisfy the following criteria: a small contraction constant (see Definition 1) and a doubly stochastic virtual mixing matrix. C3) Following these design guidelines, we propose a novel robust aggregation rule for decentralized stochastic optimization, termed as iterative outlier scissor (IOS), and validate its superior performance via extensive numerical experiments.\n\n\nII. BYZANTINE-RESILIENT DECENTRALIZED STOCHASTIC OPTIMIZATION\n\nConsider an undirected graph G := (N \u222a B, E), where N and B respectively denote the sets of honest and Byzantine workers, and E \u2286 (N \u222a B) \u00d7 (N \u222a B) denotes the set of edges without self-links. Note that the number and identities of the Byzantine workers are unknown to the honest workers. When an edge (n, m) \u2208 E exists, workers n and m are neighbors and can communicate with each other. For any worker n, denote the sets of its honest and Byzantine neighbors as N n := {m|(m, n) \u2208 E, m \u2208 N } and B n := {m|(m, n) \u2208 E, m \u2208 B}, respectively. Define the numbers of honest and Byzantine workers as N := |N | and B := |B|, respectively. For any worker n, define the numbers of its honest and Byzantine neighbors as N n = |N n | and B n = |B n |, respectively.\n\nWith these notations, the problem of Byzantine-resilient decentralized stochastic optimization can be described as finding an optimal solution of the following problem\nx * \u2208 arg min x\u2208R D f (x) := 1 N n\u2208N f n (x),(1)where f n (x) := E \u03ben [f n (x; \u03be n )].\nIn (1), f n (x) is the local aggregated cost function of honest worker n \u2208 N with optimization variable x \u2208 R D ; f n (x; \u03be n ) is the local cost function associated with random variable \u03be n , which follows the local distribution D n . The local distributions across the honest workers can be different. For notational convenience, we collect the random variables into \u03be := [\u03be n ] n\u2208N and define the overall cost function\nf (x; \u03be) := 1 N n\u2208N f n (x; \u03be n ),(2)\nsuch that (1) amounts to minimizing E \u03be [f (x; \u03be)].\n\nIn general, a decentralized algorithm to solve (1) contains three stages: computation, communication, and aggregation. Next we show the implementation of popular decentralized SGD algorithms [23], [24] when Byzantine workers are present. At time k, each honest worker n \u2208 N independently samples a random variable \u03be k n \u223c D n , computes the stochastic gradient \u2207f n (x k n ; \u03be k n ) using its current variable (also termed as model) x k n , and updates an intermediate variable x\nk+ 1 2 n by x k+ 1 2 n = x k n \u2212 \u03b1 k \u2207f n (x k n ; \u03be k n ),(3)\nwhere \u03b1 k > 0 is the step size. When Byzantine workers are absent, worker n sends x k+ 1 2 n to and receives x k+ 1 2 m from all neighbors, followed by weighted mean aggregation. However, in the presence of Byzantine attacks, each honest worker n \u2208 N cannot distinguish honest neighbors m \u2208 N n and Byzantine neighbors m \u2208 B n . If m \u2208 B, it can transmit an arbitrary vector to its neighbors. Byzantine worker m \u2208 B may send different messages to different honest neighbors n \u2208 N m . Thus, let x k+ 1 2 m,n denote the message that worker m sends to worker n at time k, asx\nk+ 1 2 m,n := x k+ 1 2 m , m \u2208 N , * , m \u2208 B,(4)\nwhere * denotes an arbitrary vector in R D . After that, each honest worker n \u2208 N takes a weighted average of the received messages to update its model x k+1\nn by x k+1 n = m\u2208Nn\u222aBn\u222a{n} w nmx k+ 1 2 m,n .(5)\nHere, w nm \u2265 0 is the weight that honest worker n assigns to its neighbor (or itself) m, with m\u2208Nn\u222aBn\u222a{n} w nm = 1.\n\nSuch an aggregation rule is vulnerable to Byzantine attacks. A Byzantine neighbor m \u2208 B n can arbitrarily manipulate x k+1 n , for example, making x k+1 n = 0 by nullifying the weighted average in (5) or blowing x k+1 n up by sending a message with infinitely large elements. Even worse, using n as an intermediate, m can indirectly affect the honest neighbors of n throughout the information diffusion process [30].\n\n\nAlgorithm 1 Byzantine-resilient Decentralized SGD\n\nRequire: step size \u03b1 k ; initialization x 0 n = x 0 for all n \u2208 N 1: for all k = 0, 1, 2, \u00b7 \u00b7 \u00b7 do 2:\n\nfor all honest workers n \u2208 N do 3: Compute stochastic gradient \u2207f n (x k n ; \u03be k n )\n\n\n4:\n\nCompute x\nk+ 1 2 n = x k n \u2212 \u03b1 k \u2207f n (x k n ; \u03be k n ) 5:\nSendx k n,m = x for all Byzantine workers n \u2208 B do 10:\n\nSendx k n,m = * to all neighbors m 11:\n\nend for 12: end for A remedy to address this issue is replacing the non-robust weighted average in (5) with an aggregation rule that is robust to Byzantine attacks. For each honest worker n \u2208 N , define its robust aggregation rule A n :\nR D \u00d7 R (Nn+Bn)\u00d7D \u2192 R D as x k+1 n = A n (x k+ 1 2 n , {x k+ 1 2 m,n } m\u2208Nn\u222aBn ).(6)\nThus, we modify decentralized SGD to its Byzantine-resilient variant, outlined in Algorithm 1. Specific choices of the robust aggregation rule will be discussed in the next section.\n\n\nIII. ROBUST AGGREGATION RULES AND OUR PROPOSAL\n\nThis section starts with a generic form of robust aggregation rules in existing Byzantine-resilient distributed stochastic optimization algorithms. Then we show empirically that direct extensions of some robust aggregation rules to the decentralized scenario may fail. Based on this, we introduce our robust aggregation rule termed as iterative outlier scissor (IOS).\n\n\nA. Generic Form of Robust Aggregation Rules\n\nWe consider the following generic form of the robust aggregation rule A n for an honest worker n \u2208 N , given by\nA n (x n , {x m,n } m\u2208Nn\u222aBn ) (7) := (1 \u2212 r n )A(x n , {x m,n } m\u2208Nn\u222aBn ) + r n x n ,\nwhere A is a base aggregator that is common among all honest workers and r n \u2208 [0, 1) is a worker-specific constant. Given all the available messages, the base aggregator outputs a Ddimensional vector. The output of the base aggregator might lose the information of x n , in which honest worker n should trust. Therefore, we consider the convex combination of the output of the base aggregator and x n , parameterized by r n . A small r n means that n trusts more in the base aggregator.\n\nWe will show in Appendix C that the existing Byzantineresilient decentralized (stochastic) optimization algorithms all fall in the generic form of (7). Besides, this generic form also allows us to extend various base aggregators to Byzantineresilient decentralized stochastic optimization.\n\nBelow we discuss a number of popular base aggregators in Byzantine-resilient stochastic optimization as examples. For notational convenience, in these examples we define the input of A as x 1 , . . . , x S , where S = N n + B n + 1. Coordinate-wise median (CooMed) returns the median for each coordinate d = 1, . . . , D as [6] [CooMed (x 1 , . . . ,\nx S )] d = Median ([x 1 ] d , . . . , [x S ] d ) , (8) in which [x] d refers to the d-th element of vector x.\nGeometric median (GeoMed) finds a point that minimizes the sum of distances to all input vectors, given by [7], [8] GeoMed (x 1 , . . . , x S ) = arg min x \u2212 x n . (9) In addition, if we can estimate the number of Byzantine workers (or a reasonable upper bound), denoted as q, we can also apply Krum [10], which returns the input vector that has the minimal distance to S \u2212 q \u2212 1 nearest vectors, given by However, below we empirically show the failure cases of these robust aggregation rules when being applied to the decentralized scenario; see the simulation results in Fig. 1 and more details in Section VII and Appendix E. Consider an Erdos-Renyi graph with 10 honest workers and 2 Byzantine workers. Each pair of workers are neighbors with probability of 0.7. Surprisingly, though CooMed, GeoMed and Krum have been proven successful in Byzantine-resilient distributed stochastic optimization, they perform poorly in the decentralized scenario. In contrast, our proposed IOS algorithm archives the highest accuracy and low disagreement measure. Next we introduce IOS first, explain why CooMed, GeoMed and Krum fail in Section IV, and reveal the generic design guidelines in Sections V and VI.\n\n\nB. Our Proposal: Iterative Outlier Scissor\n\nWe propose a novel robust aggregation rule, iterative outlier scissor (IOS), which iteratively discards outliers. IOS begins from constructing a doubly stochastic and symmetric mixing matrix W \u2208 R (N +B)\u00d7(N +B) in a decentralized manner with the existing techniques, such as the Metropolis-Hastings rule 1 [35]. For notational convenience, define the cumulative weight of worker n with respect to set U as\nW n (U) := m\u2208U w nm , U \u2286 N n \u222a B n \u222a {n}.(11)\nAt each time k, each honest worker n \u2208 N receives messages from the workers in N n \u222a B n \u222a {n}. With IOS, it iteratively discards q n messages, where q n is the estimated number of its Byzantine neighbors. At inner iteration i, each honest worker n \u2208 N maintains a trusted set U  (13). Then, it discards the model that is farthest away from x (i) avg except its own model in (14) and accordingly modifies the trusted set to U (i+1) n . This process repeats until q n models have been discarded, and outputs the weighted average of trusted models, denoted as x (qn) avg , with (16); see the summary in Algorithm 2. Under the notation of the generic robust aggregation rule in (7), the IOS aggregation at each honest worker n \u2208 N is\nA n (x n , {x m,n } m\u2208Nn\u222aBn ) = (1 \u2212 r n ) m\u2208U (qn ) n \\{n} w nmxm,n + r n x n ,(12)\nwhere r n = w nn /W n (U (qn) n ) and w nm := w nm /(W n (U (qn) ) \u2212w nn ), respectively.\n\nIOS is inspired by FABA [12], which iteratively discards models that are farthest away from the average model, not the weighted average in IOS. However, FABA was originally designed for the distributed scenario; extending it to decentralized often leads to a non-doubly stochastic virtual mixing matrix (see Definition 1), and consequently, comes with a large asymptotic learning error. We will show the limitation of the decentralized extension of FABA in Section VII. IV. CHALLENGES OF DESIGNING ROBUST AGGREGATION RULES IN DECENTRALIZED NETWORKS Although many existing base aggregators have been shown effective in Byzantine-resilient distributed stochastic optimization, directly extending them to decentralized faces two new challenges: (1) issue of disagreement; (2) issue caused by a non-doubly stochastic virtual mixing matrix. Before elaborating on these, we introduce some necessary concepts.\n\n\nA. Virtual Mixing Matrix and Contraction Constant\n\nWith the robust aggregation rule A n shown in (7), we hope that the Byzantine-resilient decentralized SGD would perform similarly to decentralized SGD without Byzantine workers. That is to say, for any honest worker n \u2208 N , we expect that the output of A n (x n , {x m,n } m\u2208Nn\u222aBn ) approximates a virtual weighted average of messages from its honest neighbors and itself, denoted as virtual weigthed averagex n := m\u2208Nn\u222a{n} w nm x m , (17) where w nm \u2265 0 represents the virtual weight 2 that honest worker n virtually assigns to its honest neighbor (or itself) Algorithm 2 Iterative outlier scissor on honest worker n Require: models {x n } \u222a {x m,n } m\u2208Nn\u222aBn ; weights {w nm } m\u2208Nn\u222aBn\u222a{n} ; estimate of number of Byzantine neighbors q n 1: Construct initial trusted set U (0) n = N n \u222a B n \u222a {n} 2: for i = 0, 1, \u00b7 \u00b7 \u00b7 , q n \u2212 1 do 3:\n\nCompute weighted average of models\nx (i) avg = 1 W n (U (i) n ) m\u2208U (i) n w nmxm,n(13) 4:\nChoose index\nm (i) = arg max m\u2208U (i) n \\{n} x m,n \u2212 x (i) avg(14)\n5:\n\nDiscard m (i) from trusted set\nU (i+1) n = U (i) n \\ {m (i) }(15)\n6: end for 7: Compute weighted average of trusted models\nx (qn) avg = 1 W n (U (qn) n ) m\u2208U (qn) n w nmxm,n(16)8: return x (qn) avg m.\nWe call a row stochastic matrix 3 W \u2208 R N \u00d7N a virtual mixing matrix if its (n, m)-th entry w nm \u2208 [0, 1] when m \u2208 N n \u222a {n} and w nm = 0, otherwise. Thus, we can associate the set of robust aggregation rules {A n } n\u2208N with a virtual mixing matrix that is essential to the later algorithm design and analysis. However, since the number and identities of the Byzantine workers are unknown, the outputs of {A n } n\u2208N are often biased from the weighted averagesx n . We further introduce a contraction constant to characterize the biases. The formal definitions are given as follows 4 .\n\nDefinition 1 (Virtual mixing matrix and contraction constant associated with {A n } n\u2208N ). Consider a matrix W \u2208 R N \u00d7N whose (n, m)-th entry w nm \u2208 [0, 1] if m \u2208 N n \u222a {n} and w nm = 0, otherwise. Further, m\u2208Nn\u222a{n} w nm = 1 for any n \u2208 N . Definex n := m\u2208Nn\u222a{n} w nm x m . If there exists a constant \u03c1 \u2265 0 for any n \u2208 N such that\nA n (x n , {x m,n } m\u2208Nn\u222aBn ) \u2212x n (18) \u2264\u03c1 max m\u2208Nn\u222a{n} x m \u2212x n ,\nthen W is the virtual mixing matrix and \u03c1 is the contraction constant associated with {A n } n\u2208N .\n\nRemark 1. For a set of robust aggregation rules {A n } n\u2208N , \u03c1 and W may not be unique, both of which affect convergence and asymptotic learning error. We will formally analyze those effects in Section V. Given {A n } n\u2208N , determining the best pair of \u03c1 and W is, however, beyond the scope of this paper. We leave it for the future work. B. Disagreement\n\nNext we demonstrate that several base aggregators developed for the distributed scenario in Section III may cause disagreement when extended to the decentralized scenario. That is to say, honest workers are never able to reach the same model in the worst case. Below we give an example.\n\nTwo-castle problem. Consider an undirected graph consisting of 6 honest workers and no Byzantine workers, as shown in Fig. 2. For simplicity, let D = 1 such that all the local models x n are scalars. The local cost functions and the local models at time k are respectively given by\nf n (x) = (x \u2212 z 1 ) 2 , n = 1, 2, 3, (x \u2212 z 2 ) 2 , n = 4, 5, 6, x k n = z 1 , n = 1, 2, 3, z 2 , n = 4, 5, 6,\nwhere z 1 = z 2 are two constants. We consider the deterministic case so that there is no random variable \u03be n in the argument of f n . Each worker n applies the robust aggregation rule A n in (7). For illustration purpose, we set the base aggregator A as CooMed to investigate the behavior of such a graph. Since \u2207f n (x k n ) = 0 for all workers n, it holds that x (3). Then according to (6) and (7), given any r n \u2208 [0, 1), it is straightforward to see that for worker 1, we have\nk+ 1 2 n = x k n \u2212 \u03b1 k \u2207f n (x k n ) = x k n according tox k+1 1 (19) =(1 \u2212 r n )CooMed(x k+ 1 2 1 , x k+ 1 2 2 , x k+ 1 2 3 , x k+ 1 2 5 , x k+ 1 2 6 ) + r n x k+ 1 2 n =(1 \u2212 r n )CooMed(z 1 , z 1 , z 1 , z 2 , z 2 ) + r n z 1 = z 1 = x k\n1 , which means that no update happens on worker 1. The same phenomenon can be observed on other workers, as\nx k+1 n =(1 \u2212 r n )CooMed(z 1 , z 1 , z 1 , z 2 , z 2 ) + r n z 1 (20) =x k n , n = 1, 2, 3, x k+1 n =(1 \u2212 r n )CooMed(z 1 , z 1 , z 2 , z 2 , z 2 ) + r n z 2 (21)\n=x k n , n = 4, 5, 6, implying that the workers cannot reach consensus forever.\n\nIn fact, a wide class of base aggregators used in Byzantineresilient distributed stochastic optimization, such as GeoMed and Krum, also suffer from the same disagreement issue. Therefore, directly extending the existing base aggregators from Byzantine-resilient distributed stochastic optimization to the decentralized scenario may not work.\n\n\nC. Non-doubly Stochastic Virtual Mixing Matrix\n\nObserve that Definition 1 in Section IV-A only requires the virtual mixing matrix W to be row stochastic, rather than doubly stochastic. For example, an equal-weight virtual mixing matrix with w nm = 1 Nn+1 corresponds to many robust aggregation rules, but for an incomplete network it is not doubly stochastic in general. We give an example to show the asymptotic learning error brought by a non-doubly stochastic virtual mixing matrix.\n\nConsider a set of 'ideal' robust aggregation rules {A n } n\u2208N with contraction constant \u03c1 = 0 in (18). That is to say, for honest worker n \u2208 N , the output of A n is exactly the weighted average of the messages from its honest neighbors and itself. The associated virtual mixing matrix W , however, is not necessarily doubly stochastic. We again illustrate its negative effect via a deterministic example. Consider the local function of honest worker n \u2208 N as\nf n (x) = 1 2 x \u2212 z n 2 ,(22)\nwhere z n \u2208 R D is a constant vector. Therefore, minimizing\n1 N n\u2208N f n (x) gives x * = 1 N n\u2208N z n .\nFor each honest worker n \u2208 N , the update (6) is given by\nx k+1 n = m\u2208Nn\u222aBn\u222a{n} w nm (x k m \u2212 \u03b1 k (x k m \u2212 z m )). (23)\nTo write it in a compact form, define\nX k :=[x k 1 , \u00b7 \u00b7 \u00b7 , x k n , \u00b7 \u00b7 \u00b7 , x k N ] \u2208 R N \u00d7D ,(24)Z :=[z 1 , \u00b7 \u00b7 \u00b7 , z n , \u00b7 \u00b7 \u00b7 , z N ] \u2208 R N \u00d7D .(25)\nWith these definitions, (23) becomes\nX k+1 = (1 \u2212 \u03b1 k )W X k + \u03b1 k W Z.(26)\nConsider a left eigenvector p of W corresponding to eigenvalue 1 (that is, p W = p ). Since p is nonnegative and nonzero [36,Theorem 8.3.1], we normalize it so that 1 p = 1 where 1 is a D-dimensional all-one vector. We multiply both sides of (26) by p and denote the weighted average as y k := n\u2208N p n x k n , then it holds that\ny k+1 = (1 \u2212 \u03b1 k )y k + \u03b1 k n\u2208N p n z n .(27)\nIf the step size \u03b1 k is properly chosen, there is a unique fixed point of (27), given by y \u221e := n\u2208N p n z n . In other words, Algorithm 1 actually optimizes a convex combination of the honest local cost functions. Only when p = 1 N 1, Algorithm 1 with a proper diminishing step size \u03b1 k can optimize the original problem (1). For this case, substituting p = 1 N 1 into p W = p , we can observe that W is also row stochastic, and consequently, W is doubly stochastic.\n\nThe fact revealed by the above example is not surprising. In Byzantine-free decentralized (stochastic) optimization, we often prefer doubly stochastic mixing matrices. Otherwise, we only minimize the convex combination of local cost functions, instead of the average [37]. Existing methods such as pushsum [38] and push-pull [39] cope with non-doubly stochastic mixing matrices with correction techniques, but they incur additional transmissions that increase the difficulty of defending against Byzantine attacks. For Byzantine-resilient decentralized deterministic optimization, [25] shows that a trimmed mean-based algorithm converges to an area determined by the convex combination of honest local cost functions. Our example is more general, covering various Byzantine-resilient decentralized algorithms that can be characterized by virtual mixing matrices W and contraction constants \u03c1.\n\nIn the next section, we will formally show the influence of disagreement and non-doubly stochastic virtual mixing matrix on the asymptotic learning error.\n\n\nV. CONVERGENCE AND ASYMPTOTIC LEARNING ERROR\n\nIn this section, we establish the convergence of the generic Byzantine-resilient decentralized SGD in Algorithm 1 and identify the factors that determine the asymptotic learning error. We begin with several assumptions.\nAssumption 1 (Lower boundedness). The aggregated cost function f (x) is lower bounded by f * ; i.e., f (x) \u2265 f * , \u2200x.\nAssumption 2 (L-smoothness). For each honest worker n \u2208 N , the local cost function f n (x; \u03be n ) is L-smooth.\n\nAssumption 3 (Bounded inner variation). For any honest worker n \u2208 N and x, the variation of its stochastic gradients with respect to its aggregated gradient is bounded by\nE \u03ben [ \u2207f n (x; \u03be n ) \u2212 \u2207f n (x) 2 ] \u2264 \u03b4 2 in .(28)\nAssumption 4 (Bounded outer variation). For any x \u2208 R D , the variation of the aggregated gradients at the honest workers with respect to the overall aggregated gradient is upperbounded by\nmax n\u2208N \u2207f n (x) \u2212 \u2207f (x) 2 \u2264 \u03b4 2 out .(29)\nAssumption 5 (Independent sampling). The stochastic gradients \u2207f n (x k n ; \u03be k n ) are independently sampled over times k = 0, 1, . . . and across honest workers n \u2208 N .\n\nAssumptions 1 and 2 are common in (stochastic) gradientbased non-convex optimization. Assumptions 3 and 4 bound the variation of stochastic gradients on each honest worker and the variation of aggregated gradients across the honest workers, respectively. Assumption 5 guarantees that the stochastic gradients are independent. They are standard in the analysis of stochastic optimization.\n\nTheorem 1 (Convergence). Consider the Byzantine-resilient decentralized SGD in Algorithm 1. Suppose that the robust aggregation rules {A n } n\u2208N satisfy (18). Withx k := 1 N n\u2208N x k n denoting the average 5 of all honest models at time k, define the disagreement measure H k as\nH k := 1 N n\u2208N x k n \u2212x k 2 .(30)Under Assumptions 1-5, if a constant step size \u03b1 k = \u03b1 \u2264 1 2 \u221a 3L is used, then it holds that 1 K K k=1 E[ \u2207f (x k ) 2 ] \u2264 2(f (x 0 ) \u2212 f * ) \u03b1K + 2\u03b1\u03b4 2 in L N (31) + 36(\u03c1 2 N + \u03c7 2 ) + 3\u03b1 2 L 2 \u03b1 2 K K k=1 E[H k ] + 96(\u03c1 2 N + \u03c7 2 )(\u03b4 2 in + \u03b4 2 out ),\nwhere the expectation is taken over all random variables \u03be 0 , \u03be 1 , \u00b7 \u00b7 \u00b7 , \u03be K , and \u03c7 2 :\n= 1 N W 1 \u2212 1 2 describes how non- doubly stochastic W is. Additionally, if the step size \u03b1 k is set as \u03b1 = O( N/(\u03b4 2 in K)), then it holds that 1 K K k=1 E[ \u2207f (x k ) 2 ] \u2264 O \u03b4 2 in N K + O L 2 K K k=1 E[H k ] + O(\u03c1 2 N + \u03c7 2 ) \u03b4 2 in N K k=1 E[H k ] + \u03b4 2 in + \u03b4 2 out .(32)\nThe proof is deferred to Section VIII. Theorem 1 asserts that the time-averaged squared gradient norm, computed on the averages of all honest modelsx k := 1 N n\u2208N x k n , is upperbounded by the summation of the three terms at the right-hand side (RHS) of (32). Among them, the first term vanishes at the rate of O( 1 \u221a K ) which also appears in the convergence analysis of distributed/decentralized SGD. Observe that when the accumulated expected disagreement measure\nK k=1 E[H k ]\nin the second term is unbounded, the third term is unbounded too, and the algorithm is not Byzantine-resilient.\n\nTherefore, at this stage, we make a hypothesis that the accumulated expected disagreement measure is upper-bounded by a constant asymptotically. In Theorem 2, we will discuss when this hypothesis holds true. With K \u2192 \u221e, (32) gives the asymptotic learning error of the Byzantine-resilient decentralized SGD in Algorithm 1, defined as lim sup\nK\u2192\u221e 1 K K k=1 E[ \u2207f (x k ) 2 ](33)\u2264O( \u03c1 2 N Est. Err. + \u03c7 2 Mix. Err. ) \u03b4 2 in N lim sup K\u2192\u221e K k=1 E[H k ]\nCon. Err.\n+\u03b4 2 in + \u03b4 2 out ,\nwhere the asymptotic learning error is determined by three factors associated with robust aggregation rules: estimation, mixing, and consensus errors. Essentially, the asymptotic learning error arises from the inaccurate estimation of the overall aggregated gradient. This is also the case in the decentralized scenario. Below we discuss three errors contributing to the asymptotic learning error. Estimation error. The error related to \u03c1 2 N reflects the bias caused by Byzantine attacks and robust aggregation rules. When the Byzantine workers are absent and a proper aggregation rule is used, this error turns to 0. Mixing error. The error related to \u03c7 2 comes from the nondoubly stochastic virtual mixing matrix W . This error becomes 0 when W is doubly stochastic. Consensus error. The effect of the consensus error, or formally the asymptotic accumulated expected disagreement, on the asymptotic learning error is added by the stochastic gradient noise (inner variation \u03b4 2 in ), the data heterogeneity (outer variation \u03b4 2 out ), and is further amplified by \u03c1 2 N + \u03c7 2 . The estimation error also appears in the distributed scenario, whereas the mixing and consensus errors are unique in the decentralized scenario.\n\nRemark 2. The effect of the inner variation \u03b4 2 in on the asymptotic learning error could be removed by variance reduction [14], [15] and momentum [16] techniques, via replacing the gradient descent step in line 4 of Algorithm 1 with a corrected gradient descent update.\n\nRecall that the asymptotic learning error in (33) is meaningful only when the accumulated expected disagreement measure K k=1 E[H k ] is upper-bounded by a constant asymptotically. However, this hypothesis may not hold in general -we have shown in Section IV-B that H k can be a constant for many existing robust aggregation rules even without Byzantine workers. Below, we show when this hypothesis holds true.\n\nApparently, if some honest workers cannot communicate with others, reaching consensus is impossible. Therefore, it is natural to make an assumption on the network connectivity. Consider a (possibly directed) graph G W := (N , E W ) that corresponds to the virtual mixing matrix W , with (n, m) \u2208 E W if and only if w nm > 0. Thus, G W is a subgraph of G. Now we introduce an assumption about G W and W .\n\nAssumption 6 (Network connectivity). The graph G W is strongly connected, which means that for any pair n, m \u2208 N , there exists at least one directed path between them. In addition, \u03bb := 1 \u2212 (I \u2212 1 N 11 )W 2 > 0, where \u00b7 is the matrix spectral norm.\n\nThe following theorem establishes the upper bound of the expected disagreement measure E[H k ] when the contraction constant \u03c1 is sufficiently small. \nk = \u03b1 \u2264 1 3L (2 \u2212 \u03c9)\u03c9 2 /(6 \u2212 2\u03c9) is used then it holds that E[H k ] \u2264\u03b1 2 \u2206(\u03b4 2 in + \u03b4 2 out )(35)\nwhere E is taken over all \u03be 0 , \u03be 1 , \u00b7 \u00b7 \u00b7 , \u03be k and the constant \u2206 :\n= 12(1\u2212\u03c9) \u03c9 3\n. Additionally, if the step size \u03b1 k is set as \u03b1 = O( N/(\u03b4 2 in K)), the consensus error is bounded by\nlim sup K\u2192\u221e K k=1 E[H k ] \u2264 O \u2206N (\u03b4 2 in + \u03b4 2 out ) \u03b4 2 in .(36)\nThe proof is deferred to Appendix A. According to Theorem 2, if we choose the same step size as in Theorem 1, the expected disagreement measure is in the order of O( 1 K ), such that the consensus error is upper-bounded. Consequently, the asymptotic learning error in (33) is upper-bounded by lim sup\nK\u2192\u221e 1 K K k=1 E[ \u2207f (x k ) 2 ](37)\n\u2264O(\u03c1 2 N + \u03c7 2 ) (1 + \u2206) (\u03b4 2 in + \u03b4 2 out ). Remark 3. The bound in (35) recovers the traditional results of Byzantine-free decentralized deterministic optimization [40] and stochastic optimization [41] when \u03c1 = 0. In this case, the asymptotic disagreement is O(\u03b1 2 (\u03b4 2 in + \u03b4 2 out )), which depends on the step size \u03b1 and the sum of inner and outer variations \u03b4 2 in + \u03b4 2 out . Theorem 2 also shows the difficulty of reaching consensus under Byzantine attacks, even when the network topology and the virtual mixing matrix are both perfect. In a fully connected network and with an equal-weight virtual mixing matrix W , we have \u03bb = 1, \u03c9 = 1 \u2212 8\u03c1 \u221a N , and thus\nE[H k ] \u2264 O \u03c1 (1 \u2212 8\u03c1 \u221a N ) 3 ,(38)\nimplying that the asymptotic disagreement measure could be large when \u03c1 is close to 1 8 \u221a N .\n\n\nVI. GUIDELINES OF DESIGNING ROBUST AGGREGATION RULES FOR A DECENTRALIZED NETWORK\n\nAccording to the analysis in Section V, we give the guidelines of designing robust aggregation rules that are suitable for a decentralized network. Further, we show that the design of IOS exactly follows the guidelines.\n\n\nA. Design Guidelines for Robust Aggregation Rules\n\nTheorem 1 shows that to reduce the estimation and mixing errors, the robust aggregation rules should have a small contraction constant \u03c1 and a doubly stochastic virtual mixing matrix W . With a small \u03c1, we can also bound the consensus error by (36) in Theorem 2. Thus, we design robust aggregation rules to satisfy the following conditions. To better understand Definition 2, we compute the contraction constant \u03c1 in a fully connected network for several existing robust aggregation rules extended to the decentralized scenario; see Table I. Therein, \u00b5 := B N +B is the proportion of Byzantine workers. When \u00b5 is sufficiently small, FABA, trimmed mean (TriMean) and CC/SCC have sufficiently small contraction constants \u03c1 that may satisfy (34). However, those of CooMed, GeoMed and Krum are always greater than 1. Therefore, CooMed, GeoMed and Krum are not RCA. This is consistent with the fact that they have the disagreement issue in Section IV-B. For Definition 3, we consider a general incomplete network and check whether the above robust aggregation rules are RDSA; see Table I.\n\nFrom Table I, we observe that most existing decentralized robust aggregation rules and those extended from the distributed scenario to decentralized are not necessarily RCA or RDSA. The only exception is SCC [31]. However, SCC needs to set task-dependent and time-varying thresholds, whose choices rely on the estimates of the true models; see the discussion in Appendix C. In practice, these thresholds are often set as constants, which explains why SCC may not perform well as is indicated by the theory. In contrast, for IOS, the parameters {q n } n\u2208N stand for the estimates of the O(1 + \u00b5) \u00d7 \u00d7 GeoMed [7], [8] O( 1\u2212\u00b5 1\u22122\u00b5 ) \u00d7 \u00d7 Krum [10] O\n(N + B \u2212 q) \u00d7 \u00d7 CC [16] O(\u00b5) \u221a \u00d7 FABA [12] O( \u00b5 1\u22123\u00b5 ) \u221a \u00d7 TriMean [9], [25]-[27] O( \u00b5(1\u2212\u00b5) (1\u22122\u00b5) 2 ) \u221a \u00d7 SCC [31] O(\u00b5) \u221a \u221a IOS (ours) O( \u00b5 1\u22123\u00b5 ) \u221a \u221a\nnumbers of Byzantine neighbors, which are relatively easy to obtain. In the numerical experiments, we will show that IOS outperforms SCC under most Byzantine attacks. Therefore, we provide the following guidelines for designing decentralized stochastic algorithms.\n\nGuidelines: For a decentralized stochastic optimization algorithm to be Byzantine-resilient, the set of aggregation rules {A n } n\u2208N should be RCA and RDSA.\n\nWe summarize the results when a set of robust aggregation rules {A n } n\u2208N are both RCA and RDSA in the next corollary. \nlim sup K\u2192\u221e K k=1 E[H k ] \u2264 O \u2206N (\u03b4 2 in + \u03b4 2 out ) \u03b4 2 in ,(39)\nand the asymptotic learning error is upper-bounded by lim sup\nK\u2192\u221e 1 K K k=1 E[ \u2207f (x k ) 2 ](40)\n\u2264O(\u03c1 2 N ) (1 + \u2206) (\u03b4 2 in + \u03b4 2 out ). When the robust aggregation rules {A n } n\u2208N are RCA, \u03c1 < \u03bb 8 \u221a N . In consequence, the asymptotic learning error in (40) is in the order of O(\u03b4 2 in + \u03b4 2 out ), which matches the bound of the distributed scenario [14].\n\n\nB. IOS is both RCA and RDSA\n\nBefore showing that the robust aggregation rules generated by IOS are simultaneously RCA and RDSA, we observe that each honest worker n \u2208 N discards q n models from the received ones, along with the weights w nm . We define a set that includes the neighbors with the largest q n weights, as \n\nAccording to the notation in (11), W n (U max n ) accumulates the largest q n weights that worker n assigns to its neighbors.\n\nThe following theorem shows the contraction factor and the virtual mixing matrix associated with IOS. The analysis holds for both complete and general incomplete networks.\n\nTheorem 3 (Contraction factor and virtual mixing matrix of IOS). If q n is chosen such that B n \u2264 q n and W n (U max n ) < 1 3 , then for the robust aggregation rules generated by IOS in Algorithm 2, the associated virtual mixing matrix W is doubly stochastic and its (n, m)-th entry is given by\nw nm = w nn + b\u2208Bn w nb , m = n, w nm , m = n,(42)\nwhile the contraction constant is bounded by\n\u03c1 \u2264 max n\u2208N 12W n (U max n ) 1 \u2212 3W n (U max n )\n.\n\nTherefore, the robust aggregation rules generated by IOS in Algorithm 2 are RDSA, and further are RCA when\nW n (U max n ) < \u03c1 * 12 + 3\u03c1 * , \u2200n \u2208 N .(44)\nTheorem 3 implies that when W n (U max n ) is sufficiently small, the virtual mixing matrix W associated with IOS is doubly stochastic, while the contraction factor \u03c1 is in the order of O(max n\u2208N W n (U max n )) and can be sufficiently small as well. To understand the condition W n (U max n ) < 1 3 , consider a complete network and let w nm = 1 Nn+Bn+1 . In this circumstance, the condition is equivalent to qn Nn+Bn+1 < 1 3 , meaning that the proportion of Byzantine workers cannot exceed 1 3 . If we further let q n = B n , (43) becomes \u03c1 = O( \u00b5 1\u22123\u00b5 ), which is the result listed in Table I. For a general incomplete network,\nw nm = min 1 Nn+Bn+1 , 1 Nm+Bm+1 \u2264 1\nNn+Bn+1 if W is constructed by the Metropolis-Hastings rule [35]. Therefore, if q n = B n and Bn Nn+Bn+1 , the portion of Byzantine neighbors is smaller than 1 3 , then W n (U max n ) \u2264 Bn Nn+Bn+1 < 1 3 . Having W n (U max n ) < 1 3 guarantees that the robust aggregation rules generated by IOS are RDSA. But to guarantee that they are also RCA, we further require \u03c1 \u2264 \u03c1 * according to (34), meaning that W n (U max n ) < \u03c1 * 12+3\u03c1 * . To meet this requirement, the numbers of Byzantine neighbors must be limited and the initial mixing matrix W must be properly chosen. Below we give two examples in which (44) holds. Example 1. When there are no Byzantine workers, we can let q n = B n = 0. Therefore, W n (U max n ) = 0 and (44) is satisfied. In this case, \u03c1 = 0.\n\nExample 2. For an Erdos-Renyi graph, \u03bb = \u0398(1) and \u03c1 * \u2264 O( 1 \u221a N ); see [42], [43]. As we have discussed below Theorem 3, W n (U max\nn ) \u2264 O( Bn Nn+Bn+1 ) O( B N +B+1 ). Thus, (44) is satisfied if B \u2264 O( \u221a N ). In this case, \u03c1 \u2264 O( B N +B+1 ). Generally speaking, if a graph has \u03bb = \u0398(N \u2212\u03b5 ) where 0 \u2264 \u03b5 < 1 2 , (44) is satisfied if B \u2264 O(N 1 2 \u2212\u03b5 ).\n\nVII. NUMERICAL EXPERIMENTS\n\nIn this section, we evaluate the performance of the proposed IOS on the softmax regression task. The dataset is MNIST, which contains 60,000 images of handwritten digits from 0 to 9. The local data distributions are i.i.d. or non-i.i.d. across the workers. In the i.i.d. case, all images in each class are   We are going to compare decentralized SGD equipped with various aggregation rules: weighted mean (WeiMean) that is not Byzantine-resilient, coordinate-wise median (CooMed), geometric median (GeoMed), Krum, trimmed mean (TriMean) [25]- [28], similarity-based reweighting (SimRew) [33], decentralized RSA (DRSA) [30] with penalty parameters 0.001 and 0.5 in the i.i.d. and non-i.i.d. cases respectively, centered clipping (CC) [13], [16] with radii 0.1 and 0.3 in the i.i.d. and non-i.i.d. cases respectively, self centered clipping (SCC) [31] with radii 0.1 and 0.3 in the i.i.d. and non-i.i.d. cases respectively, FABA, and IOS. For aggregation rules that need W , the mixing matrix of the entire network (including the Byzantine workers), we construct it by the Metropolis-Hastings rule [35]. As a baseline, we also let workers run SGD locally without any communication, marked by no communication (no comm), which is not affected by Byzantine attacks.\n\nWe test the performance of the compared algorithms under four popular Byzantine attacks: Gaussian, sign-flipping 6 , isolation, sample-duplicating, and a little is enough (ALIE) [45]. For Gaussian, Byzantine worker b \u2208 B n generates its messagex k b,n from a Gaussian distribution with mean x k n := ( m\u2208Nn w nm x k m )/( m\u2208Nn w nm ) and variance 1. For sign-flipping, Byzantine worker b \u2208 B n sets its message asx k b,n = \u2212x k n . For isolation, Byzantine worker b \u2208 B n sends x k b,n = (x k n \u2212 m\u2208Nn w nmx k n )/( m\u2208Bn w nm ) so that the messages received by honest worker n sum up to x k n , which is equivalent to having no communication when weighted mean is used. For sample-duplicating, Byzantine worker b \u2208 B n choosesx k b,n from {x k m } m\u2208Nn at random. We first test their performance in the two-castle graph with 10 honest workers and 2 Byzantine workers in Fig. 3. In the numerical experiments, accuracy (Acc) and disagreement measure (DM) are used as performance measures. Acc stands for the average accuracy of the honest workers on the test images, and DM is defined as (30). The detailed results for the i.i.d. and non-i.i.d. cases are listed in Tables II and III, respectively. According to these experimental results, we have the following conclusions.\n\nMost of the existing aggregation rules fail to work well in the decentralized network. We can observe that CooMed, GeoMed, Krum work well for the i.i.d. case but become unsatisfactory for the non-i.i.d. case. The reasons are that they are essentially designed for i.i.d. data distribution and that they have relatively large contraction constants \u03c1 even in a complete network (cf. Table I). TriMean is relatively sensitive to isolation attacks because any honest worker n discards at least 2B n neighboring messages, among which at least B n are honest. Discarding too many neighboring messages makes honest workers to be isolated easily. The methods based on penalizing differences with neighboring messages like DRSA and clipping neighboring messages with large magnitudes like CC and SCC, are sensitive to sign-flipping attacks because the directions of neighboring workers become critical.\n\nVariation is a critical factor to affect robustness. For the i.i.d. case, most existing aggregation rules achieve acceptable performance. Gaussian and sample-duplicating attacks even do not have any negative influence on some of them. However, their performance degrades sharply for the non-i.i.d. case, because the increased outer variation begins to play a prominent role, which corroborates with our analysis in Theorem 1.\n\nCollaboration benefits optimization even in the presence of Byzantine workers. For the i.i.d. case, honest workers have similar training images and consequently, similar local minima. No collaboration does not matter too much. However, for the non-i.i.d. case, collaboration becomes extremely important. Even when the network is attacked by Byzantine workers, SGD equipped with properly designed robust aggregation rules still outperform SGD without any communication.\n\nIOS works well in all numerical experiments. Observe that IOS achieves similar performance as if the Byzantine workers are absent and WeiMean is used. It outperforms all other aggregation rules, but is very close to FABA. Actually, in this two-castle graph, the virtual mixing matrix of FABA is doubly stochastic too, and thus IOS is equivalent to FABA.\n\nNon-doubly stochastic virtual mixing matrix has negative effect. To illustrate the influence of non-doubly stochastic virtual mixing matrix, we construct an octopus graph as shown in Fig. 3. The virtual mixing matrix W associated with FABA now becomes non-doubly stochastic if we also choose the weight w nm = 1 Nn+Bn+1 for n \u2208 N and m \u2208 N n \u222a B n \u222a {n} as we have done in the two-castle graph. Two pairs of algorithms with doubly and non-doubly stochastic virtual mixing matrices are compared: CC/SCC and FABA/IOS. The results listed in Table IV demonstrate that without the doubly stochastic structures in the virtual mixing matrices, CC and FABA achieve lower accuracy compared with SCC and IOS. This observation validates the theoretical result that the asymptotic learning error depends on \u03c7 2 , which describes how non-doubly stochastic W is.\n\nIOS is resilient to a large fraction of Byzantine workers. We also generate larger Erdos-Renyi graphs with 20 honest workers and varying Byzantine workers. Each pair of workers are neighbors with the probability of 0.7. We consider noni.i.d. local data distributions. Fig. 4 shows that IOS constantly performs well when the number of Byzantine workers is less than 7, and deteriorates thereafter. These results demonstrate that IOS is resilient to a large fraction of Byzantine workers.\n\nMore numerical experiments. To further demonstrate the effectiveness of IOS, we perform more numerical experiments in an Erdos-Renyi graph. The conclusions are consistent to those we have reached above. We leave them to Appendix E.\n\n\nVIII. PROOF OF THEOREM 1\n\nIn this section, we provide the proof of Theorem 1. By the L-smoothness of function f n in Assumption 2, f is also L-smooth and we have\nE \u03be k [f (x k+1 )] \u2264f (x k ) + E \u03be k [ \u2207f (x k ),x k+1 \u2212x k ] + L 2 E \u03be k [ x k+1 \u2212x k 2 ].(45)\nWith the equality x, y = 1 2 x + y 2 \u2212 1 2 x 2 \u2212 1 2 y 2 , we can rewrite the second term at the RHS of (45) as\nE \u03be k [ \u2207f (x k ),x k+1 \u2212x k ](46)=\u03b1E \u03be k [ \u2207f (x k ), \u2207f (x k ; \u03be k ) \u2212 \u2207f (x k ) + 1 \u03b1 (x k+1 \u2212x k ) ] = \u03b1 2 E \u03be k [ \u2207f (x k ; \u03be k ) + 1 \u03b1 (x k+1 \u2212x k ) 2 \u2212 \u03b1 2 \u2207f (x k ) 2 ] \u2212 \u03b1 2 E \u03be k [ \u2207f (x k ; \u03be k ) \u2212 \u2207f (x k ) + 1 \u03b1 (x k+1 \u2212x k ) 2 ].\nThe third term at the RHS of (45) is bounded by\nE \u03be k [ x k+1 \u2212x k 2 ] \u2264 2\u03b1 2 E \u03be k [ \u2207f (x k ; \u03be k )\u2212\u2207f (x k ) 2 ](47)+ 2\u03b1 2 E \u03be k [ \u2207f (x k ; \u03be k ) \u2212 \u2207f (x k ) + 1 \u03b1 (x k+1 \u2212x k ) 2 ] \u2264 2\u03b1 2 \u03b4 2 in N +2\u03b1 2 E \u03be k [ \u2207f (x k ; \u03be k )\u2212\u2207f (x k )+ 1 \u03b1 (x k+1 \u2212x k ) 2 ],\nwhere the second inequality is from Assumptions 3 and 5.\n\nAs \u03b1 \u2264 1 2L , substituting (46) and (47) into (45) yields\nE \u03be k [f (x k+1 )]\n(48)\n\u2264 f (x k ) + \u03b1 2 E \u03be k [ \u2207f (x k ; \u03be k ) + 1 \u03b1 (x k+1 \u2212x k ) 2 ] \u2212 \u03b1 2 (1 \u2212 2\u03b1L)E \u03be k [ \u2207f (x k ; \u03be k ) \u2212 \u2207f (x k ) + 1 \u03b1 (x k+1 \u2212x k ) 2 ] \u2212 \u03b1 2 \u2207f (x k ) 2 + \u03b1 2 \u03b4 2 in L N \u2264 f (x k ) + \u03b1 2 E \u03be k [ \u2207f (x k ; \u03be k ) + 1 \u03b1 (x k+1 \u2212x k ) 2 ] \u2212 \u03b1 2 \u2207f (x k ) 2 + \u03b1 2 \u03b4 2 in L N .\nAccording to the update rule of (6), we expand the second term at the RHS of (48) as Denote the squared 2 norms of the three terms at the RHS of (49) as T 1 , T 2 and T 3 , respectively. We establish their upper bounds as follows.\n\u2207f (x k ; \u03be k ) + 1 \u03b1 (x k+1 \u2212x k ) (49) =\u2207f (x k ; \u03be k ) + 1 \u03b1N n\u2208N (A n (x k+ 1 2 n , {x k+ 1 2 m,n } m\u2208Nn\u222aBn ) \u2212x k ) = \u2207f (x k ; \u03be k ) \u2212 1 N n\u2208N \u2207f n (x k n ; \u03be k n ) + 1 \u03b1N n\u2208N x k+ 1 2 n \u2212x k + \u03b1 N m\u2208N \u2207f m (x k m ; \u03be k m ) + 1 \u03b1N n\u2208N A n (x\nUpper bound of T 1 . For T 1 , it holds that\nT 1 :=E \u03be k [ \u2207f (x k ; \u03be k ) \u2212 1 N n\u2208N \u2207f n (x k n ; \u03be k n ) 2 ] (50) =E \u03be k [ 1 N n\u2208N (\u2207f n (x k ; \u03be k n ) \u2212 \u2207f n (x k n ; \u03be k n )) 2 ] \u2264 1 N n\u2208N E \u03be k n [ \u2207f n (x k ; \u03be k n ) \u2212 \u2207f n (x k n ; \u03be k n ) 2 ].\nFurther, according to Assumption 2, we have\n\u2207f n (x k ; \u03be k n ) \u2212 \u2207f n (x k n ; \u03be k n ) 2 \u2264 L 2 x k \u2212 x k n 2 .(51)\nWith this inequality, T 1 can be bounded by\nT 1 \u2264 L 2 N n\u2208N x k n \u2212x k 2 .(52)\nUpper bound of T 2 . Byx k+ 1 2 =x k \u2212 \u03b1 N n\u2208N \u2207f n (x k n ; \u03be k n ), we can rewrite the second term at the RHS of (49) as\n1 \u03b1N n\u2208N x k+ 1 2 n \u2212x k + \u03b1 N m\u2208N \u2207f m (x k m ; \u03be k m ) (53) = 1 \u03b1N n\u2208N x k+ 1 2 n \u2212x k+ 1 2 .\nStacking all local models in X as (24) and applying Cauchy-Schwarz inequality, we have\nT 2 := 1 \u03b1N n\u2208N x k+ 1 2 n \u2212x k+ 1 2 2 (54) = 1 \u03b1N 1 (W X k+ 1 2 \u2212 1 N 11 X k+ 1 2 ) 2 = 1 \u03b1 2 N 2 (1 W \u2212 1 )(X k+ 1 2 \u2212 1 N 11 X k+ 1 2 ) 2 \u2264 1 \u03b1 2 N 2 W 1 \u2212 1 2 X k+ 1 2 \u2212 1 N 11 X k+ 1 2 2 F = \u03c7 2 \u03b1 2 N n\u2208N x k+ 1 2 n \u2212x k+ 1 2 2 .\nUpper bound of T 3 . From the contraction property of robust aggregation rules {A n } n\u2208N in (18), T 3 can be bounded by \nT 3 := 1 \u03b1N n\u2208N A n (x k+ 1 2 n , {x k+ 1 2 m,n } m\u2208Nn\u222aBn ) \u2212x k+ 1 2 n 2 \u2264 1 \u03b1 2 N n\u2208N A n (xT 3 \u2264 4\u03c1 2 \u03b1 2 max n\u2208N x k+ 1 2 n \u2212x k+ 1 2 2 .(57)\nNote that x \nE \u03be k [ \u2207f (x k ; \u03be k ) + 1 \u03b1 (x k+1 \u2212x k ) 2 ](58)\u22643E \u03be k [T 1 ] + 3E \u03be k [T 2 ] + 3E \u03be k [T 3 ] \u2264 3L 2 N n\u2208N x k n \u2212x k 2 + 3\u03c7 2 \u03b1 2 N E \u03be k [ n\u2208N x k+ 1 2 n \u2212x k+ 1 2 2 ] + 12\u03c1 2 \u03b1 2 E \u03be k [max n\u2208N x k+ 1 2 n \u2212x k+ 1 2 2 ].\nPlugging (100) with v = 1 2 in Lemma 2 into (58) yields\nE \u03be k [ \u2207f (x k ) + 1 \u03b1 (x k+1 \u2212x k ) 2 ] (59) \u2264 36\u03c1 2 \u03b1 2 max n\u2208N x k n \u2212x k 2 + ( 9\u03c7 2 \u03b1 2 N + 3L 2 N ) n\u2208N x k n \u2212x k 2 + 24(4\u03c1 2 N + \u03c7 2 )(\u03b4 2 in + \u03b4 2 out ) \u2264( 36\u03c1 2 N \u03b1 2 + 9\u03c7 2 \u03b1 2 + 3L 2 )H k + 24(4\u03c1 2 N + \u03c7 2 )(\u03b4 2 in + \u03b4 2 out )\n. Reorganizing the terms in (48) and substituting (59) lead to\n\u2207f (x k ) 2 (60) \u2264 2E \u03be k [f (x k ) \u2212 f (x k+1 )] \u03b1 + 2\u03b1\u03b4 2 in L N + E \u03be k [ \u2207f (x k ) + 1 \u03b1 (x k+1 \u2212x k ) 2 ] \u2264 2E \u03be k [f (x k ) \u2212 f (x k+1 )] \u03b1 + 2\u03b1\u03b4 2 in L N + ( 36\u03c1 2 N \u03b1 2 + 9\u03c7 2 \u03b1 2 + 3L 2 )H k + 24(4\u03c1 2 N + \u03c7 2 )(\u03b4 2 in + \u03b4 2 out )\n. Taking expectation and averaging (60) over k = 1, \u00b7 \u00b7 \u00b7 , K give\n1 K K k=1 E[ \u2207f (x k ) 2 ] (61) \u2264 2E[f (x 0 ) \u2212 f (x k+1 )] \u03b1K + 2\u03b1\u03b4 2 in L N + 24(4\u03c1 2 N + \u03c7 2 )(\u03b4 2 in + \u03b4 2 out ) + 36\u03c1 2 N + 9\u03c7 2 + 3\u03b1 2 L 2 1 \u03b1 2 K K k=1 E[H k ] \u2264 2(f (x 0 ) \u2212 f * ) \u03b1K + 2\u03b1\u03b4 2 in L N + 96(\u03c1 2 N + \u03c7 2 )(\u03b4 2 in + \u03b4 2 out ) + (36(\u03c1 2 N + \u03c7 2 ) + 3\u03b1 2 L 2 ) 1 \u03b1 2 K K k=1 E[H k ],\nwhich completes the proof. IX. CONCLUSIONS This paper deals with the Byzantine-resilient decentralized stochastic optimization problem. We reveal the intrinsic challenges arising from the distributed scenario to decentralized, and give design guidelines of developing provably Byzantineresilient algorithms. Following these guidelines, we devise a novel set of robust aggregation rules, IOS, and demonstrate its superior performance with numerical experiments.\n\n\nAPPENDIX A PROOF OF THEOREM 2\n\nBefore bounding the disagreement measure, we introduce its matrix form. Following the same notation of (24) in Section IV, ( 1 N 1 X k ) = 1 N n\u2208N x k n =x k is the average of all honest models, and X k \u2212 1 N 11 X k = (I \u2212 1 N 11 )X k is the stacked disagreement matrix whose norms represent the disagreement among the honest workers. With these notations, we can write the disagreement measure H k in (30) as\nH k = 1 N n\u2208N x k n \u2212x k 2 = 1 N (I \u2212 1 N 11 )X k 2 F ,(62)\nwhere \u00b7 F is the Frobenius norm.\n\nWith these preparations, we begin our proof of Theorem 2.\n\nProof. For any u \u2208 (0, 1), it holds that\n(I \u2212 1 N 11 )X k+1 2 F (63) \u2264 1 1 \u2212 u (I \u2212 1 N 11 )W X k+ 1 2 2 F + 2 u X k+1 \u2212 W X k+ 1 2 2 F + 2 u 1 N 11 X k+1 \u2212 1 N 11 W X k+ 1 2 2 F ,\nwhere the inequality comes from x+y +z 2\nF \u2264 1 1\u2212u x 2 F + 2 u y 2 F + 2 u z 2 F .\nSince W is row stochastic, it holds that W 1 = 1, with which the first term at the RHS of (63) can be bounded by\n(I \u2212 1 N 11 )W X k+ 1 2 2 F (64) = (I \u2212 1 N 11 )W (I \u2212 1 N 11 )X k+ 1 2 2 F \u2264 (I \u2212 1 N 11 )W 2 (I \u2212 1 N 11 )X k+ 1 2 2 F =(1 \u2212 \u03bb) (I \u2212 1 N 11 )X k+ 1 2 2\nF . With the contraction property of robust aggregation rules {A n } n\u2208N in (18), we can bound the second term at the RHS of (63) by For the third term at the RHS of (63), it holds that\nX k+1 \u2212 W X k+ 1 2 2 F (65) = n\u2208N A n (x1 N 11 X k+1 \u2212 1 N 11 W X k+ 1 2 2 F (66) \u2264 1 N 11 2 X k+1 \u2212 W X k+ 1 2 2 F \u22644\u03c1 2 N (I \u2212 1 N 11 )X k+ 1 2 2 F ,\nwhere the second inequality uses 1 N 11 2 = 1 and (65). Substituting (64)-(66) back into (63), we have\n(I \u2212 1 N 11 )X k+1 2 F (67) \u2264 1 \u2212 \u03bb 1 \u2212 u + 16\u03c1 2 N u (I \u2212 1 N 11 )X k+ 1 2 2 F .\nTaking expectation over \u03be k and applying Lemma 2, we have\nE \u03be k [H k+1 ](68)\u2264 1 \u2212 \u03bb 1 \u2212 u + 16\u03c1 2 N u 1 1 \u2212 v + 6\u03b1 2 L 2 v H k + 1 \u2212 \u03bb 1 \u2212 u + 16\u03c1 2 N u 4\u03b1 2 v (\u03b4 2 in + \u03b4 2 out ) \u2264(1 \u2212 \u03bb + 8\u03c1 \u221a N ) 1 1 \u2212 v + 6\u03b1 2 L 2 v H k + (1 \u2212 \u03bb + 8\u03c1 \u221a N ) 4\u03b1 2 v (\u03b4 2 in + \u03b4 2 out ) \u2264(1 \u2212 \u03c9) 1 1 \u2212 v + 6\u03b1 2 L 2 v H k + (1 \u2212 \u03c9) 4\u03b1 2 v (\u03b4 2 in + \u03b4 2 out ),\nwhere the second inequality chooses u = 4\u03c1 \u221a N \u2264 \u03bb and uses the inequality 1\u2212\u03bb 1\u2212u \u2264 1 \u2212 \u03bb + u. To bound the coefficient of the first term at the RHS of (68), we set v = \u03c9 3 and observe that the step size rule implies\n6\u03b1 2 L 2 \u2264 (2 \u2212 \u03c9)\u03c9 2 3(3 \u2212 \u03c9) = (2\u03c9/3 \u2212 \u03c9 2 /3) 1 \u2212 \u03c9/3 \u00b7 \u03c9 3 (69) = (\u03c9 \u2212 v \u2212 v\u03c9)v 1 \u2212 v ,\nwith which we know that\n1 1 \u2212 v + 6\u03b1 2 L 2 v \u2264 1 + \u03c9 \u2212 v \u2212 v\u03c9 1 \u2212 v = 1 + \u03c9.(70)\nSubstituting (70) back into (68) yields\nE \u03be k [H k+1 ] \u2264 1 \u2212 \u03c9 2 H k + 12(1 \u2212 \u03c9) \u03c9 \u03b1 2 (\u03b4 2 in + \u03b4 2 out ).(71)\nUsing telescopic cancellation on (71) from 0 to k, we deduce that\nE[H k ] \u2264(1 \u2212 \u03c9 2 ) k H 0 \u2212 12(1 \u2212 \u03c9) \u03c9 3 \u03b1 2 (\u03b4 2 in + \u03b4 2 out ) + 12(1 \u2212 \u03c9) \u03c9 3 \u03b1 2 (\u03b4 2 in + \u03b4 2 out ).(72)\nSince for all honest workers n \u2208 N , x 0 n are initialized at the same point, it holds that H 0 = 0 and\nE[H k ] \u2264 \u03b1 2 \u2206(\u03b4 2 in + \u03b4 2 out ),(73)\nwhich completes the proof.\n\n\nAPPENDIX B PROOF OF THEOREM 3\n\nProof. Since W is a doubly stochastic matrix, we know W is doubly stochastic as well. Now we prove (43).\n\nAt any inner iteration i in Algorithm 2, the removed model x m (i) can be either Byzantine or honest for any honest worker n. Let us define N \nN := 1 W n (N (i) n ) m\u2208N (i) n w nm x m andx (i) B := 1 W n (B (i) n ) b\u2208B (i) n w nb x b .\nFor simplicity, we omit subscript n in the notations ofx\n(i) N ,x (i) B and x (i) avg .\nObserve that\nx (i) avg = m\u2208U (i) n w nm x m m\u2208U (i) n w nm (74) = m\u2208N (i) n w nm x m + b\u2208B (i) n w nb x b m\u2208U (i) n w nm =(1 \u2212 \u00b5 (i) n )x (i) N + \u00b5 (i) nx (i) B ,\nwhere \u00b5 \nN \u2212x (i) B > max m\u2208Nn x m \u2212 x (i) N /(1 \u2212 2\u00b5 (i) n ) is satisfied, it holds that \u00b5 (i) n x (i) N \u2212x (i) B + max m\u2208Nn x m \u2212x (i) N (75) <(1 \u2212 \u00b5 (i) n ) x (i) N \u2212x (i) B .\nObserve that there exists at least one Byzantine neighbor b \u2208 B (i) n such that\nx b \u2212 x (i) avg \u2265 x (i) B \u2212 x (i) avg (76) =(1 \u2212 \u00b5 (i) n ) x (i) N \u2212x (i) B\n, and for any honest neighbor n \u2208 N (i)\nn it holds that x n \u2212 x (i) avg \u2264 x n \u2212x (i) N + x (i) N \u2212 x (i) avg (77) \u2264 max m\u2208Nn x m \u2212x (i) N + \u00b5 (i) n x (i) N \u2212x (i) B .\nFrom (75), (76) and (77), there exists at least one Byzantine\nneighbor b \u2208 B (i) n such that x b \u2212 x (i) avg > x n \u2212 x (i)\navg holds for any honest neighbor n \u2208 N (i) n . Thus, we can successfully remove one Byzantine neighbor in inner iteration i. This ends the discussion of Case 1.\n\nCase 2: When inequality x (i)\nN \u2212x (i) B \u2264 max m\u2208Nn x m \u2212 x (i) N /(1 \u2212 2\u00b5 (i)\nn ) is satisfied, it becomes possible to discard the model from a honest neighbor by mistake. However, it still holds that\nx (i+1) avg \u2212x n (78) \u2264 x (i+1) avg \u2212 x (i) avg + x (i) avg \u2212x (i) N + x (i) N \u2212x n \u2264 w nm (i) W n (U (i+1) n ) x m (i) \u2212 x (i) avg + x (i) avg \u2212x (i) N + x (i) N \u2212x n \u2264 w nm (i) W n (U (i+1) n ) x m (i) \u2212x (i) N + 1 + w nm (i) W n (U (i+1) n ) x (i) avg \u2212x (i) N + x (i) N \u2212x n \u2264 w nm (i) W n (U (i+1) n ) x m (i) \u2212x (i) N + 2 x (i) avg \u2212x (i) N + x (i) N \u2212x n ,\nin which the second inequality comes from inequality (106) in Lemma 3 and the fourth inequality holds because\nW n (U (i+1) n ) \u2265 1 \u2212 W n (U max n ) \u2265 W n (U max n ) \u2265 w nm (i) .(79)Note that 1 \u2212 W n (U max n ) \u2265 W n (U max n ) as we require W n (U max n ) < 1 3 . The hypothesis on x (i) N \u2212x (i) B\nguarantees that\nx (i) avg \u2212x (i) N =\u00b5 (i) n x (i) N \u2212x (i) B (80) \u2264 \u00b5 (i) n 1 \u2212 2\u00b5 (i) n max m\u2208Nn x m \u2212x (i) N .\nFrom (106) in Lemma 3, we have\nx (i) N \u2212x n (81) \u2264 W n (N n ) + w nn + W n (B n ) \u2212 W n (N (i) n ) W n (N n ) + w nn + W n (B n ) max m\u2208Nn x m \u2212x (i) N .\nSubstituting (80) and (81) into (78) yields\nx (i+1) avg \u2212x n (82) \u2264 w nm (i) W n (U (i+1) n ) + W n (N n ) + w nn + W n (B n ) \u2212 W n (N (i) n ) W n (N n ) + w nn + W n (B n ) + 2\u00b5 (i) n 1 \u2212 2\u00b5 (i) n \u00b7 max m\u2208Nn x m \u2212x (i) N .\nNow we bound the coefficient at the RHS of (82). Since (N n \u222a {n}) \\ N (i) n is the set of the discarded honest neighbors up to iteration i, we know that m (i) / \u2208 (N n \u222a {n}) \\ N (i)\n\nn . In addition, {m (i) } \u222a ((N n \u222a {n}) \\ N (i) n ) contains no more than i + 1 \u2264 q n elements, implying\nw nm (i) + W n (N n ) + w nn \u2212 W n (N (i) n ) \u2264 W n (U max n ). (83)\nObserve the following relations W n (U\n(i+1) n ) \u2265 W n (N (i+1) n ), W n (N n ) + w nn + W n (B n ) \u2265 W n (N (i+1) n ) and W n (N (i+1) n ) \u2265 1 \u2212 2W n (U max n )\nall hold. Along with (79), we can bound the first and the third coefficients at the RHS of (82) by\nw nm (i) W n (U (i+1) n ) + W n (N n ) + w nn + W n (B n ) \u2212 W n (N (i) n ) W n (N n ) + w nn + W n (B n ) (84) \u2264 w nm (i) + W n (N n ) + w nn + W n (B n ) \u2212 W n (N (i) n ) W n (N (i+1) n ) \u2264 2W n (U max n ) 1 \u2212 2W n (U max n )\n.\n\nIn addition, \u00b5 (i) n contains the weights of Byzantine neighbors but can be bounded by\n\u00b5 (i) n = W n (B (i) n ) W n (U (i) n ) \u2264 W n (B n ) 1 \u2212 W n (U max n ) \u2264 W n (U max n ) 1 \u2212 W n (U max n ) .(85)\nPlugging (84) and (85) into (82) leads to\nx (i+1) avg \u2212x n \u2264 4W n (U max n ) 1 \u2212 3W n (U max n ) max m\u2208Nn x m \u2212x (i) N (86) \u2264 12W n (U max n ) 1 \u2212 3W n (U max n ) max m\u2208Nn x m \u2212x n .\nTo derive the last inequality of (86), we use (107) in Lemma 3. With it, we have\nmax m\u2208Nn x m \u2212x (i) N (87) \u2264 max m\u2208Nn x m \u2212x n + max m\u2208Nn x n \u2212x (i) N \u2264 1 + W n (N n ) + w nn + W n (B n ) \u2212 W n (N (i) n ) W n (N (i) n ) \u00b7 max m\u2208Nn x m \u2212x n \u2264 1 1 \u2212 2W n (U max n ) max m\u2208Nn x m \u2212x n \u22643 max m\u2208Nn x m \u2212x n .\nThis ends the discussion of Case 2. Therefore, we conclude that for\nA n (x n , {x m,n } m\u2208Nn\u222aBn ) = x (qn) avg ,(88)\nno matter which case happens at each inner iteration i = 0, \u00b7 \u00b7 \u00b7 , q n \u2212 1, we eventually have\nA n (x n ,{x m,n } m\u2208Nn\u222aBn ) \u2212x n (89) \u2264 12W n (U max n ) 1 \u2212 3W n (U max n ) max m\u2208Nn x m \u2212x n .\nAccording to Definition 1, the contraction constant \u03c1 is bounded by\n\u03c1 \u2264 max n\u2208N 12W n (U max n ) 1 \u2212 3W n (U max n ) ,(90)\nwhich completes the proof.\n\n\nAPPENDIX C COVERAGE OF GENERIC AGGREGATOR FORM\n\nWe next show that the existing Byzantine-resilient decentralized (stochastic) algorithms all fall in the form of (7).\n\nThe works of [25]- [28] adopt trimmed mean (TriMean) as the base aggregator, given by A(x n ,{x m,n } m\u2208Nn\u222aBn ) = TriMean({x m,n } m\u2208Nn\u222aBn ), (91) and set r n = 1 Nn+Bn\u22122q+1 where q is a parameter. For each coordinate d = 1, . . . , D, trimmed mean discards the largest q and the smallest q elements, and returns the average of the remaining ones.\n\nThe work of [30] proposes decentralized RSA, in which\nA n (x n , {x m,n } m\u2208Nn\u222aBn ) (92) =x n + \u03b1 k C R m\u2208Nn\u222aBn Sign(x m,n \u2212 x n ) =(1 \u2212 \u03b1 k C R )x n + \u03b1 k C R x n + m\u2208Nn\u222aBn Sign(x m,n \u2212 x n ) .\nTherein, Sign denotes the element-wise sign function, \u03b1 k > 0 is the step size, and C R is a constant. We can observe that the base aggregator A is\nA(x n , {x m,n } m\u2208Nn\u222aBn ) = x n + m\u2208Nn\u222aBn Sign(x m,n \u2212 x n ),(93)\nand r n = 1 \u2212 \u03b1 k C R , which can be time-varying. The work of [31] proposes to extend centered clipping over a distributed network to self centered clipping (SCC) over a decentralized network. The aggregation rule of honest worker n \u2208 N is given by\nA n (x n , {x m,n } m\u2208Nn\u222aBn )(94)\n= m\u2208Nn\u222aBn\u222a{n} w nm (x n + CLIP(x m,n \u2212 x n , \u03c4 n )),\n=(1 \u2212 w nn ) x n + m\u2208Nn\u222aBn w nm 1 \u2212 w nn CLIP(x m,n \u2212 x n , \u03c4 n ) + w nn x n ,\nwhere w nm is the weight that worker n assigns to worker m (see (5) for reference), \u03c4 n > 0 is a threshold, and CLIP is a function defined as CLIP(x, \u03c4 ) := min 1, \u03c4 x x.\n\nTherefore, SCC defines the base aggregator as\n\nA(x n , {x m,n } m\u2208Nn\u222aBn )\n\n=x n + m\u2208Nn\u222aBn w nm 1 \u2212 w nn CLIP(x m \u2212 x n , \u03c4 n ), and sets r n = w nn . The work of [32] defines a two-stage method UBAR to filter out the potential Byzantine attacks. The base aggregator A is UBAR, and the parameter r n is tunable.\n\nThe work of [33] proposes a similarity-based reweighting (SimRew) method. Worker n assigns a weight c nm > 0 to worker m \u2208 N \u222a B and computes an auxiliary vector y nm based on the similarity between x n andx m,n . SimRew defines the base aggregator as A n (x n , {x m,n } m\u2208Nn\u222aBn ) =x n + m\u2208Nn\u222aBn c nm y nm , = m\u2208Nn\u222aBn c nm (y nm + x n ),\n\nand sets r n = 0.\n\n\nAPPENDIX D USEFUL LEMMAS AND THEIR PROOFS\n\nIn this section we introduce some useful lemmas which are necessary in the proofs of main theorems.\n\n\nA. Lemma 1 and Its Proof\n\nThe following lemma claims that the Frobenius norm \u00b7 F is sub-multiplicative.\n\nLemma 1 (Sub-multiplicativity of \u00b7 F ). For the Frobenius norm \u00b7 F , it holds for any A, Z \u2208 R N \u00d7N that\nAZ F \u2264 A Z F ,(98)\nwhere \u00b7 is the spectral norm.\n\nProof. Decompose Z by columns as Z = [z 1 , \u00b7 \u00b7 \u00b7 , z N ]. Then, AZ = [Az 1 , \u00b7 \u00b7 \u00b7 , Az N ]. It follows that\nAZ 2 F = N n=1 Az n 2 \u2264 A 2 N n=1 z n 2 = A 2 Z 2 F ,(99)\nwhere \u00b7 is the spectrum norm for a matrix and 2 norm for a vector. This completes the proof.\n\n\nB. Lemma 2 and Its Proof\n\nThe following Lemma characterizes the connection between H k+ 1 2 and H k in the Byzantine-resilient decentralized SGD.\n\nLemma 2. Consider the Byzantine-resilient decentralized SGD in Algorithm 1. Under Assumptions 2-4, if a constant step size \u03b1 k = \u03b1 is used, for any v \u2208 (0, 1), it holds that\nE \u03be k [H k+ 1 2 ] \u2264 1 1 \u2212 v + 6\u03b1 2 L 2 v H k + 4\u03b1 2 v (\u03b4 2 in + \u03b4 2 out ).(100)\nProof. Observe that\nE \u03be k [H k+ 1 2 ] = E \u03be k [ 1 N n\u2208N x k+ 1 2 n \u2212x k+ 1 2 2 ](101)\u2264 1 1 \u2212 v 1 N n\u2208N x k n \u2212x k 2 + \u03b1 2 v E \u03be k [ 1 N\nn\u2208N \u2207f n (x k n ; \u03be k n )\u2212 \n= 1 N n\u2208N \u2207f n (x k n ) \u2212 1 N m\u2208N \u2207f m (x k m ) 2 + E \u03be k [ 1 N n\u2208N \u2207f n (x k n ; \u03be k n ) \u2212 1 N m\u2208N \u2207f m (x k m ; \u03be k m ) \u2212 (\u2207f n (x k n ) \u2212 1 N m\u2208N \u2207f m (x k m )) 2 ].\nWith Assumptions 2 and 4, the first term at the RHS of (102) can be bounded by\n1 N n\u2208N \u2207f n (x k n ) \u2212 1 N m\u2208N \u2207f m (x k m ) 2 (103) \u2264 3 1 N n\u2208N \u2207f n (x k n ) \u2212 \u2207f n (x k ) 2 + 3 1 N n\u2208N \u2207f n (x k ) \u2212 1 N m\u2208N \u2207f m (x k ) 2 + 3 1 N n\u2208N 1 N m\u2208N \u2207f m (x k ) \u2212 1 N m\u2208N \u2207f m (x k m ) 2 \u2264 6L 2 1 N n\u2208N x k n \u2212x k 2 + 3\u03b4 2 out .\nWith Assumption 3, the second term at the RHS of (102) can be bounded by\nE \u03be k [ 1 N n\u2208N \u2207f n (x k n ; \u03be k n )\u2212 1 N m\u2208N \u2207f m (x k m ; \u03be k m ) (104) \u2212 (\u2207f n (x k n ) \u2212 1 N m\u2208N \u2207f m (x k m )) 2 ] \u22642 1 N n\u2208N E \u03be k [ \u2207f n (x k n ; \u03be k n ) \u2212 \u2207f n (x k n ) 2 ] +2 1 N n\u2208N E \u03be k [ 1 N m\u2208N \u2207f m (x k m ; \u03be k m )\u2212 1 N m\u2208N \u2207f m (x k m ) 2 ]\n\u22644\u03b4 2 in . Substituting (103) and (104) into (102) yields (100).\n\n\nC. Lemma 3 and Its Proof\n\nLemma 3 describes the difference between partial weighted average and full weighted average for a set of vectors. \n\nin which {c n } n\u2208N2 are a set of positive weights. The difference between the weighted means is bounded by  \n\u0233 1 \u2212\u0233 2 \u2264 n\u2208N2\\N1 c n n\u2208N2 c n max n\u2208N2\\N1 x n \u2212\u0233 1 ,(106)\nFig. 1 .\n1Comparison in the Erdos-Renyi graph under sign-flipping attacks for the non-i.i.d. case. Performance metrics are average classification accuracy and disagreement measure, both in terms of local models of honest workers.\n\n\nas N n \u222a B n \u222a {n} when i = 0. It computes the weighted average of all models in U\n\nFig. 2 .\n2Two-castle graph of 6 honest workers. Workers marked with z 1 are numbered as 1, 2 and 3; marked with z 2 are numbered as 4, 5 and 6.\n\nTheorem 2 (\n2Consensus of robust aggregation rules). Consider the Byzantine-resilient decentralized SGD in Algorithm 1. Suppose that the robust aggregation rules {A n } n\u2208N satisfy (18) and the contraction factor \u03c1 satisfies \u03c1 < \u03c1 * := \u03bb 8 \u221a N . (34) Define a constant \u03c9 := \u03bb \u2212 8\u03c1 \u221a N . Under Assumptions 2-6, if a constant step size \u03b1\n\nDefinition 2 (\n2Robust contractive aggregation (RCA)). A set of aggregation rules {A n } n\u2208N are RCA if the associated contraction constant \u03c1 satisfies (34). Definition 3 (Robust doubly stochastic aggregation (RDSA)). A set of aggregation rules {A n } n\u2208N are RDSA if the associated virtual mixing matrix W is doubly stochastic.\n\nCorollary 1 .\n1Consider the Byzantine-resilient decentralized SGD in Algorithm 1. Suppose that the robust aggregation rules {A n } n\u2208N satisfy (18), and are both RCA and RDSA. Under Assumptions 1-6, if the step size \u03b1 k is set as \u03b1 = O( N/(\u03b4 2 in K)), then the consensus error is bounded by\n\nU\n:U \u2286Nn \u222aBn |U |=qn m\u2208U w nm .\n\nFig. 3 .\n3(Left) Two-castle graph and (Right) octopus graph, both consisting of 10 honest and 2 Byzantine workers. The blues and reds represent honest and Byzantine workers, respectively.\n\nFig. 4 .\n4IOS in Erdos-Renyi graphs under different attacks for the non-i.i.d. case. The number of honest workers is 20 and the number of Byzantine workers is varying.\n\n\nand the point. Plugging (52), (54) and (57) into (49), we have\n\nF\n, where the second inequality comes from (56).\n\n\nis the normalized weight from B(i) n . Case 1: When inequality x (i)\n\nLemma 3 .\n3Consider the two sets N 1 \u2286 N 2 and define their weighted means as y 1 := n\u2208N1 c n x n n\u2208N1 c n ,\u0233 2 := n\u2208N2 c n x n n\u2208N2 c n ,\n\nTABLE I ROBUST\nIAGGREGATION RULES, THEIR CORRESPONDING \u03c1 AND WHETHER THEY ARE RCA IN A COMPLETE NETWORK, AS WELL AS WHETHER THEY ARE RDSA IN A GENERAL INCOMPLETE NETWORKAggregation \n\u03c1 \nRCA RDSA \nCooMed [6] \n\n\nTABLE II ACCURACY\nII(ACC) AND DISAGREEMENT MEASURE (DM) IN THE TWO-CASTLE GRAPH FOR THE I.I.D. CASE.no attack \nGaussian \nsign-flipping \nisolation \nsample-duplicating \nALIE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nno comm. \n90.24 >1e-01 \n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nWeiMean \n91.71 <1e-07 16.09 >1e-01 29.23 \n1e-07 \n90.33 >1e-01 91.71 <1e-07 \n91.67 <1e-07 \nCooMed \n91.49 <1e-07 91.53 <1e-07 87.04 <1e-07 91.40 \n1e-07 \n91.56 <1e-07 \n91.38 <1e-07 \nGeoMed \n91.67 <1e-07 91.68 <1e-07 87.05 <1e-07 90.16 >1e-01 91.68 <1e-07 \n91.58 <1e-07 \nKrum \n90.72 <1e-07 90.77 \n1e-07 \n91.13 \n1e-07 \n90.72 \n1e-07 \n91.04 \n6e-07 \n91.43 <1e-07 \nTriMean \n91.70 <1e-07 91.61 <1e-07 86.10 <1e-07 91.61 <1e-07 91.65 <1e-07 \n91.59 <1e-07 \nSimRew \n76.42 >1e-01 73.96 >1e-01 73.91 >1e-01 74.00 >1e-01 73.92 >1e-01 \n73.99 >1e-01 \nDRSA \n91.68 \n2e-06 \n91.65 \n2e-06 \n89.95 \n3e-06 \n91.60 \n7e-06 \n91.65 \n2e-06 \n91.65 \n2e-06 \nCC \n91.67 <1e-07 91.65 \n2e-06 \n29.86 \n1e-07 \n91.56 \n6e-06 \n91.68 <1e-07 \n91.70 <1e-07 \nSCC \n91.70 <1e-07 91.68 \n2e-06 \n35.69 \n1e-07 \n91.62 \n6e-06 \n91.66 <1e-07 \n91.67 <1e-07 \nFABA \n91.71 <1e-07 91.68 <1e-07 91.65 <1e-07 91.71 <1e-07 91.63 <1e-07 \n91.59 <1e-07 \nIOS (ours) 91.69 <1e-07 91.68 <1e-07 91.65 <1e-07 91.67 <1e-07 91.67 <1e-07 \n91.61 <1e-07 \n\n\n\nTABLE III ACCURACY\nIII(ACC) AND DISAGREEMENT MEASURE (DM) IN THE TWO-CASTLE GRAPH FOR THE NON-I.I.D. CASE.no attack \nGaussian \nsign-flipping \nisolation \nsample-duplicating \nALIE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nno comm. \n10.00 >1e-01 \n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nWeiMean \n91.73 \n1e-07 \n15.79 >1e-01 48.99 \n5e-07 \n10.00 >1e-01 91.66 \n3e-07 \n91.11 <1e-07 \nCooMed \n77.60 \n6e-07 \n80.23 \n1e-07 \n74.55 \n1e-02 \n33.39 >1e-01 81.03 \n1e-07 \n82.03 <1e-07 \nGeoMed \n89.29 <1e-07 89.66 <1e-07 8.36 \n5e-07 \n10.00 >1e-01 89.90 \n2e-07 \n88.63 <1e-07 \nKrum \n17.45 \n3e-06 \n19.74 \n4e-07 \n10.10 <1e-07 19.74 \n4e-07 \n29.17 \n5e-07 \n82.62 \n5e-07 \nTriMean \n91.72 <1e-07 90.08 \n1e-06 \n72.02 \n8e-07 \n58.53 \n2e-06 \n87.21 \n1e-06 \n77.85 \n1e-06 \nSimRew \n10.34 >1e-01 10.48 >1e-01 10.48 >1e-01 10.48 >1e-01 10.48 >1e-01 \n10.48 >1e-01 \nDRSA \n81.55 \n4e-03 \n89.24 \n4e-03 \n11.35 \n6e-03 \n78.87 \n2e-03 \n86.06 \n5e-03 \n61.37 \n6e-03 \nCC \n91.67 <1e-07 91.60 \n2e-05 \n48.99 \n5e-07 \n86.48 \n6e-05 \n91.64 \n3e-07 \n91.16 <1e-07 \nSCC \n91.73 \n1e-07 \n91.55 \n2e-05 \n48.99 \n5e-07 \n86.12 \n6e-05 \n91.65 \n3e-07 \n91.11 <1e-07 \nFABA \n91.72 <1e-07 91.68 <1e-07 91.66 <1e-07 91.64 \n1e-07 \n88.90 \n6e-07 \n82.86 \n6e-07 \nIOS (ours) 91.73 \n1e-07 \n91.68 <1e-07 91.66 <1e-07 91.64 \n1e-07 \n88.84 \n6e-07 \n82.78 \n6e-07 \n\n\n\nTABLE IV ACCURACY\nIV(ACC) AND DISAGREEMENT MEASURE (DM) IN THE OCTOPUS GRAPH FOR THE NON-I.I.D. CASE.no attack \nGaussian \nsign-flipping \nisolation \nsample-duplicating \nALIE \nAcc.(%) CE Acc.(%) CE Acc.(%) CE Acc.(%) CE Acc.(%) \nCE \nAcc.(%) CE \nCC \n89.26 4e-06 89.17 8e-06 32.20 3e-05 62.48 2e-05 89.23 \n4e-06 \n89.14 4e-06 \nSCC \n90.19 2e-05 90.16 3e-05 32.24 1e-04 67.80 7e-05 90.24 \n2e-05 \n90.10 3e-05 \nFABA \n89.26 4e-06 89.26 4e-06 89.26 4e-06 89.24 4e-06 89.09 \n4e-06 \n88.87 4e-06 \nIOS (ours) 90.19 2e-05 90.21 2e-05 90.21 2e-05 90.20 2e-05 89.82 \n2e-05 \n89.49 3e-05 \n\nevenly distributed across all workers. In the non-i.i.d. case, \neach worker only has images from one class. Unless otherwise \nstated, we use squared 2 -norm regularization with coefficient \n0.01, step size \u03b1 k = 0.9/ \n\u221a \nk + 1 and batch size 32. \n\n\n\n1 N\nNApplying variance decomposition E[ x 2 ] = E[x] 2 + E[ x \u2212 E[x]2 ] to the second term at the RHS of (101) yieldsm\u2208N \n\n\u2207f m (x k \nm ; \u03be k \nm ) 2 ]. \n\nE \u03be k [ \n\n1 \nN \n\nn\u2208N \n\n\u2207f n (x k \nn ; \u03be k \nn ) \u2212 \n\n1 \nN \n\nm\u2208N \n\n\u2207f m (x k \nm ; \u03be k \nm ) 2 ] (102) \n\n\n\nTABLE V\nVACCURACY (ACC) AND DISAGREEMENT MEASURE (DM) IN THE ERDOS-RENYI GRAPH FOR THE I.I.D. CASE. IOS (ours) 91.68 <1e-07 91.68 <1e-07 91.66 <1e-07 91.66 <1e-07 91.70 <1e-07 91.68 <1e-07no attack \nGaussian \nsign-flipping \nisolation \nsample-duplicating \nALIE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nno comm. \n90.24 >1e-01 \n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nWeiMean \n91.70 <1e-07 13.78 >1e-01 30.93 \n3e-07 \n90.18 >1e-01 91.63 <1e-07 \n91.67 <1e-07 \nCooMed \n91.58 <1e-07 91.56 \n1e-07 \n87.00 \n1e-07 \n91.52 \n2e-07 \n91.46 \n1e-07 \n91.52 <1e-07 \nGeoMed \n91.70 <1e-07 91.68 <1e-07 87.10 <1e-07 91.29 >1e-01 91.70 <1e-07 \n91.69 <1e-07 \nKrum \n90.83 \n2e-07 \n90.83 \n3e-07 \n91.23 \n3e-07 \n90.89 \n5e-07 \n90.91 \n7e-07 \n91.45 <1e-07 \nTriMean \n91.73 <1e-07 91.61 <1e-07 86.34 <1e-07 91.57 \n1e-07 \n91.61 <1e-07 \n91.62 <1e-07 \nSimRew \n79.15 >1e-01 76.88 >1e-01 76.87 >1e-01 76.93 >1e-01 76.86 >1e-01 \n76.87 >1e-01 \nDRSA \n91.69 \n4e-06 \n91.62 \n4e-06 \n90.27 \n7e-06 \n91.61 \n7e-03 \n91.64 \n3e-06 \n91.64 \n3e-06 \nCC \n91.68 <1e-07 91.69 \n2e-06 \n31.57 \n3e-07 \n91.51 \n7e-06 \n91.63 <1e-07 \n91.63 <1e-07 \nSCC \n91.72 <1e-07 91.74 \n2e-06 \n32.56 \n3e-07 \n91.59 \n8e-06 \n91.71 <1e-07 \n91.70 <1e-07 \nFABA \n91.69 <1e-07 91.67 <1e-07 91.71 <1e-07 91.68 <1e-07 91.64 <1e-07 \n91.67 <1e-07 \n\n\nTABLE VI ACCURACY\nVI(ACC) AND DISAGREEMENT MEASURE (DM) IN THE ERDOS-RENYI GRAPH FOR THE NON-I.I.D. CASE. >1e-01 10.53 >1e-01 10.53 >1e-01 10.53 >1e-01 10.53 >1e-01no attack \nGaussian \nsign-flipping \nisolation \nsample-duplicating \nALIE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nAcc.(%) \nCE \nno comm. \n10.00 >1e-01 \n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nWeiMean \n91.70 \n9e-07 \n13.63 >1e-01 20.89 \n5e-06 \n10.00 >1e-01 91.53 \n9e-07 \n91.21 \n7e-07 \nCooMed \n75.49 \n2e-02 \n77.92 \n5e-07 \n66.38 \n5e-03 \n41.56 >1e-01 78.74 \n4e-07 \n82.46 \n3e-07 \nGeoMed \n88.00 \n2e-07 \n88.63 \n3e-07 \n23.35 \n8e-07 \n73.01 >1e-01 88.68 \n6e-07 \n89.14 \n2e-07 \nKrum \n19.71 \n4e-05 \n19.76 \n4e-05 \n19.16 \n2e-07 \n19.76 \n4e-05 \n29.20 \n3e-05 \n63.79 \n3e-06 \nTriMean \n91.61 \n5e-07 \n90.40 \n2e-06 \n70.97 \n4e-06 \n54.47 \n6e-06 \n87.54 \n3e-06 \n82.56 \n2e-06 \nSimRew \n10.40 10.53 >1e-01 \nDRSA \n76.43 \n3e-03 \n89.38 \n3e-03 \n10.50 \n4e-03 \n88.60 \n1e-03 \n85.27 \n4e-03 \n60.69 \n4e-03 \nCC \n91.60 \n5e-07 \n91.56 \n2e-05 \n45.16 \n3e-06 \n80.03 \n8e-05 \n91.53 \n7e-07 \n91.13 \n5e-07 \nSCC \n91.70 \n9e-07 \n91.57 \n2e-05 \n20.89 \n5e-06 \n83.80 \n8e-05 \n91.52 \n9e-07 \n91.21 \n7e-07 \nFABA \n91.61 \n5e-07 \n91.54 \n5e-07 \n91.55 \n5e-07 \n91.53 \n5e-07 \n88.91 \n1e-06 \n86.87 \n9e-07 \nIOS (ours) 91.70 \n9e-07 \n91.60 \n8e-07 \n91.61 \n8e-07 \n91.55 \n8e-07 \n88.22 \n2e-06 \n86.61 \n2e-06 \n\nIn many existing decentralized approaches to constructing W , each worker needs to know the degrees of all neighbors. Therefore, Byzantine neighbors can report wrong degrees in purpose. To address this issue, we can run Byzantine-resilient topology discovery algorithms (see[34] for an example) in advance and then construct W .\nNote that this is different to w nm that is explicitly assigned to any worker m, either honest or Byzantine, in(5).\nW is row stochastic if: (a) all entries wnm \u2208 [0, 1]; (b) N m=0 wnm = 1 for all n. It is doubly stochastic if both W and W are row stochastic.4 For notational convenience, here we assume that the honest workers are numbered from 1 to N . This can be easily extended to general cases.\nNote that this is different tox k n := m\u2208Nn\u222a{n} wnmx k m , the weighted average of honest neighboring and own models for worker n in Definition 1.\nSign-flipping is an implementation of inner-production manipulation[44].\n\nFederated optimization: Distributed machine learning for ondevice intelligence. Jakub Kone\u010dn\u1ef3, Brendan Mcmahan, Daniel Ramage, Peter Richt\u00e1rik, arXiv:1610.02527arXiv preprintJakub Kone\u010dn\u1ef3, H Brendan McMahan, Daniel Ramage, and Peter Richt\u00e1rik. Federated optimization: Distributed machine learning for on- device intelligence. arXiv preprint arXiv: 1610.02527, 2016.\n\nPeter Kairouz, Brendan Mcmahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, arXiv:1912.04977Advances and open problems in federated learning. Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel CummingsarXiv preprintPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv: 1912.04977, 2019.\n\nXiaoyu He, and Zibin Zheng. A decentralized federated learning framework via committee mechanism with convergence guarantee. Chunjiang Che, Xiaoli Li, Chuan Chen, arXiv:2108.00365arXiv preprintChunjiang Che, Xiaoli Li, Chuan Chen, Xiaoyu He, and Zibin Zheng. A decentralized federated learning framework via committee mechanism with convergence guarantee. arXiv preprint arXiv: 2108.00365, 2021.\n\nThe Byzantine generals problem. Leslie Lamport, Robert Shostak, Marshall Pease, ACM Transactions on Programming Languages and Systems. 43Leslie Lamport, Robert Shostak, and Marshall Pease. The Byzantine generals problem. ACM Transactions on Programming Languages and Systems, 4(3):382-401, 1982.\n\nAdversary-resilient distributed and decentralized statistical inference and machine learning: An overview of recent advances under the byzantine threat model. Zhixiong Yang, Arpita Gang, Waheed U Bajwa, IEEE Signal Processing Magazine. 373Zhixiong Yang, Arpita Gang, and Waheed U Bajwa. Adversary-resilient distributed and decentralized statistical inference and machine learning: An overview of recent advances under the byzantine threat model. IEEE Signal Processing Magazine, 37(3):146-159, 2020.\n\nByzantine-robust distributed learning: Towards optimal statistical rates. Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett, International Conference on Machine Learning. Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In International Conference on Machine Learning, pages 5650-5659, 2018.\n\nDistributed statistical machine learning in adversarial settings: Byzantine gradient descent. Yudong Chen, Lili Su, Jiaming Xu, Proceedings of the ACM on Measurement and Analysis of Computing Systems. 12Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings: Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 1(2):1-25, 2017.\n\n. Cong Xie, Oluwasanmi Koyejo, Indranil Gupta, arXiv:1802.10116Generalized Byzantine-tolerant SGD. arXiv preprintCong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized Byzantine-tolerant SGD. arXiv preprint arXiv: 1802.10116, 2018.\n\nPhocas: Dimensional Byzantine-resilient stochastic gradient descent. Cong Xie, Oluwasanmi Koyejo, Indranil Gupta, arXiv:1805.09682arXiv preprintCong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Phocas: Dimen- sional Byzantine-resilient stochastic gradient descent. arXiv preprint arXiv: 1805.09682, 2018.\n\nMachine learning with adversaries: Byzantine tolerant gradient descent. Peva Blanchard, Rachid El Mahdi El Mhamdi, Julien Guerraoui, Stainer, Advances in Neural Information Processing Systems. Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, pages 118-128, 2017.\n\nThe hidden vulnerability of distributed learning in Byzantium. Rachid El Mahdi El Mhamdi, S\u00e9bastien Guerraoui, Rouault, International Conference on Machine Learning. El Mahdi El Mhamdi, Rachid Guerraoui, and S\u00e9bastien Rouault. The hidden vulnerability of distributed learning in Byzantium. In Interna- tional Conference on Machine Learning, pages 3521-3530, 2018.\n\nFABA: An algorithm for fast aggregation against Byzantine attacks in distributed neural networks. Qi Xia, Zeyi Tao, Zijiang Hao, Qun Li, International Joint Conference on Artificial Intelligence. Qi Xia, Zeyi Tao, Zijiang Hao, and Qun Li. FABA: An algorithm for fast aggregation against Byzantine attacks in distributed neural networks. In International Joint Conference on Artificial Intelligence, pages 4824- 4830, 2019.\n\nEduard Gorbunov, Alexander Borzunov, Michael Diskin, Max Ryabinin, arXiv:2106.11257Secure distributed training at scale. arXiv preprintEduard Gorbunov, Alexander Borzunov, Michael Diskin, and Max Ryabinin. Secure distributed training at scale. arXiv preprint arXiv: 2106.11257, 2021.\n\nFederated variance-reduced stochastic gradient descent with robustness to Byzantine attacks. Zhaoxian Wu, Qing Ling, Tianyi Chen, Georgios B Giannakis, IEEE Transactions on Signal Processing. 68Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Federated variance-reduced stochastic gradient descent with robustness to Byzantine attacks. IEEE Transactions on Signal Processing, 68:4583- 4596, 2020.\n\nByzantine resilient non-convex SVRG with distributed batch gradient computations. Prashant Khanduri, Saikiran Bulusu, Pranay Sharma, Pramod K Varshney, arXiv:1912.04531arXiv preprintPrashant Khanduri, Saikiran Bulusu, Pranay Sharma, and Pramod K Varshney. Byzantine resilient non-convex SVRG with distributed batch gradient computations. arXiv preprint arXiv: 1912.04531, 2019.\n\nLearning from history for Byzantine robust optimization. Lie Sai Praneeth Karimireddy, Martin He, Jaggi, International Conference on Machine Learning. Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Learning from history for Byzantine robust optimization. In International Conference on Machine Learning, pages 5311-5319, 2021.\n\nByzantine-resilient SGD in high dimensions on heterogeneous data. Deepesh Data, Suhas Diggavi, IEEE International Symposium on Information Theory. Deepesh Data and Suhas Diggavi. Byzantine-resilient SGD in high dimensions on heterogeneous data. In IEEE International Symposium on Information Theory, pages 2310-2315, 2021.\n\nRSA: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets. Liping Li, Wei Xu, Tianyi Chen, B Georgios, Qing Giannakis, Ling, AAAI Conference on Artificial Intelligence. Liping Li, Wei Xu, Tianyi Chen, Georgios B Giannakis, and Qing Ling. RSA: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets. In AAAI Conference on Artificial Intelligence, pages 1544-1551, 2019.\n\nByzantine-robust learning on heterogeneous datasets via bucketing. Lie Sai Praneeth Karimireddy, Martin He, Jaggi, International Conference on Learning Representations. Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Byzantine-robust learning on heterogeneous datasets via bucketing. In International Conference on Learning Representations, 2022.\n\nByzantinerobust variance-reduced federated learning over distributed non-iid data. Jie Peng, Zhaoxian Wu, Qing Ling, Tianyi Chen, Information Sciences. 616Jie Peng, Zhaoxian Wu, Qing Ling, and Tianyi Chen. Byzantine- robust variance-reduced federated learning over distributed non-iid data. Information Sciences, 616:367-391, 2022.\n\nCong Xie, Oluwasanmi Koyejo, Indranil Gupta, Zeno, arXiv:1805.10032Byzantinesuspicious stochastic gradient descent. arXiv preprintCong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Zeno: Byzantine- suspicious stochastic gradient descent. arXiv preprint arXiv: 1805.10032, 2018.\n\nLearning to detect malicious clients for robust federated learning. Suyi Li, Yong Cheng, Wei Wang, Yang Liu, Tianjian Chen, arXiv:2002.00211arXiv preprintSuyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen. Learning to detect malicious clients for robust federated learning. arXiv preprint arXiv:2002.00211, 2020.\n\nDistributed subgradient methods for multi-agent optimization. Angelia Nedic, Asuman Ozdaglar, IEEE Transactions on Automatic Control. 541Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48-61, 2009.\n\nCan decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, Ji Liu, Advances in Neural Information Processing Systems. Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 5336-5346, 2017.\n\nFault-tolerant distributed optimization. Lili Su, Nitin H Vaidya, arXiv:1511.01821Constrained optimization with arbitrary directed networks. arXiv preprintpart IVLili Su and Nitin H Vaidya. Fault-tolerant distributed optimization (part IV): Constrained optimization with arbitrary directed networks. arXiv preprint arXiv: 1511.01821, 2015.\n\nByzantine-resilient multiagent optimization. Lili Su, Nitin H Vaidya, IEEE Transactions on Automatic Control. 665Lili Su and Nitin H Vaidya. Byzantine-resilient multiagent optimization. IEEE Transactions on Automatic Control, 66(5):2227-2233, 2020.\n\nBRIDGE: Byzantine-resilient decentralized gradient descent. Cheng Fang, Zhixiong Yang, Waheed U Bajwa, IEEE Transactions on Signal and Information Processing over Networks. 8Cheng Fang, Zhixiong Yang, and Waheed U Bajwa. BRIDGE: Byzantine-resilient decentralized gradient descent. IEEE Transactions on Signal and Information Processing over Networks, 8:610-626, 2022.\n\nByRDiE: Byzantine-resilient distributed coordinate descent for decentralized learning. Zhixiong Yang, U Waheed, Bajwa, IEEE Transactions on Signal and Information Processing over Networks. 54Zhixiong Yang and Waheed U Bajwa. ByRDiE: Byzantine-resilient distributed coordinate descent for decentralized learning. IEEE Trans- actions on Signal and Information Processing over Networks, 5(4):611- 627, 2019.\n\nCollaborative learning in the jungle (decentralized, Byzantine, heterogeneous, asynchronous and nonconvex learning). Sadegh El Mahdi El-Mhamdi, Rachid Farhadkhani, Arsany Guerraoui, Guirguis, Advances in Neural Information Processing Systems. L\u00ea-Nguy\u00ean Hoang, and S\u00e9bastien RouaultEl Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Arsany Guirguis, L\u00ea-Nguy\u00ean Hoang, and S\u00e9bastien Rouault. Collaborative learning in the jungle (decentralized, Byzantine, heterogeneous, asyn- chronous and nonconvex learning). Advances in Neural Information Processing Systems, pages 25044-25057, 2021.\n\nByzantine-robust decentralized stochastic optimization over static and time-varying networks. Jie Peng, Weiyu Li, Qing Ling, Signal Processing. 183108020Jie Peng, Weiyu Li, and Qing Ling. Byzantine-robust decentralized stochastic optimization over static and time-varying networks. Signal Processing, 183:108020, 2021.\n\nByzantine-robust decentralized learning via self-centered clipping. Lie He, Martin Sai Praneeth Karimireddy, Jaggi, arXiv:2202.01545arXiv preprintLie He, Sai Praneeth Karimireddy, and Martin Jaggi. Byzantine-robust decentralized learning via self-centered clipping. arXiv preprint arXiv: 2202.01545, 2022.\n\nByzantine-resilient decentralized stochastic gradient descent. Shangwei Guo, Tianwei Zhang, Han Yu, Xiaofei Xie, Lei Ma, Tao Xiang, Yang Liu, IEEE Transactions on Circuits and Systems for Video Technology. Shangwei Guo, Tianwei Zhang, Han Yu, Xiaofei Xie, Lei Ma, Tao Xiang, and Yang Liu. Byzantine-resilient decentralized stochastic gradient descent. IEEE Transactions on Circuits and Systems for Video Technology, 2021.\n\nByzantine-resilient decentralized collaborative learning. Jian Xu, Shao-Lun Huang, International Conference on Acoustics, Speech, and Signal Processing. Jian Xu and Shao-Lun Huang. Byzantine-resilient decentralized collab- orative learning. In International Conference on Acoustics, Speech, and Signal Processing, pages 5253-5257, 2022.\n\nDiscovering network topology in the presence of Byzantine faults. Mikhail Nesterenko, S\u00e9bastien Tixeuil, IEEE Transactions on Parallel and Distributed Systems. 2012Mikhail Nesterenko and S\u00e9bastien Tixeuil. Discovering network topol- ogy in the presence of Byzantine faults. IEEE Transactions on Parallel and Distributed Systems, 20(12):1777-1789, 2009.\n\nFast linear iterations for distributed averaging. Lin Xiao, Stephen Boyd, Systems & Control Letters. 531Lin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control Letters, 53(1):65-78, 2004.\n\nA Roger, Charles R Johnson Horn, Matrix Analysis. Cambridge University PressRoger A Horn and Charles R Johnson. Matrix Analysis. Cambridge University Press, 2012.\n\nFinite-time error bounds for distributed linear stochastic approximation. Yixuan Lin, Vijay Gupta, Ji Liu, arXiv:2111.12665arXiv preprintYixuan Lin, Vijay Gupta, and Ji Liu. Finite-time error bounds for distributed linear stochastic approximation. arXiv preprint arXiv: 2111.12665, 2021.\n\nDistributed optimization over timevarying directed graphs. Angelia Nedi\u0107, Alex Olshevsky, IEEE Transactions on Automatic Control. 603Angelia Nedi\u0107 and Alex Olshevsky. Distributed optimization over time- varying directed graphs. IEEE Transactions on Automatic Control, 60(3):601-615, 2014.\n\nPush-pull gradient methods for distributed optimization in networks. Shi Pu, Wei Shi, Jinming Xu, Angelia Nedi\u0107, IEEE Transactions on Automatic Control. 661Shi Pu, Wei Shi, Jinming Xu, and Angelia Nedi\u0107. Push-pull gradient methods for distributed optimization in networks. IEEE Transactions on Automatic Control, 66(1):1-16, 2020.\n\nOn the convergence of decentralized gradient descent. Kun Yuan, Qing Ling, Wotao Yin, SIAM Journal on Optimization. 263Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentral- ized gradient descent. SIAM Journal on Optimization, 26(3):1835-1854, 2016.\n\nA unified theory of decentralized SGD with changing topology and local updates. Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, Sebastian Stich, International Conference on Machine Learning. Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory of decentralized SGD with changing topology and local updates. In International Conference on Machine Learning, pages 5381-5393, 2020.\n\nExponential graph is provably efficient for decentralized deep training. Bicheng Ying, Kun Yuan, Yiming Chen, Hanbin Hu, Pan Pan, Wotao Yin, Advances in Neural Information Processing Systems. 34Bicheng Ying, Kun Yuan, Yiming Chen, Hanbin Hu, Pan Pan, and Wotao Yin. Exponential graph is provably efficient for decentralized deep training. Advances in Neural Information Processing Systems, 34:13975-13987, 2021.\n\nNetwork topology and communication-computation tradeoffs in decentralized optimization. Angelia Nedic, Alex Olshevsky, Michael G Rabbat, Proceedings of the IEEE. 1065Angelia Nedic, Alex Olshevsky, and Michael G Rabbat. Network topology and communication-computation tradeoffs in decentralized optimization. Proceedings of the IEEE, 106(5):953-976, 2018.\n\nFall of empires: Breaking Byzantine-tolerant SGD by inner product manipulation. Cong Xie, Oluwasanmi Koyejo, Indranil Gupta, Uncertainty in Artificial Intelligence. Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking Byzantine-tolerant SGD by inner product manipulation. In Uncertainty in Artificial Intelligence, pages 261-270, 2020.\n\nA little is enough: Circumventing defenses for distributed learning. Gilad Baruch, Moran Baruch, Yoav Goldberg, Advances in Neural Information Processing Systems. 32Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed learning. Advances in Neural Information Processing Systems, 32, 2019.\n", "annotations": {"author": "[{\"end\":103,\"start\":91},{\"end\":116,\"start\":104},{\"end\":127,\"start\":117}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":100},{\"end\":115,\"start\":111},{\"end\":126,\"start\":122}]", "author_first_name": "[{\"end\":99,\"start\":91},{\"end\":110,\"start\":104},{\"end\":121,\"start\":117}]", "author_affiliation": null, "title": "[{\"end\":88,\"start\":1},{\"end\":215,\"start\":128}]", "venue": null, "abstract": "[{\"end\":1461,\"start\":314}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3261,\"start\":3258},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3266,\"start\":3263},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4197,\"start\":4194},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4202,\"start\":4199},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4952,\"start\":4949},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4974,\"start\":4971},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4979,\"start\":4976},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4997,\"start\":4994},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5008,\"start\":5004},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5021,\"start\":5017},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5032,\"start\":5028},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5061,\"start\":5057},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5464,\"start\":5460},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5470,\"start\":5466},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5488,\"start\":5484},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5963,\"start\":5959},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6097,\"start\":6093},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6138,\"start\":6134},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6144,\"start\":6140},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6311,\"start\":6307},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6317,\"start\":6313},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6520,\"start\":6516},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6526,\"start\":6522},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7119,\"start\":7115},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7125,\"start\":7121},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7292,\"start\":7288},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7474,\"start\":7470},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7543,\"start\":7539},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7707,\"start\":7703},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7773,\"start\":7769},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11079,\"start\":11075},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11085,\"start\":11081},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12573,\"start\":12570},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12788,\"start\":12784},{\"end\":12980,\"start\":12978},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13293,\"start\":13290},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15465,\"start\":15462},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15709,\"start\":15706},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15714,\"start\":15711},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15766,\"start\":15763},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15903,\"start\":15899},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17153,\"start\":17149},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17580,\"start\":17576},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17675,\"start\":17671},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17876,\"start\":17872},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18231,\"start\":18227},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19598,\"start\":19594},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20939,\"start\":20938},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22848,\"start\":22845},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22871,\"start\":22868},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24487,\"start\":24483},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25452,\"start\":25448},{\"end\":25463,\"start\":25452},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26026,\"start\":26023},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26441,\"start\":26437},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26480,\"start\":26476},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26499,\"start\":26495},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26755,\"start\":26751},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28892,\"start\":28888},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30489,\"start\":30485},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32095,\"start\":32091},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32101,\"start\":32097},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32119,\"start\":32115},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34318,\"start\":34314},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34351,\"start\":34347},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":35549,\"start\":35545},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":36043,\"start\":36039},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36598,\"start\":36594},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36995,\"start\":36992},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":37000,\"start\":36997},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37028,\"start\":37024},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":38150,\"start\":38146},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39866,\"start\":39865},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":40105,\"start\":40101},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":40200,\"start\":40199},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":40431,\"start\":40427},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":40884,\"start\":40880},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":40890,\"start\":40886},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":41729,\"start\":41725},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":41735,\"start\":41731},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":41779,\"start\":41775},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41810,\"start\":41806},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":41925,\"start\":41921},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41931,\"start\":41927},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":42037,\"start\":42033},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":42288,\"start\":42284},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":42633,\"start\":42629},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":43541,\"start\":43537},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":49923,\"start\":49919},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":52579,\"start\":52575},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":55120,\"start\":55116},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":59924,\"start\":59921},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":59944,\"start\":59940},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":59950,\"start\":59946},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":60292,\"start\":60288},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":60753,\"start\":60749},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":61169,\"start\":61166},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":61440,\"start\":61436},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":61602,\"start\":61598},{\"end\":62705,\"start\":62702},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":70177,\"start\":70176},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":73211,\"start\":73207},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":73376,\"start\":73373},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":73521,\"start\":73520},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":73880,\"start\":73876}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":64609,\"start\":64379},{\"attributes\":{\"id\":\"fig_4\"},\"end\":64694,\"start\":64610},{\"attributes\":{\"id\":\"fig_5\"},\"end\":64839,\"start\":64695},{\"attributes\":{\"id\":\"fig_6\"},\"end\":65176,\"start\":64840},{\"attributes\":{\"id\":\"fig_7\"},\"end\":65506,\"start\":65177},{\"attributes\":{\"id\":\"fig_8\"},\"end\":65798,\"start\":65507},{\"attributes\":{\"id\":\"fig_9\"},\"end\":65831,\"start\":65799},{\"attributes\":{\"id\":\"fig_10\"},\"end\":66020,\"start\":65832},{\"attributes\":{\"id\":\"fig_11\"},\"end\":66189,\"start\":66021},{\"attributes\":{\"id\":\"fig_14\"},\"end\":66254,\"start\":66190},{\"attributes\":{\"id\":\"fig_15\"},\"end\":66304,\"start\":66255},{\"attributes\":{\"id\":\"fig_17\"},\"end\":66375,\"start\":66305},{\"attributes\":{\"id\":\"fig_18\"},\"end\":66515,\"start\":66376},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":66724,\"start\":66516},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":67997,\"start\":66725},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":69286,\"start\":67998},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":70106,\"start\":69287},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":70362,\"start\":70107},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":71635,\"start\":70363},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":72932,\"start\":71636}]", "paragraph": "[{\"end\":2862,\"start\":1552},{\"end\":3683,\"start\":2882},{\"end\":4595,\"start\":3685},{\"end\":5780,\"start\":4597},{\"end\":6145,\"start\":5782},{\"end\":6359,\"start\":6147},{\"end\":7065,\"start\":6361},{\"end\":8212,\"start\":7067},{\"end\":9293,\"start\":8214},{\"end\":10114,\"start\":9359},{\"end\":10283,\"start\":10116},{\"end\":10792,\"start\":10371},{\"end\":10882,\"start\":10831},{\"end\":11363,\"start\":10884},{\"end\":11999,\"start\":11427},{\"end\":12206,\"start\":12049},{\"end\":12371,\"start\":12256},{\"end\":12789,\"start\":12373},{\"end\":12944,\"start\":12843},{\"end\":13030,\"start\":12946},{\"end\":13046,\"start\":13037},{\"end\":13149,\"start\":13095},{\"end\":13189,\"start\":13151},{\"end\":13427,\"start\":13191},{\"end\":13694,\"start\":13513},{\"end\":14112,\"start\":13745},{\"end\":14271,\"start\":14160},{\"end\":14845,\"start\":14358},{\"end\":15136,\"start\":14847},{\"end\":15488,\"start\":15138},{\"end\":16796,\"start\":15599},{\"end\":17248,\"start\":16843},{\"end\":18026,\"start\":17296},{\"end\":18201,\"start\":18112},{\"end\":19105,\"start\":18203},{\"end\":19994,\"start\":19159},{\"end\":20030,\"start\":19996},{\"end\":20098,\"start\":20086},{\"end\":20154,\"start\":20152},{\"end\":20186,\"start\":20156},{\"end\":20278,\"start\":20222},{\"end\":20941,\"start\":20357},{\"end\":21273,\"start\":20943},{\"end\":21439,\"start\":21341},{\"end\":21795,\"start\":21441},{\"end\":22083,\"start\":21797},{\"end\":22366,\"start\":22085},{\"end\":22960,\"start\":22479},{\"end\":23309,\"start\":23201},{\"end\":23553,\"start\":23474},{\"end\":23896,\"start\":23555},{\"end\":24384,\"start\":23947},{\"end\":24845,\"start\":24386},{\"end\":24935,\"start\":24876},{\"end\":25035,\"start\":24978},{\"end\":25135,\"start\":25098},{\"end\":25287,\"start\":25251},{\"end\":25655,\"start\":25327},{\"end\":26168,\"start\":25702},{\"end\":27062,\"start\":26170},{\"end\":27218,\"start\":27064},{\"end\":27486,\"start\":27267},{\"end\":27716,\"start\":27606},{\"end\":27888,\"start\":27718},{\"end\":28129,\"start\":27941},{\"end\":28344,\"start\":28174},{\"end\":28733,\"start\":28346},{\"end\":29012,\"start\":28735},{\"end\":29392,\"start\":29300},{\"end\":30137,\"start\":29670},{\"end\":30263,\"start\":30152},{\"end\":30605,\"start\":30265},{\"end\":30722,\"start\":30713},{\"end\":31966,\"start\":30743},{\"end\":32238,\"start\":31968},{\"end\":32650,\"start\":32240},{\"end\":33055,\"start\":32652},{\"end\":33306,\"start\":33057},{\"end\":33458,\"start\":33308},{\"end\":33628,\"start\":33558},{\"end\":33745,\"start\":33643},{\"end\":34112,\"start\":33812},{\"end\":34813,\"start\":34148},{\"end\":34943,\"start\":34850},{\"end\":35247,\"start\":35028},{\"end\":36384,\"start\":35301},{\"end\":37030,\"start\":36386},{\"end\":37447,\"start\":37183},{\"end\":37605,\"start\":37449},{\"end\":37727,\"start\":37607},{\"end\":37855,\"start\":37794},{\"end\":38151,\"start\":37891},{\"end\":38474,\"start\":38183},{\"end\":38601,\"start\":38476},{\"end\":38774,\"start\":38603},{\"end\":39071,\"start\":38776},{\"end\":39167,\"start\":39123},{\"end\":39218,\"start\":39217},{\"end\":39326,\"start\":39220},{\"end\":40003,\"start\":39373},{\"end\":40806,\"start\":40041},{\"end\":40940,\"start\":40808},{\"end\":42449,\"start\":41188},{\"end\":43722,\"start\":42451},{\"end\":44617,\"start\":43724},{\"end\":45044,\"start\":44619},{\"end\":45514,\"start\":45046},{\"end\":45869,\"start\":45516},{\"end\":46719,\"start\":45871},{\"end\":47207,\"start\":46721},{\"end\":47440,\"start\":47209},{\"end\":47604,\"start\":47469},{\"end\":47812,\"start\":47701},{\"end\":48104,\"start\":48057},{\"end\":48379,\"start\":48323},{\"end\":48438,\"start\":48381},{\"end\":48462,\"start\":48458},{\"end\":48970,\"start\":48740},{\"end\":49263,\"start\":49219},{\"end\":49514,\"start\":49471},{\"end\":49630,\"start\":49587},{\"end\":49788,\"start\":49666},{\"end\":49971,\"start\":49885},{\"end\":50328,\"start\":50207},{\"end\":50487,\"start\":50475},{\"end\":50770,\"start\":50715},{\"end\":51072,\"start\":51010},{\"end\":51378,\"start\":51312},{\"end\":52139,\"start\":51679},{\"end\":52582,\"start\":52173},{\"end\":52675,\"start\":52643},{\"end\":52734,\"start\":52677},{\"end\":52776,\"start\":52736},{\"end\":52957,\"start\":52917},{\"end\":53112,\"start\":53000},{\"end\":53452,\"start\":53267},{\"end\":53707,\"start\":53605},{\"end\":53847,\"start\":53790},{\"end\":54350,\"start\":54133},{\"end\":54466,\"start\":54443},{\"end\":54563,\"start\":54524},{\"end\":54701,\"start\":54636},{\"end\":54916,\"start\":54813},{\"end\":54983,\"start\":54957},{\"end\":55121,\"start\":55017},{\"end\":55265,\"start\":55123},{\"end\":55415,\"start\":55359},{\"end\":55459,\"start\":55447},{\"end\":55618,\"start\":55610},{\"end\":55868,\"start\":55789},{\"end\":55984,\"start\":55945},{\"end\":56173,\"start\":56112},{\"end\":56396,\"start\":56235},{\"end\":56427,\"start\":56398},{\"end\":56599,\"start\":56477},{\"end\":57073,\"start\":56964},{\"end\":57278,\"start\":57263},{\"end\":57406,\"start\":57376},{\"end\":57573,\"start\":57530},{\"end\":57938,\"start\":57755},{\"end\":58045,\"start\":57940},{\"end\":58153,\"start\":58115},{\"end\":58375,\"start\":58277},{\"end\":58605,\"start\":58604},{\"end\":58693,\"start\":58607},{\"end\":58849,\"start\":58808},{\"end\":59071,\"start\":58991},{\"end\":59364,\"start\":59297},{\"end\":59509,\"start\":59414},{\"end\":59675,\"start\":59608},{\"end\":59757,\"start\":59731},{\"end\":59925,\"start\":59808},{\"end\":60274,\"start\":59927},{\"end\":60329,\"start\":60276},{\"end\":60618,\"start\":60471},{\"end\":60935,\"start\":60686},{\"end\":61022,\"start\":60970},{\"end\":61272,\"start\":61102},{\"end\":61319,\"start\":61274},{\"end\":61347,\"start\":61321},{\"end\":61584,\"start\":61349},{\"end\":61924,\"start\":61586},{\"end\":61943,\"start\":61926},{\"end\":62088,\"start\":61989},{\"end\":62194,\"start\":62117},{\"end\":62300,\"start\":62196},{\"end\":62349,\"start\":62320},{\"end\":62460,\"start\":62351},{\"end\":62611,\"start\":62519},{\"end\":62759,\"start\":62640},{\"end\":62934,\"start\":62761},{\"end\":63034,\"start\":63015},{\"end\":63178,\"start\":63151},{\"end\":63426,\"start\":63348},{\"end\":63742,\"start\":63670},{\"end\":64065,\"start\":64001},{\"end\":64208,\"start\":64094},{\"end\":64319,\"start\":64210}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10332,\"start\":10284},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10370,\"start\":10332},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10830,\"start\":10793},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11426,\"start\":11364},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12048,\"start\":12000},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12255,\"start\":12207},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13094,\"start\":13047},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13512,\"start\":13428},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14357,\"start\":14272},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15598,\"start\":15489},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17295,\"start\":17249},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18111,\"start\":18027},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20085,\"start\":20031},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20151,\"start\":20099},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20221,\"start\":20187},{\"attributes\":{\"id\":\"formula_15\"},\"end\":20333,\"start\":20279},{\"attributes\":{\"id\":\"formula_16\"},\"end\":20356,\"start\":20333},{\"attributes\":{\"id\":\"formula_17\"},\"end\":21340,\"start\":21274},{\"attributes\":{\"id\":\"formula_18\"},\"end\":22478,\"start\":22367},{\"attributes\":{\"id\":\"formula_19\"},\"end\":23018,\"start\":22961},{\"attributes\":{\"id\":\"formula_20\"},\"end\":23200,\"start\":23018},{\"attributes\":{\"id\":\"formula_21\"},\"end\":23473,\"start\":23310},{\"attributes\":{\"id\":\"formula_22\"},\"end\":24875,\"start\":24846},{\"attributes\":{\"id\":\"formula_23\"},\"end\":24977,\"start\":24936},{\"attributes\":{\"id\":\"formula_24\"},\"end\":25097,\"start\":25036},{\"attributes\":{\"id\":\"formula_25\"},\"end\":25197,\"start\":25136},{\"attributes\":{\"id\":\"formula_26\"},\"end\":25250,\"start\":25197},{\"attributes\":{\"id\":\"formula_27\"},\"end\":25326,\"start\":25288},{\"attributes\":{\"id\":\"formula_28\"},\"end\":25701,\"start\":25656},{\"attributes\":{\"id\":\"formula_29\"},\"end\":27605,\"start\":27487},{\"attributes\":{\"id\":\"formula_30\"},\"end\":27940,\"start\":27889},{\"attributes\":{\"id\":\"formula_31\"},\"end\":28173,\"start\":28130},{\"attributes\":{\"id\":\"formula_32\"},\"end\":29046,\"start\":29013},{\"attributes\":{\"id\":\"formula_33\"},\"end\":29299,\"start\":29046},{\"attributes\":{\"id\":\"formula_34\"},\"end\":29669,\"start\":29393},{\"attributes\":{\"id\":\"formula_35\"},\"end\":30151,\"start\":30138},{\"attributes\":{\"id\":\"formula_36\"},\"end\":30640,\"start\":30606},{\"attributes\":{\"id\":\"formula_37\"},\"end\":30712,\"start\":30640},{\"attributes\":{\"id\":\"formula_38\"},\"end\":30742,\"start\":30723},{\"attributes\":{\"id\":\"formula_39\"},\"end\":33557,\"start\":33459},{\"attributes\":{\"id\":\"formula_40\"},\"end\":33642,\"start\":33629},{\"attributes\":{\"id\":\"formula_41\"},\"end\":33811,\"start\":33746},{\"attributes\":{\"id\":\"formula_42\"},\"end\":34147,\"start\":34113},{\"attributes\":{\"id\":\"formula_43\"},\"end\":34849,\"start\":34814},{\"attributes\":{\"id\":\"formula_44\"},\"end\":37182,\"start\":37031},{\"attributes\":{\"id\":\"formula_45\"},\"end\":37793,\"start\":37728},{\"attributes\":{\"id\":\"formula_46\"},\"end\":37890,\"start\":37856},{\"attributes\":{\"id\":\"formula_48\"},\"end\":39122,\"start\":39072},{\"attributes\":{\"id\":\"formula_49\"},\"end\":39216,\"start\":39168},{\"attributes\":{\"id\":\"formula_51\"},\"end\":39372,\"start\":39327},{\"attributes\":{\"id\":\"formula_52\"},\"end\":40040,\"start\":40004},{\"attributes\":{\"id\":\"formula_53\"},\"end\":41158,\"start\":40941},{\"attributes\":{\"id\":\"formula_54\"},\"end\":47700,\"start\":47605},{\"attributes\":{\"id\":\"formula_55\"},\"end\":47847,\"start\":47813},{\"attributes\":{\"id\":\"formula_56\"},\"end\":48056,\"start\":47847},{\"attributes\":{\"id\":\"formula_57\"},\"end\":48176,\"start\":48105},{\"attributes\":{\"id\":\"formula_58\"},\"end\":48322,\"start\":48176},{\"attributes\":{\"id\":\"formula_59\"},\"end\":48457,\"start\":48439},{\"attributes\":{\"id\":\"formula_60\"},\"end\":48739,\"start\":48463},{\"attributes\":{\"id\":\"formula_61\"},\"end\":49218,\"start\":48971},{\"attributes\":{\"id\":\"formula_62\"},\"end\":49470,\"start\":49264},{\"attributes\":{\"id\":\"formula_63\"},\"end\":49586,\"start\":49515},{\"attributes\":{\"id\":\"formula_64\"},\"end\":49665,\"start\":49631},{\"attributes\":{\"id\":\"formula_65\"},\"end\":49884,\"start\":49789},{\"attributes\":{\"id\":\"formula_66\"},\"end\":50206,\"start\":49972},{\"attributes\":{\"id\":\"formula_67\"},\"end\":50423,\"start\":50329},{\"attributes\":{\"id\":\"formula_68\"},\"end\":50474,\"start\":50423},{\"attributes\":{\"id\":\"formula_69\"},\"end\":50539,\"start\":50488},{\"attributes\":{\"id\":\"formula_70\"},\"end\":50714,\"start\":50539},{\"attributes\":{\"id\":\"formula_71\"},\"end\":51009,\"start\":50771},{\"attributes\":{\"id\":\"formula_72\"},\"end\":51311,\"start\":51073},{\"attributes\":{\"id\":\"formula_73\"},\"end\":51678,\"start\":51379},{\"attributes\":{\"id\":\"formula_74\"},\"end\":52642,\"start\":52583},{\"attributes\":{\"id\":\"formula_75\"},\"end\":52916,\"start\":52777},{\"attributes\":{\"id\":\"formula_76\"},\"end\":52999,\"start\":52958},{\"attributes\":{\"id\":\"formula_77\"},\"end\":53266,\"start\":53113},{\"attributes\":{\"id\":\"formula_78\"},\"end\":53493,\"start\":53453},{\"attributes\":{\"id\":\"formula_79\"},\"end\":53604,\"start\":53493},{\"attributes\":{\"id\":\"formula_80\"},\"end\":53789,\"start\":53708},{\"attributes\":{\"id\":\"formula_81\"},\"end\":53866,\"start\":53848},{\"attributes\":{\"id\":\"formula_82\"},\"end\":54132,\"start\":53866},{\"attributes\":{\"id\":\"formula_83\"},\"end\":54442,\"start\":54351},{\"attributes\":{\"id\":\"formula_84\"},\"end\":54523,\"start\":54467},{\"attributes\":{\"id\":\"formula_85\"},\"end\":54635,\"start\":54564},{\"attributes\":{\"id\":\"formula_86\"},\"end\":54812,\"start\":54702},{\"attributes\":{\"id\":\"formula_87\"},\"end\":54956,\"start\":54917},{\"attributes\":{\"id\":\"formula_88\"},\"end\":55358,\"start\":55266},{\"attributes\":{\"id\":\"formula_89\"},\"end\":55446,\"start\":55416},{\"attributes\":{\"id\":\"formula_90\"},\"end\":55609,\"start\":55460},{\"attributes\":{\"id\":\"formula_91\"},\"end\":55788,\"start\":55619},{\"attributes\":{\"id\":\"formula_92\"},\"end\":55944,\"start\":55869},{\"attributes\":{\"id\":\"formula_93\"},\"end\":56111,\"start\":55985},{\"attributes\":{\"id\":\"formula_94\"},\"end\":56234,\"start\":56174},{\"attributes\":{\"id\":\"formula_95\"},\"end\":56476,\"start\":56428},{\"attributes\":{\"id\":\"formula_96\"},\"end\":56963,\"start\":56600},{\"attributes\":{\"id\":\"formula_97\"},\"end\":57145,\"start\":57074},{\"attributes\":{\"id\":\"formula_98\"},\"end\":57262,\"start\":57145},{\"attributes\":{\"id\":\"formula_99\"},\"end\":57375,\"start\":57279},{\"attributes\":{\"id\":\"formula_100\"},\"end\":57529,\"start\":57407},{\"attributes\":{\"id\":\"formula_101\"},\"end\":57754,\"start\":57574},{\"attributes\":{\"id\":\"formula_102\"},\"end\":58114,\"start\":58046},{\"attributes\":{\"id\":\"formula_103\"},\"end\":58276,\"start\":58154},{\"attributes\":{\"id\":\"formula_104\"},\"end\":58603,\"start\":58376},{\"attributes\":{\"id\":\"formula_105\"},\"end\":58807,\"start\":58694},{\"attributes\":{\"id\":\"formula_106\"},\"end\":58990,\"start\":58850},{\"attributes\":{\"id\":\"formula_107\"},\"end\":59296,\"start\":59072},{\"attributes\":{\"id\":\"formula_108\"},\"end\":59413,\"start\":59365},{\"attributes\":{\"id\":\"formula_109\"},\"end\":59607,\"start\":59510},{\"attributes\":{\"id\":\"formula_110\"},\"end\":59730,\"start\":59676},{\"attributes\":{\"id\":\"formula_111\"},\"end\":60470,\"start\":60330},{\"attributes\":{\"id\":\"formula_112\"},\"end\":60685,\"start\":60619},{\"attributes\":{\"id\":\"formula_113\"},\"end\":60969,\"start\":60936},{\"attributes\":{\"id\":\"formula_114\"},\"end\":61101,\"start\":61023},{\"attributes\":{\"id\":\"formula_118\"},\"end\":62319,\"start\":62301},{\"attributes\":{\"id\":\"formula_119\"},\"end\":62518,\"start\":62461},{\"attributes\":{\"id\":\"formula_120\"},\"end\":63014,\"start\":62935},{\"attributes\":{\"id\":\"formula_121\"},\"end\":63100,\"start\":63035},{\"attributes\":{\"id\":\"formula_122\"},\"end\":63150,\"start\":63100},{\"attributes\":{\"id\":\"formula_123\"},\"end\":63347,\"start\":63179},{\"attributes\":{\"id\":\"formula_124\"},\"end\":63669,\"start\":63427},{\"attributes\":{\"id\":\"formula_125\"},\"end\":64000,\"start\":63743},{\"attributes\":{\"id\":\"formula_127\"},\"end\":64379,\"start\":64320}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":35841,\"start\":35834},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36383,\"start\":36376},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36398,\"start\":36391},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":39968,\"start\":39961},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":43632,\"start\":43614},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":44112,\"start\":44105},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":46417,\"start\":46409}]", "section_header": "[{\"end\":1550,\"start\":1463},{\"end\":2880,\"start\":2865},{\"end\":9357,\"start\":9296},{\"end\":12841,\"start\":12792},{\"end\":13035,\"start\":13033},{\"end\":13743,\"start\":13697},{\"end\":14158,\"start\":14115},{\"end\":16841,\"start\":16799},{\"end\":19157,\"start\":19108},{\"end\":23945,\"start\":23899},{\"end\":27265,\"start\":27221},{\"end\":35026,\"start\":34946},{\"end\":35299,\"start\":35250},{\"end\":38181,\"start\":38154},{\"end\":41186,\"start\":41160},{\"end\":47467,\"start\":47443},{\"end\":52171,\"start\":52142},{\"end\":55015,\"start\":54986},{\"end\":59806,\"start\":59760},{\"end\":61987,\"start\":61946},{\"end\":62115,\"start\":62091},{\"end\":62638,\"start\":62614},{\"end\":64092,\"start\":64068},{\"end\":64388,\"start\":64380},{\"end\":64704,\"start\":64696},{\"end\":64852,\"start\":64841},{\"end\":65192,\"start\":65178},{\"end\":65521,\"start\":65508},{\"end\":65801,\"start\":65800},{\"end\":65841,\"start\":65833},{\"end\":66030,\"start\":66022},{\"end\":66257,\"start\":66256},{\"end\":66386,\"start\":66377},{\"end\":66531,\"start\":66517},{\"end\":66743,\"start\":66726},{\"end\":68017,\"start\":67999},{\"end\":69305,\"start\":69288},{\"end\":70111,\"start\":70108},{\"end\":70371,\"start\":70364},{\"end\":71654,\"start\":71637}]", "table": "[{\"end\":66724,\"start\":66686},{\"end\":67997,\"start\":66826},{\"end\":69286,\"start\":68105},{\"end\":70106,\"start\":69389},{\"end\":70362,\"start\":70225},{\"end\":71635,\"start\":70552},{\"end\":72932,\"start\":71801}]", "figure_caption": "[{\"end\":64609,\"start\":64390},{\"end\":64694,\"start\":64612},{\"end\":64839,\"start\":64706},{\"end\":65176,\"start\":64854},{\"end\":65506,\"start\":65194},{\"end\":65798,\"start\":65523},{\"end\":65831,\"start\":65802},{\"end\":66020,\"start\":65843},{\"end\":66189,\"start\":66032},{\"end\":66254,\"start\":66192},{\"end\":66304,\"start\":66258},{\"end\":66375,\"start\":66307},{\"end\":66515,\"start\":66388},{\"end\":66686,\"start\":66533},{\"end\":66826,\"start\":66746},{\"end\":68105,\"start\":68021},{\"end\":69389,\"start\":69308},{\"end\":70225,\"start\":70113},{\"end\":70552,\"start\":70373},{\"end\":71801,\"start\":71657}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16178,\"start\":16172},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22209,\"start\":22203},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":43327,\"start\":43321},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":46060,\"start\":46054},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":46995,\"start\":46989}]", "bib_author_first_name": "[{\"end\":73968,\"start\":73963},{\"end\":73985,\"start\":73978},{\"end\":74001,\"start\":73995},{\"end\":74015,\"start\":74010},{\"end\":74255,\"start\":74250},{\"end\":74272,\"start\":74265},{\"end\":74289,\"start\":74282},{\"end\":74305,\"start\":74297},{\"end\":74319,\"start\":74314},{\"end\":74877,\"start\":74868},{\"end\":74889,\"start\":74883},{\"end\":74899,\"start\":74894},{\"end\":75178,\"start\":75172},{\"end\":75194,\"start\":75188},{\"end\":75212,\"start\":75204},{\"end\":75604,\"start\":75596},{\"end\":75617,\"start\":75611},{\"end\":75632,\"start\":75624},{\"end\":76016,\"start\":76012},{\"end\":76028,\"start\":76022},{\"end\":76041,\"start\":76035},{\"end\":76060,\"start\":76055},{\"end\":76427,\"start\":76421},{\"end\":76438,\"start\":76434},{\"end\":76450,\"start\":76443},{\"end\":76759,\"start\":76755},{\"end\":76775,\"start\":76765},{\"end\":76792,\"start\":76784},{\"end\":77065,\"start\":77061},{\"end\":77081,\"start\":77071},{\"end\":77098,\"start\":77090},{\"end\":77373,\"start\":77369},{\"end\":77391,\"start\":77385},{\"end\":77418,\"start\":77412},{\"end\":77781,\"start\":77775},{\"end\":77811,\"start\":77802},{\"end\":78177,\"start\":78175},{\"end\":78187,\"start\":78183},{\"end\":78200,\"start\":78193},{\"end\":78209,\"start\":78206},{\"end\":78507,\"start\":78501},{\"end\":78527,\"start\":78518},{\"end\":78545,\"start\":78538},{\"end\":78557,\"start\":78554},{\"end\":78887,\"start\":78879},{\"end\":78896,\"start\":78892},{\"end\":78909,\"start\":78903},{\"end\":78926,\"start\":78916},{\"end\":79288,\"start\":79280},{\"end\":79307,\"start\":79299},{\"end\":79322,\"start\":79316},{\"end\":79339,\"start\":79331},{\"end\":79637,\"start\":79634},{\"end\":79670,\"start\":79664},{\"end\":79983,\"start\":79976},{\"end\":79995,\"start\":79990},{\"end\":80347,\"start\":80341},{\"end\":80355,\"start\":80352},{\"end\":80366,\"start\":80360},{\"end\":80374,\"start\":80373},{\"end\":80389,\"start\":80385},{\"end\":80768,\"start\":80765},{\"end\":80801,\"start\":80795},{\"end\":81136,\"start\":81133},{\"end\":81151,\"start\":81143},{\"end\":81160,\"start\":81156},{\"end\":81173,\"start\":81167},{\"end\":81387,\"start\":81383},{\"end\":81403,\"start\":81393},{\"end\":81420,\"start\":81412},{\"end\":81732,\"start\":81728},{\"end\":81741,\"start\":81737},{\"end\":81752,\"start\":81749},{\"end\":81763,\"start\":81759},{\"end\":81777,\"start\":81769},{\"end\":82051,\"start\":82044},{\"end\":82065,\"start\":82059},{\"end\":82416,\"start\":82409},{\"end\":82425,\"start\":82423},{\"end\":82437,\"start\":82433},{\"end\":82452,\"start\":82445},{\"end\":82463,\"start\":82460},{\"end\":82473,\"start\":82471},{\"end\":82860,\"start\":82856},{\"end\":82870,\"start\":82865},{\"end\":82872,\"start\":82871},{\"end\":83205,\"start\":83201},{\"end\":83215,\"start\":83210},{\"end\":83217,\"start\":83216},{\"end\":83471,\"start\":83466},{\"end\":83486,\"start\":83478},{\"end\":83501,\"start\":83493},{\"end\":83870,\"start\":83862},{\"end\":83878,\"start\":83877},{\"end\":84304,\"start\":84298},{\"end\":84331,\"start\":84325},{\"end\":84351,\"start\":84345},{\"end\":84870,\"start\":84867},{\"end\":84882,\"start\":84877},{\"end\":84891,\"start\":84887},{\"end\":85164,\"start\":85161},{\"end\":85175,\"start\":85169},{\"end\":85471,\"start\":85463},{\"end\":85484,\"start\":85477},{\"end\":85495,\"start\":85492},{\"end\":85507,\"start\":85500},{\"end\":85516,\"start\":85513},{\"end\":85524,\"start\":85521},{\"end\":85536,\"start\":85532},{\"end\":85885,\"start\":85881},{\"end\":85898,\"start\":85890},{\"end\":86234,\"start\":86227},{\"end\":86256,\"start\":86247},{\"end\":86568,\"start\":86565},{\"end\":86582,\"start\":86575},{\"end\":86744,\"start\":86743},{\"end\":86769,\"start\":86752},{\"end\":86987,\"start\":86981},{\"end\":86998,\"start\":86993},{\"end\":87008,\"start\":87006},{\"end\":87262,\"start\":87255},{\"end\":87274,\"start\":87270},{\"end\":87558,\"start\":87555},{\"end\":87566,\"start\":87563},{\"end\":87579,\"start\":87572},{\"end\":87591,\"start\":87584},{\"end\":87875,\"start\":87872},{\"end\":87886,\"start\":87882},{\"end\":87898,\"start\":87893},{\"end\":88172,\"start\":88163},{\"end\":88191,\"start\":88184},{\"end\":88205,\"start\":88200},{\"end\":88221,\"start\":88215},{\"end\":88238,\"start\":88229},{\"end\":88612,\"start\":88605},{\"end\":88622,\"start\":88619},{\"end\":88635,\"start\":88629},{\"end\":88648,\"start\":88642},{\"end\":88656,\"start\":88653},{\"end\":88667,\"start\":88662},{\"end\":89040,\"start\":89033},{\"end\":89052,\"start\":89048},{\"end\":89071,\"start\":89064},{\"end\":89073,\"start\":89072},{\"end\":89384,\"start\":89380},{\"end\":89400,\"start\":89390},{\"end\":89417,\"start\":89409},{\"end\":89733,\"start\":89728},{\"end\":89747,\"start\":89742},{\"end\":89760,\"start\":89756}]", "bib_author_last_name": "[{\"end\":73976,\"start\":73969},{\"end\":73993,\"start\":73986},{\"end\":74008,\"start\":74002},{\"end\":74025,\"start\":74016},{\"end\":74263,\"start\":74256},{\"end\":74280,\"start\":74273},{\"end\":74295,\"start\":74290},{\"end\":74312,\"start\":74306},{\"end\":74326,\"start\":74320},{\"end\":74881,\"start\":74878},{\"end\":74892,\"start\":74890},{\"end\":74904,\"start\":74900},{\"end\":75186,\"start\":75179},{\"end\":75202,\"start\":75195},{\"end\":75218,\"start\":75213},{\"end\":75609,\"start\":75605},{\"end\":75622,\"start\":75618},{\"end\":75638,\"start\":75633},{\"end\":76020,\"start\":76017},{\"end\":76033,\"start\":76029},{\"end\":76053,\"start\":76042},{\"end\":76069,\"start\":76061},{\"end\":76432,\"start\":76428},{\"end\":76441,\"start\":76439},{\"end\":76453,\"start\":76451},{\"end\":76763,\"start\":76760},{\"end\":76782,\"start\":76776},{\"end\":76798,\"start\":76793},{\"end\":77069,\"start\":77066},{\"end\":77088,\"start\":77082},{\"end\":77104,\"start\":77099},{\"end\":77383,\"start\":77374},{\"end\":77410,\"start\":77392},{\"end\":77428,\"start\":77419},{\"end\":77437,\"start\":77430},{\"end\":77800,\"start\":77782},{\"end\":77821,\"start\":77812},{\"end\":77830,\"start\":77823},{\"end\":78181,\"start\":78178},{\"end\":78191,\"start\":78188},{\"end\":78204,\"start\":78201},{\"end\":78212,\"start\":78210},{\"end\":78516,\"start\":78508},{\"end\":78536,\"start\":78528},{\"end\":78552,\"start\":78546},{\"end\":78566,\"start\":78558},{\"end\":78890,\"start\":78888},{\"end\":78901,\"start\":78897},{\"end\":78914,\"start\":78910},{\"end\":78936,\"start\":78927},{\"end\":79297,\"start\":79289},{\"end\":79314,\"start\":79308},{\"end\":79329,\"start\":79323},{\"end\":79348,\"start\":79340},{\"end\":79662,\"start\":79638},{\"end\":79673,\"start\":79671},{\"end\":79680,\"start\":79675},{\"end\":79988,\"start\":79984},{\"end\":80003,\"start\":79996},{\"end\":80350,\"start\":80348},{\"end\":80358,\"start\":80356},{\"end\":80371,\"start\":80367},{\"end\":80383,\"start\":80375},{\"end\":80399,\"start\":80390},{\"end\":80405,\"start\":80401},{\"end\":80793,\"start\":80769},{\"end\":80804,\"start\":80802},{\"end\":80811,\"start\":80806},{\"end\":81141,\"start\":81137},{\"end\":81154,\"start\":81152},{\"end\":81165,\"start\":81161},{\"end\":81178,\"start\":81174},{\"end\":81391,\"start\":81388},{\"end\":81410,\"start\":81404},{\"end\":81426,\"start\":81421},{\"end\":81432,\"start\":81428},{\"end\":81735,\"start\":81733},{\"end\":81747,\"start\":81742},{\"end\":81757,\"start\":81753},{\"end\":81767,\"start\":81764},{\"end\":81782,\"start\":81778},{\"end\":82057,\"start\":82052},{\"end\":82074,\"start\":82066},{\"end\":82421,\"start\":82417},{\"end\":82431,\"start\":82426},{\"end\":82443,\"start\":82438},{\"end\":82458,\"start\":82453},{\"end\":82469,\"start\":82464},{\"end\":82477,\"start\":82474},{\"end\":82863,\"start\":82861},{\"end\":82879,\"start\":82873},{\"end\":83208,\"start\":83206},{\"end\":83224,\"start\":83218},{\"end\":83476,\"start\":83472},{\"end\":83491,\"start\":83487},{\"end\":83507,\"start\":83502},{\"end\":83875,\"start\":83871},{\"end\":83885,\"start\":83879},{\"end\":83892,\"start\":83887},{\"end\":84323,\"start\":84305},{\"end\":84343,\"start\":84332},{\"end\":84361,\"start\":84352},{\"end\":84371,\"start\":84363},{\"end\":84875,\"start\":84871},{\"end\":84885,\"start\":84883},{\"end\":84896,\"start\":84892},{\"end\":85167,\"start\":85165},{\"end\":85200,\"start\":85176},{\"end\":85207,\"start\":85202},{\"end\":85475,\"start\":85472},{\"end\":85490,\"start\":85485},{\"end\":85498,\"start\":85496},{\"end\":85511,\"start\":85508},{\"end\":85519,\"start\":85517},{\"end\":85530,\"start\":85525},{\"end\":85540,\"start\":85537},{\"end\":85888,\"start\":85886},{\"end\":85904,\"start\":85899},{\"end\":86245,\"start\":86235},{\"end\":86264,\"start\":86257},{\"end\":86573,\"start\":86569},{\"end\":86587,\"start\":86583},{\"end\":86750,\"start\":86745},{\"end\":86774,\"start\":86770},{\"end\":86991,\"start\":86988},{\"end\":87004,\"start\":86999},{\"end\":87012,\"start\":87009},{\"end\":87268,\"start\":87263},{\"end\":87284,\"start\":87275},{\"end\":87561,\"start\":87559},{\"end\":87570,\"start\":87567},{\"end\":87582,\"start\":87580},{\"end\":87597,\"start\":87592},{\"end\":87880,\"start\":87876},{\"end\":87891,\"start\":87887},{\"end\":87902,\"start\":87899},{\"end\":88182,\"start\":88173},{\"end\":88198,\"start\":88192},{\"end\":88213,\"start\":88206},{\"end\":88227,\"start\":88222},{\"end\":88244,\"start\":88239},{\"end\":88617,\"start\":88613},{\"end\":88627,\"start\":88623},{\"end\":88640,\"start\":88636},{\"end\":88651,\"start\":88649},{\"end\":88660,\"start\":88657},{\"end\":88671,\"start\":88668},{\"end\":89046,\"start\":89041},{\"end\":89062,\"start\":89053},{\"end\":89080,\"start\":89074},{\"end\":89388,\"start\":89385},{\"end\":89407,\"start\":89401},{\"end\":89423,\"start\":89418},{\"end\":89740,\"start\":89734},{\"end\":89754,\"start\":89748},{\"end\":89769,\"start\":89761}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1610.02527\",\"id\":\"b0\"},\"end\":74248,\"start\":73883},{\"attributes\":{\"doi\":\"arXiv:1912.04977\",\"id\":\"b1\"},\"end\":74741,\"start\":74250},{\"attributes\":{\"doi\":\"arXiv:2108.00365\",\"id\":\"b2\"},\"end\":75138,\"start\":74743},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":55899582},\"end\":75435,\"start\":75140},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":211069157},\"end\":75936,\"start\":75437},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3708326},\"end\":76325,\"start\":75938},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":58534983},\"end\":76751,\"start\":76327},{\"attributes\":{\"doi\":\"arXiv:1802.10116\",\"id\":\"b7\"},\"end\":76990,\"start\":76753},{\"attributes\":{\"doi\":\"arXiv:1805.09682\",\"id\":\"b8\"},\"end\":77295,\"start\":76992},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":28527385},\"end\":77710,\"start\":77297},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3473997},\"end\":78075,\"start\":77712},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":199466325},\"end\":78499,\"start\":78077},{\"attributes\":{\"doi\":\"arXiv:2106.11257\",\"id\":\"b12\"},\"end\":78784,\"start\":78501},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":209516318},\"end\":79196,\"start\":78786},{\"attributes\":{\"doi\":\"arXiv:1912.04531\",\"id\":\"b14\"},\"end\":79575,\"start\":79198},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":229331835},\"end\":79908,\"start\":79577},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":218674420},\"end\":80232,\"start\":79910},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53250530},\"end\":80696,\"start\":80234},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":238856649},\"end\":81048,\"start\":80698},{\"attributes\":{\"id\":\"b19\"},\"end\":81381,\"start\":81050},{\"attributes\":{\"doi\":\"arXiv:1805.10032\",\"id\":\"b20\"},\"end\":81658,\"start\":81383},{\"attributes\":{\"doi\":\"arXiv:2002.00211\",\"id\":\"b21\"},\"end\":81980,\"start\":81660},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6489200},\"end\":82274,\"start\":81982},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1467846},\"end\":82813,\"start\":82276},{\"attributes\":{\"doi\":\"arXiv:1511.01821\",\"id\":\"b24\"},\"end\":83154,\"start\":82815},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":214721011},\"end\":83404,\"start\":83156},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":201309887},\"end\":83773,\"start\":83406},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":30921068},\"end\":84179,\"start\":83775},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":244772959},\"end\":84771,\"start\":84181},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":218613799},\"end\":85091,\"start\":84773},{\"attributes\":{\"doi\":\"arXiv:2202.01545\",\"id\":\"b30\"},\"end\":85398,\"start\":85093},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":239050490},\"end\":85821,\"start\":85400},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":249437696},\"end\":86159,\"start\":85823},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1026519},\"end\":86513,\"start\":86161},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6001203},\"end\":86741,\"start\":86515},{\"attributes\":{\"id\":\"b35\"},\"end\":86905,\"start\":86743},{\"attributes\":{\"doi\":\"arXiv:2111.12665\",\"id\":\"b36\"},\"end\":87194,\"start\":86907},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":8361755},\"end\":87484,\"start\":87196},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":212903457},\"end\":87816,\"start\":87486},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":18230932},\"end\":88081,\"start\":87818},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":214612476},\"end\":88530,\"start\":88083},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":239885941},\"end\":88943,\"start\":88532},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":9297955},\"end\":89298,\"start\":88945},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":73729245},\"end\":89657,\"start\":89300},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":62841522},\"end\":90000,\"start\":89659}]", "bib_title": "[{\"end\":75170,\"start\":75140},{\"end\":75594,\"start\":75437},{\"end\":76010,\"start\":75938},{\"end\":76419,\"start\":76327},{\"end\":77367,\"start\":77297},{\"end\":77773,\"start\":77712},{\"end\":78173,\"start\":78077},{\"end\":78877,\"start\":78786},{\"end\":79632,\"start\":79577},{\"end\":79974,\"start\":79910},{\"end\":80339,\"start\":80234},{\"end\":80763,\"start\":80698},{\"end\":81131,\"start\":81050},{\"end\":82042,\"start\":81982},{\"end\":82407,\"start\":82276},{\"end\":82854,\"start\":82815},{\"end\":83199,\"start\":83156},{\"end\":83464,\"start\":83406},{\"end\":83860,\"start\":83775},{\"end\":84296,\"start\":84181},{\"end\":84865,\"start\":84773},{\"end\":85461,\"start\":85400},{\"end\":85879,\"start\":85823},{\"end\":86225,\"start\":86161},{\"end\":86563,\"start\":86515},{\"end\":87253,\"start\":87196},{\"end\":87553,\"start\":87486},{\"end\":87870,\"start\":87818},{\"end\":88161,\"start\":88083},{\"end\":88603,\"start\":88532},{\"end\":89031,\"start\":88945},{\"end\":89378,\"start\":89300},{\"end\":89726,\"start\":89659}]", "bib_author": "[{\"end\":73978,\"start\":73963},{\"end\":73995,\"start\":73978},{\"end\":74010,\"start\":73995},{\"end\":74027,\"start\":74010},{\"end\":74265,\"start\":74250},{\"end\":74282,\"start\":74265},{\"end\":74297,\"start\":74282},{\"end\":74314,\"start\":74297},{\"end\":74328,\"start\":74314},{\"end\":74883,\"start\":74868},{\"end\":74894,\"start\":74883},{\"end\":74906,\"start\":74894},{\"end\":75188,\"start\":75172},{\"end\":75204,\"start\":75188},{\"end\":75220,\"start\":75204},{\"end\":75611,\"start\":75596},{\"end\":75624,\"start\":75611},{\"end\":75640,\"start\":75624},{\"end\":76022,\"start\":76012},{\"end\":76035,\"start\":76022},{\"end\":76055,\"start\":76035},{\"end\":76071,\"start\":76055},{\"end\":76434,\"start\":76421},{\"end\":76443,\"start\":76434},{\"end\":76455,\"start\":76443},{\"end\":76765,\"start\":76755},{\"end\":76784,\"start\":76765},{\"end\":76800,\"start\":76784},{\"end\":77071,\"start\":77061},{\"end\":77090,\"start\":77071},{\"end\":77106,\"start\":77090},{\"end\":77385,\"start\":77369},{\"end\":77412,\"start\":77385},{\"end\":77430,\"start\":77412},{\"end\":77439,\"start\":77430},{\"end\":77802,\"start\":77775},{\"end\":77823,\"start\":77802},{\"end\":77832,\"start\":77823},{\"end\":78183,\"start\":78175},{\"end\":78193,\"start\":78183},{\"end\":78206,\"start\":78193},{\"end\":78214,\"start\":78206},{\"end\":78518,\"start\":78501},{\"end\":78538,\"start\":78518},{\"end\":78554,\"start\":78538},{\"end\":78568,\"start\":78554},{\"end\":78892,\"start\":78879},{\"end\":78903,\"start\":78892},{\"end\":78916,\"start\":78903},{\"end\":78938,\"start\":78916},{\"end\":79299,\"start\":79280},{\"end\":79316,\"start\":79299},{\"end\":79331,\"start\":79316},{\"end\":79350,\"start\":79331},{\"end\":79664,\"start\":79634},{\"end\":79675,\"start\":79664},{\"end\":79682,\"start\":79675},{\"end\":79990,\"start\":79976},{\"end\":80005,\"start\":79990},{\"end\":80352,\"start\":80341},{\"end\":80360,\"start\":80352},{\"end\":80373,\"start\":80360},{\"end\":80385,\"start\":80373},{\"end\":80401,\"start\":80385},{\"end\":80407,\"start\":80401},{\"end\":80795,\"start\":80765},{\"end\":80806,\"start\":80795},{\"end\":80813,\"start\":80806},{\"end\":81143,\"start\":81133},{\"end\":81156,\"start\":81143},{\"end\":81167,\"start\":81156},{\"end\":81180,\"start\":81167},{\"end\":81393,\"start\":81383},{\"end\":81412,\"start\":81393},{\"end\":81428,\"start\":81412},{\"end\":81434,\"start\":81428},{\"end\":81737,\"start\":81728},{\"end\":81749,\"start\":81737},{\"end\":81759,\"start\":81749},{\"end\":81769,\"start\":81759},{\"end\":81784,\"start\":81769},{\"end\":82059,\"start\":82044},{\"end\":82076,\"start\":82059},{\"end\":82423,\"start\":82409},{\"end\":82433,\"start\":82423},{\"end\":82445,\"start\":82433},{\"end\":82460,\"start\":82445},{\"end\":82471,\"start\":82460},{\"end\":82479,\"start\":82471},{\"end\":82865,\"start\":82856},{\"end\":82881,\"start\":82865},{\"end\":83210,\"start\":83201},{\"end\":83226,\"start\":83210},{\"end\":83478,\"start\":83466},{\"end\":83493,\"start\":83478},{\"end\":83509,\"start\":83493},{\"end\":83877,\"start\":83862},{\"end\":83887,\"start\":83877},{\"end\":83894,\"start\":83887},{\"end\":84325,\"start\":84298},{\"end\":84345,\"start\":84325},{\"end\":84363,\"start\":84345},{\"end\":84373,\"start\":84363},{\"end\":84877,\"start\":84867},{\"end\":84887,\"start\":84877},{\"end\":84898,\"start\":84887},{\"end\":85169,\"start\":85161},{\"end\":85202,\"start\":85169},{\"end\":85209,\"start\":85202},{\"end\":85477,\"start\":85463},{\"end\":85492,\"start\":85477},{\"end\":85500,\"start\":85492},{\"end\":85513,\"start\":85500},{\"end\":85521,\"start\":85513},{\"end\":85532,\"start\":85521},{\"end\":85542,\"start\":85532},{\"end\":85890,\"start\":85881},{\"end\":85906,\"start\":85890},{\"end\":86247,\"start\":86227},{\"end\":86266,\"start\":86247},{\"end\":86575,\"start\":86565},{\"end\":86589,\"start\":86575},{\"end\":86752,\"start\":86743},{\"end\":86776,\"start\":86752},{\"end\":86993,\"start\":86981},{\"end\":87006,\"start\":86993},{\"end\":87014,\"start\":87006},{\"end\":87270,\"start\":87255},{\"end\":87286,\"start\":87270},{\"end\":87563,\"start\":87555},{\"end\":87572,\"start\":87563},{\"end\":87584,\"start\":87572},{\"end\":87599,\"start\":87584},{\"end\":87882,\"start\":87872},{\"end\":87893,\"start\":87882},{\"end\":87904,\"start\":87893},{\"end\":88184,\"start\":88163},{\"end\":88200,\"start\":88184},{\"end\":88215,\"start\":88200},{\"end\":88229,\"start\":88215},{\"end\":88246,\"start\":88229},{\"end\":88619,\"start\":88605},{\"end\":88629,\"start\":88619},{\"end\":88642,\"start\":88629},{\"end\":88653,\"start\":88642},{\"end\":88662,\"start\":88653},{\"end\":88673,\"start\":88662},{\"end\":89048,\"start\":89033},{\"end\":89064,\"start\":89048},{\"end\":89082,\"start\":89064},{\"end\":89390,\"start\":89380},{\"end\":89409,\"start\":89390},{\"end\":89425,\"start\":89409},{\"end\":89742,\"start\":89728},{\"end\":89756,\"start\":89742},{\"end\":89771,\"start\":89756}]", "bib_venue": "[{\"end\":73961,\"start\":73883},{\"end\":74392,\"start\":74344},{\"end\":74866,\"start\":74743},{\"end\":75273,\"start\":75220},{\"end\":75671,\"start\":75640},{\"end\":76115,\"start\":76071},{\"end\":76526,\"start\":76455},{\"end\":77059,\"start\":76992},{\"end\":77488,\"start\":77439},{\"end\":77876,\"start\":77832},{\"end\":78271,\"start\":78214},{\"end\":78620,\"start\":78584},{\"end\":78976,\"start\":78938},{\"end\":79278,\"start\":79198},{\"end\":79726,\"start\":79682},{\"end\":80055,\"start\":80005},{\"end\":80449,\"start\":80407},{\"end\":80865,\"start\":80813},{\"end\":81200,\"start\":81180},{\"end\":81497,\"start\":81450},{\"end\":81726,\"start\":81660},{\"end\":82114,\"start\":82076},{\"end\":82528,\"start\":82479},{\"end\":82954,\"start\":82897},{\"end\":83264,\"start\":83226},{\"end\":83577,\"start\":83509},{\"end\":83962,\"start\":83894},{\"end\":84422,\"start\":84373},{\"end\":84915,\"start\":84898},{\"end\":85159,\"start\":85093},{\"end\":85604,\"start\":85542},{\"end\":85974,\"start\":85906},{\"end\":86319,\"start\":86266},{\"end\":86614,\"start\":86589},{\"end\":86791,\"start\":86776},{\"end\":86979,\"start\":86907},{\"end\":87324,\"start\":87286},{\"end\":87637,\"start\":87599},{\"end\":87932,\"start\":87904},{\"end\":88290,\"start\":88246},{\"end\":88722,\"start\":88673},{\"end\":89105,\"start\":89082},{\"end\":89463,\"start\":89425},{\"end\":89820,\"start\":89771},{\"end\":74461,\"start\":74394}]"}}}, "year": 2023, "month": 12, "day": 17}