{"id": 237556679, "updated": "2023-02-24 17:00:49.594", "metadata": {"title": "GenNet framework: interpretable deep learning for predicting phenotypes from genetic data", "authors": "[{\"first\":\"Arno\",\"last\":\"van Hilten\",\"middle\":[]},{\"first\":\"Steven\",\"last\":\"Kushner\",\"middle\":[\"A.\"]},{\"first\":\"Manfred\",\"last\":\"Kayser\",\"middle\":[]},{\"first\":\"M.\",\"last\":\"Ikram\",\"middle\":[\"Arfan\"]},{\"first\":\"Hieab\",\"last\":\"Adams\",\"middle\":[\"H.\",\"H.\"]},{\"first\":\"Caroline\",\"last\":\"Klaver\",\"middle\":[\"C.\",\"W.\"]},{\"first\":\"Wiro\",\"last\":\"Niessen\",\"middle\":[\"J.\"]},{\"first\":\"Gennady\",\"last\":\"Roshchupkin\",\"middle\":[\"V.\"]}]", "venue": "Communications Biology", "journal": "Communications Biology", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Applying deep learning in population genomics is challenging because of computational issues and lack of interpretable models. Here, we propose GenNet, a novel open-source deep learning framework for predicting phenotypes from genetic variants. In this framework, interpretable and memory-efficient neural network architectures are constructed by embedding biologically knowledge from public databases, resulting in neural networks that contain only biologically plausible connections. We applied the framework to seventeen phenotypes and found well-replicated genes such as HERC2 and OCA2 for hair and eye color, and novel genes such as ZNF773 and PCNT for schizophrenia. Additionally, the framework identified ubiquitin mediated proteolysis, endocrine system and viral infectious diseases as most predictive biological pathways for schizophrenia. GenNet is a freely available, end-to-end deep learning framework that allows researchers to develop and use interpretable neural networks to obtain novel insights into the genetic architecture of complex traits and diseases.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "34535759", "pubmedcentral": "8448759", "dblp": null, "doi": "10.1038/s42003-021-02622-z"}}, "content": {"source": {"pdf_hash": "d7d2b3a408bb6b20bb189d921c560e7cc69f3300", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.nature.com/articles/s42003-021-02622-z.pdf", "status": "GOLD"}}, "grobid": {"id": "419aff802a70af85f026c023158305828490b230", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d7d2b3a408bb6b20bb189d921c560e7cc69f3300.txt", "contents": "\nGenNet framework: interpretable deep learning for predicting phenotypes from genetic data\n\n\nArno Van Hilten a.vanhilten@erasmusmc.nl \nDepartment of Radiology and Nuclear Medicine\nMedical Center\nRotterdamErasmus MCthe Netherlands\n\n\u2709 \nSteven A Kushner \nDepartment of Psychiatry\nMedical Center\nErasmus, RotterdamMCthe Netherlands\n\nManfred Kayser \nDepartment of Genetic Identification\nMedical Center\nRotterdamErasmus MCthe Netherlands\n\nM Arfan Ikram \nDepartment of Epidemiology\nMedical Center\nErasmus, RotterdamMCthe Netherlands\n\nHieab H H Adams \nDepartment of Radiology and Nuclear Medicine\nMedical Center\nRotterdamErasmus MCthe Netherlands\n\nDepartment of Clinical Genetics\nMedical Center\nRotterdamErasmus MCthe Netherlands\n\nCaroline C W Klaver \nDepartment of Epidemiology\nMedical Center\nErasmus, RotterdamMCthe Netherlands\n\nDepartment of Ophthalmology\nMedical Center\nErasmus, RotterdamMCthe Netherlands\n\nWiro J Niessen \nDepartment of Radiology and Nuclear Medicine\nMedical Center\nRotterdamErasmus MCthe Netherlands\n\nFaculty of Applied Sciences\nTU Delft\nDelftthe Netherlands\n\n&amp; Gennady \nV Roshchupkin g.roshchupkin@erasmusmc.nl \nDepartment of Radiology and Nuclear Medicine\nMedical Center\nRotterdamErasmus MCthe Netherlands\n\nDepartment of Epidemiology\nMedical Center\nErasmus, RotterdamMCthe Netherlands\n\nGenNet framework: interpretable deep learning for predicting phenotypes from genetic data\n10.1038/s42003-021-02622-zARTICLE OPEN 1\nApplying deep learning in population genomics is challenging because of computational issues and lack of interpretable models. Here, we propose GenNet, a novel open-source deep learning framework for predicting phenotypes from genetic variants. In this framework, interpretable and memory-efficient neural network architectures are constructed by embedding biologically knowledge from public databases, resulting in neural networks that contain only biologically plausible connections. We applied the framework to seventeen phenotypes and found well-replicated genes such as HERC2 and OCA2 for hair and eye color, and novel genes such as ZNF773 and PCNT for schizophrenia. Additionally, the framework identified ubiquitin mediated proteolysis, endocrine system and viral infectious diseases as most predictive biological pathways for schizophrenia. GenNet is a freely available, end-toend deep learning framework that allows researchers to develop and use interpretable neural networks to obtain novel insights into the genetic architecture of complex traits and diseases.\n\nW hile genome-wide association studies (GWAS) have identified numerous genomic loci associated with complex traits and diseases, the biological interpretation of the underlying mechanisms often remains unclear. Recent GWAS studies with increasingly large sample sizes are resulting greater numbers of significant associations, at an increasing number of independent loci. To illustrate, the latest GWAS for body height based on 700,000 individuals identified more than 3000 near-independent significantly associated single nucleotide polymorphisms (SNPs) 1 . Uncovering a clear biological interpretation from all this information is a challenging task, in which causal variants, genes, and pathways need to be identified. In response, many methods such as MAGMA 2 , ALIGATOR 3 , and INRICH 4 , have been developed to obtain a biological interpretation from GWAS summary statistics, providing insights into relevant genes and pathways for defined phenotypes of interest. These methods explore GWAS summary statics and utilize knowledge from annotated biological databases such as NCBI RefSeq 5 , KEGG 6 , Reactome 7 , and GTEx 8 , which have proven to contain crucial information for understanding the underlying biological mechanisms of the human genome 9 . Additionally, it has been shown that embedding biological knowledge from these databases in polygenic risk scores can improve interpretation, trans-ancestry portability, and genetic risk prediction [10][11][12] .\n\nGiven the increasing amount of data available via biobanks and new developments to integrate data, it is now feasible to analyze raw data with more advanced methods. Deep learning is the state of the art in many domains such as medical image analysis 13 and natural language processing 14 because of its flexibility and modeling capabilities. In many cases, deep learning yields better performance than traditional approaches, since it scales very well with data size and can model highly non-linear relationships. However, a limitation to deep learning is that these algorithms are often uninterpretable because of their complexity 15,16 . Additionally, genetic data does not lend itself well to the convolution operation, the main driver of the success of deep learning in the imaging domain. Traditional fully connected neural networks have been successfully applied to predict genetic risk. Recently, Badre et al. 17 employed a fully connected neural network for improving polygenic risk scores for breast cancer, training a neural network with up to 528,620 input variants. However, these networks are very memory-intensive and therefore often require pre-selecting SNPs using GWAS summary statistics. Applying these fully connected neural networks for millions of input variants would require infeasible amounts of computational time and memory.\n\nTo overcome these limitations, we propose GenNet, a novel framework for predicting phenotype from genotype. Within the GenNet framework, biological information from annotated biological sources such as NCBI RefSeq, KEGG, and single RNA gene expression datasets, is used to define biologically plausible connections. As a result, neural networks based on this framework are memory efficient, interpretable, and yield biological interpretations for their predictions. GenNet is an end-to-end deep learning framework available as a command-line tool (https://github.com/ArnovanHilten/GenNet/).\n\n\nResults\n\nA framework for constructing interpretable neural networks for phenotype prediction. The main concept of the GenNet framework is summarized graphically in Fig. 1. In this framework, prior knowledge is used to create groups of connected nodes to reduce the number of learnable parameters in comparison to a fully connected neural network. For example, in the first layer, biological knowledge in the form of gene annotations is used to group millions of SNPs and to connect those SNPs to their corresponding genes. The resulting layer retains only meaningful connections, significantly reducing the total number of parameters compared to a classical fully connected layer. Because of this memory-efficient approach networks in the GenNet framework are able to handle millions of inputs for genotype-to-phenotype prediction.\n\nBiological knowledge is thus used to define only meaningful connections, shaping the architecture of the neural network. Interpretability is inherent to the neural network's architecture; each node is uniquely defined by its connections and represents a biological entity (e.g., gene, pathway). For example, a network that connects SNPs-to-genes and genes-to-output. The learned weights of the connections between layers represent the effect of the SNP on the gene or the effect of the gene on the output. In the network, all neurons represent biological entities, and weights model the effects between these entities, together forming a biologically interpretable neural network.\n\nMany types of layers can be created using this principle. Apart from gene annotations, our framework provides layers built from exon annotations, pathway annotations, chromosome annotations, and cell and tissue type expressions. All these layers can be used like building blocks to form new neural network architectures.\n\nWe built a proof of concept simulations as shown in Fig. 2a Fig. 1 Overview of the GenNet framework. Neural networks are created by using prior biological knowledge to define connections between layers (i.e., SNPs are connected to their corresponding genes by using gene annotations, and genes are connected to their corresponding pathway by using pathway annotations). Prior knowledge is thus used to define each connection, creating interpretable networks.\n\nwith high heritability, high number of training samples, and low polygenicity.\n\nApplying neural network architectures based on geneannotations to population-based data. Motivated by the proof-of-concept and outcomes of the simulations, we applied the framework to data from multiple sources, including populationbased data from the UK Biobank study 18 , the Rotterdam study 19 , and the Swedish Schizophrenia Exome Sequencing study 20 . Phenotypes vary from traits that can be predicted well from only a dozen variants (eye color) to disorders in which thousands of variants explain only a small portion of the variance (schizophrenia and bipolar disorder) 21,22 . The datasets also vary in size and type of data. We used 113,241 exonic variants from imputed microarray-based GWAS data from the Rotterdam study for predicting eye color while we predict fifteen phenotypes in the UK Biobank using 6,986,636 input variants from whole exome sequencing (WES) data. Finally, we used 1,288,701 WES input variants for predicting schizophrenia in the Swedish study. An overview of the main results for networks embedded with gene and pathway information can be found in Table 1. The results for all the experiments, including expression-based networks, can be found in Supplementary Note 2.\n\nPhenotypes with more training samples and that require less variants to obtain high predictive performance, such as eye and hair color, yielded the best performance. Nonetheless, we  achieved a good predictive performance for schizophrenia, a highly polygenic disorder, with an area under the curve (AUC) of 0.74 in the held-out test set. All models based on gene annotations outperformed or matched the baseline LASSO logistic regression model with the exception of the black hair color phenotype.\n\nInspecting the networks, we found that the OCA2 gene was the most important gene to predict blond, dark, and light brown hair color. OCA2 is involved in the transport of tyrosine, a precursor of melanin 23 . The OCA2 signal is likely amplified by the nearby gene HERC2, previously identified via functional genetic studies as harboring a strong, long-distance enhancer regulating OCA2 gene expression underlaying pigmentation variation 23 . According to the network, OCA2 and HERC2 are the two most predictive genes for predicting blue iris color. Both genes were previously identified through GWAS studies of hair and eye color [24][25][26] .\n\nIn the experiments with schizophrenia as outcome, the network was able to classify cases and controls with a maximum accuracy of 0.685 (mean of 0.663 \u00b1 0.014 over 10 runs). Based on all genetic aspects, we estimate the theoretical upper limit for classification accuracy to be between 0.73 and 0.83 for schizophrenia in this dataset (Supplementary Method 6). The model achieved a maximum area under the receiver operating curve of 0.738 (mean of 0.715 \u00b1 0.016) in the held-out test set over 10 runs, thereby considerably outperforming the LASSO logistic regression baseline which obtained a maximum AUC of 0.649 (mean of 0.644 \u00b1 0.003). The genes ZNF773, PCNT, and DYSF were assigned the highest weights by the neural network and were thus considered to be the most predictive genes for schizophrenia. However, the polygenic nature of schizophrenia can be seen when comparing the Manhattan plot of the genes (Fig. 2d) to other phenotypes in this study (Supplementary Notes 3\u22125). More genes contributed to the prediction for schizophrenia than for the other phenotypes examined. As a consequence, the predictive performance deteriorates slower for schizophrenia than for eye color prediction when connections to the most predictive genes are removed before training (see Supplementary Discussion 7).\n\nTo evaluate if embedding prior knowledge also improves predictive performance, we compared networks built using gene annotations to networks with the same number of connections but randomly connected. For predicting blue eye color, the network build using gene annotations performed significantly better than a randomly connected neural network over ten runs (p = 6.5 \u00d7 10 \u22123 ). However, for predicting schizophrenia we found the opposite, a randomly connected neural network performed significantly better than networks embedded with prior knowledge (p = 2.5 \u00d7 10 \u22124 ). See Supplementary Discussion 8 for more information on all phenotypes.\n\nEmbedding pathway and expression-based annotations into the neural network architecture. Figure 3 shows the relative importance of the pathways for predicting schizophrenia. The neural network, embedded with the KEGG pathway information, obtained an AUC of 0.68 in the test set. Human diseases (30.8%), organismal systems (26.7%), and genetic information processing (26.5%) were the main contributors to the neural network's prediction of schizophrenia. The contribution of human diseases was mainly driven by viral infectious diseases (27.3%) which can be subdivided further into human papillomavirus infection (11.7%), herpes simplex infections (3.7%), and human cytomegalovirus infection (3.2%), as well as genes and SNPs assigned to these diseases. The ubiquitin-mediated proteolysis pathway, a subset of the genetic information pathway, had a relative importance of 10.0%.\n\nTo investigate if the most predictive genes are concentrated in a single cell or tissue type, we used three different gene expression datasets processed by Finucane et al. (2018) 27 to create layers subsequent to the gene-based layer. The relative importance of the trained networks showed a widespread signal with small contributions by many cell and tissue types. When GTEx data was embedded in the network, uterine genes had the greatest relative importance (4.2%). For GTEx brain expression data, the frontal cortex (15.1%) had the strongest contribution, whereas for immune cell types mesenteric lymph nodes (1.23%) contributed most. An overview of the experiments performed, including GTEx and other expression-based networks for other phenotypes, can be found in Supplementary Note 2.\n\n\nDiscussion\n\nWe presented a novel framework for predicting phenotypes from genotype with interpretable neural networks. The proposed neural networks have connections defined by prior biological knowledge only, reducing the number of connections and therefore the number of trainable parameters. Consequently, the networks can be trained on a single GPU and offer a biological interpretation for their predictions.\n\nIn the first set of experiments, simulations showed the network's performance when varying the degree of heritability, polygenicity, and sample size. In the second set of experiments, the framework was applied to UK Biobank, Rotterdam study, and Swedish Schizophrenia Exome Sequencing study data. In these experiments, phenotypes with widely different heritability, sample size, and polygenicity were predicted with generally high predictive performance. For example, we obtained an AUC of 0.96 for red hair color, an AUC of 0.74 for blue eye color, and an AUC of 0.74 for predicting schizophrenia. For schizophrenia, polygenic risk scores from genome-wide association studies obtained a similar AUC of 0.72 in an independent test cohort and a mean AUC of 0.70 in 40 leave-one-out GWAS analyses (AUC ranging from 0.49 to 0.81) 21 . However, a direct comparison between traditional polygenic risk scores and predictions given by neural networks from the GenNet framework would be unfair. The neural networks employed in this study use WES data or variants from the coding regions, while polygenic risk scores generally use all genotype data. Additionally, polygenic risk scores are built based on GWAS data with large sample sizes. For example for schizophrenia, the network was trained with roughly 4,696 cases and 6,969 controls while polygenic risk scores were derived using 32,838 cases and 44,357 controls 21 .\n\nInterpretation of the trained networks revealed well-replicated genes for traits with a known etiology such as HERC2 and OCA2 for eye color and OCA2 and TC2N for hair color [24][25][26]28 . For schizophrenia, a disorder with an unclear etiology, the network identified novel genes, including ZNF773 and PCNT. In previous studies, PCNT was only nominally associated with schizophrenia, although it is known to have interactions with DISC1 (Disrupted in Schizophrenia-1) 29 . There is a strong correlation between (prenatal) viral infections and increased risk for developing schizophrenia 30 . It is therefore interesting that by using the KEGG pathway information to create layers, we identified the viral infectious diseases pathway with its associated pathways, genes, and SNPs to be the most important for predicting schizophrenia. The results of these experiments reveal that with GenNet framework can provide insights across multiple functional levels, identifying which SNPs, genes, pathways, and tissue types are important for prediction.\n\nIn a comparison between randomly connected neural networks and networks built using gene annotations, we found that embedding gene annotations in the network architecture resulted in significantly better predictive performance for predicting eye color. However, randomly connected neural networks, with the same number of trainable parameters, performed significantly better for predicting schizophrenia. Based on these experiments using gene annotations, it seems that the optimal neural network architecture depends on the genetic architecture of the trait. But there are many factors that could influence the convergence and thus the predictive performance of a neural network aside from the genetic architecture of the trait. Among others, the quality of the available prior knowledge will be an important factor to consider.\n\nIn this study, WES data and exonic variants from microarrays have been used; however, the principles in the GenNet framework can be extended to handle diverse types of input, including genotype, gene expression, and methylation data or combinations thereof. Similarly, we explored only a fraction of the possible layers as building blocks for our networks. Any data that create unique biological groups can be used in the framework to create new layers. However, the quality of the resulting network is highly dependent on the quality of the data used. For example, networks built with biological pathway data performed generally worse than networks using only gene annotations since many genes are not annotated to pathways. But with new and better data from projects such as GTEx 8 , ENCODE 31 , and KEGG 6 the quality of the layers will only improve. Moreover, given the increasing sample sizes of biobanks in coming years and the development of algorithms, which allow distributed deep learning between cohorts without sharing raw data 32 , we foresee that our framework can be easily adopted for such settings and can be widely used for quantitate traits and functional mapping analysis.\n\nIn conclusion, we developed a freely-available framework, which can be used to build interpretable neural networks for genotype data by incorporating prior biological knowledge. In addition to computationally efficient, the architectures are interpretable, thereby alleviating one of the most important shortcomings of neural networks. We have demonstrated the effectiveness of this novel framework across multiple datasets and for multiple phenotypes. Given that each network node is interpretable, we anticipate this approach to have wide applicability for uncovering novel insights into the genetic architecture of complex traits and diseases.\n\n\nMethods\n\nSweden Schizophrenia Exome Sequencing study. Sweden-Schizophrenia Population-Based Case-Control Exome Sequencing study (dbGaP phs000473.v2.p2), is a case-control study with 4969 cases and 6245 controls 20 . All individuals aged 18\u221265, have parents born in Sweden, and provided written informed consent. The following inclusion criteria were used for cases: at least two times hospitalized with schizophrenia discharge diagnosis; no hospital register diagnosis consistent with a medical or other psychiatric disorder that mitigates the schizophrenia diagnosis. Cases were excluded if they had a relationship closer than 2nd degree relative with any other case. Controls were matched to cases and controls were never hospitalized with a discharge diagnosis of schizophrenia. Controls were excluded if they had any relation to either case or control.\n\nThe downloaded plink files were converted using HASE 33 to hierarchical data format (HDF), a format that allows fast and parallel data reading. After conversion, the data was transposed and SNPs without any variance were removed (1,288,701 Fig. 3 Sunburst plot of the important KEGG pathways for predicting schizophrenia. A neural network with layers based on KEGG pathway information was trained to predict schizophrenia. The relative importance was calculated for each pathway using the learned weights of this neural network. The sunburst plot is read from the center, which shows the output node with the prediction of the neural network. The inner ring represents the last layer with the highest-level pathways, followed by the mid-level pathways, and finally the lowest-level pathways. The gene and SNP layer are omitted for clarity.\n\nSNPs remain). The data was split into a training, validation, and test set (ratio of 60/20/20), while preserving the ratio cases and controls.\n\nUK Biobank. We applied the framework to multiple phenotypes in the UK Biobank using the first release of the WES data, providing WES for 50,000 UK Biobank participants 34 . Phenotypes are self-reported using touchscreen questions in the UK Biobank Assessment Center. Similar to the Sweden cohort all variants without variance were removed, data was converted to HDF and transposed. For every phenotype, an equal number of cases and controls were sampled. The resulting dataset is split into a train, validation, and a test set (ratio of 60/20/20). Related cases and cases with related controls, (kinship > 0.0625) are all in the training set. This is done under the assumption that related cases and controls could ease training, the shared genetic information could steer the network towards the discriminatory features. The validation and test sets contain only unrelated cases and controls within and between sets. Unrelated controls are randomly sampled and added to gain an even distribution between cases and controls in all sets. Misaligned SNPs and sex chromosomes were masked in the first layer and therefore not included in the study.\n\nRotterdam Study. The Rotterdam Study is a prospective population-based cohort study 19 . We used the first cohort consisting of 6291 participants, genotyped using the Illumina 550 and 550 K duo arrays. Samples with low call rate (<97.5%), excess autosomal heterozygosity (>0.336), or sex-mismatch were excluded, as were outliers identified by the identity-by-state clustering analysis (outliers were defined as being >3 standard deviation (SD) from the population mean or having identity-bystate probabilities > 97%). For imputation the Markov Chain Haplotyping (MACH) package version 1.0 software (Imputed to plus strand of NCBI build 37, 1000 Genomes phase I version 3) and minimac version 2012.8.6 were used (call rate > 98%, MAF > 0.001 and Hardy-Weinberg equilibrium P-value > 10 \u22126 ). From here on, processing steps are identical as described for Sweden Schizophrenia resulting in a dataset with 113,241 exonic variants for 6291 subjects.\n\nFor each subject, both eyes were examined by an ophthalmological medical researcher, and eye (iris) color was categorized into three categories; blue, intermediate, and brown using standard images and based on the predominant color and pigmentation 35 .\n\nPrior knowledge. All SNPs were annotated using Annovar 36 . From these annotations, a sparse connectivity matrix was created, connecting the SNPs to their corresponding genes. Connectivity matrices between SNPs, exons, and genes were made using intron-exon annotations. A mapping between genes and pathways was made using GeneSCF 37 and the KEGG database 6 . Due to the hierarchical structure of the KEGG database no further preprocessing was necessary and the structure could be integrated in GenNet as it is. Expression-based layers were created using publicly available group-wise t-score statistics by Finucane et al. (2018) 27 . For each tissue, we connected the genes with the top 10% highest t-statistic to a tissue to ensures that nodes are uniquely defined by their connections and therefore interpretable. This threshold is in accordance with the work of Finucane et al. (2018) 27 , they showed that their results were stable for thresholds of 5, 10, and 20% (URLs can be found in Supplementary Note 9).\n\nAdditional to the used layers, there are countless of different network layers possible. Any prior knowledge that groups data uniquely can be used to create layers in the GenNet framework.\n\nNeural network architecture. In the GenNet framework, layers are available built from biological knowledge such as exon annotations, gene annotations, pathway annotations, cell expression, and tissue expression. Information from these resources is used to define only meaningful connections, shaping an interpretable and lightweight neural network, allowing the evaluation of millions of input variants together. These networks bear similarities to the first generation of neural networks, where prior knowledge was also used to make neural networks computationally cheaper. Recently, interest for these types of networks has rekindled for biological applications [38][39][40][41] .\n\nHowever, the use of prior knowledge also restricts the network layout. The shape of the network, i.e., the number of neurons and the number of layers is determined by the prior knowledge embedded in the network (characteristics of all architectures can be found in Supplementary Note 9). Additionally, for a fair interpretation of the weights, each layer is preceded by batch normalization (without scaling and centering) resulting in an input for each layer with zero mean and unit standard deviation. For all classification task, a sigmoid function was used as a final activation function.\n\nOperating within these restrictions, we optimized the following hyperparameters: the activation function, optimizer, and the L1 kernel regularization penalty. In the first set of experiments to determine the activation function, we found that the hyperbolic tangent activation function consistently led to a higher area under the curve in the validation set than the Relu and PreLu activation functions for schizophrenia, hair color, and eye color. Using the hyperbolic tangent activation function, we optimized the remaining hyperparameters. For each phenotype, we tested different combinations of optimizers with different learning rates and L1 kernel regularization penalties. We explored combinations of the Adadelta optimizer (with learning rate of 1) or Adam optimizer (learning rates of 0.01, 0.001, and 0.0001) with kernel L1 regularization penalties of 0.1, 0.001, 0.001, 0.00001, and 0 (no penalty). The L1 regularization penalty penalizes the network for the total sum of weights, similar to LASSO regression. A higher penalty will reduce the number of variants the network will use for prediction and is therefore related to the polygenicity of the phenotype.\n\nAll models were trained on a single GPU with 11 GB memory (Nvidia GeForce GTX 1080 Ti) using a batch size of 64. The networks were optimized using the ADAM or Adadelta optimizers, using weighted binary cross-entropy with weights proportional to the imbalance of the classes. For regression tasks, mean squared error was used as a loss function in combination with ReLu activations (Supplementary Method 10). The training was stopped after not improving on the validation loss for 10 epochs (all models converged within five days). For each phenotype, the configuration of hyperparameters with the best performance in the validation set was used for a final evaluation in the test set.\n\nInterpretation. Interpretation of the network is straightforward due to the simplicity of the concept, the stronger the weight is the more it contributes to the final prediction of the network. The simplest network in the framework, a network built by gene annotations, can be seen as~20,000 (number of genes) parallel regressions followed by a single logistic regression. The learned weights in these regressions are similar to the coefficients in logistic regression. Especially the last node, the equation for a single neuron with a sigmoid activation (1) is very similar to the equation for logistic regression (2). For both, all inputs (x i to x n ) are multiplied with learnable parameters followed by an addition of a learnable bias B to obtain output Y.\nY \u00bc Sigmoid \u2211 n i\u00bc0 x i w i \u00fe B \u00f01\u00de Y \u00bc Sigmoid \u2211 n i\u00bc0 x i \u03b2 i \u00fe B\u00f02\u00de\nFor logistic regression, the inputs are generally normalized to compare coefficients (\u03b2). In the neural network, this is achieved by batch normalization (without center and scaling), normalizing the weights (w) after every activation. The weights can then be compared and used as an important measure. Since batch normalization is a batch-wise approximation, the learned weights can be multiplied with the standard deviation of the activations for a more accurate estimate. For Manhattan plots, the normalized, absolute weights between the gene layer and the output are used.\n\nIn larger networks, the relative importance is calculated by multiplying the weights for each possible path from the end node to the input SNPs. At each input, we obtain then a value denoting its contribution. These values are then aggregated (summed) according to the groups of the subsequent layers to get the importance estimate for each node in the network.\n\nIt is important to note that in this work (relative) importance is more similar to effect size or odds ratio than to statistical significance. The weights represent the direction and effect a gene or pathway has on the outcome of the network.\n\nImplementation. Technically, the computational performance of the implemented Keras/Tensorfow 42,43 layer should be on par or an improvement over similar layers. It is implemented using sparse matrix multiplication, making it faster than the slice-wise locallyconnected1D layer and more memory efficient than dense matrix multiplication. The layer is friendly to use, with only one extra input compared to a normal dense layer. This extra input, the sparse connectivity matrix, is made with prior knowledge and describes how neurons are connected between layers.\n\nThe networks behave similar to normal fully connected artificial neural networks but are pruned by removing irrelevant connections:\nY \u00bc Activation \u2211 n i\u00bc0 x i w i \u00fe B\u00f03\u00de\nWith w as a sparse matrix with learnable weights, initialized with a sparse connectivity matrix defining connections.\n\nThe GenNet framework is available as a command line tool and includes functionalities to convert data, to create, train, and evaluate and interpret neural networks.\n\nBaseline. As a baseline method, LASSO logistic regression was implemented in Tensorflow by using a dense layer of a single neuron with a sigmoid activation function and L1 regularization on weights. Polygenic Risk Scores (PRS) were not used since we used only variants from the exome.\n\nUpper bound of classification accuracy for predicting phenotypes. In contrast to the application of deep learning for image analysis, where the performance is not limited, in application for genetic analysis, we are limited by the information in DNA relevant for trait or disease, i.e., by the heritability. Therefore, we cannot expect the performance of the neural network above a certain value.\n\nPopulation characteristics can be used to calculate the upper bound of performance for a classifier for any trait. This can be done by creating a confusion matrix. The accuracy between true and false positives for a perfect classifier, based solely on genetic inputs, is given by the concordance rate between monozygotic twins. It is impossible to predict better based solely on genetic code than the rate a trait occurs in people with virtually the same genetic code. The chance of misclassifying a control should be better than the prevalence, which is often close to zero for most diseases. Creating a confusion matrix can give insights in the upper bound for accuracy, sensitivity, and specificity in the dataset. An example for schizophrenia in our dataset can be found in Supplementary Method 6.\n\nReporting summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article.\n\n\nData availability\n\nCode to run and generate data for the simulations are publicly available on GitHub. The genetic and phenotypic UK Biobank data are available upon application to the UK Biobank (https://www.ukbiobank.ac.uk/). Access to the Sweden-Schizophrenia Exome Sequencing study can be requested on dbGaP (https://www.ncbi.nlm.nih.gov/gap/) (dbGaP phs000473.v2.p2). All trained networks are available on https://github.com/ ArnovanHilten/GenNet_Zoo/.\n\n\nCode availability\n\nGenNet is an open-source framework usable from command line. GenNet and its tutorials, including how to build new layers and networks from prior knowledge, can be found on: https://github.com/arnovanhilten/GenNet/ and Zenodo 44 .\n\nFig. 2\n2Simulation results and the importance of each gene for predicting schizophrenia. a A simple, non-linear proof of concept. In this simulation, each gene in the causal region has two causal SNPs that cause the simulated disease. The magnitude of the learned weight is represented by the line thickness (contributing causal connections in red, non-contributing control connections in gray). b A secondary set of simulations show the performance of GenNet, expressed as area under the curve, for increasing levels of heritability and training set size (c). The black curve presents the theoretical maximum of the AUC versus heritability. d Manhattan plot showing genes and their relative importance according to the network, here we have shown the results for distinguishing between schizophrenia cases and controls in the Sweden exome study. This Manhattan plot is a cross-section between the gene layer (21,390 nodes) and the outcome of a trained network with 1,288,701 input variants.\n\n\nand described in Supplementary Method 1. These simulations demonstrate that the proposed network architecture is interpretable, the strongest weights are assigned to causal variants and genes. Next, we designed experiments to test the network architecture's performance under a variety of conditions. Performance is dependent on phenotype, neural network architecture, and dataset size. To disentangle this, we created simulated data with varying levels of heritability, the number of training samples, and polygenicity (Supplementary Method 1). Figures 2b, c demonstrate the major trends observed in the simulations. As expected, the network performs best for traitsPrediction \n\nGenes \n\nPathways \n\nPathways \n\nSNPs \n\nConnections based \non gene annotations \n\nLocal \n\nGlobal \n\nOther types of \nlayers available \n\n\n\nTable 1\n1Overview of the main results for the experiments using the Rotterdam Study, UK Biobank, and Swedish Schizophrenia Exome Sequencing study data.The performance in AUC for the network with gene-annotations is bold if the network outperformed or matched LASSO regression. Manhattan plots for the genes can be found in Supplementary Notes 3\u22125. *MC1R was not annotated but was identified by linkage disequilibrium.**Many genes contributed to the prediction without clear separation between genes (see Supplementary Note 4).Dataset (type) \n\nTrait \n\nSubjects & phenotype \n\nAUC gene \ntest (val) \n\nTop three \npredictive genes \n\nAUC \npathway \n\ntest (val) \nTop predictive pathway \n\nClass I \n\nClass II \n\nGlobal level \n\nMiddle level \n\nLocal level \n\nRotterdam Study \n(genotype array) \n\nEye color \n\n4041 Blue \n\n2250 Other \n\n0.75 (0.74) \n\nHERC2, OCA2, \nLAMC1 \n\n0.50 (0.52) \n\nOrganismal \nSystems \n(78.4%) \n\nDigestive \nsystem \n(72.6%), \n\nPancreatic secretion \n(59.1%) \n\nUK Biobank \n(exome) \n\nHair color \n\n1734 Red \n\n1727 Other \n\n0.93 (0.94) \n\nMC1R*, SHOC2, \nDCTN3 \n\n0.77 (0.77) \n\nGenetic \nInformation \nProcessing \n(87.4%) \n\nReplication and \n\nrepair (83.4%) \n\nFanconi anemia pathway \n(79.7%) \n\n3762 Black \n\n3753 Other \n\n0.80 (0.82) \n\nOCA2, RPL23AP8, \nMC1R* \n\n0.76 (0.78) \n\nOrganismal \nSystems \n(46.9%) \n\nEndocrine \nsystem (18.6%) \n\nAxon guidance (5.0%) \n\n4501 Blond \n\n4518 Other \n\n0.66 (0.65) \n\nOCA2, TC2N, \nSLC45A2 \n\n0.58 (0.57) \n\nOrganismal \nSystems \n(70.2%) \n\nEndocrine \nsystem \n(30.1%), \n\nAdrenergic signaling in \ncardiomyocytes (4.1%) \n\nBipolar \ndisorder \n\n343 Cases \n\n347 \nControls \n\n0.60 (0.56) \n\nLINC00266-1, \n\nCSMD1, TCERG1L \n\n0.47 (0.55) \n\nOrganismal \nSystems \n(76.9%) \n\nEndocrine \nsystem (46.5%) \n\nMelanogenesis (32.9%) \n\nAtrial \nfibrillation \n\n192 Cases \n\n194 \nControls \n\n0.56 (0.59) \n\n**BRINP1, \nSORBS3, ELM0D3 \n\n0.57 (0.63) \n\nOrganismal \nSystems \n(39.6%) \n\nSignal \ntransduction \n\n(11.2%) \n\nCytokine\u2212cytokine \n\nreceptor \ninteraction (4.4%) \n\nCoronary \nArtery \nDisease \n\n1563 Cases \n\n1600 \nControls \n\n0.56 (0.58) \n\n**STARD7-AS1, \n\nVWC2L, NSD2 \n\n0.54 (0.56) \n\nEnvironmental \nInformation \nProcessing \n(29.5%), \n\nSignal \ntransduction \n\n(27.7%) \n\nPI3K-Akt signaling \npathway (4.6%) \n\nDementia \n\n139 Cases \n\n142 \nControls \n\n0.60 (0.65) \n\nRPL23AP87, \nCTNNA3, \nLINC01003 \n\n0.55 (0.58) \n\nHuman \nDiseases \n(39.8%) \n\nSignal \ntransduction \n\n(22.2%) \n\nPathways in \ncancer (5.6%) \n\nMale balding \npattern \n\n3454 \nBalding \npattern 1 \n\n3454 \nBalding \npattern 4 \n\n0.56 (0.57) \n\nNGEF, NKRD18B, \nSYNJ2 \n\n0.54 (0.55) \n\nOrganismal \nSystems \n(34.6%) \n\nNervous \nsystem (9.7%) \n\nMetabolic \npathways (8.7%) \n\nAsthma \n\n4229 Cases \n\n4214 \nControls \n\n0.55 (0.57) \n\nHLA-DQB1, \n\nHCG9, \nLINC00266-1 \n\n0.51 (0.54) \n\nGenetic \nInformation \nProcessing \n(52.3%) \n\nFolding, sorting \n\nand degradation \n(41.5%) \n\nUbiquitin mediated \nproteolysis (22.0%) \n\nDiabetes \n\n2557 Cases \n\n2555 \nControls \n\n0.54 (0.57) \n\n**DNAH10, \nSNAR-I, PSMD13 \n\n0.54 (0.54) \n\nEnvironmental \nInformation \nProcessing \n(43.5%), \n\nSignal \ntransduction \n(40.5%), \n\nRas signaling \npathway (7.9%) \n\nBreast cancer \n\n1070 Cases \n\n1082 \nControls \n\n0.51 (0.56) \n\nRPL23AP87, \nLINC00266-1, \n\nHPSE \n\n0.51 (0.56) \n\nHuman \nDiseases \n(57.1%) \n\nInfectious \ndiseases: Viral \n(16.6%) \n\nPathways in \ncancer (6.5%) \n\nSweden (exome) \n\nSchizophrenia \n\n4969 Cases \n\n6245 \nControls \n\n0.74 (0.73) \n\nZNF773, PCNT, \nDYSF \n\n0.68 (0.67) \n\nHuman \nDiseases \n(30.8%) \n\nInfectious \ndiseases: Viral \n(27.3%) \n\nHuman papillomavirus \ninfection (11.7%) \n\n\n\u00a9 The Author(s) 2021, corrected publication 2021\nAcknowledgementsWe would like to thank Marloes Arts (University of Copenhagen) for her advice.Author contributionsA.H. and G.R. conceived and designed the method. A.H. performed experiments and implemented the method. G.R. and W.N. supervised the work. M.A.I., C.K., H.A., M.K. and S.A.K. provided or gave access to data. A.H., G.R., W.N., M.K., M.A.I. and S.A.K, wrote, revised, and approved the paper.Competing interestsW.N. is co-founder and shareholder of Quantib BV. Other authors declare no competing interests.Additional informationSupplementary information The online version contains supplementary material available at https://doi.org/10.1038/s42003-021-02622-z.Correspondence and requests for materials should be addressed to Arno van Hilten or Gennady V. Roshchupkin.Peer review information Communications Biology thanks the anonymous reviewers for their contribution to the peer review of this work. Primary Handling Editors: Eirini Marouli and Luke R. Grinham. Peer reviewer reports are available.Reprints and permission information is available at http://www.nature.com/reprintsPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/ licenses/by/4.0/.\nMeta-analysis of genome-wide association studies for height and body mass index in~700,000 individuals of European ancestry. L Yengo, Hum. Mol. Genet. 27Yengo, L. et al. Meta-analysis of genome-wide association studies for height and body mass index in~700,000 individuals of European ancestry. Hum. Mol. Genet. 27, 3641-3649 (2018).\n\nMAGMA: generalized gene-set analysis of GWAS data. C A De Leeuw, J M Mooij, T Heskes, D Posthuma, PLoS Comput Biol. 111004219de Leeuw, C. A., Mooij, J. M., Heskes, T. & Posthuma, D. MAGMA: generalized gene-set analysis of GWAS data. PLoS Comput Biol. 11, e1004219 (2015).\n\nGene ontology analysis of GWA study data sets provides insights into the biology of bipolar disorder. P Holmans, Am. J. Hum. Genet. 85Holmans, P. et al. Gene ontology analysis of GWA study data sets provides insights into the biology of bipolar disorder. Am. J. Hum. Genet. 85, 13-24 (2009).\n\nINRICH: intervalbased enrichment analysis for genome-wide association studies. P H Lee, C O&apos;dushlaine, B Thomas, S M Purcell, Bioinformatics. 28Lee, P. H., O'Dushlaine, C., Thomas, B. & Purcell, S. M. INRICH: interval- based enrichment analysis for genome-wide association studies. Bioinformatics 28, 1797-1799 (2012).\n\nNCBI reference sequences (RefSeq): a curated non-redundant sequence database of genomes, transcripts, and proteins. K D Pruitt, T Tatusova, D R Maglott, Nucleic Acids Res. 35Pruitt, K. D., Tatusova, T. & Maglott, D. R. NCBI reference sequences (RefSeq): a curated non-redundant sequence database of genomes, transcripts, and proteins. Nucleic Acids Res. 35, D61-D65 (2007).\n\nKyoto encyclopedia of genes and genomes. M Kanehisa, S Goto, Kegg, Nucleic Acids Res. 28Kanehisa, M. & Goto, S. KEGG: Kyoto encyclopedia of genes and genomes. Nucleic Acids Res. 28, 27-30 (2000).\n\nThe Reactome pathway knowledgebase. D Croft, Nucleic Acids Res. 42Croft, D. et al. The Reactome pathway knowledgebase. Nucleic Acids Res. 42, D472-D477 (2014).\n\nThe genotype-tissue expression (GTEx) project. J Lonsdale, Nat. Genet. 45580Lonsdale, J. et al. The genotype-tissue expression (GTEx) project. Nat. Genet. 45, 580 (2013).\n\nThe post-GWAS era: from association to function. M D Gallagher, A S Chen-Plotkin, Am. J. Hum. Genet. 102Gallagher, M. D. & Chen-Plotkin, A. S. The post-GWAS era: from association to function. Am. J. Hum. Genet. 102, 717-730 (2018).\n\nA biologically-informed polygenic score identifies endophenotypes and clinical conditions associated with the insulin receptor function on specific brain regions. Hari Dass, S A , EBioMedicine. 42Hari Dass, S. A. et al. A biologically-informed polygenic score identifies endophenotypes and clinical conditions associated with the insulin receptor function on specific brain regions. EBioMedicine 42, 188-202 (2019).\n\nImproving the trans-ancestry portability of polygenic risk scores by prioritizing variants in predicted cell-type-specific regulatory elements. T Amariuta, Nat. Genet. 52Amariuta, T. et al. Improving the trans-ancestry portability of polygenic risk scores by prioritizing variants in predicted cell-type-specific regulatory elements. Nat. Genet. 52, 1346-1354 (2020).\n\nPathway-specific polygenic risk scores as predictors of \u03b2amyloid deposition and cognitive function in a sample at increased risk for Alzheimer's disease. F Burcu, Darst, J. Alzheimers Dis. 176Burcu, F. Darst et al. Pathway-specific polygenic risk scores as predictors of \u03b2- amyloid deposition and cognitive function in a sample at increased risk for Alzheimer's disease. J. Alzheimers Dis. 176, 139-148 (2017).\n\nA survey on deep learning in medical image analysis. G Litjens, Med. Image Anal. 42Litjens, G. et al. A survey on deep learning in medical image analysis. Med. Image Anal. 42, 60-88 (2017).\n\nRecent trends in deep learning based natural language processing. T Young, D Hazarika, S Poria, E Cambria, IEEE Comput. Intell. Mag. 13Young, T., Hazarika, D., Poria, S. & Cambria, E. Recent trends in deep learning based natural language processing. IEEE Comput. Intell. Mag. 13, 55-75 (2018).\n\nSlave to the algorithm: why a right to an explanation is probably not the remedy you are looking for. Duke L. L Edwards, M Veale, Tech. Rev. 1618Edwards, L. & Veale, M. Slave to the algorithm: why a right to an explanation is probably not the remedy you are looking for. Duke L. Tech. Rev. 16, 18 (2017).\n\nExplainable artificial intelligence: a survey. F K Do\u0161ilovi\u0107, M Br\u010di\u0107, N Hlupi\u0107, 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO). IEEEDo\u0161ilovi\u0107, F. K., Br\u010di\u0107, M. & Hlupi\u0107, N. Explainable artificial intelligence: a survey. In 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO) 210-215 (IEEE, 2018).\n\nDeep neural network improves the estimation of polygenic risk scores for breast cancer. A Badr\u00e9, L Zhang, W Muchero, J C Reynolds, C Pan, J. Hum. Genet. 66Badr\u00e9, A., Zhang, L., Muchero, W., Reynolds, J. C. & Pan, C. Deep neural network improves the estimation of polygenic risk scores for breast cancer. J. Hum. Genet. 66, 359-369 (2021).\n\nThe UK Biobank resource with deep phenotyping and genomic data. C Bycroft, Nature. 562Bycroft, C. et al. The UK Biobank resource with deep phenotyping and genomic data. Nature 562, 203-209 (2018).\n\nObjectives, design, and main findings until 2020 from the Rotterdam Study. M A Ikram, Eur. J. Epidemiol. 35517Ikram, M. A. et al. Objectives, design, and main findings until 2020 from the Rotterdam Study. Eur. J. Epidemiol. 35, 483\u2212517 (2020).\n\nA polygenic burden of rare disruptive mutations in schizophrenia. S M Purcell, Nature. 506Purcell, S. M. et al. A polygenic burden of rare disruptive mutations in schizophrenia. Nature 506, 185-190 (2014).\n\nBiological insights from 108 schizophrenia-associated genetic loci. S Ripke, Nature. 511Ripke, S. et al. Biological insights from 108 schizophrenia-associated genetic loci. Nature 511, 421-427 (2014).\n\nGenome-wide association study identifies 30 loci associated with bipolar disorder. E A Stahl, Nat. Genet. 51Stahl, E. A. et al. Genome-wide association study identifies 30 loci associated with bipolar disorder. Nat. Genet. 51, 793-803 (2019).\n\nHERC2 rs12913832 modulates human pigmentation by attenuating chromatin-loop formation between a long-range enhancer and the OCA2 promoter. M Visser, M Kayser, R.-J Palstra, Genome Res. 22Visser, M., Kayser, M. & Palstra, R.-J. HERC2 rs12913832 modulates human pigmentation by attenuating chromatin-loop formation between a long-range enhancer and the OCA2 promoter. Genome Res. 22, 446-455 (2012).\n\nA genome-wide association study identifies novel alleles associated with hair color and skin pigmentation. J Han, PLoS Genet. 41000074Han, J. et al. A genome-wide association study identifies novel alleles associated with hair color and skin pigmentation. PLoS Genet. 4, e1000074 (2008).\n\nGenome-wide association meta-analysis of individuals of European ancestry identifies new loci explaining a substantial fraction of hair color variation and heritability. P G Hysi, Nat. Genet. 50Hysi, P. G. et al. Genome-wide association meta-analysis of individuals of European ancestry identifies new loci explaining a substantial fraction of hair color variation and heritability. Nat. Genet. 50, 652-656 (2018).\n\nEye color and the prediction of complex phenotypes from genotypes. F Liu, Curr. Biol. 19Liu, F. et al. Eye color and the prediction of complex phenotypes from genotypes. Curr. Biol. 19, R192-R193 (2009).\n\nHeritability enrichment of specifically expressed genes identifies disease-relevant tissues and cell types. H K Finucane, Nat. Genet. 50Finucane, H. K. et al. Heritability enrichment of specifically expressed genes identifies disease-relevant tissues and cell types. Nat. Genet. 50, 621-629 (2018).\n\nGenome-wide association studies of quantitatively measured skin, hair, and eye pigmentation in four European populations. S I Candille, PLoS One. 748294Candille, S. I. et al. Genome-wide association studies of quantitatively measured skin, hair, and eye pigmentation in four European populations. PLoS One 7, e48294 (2012).\n\nDISC1-binding proteins in neural development, signalling and schizophrenia. N J Bradshaw, D J Porteous, Neuropharmacology. 62Bradshaw, N. J. & Porteous, D. J. DISC1-binding proteins in neural development, signalling and schizophrenia. Neuropharmacology 62, 1230-1241 (2012).\n\nThe neurodevelopmental hypothesis of schizophrenia, revisited. S H Fatemi, T D Folsom, Schizophr. Bull. 35Fatemi, S. H. & Folsom, T. D. The neurodevelopmental hypothesis of schizophrenia, revisited. Schizophr. Bull. 35, 528-548 (2009).\n\nThe Encyclopedia of DNA elements (ENCODE): data portal update. C A Davis, Nucleic Acids Res. 46Davis, C. A. et al. The Encyclopedia of DNA elements (ENCODE): data portal update. Nucleic Acids Res. 46, D794-D801 (2018).\n\nPrivacy-preserving federated brain tumour segmentation. W Li, International Workshop on Machine Learning in Medical Imaging. Li, W. et al. Privacy-preserving federated brain tumour segmentation. International Workshop on Machine Learning in Medical Imaging 133-141 (2019).\n\nHASE: Framework for efficient high-dimensional association analyses. G V Roshchupkin, Sci. Rep. 636076Roshchupkin, G. V. et al. HASE: Framework for efficient high-dimensional association analyses. Sci. Rep. 6, 36076 (2016).\n\nExome sequencing and characterization of 49,960 individuals in the UK Biobank. C Van Hout, Nature. 586Van Hout, C. V et al. Exome sequencing and characterization of 49,960 individuals in the UK Biobank. Nature 586, 749-756 (2020).\n\nThree genome-wide association studies and a linkage analysis identify HERC2 as a human iris color gene. M Kayser, Am. J. Hum. Genet. 82Kayser, M. et al. Three genome-wide association studies and a linkage analysis identify HERC2 as a human iris color gene. Am. J. Hum. Genet. 82, 411-423 (2008).\n\nANNOVAR: functional annotation of genetic variants from high-throughput sequencing data. K Wang, M Li, H Hakonarson, Nucleic Acids Res. 38Wang, K., Li, M. & Hakonarson, H. ANNOVAR: functional annotation of genetic variants from high-throughput sequencing data. Nucleic Acids Res. 38, e164-e164 (2010).\n\nGeneSCF: a real-time based functional enrichment tool with support for multiple organisms. S Subhash, C Kanduri, BMC Bioinform. 17365Subhash, S. & Kanduri, C. GeneSCF: a real-time based functional enrichment tool with support for multiple organisms. BMC Bioinform. 17, 365 (2016).\n\nVisible machine learning for biomedicine. K Y Michael, Cell. 173Michael, K. Y. et al. Visible machine learning for biomedicine. Cell 173, 1562-1565 (2018).\n\nUsing deep learning to model the hierarchical structure and function of a cell. J Ma, Nat. Methods. 15Ma, J. et al. Using deep learning to model the hierarchical structure and function of a cell. Nat. Methods 15, 290-298 (2018).\n\nFrom genotype to phenotype: augmenting deep learning with networks and systems biology. V H Gazestani, N E Lewis, Curr. Opin. Syst. Biol. 15Gazestani, V. H. & Lewis, N. E. From genotype to phenotype: augmenting deep learning with networks and systems biology. Curr. Opin. Syst. Biol. 15, 68-73 (2019).\n\nComprehensive functional genomic resource and integrative model for the human brain. D Wang, Science. 3628464Wang, D. et al. Comprehensive functional genomic resource and integrative model for the human brain. Science 362, eaat8464 (2018).\n\nKeras: deep learning library for theano and tensorflow. F Chollet, Chollet, F. Keras: deep learning library for theano and tensorflow. https://keras.io/k (2015).\n\nTensorflow: a system for large-scale machine learning. M Abadi, 12th {USENIX} Symposium on Operating Systems Design and Implementation. Savannah{USENIX} AssociationAbadi, M. et al. Tensorflow: a system for large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16) 265-2833 ({USENIX} Association, Savannah, 2016).\n\n. A Van Hilten, 10.5281/ZENODO.5151527ArnovanHilten/GenNet: Release GenNet. 1van Hilten, A. et al. ArnovanHilten/GenNet: Release GenNet 1.4. https:// doi.org/10.5281/ZENODO.5151527 (2021).\n", "annotations": {"author": "[{\"end\":230,\"start\":93},{\"end\":233,\"start\":231},{\"end\":328,\"start\":234},{\"end\":432,\"start\":329},{\"end\":526,\"start\":433},{\"end\":722,\"start\":527},{\"end\":902,\"start\":723},{\"end\":1073,\"start\":903},{\"end\":1088,\"start\":1074},{\"end\":1305,\"start\":1089}]", "publisher": null, "author_last_name": "[{\"end\":108,\"start\":98},{\"end\":250,\"start\":243},{\"end\":343,\"start\":337},{\"end\":446,\"start\":435},{\"end\":542,\"start\":537},{\"end\":742,\"start\":736},{\"end\":917,\"start\":910},{\"end\":1087,\"start\":1080},{\"end\":1102,\"start\":1091}]", "author_first_name": "[{\"end\":97,\"start\":93},{\"end\":232,\"start\":231},{\"end\":240,\"start\":234},{\"end\":242,\"start\":241},{\"end\":336,\"start\":329},{\"end\":434,\"start\":433},{\"end\":532,\"start\":527},{\"end\":536,\"start\":533},{\"end\":731,\"start\":723},{\"end\":735,\"start\":732},{\"end\":907,\"start\":903},{\"end\":909,\"start\":908},{\"end\":1079,\"start\":1074},{\"end\":1090,\"start\":1089}]", "author_affiliation": "[{\"end\":229,\"start\":135},{\"end\":327,\"start\":252},{\"end\":431,\"start\":345},{\"end\":525,\"start\":448},{\"end\":638,\"start\":544},{\"end\":721,\"start\":640},{\"end\":821,\"start\":744},{\"end\":901,\"start\":823},{\"end\":1013,\"start\":919},{\"end\":1072,\"start\":1015},{\"end\":1225,\"start\":1131},{\"end\":1304,\"start\":1227}]", "title": "[{\"end\":90,\"start\":1},{\"end\":1395,\"start\":1306}]", "venue": null, "abstract": "[{\"end\":2509,\"start\":1437}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3067,\"start\":3066},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3766,\"start\":3765},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3971,\"start\":3967},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3975,\"start\":3971},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3979,\"start\":3975},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4236,\"start\":4234},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4619,\"start\":4616},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4621,\"start\":4619},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4903,\"start\":4901},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8577,\"start\":8575},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8602,\"start\":8600},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8660,\"start\":8658},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8886,\"start\":8883},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8888,\"start\":8886},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10215,\"start\":10213},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10448,\"start\":10446},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10643,\"start\":10639},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10647,\"start\":10643},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10651,\"start\":10647},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13655,\"start\":13633},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13658,\"start\":13656},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15514,\"start\":15512},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16097,\"start\":16095},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16278,\"start\":16274},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16282,\"start\":16278},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16286,\"start\":16282},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16288,\"start\":16286},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16572,\"start\":16570},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16691,\"start\":16689},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18774,\"start\":18772},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20735,\"start\":20733},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21835,\"start\":21833},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22897,\"start\":22895},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24008,\"start\":24006},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24069,\"start\":24067},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24640,\"start\":24618},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24643,\"start\":24641},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24899,\"start\":24877},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24902,\"start\":24900},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25885,\"start\":25881},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25889,\"start\":25885},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25893,\"start\":25889},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25897,\"start\":25893},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28971,\"start\":28968}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34714,\"start\":33722},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35527,\"start\":34715},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38983,\"start\":35528}]", "paragraph": "[{\"end\":3981,\"start\":2511},{\"end\":5334,\"start\":3983},{\"end\":5926,\"start\":5336},{\"end\":6760,\"start\":5938},{\"end\":7442,\"start\":6762},{\"end\":7764,\"start\":7444},{\"end\":8224,\"start\":7766},{\"end\":8304,\"start\":8226},{\"end\":9508,\"start\":8306},{\"end\":10008,\"start\":9510},{\"end\":10653,\"start\":10010},{\"end\":11953,\"start\":10655},{\"end\":12596,\"start\":11955},{\"end\":13475,\"start\":12598},{\"end\":14268,\"start\":13477},{\"end\":14683,\"start\":14283},{\"end\":16099,\"start\":14685},{\"end\":17146,\"start\":16101},{\"end\":17977,\"start\":17148},{\"end\":19171,\"start\":17979},{\"end\":19819,\"start\":19173},{\"end\":20678,\"start\":19831},{\"end\":21519,\"start\":20680},{\"end\":21663,\"start\":21521},{\"end\":22809,\"start\":21665},{\"end\":23755,\"start\":22811},{\"end\":24010,\"start\":23757},{\"end\":25025,\"start\":24012},{\"end\":25215,\"start\":25027},{\"end\":25899,\"start\":25217},{\"end\":26492,\"start\":25901},{\"end\":27665,\"start\":26494},{\"end\":28351,\"start\":27667},{\"end\":29114,\"start\":28353},{\"end\":29761,\"start\":29186},{\"end\":30124,\"start\":29763},{\"end\":30368,\"start\":30126},{\"end\":30932,\"start\":30370},{\"end\":31065,\"start\":30934},{\"end\":31221,\"start\":31104},{\"end\":31387,\"start\":31223},{\"end\":31673,\"start\":31389},{\"end\":32071,\"start\":31675},{\"end\":32874,\"start\":32073},{\"end\":33011,\"start\":32876},{\"end\":33470,\"start\":33033},{\"end\":33721,\"start\":33492}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":29185,\"start\":29115},{\"attributes\":{\"id\":\"formula_1\"},\"end\":31103,\"start\":31066}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":9395,\"start\":9388}]", "section_header": "[{\"end\":5936,\"start\":5929},{\"end\":14281,\"start\":14271},{\"end\":19829,\"start\":19822},{\"end\":33031,\"start\":33014},{\"end\":33490,\"start\":33473},{\"end\":33729,\"start\":33723},{\"end\":35536,\"start\":35529}]", "table": "[{\"end\":35527,\"start\":35384},{\"end\":38983,\"start\":36055}]", "figure_caption": "[{\"end\":34714,\"start\":33731},{\"end\":35384,\"start\":34717},{\"end\":36055,\"start\":35538}]", "figure_ref": "[{\"end\":6099,\"start\":6093},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7825,\"start\":7818},{\"end\":7832,\"start\":7826},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11572,\"start\":11563},{\"end\":12695,\"start\":12687},{\"end\":20926,\"start\":20920},{\"end\":28073,\"start\":28048}]", "bib_author_first_name": "[{\"end\":41239,\"start\":41238},{\"end\":41500,\"start\":41499},{\"end\":41502,\"start\":41501},{\"end\":41514,\"start\":41513},{\"end\":41516,\"start\":41515},{\"end\":41525,\"start\":41524},{\"end\":41535,\"start\":41534},{\"end\":41824,\"start\":41823},{\"end\":42094,\"start\":42093},{\"end\":42096,\"start\":42095},{\"end\":42103,\"start\":42102},{\"end\":42123,\"start\":42122},{\"end\":42133,\"start\":42132},{\"end\":42135,\"start\":42134},{\"end\":42456,\"start\":42455},{\"end\":42458,\"start\":42457},{\"end\":42468,\"start\":42467},{\"end\":42480,\"start\":42479},{\"end\":42482,\"start\":42481},{\"end\":42756,\"start\":42755},{\"end\":42768,\"start\":42767},{\"end\":42948,\"start\":42947},{\"end\":43120,\"start\":43119},{\"end\":43294,\"start\":43293},{\"end\":43296,\"start\":43295},{\"end\":43309,\"start\":43308},{\"end\":43311,\"start\":43310},{\"end\":43644,\"start\":43640},{\"end\":43652,\"start\":43651},{\"end\":43654,\"start\":43653},{\"end\":44039,\"start\":44038},{\"end\":44418,\"start\":44417},{\"end\":44729,\"start\":44728},{\"end\":44933,\"start\":44932},{\"end\":44942,\"start\":44941},{\"end\":44954,\"start\":44953},{\"end\":44963,\"start\":44962},{\"end\":45272,\"start\":45271},{\"end\":45283,\"start\":45282},{\"end\":45515,\"start\":45514},{\"end\":45517,\"start\":45516},{\"end\":45530,\"start\":45529},{\"end\":45539,\"start\":45538},{\"end\":45993,\"start\":45992},{\"end\":46002,\"start\":46001},{\"end\":46011,\"start\":46010},{\"end\":46022,\"start\":46021},{\"end\":46024,\"start\":46023},{\"end\":46036,\"start\":46035},{\"end\":46309,\"start\":46308},{\"end\":46518,\"start\":46517},{\"end\":46520,\"start\":46519},{\"end\":46754,\"start\":46753},{\"end\":46756,\"start\":46755},{\"end\":46963,\"start\":46962},{\"end\":47180,\"start\":47179},{\"end\":47182,\"start\":47181},{\"end\":47480,\"start\":47479},{\"end\":47490,\"start\":47489},{\"end\":47503,\"start\":47499},{\"end\":47847,\"start\":47846},{\"end\":48199,\"start\":48198},{\"end\":48201,\"start\":48200},{\"end\":48512,\"start\":48511},{\"end\":48758,\"start\":48757},{\"end\":48760,\"start\":48759},{\"end\":49072,\"start\":49071},{\"end\":49074,\"start\":49073},{\"end\":49351,\"start\":49350},{\"end\":49353,\"start\":49352},{\"end\":49365,\"start\":49364},{\"end\":49367,\"start\":49366},{\"end\":49614,\"start\":49613},{\"end\":49616,\"start\":49615},{\"end\":49626,\"start\":49625},{\"end\":49628,\"start\":49627},{\"end\":49851,\"start\":49850},{\"end\":49853,\"start\":49852},{\"end\":50064,\"start\":50063},{\"end\":50351,\"start\":50350},{\"end\":50353,\"start\":50352},{\"end\":50586,\"start\":50585},{\"end\":50843,\"start\":50842},{\"end\":51125,\"start\":51124},{\"end\":51133,\"start\":51132},{\"end\":51139,\"start\":51138},{\"end\":51430,\"start\":51429},{\"end\":51441,\"start\":51440},{\"end\":51663,\"start\":51662},{\"end\":51665,\"start\":51664},{\"end\":51858,\"start\":51857},{\"end\":52096,\"start\":52095},{\"end\":52098,\"start\":52097},{\"end\":52111,\"start\":52110},{\"end\":52113,\"start\":52112},{\"end\":52396,\"start\":52395},{\"end\":52608,\"start\":52607},{\"end\":52770,\"start\":52769},{\"end\":53089,\"start\":53088}]", "bib_author_last_name": "[{\"end\":41245,\"start\":41240},{\"end\":41511,\"start\":41503},{\"end\":41522,\"start\":41517},{\"end\":41532,\"start\":41526},{\"end\":41544,\"start\":41536},{\"end\":41832,\"start\":41825},{\"end\":42100,\"start\":42097},{\"end\":42120,\"start\":42104},{\"end\":42130,\"start\":42124},{\"end\":42143,\"start\":42136},{\"end\":42465,\"start\":42459},{\"end\":42477,\"start\":42469},{\"end\":42490,\"start\":42483},{\"end\":42765,\"start\":42757},{\"end\":42773,\"start\":42769},{\"end\":42779,\"start\":42775},{\"end\":42954,\"start\":42949},{\"end\":43129,\"start\":43121},{\"end\":43306,\"start\":43297},{\"end\":43324,\"start\":43312},{\"end\":43649,\"start\":43645},{\"end\":44048,\"start\":44040},{\"end\":44424,\"start\":44419},{\"end\":44431,\"start\":44426},{\"end\":44737,\"start\":44730},{\"end\":44939,\"start\":44934},{\"end\":44951,\"start\":44943},{\"end\":44960,\"start\":44955},{\"end\":44971,\"start\":44964},{\"end\":45280,\"start\":45273},{\"end\":45289,\"start\":45284},{\"end\":45527,\"start\":45518},{\"end\":45536,\"start\":45531},{\"end\":45546,\"start\":45540},{\"end\":45999,\"start\":45994},{\"end\":46008,\"start\":46003},{\"end\":46019,\"start\":46012},{\"end\":46033,\"start\":46025},{\"end\":46040,\"start\":46037},{\"end\":46317,\"start\":46310},{\"end\":46526,\"start\":46521},{\"end\":46764,\"start\":46757},{\"end\":46969,\"start\":46964},{\"end\":47188,\"start\":47183},{\"end\":47487,\"start\":47481},{\"end\":47497,\"start\":47491},{\"end\":47511,\"start\":47504},{\"end\":47851,\"start\":47848},{\"end\":48206,\"start\":48202},{\"end\":48516,\"start\":48513},{\"end\":48769,\"start\":48761},{\"end\":49083,\"start\":49075},{\"end\":49362,\"start\":49354},{\"end\":49376,\"start\":49368},{\"end\":49623,\"start\":49617},{\"end\":49635,\"start\":49629},{\"end\":49859,\"start\":49854},{\"end\":50067,\"start\":50065},{\"end\":50365,\"start\":50354},{\"end\":50595,\"start\":50587},{\"end\":50850,\"start\":50844},{\"end\":51130,\"start\":51126},{\"end\":51136,\"start\":51134},{\"end\":51150,\"start\":51140},{\"end\":51438,\"start\":51431},{\"end\":51449,\"start\":51442},{\"end\":51673,\"start\":51666},{\"end\":51861,\"start\":51859},{\"end\":52108,\"start\":52099},{\"end\":52119,\"start\":52114},{\"end\":52401,\"start\":52397},{\"end\":52616,\"start\":52609},{\"end\":52776,\"start\":52771},{\"end\":53100,\"start\":53090}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":196635851},\"end\":41446,\"start\":41113},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":215203501},\"end\":41719,\"start\":41448},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13726984},\"end\":42012,\"start\":41721},{\"attributes\":{\"id\":\"b3\"},\"end\":42337,\"start\":42014},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":54487395},\"end\":42712,\"start\":42339},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":245799510},\"end\":42909,\"start\":42714},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":12693097},\"end\":43070,\"start\":42911},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":22947725},\"end\":43242,\"start\":43072},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":23510665},\"end\":43475,\"start\":43244},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":85566420},\"end\":43892,\"start\":43477},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":227247122},\"end\":44261,\"start\":43894},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3477854},\"end\":44673,\"start\":44263},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2088679},\"end\":44864,\"start\":44675},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3397190},\"end\":45159,\"start\":44866},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9481286},\"end\":45465,\"start\":45161},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":49560076},\"end\":45902,\"start\":45467},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":222149359},\"end\":46242,\"start\":45904},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52958863},\"end\":46440,\"start\":46244},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":218495175},\"end\":46685,\"start\":46442},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4445839},\"end\":46892,\"start\":46687},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":215530813},\"end\":47094,\"start\":46894},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":91114457},\"end\":47338,\"start\":47096},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206233240},\"end\":47737,\"start\":47340},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7970691},\"end\":48026,\"start\":47739},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4901289},\"end\":48442,\"start\":48028},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7042867},\"end\":48647,\"start\":48444},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4714832},\"end\":48947,\"start\":48649},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1746979},\"end\":49272,\"start\":48949},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":9106724},\"end\":49548,\"start\":49274},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":17369027},\"end\":49785,\"start\":49550},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":52819026},\"end\":50005,\"start\":49787},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":203627027},\"end\":50279,\"start\":50007},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4920488},\"end\":50504,\"start\":50281},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":224821359},\"end\":50736,\"start\":50506},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":205327613},\"end\":51033,\"start\":50738},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":18261644},\"end\":51336,\"start\":51035},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9298633},\"end\":51618,\"start\":51338},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":49210890},\"end\":51775,\"start\":51620},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3783024},\"end\":52005,\"start\":51777},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":133120956},\"end\":52308,\"start\":52007},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":55701943},\"end\":52549,\"start\":52310},{\"attributes\":{\"id\":\"b41\"},\"end\":52712,\"start\":52551},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":6287870},\"end\":53084,\"start\":52714},{\"attributes\":{\"doi\":\"10.5281/ZENODO.5151527\",\"id\":\"b43\"},\"end\":53274,\"start\":53086}]", "bib_title": "[{\"end\":41236,\"start\":41113},{\"end\":41497,\"start\":41448},{\"end\":41821,\"start\":41721},{\"end\":42091,\"start\":42014},{\"end\":42453,\"start\":42339},{\"end\":42753,\"start\":42714},{\"end\":42945,\"start\":42911},{\"end\":43117,\"start\":43072},{\"end\":43291,\"start\":43244},{\"end\":43638,\"start\":43477},{\"end\":44036,\"start\":43894},{\"end\":44415,\"start\":44263},{\"end\":44726,\"start\":44675},{\"end\":44930,\"start\":44866},{\"end\":45269,\"start\":45161},{\"end\":45512,\"start\":45467},{\"end\":45990,\"start\":45904},{\"end\":46306,\"start\":46244},{\"end\":46515,\"start\":46442},{\"end\":46751,\"start\":46687},{\"end\":46960,\"start\":46894},{\"end\":47177,\"start\":47096},{\"end\":47477,\"start\":47340},{\"end\":47844,\"start\":47739},{\"end\":48196,\"start\":48028},{\"end\":48509,\"start\":48444},{\"end\":48755,\"start\":48649},{\"end\":49069,\"start\":48949},{\"end\":49348,\"start\":49274},{\"end\":49611,\"start\":49550},{\"end\":49848,\"start\":49787},{\"end\":50061,\"start\":50007},{\"end\":50348,\"start\":50281},{\"end\":50583,\"start\":50506},{\"end\":50840,\"start\":50738},{\"end\":51122,\"start\":51035},{\"end\":51427,\"start\":51338},{\"end\":51660,\"start\":51620},{\"end\":51855,\"start\":51777},{\"end\":52093,\"start\":52007},{\"end\":52393,\"start\":52310},{\"end\":52767,\"start\":52714}]", "bib_author": "[{\"end\":41247,\"start\":41238},{\"end\":41513,\"start\":41499},{\"end\":41524,\"start\":41513},{\"end\":41534,\"start\":41524},{\"end\":41546,\"start\":41534},{\"end\":41834,\"start\":41823},{\"end\":42102,\"start\":42093},{\"end\":42122,\"start\":42102},{\"end\":42132,\"start\":42122},{\"end\":42145,\"start\":42132},{\"end\":42467,\"start\":42455},{\"end\":42479,\"start\":42467},{\"end\":42492,\"start\":42479},{\"end\":42767,\"start\":42755},{\"end\":42775,\"start\":42767},{\"end\":42781,\"start\":42775},{\"end\":42956,\"start\":42947},{\"end\":43131,\"start\":43119},{\"end\":43308,\"start\":43293},{\"end\":43326,\"start\":43308},{\"end\":43651,\"start\":43640},{\"end\":43657,\"start\":43651},{\"end\":44050,\"start\":44038},{\"end\":44426,\"start\":44417},{\"end\":44433,\"start\":44426},{\"end\":44739,\"start\":44728},{\"end\":44941,\"start\":44932},{\"end\":44953,\"start\":44941},{\"end\":44962,\"start\":44953},{\"end\":44973,\"start\":44962},{\"end\":45282,\"start\":45271},{\"end\":45291,\"start\":45282},{\"end\":45529,\"start\":45514},{\"end\":45538,\"start\":45529},{\"end\":45548,\"start\":45538},{\"end\":46001,\"start\":45992},{\"end\":46010,\"start\":46001},{\"end\":46021,\"start\":46010},{\"end\":46035,\"start\":46021},{\"end\":46042,\"start\":46035},{\"end\":46319,\"start\":46308},{\"end\":46528,\"start\":46517},{\"end\":46766,\"start\":46753},{\"end\":46971,\"start\":46962},{\"end\":47190,\"start\":47179},{\"end\":47489,\"start\":47479},{\"end\":47499,\"start\":47489},{\"end\":47513,\"start\":47499},{\"end\":47853,\"start\":47846},{\"end\":48208,\"start\":48198},{\"end\":48518,\"start\":48511},{\"end\":48771,\"start\":48757},{\"end\":49085,\"start\":49071},{\"end\":49364,\"start\":49350},{\"end\":49378,\"start\":49364},{\"end\":49625,\"start\":49613},{\"end\":49637,\"start\":49625},{\"end\":49861,\"start\":49850},{\"end\":50069,\"start\":50063},{\"end\":50367,\"start\":50350},{\"end\":50597,\"start\":50585},{\"end\":50852,\"start\":50842},{\"end\":51132,\"start\":51124},{\"end\":51138,\"start\":51132},{\"end\":51152,\"start\":51138},{\"end\":51440,\"start\":51429},{\"end\":51451,\"start\":51440},{\"end\":51675,\"start\":51662},{\"end\":51863,\"start\":51857},{\"end\":52110,\"start\":52095},{\"end\":52121,\"start\":52110},{\"end\":52403,\"start\":52395},{\"end\":52618,\"start\":52607},{\"end\":52778,\"start\":52769},{\"end\":53102,\"start\":53088}]", "bib_venue": "[{\"end\":52858,\"start\":52850},{\"end\":41262,\"start\":41247},{\"end\":41562,\"start\":41546},{\"end\":41851,\"start\":41834},{\"end\":42159,\"start\":42145},{\"end\":42509,\"start\":42492},{\"end\":42798,\"start\":42781},{\"end\":42973,\"start\":42956},{\"end\":43141,\"start\":43131},{\"end\":43343,\"start\":43326},{\"end\":43669,\"start\":43657},{\"end\":44060,\"start\":44050},{\"end\":44450,\"start\":44433},{\"end\":44754,\"start\":44739},{\"end\":44997,\"start\":44973},{\"end\":45300,\"start\":45291},{\"end\":45663,\"start\":45548},{\"end\":46055,\"start\":46042},{\"end\":46325,\"start\":46319},{\"end\":46545,\"start\":46528},{\"end\":46772,\"start\":46766},{\"end\":46977,\"start\":46971},{\"end\":47200,\"start\":47190},{\"end\":47523,\"start\":47513},{\"end\":47863,\"start\":47853},{\"end\":48218,\"start\":48208},{\"end\":48528,\"start\":48518},{\"end\":48781,\"start\":48771},{\"end\":49093,\"start\":49085},{\"end\":49395,\"start\":49378},{\"end\":49652,\"start\":49637},{\"end\":49878,\"start\":49861},{\"end\":50130,\"start\":50069},{\"end\":50375,\"start\":50367},{\"end\":50603,\"start\":50597},{\"end\":50869,\"start\":50852},{\"end\":51169,\"start\":51152},{\"end\":51464,\"start\":51451},{\"end\":51679,\"start\":51675},{\"end\":51875,\"start\":51863},{\"end\":52143,\"start\":52121},{\"end\":52410,\"start\":52403},{\"end\":52605,\"start\":52551},{\"end\":52848,\"start\":52778},{\"end\":53160,\"start\":53124}]"}}}, "year": 2023, "month": 12, "day": 17}