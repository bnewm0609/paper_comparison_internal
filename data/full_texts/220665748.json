{"id": 220665748, "updated": "2023-10-06 12:50:41.022", "metadata": {"title": "SHEARer: Highly-Efficient Hyperdimensional Computing by Software-Hardware Enabled Multifold Approximation", "authors": "[{\"first\":\"Behnam\",\"last\":\"Khaleghi\",\"middle\":[]},{\"first\":\"Sahand\",\"last\":\"Salamat\",\"middle\":[]},{\"first\":\"Anthony\",\"last\":\"Thomas\",\"middle\":[]},{\"first\":\"Fatemeh\",\"last\":\"Asgarinejad\",\"middle\":[]},{\"first\":\"Yeseong\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Hyperdimensional computing (HD) is an emerging paradigm for machine learning based on the evidence that the brain computes on high-dimensional, distributed, representations of data. The main operation of HD is encoding, which transfers the input data to hyperspace by mapping each input feature to a hypervector, accompanied by so-called bundling procedure that simply adds up the hypervectors to realize encoding hypervector. Although the operations of HD are highly parallelizable, the massive number of operations hampers the efficiency of HD in embedded domain. In this paper, we propose SHEARer, an algorithm-hardware co-optimization to improve the performance and energy consumption of HD computing. We gain insight from a prudent scheme of approximating the hypervectors that, thanks to inherent error resiliency of HD, has minimal impact on accuracy while provides high prospect for hardware optimization. In contrast to previous works that generate the encoding hypervectors in full precision and then ex-post quantizing, we compute the encoding hypervectors in an approximate manner that saves a significant amount of resources yet affords high accuracy. We also propose a novel FPGA implementation that achieves striking performance through massive parallelism with low power consumption. Moreover, we develop a software framework that enables training HD models by emulating the proposed approximate encodings. The FPGA implementation of SHEARer achieves an average throughput boost of 104,904x (15.7x) and energy savings of up to 56,044x (301x) compared to state-of-the-art encoding methods implemented on Raspberry Pi 3 (GeForce GTX 1080 Ti) using practical machine learning datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.10330", "mag": "3044709474", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/islped/KhaleghiSTAKR20", "doi": "10.1145/3370748.3406587"}}, "content": {"source": {"pdf_hash": "4dac604d02c6422fd84c7afbab7a044d50777aed", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.10330v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3370748.3406587", "status": "BRONZE"}}, "grobid": {"id": "0ac25a7e03504a98c65ebb228cdcda91e41440bb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4dac604d02c6422fd84c7afbab7a044d50777aed.txt", "contents": "\nSHEARer : Highly-Efficient Hyperdimensional Computing by Software-Hardware Enabled Multifold AppRoximation\n\n\nBehnam Khaleghi bkhaleghi@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nSahand Salamat sasalama@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nAnthony Thomas ahthomas@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nFatemeh Asgarinejad fasgarinejad@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nYeseong Kim \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nTajana Rosing tajana@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nBehnam Khaleghi \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nSahand Salamat \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nAnthony Thomas \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nFatemeh Asgarine \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nSHEARer : Highly-Efficient Hyperdimensional Computing by Software-Hardware Enabled Multifold AppRoximation\n10.1145/nnnnnnn.nnnnnnnACM Reference Format:jad, Yeseong Kim, and Tajana Rosing. 2020. SHEARer : Highly-Efficient Hyperdimensional Computing by Software-Hardware Enabled Multifold AppRoximation. In Proceedings of (ISLPED '20). ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn\nHyperdimensional computing (HD) is an emerging paradigm for machine learning based on the evidence that the brain computes on high-dimensional, distributed, representations of data. The main operation of HD is encoding, which transfers the input data to hyperspace by mapping each input feature to a hypervector, accompanied by so-called bundling procedure that simply adds up the hypervectors to realize encoding hypervector. Although the operations of HD are highly parallelizable, the massive number of operations hampers the efficiency of HD in embedded domain. In this paper, we propose SHEARer , an algorithm-hardware cooptimization to improve the performance and energy consumption of HD computing. We gain insight from a prudent scheme of approximating the hypervectors that, thanks to inherent error resiliency of HD, has minimal impact on accuracy while provides high prospect for hardware optimization. In contrast to previous works that generate the encoding hypervectors in full precision and then ex-post quantizing, we compute the encoding hypervectors in an approximate manner that saves a significant amount of resources yet affords high accuracy. We also propose a novel FPGA implementation that achieves striking performance through massive parallelism with low power consumption. Moreover, we develop a software framework that enables training HD models by emulating the proposed approximate encodings. The FPGA implementation of SHEARer achieves an average throughput boost of 104,904\u00d7 (15.7\u00d7) and energy savings of up to 56,044\u00d7 (301\u00d7) compared to state-of-the-art encoding methods implemented on Raspberry Pi 3 (GeForce GTX 1080 Ti) using practical machine learning datasets.\n\nINTRODUCTION\n\nNetworked sensors with native computing power -otherwise known as the \"internet of things\" (IoT) -are a rapidly growing source of data. Applications based on IoT devices typically use machine learning (ML) algorithms to generate useful insights from data. While modern machine learning techniques -in particular deep neural networks (DNNs) -can produce state-of-the-art results, ISLPED '20, , 2020. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn they often entail substantial memory and compute requirements which may exceed the resources available on light-weight edge devices. Thus, there is a pressing need to develop novel machine learning techniques which provide accuracy and flexibility while meeting the tight resource constraints imposed by edge-sensing devices.\n\nHyperdimensional computing -HD for short -is an emerging paradigm for machine learning based on evidence from the neuroscience community that the brain \"computes\" on high-dimensional, distributed, representations of data [1][2][3][4][5]. In HD, the primitive units of computation are high-dimensional vectors of length d hv sampled randomly from the uniform distribution over the binary cube {\u00b11} d hv . Typical values of d hv are in the range 5-10,000. Because of their high-dimensionality, any randomly chosen pair of points will be approximately orthogonal (that is, their inner product will be approximately zero). A useful consequence of this is that sets can be encoded simply by summing (or \"bundling\") together their constituent vectors. For any collection of vectors P, Q, V their element-wise sum S = P + Q + V is, in expectation, closer to P, Q and V than any other randomly chosen vector in the space.\n\nGiven HD representations of data, this provides a simple classification scheme: we simply take the data points corresponding to a particular class and superimpose them into a single representation for the set. Then, given a new piece of data for which the correct class label is unknown, we compute the similarity with the hypervectors representing each class and return the label corresponding to the most similar one. More formally, suppose we are given a set of labeled data X = {(x i , y i )} N i=1 where x \u2208 R d iv corresponds to an observation in low-dimensional space and y \u2208 C is a categorical variable indicating the class to which a particular x belongs. In general, HD classification proceeds by generating a set of \"class hypervectors\" which represent the training data corresponding to each class. Then, given a piece of data for which we do not know the correct label -the \"query\" -we simply compute the similarity between the query and each class hypervector and return the label corresponding to the most similar. This process is illustrated in Figure 1.\n\nSuppose we wish to generate the class hypervector corresponding to some class k \u2208 C. The prototype can be generated simply by superimposing (also called \"bundling\" in the literature) the HDencoded representation of the training data corresponding to that particular class [1,6]:\nC k = i s.t. y i =k enc(x i )(1)\nwhere enc : R d iv \u2192 {\u00b11} d hv is some encoding function which maps a low-dimensional signal to a binary HD representation. Then, given some piece of \"query\" data x q for which we do not know the \nk \u22c6 = argmax k \u2208 C \u03b4 (enc(x q ), C k )(2)\nwhere \u03b4 is an appropriate similarity metric. Common choices for \u03b4 include the inner-product/cosine distance -appropriate for integer or real valued encoding schemes -and the hamming distanceappropriate for binary HD representations. This phase is commonly referred to in literature as \"associative search\". Despite the simplicity of this \"learning\" scheme, HD computing has been successfully applied to a number of practical problems in the literature ranging from optimizing the performance of web-browsers [7], to DNA sequence alignment [8,9], bio-signal processing [10,11], robotics [12,13], and privacy preserving federated learning [14,15]. The primary appeal of HD computing lies in its amenability to implementation in modern hardware accelerators. Because the HD representations (e.g. \u03d5(x)) are simply long Boolean vectors, they can be processed extremely efficiently in highly parallel platforms like GPUs, FPGAs and PIM architectures. The principal challenge of HD computing -and the focus of this paper -lies in designing good encoding schemes which (1) represent the data in a format suitable for learning and (2) are efficient to implement in hardware. In general, the encoding phase is the most expensive stage in the HD learning pipeline -in some cases taking up to 10\u00d7 longer than training or prediction [16]. Existing encoding methods require generating hypervectors in full integer-precision and then ex-post quantizing to {\u00b11}. While this accelerates the associative search phase, it does not address encoding which is the primary source of inefficiency.\n\nIn this work, we propose novel techniques to compute the encodings in an approximate manner that saves a substantial amount of resources with an insignificant impact on accuracy. Of independent interest is our novel FPGA implementation that achieves striking performance through massive parallelism with low power consumption. Approximate encodings entail models to be trained in a similar approximate fashion. Thus we also develop a software emulation to enable users to train desired HD models. Our software framework enables users to explore the tradeoff between the degree of approximation, accuracy, and resource utilization (hence power consumption) by generating a pre-compiled library that correlates approximation schemes and FPGA resource utilization and power consumption. We show our procedure leads to performance improvement of 104,904\u00d7 (15.7\u00d7) and energy savings of up to 56,044\u00d7 (301\u00d7) compared to state-of-the-art encoding methods implemented on Raspberry Pi 3 (GeForce GTX 1080 Ti).\n\n\nBACKGROUND AND MOTIVATION 2.1 HD Encoding Algorithms\n\nThe literature has proposed a number of encoding methods for the multitude of data types which arise in practical learning settings. We here focus on a method from [1,6,17] which we refer to as \"ID-vector\" based encoding. This encoding method is widely used (see for instance: [10,[17][18][19]) and works well on both discrete and continuous data. We focus the discussion on continuous data as discrete data is a simple extension.  Suppose we wish to encode some set of vectors\nX = {x i } N i=1\nwhere x i is supported on some compact subset of R d iv . To begin, we first quantize the domain of each feature into a set of L discrete values L = {l i } L i=1 and assign each l i \u2208 L a codeword L i \u2208 {\u00b11} d hv . To preserve the ordinal relationship between the quantizer bins (the l i ), we wish the similarity between the codewords L i , L j to be inversely proportional to distance between the corresponding quantization bins; e.g. \u03b4 (L i , L j ) \u221d |l i \u2212 l j | \u22121 . To enforce this property we generate the codeword L 1 corresponding to the minimal quantizer bin l 1 by sampling randomly from {\u00b11} d hv . The codeword for the second bin is generated by flipping d hv 2\u00b7L random coordinates in L 1 . The codeword for the third bin is generated analogously from L 2 and so on. Thus, the codewords for the minimal and maximal bins are orthogonal and \u03b4 (L i , L j ) decays as |j \u2212 i | increases. This scheme is appropriate for quantizers with linearly spaced bins -however, it can be extended to variable bin-width quantizers.\n\nTo complete the description of encoding, let q(x i ) be a function which returns the appropriate codeword L \u2208 L for a component x i \u2208 x. Then encoding proceeds as follows:\nX = d iv j=i q(x i ) \u2297 P i(3)\nWhere P i is a \"position hypervector\" which encodes the index of the feature value (e.g. i \u2208 {1, .., d iv }) and \u2297 is a \"binding\" operation which is typically taken to be XOR.\n\n\nMotivation\n\nWhile the basic operations of HD are simple, they are numerous due to its high-dimensional nature. Prior work has proposed varied algorithmic and hardware innovations to tackle the computational challenges of HD. Acceleration in hardware has typically focused on FPGAs [20][21][22] or ASIC-ish accelerators [23,24]. FPGA-based implementations provide a high degree of parallelism and bit-level granularity of operations that significantly improves the performance and effective utilization of resources. Furthermore, FPGAs are advantageous over more specialized ASICs as they allow for easy customization of model parameters such as lengths of hypervectors (d hv ) and input-vectors (d iv ) along with the number of quantization levels. This flexibility is important as learning applications are heterogeneous in practice. Accordingly, we here focus on an FPGA based implementation but emphasize our techniques are  generic and can be integrated with ASIC- [23] and processor-based [24] implementations. As noted in the preceding section, the element-wise sum is a critical operation in the encoding pipeline. Thus, popcount operations play a critical role in determining the efficiency of HD computing. Figure 2(a) shows a popular tree-based implementation of popcount that adds d iv binary bits (note that we can replace '\u22121's by 0 in the hardware). Each six-input look-up table (LUT-6) of conventional FPGAs consists of two LUT-5. Hence, we can implement the first stage of the tree using d iv 3 of three-port one-bit adders. Each subsequent stage comprises two-port k-bit adders where k increases by one at each stage, while the number of adders per stage decreases by a factor of 1 2 . A n-bit adder requires n LUT-6. Thus, the number of LUT-6 for a d iv -input popcount can be formulated as Equation (4).\nn LUT6 (adder-tree) = log d iv i=1 d iv 3 \u00d7 i 2 i\u22121 \u2243 4 3 d iv(4)\nHD operations can be parallelized at the granularity of a single coordinate in each hypervector: all dimensions of the encoding hypervector and associative search can be computed in parallel. Nonetheless, Equation (4) reveals that the popcount module for a popular benchmark dataset [25] with 617 features per input requires \u223c820 LUTs. This limits a mid-size low-power FPGA with \u223c50K LUTs [26] to generate only \u223c60 encoding dimension per cycle (out of d hv \u2243 5,000).\n\nTo save resources, [22] and [23] suggest using counters to implement the popcount for each dimension of encoding, as shown in Figure 2 (b). Although this seems more compact, in practice, it is less efficient than an adder-tree implementation: the counter-based implementation needs \"log d iv \" LUTs per dimension, with a perdimension latency of d iv cycles, while adder-trees require O( 4 3 d iv ) LUTs per dimension with a per-dimension throughput of one cycle, so for a given amount of resources, the conventional adder-tree is 3 4 log d iv \u00d7 more performance-efficient.\n\nWork in [20] and [21] quantize the dimensions of encoding and class hypervectors which eliminates DSP modules (or large number of cascaded LUTs) that are conventionally used for the associative search stage, since, through quantization, inner product for cosine similarity will be replaced by popcount operations in case of binary quantization, or lower-bit multiplications. The resulting improvement is minor because the quantization is applied after full-bit encoding. Furthermore, the multipliers of the associative search stage have input widths of w enc (from encoding dimensions) and w cl ass (from class dimensions), so each one needs O(w enc \u00d7 w cl ass ) LUTs. Pessimistically assuming bit-widths up to w enc = w cl ass = 16, an extreme binary quantization can eliminate 256 LUTs required for multiplication. However, the savings are again modest at best in practice: on the benchmark dataset mentioned previously, only w e nc \u00d7w cl as s w e nc \u00d7w cl as s + 4 3 d iv \u2243 23%. Therefore, in this paper, we target the popcount portion that contributes to the more significant part of resources. Indeed, ex-post quantizing of encoding hypervectors can be orthogonal to our technique for further improvement.\n\n\nPROPOSED METHOD: SHEARer 3.1 Approximate Encoding\n\nIn the previous section, we explained prior work that applies quantization after obtaining the encoding hypervector in full bit-width.\n\nAs noted there, while this approach is simple it only accelerates the associative search phase and does not improve encoding -which is often the principal bottleneck. Because the HD representation of data entails substantial redundancy and information is uniformly distributed over a large number of bits, it is robust to bit-level errors: flipping 10% of hypervectors' bits shows virtually zero accuracy drop, while 30% bit-error impairs the accuracy by a mere 4% [27]. We leverage such resilience to improve the resource utilization through approximate encoding, as shown in Figure 3. In the following, we discuss each technique in greater detail and estimate its resource usage.\n\n(1) Local majority. From Equation (4) we can observe that the number of resources (in terms of LUT-6) of the exact adder-tree to see that the complexity encoding each dimension linearly depends on the number of data features, d iv . We, therefore, aim to reduce the number of inputs to the primary adder-tree by sub-sampling using the majority function so as to shrink the tree inputs while (approximately) extracting the information contained in the input. Note that, here, 'inputs' are the binary dimensions of the level hypervectors (see Figure 1 2 and Figure 2). As shown in Figure  3(a), each LUT-6 is configured to return the majority of its six input bits. When three out of six inputs are 0/1, we break the tie by designating all LUTs that perform majority functions of a specific encoding dimension to deterministically output 0 or 1. We specify this randomly for every dimension (i.e., an entire adder-tree) but it remains fixed for a model during the training and inference. We choose groups of six bits as a single LUT-6 can vote for up to six inputs. Using smaller majority groups diminishes the resource saving, especially taking the majorities adds extra LUTs. Moreover, following the Shannon decomposition, implementing a 'k +1'-input LUT requires two k-input LUTs (and a two-input multiplexer). Thus, the number of LUTs for majority groups larger than six inputs grows exponentially.\n\nThere are d iv 6 MAJ LUTs in the first stage of Figure 3(a), hence the number of inputs for the subsequent adder-tree reduces to d iv 6 . From Equation (4) we also know that a k-input adder-tree requires  \nMAJ LUT-6 d iv 6 + adder-tree 4 3 d iv 6 = 7 18 d iv LUT-6(5)\nThis uses 1 \u2212 7 /18 4 /3 = 70.8% less LUT resources than an exact addertree.\n\nIn [21], the authors report an average accuracy loss of 1.6% by post-hoc quantizing the encodings to binary. Thus, one might think of repeating the majority functions in the subsequent stages to obtain final one-bit encoding dimensions. Using local majority functions is efficient, but degrades the encoding quality as majority is not associative. In particular, the MAJ LUTs add another layer of approximation by breaking ties. Thus, a so-called MAJ-tree causes considerable accuracy loss. Therefore, in our cascaded-MAJ design in Figure 3 6% resources compared to exact encoding. We emphasize that a cascaded all-MAJ popcount needs \u2243 i=1 1 6 i = 0.2d iv LUTs, which saves 85.0% of LUTs. So the two-stage MAJ implementation with 82.6% resource saving is nearly optimal because the first two stages of the exact tree were consuming the most resources.\n\n(2) Input overfeeding. In Figure 2(a) we can observe that each LUT-5 pair of the first stage computes s 1 s 0 = hv i + hv i+1 + hv i+2 . Since only three (out of five) inputs of them are used, these LUTs left underutilized. With one more input, the output range will be [0\u22123], which requires three bits (outputs) to represent, so we cannot add more than three bits using two LUT-5s. However, instead of using the LUT-5s to carry out regular addition, we can supply a pair of LUT-5s with five inputs to perform quantized/truncated addition. For actual outputs (sum of five bits) of 0 or 1, the LUT-5 pair would produce 00 (zero); for 2 or 3 they produce 01 (one), and for 4 or 5 they produce 10 (two). That is one LUT-5 computes the actual carry out of the five bits, and the other computes MSB of the sum. To ensure that the synthesis tool infers a single LUT-6 for each pair, we can directly instantiate LUT primitives. As a LUT-6 comprises a LUT-5 pair (with shared inputs), the number of resources of Figure   3(c) is:\nlog d iv i=1 d iv 5 \u00d7 i 2 i\u22121 \u2243 4 5 d iv LUT-6(7)\nThe first stage encompasses d iv 5 LUT-6s, and each subsequent stage contains i-bit adders while their count decreases by 1 2 \u00d7 at each stage. Total number of LUTs is reduced by 1 \u2212 4 /5 4 /3 = 40% (the same ratio of over-use of inputs). The saving is smaller than the local majority approach but we expect higher accuracy due to intuitively more moderate imposed approximation.\n\n(3) Truncated nodes. Out of 4 3 d iv LUTs used in an exact addertree, d iv (75%) are used in the intermediate adder units. More precisely, following i 2 i ratio (see Equation (4)), stages 1-4 of the adder contribute to 25%, 25%, 18.75%, and 12.5% of the total resources, respectively. Note that, although the number of adder units halves at each stage, the area of each one increases linearly. We avoid a blowup of adder sizes by truncating the least significant bit (LSB) of each adder. As demonstrated in Figure 3(d), the LSB of the second stage (which is supposed to have three-bit output) is discarded. Thus, instead of using two LUT-6s to compute s 2 s 1 s 0 = a 1 a 0 + b 1 b 0 , we can use two LUT-5s (equivalent to one LUT-6) to obtain s 2 s 1 = a 1 a 0 + b 1 b 0 , where one LUT-5 computes s 2 and the other produces s 1 using four inputs a 0 , a 1 , b 0 , and b 1 . Truncating the output of the second stage consequently decreases the output bit-width of the third stage by one bit as its inputs became two bits. Thus, we can apply the LSB truncating to the third stage to implement it using two LUT-5s, as well. We can apply the same procedure in all the consecutive nodes and implement them by only two LUT-5s. The output of the first stage is already two bits so we do not modify its original implementation.\n\nWe apply truncating to first stages particularly from the left side of Equation 4 we can perceive the first five stages that contribute to \u223c90% of the adder-tree resources. Otherwise, the decay in accuracy becomes too severe. Equation (8) characterizes the resource usage of the adder-tree in which the first k stages are implemented using 2-bit adders shown in Figure 3(d) (including the stage one, which uses the primary exact mode). the first k stages k i=1 d iv 3 We can see that for k = 1 -i.e. when none of intermediate stages are truncated -the equation returns 4 3 d iv which is equal to resources of an exact adder-tree. Setting k to 2, 3, and 4 achieves 25%, 37.5%, and 43.75% resource saving, respectively.\n1 2 i\u22121 + subsequent stages log d iv i=k+1 d iv 3 i + 1 \u2212 k 2 i\u22121 \u2243 d iv 3 (2 +\n\nSHEARer Architecture\n\nRecall from Figure 1, that the HD encoding procedure needs to convert all input features to equivalent level hypervectors, bind them with the associated ID hypervector, and bundle together (e.g. sum) the resulting hypervectors to generate the final encoding. FPGAs, however, contain limited logic resources as well as on-chip SRAMbased memory blocks (a.k.a BRAMs) to provide high performance with affordable power. Previous work, therefore, break down this step into multiple cycles whereby at each cycle they process d se\u0434 dimensions [19,20,28]. When processing dimensions n \u00b7 d se\u0434 to (n + 1) \u00b7 d se\u0434 , those architectures fetch the same dimensions of all L level hypervectors. Each of d se\u0434 adder-trees are augmented with L-to-1 multiplexers in all of their d iv input ports, where the k th \u2264 d se\u0434 adder-tree's multiplexers are connected to k th dimension of the fetched level hypervectors, and the (quantized) value of associated feature selects the right level dimension to pass. The advantage of such architectures is that only d se\u0434 \u00b7 L bits need to be fetched at each cycle. However, it requires d iv \u00b7 d seq multiplexers. For a modest L = 16, which translates to 16-input multiplexers occupying four LUTs, the total number of LUTs used for multiplexers will be 4 \u00b7 d iv \u00b7 d seq , the (exact) adder-trees occupy d seq \u00b7 4 3 d iv (in Equation (4) we showed that a d iv input exact adder-tree uses 4 3 d iv LUTs). This means that the augmented multiplexers occupy 3\u00d7 LUTs of the adder area.In our approximate encoding, this ratio would be even larger as we trim the exact adder. Thus, multiplexerbased implementation overshadows the gain of approximating the adders as we need to preserve the copious multiplexers.\n\nTo address this issue, we propose a novel FPGA implementation that relies on on-chip memories rather than adding extra resources. Figure 4 illustrates an overview of the SHEARer FPGA architecture. At each cycle, we partially process F (out of d iv ) input features, where F \u2264 d iv . Our implementation is BRAM-oriented, so each (quantized) feature translates to the address from which the corresponding level hypervector can be read. This entails a dedicated memory block group for each of F features currently being processed. The number of BRAMs in a group is equal to group size = L \u00b7d hv C bram as there are L different level hypervectors of length d hv bits, for a memory capacity of C bram bits. Therefore, the number of features F that can be partially processed in a cycle is limited to F < 2 total BRAMs group size . The coefficient 2 is because the BRAMs have two ports from which we can independently read (that is why in Figure 4 two pixels share the same BRAM group). The address translator -\"level to address\" in Figure 4) -activates only the right BRAM and row of the group, so the other BRAMs do not dissipate dynamic power. Depending on its configuration, each memory block can deliver up to d mem bits, as indicated in the figure. Certainly, we could double the d mem by duplicating the size of memory groups to process more dimensions per cycle, but then F -the number of features that can be processed -halves. Each of d mem fetched level hypervector bit is XORed with the corresponding bit of the ID (position) hypervector. As detailed in Section 2.1, each feature index is associated with an ID hypervector, which is a randomly chosen (but fixed) hypervector of length d hv . We thus require d hv \u00b7d iv C bram additional BRAM blocks to store ID hypervectors. This further limits the number of features that can be processed in a cycle due to BRAM shortage. To resolve this, we only store a single ID hypervector (seed ID) and generate the other ones by rotating the seed ID, i.e., ID of index k can be obtained by rotating the ID of index 1 (seed ID) by k \u2212 1. This does not affect the HD accuracy as the resulting ID hypervectors are still iid and approximately orthogonal. For the first feature, we need to read d mem bits, while for the subsequent F \u2212 1 features we need one more bit as each ID has d mem \u2212 1 common bits with its predecessor. Therefore we need a data-width of d mem + F \u2212 1 for ID memory, meaning that we need 1 + F d mem memory blocks of the seed ID hypervector. Thus, although the seed ID fits in a single BRAM, the required data-width demands more memory blocks. However, this is still significantly smaller than the case of storing all different IDs in BRAM blocks, which either releases BRAMs for processing the features, or power gates the unused BRAMs. Moreover, using seed ID BRAM also saves dynamic power as d mem + F \u2212 1 bits are read (compared to d mem \u00d7 F of storing different IDs). It is also noteworthy that at each cycle the first d mem bits read from the ID memory are passed to the first feature of the features currently being processed (i.e., feature 1, F + 1, 2F + 1, \u00b7 \u00b7 \u00b7 ). Similarly, bits 2 to d mem + 1 of the fetched ID are passed to the second feature, and so on. Thus, the output of ID BRAMs to processing logic needs a fixed routing.\n\nAfter XORing the fetched level hypervectors with the ID hypervectors, each of the d mem approximate adder-trees add up F binary bits, so the input size of all adders is F . Since the result is only the sum of the first F features, SHEARer utilizes a buffer to store these partial sums. In the next cycle, the procedure repeats for the next group of features, i.e., features F + 1 to 2F . Therefore, SHEARer produces d mem encoding dimensions in d iv F cycles, hence the entire encoding hypervector is generated in d hv d mem \u00d7 d iv F cycles. To make these tangible, in the Xilinx FPGAs we use for experiments, d mem is 64 and C br am = 512 r ow \u00d7 64 col . We also noticed that 16 level hypervectors gives the same accuracy of having more, so we set L = 16. We also select the hypervector lengths to be a multiple of 512. Taking the previously mentioned language recognition benchmark [25] as an example, we observed that d hv = 2,560 provides acceptable accuracy (see Section 4 for more details). For ISLPED '20, , Behnam Khaleghi, Sahand Salamat, Anthony Thomas, Fatemeh Asgarinejad, Yeseong Kim, and Tajana Rosing this benchmark we thus need group size of 16\u00d72560 512\u00d764 = 2 BRAMs, where each group can cover two input features. The FPGA we use has a total 445 BRAMs, which can make at most 445 2 = 222 groups, capable of processing 444 features per cycle. Therefore, we divide 617 input features of the benchmark into two repeating cycles using 310 BRAMs (155 BRAM groups) to process the first 310 features in the first cycle, and the rest 307 cycles in the second cycle, generating d mem = 64 encoding dimensions per 2 cycles. All 64 adder-trees have a 1-bit input sizes of 310. The entire encoding takes 2560 dim \u00d7 2 cycles 64 dim = 80 cycles. Note that reading from onchip BRAMs has just one cycle latency and the off-chip memory latency is buried in the computation pipeline.\n\n\nSoftware Layer\n\nBecause of approximation, the output of encoding and hence the class hypervectors are different than training with exact encoding. Therefore we also need to train the model using the same approximate encoding(s), as the associative search only looks for the similarity (rather than exactness) of an approximately encoded hypervector with trained class hypervectors -which are made up by bundling a manifold of encoding hypervectors. Our FPGA implementation is tailored for inference, so we carry out the training step on CPU. We developed an efficient SIMD vectorized Python implementation to emulate the exact and the proposed encoding techniques in software. The emulation of the proposed techniques is straightforward. For instance, for the local majority approximation (Figure 3(a)), instead of adding up all d iv hypervectors, we divide them to groups of six hypervectors, add up all six hypervectors of each group, and compare if each resultant dimension is larger than 3. We also break the ties in software by generating a constant vector dictating how the ties of each dimension should be served. This acts as the MAJ LUTs of the first stage. Thereafter, we simply add up all these temporary hypervectors to realize the subsequent exact adders. This guarantees to match the software output with approximate hardware's, while we also achieve a fast implementation by avoiding unnecessary imitation of hardware implementation.\n\nIn addition to d iv that is the dataset's attribute, d hv , \u03b1, epochs (number of training epochs) are the other variables of our software implementation. \u03b1 is the learning rate of HD. As explained in Section 1, HD bundles all encoding hypervectors belonging to the same-label data to create the initial class hypervectors. In the subsequent epochs iterations, HD updates the class hypervectors by observing if the model correctly predicts the training data. If the model mispredicts an encoded query H l of label l as class C l \u2032 , HD updates as shown by Equation (9). If learning rate \u03b1 is not provided, SHEARer finds the best \u03b1 through bisectioning for a certain number of iterations.\nC l = C l + \u03b1 \u00b7 H l C l \u2032 = C l \u2032 \u2212 \u03b1 \u00b7 H l(9)\nWe supply the software implementation of SHEARer with the number of BRAM and LUT resources of the target FPGA to estimate the architectural parameters according to Section 3.2 as well as using the resource utilization formulated in Section 3.1. We have also implemented the exact and approximate adder-trees of different input sizes and interpolated their measured power consumption -  \n\n\nEXPERIMENTAL RESULTS\n\n(1) General Setup. We have implemented the SHEARer architecture using Vivado High-Level Synthesis Design Suite on Xilinx Kintex-7 FPGA KC705 Evaluation Kit which embraces a XC7K325T device with 203,800 LUT-6 and 445 36 Kb BRAM memory blocks that we use in 512\u00d764bit configuration. By pipelining the adder-tree stages we could achieve a clock frequency of 200 MHz. We compare the performance and energy results with the high-end NVIDIA GeForce GTX 1080 Ti GPU, and Raspberry Pi 3 embedded processor. We optimize the CUDA implementation by packing the hypervectors within 32-bit integers, so a single logical XOR operation can bind 32 dimensions. We use speech [25], activity [29], and handwritten digit [30] recognition as well as a face detection dataset [31] as our benchmarks. Table 1 summarizes the length of hypervectors and associated accuracy of each dataset in the baseline exact mode. For a fair comparison, we first obtained the accuracies using d hv = 10,000, then decreased it until the accuracies remain within 0.5% of the original values. This avoids over-saturated hypervectors and accuracy drop due to approximation manifests better.\n\n(2) Resource Utilization. To validate the efficiency of the proposed approximation techniques, in addition to holistic high-level performance and energy comparisons, we examine them by synthesizing a 512-input adder-tree. Table 2 represents the LUT utilization of the adder implemented in exact and approximate modes. MAJ, MAJ-2, over-feed and truncate refer to the designs of Figure 3  (3) Accuracy. Table 3 summarizes the accuracies of the proposed encodings relative to the exact encoding. LUT saving, which is dataset-independent, is represented again for the comparison purpose. \"trunc-3\" and \"trunc-4\" stand for truncated encoding (Figure 3(d)) where, respectively, three and four intermediate stages are truncated. Overall, MAJ encoding (one-stage local majority shown in Figure 3(a)) achieves an acceptable accuracy with significant resource saving, though it is not always the highest-accurate one. For instance, in the face detection benchmark, the over-feed and 3-stage truncated encodings offer slightly better accuracy. More interestingly, in the digit recognition dataset, trunc-4 shows a negligible \u22120.1% accuracy drop while trunc-3 even improves the accuracy by 0.1%. This can stem from the fact that emulating the hardware approximation in SHEARer 's software layer takes a long time for the digit dataset, so we limited the software to try five different learning rate (\u03b1) and repeat the entire training for five times (with epochs = 50) so the result might be slightly skewed. For the other datasets we conducted the training for 25 times each with 50 epochs to average out the variance of results.\n\n(4) Performance. Figure 5 compares the throughput of SHEARer FPGA implementation with Raspberry Pi and Nvidia GPU. SHEARer implementation is BRAM-bound, so all the exact and approximate implementations yield the same performance. In Section 3.2 we elaborated that the speech dataset requires two cycles per d mem = 64 dimensions. We can similarly show that activity and digit datasets also need two cycles per 64 dimensions, while digit requires three cycles as its level hypervectors are larger (d hv = 6,144) and occupy more BRAMs. In the worst scenario, SHEARer improves the throughput by 58,333\u00d7 and 6.7\u00d7 compared to Raspberry Pi and GPU implementation. On average SHEARer provides a throughput of 104,904\u00d7 and 15.7\u00d7 as compared to Raspberry Pi and GPU, respectively. The substantial improvements arise from that SHEARer adds up d iv 2 \u00d7 64 (e.g., \u223c25,000) numbers per cycle while also performs the binding (XOR operations) on the fly.   Figure 6: Energy (Joule) consumption of SHEARer , Raspberry Pi and GPU for 10 million inference. Y-axis is logarithmic.\n\nHowever, Raspberry Pi executes sequentially and also its cache cannot fit all the class hypervectors with non-binary dimensions. Note that we assume that dataset is available in the off-chip memory (DRAM) of the FPGA. Otherwise, although per-sample latency would be affected, throughput remains the same as the off-chip memory latency is buried in the computation cycles.\n\n(5) Energy Consumption. Figure 6 compares the energy consumption of the exact and approximate SHEARer implementations with Raspberry Pi and GPU. We have scaled the energy to 10 million inferences for the sake of illustration (Y-axis is logarithmic). We used Hioki 3334 power meter and NVIDIA system management interface to measure the power consumption of Raspberry Pi and GPU, respectively. We used Xilinx Power Estimator (XPE) to estimate the FPGA power consumption. The average power of Raspberry Pi for all datasets hovers around 3.10 Watt, while this is \u223c120 Watt for the GPU. In FPGA implementation, powers showed more variation as the number of active LUTs and BRAMs differ between applications. E.g., The face dataset with two-stage majority encoding (MAJ-2) consumes 3.11 Watt, while the digit recognition dataset in the exact mode consumes 10.80 Watt. The smaller power consumption of face is mainly because of smaller off-chip data transfer as face has the largest hypervector length and takes 288 cycles to process an entire input, while for digit it takes 64 cycles. On average, SHEARer 's exact encoding decreases the energy consumption of by 45,988\u00d7 and 247\u00d7 (average of all datasets) as compared to Raspberry Pi and GPU implementations. MAJ-2 encoding of SHEARer consumes the minimum energy, which throttles the energy consumption by 56,044\u00d7 and 301\u00d7 compared to Raspberry Pi and GPU, respectively. Note that power improvement of the approximate encodings is not proportional to their resource (LUT) utilization as BRAM power remains the same for all encodings.\n\n\nCONCLUSION\n\nIn this paper, we leveraged the intrinsic error resiliency of HD computing to develop different approximate encodings with varied accuracy and resource utilization attributes. With a modest 1.0% accuracy drop, our approximate encoding reduces the LUT utilization by 71.1%. By effectively utilizing the on-chip BRAMs of FPGA, we also proposed a highly efficient implementation that outperforms an optimized GPU implementation over 15\u00d7, and surpasses Raspberry Pi by over five orders of magnitude. Our FPGA implementation also consumes a moderate power: a minimum of 3.11 Watt for a face detection dataset using approximate encoding, and a maximum of 10.8 Watt on a digit recognition dataset when using exact encoding. Eventually, our implementation reduces the energy consumption by 247\u00d7 (45,988\u00d7) compared to GPU and Raspberry Pi in exact encoding, which further improves by a factor of 1.22\u00d7 using approximate encoding.\n\nFigure 1 :\n1Encoding and training in HD.\n\nFigure 2 :\n2(a) Adder-tree and (b) counter-based implementation of popcount. + \u25cb denotes add operation.\n\nFigure 3 :\n3Our proposed approximate encoding techniques. MAJ and + \u25cb denote majority and addition, respectively.\n\n\n(b), we limit the MAJ stages to the first two stages. Our cascaded-MAJ utilizes:\n\nFigure 4 :\n4SHEARer datapath abstract.\n\n\n(a)-(d). It can be seen that our equations in Section 3.1 have a modest average error of 3.8%. Especially, it over-estimates the LUT count of both exact and approximate adders, so the resource saving estimations remain similar to our predicted values. For instance, synthesis\n\n\nSahand Salamat, Anthony Thomas, Fatemeh Asgarinejad, Yeseong Kim, and Tajana Rosing correct label we simple return the predicted label as:ISLPED '20, \n, \nBehnam Khaleghi, \n\nTable 1 :\n1Baseline implementation results.Parameter \u2193 Benchmark \u2192 speech activity face \ndigit \nInput features (d iv ) \n617 \n561 \n608 \n784 \nHypervector length (d hv ) \n2,560 \n3,072 \n6,144 \n2,048 \nBaseline accuracy \n93.18% \n93.91% \n95.47% 89.07% \n\n\n\nTable 2 :\n2LUT count for a 512-input adder-tree. \nexact MAJ MAJ-2 over-feed truncate \nSynthesis 638 \n183 \n116 \n383 \n340 \nEquation \n675 \n195 \n116 \n405 \n343 \nError \n5.8% 6.6% 0.0% \n5.7% \n0.9% \n\nwhich is linear w.r.t. the adder size -for different average activities \nof the adders' primary inputs. Therefore, we calculate the average \nsignal activity observed by the adders according to the values of \ntemporary-generated binding hypervectors (level XOR ID). We simi-\nlarly estimate the toggle rate of BRAMs according to consecutive \nd mem bits read from BRAMs. As alluded earlier, we do not replicate \nthe hardware implementation in software; we just need to deter-\nmine each fetched level hypervector belongs to which BRAM group \n(based on the index of feature), so we can keep track of toggle rates. \nUsing the signal information with an offline look-up table created \nfor activity-power, along with the instantiated resource information \ncalculated as mentioned, during training, SHEARer estimates the \npower consumption of an application targeted for a specific device. \n\n\n\nTable 3 :\n3Relative accuracies SHEARer approximate encodings.Figure 5: Throughput of SHEARer versus Raspberry Pi 3 and Nvidia GTX 1080 Ti. Y-axis is logarithmic scale. results indicate MAJ (MAJ-2) saves 71.3% (81.8%) LUTs, which is very close to the predicted 71.1% (82.8%).exact \nMAJ \nMAJ-2 over-feed trunc-3 trunc-4 \nspeech \n93.2% \u22120.7% \u22122.3% \n\u22120.8% \n\u22120.9% \n\u22121.9% \nactivity \n93.9% \u22120.8% \u22121.2% \n\u22121.3% \n\u22121.1% \n\u22121.0% \nface \n95.5% \u22121.8% \u22123.3% \n\u22121.7% \n\u22121.6% \n\u22121.9% \ndigit \n89.1% \u22120.8% \u22120.3% \n\u22121.7% \n0.1% \n\u22120.1% \naverage \n\u22121.0% \u22121.8% \n\u22121.4% \n\u22120.9% \n\u22121.2% \nLUT saving \n0 \n71.1% 82.8% \n40.0% \n37.5% \n43.8% \n\n1E+0 \n\n1E+1 \n\n1E+2 \n\n1E+3 \n\n1E+4 \n\n1E+5 \n\n1E+6 \n\n1E+7 \n\nspeech \nactivity \nface \ndigit \n\nInput-per-second \n\nRaspberry Pi \nGPU \nSHEARer \n\n\nACKNOWLEDGEMENTSThis work was supported in part by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA, in part by SRC Global Research Collaboration (GRC) grant, DARPA HyDDENN grant, and NSF grants #1911095 and #2003279.\n. Behnam Islped &apos;20, Khaleghi, Sahand Salamat, Anthony Thomas, Fatemeh Asgarinejad, Yeseong Kim, and Tajana Rosing REFERENCESISLPED '20, , Behnam Khaleghi, Sahand Salamat, Anthony Thomas, Fatemeh Asgarinejad, Yeseong Kim, and Tajana Rosing REFERENCES\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cognitive computation, vol. 1, no. 2, pp. 139-159, 2009.\n\nOlfactory information processing in drosophila. N Y Masse, G C Turner, G S Jefferis, Current Biology. 1916N. Y. Masse, G. C. Turner, and G. S. Jefferis, \"Olfactory information processing in drosophila, \" Current Biology, vol. 19, no. 16, pp. R700-R713, 2009.\n\nOlfactory representations by drosophila mushroom body neurons. G C Turner, M Bazhenov, G Laurent, Journal of Neurophysiology. 992G. C. Turner, M. Bazhenov, and G. Laurent, \"Olfactory representations by drosophila mushroom body neurons, \" Journal of Neurophysiology, vol. 99, no. 2, pp. 734-746, 2008.\n\nEarly olfactory processing in drosophila: mechanisms and principles. R I Wilson, Annual Review of Neuroscience. 36R. I. Wilson, \"Early olfactory processing in drosophila: mechanisms and princi- ples, \" Annual Review of Neuroscience, vol. 36, pp. 217-241, 2013.\n\nSparse coding of sensory inputs. B A Olshausen, D J Field, Current Opinion in Neurobiology. 144B. A. Olshausen and D. J. Field, \"Sparse coding of sensory inputs, \" Current Opinion in Neurobiology, vol. 14, no. 4, pp. 481-487, 2004.\n\nHolographic reduced representations. T A Plate, IEEE Transactions on Neural networks. 63T. A. Plate, \"Holographic reduced representations, \" IEEE Transactions on Neural networks, vol. 6, no. 3, pp. 623-641, 1995.\n\nWeb user clustering and web prefetching using random indexing with weight functions. M Wan, A J\u00f6nsson, C Wang, L Li, Y Yang, Knowledge and information systems. 331M. Wan, A. J\u00f6nsson, C. Wang, L. Li, and Y. Yang, \"Web user clustering and web prefetching using random indexing with weight functions, \" Knowledge and information systems, vol. 33, no. 1, pp. 89-115, 2012.\n\nGeniehd: Efficient dna pattern matching accelerator using hyperdimensional computing. Y Kim, M Imani, N Moshiri, T Rosing, IEEEin 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE -To AppearY. Kim, M. Imani, N. Moshiri, and T. Rosing, \"Geniehd: Efficient dna pattern matching accelerator using hyperdimensional computing,\" in 2020 Design, Au- tomation & Test in Europe Conference & Exhibition (DATE -To Appear), IEEE, 2020.\n\nHdna: Energy-efficient dna sequencing using hyperdimensional computing. M Imani, T Nassar, A Rahimi, T Rosing, 2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI). IEEEM. Imani, T. Nassar, A. Rahimi, and T. Rosing, \"Hdna: Energy-efficient dna se- quencing using hyperdimensional computing, \" in 2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI), pp. 271-274, IEEE, 2018.\n\nEfficient biosignal processing using hyperdimensional computing: Network templates for combined learning and classification of exg signals. A Rahimi, P Kanerva, L Benini, J M Rabaey, Proceedings of the IEEE. 1071A. Rahimi, P. Kanerva, L. Benini, and J. M. Rabaey, \"Efficient biosignal processing using hyperdimensional computing: Network templates for combined learning and classification of exg signals, \" Proceedings of the IEEE, vol. 107, no. 1, pp. 123- 143, 2018.\n\nDetection of epileptic seizures from surface eeg using hyperdimensional computing. F Asgarinejad, A Thomas, T Rosing, 2020 42nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2020to appearF. Asgarinejad, A. Thomas, and T. Rosing, \"Detection of epileptic seizures from surface eeg using hyperdimensional computing,\" in 2020 42nd Annual Interna- tional Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (to appear), 2020.\n\nLearning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception. A Mitrokhin, P Sutor, C Ferm\u00fcller, Y Aloimonos, Science Robotics. 4306736A. Mitrokhin, P. Sutor, C. Ferm\u00fcller, and Y. Aloimonos, \"Learning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception, \" Science Robotics, vol. 4, no. 30, p. eaaw6736, 2019.\n\nAn introduction to hyperdimensional computing for robotics. P Neubert, S Schubert, P Protzel, 33KI-K\u00fcnstliche IntelligenzP. Neubert, S. Schubert, and P. Protzel, \"An introduction to hyperdimensional computing for robotics, \" KI-K\u00fcnstliche Intelligenz, vol. 33, no. 4, pp. 319-330, 2019.\n\nA framework for collaborative learning in secure high-dimensional space. M Imani, Y Kim, S Riazi, J Messerly, P Liu, F Koushanfar, T Rosing, 2019 IEEE 12th International Conference on Cloud Computing (CLOUD). IEEEM. Imani, Y. Kim, S. Riazi, J. Messerly, P. Liu, F. Koushanfar, and T. Rosing, \"A framework for collaborative learning in secure high-dimensional space, \" in 2019 IEEE 12th International Conference on Cloud Computing (CLOUD), pp. 435-446, IEEE, 2019.\n\nPrive-hd: Privacy-preserved hyperdimensional computing. B Khaleghi, M Imani, T Rosing, Proceedings of the 57th Annual Design Automation Conference. the 57th Annual Design Automation Conference2020to appearB. Khaleghi, M. Imani, and T. Rosing, \"Prive-hd: Privacy-preserved hyperdi- mensional computing,\" in Proceedings of the 57th Annual Design Automation Conference (to appear), 2020.\n\nBric: Localitybased encoding for energy-efficient brain-inspired hyperdimensional computing. M Imani, J Morris, J Messerly, H Shu, Y Deng, T Rosing, Proceedings of the 56th Annual Design Automation Conference. the 56th Annual Design Automation ConferenceACM52M. Imani, J. Morris, J. Messerly, H. Shu, Y. Deng, and T. Rosing, \"Bric: Locality- based encoding for energy-efficient brain-inspired hyperdimensional computing, \" in Proceedings of the 56th Annual Design Automation Conference 2019, p. 52, ACM, 2019.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design. the 2016 International Symposium on Low Power Electronics and DesignA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy-efficient classifier using brain-inspired hyperdimensional computing,\" in Proceedings of the 2016 International Symposium on Low Power Electronics and Design, pp. 64-69, 2016.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, 2017 IEEE International Conference on Rebooting Computing (ICRC). IEEEM. Imani, D. Kong, A. Rahimi, and T. Rosing, \"Voicehd: Hyperdimensional com- puting for efficient speech recognition, \" in 2017 IEEE International Conference on Rebooting Computing (ICRC), pp. 1-8, IEEE, 2017.\n\nSparsehd: Algorithm-hardware co-optimization for efficient high-dimensional computing. M Imani, S Salamat, B Khaleghi, M Samragh, F Koushanfar, T Rosing, 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). IEEEM. Imani, S. Salamat, B. Khaleghi, M. Samragh, F. Koushanfar, and T. Rosing, \"Sparsehd: Algorithm-hardware co-optimization for efficient high-dimensional computing,\" in 2019 IEEE 27th Annual International Symposium on Field- Programmable Custom Computing Machines (FCCM), pp. 190-198, IEEE, 2019.\n\nF5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing. S Salamat, M Imani, B Khaleghi, T Rosing, Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. the ACM/SIGDA International Symposium on Field-Programmable Gate ArraysS. Salamat, M. Imani, B. Khaleghi, and T. Rosing, \"F5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing,\" in Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pp. 53- 62, 2019.\n\nQuanthd: A quantization framework for hyperdimensional computing. M Imani, S Bosch, S Datta, S Ramakrishna, S Salamat, J M Rabaey, T Rosing, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. M. Imani, S. Bosch, S. Datta, S. Ramakrishna, S. Salamat, J. M. Rabaey, and T. Rosing, \"Quanthd: A quantization framework for hyperdimensional computing, \" IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2019.\n\nHardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory. M Schmuck, L Benini, A Rahimi, ACM Journal on Emerging Technologies in Computing Systems (JETC). 154M. Schmuck, L. Benini, and A. Rahimi, \"Hardware optimizations of dense bi- nary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory,\" ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 15, no. 4, pp. 1-25, 2019.\n\nA binary learning framework for hyperdimensional computing. M Imani, J Messerly, F Wu, W Pi, T Rosing, 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEEM. Imani, J. Messerly, F. Wu, W. Pi, and T. Rosing, \"A binary learning framework for hyperdimensional computing, \" in 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 126-131, IEEE, 2019.\n\nA programmable hyperdimensional processor architecture for human-centric iot. S Datta, R A Antonio, A R Ison, J M Rabaey, IEEE Journal on Emerging and Selected Topics in Circuits and Systems. 93S. Datta, R. A. Antonio, A. R. Ison, and J. M. Rabaey, \"A programmable hyper- dimensional processor architecture for human-centric iot, \" IEEE Journal on Emerg- ing and Selected Topics in Circuits and Systems, vol. 9, no. 3, pp. 439-452, 2019.\n\nUci machine learning repository. \"Uci machine learning repository. \" http://archive.ics.uci.edu/ml/datasets/ISOLET.\n\n7 series fpgas data sheet. Data Sheet. \"7 series fpgas data sheet. \" Data Sheet, February 2108.\n\nExploring hyperdimensional associative memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEEM. Imani, A. Rahimi, D. Kong, T. Rosing, and J. M. Rabaey, \"Exploring hyperdi- mensional associative memory,\" in 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 445-456, IEEE, 2017.\n\nFach: Fpga-based acceleration of hyperdimensional computing by reducing computational complexity. M Imani, S Salamat, S Gupta, J Huang, T Rosing, Proceedings of the 24th Asia and South Pacific Design Automation Conference. the 24th Asia and South Pacific Design Automation ConferenceM. Imani, S. Salamat, S. Gupta, J. Huang, and T. Rosing, \"Fach: Fpga-based accel- eration of hyperdimensional computing by reducing computational complexity, \" in Proceedings of the 24th Asia and South Pacific Design Automation Conference, pp. 493-498, 2019.\n\nUci machine learning repository. \"Uci machine learning repository.\" https://archive.ics.uci.edu/ml/datasets/ human+activity+recognition+using+smartphones.\n\nThe mnist database of handwritten digits. Y Lecun, C Cortes, C J Burges, 1034Y. LeCun, C. Cortes, and C. J. Burges, \"The mnist database of handwritten digits, 1998, \" URL http://yann. lecun. com/exdb/mnist, vol. 10, p. 34, 1998.\n\nCaltech-256 object category dataset. G Griffin, A Holub, P Perona, G. Griffin, A. Holub, and P. Perona, \"Caltech-256 object category dataset, \" 2007.\n", "annotations": {"author": "[{\"end\":222,\"start\":110},{\"end\":333,\"start\":223},{\"end\":444,\"start\":334},{\"end\":564,\"start\":445},{\"end\":654,\"start\":565},{\"end\":762,\"start\":655},{\"end\":856,\"start\":763},{\"end\":949,\"start\":857},{\"end\":1042,\"start\":950},{\"end\":1137,\"start\":1043}]", "publisher": null, "author_last_name": "[{\"end\":125,\"start\":117},{\"end\":237,\"start\":230},{\"end\":348,\"start\":342},{\"end\":464,\"start\":453},{\"end\":576,\"start\":573},{\"end\":668,\"start\":662},{\"end\":778,\"start\":770},{\"end\":871,\"start\":864},{\"end\":964,\"start\":958},{\"end\":1059,\"start\":1051}]", "author_first_name": "[{\"end\":116,\"start\":110},{\"end\":229,\"start\":223},{\"end\":341,\"start\":334},{\"end\":452,\"start\":445},{\"end\":572,\"start\":565},{\"end\":661,\"start\":655},{\"end\":769,\"start\":763},{\"end\":863,\"start\":857},{\"end\":957,\"start\":950},{\"end\":1050,\"start\":1043}]", "author_affiliation": "[{\"end\":221,\"start\":146},{\"end\":332,\"start\":257},{\"end\":443,\"start\":368},{\"end\":563,\"start\":488},{\"end\":653,\"start\":578},{\"end\":761,\"start\":686},{\"end\":855,\"start\":780},{\"end\":948,\"start\":873},{\"end\":1041,\"start\":966},{\"end\":1136,\"start\":1061}]", "title": "[{\"end\":107,\"start\":1},{\"end\":1244,\"start\":1138}]", "venue": null, "abstract": "[{\"end\":3244,\"start\":1546}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4295,\"start\":4292},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4298,\"start\":4295},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4301,\"start\":4298},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4304,\"start\":4301},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4307,\"start\":4304},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6333,\"start\":6330},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6335,\"start\":6333},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7120,\"start\":7117},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7151,\"start\":7148},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7153,\"start\":7151},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7181,\"start\":7177},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7184,\"start\":7181},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7199,\"start\":7195},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7202,\"start\":7199},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7250,\"start\":7246},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7253,\"start\":7250},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7933,\"start\":7929},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9408,\"start\":9405},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9410,\"start\":9408},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9413,\"start\":9410},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9522,\"start\":9518},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9526,\"start\":9522},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9530,\"start\":9526},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9534,\"start\":9530},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11431,\"start\":11427},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11435,\"start\":11431},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11439,\"start\":11435},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11469,\"start\":11465},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11472,\"start\":11469},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12119,\"start\":12115},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12144,\"start\":12140},{\"end\":12846,\"start\":12843},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12967,\"start\":12964},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13322,\"start\":13318},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13428,\"start\":13424},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13526,\"start\":13522},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13535,\"start\":13531},{\"end\":14036,\"start\":14033},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14089,\"start\":14085},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14098,\"start\":14094},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15946,\"start\":15942},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17697,\"start\":17696},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17915,\"start\":17911},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19956,\"start\":19955},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22106,\"start\":22105},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22896,\"start\":22892},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22899,\"start\":22896},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22902,\"start\":22899},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23763,\"start\":23762},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28274,\"start\":28270},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31288,\"start\":31285},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32529,\"start\":32525},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32544,\"start\":32540},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32572,\"start\":32568},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":32625,\"start\":32621}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38625,\"start\":38584},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38730,\"start\":38626},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38845,\"start\":38731},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38928,\"start\":38846},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38968,\"start\":38929},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39246,\"start\":38969},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39420,\"start\":39247},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39669,\"start\":39421},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":40746,\"start\":39670},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":41486,\"start\":40747}]", "paragraph": "[{\"end\":4069,\"start\":3260},{\"end\":4984,\"start\":4071},{\"end\":6056,\"start\":4986},{\"end\":6336,\"start\":6058},{\"end\":6566,\"start\":6370},{\"end\":8182,\"start\":6609},{\"end\":9184,\"start\":8184},{\"end\":9718,\"start\":9241},{\"end\":10764,\"start\":9736},{\"end\":10937,\"start\":10766},{\"end\":11143,\"start\":10968},{\"end\":12968,\"start\":11158},{\"end\":13501,\"start\":13035},{\"end\":14075,\"start\":13503},{\"end\":15287,\"start\":14077},{\"end\":15475,\"start\":15341},{\"end\":16158,\"start\":15477},{\"end\":17560,\"start\":16160},{\"end\":17767,\"start\":17562},{\"end\":17906,\"start\":17830},{\"end\":18759,\"start\":17908},{\"end\":19782,\"start\":18761},{\"end\":20211,\"start\":19833},{\"end\":21534,\"start\":20213},{\"end\":22253,\"start\":21536},{\"end\":24078,\"start\":22357},{\"end\":27384,\"start\":24080},{\"end\":29268,\"start\":27386},{\"end\":30719,\"start\":29287},{\"end\":31407,\"start\":30721},{\"end\":31841,\"start\":31455},{\"end\":33014,\"start\":31866},{\"end\":34633,\"start\":33016},{\"end\":35696,\"start\":34635},{\"end\":36069,\"start\":35698},{\"end\":37648,\"start\":36071},{\"end\":38583,\"start\":37663}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6369,\"start\":6337},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6608,\"start\":6567},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9735,\"start\":9719},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10967,\"start\":10938},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13034,\"start\":12969},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17829,\"start\":17768},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19832,\"start\":19783},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22333,\"start\":22254},{\"attributes\":{\"id\":\"formula_8\"},\"end\":31454,\"start\":31408}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32652,\"start\":32645},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33245,\"start\":33238},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33424,\"start\":33417}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3258,\"start\":3246},{\"attributes\":{\"n\":\"2\"},\"end\":9239,\"start\":9187},{\"attributes\":{\"n\":\"2.2\"},\"end\":11156,\"start\":11146},{\"attributes\":{\"n\":\"3\"},\"end\":15339,\"start\":15290},{\"attributes\":{\"n\":\"3.2\"},\"end\":22355,\"start\":22335},{\"attributes\":{\"n\":\"3.3\"},\"end\":29285,\"start\":29271},{\"attributes\":{\"n\":\"4\"},\"end\":31864,\"start\":31844},{\"attributes\":{\"n\":\"5\"},\"end\":37661,\"start\":37651},{\"end\":38595,\"start\":38585},{\"end\":38637,\"start\":38627},{\"end\":38742,\"start\":38732},{\"end\":38940,\"start\":38930},{\"end\":39431,\"start\":39422},{\"end\":39680,\"start\":39671},{\"end\":40757,\"start\":40748}]", "table": "[{\"end\":39420,\"start\":39387},{\"end\":39669,\"start\":39465},{\"end\":40746,\"start\":39682},{\"end\":41486,\"start\":41022}]", "figure_caption": "[{\"end\":38625,\"start\":38597},{\"end\":38730,\"start\":38639},{\"end\":38845,\"start\":38744},{\"end\":38928,\"start\":38848},{\"end\":38968,\"start\":38942},{\"end\":39246,\"start\":38971},{\"end\":39387,\"start\":39249},{\"end\":39465,\"start\":39433},{\"end\":41022,\"start\":40759}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6055,\"start\":6047},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12370,\"start\":12362},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13637,\"start\":13629},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16062,\"start\":16054},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16709,\"start\":16701},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16724,\"start\":16716},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16748,\"start\":16739},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17618,\"start\":17610},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18448,\"start\":18440},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18795,\"start\":18787},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19775,\"start\":19765},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20728,\"start\":20720},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21906,\"start\":21898},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22377,\"start\":22369},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24218,\"start\":24210},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25021,\"start\":25013},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25115,\"start\":25107},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30072,\"start\":30060},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33401,\"start\":33393},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33665,\"start\":33653},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33803,\"start\":33795},{\"end\":34660,\"start\":34652},{\"end\":35585,\"start\":35577},{\"end\":36103,\"start\":36095}]", "bib_author_first_name": "[{\"end\":41733,\"start\":41727},{\"end\":42108,\"start\":42107},{\"end\":42389,\"start\":42388},{\"end\":42391,\"start\":42390},{\"end\":42400,\"start\":42399},{\"end\":42402,\"start\":42401},{\"end\":42412,\"start\":42411},{\"end\":42414,\"start\":42413},{\"end\":42664,\"start\":42663},{\"end\":42666,\"start\":42665},{\"end\":42676,\"start\":42675},{\"end\":42688,\"start\":42687},{\"end\":42972,\"start\":42971},{\"end\":42974,\"start\":42973},{\"end\":43198,\"start\":43197},{\"end\":43200,\"start\":43199},{\"end\":43213,\"start\":43212},{\"end\":43215,\"start\":43214},{\"end\":43435,\"start\":43434},{\"end\":43437,\"start\":43436},{\"end\":43697,\"start\":43696},{\"end\":43704,\"start\":43703},{\"end\":43715,\"start\":43714},{\"end\":43723,\"start\":43722},{\"end\":43729,\"start\":43728},{\"end\":44068,\"start\":44067},{\"end\":44075,\"start\":44074},{\"end\":44084,\"start\":44083},{\"end\":44095,\"start\":44094},{\"end\":44500,\"start\":44499},{\"end\":44509,\"start\":44508},{\"end\":44519,\"start\":44518},{\"end\":44529,\"start\":44528},{\"end\":45000,\"start\":44999},{\"end\":45010,\"start\":45009},{\"end\":45021,\"start\":45020},{\"end\":45031,\"start\":45030},{\"end\":45033,\"start\":45032},{\"end\":45413,\"start\":45412},{\"end\":45428,\"start\":45427},{\"end\":45438,\"start\":45437},{\"end\":45924,\"start\":45923},{\"end\":45937,\"start\":45936},{\"end\":45946,\"start\":45945},{\"end\":45959,\"start\":45958},{\"end\":46270,\"start\":46269},{\"end\":46281,\"start\":46280},{\"end\":46293,\"start\":46292},{\"end\":46571,\"start\":46570},{\"end\":46580,\"start\":46579},{\"end\":46587,\"start\":46586},{\"end\":46596,\"start\":46595},{\"end\":46608,\"start\":46607},{\"end\":46615,\"start\":46614},{\"end\":46629,\"start\":46628},{\"end\":47019,\"start\":47018},{\"end\":47031,\"start\":47030},{\"end\":47040,\"start\":47039},{\"end\":47442,\"start\":47441},{\"end\":47451,\"start\":47450},{\"end\":47461,\"start\":47460},{\"end\":47473,\"start\":47472},{\"end\":47480,\"start\":47479},{\"end\":47488,\"start\":47487},{\"end\":47950,\"start\":47949},{\"end\":47960,\"start\":47959},{\"end\":47971,\"start\":47970},{\"end\":47973,\"start\":47972},{\"end\":48445,\"start\":48444},{\"end\":48454,\"start\":48453},{\"end\":48462,\"start\":48461},{\"end\":48472,\"start\":48471},{\"end\":48850,\"start\":48849},{\"end\":48859,\"start\":48858},{\"end\":48870,\"start\":48869},{\"end\":48882,\"start\":48881},{\"end\":48893,\"start\":48892},{\"end\":48907,\"start\":48906},{\"end\":49406,\"start\":49405},{\"end\":49417,\"start\":49416},{\"end\":49426,\"start\":49425},{\"end\":49438,\"start\":49437},{\"end\":49920,\"start\":49919},{\"end\":49929,\"start\":49928},{\"end\":49938,\"start\":49937},{\"end\":49947,\"start\":49946},{\"end\":49962,\"start\":49961},{\"end\":49973,\"start\":49972},{\"end\":49975,\"start\":49974},{\"end\":49985,\"start\":49984},{\"end\":50476,\"start\":50475},{\"end\":50487,\"start\":50486},{\"end\":50497,\"start\":50496},{\"end\":50937,\"start\":50936},{\"end\":50946,\"start\":50945},{\"end\":50958,\"start\":50957},{\"end\":50964,\"start\":50963},{\"end\":50970,\"start\":50969},{\"end\":51352,\"start\":51351},{\"end\":51361,\"start\":51360},{\"end\":51363,\"start\":51362},{\"end\":51374,\"start\":51373},{\"end\":51376,\"start\":51375},{\"end\":51384,\"start\":51383},{\"end\":51386,\"start\":51385},{\"end\":51974,\"start\":51973},{\"end\":51983,\"start\":51982},{\"end\":51993,\"start\":51992},{\"end\":52001,\"start\":52000},{\"end\":52011,\"start\":52010},{\"end\":52013,\"start\":52012},{\"end\":52432,\"start\":52431},{\"end\":52441,\"start\":52440},{\"end\":52452,\"start\":52451},{\"end\":52461,\"start\":52460},{\"end\":52470,\"start\":52469},{\"end\":53075,\"start\":53074},{\"end\":53084,\"start\":53083},{\"end\":53094,\"start\":53093},{\"end\":53096,\"start\":53095},{\"end\":53300,\"start\":53299},{\"end\":53311,\"start\":53310},{\"end\":53320,\"start\":53319}]", "bib_author_last_name": "[{\"end\":41749,\"start\":41734},{\"end\":41759,\"start\":41751},{\"end\":42116,\"start\":42109},{\"end\":42397,\"start\":42392},{\"end\":42409,\"start\":42403},{\"end\":42423,\"start\":42415},{\"end\":42673,\"start\":42667},{\"end\":42685,\"start\":42677},{\"end\":42696,\"start\":42689},{\"end\":42981,\"start\":42975},{\"end\":43210,\"start\":43201},{\"end\":43221,\"start\":43216},{\"end\":43443,\"start\":43438},{\"end\":43701,\"start\":43698},{\"end\":43712,\"start\":43705},{\"end\":43720,\"start\":43716},{\"end\":43726,\"start\":43724},{\"end\":43734,\"start\":43730},{\"end\":44072,\"start\":44069},{\"end\":44081,\"start\":44076},{\"end\":44092,\"start\":44085},{\"end\":44102,\"start\":44096},{\"end\":44506,\"start\":44501},{\"end\":44516,\"start\":44510},{\"end\":44526,\"start\":44520},{\"end\":44536,\"start\":44530},{\"end\":45007,\"start\":45001},{\"end\":45018,\"start\":45011},{\"end\":45028,\"start\":45022},{\"end\":45040,\"start\":45034},{\"end\":45425,\"start\":45414},{\"end\":45435,\"start\":45429},{\"end\":45445,\"start\":45439},{\"end\":45934,\"start\":45925},{\"end\":45943,\"start\":45938},{\"end\":45956,\"start\":45947},{\"end\":45969,\"start\":45960},{\"end\":46278,\"start\":46271},{\"end\":46290,\"start\":46282},{\"end\":46301,\"start\":46294},{\"end\":46577,\"start\":46572},{\"end\":46584,\"start\":46581},{\"end\":46593,\"start\":46588},{\"end\":46605,\"start\":46597},{\"end\":46612,\"start\":46609},{\"end\":46626,\"start\":46616},{\"end\":46636,\"start\":46630},{\"end\":47028,\"start\":47020},{\"end\":47037,\"start\":47032},{\"end\":47047,\"start\":47041},{\"end\":47448,\"start\":47443},{\"end\":47458,\"start\":47452},{\"end\":47470,\"start\":47462},{\"end\":47477,\"start\":47474},{\"end\":47485,\"start\":47481},{\"end\":47495,\"start\":47489},{\"end\":47957,\"start\":47951},{\"end\":47968,\"start\":47961},{\"end\":47980,\"start\":47974},{\"end\":48451,\"start\":48446},{\"end\":48459,\"start\":48455},{\"end\":48469,\"start\":48463},{\"end\":48479,\"start\":48473},{\"end\":48856,\"start\":48851},{\"end\":48867,\"start\":48860},{\"end\":48879,\"start\":48871},{\"end\":48890,\"start\":48883},{\"end\":48904,\"start\":48894},{\"end\":48914,\"start\":48908},{\"end\":49414,\"start\":49407},{\"end\":49423,\"start\":49418},{\"end\":49435,\"start\":49427},{\"end\":49445,\"start\":49439},{\"end\":49926,\"start\":49921},{\"end\":49935,\"start\":49930},{\"end\":49944,\"start\":49939},{\"end\":49959,\"start\":49948},{\"end\":49970,\"start\":49963},{\"end\":49982,\"start\":49976},{\"end\":49992,\"start\":49986},{\"end\":50484,\"start\":50477},{\"end\":50494,\"start\":50488},{\"end\":50504,\"start\":50498},{\"end\":50943,\"start\":50938},{\"end\":50955,\"start\":50947},{\"end\":50961,\"start\":50959},{\"end\":50967,\"start\":50965},{\"end\":50977,\"start\":50971},{\"end\":51358,\"start\":51353},{\"end\":51371,\"start\":51364},{\"end\":51381,\"start\":51377},{\"end\":51393,\"start\":51387},{\"end\":51980,\"start\":51975},{\"end\":51990,\"start\":51984},{\"end\":51998,\"start\":51994},{\"end\":52008,\"start\":52002},{\"end\":52020,\"start\":52014},{\"end\":52438,\"start\":52433},{\"end\":52449,\"start\":52442},{\"end\":52458,\"start\":52453},{\"end\":52467,\"start\":52462},{\"end\":52477,\"start\":52471},{\"end\":53081,\"start\":53076},{\"end\":53091,\"start\":53085},{\"end\":53103,\"start\":53097},{\"end\":53308,\"start\":53301},{\"end\":53317,\"start\":53312},{\"end\":53327,\"start\":53321}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":41980,\"start\":41725},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":733980},\"end\":42338,\"start\":41982},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":10124667},\"end\":42598,\"start\":42340},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1551928},\"end\":42900,\"start\":42600},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13956119},\"end\":43162,\"start\":42902},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16560320},\"end\":43395,\"start\":43164},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2352281},\"end\":43609,\"start\":43397},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9184559},\"end\":43979,\"start\":43611},{\"attributes\":{\"id\":\"b8\"},\"end\":44425,\"start\":43981},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4708051},\"end\":44857,\"start\":44427},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":57365377},\"end\":45327,\"start\":44859},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":221388379},\"end\":45821,\"start\":45329},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":182118830},\"end\":46207,\"start\":45823},{\"attributes\":{\"id\":\"b13\"},\"end\":46495,\"start\":46209},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":197642766},\"end\":46960,\"start\":46497},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":218628710},\"end\":47346,\"start\":46962},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":163164623},\"end\":47857,\"start\":47348},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9812826},\"end\":48372,\"start\":47859},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":21351739},\"end\":48760,\"start\":48374},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":189824904},\"end\":49318,\"start\":48762},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":67872077},\"end\":49851,\"start\":49320},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":211016154},\"end\":50313,\"start\":49853},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":49907924},\"end\":50874,\"start\":50315},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":155109576},\"end\":51271,\"start\":50876},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":201900134},\"end\":51710,\"start\":51273},{\"attributes\":{\"id\":\"b25\"},\"end\":51827,\"start\":51712},{\"attributes\":{\"id\":\"b26\"},\"end\":51924,\"start\":51829},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1677864},\"end\":52331,\"start\":51926},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":58027670},\"end\":52874,\"start\":52333},{\"attributes\":{\"id\":\"b29\"},\"end\":53030,\"start\":52876},{\"attributes\":{\"id\":\"b30\"},\"end\":53260,\"start\":53032},{\"attributes\":{\"id\":\"b31\"},\"end\":53411,\"start\":53262}]", "bib_title": "[{\"end\":42105,\"start\":41982},{\"end\":42386,\"start\":42340},{\"end\":42661,\"start\":42600},{\"end\":42969,\"start\":42902},{\"end\":43195,\"start\":43164},{\"end\":43432,\"start\":43397},{\"end\":43694,\"start\":43611},{\"end\":44497,\"start\":44427},{\"end\":44997,\"start\":44859},{\"end\":45410,\"start\":45329},{\"end\":45921,\"start\":45823},{\"end\":46568,\"start\":46497},{\"end\":47016,\"start\":46962},{\"end\":47439,\"start\":47348},{\"end\":47947,\"start\":47859},{\"end\":48442,\"start\":48374},{\"end\":48847,\"start\":48762},{\"end\":49403,\"start\":49320},{\"end\":49917,\"start\":49853},{\"end\":50473,\"start\":50315},{\"end\":50934,\"start\":50876},{\"end\":51349,\"start\":51273},{\"end\":51854,\"start\":51829},{\"end\":51971,\"start\":51926},{\"end\":52429,\"start\":52333}]", "bib_author": "[{\"end\":41751,\"start\":41727},{\"end\":41761,\"start\":41751},{\"end\":42118,\"start\":42107},{\"end\":42399,\"start\":42388},{\"end\":42411,\"start\":42399},{\"end\":42425,\"start\":42411},{\"end\":42675,\"start\":42663},{\"end\":42687,\"start\":42675},{\"end\":42698,\"start\":42687},{\"end\":42983,\"start\":42971},{\"end\":43212,\"start\":43197},{\"end\":43223,\"start\":43212},{\"end\":43445,\"start\":43434},{\"end\":43703,\"start\":43696},{\"end\":43714,\"start\":43703},{\"end\":43722,\"start\":43714},{\"end\":43728,\"start\":43722},{\"end\":43736,\"start\":43728},{\"end\":44074,\"start\":44067},{\"end\":44083,\"start\":44074},{\"end\":44094,\"start\":44083},{\"end\":44104,\"start\":44094},{\"end\":44508,\"start\":44499},{\"end\":44518,\"start\":44508},{\"end\":44528,\"start\":44518},{\"end\":44538,\"start\":44528},{\"end\":45009,\"start\":44999},{\"end\":45020,\"start\":45009},{\"end\":45030,\"start\":45020},{\"end\":45042,\"start\":45030},{\"end\":45427,\"start\":45412},{\"end\":45437,\"start\":45427},{\"end\":45447,\"start\":45437},{\"end\":45936,\"start\":45923},{\"end\":45945,\"start\":45936},{\"end\":45958,\"start\":45945},{\"end\":45971,\"start\":45958},{\"end\":46280,\"start\":46269},{\"end\":46292,\"start\":46280},{\"end\":46303,\"start\":46292},{\"end\":46579,\"start\":46570},{\"end\":46586,\"start\":46579},{\"end\":46595,\"start\":46586},{\"end\":46607,\"start\":46595},{\"end\":46614,\"start\":46607},{\"end\":46628,\"start\":46614},{\"end\":46638,\"start\":46628},{\"end\":47030,\"start\":47018},{\"end\":47039,\"start\":47030},{\"end\":47049,\"start\":47039},{\"end\":47450,\"start\":47441},{\"end\":47460,\"start\":47450},{\"end\":47472,\"start\":47460},{\"end\":47479,\"start\":47472},{\"end\":47487,\"start\":47479},{\"end\":47497,\"start\":47487},{\"end\":47959,\"start\":47949},{\"end\":47970,\"start\":47959},{\"end\":47982,\"start\":47970},{\"end\":48453,\"start\":48444},{\"end\":48461,\"start\":48453},{\"end\":48471,\"start\":48461},{\"end\":48481,\"start\":48471},{\"end\":48858,\"start\":48849},{\"end\":48869,\"start\":48858},{\"end\":48881,\"start\":48869},{\"end\":48892,\"start\":48881},{\"end\":48906,\"start\":48892},{\"end\":48916,\"start\":48906},{\"end\":49416,\"start\":49405},{\"end\":49425,\"start\":49416},{\"end\":49437,\"start\":49425},{\"end\":49447,\"start\":49437},{\"end\":49928,\"start\":49919},{\"end\":49937,\"start\":49928},{\"end\":49946,\"start\":49937},{\"end\":49961,\"start\":49946},{\"end\":49972,\"start\":49961},{\"end\":49984,\"start\":49972},{\"end\":49994,\"start\":49984},{\"end\":50486,\"start\":50475},{\"end\":50496,\"start\":50486},{\"end\":50506,\"start\":50496},{\"end\":50945,\"start\":50936},{\"end\":50957,\"start\":50945},{\"end\":50963,\"start\":50957},{\"end\":50969,\"start\":50963},{\"end\":50979,\"start\":50969},{\"end\":51360,\"start\":51351},{\"end\":51373,\"start\":51360},{\"end\":51383,\"start\":51373},{\"end\":51395,\"start\":51383},{\"end\":51982,\"start\":51973},{\"end\":51992,\"start\":51982},{\"end\":52000,\"start\":51992},{\"end\":52010,\"start\":52000},{\"end\":52022,\"start\":52010},{\"end\":52440,\"start\":52431},{\"end\":52451,\"start\":52440},{\"end\":52460,\"start\":52451},{\"end\":52469,\"start\":52460},{\"end\":52479,\"start\":52469},{\"end\":53083,\"start\":53074},{\"end\":53093,\"start\":53083},{\"end\":53105,\"start\":53093},{\"end\":53310,\"start\":53299},{\"end\":53319,\"start\":53310},{\"end\":53329,\"start\":53319}]", "bib_venue": "[{\"end\":42139,\"start\":42118},{\"end\":42440,\"start\":42425},{\"end\":42724,\"start\":42698},{\"end\":43012,\"start\":42983},{\"end\":43254,\"start\":43223},{\"end\":43481,\"start\":43445},{\"end\":43769,\"start\":43736},{\"end\":44065,\"start\":43981},{\"end\":44618,\"start\":44538},{\"end\":45065,\"start\":45042},{\"end\":45551,\"start\":45447},{\"end\":45987,\"start\":45971},{\"end\":46267,\"start\":46209},{\"end\":46704,\"start\":46638},{\"end\":47108,\"start\":47049},{\"end\":47556,\"start\":47497},{\"end\":48065,\"start\":47982},{\"end\":48545,\"start\":48481},{\"end\":49016,\"start\":48916},{\"end\":49533,\"start\":49447},{\"end\":50071,\"start\":49994},{\"end\":50570,\"start\":50506},{\"end\":51050,\"start\":50979},{\"end\":51463,\"start\":51395},{\"end\":51743,\"start\":51712},{\"end\":51866,\"start\":51856},{\"end\":52104,\"start\":52022},{\"end\":52554,\"start\":52479},{\"end\":52907,\"start\":52876},{\"end\":53072,\"start\":53032},{\"end\":53297,\"start\":53262},{\"end\":47154,\"start\":47110},{\"end\":47602,\"start\":47558},{\"end\":48135,\"start\":48067},{\"end\":49606,\"start\":49535},{\"end\":52616,\"start\":52556}]"}}}, "year": 2023, "month": 12, "day": 17}