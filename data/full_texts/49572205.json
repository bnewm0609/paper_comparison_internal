{"id": 49572205, "updated": "2023-09-28 13:52:00.404", "metadata": {"title": "SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator", "authors": "[{\"first\":\"Cong\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Chris\",\"last\":\"Li\",\"middle\":[\"Junchi\"]},{\"first\":\"Zhouchen\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Tong\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "In this paper, we propose a new technique named \\textit{Stochastic Path-Integrated Differential EstimatoR} (SPIDER), which can be used to track many deterministic quantities of interest with significantly reduced computational cost. We apply SPIDER to two tasks, namely the stochastic first-order and zeroth-order methods. For stochastic first-order method, combining SPIDER with normalized gradient descent, we propose two new algorithms, namely SPIDER-SFO and SPIDER-SFO\\textsuperscript{+}, that solve non-convex stochastic optimization problems using stochastic gradients only. We provide sharp error-bound results on their convergence rates. In special, we prove that the SPIDER-SFO and SPIDER-SFO\\textsuperscript{+} algorithms achieve a record-breaking gradient computation cost of $\\mathcal{O}\\left( \\min( n^{1/2} \\epsilon^{-2}, \\epsilon^{-3} ) \\right)$ for finding an $\\epsilon$-approximate first-order and $\\tilde{\\mathcal{O}}\\left( \\min( n^{1/2} \\epsilon^{-2}+\\epsilon^{-2.5}, \\epsilon^{-3} ) \\right)$ for finding an $(\\epsilon, \\mathcal{O}(\\epsilon^{0.5}))$-approximate second-order stationary point, respectively. In addition, we prove that SPIDER-SFO nearly matches the algorithmic lower bound for finding approximate first-order stationary points under the gradient Lipschitz assumption in the finite-sum setting. For stochastic zeroth-order method, we prove a cost of $\\mathcal{O}( d \\min( n^{1/2} \\epsilon^{-2}, \\epsilon^{-3}) )$ which outperforms all existing results.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1807.01695", "mag": "2963411541", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/FangLLZ18", "doi": null}}, "content": {"source": {"pdf_hash": "19ddcb90a0e44d1f3f9b6f034124cb4a0bb56e5f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1807.01695v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "193102b08881d6d1fb25c624d89c6d770219e954", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/19ddcb90a0e44d1f3f9b6f034124cb4a0bb56e5f.txt", "contents": "\nSpider: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator\nJuly 4, 2018 (Initial) October 18, 2018\n\nCong Fang \nCurrent\n\n\n\u2020 Chris \nCurrent\n\n\nJunchi Li \nCurrent\n\n\nZhouchen Lin \nCurrent\n\n\nTong Zhang \nCurrent\n\n\nSpider: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator\nJuly 4, 2018 (Initial) October 18, 2018\nIn this paper, we propose a new technique named Stochastic Path-Integrated Differential EstimatoR (Spider), which can be used to track many deterministic quantities of interest with significantly reduced computational cost. We apply Spider to two tasks, namely the stochastic first-order and zeroth-order methods. For stochastic first-order method, combining Spider with normalized gradient descent, we propose two new algorithms, namely Spider-SFO and Spider-SFO + , that solve non-convex stochastic optimization problems using stochastic gradients only. We provide sharp error-bound results on their convergence rates. In special, we prove that the Spider-SFO and Spider-SFO + algorithms achieve a record-breaking gradient computation cost of O min(n 1/2 \u22122 , \u22123 ) for finding an -approximate first-order and O min(n 1/2 \u22122 + \u22122.5 , \u22123 ) for finding an ( , O( 0.5 ))-approximate second-order stationary point, respectively. In addition, we prove that Spider-SFO nearly matches the algorithmic lower bound for finding approximate first-order stationary points under the gradient Lipschitz assumption in the finite-sum setting. For stochastic zeroth-order method, we prove a cost of O(d min(n 1/2 \u22122 , \u22123 )) which outperforms all existing results.\n\nIntroduction\n\nIn this paper, we study the optimization problem\nminimize x\u2208R d f (x) \u2261 E [F (x; \u03b6)]\n(1.1)\n\nwhere the stochastic component F (x; \u03b6), indexed by some random vector \u03b6, is smooth and possibly non-convex. Non-convex optimization problem of form (1.1) contains many large-scale statistical learning tasks. Optimization methods that solve (1.1) are gaining tremendous popularity due to their favorable computational and statistical efficiencies (Bottou, 2010;Bubeck et al., 2015;Bottou et al., 2018). Typical examples of form (1.1) include principal component analysis, estimation of graphical models, as well as training deep neural networks (Goodfellow et al., 2016). The expectation-minimization structure of stochastic optimization problem (1.1) allows us to perform iterative updates and minimize the objective using its stochastic gradient \u2207F (x; \u03b6) as an estimator of its deterministic counterpart. A special case of central interest is when the stochastic vector \u03b6 is finitely sampled. In such finite-sum (or offline) case, we denote each component function as f i (x) and (1.1) can be restated as\nminimize x\u2208R d f (x) = 1 n n i=1 f i (x) (1.2)\nwhere n is the number of individual functions. Another case is when n is reasonably large or even infinite, running across of the whole dataset is exhaustive or impossible. We refer it as the online (or streaming) case. For simplicity of notations we will study the optimization problem of form (1.2) in both finite-sum and on-line cases till the rest of this paper. One important task for non-convex optimization is to search for, given the precision accuracy > 0, an -approximate first-order stationary point x \u2208 R d or \u2207f (x) \u2264 . In this paper, we aim to propose a new technique, called the Stochastic Path-Integrated Differential EstimatoR (Spider), which enables us to construct an estimator that tracks a deterministic quantity with significantly lower sampling costs. As the readers will see, the Spider technique further allows us to design an algorithm with a faster rate of convergence for non-convex problem (1.2), in which we utilize the idea of Normalized Gradient Descent (NGD) (Nesterov, 2004;Hazan et al., 2015). NGD is a variant of Gradient Descent (GD) where the stepsize is picked to be inverse-proportional to the norm of the full gradient. Compared to GD, NGD exemplifies faster convergence, especially in the neighborhood of stationary points (Levy, 2016). However, NGD has been less popular due to its requirement of accessing the full gradient and its norm at each update. In this paper, we estimate and track the gradient and its norm via the Spider technique and then hybrid it with NGD. Measured by gradient cost which is the total number of computation of stochastic gradients, our proposed Spider-SFO algorithm achieves a faster rate of convergence in O(min(n 1/2 \u22122 , \u22123 )) which outperforms the previous best-known results in both finite-sum (Allen-Zhu & Hazan, 2016) (Reddi et al., 2016) and on-line cases (Lei et al., 2017) by a factor of O(min(n 1/6 , \u22120.333 )).\n\nFor the task of finding stationary points for which we already achieved a faster convergence rate via our proposed Spider-SFO algorithm, a follow-up question to ask is: is our proposed Spider-SFO algorithm optimal for an appropriate class of smooth functions? In this paper, we provide an affirmative answer to this question in the finite-sum case. To be specific, inspired by a counterexample proposed by Carmon et al. (2017b) we are able to prove that the gradient cost upper bound of Spider-SFO algorithm matches the algorithmic lower bound. To put it differently, the gradient cost of Spider-SFO cannot be further improved for finding stationary points for some particular non-convex functions.\n\nNevertheless, it has been shown that for machine learning methods such as deep learning, approximate stationary points that have at least one negative Hessian direction, including saddle points and local maximizers, are often not sufficient and need to be avoided or escaped from (Dauphin et al., 2014;Ge et al., 2015). Specifically, under the smoothness condition for f (x) and an additional Hessian-Lipschitz condition for \u2207 2 f (x), we aim to find an ( , O( 0.5 ))-approximate second-order stationary point which is a point x \u2208 R d satisfying \u2207f (x) \u2264 and \u03bb min (\u2207 2 f (x)) \u2265 \u2212O( 0.5 ) (Nesterov & Polyak, 2006). As a side result, we propose a variant of our Spider-SFO algorithm, named Spider-SFO + (Algorithm 2) for finding an approximate second-order stationary point, based a so-called Negative-Curvature-Search method. Under an additional Hessian-Lipschitz assumption, Spider-SFO + achieves an ( , O( 0.5 ))-approximate second-order stationary point at a gradient cost of\u00d5(min(n 1/2 \u22122 + \u22122.5 , \u22123 )). In the on-line case, this indicates that our Spider-SFO algorithm improves upon the best-known gradient cost in the on-line case by a factor of\u00d5( \u22120.25 ) (Allen-Zhu & Li, 2018). For the finite-sum case, the gradient cost of Spider is sharper than that of the stateof-the-art Neon+FastCubic/CDHS algorithm in Agarwal et al. (2017); Carmon et al. (2016) by a factor of\u00d5(n 1/4 0.25 ) when n \u2265 \u22121 . 1\n\n\nRelated Works\n\nIn the recent years, there has been a surge of literatures in machine learning community that analyze the convergence property of non-convex optimization algorithms. Limited by space and our knowledge, we have listed all literatures that we believe are mostly related to this work. We refer the readers to the monograph by Jain et al. (2017) and the references therein on recent general and model-specific convergence rate results on non-convex optimization.\n\nFirst-and Zeroth-Order Optimization and Variance Reduction For the general problem of finding approximate stationary points, under the smoothness condition of f (x), it is known that vanilla Gradient Descent (GD) and Stochastic Gradient Descent (SGD), which can be traced back to Cauchy (1847) and Robbins & Monro (1951) and achieve an -approximate stationary point with a gradient cost of O(min(n \u22122 , \u22124 )) (Nesterov, 2004;Ghadimi & Lan, 2013;Nesterov & Spokoiny, 2011;Ghadimi & Lan, 2013;Shamir, 2017).\n\nRecently, the convergence rate of GD and SGD have been improved by the variance-reduction type of algorithms (Johnson & Zhang, 2013;Schmidt et al., 2017). In special, the finite-sum Stochastic Variance-Reduced Gradient (SVRG) and on-line Stochastically Controlled Stochastic Gradient (SCSG), to the gradient cost of\u00d5(min(n 2/3 \u22122 , \u22123.333 )) (Allen-Zhu & Hazan, 2016;Reddi et al., 2016;Lei et al., 2017).\n\nFirst-order method for finding approximate stationary points Recently, many literature study the problem of how to avoid or escape saddle points and achieve an approximate secondorder stationary point at a polynomial gradient cost (Ge et al., 2015;Jin et al., 2017a;Xu et al., 2017;Allen-Zhu & Li, 2018;Hazan et al., 2015;Levy, 2016;Allen-Zhu, 2018;Reddi et al., 2018;Tripuraneni et al., 2018;Jin et al., 2017b;Lee et al., 2016;Agarwal et al., 2017;Carmon et al., 2016;Paquette et al., 2018). Among them, the group of authors Ge et al. (2015); Jin et al. (2017a) proposed the noise-perturbed variants of Gradient Descent (PGD) and Stochastic Gradient Descent (SGD) that escape from all saddle points and achieve an -approximate second-order stationary point in gradient cost of\u00d5(min(n \u22122 , poly(d) \u22124 )) stochastic gradients. Levy (2016) proposed the noise-perturbed variant of NGD which yields faster evasion of saddle points than GD.\n\nThe breakthrough of gradient cost for finding second-order stationary points were achieved in 2016/2017, when the two recent lines of literatures, namely FastCubic (Agarwal et al., 2017) and CDHS (Carmon et al., 2016) as well as their stochastic versions (Allen-Zhu, 2018;Tripuraneni et al., 2018), achieve a gradient cost of\u00d5(min(n \u22121.5 + n 3/4 \u22121.75 , \u22123.5 )) which serve as the bestknown gradient cost for finding an ( , O( 0.5 ))-approximate second-order stationary point before the initial submission of this paper. 2 3 In particular, Agarwal et al. (2017);Tripuraneni et al. (2018) converted the cubic regularization method for finding second-order stationary points (Nesterov & Polyak, 2006) to stochastic-gradient based and stochastic-Hessian-vector-product-based methods, and Carmon et al. (2016); Allen-Zhu (2018) used a Negative-Curvature Search method to avoid saddle points. See also recent works by Reddi et al. (2018) for related saddle-point-escaping methods that achieve similar rates for finding an approximate second-order stationary point.\n\nOnline PCA and the NEON method In late 2017, two groups Xu et al. (2017); Allen-Zhu & Li (2018) proposed a generic saddle-point-escaping method called Neon, a Negative-Curvature-Search method using stochastic gradients. Using such Neon method, one can convert a series of optimization algorithms whose update rules use stochastic gradients and Hessian-vector products (GD,SVRG,FastCubic/CDHS,SGD,SCSG,Natasha2,etc.) to the ones using only stochastic gradients without increasing the gradient cost. The idea of Neon was built upon Oja's iteration for principal component estimation (Oja, 1982), and its global convergence rate was proved to be near-optimal (Li et al., 2017;Jain et al., 2016). Allen-Zhu & Li (2017) later extended such analysis to the rank-k case as well as the gap-free case, the latter of which serves as the pillar of the Neon method.\n\nOther concurrent works As the current work is carried out in its final phase, the authors became aware that an idea of resemblance was earlier presented in an algorithm named the StochAstic Recursive grAdient algoritHm (SARAH) (Nguyen et al., 2017a,b). Both our Spider-type of algorithms and theirs adopt the recursive stochastic gradient update framework. Nevertheless, our techniques essentially differ from the works Nguyen et al. (2017a,b) in two aspects:\n\n(i) The version of SARAH proposed by Nguyen et al. (2017a,b) can be seen as a variant of gradient descent, while ours hybrids the Spider technique with a stochastic version of NGD.\n\n(ii) Nguyen et al. (2017a,b) adopt a large stepsize setting (in fact their goal was to design a memory-saving variant of SAGA (Defazio et al., 2014)), while our algorithms adopt a small stepsize that is proportional to ;\n\nSoon after the initial submission to NIPS and arXiv release of this paper, we became aware that similar convergence rate results for stochastic first-order method were also achieved independently by the so-called SNVRG algorithm (Zhou et al., 2018b,a). 4\n\n\nOur Contributions\n\nIn this work, we propose the Stochastic Path-Integrated Differential Estimator (Spider) technique, which significantly avoids excessive access of stochastic oracles and reduces the time complexity. Such technique can be potential applied in many stochastic estimation problems.\n\n(i) As a first application of our Spider technique, we propose the Spider-SFO algorithm (Algorithm 1) for finding an approximate first-order stationary point for non-convex stochastic optimization problem (1.2), and prove the optimality of such rate in at least one case. Inspired by recent works Johnson & Zhang (2013); Carmon et al. (2016Carmon et al. ( , 2017b and independent of Zhou et al. (2018b,a), this is the first time that the gradient cost of O(min(n 1/2 \u22122 , \u22123 )) in both upper and lower (finite-sum only) bound for finding first-order stationary points for problem (1.2) were obtained.\n\n(ii) Following Carmon et al. (2016); Allen-Zhu & Li (2018); Xu et al. (2017), we propose Spider-SFO + algorithm (Algorithm 2) for finding an approximate second-order stationary point for non-convex stochastic optimization problem. To best of our knowledge, this is also the first time that the gradient cost of\u00d5(min(n 1/2 \u22122 + \u22122.5 , \u22123 )) achieved with standard assumptions.\n\n(iii) As a second application of our Spider technique, we apply it to zeroth-order optimization for problem (1.2) and achieves individual function accesses of O(min(dn 1/2 \u22122 , d \u22123 )). To best of our knowledge, this is also the first time that using Variance Reduction technique (Schmidt et al., 2017;Johnson & Zhang, 2013) to reduce the individual function accesses for non-convex problems to the aforementioned complexity.\n\n(iv) We propose a much simpler analysis for proving convergence to a stationary point. One can flexibly apply our proof techniques to analyze others algorithms, e.g. SGD, SVRG (Johnson & Zhang, 2013), and SAGA (Defazio et al., 2014).\n\nOrganization. The rest of this paper is organized as follows. \u00a72 presents the core idea of stochastic path-integrated differential estimator that can track certain quantities with much reduced computational costs. \u00a73 provides the Spider method for stochastic first-order methods and convergence rate theorems of this paper for finding approximate first-order stationary and second-order stationary points, and details a comparison with concurrent works. \u00a74 provides the Spider method for stochastic zeroth-order methods and relevant convergence rate theorems. \u00a75 concludes the paper with future directions. All the detailed proofs are deferred to the appendix in their order of appearance.\n\nNotation. Throughout this paper, we treat the parameters L, \u2206, \u03c3, and \u03c1, to be specified later as global constants. Let \u00b7 denote the Euclidean norm of a vector or spectral norm of a square matrix. Denote p n = O(q n ) for a sequence of vectors p n and positive scalars q n if there is a global constant C such that |p n | \u2264 Cq n , and p n =\u00d5(q n ) such C hides a poly-logarithmic factor of the parameters. Denote p n = \u2126(q n ) if there is a global constant C such that |p n | \u2265 Cq n . Let \u03bb min (A) denote the least eigenvalue of a real symmetric matrix A. For fixed K \u2265 k \u2265 0, let x k:K denote the sequence {x k , . . . , x K }. Let [n] = {1, . . . , n} and S denote the cardinality of a multi-set S \u2282 [n] of samples (a generic set that allows elements of multiple instances). For simplicity, we further denote the averaged sub-sampled stochastic estimator B S := (1/S) i\u2208S B i and averaged sub-sampled gradient \u2207f S := (1/S) i\u2208S \u2207f i . Other notations are explained at their first appearance.\n\n2 Stochastic Path-Integrated Differential Estimator: Core Idea\n\nIn this section, we present in detail the underlying idea of our Stochastic Path-Integrated Differential Estimator (Spider) technique behind the algorithm design. As the readers will see, such technique significantly avoids excessive access of the stochastic oracle and reduces the complexity, which is of independent interest and has potential applications in many stochastic estimation problems.\n\nLet us consider an arbitrary deterministic vector quantity Q(x). Assume that we observe a sequencex 0:K , and we want to dynamically track Q(x k ) for k = 0, 1, . . . , K. Assume further that we have an initial estimateQ(x 0 ) \u2248 Q(x 0 ), and an unbiased estimate\n\u03be k (x 0:k ) of Q(x k ) \u2212 Q(x k\u22121 ) such that for each k = 1, . . . , K E [\u03be k (x 0:k ) |x 0:k ] = Q(x k ) \u2212 Q(x k\u22121 ).\nThen we can integrate (in the discrete sense) the stochastic differential estimate as\nQ(x 0:K ) :=Q(x 0 ) + K k=1 \u03be k (x 0:k ).\n(2.1)\n\nWe call estimatorQ(x 0:K ) the Stochastic Path-Integrated Differential EstimatoR, or Spider for brevity. We conclude the following proposition which bounds the error of our estimator Q (x 0:K ) \u2212 Q(x K ) , in terms of both expectation and high probability:\nProposition 1. We have (i) The martingale variance bound has E Q (x 0:K ) \u2212 Q(x K ) 2 = E Q (x 0 ) \u2212 Q(x 0 ) 2 + K k=1 E \u03be k (x 0:k ) \u2212 (Q(x k ) \u2212 Q(x k\u22121 )) 2 . (2.2) (ii) Suppose Q (x 0 ) \u2212 Q(x 0 ) \u2264 b 0 (2.3)\nand for each k = 1, . . . , K\n\u03be k (x 0:k ) \u2212 (Q(x k ) \u2212 Q(x k\u22121 )) \u2264 b k ,(2.\n\n4)\n\nThen for any \u03b3 > 0 and a given k \u2208 {1, . . . , K} we have with probability at least 1 \u2212 4\u03b3\nQ (x 0:k ) \u2212 Q(x k ) \u2264 2 k s=0 b 2 s \u00b7 log 1 \u03b3 .\n(2.5)\n\nProposition 1(i) can be easily concluded using the property of square-integrable martingales. To prove the high-probability bound in Proposition 1(ii), we need to apply an Azuma-Hoeffding-type concentration inequality (Pinelis, 1994). See \u00a7A in the Appendix for more details. Now, let B map any x \u2208 R d to a random estimate B i (x) such that, conditioning on the observed sequence x 0:k , we have for each k = 1, . . . , K,\nE B i (x k ) \u2212 B i (x k\u22121 ) | x 0:k = V k \u2212 V k\u22121 .\n(2.6) At each step k let S * be a subset that samples S * elements in [n] with replacement, and let the stochastic estimator B S * = (1/S * ) i\u2208S * B i satisfy\nE B i (x) \u2212 B i (y) 2 \u2264 L 2 B x \u2212 y 2 ,(2.7)\nand x k \u2212 x k\u22121 \u2264 1 for all k = 1, . . . , K. Finally, we set our estimator V k of B(x k ) as\nV k = B S * (x k ) \u2212 B S * (x k\u22121 ) + V k\u22121 .\nApplying Proposition 1 immediately concludes the following lemma, which gives an error bound of the estimator V k in terms of the second moment of V k \u2212 B(x k ) :\n\nLemma 1. We have under the condition (2.7) that for all k = 1, . . . , K,\nE V k \u2212 B(x k ) 2 \u2264 kL 2 B 2 1 S * + E V 0 \u2212 B(x 0 ) 2 . (2.8)\nIt turns out that one can use Spider to track many quantities of interest, such as stochastic gradient, function values, zero-order estimate gradient, functionals of Hessian matrices, etc. Our proposed Spider-based algorithms in this paper take B i as the stochastic gradient \u2207f i and the zeroth-order estimate gradient, separately.\n\n\nSPIDER for Stochastic First-Order Method\n\nIn this section, we apply Spider to the task of finding both first-order and second-order stationary points for non-convex stochastic optimization. The main advantage of Spider-SFO lies in using SPIDER to estimate the gradient with a low computation cots. We introduce the basic settings and assumptions in \u00a73.1 and propose the main error-bound theorems for finding approximate first-order and second-order stationary points, separately in \u00a73.2 and \u00a73.3.\n\n\nSettings and Assumptions\n\nWe first introduce the formal definition of approximate first-order and second-order stationary points, as follows.\n\nDefinition 1. We call x \u2208 R d an -approximate first-order stationary point, or simply an FSP, if \u2207f (x) \u2264 .\n\n(3.1)\n\nAlso, call x an ( , \u03b4)-approximate second-order stationary point, or simply an SSP, if\n\u2207f (x) \u2264 , \u03bb min \u2207 2 f (x) \u2265 \u2212\u03b4. (3.2)\nThe definition of an ( , \u03b4)-approximate second-order stationary point generalizes the classical version where \u03b4 = \u221a \u03c1 , see e.g. Nesterov & Polyak (2006). For our purpose of analysis, we also pose the following additional assumption:\n\nAssumption 1. We assume the following\n(i) The \u2206 := f (x 0 ) \u2212 f * < \u221e where f * = inf x\u2208R d f (x) is the global infimum value of f (x);\n(ii) The component function f i (x) has an averaged L-Lipschitz gradient, i.e. for all x, y,\nE \u2207f i (x) \u2212 \u2207f i (y) 2 \u2264 L 2 x \u2212 y 2 ;\n(iii) (For on-line case only) the stochastic gradient has a finite variance bounded by \u03c3 2 < \u221e, i.e.\nE \u2207f i (x) \u2212 \u2207f (x) 2 \u2264 \u03c3 2 .\nAlternatively, to obtain high-probability results using concentration inequalities, we propose the following more stringent assumptions:\n\nAssumption 2. We assume that Assumption 1 holds and, in addition,\n(ii') (Optional) each component function f i (x) has L-Lipschitz continuous gradient, i.e. for all i, x, y, \u2207f i (x) \u2212 \u2207f i (y) \u2264 L x \u2212 y .\nNote when f is twice continuously differentiable, Assumption 1 (ii) is equivalent to E \u2207 2 f i (x) 2 \u2264 L 2 for all x and is weaker than the additional Assumption 2 (ii'), since the absolute norm squared bounds the variance for any random vector.\n\n(iii') (For on-line case only) the gradient of each component function f i (x) has finite bounded variance by \u03c3 2 < \u221e (with probability 1) , i.e. for all i, x,\n\u2207f i (x) \u2212 \u2207f (x) 2 \u2264 \u03c3 2 .\nAlgorithm 1 Spider-SFO: Input x 0 , q, S 1 , S 2 , n 0 , , and\u02dc (For finding first-order stationary point)\n1: for k = 0 to K do 2:\nif mod (k, q) = 0 then 3:\n\nDraw S 1 samples (or compute the full gradient for the finite-sum case), let v k = \u2207f S 1 (x k ) 4: else 5:\n\nDraw S 2 samples, and let   Assumption 2 is common in applying concentration laws to obtain high probability result 5 .\nv k = \u2207f S 2 (x k ) \u2212 \u2207f S 2 (x k\u22121 ) + v k\u2212x k+1 = x k \u2212 \u03b7 \u00b7 (v k / v k ) where \u03b7 = Lnx k+1 = x k \u2212 \u03b7 k v k where \u03b7 k = min Ln 0 v k , 1\nFor the problem of finding an ( , \u03b4)-approximate second-order stationary point, we pose in addition to Assumption 1 the following assumption:\n\nAssumption 3. We assume that Assumption 2 (including (ii')) holds and, in addition, each component function f i (x) has \u03c1-Lipschitz continuous Hessian, i.e. for all i, x, y,\n\u2207 2 f i (x) \u2212 \u2207 2 f i (y) \u2264 \u03c1 x \u2212 y .\nWe emphasize that Assumptions 1, 2, and 3 are standard for non-convex stochastic optimization (Agarwal et al., 2017;Carmon et al., 2017b;Jin et al., 2017a;Xu et al., 2017;Allen-Zhu & Li, 2018).\n\n\nFirst-Order Stationary Point\n\nRecall that NGD has iteration update rule\nx k+1 = x k \u2212 \u03b7 \u2207f (x k ) \u2207f (x k ) , (3.3)\nwhere \u03b7 is a constant step size. The NGD update rule (3.3) ensures x k+1 \u2212 x k being constantly equal to the stepsize \u03b7, and might fastly escape from saddle points and converge to a second-order stationary point (Levy, 2016). We propose Spider-SFO in Algorithm 1, which is like a stochastic variant of NGD with the Spider technique applied, so as to maintain an estimator in each epoch \u2207f (x k ) at a higher accuracy under limited gradient budgets.\n\nTo analyze the convergence rate of Spider-SFO, let us first consider the on-line case for Algorithm 1. We let the input parameters be\nS 1 = 2\u03c3 2 2 , S 2 = 2\u03c3 n 0 , \u03b7 = Ln 0 , \u03b7 k = min Ln 0 v k , 1 2Ln 0 , q = \u03c3n 0 , (3.4)\nwhere n 0 \u2208 [1, 2\u03c3/ ] is a free parameter to choose. 6 In this case, v k in Line 5 of Algorithm 1 is a\n\nSpider for \u2207f (x k ). To see this, recall \u2207f i (x k\u22121 ) is the stochastic gradient drawn at step k and\nE \u2207f i (x k ) \u2212 \u2207f i (x k\u22121 ) | x 0:k = \u2207f (x k ) \u2212 \u2207f (x k\u22121 ). (3.5)\nPlugging in V k = v k and B i = \u2207f i in Lemma 1 of \u00a72, we can use v k in Algorithm 1 as the Spider and conclude the following lemma that is pivotal to our analysis.\n\nLemma 2. Set the parameters S 1 , S 2 , \u03b7, and q as in (3.4), and k 0 = k/q \u00b7 q. Then under the Assumption 1, we have\nE v k \u2212 \u2207f (x k ) 2 | x 0:k 0 \u2264 2 .\nHere we compute the conditional expectation over the randomness of x (k 0 +1):k .\n\nLemma 2 shows that our Spider v k of \u2207f (x) maintains an error of O( ). Using this lemma, we are ready to present the following results for Stochastic First-Order (SFO) method for finding first-order stationary points of (1.2).\n\n\nUpper Bound for Finding First-Order Stationary Points, in Expectation\n\nTheorem 1 (First-Order Stationary Point, on-line setting, expectation). For the on-line case, set the parameters S 1 , S 2 , \u03b7, and q as in (3.4), and K = (4L\u2206n 0 ) \u22122 + 1. Then under the Assumption 1, for Algorithm 1 with OPTION I, after K iteration, we have\nE [ \u2207f (x) ] \u2264 5 . (3.6)\nThe gradient cost is bounded by 16L\u2206\u03c3 \u00b7 \u22123 + 2\u03c3 2 \u22122 + 4\u03c3n \u22121 0 \u22121 for any choice of n 0 \u2208 [1, 2\u03c3/ ].\n\nTreating \u2206, L and \u03c3 as positive constants, the stochastic gradient complexity is O( \u22123 ).\n\n6 When n0 = 1, the mini-batch size is 2\u03c3/ , which is the largest mini-batch size that Algorithm 1 allows to choose.\n\nThe relatively reduced minibatch size serves as the key ingredient for the superior performance of Spider-SFO. For illustrations, let us compare the sampling efficiency among SGD, SCSG and Spider-SFO in their special cases. With some involved analysis of these algorithms, we can conclude that to ensure a sufficient function value decrease of \u2126( 2 /L) at each iteration,\n(i) for SGD the choice of mini-batch size is O \u03c3 2 \u00b7 \u22122 ;\n(ii) for SCSG (Lei et al., 2017) and Natasha2 (Allen-Zhu, 2018) the mini-batch size is O \u03c3\u00b7 \u22121.333 ;\n\n(iii) for our Spider-SFO only needs a reduced mini-batch size of O \u03c3 \u00b7 \u22121\n\nTurning to the finite-sum case, analogous to the on-line case we let\nS 2 = n 1/2 n 0 , \u03b7 = Ln 0 , \u03b7 k = min Ln 0 v k , 1 2Ln 0 , q = n 0 n 1/2 , (3.7)\nwhere n 0 \u2208 [1, n 1/2 ]. In this case, one computes the full gradient v k = \u2207f S 1 (x k ) in Line 3 of Algorithm 1. We conclude our second upper-bound result:\n\nTheorem 2 (First-Order Stationary Point, finite-sum setting). In the finite-sum case, set the parameters S 2 , \u03b7, and q as in (3.7), K = (4L\u2206n 0 ) \u22122 + 1 and let S 1 = [n], i.e. we obtain the full gradient in Line 3. The gradient cost is bounded by n + 8(L\u2206) \u00b7 n 1/2 \u22122 + 2n \u22121 0 n 1/2 for any choice of n 0 \u2208 [1, n 1/2 ]. Treating \u2206, L and \u03c3 as positive constants, the stochastic gradient complexity is O(n + n 1/2 \u22122 ).\n\n\nLower Bound for Finding First-Order Stationary Points\n\nTo conclude the optimality of our algorithm we need an algorithmic lower bound result (Carmon et al., 2017b;Woodworth & Srebro, 2016). Consider the finite-sum case and any random algorithm A that maps functions f :\nR d \u2192 R to a sequence of iterates in R d+1 , with [x k ; i k ] = A k\u22121 \u03be, \u2207f i 0 (x 0 ), \u2207f i 1 (x 1 ), . . . , \u2207f i k\u22121 (x k\u22121 ) , k \u2265 1, (3.8) where A k are measure mapping into R d+1 , i k is the individual function chosen by A at iteration k, and \u03be is uniform random vector from [0, 1]. And [x 0 ; i 0 ] = A 0 (\u03be), where A 0 is a measure mapping.\nThe lower-bound result for solving (1.2) is stated as follows:\n\nTheorem 3 (Lower bound for SFO for the finite-sum setting). For any L > 0, \u2206 > 0, and 2 \u2264 n \u2264 O \u2206 2 L 2 \u00b7 \u22124 , for any algorithm A satisfying (3.8), there exists a dimension d =\u00d5 \u2206 2 L 2 \u00b7n 2 \u22124 , and a function f satisfies Assumption 1 in the finite-sum case, such that in order to find a pointx for which \u2207f (x) \u2264 , A must cost at least \u2126 L\u2206 \u00b7 n 1/2 \u22122 stochastic gradient accesses.\n\nNote the condition n \u2264 O( \u22124 ) in Theorem 3 ensures that our lower bound \u2126(n 1/2 \u22122 ) = \u2126(n + n 1/2 \u22122 ), and hence our upper bound in Theorem 1 matches the lower bound in Theorem 3 up to a constant factor of relevant parameters, and is hence near-optimal. Inspired by Carmon et al. (2017b), our proof of Theorem 3 utilizes a specific counterexample function that requires at least \u2126(n 1/2 \u22122 ) stochastic gradient accesses. Note Carmon et al. (2017b) analyzed such counterexample in the deterministic case n = 1 and we generalize such analysis to the finite-sum case n \u2265 1. Remark 1. Note by setting n = O( \u22124 ) the lower bound complexity in Theorem 3 can be as large as \u2126( \u22124 ). We emphasize that this does not violate the O( \u22123 ) upper bound in the on-line case [Theorem 1], since the counterexample established in the lower bound depends not on the stochastic gradient variance \u03c3 2 specified in Assumption 1(iii), but on the component number n. To obtain the lower bound result for the on-line case with the additional Assumption 1(iii), with more efforts one might be able to construct a second counterexample that requires \u2126( \u22123 ) stochastic gradient accesses with the knowledge of \u03c3 instead of n. We leave this as a future work.\n\nUpper Bound for Finding First-Order Stationary Points, in High-Probability We consider obtaining high-probability results. With Theorem 1 and Theorem 2 in hand, by Markov Inequality, we have \u2207f (x) \u2264 15 with probability 2 3 . Thus a straightforward way to obtain a high probability result is by adding an additional verification step in the end of Algorithm 1, in which we check whetherx satisfies \u2207f (x) \u2264 15 (for the on-line case when \u2207f (x) are unaccessible, under Assumption 2 (iii'), we can draw\u00d5( \u22122 ) samples to estimate \u2207f (x) in high accuracy). If not, we can restart Algorithm 1 (at most in O(log(1/p)) times) until it find a desired solution. However, because the above way needs running Algorithm 1 in multiple times, in the following, we show with Assumption 2 (including (2)), original Algorithm 1 obtains a solution with an additional polylogarithmic factor under high probability.\n\nTheorem 4 (First-Order Stationary Point, on-line setting, high probability). For the on-line case, set the parameters S 1 , S 2 , \u03b7 and q in (3.4). Set\u02dc = 10 log 4 4L\u2206n 0 \u22122 + 12 p \u22121 \u223c\u00d5( ).\n\nThen under the Assumption 2 (including (ii')), with probability at least 1\u2212p, Algorithm 1 terminates before K 0 = (4L\u2206n 0 ) \u22122 + 2 iterations and outputs an\nx K satisfying v K \u2264 2\u02dc and \u2207f (x K ) \u2264 3\u02dc . (3.9)\nThe gradient costs to find a FSP satisfying (3.9) with probability 1 \u2212 p are bounded by 16L\u2206\u03c3 \u00b7 \u22123 + 2\u03c3 2 \u22122 + 8\u03c3n \u22121 0 \u22121 for any choice of of n 0 \u2208 [1, 2\u03c3/ ]. Treating \u2206, L and \u03c3 as constants, the stochastic gradient complexity is\u00d5( \u22123 ).\n\nTheorem 5 (First-Order Stationary Point, finite-sum setting). In the finite-sum case, set the parameters S 1 , S 2 , \u03b7, and q as (3.7). let S 1 = [n], i.e. we obtain the full gradient in Line 3. Then under the Assumption 2 (including (ii')), with probability at least 1 \u2212 p, Algorithm 1 terminates before K 0 = 4L\u2206n 0 / 2 + 2 iterations and outputs an\nx K satisfying v K \u2264 2\u02dc and \u2207f (x K ) \u2264 3\u02dc .\n(3.10) where\u02dc = 16 log 4(L\u2206n 0 \u22122 + 12 p \u22121 =\u00d5( ). So the gradient costs to find a FSP satisfying (3.10) with probability 1 \u2212 p are bounded by n + 8L\u2206n 1/2 \u22122 + (2n \u22121 0 )n 1/2 + 4n \u22121 0 n 1/2 with any choice of n 0 \u2208 [1, n 1/2 ]. Treating \u2206, L and \u03c3 as constants, the stochastic gradient complexity is O(n + n 1/2 \u22122 ).\n\n\nSecond-Order Stationary Point\n\nTo find a second-order stationary point with (3.1), we can fuse our Spider-SFO in Algorithm 1 with a Negative-Curvature-Search (NC-Search) iteration that solves the following task: given a point x \u2208 R d , decide if \u03bb min (\u2207 2 f (x)) \u2265 \u2212\u03b4 or find a unit vector w 1 such that w 1 \u2207 2 f (x)w 1 \u2264 \u2212\u03b4/2 (for numerical reasons, one has to leave some room between the two bounds). For the on-line case, NC-Search can be efficiently solved by Oja's algorithm (Oja, 1982;Allen-Zhu, 2018) and also by Neon (Allen-Zhu & Li, 2018;Xu et al., 2017) with the gradient cost of\u00d5(\u03b4 \u22122 ). 7 When w 1 is found, one can set w 2 = \u00b1(\u03b4/\u03c1)w 1 where \u00b1 is a random sign. Then under Assumption 3, Taylor's expansion implies that (Allen-Zhu & Li, 2018)\nf (x + w 2 ) \u2264 f (x) + [\u2207f (x)] w 2 + 1 2 w 2 [\u2207 2 f (x)]w 2 + \u03c1 6 w 2 3 . (3.11) Taking expectation, one has Ef (x + w 2 ) \u2264 f (x) \u2212 \u03b4 3 /(2\u03c1 2 ) + \u03b4 3 /(6\u03c1 2 ) = f (x) \u2212 \u03b4 3 /(3\u03c1 2 )\n. This indicates that when we find a direction of negative curvature or Hessian, updating x \u2190 x + w 2 decreases the function value by \u2126(\u03b4 3 ) in expectation. Our Spider-SFO algorithm fused with NC-Search is described in the following steps:\n\nStep 1. Run an efficient NC-Search iteration to find an O(\u03b4)-approximate negative Hessian direction w 1 using stochastic gradients, e.g. Neon2 (Allen-Zhu & Li, 2018).\n\nStep 2. If NC-Search find a w 1 , update x \u2190 x \u00b1 (\u03b4/\u03c1)w 1 in \u03b4/(\u03c1\u03b7) mini-steps, and simultaneously use Spider v k to maintain an estimate of \u2207f (x). Then Goto Step 1.\n\nStep 3. If not, run Spider-SFO for \u03b4/(\u03c1\u03b7) steps directly using the Spider v k (without restart) in\n\nStep 2. Then Goto Step 1.\n\nStep 4. During\n\nStep 3, if we find v k \u2264 2\u02dc , return x k .\n\nThe formal pseudocode of the algorithm described above, which we refer to as Spider-SFO + , is detailed in Algorithm 2 8 . The core reason that Spider-SFO + enjoys a highly competitive convergence rate is that, instead of performing a single large step \u03b4/\u03c1 at the approximate direction of negative curvature as in Neon2(Allen-Zhu & Li, 2018), we split such one large step into \u03b4/(\u03c1\u03b7) small, equal-length mini-steps in Step 2, where each mini-step moves the iteration by an \u03b7 distance. This allows the algorithm to successively maintain the Spider estimate of the current gradient in\n\nStep 3 and avoid re-computing the gradient in Step 1.\n\nOur final result on the convergence rate of Algorithm 2 is stated as:\n\nTheorem 6 (Second-Order Stationary Point). Let Assumptions 3 hold. For the on-line case, set q, S 1 , S 2 , \u03b7 in (3.4), K = \u03b4Ln 0 \u03c1 with any choice of n 0 \u2208 [1, 2\u03c3/ ], then with probability at 7 Recall that the NEgative-curvature-Originated-from-Noise method (or Neon method for short) proposed independently by Allen-Zhu & Li (2018); Xu et al. (2017) is a generic procedure that convert an algorithm that finds an approximate first-order stationary points to the one that finds an approximate second-order stationary point.\n\n8 In our initial version, Spider-SFO + first find a FSP and then run NC-search iteration to find a SSP, which also ensures competitive\u00d5( \u22123 ) rate. Our newly Spider-SFO + are easier to fuse momentum technique when n is small. Please see the discussion later.\n\nAlgorithm 2 Spider-SFO + : Input x 0 , S 1 , S 2 , n 0 , q, \u03b7, K , k = 0, ,\u02dc , (For finding a second-order stationary point)\n1: for j = 0 to J do 2:\nRun an efficient NC-search iteration, e.g. Neon2(f, x k , 2\u03b4, 1 16J ) and obtain w 1 3:\n\nif w 1 = \u22a5 then 4:\n\nSecond-Order Descent:\n\n\n5:\n\nRandomly flip a sign, and set w 2 = \u00b1\u03b7w 1 and j = \u03b4/(\u03c1\u03b7) \u2212 1 6:\nfor k to k + K do 7:\nif mod(k, q) = 0 then 8:\nDraw S 1 samples, v k = \u2207f S 1 (x k ) 9:\nelse 10: if mod(k, q) = 0 then 18: \nDraw S 2 samples, v k = \u2207f S 2 (x k ) \u2212 \u2207f S 2 (x k\u22121 ) + v k\u22121 11: end if 12: x k+1 = x k \u2212 wDraw S 1 samples, v k = \u2207f S 1 (x k ) 19: else 20: Draw S 2 samples, v k = \u2207f S 2 (x k ) \u2212 \u2207f S 2 (x k\u22121 ) + v k\u22121 21: end if 22: if v k \u2264 2\u02dcx k+1 = x k \u2212 \u03b7 \u00b7 (v k / v k ) 26:\nend for 27: end if 28: end for least 1/2 9 , Algorithm 2 outputs an\nx k with j \u2264 J = 4 max 3\u03c1 2 \u2206 \u03b4 3 , 4\u2206\u03c1 \u03b4 + 4, and k \u2264 K 0 = 4 max 3\u03c1 2 \u2206 \u03b4 3 , 4\u2206\u03c1 \u03b4 + 4 Ln 0 \u03b4 \u03c1 satisfying \u2207f (x k ) \u2264\u02dc and \u03bb min (\u2207 2 f (x k )) \u2265 \u22123\u03b4, (3.12) with\u02dc = 10 log 256 max 3\u03c1 2 \u2206 \u03b4 3 , 4\u2206\u03c1 \u03b4 + 1 \u03b4Ln 0 \u03c1 + 64 =\u00d5( ).\nThe gradient cost to find a Second-Order Stationary Point with probability at least 1/2 is upper bounded b\u1ef9\nO \u2206L\u03c3 3 + \u2206\u03c3L\u03c1 2 \u03b4 2 + \u2206L 2 \u03c1 2 \u03b4 5 + \u2206L 2 \u03c1 \u03b4 3 + \u03c3 2 2 + L 2 \u03b4 2 + L\u03c3\u03b4 \u03c1 2 .\nAnalogously for the finite-sum case, under the same setting of Theorem 2, set q, S 1 , S 2 , \u03b7 in (3.7), K = \u03b4Ln 0 \u03c1 ,\u02dc = 16 log 256 max 3\u03c1 2 \u2206 \u03b4 3 , 4\u2206\u03c1 \u03b4 + 1 \u03b4Ln 0 \u03c1 + 64 =\u00d5( ), with probability 1/2, Algorithm 2 outputs an x k satisfying (3.12) in j \u2264 J and k \u2264 K 0 with gradients cost of\nO \u2206Ln 1/2 2 + \u2206\u03c1Ln 1/2 \u03b4 2 + \u2206L 2 \u03c1 2 \u03b4 5 + \u2206L 2 \u03c1 \u03b4 3 + n + L 2 \u03b4 2 + Ln 1/2 \u03b4 \u03c1 .\nCorollary 7. Treating \u2206, L, \u03c3, and \u03c1 as positive constants, with high probability the gradient cost for finding an ( , \u03b4)-approximate second-order stationary point is\u00d5( \u22123 + \u03b4 \u22122 \u22122 + \u03b4 \u22125 ) for the on-line case and\u00d5(n 1/2 \u22122 + n 1/2 \u03b4 \u22122 \u22121 + \u03b4 \u22123 \u22121 + \u03b4 \u22125 + n) for the finite-sum case, respectively.\nWhen \u03b4 = O( 0.5 ), the gradient cost is O(min(n 1/2 \u22122 + \u22122.5 , \u22123 )).\nNotice that one may directly apply an on-line variant of the Neon method to the Spider-SFO Algorithm 1 which alternately does Second-Order Descent (but not maintaining Spider) and First-Order Descent (Running a new Spider-SFO). Simple analysis suggests that the Neon+ Spider-SFO algorithm achieves a gradient cost of\u00d5 \u22123 + \u22122 \u03b4 \u22123 + \u03b4 \u22125 for the on-line case and O n 1/2 \u22122 + n 1/2 \u22121 \u03b4 \u22123 + \u03b4 \u22125 for the finite-sum case (Allen-Zhu & Li, 2018;Xu et al., 2017). We discuss the differences in detail.\n\n\u2022 The dominate term in the gradient cost of Neon+ Spider-SFO is the so-called coupling term in the regime of interest: \u22122 \u03b4 \u22123 for the on-line case and n 1/2 \u22121 \u03b4 \u22123 for the finite-sum case, separately. Due to this term, most convergence rate results in concurrent works for the on-line case such as Reddi et al. (2018) \u2022 Our analysis sharpens the seemingly non-improvable coupling term by modifying the single large Neon step to many mini-steps. Such modification enables us to maintain the Spider estimates and obtain a coupling term O min(n, \u22122 )\u03b4 \u22122 of Spider-SFO + , which improves upon the Neon coupling term O min(n, \u22122 )\u03b4 \u22123 by a factor of \u03b4.\n\n\u2022 For the finite-sum case, Spider-SFO + enjoys a convergence rate that is faster than existing methods only in the regime n = \u2126( \u22121 ) [ Table 1]. For the case of n = O( \u22121 ), using Spider to track the gradient in the Neon procedure can be more costly than applying appropriate acceleration techniques (Agarwal et al., 2017;Carmon et al., 2016). 10 Beacause it is wellknown that momentum technique (Nesterov, 1983) provably ensures faster convergence rates when n is sufficient small (Shalev-Shwartz & Zhang, 2016). One can also apply momentum technique to solve the sub-problem in\n\nStep 1 \n\n\nComparison with Concurrent Works\n\nThis subsection compares our Spider algorithms with concurrent works. In special, we detail our main result for applying Spider to first-order methods in the list below: We summarize the comparison with concurrent works that solve (1.2) under similar assumptions in Table 1. In addition, we provide Figure 1 which draws the gradient cost against the magnitude of n for both an approximate stationary point. 11 For simplicity, we leave out the complexities of the algorithms that has Hessian-vector product access and only record algorithms that use stochastic gradients only. 12 Specifically, the yellow-boxed complexity O(n \u22121.5 + n 3/4 \u22121.75 ) in Table 1, which was achieved by Neon+FastCubic/CDHS (Allen-Zhu & Li, 2018; Jin et al., 2017b) for finding an approximate second-order stationary point in the finite-sum case using momentum technique, are the only results that have not been outperformed by our Spider-SFO + algorithm in certain parameter regimes (n \u2264 O( \u22121 ) in this case).\n\nAlgorithm 3 Spider-SZO: Input x 0 , S 1 , S 2 , q, u, (For finding first-order stationary point)\n1: for k = 0 to K do 2:\nif mod (k, q) = 0 then 3:\nDraw S 1 = S 1 /d training samples, for each dimension j \u2208 [d], compute ( with 2S 1 total IZO costs) v k j = 1 S 1 i\u2208S 1 f i (x k + \u00b5e j ) \u2212 f i (x k ) \u00b5\nwhere e j denotes the vector with j-th natural unit basis vector.\n\n\n4:\n\nelse 5:\n\nDraw S 2 sample pairs (i, u), where i \u2208 [n] and u \u223c N (0, I d ) with i and \u00b5 being independent.\n\n\n6:\n\nUpdate\nv k = 1 S 2 (i,u)\u2208S 2 f i (x k + \u00b5u) \u2212 f i (x k ) \u00b5 u \u2212 f i (x k\u22121 + \u00b5u) \u2212 f i (x k\u22121 ) \u00b5 u + v k\u22121 7: end if 8: x k+1 = x k \u2212 \u03b7 k v k where \u03b7 k = min Ln 0 v k , 1\n2Ln 0 for convergence rates in expectation 9: end for 10: Returnx chosen uniformly at random from {x k } K\u22121 k=0 Definition 2. An IZO takes an index i \u2208 [n] and a point x \u2208 R d , and returns the f i (x).\n\nWe use Assumption 2 (including (ii')) for convergence analysis which is standard for SZO (Nesterov & Spokoiny, 2011;Ghadimi & Lan, 2013) algorithms. Because the true gradient are not allowed to obtain for SZO. Most works (Nesterov & Spokoiny, 2011;Ghadimi & Lan, 2013;Shamir, 2017) use the gradient of a smoothed version of the objective function through a two-point feedback in a stochastic setting. Following (Nesterov & Spokoiny, 2011), we consider the typical Gaussian distribution in the convolution to smooth the function. Defin\u00ea From (Nesterov & Spokoiny, 2011), the following properties holds :\nf (x) = 1 (2\u03c0) d 2 f (x + \u00b5u)e \u2212 1 2 u 2 du = E u [f (x + \u00b5u)], (4.1) where x \u2208 R d .\n(i) The gradient off satisfies:\n\u2207f (x) = 1 (2\u03c0) d 2 f (x + \u00b5u) \u2212 f (x) \u00b5 ue \u2212 1 2 u 2 du. (4.2) (ii) For any x \u2208 R d , f (x)\nhas Lipschitz continuous gradients, we have\n\u2207f (x) \u2212 \u2207f (x) \u2264 \u00b5 2 L(d + 3) 3 2 . (4.3) (iii) For any x \u2208 R d , f (x) has Lipschitz continuous gradients, we have E u 1 \u00b5 2 (f (x + \u00b5u) \u2212 f (x)) 2 u 2 \u2264 \u00b5 2 2 L 2 (d + 6) 3 + 2(d + 4) \u2207f (x) 2 . (4.4)\nFrom the (1), suppose u \u223c N (0, I d ), and i \u2208 [n], with u and i being independent, we have\nE i,u f i (x k + \u00b5u i ) \u2212 f i (x k ) \u00b5 u = 1 (2\u03c0) d 2 E i f i (x + \u00b5u) \u2212 f i (x) \u00b5 ue \u2212 1 2 u 2 du = 1 (2\u03c0) d 2 f (x + \u00b5u) \u2212 f (x) \u00b5 ue \u2212 1 2 u 2 du = \u2207f (x k ). (4.5) Also E i,u f i (x k + \u00b5u) \u2212 f i (x k ) \u00b5 u \u2212 f i (x k\u22121 + \u00b5u) \u2212 f i (x k\u22121 ) \u00b5 u = \u2207f (x k ) \u2212 \u2207f (x k\u22121 ). (4.6)\nFor non-convex case, the best known result is O(d \u22124 ) from Ghadimi & Lan (2013). We has not found a work that applying Variance Reduction technique to significantly reduce the complexity of IZO. This might because that even in finite-sum case, the full gradient is not available (with noise). In this paper, we give a stronger results by Spider technique, directly reducing the IZO from O(d \u22124 ) to O(min(dn 1/2 \u22122 , d \u22123 )).\n\nFrom (4.6), we can integrate the two-point feed-back to track \u2207f (x). The algorithm is shown in Algorithm 3. Then the following lemma shows that v k is a high accurate estimator of \u2207f (x k ) :\n\nLemma 3. Under the Assumption 2, suppose i is random number of the function index, (i \u2208 [n]) and u is a standard Gaussian random vector, i.e. u \u223c N (0, I d ), we have\nE i,u f i (x + \u00b5u) \u2212 f i (x) \u00b5 u \u2212 f i (y + \u00b5u) \u2212 f i (y) \u00b5 u 2 \u2264 2(d + 4)L 2 x \u2212 y 2 + 2\u00b5 2 (d + 6) 3 L 2 . (4.7)\nFrom (4.3), by setting a smaller \u00b5, the smoothed gradient \u2207f (x) approximates \u2207f (x), which ensures sufficient function descent in each iteration. For simpleness, we only give expectation result, shown in Theorem 8.\n\n\nTheorem 8. Under the Assumption 2 (including (ii')). For infinite case, set \u00b5 = min\n2 \u221a 6L \u221a d , \u221a 6n 0 L(d+6) 3/2 , S 1 = 96d\u03c3 2 2 , S 2 = 30(2d+9)\u03c3 n 0 , q = 5n 0 \u03c3 , where n 0 \u2208 [1, 30(2d+9)\u03c3 ].\nIn the finite-sum case, set the parameters S 2 = (2d+9)n 1/2 n 0 , and q = n 0 n 1/2\n6 , let S 1 /d = [n], i.e. v k j = f (x k + \u00b5e j ) \u2212 f (x k )/\u00b5 with j \u2208 [d], where n 0 \u2208 [1, n 1/2 6 ]. Then with \u03b7 k = min( 1 2Ln 0 , Ln 0 v k ), K = (4L\u2206n 0 ) \u22122 + 1, for Algorithm 3 we have E [ \u2207f (x) ] \u2264 6 .\n(4.8)\n\nThe IZO calls are O d min(n 1/2 \u22122 , \u22123 ) .\n\nWe propose in this work the Spider method for non-convex optimization. Our Spider-type algorithms for first-order and zeroth-order optimization have update rules that are reasonably simple and achieve excellent convergence properties. However, there are still some important questions left. For example, the lower bound results for finding a second-order stationary point are not complete. Specially, it is not yet clear if our\u00d5( \u22123 ) for the on-line case and\u00d5(n 1/2 \u22122 ) for the finite-sum case gradient cost upper bound for finding a second-order stationary point (when n \u2265 \u2126( \u22121 )) is optimal or the gradient cost can be further improved, assuming both Lipschitz gradient and Lipschitz Hessian. We leave this as a future research direction. \n\n\nA Vector-Martingale Concentration Inequality\n\nIn this and next section, we sometimes denote for brevity that E k [\u00b7] = E[\u00b7 | x 0:k ], the expectation operator conditional on x 0:k , for an arbitrary k \u2265 0.\n\nConcentration Inequality for Vector-valued Martingales We apply a result by Pinelis (1994) and conclude Proposition 2 which is an Azuma-Hoeffding-type concentration inequality. See also Kallenberg & Sztencel (1991), Lemma 4.4 in Zhang (2005 or Theorem 2.1 in Zhang (2005) and the references therein.\n\nProposition 2 (Theorem 3.5 in Pinelis (1994)). Let 1:K \u2208 R d be a vector-valued martingale difference sequence with respect to F k , i.e., for each k = 1, . . . , K, E[ k | F k\u22121 ] = 0 and k 2 \u2264 B 2 k . We have\nP K k=1 k \u2265 \u03bb \u2264 4 exp \u2212 \u03bb 2 4 K k=1 B 2 k , (A.1)\nwhere \u03bb is an arbitrary real positive number.\n\nProposition 2 is not a straightforward derivation of one-dimensional Azuma's inequality. The key observation of Proposition 2 is that, the bound on the right hand of (A.1) is dimension-free (note the Euclidean norm version of R d is (2, 1)-smooth). Such dimension-free feature could be found as early as in Kallenberg & Sztencel (1991), uses the so-called dimension reduction lemma for Hilbert space which is inspired from its continuum version proved in Kallenberg & Sztencel (1991). Now, we are ready to prove Proposition 1.\n\n\nA.1 Proof of Proposition 1\n\nProof of Proposition 1. It is straightforward to verify from the definition ofQ in (2.1) that\nQ(x 0:K ) \u2212 Q(x K ) =Q(x 0 ) \u2212 Q(x 0 ) + K k=1 \u03be k (x 0:k ) \u2212 (Q(x k ) \u2212 Q(x k\u22121 ))\nis a martingale, and hence (2.2) follows from the property of L 2 martingales (Durrett, 2010).\n\n\nA.2 Proof of Lemma 1\n\nProof of Lemma 1. For any k > 0, we have from Proposition 1 (by applyingQ = V)\nE k V k \u2212 B(x k ) 2 = E k B S * (x k ) \u2212 B(x k ) \u2212 B S * (x k\u22121 ) + B(x k\u22121 ) 2 + V k\u22121 \u2212 B(x k\u22121 ) 2 . (A.2) Then E k B S * (x k ) \u2212 B(x k ) \u2212 B S * (x k\u22121 ) + B(x k\u22121 ) 2 a = 1 S * E B i (x k ) \u2212 B(x k ) \u2212 B i (x k\u22121 ) + B(x k\u22121 ) 2 b \u2264 1 S * E B i (x k ) \u2212 B i (x k\u22121 ) 2 (2.7) \u2264 1 S * L 2 B E x k \u2212 x k\u22121 2 \u2264 L 2 B 2 1 S * , (A.3)\nwhere in a = and b \u2264, we use Eq (2.6), and S * are random sampled from [n] with replacement. Combining (A.2) and (A.3), we have\nE k V k \u2212 B(x k ) 2 \u2264 L 2 B 2 1 S * + V k\u22121 \u2212 B(x k\u22121 ) 2 . (A.4)\nTelescoping the above display for k = k \u2212 1, . . . , 0 and using the iterated law of expectation, we have\nE V k \u2212 B(x k ) 2 \u2264 kL 2 B 2 1 S * + E V 0 \u2212 B(x 0 ) 2 . (A.5) B Deferred Proofs B.1 Proof of Lemma 2\nProof of Lemma 2. For k = k 0 , we have\nE k 0 v k 0 \u2212 \u2207f (x k 0 ) 2 = E k 0 \u2207f S 1 (x k 0 ) \u2212 \u2207f (x k 0 ) 2 \u2264 \u03c3 2 S 1 = 2 2 . (B.1)\nFrom Line 14 of Algorithm 1 we have for all k \u2265 0,\nx k+1 \u2212 x k = min Ln 0 v k , 1 2Ln 0 v k \u2264 Ln 0 . (B.2) Applying Lemma 1 with 1 = /(Ln 0 ), S 2 = 2\u03c3/( n 0 ), K = k \u2212 k 0 \u2264 q = \u03c3n 0 / , we have E k 0 v k \u2212 \u2207f (x k ) 2 \u2264 \u03c3n 0 L 2 \u00b7 2 L 2 n 2 0 \u00b7 n 0 2\u03c3 + E k 0 v k 0 \u2212 \u2207f (x k 0 ) 2 (B.1) = 2 , (B.3)\ncompleting the proof.\n\n\nB.2 Proof of Expectation Results for FSP\n\nThe rest of this section devotes to the proofs of Theorems 1, 2. To prepare for them, we first conclude via standard analysis the following Lemma 4. Under the Assumption 1, setting k 0 = k/q \u00b7 q, we have\nE k 0 f (x k+1 ) \u2212 f (x k ) \u2264 \u2212 4Ln 0 E k 0 v k + 3 2 4n 0 L . (B.4)\nProof of Lemma 4. From Assumption 1 (ii), we have\n\u2207f (x) \u2212 \u2207f (y) 2 = E i (\u2207f i (x) \u2212 \u2207f i (y)) 2 \u2264 E i \u2207f i (x) \u2212 \u2207f i (y) 2 \u2264 L 2 x \u2212 y 2 . (B.5) So f (x) has L-Lipschitz continuous gradient, then f (x k+1 ) \u2264 f (x k ) + \u2207f (x k ), x k+1 \u2212 x k + L 2 x k+1 \u2212 x k 2 = f (x k ) \u2212 \u03b7 k \u2207f (x k ), v k + L(\u03b7 k ) 2 2 v k 2 = f (x k ) \u2212 \u03b7 k 1 \u2212 \u03b7 k L 2 v k 2 \u2212 \u03b7 k \u2207f (x k ) \u2212 v k , v k a \u2264 f (x k ) \u2212 \u03b7 k 1 2 \u2212 \u03b7 k L 2 v k 2 + \u03b7 k 2 v k \u2212 \u2207f (x k ) 2 , (B.6)\nwhere in a \u2264, we applied Cauchy-Schwarz inequality. Since \u03b7 k = min Ln 0 v k , 1 2Ln 0 \u2264 1 2Ln 0 \u2264 1 2L , we have\n\u03b7 k 1 2 \u2212 \u03b7 k L 2 v k 2 \u2265 1 4 \u03b7 k v k 2 = 2 8n 0 L min 2 v k , v k 2 a \u2265 v k \u2212 2 2 4n 0 L , (B.7)\nwhere in a \u2265, we use V (x) = min |x|, x 2 2 \u2265 |x| \u2212 2 for all x. Hence\nf (x k+1 ) \u2264 f (x k ) \u2212 v k 4Ln 0 + 2 2n 0 L + \u03b7 k 2 v k \u2212 \u2207f (x k ) 2 \u03b7 k \u2264 1 2Ln 0 \u2264 f (x k ) \u2212 v k 4Ln 0 + 2 2n 0 L + 1 4Ln 0 v k \u2212 \u2207f (x k ) 2 . (B.8)\nTaking expectation on the above display and using Lemma 2, we have\nE k 0 f (x k+1 ) \u2212 E k 0 f (x k ) \u2264 \u2212 4Ln 0 E k 0 v k + 3 2 4Ln 0 . (B.9)\nThe proof is done via the following lemma:\n\nLemma 5. Under Assumption 1, for all k \u2265 0, we have\nE \u2207f (x k ) \u2264 E v k + . (B.10)\nProof. By taking the total expectation in Lemma 2, we have\nE v k \u2212 \u2207f (x k ) 2 \u2264 2 . (B.11)\nThen by Jensen's inequality\nE v k \u2212 \u2207f (x k ) 2 \u2264 E v k \u2212 \u2207f (x k ) 2 \u2264 2 .\nSo using triangle inequality\nE \u2207f (x k ) = E v k \u2212 (v k \u2212 \u2207f (x k )) \u2264 E v k + E v k \u2212 \u2207f (x k ) \u2264 E v k + . (B.12)\nThis completes our proof. Now, we are ready to prove Theorem 1.\n\nProof of Theorem 1. Taking full expectation on Lemma 4, and telescoping the results from k = 0 to K \u2212 1, we have\n4Ln 0 K\u22121 k=0 E v k \u2264 f (x 0 ) \u2212 Ef (x K ) + 3K 2 4Ln 0 Ef (x K )\u2265f * \u2264 \u2206 + 3K 2 4Ln 0 . (B.13)\nDiving 4Ln 0 K both sides of (B.13), and using K = 4L\u2206n 0 2 + 1 \u2265 4L\u2206n 0 2 , we have\n1 K K\u22121 k=0 E v k \u2264 \u2206 \u00b7 4Ln 0 1 K + 3 \u2264 4 . (B.14)\nThen from the choose ofx, we have\nE \u2207f (x) = 1 K K\u22121 k=0 E \u2207f (x k ) (B.10) \u2264 1 K K\u22121 k=0 E v k + (B.14) \u2264 5 . (B.15)\nTo compute the gradient cost, note in each q iterations we access for one time S 1 stochastic gradients and for q times of S 2 stochastic gradients, and hence the cost is\nK \u00b7 1 q S 1 + KS 2 S 1 =qS 2 \u2264 2K \u00b7 S 2 + S 1 \u2264 2 4Ln 0 \u2206 2 2\u03c3 n 0 + 2\u03c3 2 2 + 2S 2 = 16L\u03c3\u2206 3 + 2\u03c3 2 2 + 4\u03c3 n 0 . (B.16)\nThis concludes a gradient cost of 16L\u2206\u03c3 \u22123 + 2\u03c3 2 \u22122 + 4\u03c3n \u22121 0 \u22121 .\n\nProof of Theorem 2. For Lemma 2, we have\nE k 0 v k 0 \u2212 \u2207f (x k 0 ) 2 = E k 0 \u2207f (x k 0 ) \u2212 \u2207f (x k 0 ) 2 = 0. (B.17)\nWith the above display, applying Lemma 1 with 1 = Ln 0 , and S 2 = n 1/2 n 0 , K = k \u2212k 0 \u2264 q = n 0 n 1/2 , we have\nE k 0 v k 0 \u2212 \u2207f (x k 0 ) 2 \u2264 n 0 n 1/2 L 2 \u00b7 2 L 2 n 2 0 \u00b7 n 0 n 1/2 + E k 0 v k 0 \u2212 \u2207f (x k 0 ) 2 (B.1) = 2 . (B.18)\nSo Lemma 2 holds. Then from the same technique of on-line case, we can obtain (B.2) and (5), and (B.15). The gradient cost analysis is computed as:\nK \u00b7 1 q S 1 + KS 2 S 1 =qS 2 \u2264 2K + S 1 \u2264 2 4Ln 0 \u2206 2 n 1/2 n 0 + n + 2S 2 = 8(L\u2206) \u00b7 n 1/2 2 + n + 2n 1/2 n 0 . (B.19)\nThis concludes a gradient cost of n + 8(L\u2206) \u00b7 n 1/2 \u22122 + 2n \u22121 0 n 1/2 .\n\n\nB.3 Proof of High Probability Results for FSP\n\nSet K be the time when Algorithm 1 stops. We have K = 0 if v 0 < 2 , and K = inf{k \u2265 0 : v k < 2 } + 1 if v 0 \u2265 2 . It is a random stopping time. Let K 0 = 4L\u2206n 0 \u22122 + 2. We have the following lemma:\n\nLemma 6. Set the parameters S 1 , S 2 , \u03b7, and q as in Theorem 4. Then under the Assumption 2, for fixed K 0 , define the event:\nH K 0 = v k \u2212 \u2207f (x k ) 2 \u2264 \u00b7\u02dc , \u2200k \u2264 min(K, K 0 ) .\nwe have H K 0 occurs with probability at least 1 \u2212 p.\n\nProof of Lemma 6. Because when k \u2265 K, the algorithm has already stopped. So if K \u2264 k \u2264 K 0 , we can define a virtual update as x k+1 = x k , and v k is still generated by Line 3 and Line 5 in Algorithm 1. Then let the eventH k = v k \u2212 \u2207f (x k ) 2 \u2264 \u00b7\u02dc , with 0 \u2264 k \u2264 K 0 . We want to prove that for any k with 0 \u2264 k \u2264 K 0 ,H k occurs with probability at least 1 \u2212 p/(K 0 + 1). If so, using the fact that\nH K 0 \u2287 v k \u2212 \u2207f (x k ) 2 \u2264 \u00b7\u02dc , \u2200k \u2264 K 0 = K 0 k=0 (H k ), we have P(H K 0 ) \u2265 P K 0 k=0 (H k ) = P K 0 k=0 (H k ) c c \u2265 1 \u2212 K 0 k=0 P(H c k ) = 1 \u2212 p.\nWe prove thatH k occurs with probability 1 \u2212 p/(K 0 + 1) for any k with 0 \u2264 k \u2264 K 0 . Let \u03be k with k \u2265 0 denote the randomness in maintaining Spider v k at iteration k. And F k = \u03c3{\u03be 0 , \u00b7 \u00b7 \u00b7 \u03be k }, where \u03c3{\u00b7} denotes the sigma field. We know that x k and v k\u22121 are measurable on F k\u22121 .\n\nThen given F k\u22121 , if k = k/q q, we set\nk,i = 1 S 1 \u2207f S 1 (i) (x k ) \u2212 \u2207f (x k )\nwhere i is the index with S 1 (i) denoting the i-th random component function selected at iteration k and 1 \u2264 i \u2264 S 1 . We have\nE[ k,i |F k\u22121 ] = 0, k,i Assum.2(iii ) \u2264 \u03c3 S 1 .\nThen from Proposition 2, we have\nP v k \u2212 \u2207f (x k ) 2 \u2265 \u00b7\u02dc | F k\u22121 = P \uf8eb \uf8ed S 1 i=1 k,i 2 \u2265 \u00b7\u02dc | F k\u22121 \uf8f6 \uf8f8 \u2264 4 exp \uf8eb \uf8ed \u2212 \u00b7\u02dc 4S 1 \u03c3 2 S 2 1 \uf8f6 \uf8f8 S 1 = 2\u03c3 2 2 ,\u02dc =10 log(4(K 0 +1)/p) \u2264 p K 0 + 1 . (B.20) So P v k \u2212 \u2207f (x k ) 2 \u2265 \u00b7\u02dc \u2264 p K 0 +1 . When k = k/q q, set k 0 = k/q q, and j,i = 1 S 2 \u2207f S 2 (i) (x j ) \u2212 \u2207f S 2 (i) (x j\u22121 ) \u2212 \u2207f (x j ) + \u2207f (x j\u22121 )\nwhere i is the index with S 2 (i) denoting the i-th random component function selected at iteration k, 1 \u2264 i \u2264 S 2 and k 0 \u2264 j \u2264 k. We have\nE[ j,i |F j\u22121 ] = 0.\nFor any x and y, we have\n\u2207f (x) \u2212 \u2207f (y) = 1 n n i=1 (\u2207f i (x) \u2212 \u2207f i (y)) \u2264 1 n n i=1 \u2207f i (x) \u2212 \u2207f i (y) Assum.2 (ii ) \u2264 L x \u2212 y , (B.21)\nSo f (x) also have L-Lipschitz continuous gradient.\n\nThen from the update rule if k < K, we have\nx k+1 \u2212 x k = \u03b7(v k / v k ) = \u03b7 = Ln 0 , if k \u2265 K, we have x k+1 \u2212 x k = 0 \u2264 Ln 0 . We have j,i \u2264 1 S 2 \u2207f i (x j ) \u2212 \u2207f i (x j\u22121 ) + \u2207f (x j ) \u2212 \u2207f (x j\u22121 ) (B.21), Assum.2 (ii ) \u2264 2L S 2 x j \u2212 x j\u22121 \u2264 2 S 2 n 0 , (B.22)\nfor all k 0 < j \u2264 k and 1 \u2264 i \u2264 S 2 . On the other hand, we have\nv k \u2212 \u2207f (x k ) = \u2207f S 2 (x k ) \u2212 \u2207f S 2 (x k\u22121 ) \u2212 \u2207f (x k ) + \u2207f (x k\u22121 ) + (v k\u22121 \u2212 \u2207f (x k\u22121 )) = k j=k 0 +1 \u2207f S 2 (x k ) \u2212 \u2207f S 2 (x k\u22121 ) \u2212 \u2207f (x k ) + \u2207f (x k\u22121 ) + \u2207f S 1 (x k 0 ) \u2212 \u2207f (x k 0 ) = k j=k 0 +1 S 1 i=1 j,i + S 2 i=1 k 0 ,i . (B.23)\nPlugging (B.22) and (B.23) together, and using Proposition 2, we have\nP v k \u2212 \u2207f (x k ) 2 \u2265 \u00b7\u02dc | F k 0 \u22121 \u2264 4 exp \uf8eb \uf8ed \u2212 \u00b7\u02dc 4S 1 \u03c3 2 S 2 1 + 4S 2 (k \u2212 k 0 ) 4 2 S 2 2 n 2 0 \uf8f6 \uf8f8 \u2264 4 exp \uf8eb \uf8ed \u2212 \u00b7\u02dc 4S 1 \u03c3 2 S 2 1 + 4S 2 q 4 2 S 2 2 n 2 0 \uf8f6 \uf8f8 a = 4 exp \uf8eb \uf8ed \u2212 2 10 log(4(K 0 + 1)/p) 4\u03c3 2 2 2\u03c3 2 + 4 n 0 2\u03c3 \u03c3n 0 4 2 n 2 0 \uf8f6 \uf8f8 \u2264 p K 0 + 1 , (B.24)\nwhere in a =, we use S 1 = 2\u03c3 2 2 , S 2 = 2\u03c3 n 0 , and q = \u03c3n 0 . So P v k \u2212 \u2207f (x k ) 2 \u2265 \u00b7\u02dc \u2264 p K 0 +1 , which completes the proof.\n\nLemma 7. Under Assumption 2, we have that on\nH K 0 \u2229 (K > K 0 ), for all 0 \u2264 k \u2264 K 0 , f (x k+1 ) \u2212 f (x k ) \u2264 \u2212 \u00b7\u02dc 4Ln 0 . (B.25) and hence f (x K 0 +1 ) \u2212 f (x 0 ) \u2264 \u2212 \u00b7\u02dc 4Ln 0 \u00b7 (K 0 ).\nProof of Lemma 7. Let \u03b7 k := \u03b7/ v k . Since f has L-Lipschitz continuous gradient from (B.21), we have\nf (x k+1 ) (B.6) \u2264 f (x k ) \u2212 \u03b7 k 1 2 \u2212 \u03b7 k L 2 v k 2 + \u03b7 k 2 v k \u2212 \u2207f (x k ) 2 . (B.26)\nBecause we are on the event H K 0 \u2229 (K > K 0 ), so K \u2212 1 \u2265 K 0 , then for all 0 \u2264 k \u2264 K 0 , we have v k \u2265 2 , thus\n\u03b7 k = Ln 0 1 v k v k \u22652\u02dc \u22652 \u2264 1 2Ln 0 \u2264 1 2L , we have \u03b7 k 1 2 \u2212 \u03b7 k L 2 v k 2 \u2265 1 4 \u00b7 Ln 0 v k v k 2 v k \u22652\u02dc \u2265 \u00b7\u02dc 2Ln 0 , (B.27)\nand for H K 0 happens, we also have\n\u03b7 k 2 v k \u2212 \u2207f (x k ) 2 \u03b7 k \u2264 1 2Ln 0 \u2264 \u00b7\u02dc 4Ln 0 . Hence f (x k+1 ) \u2264 f (x k ) \u2212 \u00b7\u02dc 2Ln 0 + \u03b7 k 2 v k \u2212 \u2207f (x k ) 2 \u2264 f (x k ) \u2212 \u00b7\u02dc 4Ln 0 , (B.28)\nBy telescoping (B.28) from 0 to K 0 , we have\nf (x K 0 +1 ) \u2212 f (x 0 ) \u2264 \u2212 \u00b7\u02dc 4Ln 0 \u00b7 (K 0 ).\nNow, we are ready to prove Theorem 4.\n\nProof of Theorem 4. We only want to prove\n(K \u2264 K 0 ) \u2287 H K 0 , so if H K 0 occurs, we have K \u2264 K 0 , and v K \u2264 2\u02dc . Because v K \u2212 \u2207f (x K ) \u2264 \u221a \u00b7\u02dc \u2264\u02dc occurs in H K 0 , so \u2207f (x K ) \u2264 3\u02dc . If (K > K 0 ) and H K 0 occur, plugging in K 0 = 4L\u2206n 0 2 + 2 \u2265 4L\u2206n 0 2 + 1 \u2265 4L\u2206n 0\n\u00b7\u02dc + 1, then from Lemma 7 at each iteration the function value descends by at least \u00b7\u02dc /(4Ln 0 ), We thus have\n\u2212\u2206 \u2264 f * \u2212 f (x 0 ) \u2264 f (x K 0 ) \u2212 f (x 0 ) \u2264 \u2212 \u2206 + \u00b7\u02dc 4Ln 0 ,\ncontradicting the fact that \u2212\u2206 > \u2212 \u2206 + \u00b7\u02dc 4Ln 0 . This indicates (K \u2264 K 0 ) \u2287 H K 0 . From Lemma 6, with probability 1 \u2212 p, H K 0 occurs, and then v K \u2264 2\u02dc and \u2207f (x K ) \u2264 3\u02dc .\n\nThen gradient cost can be bounded by the same way in Theorem 2 as:\nK 0 \u00b7 1 q S 1 + K 0 S 2 S 1 =qS 2 \u2264 2K 0 \u00b7 S 2 + S 1 \u2264 2 \u2206 2 /(4Ln 0 ) \u00b7 S 2 + S 1 + 4S 2 \u2264 2 4Ln 0 \u2206 2 2\u03c3 n 0 + 2\u03c3 2 2 + 8\u03c3 n 0 = 16L\u03c3\u2206 3 + 2\u03c3 2 2 + 8\u03c3 n 0 . (B.29)\nProof of Theorem 5. We first verify thatH k = v k \u2212 \u2207f (x k ) 2 \u2264 \u00b7\u02dc with 0 \u2264 k \u2264 K 0 occurs with probability 1 \u2212 \u03b4/(K 0 + 1) for any k. When k = k/q q, we have v k = \u2207f (x k ). When k = k/q q, set k 0 = k/q q, and\nj,i = 1 S 2 \u2207f S 2 (i) (x j ) \u2212 f S 2 (i) (x j\u22121 ) \u2212 \u2207f (x j ) + \u2207f (x j\u22121 )\nwhere i is the index with S 2 (i) denoting the i-th random component function selected at iteration k, from (B.22), we have\nE j,i |F j\u22121 = 0, j,i \u2264 2 S 2 n 0 ,\nfor all k 0 < j \u2264 k and 1 \u2264 i \u2264 S 2 . On the other hand\nv k \u2212 \u2207f (x k ) = \u2207f S 2 (x k ) \u2212 \u2207f S 2 (x k\u22121 ) \u2212 \u2207f (x k ) + \u2207f (x k\u22121 ) + (v k\u22121 \u2212 \u2207f (x k\u22121 ) = k j=k 0 +1 \u2207f S 2 (x k ) \u2212 \u2207f S 2 (x k\u22121 ) \u2212 \u2207f (x k ) + \u2207f (x k\u22121 ) + \u2207f S 1 (x k 0 ) \u2212 \u2207f (x k 0 ) = k j=k 0 +1 S 1 i=1 j,i . (B.30)\nThen from Proposition 2, we have where in a =, we use S 2 = n 1/2 /n 0 , and q = n 0 n 1/2 . So P v k \u2212 \u2207f (x k ) 2 \u2265 \u00b7\u02dc \u2264 p K 0 +1 , which completes the proof.\nP v k \u2212 \u2207f (x k ) 2 \u2265 \u00b7\u02dc | F k 0 \u22121 \u2264 4 exp \uf8eb \uf8ed \u2212 \u00b7\u02dc 4S 2 (k \u2212 k 0 ) 4 2 S 2 2 n 2 0 \uf8f6 \uf8f8 \u2264 4 exp \uf8eb \uf8ed \u2212 \u00b7\u02dc 4S 2 q 4 2\nThus Lemma 2 holds. Then using the same technique of Lemma 7 and Theorem 4, we have (K \u2264 K 0 ) \u2287 H K 0 . With probability at least 1\u2212p, H K 0 occurs, and v K \u2264 2\u02dc and \u2207f (x K ) \u2264 3\u02dc .\nK 0 \u00b7 1 q S 1 + K 0 S 2 S 1 =qS 2 \u2264 2K 0 \u00b7 S 2 + S 1 \u2264 2 \u2206 2 /(4Ln 0 ) \u00b7 S 2 + S 1 + 4S 2 = 2 4Ln 0 \u2206 2 n 1/2 n 0 + n + 4n \u22121 0 n 1/2 = 8(L\u2206) \u00b7 n 1/2 2 + n + 4n \u22121 0 n 1/2 . (B.32)\n\nB.4 Proof of Theorem 6 for SSP\n\nWe first restate the Neon result in Allen-Zhu & Li (2018) for NC-search in the following Theorem:\n\nTheorem 9 (Theorem 1 in Allen-Zhu & Li (2018), Neon2 (on-line)). Under the Assumption 2 (including (ii')), for every point x 0 \u2208 R d , for every \u03b4 \u2208 (0, L], and every p \u2208 (0, 1), the Neon2\n(NC search) output w = Neon2(f, x 0 , \u03b4, p)\nsatisfies that, with probability at least 1 \u2212 p:\n1. if w = \u22a5, then \u2207f 2 (x 0 ) \u2212\u03b4I. 2. if w = \u22a5, then w 2 = 1, and w T \u2207 2 f (x 0 )w \u2264 \u03b4 2 .\nMoreover, the total number of stochastic gradient evaluations are O log 2 ((d/p))L 2 \u03b4 \u22122 .\n\nOne can refer to Allen-Zhu & Li (2018) for more details. Now we prove Theorem 6: From Algorithm 2, we can find that all the randomness in iteration k come from 3 parts: 1) maintaining Spider v k (Line 7-11 and 17-21); 2) to conducting NC-search in Line 2 (if mod(k, K ) = 0); 3) choosing a random direction to update x k in Line 5 (if Algorithm 2 performs first-order updates). We denote the randomness from the three parts as \u03be 1 k , \u03be 2 k , \u03be 3 k , respectively. Let F k be the filtration involving the full information of x 0:k , v 0:k , i.e F k = \u03c3 \u03be 1 0:k , \u03be 2 0:k , \u03be 3 0:k\u22121 . So the randomness in iteration k given F k only comes from \u03be 3 k (choosing a random direction in Line 5). Let the random index I k = 1, if Algorithm 2 plans to perform the first-order update, I k = 2, if it plans to perform the second-order update, we know that I k is measurable on F k/K K and also on F k . Because the algorithm shall be stopped if it finds v k \u2265 2\u02dc when it plans to do first-order descent, we can define a virtual update as x k+1 = x k in Line 12 and 25, with others unchanged if the algorithm has stopped. Let H k 1 denotes the event that algorithm has not stopped before k, i.e.\nH k 1 = k i=0 v k \u2265 2\u02dc \u2229 I k = 1 I k = 2 , we have H k 1 \u2208 F k , and H 1 1 \u2287 H 2 1 \u2287 \u00b7 \u00b7 \u00b7 \u2287 H k 1 . Let H K j 2\ndenotes the event that the NC-search in iteration K j runs successfully. And H k 3 denotes the event that\nH k 3 = k i=0 v i \u2212 \u2207f (x i ) 2 \u2264 \u00b7\u02dc \uf8eb \uf8ed k/K j=0 H j\u00b7K 2 \uf8f6 \uf8f8 .\nWe know that H k 3 \u2208 F k , and H 1 3 \u2287 H 2 3 \u2287 \u00b7 \u00b7 \u00b7 \u2287 H k 3 . And if H k 3 happens, all NC-search before iteration k run successfully and v i \u2212 \u2207f (x i ) 2 \u2264 \u00b7\u02dc for all 0 \u2264 i \u2264 k.\n\nLemma 8. With the setting of Theorem 6, and under the Assumption 3, we have\nP H K 0 3 \u2265 7 8 . (B.33) Proof. Let eventH k = v k \u2212 \u2207f (x k ) 2 \u2264 \u00b7\u02dc , with 0 \u2264 k \u2264 K 0 . Once we prove thatH k occurs with probability at least 1 \u2212 1 16(K 0 +1) , we have P K 0 i=0 v i \u2212 \u2207f (x i ) 2 \u2264 \u00b7\u02dc \u2265 1 \u2212 1 16 .\nOn the other hand, from Theorem 9, we know each time the NC-search conducts successfully at probability 1 \u2212 1 16J , so P(H K 0 2 ) \u2265 1 \u2212 1 16 . Combining the above results, we obtain P H K 0 3 \u2265 7 8 . To prove P(H k ) \u2265 1 \u2212 1 16(K 0 +1) , consider the filtration F k 2 = \u03c3 \u03be 1 0:k\u22121 , \u03be 2 0:k , \u00b7 \u00b7 \u00b7 , \u03be 3 0:k\u22121 , which involves the full information of x 0:k . We know x k is measurable on F k 2 . Given F k 2 , we have\nE i \u2207f i (x k ) \u2212 \u2207f (x k ) | F k 2 = 0 when mod(k, p) = 0. For mod(k, p) = 0, we have E i \u2207f i (x k ) \u2212 \u2207f i (x k\u22121 ) \u2212 \u2207f (x k ) \u2212 \u2207f (x k\u22121 ) | F k 2 = 0.\nBecause x k is generated by one of the three ways:\n\n1. Algorithm 2 performs First-order descent, we have\nx k \u2212 x k\u22121 = \u03b7(v k\u22121 / v k\u22121 ) = \u03b7 = Ln 0 .\n2. Algorithm 2 performs Second-order descent, we have x k \u2212 x k\u22121 = \u03b7 = Ln 0 .\n\n3. Algorithm 2 has already stopped. x k \u2212 x k\u22121 = 0 \u2264 Ln 0 .\n\nSo v k \u2212 \u2207f (x k ) is martingale, and the second moment of its difference is bounded by Ln 0 . We can find that the parameters S 1 , S 2 , \u03b7, with\u02dc = 16 log(64(K 0 + 1)) for on-line case and\u02dc = 10 log(64(K 0 + 1)) for off-line case are set as the same in Lemma 6 with p = 1 16 . Thus using the same technique of Lemma 6, we can obtain P(\nH k ) \u2265 1 \u2212 1 16(K 0 +1) for all 0 \u2264 k \u2264 K 0 . Let H k 4 = H k 1 \u2229 H k 3 .\nWe show that Theorem 6 is essentially to measure the probability of the event that\nH K 0 1 c H K 0 3 . Lemma 9. If H K 0 1 c H K 0 3\nhappens, Algorithm 2 outputs an x k satisfying (3.12) before K 0 iterations.\n\nProof. Because H K 0 1 c happens, we know that Algorithm 2 has already stopped before K 0 and\noutput x k with v k \u2264 2\u02dc . For H K 0 3 happens, we have \u2207f (x k ) \u2212 v k \u2264 \u221a \u00b7\u02dc \u2264\u02dc . So \u2207f (x k ) \u2264 3\u02dc . Set k 0 = k/K K .\nSince the NC-search conducts successfully, from Theorem 9, we have \u03bb min \u2207f 2 (x k 0 ) \u2265 \u22122\u03b4I. From Assumption 2, we have\n\u2207f 2 (x) \u2212 \u2207f 2 (y) 2 \u2264 1 n n i=1 \u2207f 2 i (x) \u2212 \u2207f 2 i (y) 2 \u2264 1 n n i=1 \u2207f 2 i (x) \u2212 \u2207f 2 i (y) 2 \u2264 \u03c1 x \u2212 y .(B.34)\nSo f (\u00b7) has \u03c1-Lipschitz Hessian. We have\n\u2207f 2 (x k ) \u2212 \u2207f 2 (x k 0 ) 2 \u2264 k\u22121 i=k 0 \u2207f 2 (x i+1 ) \u2212 \u2207f 2 (x i ) 2 \u2264 k\u22121 i=k 0 \u03c1 x i+1 \u2212 x i 2 \u2264 K \u03c1 Ln 0 K = \u03b4Ln 0 \u03c1 = \u03b4. (B.35)\nThus \u03bb min (\u2207f 2 (x k )) \u2265 \u22123\u03b4I.\n\nNow, we are ready to prove Theorem 6.\n\nProof of Theorem 6. For all iteration K with mod(K, K ) = 0, given F K , we consider the case when I K = 2 and H K 4 happens. Because f (\u00b7) has \u03c1-Lipschitz Hessian, we have\nf x K+K \u2264 f x K + \u2207f x K T x K+K \u2212 x K + 1 2 x K+K \u2212 x K T \u2207 2 f x K x K+K \u2212 x K + \u03c1 6 x K+K \u2212 x K 3 . (B.36)\nBecause H K 4 happens, and I K = 2, we have w T 1 \u2207f 2 (x K ) w 1 \u2264 \u2212\u03b4, and by taking expectation on the random number of the sign, we have\nE \u2207f x K T x K+K \u2212 x K | F K = 0, thus E f x K+K | F K \u2264 f x K \u2212 \u03b4 3 2\u03c1 2 + \u03b4 3 6\u03c1 2 = f x K \u2212 \u03b4 3 3\u03c1 2 .\n(B.37) Furthermore, by analyzing the difference of f\nx K \u2212 f * 1 H K 4 , where 1 H K 4 is the indication function for the event H K 4 , we have E f x K+K \u2212 f * 1 H K+K 4 | F K \u2212 f x K \u2212 f * 1 H K 4 | F K = E f x K+K \u2212 f * 1 H K+K 4 \u2212 1 H K 4 | F K + E f x K+K \u2212 f x K 1 H K 4 | F K a \u2264 P H K 4 | F K E f x K+K \u2212 f x K |H K 4 \u2229 F K (B.37) \u2264 \u2212P H K 4 | F K \u03b4 3 3\u03c1 2 , (B.38) where in a \u2264, we use that H K 4 \u2287 H K+K 4 , so 1 H K+K 4 \u2212 1 H K 4 \u2264 0 and f x K+K \u2212 f * \u2265 0, then E f x K+K \u2212 f * 1 H K+K 4 \u2212 1 H K 4 | F K \u2264 0.\nOn the other hand, given F K , we consider the case when I K = 1 and H K 4 happens, then for any k satisfying K \u2264 k < K + K , we know I k = 1.\n\nGiven F k with K \u2264 k < K + K , then from (B.26) we have\nf x k+1 \u2264 f x k \u2212 \u03b7 k 1 2 \u2212 \u03b7 k L 2 v k 2 + \u03b7 k 2 v k \u2212 \u2207f x k 2 , (B.39)\nwith \u03b7 k = \u03b7/ v k . Also H K 4 is measurable on F k , and if H K 4 happens, we have v k \u2265 2\u02dc , and v k \u2212 \u2207f x k 2 \u2264 \u00b7\u02dc , then from (B.27) and (B.28), we have\nf x k+1 \u2264 f x k \u2212 \u00b7\u02dc 4Ln 0 . (B.40)\nTaking expectation up to F K , we have \nE f x k+1 \u2212 f x k | F K \u2229 H k 4 \u2264 \u2212 \u00b7\u02dc 4Ln 0 . (B.41) By analyzing the difference of f x k \u2212 f * 1 H k 4 , we have E f x k+1 \u2212 f * 1 H k+1 4 | F K \u2212 f x k \u2212 f * 1 H k 4 | F K = E f x k+1 \u2212 f * 1 H k+1 4 \u2212 1 H k 4 | F K + E f x k+1 \u2212 f x k 1 H k 4 | F K a \u2264 P H k 4 | F K E f x k+1 \u2212 f x k | H k 4 \u2229 F K \u2264 \u2212P H k 4 | F K \u00b7\u02dc 4Ln 0 , (B.42) where in a \u2264, we use 1 H k+1 4 \u2212 1 H k 4 \u2264 0 and f x k+1 \u2212 f * \u2265 0. By telescoping (B.42) with k from K to K + K \u2212 1, we have E f x K+K \u2212 f * 1 H K+K 4 | F K \u2212 f x K \u2212 f * 1 H k 4 | F K \u2264 \u2212 \u00b7\u02dc 4Ln 0 K+K i=K P H i 4 | F k a \u2264 \u2212P H K+K 4 | F K K 2 4Ln 0 K = \u03b4Ln 0 \u03c1 = \u2212P H K+K 4 | F K \u03b4 4\u03c1 . (B.| F k \u2265 P H K+K 4 | F K , we have E f x K+K \u2212 f * 1 H K+K 4 | F K \u2212 f x K \u2212 f * 1 H k 4 | F K \u2264 \u2212P H K+K 4 | F K min \u03b4\u02dc 4\u03c1 , \u03b4 3 3\u03c1 2 . (B.44)\nBy taking full expectation on (B.44), and telescoping the results with K = 0, K , \u00b7 \u00b7 \u00b7 , (J \u2212 1)K , and using P H K j 4 \u2264 P H JK 4 with j = 1, \u00b7 \u00b7 \u00b7 , J, we have\nE f x JK \u2212 f * 1 H JK 4 \u2212 f x 0 \u2212 f * 1 H 0 4 \u2264 \u2212P H JK 4 min \u03b4\u02dc 4\u03c1 , \u03b4 3 3\u03c1 2 J. (B.45) Substituting the inequalities f x JK \u2212f * \u2265 0, f x 0 \u2212f * \u2264 \u2206, and J = 4 max 3\u03c1 2 \u2206 \u03b4 3 , 4\u2206\u03c1 \u03b4 +4 \u2265 4\u2206 min \u03b4\u02dc 4\u03c1 , \u03b4 3 3\u03c1 2 into (B.45), we have P H K 0 4 \u2264 1 4 . (B.46)\nThen by union bound, we have\nP H K 0 1 = P H K 0 1 H K 0 3 + P H K 0 1 H K 0 3 c \u2264 P H K 0 4 + P H K 0 3 c Lemma 8 \u2264 1 4 + 1 8 . (B.47)\nThen our proof is completed by obtaining\nP H K 0 1 c H K 0 3 = 1 \u2212 P H K 0 1 H K 0 3 c \u2265 1 \u2212 P H K 0 1 \u2212 P H K 0 3 c Lemma 8 \u2265 1 2 . (B.48)\nFrom Lemma 9, with probability 1/2, the algorithm shall be terminated before K J iterations, and output a x k satisfying (3.12). The total stochastic gradient complexity consists of two parts: the Spider maintenance cost and NC-Search cost. We estimate them as follows:\n\n1. With probability 1/2, the algorithm ends in at most K 0 iterations, thus the number of stochastic gradient accesses to maintain Spider can be bounded by\nK 0 /q qS 1 + S 2 S 1 =qS 2 \u2264 2K 0 S 2 + S 1 \u2264 4 max 3\u03c1 2 \u2206 \u03b4 3 , 4\u2206\u03c1 \u03b4 + 4 (2S 2 K ) + S 1 . (B.49)\n2. With probability 1/2, the algorithm ends in K 0 iterations, thus there are at most J times of NC search. From Theorem 9, suppose the NC-search costsCL 2 \u03b4 \u22122 , whereC hides a polylogarithmic factor of d. The stochastic gradient access for NC-Search is less than:\nJL 2C \u03b4 \u22122 = 4 max 3\u03c1 2 \u2206 \u03b4 3 , 4\u2206\u03c1 \u03b4 + 4 C L 2 \u03b4 \u22122 , (B.50)\nBy summing (B.49) and (B.50), using max a, b \u2264 a + b with a \u2265 0 and b \u2265 0, we have that the total stochastic gradient complexity can be bounded:\n4 3\u03c1 2 \u2206 \u03b4 3 + 4\u2206\u03c1 \u03b4 + 2 2S 2 K +CL 2 \u03b4 \u22122 + S 1 .\nFor the on-line case, plugging into K = \u03b4Ln 0 \u03c1 , S 1 = 2\u03c3 2 , and S 2 = 2\u03c3 n 0 , the stochastic gradient complexity can be bounded:\n64\u2206L\u03c3 3 + 48\u2206\u03c3L\u03c1 2 \u03b4 2 + 12C\u2206L 2 \u03c1 2 \u03b4 5 + 16C\u2206L 2 \u03c1 \u03b4 3 + 2\u03c3 2 2 + 8CL 2 \u03b4 2 + 32L\u03c3\u03b4 \u03c1 2 .\nFor the off-line case, plugging into S 2 = n 1/2 n 0 , we obtain the stochastic gradient complexity is bounded by:\n32\u2206Ln 1/2 2 + 12\u2206\u03c1Ln 1/2 \u03b4 2 + 12C\u2206L 2 \u03c1 2 \u03b4 5 + 16C\u2206L 2 \u03c1 \u03b4 3 + n + 8CL 2 \u03b4 2 + 16Ln 1/2 \u03b4 \u03c1 .\n\nB.5 Proof for SZO\n\nProof of Lemma 3. We have that\nE i,u f i (x + \u00b5u) \u2212 f i (x) \u00b5 u \u2212 f i (y + \u00b5u) \u2212 f i (y) \u00b5 u 2 = E i,u \u2207f i (x) \u2212 \u2207f i (y), u u + f i (x + \u00b5u) \u2212 f i (x) \u2212 \u2207f i (x), \u00b5u \u00b5 u \u2212 f i (y + \u00b5u) \u2212 f i (y) \u2212 \u2207f i (y), \u00b5u \u00b5 u 2 \u2264 2E i,u \u2207f i (x) \u2212 \u2207f i (y), u u 2 +2E i,u f i (x + \u00b5u) \u2212 f i (x) \u2212 \u2207f i (x), \u00b5u \u00b5 u \u2212 f i (y + \u00b5u) \u2212 f i (y) \u2212 \u2207f i (y), \u00b5u \u00b5 u 2 \u2264 2E i,u \u2207f i (x) \u2212 \u2207f i (y), u u 2 + 4E i,u f i (x + \u00b5u) \u2212 f i (x) \u2212 \u2207f i (x), \u00b5u \u00b5 2 u 2 +4E i,u f i (y + \u00b5u) \u2212 f i (y) \u2212 \u2207f i (y), \u00b5u \u00b5 2 u 2 a \u2264 2E i,u \u2207f i (x) \u2212 \u2207f i (y), u u 2 + 8 \u00b5 2 L 2 4 E u u 6 b \u2264 2(d + 4)E i \u2207f i (x) \u2212 \u2207f i (y) 2 + 2\u00b5 2 L 2 E u u 6 \u2264 2(d + 4)E i \u2207f i (x) \u2212 \u2207f i (y) 2 + 2\u00b5 2 L 2 E u u 6 c \u2264 2(d + 4)E i \u2207f i (x) \u2212 \u2207f i (y) 2 + 2\u00b5 2 (d + 6) 3 L 2 \u2264 2(d + 4)L 2 x \u2212 y 2 + 2\u00b5 2 (d + 6) 3 L 2 , (B.51)\nwhere in a \u2264 we use\n|f i (a) \u2212 f i (b) \u2212 \u2207f i (a), a \u2212 b | \u2264 L 2 a \u2212 b 2 ,\nbecause f i has L-Lipschitz continuous gradient ( (6)  Lemma 10. Under the Assumption 2 (including (ii')), if k/q q = k, given x k , we have\nE v k \u2212 \u2207f (x k ) 2 \u2264 2 4 . (B.52)\nProof. Let E k denote that the expectation is taken on the random number at iteration k given the full information of x 0:k . Denote \u2207 j f (x) as the value in the j-th coordinate of \u2207f (x), we have that\nE k v k \u2212 \u2207f (x k ) 2 = E k j\u2208[d] v k j \u2212 \u2207 j f (x k ) 2 = j\u2208[d] E k 1 S 1 i\u2208S 1 f i (x k + \u00b5e j ) \u2212 f i (x k ) \u00b5 \u2212 \u2207 j f (x k ) 2 \u2264 2 j\u2208[d] E k 1 S 1 i\u2208S 1 f i (x k + \u00b5e j ) \u2212 f i (x k ) \u00b5 \u2212 1 S 1 i\u2208S 1 \u2207 j f i (x k ) 2 + 2 j\u2208[d] E k 1 S 1 i\u2208S 1 \u2207 j f i (x k ) \u2212 \u2207 j f (x k ) 2 a \u2264 2 S 1 j\u2208[d] E k f i (x k + \u00b5e j ) \u2212 f i (x k ) \u00b5 \u2212 \u2207 j f i (x k ) 2 + 2 j\u2208[d] E k 1 S 1 i\u2208S 1 \u2207 j f i (x k ) \u2212 \u2207 j f (x k ) 2 , (B.53)\nwhere in a \u2264, we use |a 1 + a 2 + \u00b7 \u00b7 \u00b7 + a s | 2 \u2264 s|a 1 | 2 + s|a 2 | 2 + \u00b7 \u00b7 \u00b7 + s|a s | 2 . For the first term in the right hand of (B.53), because f i (x) has L-Lipschitz continuous gradient, we have\nf i (x k + \u00b5e j ) \u2212 f i (x k ) \u00b5 \u2212 \u2207 j f i (x k ) = 1 \u00b5 f i (x k + \u00b5e j ) \u2212 f i (x k ) \u2212 \u2207f i (x k ), \u00b5e j \u2264 1 \u00b5 L 2 \u00b5e j 2 = L\u00b5 2 . (B.54)\nThus we have\nE k v k \u2212 \u2207f (x k ) 2 (B.55) \u2264 dL 2 \u00b5 2 2 + 2 j\u2208[d] E k 1 S 1 i\u2208S 1 \u2207 j f i (x k ) \u2212 \u2207 j f (x k ) 2 = dL 2 \u00b5 2 2 + 2E k 1 S 1 i\u2208S 1 \u2207f i (x k ) \u2212 \u2207f (x k ) 2 (B.56)\nFor the on-line case, due to \u00b5 \u2264 2 \u221a 6L \u221a d , and S 1 = 96d\u03c3 2 2 , we have\nE k v k \u2212 \u2207f (x k ) 2 \u2264 dL 2 2 48L 2 d + 2 S 1 \u03c3 2 \u2264 2 24 . (B.57)\nIn finite-sum case, we have E k\n1 S 1 i\u2208S 1 \u2207f i (x k ) \u2212 \u2207f (x k ) 2 = 0, so E k v k \u2212 \u2207f (x k ) 2 \u2264 2 24 .\nAlso from (4.3), and \u00b5 \u2264 \u221a 6n 0 L(d+6) 3/2 , we have\n\u2207f (x k ) \u2212 \u2207f (x k ) 2 \u2264 \u00b5 2 L 2 (d + 3) 3 4 \u2264 2 6L 2 (d + 6) 3 L 2 (d + 3) 3 4 \u2264 2 24 . (B.58)\nWe have\nE k v k \u2212 \u2207f (x k ) 2 \u2264 2 v k \u2212 \u2207f (x k ) 2 + 2 \u2207f (x k ) \u2212 \u2207f (x k ) 2 \u2264 2 6 . (B.59)\nLemma 11. From the setting of Theorem 8, and under the Assumption 2 (including (ii')), for\nk 0 = k/q \u00b7 q, we have E k 0 v k \u2212 \u2207f (x k ) \u2264 2 .\nProof. For k = k 0 , from Lemma 10, we obtain the result. When k \u2265 k 0 , from Lemma 3, we have that\nE S 2 1 S 2 (i,u)\u2208S 2 f i (x k + \u00b5u k ) \u2212 f i (x k ) \u00b5 u \u2212 f i (x k\u22121 + \u00b5u k\u22121 ) \u2212 f i (x k\u22121 ) \u00b5 u \u2212 (f (x k ) \u2212f (x k\u22121 )) 2 = 1 S 2 E i,u f i (x k + \u00b5u k ) \u2212 f i (x k ) \u00b5 u \u2212 f i (x k\u22121 + \u00b5u k\u22121 ) \u2212 f i (x k\u22121 ) \u00b5 u \u2212 (f (x k ) \u2212f (x k\u22121 )) 2 (4.7) \u2264 1 S 2 2(d + 4)L 2 x k \u2212 x k\u22121 2 + 2\u00b5 2 (d + 6) 3 L 2 \u2264 1 S 2 2(d + 4)L 2 \u03b7 k v k 2 + 2\u00b5 2 (d + 6) 3 L 2 \u03b7 k \u2264 Ln 0 v k \u2264 1 S 2 2(d + 4)L 2 2 L 2 n 2 0 + 2(d + 6) 3 L 2 2 6n 2 0 L 2 (d + 6) 3 = 1 S 2 (2d + 9) 2 n 2 0 .\n(B.60)\n\nUsing Proposition 1, for on-line case, we have\nE k 0 v k \u2212 \u2207f (x k ) 2 S 2 = 30(2d+9)\u03c3 n 0 \u2264 2 6 + k j=k 0 3 30n 0 \u03c3 q= 5n 0 \u03c3 \u2264 2 3 . (B.61)\nfor finite-sum case, we have Proof of Theorem 8. By taking full expectation on Lemma 11, we have\nE k 0 v k \u2212 \u2207f (x k ) 2 S 2 =(E v k \u2212 \u2207f (x k ) 2 \u2264 2 3 . (B.63) Thus E k v k \u2212 \u2207f (x k ) 2 \u2264 2 v k \u2212 \u2207f (x k ) 2 + 2 \u2207f (x k ) \u2212 \u2207f (x k ) 2 (B.58) \u2264 2 . (B.64)\nBy using Lemma 4, (B.13), and (B.14), we have\n1 K K\u22121 k=0 E v k \u2264 \u2206 \u00b7 4Ln 0 1 K + 3 \u2264 4 . (B.65)\nOne the other hand, by Jensen's inequality, we have\n(E v k \u2212 \u2207f (x k ) ) 2 = E v k \u2212 \u2207f (x k ) 2 \u2212 E v k \u2212 \u2207f (x k ) \u2212 E(v k \u2212 \u2207f (x k )) 2 \u2264 2 . So E \u2207f (x k ) = E v k \u2212 (v k \u2212 \u2207f (x k )) + \u2207f (x k ) \u2212 \u2207f (x k ) \u2264 E v k + E v k \u2212 \u2207f (x k ) + E \u2207f (x k ) \u2212 \u2207f (x k ) (B.58) \u2264 E v k + + 2 \u221a 6 \u2264 E v k + 2 . (B.66)\nWe have\nE \u2207f (x) = 1 K K\u22121 k=0 E \u2207f (x k ) (B.66) \u2264 1 K K\u22121 k=0 E v k + 2 (B.65) \u2264 6 . (B.67)\n\nB.6 Proof of Theorem 3 for Lower Bound\n\nOur proof is a direct extension of Carmon et al. (2017b). Before we drill into the proof of Theorem 3, we first introduce the hard instancef K with K \u2265 1 constructed by Carmon et al. (2017b).f \nK (x) := \u2212\u03a8(1)\u03a6(x 1 ) + K i=2 [\u03a8(\u2212x i\u22121 )\u03a6(\u2212x i ) \u2212 \u03a8(x i\u22121 )\u03a6(x i )] ,(i \u2208 [d], \u2207 i f (x) = 0 whenever x i\u22121 = x i = x i+1 .\nSo any deterministic algorithm can only recover \"one\" dimension in each iteration (Carmon et al., 2017b). In addition, it satisfies that : If |x i | \u2264 1 for any i \u2264 K,\n\u2207f K (x) \u2265 1. (B.71)\nThen to handle random algorithms, Carmon et al. (2017b) further consider the following extensions: (ii)f K,B K (x) has constant l (independent of K and d) Lipschitz continuous gradient.\nf K,B K (x) =f K (B K ) T \u03c1(x) + 1 10 x 2 =f K b (1) ,\n(iii) if d \u2265 52\u00b7230 2 K 2 log( 2K 2 p ), for any algorithm A solving (1.2) with n = 1, and f (x) =f K,B K (x), then with probability 1 \u2212 p, (to ensure K \u2265 1), b = l L , and R = \u221a 230K. We first verify that f (x) satisfies Assumption 1 (i). For Assumption 1 (i), from (B.73), we have\nf (0) \u2212 inf x\u2208R d f (x) \u2264 1 n n i=1 (f i (0) \u2212 inf x\u2208R d f i (x)\n) \u2264 ln 1/2 2 L 12K = ln 1/2 2 L 12\u2206L 12ln 1/2 2 = \u2206 13 .\n\nFor Assumption 1(ii), for any i, using thef K,B K i has l-Lipschitz continuous gradient, we have\n\u2207f K,B K i (C T i x/b) \u2212 \u2207f K,B K i (C T i y/b) 2 \u2264 l 2 C T i (x \u2212 y)/b 2 , (B.77) Because \u2207f i (x) \u2212 \u2207f i (y) 2 = ln 1/2 2 Lb C i \u2207f K,B K i (C T i x/b) \u2212 \u2207f K,B K i (C T i y/b) 2\n, and using C i C i = I d/n , we have\n\u2207f i (x) \u2212 \u2207f i (y) 2 \u2264 ln 1/2 2 L 2 l 2 b 4 C T i (x \u2212 y) 2 = nL 2 C T i (x \u2212 y) 2 , (B.78)\nwhere we use b = l L . Summing i = 1, . . . , n and using each C i are orthogonal matrix, we have\nE \u2207f i (x) \u2212 \u2207f i (y) 2 \u2264 L 2 x \u2212 y 2 . (B.79)\nThen with d \u2265 2 max(9n 3 K 2 , 12n 2 KR 2 ) log 2n 3 K 2 p + n 2 K \u223c O n 2 \u2206 2 L 2 4 log n 2 \u2206 2 L 2 4 p , i \u2208 [d], if I T \u22121 i 3. Let U \u0135 i t = [b j,min(K,I t j ) ; \u00b7 \u00b7 \u00b7 ; b j,K ] with j \u2208 [n], and U\u00ee t = [U 1 i t , \u00b7 \u00b7 \u00b7 , U n i t ]. U\u00ee t is analogous to U t , but is a matrix. Let\u016a = [\u0168\u00ee t , U\u00ee t ].\n\nWe have that\nP (G \u2264t ) c | G <t (B.93) = \u00eet 0 \u2208\u015c t E \u03be,U\u00eet 0 P (G \u2264t ) c | G <t ,\u00ee t =\u00ee t 0 , \u03be, U\u00eet 0 P \u00ee t =\u00ee t 0 | G <t , \u03be, U\u00eet 0 .\nFor \u00eet 0 \u2208\u015c t E \u03be,U\u00eet 0 P \u00ee t =\u00ee t 0 | G <t , \u03be, U\u00eet 0 = \u00eet 0 \u2208\u015c t P \u00ee t =\u00ee t 0 | G <t = 1, in the rest, we show that the probability P (G \u2264t ) c | G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 , for all \u03be 0 ,\u0168 0 is small. By union bound, we have\nP (G \u2264t ) c | G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 (B.94) \u2264 n i=1 u\u2208U t P u, P (t\u22121)\u22a5 i y t i \u2265 a P (t\u22121)\u22a5 i y t i | G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 .\nNote that\u00ee t 0 is a constant. Because given \u03be and\u0168\u00eet 0 , under G \u2264t , both P (t\u22121) i and y t i are known. We prove\nP U\u00eet 0 = U 0 | G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 = P U\u00eet 0 = Z i U 0 | G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 , (B.95)\nwhere Z i \u2208 R d/n\u00d7d/n , Z T i Z i = I d , and Z i u = u = Z T i u for all u \u2208 V t\u22121 i . In this way, =\u0168 0 . Because G <l happens, thus at each iteration, we can only recover one index until l \u2212 1. Then (x ) j = x j and (\u00ee ) j =\u00ee j . with j \u2264 l. By induction, we only need to prove that G l\u22121 will happen. Let u \u2208 U l\u22121 , and i \u2208 [n], we have  \nu, P (t\u22121)\u22a5 i y t i P (t\u22121)\u22a5 i y t i \u2265 a | G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 a \u2264 P P (t\u22121)\u22a5 i u P (t\u22121)\u22a5 i u , P (t\u22121)\u22a5 i y t i P (t\u22121)\u22a5 i y t i \u2265 a | G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 b \u2264 2e \u2212a 2 (d/n\u22122T ) 2 , (B.99)\nwhere in a \u2264, we use P Then by setting d/n \u2265 2 max(9n 2 K 2 , 12nKR 2 ) log( 2n 3 K 2 p ) + nK \u2265 2 max(9(T + 1) 2 , 2(2 \u221a 3T ) 2 R 2 ) log( 2n 3 K 2 p ) + 2T\n\n\u2265 2 max(9(T + 1) 2 , 2(1 + \u221a 3T ) 2 R 2 ) log( 2n 3 K 2 p ) + 2T\n\n\u2265 2 a 2 log( 2n 3 K 2 p ) + 2T, (B.101)\n\nwe have P G \u2264T c \u2264 p. This ends proof.\n\n:\nOPTION I: Return x K however, this line is not reached with high probability 17: OPTION II: Returnx chosen uniformly at random from {x k } K\u22121 k=0\n\n\n; Tripuraneni et al. (2018); Xu et al. (2017); Allen-Zhu & Li (2018); Zhou et al. (2018a) have gradient costs that cannot break the O( \u22123.5 ) barrier when \u03b4 is chosen to be O( 0.5 ). Observe that we always need to run a new Spider-SFO which at least costs O min( \u22122 , n) stochastic gradient accesses.\n\n\nand 3 likeCarmon et al. (2016); Allen-Zhu & Li (2018) when n \u2264 O( \u22121 ), and thus can achieve the state-of-the-art gradient cost of O min n \u22121.5 + n 3/4 \u22121.75 , n 1/2 \u22122 + n 1/2 \u22121 \u03b4 \u22122 + min n + n 3/4 \u03b4 \u22120.5 , \u03b4 \u22122 \u03b4\n\n(i ) Figure 1 :Figure 2 :Figure 1 :\n)121For the problem of finding an -approximate first-order stationary point, under Assumption 1 our results indicate a gradient cost of O(min( \u22123 , n 1/2 \u22122 )) which supersedes the best-known convergence rate results for stochastic optimization problem (1.2) [Theorems 1 and 2]. Before this work, the best-known result is O min( \u22123.333 , n 2/3 \u22122 ) , achieved by Allen-Zhu & Hazan(2016); Reddi et al. (2016) in the finite-sum case and Lei et al. (2017) in the on-line case, separately. Moreover, such a gradient cost achieves the algorithmic lower bound for the finitesum setting [Theorem 3]. (ii) For the problem of finding ( , \u03b4)-approximate second-order stationary point x, under both Gradient cost comparison of GD/SGD, SVRG/SCSG and SPIDER-SFO (Algorithm 1) for SFO. Gradient cost comparison of GD/SGD, SVRG/SCSG and SPIDER-SFO (Algorithm 1) for finding an (\\ep, \\sqrt{\\ep})-approximate local minimizer. Both axis is on logarithmic scale of the relevant parameters. Note we assume Hessian Lipschitz condition. Left panel: gradient cost comparison for finding an -approximate first-order stationary point. Right panel: gradient cost comparison for finding an ( , O( 0.5 ))-approximate second-order stationary points (note we assume Hessian Lipschitz condition). Both axes are on the logarithmic scale of \u22121 . Assumptions 1 and 3, the gradient cost is\u00d5( \u22123 + \u22122 \u03b4 \u22122 + \u03b4 \u22125 ) in the on-line case and O(n 1/2 \u22122 + n 1/2 \u22121 \u03b4 \u22122 + \u22121 \u03b4 \u22123 + \u03b4 \u22125 + n) in the finite-sum case [Theorem 6]. In the classical definition of second-order stationary point where \u03b4 = O( 0.5 ), such gradient cost is simply O( \u22123 ) in the on-line case. In comparison, to the best of our knowledge the best-known results only achieve a gradient cost of O( \u22123.5 ) under similar assumptions (Reddi et al., 2018; Tripuraneni et al., 2018; Allen-Zhu, 2018; Allen-Zhu & Li, 2018; Zhou et al., 2018a).\n\n\nJohnson, R. & Zhang, T. (2013). Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing.Kallenberg, O. & Sztencel, R. (1991). Some dimension-free features of vector-valued martingales. Probability Theory and Related Fields, 88(2), 215-247. Lee, J. D., Simchowitz, M., Jordan, M. I., & Recht, B. (2016). Gradient descent only converges to minimizers. In Proceedings of The 29th Conference on Learning Theory (pp. 1246-1257). Lei, L., Ju, C., Chen, J., & Jordan, M. I. (2017). Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems (pp. 2345-2355). Levy, K. Y. (2016). The power of normalization: Faster evasion of saddle points. arXiv preprint arXiv:1611.04831. Li, C. J., Wang, M., Liu, H., & Zhang, T. (2017). Near-optimal stochastic approximation for online principal component estimation. Mathematical Programming, Series B, Special Issue on Optimization Models and Algorithms for Data Science. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o (1/k\u02c62). In Doklady AN USSR, volume 269 (pp. 543-547). Nesterov, Y. (2004). Introductory lectures on convex optimization: A basic course, volume 87. Springer. Nesterov, Y. & Polyak, B. T. (2006). Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1), 177-205. Nesterov, Y. & Spokoiny, V. (2011). Random gradient-free minimization of convex functions. Technical report, Universit\u00e9 catholique de Louvain, Center for Operations Research and Econometrics (CORE). Nguyen, L. M., Liu, J., Scheinberg, K., & Tak\u00e1\u010d, M. (2017a). SARAH: A novel method for machine learning problems using stochastic recursive gradient. In D. Precup & Y. W. Teh (Eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research (pp. 2613-2621). International Convention Centre, Sydney, Australia: PMLR. Nguyen, L. M., Liu, J., Scheinberg, K., & Tak\u00e1\u010d, M. (2017b). Stochastic recursive gradient algorithm for nonconvex optimization. arXiv preprint arXiv:1705.07261. Oja, E. (1982). Simplified neuron model as a principal component analyzer. Journal of mathematical biology, 15(3), 267-273. Paquette, C., Lin, H., Drusvyatskiy, D., Mairal, J., & Harchaoui, Z. (2018). Catalyst for gradientbased nonconvex optimization. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics (pp. 613-622). Pinelis, I. (1994). Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability, (pp. 1679-1706). Reddi, S., Zaheer, M., Sra, S., Poczos, B., Bach, F., Salakhutdinov, R., & Smola, A. (2018). A generic approach for escaping saddle points. In A. Storkey & F. Perez-Cruz (Eds.), Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research (pp. 1233-1242). Playa Blanca, Lanzarote, Canary Islands: PMLR. Reddi, S. J., Hefny, A., Sra, S., Poczos, B., & Smola, A. (2016). Stochastic variance reduction for nonconvex optimization. In International conference on machine learning (pp. 314-323). Robbins, H. & Monro, S. (1951). A stochastic approximation method. The annals of mathematical statistics, (pp. 400-407). Schmidt, M., Le Roux, N., & Bach, F. (2017). Minimizing finite sums with the stochastic average gradient. Mathematical Programming, 162(1-2), 83-112. Shalev-Shwartz, S. & Zhang, T. (2016). Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. Mathematical Programming, 155(1), 105-145. Shamir, O. (2017). An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. Journal of Machine Learning Research, 18(52), 1-11. Tripuraneni, N., Stern, M., Jin, C., Regier, J., & Jordan, M. I. (2018). Stochastic cubic regularization for fast nonconvex optimization. In Advances in Neural Information Processing Systems. Vershynin, R. (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027. Woodworth, B. & Srebro, N. (2017). Lower bound for randomized first order convex optimization. arXiv preprint arXiv:1709.03594. Woodworth, B. E. & Srebro, N. (2016). Tight complexity bounds for optimizing composite objectives. In Advances in Neural Information Processing Systems (pp. 3639-3647). Xu, Y., Jin, R., & Yang, T. (2017). First-order stochastic algorithms for escaping from saddle points in almost linear time. arXiv preprint arXiv:1711.01944. Zhang, T. (2005). Learning bounds for kernel regression using effective data dimensionality. Neural Computation, 17(9), 2077-2098. Zhou, D., Xu, P., & Gu, Q. (2018a). Finding local minima via stochastic nested variance reduction. arXiv preprint arXiv:1806.08782.Zhou, D., Xu, P., & Gu, Q. (2018b). Stochastic nested variance reduction for nonconvex optimization. arXiv preprint arXiv:1806.07811.\n\nE u a, u u 2 \u2264\n2(d + 4) a 2 ; obtained from the same technique of (33) in (Nesterov & Spokoiny, 2011); in c =, E u u 6 \u2264 (d + 6) 3 in (17) of (Nesterov & Spokoiny, 2011).\n\n\ni denote the value of i-th coordinate of x, with i \u2208 [d].f K (x) constructed byCarmon et al. (2017b) is a zero-chain function, that is for every\n\n\u221a\n\u03c1(x) , . . . , b (K) 1+ x 2 /R 2 and R = 230 \u221a K, B K is chosen uniformly at random from the space of orthogonal matrices O(d, K) = {D \u2208 R d\u00d7K |D D = I K }. The functionf K,B (x) satisfies the following:(i)fK,B K (0) \u2212 inf xf K,B K (x) \u2264 12K. (B.73)\n\n\n\u2207f K,B K (x k ) \u2265 1 2 , for every k \u2264 K. (B.74)The above properties found byCarmon et al. (2017b) is very technical. One can refer toCarmon et al. (2017b) for more details.Proof of Theorem 3. Our lower bound theorem proof is as follows. The proof mirrors Theorem 2 inCarmon et al. (2017b) by further taking the number of individual function n into account. nK = [B K 1 , . . . , B K n ] is chosen uniformly at random from the space of orthogonal matricesO(d, K) = {D \u2208 R (d/n)\u00d7(nK) |D D = I (nK) }, with each B K i \u2208 {D \u2208 R (d/n)\u00d7(K) |D D = I (K) }, i \u2208 [n], C = [C 1 , . . . , C n ] is an arbitrary orthogonal matrices O(d, K) = {D \u2208 R d\u00d7d |D D = I d }, with each C K i \u2208 {D \u2208 R (d)\u00d7(d/n) |D D = I (d/n) }, i \u2208 [n].\n\nfrom\nLemma 2 of Carmon et al. (2017b) (or similarly Lemma 7 of Woodworth & Srebro (2016) and Theorem 3 of Woodworth & Srebro (2017), also refer to Lemma 12 in the end of the paper), with probability at least 1 \u2212 p, after T = nK 2 iterations (at the end of iteration T \u2212 1), for all I T \u22121 i with 13 If x 0 = 0, we can simply translate the counter example as f (x) = f (x \u2212 x 0 ), then f (x 0 ) \u2212 inf x\u2208R d f (x) \u2264 \u2206.\n\n==<t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 ==\u0168 0 .\n000U 0 | G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 U 0 , G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 ) P(G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 Z i U 0 | G <t ,\u00ee t =\u00ee t 0 , \u03be = \u03be 0 ,\u0168\u00ee 0 =\u0168 0 = P(G <t ,\u00ee t =\u00ee t 0 | \u03be = \u03be 0 , \u03be and\u016a are independent. And p(\u016a) = p(Z i\u016a ), we have p(\u03be = \u03be 0 , U\u00eet 0 = U 0 ,\u0168\u00eet 0 =\u0168 0 ) = p(\u03be = \u03be 0 , U\u00eet 0 = Z i U 0 ,\u0168\u00eet 0 =\u0168 0 ). Then we prove that if G <t and\u00ee t =\u00ee t 0 happens under U\u00eet 0 = U 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 , if and only if G <t and\u00ee t =\u00ee t iteration l \u2212 1 with l \u2264 t, we have the result. At iteration l, suppose G <l and\u00ee l =\u00ee Let x and (\u00ee ) j are generated by \u03be = \u03be\n\n\nthe span of V l i \u2286 V t\u22121 i . This shows that if G <t and\u00ee t =\u00ee t 0 happen under U\u00eet 0 = U 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 , then G <t and\u00ee t =\u00ee t happen under U\u00eet 0 = Z i U 0 , \u03be = \u03be 0 ,\u0168\u00eet 0 =\u0168 0 .In the same way, we can prove the necessity. Thus for any u \u2208 U t , if P\n\nP\n\n\n\ndistribution on the unit space. Then by union bound, we have P G \u2264t c | G <t \u2264 2(n 2 K)e\n\n\nthen23: \n\nreturn x k \n\n24: \n\nend if \n\n25: \n\n\n\nTable 1 :\n1Comparable results on the gradient cost for nonconvex optimization algorithms that use only individual (or stochastic) gradients. Note that the gradient cost hides a poly-logarithmic factors of d, n, . For clarity and brevity purposes, we record for most algorithms the gradient cost for finding an ( , O( 0.5 ))-approximate second-order stationary point. \u0398 : this entry is for n \u2265 \u2126( \u22121 ) only, in which case Spider-SFO + outperforms Neon+FastCubic/CDHS.For some algorithms we \n\nIn the finite-sum case, when n \u2264 \u22121 Spider-SFO has a slower rate of\u00d5( \u22122.5 ) than the state-of-art\u00d5(n 3/4 \u22121.75 ) rate achieved by Neon+FastCubic/CDHS (Allen-Zhu & Li, 2018). Neon+FastCubic/CDHS has exploited appropriate acceleration techniques, which has not been considered for Spider.\nAllen-Zhu (2018) also obtains a gradient cost of\u00d5( \u22123.25 ) to achieve a (modified and weakened) ( , O( 0.25 ))approximate second-order stationary point.3  Here and in many places afterwards, the gradient cost also includes the number of stochastic Hessian-vector product accesses, which has similar running time with computing per-access stochastic gradient.4 To our best knowledge, the work by Zhou et al. (2018b,a) appeared on-line on June 20, 2018 and June 22, 2018, separately. SNVRG (Zhou et al., 2018b) obtains a gradient complexity of\u00d5(min(n 1/2 \u22122 , \u22123 )) for finding an approximate first-order stationary point, and achieves\u00d5( \u22123 ) gradient complexity for finding an approximate second-order stationary point (Zhou et al., 2018a) for a wide range of \u03b4. By exploiting the third-order smoothness condition, SNVRG can also achieve an ( , O( 0.5 ))-approximate second-order stationary point in\u00d5( \u22123 ) gradient costs.\nIn this paper, we use Azuma-Hoeffding-type concentration inequality to obtain high probability results like Xu et al.(2017); Allen-Zhu & Li (2018). By applying Bernstein inequality, under the Assumption 1, the parameters in the Assumption 2 are allowed to be\u03a9( \u22121 ) larger without hurting the convergence rate.\nBy multiple times (at most in O(log(1/p)) times) of verification and restarting Algorithm 2 , one can also obtain a high-probability result.\nSpider-SFO + enjoys a faster rate than Neon+Spider-SFO where computing the \"full\" gradient dominates the gradient cost, namely \u03b4 = O(1) in the on-line case and \u03b4 = O(n 1/2 ) for the finite-sum case.\nSPIDER for Stochastic Zeroth-Order MethodFor SZO algorithms, (2.3) can be solved only from the Incremental Zeroth-Order Oracle (IZO)(Nesterov  & Spokoiny, 2011), which is defined as: 11 One of the results not included in this table isCarmon et al. (2017a), which finds an -approximate first-order stationary point in O(n \u22121.75 ) gradient evaluations. However, their result relies on a more stringent Hessian-Lipschitz condition, in which case a second-order stationary point can be found in similar gradient cost(Jin et al., 2017b).12 Due to the Neon method(Xu et al., 2017;Allen-Zhu & Li, 2018), nearly all existing Hessian-vector product based algorithms in stochastic optimization can be converted to ones that use stochastic gradients only.\n| F K .\n\nFinding approximate local minima faster than gradient descent. N Agarwal, Z Allen-Zhu, B Bullins, E Hazan, T Ma, Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing. the 49th Annual ACM SIGACT Symposium on Theory of ComputingACMAgarwal, N., Allen-Zhu, Z., Bullins, B., Hazan, E., & Ma, T. (2017). Finding approximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Sympo- sium on Theory of Computing (pp. 1195-1199).: ACM.\n\nNatasha 2: Faster non-convex optimization than sgd. Allen-Zhu, Advances in Neural Information Processing Systems. Allen-Zhu, Z. (2018). Natasha 2: Faster non-convex optimization than sgd. In Advances in Neural Information Processing Systems.\n\nVariance reduction for faster non-convex optimization. Z Allen-Zhu, E Hazan, International Conference on Machine Learning. Allen-Zhu, Z. & Hazan, E. (2016). Variance reduction for faster non-convex optimization. In International Conference on Machine Learning (pp. 699-707).\n\nFirst effcient convergence for streaming k-PCA: a global, gap-free, and near-optimal rate. Z Allen-Zhu, Y Li, The 58th Annual Symposium on Foundations of Computer Science. Allen-Zhu, Z. & Li, Y. (2017). First effcient convergence for streaming k-PCA: a global, gap-free, and near-optimal rate. The 58th Annual Symposium on Foundations of Computer Science.\n\nNeon2: Finding local minima via first-order oracles. Z Allen-Zhu, Y Li, Advances in Neural Information Processing Systems. Allen-Zhu, Z. & Li, Y. (2018). Neon2: Finding local minima via first-order oracles. In Advances in Neural Information Processing Systems.\n\nLarge-scale machine learning with stochastic gradient descent. L Bottou, Proceedings of COMPSTAT'2010. COMPSTAT'2010SpringerBottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Springer.\n\nOptimization methods for large-scale machine learning. L Bottou, F E Curtis, J Nocedal, SIAM Review. 602Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM Review, 60(2), 223-311.\n\nS Bubeck, Convex optimization: Algorithms and complexity. Foundations and Trends R in Machine Learning. 8Bubeck, S. et al. (2015). Convex optimization: Algorithms and complexity. Foundations and Trends R in Machine Learning, 8(3-4), 231-357.\n\nAccelerated methods for non-convex optimization. Y Carmon, J C Duchi, O Hinder, A Sidford, SIAM Journal on Optimization. To appear inCarmon, Y., Duchi, J. C., Hinder, O., & Sidford, A. (2016). Accelerated methods for non-convex optimization. To appear in SIAM Journal on Optimization, accepted.\n\nConvex Until Proven Guilty\": Dimension-free acceleration of gradient descent on non-convex functions. Y Carmon, J C Duchi, O Hinder, A Sidford, International Conference on Machine Learning. Carmon, Y., Duchi, J. C., Hinder, O., & Sidford, A. (2017a). \"Convex Until Proven Guilty\": Dimension-free acceleration of gradient descent on non-convex functions. In International Con- ference on Machine Learning (pp. 654-663).\n\nY Carmon, J C Duchi, O Hinder, A Sidford, arXiv:1710.11606Lower bounds for finding stationary points i. arXiv preprintCarmon, Y., Duchi, J. C., Hinder, O., & Sidford, A. (2017b). Lower bounds for finding stationary points i. arXiv preprint arXiv:1710.11606.\n\nM\u00e9thode g\u00e9n\u00e9rale pour la r\u00e9solution des systemes d\u00e9quations simultan\u00e9es. A Cauchy, Comptes Rendus de l'Academie des Science. 25Cauchy, A. (1847). M\u00e9thode g\u00e9n\u00e9rale pour la r\u00e9solution des systemes d\u00e9quations simultan\u00e9es. Comptes Rendus de l'Academie des Science, 25, 536-538.\n\nIdentifying and attacking the saddle point problem in high-dimensional non-convex optimization. Y N Dauphin, R Pascanu, C Gulcehre, K Cho, S Ganguli, Y Bengio, Advances in Neural Information Processing Systems. Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., & Bengio, Y. (2014). Identi- fying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural Information Processing Systems (pp. 2933-2941).\n\nSAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. A Defazio, F Bach, S Lacoste-Julien, Advances in Neural Information Processing Systems. Defazio, A., Bach, F., & Lacoste-Julien, S. (2014). SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems (pp. 1646-1654).\n\nR Durrett, Probability: Theory and Examples. Cambridge University Press4th editionDurrett, R. (2010). Probability: Theory and Examples (4th edition). Cambridge University Press.\n\nEscaping from saddle points -online stochastic gradient for tensor decomposition. R Ge, F Huang, C Jin, Y Yuan, Proceedings of The 28th Conference on Learning Theory. The 28th Conference on Learning TheoryGe, R., Huang, F., Jin, C., & Yuan, Y. (2015). Escaping from saddle points -online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory (pp. 797-842).\n\nStochastic first-and zeroth-order methods for nonconvex stochastic programming. S Ghadimi, G Lan, SIAM Journal on Optimization. 234Ghadimi, S. & Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4), 2341-2368.\n\nDeep Learning. I Goodfellow, Y Bengio, A Courville, MIT PressGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press. http://www. deeplearningbook.org.\n\nBeyond convexity: Stochastic quasi-convex optimization. E Hazan, K Levy, S Shalev-Shwartz, Advances in Neural Information Processing Systems. Hazan, E., Levy, K., & Shalev-Shwartz, S. (2015). Beyond convexity: Stochastic quasi-convex optimization. In Advances in Neural Information Processing Systems (pp. 1594-1602).\n\nMatching matrix Bernstein and near-optimal finite sample guarantees for Oja's algorithm. P Jain, C Jin, S M Kakade, P Netrapalli, A Sidford, Proceedings of The 29th Conference on Learning Theory. The 29th Conference on Learning TheoryJain, P., Jin, C., Kakade, S. M., Netrapalli, P., & Sidford, A. (2016). Matching matrix Bernstein and near-optimal finite sample guarantees for Oja's algorithm. In Proceedings of The 29th Conference on Learning Theory (pp. 1147-1164).\n\nNon-convex optimization for machine learning. P Jain, P Kar, Foundations and Trends R in Machine Learning. 103-4Jain, P., Kar, P., et al. (2017). Non-convex optimization for machine learning. Foundations and Trends R in Machine Learning, 10(3-4), 142-336.\n\nHow to escape saddle points efficiently. C Jin, R Ge, P Netrapalli, S M Kakade, M I Jordan, International Conference on Machine Learning. Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., & Jordan, M. I. (2017a). How to escape saddle points efficiently. In International Conference on Machine Learning (pp. 1724-1732).\n\nAccelerated gradient descent escapes saddle points faster than gradient descent. C Jin, P Netrapalli, M I Jordan, arXiv:1711.10456arXiv preprintJin, C., Netrapalli, P., & Jordan, M. I. (2017b). Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456.\n", "annotations": {"author": "[{\"end\":161,\"start\":141},{\"end\":180,\"start\":162},{\"end\":201,\"start\":181},{\"end\":225,\"start\":202},{\"end\":247,\"start\":226}]", "publisher": null, "author_last_name": "[{\"end\":150,\"start\":146},{\"end\":169,\"start\":164},{\"end\":190,\"start\":188},{\"end\":214,\"start\":211},{\"end\":236,\"start\":231}]", "author_first_name": "[{\"end\":145,\"start\":141},{\"end\":163,\"start\":162},{\"end\":187,\"start\":181},{\"end\":210,\"start\":202},{\"end\":230,\"start\":226}]", "author_affiliation": "[{\"end\":160,\"start\":152},{\"end\":179,\"start\":171},{\"end\":200,\"start\":192},{\"end\":224,\"start\":216},{\"end\":246,\"start\":238}]", "title": "[{\"end\":99,\"start\":1},{\"end\":346,\"start\":248}]", "venue": null, "abstract": "[{\"end\":1634,\"start\":387}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2103,\"start\":2089},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2123,\"start\":2103},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2143,\"start\":2123},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2312,\"start\":2287},{\"end\":3805,\"start\":3789},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3824,\"start\":3805},{\"end\":4074,\"start\":4062},{\"end\":4616,\"start\":4596},{\"end\":4653,\"start\":4635},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5122,\"start\":5101},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5697,\"start\":5675},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5713,\"start\":5697},{\"end\":6009,\"start\":5984},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6734,\"start\":6713},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6756,\"start\":6736},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7160,\"start\":7142},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7572,\"start\":7559},{\"end\":7599,\"start\":7577},{\"end\":7704,\"start\":7688},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7724,\"start\":7704},{\"end\":7750,\"start\":7724},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7770,\"start\":7750},{\"end\":7783,\"start\":7770},{\"end\":7918,\"start\":7895},{\"end\":7939,\"start\":7918},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8153,\"start\":8139},{\"end\":8172,\"start\":8153},{\"end\":8189,\"start\":8172},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8440,\"start\":8423},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8458,\"start\":8440},{\"end\":8474,\"start\":8458},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8495,\"start\":8474},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8514,\"start\":8495},{\"end\":8525,\"start\":8514},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8541,\"start\":8525},{\"end\":8560,\"start\":8541},{\"end\":8585,\"start\":8560},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8603,\"start\":8585},{\"end\":8620,\"start\":8603},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8641,\"start\":8620},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8661,\"start\":8641},{\"end\":8683,\"start\":8661},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8734,\"start\":8718},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8754,\"start\":8736},{\"end\":9029,\"start\":9018},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9315,\"start\":9293},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9346,\"start\":9325},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9401,\"start\":9384},{\"end\":9426,\"start\":9401},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9691,\"start\":9669},{\"end\":9716,\"start\":9691},{\"end\":9827,\"start\":9802},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9934,\"start\":9914},{\"end\":10061,\"start\":10042},{\"end\":10562,\"start\":10558},{\"end\":10567,\"start\":10562},{\"end\":10582,\"start\":10567},{\"end\":10586,\"start\":10582},{\"end\":10591,\"start\":10586},{\"end\":10600,\"start\":10591},{\"end\":10605,\"start\":10600},{\"end\":10782,\"start\":10771},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10863,\"start\":10846},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10881,\"start\":10863},{\"end\":11296,\"start\":11272},{\"end\":11488,\"start\":11465},{\"end\":11566,\"start\":11543},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11836,\"start\":11814},{\"end\":12161,\"start\":12139},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12805,\"start\":12786},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12828,\"start\":12805},{\"end\":12869,\"start\":12848},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13102,\"start\":13082},{\"end\":13746,\"start\":13724},{\"end\":13768,\"start\":13746},{\"end\":14070,\"start\":14047},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14103,\"start\":14081},{\"end\":17706,\"start\":17691},{\"end\":19967,\"start\":19943},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22224,\"start\":22202},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22245,\"start\":22224},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22263,\"start\":22245},{\"end\":22279,\"start\":22263},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22300,\"start\":22279},{\"end\":22644,\"start\":22632},{\"end\":25133,\"start\":25115},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26176,\"start\":26154},{\"end\":26201,\"start\":26176},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27374,\"start\":27353},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27535,\"start\":27514},{\"end\":31074,\"start\":31063},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31090,\"start\":31074},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31130,\"start\":31108},{\"end\":31146,\"start\":31130},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36201,\"start\":36179},{\"end\":36217,\"start\":36201},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37233,\"start\":37211},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37253,\"start\":37233},{\"end\":37323,\"start\":37307},{\"end\":37423,\"start\":37393},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38277,\"start\":38259},{\"end\":39501,\"start\":39474},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39521,\"start\":39501},{\"end\":39633,\"start\":39606},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39653,\"start\":39633},{\"end\":39666,\"start\":39653},{\"end\":39953,\"start\":39921},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":40901,\"start\":40881},{\"end\":43658,\"start\":43631},{\"end\":43685,\"start\":43658},{\"end\":43716,\"start\":43704},{\"end\":44389,\"start\":44361},{\"end\":44537,\"start\":44509},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":44882,\"start\":44867},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":69744,\"start\":69723},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":69878,\"start\":69857},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":70112,\"start\":70090},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":70252,\"start\":70231},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":73841,\"start\":73821},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":81234,\"start\":81213},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":81631,\"start\":81610},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":81688,\"start\":81667},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":81822,\"start\":81801},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":86300,\"start\":86279},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":86576,\"start\":86557},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":86640,\"start\":86619}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":73505,\"start\":73356},{\"attributes\":{\"id\":\"fig_1\"},\"end\":73808,\"start\":73506},{\"attributes\":{\"id\":\"fig_2\"},\"end\":74027,\"start\":73809},{\"attributes\":{\"id\":\"fig_3\"},\"end\":75933,\"start\":74028},{\"attributes\":{\"id\":\"fig_4\"},\"end\":80959,\"start\":75934},{\"attributes\":{\"id\":\"fig_6\"},\"end\":81131,\"start\":80960},{\"attributes\":{\"id\":\"fig_8\"},\"end\":81278,\"start\":81132},{\"attributes\":{\"id\":\"fig_9\"},\"end\":81531,\"start\":81279},{\"attributes\":{\"id\":\"fig_10\"},\"end\":82250,\"start\":81532},{\"attributes\":{\"id\":\"fig_11\"},\"end\":82668,\"start\":82251},{\"attributes\":{\"id\":\"fig_12\"},\"end\":83283,\"start\":82669},{\"attributes\":{\"id\":\"fig_13\"},\"end\":83549,\"start\":83284},{\"attributes\":{\"id\":\"fig_14\"},\"end\":83553,\"start\":83550},{\"attributes\":{\"id\":\"fig_15\"},\"end\":83644,\"start\":83554},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":83691,\"start\":83645},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":84183,\"start\":83692}]", "paragraph": "[{\"end\":1698,\"start\":1650},{\"end\":1740,\"start\":1735},{\"end\":2749,\"start\":1742},{\"end\":4693,\"start\":2797},{\"end\":5393,\"start\":4695},{\"end\":6801,\"start\":5395},{\"end\":7277,\"start\":6819},{\"end\":7784,\"start\":7279},{\"end\":8190,\"start\":7786},{\"end\":9127,\"start\":8192},{\"end\":10188,\"start\":9129},{\"end\":11043,\"start\":10190},{\"end\":11504,\"start\":11045},{\"end\":11686,\"start\":11506},{\"end\":11908,\"start\":11688},{\"end\":12164,\"start\":11910},{\"end\":12463,\"start\":12186},{\"end\":13065,\"start\":12465},{\"end\":13442,\"start\":13067},{\"end\":13869,\"start\":13444},{\"end\":14104,\"start\":13871},{\"end\":14795,\"start\":14106},{\"end\":15791,\"start\":14797},{\"end\":15855,\"start\":15793},{\"end\":16254,\"start\":15857},{\"end\":16518,\"start\":16256},{\"end\":16724,\"start\":16639},{\"end\":16772,\"start\":16767},{\"end\":17030,\"start\":16774},{\"end\":17272,\"start\":17243},{\"end\":17416,\"start\":17326},{\"end\":17471,\"start\":17466},{\"end\":17896,\"start\":17473},{\"end\":18108,\"start\":17949},{\"end\":18247,\"start\":18154},{\"end\":18456,\"start\":18294},{\"end\":18531,\"start\":18458},{\"end\":18927,\"start\":18595},{\"end\":19426,\"start\":18972},{\"end\":19570,\"start\":19455},{\"end\":19679,\"start\":19572},{\"end\":19686,\"start\":19681},{\"end\":19774,\"start\":19688},{\"end\":20047,\"start\":19814},{\"end\":20086,\"start\":20049},{\"end\":20277,\"start\":20185},{\"end\":20418,\"start\":20318},{\"end\":20585,\"start\":20449},{\"end\":20652,\"start\":20587},{\"end\":21038,\"start\":20793},{\"end\":21199,\"start\":21040},{\"end\":21334,\"start\":21228},{\"end\":21384,\"start\":21359},{\"end\":21493,\"start\":21386},{\"end\":21614,\"start\":21495},{\"end\":21894,\"start\":21753},{\"end\":22069,\"start\":21896},{\"end\":22301,\"start\":22108},{\"end\":22375,\"start\":22334},{\"end\":22868,\"start\":22420},{\"end\":23003,\"start\":22870},{\"end\":23195,\"start\":23093},{\"end\":23299,\"start\":23197},{\"end\":23535,\"start\":23371},{\"end\":23654,\"start\":23537},{\"end\":23772,\"start\":23691},{\"end\":24001,\"start\":23774},{\"end\":24334,\"start\":24075},{\"end\":24461,\"start\":24360},{\"end\":24552,\"start\":24463},{\"end\":24669,\"start\":24554},{\"end\":25042,\"start\":24671},{\"end\":25201,\"start\":25101},{\"end\":25276,\"start\":25203},{\"end\":25346,\"start\":25278},{\"end\":25587,\"start\":25429},{\"end\":26010,\"start\":25589},{\"end\":26282,\"start\":26068},{\"end\":26696,\"start\":26634},{\"end\":27082,\"start\":26698},{\"end\":28319,\"start\":27084},{\"end\":29217,\"start\":28321},{\"end\":29409,\"start\":29219},{\"end\":29567,\"start\":29411},{\"end\":29859,\"start\":29619},{\"end\":30212,\"start\":29861},{\"end\":30578,\"start\":30258},{\"end\":31336,\"start\":30612},{\"end\":31762,\"start\":31522},{\"end\":31930,\"start\":31764},{\"end\":32098,\"start\":31932},{\"end\":32198,\"start\":32100},{\"end\":32225,\"start\":32200},{\"end\":32241,\"start\":32227},{\"end\":32285,\"start\":32243},{\"end\":32869,\"start\":32287},{\"end\":32924,\"start\":32871},{\"end\":32995,\"start\":32926},{\"end\":33521,\"start\":32997},{\"end\":33781,\"start\":33523},{\"end\":33907,\"start\":33783},{\"end\":34019,\"start\":33932},{\"end\":34039,\"start\":34021},{\"end\":34062,\"start\":34041},{\"end\":34132,\"start\":34069},{\"end\":34178,\"start\":34154},{\"end\":34255,\"start\":34220},{\"end\":34593,\"start\":34526},{\"end\":34929,\"start\":34822},{\"end\":35299,\"start\":35009},{\"end\":35686,\"start\":35384},{\"end\":36256,\"start\":35758},{\"end\":36908,\"start\":36258},{\"end\":37490,\"start\":36910},{\"end\":37499,\"start\":37492},{\"end\":38523,\"start\":37536},{\"end\":38621,\"start\":38525},{\"end\":38671,\"start\":38646},{\"end\":38891,\"start\":38826},{\"end\":38905,\"start\":38898},{\"end\":39002,\"start\":38907},{\"end\":39015,\"start\":39009},{\"end\":39383,\"start\":39180},{\"end\":39987,\"start\":39385},{\"end\":40105,\"start\":40074},{\"end\":40242,\"start\":40199},{\"end\":40538,\"start\":40447},{\"end\":41247,\"start\":40821},{\"end\":41441,\"start\":41249},{\"end\":41609,\"start\":41443},{\"end\":41940,\"start\":41725},{\"end\":42225,\"start\":42141},{\"end\":42444,\"start\":42439},{\"end\":42489,\"start\":42446},{\"end\":43235,\"start\":42491},{\"end\":43443,\"start\":43284},{\"end\":43744,\"start\":43445},{\"end\":43956,\"start\":43746},{\"end\":44052,\"start\":44007},{\"end\":44580,\"start\":44054},{\"end\":44704,\"start\":44611},{\"end\":44883,\"start\":44789},{\"end\":44986,\"start\":44908},{\"end\":45449,\"start\":45322},{\"end\":45621,\"start\":45516},{\"end\":45763,\"start\":45724},{\"end\":45906,\"start\":45856},{\"end\":46179,\"start\":46158},{\"end\":46427,\"start\":46224},{\"end\":46546,\"start\":46497},{\"end\":47064,\"start\":46951},{\"end\":47233,\"start\":47163},{\"end\":47455,\"start\":47389},{\"end\":47572,\"start\":47530},{\"end\":47625,\"start\":47574},{\"end\":47715,\"start\":47657},{\"end\":47776,\"start\":47749},{\"end\":47853,\"start\":47825},{\"end\":48004,\"start\":47941},{\"end\":48118,\"start\":48006},{\"end\":48299,\"start\":48215},{\"end\":48384,\"start\":48351},{\"end\":48639,\"start\":48469},{\"end\":48828,\"start\":48760},{\"end\":48870,\"start\":48830},{\"end\":49062,\"start\":48947},{\"end\":49329,\"start\":49182},{\"end\":49521,\"start\":49449},{\"end\":49770,\"start\":49571},{\"end\":49900,\"start\":49772},{\"end\":50007,\"start\":49954},{\"end\":50412,\"start\":50009},{\"end\":50854,\"start\":50566},{\"end\":50895,\"start\":50856},{\"end\":51065,\"start\":50938},{\"end\":51147,\"start\":51115},{\"end\":51609,\"start\":51470},{\"end\":51655,\"start\":51631},{\"end\":51822,\"start\":51771},{\"end\":51867,\"start\":51824},{\"end\":52154,\"start\":52090},{\"end\":52478,\"start\":52409},{\"end\":52881,\"start\":52748},{\"end\":52927,\"start\":52883},{\"end\":53174,\"start\":53072},{\"end\":53378,\"start\":53264},{\"end\":53544,\"start\":53509},{\"end\":53737,\"start\":53692},{\"end\":53823,\"start\":53786},{\"end\":53866,\"start\":53825},{\"end\":54209,\"start\":54099},{\"end\":54449,\"start\":54273},{\"end\":54517,\"start\":54451},{\"end\":54898,\"start\":54684},{\"end\":55099,\"start\":54976},{\"end\":55191,\"start\":55136},{\"end\":55588,\"start\":55428},{\"end\":55889,\"start\":55706},{\"end\":56201,\"start\":56104},{\"end\":56391,\"start\":56203},{\"end\":56484,\"start\":56436},{\"end\":56668,\"start\":56577},{\"end\":57855,\"start\":56670},{\"end\":58074,\"start\":57969},{\"end\":58318,\"start\":58138},{\"end\":58395,\"start\":58320},{\"end\":59035,\"start\":58615},{\"end\":59244,\"start\":59194},{\"end\":59298,\"start\":59246},{\"end\":59422,\"start\":59344},{\"end\":59484,\"start\":59424},{\"end\":59823,\"start\":59486},{\"end\":59981,\"start\":59899},{\"end\":60108,\"start\":60032},{\"end\":60203,\"start\":60110},{\"end\":60447,\"start\":60326},{\"end\":60605,\"start\":60564},{\"end\":60773,\"start\":60741},{\"end\":60812,\"start\":60775},{\"end\":60986,\"start\":60814},{\"end\":61236,\"start\":61097},{\"end\":61395,\"start\":61343},{\"end\":62004,\"start\":61862},{\"end\":62061,\"start\":62006},{\"end\":62293,\"start\":62136},{\"end\":62369,\"start\":62330},{\"end\":63306,\"start\":63144},{\"end\":63595,\"start\":63567},{\"end\":63743,\"start\":63703},{\"end\":64112,\"start\":63843},{\"end\":64269,\"start\":64114},{\"end\":64636,\"start\":64371},{\"end\":64843,\"start\":64699},{\"end\":65027,\"start\":64895},{\"end\":65234,\"start\":65120},{\"end\":65381,\"start\":65351},{\"end\":66148,\"start\":66129},{\"end\":66344,\"start\":66204},{\"end\":66582,\"start\":66380},{\"end\":67205,\"start\":67001},{\"end\":67358,\"start\":67346},{\"end\":67598,\"start\":67524},{\"end\":67697,\"start\":67666},{\"end\":67827,\"start\":67775},{\"end\":67932,\"start\":67925},{\"end\":68110,\"start\":68020},{\"end\":68261,\"start\":68162},{\"end\":68740,\"start\":68734},{\"end\":68788,\"start\":68742},{\"end\":68980,\"start\":68884},{\"end\":69188,\"start\":69143},{\"end\":69291,\"start\":69240},{\"end\":69560,\"start\":69553},{\"end\":69881,\"start\":69688},{\"end\":70175,\"start\":70008},{\"end\":70382,\"start\":70197},{\"end\":70720,\"start\":70438},{\"end\":70842,\"start\":70786},{\"end\":70940,\"start\":70844},{\"end\":71159,\"start\":71122},{\"end\":71350,\"start\":71253},{\"end\":71701,\"start\":71398},{\"end\":71715,\"start\":71703},{\"end\":72078,\"start\":71839},{\"end\":72353,\"start\":72239},{\"end\":72822,\"start\":72479},{\"end\":73208,\"start\":73051},{\"end\":73274,\"start\":73210},{\"end\":73315,\"start\":73276},{\"end\":73355,\"start\":73317}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":1734,\"start\":1699},{\"attributes\":{\"id\":\"formula_1\"},\"end\":2796,\"start\":2750},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16638,\"start\":16519},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16766,\"start\":16725},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17242,\"start\":17031},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17320,\"start\":17273},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17465,\"start\":17417},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17948,\"start\":17897},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18153,\"start\":18109},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18293,\"start\":18248},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18594,\"start\":18532},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19813,\"start\":19775},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20184,\"start\":20087},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20317,\"start\":20278},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20448,\"start\":20419},{\"attributes\":{\"id\":\"formula_15\"},\"end\":20792,\"start\":20653},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21227,\"start\":21200},{\"attributes\":{\"id\":\"formula_17\"},\"end\":21358,\"start\":21335},{\"attributes\":{\"id\":\"formula_18\"},\"end\":21659,\"start\":21615},{\"attributes\":{\"id\":\"formula_19\"},\"end\":21702,\"start\":21659},{\"attributes\":{\"id\":\"formula_20\"},\"end\":21752,\"start\":21702},{\"attributes\":{\"id\":\"formula_21\"},\"end\":22107,\"start\":22070},{\"attributes\":{\"id\":\"formula_22\"},\"end\":22419,\"start\":22376},{\"attributes\":{\"id\":\"formula_23\"},\"end\":23092,\"start\":23004},{\"attributes\":{\"id\":\"formula_24\"},\"end\":23370,\"start\":23300},{\"attributes\":{\"id\":\"formula_25\"},\"end\":23690,\"start\":23655},{\"attributes\":{\"id\":\"formula_26\"},\"end\":24359,\"start\":24335},{\"attributes\":{\"id\":\"formula_27\"},\"end\":25100,\"start\":25043},{\"attributes\":{\"id\":\"formula_28\"},\"end\":25428,\"start\":25347},{\"attributes\":{\"id\":\"formula_29\"},\"end\":26633,\"start\":26283},{\"attributes\":{\"id\":\"formula_30\"},\"end\":29618,\"start\":29568},{\"attributes\":{\"id\":\"formula_31\"},\"end\":30257,\"start\":30213},{\"attributes\":{\"id\":\"formula_32\"},\"end\":31521,\"start\":31337},{\"attributes\":{\"id\":\"formula_33\"},\"end\":33931,\"start\":33908},{\"attributes\":{\"id\":\"formula_34\"},\"end\":34153,\"start\":34133},{\"attributes\":{\"id\":\"formula_35\"},\"end\":34219,\"start\":34179},{\"attributes\":{\"id\":\"formula_36\"},\"end\":34350,\"start\":34256},{\"attributes\":{\"id\":\"formula_37\"},\"end\":34491,\"start\":34350},{\"attributes\":{\"id\":\"formula_38\"},\"end\":34525,\"start\":34491},{\"attributes\":{\"id\":\"formula_39\"},\"end\":34821,\"start\":34594},{\"attributes\":{\"id\":\"formula_40\"},\"end\":35008,\"start\":34930},{\"attributes\":{\"id\":\"formula_41\"},\"end\":35383,\"start\":35300},{\"attributes\":{\"id\":\"formula_42\"},\"end\":35757,\"start\":35687},{\"attributes\":{\"id\":\"formula_43\"},\"end\":38645,\"start\":38622},{\"attributes\":{\"id\":\"formula_44\"},\"end\":38825,\"start\":38672},{\"attributes\":{\"id\":\"formula_45\"},\"end\":39179,\"start\":39016},{\"attributes\":{\"id\":\"formula_46\"},\"end\":40073,\"start\":39988},{\"attributes\":{\"id\":\"formula_47\"},\"end\":40198,\"start\":40106},{\"attributes\":{\"id\":\"formula_48\"},\"end\":40446,\"start\":40243},{\"attributes\":{\"id\":\"formula_49\"},\"end\":40820,\"start\":40539},{\"attributes\":{\"id\":\"formula_50\"},\"end\":41724,\"start\":41610},{\"attributes\":{\"id\":\"formula_51\"},\"end\":42140,\"start\":42027},{\"attributes\":{\"id\":\"formula_52\"},\"end\":42438,\"start\":42226},{\"attributes\":{\"id\":\"formula_53\"},\"end\":44006,\"start\":43957},{\"attributes\":{\"id\":\"formula_54\"},\"end\":44788,\"start\":44705},{\"attributes\":{\"id\":\"formula_55\"},\"end\":45321,\"start\":44987},{\"attributes\":{\"id\":\"formula_56\"},\"end\":45515,\"start\":45450},{\"attributes\":{\"id\":\"formula_57\"},\"end\":45723,\"start\":45622},{\"attributes\":{\"id\":\"formula_58\"},\"end\":45855,\"start\":45764},{\"attributes\":{\"id\":\"formula_59\"},\"end\":46157,\"start\":45907},{\"attributes\":{\"id\":\"formula_60\"},\"end\":46496,\"start\":46428},{\"attributes\":{\"id\":\"formula_61\"},\"end\":46950,\"start\":46547},{\"attributes\":{\"id\":\"formula_62\"},\"end\":47162,\"start\":47065},{\"attributes\":{\"id\":\"formula_63\"},\"end\":47388,\"start\":47234},{\"attributes\":{\"id\":\"formula_64\"},\"end\":47529,\"start\":47456},{\"attributes\":{\"id\":\"formula_65\"},\"end\":47656,\"start\":47626},{\"attributes\":{\"id\":\"formula_66\"},\"end\":47748,\"start\":47716},{\"attributes\":{\"id\":\"formula_67\"},\"end\":47824,\"start\":47777},{\"attributes\":{\"id\":\"formula_68\"},\"end\":47940,\"start\":47854},{\"attributes\":{\"id\":\"formula_69\"},\"end\":48214,\"start\":48119},{\"attributes\":{\"id\":\"formula_70\"},\"end\":48350,\"start\":48300},{\"attributes\":{\"id\":\"formula_71\"},\"end\":48468,\"start\":48385},{\"attributes\":{\"id\":\"formula_72\"},\"end\":48759,\"start\":48640},{\"attributes\":{\"id\":\"formula_73\"},\"end\":48946,\"start\":48871},{\"attributes\":{\"id\":\"formula_74\"},\"end\":49181,\"start\":49063},{\"attributes\":{\"id\":\"formula_75\"},\"end\":49448,\"start\":49330},{\"attributes\":{\"id\":\"formula_76\"},\"end\":49953,\"start\":49901},{\"attributes\":{\"id\":\"formula_77\"},\"end\":50565,\"start\":50413},{\"attributes\":{\"id\":\"formula_78\"},\"end\":50937,\"start\":50896},{\"attributes\":{\"id\":\"formula_79\"},\"end\":51114,\"start\":51066},{\"attributes\":{\"id\":\"formula_80\"},\"end\":51469,\"start\":51148},{\"attributes\":{\"id\":\"formula_81\"},\"end\":51630,\"start\":51610},{\"attributes\":{\"id\":\"formula_82\"},\"end\":51770,\"start\":51656},{\"attributes\":{\"id\":\"formula_83\"},\"end\":52089,\"start\":51868},{\"attributes\":{\"id\":\"formula_84\"},\"end\":52408,\"start\":52155},{\"attributes\":{\"id\":\"formula_85\"},\"end\":52747,\"start\":52479},{\"attributes\":{\"id\":\"formula_86\"},\"end\":53071,\"start\":52928},{\"attributes\":{\"id\":\"formula_87\"},\"end\":53263,\"start\":53175},{\"attributes\":{\"id\":\"formula_88\"},\"end\":53508,\"start\":53379},{\"attributes\":{\"id\":\"formula_89\"},\"end\":53691,\"start\":53545},{\"attributes\":{\"id\":\"formula_90\"},\"end\":53785,\"start\":53738},{\"attributes\":{\"id\":\"formula_91\"},\"end\":54098,\"start\":53867},{\"attributes\":{\"id\":\"formula_92\"},\"end\":54272,\"start\":54210},{\"attributes\":{\"id\":\"formula_93\"},\"end\":54683,\"start\":54518},{\"attributes\":{\"id\":\"formula_94\"},\"end\":54975,\"start\":54899},{\"attributes\":{\"id\":\"formula_95\"},\"end\":55135,\"start\":55100},{\"attributes\":{\"id\":\"formula_96\"},\"end\":55427,\"start\":55192},{\"attributes\":{\"id\":\"formula_97\"},\"end\":55705,\"start\":55589},{\"attributes\":{\"id\":\"formula_98\"},\"end\":56070,\"start\":55890},{\"attributes\":{\"id\":\"formula_99\"},\"end\":56435,\"start\":56392},{\"attributes\":{\"id\":\"formula_100\"},\"end\":56576,\"start\":56485},{\"attributes\":{\"id\":\"formula_101\"},\"end\":57968,\"start\":57856},{\"attributes\":{\"id\":\"formula_102\"},\"end\":58137,\"start\":58075},{\"attributes\":{\"id\":\"formula_103\"},\"end\":58614,\"start\":58396},{\"attributes\":{\"id\":\"formula_104\"},\"end\":59193,\"start\":59036},{\"attributes\":{\"id\":\"formula_105\"},\"end\":59343,\"start\":59299},{\"attributes\":{\"id\":\"formula_106\"},\"end\":59898,\"start\":59824},{\"attributes\":{\"id\":\"formula_107\"},\"end\":60031,\"start\":59982},{\"attributes\":{\"id\":\"formula_108\"},\"end\":60325,\"start\":60204},{\"attributes\":{\"id\":\"formula_109\"},\"end\":60563,\"start\":60448},{\"attributes\":{\"id\":\"formula_110\"},\"end\":60740,\"start\":60606},{\"attributes\":{\"id\":\"formula_111\"},\"end\":61096,\"start\":60987},{\"attributes\":{\"id\":\"formula_112\"},\"end\":61342,\"start\":61237},{\"attributes\":{\"id\":\"formula_113\"},\"end\":61861,\"start\":61396},{\"attributes\":{\"id\":\"formula_114\"},\"end\":62135,\"start\":62062},{\"attributes\":{\"id\":\"formula_115\"},\"end\":62329,\"start\":62294},{\"attributes\":{\"id\":\"formula_116\"},\"end\":63001,\"start\":62370},{\"attributes\":{\"id\":\"formula_117\"},\"end\":63143,\"start\":63001},{\"attributes\":{\"id\":\"formula_118\"},\"end\":63566,\"start\":63307},{\"attributes\":{\"id\":\"formula_119\"},\"end\":63702,\"start\":63596},{\"attributes\":{\"id\":\"formula_120\"},\"end\":63842,\"start\":63744},{\"attributes\":{\"id\":\"formula_121\"},\"end\":64370,\"start\":64270},{\"attributes\":{\"id\":\"formula_122\"},\"end\":64698,\"start\":64637},{\"attributes\":{\"id\":\"formula_123\"},\"end\":64894,\"start\":64844},{\"attributes\":{\"id\":\"formula_124\"},\"end\":65119,\"start\":65028},{\"attributes\":{\"id\":\"formula_125\"},\"end\":65330,\"start\":65235},{\"attributes\":{\"id\":\"formula_126\"},\"end\":66128,\"start\":65382},{\"attributes\":{\"id\":\"formula_127\"},\"end\":66203,\"start\":66149},{\"attributes\":{\"id\":\"formula_128\"},\"end\":66379,\"start\":66345},{\"attributes\":{\"id\":\"formula_129\"},\"end\":67000,\"start\":66583},{\"attributes\":{\"id\":\"formula_130\"},\"end\":67345,\"start\":67206},{\"attributes\":{\"id\":\"formula_131\"},\"end\":67523,\"start\":67359},{\"attributes\":{\"id\":\"formula_132\"},\"end\":67665,\"start\":67599},{\"attributes\":{\"id\":\"formula_133\"},\"end\":67774,\"start\":67698},{\"attributes\":{\"id\":\"formula_134\"},\"end\":67924,\"start\":67828},{\"attributes\":{\"id\":\"formula_135\"},\"end\":68019,\"start\":67933},{\"attributes\":{\"id\":\"formula_136\"},\"end\":68161,\"start\":68111},{\"attributes\":{\"id\":\"formula_137\"},\"end\":68733,\"start\":68262},{\"attributes\":{\"id\":\"formula_138\"},\"end\":68883,\"start\":68789},{\"attributes\":{\"id\":\"formula_139\"},\"end\":69011,\"start\":68981},{\"attributes\":{\"id\":\"formula_140\"},\"end\":69142,\"start\":69011},{\"attributes\":{\"id\":\"formula_141\"},\"end\":69239,\"start\":69189},{\"attributes\":{\"id\":\"formula_142\"},\"end\":69552,\"start\":69292},{\"attributes\":{\"id\":\"formula_143\"},\"end\":69646,\"start\":69561},{\"attributes\":{\"id\":\"formula_144\"},\"end\":69954,\"start\":69882},{\"attributes\":{\"id\":\"formula_145\"},\"end\":70007,\"start\":69954},{\"attributes\":{\"id\":\"formula_146\"},\"end\":70196,\"start\":70176},{\"attributes\":{\"id\":\"formula_147\"},\"end\":70437,\"start\":70383},{\"attributes\":{\"id\":\"formula_148\"},\"end\":70785,\"start\":70721},{\"attributes\":{\"id\":\"formula_149\"},\"end\":71121,\"start\":70941},{\"attributes\":{\"id\":\"formula_150\"},\"end\":71252,\"start\":71160},{\"attributes\":{\"id\":\"formula_151\"},\"end\":71397,\"start\":71351},{\"attributes\":{\"id\":\"formula_152\"},\"end\":71838,\"start\":71716},{\"attributes\":{\"id\":\"formula_153\"},\"end\":72238,\"start\":72079},{\"attributes\":{\"id\":\"formula_154\"},\"end\":72478,\"start\":72354},{\"attributes\":{\"id\":\"formula_155\"},\"end\":73050,\"start\":72823}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37053,\"start\":37046},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37809,\"start\":37802},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":38192,\"start\":38185}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1648,\"start\":1636},{\"attributes\":{\"n\":\"1.1\"},\"end\":6817,\"start\":6804},{\"attributes\":{\"n\":\"1.2\"},\"end\":12184,\"start\":12167},{\"end\":17324,\"start\":17322},{\"attributes\":{\"n\":\"3\"},\"end\":18970,\"start\":18930},{\"attributes\":{\"n\":\"3.1\"},\"end\":19453,\"start\":19429},{\"attributes\":{\"n\":\"3.2\"},\"end\":22332,\"start\":22304},{\"end\":24073,\"start\":24004},{\"end\":26066,\"start\":26013},{\"attributes\":{\"n\":\"3.3\"},\"end\":30610,\"start\":30581},{\"end\":34067,\"start\":34065},{\"attributes\":{\"n\":\"3.4\"},\"end\":37534,\"start\":37502},{\"end\":38896,\"start\":38894},{\"end\":39007,\"start\":39005},{\"end\":42026,\"start\":41943},{\"end\":43282,\"start\":43238},{\"end\":44609,\"start\":44583},{\"end\":44906,\"start\":44886},{\"end\":46222,\"start\":46182},{\"end\":49569,\"start\":49524},{\"end\":56102,\"start\":56072},{\"end\":65349,\"start\":65332},{\"end\":69686,\"start\":69648},{\"end\":73358,\"start\":73357},{\"end\":74064,\"start\":74029},{\"end\":80975,\"start\":80961},{\"end\":81281,\"start\":81280},{\"end\":82256,\"start\":82252},{\"end\":82704,\"start\":82670},{\"end\":83552,\"start\":83551},{\"end\":83702,\"start\":83693}]", "table": "[{\"end\":83691,\"start\":83651},{\"end\":84183,\"start\":84159}]", "figure_caption": "[{\"end\":73505,\"start\":73359},{\"end\":73808,\"start\":73508},{\"end\":74027,\"start\":73811},{\"end\":75933,\"start\":74069},{\"end\":80959,\"start\":75936},{\"end\":81131,\"start\":80977},{\"end\":81278,\"start\":81134},{\"end\":81531,\"start\":81282},{\"end\":82250,\"start\":81534},{\"end\":82668,\"start\":82257},{\"end\":83283,\"start\":82708},{\"end\":83549,\"start\":83286},{\"end\":83644,\"start\":83556},{\"end\":83651,\"start\":83647},{\"end\":84159,\"start\":83704}]", "figure_ref": "[{\"end\":37843,\"start\":37835}]", "bib_author_first_name": "[{\"end\":86863,\"start\":86862},{\"end\":86874,\"start\":86873},{\"end\":86887,\"start\":86886},{\"end\":86898,\"start\":86897},{\"end\":86907,\"start\":86906},{\"end\":87585,\"start\":87584},{\"end\":87598,\"start\":87597},{\"end\":87897,\"start\":87896},{\"end\":87910,\"start\":87909},{\"end\":88216,\"start\":88215},{\"end\":88229,\"start\":88228},{\"end\":88488,\"start\":88487},{\"end\":88744,\"start\":88743},{\"end\":88754,\"start\":88753},{\"end\":88756,\"start\":88755},{\"end\":88766,\"start\":88765},{\"end\":88927,\"start\":88926},{\"end\":89219,\"start\":89218},{\"end\":89229,\"start\":89228},{\"end\":89231,\"start\":89230},{\"end\":89240,\"start\":89239},{\"end\":89250,\"start\":89249},{\"end\":89568,\"start\":89567},{\"end\":89578,\"start\":89577},{\"end\":89580,\"start\":89579},{\"end\":89589,\"start\":89588},{\"end\":89599,\"start\":89598},{\"end\":89886,\"start\":89885},{\"end\":89896,\"start\":89895},{\"end\":89898,\"start\":89897},{\"end\":89907,\"start\":89906},{\"end\":89917,\"start\":89916},{\"end\":90218,\"start\":90217},{\"end\":90516,\"start\":90515},{\"end\":90518,\"start\":90517},{\"end\":90529,\"start\":90528},{\"end\":90540,\"start\":90539},{\"end\":90552,\"start\":90551},{\"end\":90559,\"start\":90558},{\"end\":90570,\"start\":90569},{\"end\":90986,\"start\":90985},{\"end\":90997,\"start\":90996},{\"end\":91005,\"start\":91004},{\"end\":91297,\"start\":91296},{\"end\":91558,\"start\":91557},{\"end\":91564,\"start\":91563},{\"end\":91573,\"start\":91572},{\"end\":91580,\"start\":91579},{\"end\":91963,\"start\":91962},{\"end\":91974,\"start\":91973},{\"end\":92188,\"start\":92187},{\"end\":92202,\"start\":92201},{\"end\":92212,\"start\":92211},{\"end\":92403,\"start\":92402},{\"end\":92412,\"start\":92411},{\"end\":92420,\"start\":92419},{\"end\":92755,\"start\":92754},{\"end\":92763,\"start\":92762},{\"end\":92770,\"start\":92769},{\"end\":92772,\"start\":92771},{\"end\":92782,\"start\":92781},{\"end\":92796,\"start\":92795},{\"end\":93182,\"start\":93181},{\"end\":93190,\"start\":93189},{\"end\":93434,\"start\":93433},{\"end\":93441,\"start\":93440},{\"end\":93447,\"start\":93446},{\"end\":93461,\"start\":93460},{\"end\":93463,\"start\":93462},{\"end\":93473,\"start\":93472},{\"end\":93475,\"start\":93474},{\"end\":93792,\"start\":93791},{\"end\":93799,\"start\":93798},{\"end\":93813,\"start\":93812},{\"end\":93815,\"start\":93814}]", "bib_author_last_name": "[{\"end\":86871,\"start\":86864},{\"end\":86884,\"start\":86875},{\"end\":86895,\"start\":86888},{\"end\":86904,\"start\":86899},{\"end\":86910,\"start\":86908},{\"end\":87347,\"start\":87338},{\"end\":87595,\"start\":87586},{\"end\":87604,\"start\":87599},{\"end\":87907,\"start\":87898},{\"end\":87913,\"start\":87911},{\"end\":88226,\"start\":88217},{\"end\":88232,\"start\":88230},{\"end\":88495,\"start\":88489},{\"end\":88751,\"start\":88745},{\"end\":88763,\"start\":88757},{\"end\":88774,\"start\":88767},{\"end\":88934,\"start\":88928},{\"end\":89226,\"start\":89220},{\"end\":89237,\"start\":89232},{\"end\":89247,\"start\":89241},{\"end\":89258,\"start\":89251},{\"end\":89575,\"start\":89569},{\"end\":89586,\"start\":89581},{\"end\":89596,\"start\":89590},{\"end\":89607,\"start\":89600},{\"end\":89893,\"start\":89887},{\"end\":89904,\"start\":89899},{\"end\":89914,\"start\":89908},{\"end\":89925,\"start\":89918},{\"end\":90225,\"start\":90219},{\"end\":90526,\"start\":90519},{\"end\":90537,\"start\":90530},{\"end\":90549,\"start\":90541},{\"end\":90556,\"start\":90553},{\"end\":90567,\"start\":90560},{\"end\":90577,\"start\":90571},{\"end\":90994,\"start\":90987},{\"end\":91002,\"start\":90998},{\"end\":91020,\"start\":91006},{\"end\":91305,\"start\":91298},{\"end\":91561,\"start\":91559},{\"end\":91570,\"start\":91565},{\"end\":91577,\"start\":91574},{\"end\":91585,\"start\":91581},{\"end\":91971,\"start\":91964},{\"end\":91978,\"start\":91975},{\"end\":92199,\"start\":92189},{\"end\":92209,\"start\":92203},{\"end\":92222,\"start\":92213},{\"end\":92409,\"start\":92404},{\"end\":92417,\"start\":92413},{\"end\":92435,\"start\":92421},{\"end\":92760,\"start\":92756},{\"end\":92767,\"start\":92764},{\"end\":92779,\"start\":92773},{\"end\":92793,\"start\":92783},{\"end\":92804,\"start\":92797},{\"end\":93187,\"start\":93183},{\"end\":93194,\"start\":93191},{\"end\":93438,\"start\":93435},{\"end\":93444,\"start\":93442},{\"end\":93458,\"start\":93448},{\"end\":93470,\"start\":93464},{\"end\":93482,\"start\":93476},{\"end\":93796,\"start\":93793},{\"end\":93810,\"start\":93800},{\"end\":93822,\"start\":93816}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11792808},\"end\":87284,\"start\":86799},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3522832},\"end\":87527,\"start\":87286},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9346356},\"end\":87803,\"start\":87529},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8794713},\"end\":88160,\"start\":87805},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3426587},\"end\":88422,\"start\":88162},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":115963355},\"end\":88686,\"start\":88424},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3119488},\"end\":88924,\"start\":88688},{\"attributes\":{\"id\":\"b7\"},\"end\":89167,\"start\":88926},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13824363},\"end\":89463,\"start\":89169},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":35265682},\"end\":89883,\"start\":89465},{\"attributes\":{\"doi\":\"arXiv:1710.11606\",\"id\":\"b10\"},\"end\":90142,\"start\":89885},{\"attributes\":{\"id\":\"b11\"},\"end\":90417,\"start\":90144},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11657534},\"end\":90883,\"start\":90419},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":218654665},\"end\":91294,\"start\":90885},{\"attributes\":{\"id\":\"b14\"},\"end\":91473,\"start\":91296},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11513606},\"end\":91880,\"start\":91475},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14112046},\"end\":92170,\"start\":91882},{\"attributes\":{\"id\":\"b17\"},\"end\":92344,\"start\":92172},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":743205},\"end\":92663,\"start\":92346},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6840558},\"end\":93133,\"start\":92665},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":51984738},\"end\":93390,\"start\":93135},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14198632},\"end\":93708,\"start\":93392},{\"attributes\":{\"doi\":\"arXiv:1711.10456\",\"id\":\"b22\"},\"end\":94017,\"start\":93710}]", "bib_title": "[{\"end\":86860,\"start\":86799},{\"end\":87336,\"start\":87286},{\"end\":87582,\"start\":87529},{\"end\":87894,\"start\":87805},{\"end\":88213,\"start\":88162},{\"end\":88485,\"start\":88424},{\"end\":88741,\"start\":88688},{\"end\":89216,\"start\":89169},{\"end\":89565,\"start\":89465},{\"end\":90215,\"start\":90144},{\"end\":90513,\"start\":90419},{\"end\":90983,\"start\":90885},{\"end\":91555,\"start\":91475},{\"end\":91960,\"start\":91882},{\"end\":92400,\"start\":92346},{\"end\":92752,\"start\":92665},{\"end\":93179,\"start\":93135},{\"end\":93431,\"start\":93392}]", "bib_author": "[{\"end\":86873,\"start\":86862},{\"end\":86886,\"start\":86873},{\"end\":86897,\"start\":86886},{\"end\":86906,\"start\":86897},{\"end\":86912,\"start\":86906},{\"end\":87349,\"start\":87338},{\"end\":87597,\"start\":87584},{\"end\":87606,\"start\":87597},{\"end\":87909,\"start\":87896},{\"end\":87915,\"start\":87909},{\"end\":88228,\"start\":88215},{\"end\":88234,\"start\":88228},{\"end\":88497,\"start\":88487},{\"end\":88753,\"start\":88743},{\"end\":88765,\"start\":88753},{\"end\":88776,\"start\":88765},{\"end\":88936,\"start\":88926},{\"end\":89228,\"start\":89218},{\"end\":89239,\"start\":89228},{\"end\":89249,\"start\":89239},{\"end\":89260,\"start\":89249},{\"end\":89577,\"start\":89567},{\"end\":89588,\"start\":89577},{\"end\":89598,\"start\":89588},{\"end\":89609,\"start\":89598},{\"end\":89895,\"start\":89885},{\"end\":89906,\"start\":89895},{\"end\":89916,\"start\":89906},{\"end\":89927,\"start\":89916},{\"end\":90227,\"start\":90217},{\"end\":90528,\"start\":90515},{\"end\":90539,\"start\":90528},{\"end\":90551,\"start\":90539},{\"end\":90558,\"start\":90551},{\"end\":90569,\"start\":90558},{\"end\":90579,\"start\":90569},{\"end\":90996,\"start\":90985},{\"end\":91004,\"start\":90996},{\"end\":91022,\"start\":91004},{\"end\":91307,\"start\":91296},{\"end\":91563,\"start\":91557},{\"end\":91572,\"start\":91563},{\"end\":91579,\"start\":91572},{\"end\":91587,\"start\":91579},{\"end\":91973,\"start\":91962},{\"end\":91980,\"start\":91973},{\"end\":92201,\"start\":92187},{\"end\":92211,\"start\":92201},{\"end\":92224,\"start\":92211},{\"end\":92411,\"start\":92402},{\"end\":92419,\"start\":92411},{\"end\":92437,\"start\":92419},{\"end\":92762,\"start\":92754},{\"end\":92769,\"start\":92762},{\"end\":92781,\"start\":92769},{\"end\":92795,\"start\":92781},{\"end\":92806,\"start\":92795},{\"end\":93189,\"start\":93181},{\"end\":93196,\"start\":93189},{\"end\":93440,\"start\":93433},{\"end\":93446,\"start\":93440},{\"end\":93460,\"start\":93446},{\"end\":93472,\"start\":93460},{\"end\":93484,\"start\":93472},{\"end\":93798,\"start\":93791},{\"end\":93812,\"start\":93798},{\"end\":93824,\"start\":93812}]", "bib_venue": "[{\"end\":87047,\"start\":86988},{\"end\":88540,\"start\":88527},{\"end\":91680,\"start\":91642},{\"end\":92899,\"start\":92861},{\"end\":86986,\"start\":86912},{\"end\":87398,\"start\":87349},{\"end\":87650,\"start\":87606},{\"end\":87975,\"start\":87915},{\"end\":88283,\"start\":88234},{\"end\":88525,\"start\":88497},{\"end\":88787,\"start\":88776},{\"end\":89028,\"start\":88936},{\"end\":89288,\"start\":89260},{\"end\":89653,\"start\":89609},{\"end\":89987,\"start\":89943},{\"end\":90267,\"start\":90227},{\"end\":90628,\"start\":90579},{\"end\":91071,\"start\":91022},{\"end\":91339,\"start\":91307},{\"end\":91640,\"start\":91587},{\"end\":92008,\"start\":91980},{\"end\":92185,\"start\":92172},{\"end\":92486,\"start\":92437},{\"end\":92859,\"start\":92806},{\"end\":93240,\"start\":93196},{\"end\":93528,\"start\":93484},{\"end\":93789,\"start\":93710}]"}}}, "year": 2023, "month": 12, "day": 17}