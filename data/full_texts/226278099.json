{"id": 226278099, "updated": "2023-10-06 09:00:53.882", "metadata": {"title": "HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification", "authors": "[{\"first\":\"Yichen\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Shikha\",\"last\":\"Bordia\",\"middle\":[]},{\"first\":\"Zheng\",\"last\":\"Zhong\",\"middle\":[]},{\"first\":\"Charles\",\"last\":\"Dognin\",\"middle\":[]},{\"first\":\"Maneesh\",\"last\":\"Singh\",\"middle\":[]},{\"first\":\"Mohit\",\"last\":\"Bansal\",\"middle\":[]}]", "venue": "FINDINGS", "journal": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We introduce HoVer (HOppy VERification), a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is supported or not-supported by the facts. In HoVer, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes. Moreover, most of the 3/4-hop claims are written in multiple sentences, which adds to the complexity of understanding long-range dependency relations such as coreference. We show that the performance of an existing state-of-the-art semantic-matching model degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of many-hop reasoning to achieve strong results. We hope that the introduction of this challenging dataset and the accompanying evaluation task will encourage research in many-hop fact retrieval and information verification.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2011.03088", "mag": "3101682885", "acl": "2020.findings-emnlp.309", "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2011-03088", "doi": "10.18653/v1/2020.findings-emnlp.309"}}, "content": {"source": {"pdf_hash": "34a5282dcc7c860755386979833e8d48e743e901", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf", "status": "HYBRID"}}, "grobid": {"id": "eef88934053a2542189b709c2ddcf64d2c8b229a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/34a5282dcc7c860755386979833e8d48e743e901.txt", "contents": "\nHOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification\nNovember 16 -20, 2020\n\nYichen Jiang yichenj@cs.unc.edu \nUNC Chapel Hill \u2021 Verisk\n\n\nShikha Bordia shikha.bordia@verisk.com \nUNC Chapel Hill \u2021 Verisk\n\n\nZheng Zhong zheng.zhong@verisk.com \nUNC Chapel Hill \u2021 Verisk\n\n\nCharles Dognin charles.dognin@verisk.com \nUNC Chapel Hill \u2021 Verisk\n\n\nManeesh Singh msingh@verisk.com \nUNC Chapel Hill \u2021 Verisk\n\n\nMohit Bansal mbansal@cs.unc.edu \nUNC Chapel Hill \u2021 Verisk\n\n\nHOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification\n\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\nthe 2020 Conference on Empirical Methods in Natural Language Processing: FindingsNovember 16 -20, 20203441\nWe introduce HOVER (HOppy VERification), a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is SUPPORTED or NOT-SUPPORTED by the facts. In HOVER, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes. Moreover, most of the 3/4-hop claims are written in multiple sentences, which adds to the complexity of understanding long-range dependency relations such as coreference. We show that the performance of an existing stateof-the-art semantic-matching model degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of many-hop reasoning to achieve strong results. We hope that the introduction of this challenging dataset and the accompanying evaluation task will encourage research in many-hop fact retrieval and information verification. 1 * Equal contribution. 1  We make HoVer dataset publicly available at https://hover-nlp.github.io\n\nIntroduction\n\nThe proliferation of social media platforms and digital content has been accompanied by a rise in deliberate disinformation and hoaxes, leading to polarized opinions among masses. With the increasing number of inexact statements, there is a large interest in a fact-checking system that can verify claims based on automatically retrieved facts and evidence. FEVER (Thorne et al., 2018) is an open-domain fact extraction and verification dataset closely related to this real-world application. However, more than 87% of the claims in FEVER require information from a single Wikipedia article, while real-world \"claims\" might refer to information from multiple sources. QA datasets like HOT-POTQA (Yang et al., 2018) and QAngaroo (Welbl et al., 2018) represent the first efforts to challenge models to reason with information from three documents at most. However, Chen and Durrett (2019) and Min et al. (2019) show that single-hop models can achieve good results in these multi-hop datasets. Moreover, most models were also shown to degrade in adversarial evaluation (Perez et al., 2020), where word-matching reasoning shortcuts are suppressed by extra adversarial documents (Jiang and Bansal, 2019). In the HOTPOTQA open-domain setting, the two supporting documents can be accurately retrieved by a neural model exploiting a single hyperlink (Nie et al., 2019b;Asai et al., 2020).\n\nHence, while providing very useful starting points for the community, FEVER is mostly restricted to a single-hop setting and existing multihop QA datasets are limited by the number of reasoning steps and the word overlapping between the question and all evidence. An ideal multi-hop example should have at least one piece of evidence (supporting document) that cannot be retrieved with high precision by shallowly performing direct semantic matching with only the claim. Instead, uncovering this document requires information from previously retrieved documents. In this paper, we try to address these issues by creating HOVER (i.e., HOppy VERification) whose claims (1) require evidence from as many as four English Wikipedia articles and (2) contain significantly less semantic overlap between the claims and some supporting documents to avoid reasoning shortcuts.We create HOVER with 26k claims in three stages. In stage 1 (left box in Fig. 1), we ask a group of trained and evaluated crowd-workers to rewrite the questionanswer pairs from HOTPOTQA (Yang et al., 2018) into claims that mention facts from two English   Wikipedia articles. We then introduce extra hops 2 to a subset of these 2-hop claims by asking crowdworkers to substitute an entity in the claim with information from another English Wikipedia article that describes the original entity. We then repeat this process on these 3-hop claims to further create 4-hop claims. To make many-hop claims more natural and readable, we encourage crowdworkers to write the 3/4-hop claims in multiple sentences and connect them using coreference. An entire evolution history from 2-hop claims to 3/4hop claims is presented in the leftmost box in Fig. 1 and Table 1, where the latter further presents the reasoning graphs of various shapes embodied by the many-hop claims.\n\nIn stage 2 (the central box in Fig. 1), we create claims that are not supported by the evidence by mutating the claims collected in stage 1 with a combination of automatic word/entity substitution and human editing. Specifically, we ask the trained crowd-workers to rewrite a claim by making it either more specific/general than or negating the original claim. We ensure the quality of the machine-generated claims using human validation detailed in Sec. 2.2. In stage 3, we follow Thorne et al. (2018) to label the claims as 2 The number of hops of a claim is the same as the number of supporting documents for this claim. SUPPORTED, REFUTED, or NOTENOUGHINFO. However, we find that the decision between RE-FUTED and NOTENOUGHINFO can be ambiguous in many-hop claims and even the high-quality, trained annotators from Appen, instead of Mturk, cannot consistently choose the correct label from these two classes. Recent works (Pavlick and Kwiatkowski, 2019;Chen et al., 2020a) have raised concern over the uncertainty of NLI tasks with categorical labels and proposed to shift to a probabilistic scale. Since this work is mainly targeting the many-hop retrieval, we combine the REFUTED and NOTENOUGHINFO into a single class, namely NOT-SUPPORTED. This binary classification task is still challenging for models given the incomplete evidence retrieved, as we will explain later.\n\nNext, we introduce the baseline system and demonstrate its limited ability in addressing manyhop claims. Following a state-of-the-art system (Nie et al., 2019a) for FEVER, we build the baseline with a TF-IDF document retrieval stage and three BERT models fine-tuned to conduct document retrieval, sentence selection, and claim verification respectively. We show that the bi-gram TF-IDF (Chen et al., 2017)'s top-100 retrieved documents can only recover all supporting documents in 80% of 2-hop claims, 39% of 3-hop claims, and 15% of 4-hop claims. The performance of down- Figure 1: Data Collection flow chart for HOVER. In the first stage, we create claims from HOTPOTQA, validate them and extend to more hops. In the second stage, we apply a variety of mutations to the claims performed by crowd-workers and automatic methods. In the final stage, we ask crowd-workers to label the resulting claims. stream neural document and sentence retrieval models also degrades significantly as the number of supporting documents increases. These results suggest that the possibility of a word-matching shortcut is reduced significantly in 3/4-hop claims. Because the complete set of evidence cannot be retrieved for most claims, the claim verification model only achieves 73.7% accuracy in classifying the claims as SUPPORTED or NOT-SUPPORTED, while the model given all evidence predicts 81.2% of the claims correctly under this oracle setting. We further provide a sanity check to show that the model can only correctly predict the labels for 63.7% of claims without any evidence. This suggests that the claims contain limited clues that can be exploited independently of the evidence during the verification, and a strong retrieval method capable of many-hop reasoning can improve the claim verification accuracy. In terms of HOVER as an integrated task, the best pipeline can only retrieve the complete set of evidence and correctly verify the claim for 14.9% of dev set examples, falling behind the 81% human performance significantly.\n\nOverall, we provide the community with a novel, challenging and large many-hop fact extraction and claim verification dataset with over 26k claims that can be comprised of multiple sentences connected by coreference, and require evidence from as many as four Wikipedia articles. We verify that the claims are challenging, especially in the 3/4hop cases, by showing the limited performance of a state-of-the-art system for both retrieval and verification. We hope that the introduction of HOVER and the accompanying evaluation task will encourage research in complex many-hop reasoning for fact extraction and claim verification.\n\n\nData Collection\n\nThe many-hop fact verification dataset, HOVER, is a collection of human-written claims about facts in English Wikipedia articles created in three main stages (shown in Fig. 1). In the Claim Creation stage (Sec. 2.1), we ask trained annotators on Appen 3 to create claims by rewriting question-answer pairs (Sec. 2.1.1) from the HOTPOTQA dataset 4 (Yang et al., 2018). The validated 2-hop claims are then extended to (Sec. 2.1.2) include facts from more Wikipedia articles. In the Claim Mutation stage (Sec. 2.2), claims generated from the above two processes are mutated with human editing and automatic word substitution. Finally, in the Claim Labeling stage (Sec. 2.3), trained crowdworkers classify the original and mutated claims as either SUPPORTED, REFUTED or NOTENOUGH-INFO. We merge the latter two labels into a single NOT-SUPPORTED class, owing to ambiguity explained in Sec. 2.3. The guidelines and design for every task are shown in the appendix.\n\n\nClaim Creation\n\nThe goal is to create claims by rewriting questionanswer pairs from HOTPOTQA (Yang et al., 2018) and extend these claims to include facts from more documents (shown in the left box in Fig. 1).\n\n\nCreating 2-Hop Claims from HOTPOTQA\n\nTo begin with, crowd-workers are asked to combine question-answer pairs to write claims. These claims require information from two Wikipedia articles. Based on the guidelines, the annotators can neither exclude any information from the original QA pairs nor introduce any new information.\n\nValidating Created Claims. We then train another group of crowd-workers to validate the claims created from Sec. 2.1.1. To ensure the quality of the claims, we only keep those where at least two out of three annotators agree that it is a valid statement and covers the same information from the original question-answer pair. These validated 2-hop claims are automatically labeled as SUPPORTED.\n\n\nExtending to 3-Hop and 4-Hop Claims\n\nConsider a valid 2-hop claim c from Sec. 2.1.1 that includes facts from 2 supporting documents A = {a 1 , a 2 }. We extend c to a new, 3-hop claim c by substituting a named entity e in c with information from another English Wikipedia article a 3 that describes e. The resulting 3-hop claim\u0109 hence has 3 supporting document {a 1 , a 2 , a 3 }. We then repeat this process to extend the 3-hop claims to include facts from the forth documents. We use two methods to substitute different entities e, leading to 4-hop claims with various reasoning graphs.\n\nMethod 1. We consider the entity e to be the title of a document a k \u2208 A. We search for English Wikipedia articles\u00e2 / \u2208 A whose text body mentions e's hyperlink. We exclude the\u00e2 whose title is mentioned in the text body of one of the document in A. We then ask crowd-workers to select a 3 from a candidate group of\u00e2 and write the 3-hop claim\u0109 by replacing e in c with a relative clause or phrase using information from a sentence s \u2208 a 3 .\n\nMethod 2. In this method, we consider e to be any other entity in the claim, which is not the title of a document a k \u2208 A but exists as a Wiki hyperlink in the text body of one document in A. The last 4-hop claim in Table 1 is created via this method and the entity e is \"NASCAR\". The remaining efforts are the same as Method 1 as we search for English Wikipedia articles\u00e2 / \u2208 A whose text body mentions e's hyperlink and ask crowd-workers to replace e with information from a 3 .\n\nTask Setup. We employ Method 1 to extend the collected 2-hop claims, for which we can find at least one\u00e2. Then we use both Method 1 and Method 2 to extend the 3-hop claims to 4hop claims of various reasoning graphs. In a 3document reasoning graph (a chain), the title of the middle document is substituted out during the extension from the 2-hop claim and thus does not exist in the 3-hop claim. Therefore, Method 1, which replaces the title of one of the three documents in the claim, can only be applied to either the leftmost or the rightmost document. In order to append the fourth document to the middle document in the 3-hop reasoning chain, we have to substitute a non-title entity in the 3-hop claim, which can be achieved by Method 2. In Table 1, the last 4-hop claim with a star-shape reasoning graph is the result of applying Method 1 for 3-hop extension and Method 2 for the 4-hop extension, while the first two 4-hop claims are created by applying Method 1 twice. We ask the crowd-workers to submit the index of the sentence and add this sentence to the supporting facts of the 2-hop claim to form the supporting facts of this new, 3-hop claim.\n\n\nClaim Mutation\n\nWe mutate the claims created in Sec. 2.1 to collect new claims that are not necessarily supported by the facts. We employ four types of mutation methods (shown in the middle column of Fig. 1) that are explained in the following sections.\n\nMaking a Claim More Specific or General. A more specific claim contains information that is not in the original claim. A more general claim contains less information than the original one. We design guidelines (shown in the appendix) and quizzes to train the annotators to use natural logic. We constrain the annotators from replacing the supporting document titles in a claim to ensure that verifying this claim requires the same set of evidence as the original claims. We also forbid mutating location entities (e.g., Manhattan \u2212 \u2192 New York) as this may introduce external evidence (\"Manhattan is in New York\") that is not in the original set of evidence.\n\nAutomatic Word Substitution. In this mutation process, we first sample a word from the claim that is neither a named entity nor a stopword. We then use a BERT-large model (Devlin et al., 2019) to predict this masked token, as we found that human annotators usually fall into a small, fixed vocabulary when thinking of the new word. We ask 3 annotators to validate whether each claim mutated by BERT is logical and grammatical to further ensure the quality and keep the claims where at least 2 workers decide it suffices our criteria. 500 BERTmutated claims passed the validation and labeling.\n\nAutomatic Entity Substitution. We design a separate mutation process to substitute named entities in the claims. First, we perform Named Entity Recognition on the claims. We then randomly select a named entity that is not the title of any supporting document, and replace it with an entity of the same type sampled from the context. 5\n\nClaim Negation. Understanding negation cues and their scope is of significant importance to NLP models. Hence, we ask crowd-workers to negate the claims by removing or adding negation words (e.g., not), or substituting a phrase with its antonyms. However, it is shown in Schuster et al.\n\n(2019) that models can exploit this bias as most claims containing a negation word have the label REFUTED. To mitigate this bias, we only include a subset of negated 2-hop claims where 60% of them don't include any explicit negation word.\n\n\nClaim Labeling\n\nIn this stage (the right column in Fig. 1), we ask annotators to assign one of the three labels (SUPPORTED, REFUTED, or NOTENOUGHINFO) to all 3/4-hop claims (original and mutated) as well as 2-hop mutated claims. The workers are asked to make judgments based on the given supporting facts solely without using any external knowledge. Each claim is annotated by five crowd-workers and we only keep those claims where at least three agree on the same label, resulting in a fleiss-kappa inter-annotator agreement score of 0.63. 6\n\nNOT-SUPPORTED Claims. The demarcation between NOTENOUGHINFO or REFUTED is subjective and the threshold could vary based on the world knowledge and perspective of annotators. Consider the claim \"Christian Bale starred in a 2010 movie directed by an American director\" and the fact \"English director Christopher Nolan directed the Dark Knight in 2010\". Although the \"American\" in the claim directly contradicts the word \"English\" in the fact, this claim should still be classified as NOTE-NOUGHINFO as Bale could have starred in another 2010 film by an American director. More of such examples are provided in the appendix. In this case, a piece of evidence contradicts a relative clause in the claim but does not refute the entire claim. Similar problems regarding the uncertainty of NLI tasks have been pointed out in previous works (Zaenen et al., 2005;Pavlick and Kwiatkowski, 2019;Chen et al., 2020a).\n\nWe design an exhaustive list of rules with abundant examples, trying to standardize the decision process for the labeling task. We acknowledge the difficulty and cognitive load it sometimes bears on well-informed annotators to think of corner cases like the example shown above. The final annotated data revealed the ambiguity between NOTE-NOUGHINFO and REFUTED labels, as in a 100sample human validation, only 63% of the labels assigned by another annotator match the majority labels collected. Hence we combine the REFUTED and NOTENOUGHINFO into a single class, namely NOT-SUPPORTED. 90% of the validation labels match the annotated labels under this binary classification setting.\n\n\nAnnotator Details\n\nMost annotators are native English speakers from the UK, US, and Canada. For all tasks, we first launch small-scale pilots to train annotators and incorporate their feedback for at least two rounds. Then for claim creation and extension tasks, we manually evaluate the claims they created and only keep those workers who can write claims of high quality. For claim validation (Sec. 2.1.1) and labeling (Sec. 2.3) tasks, we additionally launch quizzes and annotators scoring 80% accuracy in the quiz are then admitted to the job. During the job, we use test questions to ensure their consistent performance. Crowd-workers whose test-question accuracy drops below 82% are rejected from the tasks and all his/her annotations are re-annotated by other qualified workers. As suggested in Ram\u00edrez et al.\n\n(2019), we highlight the mutated words during the labeling tasks to reduce the mental workload on Split #Hops SUPPORTED NOT-SUP TOTAL   Train   2  6496  2556  9052  3  3271  2813  6084  4  1256  1779  3035  Total  11023  7148  18171   Dev   2  521  605  1126  3  968  867  1835  4  511  528  1039  Total  2000  2000  4000   Test  -2000  2000  4000   Total  -15023 11148 26171  (Yang et al., 2018). The 2-hop, 3-hop and 4-hop claims have a mean length of 19.0, 24.2, and 31.6 tokens respectively as compared to a mean length of 9.4 tokens in Thorne et al. (2018).\n\nDiverse Many-Hop Reasoning Graphs. As questions from HOTPOTQA (Yang et al., 2018) require two supporting documents, our 2-hop claims created from HOTPOTQA question-answer pairs inherit the same 2-node reasoning graph as shown in the first row in Table 1. However, as we extend the original 2-hop claims to more hops using approaches described in Sec. 2.1.2, we achieve manyhop claims with diverse reasoning graphs. Every node in a reasoning graph is a unique document that contains evidence, and an edge that connects two nodes represents a hyperlink from the original Wikipedia document or a comparison between two titles. As shown in Table 1, we have three unique 4-hop reasoning graphs that are derived from the 3-hop reasoning graph by appending the 4th node to one of the existing nodes in the graph.\n\nQualitative Analysis. The process of removing a bridge entity and replacing it with a relative clause or phrase adds a lot of information to a single hypothesis. Therefore, some of the 3/4-hop claims are of relatively longer length and have complex syntactic and reasoning structure. In systematic aptitude tests as well, humans are assessed on synthetically designed complex logical puzzles. These tests require critical problem solving abilities and are effective in evaluating logical reasoning capabilities of humans and AI models. Overly complicated claims are discarded in our labeling stage if they are reported as ungrammatical or incomprehensible by the annotators. The resulting examples form a challenging task of evidence retrieval and multi-hop reasoning.\n\n\nBaseline System\n\nFollowing a state-of-the-art system (Nie et al., 2019a) on FEVER (Thorne et al., 2018), we build a pipeline system of fact extraction and claim verification. 7 This provides an initial baseline for future works and its performance indicates the many-hop challenge posed by HOVER.\n\nRule-based Document Retrieval. We use the document retrieval component from Chen et al. (2017) that returns the k closest Wikipedia documents for a query using cosine similarity between binned uni-gram and bi-gram TF-IDF vectors. This step outputs a set P r of k r document that are processed by downstream neural models.\n\nNeural-based Document Retrieval. Similar to the retrieval model in Nie et al. (2019a), the BERTbase model (Devlin et al., 2019) takes a single document p \u2208 P r and the claim c as the input, and outputs a score that reflects the relatedness between p and c. We select a set P n of top k p documents having relatedness scores higher than a threshold of \u03ba p .\n\nNeural-based Sentence Selection. We fine-tune another BERT-base model that encodes the claim c and all sentences from a single document p \u2208 P n , and predicts the sentence relatedness scores using the first token of every sentence. We select a set   S n of top sentences from the entire P n having relatedness scores higher than a threshold of \u03ba s .\n\nClaim Verification Model. We fine-tune a BERT-base model for recognizing textual entailment between the claim c and the retrieved evidence S n . We feed the claim and retrieved evidence, separated by a [SEP] token, as the input to the model and perform a binary classification based on the output representation of the [CLS] token at the first position.\n\n\nExperiments and Results\n\nWe explain the evaluation metrics we use and report the results of the baseline in three evaluation tasks.\n\n\nEvaluation Metrics\n\nWe evaluate the final accuracy of the claim verification task to predict a claim as SUPPORTED or NOT-SUPPORTED. The document and sentence retrieval are evaluated by the exact-match and F1 scores between the predicted document/sentencelevel evidence and the ground-truth evidence for the claim. We refer to the appendix for the detailed experimental setups and hyper-parameters.\n\n\nDocument Retrieval Results\n\nThe results in Table 3 show the task becomes significantly harder for the bi-gram TF-IDF when the number of supporting documents increases. This decline in single-hop word-matching retrieval rate suggests that the method to extend the reasoning hops (Sec. 2.1.2) is effective in terms of promoting multi-hop document retrieval and minimizing word-matching reasoning shortcuts. We then use a   BERT-base model (1st row in Table 4) to re-rank the top-20 documents returned by the TF-IDF. The \"BERT \" (2nd row) is trained with an oracle training set containing all golden documents. Overall, the performances of the neural models are limited by the low recall of the 20 input documents and the F1 scores degrade as the number of hops increase. The oracle model (3rd row) is the same as \"BERT \" but evaluated on the oracle data. It indicates an upper bound of the BERT retrieval model given a perfect rule-based retrieval method. These findings again demonstrate the high quality of the many-hop claims we collected, for which the reasoning shortcuts are significantly reduced because of the approach described in Sec. 2.1.2.\n\n\nSentence Selection Results\n\nWe evaluate the neural-based sentence selection models by re-ranking the sentences within the top-5 documents returned by the best neural document retrieval method. For \"BERT \" (2nd row in Table 5), we again ensured that all golden documents are contained within the 5 input documents during the training. We then measure the oracle result by evaluating \"BERT \" on the dev set with all golden documents presented. This suggests an upper bound of the sentence retrieval model given a perfect document retrieval method. The same trend holds as the F1 scores decrease significantly as the number of hops increases. 8   \n\n\nClaim Verification Results\n\nIn an oracle (1st row in Table 6) setting where the complete set of evidence is provided, the model achieves 81.2% accuracy in verifying the claims. We also conduct a sanity check in a claim-only environment (2nd row) where the model can only exploit the bias in the claims without any evidence, in which the model achieves 63.7% accuracy. Although the model can exploit limited biases within the claims to achieve higher-than-random accuracy without any evidence, it is still 17.5% worse than the model given the complete evidence. This suggests the NLI model can benefit from an accurate evidence retrieval model significantly.\n\n\nFull Pipeline Results\n\nThe full pipeline (\"BERT+Retr\" in Table 7) uses the sentence-level evidence retrieved by the best document/sentence retrieval models as the input to the NLI model, while the \"BERT+Gold\" is the oracle in Table 6 but evaluated with retrieved evidence instead. We further propose the HOVER Score, which is the percentage of the examples where the the model must retrieve at least one supporting fact from every supporting document and predict the correct label. We show the performance of the best model (BERT+Gold in Table 7) on the test set in Table 8. Overall, the best pipeline can only retrieve the complete set of evidence and predict the correct label for 14.9% of examples on the dev set and 15.32% of examples on the test set, suggesting that our task is indeed more challenging than the previous work of this kind.\n\n\nHuman Performance\n\nWe measure the human performance on 100 sampled claims. In the document (Table 4) and sentence retrieval (Table 5) tasks, the human F1 score is 37.9% and 33.1% higher than the best base-line respectively. In the oracle claim verification (Table 6), the human accuracy is 90%, i.e., 8.8% higher than BERT's accuracy. Comparing on the full pipeline (Table 7), the human accuracy and human HOVER score are 88% and 81%, while the best BERT model only obtains 67.6% accuracy and 14.9% HOVER score respectively on the dev set. Human evaluation setup is explained in appendix.\n\n\nRelated Work\n\nNatural Language Inference and Fact Verification. Textual Entailment and natural language inference (NLI) datasets like RTE (Dagan et al., 2010), SNLI (Bowman et al., 2015) or MNLI (Williams et al., 2018) consist of single sentence premise. In this task, every premise-hypothesis pair is labeled as ENTAILMENT, CONTRADICTION, or NEUTRAL. Another related task is fact verification, where claims (hypothesis) are checked against facts (premise). Vlachos and Riedel (2014) and Ferreira and Vlachos (2016) collected statements from Poli-tiFact, a Pulitzer Prize-winning fact-checking website that covers political topics. The veracity of these facts is crowd-sourced from journalists, public figures and ordinary citizens. However, developing machine learning based assessments on datasets with less than five hundred datapoints is not feasible. Wang (2017) introduced LIAR which includes 12,832 labeled claims from PolitiFact. The dataset is based on the metadata of the speaker and their judgments. However, the evidence supporting the statements are not provided. A recent work in Table-based fact verification (Chen et al., 2020b) points out the difficulty of collecting accurate neutral labels and leaves out those neutral claims at the claim creation phase. We instead merge neutral (NOTENOUGHINFO) claims with REFUTED claims into a single class.\n\nFact Extraction and Verification. Thorne et al. (2018) introduced FEVER, a fact extraction and verification dataset. It consists of single sentence claims that are verified against the pieces of evidence retrieved from at most two documents. In our dataset, the claims vary in size from one sentence to one paragraph and the pieces of evidence are derived from information ranging from one document to four documents. More recently, Thorne et al. (2019) introduced the FEVER2.0 shared task which challenge participants to fact verify claims using evidence from Wikipedia and to attack other participant's system with adversarial models. In HOVER, the claim needs verification from multiple documents. Prior to verification, the relevant documents and the context inside these documents must also be retrieved accurately. More recently,  enriched the claim with multiple perspectives that support or oppose the claim in different scale. Each perspective can also be verified by existing facts. MultiFC (Augenstein et al., 2019) is a dataset of naturally occurred claims from multiple domains. The contribution of these two fact-checking dataset is orthogonal to ours.  (Clark et al., 2020) are synthetic datasets created to challenge models' ability to understand the complex reasoning in natural language. With the same motive, HOVER is created by humans following the guidelines and rules designed to enforce a multihop structure within the claim. Compared to synthetic datasets like RuleTaker, HOVER's examples are more natural as they are created and verified by humans and cover a wider range of vocabulary and linguistic variations. This is extremely important because models usually get close-to-perfect perfor-mance (e.g., 99% in RuleTaker) on these synthetic datasets.\n\n\nMulti-Hop\n\n\nConclusion\n\nWe present HOVER, a fact extraction and verification dataset requiring evidence retrieval from as many as four Wikipedia articles that form reasoning graphs of diverse shapes. We show that the performance of existing state-of-the-art models degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of robust many-hop reasoning in achieving strong results. We hope that HOVER will encourage the development of models capable of performing complex many-hop reasoning in the tasks of information retrieval and verification.   document retrieval, sentence selection, and claim verification. The fine-tuning is done with a batch size of 16 and the default learning rate of 5e-5 without warmup. We set k r = 20, k p = 5, \u03ba p = 0.5, and \u03ba s = 0.3 based on the memory limit and the dev set performance. We select our system with the best dev-set verification accuracy and report its scores on the hidden test set. The entire pipeline is visualized in Fig. 2. For document retrieval and sentence selection tasks, we fine-tune the BERT on 4 Nvidia V100 GPUs for 3 epochs. The training of both tasks takes around 1 hour. For claim verification task, we fine-tune the BERT on a single Nvidia V100 for 3 epochs. The training finishes in 30 minutes.\n\n\nHuman Evaluation\n\nWe measure the human performance in all three evaluation tasks on 100 sampled claims.\n\nTo perform the open-domain document retrieval task, the testee is given a claim and a python program that can retrieve the Wikipedia document from the database by its title. The testee is additionally allowed to search in the official Wikipedia web page as retrieving some documents requires matching the claim against the document content. To select the sentence-level evidence from the retrieved documents, the testee uses the documents, tokenized by sentence, returned from the python program. To verify the claim in the oracle setting, the testee is given all golden supporting documents. The testee is given infinite amounts of time for each example. Only 2 out of 100 claims are labeled as not grammatical/logical during the human evaluation.\n\n\nB Annotation Guidelines\n\n\nB.1 Claim Creation Guidelines\n\nClaim. A claim is written in single or multiple sentences that has information (true or mutated) about single or multiple entities.\n\n\nB.1.1 Simple Claim Creation\n\nThe objective of this task is to generate singlesentence claims using QA pairs from HOTPOTQA dataset as shown in Fig. 4 Instructions \u2022 Given the question and answer pair , rate the clarity of the question on a scale of 1 (very confusing) to 3 (very clear)\n\n\u2022 Extract as much information as possible from the Question and Answer and rewrite them as sentences to create claims.\n\n\u2022 Avoid including any extra information or uncommon words that are not part of the original Question and Answer\n\n\u2022 Claims must not exclude any information or uncommon words from the original Question and Answer\n\n\u2022 Claims must not include any information beyond the question and answer\n\n\u2022 Claims should be grammatically correct and in formal English\n\n\u2022 Correct capitalization and spelling of entities should be followed\n\n\u2022 Claims must not contain speculative language (e.g. probably, might be, maybe, etc.)\n\n\u2022 Some claims might not be true\n\n\u2022 Claim should be a single-sentence statement and must not contain a question mark\n\n\nB.1.2 Claim Validation\n\nThe objective of this task is to validate whether the generated claims from Simple Claim Creation meet the requirements\n\n\nInstructions\n\n\u2022 Indicate whether the claim meets the criteria mentioned in Section Sec. B.1.1\n\n\u2022 Rate the clarity of question answer pair on a scale of 1 to 5\n\nWe collect three judgments per claim and keep those claims where at least two annotators decide that it is validated. \n\n\nB.1.3 Extending to 3-hop and 4-hop\n\nThe objective of this task is to substitute an entity in the claim with the information provided in the given English Wikipedia article.\n\n\nOverview\n\n\u2022 Review the original claim and the given entity\n\n\u2022 Select a paragraph from 1 to 5 candidate paragraphs (Every paragraph mentions the entity at least once)\n\n\u2022 Replace the entity with the information from your selected paragraph that describes the entity and rewrite the claim\n\n\nInstructions\n\n\u2022 The rewritten claim must contain the title of the selected paragraph (unless the title contains the entity to be replaced.)\n\n\u2022 Do not fact check the information or use any external knowledge for this task\n\n\u2022 The claim should be broken into multiple sentences to form a coherent paragraph\n\n\u2022 In order to write coherent sentences, use proper pronoun/coreference in the latter sentence to properly refer to the entities mentioned in previous sentences\n\n\u2022 The claim must not contain the entity that need to be replaced\n\n\u2022 The claim should preserve other information from the original claim except for the entity to be replaced\n\n\u2022 Write concise claims. Use the shortest chunk of words from one selected sentence to accurately describe the entity to be replaced\n\n\u2022 When necessary, rephrase the claim to make it fluent and grammatically correct In this mutation process, we first sample a word from the claim that is not a named entity nor a stopword. We then use a pre-trained BERT-large model (Devlin et al., 2019) to predict this masked token. We only keep the claims where (1) the new word predicted by BERT and the masked word do not have a common lemma and where (2) the cosine similarity of the BERT encoding between the masked word and the predicted word lie between 0.7 and 0.8. The entire procedure is visualized in Fig. 5 \n\n\nExamples of Negated Claims\n\nOriginal: The scientific name of the true creature featured in \"Creature from the Black Lagoon\" is Eucritta melanolimnetes.\n\n\nNegated:\n\nThe scientific name of the imaginary creature featured in \"Creature from the Black Lagoon\" is Eucritta melanolimnetes.\n\n\nB.2.3 Specifically Implied Claims\n\nThe objective of this task is to create specifically implied claims from the claims created in Sec. B.1 such that the mutated claim implies the original claim.\n\n\nB.2.4 Instructions\n\n\u2022 Make the claim more specific by adding information about target entities so that the mutated claim implies the original claim.\n\n\u2022 Information must be added that is directly related to the target entities.\n\n\u2022 Annotators are discouraged to verify the added information from Wikipedia or other external sources.\n\n\u2022 Target entity must not be added to the mutated claim if it was not originally in the claim as it would decrease the number of hops in a claim.\n\n\u2022 An entity name that is explained in a relative clause or phrase in the original claim must not be added as it would decrease the number of hops in a claim.\n\nExamples of specifically implied claims Claim: Skagen Painter Peder Severin Kroyer favored naturalism along with Theodor Esbern Philipsen and the artist Ossian Elgstrom studied with in 1907.\n\nSpecifically Implied Claim: Skagen Painter Peder Severin Kroyer favored naturalism along with Theodor Esbern Philipsen and the muralist Ossian Elgstr\u00f6m studied with in 1907.\n\n\nB.2.5 Generally Implied Claims\n\nThe objective of this task is to create generally implied claims from the claims created in Sec. B.1 such that the original claim implies the mutated claim.\n\n\nInstructions\n\n\u2022 Make the claim more general by deleting information about target entities so that the original claim implies the mutated claim.\n\n\u2022 Pick an entity and consider the less specific/more generic term BERT Mutated Claim: This Maroon 5 song, is one of the tracks that Zaedan is best known for remixing. He is a Swedish producer who worked with Taylor Swift. Figure 5: Bert Mutation Procedure. We first randomly select 1-2 non-entity words from a range of Choices and mask them. Then the BERT model predict the masked token and provides the mutated claim.\n\n\nExamples of generally implied claims\n\nClaim: Skagen Painter Peder Severin Kroyer favored naturalism along with Theodor Esbern Philipsen and the artist Ossian Elgstrom studied with in 1907.\n\nGenerally Implied Claim: Skagen Painter Peder Severin Kr\u00f8yer favored naturalism along with Theodor Esbern Philipsen and the artist Ossian Elgstr\u00f6m studied with in the early 1900s.\n\n\nB.3 Claim Labeling\n\nThe objective of this task is to identify the claims to be SUPPORTED, REFUTED, or NOTENOUGHINFO given the supporting facts.\n\nSupported You have strong reasons from the supporting documents, or based on your linguistic knowledge, to justify this claim is true.\n\nRefuted Based on the supporting documents, it's impossible for this claim to be true. You can find information contradicts the supporting documents in REFUTED claims.\n\nNotEnoughInfo Any claim that doesn't fall into one of the two categories above should be labeled as NOTENOUGHINFO. This usually suggests you need ADDITIONAL information to validate whether the claim is TRUE or FALSE after reviewing the paragraphs. Whenever you are not sure whether a claim is Refuted or NOTENOUGHINFO, ask yourself \"Is it possible for this claim to be true based on the information from paragraphs?\" If yes, select NOTENOUGHINFO.\n\nExternal Knowledge. The concept of external knowledge is ambiguous and hard to define precisely, and the failure to address this issue could confuse workers regarding what information they are allowed to use when making their judgments.\n\nTo address this, we distinguish linguistic knowledge and commonsense from external, encyclopedia knowledge, as additional information that they are allowed to use in the task. Linguistic knowledge can be defined as vocabulary and syntax of an English speaker. It is invariant to most of the English speakers and can play a crucial role in this task. For example, given the supporting facts Messi is the captain of the Argentina national team., the claim was generated by substituting captain to leader. From our linguistic knowledge, captain and leader are synonyms, hence the mutated claim conveys the same idea as the provided supporting facts, and therefore should be annotated as SUPPORTED. On the other hand, if captain is replaced by goalkeeper, an English speaker can easily tell they are words of different meanings. Hence, additional information such as Messi's position should be provided in order to justify this claim. This type of information is beyond the supporting facts and should be considered as external information, and therefore the mutated claim should be annotated as NOTENOUGHINFO. In addition to linguistic knowledge, commonsense should also be taken into account. Few examples of commonsense would be: a person can only have one birth place, a person cannot perform actions after their death, etc. Hence, claims which are found to not respect commonsense are labeled as REFUTED.\n\n\nInstructions\n\n\u2022 Review the claim. Then review the supporting documents, especially the highlighted sentences.\n\n\u2022 Extract information from the supporting documents, to justify the given claim is SUP-PORTED or REFUTED. If you are not certain and need additional information, please select NOTENOUGHINFO.\n\n\u2022 Avoid using any external information that is not part of the supporting documents.\n\n\u2022 If information from the claim and supporting documents is exclusive and is impossible to be both true, the claim should be labeled as REFUTED.\n\n\u2022 If information from the claim and supporting documents is nonexclusive and it's possible that both can be true, the claim should be labeled as NOTENOUGHINFO.\n\nExamples of labeled claims Refer Table 9 for original claims, claim mutations and labels.\n\nRefuted vs NotEnoughInfo. Refer Table 10 for ambiguous examples.   \n\n\nThe Ford Fusion was introduced for model year 2006. The Rookie of The Year in the 1997 CART season drives it in the NASCAR Sprint Cup Series. Doc C: The 1997 CART PPG World Series season, the nineteenth in the CART era of U.S. open-wheel racing, consisted of 17 races, ... Rookie of the Year was Patrick Carpentier. The model of car Trevor Bayne drives was introduced for model year 2006. The Rookie of The Year in the 1997 CART season drives it in the NASCAR Sprint Cup. Doc D: Trevor Bayne is an American professional stock car racing driver. He last competed in the NASCAR Cup Series, driving the No. 6 Ford Fusion... The Ford Fusion was introduced for model year 2006. It was driven in the NASCAR Sprint Cup Series by The Rookie of The Year of a Cart season, in which the 1997 Marlboro 500 was the 17th and last round. Doc D: The 1997 Marlboro 500 was the 17th and last round of the 1997 CART season...\n\n\nReasoning Datasets. Many recently proposed datasets are created to challenge models' ability to reason across multiple sentences or documents. Khashabi et al. (2018) introduced Multi-Sentence Reading Comprehension (Mul-tiRC) which is composed of 6k multi-sentence questions. Mihaylov et al. (2018) introduced Open Book Question Answering composed of 6000 questions created upon 1326 science facts. It requires combining an open book fact with broad common knowledge in a multi-hop reasoning process. Welbl et al. (2018) constructed a multi-hop QA dataset, QAngaroo, whose queries are automatically generated upon an external knowledge base. Yang et al. (2018) introduced the HOTPOTQA dataset which does not rely on an external knowledge base and provides sentence-level evidence to explain the answer. Recent state of the art models on the open-domain setting of HOTPOTQA include Nie et al. (2019b); Qi et al. (2019); Asai et al. (2020); Fang et al. (2019); Zhao et al. (2020). The dataset is diverse and natural as it is created by human annotators. These datasets are mostly presented in the question answering format, while HOVER is instead created for the task of claim verification. Synthetic Datasets. Winograd Schema Challenge (Sakaguchi et al., 2020), Winogender schema(Rudinger et al., 2018), and RuleTaker\n\nFigure 2 :\n2Baseline system with the 4-stage architecture. In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 18-22, Baltimore, MD, USA. Association for Computational Linguistics.\n\nFigure 3 :\n3The average token length of our 2, 3, 4-hop claims.\n\nFigure 4 :\n4A 2-hop Simple Claim Creation example using HOTPOTQA pair.\n\nFigure 6 :\n6Screenshot of task to extend a 3-hop claim into a 4-hop claim.\n\nFigure 7 :\n7Screenshot of Creating More Specific Claims.\n\nFigure 8 :\n8Screenshot of Labeling Task.\n\n\nClaim: Patrick Carpentier currently drives a Ford Fusion, introduced for model year 2006, in the NASCAR Sprint Cup Series. Doc A: Ford Fusion is manufactured and marketed by Ford. Introduced for the 2006 model year, ... Doc B: Patrick Carpentier competed in the NASCAR Sprint Cup Series, driving the Ford Fusion.#H Reasoning Graph Examples \n\n2 \n\nA \nB \n\nA \nB \n\nA \nB \n\nC \n\nA \nB \n\nC \n\nA \nB \n\nC \nD \n\nA \nB \nD \n\n\n\n\nClaim: The Ford Fusion was introduced for model year 2006. The Rookie of The Year in the 1997 CART season drives it in the series held by the group that held an event at the Saugus Speedway. Doc D: Saugus Speedway is a 1/3 mile racetrack in Saugus, California on a 35 acre site. The track hosted one NASCAR Craftsman Truck Series event in 1995...Table 1: Types (graph shape) of many-hop reasoning required to extract the evidence and to verify the claim in the dataset. All claims presented are created and extended based on a single Q-A pair in HOTPOTQA. The highlighted (blue+underlined) words from the original 2/3-hop claims are replaced with the italicized phrase based on the information from the newly-introduced Docs to form the 3/4-hop claims.\n\nTable 2 :\n2The sizes of the Train-Dev-Test split for SUP-PORTED and NOT-SUPPORTED classes and different number of hops.workers and speed up the jobs. The crowd-workers are paid an average of 12 cents (the pay varies with the number of hops of a claim) per hit; and for the hop extension job, they are paid as much as 40 cents per hit since the task is time-consuming and demands the annotators to rewrite the claims after incorporating information from the extra document.Dataset Statistics. We partitioned the annotated claims and evidence into training, development, and test sets. The detailed statistics are shown inTable 2. Because of the job complexity, judgment time, and the difficulty of quality control (described in Sec. 2.4) increase drastically along with the number of hops of the claim, the first version of HOVER only uses 12k examples from the HOT-POTQA3 Dataset Analysis \n\n\n\nTable 3 :\n3The performance of the TF-IDF Document Retrieval, evaluated on the dev set.#Hops \nOverall \nModels \n2 \n3 \n4 \n\nBERT \n30.1/69.5 \n5.6/57.6 \n0.6/52.6 \n11.2/59.1 \nBERT \n34.0/69.9 \n5.8/58.2 \n1.0/53.4 \n12.5/60.2 \n\nOracle \n50.9/81.7 28.1/79.1 26.2/82.2 34.0/80.6 \nHuman \n85.0/92.5 82.4/95.3 65.8/91.4 77.0/93.5 \n\n\n\nTable 4 :\n4The EM/F1 scores of the document retrieval methods, evaluated on the dev set.\n\nTable 5 :\n5The EM/F1 scores of the sentence retrieval methods, evaluated on the dev set.#Hops \nOverall \nModels \n2 \n3 \n4 \n\nBERT + ORACLE \n79.8 83.5 78.6 \n81.2 \nClaim-only \n57.5 67.7 63.6 \n63.7 \nHuman + ORACLE 92.6 88.4 87.2 \n90.0 \n\n\n\nTable 6 :\n6The claim verification accuracy of the NLI models, evaluated on the dev set.\n\nTable 7 :\n7The claim verification accuracy and HOVER scores of the entire pipeline, evaluated on the dev set.Model Evidence F1 HOVER Score (%) \n\nBERT \n49.5 \n15.32 \n\n\n\nTable 8 :\n8The evidence F1 score and HOVER score of the best model, evaluated on the test set.\n\n\nExample of hop-extended claims 2-hop: Skagen Painter Peder Severin Kroyer favored naturalism along with Theodor Esbern Philipsen and Kristian Zahrtmann. 3-hop: Skagen Painter Peder Severin Kroyer favored naturalism along with Theodor Esbern Philipsen and the artist Ossian Elgstrom studied with in 1907.B.2 Claim Mutation \n\nB.2.1 Automatic Word Substitution using \nBERT \n\n\n\n\n. B.2.2 Claim Negation Instructions \u2022 Negate the original claim even if it is inaccurate \u2022 Negated claim must not include any extra information or uncommon words that are not part of the original claim \u2022 Negated claim MUST include all key words, have no question mark, and must end in a period \u2022 Negated claim should match the capitalization and spelling of the original claim \u2022 Negated claim should not include extra information that is not part of the original claim\n\n\n\u2022 if defender then swap for player; if goalie then player; if 1963, then 1960's . . . etc.Original Claim: This Maroon 5 song, is one of the songs that Zaedan is best known for remixing. He is a Swedish songwriter who worked with Taylor Swift. Choices:[song, one, songs, best, known, remixing, songwriter, worked]   Random Picks:[songs, songwriter]   \u2022 Removing information -Never remove the \nentire clause \n\nPreviously known as Figure-Eight and CrowdFlower: https://www.appen.com/ 4 Because of the complexity and costs (Sec. 2.4) of the data collection pipeline, we only use the HOTPOTQA dev set and 5000 examples from the training set.\nThe eight distracting documents selected by TF-IDF.6  We discarded a total of 2222 claims that received a vote of 2 vs 2 vs 1. They only account for less than 10% of all the claims that we have kept in the dataset.\nWe provide a simple visualization of the entire pipeline in the appendix.\nThe only exception is in the oracle setting because selecting sentences from 4 out of 5 documents is actually easier than selecting from 2 out of 5 documents.\nTable 9: Original Claims, Mutated Claims with their supporting documents and labels.\nTable 10: Two examples showing ambiguity between Refuted and NotEnoughInfo labels. In the first example, we need external geographical knowledge about Vermont, Illinois and Pennsylvania to refute the claim. In the second example, the claim cannot be directly refuted as Emilia Fox could have also been educated at Bryanston school and Blandford Forum.\nAcknowledgmentsWe thank the reviewers for their helpful comments and the annotators for their time and effort. This work was supported by DARPA MCS Grant N66001-19-2-4031, DARPA KAIROS Grant FA8750-19-2-1004, and Verisk. The views are those of the authors and not of the funding agency.Appendix A Experimental SetupWe use the pre-trained BERT-base uncased model (with 110M parameters) for the tasks of neuralTitleWikipedia Article Shanghai Noon 1. Shanghai Noon is a 2000 American-Hong Kong martial arts western comedy film starring Jackie Chan, Owen Wilson and Lucy Liu. 2. The first in the \"Shanghai (film series)\". 3. The film, marking the directorial debut of Tom Dey, was written by Alfred Gough and Miles Mill Tom Dey 1. Thomas Ridgeway \"Tom\" Dey (born April 14, 1965) is an American film director, screenwriter, and producer. 2. His credits include \"Shanghai Noon\", \"Showtime\", \"Failure to Launch\", and \"Marmaduke\". Roger Yuan 1. Roger Winston Yuan (born January 25, 1961) is an American Actor, martial arts fight trainer, action coordinator who trained many actors and actresses in many Hollywood films.2. As an actor himself, he also appeared in \"Shanghai Noon\"(2000)\nLearning to retrieve reasoning paths over wikipedia graph for question answering. Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong, ICLR. Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learn- ing to retrieve reasoning paths over wikipedia graph for question answering. In ICLR.\n\nMultifc: A real-world multi-domain dataset for evidencebased fact checking of claims. Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, Jakob Grue Simonsen, EMNLP. Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Chris- tian Hansen, and Jakob Grue Simonsen. 2019. Mul- tifc: A real-world multi-domain dataset for evidence- based fact checking of claims. In EMNLP.\n\nA large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, Lis- bon, Portugal. Association for Computational Lin- guistics.\n\nReading wikipedia to answer opendomain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, ACL. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer open- domain questions. In ACL.\n\nUnderstanding dataset design choices for multi-hop reasoning. Jifan Chen, Greg Durrett, NAACL. Jifan Chen and Greg Durrett. 2019. Understanding dataset design choices for multi-hop reasoning. In NAACL.\n\nSeeing things from a different angle: Discovering diverse perspectives about claims. Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, Dan Roth, NAACL. Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, and Dan Roth. 2019. Seeing things from a different angle: Discovering diverse perspec- tives about claims. In NAACL.\n\nliar, liar pants on fire\": A new benchmark dataset for fake news detection. William Yang, Wang , 10.18653/v1/P17-2067Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2William Yang Wang. 2017. \"liar, liar pants on fire\": A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics (Volume 2: Short Papers), pages 422-426, Vancouver, Canada. Association for Computational Linguistics.\n\nConstructing datasets for multi-hop reading comprehension across documents. Johannes Welbl, Pontus Stenetorp, Sebastian Riedel, Transactions of the Association for Computational Linguistics. 6Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transac- tions of the Association for Computational Linguis- tics, 6:287-302.\n\nA broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics1Long PapersAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics.\n\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing. EMNLPZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP).\n\nLocal textual inference: can it be defined or circumscribed?. Annie Zaenen, Lauri Karttunen, Richard Crouch, Proceedings of the ACL workshop on empirical modeling of semantic equivalence and entailment. the ACL workshop on empirical modeling of semantic equivalence and entailmentAnnie Zaenen, Lauri Karttunen, and Richard Crouch. 2005. Local textual inference: can it be defined or circumscribed? In Proceedings of the ACL work- shop on empirical modeling of semantic equivalence and entailment, pages 31-36.\n\nParagraph 1: Northwestern University Paragraph 2: Middlebury College Northwestern University (NU) is a private research university. Chen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul Bennett, Saurabh Tiwary, The college was founded in 1800 by Congregationalists making it the first operating college or university in Vermont. Evanston, Illinois; Washington, D.C., and San Francisco, California. Middlebury College; Middlebury, Vermont, United StatesInternational Conference on Learning RepresentationsChen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul Bennett, and Saurabh Tiwary. 2020. Transformer-xh: Multi-evidence reasoning with ex- tra hop attention. In International Conference on Learning Representations. Paragraph 1: Northwestern University Paragraph 2: Middlebury College Northwestern University (NU) is a private re- search university based in Evanston, Illinois, with other campuses located in Chicago and Doha, Qatar, and academic programs and facilities in Washington, D.C., and San Francisco, California. Middlebury College is a private liberal arts college located in Middlebury, Vermont, United States. The college was founded in 1800 by Congrega- tionalists making it the first operating college or university in Vermont...\n\nGeorge earned an MBA from Northwestern University's Kellogg School of Management. Post-Football, he appeared on Broadway in the play \"Chicago\" as the hustling lawyer Billy Flynn...Post-football, George earned an MBA from Northwestern University's Kellogg School of Management. In 2016, he appeared on Broad- way in the play \"Chicago\" as the hustling lawyer Billy Flynn....\n\nIn the introduction, the authors further explain their aim by referring specifically to \"the group historically known as the 'Little Ivies' (including Amherst. Hidden Ivies: Thirty Colleges of Excellence. Bowdoin, Middlebury, Swarthmore, Wesleyan, and WilliamsIt concerns college admissions in the United States. which the authors say ..Hidden Ivies: Thirty Colleges of Excellence is a college educational guide published in 2000. It concerns college admissions in the United States....In the introduction, the authors further explain their aim by referring specifically to \"the group historically known as the 'Little Ivies' (in- cluding Amherst, Bowdoin, Middlebury, Swarth- more, Wesleyan, and Williams)\" which the au- thors say ...\n\nWilliams and one other. That other \"Little Ivy\" and the institution where Eddie George earned an MBA from, are both private schools in Pennsylvania. Claim: The 'Little Ivies', mentioned in the book Hidden Ivies, are Amherst. Bowdoin, Swarthmore, Wesleyan; Emilia FoxParagraph 1: Flashbacks of a Fool Paragraph 2Claim: The 'Little Ivies', mentioned in the book Hidden Ivies, are Amherst, Bowdoin, Swarthmore, Wesleyan, Williams and one other. That other \"Little Ivy\" and the institution where Eddie George earned an MBA from, are both private schools in Pennsylvania. Paragraph 1: Flashbacks of a Fool Paragraph 2: Emilia Fox\n\nClaim: Emilia Fox was a cast member of Flashbacks of a Fool was educated at Blandford Forum in Blandford. Blandford, Dorset; DorsetShe also appeared as Morgause in the BBC's \"Merlin\" beginning in the programme's second seriesShe also appeared as Morgause in the BBC's \"Merlin\" beginning in the programme's second series.She was educated at Bryanston School in Blandford, Dorset. Claim: Emilia Fox was a cast member of Flashbacks of a Fool was educated at Blandford Forum in Blandford, Dorset.\n", "annotations": {"author": "[{\"end\":152,\"start\":93},{\"end\":219,\"start\":153},{\"end\":282,\"start\":220},{\"end\":351,\"start\":283},{\"end\":411,\"start\":352},{\"end\":471,\"start\":412}]", "publisher": null, "author_last_name": "[{\"end\":105,\"start\":100},{\"end\":166,\"start\":160},{\"end\":231,\"start\":226},{\"end\":297,\"start\":291},{\"end\":365,\"start\":360},{\"end\":424,\"start\":418}]", "author_first_name": "[{\"end\":99,\"start\":93},{\"end\":159,\"start\":153},{\"end\":225,\"start\":220},{\"end\":290,\"start\":283},{\"end\":359,\"start\":352},{\"end\":417,\"start\":412}]", "author_affiliation": "[{\"end\":151,\"start\":126},{\"end\":218,\"start\":193},{\"end\":281,\"start\":256},{\"end\":350,\"start\":325},{\"end\":410,\"start\":385},{\"end\":470,\"start\":445}]", "title": "[{\"end\":69,\"start\":1},{\"end\":540,\"start\":472}]", "venue": "[{\"end\":638,\"start\":542}]", "abstract": "[{\"end\":1869,\"start\":746}]", "bib_ref": "[{\"end\":2270,\"start\":2249},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2599,\"start\":2580},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2633,\"start\":2613},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2771,\"start\":2748},{\"end\":2793,\"start\":2776},{\"end\":2971,\"start\":2951},{\"end\":3083,\"start\":3059},{\"end\":3246,\"start\":3227},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3264,\"start\":3246},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4338,\"start\":4319},{\"end\":5599,\"start\":5579},{\"end\":6054,\"start\":6023},{\"end\":6073,\"start\":6054},{\"end\":6636,\"start\":6617},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6881,\"start\":6862},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9522,\"start\":9503},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10228,\"start\":10209},{\"end\":14830,\"start\":14809},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17495,\"start\":17474},{\"end\":17525,\"start\":17495},{\"end\":17544,\"start\":17525},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19447,\"start\":19428},{\"end\":19612,\"start\":19592},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19696,\"start\":19677},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21585,\"start\":21567},{\"end\":21899,\"start\":21881},{\"end\":27468,\"start\":27444},{\"end\":27496,\"start\":27470},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27528,\"start\":27505},{\"end\":28454,\"start\":28434},{\"end\":28728,\"start\":28708},{\"end\":29127,\"start\":29107},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29700,\"start\":29675},{\"end\":29862,\"start\":29842},{\"end\":35766,\"start\":35745}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43008,\"start\":42100},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44326,\"start\":43009},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44561,\"start\":44327},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44626,\"start\":44562},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44698,\"start\":44627},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44774,\"start\":44699},{\"attributes\":{\"id\":\"fig_6\"},\"end\":44832,\"start\":44775},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44874,\"start\":44833},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45283,\"start\":44875},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46038,\"start\":45284},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":46931,\"start\":46039},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":47248,\"start\":46932},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47338,\"start\":47249},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":47571,\"start\":47339},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":47660,\"start\":47572},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":47827,\"start\":47661},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":47923,\"start\":47828},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":48298,\"start\":47924},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":48769,\"start\":48299},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":49179,\"start\":48770}]", "paragraph": "[{\"end\":3265,\"start\":1885},{\"end\":5095,\"start\":3267},{\"end\":6474,\"start\":5097},{\"end\":8506,\"start\":6476},{\"end\":9136,\"start\":8508},{\"end\":10113,\"start\":9156},{\"end\":10324,\"start\":10132},{\"end\":10652,\"start\":10364},{\"end\":11048,\"start\":10654},{\"end\":11639,\"start\":11088},{\"end\":12080,\"start\":11641},{\"end\":12562,\"start\":12082},{\"end\":13721,\"start\":12564},{\"end\":13977,\"start\":13740},{\"end\":14636,\"start\":13979},{\"end\":15230,\"start\":14638},{\"end\":15566,\"start\":15232},{\"end\":15854,\"start\":15568},{\"end\":16094,\"start\":15856},{\"end\":16639,\"start\":16113},{\"end\":17545,\"start\":16641},{\"end\":18230,\"start\":17547},{\"end\":19049,\"start\":18252},{\"end\":19613,\"start\":19051},{\"end\":20420,\"start\":19615},{\"end\":21190,\"start\":20422},{\"end\":21489,\"start\":21210},{\"end\":21812,\"start\":21491},{\"end\":22170,\"start\":21814},{\"end\":22521,\"start\":22172},{\"end\":22876,\"start\":22523},{\"end\":23010,\"start\":22904},{\"end\":23410,\"start\":23033},{\"end\":24562,\"start\":23441},{\"end\":25209,\"start\":24593},{\"end\":25869,\"start\":25240},{\"end\":26716,\"start\":25895},{\"end\":27307,\"start\":26738},{\"end\":28672,\"start\":27324},{\"end\":30450,\"start\":28674},{\"end\":31770,\"start\":30477},{\"end\":31876,\"start\":31791},{\"end\":32626,\"start\":31878},{\"end\":32817,\"start\":32686},{\"end\":33104,\"start\":32849},{\"end\":33224,\"start\":33106},{\"end\":33337,\"start\":33226},{\"end\":33436,\"start\":33339},{\"end\":33510,\"start\":33438},{\"end\":33574,\"start\":33512},{\"end\":33644,\"start\":33576},{\"end\":33731,\"start\":33646},{\"end\":33764,\"start\":33733},{\"end\":33848,\"start\":33766},{\"end\":33994,\"start\":33875},{\"end\":34090,\"start\":34011},{\"end\":34155,\"start\":34092},{\"end\":34275,\"start\":34157},{\"end\":34450,\"start\":34314},{\"end\":34511,\"start\":34463},{\"end\":34618,\"start\":34513},{\"end\":34738,\"start\":34620},{\"end\":34880,\"start\":34755},{\"end\":34961,\"start\":34882},{\"end\":35044,\"start\":34963},{\"end\":35205,\"start\":35046},{\"end\":35271,\"start\":35207},{\"end\":35379,\"start\":35273},{\"end\":35512,\"start\":35381},{\"end\":36083,\"start\":35514},{\"end\":36237,\"start\":36114},{\"end\":36368,\"start\":36250},{\"end\":36565,\"start\":36406},{\"end\":36716,\"start\":36588},{\"end\":36794,\"start\":36718},{\"end\":36898,\"start\":36796},{\"end\":37044,\"start\":36900},{\"end\":37203,\"start\":37046},{\"end\":37395,\"start\":37205},{\"end\":37570,\"start\":37397},{\"end\":37761,\"start\":37605},{\"end\":37907,\"start\":37778},{\"end\":38327,\"start\":37909},{\"end\":38518,\"start\":38368},{\"end\":38699,\"start\":38520},{\"end\":38845,\"start\":38722},{\"end\":38981,\"start\":38847},{\"end\":39149,\"start\":38983},{\"end\":39597,\"start\":39151},{\"end\":39835,\"start\":39599},{\"end\":41242,\"start\":39837},{\"end\":41354,\"start\":41259},{\"end\":41546,\"start\":41356},{\"end\":41632,\"start\":41548},{\"end\":41778,\"start\":41634},{\"end\":41939,\"start\":41780},{\"end\":42030,\"start\":41941},{\"end\":42099,\"start\":42032}]", "formula": null, "table_ref": "[{\"end\":4988,\"start\":4981},{\"end\":12305,\"start\":12298},{\"end\":13318,\"start\":13311},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19414,\"start\":19179},{\"end\":19868,\"start\":19861},{\"end\":20258,\"start\":20251},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23463,\"start\":23456},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23869,\"start\":23862},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":25272,\"start\":25265},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":25937,\"start\":25929},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":26105,\"start\":26098},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":26417,\"start\":26410},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":26445,\"start\":26438},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26818,\"start\":26810},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":26851,\"start\":26843},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":26985,\"start\":26976},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":27094,\"start\":27085},{\"end\":28410,\"start\":28404},{\"end\":41981,\"start\":41974},{\"end\":42072,\"start\":42064}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1883,\"start\":1871},{\"attributes\":{\"n\":\"2\"},\"end\":9154,\"start\":9139},{\"attributes\":{\"n\":\"2.1\"},\"end\":10130,\"start\":10116},{\"attributes\":{\"n\":\"2.1.1\"},\"end\":10362,\"start\":10327},{\"attributes\":{\"n\":\"2.1.2\"},\"end\":11086,\"start\":11051},{\"attributes\":{\"n\":\"2.2\"},\"end\":13738,\"start\":13724},{\"attributes\":{\"n\":\"2.3\"},\"end\":16111,\"start\":16097},{\"attributes\":{\"n\":\"2.4\"},\"end\":18250,\"start\":18233},{\"attributes\":{\"n\":\"4\"},\"end\":21208,\"start\":21193},{\"attributes\":{\"n\":\"5\"},\"end\":22902,\"start\":22879},{\"attributes\":{\"n\":\"5.1\"},\"end\":23031,\"start\":23013},{\"attributes\":{\"n\":\"5.2\"},\"end\":23439,\"start\":23413},{\"attributes\":{\"n\":\"5.3\"},\"end\":24591,\"start\":24565},{\"attributes\":{\"n\":\"5.4\"},\"end\":25238,\"start\":25212},{\"attributes\":{\"n\":\"5.5\"},\"end\":25893,\"start\":25872},{\"attributes\":{\"n\":\"5.6\"},\"end\":26736,\"start\":26719},{\"attributes\":{\"n\":\"6\"},\"end\":27322,\"start\":27310},{\"end\":30462,\"start\":30453},{\"attributes\":{\"n\":\"7\"},\"end\":30475,\"start\":30465},{\"end\":31789,\"start\":31773},{\"end\":32652,\"start\":32629},{\"end\":32684,\"start\":32655},{\"end\":32847,\"start\":32820},{\"end\":33873,\"start\":33851},{\"end\":34009,\"start\":33997},{\"end\":34312,\"start\":34278},{\"end\":34461,\"start\":34453},{\"end\":34753,\"start\":34741},{\"end\":36112,\"start\":36086},{\"end\":36248,\"start\":36240},{\"end\":36404,\"start\":36371},{\"end\":36586,\"start\":36568},{\"end\":37603,\"start\":37573},{\"end\":37776,\"start\":37764},{\"end\":38366,\"start\":38330},{\"end\":38720,\"start\":38702},{\"end\":41257,\"start\":41245},{\"end\":44338,\"start\":44328},{\"end\":44573,\"start\":44563},{\"end\":44638,\"start\":44628},{\"end\":44710,\"start\":44700},{\"end\":44786,\"start\":44776},{\"end\":44844,\"start\":44834},{\"end\":46049,\"start\":46040},{\"end\":46942,\"start\":46933},{\"end\":47259,\"start\":47250},{\"end\":47349,\"start\":47340},{\"end\":47582,\"start\":47573},{\"end\":47671,\"start\":47662},{\"end\":47838,\"start\":47829}]", "table": "[{\"end\":45283,\"start\":45189},{\"end\":46931,\"start\":46910},{\"end\":47248,\"start\":47019},{\"end\":47571,\"start\":47428},{\"end\":47827,\"start\":47771},{\"end\":48298,\"start\":48229},{\"end\":49179,\"start\":49122}]", "figure_caption": "[{\"end\":43008,\"start\":42102},{\"end\":44326,\"start\":43011},{\"end\":44561,\"start\":44340},{\"end\":44626,\"start\":44575},{\"end\":44698,\"start\":44640},{\"end\":44774,\"start\":44712},{\"end\":44832,\"start\":44788},{\"end\":44874,\"start\":44846},{\"end\":45189,\"start\":44877},{\"end\":46038,\"start\":45286},{\"end\":46910,\"start\":46051},{\"end\":47019,\"start\":46944},{\"end\":47338,\"start\":47261},{\"end\":47428,\"start\":47351},{\"end\":47660,\"start\":47584},{\"end\":47771,\"start\":47673},{\"end\":47923,\"start\":47840},{\"end\":48229,\"start\":47926},{\"end\":48769,\"start\":48301},{\"end\":49122,\"start\":48772}]", "figure_ref": "[{\"end\":4212,\"start\":4206},{\"end\":4976,\"start\":4970},{\"end\":5134,\"start\":5128},{\"end\":7057,\"start\":7049},{\"end\":9330,\"start\":9324},{\"end\":10322,\"start\":10316},{\"end\":13930,\"start\":13924},{\"end\":16154,\"start\":16148},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31484,\"start\":31478},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32968,\"start\":32962},{\"end\":36082,\"start\":36076},{\"end\":38139,\"start\":38131}]", "bib_author_first_name": "[{\"end\":51558,\"start\":51553},{\"end\":51571,\"start\":51565},{\"end\":51591,\"start\":51583},{\"end\":51611,\"start\":51604},{\"end\":51627,\"start\":51620},{\"end\":51921,\"start\":51913},{\"end\":51943,\"start\":51934},{\"end\":51960,\"start\":51951},{\"end\":51972,\"start\":51967},{\"end\":51979,\"start\":51973},{\"end\":51992,\"start\":51986},{\"end\":52010,\"start\":52001},{\"end\":52024,\"start\":52019},{\"end\":52029,\"start\":52025},{\"end\":52354,\"start\":52353},{\"end\":52368,\"start\":52363},{\"end\":52388,\"start\":52377},{\"end\":52408,\"start\":52397},{\"end\":52410,\"start\":52409},{\"end\":53030,\"start\":53025},{\"end\":53041,\"start\":53037},{\"end\":53054,\"start\":53049},{\"end\":53070,\"start\":53063},{\"end\":53276,\"start\":53271},{\"end\":53287,\"start\":53283},{\"end\":53502,\"start\":53497},{\"end\":53515,\"start\":53509},{\"end\":53533,\"start\":53526},{\"end\":53544,\"start\":53539},{\"end\":53564,\"start\":53561},{\"end\":53843,\"start\":53836},{\"end\":53854,\"start\":53850},{\"end\":54480,\"start\":54472},{\"end\":54494,\"start\":54488},{\"end\":54515,\"start\":54506},{\"end\":54890,\"start\":54885},{\"end\":54907,\"start\":54901},{\"end\":54922,\"start\":54916},{\"end\":55710,\"start\":55704},{\"end\":55721,\"start\":55717},{\"end\":55734,\"start\":55726},{\"end\":55748,\"start\":55742},{\"end\":55764,\"start\":55757},{\"end\":55766,\"start\":55765},{\"end\":55780,\"start\":55774},{\"end\":55807,\"start\":55796},{\"end\":55809,\"start\":55808},{\"end\":56235,\"start\":56230},{\"end\":56249,\"start\":56244},{\"end\":56268,\"start\":56261},{\"end\":56815,\"start\":56811},{\"end\":56829,\"start\":56822},{\"end\":56842,\"start\":56837},{\"end\":56854,\"start\":56851},{\"end\":56865,\"start\":56861},{\"end\":56882,\"start\":56875}]", "bib_author_last_name": "[{\"end\":51563,\"start\":51559},{\"end\":51581,\"start\":51572},{\"end\":51602,\"start\":51592},{\"end\":51618,\"start\":51612},{\"end\":51633,\"start\":51628},{\"end\":51932,\"start\":51922},{\"end\":51949,\"start\":51944},{\"end\":51965,\"start\":51961},{\"end\":51984,\"start\":51980},{\"end\":51999,\"start\":51993},{\"end\":52017,\"start\":52011},{\"end\":52038,\"start\":52030},{\"end\":52361,\"start\":52355},{\"end\":52375,\"start\":52369},{\"end\":52395,\"start\":52389},{\"end\":52416,\"start\":52411},{\"end\":52425,\"start\":52418},{\"end\":53035,\"start\":53031},{\"end\":53047,\"start\":53042},{\"end\":53061,\"start\":53055},{\"end\":53077,\"start\":53071},{\"end\":53281,\"start\":53277},{\"end\":53295,\"start\":53288},{\"end\":53507,\"start\":53503},{\"end\":53524,\"start\":53516},{\"end\":53537,\"start\":53534},{\"end\":53559,\"start\":53545},{\"end\":53569,\"start\":53565},{\"end\":53848,\"start\":53844},{\"end\":54486,\"start\":54481},{\"end\":54504,\"start\":54495},{\"end\":54522,\"start\":54516},{\"end\":54899,\"start\":54891},{\"end\":54914,\"start\":54908},{\"end\":54929,\"start\":54923},{\"end\":55715,\"start\":55711},{\"end\":55724,\"start\":55722},{\"end\":55740,\"start\":55735},{\"end\":55755,\"start\":55749},{\"end\":55772,\"start\":55767},{\"end\":55794,\"start\":55781},{\"end\":55817,\"start\":55810},{\"end\":56242,\"start\":56236},{\"end\":56259,\"start\":56250},{\"end\":56275,\"start\":56269},{\"end\":56820,\"start\":56816},{\"end\":56835,\"start\":56830},{\"end\":56849,\"start\":56843},{\"end\":56859,\"start\":56855},{\"end\":56873,\"start\":56866},{\"end\":56889,\"start\":56883},{\"end\":58025,\"start\":58012}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":208267807},\"end\":51825,\"start\":51471},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":202541363},\"end\":52285,\"start\":51827},{\"attributes\":{\"doi\":\"10.18653/v1/D15-1075\",\"id\":\"b2\",\"matched_paper_id\":14604520},\"end\":52973,\"start\":52287},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3618568},\"end\":53207,\"start\":52975},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":139103297},\"end\":53410,\"start\":53209},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":85556928},\"end\":53758,\"start\":53412},{\"attributes\":{\"doi\":\"10.18653/v1/P17-2067\",\"id\":\"b6\",\"matched_paper_id\":10326133},\"end\":54394,\"start\":53760},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9192723},\"end\":54803,\"start\":54396},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3432876},\"end\":55627,\"start\":54805},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52822214},\"end\":56166,\"start\":55629},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2916595},\"end\":56677,\"start\":56168},{\"attributes\":{\"id\":\"b11\"},\"end\":57928,\"start\":56679},{\"attributes\":{\"id\":\"b12\"},\"end\":58302,\"start\":57930},{\"attributes\":{\"id\":\"b13\"},\"end\":59039,\"start\":58304},{\"attributes\":{\"id\":\"b14\"},\"end\":59665,\"start\":59041},{\"attributes\":{\"id\":\"b15\"},\"end\":60159,\"start\":59667}]", "bib_title": "[{\"end\":51551,\"start\":51471},{\"end\":51911,\"start\":51827},{\"end\":52351,\"start\":52287},{\"end\":53023,\"start\":52975},{\"end\":53269,\"start\":53209},{\"end\":53495,\"start\":53412},{\"end\":53834,\"start\":53760},{\"end\":54470,\"start\":54396},{\"end\":54883,\"start\":54805},{\"end\":55702,\"start\":55629},{\"end\":56228,\"start\":56168},{\"end\":56809,\"start\":56679},{\"end\":58462,\"start\":58304},{\"end\":59188,\"start\":59041}]", "bib_author": "[{\"end\":51565,\"start\":51553},{\"end\":51583,\"start\":51565},{\"end\":51604,\"start\":51583},{\"end\":51620,\"start\":51604},{\"end\":51635,\"start\":51620},{\"end\":51934,\"start\":51913},{\"end\":51951,\"start\":51934},{\"end\":51967,\"start\":51951},{\"end\":51986,\"start\":51967},{\"end\":52001,\"start\":51986},{\"end\":52019,\"start\":52001},{\"end\":52040,\"start\":52019},{\"end\":52363,\"start\":52353},{\"end\":52377,\"start\":52363},{\"end\":52397,\"start\":52377},{\"end\":52418,\"start\":52397},{\"end\":52427,\"start\":52418},{\"end\":53037,\"start\":53025},{\"end\":53049,\"start\":53037},{\"end\":53063,\"start\":53049},{\"end\":53079,\"start\":53063},{\"end\":53283,\"start\":53271},{\"end\":53297,\"start\":53283},{\"end\":53509,\"start\":53497},{\"end\":53526,\"start\":53509},{\"end\":53539,\"start\":53526},{\"end\":53561,\"start\":53539},{\"end\":53571,\"start\":53561},{\"end\":53850,\"start\":53836},{\"end\":53857,\"start\":53850},{\"end\":54488,\"start\":54472},{\"end\":54506,\"start\":54488},{\"end\":54524,\"start\":54506},{\"end\":54901,\"start\":54885},{\"end\":54916,\"start\":54901},{\"end\":54931,\"start\":54916},{\"end\":55717,\"start\":55704},{\"end\":55726,\"start\":55717},{\"end\":55742,\"start\":55726},{\"end\":55757,\"start\":55742},{\"end\":55774,\"start\":55757},{\"end\":55796,\"start\":55774},{\"end\":55819,\"start\":55796},{\"end\":56244,\"start\":56230},{\"end\":56261,\"start\":56244},{\"end\":56277,\"start\":56261},{\"end\":56822,\"start\":56811},{\"end\":56837,\"start\":56822},{\"end\":56851,\"start\":56837},{\"end\":56861,\"start\":56851},{\"end\":56875,\"start\":56861},{\"end\":56891,\"start\":56875},{\"end\":58027,\"start\":58012}]", "bib_venue": "[{\"end\":51639,\"start\":51635},{\"end\":52045,\"start\":52040},{\"end\":52533,\"start\":52447},{\"end\":53082,\"start\":53079},{\"end\":53302,\"start\":53297},{\"end\":53576,\"start\":53571},{\"end\":53964,\"start\":53877},{\"end\":54585,\"start\":54524},{\"end\":55073,\"start\":54931},{\"end\":55881,\"start\":55819},{\"end\":56369,\"start\":56277},{\"end\":57007,\"start\":56891},{\"end\":58010,\"start\":57930},{\"end\":58507,\"start\":58464},{\"end\":59264,\"start\":59190},{\"end\":59771,\"start\":59667},{\"end\":52622,\"start\":52535},{\"end\":54055,\"start\":53966},{\"end\":55202,\"start\":55075},{\"end\":56448,\"start\":56371},{\"end\":57132,\"start\":57009},{\"end\":58564,\"start\":58509},{\"end\":59307,\"start\":59266}]"}}}, "year": 2023, "month": 12, "day": 17}