{"id": 4800342, "updated": "2023-10-01 02:14:40.02", "metadata": {"title": "Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics", "authors": "[{\"first\":\"Alex\",\"last\":\"Kendall\",\"middle\":[]},{\"first\":\"Yarin\",\"last\":\"Gal\",\"middle\":[]},{\"first\":\"Roberto\",\"last\":\"Cipolla\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 5, "day": 19}, "abstract": "Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1705.07115", "mag": "2963677766", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/KendallGC18", "doi": "10.1109/cvpr.2018.00781"}}, "content": {"source": {"pdf_hash": "b05faf0ae510cbd7510a6242aafdda7de3088282", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1705.07115v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1705.07115", "status": "GREEN"}}, "grobid": {"id": "1f43459bb49819050868e0e8f4cc67bdfdecaeb3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b05faf0ae510cbd7510a6242aafdda7de3088282.txt", "contents": "\nMulti-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\n\n\nAlex Kendall \nUniversity of Cambridge\nUniversity of Cambridge\nUniversity of Cambridge\n\n\nYarin Gal \nUniversity of Cambridge\nUniversity of Cambridge\nUniversity of Cambridge\n\n\nRoberto Cipolla \nUniversity of Cambridge\nUniversity of Cambridge\nUniversity of Cambridge\n\n\nMulti-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\n\nNumerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.\n\nIntroduction\n\nMulti-task learning aims to improve learning efficiency and prediction accuracy by learning multiple objectives from a shared representation [1]. Multi-task learning is prevalent in many applications of machine learning -from computer vision [2] to natural language processing [3] to speech recognition [4]. We explore multi-task learning within the setting of visual scene understanding in computer vision. Scene understanding algorithms must understand both the geometry and semantics of the scene at the same time. This forms an interesting multi-task learning problem because scene understanding involves joint learning of various regression and classification tasks with different units and scales. Multi-task learning of visual scene understanding is of crucial importance in systems where long computation run-time is prohibitive, such as the ones used in robotics. Combining all tasks into a single model reduces computation and allows these systems to run in real-time.\n\nPrior approaches to simultaneously learning multiple tasks use a na\u00efve weighted sum of losses, where the loss weights are uniform, or manually tuned [2,5,6]. However, we show that performance is highly dependant on an appropriate choice of weighting between each task's loss. Searching for an optimal weighting is prohibitively expensive and difficult to resolve with manual tuning. We observe that the optimal weighting of each task is dependant on the measurement scale (e.g. meters, centimetres or millimetres) and ultimately the magnitude of the task's noise. In this work we propose a principled way of combining multiple loss functions to simultaneously learn multiple objectives using homoscedastic uncertainty. We interpret homoscedastic uncertainty as task-dependant weighting and show how to derive a principled multi-task loss function which can learn to balance various regression and classification losses. Our method can learn to balance these weightings optimally, resulting in superior performance, compared with learning each task individually.\n\nSpecifically, we demonstrate our method in learning scene geometry and semantics with three tasks. Firstly, we learn to classify objects at a pixel level, also known as semantic segmentation [7][8][9][10][11].  Figure 1: Multi-task deep learning. We derive a principled way of combining multiple regression and classification loss functions for multi-task learning. Our architecture takes a single monocular RGB image as input and produces a pixel-wise classification, an instance semantic segmentation and an estimate of per pixel depth. Multi-task learning can improve accuracy over separately trained models because cues from one task, such as depth, are used to regularize and improve the generalization of another domain, such as segmentation. Secondly, our model performs instance segmentation, which is the harder task of segmenting separate masks for each individual object in an image (for example, a separate, precise mask for each individual car on the road) [12][13][14][15]. This is a more complicated task than semantic segmentation, as it requires not only an estimate of each pixel's class, but also which object that pixel belongs to. It is also more complicated than object detection, which often predicts object bounding boxes alone [16]. Finally, our model predicts pixel-wise metric depth. Depth by recognition has been demonstrated using dense prediction networks with supervised [6] and unsupervised [17] deep learning. However it is very hard to estimate depth in a way which generalises well. We show that we can improve our estimation of geometry and depth by using semantic labels and multi-task deep learning.\n\nIn existing literature, separate deep learning models would be used to learn depth regression, semantic segmentation and instance segmentation to create a complete scene understanding system. Given a single monocular input image, our system is the first to produce a semantic segmentation, a dense estimate of metric depth and an instance level segmentation jointly ( Figure 1). While other vision models have demonstrated multi-task learning, we show how to learn to combine semantics and geometry. Combining these tasks into a single model ensures that the model agrees between the separate task outputs while reducing computation. Finally, we show that using a shared representation with multi-task learning improves performance on various metrics, making the models more effective.\n\nIn summary, the key contributions of this paper are:\n\n1. a novel and principled multi-task loss to simultaneously learn various classification and regression losses of varying quantities and units using homoscedastic task uncertainty, 2. a unified architecture for semantic segmentation, instance segmentation and depth regression, 3. demonstrating the importance of loss weighting in multi-task deep learning and how to obtain superior performance compared to equivalent separately trained models.\n\n\nRelated Work\n\nMulti-task learning aims to improve learning efficiency and prediction accuracy for for each task, when compared to training the models separately [18,19]. It can be considered an approach to inductive knowledge transfer which improves generalisation by sharing the domain information between complimentary tasks. It does this by using a shared representation to learn multiple taskswhat is learned from one task can help other tasks be learned better [1].\n\nFine-tuning [20,21] is a basic example of multi-task learning, where we can leverage different learning tasks by considering them as a pre-training step. Other models alternate learning between each training task, for example in natural language processing [3]. Multi-task learning can also be used in a data streaming setting [18], or to prevent forgetting previously learned tasks in reinforcement learning [22]. It can also be used to learn unsupervised features from various data sources with an auto-encoder [23].\n\nIn computer vision there are many examples of methods for multi-task learning. Many focus on semantic tasks, such as classification and semantic segmentation [24] or classification and detection [5]. MultiNet [25] proposes an architecture for detection, classification and semantic segmentation. CrossStitch networks [26] explore methods to combine multi-task neural activations. Uhrig et al. [27] learn semantic and instance segmentations under a classification setting. Multi-task deep learning has also been used for geometry and regression tasks. Eigen and Fergus [6] show how to learn semantic segmentation, depth and surface normals. PoseNet [28] is a model which learns camera position and orientation. UberNet [2] learns a number of different regression and classification tasks under a single architecture. In this work we are the first to propose a method for jointly learning depth regression, semantic and instance segmentation. Like the model of Eigen and Fergus [6], our model learns both semantic and geometry representations, which is important for scene understanding. However, our model learns the much harder task of instance segmentation which requires knowledge of both semantics and geometry. This is because our model must determine the class and spatial relationship for each pixel in each object for instance segmentation.\n\nMore importantly, all previous methods which learn multiple tasks simultaneously use a na\u00efve weighted sum of losses, where the loss weights are uniform, or crudely and manually tuned. In this work we propose a principled way of combining multiple loss functions to simultaneously learn multiple objectives using homoscedastic task uncertainty. We illustrate the importance of appropriately weighting each task in deep learning to achieve good performance and show that our method can learn to balance these weightings optimally.\n\n\nMulti Task Learning with Homoscedastic Uncertainty\n\nMulti-task learning concerns the problem of optimising a model with respect to multiple objectives. It is prevalent in many deep learning problems. The naive approach to combining multi objective losses would be to simply perform a weighted linear sum of the losses for each individual task:\nL total = i w i L i .(1)\nThis is the dominant approach used by prior work [5,24,25,27], for example for dense prediction tasks [2], for scene understanding tasks [6] and for rotation (in quaternions) and translation (in meters) for camera pose [28]. However, there are a number of issues with this method. Namely, model performance is extremely sensitive to weight selection, w i , as illustrated in Figure 2. These weight hyper-parameters are expensive to tune, often taking many days for each trial. Therefore, it is desirable to find a more convenient approach which is able to learn the optimal weights.\n\nMore concretely, let us consider a network which learns to predict pixel-wise depth and semantic class from an input image. In Figure 2 the two boundaries of each plot show models trained on individual tasks, with the curves showing performance for varying weights w i for each task. We observe that at some optimal weighting, the joint network performs better than separate networks trained on each task individually (performance of the model in individual tasks is seen at both edges of the plot: w = 0 and w = 1). At near-by values to the optimal weight the network performs worse on one of the tasks. However, searching for these optimal weightings is expensive and increasingly difficult with large models with numerous tasks. We next show how to learn optimal task weightings using ideas from probabilistic modelling. Additionally, we show a similar result for two regression tasks; instance segmentation and depth regression.\n\n\nHomoscedastic uncertainty as task-dependant uncertainty\n\nIn Bayesian modelling, there are two main types of uncertainty one can model [29].\n\n\u2022 Epistemic uncertainty is uncertainty in the model, which captures what our model doesn't know due to lack of training data. It can be explained away with increased training data. \u2022 Aleatoric uncertainty captures our uncertainty with respect to information which our data cannot explain. Aleatoric uncertainty can be explained away with the ability to observe all explanatory variables with increasing precision.  For some balance of weightings between each task, we observe improved performance for both tasks. All models were trained with a learning rate of 0.01 with the respective weightings applied to the losses using the loss function in (1). Results are shown using the Tiny CityScapes validation dataset using a down-sampled resolution of 128 \u00d7 256.\n\n\n(b) Comparing loss weightings when learning instance regression and depth regression\n\nAleatoric uncertainty can again be divided into two sub-categories.\n\n\u2022 Data-dependant or Heteroscedastic uncertainty is aleatoric uncertainty which depends on the input data and is predicted as a model output.\n\n\u2022 Task-dependant or Homoscedastic uncertainty is aleatoric uncertainty which is not dependant on the input data. It is not a model output, rather it is a quantity which stays constant for all input data and varies between different tasks. It can therefore be described as task-dependant uncertainty.\n\nIn a multi-task setting, we show that the task uncertainty captures the relative confidence between tasks, reflecting the uncertainty inherent to the regression or classification task. It will also depend on the task's representation or unit of measure. We propose that we can use homoscedastic uncertainty as a basis for weighting losses in a multi-task learning problem.\n\n\nMulti-task likelihoods\n\nIn this section we derive a multi-task loss function based on maximising the Gaussian likelihood with homoscedastic uncertainty. Let f W (x) be the output of a neural network with weights W on input x. We define the following probabilistic model. For regression tasks we define our likelihood as a Gaussian with mean given by the model output:\np(y|f W (x)) = N (f W (x), \u03c3 2 )(2)\nwith an observation noise scalar \u03c3. For classification we often squash the model output through a softmax function, and sample from the resulting probability vector:\np(y|f W (x)) = Softmax(f W (x)).(3)\nIn the case of multiple model outputs, we often define the likelihood to factorise over the outputs, given some sufficient statistics. We define f W (x) as our sufficient statistics, and obtain the following multi-task likelihood:\np(y 1 , ..., y K |f W (x)) = p(y 1 |f W (x))...p(y K |f W (x))(4)\nwith model outputs y 1 , ..., y K (such as semantic segmentation, depth regression, etc).\n\nIn maximum likelihood inference, we maximise the log likelihood of the model. In regression, for example, the log likelihood can be written as\nlog p(y|f W (x)) \u221d \u2212 1 2\u03c3 2 ||y \u2212 f W (x)|| 2 \u2212 log \u03c3 2(5)\nfor a Gaussian likelihood (or similarly for a Laplace likelihood) with \u03c3 the model's observation noise parameter -capturing how much noise we have in the outputs. We then maximise the log likelihood with respect to the model parameters W and observation noise parameter \u03c3.\n\nLet us now assume that our model output is composed of two vectors y 1 and y 2 , each following a Gaussian distribution:\np(y 1 , y 2 |f W (x)) = p(y 1 |f W (x)) \u00b7 p(y 2 |f W (x)) = N (y 1 ; f W (x), \u03c3 2 1 ) \u00b7 N (y 2 ; f W (x), \u03c3 2 2 ).(6)\nThis leads to the following minimisation objective (our loss) for our multi-output model:\nL(W, \u03c3 1 , \u03c3 2 ) = \u2212 log p(y 1 , y 2 |f W (x)) \u221d 1 2\u03c3 2 1 ||y 1 \u2212 f W (x)|| 2 + 1 2\u03c3 2 2 ||y 2 \u2212 f W (x)|| 2 + log \u03c3 2 1 \u03c3 2 2 = 1 2\u03c3 2 1 L 1 (W) + 1 2\u03c3 2 2 L 2 (W) + log \u03c3 2 1 \u03c3 2 2(7)\nWhere we wrote L 1 (W) = ||y 1 \u2212 f W (x)|| 2 for the loss of the first output variable, and similarly for L 2 (W).\n\nWe interpret minimising this last objective with respect to \u03c3 1 and \u03c3 2 as learning the relative weight of the losses L 1 (W) and L 2 (W) adaptively, based on the data. As \u03c3 1 -the noise parameter for the variable y 1 -increases, we have that the weight of L 1 (W) decreases. On the other hand, as the noise decreases, we have that the weight of the respective objective increases. The noise is discouraged from increasing too much (effectively ignoring the data) by the last term in the objective, which acts as a regulariser for the noise terms.\n\nThis construction can be trivially extended to multiple regression outputs. However, the extension to classification likelihoods is more interesting. We adapt the classification likelihood to squash a scaled version of the model output through a softmax function:\np(y|f W (x), \u03c3) = Softmax( 1 \u03c3 2 f W (x))(8)\nwith a positive scalar \u03c3. The log likelihood for this output can then be written as\nlog p(y = c|f W (x), \u03c3) = 1 \u03c3 2 f W c (x) \u2212 log c exp 1 \u03c3 2 f W c (x)(9)\nwith f W c (x) the c'th element of the vector f W (x). Next, assume that a model's multiple outputs are composed of a continuous output y 1 and a discrete output y 2 , modelled with a Gaussian likelihood and a softmax likelihood, respectively. Like before, the joint loss is given as:\nL(W, \u03c3 1 , \u03c3 2 ) = \u2212 log p(y 1 , y 2 = c|f W (x)) = \u2212 log N (y 1 ; f W (x), \u03c3 2 1 ) \u00b7 Softmax(y 2 = c; f W (x), \u03c3 2 ) = 1 2\u03c3 2 1 ||y 1 \u2212 f W (x)|| 2 + log \u03c3 2 1 \u2212 log p(y 2 = c|f W (x), \u03c3 2 ) = 1 2\u03c3 2 1 L 1 (W) + 1 \u03c3 2 2 L 2 (W) + log \u03c3 2 1 + log c exp 1 \u03c3 2 2 f W c (x) c exp f W c (x) 1 \u03c3 2 2 \u2248 1 2\u03c3 2 1 L 1 (W) + 1 \u03c3 2 2 L 2 (W) + log \u03c3 2 1 + log \u03c3 2 2 ,(10)\nwhere again we write L 1 (W) = ||y 1 \u2212 f W (x)|| 2 for the Euclidean loss of y 1 , write L 2 (W) = \u2212 log Softmax(y 2 , f W (x)) for the cross entropy loss of y 2 (with f W (x) not scaled), and optimise with respect to W as well as \u03c3 1 , \u03c3 2 . In the last transition we introduced the explicit simplifying\nassumption 1 \u03c3 2 2 c exp 1 \u03c3 2 2 f W c (x) \u2248 c exp f W c (x) 1 \u03c3 2\n2 which becomes an equality when \u03c3 2 2 \u2192 1. This has the advantage of simplifying the optimisation objective, as well as empirically improving results.\n\nThis last objective can be seen as learning the relative weights of the losses for each output. Large scale values \u03c3 2 will decrease the contribution of L 2 (W), whereas small scale \u03c3 2 will increase its contribution. The scale is regulated by the last term in the equation. The objective is penalised when setting \u03c3 2 too large (with the last term contributing a constant value log C -with C classes -to the loss).\n\nThe multi-task objective with homoscedastic task uncertainty now becomes:\nL(W, \u03c3 1 , \u03c3 2 , ..., \u03c3 i ) = i 1 2\u03c3 2 i L i (W) + log \u03c3 2 i(11)\nover all tasks indexed by i. Again, we write L i (W) = ||y i \u2212 f W (x)|| 2 for regression losses y i , and L i (W) = \u2212 log Softmax(y i , f W (x)) for classification losses. This construction can be trivially extended to arbitrary combinations of discrete and continuous variables, allowing us to learn the relative weights of each loss in a principled and well-founded way. This loss is smoothly differentiable, and is well formed such that the task weights will not converge to zero. In contrast, directly learning the weights using a simple linear sum of losses (1) would result in weights which quickly converge to zero. In the following sections we introduce our experimental model and present empirical results.\n\n\nScene Understanding Model\n\nTo understand semantics and geometry we first propose an architecture which can learn regression and classification outputs, at a pixel level. Our architecture is a deep convolutional encoder decoder network [8]. Our model consists of a number of convolutional encoders which produce a shared representation, followed by a corresponding number of task-specific convolutional decoders. A high level summary is shown in Figure 1 and additional model details are explained in Appendix A. Our encoder is based on ResNet-101 [30] (without the final fully connected layer). We then split the 2048 dimensional shared feature representation into individual decoders for each task. Each decoder consists of three convolutional layers for each task.\n\nSemantic Segmentation. We use the cross-entropy loss to learn pixel-wise class probabilities, averaging the loss over the pixels with semantic labels in each mini-batch.\n\nInstance Segmentation. An intuitive method for defining which instance a pixel belongs to is an association to the instance's centroid. We use a regression approach for instance segmentation [31]. This approach is inspired by [32] which identifies instances using Hough votes from object parts. In this work we extend this idea by using votes from individual pixels using deep learning. We learn an  Table 1: Quantitative improvement when learning semantic segmentation, instance segmentation and depth with our multi-task loss. Experiments were conducted on the Tiny CityScapes dataset, sub-sampled to a resolution of 256x512, results are shown from the validation set. We observe an improvement in performance when training with our multi-task loss, over both single-task models and weighted losses. Additionally, we observe an improvement when training on all three tasks (3 \u00d7 ) using our multi-task loss, compared with all pairs of tasks alone (denoted by 2 \u00d7 ). This shows that our loss function can automatically learn an better performing weighting between the tasks.\n\ninstance vector,x n , for each pixel coordinate, c n , which points to the centroid of the pixel's instance, i n , such that i n =x n + c n . We train this regression with an L 1 loss using ground truth labels x n , averaged over all labelled pixels, N I , in a mini-batch: L Instance = 1\n|N I | N I x n \u2212x n 1 .\nAn illustrated example is given in Appendix C.\n\nTo obtain segmentations for each instance, we now need to estimate the instance centres,\u00ee n . We propose to consider the estimated instance vectors,x n , as votes in a Hough parameter space and use a clustering algorithm to identify these instance centres. OPTICS [33], is an efficient density based clustering algorithm. It is able to identify an unknown number of multi-scale clusters with varying density from a given set of samples. We chose OPICS for two reasons. Crucially, it does not assume knowledge of the number of clusters like algorithms such as k-means [34]. Secondly, it does not assume a canonical instance size or density like discretised binning approaches [35]. Using OPTICS, we cluster the points c n +x n into a number of estimated instances,\u00ee. We can then assign each pixel, p n to the instance closest to its estimated instance vector, c n +x n .\n\nDepth Regression. We train with supervised labels using pixel-wise metric inverse depth using a L 1 loss function:\nL Depth = 1 |N D | N D d n \u2212d n 1 .\nOur architecture estimates inverse depth,d n , because it can represent points at infinite distance (such as sky). We can obtain inverse depth labels, d n , from a RGBD sensor or stereo imagery. Pixels which do not have an inverse depth label are ignored in the loss.\n\n\nExperiments\n\nWe demonstrate the efficacy of our method on CityScapes [36], a large dataset for road scene understanding. It comprises of stereo imagery, from automotive grade stereo cameras with a 22cm baseline, labelled with instance and semantic segmentations from 20 classes. Depth images are also provided, labelled using SGM [37], which we treat as ground truth. Additionally, we assign zero inverse depth to pixels labelled as sky. The dataset was collected from a number of cities in fine weather and consists of 3,250 training and 750 validation images at 2048 \u00d7 1024 resolution. 1,000 images are used for testing on an online evaluation server.\n\n\nModel Evaluation\n\nIn Table 1 we compare individual models to multi-task learning models using a na\u00efve weighted loss or the task uncertainty weighting we propose in this paper. To reduce the computational burden, we train each model at a reduced resolution of 128 \u00d7 256 pixels. This clearly illustrates the benefit of Results are shown on test images from the CityScapes dataset using our multi-task approach with a single network trained on all tasks. We observe that multi-task learning improves the smoothness and accuracy for depth perception because it learns a representation that uses cues from other tasks, such as segmentation (and vice versa).\n\nmulti-task learning, which obtains significantly better performing results than individual task models. For example, using our method we improve classification results from 43.1% to 46.6%.\n\nWe also compare to a number of na\u00efve multi-task losses. We compare weighting each task equally and using approximately optimal weights. Using a uniform weighting results in poor performance, in some cases not even improving on the results from the single task model. Obtaining approximately optimal weights is difficult with increasing number of tasks as it requires an expensive grid search over parameters. However, even these weights perform worse compared with our proposed method. Figure 2 shows that using task uncertainty weights can even perform better compared to optimal weights found through fine-grained grid search. We believe that this is due to two reasons. First, grid search is restricted in accuracy by the resolution of the search. Second, optimising the task weights using a homoscedastic noise term allows for the weights to be dynamic during training. In general, we observe that the uncertainty term decreases during training which improves the optimisation process.\n\nThis loss is also robust to the value we use to initialise the weights. In Appendix B we show with any reasonable initialisation of log \u03c3 2 from \u22122.0 to 5.0, the homoscedastic uncertainty terms converge to the same value after 100 training iterations. This is significantly less than the 30, 000 training iterations for the network. Therefore our model is robust to the choice of initial value for the weighting terms. Interestingly, we observe that this loss allows the network to dynamically tune the weighting. Typically, the homoscedastic noise terms decrease in magnitude as training progresses.\n\n\nConclusions\n\nWe have shown that correctly weighting loss terms is of paramount importance for multi-task learning problems. We demonstrated that homoscedastic (task) uncertainty is an effective way to weight losses. We derived a principled loss function which can learn a relative weighting automatically from the data and is robust to the weight initialization. We showed that this can improve performance for scene understanding tasks with a unified architecture for semantic segmentation, instance segmentation and per-pixel depth regression. We demonstrated modelling task-dependant homoscedastic uncertainty improves the model's representation and each task's performance when compared to separate models trained on each task individually.\n\nAn interesting question left unanswered is where the optimal location is for splitting the shared encoder network into separate decoders for each task? And, what network depth is best for the shared multi-task representation?\n\n\nA Scene Understanding Model Details\n\nThe purpose of the encoder is to learn a deep mapping to produce rich, contextual features, using domain knowledge from a number of related tasks. Our encoder is based on ResNet-101 [30] (without the final fully connected layer). We apply this encoder in a convolutional manner over the input image, which results in a 2048 dimensional shared feature representation. Inspired by the dilated convolutional approach of [38], this encoder feature map is sub-sampled by a factor of 8 compared to the input image dimensions.\n\nWe then split the network into separate decoders (with separate weights) for each task. The purpose of the decoder is to learn a mapping from the shared features to an output. Each decoder consists of three convolutional layers with kernel size 3 \u00d7 3, 1 \u00d7 1 and 1 \u00d7 1 respectively, and feature size 512, 512 and the number of output dimensions respectively.\n\n\nB Training Convergence Results\n\nOne of the attractive properties of our approach to weighting multi-task losses is that it is robust to the initialisation choice for the homoscedastic noise parameters. Figure 4 shows that for an array of initial choices of log \u03c3 2 from \u22122.0 to 5.0 the homoscedastic noise and task loss is able to converge to the same minima. Additionally, the homoscedastic noise terms converges after only 100 iterations, while the network requires 30, 000+ iterations to train.  Figure 5 details the representation we use for instance segmentation. Figure 5(a) shows the input image and a mask of the pixels which are of an instance class (at test time inferred from the predicted semantic segmentation). Figure 5(b) and Figure 5(c) show the ground truth and predicted instance vectors for both x and y coordinates. We then cluster these votes using OPTICS [33], resulting in the predicted instance segmentation output in Figure 5(d). One of the most difficult cases for instance segmentation algorithms to handle is when the instance mask is split due to occlusion. Figure 6 shows that our method can handle these situations, by allowing pixels to vote for their instance centroid with geometry. Methods which rely on watershed approaches [15], or instance edge identification approaches fail in these scenarios.\n\n\nC Instance Segmentation Parametrisation with Centroid Vectors\n\n(a) Input Image (b) Instance Segmentation Figure 6: This example shows two cars which are occluded by trees and lampposts, making the instance segmentation challenging. Our instance segmentation method can handle occlusions effectively. We can correctly handle segmentation masks which are split by occlusion, yet part of the same instance, By incorporating semantics and geometry.\n\n\nD Further Qualitative Results\n\n(a) Input image (b) Segmentation output (c) Instance output (d) Depth output \n\nFigure 2 :\n2Learning multiple tasks improves the model's representation and individual task performance. These figures and tables illustrate the advantages of multi-task learning for (a) semantic classification and depth regression and (b) instance and depth regression. Performance of the model in individual tasks is seen at both edges of the plot where w = 0 and w = 1.\n\nFigure 3 :\n3Qualitative results for multi-task learning of geometry and semantics for road scene understanding.\n\nFigure 4 :\n4Training plots showing convergence of homoscedastic noise and task loss for an array of initialisation choices for the homoscedastic uncertainty terms for all three tasks. The left plot shows that the loss converges to the same minimum from varying initialisation choices. The centre plot shows the the homoscedastic noise value optimises to the same solution from a variety of initialisations. The plots on the right show a zoomed in view of the homoscedastic noise plot, showing the initialisation and convergence over a few hundred training iterations. Despite the network taking 10, 000+ iterations for the training loss to converge, the task uncertainty converges very rapidly after only 100 iterations.\n\nFigure 5 :\n5Instance centroid regression training data. For each pixel, we regress a vector pointing to the instance's centroid. The loss is computed with the mask, and x and y instance centroid vectors.\n\nFigure 7 :\n7More qualitative results on test images from the CityScapes dataset.\n\n\n(a) Comparing loss weightings when learning semantic classification and depth regression0 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \n0.6 \n0.7 \n0.8 \n0.9 \n1 \n\n40 \n\n42 \n\n44 \n\n46 \n\nClassification Weight \n\nIoU Classification (%) \n\nClassification \n\n0 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \n0.6 \n0.7 \n0.8 \n0.9 \n1 \n\n0.72 \n\n0.74 \n\n0.76 \n\nDepth Weight \nRMS Inverse Depth Error (m \u22121 \n\n) \n\nClassification \nDepth Regression \n\nTask Weights \nClass \nDepth \nClass \nDepth \nIoU [%] RMS [m \u22121 ] \n\n1.0 \n0.0 \n43.1 \n-\n0.975 \n0.025 \n45.1 \n0.764 \n0.95 \n0.05 \n45.5 \n0.737 \n0.9 \n0.1 \n46.0 \n0.730 \n0.85 \n0.15 \n46.1 \n0.729 \n0.8 \n0.2 \n45.3 \n0.724 \n0.7 \n0.3 \n43.5 \n0.722 \n0.5 \n0.5 \n43.0 \n0.724 \n0.1 \n0.9 \n41.0 \n0.738 \n0.0 \n1.0 \n-\n0.740 \n\nLearned weights \n46.2 \n0.714 \nwith task uncertainty \n(this work, Section 3.2) \n\n0 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \n0.6 \n0.7 \n0.8 \n0.9 \n1 \n\n3.8 \n\n4 \n\n4.2 \n\n4.4 \n\n4.6 \n\n4.8 \n\n5 \n\nInstance Weight \n\nRMS Instance (px) \n\nInstance Regression \n\n0 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \n0.6 \n0.7 \n0.8 \n0.9 \n1 \n\n0.75 \n\n0.8 \n\n0.85 \n\n0.9 \n\nDepth Weight \nRMS Inverse Depth Error (m \u22121 \n\n) \nInstance Regression \nDepth Regression \n\nTask Weights \nInstance \nDepth \nInstance \nDepth \nRMS [px] RMS [m \u22121 ] \n\n1.0 \n0.0 \n4.61 \n-\n0.6 \n0.4 \n4.50 \n0.887 \n0.5 \n0.5 \n4.30 \n0.873 \n0.4 \n0.6 \n4.14 \n0.863 \n0.3 \n0.7 \n4.04 \n0.775 \n0.2 \n0.8 \n3.83 \n0.761 \n0.1 \n0.9 \n3.91 \n0.751 \n0.05 \n0.95 \n4.27 \n0.760 \n0.0 \n1.0 \n-\n0.783 \n\nLearned weights \n4.06 \n0.744 \nwith task uncertainty \n(this work, Section 3.2) \n\n\n\nMultitask learning. Rich Caruana, Learning to learn. SpringerRich Caruana. Multitask learning. In Learning to learn, pages 95-133. Springer, 1998.\n\nUbernet: Training auniversal'convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. Iasonas Kokkinos, arXiv:1609.02132arXiv preprintIasonas Kokkinos. Ubernet: Training auniversal'convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. arXiv preprint arXiv:1609.02132, 2016.\n\nA unified architecture for natural language processing: Deep neural networks with multitask learning. Ronan Collobert, Jason Weston, Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learningACMRonan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160-167. ACM, 2008.\n\nCross-language knowledge transfer using multilingual deep neural network with shared hidden layers. Jui-Ting Huang, Jinyu Li, Dong Yu, Li Deng, Yifan Gong, Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEEJui-Ting Huang, Jinyu Li, Dong Yu, Li Deng, and Yifan Gong. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 7304-7308. IEEE, 2013.\n\nOverfeat: Integrated recognition, localization and detection using convolutional networks. Pierre Sermanet, David Eigen, Xiang Zhang, Micha\u00ebl Mathieu, Rob Fergus, Yann Lecun, International Conference on Learning Representations (ICLR). Pierre Sermanet, David Eigen, Xiang Zhang, Micha\u00ebl Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. International Conference on Learning Representations (ICLR), 2014.\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. David Eigen, Rob Fergus, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionDavid Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In Proceedings of the IEEE International Conference on Computer Vision, pages 2650-2658, 2015.\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta- tion. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2015.\n\nSegnet: A deep convolutional encoder-decoder architecture for scene segmentation. Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, IEEE Transactions on Pattern Analysis and Machine Intelligence. Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for scene segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\nMulti-scale context aggregation by dilated convolutions. Fisher Yu, Vladlen Koltun, ICLR. Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016.\n\nSemantic image segmentation with deep convolutional nets and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, ICLR. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2015.\n\nConditional random fields as recurrent neural networks. Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip Torr, International Conference on Computer Vision (ICCV). Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip Torr. Conditional random fields as recurrent neural networks. In International Conference on Computer Vision (ICCV), 2015.\n\nLearning to segment object candidates. O Pedro, Ronan Pinheiro, Piotr Collobert, Dollar, Advances in Neural Information Processing Systems. Pedro O Pinheiro, Ronan Collobert, and Piotr Dollar. Learning to segment object candidates. In Advances in Neural Information Processing Systems, pages 1990-1998, 2015.\n\nHypercolumns for object segmentation and fine-grained localization. Pablo Bharath Hariharan, Ross Arbel\u00e1ez, Jitendra Girshick, Malik, Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionIEEEBharath Hariharan, Pablo Arbel\u00e1ez, Ross Girshick, and Jitendra Malik. Hypercolumns for object segmen- tation and fine-grained localization. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, pages 447-456. IEEE, 2014.\n\nInstance-aware semantic segmentation via multi-task network cascades. Jifeng Dai, Kaiming He, Jian Sun, Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionJifeng Dai, Kaiming He, and Jian Sun. Instance-aware semantic segmentation via multi-task network cascades. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2016.\n\nDeep watershed transform for instance segmentation. Min Bai, Raquel Urtasun, arXiv:1611.08303arXiv preprintMin Bai and Raquel Urtasun. Deep watershed transform for instance segmentation. arXiv preprint arXiv:1611.08303, 2016.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, pages 580-587, 2014.\n\nUnsupervised cnn for single view depth estimation: Geometry to the rescue. Ravi Garg, Ian Reid, Computer Vision-ECCV 2016. Ravi Garg and Ian Reid. Unsupervised cnn for single view depth estimation: Geometry to the rescue. Computer Vision-ECCV 2016, pages 740-756, 2016.\n\nIs learning the n-th thing any easier than learning the first?. Sebastian Thrun, Advances in neural information processing systems. MORGAN KAUFMANN PUBLISHERSSebastian Thrun. Is learning the n-th thing any easier than learning the first? In Advances in neural information processing systems, pages 640-646. MORGAN KAUFMANN PUBLISHERS, 1996.\n\nA model of inductive bias learning. Jonathan Baxter, J. Artif. Intell. Res.(JAIR). 123Jonathan Baxter et al. A model of inductive bias learning. J. Artif. Intell. Res.(JAIR), 12(149-198):3, 2000.\n\nLearning to see by moving. Pulkit Agrawal, Joao Carreira, Jitendra Malik, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionPulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In Proceedings of the IEEE International Conference on Computer Vision, pages 37-45, 2015.\n\nLearning and transferring mid-level image representations using convolutional neural networks. Maxime Oquab, Leon Bottou, Ivan Laptev, Josef Sivic, Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionIEEEMaxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, pages 1717-1724. IEEE, 2014.\n\nOvercoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Proceedings of the National Academy of Sciences. 201611835James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, page 201611835, 2017.\n\nMultimodal deep learning. Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, Andrew Y Ng, Proceedings of the 28th international conference on machine learning (ICML-11). the 28th international conference on machine learning (ICML-11)Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal deep learning. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 689-696, 2011.\n\nUnderstand scene categories by objects: A semantic regularized scene classifier using convolutional neural networks. Yiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi, Yong Liu, 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEEYiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi, and Yong Liu. Understand scene categories by objects: A semantic regularized scene classifier using convolutional neural networks. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 2318-2325. IEEE, 2016.\n\nMultinet: Real-time joint semantic reasoning for autonomous driving. Marvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla, Raquel Urtasun, arXiv:1612.07695arXiv preprintMarvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla, and Raquel Urtasun. Multinet: Real-time joint semantic reasoning for autonomous driving. arXiv preprint arXiv:1612.07695, 2016.\n\nCross-stitch networks for multi-task learning. Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIshan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3994-4003, 2016.\n\nPixel-level encoding and depth layering for instance-level semantic labeling. Jonas Uhrig, Marius Cordts, Uwe Franke, Thomas Brox, arXiv:1604.05096arXiv preprintJonas Uhrig, Marius Cordts, Uwe Franke, and Thomas Brox. Pixel-level encoding and depth layering for instance-level semantic labeling. arXiv preprint arXiv:1604.05096, 2016.\n\nConvolutional networks for real-time 6-dof camera relocalization. Alex Kendall, Matthew Grimes, Roberto Cipolla, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)Alex Kendall, Matthew Grimes, and Roberto Cipolla. Convolutional networks for real-time 6-dof camera relocalization. In Proceedings of the International Conference on Computer Vision (ICCV), 2015.\n\nWhat uncertainties do we need in bayesian deep learning for computer vision?. Alex Kendall, Yarin Gal, arXiv:1703.04977arXiv preprintAlex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? arXiv preprint arXiv:1703.04977, 2017.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2016.\n\nProposal-free network for instance-level object segmentation. Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Jianchao Yang, Liang Lin, Shuicheng Yan, arXiv:1509.02636arXiv preprintXiaodan Liang, Yunchao Wei, Xiaohui Shen, Jianchao Yang, Liang Lin, and Shuicheng Yan. Proposal-free network for instance-level object segmentation. arXiv preprint arXiv:1509.02636, 2015.\n\nRobust object detection with interleaved categorization and segmentation. Ale\u0161 Bastian Leibe, Bernt Leonardis, Schiele, International Journal of Computer Vision (IJCV). 771-3Bastian Leibe, Ale\u0161 Leonardis, and Bernt Schiele. Robust object detection with interleaved categorization and segmentation. International Journal of Computer Vision (IJCV), 77(1-3):259-289, 2008.\n\nOptics: ordering points to identify the clustering structure. Mihael Ankerst, M Markus, Hans-Peter Breunig, J\u00f6rg Kriegel, Sander, ACM Sigmod Record. ACM28Mihael Ankerst, Markus M Breunig, Hans-Peter Kriegel, and J\u00f6rg Sander. Optics: ordering points to identify the clustering structure. In ACM Sigmod Record, volume 28, pages 49-60. ACM, 1999.\n\nSome methods for classification and analysis of multivariate observations. James Macqueen, Proceedings of the fifth Berkeley symposium on mathematical statistics and probability. the fifth Berkeley symposium on mathematical statistics and probabilityOakland, CA, USA1James MacQueen et al. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281-297. Oakland, CA, USA., 1967.\n\nMean shift: A robust approach toward feature space analysis. Dorin Comaniciu, Peter Meer, IEEE Transactions. 245Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on pattern analysis and machine intelligence, 24(5):603-619, 2002.\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2016.\n\nStereo processing by semiglobal matching and mutual information. Heiko Hirschmuller, IEEE Transactions. 302Heiko Hirschmuller. Stereo processing by semiglobal matching and mutual information. IEEE Transactions on pattern analysis and machine intelligence, 30(2):328-341, 2008.\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, Deeplab, arXiv:1606.00915Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv preprintLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv preprint arXiv:1606.00915, 2016.\n", "annotations": {"author": "[{\"end\":177,\"start\":90},{\"end\":262,\"start\":178},{\"end\":353,\"start\":263}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":95},{\"end\":187,\"start\":184},{\"end\":278,\"start\":271}]", "author_first_name": "[{\"end\":94,\"start\":90},{\"end\":183,\"start\":178},{\"end\":270,\"start\":263}]", "author_affiliation": "[{\"end\":176,\"start\":104},{\"end\":261,\"start\":189},{\"end\":352,\"start\":280}]", "title": "[{\"end\":87,\"start\":1},{\"end\":440,\"start\":354}]", "venue": null, "abstract": "[{\"end\":1393,\"start\":442}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1553,\"start\":1550},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1654,\"start\":1651},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1689,\"start\":1686},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1715,\"start\":1712},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2541,\"start\":2538},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2543,\"start\":2541},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2545,\"start\":2543},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3646,\"start\":3643},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3649,\"start\":3646},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3652,\"start\":3649},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3656,\"start\":3652},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3660,\"start\":3656},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4426,\"start\":4422},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4430,\"start\":4426},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4434,\"start\":4430},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4438,\"start\":4434},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4708,\"start\":4704},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4857,\"start\":4854},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4879,\"start\":4875},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6544,\"start\":6540},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6547,\"start\":6544},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6848,\"start\":6845},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6867,\"start\":6863},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6870,\"start\":6867},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7111,\"start\":7108},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7182,\"start\":7178},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7264,\"start\":7260},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7368,\"start\":7364},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7533,\"start\":7529},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7569,\"start\":7566},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7584,\"start\":7580},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7692,\"start\":7688},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7768,\"start\":7764},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7942,\"start\":7939},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8023,\"start\":8019},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8092,\"start\":8089},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8350,\"start\":8347},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9672,\"start\":9669},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9675,\"start\":9672},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9678,\"start\":9675},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9681,\"start\":9678},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9725,\"start\":9722},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9760,\"start\":9757},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9843,\"start\":9839},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11277,\"start\":11273},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11929,\"start\":11926},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18816,\"start\":18813},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19129,\"start\":19125},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19712,\"start\":19708},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19747,\"start\":19743},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21222,\"start\":21218},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21525,\"start\":21521},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21633,\"start\":21629},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22319,\"start\":22315},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22580,\"start\":22576},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26537,\"start\":26533},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26772,\"start\":26768},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28113,\"start\":28109},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28496,\"start\":28492}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":29497,\"start\":29124},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29610,\"start\":29498},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30332,\"start\":29611},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30537,\"start\":30333},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30619,\"start\":30538},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32049,\"start\":30620}]", "paragraph": "[{\"end\":2387,\"start\":1409},{\"end\":3450,\"start\":2389},{\"end\":5089,\"start\":3452},{\"end\":5876,\"start\":5091},{\"end\":5930,\"start\":5878},{\"end\":6376,\"start\":5932},{\"end\":6849,\"start\":6393},{\"end\":7369,\"start\":6851},{\"end\":8718,\"start\":7371},{\"end\":9248,\"start\":8720},{\"end\":9594,\"start\":9303},{\"end\":10202,\"start\":9620},{\"end\":11136,\"start\":10204},{\"end\":11278,\"start\":11196},{\"end\":12039,\"start\":11280},{\"end\":12195,\"start\":12128},{\"end\":12337,\"start\":12197},{\"end\":12638,\"start\":12339},{\"end\":13012,\"start\":12640},{\"end\":13382,\"start\":13039},{\"end\":13584,\"start\":13419},{\"end\":13851,\"start\":13621},{\"end\":14007,\"start\":13918},{\"end\":14151,\"start\":14009},{\"end\":14483,\"start\":14211},{\"end\":14605,\"start\":14485},{\"end\":14813,\"start\":14724},{\"end\":15114,\"start\":15000},{\"end\":15663,\"start\":15116},{\"end\":15928,\"start\":15665},{\"end\":16057,\"start\":15974},{\"end\":16415,\"start\":16131},{\"end\":17082,\"start\":16778},{\"end\":17301,\"start\":17150},{\"end\":17718,\"start\":17303},{\"end\":17793,\"start\":17720},{\"end\":18575,\"start\":17859},{\"end\":19344,\"start\":18605},{\"end\":19515,\"start\":19346},{\"end\":20591,\"start\":19517},{\"end\":20881,\"start\":20593},{\"end\":20952,\"start\":20906},{\"end\":21823,\"start\":20954},{\"end\":21939,\"start\":21825},{\"end\":22243,\"start\":21976},{\"end\":22899,\"start\":22259},{\"end\":23554,\"start\":22920},{\"end\":23744,\"start\":23556},{\"end\":24735,\"start\":23746},{\"end\":25337,\"start\":24737},{\"end\":26084,\"start\":25353},{\"end\":26311,\"start\":26086},{\"end\":26870,\"start\":26351},{\"end\":27229,\"start\":26872},{\"end\":28565,\"start\":27264},{\"end\":29012,\"start\":28631},{\"end\":29123,\"start\":29046}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9619,\"start\":9595},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13418,\"start\":13383},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13620,\"start\":13585},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13917,\"start\":13852},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14210,\"start\":14152},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14723,\"start\":14606},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14999,\"start\":14814},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15973,\"start\":15929},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16130,\"start\":16058},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16777,\"start\":16416},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17149,\"start\":17083},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17858,\"start\":17794},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20905,\"start\":20882},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21975,\"start\":21940}]", "table_ref": "[{\"end\":19924,\"start\":19917},{\"end\":22930,\"start\":22923}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1407,\"start\":1395},{\"attributes\":{\"n\":\"2\"},\"end\":6391,\"start\":6379},{\"attributes\":{\"n\":\"3\"},\"end\":9301,\"start\":9251},{\"attributes\":{\"n\":\"3.1\"},\"end\":11194,\"start\":11139},{\"end\":12126,\"start\":12042},{\"attributes\":{\"n\":\"3.2\"},\"end\":13037,\"start\":13015},{\"attributes\":{\"n\":\"4\"},\"end\":18603,\"start\":18578},{\"attributes\":{\"n\":\"5\"},\"end\":22257,\"start\":22246},{\"attributes\":{\"n\":\"5.1\"},\"end\":22918,\"start\":22902},{\"attributes\":{\"n\":\"6\"},\"end\":25351,\"start\":25340},{\"end\":26349,\"start\":26314},{\"end\":27262,\"start\":27232},{\"end\":28629,\"start\":28568},{\"end\":29044,\"start\":29015},{\"end\":29135,\"start\":29125},{\"end\":29509,\"start\":29499},{\"end\":29622,\"start\":29612},{\"end\":30344,\"start\":30334},{\"end\":30549,\"start\":30539}]", "table": "[{\"end\":32049,\"start\":30710}]", "figure_caption": "[{\"end\":29497,\"start\":29137},{\"end\":29610,\"start\":29511},{\"end\":30332,\"start\":29624},{\"end\":30537,\"start\":30346},{\"end\":30619,\"start\":30551},{\"end\":30710,\"start\":30622}]", "figure_ref": "[{\"end\":3671,\"start\":3663},{\"end\":5467,\"start\":5459},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10003,\"start\":9995},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10339,\"start\":10331},{\"end\":19031,\"start\":19023},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24240,\"start\":24232},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27442,\"start\":27434},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27739,\"start\":27731},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27809,\"start\":27801},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27965,\"start\":27957},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27981,\"start\":27973},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28182,\"start\":28174},{\"end\":28327,\"start\":28319},{\"end\":28681,\"start\":28673}]", "bib_author_first_name": "[{\"end\":32075,\"start\":32071},{\"end\":32345,\"start\":32338},{\"end\":32690,\"start\":32685},{\"end\":32707,\"start\":32702},{\"end\":33186,\"start\":33178},{\"end\":33199,\"start\":33194},{\"end\":33208,\"start\":33204},{\"end\":33215,\"start\":33213},{\"end\":33227,\"start\":33222},{\"end\":33706,\"start\":33700},{\"end\":33722,\"start\":33717},{\"end\":33735,\"start\":33730},{\"end\":33750,\"start\":33743},{\"end\":33763,\"start\":33760},{\"end\":33776,\"start\":33772},{\"end\":34205,\"start\":34200},{\"end\":34216,\"start\":34213},{\"end\":34642,\"start\":34634},{\"end\":34653,\"start\":34649},{\"end\":34671,\"start\":34665},{\"end\":35062,\"start\":35057},{\"end\":35083,\"start\":35079},{\"end\":35100,\"start\":35093},{\"end\":35447,\"start\":35441},{\"end\":35459,\"start\":35452},{\"end\":35671,\"start\":35660},{\"end\":35684,\"start\":35678},{\"end\":35704,\"start\":35697},{\"end\":35720,\"start\":35715},{\"end\":35733,\"start\":35729},{\"end\":35735,\"start\":35734},{\"end\":35998,\"start\":35993},{\"end\":36012,\"start\":36006},{\"end\":36035,\"start\":36025},{\"end\":36058,\"start\":36052},{\"end\":36075,\"start\":36067},{\"end\":36086,\"start\":36080},{\"end\":36096,\"start\":36091},{\"end\":36110,\"start\":36104},{\"end\":36455,\"start\":36454},{\"end\":36468,\"start\":36463},{\"end\":36484,\"start\":36479},{\"end\":36798,\"start\":36793},{\"end\":36822,\"start\":36818},{\"end\":36841,\"start\":36833},{\"end\":37288,\"start\":37282},{\"end\":37301,\"start\":37294},{\"end\":37310,\"start\":37306},{\"end\":37667,\"start\":37664},{\"end\":37679,\"start\":37673},{\"end\":37925,\"start\":37921},{\"end\":37940,\"start\":37936},{\"end\":37956,\"start\":37950},{\"end\":37974,\"start\":37966},{\"end\":38411,\"start\":38407},{\"end\":38421,\"start\":38418},{\"end\":38676,\"start\":38667},{\"end\":38989,\"start\":38981},{\"end\":39175,\"start\":39169},{\"end\":39189,\"start\":39185},{\"end\":39208,\"start\":39200},{\"end\":39608,\"start\":39602},{\"end\":39620,\"start\":39616},{\"end\":39633,\"start\":39629},{\"end\":39647,\"start\":39642},{\"end\":40082,\"start\":40077},{\"end\":40102,\"start\":40096},{\"end\":40116,\"start\":40112},{\"end\":40133,\"start\":40129},{\"end\":40151,\"start\":40142},{\"end\":40170,\"start\":40164},{\"end\":40172,\"start\":40171},{\"end\":40185,\"start\":40179},{\"end\":40197,\"start\":40193},{\"end\":40209,\"start\":40204},{\"end\":40228,\"start\":40219},{\"end\":40643,\"start\":40637},{\"end\":40657,\"start\":40651},{\"end\":40672,\"start\":40666},{\"end\":40683,\"start\":40678},{\"end\":40696,\"start\":40689},{\"end\":40710,\"start\":40702},{\"end\":41192,\"start\":41188},{\"end\":41205,\"start\":41199},{\"end\":41219,\"start\":41216},{\"end\":41229,\"start\":41226},{\"end\":41239,\"start\":41235},{\"end\":41675,\"start\":41669},{\"end\":41694,\"start\":41687},{\"end\":41708,\"start\":41702},{\"end\":41726,\"start\":41719},{\"end\":41742,\"start\":41736},{\"end\":42030,\"start\":42025},{\"end\":42045,\"start\":42038},{\"end\":42066,\"start\":42059},{\"end\":42081,\"start\":42074},{\"end\":42536,\"start\":42531},{\"end\":42550,\"start\":42544},{\"end\":42562,\"start\":42559},{\"end\":42577,\"start\":42571},{\"end\":42859,\"start\":42855},{\"end\":42876,\"start\":42869},{\"end\":42892,\"start\":42885},{\"end\":43307,\"start\":43303},{\"end\":43322,\"start\":43317},{\"end\":43556,\"start\":43549},{\"end\":43568,\"start\":43561},{\"end\":43584,\"start\":43576},{\"end\":43594,\"start\":43590},{\"end\":43958,\"start\":43951},{\"end\":43973,\"start\":43966},{\"end\":43986,\"start\":43979},{\"end\":44001,\"start\":43993},{\"end\":44013,\"start\":44008},{\"end\":44028,\"start\":44019},{\"end\":44331,\"start\":44327},{\"end\":44352,\"start\":44347},{\"end\":44692,\"start\":44686},{\"end\":44703,\"start\":44702},{\"end\":44722,\"start\":44712},{\"end\":44736,\"start\":44732},{\"end\":45049,\"start\":45044},{\"end\":45541,\"start\":45536},{\"end\":45558,\"start\":45553},{\"end\":45835,\"start\":45829},{\"end\":45851,\"start\":45844},{\"end\":45868,\"start\":45859},{\"end\":45880,\"start\":45876},{\"end\":45896,\"start\":45890},{\"end\":45915,\"start\":45908},{\"end\":45929,\"start\":45926},{\"end\":45944,\"start\":45938},{\"end\":45956,\"start\":45951},{\"end\":46428,\"start\":46423},{\"end\":46647,\"start\":46636},{\"end\":46660,\"start\":46654},{\"end\":46680,\"start\":46673},{\"end\":46696,\"start\":46691},{\"end\":46709,\"start\":46705},{\"end\":46711,\"start\":46710}]", "bib_author_last_name": "[{\"end\":32083,\"start\":32076},{\"end\":32354,\"start\":32346},{\"end\":32700,\"start\":32691},{\"end\":32714,\"start\":32708},{\"end\":33192,\"start\":33187},{\"end\":33202,\"start\":33200},{\"end\":33211,\"start\":33209},{\"end\":33220,\"start\":33216},{\"end\":33232,\"start\":33228},{\"end\":33715,\"start\":33707},{\"end\":33728,\"start\":33723},{\"end\":33741,\"start\":33736},{\"end\":33758,\"start\":33751},{\"end\":33770,\"start\":33764},{\"end\":33782,\"start\":33777},{\"end\":34211,\"start\":34206},{\"end\":34223,\"start\":34217},{\"end\":34647,\"start\":34643},{\"end\":34663,\"start\":34654},{\"end\":34679,\"start\":34672},{\"end\":35077,\"start\":35063},{\"end\":35091,\"start\":35084},{\"end\":35108,\"start\":35101},{\"end\":35450,\"start\":35448},{\"end\":35466,\"start\":35460},{\"end\":35676,\"start\":35672},{\"end\":35695,\"start\":35685},{\"end\":35713,\"start\":35705},{\"end\":35727,\"start\":35721},{\"end\":35742,\"start\":35736},{\"end\":36004,\"start\":35999},{\"end\":36023,\"start\":36013},{\"end\":36050,\"start\":36036},{\"end\":36065,\"start\":36059},{\"end\":36078,\"start\":36076},{\"end\":36089,\"start\":36087},{\"end\":36102,\"start\":36097},{\"end\":36115,\"start\":36111},{\"end\":36461,\"start\":36456},{\"end\":36477,\"start\":36469},{\"end\":36494,\"start\":36485},{\"end\":36502,\"start\":36496},{\"end\":36816,\"start\":36799},{\"end\":36831,\"start\":36823},{\"end\":36850,\"start\":36842},{\"end\":36857,\"start\":36852},{\"end\":37292,\"start\":37289},{\"end\":37304,\"start\":37302},{\"end\":37314,\"start\":37311},{\"end\":37671,\"start\":37668},{\"end\":37687,\"start\":37680},{\"end\":37934,\"start\":37926},{\"end\":37948,\"start\":37941},{\"end\":37964,\"start\":37957},{\"end\":37980,\"start\":37975},{\"end\":38416,\"start\":38412},{\"end\":38426,\"start\":38422},{\"end\":38682,\"start\":38677},{\"end\":38996,\"start\":38990},{\"end\":39183,\"start\":39176},{\"end\":39198,\"start\":39190},{\"end\":39214,\"start\":39209},{\"end\":39614,\"start\":39609},{\"end\":39627,\"start\":39621},{\"end\":39640,\"start\":39634},{\"end\":39653,\"start\":39648},{\"end\":40094,\"start\":40083},{\"end\":40110,\"start\":40103},{\"end\":40127,\"start\":40117},{\"end\":40140,\"start\":40134},{\"end\":40162,\"start\":40152},{\"end\":40177,\"start\":40173},{\"end\":40191,\"start\":40186},{\"end\":40202,\"start\":40198},{\"end\":40217,\"start\":40210},{\"end\":40246,\"start\":40229},{\"end\":40649,\"start\":40644},{\"end\":40664,\"start\":40658},{\"end\":40676,\"start\":40673},{\"end\":40687,\"start\":40684},{\"end\":40700,\"start\":40697},{\"end\":40713,\"start\":40711},{\"end\":41197,\"start\":41193},{\"end\":41214,\"start\":41206},{\"end\":41224,\"start\":41220},{\"end\":41233,\"start\":41230},{\"end\":41243,\"start\":41240},{\"end\":41685,\"start\":41676},{\"end\":41700,\"start\":41695},{\"end\":41717,\"start\":41709},{\"end\":41734,\"start\":41727},{\"end\":41750,\"start\":41743},{\"end\":42036,\"start\":42031},{\"end\":42057,\"start\":42046},{\"end\":42072,\"start\":42067},{\"end\":42088,\"start\":42082},{\"end\":42542,\"start\":42537},{\"end\":42557,\"start\":42551},{\"end\":42569,\"start\":42563},{\"end\":42582,\"start\":42578},{\"end\":42867,\"start\":42860},{\"end\":42883,\"start\":42877},{\"end\":42900,\"start\":42893},{\"end\":43315,\"start\":43308},{\"end\":43326,\"start\":43323},{\"end\":43559,\"start\":43557},{\"end\":43574,\"start\":43569},{\"end\":43588,\"start\":43585},{\"end\":43598,\"start\":43595},{\"end\":43964,\"start\":43959},{\"end\":43977,\"start\":43974},{\"end\":43991,\"start\":43987},{\"end\":44006,\"start\":44002},{\"end\":44017,\"start\":44014},{\"end\":44032,\"start\":44029},{\"end\":44345,\"start\":44332},{\"end\":44362,\"start\":44353},{\"end\":44371,\"start\":44364},{\"end\":44700,\"start\":44693},{\"end\":44710,\"start\":44704},{\"end\":44730,\"start\":44723},{\"end\":44744,\"start\":44737},{\"end\":44752,\"start\":44746},{\"end\":45058,\"start\":45050},{\"end\":45551,\"start\":45542},{\"end\":45563,\"start\":45559},{\"end\":45842,\"start\":45836},{\"end\":45857,\"start\":45852},{\"end\":45874,\"start\":45869},{\"end\":45888,\"start\":45881},{\"end\":45906,\"start\":45897},{\"end\":45924,\"start\":45916},{\"end\":45936,\"start\":45930},{\"end\":45949,\"start\":45945},{\"end\":45964,\"start\":45957},{\"end\":46441,\"start\":46429},{\"end\":46652,\"start\":46648},{\"end\":46671,\"start\":46661},{\"end\":46689,\"start\":46681},{\"end\":46703,\"start\":46697},{\"end\":46718,\"start\":46712},{\"end\":46727,\"start\":46720}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":45998148},\"end\":32197,\"start\":32051},{\"attributes\":{\"doi\":\"arXiv:1609.02132\",\"id\":\"b1\"},\"end\":32581,\"start\":32199},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2617020},\"end\":33076,\"start\":32583},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6828602},\"end\":33607,\"start\":33078},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4071727},\"end\":34090,\"start\":33609},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":102496818},\"end\":34576,\"start\":34092},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1629541},\"end\":34973,\"start\":34578},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":60814714},\"end\":35382,\"start\":34975},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":17127188},\"end\":35575,\"start\":35384},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1996665},\"end\":35935,\"start\":35577},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1318262},\"end\":36413,\"start\":35937},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":140529},\"end\":36723,\"start\":36415},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":12225766},\"end\":37210,\"start\":36725},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8510667},\"end\":37610,\"start\":37212},{\"attributes\":{\"doi\":\"arXiv:1611.08303\",\"id\":\"b14\"},\"end\":37837,\"start\":37612},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":215827080},\"end\":38330,\"start\":37839},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":299085},\"end\":38601,\"start\":38332},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1016169},\"end\":38943,\"start\":38603},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9803204},\"end\":39140,\"start\":38945},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1637703},\"end\":39505,\"start\":39142},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206592191},\"end\":40020,\"start\":39507},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4704285},\"end\":40609,\"start\":40022},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":352650},\"end\":41069,\"start\":40611},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7037275},\"end\":41598,\"start\":41071},{\"attributes\":{\"doi\":\"arXiv:1612.07695\",\"id\":\"b24\"},\"end\":41976,\"start\":41600},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1923223},\"end\":42451,\"start\":41978},{\"attributes\":{\"doi\":\"arXiv:1604.05096\",\"id\":\"b26\"},\"end\":42787,\"start\":42453},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15492888},\"end\":43223,\"start\":42789},{\"attributes\":{\"doi\":\"arXiv:1703.04977\",\"id\":\"b28\"},\"end\":43501,\"start\":43225},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":206594692},\"end\":43887,\"start\":43503},{\"attributes\":{\"doi\":\"arXiv:1509.02636\",\"id\":\"b30\"},\"end\":44251,\"start\":43889},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14144539},\"end\":44622,\"start\":44253},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9378040},\"end\":44967,\"start\":44624},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6278891},\"end\":45473,\"start\":44969},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":691081},\"end\":45764,\"start\":45475},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":502946},\"end\":46356,\"start\":45766},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":18327083},\"end\":46634,\"start\":46358},{\"attributes\":{\"doi\":\"arXiv:1606.00915\",\"id\":\"b37\"},\"end\":47102,\"start\":46636}]", "bib_title": "[{\"end\":32069,\"start\":32051},{\"end\":32683,\"start\":32583},{\"end\":33176,\"start\":33078},{\"end\":33698,\"start\":33609},{\"end\":34198,\"start\":34092},{\"end\":34632,\"start\":34578},{\"end\":35055,\"start\":34975},{\"end\":35439,\"start\":35384},{\"end\":35658,\"start\":35577},{\"end\":35991,\"start\":35937},{\"end\":36452,\"start\":36415},{\"end\":36791,\"start\":36725},{\"end\":37280,\"start\":37212},{\"end\":37919,\"start\":37839},{\"end\":38405,\"start\":38332},{\"end\":38665,\"start\":38603},{\"end\":38979,\"start\":38945},{\"end\":39167,\"start\":39142},{\"end\":39600,\"start\":39507},{\"end\":40075,\"start\":40022},{\"end\":40635,\"start\":40611},{\"end\":41186,\"start\":41071},{\"end\":42023,\"start\":41978},{\"end\":42853,\"start\":42789},{\"end\":43547,\"start\":43503},{\"end\":44325,\"start\":44253},{\"end\":44684,\"start\":44624},{\"end\":45042,\"start\":44969},{\"end\":45534,\"start\":45475},{\"end\":45827,\"start\":45766},{\"end\":46421,\"start\":46358}]", "bib_author": "[{\"end\":32085,\"start\":32071},{\"end\":32356,\"start\":32338},{\"end\":32702,\"start\":32685},{\"end\":32716,\"start\":32702},{\"end\":33194,\"start\":33178},{\"end\":33204,\"start\":33194},{\"end\":33213,\"start\":33204},{\"end\":33222,\"start\":33213},{\"end\":33234,\"start\":33222},{\"end\":33717,\"start\":33700},{\"end\":33730,\"start\":33717},{\"end\":33743,\"start\":33730},{\"end\":33760,\"start\":33743},{\"end\":33772,\"start\":33760},{\"end\":33784,\"start\":33772},{\"end\":34213,\"start\":34200},{\"end\":34225,\"start\":34213},{\"end\":34649,\"start\":34634},{\"end\":34665,\"start\":34649},{\"end\":34681,\"start\":34665},{\"end\":35079,\"start\":35057},{\"end\":35093,\"start\":35079},{\"end\":35110,\"start\":35093},{\"end\":35452,\"start\":35441},{\"end\":35468,\"start\":35452},{\"end\":35678,\"start\":35660},{\"end\":35697,\"start\":35678},{\"end\":35715,\"start\":35697},{\"end\":35729,\"start\":35715},{\"end\":35744,\"start\":35729},{\"end\":36006,\"start\":35993},{\"end\":36025,\"start\":36006},{\"end\":36052,\"start\":36025},{\"end\":36067,\"start\":36052},{\"end\":36080,\"start\":36067},{\"end\":36091,\"start\":36080},{\"end\":36104,\"start\":36091},{\"end\":36117,\"start\":36104},{\"end\":36463,\"start\":36454},{\"end\":36479,\"start\":36463},{\"end\":36496,\"start\":36479},{\"end\":36504,\"start\":36496},{\"end\":36818,\"start\":36793},{\"end\":36833,\"start\":36818},{\"end\":36852,\"start\":36833},{\"end\":36859,\"start\":36852},{\"end\":37294,\"start\":37282},{\"end\":37306,\"start\":37294},{\"end\":37316,\"start\":37306},{\"end\":37673,\"start\":37664},{\"end\":37689,\"start\":37673},{\"end\":37936,\"start\":37921},{\"end\":37950,\"start\":37936},{\"end\":37966,\"start\":37950},{\"end\":37982,\"start\":37966},{\"end\":38418,\"start\":38407},{\"end\":38428,\"start\":38418},{\"end\":38684,\"start\":38667},{\"end\":38998,\"start\":38981},{\"end\":39185,\"start\":39169},{\"end\":39200,\"start\":39185},{\"end\":39216,\"start\":39200},{\"end\":39616,\"start\":39602},{\"end\":39629,\"start\":39616},{\"end\":39642,\"start\":39629},{\"end\":39655,\"start\":39642},{\"end\":40096,\"start\":40077},{\"end\":40112,\"start\":40096},{\"end\":40129,\"start\":40112},{\"end\":40142,\"start\":40129},{\"end\":40164,\"start\":40142},{\"end\":40179,\"start\":40164},{\"end\":40193,\"start\":40179},{\"end\":40204,\"start\":40193},{\"end\":40219,\"start\":40204},{\"end\":40248,\"start\":40219},{\"end\":40651,\"start\":40637},{\"end\":40666,\"start\":40651},{\"end\":40678,\"start\":40666},{\"end\":40689,\"start\":40678},{\"end\":40702,\"start\":40689},{\"end\":40715,\"start\":40702},{\"end\":41199,\"start\":41188},{\"end\":41216,\"start\":41199},{\"end\":41226,\"start\":41216},{\"end\":41235,\"start\":41226},{\"end\":41245,\"start\":41235},{\"end\":41687,\"start\":41669},{\"end\":41702,\"start\":41687},{\"end\":41719,\"start\":41702},{\"end\":41736,\"start\":41719},{\"end\":41752,\"start\":41736},{\"end\":42038,\"start\":42025},{\"end\":42059,\"start\":42038},{\"end\":42074,\"start\":42059},{\"end\":42090,\"start\":42074},{\"end\":42544,\"start\":42531},{\"end\":42559,\"start\":42544},{\"end\":42571,\"start\":42559},{\"end\":42584,\"start\":42571},{\"end\":42869,\"start\":42855},{\"end\":42885,\"start\":42869},{\"end\":42902,\"start\":42885},{\"end\":43317,\"start\":43303},{\"end\":43328,\"start\":43317},{\"end\":43561,\"start\":43549},{\"end\":43576,\"start\":43561},{\"end\":43590,\"start\":43576},{\"end\":43600,\"start\":43590},{\"end\":43966,\"start\":43951},{\"end\":43979,\"start\":43966},{\"end\":43993,\"start\":43979},{\"end\":44008,\"start\":43993},{\"end\":44019,\"start\":44008},{\"end\":44034,\"start\":44019},{\"end\":44347,\"start\":44327},{\"end\":44364,\"start\":44347},{\"end\":44373,\"start\":44364},{\"end\":44702,\"start\":44686},{\"end\":44712,\"start\":44702},{\"end\":44732,\"start\":44712},{\"end\":44746,\"start\":44732},{\"end\":44754,\"start\":44746},{\"end\":45060,\"start\":45044},{\"end\":45553,\"start\":45536},{\"end\":45565,\"start\":45553},{\"end\":45844,\"start\":45829},{\"end\":45859,\"start\":45844},{\"end\":45876,\"start\":45859},{\"end\":45890,\"start\":45876},{\"end\":45908,\"start\":45890},{\"end\":45926,\"start\":45908},{\"end\":45938,\"start\":45926},{\"end\":45951,\"start\":45938},{\"end\":45966,\"start\":45951},{\"end\":46443,\"start\":46423},{\"end\":46654,\"start\":46636},{\"end\":46673,\"start\":46654},{\"end\":46691,\"start\":46673},{\"end\":46705,\"start\":46691},{\"end\":46720,\"start\":46705},{\"end\":46729,\"start\":46720}]", "bib_venue": "[{\"end\":32102,\"start\":32085},{\"end\":32336,\"start\":32199},{\"end\":32784,\"start\":32716},{\"end\":33321,\"start\":33234},{\"end\":33843,\"start\":33784},{\"end\":34292,\"start\":34225},{\"end\":34740,\"start\":34681},{\"end\":35172,\"start\":35110},{\"end\":35472,\"start\":35468},{\"end\":35748,\"start\":35744},{\"end\":36167,\"start\":36117},{\"end\":36553,\"start\":36504},{\"end\":36918,\"start\":36859},{\"end\":37375,\"start\":37316},{\"end\":37662,\"start\":37612},{\"end\":38041,\"start\":37982},{\"end\":38453,\"start\":38428},{\"end\":38733,\"start\":38684},{\"end\":39026,\"start\":38998},{\"end\":39283,\"start\":39216},{\"end\":39714,\"start\":39655},{\"end\":40295,\"start\":40248},{\"end\":40793,\"start\":40715},{\"end\":41313,\"start\":41245},{\"end\":41667,\"start\":41600},{\"end\":42167,\"start\":42090},{\"end\":42529,\"start\":42453},{\"end\":42971,\"start\":42902},{\"end\":43301,\"start\":43225},{\"end\":43659,\"start\":43600},{\"end\":43949,\"start\":43889},{\"end\":44420,\"start\":44373},{\"end\":44771,\"start\":44754},{\"end\":45146,\"start\":45060},{\"end\":45582,\"start\":45565},{\"end\":46025,\"start\":45966},{\"end\":46460,\"start\":46443},{\"end\":46847,\"start\":46745},{\"end\":32839,\"start\":32786},{\"end\":34346,\"start\":34294},{\"end\":34795,\"start\":34742},{\"end\":36973,\"start\":36920},{\"end\":37430,\"start\":37377},{\"end\":38096,\"start\":38043},{\"end\":39337,\"start\":39285},{\"end\":39769,\"start\":39716},{\"end\":40858,\"start\":40795},{\"end\":42231,\"start\":42169},{\"end\":43027,\"start\":42973},{\"end\":43714,\"start\":43661},{\"end\":45235,\"start\":45148},{\"end\":46080,\"start\":46027}]"}}}, "year": 2023, "month": 12, "day": 17}