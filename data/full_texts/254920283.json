{"id": 254920283, "updated": "2023-12-06 14:27:45.239", "metadata": {"title": "RelHD: A Graph-based Learning on FeFET with Hyperdimensional Computing", "authors": "[{\"first\":\"Jaeyoung\",\"last\":\"Kang\",\"middle\":[]},{\"first\":\"Minxuan\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Abhinav\",\"last\":\"Bhansali\",\"middle\":[]},{\"first\":\"Weihong\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Anthony\",\"last\":\"Thomas\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[]}]", "venue": "2022 IEEE 40th International Conference on Computer Design (ICCD)", "journal": "2022 IEEE 40th International Conference on Computer Design (ICCD)", "publication_date": {"year": 2022, "month": 10, "day": 1}, "abstract": "Advances in graph neural network (GNN)-based algorithms enable machine learning on relational data. GNNs are computationally demanding since they rely upon backpropagation over the graph data that has sparse and irregular characteristics. In this paper, we propose a lightweight graph-based machine learning framework based on hyperdimensional computing (HDC) called RelHD. It maps the features of each node into a high-dimensional space and embeds relationships between nodes. Using lightweight HDC operations, RelHD enables both training and inference on graph data without backpropagation. Furthermore, we design a scalable processing in-memory (PIM) architecture based on the emerging FeFET technology to accelerate the proposed algorithm. Our strategy optimizes data allocation and operation scheduling that maximizes the accelerator performance by addressing the sparseness and irregularity of the graph. Experimental results show that RelHD offers comparable accuracy to the popular GNN-based algorithms while being up to 32\u00d7 faster on GPU. Also, our FeFET-based accelerator achieves 33\u00d7 of speedup and 59287\u00d7 energy efficiency improvement on average over the GPU. It is 10\u00d7 faster and 986\u00d7 more energy efficient on average compared to the state-of-the-art in-memory processing-based GNN accelerator.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccd/0001ZBXTR22", "doi": "10.1109/iccd56317.2022.00087"}}, "content": {"source": {"pdf_hash": "9f0cba90301f494ae094432b09ba7fd09b5d07df", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "66f4cec99fdb744424fda6611e7b45cc23b0b7d8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9f0cba90301f494ae094432b09ba7fd09b5d07df.txt", "contents": "\nRelHD: A Graph-based Learning on FeFET with Hyperdimensional Computing\n\n\nJaeyoung Kang j5kang@ucsd.edu \nUniversity of California San Diego\nUSA\n\nMinxuan Zhou \nUniversity of California San Diego\nUSA\n\nAbhinav Bhansali abhansali@ucsd.edu \nUniversity of California San Diego\nUSA\n\nWeihong Xu wexu@ucsd.edu \nUniversity of California San Diego\nUSA\n\nAnthony Thomas ahthomas@ucsd.edu \nUniversity of California San Diego\nUSA\n\nTajana Rosing tajana@ucsd.edu \nUniversity of California San Diego\nUSA\n\nRelHD: A Graph-based Learning on FeFET with Hyperdimensional Computing\n10.1109/ICCD56317.2022.00087Index Terms-Hyperdimensional ComputingGraph-based Machine LearningProcessing-in-memoryFeFET\nAdvances in graph neural network (GNN)-based algorithms enable machine learning on relational data. GNNs are computationally demanding since they rely upon backpropagation over the graph data that has sparse and irregular characteristics. In this paper, we propose a lightweight graph-based machine learning framework based on hyperdimensional computing (HDC) called RelHD. It maps the features of each node into a high-dimensional space and embeds relationships between nodes. Using lightweight HDC operations, RelHD enables both training and inference on graph data without backpropagation. Furthermore, we design a scalable processing in-memory (PIM) architecture based on the emerging FeFET technology to accelerate the proposed algorithm. Our strategy optimizes data allocation and operation scheduling that maximizes the accelerator performance by addressing the sparseness and irregularity of the graph. Experimental results show that RelHD offers comparable accuracy to the popular GNN-based algorithms while being up to 32\u00d7 faster on GPU. Also, our FeFET-based accelerator achieves 33\u00d7 of speedup and 59287\u00d7 energy efficiency improvement on average over the GPU. It is 10\u00d7 faster and 986\u00d7 more energy efficient on average compared to the state-of-the-art in-memory processing-based GNN accelerator.\n\nI. INTRODUCTION\n\nBeyond deep neural network (DNN)-based algorithms that have shown high efficacy in machine learning (ML) tasks, graph neural networks (GNNs) have emerged as they yield state-of-the-art performance in graph-based ML. By analyzing the underlying relationships between data, GNN has demonstrated its efficacy on tasks, e.g., node classification, graph classification, and link prediction. Various areas rely on GNNs, such as analysis of social networks, protein interactions, disease classification, and many others. However, training GNNs rely on backpropagation over the irregular and sparse graph data structure, which have high computational complexity.\n\nBrain-inspired hyperdimensional computing (HDC), in contrast, is very efficient and lightweight. HDC is based on Sparse Distributed Memory [1], which mathematically models human cognition with a high-dimensional (HD) vector. There have been many applications of HDC, such as language recognition [2], activity identification, speech recognition [3], recommendation system [4] and others. They showed comparable performance to existing ML algorithms without computeintensive backpropagation.\n\nHDC models data with HD vectors called hypervectors (HVs) [5] and imitates brain reasoning by applying arithmetic operations on HVs. Previous works have tackled graph-related workloads using the HDC principle. In [6], the authors represent the edge in HD space by binding the HVs assigned to the source and destination nodes. [7] exploited a method based on holographic reduced representation to map nodes into HD space but relied on a neural network during the inference. Recent work [8] exploits the memorization ability of HDC and encodes the graph structure into HV. It enables information retrieval and graph reconstruction based on memorized information. The work in [9] encodes a graph into an HV and performs graph classification. Unfortunately, no existing HDCbased solutions consider the node's structural role in a graph when addressing graph-based ML tasks. Also, they lack highefficiency hardware for HDC-based ML on the graph domain.\n\nIn this work, we propose RelHD, which is capable of efficiently learning on graph data by leveraging the strengths of HDC. RelHD encodes the information of the neighboring nodes and trains a model with a lightweight HDC operations. Hence, the proposed algorithm is easily parallelizable, making it much faster compared to existing graph-based ML algorithms, e.g., Node2Vec [10], GCN [11], and GAT [12], on GPU. Furthermore, we devise a ferroelectric field-effect transistor (FeFET)-based hardware accelerator tailored to the proposed algorithm because running HDC on processing inmemory (PIM) has been shown to be much faster and more energy-efficient than on GPUs [13]- [15]. In contrast to previous FeFET-based HDC accelerators which limit HDC operation on a single FeFET memory block, RelHD accelerator is scalable with customized hardware components for efficient inter-block computation and communication. Our solution enables graph-based ML with scalable FeFET memory blocks and optimizes data allocation and processing flow to improve the efficiency of irregular graph-based algorithms. Experimental results on GPU show that RelHD offers comparable accuracy to existing solutions but is at most 32\u00d7 faster than popular GCN-based algorithms. The proposed accelerator further improves speed and energy efficiency by 33\u00d7 and 59287\u00d7, respectively, over RelHD on GPU. Also, RelHD PIM accelerator yields 2.5 \u00d7 \u221214.7\u00d7 speedup and 127 \u00d7 \u221210282\u00d7 improved energy efficiency over the state-of-the-art PIM-based GNN accelerator, PIM-GCN [16].\n\nII. BACKGROUND AND RELATED WORK\n\n\nA. HDC Preliminaries\n\nInspired by the fact that brain events involve the simultaneous activity of a massive number of neurons, HDC rep- resents data with HD vectors called hypervector (HV) [5]. Reasoning is done by measuring the similarity between a pair of HVs, H 1 and H 2 , i.e., \u03b4(H 1 , H 2 ). Existing works use different similarity metrics based on the precision of HVs, e.g., Hamming distance for binary HVs, multi-bit content addressable memory (MCAM) distance for multi-bit HV, and cosine similarity for fixed/floating-point HVs. Memorizing, or bundling, is done with the element-wise vector addition. It preserves similarities of the combined HVs. Binding operation integrates new information into an HV using element-wise multiplication, which produces a nearly orthogonal HV to the original HVs. HDC performs permutation, denoted as \u03c1 l (H), to process sequential information. It shuffles the components of HV H with l-bit(s) rotations. The result of the operation is nearly orthogonal: \u03b4(\u03c1 l (H, H)) 0 for non-zero n and reversible, i.e., \u03c1 \u2212l (\u03c1 l (H)) = H.\n\n\nB. HDC-based ML on Graph Domain\n\nSeveral recent studies attempted to represent graph data in HD space and solve graph-based ML tasks. GrapHD [8] gathers node and neighbor information into a single HV. The encoded result can be used to reconstruct the original graph. It helps several cognitive tasks, such as object detection, by pipelining with a convolutional neural network (CNN)-based algorithm. Furthermore, they proposed a PIM-based hardware accelerator to expedite the processing. However, it is incapable of performing both on-device training and inference on data represented by a graph. Also, it only supports HDC operations on binary HVs, which can lead to data loss.\n\nThe work in [9] encodes a graph into an HV by bundling edge HVs, where edge HVs are the result of the binding of node HVs. It natively supports graph classification using the HDC principle, but it neglects the node's structural role and is only implemented on GPU, even though HDC provides superior speed and energy efficiency on PIM-based hardware platforms [13]- [15] In contrast, RelHD embeds the relationship between nodes and pipelines to an HDC-based training algorithm, thereby considering the structural role of each node and enabling on-device graph-based ML. Furthermore, we show a PIM-based solution optimized for RelHD algorithm, which offers high energy efficiency and speed over existing PIM solutions for GNN-based algorithm.\n\n\nC. In-memory Computing with FeFET\n\nA FeFET has emerged as a strong solution for in-memory computations, as it is compatible with CMOS, has low write energy, high I on /I of f ratio, and low latency [17], [18]. In FeFET memory, the polarization of the Fe layer determines the threshold voltage of the FeFET device. Under the positive gate pulse, the polarization direction is towards the channel, and this results in a low-V th state corresponding to a logic 0 Previous works have demonstrated the use of FeFET memory array to perform in-memory logic operations [19], [20] as a ternary CAM [18] or an analog CAM [21]. HDC encoding and MCAM-based nearest-neighbor search are accelerated in FeFET by [22], and both operations require supplementary analog circuitry to perform multi-bit operations. Figure 2 shows the organization of two types of FeFET blocks and the supported operations.\n\nFor in-memory computation (see Figure 2(a)), we can obtain the analog weighted sums between the input signal I and the weights stored in FeFET cells (W ) by sensing the output current of the bit-line (O) as demonstrated in [20]. The results are converted into the digital domain via ADCs, which are connected to a shift-and-add circuit to support high bitprecision (i.e., larger than 3 in this work). These operations have demonstrated that FeFET memory arrays can be used to overcome the data transfer bottleneck, which significantly affects the runtime and total power. Figure 2(b) shows the structure of FeFET block for nearest-neighbor search using multi-bit CAM FeFET cells (FCAM cells). Specifically, we can encode the query as the input to FCAM bit-lines and search the most similar rows based on FeFET CAM distance metric [23]. These operations have demonstrated that FeFET memory arrays can be used to overcome the data transfer bottleneck, which has been shown to take a significant portion of the execution time and total power. We can exploit these operations to support all HDC operations.\n\n\nIII. RELHD ALGORITHM OVERVIEW\n\nRelHD is a lightweight HDC-based learning framework for graphs. In this paper, we focus on the node classification problem, which is a key task of graph-based ML. Figure 1 describes the node classification problem in graph. Node 0 and 3 are known as Class 0, Node 1 and 5 are Class 1, and other nodes are unlabeled. The goal of node classification is to predict the class of unlabeled nodes given the graph topology, labeled nodes, and node features. It is essential to realize the structural role of the node in a graph.  To this end, RelHD encodes graph nodes by bundling not only the node features but also the multi-hop relations between different nodes. Using binding and bundling HDC operations on graph nodes and their neighbors, the proposed algorithm provides good reasoning capability on graph data. First, RelHD represents the feature of each node in the encoding phase as an HD vector called node HV ( Figure 3(a)). In turn, in the relation embedding phase, information of 1-hop and 2hop neighbors ( Figure 3(b), (c)) are collected, and these values and node HV are bound to relation HV. The training phase generates HV representing each class by bundling relation HVs with the same label. Finally, the inference finds the most similar class HV with the relation HV of the unlabeled node.\n\n\nA. Encoding Node Features\n\nThis section discusses the encoding scheme that maps graph node information to the HD space. It encodes features of each node to the node hypervector, N. For example, in Cora dataset [24], which predicts the category of a paper given the citation network, a paper is represented as a node, and an edge between nodes indicates the citation link. Each node has features that are described by a bag-of-words (BoW) model-a binary vector indicating the presence of words. The feature vector of node k, i.e., F k , is\n{f 0 , f 1 , \u00b7 \u00b7 \u00b7 , f F \u22121 } = {0, 1} F where F is the number of features and k \u2208 V .\nWe propose a new encoding strategy for a dataset with a BoW model, which is inspired by the random indexing methodology [25], [26]. Figure 3(a) shows our node feature encoding method for dataset with a BoW model. For each feature, we generate orthogonal HVs P = {+1, \u22121} D and assign each of them to distinguish the feature positions. Here, we use HDC permutation (cyclic shift). We randomly generate one base HV P base = {+1, \u22121} D . For feature index i, \u03c1 i (P) is assigned. We build a node HV based on feature vector and P by accumulating P corresponding to feature index only when feature value f exists (blue-colored box in Figure 3(a)). In other words, the node HV of node k is computed by\nN k = \u03a3 j\u2208A P j where A = {x | f x = 0, f x \u2208 F k }.\nIn the case of graph dataset with dense features, e.g., Reddit [27], we can create node HV from node features using existing HDC encoding methods such as non-linear encoding [28] and ID-Level encoding [3], [15]. In RelHD, we use ID-Level encoding since the proposed FeFET accelerator (Section IV) works on fixed-point numbers.\n\n\nB. Relation Embedding\n\nGiven the node representation in HD space, RelHD learns the graph structure by gathering information of nearby nodes in the relation embedding stage. In particular, we create relation hypervector for each node, which embeds the (1) node hypervector, (2) 1-hop neighbor information, and (3) 2-hop neighbor information. As illustrated in Figure 3(b), relation embedding stage collects information regarding 1-hop and 2-hop neighbor nodes for each node. We achieve this with an HV bundling operation. Let the HV representation of 1hop neighbors of node k is H 1 k . We aggregate node HVs of neighbors, i.e., H 1 k = \u03a3 j\u2208B N j . B is a set of the index of node k's 1-hop neighbors. It can be easily obtained from the edge set E, which has the origin and the destination of edges.\n\nIn a similar manner, 2-hop neighbor node information can be aggregated as shown in Figure 3(c). Backtracking to search for a 2-hop neighbor can be computationally demanding. Instead, we reuse H 1 and set B to build HV representation of 2-hop neighbors, H 2 . Specifically, for node k, H 2 k can be computed by \u03a3 j\u2208B H 1 j . Here we reuse B from the 1-hop neighbor HV (H 1 ) computation step.\n\nThe encoding and the relation embedding stages build three properties for each node: N, H 1 , and H 2 . RelHD treats each of them as characteristics of a node. Using the basic operations of HDC, we can bind these to relation HV, I. For node k, I k is computed by\nI = N \u03c6 0 + H 1 \u03c6 1 + H 2 \u03c6 2 (1) where\nindicates the element-wise multiplication (binding operation). Note that \u03c6 0 , \u03c6 1 , and \u03c6 2 are bipolar, orthogonal HV with D dimensionality, {+1, \u22121} D .\n\n\nC. Training and Inference\n\nDuring the training stage, we generate an HDC model that consists of class hypervectors C, which represents each class using HVs. We bundle all relation HVs (I) of the same class, i.e., ith class HV C i can be obtained with C i = j\u2208L I j where L is a set of the index of node that belongs to class i. Here, the required bit for class HV increases dramatically since the number of nodes is much greater than the number of classes. We quantize the HDC model to reduce the memory requirements and increase inference efficiency. As such, we quantize the model by only taking sign bits, which have been used in previous HDC-based algorithms [3], [28]. Our experimental results showed that quantizing the model to a single bit hardly degrades the accuracy (less than 0.01%).\n\nIn some cases, the accuracy of the HDC model can be improved by retraining. Since the label of each node in the training set is known, RelHD compares the predicted class to the ground truth based on the current HDC model. If the prediction is incorrect, the class HV is updated by subtracting it from the incorrect class and adding it to the correct class. Note that retraining can cause overfitting, especially if the training set is smaller than the test set.\n\nThe inference stage predicts the label of an unlabeled node; it finds the most similar class HV to the test data. As such, \n\n\nIV. FEFET-BASED PIM ACCELERATION FOR RELHD\n\nWe exploit the emerging FeFET technology to design an ultra-efficient PIM accelerator for RelHD. The proposed accelerator consists of multi-bit FeFET memory blocks that support efficient analog-based arithmetic computations and nearest neighbor search. Previous FeFET accelerators can efficiently process HDC operations [22], but they can only encode limited feature vectors fitting in a single FeFET block. However, in the RelHD algorithm, we need to generate HVs for each node during the encoding and relation embedding stage. The number of HVs may easily exceed the number of rows available in a FeFET block for most real-world graphs. Furthermore, the randomness of a graph may lead to unbalanced utilization of hardware resources and poor parallelism because RelHD introduces data dependency between different graph nodes in the relation embedding phases.\n\nWe propose a hardware-software co-design optimized for RelHD. We first propose a new FeFET-based PIM architecture.\n\nThe key novelty of RelHD hardware is the cluster-based architecture, which groups FeFET blocks into clusters where each cluster adopts near-memory logic to handle inter-block operations. With the near-memory logic, the PIM accelerator can exploit the high throughput of PIM for inter-block operations (i.e., HV summation). The cluster-based architecture is flexible to support various-sized graph data by carefully allocating memory resources in each step of RelHD. However, the memory allocation problem has a design space where a straightforward and naive strategy may result in inefficient performance. To efficiently address the memory allocation problem, we propose a graph-aware memory allocation algorithm that minimizes conflicts between different graph nodes and edges, thereby leading to high parallelism. A. Cluster-based PIM Architecture Figure 4 shows the proposed FeFET-based PIM architecture for RelHD. The FeFET memory consists of two types of Fe-FET blocks -compute memory blocks and content addressable memory (CAM) blocks, to support HDC operations. Here, we assume both types of blocks store up to 3-bit data in each memory cell. The compute blocks handle all arithmetic operations in the relation embedding and encoding, while the CAM blocks support a similarity check during the inference. On the top of the block-level, we add a cluster-level where we group a fixed amount of blocks into a cluster. We denote the cluster of the compute block as compute cluster and the cluster of the CAM block as the search cluster. 1) Compute Cluster: As introduced in Section II-C, each FeFET compute block supports vector-matrix multiplications between the input signal and the cell values based on Kirchhoff's law. We can exploit vector-matrix multiplications in each block to process the summation of multiple row vectors in a block, which can be utilized to support the basic operation in encoding/relation embedding steps of RelHD (i.e., sum up relevant HVs). However, in RelHD, the summation of relevant HVs depends on the graph structure, which is often random. If a memory block cannot store all vectors, vectors for each summation may be located in different memory blocks. In this case, we need to write partial sums of different blocks back to the memory to generate the global sum. Therefore, previous FeFET HDC accelerators [22] cannot scale to support RelHD because both data movements and memory writes are costly.\n\nIn the proposed PIM architecture, we exploit the clusterlevel processing to handle inter-block HV summations. The key components of per-cluster logic include a HV adder (HVA) and an SRAM-based scratchpad. The HVA has a vector adder that sums HVs from different blocks. The HVA is connected with the data link from blocks in the cluster as well as the intercluster interconnect. Considering the vector adder supporting the full length of the HV (e.g., D = 4096) can be too costly, we adopt a relatively short vector adder and sums up all HVs in multiple iterations. Each HVA has 64 8-bit adders which can accumulate 64 elements at a time. If the input of HVA comes from a block in the cluster, HVA keeps reading 64 8-bit values from the latch of shift-add results. The partial sum of each vector block is stored in the SRAM-based buffer. With the cluster-level summation, compute blocks can only handle efficient vector-matrix multiplications, while the HVAs can process inter-block HV summation and data movement independently. Such architecture adds cluster-level parallelism, which improves the throughput and internal bandwidth utilization during the inter-block vector summation.\n\n2) Search Cluster: CAM blocks in our accelerator can efficiently find the most similar HV to a query HV, similar to [23]. The search blocks store trained class HVs in rows. The query HV, provided as input to the CAM cells, is compared to all the stored class HVs, and the most similar class against the input is reported. When the dimension of HVs exceeds the number of columns in a CAM block, we concatenate several blocks and aggregate currents from all blocks during the search operation. We also adopt a cluster-based organization for CAM blocks. Specifically, we group several concatenated blocks as a cluster, where the concatenated length is sufficient for the most commonly used HV dimensions (e.g., D = 10000). Since the number of classes is usually small, each cluster can store a copy of trained HVs, and we can improve the throughput of the search by using several search clusters.\n\n\n3) Interconnect Network:\n\nTo handle the data movements required for each phase of RelHD, we connect all clusters through a shared-bus-based interconnect where per-cluster controllers can read/write data from/to other clusters. Each cluster controller has links to all blocks in the cluster, which can handle intra-cluster data movements.\n\n\nB. Processing Flow\n\nWe propose a flexible processing flow using compute and CAM clusters for RelHD. During each encoding/relation phase, we allocate compute clusters for different data and operations. We first divide all clusters into input and output clusters, which store input data and output data, respectively. For each input vector or output vector, we allocate one memory row in a block or assign multiple memory blocks if the number of vectors is larger than the number of rows in a block. After allocation, we can accordingly schedule operations for allocated data in each cluster.\n\nFor example, to calculate an output 1-hop/2-hop HV by summing up all neighboring node HVs, each memory block in the input clusters first calculates the sum of selected node HVs by (1) activating corresponding rows and (2) sensing the summation with ADCs. Intra-block calculations are processed simultaneously in all blocks and finished at the same time because the latency of PIM vector-matrix multiplication is independent of the number of activated rows. Per-block partial sums are read by cluster HVAs to generate per-cluster partial sums. The output cluster collects partial sums from input clusters using the interconnect and uses its HVA to generate the 1-hop/2-hop HVs. They are stored back in the allocated memory row. During the training phase, the output clusters first calculate the class HVs and then send the class HVs to CAM clusters for inference.\n\nIn addition to the parallel computation on the partial sums for one output HV, we parallelize computations for different output HVs if all computations do not have conflict when using hardware resources. We can also increase the compute throughput by duplicating data to resolve conflicts when the memory is sufficient. Therefore, the performance of PIM acceleration depends on the graph structure, data allocation as well as operation scheduling, resulting in a design space that needs exploration. In the next section, we propose a graph-aware optimization for data allocation and operation scheduling to maximize the efficiency of PIM acceleration.\n\n\nC. Graph-aware Optimization\n\nOperation scheduling and data allocation are critical to RelHD performance since the graph-based workload has complex data and computation patterns. The key constraint is that the PIM architecture cannot simultaneously sum up input vectors for two different outputs if they share a common set of inputs. Therefore, the performance of RelHD depends on the order of operations as well as the locations of data. Naive scheduling and allocation may cause a lot of conflicts for consecutive operations, significantly slowing down the performance. As such, we propose several data-aware optimization techniques to efficiently exploit the PIM architecture,  especially effective for the relation embedding phase of RelHD that uses edge information.\n\n\n1) Maximizing parallelism in clusters:\n\nWe divide the clusters into input and output for the relation embedding phase. Input/output vectors are allocated to different rows of memory blocks in the corresponding clusters. Figure 5 shows an example of calculating 1-hop HVs (H 1 0\u22125 ) with node HVs (N 0\u22125 ). In the naive allocation, we allocate H 1 0\u22125 to the output cluster (B0 and B1), and N 0\u22125 to the input cluster 0 (B2 and B3). Calculation of each 1-hop HV requires corresponding blocks to sum up node HVs. For example, to calculate H 1 0 , B2 first sums up N 0 and N 2 . The HVA of input cluster 0 (HV A I0 ) can directly read the sum of N 0 and N 2 from B2 and send it to the output cluster. For H 1 1 , HV A I0 needs to sum up partial sums from both B2 (N 1 ) and B3 (N 3 + N 4 ). We can exploit different forms of parallelism during the relation embedding to improve the throughput because the calculation of an output vector is independent of other output vectors. As shown in the example of Figure 5(a), we can calculate H 1 0 = N 0 +N 2 and H 1 2 = N 3 +N 5 on B0 and B1 in parallel. However, we cannot simultaneously schedule H 1 0 = N 0 + N 2 and H 1 1 = N 1 +N 3 +N 4 because of conflict on B0. As in realworld graphs, where we need to allocate thousands of nodes in memory, the naive allocation may cause a lot of conflicts that significantly limit the system's performance.\n\nOne way to reduce the conflict is to schedule independent output vectors together. For example, we can schedule H 1 1 calculation after H 1 0 and H 1 2 so that the memory can fully utilize both B2 and B3 for all calculations. However, finding the best ordering is an NP-hard problem that can only be solved by exhaustively evaluating all permutations. Another way to reduce conflict and improve parallelism is data dupli- cation: different copies can calculate different output vectors independently without conflict. As shown in Figure 5, we duplicate N 0\u22125 to the input cluster 1 (B4 and B5) which can handle the computation of H 1 1 = N 1 + N 3 + N 4 . In this case, we can calculate H 1 0 , H 1 1 , and H 1 2 in parallel without conflict. However, duplication will significantly increase the complexity of the original scheduling problem. We designed a scheduler that minimizes conflicts while considering data patterns.\n\n2) Allocation Optimization: Figure 5(b) depicts the proposed algorithm for data allocation. It helps to avoid block conflicts by allocating the O output vectors with a large number of block conflicts to different input copies. The allocator runs in two steps: (1) ordering and (2) copy scheduling. In the ordering step, we sort the output vectors based on the hardware utilization. The allocator starts by generating the input block list for each output vector based on the inputoutput relationship as well as the hardware configuration. For example, in Figure 5(a), H 1 0 only has B0 in its block list, while H 1 1 has both B0 and B1. In turn, we sort the outputs based on the number of blocks, starting from the output vector with the highest number of blocks. With the sorted outputs, we can balance the block usage more easily by allocating outputs requiring large numbers of blocks to different copies.\n\nBefore the copy scheduling step, the allocator needs to determine the number of copies for input data based on the workload and hardware configurations. Since different RelHD phases run sequentially, we maximize the number of input copies under a certain capacity to fully utilize the memory. After setting up data copies, it allocates an input copy for each output in the sorted order using an architectural simulation, which updates the timing information of memory blocks based on scheduled computations. When allocating the computation of each output, the simulation evaluates the cost of using each input copy based on the current system status. The allocator chooses the allocation that results in the minimum worst-case cost (the maximum latency among all blocks). The simulator updates the status of memory blocks by simulating the computations on allocated blocks. The allocation algorithm completes after allocating the input copy for all output elements.\n\nV\n\n\n. EVALUATION A. Experimental Setup\n\nWe implement RelHD on NVIDIA Geforce GTX 1080Ti GPU with CUDA v11.3 and the proposed FeFET accelerator. We measured the GPU power consumption using nvidia-smi. Table I shows the architectural parameters for the proposed FeFET-based accelerator. We use the published FeFET device data [22], [23] (45nm technology). Neu-roSim [29] generated latency and energy estimates for different operations in FeFET blocks as well as for peripherals (e.g., ADCs, shift-and-add, etc.). We implement the customized logic (i.e., HVA) using Verilog HDL and synthesize the design  on Synopsys Design Compiler using 65nm library (scaled to 45nm). The synthesized design is placed and routed using Synopsys IC Compiler. We integrate these values with a PyTorch-based simulation infrastructure to evaluate the endto-end performance of RelHD accelerator. We used representative network dataset, Cora (CR) [24], CiteSeer (CS) [30], Pubmed (PB) [31], and Reddit (RD) [27]. Attribute information for these datasets is listed in Table II. We compare RelHD to state of the art algorithms -Node2Vec [10], GCN [11] and GAT [12]. We used the same hyperparameters as in the original publications (e.g., number of hidden layers) and sparse matrix compression in Node2Vec. RelHD uses HV dimensionality (D) of 4096. Our PIM architecture is scalable; we can increase the number of tiles to accommodate more data duplication for better parallelism. We use enough memory tiles to enable 4-duplication for each dataset. Specifically, we use 1/1/2/72 tiles for CR/CS/PB/RD respectively.\n\n\nB. Comparison to Baselines running on GPU\n\nWe implemented RelHD on GPU using optimized GPUbased HDC implementation [32], [33]. Baseline algorithms are implemented with PyTorch Geometric [34]. RelHD trains the model with vector addition, while baseline algorithms require compute-intensive operations such as a random walk or matrix multiplication. Furthermore, the proposed algorithm does not use backpropagation and achieves high accuracy with a single training epoch. RelHD reaches the maximum accuracy without retraining steps on CR, CS, and RD. In the case of PB, RelHD only two epochs are required for retraining to achieve peak accuracy. Conversely, baseline algorithms typically need at least 70 epochs to get the best accuracy. Note that GCN and GAT showed out of memory on GPU, which is consistent with the result in existing works [35], [36].\n\nFirst, we compare the training time. For a fair comparison, we include the execution time of encoding, relation embedding, training, and retraining phase in RelHD since baseline algorithms include feature extraction during the training step. Figure 6(a) compares the training time of RelHD and baselines. RelHD yields an average speedup of 1087\u00d7, 13\u00d7, and 82\u00d7 compared to Node2Vec [10], GCN [11], and GAT [12], respectively. In the case of RD, the speedup of RelHD compared to baseline is minimal compared to other datasets; it is not directly proportional to the graph size. Datasets other than RD have a sparse feature vector, while RD has a dense feature vector. The RD case accompanies more arithmetic operations for the encoding stage than other datasets, i.e., RD achieves less speedup. Besides, the training time of Node2Vec increases exponentially as the size of the graph increase (e.g., PB, RD dataset). Therefore, RelHD achieves significant speedup compared to other baseline algorithms.\n\nDuring the inference stage in RelHD, we measure pairwise similarity between relation HVs of query nodes and class HVs, as the graph structure is pre-encoded to relation HV. As shown in Figure 6(a), RelHD is 104\u00d7, 5\u00d7 and 6.7\u00d7 faster on average compared to Node2Vec [10], GCN [11], and GAT [12], respectively. RelHD achieves significant speedup for end-toend execution compared to the baselines. As illustrated in Figure 6(a), RelHD is on average 104\u00d7, 5\u00d7, and 7\u00d7 faster compared to baseline algorithms: Node2Vec [10], GCN [11], and GAT [12], respectively.\n\nWe compared the energy consumption of RelHD and baseline algorithms on GPU. RelHD improves total energy efficiency by 1007\u00d7, 7\u00d7, and 57\u00d7 on average over Node2Vec [10], GCN [11], and GAT [12], respectively, as shown in Figure 6(b). Here, RelHD inference showed speedup and energy efficiency improvement, 19\u00d7 and 10\u00d7 on average, respectively. Improvement in inference was less than training since RelHD requires similarity computation on large HVs.\n\n\nC. FeFET-based RelHD Accelerator Efficiency\n\nWe compared the performance and efficiency of RelHD accelerator to GPU and two FeFET-based accelerators. The \"PIM baseline\" is similar to MIMHD [22], a previous FeFET-HDC accelerator which processes all computations by using FeFET analog operations. We fit MIMHD blocks in the same architecture as RelHD accelerator, except that the \"PIM baseline\" does not contain HVAs. \"PIM baseline\" exploits the FeFET blocks of the output vectors for summing up interblock input vectors, resulting in many writes. It also does not use our scheduling optimizations. The \"RelHD-noopt\" uses the same hardware as the proposed RelHD accelerator, but does not use the scheduling optimizations. The \"RelHD-opt\" is our final PIM design, which applies our optimization schemes on the \"RelHD-noopt\".\n\nOur evaluation shows that \"RelHD-opt\" achieves 33\u00d7 of speedup and 59287\u00d7 energy efficiency improvement on average over the GPU (see Figure 7). RD shows limited speedup and energy improvement compared to other datasets as the graph in RD is dense. Compared to \"PIM baseline\", RelHD is 3.5\u00d7 faster and 1.2\u00d7 more energy efficient. As compared  with \"RelHD-noopt\" systems, our optimized allocation and scheduling strategy improves the relation embedding phase speed by leveraging the data relationship information in graphs. It maximizes parallelism, resulting in 1.3\u00d7 speedup on average. Despite the reduced runtime, the difference of energy consumption between \"RelHD-opt\" and \"RelHD-noopt\" is small because our optimizations only change the order and the memory locations of the computations.\n\n\nD. Comparison to the state-of-the-art GNN accelerator\n\nWe compared our accelerator to a state-of-the-art GNN inference accelerator, PIM-GCN [16], which is the fastest GCN accelerator based on emerging crossbar memory. For the inference, RelHD accelerator uses pre-encoded relation HVs, which was done during the model training. Our evaluation shows that RelHD accelerator takes 3.8 \u00d7 10 \u22126 seconds and consumes 2.2\u00d710 \u22126 J for CR, CS, and PB workloads because a 64-row block can store all classes for all workloads, leading to a constant search time for each node. In the case of RD, our accelerator runs in 2.1 \u00d7 10 \u22124 seconds and uses 1.2 \u00d7 10 \u22124 J of energy. As a comparison, PIM-GCN takes 5.6 \u00d7 10 \u22125 , 9.5 \u00d7 10 \u22126 , 3.3 \u00d7 10 \u22125 , and 7.6 \u00d7 10 \u22123 seconds, while consuming 2.8 \u00d7 10 \u22124 , 9.2 \u00d7 10 \u22124 , 3.8 \u00d7 10 \u22123 , and 1.26 J on CR, CS, PB, and RD dataset, respectively. The results show that RelHD accelerator is 2.5\u00d7 to 14.7\u00d7 faster than PIM-GCN, while improving the energy efficiency by 127\u00d7 to 10282\u00d7. RelHD PIM accelerator achieves much better energy efficiency because of the MCAM search operations. Also, note that RelHD accelerator showed the best speedup and energy efficiency improvement in large-sized graph, RD.\n\n\nE. RelHD Scalability\n\nSimilar to the GCN and GAT, large-size graphs can be challenging to run. RelHD accelerator can offer a high degree of parallelism and scalable memory bandwidth by increasing the number of compute and CAM clusters. Also, unlike existing FeFET-based HDC accelerators [22], the proposed hardware supports massive HDC operations in a scalable manner. Figure 9 shows the speedup according to the number of compute and CAM clusters. Because different workloads have different memory requirements (from hundreds of Mbs to tens of Gbs), we scale the memory capacity from the minimum requirement (i.e., 1-scale) to 8\u00d7 of the minimum requirement for each workload. Our results show that increasing the memory by 8\u00d7 can provide 6\u00d7, 5\u00d7, 6\u00d7, and 2\u00d7 speedup for CR, CS, PB, and RD respectively. It implies that the RelHD accelerator scales well for different datasets. RD provides less speedup than other datasets because data is large and dense, introducing more data movement overhead when using more memory.\n\n\nF. Overhead Analysis\n\nHVA and 8KB SRAM-based scratchpad takes 0.057mm 2 chip area and consume 27.34mW power in each cluster. The area and power overhead of the proposed hardware are only 6.14% and 2.13% respectively. The most area and power consuming components are FeFET blocks (0.42mm 2 / 153.6mW , ADC (0.31mm 2 / 512.0mW ), DAC (0.022mm 2 / 512.0mW ), and shift-and-add (0.12mm 2 / 102.4mW ).\n\n\nG. RelHD Accuracy & Dimensionality\n\nHV dimensionality D is critical since it affects accuracy but at the cost of computational complexity and memory capacity. Figure 8(a) shows accuracy changes of RelHD as a function of D. When D \u2265 4096, the accuracy of RelHD converges. We conclude that at least 4096 dimensionality is needed to encode the node features and the relation between the nodes into HVs. Figure 8(b) depicts the achieved accuracy of RelHD and the baselines. RelHD yields better accuracy than Node2Vec while showing a slight accuracy drop (0.04 on average) compared to GNN-based solutions. For RD, the node labels are highly dependent on graph topology information. Node2Vec utilizes this information well, thereby showing high accuracy [37].\n\n\nVI. CONCLUSION\n\nWe presented RelHD, a lightweight graph-based ML framework based on HDC. It embeds neighbor information and node features in HD space and predicts the label of an unlabeled node with simple HDC operations. We designed a scalable FeFET-based PIM accelerator for RelHD, with optimized data allocation and scheduling to maximize the parallelism on an irregular graph. Evaluation results show that RelHD is up to 32\u00d7 faster compared to GNN-based algorithm on GPU while offering comparable accuracy. Furthermore, the proposed PIM accelerator significantly enhances speed and energy efficiency by 33\u00d7 and 59287\u00d7 on average, respectively, over GPU. RelHD accelerator yields on average 10\u00d7 speedup and 986\u00d7 energy efficiency improvement compared to the state-of-theart crossbar memory-based GNN accelerator.\n\n\nFig. 1: Overview of node classification problem\n\nFig. 3 :\n3Overview of encoding and relation embedding stage in RelHD. (a) Node feature encoding (b) Aggregation of 1hop neighbor information (c) Aggregation of 2-hop neighbor information. Green-colored HVs are used for relation HV generation (Eq. 1).\n\nFig. 5 :\n5Conflict issue and the proposed optimization scheme.\n\nFig. 6 :\n6Improvement of RelHD over baselines on GPU. For RD, GCN and GAT on GPU ran out of memory (OOM).\n\nFig. 7 :\n7Performance of RelHD PIM accelerator vs. RelHD GPU implementation\n\nFig. 8 :\n8Accuracy comparison. (a) RelHD accuracy according to HV dimensionality (D) (b) RelHD vs. baselines. OOM denotes out of memory.\n\nFig. 9 :\n9The scalability of RelHD.\n\n\nFig. 4: Architecture of RelHD PIM accelerator.we first build relation HV I of test nodes (queries) by reusing N, H 1 , and H 2 . For each query (I), the inference module calculates the similarity to the class HVs and selects a class with the highest similarity, i.e., arg max i \u03b4(C i , I).Block \n\nBlock \n\nBlock \nBlock \n\n\u2026 \n\u2026 \n\nHV \nAdder \n\n\u2026 \n\nBlock \n\nBlock \n\n\u2026 \n\nInterconnect \n\nHV \nAdder \n\nHV \nAdder \n\nInput Clusters \nOutput Clusters \n\nDifferent entries \n\nDifferent dimensionalities \n\nBlock \n\nBlock \n\n\u2026 \n\nHV \nAdder \n\n\u2026 \n\nDifferent dimensionalities \n\nSearch \nBlock \n\nSearch \nBlock \n\nSearch \nBlock \n\n\u2026 \n\nSearch Clusters \n\nCluster \n\nBlock row buffer / \nInterconnect \n\nSRAM Buffer \n\n+ \n\n8-bit \n\n\u2026 \n\u00d7 64 \n\n+ \n+ \n\nInterconnect \n\nSRAM Cntl \n\n\n\nTABLE I :\nIArchitecture ParametersBlock \n64* 64 3-bit cells, 8 columns/ADC \nCluster \n2048 blocks, 64 8-bit adders, 8KB SRAM buffer \nTile (1Gb) \n128 clusters, interconnect bandwidth = 8GB/s \nFeFET compute tAdd/tRD = 2.65ns, eAdd = 48.06pJ, tW R = 11ns, eW R = 104.18pJ \nFeFET CAM \ntSearch = 1.5ns, eSearch = 0.069fJ \n\n\n\nTABLE II :\nIIStatistics of the datasetsDataset \nNodes \nEdges \nFeatures Classes \n\nCora (CR) \n2708 \n5429 \n1433 \n7 \nCiteSeer (CS) \n3327 \n4732 \n3703 \n6 \nPubmed (PB) \n19717 \n44338 \n500 \n3 \nReddit (RD) \n232965 114615892 \n602 \n41 \n\n(a) Speedup \n(b) Energy efficiency \n\n\nACKNOWLEDGMENT This work was supported in part by CRISP, one of six centers in JUMP (an SRC program sponsored by DARPA), SRC Global Research Collaboration (GRC) grant, DARPA HyDDENN grant, and NSF grants #1911095, #2003279, #2100237, and #2120019.\nP Kanerva, Sparse distributed memory. MIT pressP. Kanerva, Sparse distributed memory. MIT press, 1988.\n\nA robust and energy-efficient classifier using braininspired hyperdimensional computing. A Rahimi, ISLPED. A. Rahimi et al., \"A robust and energy-efficient classifier using brain- inspired hyperdimensional computing,\" in ISLPED, 2016.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, ICRC. M. Imani et al., \"Voicehd: Hyperdimensional computing for efficient speech recognition,\" in ICRC, 2017.\n\nHyperrec: Efficient recommender systems with hyperdimensional computing. Y Guo, ASP-DAC. Y. Guo et al., \"Hyperrec: Efficient recommender systems with hyperdi- mensional computing,\" in ASP-DAC, 2021.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to comput- ing in distributed representation with high-dimensional random vectors,\" Cognitive Computation, vol. 1, no. 2, pp. 139-159, Jan. 2009.\n\nVector symbolic architectures as a computing framework for nanoscale hardware. D Kleyko, abs/2106.05268CoRR. D. Kleyko et al., \"Vector symbolic architectures as a computing frame- work for nanoscale hardware,\" CoRR, vol. abs/2106.05268, 2021.\n\nHolistic representations for memorization and inference. Y Ma, in UAI. Y. Ma et al., \"Holistic representations for memorization and inference.\" in UAI, 2018, pp. 403-413.\n\nGraphd: Graph-based hyperdimensional memorization for brain-like cognitive learning. P , Front. Neurosci. 16P. Poduval et al., \"Graphd: Graph-based hyperdimensional memorization for brain-like cognitive learning,\" Front. Neurosci., vol. 16, 2022.\n\nGraphhd: Efficient graph classification using hyperdimensional computing. I Nunes, DATE. I. Nunes et al., \"Graphhd: Efficient graph classification using hyperdi- mensional computing,\" in DATE, 2022.\n\nNode2vec: Scalable feature learning for networks. A Grover, KDD. A. Grover et al., \"Node2vec: Scalable feature learning for networks,\" in KDD, 2016.\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, ICLR. T. N. Kipf et al., \"Semi-supervised classification with graph convolu- tional networks,\" in ICLR, 2017.\n\nGraph attention networks. P Veli\u010dkovi\u0107, ICLR. P. Veli\u010dkovi\u0107 et al., \"Graph attention networks,\" in ICLR, 2018.\n\nBrain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study. T F Wu, in ISSCC. T. F. Wu et al., \"Brain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study,\" in ISSCC, 2018.\n\nHyperdimensional computing with 3d vrram inmemory kernels: Device-architecture co-design for energy-efficient, error-resilient language recognition. H Li, IEDM. H. Li et al., \"Hyperdimensional computing with 3d vrram in- memory kernels: Device-architecture co-design for energy-efficient, error-resilient language recognition,\" in IEDM, 2016.\n\nFelix: Fast and energy-efficient logic in memory. S Gupta, ICCAD. S. Gupta et al., \"Felix: Fast and energy-efficient logic in memory,\" in ICCAD, 2018.\n\nCrossbar based processing in memory accelerator architecture for graph convolutional networks. N Challapalle, ICCAD. N. Challapalle et al., \"Crossbar based processing in memory accelerator architecture for graph convolutional networks,\" in ICCAD, 2021.\n\nDesign and analysis of an ultra-dense, low-leakage, and fast fefet-based random access memory array. D Reis, JXCDC. 52D. Reis et al., \"Design and analysis of an ultra-dense, low-leakage, and fast fefet-based random access memory array,\" JXCDC, vol. 5, no. 2, pp. 103-112, 2019.\n\nFerroelectric ternary content-addressable memory for oneshot learning. K Ni, Nature Electronics. 211K. Ni et al., \"Ferroelectric ternary content-addressable memory for one- shot learning,\" Nature Electronics, vol. 2, no. 11, pp. 521-529, 2019.\n\nComputing-in-memory using ferroelectrics: From single-to multi-input logic. Q Huang, IEEE Design & Test. Q. Huang et al., \"Computing-in-memory using ferroelectrics: From single-to multi-input logic,\" IEEE Design & Test, 2021.\n\nA ferroelectric field effect transistor based synaptic weight cell. M Jerry, Journal of Physics D: Applied Physics. 5143434001M. Jerry et al., \"A ferroelectric field effect transistor based synaptic weight cell,\" Journal of Physics D: Applied Physics, vol. 51, no. 43, p. 434001, 2018.\n\nFecam: A universal compact digital and analog content addressable memory using ferroelectric. X Yin, T-ED. 677X. Yin et al., \"Fecam: A universal compact digital and analog content addressable memory using ferroelectric,\" T-ED, vol. 67, no. 7, pp. 2785- 2792, 2020.\n\nMimhd: Accurate and efficient hyperdimensional inference using multi-bit in-memory computing. A Kazemi, ISLPED. A. Kazemi et al., \"Mimhd: Accurate and efficient hyperdimensional inference using multi-bit in-memory computing,\" in ISLPED, 2021.\n\nIn-memory nearest neighbor search with fefet multibit content-addressable memories. A Kazemi, DATEA. Kazemi et al., \"In-memory nearest neighbor search with fefet multi- bit content-addressable memories,\" in DATE, 2021.\n\nAutomating the construction of internet portals with machine learning. A K Mccallum, Inf. Retr. 32A. K. McCallum et al., \"Automating the construction of internet portals with machine learning,\" Inf. Retr., vol. 3, no. 2, pp. 127-163, Jul 2000.\n\nRandom indexing of text samples for latent semantic analysis. P Kanerva, CogSci. ErlbaumP. Kanerva et al., \"Random indexing of text samples for latent semantic analysis,\" in CogSci. Erlbaum, 2000.\n\nAn introduction to random indexing. M Sahlgren, Methods and Applications of Semantic Indexing Workshop at TKE. M. Sahlgren, \"An introduction to random indexing,\" in In Methods and Applications of Semantic Indexing Workshop at TKE, 2005.\n\nInductive representation learning on large graphs. W Hamilton, NeurIPS. W. Hamilton et al., \"Inductive representation learning on large graphs,\" NeurIPS, 2017.\n\nDUAL: Acceleration of clustering algorithms using digital-based processing in-memory. M Imani, MICROM. Imani et al., \"DUAL: Acceleration of clustering algorithms using digital-based processing in-memory,\" in MICRO, Oct. 2020.\n\nDnn+neurosim: An end-to-end benchmarking framework for compute-in-memory accelerators with versatile device technologies. X Peng, IEDM. X. Peng et al., \"Dnn+neurosim: An end-to-end benchmarking framework for compute-in-memory accelerators with versatile device technologies,\" in IEDM, 2019.\n\nCiteseer: An automatic citation indexing system. C L Giles, JCDL. C. L. Giles et al., \"Citeseer: An automatic citation indexing system,\" in JCDL, 1998.\n\nCollective classification in network data. P Sen, AI Magazine. 293P. Sen et al., \"Collective classification in network data,\" AI Magazine, vol. 29, no. 3, 2008.\n\nXcelhd: An efficient gpu-powered hyperdimensional computing with parallelized training. J Kang, ASP-DAC. J. Kang et al., \"Xcelhd: An efficient gpu-powered hyperdimensional computing with parallelized training,\" in ASP-DAC, 2022.\n\nOpenhd: A gpu-powered framework for hyperdimensional computing. IEEE TC. --, \"Openhd: A gpu-powered framework for hyperdimensional com- puting,\" IEEE TC, 2022.\n\nFast graph representation learning with PyTorch Geometric. M Fey, ICLR Workshop on Representation Learning on Graphs and Manifolds. M. Fey et al., \"Fast graph representation learning with PyTorch Geo- metric,\" in ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nPimgcn: A reram-based pim design for graph convolutional network acceleration. T Yang, DAC. T. Yang et al., \"Pimgcn: A reram-based pim design for graph convolu- tional network acceleration,\" in DAC, 2021.\n\nLarge graph convolutional network training with gpuoriented data communication architecture. S W Min, VLDBS. W. Min et al., \"Large graph convolutional network training with gpu- oriented data communication architecture,\" VLDB, 2021.\n\nSpatio-temporal attentive rnn for node classification in temporal attributed graphs. D Xu, IJCAI. D. Xu et al., \"Spatio-temporal attentive rnn for node classification in temporal attributed graphs.\" in IJCAI, 2019.\n", "annotations": {"author": "[{\"end\":144,\"start\":74},{\"end\":198,\"start\":145},{\"end\":275,\"start\":199},{\"end\":341,\"start\":276},{\"end\":415,\"start\":342},{\"end\":486,\"start\":416}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":83},{\"end\":157,\"start\":153},{\"end\":215,\"start\":207},{\"end\":286,\"start\":284},{\"end\":356,\"start\":350},{\"end\":429,\"start\":423}]", "author_first_name": "[{\"end\":82,\"start\":74},{\"end\":152,\"start\":145},{\"end\":206,\"start\":199},{\"end\":283,\"start\":276},{\"end\":349,\"start\":342},{\"end\":422,\"start\":416}]", "author_affiliation": "[{\"end\":143,\"start\":105},{\"end\":197,\"start\":159},{\"end\":274,\"start\":236},{\"end\":340,\"start\":302},{\"end\":414,\"start\":376},{\"end\":485,\"start\":447}]", "title": "[{\"end\":71,\"start\":1},{\"end\":557,\"start\":487}]", "venue": null, "abstract": "[{\"end\":1985,\"start\":678}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2802,\"start\":2799},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2959,\"start\":2956},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3008,\"start\":3005},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3035,\"start\":3032},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3213,\"start\":3210},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3368,\"start\":3365},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3481,\"start\":3478},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3640,\"start\":3637},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3828,\"start\":3825},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4478,\"start\":4474},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4488,\"start\":4484},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4502,\"start\":4498},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4770,\"start\":4766},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4776,\"start\":4772},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5637,\"start\":5633},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5866,\"start\":5863},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6892,\"start\":6889},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7443,\"start\":7440},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7791,\"start\":7787},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7797,\"start\":7793},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8373,\"start\":8369},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8379,\"start\":8375},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8736,\"start\":8732},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8742,\"start\":8738},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8764,\"start\":8760},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8786,\"start\":8782},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8872,\"start\":8868},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9285,\"start\":9281},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9892,\"start\":9888},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11711,\"start\":11707},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12247,\"start\":12243},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12253,\"start\":12249},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12939,\"start\":12935},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13050,\"start\":13046},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13076,\"start\":13073},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13082,\"start\":13078},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15521,\"start\":15518},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15527,\"start\":15523},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16609,\"start\":16605},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19613,\"start\":19609},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21008,\"start\":21004},{\"end\":25714,\"start\":25711},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29528,\"start\":29524},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29534,\"start\":29530},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29568,\"start\":29564},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30126,\"start\":30122},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30146,\"start\":30142},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30164,\"start\":30160},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30186,\"start\":30182},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30314,\"start\":30310},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30324,\"start\":30320},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30337,\"start\":30333},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30908,\"start\":30904},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":30914,\"start\":30910},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30979,\"start\":30975},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31634,\"start\":31630},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31640,\"start\":31636},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32028,\"start\":32024},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32038,\"start\":32034},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32052,\"start\":32048},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32911,\"start\":32907},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32921,\"start\":32917},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32935,\"start\":32931},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33158,\"start\":33154},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33168,\"start\":33164},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33182,\"start\":33178},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33365,\"start\":33361},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33375,\"start\":33371},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33389,\"start\":33385},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33841,\"start\":33837},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35409,\"start\":35405},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36785,\"start\":36781},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":38666,\"start\":38662}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39535,\"start\":39486},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39787,\"start\":39536},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39851,\"start\":39788},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39958,\"start\":39852},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40035,\"start\":39959},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40173,\"start\":40036},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40210,\"start\":40174},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40948,\"start\":40211},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41267,\"start\":40949},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41531,\"start\":41268}]", "paragraph": "[{\"end\":2658,\"start\":2004},{\"end\":3150,\"start\":2660},{\"end\":4099,\"start\":3152},{\"end\":5638,\"start\":4101},{\"end\":5671,\"start\":5640},{\"end\":6745,\"start\":5696},{\"end\":7426,\"start\":6781},{\"end\":8168,\"start\":7428},{\"end\":9056,\"start\":8206},{\"end\":10160,\"start\":9058},{\"end\":11494,\"start\":10194},{\"end\":12035,\"start\":11524},{\"end\":12818,\"start\":12123},{\"end\":13198,\"start\":12872},{\"end\":13999,\"start\":13224},{\"end\":14392,\"start\":14001},{\"end\":14656,\"start\":14394},{\"end\":14852,\"start\":14697},{\"end\":15650,\"start\":14882},{\"end\":16113,\"start\":15652},{\"end\":16238,\"start\":16115},{\"end\":17145,\"start\":16285},{\"end\":17261,\"start\":17147},{\"end\":19701,\"start\":17263},{\"end\":20886,\"start\":19703},{\"end\":21781,\"start\":20888},{\"end\":22121,\"start\":21810},{\"end\":22714,\"start\":22144},{\"end\":23578,\"start\":22716},{\"end\":24231,\"start\":23580},{\"end\":25004,\"start\":24263},{\"end\":26396,\"start\":25047},{\"end\":27322,\"start\":26398},{\"end\":28231,\"start\":27324},{\"end\":29198,\"start\":28233},{\"end\":29201,\"start\":29200},{\"end\":30786,\"start\":29240},{\"end\":31641,\"start\":30832},{\"end\":32641,\"start\":31643},{\"end\":33197,\"start\":32643},{\"end\":33645,\"start\":33199},{\"end\":34469,\"start\":33693},{\"end\":35262,\"start\":34471},{\"end\":36491,\"start\":35320},{\"end\":37512,\"start\":36516},{\"end\":37911,\"start\":37537},{\"end\":38667,\"start\":37950},{\"end\":39485,\"start\":38686}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12122,\"start\":12036},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12871,\"start\":12819},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14696,\"start\":14657}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29407,\"start\":29400},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30250,\"start\":30242}]", "section_header": "[{\"end\":2002,\"start\":1987},{\"end\":5694,\"start\":5674},{\"end\":6779,\"start\":6748},{\"end\":8204,\"start\":8171},{\"end\":10192,\"start\":10163},{\"end\":11522,\"start\":11497},{\"end\":13222,\"start\":13201},{\"end\":14880,\"start\":14855},{\"end\":16283,\"start\":16241},{\"end\":21808,\"start\":21784},{\"end\":22142,\"start\":22124},{\"end\":24261,\"start\":24234},{\"end\":25045,\"start\":25007},{\"end\":29238,\"start\":29204},{\"end\":30830,\"start\":30789},{\"end\":33691,\"start\":33648},{\"end\":35318,\"start\":35265},{\"end\":36514,\"start\":36494},{\"end\":37535,\"start\":37515},{\"end\":37948,\"start\":37914},{\"end\":38684,\"start\":38670},{\"end\":39545,\"start\":39537},{\"end\":39797,\"start\":39789},{\"end\":39861,\"start\":39853},{\"end\":39968,\"start\":39960},{\"end\":40045,\"start\":40037},{\"end\":40183,\"start\":40175},{\"end\":40959,\"start\":40950},{\"end\":41279,\"start\":41269}]", "table": "[{\"end\":40948,\"start\":40502},{\"end\":41267,\"start\":40984},{\"end\":41531,\"start\":41308}]", "figure_caption": "[{\"end\":39535,\"start\":39488},{\"end\":39787,\"start\":39547},{\"end\":39851,\"start\":39799},{\"end\":39958,\"start\":39863},{\"end\":40035,\"start\":39970},{\"end\":40173,\"start\":40047},{\"end\":40210,\"start\":40185},{\"end\":40502,\"start\":40213},{\"end\":40984,\"start\":40961},{\"end\":41308,\"start\":41282}]", "figure_ref": "[{\"end\":8974,\"start\":8966},{\"end\":9097,\"start\":9089},{\"end\":9638,\"start\":9630},{\"end\":10365,\"start\":10357},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11116,\"start\":11108},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11214,\"start\":11206},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12263,\"start\":12255},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12760,\"start\":12752},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13568,\"start\":13560},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14092,\"start\":14084},{\"end\":18121,\"start\":18113},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25235,\"start\":25227},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26016,\"start\":26008},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26936,\"start\":26928},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27360,\"start\":27352},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27886,\"start\":27878},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31893,\"start\":31885},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32836,\"start\":32828},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33063,\"start\":33055},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33425,\"start\":33417},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34612,\"start\":34603},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":36871,\"start\":36863},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38081,\"start\":38073},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38322,\"start\":38314}]", "bib_author_first_name": "[{\"end\":41781,\"start\":41780},{\"end\":41974,\"start\":41973},{\"end\":42191,\"start\":42190},{\"end\":42384,\"start\":42383},{\"end\":42636,\"start\":42635},{\"end\":42955,\"start\":42954},{\"end\":43177,\"start\":43176},{\"end\":43377,\"start\":43376},{\"end\":43614,\"start\":43613},{\"end\":43790,\"start\":43789},{\"end\":43956,\"start\":43955},{\"end\":43958,\"start\":43957},{\"end\":44103,\"start\":44102},{\"end\":44304,\"start\":44303},{\"end\":44306,\"start\":44305},{\"end\":44622,\"start\":44621},{\"end\":44867,\"start\":44866},{\"end\":45064,\"start\":45063},{\"end\":45324,\"start\":45323},{\"end\":45573,\"start\":45572},{\"end\":45823,\"start\":45822},{\"end\":46042,\"start\":46041},{\"end\":46355,\"start\":46354},{\"end\":46621,\"start\":46620},{\"end\":46855,\"start\":46854},{\"end\":47062,\"start\":47061},{\"end\":47064,\"start\":47063},{\"end\":47298,\"start\":47297},{\"end\":47470,\"start\":47469},{\"end\":47723,\"start\":47722},{\"end\":47919,\"start\":47918},{\"end\":48182,\"start\":48181},{\"end\":48401,\"start\":48400},{\"end\":48403,\"start\":48402},{\"end\":48548,\"start\":48547},{\"end\":48755,\"start\":48754},{\"end\":49117,\"start\":49116},{\"end\":49423,\"start\":49422},{\"end\":49643,\"start\":49642},{\"end\":49645,\"start\":49644},{\"end\":49869,\"start\":49868}]", "bib_author_last_name": "[{\"end\":41789,\"start\":41782},{\"end\":41981,\"start\":41975},{\"end\":42197,\"start\":42192},{\"end\":42388,\"start\":42385},{\"end\":42644,\"start\":42637},{\"end\":42962,\"start\":42956},{\"end\":43180,\"start\":43178},{\"end\":43620,\"start\":43615},{\"end\":43797,\"start\":43791},{\"end\":43963,\"start\":43959},{\"end\":44114,\"start\":44104},{\"end\":44309,\"start\":44307},{\"end\":44625,\"start\":44623},{\"end\":44873,\"start\":44868},{\"end\":45076,\"start\":45065},{\"end\":45329,\"start\":45325},{\"end\":45576,\"start\":45574},{\"end\":45829,\"start\":45824},{\"end\":46048,\"start\":46043},{\"end\":46359,\"start\":46356},{\"end\":46628,\"start\":46622},{\"end\":46862,\"start\":46856},{\"end\":47073,\"start\":47065},{\"end\":47306,\"start\":47299},{\"end\":47479,\"start\":47471},{\"end\":47732,\"start\":47724},{\"end\":47925,\"start\":47920},{\"end\":48187,\"start\":48183},{\"end\":48409,\"start\":48404},{\"end\":48552,\"start\":48549},{\"end\":48760,\"start\":48756},{\"end\":49121,\"start\":49118},{\"end\":49428,\"start\":49424},{\"end\":49649,\"start\":49646},{\"end\":49872,\"start\":49870}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":41882,\"start\":41780},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9812826},\"end\":42118,\"start\":41884},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":21351739},\"end\":42308,\"start\":42120},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":231730928},\"end\":42508,\"start\":42310},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":733980},\"end\":42873,\"start\":42510},{\"attributes\":{\"doi\":\"abs/2106.05268\",\"id\":\"b5\",\"matched_paper_id\":235390927},\"end\":43117,\"start\":42875},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":53964350},\"end\":43289,\"start\":43119},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":246531628},\"end\":43537,\"start\":43291},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":248811442},\"end\":43737,\"start\":43539},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":207238980},\"end\":43887,\"start\":43739},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3144218},\"end\":44074,\"start\":43889},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3292002},\"end\":44186,\"start\":44076},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3869844},\"end\":44470,\"start\":44188},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":25209638},\"end\":44814,\"start\":44472},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":53235957},\"end\":44966,\"start\":44816},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":245447216},\"end\":45220,\"start\":44968},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":199580215},\"end\":45499,\"start\":45222},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":209063048},\"end\":45744,\"start\":45501},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":233830056},\"end\":45971,\"start\":45746},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":117370069},\"end\":46258,\"start\":45973},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":214802295},\"end\":46524,\"start\":46260},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":235606247},\"end\":46768,\"start\":46526},{\"attributes\":{\"id\":\"b22\"},\"end\":46988,\"start\":46770},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":349242},\"end\":47233,\"start\":46990},{\"attributes\":{\"id\":\"b24\"},\"end\":47431,\"start\":47235},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":17228581},\"end\":47669,\"start\":47433},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4755450},\"end\":47830,\"start\":47671},{\"attributes\":{\"id\":\"b27\"},\"end\":48057,\"start\":47832},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":211206012},\"end\":48349,\"start\":48059},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":514080},\"end\":48502,\"start\":48351},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":62016134},\"end\":48664,\"start\":48504},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":247037290},\"end\":48894,\"start\":48666},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":249248911},\"end\":49055,\"start\":48896},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":70349949},\"end\":49341,\"start\":49057},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":243946024},\"end\":49547,\"start\":49343},{\"attributes\":{\"id\":\"b35\"},\"end\":49781,\"start\":49549},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":199466175},\"end\":49997,\"start\":49783}]", "bib_title": "[{\"end\":41971,\"start\":41884},{\"end\":42188,\"start\":42120},{\"end\":42381,\"start\":42310},{\"end\":42633,\"start\":42510},{\"end\":42952,\"start\":42875},{\"end\":43174,\"start\":43119},{\"end\":43374,\"start\":43291},{\"end\":43611,\"start\":43539},{\"end\":43787,\"start\":43739},{\"end\":43953,\"start\":43889},{\"end\":44100,\"start\":44076},{\"end\":44301,\"start\":44188},{\"end\":44619,\"start\":44472},{\"end\":44864,\"start\":44816},{\"end\":45061,\"start\":44968},{\"end\":45321,\"start\":45222},{\"end\":45570,\"start\":45501},{\"end\":45820,\"start\":45746},{\"end\":46039,\"start\":45973},{\"end\":46352,\"start\":46260},{\"end\":46618,\"start\":46526},{\"end\":47059,\"start\":46990},{\"end\":47467,\"start\":47433},{\"end\":47720,\"start\":47671},{\"end\":48179,\"start\":48059},{\"end\":48398,\"start\":48351},{\"end\":48545,\"start\":48504},{\"end\":48752,\"start\":48666},{\"end\":48958,\"start\":48896},{\"end\":49114,\"start\":49057},{\"end\":49420,\"start\":49343},{\"end\":49866,\"start\":49783}]", "bib_author": "[{\"end\":41791,\"start\":41780},{\"end\":41983,\"start\":41973},{\"end\":42199,\"start\":42190},{\"end\":42390,\"start\":42383},{\"end\":42646,\"start\":42635},{\"end\":42964,\"start\":42954},{\"end\":43182,\"start\":43176},{\"end\":43380,\"start\":43376},{\"end\":43622,\"start\":43613},{\"end\":43799,\"start\":43789},{\"end\":43965,\"start\":43955},{\"end\":44116,\"start\":44102},{\"end\":44311,\"start\":44303},{\"end\":44627,\"start\":44621},{\"end\":44875,\"start\":44866},{\"end\":45078,\"start\":45063},{\"end\":45331,\"start\":45323},{\"end\":45578,\"start\":45572},{\"end\":45831,\"start\":45822},{\"end\":46050,\"start\":46041},{\"end\":46361,\"start\":46354},{\"end\":46630,\"start\":46620},{\"end\":46864,\"start\":46854},{\"end\":47075,\"start\":47061},{\"end\":47308,\"start\":47297},{\"end\":47481,\"start\":47469},{\"end\":47734,\"start\":47722},{\"end\":47927,\"start\":47918},{\"end\":48189,\"start\":48181},{\"end\":48411,\"start\":48400},{\"end\":48554,\"start\":48547},{\"end\":48762,\"start\":48754},{\"end\":49123,\"start\":49116},{\"end\":49430,\"start\":49422},{\"end\":49651,\"start\":49642},{\"end\":49874,\"start\":49868}]", "bib_venue": "[{\"end\":41816,\"start\":41791},{\"end\":41989,\"start\":41983},{\"end\":42203,\"start\":42199},{\"end\":42397,\"start\":42390},{\"end\":42667,\"start\":42646},{\"end\":42982,\"start\":42978},{\"end\":43188,\"start\":43182},{\"end\":43395,\"start\":43380},{\"end\":43626,\"start\":43622},{\"end\":43802,\"start\":43799},{\"end\":43969,\"start\":43965},{\"end\":44120,\"start\":44116},{\"end\":44319,\"start\":44311},{\"end\":44631,\"start\":44627},{\"end\":44880,\"start\":44875},{\"end\":45083,\"start\":45078},{\"end\":45336,\"start\":45331},{\"end\":45596,\"start\":45578},{\"end\":45849,\"start\":45831},{\"end\":46087,\"start\":46050},{\"end\":46365,\"start\":46361},{\"end\":46636,\"start\":46630},{\"end\":46852,\"start\":46770},{\"end\":47084,\"start\":47075},{\"end\":47295,\"start\":47235},{\"end\":47542,\"start\":47481},{\"end\":47741,\"start\":47734},{\"end\":47916,\"start\":47832},{\"end\":48193,\"start\":48189},{\"end\":48415,\"start\":48411},{\"end\":48565,\"start\":48554},{\"end\":48769,\"start\":48762},{\"end\":48967,\"start\":48960},{\"end\":49187,\"start\":49123},{\"end\":49433,\"start\":49430},{\"end\":49640,\"start\":49549},{\"end\":49879,\"start\":49874}]"}}}, "year": 2023, "month": 12, "day": 17}