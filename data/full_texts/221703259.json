{"id": 221703259, "updated": "2023-10-06 11:06:24.627", "metadata": {"title": "An improved quantum-inspired algorithm for linear regression", "authors": "[{\"first\":\"Andr'as\",\"last\":\"Gily'en\",\"middle\":[]},{\"first\":\"Zhao\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Ewin\",\"last\":\"Tang\",\"middle\":[]}]", "venue": "Quantum 6, 754 (2022)", "journal": null, "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We give a classical algorithm for linear regression analogous to the quantum matrix inversion algorithm [Harrow, Hassidim, and Lloyd, Physical Review Letters'09, arXiv:0811.3171] for low-rank matrices [Wossnig, Zhao, and Prakash, Physical Review Letters'18, arXiv:1704.06174], when the input matrix $A$ is stored in a data structure applicable for QRAM-based state preparation. Namely, suppose we are given an $A \\in \\mathbb{C}^{m\\times n}$ with minimum non-zero singular value $\\sigma$ which supports certain efficient $\\ell_2$-norm importance sampling queries, along with a $b \\in \\mathbb{C}^m$. Then, for some $x \\in \\mathbb{C}^n$ satisfying $\\|x - A^+b\\| \\leq \\varepsilon\\|A^+b\\|$, we can output a measurement of $|x\\rangle$ in the computational basis and output an entry of $x$ with classical algorithms that run in $\\tilde{\\mathcal{O}}\\big(\\frac{\\|A\\|_{\\mathrm{F}}^6\\|A\\|^6}{\\sigma^{12}\\varepsilon^4}\\big)$ and $\\tilde{\\mathcal{O}}\\big(\\frac{\\|A\\|_{\\mathrm{F}}^6\\|A\\|^2}{\\sigma^8\\varepsilon^4}\\big)$ time, respectively. This improves on previous\"quantum-inspired\"algorithms in this line of research by at least a factor of $\\frac{\\|A\\|^{16}}{\\sigma^{16}\\varepsilon^2}$ [Chia, Gily\\'en, Li, Lin, Tang, and Wang, STOC'20, arXiv:1910.06151]. As a consequence, we show that quantum computers can achieve at most a factor-of-12 speedup for linear regression in this QRAM data structure setting and related settings. Our work applies techniques from sketching algorithms and optimization to the quantum-inspired literature. Unlike earlier works, this is a promising avenue that could lead to feasible implementations of classical regression in a quantum-inspired settings, for comparison against future quantum computers.", "fields_of_study": "[\"Computer Science\",\"Physics\"]", "external_ids": {"arxiv": "2009.07268", "mag": "3086641254", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/quantum/GilyenST22", "doi": "10.22331/q-2022-06-30-754"}}, "content": {"source": {"pdf_hash": "c6856d7389479dda009970e9a2cb21afcd995247", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2009.07268v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9a0346a570b19bddbe922de3a225d01e047afd0e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c6856d7389479dda009970e9a2cb21afcd995247.txt", "contents": "\nAn improved quantum-inspired algorithm for linear regression\n27 Jun 2022\n\nAndr\u00e1s Gily\u00e9n \nAlfr\u00e9d R\u00e9nyi Institute of Mathematics\n\n\nZhao Song \nAdobe Research\n\n\nEwin Tang \nUniversity of Washington\n\n\nAn improved quantum-inspired algorithm for linear regression\n27 Jun 2022Andr\u00e1s Gily\u00e9n: gilyen@renyi.hu, Formerly at the Institute for Quantum Information and Matter, California Institute of Technology. Zhao Song: zsong@adobe.com Ewin Tang: ewint@cs.washington.edu Accepted in Quantum 2022-06-20, click title to verify. Published under CC-BY 4.0. 1\nWe give a classical algorithm for linear regression analogous to the quantum matrix inversion algorithm [Harrow, Hassidim, and Lloyd, Physical Review  Letters'09]  for low-rank matrices [Wossnig, Zhao, and Prakash, Physical Review  Letters'18], when the input matrix A is stored in a data structure applicable for QRAM-based state preparation.Namely, suppose we are given an A \u2208 C m\u00d7n with minimum non-zero singular value \u03c3 which supports certain efficient 2 -norm importance sampling queries, along with a b \u2208 C m . Then, for some x \u2208 C n satisfying x \u2212 A + b \u2264 \u03b5 A + b , we can output a measurement of |x in the computational basis and output an entry of x with classical algorithms that run i\u00f1time, respectively. This improves on previous \"quantum-inspired\" algorithms in this line of research by at least a factor of A 16 \u03c3 16 \u03b5 2 [Chia, Gily\u00e9n, Li, Lin, Tang, and Wang, STOC'20]. As a consequence, we show that quantum computers can achieve at most a factor-of-12 speedup for linear regression in this QRAM data structure setting and related settings. Our work applies techniques from sketching algorithms and optimization to the quantum-inspired literature. Unlike earlier works, this is a promising avenue that could lead to feasible implementations of classical regression in a quantuminspired settings, for comparison against future quantum computers.\n\nIntroduction\n\nAn important question for the future of quantum computing is whether we can use quantum computers to speed up machine learning [1]. Answering this question is a topic of active research [2,3,4]. One potential avenue to an affirmative answer is through quantum linear algebra algorithms, which can perform linear regression [5] (and compute similar linear algebra expressions [6]) in time poly-logarithmic in input dimension, with some restrictive \"quantum\" assumptions [2,3]. For machine learning tasks, a natural setting of these assumptions is that the input is low-rank, in which case the above quantum algorithms require that the input is given in a manner allowing efficient quantum state preparation. This state preparation assumption is typically satisfied via a data structure instantiated with quantum random access memory (QRAM) 1 [7,8]. A recent paper shows that, by exploiting these assumptions, classical algorithms can also perform quantum linear algebra with only a polynomial slowdown [9], meaning that here quantum computers do not give the exponential speedup in dimension that one might hope for. However, these algorithms have running times with high exponents, leaving open the possibility that quantum computers admit large polynomial speedups for these problems. For example, given our current understanding, the quantum recommendation systems algorithm [10], with running time O * A F \u03c3 , could still give a large polynomial quantum speedup, since the best-known classical algorithm has running time\u00d5 * A 6 F A 16 \u03c3 24 [9] (improving the original running time of [11]). 2 It is an open question whether any quantum linear algebra algorithm admits a large polynomial speedup, compared to classical numerical linear algebra [1,12,13].\nO * A F \u03c3 24\nWe focus on this open question for the problem of low-rank linear regression, where we are given a matrix A \u2208 C m\u00d7n with minimum singular value \u03c3 and a vector b \u2208 C m , and asked to approximate x * := arg min x 1 2 Ax \u2212 b 2 2 . Linear regression has been an influential primitive for quantum machine learning (QML) [4] since the invention of Harrow, Hassidim, and Lloyd's algorithm (HHL) originally introduced for sparse A [5,14]. The lowrank variant that we consider commonly appears in the QML literature [15,16]. These algorithms, like all the algorithms we consider, are called \"low-rank\" because their runtimes depend on A 2 F \u03c3 2 \u2265 rank A, so in some sense, good performance requires the matrix A to be strictly low-rank. However, this restriction can be relaxed with regularization, which replaces \u03c3 2 by \u03c3 2 + \u03bb, so that for reasonable choices of the regularization parameter \u03bb, A only needs to be approximately low-rank. The current state-of-the-art quantum algorithm can produce a state \u03b5-close to |A + b in 2 -norm in O * A F A A \u03c3 time [17], given A and b in the aforementioned data structure. We think about this running time as depending polynomially on the (square root of) stable rank A F A and the condition number A \u03c3 . Note that being able to produce quantum states |x := 1\n\nx i x i |i corresponding to a desired vector x is akin to a classical sampling problem, and is different from outputting x itself. Though recent \"quantum-inspired\" classical algorithms [18] demonstrate that this quantum algorithm does not admit exponential speedups, the best previous classical algorithm of Chia, Gily\u00e9n, Li, Lin, Tang and Wang [9] runs in\u00d5 * A F A 6 A \u03c3 28 time, leaving significant room for polynomial quantum speedups. 3 1 In this paper, we always use QRAM in combination with a data structure used for efficiently preparing states |0 \u2192 i\n\nx i\n\nx |i corresponding to vectors x \u2208 C n . 2 We define\u00d5(T ) as O(T \u00b7polylog(T )), and define O * to be big O notation, hiding polynomial dependence on \u03b5 and poly-logarithmic dependence on dimension. 3 The \u03b5 dependence for the quantum algorithm is log 1 \u03b5 , compared to the classical algorithm which gets\n\nOur main result tightens this gap, giving an algorithm running in\nO * A F A 6 A \u03c3 12\ntime. Roughly, this tightens the separation between quantum and classical from 1-to-28 to 1-to-12. As a bonus, this algorithm is a factor of A 4 \u03c3 4 faster if we want to compute an entry of the output, rather than to sample an index. Our algorithm is a carefully-analyzed instance of stochastic gradient descent that exploits the sampling access provided in our setting, combined with a technical sketching step to enforce sparsity of the input vector b. Since it is an iterative algorithm, it is potentially more practical to implement and use as a benchmark against future scalable quantum computers [19]. Our result suggests that other tasks that can be solved through iterative methods, like primal-dual algorithms, may have quantum-classical gaps that are smaller than existing quantum-inspired results suggest.\n\n\nOur model\n\nWe now introduce the input and output model that we and all prior quantum-inspired algorithms in this line of research use [20,9]. The motivation for this model is to be a classical analogue to the input model of QML algorithms: many such algorithms assume that input is stored in a particular data structure in QRAM [15,16,8,4,1,21], including current QML algorithms for low-rank linear regression (our focus) [17]. 4 So, our comparison classical algorithms must also assume that input is stored in this \"quantum-inspired\" data structure, which supports several fully-classical operations. We first define the quantuminspired data structure for vectors (Definition 1.1), then for matrices (Definition 1.2). Definition 1.1 (Vector-based data-structure, SQ(v) and Q(v)). For any vector v \u2208 C n , let SQ(v) denote a data-structure that supports the following operations:\n1. Sample(), which outputs the entry i with probability |v i | 2 / v 2 .\n2. Query(i), which takes i \u2208 [n] as input and outputs v i . 3. Norm(), which outputs v . 5 Let T (v) denote the max time it takes for the data structure to respond to any query. If we only allow the Query operation, the data-structure is called Q(v).\n\nNotice that the Sample function is the classical analogue of quantum state preparation, since the distribution being sampled is identical to the one attained by measuring |v in the computational basis. We now define the quantum-inspired data structure for matrices. Definition 1.2 (Matrix-based data-structure, SQ(A)). For any matrix A \u2208 C m\u00d7n , let SQ(A) denote a data-structure that supports the following operations:\n1. Sample1(), which outputs i \u2208 [m] with probability A i, * 2 / A 2 F . 1 \u03b5 6 .\nWhile this suggests an exponential speedup in \u03b5, it appears to only hold for sampling problems, such as measuring |A + b in the computational basis. Learning information from the output quantum state |A + b generally requires poly( 1 \u03b5 ) samples, preventing exponential separation for computational problems. 4 Although current QRAM proposals suggest that quantum hardware implementing QRAM may be realizable with essentially only logarithmic overhead in the running time [7], an actual physical implementation would require substantial advances in quantum technology in order to maintain coherence for a long enough time [22]. Let T (A) denote the max time the data structure takes to respond to any query.\n\n\nSample2(i), which takes a row index i \u2208 [m] as input and outputs the column index\n\nFor the sake of simplicity, we will assume all input SQ data structures respond to queries in O(1) time. There are data structures that can do this in the word RAM model [9,Remark 2.15]. The dynamic data structures that commonly appear in the QML literature [10] respond to queries in O(log(mn)) time, so using such versions only increases our running time by a logarithmic factor.\n\nThe specific choice of data structure does not appear to be important to the quantumclassical separation, though. The QRAM and data structure assumptions of typical QML algorithms can be replaced with any state preparation assumption, which is an assumption implying that quantum states corresponding to input data can be prepared in time polylogarithmic in dimension. However, to the best of our knowledge, all models admitting efficient protocols that take v stored in the standard way as input and output the quantum state |v also admit corresponding efficient classical sample and query operations that can replace the data structure described above [9,Remark 2.15]. In other words, classical algorithms in the sample and query access model can be run whenever the corresponding QML algorithm can, assuming that the input data is classical. Consequently, our results on the quantum speedup for low-rank matrix inversion appear robust to changing quantum input models.\n\nNotations. For a vector v \u2208 C n , v denotes 2 norm. For a matrix A \u2208 C m\u00d7n , A \u2020 , A + , A , and A F denote the conjugate transpose, pseudoinverse, operator norm, and Frobenius norm of A, respectively. We use A i,j to denote the entry of A at the i-th row and j-th column. We use A i, * and A * ,j to denote the i-th row and j-th column of A.\n\nWe use E[\u00b7] and V[\u00b7] to denote expectation and variance of a random variable. Abusing notation, for a random vector v, we denote\nV[v] = E[ v \u2212 E[v] 2 ].\nFor a differentiable function f : R n \u2192 R, \u2207f denotes the gradient of f . We say f is convex if, for all x, y \u2208 R n and t \u2208 [0, 1], f (tx\n+ (1 \u2212 t)y) \u2264 tf (x) + (1 \u2212 t)f (y).\nWe use the shorthand g h for g = O(h), and respectively g h and g h for g = \u2126(h) and g = \u0398(h).\u00d5(T ) denotes O(T \u00b7 polylog(T )) and O * denotes big O notation that hides polynomial dependence on \u03b5 and poly-logarithmic dependence on dimension.\n\nNote that the precise exponent on log(mn) depends on the choice of word size in our RAM models. Quantum and classical algorithms have different conventions for this choice, hence we use notation that elides this detail for simplicity.\n\n\nOur results\n\nIn this paper, we focus on solving the following problem. Prior work in this line of research has studied this problem in the \u03bb = 0 case [9].\n\nProblem (Regression with regularization). Given A \u2208 C m\u00d7n , b \u2208 C m , and a regularization parameter \u03bb \u2265 0, we define the function f : C n \u2192 R as \nf (x) := 1 2 ( Ax \u2212 b 2 + \u03bb x 2 ).Let 6 x * := arg min x\u2208C n f (x) = (A \u2020 A + \u03bbI) + A \u2020 b.\nWe will manipulate vectors by manipulating their sparse descriptions, defined as follows:\n\n\nDefinition 1.3 (sparse description). We say we have an s-sparse description of\nx \u2208 C d if we have an s-sparse v \u2208 C n such that x = A \u2020 v.\nWe use the convention that any s-sparse description is also a t-sparse description for all t \u2265 s.\n\nOur main result is that we can solve regression efficiently, assuming SQ(A). Algorithm 1 and the corresponding Theorem 1.4 directly improves the previous best result [9] due to Chia, Gily\u00e9n, Li, Lin, Tang and Wang by a factor of A 20 \u03c3 20 \u03b5 2 . 1: Init: Sparsify b as described in Lemma 2.7 to obtain an s = 800\nA 2 F b 2 (\u03c3 2 +\u03bb) 2 \u03b5 2 x * 2 -spars\u00ea b. 2: Set v (0) := 0 (and implicitly define x (t) = A \u2020 v (t) ). 3: Set \u03b7 := \u03b5 2 (\u03c3 2 +\u03bb) 32 A 2 F A 2 +16\u03bb 2 and T := ln(8/\u03b5 2 ) \u03b7(\u03c3 2 +\u03bb) = 32 ln \u221a 8 \u03b5 2 A 2 F A 2 +\u03bb 2 \u03b5 2 (\u03c3 2 +\u03bb) 2 . 4: for t = 0, 1, . . . , T \u2212 1 do 5:\nSample a row index r according to the row norms\nAr, * 2 A 2 F . 6: Sample C := A 2 F A 2 column indices {c i } i\u2208[C] i.i.d. according to |Ar,c| 2\nAr, * 2 .\n\n\n7:\n\nDefine v (t+1) as follows:\nv (t+1) := (1 \u2212 \u03b7\u03bb)v (t) + \u03b7b \u2212 \u03b7 A 2 F A r, * 2 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j (A * ,c j ) \u2020 v (t) e r .(1)\n8: end for 9: Output: v (T ) which is 32\n2 A 2 F A 2 +\u03bb 2 (\u03c3 2 +\u03bb) 2 \u03b5 2 25 b 2 2 A 2 x * 2 + ln \u221a 8 \u03b5 -sparse; set x := A \u2020 v (T ) .\nQueries to x:\nx j = A \u2020 j,i v (T ) i , so query A i,j for all non-zero v (T ) i\nand compute the sum. Sampling from |x j | 2 / x 2 : Perform rejection sampling according to Lemma 2.5.\n\n\nTheorem 1.4 (Main result). Suppose we are given\nSQ(A) \u2208 C m\u00d7n , Q(b) \u2208 C n . Denote \u03c3 := A + \u22121 and consider f (x) for \u03bb A F A . Algorithm 1 runs in O A 6 F A 2 (\u03c3 2 + \u03bb) 4 \u03b5 4 b 2 A 2 x * 2 + log 1 \u03b5 log 1 \u03b5 time and outputs an O A 2 F A 2 (\u03c3 2 +\u03bb) 2 \u03b5 2 ( b 2 A 2 x * 2 + log 1 \u03b5 ) -sparse description of an x such that x \u2212 x * \u2264 \u03b5 x * with probability \u2265 0.9. This description admits SQ(x) for T (x) = O A 6 F A 6 (\u03c3 2 + \u03bb) 6 \u03b5 4 log 2 1 \u03b5 b 4 A 4 x * 4 + log 2 1 \u03b5 b 2 A 2 F x * 2 + 1 .\nNote that the running time stated for T (x) corresponds to the running time of Sample and Norm; the running time of Query is O\nA 2 F A 2 (\u03c3 2 +\u03bb) 2 \u03b5 2 ( b 2 A 2 x * 2 + log 1 \u03b5 )\n, the sparsity of the description. So, unlike previous quantum-inspired algorithms, it may be the case that the running time of the SQ query dominates, though it's conceivable that this running time bound is an artifact of our analysis.\n\nFactors of b 2\n\nA 2 x * 2 (which is always at least one) arise because sampling error is additive with respect to b , and need to be rescaled relative to x * ; the quantum algorithm must also pay such a factor. The expression\nA 2 x * 2 b 2\ncan be thought of as the fraction of b's \"mass\" that is in the well-conditioned rowspace of A. To make this rigorous, we define the following thresholded projector.\nDefinition 1.5. For an A \u2208 C m\u00d7n with singular value decomposition A = \u03c3 i u i v \u2020 i and \u03bb \u2265 0, we define \u03a0 A,\u03bb = p A,\u03bb (\u03c3 i )u i u \u2020 i where p A,\u03bb (\u03c3) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 0 \u03c3 = 0 2\u03c3 \u221a \u03bb \u03c3 2 +\u03bb 0 < \u03c3 \u2264 \u221a \u03bb 1 \u221a \u03bb < \u03c3.\nFor intuition, \u03a0 A,0 projects onto the rowspace of A, and for nonzero \u03bb smoothly projects away the u i 's with singular values \u03c3 i smaller than\n\u221a \u03bb roughly linearly, since \u03c3 \u221a \u03bb \u2264 p A,\u03bb (\u03c3) \u2264 2 \u221a \u03bb for \u03c3 \u2208 [0, \u221a \u03bb]. We use the definition presented here because \u03a0 A,\u03bb b\ngives a natural lower bound for x * : by Fact 2.8,\nA 2 x * 2 b 2 \u2265 \u03a0 A,\u03bb b 2 2 b 2\nwhen \u03bb \u2264 A 2 . This is the \"fraction of mass\" type quantity that one would expect. 7 As mentioned previously, we use stochastic gradient descent to solve this problem, for\nT := O A 2 F A 2 (\u03c3 2 +\u03bb) 2 \u03b5 2 log 1 \u03b5\niterations. Such optimization algorithms are standard for solving regression in this setting [23,24]: the idea is that, instead of explicitly computing the closed form of the minimizer to f , which requires computing a matrix pseudoinverse, one can start at x (0) = 0 and iteratively find x (t+1) from x (t) such that the sequence {x (t) } t\u2208N converges to a local minimum of f . Updating x (t) by nudging it in the direction of f 's gradient produces such a sequence, even when we only use decent (stochastic) estimators of the gradient [25]. Because f is convex, it only has one minimizer, so x (t) converges to the global minimum. 6\n\nChallenges. Though the idea of SGD for regression is simple and well-understood, we note that some standard analyses fail in our setting, so some care is needed in analysing SGD correctly.\n\n\u2022 First, our stochastic gradient (Definition 2.1) has variance that depends on the norm of the current iterate, and projection onto the ball of vectors of bounded norm is too costly, so our analyses cannot assume a uniform bound on the second moment of our stochastic gradient.\n\n\u2022 Second, we want a bound for the final iterate x (T ) , not merely a bound on averaged iterates as is common for SGD analyses, since using an averaging scheme would complicate the analysis in Section 2.2.\n\n7 Note that this also means\nA 2 x * 2 b 2 \u2265 M b 2 2 b 2 for any matrix M satisfying M \u03a0 A,\u03bb . In particular,\nit holds when M projects onto only those ui's such that \u03c3i \u2265 \u221a \u03bb, i.e., when M is a true projector onto the well-conditioned subspace of the rowspace of A.\n\n\u2022 Third, we want a bound on the output x of our algorithm of the form x \u2212 x * \u2264 \u03b5 x * . We could have chosen to aim for the typical bound in the optimization\nliterature of f (x) \u2212 f (x * ) \u2264 \u03b5(f (x (0) ) \u2212 f (x * )\n), but our choice is common in the QML literature, and it only makes sense to have SQ(x) when x is indeed close to x * up to multiplicative error.\n\n\u2022 Finally, note that acceleration via methods like the accelerated proximal point algorithm (APPA) [26] framework cannot be used in this setting, since we cannot pay the linear time in dimension necessary to recenter gradients.\n\nThe SGD analysis of Bach and Moulines [27] meets all of our criteria, which inspired the error analysis of this paper, while we use quantum-inspired ideas for the computation of the stochastic gradients (Definition 2.1).\n\nThe running time of SGD is only good for our choice of gradient when b is sparse, but through a sketching matrix we can reduce to the case where b has O\nA 2 F A 2 \u03c3 4 \u03b5 2 b 2 AA + b 2\nnon-zero entries. Additional analysis is necessary to argue that we have efficient sample and query access to the output, since we need to rule out the possibility that our output x is given as a description A \u2020x such thatx is much larger than x, in which case computing, say, x via rejection sampling would be intractable.\n\nOur analysis of the stochastic gradient descent is completely self-contained and the analysis of the stochastic gradients only relies on a few standard results about quantum-inspired sampling techniques, so the paper shall be accessible even without having a background on convex optimization.\n\n\nDiscussion\n\nComparisons to prior work. First, we describe the prior work on quantum-inspired classical low-rank linear regression algorithms in more detail. There are three prior papers here: [28] achieves a running time of\u00d5( [29] achieves an incomparable running time of O(\nA 6 F k 6 A 16 \u03c3 22 \u03b5 6 ) (where k \u2264 A 2 F \u03c3 2 is the rank of A),A 6 F A 22 \u03c3 28 \u03b5 6\n), and [9] achieves the same running time as [29] for a more general problem (where \u03c3 can be chosen to be a threshold, and isn't necessarily the minimum singular value of A). For simplicity of exposition, we only compared our algorithm to the latter running time. However, our algorithm improves on all previous running times: an improvement of k 6 A 10 \u03c3 10 \u03b5 2 on the former, and A 16 \u03c3 16 \u03b5 2 on the latter.\n\nComparison to [23] In some sense, this work gives a version of the regression algorithm of Gupta and Sidford [23] (which, alongside [24], is the current state-of-the-art iterative algorithm for regression in many numerically sparse settings) that trades running time for weaker, more quantum-like input assumptions. Their algorithm takes A and b, with only query access to the input's nonzero entries, and explicitly outputs a vector x satisfying\nx\u2212x * \u2264 \u03b5 x * in\u00d5 * nnz(A)+ A F \u03c3 nnz(A) 2 3 n 1 6\ntime (shown here after naively bounding numerical sparsity by n). They use a stochastic first-order iterative method, that we adapt by mildly weakening the stochastic gradient to be efficient assuming the quantum-inspired data structure (instead of the bespoke data structure they design for their algorithm).\n\nTo elaborate, we make a direct comparison. Suppose we wish to apply our algorithm from Theorem 1.4 to Gupta and Sidford's setting, meaning that we are given A and b as lists of non-zero entries, and wish to output the vector x in full satisfying x \u2212 x * \u2264 \u03b5 x * . Because of the difference in setting, we need additional O(nnz(A)) time to set up the data structures to get SQ(A), and we also need O * (\nA 2 F A 2 \u03c3 4\nn) time to query for all the entries of x, given the SQ(x) from the output of Algorithm 1. In total, this takes\nO(nnz(A)) + O * A 6 F A 2 \u03c3 8 + A 2 F A 2 \u03c3 4\nn time. This is worse than Gupta and Sidford's time, even after accounting for the fact that, unlike in the setting we consider, we can apply the acceleration framework in [26]. Acceleration speeds up our algorithm and reduces our error dependence to poly-logarithmic in 1 \u03b5 , at the cost of nnz(A) time per \"phase\" throughout the logarithmically many phases. The quantum algorithm performs comparably, since one needs to run it O * (n) times so that state tomography can convert copies of |x to an explicit list of x's entries, resulting in a total running time of O(nnz(A)) +\u00d5 * A F \u03c3 n . However, our algorithm performs better than Gupta and Sidford's algorithm in a weaker setting. QML algorithms often don't take the form specified above, where the goal is to output a full vector and we assume nothing about the input. For example, the quantum recommendation system [10] only outputs a measurement from the output vector, and works in a setting (providing online recommendations, in essence a dynamic data structure problem) where the nnz(A) cost for building a data structure can be amortized over many runs of their algorithm. This is the setting we design our algorithm for, where we are given SQ(A) directly and only wish to know a measurement from the output vector via SQ(x), and we achieve a runtime independent of dimension via Algorithm 1. Gupta and Sidford's algorithm (even without acceleration) does not behave well in this setting, and would have a running time polynomial in input size, thus being exponentially slower than the quantum-inspired (and quantum) algorithm. Our goal with this result is to advance the complexity theoretic understanding of quantum speedups by limiting the possibility of quantum speedup in the most general setting possible. To this end, we assume fairly little about our input, so our classical algorithms can work for many different applications where one might hope for quantum speedups. This comes at the cost of increased runtime, including a polynomial dependence on \u03b5 \u22121 which, being exponentially larger than in other classical algorithms, potentially reduces the applicability of our result as an algorithm. On the other hand, a similar drawback is also present in the related quantum algorithms, so the resulting runtimes can be at least more directly compared.\n\nFinally, we note that sketching algorithms could also have been used to improve upon previous quantum-inspired results. In particular, one can view the 2 -type samples that our input data structure supports as A 2 F k\u03c3 2 -oversampled leverage score samples, enabling the application of typical sketching results [30] associated with approximating matrix products and approximate subspace embeddings. However, we were not quite able to find an algorithm with running time as good as Theorem 1.4 through the sketching literature, though something similar might be possible (say, the same running time but with the alternate guarantee that Ax \u2212 b \u2264 (1 + \u03b5) Ax * \u2212 b ). Our guarantee that x \u2212 x * \u2264 \u03b5 x * is perhaps more natural in this quantum-inspired setting, since we need x to be bounded away from zero to enable efficient (rejection) sampling. Sketching may be more beneficial in the slightly less restrictive setting where one wishes to find any dynamic data structure that can store A, b and output measurements from |\u2248 A + b . In this setting, one wouldn't be required to use the QRAM data structure, and could build alternative data structures that could exploit, for example, oblivious sketches. 8\n\nComparisons to concurrent and subsequent work. Around the same time as this work, Chepurko, Clarkson, Horesh, and Woodruff described algorithms for linear regression and \"quantum-inspired\" recommendation systems, heavily using technical tools from the sketching literature [31]. Their algorithm gives a roughly A 2 F A 2 improvement over our algorithm, ignoring minor details involving problem-specific parameters. 9 This is done using an algorithm similar to that of [28], but with an improved analysis and an application of conjugate gradient to solve a system of linear equations (instead of inverting the matrix directly).\n\nSubsequent work by Shao and Montanaro [32] gives an algorithm improving on this result by a factor of A 4 \u03c3 4 \u03b5 2 when \u03bb = 0 and Ax * = b exactly. Their work uses the randomized Kaczmarz method [33], which is a version of stochastic gradient descent [34]. If one changes our stochastic gradient \u2207g to the randomized Kaczmarz update used in [32], replacing A \u2020 b with the sketch Outlook. Our result and the aforementioned follow-up work show that simple iterative algorithms like stochastic gradient descent can be nicely combined with quantum-inspired linear algebra techniques, resulting in quantum-inspired algorithms that compare favorably to direct approaches based on finding approximate singular value decompositions [9]. However, one needs to be careful, as more involved iterative steps require more involved quantum-inspired subroutines that can rapidly blow up the overall complexity, as occurs in the quantum-inspired SDP solver of [9], for example. We leave it to future work to study which other iterative approaches can be similarly advantageously combined with quantum-inspired linear algebra techniques.\n\n\nProofs\n\nFor simplicity we assume knowledge of A , \u03c3, and later, x * , exactly, despite that our SQ(A) only gives us access to A F . Just an upper bound on A and lower bounds on \u03c3 and x * respectively suffice, giving a running time bound by replacing these quantities in our complexity bounds by any respective upper or lower bounds. This holds because these quantities are only used to choose the internal parameters s, \u03b7 \u22121 , T, and C, and replacing these quantities by their respective bounds only increase these internal parameters, only improving the resulting error bounds of Algorithm 1.\n\nThe algorithm we use to solve regression is stochastic gradient descent (SGD). Suppose we wish to minimize a convex function f : R n \u2192 R. Consider the following recursion, starting from x (0) \u2208 R n , with a random function \u2207g : R n \u2192 R and a deterministic sequence of positive scalars (\u03b7 t ) k\u22651 .  10 Specifically, the bottleneck of our algorithm is the large bound on the residual E[ \u2207g(x * ) 2 ], since this is the piece of the error that does not decrease exponentially. If \u03bb = 0 and Ax * = b, then making the described change leads to Ax being replaced by Ax \u2212 b in the variance bound of Lemma 2.2, which is then zero for x = x * . So, unlike in the original analysis, increasing C beyond A 2 F A 2 actually helps to reduce this residual. For example, increasing C by a factor of 1 \u03b5 2 reduces the number of iterations by a factor of 1 \u03b5 2 , decreasing the runtime (which is linear in C and quadratic in iteration count) by a factor of 1 \u03b5 2 . We point the reader to [34] for further exploration of improving the residual in SGD and randomized Kaczmarz.\nx (t) = x (t\u22121) \u2212 \u03b7 t \u2207g(x (t\u22121) )(2)\nThe idea behind SGD is that if \u2207g is a good stochastic approximation to \u2207f , then x (t) will converge to the minimizer of f .\n\nIn our case, as defined in the introduction, f (x) = 1 2 ( Ax \u2212 b 2 + \u03bb x 2 ), so we need a stochastic approximation to \u2207f (x) = A \u2020 Ax \u2212 A \u2020 b + \u03bbx. Our choice of stochastic gradient comes from observing that A \u2020 Ax = m r=1 n c=1 (A r, * ) \u2020 A r,c x c , so we can estimate this by sampling some of the summands. \n\u2207g(x) = A 2 F A r, * 2 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j x c j (A r, * ) \u2020 \u2212 A \u2020 b + \u03bbx.(3)\nNotice that the first term of this expression is the average of copies of the random vector\nA 2 F |A r,c | 2 A r,c x c (A r, * ) \u2020 with probability |A r,c | 2 A 2 F .\nAlgorithm 1 is simply running the iteration Eq. (2) with x (0) = 0, \u2207g as defined in Definition 2.1, and \u03b7 t := \u03b5 2 (\u03c3 2 +\u03bb) 32 A 2 F A 2 +16\u03bb 2 (as we will see, this comes from applying Proposition 2.3 with \u03b5 = \u03b5/2). In Section 2.1, we prove that x (T ) satisfies the desired error bound for T := \u0398\nA 2 F A 2 +\u03bb 2 (\u03c3 2 +\u03bb) 2 \u03b5 2 log 1 \u03b5 .\nIn Section 2.2, we prove that computing x (T ) and simulating SQ(x (T ) ) can be done in the desired running time, assuming that b is sparse. Finally, in Section 2.3, we show how to generalize our result to non-sparse b.\n\n\nError analysis of stochastic gradient descent\n\nWe now list the properties of the stochastic gradient from Definition 2.1: the proofs are straightforward computation and so are deferred to the appendix.\n\n\nLemma 2.2.\n\nFor fixed x, y \u2208 C n and the random function \u2207g(\u00b7) defined in Definition 2.1, the following properties hold (\u2207g(x) and \u2207g(y) use the same instance of the random function):\nPart 1. E[\u2207g(x)] = \u2207f (x) = A \u2020 Ax \u2212 A \u2020 b + \u03bbx Part 2. V[\u2207g(x)] = 1 C A 4 F x 2 + 1 \u2212 1 C A 2 F Ax 2 \u2212 A \u2020 Ax 2 Part 3. E[ \u2207g(x) \u2212 \u2207g(y) 2 2 ] = (A \u2020 A + \u03bbI)(x \u2212 y) 2 + V[\u2207g(x \u2212 y)]\nWe take C = A 2 F / A 2 , which is the largest value we can set C to before it stops having an effect on the variance of \u2207g. In Section 2.2, we will see that it's good for SGD's running time to take C to be as large as possible. In this setting, and more generally if\nC \u2264 A 2 F x 2 Ax 2\nthe variance in Part 2 can be bounded as 2 C A 4 F x 2 . Now, we show that performing SGD for T iterations gives an x sufficiently close to the optimal vector x * . \u03b7(\u03c3 2 +\u03bb) = 8 ln\n\n\nProposition 2.3. Consider a matrix\n\u221a 2 \u03b5 2 A 2 F A 2 +\u03bb 2 \u03b5 2 (\u03c3 2 +\u03bb) 2 , E[ x (T ) \u2212 x * 2 ] \u2264 \u03b5 2 x * 2 .\nIn particular, by Chebyshev's inequality, with probability \u2265 0.96,\nx (T ) \u2212 x * \u2264 5\u03b5 x * .\nProof. Stochastic gradient descent is known to require a number of iterations linear in (something like the) second moment of the stochastic gradient. To analyze SGD, we loosely follow a strategy used by Bach and Moulines [27]. First, note that\nE[ \u2207g(x) 2 ] \u2264 E[2 \u2207g(x) \u2212 \u2207g(x * ) 2 + 2 \u2207g(x * ) 2 ] since a + b 2 \u2264 2 a 2 + 2 b 2 = 2 (A \u2020 A + \u03bbI)(x \u2212 x * ) 2 + V[\u2207g(x \u2212 x * )] + V[\u2207g(x * )] since \u2207f (x * ) = 0 \u2264 2 (A \u2020 A + \u03bbI)(x \u2212 x * ) 2 + 2 A 2 F A 2 ( x \u2212 x * 2 + x * 2 ) by Lemma 2.2 \u2264 2( A \u2020 A + \u03bbI 2 + 2 A 2 F A 2 ) x \u2212 x * 2 + 4 A 2 F A 2 x * 2 .\nNext, we use the well-known fact that f (x) is (\u03c3 2 + \u03bb)-strongly convex, 11 implying that Lemma 3.11], which can also be directly verified using the formula \u2207f (\n\u2207f (x) \u2212 \u2207f (y), x \u2212 y \u2265 (\u03c3 2 + \u03bb) x \u2212 y 2 [25,x) = A \u2020 Ax \u2212 A \u2020 b + \u03bbx for the gradient. E[ x (t) \u2212 x * 2 | x (t\u22121) ] = E[ x (t\u22121) \u2212 x * 2 + 2 x (t\u22121) \u2212 x * , x (t) \u2212 x (t\u22121) + x (t) \u2212 x (t\u22121) 2 | x (t\u22121) ] = E[ x (t\u22121) \u2212 x * 2 \u2212 2\u03b7 t x (t\u22121) \u2212 x * , \u2207g(x (t\u22121) ) + \u03b7 2 t \u2207g(x (t\u22121) ) 2 | x (t\u22121) ] = x (t\u22121) \u2212 x * 2 \u2212 2\u03b7 t x (t\u22121) \u2212 x * , \u2207f (x (t\u22121) ) + \u03b7 2 t E[ \u2207g(x (t\u22121) ) 2 | x (t\u22121) ] \u2264 (1 \u2212 2\u03b7 t (\u03c3 2 + \u03bb) + 2\u03b7 2 t ( A \u2020 A + \u03bbI 2 + 2 A 2 F A 2 )) x (t\u22121) \u2212 x * 2 + 4\u03b7 2 t A 2 F A 2 x * 2 .\nWe conditioned on x (t\u22121) in the above computation, so the randomness in \u2207g came only from one iteration.\nLet \u03b4 t := E[ x (t) \u2212 x * 2 ]\n. Then by taking expectation over x (t\u22121) for both sides of the above computation, we get\n\u03b4 t \u2264 (1 \u2212 2\u03b7 t (\u03c3 2 + \u03bb) + 2\u03b7 2 t ( A \u2020 A + \u03bbI 2 + 2 A 2 F A 2 ))\u03b4 t\u22121 + 4\u03b7 2 t A 2 F A 2 x * 2 \u2264 (1 \u2212 2\u03b7 t (\u03c3 2 + \u03bb) + 2\u03b7 2 t (4 A 2 F A 2 + 2\u03bb 2 ))\u03b4 t\u22121 + 4\u03b7 2 t A 2 F A 2 x * 2 ,\nwhere the last step follows from the inequality a + b 2 \u2264 2 a 2 + 2 b 2 . Now, by substituting \u03b7 2 t = \u03b7 2 = \u03b7 \u03b5 2 (\u03c3 2 +\u03bb) 8 A 2 F A 2 +4\u03bb 2 and using that \u03b5 \u2264 1 we get\n\u03b4 t \u2264 (1 \u2212 \u03b7(\u03c3 2 + \u03bb))\u03b4 t\u22121 + \u03b5 2 2 \u03b7(\u03c3 2 + \u03bb) x * 2 .\nThis is a purely deterministic recursion on \u03b4 t , which we can bound by\n\u03b4 t \u2264 exp(\u2212t\u03b7(\u03c3 2 + \u03bb)) + \u03b5 2 2 x * 2 .\n11 Since we search for a sparse description of a solution vector we are effectively restricting the function to the image of A \u2020 , and on this subspace the function is indeed (\u03c3 2 + \u03bb)-strongly convex.\n\nSince x (0) = 0 we have \u03b4 0 = x * 2 so the bound holds for t = 0, and by induction we have\n\u03b4 t \u2264 (1 \u2212 \u03b7(\u03c3 2 + \u03bb)) exp(\u2212(t \u2212 1)\u03b7(\u03c3 2 + \u03bb)) + \u03b5 2 2 x * 2 + \u03b5 2 2 \u03b7(\u03c3 2 + \u03bb) x * 2 = (1 \u2212 \u03b7(\u03c3 2 + \u03bb)) exp(\u2212(t \u2212 1)\u03b7(\u03c3 2 + \u03bb)) + \u03b5 2 2 x * 2 \u2264 exp(\u2212t\u03b7(\u03c3 2 + \u03bb)) + \u03b5 2 2 x * 2 ,(4)\nwhich is at most \u03b5 2 x * 2 for t \u2265 T = ln(2/\u03b5 2 ) \u03b7(\u03c3 2 +\u03bb) .\n\n\nTime complexity analysis of SGD for sparse b\n\nNow, we show that, assuming SQ(A), it's possible to perform the gradient steps when b is sparse (that is, the number of non-zero entries of b, b 0 , is small). Recall from Definition 1.3 that we say we have an s-sparse description of x \u2208 C d if we have an s-sparse v \u2208 C n such that x = A \u2020 v.\nLemma 2.4. Given SQ(A), we can output x (t) as a (t + b 0 )-sparse description in time O(Ct(t + b 0 )).\nProof. First, suppose we are given x (t) as an s-sparse description, and wish to output a sparse description for the next iterate x (t+1) , which from Definition 2.1, satisfies\nx (t+1) = x (t) \u2212 \u03b7 t+1 \u2207g(x (t) ) = x (t) \u2212 \u03b7 t+1 \u00b7 A 2 F A r, * 2 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j x (t) c j (A r, * ) \u2020 \u2212 A \u2020 b + \u03bbx (t) .\nThe r and c j 's are drawn from distributions, and SQ(A) can produce such samples with its Sample queries, taking O(C) time. From inspection of the above equation, if we have x (t) in terms of its description as A \u2020 v (t) , then we can write x (t+1) as a description A \u2020 v (t+1) where v (t+1) satisfies (as in Equation (1))\nv (t+1) = v (t) \u2212 \u03b7 t+1 \u00b7 A 2 F A r, * 2 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j (A * ,c j ) \u2020 v (t) e r \u2212 b + \u03bbv (t) .(5)\nHere, e r is the vector that is one in the rth entry and zero otherwise. So, if v (t) is s-sparse, and has a support that includes the support of b, then v (t+1) is (s + 1)sparse. Furthermore, by exploiting the sparsity of v (t) , computing v (t+1) takes O(Cs) time (including the time taken to use SQ(A) to query A for all of the relevant norms and entries).\n\nSo, if we wish to compute x (t) , we begin with x (0) , which we have trivially as an b 0 -sparse description (v (0) = 0). It is sparser, but if we consider x (0) as having the same support as b, by the argument described above, we can then compute x (1) as an\n( b 0 + 1)-sparse description in O(C b 0 ) time. By iteratively computing v (i+1) from v (i) in O(C( b 0 + i)) time, we can output x (t) as a (t + b 0 )-sparse description in O t i=0 C( b 0 + i) = O(Ct(t + b 0 )) time as desired.\nIf \u03bb = O( A F A ) by Proposition 2.3, to get a good enough iterate x (T ) we can take\nT = O A 2 A 2 F (\u03c3 4 +\u03bb 2 )\u03b5 2 log 1 \u03b5 and C = A 2 F A 2 , giving a running time of O b 0 A 4 F (\u03c3 4 + \u03bb 2 )\u03b5 2 log 1 \u03b5 + A 2 A 6 F (\u03c3 8 + \u03bb 4 ) log 2 1 \u03b5 .\nNotice that it's good to scale C up to be as large as possible, since it means a corresponding linear decrease in the number of iterations, which our algorithm's running time depends on quadratically.\n\nAfter performing SGD, we have an x (T ) that is close to x * as desired, and it is given as a sparse description A \u2020 v (T ) . We want to say that we have SQ(x (T ) ) from its description. For this we invoke a result from [11], describing how to length-square sample from a vector that is a linear combination of length-square accessible vectors-that is, getting (c) estimate y to (1 \u00b1 \u03b5) multiplicative error with success probability at least 1 \u2212 \u03b4 in complexity\nSQ(c 1 v 1 + \u00b7 \u00b7 \u00b7 + c d v d ) given SQ(v 1 ), . . . , SQ(v d ).O d 2 \u2206 \u03b5 2 T (M ) \u00b7 log(1/\u03b4) .\nSo we care about the quantity \u2206 =\n1 A \u2020 v (t) 2 m i=1 A i, * 2 |v (t) i | 2 , with t = T ,\nwhere v (t) follows the recurrence according to Eq. (5) (recalling from before that r and c 1 , . . . , c C are sampled randomly and independently each iteration):\nv (t+1) = v (t) \u2212 \u03b7 t+1 \u00b7 A 2 F A r, * 2 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j (A * ,c j ) \u2020 v (t) e r \u2212 b + \u03bbv (t) \u2207g(v (t) )\n.\n\nRoughly speaking, \u2206 encodes the amount of cancellation that could occur in the product\nA \u2020 v (t)\n. We will consider it as a norm:\n\u2206 = v (t) 2 D x (t) 2 , for D a diagonal matrix with D ii = A i, * , where v D := \u221a v \u2020 D \u2020 Dv.\nThe rest of this section will be devoted to bounding \u2206 where x (t) comes from SGD as in Proposition 2.3.\n\nFirst, notice that we can show similar moment bounds for \u2207g(v (t) ) in the D norm as those for \u2207g(x (t) ) in Lemma 2.2; we defer the proof of this to the appendix.\nLemma 2.6. Let \u2207g(v) denote A 2 F A r, * 2 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j (A * ,c j ) \u2020 v e r \u2212 b + \u03bbv,\nwhere r, c 1 , . . . , c C are sampled according to the same distribution as \u2207g is (Definition 2.1).\n\nFor a fixed v \u2208 C m and C = A 2 F A 2 , we have:\nPart 1. E[\u2207g(v)] = AA \u2020 v \u2212 b + \u03bbv Part 2. E[ \u2207g(v) \u2212 E[\u2207g(v)] 2 D ] \u2264 2 A 2 F A 2 A \u2020 v 2 .\nTo bound \u2206, we will show a recurrence via Lemma 2.6. Note that\nE[ v (t+1) D | v (t) ] \u2264 v (t) D (1 \u2212 \u03b7 t+1 \u03bb) + \u03b7 t+1 E[ \u2207g(v (t) ) \u2212 \u03bbv (t) D | v (t) ] by triangle inequality \u2264 v (t) D + \u03b7 t+1 E[ \u2207g(v (t) ) \u2212 \u03bbv (t) 2 D | v (t) ] by E[Z] 2 \u2264 E[Z 2 ] \u2264 v (t) D + \u03b7 t+1 AA \u2020 v (t) \u2212 b 2 D + 2 A 2 F A 2 A \u2020 v (t) 2 by E[Z 2 ] = E[Z] 2 + V[Z] \u2264 v (t) D + \u03b7 t+1 ( AA \u2020 v (t) \u2212 b D + \u221a 2 A F A A \u2020 v (t) ) by \u221a a + b \u2264 \u221a a + \u221a b \u2264 v (t) D + \u03b7 t+1 ( b D + (1 + \u221a 2) A F A x (t) ). by x (t) = A \u2020 v (t) and u D \u2264 A u\nTaking expectation over v (t) we get\nE[ v (t+1) D ] \u2264 E[ v (t) D ] + \u03b7 t+1 ( b D + 3 A F A E[ x (t) ]),\nand since v (0) = 0 we can trivially solve this recurrence resulting in\nE[ v (t) D ] \u2264 t t =1 \u03b7 t ( b D + 3 A F A E[ x (t \u22121) ]).(6)\nFurther, using the parameter choices of Proposition 2.3 due to Eq. (4) we have\nE[ x (t ) ] \u2264 x * + E[ x (t ) \u2212 x * 2 ] = x * + \u03b4 t \u2264 3 x * , so E[ v (t) D ] \u2264 t\u03b7( b D + 9 A F A x * ).(7)\nFinally, we can bound our cancellation constant. Using Markov's inequality for Eq. (7) and combining it with Proposition 2.3 by a union bound we get that with probability \u2265 0.9, both\nx (T ) \u2212 x * \u2264 5\u03b5 x * and v (T ) D \u2264 20T \u03b7( b D + 9 A F A x * ) = 20 ln(2/\u03b5 2 ) (\u03c3 2 + \u03bb) ( b D + 9 A F A x * )\nWhen both those bounds hold and \u03b5 \u2264 1/10, we have\n\u2206 = v (T ) 2 D x (T ) 2 \u2264 v (T ) 2 D x * 2 (1 \u2212 5\u03b5) 2 log 2 (1/\u03b5) (\u03c3 2 + \u03bb) 2 b 2 D x * 2 + A 2 F A 2 log 2 (1/\u03b5) (\u03c3 2 + \u03bb) 2 A 2 b 2 x * 2 + A 2 F A 2 . (8)\nThe last inequality follows from using that b 2\nD = Db 2 \u2264 D 2 b 2 \u2264 A 2 b 2 .\n2.3 Extending to non-sparse b: Proof of Theorem 1.4\n\nIn previous sections, we have shown how to solve our regularized regression problem for sparse b: from Proposition 2.3, performing SGD for T\nA 2 F A 2 +\u03bb 2\n(\u03c3 2 +\u03bb) 2 \u03b5 2 log 1 \u03b5 iterations outputs an x with the desired error bound; from Lemma 2.4, it takes O\nA 2 F A 2 T (T + b 0 )\ntime to output x as a sparse description; and from Section 2.2, we have sample and query access to that output x given its sparse description. Let \u03bb = O( A F A ) in the rest of the section. Now, all that remains is to extend this work to the case that b is non-sparse. In this case, we will simply replace b with a sparseb that behaves similarly, and show that running SGD with this value ofb gives all the same results. The sparsity ofb will be O\nA 2 F b 2 (\u03c3 2 +\u03bb) 2 \u03b5 2 x * 2 , giving a total running time of O A 6 F A 2 (\u03c3 2 + \u03bb) 4 \u03b5 4 b 2 A 2 x * 2 + log 1 \u03b5 log 1 \u03b5 .\nWe show below that the bound in Eq. (8) also holds in this case. Using Lemma 2.5, the time it takes to respond to a query to SQ(x (T ) ) with probability 0.99 is (T + b 0 ) 2 \u2206, which gives the running time in Theorem 1.4,\nO (T + b 0 ) 2 \u2206 = O A 6 F A 6 (\u03c3 2 + \u03bb) 6 \u03b5 4 log 2 1 \u03b5 b 4 A 4 x * 4 + log 2 1 \u03b5 b 2 A 2 F x * 2 + 1 .\nThe crucial observation for sparsifying b is that we can use importance sampling to approximate the matrix product A \u2020 b, which suffices to approximate the solution x * .\n\n\nLemma 2.7 (Matrix multiplication to Frobenius norm error, [35, Lemma 4]).\n\nConsider X \u2208 C m\u00d7n , Y \u2208 C m\u00d7p , and let S \u2208 R s\u00d7m be an importance sampling matrix for X. That is, let each S i, * be independently sampled to be\nX F \u221a s X i, * e i with probability X i, * 2 X 2 F . Then E[ X \u2020 S \u2020 SY \u2212 X \u2020 Y 2 F ] \u2264 1 s X 2 F Y 2 F .\nWe have SQ(A), so we can use Lemma 2.7 with s \u2190 200\nA 2 F b 2\n(\u03c3 2 +\u03bb) 2 \u03b5 2 x * 2 , X \u2190 A, and Y \u2190 b, to find an S in O(s) time that satisfies the guarantee\nA \u2020 S \u2020 Sb \u2212 A \u2020 b \u2264 200 s A F b = \u03b5(\u03c3 2 + \u03bb) x *(9)\nwith probability \u2265 0.995, using Markov's inequality. Recall that D is defined to be the diagonal matrix with D i,i = A i, * , so D \u2020 = D, D F = A F , and importance sampling matrices for A are also importance sampling matrices for D. Consequently, we can apply Lemma 2.7 with the same argument to conclude that, with probability \u2265 0.995,\nDS \u2020 Sb \u2212 Db \u2264 200 s D F b = \u03b5(\u03c3 2 + \u03bb) x *(10)\nBy union bound, both Eq. (9) and Eq. (10) hold for the same S with probability \u2265 0.99. Assuming Eqs. (9) and (10) hold, we can perform SGD as in Proposition 2.3 onb := S \u2020 Sb (which is s-sparse) to find an x such that x \u2212x * \u2264 \u03b5 x * , wherex * is the optimum\n(A \u2020 A + \u03bbI) \u22121 A \u2020b . This implies that x \u2212 x * \u2264 2\u03b5 x * , since by Eq. (9), 12 x * \u2212 x * = (A \u2020 A + \u03bbI) + A \u2020 (b \u2212 b) \u2264 1 \u03c3 2 + \u03bb A \u2020 (b \u2212 b) \u2264 \u03b5 x * .(11)\nTo bound the running times of SQ(x), we modify the analysis of \u2206 from Section 2.2: recalling from Eq. (8), we have that, with probability \u2265 0.9,\n\u2206 log 2 (1/\u03b5) (\u03c3 2 + \u03bb) 2 b 2 D x * 2 + A 2 F A 2 log 2 (1/\u03b5) (\u03c3 2 + \u03bb) 2 b 2 D x * 2 + A 2 F A 2 .(12)\nIn other words, the same bound on \u2206 from Eq. (8) holds after approximating b byb. The last step follows from the following upper bound on\nb 2 D x * 2 : b 2 D x * 2 \u2264 b 2 D x * 2 (1 \u2212 \u03b5) 2 = DS \u2020 Sb 2 x * 2 (1 \u2212 \u03b5) 2 \u2264 ( Db + Db \u2212 DS \u2020 Sb ) 2 x * 2 (1 \u2212 \u03b5) 2 Db 2 + \u03b5 2 (\u03c3 2 + \u03bb) 2 x * 2 x * 2 (1 \u2212 \u03b5) 2 b 2 D x * 2 + \u03b5 2 (\u03c3 2 + \u03bb) 2\nwhere the first inequality follows from Eq. (11), the second inequality is triangle inequality, the third inequality follows from Eq. (10), and the last inequality holds for \u03b5 \u2264 1 2 which we can assume without loss of generality. The added term of \u03b5 2 (\u03c3 2 + \u03bb) 2 is dominated by the A 2 F A 2 term in Eq. (12). Finally, we observe the following fact about x * which can be used to convert the runtime of Theorem 1.4 to other parameters.\nFact 2.8. x * \u2265 A A 2 +\u03bb \u03a0 A,\u03bb b when A \u2265 \u221a \u03bb, and x * = 1 2 \u221a \u03bb \u03a0 A,\u03bb b otherwise. x * = (A \u2020 A + \u03bbI) + A \u2020 b = (A \u2020 A + \u03bbI) + A \u2020 \u03a0 + A,\u03bb \u03a0 A,\u03bb b since A \u2020 = A \u2020 \u03a0 + A,\u03bb \u03a0 A,\u03bb \u2265 \u03a0 A,\u03bb b min v\u2208span(A \u2020 ) v =1 (A \u2020 A + \u03bbI) + A \u2020 \u03a0 + A,\u03bb v(13)\nThe min term is equal to the minimum non-zero singular value of (A \u2020 A + \u03bbI) + A \u2020 \u03a0 + A,\u03bb . The non-zero singular values of this expression are g(\u03c3 i ), where \u03c3 i is a non-zero singular value of A and\ng(\u03c3) = \u03c3 (\u03c3 2 + \u03bb)(p A,\u03bb (\u03c3)) = \uf8f1 \uf8f2 \uf8f3 1 2 \u221a \u03bb 0 < \u03c3 \u2264 \u221a \u03bb \u03c3 \u03c3 2 +\u03bb \u221a \u03bb < \u03c3.\nThis function is non-increasing, so the minimum singular value of (A \u2020 A + \u03bbI)\n+ A \u2020 \u03a0 + A,\u03bb\nis g( A ), proving the above fact by observing that for A \u2264 \u221a \u03bb the operator (A \u2020 A + \u03bbI) + A \u2020 \u03a0 + A,\u03bb is proportional to a projector, thus Eq. (13) becomes an equality. Ar, * 2 . For this choice of r and c 1 , . . . , c C , take\n\n\nA Stochastic gradient bounds\n\u2207g(x) = A 2 F A r, * 2 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j x c j (A r, * ) \u2020 \u2212 A \u2020 b + \u03bbx.(3)\n\nLemma 2.2.\n\nFor fixed x, y \u2208 C n and the random function \u2207g(\u00b7) defined in Definition 2.1, the following properties hold (\u2207g(x) and \u2207g(y) use the same instance of the random function): \nPart 1. E[\u2207g(x)] = \u2207f (x) = A \u2020 Ax \u2212 A \u2020 b + \u03bbx Part 2. V[\u2207g(x)] = 1 C A 4 F x 2 + 1 \u2212 1 C A 2 F Ax 2 \u2212 A \u2020 Ax 2A r,c 1 2 A 2 F A 2 F A r,c 1 2 A r,c 1 x c 1 (A r, * ) \u2020 \u2212 A \u2020 b + \u03bbx = A \u2020 Ax \u2212 A \u2020 b + \u03bbx = \u2207f (x).\nPart 2. Now variance. We use that, for a random vector v,\nV[v] = E[ v \u2212 E[v] 2 ] = E[ v 2 ] \u2212 E[v] 2 . V[\u2207g(x)] = V[\u2207g(x) + A \u2020 b \u2212 \u03bbx] = E[ \u2207g(x) + A \u2020 b \u2212 \u03bbx \u2212 E[\u2207g(x) + A \u2020 b \u2212 \u03bbx] 2 ] by definition of V[\u00b7] = V A 2 F A r, * 2 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j x c j (A r, * ) \u2020 by definition of \u2207g = V 1 C C j=1 A 2 F |A r,c j | 2 A r,c j x c j (A r, * ) \u2020 = E 1 C C j=1 A 2 F |A r,c j | 2 A r,c j x c j (A r, * ) \u2020 2 \u2212 A \u2020 Ax 2 . = E 1 C C j=1 A 2 F |A r,c j | 2 A r,c j x c j 2 A r, * 2 V \u2212 A \u2020 Ax 2 .(14)\nWe expand the first term, using that it is an average of i.i.d. random variables: For a fixed v \u2208 C m and C = A 2 F A 2 , we have:\nV = m i=1 A i, * 2 A 2 F E c 1 ,...,c C 1 C C j=1 A 2 F |A i,c j | 2 A i,c j x c j 2 A i, * 2 = A 2 F m i=1 E c 1 ,...,c C 1 C C j=1 A i, * 2 |A i,c j | 2 A i,c j x c j 2 = A 2 F m i=1 1 C V c 1 A i, * 2 |A i,c 1 | 2 A i,c 1 x c 1 + |A i, * x| 2 = A 2 F m i=1 1 C n j=1 |A i,j | 2 A i, * 2 A i, * 4 |A i,j | 4 |A i,j x j | 2 \u2212 1 C |A i, * x| 2 + |A i, * x| 2 = A 2 F m i=1 1 C A i, * 2 x 2 + 1 \u2212 1 C (A i, * x) 2 = 1 C A 4 F x 2 + 1 \u2212 1 C A 2 F Ax 2Part 1. E[\u2207g(v)] = AA \u2020 v \u2212 b + \u03bbv Part 2. E[ \u2207g(v) \u2212 E[\u2207g(v)] 2 D ] \u2264 2 A 2 F A 2 A \u2020 v 2 .\nProof.\n\n\nPart 1.\n\nWe first show the expectation,\nE[\u2207g(v)] = E A 2 F A r, * 2 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j (A * ,c j ) \u2020 v e r \u2212 b + \u03bbv = m r=1 E c 1 ,...,c C 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j (A * ,c j ) \u2020 v e r \u2212 b + \u03bbv = m r=1 n c=1 A r,c ((A * ,c ) \u2020 v)e r \u2212 b + \u03bbv = AA \u2020 v \u2212 b + \u03bbv.\n\nPart 2.\n\nFor the variance in the D norm, we reuse a computation from the proof of Lemma 2.2:\nE[ \u2207g(v) \u2212 E[\u2207g(v)] 2 D ] = V D A 2 F A r, * 2 1 C C j=1 A r, * 2 |A r,c j | 2 A r,c j (A * ,c j ) \u2020 v e r = E A r, * 2 1 C C j=1 A 2 F |A r,c j | 2 A r,c j (A * ,c j ) \u2020 v 2 V from Eq. (14), where x = A \u2020 v \u2212 AA \u2020 v 2 D = 1 C A 4 F A \u2020 v 2 + 1 \u2212 1 C A 2 F AA \u2020 v 2 \u2212 AA \u2020 v 2 D \u2264 2 A 2 F A 2 A \u2020 v 2 .\nAlgorithm 1\n1Quantum-inspired regression via stochastic gradient descent Input: SQ(A), Q(b), \u03b5, and \u03bb = O( A F A ).\n\nA 2 F 2 F\n22Ar, * 2 A \u2020 r, * b r , (i.e., replacingb in Equation(1)by A Ar, * 2 b r e r ) then our analysis recovers a similar result for the special case when \u03bb = 0 and Ax * = b.10 \n\nDefinition 2. 1 .\n1We define \u2207g(x) to be the random function resulting from the following process. Draw r \u2208 [m] from the distribution that is r with probabilityAr, * 2 A 2 F , and then draw c 1 , . . . , c C i.i.d. from the distribution that is c with probability |Ar,c| 2 Ar, * 2 . For this choice of r and c 1 , . . . , c C , take\n\nF\nA \u2208 C m\u00d7n , a vector b \u2208 C n , a regularization parameter \u03bb \u2265 0, and an error parameter \u03b5 \u2208 (0, 1]. Denote \u03c3 := A + \u22121 . Let x (T ) be defined as Eq.(2), with x (0) = 0, \u03b7 t := \u03b7 := A 2 +4\u03bb 2 , and \u2207g as defined in Definition 2. Then for T := ln(2/\u03b5 2 )\n\n\nLemma 2.5 ([9, Lemmas 2.9 and 2.10]). Suppose we have SQ(M \u2020 ) for M \u2208 C n\u00d7d and Q(x) \u2208 C d . Denote y := M x and \u2206 := d i=1 M * ,i 2 x 2 i / y 2 2 . Then we can implement SQ(y) \u2208 C n with T (y) = O d 2 \u2206 log(1/\u03b4) \u00b7 T (M ) , where queries succeed with probability \u2265 1 \u2212 \u03b4. Namely, we can: (a) query for entries with complexity O(d \u00b7 T (M )); (b) sample from y with running time T sample (y) satisfying E[T sample (y)] = O d 2 \u2206 \u00b7 T (M ) and Pr[T sample (y) = O d 2 \u2206 \u00b7 log(1/\u03b4) ] \u2265 1 \u2212 \u03b4.\n\nPart 3 .FF\n3E[ \u2207g(x) \u2212 \u2207g(y) 2 2 ] = (A \u2020 A + \u03bbI)(x \u2212 y) 2 + V[\u2207g(x \u2212 y)]Proof.Part 1. First, we show \u2207g(x) is unbiased. E[\u2207g(x)] A r, * 2 A r, * 2 |A r,c j | 2 A r,c j x c j (A r, * ) \u2020 \u2212 A \u2020 b + \u03bbx by definition of \u2207g |A r,c 1 | 2 A r,c 1 x c 1 (A r, * ) \u2020 \u2212 A \u2020 b + \u03bbxsince the c i 's are i.\n\nPart 3 .\n3Finally, the expression for E[ \u2207g(x)\u2212\u2207g(y) 2 ] follows from the observation that \u2207g(x) \u2212 \u2207g(y) is simply the expression for \u2207g(x \u2212 y), taking b to be the zero vector.Lemma 2.6. Let \u2207g(v) |A r,c j | 2 A r,c j (A * ,c j ) \u2020 v e r \u2212 b + \u03bbv,where r, c 1 , . . . , c C are sampled according to the same distribution as \u2207g is (Definition 2.1).\n\n\nQuery(i, j), which takes i \u2208 [m] and j \u2208 [n] as input and outputs A i,j .j \u2208 [n] with probability |A i,j | 2 / A i, *  \n2 . \n\n3. 4. Norm(i), which takes i \u2208 [m] as input and outputs A i, *  . \n\n5. Norm(), which outputs A F . \n\n\n\n\nDefinition 2.1. We define \u2207g(x) to be the random function resulting from the following process. Draw r \u2208 [m] from the distribution that is r with probability draw c 1 , . . . , c C i.i.d. from the distribution that is c with probability |Ar,c| 2Ar, *  2 \nA 2 \n\nF \n\n, and then \n\n\nWhen we claim that we can call Norm() on output vectors, we will mean that we can output a constant approximation: a number in [0.9 v , 1.1 v ].\nNote that for \u03bb = 0 the function f might not be strictly convex and so the minimizer is not necessarily unique. Nevertheless, on the image of A \u2020 , f is strictly convex. Since we will search a solution within the image of A \u2020 , we implicitly restrict f to this subspace, thus we can assume without loss of generality that f is strictly convex.\nTo our knowledge, prior versions of[31] produced algorithms in this setting, but the current version only uses the standard QRAM data structure.\nAccepted in Quantum 2022-06-20, click title to verify. Published under CC-BY 4.0.\nWe obtain the constants in Algorithm 1 by using the error parameter\u03b5 := \u03b5/2 to satisfy x \u2212 x * \u2264 2\u03b5 x * = \u03b5 x * .\n\nQuantum Computing in the NISQ era and beyond. John Preskill, 10.22331/q-2018-08-06-79arXiv:1801.00862279John Preskill. \"Quantum Computing in the NISQ era and beyond\". Quantum 2, 79 (2018). arXiv:1801.00862.\n\nEquation solving by simulation. M Andrew, Childs, 10.1038/nphys1473Nature Physics. 5Andrew M Childs. \"Equation solving by simulation\". Nature Physics 5, 861- 861 (2009).\n\nRead the fine print. Scott Aaronson, 10.1038/nphys3272Nature Physics. 11Scott Aaronson. \"Read the fine print\". Nature Physics 11, 291-293 (2015).\n\nQuantum machine learning. Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, Seth Lloyd, 10.1038/nature23474arXiv:1611.09347Nature. 549Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. \"Quantum machine learning\". Nature 549, 195-202 (2017). arXiv:1611.09347.\n\nQuantum algorithm for linear systems of equations. Aram W Harrow, Avinatan Hassidim, Seth Lloyd, 10.1103/PhysRevLett.103.150502arXiv:0811.3171Physical Review Letters. 103150502Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. \"Quantum algorithm for linear systems of equations\". Physical Review Letters 103, 150502 (2009). arXiv:0811.3171.\n\nQuantum singular value transformation and beyond: Exponential improvements for quantum matrix arithmetics. Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, Nathan Wiebe, 10.1145/3313276.3316366arXiv:1806.01838Proceedings of the 51st ACM Symposium on the Theory of Computing (STOC). the 51st ACM Symposium on the Theory of Computing (STOC)Pages 193-204.Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, and Nathan Wiebe. \"Quantum singular value transformation and beyond: Exponential improvements for quantum matrix arithmetics\". In Proceedings of the 51st ACM Symposium on the Theory of Computing (STOC). Pages 193-204. (2019). arXiv:1806.01838.\n\nQuantum random access memory. Vittorio Giovannetti, Seth Lloyd, Lorenzo Maccone, 10.1103/PhysRevLett.100.160501arXiv:0708.1879Physical Review Letters. 100160501Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. \"Quantum random access memory\". Physical Review Letters 100, 160501 (2008). arXiv:0708.1879.\n\nQuantum algorithms for linear algebra and machine learning. Anupam Prakash, University of California at Berkeley.PhD thesisAnupam Prakash. \"Quantum algorithms for linear algebra and machine learning\". PhD thesis. University of California at Berkeley. (2014). url: www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-211.pdf.\n\nSampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. Nai-Hui Chia, Andr\u00e1s Gily\u00e9n, Tongyang Li, Han-Hsuan Lin, Ewin Tang, Chunhao Wang, 10.1145/3357713.3384314arXiv:1910.06151Proceedings of the 52nd ACM Symposium on the Theory of Computing (STOC). the 52nd ACM Symposium on the Theory of Computing (STOC)Nai-Hui Chia, Andr\u00e1s Gily\u00e9n, Tongyang Li, Han-Hsuan Lin, Ewin Tang, and Chunhao Wang. \"Sampling-based sublinear low-rank matrix arithmetic framework for dequan- tizing quantum machine learning\". In Proceedings of the 52nd ACM Symposium on the Theory of Computing (STOC). Page 387-400. (2020). arXiv:1910.06151.\n\nQuantum recommendation systems. Iordanis Kerenidis, Anupam Prakash, 10.4230/LIPIcs.ITCS.2017.49arXiv:1603.08675Proceedings of the 8th Innovations in Theoretical Computer Science Conference (ITCS). the 8th Innovations in Theoretical Computer Science Conference (ITCS)49Iordanis Kerenidis and Anupam Prakash. \"Quantum recommendation systems\". In Proceedings of the 8th Innovations in Theoretical Computer Science Conference (ITCS). Pages 49:1-49:21. (2017). arXiv:1603.08675.\n\nA quantum-inspired classical algorithm for recommendation systems. Ewin Tang, 10.1145/3313276.3316310arXiv:1807.04271Proceedings of the 51st ACM Symposium on the Theory of Computing (STOC). Pages 217-228. the 51st ACM Symposium on the Theory of Computing (STOC). Pages 217-228Ewin Tang. \"A quantum-inspired classical algorithm for recommendation systems\". In Proceedings of the 51st ACM Symposium on the Theory of Computing (STOC). Pages 217-228. (2019). arXiv:1807.04271.\n\nqmeans: A quantum algorithm for unsupervised machine learning. Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, Anupam Prakash, arXiv:1812.03584Advances in Neural Information Processing Systems. 32Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, and Anupam Prakash. \"q- means: A quantum algorithm for unsupervised machine learning\". In Advances in Neural Information Processing Systems. Volume 32. (2019). arXiv:1812.03584.\n\nQuantum gradient descent for linear systems and least squares. Iordanis Kerenidis, Anupam Prakash, 10.1103/PhysRevA.101.022316arXiv:1704.04992Physical Review A. 10122316Iordanis Kerenidis and Anupam Prakash. \"Quantum gradient descent for linear sys- tems and least squares\". Physical Review A 101, 022316 (2020). arXiv:1704.04992.\n\nQuantum linear systems algorithms: a primer. Danial Dervovic, Mark Herbster, Peter Mountney, Simone Severini, Na\u00efri Usher, Leonard Wossnig, arXiv:1802.08227Danial Dervovic, Mark Herbster, Peter Mountney, Simone Severini, Na\u00efri Usher, and Leonard Wossnig. \"Quantum linear systems algorithms: a primer\" (2018). arXiv:1802.08227.\n\nQuantum linear system algorithm for dense matrices. Leonard Wossnig, Zhikuan Zhao, Anupam Prakash, 10.1103/PhysRevLett.120.050502arXiv:1704.06174Physical Review Letters. 12050502Leonard Wossnig, Zhikuan Zhao, and Anupam Prakash. \"Quantum linear sys- tem algorithm for dense matrices\". Physical Review Letters 120, 050502 (2018). arXiv:1704.06174.\n\nQuantum support vector machine for big data classification. Patrick Rebentrost, Masoud Mohseni, Seth Lloyd, 10.1103/PhysRevLett.113.130503arXiv:1307.0471Physical Review Letters. 113130503Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. \"Quantum support vector machine for big data classification\". Physical Review Letters 113, 130503 (2014). arXiv:1307.0471.\n\nThe power of blockencoded matrix powers: Improved regression techniques via faster Hamiltonian simulation. Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, Stacey Jeffery, 10.4230/LIPIcs.ICALP.2019.33arXiv:1804.01973Proceedings of the 46th International Colloquium on Automata, Languages, and Programming (ICALP). the 46th International Colloquium on Automata, Languages, and Programming (ICALP)33Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, and Stacey Jeffery. \"The power of block- encoded matrix powers: Improved regression techniques via faster Hamiltonian simu- lation\". In Proceedings of the 46th International Colloquium on Automata, Languages, and Programming (ICALP). Pages 33:1-33:14. (2019). arXiv:1804.01973.\n\nQuantum-inspired algorithms for solving low-rank linear equation systems with logarithmic dependence on the dimension. Nai-Hui Chia, Andr\u00e1s Gily\u00e9n, Han-Hsuan Lin, Seth Lloyd, Ewin Tang, Chunhao Wang, 10.4230/LIPIcs.ISAAC.2020.47arXiv:1811.04852and1811.04909Proceedings of the 31st International Symposium on Algorithms and Computation (ISAAC). the 31st International Symposium on Algorithms and Computation (ISAAC)47mergedNai-Hui Chia, Andr\u00e1s Gily\u00e9n, Han-Hsuan Lin, Seth Lloyd, Ewin Tang, and Chunhao Wang. \"Quantum-inspired algorithms for solving low-rank linear equation systems with logarithmic dependence on the dimension\". In Proceedings of the 31st Inter- national Symposium on Algorithms and Computation (ISAAC). Pages 47:1-47:17. (2020). arXiv:1811.04852 and 1811.04909 (merged).\n\nQuantum-inspired algorithms in practice. Juan Miguel Arrazola, Alain Delgado, Seth Bhaskar Roy Bardhan, Lloyd, 10.22331/q-2020-08-13-307arXiv:1905.104154307Juan Miguel Arrazola, Alain Delgado, Bhaskar Roy Bardhan, and Seth Lloyd. \"Quantum-inspired algorithms in practice\". Quantum 4, 307 (2020). arXiv:1905.10415.\n\nQuantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions. Ewin Tang, 10.1103/PhysRevLett.127.060503arXiv:1811.00414Physical Review Letters. 12760503Ewin Tang. \"Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions\". Physical Review Letters 127, 060503 (2021). arXiv:1811.00414.\n\nQuantum machine learning: a classical perspective. Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano Pontil, Andrea Rocchetto, Simone Severini, Leonard Wossnig, 10.1098/rspa.2017.0551arXiv:1707.08561Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 47420170551Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano Pontil, An- drea Rocchetto, Simone Severini, and Leonard Wossnig. \"Quantum machine learning: a classical perspective\". Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 474, 20170551 (2018). arXiv:1707.08561.\n\nOn the robustness of bucket brigade quantum RAM. Srinivasan Arunachalam, Vlad Gheorghiu, Tomas Jochym-O&apos; Connor, Michele Mosca, Priyaa Varshinee Srinivasan, 10.1088/1367-2630/17/12/123010arXiv:1502.03450New Journal of Physics. 17123010Srinivasan Arunachalam, Vlad Gheorghiu, Tomas Jochym-O'Connor, Michele Mosca, and Priyaa Varshinee Srinivasan. \"On the robustness of bucket brigade quantum RAM\". New Journal of Physics 17, 123010 (2015). arXiv:1502.03450.\n\nExploiting numerical sparsity for efficient learning: Faster eigenvector computation and regression. Neha Gupta, Aaron Sidford, arXiv:1811.10866Advances in Neural Information Processing Systems. Pages 5269-5278. Neha Gupta and Aaron Sidford. \"Exploiting numerical sparsity for efficient learning: Faster eigenvector computation and regression\". In Advances in Neural Information Processing Systems. Pages 5269-5278. (2018). arXiv:1811.10866.\n\nCoordinate methods for matrix games. Yair Carmon, Yujia Jin, Aaron Sidford, Kevin Tian, 10.1109/focs46700.2020.00035arXiv:2009.084472020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS). Pages 283-293. IEEEYair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian. \"Coordinate methods for matrix games\". In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS). Pages 283-293. IEEE (2020). arXiv:2009.08447.\n\nConvex optimization: Algorithms and complexity. S\u00e9bastien Bubeck, 10.1561/2200000050arXiv:1405.4980Foundations and Trends in Machine Learning. 8S\u00e9bastien Bubeck. \"Convex optimization: Algorithms and complexity\". Foundations and Trends in Machine Learning 8, 231-357 (2015). arXiv:1405.4980.\n\nUn-regularizing: Approximate proximal point and faster stochastic algorithms for empirical risk minimization. Roy Frostig, Rong Ge, Sham Kakade, Aaron Sidford, arXiv:1506.07512International Conference on Machine Learning. Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. \"Un-regularizing: Approx- imate proximal point and faster stochastic algorithms for empirical risk minimiza- tion\". In International Conference on Machine Learning. Pages 2540-2548. (2015). arXiv:1506.07512.\n\nNon-asymptotic analysis of stochastic approximation algorithms for machine learning. Francis Bach, Eric Moulines, Advances in Neural Information Processing Systems. Pages 451-459. Francis Bach and Eric Moulines. \"Non-asymptotic analysis of stochastic approximation algorithms for machine learning\". In Advances in Neural Information Processing Sys- tems. Pages 451-459. (2011). url: http://papers.nips.cc/paper/4316-non-asymptotic- analysis-of-stochastic-approximation-algorithms-for-machine-learning.pdf.\n\nQuantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension. Andr\u00e1s Gily\u00e9n, Seth Lloyd, Ewin Tang, arXiv:1811.04909Andr\u00e1s Gily\u00e9n, Seth Lloyd, and Ewin Tang. \"Quantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension\" (2018). arXiv:1811.04909.\n\nQuantum-inspired sublinear classical algorithms for solving low-rank linear systems. Nai-Hui Chia, Han-Hsuan Lin, Chunhao Wang, arXiv:1811.04852Nai-Hui Chia, Han-Hsuan Lin, and Chunhao Wang. \"Quantum-inspired sublinear classical algorithms for solving low-rank linear systems\" (2018). arXiv:1811.04852.\n\nSketching as a tool for numerical linear algebra. David P Woodruff, 10.1561/0400000060Foundations and Trends in Theoretical Computer Science. 10David P. Woodruff. \"Sketching as a tool for numerical linear algebra\". Foundations and Trends in Theoretical Computer Science 10, 1-157 (2014).\n\nQuantum-inspired algorithms from randomized numerical linear algebra. Nadiia Chepurko, Kenneth L Clarkson, Lior Horesh, Honghao Lin, David P Woodruff, arXiv:2011.04125Nadiia Chepurko, Kenneth L. Clarkson, Lior Horesh, Honghao Lin, and David P. Woodruff. \"Quantum-inspired algorithms from randomized numerical linear alge- bra\" (2020). arXiv:2011.04125.\n\nFaster quantum-inspired algorithms for solving linear systems. Changpeng Shao, Ashley Montanaro, arXiv:2103.10309Changpeng Shao and Ashley Montanaro. \"Faster quantum-inspired algorithms for solving linear systems\" (2021). arXiv:2103.10309.\n\nA randomized Kaczmarz algorithm with exponential convergence. Thomas Strohmer, Roman Vershynin, 10.1007/s00041-008-9030-4arXiv:math/0702226Journal of Fourier Analysis and Applications. 15Thomas Strohmer and Roman Vershynin. \"A randomized Kaczmarz algorithm with exponential convergence\". Journal of Fourier Analysis and Applications 15, 262- 278 (2008). arXiv:math/0702226.\n\nStochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm. Deanna Needell, Nathan Srebro, Rachel Ward, 10.1007/s10107-015-0864-7arXiv:1310.5715Mathematical Programming. 155Deanna Needell, Nathan Srebro, and Rachel Ward. \"Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm\". Mathematical Pro- gramming 155, 549-573 (2015). arXiv:1310.5715.\n\nFast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix. Petros Drineas, Ravi Kannan, Michael W Mahoney, 10.1137/S0097539704442696SIAM Journal on Computing. 36Petros Drineas, Ravi Kannan, and Michael W Mahoney. \"Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix\". SIAM Journal on Computing 36, 158-183 (2006).\n", "annotations": {"author": "[{\"end\":129,\"start\":75},{\"end\":157,\"start\":130},{\"end\":195,\"start\":158}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":82},{\"end\":139,\"start\":135},{\"end\":167,\"start\":163}]", "author_first_name": "[{\"end\":81,\"start\":75},{\"end\":134,\"start\":130},{\"end\":162,\"start\":158}]", "author_affiliation": "[{\"end\":128,\"start\":90},{\"end\":156,\"start\":141},{\"end\":194,\"start\":169}]", "title": "[{\"end\":61,\"start\":1},{\"end\":256,\"start\":196}]", "venue": null, "abstract": "[{\"end\":1904,\"start\":544}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2050,\"start\":2047},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2109,\"start\":2106},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2111,\"start\":2109},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2113,\"start\":2111},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2246,\"start\":2243},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2298,\"start\":2295},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2392,\"start\":2389},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2394,\"start\":2392},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2764,\"start\":2761},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2766,\"start\":2764},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2924,\"start\":2921},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3301,\"start\":3297},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3466,\"start\":3463},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3511,\"start\":3507},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3515,\"start\":3514},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3669,\"start\":3666},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3672,\"start\":3669},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3675,\"start\":3672},{\"end\":3915,\"start\":3912},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4008,\"start\":4005},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4116,\"start\":4113},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4119,\"start\":4116},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4201,\"start\":4197},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4204,\"start\":4201},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4742,\"start\":4738},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5173,\"start\":5169},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5332,\"start\":5329},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5426,\"start\":5425},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5590,\"start\":5589},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5746,\"start\":5745},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5798,\"start\":5797},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6542,\"start\":6538},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6893,\"start\":6889},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6895,\"start\":6893},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7087,\"start\":7083},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7090,\"start\":7087},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7092,\"start\":7090},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7094,\"start\":7092},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7096,\"start\":7094},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7099,\"start\":7096},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7181,\"start\":7177},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7184,\"start\":7183},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7769,\"start\":7768},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7798,\"start\":7797},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8770,\"start\":8769},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8935,\"start\":8932},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9086,\"start\":9082},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9426,\"start\":9423},{\"end\":9438,\"start\":9426},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9515,\"start\":9511},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10293,\"start\":10290},{\"end\":10305,\"start\":10293},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11914,\"start\":11911},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12654,\"start\":12651},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15649,\"start\":15648},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15874,\"start\":15870},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15877,\"start\":15874},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16319,\"start\":16315},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17822,\"start\":17818},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17990,\"start\":17986},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19171,\"start\":19167},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19205,\"start\":19201},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19345,\"start\":19342},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19384,\"start\":19380},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19765,\"start\":19761},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19860,\"start\":19856},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19883,\"start\":19879},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21307,\"start\":21303},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22007,\"start\":22003},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23769,\"start\":23765},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24936,\"start\":24932},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25131,\"start\":25127},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25329,\"start\":25325},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25485,\"start\":25481},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25541,\"start\":25537},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25631,\"start\":25627},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26013,\"start\":26010},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26233,\"start\":26230},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27305,\"start\":27303},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27980,\"start\":27976},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30838,\"start\":30834},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31243,\"start\":31241},{\"end\":31269,\"start\":31258},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":34947,\"start\":34944},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":35854,\"start\":35850},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41385,\"start\":41382},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":42590,\"start\":42586},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":46283,\"start\":46281},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":49051,\"start\":49047}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46100,\"start\":45984},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46284,\"start\":46101},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46618,\"start\":46285},{\"attributes\":{\"id\":\"fig_3\"},\"end\":46875,\"start\":46619},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47366,\"start\":46876},{\"attributes\":{\"id\":\"fig_5\"},\"end\":47662,\"start\":47367},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48011,\"start\":47663},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":48241,\"start\":48012},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":48522,\"start\":48242}]", "paragraph": "[{\"end\":3676,\"start\":1920},{\"end\":4982,\"start\":3690},{\"end\":5542,\"start\":4984},{\"end\":5547,\"start\":5544},{\"end\":5849,\"start\":5549},{\"end\":5916,\"start\":5851},{\"end\":6752,\"start\":5936},{\"end\":7634,\"start\":6766},{\"end\":7958,\"start\":7708},{\"end\":8379,\"start\":7960},{\"end\":9167,\"start\":8460},{\"end\":9634,\"start\":9253},{\"end\":10607,\"start\":9636},{\"end\":10951,\"start\":10609},{\"end\":11081,\"start\":10953},{\"end\":11243,\"start\":11106},{\"end\":11522,\"start\":11281},{\"end\":11758,\"start\":11524},{\"end\":11915,\"start\":11774},{\"end\":12063,\"start\":11917},{\"end\":12244,\"start\":12155},{\"end\":12483,\"start\":12386},{\"end\":12796,\"start\":12485},{\"end\":13108,\"start\":13061},{\"end\":13216,\"start\":13207},{\"end\":13249,\"start\":13223},{\"end\":13408,\"start\":13368},{\"end\":13515,\"start\":13502},{\"end\":13684,\"start\":13582},{\"end\":14303,\"start\":14177},{\"end\":14593,\"start\":14357},{\"end\":14609,\"start\":14595},{\"end\":14820,\"start\":14611},{\"end\":14999,\"start\":14835},{\"end\":15356,\"start\":15213},{\"end\":15532,\"start\":15482},{\"end\":15736,\"start\":15565},{\"end\":16412,\"start\":15777},{\"end\":16602,\"start\":16414},{\"end\":16881,\"start\":16604},{\"end\":17088,\"start\":16883},{\"end\":17117,\"start\":17090},{\"end\":17354,\"start\":17199},{\"end\":17513,\"start\":17356},{\"end\":17717,\"start\":17571},{\"end\":17946,\"start\":17719},{\"end\":18168,\"start\":17948},{\"end\":18322,\"start\":18170},{\"end\":18677,\"start\":18354},{\"end\":18972,\"start\":18679},{\"end\":19249,\"start\":18987},{\"end\":19745,\"start\":19335},{\"end\":20193,\"start\":19747},{\"end\":20554,\"start\":20245},{\"end\":20958,\"start\":20556},{\"end\":21084,\"start\":20973},{\"end\":23451,\"start\":21131},{\"end\":24657,\"start\":23453},{\"end\":25285,\"start\":24659},{\"end\":26406,\"start\":25287},{\"end\":27002,\"start\":26417},{\"end\":28062,\"start\":27004},{\"end\":28226,\"start\":28101},{\"end\":28541,\"start\":28228},{\"end\":28731,\"start\":28640},{\"end\":29106,\"start\":28807},{\"end\":29367,\"start\":29147},{\"end\":29571,\"start\":29417},{\"end\":29757,\"start\":29586},{\"end\":30208,\"start\":29941},{\"end\":30409,\"start\":30228},{\"end\":30587,\"start\":30521},{\"end\":30856,\"start\":30612},{\"end\":31329,\"start\":31167},{\"end\":31934,\"start\":31829},{\"end\":32054,\"start\":31965},{\"end\":32407,\"start\":32238},{\"end\":32534,\"start\":32463},{\"end\":32776,\"start\":32575},{\"end\":32868,\"start\":32778},{\"end\":33112,\"start\":33051},{\"end\":33454,\"start\":33161},{\"end\":33735,\"start\":33559},{\"end\":34208,\"start\":33885},{\"end\":34691,\"start\":34332},{\"end\":34953,\"start\":34693},{\"end\":35269,\"start\":35184},{\"end\":35627,\"start\":35427},{\"end\":36091,\"start\":35629},{\"end\":36221,\"start\":36188},{\"end\":36442,\"start\":36279},{\"end\":36573,\"start\":36572},{\"end\":36661,\"start\":36575},{\"end\":36704,\"start\":36672},{\"end\":36905,\"start\":36801},{\"end\":37070,\"start\":36907},{\"end\":37284,\"start\":37184},{\"end\":37334,\"start\":37286},{\"end\":37490,\"start\":37428},{\"end\":37975,\"start\":37939},{\"end\":38114,\"start\":38043},{\"end\":38254,\"start\":38176},{\"end\":38545,\"start\":38363},{\"end\":38707,\"start\":38658},{\"end\":38913,\"start\":38866},{\"end\":38996,\"start\":38945},{\"end\":39138,\"start\":38998},{\"end\":39257,\"start\":39154},{\"end\":39728,\"start\":39281},{\"end\":40077,\"start\":39855},{\"end\":40353,\"start\":40183},{\"end\":40577,\"start\":40431},{\"end\":40735,\"start\":40684},{\"end\":40841,\"start\":40746},{\"end\":41232,\"start\":40895},{\"end\":41539,\"start\":41281},{\"end\":41842,\"start\":41698},{\"end\":42084,\"start\":41947},{\"end\":42717,\"start\":42280},{\"end\":43162,\"start\":42961},{\"end\":43317,\"start\":43239},{\"end\":43562,\"start\":43332},{\"end\":43877,\"start\":43705},{\"end\":44150,\"start\":44093},{\"end\":44740,\"start\":44610},{\"end\":45289,\"start\":45283},{\"end\":45331,\"start\":45301},{\"end\":45681,\"start\":45598}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3689,\"start\":3677},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5935,\"start\":5917},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7707,\"start\":7635},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8459,\"start\":8380},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11105,\"start\":11082},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11280,\"start\":11244},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12098,\"start\":12064},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12154,\"start\":12098},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12385,\"start\":12326},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13060,\"start\":12797},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13206,\"start\":13109},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13367,\"start\":13250},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13501,\"start\":13409},{\"attributes\":{\"id\":\"formula_13\"},\"end\":13581,\"start\":13516},{\"attributes\":{\"id\":\"formula_14\"},\"end\":14176,\"start\":13735},{\"attributes\":{\"id\":\"formula_15\"},\"end\":14356,\"start\":14304},{\"attributes\":{\"id\":\"formula_16\"},\"end\":14834,\"start\":14821},{\"attributes\":{\"id\":\"formula_17\"},\"end\":15212,\"start\":15000},{\"attributes\":{\"id\":\"formula_18\"},\"end\":15481,\"start\":15357},{\"attributes\":{\"id\":\"formula_19\"},\"end\":15564,\"start\":15533},{\"attributes\":{\"id\":\"formula_20\"},\"end\":15776,\"start\":15737},{\"attributes\":{\"id\":\"formula_21\"},\"end\":17198,\"start\":17118},{\"attributes\":{\"id\":\"formula_22\"},\"end\":17570,\"start\":17514},{\"attributes\":{\"id\":\"formula_23\"},\"end\":18353,\"start\":18323},{\"attributes\":{\"id\":\"formula_24\"},\"end\":19315,\"start\":19250},{\"attributes\":{\"id\":\"formula_25\"},\"end\":19334,\"start\":19315},{\"attributes\":{\"id\":\"formula_26\"},\"end\":20244,\"start\":20194},{\"attributes\":{\"id\":\"formula_27\"},\"end\":20972,\"start\":20959},{\"attributes\":{\"id\":\"formula_28\"},\"end\":21130,\"start\":21085},{\"attributes\":{\"id\":\"formula_29\"},\"end\":28100,\"start\":28063},{\"attributes\":{\"id\":\"formula_30\"},\"end\":28639,\"start\":28542},{\"attributes\":{\"id\":\"formula_31\"},\"end\":28806,\"start\":28732},{\"attributes\":{\"id\":\"formula_32\"},\"end\":29146,\"start\":29107},{\"attributes\":{\"id\":\"formula_33\"},\"end\":29940,\"start\":29758},{\"attributes\":{\"id\":\"formula_34\"},\"end\":30227,\"start\":30209},{\"attributes\":{\"id\":\"formula_35\"},\"end\":30520,\"start\":30447},{\"attributes\":{\"id\":\"formula_36\"},\"end\":30611,\"start\":30588},{\"attributes\":{\"id\":\"formula_37\"},\"end\":31166,\"start\":30857},{\"attributes\":{\"id\":\"formula_38\"},\"end\":31377,\"start\":31330},{\"attributes\":{\"id\":\"formula_39\"},\"end\":31828,\"start\":31377},{\"attributes\":{\"id\":\"formula_40\"},\"end\":31964,\"start\":31935},{\"attributes\":{\"id\":\"formula_41\"},\"end\":32237,\"start\":32055},{\"attributes\":{\"id\":\"formula_42\"},\"end\":32462,\"start\":32408},{\"attributes\":{\"id\":\"formula_43\"},\"end\":32574,\"start\":32535},{\"attributes\":{\"id\":\"formula_44\"},\"end\":33050,\"start\":32869},{\"attributes\":{\"id\":\"formula_45\"},\"end\":33558,\"start\":33455},{\"attributes\":{\"id\":\"formula_46\"},\"end\":33884,\"start\":33736},{\"attributes\":{\"id\":\"formula_47\"},\"end\":34331,\"start\":34209},{\"attributes\":{\"id\":\"formula_48\"},\"end\":35183,\"start\":34954},{\"attributes\":{\"id\":\"formula_49\"},\"end\":35426,\"start\":35270},{\"attributes\":{\"id\":\"formula_50\"},\"end\":36156,\"start\":36092},{\"attributes\":{\"id\":\"formula_51\"},\"end\":36187,\"start\":36156},{\"attributes\":{\"id\":\"formula_52\"},\"end\":36278,\"start\":36222},{\"attributes\":{\"id\":\"formula_53\"},\"end\":36571,\"start\":36443},{\"attributes\":{\"id\":\"formula_54\"},\"end\":36671,\"start\":36662},{\"attributes\":{\"id\":\"formula_55\"},\"end\":36800,\"start\":36705},{\"attributes\":{\"id\":\"formula_56\"},\"end\":37183,\"start\":37071},{\"attributes\":{\"id\":\"formula_57\"},\"end\":37427,\"start\":37335},{\"attributes\":{\"id\":\"formula_58\"},\"end\":37938,\"start\":37491},{\"attributes\":{\"id\":\"formula_59\"},\"end\":38042,\"start\":37976},{\"attributes\":{\"id\":\"formula_60\"},\"end\":38175,\"start\":38115},{\"attributes\":{\"id\":\"formula_61\"},\"end\":38362,\"start\":38255},{\"attributes\":{\"id\":\"formula_62\"},\"end\":38657,\"start\":38546},{\"attributes\":{\"id\":\"formula_63\"},\"end\":38865,\"start\":38708},{\"attributes\":{\"id\":\"formula_64\"},\"end\":38944,\"start\":38914},{\"attributes\":{\"id\":\"formula_65\"},\"end\":39153,\"start\":39139},{\"attributes\":{\"id\":\"formula_66\"},\"end\":39280,\"start\":39258},{\"attributes\":{\"id\":\"formula_67\"},\"end\":39854,\"start\":39729},{\"attributes\":{\"id\":\"formula_68\"},\"end\":40182,\"start\":40078},{\"attributes\":{\"id\":\"formula_69\"},\"end\":40683,\"start\":40578},{\"attributes\":{\"id\":\"formula_70\"},\"end\":40745,\"start\":40736},{\"attributes\":{\"id\":\"formula_71\"},\"end\":40894,\"start\":40842},{\"attributes\":{\"id\":\"formula_72\"},\"end\":41280,\"start\":41233},{\"attributes\":{\"id\":\"formula_73\"},\"end\":41697,\"start\":41540},{\"attributes\":{\"id\":\"formula_74\"},\"end\":41946,\"start\":41843},{\"attributes\":{\"id\":\"formula_75\"},\"end\":42279,\"start\":42085},{\"attributes\":{\"id\":\"formula_76\"},\"end\":42960,\"start\":42718},{\"attributes\":{\"id\":\"formula_77\"},\"end\":43238,\"start\":43163},{\"attributes\":{\"id\":\"formula_78\"},\"end\":43331,\"start\":43318},{\"attributes\":{\"id\":\"formula_79\"},\"end\":43691,\"start\":43594},{\"attributes\":{\"id\":\"formula_80\"},\"end\":43990,\"start\":43878},{\"attributes\":{\"id\":\"formula_81\"},\"end\":44092,\"start\":43990},{\"attributes\":{\"id\":\"formula_82\"},\"end\":44609,\"start\":44151},{\"attributes\":{\"id\":\"formula_83\"},\"end\":45190,\"start\":44741},{\"attributes\":{\"id\":\"formula_84\"},\"end\":45282,\"start\":45190},{\"attributes\":{\"id\":\"formula_85\"},\"end\":45587,\"start\":45332},{\"attributes\":{\"id\":\"formula_86\"},\"end\":45984,\"start\":45682}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1918,\"start\":1906},{\"attributes\":{\"n\":\"1.1\"},\"end\":6764,\"start\":6755},{\"attributes\":{\"n\":\"2.\"},\"end\":9251,\"start\":9170},{\"attributes\":{\"n\":\"1.2\"},\"end\":11772,\"start\":11761},{\"end\":12325,\"start\":12247},{\"end\":13221,\"start\":13219},{\"end\":13734,\"start\":13687},{\"attributes\":{\"n\":\"1.3\"},\"end\":18985,\"start\":18975},{\"attributes\":{\"n\":\"2\"},\"end\":26415,\"start\":26409},{\"attributes\":{\"n\":\"2.1\"},\"end\":29415,\"start\":29370},{\"end\":29584,\"start\":29574},{\"end\":30446,\"start\":30412},{\"attributes\":{\"n\":\"2.2\"},\"end\":33159,\"start\":33115},{\"end\":40429,\"start\":40356},{\"end\":43593,\"start\":43565},{\"end\":43703,\"start\":43693},{\"end\":45299,\"start\":45292},{\"end\":45596,\"start\":45589},{\"end\":45996,\"start\":45985},{\"end\":46111,\"start\":46102},{\"end\":46303,\"start\":46286},{\"end\":46621,\"start\":46620},{\"end\":47378,\"start\":47368},{\"end\":47672,\"start\":47664}]", "table": "[{\"end\":48241,\"start\":48087},{\"end\":48522,\"start\":48489}]", "figure_caption": "[{\"end\":46100,\"start\":45998},{\"end\":46284,\"start\":46114},{\"end\":46618,\"start\":46305},{\"end\":46875,\"start\":46622},{\"end\":47366,\"start\":46878},{\"end\":47662,\"start\":47380},{\"end\":48011,\"start\":47674},{\"end\":48087,\"start\":48014},{\"end\":48489,\"start\":48244}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37283,\"start\":37267}]", "bib_author_first_name": "[{\"end\":49404,\"start\":49400},{\"end\":49595,\"start\":49594},{\"end\":49759,\"start\":49754},{\"end\":49911,\"start\":49906},{\"end\":49927,\"start\":49922},{\"end\":49942,\"start\":49936},{\"end\":49960,\"start\":49953},{\"end\":49979,\"start\":49973},{\"end\":49991,\"start\":49987},{\"end\":50272,\"start\":50268},{\"end\":50274,\"start\":50273},{\"end\":50291,\"start\":50283},{\"end\":50306,\"start\":50302},{\"end\":50672,\"start\":50666},{\"end\":50685,\"start\":50681},{\"end\":50695,\"start\":50690},{\"end\":50711,\"start\":50705},{\"end\":51223,\"start\":51215},{\"end\":51241,\"start\":51237},{\"end\":51256,\"start\":51249},{\"end\":51560,\"start\":51554},{\"end\":51932,\"start\":51925},{\"end\":51945,\"start\":51939},{\"end\":51962,\"start\":51954},{\"end\":51976,\"start\":51967},{\"end\":51986,\"start\":51982},{\"end\":52000,\"start\":51993},{\"end\":52527,\"start\":52519},{\"end\":52545,\"start\":52539},{\"end\":53033,\"start\":53029},{\"end\":53507,\"start\":53499},{\"end\":53524,\"start\":53519},{\"end\":53544,\"start\":53534},{\"end\":53559,\"start\":53553},{\"end\":53942,\"start\":53934},{\"end\":53960,\"start\":53954},{\"end\":54254,\"start\":54248},{\"end\":54269,\"start\":54265},{\"end\":54285,\"start\":54280},{\"end\":54302,\"start\":54296},{\"end\":54318,\"start\":54313},{\"end\":54333,\"start\":54326},{\"end\":54590,\"start\":54583},{\"end\":54607,\"start\":54600},{\"end\":54620,\"start\":54614},{\"end\":54946,\"start\":54939},{\"end\":54965,\"start\":54959},{\"end\":54979,\"start\":54975},{\"end\":55358,\"start\":55349},{\"end\":55378,\"start\":55372},{\"end\":55393,\"start\":55387},{\"end\":56071,\"start\":56064},{\"end\":56084,\"start\":56078},{\"end\":56102,\"start\":56093},{\"end\":56112,\"start\":56108},{\"end\":56124,\"start\":56120},{\"end\":56138,\"start\":56131},{\"end\":56779,\"start\":56775},{\"end\":56802,\"start\":56797},{\"end\":56816,\"start\":56812},{\"end\":57173,\"start\":57169},{\"end\":57511,\"start\":57506},{\"end\":57527,\"start\":57523},{\"end\":57548,\"start\":57538},{\"end\":57555,\"start\":57549},{\"end\":57577,\"start\":57565},{\"end\":57592,\"start\":57586},{\"end\":57610,\"start\":57604},{\"end\":57628,\"start\":57621},{\"end\":58147,\"start\":58137},{\"end\":58165,\"start\":58161},{\"end\":58197,\"start\":58177},{\"end\":58213,\"start\":58206},{\"end\":58237,\"start\":58221},{\"end\":58656,\"start\":58652},{\"end\":58669,\"start\":58664},{\"end\":59035,\"start\":59031},{\"end\":59049,\"start\":59044},{\"end\":59060,\"start\":59055},{\"end\":59075,\"start\":59070},{\"end\":59496,\"start\":59487},{\"end\":59844,\"start\":59841},{\"end\":59858,\"start\":59854},{\"end\":59867,\"start\":59863},{\"end\":59881,\"start\":59876},{\"end\":60308,\"start\":60301},{\"end\":60319,\"start\":60315},{\"end\":60823,\"start\":60817},{\"end\":60836,\"start\":60832},{\"end\":60848,\"start\":60844},{\"end\":61127,\"start\":61120},{\"end\":61143,\"start\":61134},{\"end\":61156,\"start\":61149},{\"end\":61394,\"start\":61389},{\"end\":61396,\"start\":61395},{\"end\":61704,\"start\":61698},{\"end\":61722,\"start\":61715},{\"end\":61724,\"start\":61723},{\"end\":61739,\"start\":61735},{\"end\":61755,\"start\":61748},{\"end\":61766,\"start\":61761},{\"end\":61768,\"start\":61767},{\"end\":62054,\"start\":62045},{\"end\":62067,\"start\":62061},{\"end\":62291,\"start\":62285},{\"end\":62307,\"start\":62302},{\"end\":62691,\"start\":62685},{\"end\":62707,\"start\":62701},{\"end\":62722,\"start\":62716},{\"end\":63100,\"start\":63094},{\"end\":63114,\"start\":63110},{\"end\":63130,\"start\":63123},{\"end\":63132,\"start\":63131}]", "bib_author_last_name": "[{\"end\":49413,\"start\":49405},{\"end\":49602,\"start\":49596},{\"end\":49610,\"start\":49604},{\"end\":49768,\"start\":49760},{\"end\":49920,\"start\":49912},{\"end\":49934,\"start\":49928},{\"end\":49951,\"start\":49943},{\"end\":49971,\"start\":49961},{\"end\":49985,\"start\":49980},{\"end\":49997,\"start\":49992},{\"end\":50281,\"start\":50275},{\"end\":50300,\"start\":50292},{\"end\":50312,\"start\":50307},{\"end\":50679,\"start\":50673},{\"end\":50688,\"start\":50686},{\"end\":50703,\"start\":50696},{\"end\":50717,\"start\":50712},{\"end\":51235,\"start\":51224},{\"end\":51247,\"start\":51242},{\"end\":51264,\"start\":51257},{\"end\":51568,\"start\":51561},{\"end\":51937,\"start\":51933},{\"end\":51952,\"start\":51946},{\"end\":51965,\"start\":51963},{\"end\":51980,\"start\":51977},{\"end\":51991,\"start\":51987},{\"end\":52005,\"start\":52001},{\"end\":52537,\"start\":52528},{\"end\":52553,\"start\":52546},{\"end\":53038,\"start\":53034},{\"end\":53517,\"start\":53508},{\"end\":53532,\"start\":53525},{\"end\":53551,\"start\":53545},{\"end\":53567,\"start\":53560},{\"end\":53952,\"start\":53943},{\"end\":53968,\"start\":53961},{\"end\":54263,\"start\":54255},{\"end\":54278,\"start\":54270},{\"end\":54294,\"start\":54286},{\"end\":54311,\"start\":54303},{\"end\":54324,\"start\":54319},{\"end\":54341,\"start\":54334},{\"end\":54598,\"start\":54591},{\"end\":54612,\"start\":54608},{\"end\":54628,\"start\":54621},{\"end\":54957,\"start\":54947},{\"end\":54973,\"start\":54966},{\"end\":54985,\"start\":54980},{\"end\":55370,\"start\":55359},{\"end\":55385,\"start\":55379},{\"end\":55401,\"start\":55394},{\"end\":56076,\"start\":56072},{\"end\":56091,\"start\":56085},{\"end\":56106,\"start\":56103},{\"end\":56118,\"start\":56113},{\"end\":56129,\"start\":56125},{\"end\":56143,\"start\":56139},{\"end\":56795,\"start\":56780},{\"end\":56810,\"start\":56803},{\"end\":56836,\"start\":56817},{\"end\":56843,\"start\":56838},{\"end\":57178,\"start\":57174},{\"end\":57521,\"start\":57512},{\"end\":57536,\"start\":57528},{\"end\":57563,\"start\":57556},{\"end\":57584,\"start\":57578},{\"end\":57602,\"start\":57593},{\"end\":57619,\"start\":57611},{\"end\":57636,\"start\":57629},{\"end\":58159,\"start\":58148},{\"end\":58175,\"start\":58166},{\"end\":58204,\"start\":58198},{\"end\":58219,\"start\":58214},{\"end\":58248,\"start\":58238},{\"end\":58662,\"start\":58657},{\"end\":58677,\"start\":58670},{\"end\":59042,\"start\":59036},{\"end\":59053,\"start\":59050},{\"end\":59068,\"start\":59061},{\"end\":59080,\"start\":59076},{\"end\":59503,\"start\":59497},{\"end\":59852,\"start\":59845},{\"end\":59861,\"start\":59859},{\"end\":59874,\"start\":59868},{\"end\":59889,\"start\":59882},{\"end\":60313,\"start\":60309},{\"end\":60328,\"start\":60320},{\"end\":60830,\"start\":60824},{\"end\":60842,\"start\":60837},{\"end\":60853,\"start\":60849},{\"end\":61132,\"start\":61128},{\"end\":61147,\"start\":61144},{\"end\":61161,\"start\":61157},{\"end\":61405,\"start\":61397},{\"end\":61713,\"start\":61705},{\"end\":61733,\"start\":61725},{\"end\":61746,\"start\":61740},{\"end\":61759,\"start\":61756},{\"end\":61777,\"start\":61769},{\"end\":62059,\"start\":62055},{\"end\":62077,\"start\":62068},{\"end\":62300,\"start\":62292},{\"end\":62317,\"start\":62308},{\"end\":62699,\"start\":62692},{\"end\":62714,\"start\":62708},{\"end\":62727,\"start\":62723},{\"end\":63108,\"start\":63101},{\"end\":63121,\"start\":63115},{\"end\":63140,\"start\":63133}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.22331/q-2018-08-06-79\",\"id\":\"b0\"},\"end\":49560,\"start\":49354},{\"attributes\":{\"id\":\"b1\"},\"end\":49731,\"start\":49562},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2246294},\"end\":49878,\"start\":49733},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":25062002},\"end\":50215,\"start\":49880},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5187993},\"end\":50557,\"start\":50217},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":46941335},\"end\":51183,\"start\":50559},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":570390},\"end\":51492,\"start\":51185},{\"attributes\":{\"id\":\"b7\"},\"end\":51818,\"start\":51494},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":204509632},\"end\":52485,\"start\":51820},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":579463},\"end\":52960,\"start\":52487},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":44036160},\"end\":53434,\"start\":52962},{\"attributes\":{\"id\":\"b11\"},\"end\":53869,\"start\":53436},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":119415623},\"end\":54201,\"start\":53871},{\"attributes\":{\"id\":\"b13\"},\"end\":54529,\"start\":54203},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3714239},\"end\":54877,\"start\":54531},{\"attributes\":{\"id\":\"b15\"},\"end\":55240,\"start\":54879},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4614529},\"end\":55943,\"start\":55242},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":227276230},\"end\":56732,\"start\":55945},{\"attributes\":{\"id\":\"b18\"},\"end\":57047,\"start\":56734},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":236956378},\"end\":57453,\"start\":57049},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3306944},\"end\":58086,\"start\":57455},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10910586},\"end\":58549,\"start\":58088},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":53773950},\"end\":58992,\"start\":58551},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":221761380},\"end\":59437,\"start\":58994},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207179138},\"end\":59729,\"start\":59439},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1903720},\"end\":60214,\"start\":59731},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3806935},\"end\":60721,\"start\":60216},{\"attributes\":{\"id\":\"b27\"},\"end\":61033,\"start\":60723},{\"attributes\":{\"id\":\"b28\"},\"end\":61337,\"start\":61035},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":51783444},\"end\":61626,\"start\":61339},{\"attributes\":{\"id\":\"b30\"},\"end\":61980,\"start\":61628},{\"attributes\":{\"id\":\"b31\"},\"end\":62221,\"start\":61982},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1903919},\"end\":62596,\"start\":62223},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2370209},\"end\":62999,\"start\":62598},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5453786},\"end\":63388,\"start\":63001}]", "bib_title": "[{\"end\":49592,\"start\":49562},{\"end\":49752,\"start\":49733},{\"end\":49904,\"start\":49880},{\"end\":50266,\"start\":50217},{\"end\":50664,\"start\":50559},{\"end\":51213,\"start\":51185},{\"end\":51923,\"start\":51820},{\"end\":52517,\"start\":52487},{\"end\":53027,\"start\":52962},{\"end\":53497,\"start\":53436},{\"end\":53932,\"start\":53871},{\"end\":54581,\"start\":54531},{\"end\":54937,\"start\":54879},{\"end\":55347,\"start\":55242},{\"end\":56062,\"start\":55945},{\"end\":57167,\"start\":57049},{\"end\":57504,\"start\":57455},{\"end\":58135,\"start\":58088},{\"end\":58650,\"start\":58551},{\"end\":59029,\"start\":58994},{\"end\":59485,\"start\":59439},{\"end\":59839,\"start\":59731},{\"end\":60299,\"start\":60216},{\"end\":61387,\"start\":61339},{\"end\":62283,\"start\":62223},{\"end\":62683,\"start\":62598},{\"end\":63092,\"start\":63001}]", "bib_author": "[{\"end\":49415,\"start\":49400},{\"end\":49604,\"start\":49594},{\"end\":49612,\"start\":49604},{\"end\":49770,\"start\":49754},{\"end\":49922,\"start\":49906},{\"end\":49936,\"start\":49922},{\"end\":49953,\"start\":49936},{\"end\":49973,\"start\":49953},{\"end\":49987,\"start\":49973},{\"end\":49999,\"start\":49987},{\"end\":50283,\"start\":50268},{\"end\":50302,\"start\":50283},{\"end\":50314,\"start\":50302},{\"end\":50681,\"start\":50666},{\"end\":50690,\"start\":50681},{\"end\":50705,\"start\":50690},{\"end\":50719,\"start\":50705},{\"end\":51237,\"start\":51215},{\"end\":51249,\"start\":51237},{\"end\":51266,\"start\":51249},{\"end\":51570,\"start\":51554},{\"end\":51939,\"start\":51925},{\"end\":51954,\"start\":51939},{\"end\":51967,\"start\":51954},{\"end\":51982,\"start\":51967},{\"end\":51993,\"start\":51982},{\"end\":52007,\"start\":51993},{\"end\":52539,\"start\":52519},{\"end\":52555,\"start\":52539},{\"end\":53040,\"start\":53029},{\"end\":53519,\"start\":53499},{\"end\":53534,\"start\":53519},{\"end\":53553,\"start\":53534},{\"end\":53569,\"start\":53553},{\"end\":53954,\"start\":53934},{\"end\":53970,\"start\":53954},{\"end\":54265,\"start\":54248},{\"end\":54280,\"start\":54265},{\"end\":54296,\"start\":54280},{\"end\":54313,\"start\":54296},{\"end\":54326,\"start\":54313},{\"end\":54343,\"start\":54326},{\"end\":54600,\"start\":54583},{\"end\":54614,\"start\":54600},{\"end\":54630,\"start\":54614},{\"end\":54959,\"start\":54939},{\"end\":54975,\"start\":54959},{\"end\":54987,\"start\":54975},{\"end\":55372,\"start\":55349},{\"end\":55387,\"start\":55372},{\"end\":55403,\"start\":55387},{\"end\":56078,\"start\":56064},{\"end\":56093,\"start\":56078},{\"end\":56108,\"start\":56093},{\"end\":56120,\"start\":56108},{\"end\":56131,\"start\":56120},{\"end\":56145,\"start\":56131},{\"end\":56797,\"start\":56775},{\"end\":56812,\"start\":56797},{\"end\":56838,\"start\":56812},{\"end\":56845,\"start\":56838},{\"end\":57180,\"start\":57169},{\"end\":57523,\"start\":57506},{\"end\":57538,\"start\":57523},{\"end\":57565,\"start\":57538},{\"end\":57586,\"start\":57565},{\"end\":57604,\"start\":57586},{\"end\":57621,\"start\":57604},{\"end\":57638,\"start\":57621},{\"end\":58161,\"start\":58137},{\"end\":58177,\"start\":58161},{\"end\":58206,\"start\":58177},{\"end\":58221,\"start\":58206},{\"end\":58250,\"start\":58221},{\"end\":58664,\"start\":58652},{\"end\":58679,\"start\":58664},{\"end\":59044,\"start\":59031},{\"end\":59055,\"start\":59044},{\"end\":59070,\"start\":59055},{\"end\":59082,\"start\":59070},{\"end\":59505,\"start\":59487},{\"end\":59854,\"start\":59841},{\"end\":59863,\"start\":59854},{\"end\":59876,\"start\":59863},{\"end\":59891,\"start\":59876},{\"end\":60315,\"start\":60301},{\"end\":60330,\"start\":60315},{\"end\":60832,\"start\":60817},{\"end\":60844,\"start\":60832},{\"end\":60855,\"start\":60844},{\"end\":61134,\"start\":61120},{\"end\":61149,\"start\":61134},{\"end\":61163,\"start\":61149},{\"end\":61407,\"start\":61389},{\"end\":61715,\"start\":61698},{\"end\":61735,\"start\":61715},{\"end\":61748,\"start\":61735},{\"end\":61761,\"start\":61748},{\"end\":61779,\"start\":61761},{\"end\":62061,\"start\":62045},{\"end\":62079,\"start\":62061},{\"end\":62302,\"start\":62285},{\"end\":62319,\"start\":62302},{\"end\":62701,\"start\":62685},{\"end\":62716,\"start\":62701},{\"end\":62729,\"start\":62716},{\"end\":63110,\"start\":63094},{\"end\":63123,\"start\":63110},{\"end\":63142,\"start\":63123}]", "bib_venue": "[{\"end\":49398,\"start\":49354},{\"end\":49643,\"start\":49629},{\"end\":49801,\"start\":49787},{\"end\":50040,\"start\":50034},{\"end\":50382,\"start\":50359},{\"end\":50829,\"start\":50758},{\"end\":51334,\"start\":51311},{\"end\":51552,\"start\":51494},{\"end\":52117,\"start\":52046},{\"end\":52682,\"start\":52598},{\"end\":53165,\"start\":53079},{\"end\":53634,\"start\":53585},{\"end\":54030,\"start\":54013},{\"end\":54246,\"start\":54203},{\"end\":54699,\"start\":54676},{\"end\":55055,\"start\":55032},{\"end\":55543,\"start\":55447},{\"end\":56287,\"start\":56202},{\"end\":56773,\"start\":56734},{\"end\":57249,\"start\":57226},{\"end\":57759,\"start\":57676},{\"end\":58318,\"start\":58296},{\"end\":58761,\"start\":58695},{\"end\":59214,\"start\":59126},{\"end\":59580,\"start\":59538},{\"end\":59951,\"start\":59907},{\"end\":60394,\"start\":60330},{\"end\":60815,\"start\":60723},{\"end\":61118,\"start\":61035},{\"end\":61479,\"start\":61425},{\"end\":61696,\"start\":61628},{\"end\":62043,\"start\":61982},{\"end\":62406,\"start\":62362},{\"end\":62793,\"start\":62769},{\"end\":63192,\"start\":63167},{\"end\":50887,\"start\":50831},{\"end\":52175,\"start\":52119},{\"end\":52753,\"start\":52684},{\"end\":53238,\"start\":53167},{\"end\":55626,\"start\":55545},{\"end\":56359,\"start\":56289}]"}}}, "year": 2023, "month": 12, "day": 17}