{"id": 246869703, "updated": "2023-10-05 16:52:05.9", "metadata": {"title": "RLSS: A Deep Reinforcement Learning Algorithm for Sequential Scene Generation", "authors": "[{\"first\":\"Azimkhon\",\"last\":\"Ostonov\",\"middle\":[]},{\"first\":\"Peter\",\"last\":\"Wonka\",\"middle\":[]},{\"first\":\"Dominik\",\"last\":\"Michels\",\"middle\":[\"L.\"]}]", "venue": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2022, pp. 2723-2732", "journal": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We present RLSS: a reinforcement learning algorithm for sequential scene generation. This is based on employing the proximal policy optimization (PPO) algorithm for generative problems. In particular, we consider how to effectively reduce the action space by including a greedy search algorithm in the learning process. Our experiments demonstrate that our method converges for a relatively large number of actions and learns to generate scenes with predefined design objectives. This approach is placing objects iteratively in the virtual scene. In each step, the network chooses which objects to place and selects positions which result in maximal reward. A high reward is assigned if the last action resulted in desired properties whereas the violation of constraints is penalized. We demonstrate the capability of our method to generate plausible and diverse scenes efficiently by solving indoor planning problems and generating Angry Birds levels.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2206.02544", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2206-02544", "doi": "10.1109/wacv51458.2022.00278"}}, "content": {"source": {"pdf_hash": "41a925fdbc3cb3a8995ba7b312f2343cd150b0dd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2206.02544v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://repository.kaust.edu.sa/bitstream/10754/675837/1/0762.pdf", "status": "GREEN"}}, "grobid": {"id": "87035175b2cecbac209adcd67ae31a51bc898d01", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/41a925fdbc3cb3a8995ba7b312f2343cd150b0dd.txt", "contents": "\nRLSS: A Deep Reinforcement Learning Algorithm for Sequential Scene Generation\n\n\nAzimkhon Ostonov azimkhon.ostonov@kaust.edu.sa \nKAUST Visual Computing Center Campus\nBldg 123955ThuwalKSA\n\nPeter Wonka peter.wonka@kaust.edu.sa \nKAUST Visual Computing Center Campus\nBldg 123955ThuwalKSA\n\nDominik L Michels dominik.michels@kaust.edu.sa \nKAUST Visual Computing Center Campus\nBldg 123955ThuwalKSA\n\nRLSS: A Deep Reinforcement Learning Algorithm for Sequential Scene Generation\n\nWe present RLSS: a reinforcement learning algorithm for sequential scene generation. This is based on employing the proximal policy optimization (PPO) algorithm for generative problems. In particular, we consider how to effectively reduce the action space by including a greedy search algorithm in the learning process. Our experiments demonstrate that our method converges for a relatively large number of actions and learns to generate scenes with predefined design objectives. This approach is placing objects iteratively in the virtual scene. In each step, the network chooses which objects to place and selects positions which result in maximal reward. A high reward is assigned if the last action resulted in desired properties whereas the violation of constraints is penalized. We demonstrate the capability of our method to generate plausible and diverse scenes efficiently by solving indoor planning problems and generating Angry Birds levels.\u2022 We demonstrate the advantages of our method to generate plausible and diverse scenes efficiently by solving indoor planning problems and generating Angry Birds levels.\n\nIntroduction\n\nGenerative modeling has seen drastic improvements in recent years. Especially for images, state-of-the-art generative adversarial networks (GANs) produce fantastic results [11,15,16]. Even though generative models such as GANs or variational autoencoders (VAEs) [19] are considered to be unsupervised methods, a large amount of data is still required. While multiple attempts have been made in reproducing the success of image-based generative models in other domains, such as scenes, meshes, and point clouds, the results are far behind. A major obstacle is the lack of data, as there are no high quality data sets of scenes that contain many models. Many data sets are small, e.g. [35], or contain too many low quality models. For example, the SUNCG data set [32] contains multiple low quality models generated by amateur modelers that violate commonly accepted hard constraints (in addition, the data set is currently unavailable due to a legal dispute).\n\nIn order to make progress on generative modeling for scene generation, we propose to build a learning framework that does not require a large amount of training data. In this context, reinforcement learning is an ideal choice as the need for training data is replaced by reward function design. As reinforcement learning is traditionally applied to maximize expected cumulative reward, it is not generally used to generate a large variety of scenes. In this work we propose a reinforcement learning algorithm to be able to generate a large variety of scenes. We call our approach RLSS. It operates sequentially and places scene objects oneby-one.\n\nSpecifically, our major contributions are as follows.\n\n\u2022 We present RLSS: (to our knowledge) the first reinforcement learning based scene generation algorithm. The distinguishing characteristic is that RLSS can generate a large variety of scenes for the same input (i.e. scene boundary) taking into account domain constraints.\n\n\u2022 We propose an efficient approach for scene synthesis problem which enables to solve this task by separating the problem requirements into two different categories: hard constraints that are included in the environment and predefined design objectives, which the network learns how to achieve, given different initial scene boundaries, during the training process.\n\n\u2022 Our reward designing approach makes it easy to adapt this method for many scene synthesis problems without spending too much time on reward calculation for actions.\n\n\nRelated Work\n\nEmploying reinforcement learning in the context of generative problems has been addressed in the community.\n\nSPIRAL [10] is a reinforcement learning based adversarial agent which learns to synthesize visual programs for graphic engines in order to generate images. It has been shown that their method works well for image reconstructions on MNIST [21] and OMNIGLOT [20] data sets but fails to synthesize new examples. Model-based reinforcement learning [14] was applied to image reconstruction for stroke-based paintings. The fundamental difference between these methods and our method is that in contrast to rewarding the agents based on the discriminators' outputs, in our case this is done by simulating the environment and checking relevant constraints.\n\nFormalizing reinforcement learning as a probabilistic inference technique has been explored in [4,1,22]. Connecting reinforcement learning with posterior regularization [9] to incorporate the domain constraints to deep generative models has been considered in [13]. Compared to this approach, our method is based on directly employing reinforcement learning as a generative model.\n\nDeep generative models [11,19] have been extensively used by scientific community to synthesize scenes in indoor planning [38,24,37,29,41]. These models are mostly image-based and therefore use pixel-level reasoning to distinguish objects and extract spatial features from an image. Which can be poorly adapted when handling various domain-based constraints in scene synthesis problems. As a result, one either need to generalize the problem and accept more simplified domain constraints [24], or use constraint violation check at each step along with generative models to deal with the problem [38,37]. However, this is associated with costs: in the first case, we will solve the sub-problem instead of solving the real problem, in the second case, this leads to an increase in the synthesis time.\n\nAnother major direction to generate indoor scenes is related with using Markov chain Monte Carlo (MCMC) methods [28,25,40,17]. In these methods problem specific objective functions are optimized based on some rule based criteria. The main drawback of using MCMC sampling in these methods is generation time, which requires thousands of iterations to synthesize one plausible scene. We also would like to mention the reinforcement learning has been used in several contexts different from generative problems [23,27,42,36,12].\n\n\nMethod\n\nIn this section, we provide an overview of RLSS: our reinforcement learning based sequential scene generation algorithm. We provide required background information and discuss relevant components in detail.\n\n\nOverview\n\nWe developed RLSS based on the proximal policy optimization (PPO) [31] method for scene generation problems. The scene we would like to generate is represented as an environment in which objects are added iteratively one-byone. The neural network is trained on a 2D representation of the scene. If the visual content we are generating has three dimensions, then its most informative 2D view is used during the training. The set of all actions which can be applied in the environment is denoted with A. The reward which the agent obtains after taking an action a \u2208 A is denoted by r. This formulation of the scene allows us to use reinforcement learning algorithms. However, the use of standard reinforcement learning is limited to optimization problems, where the agent needs to collect as much reward as possible in a virtual environment. In each step the agent chooses an action among the set of possible actions which at the end will result in the highest reward. Therefore, in the standard reinforcement learning setup, the main target of the agent is to maximize the cumulative reward. In our setting we additionally consider increasing variety, which is achieved by introducing randomness in the action selection process.\n\nWe introduce the following two problem-specific key definitions which will help us to effectively explain our method.\n\n\u2022 Hard constraints are a set of constraints that should always be satisfied when synthesizing a scene. Usually, this type of constraints come from the problem domain and violating these constraints makes it impossible to continue the generation process. In our method, these constraints are included in the environment.\n\n\u2022 Design objectives comprise a set of requirements which shape the general appearance of the generated scene. Each successfully generated scene should have these features. This can be understood as a broad definition of a scene we would like to synthesize. Only such scenes which fulfill predefined design objectives could be considered as valid examples for good scenes.\n\nDifferent from the most of the scene synthesis algorithms our method learns how to generate successful scene based on predefined criteria (design objectives) and domain knowledge is applied to place objects in the scene. This method has particular advantages where relationship or rules exist for placing objects. For example, in physics based games or in indoor planning.\n\n\nScene Abstraction\n\nA scene consists of the initial scene boundary and set of objects, each defined with its center's position P (x, y, z), bounding box (box with minimal volume which can fully contain this object) and orientation \u03b1 relative to local coordinate axis.\n\nStructures (groups). First, we find independent structures in the problem domain. Each structure consists of one or more objects and together these objects play some function in the current problem domain. These structures can be derived from acceptable scene examples. We define the set of all structures we use in the problem as S. Each structure st \u2208 S has a complexity (the number of different objects in this structure) defined as C(st).\n\nPlacements. Many structures have similar arrangements in the scene. For example, table and chair, dresser and ottoman, sofa and coffee table can be placed in a similar way. For a given set of objects we define set of possible placements P to place these objects in the scene. Objects in the placements are defined by their top left coordinate of the bounding box and orientation. These placements are extracted from acceptable scene examples. Objects are placed in the scene according to one of the placements which makes generated scenes more realistic.\n\n\nBasic Network and State Representation\n\nReinforcement learning (RL) learns how to control an agent in an environment, to maximize cumulative reward. Given an observation o t at time t, the agent performs an action a t according to its policy \u03c0 and receives a reward r t for the current action. The policy is a mapping between state s t , the environment's current condition to the action. The environment can be described as a Markov Decision Process, i.e., the current state of the environment fully characterizes the process. The total reward from time step t is defined as\nR t = \u221e i=0 \u03b3 i r t+i (s i , a i ) ,\n\u03b3 \u2208 (0, 1] in this regard is a discount factor which indicates how important are the future rewards at the current point.\n\nPolicy based methods are based on adjusting policy in order to maximize the expected reward. We use PPO [31] with actor-critic network with shared parameters between the policy \u03c0(a t |s t ; \u03b8) and value function V (s t ; \u03b8 v ). The policy is updated with clipped policy gradient objective\nL CLIP (\u03b8) =\u00ca t min(r t (\u03b8)\u00c2 t , clip(r t (\u03b8), 1 \u2212 \u03f5, 1 + \u03f5)\u00c2 t ) ,\nwhere\u00c2 t advantage function estimator and r t (\u03b8) = (\u03c0(a t |s t ; \u03b8))/(\u03c0(a t |s t ; \u03b8 old )) probability ratio. The final objective term includes value updates and entropy bonus to encourage exploration [31] \nL t (\u03b8) =\u00ca t L CLIP t (\u03b8) \u2212 c 1 L V F t (\u03b8) + c 2 S[\u03c0(a t |s t ; \u03b8)](s t ) . The value function (L V F t (\u03b8))\nis updated with squared-error loss between the value function output and the total reward from time step t.\n\nState consists of: 2D representation of the scene which includes scene boundary and objects; object existence per category indicates whether the scene has at least one object from this category, takes value from {0, 1}; object availability per category which is equal to 1 if number of objects is less than maximal allowed 0 otherwise; scene condition; step indicator.\n\n\nAction Space Separation\n\nOne main concern which needs to be taken into account when using RL is dealing with a large action space. For the scene synthesis problems action space A formed from the Cartesian product of two sets O and P ,\nA = O \u00d7 P = {(o, p)|o \u2208 O and p \u2208 P }.\nWhere O denotes the set of all objects and P denotes the set of all positions to place these objects in the scene. As a result of this, action space could grow rapidly even for a smaller number of objects and positions. Thus creating problems for RL algorithms to learn efficient policies. In our approach we limit action space with objects only, A = O. The positions for placing these objects are determined by a greedy algorithm, based on the current state and the object being added to the scene. More concretely, p i = arg max j\u2208P r(s, a, j), p i -position for the current object, r(s, a, j) -reward function.\n\n\nReward Designing\n\nAt each step the scene generated with our algorithm is checked with respect to the following conditions: successful scene conditions (CheckSuccessf ulCondition()), failure conditions (CheckF ailureCondition()), constraints on the number of objects of the same type (CheckObjectCountCondition()). Each episode ends with a successful or failed scene generation. If after some steps the generated scene has met the failure condition, then the corresponding reward is \u22121. In contrast, once all of the predefined design objectives are obtained, which means a successful scene generated, then we assign the maximum reward. For both of these cases the scene generation ends and it will start again from the beginning. Otherwise, the reward is calculated depending on the type of design objectives which are achieved taking the last action.\n\nIn each step, only one type of object tp \u2208 O chosen by the RL algorithm can be placed within the scene. The objects which were already placed in the scene can be formally written as a union of substructures located in the different positions of this scene. A substructure is a part of some structure st \u2208 S. We first find the maximum complexity of substructures existing in the scene (Search()), which the current object can be grouped with, while not violating hard constraints. If there are more than one substructures with maximum complexity, then the choice is arbitrarily done between them. Then the current object is grouped with this substructure. The resulting substructure is part of some structure st \u2032 \u2208 S. The reward for the placing of this object depends on the complexity of the substructure we can get from this placement. The higher the resulting complexity, the higher is the reward. This encourages the RL algorithm to find optimal policies to get a higher cumulative reward. From the other side, we use only l different positive rewards not depending on the type of the structure, one for each complexity: r 1 \u2264 r 2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 r l , which serves to increase the variety of generated scenes. Where, l = max st \u2208 S C(st) is maximum complexity among all structures. Constraints on the number of elements of the same type are applied by assigning negative rewards. Successful and failed generation conditions depend on the problem domain. All the rewards in our method are taken from the [\u22121, 1] interval. The generation process is summarized in Algorithm 1. P laceObject() function in this scope uses one of the placement functions depending on the state, current object and complexity. Predefined design objectives. Structures in the scenes exhibit different relationships between objects. As we quantify these relationships with numbers (rewards), we can numerically assess the current condition of the generated scene. In this paper, the sum of all rewards taken so far t i=1 r i represents the current condition of the scene. From positive examples we can get minimal value R m for a scene to be considered successfully generated. Also, we can enforce other requirements along with minimal value, depending on the problem. These conditions follow form predefined design objectives.\n\n\nAlgorithm 1 Reward Assigment\n\nInput: current state, current action, reward vector sorted in descending order\n\nOutput: reward for current action \n\n\nNetwork Training\n\nThe main objective of our method is to generate a wide variety of scenes which satisfy hard constraints and at the same time have predefined properties. In each step we add an object to the scene, until we found that the final condition is reached, successful or failed scene generated. We denote this process as an episode, and the scene as an environment.\n\nAction sampling. The objective in standard RL is to maximize E[R t ] expected cumulative reward. In our setting we additionally would like to increase variety of generated scenes. We normalize \u03c0(a i |s; \u03b8) policy output (unnormalized) for a scene s, using the softmax function with temperature \u03c4 : P \u03c4 (a i |s; \u03b8) = e \u03c0(ai|s;\u03b8)/\u03c4 N z=1 e \u03c0(az|s;\u03b8)/\u03c4 .\n\nHere, P \u03c4 (a i |s; \u03b8) specifies the probability of taking action a i given state s. In order to balance between exploration and exploitation, \u03c4 is steadily decreased from 1 completely randomly to 0 greedy action sampling. And then we keep greedy action sampling until convergence during the training process. During the inference time we sample actions with some \u03c4 value. As \u03c4 close to 1 the results show more variety but the percent of scenes which have predefined properties will be small. Conversely, for the values of \u03c4 close to 0 the results have less variety and predefined properties achieved in more scenes. We use Jensen-Shannon divergence to quantify similarity between resulting distribution and uniform distribution. Uniform distribution is chosen as a perfect case for variety, the more the resulting distribution is close to the uniform distribution the more the results show diversity. In this paper we choose \u03c4 from the following condition:\n\u03c4 optimal = arg max 0<\u03c4 \u22641 min(V (\u03c4 ), W (\u03c4 )) , V (\u03c4 ) = 1 \u2212 JSD(P \u03c4 , U) .\nHere, V (\u03c4 ), W (\u03c4 ) mean variety and percentage of successful scenes for fixed value of \u03c4 respectively. JSD(P \u03c4 , U) is Jensen-Shannon divergence between resulting distribution and uniform distribution (the base 2 logarithm is used when calculating JSD, 0 \u2264 JSD(P \u03c4 , U) \u2264 1). Actions can be sampled by uniform distribution for several starting steps in the episode when it does not affect the accuracy.\n\nWe use the PPO algorithm for several purposes: this algorithm uses both policy and value based updates, and it has been shown that this method is much faster than other reinforcement learning implementations like A2C [26], A2C with trust region [39] and TRPO [30]. We also implemented our method with A3C [26]. In both cases the agent learned efficient policies, but PPO required much less iterations than A3C to converge.\n\n\nExperiments and Results\n\nIn order to evaluate our RLSS method, we address two problems: indoor planning as well as the generation of Angry Birds game levels. The initial setting is the same for all cases considered except for the number of operations.\n\n\nImplementation Details\n\nFramework. We implement our method within the ChainerRL framework [8]. In order to use it, the problem we would like to solve must be formulated as an environment. We create our own environment within the Open AI Gym [3] framework and add required functionalities to it.\n\nNetwork architecture and data representation. The network architecture we use is illustrated in Fig. 1. MLP refers to fully connected layers, Conv specifies convolutional layers. Three convolutional layers are used for pre-processed input image and a fully connected layer for each of other two inputs, all of the layers are followed by ReLU nonlinearity [2]. Then three outputs are concatenated to one feature vector. This is then continued by MLPs into two final outputs one for each of V (s t ; \u03b8 v ) value and \u03c0(s t ; \u03b8) policy. Each MLP consisted of two fully connected layers, first of which is followed by ReLU. The representation of the scene is consists of only the last frame. As a pre-processing, at first, we convert the scene to heightmaps [38,7], a topdown depth rendered view of the scene, then downsize the image to 128 \u00d7 128 size. Object existence and availability are vectors consisting of {0, 1} per object category, scene condition is normalized dividing by R m and is also represented as a vector. These three are concatenated and fed to MLP.\n\nStep indicator is represented as a one-hot encoding.\n\nParameters. We use a discount factor of \u03b3 = 0.99, the agents update of the network parameters after every 2048 actions, minibatch size is equal to 64 and number of epochs is equal to 10. As for the optimization, we use Adam optimizer [18] with a learning rate lr = 3 \u00b7 10 \u22124 . In order to stabilize the learning process we use a reward scaling with a factor of rs = 10 \u22122 . In our experiments, the value loss coefficient is set to c 1 = 1 and the entropy regularization weight is equal to c 2 = 0.01.\n\nTraining. We have used non-parallel implementation with only a single actor-learner. The neural network training took around 13 hours for 4 million steps. In the last million steps we applied only greedy action sampling.\n\n\nIndoor Planning\n\nIn this problem, different room models are given, i.e. the geometry of the room including walls, doors, floors, and ceilings. Three different types of rooms are considered: living rooms, bedrooms, and offices. Our task is to propose a generative method for placing objects, i.e. furniture, within the room.\n\nOur approach is to group objects based on their functionalities, and place the current object to one of its appropriate group by analyzing existing not completed groups in the room. This helps us to generate rooms which are more similar to human planned ones. For example, some groups we use for synthesizing bedrooms are: (1) bed, nightstand, floor lamp, and ottoman; (2) desk and chair; (3) dressing table and ottoman. In order to find groups of objects that appear together in the room, we use human planned room models. We also analyze the arrangements, how the group objects come together with usual placing relative to a room architecture and instantiate objects according to this. Our representation to train the network combines a top-down view of the generated scene including positions of the door and the windows.\n\nWe consider several hard constraints for this problem. First, two different objects should not share a common area (collision avoidance). Next, it should be possible for a human to move in the room conveniently. Finally, no object should block another object in the room, i.e. all objects should be accessible for a human. We assign rewards for each complexity of structures and an additional reward for some important objects for this type of the room, e.g., placing a bed in the bedroom. The additional reward is assigned only once when the important object(s) is first time placed in the room during the current episode.\n\nComparison. To compare our work with the state-ofthe-art, we consider recent convolutional neural network based approaches [38,24,37,29] and an optimization based method [40]. We compare these methods on 60 randomly taken generated examples for each type of the room (bedrooms, living rooms, and offices) using several constraints including object-with-object and object-with-room boundary collisions, object accessibility and door blocking problems; see Table. 1. Our method handles these constraints much more accurately compared to other methods. However, given the wide variety and complexity of the room boundaries, our method also allows for a small percentage of constraint violations, especially with object accessibility related problems. For a fair comparison we applied our structure based rules on [40] and implemented their code as it was not available. It should be noted that [40] rearranges the selected set of objects in the room, which severely limits the variety. We also consider another baseline to evaluate the importance of learning. We apply Greedy Search to place objects in the scene with our rule based Assign-Reward() function with and without rewards. In the first case objects are sampled uniformly, in the second case objects are sampled by rewards which are calculated for each object. Results show Table. 2, that our RLSS outperforms the baseline method by large margins. Scene complexity in this table specifies the maximal complexity of the structures present in the scene averaged over tests. In general, nonlearning based baselines generate scenes for smaller rooms, but failed for large rooms, also reward calculation for each object results to an increase in synthesis time.\n\nWe find that handling different room boundaries is one of the fundamental limitations in indoor planning. Methods based on generating scenes for the rectangular room boundaries [24,17,28] or for rooms which should be encircled with walls [37] might work well for special case but might not generalize well for other cases. Moreover, using the same rendering for the synthesized results is important to compare results fairly. For these purposes, we compare our method with [29]. Visual comparison of the synthesized scenes with two different methods can be found in Fig. 3.\n\nTo evaluate the diversity of the generated scenes we consider graph kernels [6]. Fig. 2 shows similar scenes to the given example scene amongst 1000 generated scenes, for each type of the room. Scene similarity in this example is evaluated based on proximity and common function of objects (relationship) in the scene. Numerical assessment of the diversity of generated scenes based on Kullback Leibler (KL) divergence of object category distribution and uniform distribution is given in the Table 1.\n\nMoreover, we evaluated the performance of our RLSS method and related work as illustrated in Table 1. As illustrated there, our RLSS method is very competitive when with respect to its performance. Also, our approach is not data-driven, it does not require training data and is capable of handling different room boundaries.\n\n\nLevel Generation\n\nAs an additional benchmark from another context, we apply our RLSS method in order to generate levels for the physics-based game Angry Birds aiming for a wide variety of stable game levels. Blocks used in the game might differ by their size, shape, and material. Some blocks can be placed in the game environment in a horizontal as well as in a vertical position. All blocks can be classified as regular and irregular blocks. We build stable structures from regu-  Figure 2. Illustration of the diversity of scenes generated with our RLSS method. The leftmost image in each column represents an example image, other images are the nearest neighbors amongst 1000 generated scenes. Scene similarity is measured using graph kernels [6]. lar blocks and add irregular blocks into these. All regular blocks have a box shape.\n\nThe game area where we build stable blocks comprises a 800 \u00d7 800 pixel rectangle. Possible actions in this problem are to place a certain block in the scene. The last block is placed onto the top of the highest block in the given position. As a hard constraint we take stability.\n\nComparison. During testing, we maintain stable structures in the memory and build levels out of these. We compare our method with the MSG v2.0 winning generator for the 2017 and 2018 Angry Birds AI (level generation) competition [34]. MSG v2.0 is based on generating the game content procedurally. This generator also generates levels from stable structures [33]. We find that MSG v2.0 takes on average about 22 seconds to generate a single level, while our method performs this task in only about 0.5 seconds. A visual comparison of the generated levels is given in the Fig. 4. Levels rendered with Science Birds [5], an opensource, Unity-based clone of the Angry Birds game. KL divergence of object category distribution and uniform distribution for MSG v2.0 and our method is approximately equal to 0.23 and 0.21, respectively. The comparison results with the non-learning baselines introduced above are presented in the Table. 2.\n\n\nQuantitative Evaluations\n\nTo quantitatively evaluate results generated by our method we conducted a 2 alternative forced choice comparison on Amazon Mechanical Turk. Participants were asked to choose the most plausible scene out of two synthesized scenes placed side-by-side, one generated with our   Table 4. Ablation study results: Accuracy and diversity of generated scenes, for our method and its variations. For the accuracy column, higher is better, while for the KL divergence column, lower is better.\n\nthe Table 3. As it can be seen from the data, that our method shows the best results for almost all comparisons. We conducted similar perceptual study to compare generated Angry Birds levels, with 30 images and 15 participants. The result of this comparison show that our levels were preferred with 62.2 \u00b1 6.9 (mean \u00b1 standard error) margins with 95% confidence.\n\nAblation. We train the neural network without Action Separation (ASE) & Reward Designing (ASE+RD) and also without our Action Sampling (ASA) for Angry Birds level generation problem. The experiments show that Naive PPO does not learn to achieve the predefined design objectives, instead it learns to pile up box objects to maximize reward. PPO with ASE+RD and sampling the next action from the top 3 with \u03f5 exploration at the beginning (\u03f5 = 0.6) does not demonstrate enough variety and accuracy Table 4.\n\n\nConclusion\n\nIn this paper, we proposed a reinforcement learning algorithm for scene generation called RLSS. To the best of our knowledge, we are the first to propose such an algorithm that can produce a wide variety of scenes. To this end, we modify the state-of-the-art PPO reinforcement learning  algorithm to sample from a large set of possible scenes. We also propose a suitable solution for reward function design, which includes two components. Hard constraints describe scene attributes that have to be fulfilled and that are strictly enforced. Predefined design objectives describe design goals or scene configurations that make a scene more desirable. Our results show that RLSS can produce a large variety of scenes with significantly higher quality than the current state of the art.\n\n\nLimitations and Future Work\n\nA limitation of our method is that it currently does not have a mechanism to combine learning from reward functions and scene examples at the same time. In our future work, if high quality scene data bases become available, we would like to investigate combinations of generative adversarial networks and reinforcement learning to tackle this challenging research problem. Also, extracting structures and implementing placement functions take additional time.\n\nIn addition, we believe that a great research direction is to extend reinforcement learning to learn scene generation from given input images. One possible approach is to design reward functions that include an estimation of the similarity of generated scenes and images. While this approach is even more difficult than learning from scene examples, it has the advantage that image data sets are much easier to come by than scene data sets.\n\nFigure 1 .\n1Network architecture for our RLSS method. State includes top-down representation of the indoor scene, per category object existence and availability in the scene, scene condition and step indicator.\n\nFigure 4 .\n4Illustration of several Angry Birds game levels generated with MSG v2.0 (top) and our RLSS method (bottom).\n\n\nTable 1. Classification and performance comparison of different indoor scene generation methods. Better results or options are indicated with bold text in each column. -means no available results.(a) Different bedroom scenes generated with Fast & Flexible (left) and our RLSS method (right). (b) Different living room scenes generated with Fast & Flexible (left) and our RLSS method (right).(c) Different office scenes generated with Fast & Flexible(left) and our RLSS method (right).Figure 3. Qualitative comparison of scenes generated with Fast & Flexible [29] and our method.RLSS method and another one with other method. For indoor planning problem each participant performed overall 63 comparison task for each method, 20 for each room type and 1 for vigilance test. Images in these comparisons represent top-down view of a scene, rendered such that all objects are visible and colored with solid colors to help participants to choose scenes by the object arrangements not by colors or other not important factors. For each task we considered answers from 10 participants, who passed all the vigilance tests. The results of this perceptual study is summarized inMethod \n\nGeneration Time \nRoom Boundary \nAcceptable scenes (by hard constraints) KL divergence \nBedroom \nLiving \nOffice \n\nDeep Priors [38] \n\u223c 240 sec. \nany \n60 % \n83.3 % \n61 % \n0.89 \nGRAINS [24] \n0.1027 sec. \nrectangular only \n64 % \n80 % \n52 % \n0.83 \nFast&Flexible [29] \n1.858 sec. \nany \n88.3 % \n86.7 % \n83.3 % \n0.91 \nPlanIT [37] \n\u223c 72 sec. \nclosed room boundaries 80 % \n83.3 % \n80 % \n0.72 \nMake it Home [40] \n\u223c 22 sec. \n-\n81 % \n81 % \n-\n-\nOurs \n\u223c 1 sec. \nany \n97.5 % \n98 % \n96 % \n0.81 \n\nMethod \n\nAccuracy \nStructure \ncomplexity \nSynthesis time \n\nIP \nAB IP \nAB IP \nAB \n\nGSearch w/o r 10 % 4 % 1.5 2.7 8 sec. 5 sec. \nGSearch w r 25 % 19 % 2.3 2.8 67 sec. 37 sec. \nRLSS \n62 % 58 % 3.4 3.0 1 sec 0.5 sec. \n\n\n\nTable 2 .\n2Comparison with Greedy Search with and without rewards. IP refers to Indoor Planning, AB specifies Angry Birds levels. Better results are indicated with bold. Fast & Flexible[29] 59.5 \u00b1 10.8 67.6 \u00b1 8.7 73.8 \u00b1 8.5PlanIT[37] 69.0 \u00b1 9.5 68.1 \u00b1 11.1 70.0 \u00b1 7.5 Make it Home[40] 64.8 \u00b1 9.4 61.4 \u00b1 8.4 -Table 3. Forced choice perceptual study results. Bold means our scenes are preferred with 95% confidence (\u00b1 standard error), regular text means no preference. -means no available results. Higher is better.Method \nBedroom \nLiving \nOffice \nDeep Priors [38] \n76.2 \u00b1 6.0 68.1 \u00b1 10.0 81.4 \u00b1 5.6 \nGRAINS [24] \n72.3 \u00b1 8.5 78.6 \u00b1 9.1 80.1 \u00b1 5.2 \nMethod \nAccuracy KL divergence \nNaive PPO \n0 % \n1.5634 \nPPO+ASE+RD 28 % \n0.6377 \nRLSS \n58 % \n0.2144 \n\n\nAcknowledgementsThis work was funded by KAUST through baseline funding. The valuable comments of the anonymous reviewers that improved the manuscript are gratefully acknowledged.\nRemi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. Abbas Abdolmaleki, Jost Springenberg, Yuval Tassa, 06Abbas Abdolmaleki, Jost Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. 06 2018.\n\n. Agarap Abien Fred, abs/1803.08375CoRRDeep learning using rectified linear units (reluAbien Fred Agarap. Deep learning using rectified linear units (relu). CoRR, abs/1803.08375, 2018.\n\n. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gymGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.\n\nUsing expectationmaximization for reinforcement learning. Peter Dayan, Geoffrey E Hinton, Neural Computation. 92Peter Dayan and Geoffrey E. Hinton. Using expectation- maximization for reinforcement learning. Neural Compu- tation, 9(2):271-278, 1997.\n\nA search-based approach for generating angry birds levels. Lucas Ferreira, Claudio Toledo, Proceedings of the 9th IEEE International Conference on Computational Intelligence in Games, CIG'14. the 9th IEEE International Conference on Computational Intelligence in Games, CIG'14Lucas Ferreira and Claudio Toledo. A search-based approach for generating angry birds levels. In Proceedings of the 9th IEEE International Conference on Computational Intel- ligence in Games, CIG'14, 2014.\n\nCharacterizing structural relationships in scenes using graph kernels. Matthew Fisher, Manolis Savva, Pat Hanrahan, ACM Trans. Graph. 3034Matthew Fisher, Manolis Savva, and Pat Hanrahan. Char- acterizing structural relationships in scenes using graph ker- nels. ACM Trans. Graph., 30:34, 07 2011.\n\nActivity-centric scene synthesis for functional 3d scene modeling. M Fisher, M Savva, Y Li, P Hanrahan, M Nie\u00dfner, ACM Transactions on Graphics (TOG). 346M. Fisher, M. Savva, Y. Li, P. Hanrahan, and M. Nie\u00dfner. Activity-centric scene synthesis for functional 3d scene mod- eling. ACM Transactions on Graphics (TOG), 34(6), 2015.\n\nChainerrl: A deep reinforcement learning library. Yasuhiro Fujita, Toshiki Kataoka, Prabhat Nagarajan, Takahiro Ishikawa, Workshop on Deep Reinforcement Learning at the 33rd Conference on Neural Information Processing Systems. Yasuhiro Fujita, Toshiki Kataoka, Prabhat Nagarajan, and Takahiro Ishikawa. Chainerrl: A deep reinforcement learn- ing library. In Workshop on Deep Reinforcement Learning at the 33rd Conference on Neural Information Processing Sys- tems, Dec. 2019.\n\nPosterior regularization for structured latent variable models. Kuzman Ganchev, Jo\u00e3o Gra\u00e7a, Jennifer Gillenwater, Ben Taskar, 01Kuzman Ganchev, Jo\u00e3o Gra\u00e7a, Jennifer Gillenwater, and Ben Taskar. Posterior regularization for structured latent variable models. 01 2009.\n\nSynthesizing programs for images using reinforced adversarial learning. Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S M Ali Eslami, Oriol Vinyals, Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, and Oriol Vinyals. Synthesizing programs for im- ages using reinforced adversarial learning, 2018.\n\n. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative adversarial networksIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.\n\nEnd-to-end multimodal image registration via reinforcement learning. Jing Hu, Ziwei Luo, Xin Wang, Shanhui Sun, Youbing Yin, Kunlin Cao, Qi Song, Siwei Lyu, Xi Wu, Medical Image Analysis. 68101878Jing Hu, Ziwei Luo, Xin Wang, Shanhui Sun, Youbing Yin, Kunlin Cao, Qi Song, Siwei Lyu, and Xi Wu. End-to-end multimodal image registration via reinforcement learning. Medical Image Analysis, 68:101878, 2021.\n\nDeep generative models with learnable knowledge constraints. Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Xiaodan Liang, Lianhui Qin, Haoye Dong, Eric Xing, Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Xiaodan Liang, Lianhui Qin, Haoye Dong, and Eric Xing. Deep gen- erative models with learnable knowledge constraints, 2018.\n\nLearning to paint with model-based deep reinforcement learning. Zhewei Huang, Shuchang Zhou, Wen Heng, IEEE/CVF International Conference on Computer Vision (ICCV). Zhewei Huang, Shuchang Zhou, and Wen Heng. Learn- ing to paint with model-based deep reinforcement learning. 2019 IEEE/CVF International Conference on Computer Vi- sion (ICCV), Oct 2019.\n\nA style-based generator architecture for generative adversarial networks. Tero Karras, Samuli Laine, Timo Aila, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4401-4410, 2019.\n\nTero Karras, Samuli Laine, Miika Aittala, arXiv:1912.04958Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. arXiv preprintTero Karras, Samuli Laine, Miika Aittala, Janne Hell- sten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. arXiv preprint arXiv:1912.04958, 2019.\n\nPing Tan, and Hao (Richard) Zhang. Learning 3D Scene Synthesis from Annotated RGB-D Images. Zicheng Zeinab Sadeghipour Kermani, Liao, Computer Graphics Forum. Zeinab Sadeghipour Kermani, Zicheng Liao, Ping Tan, and Hao (Richard) Zhang. Learning 3D Scene Synthesis from Annotated RGB-D Images. Computer Graphics Forum, 2016.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arxiv:1412the 3rd International Conference for Learning Representations. San Diego6980Published as a conference paper atDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.\n\nAuto-encoding variational bayes. P Diederik, Max Kingma, Welling, Diederik P Kingma and Max Welling. Auto-encoding varia- tional bayes, 2013.\n\nHuman-level concept learning through probabilistic program induction. Brenden Lake, Ruslan Salakhutdinov, Joshua Tenenbaum, Science. 350Brenden Lake, Ruslan Salakhutdinov, and Joshua Tenen- baum. Human-level concept learning through probabilistic program induction. Science, 350:1332-1338, 12 2015.\n\nMnist handwritten digit database. Yann Lecun, Corinna Cortes, C J Burges, 2ATT LabsYann LeCun, Corinna Cortes, and CJ Burges. Mnist hand- written digit database. ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2, 2010.\n\nReinforcement learning and control as probabilistic inference: Tutorial and review. Sergey Levine, arxiv:1805.00909Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. 2018. cite arxiv:1805.00909.\n\nTeaching uavs to race with observational imitation learning. Guohao Li, Matthias M\u00fcller, Vincent Casser, Neil Smith, L Dominik, Bernard Michels, Ghanem, abs/1803.01129CoRRGuohao Li, Matthias M\u00fcller, Vincent Casser, Neil Smith, Dominik L. Michels, and Bernard Ghanem. Teaching uavs to race with observational imitation learning. CoRR, abs/1803.01129, 2018.\n\nGrains: Generative recursive autoencoders for indoor scenes. Manyi Li, Akshay Patil, Kai Xu, Siddhartha Chaudhuri, Owais Khan, Ariel Shamir, Changhe Tu, Baoquan Chen, Daniel Cohen-Or, ACM Transactions on Graphics. 382019Manyi Li, Akshay Patil, Kai Xu, Siddhartha Chaudhuri, Owais Khan, Ariel Shamir, Changhe Tu, Baoquan Chen, and Daniel Cohen-Or. Grains: Generative recursive autoen- coders for indoor scenes. ACM Transactions on Graphics, 38:1-16, 02 2019.\n\nInteractive furniture layout using interior design guidelines. Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala, Vladlen Koltun, ACM Transactions on Graphics. 30Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala, and Vladlen Koltun. Interactive furniture layout using inte- rior design guidelines. ACM Transactions on Graphics, 30, 07 2011.\n\nAsynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International Conference on Machine Learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Ma- chine Learning, pages 1928-1937, 2016.\n\nTeaching uavs to race using ue4sim. Matthias M\u00fcller, Vincent Casser, Neil Smith, L Dominik, Bernard Michels, Ghanem, abs/1708.05884CoRRMatthias M\u00fcller, Vincent Casser, Neil Smith, Dominik L. Michels, and Bernard Ghanem. Teaching uavs to race us- ing ue4sim. CoRR, abs/1708.05884, 2017.\n\nHuman-centric indoor scene synthesis using stochastic grammar. Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, Song Zhu, 06Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and Song Zhu. Human-centric indoor scene synthesis using stochastic grammar. 06 2018.\n\nFast and flexible indoor scene synthesis via deep convolutional generative models. Daniel Ritchie, Kai Wang, Yu-An Lin, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Daniel Ritchie, Kai Wang, and Yu-An Lin. Fast and flexi- ble indoor scene synthesis via deep convolutional generative models. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2019.\n\nTrust region policy optimization. John Schulman, Sergey Levine, Philipp Moritz, Michael I Jordan, Pieter Abbeel, abs/1502.05477CoRRJohn Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad- ford, and Oleg Klimov. Proximal policy optimization algo- rithms. arXiv preprint arXiv:1707.06347, 2017.\n\nSemantic scene completion from a single depth image. Shuran Song, Fisher Yu, Andy Zeng, Angel Chang, Manolis Savva, Thomas Funkhouser, Shuran Song, Fisher Yu, Andy Zeng, Angel Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene completion from a single depth image. pages 190-198, 07 2017.\n\nGenerating varied, stable and solvable levels for angry birds style physics games. Matthew Stephenson, Jochen Renz, Matthew Stephenson and Jochen Renz. Generating var- ied, stable and solvable levels for angry birds style physics games. pages 288-295, 08 2017.\n\nThe 2017 AIBIRDS Level Generation Competition. Matthew Stephenson, Jochen Renz, Xiaoyu Ge, Lucas N Ferreira, Julian Togelius, Peng Zhang, IEEE Transactions on Games. Matthew Stephenson, Jochen Renz, Xiaoyu Ge, Lucas N. Ferreira, Julian Togelius, and Peng Zhang. The 2017 AIBIRDS Level Generation Competition. IEEE Transac- tions on Games, PP:1-10, 07 2018.\n\nLearning design patterns with bayesian grammar induction. Jerry Talton, Lingfeng Yang, Ranjitha Kumar, Maxine Lim, Noah Goodman, Radomir Mech, UIST'12 -Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology. 102012Jerry Talton, Lingfeng Yang, Ranjitha Kumar, Maxine Lim, Noah Goodman, and Radomir Mech. Learning design pat- terns with bayesian grammar induction. UIST'12 -Proceed- ings of the 25th Annual ACM Symposium on User Interface Software and Technology, 10 2012.\n\nEfficient object detection in large images using deep reinforcement learning. Burak Uzkent, Christopher Yeh, Stefano Ermon, abs/1912.03966CoRRBurak Uzkent, Christopher Yeh, and Stefano Ermon. Effi- cient object detection in large images using deep reinforce- ment learning. CoRR, abs/1912.03966, 2019.\n\nPlanit: planning and instantiating indoor scenes with relation graph and spatial prior networks. Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel Chang, Daniel Ritchie, ACM Transactions on Graphics. 38Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, An- gel Chang, and Daniel Ritchie. Planit: planning and instanti- ating indoor scenes with relation graph and spatial prior net- works. ACM Transactions on Graphics, 38:1-15, 07 2019.\n\nDeep convolutional priors for indoor scene synthesis. Kai Wang, Manolis Savva, Angel Chang, Daniel Ritchie, ACM Transactions on Graphics. 37Kai Wang, Manolis Savva, Angel Chang, and Daniel Ritchie. Deep convolutional priors for indoor scene synthesis. ACM Transactions on Graphics, 37:1-14, 07 2018.\n\nSample efficient actor-critic with experience replay. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, R\u00e9mi Munos, Koray Kavukcuoglu, Nando De Freitas, abs/1611.01224CoRRZiyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, R\u00e9mi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. CoRR, abs/1611.01224, 2016.\n\nMake it home: Automatic optimization of furniture arrangement. Lap-Fai Yu, Sai Yeung, Chi-Keung Tang, Demetri Terzopoulos, Tony Chan, Stanley Osher, ACM Trans. Graph. 3086Lap-Fai Yu, Sai Yeung, Chi-Keung Tang, Demetri Terzopou- los, Tony Chan, and Stanley Osher. Make it home: Auto- matic optimization of furniture arrangement. ACM Trans. Graph., 30:86, 07 2011.\n\nDeep generative modeling for scene synthesis via hybrid representations. Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth, Etienne Vouga, Qixing Huang, ACM Transactions on Graphics. 39Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth, Etienne Vouga, and Qixing Huang. Deep generative modeling for scene synthesis via hybrid represen- tations. ACM Transactions on Graphics, 39:1-21, 04 2020.\n\nNeural architecture search with reinforcement learning. Barret Zoph, Quoc V Le, abs/1611.01578CoRRBarret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. CoRR, abs/1611.01578, 2016.\n", "annotations": {"author": "[{\"end\":187,\"start\":81},{\"end\":284,\"start\":188},{\"end\":391,\"start\":285}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":90},{\"end\":199,\"start\":194},{\"end\":302,\"start\":295}]", "author_first_name": "[{\"end\":89,\"start\":81},{\"end\":193,\"start\":188},{\"end\":292,\"start\":285},{\"end\":294,\"start\":293}]", "author_affiliation": "[{\"end\":186,\"start\":129},{\"end\":283,\"start\":226},{\"end\":390,\"start\":333}]", "title": "[{\"end\":78,\"start\":1},{\"end\":469,\"start\":392}]", "venue": null, "abstract": "[{\"end\":1592,\"start\":471}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1784,\"start\":1780},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1787,\"start\":1784},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1790,\"start\":1787},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1874,\"start\":1870},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2295,\"start\":2291},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2373,\"start\":2369},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4213,\"start\":4209},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4444,\"start\":4440},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4462,\"start\":4458},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4550,\"start\":4546},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4950,\"start\":4947},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4952,\"start\":4950},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4955,\"start\":4952},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5024,\"start\":5021},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5116,\"start\":5112},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5261,\"start\":5257},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5264,\"start\":5261},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5360,\"start\":5356},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5363,\"start\":5360},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5366,\"start\":5363},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5369,\"start\":5366},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5372,\"start\":5369},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5726,\"start\":5722},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5833,\"start\":5829},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5836,\"start\":5833},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6150,\"start\":6146},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6153,\"start\":6150},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6156,\"start\":6153},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6159,\"start\":6156},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6546,\"start\":6542},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6549,\"start\":6546},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6552,\"start\":6549},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6555,\"start\":6552},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6558,\"start\":6555},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6859,\"start\":6855},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11319,\"start\":11315},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11775,\"start\":11771},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18950,\"start\":18946},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18978,\"start\":18974},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18992,\"start\":18988},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19038,\"start\":19034},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19501,\"start\":19498},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19652,\"start\":19649},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20062,\"start\":20059},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20461,\"start\":20457},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20463,\"start\":20461},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21061,\"start\":21057},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23451,\"start\":23447},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23454,\"start\":23451},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23457,\"start\":23454},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23460,\"start\":23457},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23498,\"start\":23494},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24138,\"start\":24134},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24219,\"start\":24215},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25220,\"start\":25216},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25223,\"start\":25220},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25226,\"start\":25223},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25281,\"start\":25277},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25516,\"start\":25512},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25693,\"start\":25690},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27193,\"start\":27190},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27795,\"start\":27791},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27924,\"start\":27920},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28179,\"start\":28176},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":34002,\"start\":33998},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34046,\"start\":34042},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34097,\"start\":34093}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31817,\"start\":31606},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31938,\"start\":31818},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33811,\"start\":31939},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34561,\"start\":33812}]", "paragraph": "[{\"end\":2565,\"start\":1608},{\"end\":3213,\"start\":2567},{\"end\":3268,\"start\":3215},{\"end\":3541,\"start\":3270},{\"end\":3908,\"start\":3543},{\"end\":4076,\"start\":3910},{\"end\":4200,\"start\":4093},{\"end\":4850,\"start\":4202},{\"end\":5232,\"start\":4852},{\"end\":6032,\"start\":5234},{\"end\":6559,\"start\":6034},{\"end\":6776,\"start\":6570},{\"end\":8016,\"start\":6789},{\"end\":8135,\"start\":8018},{\"end\":8456,\"start\":8137},{\"end\":8829,\"start\":8458},{\"end\":9203,\"start\":8831},{\"end\":9472,\"start\":9225},{\"end\":9916,\"start\":9474},{\"end\":10472,\"start\":9918},{\"end\":11050,\"start\":10515},{\"end\":11209,\"start\":11088},{\"end\":11499,\"start\":11211},{\"end\":11776,\"start\":11568},{\"end\":11994,\"start\":11887},{\"end\":12364,\"start\":11996},{\"end\":12601,\"start\":12392},{\"end\":13254,\"start\":12641},{\"end\":14107,\"start\":13275},{\"end\":16409,\"start\":14109},{\"end\":16520,\"start\":16442},{\"end\":16556,\"start\":16522},{\"end\":16934,\"start\":16577},{\"end\":17287,\"start\":16936},{\"end\":18245,\"start\":17289},{\"end\":18727,\"start\":18323},{\"end\":19151,\"start\":18729},{\"end\":19405,\"start\":19179},{\"end\":19702,\"start\":19432},{\"end\":20767,\"start\":19704},{\"end\":20821,\"start\":20769},{\"end\":21323,\"start\":20823},{\"end\":21545,\"start\":21325},{\"end\":21871,\"start\":21565},{\"end\":22697,\"start\":21873},{\"end\":23322,\"start\":22699},{\"end\":25037,\"start\":23324},{\"end\":25612,\"start\":25039},{\"end\":26114,\"start\":25614},{\"end\":26440,\"start\":26116},{\"end\":27279,\"start\":26461},{\"end\":27560,\"start\":27281},{\"end\":28495,\"start\":27562},{\"end\":29006,\"start\":28524},{\"end\":29370,\"start\":29008},{\"end\":29875,\"start\":29372},{\"end\":30672,\"start\":29890},{\"end\":31163,\"start\":30704},{\"end\":31605,\"start\":31165}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11087,\"start\":11051},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11567,\"start\":11500},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11886,\"start\":11777},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12640,\"start\":12602},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18322,\"start\":18246}]", "table_ref": "[{\"end\":23785,\"start\":23779},{\"end\":24661,\"start\":24655},{\"end\":26113,\"start\":26106},{\"end\":26216,\"start\":26209},{\"end\":28492,\"start\":28486},{\"end\":28806,\"start\":28799},{\"end\":29019,\"start\":29012},{\"end\":29874,\"start\":29867}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1606,\"start\":1594},{\"attributes\":{\"n\":\"2.\"},\"end\":4091,\"start\":4079},{\"attributes\":{\"n\":\"3.\"},\"end\":6568,\"start\":6562},{\"attributes\":{\"n\":\"3.1.\"},\"end\":6787,\"start\":6779},{\"attributes\":{\"n\":\"3.2.\"},\"end\":9223,\"start\":9206},{\"attributes\":{\"n\":\"3.3.\"},\"end\":10513,\"start\":10475},{\"attributes\":{\"n\":\"3.4.\"},\"end\":12390,\"start\":12367},{\"attributes\":{\"n\":\"3.5.\"},\"end\":13273,\"start\":13257},{\"end\":16440,\"start\":16412},{\"attributes\":{\"n\":\"3.6.\"},\"end\":16575,\"start\":16559},{\"attributes\":{\"n\":\"4.\"},\"end\":19177,\"start\":19154},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19430,\"start\":19408},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21563,\"start\":21548},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26459,\"start\":26443},{\"attributes\":{\"n\":\"4.4.\"},\"end\":28522,\"start\":28498},{\"attributes\":{\"n\":\"5.\"},\"end\":29888,\"start\":29878},{\"attributes\":{\"n\":\"5.1.\"},\"end\":30702,\"start\":30675},{\"end\":31617,\"start\":31607},{\"end\":31829,\"start\":31819},{\"end\":33822,\"start\":33813}]", "table": "[{\"end\":33811,\"start\":33108},{\"end\":34561,\"start\":34326}]", "figure_caption": "[{\"end\":31817,\"start\":31619},{\"end\":31938,\"start\":31831},{\"end\":33108,\"start\":31941},{\"end\":34326,\"start\":33824}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19806,\"start\":19800},{\"end\":25611,\"start\":25605},{\"end\":25701,\"start\":25695},{\"end\":26934,\"start\":26926},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28139,\"start\":28133}]", "bib_author_first_name": "[{\"end\":34838,\"start\":34833},{\"end\":34856,\"start\":34852},{\"end\":34876,\"start\":34871},{\"end\":35047,\"start\":35041},{\"end\":35231,\"start\":35227},{\"end\":35247,\"start\":35242},{\"end\":35262,\"start\":35256},{\"end\":35543,\"start\":35538},{\"end\":35559,\"start\":35551},{\"end\":35561,\"start\":35560},{\"end\":35795,\"start\":35790},{\"end\":35813,\"start\":35806},{\"end\":36292,\"start\":36285},{\"end\":36308,\"start\":36301},{\"end\":36319,\"start\":36316},{\"end\":36580,\"start\":36579},{\"end\":36590,\"start\":36589},{\"end\":36599,\"start\":36598},{\"end\":36605,\"start\":36604},{\"end\":36617,\"start\":36616},{\"end\":36900,\"start\":36892},{\"end\":36916,\"start\":36909},{\"end\":36933,\"start\":36926},{\"end\":36953,\"start\":36945},{\"end\":37389,\"start\":37383},{\"end\":37403,\"start\":37399},{\"end\":37419,\"start\":37411},{\"end\":37436,\"start\":37433},{\"end\":37667,\"start\":37659},{\"end\":37680,\"start\":37675},{\"end\":37695,\"start\":37691},{\"end\":37709,\"start\":37708},{\"end\":37711,\"start\":37710},{\"end\":37729,\"start\":37724},{\"end\":37911,\"start\":37908},{\"end\":37913,\"start\":37912},{\"end\":37930,\"start\":37926},{\"end\":37951,\"start\":37946},{\"end\":37963,\"start\":37959},{\"end\":37973,\"start\":37968},{\"end\":37995,\"start\":37988},{\"end\":38008,\"start\":38003},{\"end\":38026,\"start\":38020},{\"end\":38311,\"start\":38307},{\"end\":38321,\"start\":38316},{\"end\":38330,\"start\":38327},{\"end\":38344,\"start\":38337},{\"end\":38357,\"start\":38350},{\"end\":38369,\"start\":38363},{\"end\":38377,\"start\":38375},{\"end\":38389,\"start\":38384},{\"end\":38397,\"start\":38395},{\"end\":38712,\"start\":38705},{\"end\":38723,\"start\":38717},{\"end\":38736,\"start\":38730},{\"end\":38759,\"start\":38752},{\"end\":38774,\"start\":38767},{\"end\":38785,\"start\":38780},{\"end\":38796,\"start\":38792},{\"end\":39045,\"start\":39039},{\"end\":39061,\"start\":39053},{\"end\":39071,\"start\":39068},{\"end\":39405,\"start\":39401},{\"end\":39420,\"start\":39414},{\"end\":39432,\"start\":39428},{\"end\":39806,\"start\":39802},{\"end\":39821,\"start\":39815},{\"end\":39834,\"start\":39829},{\"end\":40263,\"start\":40256},{\"end\":40534,\"start\":40533},{\"end\":40550,\"start\":40545},{\"end\":40945,\"start\":40944},{\"end\":40959,\"start\":40956},{\"end\":41131,\"start\":41124},{\"end\":41144,\"start\":41138},{\"end\":41166,\"start\":41160},{\"end\":41392,\"start\":41388},{\"end\":41407,\"start\":41400},{\"end\":41417,\"start\":41416},{\"end\":41419,\"start\":41418},{\"end\":41682,\"start\":41676},{\"end\":41903,\"start\":41897},{\"end\":41916,\"start\":41908},{\"end\":41932,\"start\":41925},{\"end\":41945,\"start\":41941},{\"end\":41954,\"start\":41953},{\"end\":41971,\"start\":41964},{\"end\":42259,\"start\":42254},{\"end\":42270,\"start\":42264},{\"end\":42281,\"start\":42278},{\"end\":42296,\"start\":42286},{\"end\":42313,\"start\":42308},{\"end\":42325,\"start\":42320},{\"end\":42341,\"start\":42334},{\"end\":42353,\"start\":42346},{\"end\":42366,\"start\":42360},{\"end\":42719,\"start\":42715},{\"end\":42733,\"start\":42729},{\"end\":42750,\"start\":42744},{\"end\":42762,\"start\":42755},{\"end\":42780,\"start\":42773},{\"end\":43071,\"start\":43062},{\"end\":43083,\"start\":43078},{\"end\":43096,\"start\":43084},{\"end\":43109,\"start\":43104},{\"end\":43121,\"start\":43117},{\"end\":43137,\"start\":43130},{\"end\":43152,\"start\":43149},{\"end\":43166,\"start\":43161},{\"end\":43180,\"start\":43175},{\"end\":43549,\"start\":43541},{\"end\":43565,\"start\":43558},{\"end\":43578,\"start\":43574},{\"end\":43587,\"start\":43586},{\"end\":43604,\"start\":43597},{\"end\":43861,\"start\":43855},{\"end\":43871,\"start\":43866},{\"end\":43883,\"start\":43877},{\"end\":43900,\"start\":43891},{\"end\":43912,\"start\":43908},{\"end\":44149,\"start\":44143},{\"end\":44162,\"start\":44159},{\"end\":44174,\"start\":44169},{\"end\":44502,\"start\":44498},{\"end\":44519,\"start\":44513},{\"end\":44535,\"start\":44528},{\"end\":44551,\"start\":44544},{\"end\":44553,\"start\":44552},{\"end\":44568,\"start\":44562},{\"end\":44746,\"start\":44742},{\"end\":44762,\"start\":44757},{\"end\":44779,\"start\":44771},{\"end\":44794,\"start\":44790},{\"end\":44808,\"start\":44804},{\"end\":45111,\"start\":45105},{\"end\":45124,\"start\":45118},{\"end\":45133,\"start\":45129},{\"end\":45145,\"start\":45140},{\"end\":45160,\"start\":45153},{\"end\":45174,\"start\":45168},{\"end\":45441,\"start\":45434},{\"end\":45460,\"start\":45454},{\"end\":45667,\"start\":45660},{\"end\":45686,\"start\":45680},{\"end\":45699,\"start\":45693},{\"end\":45709,\"start\":45704},{\"end\":45711,\"start\":45710},{\"end\":45728,\"start\":45722},{\"end\":45743,\"start\":45739},{\"end\":46034,\"start\":46029},{\"end\":46051,\"start\":46043},{\"end\":46066,\"start\":46058},{\"end\":46080,\"start\":46074},{\"end\":46090,\"start\":46086},{\"end\":46107,\"start\":46100},{\"end\":46558,\"start\":46553},{\"end\":46578,\"start\":46567},{\"end\":46591,\"start\":46584},{\"end\":46878,\"start\":46875},{\"end\":46890,\"start\":46885},{\"end\":46899,\"start\":46896},{\"end\":46918,\"start\":46911},{\"end\":46931,\"start\":46926},{\"end\":46945,\"start\":46939},{\"end\":47280,\"start\":47277},{\"end\":47294,\"start\":47287},{\"end\":47307,\"start\":47302},{\"end\":47321,\"start\":47315},{\"end\":47582,\"start\":47578},{\"end\":47595,\"start\":47589},{\"end\":47610,\"start\":47603},{\"end\":47627,\"start\":47618},{\"end\":47638,\"start\":47634},{\"end\":47651,\"start\":47646},{\"end\":47670,\"start\":47665},{\"end\":47963,\"start\":47956},{\"end\":47971,\"start\":47968},{\"end\":47988,\"start\":47979},{\"end\":48002,\"start\":47995},{\"end\":48020,\"start\":48016},{\"end\":48034,\"start\":48027},{\"end\":48336,\"start\":48330},{\"end\":48351,\"start\":48344},{\"end\":48367,\"start\":48358},{\"end\":48378,\"start\":48372},{\"end\":48393,\"start\":48384},{\"end\":48407,\"start\":48400},{\"end\":48421,\"start\":48415},{\"end\":48750,\"start\":48744},{\"end\":48761,\"start\":48757},{\"end\":48763,\"start\":48762}]", "bib_author_last_name": "[{\"end\":34850,\"start\":34839},{\"end\":34869,\"start\":34857},{\"end\":34882,\"start\":34877},{\"end\":35058,\"start\":35048},{\"end\":35240,\"start\":35232},{\"end\":35254,\"start\":35248},{\"end\":35273,\"start\":35263},{\"end\":35549,\"start\":35544},{\"end\":35568,\"start\":35562},{\"end\":35804,\"start\":35796},{\"end\":35820,\"start\":35814},{\"end\":36299,\"start\":36293},{\"end\":36314,\"start\":36309},{\"end\":36328,\"start\":36320},{\"end\":36587,\"start\":36581},{\"end\":36596,\"start\":36591},{\"end\":36602,\"start\":36600},{\"end\":36614,\"start\":36606},{\"end\":36625,\"start\":36618},{\"end\":36907,\"start\":36901},{\"end\":36924,\"start\":36917},{\"end\":36943,\"start\":36934},{\"end\":36962,\"start\":36954},{\"end\":37397,\"start\":37390},{\"end\":37409,\"start\":37404},{\"end\":37431,\"start\":37420},{\"end\":37443,\"start\":37437},{\"end\":37673,\"start\":37668},{\"end\":37689,\"start\":37681},{\"end\":37706,\"start\":37696},{\"end\":37722,\"start\":37712},{\"end\":37737,\"start\":37730},{\"end\":37924,\"start\":37914},{\"end\":37944,\"start\":37931},{\"end\":37957,\"start\":37952},{\"end\":37966,\"start\":37964},{\"end\":37986,\"start\":37974},{\"end\":38001,\"start\":37996},{\"end\":38018,\"start\":38009},{\"end\":38033,\"start\":38027},{\"end\":38314,\"start\":38312},{\"end\":38325,\"start\":38322},{\"end\":38335,\"start\":38331},{\"end\":38348,\"start\":38345},{\"end\":38361,\"start\":38358},{\"end\":38373,\"start\":38370},{\"end\":38382,\"start\":38378},{\"end\":38393,\"start\":38390},{\"end\":38400,\"start\":38398},{\"end\":38715,\"start\":38713},{\"end\":38728,\"start\":38724},{\"end\":38750,\"start\":38737},{\"end\":38765,\"start\":38760},{\"end\":38778,\"start\":38775},{\"end\":38790,\"start\":38786},{\"end\":38801,\"start\":38797},{\"end\":39051,\"start\":39046},{\"end\":39066,\"start\":39062},{\"end\":39076,\"start\":39072},{\"end\":39412,\"start\":39406},{\"end\":39426,\"start\":39421},{\"end\":39437,\"start\":39433},{\"end\":39813,\"start\":39807},{\"end\":39827,\"start\":39822},{\"end\":39842,\"start\":39835},{\"end\":40290,\"start\":40264},{\"end\":40296,\"start\":40292},{\"end\":40543,\"start\":40535},{\"end\":40557,\"start\":40551},{\"end\":40561,\"start\":40559},{\"end\":40954,\"start\":40946},{\"end\":40966,\"start\":40960},{\"end\":40975,\"start\":40968},{\"end\":41136,\"start\":41132},{\"end\":41158,\"start\":41145},{\"end\":41176,\"start\":41167},{\"end\":41398,\"start\":41393},{\"end\":41414,\"start\":41408},{\"end\":41426,\"start\":41420},{\"end\":41689,\"start\":41683},{\"end\":41906,\"start\":41904},{\"end\":41923,\"start\":41917},{\"end\":41939,\"start\":41933},{\"end\":41951,\"start\":41946},{\"end\":41962,\"start\":41955},{\"end\":41979,\"start\":41972},{\"end\":41987,\"start\":41981},{\"end\":42262,\"start\":42260},{\"end\":42276,\"start\":42271},{\"end\":42284,\"start\":42282},{\"end\":42306,\"start\":42297},{\"end\":42318,\"start\":42314},{\"end\":42332,\"start\":42326},{\"end\":42344,\"start\":42342},{\"end\":42358,\"start\":42354},{\"end\":42375,\"start\":42367},{\"end\":42727,\"start\":42720},{\"end\":42742,\"start\":42734},{\"end\":42753,\"start\":42751},{\"end\":42771,\"start\":42763},{\"end\":42787,\"start\":42781},{\"end\":43076,\"start\":43072},{\"end\":43102,\"start\":43097},{\"end\":43115,\"start\":43110},{\"end\":43128,\"start\":43122},{\"end\":43147,\"start\":43138},{\"end\":43159,\"start\":43153},{\"end\":43173,\"start\":43167},{\"end\":43192,\"start\":43181},{\"end\":43556,\"start\":43550},{\"end\":43572,\"start\":43566},{\"end\":43584,\"start\":43579},{\"end\":43595,\"start\":43588},{\"end\":43612,\"start\":43605},{\"end\":43620,\"start\":43614},{\"end\":43864,\"start\":43862},{\"end\":43875,\"start\":43872},{\"end\":43889,\"start\":43884},{\"end\":43906,\"start\":43901},{\"end\":43916,\"start\":43913},{\"end\":44157,\"start\":44150},{\"end\":44167,\"start\":44163},{\"end\":44178,\"start\":44175},{\"end\":44511,\"start\":44503},{\"end\":44526,\"start\":44520},{\"end\":44542,\"start\":44536},{\"end\":44560,\"start\":44554},{\"end\":44575,\"start\":44569},{\"end\":44755,\"start\":44747},{\"end\":44769,\"start\":44763},{\"end\":44788,\"start\":44780},{\"end\":44802,\"start\":44795},{\"end\":44815,\"start\":44809},{\"end\":45116,\"start\":45112},{\"end\":45127,\"start\":45125},{\"end\":45138,\"start\":45134},{\"end\":45151,\"start\":45146},{\"end\":45166,\"start\":45161},{\"end\":45185,\"start\":45175},{\"end\":45452,\"start\":45442},{\"end\":45465,\"start\":45461},{\"end\":45678,\"start\":45668},{\"end\":45691,\"start\":45687},{\"end\":45702,\"start\":45700},{\"end\":45720,\"start\":45712},{\"end\":45737,\"start\":45729},{\"end\":45749,\"start\":45744},{\"end\":46041,\"start\":46035},{\"end\":46056,\"start\":46052},{\"end\":46072,\"start\":46067},{\"end\":46084,\"start\":46081},{\"end\":46098,\"start\":46091},{\"end\":46112,\"start\":46108},{\"end\":46565,\"start\":46559},{\"end\":46582,\"start\":46579},{\"end\":46597,\"start\":46592},{\"end\":46883,\"start\":46879},{\"end\":46894,\"start\":46891},{\"end\":46909,\"start\":46900},{\"end\":46924,\"start\":46919},{\"end\":46937,\"start\":46932},{\"end\":46953,\"start\":46946},{\"end\":47285,\"start\":47281},{\"end\":47300,\"start\":47295},{\"end\":47313,\"start\":47308},{\"end\":47329,\"start\":47322},{\"end\":47587,\"start\":47583},{\"end\":47601,\"start\":47596},{\"end\":47616,\"start\":47611},{\"end\":47632,\"start\":47628},{\"end\":47644,\"start\":47639},{\"end\":47663,\"start\":47652},{\"end\":47681,\"start\":47671},{\"end\":47966,\"start\":47964},{\"end\":47977,\"start\":47972},{\"end\":47993,\"start\":47989},{\"end\":48014,\"start\":48003},{\"end\":48025,\"start\":48021},{\"end\":48040,\"start\":48035},{\"end\":48342,\"start\":48337},{\"end\":48356,\"start\":48352},{\"end\":48370,\"start\":48368},{\"end\":48382,\"start\":48379},{\"end\":48398,\"start\":48394},{\"end\":48413,\"start\":48408},{\"end\":48427,\"start\":48422},{\"end\":48755,\"start\":48751},{\"end\":48766,\"start\":48764}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":35037,\"start\":34741},{\"attributes\":{\"doi\":\"abs/1803.08375\",\"id\":\"b1\"},\"end\":35223,\"start\":35039},{\"attributes\":{\"id\":\"b2\"},\"end\":35478,\"start\":35225},{\"attributes\":{\"id\":\"b3\"},\"end\":35729,\"start\":35480},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15546282},\"end\":36212,\"start\":35731},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13918064},\"end\":36510,\"start\":36214},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":147229},\"end\":36840,\"start\":36512},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":208921034},\"end\":37317,\"start\":36842},{\"attributes\":{\"id\":\"b8\"},\"end\":37585,\"start\":37319},{\"attributes\":{\"id\":\"b9\"},\"end\":37904,\"start\":37587},{\"attributes\":{\"id\":\"b10\"},\"end\":38236,\"start\":37906},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":225111162},\"end\":38642,\"start\":38238},{\"attributes\":{\"id\":\"b12\"},\"end\":38973,\"start\":38644},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":85502167},\"end\":39325,\"start\":38975},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":54482423},\"end\":39800,\"start\":39327},{\"attributes\":{\"doi\":\"arXiv:1912.04958\",\"id\":\"b15\"},\"end\":40162,\"start\":39802},{\"attributes\":{\"id\":\"b16\"},\"end\":40487,\"start\":40164},{\"attributes\":{\"doi\":\"arxiv:1412\",\"id\":\"b17\",\"matched_paper_id\":6628106},\"end\":40909,\"start\":40489},{\"attributes\":{\"id\":\"b18\"},\"end\":41052,\"start\":40911},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":11790493},\"end\":41352,\"start\":41054},{\"attributes\":{\"id\":\"b20\"},\"end\":41590,\"start\":41354},{\"attributes\":{\"doi\":\"arxiv:1805.00909\",\"id\":\"b21\"},\"end\":41834,\"start\":41592},{\"attributes\":{\"doi\":\"abs/1803.01129\",\"id\":\"b22\"},\"end\":42191,\"start\":41836},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":50773950},\"end\":42650,\"start\":42193},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53246134},\"end\":43006,\"start\":42652},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6875312},\"end\":43503,\"start\":43008},{\"attributes\":{\"doi\":\"abs/1708.05884\",\"id\":\"b26\"},\"end\":43790,\"start\":43505},{\"attributes\":{\"id\":\"b27\"},\"end\":44058,\"start\":43792},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":54434742},\"end\":44462,\"start\":44060},{\"attributes\":{\"doi\":\"abs/1502.05477\",\"id\":\"b29\"},\"end\":44740,\"start\":44464},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b30\"},\"end\":45050,\"start\":44742},{\"attributes\":{\"id\":\"b31\"},\"end\":45349,\"start\":45052},{\"attributes\":{\"id\":\"b32\"},\"end\":45611,\"start\":45351},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":53130314},\"end\":45969,\"start\":45613},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":17007327},\"end\":46473,\"start\":45971},{\"attributes\":{\"doi\":\"abs/1912.03966\",\"id\":\"b35\"},\"end\":46776,\"start\":46475},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":196834522},\"end\":47221,\"start\":46778},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":46997067},\"end\":47522,\"start\":47223},{\"attributes\":{\"doi\":\"abs/1611.01224\",\"id\":\"b38\"},\"end\":47891,\"start\":47524},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":14227},\"end\":48255,\"start\":47893},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":51934625},\"end\":48686,\"start\":48257},{\"attributes\":{\"doi\":\"abs/1611.01578\",\"id\":\"b41\"},\"end\":48897,\"start\":48688}]", "bib_title": "[{\"end\":35536,\"start\":35480},{\"end\":35788,\"start\":35731},{\"end\":36283,\"start\":36214},{\"end\":36577,\"start\":36512},{\"end\":36890,\"start\":36842},{\"end\":38305,\"start\":38238},{\"end\":39037,\"start\":38975},{\"end\":39399,\"start\":39327},{\"end\":40254,\"start\":40164},{\"end\":40531,\"start\":40489},{\"end\":41122,\"start\":41054},{\"end\":42252,\"start\":42193},{\"end\":42713,\"start\":42652},{\"end\":43060,\"start\":43008},{\"end\":44141,\"start\":44060},{\"end\":45658,\"start\":45613},{\"end\":46027,\"start\":45971},{\"end\":46873,\"start\":46778},{\"end\":47275,\"start\":47223},{\"end\":47954,\"start\":47893},{\"end\":48328,\"start\":48257}]", "bib_author": "[{\"end\":34852,\"start\":34833},{\"end\":34871,\"start\":34852},{\"end\":34884,\"start\":34871},{\"end\":35060,\"start\":35041},{\"end\":35242,\"start\":35227},{\"end\":35256,\"start\":35242},{\"end\":35275,\"start\":35256},{\"end\":35551,\"start\":35538},{\"end\":35570,\"start\":35551},{\"end\":35806,\"start\":35790},{\"end\":35822,\"start\":35806},{\"end\":36301,\"start\":36285},{\"end\":36316,\"start\":36301},{\"end\":36330,\"start\":36316},{\"end\":36589,\"start\":36579},{\"end\":36598,\"start\":36589},{\"end\":36604,\"start\":36598},{\"end\":36616,\"start\":36604},{\"end\":36627,\"start\":36616},{\"end\":36909,\"start\":36892},{\"end\":36926,\"start\":36909},{\"end\":36945,\"start\":36926},{\"end\":36964,\"start\":36945},{\"end\":37399,\"start\":37383},{\"end\":37411,\"start\":37399},{\"end\":37433,\"start\":37411},{\"end\":37445,\"start\":37433},{\"end\":37675,\"start\":37659},{\"end\":37691,\"start\":37675},{\"end\":37708,\"start\":37691},{\"end\":37724,\"start\":37708},{\"end\":37739,\"start\":37724},{\"end\":37926,\"start\":37908},{\"end\":37946,\"start\":37926},{\"end\":37959,\"start\":37946},{\"end\":37968,\"start\":37959},{\"end\":37988,\"start\":37968},{\"end\":38003,\"start\":37988},{\"end\":38020,\"start\":38003},{\"end\":38035,\"start\":38020},{\"end\":38316,\"start\":38307},{\"end\":38327,\"start\":38316},{\"end\":38337,\"start\":38327},{\"end\":38350,\"start\":38337},{\"end\":38363,\"start\":38350},{\"end\":38375,\"start\":38363},{\"end\":38384,\"start\":38375},{\"end\":38395,\"start\":38384},{\"end\":38402,\"start\":38395},{\"end\":38717,\"start\":38705},{\"end\":38730,\"start\":38717},{\"end\":38752,\"start\":38730},{\"end\":38767,\"start\":38752},{\"end\":38780,\"start\":38767},{\"end\":38792,\"start\":38780},{\"end\":38803,\"start\":38792},{\"end\":39053,\"start\":39039},{\"end\":39068,\"start\":39053},{\"end\":39078,\"start\":39068},{\"end\":39414,\"start\":39401},{\"end\":39428,\"start\":39414},{\"end\":39439,\"start\":39428},{\"end\":39815,\"start\":39802},{\"end\":39829,\"start\":39815},{\"end\":39844,\"start\":39829},{\"end\":40292,\"start\":40256},{\"end\":40298,\"start\":40292},{\"end\":40545,\"start\":40533},{\"end\":40559,\"start\":40545},{\"end\":40563,\"start\":40559},{\"end\":40956,\"start\":40944},{\"end\":40968,\"start\":40956},{\"end\":40977,\"start\":40968},{\"end\":41138,\"start\":41124},{\"end\":41160,\"start\":41138},{\"end\":41178,\"start\":41160},{\"end\":41400,\"start\":41388},{\"end\":41416,\"start\":41400},{\"end\":41428,\"start\":41416},{\"end\":41691,\"start\":41676},{\"end\":41908,\"start\":41897},{\"end\":41925,\"start\":41908},{\"end\":41941,\"start\":41925},{\"end\":41953,\"start\":41941},{\"end\":41964,\"start\":41953},{\"end\":41981,\"start\":41964},{\"end\":41989,\"start\":41981},{\"end\":42264,\"start\":42254},{\"end\":42278,\"start\":42264},{\"end\":42286,\"start\":42278},{\"end\":42308,\"start\":42286},{\"end\":42320,\"start\":42308},{\"end\":42334,\"start\":42320},{\"end\":42346,\"start\":42334},{\"end\":42360,\"start\":42346},{\"end\":42377,\"start\":42360},{\"end\":42729,\"start\":42715},{\"end\":42744,\"start\":42729},{\"end\":42755,\"start\":42744},{\"end\":42773,\"start\":42755},{\"end\":42789,\"start\":42773},{\"end\":43078,\"start\":43062},{\"end\":43104,\"start\":43078},{\"end\":43117,\"start\":43104},{\"end\":43130,\"start\":43117},{\"end\":43149,\"start\":43130},{\"end\":43161,\"start\":43149},{\"end\":43175,\"start\":43161},{\"end\":43194,\"start\":43175},{\"end\":43558,\"start\":43541},{\"end\":43574,\"start\":43558},{\"end\":43586,\"start\":43574},{\"end\":43597,\"start\":43586},{\"end\":43614,\"start\":43597},{\"end\":43622,\"start\":43614},{\"end\":43866,\"start\":43855},{\"end\":43877,\"start\":43866},{\"end\":43891,\"start\":43877},{\"end\":43908,\"start\":43891},{\"end\":43918,\"start\":43908},{\"end\":44159,\"start\":44143},{\"end\":44169,\"start\":44159},{\"end\":44180,\"start\":44169},{\"end\":44513,\"start\":44498},{\"end\":44528,\"start\":44513},{\"end\":44544,\"start\":44528},{\"end\":44562,\"start\":44544},{\"end\":44577,\"start\":44562},{\"end\":44757,\"start\":44742},{\"end\":44771,\"start\":44757},{\"end\":44790,\"start\":44771},{\"end\":44804,\"start\":44790},{\"end\":44817,\"start\":44804},{\"end\":45118,\"start\":45105},{\"end\":45129,\"start\":45118},{\"end\":45140,\"start\":45129},{\"end\":45153,\"start\":45140},{\"end\":45168,\"start\":45153},{\"end\":45187,\"start\":45168},{\"end\":45454,\"start\":45434},{\"end\":45467,\"start\":45454},{\"end\":45680,\"start\":45660},{\"end\":45693,\"start\":45680},{\"end\":45704,\"start\":45693},{\"end\":45722,\"start\":45704},{\"end\":45739,\"start\":45722},{\"end\":45751,\"start\":45739},{\"end\":46043,\"start\":46029},{\"end\":46058,\"start\":46043},{\"end\":46074,\"start\":46058},{\"end\":46086,\"start\":46074},{\"end\":46100,\"start\":46086},{\"end\":46114,\"start\":46100},{\"end\":46567,\"start\":46553},{\"end\":46584,\"start\":46567},{\"end\":46599,\"start\":46584},{\"end\":46885,\"start\":46875},{\"end\":46896,\"start\":46885},{\"end\":46911,\"start\":46896},{\"end\":46926,\"start\":46911},{\"end\":46939,\"start\":46926},{\"end\":46955,\"start\":46939},{\"end\":47287,\"start\":47277},{\"end\":47302,\"start\":47287},{\"end\":47315,\"start\":47302},{\"end\":47331,\"start\":47315},{\"end\":47589,\"start\":47578},{\"end\":47603,\"start\":47589},{\"end\":47618,\"start\":47603},{\"end\":47634,\"start\":47618},{\"end\":47646,\"start\":47634},{\"end\":47665,\"start\":47646},{\"end\":47683,\"start\":47665},{\"end\":47968,\"start\":47956},{\"end\":47979,\"start\":47968},{\"end\":47995,\"start\":47979},{\"end\":48016,\"start\":47995},{\"end\":48027,\"start\":48016},{\"end\":48042,\"start\":48027},{\"end\":48344,\"start\":48330},{\"end\":48358,\"start\":48344},{\"end\":48372,\"start\":48358},{\"end\":48384,\"start\":48372},{\"end\":48400,\"start\":48384},{\"end\":48415,\"start\":48400},{\"end\":48429,\"start\":48415},{\"end\":48757,\"start\":48744},{\"end\":48768,\"start\":48757}]", "bib_venue": "[{\"end\":36007,\"start\":35923},{\"end\":39580,\"start\":39518},{\"end\":40645,\"start\":40636},{\"end\":34831,\"start\":34741},{\"end\":35588,\"start\":35570},{\"end\":35921,\"start\":35822},{\"end\":36346,\"start\":36330},{\"end\":36661,\"start\":36627},{\"end\":37067,\"start\":36964},{\"end\":37381,\"start\":37319},{\"end\":37657,\"start\":37587},{\"end\":38424,\"start\":38402},{\"end\":38703,\"start\":38644},{\"end\":39137,\"start\":39078},{\"end\":39516,\"start\":39439},{\"end\":39961,\"start\":39860},{\"end\":40321,\"start\":40298},{\"end\":40634,\"start\":40573},{\"end\":40942,\"start\":40911},{\"end\":41185,\"start\":41178},{\"end\":41386,\"start\":41354},{\"end\":41674,\"start\":41592},{\"end\":41895,\"start\":41836},{\"end\":42405,\"start\":42377},{\"end\":42817,\"start\":42789},{\"end\":43238,\"start\":43194},{\"end\":43539,\"start\":43505},{\"end\":43853,\"start\":43792},{\"end\":44249,\"start\":44180},{\"end\":44496,\"start\":44464},{\"end\":44872,\"start\":44833},{\"end\":45103,\"start\":45052},{\"end\":45432,\"start\":45351},{\"end\":45777,\"start\":45751},{\"end\":46209,\"start\":46114},{\"end\":46551,\"start\":46475},{\"end\":46983,\"start\":46955},{\"end\":47359,\"start\":47331},{\"end\":47576,\"start\":47524},{\"end\":48058,\"start\":48042},{\"end\":48457,\"start\":48429},{\"end\":48742,\"start\":48688}]"}}}, "year": 2023, "month": 12, "day": 17}