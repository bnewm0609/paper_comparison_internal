{"id": 3441234, "updated": "2023-02-16 14:40:10.618", "metadata": {"title": "Personalized Response Generation via Domain adaptation", "authors": "[{\"first\":\"Min\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Zhou\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Xiaojun\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jia\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Lianqiang\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Zigang\",\"last\":\"Cao\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "In this paper, we propose a novel personalized response generation model via domain adaptation (PRG-DM). First, we learn the human responding style from large general data (without user-specific information). Second, we fine tune the model on a small size of personalized data to generate personalized responses with a dual learning mechanism. Moreover, we propose three new rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method to generate highly rewarded responses. Experimental results show that our model can generate better personalized responses for different users.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2740309605", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sigir/YangZZ0ZZC17", "doi": "10.1145/3077136.3080706"}}, "content": {"source": {"pdf_hash": "b5615fb37a04c1faaae9868469a7d1f9407befdd", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "36fb41accfb56738c5ca2f413aed656f32d2dc00", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b5615fb37a04c1faaae9868469a7d1f9407befdd.txt", "contents": "\nPersonalized Response Generation via Domain adaptation\n\n\nMin Yang min.yang1129@gmail.com \nZhou Zhao zhaozhou@zju.edu.cn \nWei Zhao Tencent \nXiaojun Chen xjchen@szu.edu.cn \nJia Zhu jzhu@m.scnu.edu.cn \nLianqiang Zhou \nZigang Cao caozigang@iie.ac.cn \n\nTencent AI Lab\nZhejiang University\nShenzhen University\nSouth China\n\n\nTencent AI Lab\nIIE, Chinese Academy of Sciences\nNormal University\n\n\nPersonalized Response Generation via Domain adaptation\n10.1145/3077136.3080706CCS CONCEPTS \u2022 Natural language processing \u2192 Response generation; KEYWORDS Response generationDomain adaptationReinforcement learning\nIn this paper, we propose a novel personalized response generation model via domain adaptation (PRG-DM). First, we learn the human responding style from large general data (without user-speci c information). Second, we ne tune the model on a small size of personalized data to generate personalized responses with a dual learning mechanism. Moreover, we propose three new rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method to generate highly rewarded responses. Experimental results show that our model can generate better personalized responses for di erent users.\n\nINTRODUCTION\n\nConversational system (also called dialogue system) has become increasingly important in a large variety of applications, such as e-commerce, technical support services, entertaining chatbots, and information retrieval dialogue systems. Generally, we have two di erent kinds of conversational systems: goal-oriented and nongoal-oriented (open domain) conversational system. In this paper, we focus on the non-goal-oriented conversational system. Instead of multiple rounds of conversation, we only consider one round of conversation, in which each round is formed by two short texts, with the former being an input (referred to as post) and the latter a response. Hopefully, we expect our work of one round conversation to help understand the intricate mechanism behind the natural language conversation. * Zigang Cao is the corresponding author.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan \u00a9 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org /10.1145/3077136.3080706 Inspired by recent success of recurrent neural network (RNN) in statistical machine translation, most non-goal-oriented conversational systems employ sequence-to-sequence (seq2seq) framework to generate responses [6,7]. In general, the seq2seq model rstly uses an encoder to summarize the post as a vector representation, and then it feeds this representation into a decoder to generate the response. Despite the remarkable progress of existing conversational systems, generating personalized responses still remains a challenge.\n\nSome recent studies [2,[13][14][15] show that the user-speci c information (e.g., identity, age, gender, personal information) is valuable, since it is directly related to the content and the style of user's responses, which may further impact the chatting process and the user experience. Arguably, personalization plays an important role in improving conversational system and making it closer to the user's actual information requirement. However, it is di cult to train a personalized conversational system because the data collected from each individual is often insu cient. In particular, if a user has only a few conversations, the generated responses often have uency or even grammatical problems. One way to solve this problem is to consider a large collection of general training data as a source domain and the personalized data as the target domain, and perform transfer learning from the source domain to the target domain. Namely, we can learn common conversation knowledge from the source domain and then adapts this knowledge to the target user.\n\nTo address the above challenge, in this paper, we propose a novel personalized response generation model via domain adaptation (PRG-DM). Firstly, we pre-train the response generation model with an attention LSTM encoder decoder architecture on a large scale general training data (without user-speci c information). Secondly, we ne-tune the model on a small size of personalized data (with user-speci c information) to generate personalized responses with a dual learning mechanism. Moreover, we propose three new rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method to generate highly rewarded responses.\n\nThe most related work is [14], which uses one single seq2seq model on both the source domain and the target domain. Our approach di ers from [14] in several aspects. First, we generate personalized responses with a dual learning mechanism, which extract promising reward signals for reinforcement learning. Second, in order to generate personalized responses, we formulate the userspeci c information as a vector representation, and then we feed it directly into the LSTM decoder as an additional input. Third, we propose three new rewards rewards (i.e., reconstruction reward and language model reward) to characterize good conversations.\n\n\nRELATED WORK\n\nInspired by recent success of RNNs in statistical machine translation, there are a number of studies attempting to extend the neural language model to the eld of dialogue modeling. [7] and [6] made use of sequence-to-sequence (seq2seq) model to learn response in short conversations.\n\nThere has been increasing interest in applying deep reinforcement learning to dialogue generation [3,5]. For example, [3] proposed a model which learned a policy by optimizing the long-term reward from ongoing dialogue simulations using policy gradient methods [8], rather than the MLE objective de ned in standard seq2seq models. On the other hand, dual learning [9] has been proposed to improve the neural machine translation.\n\nRecently, several studies have been proposed to capture personal characteristics and handle the information consistency of a user [2,5,11,12,14]. [2] integrated the speaker embedding and word embedding into a sequence to sequence learning framework. [14] extended the traditional encoder decoder approach to personalized response generation by introducing a domain adaptation scheme, namely initialization-then-adaptation. [5] proposed a transfer learning framework based on POMDP to learn a personalized dialogue system.\n\n\nPERSONALIZED RESPONSE GENERATION\n\nIn this section, we elaborate our personalized response generation system.\n\nWe use D s to denote the collection of general post-response pairs data (source domain data) that do not contain user speci c information, and use D t to denote the collection of personalized post-response pairs data (target domain data) that contain user speci c information. We use p and r to represent the post and the response, respectively.\n\nSuppose we have two agents (post agent A and response agent B) that can generate responses based on posts and generate posts based on responses, respectively. These two agents are pre-trained on the post-response pairs and response-post pairs in general dataset D s . Our goal is to ne-tune the post agent and the response agent on the personalized dataset D t . Speci cally, we expect to learn common conversation knowledge from the source domain and then adapt this knowledge to the target user.\n\n\nModel Pre-training\n\nLSTM encoder decoder model is originally described in [7], which rst uses an encoder to summarizes the input as a vector representation, then it feeds this representation into a decoder to generate the output.\n3.1.1 LSTM Encoder. The LSTM encoder converts the input sequence x = (w 1 , . . . , w T x ) into a sequence of hidden states h = {h 1 , . . . , h T x }: h t = f (x t , h t \u22121 )(1)\nwhere h t \u2208 R n is a hidden state of LSTM encoder at time t, and we use LSTM as f .\n\nFor long sequences, the last state of the LSTM encoder may not re ect important information seen at the beginning of the sequence. Thus, we adopt the bidirectional LSTM, which runs two chains: one forward through the input and another backward, i.e. reversing the tokens in the input sentence. In step t, we summarize the information in the forward and backward LSTM hidden states by taking the concatenation of the two\nLSTMs h t = [ \u2212 \u2192 h t , \u2190 \u2212 h t ].\n3.1.2 LSTM Decoder. Following [1], we use the attention signals to determine which part of the hidden representation h should be emphasized during the generation process. The LSTM decoder is essentially a LSTM language model except conditional on the context input c = {c 1 , . . . , c T }. The generation probability of the i-th word is calculated by\nP ( i | 1 , . . . , i\u22121 , x) = ( i\u22121 , s i , c i )(2)\nwhere is a LSTM decoder, and s t is the hidden state of LSTM decoder at time t, computed by\ns i = f ( i\u22121 , s i\u22121 , c i )(3)\nThe context vector c i is then computed as a weighted sum of these annotations h t :\nc i = T x t =1 \u03b1 i,t h t(4)\nThe weight \u03b1 i,t of each annotation h t is computed by\n\u03b1 i,t = exp(e i,t ) T x k=1 exp(e i,k ) ; e i,t = \u03c3 (s i\u22121 , h t )(5)\nwhere \u03c3 is a feed-forward neural network, which maps a vector to a real valued score. This attention mechanism \u03b1 i,t models the alignment between the input sequence at position t and the output at position i. We use the post-response pairs and response-post pairs in D s to pre-train the post agent (denote its parameter as \u03b8 A ) and the response agent (denote its parameter as \u03b8 B ) respectively, though the attention LSTM encoder decoder model. The response agent is trained using the same model as that of the post agent, with posts and responses interchanged.\n\n\nModel adaptation\n\nAfter obtaining the post agent and the response agent that learn common conversation knowledge from general dataset D s , in this section, we aim to exploit user-speci c information in personalized dataset D t to improve the performance of our personalized conversational system. Inspired by the success of dual learning in [9], we leverage the duality of the post agent (primal) and the response agent (dual). The primal and dual tasks can form a closed loop, and generate informative feedback signals to train the conversational system, even with a small number of personalized data.\n\nStarting form a post p in D t , we rst generate a middle response s r mid with the post agent, and then further generate the post back with the response agent. By evaluating this two-hop generation results, we will get a sense about the quality of the post agent and the response agent, and be able to improve them accordingly. This process can be iterated for many rounds until both agents converge, via policy gradient. We denote the post agent and the response agent as P (s r mid |p; \u03b8 A ) and P (p|s r mid ; \u03b8 B ), respectively.\n\n\nPersonalized LSTM decoder.\n\nTo generate personalized responses, in model adaptation step, we explicitly calculate the user vector u for each user based on the user-speci c information (e.g., identity, age, gender, personal information), and directly feed it into the LSTM decoder as an additional input. We describe the process of building user vector representation below.\n\nUser Vector Representation . In response generation, user-speci c information (e.g., age, gender, job, education) play an important role in generating personalized responses [10]. We represent each user as a vector representation which encodes user-speci c information. In this project, we mainly employ the following user attributes: User identity, Age (we divide age into ve evenly spaced classes), Gender (female, male or None), Education, Location. We convert these user speci c information into a user vector using a one-hot representation, denoted as u. Since the one-hot representation of the user pro le su ers from data sparsity, we further transform the user pro le vector to a user embedding u e which is low-dimensional and dense by using the matrix-vector product: u e = W e u. The matrix W e \u2208 R d e \u00d7|u | is a parameter to be learned, and the size of each column d e is a hyperparameter. The user pro le vector is directly incorporated into the decoder as an additional input.\n\n\nDual learning with policy gradient .\n\nIn this section, we propose a policy gradient [8] reinforcement learning algorithm to optimize long-term rewards of the post agent and the response agent. We rst introduce the state, action, policy and reward of the reinforcement learning architecture below.\n\n\nState.\n\nIn time t, a state s is denoted by the input sequence of the post agent or the response agent.\n\n\nPolicy.\n\nWe employ the stochastic policy gradient method to approximate a stochastic policy directly using an independent function approximator with its own parameters. In this paper, the policies for the post agent and the response agent are P (s r mid |p; \u03b8 A ) and P (p|s r mid ; \u03b8 B ), whose inputs are the representations of the states, whose outputs are action selection probabilities.\n\n\n3.2.5\n\nAction. An action a in timestep k is the sequence to generate (s r mid or p) by the policy (P (s r mid |p; \u03b8 A ) or P (p|s r mid ; \u03b8 B )).\n\n\n3.2.6\n\nReward. The policy gradient algorithm is a type of reinforcement learning method, which relies upon optimizing parametrized policy with respect to the expected return (long-term cumulative reward) by gradient descent. We assume that our model receives a reward r at each iteration. We discuss the major factors that contribute to the reward for post agent A below, and denote the total reward as r A . Similarly, we can get the reward r B for response agent B in the dual learning procedure.\n\nReconstruction reward. For dual learning, given the generated middle response s r mid by P (s r mid |p; \u03b8 A ), we use the log probability of post p recovered from s r mid by P (p|s r mid ; \u03b8 B ) as the reward of the reconstruction. Mathematically, we de ne the reconstruction reward as r A (r ec ) = log P (p|s r mid ; \u03b8 B ).\n\n\nLanguage model reward.\n\nTo generate grammatical responses, we have an immediate language model reward r (LM ) , indicating how natural the output response is in the language. The language model reward for the response r is:\nr (LM ) A = LM (s r m id )(6)\nWe train the language model with LSTM on Wikipedia data. Then the language model is xed and the log likelihood of a received message is used to reward the output utterance.\n\nFinally, we simply adopt a linear combination of the reconstruction reward r (r ec ) and language model reward r (LM ) as the total reward for post agent A:\nr A = \u03b3 1 r (r ec ) + \u03b3 2 r (LM )(7)\nwhere \u03b3 1 + \u03b3 2 = 1, and we set \u03b3 1 = 0.5 and \u03b3 2 = 0.5.\n\n\nUpdating model parameters.\n\nWe can explore the stateaction space and learn the policies P (s r mid |p; \u03b8 A ) and P (p|s r mid ; \u03b8 B ) that leads to the optimal expected reward E[r ]. According to the policy gradient theorem [8], we compute the gradient of the expected reward E[r ] with respect to parameters \u03b8 A and \u03b8 B :\n\u2207 \u03b8 A E[r ] = E[r A \u2207 \u03b8 A log P (s r mid |p; \u03b8 A )] (8) \u2207 \u03b8 B E[r ] = E[\u03bb 1 \u2207 \u03b8 B log P (p|s r mid ; \u03b8 B )](9)\nThe reader can refer to [8] for details of policy gradient theorem. We update the parameters using stochastic gradient descent.\n\n\nEXPERIMENTS 4.1 Datasets description\n\nIn the experiments, we rst pre-train the attention LSTM encoder decoder using a large scale general training data (without user speci c information), and then ne-tunes the model on the small size of personalized training data (with user speci c information). The detailed properties of the general dataset and the personalized dataset are described as follow. Sina Weibo conversation data without user information (Sinasource): We use the Sina Weibo dataset introduced by [6] as our general training data. On Sina Weibo 1 , a user can post short messages (referred to as posts in this paper) visible to the public or a group of users following her/him, and other users make comments on the published posts ( referred to as responses). This dataset is a collection of 4,435,959 post-response pairs (with 219,905 posts) crawled from Sina Weibo. Sina Weibo conversation data with user information (Sinatarget): We write a crawler to grab tweets from Sina Weibo as our personalized dataset. Following the strategy used in [6], we rst crawl a large number of post-response pairs, and then clean the raw data. Finally, we obtain 51,921 post-response pairs (with 6028 posts). We randomly choose 1000 post-response pairs (with 178 posts) as our test set, and treat the remaining as training set. For each involved user, we crawled his/her pro le information such as user id, province ID user located, city ID user located, user description, gender.\n\nFor both dataset, we use Stanford Chinese word segmenter 2 to split the posts and responses into sequences of words. To reduce data sparsity, we construct two separate vocabularies for posts and responses by using 20,000 most frequent words on each side. All the remaining words are replaced by a special token \"UNK\".\n\n\nBaseline Methods\n\nIn the experiments, we evaluate and compare our model with several baseline methods: Seq2Seq model [7], Neural Responding Machine (NRM) [6], Speaker-Addressee model (SA) [2], Deep Reinforcement Learning (RL) [3], Personalized Response Model (PRM) [14].\n\n\nImplementation details\n\nIn this paper, the word embeddings are initialized using 300-dimensional word2vec [4] trained on Chinese Wikipedia Dumps, and these word embeddings are ne-tuned during model training. We initialized the recurrent parameter matrices as orthogonal matrices, and all other parameters from a Gaussian random distribution with mean zero and standard deviation 0.01. We set the encoder and decoder hidden state space sizes as 300. We conduct mini-batch training with Adadelta optimization method.\n\n\nExperiment results\n\nIn this section, we compare our model with baseline methods from quantitative and qualitative perspectives.\n\n\n4.4.1\n\nantitative evaluation. Following the same evaluation as in [2], we rstly compare our model with baseline methods in terms of BLEU score, perplexity and human evaluation.  BLEU scores has been shown to correlate well with human judgment on the response generation task. Table 1 (second line) shows the BLEU scores. Our PRG-DM model achieves BLEU value of 1.32% on the test data, which is 0.09% higher than the best BLEU value of baseline methods(i.e., RL).\n\nPerplexity is a measurement of how well a probability model predicts a sample text. The lower the perplexity, the better the model. We calculate the average perplexity (log-likelihood) of the responses. We summarize the perplexity results in Table 1 (third  line). PRG-DM substantially outperforms other models.\n\nWe also adopt human annotation to compare the performance of di erent models. We generate the response for each post in the test data. Three researchers who work on natural language processing are invited to do human evaluation. Three levels are assigned to a response with scores from 0 to 2, where 2 denotes suitable response, 1 denotes neutral response, and 0 denotes unsuitable response. The experimental results based on human annotation are summarized in Table 1 (fourth line). PRG-DM outperforms other methods. For example, the average score of PRG-DM is 0.034 higher than the best results of baselines (i.e., PRM) on the test data.  questions in Table 2. We randomly select 5 users from the test dataset, and integrate their user vector representations into our personalized LSTM decoder. The model tends to generate speci c responses for di erent people in response to the factual questions.\n\n\nCONCLUSION AND FUTURE WORK\n\nIn this paper, we proposed a novel personalized response generation model via domain adaptation. We rst pre-trained the response generation model with an attention LSTM encoder decoder architecture on a large scale general training data, and then ne-tuned the model on a small size of personalized data with a dual learning mechanism. In the future, we plan to devote our e ort to adapt our model to task-oriented dialogue systems.\n\nTable 1 :\n1Quantitative evaluation results.\n\n\n4.4.2 alitative evaluation.To evaluate the proposed model qualitatively, we show the generated responses for two real caseID \nConversation 1 \nConversation 2 \nPost \n\u6211\u7684\u4eba\u751f\u53ef\u80fd\u662f\u5047\u7684 \n\u6211\u8981\u6212\u70df,\u4f60\u4fe1\u4e0d\u4fe1? \nMy life may be fake \nI will quit smoking,do you believe? \nResp 1 \n\u6211\u7761\u4e86 \n\u7b49\u7740\u77a7 \nI am sleeping \nWait and see \nResp 2 \n\u6211\u6655\u4e86 \n\u597d\u7684 \nI feel faint \nIt's good \nResp 3 \n\u592a\u6df1\u5965\u4e86 \n\u600e\u4e48\u4f1a? \nIt is too abstruse \nHow could be? \nResp 4 \n\u4e92\u76f8\u5173\u6ce8 \n\u771f\u7684,\u4f60\u786e\u5b9a? \nlet's follow each other \nReally, are you sure? \nResp 5 \n\u7d2f\u5e76\u5feb\u4e50 \n\u54c8\u54c8,\u8981\u6709\u4fe1\u5fc3 \nTired and very happy \nHaha, have con dence \n\n\n\nTable 2 :\n2Some responses generated for ve di erent users to two real case questions.\nhttp://www.weibo.com 2 http://nlp.stanford.edu/software/segmenter.shtml\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473PreprintDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. Preprint arXiv:1409.0473 (2014).\n\nA persona-based neural conversation model. Jiwei Li, Michel Galley, Chris Brockett, P Georgios, Jianfeng Spithourakis, Bill Gao, Dolan, arXiv:1603.06155PreprintJiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao, and Bill Dolan. 2016. A persona-based neural conversation model. Preprint arXiv:1603.06155 (2016).\n\nDeep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky, arXiv:1606.01541PreprintJiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Ju- rafsky. 2016. Deep reinforcement learning for dialogue generation. Preprint arXiv:1606.01541 (2016).\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Je Dean, NIPS. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS. 3111-3119.\n\nPersonalizing a Dialogue System with Transfer Learning. Kaixiang Mo, Shuangyin Li, Yu Zhang, Jiajun Li, Qiang Yang, arXiv:1610.02891PreprintKaixiang Mo, Shuangyin Li, Yu Zhang, Jiajun Li, and Qiang Yang. 2016. Person- alizing a Dialogue System with Transfer Learning. Preprint arXiv:1610.02891 (2016).\n\nNeural Responding Machine for Short-Text Conversation. Lifeng Shang, Zhengdong Lu, Hang Li, Computer Science. Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural Responding Machine for Short-Text Conversation. Computer Science (2015).\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, NIPS. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In NIPS. 3104-3112.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 8Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229-256.\n\nDual Learning for Machine Translation. Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, Wei-Ying Ma, arXiv:1611.00179PreprintYingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual Learning for Machine Translation. Preprint arXiv:1611.00179 (2016).\n\nDiscovering Author Interest Evolution in Topic Modeling. Min Yang, Jincheng Mei, Fei Xu, Wenting Tu, Ziyu Lu, SIGIR. Min Yang, Jincheng Mei, Fei Xu, Wenting Tu, and Ziyu Lu. 2016. Discovering Author Interest Evolution in Topic Modeling. In SIGIR. 801-804.\n\nDeep Markov Neural Network for Sequential Data Classi cation. Min Yang, Wenting Tu, Wenpeng Yin, Ziyu Lu, ACL. 2Min Yang, Wenting Tu, Wenpeng Yin, and Ziyu Lu. 2015. Deep Markov Neural Network for Sequential Data Classi cation. In ACL, Vol. 2. 32-37.\n\nInterest Pro ling for Security Monitoring and Forensic Investigation. Min Yang, Fei Xu, Kam-Pui Chow, ACISP. Min Yang, Fei Xu, and Kam-Pui Chow. 2016. Interest Pro ling for Security Monitoring and Forensic Investigation. In ACISP. 457-464.\n\nMin Yang, Dingju Zhu, Yong Tang, Jingxuan Wang, Authorship Attribution with Topic Drift Model. AAAI. Min Yang, Dingju Zhu, Yong Tang, and Jingxuan Wang. 2017. Authorship Attribution with Topic Drift Model. AAAI (2017).\n\nNeural Personalized Response Generation as Domain Adaptation. Weinan Zhang, Ting Liu, Yifa Wang, Qingfu Zhu, arXiv:1701.02073PreprintWeinan Zhang, Ting Liu, Yifa Wang, and Qingfu Zhu. 2017. Neural Personalized Response Generation as Domain Adaptation. Preprint arXiv:1701.02073 (2017).\n\nExpert nding for question answering via graph regularized matrix completion. Zhou Zhao, Lijun Zhang, Xiaofei He, Wilfred Ng, TKDE. Zhou Zhao, Lijun Zhang, Xiaofei He, and Wilfred Ng. 2015. Expert nding for question answering via graph regularized matrix completion. TKDE (2015).\n", "annotations": {"author": "[{\"end\":90,\"start\":58},{\"end\":121,\"start\":91},{\"end\":139,\"start\":122},{\"end\":171,\"start\":140},{\"end\":199,\"start\":172},{\"end\":215,\"start\":200},{\"end\":247,\"start\":216},{\"end\":316,\"start\":248},{\"end\":385,\"start\":317}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":62},{\"end\":100,\"start\":96},{\"end\":138,\"start\":126},{\"end\":152,\"start\":148},{\"end\":179,\"start\":176},{\"end\":214,\"start\":210},{\"end\":226,\"start\":223}]", "author_first_name": "[{\"end\":61,\"start\":58},{\"end\":95,\"start\":91},{\"end\":125,\"start\":122},{\"end\":147,\"start\":140},{\"end\":175,\"start\":172},{\"end\":209,\"start\":200},{\"end\":222,\"start\":216}]", "author_affiliation": "[{\"end\":315,\"start\":249},{\"end\":384,\"start\":318}]", "title": "[{\"end\":55,\"start\":1},{\"end\":440,\"start\":386}]", "venue": null, "abstract": "[{\"end\":1244,\"start\":598}]", "bib_ref": "[{\"end\":2829,\"start\":2805},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3046,\"start\":3043},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3048,\"start\":3046},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3384,\"start\":3381},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3388,\"start\":3384},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3392,\"start\":3388},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3396,\"start\":3392},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5139,\"start\":5135},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5255,\"start\":5251},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5950,\"start\":5947},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5958,\"start\":5955},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6152,\"start\":6149},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6154,\"start\":6152},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6172,\"start\":6169},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6315,\"start\":6312},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6418,\"start\":6415},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6614,\"start\":6611},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6616,\"start\":6614},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6619,\"start\":6616},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6622,\"start\":6619},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6625,\"start\":6622},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6630,\"start\":6627},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6735,\"start\":6731},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6907,\"start\":6904},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8039,\"start\":8036},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8945,\"start\":8942},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10592,\"start\":10589},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11941,\"start\":11937},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12844,\"start\":12841},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15439,\"start\":15436},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15673,\"start\":15670},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16835,\"start\":16832},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17696,\"start\":17693},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17733,\"start\":17730},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17767,\"start\":17764},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17805,\"start\":17802},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17845,\"start\":17841},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17958,\"start\":17955},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18565,\"start\":18562}]", "figure": "[{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20680,\"start\":20636},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":21211,\"start\":20681},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":21298,\"start\":21212}]", "paragraph": "[{\"end\":2106,\"start\":1260},{\"end\":3359,\"start\":2108},{\"end\":4422,\"start\":3361},{\"end\":5108,\"start\":4424},{\"end\":5749,\"start\":5110},{\"end\":6049,\"start\":5766},{\"end\":6479,\"start\":6051},{\"end\":7002,\"start\":6481},{\"end\":7113,\"start\":7039},{\"end\":7460,\"start\":7115},{\"end\":7959,\"start\":7462},{\"end\":8191,\"start\":7982},{\"end\":8455,\"start\":8372},{\"end\":8876,\"start\":8457},{\"end\":9263,\"start\":8912},{\"end\":9409,\"start\":9318},{\"end\":9527,\"start\":9443},{\"end\":9610,\"start\":9556},{\"end\":10244,\"start\":9681},{\"end\":10850,\"start\":10265},{\"end\":11385,\"start\":10852},{\"end\":11761,\"start\":11416},{\"end\":12754,\"start\":11763},{\"end\":13053,\"start\":12795},{\"end\":13158,\"start\":13064},{\"end\":13552,\"start\":13170},{\"end\":13700,\"start\":13562},{\"end\":14201,\"start\":13710},{\"end\":14528,\"start\":14203},{\"end\":14754,\"start\":14555},{\"end\":14957,\"start\":14785},{\"end\":15115,\"start\":14959},{\"end\":15209,\"start\":15153},{\"end\":15534,\"start\":15240},{\"end\":15773,\"start\":15646},{\"end\":17254,\"start\":15814},{\"end\":17573,\"start\":17256},{\"end\":17846,\"start\":17594},{\"end\":18363,\"start\":17873},{\"end\":18493,\"start\":18386},{\"end\":18958,\"start\":18503},{\"end\":19271,\"start\":18960},{\"end\":20173,\"start\":19273},{\"end\":20635,\"start\":20204}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8371,\"start\":8192},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8911,\"start\":8877},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9317,\"start\":9264},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9442,\"start\":9410},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9555,\"start\":9528},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9680,\"start\":9611},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14784,\"start\":14755},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15152,\"start\":15116},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15645,\"start\":15535}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18793,\"start\":18772},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19223,\"start\":19202},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19755,\"start\":19734},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19934,\"start\":19927}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1258,\"start\":1246},{\"attributes\":{\"n\":\"2\"},\"end\":5764,\"start\":5752},{\"attributes\":{\"n\":\"3\"},\"end\":7037,\"start\":7005},{\"attributes\":{\"n\":\"3.1\"},\"end\":7980,\"start\":7962},{\"attributes\":{\"n\":\"3.2\"},\"end\":10263,\"start\":10247},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":11414,\"start\":11388},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":12793,\"start\":12757},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":13062,\"start\":13056},{\"attributes\":{\"n\":\"3.2.4\"},\"end\":13168,\"start\":13161},{\"end\":13560,\"start\":13555},{\"end\":13708,\"start\":13703},{\"end\":14553,\"start\":14531},{\"attributes\":{\"n\":\"3.2.7\"},\"end\":15238,\"start\":15212},{\"attributes\":{\"n\":\"4\"},\"end\":15812,\"start\":15776},{\"attributes\":{\"n\":\"4.2\"},\"end\":17592,\"start\":17576},{\"attributes\":{\"n\":\"4.3\"},\"end\":17871,\"start\":17849},{\"attributes\":{\"n\":\"4.4\"},\"end\":18384,\"start\":18366},{\"end\":18501,\"start\":18496},{\"attributes\":{\"n\":\"5\"},\"end\":20202,\"start\":20176},{\"end\":20646,\"start\":20637},{\"end\":21222,\"start\":21213}]", "table": "[{\"end\":21211,\"start\":20805}]", "figure_caption": "[{\"end\":20680,\"start\":20648},{\"end\":20805,\"start\":20683},{\"end\":21298,\"start\":21224}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":21450,\"start\":21443},{\"end\":21470,\"start\":21461},{\"end\":21482,\"start\":21476},{\"end\":21725,\"start\":21720},{\"end\":21736,\"start\":21730},{\"end\":21750,\"start\":21745},{\"end\":21762,\"start\":21761},{\"end\":21781,\"start\":21773},{\"end\":21800,\"start\":21796},{\"end\":22075,\"start\":22070},{\"end\":22084,\"start\":22080},{\"end\":22097,\"start\":22093},{\"end\":22112,\"start\":22106},{\"end\":22129,\"start\":22121},{\"end\":22138,\"start\":22135},{\"end\":22434,\"start\":22429},{\"end\":22448,\"start\":22444},{\"end\":22463,\"start\":22460},{\"end\":22474,\"start\":22470},{\"end\":22476,\"start\":22475},{\"end\":22488,\"start\":22486},{\"end\":22739,\"start\":22731},{\"end\":22753,\"start\":22744},{\"end\":22760,\"start\":22758},{\"end\":22774,\"start\":22768},{\"end\":22784,\"start\":22779},{\"end\":23039,\"start\":23033},{\"end\":23056,\"start\":23047},{\"end\":23065,\"start\":23061},{\"end\":23272,\"start\":23268},{\"end\":23289,\"start\":23284},{\"end\":23305,\"start\":23299},{\"end\":23533,\"start\":23532},{\"end\":23774,\"start\":23768},{\"end\":23782,\"start\":23780},{\"end\":23790,\"start\":23787},{\"end\":23801,\"start\":23796},{\"end\":23815,\"start\":23808},{\"end\":23827,\"start\":23820},{\"end\":23841,\"start\":23833},{\"end\":24092,\"start\":24089},{\"end\":24107,\"start\":24099},{\"end\":24116,\"start\":24113},{\"end\":24128,\"start\":24121},{\"end\":24137,\"start\":24133},{\"end\":24354,\"start\":24351},{\"end\":24368,\"start\":24361},{\"end\":24380,\"start\":24373},{\"end\":24390,\"start\":24386},{\"end\":24614,\"start\":24611},{\"end\":24624,\"start\":24621},{\"end\":24636,\"start\":24629},{\"end\":24785,\"start\":24782},{\"end\":24798,\"start\":24792},{\"end\":24808,\"start\":24804},{\"end\":24823,\"start\":24815},{\"end\":25070,\"start\":25064},{\"end\":25082,\"start\":25078},{\"end\":25092,\"start\":25088},{\"end\":25105,\"start\":25099},{\"end\":25370,\"start\":25366},{\"end\":25382,\"start\":25377},{\"end\":25397,\"start\":25390},{\"end\":25409,\"start\":25402}]", "bib_author_last_name": "[{\"end\":21459,\"start\":21451},{\"end\":21474,\"start\":21471},{\"end\":21489,\"start\":21483},{\"end\":21728,\"start\":21726},{\"end\":21743,\"start\":21737},{\"end\":21759,\"start\":21751},{\"end\":21771,\"start\":21763},{\"end\":21794,\"start\":21782},{\"end\":21804,\"start\":21801},{\"end\":21811,\"start\":21806},{\"end\":22078,\"start\":22076},{\"end\":22091,\"start\":22085},{\"end\":22104,\"start\":22098},{\"end\":22119,\"start\":22113},{\"end\":22133,\"start\":22130},{\"end\":22147,\"start\":22139},{\"end\":22442,\"start\":22435},{\"end\":22458,\"start\":22449},{\"end\":22468,\"start\":22464},{\"end\":22484,\"start\":22477},{\"end\":22493,\"start\":22489},{\"end\":22742,\"start\":22740},{\"end\":22756,\"start\":22754},{\"end\":22766,\"start\":22761},{\"end\":22777,\"start\":22775},{\"end\":22789,\"start\":22785},{\"end\":23045,\"start\":23040},{\"end\":23059,\"start\":23057},{\"end\":23068,\"start\":23066},{\"end\":23282,\"start\":23273},{\"end\":23297,\"start\":23290},{\"end\":23308,\"start\":23306},{\"end\":23540,\"start\":23534},{\"end\":23550,\"start\":23542},{\"end\":23778,\"start\":23775},{\"end\":23785,\"start\":23783},{\"end\":23794,\"start\":23791},{\"end\":23806,\"start\":23802},{\"end\":23818,\"start\":23816},{\"end\":23831,\"start\":23828},{\"end\":23844,\"start\":23842},{\"end\":24097,\"start\":24093},{\"end\":24111,\"start\":24108},{\"end\":24119,\"start\":24117},{\"end\":24131,\"start\":24129},{\"end\":24140,\"start\":24138},{\"end\":24359,\"start\":24355},{\"end\":24371,\"start\":24369},{\"end\":24384,\"start\":24381},{\"end\":24393,\"start\":24391},{\"end\":24619,\"start\":24615},{\"end\":24627,\"start\":24625},{\"end\":24641,\"start\":24637},{\"end\":24790,\"start\":24786},{\"end\":24802,\"start\":24799},{\"end\":24813,\"start\":24809},{\"end\":24828,\"start\":24824},{\"end\":25076,\"start\":25071},{\"end\":25086,\"start\":25083},{\"end\":25097,\"start\":25093},{\"end\":25109,\"start\":25106},{\"end\":25375,\"start\":25371},{\"end\":25388,\"start\":25383},{\"end\":25400,\"start\":25398},{\"end\":25412,\"start\":25410}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b0\"},\"end\":21675,\"start\":21372},{\"attributes\":{\"doi\":\"arXiv:1603.06155\",\"id\":\"b1\"},\"end\":22015,\"start\":21677},{\"attributes\":{\"doi\":\"arXiv:1606.01541\",\"id\":\"b2\"},\"end\":22350,\"start\":22017},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16447573},\"end\":22673,\"start\":22352},{\"attributes\":{\"doi\":\"arXiv:1610.02891\",\"id\":\"b4\"},\"end\":22976,\"start\":22675},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7356547},\"end\":23214,\"start\":22978},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7961699},\"end\":23439,\"start\":23216},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2332513},\"end\":23727,\"start\":23441},{\"attributes\":{\"doi\":\"arXiv:1611.00179\",\"id\":\"b8\"},\"end\":24030,\"start\":23729},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16445119},\"end\":24287,\"start\":24032},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":17728778},\"end\":24539,\"start\":24289},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":40925952},\"end\":24780,\"start\":24541},{\"attributes\":{\"id\":\"b12\"},\"end\":25000,\"start\":24782},{\"attributes\":{\"doi\":\"arXiv:1701.02073\",\"id\":\"b13\"},\"end\":25287,\"start\":25002},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15211029},\"end\":25567,\"start\":25289}]", "bib_title": "[{\"end\":22427,\"start\":22352},{\"end\":23031,\"start\":22978},{\"end\":23266,\"start\":23216},{\"end\":23530,\"start\":23441},{\"end\":24087,\"start\":24032},{\"end\":24349,\"start\":24289},{\"end\":24609,\"start\":24541},{\"end\":25364,\"start\":25289}]", "bib_author": "[{\"end\":21461,\"start\":21443},{\"end\":21476,\"start\":21461},{\"end\":21491,\"start\":21476},{\"end\":21730,\"start\":21720},{\"end\":21745,\"start\":21730},{\"end\":21761,\"start\":21745},{\"end\":21773,\"start\":21761},{\"end\":21796,\"start\":21773},{\"end\":21806,\"start\":21796},{\"end\":21813,\"start\":21806},{\"end\":22080,\"start\":22070},{\"end\":22093,\"start\":22080},{\"end\":22106,\"start\":22093},{\"end\":22121,\"start\":22106},{\"end\":22135,\"start\":22121},{\"end\":22149,\"start\":22135},{\"end\":22444,\"start\":22429},{\"end\":22460,\"start\":22444},{\"end\":22470,\"start\":22460},{\"end\":22486,\"start\":22470},{\"end\":22495,\"start\":22486},{\"end\":22744,\"start\":22731},{\"end\":22758,\"start\":22744},{\"end\":22768,\"start\":22758},{\"end\":22779,\"start\":22768},{\"end\":22791,\"start\":22779},{\"end\":23047,\"start\":23033},{\"end\":23061,\"start\":23047},{\"end\":23070,\"start\":23061},{\"end\":23284,\"start\":23268},{\"end\":23299,\"start\":23284},{\"end\":23310,\"start\":23299},{\"end\":23542,\"start\":23532},{\"end\":23552,\"start\":23542},{\"end\":23780,\"start\":23768},{\"end\":23787,\"start\":23780},{\"end\":23796,\"start\":23787},{\"end\":23808,\"start\":23796},{\"end\":23820,\"start\":23808},{\"end\":23833,\"start\":23820},{\"end\":23846,\"start\":23833},{\"end\":24099,\"start\":24089},{\"end\":24113,\"start\":24099},{\"end\":24121,\"start\":24113},{\"end\":24133,\"start\":24121},{\"end\":24142,\"start\":24133},{\"end\":24361,\"start\":24351},{\"end\":24373,\"start\":24361},{\"end\":24386,\"start\":24373},{\"end\":24395,\"start\":24386},{\"end\":24621,\"start\":24611},{\"end\":24629,\"start\":24621},{\"end\":24643,\"start\":24629},{\"end\":24792,\"start\":24782},{\"end\":24804,\"start\":24792},{\"end\":24815,\"start\":24804},{\"end\":24830,\"start\":24815},{\"end\":25078,\"start\":25064},{\"end\":25088,\"start\":25078},{\"end\":25099,\"start\":25088},{\"end\":25111,\"start\":25099},{\"end\":25377,\"start\":25366},{\"end\":25390,\"start\":25377},{\"end\":25402,\"start\":25390},{\"end\":25414,\"start\":25402}]", "bib_venue": "[{\"end\":21441,\"start\":21372},{\"end\":21718,\"start\":21677},{\"end\":22068,\"start\":22017},{\"end\":22499,\"start\":22495},{\"end\":22729,\"start\":22675},{\"end\":23086,\"start\":23070},{\"end\":23314,\"start\":23310},{\"end\":23568,\"start\":23552},{\"end\":23766,\"start\":23729},{\"end\":24147,\"start\":24142},{\"end\":24398,\"start\":24395},{\"end\":24648,\"start\":24643},{\"end\":24881,\"start\":24830},{\"end\":25062,\"start\":25002},{\"end\":25418,\"start\":25414}]"}}}, "year": 2023, "month": 12, "day": 17}