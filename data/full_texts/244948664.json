{"id": 244948664, "updated": "2022-02-15 14:30:24.303", "metadata": {"title": "Robust KPI Anomaly Detection for Large-Scale Software Services with Partial Labels", "authors": "[{\"first\":\"Shenglin\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Chenyu\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Yicheng\",\"last\":\"Sui\",\"middle\":[]},{\"first\":\"Ya\",\"last\":\"Su\",\"middle\":[]},{\"first\":\"Yongqian\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Yuzhi\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Pei\",\"middle\":[]},{\"first\":\"Yizhe\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE)", "journal": "2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "To ensure the reliability of software services, operators collect and monitor a large number of KPI (Key Performance Indicator) streams constantly. KPI anomaly detection is vitally important for software service management. However, none of supervised learning methods, semi-supervised learning methods, transfer learning methods, or unsupervised learning methods achieve accurate anomaly detection for the large-scale, diverse, dynamically changing KPI streams with little labeling effort. In this paper, we propose PUAD, a PU learning-based method, to achieve accurate KPI anomaly detection requiring a few partial labels. It integrates clustering, PU learning, and semisupervised learning to minimize labeling effort and improve anomaly detection accuracy simultaneously. Additionally, we propose a novel active learning method that selects the samples most likely to be positive in each iteration to avoid false alarms. We apply 208 real-world KPI streams collected from a large-scale software service provider to evaluate the performance of PUAD, demonstrating that it achieves a close F1-score to supervised learning methods with much fewer manual labels, and greatly outperforms semi-supervised learning methods, transfer learning methods, and unsupervised learning methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/issre/ZhangZSSSZPW21", "doi": "10.1109/issre52982.2021.00023"}}, "content": {"source": {"pdf_hash": "37e0192fbcdc63196c457a8cabbcd68d506c2d56", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4e6d58f10874ebf52014e2f97b31b55c5d9e2f9e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/37e0192fbcdc63196c457a8cabbcd68d506c2d56.txt", "contents": "\nRobust KPI Anomaly Detection for Large-Scale Software Services with Partial Labels\n\n\nShenglin Zhang \nChenyu Zhao \nYicheng Sui suiyicheng@mail.nankai.edu.cn \nYa Su \nYongqian Sun \nYuzhi Zhang \nDan Pei peidan@tsinghua.edu.cn \nTsinghua University\nBNRist\n\nYizhe Wang wangyizhe2008@126.com \nLanling Information Technology (Shijiazhuang) Co. Ltd\nTianjin Key Laboratory of Operating System\n\n\n\u00b6 \n\nNankai University\nKuaishou Technology, suya@kuaishou.com\n\nRobust KPI Anomaly Detection for Large-Scale Software Services with Partial Labels\n10.1109/ISSRE52982.2021.00023Index Terms-Anomaly detectionMetricsAIOpsPU learn- ingActive learning\nTo ensure the reliability of software services, operators collect and monitor a large number of KPI (Key Performance Indicator) streams constantly. KPI anomaly detection is vitally important for software service management. However, none of supervised learning methods, semi-supervised learning methods, transfer learning methods, or unsupervised learning methods achieve accurate anomaly detection for the large-scale, diverse, dynamically changing KPI streams with little labeling effort. In this paper, we propose PUAD, a PU learning-based method, to achieve accurate KPI anomaly detection requiring a few partial labels. It integrates clustering, PU learning, and semisupervised learning to minimize labeling effort and improve anomaly detection accuracy simultaneously. Additionally, we propose a novel active learning method that selects the samples most likely to be positive in each iteration to avoid false alarms. We apply 208 real-world KPI streams collected from a large-scale software service provider to evaluate the performance of PUAD, demonstrating that it achieves a close F1-score to supervised learning methods with much fewer manual labels, and greatly outperforms semi-supervised learning methods, transfer learning methods, and unsupervised learning methods.\n\nI. INTRODUCTION\n\nWith the rapid development of software technology, software services, e.g., online games, social networks, search engines, have witnessed an explosion in both scale and complexity. To ensure the reliability of software systems, millions of KPI (key performance indicator) streams, including userperceived metrics, e.g., response delay, queries per second (QPS), failure ratio, and system-level metrics, e.g., CPU utilization, memory utilization, network throughput, are constantly monitored and collected at equal-space timestamps [1], [2]. Anomalies in KPI streams usually manifest as spikes, level shifts, ramp up/down and indicate potential service failures, e.g., software bugs, failed updates, network overload, external attacks [3]- [7]. KPI anomaly detection, which aims to find the anomalous behaviors in KPI streams, is vitally important for operators to proactively detect software failures and timely trigger failure diagnosis to mitigate loss. Because today's software services are at large scale and very complex, KPI streams have various shapes and KPI anomalies are highly * Ya Su is the correspondence author. Fig. 1: The manual labels needed for semi-supervised learning methods (upper one) and PU learning methods (lower one) diverse. Moreover, due to the continuous software updates and configuration changes, KPI streams and their anomalies can dramatically change over time [1], [2], [8], [9].\n\nA great number of KPI anomaly detection algorithms have been proposed over the years [6], [7], [10]- [16], which are mainly divided into supervised learning and unsupervised learning methods. Unfortunately, none of them are feasible to deal with the above scenario well: (1) For supervised learning methods, all anomalous and normal samples of each KPI stream in the training set should be manually labeled [10], [12]. Due to the large scale and high diversity of KPI streams, this labeling work, if not impossible, is time-consuming and labor-intensive.\n\n(2) Unsupervised learning methods require no labels. However, they either suffer from low accuracy [17] or require large amounts of training data for each new KPI stream (e.g., six months worth of data) [13]- [16], which cannot be used for the scenario where the patterns of KPI streams dynamically change over time because of software upgrades and/or configuration changes [2], [11].\n\nApart from the above two categories of methods, semisupervised learning methods (e.g., ADS [18]) or transfer learning methods [11] (e.g., ATAD [11]) have also been applied for KPI anomaly detection, both of which enable accurate anomaly detection for a large number of KPI streams, without Fig. 2: Examples of anomalies in KPI streams. The red parts in the KPI stream denote anomalous segments manual algorithm selection or parameter tuning. Moreover, they can be quickly initialized for those KPI streams whose patterns dynamically change over time. However, they still need a large number of KPI streams with high-quality ground truth. Specifically, all the KPI streams (time series segments) in the training set are required to be carefully labeled and periodically updated. Considering the diverse types of KPI streams and frequent updates of their patterns, it is still tedious for operators to examine the KPI streams back and forth periodically.\n\nTo solve the above problem, a natural idea is to randomly label some anomalies manually and learn anomaly patterns from these labels, which is the core idea of PU learning (Positive-Unlabeled learning) [19]. Specifically, for a KPI stream in the training set, PU learning only require labeling part of anomalous segments and does not need to label all the anomalous segments. In this way, operators' manual labeling work is minimized. For example, Fig. 1 shows the manual labels needed with semi-supervised learning/transfer learning and those with PU learning, respectively. We can see that PU learning can greatly reduce the labeling effort compared to semi-supervised learning/transfer learning.\n\nHowever, applying PU learning for KPI anomaly detection faces the following two challenges: (1) KPI streams are large in number and diverse in the pattern. On one hand, if we train a PU learning model for each KPI stream, the overall labeling effort is still very large. On the other hand, if we train a PU learning model for all KPI streams, the model will suffer from low accuracy because different KPI streams have different patterns and thus their most suitable anomaly detectors and parameters can be significantly different.\n\n(2) The performance of PU learning methods is limited due to insufficient labels, and naturally, active learning can be applied. However, the current active learning methods, which usually label anomalous samples near classification boundary [20], [21], tend to cause normal samples to be misclassified as anomalies and thus generate many false alarms.\n\nIn this work, we propose PUAD, a PU learning-based anomaly detection framework, to solve the above challenges and accurately and efficiently detect KPI anomalies with a small number of partial labels. PUAD consists of three major components: (1) Clustering. Clustering KPI streams according to shape similarity. For each cluster, operators manually label some anomalous segments for its centroid KPI stream. Because the number of clusters is much smaller than that of KPI streams, operators' labeling effort is minimized. Moreover, a newly emerging KPI stream can be assigned into one existed cluster based on its shape similarity with each cluster centroid. (2) PU learning. For each cluster centroid, PUAD applies PU learning to build a binary classifier from positive (i.e., anomalous) and unlabeled samples. It then adopts a novel active learning method to obtain reliable positive samples interactively in several iterations. In this way, we can obtain reliable anomalous and normal labels for each cluster centroid KPI stream. (3) Semi-supervised learning. For each KPI stream, we train a semi-supervised learning-based model according to the labels of its cluster centroid KPI stream. Its anomalous behavior will be detected based on this model. The contributions of this paper are summarized as follows:\n\n\u2022 To address the first challenge, PUAD integrates clustering, PU learning, and semi-supervised learning, which together minimize operators' labeling effort and achieve high accuracy simultaneously. \u2022 To address the second challenge, we propose a simple yet effective active learning model, which selects samples that are most likely to be positive in each iteration instead of those that are close to the classification boundary. In this way, a large number of false alarms are avoided. \u2022 We apply 208 real-world KPI streams collected from a large-scale software service provider to evaluate PUAD's performance. PUAD achieves a close F1-score to a wellknown supervised learning method and greatly outperforms two unsupervised learning-based methods, one semi-supervised learning-based method, and one transfer learning-based method by 103.7%, 786.2%, 28.7%, and 482.5%, respectively. To get readers better understand our work, we have made our code publicly available 1 .\n\n\nII. BACKGROUND AND CHALLENGES A. Definitions\n\nIn this section, we define some key terms about anomaly detection. KPI streams: The rapid development of software services has brought great convenience to our daily life. To ensure the reliability of a software service, operators continuously monitor the KPIs (e.g., QPS, error ratio, CPU utilization, network throughput) of every component including microservices, databases, virtual machines, containers, physical machines, etc. A KPI stream is a time series containing the monitoring data of a KPI. It is collected at equal-space timestamps and defined as X = x 1 , x 2 , . . . , x n , where x i is the observation at the ith time point and n is the length of X [22]. Because of the great number of components in large-scale software service and each component have diverse types of KPIs, a great many KPI streams are generated for each software service.\n\nAnomaly: An anomaly in a KPI stream denotes that one of its segments deviates from normal behaviors, such as a spike, a level shift, or a ramp up/down. Fig. 2 shows three examples of anomalies in KPI streams [22]. Usually, different types of KPI streams have different patterns when they become anomalous.\n\nAnomaly detection: Anomaly detection for KPI stream X is to determine whether a segment x i\u2212w+1:i is anomalous (let y i = 1 denotes an anomaly and y i = 0 otherwise), where w is the length of the sliding window required for anomaly detection. For each segment, an anomaly detection method typically computes an anomaly score to indicate its anomalous possibility, and usually a threshold is set to determine whether it is anomalous or not. If its anomaly score exceeds this threshold, it will be regarded as an anomaly.\n\n\nB. Challenges\n\nLarge-scale and diverse KPI streams. If we train a PU learning model for each KPI stream following [7], [10], [13]- [16], too many manual labels are needed because of the large number of KPI streams. However, if we try to train a universal PU learning model for all KPI streams, the model will suffer from low accuracy because KPI streams are highly diverse and it is very difficult, if not impossible, to find the combinations of anomaly detectors and parameters that are suitable for all KPI streams.\n\nActive learning strategy. Naturally, we can adopt active learning to help a PU learning method obtain more normal samples. However, the existing active learning methods usually label samples near the classification boundary in each iteration [20], [21], causing some normal samples to be misclassified as anomalies and thus generating a lot of false alarms. As shown in Fig. 3, when using these active learning methods, it is likely that the PU learning method will mistakenly label some normal samples as anomalies (red circles) and more and more normal samples will be wrongly labeled as anomalies iteratively. We can see that this active learning strategy is likely to generate many false alarms.\n\n\nIII. FRAMEWORK OF PUAD\n\n\nA. The Workflow of PUAD\n\nAs mentioned before, we integrate clustering, PU learning, and semi-supervised learning in PUAD to address the first challenge. The overall workflow of PUAD is shown in Fig. 4. In the offline training process, PUAD clusters historical KPI streams, labels a few potential anomalies, and extracts the features of cluster centroid KPI streams. To obtain more and enough credible labeled samples, PUAD exploits PU learning based on the prior extracted features and labels. For a newly emerging KPI stream A, PUAD firstly assigns it into an existing cluster by comparing its shape similarities with all cluster centroids and then extracts its features. Finally, PUAD trains a model for it through semi-supervised learning with its features as well as the features and the labels of its corresponding cluster centroid.\n\nIn the online detection process, for a newly arrived data point (i.e., x i ) of the KPI stream A, its features would be firstly extracted according to data points in segment x i\u2212w+1:i . Then the features would be fed into the trained model to get an anomaly score for x i\u2212w+1:i . PUAD will determine whether it is an anomaly according to the anomaly threshold. In this paper, we apply the threshold with which the labeled samples in the training set reach the best F1-score.\n\nIn summary, the combination of clustering, PU learning, and semi-supervised learning not only greatly reduces the labeling effort, but also improves the accuracy of anomaly detection. 1) Clustering: After observation on a large number of KPI streams, we find that despite the diversity of KPIs, many of them are similar because of their implicit associations and similarities. If similar KPI streams can be grouped into a few clusters, we can train an anomaly detection model for each cluster using a few manual labels, and \"transfer\" the trained model within each cluster. Let be the number of clusters. Because is significantly smaller than that of KPI streams, we can greatly reduce the labeling overhead. Therefore, we cluster KPI streams and obtain a few labels for each cluster centroid KPI stream, which is the most representative one in the cluster.\n\nPUAD adopts ROCKA [23] for clustering, which has been verified to be a robust and rapid time series clustering algorithm. Besides, it uses SBD (shape-based distance) [24] to measure the time series similarity, and exploits DBSCAN [25] to cluster KPI streams, and chooses the centroid for every cluster. Operators will randomly label a small number of anomalous segments for each cluster centroid. For a new KPI stream, it will be assigned into the most similar cluster by calculating its SBD with these centroid KPI streams.\n\nPlease note that the choice of clustering method is flexible, and applying ROCKA for clustering is not the main contribution of this work.\n\n2) Feature extraction: Motivated by [11], [18], PUAD extracts and categorizes the features into two groups: temporal features and forecasting error features.\n\nTemporal Features: In general, dramatic changes in time series are likely to be anomalous. To indicate the changes of KPI streams in a short period, PUAD extracts 19 temporal features, as shown in Table I. These features are calculated according to a sliding window of data points, i.e., x i\u2212w+1:i .\n\nForecasting Error Features: Following [11], we utilize a set of error indicators generated by time series forecasting methods as features. For a data point, if its actual value differs too much from its prediction, it may be an anomaly generally. PUAD adopts three classical time series prediction techniques, i.e., Holt [26], STL [27], and Holt-Winters [26], to extract three forecasting error features. Through these techniques, PUAD can predict the short-term and long-term trends of a time series, respectively.\n\nIn total, PUAD eventually extracts 22 features through the above methods.\n\n3) PU learning: After extracting the above features, PUAD adopts the feature samples of each cluster centroid as a training set, in which only a few anomalies are labeled. Let \u03b8 be the number of initially labeled samples. Then, the training set consisting of positive samples (i.e., labeled anomalies) and unlabeled samples will be input together into the PU learning component. Let \u03a9(P ), \u03a9(U ), and \u03a9(N ) represent the set of positive samples, unlabeled samples, and negative samples in the training set, respectively. Hereinafter, we use \"positive\" to refer to anomalous, and \"negative\" to refer to normal. Each sample is a feature vector. Section III-B introduces this component in detail.\n\n4) Semi-supervised learning: After obtaining enough labels by PU learning, each cluster centroid currently contains positive samples \u03a9(P ), negative samples \u03a9(N ), and unlabeled  samples \u03a9(U ). A newly emerging KPI stream would be assigned into an existing cluster firstly. Next, we extract its features and treat them as a new unlabeled dataset \u03a9(U new ). Then, we aim to train a model based on the dataset consists of \u03a9(P ), \u03a9(N ), \u03a9(U ), and \u03a9(U new ). In this work, we adopt a semi-supervised learning method called CPLE (Contrastive Pessimistic Likelihood Estimation) [28]. Verified by [18], CPLE is more robust than other semi-supervised learning algorithms because it needs no strong assumptions. Besides, we apply random forest as the base model of CPLE following [18]. After the semi-supervised learning process, PUAD gets an anomaly detection model for each KPI stream. Note that the choice of semi-supervised methods is flexible, and applying CPLE as the semi-supervised method is not the main contribution of our work. [19] improved PU learning method and applied it for time series anomaly detection. Following [19], we utilize PU learning to label anomalies as few as possible in PUAD. Fig. 5 shows an overview of PU learning with two steps. The first step is to initialize the \u03a9(N ) through pre-training. In the second step, positive samples \u03a9(P ) and negative samples \u03a9(N ) are iteratively extended through self-training and active learning.\n\n\nB. PU Learning\n\nStep 1. pre-training. In the beginning, the training set consists of several positive samples (i.e., \u03a9(P )) and a great many unlabeled samples (i.e., \u03a9(U )). To find reliable negative samples more cautiously from \u03a9(U ), employing a linear model as a classifier will be helpful [19]. Note that the choice of the linear model is flexible, and we adopt Elastic Net [29] in this paper, which has been proved to identify possible directional edges even in the presence of highly correlated data. After training the linear model, we can obtain, for each sample in \u03a9(U ), a prediction score indicating the probability of being positive. PUAD selects the top s samples with the lowest scores from \u03a9(U ) to initialize \u03a9(N ) (we set s = 0.20 in our scenario).\n\nStep 2. active learning-based self-training. After pretraining, the training set now consists of a few positive samples (i.e., \u03a9(P )) and negative samples (i.e., \u03a9(N )), and a large number of unlabeled samples (i.e., \u03a9(U )). To make better use of the unlabeled samples, we adopt a self-training method to label more unlabeled samples in \u03a9(U ) iteratively.\n\nAs shown in Fig. 3, we need to ensure that positive samples determined by the self-training method are indeed anomalies. Consequently, we adopt active learning in this self-training step, called active learning-based self-training. Algorithm 1 describes the details of this process.\n\nFirstly, PUAD trains a random forest model classifier on \u03a9(P ) and \u03a9(N ) to obtain the prediction scores of \u03a9(U ) in each iteration, which is shown from lines 1 to 2.\n\nThen, PUAD labels some unlabeled samples iteratively. In each iteration, let \u03bb be the speed denoting the number of newly added unlabeled samples, \u03c0 be the class prior representing the proportion of positive samples labeled by operators. For all iterations, p be the proportion of labeled samples by the PU learning method consisting of pre-training and self-training. PUAD sorts \u03a9(U ) according to the above scores, which is Finally, as shown in Fig. 6, we can guarantee that there are few misclassified negative samples in the set of updated positive samples. These reliable positive samples ensure the accuracy of the anomaly detection model. Evaluated in Section IV-C1, the proposed active learning method significantly improves the accuracy of PUAD.\n\n\nIV. EVALUATION\n\nA. Experimental Design 1) Dataset: We obtained a total of 208 KPI streams from a large-scale software service provider. Moreover, we selected 128 of them for clustering and the other 80 KPI streams as newly emerging KPI streams.\n\nIn PUAD, we apply ROCKA [23] to cluster the 128 historical KPI streams firstly. As evaluated in Section IV-D, is chosen with a higher F1-score, i.e., 9. Then, manually labeling several potential anomalies and extracting the features of the 9 cluster centroids. Finally, we assign the 80 new KPI streams into the existing clusters and extract their features meanwhile. In summary, we have 128 time series for clustering, 9 cluster centroids with random labels, and 80 streams for evaluation (the former 40% with its entire cluster centroid for training, the latter 60% for testing).\n\n2) Evaluation Metrics: PUAD uses a simple strategy following [14]. If any data point in an anomalous segment can be detected by a chosen threshold, it is said that this segment can be detected correctly. The points outside the anomalous segment are treated as normal. The F1-score, precision, and recall can be computed accordingly [11]. We apply this strategy because operators usually do not care about point-wise metrics, and it is acceptable for a method to issue an alarm for any time point in a contiguous anomalous segment when the delay is not too long. The best F1-score indicates the best performance of the model on a specific test set. Therefore, we evaluate the effectiveness of anomaly detection methods using the best F1-score.\n\nIn addition to the F1-score, we also apply Matthew's correlation coefficient (MCC), which takes true negatives into account, to evaluate every methods' effectiveness [30]. The value of MCC ranges from -1 to 1, and a larger value represents higher effectiveness.\n\n3) Baseline Methods: To examine the performance of PUAD, we selected several commonly-used anomaly detection algorithms as baseline methods, summarized in Table III. (1) For supervised learning based methods, Opprentice utilizes the labels to train a random forest classifier. EGADS uses a collection of anomaly detection and forecasting models with an anomaly filtering layer for detection.\n\n(2) For unsupervised learning based methods, Donut [14] and iForest [31] are chosen as baseline methods. Donut is a VAEbased method for seasonal KPI streams with local variations. iForest detects anomalies based on isolation forest.\n\n(3) For semi-supervised learning based methods, ADS [18]. enables the rapid deployment of anomaly detection models for a large number of emerging KPI streams, through clustering and semi-supervised learning. (4) Compared with the methods that also reduce the amount of labeling, we select ATAD [11], which combines transfer learning with active learning and achieves a balance between labeling effort and performance.\n\n\n4) Experimental Settings:\n\nIn the experiment, we set PUAD's hyper-parameters experimentally (more details can   Fig. 7: The effectiveness of different methods be seen in Section IV-D). In the clustering process, we set the cluster radius as 0.05 following ROCKA [15] and set the number of clusters, i.e., = 9. For feature extraction, we set the length of each sliding window, i.e., w = 6. In the PU learning process, we set the number of initially labeled samples, i.e., \u03b8 = 8, the positive class prior, i.e., \u03c0 = 0.015, with an estimation of 0.01, the speed, i.e., \u03bb = 200, the proportion of samples for initializing \u03a9(N ), i.e., s = 0.200, and the overall proportion of labeled samples by PU learning, i.e., p = 0.90. 5) Implementation: PUAD is implemented using Python. We conduct the evaluation experiments on a PowerEdge R730 server with 48 Intel Xeon E5-2650 CPUs and 128GB memory. Fig. 7 shows the average precision, recall, and best F1-score of all 80 new KPI streams using the above seven methods. We can see that both PUAD and Opprentice perform superior in KPI anomaly detection, with the average best F1-score of 0.833 and 0.840, respectively. In contrast, ADS performs a little worse than them whose average best F1-score is 0.647. The performance of the above three models is much better than that of ATAD, Donut, iForest, and EGADS, for the following reasons: (1) The clustering method of ATAD is based on time data points rather than time series, and it only pays attention to KPIs' diversity but ignores the similarity of KPI streams. Besides, ATAD requires a high anomaly labeling ratio (ranges from 1% to 5%), but this ratio is not that high (< %1) in the dataset. (2) Donut is a deep Bayesian model requiring a long period of training data (say six months' worth of data). However, the period of the KPI streams in the dataset is not that long, which affects the effectiveness of Donut. (3) iForest cannot effectively extract features from KPI streams, and it is sensitive to noises, which is not rare in the dataset. (4) EGADS heavily relies on high-quality labels and could be biased towards certain data sets [32]. Furthermore, it proves that PUAD is significantly better than ADS. The reason is that we adopt active learning to check samples and ensure label correctness. It can be seen that the average best F1score of PUAD is close to that of Opprentice, a state-of-the-art supervised model. Fig. 7 shows the average MCC of PUAD and the six baseline methods on the 80 new KPI streams. We can find that the MCCs of PUAD and Opprentice are significantly higher than those of other methods, which is consistent with the evaluation results measured through the F1-score.\n\n\nB. Evaluation of The Overall Performance\n\n\n1) Effectiveness.:\n\n2) Computational efficiency.: The complexity is analyzed by comparing the time required to detect each data point online. When a data point is generated online, PUAD, ADS, ATAD, and Opprentice need to extract its features firstly. The complexity of these algorithms is mainly reflected in the feature extraction process. According to statistics, it takes 0.501s for PUAD, 0.488s for ADS, 3.243s for ATAD, and 0.488s for Opprentice. The prediction time of other baselines is 1.219s for Donut, 0.009s for iForest, and 0.011s for EGADS. Since the data point arrives at an interval of 5 minutes online, each baseline can complete the detection within this time. Furthermore, PUAD can achieve a satisfactory detection result by only labeling a small number of samples, which is competitive considering both effectiveness and labeling cost. \n\n\n3) Generality.:\n\nTo further evaluate the generality of PUAD, we apply an open-source dataset, Numenta anomaly benchmark (NAB) [33], which consists of the time series data collected from different scenarios including AWS, Twitter, and Artificial, etc. All the data points have been manually labeled. We select AWS, Artificial, and Twitter datasets in our experiments as they are large in scale and contain more anomalies than other datasets [11]. The detailed information of the dataset is listed in Table IV. Fig. 8 shows the effectiveness of all methods on the NAB dataset, and PUAD outperforms the six baseline methods in terms of F1-score and MCC. We can observe that when the best F1-scores are achieved, the recall values of PUAD, ADS, Opprentice are high. It is because PUAD, ADS, Opprentice all can easily detect most of the anomalous segments for the following two reasons. (1) The labeled anomalies in the NAB dataset all have significant patterns. (2) As mentioned in Section IV-A2, we apply a practical strategy for calculating precision, recall, and F1score following [11], [14], [15], where if a data point of an anomaly segment labeled by operators is detected by a method, we determine this segment is accurately detected. However, both ADS and Opprentice generate much more false alarms and thus suffer from low precision. Overall, the evaluation experiments demonstrate that PUAD is general to other scenarios.\n\n\nC. Ablation Study\n\nIn this section, we evaluate the effectiveness of active learning, clustering, and semi-supervised learning, respectively, to examine the contributions of the three methods.\n\n1) Active learning: During active learning, we label the samples that are likely to be anomalies instead of those are near the classification boundary. We believe that this strategy is more effective. To evaluate the effectiveness of this strategy, we conduct the following experiments:\n\n\u2022 Remove active learning from PUAD.  Fig. 9: The effectiveness of PUAD, PUAD without clustering (overall training and separate training), and PUAD without semi-supervised learning \u2022 Remove active learning from PUAD, and add the same number of random labels as that used for active learning. \u2022 Label the same number of positive samples (i.e., \u03bb \u00d7 \u03c0) that are near the classification boundary in each iteration during active learning. TABLE V lists the best F1-scores on the nine clusters using these three baseline methods and PUAD. We can observe that active learning indeed improves the effectiveness of PUAD (the second and third columns), and labeling the samples that are likely to be anomalies is more effective than labeling the samples near the classification boundary (the fourth column).\n\n2) Clustering: Utilizing clustering to preprocess the historical KPI streams as the first step because: (1) If training only one model for all KPI streams, the detection result will be inaccurate because of the diversity of KPI streams. (2) If training an anomaly detection model separately for each KPI stream, the overhead of anomaly detection is huge when facing large-scale data.\n\nTo verify the effectiveness of clustering, we design the following two methods:\n\n(1) Train one model for all new KPI streams. We randomly select nine (consistent with the number of clusters in PUAD) of all historical KPI streams as the training set, called overall training. For a new KPI stream, we randomly select one of the nine streams together with its former 40% data points to constitute the training set and apply its latter 60% data points as the testing set.\n\n(2) Train a model for each new KPI stream separately. We utilize the former 40% data points of each new KPI stream to train a model separately and detect anomalies for its latter Fig. 10: The effectiveness of PUAD when replacing its clustering methods with K-medoids and HAC, respectively, and its semi-supervised learning methods with co-training and selftraining, respectively 60% data points called separate training. Fig. 9 shows the best average F1-scores of the above two methods. We find that clustering indeed improves the effectiveness of PUAD in terms of precision, recall, and F1-score. This is because: (1) KPI streams are diverse in the pattern. If we train only one model for all KPI streams, the pattern of the KPI streams in the training set may be completely different from that of the new KPI streams. Therefore, the trained model cannot be used for the new KPI streams. (2) If we train a separate model for each new KPI stream, it is computationally inefficient.\n\nAs mentioned in Section III-A1, the choice of a clustering method for PUAD is flexible. To verify this point, we replace the clustering method, ROCKA [23], with K-medoids [34], and Hierarchical Agglomerative Clustering (HAC) [35], respectively, and evaluate the effectiveness, respectively. The evaluation results are shown in Fig. 10, and we can find that both K-medoids and HAC achieve approximate F1-scores to ROCKA, respectively, showing that PUAD is robust to various clustering methods.\n\n3) Semi-supervised Learning: Each cluster centroid KPI stream consists of labeled and unlabeled samples after PU learning. When a new KPI stream is assigned to a particular cluster, the semi-supervised learning component is applied to train the anomaly detection model. To evaluate the effectiveness of the semi-supervised learning component, we remove it and only utilize the existing labeled samples to train the anomaly detection model. The average best F1-score is shown in Fig. 9. We can find the removal degrades the performance of PUAD because it leads to the unlabeled samples cannot be fully utilized.\n\nAs mentioned in Section III-A4, PUAD is also robust to different semi-supervised learning methods. To demonstrate this point, we replace CPLE, which is the semi-supervised learning method used in PUAD, with co-training [36] and selftraining [37], respectively, and evaluate their effectiveness. As shown in Fig. 10, when co-training or self-training is applied for semi-supervised learning, PUAD achieves close F1-scores to when CPLE is applied. It proves that the choice of semisupervised method for PUAD is indeed flexible.\n\n\nD. Evaluation of Hyper-Parameters\n\nTo evaluate the effectiveness of PUAD's hyper-parameters, we calculate the average best F1-score of PUAD as the values  Fig. 11: The F1-scores of PUAD as its parameters vary of these hyper-parameters vary, as shown in Fig. 11. More specifically, we increase from 3 to 12, w from 4 to 10, \u03b8 from 4 to 18, \u03c0 from 0.005 to 0.025, \u03bb from 200 to 1000, s from 0.05 to 0.40, and p from 0.60 to 0.95. We can find that PUAD achieves approximate average best F1-scores when these hyper-parameters vary, and it is robust to all of these parameters. Specifically, we can observe that the value of \u03c0 does not significantly impact PUAD's performance. Therefore, when fewer positive samples are labeled by operators during active learning, PUAD's performance will not significantly change. Therefore, considering both effectiveness and efficiency, we set their parameters when PUAD achieves relatively higher average best F1-score, i.e., = 9, w = 6, \u03b8 = 8, \u03c0 = 0.015, \u03bb = 200, s = 0.20, p = 0.90.\n\n\nE. Threats to Validity\n\nData quality. In this work, we use two datasets, one is public and another is collected from a real Internet company. The ground truth is based on performance issues and incident reports. Although operators have a wealth of experience, manual labeling of anomalous points may still contain a small degree of noise. We believe that the noise in these labels is only a small percentage. Besides, we adopt widely used evaluation indicators (e.g. [14], [18]) to detect continuous abnormal segments instead of point-wise anomalies, eliminating part of the label noise.\n\nGranularity influence. In our experiment, the granularity of the time series is 5 minutes, but the effectiveness of the algorithm is not affected by the granularity. For fine-grained data, we believe that the algorithm can still work without extra effort. Of course, the amount of data in our experiments is still limited. We will experiment PUAD with a larger scale of datasets on more software service systems in the future.\n\n\nV. RELATED WORK\n\n\nA. Clustering KPI streams\n\nAccording to [38], widely used clustering algorithms can be roughly divided into several groups, including Partitioning, Hierarchical, Grid-based, Model-based, and Density-based algorithms.\n\nHierarchical-based algorithms require that the time series have the same length, and they are difficult to get a satisfactory result [39] when the data length is too long. Partitioning algorithms such as K-Means [40] and K-Medoids [41] need some hyper-parameters like the number of clusters, which are hard to be predetermined definitely. Grid-based methods are rarely used in time series clustering because they either run very slowly or they are inaccurate on large datasets [38]. Model-based methods assume that they include statistical methods (e.g., COBWEB [42]) or neural network methods (e.g., ART [43]) for each cluster and find the data that best fits the models. However, these methods often follow strong assumptions (e.g., a Gaussian mixture can model the time series [44]), which are difficult to apply on complex datasets. Density-based clustering algorithms (e.g., DBSCAN [25]) are carried out according to the density distribution of samples. These algorithms do not need to specify the number of clusters and are more suitable for our scenario.\n\n\nB. PU Learning Method\n\nTraditionally, PU learning is suitable for lots of applications such as text detection [45]- [47] and bioscience [48], [49]. [50] proposes UPTAN (Uncertain Positive Tree Augmented Naive Bayes), a Bayesian network algorithm, to utilize the dependence information among uncertain attributes to create a classifier. [51] views pattern detection through PU learning, analyzes time series to learn relevant indirect contexts, and then creates a probabilistic model.\n\n\nC. Anomaly Detection Algorithms\n\nOver the years, various algorithms have been applied to KPI anomaly detection, including supervised learning, unsupervised learning, and semi-supervised learning methods.\n\nSupervised learning methods: EGADS [12] uses Ad-aBoost [52] to select anomalies, whose architecture mainly includes three components: forecasting, anomaly detection, and alerting. Opprentice [10] ensembles 14 widely-used statistical algorithms to extract features for data points and then trains a classifier using a Random Forest. Such methods need a fullylabel dataset. However, manual labeling for a large number of emerging KPI streams is not feasible.\n\nUnsupervised learning methods: The iForest [31] model assumes that anomalies are few and different, and it constructs a tree structure to separate data points. For the points, the closer to the root of the tree, the more anomalous. Donut [14], Buzz [16] and Bagel [15] are VAE based models, which focus on learning the patterns of normal data rather than anomalies and computing reconstruction probabilities for detection. Although these algorithms do not require data labels, they cannot achieve satisfactory effectiveness.\n\nSemi-supervised learning methods: [53] trained a stacked LSTM network on non-anomalous data and used it as a predictor over a number of time steps. ADS [18] used clustering and CPLE (Contrastive Pessimistic Likelihood Estimation) [28] to detect anomalies in time series. Semi-supervised learning methods use unlabeled data to modify either parameters or models obtained from labeled data alone to maximize the learning performance. They do reduce the cost of manually labeling, however, it is still difficult for operators to label and examine all anomalies in those specified time series segments.\n\n\nVI. CONCLUSION\n\nThis paper proposes PUAD, a PU learning-based method to accurately detect anomalies with a small number of partial labels for large-scale KPI streams. Clustering, PU learning, active learning, and semi-supervised learning are combined to achieve accurate anomaly detection and small labeling effort at the same time. PUAD applies a novel transfer learning strategy to avoid false alarms. Extensive evaluation experiments using real-world KPI streams from a top-tier software service provider demonstrate that PUAD achieves close accuracy to supervised methods, and significantly outperforms existing semi-supervised learning-based, transfer learning-based, and unsupervised learning-based methods. In the future, we will evaluate PUAD in more real-world scenarios and gain more insights from case studies.\n\n\nVII. ACKNOWLEDGMENT\n\nFig. 3 :\n3The labeling processes of an active learning method which labels samples near classification boundary\n\nFig. 4 :\n4The framework of PUAD\n\nFig. 5 :\n5The framework of PU learning and active learning\n\nAlgorithm 1\n1Active-learning-based self-training process Input: training data: \u03a9(P ), \u03a9(U ), \u03a9(N ); \u03c0: positive class prior; \u03bb: speed; p: the proportion of labeled samples by PU learning; s: the proportion of labeled samples by pre-training in PU learning; Output: updated \u03a9(P ), \u03a9(U ), \u03a9(N ); 1: Model M = Random F orest M odel(\u03a9(P ), \u03a9(N )); 2: score = predict(M, \u03a9(U )); 3: \u03a9(N ) = \u03a9(N ); 4: \u03a9(P ) = \u03a9(P ); 5: for |\u03a9(N )|\u2212|\u03a9(N )| < (p\u2212s)\u00d7(1\u2212\u03c0)\u00d7|\u03a9(U )|/\u03bb do 6:I = rank(score) in ascending order; 7:P candidate = {x i | (score(i) > I(end \u2212 \u03bb\u00d7 \u03c0))}; 8:P real , N real = manually check(P candidate );9: N add = {x i | (score(i) < I(\u03bb))}; 10: \u03a9(N ) = \u03a9(N ) \u222a N add \u222a N real ; 11: \u03a9(P ) = \u03a9(P ) \u222a P real 12: \u03a9(U ) = \u03a9(U ) \u2212 N add \u2212 N real \u2212 P real ; 13: Model M = Random F orest M odel(\u03a9(P ), \u03a9(N )); 14: score = predict(M, \u03a9(U )); 15: end for 16: return \u03a9(P ), \u03a9(U ), \u03a9(N ) Fig. 6: Expected results of active learning-based self-trainingshown in line 6. For the unlabeled samples with the highest scores, PUAD sets them as candidate positive samples. Next, operators manually label them to guarantee that the samples labeled as positive are truly positive. For the unlabeled samples with the lowest scores, PUAD labels them as negative samples directly. After that, as shown from lines 10 to 13, PUAD updates \u03a9(P ), \u03a9(N ), and \u03a9(U ), and then start the next iteration until enough unlabeled samples are labeled.\n\n\nThe speed of PU learning (f) The ratio of pre-training (g) The proportion of labeled samples by PU learning\n\nTABLE I :\nITemporal features extracted by PUAD. Long time slope Slope between the current point and the previous long time window. Block delta Difference between two adjacent windows. Block slope Slope between two adjacent windows. Block dping delta Difference between the current point and the previous window. Max KL shift Max shift in Kullback-Leibler divergence between two consecutive windows. LumpinessChanging variance in the remainder.Discretize time series values into ten equalsized intervals. Find maximum run length within the same bucket.Feature \nDescription \nSlope ratio \nSlope between two consecutive points. \nSum ratio \nSlope between two consecutive windows. \n\nCv delta \nDifference of coefficient of variation between \ntwo consecutive windows. \n\nCv slope \nSlope of coefficient of variation between two \nconsecutive windows. \nPing delta \nDifference between two consecutive points. \nSum delta \nDifference between two consecutive windows. \n\nLong time delta \nDifference between the current point and the \nprevious long time window. \n\nShift block dping delta \nShift between the current point and previous \nwindow. \nStd \nStandard deviation in a window. \n\nStd delta \nDifference of standard deviation between two \nconsecutive windows. \n\nMax level shift \nMax trimmed mean between two consecutive \nwindows. \n\nMax var shift \nMax variance shift between two consecutive \nwindows. \n\nFlatspots \n\n\n\nTABLE II :\nIIThe detailed information of the dataset collected from a large-scale software service providerProcess \nnumber of KPI streams \nInterval (minute) \nTotal points \nAnomaly points \nAnomaly ratio (%) \nClustering \n128 \n5 \n1024664 \n8318 \n0.81% \nAnomaly Detection \n80 \n5 \n643593 \n6839 \n1.06% \n\n\n\nTABLE III :\nIIIThe detailed information of the baseline methodsName \nContext \nDataset \nAlgorithm \nPerformance \nLabeling effort \n\nOpprentice \nInternet-based service \nPV, #SR, SRT \nRandom Forest \nrecall\u22650.66, \nprecision\u22650.66 \nFull labels \n\nEGADS \n\nA Universal Anomaly \nDetection System of \nYahoo \n\nYahoo Membership \nLogin (YML) data, \nSynthetic data \n\nA collection of anomaly detection and \nforecasting models with an anomaly \nfiltering layer \n\nbetween 0 and 0.8 \nFull labels \n\nDonut \nInternet-based service \nKPI streams from \nInternet companies \nVAE \nrange from 0.75 to \n0.90 \nNone labels \n\niForest \nGeneral anomaly \ndetection scenario \n\n11 natural datasets, a \nsynthetic dataset \n\nA binary tree structure called \nisolation tree (iTree) \n0.67\u2264AUC\u22641.0 \nNone labels \n\nADS \nInternet-based service \nKPI streams from \nInternet companies \nRocka and CPLE \nthe average best \nF1-score is 0.92 \n\nCluster centroids \nfull labels \n\nATAD \nCloud service systems \nNAB, Yahoo \nCORrelation ALignment(CORAL) \nand active learning \nbetween 0.5 and 1 \n1%-5% labels \n\n\n\nTABLE IV :\nIVThe detailed information of the NAB datasetDataset \nInterval \n(minute) \n\nTotal \npoints \n\nAnomaly \npoints \n\nAnomaly \nRatio (%) \nAWS \n5 \n67740 \n3097 \n4.57% \nArtificial \n5 \n16128 \n624 \n3.87% \nTwitter \n5 \n142765 \n217 \n0.15% \n\nFig. 8: The effectiveness of different methods on the NAB \ndataset \n\n\n\nTABLE V :\nVThe best F1-scores on 9 clustersClusters \nW/o active \nlearning \n\nWith random \nlabels \n\nLabel \nboundary \nPUAD \n\n1 \n0.612 \n0.697 \n0.812 \n0.920 \n2 \n0.545 \n0.576 \n0.667 \n0.967 \n3 \n0.819 \n0.850 \n0.860 \n0.851 \n4 \n0.745 \n0.720 \n0.860 \n0.872 \n5 \n0.596 \n0.714 \n0.785 \n0.839 \n6 \n0.458 \n0.664 \n0.638 \n0.737 \n7 \n0.871 \n0.900 \n0.872 \n0.921 \n8 \n0.714 \n0.762 \n0.793 \n0.812 \n9 \n0.587 \n0.589 \n0.673 \n0.675 \nAverage \n0.636 \n0.719 \n0.772 \n0.833 \nIncrease \nratio \n31.0% \n15.9% \n7.9% \n-\n\n\nhttps://github.com/PUAD-code/PUAD\nWe thank the anonymous reviewers for their valuable feedback. The work was supported by the National Key\nFunnel: Assessing software changes in web-based services. S Zhang, Y Liu, D Pei, Y Chen, X Qu, S Tao, Z Zang, X Jing, M Feng, IEEE Transactions on Services Computing. 111S. Zhang, Y. Liu, D. Pei, Y. Chen, X. Qu, S. Tao, Z. Zang, X. Jing, and M. Feng, \"Funnel: Assessing software changes in web-based services,\" IEEE Transactions on Services Computing, vol. 11, no. 1, pp. 34-48, 2018.\n\nRobust and rapid adaption for concept drift in software system anomaly detection. M Ma, S Zhang, D Pei, X Huang, H Dai, IEEE International Symposium on Software Reliability Engineering (ISSRE). M. Ma, S. Zhang, D. Pei, X. Huang, and H. Dai, \"Robust and rapid adaption for concept drift in software system anomaly detection,\" in IEEE International Symposium on Software Reliability Engineering (ISSRE), 2018.\n\nAutomap: Diagnose your microservice-based web applications automatically. M Ma, J Xu, Y Wang, P Chen, Z Zhang, P Wang, Proceedings of The Web Conference 2020. The Web Conference 2020M. Ma, J. Xu, Y. Wang, P. Chen, Z. Zhang, and P. Wang, \"Automap: Diagnose your microservice-based web applications automatically,\" in Proceedings of The Web Conference 2020, 2020, pp. 246-258.\n\nSelf-adaptive root cause diagnosis for large-scale microservice architecture. M Ma, W Lin, D Pan, P Wang, IEEE Transactions on Services Computing. M. Ma, W. Lin, D. Pan, and P. Wang, \"Self-adaptive root cause diagnosis for large-scale microservice architecture,\" IEEE Transactions on Services Computing, 2020.\n\nMicroscope: Pinpoint performance issues with causal graphs in micro-service environments. J Lin, P Chen, Z Zheng, International Conference on Service-Oriented Computing. SpringerJ. Lin, P. Chen, and Z. Zheng, \"Microscope: Pinpoint performance issues with causal graphs in micro-service environments,\" in International Conference on Service-Oriented Computing. Springer, 2018, pp. 3- 20.\n\nA spatiotemporal deep learning approach for unsupervised anomaly detection in cloud systems. Z He, P Chen, X Li, Y Wang, G Yu, C Chen, X Li, Z Zheng, IEEE Transactions on Neural Networks and Learning Systems. Z. He, P. Chen, X. Li, Y. Wang, G. Yu, C. Chen, X. Li, and Z. Zheng, \"A spatiotemporal deep learning approach for unsupervised anomaly detection in cloud systems,\" IEEE Transactions on Neural Networks and Learning Systems, 2020.\n\nTime-series anomaly detection service at microsoft. H Ren, B Xu, Y Wang, C Yi, C Huang, X Kou, T Xing, M Yang, J Tong, Q Zhang, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningH. Ren, B. Xu, Y. Wang, C. Yi, C. Huang, X. Kou, T. Xing, M. Yang, J. Tong, and Q. Zhang, \"Time-series anomaly detection service at microsoft,\" in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019, pp. 3009- 3017.\n\nGandalf: An intelligent, end-to-end analytics service for safe deployment in large-scale cloud infrastructure. Z Li, Q Cheng, K Hsieh, Y Dang, P Huang, P Singh, X Yang, Q Lin, Y Wu, S Levy, 17th {USENIX} Symposium on Networked Systems Design and Implementation. {NSDI} 20Z. Li, Q. Cheng, K. Hsieh, Y. Dang, P. Huang, P. Singh, X. Yang, Q. Lin, Y. Wu, S. Levy et al., \"Gandalf: An intelligent, end-to-end analytics service for safe deployment in large-scale cloud infrastructure,\" in 17th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 20), 2020, pp. 389-402.\n\nMicroservice-based performance problem detection in cyber-physical system software updates. A Gartziandia, 2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion). IEEEA. Gartziandia, \"Microservice-based performance problem detection in cyber-physical system software updates,\" in 2021 IEEE/ACM 43rd Inter- national Conference on Software Engineering: Companion Proceedings (ICSE-Companion). IEEE, 2021, pp. 147-149.\n\nOpprentice: towards practical and automatic anomaly detection through machine learning. D Liu, Y Zhao, H Xu, Y Sun, D Pei, J Luo, X Jing, M Feng, Proceedings of the 2015 ACM Conference on Internet Measurement Conference. the 2015 ACM Conference on Internet Measurement ConferenceACMD. Liu, Y. Zhao, H. Xu, Y. Sun, D. Pei, J. Luo, X. Jing, and M. Feng, \"Opprentice: towards practical and automatic anomaly detection through machine learning,\" in Proceedings of the 2015 ACM Conference on Internet Measurement Conference. ACM, 2015, pp. 211-224.\n\nCross-dataset time series anomaly detection for cloud systems. X Zhang, Q Lin, Y Xu, S Qin, H Zhang, B Qiao, Y Dang, X Yang, Q Cheng, M Chintalapati, Y Wu, K Hsieh, K Sui, X Meng, Y Xu, W Zhang, F Shen, D Zhang, 2019 USENIX Annual Technical Conference (USENIX ATC 19. Renton, WAUSENIX AssociationX. Zhang, Q. Lin, Y. Xu, S. Qin, H. Zhang, B. Qiao, Y. Dang, X. Yang, Q. Cheng, M. Chintalapati, Y. Wu, K. Hsieh, K. Sui, X. Meng, Y. Xu, W. Zhang, F. Shen, and D. Zhang, \"Cross-dataset time series anomaly detection for cloud systems,\" in 2019 USENIX Annual Technical Conference (USENIX ATC 19). Renton, WA: USENIX Association, Jul. 2019, pp. 1063-1076. [Online]. Available: https://www.usenix.org/conference/atc19/presentation/zhang-xu\n\nGeneric and scalable framework for automated time-series anomaly detection. N Laptev, S Amizadeh, I Flint, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMN. Laptev, S. Amizadeh, and I. Flint, \"Generic and scalable framework for automated time-series anomaly detection,\" in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 1939-1947.\n\nUnsupervised online anomaly detection with parameter adaptation for kpi abrupt changes. G Yu, Z Cai, S Wang, H Chen, F Liu, A Liu, IEEE Transactions on Network and Service Management. 173G. Yu, Z. Cai, S. Wang, H. Chen, F. Liu, and A. Liu, \"Unsupervised online anomaly detection with parameter adaptation for kpi abrupt changes,\" IEEE Transactions on Network and Service Management, vol. 17, no. 3, pp. 1294-1308, 2019.\n\nUnsupervised anomaly detection via variational autoencoder for seasonal kpis in web applications. H Xu, W Chen, N Zhao, Z Li, J Bu, Z Li, Y Liu, Y Zhao, D Pei, Y Feng, Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee. the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering CommitteeH. Xu, W. Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y. Liu, Y. Zhao, D. Pei, Y. Feng et al., \"Unsupervised anomaly detection via variational auto- encoder for seasonal kpis in web applications,\" in Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2018, pp. 187-196.\n\nRobust and unsupervised kpi anomaly detection based on conditional variational autoencoder. Z Li, W Chen, D Pei, 2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC). IEEEZ. Li, W. Chen, and D. Pei, \"Robust and unsupervised kpi anomaly de- tection based on conditional variational autoencoder,\" in 2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC). IEEE, 2018, pp. 1-9.\n\nUnsupervised anomaly detection for intricate kpis via adversarial training of vae. W Chen, H Xu, Z Li, D Peiy, J Chen, H Qiao, Y Feng, Z Wang, IEEE INFOCOM 2019-IEEE Conference on Computer Communications. IEEEW. Chen, H. Xu, Z. Li, D. Peiy, J. Chen, H. Qiao, Y. Feng, and Z. Wang, \"Unsupervised anomaly detection for intricate kpis via adversarial train- ing of vae,\" in IEEE INFOCOM 2019-IEEE Conference on Computer Communications. IEEE, 2019, pp. 1891-1899.\n\nAnomaly detection with partially observed anomalies. Y.-L Zhang, L Li, J Zhou, X Li, Z.-H Zhou, Companion of the The Web Conference 2018 on The Web Conference 2018. International World Wide Web Conferences Steering Committee. Y.-L. Zhang, L. Li, J. Zhou, X. Li, and Z.-H. Zhou, \"Anomaly detection with partially observed anomalies,\" in Companion of the The Web Conference 2018 on The Web Conference 2018. International World Wide Web Conferences Steering Committee, 2018, pp. 639-646.\n\nRapid deployment of anomaly detection models for large number of emerging kpi streams. J Bu, Y Liu, S Zhang, W Meng, Q Liu, X Zhu, D Pei, 2018 IEEE 37th International Performance Computing and Communications Conference. J. Bu, Y. Liu, S. Zhang, W. Meng, Q. Liu, X. Zhu, and D. Pei, \"Rapid deployment of anomaly detection models for large number of emerging kpi streams,\" in 2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC), 2018, pp. 1-8.\n\nPositive and unlabeled learning for anomaly detection with multi-features. J Zhang, Z Wang, J Yuan, Y.-P Tan, 10.1145/3123266.3123304Proceedings of the 25th ACM International Conference on Multimedia, ser. MM '17. the 25th ACM International Conference on Multimedia, ser. MM '17New York, NY, USAAssociation for Computing MachineryJ. Zhang, Z. Wang, J. Yuan, and Y.-P. Tan, \"Positive and unlabeled learning for anomaly detection with multi-features,\" in Proceedings of the 25th ACM International Conference on Multimedia, ser. MM '17. New York, NY, USA: Association for Computing Machinery, 2017, p. 854-862. [Online]. Available: https://doi.org/10.1145/3123266.3123304\n\nActive learning for multivariate time series classification with positive unlabeled data. G He, Y Duan, Y Li, T Qian, J He, X Jia, 2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI). G. He, Y. Duan, Y. Li, T. Qian, J. He, and X. Jia, \"Active learning for multivariate time series classification with positive unlabeled data,\" in 2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI), 2015, pp. 178-185.\n\nActive decision boundary annotation with deep generative models. M Huijser, J C Van Gemert, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionM. Huijser and J. C. van Gemert, \"Active decision boundary annotation with deep generative models,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 5286-5295.\n\nLabel-less: A semi-automatic labelling tool for kpi anomalies. N Zhao, J Zhu, R Liu, D Liu, M Zhang, D Pei, IEEE INFOCOM 2019 -IEEE Conference on Computer Communications. N. Zhao, J. Zhu, R. Liu, D. Liu, M. Zhang, and D. Pei, \"Label-less: A semi-automatic labelling tool for kpi anomalies,\" in IEEE INFOCOM 2019 -IEEE Conference on Computer Communications, 2019, pp. 1882- 1890.\n\nRobust and rapid clustering of kpis for large-scale anomaly detection. Z Li, Y Zhao, R Liu, D Pei, Quality of Service (IWQoS). Z. Li, Y. Zhao, R. Liu, and D. Pei, \"Robust and rapid clustering of kpis for large-scale anomaly detection,\" Quality of Service (IWQoS), pp. 1-10, 2018.\n\nk-shape: Efficient and accurate clustering of time series. J Paparrizos, L Gravano, Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data. the 2015 ACM SIGMOD International Conference on Management of DataACMJ. Paparrizos and L. Gravano, \"k-shape: Efficient and accurate clustering of time series,\" in Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data. ACM, 2015, pp. 1855-1870.\n\nRealtime superpixel segmentation by dbscan clustering algorithm. J Shen, X Hao, Z Liang, Y Liu, W Wang, L Shao, IEEE Transactions on Image Processing. 25J. Shen, X. Hao, Z. Liang, Y. Liu, W. Wang, and L. Shao, \"Real- time superpixel segmentation by dbscan clustering algorithm,\" IEEE Transactions on Image Processing, vol. 25, pp. 5933-5942, 2016.\n\nInitialization methods for multiple seasonal holt-winters forecasting models. O Trull, J C Garc\u00eda-D\u00edaz, A Troncoso, Mathematics. 82268O. Trull, J. C. Garc\u00eda-D\u00edaz, and A. Troncoso, \"Initialization methods for multiple seasonal holt-winters forecasting models,\" Mathematics, vol. 8, no. 2, p. 268, 2020.\n\nFeature-based comparison and generation of time series. L Kegel, M Hahmann, W Lehner, 10.1145/3221269.3221293Proceedings of the 30th International Conference on Scientific and Statistical Database Management, ser. SSDBM '18. the 30th International Conference on Scientific and Statistical Database Management, ser. SSDBM '18New York, NY, USAAssociation for Computing MachineryL. Kegel, M. Hahmann, and W. Lehner, \"Feature-based comparison and generation of time series,\" in Proceedings of the 30th International Conference on Scientific and Statistical Database Management, ser. SSDBM '18. New York, NY, USA: Association for Computing Machinery, 2018. [Online]. Available: https://doi.org/10.1145/3221269. 3221293\n\nContrastive pessimistic likelihood estimation for semisupervised classification. M Loog, IEEE transactions on pattern analysis and machine intelligence. 38M. Loog, \"Contrastive pessimistic likelihood estimation for semi- supervised classification,\" IEEE transactions on pattern analysis and machine intelligence, vol. 38, no. 3, pp. 462-475, 2016.\n\nNetwork topology inference via elastic net structural equation models. P A Traganitis, Y Shen, G Giannakis, 25th European Signal Processing Conference (EUSIPCO). P. A. Traganitis, Y. Shen, and G. Giannakis, \"Network topology infer- ence via elastic net structural equation models,\" 2017 25th European Signal Processing Conference (EUSIPCO), pp. 146-150, 2017.\n\nSentiment analysis based error detection for large-scale systems. K A Alharthi, A Jhumka, S Di, F Cappello, E Chuah, 51K. A. Alharthi, A. Jhumka, S. Di, F. Cappello, and E. Chuah, \"Sentiment analysis based error detection for large-scale systems,\" in 2021 51st\n\nAnnual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 2021, pp. 237-249.\n\nMultiple features and isolation forest-based fast anomaly detector for hyperspectral imagery. R Wang, F Nie, Z Wang, F He, X Li, IEEE Transactions on Geoscience and Remote Sensing. 589R. Wang, F. Nie, Z. Wang, F. He, and X. Li, \"Multiple features and isolation forest-based fast anomaly detector for hyperspectral imagery,\" IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no. 9, pp. 6664-6676, 2020.\n\nDeepAD: A Generic Framework Based on Deep Learning for Time Series Anomaly Detection. T Buda, B Caglayan, H Assem, T. Buda, B. Caglayan, and H. Assem, DeepAD: A Generic Framework Based on Deep Learning for Time Series Anomaly Detection, 06 2018, pp. 577-588.\n\nEvaluating real-time anomaly detection algorithms -the numenta anomaly benchmark. A Lavin, S Ahmad, 2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA). A. Lavin and S. Ahmad, \"Evaluating real-time anomaly detection algorithms -the numenta anomaly benchmark,\" 2015 IEEE 14th Inter- national Conference on Machine Learning and Applications (ICMLA), pp. 38-44, 2015.\n\nBanditpam: Almost linear time k-medoids clustering via multi-armed bandits. M Tiwari, M J Zhang, J Mayclin, S Thrun, C Piech, I Shomorony, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33M. Tiwari, M. J. Zhang, J. Mayclin, S. Thrun, C. Piech, and I. Shomorony, \"Banditpam: Almost linear time k-medoids clustering via multi-armed bandits,\" in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 10 211- 10 222. [Online]. Available: https://proceedings.neurips.cc/paper/2020/ file/73b817090081cef1bca77232f4532c5d-Paper.pdf\n\nObjective-based hierarchical clustering of deep embedding vectors. S Naumov, G Yaroslavtsev, D Avdiukhin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35S. Naumov, G. Yaroslavtsev, and D. Avdiukhin, \"Objective-based hierarchical clustering of deep embedding vectors,\" Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 10, pp. 9055-9063, May 2021. [Online]. Available: https://ojs.aaai.org/index. php/AAAI/article/view/17094\n\nInductive semi-supervised multilabel learning with co-training. W Zhan, M.-L Zhang, 10.1145/3097983.3098141Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD'17. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD'17New York, NY, USAAssociation for Computing MachineryW. Zhan and M.-L. Zhang, \"Inductive semi-supervised multi- label learning with co-training,\" in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD'17. New York, NY, USA: Association for Computing Machinery, 2017, p. 1305-1314. [Online]. Available: https://doi.org/10.1145/3097983.3098141\n\nMist: Multiple instance selftraining framework for video anomaly detection. J.-C Feng, F.-T Hong, W.-S Zheng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)J.-C. Feng, F.-T. Hong, and W.-S. Zheng, \"Mist: Multiple instance self- training framework for video anomaly detection,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021, pp. 14 009-14 018.\n\nTime-series clustering -a decade review. S Aghabozorgi, A Seyed Shirkhorshidi, T Ying Wah, Information Systems. 53S. Aghabozorgi, A. Seyed Shirkhorshidi, and T. Ying Wah, \"Time-series clustering -a decade review,\" Information Systems, vol. 53, pp. 16 -38, 2015. [Online]. Available: http://www.sciencedirect.com/science/ article/pii/S0306437915000733\n\nCurrency movement forecasting using time series analysis and long short-term memory. K Putri, S Halim, K. Putri and S. Halim, \"Currency movement forecasting using time series analysis and long short-term memory,\" 2020.\n\nEfficient registration of multi-view point sets by k-means clustering. J Zhu, Z Jiang, G D Evangelidis, C Zhang, S Pang, Z Li, Information Sciences. 488J. Zhu, Z. Jiang, G. D. Evangelidis, C. Zhang, S. Pang, and Z. Li, \"Efficient registration of multi-view point sets by k-means clustering,\" Information Sciences, vol. 488, pp. 205-218, 2019. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0020025519302221\n\nComparison of kmeans clustering method and k-medoids on twitter data. C Oktarina, K A Notodiputro, I Indahwati, Indonesian Journal of Statistics and Its Applications. 41C. Oktarina, K. A. Notodiputro, and I. Indahwati, \"Comparison of k- means clustering method and k-medoids on twitter data,\" Indonesian Journal of Statistics and Its Applications, vol. 4, no. 1, pp. 189-202, 2020.\n\nKnowledge acquisition via incremental conceptual clustering. D Fisher, Machine Learning. 2D. Fisher, \"Knowledge acquisition via incremental conceptual cluster- ing,\" Machine Learning, vol. 2, pp. 139-172, 09 1987.\n\nA massively parallel architecture for a self-organizing neural pattern recognition machine. G A Carpenter, S Grossberg, Computer Vision, Graphics, and Image Processing. 37G. A. Carpenter and S. Grossberg, \"A massively parallel architecture for a self-organizing neural pattern recognition machine,\" Computer Vision, Graphics, and Image Processing, vol. 37, no. 1, pp. 54 -115, 1987. [Online]. Available: http://www.sciencedirect.com/science/article/ pii/S0734189X87800142\n\nVariable selection for model-based clustering using the integrated complete-data likelihood. M Marbac, M Sedki, Statistics and Computing. 27M. Marbac and M. Sedki, \"Variable selection for model-based clustering using the integrated complete-data likelihood,\" Statistics and Computing, vol. 27, pp. 1049-1063, 2017.\n\nBuilding text classifiers using positive, unlabeled and 'outdated'examples. J Han, W Zuo, L Liu, Y Xu, T Peng, Concurrency and Computation: Practice and Experience. 2813J. Han, W. Zuo, L. Liu, Y. Xu, and T. Peng, \"Building text classifiers using positive, unlabeled and 'outdated'examples,\" Concurrency and Computation: Practice and Experience, vol. 28, no. 13, pp. 3691-3706, 2016.\n\nA robust auc maximization framework with simultaneous outlier detection and feature selection for positive-unlabeled classification. K Ren, H Yang, Y Zhao, W Chen, M Xue, H Miao, S Huang, J Liu, IEEE transactions on neural networks and learning systems. 30K. Ren, H. Yang, Y. Zhao, W. Chen, M. Xue, H. Miao, S. Huang, and J. Liu, \"A robust auc maximization framework with simultaneous outlier detection and feature selection for positive-unlabeled classification,\" IEEE transactions on neural networks and learning systems, vol. 30, no. 10, pp. 3072-3083, 2018.\n\nClassifying networked text data with positive and unlabeled examples. M Li, S Pan, Y Zhang, X Cai, Pattern Recognition Letters. 77M. Li, S. Pan, Y. Zhang, and X. Cai, \"Classifying networked text data with positive and unlabeled examples,\" Pattern Recognition Letters, vol. 77, pp. 1-7, 2016.\n\nA robust ensemble approach to learn from positive and unlabeled data using svm base models. M Claesen, F De, J A Smet, B Suykens, De Moor, Neurocomputing. 160M. Claesen, F. De Smet, J. A. Suykens, and B. De Moor, \"A robust ensemble approach to learn from positive and unlabeled data using svm base models,\" Neurocomputing, vol. 160, pp. 73-84, 2015.\n\nLearning from proportions of positive and unlabeled examples. J Hern\u00e1ndez-Gonz\u00e1lez, I Inza, J A Lozano, International Journal of Intelligent Systems. 322J. Hern\u00e1ndez-Gonz\u00e1lez, I. Inza, and J. A. Lozano, \"Learning from proportions of positive and unlabeled examples,\" International Journal of Intelligent Systems, vol. 32, no. 2, pp. 109-133, 2017.\n\nBayesian belief network for positive unlabeled learning with uncertainty. H Gan, Y Zhang, Q Song, Pattern Recognition Letters. 90H. Gan, Y. Zhang, and Q. Song, \"Bayesian belief network for posi- tive unlabeled learning with uncertainty,\" Pattern Recognition Letters, vol. 90, pp. 28-35, 2017.\n\nNow you see it, now you don't!\" Detecting Suspicious Pattern Absences in Continuous Time Series. V Vercruyssen, W Meert, J Davis, V. Vercruyssen, W. Meert, and J. Davis, \"Now you see it, now you don't!\" Detecting Suspicious Pattern Absences in Continuous Time Series, 01 2020, pp. 127-135.\n\nBinary ensemble neural network: More bits per network or more networks per bit. S Zhu, X Dong, H Su, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). S. Zhu, X. Dong, and H. Su, \"Binary ensemble neural network: More bits per network or more networks per bit?\" 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4918-4927, 2019.\n\nLong short term memory networks for anomaly detection in time series. P Malhotra, L Vig, G Shroff, P Agarwal, ESANNP. Malhotra, L. Vig, G. Shroff, and P. Agarwal, \"Long short term memory networks for anomaly detection in time series,\" in ESANN, 2015.\n", "annotations": {"author": "[{\"end\":101,\"start\":86},{\"end\":114,\"start\":102},{\"end\":157,\"start\":115},{\"end\":164,\"start\":158},{\"end\":178,\"start\":165},{\"end\":191,\"start\":179},{\"end\":251,\"start\":192},{\"end\":384,\"start\":252},{\"end\":387,\"start\":385},{\"end\":446,\"start\":388}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":95},{\"end\":113,\"start\":109},{\"end\":126,\"start\":123},{\"end\":163,\"start\":161},{\"end\":177,\"start\":174},{\"end\":190,\"start\":185},{\"end\":199,\"start\":196},{\"end\":262,\"start\":258}]", "author_first_name": "[{\"end\":94,\"start\":86},{\"end\":108,\"start\":102},{\"end\":122,\"start\":115},{\"end\":160,\"start\":158},{\"end\":173,\"start\":165},{\"end\":184,\"start\":179},{\"end\":195,\"start\":192},{\"end\":257,\"start\":252},{\"end\":386,\"start\":385}]", "author_affiliation": "[{\"end\":250,\"start\":224},{\"end\":383,\"start\":286},{\"end\":445,\"start\":389}]", "title": "[{\"end\":83,\"start\":1},{\"end\":529,\"start\":447}]", "venue": null, "abstract": "[{\"end\":1910,\"start\":629}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2463,\"start\":2460},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2468,\"start\":2465},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2666,\"start\":2663},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2671,\"start\":2668},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3327,\"start\":3324},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3332,\"start\":3329},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3337,\"start\":3334},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3342,\"start\":3339},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3433,\"start\":3430},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3438,\"start\":3435},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3444,\"start\":3440},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3450,\"start\":3446},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3756,\"start\":3752},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3762,\"start\":3758},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4004,\"start\":4000},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4108,\"start\":4104},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4114,\"start\":4110},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4278,\"start\":4275},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4284,\"start\":4280},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4382,\"start\":4378},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4417,\"start\":4413},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4434,\"start\":4430},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5447,\"start\":5443},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6719,\"start\":6715},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6725,\"start\":6721},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9830,\"start\":9826},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10232,\"start\":10228},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10966,\"start\":10963},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10972,\"start\":10968},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10978,\"start\":10974},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10984,\"start\":10980},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11614,\"start\":11610},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11620,\"start\":11616},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14291,\"start\":14287},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14439,\"start\":14435},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14503,\"start\":14499},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14975,\"start\":14971},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14981,\"start\":14977},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15437,\"start\":15433},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15720,\"start\":15716},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15730,\"start\":15726},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15753,\"start\":15749},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17259,\"start\":17255},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17277,\"start\":17273},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17458,\"start\":17454},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17717,\"start\":17713},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17810,\"start\":17806},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18439,\"start\":18435},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18524,\"start\":18520},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20748,\"start\":20744},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21368,\"start\":21364},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21639,\"start\":21635},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22217,\"start\":22213},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22758,\"start\":22754},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22775,\"start\":22771},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22993,\"start\":22989},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23235,\"start\":23231},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23623,\"start\":23619},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25493,\"start\":25489},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27083,\"start\":27079},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27397,\"start\":27393},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27838,\"start\":27835},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28037,\"start\":28033},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28043,\"start\":28039},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28049,\"start\":28045},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31655,\"start\":31651},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31676,\"start\":31672},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31730,\"start\":31726},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32830,\"start\":32826},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32852,\"start\":32848},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34625,\"start\":34621},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34631,\"start\":34627},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":35234,\"start\":35230},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":35545,\"start\":35541},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":35624,\"start\":35620},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35643,\"start\":35639},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":35889,\"start\":35885},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":35974,\"start\":35970},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36017,\"start\":36013},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36192,\"start\":36188},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36299,\"start\":36295},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":36586,\"start\":36582},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":36592,\"start\":36588},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":36612,\"start\":36608},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":36618,\"start\":36614},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":36624,\"start\":36620},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":36812,\"start\":36808},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37202,\"start\":37198},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":37222,\"start\":37218},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37358,\"start\":37354},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37668,\"start\":37664},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37863,\"start\":37859},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37874,\"start\":37870},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37889,\"start\":37885},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":38185,\"start\":38181},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":38303,\"start\":38299},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38381,\"start\":38377}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39704,\"start\":39592},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39737,\"start\":39705},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39797,\"start\":39738},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41208,\"start\":39798},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41318,\"start\":41209},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42717,\"start\":41319},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43016,\"start\":42718},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44062,\"start\":43017},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":44368,\"start\":44063},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44848,\"start\":44369}]", "paragraph": "[{\"end\":3343,\"start\":1929},{\"end\":3899,\"start\":3345},{\"end\":4285,\"start\":3901},{\"end\":5239,\"start\":4287},{\"end\":5939,\"start\":5241},{\"end\":6471,\"start\":5941},{\"end\":6825,\"start\":6473},{\"end\":8138,\"start\":6827},{\"end\":9111,\"start\":8140},{\"end\":10018,\"start\":9160},{\"end\":10325,\"start\":10020},{\"end\":10846,\"start\":10327},{\"end\":11366,\"start\":10864},{\"end\":12067,\"start\":11368},{\"end\":12932,\"start\":12120},{\"end\":13408,\"start\":12934},{\"end\":14267,\"start\":13410},{\"end\":14793,\"start\":14269},{\"end\":14933,\"start\":14795},{\"end\":15092,\"start\":14935},{\"end\":15393,\"start\":15094},{\"end\":15910,\"start\":15395},{\"end\":15985,\"start\":15912},{\"end\":16680,\"start\":15987},{\"end\":18139,\"start\":16682},{\"end\":18907,\"start\":18158},{\"end\":19264,\"start\":18909},{\"end\":19548,\"start\":19266},{\"end\":19716,\"start\":19550},{\"end\":20471,\"start\":19718},{\"end\":20718,\"start\":20490},{\"end\":21301,\"start\":20720},{\"end\":22045,\"start\":21303},{\"end\":22308,\"start\":22047},{\"end\":22701,\"start\":22310},{\"end\":22935,\"start\":22703},{\"end\":23354,\"start\":22937},{\"end\":26049,\"start\":23384},{\"end\":26950,\"start\":26115},{\"end\":28380,\"start\":26970},{\"end\":28575,\"start\":28402},{\"end\":28863,\"start\":28577},{\"end\":29661,\"start\":28865},{\"end\":30046,\"start\":29663},{\"end\":30127,\"start\":30048},{\"end\":30516,\"start\":30129},{\"end\":31499,\"start\":30518},{\"end\":31993,\"start\":31501},{\"end\":32605,\"start\":31995},{\"end\":33132,\"start\":32607},{\"end\":34151,\"start\":33170},{\"end\":34741,\"start\":34178},{\"end\":35169,\"start\":34743},{\"end\":35406,\"start\":35217},{\"end\":36469,\"start\":35408},{\"end\":36955,\"start\":36495},{\"end\":37161,\"start\":36991},{\"end\":37619,\"start\":37163},{\"end\":38145,\"start\":37621},{\"end\":38745,\"start\":38147},{\"end\":39569,\"start\":38764}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15298,\"start\":15291},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22475,\"start\":22465},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27460,\"start\":27452}]", "section_header": "[{\"end\":1927,\"start\":1912},{\"end\":9158,\"start\":9114},{\"end\":10862,\"start\":10849},{\"end\":12092,\"start\":12070},{\"end\":12118,\"start\":12095},{\"end\":18156,\"start\":18142},{\"end\":20488,\"start\":20474},{\"end\":23382,\"start\":23357},{\"end\":26092,\"start\":26052},{\"end\":26113,\"start\":26095},{\"end\":26968,\"start\":26953},{\"end\":28400,\"start\":28383},{\"end\":33168,\"start\":33135},{\"end\":34176,\"start\":34154},{\"end\":35187,\"start\":35172},{\"end\":35215,\"start\":35190},{\"end\":36493,\"start\":36472},{\"end\":36989,\"start\":36958},{\"end\":38762,\"start\":38748},{\"end\":39591,\"start\":39572},{\"end\":39601,\"start\":39593},{\"end\":39714,\"start\":39706},{\"end\":39747,\"start\":39739},{\"end\":39810,\"start\":39799},{\"end\":41329,\"start\":41320},{\"end\":42729,\"start\":42719},{\"end\":43029,\"start\":43018},{\"end\":44074,\"start\":44064},{\"end\":44379,\"start\":44370}]", "table": "[{\"end\":42717,\"start\":41871},{\"end\":43016,\"start\":42826},{\"end\":44062,\"start\":43081},{\"end\":44368,\"start\":44120},{\"end\":44848,\"start\":44413}]", "figure_caption": "[{\"end\":39704,\"start\":39603},{\"end\":39737,\"start\":39716},{\"end\":39797,\"start\":39749},{\"end\":41208,\"start\":39812},{\"end\":41318,\"start\":41211},{\"end\":41871,\"start\":41331},{\"end\":42826,\"start\":42732},{\"end\":43081,\"start\":43033},{\"end\":44120,\"start\":44077},{\"end\":44413,\"start\":44381}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":3061,\"start\":3055},{\"end\":4583,\"start\":4577},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":5695,\"start\":5689},{\"end\":10178,\"start\":10172},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11744,\"start\":11738},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12295,\"start\":12289},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17888,\"start\":17882},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19284,\"start\":19278},{\"end\":20170,\"start\":20164},{\"end\":23475,\"start\":23469},{\"end\":24251,\"start\":24245},{\"end\":25781,\"start\":25775},{\"end\":27468,\"start\":27462},{\"end\":28908,\"start\":28902},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30704,\"start\":30697},{\"end\":30945,\"start\":30939},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31835,\"start\":31828},{\"end\":32479,\"start\":32473},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32921,\"start\":32914},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33297,\"start\":33290},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33395,\"start\":33388}]", "bib_author_first_name": "[{\"end\":45047,\"start\":45046},{\"end\":45056,\"start\":45055},{\"end\":45063,\"start\":45062},{\"end\":45070,\"start\":45069},{\"end\":45078,\"start\":45077},{\"end\":45084,\"start\":45083},{\"end\":45091,\"start\":45090},{\"end\":45099,\"start\":45098},{\"end\":45107,\"start\":45106},{\"end\":45457,\"start\":45456},{\"end\":45463,\"start\":45462},{\"end\":45472,\"start\":45471},{\"end\":45479,\"start\":45478},{\"end\":45488,\"start\":45487},{\"end\":45858,\"start\":45857},{\"end\":45864,\"start\":45863},{\"end\":45870,\"start\":45869},{\"end\":45878,\"start\":45877},{\"end\":45886,\"start\":45885},{\"end\":45895,\"start\":45894},{\"end\":46238,\"start\":46237},{\"end\":46244,\"start\":46243},{\"end\":46251,\"start\":46250},{\"end\":46258,\"start\":46257},{\"end\":46561,\"start\":46560},{\"end\":46568,\"start\":46567},{\"end\":46576,\"start\":46575},{\"end\":46952,\"start\":46951},{\"end\":46958,\"start\":46957},{\"end\":46966,\"start\":46965},{\"end\":46972,\"start\":46971},{\"end\":46980,\"start\":46979},{\"end\":46986,\"start\":46985},{\"end\":46994,\"start\":46993},{\"end\":47000,\"start\":46999},{\"end\":47350,\"start\":47349},{\"end\":47357,\"start\":47356},{\"end\":47363,\"start\":47362},{\"end\":47371,\"start\":47370},{\"end\":47377,\"start\":47376},{\"end\":47386,\"start\":47385},{\"end\":47393,\"start\":47392},{\"end\":47401,\"start\":47400},{\"end\":47409,\"start\":47408},{\"end\":47417,\"start\":47416},{\"end\":47984,\"start\":47983},{\"end\":47990,\"start\":47989},{\"end\":47999,\"start\":47998},{\"end\":48008,\"start\":48007},{\"end\":48016,\"start\":48015},{\"end\":48025,\"start\":48024},{\"end\":48034,\"start\":48033},{\"end\":48042,\"start\":48041},{\"end\":48049,\"start\":48048},{\"end\":48055,\"start\":48054},{\"end\":48552,\"start\":48551},{\"end\":49018,\"start\":49017},{\"end\":49025,\"start\":49024},{\"end\":49033,\"start\":49032},{\"end\":49039,\"start\":49038},{\"end\":49046,\"start\":49045},{\"end\":49053,\"start\":49052},{\"end\":49060,\"start\":49059},{\"end\":49068,\"start\":49067},{\"end\":49538,\"start\":49537},{\"end\":49547,\"start\":49546},{\"end\":49554,\"start\":49553},{\"end\":49560,\"start\":49559},{\"end\":49567,\"start\":49566},{\"end\":49576,\"start\":49575},{\"end\":49584,\"start\":49583},{\"end\":49592,\"start\":49591},{\"end\":49600,\"start\":49599},{\"end\":49609,\"start\":49608},{\"end\":49625,\"start\":49624},{\"end\":49631,\"start\":49630},{\"end\":49640,\"start\":49639},{\"end\":49647,\"start\":49646},{\"end\":49655,\"start\":49654},{\"end\":49661,\"start\":49660},{\"end\":49670,\"start\":49669},{\"end\":49678,\"start\":49677},{\"end\":50285,\"start\":50284},{\"end\":50295,\"start\":50294},{\"end\":50307,\"start\":50306},{\"end\":50836,\"start\":50835},{\"end\":50842,\"start\":50841},{\"end\":50849,\"start\":50848},{\"end\":50857,\"start\":50856},{\"end\":50865,\"start\":50864},{\"end\":50872,\"start\":50871},{\"end\":51267,\"start\":51266},{\"end\":51273,\"start\":51272},{\"end\":51281,\"start\":51280},{\"end\":51289,\"start\":51288},{\"end\":51295,\"start\":51294},{\"end\":51301,\"start\":51300},{\"end\":51307,\"start\":51306},{\"end\":51314,\"start\":51313},{\"end\":51322,\"start\":51321},{\"end\":51329,\"start\":51328},{\"end\":52014,\"start\":52013},{\"end\":52020,\"start\":52019},{\"end\":52028,\"start\":52027},{\"end\":52451,\"start\":52450},{\"end\":52459,\"start\":52458},{\"end\":52465,\"start\":52464},{\"end\":52471,\"start\":52470},{\"end\":52479,\"start\":52478},{\"end\":52487,\"start\":52486},{\"end\":52495,\"start\":52494},{\"end\":52503,\"start\":52502},{\"end\":52885,\"start\":52881},{\"end\":52894,\"start\":52893},{\"end\":52900,\"start\":52899},{\"end\":52908,\"start\":52907},{\"end\":52917,\"start\":52913},{\"end\":53402,\"start\":53401},{\"end\":53408,\"start\":53407},{\"end\":53415,\"start\":53414},{\"end\":53424,\"start\":53423},{\"end\":53432,\"start\":53431},{\"end\":53439,\"start\":53438},{\"end\":53446,\"start\":53445},{\"end\":53870,\"start\":53869},{\"end\":53879,\"start\":53878},{\"end\":53887,\"start\":53886},{\"end\":53898,\"start\":53894},{\"end\":54555,\"start\":54554},{\"end\":54561,\"start\":54560},{\"end\":54569,\"start\":54568},{\"end\":54575,\"start\":54574},{\"end\":54583,\"start\":54582},{\"end\":54589,\"start\":54588},{\"end\":55001,\"start\":55000},{\"end\":55012,\"start\":55011},{\"end\":55014,\"start\":55013},{\"end\":55406,\"start\":55405},{\"end\":55414,\"start\":55413},{\"end\":55421,\"start\":55420},{\"end\":55428,\"start\":55427},{\"end\":55435,\"start\":55434},{\"end\":55444,\"start\":55443},{\"end\":55794,\"start\":55793},{\"end\":55800,\"start\":55799},{\"end\":55808,\"start\":55807},{\"end\":55815,\"start\":55814},{\"end\":56063,\"start\":56062},{\"end\":56077,\"start\":56076},{\"end\":56509,\"start\":56508},{\"end\":56517,\"start\":56516},{\"end\":56524,\"start\":56523},{\"end\":56533,\"start\":56532},{\"end\":56540,\"start\":56539},{\"end\":56548,\"start\":56547},{\"end\":56871,\"start\":56870},{\"end\":56880,\"start\":56879},{\"end\":56882,\"start\":56881},{\"end\":56897,\"start\":56896},{\"end\":57152,\"start\":57151},{\"end\":57161,\"start\":57160},{\"end\":57172,\"start\":57171},{\"end\":57892,\"start\":57891},{\"end\":58231,\"start\":58230},{\"end\":58233,\"start\":58232},{\"end\":58247,\"start\":58246},{\"end\":58255,\"start\":58254},{\"end\":58587,\"start\":58586},{\"end\":58589,\"start\":58588},{\"end\":58601,\"start\":58600},{\"end\":58611,\"start\":58610},{\"end\":58617,\"start\":58616},{\"end\":58629,\"start\":58628},{\"end\":59065,\"start\":59064},{\"end\":59073,\"start\":59072},{\"end\":59080,\"start\":59079},{\"end\":59088,\"start\":59087},{\"end\":59094,\"start\":59093},{\"end\":59471,\"start\":59470},{\"end\":59479,\"start\":59478},{\"end\":59491,\"start\":59490},{\"end\":59727,\"start\":59726},{\"end\":59736,\"start\":59735},{\"end\":60120,\"start\":60119},{\"end\":60130,\"start\":60129},{\"end\":60132,\"start\":60131},{\"end\":60141,\"start\":60140},{\"end\":60152,\"start\":60151},{\"end\":60161,\"start\":60160},{\"end\":60170,\"start\":60169},{\"end\":60838,\"start\":60837},{\"end\":60848,\"start\":60847},{\"end\":60864,\"start\":60863},{\"end\":61350,\"start\":61349},{\"end\":61361,\"start\":61357},{\"end\":62080,\"start\":62076},{\"end\":62091,\"start\":62087},{\"end\":62102,\"start\":62098},{\"end\":62560,\"start\":62559},{\"end\":62575,\"start\":62574},{\"end\":62581,\"start\":62576},{\"end\":62598,\"start\":62597},{\"end\":62603,\"start\":62599},{\"end\":62956,\"start\":62955},{\"end\":62965,\"start\":62964},{\"end\":63162,\"start\":63161},{\"end\":63169,\"start\":63168},{\"end\":63178,\"start\":63177},{\"end\":63180,\"start\":63179},{\"end\":63195,\"start\":63194},{\"end\":63204,\"start\":63203},{\"end\":63212,\"start\":63211},{\"end\":63594,\"start\":63593},{\"end\":63606,\"start\":63605},{\"end\":63608,\"start\":63607},{\"end\":63623,\"start\":63622},{\"end\":63968,\"start\":63967},{\"end\":64214,\"start\":64213},{\"end\":64216,\"start\":64215},{\"end\":64229,\"start\":64228},{\"end\":64688,\"start\":64687},{\"end\":64698,\"start\":64697},{\"end\":64987,\"start\":64986},{\"end\":64994,\"start\":64993},{\"end\":65001,\"start\":65000},{\"end\":65008,\"start\":65007},{\"end\":65014,\"start\":65013},{\"end\":65428,\"start\":65427},{\"end\":65435,\"start\":65434},{\"end\":65443,\"start\":65442},{\"end\":65451,\"start\":65450},{\"end\":65459,\"start\":65458},{\"end\":65466,\"start\":65465},{\"end\":65474,\"start\":65473},{\"end\":65483,\"start\":65482},{\"end\":65928,\"start\":65927},{\"end\":65934,\"start\":65933},{\"end\":65941,\"start\":65940},{\"end\":65950,\"start\":65949},{\"end\":66243,\"start\":66242},{\"end\":66254,\"start\":66253},{\"end\":66260,\"start\":66259},{\"end\":66262,\"start\":66261},{\"end\":66270,\"start\":66269},{\"end\":66564,\"start\":66563},{\"end\":66586,\"start\":66585},{\"end\":66594,\"start\":66593},{\"end\":66596,\"start\":66595},{\"end\":66925,\"start\":66924},{\"end\":66932,\"start\":66931},{\"end\":66941,\"start\":66940},{\"end\":67242,\"start\":67241},{\"end\":67257,\"start\":67256},{\"end\":67266,\"start\":67265},{\"end\":67516,\"start\":67515},{\"end\":67523,\"start\":67522},{\"end\":67531,\"start\":67530},{\"end\":67891,\"start\":67890},{\"end\":67903,\"start\":67902},{\"end\":67910,\"start\":67909},{\"end\":67920,\"start\":67919}]", "bib_author_last_name": "[{\"end\":45053,\"start\":45048},{\"end\":45060,\"start\":45057},{\"end\":45067,\"start\":45064},{\"end\":45075,\"start\":45071},{\"end\":45081,\"start\":45079},{\"end\":45088,\"start\":45085},{\"end\":45096,\"start\":45092},{\"end\":45104,\"start\":45100},{\"end\":45112,\"start\":45108},{\"end\":45460,\"start\":45458},{\"end\":45469,\"start\":45464},{\"end\":45476,\"start\":45473},{\"end\":45485,\"start\":45480},{\"end\":45492,\"start\":45489},{\"end\":45861,\"start\":45859},{\"end\":45867,\"start\":45865},{\"end\":45875,\"start\":45871},{\"end\":45883,\"start\":45879},{\"end\":45892,\"start\":45887},{\"end\":45900,\"start\":45896},{\"end\":46241,\"start\":46239},{\"end\":46248,\"start\":46245},{\"end\":46255,\"start\":46252},{\"end\":46263,\"start\":46259},{\"end\":46565,\"start\":46562},{\"end\":46573,\"start\":46569},{\"end\":46582,\"start\":46577},{\"end\":46955,\"start\":46953},{\"end\":46963,\"start\":46959},{\"end\":46969,\"start\":46967},{\"end\":46977,\"start\":46973},{\"end\":46983,\"start\":46981},{\"end\":46991,\"start\":46987},{\"end\":46997,\"start\":46995},{\"end\":47006,\"start\":47001},{\"end\":47354,\"start\":47351},{\"end\":47360,\"start\":47358},{\"end\":47368,\"start\":47364},{\"end\":47374,\"start\":47372},{\"end\":47383,\"start\":47378},{\"end\":47390,\"start\":47387},{\"end\":47398,\"start\":47394},{\"end\":47406,\"start\":47402},{\"end\":47414,\"start\":47410},{\"end\":47423,\"start\":47418},{\"end\":47987,\"start\":47985},{\"end\":47996,\"start\":47991},{\"end\":48005,\"start\":48000},{\"end\":48013,\"start\":48009},{\"end\":48022,\"start\":48017},{\"end\":48031,\"start\":48026},{\"end\":48039,\"start\":48035},{\"end\":48046,\"start\":48043},{\"end\":48052,\"start\":48050},{\"end\":48060,\"start\":48056},{\"end\":48564,\"start\":48553},{\"end\":49022,\"start\":49019},{\"end\":49030,\"start\":49026},{\"end\":49036,\"start\":49034},{\"end\":49043,\"start\":49040},{\"end\":49050,\"start\":49047},{\"end\":49057,\"start\":49054},{\"end\":49065,\"start\":49061},{\"end\":49073,\"start\":49069},{\"end\":49544,\"start\":49539},{\"end\":49551,\"start\":49548},{\"end\":49557,\"start\":49555},{\"end\":49564,\"start\":49561},{\"end\":49573,\"start\":49568},{\"end\":49581,\"start\":49577},{\"end\":49589,\"start\":49585},{\"end\":49597,\"start\":49593},{\"end\":49606,\"start\":49601},{\"end\":49622,\"start\":49610},{\"end\":49628,\"start\":49626},{\"end\":49637,\"start\":49632},{\"end\":49644,\"start\":49641},{\"end\":49652,\"start\":49648},{\"end\":49658,\"start\":49656},{\"end\":49667,\"start\":49662},{\"end\":49675,\"start\":49671},{\"end\":49684,\"start\":49679},{\"end\":50292,\"start\":50286},{\"end\":50304,\"start\":50296},{\"end\":50313,\"start\":50308},{\"end\":50839,\"start\":50837},{\"end\":50846,\"start\":50843},{\"end\":50854,\"start\":50850},{\"end\":50862,\"start\":50858},{\"end\":50869,\"start\":50866},{\"end\":50876,\"start\":50873},{\"end\":51270,\"start\":51268},{\"end\":51278,\"start\":51274},{\"end\":51286,\"start\":51282},{\"end\":51292,\"start\":51290},{\"end\":51298,\"start\":51296},{\"end\":51304,\"start\":51302},{\"end\":51311,\"start\":51308},{\"end\":51319,\"start\":51315},{\"end\":51326,\"start\":51323},{\"end\":51334,\"start\":51330},{\"end\":52017,\"start\":52015},{\"end\":52025,\"start\":52021},{\"end\":52032,\"start\":52029},{\"end\":52456,\"start\":52452},{\"end\":52462,\"start\":52460},{\"end\":52468,\"start\":52466},{\"end\":52476,\"start\":52472},{\"end\":52484,\"start\":52480},{\"end\":52492,\"start\":52488},{\"end\":52500,\"start\":52496},{\"end\":52508,\"start\":52504},{\"end\":52891,\"start\":52886},{\"end\":52897,\"start\":52895},{\"end\":52905,\"start\":52901},{\"end\":52911,\"start\":52909},{\"end\":52922,\"start\":52918},{\"end\":53405,\"start\":53403},{\"end\":53412,\"start\":53409},{\"end\":53421,\"start\":53416},{\"end\":53429,\"start\":53425},{\"end\":53436,\"start\":53433},{\"end\":53443,\"start\":53440},{\"end\":53450,\"start\":53447},{\"end\":53876,\"start\":53871},{\"end\":53884,\"start\":53880},{\"end\":53892,\"start\":53888},{\"end\":53902,\"start\":53899},{\"end\":54558,\"start\":54556},{\"end\":54566,\"start\":54562},{\"end\":54572,\"start\":54570},{\"end\":54580,\"start\":54576},{\"end\":54586,\"start\":54584},{\"end\":54593,\"start\":54590},{\"end\":55009,\"start\":55002},{\"end\":55025,\"start\":55015},{\"end\":55411,\"start\":55407},{\"end\":55418,\"start\":55415},{\"end\":55425,\"start\":55422},{\"end\":55432,\"start\":55429},{\"end\":55441,\"start\":55436},{\"end\":55448,\"start\":55445},{\"end\":55797,\"start\":55795},{\"end\":55805,\"start\":55801},{\"end\":55812,\"start\":55809},{\"end\":55819,\"start\":55816},{\"end\":56074,\"start\":56064},{\"end\":56085,\"start\":56078},{\"end\":56514,\"start\":56510},{\"end\":56521,\"start\":56518},{\"end\":56530,\"start\":56525},{\"end\":56537,\"start\":56534},{\"end\":56545,\"start\":56541},{\"end\":56553,\"start\":56549},{\"end\":56877,\"start\":56872},{\"end\":56894,\"start\":56883},{\"end\":56906,\"start\":56898},{\"end\":57158,\"start\":57153},{\"end\":57169,\"start\":57162},{\"end\":57179,\"start\":57173},{\"end\":57897,\"start\":57893},{\"end\":58244,\"start\":58234},{\"end\":58252,\"start\":58248},{\"end\":58265,\"start\":58256},{\"end\":58598,\"start\":58590},{\"end\":58608,\"start\":58602},{\"end\":58614,\"start\":58612},{\"end\":58626,\"start\":58618},{\"end\":58635,\"start\":58630},{\"end\":59070,\"start\":59066},{\"end\":59077,\"start\":59074},{\"end\":59085,\"start\":59081},{\"end\":59091,\"start\":59089},{\"end\":59097,\"start\":59095},{\"end\":59476,\"start\":59472},{\"end\":59488,\"start\":59480},{\"end\":59497,\"start\":59492},{\"end\":59733,\"start\":59728},{\"end\":59742,\"start\":59737},{\"end\":60127,\"start\":60121},{\"end\":60138,\"start\":60133},{\"end\":60149,\"start\":60142},{\"end\":60158,\"start\":60153},{\"end\":60167,\"start\":60162},{\"end\":60180,\"start\":60171},{\"end\":60845,\"start\":60839},{\"end\":60861,\"start\":60849},{\"end\":60874,\"start\":60865},{\"end\":61355,\"start\":61351},{\"end\":61367,\"start\":61362},{\"end\":62085,\"start\":62081},{\"end\":62096,\"start\":62092},{\"end\":62108,\"start\":62103},{\"end\":62572,\"start\":62561},{\"end\":62595,\"start\":62582},{\"end\":62607,\"start\":62604},{\"end\":62962,\"start\":62957},{\"end\":62971,\"start\":62966},{\"end\":63166,\"start\":63163},{\"end\":63175,\"start\":63170},{\"end\":63192,\"start\":63181},{\"end\":63201,\"start\":63196},{\"end\":63209,\"start\":63205},{\"end\":63215,\"start\":63213},{\"end\":63603,\"start\":63595},{\"end\":63620,\"start\":63609},{\"end\":63633,\"start\":63624},{\"end\":63975,\"start\":63969},{\"end\":64226,\"start\":64217},{\"end\":64239,\"start\":64230},{\"end\":64695,\"start\":64689},{\"end\":64704,\"start\":64699},{\"end\":64991,\"start\":64988},{\"end\":64998,\"start\":64995},{\"end\":65005,\"start\":65002},{\"end\":65011,\"start\":65009},{\"end\":65019,\"start\":65015},{\"end\":65432,\"start\":65429},{\"end\":65440,\"start\":65436},{\"end\":65448,\"start\":65444},{\"end\":65456,\"start\":65452},{\"end\":65463,\"start\":65460},{\"end\":65471,\"start\":65467},{\"end\":65480,\"start\":65475},{\"end\":65487,\"start\":65484},{\"end\":65931,\"start\":65929},{\"end\":65938,\"start\":65935},{\"end\":65947,\"start\":65942},{\"end\":65954,\"start\":65951},{\"end\":66251,\"start\":66244},{\"end\":66257,\"start\":66255},{\"end\":66267,\"start\":66263},{\"end\":66278,\"start\":66271},{\"end\":66287,\"start\":66280},{\"end\":66583,\"start\":66565},{\"end\":66591,\"start\":66587},{\"end\":66603,\"start\":66597},{\"end\":66929,\"start\":66926},{\"end\":66938,\"start\":66933},{\"end\":66946,\"start\":66942},{\"end\":67254,\"start\":67243},{\"end\":67263,\"start\":67258},{\"end\":67272,\"start\":67267},{\"end\":67520,\"start\":67517},{\"end\":67528,\"start\":67524},{\"end\":67534,\"start\":67532},{\"end\":67900,\"start\":67892},{\"end\":67907,\"start\":67904},{\"end\":67917,\"start\":67911},{\"end\":67928,\"start\":67921}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3423028},\"end\":45372,\"start\":44988},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":53720973},\"end\":45781,\"start\":45374},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":215839626},\"end\":46157,\"start\":45783},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218958910},\"end\":46468,\"start\":46159},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":53233176},\"end\":46856,\"start\":46470},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":223558697},\"end\":47295,\"start\":46858},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":182952311},\"end\":47870,\"start\":47297},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":199007126},\"end\":48457,\"start\":47872},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":235637010},\"end\":48927,\"start\":48459},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2707750},\"end\":49472,\"start\":48929},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":155920112},\"end\":50206,\"start\":49474},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":207227428},\"end\":50745,\"start\":50208},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":214371919},\"end\":51166,\"start\":50747},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3636669},\"end\":51919,\"start\":51168},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":155106705},\"end\":52365,\"start\":51921},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":86467618},\"end\":52826,\"start\":52367},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":13669586},\"end\":53312,\"start\":52828},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":155107845},\"end\":53792,\"start\":53314},{\"attributes\":{\"doi\":\"10.1145/3123266.3123304\",\"id\":\"b18\",\"matched_paper_id\":43987057},\"end\":54462,\"start\":53794},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":7725087},\"end\":54933,\"start\":54464},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":17121546},\"end\":55340,\"start\":54935},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":189109086},\"end\":55720,\"start\":55342},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":59236070},\"end\":56001,\"start\":55722},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52862551},\"end\":56441,\"start\":56003},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":195676271},\"end\":56790,\"start\":56443},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":213442815},\"end\":57093,\"start\":56792},{\"attributes\":{\"doi\":\"10.1145/3221269.3221293\",\"id\":\"b26\",\"matched_paper_id\":49671240},\"end\":57808,\"start\":57095},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14862546},\"end\":58157,\"start\":57810},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5627239},\"end\":58518,\"start\":58159},{\"attributes\":{\"id\":\"b29\"},\"end\":58780,\"start\":58520},{\"attributes\":{\"id\":\"b30\"},\"end\":58968,\"start\":58782},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":216421278},\"end\":59382,\"start\":58970},{\"attributes\":{\"id\":\"b32\"},\"end\":59642,\"start\":59384},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6842305},\"end\":60041,\"start\":59644},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":227275223},\"end\":60768,\"start\":60043},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":229181130},\"end\":61283,\"start\":60770},{\"attributes\":{\"doi\":\"10.1145/3097983.3098141\",\"id\":\"b36\",\"matched_paper_id\":3065281},\"end\":61998,\"start\":61285},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":233025083},\"end\":62516,\"start\":62000},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":158707},\"end\":62868,\"start\":62518},{\"attributes\":{\"id\":\"b39\"},\"end\":63088,\"start\":62870},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":115200605},\"end\":63521,\"start\":63090},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":212697104},\"end\":63904,\"start\":63523},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1249171},\"end\":64119,\"start\":63906},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":7319679},\"end\":64592,\"start\":64121},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":27778622},\"end\":64908,\"start\":64594},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":30558360},\"end\":65292,\"start\":64910},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4805094},\"end\":65855,\"start\":65294},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":43880196},\"end\":66148,\"start\":65857},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":11218910},\"end\":66499,\"start\":66150},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":8743039},\"end\":66848,\"start\":66501},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":38829576},\"end\":67142,\"start\":66850},{\"attributes\":{\"id\":\"b51\"},\"end\":67433,\"start\":67144},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":49318748},\"end\":67818,\"start\":67435},{\"attributes\":{\"id\":\"b53\"},\"end\":68070,\"start\":67820}]", "bib_title": "[{\"end\":45044,\"start\":44988},{\"end\":45454,\"start\":45374},{\"end\":45855,\"start\":45783},{\"end\":46235,\"start\":46159},{\"end\":46558,\"start\":46470},{\"end\":46949,\"start\":46858},{\"end\":47347,\"start\":47297},{\"end\":47981,\"start\":47872},{\"end\":48549,\"start\":48459},{\"end\":49015,\"start\":48929},{\"end\":49535,\"start\":49474},{\"end\":50282,\"start\":50208},{\"end\":50833,\"start\":50747},{\"end\":51264,\"start\":51168},{\"end\":52011,\"start\":51921},{\"end\":52448,\"start\":52367},{\"end\":52879,\"start\":52828},{\"end\":53399,\"start\":53314},{\"end\":53867,\"start\":53794},{\"end\":54552,\"start\":54464},{\"end\":54998,\"start\":54935},{\"end\":55403,\"start\":55342},{\"end\":55791,\"start\":55722},{\"end\":56060,\"start\":56003},{\"end\":56506,\"start\":56443},{\"end\":56868,\"start\":56792},{\"end\":57149,\"start\":57095},{\"end\":57889,\"start\":57810},{\"end\":58228,\"start\":58159},{\"end\":59062,\"start\":58970},{\"end\":59724,\"start\":59644},{\"end\":60117,\"start\":60043},{\"end\":60835,\"start\":60770},{\"end\":61347,\"start\":61285},{\"end\":62074,\"start\":62000},{\"end\":62557,\"start\":62518},{\"end\":63159,\"start\":63090},{\"end\":63591,\"start\":63523},{\"end\":63965,\"start\":63906},{\"end\":64211,\"start\":64121},{\"end\":64685,\"start\":64594},{\"end\":64984,\"start\":64910},{\"end\":65425,\"start\":65294},{\"end\":65925,\"start\":65857},{\"end\":66240,\"start\":66150},{\"end\":66561,\"start\":66501},{\"end\":66922,\"start\":66850},{\"end\":67513,\"start\":67435}]", "bib_author": "[{\"end\":45055,\"start\":45046},{\"end\":45062,\"start\":45055},{\"end\":45069,\"start\":45062},{\"end\":45077,\"start\":45069},{\"end\":45083,\"start\":45077},{\"end\":45090,\"start\":45083},{\"end\":45098,\"start\":45090},{\"end\":45106,\"start\":45098},{\"end\":45114,\"start\":45106},{\"end\":45462,\"start\":45456},{\"end\":45471,\"start\":45462},{\"end\":45478,\"start\":45471},{\"end\":45487,\"start\":45478},{\"end\":45494,\"start\":45487},{\"end\":45863,\"start\":45857},{\"end\":45869,\"start\":45863},{\"end\":45877,\"start\":45869},{\"end\":45885,\"start\":45877},{\"end\":45894,\"start\":45885},{\"end\":45902,\"start\":45894},{\"end\":46243,\"start\":46237},{\"end\":46250,\"start\":46243},{\"end\":46257,\"start\":46250},{\"end\":46265,\"start\":46257},{\"end\":46567,\"start\":46560},{\"end\":46575,\"start\":46567},{\"end\":46584,\"start\":46575},{\"end\":46957,\"start\":46951},{\"end\":46965,\"start\":46957},{\"end\":46971,\"start\":46965},{\"end\":46979,\"start\":46971},{\"end\":46985,\"start\":46979},{\"end\":46993,\"start\":46985},{\"end\":46999,\"start\":46993},{\"end\":47008,\"start\":46999},{\"end\":47356,\"start\":47349},{\"end\":47362,\"start\":47356},{\"end\":47370,\"start\":47362},{\"end\":47376,\"start\":47370},{\"end\":47385,\"start\":47376},{\"end\":47392,\"start\":47385},{\"end\":47400,\"start\":47392},{\"end\":47408,\"start\":47400},{\"end\":47416,\"start\":47408},{\"end\":47425,\"start\":47416},{\"end\":47989,\"start\":47983},{\"end\":47998,\"start\":47989},{\"end\":48007,\"start\":47998},{\"end\":48015,\"start\":48007},{\"end\":48024,\"start\":48015},{\"end\":48033,\"start\":48024},{\"end\":48041,\"start\":48033},{\"end\":48048,\"start\":48041},{\"end\":48054,\"start\":48048},{\"end\":48062,\"start\":48054},{\"end\":48566,\"start\":48551},{\"end\":49024,\"start\":49017},{\"end\":49032,\"start\":49024},{\"end\":49038,\"start\":49032},{\"end\":49045,\"start\":49038},{\"end\":49052,\"start\":49045},{\"end\":49059,\"start\":49052},{\"end\":49067,\"start\":49059},{\"end\":49075,\"start\":49067},{\"end\":49546,\"start\":49537},{\"end\":49553,\"start\":49546},{\"end\":49559,\"start\":49553},{\"end\":49566,\"start\":49559},{\"end\":49575,\"start\":49566},{\"end\":49583,\"start\":49575},{\"end\":49591,\"start\":49583},{\"end\":49599,\"start\":49591},{\"end\":49608,\"start\":49599},{\"end\":49624,\"start\":49608},{\"end\":49630,\"start\":49624},{\"end\":49639,\"start\":49630},{\"end\":49646,\"start\":49639},{\"end\":49654,\"start\":49646},{\"end\":49660,\"start\":49654},{\"end\":49669,\"start\":49660},{\"end\":49677,\"start\":49669},{\"end\":49686,\"start\":49677},{\"end\":50294,\"start\":50284},{\"end\":50306,\"start\":50294},{\"end\":50315,\"start\":50306},{\"end\":50841,\"start\":50835},{\"end\":50848,\"start\":50841},{\"end\":50856,\"start\":50848},{\"end\":50864,\"start\":50856},{\"end\":50871,\"start\":50864},{\"end\":50878,\"start\":50871},{\"end\":51272,\"start\":51266},{\"end\":51280,\"start\":51272},{\"end\":51288,\"start\":51280},{\"end\":51294,\"start\":51288},{\"end\":51300,\"start\":51294},{\"end\":51306,\"start\":51300},{\"end\":51313,\"start\":51306},{\"end\":51321,\"start\":51313},{\"end\":51328,\"start\":51321},{\"end\":51336,\"start\":51328},{\"end\":52019,\"start\":52013},{\"end\":52027,\"start\":52019},{\"end\":52034,\"start\":52027},{\"end\":52458,\"start\":52450},{\"end\":52464,\"start\":52458},{\"end\":52470,\"start\":52464},{\"end\":52478,\"start\":52470},{\"end\":52486,\"start\":52478},{\"end\":52494,\"start\":52486},{\"end\":52502,\"start\":52494},{\"end\":52510,\"start\":52502},{\"end\":52893,\"start\":52881},{\"end\":52899,\"start\":52893},{\"end\":52907,\"start\":52899},{\"end\":52913,\"start\":52907},{\"end\":52924,\"start\":52913},{\"end\":53407,\"start\":53401},{\"end\":53414,\"start\":53407},{\"end\":53423,\"start\":53414},{\"end\":53431,\"start\":53423},{\"end\":53438,\"start\":53431},{\"end\":53445,\"start\":53438},{\"end\":53452,\"start\":53445},{\"end\":53878,\"start\":53869},{\"end\":53886,\"start\":53878},{\"end\":53894,\"start\":53886},{\"end\":53904,\"start\":53894},{\"end\":54560,\"start\":54554},{\"end\":54568,\"start\":54560},{\"end\":54574,\"start\":54568},{\"end\":54582,\"start\":54574},{\"end\":54588,\"start\":54582},{\"end\":54595,\"start\":54588},{\"end\":55011,\"start\":55000},{\"end\":55027,\"start\":55011},{\"end\":55413,\"start\":55405},{\"end\":55420,\"start\":55413},{\"end\":55427,\"start\":55420},{\"end\":55434,\"start\":55427},{\"end\":55443,\"start\":55434},{\"end\":55450,\"start\":55443},{\"end\":55799,\"start\":55793},{\"end\":55807,\"start\":55799},{\"end\":55814,\"start\":55807},{\"end\":55821,\"start\":55814},{\"end\":56076,\"start\":56062},{\"end\":56087,\"start\":56076},{\"end\":56516,\"start\":56508},{\"end\":56523,\"start\":56516},{\"end\":56532,\"start\":56523},{\"end\":56539,\"start\":56532},{\"end\":56547,\"start\":56539},{\"end\":56555,\"start\":56547},{\"end\":56879,\"start\":56870},{\"end\":56896,\"start\":56879},{\"end\":56908,\"start\":56896},{\"end\":57160,\"start\":57151},{\"end\":57171,\"start\":57160},{\"end\":57181,\"start\":57171},{\"end\":57899,\"start\":57891},{\"end\":58246,\"start\":58230},{\"end\":58254,\"start\":58246},{\"end\":58267,\"start\":58254},{\"end\":58600,\"start\":58586},{\"end\":58610,\"start\":58600},{\"end\":58616,\"start\":58610},{\"end\":58628,\"start\":58616},{\"end\":58637,\"start\":58628},{\"end\":59072,\"start\":59064},{\"end\":59079,\"start\":59072},{\"end\":59087,\"start\":59079},{\"end\":59093,\"start\":59087},{\"end\":59099,\"start\":59093},{\"end\":59478,\"start\":59470},{\"end\":59490,\"start\":59478},{\"end\":59499,\"start\":59490},{\"end\":59735,\"start\":59726},{\"end\":59744,\"start\":59735},{\"end\":60129,\"start\":60119},{\"end\":60140,\"start\":60129},{\"end\":60151,\"start\":60140},{\"end\":60160,\"start\":60151},{\"end\":60169,\"start\":60160},{\"end\":60182,\"start\":60169},{\"end\":60847,\"start\":60837},{\"end\":60863,\"start\":60847},{\"end\":60876,\"start\":60863},{\"end\":61357,\"start\":61349},{\"end\":61369,\"start\":61357},{\"end\":62087,\"start\":62076},{\"end\":62098,\"start\":62087},{\"end\":62110,\"start\":62098},{\"end\":62574,\"start\":62559},{\"end\":62597,\"start\":62574},{\"end\":62609,\"start\":62597},{\"end\":62964,\"start\":62955},{\"end\":62973,\"start\":62964},{\"end\":63168,\"start\":63161},{\"end\":63177,\"start\":63168},{\"end\":63194,\"start\":63177},{\"end\":63203,\"start\":63194},{\"end\":63211,\"start\":63203},{\"end\":63217,\"start\":63211},{\"end\":63605,\"start\":63593},{\"end\":63622,\"start\":63605},{\"end\":63635,\"start\":63622},{\"end\":63977,\"start\":63967},{\"end\":64228,\"start\":64213},{\"end\":64241,\"start\":64228},{\"end\":64697,\"start\":64687},{\"end\":64706,\"start\":64697},{\"end\":64993,\"start\":64986},{\"end\":65000,\"start\":64993},{\"end\":65007,\"start\":65000},{\"end\":65013,\"start\":65007},{\"end\":65021,\"start\":65013},{\"end\":65434,\"start\":65427},{\"end\":65442,\"start\":65434},{\"end\":65450,\"start\":65442},{\"end\":65458,\"start\":65450},{\"end\":65465,\"start\":65458},{\"end\":65473,\"start\":65465},{\"end\":65482,\"start\":65473},{\"end\":65489,\"start\":65482},{\"end\":65933,\"start\":65927},{\"end\":65940,\"start\":65933},{\"end\":65949,\"start\":65940},{\"end\":65956,\"start\":65949},{\"end\":66253,\"start\":66242},{\"end\":66259,\"start\":66253},{\"end\":66269,\"start\":66259},{\"end\":66280,\"start\":66269},{\"end\":66289,\"start\":66280},{\"end\":66585,\"start\":66563},{\"end\":66593,\"start\":66585},{\"end\":66605,\"start\":66593},{\"end\":66931,\"start\":66924},{\"end\":66940,\"start\":66931},{\"end\":66948,\"start\":66940},{\"end\":67256,\"start\":67241},{\"end\":67265,\"start\":67256},{\"end\":67274,\"start\":67265},{\"end\":67522,\"start\":67515},{\"end\":67530,\"start\":67522},{\"end\":67536,\"start\":67530},{\"end\":67902,\"start\":67890},{\"end\":67909,\"start\":67902},{\"end\":67919,\"start\":67909},{\"end\":67930,\"start\":67919}]", "bib_venue": "[{\"end\":45153,\"start\":45114},{\"end\":45566,\"start\":45494},{\"end\":45940,\"start\":45902},{\"end\":46304,\"start\":46265},{\"end\":46638,\"start\":46584},{\"end\":47065,\"start\":47008},{\"end\":47521,\"start\":47425},{\"end\":48132,\"start\":48062},{\"end\":48673,\"start\":48566},{\"end\":49148,\"start\":49075},{\"end\":49740,\"start\":49686},{\"end\":50413,\"start\":50315},{\"end\":50929,\"start\":50878},{\"end\":51464,\"start\":51336},{\"end\":52122,\"start\":52034},{\"end\":52570,\"start\":52510},{\"end\":53052,\"start\":52924},{\"end\":53532,\"start\":53452},{\"end\":54006,\"start\":53927},{\"end\":54680,\"start\":54595},{\"end\":55094,\"start\":55027},{\"end\":55511,\"start\":55450},{\"end\":55847,\"start\":55821},{\"end\":56168,\"start\":56087},{\"end\":56592,\"start\":56555},{\"end\":56919,\"start\":56908},{\"end\":57318,\"start\":57204},{\"end\":57961,\"start\":57899},{\"end\":58319,\"start\":58267},{\"end\":58584,\"start\":58520},{\"end\":58864,\"start\":58782},{\"end\":59149,\"start\":59099},{\"end\":59468,\"start\":59384},{\"end\":59828,\"start\":59744},{\"end\":60231,\"start\":60182},{\"end\":60937,\"start\":60876},{\"end\":61503,\"start\":61392},{\"end\":62198,\"start\":62110},{\"end\":62628,\"start\":62609},{\"end\":62953,\"start\":62870},{\"end\":63237,\"start\":63217},{\"end\":63688,\"start\":63635},{\"end\":63993,\"start\":63977},{\"end\":64288,\"start\":64241},{\"end\":64730,\"start\":64706},{\"end\":65073,\"start\":65021},{\"end\":65546,\"start\":65489},{\"end\":65983,\"start\":65956},{\"end\":66303,\"start\":66289},{\"end\":66649,\"start\":66605},{\"end\":66975,\"start\":66948},{\"end\":67239,\"start\":67144},{\"end\":67610,\"start\":67536},{\"end\":67888,\"start\":67820},{\"end\":45965,\"start\":45942},{\"end\":47604,\"start\":47523},{\"end\":49208,\"start\":49150},{\"end\":49752,\"start\":49742},{\"end\":50498,\"start\":50415},{\"end\":51579,\"start\":51466},{\"end\":54089,\"start\":54008},{\"end\":55148,\"start\":55096},{\"end\":56236,\"start\":56170},{\"end\":57436,\"start\":57320},{\"end\":60985,\"start\":60939},{\"end\":61618,\"start\":61505},{\"end\":62273,\"start\":62200}]"}}}, "year": 2023, "month": 12, "day": 17}