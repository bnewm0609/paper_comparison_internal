{"id": 4633086, "updated": "2022-03-04 15:53:30.235", "metadata": {"title": "The OU-ISIR Gait Database Comprising the Treadmill Dataset", "authors": "[{\"first\":\"\",\"last\":\"YasushiMakihara\",\"middle\":[]},{\"first\":\"a\",\"last\":\"HidetoshiMannami\",\"middle\":[]},{\"first\":\"Akira\",\"last\":\"Tsuji\",\"middle\":[]},{\"first\":\"Md.\",\"last\":\"Hossain\",\"middle\":[\"Altab\"]},{\"first\":\"Kazushige\",\"last\":\"Sugiura\",\"middle\":[]},{\"first\":\"Yasushi\",\"last\":\"Yagi\",\"middle\":[]}]", "venue": "IPSJ Trans. Comput. Vis. Appl.", "journal": "IPSJ Trans. Comput. Vis. Appl.", "publication_date": {"year": 2012, "month": null, "day": null}, "abstract": "This paper describes a large-scale gait database comprising the Treadmill Dataset. The dataset focuses on variations in walking conditions and includes 200 subjects with 25 views, 34 subjects with 9 speed variations from 2 km/h to 10 km/h with a 1 km/h interval, and 68 subjects with at most 32 clothes variations. The range of variations in these three factors is significantly larger than that of previous gait databases, and therefore, the Treadmill Dataset can be used in research on invariant gait recognition. Moreover, the dataset contains more diverse gender and ages than the existing databases and hence it enables us to evaluate gait-based gender and age group classification in more statistically reliable way.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2040270931", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ipsjtcva/MakiharaMTHSMY12", "doi": "10.2197/ipsjtcva.4.53"}}, "content": {"source": {"pdf_hash": "55bc72e912c1db2c85dbbb6e4afe6eb7bbff89de", "pdf_src": "ScienceParseMerged", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.jstage.jst.go.jp/article/ipsjtcva/4/0/4_0_53/_pdf", "status": "BRONZE"}}, "grobid": {"id": "72a2f31a0e2015178bf5878879cd34445b3ab8b6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/55bc72e912c1db2c85dbbb6e4afe6eb7bbff89de.txt", "contents": "\nThe OU-ISIR Gait Database Comprising the Treadmill Dataset\nApr. 2012\n\nYasushi Makihara \nHidetoshi Mannami \nAkira Tsuji \nMd Altab Hossain \nKazushige Sugiura \nAtsushi Mori \nYasushi Yagi \nThe OU-ISIR Gait Database Comprising the Treadmill Dataset\n\nIPSJ Transactions on Computer Vision and Applications\n4Apr. 201210.2197/ipsjtcva.4.53]Received: June 1, 2011, Accepted: January 24, 2012, Released: April 10, 2012Technical Notegait databasetreadmillmultiple viewswalking speedclothing\nThis paper describes a large-scale gait database comprising the Treadmill Dataset. The dataset focuses on variations in walking conditions and includes 200 subjects with 25 views, 34 subjects with 9 speed variations from 2 km/h to 10 km/h with a 1 km/h interval, and 68 subjects with at most 32 clothes variations. The range of variations in these three factors is significantly larger than that of previous gait databases, and therefore, the Treadmill Dataset can be used in research on invariant gait recognition. Moreover, the dataset contains more diverse gender and ages than the existing databases and hence it enables us to evaluate gait-based gender and age group classification in more statistically reliable way.\n\nIntroduction\n\nIn modern society, there is a growing need to identify individuals in many different situations, including for surveillance and access control. For personal identification, many biometric-based authentication methods have been proposed using a wide variety of cues, such as fingerprints, irises, faces, and gait. Of these, gait identification has attracted considerable attention because it provides surveillance systems with the ability to ascertain identity at a distance. In fact, automatic gait recognition from public CCTV images has been admitted as evidence in UK courts [36], and gait evidence has been used as a cue for criminal investigations in Japan.\n\nRecently, various approaches to gait identification have been proposed. These range from model-based approaches [4], [37], [40], [41], [46] to appearance-based approaches [3], [6], [10], [14], [16], [17], [25], [26], [39]. In addition, several common gait databases have been published [7], [29], [31], [33], [44] for fair comparison of gait recognition approaches. These databases are usually constructed taking the following into account: (1) the variation in walking conditions, and (2) the number and diversity of the subjects.\n\nThe first consideration is important to ensure the robustness of the gait recognition algorithms, since walking conditions often differ between enrollment and test stages. For example, observation views are often inconsistent due to the positions of the CCTV cameras on the street and/or walking directions possibly being different. In addition, walking speeds can change depending on whether the person is merely taking a walk in the park or 1 Osaka University, Ibaraki, Osaka 567-0047, Japan 2 University of Rajshahi, Rajshahi, 6205, Bangladesh a) makihara@am.sanken.osaka-u.ac.jp is walking to the station in a hurry, and clothing almost certainly changes depending on the season.\n\nThe second consideration is also important because the number of subjects determines the upper bound of the statistical reliability of the performance evaluation. In addition, if the database is used not only for person identification, but also gender and age estimation from gait, the diversity of subjects in terms of gender and age plays an important role in the performance evaluations of such applications.\n\nIn this paper, we describe a large-scale gait database composed of the Treadmill Dataset based on the two considerations. The Treadmill Dataset is a set of gait datasets with variations in walking conditions, comprising 25 surrounding views, 9 walking speeds from 2 km/h to 10 km/h with a 1 km/h interval, at most 32 clothes combinations, and gait fluctuation variations among gait periods. The proposed gait dataset thus enables us to evaluate view-invariant, speed-invariant, and clothing-invariant gait recognition algorithms in a more extensive range. Moreover, it comprises 200 subjects of both genders and including a wide range of ages. The proposed gait database thus enables us to evaluate gait-based gender classification and age group classification.\n\nThe outline of this paper is as follows. First, existing gait databases are briefly considered in Section 2. Next, the Treadmill Dataset is addressed with related performance evaluations of gait recognition algorithms in Sections 3. Section 4 contains our conclusions, discussions, and future work in the area.\n\n\nRelated Work\n\nThe existing major gait databases are summarized in Table 1, with brief descriptions of the frequently used ones given below. A good summary of the other gait databases is found in Ref. [28].\n\nThe USF dataset [33] is one of the most widely used gait c 2012 Information Processing Society of Japan datasets and is composed of a gallery and 12 probe sequences under different walking conditions including factors such as views, shoes, surfaces, baggage, and time. As the number of factors is the largest of all the existing databases, and despite the number of variations in each factor being limited to 2, the USF database is suitable for evaluating the inter-factor, instead of intra-factor, impact on gait recognition performance.\n\nThe CMU MoBo Database [7] contains image sequences of persons walking on a treadmill captured by six cameras. As the treadmill can control the walking speed and slope, the database includes gait images with speed and slope variations as well as view variations. As a result, this database is often used for performance evaluation of speed-invariant or view-invariant gait recognition [16].\n\nThe Soton database [29] contains image sequences of a person walking around an inside track, with each subject filmed wearing a variety of footwear and clothing, carrying various bags, and walking at difference speeds. Hence, it is also used for exploratory factor analysis of gait recognition [5]. The recently published Soton Temporal database [23] contains the largest variations, up to 9 months, in elapsed time. It is, therefore, suitable for analyzing the effect of time on the performance of gait biometrics.\n\nThe CASIA dataset [44] contains the largest azimuth view variations and hence, it is useful for the analysis and modeling of the impact of view on gait recognition [45].\n\nThe OU-ISIR Large-scale database [30] contains the largest number of subjects, while the within-subject variation is limited. Therefore, it is useful for statistically reliable performance evaluation.\n\nIn this section, we further discuss three variations related to walking conditions: views, walking speeds, and clothes and also the number and diversity of subjects. Views: While the CASIA dataset [44] contains sufficient variations in terms of azimuth views, it does not contain any variation in the tilt view. Tilt view variations are quite important because most of the CCTV cameras capture pedestrians from somewhat tilted views. While the CMU MoBo Database [7] includes slightly tilted frontal and rear views, the variation in views is insufficient. Although the Soton Temporal database [23] covers 12 views including azimuth and tilt variations, the range of view variations is still smaller than that in the proposed database. Walking speeds: Variations in walking speeds are limited to less than three in most of the databases. The Georgia Tech database [35] contains four speeds with a 0.3 m/s (approx. 1.0 km/h) interval. The maximum speed is, however, less than 6.0 km/h and hence faster walking or running sequences are necessary for extensive performance analysis of speed-invariant gait recognition. Clothes: Variations in clothes are typically limited to normal clothes and a few types of coats and the numbers of variations are significantly small (at most three in the Soton database [29]). To adapt to actual variations in clothes, the database should contain various combinations of outer wear, pants (or skirts), and head wear. The number and diversity of subjects: Next, we review the number and diversity of subjects. As shown in Table 1, relatively large-scale gait databases with more than a hundred subjects are limited to the following four: the USF dataset [33], Soton database [29], CASIA dataset [44], and the OU-ISIR Large-scale dataset. Although these four databases provide a statistically reliable performance to some extent, the number of subjects is still not sufficient when compared with other biometrics such as fingerprints and faces except for the OU-ISIR Large-scale dataset.\n\nIn addition, populations of genders and ages are biased in the databases other than the OU-ISIR Large-scale dataset; e.g., there are no children in the USF dataset, while in the CASIA dataset most of the subjects are in their twenties and thirties and the ratio of males to females is 3 to 1. Such biases are undesirable in performance evaluation of gait-based gender and age estimation and performance comparison of gait recognition between genders and age groups. The proposed gait database: Contrary to existing databases, the proposed gait database aims to contain sufficient variations in terms of views, speeds, clothes, and subjects as summarized in Table 2. The proposed gait database contains gait images with the largest range of view variations (25 views: 12 azimuth views times 2 tilt angles, plus 1 top view), speed variations (9 speeds: 1 km/h interval between 2 km/h and 10 km/h), and clothing variations (up to 32 combinations), and as such, it is can be used for evaluating view-invariant [17], speed-invariant [20] and clothinginvariant [9] gait recognition. Moreover, the genders and ages of subjects are more diverse than the those of current gait database such as the CASIA dataset [44]. \n\n\nThe Treadmill Dataset\n\n\nDifference between Treadmill and Overground Gait\n\nAt the beginning, the difference between treadmill and overground gait is briefly discussed.\n\nSuch differences have actively discussed in the field of applied physiology, biomechanics, medical and health science for the last decades. Van Ingen Schenau [38] concluded that no mechanical differences exist between the two conditions in theory as long as the treadmill belt speed remains constant and that all differences must therefore originate from other than mechanical causes. The constant belt speed assumption is, however, often violated particularly at heal strike moment, and hence the differences may arise. In addition, Lee et al. [12] hypothesized that the differences arose from differences in optic flow subjects received on the treadmill and overground.\n\nMurray et al. [27] reported that no statistical differences in temporal gait parameters but claimed that subjects demonstrated trends for shorter step lengths and gait periods in case of treadmill walking. Although these trends are observed in our case in fact, they are relaxed as much as possible by providing sufficient time for each subject to practice walking on the treadmill. Moreover, in recent work [12], [32], while statistically significant differences between the two conditions are found in several aspects (e.g., kinematic parameter maxima and muscle activation patterns), it was reported that the overall patterns in joint moments and joint powers were quite similar between the two conditions.\n\nBased on the supports from these works [12], [27], [32], we conclude the treadmill gait dataset can be effectively used for the purpose of the vision-based gait recognition as well as the other overground gait datasets.\n\n\nCapturing System\n\nOur image capturing system consists primarily of a treadmill, 25 synchronous cameras * 1 (2 layers of 12 encircling cameras and an overhead camera with a mirror), and six screens surrounding the treadmill, as shown in Fig. 1. The treadmill (BIOMILL BM-2200) has a walking belt area, 550 mm wide and 2,000 mm long, and can control its speed up to 25.0 km/h with a 0.1 km/h interval. The cameras (Point Grey Research Inc. Flea2 models) are attached to camera poles aligned at the vertices of a regular dodecagon. Of these 25 cameras, 12 cameras in layer 1 are placed every 30 deg at a height of 1.3 m, 12 cameras in layer 2 are also placed every 30 deg at a height of 2.0 m, and 1 camera is placed near the side-view camera in layer 2 to observe the overhead view of a person walking on the treadmill via a large mirror attached to the ceiling. The lens focal length for each of the 24 surrounding cameras is 3.5 mm and that of the overhead view camera is 6.0 mm. The frame-rate and resolution of each camera are set to 60 fps and VGA, respectively, and the recorded format is uncompressed raw data. The surrounding screens are used as a chromakey background. Sample images captured in the system are also shown in Fig. 1. \n\n\nData Collection\n\nSubjects were obtained through open recruitment or from volunteers and signed a statement of consent regarding the use of their images for research purposes.\n\nAfter the practice sessions, subjects were asked to walk at 4 km/h or slower if necessary for children and the elderly, except during the data collection for speed variations. Subjects wore standard clothing (long-sleeved shirts and long pants, or their own casual clothes), except during the data collection for clothing variations.\n\n\nPreprocessing\n\nIn this section, we briefly describe a method for sizenormalized silhouette extraction as preprocessing. The first step involves extracting gait silhouette images, by exploiting background subtraction-based graph-cut segmentation [21]. * 1 This means that images from all the 25 views are captured at the same time.\n\nc 2012 Information Processing Society of Japan  The next step is scaling and registration of the extracted silhouette images [17]. First, the top, bottom, and horizontal center of the silhouette regions are obtained for each frame. The horizontal center is chosen as the median of the horizontal positions belonging to the region. Second, a moving average filter of 60 frames is applied to these positions. Third, we scale the silhouette images so that the height is just 128 pixels based on the averaged positions, and the aspect ratio of each region is maintained. Finally, we produce an 88 \u00d7 128 pixel image in which the averaged horizontal median corresponds to the horizontal center of the image. Examples of size-normalized silhouettes are shown in Fig. 2. * 2 Dataset A contains images of 34 subjects walking at speeds varying between 2 km/h and 10 km/h with a 1 km/h interval. The subjects walked for speeds between 2 km/h and 7 km/h and ran (or jogged) to achieve speeds of 8 km/h to 10 km/h. The number of recorded frames for each speed is listed in Table 3. Examples of size-normalized gait silhouettes are shown in Fig. 3.\n\n\nDataset A: Speed Variations\n\nThis dataset enables us to evaluate the performance of speedinvariant gait recognition algorithms. Thus, we conducted gait recognition experiments based on frequency-domain features [17] with and without a speed transformation model [20] for different speed gait recognition scenarios. The two different sub- * 2 Partially available on the website [31]. The remainder is being prepared for publication. sets used (called Dataset 1 and Dataset 2) were arranged so as to highlight the effect of the number of subjects used to train the transformation model. Datasets 1 and 2 use 14 and 9 training subjects, respectively, while both datasets have 20 testing subjects in common. In this experiment, speed variations between 2 km/h and 7 km/h were used. Two typical experimental settings, namely, matching between 4 km/h and 7 km/h, 7 km/h and 3 km/h, are evaluated by the Cumulative Matching Characteristics (CMC) curve which shows rank-k identification rate in the identification scenarios (one-tomany matching) as shown in Fig. 4. It is apparent that the speed transformation model (Dataset 1 and Dataset 2) improves performance compared with the method without transformation (No trans.).\n\nResults are also evaluated through the Equal Error Rate (EER) of the false acceptance rate and false rejection rate in the verification scenarios (one-to-one matching) as shown in  is also confirmed that the speed transformation model improves performance as a whole. We can compare our results with the other results by Tanawongsuwan et al. [35] with Georgia Tech database and also by Liu et al. [16] with the CMU MoBo datasets in terms of the rank-1 identification rate as shown in Table 4. Note that the gallery size of the Treadmill Dataset A is increased up to 25 subjects by using the 9 training subjects in the Dataset 2 in order to keep the consistency of the gallery size with those of the other databases. Moreover, we choose pairs of gallery and probe speeds similar to those in the other databases.\n\nDespite the limited speed variation range in the above experiments, it is possible in the future to evaluate how a speed-invariant gait recognition algorithm improves the performance for a much wider range of speed variations compared with the existing speedvariation gait databases [7], [29], [35], [44]. * 3 Dataset B contains images of 68 subjects with up to 32 combinations of types of clothing. Table 5 lists the clothing types, while Table 6 gives the combinations of clothing used in constructing the dataset. Figure 6 shows sample images of all the combinations of clothing types. All the gait sequences were captured twice on the same day. Thus, the total number of sequences in the dataset is 2,746. The large number of subjects and clothingvariations in the new dataset provides us with an estimate of intrasubject variations together with inter-subject variations for a better assessment of the potential of gait identification.\n\n\nDataset B: Clothing Variations\n\nWe evaluated the performance of several gait recognition ap- * 3 Fully available on the website [31].  [42], DATER [43], and CPDA [19], and a part-based frequency-domain feature approach [9]. The dataset was divided into three sets: a training set (20 subjects with all types of clothes), a gallery set (the remaining 48 subjects with a single type of clothes), and a probe set (the remaining 48 subjects with the other types of clothes) to separate the training and test sets in terms of subjects, and to separate the test gallery and test probe in terms of clothing, thereby enforcing strict separation conditions for the experimental evaluations. The gait identification and verification performances were evaluated with CMC and ROC curves as shown in Fig. 7, respectively. The results show that CPDA outperforms the other methods in the clothing-invariant gait recognition scenarios. * 4 Dataset C contains images of 200 subjects from 25 views. An example of 25 synchronous images is shown in Fig. 8. Naturally, this database enables us to evaluate the performance of multi-view gait recognition [34] and view-invariant gait recognition [17]. Moreover, because the 200 subjects comprise 100 males and 100 females with ages ranging from 4 to 75 years old, (see Fig. 9 for the age distribution), it can also be used for performance evaluation of gender and age group classification by * 4 To be prepared for publication. c 2012 Information Processing Society of Japan  gait [2], [11], [13]. Taking into account both properties of this dataset, we applied it to a multi-view gait feature analysis of gender and age. We defined four typical gender and age classes, namely children (younger than 15 years old), adult males and adult females (males and females between 15 and 65 years old, respectively), and the elderly (aged 65 years and older). Then, we analyzed the uniqueness of gait for each class. Figure 10 illustrates the average gait features for each class for typical views (side, front, right-back, and overhead) from layer 1 cameras. Figure 11 shows a comparison of these features for combinations of classes, namely children and adults (C-A), adult males and adult females (AM-AF), and adults and the elderly (A-E).\n\n\nDataset C: View Variations\n\nThe results reveal the uniqueness of gait features for the four typical classes from a computer-vision point of view. For example, the arm swings of children tend to be smaller than those of adults since walking in children is less mature. This can be observed in the side and overhead views. Moreover, males have wider shoulders, while females have more rounded bodies; both of these trends are particularly noticeable in the frontal and side views. The elderly have wider bodies than adults due to middleage spread, and this is clearly observed in the frontal view. See Ref. [22] for more detailed analyses and insights.\n\nIn addition to these analyses, the dataset C can be exploited for performance evaluations of view-invariant and multi-view gait recognition, although it remains as a future work. * 5 Dataset D contains 370 gait sequences of 185 subjects observed from the side view. The dataset focuses on gait fluctuations over a number of periods; that is, how gait silhouettes of the same phase differ across periods in a sequence. As a measure of gait fluctuation, we adopt Normalized AutoCorrelation (NAC) of size-normalized silhouettes for the temporal axis, which is often used for period detection as \n\n\nDataset D: Gait Fluctuations\nN gait = arg max N\u2208[Nmin,Nmax] C(N)(1)C(N) = x,y T (N)T (N) = N total \u2212 N \u2212 1,(2)\nwhere C(N) is the NAC for an N-frame shift, g(x, y, n) is the silhouette value at position (x, y) in the n-th frame, and N total is the total number of frames in the sequence. Successful gait period detection requires an appropriate search * 5 To be prepared for publication.\n\nc 2012 Information Processing Society of Japan  The NAC increases if gait silhouettes of the same phase across periods are similar to each other (stable gait), and vice versa (unstable gait or fluctuated gait). Hence, we define the two subsets: DB high comprising 100 subjects with the highest NAC, and DB low comprising 100 subjects with the lowest NAC. Examples of sizenormalized silhouettes for DB high and DB low are shown in Fig. 12.\n\nWe can see that silhouettes at the same phases for DB high are similar across periods, while those for DB low fluctuate across periods.\n\nNaturally, the subsets are expected to be used to evaluate how robust the gait recognition algorithms are against gait fluctuations. We evaluated the performance of several gait recognition approaches: Period-Period matching, Sequence-Period matching [24], and Sequence-Sequence matching in eigenspace [26], Average silhouette (or GEI) [8], [15], Frequency-domain feature [17], and Width vector [6]. The experiments were carried out on each subset and for each frame-rate. First, the CMC curves at 4 fps are shown in Fig. 13. In addition, EERs for all the frame-rates are shown in Fig. 14. The results show that, although Period-Period and Sequence-Period achieve relatively good per-formance for both subsets, the performance of DB low is significantly degraded compared with DB high as a whole, confirming that gait fluctuations have a large impact on gait recognition performance.\n\n\nConclusion and Discussion\n\nConclusion: This paper described a large-scale gait database composed of the Treadmill Dataset for performance evaluation of existing or future gait recognition algorithms. The dataset focuses on variations in walking conditions and includes 34 subjects with 9 speed variations from 2 km/h to 10 km/h with a 1 km/h interval (Dataset A), 68 subjects with up to 32 clothes variations (Dataset B), 200 subjects with 25 views (Dataset C), and 185 subjects with gait fluctuation variations (Dataset D). The variation in the former three factors is significantly larger than that in previous gait databases and therefore the Treadmill Dataset can be used for research on invariant gait recognition. Moreover, the Dataset C contains more diverse genders and ages than the existing databases and hence it enables us to evaluate the gait-based gender and age group classification performance in more statistically reliable way. Finally, several gait recognition approaches were tested using the proposed dataset. It was shown that the proposed database makes it possible to evaluate a wide range of gait recognition problems. Discussion: While each own gallery set is defined for each dataset in this experimental setup, experimental setup with one common gallery set is beneficial to analysis of the inter-factor impact on gait recognition performance as Sarkar et al. [33] did with the USF dataset. Such experimental setup, however, significantly limits the variety of performance evaluations, particularly in aspects of difficulty ranking caused by variations in gallery sets and the optimal gallery selection.\n\nIn fact, Sarkar et al. [33] also investigated difficulty ranking caused by variations in gallery sets with eight different gallery sets from the USF dataset. Hossain et al. [9] investigated the difficulty ranking in clothing-invariant gait recognition caused by variations in gallery clothes types with 15 different clothes types from the Treadmill Dataset B and they demonstrated that galleries with a long coat or a down jacket are much more difficult to be recognized than those with a full shirt or a parka. Color is used to denote which class' feature appears more strongly. Red indicates that the feature of the leftmost class (e.g., C of C-A) appears more strongly, while green depicts the opposite. The features are shown with their 1-and 2-times frequency multiplied 3 times for highlighting purposes.  Moreover, in the context of the view-invariant gait recognition by using view transformation model [17], the optimal view selection of a single-view gallery and the optimal combination of two-view galleries were investigated in Ref. [18]. As a result, it was reported that an oblique-view gallery is better than sideview or front-view gallery in single-view gallery case, and that a orthogonal-view combination is better in two-view gallery case, which is useful information for designing a camera alignment at an enrollment site.  These kinds of useful insights can be never acquired if gallery sets are limited to the common one (e.g., a gallery set where each subject walks at 4 km/h, wears type 9 clothes, and is observed from a side-view camera). In addition, unlike the USF dataset, the strength of the Treadmill Dataset lies in the wide intra-subject c 2012 Information Processing Society of Japan variation for each factor rather than the number of factors, and hence we would rather keep a variety of gallery sets than choosing the one common gallery set in this work. Future work: Although the proposed database has the largest diversity of all databases up to now, it is still lacking in some aspects, namely, shoes, bag, surface conditions, elapsed time, and scene types (e.g., outdoor scenes). Moreover, the number of subjects is still insufficient for statistically reliable performance evaluation of gait recognition. Therefore, we need to collect the required gait datasets by taking advantage of various demonstration events, such as outreach activities or open recruitment days in the future.\n\nFig. 2 Fig. 3\n23Examples Examples of size-normalized gait silhouettes for speed variations. Frame interval is adjusted so as to accommodate approximately one gait period.\n\nFig. 4\n4Experimental results for gait recognition incorporating speed variations[20]. The horizontal and vertical axes indicate rank and identification rate, respectively. The speed transformation model (Dataset 1 and Dataset 2) improves performance compared with the method without transformation (No trans.).\n\nFig. 5 .Fig. 5\n55It Experimental results for gait recognition incorporating speed variations[20]. The horizontal and vertical axes indicate gallery speed and EER, respectively. The speed transformation model (Dataset 1 and Dataset 2) improves performance compared with the method without transformation (No trans.).\n\nFig. 6 Fig. 7\n67Sample CMC and ROC curves for clothing-invariant gait recognition scenarios.\n\nFig. 8\n8Example of 25 synchronous images.\n\nFig. 9\n9Distribution of subjects' gender and age. range setting for the gait period candidate N in Eq. (1). Except for the dataset A (speed variations), it is assumed that each subject walks in a natural way (neither ox walk nor brisk walk), and hence we set the lower and upper bounds of the gait period for such natural walk as 0.83 sec and 1.3 sec, respectively. These bounds are then converted from second unit into frame unit by taking the frame-rate into consideration. For example, in case of 60 fps, the lower bound N min and the upper bound N max in frame unit, which are used in Eq.(1), are calculated as N min = 0.83 [sec]\u00d760 [fps] 50 [frame], N max = 1.3 [sec] \u00d7 60 [fps] 78 [frame], respectively.\n\nFig. 10 Fig. 11\n1011Average gait features for four classes (C: Children, AM: Adult Males, AF: Adult Females, E: the Elderly). The features are shown with their 1-and 2-times frequency multiplied 3 Differences in average features.\n\n( a )\naDB high (NAC: 0.96) (b) DB low (NAC: 0.85) Fig. 12 Examples of size-normalized silhouettes for DB high and DB low . Each row indicates a single period. Silhouettes at the same phases for DB high are similar across periods, while those for DB low fluctuate across periods. (a) DB high (b) DB low\n\nFig. 13\n13CMC curves for Dataset D at 4 fps.\n\nFig. 14\n14EERs for Dataset D.\n\nTable 1\n1Existing major gait databases.Database \n#Subjects \n#Sequences \nData covariates \nCMU MoBo database [7] \n25 \n600 \n6 views, 2 speeds, 2 slopes, baggage (ball) \nGeorgia Tech database [35] \n24 \n288 \n4 speeds (0.7, 1.0, 1.3, and 1.6 m/s) \nSoton database [23], [29] \n115 \n-\nViews \n25 \n\u223c2,000 \nTime (0, 1, 3, 4, 5, 8, 9 months), 12 views, \n2 clothes \nUSF dataset [33] \n122 \n1,870 \n2 views, 2 shoes, 2 surfaces, \nbaggage (w/ and w/o), time (6 months) \nCASIA dataset [44] \n124 \n13,640 \n11 views, clothing (w/ and w/o coat), \nbaggage (w/ and w/o) \n153 \n612 \n3 speeds, baggage (w/ and w/o), \nTokyoTech database [1] \n30 \n1,602 \n3 speeds \nOU-ISIR Large-scale dataset [30] \n1,035 \n2,070 \n2 views \n\n\n\nTable 2\n2Proposed gait database.Dataset \n#Subjects \n#Sequences \nData covariates \nThe Treadmill Dataset \n34 \n612 \n9 speeds (2, 3, 4, 5, 6, 7, 8, 9, and 10 km/h) \n68 \n2,746 \n32 clothes combination at most \n200 \n5,000 \n25 views (2 layers of 12 encircling cameras \nand an overhead camera) \n185 \n370 \nGait fluctuation among periods \n\nFig. 1 Overview of multi-view synchronous gait capturing system. \n\n\n\nTable 3\n3Number of recorded frames for each speed.Speed [km/h] \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n#Frames \n420 360 360 420 360 240 240 240 300 \n\n\n\nTable 4\n4Comparison of speed-invariant gait recognition by the other datasets. P 1 stands for rank-1 identification rate.Datasets \nLiterature \nGallery \nGallery speed \nProbe speed \nP 1 \nsize \n(km/h) \n(km/h) \n(%) \nThe Treadmill Dataset A \nRef. [20] \n25 \n2.0 \n6.0 \n64 \nGeorgia Tech database \nRef. [35] \n24 \n2.5 \n5.8 \n40 \nThe Treadmill Dataset A \nRef. [20] \n25 \n6.0 \n2.0 \n52 \nGeorgia Tech database \nRef. [35] \n24 \n5.8 \n2.5 \n30 \nThe Treadmill Dataset A \nRef. [20] \n25 \n4.0 \n3.0 \n96 \nCMU MoBo dataset \nRef. [16] \n25 \n4.5 \n3.3 \n84 \n\n\n\nTable 5\n5List of clothes used in the dataset (abbreviation: name).RP: Regular Pants \nHS: Half Shirt \nCW: Casual Wear \nBP: Baggy Pants \nFS: Full Shirt \nRC: Rain Coat \nSP: Short Pants \nLC: Long Coat \nHt: Hat \nSk: Skirt \nPk: Parker \nCs: Casquette Cap \nCP: Casual Pants \nDJ: Down Jacket \nMf: Muffler \n\nproaches: GEI [8]-based CSA \n\nTable 6\n6Different clothing combinations (#: clothing combination type; s i : i-th clothes slot).# \ns 1 \ns 2 \ns 3 \n2 \nRP \nHS \n-\n3 \nRP \nHS \nHt \n4 \nRP \nHS \nCs \n9 \nRP \nFS \n-\nX \nRP \nFS \nHt \nY \nRP \nFS \nCs \n5 \nRP \nLC \n-\n6 \nRP \nLC \nMf \n\n# \ns 1 \ns 2 \ns 3 \n7 \nRP \nLC \nHt \n8 \nRP \nLC \nCs \nC \nRP \nDJ \nMf \nA \nRP \nPk \n-\nB \nRP \nDJ \n-\nI \nBP \nHS \n-\nK \nBP \nFS \n-\nJ \nBP \nLC \n-\n\n# \ns 1 \ns 2 \nL \nBP \nPk \nM \nBP \nDJ \nN \nSP \nHS \nZ \nSP \nFS \nP \nSP \nPk \nS \nSk \nHS \nT \nSk \nFS \nU \nSk \nPk \n\n# \ns 1 \ns 2 \nV \nSk \nDJ \nD \nCP \nHS \nF \nCP \nFS \nE \nCP \nLC \nG \nCP \nPk \nH \nCP \nDJ \n0 \nCP \nCW \nR \nRC \nRC \n\ntype 2 \ntype 3 \ntype 4 \ntype 5 \ntype 6 \ntype 7 \ntype 8 \ntype 9 type A type B type C \n\ntype X type Y type D type E type F type G type H type 0 \ntype I \ntype J type K \n\ntype L \ntype M \ntype N \ntype P \ntype Z \ntype R \ntype S \ntype T \ntype U type V \n\n\nc 2012 Information Processing Society of Japan\n\nRobust Gait Recognition against Speed Variation. M R Aqmar, K Shinoda, S Furui, Proc. 20th Int. Conf. on Pattern Recognition. 20th Int. Conf. on Pattern RecognitionIstanbul, TurkeyAqmar, M.R., Shinoda, K. and Furui, S.: Robust Gait Recognition against Speed Variation, Proc. 20th Int. Conf. on Pattern Recognition, Istanbul, Turkey, pp.2190-2193 (2010).\n\nSupport Vector Machines for Automated Gait Classification. R Begg, IEEE Trans. Biomedical Engineering. 525Begg, R.: Support Vector Machines for Automated Gait Classifica- tion, IEEE Trans. Biomedical Engineering, Vol.52, No.5, pp.828-838 (2005).\n\nEigengait: Motion-based recognition people using image self-similarity. C Benabdelkader, R Culter, H Nanda, L Davis, Proc. Int. Conf. on Audio and Video-based Person Authentication. Int. Conf. on Audio and Video-based Person AuthenticationBenAbdelkader, C., Culter, R., Nanda, H. and Davis, L.: Eigengait: Motion-based recognition people using image self-similarity, Proc. Int. Conf. on Audio and Video-based Person Authentication, pp.284- 294 (2001).\n\nGait Recognition using Static Activityspecific Parameters. A Bobick, A Johnson, Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern Recognition1Bobick, A. and Johnson, A.: Gait Recognition using Static Activity- specific Parameters, Proc. IEEE Conf. on Computer Vision and Pattern Recognition, Vol.1, pp.423-430 (2001).\n\nExploratory Factor Analysis of Gait Recognition. I Bouchrika, M Nixon, 8th IEEE International Conference on Automatic Face and Gesture Recognition. Amsterdam, The NetherlandsBouchrika, I. and Nixon, M.: Exploratory Factor Analysis of Gait Recognition, 8th IEEE International Conference on Automatic Face and Gesture Recognition, Amsterdam, The Netherlands (2008).\n\nCombining Multiple Evidences for Gait Recognition. N Cuntoor, A Kale, R Chellappa, Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing. IEEE Int. Conf. on Acoustics, Speech, and Signal essing3Cuntoor, N., Kale, A. and Chellappa, R.: Combining Multiple Ev- idences for Gait Recognition, Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, Vol.3, pp.33-36 (2003).\n\nThe CMU Motion of Body (MoBo) Database. R Gross, J Shi, CMTTechnical reportGross, R. and Shi, J.: The CMU Motion of Body (MoBo) Database, Technical report, CMT (2001).\n\nIndividual Recognition Using Gait Energy Image. J Han, B Bhanu, Trans. Pattern Analysis and Machine Intelligence. 282Han, J. and Bhanu, B.: Individual Recognition Using Gait Energy Im- age, Trans. Pattern Analysis and Machine Intelligence, Vol.28, No.2, pp.316-322 (2006).\n\nClothing-Invariant Gait Identification using Part-based Clothing Categorization and Adaptive Weight Control. M A Hossain, Y Makihara, J Wang, Y Yagi, Pattern Recognition. 436Hossain, M.A., Makihara, Y., Wang, J. and Yagi, Y.: Clothing- Invariant Gait Identification using Part-based Clothing Categoriza- tion and Adaptive Weight Control, Pattern Recognition, Vol.43, No.6, pp.2281-2291 (2010).\n\nAction and Simultaneous Multiple-Person Identification Using Cubic Higher-Order Local Auto-Correlation. T Kobayashi, N Otsu, Proc. 17th Int. Conf. on Pattern Recognition. 17th Int. Conf. on Pattern Recognition3Kobayashi, T. and Otsu, N.: Action and Simultaneous Multiple-Person Identification Using Cubic Higher-Order Local Auto-Correlation, Proc. 17th Int. Conf. on Pattern Recognition, Vol.3, pp.741-744 (2004).\n\nRecognizing the sex of a walker from a dynamic point-light display. L Kozlowski, J Cutting, Perception and Psychophysics. 216Kozlowski, L. and Cutting, J.: Recognizing the sex of a walker from a dynamic point-light display, Perception and Psychophysics, Vol.21, No.6, pp.575-580 (1977).\n\nBiomechanics of overground vs. treadmill walking in healthy individuals. S J Lee, J Hidler, Journal of Applied Physiology. 104Lee, S.J. and Hidler, J.: Biomechanics of overground vs. tread- mill walking in healthy individuals, Journal of Applied Physiology, Vol.104, pp.747-755 (2008).\n\nGait Components and Their Application to Gender Recognition. X Li, S Maybank, S Yan, D Tao, D Xu, Trans. Systems, Man, and Cybernetics, Part C. 382Li, X., Maybank, S., Yan, S., Tao, D. and Xu, D.: Gait Components and Their Application to Gender Recognition, Trans. Systems, Man, and Cybernetics, Part C, Vol.38, No.2, pp.145-155 (2008).\n\nGait sequence analysis using frieze patterns. Y Liu, R Collins, Y Tsin, Proc. 7th European Conf. on Computer Vision. 7th European Conf. on Computer Vision2Liu, Y., Collins, R. and Tsin, Y.: Gait sequence analysis using frieze patterns, Proc. 7th European Conf. on Computer Vision, Vol.2, pp.657-671 (2002).\n\nSimplest Representation Yet for Gait Recognition: Averaged Silhouette. Z Liu, S Sarkar, Proc. 17th Int. Conf. on Pattern Recognition. 17th Int. Conf. on Pattern Recognition1Liu, Z. and Sarkar, S.: Simplest Representation Yet for Gait Recogni- tion: Averaged Silhouette, Proc. 17th Int. Conf. on Pattern Recogni- tion, Vol.1, pp.211-214 (2004).\n\nImproved Gait Recognition by Gait Dynamics Normalization. Z Liu, S Sarkar, IEEE Trans. Pattern Analysis and Machine Intelligence. 286Liu, Z. and Sarkar, S.: Improved Gait Recognition by Gait Dynam- ics Normalization, IEEE Trans. Pattern Analysis and Machine Intelli- gence, Vol.28, No.6, pp.863-876 (2006).\n\nGait Recognition Using a View Transformation Model in the Frequency Domain. Y Makihara, R Sagawa, Y Mukaigawa, T Echigo, Y Yagi, Proc. 9th European Conf. on Computer Vision. 9th European Conf. on Computer VisionGraz, AustriaMakihara, Y., Sagawa, R., Mukaigawa, Y., Echigo, T. and Yagi, Y.: Gait Recognition Using a View Transformation Model in the Fre- quency Domain, Proc. 9th European Conf. on Computer Vision, Graz, Austria, pp.151-163 (2006).\n\nWhich Reference View is Effective for Gait Identification Using a View Transformation Model?. Y Makihara, R Sagawa, Y Mukaigawa, T Echigo, Y Yagi, Proc. IEEE Computer Society Workshop on Biometrics. IEEE Computer Society Workshop on BiometricsNew York, USAMakihara, Y., Sagawa, R., Mukaigawa, Y., Echigo, T. and Yagi, Y.: Which Reference View is Effective for Gait Identification Using a View Transformation Model? Proc. IEEE Computer Society Work- shop on Biometrics 2006, New York, USA (2006).\n\nCluster-Pairwise Discriminant Analysis. Y Makihara, Y Yagi, Proc. 20th Int. Conf. on Pattern Recognition. 20th Int. Conf. on Pattern RecognitionIstanbul, TurkeyMakihara, Y. and Yagi, Y.: Cluster-Pairwise Discriminant Analy- sis, Proc. 20th Int. Conf. on Pattern Recognition, Istanbul, Turkey, pp.577-580 (2010).\n\nSilhouette Transformation based on Walking Speed for Gait Identification. Y Makihara, A Tsuji, Y Yagi, Proc. 23rd IEEE Conf. on Computer Vision and Pattern Recognition. 23rd IEEE Conf. on Computer Vision and Pattern RecognitionSan Francisco, CA, USAMakihara, Y., Tsuji, A. and Yagi, Y.: Silhouette Transformation based on Walking Speed for Gait Identification, Proc. 23rd IEEE Conf. on Computer Vision and Pattern Recognition, San Francisco, CA, USA (2010).\n\nSilhouette Extraction Based on Iterative Spatio-Temporal Local Color Transformation and Graph-Cut Segmentation. Y Makihara, Y Yagi, Proc. 19th Int. Conf. on Pattern Recognition. 19th Int. Conf. on Pattern RecognitionTampa, Florida USAMakihara, Y. and Yagi, Y.: Silhouette Extraction Based on Iterative Spatio-Temporal Local Color Transformation and Graph-Cut Segmen- tation, Proc. 19th Int. Conf. on Pattern Recognition, Tampa, Florida USA (2008).\n\nGait Analysis of Gender and Age using a Large-scale Multi-view Gait Database. H Mannami, Y Makihara, Y Yagi, Proc. 10th Asian Conf. on Computer Vision. 10th Asian Conf. on Computer VisionQueenstown, New ZealandMannami, H., Makihara, Y. and Yagi, Y.: Gait Analysis of Gender and Age using a Large-scale Multi-view Gait Database, Proc. 10th Asian Conf. on Computer Vision, Queenstown, New Zealand, pp.975-986 (2010).\n\nThe effect of time on the performance of gait biometrics. D Matovski, M Nixon, S Mahmoodi, J Carter, Proc. 4th IEEE Int. Conf. on Biometrics: Theory Applications and Systems. 4th IEEE Int. Conf. on Biometrics: Theory Applications and SystemsWashington D.C., USAMatovski, D., Nixon, M., Mahmoodi, S. and Carter, J.: The effect of time on the performance of gait biometrics, Proc. 4th IEEE Int. Conf. on Biometrics: Theory Applications and Systems, Washington D.C., USA, pp.1-6 (2010).\n\nGait Recognition using Periodbased Phase Synchronization for Low Frame-rate Videos. A Mori, Y Makihara, Y Yagi, Proc. 20th. 20thMori, A., Makihara, Y. and Yagi, Y.: Gait Recognition using Period- based Phase Synchronization for Low Frame-rate Videos, Proc. 20th\n\nInt, Conf, on Pattern Recognition. Istanbul, TurkeyInt. Conf. on Pattern Recognition, Istanbul, Turkey, pp.2194-2197 (2010).\n\nAutomatic Gait Recognition via Fourier Descriptors of Deformable Objects. S Mowbray, M Nixon, Proc. IEEE Conf. on Advanced Video and Signal Based Surveillance. IEEE Conf. on Advanced Video and Signal Based SurveillanceMowbray, S. and Nixon, M.: Automatic Gait Recognition via Fourier Descriptors of Deformable Objects, Proc. IEEE Conf. on Advanced Video and Signal Based Surveillance, pp.566-573 (2003).\n\nMoving Object Recognition in Eigenspace Representation: Gait Analysis and Lip Reading. H Murase, R Sakai, Pattern Recognition Letters. 17Murase, H. and Sakai, R.: Moving Object Recognition in Eigenspace Representation: Gait Analysis and Lip Reading, Pattern Recognition Letters, Vol.17, pp.155-162 (1996).\n\nM P Murray, G B Spurr, S B Sepic, G M Gardner, L A Mollinger, Treadmill vs. floor walking: Kinematics, electromyogram, and heart rate. 59Murray, M.P., Spurr, G.B., Sepic, S.B., Gardner, G.M. and Mollinger, L.A.: Treadmill vs. floor walking: Kinematics, electromyogram, and heart rate, Journal of Applied Physiology, Vol.59, No.1, pp.87-91 (1985).\n\nHuman Identification Based on Gait. M S Nixon, T N Tan, R Chellappa, International Series on Biometrics. Springer-VerlagNixon, M.S., Tan, T.N. and Chellappa, R.: Human Identification Based on Gait, International Series on Biometrics, Springer-Verlag (2005).\n\nExperimental plan for automatic gait recognition. M Nixon, J Carter, J Shutler, M Grant, SouthamptonTechnical reportNixon, M., Carter, J., Shutler, J. and Grant, M.: Experimental plan for automatic gait recognition, Technical report, Southampton (2001).\n\nPerformance Evaluation of Vision-based Gait Recognition using a Very Large-scale Gait Database. M Okumura, H Iwama, Y Makihara, Y Yagi, Proc. IEEE 4th Int. Conf. on Biometrics: Theory, Applications and Systems. IEEE 4th Int. Conf. on Biometrics: Theory, Applications and SystemsWashington D.C., USAOkumura, M., Iwama, H., Makihara, Y. and Yagi, Y.: Performance Evaluation of Vision-based Gait Recognition using a Very Large-scale Gait Database, Proc. IEEE 4th Int. Conf. on Biometrics: Theory, Ap- plications and Systems, Washington D.C., USA, pp.1-6 (2010).\n\n. Ou-Isir Gait Database, OU-ISIR Gait Database, available from http://www.am.sanken.osaka-u.ac.jp/GaitDB/index.html .\n\nA kinematic and kinetic comparison of overground and treadmill walking in healthy subjects. P O Riley, G Paolini, U D Croce, K W Paylo, D C Kerrigan, Gait & Posture. 261Riley, P.O., Paolini, G., Croce, U.D., Paylo, K.W. and Kerrigan, D.C.: A kinematic and kinetic comparison of overground and tread- mill walking in healthy subjects, Gait & Posture, Vol.26, No.1, pp.17- 24 (2007).\n\nThe HumanID Gait Challenge Problem: Data Sets, Performance, and Analysis. S Sarkar, J Phillips, Z Liu, I Vega, P Grother, K Bowyer, Trans. Pattern Analysis and Machine Intelligence. 272Sarkar, S., Phillips, J., Liu, Z., Vega, I., Grother, P. and Bowyer, K.: The HumanID Gait Challenge Problem: Data Sets, Performance, and Analysis, Trans. Pattern Analysis and Machine Intelligence, Vol.27, No.2, pp.162-177 (2005).\n\nGait Identification based on Multi-view Observations using Omnidirectional Camera. K Sugiura, Y Makihara, Y Yagi, Proc. 8th. 8thSugiura, K., Makihara, Y. and Yagi, Y.: Gait Identification based on Multi-view Observations using Omnidirectional Camera, Proc. 8th\n\nAsian Conf. on Computer Vision. Tokyo, Japan4843Asian Conf. on Computer Vision, LNCS 4843, Tokyo, Japan, pp.452- 461 (2007).\n\nModelling the Effects of Walking Speed on Appearance-Based Gait Recognition. R Tanawongsuwan, A Bobick, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2Tanawongsuwan, R. and Bobick, A.: Modelling the Effects of Walking Speed on Appearance-Based Gait Recognition, IEEE Computer Soci- ety Conference on Computer Vision and Pattern Recognition, Vol.2, pp.783-790 (2004).\n\nHow biometrics could change security. How biometrics could change security, available from http://news.bbc.co.uk/2/hi/programmes/click online/7702065.stm .\n\n3D Tracking for Gait Characterization and Recognition. R Urtasun, P Fua, Proc. 6th IEEE Int. Conf. on Automatic Face and Gesture Recognition. 6th IEEE Int. Conf. on Automatic Face and Gesture RecognitionUrtasun, R. and Fua, P.: 3D Tracking for Gait Characterization and Recognition, Proc. 6th IEEE Int. Conf. on Automatic Face and Ges- ture Recognition, pp.17-22 (2004).\n\nSome fundamental aspects of the biomechanics of overground versus treadmill locomotion. G J Van Ingen Schenau, Med Sci Sports Exerc. 124Van Ingen Schenau, G.J.: Some fundamental aspects of the biome- chanics of overground versus treadmill locomotion, Med Sci Sports Exerc., Vol.12, No.4, pp.257-261 (1980).\n\nWhat Image Information Is Important in Silhouette-Based Gait Recognition?. G Veres, L Gordon, J Carter, M Nixon, Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern Recognition2Veres, G., Gordon, L., Carter, J. and Nixon, M.: What Image Informa- tion Is Important in Silhouette-Based Gait Recognition? Proc. IEEE Conf. on Computer Vision and Pattern Recognition, Vol.2, pp.776-782 (2004).\n\nOn Automated Model-Based Extraction and Analysis of Gait. D Wagg, M Nixon, Proc. 6th IEEE Int. Conf. on Automatic Face and Gesture Recognition. 6th IEEE Int. Conf. on Automatic Face and Gesture RecognitionWagg, D. and Nixon, M.: On Automated Model-Based Extraction and Analysis of Gait, Proc. 6th IEEE Int. Conf. on Automatic Face and Gesture Recognition, pp.11-16 (2004).\n\nL Wang, H Ning, T Tan, W Hu, Fusion of Static and Dynamic Body Biometrics for Gait Recognition, Proc. 9th Int. Conf. on Computer Vision. 2Wang, L., Ning, H., Tan, T. and Hu, W.: Fusion of Static and Dy- namic Body Biometrics for Gait Recognition, Proc. 9th Int. Conf. on Computer Vision, Vol.2, pp.1449-1454 (2003).\n\nD Xu, S Yan, L Zhang, Z Liu, H.-J Zhang, H.-Y Shum, Concurrent Subspaces Analysis, Proc. IEEE Computer Society Conf. c 2012 Information Processing Society of Japan Computer Vision and Pattern Recognition. Xu, D., Yan, S., Zhang, L., Liu, Z., Zhang, H.-J. and Shum, H.-Y.: Concurrent Subspaces Analysis, Proc. IEEE Computer Society Conf. c 2012 Information Processing Society of Japan Computer Vision and Pattern Recognition, pp.203-208 (2005).\n\nDiscriminant Analysis with Tensor Representation. S Yan, D Xu, Q Yang, L Zhang, X Tang, H.-J Zhang, Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recognition. IEEE Computer Society Conf. Computer Vision and Pattern RecognitionYan, S., Xu, D., Yang, Q., Zhang, L., Tang, X. and Zhang, H.-J.: Dis- criminant Analysis with Tensor Representation, Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recognition, pp.526-532 (2005).\n\nA Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition. S Yu, D Tan, T Tan, Proc. 18th Int. Conf. on Pattern Recognition. 18th Int. Conf. on Pattern RecognitionHong Kong, China4Yu, S., Tan, D. and Tan, T.: A Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recogni- tion, Proc. 18th Int. Conf. on Pattern Recognition, Hong Kong, China, Vol.4, pp.441-444 (2006).\n\nModelling the Effect of View Angle Variation on Appearance-Based Gait Recognition. S Yu, D Tan, T Tan, Proc. 7th Asian Conf. on Computer Vision. 7th Asian Conf. on Computer Vision1Yu, S., Tan, D. and Tan, T.: Modelling the Effect of View Angle Vari- ation on Appearance-Based Gait Recognition, Proc. 7th Asian Conf. on Computer Vision, Vol.1, pp.807-816 (2006).\n\n3D Gait Recognition using Multiple Cameras. G Zhao, G Liu, H Li, M Pietikainen, Proc. 7th Int. Conf. on Automatic Face and Gesture Recognition. 7th Int. Conf. on Automatic Face and Gesture RecognitionZhao, G., Liu, G., Li, H. and Pietikainen, M.: 3D Gait Recognition using Multiple Cameras, Proc. 7th Int. Conf. on Automatic Face and Gesture Recognition, pp.529-534 (2006).\n", "annotations": {"author": "[{\"end\":88,\"start\":71},{\"end\":107,\"start\":89},{\"end\":120,\"start\":108},{\"end\":138,\"start\":121},{\"end\":157,\"start\":139},{\"end\":171,\"start\":158},{\"end\":185,\"start\":172}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":79},{\"end\":106,\"start\":99},{\"end\":119,\"start\":114},{\"end\":137,\"start\":124},{\"end\":156,\"start\":149},{\"end\":170,\"start\":166},{\"end\":184,\"start\":180}]", "author_first_name": "[{\"end\":78,\"start\":71},{\"end\":98,\"start\":89},{\"end\":113,\"start\":108},{\"end\":123,\"start\":121},{\"end\":148,\"start\":139},{\"end\":165,\"start\":158},{\"end\":179,\"start\":172}]", "author_affiliation": null, "title": "[{\"end\":59,\"start\":1},{\"end\":244,\"start\":186}]", "venue": "[{\"end\":299,\"start\":246}]", "abstract": "[{\"end\":1202,\"start\":480}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1800,\"start\":1796},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1997,\"start\":1994},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2003,\"start\":1999},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2009,\"start\":2005},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2015,\"start\":2011},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2021,\"start\":2017},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2056,\"start\":2053},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2061,\"start\":2058},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2067,\"start\":2063},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2073,\"start\":2069},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2079,\"start\":2075},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2085,\"start\":2081},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2091,\"start\":2087},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2097,\"start\":2093},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2103,\"start\":2099},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2171,\"start\":2168},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2177,\"start\":2173},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2183,\"start\":2179},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2189,\"start\":2185},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2195,\"start\":2191},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2859,\"start\":2858},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2910,\"start\":2909},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4793,\"start\":4789},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4816,\"start\":4812},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5361,\"start\":5358},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5724,\"start\":5720},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5750,\"start\":5746},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6024,\"start\":6021},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6077,\"start\":6073},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6266,\"start\":6262},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6412,\"start\":6408},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6452,\"start\":6448},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6818,\"start\":6814},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7082,\"start\":7079},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7213,\"start\":7209},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7483,\"start\":7479},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7922,\"start\":7918},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8305,\"start\":8301},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8326,\"start\":8322},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8346,\"start\":8342},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9645,\"start\":9641},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9667,\"start\":9663},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9693,\"start\":9690},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9842,\"start\":9838},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10177,\"start\":10173},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10564,\"start\":10560},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10706,\"start\":10702},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11100,\"start\":11096},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11106,\"start\":11102},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11442,\"start\":11438},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11448,\"start\":11444},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11454,\"start\":11450},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13624,\"start\":13620},{\"end\":13629,\"start\":13626},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13836,\"start\":13832},{\"end\":14473,\"start\":14470},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15059,\"start\":15055},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15110,\"start\":15106},{\"end\":15185,\"start\":15182},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15225,\"start\":15221},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16408,\"start\":16404},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16463,\"start\":16459},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17160,\"start\":17157},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17166,\"start\":17162},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17172,\"start\":17168},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":17178,\"start\":17174},{\"end\":17183,\"start\":17180},{\"end\":17913,\"start\":17910},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17949,\"start\":17945},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17956,\"start\":17952},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":17968,\"start\":17964},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17983,\"start\":17979},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18039,\"start\":18036},{\"end\":18740,\"start\":18737},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18953,\"start\":18949},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18994,\"start\":18990},{\"end\":19239,\"start\":19236},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19328,\"start\":19325},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19334,\"start\":19330},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19340,\"start\":19336},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20689,\"start\":20685},{\"end\":20914,\"start\":20911},{\"end\":21681,\"start\":21678},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22547,\"start\":22543},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22598,\"start\":22594},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22631,\"start\":22628},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22637,\"start\":22633},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22668,\"start\":22664},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22690,\"start\":22687},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24570,\"start\":24566},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24838,\"start\":24834},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24987,\"start\":24984},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25726,\"start\":25722},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25860,\"start\":25856},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27491,\"start\":27487},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27815,\"start\":27811}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27405,\"start\":27234},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27717,\"start\":27406},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28034,\"start\":27718},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28128,\"start\":28035},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28171,\"start\":28129},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28882,\"start\":28172},{\"attributes\":{\"id\":\"fig_7\"},\"end\":29113,\"start\":28883},{\"attributes\":{\"id\":\"fig_8\"},\"end\":29416,\"start\":29114},{\"attributes\":{\"id\":\"fig_9\"},\"end\":29462,\"start\":29417},{\"attributes\":{\"id\":\"fig_11\"},\"end\":29493,\"start\":29463},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30187,\"start\":29494},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30585,\"start\":30188},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30726,\"start\":30586},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31254,\"start\":30727},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31582,\"start\":31255},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32394,\"start\":31583}]", "paragraph": "[{\"end\":1880,\"start\":1218},{\"end\":2413,\"start\":1882},{\"end\":3098,\"start\":2415},{\"end\":3511,\"start\":3100},{\"end\":4274,\"start\":3513},{\"end\":4586,\"start\":4276},{\"end\":4794,\"start\":4603},{\"end\":5334,\"start\":4796},{\"end\":5725,\"start\":5336},{\"end\":6242,\"start\":5727},{\"end\":6413,\"start\":6244},{\"end\":6615,\"start\":6415},{\"end\":8633,\"start\":6617},{\"end\":9844,\"start\":8635},{\"end\":10013,\"start\":9921},{\"end\":10686,\"start\":10015},{\"end\":11397,\"start\":10688},{\"end\":11618,\"start\":11399},{\"end\":12860,\"start\":11639},{\"end\":13037,\"start\":12880},{\"end\":13372,\"start\":13039},{\"end\":13705,\"start\":13390},{\"end\":14841,\"start\":13707},{\"end\":16060,\"start\":14873},{\"end\":16872,\"start\":16062},{\"end\":17814,\"start\":16874},{\"end\":20077,\"start\":17849},{\"end\":20730,\"start\":20108},{\"end\":21324,\"start\":20732},{\"end\":21713,\"start\":21438},{\"end\":22153,\"start\":21715},{\"end\":22290,\"start\":22155},{\"end\":23175,\"start\":22292},{\"end\":24809,\"start\":23205},{\"end\":27233,\"start\":24811}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":21394,\"start\":21356},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21410,\"start\":21394},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21437,\"start\":21410}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":4662,\"start\":4655},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":8176,\"start\":8169},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":9299,\"start\":9292},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14774,\"start\":14767},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16553,\"start\":16546},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17281,\"start\":17274},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17321,\"start\":17314}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1216,\"start\":1204},{\"attributes\":{\"n\":\"2.\"},\"end\":4601,\"start\":4589},{\"attributes\":{\"n\":\"3.\"},\"end\":9868,\"start\":9847},{\"attributes\":{\"n\":\"3.1\"},\"end\":9919,\"start\":9871},{\"attributes\":{\"n\":\"3.2\"},\"end\":11637,\"start\":11621},{\"attributes\":{\"n\":\"3.3\"},\"end\":12878,\"start\":12863},{\"attributes\":{\"n\":\"3.4\"},\"end\":13388,\"start\":13375},{\"attributes\":{\"n\":\"3.5\"},\"end\":14871,\"start\":14844},{\"attributes\":{\"n\":\"3.6\"},\"end\":17847,\"start\":17817},{\"attributes\":{\"n\":\"3.7\"},\"end\":20106,\"start\":20080},{\"attributes\":{\"n\":\"3.8\"},\"end\":21355,\"start\":21327},{\"attributes\":{\"n\":\"4.\"},\"end\":23203,\"start\":23178},{\"end\":27248,\"start\":27235},{\"end\":27413,\"start\":27407},{\"end\":27733,\"start\":27719},{\"end\":28049,\"start\":28036},{\"end\":28136,\"start\":28130},{\"end\":28179,\"start\":28173},{\"end\":28899,\"start\":28884},{\"end\":29120,\"start\":29115},{\"end\":29425,\"start\":29418},{\"end\":29471,\"start\":29464},{\"end\":29502,\"start\":29495},{\"end\":30196,\"start\":30189},{\"end\":30594,\"start\":30587},{\"end\":30735,\"start\":30728},{\"end\":31263,\"start\":31256},{\"end\":31591,\"start\":31584}]", "table": "[{\"end\":30187,\"start\":29534},{\"end\":30585,\"start\":30221},{\"end\":30726,\"start\":30637},{\"end\":31254,\"start\":30849},{\"end\":31582,\"start\":31322},{\"end\":32394,\"start\":31681}]", "figure_caption": "[{\"end\":27405,\"start\":27251},{\"end\":27717,\"start\":27415},{\"end\":28034,\"start\":27736},{\"end\":28128,\"start\":28052},{\"end\":28171,\"start\":28138},{\"end\":28882,\"start\":28181},{\"end\":29113,\"start\":28904},{\"end\":29416,\"start\":29122},{\"end\":29462,\"start\":29428},{\"end\":29493,\"start\":29474},{\"end\":29534,\"start\":29504},{\"end\":30221,\"start\":30198},{\"end\":30637,\"start\":30596},{\"end\":30849,\"start\":30737},{\"end\":31322,\"start\":31265},{\"end\":31681,\"start\":31593}]", "figure_ref": "[{\"end\":11863,\"start\":11857},{\"end\":12859,\"start\":12852},{\"end\":14469,\"start\":14462},{\"end\":14840,\"start\":14834},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15900,\"start\":15894},{\"end\":17399,\"start\":17391},{\"end\":18610,\"start\":18604},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18852,\"start\":18846},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19119,\"start\":19113},{\"end\":19761,\"start\":19752},{\"end\":19904,\"start\":19895},{\"end\":22152,\"start\":22145},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":22816,\"start\":22809},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22881,\"start\":22873}]", "bib_author_first_name": "[{\"end\":32493,\"start\":32492},{\"end\":32495,\"start\":32494},{\"end\":32504,\"start\":32503},{\"end\":32515,\"start\":32514},{\"end\":32858,\"start\":32857},{\"end\":33118,\"start\":33117},{\"end\":33135,\"start\":33134},{\"end\":33145,\"start\":33144},{\"end\":33154,\"start\":33153},{\"end\":33558,\"start\":33557},{\"end\":33568,\"start\":33567},{\"end\":33920,\"start\":33919},{\"end\":33933,\"start\":33932},{\"end\":34287,\"start\":34286},{\"end\":34298,\"start\":34297},{\"end\":34306,\"start\":34305},{\"end\":34668,\"start\":34667},{\"end\":34677,\"start\":34676},{\"end\":34845,\"start\":34844},{\"end\":34852,\"start\":34851},{\"end\":35180,\"start\":35179},{\"end\":35182,\"start\":35181},{\"end\":35193,\"start\":35192},{\"end\":35205,\"start\":35204},{\"end\":35213,\"start\":35212},{\"end\":35570,\"start\":35569},{\"end\":35583,\"start\":35582},{\"end\":35949,\"start\":35948},{\"end\":35962,\"start\":35961},{\"end\":36242,\"start\":36241},{\"end\":36244,\"start\":36243},{\"end\":36251,\"start\":36250},{\"end\":36517,\"start\":36516},{\"end\":36523,\"start\":36522},{\"end\":36534,\"start\":36533},{\"end\":36541,\"start\":36540},{\"end\":36548,\"start\":36547},{\"end\":36840,\"start\":36839},{\"end\":36847,\"start\":36846},{\"end\":36858,\"start\":36857},{\"end\":37173,\"start\":37172},{\"end\":37180,\"start\":37179},{\"end\":37505,\"start\":37504},{\"end\":37512,\"start\":37511},{\"end\":37831,\"start\":37830},{\"end\":37843,\"start\":37842},{\"end\":37853,\"start\":37852},{\"end\":37866,\"start\":37865},{\"end\":37876,\"start\":37875},{\"end\":38297,\"start\":38296},{\"end\":38309,\"start\":38308},{\"end\":38319,\"start\":38318},{\"end\":38332,\"start\":38331},{\"end\":38342,\"start\":38341},{\"end\":38740,\"start\":38739},{\"end\":38752,\"start\":38751},{\"end\":39087,\"start\":39086},{\"end\":39099,\"start\":39098},{\"end\":39108,\"start\":39107},{\"end\":39584,\"start\":39583},{\"end\":39596,\"start\":39595},{\"end\":39999,\"start\":39998},{\"end\":40010,\"start\":40009},{\"end\":40022,\"start\":40021},{\"end\":40395,\"start\":40394},{\"end\":40407,\"start\":40406},{\"end\":40416,\"start\":40415},{\"end\":40428,\"start\":40427},{\"end\":40906,\"start\":40905},{\"end\":40914,\"start\":40913},{\"end\":40926,\"start\":40925},{\"end\":41285,\"start\":41284},{\"end\":41296,\"start\":41295},{\"end\":41703,\"start\":41702},{\"end\":41713,\"start\":41712},{\"end\":41923,\"start\":41922},{\"end\":41925,\"start\":41924},{\"end\":41935,\"start\":41934},{\"end\":41937,\"start\":41936},{\"end\":41946,\"start\":41945},{\"end\":41948,\"start\":41947},{\"end\":41957,\"start\":41956},{\"end\":41959,\"start\":41958},{\"end\":41970,\"start\":41969},{\"end\":41972,\"start\":41971},{\"end\":42307,\"start\":42306},{\"end\":42309,\"start\":42308},{\"end\":42318,\"start\":42317},{\"end\":42320,\"start\":42319},{\"end\":42327,\"start\":42326},{\"end\":42580,\"start\":42579},{\"end\":42589,\"start\":42588},{\"end\":42599,\"start\":42598},{\"end\":42610,\"start\":42609},{\"end\":42881,\"start\":42880},{\"end\":42892,\"start\":42891},{\"end\":42901,\"start\":42900},{\"end\":42913,\"start\":42912},{\"end\":43556,\"start\":43555},{\"end\":43558,\"start\":43557},{\"end\":43567,\"start\":43566},{\"end\":43578,\"start\":43577},{\"end\":43580,\"start\":43579},{\"end\":43589,\"start\":43588},{\"end\":43591,\"start\":43590},{\"end\":43600,\"start\":43599},{\"end\":43602,\"start\":43601},{\"end\":43921,\"start\":43920},{\"end\":43931,\"start\":43930},{\"end\":43943,\"start\":43942},{\"end\":43950,\"start\":43949},{\"end\":43958,\"start\":43957},{\"end\":43969,\"start\":43968},{\"end\":44346,\"start\":44345},{\"end\":44357,\"start\":44356},{\"end\":44369,\"start\":44368},{\"end\":44728,\"start\":44727},{\"end\":44745,\"start\":44744},{\"end\":45262,\"start\":45261},{\"end\":45273,\"start\":45272},{\"end\":45667,\"start\":45666},{\"end\":45669,\"start\":45668},{\"end\":45962,\"start\":45961},{\"end\":45971,\"start\":45970},{\"end\":45981,\"start\":45980},{\"end\":45991,\"start\":45990},{\"end\":46386,\"start\":46385},{\"end\":46394,\"start\":46393},{\"end\":46702,\"start\":46701},{\"end\":46710,\"start\":46709},{\"end\":46718,\"start\":46717},{\"end\":46725,\"start\":46724},{\"end\":47019,\"start\":47018},{\"end\":47025,\"start\":47024},{\"end\":47032,\"start\":47031},{\"end\":47041,\"start\":47040},{\"end\":47051,\"start\":47047},{\"end\":47063,\"start\":47059},{\"end\":47514,\"start\":47513},{\"end\":47521,\"start\":47520},{\"end\":47527,\"start\":47526},{\"end\":47535,\"start\":47534},{\"end\":47544,\"start\":47543},{\"end\":47555,\"start\":47551},{\"end\":48024,\"start\":48023},{\"end\":48030,\"start\":48029},{\"end\":48037,\"start\":48036},{\"end\":48456,\"start\":48455},{\"end\":48462,\"start\":48461},{\"end\":48469,\"start\":48468},{\"end\":48780,\"start\":48779},{\"end\":48788,\"start\":48787},{\"end\":48795,\"start\":48794},{\"end\":48801,\"start\":48800}]", "bib_author_last_name": "[{\"end\":32501,\"start\":32496},{\"end\":32512,\"start\":32505},{\"end\":32521,\"start\":32516},{\"end\":32863,\"start\":32859},{\"end\":33132,\"start\":33119},{\"end\":33142,\"start\":33136},{\"end\":33151,\"start\":33146},{\"end\":33160,\"start\":33155},{\"end\":33565,\"start\":33559},{\"end\":33576,\"start\":33569},{\"end\":33930,\"start\":33921},{\"end\":33939,\"start\":33934},{\"end\":34295,\"start\":34288},{\"end\":34303,\"start\":34299},{\"end\":34316,\"start\":34307},{\"end\":34674,\"start\":34669},{\"end\":34681,\"start\":34678},{\"end\":34849,\"start\":34846},{\"end\":34858,\"start\":34853},{\"end\":35190,\"start\":35183},{\"end\":35202,\"start\":35194},{\"end\":35210,\"start\":35206},{\"end\":35218,\"start\":35214},{\"end\":35580,\"start\":35571},{\"end\":35588,\"start\":35584},{\"end\":35959,\"start\":35950},{\"end\":35970,\"start\":35963},{\"end\":36248,\"start\":36245},{\"end\":36258,\"start\":36252},{\"end\":36520,\"start\":36518},{\"end\":36531,\"start\":36524},{\"end\":36538,\"start\":36535},{\"end\":36545,\"start\":36542},{\"end\":36551,\"start\":36549},{\"end\":36844,\"start\":36841},{\"end\":36855,\"start\":36848},{\"end\":36863,\"start\":36859},{\"end\":37177,\"start\":37174},{\"end\":37187,\"start\":37181},{\"end\":37509,\"start\":37506},{\"end\":37519,\"start\":37513},{\"end\":37840,\"start\":37832},{\"end\":37850,\"start\":37844},{\"end\":37863,\"start\":37854},{\"end\":37873,\"start\":37867},{\"end\":37881,\"start\":37877},{\"end\":38306,\"start\":38298},{\"end\":38316,\"start\":38310},{\"end\":38329,\"start\":38320},{\"end\":38339,\"start\":38333},{\"end\":38347,\"start\":38343},{\"end\":38749,\"start\":38741},{\"end\":38757,\"start\":38753},{\"end\":39096,\"start\":39088},{\"end\":39105,\"start\":39100},{\"end\":39113,\"start\":39109},{\"end\":39593,\"start\":39585},{\"end\":39601,\"start\":39597},{\"end\":40007,\"start\":40000},{\"end\":40019,\"start\":40011},{\"end\":40027,\"start\":40023},{\"end\":40404,\"start\":40396},{\"end\":40413,\"start\":40408},{\"end\":40425,\"start\":40417},{\"end\":40435,\"start\":40429},{\"end\":40911,\"start\":40907},{\"end\":40923,\"start\":40915},{\"end\":40931,\"start\":40927},{\"end\":41087,\"start\":41084},{\"end\":41093,\"start\":41089},{\"end\":41293,\"start\":41286},{\"end\":41302,\"start\":41297},{\"end\":41710,\"start\":41704},{\"end\":41719,\"start\":41714},{\"end\":41932,\"start\":41926},{\"end\":41943,\"start\":41938},{\"end\":41954,\"start\":41949},{\"end\":41967,\"start\":41960},{\"end\":41982,\"start\":41973},{\"end\":42315,\"start\":42310},{\"end\":42324,\"start\":42321},{\"end\":42337,\"start\":42328},{\"end\":42586,\"start\":42581},{\"end\":42596,\"start\":42590},{\"end\":42607,\"start\":42600},{\"end\":42616,\"start\":42611},{\"end\":42889,\"start\":42882},{\"end\":42898,\"start\":42893},{\"end\":42910,\"start\":42902},{\"end\":42918,\"start\":42914},{\"end\":43367,\"start\":43346},{\"end\":43564,\"start\":43559},{\"end\":43575,\"start\":43568},{\"end\":43586,\"start\":43581},{\"end\":43597,\"start\":43592},{\"end\":43611,\"start\":43603},{\"end\":43928,\"start\":43922},{\"end\":43940,\"start\":43932},{\"end\":43947,\"start\":43944},{\"end\":43955,\"start\":43951},{\"end\":43966,\"start\":43959},{\"end\":43976,\"start\":43970},{\"end\":44354,\"start\":44347},{\"end\":44366,\"start\":44358},{\"end\":44374,\"start\":44370},{\"end\":44742,\"start\":44729},{\"end\":44752,\"start\":44746},{\"end\":45270,\"start\":45263},{\"end\":45277,\"start\":45274},{\"end\":45687,\"start\":45670},{\"end\":45968,\"start\":45963},{\"end\":45978,\"start\":45972},{\"end\":45988,\"start\":45982},{\"end\":45997,\"start\":45992},{\"end\":46391,\"start\":46387},{\"end\":46400,\"start\":46395},{\"end\":46707,\"start\":46703},{\"end\":46715,\"start\":46711},{\"end\":46722,\"start\":46719},{\"end\":46728,\"start\":46726},{\"end\":47022,\"start\":47020},{\"end\":47029,\"start\":47026},{\"end\":47038,\"start\":47033},{\"end\":47045,\"start\":47042},{\"end\":47057,\"start\":47052},{\"end\":47068,\"start\":47064},{\"end\":47518,\"start\":47515},{\"end\":47524,\"start\":47522},{\"end\":47532,\"start\":47528},{\"end\":47541,\"start\":47536},{\"end\":47549,\"start\":47545},{\"end\":47561,\"start\":47556},{\"end\":48027,\"start\":48025},{\"end\":48034,\"start\":48031},{\"end\":48041,\"start\":48038},{\"end\":48459,\"start\":48457},{\"end\":48466,\"start\":48463},{\"end\":48473,\"start\":48470},{\"end\":48785,\"start\":48781},{\"end\":48792,\"start\":48789},{\"end\":48798,\"start\":48796},{\"end\":48813,\"start\":48802}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":12748115},\"end\":32796,\"start\":32443},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":18181838},\"end\":33043,\"start\":32798},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2974306},\"end\":33496,\"start\":33045},{\"attributes\":{\"id\":\"b3\"},\"end\":33868,\"start\":33498},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8215015},\"end\":34233,\"start\":33870},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2032491},\"end\":34625,\"start\":34235},{\"attributes\":{\"id\":\"b6\"},\"end\":34794,\"start\":34627},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":765267},\"end\":35068,\"start\":34796},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4649104},\"end\":35463,\"start\":35070},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14735721},\"end\":35878,\"start\":35465},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":143941887},\"end\":36166,\"start\":35880},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7076431},\"end\":36453,\"start\":36168},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4536769},\"end\":36791,\"start\":36455},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2508978},\"end\":37099,\"start\":36793},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14355118},\"end\":37444,\"start\":37101},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":101519},\"end\":37752,\"start\":37446},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4654536},\"end\":38200,\"start\":37754},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4652343},\"end\":38697,\"start\":38202},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4654851},\"end\":39010,\"start\":38699},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4665276},\"end\":39469,\"start\":39012},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4649636},\"end\":39918,\"start\":39471},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4677070},\"end\":40334,\"start\":39920},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14874855},\"end\":40819,\"start\":40336},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3556514},\"end\":41082,\"start\":40821},{\"attributes\":{\"id\":\"b24\"},\"end\":41208,\"start\":41084},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8133118},\"end\":41613,\"start\":41210},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14297649},\"end\":41920,\"start\":41615},{\"attributes\":{\"id\":\"b27\"},\"end\":42268,\"start\":41922},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53878328},\"end\":42527,\"start\":42270},{\"attributes\":{\"id\":\"b29\"},\"end\":42782,\"start\":42529},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4657129},\"end\":43342,\"start\":42784},{\"attributes\":{\"id\":\"b31\"},\"end\":43461,\"start\":43344},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":25943313},\"end\":43844,\"start\":43463},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":7693282},\"end\":44260,\"start\":43846},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4671431},\"end\":44522,\"start\":44262},{\"attributes\":{\"id\":\"b35\"},\"end\":44648,\"start\":44524},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3244412},\"end\":45047,\"start\":44650},{\"attributes\":{\"id\":\"b37\"},\"end\":45204,\"start\":45049},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3050175},\"end\":45576,\"start\":45206},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":22875992},\"end\":45884,\"start\":45578},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":10213657},\"end\":46325,\"start\":45886},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":8106765},\"end\":46699,\"start\":46327},{\"attributes\":{\"id\":\"b42\"},\"end\":47016,\"start\":46701},{\"attributes\":{\"id\":\"b43\"},\"end\":47461,\"start\":47018},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4541399},\"end\":47915,\"start\":47463},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1815453},\"end\":48370,\"start\":47917},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":606947},\"end\":48733,\"start\":48372},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":9873666},\"end\":49108,\"start\":48735}]", "bib_title": "[{\"end\":32490,\"start\":32443},{\"end\":32855,\"start\":32798},{\"end\":33115,\"start\":33045},{\"end\":33555,\"start\":33498},{\"end\":33917,\"start\":33870},{\"end\":34284,\"start\":34235},{\"end\":34842,\"start\":34796},{\"end\":35177,\"start\":35070},{\"end\":35567,\"start\":35465},{\"end\":35946,\"start\":35880},{\"end\":36239,\"start\":36168},{\"end\":36514,\"start\":36455},{\"end\":36837,\"start\":36793},{\"end\":37170,\"start\":37101},{\"end\":37502,\"start\":37446},{\"end\":37828,\"start\":37754},{\"end\":38294,\"start\":38202},{\"end\":38737,\"start\":38699},{\"end\":39084,\"start\":39012},{\"end\":39581,\"start\":39471},{\"end\":39996,\"start\":39920},{\"end\":40392,\"start\":40336},{\"end\":40903,\"start\":40821},{\"end\":41282,\"start\":41210},{\"end\":41700,\"start\":41615},{\"end\":42304,\"start\":42270},{\"end\":42878,\"start\":42784},{\"end\":43553,\"start\":43463},{\"end\":43918,\"start\":43846},{\"end\":44343,\"start\":44262},{\"end\":44725,\"start\":44650},{\"end\":45259,\"start\":45206},{\"end\":45664,\"start\":45578},{\"end\":45959,\"start\":45886},{\"end\":46383,\"start\":46327},{\"end\":47511,\"start\":47463},{\"end\":48021,\"start\":47917},{\"end\":48453,\"start\":48372},{\"end\":48777,\"start\":48735}]", "bib_author": "[{\"end\":32503,\"start\":32492},{\"end\":32514,\"start\":32503},{\"end\":32523,\"start\":32514},{\"end\":32865,\"start\":32857},{\"end\":33134,\"start\":33117},{\"end\":33144,\"start\":33134},{\"end\":33153,\"start\":33144},{\"end\":33162,\"start\":33153},{\"end\":33567,\"start\":33557},{\"end\":33578,\"start\":33567},{\"end\":33932,\"start\":33919},{\"end\":33941,\"start\":33932},{\"end\":34297,\"start\":34286},{\"end\":34305,\"start\":34297},{\"end\":34318,\"start\":34305},{\"end\":34676,\"start\":34667},{\"end\":34683,\"start\":34676},{\"end\":34851,\"start\":34844},{\"end\":34860,\"start\":34851},{\"end\":35192,\"start\":35179},{\"end\":35204,\"start\":35192},{\"end\":35212,\"start\":35204},{\"end\":35220,\"start\":35212},{\"end\":35582,\"start\":35569},{\"end\":35590,\"start\":35582},{\"end\":35961,\"start\":35948},{\"end\":35972,\"start\":35961},{\"end\":36250,\"start\":36241},{\"end\":36260,\"start\":36250},{\"end\":36522,\"start\":36516},{\"end\":36533,\"start\":36522},{\"end\":36540,\"start\":36533},{\"end\":36547,\"start\":36540},{\"end\":36553,\"start\":36547},{\"end\":36846,\"start\":36839},{\"end\":36857,\"start\":36846},{\"end\":36865,\"start\":36857},{\"end\":37179,\"start\":37172},{\"end\":37189,\"start\":37179},{\"end\":37511,\"start\":37504},{\"end\":37521,\"start\":37511},{\"end\":37842,\"start\":37830},{\"end\":37852,\"start\":37842},{\"end\":37865,\"start\":37852},{\"end\":37875,\"start\":37865},{\"end\":37883,\"start\":37875},{\"end\":38308,\"start\":38296},{\"end\":38318,\"start\":38308},{\"end\":38331,\"start\":38318},{\"end\":38341,\"start\":38331},{\"end\":38349,\"start\":38341},{\"end\":38751,\"start\":38739},{\"end\":38759,\"start\":38751},{\"end\":39098,\"start\":39086},{\"end\":39107,\"start\":39098},{\"end\":39115,\"start\":39107},{\"end\":39595,\"start\":39583},{\"end\":39603,\"start\":39595},{\"end\":40009,\"start\":39998},{\"end\":40021,\"start\":40009},{\"end\":40029,\"start\":40021},{\"end\":40406,\"start\":40394},{\"end\":40415,\"start\":40406},{\"end\":40427,\"start\":40415},{\"end\":40437,\"start\":40427},{\"end\":40913,\"start\":40905},{\"end\":40925,\"start\":40913},{\"end\":40933,\"start\":40925},{\"end\":41089,\"start\":41084},{\"end\":41095,\"start\":41089},{\"end\":41295,\"start\":41284},{\"end\":41304,\"start\":41295},{\"end\":41712,\"start\":41702},{\"end\":41721,\"start\":41712},{\"end\":41934,\"start\":41922},{\"end\":41945,\"start\":41934},{\"end\":41956,\"start\":41945},{\"end\":41969,\"start\":41956},{\"end\":41984,\"start\":41969},{\"end\":42317,\"start\":42306},{\"end\":42326,\"start\":42317},{\"end\":42339,\"start\":42326},{\"end\":42588,\"start\":42579},{\"end\":42598,\"start\":42588},{\"end\":42609,\"start\":42598},{\"end\":42618,\"start\":42609},{\"end\":42891,\"start\":42880},{\"end\":42900,\"start\":42891},{\"end\":42912,\"start\":42900},{\"end\":42920,\"start\":42912},{\"end\":43369,\"start\":43346},{\"end\":43566,\"start\":43555},{\"end\":43577,\"start\":43566},{\"end\":43588,\"start\":43577},{\"end\":43599,\"start\":43588},{\"end\":43613,\"start\":43599},{\"end\":43930,\"start\":43920},{\"end\":43942,\"start\":43930},{\"end\":43949,\"start\":43942},{\"end\":43957,\"start\":43949},{\"end\":43968,\"start\":43957},{\"end\":43978,\"start\":43968},{\"end\":44356,\"start\":44345},{\"end\":44368,\"start\":44356},{\"end\":44376,\"start\":44368},{\"end\":44744,\"start\":44727},{\"end\":44754,\"start\":44744},{\"end\":45272,\"start\":45261},{\"end\":45279,\"start\":45272},{\"end\":45689,\"start\":45666},{\"end\":45970,\"start\":45961},{\"end\":45980,\"start\":45970},{\"end\":45990,\"start\":45980},{\"end\":45999,\"start\":45990},{\"end\":46393,\"start\":46385},{\"end\":46402,\"start\":46393},{\"end\":46709,\"start\":46701},{\"end\":46717,\"start\":46709},{\"end\":46724,\"start\":46717},{\"end\":46730,\"start\":46724},{\"end\":47024,\"start\":47018},{\"end\":47031,\"start\":47024},{\"end\":47040,\"start\":47031},{\"end\":47047,\"start\":47040},{\"end\":47059,\"start\":47047},{\"end\":47070,\"start\":47059},{\"end\":47520,\"start\":47513},{\"end\":47526,\"start\":47520},{\"end\":47534,\"start\":47526},{\"end\":47543,\"start\":47534},{\"end\":47551,\"start\":47543},{\"end\":47563,\"start\":47551},{\"end\":48029,\"start\":48023},{\"end\":48036,\"start\":48029},{\"end\":48043,\"start\":48036},{\"end\":48461,\"start\":48455},{\"end\":48468,\"start\":48461},{\"end\":48475,\"start\":48468},{\"end\":48787,\"start\":48779},{\"end\":48794,\"start\":48787},{\"end\":48800,\"start\":48794},{\"end\":48815,\"start\":48800}]", "bib_venue": "[{\"end\":32623,\"start\":32569},{\"end\":33284,\"start\":33227},{\"end\":33692,\"start\":33639},{\"end\":34044,\"start\":34018},{\"end\":34440,\"start\":34385},{\"end\":35674,\"start\":35636},{\"end\":36947,\"start\":36910},{\"end\":37273,\"start\":37235},{\"end\":37978,\"start\":37928},{\"end\":38458,\"start\":38401},{\"end\":38859,\"start\":38805},{\"end\":39261,\"start\":39181},{\"end\":39705,\"start\":39649},{\"end\":40130,\"start\":40072},{\"end\":40597,\"start\":40511},{\"end\":40949,\"start\":40945},{\"end\":41135,\"start\":41119},{\"end\":41428,\"start\":41370},{\"end\":43082,\"start\":42995},{\"end\":44390,\"start\":44387},{\"end\":44568,\"start\":44556},{\"end\":45409,\"start\":45348},{\"end\":46113,\"start\":46060},{\"end\":46532,\"start\":46471},{\"end\":47705,\"start\":47638},{\"end\":48143,\"start\":48089},{\"end\":48551,\"start\":48517},{\"end\":48935,\"start\":48879},{\"end\":32567,\"start\":32523},{\"end\":32899,\"start\":32865},{\"end\":33225,\"start\":33162},{\"end\":33637,\"start\":33578},{\"end\":34016,\"start\":33941},{\"end\":34383,\"start\":34318},{\"end\":34665,\"start\":34627},{\"end\":34908,\"start\":34860},{\"end\":35239,\"start\":35220},{\"end\":35634,\"start\":35590},{\"end\":36000,\"start\":35972},{\"end\":36289,\"start\":36260},{\"end\":36597,\"start\":36553},{\"end\":36908,\"start\":36865},{\"end\":37233,\"start\":37189},{\"end\":37574,\"start\":37521},{\"end\":37926,\"start\":37883},{\"end\":38399,\"start\":38349},{\"end\":38803,\"start\":38759},{\"end\":39179,\"start\":39115},{\"end\":39647,\"start\":39603},{\"end\":40070,\"start\":40029},{\"end\":40509,\"start\":40437},{\"end\":40943,\"start\":40933},{\"end\":41117,\"start\":41095},{\"end\":41368,\"start\":41304},{\"end\":41748,\"start\":41721},{\"end\":42055,\"start\":41984},{\"end\":42373,\"start\":42339},{\"end\":42577,\"start\":42529},{\"end\":42993,\"start\":42920},{\"end\":43627,\"start\":43613},{\"end\":44026,\"start\":43978},{\"end\":44385,\"start\":44376},{\"end\":44554,\"start\":44524},{\"end\":44829,\"start\":44754},{\"end\":45085,\"start\":45049},{\"end\":45346,\"start\":45279},{\"end\":45709,\"start\":45689},{\"end\":46058,\"start\":45999},{\"end\":46469,\"start\":46402},{\"end\":46836,\"start\":46730},{\"end\":47221,\"start\":47070},{\"end\":47636,\"start\":47563},{\"end\":48087,\"start\":48043},{\"end\":48515,\"start\":48475},{\"end\":48877,\"start\":48815}]"}}}, "year": 2023, "month": 12, "day": 17}