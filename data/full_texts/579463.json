{"id": 579463, "updated": "2023-11-07 20:10:45.52", "metadata": {"title": "Quantum Recommendation Systems", "authors": "[{\"first\":\"Iordanis\",\"last\":\"Kerenidis\",\"middle\":[]},{\"first\":\"Anupam\",\"last\":\"Prakash\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2016, "month": 3, "day": 29}, "abstract": "A recommendation system uses the past purchases or ratings of $n$ products by a group of $m$ users, in order to provide personalised recommendations to individual users. The information is modelled as an $m \\times n$ preference matrix which is assumed to have a good $k$-rank approximation, for a small constant $k$. In this work, we present a quantum algorithm for recommendation systems that has running time $O(\\text{poly}(k)\\text{polylog}(mn))$. All classical algorithms for recommendation systems that work through reconstructing an approximation of the preference matrix run in time polynomial in the matrix dimension. Our algorithm provides good recommendations by sampling efficiently from an approximation of the preference matrix, without reconstructing the entire matrix. For this, we design an efficient quantum procedure to project a given vector onto the row space of a given matrix. This is the first algorithm for recommendation systems that runs in time polylogarithmic in the dimensions of the matrix and provides a real world application of quantum algorithms in machine learning.", "fields_of_study": "[\"Physics\",\"Computer Science\"]", "external_ids": {"arxiv": "1603.08675", "mag": "2964039664", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/innovations/KerenidisP17", "doi": "10.4230/lipics.itcs.2017.49"}}, "content": {"source": {"pdf_hash": "0f629343a70b35d9ebe2bdf4fa4cc64b7699d6cd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1603.08675v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "07ae585ba47845c1692274adeaf3c33de232d0f3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0f629343a70b35d9ebe2bdf4fa4cc64b7699d6cd.txt", "contents": "\nQuantum Recommendation Systems\n31 Mar 2016 April 1, 2016\n\nIordanis Kerenidis \nCentre for Quantum Technologies\nCNRS\nIRIF\nUniversit\u00e9 Paris Diderot\nParisFrance\n\nNational University of Singapore\nSingapore\n\nAnupam Prakash aprakash@ntu.edu.sg. \nCentre for Quantum Technologies and School of Physical and Mathematical Sciences\nNanyang Technological University\nSingapore\n\nQuantum Recommendation Systems\n31 Mar 2016 April 1, 2016arXiv:1603.08675v2 [quant-ph] 1\nA recommendation system uses the past purchases or ratings of n products by a group of m users, in order to provide personalised recommendations to individual users. The information is modelled as an m \u00d7 n preference matrix which is assumed to have a good k-rank approximation, for a small constant k.In this work, we present a quantum algorithm for recommendation systems that has running time O(poly(k)polylog(mn)). All classical algorithms for recommendation systems that work through reconstructing an approximation of the preference matrix run in time polynomial in the matrix dimension. Our algorithm provides good recommendations by sampling efficiently from an approximation of the preference matrix, without reconstructing the entire matrix. For this, we design an efficient quantum procedure to project a given vector onto the row space of a given matrix. This is the first algorithm for recommendation systems that runs in time polylogarithmic in the dimensions of the matrix and provides a real world application of quantum algorithms in machine learning.\n\nIntroduction\n\nA recommendation system uses information about past purchases or ratings of products by a group of users in order to provide personalized recommendations to individual users. More precisely, we assume there are m users, for example clients of an online platform like Amazon or Netflix, each of whom have some inherent preference or utility about n products, for example books, movies etc. The user preferences are modeled by an m \u00d7 n matrix P , where the element P ij denotes how much the user i values product j. If the preference matrix P had been known in advance, it would have been easy to make good recommendations to the users by selecting elements of this matrix with high value. However, this matrix is not known a priori. Information about P arrives in an online manner each time a user buys a product, writes a review, or fills out a survey. A recommendation system tries to utilize the already known information about all users in order to suggest products to individual users that have high utility for them and can eventually lead to a purchase.\n\nThere has been an extensive body of work on recommendation systems, since it is both a very interesting theoretical problem and also of great importance to the industry. We cite the works of [5,18,8,4] who studied the problem in a combinatorial or linear algebraic fashion. There has also been a series of works in the machine learning community many of them inspired by a practical challenge by Netflix on real world data [14,6,13].\n\nThe low-rank assumption. The underlying assumption in recommendation systems is that one can infer information about a specific user from the information about all other users because, in some sense, the majority of users belong to some well-defined \"types\". In other words, most people's likes are not unique but fall into one of a small number of categories. Hence, we can aggregate the information of \"similar\" users to predict which products have high utility for an individual user.\n\nMore formally, the assumption in recommendation systems is that the preference matrix P can be well approximated (according to some distance measure) by a low-rank matrix. There are different reasons why this assumption is indeed justified. First, from a philosophical and cognitive science perspective, it is believed that there are few inherent reasons why people buy or not a product: the price, the quality, the popularity, the brand recognition, etc. (see for example [5,21]). Each user can be thought of as weighing these small number of properties differently but still, his preference for a product can be computed by checking how the product scores in these properties. Such a model produces a matrix P which has a good k-rank approximation, for a small k, which can be thought of as a constant independent of the number of users m or the number of products n. Moreover, a number of theoretical models of users have been proposed in the literature which give rise to a matrix with good low-rank approximation. For example, if one assumes that the users belong to a small number of \"types\", where a type can be thought of as an archetypical user, and then each individual user belonging to this type is some noisy version of this archetypical user, then the matrix has a good low-rank approximation [8,18]. In addition, the study of the preference matrices that come from real data, also show that the rank of the matrix is asymptotically much smaller than the size of the matrix.\n\nFor these reasons, the assumption that the matrix P has a good low-rank approximation has been widely used in the literature. In fact, if we look a little more carefully at this assumption, we can see that if we want to justify that a recommendation system provides high-value recommendations for a user based on the information of other users, we need not only assume that users\"belong\" to a small number of user types, but that they agree with these types on the high-value elements.\n\nFor contradiction, imagine that there are k-types of users, where each type has very few high-value elements and many small value elements. Then, the users who belong to each type can agree on all the small value elements and have completely different high-value elements. In other words, even though the matrix is low-rank, the recommendations would be of no quality. Hence, the assumption that has been implicitly made, either by philosophical reasons or by modelling the users, is that there are k-types of users and the users of each type \"agree\" on the high-value elements.\n\nRecommendations by Matrix Reconstruction. One of the most powerful and common ways to provide competitive recommendation systems is through a procedure called matrix reconstruction. In this framework, we assume that there exists a hidden matrix A, in our case the preference matrix, which can be well approximated by a low-rank matrix. The reconstruction algorithm gets as input a number of samples from the matrix, in our case the previous data about the users' preferences, and outputs a k-rank matrix with the guarantee that it is \"close\" to the hidden matrix A according to some measure (for example, the 2-or the Frobenius norm). For example, the reconstruction algorithm can perform a Singular Value Decomposition of the matrix A, called the subsample matrix, where A agrees on A on the known samples and has zero on all other unknown entries, and output the projection of the matrix A on the space spanned by its top k singular vectors. The \"closeness\" property guarantees that the recommendation system will select an element that with high probability corresponds to a high-value element in the matrix A and hence it is a good recommendation ( [8,5]). Another commonly used algorithm for matrix reconstruction is a variant of alternating minimization, this has been successful in practice [14] and has been recently analyzed theoretically [11]. Note that all known algorithms for matrix reconstruction require time polynomial in the matrix dimensions.\n\nAn important remark is that matrix reconstruction is a harder task than recommendation systems, in the sense that a good recommendation system only needs to output a high value element of the matrix and not the entire matrix [20,3]. Nevertheless, classical algorithms perform a reconstruction of the entire matrix as the resources required for finding high value elements are the same as the resources needed for full reconstruction.\n\nComputational resources and performance. In order to make a precise comparison between classical recommendation systems and our proposed system, we discuss more explicitly the computational resources in recommendation systems. We are interested in systems that arise in the real world, for example on Amazon or Netflix, where the number of users can be about 100 million and the products around one million. For such large systems, storing the entire preference matrix or doing heavy computations every time a user comes into the system is prohibitive.\n\nWe define two stages in our systems. First, the data entry stage, where a new piece of information arrives to the system in the form of an element P ij of the preference matrix and needs to be stored. We require that the time to write this element in the data structure, as well as read it out, is polylogarithmic in the dimensions of the matrix, while the memory is minimized. For example, one can store the elements (i, j, P ij ) in an ordered list. This way, the read and write is efficient and the total memory needed is linear (up to polylogarithmic terms) in the sparsity of the subsample matrix (the number of entries that have already arrived to the system), which in practice is much smaller than the original matrix dimensions.\n\nIn the second stage, the recommendation stage, the system needs to output a recommendation for a user that comes into the system in an online fashion. Our goal is to minimize the computational time and use memory linear in the number of matrix entries. In most classical systems, this stage is split into a preprocessing stage, where a computation over the subsample matrix is performed once and stored, and the online stage, where an extra computation is performed when a user comes into the system. For example, a matrix reconstruction algorithm can be performed during the preprocessing stage to produce and store a low-rank approximation of the preference matrix. This computation takes time polynomial in the matrix dimensions, poly(mn), and the stored output is the k-top row singular vectors that need space O(nk). Then, when a user comes into the system, one can project the corresponding row of the subsample matrix onto the already stored k-top row singular vectors of the matrix and output a high value element in time O(nk). In general, the preprocessing time is polynomial in the dimensions of the preference matrix, i.e. poly(mn), while the time for the online recommendation is O(nk). Note that in real life applications, we need to avoid any system that needs storage O(mn), even though with such memory one can make the online recommendation time trivial by precomputing all the answers.\n\nA recommendation system performs well, when with high probability and for most users it outputs a recommendation that is good for a user. In practice, the performance of the recommendation system is usually evaluated using cross validation in the machine learning community. The available data about user-product preferences is split at random into a training set and a held out validation set. The training set is fed to the recommendation system, which outputs recommendations for the remaining users. The performance of the algorithm is measured by the quality of these recommendations on the held out data set.\n\n\nOur results\n\nIn our work, we design an efficient quantum algorithm for providing good recommendations to users in an online recommendation system which runs in time polylogarithmic in the matrix dimensions and polynomial only in the rank of the matrix, which as we have argued is assumed to be much smaller than the dimension of the matrix. This is the first algorithm for recommendation systems with complexity only polylogarithmic in the matrix dimensions.\n\nOur model. First, we describe a simple and general model for online recommendation systems. We start with a hidden preference matrix P , where the element P ij shows the preference of user i for product j. For each user, we split the products into two categories, the \"good\" and the \"bad\" recommendations, based on the matrix entries. This categorization can be done in different ways and we do not impose any constraints. For example, good recommendations can be every product with value higher than a threshold or the 100 products with the highest values etc. We can define this way a recommendation matrix T , which is a 0-1 matrix where for each row that corresponds to a user, the elements with value 1 correspond to \"good\" recommendations for this user and the elements with value 0 correspond to bad recommendations.\n\nOur assumption is that the matrix T has a good low-rank approximation. The reasons that justify this assumption are the same as the ones used already in the literature. As before, we believe that there is a small number of user types, and within each type the users \"agree\" on the high-value elements. This modeling of the users gives rise to a matrix T with a good low-rank approximation. Once we have defined the matrix T , then any algorithm that reconstructs a matrix close to T , will provide a good recommendation, since T is the indicator matrix of good recommendations.\n\nRecommendations by Matrix Sampling. The low-rank approximation of the matrix T can be computed as follows: first, define the matrix T , where with some probability each element of T is equal to the corresponding element in T normalized and otherwise it is zero. This matrix, that we call a subsample matrix, corresponds to the information the recommendation system has already gathered about the matrix T . Then, by performing a Singular Value Decomposition and computing the projection of this matrix to its k-top row singular vectors, we compute a matrix T k which can be proven to be close to the matrix T , as long as T had a good k-rank approximation.\n\nAs we remarked, in principle, we do not need to explicitly compute the entire matrix T k . We do not even need to compute all the high-value elements of T k . It is sufficient to be able to sample from the matrix T k which is close to T . Since T is a 0-1 matrix, sampling from T k means finding with high probability a 1-element in T . By the fact that T indicates the good recommendations, our algorithm will output a good recommendation with high probability. Hence, we reduce the question of providing good recommendations to being able to sample from the matrix T k . In fact, since we want to be able to recommend products to any specific user i, we need to be able, given an index i, to sample from the i-th row of the matrix T k , denoted by ( T k ) i , i.e. output an element\n( T k ) ij with probability |( T k ) ij | 2 /||( T k ) i || 2 .\nNote that the row ( T k ) i is the projection of the row T i onto the k-top row singular vectors of T .\n\nAn efficient quantum algorithm for Matrix Sampling. Here is where quantum computing becomes useful: we design a quantum procedure that samples from the row ( T k ) i in time polylog(mn). Note that the quantum algorithm does not output the row ( T k ) i , which by itself would take time linear in the dimension n, but only samples from this row. But this is exactly what is needed for recommendation systems: Sample a high-value element of the row, rather than explicitly output the entire row. More precisely, we describe an efficient quantum procedure that takes as input a vector, a matrix, and a threshold parameter, and generates the quantum state that corresponds to the projection of the vector onto the space spanned by the row singular vectors of the matrix whose corresponding singular value is higher than the threshold parameter. To do this we coherently estimate the singular value that corresponds to a singular vector of a matrix. From the outcome of this procedure it is clear how to sample a product by just measuring the quantum state in the computational basis.\n\nThe development of quantum algorithms for linear algebra was initiated by the breakthrough algorithm of Harrow, Hassidim, Lloyd [10]. The HHL algorithm takes as input a spare and wellconditioned system of linear equations and in time polylogarithmic in the dimension of the system outputs a quantum state which corresponds to the classical solution of the system. Note this algorithm does not explicitly output the classical solution, nevertheless, the quantum state enables one to sample from the solution vector. This is a very powerful algorithm and has been very influential in recent times, where several works [16,15,17] obtained quantum algorithms for machine learning problems based on similar assumptions. However, when looking at these applications, one needs to be extremely careful about two things: first, what assumptions one needs to make on the input in order to achieve efficient running time, since, for example, the running time of the HHL algorithm is polylogarithmic only when the matrix is well conditioned (i.e. the minimum singular value is at least inverse polynomially big) and sparse; and second, whether the quantum algorithm solves the original classical problem or a weaker variant to account for the fact that the classical solution is not given explicitly but is encoded in a quantum state [1,17]. In addition, we mention a recent but orthogonal proposal to use techniques inspired by the structure of quantum theory for classical recommender systems [22].\n\nLet us be more explicit about our algorithm's assumptions. We assume the data is stored in a classical data structure which enables the quantum algorithm to efficiently create superpositions of rows of the subsample matrix. The HHL algorithm also needs to be able to efficiently construct quantum states from classical vectors given as inputs. In the Appendix, we show how to store the subsample matrix T in a classical data structure that maintains some extra information about the matrix entries, so that, the data entry time remains polylogarithmic in the matrix dimensions, and at the same time, enables an algorithm with quantum access to the classical data structure to create the necessary superpositions in polylogarithmic time. The total memory needed is linear (up to polylogarithmic terms) in the sparsity of the subsample matrix. Hence, our system retains the necessary properties that the data entry stage need have. Note also, that even in the case the data has been stored in a normal array, we can preprocess it once, and in time linear in the sparsity of the matrix construct our needed data structure. Moreover, the classical complexity of matrix reconstruction does not change with the new data structure.\n\nImportantly, in the recommendation stage of our system, we do not perform any preprocessing nor do we need any extra storage memory. We only perform an online recommendation algorithm that takes time poly(k)polylog(mn), which given that the rank k is a small constant and the dimensions are of the same order, can be thought of as exponentially smaller than the classical time. Unlike the HHL algorithm, our running time does not depend neither on the sparsity of the input matrix nor on its condition number, i.e. its smallest singular value. In other words, we do not make any extra assumptions about the classical data.\n\nIt is also crucial to note that we have not changed what one needs to output, as was the case for the HHL algorithm and its applications, where instead of explicitly outputting a classical solution, they construct a quantum state that corresponds to this solution. We have instead described a real world application, where the ability to sample from the solution is precisely what is needed.\n\nThe performance of our recommendation system is similar to previous classical recommendation systems based on matrix reconstruction and depends on how good the low-rank approximation of the matrix is. Note that our algorithm works for any matrix, but as in the classical case, it guarantees good recommendations only when the matrix has a good low-rank approximation.\n\n\nPreliminaries\n\n\nLinear algebra\n\nThe set {1, 2, \u00b7 \u00b7 \u00b7 , n} is denoted by [n], the standard basis vectors in R n are denoted by e i , i \u2208 [n]. For any matrix A \u2208 R m\u00d7n , the Frobenius norm is defined as\nA 2 F = ij A 2 ij = i \u03c3 2 i ,\nwhere \u03c3 i are the singular values. We also say that we sample from the matrix A when we pick an element (i, j) with probability |A ij | 2 /||A|| 2 F , and write (i, j) \u223c A. For a vector x \u2208 R n we denote the norm\n||x|| 2 = i x 2 i . The matrix A is unitary if AA * = A * A = I, the eigenvalues of a unitary matrix have unit norm. A matrix P \u2208 R n\u00d7n is a projector if P 2 = P . If A is a matrix with orthonormal columns, then AA t is the projector onto the column space of A.\nSingular value decomposition: The singular value decomposition of A \u2208 R m\u00d7n is a decomposition of the form A = U \u03a3V t where U \u2208 R m\u00d7m , V \u2208 R n\u00d7n are unitary and \u03a3 is a diagonal matrix with positive entries. The SV D can be written as A = i\u2208[r] \u03c3 i u i v t i where r is the rank of A. The column and the row singular vectors u i and v i are the columns of U and V respectively. The Moore Penrose pseudo-inverse is defined as\nA + = V \u03a3 + U t , where A + = i\u2208[r] 1 \u03c3 i v i u t i .\nIt follows that AA + is the projection onto the column space Col(A) while A + A is the projection onto the row space Row(A). The truncation of A to the space of the singular vectors that correspond to the k largest\nsingular values is denoted by A k , that is A k = i\u2208[k] \u03c3 i u i v t i .\nWe denote by A \u2265\u03c3 the projection of the matrix A onto the space spanned by the singular vectors whose corresponding singular value is bigger than \u03c3, that is A \u2265\u03c3 = i:\u03c3 i \u2265\u03c3 \u03c3 i u i v t i .\n\n\nQuantum information\n\nWe use the standard bra-ket notation to denote quantum states. We use the following encoding for representing n dimensional vectors by quantum states,\nDefinition 2.1. The vector state |x for x \u2208 R n is defined as 1 ||x|| i\u2208[n] x i |i .\nIn case x \u2208 R mn , we can either see it as a vector in this space or as a matrix with dimensions m \u00d7 n and then we can equivalently write 1\n||x|| i\u2208[m],j\u2208[n] x ij |i, j . A quantum measurement (P OV M ) is a collection of positive operators M a 0 such that a M a = I n .\nThe probability of obtaining outcome a when state |\u03c6 is measured is T r( \u03c6|M a \u03c6 ). If |x is measured in the standard basis, then outcome i is observed with probability x 2\n\ni / x 2 . We also use a well-known quantum algorithm called phase estimation. The phase estimation algorithm estimates the eigenvalues of a unitary operator U with additive error \u01eb in time\nO(T (U ) log n/\u01eb) if T (U ) is the time required to implement the unitary U .\nTheorem 2.2. Phase estimation [12]: Let U be a unitary operator, with eigenvectors |v j and eigenvalues e \u03b9\u03b8 j for \u03b8 j \u2208 [\u2212\u03c0, \u03c0], i.e. we have U |v j = e \u03b9\u03b8 j |v j for j \u2208 [n]. For a precision parameter \u01eb > 0, there exists a quantum algorithm that runs in time O(T (U ) log n/\u01eb) and with\nprobability 1 \u2212 1/poly(n) maps a state |\u03c6 = j\u2208[n] \u03b1 j |v j to the state j\u2208[n] \u03b1 j |v j |\u03b8 j such that \u03b8 j \u2208 \u03b8 j \u00b1 \u01eb for all j \u2208 [n].\nNote that we use \u03b9 to denote the imaginary unit i to avoid confusion with summation indices. The analysis of phase estimation shows that the algorithm outputs a discrete valued estimate for each eigenvalue that is within additive error \u01eb with probability at least 0.8, the probability is boosted to 1 \u2212 1/poly(n) by repeating O(log n) times and choosing the most frequent estimate.\n\n\nA model for recommendation systems 3.1 Preferences and the recommendation matrix\n\nWe define a simple and general model for recommendation systems. We start be defining a preference matrix P of size m \u00d7 n, where every row corresponds to a user, every column to a product, and the element P ij denotes the preference of user i for product j. There are different ways to ascribe meaning to these preferences, for example, the element can denote how much a user would like a product or how likely it is that he would buy it, and the preferences could be normalised to account for the difference between the average ratings of each user.\n\nGiven these preferences, for each user one can split the products into two classes: the \"good\" recommendations and the \"bad\" recommendations for this user. Again, one can imagine a number of different ways of classifying the products, for example, the good recommendations could be the products for which the user has a preference higher than a threshold, or the hundred products with highest preference etc. This way, we can define a recommendation matrix T of size m \u00d7 n, where every row corresponds to a user, every column to a product, and the element T ij is 0 or 1 and denotes whether product j is a good recommendation for user i or not.\n\nDefinition 3.1. A product j is a good recommendation for user i iff T ij = 1, otherwise it is bad. We also write it as the pair (i, j) is a good or bad recommendation.\n\n\nSampling an approximation of the recommendation matrix\n\nNote that sampling from the recommendation matrix T would always yield a good recommendation, since the products that correspond to bad recommendations have probability 0. This remains true even when we want to sample from a specific row of the matrix in order to provide a recommendation to a specific user. Our goal now is to show that sampling from a matrix that is close to the recommendation matrix T under the Frobenius norm yields good recommendations with high probability for most users.\n\nLemma 3.2. Let T be an approximation of the matrix T such that T \u2212 T F \u2264 \u01eb T F . Then, the probability a sample according to T is a bad recommendation is\nPr (i,j)\u223c T [(i, j) bad] \u2264 \u01eb 1 \u2212 \u01eb 2\nProof. By the theorem's assumption and triangle inequality, we have\n(1 + \u01eb) T F \u2265 T F \u2265 (1 \u2212 \u01eb) T F .\nWe can rewrite the approximation guarantee as\n\u01eb 2 T 2 F \u2265 T \u2212 T 2 F = (i,j):good (1 \u2212 T ij ) 2 + (i,j):badT 2 ij \u2265 (i,j):bad T 2 ij(1)\nThe probability that sampling from T provides a bad recommendation is\nPr[(i, j) bad] = (i,j):bad T 2 ij T 2 F \u2264 (i,j):bad T 2 ij (1 \u2212 \u01eb) 2 T 2 F \u2264 \u01eb 1 \u2212 \u01eb 2 .\n(2)\n\nThe above can be rewritten as follows denoting the i-th row of T by T i ,\nPr[(i, j) bad] = (i,j):bad T 2 ij T 2 F = i\u2208[m] T i 2 F T 2 F \u00b7 j:(i,j)bad T 2 ij T i 2 F \u2264 \u01eb 1 \u2212 \u01eb 2 .(3)\nWe can see that the above lemma provides the guarantee that the probability of a bad recommendation for an average user is small, where the average is weighted according to the weight of each row. In other words, if we care more about users that have many products they like and less for users that like almost nothing, then the sampling already guarantees good performance.\n\nWhile this might be sufficient in some scenarios, it would be nice to also have a guarantee that the recommendation is good for most users, where now every user has the same importance. Note that only with the closeness guarantee in the Frobenius norm, this may not be true, since imagine the case where almost all rows of the matrix T have extremely few 1s and a few rows have almost all 1s. In this case, it might be that the approximation matrix is close to the recommendation matrix according to the Frobenius norm, nevertheless the recommendation system provides good recommendations only for the very heavy users and bad ones for almost everyone else.\n\nHence, if we would like to show that the recommendation system provides good recommendations for most users, then we need to assume that most users are \"typical\", meaning that the number of products that are good recommendations for them is close to the average. We cannot expect to provide good recommendations for example to users that like almost nothing. One way to enforce this property is, for example, to define good recommendations for each user as the 100 top products, irrespective of how high their utilities are or whether there are even more good products for some users. In what follows we prove our results in most generality, where we introduce parameters for how many users are typical and how far from the average the number of good recommendations of a typical user can be. \n1 1 + \u03b3 T 2 F m \u2264 T i 2 \u2264 (1 + \u03b3) T 2 F m .(4)\nLet T be an approximation of the matrix T such that T \u2212 T F \u2264 \u01eb T F . Then, there exists a subset S \u2032 \u2286 S of size at least (1 \u2212 \u03b4 \u2212 \u03b6)m (for \u03b4 > 0), such that on average over the users in S \u2032 , the probability that a sample from the row T i is a bad recommendation is\nPr i\u223cU S \u2032 ,j\u223c T i [(i, j) bad] \u2264 \u01eb(1+\u01eb) 1\u2212\u01eb 2 1/ \u221a 1 + \u03b3 \u2212 \u01eb/ \u221a \u03b4 2 (1 \u2212 \u03b4 \u2212 \u03b6)\n.\n\nProof. We first use the guarantee that the matrices T and T are close in the Frobenius norm to conclude that there exist at least (1 \u2212 \u03b4)m users for which\nT i \u2212 T i 2 \u2264 \u01eb 2 T 2 F \u03b4m .(5)\nIf not, summing the error of the strictly more than \u03b4m users for which equation 5 is false we get the following contradiction,\nT \u2212 T 2 F > \u03b4m \u01eb 2 T 2 F \u03b4m > \u01eb 2 T 2 F .\nThen, at least (1 \u2212 \u03b4 \u2212 \u03b6)m users both satisfy equation 5 and belong to the set S. Denote this set by S \u2032 . Using equations (4) and (5) and the triangle inequality\nT i \u2265 T i \u2212 T i \u2212 T i , we have that for all users in S \u2032 T i 2 F \u2265 T 2 F m 1 \u221a 1 + \u03b3 \u2212 \u01eb \u221a \u03b4 2 \u2265 T 2 F (1 + \u01eb) 2 m 1 \u221a 1 + \u03b3 \u2212 \u01eb \u221a \u03b4 2 .(6)\nWe now use equations (3) and (6) and have\n\u01eb 1 \u2212 \u01eb 2 \u2265 i\u2208[m] T i 2 F T 2 F \u00b7 j:(i,j)bad T 2 ij T i 2 F \u2265 1/ \u221a 1 + \u03b3 \u2212 \u01eb/ \u221a \u03b4 2 (1 + \u01eb) 2 m i\u2208S \u2032 j:(i,j)bad T 2 ij T i 2 F .(7)\nWe are now ready to conclude that,\nPr i\u223cU S \u2032 ,j\u223c T i [(i, j) bad] = 1 |S \u2032 | i\u2208S \u2032 j:(i,j)bad T 2 ij T i 2 F \u2264 \u01eb(1+\u01eb) 1\u2212\u01eb 2 1/ \u221a 1 + \u03b3 \u2212 \u01eb/ \u221a \u03b4 2 (1 \u2212 \u03b4 \u2212 \u03b6)\n.\n\nWe note that by taking reasonable values for the parameters, the error does not increase much from the original error. For example, if we assume that 90% of the users have preferences between 1/1.1 and 1.1 times the average, then the error over the typical users has increased by at most a factor of 1.5. Note also that we can easily make the quality of the recommendation system even better if we are willing to recommend a small number of products, instead of just one, and are satisfied if at least one of them is a good recommendation. This is in fact what happens in practical systems.\n\n\nMatrix Sampling\n\nWe showed in the previous section that providing good recommendations reduces to being able to sample from a matrix T which is a good approximation to the recommendation matrix T in the Frobenius norm. We will now define the approximation matrix T , by extending known matrix reconstruction techniques. The reconstruction algorithms provides good guarantees under the assumption that the recommendation matrix T has a good k-rank approximation for a small k, i.e. T \u2212 T k F \u2264 \u01eb T F (for some small constant \u01eb \u2265 0).\n\nLet us now briefly describe the matrix reconstruction algorithms. In general, the input to the reconstruction algorithm is a subsample of some matrix A. There are quite a few different ways of subsampling a matrix, for example, sampling each element of the matrix with some probability or sampling rows and/or columns of the matrix according to some distribution. We present here in more detail the first case as is described in the work of Achlioptas and McSherry [2]. Each element of the matrix A that has size m \u00d7 n is sampled with probability p and rescaled so as to obtain the random matrix A where each element is equal to A ij = A ij /p with probability p and 0 otherwise.\nNote that E[ A] = A.\nThe reconstruction algorithm computes the projection of the input matrix A onto its k-top singular vectors; we denote the projection by A k . The analysis of the algorithm shows that the approximation error A\u2212 A k is not much bigger than A\u2212A k . Projecting onto the top k singular vectors of the subsampled matrix A thus suffices to reconstruct a matrix approximating A.\n\nThe intuition for the analysis is that A is a matrix whose entries are independent random variables, thus with high probability the top k spectrum of A will be close to the one of its expectation matrix E[ A] = A. This intuition was proven in [2]. [2] Let A \u2208 R m\u00d7n be a matrix and b = max ij A ij . Define the matrix A to be a random matrix obtained by subsampling with probability p = 16nb 2 /(\u03b7||A|| F ) 2 (for \u03b7 > 0) and rescaling, that is A ij = A ij /p with probability p and 0 otherwise. With probability at least 1 \u2212 exp(\u221219(log n) 4 ) we have for any k\nA \u2212 A k F \u2264 A \u2212 A k F + 3 \u221a \u03b7k 1/4 A F .(8)\nHere, we will need to extend this result in order to be able to use it together with our quantum procedure. First, we will consider the matrix which is not the projection on the k-top singular vectors, but the projection on the singular vectors whose corresponding singular values are larger than a threshold. For any matrix A and any \u03c3 \u2265 0, we denote by A \u2265\u03c3 the projection of the matrix A onto the space spanned by the singular vectors whose corresponding singular value is bigger than \u03c3. Intuitively, since the spectrum of the matrix is highly concentrated on the top k singular vectors, then the corresponding singular values should be of order O( ||A|| F \u221a k ). Hence, by taking our threshold\nas O( ||A|| F \u221a k\n) we can achieve similar approximation guarantees.\n\nNote that we do not use anything about how the matrix A was generated, only that it satisfies equation 8. Hence our results hold for other matrix reconstruction algorithms as well, as long as we have a similar guarantee in the Frobenius norm.\n\nTheorem 4.2. Let A \u2208 R m\u00d7n be a matrix and b = max ij A ij , Define the matrix A to be a random matrix obtained by subsampling with probability p = 16nb 2 /(\u03b7||A|| F ) 2 (for \u03b7 > 0) and rescaling, that is A ij = A ij /p with probability p and 0 otherwise. Let \u00b5 > 0 a threshold parameter and denote\n\u03c3 = \u00b5 k || A|| F . With probability at least 1 \u2212 exp(\u221219(log n) 4 ) we have A \u2212 A \u2265\u03c3 F \u2264 A \u2212 A k F + (3 \u221a \u03b7k 1/4 \u00b5 \u22121/4 + \u221a \u00b5) A F .(9)\nProof. Let \u03c3 i denote the singular values of A. Let \u2113 the largest integer for which \u03c3 \u2113 \u2265 \u00b5 k || A|| F . Note that \u2113 \u2264 k \u00b5 . Then, by theorem 4.1, we have\nA \u2212 A \u2265\u03c3 F = A \u2212 A \u2113 F \u2264 A \u2212 A \u2113 F + 3 \u221a \u03b7\u2113 1/4 A F .\nWe distinguish two cases.\nIf \u2113 \u2265 k, then A \u2212 A \u2113 F \u2264 A \u2212 A k F , since A \u2113 contains more of the singular vectors of A. If k > \u2113, then A \u2212 A \u2113 F \u2264 A \u2212 A k F + A k \u2212 A \u2113 F ,\n\nwhich dominates the two cases. For the second term we have\nA k \u2212 A \u2113 2 F = k i=\u2113+1 \u03c3 2 i \u2264 k \u00b5 k ||A|| 2 F \u2264 \u00b5||A|| 2 F . Hence, A \u2212 A \u2265\u03c3 F \u2264 A \u2212 A k F + (3 \u221a \u03b7k 1/4 \u00b5 \u22121/4 + \u221a \u00b5) A F .\nLet us write A \u2212 A k F \u2264 \u01eb A F , for some \u01eb \u2265 0. We can take \u03b7 = \u01eb 3 /9 \u221a k and \u00b5 = \u01eb 2 to have that the overall error is A \u2212 A \u2265\u03c3 F \u2264 3\u01eb||A|| F .\n\nOur quantum procedure will almost produce this projection. In fact, we will need to consider a family of matrices which denote the projection of the matrix A onto the space spanned by the union of the singular vectors whose corresponding singular value is bigger than \u03c3 and also some subset of singular vectors whose corresponding singular value is in the interval [(1 \u2212 \u03ba)\u03c3, \u03c3). Think of \u03ba as a constant, for example 1/3. This subset could be empty, all such singular vectors, or any in-between subset. We denote byA \u2265\u03c3,\u03ba any matrix in this family.\n\nThe final theorem we will need is the following Theorem 4.3. Let A \u2208 R m\u00d7n be a matrix and b = max ij A ij . Define the matrix A to be a random matrix obtained by subsampling with probability p = 16nb 2 /(\u03b7||A|| F ) 2 and rescaling, that is A ij = A ij /p with probability p and 0 otherwise. Let \u00b5 > 0 a threshold parameter and denote \u03c3 = \u00b5 k || A|| F . Let \u03ba > 0 a precision parameter. With probability at least 1 \u2212 exp(\u221219(log n) 4 ),\nA \u2212 A \u2265\u03c3,\u03ba F \u2264 3 A \u2212 A k F + 3 \u221a \u03b7k 1/4 \u00b5 \u22121/4 (2 + (1 \u2212 \u03ba) \u22121/2 ) + (3 \u2212 \u03ba) \u221a \u00b5 A F .(10)\nProof. We have\nA \u2212 A \u2265\u03c3,\u03ba F \u2264 A \u2212 A \u2265\u03c3 F + A \u2265\u03c3 \u2212 A \u2265\u03c3,\u03ba F \u2264 A \u2212 A \u2265\u03c3 F + A \u2265\u03c3 \u2212 A \u2265(1\u2212\u03ba)\u03c3 F \u2264 A \u2212 A \u2265\u03c3 F + A \u2212 A \u2265\u03c3 F + A \u2212 A \u2265(1\u2212\u03ba)\u03c3 F \u2264 2 A \u2212 A \u2265\u03c3 F + A \u2212 A \u2265(1\u2212\u03ba)\u03c3 F .\nWe use Theorem 4.2 to bound the first term as\nA \u2212 A \u2265\u03c3 F \u2264 A \u2212 A k F + (3 \u221a \u03b7k 1/4 \u00b5 \u22121/4 + \u221a \u00b5) A F\nFor the second term, we can reapply Theorem 4.2 where now we need to rename \u00b5 as (1 \u2212 \u03ba) 2 \u00b5 and have\nA \u2212 A \u2265(1\u2212\u03ba)\u03c3 F \u2264 A \u2212 A k F + (3 \u221a \u03b7k 1/4 (1 \u2212 \u03ba) \u22121/2 \u00b5 \u22121/4 + (1 \u2212 \u03ba) \u221a \u00b5) A F .\n\nOverall we have\nA \u2212 A \u2265\u03c3,\u03ba F \u2264 3 A \u2212 A k F + 3 \u221a \u03b7k 1/4 \u00b5 \u22121/4 (2 + (1 \u2212 \u03ba) \u22121/2 ) + (3 \u2212 \u03ba) \u221a \u00b5 A F .\nLet A \u2212 A k F \u2264 \u01eb A F , for some \u01eb \u2265 0. We can take \u03b7 = \u01eb 3 /9 \u221a k, \u00b5 = \u01eb 2 and \u03ba = 1/3 to have\nA \u2212 A \u2265\u03c3,\u03ba F \u2264 3\u01eb||A|| F + 2\u01eb + \u01eb \u221a 1 \u2212 \u03ba + (3 \u2212 \u03ba)\u01eb ||A|| F \u2264 9\u01eb||A|| F .(11)\nWe have shown that the task of providing good recommendations for a user i reduces to being able to sample from the i-th row of the matrix T \u2265\u03c3,\u03ba , in other words sample from the projection of the i-th row of T onto the space spanned by all row singular vectors with singular values higher than \u03c3 and possibly some more row singular vectors with singular values in the interval [(1\u2212\u03ba)\u03c3, \u03c3).\n\nIn the following section, we show a quantum procedure, such that given a vector (e.g. the i-th row of T ), a matrix (e.g. the matrix T ), and parameters \u03c3 and \u03ba, outputs the quantum state |( T \u2265\u03c3,\u03ba ) i , which allows one to sample from this row by measuring in the computational basis. The algorithm runs in time polylogarihmic in the matrix dimensions and polynomial in k, since it depends inverse polynomially in \u03c3, which in our case is inverse polynomial in k.\n\n\nQuantum projections in polylogarithmic time\n\nThe main quantum primitive required for the recommendation system is a quantum projection algorithm that runs in time polylogarithmic in the matrix dimensions.\n\n\nThe data structure\n\nThe input to the quantum procedure is a vector x \u2208 R n and a matrix A \u2208 R m\u00d7n . We assume that the input is stored in a classical data structure such that an algorithm that has quantum access to the data structure can create the quantum state |x corresponding to the vector x and the quantum states |A i corresponding to each row A i of the matrix A, in time polylog(mn).\n\nIt is in fact possible to design a data structure for a matrix A that supports the efficient construction of the quantum states |A i . Moreover, we can ensure that the size of the data structure is optimal (up to polylogarithmic factors), and the data entry time, i.e. the time to store a new entry (i, j, A ij ) that arrives in the system is just polylog(mn). Note that just writing down the entry takes logarithmic time.\n\nTheorem 5.1. Let A \u2208 R m\u00d7n be a matrix. Entries (i, j, A ij ) arrive in the system in an arbitrary order and w denotes the number of entries that have already arrived in the system. There exists a data structure to store the entries of A with the following properties:\n\ni. The size of the data structure is O(w \u00b7 log 2 (mn)).\n\nii. The time to store a new entry (i, j, A ij ) is O(log 2 (mn)).\n\niii. A quantum algorithm that has quantum access to the data structure can perform the mapping U : |i |0 \u2192 |i |A i , for i \u2208 [m], corresponding to the rows of the matrix and the mapping V : |0 |j \u2192 | A |j , for j \u2208 [n], where A \u2208 R m has entries A i = A i in time polylog(mn).\n\nThe explicit description of the data structure is given in the appendix. Basically, for each row of the matrix, that we view as a vector in R n , we store an array of 2n values as a full binary tree of n leaves. The leaves hold the individual amplitudes of the vector and each internal node holds the sum of the squares of the amplitudes of the leaves rooted on this node. For each entry added to the tree, we need to update log(n) nodes in the tree. The same data structure can of course be used for the vector x as well. One need not use a fixed array of size 2n for this construction, but only ordered lists of size equal to the entries that have already arrived in the system.Alternative solutions for vector state preparation are possible, another solution based on modifying the memory organization of the QRAM is described in [19].\n\n\nQuantum Singular Value Estimation\n\nThe second tool required for the projection algorithm is an efficient quantum algorithm for singular value estimation. In the singular value estimation problem we are given a matrix A such that the vector states corresponding to its row vectors can be prepared efficiently. Given a state |x = i \u03b1 i |v i for an arbitrary vector x \u2208 R n the task is to estimate the singular values corresponding to each singular vector in coherent superposition. Note that we take the basis {v i } to span the entire space by including singular vectors with singular value 0.\n\nTheorem 5.2. Let A \u2208 R m\u00d7n be a matrix with singular value decomposition A = i \u03c3 i u i v t i stored in the data structure in theorem 5.1. Let \u01eb > 0 the precision parameter. There is an algorithm with running time O(polylog(mn)/\u01eb) that performs the mapping i \u03b1 i |v i \u2192 i \u03b1 i |v i |\u03c3 i , where \u03c3 i \u2208 \u03c3 i \u00b1 \u01eb A F for all i with probability at least 1 \u2212 1/poly(n).\n\nHere, we present a quantum singular value estimation algorithm, in the same flavor as the quantum walk based algorithm by Childs [7] for estimating eigenvalues of a matrix, and show that given quantum access to the data structure from theorem 5.1, our algorithm runs in time O(polylog(mn)/\u01eb). A different quantum algorithm for singular value estimation can be based on the work of [16] with running time O(polylog(mn)/\u01eb 3 ), and for which a coherence analysis was shown in [19].\n\nThe idea for our singular value estimation algorithm is to find isometries P \u2208 R mn\u00d7m and Q \u2208 R mn\u00d7n that can be efficiently applied, and such that A A F = P t Q. Using P and Q, we define a unitary matrix W acting on R mn , which is also efficiently implementable and such that the row singular vector v i of A with singular value \u03c3 i is mapped to an eigenvector Qv i of W with eigenvalue e \u03b9\u03b8 i such that cos(\u03b8 i /2) = \u03c3 i / A F (note that cos(\u03b8 i /2) > 0 as \u03b8 i \u2208 [\u2212\u03c0, \u03c0]). The algorithm consists of the following steps: first, map the input vector i \u03b1 i |v i to i \u03b1 i |Qv i by applying Q; then, use phase estimation as in theorem 2.2 with unitary W to compute an estimate of the eigenvalues \u03b8 i and hence of the singular values \u03c3 i = ||A|| F cos(\u03b8 i /2); and finally undo Q to recover the state i \u03b1 i |v i |\u03c3 i . This procedure is described in algorithm 5.1.\n\nIt remains to show how to construct the mappings P, Q and the unitary W that satisfy all the properties mentioned above that are required for the quantum singular value estimation algorithm. Lemma 5.3. Let A \u2208 R m\u00d7n be a matrix with singular value decomposition A = i \u03c3 i u i v t i stored in the data structure in theorem 5.1. Then, there exist matrices P \u2208 R mn\u00d7m , Q \u2208 R mn\u00d7n such that i. The matrices P, Q are a factorization of A, i.e. A A F = P t Q. Moreover, P t P = I m , Q t Q = I n , and multiplication by P, Q, i.e. the mappings |y \u2192 |P y and |x \u2192 |Qx can be performed in time O(polylog(mn)).\n\nii. The unitary W = U \u00b7 V , where U, V are the reflections U = 2P P t \u2212 I mn and V = 2QQ t \u2212 I mn can be implemented in time O(polylog(mn)).\n\niii. The isometry Q : R n \u2192 R mn maps a row singular vector v i of A with singular value \u03c3 i to an eigenvector Qv i of W with eigenvalue e \u03b9\u03b8 i such that cos(\u03b8 i /2) = \u03c3 i / A F .\n\nProof. Let P \u2208 R mn\u00d7m be a matrix with column vectors e i \u2297 A i A i for i \u2208 [m]. In quantum notation multiplication by P can be expressed as\n|P e i = |i, A i = 1 ||A i || j\u2208[n] A ij |i, j , for i \u2208 [m].\nLet A \u2208 R m be the vector of Frobenius norms of the rows of the matrix A, that is A i = A i for i \u2208 [m]. Let Q \u2208 R mn\u00d7n be a matrix with column vectors A A F \u2297 e j for j \u2208 [n]. In quantum notation multiplication by Q can be expressed as\n|Qe j = | A, j = 1 ||A|| F i\u2208[m]\n||A i || |i, j , for j \u2208 [n].\n\nThe factorization A = P t Q follows easily by expressing the matrix product in quantum notation,\n(P t Q) ij = i, A i | A, j = A i A F A ij A i = A ij A F .\nThe columns of P, Q are orthonormal by definition so P t P = I m and Q t Q = I n . Multiplication by P and Q can be implemented in time polylog(mn) using quantum access to the data structure from theorem 5.1,\n|y \u2192 |y, 0 \u2308log n\u2309 = i\u2208[m] y i |i, 0 \u2308log n\u2309 U \u2212 \u2192 i\u2208[m] y i |i, A i = |P y |x \u2192 |0 \u2308log m\u2309 , x = j\u2208[n] x j |0 \u2308log m\u2309 , j V \u2212 \u2192 j\u2208[n] x j | A, j = |Qx .(12)\nTo show (ii), note that the unitary U is a reflection in Col(P ) and can be implemented as U = U R 1 U \u22121 where U is the unitary in first line of equation (12) and R 1 is the reflection in the space |y, 0 \u2308log n\u2309 for y \u2208 R m . It can be implemented as a reflection conditioned on the second register being in state |0 \u2308log n\u2309 . The unitary V is a reflection in Col(Q) and can be implemented analogously as V = V R 0 V \u22121 where V is the unitary in the second line of equation (12) and R 0 is the reflection in the space |0 \u2308log m\u2309 , x for x \u2208 R n . It remains to show that Qv i is an eigenvector for W with eigenvalue e \u03b9\u03b8 i such that cos(\u03b8 i /2) = \u03c3 i / A F . For every pair of singular vectors (u i , v i ) of A, we define the two dimensional subspaces W i = Span(P u i , Qv i ) and let \u03b8 i /2 \u2208 [\u2212\u03c0/2, \u03c0/2] be the angle between P u i and \u00b1Qv i . Note that W i is an eigenspace for W which acts on it as a rotation by \u00b1\u03b8 i , since W is a reflection in the column space of Q followed by a reflection in the column space of P . Moreover, the relation cos(\u03b8 i /2) = \u03c3 i / A F is a consequence of the factorization in lemma 5.3, since we have\nP P t Qv i = P Av i A F = \u03c3 i A F P u i and QQ t P u i = QA t u i A F = \u03c3 i A F Qv i .(13)\nUsing the primitives from the preceding lemma, we next describe the singular value estimation algorithm and analyze it to prove theorem 5.2.\n\n\nAlgorithm 5.1 Quantum singular value estimation\n\nRequire: A \u2208 R m\u00d7n , x \u2208 R n in the data structure from theorem 5.1, precision parameter \u01eb > 0.\n1. Create |x = i \u03b1 i |v i .\n2. Append a first register |0 \u2308log m\u2309 and create the state |Qx = i \u03b1 i |Qv i as in eq. (12).\n\n3. Perform phase estimation with precision parameter 2\u01eb > 0 on the input |Qx for the unitary W = U \u00b7 V where U, V are the unitaries in lemma 5.3 and obtain i \u03b1 i |Qv i , \u03b8 i .\n\n4. Compute \u03c3 i = cos(\u03b8 i /2) A F where \u03b8 i is the estimate from phase estimation, and uncompute the output of the phase estimation.\n\n5. Apply the inverse of the transformation in step 2 to obtain i \u03b1 i |v i |\u03c3 i .\n\n\nAnalysis\n\nThe phase estimation procedure [12] with unitary W and precision parameter \u01eb on input |Qv i produces an estimate such that |\u03b8 i \u2212 \u03b8 i | \u2264 2\u01eb. The estimate for the singular value is \u03c3 i = cos(\u03b8 i /2) A F . The error in estimating \u03c3 i = cos(\u03b8 i /2) A F can be bounded as follows,\n|\u03c3 i \u2212 \u03c3 i | = | cos(\u03b8 i /2) \u2212 cos(\u03b8 i /2)| A F \u2264 sin(\u03c6) |\u03b8 i \u2212 \u03b8 i | 2 A F \u2264 \u01eb A F(14)\nwhere \u03c6 \u2208 [\u03b8 i /2 \u2212 \u01eb, \u03b8 i /2 + \u01eb]. Algorithm 5.1 therefore produces an additive error \u01eb A F estimate of the singular values, the running time is O(polylog(mn)/\u01eb) by theorem 2.2 as the unitary W is implemented in time O(polylog(mn)) by lemma 5.3. This concludes the proof of theorem 5.2. One can define an algorithm for singular value estimation with input |y = i \u03b2 i |u i . where u i are the column singular vectors, by using the operator P from lemma 5.3 instead of Q in algorithm 5.1. The correctness follows from the same argument as above.\n\n\nQuantum projection with threshold\nLet A = i \u03c3 i u i v t i . We recall that A \u2265\u03c3 = \u03c3 i \u2265\u03c3 \u03c3 i u i v t i\nis the projection of the matrix A onto the space spanned by the singular vectors whose singular values are bigger than \u03c3. Also, A \u2265\u03c3,\u03ba is the projection of the matrix A onto the space spanned by the union of the singular vectors whose corresponding singular values is bigger than \u03c3 and some subset of singular vectors whose corresponding singular values are in the interval [(1 \u2212 \u03ba)\u03c3, \u03c3). Algorithm 5.3 presents a quantum algorithm that given access to vector state x, a matrix A and parameters \u03c3, \u03ba, outputs the state |A + \u2265\u03c3,\u03ba A \u2265\u03c3,\u03ba x , namely the projection of x onto the subspace spanned by the union of the row singular vectors whose corresponding singular values are bigger than \u03c3 and some subset of row singular vectors whose corresponding singular values are in the interval [(1 \u2212 \u03ba)\u03c3, \u03c3).\n\nFor simplicity, we present the algorithm without a stopping condition and we will compute the expected running time. By stopping the algorithm after a number of iterations which is log(n) times more than the expected one, we can easily construct an algorithm with worst-case running time guarantees and whose correctness probability has only decreased by a factor of (1\u22121/poly(n)).\n\nLet {v i } denote an orthonormal basis for R n that includes all row singular vectors of the matrix A. We think of \u03ba as a constant, for example 1/3.\n\n\nAlgorithm 5.2 Quantum projection with threshold\n\nRequire: A \u2208 R m\u00d7n , x \u2208 R n in the data structure from Theorem 5.1; parameters \u03c3, \u03ba > 0.\n1. Create |x = i \u03b1 i |v i .\n2. Apply the singular value estimation on |x with precision \u01eb= \u03ba 2 \u03c3 ||A|| F to obtain the state i \u03b1 i |v i |\u03c3 i 3. Apply on a second new register the unitary V that maps |t |0 \u2192 |t |1 if t < \u03c3 \u2212 \u03ba 2 \u03c3 and |t |0 \u2192 |t |0 otherwise, to get the state\ni\u2208S \u03b1 i |v i |\u03c3 i |0 + i\u2208S \u03b1 i |v i |\u03c3 i |1 ,\nwhere S is the union of all i's such that \u03c3 i \u2265 \u03c3 and some i's with \u03c3 i \u2208 [(1 \u2212 \u03ba)\u03c3, \u03c3).\n\n\n4.\n\nApply the singular value estimation on the above state to erase the second register ).\ni\u2208S \u03b1 i |v i |0 + i\u2208S \u03b1 i |v i |1 = \u03b2 |A + \u2265\u03c3,\u03ba A \u2265\u03c3,\u03ba x |0 + 1 \u2212 |\u03b2| 2 |A + \u2265\u03c3,\u03ba A \u2265\u03c3,\u03ba x \u22a5 |1 , with \u03b2 = ||A + \u2265\u03c3,\u03ba A \u2265\u03c3,\nIt is important to notice that the running time of the quantum projection algorithm depends only on the threshold \u03c3 (which we will take to be of the order ||A|| F \u221a k ) and not on the condition number of A which may be very large. We will also show in the next section that in the recommendation systems, for most users the ratio\n||A + \u2265\u03c3 A \u2265\u03c3 x|| 2 ||x|| 2\nis constant. This will conclude the analysis and show that the running time of the quantum recommendation system is polynomial in k and polylogarithmic in the matrix dimensions.\n\nOne could also use amplitude amplification to improve the running time of algorithm 5.3, once a careful error analysis is performed as the reflections are not exact. As we will see that the\n||A + \u2265\u03c3 A \u2265\u03c3 x|| 2 ||x|| 2\nis constant for most users, this will not change asymptotically the running time of the algorithm and hence we omit the analysis.\n\n\nQuantum recommendation systems\n\nWe have all the necessary ingredients to describe the quantum algorithm that provides good recommendations for a user i and that runs in time polylogarithmic in the dimensions of the recommendation matrix and polynomial in the rank k. As we said, in recommendation systems we assume that for the recommendation matrix T we have T \u2212 T k F \u2264 \u01eb||T || F for some small approximation parameter \u01eb and small rank k (no more than 100). In the algorithm below, as in the classical recommendation systems, we assume we know k but in fact we just need to have a good estimate for it.\n\nNote again that we do not put a stopping condition to the algorithm and we compute the expected running time. Again we can turn this into an algorithm with worst-case running time guarantees by stopping after running for log(n) times more than the expected running time, and the correctness probability has only decreased by a factor of 1 \u2212 1/poly(n). \u221a k|| T i || 2 /|| T + \u2265\u03c3 T \u2265\u03c3 T i || 2 ) and returns with probability at least 1 \u2212 1/poly(n) the state | T + \u2265\u03c3,\u03ba T \u2265\u03c3,\u03ba T i . 2: Measure the above state in the computational basis to get a product j.\n\n\nAnalysis\n\nCorrectness Let us check the correctness of the algorithm. Note that T + \u2265\u03c3,\u03ba T \u2265\u03c3,\u03ba T i = ( T \u2265\u03c3,\u03ba ) i , i.e. the i-th row of the matrix T \u2265\u03c3,\u03ba . Hence, the quantum projection procedure outputs with probability at least 1 \u2212 1/poly(n) the state |( T \u2265\u03c3,\u03ba ) i , meaning that our quantum recommendation algorithm with high probability outputs a product by sampling the i-th row of the matrix T \u2265\u03c3,\u03ba . By Theorem 4.3, and by setting the parameters appropriately to get equation 11, we have that with probability at least 1 \u2212 exp(\u221219(log n) 4 ),\nT \u2212 T \u2265\u03c3,\u03ba F \u2264 9\u01eb T F .\nIn this case, we can apply Theorem 3.3 with matrix T = T \u2265\u03c3,\u03ba to show that there exists a subset of users S \u2032 of size at least (1 \u2212 \u03b4 \u2212 \u03b6)m (for \u03b4 > 0), such that on average over the users in S \u2032 , the probability that our quantum algorithm provides a bad recommendation is\nPr i\u223cU S \u2032 ,j\u223c( T \u2265\u03c3,\u03ba ) i [(i, j) bad] \u2264 9\u01eb(1+9\u01eb) 1\u22129\u01eb 2 1/ \u221a 1 + \u03b3 \u2212 9\u01eb/ \u221a \u03b4 2 (1 \u2212 \u03b4 \u2212 \u03b6)\n.\n\nExpected running time We prove the following theorem Proof. We just need to show that for most users the term W i \u2261 || T i || 2 ||( T \u2265\u03c3,\u03ba ) i || 2 that appears in the running time of the quantum projection algorithm is a constant. This is to be expected, since most typical rows of the matrix project very well onto the space spanned by the top singular vectors, since the spectrum of the matrix is well concentrated on the space of the top singular vectors.\n\nAs in Theorem 3.3, we focus on the users in the subset S \u2032 , with |S \u2032 | \u2265 (1 \u2212 \u03b4 \u2212 \u03b6)m, for which equations 4 and 6 hold. For these users we can use equation 6 with the matrix T = T \u2265\u03c3,\u03ba and error 9\u01eb. We have\nE i\u2208S \u2032 [W i ] = E i\u2208S \u2032 [ || T i || 2 ||( T \u2265\u03c3,\u03ba ) i || 2 ] \u2264 E i\u2208S \u2032 [|| T i || 2 ] T 2 F (1+\u01eb) 2 m 1 \u221a 1+\u03b3 \u2212 9\u01eb \u221a \u03b4 2 \u2264 || T || 2 F (1\u2212\u03b4\u2212\u03b6)m T 2 F (1+\u01eb) 2 m 1 \u221a 1+\u03b3 \u2212 9\u01eb \u221a \u03b4 2 \u2264 (1 + \u01eb) 2 (1 \u2212 \u03b4 \u2212 \u03b6) 1 \u221a 1+\u03b3 \u2212 9\u01eb \u221a \u03b4 2 .\nBy Markov's inequality, for at least (1 \u2212 \u03be)|S \u2032 | users in S \u2032 we have\nW i \u2264 (1+\u01eb) 2 \u03be(1\u2212\u03b4\u2212\u03b6) 1 \u221a 1+\u03b3 \u2212 9\u01eb \u221a \u03b4 2 , which\nfor appropriate parameters is a constant. Hence, for at least (1 \u2212 \u03be)(1 \u2212 \u03b4 \u2212 \u03b6)m users, the quantum recommendation algorithm has an expected running time of O(poly(k)polylog(mn)) and produces good recommendations with high probability. As we said we can easily turn this into a worst-case running time, by stopping after running log(n) times more than the expected running time and hence decreasing the correctness only by a factor of 1 \u2212 1/poly(n). \n\n\nA The data structure\n\nWe prove the following theorem.\n\nTheorem A.1. (Theorem 5.1 restated) Let A \u2208 R m\u00d7n be a matrix. Entries (i, j, A ij ) arrive in the system in some arbitrary order, and w denotes the number of entries that have already arrived in the system. There exists a data structure to store the matrix A with the following properties:\n\ni. The size of the data structure is O(w log 2 (mn)).\n\nii. The time to store a new entry (i, j, A ij ) is O(log 2 (mn)).\n\niii. A quantum algorithm that has quantum access to the data structure can perform the mapping U : |i |0 \u2192 |i |A i , for i \u2208 [m], corresponding to the rows of the matrix and the mapping V : |0 |j \u2192 | A |j , for j \u2208 [n], where A \u2208 R m has entries A i = A i in time polylog(mn).\n\nProof. The data structure consists of an array of m binary trees B i , i \u2208 [m]. The trees B i are initially empty. When a new entry (i, j, A ij ) arrives the leaf node j in tree B i is created if not present and updated otherwise. The leaf stores the value A 2 ij as well as the sign of A ij . The depth of each tree B i is at most \u2308log n\u2309 as there can be at most n leaves. An internal node v of B i stores the sum of the values of all leaves in the subtree rooted at v, i.e. the sum of the square amplitudes of the entries of A i in the subtree. Hence, the value stored at the root is ||A i || 2 . When a new entry arrives, all the nodes on the path from that leaf to the tree root are also updated. The different levels of the tree B i are stored as ordered lists so that the address of the nodes being updated can be retrieved in time O(log mn). The binary tree for a 4-dimensional unit vector for which all entries have arrived is illustrated in figure 1.\n\nThe time required to store entry (i, j, A ij ) is O(log 2 mn) as the insertion algorithm makes at most \u2308log n\u2309 updates to the data structure and each update requires time O(log mn) to retrieve the address of the updated node.\n\nThe memory requirement for the data structure is O(w log 2 mn) as for each entry (i, j, A ij ) at most \u2308log n\u2309 new nodes are added, each node requiring O(log mn) bits.\n\nWe now show how to perform U in time polylog(mn) if an algorithm has quantum access to this classical data structure. The state preparation procedure using pre-computed amplitudes is well known in the literature, for instance see [9]. The method is illustrated for a 4-dimensional state |\u03c6 corresponding to a unit vector in figure 1. The amplitudes stored in the internal nodes of B i are used to apply a sequence of conditional rotations to the initial state |0 \u2308log n\u2309 to obtain |A i . Overall, there are \u2308log n\u2309 rotations applied and for each one of them we need two quantum queries to the data structure (from each node in the superposition we query its two children).\n\nThe amplitude stored at an internal node of B i at depth t corresponding to k \u2208 {0, 1} t is,\nB i,k := j\u2208[n],j 1:t =k A 2 ij\nwhere j 1:t denotes the first t bits in the binary representation for j. Note that B i,k is the probability of observing outcome k if the first t bits of |A i are measured in the standard basis. Conditioned on the first register being |i and the first t qubits being in state |k the rotation is applied to the (t + 1) qubit as follows |i |k |0 \u2192 |i |k\n1 B i,k B i,k0 |0 + B i,k1 |1 .\nThe sign is included for rotations applied to the \u2308log n\u2309-th qubit |i |k |0 \u2192 |i |k\n1 B i,k sgn(A k0 ) B i,k0 |0 + sgn(A k1 ) B i,k1 |1 .\nLast, we show how to perform V in time polylog(mn). Note that the amplitudes of the vector A are equal to A i , and the values stored on the roots of the trees B i are equal to ||A i || 2 . Hence, by a similar construction (another binary tree) for the m roots, we can perform the unitary V efficiently.\n\nTheorem 3. 3 .\n3Let T be an m \u00d7 n recommendation matrix. Let S be a subset of rows of size |S| \u2265 (1 \u2212 \u03b6)m (for \u03b6 > 0) for which we have for some \u03b3 > 0\n\nAlgorithm 6. 1\n1Quantum recommendation algorithm. Require: A subsample matrix T \u2208 R m\u00d7n stored in the data structure from Theorem 5.1 and satisfying the conditions in Theorem 4.3; a user index i. 1: Apply the quantum projection procedure 5.3 with the matrix T , the vector corresponding to the i-th row T i , with \u03c3 = \u01eb|| T || F / \u221a k and error parameter \u03ba = 1/3. The algorithm runs in expected time O(polylog(mn)\n\nTheorem 6. 1 .\n1For at least (1 \u2212 \u03be)(1 \u2212 \u03b4 \u2212 \u03b6)m users in the subset S \u2032 , we have that the expected running time of Algorithm 6.1 is O(polylog(mn)poly(k)).\n\n\nFor the running time, note that the singular value estimation takes time O(polylog(mn)/\u01eb), while the probability we obtain |0 in step 5 is||A + \u2265\u03c3,\u03ba A \u2265\u03c3,\u03ba x|| 2 ||x|| 2 ||A + \u2265\u03c3 A \u2265\u03c3 x|| 2 ||x|| 2Theorem 5.4. Algorithm 5.3 outputs |A + \u2265\u03c3,\u03ba A \u2265\u03c3,\u03ba x with probability at least 1 \u2212 1/poly(n) and in expected time O( polylog(mn)||A|| F ||x|| 2 \u03c3||A \u2265\u03c3 A + \u2265\u03c3 x|| 2\u03ba x|| \n||x|| \n\n. \n\n5. Measure the second register in the standard basis. If the outcome is |0 , output the first \nregister and exit. Otherwise repeat step 1. \n\n\u2265 \n\n. \n\n\n\n\nLet |\u03c6 = 0.4 |00 + 0.4 |01 + 0.8 |10 + 0.2|11 .Figure 1: Vector state preparation illustrated for 4-dimensional state |\u03c6 .1.0 \n\n0.32 \n\n0.16 \n0.16 \n\n0.68 \n\n0.64 \n0.04 \n\n\u2022 Rotation on qubit 1: \n|0 |0 \u2192 ( \n\u221a \n0.32 |0 + \n\u221a \n0.68 |1 ) |0 \n\n\u2022 Rotation on qubit 2 conditioned on qubit 1: \n\n( \n\u221a \n0.32 |0 + \n\u221a \n0.68 |1 ) |0 \u2192 \n\u221a \n0.32 |0 \n1 \n\u221a \n0.32 \n(0.4 |0 + 0.4 |1 )+ \n\n\u221a \n0.68 |1 \n1 \n\u221a \n0.68 \n(0.8 |0 + 0.2 |1 ) \n\n\nAcknowledgements:IK was partially supported by projects ANR RDAM, ERC QCC and EU QAlgo. AP was supported by the Singapore National Research Foundation under NRF RF Award No. NRF-NRFF2013-13.\nRead the fine print. S Aaronson, Nature Physics. 114S. Aaronson, \"Read the fine print,\" Nature Physics, vol. 11, no. 4, pp. 291-293, 2015.\n\nFast computation of low rank matrix approximations. D Achlioptas, F Mcsherry, Proceedings of the thirty-third annual ACM symposium on Theory of computing. the thirty-third annual ACM symposium on Theory of computingACMD. Achlioptas and F. McSherry, \"Fast computation of low rank matrix approximations,\" in Proceedings of the thirty-third annual ACM symposium on Theory of computing. ACM, 2001, pp. 611-618.\n\nToward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. G Adomavicius, A Tuzhilin, IEEE Transactions on. 176Knowledge and Data EngineeringG. Adomavicius and A. Tuzhilin, \"Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions,\" Knowledge and Data Engineering, IEEE Transactions on, vol. 17, no. 6, pp. 734-749, 2005.\n\nImproved recommendation systems. B Awerbuch, B Patt-Shamir, D Peleg, M Tuttle, Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms. the sixteenth annual ACM-SIAM symposium on Discrete algorithmsB. Awerbuch, B. Patt-Shamir, D. Peleg, and M. Tuttle, \"Improved recommendation systems,\" in Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 2005, pp. 1174-1183.\n\nSpectral analysis of data. Y Azar, A Fiat, A Karlin, F Mcsherry, J Saia, Proceedings of the thirty-third annual ACM symposium on Theory of computing. the thirty-third annual ACM symposium on Theory of computingACMY. Azar, A. Fiat, A. Karlin, F. McSherry, and J. Saia, \"Spectral analysis of data,\" in Proceed- ings of the thirty-third annual ACM symposium on Theory of computing. ACM, 2001, pp. 619-626.\n\nLessons from the netflix prize challenge. R M Bell, Y Koren, ACM SIGKDD Explorations Newsletter. 92R. M. Bell and Y. Koren, \"Lessons from the netflix prize challenge,\" ACM SIGKDD Explo- rations Newsletter, vol. 9, no. 2, pp. 75-79, 2007.\n\nOn the relationship between continuous-and discrete-time quantum walk. A M Childs, Communications in Mathematical Physics. 2942A. M. Childs, \"On the relationship between continuous-and discrete-time quantum walk,\" Communications in Mathematical Physics, vol. 294, no. 2, pp. 581-603, 2010.\n\nCompetitive recommendation systems. P Drineas, I Kerenidis, P Raghavan, Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. the thiry-fourth annual ACM symposium on Theory of computingACMP. Drineas, I. Kerenidis, and P. Raghavan, \"Competitive recommendation systems,\" in Pro- ceedings of the thiry-fourth annual ACM symposium on Theory of computing. ACM, 2002, pp. 82-90.\n\nCreating superpositions that correspond to efficiently integrable probability distributions. L Grover, T Rudolph, quant-ph/0208112arXiv preprintL. Grover and T. Rudolph, \"Creating superpositions that correspond to efficiently integrable probability distributions,\" arXiv preprint quant-ph/0208112, 2002.\n\nQuantum algorithm for linear systems of equations. A W Harrow, A Hassidim, S Lloyd, Physical review letters. 10315150502A. W. Harrow, A. Hassidim, and S. Lloyd, \"Quantum algorithm for linear systems of equa- tions,\" Physical review letters, vol. 103, no. 15, p. 150502, 2009.\n\nLow-rank matrix completion using alternating minimization. P Jain, P Netrapalli, S Sanghavi, Proceedings of the forty-fifth annual ACM symposium on Theory of computing. the forty-fifth annual ACM symposium on Theory of computingACMP. Jain, P. Netrapalli, and S. Sanghavi, \"Low-rank matrix completion using alternating mini- mization,\" in Proceedings of the forty-fifth annual ACM symposium on Theory of computing. ACM, 2013, pp. 665-674.\n\nQuantum measurements and the abelian stabilizer problem. A Y Kitaev, quant-ph/9511026arXiv preprintA. Y. Kitaev, \"Quantum measurements and the abelian stabilizer problem,\" arXiv preprint quant-ph/9511026, 1995.\n\nAdvances in collaborative filtering. Y Koren, R Bell, Recommender systems handbook. SpringerY. Koren and R. Bell, \"Advances in collaborative filtering,\" in Recommender systems handbook. Springer, 2011, pp. 145-186.\n\nMatrix factorization techniques for recommender systems. Y Koren, R Bell, C Volinsky, Computer. 8Y. Koren, R. Bell, and C. Volinsky, \"Matrix factorization techniques for recommender sys- tems,\" Computer, no. 8, pp. 30-37, 2009.\n\nQuantum algorithms for supervised and unsupervised machine learning. S Lloyd, M Mohseni, P Rebentrost, Arxiv preprint:1307.0411S. Lloyd, M. Mohseni, and P. Rebentrost, \"Quantum algorithms for supervised and unsuper- vised machine learning,\" Arxiv preprint:1307.0411, 2013.\n\nQuantum self analysis. Arxiv preprint:1307.1401--, \"Quantum self analysis,\" Arxiv preprint:1307.1401, 2013.\n\nQuantum support vector machine for big feature and big data classification. Arxiv preprint:1307.0471--, \"Quantum support vector machine for big feature and big data classification,\" Arxiv preprint:1307.0471, 2013.\n\nLatent semantic indexing: A probabilistic analysis. C H Papadimitriou, H Tamaki, P Raghavan, S Vempala, Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems. the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systemsACMC. H. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala, \"Latent semantic indexing: A probabilistic analysis,\" in Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems. ACM, 1998, pp. 159-168.\n\nQuantum algorithms for linear algebra and machine learning. A Prakash, University of California, Berkeley.Ph.D ThesisA. Prakash, \"Quantum algorithms for linear algebra and machine learning,\" Ph.D Thesis, University of California, Berkeley., 2014.\n\nMining of massive datasets. A Rajaraman, J D Ullman, Cambridge University Press Cambridge77A. Rajaraman and J. D. Ullman, Mining of massive datasets. Cambridge University Press Cambridge, 2012, vol. 77.\n\nUser modeling via stereotypes*. E Rich, Cognitive science. 34E. Rich, \"User modeling via stereotypes*,\" Cognitive science, vol. 3, no. 4, pp. 329-354, 1979.\n\nRecommender systems inspired by the structure of quantum theory. C Stark, arXiv:1601.06035arXiv preprintC. Stark, \"Recommender systems inspired by the structure of quantum theory,\" arXiv preprint arXiv:1601.06035, 2016.\n", "annotations": {"author": "[{\"end\":202,\"start\":59},{\"end\":364,\"start\":203}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":68},{\"end\":217,\"start\":210}]", "author_first_name": "[{\"end\":67,\"start\":59},{\"end\":209,\"start\":203}]", "author_affiliation": "[{\"end\":157,\"start\":79},{\"end\":201,\"start\":159},{\"end\":363,\"start\":240}]", "title": "[{\"end\":31,\"start\":1},{\"end\":395,\"start\":365}]", "venue": null, "abstract": "[{\"end\":1520,\"start\":453}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2791,\"start\":2788},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2794,\"start\":2791},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2796,\"start\":2794},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2798,\"start\":2796},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3024,\"start\":3020},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3026,\"start\":3024},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3029,\"start\":3026},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3997,\"start\":3994},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4000,\"start\":3997},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4831,\"start\":4828},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4834,\"start\":4831},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7234,\"start\":7231},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7236,\"start\":7234},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7380,\"start\":7376},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7430,\"start\":7426},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7769,\"start\":7765},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7771,\"start\":7769},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15981,\"start\":15977},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16469,\"start\":16465},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16472,\"start\":16469},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16475,\"start\":16472},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17174,\"start\":17171},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17177,\"start\":17174},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17336,\"start\":17332},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22618,\"start\":22614},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30986,\"start\":30983},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31837,\"start\":31834},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":39567,\"start\":39563},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40660,\"start\":40657},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40913,\"start\":40909},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":41005,\"start\":41001},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":45904,\"start\":45900},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":55789,\"start\":55786}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":57331,\"start\":57180},{\"attributes\":{\"id\":\"fig_2\"},\"end\":57746,\"start\":57332},{\"attributes\":{\"id\":\"fig_3\"},\"end\":57904,\"start\":57747},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":58437,\"start\":57905},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":58850,\"start\":58438}]", "paragraph": "[{\"end\":2595,\"start\":1536},{\"end\":3030,\"start\":2597},{\"end\":3519,\"start\":3032},{\"end\":5009,\"start\":3521},{\"end\":5496,\"start\":5011},{\"end\":6076,\"start\":5498},{\"end\":7538,\"start\":6078},{\"end\":7973,\"start\":7540},{\"end\":8527,\"start\":7975},{\"end\":9266,\"start\":8529},{\"end\":10672,\"start\":9268},{\"end\":11288,\"start\":10674},{\"end\":11749,\"start\":11304},{\"end\":12574,\"start\":11751},{\"end\":13153,\"start\":12576},{\"end\":13811,\"start\":13155},{\"end\":14597,\"start\":13813},{\"end\":14765,\"start\":14662},{\"end\":15847,\"start\":14767},{\"end\":17337,\"start\":15849},{\"end\":18563,\"start\":17339},{\"end\":19187,\"start\":18565},{\"end\":19580,\"start\":19189},{\"end\":19949,\"start\":19582},{\"end\":20152,\"start\":19984},{\"end\":20395,\"start\":20183},{\"end\":21082,\"start\":20658},{\"end\":21351,\"start\":21137},{\"end\":21612,\"start\":21424},{\"end\":21786,\"start\":21636},{\"end\":22011,\"start\":21872},{\"end\":22315,\"start\":22143},{\"end\":22505,\"start\":22317},{\"end\":22871,\"start\":22584},{\"end\":23386,\"start\":23005},{\"end\":24021,\"start\":23471},{\"end\":24667,\"start\":24023},{\"end\":24836,\"start\":24669},{\"end\":25391,\"start\":24895},{\"end\":25546,\"start\":25393},{\"end\":25651,\"start\":25584},{\"end\":25731,\"start\":25686},{\"end\":25890,\"start\":25821},{\"end\":25983,\"start\":25980},{\"end\":26058,\"start\":25985},{\"end\":26540,\"start\":26166},{\"end\":27199,\"start\":26542},{\"end\":27994,\"start\":27201},{\"end\":28309,\"start\":28042},{\"end\":28392,\"start\":28391},{\"end\":28548,\"start\":28394},{\"end\":28707,\"start\":28581},{\"end\":28913,\"start\":28750},{\"end\":29096,\"start\":29055},{\"end\":29264,\"start\":29230},{\"end\":29390,\"start\":29389},{\"end\":29982,\"start\":29392},{\"end\":30516,\"start\":30002},{\"end\":31197,\"start\":30518},{\"end\":31589,\"start\":31219},{\"end\":32152,\"start\":31591},{\"end\":32894,\"start\":32197},{\"end\":32963,\"start\":32913},{\"end\":33207,\"start\":32965},{\"end\":33507,\"start\":33209},{\"end\":33798,\"start\":33644},{\"end\":33878,\"start\":33853},{\"end\":34358,\"start\":34212},{\"end\":34909,\"start\":34360},{\"end\":35347,\"start\":34911},{\"end\":35453,\"start\":35439},{\"end\":35656,\"start\":35611},{\"end\":35813,\"start\":35712},{\"end\":36096,\"start\":36001},{\"end\":36566,\"start\":36176},{\"end\":37031,\"start\":36568},{\"end\":37238,\"start\":37079},{\"end\":37632,\"start\":37261},{\"end\":38056,\"start\":37634},{\"end\":38326,\"start\":38058},{\"end\":38383,\"start\":38328},{\"end\":38450,\"start\":38385},{\"end\":38728,\"start\":38452},{\"end\":39568,\"start\":38730},{\"end\":40163,\"start\":39606},{\"end\":40526,\"start\":40165},{\"end\":41006,\"start\":40528},{\"end\":41869,\"start\":41008},{\"end\":42473,\"start\":41871},{\"end\":42615,\"start\":42475},{\"end\":42796,\"start\":42617},{\"end\":42938,\"start\":42798},{\"end\":43237,\"start\":43001},{\"end\":43300,\"start\":43271},{\"end\":43398,\"start\":43302},{\"end\":43666,\"start\":43458},{\"end\":44964,\"start\":43825},{\"end\":45196,\"start\":45056},{\"end\":45343,\"start\":45248},{\"end\":45464,\"start\":45372},{\"end\":45641,\"start\":45466},{\"end\":45774,\"start\":45643},{\"end\":45856,\"start\":45776},{\"end\":46146,\"start\":45869},{\"end\":46779,\"start\":46235},{\"end\":47683,\"start\":46885},{\"end\":48066,\"start\":47685},{\"end\":48216,\"start\":48068},{\"end\":48357,\"start\":48268},{\"end\":48633,\"start\":48386},{\"end\":48768,\"start\":48680},{\"end\":48861,\"start\":48775},{\"end\":49315,\"start\":48986},{\"end\":49521,\"start\":49344},{\"end\":49712,\"start\":49523},{\"end\":49870,\"start\":49741},{\"end\":50477,\"start\":49905},{\"end\":51032,\"start\":50479},{\"end\":51586,\"start\":51045},{\"end\":51884,\"start\":51611},{\"end\":51979,\"start\":51978},{\"end\":52440,\"start\":51981},{\"end\":52651,\"start\":52442},{\"end\":52947,\"start\":52876},{\"end\":53449,\"start\":52998},{\"end\":53505,\"start\":53474},{\"end\":53797,\"start\":53507},{\"end\":53852,\"start\":53799},{\"end\":53919,\"start\":53854},{\"end\":54197,\"start\":53921},{\"end\":55158,\"start\":54199},{\"end\":55385,\"start\":55160},{\"end\":55554,\"start\":55387},{\"end\":56228,\"start\":55556},{\"end\":56322,\"start\":56230},{\"end\":56705,\"start\":56354},{\"end\":56821,\"start\":56738},{\"end\":57179,\"start\":56876}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14661,\"start\":14598},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20182,\"start\":20153},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20657,\"start\":20396},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21136,\"start\":21083},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21423,\"start\":21352},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21871,\"start\":21787},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22142,\"start\":22012},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22583,\"start\":22506},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23004,\"start\":22872},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25583,\"start\":25547},{\"attributes\":{\"id\":\"formula_10\"},\"end\":25685,\"start\":25652},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25820,\"start\":25732},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25979,\"start\":25891},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26165,\"start\":26059},{\"attributes\":{\"id\":\"formula_14\"},\"end\":28041,\"start\":27995},{\"attributes\":{\"id\":\"formula_15\"},\"end\":28390,\"start\":28310},{\"attributes\":{\"id\":\"formula_16\"},\"end\":28580,\"start\":28549},{\"attributes\":{\"id\":\"formula_17\"},\"end\":28749,\"start\":28708},{\"attributes\":{\"id\":\"formula_18\"},\"end\":29054,\"start\":28914},{\"attributes\":{\"id\":\"formula_19\"},\"end\":29229,\"start\":29097},{\"attributes\":{\"id\":\"formula_20\"},\"end\":29388,\"start\":29265},{\"attributes\":{\"id\":\"formula_21\"},\"end\":31218,\"start\":31198},{\"attributes\":{\"id\":\"formula_22\"},\"end\":32196,\"start\":32153},{\"attributes\":{\"id\":\"formula_23\"},\"end\":32912,\"start\":32895},{\"attributes\":{\"id\":\"formula_24\"},\"end\":33643,\"start\":33508},{\"attributes\":{\"id\":\"formula_25\"},\"end\":33852,\"start\":33799},{\"attributes\":{\"id\":\"formula_26\"},\"end\":34024,\"start\":33879},{\"attributes\":{\"id\":\"formula_27\"},\"end\":34211,\"start\":34085},{\"attributes\":{\"id\":\"formula_28\"},\"end\":35438,\"start\":35348},{\"attributes\":{\"id\":\"formula_29\"},\"end\":35610,\"start\":35454},{\"attributes\":{\"id\":\"formula_30\"},\"end\":35711,\"start\":35657},{\"attributes\":{\"id\":\"formula_31\"},\"end\":35896,\"start\":35814},{\"attributes\":{\"id\":\"formula_32\"},\"end\":36000,\"start\":35914},{\"attributes\":{\"id\":\"formula_33\"},\"end\":36175,\"start\":36097},{\"attributes\":{\"id\":\"formula_34\"},\"end\":43000,\"start\":42939},{\"attributes\":{\"id\":\"formula_35\"},\"end\":43270,\"start\":43238},{\"attributes\":{\"id\":\"formula_36\"},\"end\":43457,\"start\":43399},{\"attributes\":{\"id\":\"formula_37\"},\"end\":43824,\"start\":43667},{\"attributes\":{\"id\":\"formula_38\"},\"end\":45055,\"start\":44965},{\"attributes\":{\"id\":\"formula_39\"},\"end\":45371,\"start\":45344},{\"attributes\":{\"id\":\"formula_40\"},\"end\":46234,\"start\":46147},{\"attributes\":{\"id\":\"formula_41\"},\"end\":46884,\"start\":46816},{\"attributes\":{\"id\":\"formula_42\"},\"end\":48385,\"start\":48358},{\"attributes\":{\"id\":\"formula_43\"},\"end\":48679,\"start\":48634},{\"attributes\":{\"id\":\"formula_44\"},\"end\":48985,\"start\":48862},{\"attributes\":{\"id\":\"formula_45\"},\"end\":49343,\"start\":49316},{\"attributes\":{\"id\":\"formula_46\"},\"end\":49740,\"start\":49713},{\"attributes\":{\"id\":\"formula_47\"},\"end\":51610,\"start\":51587},{\"attributes\":{\"id\":\"formula_48\"},\"end\":51977,\"start\":51885},{\"attributes\":{\"id\":\"formula_49\"},\"end\":52875,\"start\":52652},{\"attributes\":{\"id\":\"formula_50\"},\"end\":52997,\"start\":52948},{\"attributes\":{\"id\":\"formula_51\"},\"end\":56353,\"start\":56323},{\"attributes\":{\"id\":\"formula_52\"},\"end\":56737,\"start\":56706},{\"attributes\":{\"id\":\"formula_53\"},\"end\":56875,\"start\":56822}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1534,\"start\":1522},{\"attributes\":{\"n\":\"1.1\"},\"end\":11302,\"start\":11291},{\"attributes\":{\"n\":\"2\"},\"end\":19965,\"start\":19952},{\"attributes\":{\"n\":\"2.1\"},\"end\":19982,\"start\":19968},{\"attributes\":{\"n\":\"2.2\"},\"end\":21634,\"start\":21615},{\"attributes\":{\"n\":\"3\"},\"end\":23469,\"start\":23389},{\"attributes\":{\"n\":\"3.2\"},\"end\":24893,\"start\":24839},{\"attributes\":{\"n\":\"4\"},\"end\":30000,\"start\":29985},{\"end\":34084,\"start\":34026},{\"end\":35913,\"start\":35898},{\"attributes\":{\"n\":\"5\"},\"end\":37077,\"start\":37034},{\"attributes\":{\"n\":\"5.1\"},\"end\":37259,\"start\":37241},{\"attributes\":{\"n\":\"5.2\"},\"end\":39604,\"start\":39571},{\"end\":45246,\"start\":45199},{\"end\":45867,\"start\":45859},{\"attributes\":{\"n\":\"5.3\"},\"end\":46815,\"start\":46782},{\"end\":48266,\"start\":48219},{\"end\":48773,\"start\":48771},{\"attributes\":{\"n\":\"6\"},\"end\":49903,\"start\":49873},{\"attributes\":{\"n\":\"6.1\"},\"end\":51043,\"start\":51035},{\"end\":53472,\"start\":53452},{\"end\":57195,\"start\":57181},{\"end\":57347,\"start\":57333},{\"end\":57762,\"start\":57748}]", "table": "[{\"end\":58437,\"start\":58269},{\"end\":58850,\"start\":58562}]", "figure_caption": "[{\"end\":57331,\"start\":57197},{\"end\":57746,\"start\":57349},{\"end\":57904,\"start\":57764},{\"end\":58269,\"start\":57907},{\"end\":58562,\"start\":58440}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":55157,\"start\":55149},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":55888,\"start\":55880}]", "bib_author_first_name": "[{\"end\":59064,\"start\":59063},{\"end\":59235,\"start\":59234},{\"end\":59249,\"start\":59248},{\"end\":59700,\"start\":59699},{\"end\":59715,\"start\":59714},{\"end\":60048,\"start\":60047},{\"end\":60060,\"start\":60059},{\"end\":60075,\"start\":60074},{\"end\":60084,\"start\":60083},{\"end\":60503,\"start\":60502},{\"end\":60511,\"start\":60510},{\"end\":60519,\"start\":60518},{\"end\":60529,\"start\":60528},{\"end\":60541,\"start\":60540},{\"end\":60922,\"start\":60921},{\"end\":60924,\"start\":60923},{\"end\":60932,\"start\":60931},{\"end\":61190,\"start\":61189},{\"end\":61192,\"start\":61191},{\"end\":61446,\"start\":61445},{\"end\":61457,\"start\":61456},{\"end\":61470,\"start\":61469},{\"end\":61901,\"start\":61900},{\"end\":61911,\"start\":61910},{\"end\":62164,\"start\":62163},{\"end\":62166,\"start\":62165},{\"end\":62176,\"start\":62175},{\"end\":62188,\"start\":62187},{\"end\":62449,\"start\":62448},{\"end\":62457,\"start\":62456},{\"end\":62471,\"start\":62470},{\"end\":62886,\"start\":62885},{\"end\":62888,\"start\":62887},{\"end\":63078,\"start\":63077},{\"end\":63087,\"start\":63086},{\"end\":63314,\"start\":63313},{\"end\":63323,\"start\":63322},{\"end\":63331,\"start\":63330},{\"end\":63555,\"start\":63554},{\"end\":63564,\"start\":63563},{\"end\":63575,\"start\":63574},{\"end\":64136,\"start\":64135},{\"end\":64138,\"start\":64137},{\"end\":64155,\"start\":64154},{\"end\":64165,\"start\":64164},{\"end\":64177,\"start\":64176},{\"end\":64680,\"start\":64679},{\"end\":64896,\"start\":64895},{\"end\":64909,\"start\":64908},{\"end\":64911,\"start\":64910},{\"end\":65104,\"start\":65103},{\"end\":65295,\"start\":65294}]", "bib_author_last_name": "[{\"end\":59073,\"start\":59065},{\"end\":59246,\"start\":59236},{\"end\":59258,\"start\":59250},{\"end\":59712,\"start\":59701},{\"end\":59724,\"start\":59716},{\"end\":60057,\"start\":60049},{\"end\":60072,\"start\":60061},{\"end\":60081,\"start\":60076},{\"end\":60091,\"start\":60085},{\"end\":60508,\"start\":60504},{\"end\":60516,\"start\":60512},{\"end\":60526,\"start\":60520},{\"end\":60538,\"start\":60530},{\"end\":60546,\"start\":60542},{\"end\":60929,\"start\":60925},{\"end\":60938,\"start\":60933},{\"end\":61199,\"start\":61193},{\"end\":61454,\"start\":61447},{\"end\":61467,\"start\":61458},{\"end\":61479,\"start\":61471},{\"end\":61908,\"start\":61902},{\"end\":61919,\"start\":61912},{\"end\":62173,\"start\":62167},{\"end\":62185,\"start\":62177},{\"end\":62194,\"start\":62189},{\"end\":62454,\"start\":62450},{\"end\":62468,\"start\":62458},{\"end\":62480,\"start\":62472},{\"end\":62895,\"start\":62889},{\"end\":63084,\"start\":63079},{\"end\":63092,\"start\":63088},{\"end\":63320,\"start\":63315},{\"end\":63328,\"start\":63324},{\"end\":63340,\"start\":63332},{\"end\":63561,\"start\":63556},{\"end\":63572,\"start\":63565},{\"end\":63586,\"start\":63576},{\"end\":64152,\"start\":64139},{\"end\":64162,\"start\":64156},{\"end\":64174,\"start\":64166},{\"end\":64185,\"start\":64178},{\"end\":64688,\"start\":64681},{\"end\":64906,\"start\":64897},{\"end\":64918,\"start\":64912},{\"end\":65109,\"start\":65105},{\"end\":65301,\"start\":65296}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2246294},\"end\":59180,\"start\":59042},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2683832},\"end\":59588,\"start\":59182},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206742345},\"end\":60012,\"start\":59590},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":11027944},\"end\":60473,\"start\":60014},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9990004},\"end\":60877,\"start\":60475},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1851838},\"end\":61116,\"start\":60879},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14801066},\"end\":61407,\"start\":61118},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3097965},\"end\":61805,\"start\":61409},{\"attributes\":{\"doi\":\"quant-ph/0208112\",\"id\":\"b8\"},\"end\":62110,\"start\":61807},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5187993},\"end\":62387,\"start\":62112},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":447011},\"end\":62826,\"start\":62389},{\"attributes\":{\"doi\":\"quant-ph/9511026\",\"id\":\"b11\"},\"end\":63038,\"start\":62828},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14698210},\"end\":63254,\"start\":63040},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":58370896},\"end\":63483,\"start\":63256},{\"attributes\":{\"doi\":\"Arxiv preprint:1307.0411\",\"id\":\"b14\"},\"end\":63757,\"start\":63485},{\"attributes\":{\"doi\":\"Arxiv preprint:1307.1401\",\"id\":\"b15\"},\"end\":63866,\"start\":63759},{\"attributes\":{\"doi\":\"Arxiv preprint:1307.0471\",\"id\":\"b16\"},\"end\":64081,\"start\":63868},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1479546},\"end\":64617,\"start\":64083},{\"attributes\":{\"id\":\"b18\"},\"end\":64865,\"start\":64619},{\"attributes\":{\"id\":\"b19\"},\"end\":65069,\"start\":64867},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1623918},\"end\":65227,\"start\":65071},{\"attributes\":{\"doi\":\"arXiv:1601.06035\",\"id\":\"b21\"},\"end\":65448,\"start\":65229}]", "bib_title": "[{\"end\":59061,\"start\":59042},{\"end\":59232,\"start\":59182},{\"end\":59697,\"start\":59590},{\"end\":60045,\"start\":60014},{\"end\":60500,\"start\":60475},{\"end\":60919,\"start\":60879},{\"end\":61187,\"start\":61118},{\"end\":61443,\"start\":61409},{\"end\":62161,\"start\":62112},{\"end\":62446,\"start\":62389},{\"end\":63075,\"start\":63040},{\"end\":63311,\"start\":63256},{\"end\":64133,\"start\":64083},{\"end\":65101,\"start\":65071}]", "bib_author": "[{\"end\":59075,\"start\":59063},{\"end\":59248,\"start\":59234},{\"end\":59260,\"start\":59248},{\"end\":59714,\"start\":59699},{\"end\":59726,\"start\":59714},{\"end\":60059,\"start\":60047},{\"end\":60074,\"start\":60059},{\"end\":60083,\"start\":60074},{\"end\":60093,\"start\":60083},{\"end\":60510,\"start\":60502},{\"end\":60518,\"start\":60510},{\"end\":60528,\"start\":60518},{\"end\":60540,\"start\":60528},{\"end\":60548,\"start\":60540},{\"end\":60931,\"start\":60921},{\"end\":60940,\"start\":60931},{\"end\":61201,\"start\":61189},{\"end\":61456,\"start\":61445},{\"end\":61469,\"start\":61456},{\"end\":61481,\"start\":61469},{\"end\":61910,\"start\":61900},{\"end\":61921,\"start\":61910},{\"end\":62175,\"start\":62163},{\"end\":62187,\"start\":62175},{\"end\":62196,\"start\":62187},{\"end\":62456,\"start\":62448},{\"end\":62470,\"start\":62456},{\"end\":62482,\"start\":62470},{\"end\":62897,\"start\":62885},{\"end\":63086,\"start\":63077},{\"end\":63094,\"start\":63086},{\"end\":63322,\"start\":63313},{\"end\":63330,\"start\":63322},{\"end\":63342,\"start\":63330},{\"end\":63563,\"start\":63554},{\"end\":63574,\"start\":63563},{\"end\":63588,\"start\":63574},{\"end\":64154,\"start\":64135},{\"end\":64164,\"start\":64154},{\"end\":64176,\"start\":64164},{\"end\":64187,\"start\":64176},{\"end\":64690,\"start\":64679},{\"end\":64908,\"start\":64895},{\"end\":64920,\"start\":64908},{\"end\":65111,\"start\":65103},{\"end\":65303,\"start\":65294}]", "bib_venue": "[{\"end\":59089,\"start\":59075},{\"end\":59335,\"start\":59260},{\"end\":59746,\"start\":59726},{\"end\":60170,\"start\":60093},{\"end\":60623,\"start\":60548},{\"end\":60974,\"start\":60940},{\"end\":61239,\"start\":61201},{\"end\":61556,\"start\":61481},{\"end\":61898,\"start\":61807},{\"end\":62219,\"start\":62196},{\"end\":62556,\"start\":62482},{\"end\":62883,\"start\":62828},{\"end\":63122,\"start\":63094},{\"end\":63350,\"start\":63342},{\"end\":63552,\"start\":63485},{\"end\":63780,\"start\":63759},{\"end\":63942,\"start\":63868},{\"end\":64286,\"start\":64187},{\"end\":64677,\"start\":64619},{\"end\":64893,\"start\":64867},{\"end\":65128,\"start\":65111},{\"end\":65292,\"start\":65229},{\"end\":59397,\"start\":59337},{\"end\":60234,\"start\":60172},{\"end\":60685,\"start\":60625},{\"end\":61618,\"start\":61558},{\"end\":62617,\"start\":62558},{\"end\":64372,\"start\":64288}]"}}}, "year": 2023, "month": 12, "day": 17}