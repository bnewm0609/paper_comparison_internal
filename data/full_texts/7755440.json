{"id": 7755440, "updated": "2023-09-28 08:55:15.169", "metadata": {"title": "Exploiting Multiple Levels of Parallelism in Sparse Matrix-Matrix Multiplication", "authors": "[{\"first\":\"Ariful\",\"last\":\"Azad\",\"middle\":[]},{\"first\":\"Grey\",\"last\":\"Ballard\",\"middle\":[]},{\"first\":\"Aydin\",\"last\":\"Buluc\",\"middle\":[]},{\"first\":\"James\",\"last\":\"Demmel\",\"middle\":[]},{\"first\":\"Laura\",\"last\":\"Grigori\",\"middle\":[]},{\"first\":\"Oded\",\"last\":\"Schwartz\",\"middle\":[]},{\"first\":\"Sivan\",\"last\":\"Toledo\",\"middle\":[]},{\"first\":\"Samuel\",\"last\":\"Williams\",\"middle\":[]}]", "venue": "SIAM Journal of Scientific Computing, Volume 38, Number 6, pp. C624-C651, 2016", "journal": null, "publication_date": {"year": 2015, "month": null, "day": null}, "abstract": "Sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many high-performance graph algorithms as well as for some linear solvers, such as algebraic multigrid. The scaling of existing parallel implementations of SpGEMM is heavily bound by communication. Even though 3D (or 2.5D) algorithms have been proposed and theoretically analyzed in the flat MPI model on Erdos-Renyi matrices, those algorithms had not been implemented in practice and their complexities had not been analyzed for the general case. In this work, we present the first ever implementation of the 3D SpGEMM formulation that also exploits multiple (intra-node and inter-node) levels of parallelism, achieving significant speedups over the state-of-the-art publicly available codes at all levels of concurrencies. We extensively evaluate our implementation and identify bottlenecks that should be subject to further research.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1510.00844", "mag": "3105937213", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/AzadBBDGSTW15", "doi": "10.1137/15m104253x"}}, "content": {"source": {"pdf_hash": "f6afb08e1bdbc3fe61f3586c214f23e5c4bb96c1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1510.00844v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1510.00844", "status": "GREEN"}}, "grobid": {"id": "8b2478912a36b77d5cd87b5679fee52512ffa35b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f6afb08e1bdbc3fe61f3586c214f23e5c4bb96c1.txt", "contents": "\nEXPLOITING MULTIPLE LEVELS OF PARALLELISM IN SPARSE MATRIX-MATRIX MULTIPLICATION\n\n\nAriful Azad \nGrey Ballard \nAydin Bulu\u00e7 \nJames Demmel \nLaura Grigori \u00b6 \nOded Schwartz \nSivan Toledo \nSamuel Williams \nEXPLOITING MULTIPLE LEVELS OF PARALLELISM IN SPARSE MATRIX-MATRIX MULTIPLICATION\nParallel computingnumerical linear algebrasparse matrix-matrix multiplication25D algorithms3D algorithmsmultithreadingSpGEMM2D decompositiongraph algorithms AMS subject classifications 05C5005C8565F5068W10\nSparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many high-performance graph algorithms as well as for some linear solvers, such as algebraic multigrid. The scaling of existing parallel implementations of SpGEMM is heavily bound by communication. Even though 3D (or 2.5D) algorithms have been proposed and theoretically analyzed in the flat MPI model on Erd\u0151s-R\u00e9nyi matrices, those algorithms had not been implemented in practice and their complexities had not been analyzed for the general case. In this work, we present the first implementation of the 3D SpGEMM formulation that exploits multiple (intra-node and inter-node) levels of parallelism, achieving significant speedups over the state-of-the-art publicly available codes at all levels of concurrencies. We extensively evaluate our implementation and identify bottlenecks that should be subject to further research.\n\n1. Introduction. Multiplication of two sparse matrices (SpGEMM) is a key operation for high-performance graph computations in the language of linear algebra [31,40]. Examples include graph contraction [25], betweenness centrality [13], Markov clustering [47], peer pressure clustering [43], triangle counting [4], and cycle detection [49]. SpGEMM is also used in scientific computing. For instance, it is often a performance bottleneck for Algebraic Multigrid (AMG), where it is used in the setup phase for restricting and interpolating matrices [7]. Schur complement methods in hybrid linear solvers [48] also require fast SpGEMM. In electronic structure calculations, linear-scaling methods exploit Kohn's \"nearsightedness\" principle of electrons in many-atom systems [33]. SpGEMM and its approximate versions are often the workhorse of these computations [8,10].\n\nWe describe new parallel implementations of the SpGEMM kernel, by exploiting multiple levels of parallelism. We provide the first complete implementation and large-scale results of a \"3D algorithm\" that asymptotically reduces communication costs compared to the state-of-the-art 2D algorithms. The name \"3D\" derives from the parallelization across all 3 dimensions of the iteration space. While 2D algorithms like Sparse SUMMA [14] are based on a 2D decomposition of the output matrix with computation following an \"owner computes\" rule, a 3D algorithm also parallelizes the computation of individual output matrix entries. Our 3D formulation relies on splitting (as opposed to replicating) input submatrices across processor layers.\n\nWhile previous work [5] analyzed the communication costs of a large family of parallel SpGEMM algorithms and provided lower-bounds on random matrices, it did not present any experimental results. In particular, the following questions were left unanswered:\n\n\u2022 What is the effect of different communication patterns on relative scalability of these algorithms? The analysis was performed in terms of \"the number of words moved per processor\", which did not take into account important factors such as network contention, use of collectives, the relative sizes of the communicators, etc. \u2022 What is the effect of in-node multithreading? By intuition, one can expect a positive effect due to reduced network contention and automatic data aggregation as a result of in-node multithreading, but those have not been evaluated before. \u2022 What is the role of local data structures and local algorithms? In particular, what is the right data structure to store local sparse matrices in order to multiply them fast using a single thread and multiple threads? How do we merge local triples efficiently during the reduction phases? \u2022 How do the algorithms perform on real-world matrices, such as those with skewed degree distributions? This paper addresses these questions by presenting the first implementation of the 3D SpGEMM formulation that exploits both the additional third processor grid dimension and the in-node multithreading aspect. In particular, we show that the third processor grid dimension navigates a tradeoff between communication of the input matrices and communication of the output matrix. We also show that innode multithreading, with efficient shared-memory parallel kernels, can significantly enhance scalability. In terms of local data structures and algorithms, we use a priority queue to merge sparse vectors for in-node multithreading. This eliminates thread scaling bottlenecks which were due to asymptotically increased working set size as well as the need to modify the data structures for cache efficiency. To answer the last question, we benchmark our algorithms on real-world matrices coming from a variety of applications. Our extensive evaluation via large-scale experiments exposes bottlenecks and provides new avenues for research. Section 3 summarizes earlier results on various parallel SpGEMM formulations. Section 4 presents the distributed-memory algorithms implemented for this work, as well as the local data structures and operations in detail. In particular, our new 3D algorithm, Split-3D-SpGEMM, is presented in Section 4.4. Section 5 gives an extensive performance evaluation of these implementations using large scale parallel experiments, including a performance comparison with similar primitives offered by other publicly available libraries such as Trilinos and Intel Math Kernel Library (MKL). Various implementation decisions and their effects on performance are also detailed.\n\n\nNotation. Let\n\nA \u2208 S m\u00d7k be a sparse rectangular matrix of elements from a semiring S. We use nnz (A) to denote the number of nonzero elements in A. When the matrix is clear from context, we drop the parenthesis and simply use nnz . For sparse matrix indexing, we use the convenient Matlab colon notation, where A(:, i) denotes the ith column, A(i, :) denotes the ith row, and A(i, j) denotes the element at the (i, j)th position of matrix A. Array and vector indices are 1-based throughout this paper. The length of an array I, denoted by len(I), is equal to its number of elements. For one-dimensional arrays, I(i) denotes the ith component of the array. I(j : k) defines the range I(j), I(j + 1), . . . , I(k) and is also applicable to matrices.\n\nWe use flops(A, B), pronounced \"flops\", to denote the number of nonzero arithmetic operations required when computing the product of matrices A and B. When the operation and the operands are clear from context, we simply use flops. We ac-knowledge that semiring operations do not have to be on floating-point numbers (e.g. they can be on integers or Booleans) but we nevertheless use flops as opposed to ops to be consistent with existing literature.\n\nIn our analysis of parallel running time, the latency of sending a message over the communication interconnect is \u03b1, and the inverse bandwidth is \u03b2, both expressed as multiples of the time for a floating-point operation (also accounting for the cost of cache misses and memory indirections associated with that floating point operation). Notation f (x) = \u0398(g(x)) means that f is bounded asymptotically by g both above and below. We index a 3D process grid with P (i, j, k). Each 2D slice of this grid P (:, :, k) with the third dimension fixed is called a process \"layer\" and each 1D slice of this grid P (i, j, :) with the first two dimensions fixed is called a process \"fiber\".\n\n3. Background and Related Work. The classical serial SpGEMM algorithm for general sparse matrices was first described by Gustavson [27], and was subsequently used in Matlab [24] and CSparse [19]. For computing the product C = AB, where A \u2208 S m\u00d7l , B \u2208 S l\u00d7n and C \u2208 S m\u00d7n , Gustavson's algorithm runs in O(flops + nnz +m + n) time, which is optimal when flops is larger than nnz , m, and n. It uses the popular compressed sparse column (CSC) format for representing its sparse matrices. Algorithm 1 gives the pseudocode for this column-wise serial algorithm for SpGEMM.\n\nAlgorithm 1 Column-wise formulation of serial matrix multiplication\n1: procedure Columnwise-SpGEMM(A, B, C) 2: for k \u2190 1 to n do 3: for j where B(j, k) = 0 do 4: C(:, k) \u2190 C(:, k) + A(:, j) \u00b7 B(j, k)\nMcCourt et al. [41] target AB T and RAR T operations in the specific context of Algebraic Multigrid (AMG). A coloring of the output matrix C finds structurally orthogonal columns that can be computed simultaneously. Two columns are structurally orthogonal if the inner product of their structures (to avoid numerical cancellation) is zero. They use matrix colorings to restructure B T and R T into dense matrices by merging non-overlapping sparse columns that do not contribute to the same nonzero in the output matrix. They show that this approach would incur less memory traffic than performing sparse inner products by a factor of n/n color where n color is the number of colors used for matrix coloring. However, they do not analyze the memory traffic of other formulations of SpGEMM, which are known to outperform sparse inner products [15]. In particular, a column-wise formulation of SpGEMM using CSC incurs only O(nnz /L + flops) cache misses where L is the size of the cache line. Consider the matrix representing the Erd\u0151s-R\u00e9nyi graph G(n, p), where each edge (nonzero) in the graph (matrix) is present with probability p independently from each other. For p = d/n where d n, in expectation nd nonzeros are uniformly distributed in an n-by-n sparse matrix. In that case, SpGEMM does O(d 2 n) cache misses compared to the O(d n n color ) cache misses of the algorithm by McCourt et al. Hence, the column-wise approach not only bypasses the need for coloring, it also performs better for d \u2264 n color , which is a common case. Furthermore, their method requires precomputing the nonzero structure of the output matrix, which is asymptotically as hard as computing SpGEMM without coloring in the first place.\n\nThere has been a flurry of activity in developing algorithms and implementations of SpGEMM for Graphics Processing Units (GPUs). Among those, the algorithm of Gremse et al. [26] uses the row-wise formulation of SpGEMM. By contrast, Dalton et al. [18] uses the data-parallel ESC (expansion, sorting, and contraction) formulation, which is based on outer products. One downside of the ESC formulation is that expansion might create O(flops) intermediate storage in the worst case, depending on the number of additions performed immediately in shared memory when possible, which might be asymptotically larger than the sizes of the inputs and outputs combined. The recent work of Liu and Vinter is currently the fastest implementation on GPUs and it also addresses heterogenous CPU-GPU processors [36].\n\nIn distributed memory, under many definitions of scalability, all known parallel SpGEMM algorithms are unscalable due to increased communication costs relative to arithmetic operations. For instance, there is no way to keep the parallel efficiency (PE ) fixed for any constant 1 \u2265 PE > 0 as we increase the number of processors [34]. Recently, two attempts have been made to model the communication costs of SpGEMM in a more fine grained manner. Akbudak and Aykanat [3] proposed the first hypergraph model for outer-product formulation of SpGEMM. Unfortunately, a symbolic SpGEMM computation has to be performed initially as the hypergraph model needs full access to the computational pattern that forms the output matrix. Ballard et al. [6] recently proposed hypergraph models for a class of SpGEMM algorithms more general than Akbudak and Aykanat considered. Their model also requires the sparsity structure of the output matrix and the number of vertices in the hypergraph is O(flops), making the approach impractical.\n\nIn terms of in-node parallelism via multithreading, there has been relatively little work. Gustavson's algorithm is not thread scalable because its intermediate working set size is O(n) per thread, requiring a total of O(nt) intermediate storage, which can be larger than the matrices themselves for high thread counts. This intermediate data structure is called the sparse accumulator (SPA) [24]. Nevertheless, it is possible to get good performance out of a multithreaded parallelization of Gustavson's algorithm in current platforms, provided that accesses to SPA are further \"blocked\" for matrices with large dimensions, in order to decrease cache miss rates. In a recent work, this is achieved by partitioning the data structure of the second matrix B by columns [42].\n\nWe also mention that there has been significant research devoted to dense matrix multiplication in distributed-memory settings. In particular, the development of socalled 3D algorithms for dense matrix multiplication spans multiple decades; see [21,30,39,44] and the references therein. Many aspects of our 3D algorithm for sparse matrix multiplication are derived from the dense case, though there are important differences as we detail below.\n\n4. Distributed-memory SpGEMM. We categorize algorithms based on how they partition \"work\" (scalar multiplications) among processes, as we first advocated recently [5]. The work required by SpGEMM can be conceptualized by a cube that is sparsely populated by \"voxels\" that correspond to nonzero scalar multiplications. The algorithmic categorization is based on how these voxels are assigned to processes, which is illustrated in Figure 4.1. 1D algorithms assign a block of n-by-n-by-1 \"layers\" of this cube to processes. In practice, this is realized by having each process store a block of rows or columns of an m-by-n sparse matrix, though the 1D/2D/3D categorization is separate from the data distribution. With correctly chosen data distributions, 1D algorithms communicate entries of only one of the three matrices.\n\n2D algorithms assign a set of 1-by-1-by-n \"fibers\" of this cube to processes. In many practical realizations of 2D algorithms, processes are logically organized as a rectangular p = p r \u00d7p c process grid, so that a typical process is named P (i, j). Subma-trices are assigned to processes according to a 2D block decomposition: For a matrix M \u2208 S m\u00d7n , processor P (i, j) stores the submatrix M ij of dimensions (m/p r ) \u00d7 (n/p c ) in its local memory. With consistent data distributions, 2D algorithms communicate entries of two of the three matrices. Partitioning the work cube to processes. Image reproduced for clarity [5].\n\n3D algorithms assign subcubes (with all 3 dimensions shorter than n) to processes, which are typically organized on a p = p r \u00d7 p c \u00d7 p l grid and indexed by P (i, j, k). 3D algorithms communicate entries of A and B, as well as the (partial sums of the) intermediate products of C. While many ways of assigning submatrices to processes on a 3D process grid exist, including replicating each A ij along the process fiber P (i, j, :), our work focuses on a memory-friendly split decomposition. In this formulation, P (i, j, k) owns the following m/p r \u00d7 n/(p c p l ) submatrix of A \u2208 S m\u00d7n :\n\nA(im/p r : (i + 1)m/p r \u2212 1, jn/p c + kn/(p c p l ) : jn/p c + (k + 1)n/(p c p l ) \u2212 1).\n\nThe distribution of matrices B and C are analogous. This distribution is memory friendly because it does not replicate input or output matrix entries, which is in contrast to many 3D algorithms where the input or the output is explicitly replicated.\n\n4.1. Sparse SUMMA Algorithm. We briefly remind the reader of the Sparse SUMMA algorithm [11] for completeness as it will form the base of our 3D discussion. Sparse SUMMA is based on one formulation of the dense SUMMA algorithm [23]. The processes are logically organized on a p r \u00d7 p c process grid. The algorithm proceeds in stages where each stage involves the broadcasting of n/p r \u00d7b submatrices of A by their owners along their process row, and the broadcasting of b \u00d7 n/p c submatrices of B by their owners along their process column. The recipients multiply the submatrices they received to perform a rank-b update on their piece of the output matrix C. The rank-b update takes the form of a merge in the case of sparse matrices; several rank-b updates can be done together using a multiway merge as described in Section 4.3. We will refer to this as a \"SUMMA stage\" for the rest of the paper. Here, b is a blocking parameter, which can be as large as the inner submatrix dimension. A more complete description of the algorithm and its general implementation for rectangular matrices and process grids can be found in an earlier work [14].\n\n\n4.2.\n\nIn-node Multithreaded SpGEMM Algorithm. Our previous work [12] shows that the standard compressed sparse column or row (CSC or CSR) data structures are too wasteful for storing the local submatrices arising from a 2D decomposition. This is because the local submatrices are hypersparse, meaning that the B\" = x\"\n\n\nC\"\n\nA\" Multiplication of sparse matrices stored by columns [12]. Columns of A are accumulated as specified by the non-zero entries in a column of B using a priority queue (heap) indexed by the row indices. The contents of the heap are stored into a column of C once all required columns are accumulated.\n\nratio of nonzeros to dimension asymptotically approaches zero as the number of processors increase. The total memory across all processors for CSC format would be O(n \u221a p + nnz ), as opposed to O(n + nnz ) memory to store the whole matrix in CSC on a single processor. This observation applies to 3D algorithms as well because their execution is reminiscent of running a 2D algorithm on each processor layer P (:, :, k). Thus, local data structures used within 2D and 3D algorithms must respect hypersparsity.\n\nSimilarly, any algorithm whose complexity depends on matrix dimension, such as Gustavson's serial SpGEMM algorithm, is asymptotically too wasteful to be used as a computational kernel for multiplying the hypersparse submatrices. We use Heap-SpGEMM, first presented as Algorithm 2 of our earlier work [12], which operates on the strictly O(nnz ) doubly compressed sparse column (DCSC) data structure, and its time complexity does not depend on the matrix dimension. DCSC [12] is a further compressed version of CSC where repetitions in the column pointers array, which arise from empty columns, are not allowed. Only columns that have at least one nonzero are represented, together with their column indices. DCSC is essentially a sparse array of sparse columns, whereas CSC is a dense array of sparse columns. Although not part of the essential data structure, DCSC can support fast column indexing by building an AUX array that contains pointers to nonzero columns (columns that have at least one nonzero element) in linear time.\n\nOur HeapSpGEMM uses a heap-assisted column-by-column formulation whose time complexity is\nnzc(B) j=0 O flops(C(:, j)) log nnz (B(:, j)) ,\nwhere nzc(B) is the number of columns of B that are not entirely zero, flops(C(:, j)) is the number of nonzero multiplications and additions required to generate the jth column of C. The execution of this algorithm is illustrated in Figure 4.2, which differs from Gustavson's formulation in its use of a heap (priority queue) as opposed to a sparse accumulator (SPA).\n\nOur formulation is more suitable for multithreaded execution where we parallelize over the columns of C and each thread computes A times a subset of the columns of B. SPA is an O(n) data structure, hence a multithreaded parallelization over columns of C of the SPA-based algorithm would require O(nt) space for t threads. By contrast, since each heap in HeapSpGEMM is of size O(nnz (B(:, j)), the total temporary memory requirements of our multithreaded algorithm are always strictly smaller than the space required to hold one of the inputs, namely B.\n\n\nMultithreaded Multiway Merging and Reduction.\n\nEach stage of Sparse SUMMA generates partial result matrices that are summed together at the end of all stages to obtain the final result C. In the 3D algorithm discussed in Section 4.4, we also split C across fibers of 3D grid, and the split submatrices are summed together by each process on the fiber. To efficiently perform these two summations, we represent the intermediate matrices as lists of triples, where each triple (i, j, val) stores the row index, column index, and value of a nonzero entry, respectively. Each list of triples is kept sorted lexicographically by the (j, i) pair so that the jth column comes before the (j+1)st column. We then perform the summation of sparse matrices by merging the lists of triples that represent the matrices. The merging also covers the reduction of triples with repeated indices.\n\nTo perform a k-way merge on k lists of triples T 1 , T 2 , ..., T k , we maintain a heap of size k that stores the current lexicographically minimum entry, based on (j, i) pairs, in each list of triples. In addition to keeping triples, the heap also stores the index of the source list from where each triple was inserted into the heap. The multiway merge routine finds the minimum triple (i * , j * , val * ) from the heap and merges it into the result. When the previous triple in the merged list has the same pair of indices (i * , j * ), the algorithm simply adds the values of these two triples, reducing the indexvalue pairs with repeated indices. If (i * , j * , val * ) is originated from T l , the next triple from T l is inserted into the heap. Hence, the time complexity of a k-way merge is\nk l=1 O nnz (T l ) log k ,\nwhere nnz (T l ) is the number of nonzero entries in T l . When multithreading is employed, each thread merges a subset of columns from k lists of triples using the same k-way merge procedure described earlier. If a thread is responsible for columns j p to j q , these column indices are identified from each list via a binary search. For better load balance, we ensure there is enough parallel slackness [46] by splitting the lists into more parts than the number of threads and merging the columns in each part by a single thread. In our experiments, we created 4t parts when using t threads and employed dynamic thread scheduling.\n\n\nSplit-3D-SpGEMM Algorithm.\n\nOur parallel algorithm is an iterative 3D algorithm that splits the submatrices along the third process grid dimension (of length p l ). This way, while there is no direct relationship between the size of the third process dimension and the extra memory required for the input, the extra memory required by the output is sparsity-structure dependent. If the output is sparse enough so that there are few or no intermediate products with repeated indices, then no extra memory is required. Recall that entries with repeated indices arise when more than one scalar multiplication a ik b kj that results in a nonzero value contributes to the same output element c ij . The pseudocode of our algorithm, Split-3D-SpGEMM, is shown  Algorithm 2 Operation C \u2190 AB using Split-3D-SpGEMM\nC int ijk = A ilk l=1 p/c \u2211 B ljk A $ B $ C int$ C % x$ x$ x$ =$ =$ =$ !$ !$ !$\nInput: A \u2208 S m\u00d7l , B \u2208 S l\u00d7n : matrices on a p/c \u00d7 p/c \u00d7 c process grid Output: C \u2208 S m\u00d7n : the product AB, similarly distributed. for all processes P (i, j, k) in parallel do 4:B ijk \u2190 AlltoAll(B ij: , P (i, j, :)) redistribution of B across layers 5: for r = 1 to p/c do r is the broadcasting process column and row 6: for\nq = 1 to locinndim /b do b evenly divides locinndim 7: locindices = (q \u2212 1)b : qb \u2212 1 8: A rem \u2190 Broadcast(A irk (:, locindices), P (i, :, k)) 9:\nB rem \u2190 Broadcast(B rjk (locindices, :), P (:, j, k)) 10:\nC int ij: \u2190 C int ij: + HeapSpGEMM(A rem , B rem ) 11: C int ijk \u2190 AlltoAll(C int ij: , P (i, j, :)) 12: C ijk \u2190 LocalMerge(C int ijk )\nThe Broadcast(A irk , P (i, :, k)) syntax means that the owner of A irk becomes the root and broadcasts its submatrix to all the processes on the ith process row of the kth process layer. Similarly for Broadcast(B rjk , P (:, j, k)), the owner ofB rjk broadcasts its submatrix to all the processes on the jth process column of the kth process layer. In line 7, we find the local column (for A) and row (forB) ranges for matrices that are to be broadcast during that iteration. They are significant only at the broadcasting processes, which can be determined implicitly from the first parameter of Broadcast. In practice, we indexB by columns as opposed to rows in order to obtain the best performance from the column-based DCSC data structure. To achieve this,B gets locally transposed during redistribution in line 4. Using DCSC, the expected cost of fetching b consecutive columns of a matrix A is b plus the size (number of nonzeros) of the output. Therefore, the algorithm asymptotically has the same computation cost for all blocking parameters b.\n\nC int ij: is the intermediate submatrix that contains nonzeros that can potentially belong to all the processes on the (i, j)th fiber P (i, j, :). The AlltoAll call in line 11 packs those nonzeros and sends them to their corresponding owners in the (i, j)th fiber. This results in C int ijk for each process P (i, j, k), which contains only the nonzeros that belong to that process. C int ijk possibly contains repeated indices (i.e. multiple entries with the same index) that need to be merged and summed by the LocalMerge call in line 12, resulting in the final output.\n\nIn contrast to dense matrix algorithms [21,30,39,44], our sparse 3D formulation requires a more lenient trade-off between bandwidth-related communication costs and memory requirements. As opposed to increasing the storage requirements by a factor of p l , the relative cost of the 3D formulation is nnz (C int )/ nnz (C), which is always upper bounded by flops(A, B)/ nnz (C).\n\n\nCommunication Analysis of the Split-3D-SpGEMM Algorithm.\n\nFor our complexity analysis, the previous work [5] assumed that nonzeros of sparse n-by-n input matrices are independently and identically distributed, with d > 0 nonzeros per row and column on the average. The sparsity parameter d simplifies analysis by making different terms in the complexity comparable to each other. However, in order to capture the performance of more general matrix-matrix multiplication, we will analyze parallel complexity directly in terms of flops and the number of nonzeros in A, B, and C without resorting to the sparsity parameter d.\n\nOur algorithm can run on a wide range of configurations on a virtual 3D p = p r \u00d7 p c \u00d7 p l process grid. To simplify the analysis, we again assume that each 2D layer of the 3D grid is square, i.e. p r = p c and we use c to denote the third dimension. Thus, we assume a p/c \u00d7 p/c \u00d7 c process grid.\n\nThe communication in Algorithm 2 consists of collective operations being performed on disjoint process fibers: simultaneous broadcasts in the first two process grid dimensions at line 8 and line 9 and simultaneous all-to-alls in the third process grid dimension at line 4 and line 11. We use the notation T bcast (w,p, \u03bd, \u00b5) and T a2a (w,p, \u03bd, \u00b5) to denote the costs of broadcast and all-to-all, where w is the size of the data (per processor) in matrix elements,p is the number of processes participating in the collective, \u03bd is the number of simultaneous collectives, and \u00b5 is the number of processes per node. Parameters \u03bd and \u00b5 capture resource contention: the number of simultaneous collectives affects contention for network bandwidth, and the number of processes per node affects contention for the network interface card on each node.\n\nIn general, these cost functions can be approximated via microbenchmarks for a given machine and MPI implementation, though they can vary over different node allocations as well. If we ignore resource contention, with \u03bd = 1 and \u00b5 = 1, then the costs are typically modeled [17] as\nT bcast (w,p, 1, 1) = \u03b1 \u00b7 logp + \u03b2 \u00b7 wp \u2212 1 p and T a2a (w,p, 1, 1) = \u03b1 \u00b7 (p \u2212 1) + \u03b2 \u00b7 wp \u2212 1 p .\nThe all-to-all cost assumes a point-to-point algorithm, minimizing bandwidth cost at the expense of higher latency cost; see [5, Section 2.2] for more details on the tradeoffs within all-to-all algorithms. The time spent in communication is then given by\nT a2a nnz (B) p , c, p c , \u00b5 + n bc \u00b7T bcast b n \u00b7 nnz (A) p/c , p/c, \u221a pc, \u00b5 + n bc \u00b7T bcast b n \u00b7 nnz (B) p/c , p/c, \u221a pc, \u00b5 + T a2a flops(A, B) p , c, p c , \u00b5 .\nThe amount of data communicated in the first three terms is the average over all processes and is accurate only if the nonzeros of the input matrices are evenly distributed across all blocks. The amount of data communicated in the last term is an upper bound on the average; the number of output matrix entries communicated by each process is likely less than the number of flops performed by that process (due to the reduction of locally repeated indices prior to communication). A lower bound for the last term is given by replacing flops(A, B) with nnz (C). If we ignore resource contention, the communication cost is\n\u03b1 \u00b7 O n bc log(p/c) + c + \u03b2 \u00b7 O nnz (A) + nnz (B) \u221a pc + flops(A, B) p ,\nwhere we have assumed that nnz (B) \u2264 flops (A, B). Note that this expression matches the costs for Erd\u0151s-R\u00e9nyi matrices, up to the choice of all-to-all algorithm [5], where nnz (A) \u2248 nnz (B) \u2248 dn and flops(A, B) \u2248 d 2 n.\n\nWe make several observations based on this analysis of the communication. First, increasing c (the number of layers) results in less time spent in broadcast collectives and more time spent in all-to-all collectives (note that if c = 1 then no communication occurs in all-to-all). Second, increasing b (the blocking parameter) results in fewer collective calls but the same amount of data communicated; thus, b navigates a tradeoff between latency cost and local memory requirements (as well as greater possibility to overlap local computation and communication). Third, for a fixed number of cores, lower \u00b5 (higher value of t) will decrease network interface card contention and therefore decrease communication time overall.   [45]. Architectural details of these computers are listed in Table 5.1. In our experiments, we ran only on the CPUs and did not utilize Titan's GPU accelerators.\n\nIn both supercomputers, we used Cray's MPI implementation, which is based on MPICH2. Both chip architectures achieve memory parallelism via hardware prefetching. On Titan, we compiled our code using GCC C++ compiler version 4.6.2 with -O2 -fopenmp flags. On Edison, we compiled our code using the Intel C++ compiler (version 14.0.2) with the options -O2 -no-ipo -openmp. In order to ensure better memory affinity to NUMA nodes of Edison and Titan, we used -cc depth or -cc numa node options when submitting jobs. For example, to run the 3D algorithm on a 8\u00d78\u00d74 process grid with 6 threads, we use the following options on Edison: aprun -n 256 -d 6 -N 4 -S 2 -cc depth. In our experiments, we always allocate cores needed for a particular configuration of 3D algorithms, i.e., to run the 3D algorithm on p/c \u00d7 p/c \u00d7 c process grid with t threads per process, we allocate pt cores and run p MPI processes on the allocated cores.\n\nSeveral software libraries support SpGEMM. For GPUs, CUSP and CUSparse implement SpGEMM. For shared-memory nodes, MKL implements SpGEMM. Trilinos package implements distributed memory SpGEMM [29], which uses a 1D decomposition for its sparse matrices. In this paper, we compared the performance of 2D and 3D algorithms with SpGEMM in Cray-Trilinos package (version 11.6.1.0) available in NERSC computers, which features significant performance improvements over earlier versions. Sparse SUMMA is the 2D algorithm that had been published before [11] without in-node multithreading, and Split-3D-SpGEMM is the 3D algorithm first presented here. Sometimes we will drop the long names and just use 2D and 3D for abbreviation.\n\nIn our experiments, we used both synthetically generated matrices, as well as real matrices from several different sources. In Section 5.2, we benchmark square matrix multiplication. We use R-MAT [16], the Recursive MATrix generator to generate three different classes of synthetic matrices: (a) G500 matrices representing graphs All matrices are from the University of Florida sparse matrix collection [20], except NaluR3, which is a matrix from low Mach number, turbulent reacting flow problem [35]. For the Florida matrices, we consider the explicit zero entries to be nonzeros and update the nnz of the matrices accordingly.\n\nwith skewed degree distributions from Graph 500 benchmark [1], (b) SSCA matrices from HPCS Scalable Synthetic Compact Applications graph analysis (SSCA#2) benchmark [2], and (c) ER matrices representing Erd\u0151s-R\u00e9nyi random graphs. We use the following R-MAT seed parameters to generate these matrices:  and column. We applied a random symmetric permutation to the input matrices to balance the memory and the computational load. In other words, instead of storing and computing C = AB, we compute PCP T = (PAP T )(PBP T ). All of our experiments are performed on double-precision floating-point inputs, and matrix indices are stored as 64-bit integers. In Section 5.3, we benchmark the matrix multiplication corresponding to the restriction operation that is used in AMG. Since AMG on graphs coming from physical problems is an important case, we include several matrices from the Florida Sparse Matrix collection [20] in our experimental analysis. In addition, since AMG restriction is computationally isomorphic to the graph contraction operation performed by multilevel graph partitioners [28], we include a few matrices representing real-world graphs.\n\nThe characteristics of the real test matrices are shown in Table 5.1. Statistics about squaring real matrices and multiplying each matrix with its restriction operator R is given in Table 5.2.\n\n5.1. Intra-node Performance. Our 3D algorithm exploits intra-node parallelism in two computationally intensive functions: (a) local HeapSpGEMM performed by each MPI process at every SUMMA stage, and (b) multiway merge performed at the end of all SUMMA stages. As mentioned before, HeapSpGEMM returns a set of intermediate triples that are kept in memory and merged at the end of all SUMMA stages. In this section, we only show the intra-node scalability of these two functions and compare them against an MKL and a GNU routine.\n\n\nMultithreaded HeapSpGEMM Performance.\n\nWe study intra-node scalability of local SpGEMM by running a single MPI process on a socket of a node and varying the number of threads from one to the maximum number of threads available in a socket. We compare the performance of HeapSpGEMM with MKL routine mkl csrmultcsr. To expedite the multiway merge that is called on the output of HeapSpGEMM, we always keep column indices sorted in increasing order within each row. Hence, we ask mkl csrmultcsr to return sorted output. We show the performance of HeapSpGEMM and MKL in Figure 5.2 where these functions are used to multiply (a) two randomly generated scale 16 G500 matrices, and (b) Cage12 matrix by itself. On 12 threads of Edison, HeapSpGEMM achieves 8.5\u00d7 speedup for scale 16 G500 and 8.7\u00d7 speedup for Cage12, whereas MKL achieves 7.1\u00d7 speedup for scale 16 G500 and 9.2\u00d7 speed for Cage12. Hence, HeapSpGEMM scales as well    as MKL. However, for these matrices, HeapSpGEMM runs faster than MKL on any concurrency with up to 33-fold performance improvement for G500 matrices.\n\n\nMultithreaded Multiway Merge\n\nPerformance. On a p/c\u00d7 p/c\u00d7 c process grid, each MPI process performs two multiway-merge operations. The first one merges p/c lists of triples computed in p/c stages of SUMMA and the second one merges c lists of triples after splitting the previously merged list across layers. Since both merges are performed by the same function, we experimented the intranode performance of multiway merge on a single layer (c=1). For this experiment, we allocate 12p cores on Edison and run SUMMA on a \u221a p \u00d7 \u221a p process grid. Each MPI process is run on a socket using up to 12 available threads. Figure 5.3 shows the merge time needed by MPI rank 0 for a 4-way merge and a 16-way merge when multiplying two G500 matrices on a 4 \u00d7 4 and a 16 \u00d7 16 grid, respectively. We compare the performance of our multiway merge with a GNU multiway merge routine gnu parallel::multiway merge. However, the latter merge routine simply merges lists of triples keeping them sorted by column and row indices, but does not reduce triples with the same (row, column) pair. Hence, we reduce the repeated indices returned by gnu parallel::multiway merge by a multithreaded reduction function and report the total runtime. From Figure 5.3 we observe that our routine performs both 4-way and 16-way merges faster than augmented GNU multiway merge for G500 matrices. On 12 threads of Edison, our multiway merge attains 8.3\u00d7 speedup for 4way merge and 9.5\u00d7 speedup for 16-way merge. By contrast, the augmented GNU merge attains 5.4\u00d7 and 6.2\u00d7 speedups for 4-way and 16-way merges, respectively. We observe similar performances for other matrices as well.\n\n\nSquare Sparse Matrix Multiplication.\n\nIn the first set of experiments, we square real matrices from Table 5.1 and multiply two structurally similar randomly generated matrices. This square multiplication is representative of the expansion operation used in the Markov clustering algorithm [47]. We explore an extensive set of parameters of Sparse SUMMA (2D) and Split-3D-SpGEMM (which is the main focus of this work), identify optimum parameters on difference levels of concurrency, empirically explain where Split-3D-SpGEMM gains performance, and then show the scalability of Split-3D-SpGEMM for a comprehensive set of matrices.\n\n\nPerformance of Different Variants of 2D and 3D\n\nAlgorithms. At first, we investigate the impact of multithreading and 3D algorithm on the performance of SpGEMM. For this purpose, we fix the number of cores p and multiply two sparse matrices with different combinations of thread counts t and number of layers c. Figure 5.4 shows strong scaling of squaring of nlpkkt160 matrix on Edison. On lower concurrency (< 256 cores), multithreading improves the performance of 2D algorithm, e.g., about 1.5\u00d7 performance improvement with 6 threads on 256 cores in Figure 5.4. However, there is little or no benefit in using a 3D algorithm over a multithreaded 2D algorithm on lower concurrency because the processor grid in a layer becomes too small for 3D algorithms.\n\nFor better resolution on higher concurrency, we have not shown the runtime of 2D algorithms before 64 cores in Figure 5.4. For the completeness of our discussion, we briefly discuss the performance of our algorithm on lower concurrency and compare them against MKL and Matlab. Matlab uses an efficient CSC implementation of Gustavson's algorithm. 2D non-threaded algorithm takes about 800 seconds on a single core and attains about 50\u00d7 speedup when we go from 1 core to 256 cores, and 2D algorithm with 6 threads attains about 25\u00d7 speedup when we go from 6 cores to 216 cores. By contrast, on a single core, MKL and Matlab take about 500 and 830 seconds, respectively to square randomly permuted nlpkkt160 matrix (in Matlab, we keep explicit zero entries 1 to obtain the same number of nonzeros shown in Table 5.2). Therefore, the serial performance of Sparse SUMMA (2D) is comparable to that of MKL and Matlab. The best single node performance is obtained by multithreaded SpGEMM. Using 24 threads on 24 cores of a single node of Edison, MKL and HeapSPGEMM take about 32 and 30 seconds, respectively. We note that the above performance numbers depend significantly on nonzero structures of the input matrices. Here, we select nlpkkt160 matrix for discussion because the number  of nonzero in the square of nlpkkt160 is about 1.2 billion (c.f. Table 5.2), requiring about 28GB of memory to store the result, which is close to the available single node memory of Edison.\n\nThe performance benefits of the 3D algorithm and multithreading become more dominant on higher concurrency. In Figure 5.4, when we increase p from 256 to 16,384 (64\u00d7 increase), non-threaded 2D and 3D (c=16, t=6) algorithms run 4\u00d7 and 22\u00d7 faster, respectively. Consequently, on 16,384 cores, Split-3D-SpGEMM with c=16, t=6 multiplies nlpkkt160 matrix 8\u00d7 faster than non-threaded 2D algorithm. We observe similar trend for other real and randomly generated matrices as well. For example, Split-3D-SpGEMM with c=16, t=6 runs 10\u00d7 faster than the Sparse SUMMA (2D) algorithm when squaring of NaluR3 on 32,764 cores of Edison ( Figure 5.5), and Split-3D-SpGEMM with c=16, t=8 runs 9.5\u00d7 faster than 2D algorithm when multiplying two scale 26 RMAT matrices on 65,536 cores of Titan ( Figure 5.6).\n\nIn fact, on higher concurrency, the time Split-3D-SpGEMM takes to multiply two square matrices decreases gradually with the increase of c and t as indicated on the right side of Figure 5.4. This trend is also observed in Figures 5.5 and 5.6. Therefore, we expect that using more threads and layers will be beneficial to gain performance on even higher concurrency. time needed by multithreaded HeapSpGEMM, \"Merge Layer\" is the time to merge p/c lists of triples computed in p/c stages of SUMMA within a layer, and \"Merge Fiber\" is the time to merge c lists of triples after splitting pieces of C across processor fibers. For a fixed number of cores, the broadcast time gradually decreases with the increase of both c and t, because as we increase c and/or t, the number of MPI processes participating in broadcast within each process layer decreases. For example, in Figure 5.8, the broadcast time decreases by more than 5\u00d7 from the leftmost bar to the rightmost bar. Since broadcast is the dominating term on higher concurrency, reducing it improves the overall performance of SpGEMM. However, for a fixed number of cores, the All2All time increases with c due to the increased processor count on the fiber. The All2All time also increases with t because each MPI process owns a bigger portion of the data, increasing the All2All communication cost per process. Therefore, increased All2All time might nullify the advantage of reduced broadcast time when we increase c and t, especially on lower concurrency. For example, using c>4 does not reduce the total communication time on 8,192 as shown in Figure 5.7.   (Figure 5.9(a)), which eventually limits the scalability of 2D algorithms on higher concurrency.\n\n\nStrong Scaling of Split-3D-SpGEMM.\n\nIn this subsection, we show the strong scaling of Split-3D-SpGEMM with the best parameters on higher concurrency (c = 16, t = 6 on Edison, and c = 16, t = 8 on Titan) when multiplying real and random matrices. Figure 5.10 shows the strong scaling of Split-3D-SpGEMM when squaring seven real matrices on Edison. When we go from 512 to 32,768 cores (64\u00d7 increase of cores), the average speedup of all matrices in Table 5.1 is about 27\u00d7 (min: 9\u00d7 for delaunay n24, max: 52\u00d7 for mouse gene, standard deviation: 16). We observe that Split-3D-SpGEMM scales better when multiplying larger (e.g., it-2004) and denser matrices (e.g., mouse gene and HV15R) because of the availability of more work. By contrast, delaunay n24 is the sparsest matrix in  Next, we discuss strong scaling of Split-3D-SpGEMM for randomly generated square matrices whose dimensions range from 2 24 to 2 27 . Figures 5.11, 5.12a, and 5.12b show the strong scaling of multiplying two structurally similar random matrices from classes G500, ER, and SSCA, respectively. Once again, Split-3D-SpGEMM scales better when multiplying larger (e.g., scale 27) and denser matrices (G500 and ER matrices have 16 nonzeros per row, but SSCA matrices have 8 nonzeros per row). Multiplying matrices with more nonzeros per row and column is expected to yield better scalability for these matrices. \n\n\nMultiplication with the Restriction Operator.\n\nMultilevel methods are widely used in the solution of numerical and combinatorial problems. Such methods construct smaller problems by successive coarsening. The simplest coarsening is graph contraction: a contraction step chooses two or more vertices in the original graph G to become a single aggregate vertex in the contracted graph G . The edges of G that used to be incident to any of the vertices forming the aggregate become incident to the new aggregate vertex in G . Constructing coarser representations in AMG or graph partitioning [28] is a generalized graph contraction operation. This operation can be performed by multiplying the matrix representing the original fine domain (grid, graph, or hypergraph) by the restriction operator from the left and by the transpose of the restriction from the right [25].\n\nIn our experiments, we construct the restriction matrix R using distance-2 maximal independent set computation, as described by Bell et al. [7]. An independent set in a graph G(V, E) is a subset of its vertices in which no two are neighbors. A maximal independent set (MIS) is an independent set that is not a subset of any other independent set. MIS-2 is a generalization of MIS where no two vertices are distance-2 neighbors. In this scheme, each aggregate is defined by a vertex in MIS-2 and consists of the union of that vertex with its distance-1 neighbors.\n\nThe linear algebraic formulation of Luby's randomized MIS algorithm [37] was originally described earlier [38]. Here, we generalize it to distance-2 case, which is shown in Algorithm 3 at a high level. MxV signifies matrix-vector multiplication. EwiseAdd performs element-wise addition between two vectors, which amounts to a union operation among the index sets of those vectors. EwiseMult is the elementwise multiplication, which amounts to an intersection operation among the index sets. For both EwiseAdd and EwiseMult, wherever the index sets of two vectors overlap, the values for the overlapping indices are \"added\" according to the binary function that is passed as the third parameter. Line 5 finds the smallest random value among a vertex's neighbors using the semiring where scalar multiplication is overloaded with the operation that returns the second operand and the scalar addition is overloaded with the minimum operation. Line 6 extends this to find the smallest random value among the 2-hop neighborhood. Line 8 returns the new additions to MIS-2 if the random value of the second vector (cands) is smaller. Line 9 removes those new additions, newS, from the set of candidates. The rest of the computation is selfexplanatory.\n\nAlgorithm 3 Pseudocode for MIS-2 computation in the language of matrices Input: A \u2208 S n\u00d7n , cands \u2208 S 1\u00d7n Output: mis2 \u2208 S 1\u00d7n : distance-2 maximal independent set, empty in the beginning 1 newS adj \u2190 EwiseAdd(newS adj1, newS adj2, Any()) Union of neighbors 13: cands \u2190 EwiseMult(cands, newS adj, select1st()) 14: mis2 \u2190 EwiseAdd(mis2, newS, select1st()) Add newS to mis2\n\nOnce the set mis2 is computed, we construct the restriction matrix R by having each column represent the union of a vertex in mis2 with its distance-1 neighborhood. The neighborhood is calculated using another MxV operation. The remaining singletons are assigned to an aggregate randomly in order to ensure good load balance. Consequently, R is of dimensions n \u00d7 size(mis2).  Figure 5.13 shows the strong scaling of different variants of 2D and 3D algorithms when computing R T A with NaluR3 matrix on Edison. Split-3D-SpGEMM with c = 16, t = 6 attains 7.5\u00d7 speedup when we go from 512 cores to 32,768 cores, but other variants of 2D and 3D algorithms achieve lower speedups. Comparing the scaling of squaring NaluR3 from Figure 5.5, we observe moderate scalability of all variants of 2D and 3D algorithms when multiplying NaluR3 with the restriction matrix. This is because the number of nonzeros in R T A is only 77 million, whereas nnz (A 2 ) = 2.1 billion for NaluR3 matrix (see Table 5.2). Hence, unlike squaring NaluR3, R T A computation does not have enough work to utilize thousands of cores. However, the performance gap between 2D and 3D algorithms is larger when computing R T A. Figure 5.13 shows that Split-3D-SpGEMM with c = 16, t = 6 runs 8\u00d7 and 16\u00d7 faster than non-threaded 2D algorithm on 512 and 32,768 cores, respectively. Figure 5.14 shows the breakdown of runtime spent by Split-3D-SpGEMM (c = 16, t = 6) to compute R T A for (a) nlpkkt160, and (b) NaluR3 matrices on Edison. We observe that when computing R T A, Split-3D-SpGEMM spends a small fraction of total runtime in the multiway merge routine. For example, on 384 cores in Figure 5.14(b), 37% of total time is spent on computation, and only about 7% of total time is spent on multiway merge. This is because nnz (R T A) is smaller than nnz(A) for NaluR3 matrix (also true for other matrices in Table 5.2). Therefore, in computing R T A, nnz (R T A) dominates the runtime of multiway merge while nnz (A) dominates the local multiplication, making the former less computationally intensive. Hence,  despite good scaling of local multiplication, the overall runtime is dominated by communication even on lower concurrency, thereby limiting the overall scaling on tens of thousands of cores. By contrast, Split-3D-SpGEMM spends 64% of its total runtime in computation (with 40% of the total runtime spent in multiway merge) when squaring NaluR3 on 384 cores of Edison ( Figure 5.14(b)). Hence, squaring of matrices shows better strong scaling than multiplying matrices with restriction operators.\n\nFinally, Figure 5.15 shows the strong scaling of Split-3D-SpGEMM (c = 16, t = 6) to compute R T A for other real matrices. Split-3D-SpGEMM attains moderate  speedups of up to 10\u00d7 when we got from 512 cores to 32,768 cores because of low computational intensity in the computation of R T A. Figure 5.16 shows the scaling and breakdown of runtime spent by Split-3D-SpGEMM (c = 16, t = 6) to multiply R T A and R for (a) nlpkkt160, and (b) NaluR3 matrices on Edison. Even though (R T A)R computation can still obtain limited speedups on higher concurrency, the runtimes in Figure 5.16 and the number of nonzeros in R T AR suggest that we might want to perform this multiplication on lower concurrency if necessary without degrading the overall performance.  \n\n\nPerformance of Multiplying R T A and R.\n\n\nComparison with Trilinos.\n\nWe compared the performance of our algorithms with the distributed-memory SpGEMM available in EpetraExt package of Trilinos. We observed that SpGEMM in EpetraExt runs up to 3\u00d7 faster when we compute AR instead of R T A, especially on lower concurrency. Hence, we only consider the runtime of AR so that we compare against the best configuration of EpetraExt. By contrast, our 2D and 3D algorithms are less sensitive to the order of matrix multiplication with less than 1.5\u00d7 performance improvement in computing AR over R T A. We use a random partitioning of rows to processors for EpetraExt runs. Figure 5.17 shows the strong scaling of EpetraExt's SpGEMM implementation and our 2D/3D algorithms when computing AR on Edison. On low concurrency, EpetraExt runs slower than the 2D algorithm, but the former eventually outperforms the latter on higher concurrency. However, on all concurrencies, the 3D algorithm with c = 8, t = 6 runs at least twice as fast as EpetraExt for these two matrices. We note that these matrices are structured with good separators where 1D decomposition used in EpetraExt usually performs better. However, given the limitations of 1D decomposition for matrices without good separators, EpetraExt is not expected to perform well for graphs with power-law distributions [9]. We have tried scale 24 Graph500 matrices in EpetraExt, but received segmentation fault in I/O. We also tried other larger matrices, but EpetraExt could not finish reading the matrices from files in 24 hours, the maximum allocation limit for small jobs in Edison. Hence, we compare with EpetraExt on problems that it excels (AMG style reduction with matrices having good separators) and even there our 3D algorithm does comparably better. We could have separated the diagonal for better scaling performance [14], but we decided not to as if would break the \"black box\" nature of our algorithm.\n\n6. Conclusions and Future Work. We presented the first implementation of the 3D parallel formulation of sparse matrix-matrix multiplication (SpGEMM). Our implementation exploits inter-node parallelism within a third processor grid dimension as well as thread-level parallelism within the node. It achieves higher performance compared to other available formulations of distributed-memory SpGEMM, without compromising flexibility in the numbers of processors that can be utilized. In particular, by varying the third processor dimension as well as the number of threads, one can run our algorithm on many processor counts.\n\nThe percentage of time spent in communication (data movement) is significantly lower in our new implementation compared to a 2D implementation. This is advantageous for multiple reasons. First, the bandwidth for data movement is expected to increase at a slower rate than other system components, providing a future bottleneck. Second, communication costs more energy than computation [32, Figure 5]. Lastly, communication can be hidden by overlapping it with local computation, up to the time it takes to do the local computation. For example, up to 100% performance increase can be realized with overlapping if the communication costs 50% of overall time. However, if the communication costs 80% of the time, then overlapping can only increase performance by up to 25%. Overlapping communication with computation as well as exploiting task-based programming models are subject to future work.\n\nOur 3D implementation inherits many desirable properties of the 2D matrix decomposition, such as resiliency against matrices with skewed degree distribution that are known to be very challenging for traditional 1D distributions and algorithms. However, the 3D formulation also avoids some of the pitfalls of 2D algorithms, such as their relatively poor performance on structured matrices (due to load imbalance that occurs on the processor on the diagonal), by exploiting parallelism along the third dimension. This enabled our algorithm to beat a highly-tuned 1D implementation (the new EpetraExt) on structured matrices, without resorting to techniques such as matrix splitting that were previously required of the 2D algorithm for mitigating the aforementioned load imbalance [14].\n\nOur experimental results indicate that at large concurrencies, performance of the inter-node communication collectives becomes the determinant factor in overall performance. Even though work on the scaling of collectives on subcommunicators is under way, we believe that the effect of simultaneous communication on several subcommunicators are not well studied and should be the subject of further research. by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI). This research was supported by a grant from the United States-Israel Binational Science Foundation (BSF), Jerusalem, Israel.\n\n\nFig. 4.1: Partitioning the work cube to processes. Image reproduced for clarity [5].\n\n\nFig. 4.2: Multiplication of sparse matrices stored by columns [12]. Columns of A are accumulated as specified by the non-zero entries in a column of B using a priority queue (heap) indexed by the row indices. The contents of the heap are stored into a column of C once all required columns are accumulated.\n\nFig. 4 . 3 :\n43Execution of the Split-3D-SpGEMM algorithm for sparse matrix-matrix multiplication C = A \u00b7 B on a p/c \u00d7 p/c \u00d7 c process grid. Matrices A, B, and C int matrices are shown during the first stage of the algorithm execution (the broadcast and the local update, i.e. one \"SUMMA stage\"). The transition from C int to C happens via an all-to-all followed by a local merge, after all p/c SUMMA stages are completed.in Algorithm 2 for the simplified case of p r = p c = p/c and p l = c. The execution of the algorithm is illustrated inFigure 4.3.\n\n\n(a) a = .57, b = c = .19, and d = .05 for G500, (b) a = .6, and b = c = d = .4/3 for SSCA, and (c) a = b = c = d = .25 for ER. A scale n synthetic matrix is 2 n -by-2 n . On average, G500 and ER matrices have 16 nonzeros, and SSCA matrices have 8 nonzeros per row\n\nFig. 5 . 2 :\n52Thread scaling of our HeapSpGEMM and the MKL routine mkl csrmultcsr on 1,3,6,12 threads (with column indices sorted in the increasing order for each row) when squaring a matrix on a single socket of Edison.\n\nFig. 5 . 3 :\n53Thread scaling of our multiway merge and GNU multiway merge routine gnu parallel::multiway merge augmented with a procedure that reduces repeated indices: (a) in squaring scale 21 G500 matrix on 4\u00d74 process grid and (b) in squaring scale 26 G500 matrix on 16\u00d716 process grid on Edison.\n\nFig. 5 . 4 :\n54Strong scaling of different variants of 2D (Sparse SUMMA) and 3D (Split-3D-SpGEMM) algorithms when squaring of nlpkkt160 matrix on Edison. Performance benefits of the 3D algorithm and multithreading can be realized on higher concurrency. 2D non-threaded algorithm attains about 50\u00d7 speedup when we go from 1 core to 256 cores, and 2D algorithm with 6 threads attains about 25\u00d7 speedup when we go from 6 cores to 216 cores (not shown in the figure).\n\nFig. 5 . 5 :Fig. 5 . 6 :Fig. 5 . 7 :Fig. 5. 8 :\n5556578Strong scaling of different variants of 2D and 3D algorithms when squaring of NaluR3 matrix on Edison. 3D algorithms are an order of magnitude faster than 2D algorithms on higher concurrency. Strong scaling of different variants of 2D and 3D algorithms on Titan when multiplying two scale 26 G500 matrices. 3D algorithm and multithreading improve performance of SpGEMM on Breakdown of runtime spent by Split-3D-SpGEMM for various (c, t) configurations on 8,192 cores of Titan when multiplying two scale 26 G500 matrices. The broadcast time (the most dominating term on high concurrency) decreases gradually with the increase of both c and t, which is the primary catalyst behind the improved performance of multithreaded 3D algorithms. Breakdown of runtime spent by Split-3D-SpGEMM for various (c, t) configurations on 32,768 cores of Titan when multiplying two scale 26 G500 matrices.5.2.2. Breakdown of Runtime.To understand the performance of Split-3D-SpGEMM, we break down the time spent in communication and computation when multiplying two G500 graphs of scale 26 and show them inFigure 5.7 for 8,192 cores andFigure 5.8 for 32,768 cores on Titan. Here, \"Broadcast\" refers to the time needed to broadcast pieces of A and B within each layer, \"AlltoAll\" refers to the communication time needed to communicate pieces of C across layers, \"Local Multiply\" is the of runtime spent by (a) Sparse SUMMA (2D) algorithm with c = 1, t = 1 and (b) Split-3D-SpGEMM algorithm with c = 16, t = 6 to square NaluR3 on Edison.\n\nFigure 5 .\n57 and Figure 5.8 reveal that shorter communication time needed by Split-3D-SpGEMM makes it faster than Sparse SUMMA (2D) on higher concurrency.\n\nFigure 5 .\n59(b) demonstrates that both communication and computation time scale well for Split-3D-SpGEMM with c = 16, t = 6 when squaring NaluR3 on Edison. By contrast, communication time does not scale well for Sparse SUMMA\n\nFig. 5 .Fig. 5 .\n5510: Strong scaling of Split-3D-SpGEMM with c = 16, t = 6 when squaring real matrices on Edison. Large (e.g., it-2004) and dense (e.g., mouse gene and HV15R) matrices scale better than small and sparse (e.g., delaunay n24) matrices. 11: Strong scaling of Split-3D-SpGEMM with c = 16, t = 8 on Titan when multiplying two G500 matrices.per column, and Split-3D-SpGEMM does not scale well beyond 8,192 processor when squaring this matrix.\n\nFig. 5 .\n512: Strong scaling of Split-3D-SpGEMM with c = 16, t = 8 on Titan when multiplying two (a) ER and (b) SSCA matrices.\n\n\n5.3.1. Performance of Multiplying a Matrix with the Restriction Operator.\n\nFig. 5 .\n514: Breakdown of runtime spent by Split-3D-SpGEMM to compute R T A for (a) nlpkkt160, and (b) NaluR3 matrices with c = 16, t = 6 on Edison. Both communication and computation time scale well as we increase the number of cores.\n\nFig. 5 .\n516: Breakdown of runtime spent by Split-3D-SpGEMM to multiply R T A and R for (a) nlpkkt160, and (b) NaluR3 matrices with c = 16, t = 6 on Edison.\n\nFig. 5 .\n517: Comparison of Trilinos's EpetraExt package with 2D and 3D algorithms when computing AR for nlpkkt160 and NaluR3 matrices on Edison.\n\nTable 5 .\n51: Overview of Evaluated Platforms. 1 Only 12 threads were used. 2 Memory bandwidth is measured using the STREAM copy benchmark per node. 5. Experimental Results. We evaluate our algorithms on two supercomputers: Cray XC30 at NERSC (Edison) [22], and Cray XK6 at ORNL (Titan)\n\nTable 5 .\n52: Statistics about squaring real matrices and multiplying each matrix with its restriction operator R.Matrix (A) \nnnz(A) \nnnz(A 2 ) \nnnz(R) nnz(R T A) nnz(R T AR) \n\nmouse gene \n28,967,291 \n482,594,045 \n45,101 \n2,904,560 \n402,200 \nldoor \n46,522,475 \n145,422,935 \n952,203 \n2,308,794 \n118,093 \ndielFilterV3real \n89,306,020 \n688,649,400 \n1,102,824 \n4,316,781 \n100,126 \ncage15 \n99,199,551 \n929,023,247 \n5,154,859 46,979,396 \n17,362,065 \ndelaunay n24 \n100,663,202 \n347,322,258 16,777,216 41,188,184 \n15,813,983 \nnlpkkt160 \n229,518,112 \n1,241,294,184 \n8,345,600 45,153,930 \n3,645,423 \nHV15R \n283,073,458 \n1,768,066,720 \n2,017,169 10,257,519 \n1,400,666 \nNaluR3 \n473,712,505 \n2,187,662,967 17,598,889 77,245,697 \n7,415,297 \nit-2004 \n1,150,725,436 14,045,664,641 41,291,594 89,870,859 \n26,847,490 \n\n\n\nTable 5 .\n51 with 6 nonzeros \n\n\n\nFig. 5.13: Strong scaling of different variants of 2D and 3D algorithms to compute R T A for NaluR3 matrix on Edison.512 \n1024 \n2048 \n4096 \n8192 \n16384 \n32768 \n0.25 \n\n0.5 \n\n1 \n\n2 \n\n4 \n\n8 \n\n16 \n\nNumber of Cores \n\nTime (sec) \n\nR T A with NaluR3 (on Edison) \n\n2D (t=1) \n2D (t=6) \n3D (c=8, t=1) \n3D (c=8, t=6) \n3D (c=16, t=6) \n\n384 \n864 1,536 3,456 7,776 16,224 31,104 \n0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1 \n\n1.2 \n\n1.4 \n\nNumber of Cores \n\nTime (sec) \n\nBroadcast \nAlltoAll \nLocal Multiply \nMerge Layer \nMerge Fiber \nOther \n\n(a) nlpkkt160 \n\n384 \n864 1,536 3,456 7,776 16,224 31,104 \n0 \n\n0.5 \n\n1 \n\n1.5 \n\n2 \n\n2.5 \n\nNumber of Cores \n\nTime (sec) \n\nBroadcast \nAlltoAll \nLocal Multiply \nMerge Layer \nMerge Fiber \nOther \n\n(b) NaluR3 \n\n\n\n\nFig. 5.15: Strong scaling of Split-3D-SpGEMM to compute R T A with c = 16, t = 6 for seven real matrices on Edison.512 \n1024 2048 4096 8192 16384 32768 \n0.03125 \n\n0.0625 \n\n0.125 \n\n0.25 \n\n0.5 \n\n1 \n\n2 \n\n4 \n\nNumber of Cores \n\nTime (sec) \n\nldoor \nmouse_gene \ndielFilterV3real \ndelaunay_n24 \ncage15 \nHV15R \nit\u22122004 \n\n384 \n864 \n1,536 3,456 7,776 16,224 \n0 \n\n0.02 \n\n0.04 \n\n0.06 \n\n0.08 \n\n0.1 \n\n0.12 \n\nNumber of Cores \n\nTime (sec) \n\nBroadcast \nAlltoAll \nLocal Multiply \nMerge Layer \nMerge Fiber \nOther \n\n(a) nlpkkt160 \n\n384 \n864 \n1,536 3,456 7,776 16,224 \n0 \n\n0.05 \n\n0.1 \n\n0.15 \n\n0.2 \n\n0.25 \n\nNumber of Cores \n\nTime (sec) \n\nBroadcast \nAlltoAll \nLocal Multiply \nMerge Layer \nMerge Fiber \nOther \n\n(b) NaluR3 \n\n\nThe default Matlab behavior is to remove entries with zero values when constructing a matrix using its sparse(i,j,v)\nAcknowledgments. We sincerely thank the anonymous reviewers whose feedback greatly improved the presentation and clarity of this paper.This material is based upon work supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program under contract number DE-AC02-05CH11231.This research was supported in part by an appointment to the Sandia National Laboratories Truman Fellowship in National Security Science and Engineering, spon-\nGraph500 benchmark. www.graph500.org. Graph500 benchmark. www.graph500.org.\n\nSSCA benchmark. SSCA benchmark. http://www.graphanalysis.org/benchmark/.\n\nSimultaneous input and output matrix partitioning for outer-product-parallel sparse matrix-matrix multiplication. Kadir Akbudak, Cevdet Aykanat, SIAM Journal on Scientific Computing. 365Kadir Akbudak and Cevdet Aykanat. Simultaneous input and output matrix partitioning for outer-product-parallel sparse matrix-matrix multiplication. SIAM Journal on Scientific Computing, 36(5):C568-C590, 2014.\n\nParallel triangle counting and enumeration using matrix algebra. Ariful Azad, Ayd\u0131n Bulu\u00e7, John R Gilbert, Proceedings of the IPDPSW, Workshop on Graph Algorithm Building Blocks (GABB). the IPDPSW, Workshop on Graph Algorithm Building Blocks (GABB)Ariful Azad, Ayd\u0131n Bulu\u00e7, and John R Gilbert. Parallel triangle counting and enumeration using matrix algebra. In Proceedings of the IPDPSW, Workshop on Graph Algorithm Building Blocks (GABB), 2015.\n\nCommunication optimal parallel multiplication of sparse random matrices. Grey Ballard, Ayd\u0131n Bulu\u00e7, James Demmel, Laura Grigori, Benjamin Lipshitz, Oded Schwartz, Sivan Toledo, SPAA 2013: The 25th ACM Symposium on Parallelism in Algorithms and Architectures. Montreal, CanadaGrey Ballard, Ayd\u0131n Bulu\u00e7, James Demmel, Laura Grigori, Benjamin Lipshitz, Oded Schwartz, and Sivan Toledo. Communication optimal parallel multiplication of sparse random ma- trices. In SPAA 2013: The 25th ACM Symposium on Parallelism in Algorithms and Architectures, Montreal, Canada, 2013.\n\nBrief announcement: Hypergraph partitioning for parallel sparse matrix-matrix multiplication. Grey Ballard, Alex Druinsky, Nicholas Knight, Oded Schwartz, Proceedings of the 27th ACM on Symposium on Parallelism in Algorithms and Architectures (SPAA). the 27th ACM on Symposium on Parallelism in Algorithms and Architectures (SPAA)Grey Ballard, Alex Druinsky, Nicholas Knight, and Oded Schwartz. Brief announcement: Hypergraph partitioning for parallel sparse matrix-matrix multiplication. In Proceedings of the 27th ACM on Symposium on Parallelism in Algorithms and Architectures (SPAA), pages 86-88, 2015.\n\nExposing fine-grained parallelism in algebraic multigrid methods. Nathan Bell, Steven Dalton, Luke N Olson, SIAM Journal on Scientific Computing. 344Nathan Bell, Steven Dalton, and Luke N Olson. Exposing fine-grained parallelism in algebraic multigrid methods. SIAM Journal on Scientific Computing, 34(4):C123-C152, 2012.\n\nAn optimized sparse approximate matrix multiply for matrices with decay. Nicolas Bock, Matt Challacombe, SIAM Journal on Scientific Computing. 351Nicolas Bock and Matt Challacombe. An optimized sparse approximate matrix multiply for matrices with decay. SIAM Journal on Scientific Computing, 35(1):C72-C98, 2013.\n\nScalable matrix computations on large scale-free graphs using 2d graph partitioning. Karen D Erik G Boman, Sivasankaran Devine, Rajamanickam, Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. the International Conference on High Performance Computing, Networking, Storage and AnalysisACM50Erik G Boman, Karen D Devine, and Sivasankaran Rajamanickam. Scalable matrix computa- tions on large scale-free graphs using 2d graph partitioning. In Proceedings of the Inter- national Conference on High Performance Computing, Networking, Storage and Analysis, page 50. ACM, 2013.\n\nSparse matrix multiplication: The distributed block-compressed sparse row library. Urban Bor\u0161tnik, Joost Vandevondele, Val\u00e9ry Weber, J\u00fcrg Hutter, Parallel Computing. 405Urban Bor\u0161tnik, Joost VandeVondele, Val\u00e9ry Weber, and J\u00fcrg Hutter. Sparse matrix mul- tiplication: The distributed block-compressed sparse row library. Parallel Computing, 40(5):47-58, 2014.\n\nChallenges and advances in parallel sparse matrix-matrix multiplication. Ayd\u0131n Bulu\u00e7, John R Gilbert, ICPP'08: Proc. of the Intl. Conf. on Parallel Processing. Portland, Oregon, USAIEEE Computer SocietyAyd\u0131n Bulu\u00e7 and John R. Gilbert. Challenges and advances in parallel sparse matrix-matrix multiplication. In ICPP'08: Proc. of the Intl. Conf. on Parallel Processing, pages 503-510, Portland, Oregon, USA, 2008. IEEE Computer Society.\n\nOn the representation and multiplication of hypersparse matrices. Ayd\u0131n Bulu\u00e7, John R Gilbert, IPDPS'08: Proceedings of the IEEE International Symposium on Paral-lel&Distributed Processing. IEEE Computer SocietyAyd\u0131n Bulu\u00e7 and John R. Gilbert. On the representation and multiplication of hypersparse matrices. In IPDPS'08: Proceedings of the IEEE International Symposium on Paral- lel&Distributed Processing, pages 1-11. IEEE Computer Society, 2008.\n\nThe Combinatorial BLAS: Design, implementation, and applications. Ayd\u0131n Bulu\u00e7, John R Gilbert, International Journal of High Performance Computing Applications (IJH-PCA). 254Ayd\u0131n Bulu\u00e7 and John R. Gilbert. The Combinatorial BLAS: Design, implementation, and applications. International Journal of High Performance Computing Applications (IJH- PCA), 25(4):496-509, 2011.\n\nParallel sparse matrix-matrix multiplication and indexing: Implementation and experiments. Ayd\u0131n Bulu\u00e7, John R Gilbert, SIAM Journal of Scientific Computing (SISC). 344Ayd\u0131n Bulu\u00e7 and John R. Gilbert. Parallel sparse matrix-matrix multiplication and index- ing: Implementation and experiments. SIAM Journal of Scientific Computing (SISC), 34(4):170 -191, 2012.\n\nImplementing Sparse Matrices for Graph Algorithms. Ayd\u0131n Bulu\u00e7, John R Gilbert, Viral B Shah, Graph Algorithms in the Language of Linear Algebra. SIAM. Jeremy Kepner and John R. GilbertAyd\u0131n Bulu\u00e7, John R. Gilbert, and Viral B. Shah. Implementing Sparse Matrices for Graph Algorithms. In Jeremy Kepner and John R. Gilbert, editors, Graph Algorithms in the Language of Linear Algebra. SIAM, 2011.\n\nR-MAT: A recursive model for graph mining. Deepayan Chakrabarti, Yiping Zhan, Christos Faloutsos, SDM. SIAM. Michael W. Berry, Umeshwar Dayal, Chandrika Kamath, and David B. SkillicornDeepayan Chakrabarti, Yiping Zhan, and Christos Faloutsos. R-MAT: A recursive model for graph mining. In Michael W. Berry, Umeshwar Dayal, Chandrika Kamath, and David B. Skillicorn, editors, SDM. SIAM, 2004.\n\nCollective communication: theory, practice, and experience. Ernie Chan, Marcel Heimlich, Avi Purkayastha, Robert A Van De Geijn, Concurrency and Computation: Practice and Experience. 1913Ernie Chan, Marcel Heimlich, Avi Purkayastha, and Robert A. van de Geijn. Collective com- munication: theory, practice, and experience. Concurrency and Computation: Practice and Experience, 19(13):1749-1783, 2007.\n\nOptimizing sparse matrix-matrix multiplication for the GPU. Steven Dalton, Luke Olsen, Nathan Bell, ACM Transactions on Mathematical Software. 414to appearSteven Dalton, Luke Olsen, and Nathan Bell. Optimizing sparse matrix-matrix multiplication for the GPU. ACM Transactions on Mathematical Software, 41(4), to appear.\n\nDirect Methods for Sparse Linear Systems. Timothy A Davis, Society for Industrial and Applied Mathematics. Timothy A. Davis. Direct Methods for Sparse Linear Systems. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2006.\n\nThe university of florida sparse matrix collection. A Timothy, Yifan Davis, Hu, ACM Transactions on Mathematical Software (TOMS). 3811Timothy A Davis and Yifan Hu. The university of florida sparse matrix collection. ACM Transactions on Mathematical Software (TOMS), 38(1):1, 2011.\n\nParallel matrix and graph algorithms. E Dekel, D Nassimi, S Sahni, SIAM Journal on Computing. 104E. Dekel, D. Nassimi, and S. Sahni. Parallel matrix and graph algorithms. SIAM Journal on Computing, 10(4):657-675, 1981.\n\nSUMMA: Scalable universal matrix multiplication algorithm. R A Van De Geijn, J Watts, Concurrency: Practice and Experience. 94R. A. Van De Geijn and J. Watts. SUMMA: Scalable universal matrix multiplication algorithm. Concurrency: Practice and Experience, 9(4):255-274, 1997.\n\nSparse matrices in Matlab: Design and implementation. John R Gilbert, Cleve Moler, Robert Schreiber, SIAM Journal of Matrix Analysis and Applications. 131John R. Gilbert, Cleve Moler, and Robert Schreiber. Sparse matrices in Matlab: Design and implementation. SIAM Journal of Matrix Analysis and Applications, 13(1):333-356, 1992.\n\nA unified framework for numerical and combinatorial computing. John R Gilbert, Steve Reinhardt, Viral B Shah, Computing in Science and Engineering. 102John R. Gilbert, Steve Reinhardt, and Viral B. Shah. A unified framework for numerical and combinatorial computing. Computing in Science and Engineering, 10(2):20-25, 2008.\n\nGpuaccelerated sparse matrix-matrix multiplication by iterative row merging. Felix Gremse, Andreas Hofter, Lars Ole Schwen, Fabian Kiessling, Uwe Naumann, SIAM Journal on Scientific Computing. 371Felix Gremse, Andreas Hofter, Lars Ole Schwen, Fabian Kiessling, and Uwe Naumann. Gpu- accelerated sparse matrix-matrix multiplication by iterative row merging. SIAM Journal on Scientific Computing, 37(1):C54-C71, 2015.\n\nTwo fast algorithms for sparse matrices: Multiplication and permuted transposition. Fred G Gustavson, ACM Transactions on Mathematical Software. 43Fred G. Gustavson. Two fast algorithms for sparse matrices: Multiplication and permuted transposition. ACM Transactions on Mathematical Software, 4(3):250-269, 1978.\n\nA multilevel algorithm for partitioning graphs. Bruce Hendrickson, Robert Leland, Supercomputing '95: Proceedings of the 1995 ACM/IEEE conference on Supercomputing. New York, NY, USAACM28Bruce Hendrickson and Robert Leland. A multilevel algorithm for partitioning graphs. In Supercomputing '95: Proceedings of the 1995 ACM/IEEE conference on Supercomputing, page 28, New York, NY, USA, 1995. ACM.\n\nAn overview of the Trilinos project. Michael A Heroux, Roscoe A Bartlett, Vicki E Howle, Robert J Hoekstra, Jonathan J Hu, Tamara G Kolda, Richard B Lehoucq, Kevin R Long, Roger P Pawlowski, Eric T Phipps, Andrew G Salinger, Heidi K Thornquist, Ray S Tuminaro, James M Willenbring, Alan Williams, Kendall S Stanley, ACM Trans. Math. Softw. 313Michael A. Heroux, Roscoe A. Bartlett, Vicki E. Howle, Robert J. Hoekstra, Jonathan J. Hu, Tamara G. Kolda, Richard B. Lehoucq, Kevin R. Long, Roger P. Pawlowski, Eric T. Phipps, Andrew G. Salinger, Heidi K. Thornquist, Ray S. Tuminaro, James M. Willenbring, Alan Williams, and Kendall S. Stanley. An overview of the Trilinos project. ACM Trans. Math. Softw., 31(3):397-423, 2005.\n\nCommunication lower bounds for distributed-memory matrix multiplication. D Irony, S Toledo, A Tiskin, J. Parallel Distrib. Comput. 649D. Irony, S. Toledo, and A. Tiskin. Communication lower bounds for distributed-memory matrix multiplication. J. Parallel Distrib. Comput., 64(9):1017-1026, 2004.\n\nGraph Algorithms in the Language of Linear Algebra. J. Kepner and J.R. GilbertSIAM, PhiladelphiaJ. Kepner and J.R. Gilbert, editors. Graph Algorithms in the Language of Linear Algebra. SIAM, Philadelphia, 2011.\n\nExascale computing trends: Adjusting to the. Peter Kogge, John Shalf, Computing in Science & Engineering. 156Peter Kogge and John Shalf. Exascale computing trends: Adjusting to the. Computing in Science & Engineering, 15(6):16-26, 2013.\n\nDensity functional and density matrix method scaling linearly with the number of atoms. Walter Kohn, Physical Review Letters. 76173168Walter Kohn. Density functional and density matrix method scaling linearly with the number of atoms. Physical Review Letters, 76(17):3168, 1996.\n\nAnalyzing scalability of parallel algorithms and architectures. P Vijay, Anshul Kumar, Gupta, Journal of parallel and distributed computing. 223Vijay P. Kumar and Anshul Gupta. Analyzing scalability of parallel algorithms and architec- tures. Journal of parallel and distributed computing, 22(3):379-391, 1994.\n\nTowards extreme-scale simulations for low mach fluids with second-generation trilinos. Paul Lin, Matthew Bettencourt, Stefan Domino, Travis Fisher, Mark Hoemmen, Jonathan Hu, Eric Phipps, Andrey Prokopenko, Sivasankaran Rajamanickam, Christopher Siefert, Parallel Processing Letters. 24041442005Paul Lin, Matthew Bettencourt, Stefan Domino, Travis Fisher, Mark Hoemmen, Jonathan Hu, Eric Phipps, Andrey Prokopenko, Sivasankaran Rajamanickam, Christopher Siefert, et al. Towards extreme-scale simulations for low mach fluids with second-generation trilinos. Parallel Processing Letters, 24(04):1442005, 2014.\n\nA framework for general sparse matrix-matrix multiplication on gpus and heterogeneous processors. Weifeng Liu, Brian Vinter, Journal of Parallel and Distributed Computing. to appearWeifeng Liu and Brian Vinter. A framework for general sparse matrix-matrix multiplication on gpus and heterogeneous processors. Journal of Parallel and Distributed Computing, 2015. (to appear).\n\nA simple parallel algorithm for the maximal independent set problem. Michael Luby, SIAM Journal on Computing. 1541036Michael Luby. A simple parallel algorithm for the maximal independent set problem. SIAM Journal on Computing, 15(4):1036, 1986.\n\nParallel processing of filtered queries in attributed semantic graphs. Adam Lugowski, Shoaib Kamil, Ayd\u0131n Bulu\u00e7, Samuel Williams, Erika Duriakova, Leonid Oliker, Armando Fox, John R Gilbert, Journal of Parallel and Distributed Computing. 79Adam Lugowski, Shoaib Kamil, Ayd\u0131n Bulu\u00e7, Samuel Williams, Erika Duriakova, Leonid Oliker, Armando Fox, and John R Gilbert. Parallel processing of filtered queries in attributed semantic graphs. Journal of Parallel and Distributed Computing, 79:115-131, 2015.\n\nScalable universal matrix multiplication algorithms: 2D and 3D variations on a theme. R Van De Geijn, M Schatz, J Poulson, UT AustinTechnical reportR. van de Geijn M. Schatz, J. Poulson. Scalable universal matrix multiplication algorithms: 2D and 3D variations on a theme. Technical report, UT Austin, 2013.\n\nStandards for graph algorithm primitives. Tim Mattson, David Bader, Jonathan Berry, Ayd\u0131n Bulu\u00e7, Jack Dongarra, Christos Faloutsos, John Feo, Jeremy Gilbert, Joseph Gonzalez, Bruce Hendrickson, High Performance Extreme Computing Conference (HPEC). IEEETim Mattson, David Bader, Jonathan Berry, Ayd\u0131n Bulu\u00e7, Jack Dongarra, Christos Faloutsos, John Feo, Jeremy Gilbert, Joseph Gonzalez, Bruce Hendrickson, et al. Standards for graph algorithm primitives. In High Performance Extreme Computing Conference (HPEC), pages 1-2. IEEE, 2013.\n\nSparse matrix-matrix products executed through coloring. Michael Mccourt, Barry Smith, Hong Zhang, SIAM Journal on Matrix Analysis and Applications. 361Michael McCourt, Barry Smith, and Hong Zhang. Sparse matrix-matrix products executed through coloring. SIAM Journal on Matrix Analysis and Applications, 36(1):90-109, 2015.\n\nParallel efficient sparse matrix-matrix multiplication on multicore platforms. Ali Md Mostofa, Patwary, Nadathur Rajagopalan, Narayanan Satish, Jongsoo Sundaram, Park, J Michael, Anderson, Gautam Satya, Dipankar Vadlamudi, Das, G Sergey, Pudov, O Vadim, Pradeep Pirogov, Dubey, High Performance Computing. SpringerMd Mostofa Ali Patwary, Nadathur Rajagopalan Satish, Narayanan Sundaram, Jongsoo Park, Michael J Anderson, Satya Gautam Vadlamudi, Dipankar Das, Sergey G Pudov, Vadim O Pirogov, and Pradeep Dubey. Parallel efficient sparse matrix-matrix multiplication on multicore platforms. In High Performance Computing, pages 48-57. Springer, 2015.\n\nAn Interactive System for Combinatorial Scientific Computing with an Emphasis on Programmer Productivity. B Viral, Shah, Santa BarbaraUniversity of CaliforniaPhD thesisViral B. Shah. An Interactive System for Combinatorial Scientific Computing with an Em- phasis on Programmer Productivity. PhD thesis, University of California, Santa Barbara, June 2007.\n\nCommunication-optimal parallel 2.5 d matrix multiplication and lu factorization algorithms. Edgar Solomonik, James Demmel, Euro-Par 2011 Parallel Processing. SpringerEdgar Solomonik and James Demmel. Communication-optimal parallel 2.5 d matrix multiplica- tion and lu factorization algorithms. In Euro-Par 2011 Parallel Processing, pages 90-109. Springer, 2011.\n\n. Titan Website, Titan website. https://www.olcf.ornl.gov/titan/.\n\nOptimally universal parallel computers. G Leslie, Valiant, Opportunities and Constraints of Parallel Computing. SpringerLeslie G Valiant. Optimally universal parallel computers. In Opportunities and Constraints of Parallel Computing, pages 155-158. Springer, 1989.\n\nGraph Clustering by Flow Simulation. Stijn Van Dongen, University of UtrechtPhD thesisStijn van Dongen. Graph Clustering by Flow Simulation. PhD thesis, University of Utrecht, 2000.\n\nOn techniques to improve robustness and scalability of a parallel hybrid linear solver. Ichitaro Yamazaki, Xiaoye Li, High Performance Computing for Computational Science VECPAR. Ichitaro Yamazaki and Xiaoye Li. On techniques to improve robustness and scalability of a parallel hybrid linear solver. In High Performance Computing for Computational Science VECPAR, pages 421-434, 2010.\n\nDetecting short directed cycles using rectangular matrix multiplication and dynamic programming. Raphael Yuster, Uri Zwick, SODA '04: Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms. Raphael Yuster and Uri Zwick. Detecting short directed cycles using rectangular matrix mul- tiplication and dynamic programming. In SODA '04: Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms, pages 254-260, 2004.\n", "annotations": {"author": "[{\"end\":96,\"start\":84},{\"end\":110,\"start\":97},{\"end\":123,\"start\":111},{\"end\":137,\"start\":124},{\"end\":154,\"start\":138},{\"end\":169,\"start\":155},{\"end\":183,\"start\":170},{\"end\":200,\"start\":184}]", "publisher": null, "author_last_name": "[{\"end\":95,\"start\":91},{\"end\":109,\"start\":102},{\"end\":122,\"start\":117},{\"end\":136,\"start\":130},{\"end\":153,\"start\":144},{\"end\":168,\"start\":160},{\"end\":182,\"start\":176},{\"end\":199,\"start\":191}]", "author_first_name": "[{\"end\":90,\"start\":84},{\"end\":101,\"start\":97},{\"end\":116,\"start\":111},{\"end\":129,\"start\":124},{\"end\":143,\"start\":138},{\"end\":159,\"start\":155},{\"end\":175,\"start\":170},{\"end\":190,\"start\":184}]", "author_affiliation": null, "title": "[{\"end\":81,\"start\":1},{\"end\":281,\"start\":201}]", "venue": null, "abstract": "[{\"end\":1385,\"start\":488}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1548,\"start\":1544},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1551,\"start\":1548},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1592,\"start\":1588},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1621,\"start\":1617},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":1645,\"start\":1641},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":1676,\"start\":1672},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1699,\"start\":1696},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":1725,\"start\":1721},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1936,\"start\":1933},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1992,\"start\":1988},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2161,\"start\":2157},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2248,\"start\":2245},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2251,\"start\":2248},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2685,\"start\":2681},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3012,\"start\":3009},{\"end\":7932,\"start\":7918},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7974,\"start\":7970},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7991,\"start\":7987},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8587,\"start\":8583},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9413,\"start\":9409},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10461,\"start\":10457},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10534,\"start\":10530},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11082,\"start\":11078},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11417,\"start\":11413},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11554,\"start\":11551},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11826,\"start\":11823},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12504,\"start\":12500},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12880,\"start\":12876},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13132,\"start\":13128},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13135,\"start\":13132},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13138,\"start\":13135},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13141,\"start\":13138},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13495,\"start\":13492},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14777,\"start\":14774},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15804,\"start\":15800},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15943,\"start\":15939},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16857,\"start\":16853},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16929,\"start\":16925},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17244,\"start\":17240},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18301,\"start\":18297},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18471,\"start\":18467},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22208,\"start\":22204},{\"end\":23572,\"start\":23570},{\"end\":23640,\"start\":23638},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25655,\"start\":25651},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25658,\"start\":25655},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25661,\"start\":25658},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25664,\"start\":25661},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26099,\"start\":26096},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28034,\"start\":28030},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29415,\"start\":29412},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30204,\"start\":30200},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31486,\"start\":31482},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31839,\"start\":31835},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32214,\"start\":32210},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32421,\"start\":32417},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32514,\"start\":32510},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32705,\"start\":32702},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32812,\"start\":32809},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33561,\"start\":33557},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33739,\"start\":33735},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":37540,\"start\":37536},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":44588,\"start\":44584},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":44861,\"start\":44857},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":45007,\"start\":45004},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":45500,\"start\":45496},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":45538,\"start\":45534},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46862,\"start\":46861},{\"end\":46934,\"start\":46931},{\"end\":46986,\"start\":46983},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":51743,\"start\":51740},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":52255,\"start\":52251},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":54641,\"start\":54637},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":59449,\"start\":59447},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":60337,\"start\":60335},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":60495,\"start\":60493}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":55351,\"start\":55265},{\"attributes\":{\"id\":\"fig_1\"},\"end\":55660,\"start\":55352},{\"attributes\":{\"id\":\"fig_2\"},\"end\":56214,\"start\":55661},{\"attributes\":{\"id\":\"fig_4\"},\"end\":56480,\"start\":56215},{\"attributes\":{\"id\":\"fig_6\"},\"end\":56703,\"start\":56481},{\"attributes\":{\"id\":\"fig_8\"},\"end\":57005,\"start\":56704},{\"attributes\":{\"id\":\"fig_9\"},\"end\":57470,\"start\":57006},{\"attributes\":{\"id\":\"fig_10\"},\"end\":59042,\"start\":57471},{\"attributes\":{\"id\":\"fig_11\"},\"end\":59199,\"start\":59043},{\"attributes\":{\"id\":\"fig_12\"},\"end\":59426,\"start\":59200},{\"attributes\":{\"id\":\"fig_13\"},\"end\":59881,\"start\":59427},{\"attributes\":{\"id\":\"fig_14\"},\"end\":60009,\"start\":59882},{\"attributes\":{\"id\":\"fig_15\"},\"end\":60085,\"start\":60010},{\"attributes\":{\"id\":\"fig_16\"},\"end\":60323,\"start\":60086},{\"attributes\":{\"id\":\"fig_17\"},\"end\":60481,\"start\":60324},{\"attributes\":{\"id\":\"fig_19\"},\"end\":60628,\"start\":60482},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":60916,\"start\":60629},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61719,\"start\":60917},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":61751,\"start\":61720},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":62469,\"start\":61752},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":63171,\"start\":62470}]", "paragraph": "[{\"end\":2252,\"start\":1387},{\"end\":2987,\"start\":2254},{\"end\":3245,\"start\":2989},{\"end\":5911,\"start\":3247},{\"end\":6662,\"start\":5929},{\"end\":7114,\"start\":6664},{\"end\":7795,\"start\":7116},{\"end\":8366,\"start\":7797},{\"end\":8435,\"start\":8368},{\"end\":10282,\"start\":8568},{\"end\":11083,\"start\":10284},{\"end\":12106,\"start\":11085},{\"end\":12881,\"start\":12108},{\"end\":13327,\"start\":12883},{\"end\":14149,\"start\":13329},{\"end\":14778,\"start\":14151},{\"end\":15369,\"start\":14780},{\"end\":15459,\"start\":15371},{\"end\":15710,\"start\":15461},{\"end\":16858,\"start\":15712},{\"end\":17178,\"start\":16867},{\"end\":17484,\"start\":17185},{\"end\":17995,\"start\":17486},{\"end\":19027,\"start\":17997},{\"end\":19118,\"start\":19029},{\"end\":19534,\"start\":19167},{\"end\":20088,\"start\":19536},{\"end\":20968,\"start\":20138},{\"end\":21771,\"start\":20970},{\"end\":22432,\"start\":21799},{\"end\":23239,\"start\":22463},{\"end\":23644,\"start\":23320},{\"end\":23848,\"start\":23791},{\"end\":25037,\"start\":23985},{\"end\":25610,\"start\":25039},{\"end\":25988,\"start\":25612},{\"end\":26613,\"start\":26049},{\"end\":26912,\"start\":26615},{\"end\":27756,\"start\":26914},{\"end\":28037,\"start\":27758},{\"end\":28391,\"start\":28137},{\"end\":29176,\"start\":28556},{\"end\":29470,\"start\":29250},{\"end\":30361,\"start\":29472},{\"end\":31289,\"start\":30363},{\"end\":32012,\"start\":31291},{\"end\":32642,\"start\":32014},{\"end\":33798,\"start\":32644},{\"end\":33992,\"start\":33800},{\"end\":34521,\"start\":33994},{\"end\":35597,\"start\":34563},{\"end\":37244,\"start\":35630},{\"end\":37876,\"start\":37285},{\"end\":38635,\"start\":37927},{\"end\":40106,\"start\":38637},{\"end\":40896,\"start\":40108},{\"end\":42607,\"start\":40898},{\"end\":43992,\"start\":42646},{\"end\":44862,\"start\":44042},{\"end\":45426,\"start\":44864},{\"end\":46671,\"start\":45428},{\"end\":47044,\"start\":46673},{\"end\":49617,\"start\":47046},{\"end\":50374,\"start\":49619},{\"end\":52337,\"start\":50446},{\"end\":52960,\"start\":52339},{\"end\":53856,\"start\":52962},{\"end\":54642,\"start\":53858},{\"end\":55264,\"start\":54644}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8567,\"start\":8436},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19166,\"start\":19119},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21798,\"start\":21772},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23319,\"start\":23240},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23790,\"start\":23645},{\"attributes\":{\"id\":\"formula_5\"},\"end\":23984,\"start\":23849},{\"attributes\":{\"id\":\"formula_6\"},\"end\":28136,\"start\":28038},{\"attributes\":{\"id\":\"formula_7\"},\"end\":28555,\"start\":28392},{\"attributes\":{\"id\":\"formula_8\"},\"end\":29249,\"start\":29177}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30268,\"start\":30261},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":33866,\"start\":33859},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":33989,\"start\":33982},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":37354,\"start\":37347},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":39448,\"start\":39441},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":39989,\"start\":39981},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":43064,\"start\":43057},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":48036,\"start\":48029},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":48926,\"start\":48919}]", "section_header": "[{\"attributes\":{\"n\":\"2.\"},\"end\":5927,\"start\":5914},{\"end\":16865,\"start\":16861},{\"end\":17183,\"start\":17181},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20136,\"start\":20091},{\"attributes\":{\"n\":\"4.4.\"},\"end\":22461,\"start\":22435},{\"attributes\":{\"n\":\"4.5.\"},\"end\":26047,\"start\":25991},{\"attributes\":{\"n\":\"5.1.1.\"},\"end\":34561,\"start\":34524},{\"attributes\":{\"n\":\"5.1.2.\"},\"end\":35628,\"start\":35600},{\"attributes\":{\"n\":\"5.2.\"},\"end\":37283,\"start\":37247},{\"attributes\":{\"n\":\"5.2.1.\"},\"end\":37925,\"start\":37879},{\"attributes\":{\"n\":\"5.2.3.\"},\"end\":42644,\"start\":42610},{\"attributes\":{\"n\":\"5.3.\"},\"end\":44040,\"start\":43995},{\"attributes\":{\"n\":\"5.3.2.\"},\"end\":50416,\"start\":50377},{\"attributes\":{\"n\":\"5.4.\"},\"end\":50444,\"start\":50419},{\"end\":55674,\"start\":55662},{\"end\":56494,\"start\":56482},{\"end\":56717,\"start\":56705},{\"end\":57019,\"start\":57007},{\"end\":57519,\"start\":57472},{\"end\":59054,\"start\":59044},{\"end\":59211,\"start\":59201},{\"end\":59444,\"start\":59428},{\"end\":59891,\"start\":59883},{\"end\":60095,\"start\":60087},{\"end\":60333,\"start\":60325},{\"end\":60491,\"start\":60483},{\"end\":60639,\"start\":60630},{\"end\":60927,\"start\":60918},{\"end\":61730,\"start\":61721}]", "table": "[{\"end\":61719,\"start\":61032},{\"end\":61751,\"start\":61732},{\"end\":62469,\"start\":61871},{\"end\":63171,\"start\":62587}]", "figure_caption": "[{\"end\":55351,\"start\":55267},{\"end\":55660,\"start\":55354},{\"end\":56214,\"start\":55677},{\"end\":56480,\"start\":56217},{\"end\":56703,\"start\":56497},{\"end\":57005,\"start\":56720},{\"end\":57470,\"start\":57022},{\"end\":59042,\"start\":57527},{\"end\":59199,\"start\":59056},{\"end\":59426,\"start\":59213},{\"end\":59881,\"start\":59447},{\"end\":60009,\"start\":59893},{\"end\":60085,\"start\":60012},{\"end\":60323,\"start\":60097},{\"end\":60481,\"start\":60335},{\"end\":60628,\"start\":60493},{\"end\":60916,\"start\":60641},{\"end\":61032,\"start\":60929},{\"end\":61871,\"start\":61754},{\"end\":62587,\"start\":62472}]", "figure_ref": "[{\"end\":13766,\"start\":13758},{\"end\":19408,\"start\":19400},{\"end\":29299,\"start\":29293},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":35098,\"start\":35090},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":36221,\"start\":36213},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":36830,\"start\":36822},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":38199,\"start\":38191},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":38439,\"start\":38431},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":38756,\"start\":38748},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":40227,\"start\":40219},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":40741,\"start\":40730},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":40895,\"start\":40884},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":41084,\"start\":41076},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":41773,\"start\":41765},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":42505,\"start\":42497},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":42525,\"start\":42511},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":42864,\"start\":42856},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":43532,\"start\":43520},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":47430,\"start\":47422},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":47776,\"start\":47768},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":48245,\"start\":48237},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":48396,\"start\":48388},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":49505,\"start\":49491},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":49636,\"start\":49628},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":49917,\"start\":49909},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":50197,\"start\":50189},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":51051,\"start\":51043},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":53360,\"start\":53352}]", "bib_author_first_name": "[{\"end\":64068,\"start\":64063},{\"end\":64084,\"start\":64078},{\"end\":64416,\"start\":64410},{\"end\":64428,\"start\":64423},{\"end\":64442,\"start\":64436},{\"end\":64870,\"start\":64866},{\"end\":64885,\"start\":64880},{\"end\":64898,\"start\":64893},{\"end\":64912,\"start\":64907},{\"end\":64930,\"start\":64922},{\"end\":64945,\"start\":64941},{\"end\":64961,\"start\":64956},{\"end\":65459,\"start\":65455},{\"end\":65473,\"start\":65469},{\"end\":65492,\"start\":65484},{\"end\":65505,\"start\":65501},{\"end\":66041,\"start\":66035},{\"end\":66054,\"start\":66048},{\"end\":66069,\"start\":66063},{\"end\":66372,\"start\":66365},{\"end\":66383,\"start\":66379},{\"end\":66696,\"start\":66691},{\"end\":66698,\"start\":66697},{\"end\":66725,\"start\":66713},{\"end\":67325,\"start\":67320},{\"end\":67341,\"start\":67336},{\"end\":67362,\"start\":67356},{\"end\":67374,\"start\":67370},{\"end\":67676,\"start\":67671},{\"end\":67688,\"start\":67684},{\"end\":67690,\"start\":67689},{\"end\":68106,\"start\":68101},{\"end\":68118,\"start\":68114},{\"end\":68120,\"start\":68119},{\"end\":68557,\"start\":68552},{\"end\":68569,\"start\":68565},{\"end\":68571,\"start\":68570},{\"end\":68954,\"start\":68949},{\"end\":68966,\"start\":68962},{\"end\":68968,\"start\":68967},{\"end\":69276,\"start\":69271},{\"end\":69288,\"start\":69284},{\"end\":69290,\"start\":69289},{\"end\":69305,\"start\":69300},{\"end\":69307,\"start\":69306},{\"end\":69668,\"start\":69660},{\"end\":69688,\"start\":69682},{\"end\":69703,\"start\":69695},{\"end\":70075,\"start\":70070},{\"end\":70088,\"start\":70082},{\"end\":70102,\"start\":70099},{\"end\":70122,\"start\":70116},{\"end\":70124,\"start\":70123},{\"end\":70478,\"start\":70472},{\"end\":70491,\"start\":70487},{\"end\":70505,\"start\":70499},{\"end\":70782,\"start\":70775},{\"end\":70784,\"start\":70783},{\"end\":71031,\"start\":71030},{\"end\":71046,\"start\":71041},{\"end\":71299,\"start\":71298},{\"end\":71308,\"start\":71307},{\"end\":71319,\"start\":71318},{\"end\":71540,\"start\":71539},{\"end\":71542,\"start\":71541},{\"end\":71558,\"start\":71557},{\"end\":71815,\"start\":71811},{\"end\":71817,\"start\":71816},{\"end\":71832,\"start\":71827},{\"end\":71846,\"start\":71840},{\"end\":72156,\"start\":72152},{\"end\":72158,\"start\":72157},{\"end\":72173,\"start\":72168},{\"end\":72190,\"start\":72185},{\"end\":72192,\"start\":72191},{\"end\":72496,\"start\":72491},{\"end\":72512,\"start\":72505},{\"end\":72525,\"start\":72521},{\"end\":72529,\"start\":72526},{\"end\":72544,\"start\":72538},{\"end\":72559,\"start\":72556},{\"end\":72919,\"start\":72915},{\"end\":72921,\"start\":72920},{\"end\":73198,\"start\":73193},{\"end\":73218,\"start\":73212},{\"end\":73587,\"start\":73580},{\"end\":73589,\"start\":73588},{\"end\":73604,\"start\":73598},{\"end\":73606,\"start\":73605},{\"end\":73622,\"start\":73617},{\"end\":73624,\"start\":73623},{\"end\":73638,\"start\":73632},{\"end\":73640,\"start\":73639},{\"end\":73659,\"start\":73651},{\"end\":73661,\"start\":73660},{\"end\":73672,\"start\":73666},{\"end\":73674,\"start\":73673},{\"end\":73689,\"start\":73682},{\"end\":73691,\"start\":73690},{\"end\":73706,\"start\":73701},{\"end\":73708,\"start\":73707},{\"end\":73720,\"start\":73715},{\"end\":73722,\"start\":73721},{\"end\":73738,\"start\":73734},{\"end\":73740,\"start\":73739},{\"end\":73755,\"start\":73749},{\"end\":73757,\"start\":73756},{\"end\":73773,\"start\":73768},{\"end\":73775,\"start\":73774},{\"end\":73791,\"start\":73788},{\"end\":73793,\"start\":73792},{\"end\":73809,\"start\":73804},{\"end\":73811,\"start\":73810},{\"end\":73829,\"start\":73825},{\"end\":73847,\"start\":73840},{\"end\":73849,\"start\":73848},{\"end\":74342,\"start\":74341},{\"end\":74351,\"start\":74350},{\"end\":74361,\"start\":74360},{\"end\":74827,\"start\":74822},{\"end\":74839,\"start\":74835},{\"end\":75109,\"start\":75103},{\"end\":75360,\"start\":75359},{\"end\":75374,\"start\":75368},{\"end\":75698,\"start\":75694},{\"end\":75711,\"start\":75704},{\"end\":75731,\"start\":75725},{\"end\":75746,\"start\":75740},{\"end\":75759,\"start\":75755},{\"end\":75777,\"start\":75769},{\"end\":75786,\"start\":75782},{\"end\":75801,\"start\":75795},{\"end\":75826,\"start\":75814},{\"end\":75852,\"start\":75841},{\"end\":76321,\"start\":76314},{\"end\":76332,\"start\":76327},{\"end\":76668,\"start\":76661},{\"end\":76913,\"start\":76909},{\"end\":76930,\"start\":76924},{\"end\":76943,\"start\":76938},{\"end\":76957,\"start\":76951},{\"end\":76973,\"start\":76968},{\"end\":76991,\"start\":76985},{\"end\":77007,\"start\":77000},{\"end\":77019,\"start\":77013},{\"end\":77426,\"start\":77425},{\"end\":77442,\"start\":77441},{\"end\":77452,\"start\":77451},{\"end\":77693,\"start\":77690},{\"end\":77708,\"start\":77703},{\"end\":77724,\"start\":77716},{\"end\":77737,\"start\":77732},{\"end\":77749,\"start\":77745},{\"end\":77768,\"start\":77760},{\"end\":77784,\"start\":77780},{\"end\":77796,\"start\":77790},{\"end\":77812,\"start\":77806},{\"end\":77828,\"start\":77823},{\"end\":78246,\"start\":78239},{\"end\":78261,\"start\":78256},{\"end\":78273,\"start\":78269},{\"end\":78590,\"start\":78587},{\"end\":78643,\"start\":78634},{\"end\":78659,\"start\":78652},{\"end\":78677,\"start\":78676},{\"end\":78703,\"start\":78697},{\"end\":78719,\"start\":78711},{\"end\":78737,\"start\":78736},{\"end\":78754,\"start\":78753},{\"end\":78769,\"start\":78762},{\"end\":79266,\"start\":79265},{\"end\":79612,\"start\":79607},{\"end\":79629,\"start\":79624},{\"end\":79986,\"start\":79985},{\"end\":80490,\"start\":80482},{\"end\":80507,\"start\":80501},{\"end\":80884,\"start\":80877},{\"end\":80896,\"start\":80893}]", "bib_author_last_name": "[{\"end\":64076,\"start\":64069},{\"end\":64092,\"start\":64085},{\"end\":64421,\"start\":64417},{\"end\":64434,\"start\":64429},{\"end\":64450,\"start\":64443},{\"end\":64878,\"start\":64871},{\"end\":64891,\"start\":64886},{\"end\":64905,\"start\":64899},{\"end\":64920,\"start\":64913},{\"end\":64939,\"start\":64931},{\"end\":64954,\"start\":64946},{\"end\":64968,\"start\":64962},{\"end\":65467,\"start\":65460},{\"end\":65482,\"start\":65474},{\"end\":65499,\"start\":65493},{\"end\":65514,\"start\":65506},{\"end\":66046,\"start\":66042},{\"end\":66061,\"start\":66055},{\"end\":66075,\"start\":66070},{\"end\":66377,\"start\":66373},{\"end\":66395,\"start\":66384},{\"end\":66711,\"start\":66699},{\"end\":66732,\"start\":66726},{\"end\":66746,\"start\":66734},{\"end\":67334,\"start\":67326},{\"end\":67354,\"start\":67342},{\"end\":67368,\"start\":67363},{\"end\":67381,\"start\":67375},{\"end\":67682,\"start\":67677},{\"end\":67698,\"start\":67691},{\"end\":68112,\"start\":68107},{\"end\":68128,\"start\":68121},{\"end\":68563,\"start\":68558},{\"end\":68579,\"start\":68572},{\"end\":68960,\"start\":68955},{\"end\":68976,\"start\":68969},{\"end\":69282,\"start\":69277},{\"end\":69298,\"start\":69291},{\"end\":69312,\"start\":69308},{\"end\":69680,\"start\":69669},{\"end\":69693,\"start\":69689},{\"end\":69713,\"start\":69704},{\"end\":70080,\"start\":70076},{\"end\":70097,\"start\":70089},{\"end\":70114,\"start\":70103},{\"end\":70137,\"start\":70125},{\"end\":70485,\"start\":70479},{\"end\":70497,\"start\":70492},{\"end\":70510,\"start\":70506},{\"end\":70790,\"start\":70785},{\"end\":71039,\"start\":71032},{\"end\":71052,\"start\":71047},{\"end\":71056,\"start\":71054},{\"end\":71305,\"start\":71300},{\"end\":71316,\"start\":71309},{\"end\":71325,\"start\":71320},{\"end\":71555,\"start\":71543},{\"end\":71564,\"start\":71559},{\"end\":71825,\"start\":71818},{\"end\":71838,\"start\":71833},{\"end\":71856,\"start\":71847},{\"end\":72166,\"start\":72159},{\"end\":72183,\"start\":72174},{\"end\":72197,\"start\":72193},{\"end\":72503,\"start\":72497},{\"end\":72519,\"start\":72513},{\"end\":72536,\"start\":72530},{\"end\":72554,\"start\":72545},{\"end\":72567,\"start\":72560},{\"end\":72931,\"start\":72922},{\"end\":73210,\"start\":73199},{\"end\":73225,\"start\":73219},{\"end\":73596,\"start\":73590},{\"end\":73615,\"start\":73607},{\"end\":73630,\"start\":73625},{\"end\":73649,\"start\":73641},{\"end\":73664,\"start\":73662},{\"end\":73680,\"start\":73675},{\"end\":73699,\"start\":73692},{\"end\":73713,\"start\":73709},{\"end\":73732,\"start\":73723},{\"end\":73747,\"start\":73741},{\"end\":73766,\"start\":73758},{\"end\":73786,\"start\":73776},{\"end\":73802,\"start\":73794},{\"end\":73823,\"start\":73812},{\"end\":73838,\"start\":73830},{\"end\":73857,\"start\":73850},{\"end\":74348,\"start\":74343},{\"end\":74358,\"start\":74352},{\"end\":74368,\"start\":74362},{\"end\":74833,\"start\":74828},{\"end\":74845,\"start\":74840},{\"end\":75114,\"start\":75110},{\"end\":75366,\"start\":75361},{\"end\":75380,\"start\":75375},{\"end\":75387,\"start\":75382},{\"end\":75702,\"start\":75699},{\"end\":75723,\"start\":75712},{\"end\":75738,\"start\":75732},{\"end\":75753,\"start\":75747},{\"end\":75767,\"start\":75760},{\"end\":75780,\"start\":75778},{\"end\":75793,\"start\":75787},{\"end\":75812,\"start\":75802},{\"end\":75839,\"start\":75827},{\"end\":75860,\"start\":75853},{\"end\":76325,\"start\":76322},{\"end\":76339,\"start\":76333},{\"end\":76673,\"start\":76669},{\"end\":76922,\"start\":76914},{\"end\":76936,\"start\":76931},{\"end\":76949,\"start\":76944},{\"end\":76966,\"start\":76958},{\"end\":76983,\"start\":76974},{\"end\":76998,\"start\":76992},{\"end\":77011,\"start\":77008},{\"end\":77027,\"start\":77020},{\"end\":77439,\"start\":77427},{\"end\":77449,\"start\":77443},{\"end\":77460,\"start\":77453},{\"end\":77701,\"start\":77694},{\"end\":77714,\"start\":77709},{\"end\":77730,\"start\":77725},{\"end\":77743,\"start\":77738},{\"end\":77758,\"start\":77750},{\"end\":77778,\"start\":77769},{\"end\":77788,\"start\":77785},{\"end\":77804,\"start\":77797},{\"end\":77821,\"start\":77813},{\"end\":77840,\"start\":77829},{\"end\":78254,\"start\":78247},{\"end\":78267,\"start\":78262},{\"end\":78279,\"start\":78274},{\"end\":78601,\"start\":78591},{\"end\":78610,\"start\":78603},{\"end\":78632,\"start\":78612},{\"end\":78650,\"start\":78644},{\"end\":78668,\"start\":78660},{\"end\":78674,\"start\":78670},{\"end\":78685,\"start\":78678},{\"end\":78695,\"start\":78687},{\"end\":78709,\"start\":78704},{\"end\":78729,\"start\":78720},{\"end\":78734,\"start\":78731},{\"end\":78744,\"start\":78738},{\"end\":78751,\"start\":78746},{\"end\":78760,\"start\":78755},{\"end\":78777,\"start\":78770},{\"end\":78784,\"start\":78779},{\"end\":79272,\"start\":79267},{\"end\":79278,\"start\":79274},{\"end\":79622,\"start\":79613},{\"end\":79636,\"start\":79630},{\"end\":79893,\"start\":79880},{\"end\":79993,\"start\":79987},{\"end\":80002,\"start\":79995},{\"end\":80264,\"start\":80248},{\"end\":80499,\"start\":80491},{\"end\":80510,\"start\":80508},{\"end\":80891,\"start\":80885},{\"end\":80902,\"start\":80897}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":63873,\"start\":63798},{\"attributes\":{\"id\":\"b1\"},\"end\":63947,\"start\":63875},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":10206184},\"end\":64343,\"start\":63949},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2796736},\"end\":64791,\"start\":64345},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2836595},\"end\":65359,\"start\":64793},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15177650},\"end\":65967,\"start\":65361},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14408688},\"end\":66290,\"start\":65969},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":18334689},\"end\":66604,\"start\":66292},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14489678},\"end\":67235,\"start\":66606},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":808569},\"end\":67596,\"start\":67237},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2693294},\"end\":68033,\"start\":67598},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":397084},\"end\":68484,\"start\":68035},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":12726120},\"end\":68856,\"start\":68486},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12191214},\"end\":69218,\"start\":68858},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":36777386},\"end\":69615,\"start\":69220},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":16652959},\"end\":70008,\"start\":69617},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":18782463},\"end\":70410,\"start\":70010},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":12332464},\"end\":70731,\"start\":70412},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":60831637},\"end\":70976,\"start\":70733},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":207191190},\"end\":71258,\"start\":70978},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":19063016},\"end\":71478,\"start\":71260},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":611285},\"end\":71755,\"start\":71480},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":119805771},\"end\":72087,\"start\":71757},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":35685405},\"end\":72412,\"start\":72089},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":18099698},\"end\":72829,\"start\":72414},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7107184},\"end\":73143,\"start\":72831},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3626585},\"end\":73541,\"start\":73145},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4679315},\"end\":74266,\"start\":73543},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":762539},\"end\":74563,\"start\":74268},{\"attributes\":{\"id\":\"b29\"},\"end\":74775,\"start\":74565},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6614483},\"end\":75013,\"start\":74777},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":40708373},\"end\":75293,\"start\":75015},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":95977},\"end\":75605,\"start\":75295},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":18839139},\"end\":76214,\"start\":75607},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5172502},\"end\":76590,\"start\":76216},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1126858},\"end\":76836,\"start\":76592},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":12090821},\"end\":77337,\"start\":76838},{\"attributes\":{\"id\":\"b37\"},\"end\":77646,\"start\":77339},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":12099965},\"end\":78180,\"start\":77648},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":207067380},\"end\":78506,\"start\":78182},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":41832739},\"end\":79157,\"start\":78508},{\"attributes\":{\"id\":\"b41\"},\"end\":79513,\"start\":79159},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":13931921},\"end\":79876,\"start\":79515},{\"attributes\":{\"id\":\"b43\"},\"end\":79943,\"start\":79878},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":120127713},\"end\":80209,\"start\":79945},{\"attributes\":{\"id\":\"b45\"},\"end\":80392,\"start\":80211},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":14711578},\"end\":80778,\"start\":80394},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":6696796},\"end\":81234,\"start\":80780}]", "bib_title": "[{\"end\":64061,\"start\":63949},{\"end\":64408,\"start\":64345},{\"end\":64864,\"start\":64793},{\"end\":65453,\"start\":65361},{\"end\":66033,\"start\":65969},{\"end\":66363,\"start\":66292},{\"end\":66689,\"start\":66606},{\"end\":67318,\"start\":67237},{\"end\":67669,\"start\":67598},{\"end\":68099,\"start\":68035},{\"end\":68550,\"start\":68486},{\"end\":68947,\"start\":68858},{\"end\":69269,\"start\":69220},{\"end\":69658,\"start\":69617},{\"end\":70068,\"start\":70010},{\"end\":70470,\"start\":70412},{\"end\":70773,\"start\":70733},{\"end\":71028,\"start\":70978},{\"end\":71296,\"start\":71260},{\"end\":71537,\"start\":71480},{\"end\":71809,\"start\":71757},{\"end\":72150,\"start\":72089},{\"end\":72489,\"start\":72414},{\"end\":72913,\"start\":72831},{\"end\":73191,\"start\":73145},{\"end\":73578,\"start\":73543},{\"end\":74339,\"start\":74268},{\"end\":74820,\"start\":74777},{\"end\":75101,\"start\":75015},{\"end\":75357,\"start\":75295},{\"end\":75692,\"start\":75607},{\"end\":76312,\"start\":76216},{\"end\":76659,\"start\":76592},{\"end\":76907,\"start\":76838},{\"end\":77688,\"start\":77648},{\"end\":78237,\"start\":78182},{\"end\":78585,\"start\":78508},{\"end\":79605,\"start\":79515},{\"end\":79983,\"start\":79945},{\"end\":80480,\"start\":80394},{\"end\":80875,\"start\":80780}]", "bib_author": "[{\"end\":64078,\"start\":64063},{\"end\":64094,\"start\":64078},{\"end\":64423,\"start\":64410},{\"end\":64436,\"start\":64423},{\"end\":64452,\"start\":64436},{\"end\":64880,\"start\":64866},{\"end\":64893,\"start\":64880},{\"end\":64907,\"start\":64893},{\"end\":64922,\"start\":64907},{\"end\":64941,\"start\":64922},{\"end\":64956,\"start\":64941},{\"end\":64970,\"start\":64956},{\"end\":65469,\"start\":65455},{\"end\":65484,\"start\":65469},{\"end\":65501,\"start\":65484},{\"end\":65516,\"start\":65501},{\"end\":66048,\"start\":66035},{\"end\":66063,\"start\":66048},{\"end\":66077,\"start\":66063},{\"end\":66379,\"start\":66365},{\"end\":66397,\"start\":66379},{\"end\":66713,\"start\":66691},{\"end\":66734,\"start\":66713},{\"end\":66748,\"start\":66734},{\"end\":67336,\"start\":67320},{\"end\":67356,\"start\":67336},{\"end\":67370,\"start\":67356},{\"end\":67383,\"start\":67370},{\"end\":67684,\"start\":67671},{\"end\":67700,\"start\":67684},{\"end\":68114,\"start\":68101},{\"end\":68130,\"start\":68114},{\"end\":68565,\"start\":68552},{\"end\":68581,\"start\":68565},{\"end\":68962,\"start\":68949},{\"end\":68978,\"start\":68962},{\"end\":69284,\"start\":69271},{\"end\":69300,\"start\":69284},{\"end\":69314,\"start\":69300},{\"end\":69682,\"start\":69660},{\"end\":69695,\"start\":69682},{\"end\":69715,\"start\":69695},{\"end\":70082,\"start\":70070},{\"end\":70099,\"start\":70082},{\"end\":70116,\"start\":70099},{\"end\":70139,\"start\":70116},{\"end\":70487,\"start\":70472},{\"end\":70499,\"start\":70487},{\"end\":70512,\"start\":70499},{\"end\":70792,\"start\":70775},{\"end\":71041,\"start\":71030},{\"end\":71054,\"start\":71041},{\"end\":71058,\"start\":71054},{\"end\":71307,\"start\":71298},{\"end\":71318,\"start\":71307},{\"end\":71327,\"start\":71318},{\"end\":71557,\"start\":71539},{\"end\":71566,\"start\":71557},{\"end\":71827,\"start\":71811},{\"end\":71840,\"start\":71827},{\"end\":71858,\"start\":71840},{\"end\":72168,\"start\":72152},{\"end\":72185,\"start\":72168},{\"end\":72199,\"start\":72185},{\"end\":72505,\"start\":72491},{\"end\":72521,\"start\":72505},{\"end\":72538,\"start\":72521},{\"end\":72556,\"start\":72538},{\"end\":72569,\"start\":72556},{\"end\":72933,\"start\":72915},{\"end\":73212,\"start\":73193},{\"end\":73227,\"start\":73212},{\"end\":73598,\"start\":73580},{\"end\":73617,\"start\":73598},{\"end\":73632,\"start\":73617},{\"end\":73651,\"start\":73632},{\"end\":73666,\"start\":73651},{\"end\":73682,\"start\":73666},{\"end\":73701,\"start\":73682},{\"end\":73715,\"start\":73701},{\"end\":73734,\"start\":73715},{\"end\":73749,\"start\":73734},{\"end\":73768,\"start\":73749},{\"end\":73788,\"start\":73768},{\"end\":73804,\"start\":73788},{\"end\":73825,\"start\":73804},{\"end\":73840,\"start\":73825},{\"end\":73859,\"start\":73840},{\"end\":74350,\"start\":74341},{\"end\":74360,\"start\":74350},{\"end\":74370,\"start\":74360},{\"end\":74835,\"start\":74822},{\"end\":74847,\"start\":74835},{\"end\":75116,\"start\":75103},{\"end\":75368,\"start\":75359},{\"end\":75382,\"start\":75368},{\"end\":75389,\"start\":75382},{\"end\":75704,\"start\":75694},{\"end\":75725,\"start\":75704},{\"end\":75740,\"start\":75725},{\"end\":75755,\"start\":75740},{\"end\":75769,\"start\":75755},{\"end\":75782,\"start\":75769},{\"end\":75795,\"start\":75782},{\"end\":75814,\"start\":75795},{\"end\":75841,\"start\":75814},{\"end\":75862,\"start\":75841},{\"end\":76327,\"start\":76314},{\"end\":76341,\"start\":76327},{\"end\":76675,\"start\":76661},{\"end\":76924,\"start\":76909},{\"end\":76938,\"start\":76924},{\"end\":76951,\"start\":76938},{\"end\":76968,\"start\":76951},{\"end\":76985,\"start\":76968},{\"end\":77000,\"start\":76985},{\"end\":77013,\"start\":77000},{\"end\":77029,\"start\":77013},{\"end\":77441,\"start\":77425},{\"end\":77451,\"start\":77441},{\"end\":77462,\"start\":77451},{\"end\":77703,\"start\":77690},{\"end\":77716,\"start\":77703},{\"end\":77732,\"start\":77716},{\"end\":77745,\"start\":77732},{\"end\":77760,\"start\":77745},{\"end\":77780,\"start\":77760},{\"end\":77790,\"start\":77780},{\"end\":77806,\"start\":77790},{\"end\":77823,\"start\":77806},{\"end\":77842,\"start\":77823},{\"end\":78256,\"start\":78239},{\"end\":78269,\"start\":78256},{\"end\":78281,\"start\":78269},{\"end\":78603,\"start\":78587},{\"end\":78612,\"start\":78603},{\"end\":78634,\"start\":78612},{\"end\":78652,\"start\":78634},{\"end\":78670,\"start\":78652},{\"end\":78676,\"start\":78670},{\"end\":78687,\"start\":78676},{\"end\":78697,\"start\":78687},{\"end\":78711,\"start\":78697},{\"end\":78731,\"start\":78711},{\"end\":78736,\"start\":78731},{\"end\":78746,\"start\":78736},{\"end\":78753,\"start\":78746},{\"end\":78762,\"start\":78753},{\"end\":78779,\"start\":78762},{\"end\":78786,\"start\":78779},{\"end\":79274,\"start\":79265},{\"end\":79280,\"start\":79274},{\"end\":79624,\"start\":79607},{\"end\":79638,\"start\":79624},{\"end\":79895,\"start\":79880},{\"end\":79995,\"start\":79985},{\"end\":80004,\"start\":79995},{\"end\":80266,\"start\":80248},{\"end\":80501,\"start\":80482},{\"end\":80512,\"start\":80501},{\"end\":80893,\"start\":80877},{\"end\":80904,\"start\":80893}]", "bib_venue": "[{\"end\":63834,\"start\":63798},{\"end\":63889,\"start\":63875},{\"end\":64130,\"start\":64094},{\"end\":64529,\"start\":64452},{\"end\":65050,\"start\":64970},{\"end\":65610,\"start\":65516},{\"end\":66113,\"start\":66077},{\"end\":66433,\"start\":66397},{\"end\":66855,\"start\":66748},{\"end\":67401,\"start\":67383},{\"end\":67756,\"start\":67700},{\"end\":68223,\"start\":68130},{\"end\":68655,\"start\":68581},{\"end\":69021,\"start\":68978},{\"end\":69370,\"start\":69314},{\"end\":69724,\"start\":69715},{\"end\":70191,\"start\":70139},{\"end\":70553,\"start\":70512},{\"end\":70838,\"start\":70792},{\"end\":71106,\"start\":71058},{\"end\":71352,\"start\":71327},{\"end\":71602,\"start\":71566},{\"end\":71906,\"start\":71858},{\"end\":72235,\"start\":72199},{\"end\":72605,\"start\":72569},{\"end\":72974,\"start\":72933},{\"end\":73308,\"start\":73227},{\"end\":73881,\"start\":73859},{\"end\":74397,\"start\":74370},{\"end\":74615,\"start\":74565},{\"end\":74881,\"start\":74847},{\"end\":75139,\"start\":75116},{\"end\":75434,\"start\":75389},{\"end\":75889,\"start\":75862},{\"end\":76386,\"start\":76341},{\"end\":76700,\"start\":76675},{\"end\":77074,\"start\":77029},{\"end\":77423,\"start\":77339},{\"end\":77894,\"start\":77842},{\"end\":78329,\"start\":78281},{\"end\":78812,\"start\":78786},{\"end\":79263,\"start\":79159},{\"end\":79671,\"start\":79638},{\"end\":80055,\"start\":80004},{\"end\":80246,\"start\":80211},{\"end\":80571,\"start\":80512},{\"end\":80991,\"start\":80904},{\"end\":64593,\"start\":64531},{\"end\":65068,\"start\":65052},{\"end\":65691,\"start\":65612},{\"end\":66949,\"start\":66857},{\"end\":67779,\"start\":67758},{\"end\":73327,\"start\":73310}]"}}}, "year": 2023, "month": 12, "day": 17}