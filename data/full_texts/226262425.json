{"id": 226262425, "updated": "2023-10-06 09:01:41.827", "metadata": {"title": "doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset", "authors": "[{\"first\":\"Song\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Wan\",\"middle\":[]},{\"first\":\"Chulaka\",\"last\":\"Gunasekara\",\"middle\":[]},{\"first\":\"Siva\",\"last\":\"Patel\",\"middle\":[\"Sankalp\"]},{\"first\":\"Sachindra\",\"last\":\"Joshi\",\"middle\":[]},{\"first\":\"Luis\",\"last\":\"Lastras\",\"middle\":[\"A.\"]}]", "venue": "EMNLP", "journal": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "publication_date": {"year": 2020, "month": 11, "day": 12}, "abstract": "We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text sections as well as lower-level relations between discourse units within a section. Then we present these dialogue flows to crowd contributors to create conversational utterances. The dataset includes about 4800 annotated conversations with an average of 14 turns that are grounded in over 480 documents from four domains. Compared to the prior document-grounded dialogue datasets, this dataset covers a variety of dialogue scenes in information-seeking conversations. For evaluating the versatility of the dataset, we introduce multiple dialogue modeling tasks and present baseline approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2011.06623", "mag": "3099590177", "acl": "2020.emnlp-main.652", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/FengWGPJL20", "doi": "10.18653/v1/2020.emnlp-main.652"}}, "content": {"source": {"pdf_hash": "651d6b898e63cb4d8ed1e7c2b8929227df3aa1c7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2011.06623v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.emnlp-main.652.pdf", "status": "HYBRID"}}, "grobid": {"id": "052f5121cd7d15b242b26678b226a69fcf8a9485", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/651d6b898e63cb4d8ed1e7c2b8929227df3aa1c7.txt", "contents": "\ndoc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset\n\n\nSong Feng sfeng@us \nIBM Research AI\n\n\nHui Wan hwan@us \nIBM Research AI\n\n\nChulaka Gunasekara chulaka.gunasekara@ibm.com \nIBM Research AI\n\n\nSiva Sankalp siva.sankalp.patel@ \nIBM Research AI\n\n\nPatel Sachindra jsachind@in \nIBM Research AI\n\n\nJoshi Luis \nIBM Research AI\n\n\nA Lastras lastrasl@us.ibm.com \nIBM Research AI\n\n\ndoc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset\n\nWe introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text sections as well as lower-level relations between discourse units within a section. Then we present these dialogue flows to crowd contributors to create conversational utterances. The dataset includes about 4800 annotated conversations with an average of 14 turns that are grounded in over 480 documents from four domains. Compared to the prior document-grounded dialogue datasets, this dataset covers a variety of dialogue scenes in information-seeking conversations. For evaluating the versatility of the dataset, we introduce multiple dialogue modeling tasks and present baseline approaches.\n\nIntroduction\n\nThe task of reading documents and responding to queries has been the trigger of many recent research advances. On top of the development of contextual question answering QuAC  and CoQA (Reddy et al., 2019), more recent work MANtIS (Penha et al., 2019) and DoQA (Campos et al., 2020) included more kinds of user intents for querying over documents; while ShARC (Saeidi et al., 2018) added follow-up questions from agents and binary answers from users for the inference over documents. These exciting works confirm the importance of modeling document-grounded dialogue. Yet, it involves more complex scenes in practice, which requires better understanding of the inter-relations between conversations and documents. Thus, we aim to investigate how to create the training instances to further approach real-world applications of document-grounded dialogue for information seeking tasks.\n\nIn this work, we propose a new dataset of goaloriented document-grounded dialogue. Figure 1 shows sample utterances from dialogues D1, D2 and D3 between an assisting agent and a user, and an example document in the middle. D1 and D2 are grounded in the given document, while D3 is irrelevant to the document. It illustrates two different types of contexts that we aim to capture:\n\n(1) dialogue-based context, where a query could be formed by a single or multiple turns, and (2) document-based context, which corresponds to varied forms of knowledge represented in the document. More specifically, dialogue-based context of a query could be initiated by a user (e.g., U1 in D1) or an agent (e.g., A3 in D1), and carried out through multiple turns by both roles (e.g., all turns in D2). Document-based context could involve structural elements in documents, such as the headers T1 and T2 or list items of m1 and m2, as well as textual discourse units, such as clauses (e.g, \"If your clothing has been damaged\").\n\nFor creating such dataset, we consider the document contents for social welfare websites,such as ssa.gov and va.gov, which guide users to access various forms of information. We develop a pipeline approach for dialogue data construction. Inspired by how human authors compose user-facing web content, we utilize both the highlevel hierarchical relations between document components, as well as the low-level semantic relations between discourse units (Stede et al., 2019) to dynamically create outlines of dialogues, or we call dialogue flows. A dialogue flow is a sequence of interactions between an assisting agent and a user. Each turn contains a dialogue scene that is defined by a dialogue act, a role (user or agent) and a piece of grounding content from a document. Then we present these dialogue flows to crowd contributors to create conversational utterances. Such approach helps to avoid additional noise from the post-hoc human annotations of dialogue data, which is a known challenge (Geertzen and Bunt, 2009).\n\nThe dataset contains about 4800 annotated conversations with an average of 14 turns per dialogue. The utterances are grounded in over 480 documents from four domains. Unlike the previous work on document-grounded question answering or dialogues Reddy et al., 2019;Saeidi et al., 2018) that are based on a short text snippet, our dialogues are grounded in a much wider span of context in the associated documents.\n\nFor evaluation, we propose three tasks that are related to identifying and generating responses with grounding content in documents: (1) user utterance understanding; (2) agent response generation; and (3) relevant document identification. For each task, we present baseline approaches and evaluation results. Our goal is to elicit further research efforts on building document-grounded dialogue models that can incorporate deeper contexts for tackling goaloriented information-seeking tasks. We summarize our main contributions as follows:\n\n\u2022 We introduce a novel dataset for modeling dialogues that are grounded in documents from multiple domains. The dataset is available at http://doc2dial.github.io/.\n\n\u2022 We develop a pipeline approach for dialogue data collection, which has been adapted and evaluated for varied domains.\n\n\u2022 We propose multiple dialogue modeling tasks that are supported by our dataset, and present the baseline approaches.\n\n\nDoc2Dial\n\nWe introduce doc2dial, a new dataset that includes (1) a set of documents; and (2) conversations between an assisting agent and an end user, which are grounded in the associated documents. Figure 1 presents sample utterances from different dialogues along with a sample document from va.gov in the middle. It illustrates some prominent features in our dataset, such as the cases where a conversation involves multiple interconnected sub-tasks under a general inquiry (e.g., D1); or the cases where a conversation involves multiple interactions to verify the conditional contexts for one query (e.g., D2).\n\nRecent work, such as Saeidi et al. (2018), has started to address the challenge of modeling complex contexts by allowing follow-up questions from agents based on natural language inference rules extracted from the relevant documents. However, it also simplified the task by using only restricted forms of questions and binary answers. In our work, we not only encourage free-form utterances, but also aim to include various dialogue scenes that provoke inquires with different document-based and dialog-based contexts. A user query can be formed in single-turn or multiple-turn manners: (1) the user explicitly states a context that is associated with a text-span that contains a solution to the query, e.g., U5 on T4; (2) the user describes an implicitly stated context associated with a solution, e.g., U7; (3) the user accepts or rejects a piece of agent-stated context that is associated with a solution, e.g., U4 (rejection), and U12 & U14 (acceptance). An agent response, on the other hand, either provides a solution or poses a query depending on the context of a given user query: (1) whether the query is irrelevant to the grounding document, e.g., A17;\n\n(2) whether the query is under-specified, if so, the agent will suggest associated context, e.g., A11 and A13; (3) whether a relevant answer is identified in the grounding document, e.g, A6, A8 and A15.\n\n\nData Collection\n\nFor collecting document-grounded dialogue data, we propose a pipeline approach derived from the framework proposed by Feng et al. (2020). As shown in Figure 2, it includes the components for:\n\n(1) processing the document contents;\n\n(2) generating dynamic dialogue flows; (3) crowdsourcing the dialogue utterances.\n\n\nData Construction Approach\n\nProcessing document contents We first select documents that contain the context-indicative elements, such as hierarchical headers and explicit discourse relations (Prasad et al., 2008(Prasad et al., , 2019, since those document contents could provoke more diversified dialogue flows. Then we extract text-spans to create a graph with the spans as nodes and semantic relations as edges. Some spans in the graph correspond to a piece of information for solving user problems, while some correspond to the conditional context of those solutions, such as SP2 and SP1 in Figure 1 respectively. The semantic relations are largely determined by the heuristics derived from the document structures (Mukherjee et al., 2003) and semantic connectives (Das et al., 2018) between discourse units or clauses. Both spans and semantic relations are labeled automatically via our tool. The labels can be reviewed and annotated via crowdsourcing platforms, which is also supported by our tool.\n\nGenerating dynamic dialogue flows Each flow consists of a sequence of dialogue scenes. A dialogue scene is described with (1) role, either a user or an agent; (2) a selected span as the grounding content from the given document; (3) a dialogue act that determines how to describe the selected span in the given role. Thus, each turn is inherently annotated with the dialogue act and a reference to the document contents. The dynamics of the dialogue flows are introduced by varying the three factors that are constrained by the relations from the semantic graph and dialogue history. In principle, we randomly select content from a candidate pool of spans of conditional contexts and solutions. The pool is updated after every turn is generated based on the status of the previously selected span. The general rule for updating the candidate pool is to avoid re-selecting any spans with an established status. In addition, the dialogue flow is principally aligned with common practice of dialogue management, for instance, after an agent asks a user a question, we expect the next turn would be the user answering the question.\n\nCollecting human utterances Finally, we present the sequences of dialogue scenes to crowdsourced contributors to convert them into conversational utterances.\n\n\nCrowdsourcing Setup\n\nOur data collection task asks the crowd contributors to focus on one turn at a time so that they can carefully review the given dialogue scene and the dialogue history. Since the crowd generally prefers to work on tasks in batches, we try different settings to combine the tasks: (1) each writer plays the same role but for different dialogues per batch; or (2) each writer plays both agent and user role and completes entire dialogue in order, as inspired by Byrne et al. (2019). We also find that the conversations by the second setting tend to be more coherent and less time consuming. Many writers would make efforts to differentiate their writing styles for different roles. Therefore, our tasks were completed based on the second setting by about 70 qualified contributors from appen.com. We pay $1.5-$2 per conversation.\n\n\nDocument Data\n\nFor document contents, we consider the public government service websites that are designated to provide information to a vast group of users. We collect web contents from four domains and select over 480 documents for creating dialogue flows as shown in Table 1   ment is also represented as a sequence of spans, for which we provide indexes to the plain text and the HTML respectively.\n\n\nContent elements\n\nTo characterize the document contents, we examine the HTML source to extract the content elements with different scopes such as, tokens (tk), spans (sp), paragraphs (p) and titled sections (sec). Some of the spans within one sentence, such as SP1 and SP2 in Figure 1, are extracted via constituency parsers (Joshi et al., 2018). The paragraphs and sections are determined using HTML markups. The average counts of these elements per document in Table 1 show the rich structures that are employed across domains. While this work starts to explore the simpler semi-structured information such as D2 in Figure 1; we are yet to explore various semantics from complex list structures, tables and other multi-modal contents in the webpages for future work.\n\n\nDialogue Data\n\nGiven a grounding document, we create about multiple unique dialogue flows with an average of 14 turns for this dataset. All dialogues are created based on a unique dialogue flow. In total, there are close to 4800 conversations with about 62,000 turns from over 480 document in four domains as shown in Table 1. Each dialogue utterance is annotated with a dialogue scene, i.e., role, dialogue act and the grounding span. As it is a known challenge to annotate conversation turns for the dialogue scenes (Geertzen and Bunt, 2009), our pipeline approach for data collection helps avoid the cost and the noise from the additional human annotations. Next we further describe it from different perspectives regarding the dialogue scene.\n\nDialogue acts We adopt the hierarchical dialogue act scheme by Pareti and Lando (2018) with a focus on the ones most essential to the informationseeking tasks. We describe those dialogue acts to the crowdsourced contributors pertaining to the selected grounding content and the assigned role (detailed descriptions in Appendix A). For future work, we plan to extend current dialogue scenes with other actions such as elucidations (Azzopardi et al., 2018) and social acts (Kl\u00fcwer, 2011). To examine the dialogue distributions, we aggregate the hierarchical dialogue acts and list the total of turns, and the average length per turn under each category in Table 2. For example, \"agent -request/query\" corresponds to the queries based on document-guided dialogue management turns via an agent role; \"user -respond/yesOrNo\" corresponds to the scene where a user responds to an agent's query. Since we encourage the crowd to express \"yes\" or \"no\" in natural and creative writings, such as U10 in D2 in Figure 1, the average length of \"respond/yesOrNo\" is 7 tokens.\n\nGrounding content We aim to include the contents that are associated with varied conditional contexts based on the aforementioned span graph without introducing strong bias on certain index position in the document as discussed in Geva and Berant (2018). Therefore, we examine the coverage of the document contents from the generated dialogue flows. As illustrated in Figure 3, we create index of all the selected grounding contents to different document segments such as tokens, spans, paragraphs and titled sections (y-axis). The x-axis (numbered 1-10) indicates the position where 1 is closest to the beginning and 10 is closest to the end of a document. The numbers in the cells indicate the percentage distribution among all the ground-feedback on rejected dialogue scene % The selected-text is not a contextual condition. 74.3 The selected-text is not a solution to the query. 10.5 Cannot write a turn to be coherent with the chat history.\n\n\n10.1\n\nThere is not enough information in the selected (or adjacent) text.\n\n\n2.4\n\nThe selected-text is not Comprehensible.\n\n1.8 Other.\n\n0.9 Table 3: Feedback on the reasons for rejecting a dialogue scene by crowdsourced annotators.\n\ning contents. The heatmap shows some degree of coverage on all parts of the documents, with a higher density at the beginning as we do include the scenarios of under-specified queries that typically correspond to the intro of a document.\n\nDialogue flows For assessing the quality of the dialogue flows, we also ask the contributors to reject a dialogue turn when it is considered as infeasible to write a coherent utterance. We also solicit feedback via multiple choices on the reason as shown in Table 3. Out of 700 sampled dialogue flows, annotators reject about 4% of the turns. Among the rejected turns, 70% is due to not being able to interpret the selected span as applicable conditional context for user requests. In this dataset, we exclude the (sub)dialogues with rejected turns accordingly. However, we also observe certain \"false positive\" cases, where the crowd would rather try to adjust their writing for a less desirable dialogue scene rather than rejecting the turn, for which they get paid the same either way.\n\n\nData Recomposition\n\nOne benefit of constructing the dialogue data via our pipeline approach is that it provides a convenient and cost-effective way to reshape the existing dialogue data based on their dialogue flows. For instance, to ensure the quality, we can recollect or remove certain turns from the dialogues if they are rejected by the crowd contributors or affected by the changes in the grounding documents. In addition, for obtaining the training instances to identify the irrelevant queries, we modify an existing dialogue by inserting sub-dialogues created for another document or domain, for instance, adding D3 to D1 as irrelevant for va.gov in Figure 1. Similarly, for creating dialogues that are grounded in multiple documents, we select sub-dialogues based on different documents and combine them into one.\n\n\nTasks and Baselines\n\nFor evaluation, we propose three tasks related to identifying the grounding content for a given dialogue: (1) user utterance understanding; (2) agent response prediction; (3) relevant document identification. In our tasks, we also aim to detect the cases that are irrelevant to the associated documents, for which we modify dialogues to include irrelevant (Irr) queries via data re-composition as described in Section 2.4. We split the dialogues into train/dev/test sets as 70%, 15%, 15% with half of the dev/test set grounded in \"unseen\" documents that are not in training set.\n\n\nUser Utterance Understanding\n\nOne of our main goals for creating this dataset is to broaden the coverage of different user queries for various task goals with respect to the associated document. Thus, our first task is interpreting a user utterance in the context of the dialogue history and the associated document. In our case, we first focus on identifying the grounding span of a user utterance.\n\n\nUser Utterance Grounding\n\nIn our dataset, all turns are annotated with a dialogue scene that includes the grounding span. Interpreting the user utterance could be quite challenging, as in some cases, it would depend more on the dialogue history such as U12 and U14; while in other cases, such as U1 and U16, it would depend more on current user utterance itself. For the input of this task, it takes a user utterance along with (1) the dialogue history and 2) the document content with simplified document structure. The output is a span in the document as the reference of the given user utterance. Each grounded user turn is considered a training instance, so a dialogue with n grounded user turns is considered as n instances, with overlapping dialogue context.\n\n\nBaseline Approach\n\nWe formulate the problem as span selection, inspired by extractive question answering tasks such as SQuAD task (Rajpurkar et al., 2016(Rajpurkar et al., , 2018. As a baseline, we adopt the extractive question answering model with transformers encoder by Devlin et al. (2019). More specifically, we follow the question answering example from HuggingFace Transformers (Wolf et al., 2019) with pretrained bert-base-uncased model as encoder and fine-tune it during training.  Table 4: Evaluation results for user utterance grounding. Numbers are \"mean \u00b1 stdev\" that are computed based on the results from 3 random seeds.\n\nThe document content serves as the context input of the model. The query input is the dialogue context, for which we experiment different settings of utilizing the dialogue history: (1) last two turns (last2), i.e., the input user utterance for which we want to identify the dialogue scene, and the utterance before the given user utterance; (2) all previous turns (all), i.e., the input user utterance and all the utterances before it. We also consider different ordering type of the dialog turns: (1) in time order; and (2) in reverse order (last2-r and all-r), i.e., dialogue context is concatenated in reversed time order where the latest user utterance appears first.\n\nOften the grounding document is longer than the maximum sequence length of transformers. In such cases, we truncate the documents in sliding windows with a stride. The dialogue context and each document trunk form one instance to be fed in batch into the encoder. The sequence of the encoded embeddings is then sent to a linear layer, which maps each embedding in the sequence into two logits, representing the probability of the corresponding position being the start and end position of the span. During training, we apply the Cross Entropy loss function to compute the loss. If the ground truth span does not fall in the document trunk, the start and end positions are both considered to be the beginning of the sequence. During decoding, the start-position and end-position logits from all document trunks are considered together to find the span most favored by the model.\n\n\nEvaluation Metrics For evaluation we use Exact\n\nMatch score (EM) and token-level F1 score (F1), as in the evaluation script 2.0 of SQuAD in Table 4 and Table 5.\n\n\nExperiment Results\n\nThe experiment results are summarized in Table 4. Generally, the model performance improves with more information added to the dialogue context. It indicates that the queries in our datasets are highly conversational contextual and our dataset could serve as a valuable source for evaluating dialogue models' capability of learning from deeper context. We also conduct an experiment using dialogue data with Irr. Irr turns impose noise in understanding the context, slightly reduce the model accuracy on the original turns that are grounded to the document. However, the Irr turns themselves are relatively easy to identify and achieve a high score of 92.1 with all previous turns in reverse order. As a result, the overall score of with Irr turns is comparable to without Irr in Table 4.\n\n\nAgent Response Prediction\n\nFor this task, we aim at predicting agent responses with a focus on identifying the grounding spans in the associated document. Such kind of task can be a very important step towards building explainable conversational systems with higher practicality. In addition, we experiment with conditional text generation models to generate in-context utterance given the grounding span in the associated document.\n\n\nAgent Response Grounding Prediction\n\nThis task takes as input 1) the dialogue context; and 2) the document content with simplified document structure, and predicts a span in the document that grounds the next agent response. This task looks very similar to the user-turn grounding text prediction task in Section 3.1.1 in that they both take dialogue context and document context as input and perform a span selection inside the document. However, they are essentially different: the userturn grounding text prediction is to understand what the user has already said, whereas this task is to predict what the agent response would be based on.\n\nBaseline Approach As opposed to investigating this task from the aspect of dialogue management and planning, as a first attempt, we continue with our focus on identifying the associated grounding content in the document. Thus, we treat this as a without Irr with Irr  Dev  Test  Dev  Test  dial-ctxt  EM  F1  EM  F1  EM  F1  EM  F1  last2 30.1 \u00b1 0.9 52.1 \u00b1 0.2 33.5 \u00b1 0.5 52.4 \u00b1 1.1 36.5 \u00b1 0.8 52.8 \u00b1 0.4 40.0 \u00b1 0.7 53.9 \u00b1 0.6 all 23.8 \u00b1 0.9 42.4 \u00b1 0.3 26.9 \u00b1 0.4 42.4 \u00b1 0.2 27.0 \u00b1 0.6 36.4 \u00b1 0.2 31.2 \u00b1 0.8 39.0 \u00b1 0.3 last2-r 29.8 \u00b1 1.2 52.0 \u00b1 0.1 34.2 \u00b1 0.7 52.9 \u00b1 0.8 35.9 \u00b1 1.3 52.3 \u00b1 1.1 39.9 \u00b1 1.1 53.9 \u00b1 0.6 all-r 31.1 \u00b1 1.6 53.0 \u00b1 0.6 34.6 \u00b1 1.1 53.2 \u00b1 0.8 37.2 \u00b1 0.9 52.9 \u00b1 0.5 41.3 \u00b1 0.8 54.3 \u00b1 0.2 Table 5: Evaluation results for agent response grounding prediction. Numbers are \"mean \u00b1 stdev\" that are computed based on the results from 3 random seeds.\n\nspan selection task, and adopt the same evaluation metrics of Exact Match scores and token level F1 scores, and the same baseline approach as in Section 3.1.1. Note that with the same input dialogue context and text context, the model output in Section 3.1.1 is the dialogue scene corresponding to the given user utterance, while the model output of this task is the dialogue scene predicted for the next agent response.\n\n\nExperiment Results\n\nThe experiment results are summarized in Table 5. The scores are generally much lower than the ones from our previous task in Table 4 due to the challenging nature of the task. We do similar trends when comparing with the experiment results for grounding user utterance task in Table 4. We direct our further work on document-guided dialogue management to further improve the performance for this task.\n\n\nAgent Response Generation\n\nNext we evaluate the dataset via the task of generating agent response. This task setting considers that the span annotation is already given, then we evaluate how to generate agent utterance in context with minimized noise. Yet, this is a still quite challenging task, as in our dataset, the focused topic could be varied throughout the conversation; additionally, agent would provide either a response or follow-up \"question\" where the forms of the query turns are not restricted.\n\n\nBaseline Approach and Experiment Results\n\nWe adopt Huggingface implementation of BART (Lewis et al., 2019), using pretrained BART-large model as encoder, and fine-tune it during training. The input includes the user query along with dialogue history, grounding span of the next agent turn, the contexts of the grounding span and dialogue act for the next agent turn; the output is next agent utterance. For dialogue history, we consider two settings, all previous history (all) and last two turns (last2). For DA, we consider with (+da) and without DA in the input. We use BLEU 1-4  as evaluation of the generated utterance. For the context of the grounding span, we include the title of the document and the paragraph where the span belongs.\n\nThe BLEU scores are reported in Table 6. We observe comparable scores when we include all or last two utterances. It indicates that the model might not consider the previous dialogue history much for generating agent turns. Then it might not be able to generate the reasonable answer when agents perform multi-turn verification such as D2 in Figure 1. In addition, when we add DA, the performance drops. One reason might be because writing style of the follow-up \"question\" of agent turns by human writers in our dataset, which might be quite different than questions in the pretrained models. The results confirm that even given the grounding span, generating agent response in context is a very challenging task. We leave further investigation in our future work.\n\n\nRelevant Document Identification\n\nTo facilitate the understanding of the challenge of identifying the most relevant document given initial conversation turns, we next experiment with the task on identifying the grounding document given limited dialogue history information. Thus, the input is certain dialogue context and a pool of documents from all four domains.\n\nBaselines and Experiment Results We consider two different baselines for this task: (1) BM25 (Robertson and Zaragoza, 2009)   HuggingFace Transformers (Wolf et al., 2019), using pretrained bert-base-uncased model as the encoder (Zellers et al., 2018). BM25 method takes the full document into account to create the index and match them against the provided dialogue contexts. BERT model takes the dialogue context d and a document y together as a sequence. We use 512 tokens and feed BERT with the 256 tokens each from d and y. For each dialogue context, we create a set of triples: one triple containing the correct document (labeled with 1), and m triples containing incorrect documents sampled randomly from the set of all documents (labeled with 0). Table 7 corresponds to the setting m = 4. During evaluation, we evaluate a given dialogue context against the set of all documents. The task is evaluated with the commonly used recall (R@k) metric in retrieval tasks, which measures the fraction of times the correct document is found in the top-k predictions.\n\nAs shown in Table 7, bert-based approach shows better performance. From the perspective of examining the quality of our dataset, we also see the numbers confirms that as more turns are included, the better the dialogue is grounded to the relevant document.\n\n\nRelated Work\n\nOur work is mainly focused on modeling dialogues that are grounded in documents. It is generally inspired by the recent substantial interests on the challenges of machine reading comprehension and conversational QA, such as CoQA (Reddy et al., 2019), QuAC  and DoQA (Campos et al., 2020). Those tasks aim to support conversational question answering, which involves understanding a text passage and answering a series of interconnected questions that appear in a conversation. These tasks add the complexity of coreference resolution and contextual reasoning to the reading comprehension challenges such as SQuAD (Rajpurkar et al., 2016(Rajpurkar et al., , 2018, yet aim at identify-ing a solution from a given list of candidates by reasoning over spans from a document. Our task shares those challenges and additionally introduces the dialogue scenes where the agent asks questions when the user query is identified as under-specified or additional verification required for a resolute solution.\n\nAnother recent work Kim et al. (2020) extends MultiWOZ (Budzianowski et al., 2018) by adding turns that are grounded in the FAQ knowledge for certain entity and domain. The document-based knowledge used in our work is beyond question answer pairs about certain entity of a domain but entire documents with variable contexts. In addition, ours is also largely related to conversational search tasks, such as MANtIS (Penha et al., 2019). Similarly, it also provides multi-turn conversations with varied user intents that are grounded in documents from Stack Exchange website. In addition to the domain difference, one major distinction is that the grounding in MANtIS is determined by the hyperlinks to a document. Our grounding is defined at at a much finer level in addition to the link to a document.\n\nTo the best of our knowledge, the closest work to ours is ShARC (Saeidi et al., 2018) with dialogues that are grounded to short text snippets. It also proposes to address under-specified questions by requiring follow-up questions that are answerable with \"yes/no\" answers in similar domains. Our dataset goes beyond ShARC in several aspects nonetheless: we exploit not only paragraph-level structure but also higher-level document structure, we create conversations over much longer span of document content, where utterances are freeformed as opposed to yes/no, and we do not assume one dialogue corresponds to one single goal.\n\n\nConclusion\n\nWe have introduced doc2dial, a new dialogue dataset for goal-oriented tasks that are grounded in documents from multiple domains. Compared to previous work, our dialogues cover a greater variety of dialogue scenes that correspond to both semistructured and unstructured contents with a much wider span in the associated documents. For evaluation, we investigated three types of dialogue tasks and proposed baseline approaches. We hope this work will inspire and assist both dialogue and document modeling for tackling more goal-oriented dialogue tasks in practice.\n\nFigure 2 :\n2The overview of the process for constructing and annotating doc2dial dataset.\n\nFigure 3 :\n3An illustration of the indexes of the relevant grounding contents in the documents.\n\nA9 :\nA9Would you like to find out whether you are eligible? U10: That's exactly why I contact again! A11: Were there any damages to your clothes that were caused by prosthetic or orthopedic device or your skin medicine? U12: The latter happened. A13: Sorry to hear that. And Is it true that you need this device or skin medicine because of an injury or illness related to your military service? U14 : True. A15: You have met the requirements. \u2026 VA clothing allowance U1: I heard about a disability benefit for clothing. What is it about please? A2: One may be able to get money each year for buying new clothes. This is a disability compensation \u2026 U4: Thanks. Maybe later. I need to ask my dad. This is for him. U5 : Btw, how to apply for such allowance if he is eligible.A6: He will need to file a claim for disability compensation (LINK).A3: Would you like to know if you are eligible?SP1If your clothing has been damaged, SP2 you may be able to get money each year to help you buy new clothes. This is a disability compensation \u2026clothing allowance.You'll need to file a claim for disability compensation.A17: No relevant info is found unfortunately. \u2026 A0: How may I help you? U7: Like how much would he get? A8: $830.56 per year. It is the current clothing allowance rate. \u2026 Figure 1: Sample segments of conversations (D1, D2 and D3) with various dialogue scenes that are grounded in a webpage (middle) from va.gov. The relevant content elements, such as hierarchical headers, list-items and spans, are highlighted. A / U indicates Agent / User role.About your eligibility \nT2 \n\nT1 \n\n\u2022 A one-time payment, or \n\u2022 A yearly payment \nThe current clothing allowance rate is $830.56. \n\nBoth of these must be true: \n \u00a7 m1 , and \n \u00a7 m2 \n\nHow to get these benefits \nT4 \n\nAbout the disability benefits \nT3 \n\n\u2026 \nU16: Will it affect me applying for loads for \neducation? \n\nirrelevant \n\nD2 \n\nD3 \n\nD1 \n\nm2 \n\nm1 \n\n\u2026 \n\n\n\n\n. Our dataset provides document contents in plain text and HTML, along with the meta information of titles and URLs. Each docu-Domain \n\n#Dials #Docs \n# per doc \ntk sp \np sec \nssa \n1192 \n109 \n795 70 17 \n5 \nva \n1330 \n138 \n818 70 20 \n9 \ndmv \n1305 \n149 \n944 77 18 \n10 \nstudentaid \n966 \n91 1007 75 20 \n9 \nall \n4793 \n487 \n888 73 18 \n8 \n\n\n\nTable 1 :\n1The breakdown count of the dialogues, documents and average number of content elements per document by domain.Role \nDA \n#Turns #Tokens/Turn \nuser \nrequest/query \n22220 \n12 \nuser \nrespond/yesOrNo \n8413 \n6 \nagent request/query \n7927 \n12 \nagent respond/reply \n23482 \n21 \ntotal \nall \n62042 \n14 \n\n\n\nTable 2 :\n2The total # of turns and the average # of tokens per turn, aggregated on dialogue act category.\n\nTable 6 :\n6Evaluation results for agent response genera-\ntion with varied context input. \n\n\n\n\nbased Information Retrieval method, and (2) A multi-class sequence classifier based on multiple choice example fromn \n\nBM-25 \nBERT \nR@1 R@5 R@10 R@1 R@5 R@10 \n1 \n33.1 \n54.3 \n61.2 \n40.4 \n65.4 \n73.7 \n2 \n57.4 \n80.0 \n84.9 \n57.8 \n81.5 \n86.2 \n3 \n58.8 \n80.0 \n85.4 \n61.0 \n82.1 \n86.8 \n4 \n65.4 \n85.7 \n89.9 \n63.6 \n84.9 \n89.1 \n5 \n67.7 \n86.6 \n90.6 \n66.3 \n87.3 \n91.5 \n\n\n\nTable 7 :\n7Evaluation results for document retrieval with earliest n turns as input on dev set.\n\nConceptualizing agent-human interactions during the conversational search process. Leif Azzopardi, Mateusz Dubiel, Martin Halvey, Jeffery Dalton, The Second International Workshop on Conversational Approaches to Information Retrieval. Leif Azzopardi, Mateusz Dubiel, Martin Halvey, and Jeffery Dalton. 2018. Conceptualizing agent-human interactions during the conversational search pro- cess. In The Second International Workshop on Con- versational Approaches to Information Retrieval.\n\nMultiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling. Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Milica Osman Ramadan, Gasic, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ra- madan, and Milica Gasic. 2018. Multiwoz-a large- scale multi-domain wizard-of-oz dataset for task- oriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016-5026.\n\nTaskmaster-1: Toward a realistic and diverse dialog dataset. Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young Kim, Andy Cedilnik, 10.18653/v1/D19-1459Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsBill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young Kim, and Andy Cedilnik. 2019. Taskmaster-1: To- ward a realistic and diverse dialog dataset. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 4516- 4525, Hong Kong, China. Association for Computa- tional Linguistics.\n\nDoQA -accessing domain-specific FAQs via conversational QA. Jon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deriu, Mark Cieliebak, Eneko Agirre, 10.18653/v1/2020.acl-main.652Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsJon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deriu, Mark Cieliebak, and Eneko Agirre. 2020. DoQA -accessing domain-specific FAQs via con- versational QA. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics, pages 7302-7314, Online. Association for Computational Linguistics.\n\nQuAC: Question answering in context. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer, 10.18653/v1/D18-1241Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen- tau Yih, Yejin Choi, Percy Liang, and Luke Zettle- moyer. 2018. QuAC: Question answering in con- text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174-2184, Brussels, Belgium. Association for Computational Linguistics.\n\nConstructing a lexicon of english discourse connectives. Debopam Das, Tatjana Scheffler, Peter Bourgonje, Manfred Stede, Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue. the 19th Annual SIGdial Meeting on Discourse and DialogueDebopam Das, Tatjana Scheffler, Peter Bourgonje, and Manfred Stede. 2018. Constructing a lexicon of en- glish discourse connectives. In Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dia- logue, pages 360-365.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.\n\nDoc2dial: a framework for dialogue composition grounded in documents. Song Feng, Kshitij Fadnis, Luis A Vera Liao, Lastras, Thirty-Fourth AAAI Conference on Artificial Intelligence. Song Feng, Kshitij Fadnis, Q Vera Liao, and Luis A Lastras. 2020. Doc2dial: a framework for dialogue composition grounded in documents. In Thirty- Fourth AAAI Conference on Artificial Intelligence.\n\nMeasuring annotator agreement in a complex hierarchical dialogue act annotation scheme. Jeroen Geertzen, Harry Bunt, Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue. the 7th SIGdial Workshop on Discourse and DialogueAssociation for Computational LinguisticsJeroen Geertzen and Harry Bunt. 2009. Measuring annotator agreement in a complex hierarchical dia- logue act annotation scheme. In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 126-133. Association for Computational Lin- guistics.\n\nLearning to search in long documents using document structure. Mor Geva, Jonathan Berant, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational LinguisticsMor Geva and Jonathan Berant. 2018. Learning to search in long documents using document structure. In Proceedings of the 27th International Confer- ence on Computational Linguistics, pages 161-176, Santa Fe, New Mexico, USA. Association for Com- putational Linguistics.\n\nExtending a parser to distant domains using a few dozen partially annotated examples. Vidur Joshi, Matthew Peters, Mark Hopkins, 10.18653/v1/P18-1110Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaLong Papers1Association for Computational LinguisticsVidur Joshi, Matthew Peters, and Mark Hopkins. 2018. Extending a parser to distant domains using a few dozen partially annotated examples. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1190-1199, Melbourne, Australia. Associa- tion for Computational Linguistics.\n\nBeyond domain apis: Task-oriented conversational modeling with unstructured knowledge access. Seokhwan Kim, Mihail Eric, Karthik Gopalakrishnan, Behnam Hedayatnia, Yang Liu, Dilek Hakkani-Tur, arXiv:2006.03533arXiv preprintSeokhwan Kim, Mihail Eric, Karthik Gopalakrishnan, Behnam Hedayatnia, Yang Liu, and Dilek Hakkani- Tur. 2020. Beyond domain apis: Task-oriented con- versational modeling with unstructured knowledge access. arXiv preprint arXiv:2006.03533.\n\ni like your shirt\"-dialogue acts for enabling social talk in conversational agents. Tina Kl\u00fcwer, International Workshop on Intelligent Virtual Agents. SpringerTina Kl\u00fcwer. 2011. \"i like your shirt\"-dialogue acts for enabling social talk in conversational agents. In In- ternational Workshop on Intelligent Virtual Agents, pages 14-27. Springer.\n\nBART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, abs/1910.13461CoRRMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. BART: denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. CoRR, abs/1910.13461.\n\nAutomatic discovery of semantic structures in html documents. Saikat Mukherjee, Guizhen Yang, Wenfang Tan, Seventh International Conference on Document Analysis and Recognition. IEEESaikat Mukherjee, Guizhen Yang, Wenfang Tan, and IV Ramakrishnan. 2003. Automatic discovery of se- mantic structures in html documents. In Seventh In- ternational Conference on Document Analysis and Recognition, 2003. Proceedings., pages 245-249. IEEE.\n\nDialog intent structure: A hierarchical schema of linked dialog acts. Silvia Pareti, Tatiana Lando, Proceedings of the Eleventh International Conference on Language Resources and Evaluation. the Eleventh International Conference on Language Resources and EvaluationLRECSilvia Pareti and Tatiana Lando. 2018. Dialog intent structure: A hierarchical schema of linked dialog acts. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018).\n\nIntroducing mantis: a novel multi-domain information seeking dialogues dataset. Gustavo Penha, Alexandru Balan, Claudia Hauff, arXiv:1912.04639arXiv preprintGustavo Penha, Alexandru Balan, and Claudia Hauff. 2019. Introducing mantis: a novel multi-domain in- formation seeking dialogues dataset. arXiv preprint arXiv:1912.04639.\n\nThe Penn discourse TreeBank 2.0. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, Bonnie Webber, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08). the Sixth International Conference on Language Resources and Evaluation (LREC'08)Marrakech, MoroccoEuropean Language Resources Association (ELRARashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt- sakaki, Livio Robaldo, Aravind Joshi, and Bon- nie Webber. 2008. The Penn discourse TreeBank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08), Marrakech, Morocco. European Lan- guage Resources Association (ELRA).\n\nRashmi Prasad, Bonnie Webber, Alan Lee, Aravind Joshi, Penn Discourse Treebank Version 3.0. In LDC2019T05. Philadelphia: Linguistic Data Consortium. Rashmi Prasad, Bonnie Webber, Alan Lee, and Ar- avind Joshi. 2019. Penn Discourse Treebank Ver- sion 3.0. In LDC2019T05. Philadelphia: Linguistic Data Consortium.\n\nKnow what you don't know: Unanswerable questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, 10.18653/v1/P18-2124Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaShort Papers2Association for Computational LinguisticsPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable ques- tions for SQuAD. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784- 789, Melbourne, Australia. Association for Compu- tational Linguistics.\n\nSQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n\nCoQA: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, 10.1162/tacl_a_00266Transactions of the Association for Computational Linguistics. 7Siva Reddy, Danqi Chen, and Christopher D. Manning. 2019. CoQA: A conversational question answering challenge. Transactions of the Association for Com- putational Linguistics, 7:249-266.\n\nThe probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, 10.1561/1500000019Found. Trends Inf. Retr. 34Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and be- yond. Found. Trends Inf. Retr., 3(4):333-389.\n\nInterpretation of natural language rules in conversational machine reading. Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rockt\u00e4schel, Mike Sheldon, Guillaume Bouchard, Sebastian Riedel, 10.18653/v1/D18-1233Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsMarzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rockt\u00e4schel, Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. 2018. Interpreta- tion of natural language rules in conversational ma- chine reading. In Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Processing, pages 2087-2097, Brussels, Belgium. Association for Computational Linguistics.\n\nConnective-lex: A web-based multilingual lexical resource for connectives. discours. Revue de linguistique, psycholinguistique et informatique. A journal of linguistics. M Stede, A Scheffler, Mendes, psycholinguistics and computational linguistics. 24M Stede, T Scheffler, and A Mendes. 2019. Connective-lex: A web-based multilingual lexical resource for connectives. discours. Revue de linguis- tique, psycholinguistique et informatique. A journal of linguistics, psycholinguistics and computational linguistics,(24).\n\nHuggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R&apos;emi Louf, Morgan Funtowicz, Jamie Brew, abs/1910.03771ArXiv. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R'emi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. Huggingface's trans- formers: State-of-the-art natural language process- ing. ArXiv, abs/1910.03771.\n\nSwag: A large-scale adversarial dataset for grounded commonsense inference. Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93-104.\n", "annotations": {"author": "[{\"end\":101,\"start\":64},{\"end\":136,\"start\":102},{\"end\":201,\"start\":137},{\"end\":253,\"start\":202},{\"end\":300,\"start\":254},{\"end\":330,\"start\":301},{\"end\":379,\"start\":331}]", "publisher": null, "author_last_name": "[{\"end\":73,\"start\":69},{\"end\":109,\"start\":106},{\"end\":155,\"start\":145},{\"end\":214,\"start\":207},{\"end\":269,\"start\":260},{\"end\":311,\"start\":307},{\"end\":340,\"start\":333}]", "author_first_name": "[{\"end\":68,\"start\":64},{\"end\":105,\"start\":102},{\"end\":144,\"start\":137},{\"end\":206,\"start\":202},{\"end\":259,\"start\":254},{\"end\":306,\"start\":301},{\"end\":332,\"start\":331}]", "author_affiliation": "[{\"end\":100,\"start\":84},{\"end\":135,\"start\":119},{\"end\":200,\"start\":184},{\"end\":252,\"start\":236},{\"end\":299,\"start\":283},{\"end\":329,\"start\":313},{\"end\":378,\"start\":362}]", "title": "[{\"end\":61,\"start\":1},{\"end\":440,\"start\":380}]", "venue": null, "abstract": "[{\"end\":1335,\"start\":442}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1556,\"start\":1536},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1602,\"start\":1582},{\"end\":1633,\"start\":1607},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1732,\"start\":1711},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3718,\"start\":3698},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4268,\"start\":4243},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4535,\"start\":4516},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4555,\"start\":4535},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6290,\"start\":6270},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7771,\"start\":7753},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8162,\"start\":8142},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8184,\"start\":8162},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8693,\"start\":8669},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8737,\"start\":8719},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10745,\"start\":10726},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11846,\"start\":11826},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12814,\"start\":12790},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13106,\"start\":13083},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13474,\"start\":13450},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13505,\"start\":13491},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14334,\"start\":14312},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19040,\"start\":19017},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19065,\"start\":19040},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19180,\"start\":19160},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19291,\"start\":19272},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25463,\"start\":25443},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27358,\"start\":27328},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27405,\"start\":27386},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27485,\"start\":27463},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28822,\"start\":28802},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28860,\"start\":28839},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29209,\"start\":29186},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29234,\"start\":29209},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29608,\"start\":29591},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29653,\"start\":29626},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30005,\"start\":29985},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30459,\"start\":30438}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31672,\"start\":31582},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31769,\"start\":31673},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33677,\"start\":31770},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34011,\"start\":33678},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34316,\"start\":34012},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34424,\"start\":34317},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":34517,\"start\":34425},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34875,\"start\":34518},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":34972,\"start\":34876}]", "paragraph": "[{\"end\":2234,\"start\":1351},{\"end\":2615,\"start\":2236},{\"end\":3245,\"start\":2617},{\"end\":4269,\"start\":3247},{\"end\":4683,\"start\":4271},{\"end\":5225,\"start\":4685},{\"end\":5390,\"start\":5227},{\"end\":5511,\"start\":5392},{\"end\":5630,\"start\":5513},{\"end\":6247,\"start\":5643},{\"end\":7411,\"start\":6249},{\"end\":7615,\"start\":7413},{\"end\":7826,\"start\":7635},{\"end\":7865,\"start\":7828},{\"end\":7948,\"start\":7867},{\"end\":8954,\"start\":7979},{\"end\":10083,\"start\":8956},{\"end\":10242,\"start\":10085},{\"end\":11093,\"start\":10266},{\"end\":11498,\"start\":11111},{\"end\":12269,\"start\":11519},{\"end\":13018,\"start\":12287},{\"end\":14079,\"start\":13020},{\"end\":15026,\"start\":14081},{\"end\":15102,\"start\":15035},{\"end\":15150,\"start\":15110},{\"end\":15162,\"start\":15152},{\"end\":15259,\"start\":15164},{\"end\":15498,\"start\":15261},{\"end\":16288,\"start\":15500},{\"end\":17113,\"start\":16311},{\"end\":17715,\"start\":17137},{\"end\":18117,\"start\":17748},{\"end\":18884,\"start\":18146},{\"end\":19522,\"start\":18906},{\"end\":20196,\"start\":19524},{\"end\":21075,\"start\":20198},{\"end\":21238,\"start\":21126},{\"end\":22049,\"start\":21261},{\"end\":22484,\"start\":22079},{\"end\":23129,\"start\":22524},{\"end\":23995,\"start\":23131},{\"end\":24417,\"start\":23997},{\"end\":24842,\"start\":24440},{\"end\":25354,\"start\":24872},{\"end\":26099,\"start\":25399},{\"end\":26866,\"start\":26101},{\"end\":27233,\"start\":26903},{\"end\":28298,\"start\":27235},{\"end\":28556,\"start\":28300},{\"end\":29569,\"start\":28573},{\"end\":30372,\"start\":29571},{\"end\":31002,\"start\":30374},{\"end\":31581,\"start\":31017}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":11373,\"start\":11366},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":11971,\"start\":11964},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":12597,\"start\":12590},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":13681,\"start\":13674},{\"end\":15175,\"start\":15168},{\"end\":15765,\"start\":15758},{\"end\":19385,\"start\":19378},{\"end\":21225,\"start\":21218},{\"end\":21237,\"start\":21230},{\"end\":21309,\"start\":21302},{\"end\":22048,\"start\":22041},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23469,\"start\":23394},{\"end\":23847,\"start\":23840},{\"end\":24488,\"start\":24481},{\"end\":24573,\"start\":24566},{\"end\":24725,\"start\":24718},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":26140,\"start\":26133},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":27996,\"start\":27989},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28319,\"start\":28312}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1349,\"start\":1337},{\"attributes\":{\"n\":\"2\"},\"end\":5641,\"start\":5633},{\"attributes\":{\"n\":\"2.1\"},\"end\":7633,\"start\":7618},{\"attributes\":{\"n\":\"2.1.1\"},\"end\":7977,\"start\":7951},{\"attributes\":{\"n\":\"2.1.2\"},\"end\":10264,\"start\":10245},{\"attributes\":{\"n\":\"2.2\"},\"end\":11109,\"start\":11096},{\"end\":11517,\"start\":11501},{\"attributes\":{\"n\":\"2.3\"},\"end\":12285,\"start\":12272},{\"end\":15033,\"start\":15029},{\"end\":15108,\"start\":15105},{\"attributes\":{\"n\":\"2.4\"},\"end\":16309,\"start\":16291},{\"attributes\":{\"n\":\"3\"},\"end\":17135,\"start\":17116},{\"attributes\":{\"n\":\"3.1\"},\"end\":17746,\"start\":17718},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":18144,\"start\":18120},{\"end\":18904,\"start\":18887},{\"end\":21124,\"start\":21078},{\"end\":21259,\"start\":21241},{\"attributes\":{\"n\":\"3.2\"},\"end\":22077,\"start\":22052},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":22522,\"start\":22487},{\"end\":24438,\"start\":24420},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":24870,\"start\":24845},{\"end\":25397,\"start\":25357},{\"attributes\":{\"n\":\"3.3\"},\"end\":26901,\"start\":26869},{\"attributes\":{\"n\":\"4\"},\"end\":28571,\"start\":28559},{\"attributes\":{\"n\":\"5\"},\"end\":31015,\"start\":31005},{\"end\":31593,\"start\":31583},{\"end\":31684,\"start\":31674},{\"end\":31775,\"start\":31771},{\"end\":34022,\"start\":34013},{\"end\":34327,\"start\":34318},{\"end\":34435,\"start\":34426},{\"end\":34886,\"start\":34877}]", "table": "[{\"end\":33677,\"start\":33324},{\"end\":34011,\"start\":33807},{\"end\":34316,\"start\":34134},{\"end\":34517,\"start\":34437},{\"end\":34875,\"start\":34635}]", "figure_caption": "[{\"end\":31672,\"start\":31595},{\"end\":31769,\"start\":31686},{\"end\":33324,\"start\":31778},{\"end\":33807,\"start\":33680},{\"end\":34134,\"start\":34024},{\"end\":34424,\"start\":34329},{\"end\":34635,\"start\":34520},{\"end\":34972,\"start\":34888}]", "figure_ref": "[{\"end\":2327,\"start\":2319},{\"end\":5840,\"start\":5832},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7793,\"start\":7785},{\"end\":8553,\"start\":8545},{\"end\":11785,\"start\":11777},{\"end\":12128,\"start\":12119},{\"end\":14025,\"start\":14017},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14457,\"start\":14449},{\"end\":16957,\"start\":16949},{\"end\":26451,\"start\":26443}]", "bib_author_first_name": "[{\"end\":35061,\"start\":35057},{\"end\":35080,\"start\":35073},{\"end\":35095,\"start\":35089},{\"end\":35111,\"start\":35104},{\"end\":35560,\"start\":35555},{\"end\":35586,\"start\":35575},{\"end\":35601,\"start\":35592},{\"end\":35614,\"start\":35609},{\"end\":35632,\"start\":35626},{\"end\":35646,\"start\":35640},{\"end\":36225,\"start\":36221},{\"end\":36240,\"start\":36233},{\"end\":36269,\"start\":36257},{\"end\":36284,\"start\":36278},{\"end\":36301,\"start\":36298},{\"end\":36318,\"start\":36312},{\"end\":36335,\"start\":36330},{\"end\":36347,\"start\":36343},{\"end\":36364,\"start\":36355},{\"end\":36374,\"start\":36370},{\"end\":37363,\"start\":37360},{\"end\":37385,\"start\":37378},{\"end\":37398,\"start\":37393},{\"end\":37409,\"start\":37406},{\"end\":37421,\"start\":37417},{\"end\":37438,\"start\":37433},{\"end\":38051,\"start\":38045},{\"end\":38060,\"start\":38058},{\"end\":38070,\"start\":38065},{\"end\":38082,\"start\":38078},{\"end\":38098,\"start\":38092},{\"end\":38109,\"start\":38104},{\"end\":38121,\"start\":38116},{\"end\":38133,\"start\":38129},{\"end\":38774,\"start\":38767},{\"end\":38787,\"start\":38780},{\"end\":38804,\"start\":38799},{\"end\":38823,\"start\":38816},{\"end\":39277,\"start\":39272},{\"end\":39294,\"start\":39286},{\"end\":39308,\"start\":39302},{\"end\":39322,\"start\":39314},{\"end\":40208,\"start\":40204},{\"end\":40222,\"start\":40215},{\"end\":40235,\"start\":40231},{\"end\":40237,\"start\":40236},{\"end\":40609,\"start\":40603},{\"end\":40625,\"start\":40620},{\"end\":41115,\"start\":41112},{\"end\":41130,\"start\":41122},{\"end\":41708,\"start\":41703},{\"end\":41723,\"start\":41716},{\"end\":41736,\"start\":41732},{\"end\":42442,\"start\":42434},{\"end\":42454,\"start\":42448},{\"end\":42468,\"start\":42461},{\"end\":42491,\"start\":42485},{\"end\":42508,\"start\":42504},{\"end\":42519,\"start\":42514},{\"end\":42891,\"start\":42887},{\"end\":43267,\"start\":43263},{\"end\":43281,\"start\":43275},{\"end\":43292,\"start\":43287},{\"end\":43326,\"start\":43322},{\"end\":43340,\"start\":43333},{\"end\":43355,\"start\":43351},{\"end\":43733,\"start\":43727},{\"end\":43752,\"start\":43745},{\"end\":43766,\"start\":43759},{\"end\":44177,\"start\":44171},{\"end\":44193,\"start\":44186},{\"end\":44673,\"start\":44666},{\"end\":44690,\"start\":44681},{\"end\":44705,\"start\":44698},{\"end\":44955,\"start\":44949},{\"end\":44970,\"start\":44964},{\"end\":44983,\"start\":44979},{\"end\":44994,\"start\":44989},{\"end\":45012,\"start\":45007},{\"end\":45029,\"start\":45022},{\"end\":45043,\"start\":45037},{\"end\":45622,\"start\":45616},{\"end\":45637,\"start\":45631},{\"end\":45650,\"start\":45646},{\"end\":45663,\"start\":45656},{\"end\":45995,\"start\":45989},{\"end\":46012,\"start\":46007},{\"end\":46023,\"start\":46018},{\"end\":46670,\"start\":46664},{\"end\":46686,\"start\":46682},{\"end\":46704,\"start\":46694},{\"end\":46719,\"start\":46714},{\"end\":47320,\"start\":47316},{\"end\":47333,\"start\":47328},{\"end\":47351,\"start\":47340},{\"end\":47353,\"start\":47352},{\"end\":47698,\"start\":47691},{\"end\":47714,\"start\":47710},{\"end\":47995,\"start\":47988},{\"end\":48007,\"start\":48004},{\"end\":48024,\"start\":48017},{\"end\":48038,\"start\":48032},{\"end\":48049,\"start\":48046},{\"end\":48067,\"start\":48063},{\"end\":48086,\"start\":48077},{\"end\":48106,\"start\":48097},{\"end\":48913,\"start\":48912},{\"end\":48922,\"start\":48921},{\"end\":49342,\"start\":49336},{\"end\":49357,\"start\":49349},{\"end\":49371,\"start\":49365},{\"end\":49384,\"start\":49378},{\"end\":49402,\"start\":49395},{\"end\":49420,\"start\":49413},{\"end\":49433,\"start\":49426},{\"end\":49445,\"start\":49442},{\"end\":49463,\"start\":49453},{\"end\":49476,\"start\":49470},{\"end\":49493,\"start\":49488},{\"end\":49877,\"start\":49872},{\"end\":49894,\"start\":49887},{\"end\":49904,\"start\":49901},{\"end\":49920,\"start\":49915}]", "bib_author_last_name": "[{\"end\":35071,\"start\":35062},{\"end\":35087,\"start\":35081},{\"end\":35102,\"start\":35096},{\"end\":35118,\"start\":35112},{\"end\":35573,\"start\":35561},{\"end\":35590,\"start\":35587},{\"end\":35607,\"start\":35602},{\"end\":35624,\"start\":35615},{\"end\":35638,\"start\":35633},{\"end\":35660,\"start\":35647},{\"end\":35667,\"start\":35662},{\"end\":36231,\"start\":36226},{\"end\":36255,\"start\":36241},{\"end\":36276,\"start\":36270},{\"end\":36296,\"start\":36285},{\"end\":36310,\"start\":36302},{\"end\":36328,\"start\":36319},{\"end\":36341,\"start\":36336},{\"end\":36353,\"start\":36348},{\"end\":36368,\"start\":36365},{\"end\":36383,\"start\":36375},{\"end\":37376,\"start\":37364},{\"end\":37391,\"start\":37386},{\"end\":37404,\"start\":37399},{\"end\":37415,\"start\":37410},{\"end\":37431,\"start\":37422},{\"end\":37445,\"start\":37439},{\"end\":38056,\"start\":38052},{\"end\":38063,\"start\":38061},{\"end\":38076,\"start\":38071},{\"end\":38090,\"start\":38083},{\"end\":38102,\"start\":38099},{\"end\":38114,\"start\":38110},{\"end\":38127,\"start\":38122},{\"end\":38145,\"start\":38134},{\"end\":38778,\"start\":38775},{\"end\":38797,\"start\":38788},{\"end\":38814,\"start\":38805},{\"end\":38829,\"start\":38824},{\"end\":39284,\"start\":39278},{\"end\":39300,\"start\":39295},{\"end\":39312,\"start\":39309},{\"end\":39332,\"start\":39323},{\"end\":40213,\"start\":40209},{\"end\":40229,\"start\":40223},{\"end\":40247,\"start\":40238},{\"end\":40256,\"start\":40249},{\"end\":40618,\"start\":40610},{\"end\":40630,\"start\":40626},{\"end\":41120,\"start\":41116},{\"end\":41137,\"start\":41131},{\"end\":41714,\"start\":41709},{\"end\":41730,\"start\":41724},{\"end\":41744,\"start\":41737},{\"end\":42446,\"start\":42443},{\"end\":42459,\"start\":42455},{\"end\":42483,\"start\":42469},{\"end\":42502,\"start\":42492},{\"end\":42512,\"start\":42509},{\"end\":42531,\"start\":42520},{\"end\":42898,\"start\":42892},{\"end\":43273,\"start\":43268},{\"end\":43285,\"start\":43282},{\"end\":43320,\"start\":43293},{\"end\":43331,\"start\":43327},{\"end\":43349,\"start\":43341},{\"end\":43367,\"start\":43356},{\"end\":43743,\"start\":43734},{\"end\":43757,\"start\":43753},{\"end\":43770,\"start\":43767},{\"end\":44184,\"start\":44178},{\"end\":44199,\"start\":44194},{\"end\":44679,\"start\":44674},{\"end\":44696,\"start\":44691},{\"end\":44711,\"start\":44706},{\"end\":44962,\"start\":44956},{\"end\":44977,\"start\":44971},{\"end\":44987,\"start\":44984},{\"end\":45005,\"start\":44995},{\"end\":45020,\"start\":45013},{\"end\":45035,\"start\":45030},{\"end\":45050,\"start\":45044},{\"end\":45629,\"start\":45623},{\"end\":45644,\"start\":45638},{\"end\":45654,\"start\":45651},{\"end\":45669,\"start\":45664},{\"end\":46005,\"start\":45996},{\"end\":46016,\"start\":46013},{\"end\":46029,\"start\":46024},{\"end\":46680,\"start\":46671},{\"end\":46692,\"start\":46687},{\"end\":46712,\"start\":46705},{\"end\":46725,\"start\":46720},{\"end\":47326,\"start\":47321},{\"end\":47338,\"start\":47334},{\"end\":47361,\"start\":47354},{\"end\":47708,\"start\":47699},{\"end\":47723,\"start\":47715},{\"end\":48002,\"start\":47996},{\"end\":48015,\"start\":48008},{\"end\":48030,\"start\":48025},{\"end\":48044,\"start\":48039},{\"end\":48061,\"start\":48050},{\"end\":48075,\"start\":48068},{\"end\":48095,\"start\":48087},{\"end\":48113,\"start\":48107},{\"end\":48919,\"start\":48914},{\"end\":48932,\"start\":48923},{\"end\":48940,\"start\":48934},{\"end\":49347,\"start\":49343},{\"end\":49363,\"start\":49358},{\"end\":49376,\"start\":49372},{\"end\":49393,\"start\":49385},{\"end\":49411,\"start\":49403},{\"end\":49424,\"start\":49421},{\"end\":49440,\"start\":49434},{\"end\":49451,\"start\":49446},{\"end\":49468,\"start\":49464},{\"end\":49486,\"start\":49477},{\"end\":49498,\"start\":49494},{\"end\":49885,\"start\":49878},{\"end\":49899,\"start\":49895},{\"end\":49913,\"start\":49905},{\"end\":49925,\"start\":49921}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":65257907},\"end\":35460,\"start\":34974},{\"attributes\":{\"id\":\"b1\"},\"end\":36158,\"start\":35462},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1459\",\"id\":\"b2\",\"matched_paper_id\":202565569},\"end\":37298,\"start\":36160},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.652\",\"id\":\"b3\",\"matched_paper_id\":218487043},\"end\":38006,\"start\":37300},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1241\",\"id\":\"b4\",\"matched_paper_id\":52057510},\"end\":38708,\"start\":38008},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51918672},\"end\":39188,\"start\":38710},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b6\",\"matched_paper_id\":52967399},\"end\":40132,\"start\":39190},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":219182202},\"end\":40513,\"start\":40134},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2574778},\"end\":41047,\"start\":40515},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":47017117},\"end\":41615,\"start\":41049},{\"attributes\":{\"doi\":\"10.18653/v1/P18-1110\",\"id\":\"b10\",\"matched_paper_id\":21712653},\"end\":42338,\"start\":41617},{\"attributes\":{\"doi\":\"arXiv:2006.03533\",\"id\":\"b11\"},\"end\":42801,\"start\":42340},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6255730},\"end\":43147,\"start\":42803},{\"attributes\":{\"doi\":\"abs/1910.13461\",\"id\":\"b13\"},\"end\":43663,\"start\":43149},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2277027},\"end\":44099,\"start\":43665},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":21699870},\"end\":44584,\"start\":44101},{\"attributes\":{\"doi\":\"arXiv:1912.04639\",\"id\":\"b16\"},\"end\":44914,\"start\":44586},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":13374927},\"end\":45614,\"start\":44916},{\"attributes\":{\"id\":\"b18\"},\"end\":45927,\"start\":45616},{\"attributes\":{\"doi\":\"10.18653/v1/P18-2124\",\"id\":\"b19\",\"matched_paper_id\":47018994},\"end\":46601,\"start\":45929},{\"attributes\":{\"doi\":\"10.18653/v1/D16-1264\",\"id\":\"b20\",\"matched_paper_id\":11816014},\"end\":47261,\"start\":46603},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00266\",\"id\":\"b21\",\"matched_paper_id\":52055325},\"end\":47633,\"start\":47263},{\"attributes\":{\"doi\":\"10.1561/1500000019\",\"id\":\"b22\",\"matched_paper_id\":207178704},\"end\":47910,\"start\":47635},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1233\",\"id\":\"b23\",\"matched_paper_id\":52165754},\"end\":48740,\"start\":47912},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":209057427},\"end\":49260,\"start\":48742},{\"attributes\":{\"doi\":\"abs/1910.03771\",\"id\":\"b25\",\"matched_paper_id\":204509627},\"end\":49794,\"start\":49262},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":52019251},\"end\":50333,\"start\":49796}]", "bib_title": "[{\"end\":35055,\"start\":34974},{\"end\":35553,\"start\":35462},{\"end\":36219,\"start\":36160},{\"end\":37358,\"start\":37300},{\"end\":38043,\"start\":38008},{\"end\":38765,\"start\":38710},{\"end\":39270,\"start\":39190},{\"end\":40202,\"start\":40134},{\"end\":40601,\"start\":40515},{\"end\":41110,\"start\":41049},{\"end\":41701,\"start\":41617},{\"end\":42885,\"start\":42803},{\"end\":43725,\"start\":43665},{\"end\":44169,\"start\":44101},{\"end\":44947,\"start\":44916},{\"end\":45987,\"start\":45929},{\"end\":46662,\"start\":46603},{\"end\":47314,\"start\":47263},{\"end\":47689,\"start\":47635},{\"end\":47986,\"start\":47912},{\"end\":48910,\"start\":48742},{\"end\":49334,\"start\":49262},{\"end\":49870,\"start\":49796}]", "bib_author": "[{\"end\":35073,\"start\":35057},{\"end\":35089,\"start\":35073},{\"end\":35104,\"start\":35089},{\"end\":35120,\"start\":35104},{\"end\":35575,\"start\":35555},{\"end\":35592,\"start\":35575},{\"end\":35609,\"start\":35592},{\"end\":35626,\"start\":35609},{\"end\":35640,\"start\":35626},{\"end\":35662,\"start\":35640},{\"end\":35669,\"start\":35662},{\"end\":36233,\"start\":36221},{\"end\":36257,\"start\":36233},{\"end\":36278,\"start\":36257},{\"end\":36298,\"start\":36278},{\"end\":36312,\"start\":36298},{\"end\":36330,\"start\":36312},{\"end\":36343,\"start\":36330},{\"end\":36355,\"start\":36343},{\"end\":36370,\"start\":36355},{\"end\":36385,\"start\":36370},{\"end\":37378,\"start\":37360},{\"end\":37393,\"start\":37378},{\"end\":37406,\"start\":37393},{\"end\":37417,\"start\":37406},{\"end\":37433,\"start\":37417},{\"end\":37447,\"start\":37433},{\"end\":38058,\"start\":38045},{\"end\":38065,\"start\":38058},{\"end\":38078,\"start\":38065},{\"end\":38092,\"start\":38078},{\"end\":38104,\"start\":38092},{\"end\":38116,\"start\":38104},{\"end\":38129,\"start\":38116},{\"end\":38147,\"start\":38129},{\"end\":38780,\"start\":38767},{\"end\":38799,\"start\":38780},{\"end\":38816,\"start\":38799},{\"end\":38831,\"start\":38816},{\"end\":39286,\"start\":39272},{\"end\":39302,\"start\":39286},{\"end\":39314,\"start\":39302},{\"end\":39334,\"start\":39314},{\"end\":40215,\"start\":40204},{\"end\":40231,\"start\":40215},{\"end\":40249,\"start\":40231},{\"end\":40258,\"start\":40249},{\"end\":40620,\"start\":40603},{\"end\":40632,\"start\":40620},{\"end\":41122,\"start\":41112},{\"end\":41139,\"start\":41122},{\"end\":41716,\"start\":41703},{\"end\":41732,\"start\":41716},{\"end\":41746,\"start\":41732},{\"end\":42448,\"start\":42434},{\"end\":42461,\"start\":42448},{\"end\":42485,\"start\":42461},{\"end\":42504,\"start\":42485},{\"end\":42514,\"start\":42504},{\"end\":42533,\"start\":42514},{\"end\":42900,\"start\":42887},{\"end\":43275,\"start\":43263},{\"end\":43287,\"start\":43275},{\"end\":43322,\"start\":43287},{\"end\":43333,\"start\":43322},{\"end\":43351,\"start\":43333},{\"end\":43369,\"start\":43351},{\"end\":43745,\"start\":43727},{\"end\":43759,\"start\":43745},{\"end\":43772,\"start\":43759},{\"end\":44186,\"start\":44171},{\"end\":44201,\"start\":44186},{\"end\":44681,\"start\":44666},{\"end\":44698,\"start\":44681},{\"end\":44713,\"start\":44698},{\"end\":44964,\"start\":44949},{\"end\":44979,\"start\":44964},{\"end\":44989,\"start\":44979},{\"end\":45007,\"start\":44989},{\"end\":45022,\"start\":45007},{\"end\":45037,\"start\":45022},{\"end\":45052,\"start\":45037},{\"end\":45631,\"start\":45616},{\"end\":45646,\"start\":45631},{\"end\":45656,\"start\":45646},{\"end\":45671,\"start\":45656},{\"end\":46007,\"start\":45989},{\"end\":46018,\"start\":46007},{\"end\":46031,\"start\":46018},{\"end\":46682,\"start\":46664},{\"end\":46694,\"start\":46682},{\"end\":46714,\"start\":46694},{\"end\":46727,\"start\":46714},{\"end\":47328,\"start\":47316},{\"end\":47340,\"start\":47328},{\"end\":47363,\"start\":47340},{\"end\":47710,\"start\":47691},{\"end\":47725,\"start\":47710},{\"end\":48004,\"start\":47988},{\"end\":48017,\"start\":48004},{\"end\":48032,\"start\":48017},{\"end\":48046,\"start\":48032},{\"end\":48063,\"start\":48046},{\"end\":48077,\"start\":48063},{\"end\":48097,\"start\":48077},{\"end\":48115,\"start\":48097},{\"end\":48921,\"start\":48912},{\"end\":48934,\"start\":48921},{\"end\":48942,\"start\":48934},{\"end\":49349,\"start\":49336},{\"end\":49365,\"start\":49349},{\"end\":49378,\"start\":49365},{\"end\":49395,\"start\":49378},{\"end\":49413,\"start\":49395},{\"end\":49426,\"start\":49413},{\"end\":49442,\"start\":49426},{\"end\":49453,\"start\":49442},{\"end\":49470,\"start\":49453},{\"end\":49488,\"start\":49470},{\"end\":49500,\"start\":49488},{\"end\":49887,\"start\":49872},{\"end\":49901,\"start\":49887},{\"end\":49915,\"start\":49901},{\"end\":49927,\"start\":49915}]", "bib_venue": "[{\"end\":35828,\"start\":35757},{\"end\":36758,\"start\":36582},{\"end\":37637,\"start\":37565},{\"end\":38343,\"start\":38255},{\"end\":38962,\"start\":38905},{\"end\":39647,\"start\":39498},{\"end\":40749,\"start\":40699},{\"end\":41305,\"start\":41218},{\"end\":41947,\"start\":41855},{\"end\":44366,\"start\":44292},{\"end\":45249,\"start\":45150},{\"end\":46232,\"start\":46140},{\"end\":46919,\"start\":46835},{\"end\":48311,\"start\":48223},{\"end\":50086,\"start\":50015},{\"end\":35207,\"start\":35120},{\"end\":35755,\"start\":35669},{\"end\":36580,\"start\":36405},{\"end\":37563,\"start\":37476},{\"end\":38253,\"start\":38167},{\"end\":38903,\"start\":38831},{\"end\":39496,\"start\":39354},{\"end\":40314,\"start\":40258},{\"end\":40697,\"start\":40632},{\"end\":41216,\"start\":41139},{\"end\":41853,\"start\":41766},{\"end\":42432,\"start\":42340},{\"end\":42952,\"start\":42900},{\"end\":43261,\"start\":43149},{\"end\":43841,\"start\":43772},{\"end\":44290,\"start\":44201},{\"end\":44664,\"start\":44586},{\"end\":45148,\"start\":45052},{\"end\":45763,\"start\":45671},{\"end\":46138,\"start\":46051},{\"end\":46833,\"start\":46747},{\"end\":47444,\"start\":47383},{\"end\":47766,\"start\":47743},{\"end\":48221,\"start\":48135},{\"end\":48989,\"start\":48942},{\"end\":49519,\"start\":49514},{\"end\":50013,\"start\":49927}]"}}}, "year": 2023, "month": 12, "day": 17}