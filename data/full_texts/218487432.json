{"id": 218487432, "updated": "2023-11-08 00:06:51.458", "metadata": {"title": "An Imitation Game for Learning Semantic Parsers from User Interaction", "authors": "[{\"first\":\"Ziyu\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Yiqi\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Wen-tau\",\"last\":\"Yih\",\"middle\":[]},{\"first\":\"Huan\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Su\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "publication_date": {"year": 2020, "month": 5, "day": 2}, "abstract": "Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users. A semantic parser should be introspective of its uncertainties and prompt for user demonstration when uncertain. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions. To combat the sparsity of demonstration, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and re-trains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011). We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.00689", "mag": "3102841213", "acl": "2020.emnlp-main.559", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/YaoTYSS20", "doi": "10.18653/v1/2020.emnlp-main.559"}}, "content": {"source": {"pdf_hash": "65dd4ff4074812ef4123ecfca9609ef3db87f9de", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2005.00689v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.emnlp-main.559.pdf", "status": "HYBRID"}}, "grobid": {"id": "9ffdee97ab2d5cc5e89e5873cc94b1dfa1145cfe", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/65dd4ff4074812ef4123ecfca9609ef3db87f9de.txt", "contents": "\nAn Imitation Game for Learning Semantic Parsers from User Interaction\n\n\nZiyu Yao \nThe Ohio State University\n\n\nYiqi Tang \nThe Ohio State University\n\n\nWen-Tau Yih \nFacebook AI Research\nSeattle\n\nHuan Sun \nThe Ohio State University\n\n\nYu Su \nThe Ohio State University\n\n\nAn Imitation Game for Learning Semantic Parsers from User Interaction\n\nDespite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, humanin-the-loop methodology for learning semantic parsers directly from users. A semantic parser should be introspective of its uncertainties and prompt for user demonstration when uncertain. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions. To combat the sparsity of demonstration, we propose a novel annotationefficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and re-trains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011). We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem.\n\nIntroduction\n\nSemantic parsing has found tremendous applications in building natural language interfaces that allow users to query data and invoke services without programming (Woods, 1973;Zettlemoyer and Collins, 2005;Berant et al., 2013;Su et al., 2017;Yu et al., 2018). The lifecycle of a semantic parser typically consists of two stages: (1) bootstraping, where we keep collecting labeled data via trained annotators and/or crowdsourcing for model training until it reaches commercial-grade performance (e.g., 95% accuracy on a surrogate test set), and (2) fine-tuning, where we deploy the system, analyze the usage, and collect and annotate new data to address the identified problems or emerging needs. Figure 1: A semantic parser proactively interacts with the user in a friendly way to resolve its uncertainties. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions.\n\nHowever, it poses several challenges for scaling up or building semantic parsers for new domains: (1) high boostrapping cost because mainstream neural parsing models are data-hungry and annotation cost of semantic parsing data is relatively high, (2) high fine-tuning cost from continuously analyzing usage and annotating new data, and (3) privacy risks arising from exposing private user conversations to annotators and developers (Lomas, 2019).\n\nIn this paper, we suggest an alternative methodology for building semantic parsers that could potentially address all the aforementioned problems.\n\nThe key is to involve human users in the learning loop. A semantic parser should be introspective of its uncertainties and proactively prompt for demonstration from the user, who knows the question best, to resolve them. In doing so, the semantic parser would be able to accumulate targeted training data and continue improving itself autonomously without involving any annotators or developers, hence also minimizing privacy risks. The bootstrapping cost could also be significantly reduced because an interactive system needs not to be almost perfectly accurate to be deployed. On the other hand, such interaction opens up the black box and allows users to know more about the reasoning underneath the system and better interpret the final results (Su et al., 2018). A human-in-the-loop methodology like this also opens the door for domain adaptation and personalization.\n\nThis work builds on the recent line of research on interactive semantic parsing (Li and Jagadish, 2014;Chaurasia and Mooney, 2017;Gur et al., 2018;Yao et al., 2019b). Specifically, Yao et al. (2019b) provide a general framework, MISP (Model-based Interactive Semantic Parsing), which handles uncertainty modeling and natural language generation. We will leverage MISP for user interaction to prove the feasibility of the envisioned methodology. However, existing studies only focus on interacting with users to resolve uncertainties. None of them has answered the crucial question of how to learn from user interaction, which is the technical focus of this study.\n\nOne form of user interaction explored for learning semantic parsers is asking users to validate the execution results (Clarke et al., 2010;Iyer et al., 2017). While appealing, in practice it may be a difficult task for real users because they would not need to ask the question if they knew the answer in the first place. We instead aim to learn semantic parsers from fine-grained interaction where users only need to answer simple questions covered by their background knowledge ( Figure 1). However, learning signals from such fine-grained interactions are bound to be sparse because the system needs to avoid asking too many questions and overwhelming the user, which poses a challenge for learning.\n\nTo this end, we propose a novel annotationefficient imitation learning algorithm for learning semantic parsers from such sparse, fine-grained demonstration: The agent (semantic parser) only requests for demonstration when it is uncertain about a state (parsing step). For the certain/confident states, the actions chosen by the current policy are deemed as correct. The policy is updated iteratively in a Dataset Aggregation fashion (Ross et al., 2011). At each iteration, all the state-action pairs, demon-strated or confident, are included to form a new training set and train a new policy in a supervised way. Intuitively, using confident predictions for training mitigates the sparsity issue, but it may also introduce noise. We provide a theoretical analysis of the proposed algorithm and show that, under mild assumptions, the quality of the final policy is mainly determined by the quality of the initial policy and confidence estimation accuracy.\n\nUsing simulated users, we also empirically compare our method with a number of baselines on the text-to-SQL parsing problem, including the powerful but costly baseline of full expert annotation. On the WikiSQL (Zhong et al., 2017) dataset, compared with the full annotation baseline, we show that, when bootstrapped using only 10% of the training data, our method can achieve almost the same test accuracy (2% absolute loss) while using less than 10% of the annotations, without even taking into account the different unit cost of annotation from users vs. domain experts. We also show that the quality of the final policy is largely determined by the quality of the initial policy, further confirming the theoretical analysis. Finally, we demonstrate that the system can generalize to more complicated semantic parsing tasks such as Spider (Yu et al., 2018).\n\n\nRelated Work\n\nInteractive Semantic Parsing. Our work extends interactive semantic parsing, a recent idea that leverages system-user interactions to improve semantic parsing on the fly (Li and Jagadish, 2014;He et al., 2016;Chaurasia and Mooney, 2017;Su et al., 2018;Gur et al., 2018;Yao et al., 2019a,b). As an example, Gur et al. (2018) built a neural model to identify and correct error spans in a generated SQL query via dialogues. Yao et al. (2019b) further generalized the interaction framework by formalizing a model-based intelligent agent called MISP. Our system leverages MISP to support interactivity but focuses on developing an algorithm for continually improving the base parser from end user interactions, which has not been accomplished by previous work.\n\n\nFeedback-based Interactive Learning.\n\nLearning interactively from user feedback has been studied for machine translation (Nguyen et al., 2017;Petrushkov et al., 2018;Kreutzer and Riezler, 2019) and other NLP tasks (Sokolov et al., 2016;Gao et al., 2018;Hancock et al., 2019). Most relevant to this work, Hancock et al. (2019) constructed a chatbot that learns to request feedback when the user is unsatisfied with the system response, and then further improves itself periodically from the satisfied responses and feedback responses. The work reaffirms the necessity of human-in-the-loop autonomous learning systems like ours.\n\nIn the field of semantic parsing, Clarke et al. (2010) andIyer et al. (2017) learned semantic parsers from user validation on the query execution results. However, often times it may not very practical to expect end users able to validate answer correctness (e.g., consider validating an answer \"103\" for the question \"how many students have a GPA higher than 3.5\" from a massive table). Active learning is also leveraged to selectively obtain gold labels for semantic parsing and save human annotations (Duong et al., 2018;Ni et al., 2020). Our work is complementary to this line of research as we focus on learning interactively from end users (not \"teachers\").\n\nImitation Learning. Traditional imitation learning algorithms (Daum\u00e9 et al., 2009;Ross and Bagnell, 2010;Ross et al., 2011;Ross and Bagnell, 2014) iteratively execute and train a policy by collecting expert demonstrations for every policy decision. Despite its efficacy, the learning demands costly annotations from experts. In contrast, we save expert effort by selectively requesting for demonstrations. This idea is related to active imitation learning (Chernova and Veloso, 2009;Kim and Pineau, 2013;Judah et al., 2014;Zhang and Cho, 2017). For example, Judah et al. (2014) used active learning to select informative trajectories from the unlabeled data pool for expert demonstrations. However, their setting assumes a \"teacher\" to intentionally provide labels and an unlabeled data pool, while our algorithm targets at end users who are using the system. Similar to our approach, Chernova and Veloso (2009) solicited expert demonstrations only for uncertain states. However, their algorithm simply abandons policy actions that are confident, leading to sparse training data. Instead, our algorithm utilizes confident policy actions to combat the sparsity issue and is additionally provided with a theoretical analysis.\n\n\nPreliminaries\n\nFormally, we assume the semantic parsing model generates a semantic parse by executing a sequence of actions a t (parsing decisions) at each time step t. In practice, the definition of action depends on the specific semantic parsing model, as we will illustrate shortly. A state s t is then defined as a tuple of (q, a 1:t\u22121 ), where q is the initial natural language question and a 1:t\u22121 = (a 1 , ..., a t\u22121 ) is the current partial parse. In particular, the initial state s 1 = (q, \u03c6) contains only the question. Denote a semantic parser as\u03c0, which is a policy function (Sutton and Barto, 2018) that takes a state s t as input and outputs a probability distribution over the action space. The semantic parsing process can be formulated as sampling a trajectory \u03c4 by alternately observing a state and sampling an action from the policy, i.e., \u03c4 = (s 1 , a 1 \u223c\u03c0(s 1 ), ..., s T , a T \u223c\u03c0(s T )), assuming a trajectory length T . The probability of the generated semantic parse becomes:\n\np\u03c0(a 1:T |s 1 ) = T t=1 p\u03c0(a t |s t ).\n\nAn interactive semantic parser typically follows the aforementioned definition and requests the user's validation of a specific action a t . Based on the feedback, a correct action a * t can be inferred to replace the original a t . The parsing process continues with a * t afterwards. In this work, we adopt MISP (Yao et al., 2019b) as the back-end interactive semantic parsing framework, which enables system-user interaction via a policy probability based uncertainty estimator, a grammar-based natural language generator, and a multi-choice questionanswering interaction design, as shown in Figure 1.\n\nExample. Consider the SQLova parser (Hwang et al., 2019), which generates a query by filling \"slots\" in a pre-defined SQL sketch \"SELECT Agg SCol WHERE WCol OP VAL\". To complete the SQL query in Figure 1, it first takes three steps:\n\nSCol=\"School/Club Team\" (a 1 ), Agg=\"COUNT\" (a 2 ) and WCol=\"School/Club Team\" (a 3 ). MISP detects that a 3 is uncertain because its probability is lower than a pre-specified threshold. It validates a 3 with the user and corrects it with WCol=\"Player\" (a * 3 ). The parsing continues with OP=\"=\" (a 4 ) and VAL=\"jalen rose\" (a 5 ). The trajectory length T = 5 in this case.\n\n\nLearning Semantic Parsers from User Interaction\n\nIn this section, we present an imitation learning algorithm for learning semantic parsers from user interactions. The algorithm is annotation-efficient and can train a parser without requiring a large amount of user feedback (or \"annotations\"), an important property for practical use in an end-userfacing system. Note that while we apply this algorithm to semantic parsing in this work, in principle the algorithm can be applied to other structured prediction tasks (e.g., text summarization or machine translation) as well.\n\n\nAn Imitation Learning Formulation\n\nUnder the interactive semantic parsing framework, a learning algorithm intuitively can aggregate (s t , a * t ) pairs collected from user interactions and trains the parser to enforce a * t under the state s t = (q, a 1:t\u22121 ). However, this is not achievable by conventional supervised learning since the training needs to be conducted in an interactive environment, where the partial parse a 1:t\u22121 is generated by the parser itself.\n\nInstead, we formulate it as an imitation learning problem (Daum\u00e9 et al., 2009;Ross and Bagnell, 2010;Ross et al., 2011;Ross and Bagnell, 2014). Consider the user as a demonstrator, then the derived action a * t can be viewed as an expert demonstration which is interactively sampled from the demonstrator's policy (or expert policy) \u03c0 * , 2 i.e., a * t \u223c \u03c0 * (s t ). The goal of our algorithm is thus to train policy\u03c0 to imitate the expert policy \u03c0 * . A general procedure is described in Algorithm 1, wher\u00ea \u03c0 is learned iteratively for every m user questions (Line 1-9).\n\n\nAnnotation-efficient Imitation Learning\n\nConsider parsing a user question and collecting training data using the parser\u03c0 i in the i-th iteration (Line 5). A standard imitation learning algorithm such as DAGGER (Ross et al., 2011) usually requests expert demonstration a * t for every state s t in the sampled trajectory. However, it requires a considerable amount of user annotations, which may not be practical when interacting with end users.\n\nWe propose an annotation-efficient imitation learning algorithm, which saves user annotations by selectively requesting user intervention, as shown in function PARSE&COLLECT. Specifically, in each parsing step, the system first previews whether it is confident about its own decision a t (Line 13-14), which is determined when its probability is \nD i \u2190 m j=1 PARSE&COLLECT(\u00b5, q j ,\u03c0 i , \u03c0 * ); 6: Aggregate dataset D \u2190 D D i ; 7:\nTrain policy\u03c0 i+1 on D using Eq. (1). 8: end for 9: return best\u03c0 i on validation.\n\n10: function PARSE&COLLECT(\u00b5, q,\u03c0 i , \u03c0 * ) 11:\n\nInitialize D i \u2190 \u2205, s 1 = (q, \u03c6).\n\n\n12:\n\nfor t = 1 to T do 13:\n\nPreview action a t = arg max a\u03c0i (s t );\n\n\n14:\n\nif p\u03c0 i (a t |s t ) \u2265 \u00b5 then 15: Trigger user interaction and derive expert demonstration a * t \u223c \u03c0 * (s t ); 20:\nw t \u2190 1; 16: Collect D i \u2190 D i {(s t , a t ,w t \u2190 1 if a * t is valid; 0 otherwise; 21: Collect D i \u2190 D i {(s t , a * t , w t )}; 22:\nExecute a * t . return D i . 26: end function no less than a threshold, i.e., p\u03c0 i (a t |s t ) \u2265 \u00b5. In this case, the algorithm executes and collects the policy action a t (Line 15-16); otherwise, a systemuser interaction will be triggered and the derived demonstration a * t \u223c \u03c0 * (s t ) will be collected and executed to continue parsing (Line 17-22).\n\nDenote a collected state-action pair as (s t ,\u00e3 t ), where\u00e3 t could be a t or a * t depending on whether an interaction is requested. To train\u03c0 i+1 (Line 7), our algorithm adopts a reduction-based approach similar to DAGGER and reduces imitation learning to iterative supervised learning. Formally, we define our training loss function as a weighted negative log-likelihood:\nL(\u03c0 i+1 ) = \u2212 1 |D| (st,\u00e3t,wt)\u2208D w t log p\u03c0 i (\u00e3 t |s t ),(1)\nwhere D is the aggregated training data over i iterations and w t denotes the weight of (s t ,\u00e3 t ).\n\nWe consider assigning weight w t in three cases: (1) For confident actions a t , we set w t = 1. This essentially treats confident actions as gold decisions, which resembles self-training (Nigam and Ghani, 2000).\n\n(2) For user-confirmed decisions (valid demonstrations a * t ), such as enforcing a WHERE condition on \"Player\" in Figure 1, w t is also set to 1 to encourage the parser to imitate the correct decisions from users. (3) For uncertain actions that cannot be addressed via human interactions (invalid demonstrations a * t ), we assign w t = 0. This could happen when some of the incorrect precedent actions are not fixed. For example, in Figure 1, if the system missed correcting the WHERE condition on \"School/Club Team\", then whatever value it generates after \"WHERE School/Club Team=\" is wrong, and thus any action a * t derived from human feedback would be invalid. A possible training strategy in this case can set w t to be negative, similar to (Welleck et al., 2020). However, empirically we find this strategy fails to train the parser to correct its mistake in generating School/Club Team but rather disturbs model training. To solve this problem, we directly set w t = 0 to remove the impact of unaddressed actions. A similar solution is also adopted in (Petrushkov et al., 2018;Kreutzer and Riezler, 2019). As shown in Section 6, this way of training weight assignment enables stable improvement in iterative model learning while requiring fewer user annotations.\n\n\nTheoretical Analysis\n\nWhile our system enjoys the benefit of learning from a small amount of user feedback, one crucial question is whether it can still achieve the same level of performance as a system trained on full expert annotations, if one could afford that and manage the privacy risk. In other words, what is the performance gap between our system and a fully supervised system? In this section, we answer this question by showing that the performance gap is mainly bounded by the learning policy's probability of trusting a confident action that turns out to be wrong, which can be controlled in practice.\n\nIn the analysis, we follow prior work (Ross and Bagnell, 2010;Ross et al., 2011) to assume a unified trajectory length T and focus the proof on the \"infinite sample\" case, which assumes an infinite number of samples in each iteration (i.e., m = \u221e in Algorithm 1), such that the state space can be full explored by the current policy. An analysis under the \"finite sample\" case can be found in Appendix A.5.\n\n\nCost Function for Analysis\n\nDifferent from typical imitation learning tasks (e.g., Super Tux Kart (Ross et al., 2011)), in semantic parsing, there exists only one gold trajectory semantically identical to the question and can return correct execution results. 3 Whenever a policy action is different from the gold one, the whole trajectory will not yield the correct semantic meaning. Therefore, we analyze a policy's performance only when it is conditioned on a gold partial parse, i.e., s t \u2208 d t \u03c0 * , where d t \u03c0 * is the state distribution in step t when executing the expert policy \u03c0 * for first t-1 steps. Let (s,\u03c0) = 1 \u2212 p\u03c0(a = a * |s) be the loss of\u03c0 making a mistake at state s. By summing up a policy's expected loss over T steps, we define the cost of the policy as:\nJ(\u03c0) = T t=1 E st\u2208d t \u03c0 * (s t ,\u03c0) = T E s\u223cd \u03c0 * (s,\u03c0) ,(2)\nwhere d \u03c0 * = 1 T T t=1 d t \u03c0 * denotes the average expert state distribution (assuming time step t is a random variable uniformly sampled from 1 \u223c T ). A detailed derivation is shown in Appendix A.1.\n\nThe better\u03c0 is, the smaller this cost becomes. Although it is not exactly the same as the objective evaluated in experiments, which measures the correctness of a complete trajectory (rather than a single policy action) sampled from\u03c0, this simplified version makes theoretical analysis easier and reflects a consistent relative performance among algorithms. Next, we will derive the bound of each policy's cost in order to compare their performance.\n\n\nCost Bound of Supervised Approach\n\nA fully supervised system trains a parser on expertannotated (q, a * 1:T ) pairs, where the gold semantic parse a * 1:T can be viewed as generated by executing the expert policy \u03c0 * . This gives the policy\u03c0 sup :\n\u03c0 sup = arg min \u03c0\u2208\u03a0 E s\u223cd \u03c0 * [l(s, \u03c0)],\nwhere \u03a0 is the policy space induced by the model architecture. A detailed derivation in Appendix A.2 shows the cost bound of the supervised approach:\n\nTheorem 5.1. For supervised approach, let N = min \u03c0\u2208\u03a0 E s\u223cd \u03c0 * [l(s, \u03c0)], then J(\u03c0 sup ) = T N .\n\nThe theorem gives an exact bound (as shown by the equality) since the supervised approach, given the \"infinite sample\" assumption, trains a policy under the same state distribution d \u03c0 * as the one being evaluated in the cost function (Eq. (2)). As we will show next, when adopting an annotation-efficient learning strategy, our proposed algorithm breaks this consistency and thus induces a performance gap compared with the supervised approach.\n\n\nCost Bound of Our Proposed Algorithm\n\nDuring its iterative learning, Algorithm 1 produces a sequence of policies\u03c0 1:N = (\u03c0 1 ,\u03c0 2 , ...,\u03c0 N ), where N is the number of training iterations, and returns the one with the best test-time performance on validation as\u03c0 (Line 9). Recall that our algorithm samples a trajectory by executing actions from both the previously learned policy\u03c0 i and the expert policy \u03c0 * (when an interaction is requested). Let \u03c0 i denote such a \"mixture\" policy. The cost of the learned policy\u03c0 can be bounded as:\nJ(\u03c0) = min \u03c0 \u2208\u03c0 1:N T E s\u223cd \u03c0 * (s,\u03c0 ) \u2264 T N N i=1 E s\u223cd \u03c0 * (s,\u03c0 i ) \u2264 T N N i=1 E s\u223cd\u03c0 i [ (s,\u03c0 i )] + max ||d \u03c0 i \u2212 d \u03c0 * || 1 .\nThe above derivation shows that the bound comprises of two parts. The first term E s\u223cd\u03c0 i [ (s,\u03c0 i )] calculates the expected training loss of\u03c0 i . Notice that, in training, each trajectory is sampled from the mixture policy (s \u223c d \u03c0 i ), while in evaluation, we measure a policy's performance conditioned on a gold partial parse (s \u223c d \u03c0 * in Eq. (2)). This discrepancy, which does not exist in the supervised approach, explains the performance loss of our algorithm, which is bounded by the second term max ||d \u03c0 i \u2212 d \u03c0 * || 1 , the L 1 distance between d \u03c0 i and d \u03c0 * weighted by the maximum loss value l max that\u03c0 i encounters over the training. Bounding the two terms gives the following theorem:\nTheorem 5.2. For the proposed annotation- efficient imitation learning algorithm, if N is\u00d5(T ), there exists a policy\u03c0 \u2208\u03c0 1:N s.t. J(\u03c0) \u2264 T N + 2T max N N i=1 e i + O(1). Here, N = min \u03c0\u2208\u03a0 1 N N i=1 E s\u223cd\u03c0 i [ (s, \u03c0)]\ndenotes the best expected policy loss in hindsight, and e i denotes the probability that\u03c0 i does not query the expert policy (i.e., being confident) but its own action is wrong under d \u03c0 * . A detailed derivation can be found in Appendix A.3-A.4.\n\n\nRemarks.\n\nA comparison of Theorem 5.1 and Theorem 5.2 shows that the performance gap led by our algorithm is mainly bounded by 1 N N i=1 e i . Intuitively this is because whenever a learning policy in our algorithm collects its own, but wrong, action as the gold one for training, it introduces noise that does not exist in the supervised approach's training set. This finding inspires us to restrict the gap by lowering down the learning policy's error rate when it does not query the expert. Empirically this can be achieved by setting:\n\n\u2022 Accurate policy confidence estimation, such that actions regarded confident are generally correct. \u2022 Moderate model initialization, such that generally the policy is less likely to make wrong actions throughout the iterative training. For the first point, we set a high confidence threshold \u00b5, which has been demonstrated to be reliable for MISP (Yao et al., 2019b). In the future, it can even be replaced by a machine learning module (see a discussion in Section 7). We empirically validate the second point in our experiments.\n\n\nExperiments\n\nIn this section, we conduct experiments to demonstrate the annotation efficiency of our algorithm (Section 4) and that it can train semantic parsers to reach high performance when the system is reasonably instantiated, consistent with our theoretical analysis in Section 5.\n\n\nExperimental Setup\n\nWe test our system on the WikiSQL dataset (Zhong et al., 2017). The dataset contains a large scale of annotated question-SQL pairs (56,355 pairs for training) and thus serves as a good resource for experimenting iterative learning. For the base semantic parser, we choose SQLova (Hwang et al., 2019), one of the top-performing models on Wik-iSQL, to ensure a reasonable model capacity in terms of data utility along iterative training.\n\nTo instantiate the proposed algorithm, we set a high confidence threshold \u00b5 = 0.95 following Yao et al. (2019b) and experiment with different initialization settings as suggested by our analysis in Section 5, using 10%, 5% and 1% of the total training data. During iterative learning, questions from the remaining training data arrive in a random order to simulate user questions. The parser is trained with simulated user feedback (which is obtained by directly comparing the synthesized query with the gold one) iteratively for every m = 1, 000 questions. We test systems under different training iterations N and report results averaged over three random runs. More implementation details are included in Appendix B.1.\n\n\nSystem Comparison\n\nWe compare our system (denoted as MISP-L since it builds a Learning algorithm upon MISP) with the traditional supervised approach (denoted as Full Expert). To investigate the skyline capability of our system, we also present a variant called MISP-L*, which is assumed with perfect confidence measurement and interaction design, so that it can precisely identify and correct its mistakes during parsing. This is implemented by allowing the system to compare its synthesized query with the gold one. Note that this is not a realized automatic system; we show its performance as an upper bound of MISP-L.\n\nOn the other hand, while the learning systems by Clarke et al. (2010) and Iyer et al. (2017), which request user validation on query execution results, may not very practical to interact with end users, we include them nonetheless in the interest of comprehensive comparison. This leads to two baseline systems. The Binary User system requests binary user feedback on whether executing the generated SQL query returns correct database results and collects only queries with correct execution results to further improve the parser, similar to (Clarke et al., 2010). The Binary User+Expert system additionally collects full expert SQL annotations when the execution results of the generated SQL queries are wrong, similar to (Iyer et al., 2017).\n\n\nExperimental Results\n\nWe evaluate each system by answering the two research questions (RQs):\n\n\u2022 RQ1: Can the system learn a semantic parser without requiring a large amount of annotations?\n\n\u2022 RQ2: For interactive systems, while requiring weaker supervision, can they train the parser to reach a performance comparable to the traditional supervised system?\n\nFor RQ1, we measure the number of user/expert annotations a system requires to train a parser. For Full Expert, this number is equal to the trajectory length of the gold query (e.g., 5 for the query in Figure 1); for MISP-L and MISP-L*, it is the number of user interactions during training. For Binary User(+Expert), it is hard to quantify \"one annotation\", which varies according to the actual database size and the query difficulty. In experiments, we approximate this number by calculating it in the same way as Full Expert, with the assumption that in general validating an answer is as hard as validating the SQL query itself. More accurate metrics can be explored by conducting a user study, as we discussed in Section 7. Note that while we do not differentiate the actual cost (e.g., time and financial cost) of users and experts in this aspect, we emphasize that our system enjoys an additional benefit of collecting training examples from a much cheaper and more abundant source while serving end users' needs at the same time.\n\nFigure 2 (top) shows each system's parsing accuracy on WikiSQL test set after they have been trained on certain amounts of annotations. Consistently under all initialization conditions, MISP-L consumes a comparable or smaller amount of annotations to train the parser to reach the same parsing accuracy. As shown in Figure 5 in Appendix, MISP-L requires an average of no more than one interaction for most questions along the iterative training. Given the limited size of WikiSQL training set, the simulation experiments currently can only show the system's performance under a small number of annotations. However, we expect this gain to continue as it receives more user questions in the long-term deployment.\n\nTo answer RQ2, Figure 2 (bottom) compares each system's parsing accuracy after they have been trained for the same number of iterations. The results demonstrate that when a semantic parser is moderately initialized (10%/5% initialization setting), MISP-L can further improve it to reach a comparable accuracy as Full Expert (0.776/0.761 vs. 0.794 in the last iteration). In the extremely weak 1% initialization setting (using only around 500 initial training examples), all interactive learning systems suffer from a huge performance loss. . We experiment systems with three initialization settings, using 10%, 5% and 1% of the training data respectively. This is consistent with our finding in theoretical analysis (Section 5). In Appendix C, we plot the value of e i , the probability that\u03c0 i makes a confident but wrong decision given a gold partial parse, showing that a better initialized policy generally obtains a smaller e i throughout the training and thus a tighter cost bound.\n\nFor both RQ1 and RQ2, our system surpasses Binary User, the execution feedback-based system. In experiments, we find out that the inferior performance of Binary User is mainly due to the \"spurious program\" issue (Guu et al., 2017), i.e., a SQL query having correct execution results can still be incorrect in terms of semantics. 4 MISP-L circumvents this issue by directly validating the semantic meaning of intermediate parsing decisions.\n\nFinally, when it is assumed with perfect interaction design and confidence estimator, MISP-L* shows striking superiority in both aspects. Since it always corrects wrong decisions immediately, MISP-L* can collect and derive the same training examples as Full Expert, and thus trains the parser to Full Expert's performance level. Meanwhile, it requires only 10% of the annotations that Full Expert consumes. These observations implies large 4 For example, contrast \"WHERE C1=A\" with \"WHERE C1=A and C2=B\". They can give the same execution results when all records satisfying \"C1=A\" also meet \"C2=B\" by accident. However, semantically the latter includes an extra condition which may not be specified by the question. room for MISP-L to be improved in the future.\n\n\nGeneralize to Complex SQL Queries\n\nSince queries in WikiSQL are generally simple and follow a pre-specified \"SELECT...WHERE...\" sketch, the last part of our experiments investigates whether our system can generalize to complex SQL queries. To this end, we test our system on the Spider dataset (Yu et al., 2018), where SQL queries can contain complicated keywords like GROUP BY. For the base semantic parser, we choose EditSQL (Zhang et al., 2019), one of the open-sourced top models on Spider. Given the small size of Spider (7,377 question-SQL query pairs for training after data cleaning), we only experiment with one initialization setting, using 10% of the training set. Since all Spider models do not predict the specific values in a SQL query (e.g., \"jalen rose\" in Figure 1), 5 we cannot execute the generated query to simulate the binary execution feedback. Therefore, we only compare our system with the Full Expert baseline. Parsers are evaluated on Spider Dev set since the test set is not publicly available. We include all implementation details in Appendix B.2. Figure 3 (top) shows that our system and its variant consistently achieve comparable or better annotation efficiency. We expect this advantage to continue as the system receives more questions and interactions from users beyond the Spider dataset. However, we also notice that the gain is smaller and MISP-L suffers from a larger performance loss compared with Full Expert (Figure 3, bottom), due to the poor parser initialization and the SQL query complexity. This can be addressed via adopting better interaction designs and a more accurate confidence estimation, as shown by MISP-L*.\n\n\nConclusion and Future Work\n\nIn this paper, we explore building an interactive semantic parser that continually improves itself from end user interaction, without involving annotators or developers. To this end, we propose an annotation-efficient imitation learning algorithm to learn from the sparse, fine-grained demonstrations. We prove the quality of the algorithm theoretically and show its advantage over the traditional full expert annotation approach via experiments.\n\nAs a pilot study on this research topic, we train systems with simulated user feedback. One important future work is to conduct a large-scale user study and collect interactions from real users. This is not trivial and has to account for uncertainties such as noisy user feedback. By analyzing real users' statistics (e.g., average time spent on each question), we believe a more accurate and realistic formulation of user/expert annotation cost can be derived to guide future research.\n\nBesides, we would like to explore more accurate confidence measurement to improve our system, as suggested by our theoretical analysis. In experiments, we observe that the two neural semantic parsers (especially the more complicated EditSQL) tend to be overconfident, and training them with more data does not mitigate this issue. To address that, future directions include neural network calibration (Guo et al., 2017) and using machine learning components (e.g., a reinforcement learning-based active selector (Fang et al., 2017)) to replace the confidence threshold.\n\nFinally, the proposed annotation-efficient imitation learning algorithm can be generalized to other NLP tasks (Sokolov et al., 2016)  In this section, we give a detailed theoretical analysis to derive the cost bound of the supervised approach and our proposed annotation-efficient imitation learning algorithm in Section 4. Following Ross et al. (2011), we first focus the proof on an infinite sample case, which assumes an infinite number of samples to train a policy in each iteration (i.e., m = \u221e in Algorithm 1). As an overview, we start the analysis by introducing the \"cost function\" we use to analyze each policy in Appendix A.1, which represents an inverse quality of a policy. In Appendix A.2, we derive the bound of the cost of the supervised approach. Appendix A.3 and Appendix A.4 then discuss the cost bound of our proposed algorithm. Finally, in Appendix A.5, we show the cost bound of our algorithm in finite sample case.\n\n\nA.1 Cost Function for Analysis\n\nIn a semantic parsing task, whenever a policy action is different from the gold one, the whole trajectory cannot yield the correct semantic meaning. Therefore, we analyze a policy's performance only when it is conditioned on a gold partial parse, i.e., s t \u2208 d t \u03c0 * , where d t \u03c0 * is the state distribution in step t when executing the expert policy \u03c0 * for first t-1 steps. Given a question q and denoting a * 1:t as the gold partial trajectory sampled by the expert policy \u03c0 * , we define the cost of sampling a partial trajectory a 1:t = (a 1 , ..., a t ) as:\nC(q, a 1:t ) = 0 if a 1:t = a * 1:t 1 otherwise .\nBased on this definition, we further define the expected cost of\u03c0 in a single time step t, given the question q and the gold partial parse a 1:t\u22121 \u223c \u03c0 * , as:\nC t \u03c0 (q) = E a 1:t\u22121 \u223c\u03c0 * E at\u223c\u03c0 [C(q, a 1:t )] = E a 1:t\u22121 \u223c\u03c0 * [1 \u2212 p\u03c0(a t = a * t )],\nwhere p\u03c0(a t = a * t ) denotes the probability of sampling a gold action a * t from the policy\u03c0. By taking an expectation over all questions q \u2208 Q, we have the following derivations:\nE q\u2208Q [C t \u03c0 (q)] = E q\u2208Q,a 1:t\u22121 \u223c\u03c0 * [1 \u2212 p\u03c0(a t = a * t )] = E st\u223cd t \u03c0 * [1 \u2212 p\u03c0(a t = a * t )].\nThe second equality holds by the definition s t = (q, a 1:t\u22121 ). In this analysis, we follow Ross and Bagnell (2010); Ross et al. (2011) to assume a unified decision length T . By summing up the above expected cost over the T steps, we define the total cost of executing policy\u03c0 for T steps as:\nJ(\u03c0) = T t=1 E q\u2208Q [C t \u03c0 (q)] = T t=1 E st\u223cd t \u03c0 * [1 \u2212 p\u03c0(a t = a * t |s t )].\nDenote (s,\u03c0) = 1 \u2212 p\u03c0(a = a * |s), a \u223c \u03c0(s), a * \u223c \u03c0 * (s) as the \"loss function\" in our analysis, which is bounded within [0, 1], then the cost of policy\u03c0 can be simplified as:\nJ(\u03c0) = T t=1 E st\u223cd t \u03c0 * (s t ,\u03c0) = T E t\u223cU (1,T ) E st\u223cd t \u03c0 * (s t ,\u03c0) = T E s\u223cd \u03c0 * (s,\u03c0) ,(3)\nwhere d \u03c0 * = 1 T T t=1 d t \u03c0 * is the average expert state distribution, when we assume the time step t to be a random variable under the uniform distribution U(1, T ) (the second equality).\n\nThe better a policy\u03c0 is, the smaller this cost becomes. Our analysis thus compares each policy by deriving the \"bound\" of their costs.\n\n\nA.2 Derivation of Cost Bound for Supervised Approach\n\nIn this section, we analyze the cost bound for the supervised approach. Recall that the supervised approach trains a policy\u03c0 using the standard supervised learning algorithm with supervision from \u03c0 * at every decision step. Therefore, it finds the best policy\u03c0 sup on infinite samples as:\n\u03c0 sup = arg min \u03c0\u2208\u03a0 E s\u223cd \u03c0 * [ (s, \u03c0)],(4)\nwhere \u03a0 denotes the policy space induced by the model architecture, and the expectation over s is sampled from the whole d \u03c0 * state space because of the \"infinite sample\" assumption. The supervised approach thus obtains the following cost bound:\nJ(\u03c0 sup ) =T E s\u223cd \u03c0 * [ (s,\u03c0 sup )] =T min \u03c0\u2208\u03a0 E s\u223cd \u03c0 * [ (s, \u03c0)].\nThis gives the following theorem:\n\nTheorem A.1. For supervised approach, let N = min \u03c0\u2208\u03a0 E s\u223cd \u03c0 * [ (s, \u03c0)], then J(\u03c0 sup ) = T N .\n\nThe cost bound of the supervised approach represents its exact performance as implied by the equality. This is because the approach trains a policy (Eq. (4)) under the same state distribution d \u03c0 * (given the \"infinite sample\" assumption) as in evaluation (Eq. (3)). As we will show next, the proposed annotation-efficient imitation learning algorithm breaks this consistency while enjoying the benefit of high annotation efficiency, which explains the performance gap.\n\n\nA.3 No-regret Assumption\n\nThe derivation of our proposed annotation-efficient imitation learning algorithm's cost bound leverages a \"no-regret\" assumption:\nAssumption A.1. No-regret assumption. De- fine i (\u03c0) = E s\u223cd\u03c0 i [l(s, \u03c0)] and N = min \u03c0\u2208\u03a0 1 N N i=1 i (\u03c0), then 1 N N i=1 i (\u03c0 i ) \u2212 N \u2264 \u03b3 N for lim N \u2192\u221e \u03b3 N = 0 (usually \u03b3 N \u2208\u00d5( 1 N )\n). Many no-regret algorithms (Hazan et al., 2007;Kakade and Tewari, 2009) that guarantee \u03b3 N \u2208 O( 1 N ) require convexity or strongly-convexity of the loss function. However, the loss function used in our application, which is built on the top of a deep neural network model, does not satisfy this requirement. In this analysis, we simplify the setting and directly make this assumption for convenience of the proof. A more accurate regret bound for non-convex neural networks can be researched in the future.\n\nAnother concern is that the collected online training labels come from not only the expert policy \u03c0 * (when it is queried), but also the learning policy\u03c0 i (when the agent has a high confidence on its policy action). Labels from the learning policy may bring noise amid the model fitting to the expert policy. However, in practice the impact from such noisy labels are limited when the confidence threshold \u00b5 is set at a high value (e.g., 0.95). In this case, labels from\u03c0 i are generally clean and lead to increasing performance during iterative training. Therefore, it is still safe to make this no-regret assumption.\n\n\nA.4 Derivation of Cost Bound for Our Proposed Algorithm\n\nAs shown in Algorithm 1, our algorithm produces a sequence of policies\u03c0 1:N = (\u03c0 1 ,\u03c0 2 , ...,\u03c0 N ), where N is the number of training iterations, and the algorithm returns the one with the best test-time performance on validation as\u03c0. In training, our algorithm executes actions from both the learning policy\u03c0 i (when the model is confident) and the expert policy \u03c0 * . We denote this \"mixture\" policy as \u03c0 i . Then for the first N iterations, we have the cost bound of our algorithm as:\nJ(\u03c0) = min \u03c0 \u2208\u03c0 1:N T E s\u223cd \u03c0 * (s,\u03c0 ) \u2264 T N N i=1 E s\u223cd \u03c0 * (s,\u03c0 i ) \u2264 T N N i=1 E s\u223cd\u03c0 i [ (s,\u03c0 i )] + max ||d \u03c0 i \u2212 d \u03c0 * || 1 .(5)\nFrom the last inequality, we can see that the cost bound of our algorithm is restricted by two terms. The first term E s\u223cd\u03c0 i [ (s,\u03c0 i )] denotes the expected loss of\u03c0 i under the state induced by \u03c0 i during training (under the \"infinite sample\" assumption, as mentioned in the beginning of the analysis). By applying the no-regret assumption (Assumption A.1), this term can be bound by 1\nN N i=1 E s\u223cd\u03c0 i [ (s,\u03c0 i )] \u2264 N + \u03b3 N . Here, N = min \u03c0\u2208\u03a0 1 N N i=1 i (\u03c0)\ndenotes the best expected training loss in hindsight.\n\nThe second term denotes the L 1 distance between state distributions induced by \u03c0 i and \u03c0 * i , weighted by the maximum loss value l max that\u03c0 i encounters over the training. As we notice, unlike the supervised approach, our algorithm trains a policy under d \u03c0 i , which is different from the state distribution d \u03c0 * used to evaluate the policy (Eq. (3)). This discrepancy explains the performance loss of our algorithm compared to the supervised approach and is bounded by the aforementioned L 1 distance. To further bound this term, we define e i as the probability that\u03c0 i makes a confident (i.e., without querying the expert policy) but wrong action under d \u03c0 * , and introduce the following lemma:\nLemma A.1. ||d \u03c0 i \u2212 d \u03c0 * || 1 \u2264 2T e i .\nProof. Let \u03b2 it be the probability of querying the expert policy under d t \u03c0 * ,\u02dc it the error rate of\u03c0 i under d t \u03c0 * , and d any state distribution besides d \u03c0 * . We can then express d \u03c0 i by:\nd \u03c0 i = T t=1 (\u03b2 it + (1 \u2212 \u03b2 it )(1 \u2212\u02dc it ))d \u03c0 * + (1 \u2212 T t=1 (\u03b2 it + (1 \u2212 \u03b2 it )(1 \u2212\u02dc it )))d.\nThe distance between d \u03c0 i and d \u03c0 * thus becomes\n||d \u03c0 i \u2212 d \u03c0 * || 1 =(1 \u2212 T t=1 (\u03b2 it + (1 \u2212 \u03b2 it )(1 \u2212\u02dc it )))||d \u2212 d \u03c0 * || 1 \u22642(1 \u2212 T t=1 (\u03b2 it + (1 \u2212 \u03b2 it )(1 \u2212\u02dc it ))) \u22642 T t=1 [1 \u2212 (\u03b2 it + (1 \u2212 \u03b2 it )(1 \u2212\u02dc it ))] \u22642 T t=1 [\u02dc it (1 \u2212 \u03b2 it )] \u22642 T t=1 e it =2T e i .\n\nThe second inequality uses\n1 \u2212 T t=1 x t \u2264 T t=1\n(1 \u2212 x t ), which holds when x t \u2208 [0, 1].\n\nBy applying Assumption A.1 and Lemma A.1 to Eq. (3), we derive the following inequality:\nJ(\u03c0) \u2264 T \u03b3 N + N + 2T max N N i=1 e i .\nGiven a large enough N (N \u2208\u00d5(T )), by the no-regret assumption, we can further simplify the above as:\nJ(\u03c0) \u2264 T N + 2T max N N i=1 e i + O(1),\nwhich leads to our theorem: This instantiation is assumed with perfect confidence estimation and interaction design, such that it can precisely detects and corrects its intermediate mistakes during parsing. Therefore, MISP-L* presents an upper bound performance (i.e., the tightest cost bound) of our algorithm. This can be interpreted theoretically. In fact, for MISP-L*, e i is always zero since the system has ensured that its policy action is correct when it does not query the expert policy. In this case, d \u03c0 i = d \u03c0 * , so N = min \u03c0\u2208\u03a0 l(s, \u03c0)]. Therefore, according to Theorem A.2, MISP-L* has a cost bound of:\n1 N N i=1 E s\u223cd \u03c0 * [l(s, \u03c0)] = min \u03c0\u2208\u03a0 E s\u223cd \u03c0 * [J(\u03c0) \u2264 T N + O(1), where N = min \u03c0\u2208\u03a0 E s\u223cd \u03c0 * [l(s, \u03c0)].\nBy comparing this bound with the cost bound in Theorem A.1, it is observed that MISP-L* shares the same cost bound as the supervised approach (except for the inequality relation and the constant). This is explainable since MISP-L* indeed collects exactly the same training labels (from \u03c0 * ) as the supervised approach.\n\n\nA.5 Cost Bound of Our Proposed Algorithm in Finite Sample Case\n\nThe theorems in previous sections hold when the algorithm observes infinite trajectories. However, in practice, our algorithm will observe the training loss from only a finite set of m trajectories at each iteration i using \u03c0 i . For this consideration, in the following discussion, we provide a proof of the cost bound of our proposed algorithm under the finite sample case. In the finite sample setting, our algorithm observes the training loss from a finite number of trajectories. We define D i as the m trajectories collected in the i th iteration. In every iteration, the algorithm observes loss i (\u03c0 i ) = E s\u223cD i ( (s,\u03c0 i )). By the no-regret assumption (Assumption A.1), the average observed loss for each iterations can still be bounded by the following inequality: 1 \u03c0) to denote the loss of the best policy on the finite samples.\nN N i=1 E s\u223cD i (s, \u03c0 i ) \u2212 min \u03c0\u2208\u03a0 1 N N i=1 E s\u223cD i (s, \u03c0) \u2264\u03b3 N . We us\u1ebd N = min \u03c0\u2208\u03a0 1 N N i=1 E s\u223cD i (s,\nFollowing Eq. (5), we need to switch the derivation from the expected loss of\u03c0 i over d \u03c0 i (i.e., E s\u223cd\u03c0 i [ (s,\u03c0 i )]) to that over D i (i.e., E s\u223cD i [ (s,\u03c0 i )]), the actual state distribution that \u03c0 i is trained on. To fill this gap, we introduce Y ij to denote the difference between the expected loss of\u03c0 i under d \u03c0 i and the average loss of\u03c0 i under the j th sample trajectory with \u03c0 at iteration i. The random variables Y ij over all i \u2208 {1, 2, ..., N } and j \u2208 {1, 2, ..., m} are all zero mean, bounded in [\u2212 max , max ] and form a martingale in the order of Y 11 , Y 12 , ..., Y 1m , Y 21 , ..., Y N m . By Azuma-Hoeffding's inequality (Azuma, 1967;Hoeffding, 1994)\n, 1 mN N i=1 m j=1 Y ij \u2264 max 2 log(1/\u03b4) mN\nwith probability 1 \u2212 \u03b4. Following the derivations in Eq. (5) and by introducing Y ij , with probability of 1 \u2212 \u03b4, we obtain the following inequalities by definition:\nJ(\u03c0) \u2264 T N N i=1 E s\u223cd\u03c0 i [ (s,\u03c0 i )] + max ||d \u03c0 i \u2212 d \u03c0 * || 1 \u2264 T N N i=1 E s\u223cD i [ (s,\u03c0 i )] + max ||d \u03c0 i \u2212 d \u03c0 * || 1 + T mN N i=1 m j=1 Y ij \u2264 T N N i=1 E s\u223cD i [ (s,\u03c0 i )] + max ||d \u03c0 i \u2212 d \u03c0 * || 1 + max T 2 log(1/\u03b4) mN \u2264T \u03b3 N +\u02dc N + max 2 log(1/\u03b4) mN + 2 max T N N i=1 e i .\nNotice that we need mN to be at least O(T 2 log(1/\u03b4)), so that\u03b3 N and l max 2 log(1/\u03b4) mN are negligible. This leads to the following theorem:\n\nTheorem A.3. For our proposed annotationefficient imitation learning algorithm, with probability at least 1 \u2212 \u03b4, when mN is\u00d5(T 2 log(1/\u03b4)), there exists a policy\u03c0 \u2208\u03c0 1:\nN s.t. J(\u03c0) \u2264 T \u02dc N + 2lmaxT N N i=1 e i + O(1).\nThe above shows that the cost of our algorithm can still be bounded in the finite sample setting. Comparing this bound with the bound under the infinite sample setting, we can observe that the bound is still related to e i , the probability that\u03c0 i takes a confident but incorrect action under d \u03c0 * . B Implementation Details\n\n\nB.1 Interactive Semantic Parsing Framework\n\nOur system assumes an interactive semantic parsing framework to collect user feedback. In experiments, this is implemented by adapting MISP (Yao et al., 2019b), an open-sourced framework that has demonstrated a strong ability to improve test-time parsing accuracy. 6 In this framework, an agent is comprised of three components: a world model that wraps the base semantic parser and a feedback incorporation module to interpret user feeds and update the semantic parse, an error detector that decides whether to request for user intervention, and an actuator that delivers the agent's request by asking a natural language question, such that users without domain expertise can understand.\n\nWe follow MISP's instantiation for text-to-SQL tasks to adopt a probability-based uncertainty estimator as the error detector, which triggers user interactions when the probability of the current decision is lower than a threshold. 7 The actuator is instantiated by a grammar-based natural language generator. We use the latest version of MISP that allows multi-choice interactions to improve the system efficiency, i.e., when the parser's current decision is validated as wrong, the system presents multiple alternative options for user selection. An additional \"None of the above options\" option is included in case all top options from the system are wrong. Figure 1 shows an example of the user interaction. From there, the system can derive a correct decision to address its uncertainty (e.g., taking \"Player\" as a WHERE column).\n\nUser Simulator. Our experiments train each system with simulated user feedback. To this end, we build a user simulator similar to the one used by Yao et al. (2019b), which can access the groundtruth SQL queries. It gives yes/no answer or selects a choice by directly comparing the sampled policy action with the true one in the gold query.\n\n\nB.2 EditSQL Experiment Details\n\nIn the data preprocessing step, EditSQL (Zhang et al., 2019) transforms each gold SQL query into a sequence of tokens, where the From clause is removed and each column Col is prepended by its paired table name, i.e., Tab.Col. However, we observe that sometimes this transformation is not convertible. For example, consider the question \"what are the first name and last name of all candidates?\" and its gold SQL query: \"SELECT T2.first name , T2.last name FROM candidates AS T1 JOIN people AS T2 ON T1.candidate id = T2.person id\". EditSQL transforms this query into : \"select people.first name , people.last name\".\n\nThe transformed sequence accidentally removes the information about table candidates in the original SQL query, leading to semantic meaning inconsistent with the question. When using such erroneous sequences as the gold targets in model training, we cannot simulate consistent user feedback, e.g., when the user is asked whether her query is relevant to the table candidates, the simulated user cannot give an affirmative answer given the transformed sequence. To avoid inconsistent user feedback, we remove question-SQL pairs whose transformed sequence is inconsistent with the original gold SQL query, from the training data. This reduces the size of the training set from 8,421 to 7,377. The validation set is kept untouched for fair evaluation.\n\nThe implementation of interactive semantic parsing for EditSQL is the same as Section B.1, except that, in order to cope with the complicated structure of Spider SQL queries, for columns in WHERE, GROUP BY, ORDER BY and HAVING clauses, we additionally provide an option for the user to \"remove\" the clause, e.g., removing a WHERE clause by picking the \"The system does not need to consider any conditions.\" option. The confidence threshold \u00b5 is 0.995 as we observe that EditSQL tends to be overconfident. Figure 4 shows MISP-L's performance on Wik-iSQL validation set. We also show in Figure 5 the average number of annotations (i.e., user interactions) per question during the iterative training. Overall, as the base parser is further trained, the system tends to request fewer user interactions. In most cases throughout the training, the system requests no more than one user interaction, demonstrating the annotation efficiency of our algorithm.\n\n\nC Additional Experimental Results\n\n\nC.1 SQLova Results on Dev set\n\n\nC.2 SQLova Results in Theoretical Analysis\n\nAs we proved in Section 5, the performance gap between our proposed algorithm and the supervised approach is mainly decided by 1 N N i=1 e i , an average probability that\u03c0 i makes a confident but wrong decision under d \u03c0 * (i.e., given a gold partial parse) over N training iterations. More specifically, from our proof of Lemma A.1, e i can be expressed as:\ne i = 1 T T t=1 e it = 1 T T t=1\u02dc it (1 \u2212 \u03b2 it ),\nwhere\u02dc it denotes policy\u03c0 i 's conditional error rate under d t \u03c0 * when it does not query the expert (i.e., being confident about its own action) at step t, and 1\u2212\u03b2 it denotes the probability that\u03c0 i does not query the expert under d t \u03c0 * .\u02dc it (1 \u2212 \u03b2 it ) thus represents a joint probability that\u03c0 i makes confident but wrong action under d t \u03c0 * at step t. To show a reflection of our theoretical analysis on the experiments, we present the values of the following three variables during training: (1) i = 1 T T t=1\u02dc it , the average value of\u02dc it over T time steps. A smaller\u02dc i implies a lower conditional error rate and thus a smaller e i and a smaller performance gap. (2) \u03b2 i = 1 T T t=1 \u03b2 it , the average value of \u03b2 it over T time steps. A smaller \u03b2 i (i.e., a larger 1 \u2212 \u03b2 i ) means a smaller probability that\u03c0 i queries the expert (i.e., being more confident). This could lead to a larger e i and thus a larger performance gap.\n\n(3) e i as defined above. A smaller e i indicates a smaller performance gap between our algorithm and the supervised approach.\n\nWe plot the results in Figure 6. For all initialization settings, we observe that the base parser tends to make more confident actions under a gold partial parse (i.e., decreasing \u03b2 i ) when it is trained for more iterations. Meanwhile, the error rate of its confident actions under a gold partial parse is also reduced (i.e., decreasing\u02dc i ). When combining the two factors, e i is shown to keep decreasing, implying that with more iterations that the parser is trained, it gets a tighter cost bound and better performance.\n\nFinally, we notice that a differently initialized parser can end up with a different performance. This is reasonable since a better initialized parser presumably should have a better overall error rate. This is also consistent with our observation in the main experimental results (Section 6). . We experiment systems with three initialization settings, using 10%, 5% and 1% of the training data respectively. \n\nFigure 2 :\n2System parsing accuracy on WikiSQL test set when they are trained with various numbers of user/expert annotations (top) and for different iterations (bottom)\n\nFigure 3 :\n3System parsing accuracy on Spider Dev set when they are trained with various numbers of user/expert annotations and for different iterations.\n\nTheorem A. 2 .\n2For our proposed annotationefficient imitation learning algorithm, if N is\u00d5(T ), there exists a policy\u03c0 \u2208\u03c0 1:N s.t. J(\u03c0) \u2264 T N + 2T max N N i=1 e i + O(1).In experiments, we consider a skyline instantiation of the proposed algorithm, called MISP-L*.\n\nFigure 4 :\n4System parsing accuracy on WikiSQL validation set when they are trained with various numbers of user/expert annotations (top) and for different iterations (bottom)\n\nFigure 5 :Figure 6 :\n56Average number of user annotations per question along training iterations (on WikiSQL), when the parser is initialized using 10%, 5% and 1% of training data. The values of\u02dc i (a), \u03b2 i (b) and e i (c) in MISP-L throughout the training (on WikiSQL), under different initialization settings.\n\n\nw t )};17: \n\nExecute a t ; \n\n18: \n\nelse \n\n19: \n\n\n\n\nand classical imitation learning problems (Ross et al., 2011). We expect this algorithm to save human annotation effort particularly for low-resource tasks (Mayhew et al., 2019). Sham M Kakade and Ambuj Tewari. 2009. On the generalization ability of online strongly convex programming algorithms. In Advances in Neural Information Processing Systems, pages 801-808. Luke S Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: structured classification with probabilistic categorial grammars. In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, pages 658-666.Long Duong, Hadi Afshar, Dominique Estival, Glen \nPink, Philip R Cohen, and Mark Johnson. 2018. Ac-\ntive learning for deep semantic parsing. In Proceed-\nings of the 56th Annual Meeting of the Association \nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 43-48. \n\nMeng Fang, Yuan Li, and Trevor Cohn. 2017. Learning \nhow to active learn: A deep reinforcement learning \napproach. In Proceedings of the 2017 Conference on \nEmpirical Methods in Natural Language Processing, \npages 595-605. \n\nYang Gao, Christian M Meyer, and Iryna Gurevych. \n2018. APRIL: Interactively learning to summarise \nby combining active preference learning and rein-\nforcement learning. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language \nProcessing, pages 4120-4130. \n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In Proceedings of the 34th International \nConference on Machine Learning-Volume 70, pages \n1321-1330. JMLR. org. \n\nIzzeddin Gur, Semih Yavuz, Yu Su, and Xifeng Yan. \n2018. DialSQL: Dialogue based structured query \ngeneration. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics \n(Volume 1: Long Papers), pages 1339-1349. \n\nKelvin Guu, Panupong Pasupat, Evan Liu, and Percy \nLiang. 2017. From language to programs: Bridg-\ning reinforcement learning and maximum marginal \nlikelihood. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics \n(Volume 1: Long Papers), pages 1051-1062. \n\nBraden Hancock, Antoine Bordes, Pierre-Emmanuel \nMazare, and Jason Weston. 2019. Learning from di-\nalogue after deployment: Feed yourself, chatbot! In \nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3667-\n3684. \n\nElad Hazan, Amit Agarwal, and Satyen Kale. 2007. \nLogarithmic regret algorithms for online convex op-\ntimization. Machine Learning, 69(2-3):169-192. \n\nLuheng He, Julian Michael, Mike Lewis, and Luke \nZettlemoyer. 2016. Human-in-the-loop parsing. In \nProceedings of the 2016 Conference on Empirical \nMethods in Natural Language Processing, pages \n2337-2342. \n\nWassily Hoeffding. 1994. Probability inequalities for \nsums of bounded random variables. In The Col-\nlected Works of Wassily Hoeffding, pages 409-426. \nSpringer. \n\nWonseok Hwang, Jinyeung Yim, Seunghyun Park, and \nMinjoon Seo. 2019. A comprehensive exploration \non WikiSQL with table-aware word contextualiza-\ntion. arXiv preprint arXiv:1902.01069. \n\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant \nKrishnamurthy, and Luke Zettlemoyer. 2017. Learn-\ning a neural semantic parser from user feedback. In \nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: \nLong Papers), pages 963-973. \n\nKshitij Judah, Alan P Fern, Thomas G Dietterich, and \nPrasad Tadepalli. 2014. Active imitation learning: \nFormal and practical reductions to iid learning. Jour-\nnal of Machine Learning Research, 15:4105-4143. \n\nBeomjoon Kim and Joelle Pineau. 2013. Maximum \nmean discrepancy imitation learning. Robotics: Sci-\nence and systems. \n\nJulia Kreutzer and Stefan Riezler. 2019. Self-regulated \ninteractive sequence-to-sequence learning. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 303-315. \n\nFei Li and HV Jagadish. 2014. Constructing an in-\nteractive natural language interface for relational \ndatabases. Proceedings of the VLDB Endowment, \n8(1):73-84. \n\nNatasha Lomas. 2019. Google ordered to halt hu-\nman review of voice AI recordings over privacy \nrisks. https://techcrunch.com/2019/08/02/ \n\ngoogle-ordered-to-halt-human-review-\nof-voice-ai-recordings-over-privacy-\n\nrisks/. Accessed: 2020-04-28. \n\nStephen Mayhew, Snigdha Chaturvedi, Chen-Tse Tsai, \nand Dan Roth. 2019. Named entity recognition with \npartially annotated training data. In Proceedings of \nthe 23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL), pages 645-655. \n\nKhanh Nguyen, Hal Daum\u00e9 III, and Jordan Boyd-\nGraber. 2017. Reinforcement learning for bandit \nneural machine translation with simulated human \nfeedback. In Proceedings of the 2017 Conference on \nEmpirical Methods in Natural Language Processing, \npages 1464-1474. \n\nAnsong Ni, Pengcheng Yin, and Graham Neubig. 2020. \nMerging weak and active supervision for semantic \nparsing. In Thirty-Fourth AAAI Conference on Arti-\nficial Intelligence (AAAI), New York, USA. \n\nKamal Nigam and Rayid Ghani. 2000. Analyzing the \neffectiveness and applicability of co-training. In \nProceedings of the ninth international conference on \nInformation and knowledge management, pages 86-\n93. \n\nPavel Petrushkov, Shahram Khadivi, and Evgeny Ma-\ntusov. 2018. Learning from chunk-based feedback \nin neural machine translation. In Proceedings of the \n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages \n326-331. \n\nSt\u00e9phane Ross and Drew Bagnell. 2010. Efficient re-\nductions for imitation learning. In Proceedings of \nthe thirteenth international conference on artificial \nintelligence and statistics, pages 661-668. \n\nSt\u00e9phane Ross and J. Andrew Bagnell. 2014. Rein-\nforcement and imitation learning via interactive no-\nregret learning. ArXiv, abs/1406.5979. \n\nSt\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. \n2011. A reduction of imitation learning and struc-\ntured prediction to no-regret online learning. In Pro-\nceedings of the fourteenth international conference \non artificial intelligence and statistics, pages 627-\n635. \n\nArtem Sokolov, Julia Kreutzer, Christopher Lo, and \nStefan Riezler. 2016. Learning structured predictors \nfrom bandit feedback for interactive nlp. In Proceed-\nings of the 54th Annual Meeting of the Association \nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1610-1620. \n\nYu Su, Ahmed Hassan Awadallah, Madian Khabsa, \nPatrick Pantel, Michael Gamon, and Mark Encar-\nnacion. 2017. Building natural language interfaces \nto web apis. In Proceedings of the International \nConference on Information and Knowledge Manage-\nment. \n\nYu Su, Ahmed Hassan Awadallah, Miaosen Wang, and \nRyen W White. 2018. Natural language interfaces \nwith fine-grained user interaction: A case study on \nweb APIs. In Proceedings of the International ACM \nSIGIR Conference on Research and Development in \nInformation Retrieval. \n\nRichard S Sutton and Andrew G Barto. 2018. Rein-\nforcement learning: An introduction. MIT press. \n\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In \nInternational Conference on Learning Representa-\ntions. \n\nWilliam A Woods. 1973. Progress in natural language \nunderstanding: an application to lunar geology. In \nProceedings of the American Federation of Informa-\ntion Processing Societies Conference. \n\nZiyu Yao, Xiujun Li, Jianfeng Gao, Brian Sadler, and \nHuan Sun. 2019a. Interactive semantic parsing for \nif-then recipes via hierarchical reinforcement learn-\ning. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, volume 33, pages 2547-2554. \n\nZiyu Yao, Yu Su, Huan Sun, and Wen-tau Yih. 2019b. \nModel-based interactive semantic parsing: A unified \nframework and a text-to-SQL case study. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language \nProcessing (EMNLP-IJCNLP), pages 5450-5461. \n\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, \nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, et al. 2018. Spider: A \nlarge-scale human-labeled dataset for complex and \ncross-domain semantic parsing and text-to-SQL task. \nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages \n3911-3921. \n\nJiakai Zhang and Kyunghyun Cho. 2017. Query-\nefficient imitation learning for end-to-end simulated \ndriving. In Thirty-First AAAI Conference on Artifi-\ncial Intelligence. \n\nRui Zhang, Tao Yu, Heyang Er, Sungrok Shim, \nEric Xue, Xi Victoria Lin, Tianze Shi, Caim-\ning Xiong, Richard Socher, and Dragomir Radev. \n2019. Editing-based SQL query generation for \ncross-domain context-dependent questions. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language \nProcessing (EMNLP-IJCNLP), pages 5341-5352. \n\nVictor Zhong, Caiming Xiong, and Richard Socher. \n2017. Seq2SQL: Generating structured queries \nfrom natural language using reinforcement learning. \narXiv preprint arXiv:1709.00103. \n\nCode will be available at https://github.com/ sunlab-osu/MISP.\nWe follow the imitation learning literature and use \"expert\" to refer to the imitation target, but the user in our setting by no means needs to be a \"domain (SQL) expert\".\nWe assume a canonical order for swappable components in a parse. In practice, it may be possible, though rare, for one question to have multiple gold parses.\n Yu et al. (2018)  promoted that to encourage a focus on more fundamental semantic parsing issues. The evaluation does not count the specific values either.\nhttps://github.com/sunlab-osu/MISP. 7 While a dropout-based error detector is also possible, empirically we found it much slower than the probabilitybased one and thus is not preferable.\nAcknowledgmentsThis research was sponsored in part by the Army Research Office under cooperative agreements W911NF-17-1-0412, NSF Grant IIS1815674, Fujitsu gift grant, and Ohio Supercomputer Center (Center, 1987). The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.", "annotations": {"author": "[{\"end\":110,\"start\":73},{\"end\":149,\"start\":111},{\"end\":192,\"start\":150},{\"end\":230,\"start\":193},{\"end\":265,\"start\":231}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":78},{\"end\":120,\"start\":116},{\"end\":161,\"start\":158},{\"end\":201,\"start\":198},{\"end\":236,\"start\":234}]", "author_first_name": "[{\"end\":77,\"start\":73},{\"end\":115,\"start\":111},{\"end\":157,\"start\":150},{\"end\":197,\"start\":193},{\"end\":233,\"start\":231}]", "author_affiliation": "[{\"end\":109,\"start\":83},{\"end\":148,\"start\":122},{\"end\":191,\"start\":163},{\"end\":229,\"start\":203},{\"end\":264,\"start\":238}]", "title": "[{\"end\":70,\"start\":1},{\"end\":335,\"start\":266}]", "venue": null, "abstract": "[{\"end\":1383,\"start\":337}]", "bib_ref": "[{\"end\":1574,\"start\":1561},{\"end\":1604,\"start\":1574},{\"end\":1624,\"start\":1604},{\"end\":1640,\"start\":1624},{\"end\":1656,\"start\":1640},{\"end\":2846,\"start\":2833},{\"end\":3764,\"start\":3747},{\"end\":3975,\"start\":3952},{\"end\":4002,\"start\":3975},{\"end\":4019,\"start\":4002},{\"end\":4037,\"start\":4019},{\"end\":4071,\"start\":4039},{\"end\":4676,\"start\":4655},{\"end\":4694,\"start\":4676},{\"end\":5693,\"start\":5674},{\"end\":6427,\"start\":6399},{\"end\":7055,\"start\":7038},{\"end\":7266,\"start\":7243},{\"end\":7282,\"start\":7266},{\"end\":7309,\"start\":7282},{\"end\":7325,\"start\":7309},{\"end\":7342,\"start\":7325},{\"end\":7362,\"start\":7342},{\"end\":7396,\"start\":7379},{\"end\":7512,\"start\":7494},{\"end\":7973,\"start\":7952},{\"end\":7997,\"start\":7973},{\"end\":8024,\"start\":7997},{\"end\":8067,\"start\":8045},{\"end\":8084,\"start\":8067},{\"end\":8105,\"start\":8084},{\"end\":8517,\"start\":8493},{\"end\":8535,\"start\":8517},{\"end\":8983,\"start\":8963},{\"end\":8999,\"start\":8983},{\"end\":9206,\"start\":9186},{\"end\":9229,\"start\":9206},{\"end\":9247,\"start\":9229},{\"end\":9270,\"start\":9247},{\"end\":9607,\"start\":9580},{\"end\":9628,\"start\":9607},{\"end\":9647,\"start\":9628},{\"end\":9667,\"start\":9647},{\"end\":9701,\"start\":9669},{\"end\":11724,\"start\":11705},{\"end\":12053,\"start\":12033},{\"end\":13733,\"start\":13713},{\"end\":13756,\"start\":13733},{\"end\":13774,\"start\":13756},{\"end\":13797,\"start\":13774},{\"end\":16702,\"start\":16679},{\"end\":17791,\"start\":17766},{\"end\":17818,\"start\":17791},{\"end\":18657,\"start\":18633},{\"end\":18675,\"start\":18657},{\"end\":24230,\"start\":24211},{\"end\":24767,\"start\":24747},{\"end\":25253,\"start\":25235},{\"end\":27051,\"start\":27030},{\"end\":30562,\"start\":30544},{\"end\":31848,\"start\":31831},{\"end\":34698,\"start\":34679},{\"end\":34870,\"start\":34848},{\"end\":36993,\"start\":36975},{\"end\":39539,\"start\":39519},{\"end\":39562,\"start\":39539},{\"end\":44052,\"start\":44045},{\"end\":46228,\"start\":46215},{\"end\":46244,\"start\":46228},{\"end\":47634,\"start\":47615},{\"end\":49165,\"start\":49147}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54392,\"start\":54222},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54547,\"start\":54393},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54814,\"start\":54548},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54991,\"start\":54815},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55304,\"start\":54992},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":55355,\"start\":55305},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":64659,\"start\":55356}]", "paragraph": "[{\"end\":2399,\"start\":1399},{\"end\":2847,\"start\":2401},{\"end\":2995,\"start\":2849},{\"end\":3870,\"start\":2997},{\"end\":4535,\"start\":3872},{\"end\":5239,\"start\":4537},{\"end\":6195,\"start\":5241},{\"end\":7056,\"start\":6197},{\"end\":7828,\"start\":7073},{\"end\":8457,\"start\":7869},{\"end\":9122,\"start\":8459},{\"end\":10347,\"start\":9124},{\"end\":11349,\"start\":10365},{\"end\":11389,\"start\":11351},{\"end\":11995,\"start\":11391},{\"end\":12229,\"start\":11997},{\"end\":12605,\"start\":12231},{\"end\":13182,\"start\":12657},{\"end\":13653,\"start\":13220},{\"end\":14226,\"start\":13655},{\"end\":14673,\"start\":14270},{\"end\":15021,\"start\":14675},{\"end\":15186,\"start\":15105},{\"end\":15235,\"start\":15188},{\"end\":15270,\"start\":15237},{\"end\":15299,\"start\":15278},{\"end\":15341,\"start\":15301},{\"end\":15462,\"start\":15349},{\"end\":15950,\"start\":15597},{\"end\":16326,\"start\":15952},{\"end\":16489,\"start\":16389},{\"end\":16703,\"start\":16491},{\"end\":17976,\"start\":16705},{\"end\":18593,\"start\":18001},{\"end\":19001,\"start\":18595},{\"end\":19782,\"start\":19032},{\"end\":20043,\"start\":19843},{\"end\":20493,\"start\":20045},{\"end\":20743,\"start\":20531},{\"end\":20934,\"start\":20785},{\"end\":21033,\"start\":20936},{\"end\":21480,\"start\":21035},{\"end\":22019,\"start\":21521},{\"end\":22855,\"start\":22152},{\"end\":23320,\"start\":23074},{\"end\":23861,\"start\":23333},{\"end\":24393,\"start\":23863},{\"end\":24682,\"start\":24409},{\"end\":25140,\"start\":24705},{\"end\":25863,\"start\":25142},{\"end\":26486,\"start\":25885},{\"end\":27231,\"start\":26488},{\"end\":27326,\"start\":27256},{\"end\":27422,\"start\":27328},{\"end\":27589,\"start\":27424},{\"end\":28628,\"start\":27591},{\"end\":29341,\"start\":28630},{\"end\":30330,\"start\":29343},{\"end\":30771,\"start\":30332},{\"end\":31534,\"start\":30773},{\"end\":33200,\"start\":31572},{\"end\":33677,\"start\":33231},{\"end\":34165,\"start\":33679},{\"end\":34736,\"start\":34167},{\"end\":35674,\"start\":34738},{\"end\":36273,\"start\":35709},{\"end\":36482,\"start\":36324},{\"end\":36755,\"start\":36573},{\"end\":37151,\"start\":36857},{\"end\":37410,\"start\":37233},{\"end\":37701,\"start\":37510},{\"end\":37837,\"start\":37703},{\"end\":38182,\"start\":37894},{\"end\":38473,\"start\":38227},{\"end\":38576,\"start\":38543},{\"end\":38675,\"start\":38578},{\"end\":39146,\"start\":38677},{\"end\":39304,\"start\":39175},{\"end\":39999,\"start\":39490},{\"end\":40620,\"start\":40001},{\"end\":41168,\"start\":40680},{\"end\":41692,\"start\":41304},{\"end\":41821,\"start\":41768},{\"end\":42526,\"start\":41823},{\"end\":42766,\"start\":42570},{\"end\":42913,\"start\":42864},{\"end\":43230,\"start\":43188},{\"end\":43320,\"start\":43232},{\"end\":43462,\"start\":43361},{\"end\":44120,\"start\":43503},{\"end\":44549,\"start\":44230},{\"end\":45457,\"start\":44616},{\"end\":46244,\"start\":45567},{\"end\":46454,\"start\":46289},{\"end\":46882,\"start\":46740},{\"end\":47052,\"start\":46884},{\"end\":47428,\"start\":47102},{\"end\":48163,\"start\":47475},{\"end\":48999,\"start\":48165},{\"end\":49340,\"start\":49001},{\"end\":49990,\"start\":49375},{\"end\":50740,\"start\":49992},{\"end\":51692,\"start\":50742},{\"end\":52165,\"start\":51807},{\"end\":53155,\"start\":52216},{\"end\":53283,\"start\":53157},{\"end\":53809,\"start\":53285},{\"end\":54221,\"start\":53811}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15104,\"start\":15022},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15507,\"start\":15463},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15596,\"start\":15507},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16388,\"start\":16327},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19842,\"start\":19783},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20784,\"start\":20744},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22151,\"start\":22020},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23073,\"start\":22856},{\"attributes\":{\"id\":\"formula_8\"},\"end\":36323,\"start\":36274},{\"attributes\":{\"id\":\"formula_9\"},\"end\":36572,\"start\":36483},{\"attributes\":{\"id\":\"formula_10\"},\"end\":36856,\"start\":36756},{\"attributes\":{\"id\":\"formula_11\"},\"end\":37232,\"start\":37152},{\"attributes\":{\"id\":\"formula_12\"},\"end\":37509,\"start\":37411},{\"attributes\":{\"id\":\"formula_13\"},\"end\":38226,\"start\":38183},{\"attributes\":{\"id\":\"formula_14\"},\"end\":38542,\"start\":38474},{\"attributes\":{\"id\":\"formula_15\"},\"end\":39489,\"start\":39305},{\"attributes\":{\"id\":\"formula_16\"},\"end\":41303,\"start\":41169},{\"attributes\":{\"id\":\"formula_17\"},\"end\":41767,\"start\":41693},{\"attributes\":{\"id\":\"formula_18\"},\"end\":42569,\"start\":42527},{\"attributes\":{\"id\":\"formula_19\"},\"end\":42863,\"start\":42767},{\"attributes\":{\"id\":\"formula_20\"},\"end\":43137,\"start\":42914},{\"attributes\":{\"id\":\"formula_21\"},\"end\":43187,\"start\":43166},{\"attributes\":{\"id\":\"formula_22\"},\"end\":43360,\"start\":43321},{\"attributes\":{\"id\":\"formula_23\"},\"end\":43502,\"start\":43463},{\"attributes\":{\"id\":\"formula_24\"},\"end\":44172,\"start\":44121},{\"attributes\":{\"id\":\"formula_25\"},\"end\":44229,\"start\":44172},{\"attributes\":{\"id\":\"formula_26\"},\"end\":45566,\"start\":45458},{\"attributes\":{\"id\":\"formula_27\"},\"end\":46288,\"start\":46245},{\"attributes\":{\"id\":\"formula_28\"},\"end\":46739,\"start\":46455},{\"attributes\":{\"id\":\"formula_29\"},\"end\":47101,\"start\":47053},{\"attributes\":{\"id\":\"formula_30\"},\"end\":52215,\"start\":52166}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1397,\"start\":1385},{\"attributes\":{\"n\":\"2\"},\"end\":7071,\"start\":7059},{\"end\":7867,\"start\":7831},{\"attributes\":{\"n\":\"3\"},\"end\":10363,\"start\":10350},{\"attributes\":{\"n\":\"4\"},\"end\":12655,\"start\":12608},{\"attributes\":{\"n\":\"4.1\"},\"end\":13218,\"start\":13185},{\"attributes\":{\"n\":\"4.2\"},\"end\":14268,\"start\":14229},{\"end\":15276,\"start\":15273},{\"end\":15347,\"start\":15344},{\"attributes\":{\"n\":\"5\"},\"end\":17999,\"start\":17979},{\"attributes\":{\"n\":\"5.1\"},\"end\":19030,\"start\":19004},{\"attributes\":{\"n\":\"5.2\"},\"end\":20529,\"start\":20496},{\"attributes\":{\"n\":\"5.3\"},\"end\":21519,\"start\":21483},{\"end\":23331,\"start\":23323},{\"attributes\":{\"n\":\"6\"},\"end\":24407,\"start\":24396},{\"attributes\":{\"n\":\"6.1\"},\"end\":24703,\"start\":24685},{\"attributes\":{\"n\":\"6.2\"},\"end\":25883,\"start\":25866},{\"attributes\":{\"n\":\"6.3\"},\"end\":27254,\"start\":27234},{\"attributes\":{\"n\":\"6.4\"},\"end\":31570,\"start\":31537},{\"attributes\":{\"n\":\"7\"},\"end\":33229,\"start\":33203},{\"end\":35707,\"start\":35677},{\"end\":37892,\"start\":37840},{\"end\":39173,\"start\":39149},{\"end\":40678,\"start\":40623},{\"end\":43165,\"start\":43139},{\"end\":44614,\"start\":44552},{\"end\":47473,\"start\":47431},{\"end\":49373,\"start\":49343},{\"end\":51728,\"start\":51695},{\"end\":51760,\"start\":51731},{\"end\":51805,\"start\":51763},{\"end\":54233,\"start\":54223},{\"end\":54404,\"start\":54394},{\"end\":54563,\"start\":54549},{\"end\":54826,\"start\":54816},{\"end\":55013,\"start\":54993}]", "table": "[{\"end\":55355,\"start\":55314},{\"end\":64659,\"start\":55982}]", "figure_caption": "[{\"end\":54392,\"start\":54235},{\"end\":54547,\"start\":54406},{\"end\":54814,\"start\":54565},{\"end\":54991,\"start\":54828},{\"end\":55304,\"start\":55016},{\"end\":55314,\"start\":55307},{\"end\":55982,\"start\":55358}]", "figure_ref": "[{\"end\":2102,\"start\":2094},{\"end\":5027,\"start\":5019},{\"end\":11994,\"start\":11986},{\"end\":12200,\"start\":12192},{\"end\":16828,\"start\":16820},{\"end\":17148,\"start\":17140},{\"end\":27801,\"start\":27793},{\"end\":28954,\"start\":28946},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29366,\"start\":29358},{\"end\":32318,\"start\":32310},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32622,\"start\":32614},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33005,\"start\":32987},{\"end\":45396,\"start\":45394},{\"end\":48834,\"start\":48826},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":51255,\"start\":51247},{\"end\":51335,\"start\":51327},{\"end\":53316,\"start\":53308}]", "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}