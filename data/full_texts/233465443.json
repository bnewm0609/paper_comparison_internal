{"id": 233465443, "updated": "2023-10-06 04:24:01.202", "metadata": {"title": "Energy Efficient In-memory Hyperdimensional Encoding for Spatio-temporal Signal Processing", "authors": "[{\"first\":\"Geethan\",\"last\":\"Karunaratne\",\"middle\":[]},{\"first\":\"Manuel\",\"last\":\"Gallo\",\"middle\":[\"Le\"]},{\"first\":\"Michael\",\"last\":\"Hersche\",\"middle\":[]},{\"first\":\"Giovanni\",\"last\":\"Cherubini\",\"middle\":[]},{\"first\":\"Luca\",\"last\":\"Benini\",\"middle\":[]},{\"first\":\"Abu\",\"last\":\"Sebastian\",\"middle\":[]},{\"first\":\"Abbas\",\"last\":\"Rahimi\",\"middle\":[]}]", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 68, no. 5, pp. 1725-1729, May 2021", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The emerging brain-inspired computing paradigm known as hyperdimensional computing (HDC) has been proven to provide a lightweight learning framework for various cognitive tasks compared to the widely used deep learning-based approaches. Spatio-temporal (ST) signal processing, which encompasses biosignals such as electromyography (EMG) and electroencephalography (EEG), is one family of applications that could benefit from an HDC-based learning framework. At the core of HDC lie manipulations and comparisons of large bit patterns, which are inherently ill-suited to conventional computing platforms based on the von-Neumann architecture. In this work, we propose an architecture for ST signal processing within the HDC framework using predominantly in-memory compute arrays. In particular, we introduce a methodology for the in-memory hyperdimensional encoding of ST data to be used together with an in-memory associative search module. We show that the in-memory HDC encoder for ST signals offers at least 1.80x energy efficiency gains, 3.36x area gains, as well as 9.74x throughput gains compared with a dedicated digital hardware implementation. At the same time it achieves a peak classification accuracy within 0.04% of that of the baseline HDC framework.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.11654", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2106-11654", "doi": "10.1109/tcsii.2021.3068126"}}, "content": {"source": {"pdf_hash": "483839b588da46fc3d8e91f3d58f8744c63e78bf", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.11654v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://zenodo.org/record/5301648/files/Y2021_TCAS_archive.pdf", "status": "GREEN"}}, "grobid": {"id": "f547e24454cda653f19cbd0a15e0df52badbf4fc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/483839b588da46fc3d8e91f3d58f8744c63e78bf.txt", "contents": "\nEnergy Efficient In-memory Hyperdimensional Encoding for Spatio-temporal Signal Processing\n\n\nGeethan Karunaratne \nIBM Research\nZ\u00fcrichSwitzerland\n\nETH Z\u00fcrich\nZ\u00fcrichSwitzerland\n\nManuel Le Gallo \nIBM Research\nZ\u00fcrichSwitzerland\n\nMichael Hersche \nIBM Research\nZ\u00fcrichSwitzerland\n\nETH Z\u00fcrich\nZ\u00fcrichSwitzerland\n\nGiovanni Cherubini \nIBM Research\nZ\u00fcrichSwitzerland\n\nLuca Benini \nETH Z\u00fcrich\nZ\u00fcrichSwitzerland\n\nAbu Sebastian \nIBM Research\nZ\u00fcrichSwitzerland\n\nAbbas Rahimi \nIBM Research\nZ\u00fcrichSwitzerland\n\nEnergy Efficient In-memory Hyperdimensional Encoding for Spatio-temporal Signal Processing\n10.1109/TCSII.2021.30681261Index Terms-Hyperdimensional computingIn-memory com- putingBiosignal processing\nThe emerging brain-inspired computing paradigm known as hyperdimensional computing (HDC) has been proven to provide a lightweight learning framework for various cognitive tasks compared to the widely used deep learning-based approaches. Spatio-temporal (ST) signal processing, which encompasses biosignals such as electromyography (EMG) and electroencephalography (EEG), is one family of applications that could benefit from an HDC-based learning framework. At the core of HDC lie manipulations and comparisons of large bit patterns, which are inherently ill-suited to conventional computing platforms based on the von-Neumann architecture. In this work, we propose an architecture for ST signal processing within the HDC framework using predominantly in-memory compute arrays. In particular, we introduce a methodology for the in-memory hyperdimensional encoding of ST data to be used together with an in-memory associative search module. We show that the in-memory HDC encoder for ST signals offers at least 1.80\u00d7 energy efficiency gains, 3.36\u00d7 area gains, as well as 9.74\u00d7 throughput gains compared with a dedicated digital hardware implementation. At the same time it achieves a peak classification accuracy within 0.04% of that of the baseline HDC framework.\n\nI. INTRODUCTION\n\nA LMOST all breakthroughs in artificial intelligence in the last decade are characterized by an underlying machine learning model that entails a higher complexity in terms of the number of operations and parameters compared to contemporary models. Such increased model complexity demands more energy to perform training and inference tasks. Nevertheless, the human brain, with its far-reaching cognitive capabilities, consumes several orders of magnitude less power. This disparity has paved the way to exploring brain-inspired alternatives. Hyperdimensional computing (HDC) [1] is one promising brain-inspired computing approach that relies on representing entities using high-dimensional (up to 10,000 dimensions) vectors called hypervectors. Similarly to the brain, where representations are spread across thousands of randomly originated neurons, a set of (pseudo)random orthogonal hypervectors forms the basis in the HDC framework. These hypervectors are then combined and compared using a welldefined set of algebraic operations to derive representations for composite entities and to find similarities, respectively.\n\nHDC has been deployed in a diverse set of application domains, for instance in solving Raven's progressive matrices [2], analogical reasoning [3], natural language processing [4], robotics [5], [6], text classification [7]- [10], activity recognition [11], DNA sequencing [12], and biosignal processing [13]- [18] (see [19] for an overview).\n\nA promising application domain for HDC is the spatiotemporal (ST) processing of signals, acquired by EMG sensors for example. The corresponding HDC algorithm was presented in [13], and later scaled up for high-density flexible EMG sensors [14], [18]. The same HDC algorithm has been used in a variety of applications such as EEG [16], iEEG [17], ExG [19] in general, as well as speech recognition [20], delivering higher classification accuracy than the established approaches. ST signal processing differs from other classes of applications because numerical data sequences received from multiple channels within a certain time window are considered as the input. Due to often being deployed at the edge of the Internet of Things and the confidential nature of the input data, the ST applications identified above could benefit immensely from energy-efficient hardware platforms.\n\nEarlier works address ST HDC signal processing in lowpower hardware platforms, such as PULP [15] and ARM Cortex-A53 [21]. For example, PULP-HD [15] describes the implementation of the ST HD encoder on multiple cores in a PULP cluster for achieving a target of 10 ms detection latency in real-time. Nevertheless, the energy efficiency could be improved further by using in-memory computing approaches [10], [22], [23]. In-memory computing is an emerging paradigm where the physical attributes of memory devices are exploited to compute in place [24]. Operations that require manipulation and comparison of large strings of bit patterns, which are at the core of the HDC framework, are particularly well suited for in-memory computing [10].\n\nIn this work, we propose an in-memory computing-based system for ST signal processing with HDC. An illustration of the system is given in Fig. 1, taking the EMG-based hand gesture recognition as a use case. Compared to prior work on in-memory HDC encoding [10], we present a novel inmemory computing HDC encoding architecture tailored for ST inputs. When coupled with an in-memory associative memory search module, such as the one presented in [10], we get a complete in-memory HDC processor for ST signal processing. We derive classification accuracy results from simulations using a statistical model of phase-change memory (PCM) \u00a92021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists,  Fig. 1. Concept of in-memory hyperdimensional encoding of spatio-temporal signals within the application of hand gesture recognition. First, the EMG signals are acquired from the electrodes connected to different parts of the subject's arm. After a pre-processing step, the data from different channels are embedded into one hypervector using spatial encoding and temporal encoding in a first memristive crossbar array, which is the focus of this work. The resulting query vector is passed to a second crossbar array to perform the associative memory search. The final result collected from the peripheral of the second crossbar predicts the class of the hand gesture.\n\ncrossbar arrays. Furthermore, we estimate the throughput, area, and energy efficiency of the in-memory ST HDC encoder and compare it with a dedicated digital encoder as well as a ST HDC encoder running on a low-power general purpose compute platform, PULP-HD [15].\n\n\nII. ALGORITHM A. Conventional ST HDC encoding algorithm\n\nIn the conventional ST HDC encoding algorithm [13], the input acquired from each channel goes through several preprocessing steps [15] and is converted to a stream of discrete time samples. An HDC encoder maintains a record of N consecutive time samples from M channels to generate an encoded N-gram hypervector embedding defined as:\nf : {s n,m } N\u00d7M \u2208 {l 1 , l 2 , ..., l L } N\u00d7M \u2192 G \u2208 {0, 1} D ,(1)\nwhere n \u2208 {1, 2, ..., N} is the relative time index, m \u2208 {1, 2, ..., M} denotes the source channel index and {l 1 , l 2 , ..., l L } are the L discrete quantization levels given in ascending order l 1 < l 2 < ... < l L . The output of the embedding function G is a D-dimensional binary hypervector.\n\nFirst, the encoder projects the sample values s n,m to a high dimensional space using a so-called continuous item memory (CiM ) [13]. As opposed to assigning quasi-orthogonal hypervectors to every unique discrete sample value, the CiM sets the Hamming distance (HamD) between two vectors to be proportional to the absolute difference between the corresponding sample values. This is achieved by choosing quasiorthogonal hypervectors for the minimum and maximum levels l 1 and l L , and hypervectors corresponding to the intermediate levels that satisfy:\nHamD(CiM (l i ), CiM (l j )) = F loor D l i \u2212 l j 2|l L \u2212 l 1 | (2)\nThe CiM -projected vectors are then bound to the relevant channel ID hypervector E m . The channel ID hypervectors are stored in an item memory (IM ) and are quasiorthogonal. The channel-bound hypervectors are given by I m,n = CiM (s n,m ) * E m , where * denotes the element-wise XOR binding operation. The channel-bound hypervectors are then bundled together to produce spatial hypervectors S n = M ajority(I 1,n , I 2,n , ..., I M,n ),\n\nwhere, for each dimension, the majority function outputs 1(0) if the majority of channels have 1(0). The spatial hypervectors S 1 , S 2 , ..., S N then enter a temporal encoding stage, which outputs the final encoded N-gram G according to the following equation:\nG t = \u03c1 N\u22121 S 1 * \u03c1 N\u22122 S 2 * ... * S N(4)\nwhere t is the time step at which the N th sample of the current data record enters the system, and \u03c1 is the vector permutation operator, which is implemented as a circular right shift. During the training phase, N-grams collected from the same class are further bundled to produce class prototype hypervectors P c , where c \u2208 {1, 2, ..., C}, and C is the number of classes. During the inference phase, the N-gram produced from the same encoder is called a query hypervector Q and used to measure binary dot product similarity against each of the prototype hypervectors P c . The class with the highest similarity is selected as the predicted class.\n\n\nB. Adaptations to suit in-memory computing\n\nWe propose several adaptations to the conventional algorithm to suit in-memory computing. First, all channel-bound hypervectors are pre-computed and unrolled using:  \nI l m = CiM (l) * E m \u2200l \u2208 {1, ..., L}, \u2200m \u2208 {1, ..., M} (5)(2) (3) (D) (1) (2)(1)T m = \u03c1 N\u22121 I 1,m * \u03c1 N\u22122 I 2,m * ... * I N,m(6)\ninstead of waiting for the spatial bundling given in (3). Finally, the hypervectors T m are bundled to produce the adapted Ngram hypervector G , given by:\nG t = M ajority(T 1 , T 2 , ..., T M )(7)\nIn summary, compared to the ST encoder in [13], the proposed in-memory ST encoding algorithm pre-computes channelbound hypervectors and pushes the bundling portion of the spatial encoding step downstream of the temporal encoding step. This offers further flexibility to set individual quantization levels and N-gram sizes per channel, which is not possible with the conventional encoder. This is a useful feature to exploit channel specific spatial and temporal dynamics.\n\n\nIII. ARCHITECTURE\n\nThe architecture of the in-memory ST HDC encoder is shown in Fig. 2. It consists of a memristive crossbar array and a few peripheral circuits, namely a circular buffer, a binder, and a bundler. The circular buffer is maintains the last N samples and reading them sequentially. It has M write pointers (wp 1 , ..., wp M ), synchronized to each one of the low-frequency external input channels that write data in parallel to the next allotted locations in the buffer. There is a single read pointer that is synchronized to the internal clock frequency which is set at least N M \u00d7 faster than the external frequency to avoid any data loss. The read pointer (rp) traverses the whole input data record sequentially: it samples in chronological order within each channel and repeats over all channels.\n\nAs shown in Fig. 2, the pre-computed channel-bound hypervectors I l m given in (5) are stored along the rows of the M \u00b7 L \u00d7 D crossbar array. This allows us to save D \u00b7 N \u00b7 M XOR operations per input data record. Performing temporal encoding on each channel separately allows us to reduce the number of intermediate buffers in the digital domain that must possess read/write capability at the expense of additional readonly storage in the PCM crossbar array. This is an acceptable trade-off because the PCM device consumes approximately 23 fJ of energy per read operation [25]. This is just a fraction of the energy incurred by digital read/write buffers. Furthermore, thanks to their non-volatile nature, PCM devices do not consume energy when retaining their content in idle mode. The output of the crossbar array is connected to an array of sense amplifiers. Sequential processing of the data record allows time sharing the sense amplifier array and the downstream binder module, allowing us to save a significant amount of energy and area in the peripherals. The binder module consists of an array of D XOR gates daisy-chained with D registers, which collectively implement (6).\n\nThe bundler module in the architecture implements (7). An optional scan chain, which propagates a random hypervector generated bit-by-bit from a linear feedback shift register, is activated at the start of the encoding cycle when the number of input channels is even, with ties that are broken randomly. The majority function is implemented as an array of log 2 Mbit accumulators, followed by an array of comparators whose reference is set at ceil((M + 1)/2) \u2212 0.5.\n\nThe controller module receives the encoding parameters and coordinates the flow across the rest of the modules. For example, it communicates the start/end addresses for each of the write pointers, the offset value added to circular buffer read data to derive the row address in the crossbar array, when to update the 1-bit register array in the binder module, when to update accumulators in the bundler, etc.\n\n\nIV. RESULTS AND DISCUSSION\n\n\nA. Experimental setup\n\nThe proposed in-memory ST HDC architecture is benchmarked on the EMG hand gesture recognition dataset [13]. It includes data acquired from five subjects who perform five classes of gestures. The data is sampled at a 500 Hz frequency via four EMG electrodes attached to each subject's forearm. The class label and channel readings are provided at each time frame. We use 25% of the 175\u00d7 down-sampled data to train a 10,000-D HDC model for each subject and test on 800 queries on average per subject. The result is averaged across the subjects to obtain the classification accuracy. For PCM simulations, the statistical model described in [10] is used, which captures non-ideal effects such as spatial and temporal variations in the PCM crossbar array. Fig. 3 shows the classification accuracy obtained from an inmemory ST encoder running in software, as well as the same encoder simulated using the statistical model, and comparing these results with the baseline ST encoder [13]. In all three models of encoders, the in-memory associative memory search module is also simulated with the statistical model. The proposed in-memory ST encoder achieves a peak accuracy of 98.9% (see Fig. 3(a)) when N=9, L=15. This is only 0.04% lower than the peak accuracy in the baseline ST encoder. It is also a 1.1% improvement over the peak accuracy of 97.8% reported in the reference encoder [13], which uses the binding result of two channels to break ties while performing the associative memory search using Hamming distance.\n\n\nB. Classification accuracy results\n\nAs the N-gram size decreases, the spatial encoding plays a more prominent role than the temporal encoding. Thus, the higher accuracy delivered by the in-memory ST encoder compared to the conventional ST encoder (see Fig. 3(a)) for smaller N-gram sizes can be explained by the spatial bundling operation being relocated downstream in the in-memory ST encoder. This allows retaining more useful spatial information in the encoded N-gram hypervector. Fig. 3(b) shows that the in-memory encoder simulated with the statistical model exhibits an increasing accuracy drop, compared to the same encoder running in software, as the quantization levels increase. This is because as the quantization levels are increased, the PCM crossbar array size increases linearly, thereby amplifying the negative effect of spatial PCM variations on the classification accuracy. However given that the in-memory ST encoding operations in Equations (5) to (7) involve element-wise operations, or operations that involve neighboring elements, the crossbar array can be easily split into several realistic size [26] subarrays with simple single wire connectivity between subarray peripherals. This facilitates the silicon realization with negligible additional cost in terms of energy and area, as well as the mitigation of the effect of PCM spatial variations by compensating for subarray level conductance variation.\n\n\nC. Energy efficiency study and benchmark\n\nWe performed an energy efficiency study of the in-memory ST encoder. The binary PCM device specifications given in  [10] are used as the reference for obtaining power and timing numbers of the crossbars. The power and timing numbers for the digital peripherals are obtained from component-wise simulations of a post-synthesis netlist generated with 65nm CMOS technology. For comparison, we considered an equivalent digital ST encoder operating entirely in CMOS, whose power and timing numbers are obtained from a componentwise simulation of the post-synthesis netlist generated at the same technology node. Both digital peripherals and the equivalent CMOS encoder operate at 440 MHz and 1.2 V supply voltage. We observe that the in-memory ST encoder is able to produce 31.5M, 18.9M, and 10.5M N-grams/s for N-gram sizes of 3, 5, and 9, respectively, which is a throughput improvement of 9.74\u00d7 over the digital counterpart and a 0.28M\u00d7 improvement over the 10 ms fixed latency PULP-HD [15]. The total area of the in-memory ST encoder varies from 0.37 to 0.44 and 0.51 mm 2 when quantization levels are set to 3, 12 and 21, respectively. This is a 3.36\u00d7, 5.46\u00d7, and 6.97\u00d7 area reduction, respectively, compared to the digital CMOS ST encoder. When breaking down the area numbers further we find that, irrespective of the number of quantization levels, a fixed area of 0.32 mm 2 is occupied by the digital peripheral logic including the circular buffer, the binder and the bundler; another area of 0.02 mm 2 is occupied by the row decoders and the sense amplifiers; while the rest of the area is taken by the PCM device array itself.\n\nWe estimated the energy efficiency of the in-memory ST encoder and compared it with the digital ST encoder as shown in Fig. 4. The in-memory ST encoder achieves a peak energy efficiency of 75.1M N-grams/s/W with N=3 and L=3. The total energy required for encoding an N-gram using this configuration is 13.3 nJ, 91.4% of which are spent on digital peripheral circuits, 8.4% on sense amplifiers/row decoders, and a mere 0.12% on PCM devices. This results in a 1.80\u00d7 energy efficiency gain compared to a similarly configured digital ST encoder. The gain improves to a maximum of 8.83\u00d7 as the N-gram sizes and quantization levels are increased (see Fig. 4). Table I presents physical and performance characteristics of 1-core and 4-core PULP-HD encoders compared with a dedicated digital CMOS encoder and the proposed PCM-based in-memory ST encoder with the same parameter configuration (N = 3, L = 21). The energy required for N-gram encoding is reduced from the \u00b5J range for the PULP-HD implementations to the nJ range for the dedicated CMOS and PCM-based inmemory encoders. In summary, when compared with 1-core and 4-core PULP-HD encoders, the in-memory ST encoder achieves 1320\u00d7 and 284\u00d7 higher energy efficiency, respectively.\n\n\nV. CONCLUSION\n\nIn this paper, we have demonstrated HDC encoding on spatio-temporal signals using in-memory computing techniques on memristive crossbar arrays. This approach allows selecting separate parameter combinations for each channel, further enhancing the flexibility of the encoding process. By simulating our architecture with a phase-change memory statistical model, we obtain a peak classification accuracy of 98.9% (within 0.04% of the baseline), while achieving 1.80\u00d7-8.83\u00d7 higher energy efficiency over a dedicated digital CMOS encoder and a 284\u00d7 gain energy efficiency over an encoder running on a low-power general purpose computing platform.\n\nFig. 3 .\n3Classification accuracies obtained from a baseline software ST encoder, an in-memory friendly ST encoder running in software, and an in-memory ST encoder simulated using the PCM statistical model. In (a), accuracy variation with N-gram size for a fixed quantization level is plotted. In (b), accuracy variation with quantization levels for a fixed N-gram size is plotted.\n\n\nor reuse of any copyrighted component of this work in other works. IEEE Transactions on Circuits and Systems II: Express Briefs DOI: 10.1109/TCSII.2021.3068126 URL: https://ieeexplore.ieee.org/document/9387248 arXiv:2106.11654v1 [cs.ET] 22 Jun 2021Channel 1 \n\nChannel 2 \n\nChannel 3 \n\nChannel M \n\nIn-memory \nSpatiotemporal HD Encoding \nIn-memory \nAssociative Memory Search \n\nPreprocessing \n\nGesture Classes \n\nSpatio-temporal Encoding \nAssociative Memory Search \n\nEncoded \nN-gram \nHypervector \n\nQ \nP 1 P 2 \nP C \n\nIM \n\n+ \n\n\u03c1 \n\n\u03c1 \n\n\u03c1 \n\nComparator \n\n1 \n\n2 \n\n3 \n\n4 \n\nC \n\nOn-chip Accelerator \n\n* \n\n* \n\n* \n* \n\n* \n\n(1) \n(2) \n\n(L) \n\nCrossbar I \n\nPeripheral \n\nP e r i p h \n\ne \nr \na \nl \n\nCrossbar II \n\nPeripheral \n\nP e ri p h e ra \n\nl \n\n(1) \n\n(2) \n\n(N) \n\nCiM \n\n\n\nTABLE I ENCODER\nIPERFORMANCE COMPARISONPULPv3 \n1 core \n\nPULPv3 \n4 cores \nCMOS \nPCM \n\nTechnology (nm) \n28 \n28 \n65  *  \n65  *  \nSupply Voltage (V) \n0.7 \n0.5 \n1.2  *  \n1.2  *  \nFrequency (Hz) \n53.3M \n14.3M \n440M  *  \n440M  *  \nThroughput (Ngram/s) \n108 \n111 \n3.23M \n31.5M \nCore Power (mW) \n1.90 \n0.42 \n175  *  \n-\nEnergy (J/Ngram) \n17.5\u00b5 \n3.79\u00b5 \n54.1n \n13.3n \n\n *  fast clock CMOS components \n\n\n\nHyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors,\" Cognitive Computation, vol. 1, no. 2, pp. 139-159, 2009.\n\nAnalogical mapping and inference with binary spatter codes and sparse distributed memory. B Emruli, The 2013 International Joint Conference on Neural Networks (IJCNN). B. Emruli et al., \"Analogical mapping and inference with binary spatter codes and sparse distributed memory,\" in The 2013 International Joint Conference on Neural Networks (IJCNN), 2013, pp. 1-8.\n\nWhat we mean when we say \"what's the dollar of mexico?\": Prototypes and mapping in concept space. P Kanerva, AAAI Fall Symposium: Quantum Informatics for Cognitive, Social, and Semantic Processes. P. Kanerva, \"What we mean when we say \"what's the dollar of mexico?\": Prototypes and mapping in concept space,\" in AAAI Fall Symposium: Quantum Informatics for Cognitive, Social, and Semantic Processes, 2010, pp. 2-6.\n\nHyperembed: Tradeoffs between resources and performance in nlp tasks with hyperdimensional computing enabled embedding of n-gram statistics. P Alonso, arXiv:2003.01821arXiv preprintP. Alonso et al., \"Hyperembed: Tradeoffs between resources and performance in nlp tasks with hyperdimensional computing enabled embedding of n-gram statistics,\" arXiv preprint arXiv:2003.01821, 2020.\n\nLearning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception. A Mitrokhin, Science Robotics. 430A. Mitrokhin et al., \"Learning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception,\" Science Robotics, vol. 4, no. 30, 2019.\n\nIntegrating event-based dynamic vision sensors with sparse hyperdimensional computing: A low-power accelerator with online learning capability. M Hersche, Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design, ser. ISLPED '20. the ACM/IEEE International Symposium on Low Power Electronics and Design, ser. ISLPED '20New York, NY, USAAssociation for Computing MachineryM. Hersche et al., \"Integrating event-based dynamic vision sensors with sparse hyperdimensional computing: A low-power accelerator with online learning capability,\" in Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design, ser. ISLPED '20. New York, NY, USA: Association for Computing Machinery, 2020, p. 169-174.\n\nLanguage geometry using random indexing. A Joshi, Quantum Interaction, J. A. de Barros et al.Springer International PublishingA. Joshi et al., \"Language geometry using random indexing,\" in Quantum Interaction, J. A. de Barros et al., Eds. Cham: Springer International Publishing, 2017, pp. 265-274.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. A Rahimi, Proceedings of the 2016 International Symposium on Low Power Electronics and Design, ser. ISLPED '16. the 2016 International Symposium on Low Power Electronics and Design, ser. ISLPED '16New York, NY, USAACMA. Rahimi et al., \"A robust and energy-efficient classifier using brain-inspired hyperdimensional computing,\" in Proceedings of the 2016 International Symposium on Low Power Electronics and Design, ser. ISLPED '16. New York, NY, USA: ACM, 2016, pp. 64-69.\n\nHyperdimensional computing for text classification. F R Najafabadi, ACM/IEEE Design, Automation, and Test in Europe Conference (DATE). F. R. Najafabadi et al., \"Hyperdimensional computing for text classification,\" ACM/IEEE Design, Automation, and Test in Europe Conference (DATE), March 2016.\n\nIn-memory hyperdimensional computing. G Karunaratne, Nature Electronics. 36G. Karunaratne et al., \"In-memory hyperdimensional computing,\" Nature Electronics, vol. 3, no. 6, pp. 327-337, 2020.\n\nModeling dependencies in multiple parallel data streams with hyperdimensional computing. O R\u00e4s\u00e4nen, IEEE Signal Processing Letters. 217O. R\u00e4s\u00e4nen et al., \"Modeling dependencies in multiple parallel data streams with hyperdimensional computing,\" IEEE Signal Processing Letters, vol. 21, no. 7, pp. 899-903, 2014.\n\nHdna: Energy-efficient dna sequencing using hyperdimensional computing. M Imani, 2018 IEEE EMBS International Conference on Biomedical Health Informatics (BHI). M. Imani et al., \"Hdna: Energy-efficient dna sequencing using hyperdi- mensional computing,\" in 2018 IEEE EMBS International Conference on Biomedical Health Informatics (BHI), 2018, pp. 271-274.\n\nHyperdimensional biosignal processing: A case study for emg-based hand gesture recognition. A Rahimi, 2016 IEEE International Conference on Rebooting Computing (ICRC). A. Rahimi et al., \"Hyperdimensional biosignal processing: A case study for emg-based hand gesture recognition,\" in 2016 IEEE International Conference on Rebooting Computing (ICRC), 2016, pp. 1-8.\n\nAn emg gesture recognition system with flexible highdensity sensors and brain-inspired high-dimensional classifier. A Moin, 2018 IEEE International Symposium on Circuits and Systems (ISCAS). A. Moin et al., \"An emg gesture recognition system with flexible high- density sensors and brain-inspired high-dimensional classifier,\" in 2018 IEEE International Symposium on Circuits and Systems (ISCAS), 2018, pp. 1-5.\n\nPulp-hd: Accelerating brain-inspired highdimensional computing on a parallel ultra-low power platform. F Montagna, Proceedings of the 55th Annual Design Automation Conference, ser. DAC '18. the 55th Annual Design Automation Conference, ser. DAC '18New York, NY, USAACM111F. Montagna et al., \"Pulp-hd: Accelerating brain-inspired high- dimensional computing on a parallel ultra-low power platform,\" in Proceedings of the 55th Annual Design Automation Conference, ser. DAC '18. New York, NY, USA: ACM, 2018, pp. 111:1-111:6.\n\nHyperdimensional computing for noninvasive braincomputer interfaces: Blind and one-shot classification of eeg errorrelated potentials. A Rahimi, 10th EAI Int. Conf. on Bio-inspired Information and Communications Technologies. A. Rahimi et al., \"Hyperdimensional computing for noninvasive brain- computer interfaces: Blind and one-shot classification of eeg error- related potentials,\" 10th EAI Int. Conf. on Bio-inspired Information and Communications Technologies, 2017.\n\nOne-shot learning for ieeg seizure detection using end-to-end binary operations: Local binary patterns with hyperdimensional computing. A Burrello, 2018 IEEE Biomedical Circuits and Systems Conference. A. Burrello et al., \"One-shot learning for ieeg seizure detection using end-to-end binary operations: Local binary patterns with hyperdimen- sional computing,\" in 2018 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2018, pp. 1-4.\n\nA wearable biosensing system with in-sensor adaptive machine learning for hand gesture recognition. A Moin, Nature Electronics. 41A. Moin et al., \"A wearable biosensing system with in-sensor adaptive machine learning for hand gesture recognition,\" Nature Electronics, vol. 4, no. 1, pp. 54-63, 2021.\n\nEfficient biosignal processing using hyperdimensional computing: Network templates for combined learning and classification of exg signals. A Rahimi, Proceedings of the IEEE. 1071A. Rahimi et al., \"Efficient biosignal processing using hyperdimensional computing: Network templates for combined learning and classification of exg signals,\" Proceedings of the IEEE, vol. 107, no. 1, pp. 123-143, 2019.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, 2017 IEEE International Conference on Rebooting Computing (ICRC. M. Imani et al., \"Voicehd: Hyperdimensional computing for efficient speech recognition,\" in 2017 IEEE International Conference on Reboot- ing Computing (ICRC), 2017, pp. 1-8.\n\nEfficient human activity recognition using hyperdimensional computing. Y Kim, Proceedings of the 8th International Conference on the Internet of Things, ser. IOT '18. the 8th International Conference on the Internet of Things, ser. IOT '18New York, NY, USAAssociation for Computing MachineryY. Kim et al., \"Efficient human activity recognition using hyperdimensional computing,\" in Proceedings of the 8th International Conference on the Internet of Things, ser. IOT '18. New York, NY, USA: Association for Computing Machinery, 2018.\n\nDevice-architecture co-design for hyperdimensional computing with 3d vertical resistive switching random access memory (3d vrram). H Li, 2017 International Symposium on VLSI Technology, Systems and Application. VLSI-TSAH. Li et al., \"Device-architecture co-design for hyperdimensional com- puting with 3d vertical resistive switching random access memory (3d vrram),\" in 2017 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA), 2017, pp. 1-2.\n\nBrain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study. T F Wu, 2018 IEEE International Solid -State Circuits Conference -(ISSCC). T. F. Wu et al., \"Brain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study,\" in 2018 IEEE International Solid -State Circuits Conference -(ISSCC), 2018, pp. 492-494.\n\nMemory devices and applications for in-memory computing. A Sebastian, Nature Nanotechnology. 15A. Sebastian et al., \"Memory devices and applications for in-memory computing,\" Nature Nanotechnology, vol. 15, pp. 529-544, 2020.\n\nMixed-precision deep learning based on computational memory. S R Nandakumar, Frontiers in Neuroscience. 14406S. R. Nandakumar et al., \"Mixed-precision deep learning based on computational memory,\" Frontiers in Neuroscience, vol. 14, p. 406, 2020.\n\nMixed-precision in-memory computing. M , Le Gallo, Nature Electronics. 14M. Le Gallo et al., \"Mixed-precision in-memory computing,\" Nature Electronics, vol. 1, no. 4, pp. 246-253, 2018.\n", "annotations": {"author": "[{\"end\":176,\"start\":94},{\"end\":225,\"start\":177},{\"end\":304,\"start\":226},{\"end\":356,\"start\":305},{\"end\":399,\"start\":357},{\"end\":446,\"start\":400},{\"end\":492,\"start\":447}]", "publisher": null, "author_last_name": "[{\"end\":113,\"start\":102},{\"end\":192,\"start\":184},{\"end\":241,\"start\":234},{\"end\":323,\"start\":314},{\"end\":368,\"start\":362},{\"end\":413,\"start\":404},{\"end\":459,\"start\":453}]", "author_first_name": "[{\"end\":101,\"start\":94},{\"end\":183,\"start\":177},{\"end\":233,\"start\":226},{\"end\":313,\"start\":305},{\"end\":361,\"start\":357},{\"end\":403,\"start\":400},{\"end\":452,\"start\":447}]", "author_affiliation": "[{\"end\":145,\"start\":115},{\"end\":175,\"start\":147},{\"end\":224,\"start\":194},{\"end\":273,\"start\":243},{\"end\":303,\"start\":275},{\"end\":355,\"start\":325},{\"end\":398,\"start\":370},{\"end\":445,\"start\":415},{\"end\":491,\"start\":461}]", "title": "[{\"end\":91,\"start\":1},{\"end\":583,\"start\":493}]", "venue": null, "abstract": "[{\"end\":1954,\"start\":691}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2551,\"start\":2548},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3217,\"start\":3214},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3243,\"start\":3240},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3276,\"start\":3273},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3290,\"start\":3287},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3295,\"start\":3292},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3320,\"start\":3317},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3326,\"start\":3322},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3353,\"start\":3349},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3374,\"start\":3370},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3405,\"start\":3401},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3411,\"start\":3407},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3421,\"start\":3417},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3620,\"start\":3616},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3684,\"start\":3680},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3690,\"start\":3686},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3774,\"start\":3770},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3785,\"start\":3781},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3795,\"start\":3791},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3842,\"start\":3838},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4419,\"start\":4415},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4443,\"start\":4439},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4470,\"start\":4466},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4727,\"start\":4723},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4733,\"start\":4729},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4739,\"start\":4735},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4871,\"start\":4867},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5060,\"start\":5056},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5323,\"start\":5319},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5511,\"start\":5507},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6945,\"start\":6941},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7056,\"start\":7052},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7140,\"start\":7136},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7839,\"start\":7835},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10125,\"start\":10122},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10312,\"start\":10308},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12132,\"start\":12128},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12737,\"start\":12734},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13776,\"start\":13772},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14311,\"start\":14307},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14648,\"start\":14644},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15052,\"start\":15048},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16312,\"start\":16308},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17648,\"start\":17644}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":20563,\"start\":20181},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":21315,\"start\":20564},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":21707,\"start\":21316}]", "paragraph": "[{\"end\":3096,\"start\":1973},{\"end\":3439,\"start\":3098},{\"end\":4321,\"start\":3441},{\"end\":5061,\"start\":4323},{\"end\":6680,\"start\":5063},{\"end\":6946,\"start\":6682},{\"end\":7339,\"start\":7006},{\"end\":7705,\"start\":7407},{\"end\":8260,\"start\":7707},{\"end\":8767,\"start\":8329},{\"end\":9031,\"start\":8769},{\"end\":9724,\"start\":9075},{\"end\":9937,\"start\":9771},{\"end\":10223,\"start\":10069},{\"end\":10737,\"start\":10266},{\"end\":11554,\"start\":10759},{\"end\":12738,\"start\":11556},{\"end\":13205,\"start\":12740},{\"end\":13615,\"start\":13207},{\"end\":15184,\"start\":13670},{\"end\":16615,\"start\":15223},{\"end\":18290,\"start\":16660},{\"end\":19520,\"start\":18292},{\"end\":20180,\"start\":19538}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7406,\"start\":7340},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8328,\"start\":8261},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9074,\"start\":9032},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9998,\"start\":9938},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10020,\"start\":9998},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10068,\"start\":10020},{\"attributes\":{\"id\":\"formula_8\"},\"end\":10265,\"start\":10224}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18953,\"start\":18946}]", "section_header": "[{\"end\":1971,\"start\":1956},{\"end\":7004,\"start\":6949},{\"end\":9769,\"start\":9727},{\"end\":10757,\"start\":10740},{\"end\":13644,\"start\":13618},{\"end\":13668,\"start\":13647},{\"end\":15221,\"start\":15187},{\"end\":16658,\"start\":16618},{\"end\":19536,\"start\":19523},{\"end\":20190,\"start\":20182},{\"end\":21332,\"start\":21317}]", "table": "[{\"end\":21315,\"start\":20814},{\"end\":21707,\"start\":21356}]", "figure_caption": "[{\"end\":20563,\"start\":20192},{\"end\":20814,\"start\":20566},{\"end\":21356,\"start\":21334}]", "figure_ref": "[{\"end\":5207,\"start\":5201},{\"end\":6018,\"start\":6012},{\"end\":10826,\"start\":10820},{\"end\":11574,\"start\":11568},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14427,\"start\":14421},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14858,\"start\":14849},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15448,\"start\":15439},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15680,\"start\":15671},{\"end\":18417,\"start\":18411},{\"end\":18943,\"start\":18937}]", "bib_author_first_name": "[{\"end\":21835,\"start\":21834},{\"end\":22158,\"start\":22157},{\"end\":22531,\"start\":22530},{\"end\":22990,\"start\":22989},{\"end\":23331,\"start\":23330},{\"end\":23673,\"start\":23672},{\"end\":24321,\"start\":24320},{\"end\":24670,\"start\":24669},{\"end\":25196,\"start\":25195},{\"end\":25198,\"start\":25197},{\"end\":25476,\"start\":25475},{\"end\":25720,\"start\":25719},{\"end\":26016,\"start\":26015},{\"end\":26393,\"start\":26392},{\"end\":26782,\"start\":26781},{\"end\":27182,\"start\":27181},{\"end\":27738,\"start\":27737},{\"end\":28212,\"start\":28211},{\"end\":28620,\"start\":28619},{\"end\":28961,\"start\":28960},{\"end\":29292,\"start\":29291},{\"end\":29613,\"start\":29612},{\"end\":30207,\"start\":30206},{\"end\":30663,\"start\":30662},{\"end\":30665,\"start\":30664},{\"end\":31019,\"start\":31018},{\"end\":31250,\"start\":31249},{\"end\":31252,\"start\":31251},{\"end\":31474,\"start\":31473},{\"end\":31479,\"start\":31477}]", "bib_author_last_name": "[{\"end\":21843,\"start\":21836},{\"end\":22165,\"start\":22159},{\"end\":22539,\"start\":22532},{\"end\":22997,\"start\":22991},{\"end\":23341,\"start\":23332},{\"end\":23681,\"start\":23674},{\"end\":24327,\"start\":24322},{\"end\":24677,\"start\":24671},{\"end\":25209,\"start\":25199},{\"end\":25488,\"start\":25477},{\"end\":25728,\"start\":25721},{\"end\":26022,\"start\":26017},{\"end\":26400,\"start\":26394},{\"end\":26787,\"start\":26783},{\"end\":27191,\"start\":27183},{\"end\":27745,\"start\":27739},{\"end\":28221,\"start\":28213},{\"end\":28625,\"start\":28621},{\"end\":28968,\"start\":28962},{\"end\":29298,\"start\":29293},{\"end\":29617,\"start\":29614},{\"end\":30210,\"start\":30208},{\"end\":30668,\"start\":30666},{\"end\":31029,\"start\":31020},{\"end\":31263,\"start\":31253},{\"end\":31485,\"start\":31480}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":733980},\"end\":22065,\"start\":21709},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15654769},\"end\":22430,\"start\":22067},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7149851},\"end\":22846,\"start\":22432},{\"attributes\":{\"doi\":\"arXiv:2003.01821\",\"id\":\"b3\"},\"end\":23228,\"start\":22848},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":182118830},\"end\":23526,\"start\":23230},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":220635307},\"end\":24277,\"start\":23528},{\"attributes\":{\"id\":\"b6\"},\"end\":24577,\"start\":24279},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9812826},\"end\":25141,\"start\":24579},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":201625025},\"end\":25435,\"start\":25143},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":174797921},\"end\":25628,\"start\":25437},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1690456},\"end\":25941,\"start\":25630},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4708051},\"end\":26298,\"start\":25943},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":12008695},\"end\":26663,\"start\":26300},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3553418},\"end\":27076,\"start\":26665},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52812586},\"end\":27600,\"start\":27078},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8996877},\"end\":28073,\"start\":27602},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52168429},\"end\":28517,\"start\":28075},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":231705788},\"end\":28818,\"start\":28519},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":57365377},\"end\":29219,\"start\":28820},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":21351739},\"end\":29539,\"start\":29221},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":52978766},\"end\":30073,\"start\":29541},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10674378},\"end\":30545,\"start\":30075},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3869844},\"end\":30959,\"start\":30547},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":214704544},\"end\":31186,\"start\":30961},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":211004036},\"end\":31434,\"start\":31188},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4925786},\"end\":31621,\"start\":31436}]", "bib_title": "[{\"end\":21832,\"start\":21709},{\"end\":22155,\"start\":22067},{\"end\":22528,\"start\":22432},{\"end\":23328,\"start\":23230},{\"end\":23670,\"start\":23528},{\"end\":24667,\"start\":24579},{\"end\":25193,\"start\":25143},{\"end\":25473,\"start\":25437},{\"end\":25717,\"start\":25630},{\"end\":26013,\"start\":25943},{\"end\":26390,\"start\":26300},{\"end\":26779,\"start\":26665},{\"end\":27179,\"start\":27078},{\"end\":27735,\"start\":27602},{\"end\":28209,\"start\":28075},{\"end\":28617,\"start\":28519},{\"end\":28958,\"start\":28820},{\"end\":29289,\"start\":29221},{\"end\":29610,\"start\":29541},{\"end\":30204,\"start\":30075},{\"end\":30660,\"start\":30547},{\"end\":31016,\"start\":30961},{\"end\":31247,\"start\":31188},{\"end\":31471,\"start\":31436}]", "bib_author": "[{\"end\":21845,\"start\":21834},{\"end\":22167,\"start\":22157},{\"end\":22541,\"start\":22530},{\"end\":22999,\"start\":22989},{\"end\":23343,\"start\":23330},{\"end\":23683,\"start\":23672},{\"end\":24329,\"start\":24320},{\"end\":24679,\"start\":24669},{\"end\":25211,\"start\":25195},{\"end\":25490,\"start\":25475},{\"end\":25730,\"start\":25719},{\"end\":26024,\"start\":26015},{\"end\":26402,\"start\":26392},{\"end\":26789,\"start\":26781},{\"end\":27193,\"start\":27181},{\"end\":27747,\"start\":27737},{\"end\":28223,\"start\":28211},{\"end\":28627,\"start\":28619},{\"end\":28970,\"start\":28960},{\"end\":29300,\"start\":29291},{\"end\":29619,\"start\":29612},{\"end\":30212,\"start\":30206},{\"end\":30670,\"start\":30662},{\"end\":31031,\"start\":31018},{\"end\":31265,\"start\":31249},{\"end\":31477,\"start\":31473},{\"end\":31487,\"start\":31477}]", "bib_venue": "[{\"end\":21866,\"start\":21845},{\"end\":22233,\"start\":22167},{\"end\":22627,\"start\":22541},{\"end\":22987,\"start\":22848},{\"end\":23359,\"start\":23343},{\"end\":23787,\"start\":23683},{\"end\":24318,\"start\":24279},{\"end\":24779,\"start\":24679},{\"end\":25276,\"start\":25211},{\"end\":25508,\"start\":25490},{\"end\":25760,\"start\":25730},{\"end\":26102,\"start\":26024},{\"end\":26466,\"start\":26402},{\"end\":26854,\"start\":26789},{\"end\":27266,\"start\":27193},{\"end\":27826,\"start\":27747},{\"end\":28275,\"start\":28223},{\"end\":28645,\"start\":28627},{\"end\":28993,\"start\":28970},{\"end\":29363,\"start\":29300},{\"end\":29706,\"start\":29619},{\"end\":30284,\"start\":30212},{\"end\":30735,\"start\":30670},{\"end\":31052,\"start\":31031},{\"end\":31290,\"start\":31265},{\"end\":31505,\"start\":31487},{\"end\":23895,\"start\":23789},{\"end\":24883,\"start\":24781},{\"end\":27343,\"start\":27268},{\"end\":29797,\"start\":29708}]"}}}, "year": 2023, "month": 12, "day": 17}