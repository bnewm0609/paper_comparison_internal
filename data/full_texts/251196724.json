{"id": 251196724, "updated": "2023-10-05 11:53:04.955", "metadata": {"title": "Dataset and Evaluation algorithm design for GOALS Challenge", "authors": "[{\"first\":\"Huihui\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Fei\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Huazhu\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Junde\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Xiulan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yanwu\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Glaucoma causes irreversible vision loss due to damage to the optic nerve, and there is no cure for glaucoma.OCT imaging modality is an essential technique for assessing glaucomatous damage since it aids in quantifying fundus structures. To promote the research of AI technology in the field of OCT-assisted diagnosis of glaucoma, we held a Glaucoma OCT Analysis and Layer Segmentation (GOALS) Challenge in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022 to provide data and corresponding annotations for researchers studying layer segmentation from OCT images and the classification of glaucoma. This paper describes the released 300 circumpapillary OCT images, the baselines of the two sub-tasks, and the evaluation methodology. The GOALS Challenge is accessible at https://aistudio.baidu.com/aistudio/competition/detail/230.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.14447", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/miccai/FangLFWZX22", "doi": "10.48550/arxiv.2207.14447"}}, "content": {"source": {"pdf_hash": "2bdc0c1a3e4dbd8bbf1b26c9914a8832fd2f599e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2207.14447v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4fcc1ee941286cd28eafbb47c13f0653e0b55058", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2bdc0c1a3e4dbd8bbf1b26c9914a8832fd2f599e.txt", "contents": "\nDataset and Evaluation algorithm design for GOALS Challenge\n\n\nHuihui Fang \nIntelligent Healthcare Unit\nBaidu Inc\nBeijingChina\n\nFei Li \nState Key Laboratory of Ophthalmology, Zhongshan Ophthalmic Center, Sun Yat-sen University, Guangdong Provincial Key Laboratory of Ophthalmology and Visual Science\nGuangzhouChina\n\nHuazhu Fu \nInstitute of High Performance Computing, Agency for Science, Technology and Research\nSingapore\n\nJunde Wu \nIntelligent Healthcare Unit\nBaidu Inc\nBeijingChina\n\nXiulan Zhang zhangxl2@mail.sysu.edu.cn \nState Key Laboratory of Ophthalmology, Zhongshan Ophthalmic Center, Sun Yat-sen University, Guangdong Provincial Key Laboratory of Ophthalmology and Visual Science\nGuangzhouChina\n\nYanwu Xu ywxu@ieee.org \nIntelligent Healthcare Unit\nBaidu Inc\nBeijingChina\n\nDataset and Evaluation algorithm design for GOALS Challenge\nGOALS Challenge \u00b7 glaucoma classification \u00b7 OCT layer segmentation \u00b7 Circumpapillary OCT\nGlaucoma causes irreversible vision loss due to damage to the optic nerve, and there is no cure for glaucoma.OCT imaging modality is an essential technique for assessing glaucomatous damage since it aids in quantifying fundus structures. To promote the research of AI technology in the field of OCT-assisted diagnosis of glaucoma, we held a Glaucoma OCT Analysis and Layer Segmentation (GOALS) Challenge in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022 to provide data and corresponding annotations for researchers studying layer segmentation from OCT images and the classification of glaucoma. This paper describes the released 300 circumpapillary OCT images, the baselines of the two sub-tasks, and the evaluation methodology. The GOALS Challenge is accessible at https://aistudio.baidu.com/aistudio/competition/ detail/230.\n\nIntroduction\n\nGlaucoma is a chronic neurodegenerative condition that is one of the leading causes of irreversible blindness in the world. It is a multifactorial optic neuropathy characterized by progressive neurodegeneration of retinal ganglion cells (RGCs) and their axons, resulting in retinal nerve fiber layer (RNFL) attenuation, a specific pattern of damage to the optic nerve head, and visual field loss [1]. In 2020, about 80 million people have glaucoma worldwide [2], and this number is projected to be 111.8 million in 2040 [3]. Optical coherence tomography (OCT) is a powerful tool for diagnosing ocular diseases because of its no radiation, non-invasive, high resolution, high detection sensitivity and other characteristics [4,5]. In contrast to color fundus images, which can only provide information about the surface of the retina, OCT images can provide crosssectional information about fundus structures. The retinal structures contain RNFL, ganglion cell-inner plexiform layer (GCIPL), inner nuclear layer (INL), outer plexiform layer (OPL), outer nuclear layer (ONL), external limiting membrane (ELM), inner photoreceptor segment, inner/outer photoreceptor segment junction, outer photoreceptor segment, retinal pigment epithelium (RPE) interdigitation, RPE/Bruch's membrane complex, as well as choroid layer [6,7]. In the diagnosis of glaucoma, the disease can be judged by observing changes in the thickness of the optic nerve fiber layer, etc., which is easier to detect early glaucoma than by observing fundus color images. The circumpapillary OCT image corresponds to a circular scan located around the optic nerve head, where a wealth of information about the different retinal layers can be found [8].\n\nCurrently, there are only a limited number of OCT datasets [9,10] available in public, to facilitate researchers to conduct research on OCT images, we hold a GOALS Challenge in conjunction with MICCAI 2022, aiming to provide circumpapillary OCT images for studying layer segmentation and glaucoma classification. This paper mainly introduces the 300 circumpapillary OCT images released in the GOALS challenge, and provides baselines for the two sub-tasks (Layer segmentation, and Glaucoma classification). Meanwhile, the evaluation methods are described in detail.\n\n\nDataset\n\nThe 300 circumpapillary OCT images are randomly selected from previous glaucoma study cohorts collected over the past five years in Zhongshan Ophthalmic Center, Sun Yat-sen University, Guangzhou, China. The images are all acquired by using a TOPCON DRI Swept Source OCT [11]. The acquired images are saved in BMP format with a resolution of 1024 \u00d7 247 or in JPG format with 1270 \u00d7 763. In the GOALS Challenge, we store the images in PNG format with a resolution of 1100\u00d7800. The summary of the GOALS dataset and its population demographic are shown in Table 1. The GOALS dataset provides the glaucoma labels and the segmentation masks of RNFL, GCIPL and choriod layers in the circumpapillary OCT images. The glaucoma labels are determined by the clinical records, which can reflect the findings from a series of eye examinations. The annotations of the layer segmentation are manually marked by the ophthalmologists of the iChallenge-GOALS study group. The iChallenge-GOALS study group contains ten ophthalmologists from different hospitals who have been working in the ocular field for 5 years or more. The 300 images are randomly divided into 2 subsets, and repeating this operation 5 times, we obtain 10 subsets of the data, where each image appears in 5 different subsets. The 10 subsets are randomly assigned to the 10 ophthalmologists, that is, each image is labeled by 5 different ophthalmologists. The results of the 5 initial labeling results are then aggregated by a more senior ophthalmologist for fusion. Specifically, The ophthalmologist need to delineate the upper and lower margin of the RNFL, GCIPL and choroid regions, as shown in Fig. 1(A).After that, the senior ophthalmologist analyze the 5 initial annotations of each image, remove the annotations with large deviations, and average the remaining initial annotations as the final annotation result for each image. We then assigne different pixel values to pixels within the boundaries of RNFL, GCIPL and choroid layer to obtain the final ground truths of the layer segmentation (RNFL: 0, GCIPL:80, choroid:160, elsewhere:255, as shown in Fig. 1(B)).\n\nIn GOALS Challenge, 300 images are divided into three partitions according to the patient dimension, i.e. the images acquired from the same patient's eyes are guaranteed to be divided into the same partition. These three data partitions correspond to the training set, the preliminary set, and the final set. The data in the training set contains the original OCT images and their glaucoma labels and layer segmentation masks, which are used for training models. While the preliminary and final sets only contain the original OCT images, which are used for testing models in preliminary and final rounds.\n\n\nBaseline\n\nWe design a baseline model for each of the two challenge sub-tasks. As shown in Figs. 2 and 3, we utilize a U-shape network with residual concept to achieve the layer segmentation, and utilize a ResNet50 to perform the glaucoma classification. The baseline codes are avaliable at https://aistudio.baidu.com/ aistudio/competition/detail/230/0/related-material.\n\nWe implement the baselines via PaddlePaddle. During training, we use an Adam optimizer with learning rate = 10 \u22123 in the layer segmentation task, as well as with learning rate = 10 \u22126 in the glaucoma classification task. The training procedure consist of 3000 iterations and 1000 iterations for layer segmentation and glaucoma classification with a Nvidia Tesla V100-SXM2 GPU, respectively. The batch sizes are 8 for both tasks.   \n\n\nEvaluation\n\nIn this section, we introduce the evaluation metrics for the two challenge subtasks. For the layer segmentation task, a DICE coefficient and a mean Euclidean distance (MED) are used to evaluate the predicted region and boundary, respectively. For the glaucoma classification, the weighted combination of sensitivity (Sen), specificity (Spe), accuracy (Acc), F 1 score, and area under the receiver operating characteristic curve (AUC) are utilized to evaluate the predicted results.\n\n\nTask 1: Layer Segmentation\n\nTo measure the accuracy of the predicted region, we use the frequently-used DICE coefficient in the segmentation task:\nDice = 2|X \u2229 Y | |X| + |Y | (1)\nwhere, X represents the segmented target pixel point set in the ground truth; Y represents the segmented pixel point set in the prediction result; |X \u2229 Y | represents the intersection between X and Y ; |X| and |Y | represent the number of the elements of X and Y . The formula for calculating the score corresponding to the DICE metric is\nScore Dice = DICE \u00d7 10(2)\nIn addition to evaluating the accuracy of the region segmentation, we also evaluate the accuracy of the boundary of the segmentation results by using Euclidean distance, due to the importance of the boundaries between the structural layers in the OCT images. Specifically, we first traverse each pixel on the predicted boundary, and calculate the Euclidean distance from each pixel to the nearest pixel on the gold standard boundary. Then the sum of the above Euclidean distances is averaged based on the number of pixels on the predicted boundary:\nM ED = 1 N N i=1 (x i \u2212 x 0 i ) 2 + (y i \u2212 y 0 i ) 2(3)\nwhere N is the number of pixels on the predicted boundary, (x i , y i ) is the ith pixel on the predicted boundary, and (x 0 i , y 0 i ) is the nearest pixel on the boundary of the ground truth to (x i , y i ). The score corresponding to the MED metric is calculated by\nScore M ED = (M ED + 1) \u22120.3(4)\nSince the layer segmentation task contains the segmentation of the three regions of RNFL, GCIPL and choroid, the DICE and MED metrics of these three regions should be taken into account in the score calculation. In addition, because the RNFL layer is more important for the diagnosis of glaucoma, we assign higher weights to the scores obtained from RNFL segmentation:\nScore task1 = 0.4 \u00d7 Score RN F L + 0.3 \u00d7 Score GCIP L + 0.3 \u00d7 Score choroid (5) Score region =0.5 \u00d7 Score DICEregion + 0.5 \u00d7 Score M EDregion , region \u2208 {RN F L, GCIP L, choroid}(6)\n\nTask 2: Glaucoma Classification\n\nFor glaucoma classification, we adopt five common metrics including Sen, Spe, Acc, F 1 , and AUC:\nSen = T P T P + F N (7) Spe = T N T P + F P(8)\nAcc = T P + T N T P + F N + T N + F P (9)\nF 1 = 2 \u00d7 T P 2 \u00d7 T P + F P + F N(10)\nwhere T P , T N , F P and F N represent the numbers of true positive, true negative, false positive, and false negative detection of the glaucoma. Sen,Spe and Acc can reflect the proportions of positive samples, negative samples and all samples predicted correctly, respectively. F 1 provides a overall metric of the model's ability to detect comprehensively and accurately. And AUC reflects the classification ability of the model when the positive and negative samples are unbalanced. In our evaluation framework, these metrics are implemented via scikit-learn package [12], which is an open source machine learning toolkit base on Python. Since the GOALS dataset has a balanced distribution of positive and negative samples, we assign the lowest weight to the AUC metric in the score calculation.\n\nScore task2 = (0.1\u00d7AU C+0.25\u00d7Sen+0.25\u00d7Spe+0.2\u00d7ACC+0.2\u00d7F 1 )\u00d710 (11) Based on the results of the baseline model, we find that positive and negative samples in the GOALS dataset have obvious distinguishable image features, and therefore score high in Task 2. Hence, for preliminary and final rounds, a lower weight is assigned to Task 2 in the score calculation:\nScore round = 0.8 \u00d7 Score task1 + 0.2 \u00d7 Score task2 , round \u2208 {preliminary, f inal}(12)\nSince the preliminary leaderboard is visible to the players, one can adjust the model parameters or strategies to get the best prediction on the preliminary set.\n\nTo avoid players' results from getting overfitting results on the preliminary set and getting high scores, we assign lower weights to the preliminary score when counting the total challenge scores. Hence, the total score is:\nScore = 0.3 \u00d7 Score preliminary + 0.7 \u00d7 Score f inal(13)\nBased on the evaluation criteria, our baselines receive 7.2802 score on the preliminary set and 7.2398 score on the final set. The results of each specific evaluation index are shown in Table. 2. \n\n\nConclusion\n\nIn this paper, we introduce the GOALS Challenge at MICCAI 2022. In the challenge, we focus on OCT which is a powerful imaging technology for glaucoma diagnostics. We design two challenge sub-tasks, including OCT layer segmentation of RNFL, GCIPL and choroid, and glaucoma classification. The dataset collection and labeling process, as well as the result evaluation design are described in detail in the paper. GOALS Challenge dataset and evaluation framework are publicly accessible through the AI Studio website at https://aistudio.baidu. com/aistudio/competition/detail/230. Participants are welcome to join the GOALS Challenge and submit their predicted results on the website.\n\nFig. 1 .\n1Schematic diagram of the annotations for RNFL, GCIPL, and choroid layer segmentation.(A) Annotations for the boundaries of the targets; (B) Segmentation masks.\n\nFig. 3 .\n3A baseline framework for glaucoma classification.\n\nTable 1 .\n1Summary of the GOALS dataset and the demographic of the population.Person Eyes \nAge \nGender (Female) \n\nTotal dataset \nTotal \n66 \n99 45.91\u00b115.04 \n36.40% \nGlaucoma 13 \n22 44.59\u00b112.77 \n30.80% \n\nTraining set \nTotal \n16 \n24 39.08\u00b114.08 \n31.30% \nGlaucoma \n4 \n7 40.86\u00b19.23 \n25% \n\nPreliminary set \nTotal \n19 \n30 44.8\u00b116.32 \n42.11% \nGlaucoma \n4 \n7 42.92\u00b117.09 \n75% \n\nFinal set \nTotal \n31 \n45 50.05\u00b114.48 \n35.50% \nGlaucoma \n5 \n8 \n50\u00b18.48 \n0% \n\n\n\nFig. 2. A baseline framework for OCT layer segmentation.Conv <3\u00d73> with Batch Normalization and ReLU \nMax Pooling \n\nConv2DTranspose with Batch Normalization and ReLU \n\nConv <3\u00d73, stride=2> with Batch Normalization and ReLU \nReLU \n\nConv <3\u00d73> with Batch Normalization \n\nUpsample \n\nConv2DTranspose with Batch Normalization \n\nElement-wise add \nConv <1\u00d71> \n\n128 \n256 256 \n\n64 \n128 128 \n\n64 64 \n32 \n\n128 \n\n64 \n32 \n32 \n\n32 4 \n\n256 256 \n\n32 \n\n64 \n\n128 \n\n256 256 \n\n64 \n64 \n\n128 \n\n256 \n\n256 256 \n256 \n128 \n256 \n\n128 64 64 \n128 \n256 128 \n\n128 64 \n\n32 \n64 \n\nConv <7 \u00d7 7, stride=2> \n\nMax Pooling <3 \u00d7 3, stride=2> \n\nConv <1\u00d7 1> \n\nConv <3 \u00d7 3> \n\nAverage pool \n\nFully connected \n\nGlaucoma/ \nnon-glaucoma \n\n\u2026 \n\n64 64 \n1024 \n2048 \n\n\u2026 \n\u2026 \n\u2026 \n\n64 64 256 \n64 64 256 128 128512 \n512 5122048 \n256 2561024 \n256 2561024 \n128 128512 \n512 5122048 \n\nBlock=3 \nBlock= 4 \nBlock=6 \nBlock=3 \n\n512 \n1024 \n2048 \n\n\n\nTable 2 .\n2The evaluation results of the baseline model on different datasets.Dataset \nPreliminary set Final set \nScore \n7.2802 \n7.2398 \n\nLayer Segmentation \n\nRNFL DICE \n0.8161 \n0.8433 \nRNFL ED \n4.0597 \n4.151 \nGCIPL DICE \n0.6295 \n0.6234 \nGCIPL ED \n3.318 \n3.6011 \nchoroid DICE \n0.8193 \n0.8746 \nchoroid ED \n8.9155 \n9.8953 \n\nGlaucoma Classification \n\nAUC \n0.9984 \n0.9927 \nF1 \n0.9346 \n0.8829 \nACC \n0.93 \n0.8687 \nSEN \n1 \n1 \nSPE \n0.86 \n0.74 \n\n\n\nAdvanced Imaging for Glaucoma Study Group, et al. Retinal nerve fiber layer atrophy is associated with visual field loss over time in glaucoma suspect and glaucomatous eyes. Mitra Sehi, Xinbo Zhang, S David, Yunsuk Greenfield, Gadi Chung, Wollstein, A Brian, Joel S Francis, Rohit Schuman, David Varma, Huang, American journal of ophthalmology. 1551Mitra Sehi, Xinbo Zhang, David S Greenfield, YunSuk Chung, Gadi Wollstein, Brian A Francis, Joel S Schuman, Rohit Varma, David Huang, Advanced Imaging for Glaucoma Study Group, et al. Retinal nerve fiber layer atrophy is associ- ated with visual field loss over time in glaucoma suspect and glaucomatous eyes. American journal of ophthalmology, 155(1):73-82, 2013.\n\nGlaucoma: Facts and figures. Glaucoma: Facts and figures. https://www.brightfocus.org/glaucoma/article/ glaucoma-facts-figures.\n\nGlobal prevalence of glaucoma and projections of glaucoma burden through 2040: a systematic review and meta-analysis. Yih-Chung Tham, Xiang Li, Y Tien, Wong, A Harry, Tin Quigley, Ching-Yu Aung, Cheng, Ophthalmology. 12111Yih-Chung Tham, Xiang Li, Tien Y Wong, Harry A Quigley, Tin Aung, and Ching-Yu Cheng. Global prevalence of glaucoma and projections of glaucoma burden through 2040: a systematic review and meta-analysis. Ophthalmology, 121(11):2081-2090, 2014.\n\nHighresolution optical coherence tomography retinal imaging: a case series illustrating potential and limitations. Olena Puzyeyeva, Wai Ching Lam, John G Flanagan, H Michael, Robert G Brent, Devenyi, S Mark, Tien Mandelcorn, Christopher Wong, Hudson, Journal of ophthalmology. Olena Puzyeyeva, Wai Ching Lam, John G Flanagan, Michael H Brent, Robert G Devenyi, Mark S Mandelcorn, Tien Wong, and Christopher Hudson. High- resolution optical coherence tomography retinal imaging: a case series illustrating potential and limitations. Journal of ophthalmology, 2011, 2011.\n\nZahid Yaqoob, Jigang Wu, Changhuei Yang, Spectral domain optical coherence tomography: a better oct imaging strategy. 39Zahid Yaqoob, Jigang Wu, and Changhuei Yang. Spectral domain optical coherence tomography: a better oct imaging strategy. Biotechniques, 39(6):S6-S13, 2005.\n\nRetinal layer segmentation in pathological sd-oct images using boisterous obscure ratio approach and its limitation. G Mohandass, S R Ananda Natarajan, Sendilvelan, Biomedical and Pharmacology Journal. 103G Mohandass, R Ananda Natarajan, and S Sendilvelan. Retinal layer segmenta- tion in pathological sd-oct images using boisterous obscure ratio approach and its limitation. Biomedical and Pharmacology Journal, 10(3):1585-1591, 2017.\n\nDetection of glaucoma progression with stratus oct retinal nerve fiber layer, optic nerve head, and macular thickness measurements. A Felipe, Linda M Medeiros, Luciana M Zangwill, Christopher Alencar, Pamela A Bowd, Remo Sample, Robert N Susanna, Weinreb, Investigative ophthalmology & visual science. 5012Felipe A Medeiros, Linda M Zangwill, Luciana M Alencar, Christopher Bowd, Pamela A Sample, Remo Susanna, and Robert N Weinreb. Detection of glau- coma progression with stratus oct retinal nerve fiber layer, optic nerve head, and macular thickness measurements. Investigative ophthalmology & visual science, 50(12):5741-5748, 2009.\n\nGlaucoma detection from raw circumpapillary oct images using fully convolutional neural networks. Gabriel Garc\u00eda, Roc\u00edo Del Amor, Adri\u00e1n Colomer, Valery Naranjo, 2020 IEEE international conference on image processing (ICIP). IEEEGabriel Garc\u00eda, Roc\u00edo del Amor, Adri\u00e1n Colomer, and Valery Naranjo. Glaucoma detection from raw circumpapillary oct images using fully convolutional neural networks. In 2020 IEEE international conference on image processing (ICIP), pages 2526-2530. IEEE, 2020.\n\nMacular oct classification using a multi-scale convolutional neural network ensemble. Reza Rasti, Hossein Rabbani, Alireza Mehridehnavi, Fedra Hajizadeh, IEEE transactions on medical imaging. 374Reza Rasti, Hossein Rabbani, Alireza Mehridehnavi, and Fedra Hajizadeh. Macular oct classification using a multi-scale convolutional neural network ensemble. IEEE transactions on medical imaging, 37(4):1024-1034, 2017.\n\nOctid: Optical coherence tomography image database. Peyman Gholami, Priyanka Roy, Mohana Kuppuswamy Parthasarathy, Vasudevan Lakshminarayanan, Computers & Electrical Engineering. 81106532Peyman Gholami, Priyanka Roy, Mohana Kuppuswamy Parthasarathy, and Va- sudevan Lakshminarayanan. Octid: Optical coherence tomography image database. Computers & Electrical Engineering, 81:106532, 2020.\n\nDri oct triton series. Dri oct triton series. https://topconhealthcare.eu/uploads/media/ 60cb7b98ea585/topcon-triton-brochure-rev5-27-05-21-e325-lores.pdf.\n\nScikit-learn: Machine learning in python. the. Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Journal of machine Learning research. 12Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.\n", "annotations": {"author": "[{\"end\":127,\"start\":63},{\"end\":315,\"start\":128},{\"end\":422,\"start\":316},{\"end\":484,\"start\":423},{\"end\":704,\"start\":485},{\"end\":780,\"start\":705}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":70},{\"end\":134,\"start\":132},{\"end\":325,\"start\":323},{\"end\":431,\"start\":429},{\"end\":497,\"start\":492},{\"end\":713,\"start\":711}]", "author_first_name": "[{\"end\":69,\"start\":63},{\"end\":131,\"start\":128},{\"end\":322,\"start\":316},{\"end\":428,\"start\":423},{\"end\":491,\"start\":485},{\"end\":710,\"start\":705}]", "author_affiliation": "[{\"end\":126,\"start\":76},{\"end\":314,\"start\":136},{\"end\":421,\"start\":327},{\"end\":483,\"start\":433},{\"end\":703,\"start\":525},{\"end\":779,\"start\":729}]", "title": "[{\"end\":60,\"start\":1},{\"end\":840,\"start\":781}]", "venue": null, "abstract": "[{\"end\":1832,\"start\":930}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2247,\"start\":2244},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2309,\"start\":2306},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2371,\"start\":2368},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2574,\"start\":2571},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2576,\"start\":2574},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3166,\"start\":3163},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3168,\"start\":3166},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3561,\"start\":3558},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3626,\"start\":3623},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3629,\"start\":3626},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4414,\"start\":4410},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11006,\"start\":11002},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11299,\"start\":11295}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":13189,\"start\":13019},{\"attributes\":{\"id\":\"fig_2\"},\"end\":13250,\"start\":13190},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":13696,\"start\":13251},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":14579,\"start\":13697},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":15018,\"start\":14580}]", "paragraph": "[{\"end\":3562,\"start\":1848},{\"end\":4128,\"start\":3564},{\"end\":6260,\"start\":4140},{\"end\":6866,\"start\":6262},{\"end\":7238,\"start\":6879},{\"end\":7671,\"start\":7240},{\"end\":8167,\"start\":7686},{\"end\":8316,\"start\":8198},{\"end\":8687,\"start\":8349},{\"end\":9262,\"start\":8714},{\"end\":9588,\"start\":9319},{\"end\":9989,\"start\":9621},{\"end\":10303,\"start\":10206},{\"end\":10392,\"start\":10351},{\"end\":11230,\"start\":10431},{\"end\":11592,\"start\":11232},{\"end\":11842,\"start\":11681},{\"end\":12068,\"start\":11844},{\"end\":12322,\"start\":12126},{\"end\":13018,\"start\":12337}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8348,\"start\":8317},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8713,\"start\":8688},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9318,\"start\":9263},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9620,\"start\":9589},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10171,\"start\":9990},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10350,\"start\":10304},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10430,\"start\":10393},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11680,\"start\":11593},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12125,\"start\":12069}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":4699,\"start\":4692},{\"end\":12318,\"start\":12312}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1846,\"start\":1834},{\"attributes\":{\"n\":\"2\"},\"end\":4138,\"start\":4131},{\"attributes\":{\"n\":\"3\"},\"end\":6877,\"start\":6869},{\"attributes\":{\"n\":\"4\"},\"end\":7684,\"start\":7674},{\"attributes\":{\"n\":\"4.1\"},\"end\":8196,\"start\":8170},{\"attributes\":{\"n\":\"4.2\"},\"end\":10204,\"start\":10173},{\"attributes\":{\"n\":\"5\"},\"end\":12335,\"start\":12325},{\"end\":13028,\"start\":13020},{\"end\":13199,\"start\":13191},{\"end\":13261,\"start\":13252},{\"end\":14590,\"start\":14581}]", "table": "[{\"end\":13696,\"start\":13330},{\"end\":14579,\"start\":13755},{\"end\":15018,\"start\":14659}]", "figure_caption": "[{\"end\":13189,\"start\":13030},{\"end\":13250,\"start\":13201},{\"end\":13330,\"start\":13263},{\"end\":13755,\"start\":13699},{\"end\":14659,\"start\":14592}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5797,\"start\":5788},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6258,\"start\":6249},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":6972,\"start\":6959}]", "bib_author_first_name": "[{\"end\":15199,\"start\":15194},{\"end\":15211,\"start\":15206},{\"end\":15220,\"start\":15219},{\"end\":15234,\"start\":15228},{\"end\":15251,\"start\":15247},{\"end\":15271,\"start\":15270},{\"end\":15283,\"start\":15279},{\"end\":15285,\"start\":15284},{\"end\":15300,\"start\":15295},{\"end\":15315,\"start\":15310},{\"end\":15991,\"start\":15982},{\"end\":16003,\"start\":15998},{\"end\":16009,\"start\":16008},{\"end\":16023,\"start\":16022},{\"end\":16034,\"start\":16031},{\"end\":16052,\"start\":16044},{\"end\":16451,\"start\":16446},{\"end\":16466,\"start\":16463},{\"end\":16472,\"start\":16467},{\"end\":16482,\"start\":16478},{\"end\":16484,\"start\":16483},{\"end\":16496,\"start\":16495},{\"end\":16512,\"start\":16506},{\"end\":16514,\"start\":16513},{\"end\":16532,\"start\":16531},{\"end\":16543,\"start\":16539},{\"end\":16567,\"start\":16556},{\"end\":16907,\"start\":16902},{\"end\":16922,\"start\":16916},{\"end\":16936,\"start\":16927},{\"end\":17298,\"start\":17297},{\"end\":17311,\"start\":17310},{\"end\":17750,\"start\":17749},{\"end\":17764,\"start\":17759},{\"end\":17766,\"start\":17765},{\"end\":17784,\"start\":17777},{\"end\":17786,\"start\":17785},{\"end\":17808,\"start\":17797},{\"end\":17824,\"start\":17818},{\"end\":17826,\"start\":17825},{\"end\":17837,\"start\":17833},{\"end\":17852,\"start\":17846},{\"end\":17854,\"start\":17853},{\"end\":18360,\"start\":18353},{\"end\":18374,\"start\":18369},{\"end\":18391,\"start\":18385},{\"end\":18407,\"start\":18401},{\"end\":18836,\"start\":18832},{\"end\":18851,\"start\":18844},{\"end\":18868,\"start\":18861},{\"end\":18888,\"start\":18883},{\"end\":19219,\"start\":19213},{\"end\":19237,\"start\":19229},{\"end\":19260,\"start\":19243},{\"end\":19285,\"start\":19276},{\"end\":19761,\"start\":19755},{\"end\":19777,\"start\":19773},{\"end\":19798,\"start\":19789},{\"end\":19816,\"start\":19809},{\"end\":19833,\"start\":19825},{\"end\":19850,\"start\":19843},{\"end\":19866,\"start\":19859},{\"end\":19881,\"start\":19876},{\"end\":19899,\"start\":19896},{\"end\":19914,\"start\":19907}]", "bib_author_last_name": "[{\"end\":15204,\"start\":15200},{\"end\":15217,\"start\":15212},{\"end\":15226,\"start\":15221},{\"end\":15245,\"start\":15235},{\"end\":15257,\"start\":15252},{\"end\":15268,\"start\":15259},{\"end\":15277,\"start\":15272},{\"end\":15293,\"start\":15286},{\"end\":15308,\"start\":15301},{\"end\":15321,\"start\":15316},{\"end\":15328,\"start\":15323},{\"end\":15996,\"start\":15992},{\"end\":16006,\"start\":16004},{\"end\":16014,\"start\":16010},{\"end\":16020,\"start\":16016},{\"end\":16029,\"start\":16024},{\"end\":16042,\"start\":16035},{\"end\":16057,\"start\":16053},{\"end\":16064,\"start\":16059},{\"end\":16461,\"start\":16452},{\"end\":16476,\"start\":16473},{\"end\":16493,\"start\":16485},{\"end\":16504,\"start\":16497},{\"end\":16520,\"start\":16515},{\"end\":16529,\"start\":16522},{\"end\":16537,\"start\":16533},{\"end\":16554,\"start\":16544},{\"end\":16572,\"start\":16568},{\"end\":16580,\"start\":16574},{\"end\":16914,\"start\":16908},{\"end\":16925,\"start\":16923},{\"end\":16941,\"start\":16937},{\"end\":17308,\"start\":17299},{\"end\":17330,\"start\":17312},{\"end\":17343,\"start\":17332},{\"end\":17757,\"start\":17751},{\"end\":17775,\"start\":17767},{\"end\":17795,\"start\":17787},{\"end\":17816,\"start\":17809},{\"end\":17831,\"start\":17827},{\"end\":17844,\"start\":17838},{\"end\":17862,\"start\":17855},{\"end\":17871,\"start\":17864},{\"end\":18367,\"start\":18361},{\"end\":18383,\"start\":18375},{\"end\":18399,\"start\":18392},{\"end\":18415,\"start\":18408},{\"end\":18842,\"start\":18837},{\"end\":18859,\"start\":18852},{\"end\":18881,\"start\":18869},{\"end\":18898,\"start\":18889},{\"end\":19227,\"start\":19220},{\"end\":19241,\"start\":19238},{\"end\":19274,\"start\":19261},{\"end\":19302,\"start\":19286},{\"end\":19771,\"start\":19762},{\"end\":19787,\"start\":19778},{\"end\":19807,\"start\":19799},{\"end\":19823,\"start\":19817},{\"end\":19841,\"start\":19834},{\"end\":19857,\"start\":19851},{\"end\":19874,\"start\":19867},{\"end\":19894,\"start\":19882},{\"end\":19905,\"start\":19900},{\"end\":19922,\"start\":19915}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":236441881},\"end\":15733,\"start\":15020},{\"attributes\":{\"id\":\"b1\"},\"end\":15862,\"start\":15735},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8298120},\"end\":16329,\"start\":15864},{\"attributes\":{\"id\":\"b3\"},\"end\":16900,\"start\":16331},{\"attributes\":{\"id\":\"b4\"},\"end\":17178,\"start\":16902},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":55915297},\"end\":17615,\"start\":17180},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6927982},\"end\":18253,\"start\":17617},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":219176520},\"end\":18744,\"start\":18255},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4552464},\"end\":19159,\"start\":18746},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":56170332},\"end\":19549,\"start\":19161},{\"attributes\":{\"id\":\"b10\"},\"end\":19706,\"start\":19551},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10659969},\"end\":20243,\"start\":19708}]", "bib_title": "[{\"end\":15192,\"start\":15020},{\"end\":15980,\"start\":15864},{\"end\":16444,\"start\":16331},{\"end\":17295,\"start\":17180},{\"end\":17747,\"start\":17617},{\"end\":18351,\"start\":18255},{\"end\":18830,\"start\":18746},{\"end\":19211,\"start\":19161},{\"end\":19753,\"start\":19708}]", "bib_author": "[{\"end\":15206,\"start\":15194},{\"end\":15219,\"start\":15206},{\"end\":15228,\"start\":15219},{\"end\":15247,\"start\":15228},{\"end\":15259,\"start\":15247},{\"end\":15270,\"start\":15259},{\"end\":15279,\"start\":15270},{\"end\":15295,\"start\":15279},{\"end\":15310,\"start\":15295},{\"end\":15323,\"start\":15310},{\"end\":15330,\"start\":15323},{\"end\":15998,\"start\":15982},{\"end\":16008,\"start\":15998},{\"end\":16016,\"start\":16008},{\"end\":16022,\"start\":16016},{\"end\":16031,\"start\":16022},{\"end\":16044,\"start\":16031},{\"end\":16059,\"start\":16044},{\"end\":16066,\"start\":16059},{\"end\":16463,\"start\":16446},{\"end\":16478,\"start\":16463},{\"end\":16495,\"start\":16478},{\"end\":16506,\"start\":16495},{\"end\":16522,\"start\":16506},{\"end\":16531,\"start\":16522},{\"end\":16539,\"start\":16531},{\"end\":16556,\"start\":16539},{\"end\":16574,\"start\":16556},{\"end\":16582,\"start\":16574},{\"end\":16916,\"start\":16902},{\"end\":16927,\"start\":16916},{\"end\":16943,\"start\":16927},{\"end\":17310,\"start\":17297},{\"end\":17332,\"start\":17310},{\"end\":17345,\"start\":17332},{\"end\":17759,\"start\":17749},{\"end\":17777,\"start\":17759},{\"end\":17797,\"start\":17777},{\"end\":17818,\"start\":17797},{\"end\":17833,\"start\":17818},{\"end\":17846,\"start\":17833},{\"end\":17864,\"start\":17846},{\"end\":17873,\"start\":17864},{\"end\":18369,\"start\":18353},{\"end\":18385,\"start\":18369},{\"end\":18401,\"start\":18385},{\"end\":18417,\"start\":18401},{\"end\":18844,\"start\":18832},{\"end\":18861,\"start\":18844},{\"end\":18883,\"start\":18861},{\"end\":18900,\"start\":18883},{\"end\":19229,\"start\":19213},{\"end\":19243,\"start\":19229},{\"end\":19276,\"start\":19243},{\"end\":19304,\"start\":19276},{\"end\":19773,\"start\":19755},{\"end\":19789,\"start\":19773},{\"end\":19809,\"start\":19789},{\"end\":19825,\"start\":19809},{\"end\":19843,\"start\":19825},{\"end\":19859,\"start\":19843},{\"end\":19876,\"start\":19859},{\"end\":19896,\"start\":19876},{\"end\":19907,\"start\":19896},{\"end\":19924,\"start\":19907}]", "bib_venue": "[{\"end\":15363,\"start\":15330},{\"end\":15762,\"start\":15735},{\"end\":16079,\"start\":16066},{\"end\":16606,\"start\":16582},{\"end\":17018,\"start\":16943},{\"end\":17380,\"start\":17345},{\"end\":17917,\"start\":17873},{\"end\":18478,\"start\":18417},{\"end\":18936,\"start\":18900},{\"end\":19338,\"start\":19304},{\"end\":19572,\"start\":19551},{\"end\":19960,\"start\":19924}]"}}}, "year": 2023, "month": 12, "day": 17}