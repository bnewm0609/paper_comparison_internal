{"id": 227232051, "updated": "2023-04-05 20:58:56.016", "metadata": {"title": "Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition", "authors": "[{\"first\":\"Ke\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Yifan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Congqi\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Hanqing\",\"last\":\"Lu\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": ",", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3108496296", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/ChengZCSCL20", "doi": "10.1007/978-3-030-58586-0_32"}}, "content": {"source": {"pdf_hash": "b701aa8494d0d467d0ca8785686eb36cc4c40a8e", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "436df5443623188b2eb60ac3aed0a589eaa104bd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b701aa8494d0d467d0ca8785686eb36cc4c40a8e.txt", "contents": "\nDecoupling GCN with DropGraph Module for Skeleton-Based Action Recognition\n\n\nKe Cheng \nInstitute of Automation\nNLPR & AIRIA\nChinese Academy of Sciences\n\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\n\n\nYifan Zhang \nInstitute of Automation\nNLPR & AIRIA\nChinese Academy of Sciences\n\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\n\n\nCongqi Cao congqi.cao@nwpu.edu.cn \nSchool of Computer Science\nNorthwestern Polytechnical University\n\n\nLei Shi \nInstitute of Automation\nNLPR & AIRIA\nChinese Academy of Sciences\n\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\n\n\nJian Cheng \nInstitute of Automation\nNLPR & AIRIA\nChinese Academy of Sciences\n\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\n\n\nCAS Center for Excellence in Brain Science and Intelligence Technology\n\n\nHanqing Lu \nInstitute of Automation\nNLPR & AIRIA\nChinese Academy of Sciences\n\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\n\n\nDecoupling GCN with DropGraph Module for Skeleton-Based Action Recognition\nskeleton-based action recognitiondecoupling GCNDrop- Graph\nIn skeleton-based action recognition, graph convolutional networks (GCNs) have achieved remarkable success. Nevertheless, how to efficiently model the spatial-temporal skeleton graph without introducing extra computation burden is a challenging problem for industrial deployment. In this paper, we rethink the spatial aggregation in existing GCN-based skeleton action recognition methods and discover that they are limited by coupling aggregation mechanism. Inspired by the decoupling aggregation mechanism in CNNs, we propose decoupling GCN to boost the graph modeling ability with no extra computation, no extra latency, no extra GPU memory cost, and less than 10% extra parameters. Another prevalent problem of GCNs is over-fitting. Although dropout is a widely used regularization technique, it is not effective for GCNs, due to the fact that activation units are correlated between neighbor nodes. We propose DropGraph to discard features in correlated nodes, which is particularly effective on GCNs. Moreover, we introduce an attentionguided drop mechanism to enhance the regularization effect. All our contributions introduce zero extra computation burden at deployment. We conduct experiments on three datasets (NTU-RGBD, NTU-RGBD-120, and Northwestern-UCLA) and exceed the state-of-the-art performance with less computation cost.\n\nIntroduction\n\nHuman action recognition, which plays an essential role in video understanding and human-computer interaction, attracts more attention in recent years [43,39,46]. Compared with action recognition with RGB video, skeleton-based action recognition is robust to circumstance changes and illumination variations [3,40,36,48,50,25,46,17,33,34,29].\n\nTraditional methods mainly focus on designing hand-crafted features [39,4]. However, the performance of these handcrafted-features-based methods is barely satisfactory. Deep learning methods usually rearrange a skeleton sequence as a pseudo-image [21,14,9] or a series of joint coordinates [32,50,25], then use CNNs or RNNs to predict action labels. Recently, graph convolutional networks (GCNs), which generalize CNNs from image to graph, have been successfully adopted to model skeleton data [46]. The key component of GCN is spatial aggregation, which aggregates features of different body joints. To increase the flexibility of the skeleton graph construction, researchers propose various modules to enhance the spatial aggregation ability for GCNs [44,34,13,35,18].\n\nIn this paper, we rethink the spatial aggregation of GCNs, which is derived from CNNs. We discover that existing GCN-based skeleton action recognition methods neglect an important mechanism in CNNs: decoupling aggregation. Concretely, every channel has an independent spatial aggregation kernel in CNNs, capturing different spatial information in different frequencies, orientations and colors, which is crucial for the success of CNNs. However, all the channels in a graph convolution share one spatial aggregation kernel: the adjacent matrix. Although some researchers partition one adjacent matrix into multiple adjacent matrices and ensemble multiple graph convolution results of these adjacent matrices [46,34,33,17,13,35,44], the number of adjacent matrices is typically less than 3, which limits the expressiveness of spatial aggregation. Increasing the number of adjacent matrices will cause multiplying growth of computation cost and reduce efficiency.\n\nInspired by the decoupling aggregation in CNNs, we propose DeCoupling Graph Convolutional Networks (DC-GCN) to address the above dilemma with no extra FLOPs, latency, and GPU memory. DC-GCN split channels into g decoupling groups, and each group has a trainable adjacent matrix, which largely increases the expressiveness of spatial aggregation. Note that the FLOPs of decoupling graph convolution is exactly the same with conventional graph convolution. More importantly, DC-GCN is hardware-friendly and increases no extra time and GPU memory, the two most determining factors in industrial deployment. Besides, DC-GCN only cost 5% \u223c 10% extra parameters.\n\nAnother prevalent problem in graph convolution is over-fitting. Although dropout [37] is widely used in GCNs, we discover that the performance does not increase obviously with dropout layer. Because graph convolution is actually a special form of Laplacian smoothing [19], activation units are correlated between neighbor nodes. Even one node in a graph is dropped, information about this node can still be obtained from its neighbor nodes, which causes over-fitting. To relieve the over-fitting problem, we propose DropGraph, a particularly effective regularization method for graph convolutional networks. The key idea is: when we drop one node, we drop its neighbor node together. In addition, we propose an attention-guided drop mechanism to enhance the regularization effect.\n\nThe main contributions of this work are summarized as follows: 1) We propose DC-GCN, which efficiently enhances the expressiveness of graph convolution with zero extra computation cost. 2) We propose ADG to effectively relieve the crucial over-fitting problem in GCNs. 3) Our approach exceeds the stateof-the-art method with less computation cost. Code will be available at https: //github.com/kchengiva/DecoupleGCN-DropGraph.\n\n\nBackground\n\nHuman skeleton graph convolution The skeleton data represents a human action as multiple skeleton frames. Every skeleton frame is represented as a graph G(V, E), where V is the set of n body joints and E is a set of m bones. For example, 3D joints skeleton positions across T frames can be represented as X \u2208 R n\u00d73\u00d7T , and the 3D joints skeleton frames in the t-th frame is denoted as X t = X :,:,t \u2208 R n\u00d73 . GCN-based action recognition models [46,44,34,33,13,35,18] are composed of several spatial-temporal GCN blocks, where spatial graph convolution is the key component.\n\nLet X \u2208 R n\u00d7C be the input features in one frame, and X \u2208 R n\u00d7C be the output features of these joints, where C and C are the input and output feature dimension respectively. The spatial graph convolution is\nX = p\u2208P X (p) = p\u2208P A (p) XW (p) ,(1)\nwhere P ={root,centripetal,centrifugal} denotes the partition subsets [46].\nA (p) is initialized as D (p) \u2212 1 2 A (p) D (p) \u2212 1 2 , where D (p) ii = j (A (p) ij ) + \u03b5.\nHere \u03b5 is set to 0.001 to avoid empty rows. Recent works let both A (p) \u2208 R n\u00d7n and W (p) \u2208 R C\u00d7C trainable [34,33].\n\n\nRegularization method\n\nOver-fitting is a crucial problem in deep neural networks, including GCNs. Dropout [37] is a common regularization method. Although dropout is very effective at regularizing fully-connected layers, it is not powerful when used in convolutional layers. [2] proposed Cutout for regularizing CNNs, which randomly removes contiguous region in the input images. [5] proposed DropBlock, which applying Cutout at every feature map in CNNs. The reason why Cutout and DropBlock are efficient is that features are spatially correlated in CNNs. GCNs have the similar problems with CNNs, where common dropout is not effective. Inspired from DropBlock [5] in CNNs, we proposed DropGraph to effectively regularize GCNs.\n\n\nApproach\n\nIn this section, we analyze the limitation of the human skeleton graph convolutional networks and propose DeCoupling Graph Convolutional Network (DC-GCN). In addition, we propose an attention-guided DropGraph (ADG) to relieve the prevalent overfitting problem in GCNs.\n\n\nDecoupling graph convolutional network\n\nFor clarity, we first discuss the case of graph convolution with a single partition set, then naturally extend to the multiple partition case.\n\nMotivation Graph convolution contains two matrix multiplication processes: AX and XW. AX computes the aggregation information between different skeletons, so we call it spatial aggregation. XW compute the correlate information between different channels, so we call it channel correlation.\n\nAs shown in Fig.1 (a), the spatial aggregation ( AX) can be decomposed into computing the aggregation on every channel respectively. Note that all the channels of feature X share one adjacency matrix A (drawn in the same color), which means all the channels share the same aggregation kernel. We call it coupling aggregation. All existing GCN-based skeleton action recognition methods adopt the coupling aggregation, such as ST-GCN [46], Nonlocal adaptive GCN [34], AS-GCN [18], Directed-GNN [33]. We collectively call them coupling graph convolution. However, CNNs, the source of inspiration for GCNs, do not adopt the coupling aggregation. As shown in Fig.1 (b), different channels have independent spatial aggregation kernels, shown in different color. We call this mechanism decoupling aggregation. Decoupling aggregation mechanism can largely increase the spatial aggregation ability, which is essential for the success of CNNs.\n\nDeCoupling GCN Conventional graph convolution limited by coupling aggregation can be analogous to a \"degenerate depthwise convolution\" whose convolution kernels are shared between channels. The expressiveness of the \"degenerate depthwise convolution\" is notably weaker than a standard depthwise convolution. Therefore, we deduce that existing GCN-based skeleton action recognition models [46,34,18,33,45,35] lack the decoupling aggregation mechanism.\n\nIn this paper, we propose decoupling graph convolution for skeleton action recognition, where different channel has independent trainable adjacent matrix, shown in Fig.1 (c). Decoupling graph convolution largely increases the variety of adjacent matrix. Similar to the redundancy of CNN kernels [30], decoupling graph convolution may introduce redundant adjacent matrix. Hence we split channels into g groups. Channels in a group share one trainable adjacent matrix. When g = C, every channel has its own spatial aggregation kernel which causes large number of redundant parameters; when g = 1, decoupling graph convolution degenerates into coupling graph convolution. Interestingly, experiments show that 8 \u223c 16 groups are enough. In this case, we only increase 5% \u223c 10% extra parameters. The equation of decoupling graph convolution is shown as below:\nX = A d :,:,1 X w :,: C g A d :,:,2 X w :, C g : 2C g \u00b7 \u00b7 \u00b7 A d :,:,g X w :, (g\u22121)C g :(2)\nwhere X w = XW, A d \u2208 R n\u00d7n\u00d7g is the decoupling adjacent matrices. Indexes of A d and X w are in Python notation, and represents channel-wise concatenation. By replacing coupling graph convolution with decoupling graph convolution, we construct DeCoupling GCN (DC-GCN). Although the number of parameters is slightly increased, the floating-number operations (FLOPs) of DC-GCN is exactly the same with conventional GCN (n 2 C + nC 2 ). More importantly, DC-GCN costs no extra time and GPU memory, the two determining factors for deployment. Compared with other variants of ST-GCNs [34,33,44,13], DC-GCN achieves higher performance without incurring any extra computations.\n\nDiscussion DC-GCN can be naturally extended to multiple partition cases by introducing decoupling graph convolution into every partition. Note that our DC-GCN is different from the multi-partition strategy [46], which ensembles multiple graph convolutions with different adjacent matrices. The FLOPs of the multi-partition strategy is proportional to the number of adjacency matrices, while DC-GCN introduces various adjacency matrices with no extra computation. Besides, all our experiments use 3-partition ST-GCN as baseline, which shows the complementarity between multi-partition strategy and DC-GCN.\n\nDC-GCN is different from SemGCN [49] in many aspects: 1) SemGCN focus on pose regression, while we focus on action recognition. 2) The receptive field of SemGCN is localized, and a heavy non-local module is inserted for non-local modeling. Our DC-GCN has non-local receptive fields and increases no extra FLOPs.\n\n3) The parameter cost of SemGCN is nearly double of baseline. Our DC-GCN only increases 5% \u223c 10% extra parameters.\n\n\nAttention-guided DropGraph\n\nMotivation Although dropout is a widely used regularization method, the performance of GCNs does not increase obviously with dropout layer. A possible reason is that graph features are correlated between nearby neighbors. As shown in [19], graph convolution is a special form of Laplacian smoothing, which mixes the features of a node and its neighbors. Even one node is dropped, information about this node can still be obtained from its neighbor node, leading to overfitting. We propose DropGraph to effectively regularize GCNs, and design an attention-guided drop mechanism to further enhance the regularization effect.\n\nDropGraph The main idea of DropGraph is: when we drop one node, we drop its neighbor node set together. DropGraph has two main parameters: \u03b3 and K. \u03b3 controls the sample probability, and K controls the size of the neighbor set to be dropped. On an input feature map, we first sample root nodes v root with the Bernoulli distribution with probability \u03b3, then drop the activation of v root and the nodes that are at maximum K steps away from v root . DropGraph can be implemented as Algorithm 1.\n\n\nAlgorithm 1 DropGraph\n\nInput: a GCN feature X \u2208 R n\u00d7C , adjacent matrix A, \u03b3, K, mode 1: if mode == Inf erence then 2: return X 3: else 4:\n\nRandomly sample Vroot \u2208 R n , every element in Vroot is in Bernoulli distribution with probability \u03b3.\n\n\n5:\n\nCompute the drop mask M \u2208 R n to mask the nodes that are at maximum K steps away from Vroot:\nM = 1 \u2212 Bool((A + I) K V root ),\nwhere Bool is function setting non-zero element to 1. 6:\n\nApply the mask: X = X \u00d7 M 7:\n\nNormalize the feature:\nX = X \u00d7 count(M)/count ones(M) 8: end if\nLet keep prob denote the probability of an activation unit to be kept. For conventional dropout, keep prob = 1\u2212\u03b3. But for DropGraph, every zero entry on v root is expanded to its 1 st , 2 ed , \u00b7 \u00b7 \u00b7 , K th -order neighborhood. Thus, keep prob depends on both \u03b3 and K. In a graph with n nodes and e edges, we define the average degree of each node as d ave = 2e/n. The expectation number of nodes in the i th -order neighborhood of a random sampled node can be estimated as:\nB i = d ave \u00d7 (d ave \u2212 1) i\u22121(3)\nThe average expanded drop size is estimated as:\ndrop size = 1 + K i=1 B i(4)\nIf we want to keep activation units with the probability of keep prob, we set:\n\u03b3 = 1 \u2212 keep prob drop size(5)\nNote that there might be some overlaps between drop areas, so this equation is only an approximation. In our experiments, we first estimate the keep prob to use (between 0.75-0.95), and then compute \u03b3 as Eq.5.\n\nAttention-guided drop mechanism To enhance the regularization effect, we let the attention area have higher probability to sample v root . Let v be a node, \u03b3 v denote the probability of sampling the node v as v root . We modify Eq.5 as:\n\u03b3 v = \u03b1 v 1 \u2212 keep prob drop size = \u03b1 v count(\u03b1) \u03b1 1 \u2212 keep prob drop size(6)\nwhere \u03b1 is the attention map, \u03b1 is the normalized attention map, count(\u03b1) is the number of elements in \u03b1. To assess the distribution of attention area, a common implicit assumption is that the absolute value of an activation is an indication about the importance of one unit [47]. We follow this assumption and generate \u03b1 by averaging the absolute value across the channel dimension.  Spatial-temporal ADG In skeleton action recognition, the input of attentionguided DropGraph (ADG) is a spatiotemporal feature X \u2208 R n\u00d7C\u00d7T . As shown in Fig.2, we apply ADG to spatial graph and temporal graph respectively.\n\nThe spatial aspect of the graph is the human physical structure with the number of nodes n. We generate spatial attention on every skeleton \u03b1 S \u2208 R n by compressing the absolute value of X using average pooling on channel dimension and temporal dimension. After sampling v root , we expand the drop area to its spatial neighbors. Then we broadcast the drop area to all temporal frames.\n\nThe temporal aspect of the graph is constructed by connecting consecutive frames on temporal dimension, with the number of nodes T . We generate temporal attention on every frame \u03b1 T \u2208 R T by compressing the absolute value of X using average pooling on channel dimension and skeleton dimension. After sampling v root , we expand the drop area to its temporal neighbors. Then we broadcast the drop area to all body joints.\n\nWe cascade spatial ADG and temporal ADG to construct spatiotemporal ADG. We apply ADG on both GCN branch and skip connection branch. We adopt linear scheme [5] to decreasing keep prob over time from 1 to target value.\n\nComparison with other regularization methods. We compare DropGraph with other two regularization methods for GCNs: (a) dropout [37], which randomly drops the nodes with a certain probability; (b) DropEdge [31], which randomly drop the edges in a graph with a certain probability. As shown in Fig.2, the drop area of both dropout and DropEdge are isolated, which can not effectively remove related information of the dropped node. For dropout, even if one node is dropped, information about this node can still be obtained from its neighbor node. For DropEdge, even if one edge is dropped, related information can still reach this node through other edges. DropGraph addresses their drawbacks and achieve notably better performance (details in Sec.4.2).\n\n\nExperiments\n\n\nDatasets and Model Configuration\n\nNTU-RGBD. NTU-RGBD is the most widely used 3D joint coordinates dataset. It contains 56,880 action samples in 60 action classes. These samples are performed by 40 distinct subjects. The 3D skeleton data is captured by Kinect V2. Each action is captured by 3 cameras from different horizontal angles: \u221245 \u2022 , 0 \u2022 , 45 \u2022 . The original paper [32] recommends two protocols. 1) Cross-Subject (Xsub): training data comes from 20 subjects, and the remaining 20 subjects are used for validation. 2) Cross-View (X-view): training data comes from the camera 0 \u2022 and 45 \u2022 , and validation data comes from camera \u221245 \u2022 . NTU-RGBD-120. NTU-RGBD-120 is the extended version of the NTU-RGBD dataset. It contains 114,480 action samples in 120 action classes, performed by 106 distinct subjects. This dataset contains 32 setups, and every different setup has a specific location and background. The original paper [22] recommends two evaluation protocols. 1). Cross-Subject (X-sub): training data comes from 53 subjects, and the remaining 53 subjects are used for validation. 2). Cross-Setup (X-setup): picking all the samples with even setup IDs for training, and the remaining samples with odd setup IDs for validation. Northwestern-UCLA. Northwestern-UCLA (NW-UCLA) dataset [42] contains 1494 video clips covering 10 categories, which is captured by three Kinect cameras. Each action is performed by 10 different subjects. We adopt the same protocol as [42]: training data comes from the first two cameras, and samples from the other camera are used for validation. Model Setting. We construct the backbone as ST-GCN [46]. The batch size is 64. We use SGD to train the model for 100 epochs. We use momentum of 0.9 and weight decay of 1e-4. The learning rate is set as 0.1 and is divided by 10 at epoch 60 and 80. For NTU-RGBD and NTU-RGBD-120, we use the same data preprocess as [34]. For NW-UCLA, we use the same data preprocess as [35].\n\n\nAblation Study\n\nDecoupling graph convolution In this subsection, we demonstrate the effectiveness and efficiency of DC-GCN.\n\n(1) Efficacy of DC-GCN. We perform ablation study on different decoupling groups, shown in Fig.3. Our baseline is ST-GCN [46]. We also compare the performance with non-local adaptive graph module (CVPR 2019) [34] and SE module [8]. From Fig.3, we can draw the following conclusions: -DC-GCN outperforms the baseline at 1.0% on NTU X-sub task and 0.8% on NTU X-view task. Compared with non-local adaptive graph and SE module, DC-GCN achieves higher performance at no extra computation. -Compared to coupling graph convolution network (group=1), decoupling graph convolution network achieves higher performance. We do not need to decouple the adjacent matrix of every channel. 8 groups are enough for NTU-RGBD X-view task and 16 groups are enough for NTU-RGBD X-sub task, which is used as our default setting in the following discussion.\n(b) X-view (a) X-sub\n-Non-local adaptive GCN [34] uses a non-local module to predict the datadependent graph for each sample. Compared to it, our DC-GCN employs several static graphs, but get even higher performance with less FLOPs. (2) FLOPs, speed and GPU memory. DC-GCN is not only theoretically efficient but also has high throughput and efficient GPU memory cost, as shown in Table 1. We can conclude that: 480M extra GPU memory compared with baseline. This is, the theoretical efficiency is not equivalent to fast speed and efficient GPU memory cost. Compared with SE module, our DC-GCN is a hardware-friendly approach.\n\n(3) Parameter cost. Decoupling GCN introduces extra parameters to baseline. Because our group decoupling mechanism, DC-GCN only introduces 5% \u223c 10% extra parameters when g = 8 \u223c 16. If we increase the number of channels of baseline to the same parameter cost, we do not get notable improvement.\n\n(4) Visualization of the learned adjacent matrices. We visualize the learned adjacent matrices of coupling GCN (group=1) and decoupling GCN (group=8), shown in Fig.4. Compared with coupling GCN where every channel shares one adjacent matrix, decoupling GCN (group=8) largely increases the variety of spatial aggregation. In this way, decoupling GCN can model diverse relations among joints. In shallow layers (e.g., layer 1), the skeleton connections in decoupling GCN tend to be local, as shown in Fig.4 (d). For example, some adjacent matrices have strong connections between head and hand (e.g., Group 4 and Group 8 in Fig.4 (d)), which are helpful for recognizing \"wipe face\" and \"brush teeth\"; some adjacent matrices have strong connections between head and neck (e.g., Group 1 in Fig.4 (d)), which are helpful for recognizing \"nod head\" and \"shake head\"; some adjacent matrices have strong connections between hand and wrist (e.g., Group 3 in Fig.4 (d)), which are helpful for recognizing \"write\" and \"count money\" ; some adjacent matrices have strong connections between two hands (e.g., Group 2, Group 5 and Group 7 in Fig.4 (d)), which are helpful for recognizing \"clap\" and \"rub two hands\". This characteristic makes decoupling GCN work well in action recognition tasks.\n\nIn deep layers (e.g., layer 9), the skeleton connections in decoupling GCN tend to be global, as shown in Fig.4 (f). These adjacent matrices tend to gather the global feature to one joint. In this way, the deep layers can integrate global information (the whole human body) with local information (each single joint), which helps predict the final classification score.\n\nAttention-guided DropGraph In this subsection, we demonstrate the effectiveness of attention-guided DropGraph (ADG).\n\n(1) Comparison with other regularization methods. We compare with three regularization methods: dropout [37], label smoothing [38] and DropEdge [31]. For our proposed DropGraph, we set K = 1 for spatial DropGraph and K = 20 for temporal DropGraph respectively. The detail ablation study on K is provided in the supplement material. Note that when K = 0, DropGraph degenerates to dropout [37]. As shown in Table 2, dropout is not powerful in GCN. DropGraph notably exceeds the other regularization methods. With the attention-guide drop mechanism, the regularization effect is further enhanced.\n\n\nModel\n\nRegularization method X-sub \u2206 X-view \u2206  Table 2. Compare with other regularization methods. The top-1 accuracy (%) is evaluated on NTU-RGBD. \u2206 shows the improvement of accuracy. (2) The setting of keep prob. We discuss the setting of keep prob on dropout, DropEdge and our proposed ADG. As shown in Fig.5, ADG provides efficient regularization when keep prob = 0.85 \u223c 0.9. ADG has a notable improvement compared to the best result of dropout and DropEdge.\n\nAblation studies on NW-UCLA and NTU-RGBD-120 Besides the above ablation study on NTU-RGBD dataset, we also perform ablation studies on NW-UCLA and NTU-RGBD-120 datasets, shown in Table 3.\n\nBackbone +DC +ADG NW-UCLA(%) NTU120 X-sub(%) NTU120 X-setup(%) ST Table 3. Ablation study on NW-UCLA and NTU-RGBD-120. DC refers to decoupling.\n\n\nComparisons to the State-of-the-Art\n\nMulti-stream strategy is commonly employed in previous state-of-the-art approaches [46,34,35,43,33]. We adopt the same multi-stream ensemble strategy with [33], which ensembles 4 streams: joint, bone, motion, and bone motion. The joint stream uses the original joint position as input; the bone stream uses the difference between adjacent joints as input; the motion stream uses the difference between adjacent frames as input.\n\nWe conduct extensive experiments on three datasets: NTU-RGBD dataset, NW-UCLA dataset, and the recently proposed NTU-RGBD-120 dataset, shown in Table 4, Table 5, and Table 6 respectively. Our approach exceeds all the previous methods with a notable margin.\n\nNote that the comparison with Directed-GNN is unfair because of the computational cost disparity. Directed-GNN doubles the number of channels in temporal convolution and introduces extra directed graph modules, whose computational cost (127G FLOPs) is nearly double of ours (65G FLOPs) 5 . Nevertheless, we outperform the current state-of-the-art method Directed-GNN at 0.9% on NTU-RGBD X-sub task. On NW-UCLA, we outperform the current state-of-the-art method AGC-LSTM at 2.0%. On NTU-120 RGB+D dataset, we obviously exceed all previously reported performance.\n\n\nMethods\n\nX-sub X-view Lie Group [39] 50.1 52.8 STA-LSTM [36] 73.4 81.2 VA-LSTM [48] 79.2 87.7 ARRN-LSTM [16] 80.7 88.8 Ind-RNN [20] 81.8 88.0 2-Stream 3DCNN [21] 66.8 72.6 TCN [11] 74.3 83.1 ClipCNN+MTLN [9] 79.6 84.8 Synthesized CNN [27] 80.0 87.2 CNN+Motion+Trans [15] 83.2 88.8 ST-GCN [46] 81.5 88.3 Motif+VTDB [44] 84.2 90.2 STGR-GCN [13] 86.9 92.3 AS-GCN [18] 86.8 94.2 Non-local adaptive GCN [34] 88.5 95.1 AGC-LSTM [35] 89.2 95.0 Directed-GNN [33] 89.9 96.1 DC-GCN+ADG (ours) 90.8 96.6 Table 4. Comparisions of the top-1 accuracy (%) with the state-of-the-art methods on the NTU-RGBD dataset.\n\n\nMethods\n\nYear Top-1 Lie Group [39] 2014 74.2 Actionlet ensemble [41] 2014 76.0 Visualization CNN [28] 2017 86.1 Ensemble TS-LSTM [12] 2017 89.2 2s AGC-LSTM [35] 2019 93.3 DC-GCN+ADG (ours) -95.3 Table 5. Comparisions of the accuracy (%) with the state-of-the-art methods on the NW-UCLA dataset.\n\n\nMethods\n\nX-sub X-setup Part-Aware LSTM [32] 25.5 26.3 Soft RNN [7] 36.3 44.9 Dynamic Skeleton [6] 50.8 54.7 Spatio-Temporal LSTM [25] 55.7 57.9 Internal Feature Fusion [24] 58.2 60.9 GCA-LSTM [26] 58.3 59.2 Multi-Task Learning Network [9] 58.4 57.9 FSNet [23] 59.9 62.4 Multi CNN + RotClips [10] 62.2 61.8 Pose Evolution Map [29] 64.6 66.9 SkeleMotion [1] 67.7 66.9 DC-GCN+ADG (ours) 86.5 88.1 Table 6. Comparisions of the top-1 accuracy (%) with the state-of-the-art methods on the NTU-RGBD-120 dataset.\n\n\nConclusion\n\nIn this work, we propose decoupling GCN to boost the graph modeling ability for skeleton-based action recognition. In addition, we propose an attentionguided DropGraph module to effectively relieve the crucial over-fitting problem in GCNs. Both these two contributions introduce zero extra computation, zero extra latency, and zero extra GPU memory cost at deployment. Hence, our approach is not only theoretically efficient but also has well practicality and application prospects. Our approach exceeds the current state-of-the-art method on three datasets: NTU-RGBD, NTU-RGBD-120, and NW-UCLA with even less computation. Since enhancing the effectiveness of the graph modeling and reducing the over-fitting risk are two prevalent problems in GCNs, our approach has potential application value for other GCN tasks, such as recommender systems, traffic analysis, natural language processing, computational chemistry.\n\nAcknowledgement This work was supported in part by the National Natural Science Foundation of China under Grant 61876182 and 61872364, in part by the Jiangsu Leading Technology Basic Research Project BK20192004. This work was partly supported by the Open Projects Program of National Laboratory of Pattern Recognition.\n\nFig. 1 .\n1Conventional GCNs (a) employ coupling aggregation, while CNNs (b) employ decoupling aggregation. We introduce the decoupling aggregation mechanism into GCNs and propose decoupling GCN (c).\n\nFig. 2 .\n2Spatial-temporal DropGraph.\n\nFig. 3 .\n3Decoupling GCN on NTU-RGBD dataset.\n\n-\nDecoupling GCN (g = 4, 8, 16, C) introduces no extra FLOPs/latency/GPU memory compared to coupling GCN (g = 1). In addition, the latency and GPU memory cost of DC-GCN are almost the same as ST-GCN baseline. -DC-GCN is more efficient than non-local adaptive GCN. Non-local adaptive GCN increases 10% extra FLOPs and 589M extra GPU memory, and cost 92 % extra time compared to DC-GCN. -Although SE module is efficient at FLOPs, it costs 33% extra time and\n\nFig. 4 .\n4Visualization of the learned adjacent matrices. The green lines show the body physical connections. The thickness of red lines shows the connection strength of the learned adjacent matrices.\n\nFig. 5 .\n5Compare dropout, DropEdge and our ADG at different keep prob.\nDetails about the computational complexity are provided in supplement material.\n\nSkelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition. C Caetano, J Sena, F Br\u00e9mond, J A Santos, W R Schwartz, arXiv:1907.13025arXiv preprintCaetano, C., Sena, J., Br\u00e9mond, F., Santos, J.A.d., Schwartz, W.R.: Skelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition. arXiv preprint arXiv:1907.13025 (2019)\n\nT Devries, G W Taylor, arXiv:1708.04552Improved regularization of convolutional neural networks with cutout. arXiv preprintDeVries, T., Taylor, G.W.: Improved regularization of convolutional neural net- works with cutout. arXiv preprint arXiv:1708.04552 (2017)\n\nHierarchical recurrent neural network for skeleton based action recognition. Y Du, W Wang, L Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionDu, Y., Wang, W., Wang, L.: Hierarchical recurrent neural network for skeleton based action recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1110-1118 (2015)\n\nModeling video evolution for action recognition. B Fernando, E Gavves, J M Oramas, A Ghodrati, T Tuytelaars, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionFernando, B., Gavves, E., Oramas, J.M., Ghodrati, A., Tuytelaars, T.: Modeling video evolution for action recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5378-5387 (2015)\n\nDropblock: A regularization method for convolutional networks. G Ghiasi, T Y Lin, Q V Le, Advances in Neural Information Processing Systems. Ghiasi, G., Lin, T.Y., Le, Q.V.: Dropblock: A regularization method for convo- lutional networks. In: Advances in Neural Information Processing Systems. pp. 10727-10737 (2018)\n\nJointly learning heterogeneous features for rgb-d activity recognition. J F Hu, W S Zheng, J Lai, J Zhang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHu, J.F., Zheng, W.S., Lai, J., Zhang, J.: Jointly learning heterogeneous features for rgb-d activity recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5344-5352 (2015)\n\nEarly action prediction by soft regression. J F Hu, W S Zheng, L Ma, G Wang, J H Lai, J Zhang, Hu, J.F., Zheng, W.S., Ma, L., Wang, G., Lai, J.H., Zhang, J.: Early action pre- diction by soft regression. IEEE transactions on pattern analysis and machine intelligence (2018)\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7132-7141 (2018)\n\nA new representation of skeleton sequences for 3d action recognition. Q Ke, M Bennamoun, S An, F Sohel, F Boussaid, Proceedings of the IEEE confer. the IEEE conferKe, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F.: A new representation of skeleton sequences for 3d action recognition. In: Proceedings of the IEEE confer- ence on computer vision and pattern recognition. pp. 3288-3297 (2017)\n\nLearning clip representations for skeleton-based 3d action recognition. Q Ke, M Bennamoun, S An, F Sohel, F Boussaid, IEEE Transactions on Image Processing. 276Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F.: Learning clip representa- tions for skeleton-based 3d action recognition. IEEE Transactions on Image Pro- cessing 27(6), 2842-2855 (2018)\n\nInterpretable 3d human action analysis with temporal convolutional networks. T S Kim, A Reiter, 2017 IEEE conference on computer vision and pattern recognition workshops (CVPRW). IEEEKim, T.S., Reiter, A.: Interpretable 3d human action analysis with temporal con- volutional networks. In: 2017 IEEE conference on computer vision and pattern recognition workshops (CVPRW). pp. 1623-1631. IEEE (2017)\n\nEnsemble deep learning for skeleton-based action recognition using temporal sliding lstm networks. I Lee, D Kim, S Kang, S Lee, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLee, I., Kim, D., Kang, S., Lee, S.: Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1012-1020 (2017)\n\nSpatio-temporal graph routing for skeleton-based action recognition. B Li, X Li, Z Zhang, F Wu, Li, B., Li, X., Zhang, Z., Wu, F.: Spatio-temporal graph routing for skeleton-based action recognition (2019)\n\nSkeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn. B Li, Y Dai, X Cheng, H Chen, Y Lin, M He, 2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW). IEEELi, B., Dai, Y., Cheng, X., Chen, H., Lin, Y., He, M.: Skeleton based action recogni- tion using translation-scale invariant image mapping and multi-scale deep cnn. In: 2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW). pp. 601-604. IEEE (2017)\n\nSkeleton-based action recognition with convolutional neural networks. C Li, Q Zhong, D Xie, S Pu, 10.1109/ICMEW.2017.80262852017 IEEE International Conference on Multimedia & Expo Workshops, ICME Workshops. Hong Kong, ChinaLi, C., Zhong, Q., Xie, D., Pu, S.: Skeleton-based action recognition with con- volutional neural networks. In: 2017 IEEE International Conference on Multi- media & Expo Workshops, ICME Workshops, Hong Kong, China, July 10-14, 2017. pp. 597-600 (2017). https://doi.org/10.1109/ICMEW.2017.8026285, https: //doi.org/10.1109/ICMEW.2017.8026285\n\nSkeleton-based relational modeling for action recognition. L Li, W Zheng, Z Zhang, Y Huang, L Wang, CoRR abs/1805.02556Li, L., Zheng, W., Zhang, Z., Huang, Y., Wang, L.: Skeleton-based relational mod- eling for action recognition. CoRR abs/1805.02556 (2018), http://arxiv.org/ abs/1805.02556\n\nActional-structural graph convolutional networks for skeleton-based action recognition. M Li, S Chen, X Chen, Y Zhang, Y Wang, Q Tian, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Li, M., Chen, S., Chen, X., Zhang, Y., Wang, Y., Tian, Q.: Actional-structural graph convolutional networks for skeleton-based action recognition. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)\n\nActional-structural graph convolutional networks for skeleton-based action recognition. M Li, S Chen, X Chen, Y Zhang, Y Wang, Q Tian, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLi, M., Chen, S., Chen, X., Zhang, Y., Wang, Y., Tian, Q.: Actional-structural graph convolutional networks for skeleton-based action recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3595- 3603 (2019)\n\nDeeper insights into graph convolutional networks for semi-supervised learning. Q Li, Z Han, X M Wu, Thirty-Second AAAI Conference on Artificial Intelligence. Li, Q., Han, Z., Wu, X.M.: Deeper insights into graph convolutional networks for semi-supervised learning. In: Thirty-Second AAAI Conference on Artificial Intel- ligence (2018)\n\nIndependently recurrent neural network (indrnn): Building a longer and deeper RNN. S Li, W Li, C Cook, C Zhu, Y Gao, 10.1109/CVPR.2018.005722018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018. Salt Lake City, UT, USALi, S., Li, W., Cook, C., Zhu, C., Gao, Y.: Independently recurrent neural network (indrnn): Building a longer and deeper RNN. In: 2018 IEEE Conference on Com- puter Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. pp. 5457-5466 (2018). https://doi.org/10.1109/CVPR.2018.00572, http://openaccess.thecvf.com/content_cvpr_2018/html/Li_Independently_ Recurrent_Neural_CVPR_2018_paper.html\n\nTwo-stream 3d convolutional neural network for skeletonbased action recognition. H Liu, J Tu, M Liu, arXiv:1705.08106arXiv preprintLiu, H., Tu, J., Liu, M.: Two-stream 3d convolutional neural network for skeleton- based action recognition. arXiv preprint arXiv:1705.08106 (2017)\n\nNTU RGB+D 120: A large-scale benchmark for 3d human activity understanding. J Liu, A Shahroudy, M Perez, G Wang, L Duan, A C Kot, CoRR abs/1905.04757Liu, J., Shahroudy, A., Perez, M., Wang, G., Duan, L., Kot, A.C.: NTU RGB+D 120: A large-scale benchmark for 3d human activity understanding. CoRR abs/1905.04757 (2019), http://arxiv.org/abs/1905.04757\n\nSkeleton-based online action prediction using scale selection network. J Liu, A Shahroudy, G Wang, L Y Duan, A K Chichung, Liu, J., Shahroudy, A., Wang, G., Duan, L.Y., Chichung, A.K.: Skeleton-based online action prediction using scale selection network. IEEE transactions on pattern analysis and machine intelligence (2019)\n\nSkeleton-based action recognition using spatio-temporal lstm network with trust gates. J Liu, A Shahroudy, D Xu, A C Kot, G Wang, IEEE transactions. 4012Liu, J., Shahroudy, A., Xu, D., Kot, A.C., Wang, G.: Skeleton-based action recog- nition using spatio-temporal lstm network with trust gates. IEEE transactions on pattern analysis and machine intelligence 40(12), 3007-3021 (2017)\n\nSpatio-temporal lstm with trust gates for 3d human action recognition. J Liu, A Shahroudy, D Xu, G Wang, European Conference on Computer Vision. SpringerLiu, J., Shahroudy, A., Xu, D., Wang, G.: Spatio-temporal lstm with trust gates for 3d human action recognition. In: European Conference on Computer Vision. pp. 816-833. Springer (2016)\n\nGlobal context-aware attention lstm networks for 3d action recognition. J Liu, G Wang, P Hu, L Y Duan, A C Kot, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLiu, J., Wang, G., Hu, P., Duan, L.Y., Kot, A.C.: Global context-aware attention lstm networks for 3d action recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1647-1656 (2017)\n\nEnhanced skeleton visualization for view invariant human action recognition. M Liu, H Liu, C Chen, 10.1016/j.patcog.2017.02.030Pattern Recognition. 68Liu, M., Liu, H., Chen, C.: Enhanced skeleton visualization for view in- variant human action recognition. Pattern Recognition 68, 346-362 (2017). https://doi.org/10.1016/j.patcog.2017.02.030, https://doi.org/10.1016/j. patcog.2017.02.030\n\nEnhanced skeleton visualization for view invariant human action recognition. M Liu, H Liu, C Chen, Pattern Recognition. 68Liu, M., Liu, H., Chen, C.: Enhanced skeleton visualization for view invariant human action recognition. Pattern Recognition 68, 346-362 (2017)\n\nRecognizing human actions as the evolution of pose estimation maps. M Liu, J Yuan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLiu, M., Yuan, J.: Recognizing human actions as the evolution of pose estimation maps. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1159-1168 (2018)\n\nPruning convolutional neural networks for resource efficient inference. P Molchanov, S Tyree, T Karras, T Aila, J Kautz, arXiv:1611.06440arXiv preprintMolchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J.: Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440 (2016)\n\nDropedge: Towards deep graph convolutional networks on node classification. Y Rong, W Huang, T Xu, J Huang, International Conference on Learning Representations. Rong, Y., Huang, W., Xu, T., Huang, J.: Dropedge: Towards deep graph convo- lutional networks on node classification. In: International Conference on Learning Representations (2020)\n\nNtu rgb+ d: A large scale dataset for 3d human activity analysis. A Shahroudy, J Liu, T T Ng, G Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionShahroudy, A., Liu, J., Ng, T.T., Wang, G.: Ntu rgb+ d: A large scale dataset for 3d human activity analysis. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1010-1019 (2016)\n\nSkeleton-based action recognition with directed graph neural networks. L Shi, Y Zhang, J Cheng, H Lu, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Shi, L., Zhang, Y., Cheng, J., Lu, H.: Skeleton-based action recognition with di- rected graph neural networks. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)\n\nTwo-stream adaptive graph convolutional networks for skeleton-based action recognition. L Shi, Y Zhang, J Cheng, H Lu, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Shi, L., Zhang, Y., Cheng, J., Lu, H.: Two-stream adaptive graph convolutional networks for skeleton-based action recognition. In: The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) (June 2019)\n\nAn attention enhanced graph convolutional lstm network for skeleton-based action recognition. C Si, W Chen, W Wang, L Wang, T Tan, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Si, C., Chen, W., Wang, W., Wang, L., Tan, T.: An attention enhanced graph convolutional lstm network for skeleton-based action recognition. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)\n\nAn end-to-end spatio-temporal attention model for human action recognition from skeleton data. S Song, C Lan, J Xing, W Zeng, J Liu, Thirty-first AAAI conference on artificial intelligence. Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: An end-to-end spatio-temporal attention model for human action recognition from skeleton data. In: Thirty-first AAAI conference on artificial intelligence (2017)\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, The journal of machine learning research. 151Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research 15(1), 1929-1958 (2014)\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the incep- tion architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818-2826 (2016)\n\nDifferential recurrent neural networks for action recognition. V Veeriah, N Zhuang, G J Qi, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionVeeriah, V., Zhuang, N., Qi, G.J.: Differential recurrent neural networks for action recognition. In: Proceedings of the IEEE international conference on computer vision. pp. 4041-4049 (2015)\n\nHuman action recognition by representing 3d skeletons as points in a lie group. R Vemulapalli, F Arrate, R Chellappa, 10.1109/CVPR.2014.822014 IEEE Conference on Computer Vision and Pattern Recognition. Columbus, OH, USAVemulapalli, R., Arrate, F., Chellappa, R.: Human action recognition by rep- resenting 3d skeletons as points in a lie group. In: 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014. pp. 588-595 (2014). https://doi.org/10.1109/CVPR.2014.82, https://doi.org/10.1109/CVPR.2014.82\n\nLearning actionlet ensemble for 3d human action recognition. J Wang, Z Liu, Y Wu, J Yuan, IEEE transactions on pattern analysis and machine intelligence. 365Wang, J., Liu, Z., Wu, Y., Yuan, J.: Learning actionlet ensemble for 3d human action recognition. IEEE transactions on pattern analysis and machine intelligence 36(5), 914-927 (2013)\n\nCross-view action modeling, learning and recognition. J Wang, X Nie, Y Xia, Y Wu, S C Zhu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWang, J., Nie, X., Xia, Y., Wu, Y., Zhu, S.C.: Cross-view action modeling, learning and recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2649-2656 (2014)\n\nTemporal segment networks: Towards good practices for deep action recognition. L Wang, Y Xiong, Z Wang, Y Qiao, D Lin, X Tang, L Van Gool, European conference on computer vision. SpringerWang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Tem- poral segment networks: Towards good practices for deep action recognition. In: European conference on computer vision. pp. 20-36. Springer (2016)\n\nGraph cnns with motif and variable temporal block for skeleton-based action recognition. Y H Wen, L Gao, H Fu, F L Zhang, S Xia, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Wen, Y.H., Gao, L., Fu, H., Zhang, F.L., Xia, S.: Graph cnns with motif and variable temporal block for skeleton-based action recognition. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 33, pp. 8989-8996 (2019)\n\nGraph cnns with motif and variable temporal block for skeleton-based action recognition. Y Wen, L Gao, H Fu, F Zhang, S Xia, The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019. Honolulu, Hawaii, USAWen, Y., Gao, L., Fu, H., Zhang, F., Xia, S.: Graph cnns with motif and vari- able temporal block for skeleton-based action recognition. In: The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Inno- vative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019. pp. 8989-8996 (2019), https://aaai.org/ojs/index.php/AAAI/article/view/4929\n\nSpatial temporal graph convolutional networks for skeleton-based action recognition. S Yan, Y Xiong, D Lin, Thirty-Second AAAI Conference on Artificial Intelligence. Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional networks for skeleton-based action recognition. In: Thirty-Second AAAI Conference on Artificial Intelligence (2018)\n\nPaying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. S Zagoruyko, N Komodakis, arXiv:1612.03928arXiv preprintZagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928 (2016)\n\nView adaptive recurrent neural networks for high performance human action recognition from skeleton data. P Zhang, C Lan, J Xing, W Zeng, J Xue, N Zheng, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZhang, P., Lan, C., Xing, J., Zeng, W., Xue, J., Zheng, N.: View adaptive recurrent neural networks for high performance human action recognition from skeleton data. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2117-2126 (2017)\n\nSemantic graph convolutional networks for 3d human pose regression. L Zhao, X Peng, Y Tian, M Kapadia, D N Metaxas, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAIEEEZhao, L., Peng, X., Tian, Y., Kapadia, M., Metaxas, D.N.: Semantic graph convo- lutional networks for 3d human pose regression. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. pp. 3425-3435. Computer Vision Foundation / IEEE (2019)\n\nSkeleton-based relational modeling for action recognition. W Zheng, L Li, Z Zhang, Y Huang, L Wang, arXiv:1805.02556arXiv preprintZheng, W., Li, L., Zhang, Z., Huang, Y., Wang, L.: Skeleton-based relational mod- eling for action recognition. arXiv preprint arXiv:1805.02556 (2018)\n", "annotations": {"author": "[{\"end\":232,\"start\":78},{\"end\":390,\"start\":233},{\"end\":492,\"start\":391},{\"end\":646,\"start\":493},{\"end\":876,\"start\":647},{\"end\":1033,\"start\":877}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":81},{\"end\":244,\"start\":239},{\"end\":401,\"start\":398},{\"end\":500,\"start\":497},{\"end\":657,\"start\":652},{\"end\":887,\"start\":885}]", "author_first_name": "[{\"end\":80,\"start\":78},{\"end\":238,\"start\":233},{\"end\":397,\"start\":391},{\"end\":496,\"start\":493},{\"end\":651,\"start\":647},{\"end\":884,\"start\":877}]", "author_affiliation": "[{\"end\":153,\"start\":88},{\"end\":231,\"start\":155},{\"end\":311,\"start\":246},{\"end\":389,\"start\":313},{\"end\":491,\"start\":426},{\"end\":567,\"start\":502},{\"end\":645,\"start\":569},{\"end\":724,\"start\":659},{\"end\":802,\"start\":726},{\"end\":875,\"start\":804},{\"end\":954,\"start\":889},{\"end\":1032,\"start\":956}]", "title": "[{\"end\":75,\"start\":1},{\"end\":1108,\"start\":1034}]", "venue": null, "abstract": "[{\"end\":2506,\"start\":1168}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2677,\"start\":2673},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2680,\"start\":2677},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2683,\"start\":2680},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2833,\"start\":2830},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2836,\"start\":2833},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2839,\"start\":2836},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2842,\"start\":2839},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2845,\"start\":2842},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2848,\"start\":2845},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2851,\"start\":2848},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2854,\"start\":2851},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2857,\"start\":2854},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2860,\"start\":2857},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2863,\"start\":2860},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2938,\"start\":2934},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2940,\"start\":2938},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3117,\"start\":3113},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3120,\"start\":3117},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3122,\"start\":3120},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3160,\"start\":3156},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3163,\"start\":3160},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3166,\"start\":3163},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3364,\"start\":3360},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3623,\"start\":3619},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3626,\"start\":3623},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3629,\"start\":3626},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3632,\"start\":3629},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3635,\"start\":3632},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4350,\"start\":4346},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4353,\"start\":4350},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4356,\"start\":4353},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4359,\"start\":4356},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4362,\"start\":4359},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4365,\"start\":4362},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4368,\"start\":4365},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5344,\"start\":5340},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5530,\"start\":5526},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6931,\"start\":6927},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6934,\"start\":6931},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6937,\"start\":6934},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6940,\"start\":6937},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6943,\"start\":6940},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6946,\"start\":6943},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6949,\"start\":6946},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7378,\"start\":7374},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7584,\"start\":7580},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7587,\"start\":7584},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7701,\"start\":7697},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7869,\"start\":7866},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7974,\"start\":7971},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8256,\"start\":8253},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9514,\"start\":9510},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9542,\"start\":9538},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9555,\"start\":9551},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9574,\"start\":9570},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10405,\"start\":10401},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10408,\"start\":10405},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10411,\"start\":10408},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10414,\"start\":10411},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10417,\"start\":10414},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10420,\"start\":10417},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10764,\"start\":10760},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11994,\"start\":11990},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11997,\"start\":11994},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12000,\"start\":11997},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12003,\"start\":12000},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12293,\"start\":12289},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12725,\"start\":12721},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13385,\"start\":13381},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16292,\"start\":16288},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17590,\"start\":17587},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17781,\"start\":17777},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17859,\"start\":17855},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18797,\"start\":18793},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19355,\"start\":19351},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19718,\"start\":19714},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19897,\"start\":19893},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20061,\"start\":20057},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20323,\"start\":20319},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20377,\"start\":20373},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20631,\"start\":20627},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20718,\"start\":20714},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20736,\"start\":20733},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21391,\"start\":21387},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24144,\"start\":24140},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24166,\"start\":24162},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24184,\"start\":24180},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24427,\"start\":24423},{\"end\":25350,\"start\":25348},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":25555,\"start\":25551},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25558,\"start\":25555},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25561,\"start\":25558},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25564,\"start\":25561},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25567,\"start\":25564},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25627,\"start\":25623},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26755,\"start\":26751},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26779,\"start\":26775},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":26802,\"start\":26798},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26827,\"start\":26823},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26850,\"start\":26846},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26880,\"start\":26876},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26899,\"start\":26895},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26926,\"start\":26923},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26957,\"start\":26953},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26989,\"start\":26985},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27011,\"start\":27007},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27037,\"start\":27033},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":27061,\"start\":27057},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27083,\"start\":27079},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27121,\"start\":27117},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":27145,\"start\":27141},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27173,\"start\":27169},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27355,\"start\":27351},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":27389,\"start\":27385},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27422,\"start\":27418},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27454,\"start\":27450},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":27481,\"start\":27477},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27661,\"start\":27657},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27684,\"start\":27681},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27715,\"start\":27712},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27751,\"start\":27747},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27790,\"start\":27786},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27814,\"start\":27810},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27856,\"start\":27853},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27877,\"start\":27873},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27913,\"start\":27909},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27947,\"start\":27943},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27973,\"start\":27970}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29573,\"start\":29374},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29612,\"start\":29574},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29659,\"start\":29613},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30116,\"start\":29660},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30318,\"start\":30117},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30391,\"start\":30319}]", "paragraph": "[{\"end\":2864,\"start\":2522},{\"end\":3636,\"start\":2866},{\"end\":4599,\"start\":3638},{\"end\":5257,\"start\":4601},{\"end\":6039,\"start\":5259},{\"end\":6467,\"start\":6041},{\"end\":7056,\"start\":6482},{\"end\":7265,\"start\":7058},{\"end\":7379,\"start\":7304},{\"end\":7588,\"start\":7472},{\"end\":8319,\"start\":7614},{\"end\":8600,\"start\":8332},{\"end\":8785,\"start\":8643},{\"end\":9076,\"start\":8787},{\"end\":10011,\"start\":9078},{\"end\":10463,\"start\":10013},{\"end\":11318,\"start\":10465},{\"end\":12081,\"start\":11410},{\"end\":12687,\"start\":12083},{\"end\":13000,\"start\":12689},{\"end\":13116,\"start\":13002},{\"end\":13769,\"start\":13147},{\"end\":14264,\"start\":13771},{\"end\":14405,\"start\":14290},{\"end\":14508,\"start\":14407},{\"end\":14607,\"start\":14515},{\"end\":14697,\"start\":14641},{\"end\":14727,\"start\":14699},{\"end\":14751,\"start\":14729},{\"end\":15266,\"start\":14793},{\"end\":15347,\"start\":15300},{\"end\":15455,\"start\":15377},{\"end\":15696,\"start\":15487},{\"end\":15934,\"start\":15698},{\"end\":16619,\"start\":16013},{\"end\":17006,\"start\":16621},{\"end\":17429,\"start\":17008},{\"end\":17648,\"start\":17431},{\"end\":18402,\"start\":17650},{\"end\":20378,\"start\":18453},{\"end\":20504,\"start\":20397},{\"end\":21341,\"start\":20506},{\"end\":21967,\"start\":21363},{\"end\":22263,\"start\":21969},{\"end\":23545,\"start\":22265},{\"end\":23916,\"start\":23547},{\"end\":24034,\"start\":23918},{\"end\":24629,\"start\":24036},{\"end\":25094,\"start\":24639},{\"end\":25283,\"start\":25096},{\"end\":25428,\"start\":25285},{\"end\":25895,\"start\":25468},{\"end\":26153,\"start\":25897},{\"end\":26716,\"start\":26155},{\"end\":27318,\"start\":26728},{\"end\":27615,\"start\":27330},{\"end\":28122,\"start\":27627},{\"end\":29053,\"start\":28137},{\"end\":29373,\"start\":29055}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7303,\"start\":7266},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7471,\"start\":7380},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11409,\"start\":11319},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14640,\"start\":14608},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14792,\"start\":14752},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15299,\"start\":15267},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15376,\"start\":15348},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15486,\"start\":15456},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16012,\"start\":15935},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21362,\"start\":21342}]", "table_ref": "[{\"end\":21730,\"start\":21723},{\"end\":24448,\"start\":24441},{\"end\":24686,\"start\":24679},{\"end\":25282,\"start\":25275},{\"end\":25358,\"start\":25351},{\"end\":26048,\"start\":26041},{\"end\":26057,\"start\":26050},{\"end\":26070,\"start\":26063},{\"end\":27219,\"start\":27212},{\"end\":27523,\"start\":27516},{\"end\":28019,\"start\":28012}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2520,\"start\":2508},{\"attributes\":{\"n\":\"2\"},\"end\":6480,\"start\":6470},{\"end\":7612,\"start\":7591},{\"attributes\":{\"n\":\"3\"},\"end\":8330,\"start\":8322},{\"attributes\":{\"n\":\"3.1\"},\"end\":8641,\"start\":8603},{\"attributes\":{\"n\":\"3.2\"},\"end\":13145,\"start\":13119},{\"end\":14288,\"start\":14267},{\"end\":14513,\"start\":14511},{\"attributes\":{\"n\":\"4\"},\"end\":18416,\"start\":18405},{\"attributes\":{\"n\":\"4.1\"},\"end\":18451,\"start\":18419},{\"attributes\":{\"n\":\"4.2\"},\"end\":20395,\"start\":20381},{\"end\":24637,\"start\":24632},{\"attributes\":{\"n\":\"4.3\"},\"end\":25466,\"start\":25431},{\"end\":26726,\"start\":26719},{\"end\":27328,\"start\":27321},{\"end\":27625,\"start\":27618},{\"attributes\":{\"n\":\"5\"},\"end\":28135,\"start\":28125},{\"end\":29383,\"start\":29375},{\"end\":29583,\"start\":29575},{\"end\":29622,\"start\":29614},{\"end\":29662,\"start\":29661},{\"end\":30126,\"start\":30118},{\"end\":30328,\"start\":30320}]", "table": null, "figure_caption": "[{\"end\":29573,\"start\":29385},{\"end\":29612,\"start\":29585},{\"end\":29659,\"start\":29624},{\"end\":30116,\"start\":29663},{\"end\":30318,\"start\":30128},{\"end\":30391,\"start\":30330}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9099,\"start\":9090},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9741,\"start\":9732},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10638,\"start\":10629},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16555,\"start\":16550},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17947,\"start\":17942},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20602,\"start\":20597},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20748,\"start\":20738},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22430,\"start\":22425},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22773,\"start\":22764},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22896,\"start\":22887},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23060,\"start\":23051},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23223,\"start\":23214},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23401,\"start\":23392},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23662,\"start\":23653},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24943,\"start\":24938}]", "bib_author_first_name": "[{\"end\":30591,\"start\":30590},{\"end\":30602,\"start\":30601},{\"end\":30610,\"start\":30609},{\"end\":30621,\"start\":30620},{\"end\":30623,\"start\":30622},{\"end\":30633,\"start\":30632},{\"end\":30635,\"start\":30634},{\"end\":30902,\"start\":30901},{\"end\":30913,\"start\":30912},{\"end\":30915,\"start\":30914},{\"end\":31241,\"start\":31240},{\"end\":31247,\"start\":31246},{\"end\":31255,\"start\":31254},{\"end\":31663,\"start\":31662},{\"end\":31675,\"start\":31674},{\"end\":31685,\"start\":31684},{\"end\":31687,\"start\":31686},{\"end\":31697,\"start\":31696},{\"end\":31709,\"start\":31708},{\"end\":32151,\"start\":32150},{\"end\":32161,\"start\":32160},{\"end\":32163,\"start\":32162},{\"end\":32170,\"start\":32169},{\"end\":32172,\"start\":32171},{\"end\":32478,\"start\":32477},{\"end\":32480,\"start\":32479},{\"end\":32486,\"start\":32485},{\"end\":32488,\"start\":32487},{\"end\":32497,\"start\":32496},{\"end\":32504,\"start\":32503},{\"end\":32918,\"start\":32917},{\"end\":32920,\"start\":32919},{\"end\":32926,\"start\":32925},{\"end\":32928,\"start\":32927},{\"end\":32937,\"start\":32936},{\"end\":32943,\"start\":32942},{\"end\":32951,\"start\":32950},{\"end\":32953,\"start\":32952},{\"end\":32960,\"start\":32959},{\"end\":33182,\"start\":33181},{\"end\":33188,\"start\":33187},{\"end\":33196,\"start\":33195},{\"end\":33579,\"start\":33578},{\"end\":33585,\"start\":33584},{\"end\":33598,\"start\":33597},{\"end\":33604,\"start\":33603},{\"end\":33613,\"start\":33612},{\"end\":33977,\"start\":33976},{\"end\":33983,\"start\":33982},{\"end\":33996,\"start\":33995},{\"end\":34002,\"start\":34001},{\"end\":34011,\"start\":34010},{\"end\":34337,\"start\":34336},{\"end\":34339,\"start\":34338},{\"end\":34346,\"start\":34345},{\"end\":34759,\"start\":34758},{\"end\":34766,\"start\":34765},{\"end\":34773,\"start\":34772},{\"end\":34781,\"start\":34780},{\"end\":35209,\"start\":35208},{\"end\":35215,\"start\":35214},{\"end\":35221,\"start\":35220},{\"end\":35230,\"start\":35229},{\"end\":35455,\"start\":35454},{\"end\":35461,\"start\":35460},{\"end\":35468,\"start\":35467},{\"end\":35477,\"start\":35476},{\"end\":35485,\"start\":35484},{\"end\":35492,\"start\":35491},{\"end\":35917,\"start\":35916},{\"end\":35923,\"start\":35922},{\"end\":35932,\"start\":35931},{\"end\":35939,\"start\":35938},{\"end\":36471,\"start\":36470},{\"end\":36477,\"start\":36476},{\"end\":36486,\"start\":36485},{\"end\":36495,\"start\":36494},{\"end\":36504,\"start\":36503},{\"end\":36793,\"start\":36792},{\"end\":36799,\"start\":36798},{\"end\":36807,\"start\":36806},{\"end\":36815,\"start\":36814},{\"end\":36824,\"start\":36823},{\"end\":36832,\"start\":36831},{\"end\":37233,\"start\":37232},{\"end\":37239,\"start\":37238},{\"end\":37247,\"start\":37246},{\"end\":37255,\"start\":37254},{\"end\":37264,\"start\":37263},{\"end\":37272,\"start\":37271},{\"end\":37754,\"start\":37753},{\"end\":37760,\"start\":37759},{\"end\":37767,\"start\":37766},{\"end\":37769,\"start\":37768},{\"end\":38094,\"start\":38093},{\"end\":38100,\"start\":38099},{\"end\":38106,\"start\":38105},{\"end\":38114,\"start\":38113},{\"end\":38121,\"start\":38120},{\"end\":38755,\"start\":38754},{\"end\":38762,\"start\":38761},{\"end\":38768,\"start\":38767},{\"end\":39030,\"start\":39029},{\"end\":39037,\"start\":39036},{\"end\":39050,\"start\":39049},{\"end\":39059,\"start\":39058},{\"end\":39067,\"start\":39066},{\"end\":39075,\"start\":39074},{\"end\":39077,\"start\":39076},{\"end\":39377,\"start\":39376},{\"end\":39384,\"start\":39383},{\"end\":39397,\"start\":39396},{\"end\":39405,\"start\":39404},{\"end\":39407,\"start\":39406},{\"end\":39415,\"start\":39414},{\"end\":39417,\"start\":39416},{\"end\":39720,\"start\":39719},{\"end\":39727,\"start\":39726},{\"end\":39740,\"start\":39739},{\"end\":39746,\"start\":39745},{\"end\":39748,\"start\":39747},{\"end\":39755,\"start\":39754},{\"end\":40088,\"start\":40087},{\"end\":40095,\"start\":40094},{\"end\":40108,\"start\":40107},{\"end\":40114,\"start\":40113},{\"end\":40429,\"start\":40428},{\"end\":40436,\"start\":40435},{\"end\":40444,\"start\":40443},{\"end\":40450,\"start\":40449},{\"end\":40452,\"start\":40451},{\"end\":40460,\"start\":40459},{\"end\":40462,\"start\":40461},{\"end\":40914,\"start\":40913},{\"end\":40921,\"start\":40920},{\"end\":40928,\"start\":40927},{\"end\":41304,\"start\":41303},{\"end\":41311,\"start\":41310},{\"end\":41318,\"start\":41317},{\"end\":41562,\"start\":41561},{\"end\":41569,\"start\":41568},{\"end\":41982,\"start\":41981},{\"end\":41995,\"start\":41994},{\"end\":42004,\"start\":42003},{\"end\":42014,\"start\":42013},{\"end\":42022,\"start\":42021},{\"end\":42308,\"start\":42307},{\"end\":42316,\"start\":42315},{\"end\":42325,\"start\":42324},{\"end\":42331,\"start\":42330},{\"end\":42643,\"start\":42642},{\"end\":42656,\"start\":42655},{\"end\":42663,\"start\":42662},{\"end\":42665,\"start\":42664},{\"end\":42671,\"start\":42670},{\"end\":43106,\"start\":43105},{\"end\":43113,\"start\":43112},{\"end\":43122,\"start\":43121},{\"end\":43131,\"start\":43130},{\"end\":43495,\"start\":43494},{\"end\":43502,\"start\":43501},{\"end\":43511,\"start\":43510},{\"end\":43520,\"start\":43519},{\"end\":43907,\"start\":43906},{\"end\":43913,\"start\":43912},{\"end\":43921,\"start\":43920},{\"end\":43929,\"start\":43928},{\"end\":43937,\"start\":43936},{\"end\":44338,\"start\":44337},{\"end\":44346,\"start\":44345},{\"end\":44353,\"start\":44352},{\"end\":44361,\"start\":44360},{\"end\":44369,\"start\":44368},{\"end\":44711,\"start\":44710},{\"end\":44725,\"start\":44724},{\"end\":44735,\"start\":44734},{\"end\":44749,\"start\":44748},{\"end\":44762,\"start\":44761},{\"end\":45094,\"start\":45093},{\"end\":45105,\"start\":45104},{\"end\":45118,\"start\":45117},{\"end\":45127,\"start\":45126},{\"end\":45137,\"start\":45136},{\"end\":45578,\"start\":45577},{\"end\":45589,\"start\":45588},{\"end\":45599,\"start\":45598},{\"end\":45601,\"start\":45600},{\"end\":46001,\"start\":46000},{\"end\":46016,\"start\":46015},{\"end\":46026,\"start\":46025},{\"end\":46541,\"start\":46540},{\"end\":46549,\"start\":46548},{\"end\":46556,\"start\":46555},{\"end\":46562,\"start\":46561},{\"end\":46875,\"start\":46874},{\"end\":46883,\"start\":46882},{\"end\":46890,\"start\":46889},{\"end\":46897,\"start\":46896},{\"end\":46903,\"start\":46902},{\"end\":46905,\"start\":46904},{\"end\":47338,\"start\":47337},{\"end\":47346,\"start\":47345},{\"end\":47355,\"start\":47354},{\"end\":47363,\"start\":47362},{\"end\":47371,\"start\":47370},{\"end\":47378,\"start\":47377},{\"end\":47386,\"start\":47385},{\"end\":47762,\"start\":47761},{\"end\":47764,\"start\":47763},{\"end\":47771,\"start\":47770},{\"end\":47778,\"start\":47777},{\"end\":47784,\"start\":47783},{\"end\":47786,\"start\":47785},{\"end\":47795,\"start\":47794},{\"end\":48239,\"start\":48238},{\"end\":48246,\"start\":48245},{\"end\":48253,\"start\":48252},{\"end\":48259,\"start\":48258},{\"end\":48268,\"start\":48267},{\"end\":49156,\"start\":49155},{\"end\":49163,\"start\":49162},{\"end\":49172,\"start\":49171},{\"end\":49539,\"start\":49538},{\"end\":49552,\"start\":49551},{\"end\":49890,\"start\":49889},{\"end\":49899,\"start\":49898},{\"end\":49906,\"start\":49905},{\"end\":49914,\"start\":49913},{\"end\":49922,\"start\":49921},{\"end\":49929,\"start\":49928},{\"end\":50388,\"start\":50387},{\"end\":50396,\"start\":50395},{\"end\":50404,\"start\":50403},{\"end\":50412,\"start\":50411},{\"end\":50423,\"start\":50422},{\"end\":50425,\"start\":50424},{\"end\":50888,\"start\":50887},{\"end\":50897,\"start\":50896},{\"end\":50903,\"start\":50902},{\"end\":50912,\"start\":50911},{\"end\":50921,\"start\":50920}]", "bib_author_last_name": "[{\"end\":30599,\"start\":30592},{\"end\":30607,\"start\":30603},{\"end\":30618,\"start\":30611},{\"end\":30630,\"start\":30624},{\"end\":30644,\"start\":30636},{\"end\":30910,\"start\":30903},{\"end\":30922,\"start\":30916},{\"end\":31244,\"start\":31242},{\"end\":31252,\"start\":31248},{\"end\":31260,\"start\":31256},{\"end\":31672,\"start\":31664},{\"end\":31682,\"start\":31676},{\"end\":31694,\"start\":31688},{\"end\":31706,\"start\":31698},{\"end\":31720,\"start\":31710},{\"end\":32158,\"start\":32152},{\"end\":32167,\"start\":32164},{\"end\":32175,\"start\":32173},{\"end\":32483,\"start\":32481},{\"end\":32494,\"start\":32489},{\"end\":32501,\"start\":32498},{\"end\":32510,\"start\":32505},{\"end\":32923,\"start\":32921},{\"end\":32934,\"start\":32929},{\"end\":32940,\"start\":32938},{\"end\":32948,\"start\":32944},{\"end\":32957,\"start\":32954},{\"end\":32966,\"start\":32961},{\"end\":33185,\"start\":33183},{\"end\":33193,\"start\":33189},{\"end\":33200,\"start\":33197},{\"end\":33582,\"start\":33580},{\"end\":33595,\"start\":33586},{\"end\":33601,\"start\":33599},{\"end\":33610,\"start\":33605},{\"end\":33622,\"start\":33614},{\"end\":33980,\"start\":33978},{\"end\":33993,\"start\":33984},{\"end\":33999,\"start\":33997},{\"end\":34008,\"start\":34003},{\"end\":34020,\"start\":34012},{\"end\":34343,\"start\":34340},{\"end\":34353,\"start\":34347},{\"end\":34763,\"start\":34760},{\"end\":34770,\"start\":34767},{\"end\":34778,\"start\":34774},{\"end\":34785,\"start\":34782},{\"end\":35212,\"start\":35210},{\"end\":35218,\"start\":35216},{\"end\":35227,\"start\":35222},{\"end\":35233,\"start\":35231},{\"end\":35458,\"start\":35456},{\"end\":35465,\"start\":35462},{\"end\":35474,\"start\":35469},{\"end\":35482,\"start\":35478},{\"end\":35489,\"start\":35486},{\"end\":35495,\"start\":35493},{\"end\":35920,\"start\":35918},{\"end\":35929,\"start\":35924},{\"end\":35936,\"start\":35933},{\"end\":35942,\"start\":35940},{\"end\":36474,\"start\":36472},{\"end\":36483,\"start\":36478},{\"end\":36492,\"start\":36487},{\"end\":36501,\"start\":36496},{\"end\":36509,\"start\":36505},{\"end\":36796,\"start\":36794},{\"end\":36804,\"start\":36800},{\"end\":36812,\"start\":36808},{\"end\":36821,\"start\":36816},{\"end\":36829,\"start\":36825},{\"end\":36837,\"start\":36833},{\"end\":37236,\"start\":37234},{\"end\":37244,\"start\":37240},{\"end\":37252,\"start\":37248},{\"end\":37261,\"start\":37256},{\"end\":37269,\"start\":37265},{\"end\":37277,\"start\":37273},{\"end\":37757,\"start\":37755},{\"end\":37764,\"start\":37761},{\"end\":37772,\"start\":37770},{\"end\":38097,\"start\":38095},{\"end\":38103,\"start\":38101},{\"end\":38111,\"start\":38107},{\"end\":38118,\"start\":38115},{\"end\":38125,\"start\":38122},{\"end\":38759,\"start\":38756},{\"end\":38765,\"start\":38763},{\"end\":38772,\"start\":38769},{\"end\":39034,\"start\":39031},{\"end\":39047,\"start\":39038},{\"end\":39056,\"start\":39051},{\"end\":39064,\"start\":39060},{\"end\":39072,\"start\":39068},{\"end\":39081,\"start\":39078},{\"end\":39381,\"start\":39378},{\"end\":39394,\"start\":39385},{\"end\":39402,\"start\":39398},{\"end\":39412,\"start\":39408},{\"end\":39426,\"start\":39418},{\"end\":39724,\"start\":39721},{\"end\":39737,\"start\":39728},{\"end\":39743,\"start\":39741},{\"end\":39752,\"start\":39749},{\"end\":39760,\"start\":39756},{\"end\":40092,\"start\":40089},{\"end\":40105,\"start\":40096},{\"end\":40111,\"start\":40109},{\"end\":40119,\"start\":40115},{\"end\":40433,\"start\":40430},{\"end\":40441,\"start\":40437},{\"end\":40447,\"start\":40445},{\"end\":40457,\"start\":40453},{\"end\":40466,\"start\":40463},{\"end\":40918,\"start\":40915},{\"end\":40925,\"start\":40922},{\"end\":40933,\"start\":40929},{\"end\":41308,\"start\":41305},{\"end\":41315,\"start\":41312},{\"end\":41323,\"start\":41319},{\"end\":41566,\"start\":41563},{\"end\":41574,\"start\":41570},{\"end\":41992,\"start\":41983},{\"end\":42001,\"start\":41996},{\"end\":42011,\"start\":42005},{\"end\":42019,\"start\":42015},{\"end\":42028,\"start\":42023},{\"end\":42313,\"start\":42309},{\"end\":42322,\"start\":42317},{\"end\":42328,\"start\":42326},{\"end\":42337,\"start\":42332},{\"end\":42653,\"start\":42644},{\"end\":42660,\"start\":42657},{\"end\":42668,\"start\":42666},{\"end\":42676,\"start\":42672},{\"end\":43110,\"start\":43107},{\"end\":43119,\"start\":43114},{\"end\":43128,\"start\":43123},{\"end\":43134,\"start\":43132},{\"end\":43499,\"start\":43496},{\"end\":43508,\"start\":43503},{\"end\":43517,\"start\":43512},{\"end\":43523,\"start\":43521},{\"end\":43910,\"start\":43908},{\"end\":43918,\"start\":43914},{\"end\":43926,\"start\":43922},{\"end\":43934,\"start\":43930},{\"end\":43941,\"start\":43938},{\"end\":44343,\"start\":44339},{\"end\":44350,\"start\":44347},{\"end\":44358,\"start\":44354},{\"end\":44366,\"start\":44362},{\"end\":44373,\"start\":44370},{\"end\":44722,\"start\":44712},{\"end\":44732,\"start\":44726},{\"end\":44746,\"start\":44736},{\"end\":44759,\"start\":44750},{\"end\":44776,\"start\":44763},{\"end\":45102,\"start\":45095},{\"end\":45115,\"start\":45106},{\"end\":45124,\"start\":45119},{\"end\":45134,\"start\":45128},{\"end\":45143,\"start\":45138},{\"end\":45586,\"start\":45579},{\"end\":45596,\"start\":45590},{\"end\":45604,\"start\":45602},{\"end\":46013,\"start\":46002},{\"end\":46023,\"start\":46017},{\"end\":46036,\"start\":46027},{\"end\":46546,\"start\":46542},{\"end\":46553,\"start\":46550},{\"end\":46559,\"start\":46557},{\"end\":46567,\"start\":46563},{\"end\":46880,\"start\":46876},{\"end\":46887,\"start\":46884},{\"end\":46894,\"start\":46891},{\"end\":46900,\"start\":46898},{\"end\":46909,\"start\":46906},{\"end\":47343,\"start\":47339},{\"end\":47352,\"start\":47347},{\"end\":47360,\"start\":47356},{\"end\":47368,\"start\":47364},{\"end\":47375,\"start\":47372},{\"end\":47383,\"start\":47379},{\"end\":47395,\"start\":47387},{\"end\":47768,\"start\":47765},{\"end\":47775,\"start\":47772},{\"end\":47781,\"start\":47779},{\"end\":47792,\"start\":47787},{\"end\":47799,\"start\":47796},{\"end\":48243,\"start\":48240},{\"end\":48250,\"start\":48247},{\"end\":48256,\"start\":48254},{\"end\":48265,\"start\":48260},{\"end\":48272,\"start\":48269},{\"end\":49160,\"start\":49157},{\"end\":49169,\"start\":49164},{\"end\":49176,\"start\":49173},{\"end\":49549,\"start\":49540},{\"end\":49562,\"start\":49553},{\"end\":49896,\"start\":49891},{\"end\":49903,\"start\":49900},{\"end\":49911,\"start\":49907},{\"end\":49919,\"start\":49915},{\"end\":49926,\"start\":49923},{\"end\":49935,\"start\":49930},{\"end\":50393,\"start\":50389},{\"end\":50401,\"start\":50397},{\"end\":50409,\"start\":50405},{\"end\":50420,\"start\":50413},{\"end\":50433,\"start\":50426},{\"end\":50894,\"start\":50889},{\"end\":50900,\"start\":50898},{\"end\":50909,\"start\":50904},{\"end\":50918,\"start\":50913},{\"end\":50926,\"start\":50922}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1907.13025\",\"id\":\"b0\"},\"end\":30899,\"start\":30473},{\"attributes\":{\"doi\":\"arXiv:1708.04552\",\"id\":\"b1\"},\"end\":31161,\"start\":30901},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8040013},\"end\":31611,\"start\":31163},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1945911},\"end\":32085,\"start\":31613},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":53111490},\"end\":32403,\"start\":32087},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5652420},\"end\":32871,\"start\":32405},{\"attributes\":{\"id\":\"b6\"},\"end\":33146,\"start\":32873},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":140309863},\"end\":33506,\"start\":33148},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9085113},\"end\":33902,\"start\":33508},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4087603},\"end\":34257,\"start\":33904},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3243570},\"end\":34657,\"start\":34259},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":12965906},\"end\":35137,\"start\":34659},{\"attributes\":{\"id\":\"b12\"},\"end\":35344,\"start\":35139},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1709967},\"end\":35844,\"start\":35346},{\"attributes\":{\"doi\":\"10.1109/ICMEW.2017.8026285\",\"id\":\"b14\",\"matched_paper_id\":12354538},\"end\":36409,\"start\":35846},{\"attributes\":{\"doi\":\"CoRR abs/1805.02556\",\"id\":\"b15\"},\"end\":36702,\"start\":36411},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":139101198},\"end\":37142,\"start\":36704},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":139101198},\"end\":37671,\"start\":37144},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11118105},\"end\":38008,\"start\":37673},{\"attributes\":{\"doi\":\"10.1109/CVPR.2018.00572\",\"id\":\"b19\",\"matched_paper_id\":3880365},\"end\":38671,\"start\":38010},{\"attributes\":{\"doi\":\"arXiv:1705.08106\",\"id\":\"b20\"},\"end\":38951,\"start\":38673},{\"attributes\":{\"doi\":\"CoRR abs/1905.04757\",\"id\":\"b21\"},\"end\":39303,\"start\":38953},{\"attributes\":{\"id\":\"b22\"},\"end\":39630,\"start\":39305},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":20813768},\"end\":40014,\"start\":39632},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2654595},\"end\":40354,\"start\":40016},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":413488},\"end\":40834,\"start\":40356},{\"attributes\":{\"doi\":\"10.1016/j.patcog.2017.02.030\",\"id\":\"b26\",\"matched_paper_id\":43823483},\"end\":41224,\"start\":40836},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":43823483},\"end\":41491,\"start\":41226},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":49553922},\"end\":41907,\"start\":41493},{\"attributes\":{\"doi\":\"arXiv:1611.06440\",\"id\":\"b29\"},\"end\":42229,\"start\":41909},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":212859361},\"end\":42574,\"start\":42231},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15928602},\"end\":43032,\"start\":42576},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":195446390},\"end\":43404,\"start\":43034},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":195440283},\"end\":43810,\"start\":43406},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":67856333},\"end\":44240,\"start\":43812},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14298099},\"end\":44641,\"start\":44242},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6844431},\"end\":45032,\"start\":44643},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206593880},\"end\":45512,\"start\":45034},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":11060863},\"end\":45918,\"start\":45514},{\"attributes\":{\"doi\":\"10.1109/CVPR.2014.82\",\"id\":\"b39\",\"matched_paper_id\":1732632},\"end\":46477,\"start\":45920},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":206765283},\"end\":46818,\"start\":46479},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2239612},\"end\":47256,\"start\":46820},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5711057},\"end\":47670,\"start\":47258},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":69769666},\"end\":48147,\"start\":47672},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":69769666},\"end\":49068,\"start\":48149},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":19167105},\"end\":49417,\"start\":49070},{\"attributes\":{\"doi\":\"arXiv:1612.03928\",\"id\":\"b46\"},\"end\":49781,\"start\":49419},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":12699455},\"end\":50317,\"start\":49783},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":102351949},\"end\":50826,\"start\":50319},{\"attributes\":{\"doi\":\"arXiv:1805.02556\",\"id\":\"b49\"},\"end\":51108,\"start\":50828}]", "bib_title": "[{\"end\":31238,\"start\":31163},{\"end\":31660,\"start\":31613},{\"end\":32148,\"start\":32087},{\"end\":32475,\"start\":32405},{\"end\":33179,\"start\":33148},{\"end\":33576,\"start\":33508},{\"end\":33974,\"start\":33904},{\"end\":34334,\"start\":34259},{\"end\":34756,\"start\":34659},{\"end\":35452,\"start\":35346},{\"end\":35914,\"start\":35846},{\"end\":36790,\"start\":36704},{\"end\":37230,\"start\":37144},{\"end\":37751,\"start\":37673},{\"end\":38091,\"start\":38010},{\"end\":39717,\"start\":39632},{\"end\":40085,\"start\":40016},{\"end\":40426,\"start\":40356},{\"end\":40911,\"start\":40836},{\"end\":41301,\"start\":41226},{\"end\":41559,\"start\":41493},{\"end\":42305,\"start\":42231},{\"end\":42640,\"start\":42576},{\"end\":43103,\"start\":43034},{\"end\":43492,\"start\":43406},{\"end\":43904,\"start\":43812},{\"end\":44335,\"start\":44242},{\"end\":44708,\"start\":44643},{\"end\":45091,\"start\":45034},{\"end\":45575,\"start\":45514},{\"end\":45998,\"start\":45920},{\"end\":46538,\"start\":46479},{\"end\":46872,\"start\":46820},{\"end\":47335,\"start\":47258},{\"end\":47759,\"start\":47672},{\"end\":48236,\"start\":48149},{\"end\":49153,\"start\":49070},{\"end\":49887,\"start\":49783},{\"end\":50385,\"start\":50319}]", "bib_author": "[{\"end\":30601,\"start\":30590},{\"end\":30609,\"start\":30601},{\"end\":30620,\"start\":30609},{\"end\":30632,\"start\":30620},{\"end\":30646,\"start\":30632},{\"end\":30912,\"start\":30901},{\"end\":30924,\"start\":30912},{\"end\":31246,\"start\":31240},{\"end\":31254,\"start\":31246},{\"end\":31262,\"start\":31254},{\"end\":31674,\"start\":31662},{\"end\":31684,\"start\":31674},{\"end\":31696,\"start\":31684},{\"end\":31708,\"start\":31696},{\"end\":31722,\"start\":31708},{\"end\":32160,\"start\":32150},{\"end\":32169,\"start\":32160},{\"end\":32177,\"start\":32169},{\"end\":32485,\"start\":32477},{\"end\":32496,\"start\":32485},{\"end\":32503,\"start\":32496},{\"end\":32512,\"start\":32503},{\"end\":32925,\"start\":32917},{\"end\":32936,\"start\":32925},{\"end\":32942,\"start\":32936},{\"end\":32950,\"start\":32942},{\"end\":32959,\"start\":32950},{\"end\":32968,\"start\":32959},{\"end\":33187,\"start\":33181},{\"end\":33195,\"start\":33187},{\"end\":33202,\"start\":33195},{\"end\":33584,\"start\":33578},{\"end\":33597,\"start\":33584},{\"end\":33603,\"start\":33597},{\"end\":33612,\"start\":33603},{\"end\":33624,\"start\":33612},{\"end\":33982,\"start\":33976},{\"end\":33995,\"start\":33982},{\"end\":34001,\"start\":33995},{\"end\":34010,\"start\":34001},{\"end\":34022,\"start\":34010},{\"end\":34345,\"start\":34336},{\"end\":34355,\"start\":34345},{\"end\":34765,\"start\":34758},{\"end\":34772,\"start\":34765},{\"end\":34780,\"start\":34772},{\"end\":34787,\"start\":34780},{\"end\":35214,\"start\":35208},{\"end\":35220,\"start\":35214},{\"end\":35229,\"start\":35220},{\"end\":35235,\"start\":35229},{\"end\":35460,\"start\":35454},{\"end\":35467,\"start\":35460},{\"end\":35476,\"start\":35467},{\"end\":35484,\"start\":35476},{\"end\":35491,\"start\":35484},{\"end\":35497,\"start\":35491},{\"end\":35922,\"start\":35916},{\"end\":35931,\"start\":35922},{\"end\":35938,\"start\":35931},{\"end\":35944,\"start\":35938},{\"end\":36476,\"start\":36470},{\"end\":36485,\"start\":36476},{\"end\":36494,\"start\":36485},{\"end\":36503,\"start\":36494},{\"end\":36511,\"start\":36503},{\"end\":36798,\"start\":36792},{\"end\":36806,\"start\":36798},{\"end\":36814,\"start\":36806},{\"end\":36823,\"start\":36814},{\"end\":36831,\"start\":36823},{\"end\":36839,\"start\":36831},{\"end\":37238,\"start\":37232},{\"end\":37246,\"start\":37238},{\"end\":37254,\"start\":37246},{\"end\":37263,\"start\":37254},{\"end\":37271,\"start\":37263},{\"end\":37279,\"start\":37271},{\"end\":37759,\"start\":37753},{\"end\":37766,\"start\":37759},{\"end\":37774,\"start\":37766},{\"end\":38099,\"start\":38093},{\"end\":38105,\"start\":38099},{\"end\":38113,\"start\":38105},{\"end\":38120,\"start\":38113},{\"end\":38127,\"start\":38120},{\"end\":38761,\"start\":38754},{\"end\":38767,\"start\":38761},{\"end\":38774,\"start\":38767},{\"end\":39036,\"start\":39029},{\"end\":39049,\"start\":39036},{\"end\":39058,\"start\":39049},{\"end\":39066,\"start\":39058},{\"end\":39074,\"start\":39066},{\"end\":39083,\"start\":39074},{\"end\":39383,\"start\":39376},{\"end\":39396,\"start\":39383},{\"end\":39404,\"start\":39396},{\"end\":39414,\"start\":39404},{\"end\":39428,\"start\":39414},{\"end\":39726,\"start\":39719},{\"end\":39739,\"start\":39726},{\"end\":39745,\"start\":39739},{\"end\":39754,\"start\":39745},{\"end\":39762,\"start\":39754},{\"end\":40094,\"start\":40087},{\"end\":40107,\"start\":40094},{\"end\":40113,\"start\":40107},{\"end\":40121,\"start\":40113},{\"end\":40435,\"start\":40428},{\"end\":40443,\"start\":40435},{\"end\":40449,\"start\":40443},{\"end\":40459,\"start\":40449},{\"end\":40468,\"start\":40459},{\"end\":40920,\"start\":40913},{\"end\":40927,\"start\":40920},{\"end\":40935,\"start\":40927},{\"end\":41310,\"start\":41303},{\"end\":41317,\"start\":41310},{\"end\":41325,\"start\":41317},{\"end\":41568,\"start\":41561},{\"end\":41576,\"start\":41568},{\"end\":41994,\"start\":41981},{\"end\":42003,\"start\":41994},{\"end\":42013,\"start\":42003},{\"end\":42021,\"start\":42013},{\"end\":42030,\"start\":42021},{\"end\":42315,\"start\":42307},{\"end\":42324,\"start\":42315},{\"end\":42330,\"start\":42324},{\"end\":42339,\"start\":42330},{\"end\":42655,\"start\":42642},{\"end\":42662,\"start\":42655},{\"end\":42670,\"start\":42662},{\"end\":42678,\"start\":42670},{\"end\":43112,\"start\":43105},{\"end\":43121,\"start\":43112},{\"end\":43130,\"start\":43121},{\"end\":43136,\"start\":43130},{\"end\":43501,\"start\":43494},{\"end\":43510,\"start\":43501},{\"end\":43519,\"start\":43510},{\"end\":43525,\"start\":43519},{\"end\":43912,\"start\":43906},{\"end\":43920,\"start\":43912},{\"end\":43928,\"start\":43920},{\"end\":43936,\"start\":43928},{\"end\":43943,\"start\":43936},{\"end\":44345,\"start\":44337},{\"end\":44352,\"start\":44345},{\"end\":44360,\"start\":44352},{\"end\":44368,\"start\":44360},{\"end\":44375,\"start\":44368},{\"end\":44724,\"start\":44710},{\"end\":44734,\"start\":44724},{\"end\":44748,\"start\":44734},{\"end\":44761,\"start\":44748},{\"end\":44778,\"start\":44761},{\"end\":45104,\"start\":45093},{\"end\":45117,\"start\":45104},{\"end\":45126,\"start\":45117},{\"end\":45136,\"start\":45126},{\"end\":45145,\"start\":45136},{\"end\":45588,\"start\":45577},{\"end\":45598,\"start\":45588},{\"end\":45606,\"start\":45598},{\"end\":46015,\"start\":46000},{\"end\":46025,\"start\":46015},{\"end\":46038,\"start\":46025},{\"end\":46548,\"start\":46540},{\"end\":46555,\"start\":46548},{\"end\":46561,\"start\":46555},{\"end\":46569,\"start\":46561},{\"end\":46882,\"start\":46874},{\"end\":46889,\"start\":46882},{\"end\":46896,\"start\":46889},{\"end\":46902,\"start\":46896},{\"end\":46911,\"start\":46902},{\"end\":47345,\"start\":47337},{\"end\":47354,\"start\":47345},{\"end\":47362,\"start\":47354},{\"end\":47370,\"start\":47362},{\"end\":47377,\"start\":47370},{\"end\":47385,\"start\":47377},{\"end\":47397,\"start\":47385},{\"end\":47770,\"start\":47761},{\"end\":47777,\"start\":47770},{\"end\":47783,\"start\":47777},{\"end\":47794,\"start\":47783},{\"end\":47801,\"start\":47794},{\"end\":48245,\"start\":48238},{\"end\":48252,\"start\":48245},{\"end\":48258,\"start\":48252},{\"end\":48267,\"start\":48258},{\"end\":48274,\"start\":48267},{\"end\":49162,\"start\":49155},{\"end\":49171,\"start\":49162},{\"end\":49178,\"start\":49171},{\"end\":49551,\"start\":49538},{\"end\":49564,\"start\":49551},{\"end\":49898,\"start\":49889},{\"end\":49905,\"start\":49898},{\"end\":49913,\"start\":49905},{\"end\":49921,\"start\":49913},{\"end\":49928,\"start\":49921},{\"end\":49937,\"start\":49928},{\"end\":50395,\"start\":50387},{\"end\":50403,\"start\":50395},{\"end\":50411,\"start\":50403},{\"end\":50422,\"start\":50411},{\"end\":50435,\"start\":50422},{\"end\":50896,\"start\":50887},{\"end\":50902,\"start\":50896},{\"end\":50911,\"start\":50902},{\"end\":50920,\"start\":50911},{\"end\":50928,\"start\":50920}]", "bib_venue": "[{\"end\":31403,\"start\":31341},{\"end\":31863,\"start\":31801},{\"end\":32653,\"start\":32591},{\"end\":33343,\"start\":33281},{\"end\":33671,\"start\":33656},{\"end\":34908,\"start\":34856},{\"end\":36069,\"start\":36053},{\"end\":37420,\"start\":37358},{\"end\":38249,\"start\":38226},{\"end\":40609,\"start\":40547},{\"end\":41717,\"start\":41655},{\"end\":42819,\"start\":42757},{\"end\":45286,\"start\":45224},{\"end\":45727,\"start\":45675},{\"end\":46140,\"start\":46123},{\"end\":47052,\"start\":46990},{\"end\":47910,\"start\":47864},{\"end\":48546,\"start\":48525},{\"end\":50058,\"start\":50006},{\"end\":50525,\"start\":50506},{\"end\":30588,\"start\":30473},{\"end\":31008,\"start\":30940},{\"end\":31339,\"start\":31262},{\"end\":31799,\"start\":31722},{\"end\":32226,\"start\":32177},{\"end\":32589,\"start\":32512},{\"end\":32915,\"start\":32873},{\"end\":33279,\"start\":33202},{\"end\":33654,\"start\":33624},{\"end\":34059,\"start\":34022},{\"end\":34436,\"start\":34355},{\"end\":34854,\"start\":34787},{\"end\":35206,\"start\":35139},{\"end\":35570,\"start\":35497},{\"end\":36051,\"start\":35970},{\"end\":36468,\"start\":36411},{\"end\":36908,\"start\":36839},{\"end\":37356,\"start\":37279},{\"end\":37830,\"start\":37774},{\"end\":38224,\"start\":38150},{\"end\":38752,\"start\":38673},{\"end\":39027,\"start\":38953},{\"end\":39374,\"start\":39305},{\"end\":39779,\"start\":39762},{\"end\":40159,\"start\":40121},{\"end\":40545,\"start\":40468},{\"end\":40982,\"start\":40963},{\"end\":41344,\"start\":41325},{\"end\":41653,\"start\":41576},{\"end\":41979,\"start\":41909},{\"end\":42391,\"start\":42339},{\"end\":42755,\"start\":42678},{\"end\":43205,\"start\":43136},{\"end\":43594,\"start\":43525},{\"end\":44012,\"start\":43943},{\"end\":44430,\"start\":44375},{\"end\":44818,\"start\":44778},{\"end\":45222,\"start\":45145},{\"end\":45673,\"start\":45606},{\"end\":46121,\"start\":46058},{\"end\":46631,\"start\":46569},{\"end\":46988,\"start\":46911},{\"end\":47435,\"start\":47397},{\"end\":47862,\"start\":47801},{\"end\":48523,\"start\":48274},{\"end\":49234,\"start\":49178},{\"end\":49536,\"start\":49419},{\"end\":50004,\"start\":49937},{\"end\":50504,\"start\":50435},{\"end\":50885,\"start\":50828}]"}}}, "year": 2023, "month": 12, "day": 17}