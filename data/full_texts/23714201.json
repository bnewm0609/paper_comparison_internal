{"id": 23714201, "updated": "2023-09-28 15:52:39.427", "metadata": {"title": "Improved Regularization of Convolutional Neural Networks with Cutout", "authors": "[{\"first\":\"Terrance\",\"last\":\"DeVries\",\"middle\":[]},{\"first\":\"Graham\",\"last\":\"Taylor\",\"middle\":[\"W.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 8, "day": 15}, "abstract": "Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results with almost no additional computational cost. We also show improved performance in the low-data regime on the STL-10 dataset.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1708.04552", "mag": "2746314669", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1708-04552", "doi": null}}, "content": {"source": {"pdf_hash": "5495926e45784daada9a5f8f60f966a4eafaf54d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1708.04552v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1068ce6a02cdb7a1c7815f7b6a2f19be5e4746a7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5495926e45784daada9a5f8f60f966a4eafaf54d.txt", "contents": "\nImproved Regularization of Convolutional Neural Networks with Cutout\n\n\nTerrance Devries \nUniversity of Guelph\n\n\nGraham W Taylor \nUniversity of Guelph\n\n\nCanadian Institute for Advanced Research and Vector Institute\n\n\nImproved Regularization of Convolutional Neural Networks with Cutout\n\nConvolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well.In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results with almost no additional computational cost. We also show improved performance in the low-data regime on the STL-10 dataset.\n\nIntroduction\n\nIn recent years deep learning has contributed to considerable advances in the field of computer vision, resulting in state-of-the-art performance in many challenging vision tasks such as object recognition [8], semantic segmentation [11], image captioning [19], and human pose estimation [17]. Much of these improvements can be attributed to the use of convolutional neural networks (CNNs) [9], which are capable of learning complex hierarchical feature representations of images. As the complexity of the task to be solved increases, the resource utilization of such models increases as well: memory footprint, parameters, operations count, inference time and power consumption [2]. Modern networks commonly contain on the order of tens to hundreds of millions of learned parameters which provide the necessary representational power for such tasks, but with the increased representational power also comes increased probability of overfitting, leading to poor generalization.\n\nIn order to combat the potential for overfitting, several different regularization techniques can be applied, such as data augmentation or the judicious addition of noise to activations, parameters, or data. In the domain of computer vision, data augmentation is almost ubiquitous due to its ease of implementation and effectiveness. Simple image transforms such as mirroring or cropping can be applied to create new training data which can be used to improve model robustness and increase accuracy [9]. Large models can also be regularized by adding noise during the training process, whether it be added to the input, weights, or gradients. One of the most common uses of noise for improving model accuracy is dropout [6], which stochastically drops neuron activations during training and as a result discourages the co-adaptation of feature detectors.\n\nIn this work we consider applying noise in a similar fashion to dropout, but with two important distinctions. The first difference is that units are dropped out only at the input layer of a CNN, rather than in the intermediate feature layers. The second difference is that we drop out contiguous sections of inputs rather than individual pixels, as demon-strated in Figure 1. In this fashion, dropped out regions are propagated through all subsequent feature maps, producing a final representation of the image which contains no trace of the removed input, other than what can be recovered by its context. This technique encourages the network to better utilize the full context of the image, rather than relying on the presence of a small set of specific visual features. This method, which we call cutout, can be interpreted as applying a spatial prior to dropout in input space, much in the same way that convolutional neural networks leverage information about spatial structure in order to improve performance over that of feed-forward networks.\n\nIn the remainder of this paper, we introduce cutout and demonstrate that masking out contiguous sections of the input to convolutional neural networks can improve model robustness and ultimately yield better model performance. We show that this simple method works in conjunction with other current state-of-the-art techniques such as residual networks and batch normalization, and can also be combined with most regularization techniques, including standard dropout and data augmentation. Additionally, cutout can be applied during data loading in parallel with the main training task, making it effectively computationally free. To evaluate this technique we conduct tests on several popular image recognition datasets, achieving state-of-the-art results on CIFAR-10, CIFAR-100, and SVHN. We also achieve competitive results on STL-10, demonstrating the usefulness of cutout for low data and higher resolution problems.\n\n\nRelated Work\n\nOur work is most closely related to two common regularization techniques: data augmentation and dropout. Here we examine the use of both methods in the setting of training convolutional neural networks. We also discuss denoising auto-encoders and context encoders, which share some similarities with our work.\n\n\nData Augmentation for Images\n\nData augmentation has long been used in practice when training convolutional neural networks. When training LeNet5 [9] for optical character recognition, LeCun et al. apply various affine transforms, including horizontal and vertical translation, scaling, squeezing, and horizontal shearing to improve their model's accuracy and robustness.\n\nIn [1], Bengio et al. demonstrate that deep architectures benefit much more from data augmentation than shallow architectures. They apply a large variety of transformations to their handwritten character dataset, including local elastic deformation, motion blur, Gaussian smoothing, Gaussian noise, salt and pepper noise, pixel permutation, and adding fake scratches and other occlusions to the images, in addition to affine transformations.\n\nTo improve the performance of AlexNet [8] for the 2012 ImageNet Large Scale Visual Recognition Competition, Krizhevsky et al. apply image mirroring, cropping, as well as randomly adjusting colour and intensity values based on ranges determined using principal component analysis on the dataset.\n\nWu et al. take a more aggressive approach with image augmentation when training Deep Image [21] on the Ima-geNet dataset. In addition to flipping and cropping they apply a wide range of colour casting, vignetting, rotation, and lens distortion (pin cushion and barrel distortion), as well as horizontal and vertical stretching.\n\nLemley et al. tackle the issue of data augmentation with a learned end-to-end approach called Smart Augmentation [10] instead of relying on hard-coded transformations. In this method, a neural network is trained to intelligently combine existing samples in order to generate additional data that is useful for the training process.\n\nOf these techniques ours is closest to the occlusions applied in [1], however their occlusions generally take the form of scratches, dots, or scribbles that overlay the target character, while we use zero-masking to completely obstruct an entire region.\n\n\nDropout in Convolutional Neural Networks\n\nAnother common regularization technique is dropout [6,15], which was first introduced by Hinton et al. Dropout is implemented by setting hidden unit activations to zero with some fixed probability during training. All activations are kept when evaluating the network, but the resulting output is scaled according to the dropout probability. This technique has the effect of approximately averaging over an exponential number of smaller sub-networks, and works well as a robust type of bagging, which discourages the co-adaptation of feature detectors within the network.\n\nWhile dropout was found to be very effective at regularizing fully-connected layers, it appears to be less powerful when used with convolutional layers [16]. This reduction in potency can largely be attributed to two factors. The first is that convolutional layers already have much fewer parameters than fully-connected layers, and therefore require less regularization. The second factor is that neighbouring pixels in images share much of the same information. If any of them are dropped out then the information they contain will likely still be passed on from the neighbouring pixels that are still active. For these reasons, dropout in convolutional layers simply acts to increase robustness to noisy inputs, rather than having the same model averaging effect that is observed in fully-connected layers.\n\nIn an attempt to increase the effectiveness of dropout in convolutional layers, several variations on the original dropout formula have been proposed. Tompson et al. introduce SpatialDropout [16], which randomly discards en-tire feature maps rather than individual pixels, effectively bypassing the issue of neighbouring pixels passing similar information.\n\nWu and Gu propose probabilistic weighted pooling [20], wherein activations in each pooling region are dropped with some probability. This approach is similar to applying dropout before each pooling layer, except that instead of scaling the output with respect to the dropout probability at test time, the output of each pooling function is selected to be the sum of the activations weighted by the dropout probability. The authors claim that this approach approximates averaging over an exponential number of sub-networks as dropout does.\n\nIn a more targeted approach, Park and Kwak introduce max-drop [13], which drops the maximal activation across feature maps or channels with some probability. While this regularization method performed better than conventional dropout on convolutional layers in some cases, they found that when used in CNNs that utilized batch normalization, both max-drop and SpatialDropout performed worse than standard dropout.\n\n\nDenoising Auto-encoders & Context Encoders\n\nDenosing auto-encoders [18] and context encoders [14] both rely on self-supervised learning to elicit useful feature representations of images. These models work by corrupting input images and requiring the network to reconstruct them using the remaining pixels as context to determine how best to fill in the blanks. Specifically, denoising autoencoders that apply Bernoulli noise randomly erase individual pixels in the input image, while context encoders erase larger spatial regions. In order to properly fill in the missing information, the auto-encoders are forced to learn how to extract useful features from the images, rather than simply learning an identity function. As context encoders are required to fill in a larger region of the image they are required to have a better understanding of the global content of the image, and therefore they learn higher-level features compared to denoising auto-encoders [14]. These feature representations have been demonstrated to be useful for pre-training classification, detection, and semantic segmentation models.\n\nWhile removing contiguous sections of the input has previously been used as an image corruption technique, like in context encoders, to our knowledge it has not previously been applied directly to the training of supervised models.\n\n\nCutout\n\nCutout is a simple regularization technique for convolutional neural networks that involves removing contiguous sections of input images, effectively augmenting the dataset with partially occluded versions of existing samples. This technique can be interpreted as an extension of dropout in input space, but with a spatial prior applied, much in the same way that CNNs apply a spatial prior to achieve improved performance over feed-forward networks on image data.\n\nFrom the comparison between dropout and cutout, we can also draw parallels to denoising autoencoders and context encoders. While both models have the same goal, context encoders are more effective at representation learning, as they force the model to understand the content of the image in a global sense, rather than a local sense as denoising auto-encoders do. In the same way, cutout forces models to take more of the full image context into consideration, rather than focusing on a few key visual features, which may not always be present.\n\nOne of the major differences between cutout and other dropout variants is that units are dropped at the input stage of the network rather than in the intermediate layers. This approach has the effect that visual features, including objects that are removed from the input image, are correspondingly removed from all subsequent feature maps. Other dropout variants generally consider each feature map individually, and as a result, features that are randomly removed from one feature map may still be present in others. These inconsistencies produce a noisy representation of the input image, thereby forcing the network to become more robust to noisy inputs. In this sense, cutout is much closer to data augmentation than dropout, as it is not creating noise, but instead generating images that appear novel to the network.\n\n\nMotivation\n\nThe main motivation for cutout comes from the problem of object occlusion, which is commonly encountered in many computer vision tasks, such as object recognition, tracking, or human pose estimation. By generating new images which simulate occluded examples, we not only better prepare the model for encounters with occlusions in the real world, but the model also learns to take more of the image context into consideration when making decisions.\n\nWe initially developed cutout as a targeted approach that specifically removed important visual features from the input of the image. This approach was similar to maxdrop [13], in that we aimed to remove maximally activated features in order to encourage the network to consider less prominent features. To accomplish this goal, we extracted and stored the maximally activated feature map for each image in the dataset at each epoch. During the next epoch we then upsampled the saved feature maps back to the input resolution, and thresholded them at the mean feature map value to obtain a binary mask, which was finally overlaid on the original image before being passed through the CNN. Figure 2 demonstrates this early version of cutout.\n\nWhile this targeted cutout method performed well, we found that randomly removing regions of a fixed size per- formed just as well as the targeted approach, without requiring any manipulation of the feature maps. Due to the inherent simplicity of this alternative approach, we focus on removing fixed-size regions for all of our experiments.\n\n\nImplementation Details\n\nTo implement cutout, we simply apply a fixed-size zeromask to a random location of each input image during each epoch of training, as shown in Figure 1. Unlike dropout and its variants, we do not apply any rescaling of weights at test time. For best performance, the dataset should be normalized about zero so that modified images will not have a large effect on the expected batch statistics.\n\nIn general, we found that the size of the cutout region is a more important hyperparameter than the shape, so for simplicity, we conduct all of our experiments using a square patch as the cutout region. When cutout is applied to an image, we randomly select a pixel coordinate within the image as a center point and then place the cutout mask around that location. This method allows for the possibility that not all parts of the cutout mask are contained within the image. Interestingly, we found that allowing portions of the patches to lay outside the borders of the image (rather than constraining the entire patch to be within the image) was critical to achieving good performance. Our explanation for this phenomenon is that it is important for the model to receive some examples where a large portion of the image is visible during training. An alternative approach that achieves similar performance is to randomly apply cutout constrained within the image region, but with 50% probability so that the network sometimes receives unmodified images.\n\nThe cutout operation can easily be applied on the CPU along with any other data augmentation steps during data loading. By implementing this operation on the CPU in parallel with the main GPU training task, we can hide the computation and obtain performance improvements for virtually free.\n\n\nExperiments\n\nTo evaluate the performance of cutout, we apply it to a variety of natural image recognition datasets: CIFAR-10, CIFAR-100, SVHN, and STL-10.\n\n\nCIFAR-10 and CIFAR-100\n\nBoth of the CIFAR datasets [7] consist of 60,000 colour images of size 32 \u00d7 32 pixels. CIFAR-10 has 10 distinct classes, such as cat, dog, car, and boat. CIFAR-100 contains 100 classes, but requires much more fine-grained recognition compared to CIFAR-10 as some classes are very visually similar. For example, it contains five different classes of trees: maple, oak, palm, pine, and willow. Each dataset is split into a training set with 50,000 images and a test set with 10,000 images.\n\nBoth datasets were normalized using per-channel mean and standard deviation. When required, we apply the standard data augmentation scheme for these datasets [5]. Images are first zero-padded with 4 pixels on each side to obtain a 40 \u00d7 40 pixel image, then a 32 \u00d7 32 crop is randomly extracted. Images are also randomly mirrored horizontally with 50% probability.\n\nTo evaluate cutout on the CIFAR datasets, we train models using two modern architectures: a deep residual network [5] with a depth of 18 (ResNet18), and a wide residual network [22] with a depth of 28, a widening factor of 10, and dropout with a drop probability of p = 0.3 in the convolutional layers (WRN-28-10). For both of these experiments, we use the same training procedure as specified in [22]. That is, we train for 200 epochs with batches of 128 images using SGD, Nesterov momentum of 0.9, and weight decay of 5e-4. The learning rate is initially set to 0.1, but is scheduled to decrease by a factor of 5x after each of the 60th, 120th, and 160th epochs. We also apply cutout to shake-shake regularization models [4] that currently achieve state-of-the-art performance on the CIFAR datasets, specifically a 26 2 \u00d7 96d \"Shake-Shake-Image\" ResNet for CIFAR-10 and a 29 2 \u00d7 4 \u00d7 64d \"Shake-Even-Image\" ResNeXt for CIFAR-100. For our tests, we use the original code and training settings provided by the author of [4], with the only change being the addition of cutout.\n\nTo find the best parameters for cutout we isolate 10% of the training set to use as a validation set and train on the remaining images. As our cutout shape is square, we perform a grid search over the side length parameter to find the optimal size. We find that model accuracy follows a parabolic trend, increasing proportionally to the cutout size until an optimal point, after which accuracy again decreases and eventually drops below that of the baseline model. This behaviour can be observed in Figure 3a and 3b, which depict the grid searches conducted on CIFAR-10 and CIFAR-100 respectively. Based on these validation results we select a cutout size of 16 \u00d7 16 pixels to use on CIFAR-10 and a   Table 1: Test error rates (%) on CIFAR (C10, C100) and SVHN datasets. \"+\" indicates standard data augmentation (mirror + crop). Results averaged over five runs, with the exception of shake-shake regularization which only had three runs each. Baseline shake-shake regularization results taken from [4].\n\ncutout size of 8 \u00d7 8 pixels for CIFAR-100 when training on the full datasets. Interestingly, it appears that as the number of classes increases, the optimal cutout size decreases. This makes sense, as when more fine-grained detection is required then the context of the image will be less useful for identifying the category. Instead, smaller and more nuanced details are important.\n\nAs shown in Table 1, the addition of cutout to the ResNet18 and WRN-28-10 models increased their accuracy on CIFAR-10 and CIFAR-100 by between 0.4 to 2.0 percentage points. We draw attention to the fact that cutout yields these performance improvements even when applied to complex models that already utilize batch normalization, dropout, and data augmentation. Adding cutout to the current state-of-the-art shake-shake regularization models improves performance by 0.3 and 0.6 percentage points on CIFAR-10 and CIFAR-100 respectively, yielding new stateof-the-art results of 2.56% and 15.20% test error.\n\n\nSVHN\n\nThe Street View House Numbers (SVHN) dataset [12] contains a total of 630,420 colour images with a resolution of 32 \u00d7 32 pixels. Each image is centered about a number from one to ten, which needs to be identified. The official dataset split contains 73,257 training images and 26,032 test images, but there are also 531,131 additional training images available. Following standard procedure for this dataset [22], we use both available training sets when training our models, and do not apply any data augmentation. All images are normalized using per-channel mean and standard deviation.\n\nTo evalute cutout on the SVHN dataset we apply it to a WideResNet with a depth of 16, a widening factor of 8, and dropout on the convolutional layers with a dropout rate of p = 0.4 (WRN- . This particular configuration currently holds state-of-the-art performance on the SVHN dataset with a test error of 1.54% [22]. We repeat the same training procedure as specified in [22] by training for 160 epochs with batches of 128 images. The network is optimized using SGD with Nesterov momentum of 0.9 and weight decay of 5e-4. The learning rate is initially set to 0.01, but is reduced by a factor of 10x after the 80th and 120th epochs. The one change we do make to the original training procedure (for both baseline and cutout) is to normalize the data so that it is compatible with cutout (see \u00a7 3.2). The original implementation scales data to lie between 0 and 1.\n\nTo find the optimal size for the cutout region we conduct a grid search using 10% of the training set for validation and ultimately select a cutout size of 20 \u00d7 20 pixels. While this may seem like a large portion of the image to remove, it is important to remember that the cutout patches are not constrained to lie fully within the bounds of the image.\n\nUsing these settings we train the WRN-16-8 and observe an average reduction in test error of 0.3 percentage points, resulting in a new state-of-the-art performance of 1.30% test error, as shown in Table 1.\n\n\nSTL-10\n\nThe STL-10 dataset [3] consists of a total of 113,000 colour images with a resolution of 96 \u00d7 96 pixels. The training set only contains 5,000 images while the test set consists of 8,000 images. All training and test set images belong to one of ten classes, such as airplane, bird, or horse. The remainder of the dataset is composed of 100,000 unlabeled images belonging to the target ten classes, plus additional but visually similar classes. While the main purpose of the STL-10 dataset is to test semi-supervised learning algorithms, we use it to observe how cutout performs when applied to higher resolution images in a low data setting. For this reason, we discard the unlabeled portion of the dataset and only use the labeled training set.\n\nThe dataset was normalized by subtracting the perchannel mean and dividing by the per-channel standard deviation. Simple data augmentation was also applied in a similar fashion to the CIFAR datasets. Specifically, images were zero-padded with 12 pixels on each side and then a 96 \u00d7 96 crop was randomly extracted. Mirroring horizontally was also applied with 50% probability.\n\nTo evaluate the performance of cutout on the STL-10 dataset we use a WideResNet with a depth of 16, a widening factor of 8, and dropout with a drop rate of p = 0.3 in the convolutional layers. We train the model for 1000 epochs with batches of 128 images using SGD with Nesterov momentum of 0.9 and weight decay of 5e-4. The learning rate is initially set to 0.1 but is reduced by a factor of 5x after the 300th, 400th, 600th, and 800th epochs.\n\nWe perform a grid search over the cutout size parameter using 10% of the training images as a validation set and select a square size of 24 \u00d7 24 pixels for the no dataaugmentation case and 32 \u00d7 32 pixels for training STL-10 with data augmentation. Training the model using these values yields a reduction in test error of 2.7 percentage points in the no data augmentation case, and 1.5 percentage points when also using data augmentation, as shown in Table 2.\nModel STL10 STL10+ WideResNet\n23.48 \u00b1 0.68 14.21 \u00b1 0.29 WideResNet + cutout 20.77 \u00b1 0.38 12.74 \u00b1 0.23 Table 2: Test error rates on STL-10 dataset. \"+\" indicates standard data augmentation (mirror + crop). Results averaged over five runs on full training set.\n\n\nAnalysis of Cutout's Effect on Activations\n\nIn order to better understand the effect of cutout, we compare the average magnitude of feature activations in a ResNet18 when trained with and without cutout on CIFAR-10. The models were trained with data augmentation using the same settings as defined in Section 4.1, achieving scores of 3.89% and 4.94% test error respectively.\n\nIn Figure 4, we sort the activations within each layer by ascending magnitude, averaged over all samples in the test set. We observe that the shallow layers of the network experience a general increase in activation strength, while in deeper layers, we see more activations in the tail end of the distribution. The latter observation illustrates that cutout is indeed encouraging the network to take into account a wider variety of features when making predictions, rather than relying on the presence of a smaller number of features. \n\n\nConclusion\n\nCutout was originally conceived as a targeted method for removing visual features with high activations in later layers of a CNN. Our motivation was to encourage the network to focus more on complimentary and less prominent features, in order to generalize to situations like occlusion. However, we discovered that the conceptually and computationally simpler approach of randomly masking square sections of the image performed equivalently in the experiments we conducted. Importantly, this simple regularizer proved to be complementary to existing forms of data augmentation and regularization. Applied to modern architectures, such as wide residual networks or shake-shake regularization models, it achieves state-of-the-art performance on the CIFAR-10, CIFAR-100, and SVHN vision benchmarks. So why hasn't it been reported or analyzed to date? One reason could be the fact that using a combination of corrupted and clean images appears to be important for its success. Future work will return to our original investigation of visual feature removal informed by activations. \n\nFigure 1 :\n1Cutout applied to images from the CIFAR-10 dataset.\n\nFigure 2 :\n2An early version of cutout applied to images from the CIFAR-10 dataset. This targeted approach often occludes part-level features of the image, such as heads, legs, or wheels.\n\nFigure 3 :\n3Cutout patch length with respect to validation accuracy with 95% confidence intervals (average of five runs). Tests run on CIFAR-10 and CIFAR-100 datasets using WRN-28-10 and standard data augmentation. Baseline indicates a model trained with no cutout.\n\n\nFigure 5 demonstrates similar observations for individual samples, where the effects of cutout are more pronounced.\n\nFigure 4 :Figure 5 :\n45Magnitude of feature activations, sorted by descending value, and averaged over all test samples. A standard ResNet18 is compared with a ResNet18 trained with cutout at three different depths. Magnitude of feature activations, sorted by descending value. Each row represents a different test sample. A standard ResNet18 is compared with a ResNet18 trained with cutout at three different depths.\nAcknowledgementsThe authors thank Daniel Jiwoong Im for feedback on the paper and for suggesting the analysis in \u00a7 4.4. The authors also thank NVIDIA for the donation of a Titan X GPU.\nDeep learners benefit more from out-of-distribution examples. Y Bengio, A Bergeron, N Boulanger-Lewandowski, T Breuel, Y Chherawala, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. the Fourteenth International Conference on Artificial Intelligence and StatisticsY. Bengio, A. Bergeron, N. Boulanger-Lewandowski, T. Breuel, Y. Chherawala, et al. Deep learners benefit more from out-of-distribution examples. In Proceedings of the Fourteenth International Conference on Artificial Intelli- gence and Statistics, pages 164-172, 2011.\n\nAn analysis of deep neural network models for practical applications. A Canziani, A Paszke, E Culurciello, IEEE International Symposium on Circuits & Systems. A. Canziani, A. Paszke, and E. Culurciello. An analysis of deep neural network models for practical applications. In IEEE International Symposium on Circuits & Systems, 2016.\n\nAn analysis of single-layer networks in unsupervised feature learning. A Coates, A Ng, H Lee, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. the Fourteenth International Conference on Artificial Intelligence and StatisticsA. Coates, A. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the Fourteenth International Conference on Artificial In- telligence and Statistics, pages 215-223, 2011.\n\nX Gastaldi, arXiv:1705.07485Shake-shake regularization. arXiv preprintX. Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485, 2017.\n\nIdentity mappings in deep residual networks. K He, X Zhang, S Ren, J Sun, European Conference on Computer Vision. SpringerK. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Com- puter Vision, pages 630-645. Springer, 2016.\n\nImproving neural networks by preventing co-adaptation of feature detectors. G E Hinton, N Srivastava, A Krizhevsky, I Sutskever, R R Salakhutdinov, arXiv:1207.0580arXiv preprintG. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by pre- venting co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. 2009.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097-1105, 2012.\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998.\n\nSmart augmentation-learning an optimal data augmentation strategy. J Lemley, S Bazrafkan, P Corcoran, IEEE AccessJ. Lemley, S. Bazrafkan, and P. Corcoran. Smart augmentation-learning an optimal data augmentation strat- egy. IEEE Access, 2017.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages 3431- 3440, 2015.\n\nReading digits in natural images with unsupervised feature learning. Y Netzer, T Wang, A Coates, A Bissacco, B Wu, A Y Ng, NIPS Workshop on Deep Learning and Unsupervised Feature Learning. 2011Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised fea- ture learning. In NIPS Workshop on Deep Learning and Un- supervised Feature Learning, volume 2011, page 5, 2011.\n\nAnalysis on the dropout effect in convolutional neural networks. S Park, N Kwak, Asian Conference on Computer Vision. SpringerS. Park and N. Kwak. Analysis on the dropout effect in con- volutional neural networks. In Asian Conference on Com- puter Vision, pages 189-204. Springer, 2016.\n\nContext encoders: Feature learning by inpainting. D Pathak, P Krahenbuhl, J Donahue, T Darrell, A A Efros, CVPR. D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. In CVPR, pages 2536-2544, 2016.\n\nDropout: A simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, The Journal of Machine Learning Research. 151N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929-1958, 2014.\n\nEfficient object localization using convolutional networks. J Tompson, R Goroshin, A Jain, Y Lecun, C Bregler, CVPR. J. Tompson, R. Goroshin, A. Jain, Y. LeCun, and C. Bregler. Efficient object localization using convolutional networks. In CVPR, pages 648-656, 2015.\n\nDeeppose: Human pose estimation via deep neural networks. A Toshev, C Szegedy, CVPR. A. Toshev and C. Szegedy. Deeppose: Human pose estima- tion via deep neural networks. In CVPR, pages 1653-1660, 2014.\n\nStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. P Vincent, H Larochelle, I Lajoie, Y Bengio, P.-A Manzagol, Journal of Machine Learning Research. 11P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.- A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local de- noising criterion. Journal of Machine Learning Research, 11(Dec):3371-3408, 2010.\n\nShow and tell: A neural image caption generator. O Vinyals, A Toshev, S Bengio, D Erhan, CVPR. O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, pages 3156-3164, 2015.\n\nTowards dropout training for convolutional neural networks. H Wu, X Gu, Neural Networks. 71H. Wu and X. Gu. Towards dropout training for convolu- tional neural networks. Neural Networks, 71:1-10, 2015.\n\nR Wu, S Yan, Y Shan, Q Dang, G Sun, arXiv:1501.02876Deep image: Scaling up image recognition. 7arXiv preprintR. Wu, S. Yan, Y. Shan, Q. Dang, and G. Sun. Deep image: Scaling up image recognition. arXiv preprint arXiv:1501.02876, 7(8), 2015.\n\nWide residual networks. S Zagoruyko, N Komodakis, British Machine Vision Conference (BMVC). S. Zagoruyko and N. Komodakis. Wide residual networks. British Machine Vision Conference (BMVC), 2016.\n", "annotations": {"author": "[{\"end\":112,\"start\":72},{\"end\":216,\"start\":113}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":81},{\"end\":128,\"start\":122}]", "author_first_name": "[{\"end\":80,\"start\":72},{\"end\":119,\"start\":113},{\"end\":121,\"start\":120}]", "author_affiliation": "[{\"end\":111,\"start\":90},{\"end\":151,\"start\":130},{\"end\":215,\"start\":153}]", "title": "[{\"end\":69,\"start\":1},{\"end\":285,\"start\":217}]", "venue": null, "abstract": "[{\"end\":1375,\"start\":287}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1600,\"start\":1597},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1628,\"start\":1624},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1651,\"start\":1647},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1683,\"start\":1679},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1784,\"start\":1781},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2073,\"start\":2070},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2872,\"start\":2869},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3093,\"start\":3090},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5676,\"start\":5673},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5906,\"start\":5903},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6384,\"start\":6381},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6734,\"start\":6730},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7085,\"start\":7081},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7369,\"start\":7366},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7653,\"start\":7650},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7656,\"start\":7653},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8327,\"start\":8323},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9177,\"start\":9173},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9393,\"start\":9389},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9946,\"start\":9942},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10367,\"start\":10363},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10393,\"start\":10389},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11263,\"start\":11259},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14126,\"start\":14122},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17016,\"start\":17013},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17636,\"start\":17633},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17957,\"start\":17954},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18021,\"start\":18017},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18241,\"start\":18237},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18566,\"start\":18563},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18862,\"start\":18859},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19917,\"start\":19914},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20967,\"start\":20963},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21330,\"start\":21326},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21823,\"start\":21819},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21883,\"start\":21879},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22966,\"start\":22963}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27303,\"start\":27239},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27492,\"start\":27304},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27759,\"start\":27493},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27877,\"start\":27760},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28296,\"start\":27878}]", "paragraph": "[{\"end\":2368,\"start\":1391},{\"end\":3224,\"start\":2370},{\"end\":4276,\"start\":3226},{\"end\":5199,\"start\":4278},{\"end\":5525,\"start\":5216},{\"end\":5898,\"start\":5558},{\"end\":6341,\"start\":5900},{\"end\":6637,\"start\":6343},{\"end\":6966,\"start\":6639},{\"end\":7299,\"start\":6968},{\"end\":7554,\"start\":7301},{\"end\":8169,\"start\":7599},{\"end\":8980,\"start\":8171},{\"end\":9338,\"start\":8982},{\"end\":9878,\"start\":9340},{\"end\":10293,\"start\":9880},{\"end\":11408,\"start\":10340},{\"end\":11641,\"start\":11410},{\"end\":12116,\"start\":11652},{\"end\":12662,\"start\":12118},{\"end\":13487,\"start\":12664},{\"end\":13949,\"start\":13502},{\"end\":14691,\"start\":13951},{\"end\":15034,\"start\":14693},{\"end\":15454,\"start\":15061},{\"end\":16510,\"start\":15456},{\"end\":16802,\"start\":16512},{\"end\":16959,\"start\":16818},{\"end\":17473,\"start\":16986},{\"end\":17838,\"start\":17475},{\"end\":18914,\"start\":17840},{\"end\":19918,\"start\":18916},{\"end\":20302,\"start\":19920},{\"end\":20909,\"start\":20304},{\"end\":21506,\"start\":20918},{\"end\":22371,\"start\":21508},{\"end\":22726,\"start\":22373},{\"end\":22933,\"start\":22728},{\"end\":23688,\"start\":22944},{\"end\":24065,\"start\":23690},{\"end\":24511,\"start\":24067},{\"end\":24972,\"start\":24513},{\"end\":25231,\"start\":25003},{\"end\":25608,\"start\":25278},{\"end\":26145,\"start\":25610},{\"end\":27238,\"start\":26160}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":25002,\"start\":24973}]", "table_ref": "[{\"end\":19624,\"start\":19617},{\"end\":20323,\"start\":20316},{\"end\":22932,\"start\":22925},{\"end\":24971,\"start\":24964},{\"end\":25082,\"start\":25075}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1389,\"start\":1377},{\"attributes\":{\"n\":\"2.\"},\"end\":5214,\"start\":5202},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5556,\"start\":5528},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7597,\"start\":7557},{\"attributes\":{\"n\":\"2.3.\"},\"end\":10338,\"start\":10296},{\"attributes\":{\"n\":\"3.\"},\"end\":11650,\"start\":11644},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13500,\"start\":13490},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15059,\"start\":15037},{\"attributes\":{\"n\":\"4.\"},\"end\":16816,\"start\":16805},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16984,\"start\":16962},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20916,\"start\":20912},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22942,\"start\":22936},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25276,\"start\":25234},{\"attributes\":{\"n\":\"5.\"},\"end\":26158,\"start\":26148},{\"end\":27250,\"start\":27240},{\"end\":27315,\"start\":27305},{\"end\":27504,\"start\":27494},{\"end\":27899,\"start\":27879}]", "table": null, "figure_caption": "[{\"end\":27303,\"start\":27252},{\"end\":27492,\"start\":27317},{\"end\":27759,\"start\":27506},{\"end\":27877,\"start\":27762},{\"end\":28296,\"start\":27902}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3600,\"start\":3592},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14648,\"start\":14640},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15212,\"start\":15204},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19424,\"start\":19415},{\"end\":25621,\"start\":25613}]", "bib_author_first_name": "[{\"end\":28545,\"start\":28544},{\"end\":28555,\"start\":28554},{\"end\":28567,\"start\":28566},{\"end\":28592,\"start\":28591},{\"end\":28602,\"start\":28601},{\"end\":29135,\"start\":29134},{\"end\":29147,\"start\":29146},{\"end\":29157,\"start\":29156},{\"end\":29471,\"start\":29470},{\"end\":29481,\"start\":29480},{\"end\":29487,\"start\":29486},{\"end\":29899,\"start\":29898},{\"end\":30095,\"start\":30094},{\"end\":30101,\"start\":30100},{\"end\":30110,\"start\":30109},{\"end\":30117,\"start\":30116},{\"end\":30407,\"start\":30406},{\"end\":30409,\"start\":30408},{\"end\":30419,\"start\":30418},{\"end\":30433,\"start\":30432},{\"end\":30447,\"start\":30446},{\"end\":30460,\"start\":30459},{\"end\":30462,\"start\":30461},{\"end\":30763,\"start\":30762},{\"end\":30777,\"start\":30776},{\"end\":30943,\"start\":30942},{\"end\":30957,\"start\":30956},{\"end\":30970,\"start\":30969},{\"end\":30972,\"start\":30971},{\"end\":31304,\"start\":31303},{\"end\":31313,\"start\":31312},{\"end\":31323,\"start\":31322},{\"end\":31333,\"start\":31332},{\"end\":31571,\"start\":31570},{\"end\":31581,\"start\":31580},{\"end\":31594,\"start\":31593},{\"end\":31804,\"start\":31803},{\"end\":31812,\"start\":31811},{\"end\":31825,\"start\":31824},{\"end\":32040,\"start\":32039},{\"end\":32050,\"start\":32049},{\"end\":32058,\"start\":32057},{\"end\":32068,\"start\":32067},{\"end\":32080,\"start\":32079},{\"end\":32086,\"start\":32085},{\"end\":32088,\"start\":32087},{\"end\":32464,\"start\":32463},{\"end\":32472,\"start\":32471},{\"end\":32737,\"start\":32736},{\"end\":32747,\"start\":32746},{\"end\":32761,\"start\":32760},{\"end\":32772,\"start\":32771},{\"end\":32783,\"start\":32782},{\"end\":32785,\"start\":32784},{\"end\":33017,\"start\":33016},{\"end\":33031,\"start\":33030},{\"end\":33041,\"start\":33040},{\"end\":33055,\"start\":33054},{\"end\":33068,\"start\":33067},{\"end\":33400,\"start\":33399},{\"end\":33411,\"start\":33410},{\"end\":33423,\"start\":33422},{\"end\":33431,\"start\":33430},{\"end\":33440,\"start\":33439},{\"end\":33666,\"start\":33665},{\"end\":33676,\"start\":33675},{\"end\":33928,\"start\":33927},{\"end\":33939,\"start\":33938},{\"end\":33953,\"start\":33952},{\"end\":33963,\"start\":33962},{\"end\":33976,\"start\":33972},{\"end\":34329,\"start\":34328},{\"end\":34340,\"start\":34339},{\"end\":34350,\"start\":34349},{\"end\":34360,\"start\":34359},{\"end\":34565,\"start\":34564},{\"end\":34571,\"start\":34570},{\"end\":34708,\"start\":34707},{\"end\":34714,\"start\":34713},{\"end\":34721,\"start\":34720},{\"end\":34729,\"start\":34728},{\"end\":34737,\"start\":34736},{\"end\":34974,\"start\":34973},{\"end\":34987,\"start\":34986}]", "bib_author_last_name": "[{\"end\":28552,\"start\":28546},{\"end\":28564,\"start\":28556},{\"end\":28589,\"start\":28568},{\"end\":28599,\"start\":28593},{\"end\":28613,\"start\":28603},{\"end\":29144,\"start\":29136},{\"end\":29154,\"start\":29148},{\"end\":29169,\"start\":29158},{\"end\":29478,\"start\":29472},{\"end\":29484,\"start\":29482},{\"end\":29491,\"start\":29488},{\"end\":29908,\"start\":29900},{\"end\":30098,\"start\":30096},{\"end\":30107,\"start\":30102},{\"end\":30114,\"start\":30111},{\"end\":30121,\"start\":30118},{\"end\":30416,\"start\":30410},{\"end\":30430,\"start\":30420},{\"end\":30444,\"start\":30434},{\"end\":30457,\"start\":30448},{\"end\":30476,\"start\":30463},{\"end\":30774,\"start\":30764},{\"end\":30784,\"start\":30778},{\"end\":30954,\"start\":30944},{\"end\":30967,\"start\":30958},{\"end\":30979,\"start\":30973},{\"end\":31310,\"start\":31305},{\"end\":31320,\"start\":31314},{\"end\":31330,\"start\":31324},{\"end\":31341,\"start\":31334},{\"end\":31578,\"start\":31572},{\"end\":31591,\"start\":31582},{\"end\":31603,\"start\":31595},{\"end\":31809,\"start\":31805},{\"end\":31822,\"start\":31813},{\"end\":31833,\"start\":31826},{\"end\":32047,\"start\":32041},{\"end\":32055,\"start\":32051},{\"end\":32065,\"start\":32059},{\"end\":32077,\"start\":32069},{\"end\":32083,\"start\":32081},{\"end\":32091,\"start\":32089},{\"end\":32469,\"start\":32465},{\"end\":32477,\"start\":32473},{\"end\":32744,\"start\":32738},{\"end\":32758,\"start\":32748},{\"end\":32769,\"start\":32762},{\"end\":32780,\"start\":32773},{\"end\":32791,\"start\":32786},{\"end\":33028,\"start\":33018},{\"end\":33038,\"start\":33032},{\"end\":33052,\"start\":33042},{\"end\":33065,\"start\":33056},{\"end\":33082,\"start\":33069},{\"end\":33408,\"start\":33401},{\"end\":33420,\"start\":33412},{\"end\":33428,\"start\":33424},{\"end\":33437,\"start\":33432},{\"end\":33448,\"start\":33441},{\"end\":33673,\"start\":33667},{\"end\":33684,\"start\":33677},{\"end\":33936,\"start\":33929},{\"end\":33950,\"start\":33940},{\"end\":33960,\"start\":33954},{\"end\":33970,\"start\":33964},{\"end\":33985,\"start\":33977},{\"end\":34337,\"start\":34330},{\"end\":34347,\"start\":34341},{\"end\":34357,\"start\":34351},{\"end\":34366,\"start\":34361},{\"end\":34568,\"start\":34566},{\"end\":34574,\"start\":34572},{\"end\":34711,\"start\":34709},{\"end\":34718,\"start\":34715},{\"end\":34726,\"start\":34722},{\"end\":34734,\"start\":34730},{\"end\":34741,\"start\":34738},{\"end\":34984,\"start\":34975},{\"end\":34997,\"start\":34988}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2462590},\"end\":29062,\"start\":28482},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7256695},\"end\":29397,\"start\":29064},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":308212},\"end\":29896,\"start\":29399},{\"attributes\":{\"doi\":\"arXiv:1705.07485\",\"id\":\"b3\"},\"end\":30047,\"start\":29898},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6447277},\"end\":30328,\"start\":30049},{\"attributes\":{\"doi\":\"arXiv:1207.0580\",\"id\":\"b5\"},\"end\":30705,\"start\":30330},{\"attributes\":{\"id\":\"b6\"},\"end\":30875,\"start\":30707},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195908774},\"end\":31220,\"start\":30877},{\"attributes\":{\"id\":\"b8\"},\"end\":31501,\"start\":31222},{\"attributes\":{\"id\":\"b9\"},\"end\":31745,\"start\":31503},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1629541},\"end\":31968,\"start\":31747},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16852518},\"end\":32396,\"start\":31970},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":43954515},\"end\":32684,\"start\":32398},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2202933},\"end\":32947,\"start\":32686},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6844431},\"end\":33337,\"start\":32949},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206592615},\"end\":33605,\"start\":33339},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206592152},\"end\":33809,\"start\":33607},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17804904},\"end\":34277,\"start\":33811},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1169492},\"end\":34502,\"start\":34279},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14459736},\"end\":34705,\"start\":34504},{\"attributes\":{\"doi\":\"arXiv:1501.02876\",\"id\":\"b20\"},\"end\":34947,\"start\":34707},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15276198},\"end\":35143,\"start\":34949}]", "bib_title": "[{\"end\":28542,\"start\":28482},{\"end\":29132,\"start\":29064},{\"end\":29468,\"start\":29399},{\"end\":30092,\"start\":30049},{\"end\":30940,\"start\":30877},{\"end\":31801,\"start\":31747},{\"end\":32037,\"start\":31970},{\"end\":32461,\"start\":32398},{\"end\":32734,\"start\":32686},{\"end\":33014,\"start\":32949},{\"end\":33397,\"start\":33339},{\"end\":33663,\"start\":33607},{\"end\":33925,\"start\":33811},{\"end\":34326,\"start\":34279},{\"end\":34562,\"start\":34504},{\"end\":34971,\"start\":34949}]", "bib_author": "[{\"end\":28554,\"start\":28544},{\"end\":28566,\"start\":28554},{\"end\":28591,\"start\":28566},{\"end\":28601,\"start\":28591},{\"end\":28615,\"start\":28601},{\"end\":29146,\"start\":29134},{\"end\":29156,\"start\":29146},{\"end\":29171,\"start\":29156},{\"end\":29480,\"start\":29470},{\"end\":29486,\"start\":29480},{\"end\":29493,\"start\":29486},{\"end\":29910,\"start\":29898},{\"end\":30100,\"start\":30094},{\"end\":30109,\"start\":30100},{\"end\":30116,\"start\":30109},{\"end\":30123,\"start\":30116},{\"end\":30418,\"start\":30406},{\"end\":30432,\"start\":30418},{\"end\":30446,\"start\":30432},{\"end\":30459,\"start\":30446},{\"end\":30478,\"start\":30459},{\"end\":30776,\"start\":30762},{\"end\":30786,\"start\":30776},{\"end\":30956,\"start\":30942},{\"end\":30969,\"start\":30956},{\"end\":30981,\"start\":30969},{\"end\":31312,\"start\":31303},{\"end\":31322,\"start\":31312},{\"end\":31332,\"start\":31322},{\"end\":31343,\"start\":31332},{\"end\":31580,\"start\":31570},{\"end\":31593,\"start\":31580},{\"end\":31605,\"start\":31593},{\"end\":31811,\"start\":31803},{\"end\":31824,\"start\":31811},{\"end\":31835,\"start\":31824},{\"end\":32049,\"start\":32039},{\"end\":32057,\"start\":32049},{\"end\":32067,\"start\":32057},{\"end\":32079,\"start\":32067},{\"end\":32085,\"start\":32079},{\"end\":32093,\"start\":32085},{\"end\":32471,\"start\":32463},{\"end\":32479,\"start\":32471},{\"end\":32746,\"start\":32736},{\"end\":32760,\"start\":32746},{\"end\":32771,\"start\":32760},{\"end\":32782,\"start\":32771},{\"end\":32793,\"start\":32782},{\"end\":33030,\"start\":33016},{\"end\":33040,\"start\":33030},{\"end\":33054,\"start\":33040},{\"end\":33067,\"start\":33054},{\"end\":33084,\"start\":33067},{\"end\":33410,\"start\":33399},{\"end\":33422,\"start\":33410},{\"end\":33430,\"start\":33422},{\"end\":33439,\"start\":33430},{\"end\":33450,\"start\":33439},{\"end\":33675,\"start\":33665},{\"end\":33686,\"start\":33675},{\"end\":33938,\"start\":33927},{\"end\":33952,\"start\":33938},{\"end\":33962,\"start\":33952},{\"end\":33972,\"start\":33962},{\"end\":33987,\"start\":33972},{\"end\":34339,\"start\":34328},{\"end\":34349,\"start\":34339},{\"end\":34359,\"start\":34349},{\"end\":34368,\"start\":34359},{\"end\":34570,\"start\":34564},{\"end\":34576,\"start\":34570},{\"end\":34713,\"start\":34707},{\"end\":34720,\"start\":34713},{\"end\":34728,\"start\":34720},{\"end\":34736,\"start\":34728},{\"end\":34743,\"start\":34736},{\"end\":34986,\"start\":34973},{\"end\":34999,\"start\":34986}]", "bib_venue": "[{\"end\":28711,\"start\":28615},{\"end\":29221,\"start\":29171},{\"end\":29589,\"start\":29493},{\"end\":29952,\"start\":29926},{\"end\":30161,\"start\":30123},{\"end\":30404,\"start\":30330},{\"end\":30760,\"start\":30707},{\"end\":31030,\"start\":30981},{\"end\":31301,\"start\":31222},{\"end\":31568,\"start\":31503},{\"end\":31839,\"start\":31835},{\"end\":32157,\"start\":32093},{\"end\":32514,\"start\":32479},{\"end\":32797,\"start\":32793},{\"end\":33124,\"start\":33084},{\"end\":33454,\"start\":33450},{\"end\":33690,\"start\":33686},{\"end\":34023,\"start\":33987},{\"end\":34372,\"start\":34368},{\"end\":34591,\"start\":34576},{\"end\":34799,\"start\":34759},{\"end\":35039,\"start\":34999},{\"end\":28794,\"start\":28713},{\"end\":29672,\"start\":29591}]"}}}, "year": 2023, "month": 12, "day": 17}