{"id": 247922740, "updated": "2023-10-05 15:23:18.333", "metadata": {"title": "Zero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition", "authors": "[{\"first\":\"Gerasimos\",\"last\":\"Chatzoudis\",\"middle\":[]},{\"first\":\"Manos\",\"last\":\"Plitsis\",\"middle\":[]},{\"first\":\"Spyridoula\",\"last\":\"Stamouli\",\"middle\":[]},{\"first\":\"Athanasia-Lida\",\"last\":\"Dimou\",\"middle\":[]},{\"first\":\"Athanasios\",\"last\":\"Katsamanis\",\"middle\":[]},{\"first\":\"Vassilis\",\"last\":\"Katsouros\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Aphasia is a common speech and language disorder, typically caused by a brain injury or a stroke, that affects millions of people worldwide. Detecting and assessing Aphasia in patients is a difficult, time-consuming process, and numerous attempts to automate it have been made, the most successful using machine learning models trained on aphasic speech data. Like in many medical applications, aphasic speech data is scarce and the problem is exacerbated in so-called\"low resource\"languages, which are, for this task, most languages excluding English. We attempt to leverage available data in English and achieve zero-shot aphasia detection in low-resource languages such as Greek and French, by using language-agnostic linguistic features. Current cross-lingual aphasia detection approaches rely on manually extracted transcripts. We propose an end-to-end pipeline using pre-trained Automatic Speech Recognition (ASR) models that share cross-lingual speech representations and are fine-tuned for our desired low-resource languages. To further boost our ASR model's performance, we also combine it with a language model. We show that our ASR-based end-to-end pipeline offers comparable results to previous setups using human-annotated transcripts.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.00448", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/ChatzoudisPSDKK22", "doi": "10.21437/interspeech.2022-10681"}}, "content": {"source": {"pdf_hash": "17d45b75c5841471f75c07914e1dc9a809b9f8c9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.00448v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "490207969bb68ae26cdb8663546d8fb5535b26f4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/17d45b75c5841471f75c07914e1dc9a809b9f8c9.txt", "contents": "\nZero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition\n\n\nGerasimos Chatzoudis gerasimos.chatzoudis@athenarc.gr \nInstitute for Language and Speech Processing\nAthena Research Center\nAthensGreece\n\nManos Plitsis manos.plitsis@athenarc.gr \nInstitute for Language and Speech Processing\nAthena Research Center\nAthensGreece\n\nNational and Kapodistrian University of Athens\nAthensGreece\n\nSpyridoula Stamouli \nInstitute for Language and Speech Processing\nAthena Research Center\nAthensGreece\n\nAthanasia-Lida Dimou ndimou@athenarc.gr \nInstitute for Language and Speech Processing\nAthena Research Center\nAthensGreece\n\nAthanasios Katsamanis \nInstitute for Language and Speech Processing\nAthena Research Center\nAthensGreece\n\nBehavioral Signal Technologies\nLos AngelesCAUSA\n\nVassilis Katsouros \nInstitute for Language and Speech Processing\nAthena Research Center\nAthensGreece\n\nZero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition\nIndex Terms: Pathological speech assessmentaphasiaGreekAphasiaBankDisordered speech recognitionmachine learn- ingzero-shot classification\nAphasia is a common speech and language disorder, typically caused by a brain injury or a stroke, that affects millions of people worldwide. Detecting and assessing Aphasia in patients is a difficult, time-consuming process, and numerous attempts to automate it have been made, the most successful using machine learning models trained on aphasic speech data. Like in many medical applications, aphasic speech data is scarce and the problem is exacerbated in so-called \"low resource\" languages, which are, for this task, most languages excluding English. We attempt to leverage available data in English and achieve zero-shot aphasia detection in low-resource languages such as Greek and French, by using language-agnostic linguistic features. Current cross-lingual aphasia detection approaches rely on manually extracted transcripts. We propose an end-toend pipeline using pre-trained Automatic Speech Recognition (ASR) models that share cross-lingual speech representations and are fine-tuned for our desired low-resource languages. To further boost our ASR model's performance, we also combine it with a language model. We show that our ASR-based end-toend pipeline offers comparable results to previous setups using human-annotated transcripts.\n\nIntroduction\n\nAphasia is a common speech and language disorder resulting from a brain injury that is usually caused by a stroke. According to the National Aphasia Association, at least 2,000,000 people in the USA and 250,000 in the UK are currently being affected by aphasia and this number is increasing at a rate of 180,000 people who acquire aphasia each year [1]. People with Aphasia (PWA) face communication difficulties making social interaction inefficient due to their impaired speech production and/or language understanding. To mitigate these communication inefficiencies, PWA need to attend long-term, intensive targeted therapies with expertly trained Speech-Language Pathologists (SLPs) [2]. Such conventional in-clinic therapies are timeconsuming, expensive, and, in some circumstances, unattainable for a lot of people due to the lack of local and long-term options.\n\nTowards this end, several attempts have been made to detect aphasia from spontaneous speech [3], and develop end-toend automatic assessment systems [4,5]. These studies indicate that it is possible to automatically detect and evaluate aphasia using acoustic and linguistic features from either manual Figure 1: Schematic diagram of the proposed end-to-end pipeline using language-agnostic linguistic features. Our classifier is trained using English data (left) and evaluated in our low-resource languages, i.e. French and Greek (right).\n\nor ASR transcriptions. However, these works only focus on languages with sufficient data, such as English. Data scarcity of aphasic speech in low-resource languages makes it difficult to democratize these Machine Learning approaches, and enable aphasia assessment for everyone. One solution to the problem of data scarcity is given by cross-lingual methods that leverage language-agnostic features from resource-rich languages, and transfer them to low-resource settings. Previous attempts have been made to achieve few-shot and zero-shot aphasia detection from English to both closely related (e.g., French) and more distant languages (e.g., Mandarin) [6,7]. However, these studies rely heavily on manual, human-annotated transcriptions by professional SLPs, a procedure that is costly and tedious, even for a small amount of data.\n\nFor that purpose, we investigate extending current crosslingual aphasia detection pipelines by incorporating Automatic Speech Recognition models for two closely related to English low-resource languages, i.e., French and Greek. Since training an ASR domain-specific model for these languages requires a considerable amount of collected speech data that is currently unavailable, we use large pre-trained ASR models that have demonstrated significant capabilities across several domains [8]. Specifically, we leverage pre-trained XLSR-53 architectures [9] based on the Wav2Vec2.0 audio representation [10], that have been proven to work for other low-resource languages on AphasiaBank, i.e., Spanish [11]. To augment ASR performance, we incorporate language models trained on out-of-domain data for each language in our architecture.\n\nOur main contribution is an end-to-end cross-lingual aphasia detection system for Greek and French, using languageagnostic linguistic features directly extracted from pre-trained ASR systems (as shown in Figure 1) 1 . This research activity is part of the ongoing project \"PLan-V: A Speech and Language Therapy Platform with Virtual Agent\", which aims to develop a technologically-assisted speech and language intervention platform for people with chronic neurogenic communication disorders, such as aphasia, integrating a system for the automated assessment of aphasia severity 2 .\n\n\nRelated Work\n\nThe first attempts at aphasic speech recognition involved short, prompted speech segments [12]. One of the first papers to combine aphasic speech recognition with aphasia severity assessment was [4]. In this work, the researchers use human-annotated and ASR-based transcriptions from the English AphasiaBank, to extract certain linguistic features and develop a model that ultimately predicts a speaker's AQ score. A similar ASR-based approach, using features extracted with deep neural networks, was adopted by [5] for the Cantonese AphasiaBank. Both teams focused more on the robustness of the features used for the predicting model in their respective language, rather than on speech recognition performance. Both teams have also attempted to use out-of-domain speaker data to improve ASR performance in various ways [13,14]. Recently, FacebookAI researchers have improved ASR performance in the English AphasiaBank by using neural models trained with vast amounts of data, and then adapted to various domains [8]. Semi-supervised learning was also used recently in an attempt to improve ASR performance in both English and Spanish [11].\n\nSeveral attempts have also been made to classify different aphasia types from manual transcripts. Researchers in [15] and [16] use similar features extracted from the AphasiaBank manual transcriptions. Interestingly, in [16] the two features that were found most significant were the number of words and number of closed terms in the Cinderella story, which are respective measures of productivity and grammaticality, qualities that we also find important when assessing aphasic status, while in [15] we see similar patterns in the most important features.\n\nOur work attempts to solve the problem of training an aphasia detection model from speech, for a low-resource language, for which there is a very limited amount of data available (or even no data at all). We are currently aware of two works that have attempted to solve this problem, both, however, using linguistic features extracted only from human-annotated textual transcriptions of available speech data [6,7]. Both use transfer learning techniques to apply zero-shot and few-shot aphasia classification. Balagopalan et al. in [6] claim that it may be possible to transfer features from languages that are not distant and this is also validated in [7], where a wider set of linguistic features is also used. Last, in [17], the authors used crosslingual linguistic features to assess morphosyntactic production in agrammatic aphasia.\n\n\nDatasets\n\nOur dataset consists of speech samples from the AphasiaBank database for English and French, as well as a Greek corpus collected by various researchers. The amount of data available for each language can be seen in Table 1.\n\nAphasiaBank is a shared database of multimedia interactions with the purpose of studying aphasia communication [18,19]. It consists of a collection of transcribed audio and/or video recordings of interviews between clinicians and aphasic or healthy subjects. Depending on the discourse language and the contributing research group, various speech elicitation protocols were followed, with the majority of protocols being variants of the AphasiaBank data collection protocol. In particular, we used speech samples from the English and French AphasiaBank subgroup, and specifically, only data that were collected according to the AphasiaBank protocol. Aphasia-Bank's transcriptions are produced using the CHAT transcription format [20] that includes, besides textual transcriptions, a variety of symbols denoting non-speech sounds such as laughs and coughs, as well as a system for transcribing paraphasias, or mispronounced words, in the International Phonetic Alphabet.\n\nOur Greek data come from two main sources: a) the GREECAD corpus [21], a dataset of spoken narratives collected from 22 Greek-speaking PWA and 10 unimpaired controls using a protocol of 4 narrative tasks and b) the PLan-V project corpus [22], a dataset of spoken narratives collected from 10 unimpaired controls using an augmented narrative discourse elicitation protocol, which includes 4 tasks from the Aphasia-Bank protocol and 3 tasks from the GREECAD protocol. In both datasets, spoken language samples have been manually transcribed in an orthographic format and segmented into timealigned utterances, following the CHAT guidelines for utterance segmentation. We chose not to incorporate the Greek datasets of the AphasiaBank repository, mainly because of their conversational form, in contrast to the GREECAD and PLan-V data, which represent different types of monologic narrative discourse, with minimal interventions from the investigator. 4. Methods\n\n\nPreprocessing\n\nAll available transcripts in each language contain various annotations, such as target phrases and pauses. These annotations, although useful, are unavailable in our automated assessment system, so we remove them from all transcripts using PyLan-gAcq [23]. We only retain the cleaned transcripts, with their corresponding duration information. We refer to these as oracle transcripts. For each oracle transcript, we downsample its corresponding audio to 16 kHz. Following data cleaning, we pass both oracle and ASR transcripts through a part-of-speech tagging, tokenization and lemmatization module. Tokens that correspond to out-of-vocabulary words are not omitted from the process. For English and French, we employ spaCy's transformer-based models 3 , whereas for Greek, we use the Neural NLP toolkit for Greek [24].\n\n\nAutomatic Speech Recognition Models\n\nOur suggested ASR model uses the XLSR-53 architecture [9] which is based on the Wav2Vec2.0 audio representation [10] and was trained with 56,000 hours of speech data in 53 languages. Because it is based on a language-agnostic representation of speech, this architecture is particularly well-suited for multi-language tasks, where it can be fine-tuned for each language using very few data instances. Specifically, we used finetuned XLSR-53 models for each language, that were pre-trained on the Common Voice dataset [25].\n\nHugging Face provides open-source versions of these models in English [26], French [27], and Greek [28].\n\nTo further optimize the performance of the fine-tuned XLSR-53 models, we combined them with a language model for each language and employed a beam search decoding strategy. Each experiment used a beam width of 10. To train the language model, we first collected textual data from Wikipedia [29] and then trained Kneser-Ney smoothed n-gram language models using the KENLM toolkit [30]. Specifically, we trained a 3-gram model for all languages. The language models are integrated into the XLSR-53 architecture using the pyctcdecode library 4 , a fast and feature-rich CTC beam search decoder for speech recognition in Python. We evaluate our models on both Word-Error-Rate (WER) and Character-Error-Rate (CER) metrics [31].\n\n\nLanguage-agnostic Feature Extraction\n\nPrevious works in automatic assessment of aphasia indicate that using linguistic features is considered standard practice (see Section 2). Given the condition's influence on the brain's language centers it is necessary for any reliable system to be able to access semantic and grammatical information. Over 500 linguistic indicators have been applied to assess spoken language ability and the efficacy of PWA interventions [22].\n\nFollowing recent works in the analysis of discourse production [32,33,22], we focus on linguistic features that evaluate 6 language abilities, i.e., linguistic productivity, content richness, fluency, syntactic complexity, lexical diversity, and gross output. A complete list of the extracted features is presented in Table 2. These make up a vector of size 24 per speaker.\n\nWe chose to extract all features at the speaker level to minimize variability due to utterance length and number of stories between speakers. Our end-to-end approach can help minimizing the impact of this type of variability that is very common across datasets due to the different transcription guidelines that may have been followed in each case.\n\n\nCross-lingual aphasia detection\n\nWe initially examine the feasibility of aphasia detection in English prior to developing our transfer learning pipeline across languages. We compare multiple classification models in this monolingual scenario, including SVM [34], XGBoost [35], Decision Tree and Random Forest classifiers using Scikit-learn 3 https://spacy.io/ 4 https://github.com/kensho-technologies/pyctcdecode  [36]. We evaluate our models' accuracy using a Leave-onesubject-out (LOSO) cross-validation approach.\n\nWe perform zero-shot cross-lingual binary classification of aphasic vs control subjects, from English to French and Greek respectively. We first test the performance of our models using oracle transcriptions in all languages. Because we introduce a generic end-to-end pipeline, we normalize the target languages' input features using Scikit-learn's Standard scaler [36] that was fitted in the source language, i.e., English.\n\nThen, we investigate the integration of ASR transcripts under two settings. In the first setting, we train our classifier leveraging the English oracle transcripts, and we evaluate it using the ASR transcripts from the low-resource, target languages. In the second setting, we replace the English oracle training data with ASR transcripts in order to develop a fully end-to-end crosslingual aphasia detection system. With the last experiment we want to explore whether such a system can be developed without any manually transcribed data. This would allow us to benefit from a much greater amount of data for training and potentially improve overall detection performance.\n\n\nResults and Discussion\n\n\nAutomatic Speech Recognition Performance\n\nWe present the performance of the ASR models for each language in Table 3, as measured by Word Error Rate (WER). As expected, ASR models perform worse in aphasic speech than in healthy speech. We observe that the English XLSR-53 model outperforms the other two, while the Greek model outperforms the French one in all categories. Adding a language model to the ASR system significantly drops WER in English and French, but exhibits inconsistent results in Greek. \n\n\nMonolingual Aphasia Detection -English Only\n\nWe compare the performance of various classification models trained and evaluated solely on English control and aphasic speakers, and we present the results in Table 4. Three different scenarios are evaluated, namely using oracle transcripts, ASR results, and ASR with LM results for both training and testing. As shown, the SVM classifier outperforms the XGBoost, Decision Tree and Random Forest classifiers in every setting. We conclude that, given our feature set, we can achieve satisfactory results for aphasia detection in English, even when using ASR transcripts.  \n\n\nCross-Lingual Aphasia Detection using Oracle Transcripts during Training\n\nIn Table 5, we show the results for the cross-lingual experiment using oracle transcripts for all languages. Since both the French and Greek datasets are balanced we only report accuracy. XG-Boost classifier outperforms other classifiers in all settings and thus we chose to exclusively use it in the next experiments. We achieve almost perfect results in both Greek and French using oracle transcripts. As shown in Table 6, when using ASR transcripts for the test set, our classifier's accuracy drops by almost 19 % and 14 % in French and Greek respectively. The addition of the language model, while increasing accuracy in French, actually decreases performance in Greek.  Table 7, we demonstrate that training our classifier on English ASR-derived transcriptions produces results comparable to those obtained using oracle transcriptions (Section 5.3), supporting our hypothesis that an end-to-end pipeline is achievable in the cross-lingual setting. Specifically, we achieve up to 88.88 % and 76.19 % accuracy in French and Greek respectively, depending on whether we use a language model for these languages. The addition of the language model for French and Greek worsened accuracy results in almost every case. Based on our results so far, we are unable to reach a resolution regarding the integration of a language model. This could be because the language models were trained on a generic data corpus (Wikipedia) rather than a dataset-specific domain corpus. \n\n\nConclusion and Future Work\n\nIn this work, we established an end-to-end pipeline for crosslingual aphasia detection by using language-agnostic linguistic features from automatic speech recognition transcripts. We achieved up to 81 % and 83 % detection accuracy in French and Greek respectively, when training our model using humanannotated speech transcripts, and up to 88 % and 76 % for the end-to-end case when only using ASR output. Our results indicate that it is possible to apply transfer learning between two closely related languages even without oracle transcripts in both source and target languages. In the future, we aim to extend our research by developing a similar procedure for aphasia severity assessment and aphasia type classification, as well as study the effects of adapting the ASR and language models to domainspecific data.\n\nTable 1 :\n1Dataset StatisticsLanguage \nNumber of Speakers \nTotal Control Aphasia \n\nEnglish \n705 \n245 \n460 \nFrench \n27 \n14 \n13 \nGreek \n42 \n20 \n22 \n\n\n\nTable 2 :\n2Language-agnostic linguistic features across 6 language ability categories.Language ability \nFeature \n\nLinguistic Productivity Mean Length of Utterance \nContent Richness \nVerb/Word Ratio \nNoun/Word Ratio \nAdjective/Word Ratio \nAdverb/Word Ratio \nPreposition/Word Ratio \nPropositional density \nFluency \nWords per Minute \nSyntactic Complexity \nVerbs per Utterance \nNoun Verb Ratio \nOpen-closed class words \nConjunction/Word Ratio \nMean Clauses per utterance \nMean dependent clauses \nMean independent clauses \nDependent to all clauses ratio \nMean Tree height \nMax Tree depth \nNumber of independent clauses \nNumber of dependent clauses \nLexical Diversity \nLemma/Token Ratio \nWords in Vocabulary per Words \nUnique words in vocabulary per Words \nGross output \nNumber of words \n\n\n\nTable 3 :\n3Pre-trained XLSR-53 ASR models performance in English, French and Greek (with and without a Language Model)Language \nWER (%) \nTotal Control Aphasia \n\nenglish-no-lm \n52.57 41.45 \n62.74 \nenglish-with-lm 47.14 37.46 \n55.98 \nfrench-no-lm \n71.97 66.67 \n85.71 \nfrench-with-lm \n66.88 62.78 \n77.52 \ngreek-no-lm \n65.66 61.98 \n72.24 \ngreek-with-lm \n67.34 65.11 \n71.33 \n\n\n\nTable 4 :\n4Comparison of four classifiers in terms of accuracy (%) in the English-only experiment, using oracle and ASR transcriptions (with and without a Language Model).Accuracy(%) \nModel \nOracle ASR ASR with LM \n\nSVM \n97.45 \n93.90 \n93.76 \nDecision Tree \n92.06 \n87.38 \n89.22 \nXGBoost \n96.31 \n93.62 \n93.48 \nRandom Forest \n96.03 \n92.34 \n92.20 \n\n\n\nTable 5 :\n5Comparison of four classifiers in terms of accuracy (%) in the cross-lingual experiment using oracle transcriptions for all languages.Accuracy(%) \nExperiment \nSVM Decision Tree XGBoost Random Forest \n\nEnglish \u2192 French 48.15 \n81.48 \n100 \n100 \nEnglish \u2192 Greek \n52.38 \n59.52 \n97.62 \n85.71 \n\n\n\nTable 6 :\n6Zero-shot cross-lingual aphasia detection results from English to French and Greek. The classifier is trained on oracle transcripts and tested on ASR output. Oracle \u2192 ASR with LM 66.67 5.4. Cross-Lingual Aphasia Detection using ASR Transcripts during TrainingExperiment \nTranscriptions \nAccuracy (%) \n\nEnglish \u2192 French Oracle \u2192 Oracle \n100 \nOracle \u2192 ASR \n77.78 \nOracle \u2192 ASR with LM 81.48 \nEnglish \u2192 Greek \nOracle \u2192 Oracle \n97.62 \nOracle \u2192 ASR \n83.33 \nIn \n\nTable 7 :\n7Zero-shot cross-lingual aphasia detection results from English to French and Greek. For each language pair, we evaluate the performance of our classifier in terms of accuracy using ASR-derived transcriptions for training too. ASR with LM\u2192 ASR 76.19 ASR with LM\u2192 ASR with LM 76.19Experiment \nTranscriptions \nAccuracy (%) \n\nEnglish \u2192 French ASR\u2192 Oracle \n81.48 \nASR \u2192 ASR \n77.78 \nASR \u2192 ASR with LM \n62.96 \nASR with LM\u2192 Oracle \n88.88 \nASR with LM\u2192 ASR \n88.88 \nASR with LM\u2192 ASR with LM \n77.77 \nEnglish \u2192 Greek \nASR\u2192 Oracle \n83.33 \nASR \u2192 ASR \n73.81 \nASR \u2192 ASR with LM \n66.67 \nASR with LM\u2192 Oracle \n92.86 \n\nThe source code for reproducing all our experiments can be found at https://gitlab.com/ilsp-spmd-all/public/crosslingual-aphasiadetection2 This research has been co-financed by the European Regional Development Fund of the European Union and Greek national funds through the Operational Program Competitiveness, Entrepreneurship and Innovation, under the call RESEARCH-CREATE-INNOVATE (Project title: PLan-V: A Speech and Language Therapy Platform with Virtual Agent, Project code: T2EDK-02159).\n\nAphasia. N A Association, N. A. Association, \"Aphasia.\" [Online]. Available: https: //www.aphasia.org/\n\nSpeech and language therapy for aphasia following stroke. M C Brady, H Kelly, J Godwin, P Enderby, P Campbell, Cochrane database of systematic reviews. 6M. C. Brady, H. Kelly, J. Godwin, P. Enderby, and P. Camp- bell, \"Speech and language therapy for aphasia following stroke,\" Cochrane database of systematic reviews, no. 6, 2016.\n\nAutomatic Speech Assessment for Aphasic Patients Based on Syllable-Level Embedding and Supra-Segmental Duration Features. Y Qin, T Lee, A P H Kong, ICASSP. Y. Qin, T. Lee, and A. P. H. Kong, \"Automatic Speech Assess- ment for Aphasic Patients Based on Syllable-Level Embedding and Supra-Segmental Duration Features,\" in ICASSP, Apr. 2018, pp. 5994-5998.\n\nAutomatic quantitative analysis of spontaneous aphasic speech. D Le, K Licata, E. Mower Provost, Speech Communication. 100D. Le, K. Licata, and E. Mower Provost, \"Automatic quantitative analysis of spontaneous aphasic speech,\" Speech Communication, vol. 100, pp. 1-12, Jun. 2018.\n\nAutomatic Assessment of Speech Impairment in Cantonese-Speaking People with Aphasia. Y Qin, T Lee, A P H Kong, IEEE Journal of Selected Topics in Signal Processing. 142Y. Qin, T. Lee, and A. P. H. Kong, \"Automatic Assessment of Speech Impairment in Cantonese-Speaking People with Aphasia,\" IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331-345, Feb. 2020.\n\nCross-Language Aphasia Detection using Optimal Transport Domain Adaptation. A Balagopalan, J Novikova, M B A Mcdermott, B Nestor, T Naumann, M Ghassemi, Proceedings of the Machine Learning for Health NeurIPS Workshop. the Machine Learning for Health NeurIPS WorkshopPMLR116A. Balagopalan, J. Novikova, M. B. A. Mcdermott, B. Nestor, T. Naumann, and M. Ghassemi, \"Cross-Language Aphasia De- tection using Optimal Transport Domain Adaptation,\" in Proceed- ings of the Machine Learning for Health NeurIPS Workshop, vol. 116. PMLR, 2020, pp. 202-219.\n\nBlabla: Linguistic feature extraction for clinical analysis in multiple languages. A Shivkumar, J Weston, R Lenain, E Fristed, Proc. Interspeech 2020. Interspeech 2020A. Shivkumar, J. Weston, R. Lenain, and E. Fristed, \"Blabla: Linguistic feature extraction for clinical analysis in multiple lan- guages,\" Proc. Interspeech 2020, pp. 2542-2546, 2020.\n\nA Xiao, W Zheng, G Keren, D Le, F Zhang, C Fuegen, O Kalinli, Y Saraf, A Mohamed, arXiv:2111.05948Scaling ASR Improves Zero and Few Shot Learning. cs, eessA. Xiao, W. Zheng, G. Keren, D. Le, F. Zhang, C. Fuegen, O. Kalinli, Y. Saraf, and A. Mohamed, \"Scaling ASR Improves Zero and Few Shot Learning,\" arXiv:2111.05948 [cs, eess], Nov. 2021.\n\nUnsupervised cross-lingual representation learning for speech recognition. A Conneau, A Baevski, R Collobert, A Mohamed, M Auli, CoRR. A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, \"Unsupervised cross-lingual representation learning for speech recognition,\" CoRR, vol. abs/2006.13979, 2020.\n\nwav2vec 2.0: A framework for self-supervised learning of speech representations. A Baevski, Y Zhou, A Mohamed, M Auli, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33460A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \"wav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\" in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 12 449-12 460.\n\nImproving Aphasic Speech Recognition by Using Novel Semi-Supervised Learning Methods on AphasiaBank for English and Spanish. I G Torre, M Romero, A \u00c1lvarez, Applied Sciences. 11198872I. G. Torre, M. Romero, and A.\u00c1lvarez, \"Improving Aphasic Speech Recognition by Using Novel Semi-Supervised Learning Methods on AphasiaBank for English and Spanish,\" Applied Sci- ences, vol. 11, no. 19, p. 8872, Jan. 2021.\n\nAutomatic word naming recognition for an on-line aphasia treatment system. A Abad, A Pompili, A Costa, I Trancoso, J Fonseca, G Leal, L Farrajota, I P Martins, Computer Speech & Language. 276A. Abad, A. Pompili, A. Costa, I. Trancoso, J. Fonseca, G. Leal, L. Farrajota, and I. P. Martins, \"Automatic word naming recogni- tion for an on-line aphasia treatment system,\" Computer Speech & Language, vol. 27, no. 6, pp. 1235-1248, Sep. 2013.\n\nImproving automatic recognition of aphasic speech with aphasiabank. D Le, E M Provost, Interspeech. D. Le and E. M. Provost, \"Improving automatic recognition of aphasic speech with aphasiabank.\" in Interspeech, 2016, pp. 2681-2685.\n\nAutomatic Speech Assessment for People with Aphasia Using TDNN-BLSTM with Multi-Task Learning. Y Qin, T Lee, S Feng, A P H Kong, ISCA. Y. Qin, T. Lee, S. Feng, and A. P. H. Kong, \"Automatic Speech Assessment for People with Aphasia Using TDNN-BLSTM with Multi-Task Learning,\" in Interspeech 2018. ISCA, Sep. 2018, pp. 3418-3422.\n\nAutomated classification of primary progressive aphasia subtypes from narrative speech transcripts. K C Fraser, J A Meltzer, N L Graham, C Leonard, G Hirst, S E Black, E Rochon, Cortex. 55K. C. Fraser, J. A. Meltzer, N. L. Graham, C. Leonard, G. Hirst, S. E. Black, and E. Rochon, \"Automated classification of primary progressive aphasia subtypes from narrative speech transcripts,\" Cortex, vol. 55, pp. 43-60, Jun. 2014.\n\nEnhancing the classification of aphasia: a statistical analysis using connected speech. D Fromm, J Greenhouse, M Pudil, Y Shi, B Macwhinney, Aphasiology. 00D. Fromm, J. Greenhouse, M. Pudil, Y. Shi, and B. MacWhinney, \"Enhancing the classification of aphasia: a statistical analysis us- ing connected speech,\" Aphasiology, vol. 0, no. 0, pp. 1-28, Sep. 2021.\n\nAutomatic Subtyping of Individuals with Primary Progressive Aphasia. C Themistocleous, B Ficek, K Webster, D.-B Den Ouden, A E Hillis, K Tsapkini, Journal of Alzheimer's disease : JAD. 793IOS PressC. Themistocleous, B. Ficek, K. Webster, D.-B. den Ouden, A. E. Hillis, and K. Tsapkini, \"Automatic Subtyping of Individuals with Primary Progressive Aphasia,\" Journal of Alzheimer's disease : JAD, vol. 79, no. 3, pp. 1185-1194, 2021, publisher: IOS Press.\n\nAphasi-aBank: Methods for Studying Discourse. B Macwhinney, D Fromm, M Forbes, A Holland, Aphasiology. 25B. Macwhinney, D. Fromm, M. Forbes, and A. Holland, \"Aphasi- aBank: Methods for Studying Discourse,\" Aphasiology, vol. 25, pp. 1286-1307, Nov. 2011.\n\nAphasiaBank: A resource for clinicians. M M Forbes, D Fromm, B Macwhinney, Seminars in speech and language. 333M. M. Forbes, D. Fromm, and B. MacWhinney, \"AphasiaBank: A resource for clinicians,\" Seminars in speech and language, vol. 33, no. 3, pp. 217-222, Aug. 2012.\n\nThe childes project: Tools for analyzing talk: Volume i: Transcription format and programs, volume ii: The database. B Macwhinney, B. MacWhinney, \"The childes project: Tools for analyzing talk: Volume i: Transcription format and programs, volume ii: The database,\" 2000.\n\nA greek corpus of aphasic discourse: collection, transcription, and annotation specifications. S Varlokosta, S Stamouli, A Karasimos, G Markopoulos, M Kakavoulia, M Nerantzini, A Pantoula, V Fyndanis, A Economou, A Protopapas, Proceedings of RaPID-2016 Workshop. RaPID-2016 WorkshopS. Varlokosta, S. Stamouli, A. Karasimos, G. Markopoulos, M. Kakavoulia, M. Nerantzini, A. Pantoula, V. Fyndanis, A. Economou, and A. Protopapas, \"A greek corpus of aphasic dis- course: collection, transcription, and annotation specifications,\" in Proceedings of RaPID-2016 Workshop, May 2016, no. 128, 2016.\n\nA web-based application for eliciting narrative discourse from greek-speaking people with aphasia.\" (submitted for publication. S Stamouli, M Nerantzini, I Papakyritsis, A Katsamanis, G Chatzoudis, A.-L Dimou, M Plitsis, V Katsouros, S Varlokosta, A Terzi, Frontiers in Communication, Research Topic: Remote Online Language Assessment: Eliciting Discourse from Children and Adults. S. Stamouli, M. Nerantzini, I. Papakyritsis, A. Katsamanis, G. Chatzoudis, A.-L. Dimou, M. Plitsis, V. Katsouros, S. Var- lokosta, and A. Terzi, \"A web-based application for eliciting nar- rative discourse from greek-speaking people with aphasia.\" (sub- mitted for publication), Frontiers in Communication, Research Topic: Remote Online Language Assessment: Eliciting Discourse from Children and Adults, 2022.\n\nWorking with chat transcripts in python. J L Lee, R Burkholder, G B Flinn, E R Coppess, TR-2016-02Department of Computer Science. University of ChicagoTech. Rep.J. L. Lee, R. Burkholder, G. B. Flinn, and E. R. Coppess, \"Work- ing with chat transcripts in python,\" Department of Computer Sci- ence, University of Chicago, Tech. Rep. TR-2016-02, 2016.\n\nA neural nlp toolkit for greek. P Prokopidis, S Piperidis, 11th Hellenic Conference on Artificial Intelligence, ser. SETN 2020. New York, NY, USAAssociation for Computing MachineryP. Prokopidis and S. Piperidis, \"A neural nlp toolkit for greek,\" in 11th Hellenic Conference on Artificial Intelligence, ser. SETN 2020. New York, NY, USA: Association for Computing Machin- ery, 2020, p. 125-128.\n\nCommon voice: A massively-multilingual speech corpus. R Ardila, M Branson, K Davis, M Henretty, M Kohler, J Meyer, R Morais, L Saunders, F M Tyers, G Weber, Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020. the 12th Conference on Language Resources and Evaluation (LREC 2020R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, \"Common voice: A massively-multilingual speech corpus,\" in Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), 2020, pp. 4211-4215.\n\nXlsr wav2vec2 english by jonatas grosman. J Grosman, J. Grosman, \"Xlsr wav2vec2 english by jonatas grosman,\" https:// huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english, 2021.\n\nXlsr wav2vec2 french by jonatas grosman. --, \"Xlsr wav2vec2 french by jonatas grosman,\" https:// huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-french, 2021.\n\nXlsr wav2vec2 greek by jonatas grosman. --, \"Xlsr wav2vec2 greek by jonatas grosman,\" https: //huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-greek, 2021.\n\nWikimedia downloads. W Foundation, W. Foundation. Wikimedia downloads. [Online]. Available: https://dumps.wikimedia.org\n\nKenlm: Faster and smaller language model queries. K Heafield, Proceedings of the Sixth Workshop on Statistical Machine Translation, ser. WMT '11. USA: Association for Computational Linguistics. the Sixth Workshop on Statistical Machine Translation, ser. WMT '11. USA: Association for Computational LinguisticsK. Heafield, \"Kenlm: Faster and smaller language model queries,\" in Proceedings of the Sixth Workshop on Statistical Machine Translation, ser. WMT '11. USA: Association for Computa- tional Linguistics, 2011, p. 187-197.\n\nFrom wer and ril to mer and wil: improved evaluation measures for connected speech recognition. A Morris, V Maier, P Green, 01A. Morris, V. Maier, and P. Green, \"From wer and ril to mer and wil: improved evaluation measures for connected speech recogni- tion.\" 01 2004.\n\nA comparison of three discourse elicitation methods in aphasia and age-matched adults: Implications for language assessment and outcome. B C Stark, American Journal of Speech-Language Pathology. 283B. C. Stark, \"A comparison of three discourse elicitation methods in aphasia and age-matched adults: Implications for language as- sessment and outcome,\" American Journal of Speech-Language Pathology, vol. 28, no. 3, pp. 1067-1083, 2019.\n\nEnhancing the classification of aphasia: a statistical analysis using connected speech. D Fromm, J Greenhouse, M Pudil, Y Shi, B Macwhinney, Aphasiology. D. Fromm, J. Greenhouse, M. Pudil, Y. Shi, and B. MacWhinney, \"Enhancing the classification of aphasia: a statistical analysis us- ing connected speech,\" Aphasiology, pp. 1-28, 2021.\n\nSupport-vector networks. C Cortes, V Vapnik, Machine learning. 203C. Cortes and V. Vapnik, \"Support-vector networks,\" Machine learning, vol. 20, no. 3, pp. 273-297, 1995.\n\nXGBoost: A scalable tree boosting system. T Chen, C Guestrin, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '16. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '16New York, NY, USAACMT. Chen and C. Guestrin, \"XGBoost: A scalable tree boosting system,\" in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '16. New York, NY, USA: ACM, 2016, pp. 785-794.\n\nScikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, \"Scikit-learn: Machine learning in Python,\" Journal of Machine Learning Research, vol. 12, pp. 2825-2830, 2011.\n", "annotations": {"author": "[{\"end\":216,\"start\":80},{\"end\":400,\"start\":217},{\"end\":503,\"start\":401},{\"end\":626,\"start\":504},{\"end\":780,\"start\":627},{\"end\":882,\"start\":781}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":90},{\"end\":230,\"start\":223},{\"end\":420,\"start\":412},{\"end\":524,\"start\":519},{\"end\":648,\"start\":638},{\"end\":799,\"start\":790}]", "author_first_name": "[{\"end\":89,\"start\":80},{\"end\":222,\"start\":217},{\"end\":411,\"start\":401},{\"end\":518,\"start\":504},{\"end\":637,\"start\":627},{\"end\":789,\"start\":781}]", "author_affiliation": "[{\"end\":215,\"start\":135},{\"end\":338,\"start\":258},{\"end\":399,\"start\":340},{\"end\":502,\"start\":422},{\"end\":625,\"start\":545},{\"end\":730,\"start\":650},{\"end\":779,\"start\":732},{\"end\":881,\"start\":801}]", "title": "[{\"end\":77,\"start\":1},{\"end\":959,\"start\":883}]", "venue": null, "abstract": "[{\"end\":2346,\"start\":1098}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2714,\"start\":2711},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3051,\"start\":3048},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3326,\"start\":3323},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3382,\"start\":3379},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3384,\"start\":3382},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4426,\"start\":4423},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4428,\"start\":4426},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5093,\"start\":5090},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5158,\"start\":5155},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5208,\"start\":5204},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5307,\"start\":5303},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6131,\"start\":6127},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6235,\"start\":6232},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6552,\"start\":6549},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6861,\"start\":6857},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6864,\"start\":6861},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7053,\"start\":7050},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7176,\"start\":7172},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7296,\"start\":7292},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7305,\"start\":7301},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7403,\"start\":7399},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7679,\"start\":7675},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8149,\"start\":8146},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8151,\"start\":8149},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8272,\"start\":8269},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8393,\"start\":8390},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8463,\"start\":8459},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8927,\"start\":8923},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8930,\"start\":8927},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9545,\"start\":9541},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9852,\"start\":9848},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9902,\"start\":9900},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10024,\"start\":10020},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11015,\"start\":11011},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11512,\"start\":11511},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11578,\"start\":11574},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11676,\"start\":11673},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11735,\"start\":11731},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12139,\"start\":12135},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12216,\"start\":12212},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12229,\"start\":12225},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12245,\"start\":12241},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12542,\"start\":12538},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12631,\"start\":12627},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12969,\"start\":12965},{\"end\":13148,\"start\":13138},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13438,\"start\":13434},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13508,\"start\":13504},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13511,\"start\":13508},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13514,\"start\":13511},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14428,\"start\":14424},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14442,\"start\":14438},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14585,\"start\":14581},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15053,\"start\":15049},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22501,\"start\":22500}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":19477,\"start\":19329},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20262,\"start\":19478},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":20635,\"start\":20263},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":20982,\"start\":20636},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":21283,\"start\":20983},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":21751,\"start\":21284},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":22362,\"start\":21752}]", "paragraph": "[{\"end\":3229,\"start\":2362},{\"end\":3768,\"start\":3231},{\"end\":4602,\"start\":3770},{\"end\":5436,\"start\":4604},{\"end\":6020,\"start\":5438},{\"end\":7177,\"start\":6037},{\"end\":7735,\"start\":7179},{\"end\":8574,\"start\":7737},{\"end\":8810,\"start\":8587},{\"end\":9781,\"start\":8812},{\"end\":10742,\"start\":9783},{\"end\":11579,\"start\":10760},{\"end\":12140,\"start\":11619},{\"end\":12246,\"start\":12142},{\"end\":12970,\"start\":12248},{\"end\":13439,\"start\":13011},{\"end\":13814,\"start\":13441},{\"end\":14164,\"start\":13816},{\"end\":14682,\"start\":14200},{\"end\":15108,\"start\":14684},{\"end\":15782,\"start\":15110},{\"end\":16315,\"start\":15852},{\"end\":16935,\"start\":16363},{\"end\":18479,\"start\":17012},{\"end\":19328,\"start\":18510}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":8809,\"start\":8802},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13766,\"start\":13759},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15925,\"start\":15918},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16530,\"start\":16523},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17022,\"start\":17015},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17435,\"start\":17428},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":17694,\"start\":17687}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2360,\"start\":2348},{\"attributes\":{\"n\":\"2.\"},\"end\":6035,\"start\":6023},{\"attributes\":{\"n\":\"3.\"},\"end\":8585,\"start\":8577},{\"attributes\":{\"n\":\"4.1.\"},\"end\":10758,\"start\":10745},{\"attributes\":{\"n\":\"4.2.\"},\"end\":11617,\"start\":11582},{\"attributes\":{\"n\":\"4.3.\"},\"end\":13009,\"start\":12973},{\"attributes\":{\"n\":\"4.4.\"},\"end\":14198,\"start\":14167},{\"attributes\":{\"n\":\"5.\"},\"end\":15807,\"start\":15785},{\"attributes\":{\"n\":\"5.1.\"},\"end\":15850,\"start\":15810},{\"attributes\":{\"n\":\"5.2.\"},\"end\":16361,\"start\":16318},{\"attributes\":{\"n\":\"5.3.\"},\"end\":17010,\"start\":16938},{\"attributes\":{\"n\":\"6.\"},\"end\":18508,\"start\":18482},{\"end\":19339,\"start\":19330},{\"end\":19488,\"start\":19479},{\"end\":20273,\"start\":20264},{\"end\":20646,\"start\":20637},{\"end\":20993,\"start\":20984},{\"end\":21294,\"start\":21285},{\"end\":21762,\"start\":21753}]", "table": "[{\"end\":19477,\"start\":19359},{\"end\":20262,\"start\":19565},{\"end\":20635,\"start\":20382},{\"end\":20982,\"start\":20808},{\"end\":21283,\"start\":21129},{\"end\":21751,\"start\":21555},{\"end\":22362,\"start\":22043}]", "figure_caption": "[{\"end\":19359,\"start\":19341},{\"end\":19565,\"start\":19490},{\"end\":20382,\"start\":20275},{\"end\":20808,\"start\":20648},{\"end\":21129,\"start\":20995},{\"end\":21555,\"start\":21296},{\"end\":22043,\"start\":21764}]", "figure_ref": "[{\"end\":3540,\"start\":3532},{\"end\":5650,\"start\":5642}]", "bib_author_first_name": "[{\"end\":22870,\"start\":22869},{\"end\":22872,\"start\":22871},{\"end\":23023,\"start\":23022},{\"end\":23025,\"start\":23024},{\"end\":23034,\"start\":23033},{\"end\":23043,\"start\":23042},{\"end\":23053,\"start\":23052},{\"end\":23064,\"start\":23063},{\"end\":23420,\"start\":23419},{\"end\":23427,\"start\":23426},{\"end\":23434,\"start\":23433},{\"end\":23438,\"start\":23435},{\"end\":23716,\"start\":23715},{\"end\":23722,\"start\":23721},{\"end\":23739,\"start\":23731},{\"end\":24019,\"start\":24018},{\"end\":24026,\"start\":24025},{\"end\":24033,\"start\":24032},{\"end\":24037,\"start\":24034},{\"end\":24395,\"start\":24394},{\"end\":24410,\"start\":24409},{\"end\":24422,\"start\":24421},{\"end\":24426,\"start\":24423},{\"end\":24439,\"start\":24438},{\"end\":24449,\"start\":24448},{\"end\":24460,\"start\":24459},{\"end\":24950,\"start\":24949},{\"end\":24963,\"start\":24962},{\"end\":24973,\"start\":24972},{\"end\":24983,\"start\":24982},{\"end\":25219,\"start\":25218},{\"end\":25227,\"start\":25226},{\"end\":25236,\"start\":25235},{\"end\":25245,\"start\":25244},{\"end\":25251,\"start\":25250},{\"end\":25260,\"start\":25259},{\"end\":25270,\"start\":25269},{\"end\":25281,\"start\":25280},{\"end\":25290,\"start\":25289},{\"end\":25636,\"start\":25635},{\"end\":25647,\"start\":25646},{\"end\":25658,\"start\":25657},{\"end\":25671,\"start\":25670},{\"end\":25682,\"start\":25681},{\"end\":25951,\"start\":25950},{\"end\":25962,\"start\":25961},{\"end\":25970,\"start\":25969},{\"end\":25981,\"start\":25980},{\"end\":26571,\"start\":26570},{\"end\":26573,\"start\":26572},{\"end\":26582,\"start\":26581},{\"end\":26592,\"start\":26591},{\"end\":26928,\"start\":26927},{\"end\":26936,\"start\":26935},{\"end\":26947,\"start\":26946},{\"end\":26956,\"start\":26955},{\"end\":26968,\"start\":26967},{\"end\":26979,\"start\":26978},{\"end\":26987,\"start\":26986},{\"end\":27000,\"start\":26999},{\"end\":27002,\"start\":27001},{\"end\":27360,\"start\":27359},{\"end\":27366,\"start\":27365},{\"end\":27368,\"start\":27367},{\"end\":27620,\"start\":27619},{\"end\":27627,\"start\":27626},{\"end\":27634,\"start\":27633},{\"end\":27642,\"start\":27641},{\"end\":27646,\"start\":27643},{\"end\":27955,\"start\":27954},{\"end\":27957,\"start\":27956},{\"end\":27967,\"start\":27966},{\"end\":27969,\"start\":27968},{\"end\":27980,\"start\":27979},{\"end\":27982,\"start\":27981},{\"end\":27992,\"start\":27991},{\"end\":28003,\"start\":28002},{\"end\":28012,\"start\":28011},{\"end\":28014,\"start\":28013},{\"end\":28023,\"start\":28022},{\"end\":28366,\"start\":28365},{\"end\":28375,\"start\":28374},{\"end\":28389,\"start\":28388},{\"end\":28398,\"start\":28397},{\"end\":28405,\"start\":28404},{\"end\":28707,\"start\":28706},{\"end\":28725,\"start\":28724},{\"end\":28734,\"start\":28733},{\"end\":28748,\"start\":28744},{\"end\":28761,\"start\":28760},{\"end\":28763,\"start\":28762},{\"end\":28773,\"start\":28772},{\"end\":29139,\"start\":29138},{\"end\":29153,\"start\":29152},{\"end\":29162,\"start\":29161},{\"end\":29172,\"start\":29171},{\"end\":29388,\"start\":29387},{\"end\":29390,\"start\":29389},{\"end\":29400,\"start\":29399},{\"end\":29409,\"start\":29408},{\"end\":29735,\"start\":29734},{\"end\":29985,\"start\":29984},{\"end\":29999,\"start\":29998},{\"end\":30011,\"start\":30010},{\"end\":30024,\"start\":30023},{\"end\":30039,\"start\":30038},{\"end\":30053,\"start\":30052},{\"end\":30067,\"start\":30066},{\"end\":30079,\"start\":30078},{\"end\":30091,\"start\":30090},{\"end\":30103,\"start\":30102},{\"end\":30610,\"start\":30609},{\"end\":30622,\"start\":30621},{\"end\":30636,\"start\":30635},{\"end\":30652,\"start\":30651},{\"end\":30666,\"start\":30665},{\"end\":30683,\"start\":30679},{\"end\":30692,\"start\":30691},{\"end\":30703,\"start\":30702},{\"end\":30716,\"start\":30715},{\"end\":30730,\"start\":30729},{\"end\":31316,\"start\":31315},{\"end\":31318,\"start\":31317},{\"end\":31325,\"start\":31324},{\"end\":31339,\"start\":31338},{\"end\":31341,\"start\":31340},{\"end\":31350,\"start\":31349},{\"end\":31352,\"start\":31351},{\"end\":31658,\"start\":31657},{\"end\":31672,\"start\":31671},{\"end\":32075,\"start\":32074},{\"end\":32085,\"start\":32084},{\"end\":32096,\"start\":32095},{\"end\":32105,\"start\":32104},{\"end\":32117,\"start\":32116},{\"end\":32127,\"start\":32126},{\"end\":32136,\"start\":32135},{\"end\":32146,\"start\":32145},{\"end\":32158,\"start\":32157},{\"end\":32160,\"start\":32159},{\"end\":32169,\"start\":32168},{\"end\":32655,\"start\":32654},{\"end\":33148,\"start\":33147},{\"end\":33298,\"start\":33297},{\"end\":33874,\"start\":33873},{\"end\":33884,\"start\":33883},{\"end\":33893,\"start\":33892},{\"end\":34186,\"start\":34185},{\"end\":34188,\"start\":34187},{\"end\":34574,\"start\":34573},{\"end\":34583,\"start\":34582},{\"end\":34597,\"start\":34596},{\"end\":34606,\"start\":34605},{\"end\":34613,\"start\":34612},{\"end\":34849,\"start\":34848},{\"end\":34859,\"start\":34858},{\"end\":35038,\"start\":35037},{\"end\":35046,\"start\":35045},{\"end\":35561,\"start\":35560},{\"end\":35574,\"start\":35573},{\"end\":35587,\"start\":35586},{\"end\":35599,\"start\":35598},{\"end\":35609,\"start\":35608},{\"end\":35620,\"start\":35619},{\"end\":35630,\"start\":35629},{\"end\":35641,\"start\":35640},{\"end\":35657,\"start\":35656},{\"end\":35666,\"start\":35665},{\"end\":35677,\"start\":35676},{\"end\":35691,\"start\":35690},{\"end\":35701,\"start\":35700},{\"end\":35715,\"start\":35714},{\"end\":35726,\"start\":35725},{\"end\":35736,\"start\":35735}]", "bib_author_last_name": "[{\"end\":22884,\"start\":22873},{\"end\":23031,\"start\":23026},{\"end\":23040,\"start\":23035},{\"end\":23050,\"start\":23044},{\"end\":23061,\"start\":23054},{\"end\":23073,\"start\":23065},{\"end\":23424,\"start\":23421},{\"end\":23431,\"start\":23428},{\"end\":23443,\"start\":23439},{\"end\":23719,\"start\":23717},{\"end\":23729,\"start\":23723},{\"end\":23747,\"start\":23740},{\"end\":24023,\"start\":24020},{\"end\":24030,\"start\":24027},{\"end\":24042,\"start\":24038},{\"end\":24407,\"start\":24396},{\"end\":24419,\"start\":24411},{\"end\":24436,\"start\":24427},{\"end\":24446,\"start\":24440},{\"end\":24457,\"start\":24450},{\"end\":24469,\"start\":24461},{\"end\":24960,\"start\":24951},{\"end\":24970,\"start\":24964},{\"end\":24980,\"start\":24974},{\"end\":24991,\"start\":24984},{\"end\":25224,\"start\":25220},{\"end\":25233,\"start\":25228},{\"end\":25242,\"start\":25237},{\"end\":25248,\"start\":25246},{\"end\":25257,\"start\":25252},{\"end\":25267,\"start\":25261},{\"end\":25278,\"start\":25271},{\"end\":25287,\"start\":25282},{\"end\":25298,\"start\":25291},{\"end\":25644,\"start\":25637},{\"end\":25655,\"start\":25648},{\"end\":25668,\"start\":25659},{\"end\":25679,\"start\":25672},{\"end\":25687,\"start\":25683},{\"end\":25959,\"start\":25952},{\"end\":25967,\"start\":25963},{\"end\":25978,\"start\":25971},{\"end\":25986,\"start\":25982},{\"end\":26579,\"start\":26574},{\"end\":26589,\"start\":26583},{\"end\":26600,\"start\":26593},{\"end\":26933,\"start\":26929},{\"end\":26944,\"start\":26937},{\"end\":26953,\"start\":26948},{\"end\":26965,\"start\":26957},{\"end\":26976,\"start\":26969},{\"end\":26984,\"start\":26980},{\"end\":26997,\"start\":26988},{\"end\":27010,\"start\":27003},{\"end\":27363,\"start\":27361},{\"end\":27376,\"start\":27369},{\"end\":27624,\"start\":27621},{\"end\":27631,\"start\":27628},{\"end\":27639,\"start\":27635},{\"end\":27651,\"start\":27647},{\"end\":27964,\"start\":27958},{\"end\":27977,\"start\":27970},{\"end\":27989,\"start\":27983},{\"end\":28000,\"start\":27993},{\"end\":28009,\"start\":28004},{\"end\":28020,\"start\":28015},{\"end\":28030,\"start\":28024},{\"end\":28372,\"start\":28367},{\"end\":28386,\"start\":28376},{\"end\":28395,\"start\":28390},{\"end\":28402,\"start\":28399},{\"end\":28416,\"start\":28406},{\"end\":28722,\"start\":28708},{\"end\":28731,\"start\":28726},{\"end\":28742,\"start\":28735},{\"end\":28758,\"start\":28749},{\"end\":28770,\"start\":28764},{\"end\":28782,\"start\":28774},{\"end\":29150,\"start\":29140},{\"end\":29159,\"start\":29154},{\"end\":29169,\"start\":29163},{\"end\":29180,\"start\":29173},{\"end\":29397,\"start\":29391},{\"end\":29406,\"start\":29401},{\"end\":29420,\"start\":29410},{\"end\":29746,\"start\":29736},{\"end\":29996,\"start\":29986},{\"end\":30008,\"start\":30000},{\"end\":30021,\"start\":30012},{\"end\":30036,\"start\":30025},{\"end\":30050,\"start\":30040},{\"end\":30064,\"start\":30054},{\"end\":30076,\"start\":30068},{\"end\":30088,\"start\":30080},{\"end\":30100,\"start\":30092},{\"end\":30114,\"start\":30104},{\"end\":30619,\"start\":30611},{\"end\":30633,\"start\":30623},{\"end\":30649,\"start\":30637},{\"end\":30663,\"start\":30653},{\"end\":30677,\"start\":30667},{\"end\":30689,\"start\":30684},{\"end\":30700,\"start\":30693},{\"end\":30713,\"start\":30704},{\"end\":30727,\"start\":30717},{\"end\":30736,\"start\":30731},{\"end\":31322,\"start\":31319},{\"end\":31336,\"start\":31326},{\"end\":31347,\"start\":31342},{\"end\":31360,\"start\":31353},{\"end\":31669,\"start\":31659},{\"end\":31682,\"start\":31673},{\"end\":32082,\"start\":32076},{\"end\":32093,\"start\":32086},{\"end\":32102,\"start\":32097},{\"end\":32114,\"start\":32106},{\"end\":32124,\"start\":32118},{\"end\":32133,\"start\":32128},{\"end\":32143,\"start\":32137},{\"end\":32155,\"start\":32147},{\"end\":32166,\"start\":32161},{\"end\":32175,\"start\":32170},{\"end\":32663,\"start\":32656},{\"end\":33159,\"start\":33149},{\"end\":33307,\"start\":33299},{\"end\":33881,\"start\":33875},{\"end\":33890,\"start\":33885},{\"end\":33899,\"start\":33894},{\"end\":34194,\"start\":34189},{\"end\":34580,\"start\":34575},{\"end\":34594,\"start\":34584},{\"end\":34603,\"start\":34598},{\"end\":34610,\"start\":34607},{\"end\":34624,\"start\":34614},{\"end\":34856,\"start\":34850},{\"end\":34866,\"start\":34860},{\"end\":35043,\"start\":35039},{\"end\":35055,\"start\":35047},{\"end\":35571,\"start\":35562},{\"end\":35584,\"start\":35575},{\"end\":35596,\"start\":35588},{\"end\":35606,\"start\":35600},{\"end\":35617,\"start\":35610},{\"end\":35627,\"start\":35621},{\"end\":35638,\"start\":35631},{\"end\":35654,\"start\":35642},{\"end\":35663,\"start\":35658},{\"end\":35674,\"start\":35667},{\"end\":35688,\"start\":35678},{\"end\":35698,\"start\":35692},{\"end\":35712,\"start\":35702},{\"end\":35723,\"start\":35716},{\"end\":35733,\"start\":35727},{\"end\":35746,\"start\":35737}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":22962,\"start\":22860},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":36496524},\"end\":23295,\"start\":22964},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52291361},\"end\":23650,\"start\":23297},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":48358198},\"end\":23931,\"start\":23652},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":213601901},\"end\":24316,\"start\":23933},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":209140461},\"end\":24864,\"start\":24318},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":218719085},\"end\":25216,\"start\":24866},{\"attributes\":{\"doi\":\"arXiv:2111.05948\",\"id\":\"b7\"},\"end\":25558,\"start\":25218},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":220055837},\"end\":25867,\"start\":25560},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219966759},\"end\":26443,\"start\":25869},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":244233851},\"end\":26850,\"start\":26445},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":18175638},\"end\":27289,\"start\":26852},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8691246},\"end\":27522,\"start\":27291},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52188902},\"end\":27852,\"start\":27524},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2677498},\"end\":28275,\"start\":27854},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":239163039},\"end\":28635,\"start\":28277},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":215731483},\"end\":29090,\"start\":28637},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":23810506},\"end\":29345,\"start\":29092},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":23783359},\"end\":29615,\"start\":29347},{\"attributes\":{\"id\":\"b19\"},\"end\":29887,\"start\":29617},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":30774188},\"end\":30479,\"start\":29889},{\"attributes\":{\"id\":\"b21\"},\"end\":31272,\"start\":30481},{\"attributes\":{\"doi\":\"TR-2016-02\",\"id\":\"b22\",\"matched_paper_id\":196099491},\"end\":31623,\"start\":31274},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":221388963},\"end\":32018,\"start\":31625},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":209376338},\"end\":32610,\"start\":32020},{\"attributes\":{\"id\":\"b25\"},\"end\":32797,\"start\":32612},{\"attributes\":{\"id\":\"b26\"},\"end\":32962,\"start\":32799},{\"attributes\":{\"id\":\"b27\"},\"end\":33124,\"start\":32964},{\"attributes\":{\"id\":\"b28\"},\"end\":33245,\"start\":33126},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8313873},\"end\":33775,\"start\":33247},{\"attributes\":{\"id\":\"b30\"},\"end\":34046,\"start\":33777},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":184484975},\"end\":34483,\"start\":34048},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":239163039},\"end\":34821,\"start\":34485},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":52874011},\"end\":34993,\"start\":34823},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4650265},\"end\":35516,\"start\":34995},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10659969},\"end\":36107,\"start\":35518}]", "bib_title": "[{\"end\":23020,\"start\":22964},{\"end\":23417,\"start\":23297},{\"end\":23713,\"start\":23652},{\"end\":24016,\"start\":23933},{\"end\":24392,\"start\":24318},{\"end\":24947,\"start\":24866},{\"end\":25633,\"start\":25560},{\"end\":25948,\"start\":25869},{\"end\":26568,\"start\":26445},{\"end\":26925,\"start\":26852},{\"end\":27357,\"start\":27291},{\"end\":27617,\"start\":27524},{\"end\":27952,\"start\":27854},{\"end\":28363,\"start\":28277},{\"end\":28704,\"start\":28637},{\"end\":29136,\"start\":29092},{\"end\":29385,\"start\":29347},{\"end\":29982,\"start\":29889},{\"end\":30607,\"start\":30481},{\"end\":31313,\"start\":31274},{\"end\":31655,\"start\":31625},{\"end\":32072,\"start\":32020},{\"end\":33295,\"start\":33247},{\"end\":34183,\"start\":34048},{\"end\":34571,\"start\":34485},{\"end\":34846,\"start\":34823},{\"end\":35035,\"start\":34995},{\"end\":35558,\"start\":35518}]", "bib_author": "[{\"end\":22886,\"start\":22869},{\"end\":23033,\"start\":23022},{\"end\":23042,\"start\":23033},{\"end\":23052,\"start\":23042},{\"end\":23063,\"start\":23052},{\"end\":23075,\"start\":23063},{\"end\":23426,\"start\":23419},{\"end\":23433,\"start\":23426},{\"end\":23445,\"start\":23433},{\"end\":23721,\"start\":23715},{\"end\":23731,\"start\":23721},{\"end\":23749,\"start\":23731},{\"end\":24025,\"start\":24018},{\"end\":24032,\"start\":24025},{\"end\":24044,\"start\":24032},{\"end\":24409,\"start\":24394},{\"end\":24421,\"start\":24409},{\"end\":24438,\"start\":24421},{\"end\":24448,\"start\":24438},{\"end\":24459,\"start\":24448},{\"end\":24471,\"start\":24459},{\"end\":24962,\"start\":24949},{\"end\":24972,\"start\":24962},{\"end\":24982,\"start\":24972},{\"end\":24993,\"start\":24982},{\"end\":25226,\"start\":25218},{\"end\":25235,\"start\":25226},{\"end\":25244,\"start\":25235},{\"end\":25250,\"start\":25244},{\"end\":25259,\"start\":25250},{\"end\":25269,\"start\":25259},{\"end\":25280,\"start\":25269},{\"end\":25289,\"start\":25280},{\"end\":25300,\"start\":25289},{\"end\":25646,\"start\":25635},{\"end\":25657,\"start\":25646},{\"end\":25670,\"start\":25657},{\"end\":25681,\"start\":25670},{\"end\":25689,\"start\":25681},{\"end\":25961,\"start\":25950},{\"end\":25969,\"start\":25961},{\"end\":25980,\"start\":25969},{\"end\":25988,\"start\":25980},{\"end\":26581,\"start\":26570},{\"end\":26591,\"start\":26581},{\"end\":26602,\"start\":26591},{\"end\":26935,\"start\":26927},{\"end\":26946,\"start\":26935},{\"end\":26955,\"start\":26946},{\"end\":26967,\"start\":26955},{\"end\":26978,\"start\":26967},{\"end\":26986,\"start\":26978},{\"end\":26999,\"start\":26986},{\"end\":27012,\"start\":26999},{\"end\":27365,\"start\":27359},{\"end\":27378,\"start\":27365},{\"end\":27626,\"start\":27619},{\"end\":27633,\"start\":27626},{\"end\":27641,\"start\":27633},{\"end\":27653,\"start\":27641},{\"end\":27966,\"start\":27954},{\"end\":27979,\"start\":27966},{\"end\":27991,\"start\":27979},{\"end\":28002,\"start\":27991},{\"end\":28011,\"start\":28002},{\"end\":28022,\"start\":28011},{\"end\":28032,\"start\":28022},{\"end\":28374,\"start\":28365},{\"end\":28388,\"start\":28374},{\"end\":28397,\"start\":28388},{\"end\":28404,\"start\":28397},{\"end\":28418,\"start\":28404},{\"end\":28724,\"start\":28706},{\"end\":28733,\"start\":28724},{\"end\":28744,\"start\":28733},{\"end\":28760,\"start\":28744},{\"end\":28772,\"start\":28760},{\"end\":28784,\"start\":28772},{\"end\":29152,\"start\":29138},{\"end\":29161,\"start\":29152},{\"end\":29171,\"start\":29161},{\"end\":29182,\"start\":29171},{\"end\":29399,\"start\":29387},{\"end\":29408,\"start\":29399},{\"end\":29422,\"start\":29408},{\"end\":29748,\"start\":29734},{\"end\":29998,\"start\":29984},{\"end\":30010,\"start\":29998},{\"end\":30023,\"start\":30010},{\"end\":30038,\"start\":30023},{\"end\":30052,\"start\":30038},{\"end\":30066,\"start\":30052},{\"end\":30078,\"start\":30066},{\"end\":30090,\"start\":30078},{\"end\":30102,\"start\":30090},{\"end\":30116,\"start\":30102},{\"end\":30621,\"start\":30609},{\"end\":30635,\"start\":30621},{\"end\":30651,\"start\":30635},{\"end\":30665,\"start\":30651},{\"end\":30679,\"start\":30665},{\"end\":30691,\"start\":30679},{\"end\":30702,\"start\":30691},{\"end\":30715,\"start\":30702},{\"end\":30729,\"start\":30715},{\"end\":30738,\"start\":30729},{\"end\":31324,\"start\":31315},{\"end\":31338,\"start\":31324},{\"end\":31349,\"start\":31338},{\"end\":31362,\"start\":31349},{\"end\":31671,\"start\":31657},{\"end\":31684,\"start\":31671},{\"end\":32084,\"start\":32074},{\"end\":32095,\"start\":32084},{\"end\":32104,\"start\":32095},{\"end\":32116,\"start\":32104},{\"end\":32126,\"start\":32116},{\"end\":32135,\"start\":32126},{\"end\":32145,\"start\":32135},{\"end\":32157,\"start\":32145},{\"end\":32168,\"start\":32157},{\"end\":32177,\"start\":32168},{\"end\":32665,\"start\":32654},{\"end\":33161,\"start\":33147},{\"end\":33309,\"start\":33297},{\"end\":33883,\"start\":33873},{\"end\":33892,\"start\":33883},{\"end\":33901,\"start\":33892},{\"end\":34196,\"start\":34185},{\"end\":34582,\"start\":34573},{\"end\":34596,\"start\":34582},{\"end\":34605,\"start\":34596},{\"end\":34612,\"start\":34605},{\"end\":34626,\"start\":34612},{\"end\":34858,\"start\":34848},{\"end\":34868,\"start\":34858},{\"end\":35045,\"start\":35037},{\"end\":35057,\"start\":35045},{\"end\":35573,\"start\":35560},{\"end\":35586,\"start\":35573},{\"end\":35598,\"start\":35586},{\"end\":35608,\"start\":35598},{\"end\":35619,\"start\":35608},{\"end\":35629,\"start\":35619},{\"end\":35640,\"start\":35629},{\"end\":35656,\"start\":35640},{\"end\":35665,\"start\":35656},{\"end\":35676,\"start\":35665},{\"end\":35690,\"start\":35676},{\"end\":35700,\"start\":35690},{\"end\":35714,\"start\":35700},{\"end\":35725,\"start\":35714},{\"end\":35735,\"start\":35725},{\"end\":35748,\"start\":35735}]", "bib_venue": "[{\"end\":22867,\"start\":22860},{\"end\":23114,\"start\":23075},{\"end\":23451,\"start\":23445},{\"end\":23769,\"start\":23749},{\"end\":24096,\"start\":24044},{\"end\":24534,\"start\":24471},{\"end\":25015,\"start\":24993},{\"end\":25363,\"start\":25316},{\"end\":25693,\"start\":25689},{\"end\":26037,\"start\":25988},{\"end\":26618,\"start\":26602},{\"end\":27038,\"start\":27012},{\"end\":27389,\"start\":27378},{\"end\":27657,\"start\":27653},{\"end\":28038,\"start\":28032},{\"end\":28429,\"start\":28418},{\"end\":28820,\"start\":28784},{\"end\":29193,\"start\":29182},{\"end\":29453,\"start\":29422},{\"end\":29732,\"start\":29617},{\"end\":30150,\"start\":30116},{\"end\":30861,\"start\":30738},{\"end\":31402,\"start\":31372},{\"end\":31751,\"start\":31684},{\"end\":32259,\"start\":32177},{\"end\":32652,\"start\":32612},{\"end\":32838,\"start\":32799},{\"end\":33002,\"start\":32964},{\"end\":33145,\"start\":33126},{\"end\":33439,\"start\":33309},{\"end\":33871,\"start\":33777},{\"end\":34241,\"start\":34196},{\"end\":34637,\"start\":34626},{\"end\":34884,\"start\":34868},{\"end\":35169,\"start\":35057},{\"end\":35784,\"start\":35748},{\"end\":24584,\"start\":24536},{\"end\":25033,\"start\":25017},{\"end\":30171,\"start\":30152},{\"end\":31770,\"start\":31753},{\"end\":32328,\"start\":32261},{\"end\":33556,\"start\":33441},{\"end\":35285,\"start\":35171}]"}}}, "year": 2023, "month": 12, "day": 17}