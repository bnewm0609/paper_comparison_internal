{"id": 237142564, "updated": "2023-10-06 00:04:54.836", "metadata": {"title": "DRAEM -- A discriminatively trained reconstruction embedding for surface anomaly detection", "authors": "[{\"first\":\"Vitjan\",\"last\":\"Zavrtanik\",\"middle\":[]},{\"first\":\"Matej\",\"last\":\"Kristan\",\"middle\":[]},{\"first\":\"Danijel\",\"last\":\"Skovcaj\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 8, "day": 17}, "abstract": "Visual surface anomaly detection aims to detect local image regions that significantly deviate from normal appearance. Recent surface anomaly detection methods rely on generative models to accurately reconstruct the normal areas and to fail on anomalies. These methods are trained only on anomaly-free images, and often require hand-crafted post-processing steps to localize the anomalies, which prohibits optimizing the feature extraction for maximal detection capability. In addition to reconstructive approach, we cast surface anomaly detection primarily as a discriminative problem and propose a discriminatively trained reconstruction anomaly embedding model (DRAEM). The proposed method learns a joint representation of an anomalous image and its anomaly-free reconstruction, while simultaneously learning a decision boundary between normal and anomalous examples. The method enables direct anomaly localization without the need for additional complicated post-processing of the network output and can be trained using simple and general anomaly simulations. On the challenging MVTec anomaly detection dataset, DRAEM outperforms the current state-of-the-art unsupervised methods by a large margin and even delivers detection performance close to the fully-supervised methods on the widely used DAGM surface-defect detection dataset, while substantially outperforming them in localization accuracy.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.07610", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/ZavrtanikKS21", "doi": "10.1109/iccv48922.2021.00822"}}, "content": {"source": {"pdf_hash": "95a26eafabf06b1fc5dec6c460a927cf5964e97e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.07610v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ff99dcca08bb2ffbdd39f2da9eef0dd7d7702325", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/95a26eafabf06b1fc5dec6c460a927cf5964e97e.txt", "contents": "\nDRAEM -A discriminatively trained reconstruction embedding for surface anomaly detection\n\n\nVitjan Zavrtanik vitjan.zavrtanik@fri.uni-lj.si \nFaculty of Computer and Information Science\nUniversity of Ljubljana\n\n\nMatej Kristan matej.kristan@fri.uni-lj.si \nFaculty of Computer and Information Science\nUniversity of Ljubljana\n\n\nDanijel Sko\u010daj danijel.skocaj@fri.uni-lj.si \nFaculty of Computer and Information Science\nUniversity of Ljubljana\n\n\nDRAEM -A discriminatively trained reconstruction embedding for surface anomaly detection\n\nVisual surface anomaly detection aims to detect local image regions that significantly deviate from normal appearance. Recent surface anomaly detection methods rely on generative models to accurately reconstruct the normal areas and to fail on anomalies. These methods are trained only on anomaly-free images, and often require hand-crafted post-processing steps to localize the anomalies, which prohibits optimizing the feature extraction for maximal detection capability. In addition to reconstructive approach, we cast surface anomaly detection primarily as a discriminative problem and propose a discriminatively trained reconstruction anomaly embedding model (DRAEM). The proposed method learns a joint representation of an anomalous image and its anomaly-free reconstruction, while simultaneously learning a decision boundary between normal and anomalous examples. The method enables direct anomaly localization without the need for additional complicated post-processing of the network output and can be trained using simple and general anomaly simulations. On the challenging MVTec anomaly detection dataset, DRAEM outperforms the current state-of-theart unsupervised methods by a large margin and even delivers detection performance close to the fully-supervised methods on the widely used DAGM surface-defect detection dataset, while substantially outperforming them in localization accuracy.\n\nIntroduction\n\nSurface anomaly detection addresses localization of image regions that deviate from a normal appearance (Figure 1). A closely related general anomaly detection problem considers anomalies as entire images that significantly differ from the non-anomalous training set images. In contrast, in surface anomaly detection problems, the anomalies occupy only a small fraction of image pixels and are typically close to the training set distribution. This is a particu-M o Figure 1. DRAEM estimates the decision boundary between the normal an anomalous pixels solely by training on synthetic anomalies automatically generated on anomaly-free images (left) and generalizes to a variety of real-world anomalies (right). The result (Mo) closely matches the ground truth (GT). larly challenging task, which is common in quality control and surface defect localization applications.\n\nIn practice, anomaly appearances may significantly vary, and in applications like quality control, images with anomalies present are rare and manual annotation may be overly time consuming. This leads to highly imbalanced training sets, often containing only anomaly-free images. Significant effort has thus been recently invested in designing robust surface anomaly detection methods that preferably require minimal supervision from manual annotation.\n\nReconstructive methods, such as Autoencoders [5,1,2,26] and GANs [24,23], have been extensively explored since they enable learning of a powerful reconstruction subspace, using only anomaly-free images. Relying on poor reconstruction capability of anomalous regions, not observed in training, the anomalies can then be detected by thresholding the difference between the input image and its re- Figure 2. Autoencoders over-generalize to anomalies, while discriminative approaches over-fit to the synthetic anomalies and do not generalize to real data. Our approach jointly discriminatively learns the reconstruction subspace and a hyper-plane over the joint original and reconstructed space using the simulated anomalies and leads to substantially better generalization to real anomalies.\n\nconstruction. However, determining the presence of anomalies that are not substantially different from normal appearance remains challenging, since these are often well reconstructed, as depicted in Figure 2, top-left.\n\nRecent improvements thus consider the difference between deep features extracted from a general-purpose network and a network specialized for anomaly-free images [4]. Discrimination can also be formulated as a deviation from a dense clustering of non-anomalous textures within the deep subspace [22,7], as forming such a compact subspace prevents anomalies from being mapped close to anomaly-free samples. A common drawback of the generative methods is that they only learn the model from anomaly-free data, and are not explicitly optimized for discriminative anomaly detection, since positive examples (i.e., anomalies) are not available at training time. Synthetic anomalies could be considered to train discriminative segmentation methods [8,21], but this leads to over-fitting to synthetic appearances and results in a learned decision boundary that generalizes poorly to real anomalies ( Figure  2, top-right).\n\nWe hypothesize that over-fitting can be substantially reduced by training a discriminative model over the joint, reconstructed and original, appearance along with the reconstruction subspace. This way the model does not overfit to synthetic appearance, but rather learns a local-appearanceconditioned distance function between the original and reconstructed anomaly appearance, which generalizes well over a range of real anomalies (see Figure 2, bottom).\n\nTo validate our hypothesis, we propose, as our main contribution, a new deep surface anomaly detection network, discriminatively trained in an end-to-end manner on synthetically generated just-out-of-distribution patterns, which do not have to faithfully represent the target-domain anomalies. The network is composed of a reconstructive subnetwork, followed by a discriminative sub-network (Figure 3). The reconstructive sub-network is trained to learn anomaly-free reconstruction, while the discriminative subnetwork learns a discriminative model over the joint appearance of the original and reconstructed images, producing a high-fidelity per-pixel anomaly detection map ( Figure 1).\n\nIn contrast to related approaches that learn surrogate generative tasks, the proposed model is trained discriminatively, yet does not require the synthetic anomaly appearances to closely match the anomalies at test time and outperforms the recent, more complex, state-of-the-art methods by a large margin.\n\n\nRelated work\n\nMany surface anomaly detection methods focus on image reconstruction and detect anomalies based on image reconstruction error [1,2,5,24,23,26,31]. Auto-encoders are commonly used for image reconstruction [5]. In [1,2,26] auto-encoders are trained with adversarial losses. The anomaly score of the image is then based on the image reconstruction quality or in the case of adversarially trained auto-encoders, the discriminator output. In [24,23] a GAN [13] is trained to generate images that fit the training distribution. In [23] an encoder network is additionally trained that finds the latent representation of the input image that minimizes the reconstruction loss when used as the input by the pretrained generator. The anomaly score is then based on the reconstruction quality and the discriminator output. In [29] an interpolation auto-encoder is trained to learn a dense representation space of in-distribution samples. The anomaly score is then based on a discriminator, trained to estimate the distance between the input-input and input-output joint distributions, however the approach to surface anomaly detection remains generative as the discriminator evaluates the reconstruction quality.\n\nInstead of the commonly used image space reconstruction, the reconstruction of pretrained network features can also be used for surface anomaly detection [4,25]. Anomalies are detected based on the assumption that features of a pre-trained network will not be faithfully reconstructued by another network trained only on anomaly-free images. Alternatively [20,11] propose surface anomaly detection as identifying significant deviations from a Gaussian fitted to anomaly-free features of a pre-trained network. This requires a unimodal distribution of the anomaly-free visual features which is problematic on diverse datasets. [16] propose a one-class variational auto-encoder gradient-based attention maps as output anomaly maps. However the method is sensitive to subtle anomalies close to the normal sample distribution.\n\nRecently Patch-based one-class classification methods have been considered for surface anomaly detection [30]. These are based on one-class methods [22,7] which attempt to estimate a decision boundary around anomaly-free data that separates it from anomalous samples by assuming a unimodal distribution of the anomaly-free data. This assumption is often violated in surface anomaly data.\n\n\nDRAEM\n\nThe proposed discriminative joint reconstructionanomaly embedding method (DRAEM) is composed from a reconstructive and a discriminative sub-networks (see Figure 3). The reconstructive sub-network is trained to implicitly detect and reconstruct the anomalies with semantically plausible anomaly-free content, while keeping the non-anomalous regions of the input image unchanged. Simultaneously, the discriminative sub-network learns a joint reconstruction-anomaly embedding and produces accurate anomaly segmentation maps from the concatenated reconstructed and original appearance. Anomalous training examples are created by a conceptually simple process that simulates anomalies on anomaly-free images. This anomaly generation method provides an arbitrary amount of anomalous samples as well as pixel-perfect anomaly segmentation maps which can be used for training the proposed method without real anomalous samples.\n\n\nReconstructive sub-network\n\nThe reconstructive sub-network is formulated as an encoder-decoder architecture that converts the local patterns of an input image into patterns closer to the distribution of normal samples. The network is trained to reconstruct the original image I from an artificially corrupted version I a obtained by a simulator (see Section 3.3).\n\nAn l 2 loss is often used in reconstruction based anomaly detection methods [1,2], however this assumes an independence between neighboring pixels, therefore a patch based SSIM [27] loss is additionally used as in [5,31]:\nL SSIM (I, I r ) = 1 N p H i=1 W j=1 1 \u2212 SSIM I, I r (i,j) ,(1)\nwhere H and W are the height and width of image I, respectively. N p is equal to the number of pixels in I. I r is the reconstructed image output by the network. SSIM (I, I r ) (i,j) is the SSIM value for patches of I and I r , centered at image coordinates (i, j). The reconstruction loss is therefore:\nL rec (I, I r ) = \u03bbL SSIM (I, I r ) + l 2 (I, I r ),(2)\nwhere \u03bb is a loss balancing hyper-parameter. Note that an additional training signal is acquired from the downstream discriminative network (Section 3.2), which performs anomaly localization by detecting the reconstruction difference.\n\n\nDiscriminative sub-network\n\nThe discriminative sub-network uses U-Net [21]-like architecture. The sub-network input I c is defined as the channel-wise concatenation of the reconstructive subnetwork output I r and the input image I. Due to the normality-restoring property of the reconstructive subnetwork, the joint appearance of I and I r differs significantly in anomalous images, providing the information necessary for anomaly segmentation. In reconstruction-based anomaly detection methods anomaly maps are obtained using similarity functions such as SSIM [27] to compare the original image to its reconstruction, however a surface anomaly detection-specific similarity measure is difficult to hand-craft. In contrast, the discriminative sub-network learns the appropriate distance measure automatically. The network outputs an anomaly score map M o of the same size as I. Focal Loss [14] (L seg ) is applied on the discriminative sub-network output to increase robustness towards accurate segmentation of hard examples.\n\nConsidering both the segmentation and the reconstructive objectives of the two sub-networks, the total loss used in training DRAEM is\nL(I, I r , M a , M ) = L rec (I, I r ) + L seg (M a , M ), (3)\nwhere M a and M are the ground truth and the output anomaly segmentation masks, respectively.\n\n\nSimulated anomaly generation\n\nDRAEM does not require simulations to realistically reflect the real anomaly appearance in the target domain, but rather to generate just-out-of-distribution appearances, which allow learning the appropriate distance function to recognize the anomaly by its deviation from normality. The proposed anomaly simulator follows this paradigm.\n\nA noise image is generated by a Perlin noise generator [18] to capture a variety of anomaly shapes (Figure 4, P ) and binarized by a threshod sampled uniformly at random ( Figure 4, M a ) into an anomaly map M a . The anomaly texture source image A is sampled from an anomaly source image dataset which is unrelated to the input image distribution (Figure 4, A). Random augmentation sampling, inspired by RandAugment [10], is then applied by a set\n\n\nAnomaly generation\n\nReconstructive sub-network Discriminative sub-network Figure 3. The anomaly detection process of the proposed method. First anomalous regions are implicitly detected and inpainted by the reconstructive sub-network trained using Lrec. The output of the reconstructive sub-network and the input image are then concatenated and fed into the discriminative sub-network. The segmentation network, trained using the Focal loss L f ocal [14], localizes the anomalous region and produces an anomaly map. The image level anomaly score \u03b7 is acquired from the anomaly score map.\n\nof 3 random augmentation functions sampled from the set: {posterize, sharpness, solarize, equalize, brightness change, color change, auto-contrast}. The augmented texture image A is masked with the anomaly map M a and blended with I to create anomalies that are just-out-ofdistribution, and thus help tighten the decision boundary in the trained network. The augmented training image I a is therefore defined as\nI a = M a I + (1 \u2212 \u03b2)(M a I) + \u03b2(M a A), (4)\nwhere M a is the inverse of M a , is the element-wise multiplication operation and \u03b2 is the opacity parameter in blending. This parameter is sampled uniformly from an interval, i.e., \u03b2 \u2208 [0.1, 1.0]. The randomized blending and augmentation afford generating diverse anomalous images from as little as a single texture (see Figure 5).\n\nThe above described simulator thus generates training sample triplets containing the original anomaly-free image I, the augmented image containing simulated anomalies I a and the pixel-perfect anomaly mask M a .\n\n\nSurface anomaly localization and detection\n\nThe output of the discriminative sub-network is a pixellevel anomaly detection mask M o , which can be interpreted in a straight-forward way for the image-level anomaly score estimation, i.e., whether an anomaly is present in the image.\n\nFirst, M o is smoothed by a mean filter convolution layer to aggregate the local anomaly response information. The final image-level anomaly score \u03b7 is computed by taking the maximum value of the smoothed anomaly score map:\n\u03b7 = max M o * f s f \u00d7s f ,(5)\nwhere f s f \u00d7s f is a mean filter of size s f \u00d7 s f and * is the convolution operator. In a preliminary study, we trained a classification network for the image-level anomaly classification, but did not observe improvements over the direct score estimation method (5).  \n\n\nExperiments\n\nDRAEM is extensively evaluated and compared with the recent state-of-the-art on unsupervised surface anomaly detection and localization. Additionally, individual components of the proposed method and the effectiveness of training on simulated anomalies are evaluated by an ablation study. Finally, the results are placed in a broader perspective by comparing DRAEM with state-of-the-art weaklysupervised and fully-supervised surface-defect detection methods.\n\n\nComparison with unsupervised methods\n\nDRAEM is evaluated on the recent challenging MVTec anomaly detection dataset [3], which has been established as a standard benchmark dataset for evaluating unsupervised surface anomaly detection methods. We evaluate DRAEM on the tasks of surface anomaly detection and localisation. The MVTec dataset contains 15 object classes with a diverse set anomalies which enables a general evaluation of surface anomaly detection methods. Anomalous examples of the MVTec dataset are shown in Figure 8. For evaluation, the standard metric in anomaly detection, AU-ROC, is used. Image-level AUROC is used for anomaly detection and a pixel-based AUROC for evaluating anomaly localization [5,24,17,26]. The AUROC, however, does not reflect the localization accuracy well in surface anomaly detection setups, where only a small fraction of pixels are anomalous. The reason is that false positive rate is dominated by the a-priori very high number of non-anomalous pixels and is thus kept low despite of false positive detections. We thus additionally report the pixel-wise average precision metric (AP), which is more appropriate for highly imbalanced classes and in particular for surface anomaly detection, where the precision plays an important role.\n\nIn our experiments, the network is trained for 700 epochs on the MVTec anomaly detection dataset [3]. The learning rate is set to 10 \u22124 and is multiplied by 0.1 after 400 and 600 epochs. Image rotation in the range of (\u221245, 45) degrees is used as a data augmentation method on anomaly free images during training to alleviate overfitting due to the relatively small anomaly-free training set size. The Describable Textures Dataset [9] is used as the anomaly source dataset.\n\nA number of obtained qualitative examples are presented in Figure 8. As one can observe, the obtained anomaly masks are very detailed and resemble the given ground truth labels to a high degree of accuracy. Consequently, DRAEM achieves state-of-the-art quantitative results across all MVTec classes for surface anomaly detection as well as localization.\n\nSurface Anomaly Detection. Table 1 quantitatively compares DRAEM with recent approaches on the task of image-level surface anomaly detection. DRAEM significantly outperforms all recent surface anomaly detec- tion methods, achieving the highest AUROC in 9 out of 15 classes and achieving comparable results in the other classes. It surpasses the previous best state-of-the-art approach by 2.5 percentage points. The reduced performance in some classes could be explained by particularly difficult anomalies that are close to the normal image distribution. The absence of a part of the object is especially difficult to detect. Regions, where the object features are missing, usually contain other commonly occurring features. This makes such anomalies difficult to distinguish from anomaly-free regions. An example of this can be seen in Figure 6, where some of the transistor leads had been cut. The ground truth marks the area where the broken lead should be as anomalous. DRAEM only detects anomalous features in a small region of the cut lead, as the background features are common during training. Anomaly Localization. Table 2 compares DRAEM to the recent state-of-the-art on the task of pixel-level surface anomaly detection. DRAEM achieves comparable results to the previous best-performing methods in terms of AUROC scores and surpasses the state-of-the-art by 13.4 percentage points in terms of AP. A better AP score is achieved in 11 out of 15 classes and is comparable to the state-of-the-art in other classes. A qualitative comparison with the state-ofthe-art method Uninformed Students [4] and PaDim [11] is shown in Figure 7. DRAEM achieves a significant improvement in anomaly segmentation accuracy.\n\nA detailed inspection showed that some of the detection errors can be attributed to the inaccurate ground truth labels on ambiguous anomalies. An example of this is shown in Figure 6, where the ground truth covers the entire surface of the pill, yet only the yellow dots are anomalous. DRAEM produces an anomaly map that correctly localizes the yellow dots, but the discrepancy with the ground truth mask  increases the performance error. These annotation ambiguities also impact the AP score of the evaluated methods. \n\n\nAblation Study\n\nThe DRAEM design choices are analyzed by groups of experiments evaluating (i) the method architecture, (ii) the choice of anomaly appearance patterns and (iii) low perturbation example generation. Results are visually grouped by shades of gray in Table 3.\n\nArchitecture. The DRAEM reconstructive sub-network impact on the downstream surface anomaly detection performance is evaluated by removing it from the pipeline and training the discriminative sub-network alone. The results are shown in Table 3, experiment Disc. Note a reduction in performance in comparison to the full DRAEM architecture ( Table 3, experiment DRAEM). The performance drop is due to overfitting of the discriminative sub-network to the simulated anomalies, which are not a faithful representation of the real ones.\n\nNext, the discriminative power of the reconstructive subnetwork alone is analyzed by evaluating it as an autoencoder-based surface anomaly detector. The reconstructed image output of the sub-network is compared to the input image using the SSIM function [27] to generate the anomaly map. The results of this approach are shown in Table 3, experiment Recon.-AE. Recon.-AE outperforms the recent auto-encoder-based surface anomaly detection method AE-SSIM [5] (see results in Table 2) This suggests that simulated anomaly training introduces additional information into the auto-encoder-based training, but judging by the performance gap to DRAEM, the SSIM similarity function may not be optimal for extraction of the anomaly information. Indeed, using the recently proposed similarity function MS-GMS [31] (Recon.-AE M SGM S ) improves the performance, but the results are still significantly worse than when using the entire DRAEM architecture, which indicates that both reconstructive and discriminative parts are required for optimal results.\n\nTo further emphasize the contribution of the DRAEM backbone, we replace it entirely by the recent state-of-theart supervised discriminative surface anomaly detection network [6] and re-train with the simulated anomalies (Table 3, Bo\u017ei\u010d et al.). Performance substantially drops, which further supports the power of learning the anomaly deviation extent from normality rather than the anomaly or normality appearance.\n\nAnomaly appearance patterns. DRAEM is re-trained using ImageNet [12] as the texture source in the anomaly simulator to study the influence of the anomaly generation dataset (DRAEM ImageN et in Table 3). Results are comparable to using the much smaller DTD [9] dataset. Figure 9 shows the performance at various anomaly source dataset sizes. Results suggest that the augmentation and opacity randomization substantially contribute to performance allowing remarkably small number of texture images (less than 10). As an extreme case, the anomaly textures are   Table 3. Surface anomaly detection (Det.) and localization (Loc.) experiments of the ablation study grouped by shades of gray into (i) method architecture, (ii) anomaly source dataset, (iii) hard simulated anomaly generation, (iv) simulated anomaly shape, and (v) the performance of DRAEM for reference. generated as homogeneous regions of a randomly sampled color (DRAEM color ). Note that DRAEM color still achieves state-of-the-art results, further suggesting that DRAEM does not require simulations to closely match the real anomalies. The impact of the anomaly shape generator is evaluated by replacing the Perlin noise generator by a rectangular region generator. The anomaly mask is thus generated by sampling multiple rectangular areas for the anomalous regions (DRAEM rect in Table 3). Training on rectangular anomalies causes only a slight performance drop and suggests that the simulated anomaly shape does not have to be realistic to generalize well to real world anomalies. Examples of anomalies generated in anomaly appearance ablation experiments are shown in Figure 10.\n\nLow perturbation examples. The anomaly source image augmentation and the opacity randomization are re-sponsible for tightening the decision boundary around the anomaly-free training distribution. Table 3 reports the results of DRAEM variants trained (i) without image augmentation and opacity randomization (DRAEM no aug ), (ii) using only image augmentation (DRAEM img aug ) and (iii) using only opacity randomization (DRAEM \u03b2 ). There is a significant localization performance gap between DRAEM no aug and DRAEM, however, this can be significantly narrowed by using the opacity randomization in training even without image data augmentation.\n\n\nComparison with supervised methods\n\nSupervised methods require anomaly annotations at training time and cannot be evaluated on MVTec. We thus compare DRAEM with the supervised methods on the DAGM dataset [28] that contains 10 textured object classes Figure 9. DRAEM achieves a remarkable detection and localization performance already at as low as 10 texture source images in the simulator when augmentation is applied. Figure 10. Anomalies simulated using the DTD [9] (DRAEM), ImageNet [12] (DRAEMImageNet), homogeneous color regions (DRAEM color ) and rectangular masks (DRAEMrect), from left to right. with small anomalies visually very similar to the background, which makes the dataset particularly challenging for the unsupervised methods.\n\nDRAEM is trained only on anomaly-free training samples using the same parameters as in previous experiments. The standard evaluation protocol on this dataset [19,32,15,6] is used -the challenge is to classify whether the image contains the anomaly; localization accuracy is not measured, since the anomalies are only coarsely labeled. Table 4 shows that the best fully supervised methods nearly perfectly classify anomalous images, while the stateof-the-art unsupervised methods like RIAD [31] and US [4] struggle with subtle anomalies on highly textured regions 1 . DRAEM significantly outperforms these methods, and even the weakly supervised CADN [32] by a large margin, obtaining classification performance close to the best fullysupervised methods, which is a remarkable result.\n\nFurthermore, DRAEM outperforms all supervised methods in terms of anomaly localization accuracy on this dataset. Since the training images are only coarsely annotated with ellipses that approximately cover the surface defects and contain background, the supervised methods produce inaccurate localization in test images as well. In contrast, DRAEM does not use the labels at all, and thus produces more accurate anomaly maps, as shown in Figure 11.  [19] 99.6 99.9 99.5 -Lin et al. [15] 99.0 99.4 99.9 -Bo\u017ei\u010d et al. [6] 100 100 100 100 Table 4. DRAEM outperforms unsupervised methods on DAGM dataset and performs on par with fully supervised ones. Figure 11. Supervised methods replicate the approximate ground truth training annotations, leading to a low localization accuracy. DRAEM does not use the ground truth, yet produces far better localization.\n\n\nConclusion\n\nA discriminative end-to-end trainable surface anomaly detection and localization method DRAEM was presented. DRAEM outperforms the current state-of-the-art on the MVTec dataset [3] by 2.5 AUROC points on the surface anomaly detection task and by 13.5 AP points on the localization task. On the DAGM dataset [28], DRAEM delivers anomalous image classification accuracy close to fully supervised methods, while outperforming them in localization accuracy. This is a remarkable result since DRAEM is not trained on real anomalies. In fact, a detailed analysis shows that our paradigm of learning a joint reconstruction-anomaly embedding through a reconstructive sub-network significantly improves the results over standard methods and that an accurate decision boundary can be well estimated by learning the extent of deviation from reconstruction on simple simulations rather than learning either the normality or real anomaly appearance.\n\nFigure 4 .\n4Simulated anomaly generation process. The binary anomaly mask Ma is generated from Perlin noise P . The anomalous regions are sampled from A according to Ma and placed on the anomaly free image I to generate the anomalous image Ia.\n\nFigure 5 .\n5The original anomaly source image (left) can be augmented several times (center) to generate a wide variety of simulated anomalous regions (right).\n\nFigure 6 .\n6The original image (a) contains anomalies which are difficult to mark in the ground truth mask (b) which causes a discrepancy between the ground truth and the output anomaly map (c,d).\n\nFigure 7 .\n7The anomalous images are shown in the first row. The middle three rows show the anomaly maps generated by our implementation of Uninformed Students[4], PaDim[11] and DRAEM, respectively. The last row shows the direct anomaly map output of DRAEM.\n\nFigure 8 .\n8Qualitative examples. The original image, the anomaly map overlay, the anomaly map and the ground truth map are shown.\nPlease see the supplementary material for additional qualitative results.\n\nGANomaly: Semi-supervised anomaly detection via adversarial training. Samet Akcay, Amir Atapour-Abarghouei, Toby P Breckon, Asian Conference on Computer Vision. SpringerSamet Akcay, Amir Atapour-Abarghouei, and Toby P Breckon. GANomaly: Semi-supervised anomaly detection via adversarial training. In Asian Conference on Computer Vision, pages 622-637. Springer, 2018. 1, 2, 3, 5\n\nSkip-GANomaly: Skip connected and adversarially trained encoder-decoder anomaly detection. Samet Ak\u00e7ay, Amir Atapour-Abarghouei, Toby P Breckon, 2019 International Joint Conference on Neural Networks (IJCNN). IEEE23Samet Ak\u00e7ay, Amir Atapour-Abarghouei, and Toby P Breckon. Skip-GANomaly: Skip connected and adversari- ally trained encoder-decoder anomaly detection. In 2019 In- ternational Joint Conference on Neural Networks (IJCNN), pages 1-8. IEEE, jul 2019. 1, 2, 3\n\nMVTec AD -A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition5Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. MVTec AD -A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9592-9600, 2019. 5, 8\n\nUninformed students: Student-teacher anomaly detection with discriminative latent embeddings. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition6Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4183-4192, 2020. 2, 5, 6, 8\n\nImproving unsupervised defect segmentation by applying structural similarity to autoencoders. Paul Bergmann, Sindy L\u00f6we, Michael Fauser, David Sattlegger, Carsten Steger, 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics theory and Applications. 56Paul Bergmann, Sindy L\u00f6we, Michael Fauser, David Sattleg- ger, and Carsten Steger. Improving unsupervised defect seg- mentation by applying structural similarity to autoencoders. In 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics theory and Applications, volume 5, pages 372-380, 2018. 1, 2, 3, 5, 6\n\nEnd-toend training of a two-stage neural network for defect detection. Jakob Bo\u017ei\u010d, Domen Tabernik, Danijel Sko\u010daj, 25th International Conference on Pattern Recognition ICPR. 7Jakob Bo\u017ei\u010d, Domen Tabernik, and Danijel Sko\u010daj. End-to- end training of a two-stage neural network for defect detec- tion. 25th International Conference on Pattern Recognition ICPR, 2020. 6, 7, 8\n\nAnomaly detection using one-class neural networks. Raghavendra Chalapathy, Aditya Krishna Menon, Sanjay Chawla, arXiv:1802.0636023arXiv preprintRaghavendra Chalapathy, Aditya Krishna Menon, and San- jay Chawla. Anomaly detection using one-class neural net- works. arXiv preprint arXiv:1802.06360, 2018. 2, 3\n\nRethinking atrous convolution for semantic image segmentation. Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam, arXiv:1706.05587arXiv preprintLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for seman- tic image segmentation. arXiv preprint arXiv:1706.05587, 2017. 2\n\nDescribing textures in the wild. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, Andrea Vedaldi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition6Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3606-3613, 2014. 5, 6, 8\n\nRandaugment: Practical automated data augmentation with a reduced search space. Barret Ekin D Cubuk, Jonathon Zoph, Quoc V Shlens, Le, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition WorkshopsEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmenta- tion with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702-703, 2020. 3\n\nPadim: a patch distribution modeling framework for anomaly detection and localization. Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier, 1st International Workshop on Industrial Machine Learning, ICPR 2020. 6Thomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier. Padim: a patch distribution modeling framework for anomaly detection and localization. In 1st In- ternational Workshop on Industrial Machine Learning, ICPR 2020, 2020. 2, 5, 6, 8\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee6Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. 6, 8\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014. 2\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision34Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017. 3, 4\n\nAn efficient network for surface defect detection. Zesheng Lin, Hongxia Ye, Bin Zhan, Xiaofeng Huang, Applied Sciences. 10176085Zesheng Lin, Hongxia Ye, Bin Zhan, and Xiaofeng Huang. An efficient network for surface defect detection. Applied Sciences, 10(17):6085, 2020. 8\n\nTowards visually explaining variational autoencoders. Wenqian Liu, Runze Li, Meng Zheng, Srikrishna Karanam, Ziyan Wu, Bir Bhanu, J Richard, Octavia Radke, Camps, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionWenqian Liu, Runze Li, Meng Zheng, Srikrishna Karanam, Ziyan Wu, Bir Bhanu, Richard J Radke, and Octavia Camps. Towards visually explaining variational autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 8642-8651, 2020. 3\n\nFuture frame prediction for anomaly detection-a new baseline. Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Fu- ture frame prediction for anomaly detection-a new baseline. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6536-6545, 2018. 5\n\nAn image synthesizer. Ken Perlin, ACM Siggraph Computer Graphics. 193Ken Perlin. An image synthesizer. ACM Siggraph Computer Graphics, 19(3):287-296, 1985. 3\n\nA compact convolutional neural network for textured surface anomaly detection. Domen Ra\u010dki, Dejan Toma\u017eevi\u010d, Danijel Sko\u010daj, 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). Domen Ra\u010dki, Dejan Toma\u017eevi\u010d, and Danijel Sko\u010daj. A compact convolutional neural network for textured surface anomaly detection. In 2018 IEEE Winter Conference on Ap- plications of Computer Vision (WACV), pages 1331-1339, 2018. 8\n\nModeling the distribution of normal data in pre-trained deep features for anomaly detection. Oliver Rippel, Patrick Mertens, Dorit Merhof, ICPR. 8Oliver Rippel, Patrick Mertens, and Dorit Merhof. Modeling the distribution of normal data in pre-trained deep features for anomaly detection. ICPR, 2020. 2, 5, 8\n\nUnet: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. Springer23Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical image com- puting and computer-assisted intervention, pages 234-241. Springer, 2015. 2, 3\n\nDeep one-class classification. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Ahmed Shoaib, Alexander Siddiqui, Emmanuel Binder, Marius M\u00fcller, Kloft, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine Learning803Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Em- manuel M\u00fcller, and Marius Kloft. Deep one-class classifi- cation. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 4393-4402, 2018. 2, 3\n\nf-anogan: Fast unsupervised anomaly detection with generative adversarial networks. Thomas Schlegl, Philipp Seeb\u00f6ck, Georg Sebastian M Waldstein, Ursula Langs, Schmidt-Erfurth, Medical image analysis. 542Thomas Schlegl, Philipp Seeb\u00f6ck, Sebastian M Waldstein, Georg Langs, and Ursula Schmidt-Erfurth. f-anogan: Fast unsupervised anomaly detection with generative adversarial networks. Medical image analysis, 54:30-44, 2019. 1, 2\n\nUnsupervised anomaly detection with generative adversarial networks to guide marker discovery. Thomas Schlegl, Philipp Seeb\u00f6ck, Ursula Sebastian M Waldstein, Georg Schmidt-Erfurth, Langs, International Conference on Information Processing in Medical Imaging. Springer15Thomas Schlegl, Philipp Seeb\u00f6ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International Conference on In- formation Processing in Medical Imaging, pages 146-157. Springer, 2017. 1, 2, 5\n\nUnsupervised anomaly segmentation via deep feature reconstruction. Yong Shi, Jie Yang, Zhiquan Qi, Neurocomputing. 4242Yong Shi, Jie Yang, and Zhiquan Qi. Unsupervised anomaly segmentation via deep feature reconstruction. Neurocomput- ing, 424:9-22, 2021. 2\n\nAnomaly detection neural network with dual auto-encoders gan and its industrial inspection applications. Ta-Wei Tang, Wei-Han Kuo, Chien-Fang Lan, Jauh-Hsiang Nad Ding, Hakiem Hsu, Hong-Tsu Young, 2020. 1Sensors. 20125Ta-Wei Tang, Wei-Han Kuo, Chien-Fang Lan, Jauh-Hsiang nad Ding, Hakiem Hsu, and Hong-Tsu Young. Anomaly de- tection neural network with dual auto-encoders gan and its industrial inspection applications. Sensors, 20(12), 2020. 1, 2, 5\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, IEEE transactions on image processing. 1346Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004. 3, 6\n\nWeakly supervised learning for industrial optical inspection. M Wieler, Hahn, 7M Wieler and T Hahn. Weakly supervised learning for in- dustrial optical inspection, 2007. 7, 8\n\nMirrored autoencoders with simplex interpolation for unsupervised anomaly detection. Yexin Wu, Yogesh Balaji, Bhanukiran Vinzamuri, Soheil Feizi, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020Yexin Wu, Yogesh Balaji, Bhanukiran Vinzamuri, and Soheil Feizi. Mirrored autoencoders with simplex interpolation for unsupervised anomaly detection. Proceedings of the Euro- pean Conference on Computer Vision (ECCV), 2020. 2\n\nPatch svdd: Patch-level svdd for anomaly detection and segmentation. Jihun Yi, Sungroh Yoon, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer Vision2020Jihun Yi and Sungroh Yoon. Patch svdd: Patch-level svdd for anomaly detection and segmentation. In Proceedings of the Asian Conference on Computer Vision, 2020. 3\n\nReconstruction by inpainting for visual anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Sko\u010daj, Pattern Recognition. 68Vitjan Zavrtanik, Matej Kristan, and Danijel Sko\u010daj. Recon- struction by inpainting for visual anomaly detection. Pattern Recognition, 2020. 2, 3, 5, 6, 8\n\nCadn: A weakly supervised learningbased category-aware object detection network for surface defect detection. Jiabin Zhang, Hu Su, Wei Zou, Xinyi Gong, Zhengtao Zhang, Fei Shen, Pattern Recognition. 1098107571Jiabin Zhang, Hu Su, Wei Zou, Xinyi Gong, Zhengtao Zhang, and Fei Shen. Cadn: A weakly supervised learning- based category-aware object detection network for surface defect detection. Pattern Recognition, 109:107571, 2021. 8\n", "annotations": {"author": "[{\"end\":210,\"start\":92},{\"end\":323,\"start\":211},{\"end\":438,\"start\":324}]", "publisher": null, "author_last_name": "[{\"end\":108,\"start\":99},{\"end\":224,\"start\":217},{\"end\":338,\"start\":332}]", "author_first_name": "[{\"end\":98,\"start\":92},{\"end\":216,\"start\":211},{\"end\":331,\"start\":324}]", "author_affiliation": "[{\"end\":209,\"start\":141},{\"end\":322,\"start\":254},{\"end\":437,\"start\":369}]", "title": "[{\"end\":89,\"start\":1},{\"end\":527,\"start\":439}]", "venue": null, "abstract": "[{\"end\":1931,\"start\":529}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3321,\"start\":3318},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3323,\"start\":3321},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3325,\"start\":3323},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3328,\"start\":3325},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3342,\"start\":3338},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3345,\"start\":3342},{\"end\":3676,\"start\":3668},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4448,\"start\":4445},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4582,\"start\":4578},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4584,\"start\":4582},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5028,\"start\":5025},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5031,\"start\":5028},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6797,\"start\":6794},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6799,\"start\":6797},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6801,\"start\":6799},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6804,\"start\":6801},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6807,\"start\":6804},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6810,\"start\":6807},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6813,\"start\":6810},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6875,\"start\":6872},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6883,\"start\":6880},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6885,\"start\":6883},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6888,\"start\":6885},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7109,\"start\":7105},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7112,\"start\":7109},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7123,\"start\":7119},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7197,\"start\":7193},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7487,\"start\":7483},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8028,\"start\":8025},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8031,\"start\":8028},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8231,\"start\":8227},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8234,\"start\":8231},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8501,\"start\":8497},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8804,\"start\":8800},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8847,\"start\":8843},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8849,\"start\":8847},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10457,\"start\":10454},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10459,\"start\":10457},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10559,\"start\":10555},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10595,\"start\":10592},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10598,\"start\":10595},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11335,\"start\":11331},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11826,\"start\":11822},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12154,\"start\":12150},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13009,\"start\":13005},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13371,\"start\":13367},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13854,\"start\":13850},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15798,\"start\":15795},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16396,\"start\":16393},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16994,\"start\":16991},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16997,\"start\":16994},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17000,\"start\":16997},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17003,\"start\":17000},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17656,\"start\":17653},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17990,\"start\":17987},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19988,\"start\":19985},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20003,\"start\":19999},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21688,\"start\":21684},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21887,\"start\":21884},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22234,\"start\":22230},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22653,\"start\":22650},{\"end\":22719,\"start\":22706},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22961,\"start\":22957},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23152,\"start\":23149},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25393,\"start\":25389},{\"end\":25443,\"start\":25435},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25653,\"start\":25650},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25676,\"start\":25672},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26094,\"start\":26090},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26097,\"start\":26094},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26100,\"start\":26097},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26102,\"start\":26100},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26425,\"start\":26421},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26436,\"start\":26433},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26586,\"start\":26582},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27171,\"start\":27167},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27203,\"start\":27199},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27236,\"start\":27233},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27896,\"start\":27892},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29289,\"start\":29286},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29300,\"start\":29296}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28766,\"start\":28522},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28927,\"start\":28767},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29125,\"start\":28928},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29384,\"start\":29126},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29516,\"start\":29385}]", "paragraph": "[{\"end\":2817,\"start\":1947},{\"end\":3271,\"start\":2819},{\"end\":4061,\"start\":3273},{\"end\":4281,\"start\":4063},{\"end\":5198,\"start\":4283},{\"end\":5655,\"start\":5200},{\"end\":6344,\"start\":5657},{\"end\":6651,\"start\":6346},{\"end\":7869,\"start\":6668},{\"end\":8693,\"start\":7871},{\"end\":9082,\"start\":8695},{\"end\":10010,\"start\":9092},{\"end\":10376,\"start\":10041},{\"end\":10599,\"start\":10378},{\"end\":10967,\"start\":10664},{\"end\":11258,\"start\":11024},{\"end\":12286,\"start\":11289},{\"end\":12421,\"start\":12288},{\"end\":12578,\"start\":12485},{\"end\":12948,\"start\":12611},{\"end\":13397,\"start\":12950},{\"end\":13987,\"start\":13420},{\"end\":14400,\"start\":13989},{\"end\":14779,\"start\":14446},{\"end\":14992,\"start\":14781},{\"end\":15275,\"start\":15039},{\"end\":15500,\"start\":15277},{\"end\":15801,\"start\":15531},{\"end\":16275,\"start\":15817},{\"end\":17554,\"start\":16316},{\"end\":18029,\"start\":17556},{\"end\":18384,\"start\":18031},{\"end\":20100,\"start\":18386},{\"end\":20621,\"start\":20102},{\"end\":20895,\"start\":20640},{\"end\":21428,\"start\":20897},{\"end\":22474,\"start\":21430},{\"end\":22891,\"start\":22476},{\"end\":24537,\"start\":22893},{\"end\":25182,\"start\":24539},{\"end\":25930,\"start\":25221},{\"end\":26715,\"start\":25932},{\"end\":27570,\"start\":26717},{\"end\":28521,\"start\":27585}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10663,\"start\":10600},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11023,\"start\":10968},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12484,\"start\":12422},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14445,\"start\":14401},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15530,\"start\":15501}]", "table_ref": "[{\"end\":18420,\"start\":18413},{\"end\":19517,\"start\":19510},{\"end\":20894,\"start\":20887},{\"end\":21140,\"start\":21133},{\"end\":21245,\"start\":21238},{\"end\":21767,\"start\":21760},{\"end\":21911,\"start\":21904},{\"end\":22704,\"start\":22696},{\"end\":23093,\"start\":23086},{\"end\":23459,\"start\":23452},{\"end\":24244,\"start\":24237},{\"end\":24742,\"start\":24735},{\"end\":26274,\"start\":26267},{\"end\":27260,\"start\":27253}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1945,\"start\":1933},{\"attributes\":{\"n\":\"2.\"},\"end\":6666,\"start\":6654},{\"attributes\":{\"n\":\"3.\"},\"end\":9090,\"start\":9085},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10039,\"start\":10013},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11287,\"start\":11261},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12609,\"start\":12581},{\"end\":13418,\"start\":13400},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15037,\"start\":14995},{\"attributes\":{\"n\":\"4.\"},\"end\":15815,\"start\":15804},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16314,\"start\":16278},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20638,\"start\":20624},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25219,\"start\":25185},{\"attributes\":{\"n\":\"5.\"},\"end\":27583,\"start\":27573},{\"end\":28533,\"start\":28523},{\"end\":28778,\"start\":28768},{\"end\":28939,\"start\":28929},{\"end\":29137,\"start\":29127},{\"end\":29396,\"start\":29386}]", "table": null, "figure_caption": "[{\"end\":28766,\"start\":28535},{\"end\":28927,\"start\":28780},{\"end\":29125,\"start\":28941},{\"end\":29384,\"start\":29139},{\"end\":29516,\"start\":29398}]", "figure_ref": "[{\"end\":2061,\"start\":2051},{\"end\":2421,\"start\":2413},{\"end\":4270,\"start\":4262},{\"end\":5185,\"start\":5176},{\"end\":5645,\"start\":5637},{\"end\":6058,\"start\":6048},{\"end\":6342,\"start\":6334},{\"end\":9254,\"start\":9246},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13058,\"start\":13049},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13130,\"start\":13122},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13311,\"start\":13298},{\"end\":13482,\"start\":13474},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14777,\"start\":14769},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16806,\"start\":16798},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18098,\"start\":18090},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19231,\"start\":19223},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20024,\"start\":20016},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20284,\"start\":20276},{\"end\":23170,\"start\":23162},{\"end\":24536,\"start\":24527},{\"end\":25614,\"start\":25605},{\"end\":27164,\"start\":27155},{\"end\":27374,\"start\":27365}]", "bib_author_first_name": "[{\"end\":29667,\"start\":29662},{\"end\":29679,\"start\":29675},{\"end\":29704,\"start\":29700},{\"end\":29706,\"start\":29705},{\"end\":30068,\"start\":30063},{\"end\":30080,\"start\":30076},{\"end\":30105,\"start\":30101},{\"end\":30107,\"start\":30106},{\"end\":30528,\"start\":30524},{\"end\":30546,\"start\":30539},{\"end\":30560,\"start\":30555},{\"end\":30580,\"start\":30573},{\"end\":31092,\"start\":31088},{\"end\":31110,\"start\":31103},{\"end\":31124,\"start\":31119},{\"end\":31144,\"start\":31137},{\"end\":31687,\"start\":31683},{\"end\":31703,\"start\":31698},{\"end\":31717,\"start\":31710},{\"end\":31731,\"start\":31726},{\"end\":31751,\"start\":31744},{\"end\":32288,\"start\":32283},{\"end\":32301,\"start\":32296},{\"end\":32319,\"start\":32312},{\"end\":32648,\"start\":32637},{\"end\":32667,\"start\":32661},{\"end\":32675,\"start\":32668},{\"end\":32689,\"start\":32683},{\"end\":32969,\"start\":32958},{\"end\":32982,\"start\":32976},{\"end\":33002,\"start\":32995},{\"end\":33019,\"start\":33012},{\"end\":33274,\"start\":33268},{\"end\":33292,\"start\":33283},{\"end\":33306,\"start\":33299},{\"end\":33322,\"start\":33317},{\"end\":33338,\"start\":33332},{\"end\":33807,\"start\":33801},{\"end\":33830,\"start\":33822},{\"end\":33843,\"start\":33837},{\"end\":34379,\"start\":34373},{\"end\":34397,\"start\":34388},{\"end\":34415,\"start\":34406},{\"end\":34431,\"start\":34424},{\"end\":34822,\"start\":34819},{\"end\":34832,\"start\":34829},{\"end\":34846,\"start\":34839},{\"end\":34861,\"start\":34855},{\"end\":34869,\"start\":34866},{\"end\":34876,\"start\":34874},{\"end\":35213,\"start\":35210},{\"end\":35230,\"start\":35226},{\"end\":35251,\"start\":35246},{\"end\":35263,\"start\":35259},{\"end\":35273,\"start\":35268},{\"end\":35295,\"start\":35288},{\"end\":35308,\"start\":35303},{\"end\":35326,\"start\":35320},{\"end\":35701,\"start\":35693},{\"end\":35712,\"start\":35707},{\"end\":35724,\"start\":35720},{\"end\":36130,\"start\":36123},{\"end\":36143,\"start\":36136},{\"end\":36151,\"start\":36148},{\"end\":36166,\"start\":36158},{\"end\":36407,\"start\":36400},{\"end\":36418,\"start\":36413},{\"end\":36427,\"start\":36423},{\"end\":36445,\"start\":36435},{\"end\":36460,\"start\":36455},{\"end\":36468,\"start\":36465},{\"end\":36477,\"start\":36476},{\"end\":36494,\"start\":36487},{\"end\":37003,\"start\":37000},{\"end\":37015,\"start\":37009},{\"end\":37027,\"start\":37021},{\"end\":37042,\"start\":37034},{\"end\":37438,\"start\":37435},{\"end\":37656,\"start\":37651},{\"end\":37669,\"start\":37664},{\"end\":37688,\"start\":37681},{\"end\":38098,\"start\":38092},{\"end\":38114,\"start\":38107},{\"end\":38129,\"start\":38124},{\"end\":38377,\"start\":38373},{\"end\":38398,\"start\":38391},{\"end\":38414,\"start\":38408},{\"end\":38805,\"start\":38800},{\"end\":38818,\"start\":38812},{\"end\":38837,\"start\":38833},{\"end\":38853,\"start\":38848},{\"end\":38867,\"start\":38862},{\"end\":38885,\"start\":38876},{\"end\":38904,\"start\":38896},{\"end\":38919,\"start\":38913},{\"end\":39437,\"start\":39431},{\"end\":39454,\"start\":39447},{\"end\":39469,\"start\":39464},{\"end\":39499,\"start\":39493},{\"end\":39879,\"start\":39873},{\"end\":39896,\"start\":39889},{\"end\":39912,\"start\":39906},{\"end\":39941,\"start\":39936},{\"end\":40426,\"start\":40422},{\"end\":40435,\"start\":40432},{\"end\":40449,\"start\":40442},{\"end\":40725,\"start\":40719},{\"end\":40739,\"start\":40732},{\"end\":40755,\"start\":40745},{\"end\":40772,\"start\":40761},{\"end\":40789,\"start\":40783},{\"end\":40803,\"start\":40795},{\"end\":41145,\"start\":41141},{\"end\":41156,\"start\":41152},{\"end\":41158,\"start\":41157},{\"end\":41167,\"start\":41166},{\"end\":41181,\"start\":41175},{\"end\":41514,\"start\":41513},{\"end\":41717,\"start\":41712},{\"end\":41728,\"start\":41722},{\"end\":41747,\"start\":41737},{\"end\":41765,\"start\":41759},{\"end\":42193,\"start\":42188},{\"end\":42205,\"start\":42198},{\"end\":42540,\"start\":42534},{\"end\":42557,\"start\":42552},{\"end\":42574,\"start\":42567},{\"end\":42878,\"start\":42872},{\"end\":42888,\"start\":42886},{\"end\":42896,\"start\":42893},{\"end\":42907,\"start\":42902},{\"end\":42922,\"start\":42914},{\"end\":42933,\"start\":42930}]", "bib_author_last_name": "[{\"end\":29673,\"start\":29668},{\"end\":29698,\"start\":29680},{\"end\":29714,\"start\":29707},{\"end\":30074,\"start\":30069},{\"end\":30099,\"start\":30081},{\"end\":30115,\"start\":30108},{\"end\":30537,\"start\":30529},{\"end\":30553,\"start\":30547},{\"end\":30571,\"start\":30561},{\"end\":30587,\"start\":30581},{\"end\":31101,\"start\":31093},{\"end\":31117,\"start\":31111},{\"end\":31135,\"start\":31125},{\"end\":31151,\"start\":31145},{\"end\":31696,\"start\":31688},{\"end\":31708,\"start\":31704},{\"end\":31724,\"start\":31718},{\"end\":31742,\"start\":31732},{\"end\":31758,\"start\":31752},{\"end\":32294,\"start\":32289},{\"end\":32310,\"start\":32302},{\"end\":32326,\"start\":32320},{\"end\":32659,\"start\":32649},{\"end\":32681,\"start\":32676},{\"end\":32696,\"start\":32690},{\"end\":32974,\"start\":32970},{\"end\":32993,\"start\":32983},{\"end\":33010,\"start\":33003},{\"end\":33024,\"start\":33020},{\"end\":33281,\"start\":33275},{\"end\":33297,\"start\":33293},{\"end\":33315,\"start\":33307},{\"end\":33330,\"start\":33323},{\"end\":33346,\"start\":33339},{\"end\":33820,\"start\":33808},{\"end\":33835,\"start\":33831},{\"end\":33850,\"start\":33844},{\"end\":33854,\"start\":33852},{\"end\":34386,\"start\":34380},{\"end\":34404,\"start\":34398},{\"end\":34422,\"start\":34416},{\"end\":34440,\"start\":34432},{\"end\":34827,\"start\":34823},{\"end\":34837,\"start\":34833},{\"end\":34853,\"start\":34847},{\"end\":34864,\"start\":34862},{\"end\":34872,\"start\":34870},{\"end\":34884,\"start\":34877},{\"end\":35224,\"start\":35214},{\"end\":35244,\"start\":35231},{\"end\":35257,\"start\":35252},{\"end\":35266,\"start\":35264},{\"end\":35286,\"start\":35274},{\"end\":35301,\"start\":35296},{\"end\":35318,\"start\":35309},{\"end\":35333,\"start\":35327},{\"end\":35705,\"start\":35702},{\"end\":35718,\"start\":35713},{\"end\":35733,\"start\":35725},{\"end\":36134,\"start\":36131},{\"end\":36146,\"start\":36144},{\"end\":36156,\"start\":36152},{\"end\":36172,\"start\":36167},{\"end\":36411,\"start\":36408},{\"end\":36421,\"start\":36419},{\"end\":36433,\"start\":36428},{\"end\":36453,\"start\":36446},{\"end\":36463,\"start\":36461},{\"end\":36474,\"start\":36469},{\"end\":36485,\"start\":36478},{\"end\":36500,\"start\":36495},{\"end\":36507,\"start\":36502},{\"end\":37007,\"start\":37004},{\"end\":37019,\"start\":37016},{\"end\":37032,\"start\":37028},{\"end\":37046,\"start\":37043},{\"end\":37445,\"start\":37439},{\"end\":37662,\"start\":37657},{\"end\":37679,\"start\":37670},{\"end\":37695,\"start\":37689},{\"end\":38105,\"start\":38099},{\"end\":38122,\"start\":38115},{\"end\":38136,\"start\":38130},{\"end\":38389,\"start\":38378},{\"end\":38406,\"start\":38399},{\"end\":38419,\"start\":38415},{\"end\":38810,\"start\":38806},{\"end\":38831,\"start\":38819},{\"end\":38846,\"start\":38838},{\"end\":38860,\"start\":38854},{\"end\":38874,\"start\":38868},{\"end\":38894,\"start\":38886},{\"end\":38911,\"start\":38905},{\"end\":38926,\"start\":38920},{\"end\":38933,\"start\":38928},{\"end\":39445,\"start\":39438},{\"end\":39462,\"start\":39455},{\"end\":39491,\"start\":39470},{\"end\":39505,\"start\":39500},{\"end\":39522,\"start\":39507},{\"end\":39887,\"start\":39880},{\"end\":39904,\"start\":39897},{\"end\":39934,\"start\":39913},{\"end\":39957,\"start\":39942},{\"end\":39964,\"start\":39959},{\"end\":40430,\"start\":40427},{\"end\":40440,\"start\":40436},{\"end\":40452,\"start\":40450},{\"end\":40730,\"start\":40726},{\"end\":40743,\"start\":40740},{\"end\":40759,\"start\":40756},{\"end\":40781,\"start\":40773},{\"end\":40793,\"start\":40790},{\"end\":40809,\"start\":40804},{\"end\":41150,\"start\":41146},{\"end\":41164,\"start\":41159},{\"end\":41173,\"start\":41168},{\"end\":41188,\"start\":41182},{\"end\":41200,\"start\":41190},{\"end\":41521,\"start\":41515},{\"end\":41527,\"start\":41523},{\"end\":41720,\"start\":41718},{\"end\":41735,\"start\":41729},{\"end\":41757,\"start\":41748},{\"end\":41771,\"start\":41766},{\"end\":42196,\"start\":42194},{\"end\":42210,\"start\":42206},{\"end\":42550,\"start\":42541},{\"end\":42565,\"start\":42558},{\"end\":42581,\"start\":42575},{\"end\":42884,\"start\":42879},{\"end\":42891,\"start\":42889},{\"end\":42900,\"start\":42897},{\"end\":42912,\"start\":42908},{\"end\":42928,\"start\":42923},{\"end\":42938,\"start\":42934}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":21688963},\"end\":29970,\"start\":29592},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":59291942},\"end\":30441,\"start\":29972},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":189857704},\"end\":30992,\"start\":30443},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207880670},\"end\":31587,\"start\":30994},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49567058},\"end\":32210,\"start\":31589},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":220525605},\"end\":32584,\"start\":32212},{\"attributes\":{\"doi\":\"arXiv:1802.06360\",\"id\":\"b6\"},\"end\":32893,\"start\":32586},{\"attributes\":{\"doi\":\"arXiv:1706.05587\",\"id\":\"b7\"},\"end\":33233,\"start\":32895},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4309276},\"end\":33719,\"start\":33235},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":208006202},\"end\":34284,\"start\":33721},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":226976039},\"end\":34764,\"start\":34286},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":57246310},\"end\":35179,\"start\":34766},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1033682},\"end\":35622,\"start\":35181},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206771220},\"end\":36070,\"start\":35624},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":225210708},\"end\":36344,\"start\":36072},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":208139191},\"end\":36936,\"start\":36346},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3865699},\"end\":37411,\"start\":36938},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":62267694},\"end\":37570,\"start\":37413},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":19133374},\"end\":37997,\"start\":37572},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":218971560},\"end\":38307,\"start\":37999},{\"attributes\":{\"id\":\"b20\"},\"end\":38767,\"start\":38309},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":49312162},\"end\":39345,\"start\":38769},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":73516151},\"end\":39776,\"start\":39347},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17427022},\"end\":40353,\"start\":39778},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":229450862},\"end\":40612,\"start\":40355},{\"attributes\":{\"doi\":\"2020. 1\",\"id\":\"b25\",\"matched_paper_id\":219726774},\"end\":41065,\"start\":40614},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":207761262},\"end\":41449,\"start\":41067},{\"attributes\":{\"id\":\"b27\"},\"end\":41625,\"start\":41451},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":214623198},\"end\":42117,\"start\":41627},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":220250825},\"end\":42473,\"start\":42119},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":225114154},\"end\":42760,\"start\":42475},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":224805840},\"end\":43195,\"start\":42762}]", "bib_title": "[{\"end\":29660,\"start\":29592},{\"end\":30061,\"start\":29972},{\"end\":30522,\"start\":30443},{\"end\":31086,\"start\":30994},{\"end\":31681,\"start\":31589},{\"end\":32281,\"start\":32212},{\"end\":33266,\"start\":33235},{\"end\":33799,\"start\":33721},{\"end\":34371,\"start\":34286},{\"end\":34817,\"start\":34766},{\"end\":35208,\"start\":35181},{\"end\":35691,\"start\":35624},{\"end\":36121,\"start\":36072},{\"end\":36398,\"start\":36346},{\"end\":36998,\"start\":36938},{\"end\":37433,\"start\":37413},{\"end\":37649,\"start\":37572},{\"end\":38090,\"start\":37999},{\"end\":38371,\"start\":38309},{\"end\":38798,\"start\":38769},{\"end\":39429,\"start\":39347},{\"end\":39871,\"start\":39778},{\"end\":40420,\"start\":40355},{\"end\":40717,\"start\":40614},{\"end\":41139,\"start\":41067},{\"end\":41710,\"start\":41627},{\"end\":42186,\"start\":42119},{\"end\":42532,\"start\":42475},{\"end\":42870,\"start\":42762}]", "bib_author": "[{\"end\":29675,\"start\":29662},{\"end\":29700,\"start\":29675},{\"end\":29716,\"start\":29700},{\"end\":30076,\"start\":30063},{\"end\":30101,\"start\":30076},{\"end\":30117,\"start\":30101},{\"end\":30539,\"start\":30524},{\"end\":30555,\"start\":30539},{\"end\":30573,\"start\":30555},{\"end\":30589,\"start\":30573},{\"end\":31103,\"start\":31088},{\"end\":31119,\"start\":31103},{\"end\":31137,\"start\":31119},{\"end\":31153,\"start\":31137},{\"end\":31698,\"start\":31683},{\"end\":31710,\"start\":31698},{\"end\":31726,\"start\":31710},{\"end\":31744,\"start\":31726},{\"end\":31760,\"start\":31744},{\"end\":32296,\"start\":32283},{\"end\":32312,\"start\":32296},{\"end\":32328,\"start\":32312},{\"end\":32661,\"start\":32637},{\"end\":32683,\"start\":32661},{\"end\":32698,\"start\":32683},{\"end\":32976,\"start\":32958},{\"end\":32995,\"start\":32976},{\"end\":33012,\"start\":32995},{\"end\":33026,\"start\":33012},{\"end\":33283,\"start\":33268},{\"end\":33299,\"start\":33283},{\"end\":33317,\"start\":33299},{\"end\":33332,\"start\":33317},{\"end\":33348,\"start\":33332},{\"end\":33822,\"start\":33801},{\"end\":33837,\"start\":33822},{\"end\":33852,\"start\":33837},{\"end\":33856,\"start\":33852},{\"end\":34388,\"start\":34373},{\"end\":34406,\"start\":34388},{\"end\":34424,\"start\":34406},{\"end\":34442,\"start\":34424},{\"end\":34829,\"start\":34819},{\"end\":34839,\"start\":34829},{\"end\":34855,\"start\":34839},{\"end\":34866,\"start\":34855},{\"end\":34874,\"start\":34866},{\"end\":34886,\"start\":34874},{\"end\":35226,\"start\":35210},{\"end\":35246,\"start\":35226},{\"end\":35259,\"start\":35246},{\"end\":35268,\"start\":35259},{\"end\":35288,\"start\":35268},{\"end\":35303,\"start\":35288},{\"end\":35320,\"start\":35303},{\"end\":35335,\"start\":35320},{\"end\":35707,\"start\":35693},{\"end\":35720,\"start\":35707},{\"end\":35735,\"start\":35720},{\"end\":36136,\"start\":36123},{\"end\":36148,\"start\":36136},{\"end\":36158,\"start\":36148},{\"end\":36174,\"start\":36158},{\"end\":36413,\"start\":36400},{\"end\":36423,\"start\":36413},{\"end\":36435,\"start\":36423},{\"end\":36455,\"start\":36435},{\"end\":36465,\"start\":36455},{\"end\":36476,\"start\":36465},{\"end\":36487,\"start\":36476},{\"end\":36502,\"start\":36487},{\"end\":36509,\"start\":36502},{\"end\":37009,\"start\":37000},{\"end\":37021,\"start\":37009},{\"end\":37034,\"start\":37021},{\"end\":37048,\"start\":37034},{\"end\":37447,\"start\":37435},{\"end\":37664,\"start\":37651},{\"end\":37681,\"start\":37664},{\"end\":37697,\"start\":37681},{\"end\":38107,\"start\":38092},{\"end\":38124,\"start\":38107},{\"end\":38138,\"start\":38124},{\"end\":38391,\"start\":38373},{\"end\":38408,\"start\":38391},{\"end\":38421,\"start\":38408},{\"end\":38812,\"start\":38800},{\"end\":38833,\"start\":38812},{\"end\":38848,\"start\":38833},{\"end\":38862,\"start\":38848},{\"end\":38876,\"start\":38862},{\"end\":38896,\"start\":38876},{\"end\":38913,\"start\":38896},{\"end\":38928,\"start\":38913},{\"end\":38935,\"start\":38928},{\"end\":39447,\"start\":39431},{\"end\":39464,\"start\":39447},{\"end\":39493,\"start\":39464},{\"end\":39507,\"start\":39493},{\"end\":39524,\"start\":39507},{\"end\":39889,\"start\":39873},{\"end\":39906,\"start\":39889},{\"end\":39936,\"start\":39906},{\"end\":39959,\"start\":39936},{\"end\":39966,\"start\":39959},{\"end\":40432,\"start\":40422},{\"end\":40442,\"start\":40432},{\"end\":40454,\"start\":40442},{\"end\":40732,\"start\":40719},{\"end\":40745,\"start\":40732},{\"end\":40761,\"start\":40745},{\"end\":40783,\"start\":40761},{\"end\":40795,\"start\":40783},{\"end\":40811,\"start\":40795},{\"end\":41152,\"start\":41141},{\"end\":41166,\"start\":41152},{\"end\":41175,\"start\":41166},{\"end\":41190,\"start\":41175},{\"end\":41202,\"start\":41190},{\"end\":41523,\"start\":41513},{\"end\":41529,\"start\":41523},{\"end\":41722,\"start\":41712},{\"end\":41737,\"start\":41722},{\"end\":41759,\"start\":41737},{\"end\":41773,\"start\":41759},{\"end\":42198,\"start\":42188},{\"end\":42212,\"start\":42198},{\"end\":42552,\"start\":42534},{\"end\":42567,\"start\":42552},{\"end\":42583,\"start\":42567},{\"end\":42886,\"start\":42872},{\"end\":42893,\"start\":42886},{\"end\":42902,\"start\":42893},{\"end\":42914,\"start\":42902},{\"end\":42930,\"start\":42914},{\"end\":42940,\"start\":42930}]", "bib_venue": "[{\"end\":30730,\"start\":30668},{\"end\":31302,\"start\":31236},{\"end\":33489,\"start\":33427},{\"end\":34025,\"start\":33949},{\"end\":35856,\"start\":35804},{\"end\":36658,\"start\":36592},{\"end\":37189,\"start\":37127},{\"end\":39058,\"start\":39005},{\"end\":41888,\"start\":41839},{\"end\":42307,\"start\":42268},{\"end\":29751,\"start\":29716},{\"end\":30179,\"start\":30117},{\"end\":30666,\"start\":30589},{\"end\":31234,\"start\":31153},{\"end\":31869,\"start\":31760},{\"end\":32385,\"start\":32328},{\"end\":32635,\"start\":32586},{\"end\":32956,\"start\":32895},{\"end\":33425,\"start\":33348},{\"end\":33947,\"start\":33856},{\"end\":34510,\"start\":34442},{\"end\":34949,\"start\":34886},{\"end\":35384,\"start\":35335},{\"end\":35802,\"start\":35735},{\"end\":36190,\"start\":36174},{\"end\":36590,\"start\":36509},{\"end\":37125,\"start\":37048},{\"end\":37477,\"start\":37447},{\"end\":37766,\"start\":37697},{\"end\":38142,\"start\":38138},{\"end\":38507,\"start\":38421},{\"end\":39003,\"start\":38935},{\"end\":39546,\"start\":39524},{\"end\":40035,\"start\":39966},{\"end\":40468,\"start\":40454},{\"end\":40825,\"start\":40818},{\"end\":41239,\"start\":41202},{\"end\":41511,\"start\":41451},{\"end\":41837,\"start\":41773},{\"end\":42266,\"start\":42212},{\"end\":42602,\"start\":42583},{\"end\":42959,\"start\":42940}]"}}}, "year": 2023, "month": 12, "day": 17}