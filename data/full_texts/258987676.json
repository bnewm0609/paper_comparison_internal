{"id": 258987676, "updated": "2023-10-21 15:12:16.727", "metadata": {"title": "PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions", "authors": "[{\"first\":\"Guanghou\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yongmao\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Lei\",\"middle\":[]},{\"first\":\"Yunlin\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Rui\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zhifei\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Xie\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Style transfer TTS has shown impressive performance in recent years. However, style control is often restricted to systems built on expressive speech recordings with discrete style categories. In practical situations, users may be interested in transferring style by typing text descriptions of desired styles, without the reference speech in the target style. The text-guided content generation techniques have drawn wide attention recently. In this work, we explore the possibility of controllable style transfer with natural language descriptions. To this end, we propose PromptStyle, a text prompt-guided cross-speaker style transfer system. Specifically, PromptStyle consists of an improved VITS and a cross-modal style encoder. The cross-modal style encoder constructs a shared space of stylistic and semantic representation through a two-stage training process. Experiments show that PromptStyle can achieve proper style transfer with text prompts while maintaining relatively high stability and speaker similarity. Audio samples are available in our demo page.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2305.19522", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-19522", "doi": "10.21437/interspeech.2023-1779"}}, "content": {"source": {"pdf_hash": "eaf67634116e407774be85a6abcaee491be4ca3a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.19522v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bc113f18dc0c721e0c4e4f5b7562df8bec6c28d9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/eaf67634116e407774be85a6abcaee491be4ca3a.txt", "contents": "\nPromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions\n\n\nGuanghou Liu ghliu@mail.nwpu.edu.cn \nAudio, Speech and Language Processing Group (ASLP@NPU) School of Computer Science\nNorthwestern Polytechnical University\nXi'anChina\n\nYongmao Zhang \nAudio, Speech and Language Processing Group (ASLP@NPU) School of Computer Science\nNorthwestern Polytechnical University\nXi'anChina\n\nYi Lei \nAudio, Speech and Language Processing Group (ASLP@NPU) School of Computer Science\nNorthwestern Polytechnical University\nXi'anChina\n\nYunlin Chen yunlinchen@mobvoi.com \nShanghai Mobvoi Information Technology Co\nLtd\n\nRui Wang \nShanghai Mobvoi Information Technology Co\nLtd\n\nZhifei Li \nLei Xie lxie@nwpu.edu.cn \nAudio, Speech and Language Processing Group (ASLP@NPU) School of Computer Science\nNorthwestern Polytechnical University\nXi'anChina\n\nShanghai Mobvoi Information Technology Co\nLtd\n\nPromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions\nIndex Terms: text-to-speechstyle transferstyle prompt\nStyle transfer TTS has shown impressive performance in recent years. However, style control is often restricted to systems built on expressive speech recordings with discrete style categories. In practical situations, users may be interested in transferring style by typing text descriptions of desired styles, without the reference speech in the target style. The text-guided content generation techniques have drawn wide attention recently. In this work, we explore the possibility of controllable style transfer with natural language descriptions. To this end, we propose PromptStyle, a text prompt-guided cross-speaker style transfer system. Specifically, PromptStyle consists of an improved VITS and a cross-modal style encoder. The crossmodal style encoder constructs a shared space of stylistic and semantic representation through a two-stage training process. Experiments show that PromptStyle can achieve proper style transfer with text prompts while maintaining relatively high stability and speaker similarity. Audio samples are available in our demo page 1 .\n\nIntroduction\n\nText-to-speech (TTS) [1,2] aims to produce human-like speech from input text. Recent progress in deep learning approaches has greatly improved the naturalness of speech [3]. With the wide applications of TTS in real-world human-computer interaction, expressive TTS with diverse styles attracts more attention. Generating stylistic speech for a specific speaker intuitively needs the same speaker's high-quality expressive speech recordings, which incurs a high cost for data collection. To solve the problem of synthesizing expressive speech for the target speaker without diverse speaking styles, cross-speaker style transfer [4,5,6,7,8] is a feasible solution.\n\nFor the style representations in style transfer scenarios, existing works mainly include two different methods, i.e. the predefined style id category index [9,10,11,12] and hidden variables extracted from the reference signal [13,14,15,16,17]. However, the id-based methods are limited to the styles of fixed discrete categories, which leads to less flexibility. Although the reference-based methods can produce various speech through different references, the extracted style representation is not interpretable. Moreover, in practical applications, it is difficult to accurately and conveniently select an appropriate reference for arbitrary textual content.\n\nWith the success of text and image generation from prompt descriptions [18,19], some prompt-based TTS methods are pro-posed to improve the expressiveness and naturalness of synthetic speech. Style-Tagging-TTS (ST-TTS) [20] produces expressive speech based on style tags, which are stylistic words or phrases labeled from audiobook datasets. But it is difficult to describe complex styles by a single word or phrase, and the style tags are hard to label in common audiobook datasets as most utterances may be recorded in a less-expressive reading style. PromptTTS [21] proposes to use a style prompt from five different factors (i.e. gender, pitch, speaking speed, volume, and emotion) to guide the style expression for the generated speech. The recent InstructTTS [22] can synthesize stylistic speech with the guidance of natural language descriptions without formal constraints as style prompts. It's a three-stage training approach to capture semantic information from natural language style prompts as conditioning to the TTS system. Intuitively, natural language description is a convenient and userfriendly way to describe the desired style since no prior acoustic knowledge is required.\n\nIn this study, we focus on style transfer in the audiobook generation, where the target speaker has little expressive data and no style description prompts. Through other expressive data and transferring diversified styles with natural language descriptions, expressive audiobook speech can be generated for the target speaker. Specifically, this paper proposes to leverage natural language description prompts to transfer style from the source speaker to the target speaker who has no expressive speech data. The proposed approach -PromptStyle -has a two-stage procedure utilizing text prompts to model the style appearance. In the first stage, based on an improved VITS [23], we use a style encoder to extract a style hidden representation from reference speech as the condition of the TTS system. Multi-speaker multistyle expressive data without style annotation is involved in this stage to achieve cross-speaker style transfer through diverse reference speech. In the second stage, we design a prompt encoder to model the style embedding from the style prompt. Expressive speech data with style annotations in natural language descriptions is involved to fine-tune the pre-trained language model and TTS acoustic model, capturing the relationship between prompt embedding and style embedding space. Due to the generalization capability of the language model, style transfer from unseen prompts is also feasible.\n\nWe summarize the contributions of PromptStyle as follows.\n\n\u2022 We propose a two-stage TTS approach for cross-speaker style transfer with natural language descriptions, which is more user-friendly and controllable than previous works. \u2022 The proposed two-stage approach first uses a large amount of data without annotations to train a reference-based style transfer TTS model, and then leverages only a small amount of labeled data with style prompts to fine-tune a prompt en- \n\n\nStyle Encoder\n\n\nMulti-speaker and multi-style VITS\n\n\nA\n\n\nMonotonic Alignment Search\n\nText Encoder Experiments show that our proposed method can conduct cross-speaker style transfer with flexible text prompts and achieve high speech quality.\n\u03bc \u03b8 \u03c3 \u03b8 f \u03b8 (z)\n\nPromptStyle\n\n\nModel Overview\n\nThe overall architecture of the model we proposed is shown in Fig 1. The proposed model consists of three parts, including a style encoder, a prompt encoder, and a TTS system. Our TTS framework is based on VITS[x], an end-to-end TTS model that directly converts the phoneme sequence to the speech waveform. VITS can generate high-quality speech attributed to its end-to-end learning manner without the possible mismatch between the acoustic model and the vocoder in the conventional paradigm. The style encoder is used to extract style information from reference speech and the prompt encoder is used to extract semantic information from natural language descriptions.\n\nBased on the VITS framework, we propose a two-stage training procedure to achieve style transfer with text prompts. In the first stage, multi-style expressive data with speaker annotations are used to train a style transfer system based on VITS with the style encoder, aiming to learn a style embedding space. In the second stage, we use expressive speech with natural language description prompts to fine-tune the whole model to construct the relationship between the semantic embedding space and the style embedding space, so that we can achieve style transfer with text prompts.\n\n\nCross-modal Style Encoder\n\nWe introduce a cross-modal style encoder consisting of a style encoder and a prompt encoder for constructing a shared space of stylistic and semantic representation.\n\nThe style encoder is used to extract the style information from Mel-spectrograms but not linear spectrograms, because linear spectrograms contain too much information which may prevent capturing the style information accurately. The style encoder consists of a reference encoder and an auxiliary adversarial component. The reference encoder almost has a similar structure to the Global Style Tokens (GSTs) [24] discarding style token. The auxiliary adversarial component consists of a gradient reversal layer, feed-forward layers, and a speaker classifier, to disentangle the speaker and style information. The style encoder aims to construct a style embedding space for style transfer from large amounts of expressive speech without style annotations.\n\nA prompt encoder is used to build a prompt embedding space for controllable style transfer with natural language descriptions. The prompt encoder consists of a pre-trained BERT [25] and an adapter layer. First, the text prompt is fed into the BERT model to extract semantic features. Then the adapter layer converts the semantic features into the prompt embedding of the same size as style embedding from the style encoder, and captures the relationship between the prompt embedding and the style embedding space.\n\n\nTraining and Inference\n\nWe introduce a two-stage training procedure to obtain a style transfer model with text prompts. This bi-modal training approach has the following two advantages. 1) By establishing the relationship between the style and prompt spaces, either reference speech or text descriptions can be used to control the style transfer.\n\n2) The type and scale of the annotated data can be determined specifically according to the actual domain requirements, and the fine-tuning process is time-saving.\n\n\nStage 1: Style transfer by reference Mel\n\nIn the first stage, a style transfer system with reference speech is developed. We particularly study the role of each main module in VITS for the style transfer task. The text encoder processes the input phonemes, independent of the speaker and style information. The normalizing flow improves the flexibility of the prior distribution. The stochastic duration predictor estimates the distribution of phoneme duration from text embedding. We believe that duration is an important aspect of style, so only style embedding is added to the input of the stochastic duration predictor. For simplicity, the normalizing flow is designed to be a volume-preserving transformation with the Jacobian determinant of one. So we only add style embedding to the normalizing flow through global conditioning, considering that it is hard to take into account both speaker and style embedding in the construction of the prior distribution. The posterior encoder produces the normal posterior distribution from a linear spectrogram and the decoder reconstructs the latent variables z to spectrogram. Referring to DelightfulTTS [26], SSIM loss Lssim is added, and the speaker classifier in the adversarial component is trained with a cross-entropy loss with gradient inversion L adv . Specifically, the loss of the first stage is\nLstage1 = Lvits + Lssim + L adv .(1)\n\nStage 2: Style transfer by text prompt\n\nThe second stage aims to achieve controllable style transfer with text prompts. To this end, we use a prompt encoder to extract prompt embeddings from natural language descriptions, and the prompt embedding learns style embedding by cosine similarity loss referring to Lcons. The cosine similarity loss is not only used by a separate optimizer for the prompt encoder but also used to train the TTS system. Note that, we only train the last attention layer of BERT, and the parameters of the style encoder are frozen throughout this training process. The loss of the second stage is\nLstage2 = Lvits + Lcons.(2)\n3. Experiments\n\n\nDatasets\n\nWe use 2 hours of an audiobook corpus recorded from a single speaker as the target speaker. Most recordings are common reading style without much expressive speech. As there is no public expressive dataset with abundant natural language prompts, we use an internally expressive Mandarin Chinese speech corpus as source speakers. The corpus contains 12 hours of speech data from 8 female speakers and about 6 hours of speech is associated with natural language descriptions. To obtain the natural language descriptions (text prompts), we invite 9 professional annotators to use a phrase or a sentence to describe the style of each utterance. They are told to focus on the speaking style and ignore the linguistic content.\n\nFor our experiments, all the speech recordings are downsampled to 24 kHz. Linear spectrograms obtained from raw waveforms through the Short-time Fourier transform (STFT) are used as the input of VITS. Eighty-dimensional Melfrequency spectrograms are extracted with 12.5 ms frame shift and 50 ms frame length. The text prompts are tokenized by a pre-trained BERT.\n\n\nModel Configuration\n\nOur work focuses on style transfer with text prompts based on the audiobook datasets. Thus two models of style transfer on audiobook datasets are established as baselines -one with text prompt and the other using reference speech. \u2022 CST-TTS: Following [27], a cross-speaker style transfer framework is built based on non-autoregressive feedforward structure which can control the expressiveness by style tags. \u2022 GST-MLTTS: A cross-speaker emotion transfer model [28] based on semi-supervised training and SCLN. \u2022 PromptStyle: The proposed style transfer system. The hyperparameters of VITS are set to be the same as in [23]. The reference encoder has the same architecture as GSTs without style tokens. The prompt encoder uses a pre-trained BERT to extract 768-dimensional global semantic embedding, and an adapter layer consisting of three linear layers with ReLU activation converting the semantic embedding to 256-dimensional prompt embedding. All models are trained with a batch size of 32 for 200k steps, while the fine-tuning stage of StylePrompt is trained with 16 batches for 10k steps. HiFi-GAN [29] is used in CST-TTS and GST-MLTTS as the neural vocoder for reconstructing the waveform.\n\n\nPerformance on the Style Transfer\n\n\nSubjective Evaluation\n\nThe performances of three comparison models on style transfer are evaluated in terms of speech quality, speaker similarity, and style similarity with a Mean Opinion Score (MOS) test.  \nM1 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 M2 \u2713 \u2713 \u2713 \u2713 \u2713 PromptStyle \u2713 \u2713 \u2713 \u2713 M3 \u2713 \u2713 \u2713\nSpecifically, 50 speech samples are synthesized for each model, and 15 listeners who are native Chinese take part in this experiment. They are asked to score on a 1\u223c5 point with 0.5 intervals for the speech quality, speaker similarity with the target speaker, and style similarity with the source speaker. As shown in Table 1, PromptStyle has clearly better performance in terms of speech quality, speaker similarity, and style similarity. We believe such good performance is due to the end-to-end structure and the style embedding as well as the speaker embedding added to different modules as conditions.\n\n\nObjective evaluation\n\nTo visualize style embedding space by t-SNE [30], We manually selected 13 common styles from the training dataset for evaluation. Style embedding is extracted from the selected speech using PromptStyle, ST-TTS, and GST-MLTTS respectively. As shown in Fig 2, each point indicates a style embedding and points with the same color are from the same style category. The distance between the two points indicates the relative similarity of the embeddings. Smaller distances indicate more similar embeddings. An ideal style encoder should make the style embeddings from the same category closer to each other while embeddings from different categories should be separated apart. From Fig 2, we observe that MST-TTS gets the worst performance and GST-MLTTS performs better but the points from the same category are still diversely distributed. In contrast, in Fig. 2(c), the style embeddings from the same category are clustered together, and different clusters are apart from each other. It indicates that PromptStyle presents significant superiority in extracting style embeddings.\n\n\nAblation Study\n\nIn order to verify the influence of the style embedding and speaker embedding added to different modules as conditions, we conduct the ablation experiments considering the posterior encoder, the normalizing flow, and the decoder. Intuitively, we add both style embeddings and speaker embeddings to all their modules, referring to M1. Then we remove speaker embeddings from the flow, referring to M2. On the base of M2, removing style embeddings from the decoder is our final version of StylePrompt. Finally, on the base of StylePrompt, we remove the style embeddings from the posterior encoder, referring to M3. Similarly, these models are evaluated in terms of speech   Table 2 and Table 3, respectively. As shown in Table 3, M1 maintains well the timbre of the target speaker but fails to achieve decent style transfer. Comparing M2 and M1, the results indicate that it is hard for a normalizing flow to extract a proper style representation with both style embedding and speaker embedding as conditions. Comparing PromptStyle with M2, it is verified that PromptStyle can capture more speaker timbre when the speaker embedding is added to the decoder of VITS. Finally, comparing M3 with Prompt-Style, we find that adding the style embedding to the posterior encoder as conditions is beneficial to improve the expressiveness of the synthesized speech.\n\n\nStyle Control by Text Prompt\n\nTo evaluate the performance of PromptStyle in style transfer by text prompts, we further conduct ABX preference tests between the prompt embedding and the style embedding in terms of speech quality, speaker similarity, and style similarity. Note that style embedding serves as the performance top line, as speech reference contains more direct style information than the text prompt. Precisely, we extract style embedding and prompt embedding from the reference speech and the text prompt, respectively, to control the transferred style. Listeners are offered a pair of randomly selected samples and asked to choose which one is as expressive as the descriptions of the provided text prompts. As the preference test results shown in Fig. 3, the listeners give higher preference to No preference, which means that our proposed method achieves the goal of text prompt guided style transfer with precise style control and high speech quality.\n\n\nConclusions\n\nIn this work, we propose PromptStyle, a style transfer model with natural language prompts, to make the style transfer more controllable and user-friendly. PromptStyle establishes the relationship between the style embedding space and the prompt embedding space through two-stage training. Experiments on expressive audiobook synthesis show that PromptStyle can achieve the goal of style transfer with text prompts while maintaining relatively high stability and speaker similarity.\n\nFigure 1 :\n1Architecture of PromptStyle coder and the acoustic model. \u2022 With the generalization capability of the pre-trained language model, our approach can generate stylistic speech for the target speaker from an unseen style prompt.\n\nFigure 2 :\n2Visualization of the style embeddings from different models -(a) CST-TTS, (b) GST-MLTTS and (c) PromptStyle.\n\nFigure 3 :\n3ABX preference results quality, speaker similarity, and style similarity with a MOS test. The model setups and experimental results are shown in\n\nTable 1 :\n1Similarity/Quality MOS results (Average score and 95% confidence interval)Models \nSpeakers \nSimilarity \n\nStyle \nSimilarity \n\nSpeech \nQuality \nCST-TTS \n3.01 \n3.23 \n3.14 \nGST-MLTTS \n3.46 \n3.42 \n3.53 \nPromptStyle \n3.63 \n3.78 \n4.06 \n\n\n\nTable 2 :\n2The setup of the ablation experiments.Models \nPosterior Encoder \nFlow \nDecoder \nstyle speaker style speaker style speaker \n\n\n\nTable 3 :\n3Similarity/Quality MOS results of ablations (Average score and 95% confidence interval)Models \nSpeakers \nSimilarity \n\nStyle \nSimilarity \n\nSpeech \nQuality \nM1 \n3.96 \n2.87 \n4.07 \nM2 \n3.43 \n3.78 \n3.98 \nPromptStyle \n3.62 \n3.82 \n4.05 \nM3 \n3.57 \n3.73 \n4.01 \n\nSpeech Quality \n\nSpeaker Similarity \n\nStyle Similarity \n\n70.5% \n\n24.4% \n\n13.9% \n15.6% \n\n73.5% \n11.7% \n\n15.3% \n68.8% \n15.9% \n\nSpeech reference \nNo preference \nPrompt reference \n\n\n\nTacotron: Towards end-to-end speech synthesis. Y Wang, R Skerry-Ryan, D Stanton, Y Wu, R J Weiss, N Jaitly, Z Yang, Y Xiao, Z Chen, S Bengio, Proc. Interspeech. InterspeechY. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio et al., \"Tacotron: Towards end-to-end speech synthesis,\" Proc. Interspeech 2017, pp. 4006-4010, 2017.\n\nFastspeech 2: Fast and high-quality end-to-end text to speech. Y Ren, C Hu, X Tan, T Qin, S Zhao, Z Zhao, T.-Y Liu, International Conference on Learning Representations. Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \"Fastspeech 2: Fast and high-quality end-to-end text to speech,\" in International Conference on Learning Representations, 2020.\n\nA survey on neural speech synthesis. X Tan, T Qin, F Soong, T.-Y Liu, arXiv:2106.15561arXiv preprintX. Tan, T. Qin, F. Soong, and T.-Y. Liu, \"A survey on neural speech synthesis,\" arXiv preprint arXiv:2106.15561, 2021.\n\nMulti-reference tacotron by intercross training for style disentangling,transfer and control in speech synthesis. Y Bian, C Chen, Y Kang, Z Pan, abs/1904.02373CoRR. Y. Bian, C. Chen, Y. Kang, and Z. Pan, \"Multi-reference tacotron by intercross training for style disentangling,transfer and control in speech synthesis,\" CoRR, vol. abs/1904.02373, 2019.\n\nCross-speaker emotion disentangling and transfer for end-to-end speech synthesis. T Li, X Wang, Q Xie, Z Wang, L Xie, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 30T. Li, X. Wang, Q. Xie, Z. Wang, and L. Xie, \"Cross-speaker emotion disentangling and transfer for end-to-end speech synthe- sis,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 1448-1460, 2022.\n\niemotts: Toward robust cross-speaker emotion transfer and control for speech synthesis based on disentanglement between prosody and timbre. G Zhang, Y Qin, W Zhang, J Wu, M Li, Y Gai, F Jiang, T Lee, arXiv:2206.14866arXiv preprintG. Zhang, Y. Qin, W. Zhang, J. Wu, M. Li, Y. Gai, F. Jiang, and T. Lee, \"iemotts: Toward robust cross-speaker emotion trans- fer and control for speech synthesis based on disentanglement between prosody and timbre,\" arXiv preprint arXiv:2206.14866, 2022.\n\nCrossspeaker Emotion Transfer Based On Prosody Compensation for End-to-End Speech Synthesis. T Li, X Wang, Q Xie, Z Wang, M Jiang, L Xie, Proc. Interspeech. InterspeechT. Li, X. Wang, Q. Xie, Z. Wang, M. Jiang, and L. Xie, \"Cross- speaker Emotion Transfer Based On Prosody Compensation for End-to-End Speech Synthesis,\" in Proc. Interspeech 2022, 2022, pp. 5498-5502.\n\nIncorporating cross-speaker style transfer for multi-language text-tospeech. Z Shang, Z Huang, H Zhang, P Zhang, Y Yan, Interspeech, 2021Z. Shang, Z. Huang, H. Zhang, P. Zhang, and Y. Yan, \"Incor- porating cross-speaker style transfer for multi-language text-to- speech.\" in Interspeech, 2021, pp. 1619-1623.\n\nEmphasis: An emotional phonemebased acoustic model for speech synthesis system. H Li, Y Kang, Z Wang, Proc. Inter. H. Li, Y. Kang, and Z. Wang, \"Emphasis: An emotional phoneme- based acoustic model for speech synthesis system,\" Proc. Inter- speech 2018, pp. 3077-3081, 2018.\n\nControlling emotion strength with relative attribute for end-to-end speech synthesis. X Zhu, S Yang, G Yang, L Xie, 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). X. Zhu, S. Yang, G. Yang, and L. Xie, \"Controlling emotion strength with relative attribute for end-to-end speech synthesis,\" 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 192-199, 2019.\n\nVisualization and interpretation of latent spaces for controlling expressive speech synthesis through audio analysis. N Tits, F Wang, K E Haddad, V Pagel, T Dutoit, arXiv:1903.11570arXiv preprintN. Tits, F. Wang, K. E. Haddad, V. Pagel, and T. Dutoit, \"Vi- sualization and interpretation of latent spaces for controlling ex- pressive speech synthesis through audio analysis,\" arXiv preprint arXiv:1903.11570, 2019.\n\nExploring transfer learning for low resource emotional tts. N Tits, K El Haddad, T Dutoit, Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys. Springer1N. Tits, K. El Haddad, and T. Dutoit, \"Exploring transfer learning for low resource emotional tts,\" in Intelligent Systems and Appli- cations: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 1. Springer, 2020, pp. 52-60.\n\nTowards end-to-end prosody transfer for expressive speech synthesis with tacotron. R Skerry-Ryan, E Battenberg, Y Xiao, Y Wang, D Stanton, J Shor, R Weiss, R Clark, R A Saurous, international conference on machine learning. PMLRR. Skerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D. Stan- ton, J. Shor, R. Weiss, R. Clark, and R. A. Saurous, \"To- wards end-to-end prosody transfer for expressive speech synthesis with tacotron,\" in international conference on machine learning. PMLR, 2018, pp. 4693-4702.\n\nLearning latent representations for style control and transfer in end-to-end speech synthesis. Y.-J Zhang, S Pan, L He, Z.-H Ling, ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Y.-J. Zhang, S. Pan, L. He, and Z.-H. Ling, \"Learning latent rep- resentations for style control and transfer in end-to-end speech synthesis,\" ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6945- 6949, 2019.\n\nImproving transfer of expressivity for end-to-end multispeaker text-to-speech synthesis. A Kulkarni, V Colotte, D Jouvet, 2021 29th European Signal Processing Conference (EUSIPCO). A. Kulkarni, V. Colotte, and D. Jouvet, \"Improving transfer of ex- pressivity for end-to-end multispeaker text-to-speech synthesis,\" 2021 29th European Signal Processing Conference (EUSIPCO), pp. 31-35, 2021.\n\nTransfer learning from speaker verification to multispeaker text-to-speech synthesis. Y Jia, Y Zhang, R J Weiss, Q Wang, J Shen, F Ren, Z Chen, P Nguyen, R Pang, I L Moreno, Proceedings of the 32nd International Conference on Neural Information Processing Systems. the 32nd International Conference on Neural Information Processing SystemsY. Jia, Y. Zhang, R. J. Weiss, Q. Wang, J. Shen, F. Ren, Z. Chen, P. Nguyen, R. Pang, I. L. Moreno et al., \"Transfer learning from speaker verification to multispeaker text-to-speech synthesis,\" in Proceedings of the 32nd International Conference on Neural In- formation Processing Systems, 2018, pp. 4485-4495.\n\nControllable emotion transfer for end-to-end speech synthesis. T Li, S Yang, L Xue, L Xie, 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP). T. Li, S. Yang, L. Xue, and L. Xie, \"Controllable emotion transfer for end-to-end speech synthesis,\" 2021 12th International Sympo- sium on Chinese Spoken Language Processing (ISCSLP), pp. 1-5, 2021.\n\nCpm-2: Large-scale cost-effective pre-trained language models. Z Zhang, Y Gu, X Han, S Chen, C Xiao, Z Sun, Y Yao, F Qi, J Guan, P Ke, AI Open. 2Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi, J. Guan, P. Ke et al., \"Cpm-2: Large-scale cost-effective pre-trained language models,\" AI Open, vol. 2, pp. 216-224, 2021.\n\nEnd-to-end generative pretraining for multimodal video captioning. P H Seo, A Nagrani, A Arnab, C Schmid, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionP. H. Seo, A. Nagrani, A. Arnab, and C. Schmid, \"End-to-end generative pretraining for multimodal video captioning,\" in Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 17 959-17 968.\n\nExpressive text-to-speech using style tag. M Kim, S J Cheon, B J Choi, J J Kim, N S Kim, arXiv:2104.00436arXiv preprintM. Kim, S. J. Cheon, B. J. Choi, J. J. Kim, and N. S. Kim, \"Expressive text-to-speech using style tag,\" arXiv preprint arXiv:2104.00436, 2021.\n\nPrompttts: Controllable text-to-speech with text descriptions. Z Guo, Y Leng, Y Wu, S Zhao, X Tan, arXiv:2211.12171arXiv preprintZ. Guo, Y. Leng, Y. Wu, S. Zhao, and X. Tan, \"Prompttts: Con- trollable text-to-speech with text descriptions,\" arXiv preprint arXiv:2211.12171, 2022.\n\nInstructtts: Modelling expressive tts in discrete latent space with natural language style prompt. D Yang, S Liu, R Huang, G Lei, C Weng, H Meng, D Yu, arXiv:2301.13662arXiv preprintD. Yang, S. Liu, R. Huang, G. Lei, C. Weng, H. Meng, and D. Yu, \"Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt,\" arXiv preprint arXiv:2301.13662, 2023.\n\nConditional variational autoencoder with adversarial learning for end-to-end text-to-speech. J Kim, J Kong, J Son, International Conference on Machine Learning. PMLR, 2021. J. Kim, J. Kong, and J. Son, \"Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\" in Inter- national Conference on Machine Learning. PMLR, 2021, pp. 5530-5540.\n\nStyle tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis. Y Wang, D Stanton, Y Zhang, R Skerry-Ryan, E Battenberg, J Shor, Y Xiao, F Ren, Y Jia, R A Saurous, Proc. ICML. ICMLY. Wang, D. Stanton, Y. Zhang, R. Skerry-Ryan, E. Battenberg, J. Shor, Y. Xiao, F. Ren, Y. Jia, and R. A. Saurous, \"Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,\" in Proc. ICML, 2018, pp. 5180-5189.\n\nNeural speech synthesis with transformer network. N Li, S Liu, Y Liu, S Zhao, M Liu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence33N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu, \"Neural speech synthe- sis with transformer network,\" in Proceedings of the AAAI con- ference on artificial intelligence, vol. 33, no. 01, 2019, pp. 6706- 6713.\n\nDelightfultts: The microsoft speech synthesis system for blizzard challenge 2021. Y Liu, Z Xu, G Wang, K Chen, B Li, X Tan, J Li, L He, S Zhao, arXiv:2110.12612arXiv preprintY. Liu, Z. Xu, G. Wang, K. Chen, B. Li, X. Tan, J. Li, L. He, and S. Zhao, \"Delightfultts: The microsoft speech synthesis system for blizzard challenge 2021,\" arXiv preprint arXiv:2110.12612, 2021.\n\nText-driven emotional style control and cross-speaker style transfer in neural tts. Y Shin, Y Lee, S Jo, Y Hwang, T Kim, arXiv:2207.06000arXiv preprintY. Shin, Y. Lee, S. Jo, Y. Hwang, and T. Kim, \"Text-driven emo- tional style control and cross-speaker style transfer in neural tts,\" arXiv preprint arXiv:2207.06000, 2022.\n\nEnd-to-end emotional speech synthesis using style tokens and semi-supervised training. P.-F Wu, Z Ling, L Liu, Y Jiang, H.-C Wu, L.-R Dai, 2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (AP-SIPA ASC). P.-F. Wu, Z. Ling, L. juan Liu, Y. Jiang, H.-C. Wu, and L.-R. Dai, \"End-to-end emotional speech synthesis using style tokens and semi-supervised training,\" 2019 Asia-Pacific Signal and Informa- tion Processing Association Annual Summit and Conference (AP- SIPA ASC), pp. 623-627, 2019.\n\nHifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. J Kong, J Kim, J Bae, Advances in Neural Information Processing Systems. 3333J. Kong, J. Kim, and J. Bae, \"Hifi-gan: Generative adversarial net- works for efficient and high fidelity speech synthesis,\" Advances in Neural Information Processing Systems, vol. 33, pp. 17 022- 17 033, 2020.\n\nVisualizing data using t-SNE. V D M Laurens, G Hinton, Journal of Machine Learning Research. 92605V. D. M. Laurens and G. Hinton, \"Visualizing data using t-SNE,\" Journal of Machine Learning Research, vol. 9, no. 2605, pp. 2579-2605, 2008.\n", "annotations": {"author": "[{\"end\":266,\"start\":98},{\"end\":413,\"start\":267},{\"end\":553,\"start\":414},{\"end\":635,\"start\":554},{\"end\":692,\"start\":636},{\"end\":703,\"start\":693},{\"end\":908,\"start\":704}]", "publisher": null, "author_last_name": "[{\"end\":110,\"start\":107},{\"end\":280,\"start\":275},{\"end\":420,\"start\":417},{\"end\":565,\"start\":561},{\"end\":644,\"start\":640},{\"end\":702,\"start\":700},{\"end\":711,\"start\":708}]", "author_first_name": "[{\"end\":106,\"start\":98},{\"end\":274,\"start\":267},{\"end\":416,\"start\":414},{\"end\":560,\"start\":554},{\"end\":639,\"start\":636},{\"end\":699,\"start\":693},{\"end\":707,\"start\":704}]", "author_affiliation": "[{\"end\":265,\"start\":135},{\"end\":412,\"start\":282},{\"end\":552,\"start\":422},{\"end\":634,\"start\":589},{\"end\":691,\"start\":646},{\"end\":860,\"start\":730},{\"end\":907,\"start\":862}]", "title": "[{\"end\":95,\"start\":1},{\"end\":1003,\"start\":909}]", "venue": null, "abstract": "[{\"end\":2128,\"start\":1058}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2168,\"start\":2165},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2170,\"start\":2168},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2316,\"start\":2313},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2774,\"start\":2771},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2776,\"start\":2774},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2778,\"start\":2776},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2780,\"start\":2778},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2782,\"start\":2780},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2967,\"start\":2964},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2970,\"start\":2967},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2973,\"start\":2970},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2976,\"start\":2973},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3038,\"start\":3034},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3041,\"start\":3038},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3044,\"start\":3041},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3047,\"start\":3044},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3050,\"start\":3047},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3545,\"start\":3541},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3548,\"start\":3545},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3692,\"start\":3688},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4037,\"start\":4033},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4238,\"start\":4234},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5340,\"start\":5336},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8704,\"start\":8700},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9229,\"start\":9225},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11233,\"start\":11229},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13510,\"start\":13506},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13720,\"start\":13716},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13877,\"start\":13873},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14362,\"start\":14358},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15433,\"start\":15429}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19540,\"start\":19303},{\"attributes\":{\"id\":\"fig_1\"},\"end\":19662,\"start\":19541},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19820,\"start\":19663},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20063,\"start\":19821},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":20200,\"start\":20064},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":20643,\"start\":20201}]", "paragraph": "[{\"end\":2806,\"start\":2144},{\"end\":3468,\"start\":2808},{\"end\":4662,\"start\":3470},{\"end\":6080,\"start\":4664},{\"end\":6139,\"start\":6082},{\"end\":6555,\"start\":6141},{\"end\":6798,\"start\":6643},{\"end\":7514,\"start\":6846},{\"end\":8097,\"start\":7516},{\"end\":8292,\"start\":8127},{\"end\":9046,\"start\":8294},{\"end\":9561,\"start\":9048},{\"end\":9910,\"start\":9588},{\"end\":10075,\"start\":9912},{\"end\":11430,\"start\":10120},{\"end\":12090,\"start\":11509},{\"end\":12133,\"start\":12119},{\"end\":12866,\"start\":12146},{\"end\":13230,\"start\":12868},{\"end\":14450,\"start\":13254},{\"end\":14696,\"start\":14512},{\"end\":15360,\"start\":14754},{\"end\":16461,\"start\":15385},{\"end\":17832,\"start\":16480},{\"end\":18804,\"start\":17865},{\"end\":19302,\"start\":18820}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6814,\"start\":6799},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11467,\"start\":11431},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12118,\"start\":12091},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14753,\"start\":14697}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":15079,\"start\":15072},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17158,\"start\":17151},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":17170,\"start\":17163},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":17205,\"start\":17198}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2142,\"start\":2130},{\"end\":6571,\"start\":6558},{\"end\":6608,\"start\":6574},{\"end\":6612,\"start\":6611},{\"end\":6641,\"start\":6615},{\"attributes\":{\"n\":\"2.\"},\"end\":6827,\"start\":6816},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6844,\"start\":6830},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8125,\"start\":8100},{\"attributes\":{\"n\":\"2.3.\"},\"end\":9586,\"start\":9564},{\"attributes\":{\"n\":\"2.3.1.\"},\"end\":10118,\"start\":10078},{\"attributes\":{\"n\":\"2.3.2.\"},\"end\":11507,\"start\":11469},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12144,\"start\":12136},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13252,\"start\":13233},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14486,\"start\":14453},{\"attributes\":{\"n\":\"3.3.1.\"},\"end\":14510,\"start\":14489},{\"attributes\":{\"n\":\"3.3.2.\"},\"end\":15383,\"start\":15363},{\"attributes\":{\"n\":\"3.4.\"},\"end\":16478,\"start\":16464},{\"attributes\":{\"n\":\"3.5.\"},\"end\":17863,\"start\":17835},{\"attributes\":{\"n\":\"4.\"},\"end\":18818,\"start\":18807},{\"end\":19314,\"start\":19304},{\"end\":19552,\"start\":19542},{\"end\":19674,\"start\":19664},{\"end\":19831,\"start\":19822},{\"end\":20074,\"start\":20065},{\"end\":20211,\"start\":20202}]", "table": "[{\"end\":20063,\"start\":19907},{\"end\":20200,\"start\":20114},{\"end\":20643,\"start\":20300}]", "figure_caption": "[{\"end\":19540,\"start\":19316},{\"end\":19662,\"start\":19554},{\"end\":19820,\"start\":19676},{\"end\":19907,\"start\":19833},{\"end\":20114,\"start\":20076},{\"end\":20300,\"start\":20213}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6914,\"start\":6908},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15642,\"start\":15636},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16069,\"start\":16058},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16247,\"start\":16238},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18604,\"start\":18598}]", "bib_author_first_name": "[{\"end\":20693,\"start\":20692},{\"end\":20701,\"start\":20700},{\"end\":20716,\"start\":20715},{\"end\":20727,\"start\":20726},{\"end\":20733,\"start\":20732},{\"end\":20735,\"start\":20734},{\"end\":20744,\"start\":20743},{\"end\":20754,\"start\":20753},{\"end\":20762,\"start\":20761},{\"end\":20770,\"start\":20769},{\"end\":20778,\"start\":20777},{\"end\":21089,\"start\":21088},{\"end\":21096,\"start\":21095},{\"end\":21102,\"start\":21101},{\"end\":21109,\"start\":21108},{\"end\":21116,\"start\":21115},{\"end\":21124,\"start\":21123},{\"end\":21135,\"start\":21131},{\"end\":21426,\"start\":21425},{\"end\":21433,\"start\":21432},{\"end\":21440,\"start\":21439},{\"end\":21452,\"start\":21448},{\"end\":21723,\"start\":21722},{\"end\":21731,\"start\":21730},{\"end\":21739,\"start\":21738},{\"end\":21747,\"start\":21746},{\"end\":22045,\"start\":22044},{\"end\":22051,\"start\":22050},{\"end\":22059,\"start\":22058},{\"end\":22066,\"start\":22065},{\"end\":22074,\"start\":22073},{\"end\":22515,\"start\":22514},{\"end\":22524,\"start\":22523},{\"end\":22531,\"start\":22530},{\"end\":22540,\"start\":22539},{\"end\":22546,\"start\":22545},{\"end\":22552,\"start\":22551},{\"end\":22559,\"start\":22558},{\"end\":22568,\"start\":22567},{\"end\":22954,\"start\":22953},{\"end\":22960,\"start\":22959},{\"end\":22968,\"start\":22967},{\"end\":22975,\"start\":22974},{\"end\":22983,\"start\":22982},{\"end\":22992,\"start\":22991},{\"end\":23307,\"start\":23306},{\"end\":23316,\"start\":23315},{\"end\":23325,\"start\":23324},{\"end\":23334,\"start\":23333},{\"end\":23343,\"start\":23342},{\"end\":23620,\"start\":23619},{\"end\":23626,\"start\":23625},{\"end\":23634,\"start\":23633},{\"end\":23902,\"start\":23901},{\"end\":23909,\"start\":23908},{\"end\":23917,\"start\":23916},{\"end\":23925,\"start\":23924},{\"end\":24344,\"start\":24343},{\"end\":24352,\"start\":24351},{\"end\":24360,\"start\":24359},{\"end\":24362,\"start\":24361},{\"end\":24372,\"start\":24371},{\"end\":24381,\"start\":24380},{\"end\":24702,\"start\":24701},{\"end\":24710,\"start\":24709},{\"end\":24713,\"start\":24711},{\"end\":24723,\"start\":24722},{\"end\":25180,\"start\":25179},{\"end\":25195,\"start\":25194},{\"end\":25209,\"start\":25208},{\"end\":25217,\"start\":25216},{\"end\":25225,\"start\":25224},{\"end\":25236,\"start\":25235},{\"end\":25244,\"start\":25243},{\"end\":25253,\"start\":25252},{\"end\":25262,\"start\":25261},{\"end\":25264,\"start\":25263},{\"end\":25698,\"start\":25694},{\"end\":25707,\"start\":25706},{\"end\":25714,\"start\":25713},{\"end\":25723,\"start\":25719},{\"end\":26188,\"start\":26187},{\"end\":26200,\"start\":26199},{\"end\":26211,\"start\":26210},{\"end\":26576,\"start\":26575},{\"end\":26583,\"start\":26582},{\"end\":26592,\"start\":26591},{\"end\":26594,\"start\":26593},{\"end\":26603,\"start\":26602},{\"end\":26611,\"start\":26610},{\"end\":26619,\"start\":26618},{\"end\":26626,\"start\":26625},{\"end\":26634,\"start\":26633},{\"end\":26644,\"start\":26643},{\"end\":26652,\"start\":26651},{\"end\":26654,\"start\":26653},{\"end\":27205,\"start\":27204},{\"end\":27211,\"start\":27210},{\"end\":27219,\"start\":27218},{\"end\":27226,\"start\":27225},{\"end\":27579,\"start\":27578},{\"end\":27588,\"start\":27587},{\"end\":27594,\"start\":27593},{\"end\":27601,\"start\":27600},{\"end\":27609,\"start\":27608},{\"end\":27617,\"start\":27616},{\"end\":27624,\"start\":27623},{\"end\":27631,\"start\":27630},{\"end\":27637,\"start\":27636},{\"end\":27645,\"start\":27644},{\"end\":27919,\"start\":27918},{\"end\":27921,\"start\":27920},{\"end\":27928,\"start\":27927},{\"end\":27939,\"start\":27938},{\"end\":27948,\"start\":27947},{\"end\":28381,\"start\":28380},{\"end\":28388,\"start\":28387},{\"end\":28390,\"start\":28389},{\"end\":28399,\"start\":28398},{\"end\":28401,\"start\":28400},{\"end\":28409,\"start\":28408},{\"end\":28411,\"start\":28410},{\"end\":28418,\"start\":28417},{\"end\":28420,\"start\":28419},{\"end\":28664,\"start\":28663},{\"end\":28671,\"start\":28670},{\"end\":28679,\"start\":28678},{\"end\":28685,\"start\":28684},{\"end\":28693,\"start\":28692},{\"end\":28981,\"start\":28980},{\"end\":28989,\"start\":28988},{\"end\":28996,\"start\":28995},{\"end\":29005,\"start\":29004},{\"end\":29012,\"start\":29011},{\"end\":29020,\"start\":29019},{\"end\":29028,\"start\":29027},{\"end\":29362,\"start\":29361},{\"end\":29369,\"start\":29368},{\"end\":29377,\"start\":29376},{\"end\":29741,\"start\":29740},{\"end\":29749,\"start\":29748},{\"end\":29760,\"start\":29759},{\"end\":29769,\"start\":29768},{\"end\":29784,\"start\":29783},{\"end\":29798,\"start\":29797},{\"end\":29806,\"start\":29805},{\"end\":29814,\"start\":29813},{\"end\":29821,\"start\":29820},{\"end\":29828,\"start\":29827},{\"end\":29830,\"start\":29829},{\"end\":30157,\"start\":30156},{\"end\":30163,\"start\":30162},{\"end\":30170,\"start\":30169},{\"end\":30177,\"start\":30176},{\"end\":30185,\"start\":30184},{\"end\":30591,\"start\":30590},{\"end\":30598,\"start\":30597},{\"end\":30604,\"start\":30603},{\"end\":30612,\"start\":30611},{\"end\":30620,\"start\":30619},{\"end\":30626,\"start\":30625},{\"end\":30633,\"start\":30632},{\"end\":30639,\"start\":30638},{\"end\":30645,\"start\":30644},{\"end\":30966,\"start\":30965},{\"end\":30974,\"start\":30973},{\"end\":30981,\"start\":30980},{\"end\":30987,\"start\":30986},{\"end\":30996,\"start\":30995},{\"end\":31297,\"start\":31293},{\"end\":31303,\"start\":31302},{\"end\":31311,\"start\":31310},{\"end\":31318,\"start\":31317},{\"end\":31330,\"start\":31326},{\"end\":31339,\"start\":31335},{\"end\":31833,\"start\":31832},{\"end\":31841,\"start\":31840},{\"end\":31848,\"start\":31847},{\"end\":32152,\"start\":32151},{\"end\":32156,\"start\":32153},{\"end\":32167,\"start\":32166}]", "bib_author_last_name": "[{\"end\":20698,\"start\":20694},{\"end\":20713,\"start\":20702},{\"end\":20724,\"start\":20717},{\"end\":20730,\"start\":20728},{\"end\":20741,\"start\":20736},{\"end\":20751,\"start\":20745},{\"end\":20759,\"start\":20755},{\"end\":20767,\"start\":20763},{\"end\":20775,\"start\":20771},{\"end\":20785,\"start\":20779},{\"end\":21093,\"start\":21090},{\"end\":21099,\"start\":21097},{\"end\":21106,\"start\":21103},{\"end\":21113,\"start\":21110},{\"end\":21121,\"start\":21117},{\"end\":21129,\"start\":21125},{\"end\":21139,\"start\":21136},{\"end\":21430,\"start\":21427},{\"end\":21437,\"start\":21434},{\"end\":21446,\"start\":21441},{\"end\":21456,\"start\":21453},{\"end\":21728,\"start\":21724},{\"end\":21736,\"start\":21732},{\"end\":21744,\"start\":21740},{\"end\":21751,\"start\":21748},{\"end\":22048,\"start\":22046},{\"end\":22056,\"start\":22052},{\"end\":22063,\"start\":22060},{\"end\":22071,\"start\":22067},{\"end\":22078,\"start\":22075},{\"end\":22521,\"start\":22516},{\"end\":22528,\"start\":22525},{\"end\":22537,\"start\":22532},{\"end\":22543,\"start\":22541},{\"end\":22549,\"start\":22547},{\"end\":22556,\"start\":22553},{\"end\":22565,\"start\":22560},{\"end\":22572,\"start\":22569},{\"end\":22957,\"start\":22955},{\"end\":22965,\"start\":22961},{\"end\":22972,\"start\":22969},{\"end\":22980,\"start\":22976},{\"end\":22989,\"start\":22984},{\"end\":22996,\"start\":22993},{\"end\":23313,\"start\":23308},{\"end\":23322,\"start\":23317},{\"end\":23331,\"start\":23326},{\"end\":23340,\"start\":23335},{\"end\":23347,\"start\":23344},{\"end\":23623,\"start\":23621},{\"end\":23631,\"start\":23627},{\"end\":23639,\"start\":23635},{\"end\":23906,\"start\":23903},{\"end\":23914,\"start\":23910},{\"end\":23922,\"start\":23918},{\"end\":23929,\"start\":23926},{\"end\":24349,\"start\":24345},{\"end\":24357,\"start\":24353},{\"end\":24369,\"start\":24363},{\"end\":24378,\"start\":24373},{\"end\":24388,\"start\":24382},{\"end\":24707,\"start\":24703},{\"end\":24720,\"start\":24714},{\"end\":24730,\"start\":24724},{\"end\":25192,\"start\":25181},{\"end\":25206,\"start\":25196},{\"end\":25214,\"start\":25210},{\"end\":25222,\"start\":25218},{\"end\":25233,\"start\":25226},{\"end\":25241,\"start\":25237},{\"end\":25250,\"start\":25245},{\"end\":25259,\"start\":25254},{\"end\":25272,\"start\":25265},{\"end\":25704,\"start\":25699},{\"end\":25711,\"start\":25708},{\"end\":25717,\"start\":25715},{\"end\":25728,\"start\":25724},{\"end\":26197,\"start\":26189},{\"end\":26208,\"start\":26201},{\"end\":26218,\"start\":26212},{\"end\":26580,\"start\":26577},{\"end\":26589,\"start\":26584},{\"end\":26600,\"start\":26595},{\"end\":26608,\"start\":26604},{\"end\":26616,\"start\":26612},{\"end\":26623,\"start\":26620},{\"end\":26631,\"start\":26627},{\"end\":26641,\"start\":26635},{\"end\":26649,\"start\":26645},{\"end\":26661,\"start\":26655},{\"end\":27208,\"start\":27206},{\"end\":27216,\"start\":27212},{\"end\":27223,\"start\":27220},{\"end\":27230,\"start\":27227},{\"end\":27585,\"start\":27580},{\"end\":27591,\"start\":27589},{\"end\":27598,\"start\":27595},{\"end\":27606,\"start\":27602},{\"end\":27614,\"start\":27610},{\"end\":27621,\"start\":27618},{\"end\":27628,\"start\":27625},{\"end\":27634,\"start\":27632},{\"end\":27642,\"start\":27638},{\"end\":27648,\"start\":27646},{\"end\":27925,\"start\":27922},{\"end\":27936,\"start\":27929},{\"end\":27945,\"start\":27940},{\"end\":27955,\"start\":27949},{\"end\":28385,\"start\":28382},{\"end\":28396,\"start\":28391},{\"end\":28406,\"start\":28402},{\"end\":28415,\"start\":28412},{\"end\":28424,\"start\":28421},{\"end\":28668,\"start\":28665},{\"end\":28676,\"start\":28672},{\"end\":28682,\"start\":28680},{\"end\":28690,\"start\":28686},{\"end\":28697,\"start\":28694},{\"end\":28986,\"start\":28982},{\"end\":28993,\"start\":28990},{\"end\":29002,\"start\":28997},{\"end\":29009,\"start\":29006},{\"end\":29017,\"start\":29013},{\"end\":29025,\"start\":29021},{\"end\":29031,\"start\":29029},{\"end\":29366,\"start\":29363},{\"end\":29374,\"start\":29370},{\"end\":29381,\"start\":29378},{\"end\":29746,\"start\":29742},{\"end\":29757,\"start\":29750},{\"end\":29766,\"start\":29761},{\"end\":29781,\"start\":29770},{\"end\":29795,\"start\":29785},{\"end\":29803,\"start\":29799},{\"end\":29811,\"start\":29807},{\"end\":29818,\"start\":29815},{\"end\":29825,\"start\":29822},{\"end\":29838,\"start\":29831},{\"end\":30160,\"start\":30158},{\"end\":30167,\"start\":30164},{\"end\":30174,\"start\":30171},{\"end\":30182,\"start\":30178},{\"end\":30189,\"start\":30186},{\"end\":30595,\"start\":30592},{\"end\":30601,\"start\":30599},{\"end\":30609,\"start\":30605},{\"end\":30617,\"start\":30613},{\"end\":30623,\"start\":30621},{\"end\":30630,\"start\":30627},{\"end\":30636,\"start\":30634},{\"end\":30642,\"start\":30640},{\"end\":30650,\"start\":30646},{\"end\":30971,\"start\":30967},{\"end\":30978,\"start\":30975},{\"end\":30984,\"start\":30982},{\"end\":30993,\"start\":30988},{\"end\":31000,\"start\":30997},{\"end\":31300,\"start\":31298},{\"end\":31308,\"start\":31304},{\"end\":31315,\"start\":31312},{\"end\":31324,\"start\":31319},{\"end\":31333,\"start\":31331},{\"end\":31343,\"start\":31340},{\"end\":31838,\"start\":31834},{\"end\":31845,\"start\":31842},{\"end\":31852,\"start\":31849},{\"end\":32164,\"start\":32157},{\"end\":32174,\"start\":32168}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4689304},\"end\":21023,\"start\":20645},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":219531522},\"end\":21386,\"start\":21025},{\"attributes\":{\"doi\":\"arXiv:2106.15561\",\"id\":\"b2\"},\"end\":21606,\"start\":21388},{\"attributes\":{\"doi\":\"abs/1904.02373\",\"id\":\"b3\",\"matched_paper_id\":102354807},\"end\":21960,\"start\":21608},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":247869533},\"end\":22372,\"start\":21962},{\"attributes\":{\"doi\":\"arXiv:2206.14866\",\"id\":\"b5\"},\"end\":22858,\"start\":22374},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":250264955},\"end\":23227,\"start\":22860},{\"attributes\":{\"id\":\"b7\"},\"end\":23537,\"start\":23229},{\"attributes\":{\"id\":\"b8\"},\"end\":23813,\"start\":23539},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211243599},\"end\":24223,\"start\":23815},{\"attributes\":{\"doi\":\"arXiv:1903.11570\",\"id\":\"b10\"},\"end\":24639,\"start\":24225},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":58004671},\"end\":25094,\"start\":24641},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4425995},\"end\":25597,\"start\":25096},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":54560617},\"end\":26096,\"start\":25599},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":235445740},\"end\":26487,\"start\":26098},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":48363067},\"end\":27139,\"start\":26489},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":226975831},\"end\":27513,\"start\":27141},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":235490263},\"end\":27849,\"start\":27515},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":246063824},\"end\":28335,\"start\":27851},{\"attributes\":{\"doi\":\"arXiv:2104.00436\",\"id\":\"b19\"},\"end\":28598,\"start\":28337},{\"attributes\":{\"doi\":\"arXiv:2211.12171\",\"id\":\"b20\"},\"end\":28879,\"start\":28600},{\"attributes\":{\"doi\":\"arXiv:2301.13662\",\"id\":\"b21\"},\"end\":29266,\"start\":28881},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":235417304},\"end\":29642,\"start\":29268},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4349820},\"end\":30104,\"start\":29644},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":59413863},\"end\":30506,\"start\":30106},{\"attributes\":{\"doi\":\"arXiv:2110.12612\",\"id\":\"b25\"},\"end\":30879,\"start\":30508},{\"attributes\":{\"doi\":\"arXiv:2207.06000\",\"id\":\"b26\"},\"end\":31204,\"start\":30881},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":195658018},\"end\":31738,\"start\":31206},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":222291664},\"end\":32119,\"start\":31740},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5855042},\"end\":32359,\"start\":32121}]", "bib_title": "[{\"end\":20690,\"start\":20645},{\"end\":21086,\"start\":21025},{\"end\":21720,\"start\":21608},{\"end\":22042,\"start\":21962},{\"end\":22951,\"start\":22860},{\"end\":23617,\"start\":23539},{\"end\":23899,\"start\":23815},{\"end\":24699,\"start\":24641},{\"end\":25177,\"start\":25096},{\"end\":25692,\"start\":25599},{\"end\":26185,\"start\":26098},{\"end\":26573,\"start\":26489},{\"end\":27202,\"start\":27141},{\"end\":27576,\"start\":27515},{\"end\":27916,\"start\":27851},{\"end\":29359,\"start\":29268},{\"end\":29738,\"start\":29644},{\"end\":30154,\"start\":30106},{\"end\":31291,\"start\":31206},{\"end\":31830,\"start\":31740},{\"end\":32149,\"start\":32121}]", "bib_author": "[{\"end\":20700,\"start\":20692},{\"end\":20715,\"start\":20700},{\"end\":20726,\"start\":20715},{\"end\":20732,\"start\":20726},{\"end\":20743,\"start\":20732},{\"end\":20753,\"start\":20743},{\"end\":20761,\"start\":20753},{\"end\":20769,\"start\":20761},{\"end\":20777,\"start\":20769},{\"end\":20787,\"start\":20777},{\"end\":21095,\"start\":21088},{\"end\":21101,\"start\":21095},{\"end\":21108,\"start\":21101},{\"end\":21115,\"start\":21108},{\"end\":21123,\"start\":21115},{\"end\":21131,\"start\":21123},{\"end\":21141,\"start\":21131},{\"end\":21432,\"start\":21425},{\"end\":21439,\"start\":21432},{\"end\":21448,\"start\":21439},{\"end\":21458,\"start\":21448},{\"end\":21730,\"start\":21722},{\"end\":21738,\"start\":21730},{\"end\":21746,\"start\":21738},{\"end\":21753,\"start\":21746},{\"end\":22050,\"start\":22044},{\"end\":22058,\"start\":22050},{\"end\":22065,\"start\":22058},{\"end\":22073,\"start\":22065},{\"end\":22080,\"start\":22073},{\"end\":22523,\"start\":22514},{\"end\":22530,\"start\":22523},{\"end\":22539,\"start\":22530},{\"end\":22545,\"start\":22539},{\"end\":22551,\"start\":22545},{\"end\":22558,\"start\":22551},{\"end\":22567,\"start\":22558},{\"end\":22574,\"start\":22567},{\"end\":22959,\"start\":22953},{\"end\":22967,\"start\":22959},{\"end\":22974,\"start\":22967},{\"end\":22982,\"start\":22974},{\"end\":22991,\"start\":22982},{\"end\":22998,\"start\":22991},{\"end\":23315,\"start\":23306},{\"end\":23324,\"start\":23315},{\"end\":23333,\"start\":23324},{\"end\":23342,\"start\":23333},{\"end\":23349,\"start\":23342},{\"end\":23625,\"start\":23619},{\"end\":23633,\"start\":23625},{\"end\":23641,\"start\":23633},{\"end\":23908,\"start\":23901},{\"end\":23916,\"start\":23908},{\"end\":23924,\"start\":23916},{\"end\":23931,\"start\":23924},{\"end\":24351,\"start\":24343},{\"end\":24359,\"start\":24351},{\"end\":24371,\"start\":24359},{\"end\":24380,\"start\":24371},{\"end\":24390,\"start\":24380},{\"end\":24709,\"start\":24701},{\"end\":24722,\"start\":24709},{\"end\":24732,\"start\":24722},{\"end\":25194,\"start\":25179},{\"end\":25208,\"start\":25194},{\"end\":25216,\"start\":25208},{\"end\":25224,\"start\":25216},{\"end\":25235,\"start\":25224},{\"end\":25243,\"start\":25235},{\"end\":25252,\"start\":25243},{\"end\":25261,\"start\":25252},{\"end\":25274,\"start\":25261},{\"end\":25706,\"start\":25694},{\"end\":25713,\"start\":25706},{\"end\":25719,\"start\":25713},{\"end\":25730,\"start\":25719},{\"end\":26199,\"start\":26187},{\"end\":26210,\"start\":26199},{\"end\":26220,\"start\":26210},{\"end\":26582,\"start\":26575},{\"end\":26591,\"start\":26582},{\"end\":26602,\"start\":26591},{\"end\":26610,\"start\":26602},{\"end\":26618,\"start\":26610},{\"end\":26625,\"start\":26618},{\"end\":26633,\"start\":26625},{\"end\":26643,\"start\":26633},{\"end\":26651,\"start\":26643},{\"end\":26663,\"start\":26651},{\"end\":27210,\"start\":27204},{\"end\":27218,\"start\":27210},{\"end\":27225,\"start\":27218},{\"end\":27232,\"start\":27225},{\"end\":27587,\"start\":27578},{\"end\":27593,\"start\":27587},{\"end\":27600,\"start\":27593},{\"end\":27608,\"start\":27600},{\"end\":27616,\"start\":27608},{\"end\":27623,\"start\":27616},{\"end\":27630,\"start\":27623},{\"end\":27636,\"start\":27630},{\"end\":27644,\"start\":27636},{\"end\":27650,\"start\":27644},{\"end\":27927,\"start\":27918},{\"end\":27938,\"start\":27927},{\"end\":27947,\"start\":27938},{\"end\":27957,\"start\":27947},{\"end\":28387,\"start\":28380},{\"end\":28398,\"start\":28387},{\"end\":28408,\"start\":28398},{\"end\":28417,\"start\":28408},{\"end\":28426,\"start\":28417},{\"end\":28670,\"start\":28663},{\"end\":28678,\"start\":28670},{\"end\":28684,\"start\":28678},{\"end\":28692,\"start\":28684},{\"end\":28699,\"start\":28692},{\"end\":28988,\"start\":28980},{\"end\":28995,\"start\":28988},{\"end\":29004,\"start\":28995},{\"end\":29011,\"start\":29004},{\"end\":29019,\"start\":29011},{\"end\":29027,\"start\":29019},{\"end\":29033,\"start\":29027},{\"end\":29368,\"start\":29361},{\"end\":29376,\"start\":29368},{\"end\":29383,\"start\":29376},{\"end\":29748,\"start\":29740},{\"end\":29759,\"start\":29748},{\"end\":29768,\"start\":29759},{\"end\":29783,\"start\":29768},{\"end\":29797,\"start\":29783},{\"end\":29805,\"start\":29797},{\"end\":29813,\"start\":29805},{\"end\":29820,\"start\":29813},{\"end\":29827,\"start\":29820},{\"end\":29840,\"start\":29827},{\"end\":30162,\"start\":30156},{\"end\":30169,\"start\":30162},{\"end\":30176,\"start\":30169},{\"end\":30184,\"start\":30176},{\"end\":30191,\"start\":30184},{\"end\":30597,\"start\":30590},{\"end\":30603,\"start\":30597},{\"end\":30611,\"start\":30603},{\"end\":30619,\"start\":30611},{\"end\":30625,\"start\":30619},{\"end\":30632,\"start\":30625},{\"end\":30638,\"start\":30632},{\"end\":30644,\"start\":30638},{\"end\":30652,\"start\":30644},{\"end\":30973,\"start\":30965},{\"end\":30980,\"start\":30973},{\"end\":30986,\"start\":30980},{\"end\":30995,\"start\":30986},{\"end\":31002,\"start\":30995},{\"end\":31302,\"start\":31293},{\"end\":31310,\"start\":31302},{\"end\":31317,\"start\":31310},{\"end\":31326,\"start\":31317},{\"end\":31335,\"start\":31326},{\"end\":31345,\"start\":31335},{\"end\":31840,\"start\":31832},{\"end\":31847,\"start\":31840},{\"end\":31854,\"start\":31847},{\"end\":32166,\"start\":32151},{\"end\":32176,\"start\":32166}]", "bib_venue": "[{\"end\":20804,\"start\":20787},{\"end\":21193,\"start\":21141},{\"end\":21423,\"start\":21388},{\"end\":21771,\"start\":21767},{\"end\":22143,\"start\":22080},{\"end\":22512,\"start\":22374},{\"end\":23015,\"start\":22998},{\"end\":23304,\"start\":23229},{\"end\":23652,\"start\":23641},{\"end\":24003,\"start\":23931},{\"end\":24341,\"start\":24225},{\"end\":24836,\"start\":24732},{\"end\":25318,\"start\":25274},{\"end\":25829,\"start\":25730},{\"end\":26277,\"start\":26220},{\"end\":26752,\"start\":26663},{\"end\":27312,\"start\":27232},{\"end\":27657,\"start\":27650},{\"end\":28038,\"start\":27957},{\"end\":28378,\"start\":28337},{\"end\":28661,\"start\":28600},{\"end\":28978,\"start\":28881},{\"end\":29439,\"start\":29383},{\"end\":29850,\"start\":29840},{\"end\":30252,\"start\":30191},{\"end\":30588,\"start\":30508},{\"end\":30963,\"start\":30881},{\"end\":31451,\"start\":31345},{\"end\":31903,\"start\":31854},{\"end\":32212,\"start\":32176},{\"end\":20817,\"start\":20806},{\"end\":23028,\"start\":23017},{\"end\":26828,\"start\":26754},{\"end\":28106,\"start\":28040},{\"end\":29856,\"start\":29852},{\"end\":30300,\"start\":30254}]"}}}, "year": 2023, "month": 12, "day": 17}