{"id": 186206974, "updated": "2023-09-28 03:19:20.395", "metadata": {"title": "Unsupervised Question Answering by Cloze Translation", "authors": "[{\"first\":\"Patrick\",\"last\":\"Lewis\",\"middle\":[]},{\"first\":\"Ludovic\",\"last\":\"Denoyer\",\"middle\":[]},{\"first\":\"Sebastian\",\"last\":\"Riedel\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Obtaining training data for Question Answering (QA) is time-consuming and resource-intensive, and existing QA datasets are only available for limited domains and languages. In this work, we explore to what extent high quality training data is actually required for Extractive QA, and investigate the possibility of unsupervised Extractive QA. We approach this problem by first learning to generate context, question and answer triples in an unsupervised manner, which we then use to synthesize Extractive QA training data automatically. To generate such triples, we first sample random context paragraphs from a large corpus of documents and then random noun phrases or Named Entity mentions from these paragraphs as answers. Next we convert answers in context to \u201cfill-in-the-blank\u201d cloze questions and finally translate them into natural questions. We propose and compare various unsupervised ways to perform cloze-to-natural question translation, including training an unsupervised NMT model using non-aligned corpora of natural questions and cloze questions as well as a rule-based approach. We find that modern QA models can learn to answer human questions surprisingly well using only synthetic training data. We demonstrate that, without using the SQuAD training data at all, our approach achieves 56.4 F1 on SQuAD v1 (64.5 F1 when the answer is a Named Entity mention), outperforming early supervised models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1906.04980", "mag": "3104467951", "acl": "P19-1484", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/LewisDR19", "doi": "10.18653/v1/p19-1484"}}, "content": {"source": {"pdf_hash": "745f6caff725355d1f30cbc0ac4378757f56bf34", "pdf_src": "Anansi", "pdf_uri": "[\"https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/78642687_476509739626215_2913854586978566144_n.pdf?_nc_cat=110&_nc_ht=scontent-sea1-1.xx&_nc_oc=AQkciO02R9t5o3uJbpqGvf_51I7W6Cxp8cxfxgonq7a-UKGjVjuPYmH9sNjUOFTcU-0&_nc_sid=ae5e01&oe=5EAFC783&oh=18d4a1fc88482a1be0b422858ed71773\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/P19-1484.pdf", "status": "HYBRID"}}, "grobid": {"id": "8013e56ac13f1563d0a7ce7fd43b992dbe0ee98f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/745f6caff725355d1f30cbc0ac4378757f56bf34.txt", "contents": "\nUnsupervised Question Answering by Cloze Translation\n\n\nPatrick Lewis plewis@fb.com \nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nUniversity College London\nUniversity College London\n\n\nLudovic Denoyer denoyer@fb.com \nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nUniversity College London\nUniversity College London\n\n\nSebastian Riedel sriedel@fb.com \nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nUniversity College London\nUniversity College London\n\n\nUnsupervised Question Answering by Cloze Translation\n\nObtaining training data for Question Answering (QA) is time-consuming and resourceintensive, and existing QA datasets are only available for limited domains and languages. In this work, we explore to what extent high quality training data is actually required for Extractive QA, and investigate the possibility of unsupervised Extractive QA. We approach this problem by first learning to generate context, question and answer triples in an unsupervised manner, which we then use to synthesize Extractive QA training data automatically. To generate such triples, we first sample random context paragraphs from a large corpus of documents and then random noun phrases or named entity mentions from these paragraphs as answers. Next we convert answers in context to \"fill-in-the-blank\" cloze questions and finally translate them into natural questions. We propose and compare various unsupervised ways to perform cloze-tonatural question translation, including training an unsupervised NMT model using nonaligned corpora of natural questions and cloze questions as well as a rule-based approach. We find that modern QA models can learn to answer human questions surprisingly well using only synthetic training data. We demonstrate that, without using the SQuAD training data at all, our approach achieves 56.4 F1 on SQuAD v1 (64.5 F1 when the answer is a Named entity mention), outperforming early supervised models.\n\nIntroduction\n\nExtractive Question Answering (EQA) is the task of answering questions given a context document under the assumption that answers are spans of tokens within the given document. There has been substantial progress in this task in English. For SQuAD (Rajpurkar et al., 2016), a common EQA benchmark dataset, current models beat human Figure 1: A schematic of our approach. The right side (dotted arrows) represents traditional EQA. We introduce unsupervised data generation (left side, solid arrows), which we use to train standard EQA models performance; For SQuAD 2.0 (Rajpurkar et al., 2018), ensembles based on BERT (Devlin et al., 2018) now match human performance. Even for the recently introduced Natural Questions corpus (Kwiatkowski et al., 2019), human performance is already in reach. In all these cases, very large amounts of training data are available. But, for new domains (or languages), collecting such training data is not trivial and can require significant resources. What if no training data was available at all?\n\nIn this work we address the above question by exploring the idea of unsupervised EQA, a setting in which no aligned question, context and answer data is available. We propose to tackle this by reduction to unsupervised question generation: If we had a method, without using QA supervision, to generate accurate questions given a context document, we could train a QA system using the generated questions. This approach allows us to directly leverage progress in QA, such as model architectures and pretraining routines. This framework is attractive in both its flexibility and extensibility. In addition, our method can also be used to generate additional training data in semi-supervised settings.\n\nOur proposed method, shown schematically in Figure 1, generates EQA training data in three steps. 1) We first sample a paragraph in a target domain-in our case, English Wikipedia. 2) We sample from a set of candidate answers within that context, using pretrained components (NER or noun chunkers) to identify such candidates. These require supervision, but no aligned (question, answer) or (question, context) data. Given a candidate answer and context, we can extract \"fillthe-blank\" cloze questions 3) Finally, we convert cloze questions into natural questions using an unsupervised cloze-to-natural question translator.\n\nThe conversion of cloze questions into natural questions is the most challenging of these steps. While there exist sophisticated rule-based systems (Heilman and Smith, 2010) to transform statements into questions (for English), we find their performance to be empirically weak for QA (see Section 3). Moreover, for specific domains or other languages, a substantial engineering effort will be required to develop similar algorithms. Also, whilst supervised models exist for this task, they require the type of annotation unavailable in this setting (Du et al. 2017;Du and Cardie 2018;Hosking and Riedel 2019, inter alia). We overcome this issue by leveraging recent progress in unsupervised machine translation Lample and Conneau, 2019;Artetxe et al., 2018). In particular, we collect a large corpus of natural questions and an unaligned corpus of cloze questions, and train a seq2seq model to map between natural and cloze question domains using a combination of online back-translation and de-noising auto-encoding.\n\nIn our experiments, we find that in conjunction with the use of modern QA model architectures, unsupervised QA can lead to performances surpassing early supervised approaches (Rajpurkar et al., 2016). We show that forms of cloze \"translation\" that produce (unnatural) questions via word removal and flips of the cloze question lead to better performance than an informed rule-based translator. Moreover, the unsupervised seq2seq model outperforms both the noise and rule-based system. We also demonstrate that our method can be used in a few-shot learning setting, for example obtaining 59.3 F1 with 32 labelled examples, compared to 40.0 F1 without our method.\n\nTo summarize, this paper makes the following contributions: i) The first approach for unsupervised QA, reducing the problem to unsupervised cloze translation, using methods from unsupervised machine translation ii) Extensive experiments testing the impact of various cloze question translation algorithms and assumptions iii) Experiments demonstrating the application of our method for few-shot learning in EQA. 1\n\n\nUnsupervised Extractive QA\n\nWe consider extractive QA where we are given a question q and a context paragraph c and need to provide an answer a = (b, e) with beginning b and end e character indices in c. Figure 1 (right-hand side) shows a schematic representation of this task.\n\nWe propose to address unsupervised QA in a two stage approach. We first develop a generative model p(q, a, c) using no (QA) supervision, and then train a discriminative model p r (a|q, c) using p as training data generator. The generator p(q, a, c) = p(c)p(a|c)p(q|a, c) will generate data in a \"reverse direction\", first sampling a context via p(c), then an answer within the context via p(a|c) and finally a question for the answer and context via p(q|a, c). In the following we present variants of these components.\n\n\nContext and Answer Generation\n\nGiven a corpus of documents our context generator p(c) uniformly samples a paragraph c of appropriate length from any document, and the answer generation step creates answer spans a for c via p(a|c). This step incorporates prior beliefs about what constitutes good answers. We propose two simple variants for p(a|c):\n\nNoun Phrases We extract all noun phrases from paragraph c and sample uniformly from this set to generate a possible answer span. This requires a chunking algorithm for our language and domain.\n\n\nNamed Entities\n\nWe can further restrict the possible answer candidates and focus entirely on named entities. Here we extract all named entity mentions using an NER system and then sample uniformly from these. Whilst this reduces the variety of questions that can be answered, it proves to be empirically effective as discussed in Section 3.2.\n\n\nQuestion Generation\n\nArguably, the core challenge in QA is modelling the relation between question and answer. This is captured in the question generator p(q|a, c) that produces questions from a given answer in context. We divide this step into two steps: cloze generation q = cloze(a, c) and translation, p(q|q ).\n\n\nCloze Generation\n\nCloze questions are statements with the answer masked. In the first step of cloze generation, we reduce the scope of the context to roughly match the level of detail of actual questions in extractive QA. A natural option is the sentence around the answer. Using the context and answer from Figure 1, this might leave us with the sentence \"For many years the London Sevens was the last tournament of each season but the Paris Sevens became the last stop on the calendar in \". We can further reduce length by restricting to subclauses around the answer, based on access to an English syntactic parser, leaving us with \"the Paris Sevens became the last stop on the calendar in \".\n\n\nCloze Translation\n\nOnce we have generated a cloze question q we translate it into a form closer to what we expect in real QA tasks. We explore four approaches here.\n\nIdentity Mapping We consider that cloze questions themselves provide a signal to learn some form of QA behaviour. To test this hypothesis, we use the identity mapping as a baseline for cloze translation. To produce \"questions\" that use the same vocabulary as real QA tasks, we replace the mask token with a wh* word (randomly chosen or with a simple heuristic described in Section 2.4).\n\nNoisy Clozes One way to characterize the difference between cloze and natural questions is as a form of perturbation. To improve robustness to pertubations, we can inject noise into cloze questions. We implement this as follows. First we delete the mask token from cloze q , apply a simple noise function from , and prepend a wh* word (randomly or with the heuristic in Section 2.4) and append a question mark. The noise function consists of word dropout, word order permutation and word masking. The motivation is that, at least for SQuAD, it may be sufficient to simply learn a function to identify a span surrounded by high n-gram overlap to the question, with a tolerance to word order perturbations.\n\nRule-Based Turning an answer embedded in a sentence into a (q, a) pair can be understood as a syntactic transformation with wh-movement and a type-dependent choice of wh-word. For English, off-the-shelf software exists for this purpose. We use the popular statement-to-question generator from Heilman and Smith (2010) which uses a set of rules to generate many candidate questions, and a ranking system to select the best ones.\n\n\nSeq2Seq\n\nThe above approaches either require substantial engineering and prior knowledge (rulebased) or are still far from generating naturallooking questions (identity, noisy clozes). We propose to overcome both issues through unsupervised training of a seq2seq model that translates between cloze and natural questions. More details of this approach are in Section 2.4.\n\n\nQuestion Answering\n\nExtractive Question Answering amounts to finding the best answer a given question q and context c. We have at least two ways to achieve this using our generative model:\n\nTraining a separate QA system The generator is a source of training data for any QA architecture at our disposal. Whilst the data we generate is unlikely to match the quality of real QA data, we hope QA models will learn basic QA behaviours.\n\nUsing Posterior Another way to extract the answer is to find a with the highest posterior p(a|c, q). Assuming uniform answer probabilities conditioned on context p(a|c), this amounts to calculating arg max a p(q|a , c) by testing how likely each possible candidate answer could have generated the question, a similar method to the supervised approach of Lewis and Fan (2019).\n\n\nUnsupervised Cloze Translation\n\nTo train a seq2seq model for cloze translation we borrow ideas from recent work in unsupervised Neural Machine Translation (NMT). At the heart of most these approaches are nonparallel corpora of source and target language sentences. In such corpora, no source sentence has any translation in the target corpus and vice versa. Concretely, in our setting, we aim to learn a function which maps between the question (target) and cloze question (source) domains without requiring aligned corpora. For this, we need large corpora of cloze questions C and natural questions Q.\n\nCloze Corpus We create the cloze corpus C by applying the procedure outlined in Section 2.2.2. Specifically we consider Noun Phrase (NP) and Named Entity mention (NE) answer spans, and cloze question boundaries set either by the sentence or sub-clause that contains the answer. 2 We extract 5M cloze questions from randomly sampled wikipedia paragraphs, and build a corpus C for each choice of answer span and cloze boundary technique.\n\nWhere there is answer entity typing information (i.e. NE labels), we use type-specific mask tokens to represent one of 5 high level answer types. See Appendix A.1 for further details.\n\nQuestion Corpus We mine questions from English pages from a recent dump of common crawl using simple selection criteria: 3 We select sentences that start in one of a few common wh* words, (\"how much\", \"how many\", \"what\", \"when\", \"where\" and \"who\") and end in a question mark. We reject questions that have repeated question marks or \"?!\", or are longer than 20 tokens. This process yields over 100M english questions when deduplicated. Corpus Q is created by sampling 5M questions such that there are equal numbers of questions starting in each wh* word. Following , we use C and Q to train translation models p s\u2192t (q|q ) and p t\u2192s (q |q) which translate cloze questions into natural questions and vice-versa. This is achieved by a combination of in-domain training via denoising autoencoding and cross-domain training via online-backtranslation. This could also be viewed as a style transfer task, similar to Subramanian et al. (2018). At inference time, 'natural' questions are generated from cloze questions as arg max q p s\u2192t (q|q ). 4 Further experimental detail can be found in Appendix A.2.\n\nWh* heuristic In order to provide an appropriate wh* word for our \"identity\" and \"noisy cloze\" baseline question generators, we introduce a simple heuristic rule that maps each answer type to the most appropriate wh* word. For example, the \"TEMPORAL\" answer type is mapped to \"when\". During experiments, we find that the unsupervised NMT translation functions sometimes generate inappropriate wh* words for the answer entity type, so we also experiment with applying the wh* heuristic to these question generators. For the NMT models, we apply the heuristic by prepending target questions with the answer type token mapped to their wh* words at training time. E.g. questions that start with \"when\" are prepended with the token \"TEMPORAL\". Further details on the wh* heuristic are in Appendix A.3.\n\n\nExperiments\n\nWe want to explore what QA performance can be achieved without using aligned q, a data, and how this compares to supervised learning and other approaches which do not require training data. Furthermore, we seek to understand the impact of different design decisions upon QA performance of our system and to explore whether the approach is amenable to few-shot learning when only a few q,a pairs are available. Finally, we also wish to assess whether unsupervised NMT can be used as an effective method for question generation.\n\n\nUnsupervised QA Experiments\n\nFor the synthetic dataset training method, we consider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). 5 For the posterior maximisation method, we extract cloze questions from both sentences and sub-clauses, and use the NMT models to estimate p(q|c, a). We evaluate using the standard Exact Match (EM) and F1 metrics.\n\nAs we cannot assume access to a development dataset when training unsupervised models, the QA model training is halted when QA performance on a held-out set of synthetic QA data plateaus. We do, however, use the SQuAD development set to assess which model components are  (Dhingra et al., 2018) 3.2 \u2020 6.8 \u2020 BiDAF+SA (Dhingra et al., 2018) \u2021 10.0* 15.0* BERT-Large (Dhingra et al., 2018) \u2021 28.4* 35.8*\n\n\nBaselines\n\n\nEM F1\n\nSliding window (Rajpurkar et al., 2016) 13.0 20.0 Context-only (Kaushik and Lipton, 2018) 10.9 14.8 Random (Rajpurkar et al., 2016) 1.3 4.3\n\nFully Supervised Models EM F1\n\nBERT-Large (Devlin et al., 2018) 84.1 90.9 BiDAF+SA (Clark and Gardner, 2017) 72.1 81.1 Log. Reg. + FE (Rajpurkar et al., 2016) 40.4 51.0 Table 1: Our best performing unsupervised QA models compared to various baselines and supervised models. * indicates results on SQuAD dev set. \u2020 indicates results on non-standard test set created by Dhingra et al. (2018). \u2021 indicates our re-implementation important (Section 3.2). To preserve the integrity of the SQuAD test set, we only submit our best performing system to the test server.\n\nWe shall compare our results to some published baselines. Rajpurkar et al. (2016) use a supervised logistic regression model with feature engineering, and a sliding window approach that finds answers using word overlap with the question. Kaushik and Lipton (2018) train (supervised) models that disregard the input question and simply extract the most likely answer span from the context. To our knowledge, ours is the first work to deliberately target unsupervised QA on SQuAD. Dhingra et al. (2018) focus on semi-supervised QA, but do publish an unsupervised evaluation. To enable fair comparison, we re-implement their approach using their publicly available data, and train a variant with BERT-Large. 6 Their approach also uses cloze questions, but without translation, and heavily relies on the structure of wikipedia articles.\n\nOur best approach attains 54.7 F1 on the SQuAD test set; an ensemble of 5 models (different seeds) achieves 56.4 F1. Table 1 shows the result in context of published baselines and supervised results. Our approach significantly outperforms baseline systems and Dhingra et al. (2018) and surpasses early supervised methods.\n\n\nAblation Studies and Analysis\n\nTo understand the different contributions to the performance, we undertake an ablation study. All ablations are evaluated using the SQUAD development set. We ablate using BERT-Base and BiDAF+SA, and our best performing setup is then used to fine-tune a final BERT-Large model, which is the model in Table 1. All experiments with BERT-Base were repeated with 3 seeds to account for some instability encountered in training; we report mean results. Results are shown in Table 2, and observations and aggregated trends are highlighted below.\n\nPosterior Maximisation vs. Training on generated data Comparing Posterior Maximisation with BERT-Base and BiDAF+SA columns in Table 2 shows that training QA models is more effective than maximising question likelihood. As shown later, this could partly be attributed to QA models being able to generalise answer spans, returning answers at test-time that are not always named entity mentions. BERT models also have the advantage of linguistic pretraining, further adding to generalisation ability.\n\nEffect of Answer Prior Named Entities (NEs) are a more effective answer prior than noun phrases (NPs). Equivalent BERT-Base models trained with NEs improve on average by 8.9 F1 over NPs. Rajpurkar et al. (2016) estimate 52.4% of answers in SQuAD are NEs, whereas (assuming NEs are a subset of NPs), 84.2% are NPs. However, we found that there are on average 14 NEs per context compared to 33 NPs, so using NEs in training may help reduce the search space of possible answer candidates a model must consider.\n\nEffect of Question Length and Overlap As shown in Figure 2, using sub-clauses for generation leads to shorter questions and shorter common subsequences to the context, which more closely match the distribution of SQuAD questions. Reducing the length of cloze questions helps the translation components produce simpler, more precise questions. Using sub-clauses leads to, on average +4.0 F1 across equivalent sentencelevel BERT-Base models. The \"noisy cloze\" generator produces shorter questions than the NMT model due to word dropout, and shorter common subsequences due to the word perturbation noise.   Effect of Cloze Translation Noise acts as helpful regularization when comparing the \"identity\" cloze translation functions to \"noisy cloze\", (mean +9.8 F1 across equivalent BERT-Base models). Unsupervised NMT question translation is also helpful, leading to a mean improvement of 1.8 F1 on BERT-Base for otherwise equivalent \"noisy cloze\" models. The improvement over noisy clozes is surprisingly modest, and is discussed in more detail in Section 5.\n\nEffect of QA model BERT-Base is more effective than BiDAF+SA (an architecture specifically designed for QA). BERT-Large (not shown in Table 2) gives a further boost, improving our best configuration by 6.9 F1.\n\nEffect of Rule-based Generation QA models trained on QA datasets generated by the Rule-   (2010) do not perform favourably compared to our NMT approach. To test whether this is due to different answer types used, we a) remove questions of their system that are not consistent with our (NE) answers, and b) remove questions of our system that are not consistent with their answers. Table 3 shows that while answer types matter in that using our restrictions help their system, and using their restrictions hurts ours, they cannot fully explain the difference. The RB system therefore appears to be unable to generate the variety of questions and answers required for the task, and does not generate questions from a sufficient variety of contexts. Also, whilst on average, question lengths are shorter for the RB model than the NMT model, the distribution of longest common sequences are similar, as shown in Figure 2, perhaps suggesting that the RB system copies a larger proportion of its input.\n\n\nError Analysis\n\nWe find that the QA model predicts answer spans that are not always detected as named entity mentions (NEs) by the NER tagger, despite being trained with solely NE answer spans. In fact, when we split SQuAD into questions where the correct answer is an automatically-tagged NE, our model's performance improves to 64.5 F1, but it still achieves 47.9 F1 on questions which do not have automatically-tagged NE answers (not shown in our tables). We attribute this to the effect of BERT's linguistic pretraining allowing it to generalise the semantic role played by NEs in a sentence rather than simply learning to mimic the NER system. An equivalent BiDAF+SA model scores 58.9 F1 when the answer is an NE but drops severely to 23.0 F1 when the answer is not an NE. Figure 3 shows the performance of our system for different kinds of question and answer type. The model performs best with \"when\" questions which tend to have fewer potential answers, but struggles with \"what\" questions, which have a broader range of answer semantic types, and hence more plausible answers per context. The model performs well on \"TEMPORAL\" answers, consistent with the good performance of \"when\" questions.\n\n\nUNMT-generated Question Analysis\n\nWhilst our main aim is to optimise for downstream QA performance, it is also instructive to examine the output of the unsupervised NMT cloze translation system. Unsupervised NMT has been used in monolingual settings (Subramanian et al., 2018), but cloze-to-question generation presents new challenges -The cloze and question are asymmetric in terms of word length, and successful translation must preserve the answer, not just superficially transfer style. Figure 4 shows that without the wh* heuristic, the model learns to generate questions with broadly appropriate wh* words for the answer type, but can struggle, par-ticularly with Person/Org/Norp and Numeric answers. Table 4 shows representative examples from the NE unsupervised NMT model. The model generally copies large segments of the input. Also shown in Figure 2, generated questions have, on average, a 9.1 token contiguous sub-sequence from the context, corresponding to 56.9% of a generated question copied verbatim, compared to 4.7 tokens (46.1%) for SQuAD questions. This is unsurprising, as the backtranslation training objective is to maximise the reconstruction of inputs, encouraging conservative translation.\n\nThe model exhibits some encouraging, nontrivial syntax manipulation and generation, particularly at the start of questions, such as example 7 in Table 4, where word order is significantly modified and \"sold\" is replaced by \"buy\". Occasionally, it hallucinates common patterns in the question corpus (example 6). The model can struggle with lists (example 4), and often prefers present tense and second person (example 5). Finally, semantic drift is an issue, with generated questions being relatively coherent but often having different answers to the inputted cloze questions (example 2).\n\nWe can estimate the quality and grammaticality of generated questions by using the well-formed question dataset of Faruqui and Das (2018). This dataset consists of search engine queries annotated with whether the query is a well-formed question or not. We train a classifier on this task, and then measure how many questions are classified as \"well-formed\" for our question generation methods. Full details are given in Appendix A.5. We find that 68% of questions generated by UNMT model are classified as well-formed, compared to 75.6% for the rule-based system and 92.3% for SQuAD questions. We also note that using language model pretraining improves the quality of questions generated by UNMT model, with 78.5% classified as well-formed, surpassing the rule-based system (see Appendix A.6).\n\n\nFew-Shot Question Answering\n\nFinally, we consider a few-shot learning task with very limited numbers of labelled training examples. We follow the methodology of Dhingra et al. (2018) and Yang et al. (2017), training on a small number of training examples and using a development set for early stopping. We use the splits made # Cloze Question  available by Dhingra et al. (2018), but switch the development and test splits, so that the test split has n-way annotated answers. We first pretrain a BERT-large QA model using our best configuration from Section 3, then fine-tune with a small amount of SQuAD training data. We compare this to our re-implementation of Dhingra et al. (2018), and training the QA model directly on the available data without unsupervised QA pretraining. Figure 5 shows performance for progressively larger amounts of training data. As with Dhingra et al. (2018), our numbers are attained using a development set for early stopping that can be larger than the training set. Hence this is not a true reflection of performance in low data regimes, but does allow for comparative analysis between models. We find our approach performs best in very data poor regimes, and similarly to Dhingra et al. (2018) with modest amounts of data. We also note BERT-Large itself is remarkably efficient, reaching \u223c60% F1 with only 1% of the available data.\n\n\nRelated Work\n\nUnsupervised Learning in NLP Most representation learning approaches use latent variables (Hofmann, 1999;Blei et al., 2003), or language  (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014;Radford et al., 2018;Devlin et al., 2018). Most relevant to us is unsupervised NMT Artetxe et al., 2018) and style transfer (Subramanian et al., 2018). We build upon this work, but instead of using models directly, we use them for training data generation. Radford et al. (2019) report that very powerful language models can be used to answer questions from a conversational QA task, CoQA (Reddy et al., 2018) in an unsupervised manner. Their method differs significantly to ours, and may require \"seeding\" from QA dialogs to encourage the language model to generate answers.\n\nSemi-supervised QA Yang et al. (2017) train a QA model and also generate new questions for greater data efficiency, but require labelled data. Dhingra et al. (2018) simplify the approach and remove the supervised requirement for question generation, but do not target unsupervised QA or attempt to generate natural questions. They also make stronger assumptions about the text used for question generation and require Wikipedia summary paragraphs. Wang et al. (2018) consider semi-supervised cloze QA,  use semi-supervision to improve semantic parsing on WebQuestions (Berant et al., 2013), andLei et al. (2016) leverage semi-supervision for question similarity modelling. Finally, injecting external knowledge into QA systems could be viewed as semi-supervision, and Weissenborn et al. (2017) and Mihaylov and Frank (2018) use Conceptnet (Speer et al., 2016) for QA tasks.\n\nQuestion Generation has been tackled with pipelines of templates and syntax rules (Rus et al., 2010). Heilman and Smith (2010) augment this with a model to rank generated questions, and Yao et al. (2012) and Olney et al. (2012) investigate symbolic approaches. Recently there has been interest in question generation using supervised neural models, many trained to generate questions from c, a pairs in SQuAD (Du et al., 2017;Yuan et al., 2017;Du and Cardie, 2018;Hosking and Riedel, 2019) \n\n\nDiscussion\n\nIt is worth noting that to attain our best performance, we require the use of both an NER system, indirectly using labelled data from OntoNotes 5, and a constituency parser for extracting subclauses, trained on the Penn Treebank (Marcus et al., 1994). 7 Moreover, a language-specific wh* heuristic was used for training the best performing NMT models. This limits the applicability and flexibility of our best-performing approach to domains and languages that already enjoy extensive linguistic resources (named entity recognition and treebank datasets), as well as requiring some human engineering to define new heuristics.\n\nNevertheless, our approach is unsupervised from the perspective of requiring no labelled (question, answer) or (question, context) pairs, which are usually the most challenging aspects of annotating large-scale QA training datasets.\n\nWe note the \"noisy cloze\" system, consisting of very simple rules and noise, performs nearly as well as our more complex best-performing system, despite the lack of grammaticality and syntax associated with questions. The questions generated by the noisy cloze system also perform poorly on the \"well-formedness\" analysis mentioned in Sec-7 Ontonotes 5: https://catalog.ldc.upenn. edu/LDC2013T19 tion 3.4, with only 2.7% classified as well-formed. This intriguing result suggests natural questions are perhaps less important for SQuAD and strong question-context word matching is enough to do well, reflecting work from Jia and Liang (2017) who demonstrate that even supervised models rely on word-matching.\n\nAdditionally, questions generated by our approach require no multi-hop or multi-sentence reasoning, but can still be used to achieve non-trivial SQuAD performance. Indeed, Min et al. (2018) note 90% of SQuAD questions only require a single sentence of context, and Sugawara et al. (2018) find 76% of SQuAD has the answer in the sentence with highest token overlap to the question.\n\n\nConclusion\n\nIn this work, we explore whether it is possible to to learn extractive QA behaviour without the use of labelled QA data. We find that it is indeed possible, surpassing simple supervised systems, and strongly outperforming other approaches that do not use labelled data, achieving 56.4% F1 on the popular SQuAD dataset, and 64.5% F1 on the subset where the answer is a named entity mention. However, we note that whilst our results are encouraging on this relatively simple QA task, further work is required to handle more challenging QA elements and to reduce our reliance on linguistic resources and heuristics. Cloze questions are featurized as follows. Assume we have a cloze question extracted from a paragraph \"the Paris Sevens became the last stop on the calendar in .\", and the answer \"2018\". We first tokenize the cloze question, and discard it if it is longer than 40 tokens. We then replace the \"blank\" with a special mask token. If the answer was extracted using the noun phrase chunker, there is no specific answer entity typing so we just use a single mask token \"MASK\". However, when we use the named entity answer generator, answers have a named entity label, which we can use to give the cloze translator a high level idea of the answer semantics. In the example above, the answer \"2018\" has the named entity type \"DATE\". We group fine grained entity types into higher level categories, each with its own masking token as shown in Table 5, and so the mask token for this example is \"TEMPORAL\".\n\n\nA.2 Unsupervised NMT Training Setup Details\n\nHere we describe experimental details for unsupervised NMT setup. We use the English tokenizer from Moses (Koehn et al., 2007), and use FastBPE (https://github.com/ glample/fastBPE) to split into subword units, with a vocabulary size of 60000. The architecture uses a 4-layer transformer encoder and 4-layer transformer decoder, where one layer is language specific for both the encoder and decoder, the rest are shared. We use the standard hyperparameter settings recommended by . The models are initialised with random weights, and the input word embedding matrix is initialised using FastText vectors (Bojanowski et al., 2016) trained on the concatenation of the C and Q corpora. Initially, the auto-encoding loss and backtranslation loss have equal weight, with the autoencoding loss coefficient reduced to 0.1 by 100K steps and to 0 by 300k steps. We train using 5M cloze questions and natural questions, and cease training when the BLEU scores between backtranslated and input questions stops improving, usually around 300K optimisation steps. When generating, we decode greedily, and note that decoding with a beam size of 5 did not significantly change downstream QA performance, or greatly change the fluency of generations.\n\n\nA.3 Wh* Heuristic\n\nWe defined a heuristic to encourage appropriate wh* words for the inputted cloze question's answer type. This heuristic is used to provide a relevant wh* word for the \"noisy cloze\" and \"identity\" baselines, as well as to assist the NMT model to produce more precise questions. To this end, we map each high level answer category to the most appropriate wh* word, as shown on the right hand column of Table 5 (In the case of NUMERIC types, we randomly choose between \"How much\" and \"How many\"). Before training, we prepend the high level answer category masking token to the start of questions that start with the corresponding wh* word, e.g. the question \"Where is Mount Vesuvius?\" would be transformed into \"PLACE Where is Mount Vesuvius ?\". This allows the model to learn a much stronger association between the wh* word and answer mask type.\n\n\nA.4 QA Model Setup Details\n\nWe train BiDAF + Self Attention using the default settings. We evaluate using a synthetic development set of data generated from 1000 context paragraphs every 500 training steps, and halt when the performance has not changed by 0.1% for the last 5 evaluations. We train BERT-Base and BERT-Large with a batch size of 16, and the default learning rate hyperparameters. For BERT-Base, we evaluate using a synthetic development set of data generated from 1000 context paragraphs every 500 training steps, and halt when the performance has not changed by 0.1% for the last 5 evaluations. For BERT-Large, due to larger model size, training takes longer, so we manually halt training when the synthetic development set performance plateaus, rather than using the automatic early stopping.\n\n\nA.5 Question Well-Formedness\n\nWe can estimate how well-formed the questions generated by various configurations of our model are using the Well-formed query dataset of Faruqui and Das (2018). This dataset consists of 25,100  Rule-Based (Heilman and Smith, 2010) 75.6\n\nSQuAD Questions (Rajpurkar et al., 2016) 92.3 Table 6: Fraction of questions classified as \"wellformed\" by a classifier trained on the dataset of Faruqui and Das (2018) for different question generation models. * indicates MLM pretraining was applied before UNMT training search engine queries, annotated with whether the query is a well-formed question. We train a BERT-Base classifier on the binary classification task, achieving a test set accuracy of 80.9% (compared to the previous state of the art of 70.7%). We then use this classifier to measure what proportion of questions generated by our models are classified as \"well-formed\". Table 6 shows the full results. Our best unsupervised question generation configuration achieves 68.0%, demonstrating the model is capable of generating relatively well-formed questions, but there is room for improvement, as the rule-based generator achieves 75.6%. MLM pretraining (see Appendix A.6) greatly improves the well-formedness score. The classifier predicts that 92.3% of SQuAD questions are well-formed, suggesting it is able to detect high quality questions. The classifier appears to be sensitive to fluency and grammar, with the \"identity\" cloze translation models scoring much higher than their \"noisy cloze\" counterparts.\n\n\nA.6 Language Model Pretraining\n\nWe experimented with Masked Language Model (MLM) pretraining of the translation models, p s\u2192t (q|q ) and p t\u2192s (q |q). We use the XLM implementation (https://github. com/facebookresearch/XLM) and use default hyperparameters for both MLM pretraining and and unsupervised NMT fine-tuning. The UNMT encoder is initialized with the MLM model's parameters, and the decoder is randomly initialized. We find translated questions to be qualitatively more fluent and abstractive than the those from the models used in the main paper. Table 6 supports this observation, demonstrating that questions produced by models with MLM pretraining are classified as well-formed 10.5% more often than those without pretraining, surpassing the rule-based question generator of Heilman and Smith (2010). However, using MLM pretraining did not lead to significant differences for question answering performance (the main focus of this paper), so we leave a thorough investigation into language model pretraining for unsupervised question answering as future work. Table 4 shows examples of cloze question translations from our model, but due to space constraints, only a few examples can be shown there. Table 7 shows many more examples.\n\n\nA.7 More Examples of Unsupervised NMT Cloze Translations\n\nFigure 2 :\n2Lengths (blue, hashed)  and longest common subsequence with context (red, solid) for SQuAD questions and various question generation methods.\n\nFigure 3 :\n3Breakdown of performance for our best QA model on SQuAD for different question types (left) and different NE answer categories (right)\n\nFigure 5 :\n5F1 score on the SQuAD development set for progressively larger training dataset sizes model-inspired criteria\n\nTable 2 :\n2Ablations on the SQuAD development set. \"Wh* Heuristic\" indicates if a heuristic was used to choose sensible Wh* words during cloze translation. NE and NP refer to named entity mention and noun phrase answer generation.\n\nTable 3 :\n3Ablations on SQuAD development set probing the performance of the rule based system.based (RB) system of Heilman and Smith\n\nTable 4 :\n4Examples of cloze translations for the UNMT model using the wh* heuristic and subclause cloze extraction. More examples can be found in appendix A.7 Figure 4: Wh* words generated by the UNMT model for cloze questions with different answer types.\n\n\non Empirical Methods in Natural Language Processing, pages 5039-5049, Brussels, Belgium. Association for Computational Linguistics.Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi \nJaakkola, Kateryna Tymoshenko, Alessandro Mos-\nchitti, and Llus Mrquez. 2016. Semi-supervised \nQuestion Retrieval with Gated Convolutions. In \nProceedings of the 2016 Conference of the North \nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, \npages 1279-1289, San Diego, California. Associa-\ntion for Computational Linguistics. \n\nMike Lewis and Angela Fan. 2019. Generative ques-\ntion answering: Learning to answer the whole ques-\ntion. In International Conference on Learning Rep-\nresentations. \n\nMitchell Marcus, \nGrace Kim, \nMary Ann \nMarcinkiewicz, Robert MacIntyre, Ann Bies, \nMark Ferguson, Karen Katz, and Britta Schas-\nberger. 1994. The Penn Treebank: annotating \npredicate argument structure. In Proceedings of \nthe workshop on Human Language Technology -\nHLT '94, page 114, Plainsboro, NJ. Association for \nComputational Linguistics. \n\nTodor Mihaylov and Anette Frank. 2018. Knowledge-\nable Reader: Enhancing Cloze-Style Reading Com-\nprehension with External Commonsense Knowl-\nedge. In Proceedings of the 56th Annual Meeting of \nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 821-832, Melbourne, \nAustralia. Association for Computational Linguis-\ntics. \n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their composition-\nality. In C. J. C. Burges, L. Bottou, M. Welling, \nZ. Ghahramani, and K. Q. Weinberger, editors, Ad-\nvances in Neural Information Processing Systems \n26, pages 3111-3119. Curran Associates, Inc. \nSupplementary Materials for ACL 2019 Paper: \nUnsupervised Question Answering by Cloze Translation \n\nA Appendices \n\nA.1 Cloze Question Featurization and \nTranslation \n\n\n\n\nHigh Level Answer Category Named Entity labels Most appropriate wh* PERSON/NORP/ORG PERSON, NORP, ORG Who PLACE GPE, LOC, FAC Where THING PRODUCT, EVENT, WORKOFART, LAW, LANGUAGE What TEMPORAL TIME, DATE When NUMERIC PERCENT, MONEY, QUANTITY, ORDINAL, CARDINAL How much/How many\n\nTable 5 :\n5High level answer categories for the different named entity labelsCloze \nAnswer \n\nCloze \nBoundary \n\nCloze \nTranslation \n\nWh* \nHeuristic \n\n% Well-\nformed \n\nNE \nSub-clause \nUNMT \n68.0 \nNE \nSub-clause \nUNMT \n\u00d7 \n65.3 \nNE \nSentence \nUNMT \n\u00d7 \n61.3 \nNP \nSentence \nUNMT \n\u00d7 \n61.9 \n\nNE \nSub-clause Noisy Cloze \n2.7 \nNE \nSub-clause Noisy Cloze \n\u00d7 \n2.4 \nNE \nSentence Noisy Cloze \n\u00d7 \n0.7 \nNP \nSentence Noisy Cloze \n\u00d7 \n0.8 \n\nNE \nSub-clause \nIdentity \n30.8 \nNE \nSub-clause \nIdentity \n\u00d7 \n20.0 \nNE \nSentence \nIdentity \n\u00d7 \n49.5 \nNP \nSentence \nIdentity \n\u00d7 \n48.0 \n\nNE \nSub-clause UNMT* \n78.5 \n\n\nSynthetic EQA training data and models that generate it will be made publicly available at https://github. com/facebookresearch/UnsupervisedQA\nWe use SpaCy for Noun Chunking and NER, and Al-lenNLP for theStern et al. (2017) parser.3 http://commoncrawl.org/ 4 We also experimented with language model pretraining in a method similar toLample and Conneau (2019). Whilst generated questions were generally more fluent and wellformed, we did not observe significant changes in QA performance. Further details in Appendix A.6\nWe use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https:// github.com/allenai/document-qa\nhttp://bit.ly/semi-supervised-qa\nAcknowledgmentsThe authors would like to thank Tom Hosking, Max Bartolo, Johannes Welbl, Tim Rockt\u00e4schel, Fabio Petroni, Guillaume Lample and the anonymous reviewers for their insightful comments and feedback.Cloze QuestionAnswer Generated Question to record their sixth album in TEMPORAL 2005 When will they record their sixth album ?Redline management got word that both were negotiating with THING\nUnsupervised statistical machine translation. Mikel Artetxe, Gorka Labaka, Eneko Agirre, EMNLP. Association for Computational LinguisticsMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. Unsupervised statistical machine translation. In EMNLP, pages 3632-3642. Association for Compu- tational Linguistics.\n\nSemantic Parsing on Freebase from Question-Answer Pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational LinguisticsJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533-1544, Seattle, Wash- ington, USA. Association for Computational Lin- guistics.\n\nLatent dirichlet allocation. David M Blei, Andrew Y Ng, Michael I Jordan, J. Mach. Learn. Res. 3David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993-1022.\n\nPiotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, arXiv:1607.04606Enriching Word Vectors with Subword Information. csPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching Word Vectors with Subword Information. arXiv:1607.04606 [cs].\n\nSemi-Supervised Lexicon Learning for Wide-Coverage Semantic Parsing. Bo Chen, Bo An, Le Sun, Xianpei Han, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational LinguisticsBo Chen, Bo An, Le Sun, and Xianpei Han. 2018. Semi-Supervised Lexicon Learning for Wide- Coverage Semantic Parsing. In Proceedings of the 27th International Conference on Computational Linguistics, pages 892-904, Santa Fe, New Mexico, USA. Association for Computational Linguistics.\n\nSimple and Effective Multi-Paragraph Reading Comprehension. Christopher Clark, Matt Gardner, arXiv:1710.10723[cs].ArXiv:1710.10723Christopher Clark and Matt Gardner. 2017. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs]. ArXiv: 1710.10723.\n\nA unified architecture for natural language processing: Deep neural networks with multitask learning. Ronan Collobert, Jason Weston, 10.1145/1390156.1390177Proceedings of the 25th International Conference on Machine Learning, ICML '08. the 25th International Conference on Machine Learning, ICML '08New York, NY, USAACMRonan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Pro- ceedings of the 25th International Conference on Machine Learning, ICML '08, pages 160-167, New York, NY, USA. ACM.\n\nWord translation without parallel data. Alexis Conneau, Guillaume Lample, Marc&apos;aurelio Ranzato, Ludovic Denoyer, Herv\u00e9 J\u00e9gou, abs/1710.04087CoRRAlexis Conneau, Guillaume Lample, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. 2017. Word translation without parallel data. CoRR, abs/1710.04087.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805[cs].ArXiv:1810.04805BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs]. ArXiv: 1810.04805.\n\nSimple and Effective Semi-Supervised Question Answering. Bhuwan Dhingra, Danish Danish, Dheeraj Rajagopal, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics2Short PapersBhuwan Dhingra, Danish Danish, and Dheeraj Ra- jagopal. 2018. Simple and Effective Semi- Supervised Question Answering. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 582-587, New Orleans, Louisiana. Association for Computational Linguistics.\n\nHarvesting Paragraph-level Question-Answer Pairs from Wikipedia. Xinya Du, Claire Cardie, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Xinya Du and Claire Cardie. 2018. Harvest- ing Paragraph-level Question-Answer Pairs from Wikipedia. In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1907-1917, Mel- bourne, Australia. Association for Computational Linguistics.\n\nLearning to Ask: Neural Question Generation for Reading Comprehension. Xinya Du, Junru Shao, Claire Cardie, Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn- ing to Ask: Neural Question Generation for Reading Comprehension.\n\nManaal Faruqui, Dipanjan Das, arXiv:1808.09419[cs].ArXiv:1808.09419Identifying Well-formed Natural Language Questions. Manaal Faruqui and Dipanjan Das. 2018. Iden- tifying Well-formed Natural Language Questions. arXiv:1808.09419 [cs]. ArXiv: 1808.09419.\n\nGood Question! Statistical Ranking for Question Generation. Michael Heilman, A Noah, Smith, Human Language Technologies: The. Michael Heilman and Noah A. Smith. 2010. Good Question! Statistical Ranking for Question Gener- ation. In Human Language Technologies: The 2010\n\nAnnual Conference of the North American Chapter of the Association for Computational Linguistics, HLT '10. Stroudsburg, PA, USAAssociation for Computational LinguisticsAnnual Conference of the North American Chap- ter of the Association for Computational Linguistics, HLT '10, pages 609-617, Stroudsburg, PA, USA. Association for Computational Linguistics.\n\nProbabilistic latent semantic indexing. Thomas Hofmann, 10.1145/312624.312649Proceedings of the 22Nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '99. the 22Nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '99New York, NY, USAACMThomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22Nd Annual Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '99, pages 50-57, New York, NY, USA. ACM.\n\nEvaluating Rewards for Question Generation Models. Tom Hosking, Sebastian Riedel, arXiv:1902.11049[cs].ArXiv:1902.11049Tom Hosking and Sebastian Riedel. 2019. Eval- uating Rewards for Question Generation Models. arXiv:1902.11049 [cs]. ArXiv: 1902.11049.\n\nAdversarial Examples for Evaluating Reading Comprehension Systems. Robin Jia, Percy Liang, 10.18653/v1/D17-1215Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsRobin Jia and Percy Liang. 2017. Adversarial Exam- ples for Evaluating Reading Comprehension Sys- tems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing, pages 2021-2031, Copenhagen, Denmark. As- sociation for Computational Linguistics.\n\nHow Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks. Divyansh Kaushik, Zachary C Lipton, arXiv:1808.04926cs, statDivyansh Kaushik and Zachary C. Lipton. 2018. How Much Reading Does Reading Comprehension Re- quire? A Critical Investigation of Popular Bench- marks. arXiv:1808.04926 [cs, stat].\n\nMoses: Open Source Toolkit for Statistical Machine Translation. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions. the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster SessionsPrague, Czech RepublicAssociation for Computational LinguisticsPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Compan- ion Volume Proceedings of the Demo and Poster Sessions, pages 177-180. Association for Computa- tional Linguistics. Event-place: Prague, Czech Re- public.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics. Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav PetrovKenton LeeTom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu- ral Questions: a Benchmark for Question Answering Research. Transactions of the Association of Com- putational Linguistics.\n\nGuillaume Lample, Alexis Conneau, arXiv:1901.07291[cs].ArXiv:1901.07291Cross-lingual Language Model Pretraining. Guillaume Lample and Alexis Conneau. 2019. Cross-lingual Language Model Pretraining. arXiv:1901.07291 [cs]. ArXiv: 1901.07291.\n\nUnsupervised Machine Translation Using Monolingual Corpora Only. Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc&apos;aurelio Ranzato, Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato. 2017. Unsupervised Machine Translation Using Monolingual Corpora Only.\n\nPhrase-Based & Neural Unsupervised Machine Translation. Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc&apos;aurelio Ranzato, Proceedings of the 2018 Conference. the 2018 ConferenceGuillaume Lample, Myle Ott, Alexis Conneau, Lu- dovic Denoyer, and Marc'Aurelio Ranzato. 2018. Phrase-Based & Neural Unsupervised Machine Translation. In Proceedings of the 2018 Conference\n\nEfficient and Robust Question Answering from Minimal Context over Documents. Sewon Min, Victor Zhong, Richard Socher, Caiming Xiong, arXiv:1805.08092[cs].ArXiv:1805.08092Sewon Min, Victor Zhong, Richard Socher, and Caim- ing Xiong. 2018. Efficient and Robust Question Answering from Minimal Context over Documents. arXiv:1805.08092 [cs]. ArXiv: 1805.08092.\n\nQuestion Generation from Concept Maps. Andrew M Olney, Arthur C Graesser, Natalie K Person, 10.5087/d&d.v3i2.1480Dialogue & Discourse. 32Andrew M. Olney, Arthur C. Graesser, and Natalie K. Person. 2012. Question Generation from Concept Maps. Dialogue & Discourse, 3(2):75-99-99.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, In In EMNLPJeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In In EMNLP.\n\nImproving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya SutskeverAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\nKnow What You Dont Know: Unanswerable Questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaShort Papers2Association for Computational LinguisticsPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Dont Know: Unanswerable Ques- tions for SQuAD. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784- 789, Melbourne, Australia. Association for Compu- tational Linguistics.\n\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Lin- guistics.\n\nCoQA: A Conversational Question Answering Challenge. Siva Reddy, Danqi Chen, Christopher D Manning, arXiv:1808.07042[cs].ArXiv:1808.07042CitationKey:reddyCoQAConversa-tionalQuestion2018Siva Reddy, Danqi Chen, and Christopher D. Man- ning. 2018. CoQA: A Conversational Question An- swering Challenge. arXiv:1808.07042 [cs]. ArXiv: 1808.07042 Citation Key: reddyCoQAConversa- tionalQuestion2018.\n\nThe First Question Generation Shared Task Evaluation Challenge. Brendan Vasile Rus, Paul Wyse, Mihai Piwek, Svetlana Lintean, Cristian Stoyanchev, Moldovan, Proceedings of the 6th International Natural Language Generation Conference, INLG '10. the 6th International Natural Language Generation Conference, INLG '10Stroudsburg, PA, USA; Trim, Co. Meath, IrelandAssociation for Computational LinguisticsVasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean, Svetlana Stoyanchev, and Cristian Moldovan. 2010. The First Question Generation Shared Task Evalu- ation Challenge. In Proceedings of the 6th Inter- national Natural Language Generation Conference, INLG '10, pages 251-257, Stroudsburg, PA, USA. Association for Computational Linguistics. Event- place: Trim, Co. Meath, Ireland.\n\nConceptNet 5.5: An Open Multilingual Graph of General Knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, arXiv:1612.03975[cs].ArXiv:1612.03975Robyn Speer, Joshua Chin, and Catherine Havasi. 2016. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. arXiv:1612.03975 [cs]. ArXiv: 1612.03975.\n\n. Mitchell Stern, Jacob Andreas, Dan Klein, arXiv:1705.03919[cs].ArXiv:1705.03919A Minimal Span-Based Neural Constituency Parser.Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A Minimal Span-Based Neural Constituency Parser. arXiv:1705.03919 [cs]. ArXiv: 1705.03919.\n\n. Sandeep Subramanian, Guillaume Lample, Eric Michael Smith, Ludovic Denoyer, Marc&apos;aurelio Ranzato, Y.-Lan Boureau, arXiv:1811.00552[cs].ArXiv:1811.00552Multiple-Attribute Text Style TransferSandeep Subramanian, Guillaume Lample, Eric Michael Smith, Ludovic Denoyer, Marc'Aurelio Ranzato, and Y.-Lan Boureau. 2018. Multiple-Attribute Text Style Transfer. arXiv:1811.00552 [cs]. ArXiv: 1811.00552.\n\nWhat Makes Reading Comprehension Questions Easier?. Saku Sugawara, Kentaro Inui, Satoshi Sekine, Akiko Aizawa, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsSaku Sugawara, Kentaro Inui, Satoshi Sekine, and Akiko Aizawa. 2018. What Makes Reading Com- prehension Questions Easier? In Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing, pages 4208-4219, Brus- sels, Belgium. Association for Computational Lin- guistics.\n\nMulti-Perspective Context Aggregation for Semisupervised Cloze-style Reading Comprehension. Liang Wang, Sujian Li, Wei Zhao, Kewei Shen, Meng Sun, Ruoyu Jia, Jingming Liu, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational LinguisticsLiang Wang, Sujian Li, Wei Zhao, Kewei Shen, Meng Sun, Ruoyu Jia, and Jingming Liu. 2018. Multi-Perspective Context Aggregation for Semi- supervised Cloze-style Reading Comprehension. In Proceedings of the 27th International Conference on Computational Linguistics, pages 857-867, Santa Fe, New Mexico, USA. Association for Computa- tional Linguistics.\n\nDynamic Integration of Background Knowledge in. Dirk Weissenborn, Tom Koisk, Chris Dyer, Dirk Weissenborn, Tom Koisk, and Chris Dyer. 2017. Dynamic Integration of Background Knowledge in\n\n. Nlu Neural, Systems, arXiv:1706.02596Neural NLU Systems. arXiv:1706.02596 [cs].\n\nSemi-Supervised QA with Generative Domain-Adaptive Nets. Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, William Cohen, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1Long Papers)Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and William Cohen. 2017. Semi-Supervised QA with Generative Domain-Adaptive Nets. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1040-1050, Vancouver, Canada. Asso- ciation for Computational Linguistics.\n\nSemantics-based Question Generation and Implementation. Xuchen Yao, Gosse Bouma, Yi Zhang, D&D. 3Xuchen Yao, Gosse Bouma, and Yi Zhang. 2012. Semantics-based Question Generation and Imple- mentation. D&D, 3:11-42.\n\nXingdi Yuan, Tong Wang, Caglar Gulcehre, Alessandro Sordoni, Philip Bachman, arXiv:1705.02012[cs].ArXiv:1705.02012Sandeep Subramanian, Saizheng Zhang, and Adam Trischler. 2017. Machine Comprehension by Text-to-Text Neural Question Generation. Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessan- dro Sordoni, Philip Bachman, Sandeep Subra- manian, Saizheng Zhang, and Adam Trischler. 2017. Machine Comprehension by Text-to-Text Neural Question Generation. arXiv:1705.02012 [cs]. ArXiv: 1705.02012.\n\nParagraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks. Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, Qifa Ke, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsYao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. 2018. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Net- works. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pages 3901-3910, Brussels, Belgium. Associ- ation for Computational Linguistics.\n", "annotations": {"author": "[{\"end\":201,\"start\":56},{\"end\":350,\"start\":202},{\"end\":500,\"start\":351}]", "publisher": null, "author_last_name": "[{\"end\":69,\"start\":64},{\"end\":217,\"start\":210},{\"end\":367,\"start\":361}]", "author_first_name": "[{\"end\":63,\"start\":56},{\"end\":209,\"start\":202},{\"end\":360,\"start\":351}]", "author_affiliation": "[{\"end\":200,\"start\":85},{\"end\":349,\"start\":234},{\"end\":499,\"start\":384}]", "title": "[{\"end\":53,\"start\":1},{\"end\":553,\"start\":501}]", "venue": null, "abstract": "[{\"end\":1968,\"start\":555}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2256,\"start\":2232},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2576,\"start\":2552},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2623,\"start\":2602},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2737,\"start\":2711},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4515,\"start\":4490},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4907,\"start\":4891},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4926,\"start\":4907},{\"end\":4962,\"start\":4926},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5078,\"start\":5053},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5099,\"start\":5078},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5560,\"start\":5536},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10720,\"start\":10696},{\"end\":12901,\"start\":12900},{\"end\":13366,\"start\":13365},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14180,\"start\":14155},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15821,\"start\":15800},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15874,\"start\":15849},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16386,\"start\":16364},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16432,\"start\":16408},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16480,\"start\":16456},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16553,\"start\":16529},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16603,\"start\":16577},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16645,\"start\":16621},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16718,\"start\":16697},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16763,\"start\":16738},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16813,\"start\":16789},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17044,\"start\":17023},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17298,\"start\":17275},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17480,\"start\":17455},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17717,\"start\":17696},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18332,\"start\":18311},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19655,\"start\":19632},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23702,\"start\":23676},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25371,\"start\":25349},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26213,\"start\":26192},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26236,\"start\":26218},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26409,\"start\":26388},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26716,\"start\":26695},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26919,\"start\":26898},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27259,\"start\":27238},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27519,\"start\":27504},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27537,\"start\":27519},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27580,\"start\":27552},{\"end\":27601,\"start\":27580},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27625,\"start\":27601},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27646,\"start\":27625},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27666,\"start\":27646},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27729,\"start\":27708},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27775,\"start\":27749},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27903,\"start\":27882},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28034,\"start\":28014},{\"end\":28239,\"start\":28218},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28366,\"start\":28345},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28668,\"start\":28650},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28796,\"start\":28770},{\"end\":28813,\"start\":28796},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28995,\"start\":28970},{\"end\":29025,\"start\":29000},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29061,\"start\":29041},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29177,\"start\":29159},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29203,\"start\":29179},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29280,\"start\":29263},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29304,\"start\":29285},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29503,\"start\":29486},{\"end\":29521,\"start\":29503},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29541,\"start\":29521},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29566,\"start\":29541},{\"end\":29832,\"start\":29811},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31082,\"start\":31062},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31340,\"start\":31323},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31438,\"start\":31416},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33229,\"start\":33209},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33732,\"start\":33707},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36207,\"start\":36185},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36278,\"start\":36253},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36325,\"start\":36301},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36453,\"start\":36431},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":38378,\"start\":38354},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":42962,\"start\":42943},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":43098,\"start\":43073}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39026,\"start\":38872},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39174,\"start\":39027},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39297,\"start\":39175},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39529,\"start\":39298},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39664,\"start\":39530},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":39922,\"start\":39665},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":41870,\"start\":39923},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":42151,\"start\":41871},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":42738,\"start\":42152}]", "paragraph": "[{\"end\":3016,\"start\":1984},{\"end\":3716,\"start\":3018},{\"end\":4340,\"start\":3718},{\"end\":5359,\"start\":4342},{\"end\":6022,\"start\":5361},{\"end\":6437,\"start\":6024},{\"end\":6717,\"start\":6468},{\"end\":7237,\"start\":6719},{\"end\":7587,\"start\":7271},{\"end\":7781,\"start\":7589},{\"end\":8126,\"start\":7800},{\"end\":8443,\"start\":8150},{\"end\":9140,\"start\":8464},{\"end\":9307,\"start\":9162},{\"end\":9695,\"start\":9309},{\"end\":10401,\"start\":9697},{\"end\":10830,\"start\":10403},{\"end\":11204,\"start\":10842},{\"end\":11395,\"start\":11227},{\"end\":11638,\"start\":11397},{\"end\":12015,\"start\":11640},{\"end\":12620,\"start\":12050},{\"end\":13057,\"start\":12622},{\"end\":13242,\"start\":13059},{\"end\":14342,\"start\":13244},{\"end\":15140,\"start\":14344},{\"end\":15682,\"start\":15156},{\"end\":16090,\"start\":15714},{\"end\":16492,\"start\":16092},{\"end\":16653,\"start\":16514},{\"end\":16684,\"start\":16655},{\"end\":17215,\"start\":16686},{\"end\":18049,\"start\":17217},{\"end\":18372,\"start\":18051},{\"end\":18944,\"start\":18406},{\"end\":19443,\"start\":18946},{\"end\":19952,\"start\":19445},{\"end\":21009,\"start\":19954},{\"end\":21220,\"start\":21011},{\"end\":22218,\"start\":21222},{\"end\":23423,\"start\":22237},{\"end\":24641,\"start\":23460},{\"end\":25232,\"start\":24643},{\"end\":26028,\"start\":25234},{\"end\":27397,\"start\":26060},{\"end\":28200,\"start\":27414},{\"end\":29075,\"start\":28202},{\"end\":29567,\"start\":29077},{\"end\":30206,\"start\":29582},{\"end\":30440,\"start\":30208},{\"end\":31149,\"start\":30442},{\"end\":31531,\"start\":31151},{\"end\":33055,\"start\":31546},{\"end\":34336,\"start\":33103},{\"end\":35202,\"start\":34358},{\"end\":36014,\"start\":35233},{\"end\":36283,\"start\":36047},{\"end\":37563,\"start\":36285},{\"end\":38812,\"start\":37598}]", "formula": null, "table_ref": "[{\"end\":16831,\"start\":16824},{\"end\":18175,\"start\":18168},{\"end\":18712,\"start\":18705},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18881,\"start\":18874},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21610,\"start\":21603},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24140,\"start\":24133},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24795,\"start\":24788},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":33000,\"start\":32993},{\"end\":36338,\"start\":36331},{\"end\":36932,\"start\":36925},{\"end\":38130,\"start\":38123},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":38646,\"start\":38639},{\"end\":38786,\"start\":38779}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1982,\"start\":1970},{\"attributes\":{\"n\":\"2\"},\"end\":6466,\"start\":6440},{\"attributes\":{\"n\":\"2.1\"},\"end\":7269,\"start\":7240},{\"end\":7798,\"start\":7784},{\"attributes\":{\"n\":\"2.2\"},\"end\":8148,\"start\":8129},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":8462,\"start\":8446},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":9160,\"start\":9143},{\"end\":10840,\"start\":10833},{\"attributes\":{\"n\":\"2.3\"},\"end\":11225,\"start\":11207},{\"attributes\":{\"n\":\"2.4\"},\"end\":12048,\"start\":12018},{\"attributes\":{\"n\":\"3\"},\"end\":15154,\"start\":15143},{\"attributes\":{\"n\":\"3.1\"},\"end\":15712,\"start\":15685},{\"end\":16504,\"start\":16495},{\"end\":16512,\"start\":16507},{\"attributes\":{\"n\":\"3.2\"},\"end\":18404,\"start\":18375},{\"attributes\":{\"n\":\"3.3\"},\"end\":22235,\"start\":22221},{\"attributes\":{\"n\":\"3.4\"},\"end\":23458,\"start\":23426},{\"attributes\":{\"n\":\"3.5\"},\"end\":26058,\"start\":26031},{\"attributes\":{\"n\":\"4\"},\"end\":27412,\"start\":27400},{\"attributes\":{\"n\":\"5\"},\"end\":29580,\"start\":29570},{\"attributes\":{\"n\":\"6\"},\"end\":31544,\"start\":31534},{\"end\":33101,\"start\":33058},{\"end\":34356,\"start\":34339},{\"end\":35231,\"start\":35205},{\"end\":36045,\"start\":36017},{\"end\":37596,\"start\":37566},{\"end\":38871,\"start\":38815},{\"end\":38883,\"start\":38873},{\"end\":39038,\"start\":39028},{\"end\":39186,\"start\":39176},{\"end\":39308,\"start\":39299},{\"end\":39540,\"start\":39531},{\"end\":39675,\"start\":39666},{\"end\":42162,\"start\":42153}]", "table": "[{\"end\":41870,\"start\":40056},{\"end\":42738,\"start\":42230}]", "figure_caption": "[{\"end\":39026,\"start\":38885},{\"end\":39174,\"start\":39040},{\"end\":39297,\"start\":39188},{\"end\":39529,\"start\":39310},{\"end\":39664,\"start\":39542},{\"end\":39922,\"start\":39677},{\"end\":40056,\"start\":39925},{\"end\":42151,\"start\":41873},{\"end\":42230,\"start\":42164}]", "figure_ref": "[{\"end\":2324,\"start\":2316},{\"end\":3770,\"start\":3762},{\"end\":6652,\"start\":6644},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20012,\"start\":20004},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22138,\"start\":22130},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23007,\"start\":22999},{\"end\":23925,\"start\":23917},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24285,\"start\":24277},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26820,\"start\":26812}]", "bib_author_first_name": "[{\"end\":43962,\"start\":43957},{\"end\":43977,\"start\":43972},{\"end\":43991,\"start\":43986},{\"end\":44285,\"start\":44277},{\"end\":44300,\"start\":44294},{\"end\":44310,\"start\":44307},{\"end\":44325,\"start\":44320},{\"end\":44898,\"start\":44893},{\"end\":44900,\"start\":44899},{\"end\":44913,\"start\":44907},{\"end\":44915,\"start\":44914},{\"end\":44927,\"start\":44920},{\"end\":44929,\"start\":44928},{\"end\":45087,\"start\":45082},{\"end\":45107,\"start\":45100},{\"end\":45121,\"start\":45115},{\"end\":45135,\"start\":45130},{\"end\":45429,\"start\":45427},{\"end\":45438,\"start\":45436},{\"end\":45445,\"start\":45443},{\"end\":45458,\"start\":45451},{\"end\":46027,\"start\":46016},{\"end\":46039,\"start\":46035},{\"end\":46338,\"start\":46333},{\"end\":46355,\"start\":46350},{\"end\":46863,\"start\":46857},{\"end\":46882,\"start\":46873},{\"end\":46908,\"start\":46891},{\"end\":46925,\"start\":46918},{\"end\":46940,\"start\":46935},{\"end\":47130,\"start\":47125},{\"end\":47147,\"start\":47139},{\"end\":47161,\"start\":47155},{\"end\":47175,\"start\":47167},{\"end\":47566,\"start\":47560},{\"end\":47582,\"start\":47576},{\"end\":47598,\"start\":47591},{\"end\":48404,\"start\":48399},{\"end\":48415,\"start\":48409},{\"end\":49027,\"start\":49022},{\"end\":49037,\"start\":49032},{\"end\":49050,\"start\":49044},{\"end\":49186,\"start\":49180},{\"end\":49204,\"start\":49196},{\"end\":49502,\"start\":49495},{\"end\":49513,\"start\":49512},{\"end\":50110,\"start\":50104},{\"end\":50696,\"start\":50693},{\"end\":50715,\"start\":50706},{\"end\":50969,\"start\":50964},{\"end\":50980,\"start\":50975},{\"end\":51617,\"start\":51609},{\"end\":51634,\"start\":51627},{\"end\":51636,\"start\":51635},{\"end\":51921,\"start\":51914},{\"end\":51933,\"start\":51929},{\"end\":51950,\"start\":51941},{\"end\":51963,\"start\":51958},{\"end\":51988,\"start\":51980},{\"end\":52005,\"start\":51999},{\"end\":52022,\"start\":52016},{\"end\":52034,\"start\":52030},{\"end\":52050,\"start\":52041},{\"end\":52065,\"start\":52058},{\"end\":52077,\"start\":52072},{\"end\":52090,\"start\":52084},{\"end\":52107,\"start\":52098},{\"end\":52124,\"start\":52120},{\"end\":53015,\"start\":53012},{\"end\":53039,\"start\":53029},{\"end\":53056,\"start\":53050},{\"end\":53074,\"start\":53067},{\"end\":53089,\"start\":53084},{\"end\":53103,\"start\":53098},{\"end\":53121,\"start\":53113},{\"end\":53136,\"start\":53131},{\"end\":53156,\"start\":53149},{\"end\":53170,\"start\":53165},{\"end\":53854,\"start\":53845},{\"end\":53869,\"start\":53863},{\"end\":54160,\"start\":54151},{\"end\":54175,\"start\":54169},{\"end\":54192,\"start\":54185},{\"end\":54219,\"start\":54202},{\"end\":54443,\"start\":54434},{\"end\":54456,\"start\":54452},{\"end\":54468,\"start\":54462},{\"end\":54485,\"start\":54478},{\"end\":54512,\"start\":54495},{\"end\":54849,\"start\":54844},{\"end\":54861,\"start\":54855},{\"end\":54876,\"start\":54869},{\"end\":54892,\"start\":54885},{\"end\":55170,\"start\":55164},{\"end\":55172,\"start\":55171},{\"end\":55186,\"start\":55180},{\"end\":55188,\"start\":55187},{\"end\":55206,\"start\":55199},{\"end\":55208,\"start\":55207},{\"end\":55459,\"start\":55452},{\"end\":55479,\"start\":55472},{\"end\":55499,\"start\":55488},{\"end\":55501,\"start\":55500},{\"end\":55720,\"start\":55716},{\"end\":55737,\"start\":55730},{\"end\":55977,\"start\":55973},{\"end\":55994,\"start\":55987},{\"end\":56004,\"start\":55999},{\"end\":56017,\"start\":56012},{\"end\":56029,\"start\":56024},{\"end\":56042,\"start\":56038},{\"end\":56264,\"start\":56258},{\"end\":56281,\"start\":56276},{\"end\":56292,\"start\":56287},{\"end\":56918,\"start\":56912},{\"end\":56934,\"start\":56930},{\"end\":56952,\"start\":56942},{\"end\":56967,\"start\":56962},{\"end\":57550,\"start\":57546},{\"end\":57563,\"start\":57558},{\"end\":57581,\"start\":57570},{\"end\":57583,\"start\":57582},{\"end\":57959,\"start\":57952},{\"end\":57976,\"start\":57972},{\"end\":57988,\"start\":57983},{\"end\":58004,\"start\":57996},{\"end\":58022,\"start\":58014},{\"end\":58741,\"start\":58736},{\"end\":58755,\"start\":58749},{\"end\":58771,\"start\":58762},{\"end\":58989,\"start\":58981},{\"end\":59002,\"start\":58997},{\"end\":59015,\"start\":59012},{\"end\":59261,\"start\":59254},{\"end\":59284,\"start\":59275},{\"end\":59297,\"start\":59293},{\"end\":59305,\"start\":59298},{\"end\":59320,\"start\":59313},{\"end\":59347,\"start\":59330},{\"end\":59363,\"start\":59357},{\"end\":59711,\"start\":59707},{\"end\":59729,\"start\":59722},{\"end\":59743,\"start\":59736},{\"end\":59757,\"start\":59752},{\"end\":60379,\"start\":60374},{\"end\":60392,\"start\":60386},{\"end\":60400,\"start\":60397},{\"end\":60412,\"start\":60407},{\"end\":60423,\"start\":60419},{\"end\":60434,\"start\":60429},{\"end\":60448,\"start\":60440},{\"end\":61067,\"start\":61063},{\"end\":61084,\"start\":61081},{\"end\":61097,\"start\":61092},{\"end\":61208,\"start\":61205},{\"end\":61349,\"start\":61343},{\"end\":61362,\"start\":61356},{\"end\":61373,\"start\":61367},{\"end\":61396,\"start\":61389},{\"end\":62026,\"start\":62020},{\"end\":62037,\"start\":62032},{\"end\":62047,\"start\":62045},{\"end\":62185,\"start\":62179},{\"end\":62196,\"start\":62192},{\"end\":62209,\"start\":62203},{\"end\":62230,\"start\":62220},{\"end\":62246,\"start\":62240},{\"end\":62776,\"start\":62773},{\"end\":62792,\"start\":62783},{\"end\":62805,\"start\":62797},{\"end\":62816,\"start\":62812}]", "bib_author_last_name": "[{\"end\":43970,\"start\":43963},{\"end\":43984,\"start\":43978},{\"end\":43998,\"start\":43992},{\"end\":44292,\"start\":44286},{\"end\":44305,\"start\":44301},{\"end\":44318,\"start\":44311},{\"end\":44331,\"start\":44326},{\"end\":44905,\"start\":44901},{\"end\":44918,\"start\":44916},{\"end\":44936,\"start\":44930},{\"end\":45098,\"start\":45088},{\"end\":45113,\"start\":45108},{\"end\":45128,\"start\":45122},{\"end\":45143,\"start\":45136},{\"end\":45434,\"start\":45430},{\"end\":45441,\"start\":45439},{\"end\":45449,\"start\":45446},{\"end\":45462,\"start\":45459},{\"end\":46033,\"start\":46028},{\"end\":46047,\"start\":46040},{\"end\":46348,\"start\":46339},{\"end\":46362,\"start\":46356},{\"end\":46871,\"start\":46864},{\"end\":46889,\"start\":46883},{\"end\":46916,\"start\":46909},{\"end\":46933,\"start\":46926},{\"end\":46946,\"start\":46941},{\"end\":47137,\"start\":47131},{\"end\":47153,\"start\":47148},{\"end\":47165,\"start\":47162},{\"end\":47185,\"start\":47176},{\"end\":47574,\"start\":47567},{\"end\":47589,\"start\":47583},{\"end\":47608,\"start\":47599},{\"end\":48407,\"start\":48405},{\"end\":48422,\"start\":48416},{\"end\":49030,\"start\":49028},{\"end\":49042,\"start\":49038},{\"end\":49057,\"start\":49051},{\"end\":49194,\"start\":49187},{\"end\":49208,\"start\":49205},{\"end\":49510,\"start\":49503},{\"end\":49518,\"start\":49514},{\"end\":49525,\"start\":49520},{\"end\":50118,\"start\":50111},{\"end\":50704,\"start\":50697},{\"end\":50722,\"start\":50716},{\"end\":50973,\"start\":50970},{\"end\":50986,\"start\":50981},{\"end\":51625,\"start\":51618},{\"end\":51643,\"start\":51637},{\"end\":51927,\"start\":51922},{\"end\":51939,\"start\":51934},{\"end\":51956,\"start\":51951},{\"end\":51978,\"start\":51964},{\"end\":51997,\"start\":51989},{\"end\":52014,\"start\":52006},{\"end\":52028,\"start\":52023},{\"end\":52039,\"start\":52035},{\"end\":52056,\"start\":52051},{\"end\":52070,\"start\":52066},{\"end\":52082,\"start\":52078},{\"end\":52096,\"start\":52091},{\"end\":52118,\"start\":52108},{\"end\":52131,\"start\":52125},{\"end\":53027,\"start\":53016},{\"end\":53048,\"start\":53040},{\"end\":53065,\"start\":53057},{\"end\":53082,\"start\":53075},{\"end\":53096,\"start\":53090},{\"end\":53111,\"start\":53104},{\"end\":53129,\"start\":53122},{\"end\":53147,\"start\":53137},{\"end\":53163,\"start\":53157},{\"end\":53177,\"start\":53171},{\"end\":53861,\"start\":53855},{\"end\":53877,\"start\":53870},{\"end\":54167,\"start\":54161},{\"end\":54183,\"start\":54176},{\"end\":54200,\"start\":54193},{\"end\":54227,\"start\":54220},{\"end\":54450,\"start\":54444},{\"end\":54460,\"start\":54457},{\"end\":54476,\"start\":54469},{\"end\":54493,\"start\":54486},{\"end\":54520,\"start\":54513},{\"end\":54853,\"start\":54850},{\"end\":54867,\"start\":54862},{\"end\":54883,\"start\":54877},{\"end\":54898,\"start\":54893},{\"end\":55178,\"start\":55173},{\"end\":55197,\"start\":55189},{\"end\":55215,\"start\":55209},{\"end\":55470,\"start\":55460},{\"end\":55486,\"start\":55480},{\"end\":55509,\"start\":55502},{\"end\":55728,\"start\":55721},{\"end\":55748,\"start\":55738},{\"end\":55985,\"start\":55978},{\"end\":55997,\"start\":55995},{\"end\":56010,\"start\":56005},{\"end\":56022,\"start\":56018},{\"end\":56036,\"start\":56030},{\"end\":56052,\"start\":56043},{\"end\":56274,\"start\":56265},{\"end\":56285,\"start\":56282},{\"end\":56298,\"start\":56293},{\"end\":56928,\"start\":56919},{\"end\":56940,\"start\":56935},{\"end\":56960,\"start\":56953},{\"end\":56973,\"start\":56968},{\"end\":57556,\"start\":57551},{\"end\":57568,\"start\":57564},{\"end\":57591,\"start\":57584},{\"end\":57970,\"start\":57960},{\"end\":57981,\"start\":57977},{\"end\":57994,\"start\":57989},{\"end\":58012,\"start\":58005},{\"end\":58033,\"start\":58023},{\"end\":58043,\"start\":58035},{\"end\":58747,\"start\":58742},{\"end\":58760,\"start\":58756},{\"end\":58778,\"start\":58772},{\"end\":58995,\"start\":58990},{\"end\":59010,\"start\":59003},{\"end\":59021,\"start\":59016},{\"end\":59273,\"start\":59262},{\"end\":59291,\"start\":59285},{\"end\":59311,\"start\":59306},{\"end\":59328,\"start\":59321},{\"end\":59355,\"start\":59348},{\"end\":59371,\"start\":59364},{\"end\":59720,\"start\":59712},{\"end\":59734,\"start\":59730},{\"end\":59750,\"start\":59744},{\"end\":59764,\"start\":59758},{\"end\":60384,\"start\":60380},{\"end\":60395,\"start\":60393},{\"end\":60405,\"start\":60401},{\"end\":60417,\"start\":60413},{\"end\":60427,\"start\":60424},{\"end\":60438,\"start\":60435},{\"end\":60452,\"start\":60449},{\"end\":61079,\"start\":61068},{\"end\":61090,\"start\":61085},{\"end\":61102,\"start\":61098},{\"end\":61215,\"start\":61209},{\"end\":61224,\"start\":61217},{\"end\":61354,\"start\":61350},{\"end\":61365,\"start\":61363},{\"end\":61387,\"start\":61374},{\"end\":61402,\"start\":61397},{\"end\":62030,\"start\":62027},{\"end\":62043,\"start\":62038},{\"end\":62053,\"start\":62048},{\"end\":62190,\"start\":62186},{\"end\":62201,\"start\":62197},{\"end\":62218,\"start\":62210},{\"end\":62238,\"start\":62231},{\"end\":62254,\"start\":62247},{\"end\":62781,\"start\":62777},{\"end\":62795,\"start\":62793},{\"end\":62810,\"start\":62806},{\"end\":62819,\"start\":62817}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52166727},\"end\":44218,\"start\":43911},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6401679},\"end\":44862,\"start\":44220},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3177797},\"end\":45080,\"start\":44864},{\"attributes\":{\"doi\":\"arXiv:1607.04606\",\"id\":\"b3\"},\"end\":45356,\"start\":45082},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52011119},\"end\":45954,\"start\":45358},{\"attributes\":{\"doi\":\"arXiv:1710.10723[cs].ArXiv:1710.10723\",\"id\":\"b5\"},\"end\":46229,\"start\":45956},{\"attributes\":{\"doi\":\"10.1145/1390156.1390177\",\"id\":\"b6\",\"matched_paper_id\":2617020},\"end\":46815,\"start\":46231},{\"attributes\":{\"doi\":\"abs/1710.04087\",\"id\":\"b7\"},\"end\":47123,\"start\":46817},{\"attributes\":{\"doi\":\"arXiv:1810.04805[cs].ArXiv:1810.04805\",\"id\":\"b8\"},\"end\":47501,\"start\":47125},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4570782},\"end\":48332,\"start\":47503},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":21702856},\"end\":48949,\"start\":48334},{\"attributes\":{\"id\":\"b11\"},\"end\":49178,\"start\":48951},{\"attributes\":{\"doi\":\"arXiv:1808.09419[cs].ArXiv:1808.09419\",\"id\":\"b12\"},\"end\":49433,\"start\":49180},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1809816},\"end\":49704,\"start\":49435},{\"attributes\":{\"id\":\"b14\"},\"end\":50062,\"start\":49706},{\"attributes\":{\"doi\":\"10.1145/312624.312649\",\"id\":\"b15\",\"matched_paper_id\":10648980},\"end\":50640,\"start\":50064},{\"attributes\":{\"doi\":\"arXiv:1902.11049[cs].ArXiv:1902.11049\",\"id\":\"b16\"},\"end\":50895,\"start\":50642},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1215\",\"id\":\"b17\",\"matched_paper_id\":7228830},\"end\":51506,\"start\":50897},{\"attributes\":{\"doi\":\"arXiv:1808.04926\",\"id\":\"b18\"},\"end\":51848,\"start\":51508},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":794019},\"end\":53010,\"start\":51850},{\"attributes\":{\"id\":\"b20\"},\"end\":53843,\"start\":53012},{\"attributes\":{\"doi\":\"arXiv:1901.07291[cs].ArXiv:1901.07291\",\"id\":\"b21\"},\"end\":54084,\"start\":53845},{\"attributes\":{\"id\":\"b22\"},\"end\":54376,\"start\":54086},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5033497},\"end\":54765,\"start\":54378},{\"attributes\":{\"doi\":\"arXiv:1805.08092[cs].ArXiv:1805.08092\",\"id\":\"b24\"},\"end\":55123,\"start\":54767},{\"attributes\":{\"doi\":\"10.5087/d&d.v3i2.1480\",\"id\":\"b25\",\"matched_paper_id\":115215973},\"end\":55403,\"start\":55125},{\"attributes\":{\"id\":\"b26\"},\"end\":55653,\"start\":55405},{\"attributes\":{\"id\":\"b27\"},\"end\":55918,\"start\":55655},{\"attributes\":{\"id\":\"b28\"},\"end\":56197,\"start\":55920},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":47018994},\"end\":56849,\"start\":56199},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11816014},\"end\":57491,\"start\":56851},{\"attributes\":{\"doi\":\"arXiv:1808.07042[cs].ArXiv:1808.07042CitationKey:reddyCoQAConversa-tionalQuestion2018\",\"id\":\"b31\"},\"end\":57886,\"start\":57493},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":215754656},\"end\":58669,\"start\":57888},{\"attributes\":{\"doi\":\"arXiv:1612.03975[cs].ArXiv:1612.03975\",\"id\":\"b33\"},\"end\":58977,\"start\":58671},{\"attributes\":{\"doi\":\"arXiv:1705.03919[cs].ArXiv:1705.03919\",\"id\":\"b34\"},\"end\":59250,\"start\":58979},{\"attributes\":{\"doi\":\"arXiv:1811.00552[cs].ArXiv:1811.00552\",\"id\":\"b35\"},\"end\":59653,\"start\":59252},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52113519},\"end\":60280,\"start\":59655},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52010859},\"end\":61013,\"start\":60282},{\"attributes\":{\"id\":\"b38\"},\"end\":61201,\"start\":61015},{\"attributes\":{\"doi\":\"arXiv:1706.02596\",\"id\":\"b39\"},\"end\":61284,\"start\":61203},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":15164488},\"end\":61962,\"start\":61286},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":18299631},\"end\":62177,\"start\":61964},{\"attributes\":{\"doi\":\"arXiv:1705.02012[cs].ArXiv:1705.02012\",\"id\":\"b42\"},\"end\":62673,\"start\":62179},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":53081407},\"end\":63369,\"start\":62675}]", "bib_title": "[{\"end\":43955,\"start\":43911},{\"end\":44275,\"start\":44220},{\"end\":44891,\"start\":44864},{\"end\":45425,\"start\":45358},{\"end\":46331,\"start\":46231},{\"end\":47558,\"start\":47503},{\"end\":48397,\"start\":48334},{\"end\":49493,\"start\":49435},{\"end\":50102,\"start\":50064},{\"end\":50962,\"start\":50897},{\"end\":51912,\"start\":51850},{\"end\":54432,\"start\":54378},{\"end\":55162,\"start\":55125},{\"end\":56256,\"start\":56199},{\"end\":56910,\"start\":56851},{\"end\":57950,\"start\":57888},{\"end\":59705,\"start\":59655},{\"end\":60372,\"start\":60282},{\"end\":61341,\"start\":61286},{\"end\":62018,\"start\":61964},{\"end\":62771,\"start\":62675}]", "bib_author": "[{\"end\":43972,\"start\":43957},{\"end\":43986,\"start\":43972},{\"end\":44000,\"start\":43986},{\"end\":44294,\"start\":44277},{\"end\":44307,\"start\":44294},{\"end\":44320,\"start\":44307},{\"end\":44333,\"start\":44320},{\"end\":44907,\"start\":44893},{\"end\":44920,\"start\":44907},{\"end\":44938,\"start\":44920},{\"end\":45100,\"start\":45082},{\"end\":45115,\"start\":45100},{\"end\":45130,\"start\":45115},{\"end\":45145,\"start\":45130},{\"end\":45436,\"start\":45427},{\"end\":45443,\"start\":45436},{\"end\":45451,\"start\":45443},{\"end\":45464,\"start\":45451},{\"end\":46035,\"start\":46016},{\"end\":46049,\"start\":46035},{\"end\":46350,\"start\":46333},{\"end\":46364,\"start\":46350},{\"end\":46873,\"start\":46857},{\"end\":46891,\"start\":46873},{\"end\":46918,\"start\":46891},{\"end\":46935,\"start\":46918},{\"end\":46948,\"start\":46935},{\"end\":47139,\"start\":47125},{\"end\":47155,\"start\":47139},{\"end\":47167,\"start\":47155},{\"end\":47187,\"start\":47167},{\"end\":47576,\"start\":47560},{\"end\":47591,\"start\":47576},{\"end\":47610,\"start\":47591},{\"end\":48409,\"start\":48399},{\"end\":48424,\"start\":48409},{\"end\":49032,\"start\":49022},{\"end\":49044,\"start\":49032},{\"end\":49059,\"start\":49044},{\"end\":49196,\"start\":49180},{\"end\":49210,\"start\":49196},{\"end\":49512,\"start\":49495},{\"end\":49520,\"start\":49512},{\"end\":49527,\"start\":49520},{\"end\":50120,\"start\":50104},{\"end\":50706,\"start\":50693},{\"end\":50724,\"start\":50706},{\"end\":50975,\"start\":50964},{\"end\":50988,\"start\":50975},{\"end\":51627,\"start\":51609},{\"end\":51645,\"start\":51627},{\"end\":51929,\"start\":51914},{\"end\":51941,\"start\":51929},{\"end\":51958,\"start\":51941},{\"end\":51980,\"start\":51958},{\"end\":51999,\"start\":51980},{\"end\":52016,\"start\":51999},{\"end\":52030,\"start\":52016},{\"end\":52041,\"start\":52030},{\"end\":52058,\"start\":52041},{\"end\":52072,\"start\":52058},{\"end\":52084,\"start\":52072},{\"end\":52098,\"start\":52084},{\"end\":52120,\"start\":52098},{\"end\":52133,\"start\":52120},{\"end\":53029,\"start\":53012},{\"end\":53050,\"start\":53029},{\"end\":53067,\"start\":53050},{\"end\":53084,\"start\":53067},{\"end\":53098,\"start\":53084},{\"end\":53113,\"start\":53098},{\"end\":53131,\"start\":53113},{\"end\":53149,\"start\":53131},{\"end\":53165,\"start\":53149},{\"end\":53179,\"start\":53165},{\"end\":53863,\"start\":53845},{\"end\":53879,\"start\":53863},{\"end\":54169,\"start\":54151},{\"end\":54185,\"start\":54169},{\"end\":54202,\"start\":54185},{\"end\":54229,\"start\":54202},{\"end\":54452,\"start\":54434},{\"end\":54462,\"start\":54452},{\"end\":54478,\"start\":54462},{\"end\":54495,\"start\":54478},{\"end\":54522,\"start\":54495},{\"end\":54855,\"start\":54844},{\"end\":54869,\"start\":54855},{\"end\":54885,\"start\":54869},{\"end\":54900,\"start\":54885},{\"end\":55180,\"start\":55164},{\"end\":55199,\"start\":55180},{\"end\":55217,\"start\":55199},{\"end\":55472,\"start\":55452},{\"end\":55488,\"start\":55472},{\"end\":55511,\"start\":55488},{\"end\":55730,\"start\":55716},{\"end\":55750,\"start\":55730},{\"end\":55987,\"start\":55973},{\"end\":55999,\"start\":55987},{\"end\":56012,\"start\":55999},{\"end\":56024,\"start\":56012},{\"end\":56038,\"start\":56024},{\"end\":56054,\"start\":56038},{\"end\":56276,\"start\":56258},{\"end\":56287,\"start\":56276},{\"end\":56300,\"start\":56287},{\"end\":56930,\"start\":56912},{\"end\":56942,\"start\":56930},{\"end\":56962,\"start\":56942},{\"end\":56975,\"start\":56962},{\"end\":57558,\"start\":57546},{\"end\":57570,\"start\":57558},{\"end\":57593,\"start\":57570},{\"end\":57972,\"start\":57952},{\"end\":57983,\"start\":57972},{\"end\":57996,\"start\":57983},{\"end\":58014,\"start\":57996},{\"end\":58035,\"start\":58014},{\"end\":58045,\"start\":58035},{\"end\":58749,\"start\":58736},{\"end\":58762,\"start\":58749},{\"end\":58780,\"start\":58762},{\"end\":58997,\"start\":58981},{\"end\":59012,\"start\":58997},{\"end\":59023,\"start\":59012},{\"end\":59275,\"start\":59254},{\"end\":59293,\"start\":59275},{\"end\":59313,\"start\":59293},{\"end\":59330,\"start\":59313},{\"end\":59357,\"start\":59330},{\"end\":59373,\"start\":59357},{\"end\":59722,\"start\":59707},{\"end\":59736,\"start\":59722},{\"end\":59752,\"start\":59736},{\"end\":59766,\"start\":59752},{\"end\":60386,\"start\":60374},{\"end\":60397,\"start\":60386},{\"end\":60407,\"start\":60397},{\"end\":60419,\"start\":60407},{\"end\":60429,\"start\":60419},{\"end\":60440,\"start\":60429},{\"end\":60454,\"start\":60440},{\"end\":61081,\"start\":61063},{\"end\":61092,\"start\":61081},{\"end\":61104,\"start\":61092},{\"end\":61217,\"start\":61205},{\"end\":61226,\"start\":61217},{\"end\":61356,\"start\":61343},{\"end\":61367,\"start\":61356},{\"end\":61389,\"start\":61367},{\"end\":61404,\"start\":61389},{\"end\":62032,\"start\":62020},{\"end\":62045,\"start\":62032},{\"end\":62055,\"start\":62045},{\"end\":62192,\"start\":62179},{\"end\":62203,\"start\":62192},{\"end\":62220,\"start\":62203},{\"end\":62240,\"start\":62220},{\"end\":62256,\"start\":62240},{\"end\":62783,\"start\":62773},{\"end\":62797,\"start\":62783},{\"end\":62812,\"start\":62797},{\"end\":62821,\"start\":62812}]", "bib_venue": "[{\"end\":44005,\"start\":44000},{\"end\":44419,\"start\":44333},{\"end\":44957,\"start\":44938},{\"end\":45208,\"start\":45161},{\"end\":45541,\"start\":45464},{\"end\":46014,\"start\":45956},{\"end\":46465,\"start\":46387},{\"end\":46855,\"start\":46817},{\"end\":47304,\"start\":47224},{\"end\":47752,\"start\":47610},{\"end\":48511,\"start\":48424},{\"end\":49020,\"start\":48951},{\"end\":49297,\"start\":49247},{\"end\":49559,\"start\":49527},{\"end\":49811,\"start\":49706},{\"end\":50270,\"start\":50141},{\"end\":50691,\"start\":50642},{\"end\":51094,\"start\":51008},{\"end\":51607,\"start\":51508},{\"end\":52281,\"start\":52133},{\"end\":53303,\"start\":53179},{\"end\":53956,\"start\":53916},{\"end\":54149,\"start\":54086},{\"end\":54556,\"start\":54522},{\"end\":54842,\"start\":54767},{\"end\":55258,\"start\":55238},{\"end\":55450,\"start\":55405},{\"end\":55714,\"start\":55655},{\"end\":55971,\"start\":55920},{\"end\":56387,\"start\":56300},{\"end\":57061,\"start\":56975},{\"end\":57544,\"start\":57493},{\"end\":58130,\"start\":58045},{\"end\":58734,\"start\":58671},{\"end\":59852,\"start\":59766},{\"end\":60531,\"start\":60454},{\"end\":61061,\"start\":61015},{\"end\":61491,\"start\":61404},{\"end\":62058,\"start\":62055},{\"end\":62420,\"start\":62293},{\"end\":62907,\"start\":62821},{\"end\":44516,\"start\":44421},{\"end\":45630,\"start\":45543},{\"end\":46547,\"start\":46467},{\"end\":47903,\"start\":47754},{\"end\":48605,\"start\":48513},{\"end\":49833,\"start\":49813},{\"end\":50403,\"start\":50272},{\"end\":51186,\"start\":51096},{\"end\":52423,\"start\":52283},{\"end\":53420,\"start\":53410},{\"end\":54577,\"start\":54558},{\"end\":56481,\"start\":56389},{\"end\":57147,\"start\":57063},{\"end\":58248,\"start\":58132},{\"end\":59942,\"start\":59854},{\"end\":60620,\"start\":60533},{\"end\":61582,\"start\":61493},{\"end\":62997,\"start\":62909}]"}}}, "year": 2023, "month": 12, "day": 17}