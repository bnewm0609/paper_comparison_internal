{"id": 6788443, "updated": "2023-11-10 22:51:42.001", "metadata": {"title": "Learning with Symmetric Label Noise: The Importance of Being Unhinged", "authors": "[{\"first\":\"Brendan\",\"last\":\"Rooyen\",\"middle\":[\"van\"]},{\"first\":\"Aditya\",\"last\":\"Menon\",\"middle\":[\"Krishna\"]},{\"first\":\"Robert\",\"last\":\"Williamson\",\"middle\":[\"C.\"]}]", "venue": "NIPS", "journal": "10-18", "publication_date": {"year": 2015, "month": 5, "day": 28}, "abstract": "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the SLN-robustness of the unhinged loss.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2952133801", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/RooyenMW15", "doi": null}}, "content": {"source": {"pdf_hash": "d94eed76926168eef5b967b1f96d34ec687abfbb", "pdf_src": "ArXiv", "pdf_uri": "[\"https://arxiv.org/pdf/1505.07634v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bc0c329249cd85d6d26ffa9c2e3fd849ff2eb778", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/d94eed76926168eef5b967b1f96d34ec687abfbb.txt", "contents": "\nLearning with Symmetric Label Noise: The Importance of Being Unhinged\n28 May 2015\n\nBrendan Van Rooyen brendan.vanrooyen@nicta.com.au \nThe Australian National University \u2020 National ICT\nAustralia\n\nAditya Krishna Menon aditya.menon@nicta.com.au \nThe Australian National University \u2020 National ICT\nAustralia\n\nRobert C Williamson bob.williamson@nicta.com.au \nThe Australian National University \u2020 National ICT\nAustralia\n\nLearning with Symmetric Label Noise: The Importance of Being Unhinged\n28 May 20157F8836B599FF654F082F1CD07E0B7790arXiv:1505.07634v1[cs.LG]\nConvex potential minimisation is the de facto approach to binary classification.However, Long and Servedio [2010]  proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing.This ostensibly shows that convex losses are not SLN-robust.In this paper, we propose a convex, classificationcalibrated loss and prove that it is SLN-robust.The loss avoids the Long and Servedio [2010]  result by virtue of being negatively unbounded.The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss.We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong 2 regularisation makes most standard learners SLN-robust.Experiments confirm the SLN-robustness of the unhinged loss.Learning with symmetric label noiseBinary classification is the canonical supervised learning problem.Given an instance space X, and samples from some distribution D over X \u00d7 {\u00b11}, the goal is to learn a scorer s : X \u2192 R with low misclassification error on future samples drawn from D. Our interest is in the more realistic scenario where the learner observes samples from a distribution D, which is a corruption of D where labels have some constant probability of being flipped.The goal is still to perform well with respect to the unobserved distribution D. This is known as the problem of learning from symmetric label noise (SLN learning)[Angluin and Laird, 1988].Long and Servedio [2010]proved the following negative result on what is possible in SLN learning: there exists a linearly separable D where, when the learner observes some corruption D with symmetric label noise of any nonzero rate, minimisation of any convex potential over a linear function class results in classification performance on D that is equivalent to random guessing.Ostensibly, this establishes that convex losses are not \"SLN-robust\" and motivates the use of non-convex losses [\n\nThe classifier resulting from minimising the unhinged loss is not new [Devroye et al., 1996, Chapter 10], [Sch\u00f6lkopf and Smola, 2002, Section 1.2], [Shawe-Taylor and Cristianini, 2004, Section 5.1].However, establishing this classifier's SLN-robustness, its equivalence to a highly regularised SVM solution, and showing the underlying loss uniquely satisfies a notion of strong SLN-robustness, to our knowledge is novel.\n\n\nBackground and problem setup\n\nFix an instance space X.We denote by D some distribution over X \u00d7 {\u00b11}, with random variables (X, Y) \u223c D. Any D may be expressed via the class-conditional distributions (P, Q) = (P(X | Y = 1), P(X | Y = \u22121)) and base rate \u03c0 = P(Y = 1), or equivalently via the marginal distribution M = P(X) and classprobability function \u03b7 : x \u2192 P(Y = 1 | X = x).We interchangeably write D as D P,Q,\u03c0 or D M,\u03b7 .\n\n\nClassifiers, scorers, and risks\n\nA scorer is any function s : X \u2192 R. A loss is any function : {\u00b11} \u00d7 R \u2192 R. We use \u22121 , 1 to refer to (\u22121, \u2022) and (1, \u2022).The -conditional risk L : (v).Given a distribution D, the -risk of a scorer s is defined as\n[0, 1] \u00d7 R \u2192 R is defined as L : (\u03b7, v) \u2192 \u03b7 \u2022 1 (v) + (1 \u2212 \u03b7) \u2022 \u22121L D (s) . = E (X,Y)\u223cD [ (Y, s(X))] ,(1)\nor equivalently L D (s) = E X\u223cM [L (\u03b7(X), s(X))].For a set S, L D (S) is the set of -risks for all scorers in S.\n\nA function class is any F \u2286 R X .Given some F, the set of restricted Bayes-optimal scorers for a loss are those scorers in F that minimise the -risk:\nS D,F, * . = Argmin s\u2208F L D (s).\nThe set of (unrestricted) Bayes-optimal scorers is S D, * = S D,F, * for F = R X .The restricted -regret of a scorer is its excess risk over that of any restricted Bayes-optimal scorer:\nregret D,F (s) . = L D (s) \u2212 inf t\u2208F L D (t).\nBinary classification is concerned with the risk corresponding to the zero-one loss, 01 : (y, v) \u2192 yv < 0 + 1 2 v = 0 .A loss is classification-calibrated if all its Bayes-optimal scorers are also optimal for zeroone loss: (\u2200D) S D, * \u2286 S D, * 01 .A convex potential is any loss : (y, v) \u2192 \u03c6(yv), where \u03c6 : R \u2192 R + is convex, non-increasing, differentiable with \u03c6 (0) < 0, and \u03c6(+\u221e) = 0 [Long and Servedio, 2010, Definition 1].All convex potential losses are classification-calibrated [Bartlett et al., 2006, Theorem 2.1].\n\n\nLearning with symmetric label noise (SLN learning)\n\nThe problem of learning with symmetric label noise (SLN learning) is the following [Angluin and Laird, 1988, Kearns, 1998, Blum and Mitchell, 1998, Natarajan et al., 2013].For some notional \"clean\" distribution D, which we would like to observe, we instead observe samples from some corrupted distribution SLN(D, \u03c1), for some \u03c1 \u2208 [0, 1/2).The distribution SLN(D, \u03c1) is such that the marginal distribution of instances is unchanged, but each label is independently flipped with probability \u03c1.The goal is to learn a scorer from these corrupted samples such that L D 01 (s) is small.For any quantity in D, we denote its corrupted counterparts in SLN(D, \u03c1) with a bar, e.g.M for the corrupted marginal distribution, and \u03b7 for the corrupted class-probability function; additionally, when \u03c1 is clear from context, we will occasionally refer to SLN(D, \u03c1) by D. By definition of the corruption process, the corruption marginal distribution M = M , and [Natarajan et al., 2013, Lemma 7] (\u2200x \u2208 X) \u03b7(x) = (1 \u2212 2\u03c1) \u2022 \u03b7(x) + \u03c1.\n\n(2)\n\n\nSLN-robustness: formalisation\n\nFor our purposes, a learner ( , F) comprises a loss , and a function class F, with learning being the search for some s \u2208 F that minimises the -risk.Informally, the learner ( , F) is \"robust\" to symmetric label noise (SLN-robust) if minimising over F gives the same classifier on both the clean distribution D, which the learner would like to observe, and SLN(D, \u03c1) for any \u03c1 \u2208 [0, 1/2), which the learner actually observes.We now formalise this notion, and review what is known about the existence of SLN-robust learners.\n\n\nSLN-robust learners: a formal definition\n\nFor some fixed instance space X, let \u2206 denote the set of distributions on X \u00d7 {\u00b11}.Given a notional \"clean\" distribution D, N sln : \u2206 \u2192 2 \u2206 returns the set of possible corrupted versions of D the learner may observe, where labels are flipped with unknown probability \u03c1:\nN sln : D \u2192 SLN(D, \u03c1) | \u03c1 \u2208 0, 1 2 .\nEquipped with this, we define our notion of SLN-robustness.\n\nDefinition 1 (SLN-robustness).We say that a learner ( , F) is SLN-robust if\n(\u2200D \u2208 \u2206) (\u2200D \u2208 N sln (D)) L D 01 (S D,F, * ) = L D 01 (S D,F, * ).(3)\nThat is, SLN-robustness requires that for any level of label noise in the observed distribution D, the classification performance (wrt D) of the learner is the same as if the learner directly observes D. Unfortunately, as we will now see, a widely adopted class of learners is not SLN-robust.\n\n\nConvex potentials with linear function classes are not SLN-robust\n\nFix X = R d , and consider learners employing a convex potential , and a function class of linear scorers\nF lin = {x \u2192 w, x | w \u2208 R d }.\nThis captures e.g. the linear SVM and logistic regression, which are widely studied in theory and applied in practice.Unfortunately, these learners are not SLN-robust: Long and Servedio [2010, Theorem 2] give an example where, when learning under symmetric label noise, for any convex potential , the corrupted -risk minimiser over F lin has classification performance equivalent to random guessing on D. This implies that ( , F lin ) is not SLN-robust1 as per Definition 1. (All Proofs may be found in Appendix A.) Proposition 1 (Long and Servedio [2010, Theorem 2]).Let X = R d for any d \u2265 2. Pick any convex potential .Then, ( , F lin ) is not SLN-robust.\n\nThe widespread practical use of convex potential based learners makes Proposition 1 a disheartening result, and motivates the search for other learners that are SLN-robust.\n\n\nThe fallout: what learners are SLN-robust?\n\nIn light of Proposition 1, there are two ways to proceed in order to obtain SLN-robust learners: either we change the class of losses , or we change the function class F.\n\nThe first approach has been pursued in a large body of work that embraces non-convex losses [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010, Ding and Vishwanathan, 2010, Denchev et al., 2012, Manwani and Sastry, 2013].However, while such losses avoid the conditions of Proposition 1, this does not automatically imply that they are SLN-robust when used with F lin .In Appendix B, we present evidence that some of these losses are in fact not SLN-robust when used with F lin .\n\nThe second approach is to instead consider suitably rich F that contains the Bayes-optimal scorer for D, e.g. by employing a universal kernel.With this choice, one can still use a convex potential loss; in fact, owing to Equation 2, using any classification-calibrated loss will result in an SLN-robust learner when F = R X .\n\nProposition 2. Pick any classification-calibrated .Then, ( , R X ) is SLN-robust.\n\nBoth approaches have drawbacks.The first approach has a computational penalty, as it requires optimising a non-convex loss.The second approach has a statistical penalty, as estimation rates with a rich F will require a larger sample size.Thus, it appears that SLN-robustness involves a computational-statistical tradeoff.\n\nHowever, there is a variant of the first option: pick a loss that is convex, but not a convex potential.If an SLN-robust loss of this type exists, it affords the computational and statistical advantages of minimising convex risks with linear scorers.Manwani and Sastry [2013] demonstrated that square loss, (y, v) = (1 \u2212 yv) 2 , is one such loss.We will show that there is a simpler loss that is similarly convex, classification-calibrated, and SLN-robust, but is not in the class of convex potentials by virtue of being negatively unbounded.To derive this loss, it is helpful to interpret robustness in terms of a noise-correction procedure on loss functions.\n\n\nSLN-robustness: a noise-corrected loss perspective\n\nThe definition of SLN-robustness (Equation 3) involves optimal scorers with the same loss over two different distributions.We now re-express this to reason about optimal scorers on the same distribution, but with two different losses.This will help characterise the set of losses that are SLN-robust.\n\n\nReformulating SLN-robustness via noise-corrected losses\n\nGiven any \u03c1 \u2208 [0, 1/2), Natarajan et al. [2013, Lemma 1] showed how to associate with a loss a noisecorrected counterpart , such that for any D, L D (s) = L D (s).The loss is defined as follows.\n\nDefinition 2 (Noise-corrected loss).Given any loss and \u03c1 \u2208 [0, 1/2), the noise-corrected loss is\n(\u2200y \u2208 {\u00b11}) (\u2200v \u2208 R) (y, v) = (1 \u2212 \u03c1) \u2022 (y, v) \u2212 \u03c1 \u2022 (\u2212y, v) 1 \u2212 2\u03c1 .(4)\nSince depends on the unknown parameter \u03c1, it is not directly usable to design an SLN-robust learner.Nonetheless, it is a useful theoretical construct, since the risk equivalence between L D (s) and L D (s) means that for any F, minimisation of the -risk on D over F is equivalent to minimisation of the -risk on D over F, i.e. S D,F, * = S D,F, * .With this, we can re-express the SLN-robustness of a learner ( , F) as\n(\u2200D \u2208 \u2206) (\u2200D \u2208 N sln (D)) L D 01 (S D,F, * ) = L D 01 (S D,F, * ). (5)\nThis reformulation is useful, because to characterise SLN-robustness of ( , F), we can now consider conditions on such that and its noise-corrected counterpart induce the same restricted Bayes-optimal scorers.\n\n\nCharacterising a stronger notion of SLN-robustness\n\nManwani and Sastry [2013, Theorem 1] proved a sufficient condition on such that Equation 5 holds, namely,\n(\u2203C \u2208 R)(\u2200v \u2208 R) 1 (v) + \u22121 (v) = C.(6)\nFor such a loss, is a scaled and translated version of , so that trivially S D,F, * = S D,F, * .Ideally, one would like to characterise when Equation 5 holds.While this is an open question, interestingly, we can show that under a stronger requirement on the losses and , the condition in Equation 6 is also necessary.The stronger requirement is that the corresponding risks order all stochastic scorers identically.A stochastic scorer is simply a mapping f : X \u2192 \u2206 R , where \u2206 R is the set of distributions over the reals.In a slight abuse of notation, we denote the -stochastic risk of f by\nL D (f ) = E (X,Y)\u223cD E S\u223cf (X) [ (Y, S)] .\nEquipped with this, we define a notion of order equivalence of loss pairs.Definition 3 (Order equivalent loss pairs).We say that a pair of losses ( , \u02dc ) are order equivalent if\n(\u2200D) (\u2200f, g \u2208 \u2206 X R ) L D (f ) \u2264 L D (g) \u21d0\u21d2 L D \u02dc (f ) \u2264 L D \u02dc (g).\nClearly, if two losses are order equivalent, their corresponding risks have the same restricted minimisers.Consequently, if ( , ) are order equivalent for every \u03c1 \u2208 [0, 1/2), this implies that S D,F, * = S D,F, * for any F, which by Equation 5 means that for any F, the learner ( , F) is SLN-robust.We can thus think of order equivalence of ( , ) as signifying strong SLN-robustness of a loss .\n\nDefinition 4 (Strong SLN-robustness).We say a loss is strongly SLN-robust if for every \u03c1 \u2208 [0, 1/2), ( , ) are order equivalent.\n\nWe establish that the sufficient condition of Equation 6 is also necessary for strong SLN-robustness of .Proposition 3. A loss is strongly SLN-robust if and only if it satisfies Equation 6.\n\nWe now return to our original goal, which was to find a convex that is SLN-robust for F lin (and ideally more general function classes).The above suggests that to do so, it is reasonable to consider as admissible those losses that satisfy Equation 6.Unfortunately, it is evident that if is convex, non-constant, and bounded below by zero, then it cannot possibly be admissible in this sense.But we now show that removing the boundedness restriction allows for the existence of a convex admissible loss.\n\n5 The unhinged loss: a convex, classification-calibrated, strongly SLNrobust loss\n\nConsider the following simple, but non-standard convex loss:\nunh 1 (v) = 1 \u2212 v and unh \u22121 (v) = 1 + v.\nA peculiar property of the loss is that it is negatively unbounded, an issue we discuss in \u00a75.3.Compared to the hinge loss, the loss does not clamp at zero, i.e. it does not have a hinge.Thus, we call this the unhinged loss2 .The loss has a number of attractive properties, the most immediate of which is its SLN-robustness.\n\n\nThe unhinged loss is strongly SLN-robust\n\nSince unh 1 (v) + unh \u22121 (v) = 0 we conclude from Proposition 3 that unh is strongly SLN-robust, and thus that ( unh , F) is SLN-robust for any choice of F. Further, the following uniqueness property is not hard to show.Proposition 4. Pick any convex loss .Then,\n(\u2203C \u2208 R) 1 (v) + \u22121 (v) = C \u21d0\u21d2 (\u2203A, B, D \u2208 R) 1 (v) = \u2212A \u2022 v + B, \u22121 (v) = A \u2022 v + D.\nThat is, up to scaling and translation, unh is the only convex loss that is strongly SLN-robust.\n\nReturning to the case of linear scorers, the above implies that ( unh , F lin ) is SLN-robust.This does not contradict Proposition 1, since unh is not a convex potential as it is negatively unbounded.Intuitively, this property allows the loss to compensate for the high penalty incurred by instances that are misclassified with high margin by allowing for a high gain for instances that correctly classified with high margin.\n\n\nThe unhinged loss is classification calibrated\n\nSLN-robustness is by itself insufficient for a learner to be useful.For example, a loss that is uniformly zero is strongly SLN-robust, but is useless as it is not classification-calibrated. Fortunately, the unhinged loss is classification-calibrated, as we now establish.For reasons that shall be discussed subsequently, we consider minimisation of the risk over F B = [\u2212B, +B] X , the set of scorers with range bounded by B \u2208 [0, \u221e).\nProposition 5. Fix = unh . Then, for any D M,\u03b7 , B \u2208 [0, \u221e), S D,F B , * = {x \u2192 B \u2022 sign(2\u03b7(x) \u2212 1)}.\nThus, for every B \u2208 [0, \u221e), the restricted Bayes-optimal scorer over F B has the same sign as the Bayesoptimal classifier for 0-1 loss.In the limiting case where F = R X , the optimal scorer is attainable if we operate over the extended reals R \u222a {\u00b1\u221e}, in which case we can conclude that unh is classification-calibrated.\n\n\nEnforcing boundedness of the loss\n\nWhile the classification-calibration of unh is encouraging, Proposition 5 implies that its (unrestricted) Bayesrisk is \u2212\u221e.Thus, the regret of every non-optimal scorer s is identically +\u221e, which hampers analysis of consistency.In orthodox decision theory, analogous theoretical issues arise when attempting to establish basic theorems with unbounded losses [Ferguson, 1967, pg. 78], [Schervish, 1995, pg. 172].\n\nWe can side-step this issue by restricting attention to bounded scorers, so that unh is effectively bounded.By Proposition 5, this does not affect the classification-calibration of the loss.In the context of linear scorers, boundedness of scorers can be achieved by regularisation: instead of working with F lin , one can instead use\nF lin,\u03bb = {x \u2192 w, x | ||w|| 2 \u2264 1/ \u221a \u03bb}, where \u03bb > 0, so that F lin,\u03bb \u2286 F R/ \u221a \u03bb for R = sup x\u2208X ||x|| 2 .\nObserve that restricting to bounded scorers does not affect the SLN-robustness of unh , because ( unh , F) is SLN-robust for any F. Thus, for example, ( unh , F lin,\u03bb ) is SLN-robust for any \u03bb > 0. As we shall see in \u00a76.3, working with F lin,\u03bb also lets us establish SLN-robustness of the hinge loss when \u03bb is large.\n\n\nUnhinged loss minimisation on corrupted distribution is consistent\n\nUsing bounded scorers makes it possible to establish a surrogate regret bound for the unhinged loss.This shows classification consistency of unhinged loss minimisation on the corrupted distribution.\n\nProposition 6. Fix = unh .Then, for any D, \u03c1\n\u2208 [0, 1/2), B \u2208 [1, \u221e), and scorer s \u2208 F B , regret D 01 (s) \u2264 regret D,F B (s) = 1 1 \u2212 2\u03c1 \u2022 regret D,F B (s).\nStandard rates of convergence via generalisation bounds are also trivial to derive; see Appendix D. We now turn to the question of how to minimise the unhinged loss when using a kernelised scorer.\n\n\nLearning with the unhinged loss and kernels\n\nWe now show that the optimal solution for the unhinged loss when employing regularisation and kernelised scorers has a simple form.This sheds further light on SLN-robustness and regularisation.\n\n\nThe centroid classifier optimises the unhinged loss\n\nConsider minimising the unhinged risk over some ball in a reproducing kernel Hilbert space H with kernel k, i.e. consider the function class of kernelised scorers\nF H,\u03bb = {s : x \u2192 w, \u03a6(x) H | ||w|| H \u2264 1/\n\u221a \u03bb} for some \u03bb > 0, where \u03a6 : X \u2192 H is some feature mapping.Equivalently, given a distribution3 D, we want\nw * unh,\u03bb = argmin w\u2208H E (X,Y)\u223cD [1 \u2212 Y \u2022 w, \u03a6(X) ] + \u03bb 2 w, w H .(7)\nThe first-order optimality condition implies that\nw * unh,\u03bb = 1 \u03bb \u2022 E (X,Y)\u223cD [Y \u2022 \u03a6(X)] .(8)\nThus, the optimal scorer for the unhinged loss is simply\ns * unh,\u03bb : x \u2192 1 \u03bb \u2022 E (X,Y)\u223cD [Y \u2022 k(X, x)] = x \u2192 1 \u03bb \u2022 \u03c0 \u2022 E X\u223cP [k(X, x)] \u2212 (1 \u2212 \u03c0) \u2022 E X\u223cQ [k(X, x)] .(9)\nThat is, we score an instance based on the difference of the aggregate similarity to the positive instances, and the aggregate similarity to the negative instances.This is equivalent to a nearest centroid classifier [Manning et al., 2008, pg. 181] [Tibshirani et al., 2002] [Shawe-Taylor and Cristianini, 2004, Section 5.1].The quantity w * unh,\u03bb can be interpreted as the kernel mean map of D; see Appendix E for more related work.Equation 9gives a simple way to understand the SLN-robustness of ( unh , F H,\u03bb ): it is easy to establish (see Appendix C) that the optimal scorers on the clean and corrupted distributions only differ by a scaling, i.e.\n\n(\u2200x \u2208 X)\nE (X,Y)\u223cD [Y \u2022 k(X, x)] = 1 1 \u2212 2\u03c1 \u2022 E (X,Y)\u223cD Y \u2022 k(X, x) .\n(10)\n\n\nPractical considerations\n\nWe note several points relating to practical usage of the unhinged loss with kernelised scorers.First, crossvalidation is not required to select \u03bb, since s * unh,\u03bb depends trivially on the regularisation constant: changing \u03bb only changes the magnitude of scores, not their sign.Thus, regularisation simply controls the scale of the predicted scores, and for the purposes of classification, one can simply use \u03bb = 1.\n\nSecond, we can easily extend the scorers to use a bias regularised with strength 0 < \u03bb b = \u03bb.Tuning \u03bb b is equivalent to computing s * unh,\u03bb as per Equation 9, and tuning a threshold on a holdout set.Third, when H = R d for d small, we can store w * unh,\u03bb explicitly, and use this to make predictions.For high (or infinite) dimensional H, we can make predictions directly via Equation 9.However, when learning with a training sample S \u223c D n , this would require storing the entire sample for use at test time, which is undesirable.To alleviate this, for a translation-invariant kernel one can use random Fourier features [Rahimi and Recht, 2007] to find an approximate embedding of H into some low-dimensional R d , and then store w * unh,\u03bb as usual.Alternately, one can post hoc search for a sparse approximation to w * unh,\u03bb , for example using kernel herding [Chen et al., 2012].\n\nWe now show that under some assumptions, w * unh,\u03bb coincides with the solution of two established methods; Appendix E discusses some further relationships, e.g. to the maximum mean discrepancy.\n\n\nEquivalence to a highly regularised SVM and other convex potentials\n\nThere is an interesting equivalence between the unhinged solution and that of a highly regularised SVM.\n\nProposition 7. Pick any D and \u03a6 :\nX \u2192 H such that R = sup x\u2208X ||\u03a6(x)|| H < \u221e. For any \u03bb > 0, let w * hinge,\u03bb = argmin w\u2208H E (X,Y)\u223cD [max(0, 1 \u2212 Y \u2022 w, \u03a6(x) H )] + \u03bb 2 w, w H be the soft-margin SVM solution. Then, if \u03bb \u2265 R 2 , w * hinge,\u03bb = w * unh,\u03bb .\nSince we know that ( unh , F H,\u03bb ) is SLN-robust, it follows immediately that for hinge : (y, v) \u2192 max(0, 1\u2212 yv), ( hinge , F H,\u03bb ) is similarly SLN-robust provided \u03bb is sufficiently large.That is, strong 2 regularisation (and a bounded feature map) endows the hinge loss with SLN-robustness4 .\n\nProposition 7 can be generalised to show that with sufficiently strong regularisation, the limiting solution of any twice differentiable convex potential will be the unhinged solution, i.e. , the centroid classifier.Intuitively, with strong regularisation, one only considers the behaviour of a loss near zero; but since a convex potential \u03c6 has \u03c6 (0) < 0, it will be well-approximated by the unhinged loss near zero (which is simply the linear approximation to \u03c6).This shows that strong 2 regularisation endows most learners with SLNrobustness.\n\nProposition 8. Pick any D, bounded feature mapping \u03a6 : X \u2192 H, and twice differentiable convex potential \u03c6.Let w * \u03c6,\u03bb be the minimiser of the regularised \u03c6 risk.Then,\n(\u2200 > 0) (\u2203\u03bb 0 > 0) (\u2200\u03bb > \u03bb 0 ) w * \u03c6,\u03bb ||w * \u03c6,\u03bb || H \u2212 w * unh,\u03bb ||w * unh,\u03bb || H 2 H \u2264 .\n\nEquivalence to Fisher Linear Discriminant with whitened data\n\nRecall that for binary classification on D M,\u03b7 , the Fisher Linear Discriminant (FLD) finds a weight vector proportional to the minimiser of square loss sq : (y, v)\n\u2192 (1 \u2212 yv) 2 [Bishop, 2006, Section 4.1.5], w * sq,\u03bb = (E X\u223cM [XX T ] + \u03bbI) \u22121 \u2022 E (X,Y)\u223cD [Y \u2022 X].(11)\nBy Equation 10, and the fact that the corrupted marginal M = M , we see that w * sq,\u03bb is only changed by a scaling factor under label noise.This provides an alternate proof of the fact that ( sq , F lin ) is SLN-robust5 [Manwani and Sastry, 2013, Theorem 2].\n\nClearly, the unhinged loss solution w * unh,\u03bb is equivalent to the FLD and square loss solution w * sq,\u03bb when the input data is whitened i.e.E X\u223cM XX T = I.With a well-specified F, e.g. with a universal kernel, both the unhinged and square loss asymptotically recover the optimal classifier, but the unhinged loss does not require a matrix inversion.With a misspecified F, one cannot in general argue for the superiority of the unhinged loss over square loss, or vice-versa, as there is no universally good surrogate to the 0-1 loss [Reid and Williamson, 2010, Appendix A]; Appendix F, Appendix G illustrate examples where both losses may underperform.\n\n\nSLN-robustness of unhinged loss: empirical illustration\n\nWe now illustrate that the SLN-robustness of the unhinged loss is empirically manifest.We reiterate that with high regularisation, the unhinged solution is equivalent to an SVM (and in the limit to any classificationcalibrated loss) solution.Thus, the experiments do not aim to assert that the unhinged loss is \"better\" than other losses, but rather, to demonstrate that its SLN-robustness is not purely theoretical.\n\nWe first show that the unhinged risk minimiser performs well on the example of Long and Servedio [2010].Figure 1 shows the distribution D, where X = {(1, 0), (\u03b3, 5\u03b3), (\u03b3, \u2212\u03b3)} \u2282 R 2 , with marginal distribution M = { 1 4 , 1 4 , 1 2 } and all three instances are deterministically positive.We pick \u03b3 = 1/2.From Figure 1, we see the unhinged minimiser perfectly classifies all three points, regardless of the level of label noise.The hinge risk minimiser is perfect when there is no label noise, but with even a small amount of label noise, achieves an error rate of 50%.Hinge t-logistic Unhinged \u03c1 = 0 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.1 0.15 \u00b1 0.27 0.00 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.2 0.21 \u00b1 0.30 0.00 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.3 0.38 \u00b1 0.37 0.22 \u00b1 0.08 0.00 \u00b1 0.00 \u03c1 = 0.4 0.42 \u00b1 0.36 0.22 \u00b1 0.08 0.00 \u00b1 0.00 \u03c1 = 0.49 0.47 \u00b1 0.38 0.39 \u00b1 0.23 0.34 \u00b1 0.48\n\nTable 1: Mean and standard deviation of the 0-1 error over 125 trials on Long and Servedio [2010].Grayed cells denote the best performer at that noise rate.\n\nWe next consider minimisers of the empirical risk from a random training sample: we construct a training set of 800 instances, injected with varying levels of label noise, and evaluate classification performance on a test set of 1000 instances.We compare the hinge, t-logistic (for t = 2) [Ding and Vishwanathan, 2010] and unhinged minimisers.For each loss, we use a linear scorer without a bias term, and set the regularisation strength \u03bb = 10 \u221216 .From Table 1, it is apparent that even at 40% label noise, the unhinged classifier is able to find a perfect solution.By contrast, both other losses suffer at even moderate noise rates.\n\nWe next report results on some UCI datasets, where we additionally tune a threshold so as to ensure the best training set 0-1 accuracy.Table 2 summarises results on a sample of four datasets.(Appendix H contains results with more datasets, performance metrics, and losses.)While the unhinged loss is sometimes outperformed at low noise, it tends to be much more robust at high levels of noise: even at noise close to 50%, it is often able to learn a classifier with some discriminative power.\n\n\nConclusion and future work\n\nWe have proposed a convex, classification-calibrated loss, proved that is robust to symmetric label noise (SLN-robust), shown it is the unique loss that satisfies a notion of strong SLN-robustness, established that it is optimised by the nearest centroid classifier, and also shown how the nature of the optimal solution implies that most convex potentials, such as the SVM, are also SLN-robust when highly regularised.Future work includes studying losses robust to asymmetric noise, and outliers in instance space.\n\nProofs for \"Learning with Symmetric Label Noise: The Importance of Being Unhinged\"\n\n\nA Proofs of results in main body\n\nWe now present proofs of all results in the main body.\n\nProof of Proposition 1.This result is stated implicitly in Long and Servedio [2010, Theorem 2]; the aim of this proof is simply to make the result explicit.Let X = {(1, 0), (\u03b3, 5\u03b3), (\u03b3, \u2212\u03b3), (\u03b3, \u2212\u03b3)} \u2282 R 2 , for some \u03b3 < 1/6.Let the marginal distribution over X be uniform.Let \u03b7 : x \u2192 1, i.e. let every example be deterministically positive.Now suppose we observe some SLN(D, \u03c1), for \u03c1 \u2208 [0, 1/2).We minimise the -risk some convex potential : (y, v) \u2192 \u03c6(y, v) using a linear function class6 F lin .Then, Long and Servedio [2010, Theorem 2] establishes that\n(\u2200s \u2208 S D,F lin , * ) L D 01 (s) = 1 2 .\nOn the other hand, since D is linearly separable and a convex potential is classification-calibrated, we must have L D 01 (S D,F lin , * ) = 0. Consequently, for any convex potential , ( , F lin ) is not SLN-robust.\n\nProof of Proposition 2. Let \u03b7 be the class-probability function of D. By [Natarajan et al., 2013, Lemma 7],\n\n(\u2200x \u2208 X) sign(2\u03b7(x) \u2212 1) = sign(2\u03b7(x) \u2212 1), so that the optimal classifiers on the clean and corrupted distributions coincide.Therefore, intuitively, if the Bayes-optimal solution for loss recovers sign(2\u03b7(x) \u2212 1), it will also recover sign(2\u03b7(x) \u2212 1).Formally, since is classification-calibrated, for any D \u2208 \u2206, and s \u2208 S D, * (\u2200x \u2208 X) sign(s(x)) = sign(2\u03b7(x) \u2212 1), and similarly, for any D \u2208 N sln (D), and s \u2208 S D, *\n\n(\u2200x \u2208 X) sign(s(x)) = sign(2\u03b7(x) \u2212 1).\n\nThus, for any D, D, since the 0-1 risk of a scorer depends only on its sign,\nL D 01 (s) = L D 01 (s) = L D 01 (sign(2\u03b7 \u2212 1)) = L D 01 (sign(2\u03b7 \u2212 1)) = L D 01 (s). Consequently, i.e. ( , R X ) is SLN-robust.\nProof of Proposition 3. ( \u21d0= ).If satisfies Equation 6, then its noise corrected counterpart is\n(\u2200y \u2208 {\u00b11})(\u2200v \u2208 R) (y, v) = 1 1 \u2212 2\u03c1 \u2022 (y, v) \u2212 C \u2022 \u03c1 1 \u2212 2\u03c1 ,\nthat is, it is a scaled and translated version of .Consequently, for any \u03c1, the corresponding risk will be a scaled and translated version of the -risk.It is immediate that the two losses will be order equivalent for any \u03c1.\n\n( =\u21d2 ).Recall that S denotes the distribution of scores.For any stochastic scorer f , let\nS f : a \u2192 P(S = a)\nbe the corresponding marginal distribution of scores.Similarly, let\nM a : x \u2192 P(X = x | S = a)\nbe the conditional distribution of instances given a predicted score a \u2208 R. Finally, for any a \u2208 R, let D a = (M a , \u03b7) be an induced distribution over X \u00d7 {\u00b11}.\n\nWith the above, we can rewrite the stochastic risk as\nL D (f ) = E S\u223cS f E (X,Y)\u223cD S [ (Y, S)] = E S\u223cS f L D S (S) .\nThat is, we average, over all achievable scores according to f , the risks of that constant prediction on an appropriately reweighed version of the original distribution D.Then, for some fixed \u03c1 \u2208 [0, 1/2), the fact that and are order equivalent can be written Then, order equivalence can be trivially re-expressed as\n(\u2200D) (\u2200f, g \u2208 \u2206 X R ) E S\u223cS f L D S (S) \u2264 E S\u223cSg L D S (S) \u21d0\u21d2 E S\u223cS f L D S (S) \u2264 E S\u223cSg L D S (S) .(\u2200D) (\u2200f, g \u2208 \u2206 X R ) E S\u223cS f U D (S) \u2265 E S\u223cSg U D (S) \u21d0\u21d2 E S\u223cS f V D (S) \u2265 E S\u223cSg V D (S) .\nThat is, for any fixed distribution D, the utility functions U D , V D specify the same ordering over distributions in \u2206 R .Therefore, by DeGroot [1970, Section 7.9, Theorem 2], for any fixed D, they must be affinely related:\n(\u2200D) (\u2203\u03b1, \u03b2 \u2208 R) (\u2200a \u2208 R) U D (a) = \u03b1 \u2022 V D (a) + \u03b2.\nConverting this back to losses, and using the definition of strong SLN-robustness,\n(\u2200D) (\u2200\u03c1 \u2208 [0, 1/2)) (\u2203\u03b1, \u03b2 \u2208 R) (\u2200a \u2208 R) L Da (a) = \u03b1 \u2022 L Da (a) + \u03b2 or (\u2200D) (\u2200\u03c1 \u2208 [0, 1/2)) (\u2203\u03b1, \u03b2 \u2208 R) (\u2200a \u2208 R) E (X,Y)\u223cDa (Y, a) \u2212 (\u03b1 \u2022 (Y, a) + \u03b2) = 0.\nFor this to hold for all possible D, it must be true that\n(\u2200\u03c1 \u2208 [0, 1/2)) (\u2203\u03b1, \u03b2 \u2208 R) (\u2200y, v) (y, v) = \u03b1 \u2022 (y, v) + \u03b2.\nBy Lemma 9, the result follows.\n\nProof of Proposition 4. ( \u21d0= ).Clearly for an satisfying the given condition,\n1 (v) + \u22121 (v) = B + C, a constant. ( =\u21d2 ).\nBy assumption, 1 is convex.By the given condition, equivalently, (\u2203C \u2208 R) C \u2212 1 is convex.But this is in turn equivalent to \u2212 1 also being convex.The only possibility for both 1 and \u2212 1 being convex is that 1 is affine, hence showing the desired implication.\n\nProof of Proposition 5. Fix = unh .It is easy to check that\n(\u2200\u03b7 \u2208 [0, 1]) (\u2200v \u2208 R) L (\u03b7, v) = (1 \u2212 2\u03b7) \u2022 v,(12)\nand so\n(\u2200\u03b7 \u2208 [0, 1]) argmin v\u2208[\u2212B,+B] L (\u03b7, v) = +B if \u03b7 > 1 2 \u2212B else.\nProof of Proposition 6. Fix = unh .Since by Equation 12L unh (\u03b7, v) = (1 \u2212 2\u03b7) \u2022 v, we have that\nL D (s) = \u2212 E X\u223cM [(2\u03b7(X) \u2212 1) \u2022 s(X)] ,\nand since the restricted Bayes-optimal scorer is x \u2192 B \u2022 sign(2\u03b7(x) \u2212 1),\nL D,F B , * = \u2212B \u2022 E X\u223cM [|2\u03b7(X) \u2212 1|] . Thus, regret D,F B (s) = E X\u223cM [|2\u03b7(X) \u2212 1| \u2022 (B \u2212 s(X) \u2022 sign(2\u03b7(X) \u2212 1))]\nNow, since the scorer x \u2192 sign(2\u03b7(x) \u2212 1) \u2208 S D, * 01 \u2229 F B , we have that regret D,F B\n\n\n01\n\n(s) = regret D 01 (s).Further, we have that\nregret D 01 (s) = E X\u223cM [|2\u03b7(X) \u2212 1| \u2022 s(X) \u2022 sign(2\u03b7(X) \u2212 1) < 0 ] . But if B \u2265 1, v < 0 \u2264 B \u2212 v. Thus, regret D,F B 01 (s) \u2264 regret D,F B (s).\nFinally, by Equation 4, for = unh ,\n(\u2200y \u2208 {\u00b11}) (\u2200v \u2208 R) (y, v) = 1 1 \u2212 2\u03c1 \u2022 (y, v),\ni.e. the unhinged loss is its own noise-corrected loss, with a scaling factor of 1 1\u22122\u03c1 .Thus, since the -regret on D and -regret on D coincide,\nregret D,F B (s) = regret D,F B (s) = 1 1 \u2212 2\u03c1 \u2022 regret D,F B (s).\nProof of Proposition 7. On a distribution D, a soft-margin SVM solves\nmin w\u2208H E (X,Y)\u223cD [max(0, 1 \u2212 Y \u2022 w, \u03a6(x) H )] + \u03bb 2 w, w 2 H .\nLet w * hinge,\u03bb denote the optimal solution to this objective.Now, by Shalev-Shwartz et al. [2007, Theorem 1],\n||w * hinge,\u03bb || H \u2264 1 \u221a \u03bb . Now suppose that R = sup x\u2208X ||\u03a6(x)|| H < \u221e.\nThen, by the Cauchy-Schwartz inequality,\n(\u2200x \u2208 X) | w * hinge,\u03bb , \u03a6(x) H | \u2264 ||w * hinge,\u03bb || H \u2022 ||\u03a6(x)|| H \u2264 R \u221a \u03bb . It follows that if \u03bb \u2265 R 2 , then (\u2200x \u2208 X) | w * hinge,\u03bb , \u03a6(x) H | \u2264 1.\nBut this means that we never activate the flat portion of the hinge loss.Thus, for \u03bb \u2265 R 2 , the SVM objective is equivalent to\nmin w\u2208H E (X,Y)\u223cD [1 \u2212 Y \u2022 w, \u03a6(x) H ] + \u03bb 2 w, w 2 H .\nwhich means the optimal solution will coincide with that of the regularised unhinged loss.Therefore, we can view unhinged loss minimisation as corresponding to learning a highly regularised SVM7 .\n\nProof of Proposition 8. Fix some distribution D. Let\n\u00b5 = E (X,Y)\u223cD [Y \u2022 \u03a6(X)]\nbe the optimal unhinged solution with regularisation strength \u03bb\n= 1. Observe that ||\u00b5|| H \u2264 R = sup x\u2208X ||\u03a6(x)|| H < \u221e. For some r > 0, let w * \u03c6 = argmin ||w|| H \u2264r L D \u03c6 (w)\nbe the optimal \u03c6 solution with norm bounded by r.Similarly, let\nw * unh = ||w * \u03c6 || \u2022 \u00b5 ||\u00b5|| H\nbe the optimal unhinged solution with the same norm as the optimal \u03c6 solution.We will show that these two vectors have similar unhinged risks, and use this to show that the corresponding unit vectors must be close.\n\nBy definition, a convex potential has \u03c6 (0) < 0. Without loss of generality, we can scale the potential so that \u03c6 (0) = \u22121.Then, since \u03c6 is convex, it is lower bounded by the linear approximation at zero:\n(\u2200v \u2208 R) \u03c6(v) \u2212 \u03c6(0) \u2265 \u2212v.\nObserve that the RHS is the unhinged loss.Thus, the unhinged risk can be bounded by its \u03c6 counterpart.In particular, at the optimal \u03c6 solution,\nL unh (w * \u03c6 ) \u2264 L \u03c6 (w * \u03c6 ) \u2212 \u03c6(0).\nTherefore, the difference between the unhinged and \u03c6 optimal solutions is\nL unh (w * \u03c6 ) \u2212 L unh (w * unh ) \u2264 L \u03c6 (w * \u03c6 ) \u2212 L unh (w * unh ) \u2212 \u03c6(0) \u2264 L \u03c6 (w * unh ) \u2212 L unh (w * unh ) \u2212 \u03c6(0) = E (X,Y)\u223cD [\u03c6(Y w * unh , \u03a6(X) H ) + Y w * unh , \u03a6(X) H ] \u2212 \u03c6(0) = E (X,Y)\u223cD \u03c6(Y w * unh , \u03a6(X) H ) ,(13)\nwhere \u03c6 : v \u2192 \u03c6(v) \u2212 \u03c6(0) + v. (The second line follows by definition of optimality of w * \u03c6 amongst all vectors with norm bounded by r.)We have already established that \u03c6 \u2265 0. Now, by Taylor's remainder theorem,\n(\u2200v \u2208 (\u22121, 1)) \u03c6(v) \u2264 a 2 v 2 , (14)\nwhere a = max v\u2208[\u22121,1] \u03c6 (v).But by Cauchy-Schwartz, we can restrict attention in Equation 13to the behaviour of \u03c6 in the interval 14and a further application of Cauchy-Schwartz yield\nI = [\u2212||w * unh || H \u2022 R, ||w * unh || H \u2022 R], where R = sup x\u2208X ||\u03a6(x)|| H < \u221e. Therefore, if r \u2264 1 R , EquationL unh (w * \u03c6 ) \u2212 L unh (w * unh ) \u2264 a 2 \u2022 E (X,Y)\u223cD w * unh , \u03a6(X) 2 H ) \u2264 a 2 \u2022 E X\u223cM ||w * unh || 2 H \u2022 ||\u03a6(X)|| 2 H \u2264 aR 2 2 \u2022 ||w * unh || 2 H . Now, the unhinged risk is L D unh (w) = \u2212 w, \u00b5 H . Thus, \u2212 w * \u03c6 , \u00b5 H + w * unh , \u00b5 H \u2264 aR 2 2 \u2022 ||w * unh || 2 H .\nRearranging, and by definition of w * unh ,\nw * \u03c6 , \u00b5 H \u2265 w * unh , \u00b5 H \u2212 aR 2 2 \u2022 ||w * unh || 2 H = ||w * \u03c6 || H \u2022 ||\u00b5|| H \u2212 aR 2 2 \u2022 ||w * \u03c6 || 2 H = ||w * \u03c6 || H \u2022 ||\u00b5|| H \u2022 1 \u2212 aR 2 2||\u00b5|| H \u2022 ||w * \u03c6 || H \u2265 ||w * \u03c6 || H \u2022 ||\u00b5|| H \u2022 1 \u2212 aR 2 2||\u00b5|| H \u2022 r ,\nwhere the last line is since by definition ||w * \u03c6 || H \u2264 r.Thus, for = aR 2 2||\u00b5|| H ,\nw * \u03c6 ||w * \u03c6 || H , \u00b5 ||\u00b5|| H H \u2265 1 \u2212 .\nIt follows that the two unit vectors can be made arbitrarily close to each other by decreasing r.Since this corresponds to increasing the strength of regularisation (by Lagrange duality), and since \u00b5 ||\u00b5|| H corresponds to the normalised unhinged solution for any regularisation strength, the result follows.\n\n\nA.1 Additional helper lemmas\n\nLemma 9. Pick any loss .Suppose that\n(\u2200\u03c1 \u2208 [0, 1/2)) (\u2203\u03b1, \u03b2 \u2208 R) (\u2200y, v) (y, v) = \u03b1 \u2022 (y, v) + \u03b2. Then, (\u2203C \u2208 R) (\u2200v \u2208 R) 1 (v) + \u22121 (v) = C.\nProof of Lemma 9.By the definition of the noise-corrected loss (Equation 4), the given statement is that there exist \u03b1, \u03b2 : [0, 1/2) \u2192 R with\n(\u2200\u03c1 \u2208 [0, 1/2)) (\u2200v \u2208 R) 1 (v) \u22121 (v) = \u03b1(\u03c1) \u2022 1 \u2212 \u03c1 \u03c1 \u03c1 1 \u2212 \u03c1 \u22121 \u2022 1 (v) \u22121 (v) + \u03b2(\u03c1).\nExpanding out the matrix inverse,\n(\u2200\u03c1 \u2208 [0, 1/2)) (\u2200v \u2208 R) 1 (v) \u22121 (v) = \u03b1(\u03c1) 1 \u2212 2\u03c1 \u2022 1 \u2212 \u03c1 \u2212\u03c1 \u2212\u03c1 1 \u2212 \u03c1 \u2022 1 (v) \u22121 (v) + \u03b2(\u03c1).\nAdding together the two sets of equations,\n(\u2200\u03c1 \u2208 [0, 1/2)) (\u2200v \u2208 R) 1 (v) + \u22121 (v) = \u03b1(\u03c1) \u2022 ( 1 (v) + \u22121 (v)) + \u03b2(\u03c1), or (\u2200\u03c1 \u2208 [0, 1/2)) (\u2200v \u2208 R) (1 \u2212 \u03b1(\u03c1)) \u2022 ( 1 (v) + \u22121 (v)) = \u03b2(\u03c1).\nSince the RHS is independent of v, the LHS cannot depend on v, i.e. 1 (v) + \u22121 (v) must be a constant.\n\nAdditional Discussion for \"Learning with Symmetric Label Noise: The Importance of Being Unhinged\"' B Evidence that non-convex losses and linear scorers may not be SLNrobust\n\nWe now present evidence that for being the TangentBoost loss,\n(y, v) = (2 tan \u22121 (yv) \u2212 1) 2 ,\nor the t-logistic regression loss for t = 2,\n(y, v) = log(1 \u2212 yv + 1 + v 2 ),\n( , F lin ) is not SLN-robust.We do this by looking at the minimisers of these losses on the 2D example of Long and Servedio [2010].Of course, as these losses are non-convex, exact minimisation of the risk is challenging.However, as the search space is R 2 , we construct a grid of resolution 0.025 over [\u221210, 10] 2 .We then exhaustively compute the objective for all grid points, and seek the minimiser.\n\nWe apply this procedure to the Long and Servedio [2010] dataset with \u03b3 = 1 60 , and with a 30% noise rate.Figure 2 plots the results of the objective for the TangentBoost loss.We find that the minimiser is at w * = (0.2, 1.3).This results in a classifier with error rate of 1 2 on D. Similarly, from Figure 3, we find that the minimiser is w * = (1.025,5.1), which also results in a classifier with error rate of 1 2 .\n\nFigure 2: Risk values for various weight vectors w = (w 1 , w 2 ), TangentBoost, Long and Servedio [2010] dataset.\n\nThe shape of these plots suggests that the minimiser is indeed found in the interval [\u221210, 10] 2 .To further verify this, we performed L-BFGS minimisation of these losses using 100 different random initialisations, uniformly from [\u2212100, 100] 2 .We find that in each trial, the TangentBoost solution converges to w * = (0.2122, 1.3031), while the t-logistic solution converges to w * = (1.0372,5.0873), both of which result in accuracy of 1 2 on D.\n\n\nB.1 In defence of non-convex losses: beyond SLN-robustness\n\nThe above illustrates the possible non SLN-robustness of two non-convex losses.However, there may be other notions under which these losses are robust.For example, Ding and Vishwanathan [2010] defines robustness to be a stability of the asymptotic maximum likelihood solution when adding a new labelled instance (chosen arbitrarily from X \u00d7 {\u00b11}), based on a definition in O'Hagan [1979].Intuitively, this captures robustness to outliers in the instance space, so that e.g. an adversarial mislabelling of an instance far from the true decision boundary does not adversely affect the learned model.Such a notion of robustness is clearly of practical interest, and future study of such alternate notions would be of value.\n\n\nB.2 Conjecture: (most) strictly proper composite losses are not SLN-robust\n\nMore abstractly, we conjecture the above can be generalised to the following.Recall that a loss is strictly proper composite [Reid and Williamson, 2010] if its (unique) Bayes-optimal scorer is some strictly monotone transformation \u03c8 of the class-probability function: (\u2200D) S D, * = {\u03c8 \u2022 \u03b7}.\n\nConjecture 1. Pick any strictly proper composite (but not necessarily convex) whose link function has range R.Then, ( , F lin ) is not SLN-robust.\n\nWe believe the above is true for the following reason.Suppose D is some linearly separable distribution, with \u03b7 : x \u2192 w * , x > 0 for some w * .Then, minimising with F lin will be well-specified: the Bayesoptimal scorer is \u03c8( w * , x > 0 ).If the range of \u03c8 is R, then this is equivalent to \u221e \u2022 (2 w * , x > 0 \u2212 1), which is in F lin if we allow for the extended reals.The resulting classifier will thus have 100% accuracy.\n\nHowever, by injecting any non-zero label noise, minimising with F lin will no longer be well-specified, as \u03b7 takes on the values {1 \u2212 \u03c1, \u03c1}, which cannot be the sole set of output scores for any linear scorer if |X| > 3. We believe it unlikely that every such misspecified solution have 100% accuracy on D. We further believe it likely that one can exhibit a scenario, possibly the same as the Long and Servedio [2010] example, where the resulting solution has accuracy 50%.\n\nTwo further comments are in order.First, if a loss is strictly proper composite, then it cannot satisfy Equation 6, and hence it cannot be strongly SLN-robust. (However, this does leave open the possibility that with F lin , the loss is SLN-robust.)Second, observe that the restriction that \u03c8 have range R is necessary to rule out cases such as square loss, where the link function has range [\u22121, 1].\n\n\nC Preservation of mean maps\n\nPick any D, and \u03c1 \u2208 [0, 1/2).Then,\n(\u2200x \u2208 X) 2\u03b7(x) \u2212 1 = 2 \u2022 ((1 \u2212 2\u03c1) \u2022 \u03b7(x) + \u03c1) \u2212 1 = (1 \u2212 2\u03c1) \u2022 (2\u03b7(x) \u2212 1).\nThus, for any feature mapping \u03a6 : X \u2192 H, the kernel mean map of the clean distribution is\nE (X,Y)\u223cD [Y \u2022 \u03a6(X)] = E X\u223cM [(2\u03b7(X) \u2212 1) \u2022 \u03a6(X)] = 1 (1 \u2212 2\u03c1 \u2022 E X\u223cM [(2\u03b7(X) \u2212 1) \u2022 \u03a6(X)] = 1 (1 \u2212 2\u03c1) \u2022 E (X,Y)\u223cSLN(D,\u03c1) [Y \u2022 \u03a6(X)] ,\nwhich is a scaled version of the kernel mean map of the noisy distribution.That is, the kernel mean map is preserved under symmetric label noise.Instantiating the above with a specific instance x \u2208 X gives Equation 10.Proof of Proposition 10.The standard Rademacher-complexity generalisation bound [Bartlett and Mendelson, 2002, Theorem 7], [Boucheron et al., 2005, Theorem 4.1] states that with probability at least 1 \u2212 \u03b4 over the choice of S,\n\n\nD Additional theoretical considerations\ndev D,S (s) \u2264 2 \u2022 ||( ) || \u221e \u2022 R n (F B , S) + || || \u221e \u2022 log 2 \u03b4 2n .\n\nE Additional relations to existing methods\n\nWe discuss some further connections of the unhinged loss to existing methods.\n\n\nE.1 Unhinging the SVM\n\nWe can motivate the unhinged loss intuitively by studying the noise-corrected versions of the hinge loss, as per Equation 4. Figure 4 shows the noise corrected hinge loss for \u03c1 \u2208 {0, 0.2, 0.4}.We see that as the noise rate increases, the effect is to slightly unhinge the original loss, by removing its flat portion8 .Thus, if we knew the noise rate \u03c1, we could use these slightly unhinged losses to learn.Of course, in general we do not know the noise rate.Further, the slightly unhinged losses are non-convex.So, in order to be robust to an arbitrary noise rate \u03c1, we can completely unhinge the loss, yielding\nunh 1 (v) = 1 \u2212 v and unh \u22121 (v) = 1 + v.\n\nE.2 Relation to centroid classifiers\n\nAs established in \u00a76.1, the optimal unhinged classifier (Equation 9) is equivalent to a centroid classifier, where one replaces the positive and negative classes by their centroids, and performs classification based on the distance of an instance to the two centroids.Such a classifier has been proposed as a prototypical example of a simple kernel-based classifier [Sch\u00f6lkopf and Smola, 2002, Section 1.2], [Shawe-Taylor andCristianini, 2004, Section 5.1] Balcan et al. [2008, Definition 4] considers such classification rules using general similarity functions in place of kernels corresponding to an RKHS.The optimal unhinged classifier is also closely related to the Rocchio classifier in information retrieval [Manning et al., 2008, pg. 181], and the nearest centroid classifier in computational genomics [Tibshirani et al., 2002].The optimal kernelised scorer for these approaches is [Doloc-Mihu et al., 2003] s\n* : x \u2192 E X\u223cP [k(X, x)] \u2212 E X\u223cQ [k(X, x)] ,\ni.e. it does not weight each of the kernel means.\n\n\nE.3 Relation to kernel density estimation\n\nWhen working with an RKHS with a translation invariant kernel9 , the optimal unhinged scorer (Equation 9) can be interpreted as follows: perform kernel density estimation on the positive and negative classes, and then classify instances according to Bayes' rule.For example, with a Gaussian RBF kernel, the classifier is equivalent to using a Gaussian kernel to compute density estimates of P, Q, and using these to classify.This is known as a kernel classification rule [Devroye et al., 1996, Chapter 10].\n\nThis perspective suggests that in computing s * unh,\u03bb , we may also estimate the corrupted class-probability function.In particular, observe that if we compute \u03c0\n1\u2212\u03c0 \u2022 E X\u223cP [k(X,x)] E X\u223cQ [k(X,x)]\n, similar to the Nadaraya-Watson estimator [Bishop, 2006, pg. 300], then this provides an estimate of \u03b7(x) 1\u2212\u03b7(x) .Of course, such an approach will succumb to the curse of dimensionality10 .\n\nAn alternative is to use the Probing reduction [Langford and Zadrozny, 2005], by computing an ensemble of cost-sensitive classifiers at varying cost ratios.To this end, observe that the following weighted unhinged (or whinge) loss, whinge 1\n(v) = c 1 \u2022 \u2212v whinge \u22121 (v) = c \u22121 \u2022 v for some c \u22121 \u2208 [0, 1] and c 1 = 1 \u2212 c \u22121 , will have a restricted Bayes-optimal scorer of B \u2022 sign(\u03b7(x) \u2212 c \u22121 ) over F B .\nFurther, it will result in an optimal scorer that simply weights each of the kernel means,\ns * whinge,\u03bb : x \u2192 1 \u03bb \u2022 E (X,Y)\u223cD [c Y \u2022 Y \u2022 k(X, x)] ,\nmaking it trivial to compute as c is varied.\n\n\nE.4 Relation to the MMD witness\n\nThe optimal weight vector for unhinged loss (Equation 8) can be expressed as\nw * unh,\u03bb = 1 \u03bb \u2022 (\u03c0 \u2022 \u00b5 P \u2212 (1 \u2212 \u03c0) \u2022 \u00b5 Q ),\nwhere \u00b5 P and \u00b5 Q are the kernel mean maps with respect to H of the positive and negative class-conditionals distributions,\n\u00b5 P = E X\u223cP [\u03a6(X)] \u00b5 Q = E X\u223cQ [\u03a6(X)] .\nWhen \u03c0 = 1 2 , ||w * 1 || H is precisely the maximum mean discrepancy (MMD) [Gretton et al., 2012] between P and Q, using all functions in the unit ball of H.The mapping x \u2192 w * 1 , x H itself is referred to as the witness function [Gretton et al., 2012, \u00a72.3].While the motivation of MMD is to perform hypothesis testing so as to distinguish between two distributions P, Q, rather than constructing a suitable scorer, the fact that it arises from the optimal scorer for the unhinged loss has been previously noted [Sriperumbudur et al., 2009, Theorem 1].\n\n\nF Example of poor classification with square loss\n\nWe illustrate that square loss with a linear function class may perform poorly even when the underlying distribution is linearly separable.We consider the dataset of Long and Servedio [2010], with no label noise.That is, we have X = {(1, 0), (\u03b3, 5\u03b3), (\u03b3, \u2212\u03b3), (\u03b3, \u2212\u03b3)} \u2282 R 2 , and \u03b7 : x \u2192 1.Let X \u2208 R 4\u00d72 be the feature matrix of the four data points.Then, the optimal weight vector learned by square loss is\nw * = (X T X) \u22121 X T \uf8ee \uf8ef \uf8ef \uf8f0 1 1 1 1 \uf8f9 \uf8fa \uf8fa \uf8fb = 8\u03b3+3 8\u03b3 2 +3 \u2212 \u03b3+1 3\u03b3\u2022(8\u03b3 2 +3) .\nIt is easy to check that the predicted scores are then\ns * = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 8\u03b3+3 8\u03b3 2 +3 \u03b3\u2022(8\u03b3+3) 8\u03b3 2 +3\u2212 5\u2022(\u03b3\u22121)\u03b3 24\u03b3 3 +9\u03b3 (\u03b3\u22121)\u2022\u03b3 24\u03b3 3 +9\u03b3+ \u03b3\u2022(8\u03b3+3) 8\u03b3 2 +3 (\u03b3\u22121)\u2022\u03b3 24\u03b3 3 +9\u03b3+ \u03b3\u2022(8\u03b3+3) 8\u03b3 2 +3 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb\n.\n\nBut for \u03b3 < 1 12 , this means that the predicted scores for the last two examples are negative.That is, the resulting classifier will have 50% accuracy.(This does not contradict the robustness of square loss, as robustness simply requires that performance is the same with and without noise.)\n\nIt is initially surprising that square loss fails in this example, as we are employing a linear function class, and the true \u03b7 is expressible as a linear function.However, recall that the Bayes-optimal scorer for square loss is S D, * = {s : x \u2192 2\u03b7(x) \u2212 1}.\n\nIn this case, the Bayes-optimal scorer is\ns * : x \u2192 2 x 1 > 0 \u2212 1.\nThe application of a threshold means the that scorer is not expressible as a linear model.Therefore, the combination of loss and function class is in fact not well-specified for the problem.\n\nTo clarify this point, consider the use of the squared hinge loss, (y, v) = max(0, 1 \u2212 yv) 2 .This loss induces a set of Bayes-optimal scorers, which are:\nS D, * = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 s | (\u2200x \u2208 X) \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b7(x) = 1 =\u21d2 s(x) \u2208 [1, \u221e) \u03b7(x) \u2208 (0, 1) =\u21d2 s(x) = 2\u03b7(x) \u2212 1 \u03b7(x) = 0 =\u21d2 s(x) \u2208 (\u2212\u221e, 1]. \uf8fc \uf8f4 \uf8fd \uf8f4 \uf8fe\nCrucially, we can find a linear scorer that is in this set: for, say, v = ( 1 \u03b3 , 0), we clearly have v, x \u2265 1 for every x \u2208 X, and so this is a Bayes-optimal scorer.Thus, minimising the square hinge loss on this distribution will indeed find a classifier with 100% accuracy.\n\n\nG Example of poor classification with unhinged loss\n\nWe illustrate that the unhinged loss with a linear function class may perform poorly even when the underlying distribution is linearly separable.(For another example where instances are on the unit ball, see Balcan et al. [2008, Figure 1].)Consider a distribution D M,\u03b7 uniformly concentrated on X = {x 1 , x 2 , x 3 } with x 1 = (1, 2), x 2 = (1, \u22124), x 3 = (\u22121, 1), with \u03b7(x 1 ) = \u03b7(x 2 ) = 1 and \u03b7(x 3 ) = 0, i.e. the first two instances are positive, and the third instance negative.Then it is evident that the optimal unhinged hyperplane, with regularisation strength 1, is w * = (1, \u22121).This will misclassify the first instance as being negative.Figure 5 illustrates.\n\nIt is easy to check that for this particular distribution, the optimal weight for square loss is w * = (1, 0).This results in perfect classification.Thus, we have a reversal of the scenario of the previous section -here, square loss classifies perfectly, while the unhinged loss classifies no better than random guessing.\n\nIt may appear that the above contradicts the classification-calibration of the unhinged loss: there certainly is a linear scorer that is Bayes-optimal over F B , namely, w * = (B, 0).The subtlety is that in this case, minimisation over the unit ball ||w|| 2 \u2264 1 (as implied by 2 regularisation) is unable to restrict attention to the desired scorer.\n\nThere are two ways to rectify examples such as the above.First, as in general, we can employ a suitably rich kernel, e.g. a Gaussian RBF kernel.It is not hard to verify that on this dataset, such a kernel will find a perfect classifier.Second, we can look to explicitly enforce that minimisation is over all w satisfying | w, x n | \u2264 1.This will result in a linear program (LP) that may be solved easily, but does not admit a closed form solution as in the case of minimising over the unit ball.It may be checked that the resulting LP will recover the optimal weight w * = (1, 0).While this approach is suitable for this particular example, issues arise when dealing with infinite dimensional feature mappings (as we lose the existence of a representer theorem without regularisation based on the norm in the Hilbert space [Yu et al., 2013]).\n\n\nAdditional Experiments for \"Learning with Symmetric\n\nLabel Noise: The Importance of Being Unhinged\"\n\n\nH Additional experimental results\n\nTable 4 reports the 0-1 error for a range of losses on the Long and Servedio [2010] dataset.TanBoost refers to the loss of Masnadi-Shirazi et al. [2010].As before, we find the unhinged loss to generally find a good classifier.Observe that the relatively poor performance of the square and TanBoost loss can be attributed to the findings of Appendix B, F. We next report the 0-1 error and one minus the AUC for a range of datasets.We begin with a dataset of Mease and Wyner [2008], where X = [0, 1] 20 , and M is the uniform distribution.Further, we have \u03b7 : x \u2192 w * , x > 2.5 for w * = 1 5 0 15 , i.e. there is a sparse separating hyperplane.Table 5 reports the results on this dataset injected with various levels of symmetric noise.On this dataset, the t-logistic loss generally performs the best.\n\nFinally, we report the 0-1 error and one minus the AUC on some UCI datasets in Tables 6 -7.Table 3 summarises statistics of the UCI data.Several datasets are imbalanced, meaning that 0-1 error is not the ideal measure of performance (as it can be made small with a trivial majority classifier).The AUC is thus arguably a better indication of performance for these datasets.We generally find that at high noise rates (40%), the AUC of the unhinged loss is superior to that of other losses.\n\n\nDataset\n\nFigure 1 :\n1\nFigure 1: Long and Servedio [2010] dataset.\n\n\n\n\nNow define the utility functions U D : a \u2192 \u2212L Da (a) and V D : a \u2192 \u2212L Da (a).\n\n\nFigure 3 :\n3\nFigure 3: Risk values for various weight vectors w = (w 1 , w 2 ), t-logistic regression, Long and Servedio [2010] dataset.\n\n\nD. 1\n1\nGeneralisation boundsGeneralisation bounds are readily derived for the unhinged loss.For a training sample S \u223c D n , define the -deviation of a scorer s : X \u2192 R to be the difference in its population and empirical -risk,dev D,S (s) = L D (s) \u2212 L S (s).This quantity is of interest because a standard result says that for the empirical risk minimiser s n over some function classF, regret D,F (s n ) \u2264 2\u2022sup s\u2208F |dev D,S(s)| [Boucheron et al., 2005, Equation 2].For unhinged loss, we have the following Rademacher based bound.Proposition 10.Pick any D and n \u2208 N + .Let S \u223c D n denote an empirical sample.For some B \u2208 R + , let s \u2208 F B .Then, with probability at least 1 \u2212 \u03b4 over the choice of S, for = unh , dev D,S (s) \u2264 2 \u2022 R n (F B , S) + B \u2022 log 2 \u03b4 2n where R n (F B , S) is the empirical Rademacher complexity of F B on sample S.\n\n\nFor\n\nthe unhinged loss, ||( unh ) || \u221e = 1.Further, since we work over bounded scorers, || unh || \u221e = B.The result follows.Proposition 10 holds equally when learning from a corrupted sample S \u223c D n .Since regret D,F unh (s n ) = 1 1\u22122\u03c1 \u2022 regret D,F unh (s n ) by Proposition 6, by minimising the unhinged loss on the corrupted sample, we can bound the regret on the clean distribution.\n\n\nFigure 4 :\n4\nFigure 4: Noise-corrected versions of hinge loss, 1 (v) = max(0, 1 \u2212 v).Best viewed in colour.\n\n\nFigure 5 :\n5\nFigure 5: Example of linearly separable distribution where, when learning with the unhinged loss and a linear function class, the resulting hyperplane (in red) misclassifies one of the instances.\n\n\n\n\n\n\n\nTable 2 :\n2\nMean and standard deviation of the 0-1 error over 125 trials on UCI datasets.\n\n\nTable 3 :\n3\nSummary of UCI datasets.Here, N denotes the total number of samples, and D the dimensionality of the feature space.\nNDP(Y = 1)Iris15040.3333Ionosphere 351340.3590Housing506130.0692Car1,728 80.0376USPS 0v72,200 256 0.5000Splice3,190 610.2404Spambase4,601 570.3940\nEven if we weaken the notion of SLN-robustness to allow for a difference of \u2208 [0, 1/2] between the clean and corrupted minimisers' performance,Long and Servedio [2010, \nTheorem 2] implies that in the worst case = 1/2.\nThis loss has been considered inSriperumbudur et al. [2009],Reid and Williamson [2011] in the context of maximum mean discrepancy; see Appendix E.4. The analysis of its SLN-robustness is to our knowledge novel.\nGiven a training sample S \u223c D n , we can use plugin estimates as appropriate.\n By contrast, Long and Servedio [2010, Section 6] establish that 1 regularisation does not endow SLN-robustness.\nSquare loss escapes the result ofLong and Servedio [2010] since it is not monotone decreasing.\nThe result actually requires that one not include a bias term; with a bias term, it can be checked that the example as-stated has a trivial solution.\nThis also holds if we add a regularised bias term. With an unregularised bias term,Bedo et al. [2006] showed that the limiting solution of a soft-margin SVM is distribution dependent.\nAnother interesting observation is that these noise-corrected losses are negatively unbounded -that is, minimising hinge loss on D is equivalent to minimising a negatively unbounded loss on D. This is another justification for studying negatively unbounded losses.\nFor a general (not necessarily translation invariant) kernel, this is known as a potential function rule[Devroye et al., 1996,  \u00a710.3]. The use of \"potential\" here is distinct from that of a \"convex potential\".\nThis refers to the rate of convergence of the estimate of \u03b7 to the true \u03b7. By contrast, generalisation bounds establish that the rate of convergence of the estimate of the corresponding classifier to the Bayes-optimal classifier sign(2\u03b7(x) \u2212 1) is independent of the dimension of the feature space.\nAcknowledgmentsNICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program.The authors thank Cheng Soon Ong for valuable comments on a draft of this paper.HingeLogisticSquare t-logistic TanBoost Unhinged \u03c1 = 0 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.25 \u00b1 0.00 0.00 \u00b1 0.00 0.25 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.1 0.15 \u00b1 0.27 0.24 \u00b1 0.05 0.25 \u00b1 0.00 0.00 \u00b1 0.00 0.25 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.2 0.21 \u00b1 0.30 0.25 \u00b1 0.00 0.25 \u00b1 0.00 0.00 \u00b1 0.00 0.25 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.3 0.38 \u00b1 0.37 0.25 \u00b1 0.03 0.25 \u00b1 0.02 0.22 \u00b1 0.08 0.25 \u00b1 0.03 0.00 \u00b1 0.00 \u03c1 = 0.4 0.42 \u00b1 0.36 0.22 \u00b1 0.08 0.22 \u00b1 0.08 0.22 \u00b1 0.08 0.22 \u00b1 0.08 0.00 \u00b1 0.00 \u03c1 = 0.49 0.46 \u00b1 0.38 0.39 \u00b1 0.23 0.39 \u00b1 0.23 0.39 \u00b1 0.23 0.39 \u00b1 0.23 0.34 \u00b1 0.48 Table4: Results onLong and Servedio [2010]dataset.Reported is the mean and standard deviation of the 0-1 error over 125 trials.Grayed cells denote the best performer at that noise rate.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.02 \u00b1 0.00 0.01 \u00b1 0.00 0.03 \u00b1 0.00 0.01 \u00b1 0.00 0.02 \u00b1 0.00 0.05 \u00b1 0.00 \u03c1 = 0.1 0.13 \u00b1 0.01 0.05 \u00b1 0.01 0.06 \u00b1 0.01 0.03 \u00b1 0.01 0.05 \u00b1 0.01 0.06 \u00b1 0.01 \u03c1 = 0.2 0.14 \u00b1 0.01 0.09 \u00b1 0.02 0.09 \u00b1 0.02 0.06 \u00b1 0.02 0.08 \u00b1 0.02 0.08 \u00b1 0.02 \u03c1 = 0.3 0.15 \u00b1 0.01 0.13 \u00b1 0.03 0.13 \u00b1 0.03 0.12 \u00b1 0.03 0.12 \u00b1 0.03 0.12 \u00b1 0.02 \u03c1 = 0.4 0.17 \u00b1 0.05 0.24 \u00b1 0.08 0.24 \u00b1 0.08 0.23 \u00b1 0.07 0.23 \u00b1 0.08 0.23 \u00b1 0.08 \u03c1 = 0.49 0.47 \u00b1 0.24 0.46 \u00b1 0.11 0.47 \u00b1 0.11 0.48 \u00b1 0.10 0.47 \u00b1 0.12 0.48 \u00b1 0.12 (a) 0-1 Error.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.01 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.01 \u00b1 0.00 \u03c1 = 0.1 0.25 \u00b1 0.10 0.02 \u00b1 0.01 0.02 \u00b1 0.01 0.00 \u00b1 0.00 0.02 \u00b1 0.01 0.02 \u00b1 0.01 \u03c1 = 0.2 0.34 \u00b1 0.10 0.05 \u00b1 0.02 0.05 \u00b1 0.02 0.02 \u00b1 0.01 0.04 \u00b1 0.02 0.05 \u00b1 0.02 \u03c1 = 0.3 0.41 \u00b1 0.11 0.11 \u00b1 0.04 0.11 \u00b1 0.04 0.09 \u00b1 0.04 0.11 \u00b1 0.04 0.10 \u00b1 0.04 \u03c1 = 0.4 0.44 \u00b1 0.12 0.24 \u00b1 0.08 0.24 \u00b1 0.08 0.24 \u00b1 0.08 0.24 \u00b1 0.08 0.23 \u00b1 0.08 \u03c1 = 0.49 0.50 \u00b1 0.13 0.47 \u00b1 0.11 0.47 \u00b1 0.11 0.47 \u00b1 0.11 0.47 \u00b1 0.11 0.46 \u00b1 0.11 (b) 1 -AUC.\u03c1 = 0 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.1 0.01 \u00b1 0.03 0.01 \u00b1 0.01 0.01 \u00b1 0.02 0.01 \u00b1 0.03 0.01 \u00b1 0.02 0.00 \u00b1 0.00 \u03c1 = 0.2 0.06 \u00b1 0.12 0.02 \u00b1 0.05 0.03 \u00b1 0.04 0.04 \u00b1 0.05 0.03 \u00b1 0.05 0.00 \u00b1 0.01 \u03c1 = 0.3 0.17 \u00b1 0.20 0.09 \u00b1 0.10 0.08 \u00b1 0.09 0.09 \u00b1 0.11 0.09 \u00b1 0.10 0.02 \u00b1 0.07 \u03c1 = 0.4 0.35 \u00b1 0.24 0.24 \u00b1 0.17 0.24 \u00b1 0.17 0.24 \u00b1 0.16 0.24 \u00b1 0.17 0.13 \u00b1 0.22 \u03c1 = 0.49 0.60 \u00b1 0.20 0.49 \u00b1 0.20 0.49 \u00b1 0.19 0.49 \u00b1 0.20 0.49 \u00b1 0.19 0.45 \u00b1 0.33 (a) 0-1 Error.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.1 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.2 0.03 \u00b1 0.11 0.00 \u00b1 0.01 0.00 \u00b1 0.00 0.00 \u00b1 0.01 0.00 \u00b1 0.01 0.00 \u00b1 0.00 \u03c1 = 0.3 0.14 \u00b1 0.26 0.02 \u00b1 0.06 0.02 \u00b1 0.05 0.02 \u00b1 0.06 0.02 \u00b1 0.05 0.01 \u00b1 0.06 \u03c1 = 0.4 0.36 \u00b1 0.38 0.13 \u00b1 0.18 0.13 \u00b1 0.18 0.14 \u00b1 0.18 0.13 \u00b1 0.18 0.09 \u00b1 0.27 \u03c1 = 0.49 0.72 \u00b1 0.34 0.47 \u00b1 0.31 0.48 \u00b1 0.30 0.48 \u00b1 0.30 0.48 \u00b1 0.30 0.45 \u00b1 0.48Table6: Results on iris dataset.Reported is the mean and standard deviation of performance over 125 trials.Grayed cells denote the best performer at that noise rate.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.11 \u00b1 0.00 0.13 \u00b1 0.00 0.17 \u00b1 0.00 0.24 \u00b1 0.00 0.17 \u00b1 0.00 0.20 \u00b1 0.00 \u03c1 = 0.1 0.17 \u00b1 0.04 0.18 \u00b1 0.04 0.16 \u00b1 0.03 0.19 \u00b1 0.05 0.17 \u00b1 0.04 0.19 \u00b1 0.02 \u03c1 = 0.2 0.20 \u00b1 0.05 0.19 \u00b1 0.05 0.18 \u00b1 0.04 0.21 \u00b1 0.06 0.18 \u00b1 0.04 0.19 \u00b1 0.02 \u03c1 = 0.3 0.23 \u00b1 0.06 0.22 \u00b1 0.05 0.22 \u00b1 0.05 0.24 \u00b1 0.06 0.22 \u00b1 0.05 0.21 \u00b1 0.03 \u03c1 = 0.4 0.31 \u00b1 0.11 0.31 \u00b1 0.10 0.29 \u00b1 0.09 0.32 \u00b1 0.09 0.30 \u00b1 0.10 0.27 \u00b1 0.12 \u03c1 = 0.49 0.48 \u00b1 0.16 0.47 \u00b1 0.16 0.47 \u00b1 0.16 0.47 \u00b1 0.14 0.45 \u00b1 0.15 0.46 \u00b1 0.22 (a) 0-1 Error.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.12 \u00b1 0.00 0.13 \u00b1 0.00 0.07 \u00b1 0.00 0.20 \u00b1 0.00 0.07 \u00b1 0.00 0.21 \u00b1 0.00 \u03c1 = 0.1 0.18 \u00b1 0.07 0.18 \u00b1 0.07 0.12 \u00b1 0.04 0.22 \u00b1 0.07 0.13 \u00b1 0.05 0.21 \u00b1 0.00 \u03c1 = 0.2 0.23 \u00b1 0.09 0.22 \u00b1 0.09 0.18 \u00b1 0.07 0.25 \u00b1 0.08 0.19 \u00b1 0.08 0.21 \u00b1 0.01 \u03c1 = 0.3 0.31 \u00b1 0.11 0.29 \u00b1 0.09 0.26 \u00b1 0.09 0.30 \u00b1 0.09 0.27 \u00b1 0.09 0.21 \u00b1 0.01 \u03c1 = 0.4 0.40 \u00b1 0.11 0.40 \u00b1 0.10 0.38 \u00b1 0.10 0.40 \u00b1 0.10 0.38 \u00b1 0.10 0.25 \u00b1 0.12 \u03c1 = 0.49 0.49 \u00b1 0.12 0.50 \u00b1 0.10 0.50 \u00b1 0.10 0.50 \u00b1 0.10 0.50 \u00b1 0.10 0.46 \u00b1 0.25 (b) 1 -AUC.TanBoost Unhinged \u03c1 = 0 0.05 \u00b1 0.00 0.05 \u00b1 0.00 0.07 \u00b1 0.00 0.05 \u00b1 0.00 0.07 \u00b1 0.00 0.05 \u00b1 0.00 \u03c1 = 0.1 0.06 \u00b1 0.01 0.06 \u00b1 0.02 0.07 \u00b1 0.02 0.07 \u00b1 0.02 0.07 \u00b1 0.02 0.05 \u00b1 0.00 \u03c1 = 0.2 0.06 \u00b1 0.01 0.07 \u00b1 0.03 0.07 \u00b1 0.02 0.08 \u00b1 0.03 0.07 \u00b1 0.02 0.05 \u00b1 0.00 \u03c1 = 0.3 0.08 \u00b1 0.04 0.10 \u00b1 0.06 0.11 \u00b1 0.06 0.11 \u00b1 0.05 0.11 \u00b1 0.06 0.05 \u00b1 0.01 \u03c1 = 0.4 0.14 \u00b1 0.10 0.21 \u00b1 0.12 0.22 \u00b1 0.12 0.24 \u00b1 0.13 0.22 \u00b1 0.13 0.09 \u00b1 0.10 \u03c1 = 0.49 0.45 \u00b1 0.26 0.49 \u00b1 0.16 0.50 \u00b1 0.16 0.49 \u00b1 0.16 0.51 \u00b1 0.17 0.46 \u00b1 0.30 (a) 0-1 Error.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.25 \u00b1 0.00 0.15 \u00b1 0.00 0.17 \u00b1 0.00 0.25 \u00b1 0.00 0.17 \u00b1 0.00 0.69 \u00b1 0.00 \u03c1 = 0.1 0.38 \u00b1 0.12 0.27 \u00b1 0.07 0.27 \u00b1 0.07 0.30 \u00b1 0.09 0.27 \u00b1 0.07 0.69 \u00b1 0.00 \u03c1 = 0.2 0.41 \u00b1 0.13 0.35 \u00b1 0.10 0.35 \u00b1 0.10 0.35 \u00b1 0.10 0.35 \u00b1 0.10 0.68 \u00b1 0.00 \u03c1 = 0.3 0.44 \u00b1 0.12 0.40 \u00b1 0.11 0.40 \u00b1 0.11 0.40 \u00b1 0.11 0.40 \u00b1 0.11 0.69 \u00b1 0.01 \u03c1 = 0.4 0.43 \u00b1 0.12 0.45 \u00b1 0.12 0.45 \u00b1 0.12 0.45 \u00b1 0.12 0.45 \u00b1 0.12 0.68 \u00b1 0.02 \u03c1 = 0.49 0.45 \u00b1 0.13 0.49 \u00b1 0.13 0.49 \u00b1 0.13 0.49 \u00b1 0.13 0.49 \u00b1 0.13 0.57 \u00b1 0.16Table8: Results on housing dataset.Reported is the mean and standard deviation of performance over 125 trials.Grayed cells denote the best performer at that noise rate.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.01 \u00b1 0.00 0.02 \u00b1 0.00 0.03 \u00b1 0.00 0.03 \u00b1 0.00 0.02 \u00b1 0.00 0.03 \u00b1 0.00 \u03c1 = 0.1 0.05 \u00b1 0.00 0.04 \u00b1 0.01 0.04 \u00b1 0.01 0.02 \u00b1 0.01 0.04 \u00b1 0.01 0.04 \u00b1 0.01 \u03c1 = 0.2 0.05 \u00b1 0.00 0.05 \u00b1 0.01 0.05 \u00b1 0.01 0.04 \u00b1 0.01 0.05 \u00b1 0.01 0.05 \u00b1 0.01 \u03c1 = 0.3 0.05 \u00b1 0.01 0.06 \u00b1 0.01 0.06 \u00b1 0.01 0.06 \u00b1 0.02 0.06 \u00b1 0.01 0.06 \u00b1 0.01 \u03c1 = 0.4 0.06 \u00b1 0.02 0.11 \u00b1 0.06 0.11 \u00b1 0.06 0.11 \u00b1 0.06 0.11 \u00b1 0.06 0.10 \u00b1 0.05 \u03c1 = 0.49 0.33 \u00b1 0.27 0.46 \u00b1 0.16 0.46 \u00b1 0.16 0.47 \u00b1 0.16 0.47 \u00b1 0.16 0.46 \u00b1 0.16 (a) 0-1 Error.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.01 \u00b1 0.00 0.00 \u00b1 0.00 0.01 \u00b1 0.00 0.02 \u00b1 0.00 \u03c1 = 0.1 0.34 \u00b1 0.18 0.03 \u00b1 0.02 0.03 \u00b1 0.02 0.00 \u00b1 0.00 0.03 \u00b1 0.02 0.04 \u00b1 0.02 \u03c1 = 0.2 0.40 \u00b1 0.17 0.07 \u00b1 0.05 0.08 \u00b1 0.05 0.04 \u00b1 0.04 0.07 \u00b1 0.05 0.08 \u00b1 0.05 \u03c1 = 0.3 0.43 \u00b1 0.17 0.17 \u00b1 0.10 0.17 \u00b1 0.10 0.14 \u00b1 0.10 0.16 \u00b1 0.10 0.16 \u00b1 0.10 \u03c1 = 0.4 0.44 \u00b1 0.18 0.30 \u00b1 0.16 0.30 \u00b1 0.16 0.30 \u00b1 0.16 0.30 \u00b1 0.16 0.30 \u00b1 0.16 \u03c1 = 0.49 0.51 \u00b1 0.19 0.46 \u00b1 0.17 0.46 \u00b1 0.17 0.46 \u00b1 0.17 0.46 \u00b1 0.17 0.46 \u00b1 0.18 (b) 1 -AUC.TanBoost Unhinged \u03c1 = 0 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.1 0.10 \u00b1 0.08 0.05 \u00b1 0.01 0.01 \u00b1 0.01 0.11 \u00b1 0.02 0.02 \u00b1 0.01 0.00 \u00b1 0.00 \u03c1 = 0.2 0.19 \u00b1 0.11 0.09 \u00b1 0.02 0.05 \u00b1 0.02 0.15 \u00b1 0.02 0.06 \u00b1 0.02 0.00 \u00b1 0.00 \u03c1 = 0.3 0.31 \u00b1 0.13 0.17 \u00b1 0.03 0.14 \u00b1 0.02 0.22 \u00b1 0.03 0.16 \u00b1 0.03 0.01 \u00b1 0.00 \u03c1 = 0.4 0.39 \u00b1 0.13 0.31 \u00b1 0.04 0.30 \u00b1 0.04 0.33 \u00b1 0.04 0.31 \u00b1 0.04 0.02 \u00b1 0.02 \u03c1 = 0.49 0.50 \u00b1 0.16 0.48 \u00b1 0.04 0.47 \u00b1 0.04 0.48 \u00b1 0.04 0.48 \u00b1 0.04 0.34 \u00b1 0.21 (a) 0-1 Error.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.1 0.05 \u00b1 0.06 0.01 \u00b1 0.00 0.00 \u00b1 0.00 0.05 \u00b1 0.01 0.00 \u00b1 0.00 0.00 \u00b1 0.00 \u03c1 = 0.2 0.12 \u00b1 0.11 0.03 \u00b1 0.01 0.01 \u00b1 0.00 0.07 \u00b1 0.01 0.02 \u00b1 0.01 0.00 \u00b1 0.00 \u03c1 = 0.3 0.26 \u00b1 0.18 0.10 \u00b1 0.02 0.07 \u00b1 0.02 0.14 \u00b1 0.03 0.08 \u00b1 0.02 0.00 \u00b1 0.00 \u03c1 = 0.4 0.37 \u00b1 0.19 0.25 \u00b1 0.04 0.24 \u00b1 0.04 0.27 \u00b1 0.04 0.24 \u00b1 0.04 0.00 \u00b1 0.00 \u03c1 = 0.49 0.51 \u00b1 0.23 0.47 \u00b1 0.05 0.46 \u00b1 0.05 0.47 \u00b1 0.05 0.47 \u00b1 0.05 0.25 \u00b1 0.29Table10: Results on usps 0 vs 7 dataset.Reported is the mean and standard deviation of performance over 125 trials.Grayed cells denote the best performer at that noise rate.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.05 \u00b1 0.00 0.04 \u00b1 0.00 0.02 \u00b1 0.00 0.04 \u00b1 0.00 0.02 \u00b1 0.00 0.19 \u00b1 0.00 \u03c1 = 0.1 0.15 \u00b1 0.03 0.05 \u00b1 0.01 0.04 \u00b1 0.01 0.24 \u00b1 0.00 0.04 \u00b1 0.01 0.19 \u00b1 0.01 \u03c1 = 0.2 0.21 \u00b1 0.03 0.08 \u00b1 0.01 0.07 \u00b1 0.01 0.24 \u00b1 0.00 0.07 \u00b1 0.01 0.19 \u00b1 0.01 \u03c1 = 0.3 0.25 \u00b1 0.03 0.14 \u00b1 0.02 0.14 \u00b1 0.02 0.24 \u00b1 0.00 0.14 \u00b1 0.02 0.19 \u00b1 0.03 \u03c1 = 0.4 0.31 \u00b1 0.05 0.28 \u00b1 0.05 0.28 \u00b1 0.04 0.24 \u00b1 0.00 0.28 \u00b1 0.04 0.22 \u00b1 0.05 \u03c1 = 0.49 0.48 \u00b1 0.09 0.47 \u00b1 0.06 0.48 \u00b1 0.05 0.40 \u00b1 0.24 0.48 \u00b1 0.05 0.45 \u00b1 0.08 (a) 0-1 Error.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.01 \u00b1 0.00 0.01 \u00b1 0.00 0.00 \u00b1 0.00 0.01 \u00b1 0.00 0.00 \u00b1 0.00 0.09 \u00b1 0.00 \u03c1 = 0.1 0.10 \u00b1 0.03 0.01 \u00b1 0.00 0.01 \u00b1 0.00 0.03 \u00b1 0.01 0.01 \u00b1 0.00 0.09 \u00b1 0.01 \u03c1 = 0.2 0.20 \u00b1 0.05 0.03 \u00b1 0.01 0.02 \u00b1 0.01 0.04 \u00b1 0.01 0.02 \u00b1 0.01 0.10 \u00b1 0.02 \u03c1 = 0.3 0.30 \u00b1 0.06 0.08 \u00b1 0.02 0.08 \u00b1 0.02 0.09 \u00b1 0.02 0.07 \u00b1 0.02 0.11 \u00b1 0.03 \u03c1 = 0.4 0.40 \u00b1 0.07 0.22 \u00b1 0.04 0.22 \u00b1 0.04 0.23 \u00b1 0.04 0.22 \u00b1 0.04 0.16 \u00b1 0.07 \u03c1 = 0.49 0.49 \u00b1 0.08 0.46 \u00b1 0.05 0.46 \u00b1 0.05 0.46 \u00b1 0.05 0.45 \u00b1 0.05 0.42 \u00b1 0.15 (b) 1 -AUC.TanBoost Unhinged \u03c1 = 0 0.16 \u00b1 0.01 0.08 \u00b1 0.00 0.10 \u00b1 0.00 0.24 \u00b1 0.00 0.09 \u00b1 0.00 0.15 \u00b1 0.00 \u03c1 = 0.1 0.14 \u00b1 0.03 0.10 \u00b1 0.02 0.10 \u00b1 0.01 0.13 \u00b1 0.06 0.10 \u00b1 0.01 0.14 \u00b1 0.01 \u03c1 = 0.2 0.17 \u00b1 0.03 0.11 \u00b1 0.02 0.11 \u00b1 0.01 0.13 \u00b1 0.05 0.11 \u00b1 0.01 0.14 \u00b1 0.01 \u03c1 = 0.3 0.23 \u00b1 0.05 0.13 \u00b1 0.02 0.12 \u00b1 0.01 0.14 \u00b1 0.04 0.13 \u00b1 0.02 0.15 \u00b1 0.01 \u03c1 = 0.4 0.33 \u00b1 0.07 0.20 \u00b1 0.04 0.19 \u00b1 0.03 0.21 \u00b1 0.04 0.19 \u00b1 0.03 0.17 \u00b1 0.03 \u03c1 = 0.49 0.49 \u00b1 0.10 0.45 \u00b1 0.07 0.44 \u00b1 0.07 0.45 \u00b1 0.07 0.45 \u00b1 0.07 0.43 \u00b1 0.12 (a) 0-1 Error.HingeLogistic Square t-logistic TanBoost Unhinged \u03c1 = 0 0.03 \u00b1 0.00 0.02 \u00b1 0.00 0.05 \u00b1 0.00 0.02 \u00b1 0.00 0.04 \u00b1 0.00 0.07 \u00b1 0.00 \u03c1 = 0.1 0.06 \u00b1 0.01 0.04 \u00b1 0.00 0.05 \u00b1 0.00 0.03 \u00b1 0.00 0.04 \u00b1 0.00 0.07 \u00b1 0.00 \u03c1 = 0.2 0.10 \u00b1 0.03 0.05 \u00b1 0.00 0.05 \u00b1 0.00 0.04 \u00b1 0.00 0.05 \u00b1 0.00 0.07 \u00b1 0.00 \u03c1 = 0.3 0.17 \u00b1 0.06 0.06 \u00b1 0.01 0.06 \u00b1 0.01 0.06 \u00b1 0.01 0.06 \u00b1 0.01 0.07 \u00b1 0.01 \u03c1 = 0.4 0.32 \u00b1 0.12 0.12 \u00b1 0.02 0.12 \u00b1 0.02 0.12 \u00b1 0.02 0.12 \u00b1 0.02 0.09 \u00b1 0.02 \u03c1 = 0.49 0.49 \u00b1 0.14 0.43 \u00b1 0.08 0.43 \u00b1 0.08 0.43 \u00b1 0.07 0.43 \u00b1 0.08 0.39 \u00b1 0.19 (b) 1 -AUC.Table12: Results on spambase dataset.Reported is the mean and standard deviation of performance over 125 trials.Grayed cells denote the best performer at that noise rate.\nA theory of learning with similarity functions. Dana Angluin, Philip Laird, Maria-Florina Balcan, Avrim Blum, Nathan Srebro, Machine Learning. 1988. August 20082Machine Learning\n\nRademacher and Gaussian complexities: Risk bounds and structural results. Peter L Bartlett, Shahar Mendelson, Journal of Machine Learning Research. 1532-443532002\n\nConvexity, classification, and risk bounds. L Peter, Michael I Bartlett, Jon D Jordan, Mcauliffe, Journal of the American Statistical Association. 1014732006\n\nAn efficient alternative to SVM based recursive feature elimination with applications in natural language processing and bioinformatics. Justin Bedo, Conrad Sanderson, Adam Kowalczyk, AI 2006: Advances in Artificial Intelligence. Lecture Notes in Computer Science. Abdul Sattar, Byeong-Ho Kang, Berlin HeidelbergSpringer20064304\n\nM Christopher, Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc2006\n\nCombining labeled and unlabeled data with co-training. Avrim Blum, Tom Mitchell, Conference on Computational Learning Theory (COLT). 1998\n\nTheory of classification: a survey of some recent advances. St\u00e9phane Boucheron, Olivier Bousquet, G\u00e1bor Lugosi, ESAIM: Probability and Statistics. 20059\n\nSuper-samples from kernel herding. Yutian Chen, Max Welling, Alex J Smola, Uncertainty in Artificial Intelligence (UAI). 2012\n\nOptimal Statistical Decisions. H Morris, Degroot, 1970John Wiley & Sons\n\nRobust classification with adiabatic quantum optimization. Nan Vasil Denchev, Hartmut Ding, S V N Neven, Vishwanathan, International Conference on Machine Learning (ICML). 2012\n\nA Probabilistic Theory of Pattern Recognition. Luc Devroye, L\u00e1szl\u00f3 Gy\u00f6rfi, G\u00e1bor Lugosi, 1996Springer\n\nt-logistic regression. Nan Ding, S V N Vishwanathan, Advances in Neural Information Processing Systems (NIPS). Curran Associates, Inc2010\n\nColor retrieval in vector space model. A Doloc-Mihu, V V Raghavan, P Bollmann-Sdorra, ACM SIGIR Workshop on Mathematical/Formal Methods in Information Retrieval. 2003\n\nMathematical Statistics: A Decision Theoretic Approach. Thomas S Ferguson, 1967Academic Press\n\nA kernel two-sample test. Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, Alexander Smola, Journal of Machine Learning Research. 1532-443513March 2012\n\nEfficient noise-tolerant learning from statistical queries. Michael Kearns, Journal of the ACM. 56November 1998\n\nEstimating class membership probabilities using classifier learners. John Langford, Bianca Zadrozny, 2005\n\nRandom classification noise defeats all convex potential boosters. Philip M Long, Rocco A Servedio, Machine Learning. 201078\n\nIntroduction to Information Retrieval. D Christopher, Prabhakar Manning, Hinrich Raghavan, Sch\u00fctze, 2008Cambridge University Press9780521865715New York, NY, USA\n\nNoise tolerance under risk minimization. Naresh Manwani, P S Sastry, IEEE Transactions on Cybernetics. 433June 2013\n\nOn the design of robust classifiers for computer vision. Hamed Masnadi-Shirazi, Vijay Mahadevan, Nuno Vasconcelos, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2010\n\nEvidence contrary to the statistical view of boosting. David Mease, Abraham Wyner, Journal of Machine Learning Research. 1532-44359June 2008\n\nLearning with noisy labels. Nagarajan Natarajan, Inderjit S Dhillon, Pradeep D Ravikumar, Ambuj Tewari, Advances in Neural Information Processing Systems (NIPS). 2013\n\nOn outlier rejection phenomena in bayes inference. O' Anthony, Hagan, Journal of the Royal Statistical Society. Series B (Methodological). 003592464131979\n\nRandom features for large-scale kernel machines. Ali Rahimi, Benjamin Recht, Advances in Neural Information Processing Systems (NIPS). 2007\n\nComposite binary losses. Mark D Reid, Robert C Williamson, Journal of Machine Learning Research. 11December 2010\n\nInformation, divergence and risk for binary experiments. D Mark, Robert C Reid, Williamson, Journal of Machine Learning Research. 12Mar 2011\n\nJ Mark, Schervish, Theory of Statistics. Springer-Verlag1995\n\nPegasos: Primal estimated sub-gradient solver for svm. Bernhard Sch\u00f6lkopf, Alexander J Smola ; Singer, Nathan Srebro, Proceedings of the 24th International Conference on Machine Learning, ICML '07. the 24th International Conference on Machine Learning, ICML '07New York, NY, USAACM2002. 2007129Learning with kernels\n\nKernel Methods for Pattern Analysis. John Shawe, - Taylor, Nello Cristianini, doi: 10.22772004Cambridge University Press47\n\nKernel choice and classifiability for RKHS embeddings of probability distributions. K Bharath, Kenji Sriperumbudur, Arthur Fukumizu, Gert R G Gretton, Bernhard Lanckriet, Sch\u00f6lkopf, Advances in Neural Information Processing Systems. 2009\n\nLearning SVMs from sloppily labeled data. Guillaume Stempfel, Liva Ralaivola, Artificial Neural Networks (ICANN). Berlin HeidelbergSpringer20095768\n\nDiagnosis of multiple cancer types by shrunken centroids of gene expression. Robert Tibshirani, Trevor Hastie, Balasubramanian Narasimhan, Gilbert Chu, Proceedings of the National Academy of Sciences. 99102002\n\nCharacterizing the representer theorem. Yaoliang Yu, Hao Cheng, Dale Schuurmans, Csaba Szepesv\u00e1ri, International Conference on Machine Learning (ICML). 201328JMLR.org\n", "annotations": {"author": "[{\"end\":195,\"start\":84},{\"end\":304,\"start\":196},{\"end\":414,\"start\":305}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":92},{\"end\":216,\"start\":211},{\"end\":324,\"start\":314}]", "author_first_name": "[{\"end\":91,\"start\":84},{\"end\":202,\"start\":196},{\"end\":210,\"start\":203},{\"end\":311,\"start\":305},{\"end\":313,\"start\":312}]", "author_affiliation": "[{\"end\":194,\"start\":135},{\"end\":303,\"start\":244},{\"end\":413,\"start\":354}]", "title": "[{\"end\":70,\"start\":1},{\"end\":484,\"start\":415}]", "venue": null, "abstract": "[{\"end\":2674,\"start\":554}]", "bib_ref": "[{\"end\":2780,\"start\":2746},{\"end\":2822,\"start\":2782},{\"end\":2873,\"start\":2824},{\"end\":3708,\"start\":3705},{\"end\":4832,\"start\":4793},{\"end\":4927,\"start\":4891},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5090,\"start\":5066},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5104,\"start\":5090},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5129,\"start\":5104},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5154,\"start\":5129},{\"end\":5960,\"start\":5927},{\"end\":7819,\"start\":7784},{\"end\":8182,\"start\":8146},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8788,\"start\":8759},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8818,\"start\":8788},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8847,\"start\":8818},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8869,\"start\":8847},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8896,\"start\":8869},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10164,\"start\":10139},{\"end\":11020,\"start\":10988},{\"end\":17086,\"start\":17062},{\"end\":17114,\"start\":17088},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19676,\"start\":19655},{\"end\":19711,\"start\":19676},{\"end\":19762,\"start\":19711},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21257,\"start\":21233},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21493,\"start\":21474},{\"end\":23808,\"start\":23771},{\"end\":24383,\"start\":24344},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25044,\"start\":25020},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25898,\"start\":25874},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26277,\"start\":26248},{\"end\":27905,\"start\":27870},{\"end\":28350,\"start\":28315},{\"end\":28732,\"start\":28699},{\"end\":33172,\"start\":33133},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37916,\"start\":37892},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":38246,\"start\":38222},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":38716,\"start\":38692},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":39429,\"start\":39401},{\"end\":39624,\"start\":39618},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":40188,\"start\":40161},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":41319,\"start\":41295},{\"end\":41551,\"start\":41525},{\"end\":42486,\"start\":42445},{\"end\":42522,\"start\":42488},{\"end\":43951,\"start\":43911},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":43970,\"start\":43953},{\"end\":44035,\"start\":43970},{\"end\":44291,\"start\":44260},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":44380,\"start\":44355},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":44460,\"start\":44435},{\"end\":45107,\"start\":45073},{\"end\":45374,\"start\":45351},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":45576,\"start\":45547},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":46519,\"start\":46497},{\"end\":46681,\"start\":46653},{\"end\":46975,\"start\":46936},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":47220,\"start\":47196},{\"end\":49413,\"start\":49384},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":51365,\"start\":51348},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":51590,\"start\":51566},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":51659,\"start\":51630},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":51986,\"start\":51964},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":55180,\"start\":55156},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":55290,\"start\":55263},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":55317,\"start\":55291},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":55690,\"start\":55666},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":55979,\"start\":55961}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52865,\"start\":52807},{\"attributes\":{\"id\":\"fig_1\"},\"end\":52947,\"start\":52866},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53086,\"start\":52948},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53930,\"start\":53087},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54318,\"start\":53931},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54428,\"start\":54319},{\"attributes\":{\"id\":\"fig_6\"},\"end\":54639,\"start\":54429},{\"end\":54644,\"start\":54640},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54736,\"start\":54645},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":55012,\"start\":54737}]", "paragraph": "[{\"end\":3096,\"start\":2676},{\"end\":3523,\"start\":3129},{\"end\":3770,\"start\":3559},{\"end\":3989,\"start\":3877},{\"end\":4140,\"start\":3991},{\"end\":4359,\"start\":4174},{\"end\":4928,\"start\":4406},{\"end\":5997,\"start\":4983},{\"end\":6002,\"start\":5999},{\"end\":6558,\"start\":6036},{\"end\":6872,\"start\":6603},{\"end\":6969,\"start\":6910},{\"end\":7046,\"start\":6971},{\"end\":7409,\"start\":7117},{\"end\":7584,\"start\":7479},{\"end\":8274,\"start\":7616},{\"end\":8448,\"start\":8276},{\"end\":8665,\"start\":8495},{\"end\":9154,\"start\":8667},{\"end\":9481,\"start\":9156},{\"end\":9564,\"start\":9483},{\"end\":9887,\"start\":9566},{\"end\":10549,\"start\":9889},{\"end\":10904,\"start\":10604},{\"end\":11158,\"start\":10964},{\"end\":11256,\"start\":11160},{\"end\":11748,\"start\":11330},{\"end\":12029,\"start\":11820},{\"end\":12189,\"start\":12084},{\"end\":12821,\"start\":12230},{\"end\":13042,\"start\":12865},{\"end\":13505,\"start\":13111},{\"end\":13635,\"start\":13507},{\"end\":13826,\"start\":13637},{\"end\":14330,\"start\":13828},{\"end\":14413,\"start\":14332},{\"end\":14475,\"start\":14415},{\"end\":14842,\"start\":14518},{\"end\":15149,\"start\":14887},{\"end\":15332,\"start\":15236},{\"end\":15759,\"start\":15334},{\"end\":16244,\"start\":15810},{\"end\":16668,\"start\":16347},{\"end\":17115,\"start\":16706},{\"end\":17450,\"start\":17117},{\"end\":17874,\"start\":17558},{\"end\":18143,\"start\":17945},{\"end\":18189,\"start\":18145},{\"end\":18497,\"start\":18301},{\"end\":18738,\"start\":18545},{\"end\":18956,\"start\":18794},{\"end\":19106,\"start\":18999},{\"end\":19226,\"start\":19177},{\"end\":19327,\"start\":19271},{\"end\":20090,\"start\":19439},{\"end\":20100,\"start\":20092},{\"end\":20166,\"start\":20162},{\"end\":20610,\"start\":20195},{\"end\":21494,\"start\":20612},{\"end\":21689,\"start\":21496},{\"end\":21864,\"start\":21761},{\"end\":21899,\"start\":21866},{\"end\":22412,\"start\":22118},{\"end\":22959,\"start\":22414},{\"end\":23127,\"start\":22961},{\"end\":23446,\"start\":23282},{\"end\":23809,\"start\":23551},{\"end\":24463,\"start\":23811},{\"end\":24939,\"start\":24523},{\"end\":25799,\"start\":24941},{\"end\":25957,\"start\":25801},{\"end\":26594,\"start\":25959},{\"end\":27088,\"start\":26596},{\"end\":27634,\"start\":27119},{\"end\":27718,\"start\":27636},{\"end\":27809,\"start\":27755},{\"end\":28367,\"start\":27811},{\"end\":28624,\"start\":28409},{\"end\":28733,\"start\":28626},{\"end\":29154,\"start\":28735},{\"end\":29194,\"start\":29156},{\"end\":29272,\"start\":29196},{\"end\":29498,\"start\":29403},{\"end\":29786,\"start\":29563},{\"end\":29877,\"start\":29788},{\"end\":29964,\"start\":29897},{\"end\":30153,\"start\":29992},{\"end\":30208,\"start\":30155},{\"end\":30589,\"start\":30272},{\"end\":31008,\"start\":30783},{\"end\":31144,\"start\":31062},{\"end\":31359,\"start\":31302},{\"end\":31452,\"start\":31421},{\"end\":31531,\"start\":31454},{\"end\":31834,\"start\":31576},{\"end\":31895,\"start\":31836},{\"end\":31954,\"start\":31948},{\"end\":32116,\"start\":32020},{\"end\":32231,\"start\":32158},{\"end\":32436,\"start\":32349},{\"end\":32486,\"start\":32443},{\"end\":32667,\"start\":32632},{\"end\":32861,\"start\":32717},{\"end\":32998,\"start\":32929},{\"end\":33173,\"start\":33063},{\"end\":33288,\"start\":33248},{\"end\":33567,\"start\":33440},{\"end\":33820,\"start\":33624},{\"end\":33874,\"start\":33822},{\"end\":33963,\"start\":33900},{\"end\":34139,\"start\":34076},{\"end\":34387,\"start\":34173},{\"end\":34593,\"start\":34389},{\"end\":34764,\"start\":34621},{\"end\":34876,\"start\":34803},{\"end\":35314,\"start\":35102},{\"end\":35535,\"start\":35352},{\"end\":35958,\"start\":35915},{\"end\":36264,\"start\":36177},{\"end\":36614,\"start\":36306},{\"end\":36683,\"start\":36647},{\"end\":36930,\"start\":36789},{\"end\":37053,\"start\":37020},{\"end\":37191,\"start\":37149},{\"end\":37436,\"start\":37334},{\"end\":37610,\"start\":37438},{\"end\":37673,\"start\":37612},{\"end\":37751,\"start\":37707},{\"end\":38189,\"start\":37785},{\"end\":38609,\"start\":38191},{\"end\":38725,\"start\":38611},{\"end\":39174,\"start\":38727},{\"end\":39957,\"start\":39237},{\"end\":40326,\"start\":40036},{\"end\":40474,\"start\":40328},{\"end\":40899,\"start\":40476},{\"end\":41375,\"start\":40901},{\"end\":41777,\"start\":41377},{\"end\":41843,\"start\":41809},{\"end\":42010,\"start\":41921},{\"end\":42591,\"start\":42147},{\"end\":42826,\"start\":42749},{\"end\":43463,\"start\":42852},{\"end\":44462,\"start\":43545},{\"end\":44556,\"start\":44507},{\"end\":45108,\"start\":44602},{\"end\":45271,\"start\":45110},{\"end\":45498,\"start\":45308},{\"end\":45740,\"start\":45500},{\"end\":45996,\"start\":45906},{\"end\":46098,\"start\":46054},{\"end\":46210,\"start\":46134},{\"end\":46380,\"start\":46257},{\"end\":46976,\"start\":46421},{\"end\":47438,\"start\":47030},{\"end\":47574,\"start\":47520},{\"end\":47736,\"start\":47735},{\"end\":48030,\"start\":47738},{\"end\":48289,\"start\":48032},{\"end\":48332,\"start\":48291},{\"end\":48548,\"start\":48358},{\"end\":48704,\"start\":48550},{\"end\":49120,\"start\":48845},{\"end\":49849,\"start\":49176},{\"end\":50172,\"start\":49851},{\"end\":50523,\"start\":50174},{\"end\":51367,\"start\":50525},{\"end\":51469,\"start\":51423},{\"end\":52306,\"start\":51507},{\"end\":52796,\"start\":52308},{\"end\":52864,\"start\":52821},{\"end\":52946,\"start\":52869},{\"end\":53085,\"start\":52962},{\"end\":53929,\"start\":53095},{\"end\":54317,\"start\":53937},{\"end\":54427,\"start\":54333},{\"end\":54638,\"start\":54443},{\"end\":54735,\"start\":54658},{\"end\":54865,\"start\":54750}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3837,\"start\":3771},{\"attributes\":{\"id\":\"formula_1\"},\"end\":3876,\"start\":3837},{\"attributes\":{\"id\":\"formula_2\"},\"end\":4173,\"start\":4141},{\"attributes\":{\"id\":\"formula_3\"},\"end\":4405,\"start\":4360},{\"attributes\":{\"id\":\"formula_4\"},\"end\":6909,\"start\":6873},{\"attributes\":{\"id\":\"formula_5\"},\"end\":7116,\"start\":7047},{\"attributes\":{\"id\":\"formula_6\"},\"end\":7615,\"start\":7585},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11329,\"start\":11257},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11818,\"start\":11749},{\"attributes\":{\"id\":\"formula_9\"},\"end\":11819,\"start\":11818},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12229,\"start\":12190},{\"attributes\":{\"id\":\"formula_11\"},\"end\":12864,\"start\":12822},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13110,\"start\":13043},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14517,\"start\":14476},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15235,\"start\":15150},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16346,\"start\":16245},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17557,\"start\":17451},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18300,\"start\":18190},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18998,\"start\":18957},{\"attributes\":{\"id\":\"formula_19\"},\"end\":19176,\"start\":19107},{\"attributes\":{\"id\":\"formula_20\"},\"end\":19270,\"start\":19227},{\"attributes\":{\"id\":\"formula_21\"},\"end\":19438,\"start\":19328},{\"attributes\":{\"id\":\"formula_22\"},\"end\":20161,\"start\":20101},{\"attributes\":{\"id\":\"formula_23\"},\"end\":22117,\"start\":21900},{\"attributes\":{\"id\":\"formula_24\"},\"end\":23218,\"start\":23128},{\"attributes\":{\"id\":\"formula_25\"},\"end\":23550,\"start\":23447},{\"attributes\":{\"id\":\"formula_26\"},\"end\":28408,\"start\":28368},{\"attributes\":{\"id\":\"formula_27\"},\"end\":29402,\"start\":29273},{\"attributes\":{\"id\":\"formula_28\"},\"end\":29562,\"start\":29499},{\"attributes\":{\"id\":\"formula_29\"},\"end\":29896,\"start\":29878},{\"attributes\":{\"id\":\"formula_30\"},\"end\":29991,\"start\":29965},{\"attributes\":{\"id\":\"formula_31\"},\"end\":30271,\"start\":30209},{\"attributes\":{\"id\":\"formula_32\"},\"end\":30690,\"start\":30590},{\"attributes\":{\"id\":\"formula_33\"},\"end\":30782,\"start\":30690},{\"attributes\":{\"id\":\"formula_34\"},\"end\":31061,\"start\":31009},{\"attributes\":{\"id\":\"formula_35\"},\"end\":31301,\"start\":31145},{\"attributes\":{\"id\":\"formula_36\"},\"end\":31420,\"start\":31360},{\"attributes\":{\"id\":\"formula_37\"},\"end\":31575,\"start\":31532},{\"attributes\":{\"id\":\"formula_38\"},\"end\":31947,\"start\":31896},{\"attributes\":{\"id\":\"formula_39\"},\"end\":32019,\"start\":31955},{\"attributes\":{\"id\":\"formula_40\"},\"end\":32157,\"start\":32117},{\"attributes\":{\"id\":\"formula_41\"},\"end\":32348,\"start\":32232},{\"attributes\":{\"id\":\"formula_42\"},\"end\":32631,\"start\":32487},{\"attributes\":{\"id\":\"formula_43\"},\"end\":32716,\"start\":32668},{\"attributes\":{\"id\":\"formula_44\"},\"end\":32928,\"start\":32862},{\"attributes\":{\"id\":\"formula_45\"},\"end\":33062,\"start\":32999},{\"attributes\":{\"id\":\"formula_46\"},\"end\":33247,\"start\":33174},{\"attributes\":{\"id\":\"formula_47\"},\"end\":33439,\"start\":33289},{\"attributes\":{\"id\":\"formula_48\"},\"end\":33623,\"start\":33568},{\"attributes\":{\"id\":\"formula_49\"},\"end\":33899,\"start\":33875},{\"attributes\":{\"id\":\"formula_50\"},\"end\":34075,\"start\":33964},{\"attributes\":{\"id\":\"formula_51\"},\"end\":34172,\"start\":34140},{\"attributes\":{\"id\":\"formula_52\"},\"end\":34620,\"start\":34594},{\"attributes\":{\"id\":\"formula_53\"},\"end\":34802,\"start\":34765},{\"attributes\":{\"id\":\"formula_54\"},\"end\":35101,\"start\":34877},{\"attributes\":{\"id\":\"formula_55\"},\"end\":35350,\"start\":35315},{\"attributes\":{\"id\":\"formula_56\"},\"end\":35351,\"start\":35350},{\"attributes\":{\"id\":\"formula_57\"},\"end\":35649,\"start\":35536},{\"attributes\":{\"id\":\"formula_58\"},\"end\":35914,\"start\":35649},{\"attributes\":{\"id\":\"formula_59\"},\"end\":36176,\"start\":35959},{\"attributes\":{\"id\":\"formula_60\"},\"end\":36305,\"start\":36265},{\"attributes\":{\"id\":\"formula_61\"},\"end\":36788,\"start\":36684},{\"attributes\":{\"id\":\"formula_62\"},\"end\":37019,\"start\":36931},{\"attributes\":{\"id\":\"formula_63\"},\"end\":37148,\"start\":37054},{\"attributes\":{\"id\":\"formula_64\"},\"end\":37333,\"start\":37192},{\"attributes\":{\"id\":\"formula_65\"},\"end\":37706,\"start\":37674},{\"attributes\":{\"id\":\"formula_66\"},\"end\":37784,\"start\":37752},{\"attributes\":{\"id\":\"formula_67\"},\"end\":41920,\"start\":41844},{\"attributes\":{\"id\":\"formula_68\"},\"end\":42146,\"start\":42011},{\"attributes\":{\"id\":\"formula_69\"},\"end\":42703,\"start\":42634},{\"attributes\":{\"id\":\"formula_70\"},\"end\":43505,\"start\":43464},{\"attributes\":{\"id\":\"formula_71\"},\"end\":44506,\"start\":44463},{\"attributes\":{\"id\":\"formula_72\"},\"end\":45307,\"start\":45272},{\"attributes\":{\"id\":\"formula_73\"},\"end\":45905,\"start\":45741},{\"attributes\":{\"id\":\"formula_74\"},\"end\":46053,\"start\":45997},{\"attributes\":{\"id\":\"formula_75\"},\"end\":46256,\"start\":46211},{\"attributes\":{\"id\":\"formula_76\"},\"end\":46420,\"start\":46381},{\"attributes\":{\"id\":\"formula_77\"},\"end\":47519,\"start\":47439},{\"attributes\":{\"id\":\"formula_78\"},\"end\":47734,\"start\":47575},{\"attributes\":{\"id\":\"formula_79\"},\"end\":48357,\"start\":48333},{\"attributes\":{\"id\":\"formula_80\"},\"end\":48844,\"start\":48705}]", "table_ref": "[{\"end\":25808,\"start\":25807},{\"end\":26421,\"start\":26420},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26738,\"start\":26737},{\"end\":51514,\"start\":51513},{\"end\":52156,\"start\":52155},{\"end\":52398,\"start\":52394},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":52406,\"start\":52405}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":3127,\"start\":3099},{\"attributes\":{\"n\":\"2.1\"},\"end\":3557,\"start\":3526},{\"attributes\":{\"n\":\"2.2\"},\"end\":4981,\"start\":4931},{\"attributes\":{\"n\":\"3\"},\"end\":6034,\"start\":6005},{\"attributes\":{\"n\":\"3.1\"},\"end\":6601,\"start\":6561},{\"attributes\":{\"n\":\"3.2\"},\"end\":7477,\"start\":7412},{\"attributes\":{\"n\":\"3.3\"},\"end\":8493,\"start\":8451},{\"attributes\":{\"n\":\"4\"},\"end\":10602,\"start\":10552},{\"attributes\":{\"n\":\"4.1\"},\"end\":10962,\"start\":10907},{\"attributes\":{\"n\":\"4.2\"},\"end\":12082,\"start\":12032},{\"attributes\":{\"n\":\"5.1\"},\"end\":14885,\"start\":14845},{\"attributes\":{\"n\":\"5.2\"},\"end\":15808,\"start\":15762},{\"attributes\":{\"n\":\"5.3\"},\"end\":16704,\"start\":16671},{\"attributes\":{\"n\":\"5.4\"},\"end\":17943,\"start\":17877},{\"attributes\":{\"n\":\"6\"},\"end\":18543,\"start\":18500},{\"attributes\":{\"n\":\"6.1\"},\"end\":18792,\"start\":18741},{\"attributes\":{\"n\":\"6.2\"},\"end\":20193,\"start\":20169},{\"attributes\":{\"n\":\"6.3\"},\"end\":21759,\"start\":21692},{\"attributes\":{\"n\":\"6.4\"},\"end\":23280,\"start\":23220},{\"attributes\":{\"n\":\"7\"},\"end\":24521,\"start\":24466},{\"attributes\":{\"n\":\"8\"},\"end\":27117,\"start\":27091},{\"end\":27753,\"start\":27721},{\"end\":32441,\"start\":32439},{\"end\":36645,\"start\":36617},{\"end\":39235,\"start\":39177},{\"end\":40034,\"start\":39960},{\"end\":41807,\"start\":41780},{\"end\":42633,\"start\":42594},{\"end\":42747,\"start\":42705},{\"end\":42850,\"start\":42829},{\"end\":43543,\"start\":43507},{\"end\":44600,\"start\":44559},{\"end\":46132,\"start\":46101},{\"end\":47028,\"start\":46979},{\"end\":49174,\"start\":49123},{\"end\":51421,\"start\":51370},{\"end\":51505,\"start\":51472},{\"end\":52806,\"start\":52799},{\"end\":52818,\"start\":52808},{\"end\":52959,\"start\":52949},{\"end\":53092,\"start\":53088},{\"end\":53935,\"start\":53932},{\"end\":54330,\"start\":54320},{\"end\":54440,\"start\":54430},{\"end\":54655,\"start\":54646},{\"end\":54747,\"start\":54738}]", "table": "[{\"end\":55012,\"start\":54866}]", "figure_caption": "[{\"end\":52865,\"start\":52820},{\"end\":52947,\"start\":52868},{\"end\":53086,\"start\":52961},{\"end\":53930,\"start\":53094},{\"end\":54318,\"start\":53936},{\"end\":54428,\"start\":54332},{\"end\":54639,\"start\":54442},{\"end\":54644,\"start\":54642},{\"end\":54736,\"start\":54657},{\"end\":54866,\"start\":54749}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25053,\"start\":25052},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25260,\"start\":25259},{\"end\":38305,\"start\":38304},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":38499,\"start\":38498},{\"end\":38619,\"start\":38618},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":42985,\"start\":42984},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":49836,\"start\":49835}]", "bib_author_first_name": "[{\"end\":67023,\"start\":67019},{\"end\":67039,\"start\":67033},{\"end\":67060,\"start\":67047},{\"end\":67074,\"start\":67069},{\"end\":67087,\"start\":67081},{\"end\":67229,\"start\":67224},{\"end\":67231,\"start\":67230},{\"end\":67248,\"start\":67242},{\"end\":67359,\"start\":67358},{\"end\":67374,\"start\":67367},{\"end\":67376,\"start\":67375},{\"end\":67390,\"start\":67387},{\"end\":67392,\"start\":67391},{\"end\":67616,\"start\":67610},{\"end\":67629,\"start\":67623},{\"end\":67645,\"start\":67641},{\"end\":67743,\"start\":67738},{\"end\":67761,\"start\":67752},{\"end\":67804,\"start\":67803},{\"end\":68000,\"start\":67995},{\"end\":68010,\"start\":68007},{\"end\":68147,\"start\":68139},{\"end\":68166,\"start\":68159},{\"end\":68182,\"start\":68177},{\"end\":68274,\"start\":68268},{\"end\":68284,\"start\":68281},{\"end\":68298,\"start\":68294},{\"end\":68300,\"start\":68299},{\"end\":68392,\"start\":68391},{\"end\":68495,\"start\":68492},{\"end\":68518,\"start\":68511},{\"end\":68526,\"start\":68525},{\"end\":68530,\"start\":68527},{\"end\":68661,\"start\":68658},{\"end\":68677,\"start\":68671},{\"end\":68691,\"start\":68686},{\"end\":68740,\"start\":68737},{\"end\":68748,\"start\":68747},{\"end\":68752,\"start\":68749},{\"end\":68893,\"start\":68892},{\"end\":68907,\"start\":68906},{\"end\":68909,\"start\":68908},{\"end\":68921,\"start\":68920},{\"end\":69083,\"start\":69077},{\"end\":69085,\"start\":69084},{\"end\":69148,\"start\":69142},{\"end\":69165,\"start\":69158},{\"end\":69167,\"start\":69166},{\"end\":69184,\"start\":69179},{\"end\":69186,\"start\":69185},{\"end\":69202,\"start\":69194},{\"end\":69223,\"start\":69214},{\"end\":69359,\"start\":69352},{\"end\":69478,\"start\":69474},{\"end\":69495,\"start\":69489},{\"end\":69585,\"start\":69579},{\"end\":69587,\"start\":69586},{\"end\":69599,\"start\":69594},{\"end\":69601,\"start\":69600},{\"end\":69678,\"start\":69677},{\"end\":69701,\"start\":69692},{\"end\":69718,\"start\":69711},{\"end\":69847,\"start\":69841},{\"end\":69858,\"start\":69857},{\"end\":69860,\"start\":69859},{\"end\":69979,\"start\":69974},{\"end\":70002,\"start\":69997},{\"end\":70018,\"start\":70014},{\"end\":70165,\"start\":70160},{\"end\":70180,\"start\":70173},{\"end\":70284,\"start\":70275},{\"end\":70304,\"start\":70296},{\"end\":70306,\"start\":70305},{\"end\":70323,\"start\":70316},{\"end\":70325,\"start\":70324},{\"end\":70342,\"start\":70337},{\"end\":70468,\"start\":70466},{\"end\":70623,\"start\":70620},{\"end\":70640,\"start\":70632},{\"end\":70741,\"start\":70737},{\"end\":70743,\"start\":70742},{\"end\":70756,\"start\":70750},{\"end\":70758,\"start\":70757},{\"end\":70884,\"start\":70883},{\"end\":70897,\"start\":70891},{\"end\":70899,\"start\":70898},{\"end\":70969,\"start\":70968},{\"end\":71093,\"start\":71085},{\"end\":71124,\"start\":71105},{\"end\":71139,\"start\":71133},{\"end\":71388,\"start\":71384},{\"end\":71397,\"start\":71396},{\"end\":71411,\"start\":71406},{\"end\":71556,\"start\":71555},{\"end\":71571,\"start\":71566},{\"end\":71593,\"start\":71587},{\"end\":71608,\"start\":71604},{\"end\":71612,\"start\":71609},{\"end\":71630,\"start\":71622},{\"end\":71761,\"start\":71752},{\"end\":71776,\"start\":71772},{\"end\":71942,\"start\":71936},{\"end\":71961,\"start\":71955},{\"end\":71985,\"start\":71970},{\"end\":72005,\"start\":71998},{\"end\":72118,\"start\":72110},{\"end\":72126,\"start\":72123},{\"end\":72138,\"start\":72134},{\"end\":72156,\"start\":72151}]", "bib_author_last_name": "[{\"end\":67031,\"start\":67024},{\"end\":67045,\"start\":67040},{\"end\":67067,\"start\":67061},{\"end\":67079,\"start\":67075},{\"end\":67094,\"start\":67088},{\"end\":67240,\"start\":67232},{\"end\":67258,\"start\":67249},{\"end\":67365,\"start\":67360},{\"end\":67385,\"start\":67377},{\"end\":67399,\"start\":67393},{\"end\":67410,\"start\":67401},{\"end\":67621,\"start\":67617},{\"end\":67639,\"start\":67630},{\"end\":67655,\"start\":67646},{\"end\":67750,\"start\":67744},{\"end\":67766,\"start\":67762},{\"end\":67816,\"start\":67805},{\"end\":67824,\"start\":67818},{\"end\":68005,\"start\":68001},{\"end\":68019,\"start\":68011},{\"end\":68157,\"start\":68148},{\"end\":68175,\"start\":68167},{\"end\":68189,\"start\":68183},{\"end\":68279,\"start\":68275},{\"end\":68292,\"start\":68285},{\"end\":68306,\"start\":68301},{\"end\":68399,\"start\":68393},{\"end\":68408,\"start\":68401},{\"end\":68509,\"start\":68496},{\"end\":68523,\"start\":68519},{\"end\":68536,\"start\":68531},{\"end\":68550,\"start\":68538},{\"end\":68669,\"start\":68662},{\"end\":68684,\"start\":68678},{\"end\":68698,\"start\":68692},{\"end\":68745,\"start\":68741},{\"end\":68765,\"start\":68753},{\"end\":68904,\"start\":68894},{\"end\":68918,\"start\":68910},{\"end\":68937,\"start\":68922},{\"end\":69094,\"start\":69086},{\"end\":69156,\"start\":69149},{\"end\":69177,\"start\":69168},{\"end\":69192,\"start\":69187},{\"end\":69212,\"start\":69203},{\"end\":69229,\"start\":69224},{\"end\":69366,\"start\":69360},{\"end\":69487,\"start\":69479},{\"end\":69504,\"start\":69496},{\"end\":69592,\"start\":69588},{\"end\":69610,\"start\":69602},{\"end\":69690,\"start\":69679},{\"end\":69709,\"start\":69702},{\"end\":69727,\"start\":69719},{\"end\":69736,\"start\":69729},{\"end\":69855,\"start\":69848},{\"end\":69867,\"start\":69861},{\"end\":69995,\"start\":69980},{\"end\":70012,\"start\":70003},{\"end\":70030,\"start\":70019},{\"end\":70171,\"start\":70166},{\"end\":70186,\"start\":70181},{\"end\":70294,\"start\":70285},{\"end\":70314,\"start\":70307},{\"end\":70335,\"start\":70326},{\"end\":70349,\"start\":70343},{\"end\":70476,\"start\":70469},{\"end\":70483,\"start\":70478},{\"end\":70630,\"start\":70624},{\"end\":70646,\"start\":70641},{\"end\":70748,\"start\":70744},{\"end\":70769,\"start\":70759},{\"end\":70889,\"start\":70885},{\"end\":70904,\"start\":70900},{\"end\":70916,\"start\":70906},{\"end\":70974,\"start\":70970},{\"end\":70985,\"start\":70976},{\"end\":71103,\"start\":71094},{\"end\":71131,\"start\":71125},{\"end\":71146,\"start\":71140},{\"end\":71394,\"start\":71389},{\"end\":71404,\"start\":71398},{\"end\":71423,\"start\":71412},{\"end\":71564,\"start\":71557},{\"end\":71585,\"start\":71572},{\"end\":71602,\"start\":71594},{\"end\":71620,\"start\":71613},{\"end\":71640,\"start\":71631},{\"end\":71651,\"start\":71642},{\"end\":71770,\"start\":71762},{\"end\":71786,\"start\":71777},{\"end\":71953,\"start\":71943},{\"end\":71968,\"start\":71962},{\"end\":71996,\"start\":71986},{\"end\":72009,\"start\":72006},{\"end\":72121,\"start\":72119},{\"end\":72132,\"start\":72127},{\"end\":72149,\"start\":72139},{\"end\":72167,\"start\":72157}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":5662969},\"end\":67148,\"start\":66971},{\"attributes\":{\"doi\":\"1532-4435\",\"id\":\"b1\",\"matched_paper_id\":463216},\"end\":67312,\"start\":67150},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2833811},\"end\":67471,\"start\":67314},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6107503},\"end\":67801,\"start\":67473},{\"attributes\":{\"id\":\"b4\"},\"end\":67938,\"start\":67803},{\"attributes\":{\"id\":\"b5\"},\"end\":68077,\"start\":67940},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":749141},\"end\":68231,\"start\":68079},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7124218},\"end\":68358,\"start\":68233},{\"attributes\":{\"id\":\"b8\"},\"end\":68431,\"start\":68360},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6317848},\"end\":68609,\"start\":68433},{\"attributes\":{\"id\":\"b10\"},\"end\":68712,\"start\":68611},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10668924},\"end\":68851,\"start\":68714},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5836684},\"end\":69019,\"start\":68853},{\"attributes\":{\"id\":\"b13\"},\"end\":69114,\"start\":69021},{\"attributes\":{\"doi\":\"1532-4435\",\"id\":\"b14\",\"matched_paper_id\":10742222},\"end\":69290,\"start\":69116},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6392609},\"end\":69403,\"start\":69292},{\"attributes\":{\"id\":\"b16\"},\"end\":69510,\"start\":69405},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53861},\"end\":69636,\"start\":69512},{\"attributes\":{\"id\":\"b18\"},\"end\":69798,\"start\":69638},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":391854},\"end\":69915,\"start\":69800},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":632758},\"end\":70103,\"start\":69917},{\"attributes\":{\"doi\":\"1532-4435\",\"id\":\"b21\",\"matched_paper_id\":7583533},\"end\":70245,\"start\":70105},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":423350},\"end\":70413,\"start\":70247},{\"attributes\":{\"doi\":\"00359246\",\"id\":\"b23\",\"matched_paper_id\":125796822},\"end\":70569,\"start\":70415},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":877929},\"end\":70710,\"start\":70571},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":392161},\"end\":70824,\"start\":70712},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1547785},\"end\":70966,\"start\":70826},{\"attributes\":{\"id\":\"b27\"},\"end\":71028,\"start\":70968},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53306004},\"end\":71345,\"start\":71030},{\"attributes\":{\"doi\":\"doi: 10.2277\",\"id\":\"b29\"},\"end\":71469,\"start\":71347},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":156282},\"end\":71708,\"start\":71471},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":18313024},\"end\":71857,\"start\":71710},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2478905},\"end\":72068,\"start\":71859},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13911236},\"end\":72236,\"start\":72070}]", "bib_title": "[{\"end\":67017,\"start\":66971},{\"end\":67222,\"start\":67150},{\"end\":67356,\"start\":67314},{\"end\":67608,\"start\":67473},{\"end\":67993,\"start\":67940},{\"end\":68137,\"start\":68079},{\"end\":68266,\"start\":68233},{\"end\":68490,\"start\":68433},{\"end\":68735,\"start\":68714},{\"end\":68890,\"start\":68853},{\"end\":69140,\"start\":69116},{\"end\":69350,\"start\":69292},{\"end\":69577,\"start\":69512},{\"end\":69839,\"start\":69800},{\"end\":69972,\"start\":69917},{\"end\":70158,\"start\":70105},{\"end\":70273,\"start\":70247},{\"end\":70464,\"start\":70415},{\"end\":70618,\"start\":70571},{\"end\":70735,\"start\":70712},{\"end\":70881,\"start\":70826},{\"end\":71083,\"start\":71030},{\"end\":71553,\"start\":71471},{\"end\":71750,\"start\":71710},{\"end\":71934,\"start\":71859},{\"end\":72108,\"start\":72070}]", "bib_author": "[{\"end\":67033,\"start\":67019},{\"end\":67047,\"start\":67033},{\"end\":67069,\"start\":67047},{\"end\":67081,\"start\":67069},{\"end\":67096,\"start\":67081},{\"end\":67242,\"start\":67224},{\"end\":67260,\"start\":67242},{\"end\":67367,\"start\":67358},{\"end\":67387,\"start\":67367},{\"end\":67401,\"start\":67387},{\"end\":67412,\"start\":67401},{\"end\":67623,\"start\":67610},{\"end\":67641,\"start\":67623},{\"end\":67657,\"start\":67641},{\"end\":67818,\"start\":67803},{\"end\":67826,\"start\":67818},{\"end\":68007,\"start\":67995},{\"end\":68021,\"start\":68007},{\"end\":68159,\"start\":68139},{\"end\":68177,\"start\":68159},{\"end\":68191,\"start\":68177},{\"end\":68281,\"start\":68268},{\"end\":68294,\"start\":68281},{\"end\":68308,\"start\":68294},{\"end\":68401,\"start\":68391},{\"end\":68410,\"start\":68401},{\"end\":68511,\"start\":68492},{\"end\":68525,\"start\":68511},{\"end\":68538,\"start\":68525},{\"end\":68552,\"start\":68538},{\"end\":68671,\"start\":68658},{\"end\":68686,\"start\":68671},{\"end\":68700,\"start\":68686},{\"end\":68747,\"start\":68737},{\"end\":68767,\"start\":68747},{\"end\":68906,\"start\":68892},{\"end\":68920,\"start\":68906},{\"end\":68939,\"start\":68920},{\"end\":69096,\"start\":69077},{\"end\":69158,\"start\":69142},{\"end\":69179,\"start\":69158},{\"end\":69194,\"start\":69179},{\"end\":69214,\"start\":69194},{\"end\":69231,\"start\":69214},{\"end\":69368,\"start\":69352},{\"end\":69489,\"start\":69474},{\"end\":69506,\"start\":69489},{\"end\":69594,\"start\":69579},{\"end\":69612,\"start\":69594},{\"end\":69692,\"start\":69677},{\"end\":69711,\"start\":69692},{\"end\":69729,\"start\":69711},{\"end\":69738,\"start\":69729},{\"end\":69857,\"start\":69841},{\"end\":69869,\"start\":69857},{\"end\":69997,\"start\":69974},{\"end\":70014,\"start\":69997},{\"end\":70032,\"start\":70014},{\"end\":70173,\"start\":70160},{\"end\":70188,\"start\":70173},{\"end\":70296,\"start\":70275},{\"end\":70316,\"start\":70296},{\"end\":70337,\"start\":70316},{\"end\":70351,\"start\":70337},{\"end\":70478,\"start\":70466},{\"end\":70485,\"start\":70478},{\"end\":70632,\"start\":70620},{\"end\":70648,\"start\":70632},{\"end\":70750,\"start\":70737},{\"end\":70771,\"start\":70750},{\"end\":70891,\"start\":70883},{\"end\":70906,\"start\":70891},{\"end\":70918,\"start\":70906},{\"end\":70976,\"start\":70968},{\"end\":70987,\"start\":70976},{\"end\":71105,\"start\":71085},{\"end\":71133,\"start\":71105},{\"end\":71148,\"start\":71133},{\"end\":71396,\"start\":71384},{\"end\":71406,\"start\":71396},{\"end\":71425,\"start\":71406},{\"end\":71566,\"start\":71555},{\"end\":71587,\"start\":71566},{\"end\":71604,\"start\":71587},{\"end\":71622,\"start\":71604},{\"end\":71642,\"start\":71622},{\"end\":71653,\"start\":71642},{\"end\":71772,\"start\":71752},{\"end\":71788,\"start\":71772},{\"end\":71955,\"start\":71936},{\"end\":71970,\"start\":71955},{\"end\":71998,\"start\":71970},{\"end\":72011,\"start\":71998},{\"end\":72123,\"start\":72110},{\"end\":72134,\"start\":72123},{\"end\":72151,\"start\":72134},{\"end\":72169,\"start\":72151}]", "bib_venue": "[{\"end\":67112,\"start\":67096},{\"end\":67296,\"start\":67260},{\"end\":67459,\"start\":67412},{\"end\":67701,\"start\":67657},{\"end\":67736,\"start\":67703},{\"end\":67903,\"start\":67826},{\"end\":68071,\"start\":68021},{\"end\":68224,\"start\":68191},{\"end\":68352,\"start\":68308},{\"end\":68389,\"start\":68360},{\"end\":68603,\"start\":68552},{\"end\":68656,\"start\":68611},{\"end\":68823,\"start\":68767},{\"end\":69013,\"start\":68939},{\"end\":69075,\"start\":69021},{\"end\":69267,\"start\":69231},{\"end\":69386,\"start\":69368},{\"end\":69472,\"start\":69405},{\"end\":69628,\"start\":69612},{\"end\":69675,\"start\":69638},{\"end\":69901,\"start\":69869},{\"end\":70097,\"start\":70032},{\"end\":70224,\"start\":70188},{\"end\":70407,\"start\":70351},{\"end\":70552,\"start\":70485},{\"end\":70704,\"start\":70648},{\"end\":70807,\"start\":70771},{\"end\":70954,\"start\":70918},{\"end\":71007,\"start\":70987},{\"end\":71226,\"start\":71148},{\"end\":71382,\"start\":71347},{\"end\":71702,\"start\":71653},{\"end\":71822,\"start\":71788},{\"end\":72058,\"start\":72011},{\"end\":72220,\"start\":72169},{\"end\":67785,\"start\":67768},{\"end\":71308,\"start\":71228},{\"end\":71841,\"start\":71824}]"}}}, "year": 2023, "month": 12, "day": 17}