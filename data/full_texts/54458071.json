{"id": 54458071, "updated": "2023-09-29 19:10:52.675", "metadata": {"title": "Moment Matching for Multi-Source Domain Adaptation", "authors": "[{\"first\":\"Xingchao\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Qinxun\",\"last\":\"Bai\",\"middle\":[]},{\"first\":\"Xide\",\"last\":\"Xia\",\"middle\":[]},{\"first\":\"Zijun\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Kate\",\"last\":\"Saenko\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 12, "day": 4}, "abstract": "Conventional unsupervised domain adaptation (UDA) assumes that training data are sampled from a single domain. This neglects the more practical scenario where training data are collected from multiple sources, requiring multi-source domain adaptation. We make three major contributions towards addressing this problem. First, we collect and annotate by far the largest UDA dataset with six domains and about 0.6 million images distributed among 345 categories, addressing the gap in data availability for multi-source UDA research. Second, we propose a new deep learning approach, Moment Matching for Multi-Source Domain Adaptation M3SDA, which aims to transfer knowledge learned from multiple labeled source domains to an unlabeled target domain by dynamically aligning moments of their feature distributions. Third, we provide new theoretical insight specifically for moment matching approaches in both single and multiple source domain adaptation. Extensive experiments are conducted to demonstrate the power of our new dataset in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1812.01754", "mag": "2981720610", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/PengBXHSW19", "doi": "10.1109/iccv.2019.00149"}}, "content": {"source": {"pdf_hash": "d02bcdab816c041a07ef3b74dd1285162ef77101", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1812.01754v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1812.01754", "status": "GREEN"}}, "grobid": {"id": "e2bfdebed3afa772726fa4492a8adb89d28cf4e3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d02bcdab816c041a07ef3b74dd1285162ef77101.txt", "contents": "\nMoment Matching for Multi-Source Domain Adaptation\n\n\nXingchao Peng xpeng@bu.edu \nDepartment of Computer Science\nBoston University\n\n\nQinxun Bai qinxun@bu.edu \nDepartment of Computer Science\nBoston University\n\n\nXide Xia xidexia@bu.edu \nDepartment of Computer Science\nBoston University\n\n\nZijun Huang zijun.huang@columbia.edu \nColumbia University & Mado AI Research\n\n\nKate Saenko saenko@bu.edu \nDepartment of Computer Science\nBoston University\n\n\nBo Wang bowang@vectorinstitute.ai \nVector Institute for Artificial Intelligence\n\n\nMoment Matching for Multi-Source Domain Adaptation\n\nConventional unsupervised domain adaptation (UDA) assumes that training data are sampled from a single domain. This neglects the more practical scenario where training data are collected from multiple sources, requiring multi-source domain adaptation. We make three major contributions towards addressing this problem. First, we collect and annotate by far the largest UDA dataset with six domains and about 0.6 million images distributed among 345 categories, addressing the gap in data availability for multisource UDA research. Second, we propose a new deep learning approach, Moment Matching for Multi-Source Domain Adaptation (M 3 SDA), which aims to transfer knowledge learned from multiple labeled source domains to an unlabeled target domain by dynamically aligning moments of their feature distributions. Third, we provide new theoretical insight specifically for moment matching approaches in both single and multiple source domain adaptation. Extensive experiments are conducted to demonstrate the power of our new dataset in benchmarking state-of-the-art multisource domain adaptation methods, as well as the advantage of our proposed model. Data and source code are available at\n\nIntroduction\n\nGeneralizing models learned on one visual domain to novel domains has been a major obstacle in the quest for universal object recognition. The performance of the learned models degrades significantly when testing on novel domains due to the presence of domain shift [36].\n\nRecently, transfer learning and domain adaptation methods have been proposed to mitigate the domain gap. For example, several UDA methods [27,41,25] incorporate Maximum Mean Discrepancy loss into a neural network to diminish the domain discrepancy; other models introduce different learning schema to align the source and target domains, including aligning second order correlation [39,32], moment matching [47], adversarial domain confusion [40,8,38] and GAN-based alignment [50,15,23]. However, sketch real quickdraw painting infograph clipart airplane clock axe ball bicycle bird strawberry flower pizza butterfly most of current UDA methods assume that source samples are collected from a single domain. This assumption neglects the more practical scenarios where labeled images are typically collected from multiple domains. For example, the training images can be taken under different weather or lighting conditions, share different visual cues, and even have different modalities (as shown in Figure 1). In this paper, we consider multi-source domain adaptation (MSDA), a more difficult but practical problem of knowledge transfer from multiple distinct domains to one unlabeled target domain. The main challenges in the research of MSDA are that: (1) the source data has multiple domains, which hampers the effectiveness of mainstream single UDA method; (2) source domains also possess domain shift with each other; (3) the lack of large-scale multidomain dataset hinders the development of MSDA models.\n\nIn the context of MSDA, some theoretical analysis [1,28,4,49,14] has been proposed for multi-source domain adaptation (MSDA). Ben-David et al [1] pioneer this direction by introducing an H\u2206H-divergence between the weighted combination of source domains and target domain. More applied works [6,45] use an adversarial dis-\n\n\nDataset Year Images Classes Domains Description\n\nDigit-Five -\u223c100,000 10 5 digit Office [37] 2010 4,110 31 3 office Office-Caltech [11] 2012 2,533 10 4 office CAD-Pascal [33] 2015 12,000 20 6 animal,vehicle\n\nOffice-Home [43] 2017 15,500 65 4 office, home PACS [21] 2017 9,991 7 4 animal, stuff Open MIC [17] 2018 16,156 --museum Syn2Real [35] 2018 280,157 12 3 animal,vehicle LSDAC (Ours) -569,010 345 6 see Appendix Table 1. A collection of most notable datasets to evaluate domain adaptation methods. Specifically, \"Digit-Five\" dataset indicates five most popular digit datasets (MNIST [19], MNIST-M [8] USPS, Synthetic Digits [8], SVHN, and USPS) which are widely used to evaluate domain adaptation models. Our dataset is challenging as it contains more images, categories, and domains than other datasets. (see Table 9, Table 10, and Table 11 in Appendix for detailed categories.)\n\ncriminator to align the multi-source domains with the target domain. However, these works focus only on aligning the source domains with the target, neglecting the domain shift between the source domains. Moreover, H\u2206Hdivergence based analysis does not directly correspond to moment matching approaches. In terms of data, research has been hampered due to the lack of large-scale domain adaptation datasets, as state-ofthe-art datasets contain only a few images or have a limited number of classes. Many domain adaptation models exhibit saturation when evaluated on these datasets. For example, many methods achieve \u223c90 accuracy on the popular Office [37] dataset; Self-Ensembling [7] reports \u223c99% accuracy on the \"Digit-Five\" dataset and \u223c92% accuracy on Syn2Real [35] dataset.\n\nIn this paper, we first collect and label a new multidomain dataset called \"Large Scale Domain Adaptation Challenge\" (LSDAC), aiming to overcome benchmark saturation. Our dataset consists of six distinct domains, 345 categories and \u223c0.6 million images. A comparison of LS-DAC and several existing datasets is shown in Table 1, and example images are illustrated in Figure 1. We evaluate several state-of-the-art single domain adaptation methods on our dataset, leading to surprising findings (see Section 5). We also extensively evaluate our model on existing datasets and on LSDAC and show that it outperforms the existing single-and multi-source approaches.\n\nSecondly, we propose a novel approach called M 3 SDAto tackle MSDA task by aligning the source domains with the target domain, and aligning the source domains with each other simultaneously. We dispose multiple complex adversarial training procedures presented in [45], but directly align the moments of their deep feature distributions, leading to a more robust and effective MSDA model. To our best knowledge, we are the first to empirically demonstrate that aligning the source domains is beneficial for MSDA tasks.\n\nFinally, we extend existing theoretical analysis [1,14,49] to the case of moment-based divergence between source and target domains, which provides new theoretical insight specifically for moment matching approaches in domain adaptation, including our approach and many others.\n\n\nRelated Work\n\nWe first summarize the state-of-the-art domain adaptation benchmarks and explain how our LSDAC differs from previous ones. Then, we describe related domain adaptation methods.\n\nDomain Adaptation Datasets Several notable datasets that can be utilized to evaluate domain adaptation approaches are summarized in Table 1. The Office dataset [37] is a popular benchmark for office environment objects. It contains 31 categories captured in three domains: office environment images taken with a high quality camera (DSLR), office environment images taken with a low quality camera (Webcam), and images from an online merchandising website (Amazon). The office dataset and its extension, Office-Caltech10 [11], have been used in numerous domain adaptation papers [25,40,27,39,45], and the adaptation performance has reached \u223c90% accuracy. More recent benchmarks [43,17,34] are proposed to evaluate the effectiveness of domain adaptation models. However, these datasets are small-scale and limited by their specific environments, such as office, home, and museum. Our dataset contains about 600k images, distributed in 345 categories and 6 distinct domains. We capture various object divisions, ranging from furniture, cloth, electronic to mammal, building, etc.\n\nSingle-source UDA Over the past decades, various singlesource UDA methods have been proposed. These methods can be taxonomically divided into three categories. The first category is the discrepancy-based DA approach, which utilizes different metric learning schemas to diminish the domain shift between source and target domains. Inspired by the kernel two-sample test [12], Maximum Mean Discrepancy (MMD) is applied to reduce distribution shift in various methods [27,41,9,44]. Other commonly used methods include correlation alignment [39,32], Kullback-Leibler (KL) divergence [51], and H divergence [1]. The second category is the adversarial-based approach [24,40]. A domain discriminator is leveraged to encourage the domain confusion by an adversarial objective. Among these approaches, generative adversarial networks are widely used to learn domain-invariant features as well to generate fake source or target data. Other frameworks utilize only adversarial loss to bridge two domains. The third category is reconstruction-based, which assumes the data reconstruction helps the DA models to learn domain-invariant features. The reconstruction is obtained via an encoderdecoder [3,10] or a GAN discriminator, such as dual tree  golf club  squirrel  dog  whale  spreadsheet  snowman  tiger  table  shoe  windmill  submarine  feather  bird  spider  strawberry  nail  beard  bread  train  watermelon  zebra  sheep  elephant  teapot  eye  mushroom   sea turtle   sword  streetlight  lighthouse  owl  horse  penguin  pond  sock  snorkel  helicopter  snake  butterfly  umbrella  river  fish  van  grapes   hot air balloon  wine glass  teddy-bear  speedboat  sun  swan  bicycle  brain  bracelet  tornado  flower  stairs  cup  steak  tractor  wristwatch  toothbrush  suitcase  triangle  parrot  zigzag   ice cream   mug  beach  cat  raccoon  garden  monkey  shark   animal migration   lion  saxophone  asparagus  tent  firetruck  hand  spoon  squiggle  palm tree  octopus  toaster  skateboard  dumbbell  mountain  bottlecap  pig  shovel   washing machine  wine bottle   stove  coffee cup   leaf  goatee  drums  yoga  bowtie  sailboat  scissors  onion  snail  bus   house plant   map  moon  lobster  canoe  pineapple  necklace  basket  bear  envelope  bee  grass  motorbike  bed  donut  face  hat  skull  school bus  dolphin  cruise ship  toothpaste  blueberry  shorts  eyeglasses  backpack  book  broccoli  duck  hamburger  helmet  cake  rhinoceros  ladder  trombone  hedgehog  television  scorpion  pear  flashlight  barn  leg  ocean  telephone  bench  pillow  hot tub  fence  flamingo  waterslide  crocodile  sweater  moustache  rollerskates  circle  giraffe  syringe  pool  crab  candle  carrot  soccer ball  broom  sandwich  snowflake  parachute  castle  sleeping bag  tooth  binoculars  kangaroo  rifle  wheel   pickup truck  hot dog  pants  panda  screwdriver  police car  camel  lightning  pencil  arm  microphone  fireplace  megaphone  piano  lollipop  trumpet  keyboard  peas   Detailed numbers are shown in Table 9, Table 10 and Table 11 in Appendix. (Zoom in to see the exact class names!) GAN [46], cycle-GAN [50], disco-GAN [16], and Cy-CADA [15]. Though these methods make progress on UDA, few of them consider the practical scenario where training data are collected from multiple sources. Our paper proposes a model to tackle multi-source domain adaptation, which is a more general and challenging scenario.\n\nMulti-Source Domain Adaptation Compared with single source UDA, multi-source domain adaptation assumes that training data from multiple sources are available. Originated from the early theoretical analysis [1,28,4], MSDA has many practical applications [45,6]. Ben-David et al [1] introduce an H\u2206H-divergence between the weighted combination of source domains and target domain. Crammer et al [4] establish a general bound on the expected loss of the model by minimizing the empirical loss on the nearest k sources. Mansour et al [28] claim that the target hypothesis can be represented by a weighted combination of source hypotheses. In the more applied works, Deep Cocktail Network (DCTN) [45] proposes a k-way domain discriminator and category classifier for digit classification and real-world object recognition. Hoffman et al [14] propose normalized solutions with theoretical guarantees for cross-entropy loss, aiming to provide a solution for the MSDA problem with very practical benefits. Duan et al [6] propose Domain Selection Machine for event recognition in consumer videos by leveraging a large number of loosely labeled web images from different sources. Different from these methods, our model directly matches all the distributions by matching the moments. Moreover, we provide a concrete proof of why matching the moments of multiple distributions works for multi-source domain adaptation.\n\n\nMoment Matching\n\nThe moments of distributions have been studied by the machine learning community for a long time. In order to diminish the domain discrepancy be-tween two domains, different moment matching schemes have been proposed. For example, MMD matches the first moments of two distributions. Sun et al [39] propose an approach that matches the second moments. Zhang et al [48] propose to align infinte-dimensional covariance matrices in RKHS. Zellinger et al [47] introduce a moment matching regularizer to match high moments. As the generative adversarial network (GAN) becomes popular, many GANbased moment matching approaches have been proposed.\n\nMcGAN [29] utilizes a GAN to match the mean and covariance of feature distributions. GMMN [22] and MMD GAN [20] are proposed for aligning distribution moments with generative neural networks. Compared to these methods, our work focuses on matching distribution moments for multiple domains and more importantly, we demonstrate that this is crucial for multi-source domain adaptation.\n\n\nThe LSDAC dataset\n\nIt is well-known that deep models require massive amounts of training data. Unfortunately, existing datasets for visual domain adaptation are either small-scale or limited in the number of categories. We collect by far the largest domain adaptation dataset to date, LSDAC (Large Scale Domain Adaptation Challenge). The LSDAC contains six domains, with each domain containing 345 categories of common objects, as listed in Table 9, Table 10,  and Table 11 (see Appendix). The domains include Clipart (clp, see Appendix, Figure 8): collection of clipart images; Infograph (inf, see Figure 9): infographic images with specific object; Painting (pnt, see Figure 10): artistic depictions of objects in the form of paintings; Quickdraw (qdr, see Figure 11): drawings of the worldwide players of game \"Quick Draw!\" 1 ; Real (rel, see Figure 12): pho-  tos and real world images; and Sketch (skt, see Figure 13): sketches of specific objects. The images from clipart, infograph, painting, real, and sketch domains are collected by searching a category name combined with a domain name (e.g. \"aeroplane painting\") in different image search engines. One of the main challenges is that the downloaded data contain a large portion of outliers. To clean the dataset, we hire 20 annotators to manually filter out the outliers. This process took around 2,500 hours (more than 2 weeks) in total. To control the annotation quality, we assign two annotators to each image, and only take the images agreed by both annotators. After the filtering process, we keep 423.5k images from the 1.2 million images crawled from the web. The dataset has an average of around 150 images per category for clipart and infograph domain, around 220 per category for painting and sketch domain, and around 510 for real domain. A statistical overview of the dataset is shown in Figure 2.\n\nThe quickdraw domain is downloaded directly from https://quickdraw.withgoogle.com/. The raw data are presented as a series of discrete points with temporal information. We use the B-spline [5] algorithm to connect all the points in each strike to get a complete drawing. We choose 500 images for each category to form the quickdraw domain, which contains 172.5k images in total.\n\n\nMoment Matching for Multi-Source DA\n\nGiven D S = {D 1 , D 2 , ..., D N } the collection of labeled source domains and D T the unlabeled target domain, where all domains are defined by bounded rational measures on input space X , the multi-source domain adaptation problem aims to find a hypothesis in the given hypothesis space H, which minimizes the testing target error on D T . \nDefinition 1. Assume X 1 , X 2 , ..., X N , X T are collections of i.i.d. samples from D 1 , D 2 , ..., D N , D T respectively, then the Moment Distance between D S and D T is defined as M D 2 (D S , D T ) = 2 k=1 1 N N i=1 E(X k i ) \u2212 E(X k T ) 2 + N 2 \u22121 N \u22121 i=1 N j=i+1 E(X k i ) \u2212 E(X k j ) 2 . (1)min G,C N i=1 L Di + \u03bb min G M D 2 (D S , D T ),(2)\nwhere L Di is the softmax cross entropy loss for the classifier C i on domain D i , and \u03bb is the trade-off parameter. M 3 SDA assumes that p(y|x) will be aligned automatically when aligning p(x), which might not hold in practice. To mitigate this limitation, we further propose M 3 SDA-\u03b2.\n\nM 3 SDA-\u03b2 In order to align p(y|x) and p(x) at the same time, we follow the training paradigm proposed by [38].\n\nIn particular, we leverage two classifiers per domain to form N pairs of classifiers C =\n{(C 1 , C 1 ), (C 2 , C 2 ), ..., (C N , C N )}.\nThe training procedure includes three steps. i). We train G and C to classify the multi-source samples correctly. The objective is similar to Equation 2. ii). We then train the classifier pairs for a fixed G. The goal is to make the discrepancy of each pair of classifiers as large as possible on the target domain. For example, the outputs of C 1 and C 1 should possess a large discrepancy. Following [38], we define the discrepancy of two classifiers as the L1-distance between the outputs of the two classifiers. The objective is:\nmin C N i=1 L Di \u2212 N i |P Ci (D T ) \u2212 P Ci (D T )|,(3)\nwhere P Ci (D T ), P Ci (D T ) denote the outputs of C i , C i respectively on the target domain. iii). Finally, we fix C and train G to minimize the discrepancy of each classifier pair on the target domain. The objective function is as follows:\nmin G N i |P Ci (D T ) \u2212 P Ci (D T )|,(4)\nThese three training steps are performed periodically until the whole network converges. Ensemble Schema In the testing phase, testing data from the target domain are forwarded through the feature generator and the N classifiers. We propose two schemas to combine the outputs of the classifiers:\n\n\u2022 average the outputs of the classifiers, marked as\nM 3 SDA * \u2022 Derive a weight vector W = (w 1 , . . . , w N \u22121 ) ( N \u22121 i=1 w i = 1,\nassuming N -th domain is the target). The final prediction is the weighted average of the outputs.\n\nTo this end, how to derive the weight vector becomes a critical problem. The main philosophy of the weight vector is to make it represent the intrinsic closeness between the target domain and source domains. In our setting, the weighted vector is derived by the source-only accuracy between the i-th domain and the N -th domain, i.e.\nw i = acc i / N \u22121 j=1 acc j .\n\nTheoretical Insight\n\nFollowing [1], we introduce a rigorous model of multisource domain adaptation for binary classification. A domain D = (\u00b5, f ) is defined by a probability measure (distribution) \u00b5 on the input space X and a labeling function f :\nX \u2192 {0, 1}. A hypothesis is a function h : X \u2192 {0, 1}.\nThe probability that h disagrees with the domain labeling function f under the domain distribution \u00b5 is defined as:\nD (h) = D (h, f ) = E \u00b5 [|h(x) \u2212 f (x)|].(5)\nFor a source domain D S and a target domain D T , we refer to the source error and the target error of a hypothesis h as S (h) = D S (h) and T (h) = D T (h) respectively. When the expectation in Equation 5 is computed with respect to an empirical distribution, we denote the corresponding empirical error by\u02c6 D (h), such as\u02c6 S (h) and T (h). In particular, we examine algorithms that minimize convex combinations of source errors, i.e., given a weight vector \u03b1 = (\u03b1 1 , . . . , \u03b1 N ) with N j=1 \u03b1 j = 1, we define the \u03b1-weighted source error of a hypothesis h as\n\u03b1 (h) = N j=1 \u03b1 j j (h), where j (h) is the shorthand of Dj (h).\nThe empirical \u03b1-weighted source error can be defined analogously and denoted by\u02c6 \u03b1 (h).\n\nPrevious theoretical bounds [1,14,49] on the target error are based on the H\u2206H-divergence between the source and target domains. While providing theoretical insights for general multi-source domain adaptation, these H\u2206Hdivergence based bounds do not directly motivate momentbased approaches. In order to provide a specific insight for moment-based approaches, we introduce the k-th order cross-moment divergence between domains, denoted by d CM k (\u00b7, \u00b7), and extend the analysis in [1] to derive the following moment-based bound for multi-source domain adaptation. See Appendix for the definition of the crossmoment divergence and the proof of the theorem.\nTheorem 1. Let H be a hypothesis space of V C dimension d.\nLet m be the size of labeled samples from all sources {D 1 , D 2 , ..., D N }, S j be the labeled sample set of size \u03b2 j m ( j \u03b2 j = 1) drawn from \u00b5 j and labeled by the groundtruth labeling function f j . If\u0125 \u2208 H is the empirical minimizer of \u03b1 (h) for a fixed weight vector \u03b1 and h * T = min h\u2208H T (h) is the target error minimizer, then for any \u03b4 \u2208 (0, 1) and any > 0, there exist N integers {n j } N j=1 and N constants {a n j } N j=1 , such that with probability at least 1 \u2212 \u03b4,\nT (\u0125) \u2264 T (h * T ) + \u03b7 \u03b1,\u03b2,m,\u03b4 + + N j=1 \u03b1 j 2\u03bb j + a n j n j k=1 d CM k (D j , D T ) ,(6)\nwhere \u03b7 \u03b1,\u03b2,m,\u03b4 = 4 ( N j=1\n\u03b1 2 j \u03b2j )( 2d(log( 2m d )+1)+2 log( 4 \u03b4 ) m ) and \u03bb j = min h\u2208H { T (h) + j (h)}.\nTheorem 1 shows that the upper bound on the target error of the learned hypothesis depends on the pairwise moment divergence d CM k (D S , D T ) between the target domain and each source domain. 2 This provides a direct motivation for moment matching approaches beyond ours. In particular, it motivates our multi-source domain adaptation approach to align the moments between each target-source pair. Moreover, it is obvious that the last term of the bound,\nk d CM k (D j , D T )\n, is lower bounded by the pairwise divergences between source domains. To see this, consider the toy example consisting of two sources D 1 , D 2 , and a target D T , since d CM k (\u00b7, \u00b7) is a metric, triangle inequality implies the following lower bound:\nd CM k (D 1 , D T ) + d CM k (D 2 , D T ) \u2265 d CM k (D 1 , D 2 ).\nThis motivates our algorithm to also align the moments between each pair of source domains. Further discussions of Theorem 1 and its relationship with our algorithm are provided in the Appendix.\n\n\nExperiments\n\nWe perform an extensive evaluation on the following tasks: digit classification (MNIST, SVHN, USPS, MNIST-M, Sythetic Digits), Office-Caltech10), and LSDAC dataset. In total, we conduct 714 experiments. The experiments are run on a GPU-cluster with 24 GPUs and the total running time is more than 21,440 GPU-hours. Due to space limitations, we only report major results; more implementation details are provided in the supplementary material. All of our experiments are implemented in the PyTorch 3 platform. \n\n\nExperiments on Digit Recognition\n\n\nExperiments on Office-Caltech10\n\nThe Office-Caltech10 [11] dataset is extended from the standard Office31 [37] dataset. It consists of the same 10 object categories from 4 different domains: Amazon, Caltech, DSLR, and Webcam.\n\nThe experimental results on Office-Caltech10 dataset are shown in Table 4. Our model M 3 SDA gets a 96.1% average accuracy on this dataset, and M 3 SDA-\u03b2 further boosts the performance to 96.4%. All the experiments are based on ResNet-101 pre-trained on ImageNet. As far as we know, our models achieve the best performance among all the results ever reported on this dataset. We have also tried AlexNet, but it did not work as well as ResNet-101.  Table 3. Single-source baselines on the LSDAC dataset. Several single-source adaptation baselines are evaluated on the LSDAC dataset, including AlexNet [18], DAN [25], JAN [27], DANN [8], RTN [26], ADDA [40], MCD [38], SE [7]. In each sub-table, the column-wise domains are selected as the source domain and the row-wise domains are selected as the target domain. The green numbers represent the average performance of each column or row. The red numbers denote the average accuracy for all the 30 (source, target) combinations.  \n\n\nExperiments on LSDAC\n\nSingle-Source Adaptation To demonstrate the intrinsic difficulty of LSDAC, we evaluate multiple state-ofthe-art algorithms for single-source adaptation: Deep Alignment Network (DAN) [25], Joint Adaptation Network (JAN) [27], Domain Adversarial Neural Network (DANN) [8], Residual Transfer Network (RTN) [26], Adversarial Deep Domain Adaptation (ADDA) [40], Maximum Classifier Discrepancy (MCD) [38], and Self-Ensembling (SE) [7].  our dataset.\n\nMulti-Source Domain Adaptation LSDAC contains six domains. Inspired by Xu et al [45], we introduce two MSDA standards: (1) single best, reporting the single bestperforming source transfer result on the test set, and (2) source combine, combining the source domains to a single domain and performing traditional single-source adaptation. The first standard evaluates whether MSDA can improve the best single source UDA results; the second testify whether MSDA is necessary to exploit. Baselines For both single best and source combine experiment setting, we take the following state-of-the-art methods as our baselines: Deep Alignment Network (DAN) [25], Joint Adaptation Network (JAN) [27], Domain Adversarial Neural Network (DANN) [8], Residual Transfer Network (RTN) [26], Adversarial Deep Domain Adaptation (ADDA) [40], Maximum Classifier Discrepancy (MCD) [38], and Self-Ensembling (SE) [7]. For multisource domain adaptation, we take Deep Cocktail Network (DCTN) [45] as our baseline.\n\nResults The experimental results of multi-source domain adaptation are shown in Table 5. We report the results of the two different weighting schemas and all the baseline results in Table 5 models designed for MSDA outperform the single best UDA results, the source combine results, and the multisource baseline. From the experimental results, we make three interesting observations. (1)The performance of M 3 SDA * is 40.8%. After applying the weight vector W, M 3 SDAimproves the mean accuracy by 0.7 percent. (2) In clp,inf,pnt,rel,skt\u2192qdr setting, the performances of our models (as well as DCTN [45]) are worse than sourceonly baseline, which indicates that negative transfer [31] occurs. (3) In the source combine setting, the performances of DAN [25], RTN [26], JAN [27], DANN [8] are lower than the source only baseline, indicating the negative transfer happens when the training data are from multiple source domains.\n\n\nEffect of Category Number\n\nTo show how the number of categories affects the performance of state-of-the-art domain adaptation methods, we choose the painting\u2192real setting in LSDAC and gradually increase the number of category from 20 to 345. The results are in Figure 4. An interesting observation is that when the number of categories is small (which is exactly the case in most domain adaptation benchmarks), all methods tend to perform well. However, their performances drop at different rates when the number of categories increases. For example, SE [7] performs the best when there is a limit number of categories, but worst when the number of categories is larger than 150.\n\n\nAblation Study\n\nTo show how much performance gain we can get through source-source alignment (S-S) and source-target (S-T) alignment, we perform ablation study based on our model. From Table 6, we observe the key factor to the performance boost is matching the moments of source distributions to the target distribution. Matching source domains with each other further boosts the performance. The experimental results empirically demonstrate that aligning source domains is essential for MSDA.  Table 6. S-S only: only matching source domains with each other; S-T only: only matching source with target; \"+\": performance gain from baseline.\n\n\nConclusion\n\nIn this paper, we have collected, annotated and evaluated by far the largest domain adaptation dataset named LSDAC. The dataset is challenging due to the presence of notable domain gaps and a large number of categories. We hope it will be beneficial to evaluate future single-and multi-source UDA methods.\n\nWe have also proposed M 3 SDA to align multiple source domains with the target domain. We derive a meaningful error bound for our method under the framework of crossmoment divergence. Further, we incorporate the moment matching component into deep neural network and train the model in an end-to-end fashion. Extensive experiments on multi-source domain adaptation benchmarks demonstrate that our model outperforms all the multi-source baselines as well as the best single-source domain adaptation method.\n\n\nAppendix\n\nThe appendix is organized as follows: Section A introduces the formal definition of the cross-moment divergence; Section B gives the proof of Theorem 1 and further discussions; Section C provides the details of experiments on \"Digit-Five\" dataset; Section D shows feature visualization with t-SNE plot; Section E shows how the number of categories will affect the performance of the state-of-theart models; Section F and Section G introduce the ResNet baselines and Train/Test split of our LSDAC dataset, respectively; Section H and Section I show the image samples and the detailed statistics of our LSDAC dataset.\n\nA. Cross-moment Divergence Definition 2 (cross-moment divergence). Given a compact domain X \u2282 R n and two probability measures \u00b5, \u00b5 on X , the k-th order cross-moment divergence between \u00b5 and \u00b5 is\nd CM k (\u00b5, \u00b5 ) = i\u2208\u2206 k X n j=1 (x j ) ij d\u00b5(x) \u2212 X n j=1 (x j ) ij d\u00b5 (x) , where \u2206 k = {(i 1 , i 2 , . . . , i n ) \u2208 N n 0 | n j=1 i j = k}.\nAs seen in the rest of the paper, for two domains D = (\u00b5, f ) and D = (\u00b5 , f ), we use d CM k (D, D ) to denote d CM k (\u00b5, \u00b5 ) for readability concerns.\n\n\nB. Proof of Theorem 1\n\nTheorem 2 (Weierstrass Approximation Theorem). Let f : C \u2192 R be continuous, where C is a compact subset of R n . There exists a sequence of real polynomials (P m (x)) m\u2208N , such that sup x\u2208C |f (x) \u2212 P m (x)|\u2192 0, as m \u2192 \u221e.\n\nNote that for x \u2208 R n , a multivariate polynomial P m :\nR n \u2192 R is of the form P m (x) = m k=1 i\u2208\u2206 k a i n j=1 (x j ) ij , where \u2206 k = {(i 1 , i 2 , . . . , i n ) \u2208 N n 0 | n j=1 i j = k}.\nLemma 3. For any hypothesis h, h \u2208 H, for any > 0, there exist an integer n and a constant a n , such that\n| S (h, h ) \u2212 T (h, h )|\u2264 1 2 a n n k=1 d CM k (D S , D T ) + .\nProof.\n| S (h, h ) \u2212 T (h, h )|\u2264 sup h,h \u2208H | S (h, h ) \u2212 T (h, h )| = sup h,h \u2208H |P x\u223cD S [h(x) = h (x)] \u2212 P x\u223cD T [h(x) = h (x)]| = sup h,h \u2208H X 1 h(x) =h (x) d\u00b5 S \u2212 X 1 h(x) =h (x) d\u00b5 T ,(7)\nwhere X is a compact subset of R n . For any fixed h, h , the indicator function 1 h(x) =h (x) (x) is a Lebesgue integrable function (L 1 function) on X . It is known that the space of continuous functions with compact support, denoted by C c (X ), is dense in L 1 (X ), i.e., any L 1 function on X can be approximated arbitrarily well 4 by functions in C c (X ). As a result, for any 2 > 0, there exists f \u2208 C c (X ), such that,\nsup h,h \u2208H X 1 h(x) =h (x) d\u00b5 S \u2212 X 1 h(x) =h (x) d\u00b5 T \u2264 X f (x)d\u00b5 S \u2212 X f (x)d\u00b5 T + 2 .(8)\nUsing Theorem 2, for any 2 , there exists a polynomial 4 with respect to the corresponding norm\nP n (x) = n k=1 i\u2208\u2206 k \u03b1 i n j=1 (x j ) ij , such that X f (x)d\u00b5 S \u2212 X f (x)d\u00b5 T \u2264 X P n (x)d\u00b5 S \u2212 X P n (x)d\u00b5 T + 2 \u2264 n k=1 i\u2208\u2206 k a i X n j=1 (x j ) ij d\u00b5 S \u2212 i\u2208\u2206 k a i X n j=1 (x j ) ij d\u00b5 T + 2 \u2264 n k=1 i\u2208\u2206 k |a i | X n j=1 (x j ) ij d\u00b5 S \u2212 X n j=1 (x j ) ij d\u00b5 T + 2 \u2264 n k=1 a \u2206 k i\u2208\u2206 k X n j=1 (x j ) ij d\u00b5 S \u2212 X n j=1 (x j ) ij d\u00b5 T + 2 = n k=1 a \u2206 k d CM k (D S , D T ) + 2 \u2264 1 2 a n n k=1 d CM k (D S , D T ) + 2 ,(9)\nwhere a \u2206 k = max i\u2208\u2206 k |a i | and a n = 2 max 1\u2264k\u2264n a \u2206 k . Combining Equation 7, 8, 9, we prove the lemma.\n\nNote that the constants a \u2206 k can actually be meaningfully bounded when applying the Weierstrass Approximation Theorem. According to [30], a sequence of positive numbers {M k } can be constructed, such that, for any > 0, there exists a polynomial P n such that |P n (x) \u2212 f (x)| < and |a \u2206 k | < M k , \u2200k = 1, . . . , n .\n\nLemma 4 (Lemma 6, [1]). For each D j \u2208 {D 1 , . . . , D N }, let S j be a labeled sample set of size \u03b2 j m drawn from \u00b5 j and labeled by the groundtruth labeling function f j . For any fixed weight vector \u03b1, let\u02c6 \u03b1 (h) be the empirical \u03b1weighted error of some fixed hypothesis h on these sample sets, and let \u03b1 (h) be the true \u03b1-weighted error, then\nP[|\u02c6 \u03b1 (h) \u2212 \u03b1 (h)|\u2265 ] \u2264 2 exp \u22122m 2 N j=1 \u03b1 2 j \u03b2j .\nLemma 4 is a slight modification of the Hoeffdings inequality for the empirical \u03b1-weighted source error, which will be useful in proving the uniform convergence bound for hypothesis space of finite VC dimension. Now we are ready to prove Theorem 1.\n\n\nTheorem 1.\n\nLet H be a hypothesis space of V C dimension d. Let m be the size of labeled samples from all sources {D 1 , D 2 , ..., D N }, S j be the labeled sample set of size \u03b2 j m ( j \u03b2 j = 1) drawn from \u00b5 j and labeled by the groundtruth labeling function f j . If\u0125 \u2208 H is the empirical minimizer of \u03b1 (h) for a fixed weight vector \u03b1 and h * T = min h\u2208H T (h) is the target error minimizer, then for any \u03b4 \u2208 (0, 1) and any > 0, there exist N integers {n j } N j=1 and N constants {a n j } N j=1 , such that with probability at least 1 \u2212 \u03b4,\nT (\u0125) \u2264 T (h * T ) + \u03b7 \u03b1,\u03b2,m,\u03b4 + + N j=1 \u03b1 j 2\u03bb j + a n j n j k=1 d CM k (D j , D T ) ,(6)\nwhere \u03b7 \u03b1,\u03b2,m,\u03b4 = 4 ( N j=1\n\u03b1 2 j \u03b2j )( 2d(log( 2m d )+1)+2 log( 4 \u03b4 ) m ) and \u03bb j = min h\u2208H { T (h) + j (h)}. Proof. Let h * j = arg min h\u2208H { T (h) + j (h)}. Then for any > 0, there exists N integers {n j } N j=1 and N constant {a n j } N j=1 , such that | \u03b1 (h) \u2212 T (h)| \u2264 N j=1 \u03b1 j j (h) \u2212 T (h) \u2264 N j=1 \u03b1 j | j (h) \u2212 T (h)| \u2264 N j=1 \u03b1 j | j (h) \u2212 j (h, h * j )|+| j (h, h * j ) \u2212 T (h, h * j )| + | T (h, h * j ) \u2212 T (h)| \u2264 N j=1 \u03b1 j ( j (h * j ) + | j (h, h * j ) \u2212 T (h, h * j )|+ T (h * j )) \u2264 N j=1 \u03b1 j \u03bb j + 1 2 a j n n j k=1 d CM k (D j , D T ) + 2 .(10)\nThe third inequality follows from the triangle inequality of classification error 5 [2,4]. The last inequality follows from the definition of \u03bb j and Lemma 3. Now using both Equation 10 and Lemma 4, we have for any \u03b4 \u2208 (0, 1) and any > 0, with probability 1 \u2212 \u03b4,\nT (\u0125) \u2264 \u03b1 (\u0125) + 2 + N j=1 \u03b1 j \u03bb j + 1 2 a j n n j k=1 d CM k (D j , D T ) \u2264\u02c6 \u03b1 (\u0125) + 1 2 \u03b7 \u03b1,\u03b2,m,\u03b4 + 2 + N j=1 \u03b1 j \u03bb j + 1 2 a j n n j k=1 d CM k (D j , D T ) \u2264\u02c6 \u03b1 (h * T ) + 1 2 \u03b7 \u03b1,\u03b2,m,\u03b4 + 2 + N j=1 \u03b1 j \u03bb j + 1 2 a j n n j k=1 d CM k (D j , D T ) \u2264 \u03b1 (h * T ) + \u03b7 \u03b1,\u03b2,m,\u03b4 + 2 + N j=1 \u03b1 j \u03bb j + 1 2 a j n n j k=1 d CM k (D j , D T ) \u2264 T (h * T ) + \u03b7 \u03b1,\u03b2,m,\u03b4 + + N j=1 \u03b1 j 2\u03bb j + a j n n j k=1 d CM k (D j , D T ) .\nThe first and the last inequalities follow from Equation 10, the second and the fourth inequalities follow from applying Lemma 4 (instead of standard Hoeffding's inequality) in the standard proof of uniform convergence for empirical risk minimizers [42]. The third inequality follows from the definition of\u0125.\n\nTo better understand the bounds in Theorem 1, the second term of the bound is the VC-dimension based generalization error, which is the upper bound of the difference between the empirical error\u02c6 \u03b1 and the true expected error \u03b1 . The last term (a summation), as shown in Equation 10, characterizes the upper bound of the difference between the \u03b1-weighted error \u03b1 and the target error T . The constants {a n j } N j=1 in this term can be meaningfully bounded, as explained at the end of the proof of Lemma 3.\n\nNote that the bound explicitly depends on cross-moment divergence terms d CM k (D j , D T ), and thus sheds new light on the theoretical motivation of moment matching approaches, including our proposed approach and many existing approaches for both single and multiple source domain adaptation. To the best of our knowledge, this is the first target error bound in the literature of domain adaptation that explicitly incorporates a moment-based divergence between the source(s) and the target domains.\n\n\nC. Details of Digit Experiments\n\nNetwork Architecture In our digital experiments (Table 2), our feature extractor is composed of three conv layers and two fc layers. We present the conv layers as (input, output, kernel, stride, padding) and fc layers as (input, output). The three conv layers are: conv1 (3, 64, 5, 1, 2), conv2 (64, 64, 5, 1, 2), conv3 (64, 128, 5, 1, 2). The two fc layers are: fc1 (8192, 3072), fc2 (3072, 2048). The architecture of the feature generator is: (conv1, bn1, relu1, pool1)-(conv2, bn2, relu2, pool2)-(conv3, bn3, relu3)-(fc1, bn4, relu4, dropout)-(fc2, bn5, relu5). The classifier is a single fc layer, i.e. fc3 (2048, 10). Convergence Analysis As our framework involves multiple classifiers and the model is trained with multiple losses, we visualize the learning procedure of mm,mt,sv,sy\u2192up setting in Figure 7. The figure shows the training errors of multiple classifiers are decreasing, despite some frequent deviations. The MD (Moment Discrepancy) loss decreases in a steady way, demonstrating that the MD loss and the cross-entropy loss gradually converge.\n\n\nD. Feature visualization\n\nTo demonstrate the transfer ability of our model, we visualize the DAN [25] features and M 3 SDA-\u03b2 features with t-SNE embedding in two tasks: mm,mt,up,sy\u2192sv and A,D,W\u2192C. The results are shown in Figure 6. We make two important observations: i) Comparing Figure 6(a) with Figure 6(b), we find that M 3 SDA-\u03b2 is capable of learning more discriminative features; ii) From Figure 6(c) and Figure 6(d), we find that the clusters of M 3 SDA-\u03b2 features are more compact than those of DAN, which suggests that the features learned by M 3 SDA-\u03b2 attain more desirable discriminative property. These observations imply the superiority of our model over DAN in multi-source domain adaptation.\n\n\nE. Effect of Category Number\n\nIn this section, we show more results to clarify how the number of categories affects the performances of the stateof-the-art models. We choose the following four settings, i.e., painting\u2192real ( Figure 5 \u2022 All the models perform well when the number of categories is small. However, their performances drop rapidly when the number of categories increases.\n\n\u2022 Self-Ensembling [7] model has a good performance when the number of categories is small, but it is not suitable to large scale domain adaptation.  Figure 5. Accuracy vs. Number of Catogries. We plot how the performances of different models will change when the number of categories increases. We select four UDA settings, i.e., painting\u2192real, infograph\u2192real, sketch\u2192clipart, quickdraw\u2192clipart. The figure shows the performance of all models drop significantly with the increase of category number.\n\n\nF. ResNet baselines\n\nWe report ResNet [13] source only baselines in Table 7. The MCD [38] and the SE [7] methods (In Table 3) are based on ResNet101 and ResNet152, respectively. We have observed these two methods both perform worse than their source only baselines, which indicates negative transfer [31] phenomenon occurs in these two scenarios. Exploring why negative transfer happens is beyond the scope of this literature. One preliminary guess is due to the large number of categories.  Table 8. Train/Test split. We split LSDAC with a 70%/30% ratio. The \"Per-Class\" row shows the average number of images that each category contains.\n\n\nG. Train/Test Split\n\nWe show the detailed number of images we used in our experiments in Table 8. For each domain, we follow a 70%/30% schema to split the dataset to training and testing trunk. The \"Per-Class\" row shows the average number of images that each category contains. \n\n\nH. Image Samples\n\nWe sample the images for each domain and show them in Figure 8 (clipart), Figure 9 (infograph), Figure 10 Table 9, Table 10, and Table 11 show the detailed statistics of our LSDAC dataset. Our dataset contains 6 distinct domains, 345 categories and \u223c0.6 million images. The categories are from 24 divisions, which are: Furniture, Mammal, Tool, Cloth, Electricity, Building, Office, Human Body, Road Transportation, Food, Nature, Cold Blooded, Music, Fruit, Sport, Tree, Bird, Vegetable, Shape, Kitchen, Water Transportation, Sky Transportation, Insect, Others.      \n\n\nI. Dataset Statistics\n\nFigure 1 .\n1We address Multi-Source Domain Adaptation where source images come from multiple domains. We collect a large scale dataset with six domains, 345 categories, and \u223c0.6 million images. And we propose a model M 3 SDA to transfer knowledge from multiple source domains to an unlabeled target domain.\n\nFigure 2 .\n2Statistics for our LSDAC dataset. The two plots show object classes sorted by the total number of instances. The top figure shows the percentages each domain takes in the dataset. The bottom figure shows the number of instances grouped by 24 different divisions.\n\nFigure 3 .\n3The framework of Moment Matching for Multi-source Domain Adaptation (M 3 SDA). Our model consists of three components: i) feature extractor, ii) moment matching component, and iii) classifiers. Our model takes multi-source annotated training data as input and transfers the learned knowledge to classify the unlabeled target samples. Without loss of generality, we show the i-th domain and j-th domain as an example. The feature extractor maps the source domains into a common feature space. The moment matching component attempts to match the i-th and j-th domains with the target domain, as well as matching the i-th domain with the j-th domain. The final predictions of target samples are based on the weighted outputs of the i-th and j-th classifiers. (Best viewed in color!)\n\nM 3 SDA\n3We propose a moment-matching model for MSDA based on deep neural networks. As shown in Figure 3, our model comprises of a feature extractor G, a momentmatching component, and a set of N classifiers C = {C 1 , C 2 , ..., C N }. The feature extractor G maps D S , D T to a common latent feature space. The moment matching component minimizes the moment-related distance defined in Equation 1. The N classifiers are trained on the annotated source domains with cross-entropy loss. The overall objective function is:\n\nFive\ndigit datasets are sampled from five different sources, namely MNIST[19], Synthetic Digits[8], MNIST-M[8], SVHN, and USPS. Following DCTN[45], we sample 25000 images from training subset and 9000 from testing subset in MNIST, MINST-M, SVHN, and Synthetic Digits. USPS dataset contains only 9298 images in total, so we take the entire dataset as a domain. In all of our experiments, we take turns to set one domain as the target domain and the rest as the source domains.3 http://pytorch.org We take four state-of-the-art discrepancy-based approaches: Deep Adaptation Network [25] (DAN), Joint Adaptation Network (JAN), Manifold Embedded Distribution Alignment (MEDA), and Correlation Alignment [39] (CORAL), and four adversarial-based approaches: Domain Adversarial Neural Network [8] (DANN), Adversarial Discriminative Domain Adaptation [40] (ADDA), Maximum Classifier Discrepancy (MCD) and Deep Cocktail Network [45] (DCTN) as our baselines. In the source combine setting, all the source domains are combined to a single domain, and the baseline experiments are conducted in a traditional single domain adaptation manner.The results are shown inTable 2. Our model M 3 SDA achieves an 86.13% average accuracy, and M 3 SDA-\u03b2 boosts the performance to 87.65%, outperforming other baselines by a large margin. For a fair comparison, all the experiments are based on the same network architecture. For each experiment, we run the same setting for five times and report the mean and standard deviation. (See Appendix for detailed experiment settings and analyses.)\n\nFigure 4 .\n4Accuracy vs. Number of categories. This plot shows the painting\u2192real scenario. More plots with similar trend can be accessed inFigure 5(see Appendix).\n\n\n(a)), infograph\u2192real (Figure 5(b)), sketch\u2192clipart (Figure 5(c)), quickdraw\u2192clipart (Figure 5(d)), and gradually increase the number of categories from 20 to 345. From the four figures, we make the following interesting observations:\n\nFigure 6 .Figure 7 .\n67Feature visualization: t-SNE plot of DAN features and M 3 SDA-\u03b2 features on SVHN in mm,mt,up,sy\u2192sv setting; t-SNE of DAN features and M 3 SDA-\u03b2 features on Caltech in A,D,W\u2192C setting. We use different markers and different colors to denote different categories. (Best viewed in color.) Analysis: training error of each classifier, Moment Discrepancy (MD, defined in Equation 1), and accuracy (red bold line) w.r.t. training epochs in mm,mt,sv,sy\u2192up setting. The bold red line shows how the accuracy changes w.r.t. training epochs.\n\n\n(painting), Figure 11 (quickdraw), Figure 12 (real), and Figure 13 (sketch).\n\nFigure 8 .\n8Images sampled from clipart domain of the LSDAC dataset.\n\nFigure 9 .\n9Images sampled from infograph domain of the LSDAC dataset.\n\nFigure 10 .\n10Images sampled from painting domain of the LSDAC dataset.\n\nFigure 11 .\n11Images sampled from quickdraw domain of the LSDAC dataset.\n\nFigure 12 .\n12Images sampled from real domain of the LSDAC dataset.\n\nFigure 13 .\n13Images sampled from sketch domain of the LSDAC dataset.\n\n\nAlexNet clp inf pnt qdr rel skt Avg. DAN clp inf pnt qdr rel skt Avg. JAN clp inf pnt qdr rel skt Avg. DANN clp inf pnt qdr rel skt Avg.28.1 15.1 17.8 Avg. 27.6 8.0 21.0 10.5 29.9 21.2 19.7 Avg. 26.3 7.1 22.2 10.2 30.8 19.5 19.4 Avg. 27.6 7.8 20.9 7.8 30.3 20.3 19.1 RTN clp inf pnt qdr rel skt Avg. ADDA clp inf pnt qdr rel skt Avg. MCD clp inf pnt qdr rel skt Avg. SE clp inf pnt qdr rel skt Avg.clp 65.5 8.2 21.4 10.5 36.1 10.8 17.4 \nclp N/A 9.1 23.4 16.2 37.9 29.7 23.2 clp N/A 7.8 24.5 14.3 38.1 25.7 22.1 \nclp N/A 9.1 23.2 13.7 37.6 28.6 22.4 \ninf 32.9 27.7 23.8 2.2 26.4 13.7 19.8 \ninf 17.2 N/A 15.6 4.4 24.8 13.5 15.1 inf 17.6 N/A 18.7 8.7 28.1 15.3 17.7 \ninf 17.9 N/A 16.4 2.1 27.8 13.3 15.5 \npnt 28.1 7.5 57.6 2.6 41.6 20.8 20.1 \npnt 29.9 8.9 N/A 7.9 42.1 26.1 23.0 pnt 27.5 8.2 N/A 7.1 43.1 23.9 22.0 \npnt 29.1 8.6 N/A 5.1 41.5 24.7 21.8 \nqdr 13.4 1.2 2.5 68.0 5.5 7.1 5.9 \nqdr 14.2 1.6 4.4 N/A 8.5 10.1 7.8 qdr 17.8 2.2 7.4 N/A 8.1 10.9 9.3 \nqdr 16.8 1.8 4.8 N/A 9.3 10.2 8.6 \nrel 36.9 10.2 33.9 4.9 72.8 23.1 21.8 \nrel 37.4 11.5 33.3 10.1 N/A 26.4 23.7 rel 33.5 9.1 32.5 7.5 N/A 21.9 20.9 \nrel 36.5 11.4 33.9 5.9 N/A 24.5 22.4 \nskt 35.5 7.1 21.9 11.8 30.8 56.3 21.4 \nskt 39.1 8.8 28.2 13.9 36.2 N/A 25.2 skt 35.3 8.2 27.7 13.3 36.8 N/A 24.3 \nskt 37.9 8.2 26.3 12.2 35.3 N/A 24.0 \nAvg. 29.4 6.8 20.7 6.4 clp N/A 8.1 21.1 13.1 36.1 26.5 21.0 \nclp N/A 11.2 24.1 3.2 41.9 30.7 22.2 clp N/A 14.2 26.1 1.6 45.0 33.8 24.1 \nclp N/A 9.7 12.2 2.2 33.4 23.1 16.1 \ninf 15.6 N/A 15.3 3.4 25.1 12.8 14.4 \ninf 19.1 N/A 16.4 3.2 26.9 14.6 16.0 inf 23.6 N/A 21.2 1.5 36.7 18.0 20.2 \ninf 10.3 N/A 9.6 1.2 13.1 6.9 8.2 \npnt 26.8 8.1 N/A 5.2 40.6 22.6 20.7 \npnt 31.2 9.5 N/A 8.4 39.1 25.4 22.7 pnt 34.4 14.8 N/A 1.9 50.5 28.4 26.0 \npnt 17.1 9.4 N/A 2.1 28.4 15.9 14.6 \nqdr 15.1 1.8 4.5 N/A 8.5 8.9 7.8 \nqdr 15.7 2.6 5.4 N/A 9.9 11.9 9.1 qdr 15.0 3.0 7.0 N/A 11.5 10.2 9.3 \nqdr 13.6 3.9 11.6 N/A 16.4 11.5 11.4 \nrel 35.3 10.7 31.7 7.5 N/A 22.9 21.6 \nrel 39.5 14.5 29.1 12.1 N/A 25.7 24.2 rel 42.6 19.6 42.6 2.2 N/A 29.3 27.2 \nrel 31.7 12.9 19.9 3.7 N/A 26.3 18.9 \nskt 34.1 7.4 23.3 12.6 32.1 N/A 21.9 \nskt 35.3 8.9 25.2 14.9 37.6 N/A 25.4 skt 41.2 13.7 27.6 3.8 34.8 N/A 24.2 \nskt 18.7 7.8 12.2 7.7 28.9 N/A 15.1 \nAvg. 25.4 7.2 19.2 8.4 28.4 18.7 17.9 Avg. 28.2 9.3 20.1 8.4 31.1 21.7 19.8 Avg. 31.4 13.1 24.9 2.2 35.7 23.9 21.9 Avg. 18.3 8.7 13.1 3.4 24.1 16.7 14.1 \n\n\n\nTable 4 .\n4Results on Office-Caltech10 dataset. A,C,W and D represent Amazon, Caltech, Webcam and DSLR, respectively. All the experiments are based on ResNet-101 pre-trained on ImageNet.\n\n\nResNet101 clp inf pnt qdr rel skt Avg. ResNet152 clp inf pnt qdr rel skt Avg.Table 7. Single-source ResNet101 and ResNet152[13] baselines on the LSDAC dataset. We provide ResNet baselines forTable 3. In each sub-table, the column-wise domains are selected as the source domain and the row-wise domains are selected as the target domain. The green numbers represent the average performance of each column or row. The red numbers denote the average accuracy for all the 30 (source, target) combinations. (clp: clipart, inf: infograph, pnt: painting, qdr: quickdraw, rel: real, skt: sketch.)clp \nN/A 19.3 37.5 11.1 52.2 41.0 32.2 \nclp \nN/A 19.8 37.9 12.2 52.3 44.8 33.4 \ninf \n30.2 N/A 31.2 3.6 44.0 27.9 27.4 \ninf \n31.3 N/A 31.1 4.7 45.5 29.6 28.4 \npnt \n39.6 18.7 N/A 4.9 54.5 36.3 30.8 \npnt \n42.0 19.5 N/A 7.4 55.0 37.7 32.3 \nqdr \n7.0 0.9 1.4 N/A 4.1 8.3 4.3 \nqdr \n12.2 1.8 2.9 N/A 6.3 9.4 6.5 \nrel \n48.4 22.2 49.4 6.4 N/A 38.8 33.0 \nrel \n50.5 24.4 49.0 6.2 N/A 39.9 34.0 \nskt \n46.9 15.4 37.0 10.9 47.0 N/A 31.4 \nskt \n51.0 18.2 39.7 12.5 47.4 N/A 33.8 \nAvg. \n34.4 15.3 31.3 7.4 40.4 30.5 26.5 \nAvg. \n37.4 16.7 32.1 8.6 41.3 32.3 28.1 \n\n20 \n50 \n100 \n150 \n200 \n250 \n300 \n345 \n\nNumber of Categories \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\nAccuracy \n\nSE \nMCD \nDAN \nJAN \nRTN \nDANN \nADDA \nAlexNet \n\n(a) painting\u2192real \n\n20 50 \n100 \n150 \n200 \n250 \n300 \n345 \n\nNumber of Categories \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\nAccuracy \n\nSE \nMCD \nDAN \nJAN \nRTN \nDANN \nADDA \nAlexNet \n\n(b) infograph\u2192real \n\n20 \n50 \n100 \n150 \n200 \n250 \n300 \n345 \n\nNumber of Categories \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\nAccuracy \n\nSE \nMCD \nDAN \nJAN \nRTN \nDANN \nADDA \nAlexNet \n\n(c) sketch\u2192clipart \n\n20 50 \n100 \n150 \n200 \n250 \n300 \n345 \n\nNumber of Categories \n\n0.2 \n\n0.4 \n\n0.6 \n\nAccuracy \n\nSE \nMCD \nDAN \nJAN \nRTN \nDANN \nADDA \nAlexNet \n\n(d) quickdraw\u2192clipart \n\n\nhttps://quickdraw.withgoogle.com/data\nNote that single source is just a special case when N = 1.\nFor any labeling functionf 1 , f 2 , f 3 , we have (f 1 , f 2 ) \u2264 (f 1 , f 3 )+ (f 2 , f 3 ).\nAcknowledgementWe thank Ruiqi Gao, Yizhe Zhu, Saito Kuniaki, Ben Usman, Ping Hu for their useful discussions and suggestions. We thank anonymous annotators for their hard work to label the data. This work was partially supported by NSF and Honda Research Institute.Table 11. Detailed statistics of the LSDAC dataset.\nA theory of learning from different domains. S Ben-David, J Blitzer, K Crammer, A Kulesza, F Pereira, J W Vaughan, Machine learning. 791-212S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different do- mains. Machine learning, 79(1-2):151-175, 2010. 1, 2, 3, 5, 12\n\nAnalysis of representations for domain adaptation. S Ben-David, J Blitzer, K Crammer, F Pereira, Advances in neural information processing systems. 12S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al. Anal- ysis of representations for domain adaptation. Advances in neural information processing systems, pages 137-144, 2007. 12\n\nDomain separation networks. K Bousmalis, G Trigeorgis, N Silberman, D Krishnan, D Erhan, Advances in Neural Information Processing Systems. K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan. Domain separation networks. In Advances in Neu- ral Information Processing Systems, pages 343-351, 2016. 2\n\nLearning from multiple sources. K Crammer, M Kearns, J Wortman, Journal of Machine Learning Research. 912K. Crammer, M. Kearns, and J. Wortman. Learning from multiple sources. Journal of Machine Learning Research, 9(Aug):1757-1774, 2008. 1, 3, 12\n\nA practical guide to splines. C De Boor, C De, E.-U Boor, C De Math\u00e9maticien, C. De Boor, Boor, Springer-Verlag27New YorkC. De Boor, C. De Boor, E.-U. Math\u00e9maticien, C. De Boor, and C. De Boor. A practical guide to splines, volume 27. Springer-Verlag New York, 1978. 4\n\nExploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach. L Duan, D Xu, S.-F Chang, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE13L. Duan, D. Xu, and S.-F. Chang. Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach. In Computer Vision and Pat- tern Recognition (CVPR), 2012 IEEE Conference on, pages 1338-1345. IEEE, 2012. 1, 3\n\nSelf-ensembling for visual domain adaptation. G French, M Mackiewicz, M Fisher, International Conference on Learning Representations. 1314G. French, M. Mackiewicz, and M. Fisher. Self-ensembling for visual domain adaptation. In International Conference on Learning Representations, 2018. 2, 7, 8, 13, 14\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, Proceedings of the 32nd International Conference on Machine Learning. F. Bach and D. Bleithe 32nd International Conference on Machine LearningLille, France37PMLR. 1, 2, 6Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In F. Bach and D. Blei, editors, Pro- ceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1180-1189, Lille, France, 07-09 Jul 2015. PMLR. 1, 2, 6, 7, 8\n\nDomain adaptive neural networks for object recognition. M Ghifary, W B Kleijn, M Zhang, Pacific Rim international conference on artificial intelligence. SpringerM. Ghifary, W. B. Kleijn, and M. Zhang. Domain adaptive neural networks for object recognition. In Pacific Rim in- ternational conference on artificial intelligence, pages 898- 904. Springer, 2014. 2\n\nDeep reconstruction-classification networks for unsupervised domain adaptation. M Ghifary, W B Kleijn, M Zhang, D Balduzzi, W Li, European Conference on Computer Vision. SpringerM. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and W. Li. Deep reconstruction-classification networks for un- supervised domain adaptation. In European Conference on Computer Vision, pages 597-613. Springer, 2016. 2\n\nGeodesic flow kernel for unsupervised domain adaptation. B Gong, Y Shi, F Sha, K Grauman, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. 26B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow ker- nel for unsupervised domain adaptation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2066-2073. IEEE, 2012. 2, 6\n\nA kernel method for the two-sample-problem. A Gretton, K M Borgwardt, M Rasch, B Sch\u00f6lkopf, A J Smola, Advances in neural information processing systems. A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch\u00f6lkopf, and A. J. Smola. A kernel method for the two-sample-problem. In Advances in neural information processing systems, pages 513-520, 2007. 2\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition714K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 770-778, 2016. 7, 14\n\nAlgorithms and theory for multiple-source adaptation. J Hoffman, M Mohri, N Zhang, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31J. Hoffman, M. Mohri, and N. Zhang. Algorithms and the- ory for multiple-source adaptation. In S. Bengio, H. Wal- lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Process- ing Systems 31, pages 8246-8256. Curran Associates, Inc., 2018. 1, 2, 3, 5\n\nCyCADA: Cycle-consistent adversarial domain adaptation. J Hoffman, E Tzeng, T Park, J.-Y Zhu, P Isola, K Saenko, A Efros, T Darrell, PMLR. 1Proceedings of the 35th International Conference on Machine Learning. J. Dy and A. Krausethe 35th International Conference on Machine LearningStockholmsmssan, Stockholm Sweden803J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros, and T. Darrell. CyCADA: Cycle-consistent ad- versarial domain adaptation. In J. Dy and A. Krause, ed- itors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1989-1998, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. 1, 3\n\nLearning to discover cross-domain relations with generative adversarial networks. T Kim, M Cha, H Kim, J K Lee, J Kim, PMLR. 3Proceedings of the 34th International Conference on Machine Learning. D. Precup and Y. W. Tehthe 34th International Conference on Machine LearningSydney, Australia70International Convention CentreT. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to discover cross-domain relations with generative adversarial networks. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1857-1865, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. 3\n\nMuseum exhibit identification challenge for the supervised domain adaptation and beyond. P Koniusz, Y Tas, H Zhang, M Harandi, F Porikli, R Zhang, The European Conference on Computer Vision (ECCV). P. Koniusz, Y. Tas, H. Zhang, M. Harandi, F. Porikli, and R. Zhang. Museum exhibit identification challenge for the supervised domain adaptation and beyond. In The European Conference on Computer Vision (ECCV), September 2018. 2\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012. 7\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 866Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998. 2, 6\n\nMmd gan: Towards deeper understanding of moment matching network. C.-L Li, W.-C Chang, Y Cheng, Y Yang, B P\u00f3czos, Advances in Neural Information Processing Systems. C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. P\u00f3czos. Mmd gan: Towards deeper understanding of moment match- ing network. In Advances in Neural Information Processing Systems, pages 2203-2213, 2017. 3\n\nDeeper, broader and artier domain generalization. D Li, Y Yang, Y.-Z Song, T Hospedales, International Conference on Computer Vision. D. Li, Y. Yang, Y.-Z. Song, and T. Hospedales. Deeper, broader and artier domain generalization. In International Conference on Computer Vision, 2017. 2\n\nGenerative moment matching networks. Y Li, K Swersky, R Zemel, International Conference on Machine Learning. Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In International Conference on Machine Learning, pages 1718-1727, 2015. 3\n\nUnsupervised image-toimage translation networks. M.-Y Liu, T Breuel, J Kautz, Advances in Neural Information Processing Systems. M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to- image translation networks. In Advances in Neural Informa- tion Processing Systems, pages 700-708, 2017. 1\n\nCoupled generative adversarial networks. M.-Y Liu, O Tuzel, Advances in neural information processing systems. M.-Y. Liu and O. Tuzel. Coupled generative adversarial net- works. In Advances in neural information processing sys- tems, pages 469-477, 2016. 2\n\nLearning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, M Jordan, Proceedings of the 32nd International Conference on Machine Learning. F. Bach and D. Bleithe 32nd International Conference on Machine LearningLille, France3713PMLR. 1, 2, 6, 7, 8M. Long, Y. Cao, J. Wang, and M. Jordan. Learning trans- ferable features with deep adaptation networks. In F. Bach and D. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 97-105, Lille, France, 07-09 Jul 2015. PMLR. 1, 2, 6, 7, 8, 13\n\nUnsupervised domain adaptation with residual transfer networks. M Long, H Zhu, J Wang, M I Jordan, Advances in Neural Information Processing Systems. 7M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsuper- vised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pages 136-144, 2016. 7, 8\n\nDeep transfer learning with joint adaptation networks. M Long, H Zhu, J Wang, M I Jordan, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, Australia7M. Long, H. Zhu, J. Wang, and M. I. Jordan. Deep trans- fer learning with joint adaptation networks. In Proceed- ings of the 34th International Conference on Machine Learn- ing, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 2208-2217, 2017. 1, 2, 6, 7, 8\n\nDomain adaptation with multiple sources. Y Mansour, M Mohri, A Rostamizadeh, A R , Advances in Neural Information Processing Systems 21. D. Koller, D. Schuurmans, Y. Bengio, and L. BottouCurran Associates, Inc13Y. Mansour, M. Mohri, A. Rostamizadeh, and A. R. Domain adaptation with multiple sources. In D. Koller, D. Schuur- mans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1041-1048. Cur- ran Associates, Inc., 2009. 1, 3\n\nMean and covariance feature matching GAN. Y Mroueh, T Sercu, V Goel, Mcgan, PMLR. 3Proceedings of the 34th International Conference on Machine Learning. D. Precup and Y. W. Tehthe 34th International Conference on Machine LearningSydney, Australia70International Convention CentreY. Mroueh, T. Sercu, and V. Goel. McGan: Mean and covari- ance feature matching GAN. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2527-2535, International Conven- tion Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. 3\n\nAbsolute values of the coefficients of the polynomials in weierstrass's approximation theorem. O Muradyan, S Y Khavinson, Mathematical notes of the Academy of Sciences of the USSR. 22212O. Muradyan and S. Y. Khavinson. Absolute values of the coefficients of the polynomials in weierstrass's approxima- tion theorem. Mathematical notes of the Academy of Sci- ences of the USSR, 22(2):641-645, 1977. 12\n\nA survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on knowledge and data engineering. 221014S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345-1359, 2010. 8, 14\n\nSynthetic to real adaptation with generative correlation alignment networks. X Peng, K Saenko, 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018. Lake Tahoe, NV, USA1X. Peng and K. Saenko. Synthetic to real adaptation with generative correlation alignment networks. In 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018, Lake Tahoe, NV, USA, March 12-15, 2018, pages 1982-1991, 2018. 1, 2\n\nLearning deep object detectors from 3d models. X Peng, B Sun, K Ali, K Saenko, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionX. Peng, B. Sun, K. Ali, and K. Saenko. Learning deep ob- ject detectors from 3d models. In Proceedings of the IEEE International Conference on Computer Vision, pages 1278- 1286, 2015. 2\n\nX Peng, B Usman, N Kaushik, J Hoffman, D Wang, K Saenko, arXiv:1710.06924Visda: The visual domain adaptation challenge. arXiv preprintX. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017. 2\n\nSyn2real: A new benchmark forsynthetic-to-real visual domain adaptation. X Peng, B Usman, K Saito, N Kaushik, J Hoffman, K Saenko, abs/1806.09755CoRRX. Peng, B. Usman, K. Saito, N. Kaushik, J. Hoffman, and K. Saenko. Syn2real: A new benchmark forsynthetic-to-real visual domain adaptation. CoRR, abs/1806.09755, 2018. 2\n\nDataset Shift in Machine Learning. J Quionero-Candela, M Sugiyama, A Schwaighofer, N D Lawrence, The MIT PressJ. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. 1\n\nAdapting visual category models to new domains. K Saenko, B Kulis, M Fritz, T Darrell, European conference on computer vision. Springer26K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi- sual category models to new domains. In European confer- ence on computer vision, pages 213-226. Springer, 2010. 2, 6\n\nMaximum classifier discrepancy for unsupervised domain adaptation. K Saito, K Watanabe, Y Ushiku, T Harada, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 814K. Saito, K. Watanabe, Y. Ushiku, and T. Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 1, 4, 5, 6, 7, 8, 14\n\nReturn of frustratingly easy domain adaptation. B Sun, J Feng, K Saenko, AAAI. 66B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In AAAI, volume 6, page 8, 2016. 1, 2, 3, 6\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, Computer Vision and Pattern Recognition (CVPR). 1E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. In Computer Vision and Pattern Recognition (CVPR), volume 1, page 4, 2017. 1, 2, 6, 7, 8\n\nE Tzeng, J Hoffman, N Zhang, K Saenko, T Darrell, arXiv:1412.3474Deep domain confusion: Maximizing for domain invariance. 1arXiv preprintE. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. 1, 2\n\nOn the uniform convergence of relative frequencies of events to their probabilities. V N Vapnik, A Y Chervonenkis, Measures of complexity. SpringerV. N. Vapnik and A. Y. Chervonenkis. On the uniform con- vergence of relative frequencies of events to their probabil- ities. In Measures of complexity, pages 11-30. Springer, 2015. 13\n\nDeep hashing network for unsupervised domain adaptation. H Venkateswara, J Eusebio, S Chakraborty, S Panchanathan, (IEEE) Conference on Computer Vision and Pattern Recognition. H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Pan- chanathan. Deep hashing network for unsupervised domain adaptation. In (IEEE) Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2\n\nVisual domain adaptation with manifold embedded distribution alignment. J Wang, W Feng, Y Chen, H Yu, M Huang, P S Yu, ACM Multimedia Conference. 67J. Wang, W. Feng, Y. Chen, H. Yu, M. Huang, and P. S. Yu. Visual domain adaptation with manifold embedded distribu- tion alignment. In ACM Multimedia Conference, 2018. 2, 6, 7\n\nDeep cocktail network: Multi-source unsupervised domain adaptation with category shift. R Xu, Z Chen, W Zuo, J Yan, L Lin, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition7R. Xu, Z. Chen, W. Zuo, J. Yan, and L. Lin. Deep cock- tail network: Multi-source unsupervised domain adaptation with category shift. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3964- 3973, 2018. 1, 2, 3, 6, 7, 8\n\nDualgan: Unsupervised dual learning for image-to-image translation. Z Yi, H R Zhang, P Tan, M Gong, ICCV. Z. Yi, H. R. Zhang, P. Tan, and M. Gong. Dualgan: Unsuper- vised dual learning for image-to-image translation. In ICCV, pages 2868-2876, 2017. 3\n\nCentral moment discrepancy (CMD) for domain-invariant representation learning. W Zellinger, T Grubinger, E Lughofer, T Natschl\u00e4ger, S Saminger-Platz, abs/1702.08811CoRR13W. Zellinger, T. Grubinger, E. Lughofer, T. Natschl\u00e4ger, and S. Saminger-Platz. Central moment discrepancy (CMD) for domain-invariant representation learning. CoRR, abs/1702.08811, 2017. 1, 3\n\nAligning infinite-dimensional covariance matrices in reproducing kernel hilbert spaces for domain adaptation. Z Zhang, M Wang, Y Huang, A Nehorai, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZ. Zhang, M. Wang, Y. Huang, and A. Nehorai. Aligning infinite-dimensional covariance matrices in reproducing ker- nel hilbert spaces for domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3437-3445, 2018. 3\n\nAdversarial multiple source domain adaptation. H Zhao, S Zhang, G Wu, J M Moura, J P Costeira, G J Gordon, Advances in Neural Information Processing Systems. 15H. Zhao, S. Zhang, G. Wu, J. M. Moura, J. P. Costeira, and G. J. Gordon. Adversarial multiple source domain adapta- tion. In Advances in Neural Information Processing Systems, pages 8568-8579, 2018. 1, 2, 5\n\nUnpaired imageto-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, Computer Vision (ICCV. 13J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image- to-image translation using cycle-consistent adversarial net- works. In Computer Vision (ICCV), 2017 IEEE International Conference on, 2017. 1, 3\n\nSupervised representation learning: Transfer learning with deep autoencoders. F Zhuang, X Cheng, P Luo, S J Pan, Q He, IJCAI. F. Zhuang, X. Cheng, P. Luo, S. J. Pan, and Q. He. Super- vised representation learning: Transfer learning with deep autoencoders. In IJCAI, pages 4119-4125, 2015. 2\n", "annotations": {"author": "[{\"end\":132,\"start\":54},{\"end\":209,\"start\":133},{\"end\":285,\"start\":210},{\"end\":364,\"start\":286},{\"end\":442,\"start\":365},{\"end\":524,\"start\":443}]", "publisher": null, "author_last_name": "[{\"end\":67,\"start\":63},{\"end\":143,\"start\":140},{\"end\":218,\"start\":215},{\"end\":297,\"start\":292},{\"end\":376,\"start\":370},{\"end\":450,\"start\":446}]", "author_first_name": "[{\"end\":62,\"start\":54},{\"end\":139,\"start\":133},{\"end\":214,\"start\":210},{\"end\":291,\"start\":286},{\"end\":369,\"start\":365},{\"end\":445,\"start\":443}]", "author_affiliation": "[{\"end\":131,\"start\":82},{\"end\":208,\"start\":159},{\"end\":284,\"start\":235},{\"end\":363,\"start\":324},{\"end\":441,\"start\":392},{\"end\":523,\"start\":478}]", "title": "[{\"end\":51,\"start\":1},{\"end\":575,\"start\":525}]", "venue": null, "abstract": "[{\"end\":1768,\"start\":577}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2054,\"start\":2050},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2199,\"start\":2195},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2202,\"start\":2199},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2205,\"start\":2202},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2443,\"start\":2439},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2446,\"start\":2443},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2468,\"start\":2464},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2503,\"start\":2499},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2505,\"start\":2503},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2508,\"start\":2505},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2537,\"start\":2533},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2540,\"start\":2537},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2543,\"start\":2540},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3423,\"start\":3420},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3624,\"start\":3621},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3627,\"start\":3624},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3629,\"start\":3627},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3632,\"start\":3629},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3635,\"start\":3632},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3716,\"start\":3713},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3865,\"start\":3862},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3868,\"start\":3865},{\"end\":3975,\"start\":3965},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3987,\"start\":3983},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4001,\"start\":3999},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4030,\"start\":4026},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4069,\"start\":4065},{\"end\":4086,\"start\":4082},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4119,\"start\":4115},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4159,\"start\":4155},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4202,\"start\":4198},{\"end\":4214,\"start\":4203},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4237,\"start\":4233},{\"end\":4255,\"start\":4251},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4487,\"start\":4483},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4500,\"start\":4497},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4527,\"start\":4524},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5436,\"start\":5432},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5465,\"start\":5462},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5550,\"start\":5546},{\"end\":6068,\"start\":6058},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6490,\"start\":6486},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6794,\"start\":6791},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6797,\"start\":6794},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6800,\"start\":6797},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7377,\"start\":7373},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7738,\"start\":7734},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7796,\"start\":7792},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7799,\"start\":7796},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7802,\"start\":7799},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7805,\"start\":7802},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7808,\"start\":7805},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7895,\"start\":7891},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7898,\"start\":7895},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7901,\"start\":7898},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8665,\"start\":8661},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8761,\"start\":8757},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8764,\"start\":8761},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8766,\"start\":8764},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8769,\"start\":8766},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8833,\"start\":8829},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8836,\"start\":8833},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8875,\"start\":8871},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8897,\"start\":8894},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8957,\"start\":8953},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8960,\"start\":8957},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9480,\"start\":9477},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9483,\"start\":9480},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11400,\"start\":11396},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11416,\"start\":11412},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11432,\"start\":11428},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11450,\"start\":11446},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11925,\"start\":11922},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11928,\"start\":11925},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11930,\"start\":11928},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11973,\"start\":11969},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11975,\"start\":11973},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11996,\"start\":11993},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12112,\"start\":12109},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12250,\"start\":12246},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12411,\"start\":12407},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12552,\"start\":12548},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12728,\"start\":12725},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13440,\"start\":13436},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13510,\"start\":13506},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13597,\"start\":13593},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13794,\"start\":13790},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13878,\"start\":13874},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13895,\"start\":13891},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16233,\"start\":16230},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17559,\"start\":17555},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18106,\"start\":18102},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19509,\"start\":19506},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20688,\"start\":20685},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20691,\"start\":20688},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20694,\"start\":20691},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21142,\"start\":21139},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22255,\"start\":22254},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23673,\"start\":23669},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23725,\"start\":23721},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24446,\"start\":24442},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24456,\"start\":24452},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24466,\"start\":24462},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24476,\"start\":24473},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24486,\"start\":24482},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24497,\"start\":24493},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24507,\"start\":24503},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24515,\"start\":24512},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25031,\"start\":25027},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25068,\"start\":25064},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25114,\"start\":25111},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25152,\"start\":25148},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25200,\"start\":25196},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25243,\"start\":25239},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25273,\"start\":25270},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25374,\"start\":25370},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25942,\"start\":25938},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25979,\"start\":25975},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26025,\"start\":26022},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26063,\"start\":26059},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26111,\"start\":26107},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26154,\"start\":26150},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26184,\"start\":26181},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26262,\"start\":26258},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26796,\"start\":26793},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26885,\"start\":26881},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26966,\"start\":26962},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27038,\"start\":27034},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27048,\"start\":27044},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27058,\"start\":27054},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27068,\"start\":27065},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27767,\"start\":27764},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31862,\"start\":31861},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32573,\"start\":32569},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32780,\"start\":32777},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34701,\"start\":34698},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34703,\"start\":34701},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35546,\"start\":35542},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37813,\"start\":37809},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":38830,\"start\":38827},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":39353,\"start\":39349},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":39400,\"start\":39396},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":39415,\"start\":39412},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":39615,\"start\":39611},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":42821,\"start\":42817},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":42842,\"start\":42839},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":42854,\"start\":42851},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":42890,\"start\":42886},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":45913,\"start\":45911},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":48454,\"start\":48450}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41150,\"start\":40843},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41426,\"start\":41151},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42219,\"start\":41427},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42742,\"start\":42220},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44309,\"start\":42743},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44473,\"start\":44310},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44709,\"start\":44474},{\"attributes\":{\"id\":\"fig_8\"},\"end\":45264,\"start\":44710},{\"attributes\":{\"id\":\"fig_9\"},\"end\":45343,\"start\":45265},{\"attributes\":{\"id\":\"fig_10\"},\"end\":45413,\"start\":45344},{\"attributes\":{\"id\":\"fig_11\"},\"end\":45485,\"start\":45414},{\"attributes\":{\"id\":\"fig_12\"},\"end\":45558,\"start\":45486},{\"attributes\":{\"id\":\"fig_13\"},\"end\":45632,\"start\":45559},{\"attributes\":{\"id\":\"fig_14\"},\"end\":45701,\"start\":45633},{\"attributes\":{\"id\":\"fig_15\"},\"end\":45772,\"start\":45702},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":48136,\"start\":45773},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":48324,\"start\":48137},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":50111,\"start\":48325}]", "paragraph": "[{\"end\":2055,\"start\":1784},{\"end\":3569,\"start\":2057},{\"end\":3892,\"start\":3571},{\"end\":4101,\"start\":3944},{\"end\":4779,\"start\":4103},{\"end\":5559,\"start\":4781},{\"end\":6220,\"start\":5561},{\"end\":6740,\"start\":6222},{\"end\":7019,\"start\":6742},{\"end\":7211,\"start\":7036},{\"end\":8290,\"start\":7213},{\"end\":11714,\"start\":8292},{\"end\":13123,\"start\":11716},{\"end\":13782,\"start\":13143},{\"end\":14167,\"start\":13784},{\"end\":16039,\"start\":14189},{\"end\":16419,\"start\":16041},{\"end\":16803,\"start\":16459},{\"end\":17447,\"start\":17159},{\"end\":17560,\"start\":17449},{\"end\":17650,\"start\":17562},{\"end\":18233,\"start\":17700},{\"end\":18534,\"start\":18289},{\"end\":18872,\"start\":18577},{\"end\":18925,\"start\":18874},{\"end\":19107,\"start\":19009},{\"end\":19442,\"start\":19109},{\"end\":19723,\"start\":19496},{\"end\":19894,\"start\":19779},{\"end\":20502,\"start\":19940},{\"end\":20655,\"start\":20568},{\"end\":21313,\"start\":20657},{\"end\":21856,\"start\":21373},{\"end\":21975,\"start\":21948},{\"end\":22516,\"start\":22059},{\"end\":22792,\"start\":22539},{\"end\":23052,\"start\":22858},{\"end\":23577,\"start\":23068},{\"end\":23840,\"start\":23648},{\"end\":24820,\"start\":23842},{\"end\":25288,\"start\":24845},{\"end\":26279,\"start\":25290},{\"end\":27207,\"start\":26281},{\"end\":27889,\"start\":27237},{\"end\":28532,\"start\":27908},{\"end\":28852,\"start\":28547},{\"end\":29359,\"start\":28854},{\"end\":29987,\"start\":29372},{\"end\":30185,\"start\":29989},{\"end\":30480,\"start\":30328},{\"end\":30728,\"start\":30506},{\"end\":30785,\"start\":30730},{\"end\":31025,\"start\":30919},{\"end\":31096,\"start\":31090},{\"end\":31713,\"start\":31284},{\"end\":31901,\"start\":31806},{\"end\":32434,\"start\":32326},{\"end\":32757,\"start\":32436},{\"end\":33108,\"start\":32759},{\"end\":33411,\"start\":33163},{\"end\":33957,\"start\":33426},{\"end\":34076,\"start\":34049},{\"end\":34876,\"start\":34614},{\"end\":35601,\"start\":35293},{\"end\":36109,\"start\":35603},{\"end\":36612,\"start\":36111},{\"end\":37709,\"start\":36648},{\"end\":38419,\"start\":37738},{\"end\":38807,\"start\":38452},{\"end\":39308,\"start\":38809},{\"end\":39950,\"start\":39332},{\"end\":40231,\"start\":39974},{\"end\":40818,\"start\":40252}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17107,\"start\":16804},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17158,\"start\":17107},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17699,\"start\":17651},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18288,\"start\":18234},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18576,\"start\":18535},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19008,\"start\":18926},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19473,\"start\":19443},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19778,\"start\":19724},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19939,\"start\":19895},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20567,\"start\":20503},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21372,\"start\":21314},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21947,\"start\":21857},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22058,\"start\":21976},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22538,\"start\":22517},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22857,\"start\":22793},{\"attributes\":{\"id\":\"formula_15\"},\"end\":30327,\"start\":30186},{\"attributes\":{\"id\":\"formula_16\"},\"end\":30918,\"start\":30786},{\"attributes\":{\"id\":\"formula_17\"},\"end\":31089,\"start\":31026},{\"attributes\":{\"id\":\"formula_18\"},\"end\":31283,\"start\":31097},{\"attributes\":{\"id\":\"formula_19\"},\"end\":31805,\"start\":31714},{\"attributes\":{\"id\":\"formula_20\"},\"end\":32325,\"start\":31902},{\"attributes\":{\"id\":\"formula_21\"},\"end\":33162,\"start\":33109},{\"attributes\":{\"id\":\"formula_22\"},\"end\":34048,\"start\":33958},{\"attributes\":{\"id\":\"formula_23\"},\"end\":34613,\"start\":34077},{\"attributes\":{\"id\":\"formula_24\"},\"end\":35292,\"start\":34877}]", "table_ref": "[{\"end\":4319,\"start\":4312},{\"end\":4717,\"start\":4710},{\"end\":4727,\"start\":4719},{\"end\":4741,\"start\":4733},{\"end\":5886,\"start\":5879},{\"end\":7352,\"start\":7345},{\"end\":11275,\"start\":9521},{\"end\":11315,\"start\":11308},{\"end\":11325,\"start\":11317},{\"end\":11338,\"start\":11330},{\"end\":14643,\"start\":14611},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23915,\"start\":23908},{\"end\":24297,\"start\":24290},{\"end\":26368,\"start\":26361},{\"end\":26470,\"start\":26463},{\"end\":28084,\"start\":28077},{\"end\":28394,\"start\":28387},{\"end\":36705,\"start\":36696},{\"end\":39386,\"start\":39379},{\"end\":39435,\"start\":39428},{\"end\":39810,\"start\":39803},{\"end\":40049,\"start\":40042},{\"end\":40365,\"start\":40358},{\"end\":40389,\"start\":40367}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1782,\"start\":1770},{\"end\":3942,\"start\":3895},{\"attributes\":{\"n\":\"2.\"},\"end\":7034,\"start\":7022},{\"end\":13141,\"start\":13126},{\"attributes\":{\"n\":\"3.\"},\"end\":14187,\"start\":14170},{\"attributes\":{\"n\":\"4.\"},\"end\":16457,\"start\":16422},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19494,\"start\":19475},{\"attributes\":{\"n\":\"5.\"},\"end\":23066,\"start\":23055},{\"attributes\":{\"n\":\"5.1.\"},\"end\":23612,\"start\":23580},{\"attributes\":{\"n\":\"5.2.\"},\"end\":23646,\"start\":23615},{\"attributes\":{\"n\":\"5.3.\"},\"end\":24843,\"start\":24823},{\"end\":27235,\"start\":27210},{\"attributes\":{\"n\":\"5.4.\"},\"end\":27906,\"start\":27892},{\"attributes\":{\"n\":\"6.\"},\"end\":28545,\"start\":28535},{\"attributes\":{\"n\":\"8.\"},\"end\":29370,\"start\":29362},{\"end\":30504,\"start\":30483},{\"end\":33424,\"start\":33414},{\"end\":36646,\"start\":36615},{\"end\":37736,\"start\":37712},{\"end\":38450,\"start\":38422},{\"end\":39330,\"start\":39311},{\"end\":39972,\"start\":39953},{\"end\":40250,\"start\":40234},{\"end\":40842,\"start\":40821},{\"end\":40854,\"start\":40844},{\"end\":41162,\"start\":41152},{\"end\":41438,\"start\":41428},{\"end\":42228,\"start\":42221},{\"end\":42748,\"start\":42744},{\"end\":44321,\"start\":44311},{\"end\":44731,\"start\":44711},{\"end\":45355,\"start\":45345},{\"end\":45425,\"start\":45415},{\"end\":45498,\"start\":45487},{\"end\":45571,\"start\":45560},{\"end\":45645,\"start\":45634},{\"end\":45714,\"start\":45703},{\"end\":48147,\"start\":48138}]", "table": "[{\"end\":48136,\"start\":46173},{\"end\":50111,\"start\":48915}]", "figure_caption": "[{\"end\":41150,\"start\":40856},{\"end\":41426,\"start\":41164},{\"end\":42219,\"start\":41440},{\"end\":42742,\"start\":42230},{\"end\":44309,\"start\":42749},{\"end\":44473,\"start\":44323},{\"end\":44709,\"start\":44476},{\"end\":45264,\"start\":44734},{\"end\":45343,\"start\":45267},{\"end\":45413,\"start\":45357},{\"end\":45485,\"start\":45427},{\"end\":45558,\"start\":45501},{\"end\":45632,\"start\":45574},{\"end\":45701,\"start\":45648},{\"end\":45772,\"start\":45717},{\"end\":46173,\"start\":45775},{\"end\":48324,\"start\":48149},{\"end\":48915,\"start\":48327}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3066,\"start\":3058},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5934,\"start\":5926},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":14716,\"start\":14708},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":14777,\"start\":14769},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14849,\"start\":14840},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14938,\"start\":14929},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15025,\"start\":15016},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15091,\"start\":15082},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16038,\"start\":16030},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27479,\"start\":27471},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37045,\"start\":37015},{\"end\":37459,\"start\":37451},{\"end\":37942,\"start\":37934},{\"end\":38001,\"start\":37993},{\"end\":38018,\"start\":38010},{\"end\":38116,\"start\":38108},{\"end\":38655,\"start\":38647},{\"end\":38966,\"start\":38958},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":40314,\"start\":40306},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":40334,\"start\":40326},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40357,\"start\":40348}]", "bib_author_first_name": "[{\"end\":50666,\"start\":50665},{\"end\":50679,\"start\":50678},{\"end\":50690,\"start\":50689},{\"end\":50701,\"start\":50700},{\"end\":50712,\"start\":50711},{\"end\":50723,\"start\":50722},{\"end\":50725,\"start\":50724},{\"end\":50997,\"start\":50996},{\"end\":51010,\"start\":51009},{\"end\":51021,\"start\":51020},{\"end\":51032,\"start\":51031},{\"end\":51310,\"start\":51309},{\"end\":51323,\"start\":51322},{\"end\":51337,\"start\":51336},{\"end\":51350,\"start\":51349},{\"end\":51362,\"start\":51361},{\"end\":51632,\"start\":51631},{\"end\":51643,\"start\":51642},{\"end\":51653,\"start\":51652},{\"end\":51878,\"start\":51877},{\"end\":51881,\"start\":51879},{\"end\":51889,\"start\":51888},{\"end\":51898,\"start\":51894},{\"end\":51906,\"start\":51905},{\"end\":51909,\"start\":51907},{\"end\":51930,\"start\":51925},{\"end\":52228,\"start\":52227},{\"end\":52236,\"start\":52235},{\"end\":52245,\"start\":52241},{\"end\":52635,\"start\":52634},{\"end\":52645,\"start\":52644},{\"end\":52659,\"start\":52658},{\"end\":52945,\"start\":52944},{\"end\":52954,\"start\":52953},{\"end\":53500,\"start\":53499},{\"end\":53511,\"start\":53510},{\"end\":53513,\"start\":53512},{\"end\":53523,\"start\":53522},{\"end\":53886,\"start\":53885},{\"end\":53897,\"start\":53896},{\"end\":53899,\"start\":53898},{\"end\":53909,\"start\":53908},{\"end\":53918,\"start\":53917},{\"end\":53930,\"start\":53929},{\"end\":54260,\"start\":54259},{\"end\":54268,\"start\":54267},{\"end\":54275,\"start\":54274},{\"end\":54282,\"start\":54281},{\"end\":54623,\"start\":54622},{\"end\":54634,\"start\":54633},{\"end\":54636,\"start\":54635},{\"end\":54649,\"start\":54648},{\"end\":54658,\"start\":54657},{\"end\":54671,\"start\":54670},{\"end\":54673,\"start\":54672},{\"end\":54971,\"start\":54970},{\"end\":54977,\"start\":54976},{\"end\":54986,\"start\":54985},{\"end\":54993,\"start\":54992},{\"end\":55395,\"start\":55394},{\"end\":55406,\"start\":55405},{\"end\":55415,\"start\":55414},{\"end\":55941,\"start\":55940},{\"end\":55952,\"start\":55951},{\"end\":55961,\"start\":55960},{\"end\":55972,\"start\":55968},{\"end\":55979,\"start\":55978},{\"end\":55988,\"start\":55987},{\"end\":55998,\"start\":55997},{\"end\":56007,\"start\":56006},{\"end\":56672,\"start\":56671},{\"end\":56679,\"start\":56678},{\"end\":56686,\"start\":56685},{\"end\":56693,\"start\":56692},{\"end\":56695,\"start\":56694},{\"end\":56702,\"start\":56701},{\"end\":57386,\"start\":57385},{\"end\":57397,\"start\":57396},{\"end\":57404,\"start\":57403},{\"end\":57413,\"start\":57412},{\"end\":57424,\"start\":57423},{\"end\":57435,\"start\":57434},{\"end\":57790,\"start\":57789},{\"end\":57804,\"start\":57803},{\"end\":57817,\"start\":57816},{\"end\":57819,\"start\":57818},{\"end\":58153,\"start\":58152},{\"end\":58162,\"start\":58161},{\"end\":58172,\"start\":58171},{\"end\":58182,\"start\":58181},{\"end\":58428,\"start\":58424},{\"end\":58437,\"start\":58433},{\"end\":58446,\"start\":58445},{\"end\":58455,\"start\":58454},{\"end\":58463,\"start\":58462},{\"end\":58779,\"start\":58778},{\"end\":58785,\"start\":58784},{\"end\":58796,\"start\":58792},{\"end\":58804,\"start\":58803},{\"end\":59054,\"start\":59053},{\"end\":59060,\"start\":59059},{\"end\":59071,\"start\":59070},{\"end\":59323,\"start\":59319},{\"end\":59330,\"start\":59329},{\"end\":59340,\"start\":59339},{\"end\":59611,\"start\":59607},{\"end\":59618,\"start\":59617},{\"end\":59887,\"start\":59886},{\"end\":59895,\"start\":59894},{\"end\":59902,\"start\":59901},{\"end\":59910,\"start\":59909},{\"end\":60495,\"start\":60494},{\"end\":60503,\"start\":60502},{\"end\":60510,\"start\":60509},{\"end\":60518,\"start\":60517},{\"end\":60520,\"start\":60519},{\"end\":60828,\"start\":60827},{\"end\":60836,\"start\":60835},{\"end\":60843,\"start\":60842},{\"end\":60851,\"start\":60850},{\"end\":60853,\"start\":60852},{\"end\":61319,\"start\":61318},{\"end\":61330,\"start\":61329},{\"end\":61339,\"start\":61338},{\"end\":61355,\"start\":61354},{\"end\":61357,\"start\":61356},{\"end\":61797,\"start\":61796},{\"end\":61807,\"start\":61806},{\"end\":61816,\"start\":61815},{\"end\":62472,\"start\":62471},{\"end\":62484,\"start\":62483},{\"end\":62486,\"start\":62485},{\"end\":62810,\"start\":62809},{\"end\":62812,\"start\":62811},{\"end\":62819,\"start\":62818},{\"end\":63101,\"start\":63100},{\"end\":63109,\"start\":63108},{\"end\":63508,\"start\":63507},{\"end\":63516,\"start\":63515},{\"end\":63523,\"start\":63522},{\"end\":63530,\"start\":63529},{\"end\":63849,\"start\":63848},{\"end\":63857,\"start\":63856},{\"end\":63866,\"start\":63865},{\"end\":63877,\"start\":63876},{\"end\":63888,\"start\":63887},{\"end\":63896,\"start\":63895},{\"end\":64212,\"start\":64211},{\"end\":64220,\"start\":64219},{\"end\":64229,\"start\":64228},{\"end\":64238,\"start\":64237},{\"end\":64249,\"start\":64248},{\"end\":64260,\"start\":64259},{\"end\":64495,\"start\":64494},{\"end\":64515,\"start\":64514},{\"end\":64527,\"start\":64526},{\"end\":64543,\"start\":64542},{\"end\":64545,\"start\":64544},{\"end\":64748,\"start\":64747},{\"end\":64758,\"start\":64757},{\"end\":64767,\"start\":64766},{\"end\":64776,\"start\":64775},{\"end\":65083,\"start\":65082},{\"end\":65092,\"start\":65091},{\"end\":65104,\"start\":65103},{\"end\":65114,\"start\":65113},{\"end\":65469,\"start\":65468},{\"end\":65476,\"start\":65475},{\"end\":65484,\"start\":65483},{\"end\":65673,\"start\":65672},{\"end\":65682,\"start\":65681},{\"end\":65693,\"start\":65692},{\"end\":65703,\"start\":65702},{\"end\":65948,\"start\":65947},{\"end\":65957,\"start\":65956},{\"end\":65968,\"start\":65967},{\"end\":65977,\"start\":65976},{\"end\":65987,\"start\":65986},{\"end\":66330,\"start\":66329},{\"end\":66332,\"start\":66331},{\"end\":66342,\"start\":66341},{\"end\":66344,\"start\":66343},{\"end\":66635,\"start\":66634},{\"end\":66651,\"start\":66650},{\"end\":66662,\"start\":66661},{\"end\":66677,\"start\":66676},{\"end\":67033,\"start\":67032},{\"end\":67041,\"start\":67040},{\"end\":67049,\"start\":67048},{\"end\":67057,\"start\":67056},{\"end\":67063,\"start\":67062},{\"end\":67072,\"start\":67071},{\"end\":67074,\"start\":67073},{\"end\":67374,\"start\":67373},{\"end\":67380,\"start\":67379},{\"end\":67388,\"start\":67387},{\"end\":67395,\"start\":67394},{\"end\":67402,\"start\":67401},{\"end\":67877,\"start\":67876},{\"end\":67883,\"start\":67882},{\"end\":67885,\"start\":67884},{\"end\":67894,\"start\":67893},{\"end\":67901,\"start\":67900},{\"end\":68140,\"start\":68139},{\"end\":68153,\"start\":68152},{\"end\":68166,\"start\":68165},{\"end\":68178,\"start\":68177},{\"end\":68193,\"start\":68192},{\"end\":68534,\"start\":68533},{\"end\":68543,\"start\":68542},{\"end\":68551,\"start\":68550},{\"end\":68560,\"start\":68559},{\"end\":69024,\"start\":69023},{\"end\":69032,\"start\":69031},{\"end\":69041,\"start\":69040},{\"end\":69047,\"start\":69046},{\"end\":69049,\"start\":69048},{\"end\":69058,\"start\":69057},{\"end\":69060,\"start\":69059},{\"end\":69072,\"start\":69071},{\"end\":69074,\"start\":69073},{\"end\":69428,\"start\":69424},{\"end\":69435,\"start\":69434},{\"end\":69443,\"start\":69442},{\"end\":69452,\"start\":69451},{\"end\":69454,\"start\":69453},{\"end\":69775,\"start\":69774},{\"end\":69785,\"start\":69784},{\"end\":69794,\"start\":69793},{\"end\":69801,\"start\":69800},{\"end\":69803,\"start\":69802},{\"end\":69810,\"start\":69809}]", "bib_author_last_name": "[{\"end\":50676,\"start\":50667},{\"end\":50687,\"start\":50680},{\"end\":50698,\"start\":50691},{\"end\":50709,\"start\":50702},{\"end\":50720,\"start\":50713},{\"end\":50733,\"start\":50726},{\"end\":51007,\"start\":50998},{\"end\":51018,\"start\":51011},{\"end\":51029,\"start\":51022},{\"end\":51040,\"start\":51033},{\"end\":51320,\"start\":51311},{\"end\":51334,\"start\":51324},{\"end\":51347,\"start\":51338},{\"end\":51359,\"start\":51351},{\"end\":51368,\"start\":51363},{\"end\":51640,\"start\":51633},{\"end\":51650,\"start\":51644},{\"end\":51661,\"start\":51654},{\"end\":51886,\"start\":51882},{\"end\":51892,\"start\":51890},{\"end\":51903,\"start\":51899},{\"end\":51923,\"start\":51910},{\"end\":51935,\"start\":51931},{\"end\":51941,\"start\":51937},{\"end\":52233,\"start\":52229},{\"end\":52239,\"start\":52237},{\"end\":52251,\"start\":52246},{\"end\":52642,\"start\":52636},{\"end\":52656,\"start\":52646},{\"end\":52666,\"start\":52660},{\"end\":52951,\"start\":52946},{\"end\":52964,\"start\":52955},{\"end\":53508,\"start\":53501},{\"end\":53520,\"start\":53514},{\"end\":53529,\"start\":53524},{\"end\":53894,\"start\":53887},{\"end\":53906,\"start\":53900},{\"end\":53915,\"start\":53910},{\"end\":53927,\"start\":53919},{\"end\":53933,\"start\":53931},{\"end\":54265,\"start\":54261},{\"end\":54272,\"start\":54269},{\"end\":54279,\"start\":54276},{\"end\":54290,\"start\":54283},{\"end\":54631,\"start\":54624},{\"end\":54646,\"start\":54637},{\"end\":54655,\"start\":54650},{\"end\":54668,\"start\":54659},{\"end\":54679,\"start\":54674},{\"end\":54974,\"start\":54972},{\"end\":54983,\"start\":54978},{\"end\":54990,\"start\":54987},{\"end\":54997,\"start\":54994},{\"end\":55403,\"start\":55396},{\"end\":55412,\"start\":55407},{\"end\":55421,\"start\":55416},{\"end\":55949,\"start\":55942},{\"end\":55958,\"start\":55953},{\"end\":55966,\"start\":55962},{\"end\":55976,\"start\":55973},{\"end\":55985,\"start\":55980},{\"end\":55995,\"start\":55989},{\"end\":56004,\"start\":55999},{\"end\":56015,\"start\":56008},{\"end\":56676,\"start\":56673},{\"end\":56683,\"start\":56680},{\"end\":56690,\"start\":56687},{\"end\":56699,\"start\":56696},{\"end\":56706,\"start\":56703},{\"end\":57394,\"start\":57387},{\"end\":57401,\"start\":57398},{\"end\":57410,\"start\":57405},{\"end\":57421,\"start\":57414},{\"end\":57432,\"start\":57425},{\"end\":57441,\"start\":57436},{\"end\":57801,\"start\":57791},{\"end\":57814,\"start\":57805},{\"end\":57826,\"start\":57820},{\"end\":58159,\"start\":58154},{\"end\":58169,\"start\":58163},{\"end\":58179,\"start\":58173},{\"end\":58190,\"start\":58183},{\"end\":58431,\"start\":58429},{\"end\":58443,\"start\":58438},{\"end\":58452,\"start\":58447},{\"end\":58460,\"start\":58456},{\"end\":58470,\"start\":58464},{\"end\":58782,\"start\":58780},{\"end\":58790,\"start\":58786},{\"end\":58801,\"start\":58797},{\"end\":58815,\"start\":58805},{\"end\":59057,\"start\":59055},{\"end\":59068,\"start\":59061},{\"end\":59077,\"start\":59072},{\"end\":59327,\"start\":59324},{\"end\":59337,\"start\":59331},{\"end\":59346,\"start\":59341},{\"end\":59615,\"start\":59612},{\"end\":59624,\"start\":59619},{\"end\":59892,\"start\":59888},{\"end\":59899,\"start\":59896},{\"end\":59907,\"start\":59903},{\"end\":59917,\"start\":59911},{\"end\":60500,\"start\":60496},{\"end\":60507,\"start\":60504},{\"end\":60515,\"start\":60511},{\"end\":60527,\"start\":60521},{\"end\":60833,\"start\":60829},{\"end\":60840,\"start\":60837},{\"end\":60848,\"start\":60844},{\"end\":60860,\"start\":60854},{\"end\":61327,\"start\":61320},{\"end\":61336,\"start\":61331},{\"end\":61352,\"start\":61340},{\"end\":61804,\"start\":61798},{\"end\":61813,\"start\":61808},{\"end\":61821,\"start\":61817},{\"end\":61828,\"start\":61823},{\"end\":62481,\"start\":62473},{\"end\":62496,\"start\":62487},{\"end\":62816,\"start\":62813},{\"end\":62824,\"start\":62820},{\"end\":63106,\"start\":63102},{\"end\":63116,\"start\":63110},{\"end\":63513,\"start\":63509},{\"end\":63520,\"start\":63517},{\"end\":63527,\"start\":63524},{\"end\":63537,\"start\":63531},{\"end\":63854,\"start\":63850},{\"end\":63863,\"start\":63858},{\"end\":63874,\"start\":63867},{\"end\":63885,\"start\":63878},{\"end\":63893,\"start\":63889},{\"end\":63903,\"start\":63897},{\"end\":64217,\"start\":64213},{\"end\":64226,\"start\":64221},{\"end\":64235,\"start\":64230},{\"end\":64246,\"start\":64239},{\"end\":64257,\"start\":64250},{\"end\":64267,\"start\":64261},{\"end\":64512,\"start\":64496},{\"end\":64524,\"start\":64516},{\"end\":64540,\"start\":64528},{\"end\":64554,\"start\":64546},{\"end\":64755,\"start\":64749},{\"end\":64764,\"start\":64759},{\"end\":64773,\"start\":64768},{\"end\":64784,\"start\":64777},{\"end\":65089,\"start\":65084},{\"end\":65101,\"start\":65093},{\"end\":65111,\"start\":65105},{\"end\":65121,\"start\":65115},{\"end\":65473,\"start\":65470},{\"end\":65481,\"start\":65477},{\"end\":65491,\"start\":65485},{\"end\":65679,\"start\":65674},{\"end\":65690,\"start\":65683},{\"end\":65700,\"start\":65694},{\"end\":65711,\"start\":65704},{\"end\":65954,\"start\":65949},{\"end\":65965,\"start\":65958},{\"end\":65974,\"start\":65969},{\"end\":65984,\"start\":65978},{\"end\":65995,\"start\":65988},{\"end\":66339,\"start\":66333},{\"end\":66357,\"start\":66345},{\"end\":66648,\"start\":66636},{\"end\":66659,\"start\":66652},{\"end\":66674,\"start\":66663},{\"end\":66690,\"start\":66678},{\"end\":67038,\"start\":67034},{\"end\":67046,\"start\":67042},{\"end\":67054,\"start\":67050},{\"end\":67060,\"start\":67058},{\"end\":67069,\"start\":67064},{\"end\":67077,\"start\":67075},{\"end\":67377,\"start\":67375},{\"end\":67385,\"start\":67381},{\"end\":67392,\"start\":67389},{\"end\":67399,\"start\":67396},{\"end\":67406,\"start\":67403},{\"end\":67880,\"start\":67878},{\"end\":67891,\"start\":67886},{\"end\":67898,\"start\":67895},{\"end\":67906,\"start\":67902},{\"end\":68150,\"start\":68141},{\"end\":68163,\"start\":68154},{\"end\":68175,\"start\":68167},{\"end\":68190,\"start\":68179},{\"end\":68208,\"start\":68194},{\"end\":68540,\"start\":68535},{\"end\":68548,\"start\":68544},{\"end\":68557,\"start\":68552},{\"end\":68568,\"start\":68561},{\"end\":69029,\"start\":69025},{\"end\":69038,\"start\":69033},{\"end\":69044,\"start\":69042},{\"end\":69055,\"start\":69050},{\"end\":69069,\"start\":69061},{\"end\":69081,\"start\":69075},{\"end\":69432,\"start\":69429},{\"end\":69440,\"start\":69436},{\"end\":69449,\"start\":69444},{\"end\":69460,\"start\":69455},{\"end\":69782,\"start\":69776},{\"end\":69791,\"start\":69786},{\"end\":69798,\"start\":69795},{\"end\":69807,\"start\":69804},{\"end\":69813,\"start\":69811}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8577357},\"end\":50943,\"start\":50620},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10908021},\"end\":51279,\"start\":50945},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2127515},\"end\":51597,\"start\":51281},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2709712},\"end\":51845,\"start\":51599},{\"attributes\":{\"id\":\"b4\"},\"end\":52115,\"start\":51847},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4974063},\"end\":52586,\"start\":52117},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3411732},\"end\":52891,\"start\":52588},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6755881},\"end\":53441,\"start\":52893},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":17674695},\"end\":53803,\"start\":53443},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52459},\"end\":54200,\"start\":53805},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6742009},\"end\":54576,\"start\":54202},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1993257},\"end\":54922,\"start\":54578},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":55338,\"start\":54924},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":46895919},\"end\":55882,\"start\":55340},{\"attributes\":{\"doi\":\"PMLR. 1\",\"id\":\"b14\",\"matched_paper_id\":7646250},\"end\":56587,\"start\":55884},{\"attributes\":{\"doi\":\"PMLR. 3\",\"id\":\"b15\",\"matched_paper_id\":8239952},\"end\":57294,\"start\":56589},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52957835},\"end\":57722,\"start\":57296},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":195908774},\"end\":58069,\"start\":57724},{\"attributes\":{\"id\":\"b18\"},\"end\":58356,\"start\":58071},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4685015},\"end\":58726,\"start\":58358},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6037691},\"end\":59014,\"start\":58728},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":536962},\"end\":59268,\"start\":59016},{\"attributes\":{\"id\":\"b22\"},\"end\":59564,\"start\":59270},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10627900},\"end\":59822,\"start\":59566},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":556999},\"end\":60428,\"start\":59824},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":745350},\"end\":60770,\"start\":60430},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12757870},\"end\":61275,\"start\":60772},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3026868},\"end\":61752,\"start\":61277},{\"attributes\":{\"doi\":\"PMLR. 3\",\"id\":\"b28\",\"matched_paper_id\":12435394},\"end\":62374,\"start\":61754},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":121372635},\"end\":62776,\"start\":62376},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":740063},\"end\":63021,\"start\":62778},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206858797},\"end\":63458,\"start\":63023},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":554423},\"end\":63846,\"start\":63460},{\"attributes\":{\"doi\":\"arXiv:1710.06924\",\"id\":\"b33\"},\"end\":64136,\"start\":63848},{\"attributes\":{\"doi\":\"abs/1806.09755\",\"id\":\"b34\"},\"end\":64457,\"start\":64138},{\"attributes\":{\"id\":\"b35\"},\"end\":64697,\"start\":64459},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7534823},\"end\":65013,\"start\":64699},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":4619542},\"end\":65418,\"start\":65015},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":16439870},\"end\":65624,\"start\":65420},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":4357800},\"end\":65945,\"start\":65626},{\"attributes\":{\"doi\":\"arXiv:1412.3474\",\"id\":\"b40\"},\"end\":66242,\"start\":65947},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":8142232},\"end\":66575,\"start\":66244},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":2928248},\"end\":66958,\"start\":66577},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":49883347},\"end\":67283,\"start\":66960},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3675569},\"end\":67806,\"start\":67285},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1082740},\"end\":68058,\"start\":67808},{\"attributes\":{\"doi\":\"abs/1702.08811\",\"id\":\"b46\"},\"end\":68421,\"start\":68060},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":51793458},\"end\":68974,\"start\":68423},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":55701085},\"end\":69342,\"start\":68976},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":233404466},\"end\":69694,\"start\":69344},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":15019934},\"end\":69987,\"start\":69696}]", "bib_title": "[{\"end\":50663,\"start\":50620},{\"end\":50994,\"start\":50945},{\"end\":51307,\"start\":51281},{\"end\":51629,\"start\":51599},{\"end\":52225,\"start\":52117},{\"end\":52632,\"start\":52588},{\"end\":52942,\"start\":52893},{\"end\":53497,\"start\":53443},{\"end\":53883,\"start\":53805},{\"end\":54257,\"start\":54202},{\"end\":54620,\"start\":54578},{\"end\":54968,\"start\":54924},{\"end\":55392,\"start\":55340},{\"end\":55938,\"start\":55884},{\"end\":56669,\"start\":56589},{\"end\":57383,\"start\":57296},{\"end\":57787,\"start\":57724},{\"end\":58422,\"start\":58358},{\"end\":58776,\"start\":58728},{\"end\":59051,\"start\":59016},{\"end\":59317,\"start\":59270},{\"end\":59605,\"start\":59566},{\"end\":59884,\"start\":59824},{\"end\":60492,\"start\":60430},{\"end\":60825,\"start\":60772},{\"end\":61316,\"start\":61277},{\"end\":61794,\"start\":61754},{\"end\":62469,\"start\":62376},{\"end\":62807,\"start\":62778},{\"end\":63098,\"start\":63023},{\"end\":63505,\"start\":63460},{\"end\":64745,\"start\":64699},{\"end\":65080,\"start\":65015},{\"end\":65466,\"start\":65420},{\"end\":65670,\"start\":65626},{\"end\":66327,\"start\":66244},{\"end\":66632,\"start\":66577},{\"end\":67030,\"start\":66960},{\"end\":67371,\"start\":67285},{\"end\":67874,\"start\":67808},{\"end\":68531,\"start\":68423},{\"end\":69021,\"start\":68976},{\"end\":69422,\"start\":69344},{\"end\":69772,\"start\":69696}]", "bib_author": "[{\"end\":50678,\"start\":50665},{\"end\":50689,\"start\":50678},{\"end\":50700,\"start\":50689},{\"end\":50711,\"start\":50700},{\"end\":50722,\"start\":50711},{\"end\":50735,\"start\":50722},{\"end\":51009,\"start\":50996},{\"end\":51020,\"start\":51009},{\"end\":51031,\"start\":51020},{\"end\":51042,\"start\":51031},{\"end\":51322,\"start\":51309},{\"end\":51336,\"start\":51322},{\"end\":51349,\"start\":51336},{\"end\":51361,\"start\":51349},{\"end\":51370,\"start\":51361},{\"end\":51642,\"start\":51631},{\"end\":51652,\"start\":51642},{\"end\":51663,\"start\":51652},{\"end\":51888,\"start\":51877},{\"end\":51894,\"start\":51888},{\"end\":51905,\"start\":51894},{\"end\":51925,\"start\":51905},{\"end\":51937,\"start\":51925},{\"end\":51943,\"start\":51937},{\"end\":52235,\"start\":52227},{\"end\":52241,\"start\":52235},{\"end\":52253,\"start\":52241},{\"end\":52644,\"start\":52634},{\"end\":52658,\"start\":52644},{\"end\":52668,\"start\":52658},{\"end\":52953,\"start\":52944},{\"end\":52966,\"start\":52953},{\"end\":53510,\"start\":53499},{\"end\":53522,\"start\":53510},{\"end\":53531,\"start\":53522},{\"end\":53896,\"start\":53885},{\"end\":53908,\"start\":53896},{\"end\":53917,\"start\":53908},{\"end\":53929,\"start\":53917},{\"end\":53935,\"start\":53929},{\"end\":54267,\"start\":54259},{\"end\":54274,\"start\":54267},{\"end\":54281,\"start\":54274},{\"end\":54292,\"start\":54281},{\"end\":54633,\"start\":54622},{\"end\":54648,\"start\":54633},{\"end\":54657,\"start\":54648},{\"end\":54670,\"start\":54657},{\"end\":54681,\"start\":54670},{\"end\":54976,\"start\":54970},{\"end\":54985,\"start\":54976},{\"end\":54992,\"start\":54985},{\"end\":54999,\"start\":54992},{\"end\":55405,\"start\":55394},{\"end\":55414,\"start\":55405},{\"end\":55423,\"start\":55414},{\"end\":55951,\"start\":55940},{\"end\":55960,\"start\":55951},{\"end\":55968,\"start\":55960},{\"end\":55978,\"start\":55968},{\"end\":55987,\"start\":55978},{\"end\":55997,\"start\":55987},{\"end\":56006,\"start\":55997},{\"end\":56017,\"start\":56006},{\"end\":56678,\"start\":56671},{\"end\":56685,\"start\":56678},{\"end\":56692,\"start\":56685},{\"end\":56701,\"start\":56692},{\"end\":56708,\"start\":56701},{\"end\":57396,\"start\":57385},{\"end\":57403,\"start\":57396},{\"end\":57412,\"start\":57403},{\"end\":57423,\"start\":57412},{\"end\":57434,\"start\":57423},{\"end\":57443,\"start\":57434},{\"end\":57803,\"start\":57789},{\"end\":57816,\"start\":57803},{\"end\":57828,\"start\":57816},{\"end\":58161,\"start\":58152},{\"end\":58171,\"start\":58161},{\"end\":58181,\"start\":58171},{\"end\":58192,\"start\":58181},{\"end\":58433,\"start\":58424},{\"end\":58445,\"start\":58433},{\"end\":58454,\"start\":58445},{\"end\":58462,\"start\":58454},{\"end\":58472,\"start\":58462},{\"end\":58784,\"start\":58778},{\"end\":58792,\"start\":58784},{\"end\":58803,\"start\":58792},{\"end\":58817,\"start\":58803},{\"end\":59059,\"start\":59053},{\"end\":59070,\"start\":59059},{\"end\":59079,\"start\":59070},{\"end\":59329,\"start\":59319},{\"end\":59339,\"start\":59329},{\"end\":59348,\"start\":59339},{\"end\":59617,\"start\":59607},{\"end\":59626,\"start\":59617},{\"end\":59894,\"start\":59886},{\"end\":59901,\"start\":59894},{\"end\":59909,\"start\":59901},{\"end\":59919,\"start\":59909},{\"end\":60502,\"start\":60494},{\"end\":60509,\"start\":60502},{\"end\":60517,\"start\":60509},{\"end\":60529,\"start\":60517},{\"end\":60835,\"start\":60827},{\"end\":60842,\"start\":60835},{\"end\":60850,\"start\":60842},{\"end\":60862,\"start\":60850},{\"end\":61329,\"start\":61318},{\"end\":61338,\"start\":61329},{\"end\":61354,\"start\":61338},{\"end\":61360,\"start\":61354},{\"end\":61806,\"start\":61796},{\"end\":61815,\"start\":61806},{\"end\":61823,\"start\":61815},{\"end\":61830,\"start\":61823},{\"end\":62483,\"start\":62471},{\"end\":62498,\"start\":62483},{\"end\":62818,\"start\":62809},{\"end\":62826,\"start\":62818},{\"end\":63108,\"start\":63100},{\"end\":63118,\"start\":63108},{\"end\":63515,\"start\":63507},{\"end\":63522,\"start\":63515},{\"end\":63529,\"start\":63522},{\"end\":63539,\"start\":63529},{\"end\":63856,\"start\":63848},{\"end\":63865,\"start\":63856},{\"end\":63876,\"start\":63865},{\"end\":63887,\"start\":63876},{\"end\":63895,\"start\":63887},{\"end\":63905,\"start\":63895},{\"end\":64219,\"start\":64211},{\"end\":64228,\"start\":64219},{\"end\":64237,\"start\":64228},{\"end\":64248,\"start\":64237},{\"end\":64259,\"start\":64248},{\"end\":64269,\"start\":64259},{\"end\":64514,\"start\":64494},{\"end\":64526,\"start\":64514},{\"end\":64542,\"start\":64526},{\"end\":64556,\"start\":64542},{\"end\":64757,\"start\":64747},{\"end\":64766,\"start\":64757},{\"end\":64775,\"start\":64766},{\"end\":64786,\"start\":64775},{\"end\":65091,\"start\":65082},{\"end\":65103,\"start\":65091},{\"end\":65113,\"start\":65103},{\"end\":65123,\"start\":65113},{\"end\":65475,\"start\":65468},{\"end\":65483,\"start\":65475},{\"end\":65493,\"start\":65483},{\"end\":65681,\"start\":65672},{\"end\":65692,\"start\":65681},{\"end\":65702,\"start\":65692},{\"end\":65713,\"start\":65702},{\"end\":65956,\"start\":65947},{\"end\":65967,\"start\":65956},{\"end\":65976,\"start\":65967},{\"end\":65986,\"start\":65976},{\"end\":65997,\"start\":65986},{\"end\":66341,\"start\":66329},{\"end\":66359,\"start\":66341},{\"end\":66650,\"start\":66634},{\"end\":66661,\"start\":66650},{\"end\":66676,\"start\":66661},{\"end\":66692,\"start\":66676},{\"end\":67040,\"start\":67032},{\"end\":67048,\"start\":67040},{\"end\":67056,\"start\":67048},{\"end\":67062,\"start\":67056},{\"end\":67071,\"start\":67062},{\"end\":67079,\"start\":67071},{\"end\":67379,\"start\":67373},{\"end\":67387,\"start\":67379},{\"end\":67394,\"start\":67387},{\"end\":67401,\"start\":67394},{\"end\":67408,\"start\":67401},{\"end\":67882,\"start\":67876},{\"end\":67893,\"start\":67882},{\"end\":67900,\"start\":67893},{\"end\":67908,\"start\":67900},{\"end\":68152,\"start\":68139},{\"end\":68165,\"start\":68152},{\"end\":68177,\"start\":68165},{\"end\":68192,\"start\":68177},{\"end\":68210,\"start\":68192},{\"end\":68542,\"start\":68533},{\"end\":68550,\"start\":68542},{\"end\":68559,\"start\":68550},{\"end\":68570,\"start\":68559},{\"end\":69031,\"start\":69023},{\"end\":69040,\"start\":69031},{\"end\":69046,\"start\":69040},{\"end\":69057,\"start\":69046},{\"end\":69071,\"start\":69057},{\"end\":69083,\"start\":69071},{\"end\":69434,\"start\":69424},{\"end\":69442,\"start\":69434},{\"end\":69451,\"start\":69442},{\"end\":69462,\"start\":69451},{\"end\":69784,\"start\":69774},{\"end\":69793,\"start\":69784},{\"end\":69800,\"start\":69793},{\"end\":69809,\"start\":69800},{\"end\":69815,\"start\":69809}]", "bib_venue": "[{\"end\":53121,\"start\":53055},{\"end\":55140,\"start\":55078},{\"end\":56199,\"start\":56113},{\"end\":56878,\"start\":56808},{\"end\":60074,\"start\":60008},{\"end\":61007,\"start\":60932},{\"end\":62000,\"start\":61930},{\"end\":63212,\"start\":63193},{\"end\":63660,\"start\":63608},{\"end\":67549,\"start\":67487},{\"end\":68711,\"start\":68649},{\"end\":50751,\"start\":50735},{\"end\":51091,\"start\":51042},{\"end\":51419,\"start\":51370},{\"end\":51699,\"start\":51663},{\"end\":51875,\"start\":51847},{\"end\":52324,\"start\":52253},{\"end\":52720,\"start\":52668},{\"end\":53034,\"start\":52966},{\"end\":53594,\"start\":53531},{\"end\":53973,\"start\":53935},{\"end\":54363,\"start\":54292},{\"end\":54730,\"start\":54681},{\"end\":55076,\"start\":54999},{\"end\":55472,\"start\":55423},{\"end\":56092,\"start\":56024},{\"end\":56783,\"start\":56715},{\"end\":57492,\"start\":57443},{\"end\":57877,\"start\":57828},{\"end\":58150,\"start\":58071},{\"end\":58521,\"start\":58472},{\"end\":58860,\"start\":58817},{\"end\":59123,\"start\":59079},{\"end\":59397,\"start\":59348},{\"end\":59675,\"start\":59626},{\"end\":59987,\"start\":59919},{\"end\":60578,\"start\":60529},{\"end\":60930,\"start\":60862},{\"end\":61412,\"start\":61360},{\"end\":61905,\"start\":61837},{\"end\":62555,\"start\":62498},{\"end\":62877,\"start\":62826},{\"end\":63191,\"start\":63118},{\"end\":63606,\"start\":63539},{\"end\":63966,\"start\":63921},{\"end\":64209,\"start\":64138},{\"end\":64492,\"start\":64459},{\"end\":64824,\"start\":64786},{\"end\":65192,\"start\":65123},{\"end\":65497,\"start\":65493},{\"end\":65759,\"start\":65713},{\"end\":66067,\"start\":66012},{\"end\":66381,\"start\":66359},{\"end\":66752,\"start\":66692},{\"end\":67104,\"start\":67079},{\"end\":67485,\"start\":67408},{\"end\":67912,\"start\":67908},{\"end\":68137,\"start\":68060},{\"end\":68647,\"start\":68570},{\"end\":69132,\"start\":69083},{\"end\":69483,\"start\":69462},{\"end\":69820,\"start\":69815}]"}}}, "year": 2023, "month": 12, "day": 17}