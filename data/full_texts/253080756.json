{"id": 253080756, "updated": "2023-10-05 09:23:25.002", "metadata": {"title": "ConfMix: Unsupervised Domain Adaptation for Object Detection via Confidence-based Mixing", "authors": "[{\"first\":\"Giulio\",\"last\":\"Mattolin\",\"middle\":[]},{\"first\":\"Luca\",\"last\":\"Zanella\",\"middle\":[]},{\"first\":\"Elisa\",\"last\":\"Ricci\",\"middle\":[]},{\"first\":\"Yiming\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Unsupervised Domain Adaptation (UDA) for object detection aims to adapt a model trained on a source domain to detect instances from a new target domain for which annotations are not available. Different from traditional approaches, we propose ConfMix, the first method that introduces a sample mixing strategy based on region-level detection confidence for adaptive object detector learning. We mix the local region of the target sample that corresponds to the most confident pseudo detections with a source image, and apply an additional consistency loss term to gradually adapt towards the target data distribution. In order to robustly define a confidence score for a region, we exploit the confidence score per pseudo detection that accounts for both the detector-dependent confidence and the bounding box uncertainty. Moreover, we propose a novel pseudo labelling scheme that progressively filters the pseudo target detections using the confidence metric that varies from a loose to strict manner along the training. We perform extensive experiments with three datasets, achieving state-of-the-art performance in two of them and approaching the supervised target model performance in the other. Code is available at: https://github.com/giuliomattolin/ConfMix.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2210.11539", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wacv/MattolinZRW23", "doi": "10.1109/wacv56688.2023.00050"}}, "content": {"source": {"pdf_hash": "d4b95d58ee696cf89e71910bf7dea69489b22b47", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.11539v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "790e38aeb4e1e3cdcab230cd095b8120d5ebdf8a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d4b95d58ee696cf89e71910bf7dea69489b22b47.txt", "contents": "\nConfMix: Unsupervised Domain Adaptation for Object Detection via Confidence-based Mixing\n\n\nGiulio Mattolin \nUniversity of Trento\nTrentoItaly\n\nLuca Zanella lzanella@fbk.eu \nFondazione Bruno Kessler\nTrentoItaly\n\nElisa Ricci \nUniversity of Trento\nTrentoItaly\n\nFondazione Bruno Kessler\nTrentoItaly\n\nYiming Wang \nFondazione Bruno Kessler\nTrentoItaly\n\nConfMix: Unsupervised Domain Adaptation for Object Detection via Confidence-based Mixing\n\nUnsupervised Domain Adaptation (UDA) for object detection aims to adapt a model trained on a source domain to detect instances from a new target domain for which annotations are not available. Different from traditional approaches, we propose ConfMix, the first method that introduces a sample mixing strategy based on region-level detection confidence for adaptive object detector learning. We mix the local region of the target sample that corresponds to the most confident pseudo detections with a source image, and apply an additional consistency loss term to gradually adapt towards the target data distribution. In order to robustly define a confidence score for a region, we exploit the confidence score per pseudo detection that accounts for both the detector-dependent confidence and the bounding box uncertainty. Moreover, we propose a novel pseudo labelling scheme that progressively filters the pseudo target detections using the confidence metric that varies from a loose to strict manner along the training. We perform extensive experiments with three datasets, achieving state-ofthe-art performance in two of them and approaching the supervised target model performance in the other. Code is available at https://github.com/giuliomattolin/ConfMix.\n\nIntroduction\n\nObject detection is a fundamental task in computer vision which involves the classification and localisation, e.g. by bounding boxes, of objects of interest belonging to certain predefined categories. Due to its importance in many applications such as autonomous driving, video surveillance and robotic perception, object detection has received significant attention, leading to the development of several different models [15,34,41,35]. However, as detectors mostly  rely on deep learning, it is a well known fact that they suffer from severe performance degradation when being tested on images that are visually different from the ones encountered during training, due to the domain shift [4].\n\nTo address this problem, recent research efforts have been put on devising Unsupervised Domain Adaptation (UDA) techniques for building deep models that can adapt from an annotated source dataset to a target one without tedious manual annotations [40,11,1,16,9]. The vast majority of UDA methods for detection resort on adversarial training and on exploiting the Gradient Reversal Layer (GRL) [11] to perform adaptation both at image-level and instance-level [4,37,55,39,43]. Other approaches mostly focus on robustly producing pseudo detections in order to effectively finetune the model on the target data [49,52,44]. In general, while over the last few years several solutions have been proposed in the literature for adapting two-stage object detectors, we argue that devising UDA approaches which can also be applied to one-stage detectors would be desirable. Indeed, the latter methods are more appropriate in applications such as autonomous driving that necessitate of real-time processing and high computational efficiency.\n\nConcurrently, recent works in computer vision have shown the benefit of adopting sophisticated data augmentation techniques by synthesising mixed samples with target and source images in order to improve generalisation ability of deep architectures [51,50,17]. These methods have been considered in the context of UDA for classification [45,30] and semantic segmentation [10,32,20,5], demonstrating some empirical advantage. However, extending these approaches to UDA for detection is far from trivial.\n\nInspired by these previous works, in this paper we propose ConfMix, the first mixing-based UDA approach for object detection based on the regional confidence of pseudo detections. The main idea behind ConfMix is illustrated in Figure 1. Specifically, we propose to artificially generate samples by combining the region of target images where the model is most confident with source images. We also introduce during training an associated consistency loss to enforce coherent predictions among generated images. Our intuition is that, by combining source and target images and forming new mixed samples, we are training our model on novel, synthetically generated sample images with reliable pseudo detections and with visual appearance close to the samples of target domain, thus improving the generalisation capabilities of the detector. Moreover, the quality of pseudo detections plays an essential role during adaptation, and is tightly related to the confidence metric. By exploiting a stricter confidence metric, e.g. enriching the detectordependent confidence with bounding box uncertainty [6], one can obtain more reliable pseudo detections, however with a reduced number. To mitigate this, we propose to progressively restrict the confidence metric for pseudo labelling. With a less strict confidence metric at the initial adaptation phase, we allow more pseudo detections in order to learn the representation of the target domain, while with a gradually stricter confidence metric, we aim to improve the detection accuracy with more trustworthy pseudo detections. We conduct extensive experiments on different datasets (Cityscapes [7] \u2192 FoggyCityscapes [38], Sim10K [19] \u2192 Cityscapes and KITTI [13] \u2192 Cityscapes) and we show that our approach outperforms existing algorithms in most setups.\n\nWe summarise our main contributions as below:\n\n\u2022 We introduce the first sample-mixing UDA method for object detection. Our approach, named ConfMix, mixes samples from source and target domains based on the regional confidence of target pseudo detections. \u2022 We propose a novel Progressive Pseudo-labelling scheme by gradually restricting the confidence metric along the adaptive learning, which allows for a smooth transition when learning target representation, thus improving detection accuracy. \u2022 ConfMix scores the new state-of-the-art adaptation performance, achieving +1.7% on Sim10k \u2192 Cityscapes, and +3.7% on KITTI \u2192 Cityscapes in terms of mean Average Precision (mAP).\n\n\nRelated work\n\nObject Detection. Current object detection models can be grouped into two main categories: one-stage and two-stage approaches. One-stage object detectors, such as YOLO [34] and FCOS [41], adopt a unified framework to obtain final results directly from the feature maps generated by a CNN backbone. These frameworks are very computationally efficient and are able to achieve near real-time speed during inference. On the other hand, two-stage object detectors, such as RCNN [15], generate predictions by first extracting region proposals and then, leveraging this information, produce classification labels and bounding box coordinates. Such models are widely adopted for their high performance but, although research has been conducted to improve detection speed [14,35,8], they are considerably slower compared to one-stage detectors. Unsupervised Domain Adaptation. Given a labelled source domain and an unlabelled target domain, UDA aims to use the available data to produce a model that is able to generalise and perform well on the target domain. A conventional approach is to reduce the domain gap by directly minimising the distance between feature distributions using discrepancy loss functions [29,40]. On the other hand, adversarial-based methods [11,12,42], employ a domain discriminator and a feature extractor that learns to produce domain-invariant feature representations by fooling the discriminator. Many works demonstrated the benefit of using pseudo labels to maximally leverage information from the target domain [28,23,24], eventually considering a gradual scheme for incorporating them [48]. Other works have focused on adopting sample mixing techniques, such as mixup [51] or CutMix [50], to improve generalisation. For instance, in [45,47] domain-level mixup regularisation is applied to ensure domain invariance in the learned feature representations, while in [3,33] the model's attention is used to re-assign the confidence of saliency-guided samples and labels. Similar ideas are implemented in previous works considering the segmentation task [10,32,20,5,31]. However, to the best of our knowledge, no previous works have been proposed to exploit mixing techniques for UDA in the context of object detection. UDA for Object Detection. In the context of object detection, UDA was recently introduced by [4], which proposed image-and instance-level alignment using two GRLs [11] on Faster R-CNN. Subsequently, several methods started to address this problem mainly using two-stage detectors. Focusing on image-level, [37] showed that strong-local alignment and weak-global alignment of the features extracted from the backbone improve adaptation, while [55], focusing on instance-level, exploited RPN proposals to perform region-level alignment. To adapt the source-biased decision boundary to the target data, [2] combined adversarial training with image-to-image translation by generating interpolated samples using Cycle-GAN [54]. Other recent works have proposed applying self-training with pseudo detections to perform the adaptation. To address the risk of performance degradation caused by overfitting noisy pseudo detections, [49] introduces an uncertainty-based fusion of pseudo detections sets generated via stochastic inference, [27] proposes self-entropy descent (SED) as a metric to search for an appropriate confidence threshold for reliable pseudo detections, while [44] uses a student-teacher framework and gradually updates the source-trained model. Few works have addressed UDA for one-stage detectors, e.g. FCOS [25,26,18] or SSD [21]. In particular, adopting a self-training procedure reduces the negative effects of inaccurate pseudo detections by performing hard negative pseudo detections mining followed by a weak negative mining strategy, where instance-level scores are computed for each detection considering all neighbouring boxes [21]. In addition, adversarial learning is employed using GRL [11] and a discriminator with the aim of extracting discriminative background features and reducing the domain shift. However, our approach is radically different, as it does not require additional architectural components to the network, but proposes a mixing-based data augmentation strategy to promote regularisation of the model.\n\n\nMethod\n\nThe proposed ConfMix, as illustrated in Figure 2, synthesises an image x M \u2208 R W \u00d7H\u00d7C by mixing a source image x S \u2208 R W \u00d7H\u00d7C and the local region of a target image x T \u2208 R W \u00d7H\u00d7C with the most reliable pseudo detections. We first predict a set of N T pseudo detections y T = \u1ef9 i T |i \u2208 [1, N T ] on the target image and compute the confidence per pseudo detection using the detector network F (\u0398) that is parameterised with \u0398 and is originally trained only on the source data. We opt to follow a Gaussian modelling of the bounding box predictions, instead of the deterministic one, in order to improve the reliability of the detector confidence with the uncertainty of the bounding box prediction. Next, we divide the target image x T into regions of equal size and select the region with the highest average confidence of pseudo detections to mix with the source sample x S , forming the mixed sample x M .\n\nWe pass x T , x S , and x M to the detector F (\u0398) and obtain their corresponding detections\u1ef9 T ,\u1ef9 S and\u1ef9 M , respectively. The detector then learns to adapt to the target domain by imposing a consistency loss L cons which promotes the similarity between\u1ef9 M and the combined detections\u1ef9 S,T by merging the source\u1ef9 S and target\u1ef9 T detections according to how the two sample images are mixed. The supervision of source ground-truth detections y S is achieved with the detector-related loss L det in order to maintain the detector capability during adaptation.\n\nIn the following sections, we describe our proposed ConfMix in details, where we first introduce the estimation of the Gaussian-based detection confidence in Sec. 3.1, followed by the confidence-based region mixing strategy for synthesising training samples in Sec. 3.2 and the progressive pseudo labelling in Sec. 3.3. Finally, we present the training objectives with losses in Sec. 3.4.\n\n\nGaussian-based detection confidence\n\nConventional object detectors, such as YOLO [34], Faster R-CNN [35] and FCOS [41], compute and assign to each detection a confidence score C det \u2208 [0, 1] that is often detector-dependent and is used to filter out unreliable predictions via non-maximum suppression. However, such confidence score does not account for the reliability of the predicted bounding box\nb = [b x , b y , b h , b w ], where [b x , b y ]\nare the position of bounding box on the image and b h and b w represent the height and width, respectively. As suggested in [6], by taking into consideration both the detector-dependent confidence and the confidence that is derived from the uncertainty of bounding box prediction, one can improve the reliability of pseudo detections and reduce the number of false positives.\n\nIn order to compute the bounding box uncertainty, b requires a Gaussian-based modelling. Specifically, for each element in b, the detector model predicts both a mean \u00b5 and a variance \u03a3, where the variance represents the localisation uncertainty. Thus, we can express the Gaussian-based bounding boxb as:\nb = [\u00b5 bx , \u00b5 by , \u00b5 bh , \u00b5 bw , \u03a3 bx , \u03a3 by , \u03a3 bh , \u03a3 bw ] ,(1)\nwhere both the meansb \u00b5 = [\u00b5 bx , \u00b5 by , \u00b5 bh , \u00b5 bw ] and the variancesb \u03a3 = [ \u03a3 bx , \u03a3 by , \u03a3 bh , \u03a3 bw ] are predicted by the detector with an updated regression loss (see details in Sec. 3.4). Note that a sigmoid function \u03c3(\u00b7) is applied to the predicted variance value to ensure its range is between 0 and 1. As a larger variance value implies a higher uncertainty, the confidence of a bounding box is computed as:\nC bbx = 1 \u2212 mean(b \u03a3 ),(2)\nwhere mean(\u00b7) computes the average variance ofb \u03a3 . The combined confidence can thus be computed as:\nC comb = C det \u00b7 C bbx .(3)\n\nConfidence-based Region Mixing\n\nWith the estimated confidence for each pseudo detection on the target image, we design a novel mixing strategy to synthesise new training samples with highly reliable pseudo Figure 2. Overview of the proposed ConfMix method. We pass the source sample xS and target sample xT to the detector model F (\u0398), obtaining the prediction\u1ef9 S and\u1ef9 T , respectively. We select the target region with the highest regional confidence to form the mixed sample xM with the source image, which is then fed to the detector model F (\u0398), producing predictions\u1ef9 M . We train the model with the supervised detection loss using the source annotations y S and the self-supervised consistency loss by comparing\u1ef9 M with the combined source and target predictions\u1ef9 S,T .\n\ndetections. Instead of extracting only pseudo detections or randomly selecting part of the target image [50] to mix, we propose a novel region-level mixing strategy whose synthesised samples contain both the foreground and background features from the two domains, contributing to a more effective adaptation towards the target domain.\n\nSpecifically, we randomly sample a source image x S and a target image x T . The target x T is then passed to the object detector F (\u0398), producing the predictions\u1ef9 T . The target image x T is then evenly divided into 4 regions as shown in Figure 1. Each region is considered to contain a predictio\u00f1 y i T if its centre coordinate resides within the region. The region confidence is computed as the average of the confidences of all the pseudo detections that lie within the region. We select the region with the highest region confidence to mix with the sampled source image x S and generate the synthesised image x M :\nx M = M T x S + (1 \u2212 M T ) x T ,(4)\nwhere M T \u2208 R W \u00d7H is the mask matrix indicating which pixels of the target image should be masked.\n\n\nProgressive Pseudo Labelling\n\nThe correctness of the pseudo detections are tightly related to the confidence metric which is used for filtering the detections. At the early stage of the adaptation, the confidence tends to be less reliable and in general of a lower value due to the large domain gap. Thus, the non-maximum suppression ends up in filtering out most of the pseudo detections if a strict confidence metric, such as C comb , is applied. Therefore, we propose to perform a gradual transition of the confidence metric from a loose to strict manner, to first learn an initial representation of the target domain by allowing more pseudo detections and then gradually shift towards a stricter confidence metric to improve detection accuracy with more reliable pseudo detections.\n\nTo this end, we start with the loose confidence metric C det for filtering the pseudo detections. As iterations continue, we progressively assign more importance to C comb with a shifting weight \u03b4:\nC = (1 \u2212 \u03b4) \u00b7 C det + \u03b4 \u00b7 C comb .(5)\nThe shifting weight \u03b4 varies based on the progress of the training, thus it is dependent on the iteration t, epoch e and the number of batches in one epoch N b . We devise \u03b4 with a non-linear function to gradually increase from 0 to 1:\n\u03b4 = 2 1 + exp(\u2212\u03b1 \u00b7 r) \u2212 1,(6)r = t N b \u00b7 e ,(7)\nwhere r is the ratio of the current iteration to the total number of iterations, with its scale modulated by \u03b1.\n\nThe pseudo detection with a confidence value that is higher than a predefined threshold value C th , i.e. C > C th , is considered a valid detection, and it will be accounted during the confidence-based region mixing and the training for detector adaptation.\n\n\nAdaptive detector training\n\nTo facilitate the adaptive learning of the detector F (\u0398), we rely on two main losses: a self-supervised consistency loss term L cons on the mixed samples and a supervised detector loss term L det on the labelled source samples. L det aims to maintain the task-specific knowledge during adaptation, while the consistency loss L cons aims to adapt towards the target representation by penalising the difference between\u1ef9 M and the combined detections\u1ef9 S,T by merging the source\u1ef9 S and target\u1ef9 T detections based on how the mixed samples are formed.\n\nSpecifically, let\u1ef9 R T be the set of target pseudo detections residing within the selected target region, while\u1ef9 R\u2212 S be the set of source pseudo detections residing outside the selected target region. The combined detections\u1ef9 S,T is the union of the two sets, i.e.\u1ef9 S,T = \u1ef9 R T ,\u1ef9 R\u2212 S . It can happen that the bounding box dimension of\u1ef9 i T \u2208\u1ef9 R T (or\u1ef9 i S \u2208\u1ef9 R\u2212 S ) can exceed the selected target (or source) region, leading to inaccurate pseudo detections. We therefore clip such bounding boxes by their corresponding region boundary.\n\nWe define L cons and L det as L cons = L(\u1ef9 M ,\u1ef9 S,T ) and L det = L(\u1ef9 S , y S ), where both the supervised detection loss L det and the self-supervised consistency loss L cons share the same loss function L(\u00b7). While L det aims to penalise the difference between the predicted detections\u1ef9 S and the ground-truth detections y S on the source samples, L cons aims to penalise the difference between the predicted detections\u1ef9 M and the pseudo detections\u1ef9 S,T on the mixed samples. Note that L(\u00b7) is dependent on the employed object detector. In the case of the one-stage YOLOv5, L(\u00b7) is a combination of three terms: L box is a Complete-IoU (CIoU) loss for regressing the bounding box coordinates, L obj is the Binary Cross Entropy (BCE) loss for the objectness score and L cl is a BCE loss for the classification score.\n\nIn particular, as our predicted bounding box follows a Gaussian modelling, the regression loss per sample image is updated as follows:\nL box = 1 N N i=1 (1 \u2212 mean(N (y i |b i \u00b5 ,b i \u03a3 )),(8)\nwhere N (\u00b7) is the probability density function of a normal distribution for calculating the conditional probability of obtaining the ground-truth y i \u2208 y S for L det , or pseudo detection y i \u2208\u1ef9 S,T for L cons , given the respective mean\u015d b i \u00b5 and variancesb i \u03a3 predicted by the object detector. N represents the total number of y i .\n\nFinally, the total loss is expressed as a weighted sum of L det and L cons :\nL total = L det + \u03b3L cons ,(9)\nwhere \u03b3 is a hyperparameter to balance the supervised and self-supervised terms. The consistency loss L cons can have a greater importance when the pseudo detections are more reliable, and vice versa. We therefore define \u03b3 as the ratio of the number of pseudo detections on x M with confidence greater than C \u03b3 th and the total number of pseudo detections after non-maximum suppression, to reflect the reliability of the pseudo detections:\n\u03b3 = \u1ef9 i S,T \u2208\u1ef9 i S,T : C i \u2265 C \u03b3 th |\u1ef9 S,T | ,(10)\nwhere | \u00b7 | is the cardinality of a set.\n\n\nExperiments\n\nWe evaluate our proposed method ConfMix against state-of-the-art methods on three common benchmark adaptation scenarios, together with extensive ablation studies to prove the effectiveness of our design choices. Datasets. We evaluate our method on the four datasets:\n\n\u2022 Cityscapes [7] is a collection of urban street scenes for semantic understanding. Images were collected in 50 cities over several months, during the day, and in good weather conditions. Single instance annotations are available for the following 8 categories: person, rider, car, truck, bus, train, motorcycle and bicycle. \u2022 FoggyCityscapes [38] is an extension of Cityscapes in which images are augmented by applying a fog filter. FoggyCityscapes includes the same images and 8 categories as Cityscapes. \u2022 Sim10K [19] is a synthetic dataset consisting of 10,000 images derived from the video game Grand Theft Auto V, including only the car category. \u2022 KITTI [13] is a dataset of several hours of traffic video recorded by high-resolution colour and greyscale cameras, containing 7481 training images with annotations provided for 8 categories: car, van, truck, pedestrian, person sitting, cyclist, tram and misc. Following [18], we experiment on the benchmark Cityscapes \u2192 FoggyCityscapes regarding weather adaptation, Sim10K \u2192 Cityscapes regarding synthetic-to-real adaptation, and KITTI \u2192 Cityscapes regarding crosscamera adaptation. In the latter synthetic-to-real and crosscamera adaptations, we only consider the car category, while for Cityscapes \u2192 FoggyCityscapes, we consider the complete 8 categories. Evaluation Metrics. We evaluate our proposed method on the target domain in terms of Average Precision (AP), which is computed by combining precision and recall for each object category separately. We obtain the mean AP (mAP) by averaging the AP across all object categories. Implementation details. We based our experiments on the YOLOv5s architecture for its lightness among the YOLOv5 series, using PyTorch and the default settings. We set the batch size to 2, with each batch containing a source image and a target image of the size of 600 \u00d7 600. In all our experiments, we pre-train the model on the source domain for 20 epochs with the COCO-pretrained weights as initialisation, and perform adaptive learning for 50 epochs. In the non-maximum suppression stage, we set the IoU threshold to 0.5, and the confidence threshold C th to 0.25 for producing pseudo detections. For the computation of \u03b3, we set the confidence threshold C \u03b3 th to 0.5. Please refer details regarding the hyper-parameters in the Supplementary Material.\n\n\nComparisons\n\nWe compare ConfMix against recent state-of-the-art UDA approaches for adaptive object detection on three benchmarks. In particular, we compare with adversarial feature learning methods, such as MGA [53], MeGA-CDA [43], SSOD [36], EPM [18], CDN [39], SAPN [22]; pseudo-label based self-training techniques such as SC-UDA [49], IRL [44], FL-UDA [27], CTRP [52]; and graph reasoning works such as SCAN [25], SIGMA [26], GIPA [46]. We also include \"Source only\", the detector model that is trained only with labelled source data, serving as the performance lower-bound, and \"Oracle\", the detector model that is trained with labelled target data, serving as the performance upper-bound. Result Discussion. Table 1 reports the results of ConfMix and all compared methods in the synthetic-toreal scenario Sim10k \u2192 Cityscapes and the cross-camera scenario KITTI \u2192 Cityscapes. On both benchmarks, our ConfMix scores the new state-of-the-art adaptation performance, achieving +1.7% on Sim10k \u2192 Cityscapes, and +3.7% on KITTI \u2192 Cityscapes in terms of mAP. Table 2 reports the per-class detection performance of ConfMix and all compared methods in the weather adaptation scenario Cityscapes \u2192 FoggyCityscapes. The gap between ConfMix and our upper-bound \"Oracle\" is rather narrow, i.e. -1.5%. Limited by the low \"Oracle\" performance on this benchmark, which is lower than MGA [53], it is quite unlikely our method can outperform existing approaches regarding mAP. Nevertheless, our method achieves a +1% AP gain in the person class compared to SIGMA [26] and a +2% AP gain in the car class compared to MGA [53], which are the approaches obtaining the second best AP in these classes, respectively. \n\n\nAblation study\n\nWe verify the impact of the main design choices of our method with an ablation study on Sim10K \u2192 Cityscapes. Does the confidence-based region mixing help? We anal- yse a variety of different mixing strategies and their impact in terms of object detection performance after adaptation. Specifically, we run our method using the C comb confidence without the proposed progressive pseudo labelling. We vary the mixing strategy with 5 different options (as shown in Fig. 4): CutMix [50] randomly cuts a target region and mixes it with the source image; ConfMix (Vertical Mix) vertically cuts both source and target images in the middle and mixes the most confident target region; ConfMix (Horizontal Mix) horizontally cuts both source and target images in the middle and mixes the most confident target region; ConfMix (2-region Mix) selects the two most confident regions of the target image for mixing; and ConfMix selects only the most confident region of the target image for mixing. We further examine the 4-division scheme of ConfMix by varying the number of divided regions into 6 (2 \u00d7 3) and 9 (3 \u00d7 3), respectively. As can be seen in Table 3, our ConfMix achieves a mAP gain of +5.6% compared to CutMix, meaning that considering the most confident target region to mix is more beneficial than randomly cutting a target region and mixing it with the source image for adaptation. ConfMix shows also a superior performance than ConfMix (Vertical Mix), ConfMix (Horizontal Mix), and ConfMix (2-region Mix). This means that mixing more target regions with the source image can impact the adaptation performance negatively, which might be due to the inclusion of a larger amount of less confident target pseudo detections. Interestingly, we notice that the cutting direction, i.e. vertical or horizontal, impacts the adaptation performance, where a vertical mixing demonstrates a better adaptation performance than the horizontal mixing; we believe that this phenomenon is scenario-dependent. In particular, for datasets concerning autonomous driving scenarios, a vertical mix always includes the road area of both target and source samples, thus being more likely to include objects, while a horizontal mix might only include further scenes where it is less likely to  . Qualitative results on Sim10K \u2192 Cityscapes scenario of ConfMix models trained with different confidence settings. We visualise true positives in blue bbx, false negatives in red bbx and false positives in orange bbx. By introducing a gradual transition between confidence metrics, we achieve less false positive detections compared to training only using C det and less false negative detections compared to training only using C comb . include objects. Finally, our 4-region division scheme leads to the best adaptation performance, with a mAP improvement of +0.6% and +1.2% compared to 6 and 9 divisions, respectively. This is supported by the intuition that smaller regions produce mixed samples containing a larger portion of the source domain, and increase the probability of occluded target objects, thus limiting the adaptive learning of their complete representation. Does the progressive pseudo labelling help? We investigate a set of variants of pseudo labelling in order to verify the proposed progressive pseudo labelling strategy. Specifically, we ablate the usage of only C det or C comb for thresholding the pseudo detections. We also investigate different directions for the weight adjustment, i.e. C det \u2192 C comb and C comb \u2192 C det , as well as different shifting weights with r (in Equation 7) representing a linear decay and \u03b4 (in Equation 6) representing a non-linear decay. Our proposed strategy (C det \u2192 C comb (\u03b4)) gradually shifts from C det to the stricter C comb using the proposed shifting weight \u03b4.\n\nAs shown in Table 4, using only C comb demonstrates to be more advantageous than using only C det for adaptation, and this is mainly due to more reliable pseudo detections. Moreover, we achieve the best result by gradually exploiting from C det to C comb with the non-linear weight \u03b4. A less restrictive confidence metric C det at the early adaptation allows more target pseudo detections and this can help with the target representation learning, while by gradually shifting to the usage of C comb , we improve the reliability of pseudo detection, thus benefiting the detector accuracy. Figure 3 shows qualitative detection results when using only C det , only C comb , and our proposed strategy C det \u2192 C comb (\u03b4). As can be observed, the model is more likely to predict false positives when using only C det , whereas the model using only C comb generates more false negatives. Our proposed gradual transition strategy can better combine the two confidence metrics for adaptive training, achieving the best adaptation performance on the target domain.  Table 4. Target detection accuracy with various confidence-based pseudo labelling.\nConfidence C det C comb C comb C det C comb C det \u2192 C det (r) \u2192 C comb (r) \u2192 C det (\u03b4) \u2192 C comb (\u03b4)\nDoes the weight of consistency loss matter? As pseudo detections are inevitably noisy, we are motivated to weight the consistency loss appropriately in order to avoid the introduction of pseudo detection errors. We therefore investigate how the weight of consistency loss impacts the adaptation performance by using a set of constant weights, in comparison to our dynamic weight \u03b3. Regarding the constant weight, we vary it from 0.2 to 1 in the step of 0.2. As shown in Table 5, using \u03b3 as the consistency weight leads to the best mAP performance, with an improvement of +0.7% compared to the best constant weight of 0.6. Therefore, the use of a dynamic weight whose value varies depending on the pseudo detections confidence, can improve the performance of the model by stabilising the training and mitigating the problem of over-fitting unreliable pseudo detections. Does the number of pseudo detections before mixing matter? We analyse the number of pseudo detections be- fore mixing by retaining with only 25%, 50%, 75% and 100% (i.e. our setting) of the most reliable pseudo detections in\u1ef9 T , with the confidence threshold C th fixed for filtering the detections. As shown in Table 6, decreasing the number of pseudo detections consistently leads to a worse result than using all of them, i.e. our setting. Thanks to our progressive pseudo labelling scheme, most of the false positives could be filtered out already, thus detections with a confidence greater than C th are generally useful for the model to learn the target features.\n\nPseudo detections (%) 25% 50% 75% 100% mAP 45.1 48.9 51.7 56.3 Table 6. Target detection accuracy at varying number of pseudo detections.\n\nDoes sample mixing performs better than simple selftraining? We compare ConfMix with a simple baseline that consists in applying naive finetuning with pseudo detections.\n\nWith the model trained on the source domain, we generate pseudo detections on the target dataset using the same confidence threshold C th used in ConfMix to filter the boxes on top of nonmaximum suppression. We then expand the training dataset containing both source and target samples for further training. With such self-training, we achieve a mAP of 30.5 (Cityscapes\u2192Foggy Cityscapes), 55.4 (Sim10K\u2192Cityscapes) and 46.4 (KITTI\u2192 Cityscapes), which are much inferior than ConfMix as reported in Table 1 and 2, confirming the effectiveness of our proposal.\n\n\nConclusion\n\nWe proposed ConfMix, a novel confidence-based mixing method for adapting object detectors trained on a source domain to a target domain in an unsupervised manner. We introduced a region-level strategy for sample data augmentation by mixing the region of the target image with the most confident pseudo detections with the source image, and achieved adaptation with a consistency loss on the pseudo detections. We also introduced the progressive pseudo labelling scheme by gradually restricting the confidence metric in order to facilitate the smooth transition from learning the target representation to improving detection accuracy. We compared our approach with state-of-the-art methods, demonstrating its superior performance on two benchmarks. As future work, we will apply our method in other practical scenarios, other than autonomous driving, and improve its compatibility with different object detection frameworks.\n\n\nA. Supplementary Material\n\nIn this Supplementary Material, we provide additional experiments to demonstrate the effect of pretrained detector backbone, i.e. with/without COCO-pretrained weights, on the adaptation performance using our proposed ConfMix. Moreover, we also justify key hyperparameter choices in the implementation, including the confidence threshold C th to filter the detections on top of non-maximum suppression, and \u03b1 that is used to scale the impact of the current iteration to the total number of iterations for the calculation of the shifting weight \u03b4. Consistently with the main manuscript, we conduct the ablation study on the Sim10K \u2192 Cityscapes setup. Does backbone initialisation negate the effect of domain adaptation? As there are works using ImageNetpretrained networks [27,22,44,46,25,52,53,26,18] and works that do not specify whether they are pretrained or not [49,43,39,36], we are motivated to examine how the initialisation of our backbone affects the proposed adaptation strategy. Therefore, we experiment with the initialisation of the backbone with random weights. Compared to the setting described in the main manuscript, we only vary the confidence threshold C th used to filter the detections on top of the non-maximum suppression from 0.25 to 0.3 for the random weight setting, in order to account for less reliable predictions at the initial training phase. As shown in Table 7, with random weights initialisation, Source only, ConfMix and Oracle achieve lower performance than their corresponding ones with the COCO pretrained weights. Moreover, we notice that ConfMix with random weights obtains a mAP gain of +12.3% and +28.4% compared to its Source-only counterpart, on Sim10K\u2192Cityscapes and KITTI\u2192Cityscapes, respectively. While with COCOpretrained weights, ConfMix achieves a mAP gain of +6.8% and +12.3% compared to its Source-only counterpart, on Sim10K\u2192Cityscapes and KITTI\u2192Cityscapes, respectively. This shows that the adaptation of ConfMix is more effective when the backbone is not pretrained, although its general detection performance on the target domain is bounded by the Oracle's performance. How does C th for filtering out detections affect adaptation? We experiment with varying confidence threshold C th , i.e. 0.1, 0.25 (our setting), 0.5, and 0.7, to filter the detections on top of non-maximum suppression. The weight used to balance the consistency loss \u03b3 = 1 is fixed throughout this experiment. As shown in Table 8, using C th = 0.5 leads to the best adaptation performance, with a mAP improvement of +10.6% and +10.9% compared to C th = 0.1 and C th = 0.7, respectively. While the use of low values for C th leads the model to keep erroneous pseudo detections, the use of high values filters out those pseudo detections that are useful for the model to learn the target features.\n\nHowever, the use of C th is closely related to C \u03b3 th , which   Table 9. Target detection accuracy with different confidence thresholds C \u03b3 th under C th = 0.25 and C th = 0.5.  is the confidence threshold used to calculate the reliability of pseudo detections \u03b3, defined as the ratio of valid detections after non-maximum suppression with confidence greater than C \u03b3 th . We therefore vary C \u03b3 th from 0.4 to 0.9 under C th = 0.25, and from 0.6 to 0.9 under C th = 0.5 (note that the value of C \u03b3 th should be larger than C th to function meaningfully). From the results reported in Table 9, we can see that the best performance is given by the combination of C th = 0.25 and C \u03b3 th = 0.5, which is our experimental setting reported in the main manuscript. How does \u03b1 for the confidence transition affect adaptation? We analyse the effect of the hyperparameter \u03b1 used to calculate the shifting weight \u03b4 for the smooth transition from C det to C comb during training. As illustrated in Figure 5, a lower value of \u03b1 gives a greater importance to the less strict confidence C det , while a higher value of \u03b1 gives a greater importance to the stricter confidence C comb . As shown in Table 10, using \u03b1 = 5 leads to the best mAP performance, with an improvement of +0.8% compared to \u03b1 = 1, +0.6% compared to \u03b1 = 3, and +0.9% compared to \u03b1 = 10.\n\n\nThis work has been supported by the European Union's Horizon 2020 research and innovation programme under grant agreement No. 957337, and the European Commission Internal Security Fund for Police under grant agreement No. ISFP-2020-AG-PROTECT-101034216-PROTECTOR.\n\nFigure 1 .\n1ConfMix is based on a novel sample mixing strategy which combines the source image and the target region (orange box) with the highest pseudo detection confidence.\n\n\nReal-time analysis. We perform all experiments on a single NVIDIA Tesla V100. At adaptive training time, each epoch took \u223c13 minutes with a batch size of 2, while training YOLOv5 per epoch the time is \u223c6 minutes on Sim10K \u2192 Cityscapes. On the Cityscape dataset, the detection speed of ConfMix is 76 frames per second, almost equal to the 79 frames per second of YOLOv5.\n\nFigure 3\n3Figure 3. Qualitative results on Sim10K \u2192 Cityscapes scenario of ConfMix models trained with different confidence settings. We visualise true positives in blue bbx, false negatives in red bbx and false positives in orange bbx. By introducing a gradual transition between confidence metrics, we achieve less false positive detections compared to training only using C det and less false negative detections compared to training only using C comb .\n\nFigure 4 .\n4Illustration of different mixing strategies. CutMix[50] randomly cuts a target region and mixes it with the source image. ConfMix (Vertical Mix) vertically cuts the source and target image in the middle and mixes the most confident target region. ConfMix (Horizontal Mix) horizontally cuts the source and target image in the middle and mixes the most confident target region. ConfMix (2-region Mix) selects the two most confident regions of the target image for mixing. Finally, ConfMix (6-division), ConfMix (9-division), and ConfMix divides the target image into 6, 9 and 4 regions, respectively, and selects only the most confident target region for mixing.\n\n\nTable 7. Quantitative results (mAP) for Sim10K/KITTI \u2192 Cityscapes benchmark.Table 8. Target detection accuracy with different confidence thresholds C th .Sim10K\u2192 \nKITTI\u2192 \nCityscapes Cityscapes \nMethod \nDetector \nBackbone \nPretrained \nmAP \nmAP \nSource only \nYOLOv5 CSP-Darknet53 \nNo \n33.9 \n21.7 \nConfMix (Ours) YOLOv5 CSP-Darknet53 \nNo \n46.2 \n50.1 \nOracle \nYOLOv5 CSP-Darknet53 \nNo \n64.1 \n64.1 \nSource only \nYOLOv5 CSP-Darknet53 \nCOCO \n49.5 \n39.9 \nConfMix (Ours) YOLOv5 CSP-Darknet53 \nCOCO \n56.3 \n52.2 \nOracle \nYOLOv5 CSP-Darknet53 \nCOCO \n70.3 \n70.3 \n\nC th \n0.1 0.25 0.5 \n0.7 \nmAP 42.1 47.7 52.7 41.8 \n\n\n\nTable 10 .\n10Target detection accuracy with different confidence transition magnitudes.\n\nProgressive feature alignment for unsupervised domain adaptation. Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, Junzhou Huang, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern RecognitionChaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, and Junzhou Huang. Progressive feature alignment for unsupervised do- main adaptation. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 627- 636, 2019.\n\nVisualisation of the evolution of \u03b4 throughout the training iterations with different values of \u03b1. Note that \u03b4 = r is the linear function already ablated in the main manuscript. Figure 5Figure 5. Visualisation of the evolution of \u03b4 throughout the training iterations with different values of \u03b1. Note that \u03b4 = r is the linear function already ablated in the main manuscript.\n\nHarmonizing transferability and discriminability for adapting object detectors. Chaoqi Chen, Zebiao Zheng, Xinghao Ding, Yue Huang, Qi Dou, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern RecognitionChaoqi Chen, Zebiao Zheng, Xinghao Ding, Yue Huang, and Qi Dou. Harmonizing transferability and discriminability for adapting object detectors. In Proceedings of IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 8869-8878, 2020.\n\nTransmix: Attend to mix for vision transformers. Jie-Neng Chen, Shuyang Sun, Ju He, H S Philip, Alan Torr, Song Yuille, Bai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJie-Neng Chen, Shuyang Sun, Ju He, Philip HS Torr, Alan Yuille, and Song Bai. Transmix: Attend to mix for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12135- 12144, 2022.\n\nDomain adaptive faster r-cnn for object detection in the wild. Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionYuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 3339- 3348, 2018.\n\nCrdoco: Pixel-level domain transfer with crossdomain consistency. Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, Jia-Bin Huang, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern RecognitionYun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia- Bin Huang. Crdoco: Pixel-level domain transfer with cross- domain consistency. In Proceedings of IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pages 1791-1800, 2019.\n\nGaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving. Jiwoong Choi, Dayoung Chun, Hyun Kim, Hyuk-Jae Lee, Proceedings of IEEE/CVF International Conference on Computer Vision. IEEE/CVF International Conference on Computer VisionJiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae Lee. Gaussian yolov3: An accurate and fast object detec- tor using localization uncertainty for autonomous driving. In Proceedings of IEEE/CVF International Conference on Computer Vision, pages 502-511, 2019.\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceed- ings of IEEE Conference on Computer Vision and Pattern Recognition, pages 3213-3223, 2016.\n\nKaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks. Advances in neural information processing systems. Jifeng Dai, Yi Li, 29Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks. Ad- vances in neural information processing systems, 29, 2016.\n\nDomain attention consistency for multi-source domain adaptation. Zhongying Deng, Kaiyang Zhou, Yongxin Yang, Tao Xiang, arXiv:2111.03911arXiv preprintZhongying Deng, Kaiyang Zhou, Yongxin Yang, and Tao Xi- ang. Domain attention consistency for multi-source domain adaptation. arXiv preprint arXiv:2111.03911, 2021.\n\nSemi-supervised semantic segmentation needs strong, varied perturbations. Geoff French, Samuli Laine, Timo Aila, Michal Mackiewicz, Graham Finlayson, arXiv:1906.01916arXiv preprintGeoff French, Samuli Laine, Timo Aila, Michal Mackiewicz, and Graham Finlayson. Semi-supervised semantic segmen- tation needs strong, varied perturbations. arXiv preprint arXiv:1906.01916, 2019.\n\nUnsupervised domain adaptation by backpropagation. Yaroslav Ganin, Victor Lempitsky, Proceedings of International Conference on Machine Learning. International Conference on Machine LearningPMLRYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of Interna- tional Conference on Machine Learning, pages 1180-1189. PMLR, 2015.\n\nDomain-adversarial training of neural networks. The journal of machine learning research. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, Victor Lempitsky, 17Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial train- ing of neural networks. The journal of machine learning research, 17(1):2096-2030, 2016.\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. Andreas Geiger, Philip Lenz, Raquel Urtasun, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Proceedings of IEEE Conference on Computer Vi- sion and Pattern Recognition, pages 3354-3361, 2012.\n\nFast r-cnn. Ross Girshick, Proceedings of IEEE International Conference on Computer Vision. IEEE International Conference on Computer VisionRoss Girshick. Fast r-cnn. In Proceedings of IEEE Inter- national Conference on Computer Vision, pages 1440-1448, 2015.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of IEEE Con- ference on Computer Vision and Pattern Recognition, pages 580-587, 2014.\n\nImage-to-image translation for cross-domain disentanglement. Abel Gonzalez-Garcia, Joost Van De, Yoshua Weijer, Bengio, Advances in neural information processing systems. 31Abel Gonzalez-Garcia, Joost Van De Weijer, and Yoshua Bengio. Image-to-image translation for cross-domain dis- entanglement. Advances in neural information processing systems, 31, 2018.\n\nAugmix: A simple data processing method to improve robustness and uncertainty. Dan Hendrycks, Norman Mu, D Ekin, Barret Cubuk, Justin Zoph, Balaji Gilmer, Lakshminarayanan, arXiv:1912.02781arXiv preprintDan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019.\n\nEvery pixel matters: Center-aware feature alignment for domain adaptive object detector. Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, Ming-Hsuan Yang, Proceedings of European Conference on Computer Vision. European Conference on Computer VisionSpringerCheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming- Hsuan Yang. Every pixel matters: Center-aware feature alignment for domain adaptive object detector. In Proceed- ings of European Conference on Computer Vision, pages 733-748. Springer, 2020.\n\nDriving in the matrix: Can virtual worlds replace humangenerated annotations for real world tasks?. Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, arXiv:1610.01983arXiv preprintSharath Nittur Sridhar, Karl Rosaen, and Ram VasudevanMatthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan. Driving in the matrix: Can virtual worlds replace human- generated annotations for real world tasks? arXiv preprint arXiv:1610.01983, 2016.\n\nStructured consistency loss for semi-supervised semantic segmentation. Jongmok Kim, Jooyoung Jang, Hyunwoo Park, arXiv:2001.04647arXiv preprintJongmok Kim, Jooyoung Jang, and Hyunwoo Park. Struc- tured consistency loss for semi-supervised semantic segmen- tation. arXiv preprint arXiv:2001.04647, 2020.\n\nSelf-training and adversarial background regularization for unsupervised domain adaptive one-stage object detection. Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, Changick Kim, Proceedings of IEEE/CVF International Conference on Computer Vision. IEEE/CVF International Conference on Computer VisionSeunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Chang- ick Kim. Self-training and adversarial background regular- ization for unsupervised domain adaptive one-stage object detection. In Proceedings of IEEE/CVF International Con- ference on Computer Vision, pages 6092-6101, 2019.\n\nSpatial attention pyramid network for unsupervised domain adaptation. Congcong Li, Dawei Du, Libo Zhang, Longyin Wen, Tiejian Luo, Yanjun Wu, Pengfei Zhu, Proceedings of European Conference on Computer Vision. European Conference on Computer VisionSpringerCongcong Li, Dawei Du, Libo Zhang, Longyin Wen, Tiejian Luo, Yanjun Wu, and Pengfei Zhu. Spatial attention pyra- mid network for unsupervised domain adaptation. In Pro- ceedings of European Conference on Computer Vision, pages 481-497. Springer, 2020.\n\nCrossdomain adaptive clustering for semi-supervised domain adaptation. Jichang Li, Guanbin Li, Yemin Shi, Yizhou Yu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJichang Li, Guanbin Li, Yemin Shi, and Yizhou Yu. Cross- domain adaptive clustering for semi-supervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\n\nEcacl: A holistic framework for semi-supervised domain adaptation. Kai Li, Chang Liu, Handong Zhao, Yulun Zhang, Yun Fu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionKai Li, Chang Liu, Handong Zhao, Yulun Zhang, and Yun Fu. Ecacl: A holistic framework for semi-supervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.\n\nScan: Cross domain object detection with semantic conditioned adaptation. Wuyang Li, Xinyu Liu, Xiwen Yao, Yixuan Yuan, AAAI. 67Wuyang Li, Xinyu Liu, Xiwen Yao, and Yixuan Yuan. Scan: Cross domain object detection with semantic conditioned adaptation. In AAAI, volume 6, page 7, 2022.\n\nSigma: Semanticcomplete graph matching for domain adaptive object detection. Wuyang Li, Xinyu Liu, Yixuan Yuan, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern RecognitionWuyang Li, Xinyu Liu, and Yixuan Yuan. Sigma: Semantic- complete graph matching for domain adaptive object detec- tion. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5291-5300, 2022.\n\nA free lunch for unsupervised domain adaptive object detection without source data. Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, Yueting Zhuang, Proceedings of Conference on Artificial Intelligence. Conference on Artificial Intelligence35Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsuper- vised domain adaptive object detection without source data. In Proceedings of Conference on Artificial Intelligence, vol- ume 35, pages 8474-8481, 2021.\n\nDomain adaptation with auxiliary target domain-oriented classifier. Jian Liang, Dapeng Hu, Jiashi Feng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJian Liang, Dapeng Hu, and Jiashi Feng. Domain adaptation with auxiliary target domain-oriented classifier. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16632-16642, 2021.\n\nLearning transferable features with deep adaptation networks. Mingsheng Long, Yue Cao, Jianmin Wang, Michael Jordan, Proceedings of International Conference on Machine Learning. International Conference on Machine LearningPMLRMingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor- dan. Learning transferable features with deep adaptation net- works. In Proceedings of International Conference on Ma- chine Learning, pages 97-105. PMLR, 2015.\n\nVirtual mixup training for unsupervised domain adaptation. Xudong Mao, Yun Ma, Zhenguo Yang, Yangbin Chen, Qing Li, arXiv:1905.04215arXiv preprintXudong Mao, Yun Ma, Zhenguo Yang, Yangbin Chen, and Qing Li. Virtual mixup training for unsupervised domain adaptation. arXiv preprint arXiv:1905.04215, 2019.\n\nPixmatch: Unsupervised domain adaptation via pixelwise consistency training. Luke Melas, -Kyriazi , Arjun K Manrai, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern RecognitionLuke Melas-Kyriazi and Arjun K Manrai. Pixmatch: Unsu- pervised domain adaptation via pixelwise consistency train- ing. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12435-12445, 2021.\n\nClassmix: Segmentation-based data augmentation for semi-supervised learning. Viktor Olsson, Wilhelm Tranheden, Juliano Pinto, Lennart Svensson, Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision. IEEE/CVF Winter Conference on Applications of Computer VisionViktor Olsson, Wilhelm Tranheden, Juliano Pinto, and Lennart Svensson. Classmix: Segmentation-based data aug- mentation for semi-supervised learning. In Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1369-1378, 2021.\n\nSaliency grafting: Innocuous attribution-guided mixup with calibrated label mixing. Joonhyung Park, June Yong Yang, Jinwoo Shin, Sung Ju Hwang, Eunho Yang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence36Joonhyung Park, June Yong Yang, Jinwoo Shin, Sung Ju Hwang, and Eunho Yang. Saliency grafting: Innocuous attribution-guided mixup with calibrated label mixing. In Proceedings of the AAAI Conference on Artificial Intelli- gence, volume 36, pages 7957-7965, 2022.\n\nYou only look once: Unified, real-time object detection. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, Proceedings of IEEE conference on computer vision and pattern recognition. IEEE conference on computer vision and pattern recognitionJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detec- tion. In Proceedings of IEEE conference on computer vision and pattern recognition, pages 779-788, 2016.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in neural information processing systems. 28Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information process- ing systems, 28, 2015.\n\nSeeking similarities over differences: Similarity-based domain alignment for adaptive object detection. Rakshith Farzaneh Rezaeianaran, Rahaf Shetty, Daniel Aljundi, Shanshan Olmeda Reino, Bernt Zhang, Schiele, Proceedings of IEEE/CVF International Conference on Computer Vision. IEEE/CVF International Conference on Computer VisionFarzaneh Rezaeianaran, Rakshith Shetty, Rahaf Aljundi, Daniel Olmeda Reino, Shanshan Zhang, and Bernt Schiele. Seeking similarities over differences: Similarity-based do- main alignment for adaptive object detection. In Proceedings of IEEE/CVF International Conference on Computer Vision, pages 9204-9213, 2021.\n\nStrong-weak distribution alignment for adaptive object detection. Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, Kate Saenko, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern RecognitionKuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Strong-weak distribution alignment for adaptive object detection. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6956- 6965, 2019.\n\nSemantic foggy scene understanding with synthetic data. Christos Sakaridis, Dengxin Dai, Luc Van Gool, International Journal of Computer Vision. 1269Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Seman- tic foggy scene understanding with synthetic data. Interna- tional Journal of Computer Vision, 126(9):973-992, 2018.\n\nAdapting object detectors with conditional domain normalization. Peng Su, Kun Wang, Xingyu Zeng, Shixiang Tang, Dapeng Chen, Di Qiu, Xiaogang Wang, Proceedings of European Conference on Computer Vision. European Conference on Computer VisionSpringerPeng Su, Kun Wang, Xingyu Zeng, Shixiang Tang, Dapeng Chen, Di Qiu, and Xiaogang Wang. Adapting object detec- tors with conditional domain normalization. In Proceedings of European Conference on Computer Vision, pages 403- 419. Springer, 2020.\n\nDeep coral: Correlation alignment for deep domain adaptation. Baochen Sun, Kate Saenko, Proceedings of European Conference on Computer Vision. European Conference on Computer VisionBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Proceedings of European Conference on Computer Vision, pages 443-450.\n\n. Springer, Springer, 2016.\n\nFcos: Fully convolutional one-stage object detection. Zhi Tian, Chunhua Shen, Hao Chen, Tong He, Proceedings of IEEE/CVF International Conference on Computer Vision. IEEE/CVF International Conference on Computer VisionZhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceed- ings of IEEE/CVF International Conference on Computer Vi- sion, pages 9627-9636, 2019.\n\nAdversarial discriminative domain adaptation. Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionEric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceed- ings of IEEE Conference on Computer Vision and Pattern Recognition, pages 7167-7176, 2017.\n\nMega-cda: Memory guided attention for category-aware unsupervised domain adaptive object detection. Vibashan Vs, Vikram Gupta, Poojan Oza, A Vishwanath, Sindagi, Patel, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern RecognitionVibashan Vs, Vikram Gupta, Poojan Oza, Vishwanath A Sindagi, and Vishal M Patel. Mega-cda: Memory guided attention for category-aware unsupervised domain adaptive object detection. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4516- 4526, 2021.\n\nInstance relation graph guided source-free domain adaptive object detection. V S Vibashan, Poojan Oza, Patel, arXiv:2203.15793arXiv preprintVibashan VS, Poojan Oza, and Vishal M Patel. Instance re- lation graph guided source-free domain adaptive object de- tection. arXiv preprint arXiv:2203.15793, 2022.\n\nDual mixup regularized learning for adversarial domain adaptation. Yuan Wu, Diana Inkpen, Ahmed El-Roby, Proceedings of European Conference on Computer Vision. European Conference on Computer VisionSpringerYuan Wu, Diana Inkpen, and Ahmed El-Roby. Dual mixup regularized learning for adversarial domain adaptation. In Proceedings of European Conference on Computer Vision, pages 540-555. Springer, 2020.\n\nCross-domain detection via graph-induced prototype alignment. Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, Wenjun Zhang, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern RecognitionMinghao Xu, Hang Wang, Bingbing Ni, Qi Tian, and Wen- jun Zhang. Cross-domain detection via graph-induced pro- totype alignment. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12355- 12364, 2020.\n\nAdversarial domain adaptation with domain mixup. Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, Wenjun Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang. Adversarial domain adaptation with domain mixup. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 6502-6509, 2020.\n\nSemisupervised domain adaptation via sample-to-sample selfdistillation. Jeongbeen Yoon, Dahyun Kang, Minsu Cho, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionJeongbeen Yoon, Dahyun Kang, and Minsu Cho. Semi- supervised domain adaptation via sample-to-sample self- distillation. In Proceedings of the IEEE/CVF Winter Confer- ence on Applications of Computer Vision, pages 1978-1987, 2022.\n\nSc-uda: Style and content gaps aware unsupervised domain adaptation for object detection. Fuxun Yu, Di Wang, Yinpeng Chen, Nikolaos Karianakis, Tong Shen, Pei Yu, Dimitrios Lymberopoulos, Sidi Lu, Weisong Shi, Xiang Chen, Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)Fuxun Yu, Di Wang, Yinpeng Chen, Nikolaos Karianakis, Tong Shen, Pei Yu, Dimitrios Lymberopoulos, Sidi Lu, Weisong Shi, and Xiang Chen. Sc-uda: Style and content gaps aware unsupervised domain adaptation for object de- tection. In Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1061-1070, 2022.\n\nCutmix: Regularization strategy to train strong classifiers with localizable features. Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe, Yoo, Proceedings of IEEE/CVF International Conference on Computer Vision. IEEE/CVF International Conference on Computer VisionSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu- larization strategy to train strong classifiers with localizable features. In Proceedings of IEEE/CVF International Confer- ence on Computer Vision, pages 6023-6032, 2019.\n\nHongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, arXiv:1710.09412mixup: Beyond empirical risk minimization. arXiv preprintHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017.\n\nCollaborative training between region proposal localization and classification for domain adaptive object detection. Ganlong Zhao, Guanbin Li, Ruijia Xu, Liang Lin, Proceedings of European Conference on Computer Vision. European Conference on Computer VisionSpringerGanlong Zhao, Guanbin Li, Ruijia Xu, and Liang Lin. Col- laborative training between region proposal localization and classification for domain adaptive object detection. In Pro- ceedings of European Conference on Computer Vision, pages 86-102. Springer, 2020.\n\nMulti-granularity alignment domain adaptation for object detection. Wenzhang Zhou, Dawei Du, Libo Zhang, Tiejian Luo, Yanjun Wu, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern RecognitionWenzhang Zhou, Dawei Du, Libo Zhang, Tiejian Luo, and Yanjun Wu. Multi-granularity alignment domain adaptation for object detection. In Proceedings of IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pages 9581-9590, 2022.\n\nUnpaired image-to-image translation using cycleconsistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, Proceedings of IEEE International Conference on Computer Vision. IEEE International Conference on Computer VisionJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle- consistent adversarial networks. In Proceedings of IEEE International Conference on Computer Vision, pages 2223- 2232, 2017.\n\nAdapting object detectors via selective crossdomain alignment. Xinge Zhu, Jiangmiao Pang, Ceyuan Yang, Jianping Shi, Dahua Lin, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern RecognitionXinge Zhu, Jiangmiao Pang, Ceyuan Yang, Jianping Shi, and Dahua Lin. Adapting object detectors via selective cross- domain alignment. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 687- 696, 2019.\n", "annotations": {"author": "[{\"end\":142,\"start\":92},{\"end\":210,\"start\":143},{\"end\":295,\"start\":211},{\"end\":346,\"start\":296}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":99},{\"end\":155,\"start\":148},{\"end\":222,\"start\":217},{\"end\":307,\"start\":303}]", "author_first_name": "[{\"end\":98,\"start\":92},{\"end\":147,\"start\":143},{\"end\":216,\"start\":211},{\"end\":302,\"start\":296}]", "author_affiliation": "[{\"end\":141,\"start\":109},{\"end\":209,\"start\":173},{\"end\":256,\"start\":224},{\"end\":294,\"start\":258},{\"end\":345,\"start\":309}]", "title": "[{\"end\":89,\"start\":1},{\"end\":435,\"start\":347}]", "venue": null, "abstract": "[{\"end\":1699,\"start\":437}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2142,\"start\":2138},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2145,\"start\":2142},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2148,\"start\":2145},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2151,\"start\":2148},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2409,\"start\":2406},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2663,\"start\":2659},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2666,\"start\":2663},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2668,\"start\":2666},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2671,\"start\":2668},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2673,\"start\":2671},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2809,\"start\":2805},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2874,\"start\":2871},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2877,\"start\":2874},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2880,\"start\":2877},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2883,\"start\":2880},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2886,\"start\":2883},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3024,\"start\":3020},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3027,\"start\":3024},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3030,\"start\":3027},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3698,\"start\":3694},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3701,\"start\":3698},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3704,\"start\":3701},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3786,\"start\":3782},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3789,\"start\":3786},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3820,\"start\":3816},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3823,\"start\":3820},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3826,\"start\":3823},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3828,\"start\":3826},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5048,\"start\":5045},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5592,\"start\":5589},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5615,\"start\":5611},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5628,\"start\":5624},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5656,\"start\":5652},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6615,\"start\":6611},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6629,\"start\":6625},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6920,\"start\":6916},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7210,\"start\":7206},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7213,\"start\":7210},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7215,\"start\":7213},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7650,\"start\":7646},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7653,\"start\":7650},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7704,\"start\":7700},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7707,\"start\":7704},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7710,\"start\":7707},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7980,\"start\":7976},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7983,\"start\":7980},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7986,\"start\":7983},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8055,\"start\":8051},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8138,\"start\":8134},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8153,\"start\":8149},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8203,\"start\":8199},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8206,\"start\":8203},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8332,\"start\":8329},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8335,\"start\":8332},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8519,\"start\":8515},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8522,\"start\":8519},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8525,\"start\":8522},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8527,\"start\":8525},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8530,\"start\":8527},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8777,\"start\":8774},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8848,\"start\":8844},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8991,\"start\":8987},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9127,\"start\":9123},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9284,\"start\":9281},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9402,\"start\":9398},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9608,\"start\":9604},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9714,\"start\":9710},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9855,\"start\":9851},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10005,\"start\":10001},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10008,\"start\":10005},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10011,\"start\":10008},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10023,\"start\":10019},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10333,\"start\":10329},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10395,\"start\":10391},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12679,\"start\":12675},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12698,\"start\":12694},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12712,\"start\":12708},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13170,\"start\":13167},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15252,\"start\":15248},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21324,\"start\":21321},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21655,\"start\":21651},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21828,\"start\":21824},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21973,\"start\":21969},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22238,\"start\":22234},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":23871,\"start\":23867},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23886,\"start\":23882},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23897,\"start\":23893},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23907,\"start\":23903},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23917,\"start\":23913},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23928,\"start\":23924},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23993,\"start\":23989},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24003,\"start\":23999},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24016,\"start\":24012},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":24027,\"start\":24023},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24072,\"start\":24068},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24084,\"start\":24080},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24095,\"start\":24091},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25037,\"start\":25033},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25211,\"start\":25207},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25267,\"start\":25263},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25856,\"start\":25852},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34562,\"start\":34558},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34565,\"start\":34562},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":34568,\"start\":34565},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":34571,\"start\":34568},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34574,\"start\":34571},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":34577,\"start\":34574},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":34580,\"start\":34577},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34583,\"start\":34580},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34586,\"start\":34583},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":34656,\"start\":34652},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":34659,\"start\":34656},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34662,\"start\":34659},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34665,\"start\":34662},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":39293,\"start\":39289}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38217,\"start\":37952},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38394,\"start\":38218},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38766,\"start\":38395},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39224,\"start\":38767},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39898,\"start\":39225},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":40503,\"start\":39899},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":40592,\"start\":40504}]", "paragraph": "[{\"end\":2410,\"start\":1715},{\"end\":3443,\"start\":2412},{\"end\":3947,\"start\":3445},{\"end\":5748,\"start\":3949},{\"end\":5795,\"start\":5750},{\"end\":6426,\"start\":5797},{\"end\":10724,\"start\":6443},{\"end\":11643,\"start\":10735},{\"end\":12201,\"start\":11645},{\"end\":12591,\"start\":12203},{\"end\":12993,\"start\":12631},{\"end\":13418,\"start\":13043},{\"end\":13723,\"start\":13420},{\"end\":14209,\"start\":13790},{\"end\":14337,\"start\":14237},{\"end\":15142,\"start\":14399},{\"end\":15479,\"start\":15144},{\"end\":16100,\"start\":15481},{\"end\":16236,\"start\":16137},{\"end\":17024,\"start\":16269},{\"end\":17223,\"start\":17026},{\"end\":17497,\"start\":17262},{\"end\":17657,\"start\":17546},{\"end\":17917,\"start\":17659},{\"end\":18494,\"start\":17948},{\"end\":19034,\"start\":18496},{\"end\":19853,\"start\":19036},{\"end\":19989,\"start\":19855},{\"end\":20383,\"start\":20046},{\"end\":20461,\"start\":20385},{\"end\":20932,\"start\":20493},{\"end\":21024,\"start\":20984},{\"end\":21306,\"start\":21040},{\"end\":23653,\"start\":21308},{\"end\":25355,\"start\":23669},{\"end\":29171,\"start\":25374},{\"end\":30311,\"start\":29173},{\"end\":31951,\"start\":30412},{\"end\":32090,\"start\":31953},{\"end\":32261,\"start\":32092},{\"end\":32819,\"start\":32263},{\"end\":33757,\"start\":32834},{\"end\":36609,\"start\":33787},{\"end\":37951,\"start\":36611}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13042,\"start\":12994},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13789,\"start\":13724},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14236,\"start\":14210},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14365,\"start\":14338},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16136,\"start\":16101},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17261,\"start\":17224},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17527,\"start\":17498},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17545,\"start\":17527},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20045,\"start\":19990},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20492,\"start\":20462},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20983,\"start\":20933},{\"attributes\":{\"id\":\"formula_11\"},\"end\":30411,\"start\":30312}]", "table_ref": "[{\"end\":24377,\"start\":24370},{\"end\":24721,\"start\":24714},{\"end\":26520,\"start\":26513},{\"end\":29192,\"start\":29185},{\"end\":30236,\"start\":30229},{\"end\":30889,\"start\":30882},{\"end\":31601,\"start\":31594},{\"end\":32023,\"start\":32016},{\"end\":32766,\"start\":32759},{\"end\":35179,\"start\":35172},{\"end\":36243,\"start\":36236},{\"end\":36682,\"start\":36675},{\"end\":37202,\"start\":37195},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":37800,\"start\":37792}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1713,\"start\":1701},{\"attributes\":{\"n\":\"2.\"},\"end\":6441,\"start\":6429},{\"attributes\":{\"n\":\"3.\"},\"end\":10733,\"start\":10727},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12629,\"start\":12594},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14397,\"start\":14367},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16267,\"start\":16239},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17946,\"start\":17920},{\"attributes\":{\"n\":\"4.\"},\"end\":21038,\"start\":21027},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23667,\"start\":23656},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25372,\"start\":25358},{\"attributes\":{\"n\":\"5.\"},\"end\":32832,\"start\":32822},{\"end\":33785,\"start\":33760},{\"end\":38229,\"start\":38219},{\"end\":38776,\"start\":38768},{\"end\":39236,\"start\":39226},{\"end\":40515,\"start\":40505}]", "table": "[{\"end\":40503,\"start\":40055}]", "figure_caption": "[{\"end\":38217,\"start\":37954},{\"end\":38394,\"start\":38231},{\"end\":38766,\"start\":38397},{\"end\":39224,\"start\":38778},{\"end\":39898,\"start\":39238},{\"end\":40055,\"start\":39901},{\"end\":40592,\"start\":40518}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4184,\"start\":4176},{\"end\":10783,\"start\":10775},{\"end\":14581,\"start\":14573},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15728,\"start\":15720},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25843,\"start\":25836},{\"end\":29005,\"start\":28995},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29769,\"start\":29761},{\"end\":37605,\"start\":37597}]", "bib_author_first_name": "[{\"end\":40666,\"start\":40660},{\"end\":40680,\"start\":40673},{\"end\":40693,\"start\":40686},{\"end\":40703,\"start\":40701},{\"end\":40717,\"start\":40710},{\"end\":40727,\"start\":40724},{\"end\":40743,\"start\":40735},{\"end\":40755,\"start\":40748},{\"end\":41645,\"start\":41639},{\"end\":41658,\"start\":41652},{\"end\":41673,\"start\":41666},{\"end\":41683,\"start\":41680},{\"end\":41693,\"start\":41691},{\"end\":42149,\"start\":42141},{\"end\":42163,\"start\":42156},{\"end\":42171,\"start\":42169},{\"end\":42177,\"start\":42176},{\"end\":42179,\"start\":42178},{\"end\":42192,\"start\":42188},{\"end\":42203,\"start\":42199},{\"end\":42674,\"start\":42669},{\"end\":42684,\"start\":42681},{\"end\":42697,\"start\":42689},{\"end\":42716,\"start\":42709},{\"end\":42725,\"start\":42722},{\"end\":43182,\"start\":43174},{\"end\":43195,\"start\":43189},{\"end\":43211,\"start\":43201},{\"end\":43225,\"start\":43218},{\"end\":43730,\"start\":43723},{\"end\":43744,\"start\":43737},{\"end\":43755,\"start\":43751},{\"end\":43769,\"start\":43761},{\"end\":44226,\"start\":44220},{\"end\":44242,\"start\":44235},{\"end\":44259,\"start\":44250},{\"end\":44271,\"start\":44267},{\"end\":44287,\"start\":44281},{\"end\":44306,\"start\":44299},{\"end\":44320,\"start\":44317},{\"end\":44335,\"start\":44329},{\"end\":44347,\"start\":44342},{\"end\":44952,\"start\":44946},{\"end\":44960,\"start\":44958},{\"end\":45221,\"start\":45212},{\"end\":45235,\"start\":45228},{\"end\":45249,\"start\":45242},{\"end\":45259,\"start\":45256},{\"end\":45542,\"start\":45537},{\"end\":45557,\"start\":45551},{\"end\":45569,\"start\":45565},{\"end\":45582,\"start\":45576},{\"end\":45601,\"start\":45595},{\"end\":45898,\"start\":45890},{\"end\":45912,\"start\":45906},{\"end\":46315,\"start\":46307},{\"end\":46331,\"start\":46323},{\"end\":46346,\"start\":46342},{\"end\":46361,\"start\":46355},{\"end\":46375,\"start\":46371},{\"end\":46396,\"start\":46388},{\"end\":46414,\"start\":46409},{\"end\":46431,\"start\":46425},{\"end\":46781,\"start\":46774},{\"end\":46796,\"start\":46790},{\"end\":46809,\"start\":46803},{\"end\":47192,\"start\":47188},{\"end\":47523,\"start\":47519},{\"end\":47538,\"start\":47534},{\"end\":47554,\"start\":47548},{\"end\":47572,\"start\":47564},{\"end\":48027,\"start\":48023},{\"end\":48050,\"start\":48045},{\"end\":48065,\"start\":48059},{\"end\":48404,\"start\":48401},{\"end\":48422,\"start\":48416},{\"end\":48428,\"start\":48427},{\"end\":48441,\"start\":48435},{\"end\":48455,\"start\":48449},{\"end\":48468,\"start\":48462},{\"end\":48840,\"start\":48830},{\"end\":48854,\"start\":48846},{\"end\":48867,\"start\":48861},{\"end\":48883,\"start\":48873},{\"end\":49344,\"start\":49337},{\"end\":49370,\"start\":49363},{\"end\":49384,\"start\":49378},{\"end\":49806,\"start\":49799},{\"end\":49820,\"start\":49812},{\"end\":49834,\"start\":49827},{\"end\":50159,\"start\":50149},{\"end\":50172,\"start\":50165},{\"end\":50187,\"start\":50179},{\"end\":50201,\"start\":50193},{\"end\":50687,\"start\":50679},{\"end\":50697,\"start\":50692},{\"end\":50706,\"start\":50702},{\"end\":50721,\"start\":50714},{\"end\":50734,\"start\":50727},{\"end\":50746,\"start\":50740},{\"end\":50758,\"start\":50751},{\"end\":51196,\"start\":51189},{\"end\":51208,\"start\":51201},{\"end\":51218,\"start\":51213},{\"end\":51230,\"start\":51224},{\"end\":51670,\"start\":51667},{\"end\":51680,\"start\":51675},{\"end\":51693,\"start\":51686},{\"end\":51705,\"start\":51700},{\"end\":51716,\"start\":51713},{\"end\":52138,\"start\":52132},{\"end\":52148,\"start\":52143},{\"end\":52159,\"start\":52154},{\"end\":52171,\"start\":52165},{\"end\":52427,\"start\":52421},{\"end\":52437,\"start\":52432},{\"end\":52449,\"start\":52443},{\"end\":52915,\"start\":52907},{\"end\":52926,\"start\":52920},{\"end\":52935,\"start\":52933},{\"end\":52947,\"start\":52941},{\"end\":52958,\"start\":52954},{\"end\":52973,\"start\":52965},{\"end\":52985,\"start\":52978},{\"end\":53430,\"start\":53426},{\"end\":53444,\"start\":53438},{\"end\":53455,\"start\":53449},{\"end\":53904,\"start\":53895},{\"end\":53914,\"start\":53911},{\"end\":53927,\"start\":53920},{\"end\":53941,\"start\":53934},{\"end\":54342,\"start\":54336},{\"end\":54351,\"start\":54348},{\"end\":54363,\"start\":54356},{\"end\":54377,\"start\":54370},{\"end\":54388,\"start\":54384},{\"end\":54664,\"start\":54660},{\"end\":54680,\"start\":54672},{\"end\":54690,\"start\":54683},{\"end\":55151,\"start\":55145},{\"end\":55167,\"start\":55160},{\"end\":55186,\"start\":55179},{\"end\":55201,\"start\":55194},{\"end\":55699,\"start\":55690},{\"end\":55710,\"start\":55706},{\"end\":55715,\"start\":55711},{\"end\":55728,\"start\":55722},{\"end\":55739,\"start\":55735},{\"end\":55742,\"start\":55740},{\"end\":55755,\"start\":55750},{\"end\":56199,\"start\":56193},{\"end\":56215,\"start\":56208},{\"end\":56229,\"start\":56225},{\"end\":56243,\"start\":56240},{\"end\":56696,\"start\":56689},{\"end\":56715,\"start\":56711},{\"end\":56724,\"start\":56720},{\"end\":57104,\"start\":57096},{\"end\":57133,\"start\":57128},{\"end\":57148,\"start\":57142},{\"end\":57166,\"start\":57158},{\"end\":57186,\"start\":57181},{\"end\":57710,\"start\":57703},{\"end\":57727,\"start\":57718},{\"end\":57743,\"start\":57736},{\"end\":57756,\"start\":57752},{\"end\":58209,\"start\":58201},{\"end\":58228,\"start\":58221},{\"end\":58237,\"start\":58234},{\"end\":58539,\"start\":58535},{\"end\":58547,\"start\":58544},{\"end\":58560,\"start\":58554},{\"end\":58575,\"start\":58567},{\"end\":58588,\"start\":58582},{\"end\":58597,\"start\":58595},{\"end\":58611,\"start\":58603},{\"end\":59033,\"start\":59026},{\"end\":59043,\"start\":59039},{\"end\":59396,\"start\":59393},{\"end\":59410,\"start\":59403},{\"end\":59420,\"start\":59417},{\"end\":59431,\"start\":59427},{\"end\":59808,\"start\":59804},{\"end\":59820,\"start\":59816},{\"end\":59834,\"start\":59830},{\"end\":59849,\"start\":59843},{\"end\":60309,\"start\":60301},{\"end\":60320,\"start\":60314},{\"end\":60334,\"start\":60328},{\"end\":60341,\"start\":60340},{\"end\":60877,\"start\":60876},{\"end\":60879,\"start\":60878},{\"end\":60896,\"start\":60890},{\"end\":61176,\"start\":61172},{\"end\":61186,\"start\":61181},{\"end\":61200,\"start\":61195},{\"end\":61579,\"start\":61572},{\"end\":61588,\"start\":61584},{\"end\":61603,\"start\":61595},{\"end\":61610,\"start\":61608},{\"end\":61623,\"start\":61617},{\"end\":62066,\"start\":62059},{\"end\":62075,\"start\":62071},{\"end\":62091,\"start\":62083},{\"end\":62100,\"start\":62096},{\"end\":62113,\"start\":62105},{\"end\":62122,\"start\":62120},{\"end\":62135,\"start\":62129},{\"end\":62573,\"start\":62564},{\"end\":62586,\"start\":62580},{\"end\":62598,\"start\":62593},{\"end\":63077,\"start\":63072},{\"end\":63084,\"start\":63082},{\"end\":63098,\"start\":63091},{\"end\":63113,\"start\":63105},{\"end\":63130,\"start\":63126},{\"end\":63140,\"start\":63137},{\"end\":63154,\"start\":63145},{\"end\":63174,\"start\":63170},{\"end\":63186,\"start\":63179},{\"end\":63197,\"start\":63192},{\"end\":63791,\"start\":63784},{\"end\":63805,\"start\":63797},{\"end\":63819,\"start\":63811},{\"end\":63841,\"start\":63835},{\"end\":63857,\"start\":63848},{\"end\":64272,\"start\":64266},{\"end\":64289,\"start\":64280},{\"end\":64302,\"start\":64297},{\"end\":64680,\"start\":64673},{\"end\":64694,\"start\":64687},{\"end\":64705,\"start\":64699},{\"end\":64715,\"start\":64710},{\"end\":65160,\"start\":65152},{\"end\":65172,\"start\":65167},{\"end\":65181,\"start\":65177},{\"end\":65196,\"start\":65189},{\"end\":65208,\"start\":65202},{\"end\":65682,\"start\":65675},{\"end\":65695,\"start\":65688},{\"end\":65709,\"start\":65702},{\"end\":65723,\"start\":65717},{\"end\":65725,\"start\":65724},{\"end\":66151,\"start\":66146},{\"end\":66166,\"start\":66157},{\"end\":66179,\"start\":66173},{\"end\":66194,\"start\":66186},{\"end\":66205,\"start\":66200}]", "bib_author_last_name": "[{\"end\":40671,\"start\":40667},{\"end\":40684,\"start\":40681},{\"end\":40699,\"start\":40694},{\"end\":40708,\"start\":40704},{\"end\":40722,\"start\":40718},{\"end\":40733,\"start\":40728},{\"end\":40746,\"start\":40744},{\"end\":40761,\"start\":40756},{\"end\":41650,\"start\":41646},{\"end\":41664,\"start\":41659},{\"end\":41678,\"start\":41674},{\"end\":41689,\"start\":41684},{\"end\":41697,\"start\":41694},{\"end\":42154,\"start\":42150},{\"end\":42167,\"start\":42164},{\"end\":42174,\"start\":42172},{\"end\":42186,\"start\":42180},{\"end\":42197,\"start\":42193},{\"end\":42210,\"start\":42204},{\"end\":42215,\"start\":42212},{\"end\":42679,\"start\":42675},{\"end\":42687,\"start\":42685},{\"end\":42707,\"start\":42698},{\"end\":42720,\"start\":42717},{\"end\":42734,\"start\":42726},{\"end\":43187,\"start\":43183},{\"end\":43199,\"start\":43196},{\"end\":43216,\"start\":43212},{\"end\":43231,\"start\":43226},{\"end\":43735,\"start\":43731},{\"end\":43749,\"start\":43745},{\"end\":43759,\"start\":43756},{\"end\":43773,\"start\":43770},{\"end\":44233,\"start\":44227},{\"end\":44248,\"start\":44243},{\"end\":44265,\"start\":44260},{\"end\":44279,\"start\":44272},{\"end\":44297,\"start\":44288},{\"end\":44315,\"start\":44307},{\"end\":44327,\"start\":44321},{\"end\":44340,\"start\":44336},{\"end\":44355,\"start\":44348},{\"end\":44956,\"start\":44953},{\"end\":44963,\"start\":44961},{\"end\":45226,\"start\":45222},{\"end\":45240,\"start\":45236},{\"end\":45254,\"start\":45250},{\"end\":45265,\"start\":45260},{\"end\":45549,\"start\":45543},{\"end\":45563,\"start\":45558},{\"end\":45574,\"start\":45570},{\"end\":45593,\"start\":45583},{\"end\":45611,\"start\":45602},{\"end\":45904,\"start\":45899},{\"end\":45922,\"start\":45913},{\"end\":46321,\"start\":46316},{\"end\":46340,\"start\":46332},{\"end\":46353,\"start\":46347},{\"end\":46369,\"start\":46362},{\"end\":46386,\"start\":46376},{\"end\":46407,\"start\":46397},{\"end\":46423,\"start\":46415},{\"end\":46441,\"start\":46432},{\"end\":46788,\"start\":46782},{\"end\":46801,\"start\":46797},{\"end\":46817,\"start\":46810},{\"end\":47201,\"start\":47193},{\"end\":47532,\"start\":47524},{\"end\":47546,\"start\":47539},{\"end\":47562,\"start\":47555},{\"end\":47578,\"start\":47573},{\"end\":48043,\"start\":48028},{\"end\":48057,\"start\":48051},{\"end\":48072,\"start\":48066},{\"end\":48080,\"start\":48074},{\"end\":48414,\"start\":48405},{\"end\":48425,\"start\":48423},{\"end\":48433,\"start\":48429},{\"end\":48447,\"start\":48442},{\"end\":48460,\"start\":48456},{\"end\":48475,\"start\":48469},{\"end\":48493,\"start\":48477},{\"end\":48844,\"start\":48841},{\"end\":48859,\"start\":48855},{\"end\":48871,\"start\":48868},{\"end\":48888,\"start\":48884},{\"end\":49361,\"start\":49345},{\"end\":49376,\"start\":49371},{\"end\":49390,\"start\":49385},{\"end\":49810,\"start\":49807},{\"end\":49825,\"start\":49821},{\"end\":49839,\"start\":49835},{\"end\":50163,\"start\":50160},{\"end\":50177,\"start\":50173},{\"end\":50191,\"start\":50188},{\"end\":50205,\"start\":50202},{\"end\":50690,\"start\":50688},{\"end\":50700,\"start\":50698},{\"end\":50712,\"start\":50707},{\"end\":50725,\"start\":50722},{\"end\":50738,\"start\":50735},{\"end\":50749,\"start\":50747},{\"end\":50762,\"start\":50759},{\"end\":51199,\"start\":51197},{\"end\":51211,\"start\":51209},{\"end\":51222,\"start\":51219},{\"end\":51233,\"start\":51231},{\"end\":51673,\"start\":51671},{\"end\":51684,\"start\":51681},{\"end\":51698,\"start\":51694},{\"end\":51711,\"start\":51706},{\"end\":51719,\"start\":51717},{\"end\":52141,\"start\":52139},{\"end\":52152,\"start\":52149},{\"end\":52163,\"start\":52160},{\"end\":52176,\"start\":52172},{\"end\":52430,\"start\":52428},{\"end\":52441,\"start\":52438},{\"end\":52454,\"start\":52450},{\"end\":52918,\"start\":52916},{\"end\":52931,\"start\":52927},{\"end\":52939,\"start\":52936},{\"end\":52952,\"start\":52948},{\"end\":52963,\"start\":52959},{\"end\":52976,\"start\":52974},{\"end\":52992,\"start\":52986},{\"end\":53436,\"start\":53431},{\"end\":53447,\"start\":53445},{\"end\":53460,\"start\":53456},{\"end\":53909,\"start\":53905},{\"end\":53918,\"start\":53915},{\"end\":53932,\"start\":53928},{\"end\":53948,\"start\":53942},{\"end\":54346,\"start\":54343},{\"end\":54354,\"start\":54352},{\"end\":54368,\"start\":54364},{\"end\":54382,\"start\":54378},{\"end\":54391,\"start\":54389},{\"end\":54670,\"start\":54665},{\"end\":54697,\"start\":54691},{\"end\":55158,\"start\":55152},{\"end\":55177,\"start\":55168},{\"end\":55192,\"start\":55187},{\"end\":55210,\"start\":55202},{\"end\":55704,\"start\":55700},{\"end\":55720,\"start\":55716},{\"end\":55733,\"start\":55729},{\"end\":55748,\"start\":55743},{\"end\":55760,\"start\":55756},{\"end\":56206,\"start\":56200},{\"end\":56223,\"start\":56216},{\"end\":56238,\"start\":56230},{\"end\":56251,\"start\":56244},{\"end\":56709,\"start\":56697},{\"end\":56718,\"start\":56716},{\"end\":56733,\"start\":56725},{\"end\":56738,\"start\":56735},{\"end\":57126,\"start\":57105},{\"end\":57140,\"start\":57134},{\"end\":57156,\"start\":57149},{\"end\":57179,\"start\":57167},{\"end\":57192,\"start\":57187},{\"end\":57201,\"start\":57194},{\"end\":57716,\"start\":57711},{\"end\":57734,\"start\":57728},{\"end\":57750,\"start\":57744},{\"end\":57763,\"start\":57757},{\"end\":58219,\"start\":58210},{\"end\":58232,\"start\":58229},{\"end\":58246,\"start\":58238},{\"end\":58542,\"start\":58540},{\"end\":58552,\"start\":58548},{\"end\":58565,\"start\":58561},{\"end\":58580,\"start\":58576},{\"end\":58593,\"start\":58589},{\"end\":58601,\"start\":58598},{\"end\":58616,\"start\":58612},{\"end\":59037,\"start\":59034},{\"end\":59050,\"start\":59044},{\"end\":59320,\"start\":59312},{\"end\":59401,\"start\":59397},{\"end\":59415,\"start\":59411},{\"end\":59425,\"start\":59421},{\"end\":59434,\"start\":59432},{\"end\":59814,\"start\":59809},{\"end\":59828,\"start\":59821},{\"end\":59841,\"start\":59835},{\"end\":59857,\"start\":59850},{\"end\":60312,\"start\":60310},{\"end\":60326,\"start\":60321},{\"end\":60338,\"start\":60335},{\"end\":60352,\"start\":60342},{\"end\":60361,\"start\":60354},{\"end\":60368,\"start\":60363},{\"end\":60888,\"start\":60880},{\"end\":60900,\"start\":60897},{\"end\":60907,\"start\":60902},{\"end\":61179,\"start\":61177},{\"end\":61193,\"start\":61187},{\"end\":61208,\"start\":61201},{\"end\":61582,\"start\":61580},{\"end\":61593,\"start\":61589},{\"end\":61606,\"start\":61604},{\"end\":61615,\"start\":61611},{\"end\":61629,\"start\":61624},{\"end\":62069,\"start\":62067},{\"end\":62081,\"start\":62076},{\"end\":62094,\"start\":62092},{\"end\":62103,\"start\":62101},{\"end\":62118,\"start\":62114},{\"end\":62127,\"start\":62123},{\"end\":62141,\"start\":62136},{\"end\":62578,\"start\":62574},{\"end\":62591,\"start\":62587},{\"end\":62602,\"start\":62599},{\"end\":63080,\"start\":63078},{\"end\":63089,\"start\":63085},{\"end\":63103,\"start\":63099},{\"end\":63124,\"start\":63114},{\"end\":63135,\"start\":63131},{\"end\":63143,\"start\":63141},{\"end\":63168,\"start\":63155},{\"end\":63177,\"start\":63175},{\"end\":63190,\"start\":63187},{\"end\":63202,\"start\":63198},{\"end\":63795,\"start\":63792},{\"end\":63809,\"start\":63806},{\"end\":63833,\"start\":63820},{\"end\":63846,\"start\":63842},{\"end\":63862,\"start\":63858},{\"end\":63867,\"start\":63864},{\"end\":64278,\"start\":64273},{\"end\":64295,\"start\":64290},{\"end\":64317,\"start\":64303},{\"end\":64328,\"start\":64319},{\"end\":64685,\"start\":64681},{\"end\":64697,\"start\":64695},{\"end\":64708,\"start\":64706},{\"end\":64719,\"start\":64716},{\"end\":65165,\"start\":65161},{\"end\":65175,\"start\":65173},{\"end\":65187,\"start\":65182},{\"end\":65200,\"start\":65197},{\"end\":65211,\"start\":65209},{\"end\":65686,\"start\":65683},{\"end\":65700,\"start\":65696},{\"end\":65715,\"start\":65710},{\"end\":65731,\"start\":65726},{\"end\":66155,\"start\":66152},{\"end\":66171,\"start\":66167},{\"end\":66184,\"start\":66180},{\"end\":66198,\"start\":66195},{\"end\":66209,\"start\":66206}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":53763041},\"end\":41182,\"start\":40594},{\"attributes\":{\"id\":\"b1\"},\"end\":41557,\"start\":41184},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":212718086},\"end\":42090,\"start\":41559},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":244346829},\"end\":42604,\"start\":42092},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3780471},\"end\":43106,\"start\":42606},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":149453173},\"end\":43612,\"start\":43108},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":104292012},\"end\":44155,\"start\":43614},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":502946},\"end\":44796,\"start\":44157},{\"attributes\":{\"id\":\"b8\"},\"end\":45145,\"start\":44798},{\"attributes\":{\"doi\":\"arXiv:2111.03911\",\"id\":\"b9\"},\"end\":45461,\"start\":45147},{\"attributes\":{\"doi\":\"arXiv:1906.01916\",\"id\":\"b10\"},\"end\":45837,\"start\":45463},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6755881},\"end\":46215,\"start\":45839},{\"attributes\":{\"id\":\"b12\"},\"end\":46701,\"start\":46217},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6724907},\"end\":47174,\"start\":46703},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206770307},\"end\":47435,\"start\":47176},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":215827080},\"end\":47960,\"start\":47437},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":44068381},\"end\":48320,\"start\":47962},{\"attributes\":{\"doi\":\"arXiv:1912.02781\",\"id\":\"b17\"},\"end\":48739,\"start\":48322},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":221172979},\"end\":49235,\"start\":48741},{\"attributes\":{\"doi\":\"arXiv:1610.01983\",\"id\":\"b19\"},\"end\":49726,\"start\":49237},{\"attributes\":{\"doi\":\"arXiv:2001.04647\",\"id\":\"b20\"},\"end\":50030,\"start\":49728},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":202540648},\"end\":50607,\"start\":50032},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":214713640},\"end\":51116,\"start\":50609},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":233296777},\"end\":51598,\"start\":51118},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":237267340},\"end\":52056,\"start\":51600},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":247625561},\"end\":52342,\"start\":52058},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":247447427},\"end\":52821,\"start\":52344},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":228083657},\"end\":53356,\"start\":52823},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":229171234},\"end\":53831,\"start\":53358},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":556999},\"end\":54275,\"start\":53833},{\"attributes\":{\"doi\":\"arXiv:1905.04215\",\"id\":\"b30\"},\"end\":54581,\"start\":54277},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":234763219},\"end\":55066,\"start\":54583},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":220545989},\"end\":55604,\"start\":55068},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":245219301},\"end\":56134,\"start\":55606},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206594738},\"end\":56607,\"start\":56136},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10328909},\"end\":56990,\"start\":56609},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":238259585},\"end\":57635,\"start\":56992},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":54469831},\"end\":58143,\"start\":57637},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1103605},\"end\":58468,\"start\":58145},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":212725633},\"end\":58962,\"start\":58470},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":12453047},\"end\":59308,\"start\":58964},{\"attributes\":{\"id\":\"b41\"},\"end\":59337,\"start\":59310},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":91184137},\"end\":59756,\"start\":59339},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":4357800},\"end\":60199,\"start\":59758},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":232147762},\"end\":60797,\"start\":60201},{\"attributes\":{\"doi\":\"arXiv:2203.15793\",\"id\":\"b45\"},\"end\":61103,\"start\":60799},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":220381573},\"end\":61508,\"start\":61105},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":214714362},\"end\":62008,\"start\":61510},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":208617520},\"end\":62490,\"start\":62010},{\"attributes\":{\"id\":\"b49\"},\"end\":62980,\"start\":62492},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":246870560},\"end\":63695,\"start\":62982},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":152282661},\"end\":64264,\"start\":63697},{\"attributes\":{\"doi\":\"arXiv:1710.09412\",\"id\":\"b52\"},\"end\":64554,\"start\":64266},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":221761445},\"end\":65082,\"start\":64556},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":247839425},\"end\":65593,\"start\":65084},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":233404466},\"end\":66081,\"start\":65595},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":195781428},\"end\":66589,\"start\":66083}]", "bib_title": "[{\"end\":40658,\"start\":40594},{\"end\":41637,\"start\":41559},{\"end\":42139,\"start\":42092},{\"end\":42667,\"start\":42606},{\"end\":43172,\"start\":43108},{\"end\":43721,\"start\":43614},{\"end\":44218,\"start\":44157},{\"end\":45888,\"start\":45839},{\"end\":46772,\"start\":46703},{\"end\":47186,\"start\":47176},{\"end\":47517,\"start\":47437},{\"end\":48021,\"start\":47962},{\"end\":48828,\"start\":48741},{\"end\":50147,\"start\":50032},{\"end\":50677,\"start\":50609},{\"end\":51187,\"start\":51118},{\"end\":51665,\"start\":51600},{\"end\":52130,\"start\":52058},{\"end\":52419,\"start\":52344},{\"end\":52905,\"start\":52823},{\"end\":53424,\"start\":53358},{\"end\":53893,\"start\":53833},{\"end\":54658,\"start\":54583},{\"end\":55143,\"start\":55068},{\"end\":55688,\"start\":55606},{\"end\":56191,\"start\":56136},{\"end\":56687,\"start\":56609},{\"end\":57094,\"start\":56992},{\"end\":57701,\"start\":57637},{\"end\":58199,\"start\":58145},{\"end\":58533,\"start\":58470},{\"end\":59024,\"start\":58964},{\"end\":59391,\"start\":59339},{\"end\":59802,\"start\":59758},{\"end\":60299,\"start\":60201},{\"end\":61170,\"start\":61105},{\"end\":61570,\"start\":61510},{\"end\":62057,\"start\":62010},{\"end\":62562,\"start\":62492},{\"end\":63070,\"start\":62982},{\"end\":63782,\"start\":63697},{\"end\":64671,\"start\":64556},{\"end\":65150,\"start\":65084},{\"end\":65673,\"start\":65595},{\"end\":66144,\"start\":66083}]", "bib_author": "[{\"end\":40673,\"start\":40660},{\"end\":40686,\"start\":40673},{\"end\":40701,\"start\":40686},{\"end\":40710,\"start\":40701},{\"end\":40724,\"start\":40710},{\"end\":40735,\"start\":40724},{\"end\":40748,\"start\":40735},{\"end\":40763,\"start\":40748},{\"end\":41652,\"start\":41639},{\"end\":41666,\"start\":41652},{\"end\":41680,\"start\":41666},{\"end\":41691,\"start\":41680},{\"end\":41699,\"start\":41691},{\"end\":42156,\"start\":42141},{\"end\":42169,\"start\":42156},{\"end\":42176,\"start\":42169},{\"end\":42188,\"start\":42176},{\"end\":42199,\"start\":42188},{\"end\":42212,\"start\":42199},{\"end\":42217,\"start\":42212},{\"end\":42681,\"start\":42669},{\"end\":42689,\"start\":42681},{\"end\":42709,\"start\":42689},{\"end\":42722,\"start\":42709},{\"end\":42736,\"start\":42722},{\"end\":43189,\"start\":43174},{\"end\":43201,\"start\":43189},{\"end\":43218,\"start\":43201},{\"end\":43233,\"start\":43218},{\"end\":43737,\"start\":43723},{\"end\":43751,\"start\":43737},{\"end\":43761,\"start\":43751},{\"end\":43775,\"start\":43761},{\"end\":44235,\"start\":44220},{\"end\":44250,\"start\":44235},{\"end\":44267,\"start\":44250},{\"end\":44281,\"start\":44267},{\"end\":44299,\"start\":44281},{\"end\":44317,\"start\":44299},{\"end\":44329,\"start\":44317},{\"end\":44342,\"start\":44329},{\"end\":44357,\"start\":44342},{\"end\":44958,\"start\":44946},{\"end\":44965,\"start\":44958},{\"end\":45228,\"start\":45212},{\"end\":45242,\"start\":45228},{\"end\":45256,\"start\":45242},{\"end\":45267,\"start\":45256},{\"end\":45551,\"start\":45537},{\"end\":45565,\"start\":45551},{\"end\":45576,\"start\":45565},{\"end\":45595,\"start\":45576},{\"end\":45613,\"start\":45595},{\"end\":45906,\"start\":45890},{\"end\":45924,\"start\":45906},{\"end\":46323,\"start\":46307},{\"end\":46342,\"start\":46323},{\"end\":46355,\"start\":46342},{\"end\":46371,\"start\":46355},{\"end\":46388,\"start\":46371},{\"end\":46409,\"start\":46388},{\"end\":46425,\"start\":46409},{\"end\":46443,\"start\":46425},{\"end\":46790,\"start\":46774},{\"end\":46803,\"start\":46790},{\"end\":46819,\"start\":46803},{\"end\":47203,\"start\":47188},{\"end\":47534,\"start\":47519},{\"end\":47548,\"start\":47534},{\"end\":47564,\"start\":47548},{\"end\":47580,\"start\":47564},{\"end\":48045,\"start\":48023},{\"end\":48059,\"start\":48045},{\"end\":48074,\"start\":48059},{\"end\":48082,\"start\":48074},{\"end\":48416,\"start\":48401},{\"end\":48427,\"start\":48416},{\"end\":48435,\"start\":48427},{\"end\":48449,\"start\":48435},{\"end\":48462,\"start\":48449},{\"end\":48477,\"start\":48462},{\"end\":48495,\"start\":48477},{\"end\":48846,\"start\":48830},{\"end\":48861,\"start\":48846},{\"end\":48873,\"start\":48861},{\"end\":48890,\"start\":48873},{\"end\":49363,\"start\":49337},{\"end\":49378,\"start\":49363},{\"end\":49392,\"start\":49378},{\"end\":49812,\"start\":49799},{\"end\":49827,\"start\":49812},{\"end\":49841,\"start\":49827},{\"end\":50165,\"start\":50149},{\"end\":50179,\"start\":50165},{\"end\":50193,\"start\":50179},{\"end\":50207,\"start\":50193},{\"end\":50692,\"start\":50679},{\"end\":50702,\"start\":50692},{\"end\":50714,\"start\":50702},{\"end\":50727,\"start\":50714},{\"end\":50740,\"start\":50727},{\"end\":50751,\"start\":50740},{\"end\":50764,\"start\":50751},{\"end\":51201,\"start\":51189},{\"end\":51213,\"start\":51201},{\"end\":51224,\"start\":51213},{\"end\":51235,\"start\":51224},{\"end\":51675,\"start\":51667},{\"end\":51686,\"start\":51675},{\"end\":51700,\"start\":51686},{\"end\":51713,\"start\":51700},{\"end\":51721,\"start\":51713},{\"end\":52143,\"start\":52132},{\"end\":52154,\"start\":52143},{\"end\":52165,\"start\":52154},{\"end\":52178,\"start\":52165},{\"end\":52432,\"start\":52421},{\"end\":52443,\"start\":52432},{\"end\":52456,\"start\":52443},{\"end\":52920,\"start\":52907},{\"end\":52933,\"start\":52920},{\"end\":52941,\"start\":52933},{\"end\":52954,\"start\":52941},{\"end\":52965,\"start\":52954},{\"end\":52978,\"start\":52965},{\"end\":52994,\"start\":52978},{\"end\":53438,\"start\":53426},{\"end\":53449,\"start\":53438},{\"end\":53462,\"start\":53449},{\"end\":53911,\"start\":53895},{\"end\":53920,\"start\":53911},{\"end\":53934,\"start\":53920},{\"end\":53950,\"start\":53934},{\"end\":54348,\"start\":54336},{\"end\":54356,\"start\":54348},{\"end\":54370,\"start\":54356},{\"end\":54384,\"start\":54370},{\"end\":54393,\"start\":54384},{\"end\":54672,\"start\":54660},{\"end\":54683,\"start\":54672},{\"end\":54699,\"start\":54683},{\"end\":55160,\"start\":55145},{\"end\":55179,\"start\":55160},{\"end\":55194,\"start\":55179},{\"end\":55212,\"start\":55194},{\"end\":55706,\"start\":55690},{\"end\":55722,\"start\":55706},{\"end\":55735,\"start\":55722},{\"end\":55750,\"start\":55735},{\"end\":55762,\"start\":55750},{\"end\":56208,\"start\":56193},{\"end\":56225,\"start\":56208},{\"end\":56240,\"start\":56225},{\"end\":56253,\"start\":56240},{\"end\":56711,\"start\":56689},{\"end\":56720,\"start\":56711},{\"end\":56735,\"start\":56720},{\"end\":56740,\"start\":56735},{\"end\":57128,\"start\":57096},{\"end\":57142,\"start\":57128},{\"end\":57158,\"start\":57142},{\"end\":57181,\"start\":57158},{\"end\":57194,\"start\":57181},{\"end\":57203,\"start\":57194},{\"end\":57718,\"start\":57703},{\"end\":57736,\"start\":57718},{\"end\":57752,\"start\":57736},{\"end\":57765,\"start\":57752},{\"end\":58221,\"start\":58201},{\"end\":58234,\"start\":58221},{\"end\":58248,\"start\":58234},{\"end\":58544,\"start\":58535},{\"end\":58554,\"start\":58544},{\"end\":58567,\"start\":58554},{\"end\":58582,\"start\":58567},{\"end\":58595,\"start\":58582},{\"end\":58603,\"start\":58595},{\"end\":58618,\"start\":58603},{\"end\":59039,\"start\":59026},{\"end\":59052,\"start\":59039},{\"end\":59322,\"start\":59312},{\"end\":59403,\"start\":59393},{\"end\":59417,\"start\":59403},{\"end\":59427,\"start\":59417},{\"end\":59436,\"start\":59427},{\"end\":59816,\"start\":59804},{\"end\":59830,\"start\":59816},{\"end\":59843,\"start\":59830},{\"end\":59859,\"start\":59843},{\"end\":60314,\"start\":60301},{\"end\":60328,\"start\":60314},{\"end\":60340,\"start\":60328},{\"end\":60354,\"start\":60340},{\"end\":60363,\"start\":60354},{\"end\":60370,\"start\":60363},{\"end\":60890,\"start\":60876},{\"end\":60902,\"start\":60890},{\"end\":60909,\"start\":60902},{\"end\":61181,\"start\":61172},{\"end\":61195,\"start\":61181},{\"end\":61210,\"start\":61195},{\"end\":61584,\"start\":61572},{\"end\":61595,\"start\":61584},{\"end\":61608,\"start\":61595},{\"end\":61617,\"start\":61608},{\"end\":61631,\"start\":61617},{\"end\":62071,\"start\":62059},{\"end\":62083,\"start\":62071},{\"end\":62096,\"start\":62083},{\"end\":62105,\"start\":62096},{\"end\":62120,\"start\":62105},{\"end\":62129,\"start\":62120},{\"end\":62143,\"start\":62129},{\"end\":62580,\"start\":62564},{\"end\":62593,\"start\":62580},{\"end\":62604,\"start\":62593},{\"end\":63082,\"start\":63072},{\"end\":63091,\"start\":63082},{\"end\":63105,\"start\":63091},{\"end\":63126,\"start\":63105},{\"end\":63137,\"start\":63126},{\"end\":63145,\"start\":63137},{\"end\":63170,\"start\":63145},{\"end\":63179,\"start\":63170},{\"end\":63192,\"start\":63179},{\"end\":63204,\"start\":63192},{\"end\":63797,\"start\":63784},{\"end\":63811,\"start\":63797},{\"end\":63835,\"start\":63811},{\"end\":63848,\"start\":63835},{\"end\":63864,\"start\":63848},{\"end\":63869,\"start\":63864},{\"end\":64280,\"start\":64266},{\"end\":64297,\"start\":64280},{\"end\":64319,\"start\":64297},{\"end\":64330,\"start\":64319},{\"end\":64687,\"start\":64673},{\"end\":64699,\"start\":64687},{\"end\":64710,\"start\":64699},{\"end\":64721,\"start\":64710},{\"end\":65167,\"start\":65152},{\"end\":65177,\"start\":65167},{\"end\":65189,\"start\":65177},{\"end\":65202,\"start\":65189},{\"end\":65213,\"start\":65202},{\"end\":65688,\"start\":65675},{\"end\":65702,\"start\":65688},{\"end\":65717,\"start\":65702},{\"end\":65733,\"start\":65717},{\"end\":66157,\"start\":66146},{\"end\":66173,\"start\":66157},{\"end\":66186,\"start\":66173},{\"end\":66200,\"start\":66186},{\"end\":66211,\"start\":66200}]", "bib_venue": "[{\"end\":40840,\"start\":40763},{\"end\":41360,\"start\":41184},{\"end\":41776,\"start\":41699},{\"end\":42298,\"start\":42217},{\"end\":42809,\"start\":42736},{\"end\":43310,\"start\":43233},{\"end\":43842,\"start\":43775},{\"end\":44430,\"start\":44357},{\"end\":44944,\"start\":44798},{\"end\":45210,\"start\":45147},{\"end\":45535,\"start\":45463},{\"end\":45983,\"start\":45924},{\"end\":46305,\"start\":46217},{\"end\":46892,\"start\":46819},{\"end\":47266,\"start\":47203},{\"end\":47653,\"start\":47580},{\"end\":48131,\"start\":48082},{\"end\":48399,\"start\":48322},{\"end\":48943,\"start\":48890},{\"end\":49335,\"start\":49237},{\"end\":49797,\"start\":49728},{\"end\":50274,\"start\":50207},{\"end\":50817,\"start\":50764},{\"end\":51316,\"start\":51235},{\"end\":51792,\"start\":51721},{\"end\":52182,\"start\":52178},{\"end\":52533,\"start\":52456},{\"end\":53046,\"start\":52994},{\"end\":53543,\"start\":53462},{\"end\":54009,\"start\":53950},{\"end\":54334,\"start\":54277},{\"end\":54776,\"start\":54699},{\"end\":55288,\"start\":55212},{\"end\":55823,\"start\":55762},{\"end\":56326,\"start\":56253},{\"end\":56789,\"start\":56740},{\"end\":57270,\"start\":57203},{\"end\":57842,\"start\":57765},{\"end\":58288,\"start\":58248},{\"end\":58671,\"start\":58618},{\"end\":59105,\"start\":59052},{\"end\":59503,\"start\":59436},{\"end\":59932,\"start\":59859},{\"end\":60447,\"start\":60370},{\"end\":60874,\"start\":60799},{\"end\":61263,\"start\":61210},{\"end\":61708,\"start\":61631},{\"end\":62204,\"start\":62143},{\"end\":62684,\"start\":62604},{\"end\":63287,\"start\":63204},{\"end\":63936,\"start\":63869},{\"end\":64387,\"start\":64346},{\"end\":64774,\"start\":64721},{\"end\":65290,\"start\":65213},{\"end\":65796,\"start\":65733},{\"end\":66288,\"start\":66211},{\"end\":40904,\"start\":40842},{\"end\":41840,\"start\":41778},{\"end\":42366,\"start\":42300},{\"end\":42869,\"start\":42811},{\"end\":43374,\"start\":43312},{\"end\":43896,\"start\":43844},{\"end\":44490,\"start\":44432},{\"end\":46029,\"start\":45985},{\"end\":46952,\"start\":46894},{\"end\":47316,\"start\":47268},{\"end\":47713,\"start\":47655},{\"end\":48983,\"start\":48945},{\"end\":50328,\"start\":50276},{\"end\":50857,\"start\":50819},{\"end\":51384,\"start\":51318},{\"end\":51850,\"start\":51794},{\"end\":52597,\"start\":52535},{\"end\":53085,\"start\":53048},{\"end\":53611,\"start\":53545},{\"end\":54055,\"start\":54011},{\"end\":54840,\"start\":54778},{\"end\":55351,\"start\":55290},{\"end\":55871,\"start\":55825},{\"end\":56386,\"start\":56328},{\"end\":57324,\"start\":57272},{\"end\":57906,\"start\":57844},{\"end\":58711,\"start\":58673},{\"end\":59145,\"start\":59107},{\"end\":59557,\"start\":59505},{\"end\":59992,\"start\":59934},{\"end\":60511,\"start\":60449},{\"end\":61303,\"start\":61265},{\"end\":61772,\"start\":61710},{\"end\":62252,\"start\":62206},{\"end\":62751,\"start\":62686},{\"end\":63357,\"start\":63289},{\"end\":63990,\"start\":63938},{\"end\":64814,\"start\":64776},{\"end\":65354,\"start\":65292},{\"end\":65846,\"start\":65798},{\"end\":66352,\"start\":66290}]"}}}, "year": 2023, "month": 12, "day": 17}