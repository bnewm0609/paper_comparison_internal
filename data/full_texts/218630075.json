{"id": 218630075, "updated": "2023-10-06 15:24:27.989", "metadata": {"title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification", "authors": "[{\"first\":\"Brecht\",\"last\":\"Desplanques\",\"middle\":[]},{\"first\":\"Jenthe\",\"last\":\"Thienpondt\",\"middle\":[]},{\"first\":\"Kris\",\"last\":\"Demuynck\",\"middle\":[]}]", "venue": "Interspeech 2020", "journal": "Interspeech 2020", "publication_date": {"year": 2020, "month": 5, "day": 14}, "abstract": "Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel's statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2005.07143", "mag": "3024869864", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/DesplanquesTD20", "doi": "10.21437/interspeech.2020-2650"}}, "content": {"source": {"pdf_hash": "9609f4817a7e769f5e3e07084db35e46696e82cd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2005.07143v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://biblio.ugent.be/publication/8680078/file/8680892.pdf", "status": "GREEN"}}, "grobid": {"id": "1454f11a85840d984ca9e7ad1b4f92bd2127ec70", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9609f4817a7e769f5e3e07084db35e46696e82cd.txt", "contents": "\nECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification\n\n\nBrecht Desplanques brecht.desplanques@ugent.be \nDepartment of Electronics and Information Systems\nIDLab\nimec -Ghent University\nBelgium\n\nJenthe Thienpondt jenthe.thienpondt@ugent.be \nDepartment of Electronics and Information Systems\nIDLab\nimec -Ghent University\nBelgium\n\nKris Demuynck \nDepartment of Electronics and Information Systems\nIDLab\nimec -Ghent University\nBelgium\n\nECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification\nIndex Terms: speaker recognitionspeaker verificationdeep neural networksx-vectorschannel attention\nCurrent speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel's statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-ofthe-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.\n\nIntroduction\n\nIn recent years, x-vectors [1] and their subsequent improvements [2,3,4] have consistently provided state-of-the-art results on the task of speaker verification. Improving upon the original Time Delay Neural Network (TDNN) architecture is an active area of research. Usually, the neural networks are trained on the speaker identification task. After convergence, low dimensional speaker embeddings can be extracted from the bottleneck layer preceding the output layer to characterize the speaker in the input recording. Speaker verification can be accomplished by comparing the two embeddings corresponding with an enrollment and a test recording to accept or reject the hypothesis that both recordings contain the same speaker. A simple cosine distance measurement can be used for this comparison. In addition, a more complicated scoring backend can be trained such as Probabilistic Linear Discriminant Analysis (PLDA) [5].\n\nThe rising popularity of the x-vector system has resulted in significant architectural improvements and optimized training procedures [6] over the original approach. The topology of the system was improved by incorporating elements of the popular ResNet [7] architecture. Adding residual connections between the frame-level layers has been shown to enhance the em-beddings [3,4]. Additionally, residual connections enable the back-propagation algorithm to converge faster and help avoid the vanishing gradient problem [7].\n\nThe statistics pooling layer in the x-vector system projects the variable-length input to a fixed-length representation by gathering simple statistics of hidden node activations across time. The authors in [8,9] introduce a temporal self-attention system to this pooling layer which allows the network to only focus on frames it deems important. It can also be interpreted as a Voice Activity Detection (VAD) preprocessing step to detect the irrelevant non-speech frames.\n\nIn this work we propose further architectural enhancements to the TDNN architecture and statistics pooling layer. We introduce additional skip connections to propagate and aggregate channels throughout the system. Channel attention that uses a global context is incorporated in the frame layers and statistics pooling layer to improve the results even further.\n\nThe paper is organized as follows: Section 2 will describe the current state-of-the-art speaker recognition systems which will be used as baseline. Section 3 will explain and motivate the novel components of our proposed architecture. Section 4 will explain our experimental setup to test the impact of the individual components in our architecture on the popular VoxCeleb datasets [10,11,12]. We discuss the results of these experiments in Section 5. In addition, a comparison between popular state-of-the-art baseline systems will be provided. Section 6 will conclude with a brief overview of our findings.\n\n\nDNN speaker recognition systems\n\nTwo types of DNN-based speaker recognition architectures will serve as strong baselines to measure the impact of our proposed architecture: an x-vector and a ResNet based system which both currently provide state-of-the-art performance on speaker verification tasks such as VoxSRC [12].\n\n\nExtended-TDNN x-vector\n\nThe first baseline system is the Extended TDNN x-vector architecture [2,3,4] and improves upon the original x-vector system introduced in [1]. The initial frame layers consist out of 1dimensional dilated convolutional layers interleaved with dense layers. Every filter has access to all the features of the previous layer or input layer. The task of the dilated convolutional layers is to gradually build up the temporal context. Residual connections are introduced in all frame-level layers. The frame layers are followed by an attentive statistics pooling layer that calculates the mean and standard deviations of the final frame-level features. The attention system [8] allows the model to select the frames it deems relevant. After the statistics pooling, two fully-connected layers are introduced with the first one acting as a bottleneck layer to generate the low-dimensional speaker characterizing embedding.\n\n\nResNet-based r-vector\n\nThe second baseline system is the r-vector system proposed in [4]. It is based on the ResNet18 and ResNet34 implementations of the successful ResNet architecture [7]. The convolutional frame layers of this network process the features as a 2-dimensional signal before collecting the mean and standard deviation statistics in the pooling layer. See [4] for more details about the topology.\n\n\nProposed ECAPA-TDNN architecture\n\nIn this section, we examine some of the limitations of the xvector architecture and incorporate potential solutions in our ECAPA-TDNN architecture. The following subsections will focus on frame-level and pooling-level enhancements. An overview of the complete architecture is given by Figure 2. BN stands for Batch Normalization [13] and the non-linearities are Rectified Linear Units (ReLU).\n\n\nChannel-and context-dependent statistics pooling\n\nIn recent x-vector architectures, soft self-attention is used for calculating weighted statistics in the temporal pooling layer [8]. Success with multi-headed attention has shown that certain speaker properties can be extracted on different sets of frames [9]. Due to these results, we argue that it might be beneficial to extend this attention mechanism even further to the channel dimension. This enables the network to focus more on speaker characteristics that do not activate on identical or similar time instances, e.g. vowels versus consonants.\n\nWe implement the attention mechanism as described in [8] and adapt it to be channel-dependent:\net,c = v v v T c f (W W Wh h ht + b b b) + kc,(1)\nwhere h h ht are the activations of the last frame layer at time step t. The parameters W W W \u2208 R R\u00d7C and b b b \u2208 R R\u00d71 project the information for self-attention to a smaller R-dimensional representation that is shared across all C channels to reduce the parameter count and overfitting. After a non-linearity f (\u00b7) this information is transformed to a channel-dependent self-attention score through a linear layer with weights v v vc \u2208 R R\u00d71 and bias kc. This scalar score et,c is then normalized over all frames by applying the softmax function channel-wise across time:\n\u03b1t,c = exp(et,c) T \u03c4 exp(e\u03c4,c) .(2)\nThe self-attention score \u03b1t,c represents the importance of each frame given the channel and is used to calculate the weighted statistics of channel c. For each utterance the channel component\u03bcc of the weighted mean vector\u03bc \u03bc \u00b5 is estimated as:\u03bc\nc = T t \u03b1t,cht,c.(3)\nThe channel component\u03c3c of the weighted standard deviation vector\u03c3 \u03c3 \u03c3 is constructed as follows:\n\u03c3c = T t \u03b1t,ch 2 t,c \u2212\u03bc 2 c .(4)\nThe final output of the pooling layer is given by concatenating the vectors of the weighted mean\u03bc \u03bc \u00b5 and weighted standard deviation\u03c3 \u03c3 \u03c3.\n\nFurthermore, we expand the temporal context of the pooling layer by allowing the self-attention to look at global properties of the utterance. We concatenate the local input h h ht in (1) with the global non-weighted mean and standard deviation of h h ht across the time domain. This context vector should allow the attention mechanism to adapt itself to global properties of the utterance such as noise or recording conditions.\n\n\n1-Dimensional Squeeze-Excitation Res2Blocks\n\nThe temporal context of frame layers in the original x-vector system is limited to 15 frames. As the network apparently benefits from a wider temporal context [2,4,3] we argue it could be beneficial to rescale the frame-level features given global properties of the recording, similar to the global context in the attention module described above. For this purpose we introduce 1dimensional Squeeze-Excitation (SE) blocks, as this computer vision approach to model global channel interdependencies has been proved successful [14].\n\nThe first component of an SE-block is the squeeze operation which generates a descriptor for each channel. The squeeze operation simply consists of calculating the mean vector z z z of the frame-level features across the time domain:\nz z z = 1 T T t h h ht.(5)\nThe descriptors in z z z are then used in the excitation operation to calculate a weight for each channel. We define the subsequent excitation operation as:\ns s s = \u03c3(W W W 2f (W W W 1z z z + b b b1) + b b b2)(6)\nwith \u03c3 denoting the sigmoid function and W W W 1 \u2208 R R\u00d7C and W W W 2 \u2208 R C\u00d7R . This operation acts as a bottleneck layer with C and R referring to the number of input channels and reduced dimensionality respectively. The resulting vector s s s contains weights sc between zero and one, which are applied to the original input through channel-wise multiplicatio\u00f1 h h hc = sch h hc\n\nThe 1-dimensional SE-block can be integrated in the xvector architecture in various ways, with using them after each dilated convolution being the most straightforward one. However, we want to combine them with the benefits of residual connections [7]. Simultaneously, we do not want to increase the total amount of parameters too much compared to the baseline systems.\n\nThe SE-Res2Block shown in Figure 1 incorporates the requirements mentioned above. We contain the dilated convolutions with a preceding and succeeding dense layer with a context of 1 frame. The first dense layer can be used to reduce the feature dimension, while the second dense layer restores the number of features to the original dimension. This is followed by an SE-block to scale each channel. The whole unit is covered by a skip connection. The use of these traditional Res-Blocks makes it easy to incorporate advancements concerning this popular computer vision architecture. The recent Res2Net module [15], for example, enhances the central convolutional layer so that it can process multi-scale features by constructing hierarchical residual-like connections within. The integration of this module improves performance, while significantly reducing the number of model parameters. \n\n\nMulti-layer feature aggregation and summation\n\nThe original x-vector system only uses the feature map of the last frame-layer for calculating the pooled statistics. Given the hierarchical nature of a CNN, these deeper level features are the most complex ones and should be strongly correlated with the speaker identities. However, due to evidence in [16,17] we argue that the more shallow feature maps can also contribute towards more robust speaker embeddings. For each frame, our proposed system concatenates the output feature maps of all the SE-Res2Blocks. After this Multi-layer Feature Aggregation (MFA), a dense layer processes the concatenated information to generate the features for the attentive statistics pooling.\n\nAnother, complementary way to exploit multi-layer information is to use the output of all preceding SE-Res2Blocks and initial convolutional layer as input for each frame layer block [16,18]. We implement this by defining the residual connection in each SE-Res2Block as the sum of the outputs of all the previous blocks. We opt for a summation of the feature maps instead of concatenation to restrain the model parameter count. The final architecture without the summed residual connections is shown in Figure 2.\n\n\nExperimental setup 4.1. Training the speaker embedding extractors\n\nWe apply the fixed-condition VoxSRC 2019 training restrictions [12] and only use the development part of the VoxCeleb2 dataset [11] with 5994 speakers as training data. A small subset of about 2% of the data is reserved as a validation set for hyperparameter optimization. It is a well known fact that neural networks benefit from data augmentation which generates extra training samples. We generate a total of 6 extra samples for each utterance. The first set of augmentations follow the Kaldi recipe [2] in combination with the publicly available MUSAN dataset (babble, noise) [19] and the RIR dataset (reverb) provided in [20]. The remaining three augmentations are generated with the open-source SoX (tempo up, tempo down) and FFmpeg (alternating opus or aac compression) libraries. The input features are 80-dimensional MFCCs from a 25 ms window with a 10 ms frame shift. The MFCCs are normalized through cepstral mean subtraction and no voice activity detection is applied. We use a random crop of 2 seconds on the normalized features. As a final augmentation step, we apply SpecAugment [21] on the log mel spectrogram of the samples. The algorithm randomly masks 0 to 5 frames in the time domain and 0 to 10 channels in the frequency domain.\n\nAll models are trained with a cyclical learning rate varying between 1e-8 and 1e-3 using the triangular2 policy as described in [22] in conjunction with the Adam optimizer [23]. The duration of one cycle is set to 130000 iterations. All systems are trained using AAM-softmax [6,24] with a margin of 0.2 and softmax prescaling of 30 for 4 cycles. To prevent overfitting, we apply a weight decay on all weights in the model of 2e-5, except for the AAM-softmax weights, which uses 2e-4. The mini-batch size for training is 128.\n\nWe study two setups of the proposed ECAPA-TDNN architecture with either 512 or 1024 channels in the convolutional frame layers. The dimension of the bottleneck in the SE-Block and the attention module is set to 128. The scale dimension s in the Res2Block [15] is set to 8. The number of nodes in the final fully-connected layer is 192. The performance of this system will be compared to the baselines described in Section 2.\n\n\nSpeaker verification\n\nSpeaker embeddings are extracted from the final fullyconnected layer for all systems. Trial scores are always produced using the cosine distance between embeddings. Subsequently, all scores are normalized using adaptive s-norm [25]. The imposter cohort consists of the speaker-wise averages of the length-normalized embeddings of all training utterances. The size of the imposter cohort was set to 1000 for the VoxCeleb test sets and to a more robust value of 50 for the cross-dataset VoxSRC 2019 evaluation.\n\n\nEvaluation protocol\n\nThe system is evaluated on the popular VoxCeleb1 test sets [10] and VoxSRC 2019 evaluation set [12]. Performance will be measured by providing the Equal Error Rate (EER) and the minimum normalized detection cost MinDCF with Ptarget = 10 \u22122 and CF A = CMiss = 1. A concise ablation study is used to gain a deeper understanding how each of the proposed improvements affects the performance.\n\n\nResults\n\nA performance overview of the baseline systems described in Section 2 and our proposed ECAPA-TDNN system is given in Table 1, together with the number of model parameters in the embedding extractor. We implement two setups with the number of filters C in the convolutional layers either set to 512 or 1024. Our proposed architecture significantly outperforms all baselines while using fewer model parameters. The larger ECAPA-TDNN system gives an average relative improvement of 18.7% in EER and 12.5% in MinDCF over the best scoring baseline for each test set. We note that the performance of the baselines supersedes the numbers reported in [3,4] in most cases. We continue with an ablation study of the individual components introduced in Section 3. An overview of these results is given in Table 2. To measure the impact of our proposed attention module, we run an experiment A.1, that uses the attention module from [8]. We also run a separate experiment A.2 that does not supply the context vector to the proposed attention. The channel-and context-dependent statistics pooling system improves the EER and MinDCF metric with 9.8% and 3.2%, respectively. This confirms the benefits of applying different temporal attention to each channel. Addition of the context vector results in very small performance gains with the system relatively improving about 1.9% in EER and 1.1% in MinDCF. Nonetheless, this strengthens our belief that a TDNN-based architecture should try to exploit global context information.\n\nThis intuition is confirmed with experiment B.1 that clearly shows the importance of the SE-blocks described in Section 3.2. Incorporating the SE-modules in the Res2Blocks results in relative improvements of 20.5% in EER and 11.9% in the MinDCF metric. This indicates that the limited temporal context of the frame-level features is insufficient and should be complemented with global utterance-based information. In experiment B.2 we replaced the multi-scale features of the Res2Blocks with the standard central dilated 1D convolutional of the ResNet counterpart. Aside from a substantial 30% relative reduction in model parameters, the multi-scale Res2Net approach also leads towards a relative improvement of 5.6% in EER and 3.2% in MinDCF.\n\nIn experiment C.1, we only use the output of the final SE-Res2Block instead of aggregating the information of all SE-Res2Blocks. Aggregation of the outputs leads to relative improvements of 8.2% in EER and 2.8% in the MinDCF value. Removing all residual connections (experiment C.2) shows a similar rate of degradation. Replacing a standard ResNet skip connection in the SE-Res2Blocks by the sum of the outputs of all previous SE-Res2Blocks improves the EER with 6.5%, while slightly degrading the MinDCF score in experiment experiment C.3. However, further experience on challenging datasets such as [26] convinced us to incorporate summed residuals in the final ECAPA-TDNN architecture.\n\n\nConclusion\n\nIn this paper we presented ECAPA-TDNN, a novel TDNNbased speaker embedding extractor for speaker verification. We built further upon the original x-vector architecture and put more Emphasis on Channel Attention, Propagation and Aggregation. The incorporation of Squeeze-Excitation blocks, multiscale Res2Net features, extra skip connections and channeldependent attentive statistics pooling, led to significant relative improvements of 19% in EER on average over strong baseline systems on the VoxCeleb and VoxSRC 2019 evaluation sets.\n\nFigure 1 :\n1The SE-Res2Block of the ECAPA-TDNN architecture. The standard Conv1D layers have a kernel size of 1. The central Res2Net[15] Conv1D with scale dimension s = 8 expands the temporal context through kernel size k and dilation spacing d.\n\nFigure 2 :\n2Network topology of the ECAPA-TDNN. We denote k for kernel size and d for dilation spacing of the Conv1D layers or SE-Res2Blocks. C and T correspond to the channel and temporal dimension of the intermediate feature-maps respectively. S is the number of training speakers.\n\nTable 1 :\n1EER and MinDCF performance of all systems on the standard VoxCeleb1 and VoxSRC 2019 test sets.Architecture \n# Params \nVoxCeleb1 \nVoxCeleb1-E \nVoxCeleb1-H \nVoxSRC19 \n\nEER(%) MinDCF EER(%) MinDCF EER(%) MinDCF \nEER(%) \n\nE-TDNN \n6.8M \n1.49 \n0.1604 \n1.61 \n0.1712 \n2.69 \n0.2419 \n1.81 \nE-TDNN (large) \n20.4M \n1.26 \n0.1399 \n1.37 \n0.1487 \n2.35 \n0.2153 \n1.61 \n\nResNet18 \n13.8M \n1.47 \n0.1772 \n1.60 \n0.1789 \n2.88 \n0.2672 \n1.97 \nResNet34 \n23.9M \n1.19 \n0.1592 \n1.33 \n0.1560 \n2.46 \n0.2288 \n1.57 \n\nECAPA-TDNN (C=512) \n6.2M \n1.01 \n0.1274 \n1.24 \n0.1418 \n2.32 \n0.2181 \n1.32 \nECAPA-TDNN (C=1024) \n14.7M \n0.87 \n0.1066 \n1.12 \n0.1318 \n2.12 \n0.2101 \n1.22 \n\n\n\nTable 2 :\n2Ablation study of the ECAPA-TDNN architecture.Systems \nEER(%) MinDCF \n\nECAPA-TDNN (C=512) \n1.01 \n0.1274 \n\nA.1 \nAttentive Statistics [8] \n1.12 \n0.1316 \nA.2 \nChannel Att. w/o Context \n1.03 \n0.1288 \n\nB.1 \nNo SE-Block \n1.27 \n0.1446 \nB.2 \nNo Res2Net-Block \n1.07 \n0.1316 \n\nC.1 \nNo MFA \n1.10 \n0.1311 \nC.2 \nNo Res. Connections \n1.08 \n0.1310 \nC.3 No Sum Res. Connections \n1.08 \n0.1217 \n\n\n\nX-vectors: Robust DNN embeddings for speaker recognition. D Snyder, D Garcia-Romero, G Sell, D Povey, S Khudanpur, Proc. ICASSP. ICASSPD. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan- pur, \"X-vectors: Robust DNN embeddings for speaker recogni- tion,\" in Proc. ICASSP, 2018, pp. 5329-5333.\n\nSpeaker recognition for multi-speaker conversations using x-vectors. D Snyder, D Garcia-Romero, G Sell, A Mccree, D Povey, S Khudanpur, Proc. ICASSP. ICASSPD. Snyder, D. Garcia-Romero, G. Sell, A. McCree, D. Povey, and S. Khudanpur, \"Speaker recognition for multi-speaker conversa- tions using x-vectors,\" in Proc. ICASSP, 2019, pp. 5796-5800.\n\nJHU-HLTCOE system for the VoxSRC speaker recognition challenge. D Garcia-Romero, A Mccree, D Snyder, G Sell, Proc. ICASSP, 2020. ICASSP, 2020D. Garcia-Romero, A. McCree, D. Snyder, and G. Sell, \"JHU- HLTCOE system for the VoxSRC speaker recognition challenge,\" in Proc. ICASSP, 2020, pp. 7559-7563.\n\nBUT system description to VoxCeleb speaker recognition challenge 2019. H Zeinali, S Wang, A Silnova, P Matjka, O Plchot, H. Zeinali, S. Wang, A. Silnova, P. Matjka, and O. Plchot, \"BUT system description to VoxCeleb speaker recognition challenge 2019,\" 2019.\n\nProbabilistic linear discriminant analysis. S Ioffe, ECCV. S. Ioffe, \"Probabilistic linear discriminant analysis,\" in ECCV, 2006, pp. 531-542.\n\nArcFace: Additive angular margin loss for deep face recognition. J Deng, J Guo, N Xue, S Zafeiriou, 2019 IEEE/CVF CVPR. J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \"ArcFace: Additive angular margin loss for deep face recognition,\" in 2019 IEEE/CVF CVPR, 2019, pp. 4685-4694.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE/CVF CVPR. K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in IEEE/CVF CVPR, 2016, pp. 770-778.\n\nAttentive statistics pooling for deep speaker embedding. K Okabe, T Koshinaka, K Shinoda, Proc. Interspeech. InterspeechK. Okabe, T. Koshinaka, and K. Shinoda, \"Attentive statistics pooling for deep speaker embedding,\" in Proc. Interspeech, 2018, pp. 2252-2256.\n\nSelfattentive speaker embeddings for text-independent speaker verification. Y Zhu, T Ko, D Snyder, B K , .-W Mak, D Povey, Proc. Interspeech. InterspeechY. Zhu, T. Ko, D. Snyder, B. K.-W. Mak, and D. Povey, \"Self- attentive speaker embeddings for text-independent speaker verifi- cation,\" in Proc. Interspeech, 2018, pp. 3573-3577.\n\nVoxCeleb: A largescale speaker identification dataset. A Nagrani, J S Chung, A Zisserman, Proc. Interspeech. InterspeechA. Nagrani, J. S. Chung, and A. Zisserman, \"VoxCeleb: A large- scale speaker identification dataset,\" in Proc. Interspeech, 2017, pp. 2616-2620.\n\nVoxCeleb2: Deep speaker recognition. J S Chung, A Nagrani, A Zisserman, Proc. Interspeech. InterspeechJ. S. Chung, A. Nagrani, and A. Zisserman, \"VoxCeleb2: Deep speaker recognition,\" in Proc. Interspeech, 2018, pp. 1086-1090.\n\nVoxSRC 2019: The first VoxCeleb speaker recognition challenge. J S Chung, A Nagrani, E Coto, W Xie, M Mclaren, D A Reynolds, A Zisserman, J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, and A. Zisserman, \"VoxSRC 2019: The first VoxCeleb speaker recognition challenge,\" 2019.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, Proc. ICML. ICMLS. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing internal covariate shift,\" in Proc. ICML, 2015, pp. 448-456.\n\nSqueeze-and-Excitation networks. J Hu, L Shen, G Sun, Proc. IEEE/CVF CVPR. IEEE/CVF CVPRJ. Hu, L. Shen, and G. Sun, \"Squeeze-and-Excitation networks,\" in Proc. IEEE/CVF CVPR, 2018, pp. 7132-7141.\n\nRes2Net: A new multi-scale backbone architecture. S Gao, M.-M Cheng, K Zhao, X Zhang, M.-H Yang, P H S Torr, IEEE TPAMI. S. Gao, M.-M. Cheng, K. Zhao, X. Zhang, M.-H. Yang, and P. H. S. Torr, \"Res2Net: A new multi-scale backbone architec- ture,\" IEEE TPAMI, 2019.\n\nMulti-level and multi-scale feature aggregation using sample-level deep convolutional neural networks for music classification. J Lee, J Nam, J. Lee and J. Nam, \"Multi-level and multi-scale feature aggrega- tion using sample-level deep convolutional neural networks for music classification,\" 2017.\n\nImproving Aggregation and Loss Function for Better Embedding Learning in End-to-End Speaker Verification System. Z Gao, Y Song, I Mcloughlin, P Li, Y Jiang, L.-R Dai, Proc. Interspeech. InterspeechZ. Gao, Y. Song, I. McLoughlin, P. Li, Y. Jiang, and L.-R. Dai, \"Improving Aggregation and Loss Function for Better Embedding Learning in End-to-End Speaker Verification System,\" in Proc. Interspeech, 2019, pp. 361-365.\n\nSemi-orthogonal low-rank matrix factorization for deep neural networks. D Povey, G Cheng, Y Wang, K Li, H Xu, M Yarmohammadi, S Khudanpur, Proc. Interspeech. InterspeechD. Povey, G. Cheng, Y. Wang, K. Li, H. Xu, M. Yarmohammadi, and S. Khudanpur, \"Semi-orthogonal low-rank matrix factoriza- tion for deep neural networks,\" in Proc. Interspeech, 2018, pp. 3743-3747.\n\nMUSAN: A music, speech, and noise corpus. D Snyder, G Chen, D Povey, D. Snyder, G. Chen, and D. Povey, \"MUSAN: A music, speech, and noise corpus,\" 2015.\n\nA study on data augmentation of reverberant speech for robust speech recognition. T Ko, V Peddinti, D Povey, M L Seltzer, S Khudanpur, Proc. ICASSP. ICASSPT. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, \"A study on data augmentation of reverberant speech for robust speech recognition,\" in Proc. ICASSP, 2017, pp. 5220-5224.\n\nSpecAugment: A simple data augmentation method for automatic speech recognition. D S Park, W Chan, Y Zhang, C.-C Chiu, B Zoph, E D Cubuk, Q V Le, Proc. Interspeech. InterspeechD. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \"SpecAugment: A simple data augmen- tation method for automatic speech recognition,\" in Proc. Inter- speech, 2019.\n\nCyclical learning rates for training neural networks. L N Smith, IEEE. L. N. Smith, \"Cyclical learning rates for training neural net- works,\" in IEEE WACV, 2017, pp. 464-472.\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, Proc. ICLR. ICLRD. Kingma and J. Ba, \"Adam: A method for stochastic optimiza- tion,\" Proc. ICLR, 2014.\n\nMargin matters: Towards more discriminative deep neural network embeddings for speaker recognition. X Xiang, S Wang, H Huang, Y Qian, K Yu, X. Xiang, S. Wang, H. Huang, Y. Qian, and K. Yu, \"Margin mat- ters: Towards more discriminative deep neural network embed- dings for speaker recognition,\" 2019.\n\nAnalysis of score normalization in multilingual speaker recognition. P Matejka, O Novotn, O Plchot, L Burget, M Diez, J Ernock, Proc. Interspeech. InterspeechP. Matejka, O. Novotn, O. Plchot, L. Burget, M. Diez, and J. er- nock, \"Analysis of score normalization in multilingual speaker recognition,\" in Proc. Interspeech, 2017, pp. 1567-1571.\n\nShort-duration speaker verification (SdSV) challenge 2020: the challenge evaluation plan. H Zeinali, K A Lee, J Alam, L Burget, H. Zeinali, K. A. Lee, J. Alam, and L. Burget, \"Short-duration speaker verification (SdSV) challenge 2020: the challenge evalu- ation plan,\" 2019.\n", "annotations": {"author": "[{\"end\":243,\"start\":108},{\"end\":377,\"start\":244},{\"end\":480,\"start\":378}]", "publisher": null, "author_last_name": "[{\"end\":126,\"start\":115},{\"end\":261,\"start\":251},{\"end\":391,\"start\":383}]", "author_first_name": "[{\"end\":114,\"start\":108},{\"end\":250,\"start\":244},{\"end\":382,\"start\":378}]", "author_affiliation": "[{\"end\":242,\"start\":156},{\"end\":376,\"start\":290},{\"end\":479,\"start\":393}]", "title": "[{\"end\":105,\"start\":1},{\"end\":585,\"start\":481}]", "venue": null, "abstract": "[{\"end\":2156,\"start\":685}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2202,\"start\":2199},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2240,\"start\":2237},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2242,\"start\":2240},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2244,\"start\":2242},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3095,\"start\":3092},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3235,\"start\":3232},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3355,\"start\":3352},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3474,\"start\":3471},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3476,\"start\":3474},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3619,\"start\":3616},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3831,\"start\":3828},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3833,\"start\":3831},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4843,\"start\":4839},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4846,\"start\":4843},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4849,\"start\":4846},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5386,\"start\":5382},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5486,\"start\":5483},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5488,\"start\":5486},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5490,\"start\":5488},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5555,\"start\":5552},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6086,\"start\":6083},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6420,\"start\":6417},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6520,\"start\":6517},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6706,\"start\":6703},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7113,\"start\":7109},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7356,\"start\":7353},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7484,\"start\":7481},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7834,\"start\":7831},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9709,\"start\":9706},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9711,\"start\":9709},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9713,\"start\":9711},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10076,\"start\":10072},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11185,\"start\":11182},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11918,\"start\":11914},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12552,\"start\":12548},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12555,\"start\":12552},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13112,\"start\":13108},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13115,\"start\":13112},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13574,\"start\":13570},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13638,\"start\":13634},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14013,\"start\":14010},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14091,\"start\":14087},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14137,\"start\":14133},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14605,\"start\":14601},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14890,\"start\":14886},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14934,\"start\":14930},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15036,\"start\":15033},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15039,\"start\":15036},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15543,\"start\":15539},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15964,\"start\":15960},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16328,\"start\":16324},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16364,\"start\":16360},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17311,\"start\":17308},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17313,\"start\":17311},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17589,\"start\":17586},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19529,\"start\":19525},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20300,\"start\":20296}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20409,\"start\":20163},{\"attributes\":{\"id\":\"fig_1\"},\"end\":20694,\"start\":20410},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":21341,\"start\":20695},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":21732,\"start\":21342}]", "paragraph": "[{\"end\":3096,\"start\":2172},{\"end\":3620,\"start\":3098},{\"end\":4093,\"start\":3622},{\"end\":4455,\"start\":4095},{\"end\":5065,\"start\":4457},{\"end\":5387,\"start\":5101},{\"end\":6329,\"start\":5414},{\"end\":6743,\"start\":6355},{\"end\":7172,\"start\":6780},{\"end\":7776,\"start\":7225},{\"end\":7872,\"start\":7778},{\"end\":8496,\"start\":7923},{\"end\":8777,\"start\":8533},{\"end\":8896,\"start\":8799},{\"end\":9069,\"start\":8930},{\"end\":9499,\"start\":9071},{\"end\":10077,\"start\":9547},{\"end\":10312,\"start\":10079},{\"end\":10496,\"start\":10340},{\"end\":10932,\"start\":10553},{\"end\":11303,\"start\":10934},{\"end\":12195,\"start\":11305},{\"end\":12924,\"start\":12245},{\"end\":13437,\"start\":12926},{\"end\":14756,\"start\":13507},{\"end\":15282,\"start\":14758},{\"end\":15708,\"start\":15284},{\"end\":16241,\"start\":15733},{\"end\":16653,\"start\":16265},{\"end\":18177,\"start\":16665},{\"end\":18922,\"start\":18179},{\"end\":19612,\"start\":18924},{\"end\":20162,\"start\":19627}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7922,\"start\":7873},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8532,\"start\":8497},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8798,\"start\":8778},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8929,\"start\":8897},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10339,\"start\":10313},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10552,\"start\":10497}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16789,\"start\":16782},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17466,\"start\":17459}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2170,\"start\":2158},{\"attributes\":{\"n\":\"2.\"},\"end\":5099,\"start\":5068},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5412,\"start\":5390},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6353,\"start\":6332},{\"attributes\":{\"n\":\"3.\"},\"end\":6778,\"start\":6746},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7223,\"start\":7175},{\"attributes\":{\"n\":\"3.2.\"},\"end\":9545,\"start\":9502},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12243,\"start\":12198},{\"attributes\":{\"n\":\"4.\"},\"end\":13505,\"start\":13440},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15731,\"start\":15711},{\"attributes\":{\"n\":\"4.3.\"},\"end\":16263,\"start\":16244},{\"attributes\":{\"n\":\"5.\"},\"end\":16663,\"start\":16656},{\"attributes\":{\"n\":\"6.\"},\"end\":19625,\"start\":19615},{\"end\":20174,\"start\":20164},{\"end\":20421,\"start\":20411},{\"end\":20705,\"start\":20696},{\"end\":21352,\"start\":21343}]", "table": "[{\"end\":21341,\"start\":20801},{\"end\":21732,\"start\":21400}]", "figure_caption": "[{\"end\":20409,\"start\":20176},{\"end\":20694,\"start\":20423},{\"end\":20801,\"start\":20707},{\"end\":21400,\"start\":21354}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7073,\"start\":7065},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11339,\"start\":11331},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13436,\"start\":13428}]", "bib_author_first_name": "[{\"end\":21793,\"start\":21792},{\"end\":21803,\"start\":21802},{\"end\":21820,\"start\":21819},{\"end\":21828,\"start\":21827},{\"end\":21837,\"start\":21836},{\"end\":22108,\"start\":22107},{\"end\":22118,\"start\":22117},{\"end\":22135,\"start\":22134},{\"end\":22143,\"start\":22142},{\"end\":22153,\"start\":22152},{\"end\":22162,\"start\":22161},{\"end\":22448,\"start\":22447},{\"end\":22465,\"start\":22464},{\"end\":22475,\"start\":22474},{\"end\":22485,\"start\":22484},{\"end\":22755,\"start\":22754},{\"end\":22766,\"start\":22765},{\"end\":22774,\"start\":22773},{\"end\":22785,\"start\":22784},{\"end\":22795,\"start\":22794},{\"end\":22988,\"start\":22987},{\"end\":23153,\"start\":23152},{\"end\":23161,\"start\":23160},{\"end\":23168,\"start\":23167},{\"end\":23175,\"start\":23174},{\"end\":23409,\"start\":23408},{\"end\":23415,\"start\":23414},{\"end\":23424,\"start\":23423},{\"end\":23431,\"start\":23430},{\"end\":23633,\"start\":23632},{\"end\":23642,\"start\":23641},{\"end\":23655,\"start\":23654},{\"end\":23915,\"start\":23914},{\"end\":23922,\"start\":23921},{\"end\":23928,\"start\":23927},{\"end\":23938,\"start\":23937},{\"end\":23940,\"start\":23939},{\"end\":23946,\"start\":23943},{\"end\":23953,\"start\":23952},{\"end\":24227,\"start\":24226},{\"end\":24238,\"start\":24237},{\"end\":24240,\"start\":24239},{\"end\":24249,\"start\":24248},{\"end\":24475,\"start\":24474},{\"end\":24477,\"start\":24476},{\"end\":24486,\"start\":24485},{\"end\":24497,\"start\":24496},{\"end\":24729,\"start\":24728},{\"end\":24731,\"start\":24730},{\"end\":24740,\"start\":24739},{\"end\":24751,\"start\":24750},{\"end\":24759,\"start\":24758},{\"end\":24766,\"start\":24765},{\"end\":24777,\"start\":24776},{\"end\":24779,\"start\":24778},{\"end\":24791,\"start\":24790},{\"end\":25058,\"start\":25057},{\"end\":25067,\"start\":25066},{\"end\":25283,\"start\":25282},{\"end\":25289,\"start\":25288},{\"end\":25297,\"start\":25296},{\"end\":25497,\"start\":25496},{\"end\":25507,\"start\":25503},{\"end\":25516,\"start\":25515},{\"end\":25524,\"start\":25523},{\"end\":25536,\"start\":25532},{\"end\":25544,\"start\":25543},{\"end\":25548,\"start\":25545},{\"end\":25840,\"start\":25839},{\"end\":25847,\"start\":25846},{\"end\":26125,\"start\":26124},{\"end\":26132,\"start\":26131},{\"end\":26140,\"start\":26139},{\"end\":26154,\"start\":26153},{\"end\":26160,\"start\":26159},{\"end\":26172,\"start\":26168},{\"end\":26502,\"start\":26501},{\"end\":26511,\"start\":26510},{\"end\":26520,\"start\":26519},{\"end\":26528,\"start\":26527},{\"end\":26534,\"start\":26533},{\"end\":26540,\"start\":26539},{\"end\":26556,\"start\":26555},{\"end\":26839,\"start\":26838},{\"end\":26849,\"start\":26848},{\"end\":26857,\"start\":26856},{\"end\":27033,\"start\":27032},{\"end\":27039,\"start\":27038},{\"end\":27051,\"start\":27050},{\"end\":27060,\"start\":27059},{\"end\":27062,\"start\":27061},{\"end\":27073,\"start\":27072},{\"end\":27373,\"start\":27372},{\"end\":27375,\"start\":27374},{\"end\":27383,\"start\":27382},{\"end\":27391,\"start\":27390},{\"end\":27403,\"start\":27399},{\"end\":27411,\"start\":27410},{\"end\":27419,\"start\":27418},{\"end\":27421,\"start\":27420},{\"end\":27430,\"start\":27429},{\"end\":27432,\"start\":27431},{\"end\":27717,\"start\":27716},{\"end\":27719,\"start\":27718},{\"end\":27883,\"start\":27882},{\"end\":27893,\"start\":27892},{\"end\":28103,\"start\":28102},{\"end\":28112,\"start\":28111},{\"end\":28120,\"start\":28119},{\"end\":28129,\"start\":28128},{\"end\":28137,\"start\":28136},{\"end\":28374,\"start\":28373},{\"end\":28385,\"start\":28384},{\"end\":28395,\"start\":28394},{\"end\":28405,\"start\":28404},{\"end\":28415,\"start\":28414},{\"end\":28423,\"start\":28422},{\"end\":28739,\"start\":28738},{\"end\":28750,\"start\":28749},{\"end\":28752,\"start\":28751},{\"end\":28759,\"start\":28758},{\"end\":28767,\"start\":28766}]", "bib_author_last_name": "[{\"end\":21800,\"start\":21794},{\"end\":21817,\"start\":21804},{\"end\":21825,\"start\":21821},{\"end\":21834,\"start\":21829},{\"end\":21847,\"start\":21838},{\"end\":22115,\"start\":22109},{\"end\":22132,\"start\":22119},{\"end\":22140,\"start\":22136},{\"end\":22150,\"start\":22144},{\"end\":22159,\"start\":22154},{\"end\":22172,\"start\":22163},{\"end\":22462,\"start\":22449},{\"end\":22472,\"start\":22466},{\"end\":22482,\"start\":22476},{\"end\":22490,\"start\":22486},{\"end\":22763,\"start\":22756},{\"end\":22771,\"start\":22767},{\"end\":22782,\"start\":22775},{\"end\":22792,\"start\":22786},{\"end\":22802,\"start\":22796},{\"end\":22994,\"start\":22989},{\"end\":23158,\"start\":23154},{\"end\":23165,\"start\":23162},{\"end\":23172,\"start\":23169},{\"end\":23185,\"start\":23176},{\"end\":23412,\"start\":23410},{\"end\":23421,\"start\":23416},{\"end\":23428,\"start\":23425},{\"end\":23435,\"start\":23432},{\"end\":23639,\"start\":23634},{\"end\":23652,\"start\":23643},{\"end\":23663,\"start\":23656},{\"end\":23919,\"start\":23916},{\"end\":23925,\"start\":23923},{\"end\":23935,\"start\":23929},{\"end\":23950,\"start\":23947},{\"end\":23959,\"start\":23954},{\"end\":24235,\"start\":24228},{\"end\":24246,\"start\":24241},{\"end\":24259,\"start\":24250},{\"end\":24483,\"start\":24478},{\"end\":24494,\"start\":24487},{\"end\":24507,\"start\":24498},{\"end\":24737,\"start\":24732},{\"end\":24748,\"start\":24741},{\"end\":24756,\"start\":24752},{\"end\":24763,\"start\":24760},{\"end\":24774,\"start\":24767},{\"end\":24788,\"start\":24780},{\"end\":24801,\"start\":24792},{\"end\":25064,\"start\":25059},{\"end\":25075,\"start\":25068},{\"end\":25286,\"start\":25284},{\"end\":25294,\"start\":25290},{\"end\":25301,\"start\":25298},{\"end\":25501,\"start\":25498},{\"end\":25513,\"start\":25508},{\"end\":25521,\"start\":25517},{\"end\":25530,\"start\":25525},{\"end\":25541,\"start\":25537},{\"end\":25553,\"start\":25549},{\"end\":25844,\"start\":25841},{\"end\":25851,\"start\":25848},{\"end\":26129,\"start\":26126},{\"end\":26137,\"start\":26133},{\"end\":26151,\"start\":26141},{\"end\":26157,\"start\":26155},{\"end\":26166,\"start\":26161},{\"end\":26176,\"start\":26173},{\"end\":26508,\"start\":26503},{\"end\":26517,\"start\":26512},{\"end\":26525,\"start\":26521},{\"end\":26531,\"start\":26529},{\"end\":26537,\"start\":26535},{\"end\":26553,\"start\":26541},{\"end\":26566,\"start\":26557},{\"end\":26846,\"start\":26840},{\"end\":26854,\"start\":26850},{\"end\":26863,\"start\":26858},{\"end\":27036,\"start\":27034},{\"end\":27048,\"start\":27040},{\"end\":27057,\"start\":27052},{\"end\":27070,\"start\":27063},{\"end\":27083,\"start\":27074},{\"end\":27380,\"start\":27376},{\"end\":27388,\"start\":27384},{\"end\":27397,\"start\":27392},{\"end\":27408,\"start\":27404},{\"end\":27416,\"start\":27412},{\"end\":27427,\"start\":27422},{\"end\":27435,\"start\":27433},{\"end\":27725,\"start\":27720},{\"end\":27890,\"start\":27884},{\"end\":27896,\"start\":27894},{\"end\":28109,\"start\":28104},{\"end\":28117,\"start\":28113},{\"end\":28126,\"start\":28121},{\"end\":28134,\"start\":28130},{\"end\":28140,\"start\":28138},{\"end\":28382,\"start\":28375},{\"end\":28392,\"start\":28386},{\"end\":28402,\"start\":28396},{\"end\":28412,\"start\":28406},{\"end\":28420,\"start\":28416},{\"end\":28430,\"start\":28424},{\"end\":28747,\"start\":28740},{\"end\":28756,\"start\":28753},{\"end\":28764,\"start\":28760},{\"end\":28774,\"start\":28768}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":46954166},\"end\":22036,\"start\":21734},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":146117456},\"end\":22381,\"start\":22038},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":216523461},\"end\":22681,\"start\":22383},{\"attributes\":{\"id\":\"b3\"},\"end\":22941,\"start\":22683},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":41807584},\"end\":23085,\"start\":22943},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8923541},\"end\":23360,\"start\":23087},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206594692},\"end\":23573,\"start\":23362},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4407761},\"end\":23836,\"start\":23575},{\"attributes\":{\"id\":\"b8\"},\"end\":24169,\"start\":23838},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10475843},\"end\":24435,\"start\":24171},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49211906},\"end\":24663,\"start\":24437},{\"attributes\":{\"id\":\"b11\"},\"end\":24961,\"start\":24665},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5808102},\"end\":25247,\"start\":24963},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":140309863},\"end\":25444,\"start\":25249},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":91184391},\"end\":25709,\"start\":25446},{\"attributes\":{\"id\":\"b15\"},\"end\":26009,\"start\":25711},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":202714308},\"end\":26427,\"start\":26011},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4949673},\"end\":26794,\"start\":26429},{\"attributes\":{\"id\":\"b18\"},\"end\":26948,\"start\":26796},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":23138179},\"end\":27289,\"start\":26950},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":121321299},\"end\":27660,\"start\":27291},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15247298},\"end\":27836,\"start\":27662},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6628106},\"end\":28000,\"start\":27838},{\"attributes\":{\"id\":\"b23\"},\"end\":28302,\"start\":28002},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2716412},\"end\":28646,\"start\":28304},{\"attributes\":{\"id\":\"b25\"},\"end\":28922,\"start\":28648}]", "bib_title": "[{\"end\":21790,\"start\":21734},{\"end\":22105,\"start\":22038},{\"end\":22445,\"start\":22383},{\"end\":22985,\"start\":22943},{\"end\":23150,\"start\":23087},{\"end\":23406,\"start\":23362},{\"end\":23630,\"start\":23575},{\"end\":23912,\"start\":23838},{\"end\":24224,\"start\":24171},{\"end\":24472,\"start\":24437},{\"end\":25055,\"start\":24963},{\"end\":25280,\"start\":25249},{\"end\":25494,\"start\":25446},{\"end\":26122,\"start\":26011},{\"end\":26499,\"start\":26429},{\"end\":27030,\"start\":26950},{\"end\":27370,\"start\":27291},{\"end\":27714,\"start\":27662},{\"end\":27880,\"start\":27838},{\"end\":28371,\"start\":28304}]", "bib_author": "[{\"end\":21802,\"start\":21792},{\"end\":21819,\"start\":21802},{\"end\":21827,\"start\":21819},{\"end\":21836,\"start\":21827},{\"end\":21849,\"start\":21836},{\"end\":22117,\"start\":22107},{\"end\":22134,\"start\":22117},{\"end\":22142,\"start\":22134},{\"end\":22152,\"start\":22142},{\"end\":22161,\"start\":22152},{\"end\":22174,\"start\":22161},{\"end\":22464,\"start\":22447},{\"end\":22474,\"start\":22464},{\"end\":22484,\"start\":22474},{\"end\":22492,\"start\":22484},{\"end\":22765,\"start\":22754},{\"end\":22773,\"start\":22765},{\"end\":22784,\"start\":22773},{\"end\":22794,\"start\":22784},{\"end\":22804,\"start\":22794},{\"end\":22996,\"start\":22987},{\"end\":23160,\"start\":23152},{\"end\":23167,\"start\":23160},{\"end\":23174,\"start\":23167},{\"end\":23187,\"start\":23174},{\"end\":23414,\"start\":23408},{\"end\":23423,\"start\":23414},{\"end\":23430,\"start\":23423},{\"end\":23437,\"start\":23430},{\"end\":23641,\"start\":23632},{\"end\":23654,\"start\":23641},{\"end\":23665,\"start\":23654},{\"end\":23921,\"start\":23914},{\"end\":23927,\"start\":23921},{\"end\":23937,\"start\":23927},{\"end\":23943,\"start\":23937},{\"end\":23952,\"start\":23943},{\"end\":23961,\"start\":23952},{\"end\":24237,\"start\":24226},{\"end\":24248,\"start\":24237},{\"end\":24261,\"start\":24248},{\"end\":24485,\"start\":24474},{\"end\":24496,\"start\":24485},{\"end\":24509,\"start\":24496},{\"end\":24739,\"start\":24728},{\"end\":24750,\"start\":24739},{\"end\":24758,\"start\":24750},{\"end\":24765,\"start\":24758},{\"end\":24776,\"start\":24765},{\"end\":24790,\"start\":24776},{\"end\":24803,\"start\":24790},{\"end\":25066,\"start\":25057},{\"end\":25077,\"start\":25066},{\"end\":25288,\"start\":25282},{\"end\":25296,\"start\":25288},{\"end\":25303,\"start\":25296},{\"end\":25503,\"start\":25496},{\"end\":25515,\"start\":25503},{\"end\":25523,\"start\":25515},{\"end\":25532,\"start\":25523},{\"end\":25543,\"start\":25532},{\"end\":25555,\"start\":25543},{\"end\":25846,\"start\":25839},{\"end\":25853,\"start\":25846},{\"end\":26131,\"start\":26124},{\"end\":26139,\"start\":26131},{\"end\":26153,\"start\":26139},{\"end\":26159,\"start\":26153},{\"end\":26168,\"start\":26159},{\"end\":26178,\"start\":26168},{\"end\":26510,\"start\":26501},{\"end\":26519,\"start\":26510},{\"end\":26527,\"start\":26519},{\"end\":26533,\"start\":26527},{\"end\":26539,\"start\":26533},{\"end\":26555,\"start\":26539},{\"end\":26568,\"start\":26555},{\"end\":26848,\"start\":26838},{\"end\":26856,\"start\":26848},{\"end\":26865,\"start\":26856},{\"end\":27038,\"start\":27032},{\"end\":27050,\"start\":27038},{\"end\":27059,\"start\":27050},{\"end\":27072,\"start\":27059},{\"end\":27085,\"start\":27072},{\"end\":27382,\"start\":27372},{\"end\":27390,\"start\":27382},{\"end\":27399,\"start\":27390},{\"end\":27410,\"start\":27399},{\"end\":27418,\"start\":27410},{\"end\":27429,\"start\":27418},{\"end\":27437,\"start\":27429},{\"end\":27727,\"start\":27716},{\"end\":27892,\"start\":27882},{\"end\":27898,\"start\":27892},{\"end\":28111,\"start\":28102},{\"end\":28119,\"start\":28111},{\"end\":28128,\"start\":28119},{\"end\":28136,\"start\":28128},{\"end\":28142,\"start\":28136},{\"end\":28384,\"start\":28373},{\"end\":28394,\"start\":28384},{\"end\":28404,\"start\":28394},{\"end\":28414,\"start\":28404},{\"end\":28422,\"start\":28414},{\"end\":28432,\"start\":28422},{\"end\":28749,\"start\":28738},{\"end\":28758,\"start\":28749},{\"end\":28766,\"start\":28758},{\"end\":28776,\"start\":28766}]", "bib_venue": "[{\"end\":21869,\"start\":21863},{\"end\":22194,\"start\":22188},{\"end\":22524,\"start\":22512},{\"end\":23695,\"start\":23684},{\"end\":23991,\"start\":23980},{\"end\":24291,\"start\":24280},{\"end\":24539,\"start\":24528},{\"end\":25093,\"start\":25089},{\"end\":25337,\"start\":25324},{\"end\":26208,\"start\":26197},{\"end\":26598,\"start\":26587},{\"end\":27105,\"start\":27099},{\"end\":27467,\"start\":27456},{\"end\":27914,\"start\":27910},{\"end\":28462,\"start\":28451},{\"end\":21861,\"start\":21849},{\"end\":22186,\"start\":22174},{\"end\":22510,\"start\":22492},{\"end\":22752,\"start\":22683},{\"end\":23000,\"start\":22996},{\"end\":23205,\"start\":23187},{\"end\":23450,\"start\":23437},{\"end\":23682,\"start\":23665},{\"end\":23978,\"start\":23961},{\"end\":24278,\"start\":24261},{\"end\":24526,\"start\":24509},{\"end\":24726,\"start\":24665},{\"end\":25087,\"start\":25077},{\"end\":25322,\"start\":25303},{\"end\":25565,\"start\":25555},{\"end\":25837,\"start\":25711},{\"end\":26195,\"start\":26178},{\"end\":26585,\"start\":26568},{\"end\":26836,\"start\":26796},{\"end\":27097,\"start\":27085},{\"end\":27454,\"start\":27437},{\"end\":27731,\"start\":27727},{\"end\":27908,\"start\":27898},{\"end\":28100,\"start\":28002},{\"end\":28449,\"start\":28432},{\"end\":28736,\"start\":28648}]"}}}, "year": 2023, "month": 12, "day": 17}