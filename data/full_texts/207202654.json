{"id": 207202654, "updated": "2023-07-19 05:51:16.108", "metadata": {"title": "Bundled camera paths for video stabilization", "authors": "[{\"first\":\"Shuaicheng\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Lu\",\"last\":\"Yuan\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Sun\",\"middle\":[]}]", "venue": null, "journal": "ACM Transactions on Graphics (TOG)", "publication_date": {"year": 2013, "month": null, "day": null}, "abstract": "We present a novel video stabilization method which models camera motion with a bundle of (multiple) camera paths. The proposed model is based on a mesh-based, spatially-variant motion representation and an adaptive, space-time path optimization. Our motion representation allows us to fundamentally handle parallax and rolling shutter effects while it does not require long feature trajectories or sparse 3D reconstruction. We introduce the 'as-similar-as-possible' idea to make motion estimation more robust. Our space-time path smoothing adaptively adjusts smoothness strength by considering discontinuities, cropping size and geometrical distortion in a unified optimization framework. The evaluation on a large variety of consumer videos demonstrates the merits of our method.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2057412674", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tog/LiuYT013", "doi": "10.1145/2461912.2461995"}}, "content": {"source": {"pdf_hash": "e19dc0fb2d78266143e7b18759326bdb87c7f9df", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cf2bced99233e567342c923e046eac058aef03ac", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e19dc0fb2d78266143e7b18759326bdb87c7f9df.txt", "contents": "\nBundled Camera Paths for Video Stabilization\n2013. July 2013\n\nAcm Reference Format \nS Liu \nL Yuan \nP Tan \nJ Sun \nBundled Camera Paths for Video Stabilization\n\nACM Trans. Graph\n322013. July 201310.1145/2461912.2461995/13/07-ART78 $15.00.CR Categories: I43 [Image Processing and Computer Vision]: Enhancement-Registration Keywords: video stabilizationimage warpingcamera paths Links: DL PDF\na) a single global path (b) our bundled pathsFigure 1: Comparison between traditional 2D stabilization (a single global camera path) and our bundled camera paths stabilization. We plot the camera trajectories (visualized by the y-axis translation over time) and show the original path (red) and the smoothed path (blue) for both methods. Our bundled paths rely on a 2D mesh-based motion representation, and are smoothed in space-time.AbstractWe present a novel video stabilization method which models camera motion with a bundle of (multiple) camera paths. The proposed model is based on a mesh-based, spatially-variant motion representation and an adaptive, space-time path optimization. Our motion representation allows us to fundamentally handle parallax and rolling shutter effects while it does not require long feature trajectories or sparse 3D reconstruction. We introduce the 'as-similaras-possible' idea to make motion estimation more robust. Our space-time path smoothing adaptively adjusts smoothness strength by considering discontinuities, cropping size and geometrical distortion in a unified optimization framework. The evaluation on a large variety of consumer videos demonstrates the merits of our method.\n\nIntroduction\n\nA video captured with a hand-held device (e.g., a cell-phone or a portable camcorder) often appears remarkably shaky and undirected. Digital video stabilization improves the video quality by removing unwanted camera motion. It is of great practical importance because the devices (mobile phones, tablets, camcorders) capable of capturing video have become widespread and online sharing is so ubiquitous.\n\nPrior video stabilization methods synthesized a new stabilized video by estimating and smoothing 2D camera motion [Matsushita et al. 2006;Grundmann et al. 2011] or 3D camera motion [Liu et al. 2009;Liu et al. 2012]. In general, 2D methods are more robust and faster because they only estimate a linear transformation (affine or homography) between consecutive frames. But the 2D linear motion model is too weak to fundamentally handle the parallax caused by non-trivial depth variation in the scene. On the contrary, the 3D methods can deal with the parallax in principle and generate strongly stabilized results. However, their motion model estimation is less robust to various degenerations such as feature tracking failure, motion blur, camera zooming, and rapid rotation. Briefly, 2D methods are more robust but may sacrifice quality (e.g., introducing unpleasant geometrical distortion or producing less stabilized output), while 3D methods can achieve high-quality results but are more fragile.\n\nSome recent methods Goldstein and Fattal 2012] have successfully combined the advantages of these two kinds of methods. Liu et al. [2011] applied a low-rank, subspace constraint on 2D feature trajectories, which is an effective simplification of 3D reconstruction. Goldstein and Fattal [2012] avoided 3D reconstruction by exploiting the 'epipolar transfer' technique. These methods relax the requirement from 3D reconstruction to 2D long feature tracking. Nevertheless, requiring long feature tracking (typically over 20 frames) makes it difficult to handle more challenging cases (e.g., rapid motion, fast scene transition, large occlusion) in the consumer videos.\n\nThis paper aims at the same goal of robust high-quality result but from an opposite direction: we propose a more powerful 2D camera motion model. Specifically, we present bundled camera paths model which maintains multiple, spatially-variant camera paths. In other words, each different location in the video has its own camera path. This flexible model allows us to fundamentally deal with nonlinear motion caused by parallax and rolling shutter effects [Liang et al. 2008;Baker et al. 2010;Grundmann et al. 2012]. At the same time, the model enjoys the robustness and simplicity of 2D methods, because it only requires feature correspondences between two consecutive frames.\n\nOur bundled camera paths model is built on two novel components: a warping-based motion representation (and estimation), and an adaptive space-time path smoothing. The first component represents the motion between two consecutive frames by mesh-based, spatially-variant homographies (Figure 1(b)) with a 'as-similar-aspossible' regularization constraint [Igarashi et al. 2005;Schaefer et al. 2006]. This constraint is critical because estimating a model with such a high degree of freedom is usually risky in the cases of insufficient features or large occlusions. To the best of our knowledge, this is the first work to employ the mesh-based 'as-similar-aspossible' regularization for spatially-variant motion estimation in video stabilization. Notice that the 'as-similar-as-possible' warping was used in [Liu et al. 2009;Liu et al. 2011] for video stabilization. But we directly use the mesh vertices as the motion model itself. No intermediate representation is used, such as 3D reconstruction [Liu et al. 2009] or subspace ].\n\nBased on the proposed motion representation, we construct a bundle of camera paths, each of which is the concatenation of local homographies at the same grid cell over time (Figure 1(b)). Our second component smooths all bundled camera paths as a whole to maintain both spatial and temporal coherences. Furthermore, to avoid excessive cropping/geometrical distortion and approximate cinematography favored path, we adopt a discontinuity-preserving idea similar to bilateral filtering [Tomasi and Manduchi 1998] to adaptively control the strength of smoothing.\n\nFor a quantitative evaluation, we provide a comprehensive dataset (including both public examples and our own video clips of different kinds of motions). We show that our new 2D method is comparable to or outperforms other competitive 2D or 3D methods.\n\n\nRelated Work\n\n2D Methods estimate 2D transformations between consecutive video frames and smooth them over time to generate a steady video. Most previously developed methods apply an affine or homography model, and focus on the design of the smoothing algorithm. Earlier works [Morimoto and Chellappa 1998;Matsushita et al. 2006] apply low-pass filters to individual model parameters. Some methods assume prior motion models such as polynomial curves  for desired camera trajectories. Gleicher and Liu [2007] divide the original camera trajectory into multiple segments for subsequent individual smoothing. More recently, Grundmann et al. [2011] gracefully apply L1-norm optimization to generate a camera path consisting of constant, linear and parabolic motions, which follow cinematography rules. Grundmann et al. [2012] further adopt a homography-array-based motion model to deal with rolling shutter effects. These two techniques have been integrated into Google YouTube. It is robust, follows cinematography rules, and performs well on many consumer videos.\n\nOur method belongs to this category. But we use a spatially-variant model to represent the motion between video frames and design an appropriate smoothing technique for this model.\n\n\n3D\n\nMethods often rely on robust feature tracking for stabilization. Beuhler et al. [2001] perform stabilization with a projective 3D reconstruction of the scene from an uncalibrated camera. Liu et al. [2009] develop the first successful 3D video stabilization system and are the first to introduce 'content-preserving' warping for stabilization.\n\nSince 3D reconstruction is difficult, recent methods directly smooth the trajectories of tracked features. Liu et al. [2011] smooth some basis trajectories (preferably longer than 50 frames) of the subspace formed by the feature tracks. This method achieves similar quality to 3D reconstruction-based methods, while reducing the require-ment from 3D reconstruction to long feature tracking. It has been transferred to Adobe After Effects as a feature called \"Warp Stabilizer\". Goldstein and Fattal [2012] utilize an \"epipolar transfer\" technique to avoid the fragile 3D reconstruction. This technique also alleviates the strain on long feature tracks. But it still requires moderate feature track length (typically over 20 frames). Feature track smoothing is also used in light-field camera video stabilization work [Smith et al. 2009]. To address the occlusion issue, Lee et al. [2009] introduce feature pruning to choose robust feature trajectories for smoothing.\n\nNearly all methods involving feature tracking face a common obstacle -in many consumer videos obtaining long feature tracks is fragile due to occlusion, motion blur or rapid camera motion. Our method does not encounter this issue since it only computes relative motion between consecutive frames.\n\nMotion Estimation computes the transition between two images with view overlap. Optical flow algorithms [Lucas and Kanade 1981] model this transition by individual displacement vectors at every pixel. When there is no parallax, this transition can be represented elegantly by a global homography transformation [Hartley and Zisserman 2003]. Local alignment [Shum and Szeliski 2000] or a dual-homography model [Gao et al. 2011] can reduce alignment error caused by parallax. Szeliski and Shum [1996] represent motion using a mixture of spline models with spatially variant spatial support to facilitate registration. Lin et al. [2011] estimate a smoothly varying affine field to align images of large viewpoint changes. This model can be potentially used for video stabilization. However, its current motion estimation technique is slow (may take 8 minutes to process a 720p frame).\n\nOur motion model is essentially a mesh-based, spatially-variant homography model, inspired by recent image warping techniques [Igarashi et al. 2005;Schaefer et al. 2006;Liu et al. 2009]. We extend the \"as-similar-as-possible\" idea from image synthesis to motion estimation, and apply it to video stabilization. It is very efficient to estimate our motion model (may take only 50 milliseconds to process a 720p frame).\n\nRolling Shutter Removal estimates and corrects inter-row motion caused by the row-parallel readout, i.e., electronic rolling shutter [Nakamura 2005] mainly in CMOS sensors. Prior works design different parametric inter-row motion models, including a per-row translation model [Liang et al. 2008;Baker et al. 2010] and 3D rotation model [Forss\u00e9n and Ringaby 2010]. Recently, Grundmann et al. [2012] proposed a calibration-free homography mixture model, which shows significant improvement. Karpenko et al. [2011] use dedicated hardware -the gyroscope on mobile devices, to correct the rolling shutter effects in real-time.\n\nSimilar to [Grundmann et al. 2012], our method corrects rolling shutter effects without any prior calibration. Our warping-based model naturally handles the rolling shutter effects as a special kind of spatially variant motion. So we do not need a separate rolling shutter correction step in our stabilization.\n\n\nBundled Camera Paths\n\nIn this section, we introduce our warping-based motion model and bundled camera paths.\n\n\nWarping-based Motion Model\n\nWe propose using an image warping model to represent the motion between consecutive video frames, which provides stronger modeling power than conventional single, 2D linear transformations. We adopt the warping model in [Igarashi et al. 2005;Liu et al. 2009  The as-similar-as-possible term requires each trianglev,v0,v1 to follow a similarity transformation.\n\nthough more general models such as 'moving-least-square' [Schaefer et al. 2006] or parameterized optical flow [Nir et al. 2008] might be used.\n\nModel At each frame, we define a uniform grid mesh as illustrated in Figure 2. The motion is represented by an (unknown) warping of the grid mesh to register two frames (in fact, their corresponding feature points). We require matched features (e.g., p andp in Figure 2) to share the same bilinear interpolation of the four corners of the enclosing grid cell after warping. At the i-th grid cell, the warping from frame t to frame t + 1 introduces a homography Fi(t), which can be determined from the motion of the four enclosing vertices. Thus, the warping-based motion model is actually a set of spatially-variant homographies on a 2D grid.\n\nNote that this highly flexible model is able to handle parallax. It is between global homography and per-pixel optical flow. However, estimating a model with such a high degree of freedom is very risky because we may not have sufficient features (due to textureless regions or occlusions) in every cell.\n\nRegularization To address this challenge, we propose imposing a shape-preserving (i.e., \"as-similar-as-possible\" [Igarashi et al. 2005]) constraint. The combination of the shape-preserving and mesh representation together provides two kinds of regularizations: 1) for each cell, the fitted homography should be biased toward a reduced similarity (or rigid) transformation; 2) the intrinsic connection of the mesh (two neighboring mesh cells share two vertices) enforces a first-order continuity constraint. They can help to propagate or fill in information from regions with sufficient features to other regions.\n\nFinally, we estimate the motion by minimizing two energy terms: a data term for matching features, and a shape-preserving term for enforcing regularization.\n\n\nModel Estimation\n\nWe first describe our basic method by following [Liu et al. 2009], and later extend it for better robustness in the next subsection.\n\nData term As shown in Figure 2, suppose {p,p} is the p-th matched feature pair from frame t to frame t + 1. The feature p can be represented by a 2D bilinear interpolation of the four vertices\nVp = [v 1 p , v 2 p , v 3 p , v 4 p ] of the enclosing grid cell: p = Vpwp, where wp = [w 1 p , w 2 p , w 3 p , w 4 p ]\nare interpolation weights that sum to 1. We expect that the corresponding featurep can be represented by the same weights of the warped grid verticesVp\n= [v 1 p ,v 2 p ,v 3 p ,v 4 p ]\n. Therefore the data term is defined as\nE d (V ) = p ||Vpwp \u2212p|| 2 .\n(1)\n\nwith shape-preserving no shape-preserving Figure 3: Comparison of motion estimation with and without the shape-preserving term.\n\nHereV contains all the warped grid vertices. SolvingV determines the warping of the grid.\n\nShape-preserving term We use the same shape-preserving term as [Liu et al. 2009] involving all vertices inV ,\nEs(V ) = v v \u2212v1 \u2212 sR90(v0 \u2212v1) 2 , R90 = 0 1 \u22121 0 ,(2)\nwhere s = v \u2212 v1 / v0 \u2212 v1 is a known scalar computed from the initial mesh. This shape-preserving term requires the triangle of neighboring vertices v, v0, v1 to follow a similarity transformation.\n\nLinearly combining two terms forms our final energy E(V ):\nE(V ) = E d (V ) + \u03b1Es(V ),(3)\nwhere \u03b1 is an important weight to control the amount of regularization. We will discuss how to adaptively determine it later. Since the energy E(V ) is quadratic, the warped meshV can be easily solved by a sparse linear system solver.\n\nEstimating homographies After having a new mesh, we can estimate each local homography Fi(t) in the grid cell i of frame t by solving a linear equation\n:V i = Fi(t)Vi,(4)\nwhere Vi andVi are the four vertices before and after the warping. Figure 3 shows the warped mesh grid according to the estimated motion. Left and right are the results with and without the shapepreserving term. It is clear that the regularization term helps maintain a smooth varying mesh representation.\n\n\nRobust Estimation\n\nWe further generalize our motion estimation to make it more robust.\n\nOutlier rejection We reject incorrectly matched features at two scales. At the coarse scale (the whole image), we apply RANSAC algorithm [Fischler and Bolles 1981] to fit a global homograph\u0233 F (t) and discard features by a relatively large threshold on fitting error (6% image width). At the fine scale (4 \u00d7 4 sub-images), we apply RANSAC again to reject features by a relatively small threshold (2% image width).\n\nPre-warping To facilitate the warping estimation, we use global homographyF (t) to bring matching features closer. We then solve the warping to estimate the residual motion, which generates a homography F i (t) at each grid cell. The final homography Fi(t) is simply computed as F i (t) \u00d7F (t). Note that this coarse-to-fine strategy has been used in [Liu et al. 2009] for image synthesis and proven effective in motion estimation literature [Brox et al. 2004].\n\nBundled Camera Paths for Video Stabilization \u2022 78:3  Adaptive regularization A good regularization should be adaptive to image content. For example, if reliable features are uniformly distributed over the whole image, we should trust the data term more and use a smaller weight \u03b1 in Equation 3 for a weaker regularization. But when there is occlusion or insufficient features, we prefer stronger regularization as the data term is less reliable. To implement this strategy, we adaptively set \u03b1 per frame, based on two errors: fitting error e h and smoothness error es.\n0.9 optimal \u03b1 \u03b1 = = 0.9 \u03b1 = 3.0 \u03b1 = 3.0 optimal \u03b1 \u03b1 = = (a) (b) 0. 3 0.6 0.\nThe fitting error e h is the average residual of the feature matching under the estimated homographies, i.e., e h = 1 n p Fp \u00d7 p \u2212p 2 , where Fp is the homography in the cell containing p, and n is the number of feature pairs. The smoothness error es measures the similarity (L2 distance) between neighboring local homographies by es = \u03b2 j\u2208\u2126 i Fi \u2212 Fj 2 , where \u2126i consists of the neighboring cells of i. Here, the homography matrix is normalized so that sum of all its elements is one. We empirically set \u03b2 = 0.01, since it makes the scale of e h and es similar on most of the examples. Then we define the combined error as e = e h + es. We equally discretize \u03b1 into 10 values between 0.3 and 3. We perform the model estimation using every discretized value and select the model with minimum error e. Figure 4(a), for simple scenes with smooth depth variation, neighboring cells tend to have similar homographies. So we choose a small \u03b1(=0.9) to better minimize the data error. On the contrary, for scenes with large occlusion (Figure 4(b)), neighboring local homographies are less similar. The smoothness error can be significantly reduced by increasing \u03b1. So our system will automatically choose a large \u03b1(=3.0) to ensure consistent local motion.\n\n\nAs shown in\n\nFinally, we show an example in Figure 5 to verify the strength of the regularization of our method. In this example, we compare two meshes estimated using all features and a subset of features. Two similar results indicate our method can robustly deal with regions of insufficient features.\n\n\nBundled Camera Paths\n\nWith estimated local homographies, we can define a bundle of spatially-variant camera paths for the whole video. Let Ci(t) be ( ) the camera pose of the grid cell i at frame t. It can be written as:\ni C t ( ) j C t ( 1) j C t + ( 1) i C t + ( ) i F t ( ) j F t ( ) F t ( ) C t ( 1) C t + ( 1) P t + ( ) P t ( ) B t ( 1) B t +Ci(t) = Ci(t \u2212 1)Fi(t \u2212 1), \u21d2 Ci(t) = Fi(0)Fi(1) \u00b7 \u00b7 \u00b7 Fi(t \u2212 1),\nwhere {Fi (0), ..., Fi(t \u2212 1)} are estimated local homographies at the same grid cell i, as shown in Figure 6 (a). We call these spatially-variant paths as \"bundled camera paths\". In the next section, we describe how we smoothen these bundled paths for video stabilization.\n\n\nPath Optimization\n\nWe first describe our smoothing method for a single camera path, and extend it to a bundle of camera paths.\n\n\nOptimizing a Single Path\n\nA good camera path smoothing should consider multiple competing factors: removing jitters, avoiding excessive cropping, and minimizing various geometrical distortions (shearing/skewing, wobble). To reach a desired balance, we propose an optimization-based framework taking all factors into account.\n\nFormulation Given an original path C = {C(t)}, we seek an optimized path P = {P (t)} by minimizing the following function:\nO ({P (t)}) = t P (t) \u2212 C(t) 2 + \u03bbt r\u2208\u2126 t \u03c9t,r (C) \u00b7 P (t) \u2212 P (r) 2 ,(5)\nwhere \u2126t are the neighborhood at frame t. The other terms are:\n\n\u2022 data term P (t) \u2212 C(t) 2 enforcing the new camera path to be close to the original one to reduce cropping and distortion;\n\n\u2022 smoothness term P (t) \u2212 P (r) 2 stabilizing the path;\n\n\u2022 weight \u03c9t,r (C) to preserve motion discontinuities under fast panning/rotation or scene transition;  \u2022 parameter \u03bbt to balance the above two terms.\n\nSince Equation 5 is quadratic, we can solve it with any linear system solver. Here, we use a Jacobi-based iterative solver [Bronshtein and Semendyayev 1997]:\nP (\u03be+1) (t) = 1 \u03b3 C(t) + r\u2208\u2126 t ,r =t 2\u03bbt\u03c9t,r \u03b3 P (\u03be) (r),(6)\nwhere \u03b3 = 1 + 2\u03bbt r\u2208\u2126 t ,r =t \u03c9t,r, and \u03be is an iteration index. At initialization, P (0) (t) = C(t). Once we obtain the optimized path P, we compute the warping transform B(t) = C \u22121 (t)P (t) to warp the original video frame to the stabilized result ( Figure 6(b)).\n\n\nDiscontinuity-preserving\n\nThe adaptive weight \u03c9t,r is important to preserve motion discontinuity. We follow the idea of bilateral filter [Tomasi and Manduchi 1998] and design it by two Gaussian functions:\n\u03c9t,r = G t ( r \u2212 t ) \u00b7 G m ( C(r) \u2212 C(t) ) ,(7)\nwhere Gt() gives larger weight to the nearby frames. Gm() measures the changes of two camera poses.\n\nWe use a large kernel to ensure successful suppression of both highfrequency jitters (e.g., handshake) and low-frequency bounces (e.g., walking). In our implementation, we set \u2126t to 60 neighboring frames and the standard deviation of Gt() to 10. In contrast, previous low-pass filtering based methods [Matsushita et al. 2006] typically need a smaller amount of support (e.g., 10 frames) to avoid aggressive cropping and distortion. But such a small kernel is often insufficient in suppressing low frequency bounces.\n\nThe reason why we can use a larger kernel lies in Gm(). In video stabilization, for rapid camera motion (e.g, caused by fast panning or scene transition), an inappropriate amount of smoothing may lead to excessive cropping, as shown in Figure 7. In this case, the camera pans quickly, and na\u00efve Gaussian smoothing (second row) causes the camera path to significantly deviate from its original path, as indicated by the dashed lines in the left plot on top. The corresponding frames shown on the second row will require large cropping. Our adaptive term Gm() preserves the sudden camera motions to a certain degree. The result from our adaptive smoothing (bottom row) produces much less cropping.\n\nTo measure the camera motion, we use the change in translation components \u00b5x(t), \u00b5y(t) extracted from the camera pose C(t), namely |\u00b5x(t) \u2212 \u00b5x(r)| + |\u00b5y(t) \u2212 \u00b5y(r)|. The frame translation \u00b5x(t), \u00b5y(t) can describe most camera motions in practice except for an in-plane rotation or scale around the principal axis.\n\nCropping and distortion control The above adaptive term \u03c9t,r can give us a certain amount of ability to control cropping and distortion. However, the user may want to have strict control on the cropping ratio and distortion. In principle, we could formulate a constrained optimization to address this issue. But it may be too complex to be solved or reproduced.\n\nIn this work, we resort to a simple but effective method -adaptively adjust the parameter \u03bbt for each frame. We first run the optimization with a global fixed \u03bbt = \u03bb (empirically set to 5) and then check the cropping ratio and distortion of every frame. For any frame that does not satisfy the user requirements (cropping ratio or distortion is smaller than a pre-defined threshold), we decrease its parameter \u03bbt by a step (1/10\u03bbt) and re-run the optimization. Note, according to Equation 6, a smaller \u03bb will make the optimized path closer to the original one, which has less cropping and distortions. The procedure is iterated until all frames satisfy the requirements.\n\nWe measure the cropping ratio and distortion from the warping transform B(t) = C \u22121 (t)P (t). The anisotropic scaling of B(t) measures the distortion. It can be computed by the ratio of the two largest eigenvalues of the affine part of B(t) [Hartley and Zisserman 2003]. We use B(t) to compute the overlapping area of the original video frame and the stabilized frame. The cropping ratio is the ratio of this area and the original frame area. In our experiments, we require the cropping ratio to be larger than 0.8, and the distortion score to be larger than 0.95 for all examples. In principle, we can further measure the perspective distortion by the two perspective components in B(t). But we empirically find they are always too small when compared with the affine components and do not include them.\n\n\nOptimizing Bundled Paths\n\nOur motion model generates a bundle of camera paths. If these paths are optimized independently, neighboring paths could be less consistent, which may generate distortion in the final rendered video. Hence, we do a space-time optimization of all paths by minimizing the following objective function\ni O ({Pi(t)}) + t j\u2208N (i) Pi(t) \u2212 Pj(t) 2 ,(8)\nwhere N (i) includes eight neighbors of the grid cell i.\n\nThe first term is the objective function in Equation 5 for each single path, and the second term enforces the smoothness between neighboring paths. This optimization is also quadratic and the optimum result can be obtained by solving a large sparse linear system. Again, our solution is updated by a Jacobi-based iteration [Bronshtein and Semendyayev 1997]:\nP (\u03be+1) i (t) = 1 \u03b3 (Ci(t)+ r\u2208\u2126 t r =t 2\u03bbtwt,rP (\u03be) i (r)+ j\u2208N (i) j =i 2P (\u03be) j (t)),\nwhere \u03b3 = 2\u03bbt r\u2208\u2126 t ,r =t wt,r + 2N (i) \u2212 1.\n\n\nBundled Camera\nPaths for Video Stabilization \u2022 78:5\nWe typically iterate 20 times to optimize camera paths.\n\nDuring optimization, the motion-adaptive term Gm(\u00b7) is evaluated at individual cells, since different cells have different motion. In comparison, \u03bbt is determined from the global path (generated by concatenating the pre-warping global homographies), because it controls the overall cropping and distortion. Then, we use \u03bbt to optimize the camera paths in all cells.\n\n\nResult synthesis\n\nAfter path optimization, we compute the warping matrix Bi(t) for each cell i by Bi(t) = C \u22121 i (t)Pi(t). We then apply Bi(t) to warp the i-th cell at the t-th frame to generate the final output video. Usually, applying Bi(t) directly generates good results. This is because our motion estimation ensures first order smoothness of the original paths. Furthermore, the bundled optimization in Equation 8 requires nearby optimized paths to be similar. Thus, the smoothness is naturally satisfied by Bi(t) most of the time. Sometimes, there are slight distortions (e.g., seams of about 1-pixel width), in which case we perform a bilinear interpolation to fix them.\n\n\nCorrecting Rolling Shutter Effects\n\nOur bundled paths model can naturally handle rolling shutter effects without pre-calibration. The principle of our method is similar to that of [Grundmann et al. 2012]. Our system does rolling shutter correction while simultaneously stabilizing the video. In a shaky video, a rolling shutter causes spatially variant high frequency jitters. When smoothing the camera paths, we simultaneously rectify rolling shutter effects and other jitters caused by camera shake.\n\n\nResults\n\nWe run our method on an Intel i7 3.2GHZ Quad-Core machine with 8G RAM. We extract 400-600 SURF features [Bay et al. 2008] per frame. For motion estimation, we always divide the video frame to 16 \u00d7 16 cells. For a video of 1280 \u00d7 720 resolution, our unoptimized system takes 392 milliseconds to process a frame (around 2.5fps). Specifically, we spend 300ms, 50ms, 12ms and 30ms to extract features, estimate motion, optimize camera paths and render the final result. All original and result videos are provided on our webpage 1 .\n\n\nAlgorithm Validation\n\nWe first verify the effectiveness of different components of the proposed approach.\n\nA Global Path vs. Bundled Paths For the example in Figure 1, the result according to a global path has remaining jitters in some image regions. This is because the parallax makes the global homography motion model invalid, therefore some image regions cannot be stabilized very well. But our bundled paths can handle this kind of typical situation. Please refer to our accompanying video for a visual comparison. Grundmann et al. [2012] proposed a homography mixture model for rolling shutter correction. They divide a video frame into a 1D array of horizontal blocks, and use a Gaussian mixture of homographies for each block. This model is beyond a single 2D transformation and able to partially handle parallax. Compared with our 2D mesh-based, spatially-variant homographies, this model has two limitations: 1) it does not address horizontal depth variation; 2) it uses weaker feature points (which apply lower threshold level for feature detection) and a simple Gaussian mixture for the regularization. Weaker feature points may result in larger fitting errors and the ability to use simple Gaussian smoothing is limited. Figure 8 shows a comparison of these two models. In this example, the scene has horizontal depth variation and the sky region lacks feature points. Figure 8 (a) is the result of using YouTube Stabilizer (integrated Homography Mixture feature). We can observe severe geometrical distortions. To further verify our observation, we replace our spatially-variant model with the homography mixture model (our implementation) in our framework and generate the result in Figure 8 (d), where we observe similar distortion. In comparison, our warping-based motion estimation can fundamentally handle depth variation (not limited to vertical direction). Our result (Figure 8 (c)) does not suffer from such distortion. Please also see the comparison in the accompanying video.\n\n\nSpatially-variant Homographies vs. Homography Mixture\n\nRolling Shutter Handling Figure 9 compares our methods with [Grundmann et al. 2012] on two example videos from their paper. Our model accounts for frame distortions such as skew (left example) and local wobble (right example). More examples are included in the supplementary video, which shows we achieve similar results on correcting rolling shutter distortion as [Grundmann et al. 2012].\n\n\nQuantitative Evaluation\n\nTo quantitatively evaluate and measure the result from different aspects, we define three objective metrics.\n\nCropping and distortion Our first two metrics measure cropping ratio and global distortion. We first fit a global homography at each frame between input video and output video. We then compute the cropping ratio and distortion for each frame. The cropping ratio can be directly computed from the scale component of the homography. There is one global cropping ratio for the whole sequence, and each frame provides an estimation. We average these estimations at all frames as the final metric. The distortion is computed as defined in Section 4.1. Because any distortion in a single frame will destroy the perfection of the whole result, we choose their minimum across the whole sequence as the final metric. This \"worst-case\" metric 78:6 \u2022 S. Liu et al.\n\ninput frames [Grundmann et al. 2012] our results input frames [Grundmann et al. 2012] our results Figure 9: Two rolling shutter removal examples using our method and [Grundmann et al. 2012]. Our results are on par with that from [Grundmann et al. 2012]. Please see video for a full comparison.\n\nallows us to easily see whether the whole result video is completely successful. For a good result, both metrics should be close to 1.\n\nStability The third metric measures the stability of the result. Designing a good metric is non-trivial because it is hard to compare two different videos. We suggest an empirically good metric using frequency analysis on estimated 2D motion from a video. Our basic assumption is that the more energy is contained in the low frequency part of the motion, the more stable a video is.\n\nComputationally, we estimate our bundled camera paths to approximate the true motion (optical flow) in a video. We do not smooth out anything after the estimation. Then, we extract translation and rotation components from each path. Each component is a 1D temporal signal. Finally, we evaluate the energy percentage of the low frequency components (expect for DC component) in these 1D signals to measure the stability.\n\nSpecifically, we take a few of the lowest (empirically set as from the 2nd to the 6th) frequencies and calculate the energy percentage over full frequencies (excluded by the DC component). Similar to the distortion, we take the smallest measurement among the translation and rotation as the final metric. For a good result, the metric should approach 1 here as well.\n\n\nComparison with Publicly Available Results\n\nThe purpose of this comparison is to test whether our results are comparable with (if not better than) previous \"successful\" results in [Liu et al. 2009;Liu et al. 2011;Goldstein and Fattal 2012;Grundmann et al. 2011]. We collect eleven test videos from these papers (thumbnails in Figure 10), and compare our results with their published results (all from authors' project webpages).\n\nOverall, all methods generate similar stability both subjectively and quantitatively ( Figure 10) on these examples, while our results are slightly better on some videos in terms of cropping ratio and distortion.\n\nFor video (2)-(4), 3D stabilization [Liu et al. 2009] achieves the best stability and distortion scores. It suggests that 3D methods are the first choice (in term of stability and distortion error), when the 3D motion can be successfully estimated. Although our results are slightly worse in stability, the visual difference is quite small (please verify from the supplementary video). Furthermore, the aggressive smoothing in 3D methods sometimes leads to an output FOV that is too small as demonstrated by the cropping score. Our method manages to provide a good trade-off. For video (5-9), , [Goldstein and Fattal 2012], and our method achieve similar stability, while our method is slightly better in cropping and ] input our result Figure 11: Comparison with a failure case of prior methods.\n\ndistortion. For video (10-11) 2 , our method outperforms the L1optimization [Grundmann et al. 2011] in stability (slightly), cropping ratio, and distortion scores. \n\n\nComparison with the State-of-the-Art Systems\n\nDue to no publicly available implementation of previous works, we compare our system with two well-known commercial systems -YouTube Stabilizer and 'Warp Stabilizer' in Adobe After Effects CS6. The YouTube Stabilizer is based on the combination of the L1-norm path optimization [Grundmann et al. 2011] and homography mixtures [Grundmann et al. 2012]. The 'Warp Stabilizer' in Adobe After Effects is largely based on subspace stabilization . We understand that commercial products are often different from a given research system. But we believe these two systems represent the essential elements of research conducted in this field, and the comparison makes sense for examining strengths or weaknesses and robustness (for various videos using a set of fixed parameters) of our system.\n\n\nDataset\n\nWe assemble a comprehensive dataset of 174 short videos (10 \u223c 60 seconds) from previous publications, Internet, and our own captures. To know the strength and weakness of a method in different situations, we roughly divide our data into 7 categories based on camera motion and scene type. They are: (I) simple, (II) quick rotation, (III) zooming, (IV) large parallax, (V) driving, (VI) crowd, and (VII) running. YouTube Stabilizer is a parameter-free online tool. But 'Warp Stabilizer' is an interactive system, and the user might carefully tune a few parameters. Here, we wish to examine its robustness as an automatic tool by fixing its parameters. We use the example videos in ] to decide the best parameters. Finally, we choose the default parameters (smoothness: 50%, 'Smooth Motion' and 'Subspace Warp') to produce results.\n\n\nQuantitative Comparison\n\nFor each category, we compute the average metrics and standard deviation of three systems (Figure 12 (a)). We discuss the results with regard to each system in detail below.\n\nAll three systems perform well in category (I) \"simple\", since this category contains videos with relatively smooth camera motion and mild depth variations. Though our method has a minor advantage, the users can safely choose any of three to get a desired result.\n\nAmong the remaining categories, we want to highlight the category (IV) \"large parallax\". The three systems achieve similar stability, while our system is clearly better in terms of distortion. We show two examples in Figure 12  Categories (II-III) contain quick rotation or zooming, which are challenging cases for methods requiring long feature tracking. 'Warp Stabilizer' often generates significant cropping. Figure 12(e) is such an example. To alleviate this problem, we try to interactively tune its smoothing parameters. When applying a weaker smoothing, however, we find its result becomes shaky. In comparison, our method generates stable results with much less cropping. For categories (V-VII), the three systems generate similar stability levels ('Warp Stabilizer' is slightly better in category VII), while our sys-tem is consistently better with respect to either cropping ratio or distortion control.\n\nWe notice that our method generates relatively smaller standard deviations of the three metrics for all categories. It suggests that our method generates more consistent results from various inputs.\n\n\nUser Study\n\nWe further conduct a user study with 40 participants to evaluate and compare our method with the YouTube Stabilizer and the 'Warp Stabilizer' in Adobe AfterEffects CS6. Every participant is required to evaluate results on 28 different input videos (randomly sampled from our dataset), in which there are 4 videos for each category mentioned above (The 4 video are prepared in the way that two of them compare our result to YouTube Stabilizer, and the other two to 'Warp Stabilizer'). In the user study, we use the scheme of forced two-alternative choice. Every participant is asked to pick a better one between the results of our method and YouTube Stabilizer, or between the results of our method and the 'Warp Stabilizer'. These videos are displayed to the subjects in a random order. The subjects are unaware of the video categories. Neither do they know which technique is used to produce the stabilized results. Figure 13 (a) shows such an interface for the user study. The original video is displayed on the top. The two stabilized ones are shown side-by-side below. Users can simultaneously play input video and both two results to better examine the difference. And these videos can be played back and forth, or be paused at a certain frame to help users carefully make their decision. The user can also play each of these videos individually to examine their quality without other distractions. We ask users to disregard differences in aspect ratio, or sharpness since each one may undergo different video codecs or further post-processing which makes uniform treatment difficult.\n\nThe user study results are shown in 13 (b). For each category, we show the average percentage of user preference. In general, the majority of all users showed significant preference towards our results when compared to any of the other two systems respectively. In particular, the participants prefer the overall quality of our results for category (IV) \"large parallax\" over YouTube Stabilizer (72% vs. 28%) and 'Warp Stabilizer' (69% vs. 31%). The result is consistent with our metric evaluation. For category (II-III) containing quick rotation or zooming, users show a strong bias in preference toward our results over 'Warp Stabilizer' (93% vs. 7% for rotation,   83% vs. 17% for zooming). This is possibly due to the significant cropping in the results of 'Warp Stabilizer'. For categories (V-VII), more participants prefer our results to the other two systems, although the three systems generate similar stability levels according to our stability metric. It is likely because of the superior distortion and cropping control in our method. In category (I) \"simple\", users express similar preference toward three results.\n\nAfter the user study, we also ask all participants to articulate the criteria for their feedbacks. We conclude the main criteria for unacceptable videos: 1) the video gets a smaller field of view or even contains frames with visible empty (black) area; 2) the video presents structure distortions in individual frames; 3) the motions in some video frames vibrate or oscillate; 4) the scene transition looks abrupt or not smoothed in the video. From these criteria, our proposed metrics can be partially related with human preferences. And both quantitative evaluation and user study results consistently indicate our system performs better than the other two systems.\n\n\nLimitations and Discussion\n\nWe find that when 3D reconstruction is successful, 3D methods often generate the best results. However, our system is more robust as we do not require feature tracking, and it produces comparable or only slightly worse results. It is interesting to note that our adaptive path optimization can also be applied to path smoothing for 3D methods [Liu et al. 2009;Liu et al. 2011;Goldstein and Fattal 2012], which often use low-pass filtering (Gaussian smoothing), or curve fitting for path planning. In comparison, our adaptive camera path smoothing technique can automatically adjust the smoothness strength by considering discontinuity and distortion. We show such an example video on our project webpage.\n\nThere are cases where the warping-based motion model fails to handle severe occlusions or dis-occlusions, especially when combined with rolling shutter effects. Figure 14 shows two such examples. Our warping-based motion model chooses a large \u03b1 to enforce strong coherence between grid cells. In this way, we can minimize the geometrical distortion, but at the same time, we sacrifice motion accuracy and eventually the stability of the result. In general, we find geometrical distortion is more disruptive than some slight remaining jitters.\n\nOur path optimization does not strictly follow cinematography rules, which may be desirable in certain applications. But our discontinuity-preservation optimization produces visually pleasing results in most examples. If necessary, we could apply the strategy in [Gleicher and Liu 2007] as a post-process to solve this problem. We also do not deal with motion blur. Sometimes, the stabilized results contain visible blur artifacts. This problem can be addressed by the recent work [Cho et al. 2012].\n\n\nConclusion\n\nWe have presented a new 2D video stabilization method with a bundled camera paths model. The proposed method can simultaneously generate comparable results to 3D methods while keeping merits of 2D methods. Using image warping techniques for motion representation is an interesting finding in this paper. In the future, we would  Figure 14: Two failure cases. The left is due to severe occlusion together with rolling shutter effects. The right is caused by the crowd.\n\nextend this kind of representation to other video-based applications.\n\nFigure 2 :\n2(a) Parameterization of the motion between two frames by a regular grid mesh, where a pair of matched features (p,p) should be represented by the same bilinear interpolation of their four enclosing vertices. (b)\n\nFigure 4 :\n4Our method automatically chooses an appropriate \u03b1 for different scenes: (a) a scene free of occlusion; (b) a scene with severe occlusion.\n\nFigure 5 :\n5Left: the estimated warping mesh from all feature points. Right: we exclude all the features in the orange box when estimating the warping model. A similar mesh can be obtained despite the lack of features.\n\nFigure 6 :\n6(a) Bundled camera paths. (b) Relationships among original path {C(t)}, smoothed path {P (t)}, and transformations {B(t)}\n\nFigure 7 :\n7Comparison of with and without adaptive weights Gm() for a video with rapid camera panning. The camera paths on the top plot the x-translation over time.\n\nFigure 8 :\n8Comparison with the homography mixture models in [Grundmann et al. 2012]. (a) A sample frame in the original video. (b) The output frame produced by YouTube Stabilizer. (c) The result produced by our method. (d) The result produced using our implementation of homography mixture [Grundmann et al. 2012] (with the same bundled path smoothing).\n\nFigure 11 highlights\n11the most challenging video (10) in this dataset.Liu et al. [2011] refer this example as a failure case because a single subspace cannot account for the feature trajectories on both the face and the background. Their results have visible distortion.[Grundmann et al. 2011] produced better result on this example. But in the video result, we still observe large temporal distortion on the background region. (See our accompanying video.) In comparison, our method can successfully handle this example (achieve best in terms of all three metrics) because the warping-based motion model can represent this complicated motion.\n\nFigure 10 :\n10Quantitative comparison with existing stabilization techniques on publicly available data.\n\n\n(b) and (c) for visual comparison of our system and the YouTube Stabilizer. These examples show the limitation of a 1D array of homography mixtures -it cannot model depth changes in horizontal direction. Warp Stabilizer also generates some shearing/skewing artifacts in some video frames, though in principle this 3D method should be able to handle parallax. shows such an example (please note the shearing of the bookshelf). This is probably due to the subspace analysis failure caused by occlusion. Our method succeeds in all of these examples. Comparison in this category clearly demonstrates the advantages of our warping-based motion model in dealing with a large parallax.\n\nFigure 12 :\n12Comparisons with two popular systems: YouTube Stabilizer and Adobe After Effect \"Warp Stabilizer\". Top: quantitative comparisons by three metrics: cropping (C), distortion (D) and stability (S). Bottom: some sample video frames for visual comparisons.\n\nFigure 13 :\n13simple (II) quick rotation (III) zooming (IV) large parallax (V) driving (VI) crowd (VII) running (a) Pair-wise comparison interface for user study. (b) User study results by comparing our method with two popular systems: YouTube Stabilizer and Adobe After Effect \"Warp Stabilizer\".\nACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\nhttp://www.ece.nus.edu.sg/stfpage/eletp/Projects/Stabilization/Stabili-zationSig13.html\nTo better measure stability on background motion (caused by camera shake), we use a manual foreground mask to exclude foreground motion.Bundled Camera Paths for Video Stabilization \u2022 78:7 ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\nAcknowledgementsWe thank all the reviewers for their helpful discussions, Kaimo Lin and other subjects for their help in the user study, Nathan Holdstein and Jiangyu Liu for their help in proofreading. This work is also partially supported by the Singapore project R-263-000-620-112.\nRemoving rolling shutter wobble. S Baker, E P Bennett, S B Kang, R Szeliski, Proc. CVPR. CVPRBAKER, S., BENNETT, E. P., KANG, S. B., AND SZELISKI, R. 2010. Removing rolling shutter wobble. In Proc. CVPR.\n\nSpeeded-up robust features (surf). H Bay, A Ess, T Tuytelaars, L Van Gool, Comput. Vis. Image Underst. 110BAY, H., ESS, A., TUYTELAARS, T., AND VAN GOOL, L. 2008. Speeded-up robust features (surf). Comput. Vis. Image Underst. 110, 3, 346-359.\n\nHandbook of Mathematics. I N Bronshtein, K A Semendyayev, Springer-VerlagNew York, NY, USABRONSHTEIN, I. N., AND SEMENDYAYEV, K. A. 1997. Hand- book of Mathematics. Springer-Verlag, New York, NY, USA.\n\nHigh accuracy optical flow estimation based on a theory for warping. T Brox, A Bruhn, N Papenberg, J Weickert, Proc. ECCV. ECCVBROX, T., BRUHN, A., PAPENBERG, N., AND WEICKERT, J. 2004. High accuracy optical flow estimation based on a theory for warping. In Proc. ECCV.\n\nNonmetric image-based rendering for video stabilization. C Buehler, M Bosse, L Mcmillan, Proc. CVPR. CVPRBUEHLER, C., BOSSE, M., AND MCMILLAN, L. 2001. Non- metric image-based rendering for video stabilization. In Proc. CVPR.\n\nCapturing intention-based full-frame video stabilization. B.-Y Chen, K.-Y Lee, W.-T Huang, J.-S Lin, Computer Graphics Forum. 27CHEN, B.-Y., LEE, K.-Y., HUANG, W.-T., AND LIN, J.-S. 2008. Capturing intention-based full-frame video stabilization. Com- puter Graphics Forum 27, 7, 1805-1814.\n\nVideo deblurring for hand-held cameras using patch-based synthesis. S Cho, J Wang, S Lee, Proc. of SIGGRAPH). of SIGGRAPH)4CHO, S., WANG, J., AND LEE, S. 2012. Video deblurring for hand-held cameras using patch-based synthesis. ACM Trans. Graph. (Proc. of SIGGRAPH) 31, 4.\n\nRandom sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. M A Fischler, R C Bolles, Commun. ACM. 24FISCHLER, M. A., AND BOLLES, R. C. 1981. Random sample consensus: a paradigm for model fitting with applications to im- age analysis and automated cartography. Commun. ACM 24, 6, 381-395.\n\nRectifying rolling shutter video from hand-held devices. P.-E Forss\u00e9n, E Ringaby, CVPR. FORSS\u00c9N, P.-E., AND RINGABY, E. 2010. Rectifying rolling shutter video from hand-held devices. In CVPR.\n\nConstructing image panoramas using dual-homography warping. J Gao, S J Kim, M S Brown, Proc. CVPR. CVPRGAO, J., KIM, S. J., AND BROWN, M. S. 2011. Constructing im- age panoramas using dual-homography warping. In Proc. CVPR.\n\nRe-cinematography: Improving the camera dynamics of casual video. M L Gleicher, F Liu, Proc. of ACM Multimedia. of ACM MultimediaGLEICHER, M. L., AND LIU, F. 2007. Re-cinematography: Im- proving the camera dynamics of casual video. In Proc. of ACM Multimedia.\n\nVideo stabilization using epipolar geometry. A Goldstein, R Fattal, ACM Trans. Graph. (TOG). 3110GOLDSTEIN, A., AND FATTAL, R. 2012. Video stabilization using epipolar geometry. ACM Trans. Graph. (TOG) 31, 5, 126:1- 126:10.\n\nAutodirected video stabilization with robust l1 optimal camera paths. M Grundmann, V Kwatra, I Essa, Proc. CVPR. CVPRGRUNDMANN, M., KWATRA, V., AND ESSA, I. 2011. Auto- directed video stabilization with robust l1 optimal camera paths. In Proc. CVPR.\n\nCalibration-free rolling shutter removal. M Grundmann, V Kwatra, D Castro, I Essa, Proc. ICCP. ICCPGRUNDMANN, M., KWATRA, V., CASTRO, D., AND ESSA, I. 2012. Calibration-free rolling shutter removal. In Proc. ICCP.\n\nMultiple View Geometry in Computer Vision. R Hartley, A Zisserman, Cambridge University Press2New York, NY, USAHARTLEY, R., AND ZISSERMAN, A. 2003. Multiple View Geome- try in Computer Vision, 2 ed. Cambridge University Press, New York, NY, USA.\n\nAsrigid-as-possible shape manipulation. T Igarashi, T Moscovich, J F Hughes, ACM Trans. Graph. (Proc. of SIGGRAPH). 24IGARASHI, T., MOSCOVICH, T., AND HUGHES, J. F. 2005. As- rigid-as-possible shape manipulation. ACM Trans. Graph. (Proc. of SIGGRAPH) 24, 3, 1134-1141.\n\nDigital video stabilization and rolling shutter correction using gyroscopes. A Karpenko, D Jacobs, J Baek, M Levoy, Stanford CS Tech ReportKARPENKO, A., JACOBS, D., BAEK, J., AND LEVOY, M. 2011. Digital video stabilization and rolling shutter correction using gyroscopes. In Stanford CS Tech Report.\n\nVideo stabilization using robust feature trajectories. K.-Y Lee, Y.-Y Chuang, B.-Y Chen, M Ouhyoung, Proc. ICCV. ICCVLEE, K.-Y., CHUANG, Y.-Y., CHEN, B.-Y., AND OUHYOUNG, M. 2009. Video stabilization using robust feature trajectories. In Proc. ICCV.\n\nAnalysis and compensation of rolling shutter effect. C.-K Liang, L.-W Chang, H H Chen, IEEE Trans. on Image Processing. LIANG, C.-K., CHANG, L.-W., AND CHEN, H. H. 2008. Analysis and compensation of rolling shutter effect. In IEEE Trans. on Image Processing.\n\nSmoothly varying affine stitching. W.-Y Lin, S Liu, Y Matsushita, T.-T Ng, L.-F Cheong, Proc. CVPR. CVPRLIN, W.-Y., LIU, S., MATSUSHITA, Y., NG, T.-T., AND CHEONG, L.-F. 2011. Smoothly varying affine stitching. In Proc. CVPR.\n\nContent-preserving warps for 3d video stabilization. F Liu, M Gleicher, H Jin, A Agarwala, Proc. of SIGGRAPH). of SIGGRAPH)28LIU, F., GLEICHER, M., JIN, H., AND AGARWALA, A. 2009. Content-preserving warps for 3d video stabilization. ACM Trans. Graph. (Proc. of SIGGRAPH) 28.\n\nSubspace video stabilization. F Liu, M Gleicher, J Wang, H Jin, A Agarwala, ACM Trans. Graph. 30LIU, F., GLEICHER, M., WANG, J., JIN, H., AND AGARWALA, A. 2011. Subspace video stabilization. ACM Trans. Graph. 30.\n\nVideo stabilization with a depth camera. S Liu, Y Wang, L Yuan, J Bu, P Tan, J Sun, Proc. CVPR. CVPRLIU, S., WANG, Y., YUAN, L., BU, J., TAN, P., AND SUN, J. 2012. Video stabilization with a depth camera. In Proc. CVPR.\n\nAn iterative image registration technique with an application to stereo vision. B D Lucas, T Kanade, Proc. of the International Joint Conference on Artificial Intelligence (IJCAI). of the International Joint Conference on Artificial Intelligence (IJCAI)LUCAS, B. D., AND KANADE, T. 1981. An iterative image reg- istration technique with an application to stereo vision. In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI), 674-679.\n\nFull-frame video stabilization with motion inpainting. Y Matsushita, E Ofek, W Ge, X Tang, H.-Y Shum, IEEE Trans. Pattern Anal. Mach. Intell. 28MATSUSHITA, Y., OFEK, E., GE, W., TANG, X., AND SHUM, H.- Y. 2006. Full-frame video stabilization with motion inpainting. IEEE Trans. Pattern Anal. Mach. Intell. 28, 1150-1163.\n\nEvaluation of image stabilization algorithms. C Morimoto, R Chellappa, Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing. of IEEE International Conference on Acoustics, Speech and Signal essingMORIMOTO, C., AND CHELLAPPA, R. 1998. Evaluation of image stabilization algorithms. In Proc. of IEEE International Confer- ence on Acoustics, Speech and Signal Processing, 2789 -2792.\n\nImage Sensors and Signal Processing for Digital Still Cameras. J Nakamura, CRC Press, IncNAKAMURA, J. 2005. Image Sensors and Signal Processing for Digital Still Cameras. CRC Press, Inc.\n\nOverparameterized variational optical flow. T Nir, A M Bruckstein, R Kimmel, Int. J. Comput. Vision (IJCV). 76NIR, T., BRUCKSTEIN, A. M., AND KIMMEL, R. 2008. Over- parameterized variational optical flow. Int. J. Comput. Vision (IJCV) 76, 2, 205-216.\n\nImage deformation using moving least squares. S Schaefer, T Mcphail, J Warren, ACM Trans. Graph. (Proc. of SIGGRAPH). 25SCHAEFER, S., MCPHAIL, T., AND WARREN, J. 2006. Image deformation using moving least squares. ACM Trans. Graph. (Proc. of SIGGRAPH) 25, 3, 533-540.\n\nConstruction of panoramic image mosaics with global and local alignment. H.-Y Shum, R Szeliski, Int. J. Comput. Vision (IJCV). 36SHUM, H.-Y., AND SZELISKI, R. 2000. Construction of panoramic image mosaics with global and local alignment. Int. J. Comput. Vision (IJCV) 36, 2, 101-130.\n\nLight field video stabilization. B M Smith, L Zhang, H Jin, A Agarwala, Proc. ICCV. ICCVSMITH, B. M., ZHANG, L., JIN, H., AND AGARWALA, A. 2009. Light field video stabilization. In Proc. ICCV.\n\nMotion estimation with quadtree splines. R Szeliski, IEEE Trans. Pattern Anal. Mach. Intell. 18SZELISKI, R. 1996. Motion estimation with quadtree splines. IEEE Trans. Pattern Anal. Mach. Intell. 18, 12, 1199-1210.\n\nBilateral filtering for gray and color images. C Tomasi, R Manduchi, Proc. ICCV. ICCVTOMASI, C., AND MANDUCHI, R. 1998. Bilateral filtering for gray and color images. In Proc. ICCV, 839-846.\n", "annotations": {"author": "[{\"end\":84,\"start\":63},{\"end\":91,\"start\":85},{\"end\":99,\"start\":92},{\"end\":106,\"start\":100},{\"end\":113,\"start\":107}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":63},{\"end\":90,\"start\":87},{\"end\":98,\"start\":94},{\"end\":105,\"start\":102},{\"end\":112,\"start\":109}]", "author_first_name": "[{\"end\":86,\"start\":85},{\"end\":93,\"start\":92},{\"end\":101,\"start\":100},{\"end\":108,\"start\":107}]", "author_affiliation": null, "title": "[{\"end\":45,\"start\":1},{\"end\":158,\"start\":114}]", "venue": "[{\"end\":176,\"start\":160}]", "abstract": "[{\"end\":1612,\"start\":390}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2171,\"start\":2147},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2193,\"start\":2171},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2231,\"start\":2214},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2246,\"start\":2231},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3081,\"start\":3055},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3172,\"start\":3155},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3327,\"start\":3300},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4176,\"start\":4157},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4194,\"start\":4176},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4215,\"start\":4194},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4756,\"start\":4734},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4776,\"start\":4756},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5204,\"start\":5187},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5220,\"start\":5204},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5395,\"start\":5378},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5921,\"start\":5896},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6534,\"start\":6505},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6556,\"start\":6534},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6736,\"start\":6713},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6873,\"start\":6850},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7050,\"start\":7027},{\"end\":7565,\"start\":7544},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7683,\"start\":7666},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7947,\"start\":7930},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8327,\"start\":8300},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8657,\"start\":8639},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8709,\"start\":8692},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9215,\"start\":9192},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9427,\"start\":9399},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9469,\"start\":9445},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9514,\"start\":9497},{\"end\":9586,\"start\":9562},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9721,\"start\":9704},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10119,\"start\":10097},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10140,\"start\":10119},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10155,\"start\":10140},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10537,\"start\":10523},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10685,\"start\":10666},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10702,\"start\":10685},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10752,\"start\":10726},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10787,\"start\":10764},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10901,\"start\":10879},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11046,\"start\":11024},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11707,\"start\":11685},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11722,\"start\":11707},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11905,\"start\":11883},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11952,\"start\":11936},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13053,\"start\":13032},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13774,\"start\":13758},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14714,\"start\":14698},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16057,\"start\":16031},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16676,\"start\":16660},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16768,\"start\":16751},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20869,\"start\":20836},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21363,\"start\":21338},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21879,\"start\":21856},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24388,\"start\":24360},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27209,\"start\":27186},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27639,\"start\":27623},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28593,\"start\":28570},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30189,\"start\":30167},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30494,\"start\":30472},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31425,\"start\":31402},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31474,\"start\":31451},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31577,\"start\":31555},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31641,\"start\":31618},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33191,\"start\":33174},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33207,\"start\":33191},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33233,\"start\":33207},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33255,\"start\":33233},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33690,\"start\":33674},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34260,\"start\":34233},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34535,\"start\":34512},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34949,\"start\":34927},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34997,\"start\":34975},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":41648,\"start\":41631},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":41664,\"start\":41648},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":41690,\"start\":41664},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42823,\"start\":42801},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":43035,\"start\":43019},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":44934,\"start\":44917},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":45139,\"start\":45117}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43815,\"start\":43591},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43966,\"start\":43816},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44186,\"start\":43967},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44321,\"start\":44187},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44488,\"start\":44322},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44844,\"start\":44489},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45490,\"start\":44845},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45596,\"start\":45491},{\"attributes\":{\"id\":\"fig_8\"},\"end\":46277,\"start\":45597},{\"attributes\":{\"id\":\"fig_9\"},\"end\":46544,\"start\":46278},{\"attributes\":{\"id\":\"fig_10\"},\"end\":46842,\"start\":46545}]", "paragraph": "[{\"end\":2031,\"start\":1628},{\"end\":3033,\"start\":2033},{\"end\":3700,\"start\":3035},{\"end\":4378,\"start\":3702},{\"end\":5410,\"start\":4380},{\"end\":5971,\"start\":5412},{\"end\":6225,\"start\":5973},{\"end\":7290,\"start\":6242},{\"end\":7472,\"start\":7292},{\"end\":7821,\"start\":7479},{\"end\":8788,\"start\":7823},{\"end\":9086,\"start\":8790},{\"end\":9969,\"start\":9088},{\"end\":10388,\"start\":9971},{\"end\":11011,\"start\":10390},{\"end\":11323,\"start\":11013},{\"end\":11434,\"start\":11348},{\"end\":11824,\"start\":11465},{\"end\":11968,\"start\":11826},{\"end\":12612,\"start\":11970},{\"end\":12917,\"start\":12614},{\"end\":13531,\"start\":12919},{\"end\":13689,\"start\":13533},{\"end\":13842,\"start\":13710},{\"end\":14036,\"start\":13844},{\"end\":14308,\"start\":14157},{\"end\":14380,\"start\":14341},{\"end\":14413,\"start\":14410},{\"end\":14542,\"start\":14415},{\"end\":14633,\"start\":14544},{\"end\":14744,\"start\":14635},{\"end\":14999,\"start\":14801},{\"end\":15059,\"start\":15001},{\"end\":15325,\"start\":15091},{\"end\":15478,\"start\":15327},{\"end\":15803,\"start\":15498},{\"end\":15892,\"start\":15825},{\"end\":16307,\"start\":15894},{\"end\":16770,\"start\":16309},{\"end\":17340,\"start\":16772},{\"end\":18666,\"start\":17417},{\"end\":18972,\"start\":18682},{\"end\":19195,\"start\":18997},{\"end\":19661,\"start\":19388},{\"end\":19790,\"start\":19683},{\"end\":20117,\"start\":19819},{\"end\":20241,\"start\":20119},{\"end\":20378,\"start\":20316},{\"end\":20503,\"start\":20380},{\"end\":20560,\"start\":20505},{\"end\":20711,\"start\":20562},{\"end\":20870,\"start\":20713},{\"end\":21198,\"start\":20932},{\"end\":21405,\"start\":21227},{\"end\":21553,\"start\":21454},{\"end\":22070,\"start\":21555},{\"end\":22767,\"start\":22072},{\"end\":23082,\"start\":22769},{\"end\":23445,\"start\":23084},{\"end\":24117,\"start\":23447},{\"end\":24923,\"start\":24119},{\"end\":25250,\"start\":24952},{\"end\":25354,\"start\":25298},{\"end\":25713,\"start\":25356},{\"end\":25845,\"start\":25801},{\"end\":25955,\"start\":25900},{\"end\":26322,\"start\":25957},{\"end\":27003,\"start\":26343},{\"end\":27507,\"start\":27042},{\"end\":28047,\"start\":27519},{\"end\":28155,\"start\":28072},{\"end\":30049,\"start\":28157},{\"end\":30496,\"start\":30107},{\"end\":30632,\"start\":30524},{\"end\":31387,\"start\":30634},{\"end\":31682,\"start\":31389},{\"end\":31818,\"start\":31684},{\"end\":32202,\"start\":31820},{\"end\":32623,\"start\":32204},{\"end\":32991,\"start\":32625},{\"end\":33422,\"start\":33038},{\"end\":33636,\"start\":33424},{\"end\":34434,\"start\":33638},{\"end\":34600,\"start\":34436},{\"end\":35433,\"start\":34649},{\"end\":36274,\"start\":35445},{\"end\":36475,\"start\":36302},{\"end\":36740,\"start\":36477},{\"end\":37655,\"start\":36742},{\"end\":37855,\"start\":37657},{\"end\":39459,\"start\":37870},{\"end\":40588,\"start\":39461},{\"end\":41257,\"start\":40590},{\"end\":41992,\"start\":41288},{\"end\":42536,\"start\":41994},{\"end\":43037,\"start\":42538},{\"end\":43519,\"start\":43052},{\"end\":43590,\"start\":43521}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14156,\"start\":14037},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14340,\"start\":14309},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14409,\"start\":14381},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14800,\"start\":14745},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15090,\"start\":15060},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15497,\"start\":15479},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17416,\"start\":17341},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19322,\"start\":19196},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19387,\"start\":19322},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20315,\"start\":20242},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20931,\"start\":20871},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21453,\"start\":21406},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25297,\"start\":25251},{\"attributes\":{\"id\":\"formula_13\"},\"end\":25800,\"start\":25714}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1626,\"start\":1614},{\"attributes\":{\"n\":\"2\"},\"end\":6240,\"start\":6228},{\"end\":7477,\"start\":7475},{\"attributes\":{\"n\":\"3\"},\"end\":11346,\"start\":11326},{\"attributes\":{\"n\":\"3.1\"},\"end\":11463,\"start\":11437},{\"attributes\":{\"n\":\"3.2\"},\"end\":13708,\"start\":13692},{\"attributes\":{\"n\":\"3.3\"},\"end\":15823,\"start\":15806},{\"end\":18680,\"start\":18669},{\"attributes\":{\"n\":\"3.4\"},\"end\":18995,\"start\":18975},{\"attributes\":{\"n\":\"4\"},\"end\":19681,\"start\":19664},{\"attributes\":{\"n\":\"4.1\"},\"end\":19817,\"start\":19793},{\"end\":21225,\"start\":21201},{\"attributes\":{\"n\":\"4.2\"},\"end\":24950,\"start\":24926},{\"end\":25862,\"start\":25848},{\"end\":26341,\"start\":26325},{\"attributes\":{\"n\":\"4.3\"},\"end\":27040,\"start\":27006},{\"attributes\":{\"n\":\"5\"},\"end\":27517,\"start\":27510},{\"attributes\":{\"n\":\"5.1\"},\"end\":28070,\"start\":28050},{\"end\":30105,\"start\":30052},{\"attributes\":{\"n\":\"5.2\"},\"end\":30522,\"start\":30499},{\"attributes\":{\"n\":\"5.3\"},\"end\":33036,\"start\":32994},{\"attributes\":{\"n\":\"5.4\"},\"end\":34647,\"start\":34603},{\"end\":35443,\"start\":35436},{\"end\":36300,\"start\":36277},{\"end\":37868,\"start\":37858},{\"attributes\":{\"n\":\"5.5\"},\"end\":41286,\"start\":41260},{\"attributes\":{\"n\":\"6\"},\"end\":43050,\"start\":43040},{\"end\":43602,\"start\":43592},{\"end\":43827,\"start\":43817},{\"end\":43978,\"start\":43968},{\"end\":44198,\"start\":44188},{\"end\":44333,\"start\":44323},{\"end\":44500,\"start\":44490},{\"end\":44866,\"start\":44846},{\"end\":45503,\"start\":45492},{\"end\":46290,\"start\":46279},{\"end\":46557,\"start\":46546}]", "table": null, "figure_caption": "[{\"end\":43815,\"start\":43604},{\"end\":43966,\"start\":43829},{\"end\":44186,\"start\":43980},{\"end\":44321,\"start\":44200},{\"end\":44488,\"start\":44335},{\"end\":44844,\"start\":44502},{\"end\":45490,\"start\":44869},{\"end\":45596,\"start\":45506},{\"end\":46277,\"start\":45599},{\"end\":46544,\"start\":46293},{\"end\":46842,\"start\":46560}]", "figure_ref": "[{\"end\":4675,\"start\":4663},{\"end\":5597,\"start\":5585},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12047,\"start\":12039},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12239,\"start\":12231},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13874,\"start\":13866},{\"end\":14465,\"start\":14457},{\"end\":15573,\"start\":15565},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18227,\"start\":18219},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18457,\"start\":18445},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18721,\"start\":18713},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19497,\"start\":19489},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21196,\"start\":21185},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22316,\"start\":22308},{\"end\":28216,\"start\":28208},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29292,\"start\":29284},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29444,\"start\":29432},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29756,\"start\":29748},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29948,\"start\":29939},{\"end\":30140,\"start\":30132},{\"end\":31495,\"start\":31487},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33329,\"start\":33320},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33521,\"start\":33511},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34384,\"start\":34375},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36402,\"start\":36392},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36968,\"start\":36959},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37163,\"start\":37154},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":38796,\"start\":38787},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":42164,\"start\":42155},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43390,\"start\":43381}]", "bib_author_first_name": "[{\"end\":47609,\"start\":47608},{\"end\":47618,\"start\":47617},{\"end\":47620,\"start\":47619},{\"end\":47631,\"start\":47630},{\"end\":47633,\"start\":47632},{\"end\":47641,\"start\":47640},{\"end\":47816,\"start\":47815},{\"end\":47823,\"start\":47822},{\"end\":47830,\"start\":47829},{\"end\":47844,\"start\":47843},{\"end\":48050,\"start\":48049},{\"end\":48052,\"start\":48051},{\"end\":48066,\"start\":48065},{\"end\":48068,\"start\":48067},{\"end\":48296,\"start\":48295},{\"end\":48304,\"start\":48303},{\"end\":48313,\"start\":48312},{\"end\":48326,\"start\":48325},{\"end\":48555,\"start\":48554},{\"end\":48566,\"start\":48565},{\"end\":48575,\"start\":48574},{\"end\":48786,\"start\":48782},{\"end\":48797,\"start\":48793},{\"end\":48807,\"start\":48803},{\"end\":48819,\"start\":48815},{\"end\":49084,\"start\":49083},{\"end\":49091,\"start\":49090},{\"end\":49099,\"start\":49098},{\"end\":49407,\"start\":49406},{\"end\":49409,\"start\":49408},{\"end\":49421,\"start\":49420},{\"end\":49423,\"start\":49422},{\"end\":49697,\"start\":49693},{\"end\":49708,\"start\":49707},{\"end\":49890,\"start\":49889},{\"end\":49897,\"start\":49896},{\"end\":49899,\"start\":49898},{\"end\":49906,\"start\":49905},{\"end\":49908,\"start\":49907},{\"end\":50121,\"start\":50120},{\"end\":50123,\"start\":50122},{\"end\":50135,\"start\":50134},{\"end\":50361,\"start\":50360},{\"end\":50374,\"start\":50373},{\"end\":50611,\"start\":50610},{\"end\":50624,\"start\":50623},{\"end\":50634,\"start\":50633},{\"end\":50834,\"start\":50833},{\"end\":50847,\"start\":50846},{\"end\":50857,\"start\":50856},{\"end\":50867,\"start\":50866},{\"end\":51050,\"start\":51049},{\"end\":51061,\"start\":51060},{\"end\":51294,\"start\":51293},{\"end\":51306,\"start\":51305},{\"end\":51319,\"start\":51318},{\"end\":51321,\"start\":51320},{\"end\":51601,\"start\":51600},{\"end\":51613,\"start\":51612},{\"end\":51623,\"start\":51622},{\"end\":51631,\"start\":51630},{\"end\":51883,\"start\":51879},{\"end\":51893,\"start\":51889},{\"end\":51906,\"start\":51902},{\"end\":51914,\"start\":51913},{\"end\":52132,\"start\":52128},{\"end\":52144,\"start\":52140},{\"end\":52153,\"start\":52152},{\"end\":52155,\"start\":52154},{\"end\":52374,\"start\":52370},{\"end\":52381,\"start\":52380},{\"end\":52388,\"start\":52387},{\"end\":52405,\"start\":52401},{\"end\":52414,\"start\":52410},{\"end\":52616,\"start\":52615},{\"end\":52623,\"start\":52622},{\"end\":52635,\"start\":52634},{\"end\":52642,\"start\":52641},{\"end\":52869,\"start\":52868},{\"end\":52876,\"start\":52875},{\"end\":52888,\"start\":52887},{\"end\":52896,\"start\":52895},{\"end\":52903,\"start\":52902},{\"end\":53094,\"start\":53093},{\"end\":53101,\"start\":53100},{\"end\":53109,\"start\":53108},{\"end\":53117,\"start\":53116},{\"end\":53123,\"start\":53122},{\"end\":53130,\"start\":53129},{\"end\":53354,\"start\":53353},{\"end\":53356,\"start\":53355},{\"end\":53365,\"start\":53364},{\"end\":53792,\"start\":53791},{\"end\":53806,\"start\":53805},{\"end\":53814,\"start\":53813},{\"end\":53820,\"start\":53819},{\"end\":53831,\"start\":53827},{\"end\":54105,\"start\":54104},{\"end\":54117,\"start\":54116},{\"end\":54532,\"start\":54531},{\"end\":54701,\"start\":54700},{\"end\":54708,\"start\":54707},{\"end\":54710,\"start\":54709},{\"end\":54724,\"start\":54723},{\"end\":54955,\"start\":54954},{\"end\":54967,\"start\":54966},{\"end\":54978,\"start\":54977},{\"end\":55254,\"start\":55250},{\"end\":55262,\"start\":55261},{\"end\":55496,\"start\":55495},{\"end\":55498,\"start\":55497},{\"end\":55507,\"start\":55506},{\"end\":55516,\"start\":55515},{\"end\":55523,\"start\":55522},{\"end\":55698,\"start\":55697},{\"end\":55919,\"start\":55918},{\"end\":55929,\"start\":55928}]", "bib_author_last_name": "[{\"end\":47615,\"start\":47610},{\"end\":47628,\"start\":47621},{\"end\":47638,\"start\":47634},{\"end\":47650,\"start\":47642},{\"end\":47820,\"start\":47817},{\"end\":47827,\"start\":47824},{\"end\":47841,\"start\":47831},{\"end\":47853,\"start\":47845},{\"end\":48063,\"start\":48053},{\"end\":48080,\"start\":48069},{\"end\":48301,\"start\":48297},{\"end\":48310,\"start\":48305},{\"end\":48323,\"start\":48314},{\"end\":48335,\"start\":48327},{\"end\":48563,\"start\":48556},{\"end\":48572,\"start\":48567},{\"end\":48584,\"start\":48576},{\"end\":48791,\"start\":48787},{\"end\":48801,\"start\":48798},{\"end\":48813,\"start\":48808},{\"end\":48823,\"start\":48820},{\"end\":49088,\"start\":49085},{\"end\":49096,\"start\":49092},{\"end\":49103,\"start\":49100},{\"end\":49418,\"start\":49410},{\"end\":49430,\"start\":49424},{\"end\":49705,\"start\":49698},{\"end\":49716,\"start\":49709},{\"end\":49894,\"start\":49891},{\"end\":49903,\"start\":49900},{\"end\":49914,\"start\":49909},{\"end\":50132,\"start\":50124},{\"end\":50139,\"start\":50136},{\"end\":50371,\"start\":50362},{\"end\":50381,\"start\":50375},{\"end\":50621,\"start\":50612},{\"end\":50631,\"start\":50625},{\"end\":50639,\"start\":50635},{\"end\":50844,\"start\":50835},{\"end\":50854,\"start\":50848},{\"end\":50864,\"start\":50858},{\"end\":50872,\"start\":50868},{\"end\":51058,\"start\":51051},{\"end\":51071,\"start\":51062},{\"end\":51303,\"start\":51295},{\"end\":51316,\"start\":51307},{\"end\":51328,\"start\":51322},{\"end\":51610,\"start\":51602},{\"end\":51620,\"start\":51614},{\"end\":51628,\"start\":51624},{\"end\":51637,\"start\":51632},{\"end\":51887,\"start\":51884},{\"end\":51900,\"start\":51894},{\"end\":51911,\"start\":51907},{\"end\":51923,\"start\":51915},{\"end\":52138,\"start\":52133},{\"end\":52150,\"start\":52145},{\"end\":52160,\"start\":52156},{\"end\":52378,\"start\":52375},{\"end\":52385,\"start\":52382},{\"end\":52399,\"start\":52389},{\"end\":52408,\"start\":52406},{\"end\":52421,\"start\":52415},{\"end\":52620,\"start\":52617},{\"end\":52632,\"start\":52624},{\"end\":52639,\"start\":52636},{\"end\":52651,\"start\":52643},{\"end\":52873,\"start\":52870},{\"end\":52885,\"start\":52877},{\"end\":52893,\"start\":52889},{\"end\":52900,\"start\":52897},{\"end\":52912,\"start\":52904},{\"end\":53098,\"start\":53095},{\"end\":53106,\"start\":53102},{\"end\":53114,\"start\":53110},{\"end\":53120,\"start\":53118},{\"end\":53127,\"start\":53124},{\"end\":53134,\"start\":53131},{\"end\":53362,\"start\":53357},{\"end\":53372,\"start\":53366},{\"end\":53803,\"start\":53793},{\"end\":53811,\"start\":53807},{\"end\":53817,\"start\":53815},{\"end\":53825,\"start\":53821},{\"end\":53836,\"start\":53832},{\"end\":54114,\"start\":54106},{\"end\":54127,\"start\":54118},{\"end\":54541,\"start\":54533},{\"end\":54705,\"start\":54702},{\"end\":54721,\"start\":54711},{\"end\":54731,\"start\":54725},{\"end\":54964,\"start\":54956},{\"end\":54975,\"start\":54968},{\"end\":54985,\"start\":54979},{\"end\":55259,\"start\":55255},{\"end\":55271,\"start\":55263},{\"end\":55504,\"start\":55499},{\"end\":55513,\"start\":55508},{\"end\":55520,\"start\":55517},{\"end\":55532,\"start\":55524},{\"end\":55707,\"start\":55699},{\"end\":55926,\"start\":55920},{\"end\":55938,\"start\":55930}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16328480},\"end\":47778,\"start\":47575},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14777911},\"end\":48022,\"start\":47780},{\"attributes\":{\"id\":\"b2\"},\"end\":48224,\"start\":48024},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":76390},\"end\":48495,\"start\":48226},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7854090},\"end\":48722,\"start\":48497},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":17338386},\"end\":49013,\"start\":48724},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":144684},\"end\":49287,\"start\":49015},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":972888},\"end\":49634,\"start\":49289},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9692181},\"end\":49827,\"start\":49636},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11962731},\"end\":50052,\"start\":49829},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2597775},\"end\":50313,\"start\":50054},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":17754191},\"end\":50538,\"start\":50315},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":17707171},\"end\":50789,\"start\":50540},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":14682214},\"end\":51004,\"start\":50791},{\"attributes\":{\"id\":\"b14\"},\"end\":51251,\"start\":51006},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3394943},\"end\":51521,\"start\":51253},{\"attributes\":{\"id\":\"b16\"},\"end\":51822,\"start\":51523},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":266245},\"end\":52073,\"start\":51824},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14194210},\"end\":52333,\"start\":52075},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14367134},\"end\":52560,\"start\":52335},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8264664},\"end\":52836,\"start\":52562},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1305354},\"end\":53050,\"start\":52838},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14783851},\"end\":53271,\"start\":53052},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2121536},\"end\":53734,\"start\":53273},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":17293101},\"end\":54056,\"start\":53736},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6015136},\"end\":54466,\"start\":54058},{\"attributes\":{\"id\":\"b26\"},\"end\":54654,\"start\":54468},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7726895},\"end\":54906,\"start\":54656},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":9420357},\"end\":55175,\"start\":54908},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":61448848},\"end\":55460,\"start\":55177},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":137341},\"end\":55654,\"start\":55462},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":5428587},\"end\":55869,\"start\":55656},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14308539},\"end\":56061,\"start\":55871}]", "bib_title": "[{\"end\":47606,\"start\":47575},{\"end\":47813,\"start\":47780},{\"end\":48293,\"start\":48226},{\"end\":48552,\"start\":48497},{\"end\":48780,\"start\":48724},{\"end\":49081,\"start\":49015},{\"end\":49404,\"start\":49289},{\"end\":49691,\"start\":49636},{\"end\":49887,\"start\":49829},{\"end\":50118,\"start\":50054},{\"end\":50358,\"start\":50315},{\"end\":50608,\"start\":50540},{\"end\":50831,\"start\":50791},{\"end\":51291,\"start\":51253},{\"end\":51877,\"start\":51824},{\"end\":52126,\"start\":52075},{\"end\":52368,\"start\":52335},{\"end\":52613,\"start\":52562},{\"end\":52866,\"start\":52838},{\"end\":53091,\"start\":53052},{\"end\":53351,\"start\":53273},{\"end\":53789,\"start\":53736},{\"end\":54102,\"start\":54058},{\"end\":54698,\"start\":54656},{\"end\":54952,\"start\":54908},{\"end\":55248,\"start\":55177},{\"end\":55493,\"start\":55462},{\"end\":55695,\"start\":55656},{\"end\":55916,\"start\":55871}]", "bib_author": "[{\"end\":47617,\"start\":47608},{\"end\":47630,\"start\":47617},{\"end\":47640,\"start\":47630},{\"end\":47652,\"start\":47640},{\"end\":47822,\"start\":47815},{\"end\":47829,\"start\":47822},{\"end\":47843,\"start\":47829},{\"end\":47855,\"start\":47843},{\"end\":48065,\"start\":48049},{\"end\":48082,\"start\":48065},{\"end\":48303,\"start\":48295},{\"end\":48312,\"start\":48303},{\"end\":48325,\"start\":48312},{\"end\":48337,\"start\":48325},{\"end\":48565,\"start\":48554},{\"end\":48574,\"start\":48565},{\"end\":48586,\"start\":48574},{\"end\":48793,\"start\":48782},{\"end\":48803,\"start\":48793},{\"end\":48815,\"start\":48803},{\"end\":48825,\"start\":48815},{\"end\":49090,\"start\":49083},{\"end\":49098,\"start\":49090},{\"end\":49105,\"start\":49098},{\"end\":49420,\"start\":49406},{\"end\":49432,\"start\":49420},{\"end\":49707,\"start\":49693},{\"end\":49718,\"start\":49707},{\"end\":49896,\"start\":49889},{\"end\":49905,\"start\":49896},{\"end\":49916,\"start\":49905},{\"end\":50134,\"start\":50120},{\"end\":50141,\"start\":50134},{\"end\":50373,\"start\":50360},{\"end\":50383,\"start\":50373},{\"end\":50623,\"start\":50610},{\"end\":50633,\"start\":50623},{\"end\":50641,\"start\":50633},{\"end\":50846,\"start\":50833},{\"end\":50856,\"start\":50846},{\"end\":50866,\"start\":50856},{\"end\":50874,\"start\":50866},{\"end\":51060,\"start\":51049},{\"end\":51073,\"start\":51060},{\"end\":51305,\"start\":51293},{\"end\":51318,\"start\":51305},{\"end\":51330,\"start\":51318},{\"end\":51612,\"start\":51600},{\"end\":51622,\"start\":51612},{\"end\":51630,\"start\":51622},{\"end\":51639,\"start\":51630},{\"end\":51889,\"start\":51879},{\"end\":51902,\"start\":51889},{\"end\":51913,\"start\":51902},{\"end\":51925,\"start\":51913},{\"end\":52140,\"start\":52128},{\"end\":52152,\"start\":52140},{\"end\":52162,\"start\":52152},{\"end\":52380,\"start\":52370},{\"end\":52387,\"start\":52380},{\"end\":52401,\"start\":52387},{\"end\":52410,\"start\":52401},{\"end\":52423,\"start\":52410},{\"end\":52622,\"start\":52615},{\"end\":52634,\"start\":52622},{\"end\":52641,\"start\":52634},{\"end\":52653,\"start\":52641},{\"end\":52875,\"start\":52868},{\"end\":52887,\"start\":52875},{\"end\":52895,\"start\":52887},{\"end\":52902,\"start\":52895},{\"end\":52914,\"start\":52902},{\"end\":53100,\"start\":53093},{\"end\":53108,\"start\":53100},{\"end\":53116,\"start\":53108},{\"end\":53122,\"start\":53116},{\"end\":53129,\"start\":53122},{\"end\":53136,\"start\":53129},{\"end\":53364,\"start\":53353},{\"end\":53374,\"start\":53364},{\"end\":53805,\"start\":53791},{\"end\":53813,\"start\":53805},{\"end\":53819,\"start\":53813},{\"end\":53827,\"start\":53819},{\"end\":53838,\"start\":53827},{\"end\":54116,\"start\":54104},{\"end\":54129,\"start\":54116},{\"end\":54543,\"start\":54531},{\"end\":54707,\"start\":54700},{\"end\":54723,\"start\":54707},{\"end\":54733,\"start\":54723},{\"end\":54966,\"start\":54954},{\"end\":54977,\"start\":54966},{\"end\":54987,\"start\":54977},{\"end\":55261,\"start\":55250},{\"end\":55273,\"start\":55261},{\"end\":55506,\"start\":55495},{\"end\":55515,\"start\":55506},{\"end\":55522,\"start\":55515},{\"end\":55534,\"start\":55522},{\"end\":55709,\"start\":55697},{\"end\":55928,\"start\":55918},{\"end\":55940,\"start\":55928}]", "bib_venue": "[{\"end\":47668,\"start\":47664},{\"end\":48353,\"start\":48349},{\"end\":48602,\"start\":48598},{\"end\":49137,\"start\":49125},{\"end\":49932,\"start\":49928},{\"end\":50183,\"start\":50166},{\"end\":50657,\"start\":50653},{\"end\":50890,\"start\":50886},{\"end\":51941,\"start\":51937},{\"end\":52439,\"start\":52435},{\"end\":52685,\"start\":52673},{\"end\":53152,\"start\":53148},{\"end\":53526,\"start\":53454},{\"end\":54283,\"start\":54212},{\"end\":55550,\"start\":55546},{\"end\":55956,\"start\":55952},{\"end\":47662,\"start\":47652},{\"end\":47881,\"start\":47855},{\"end\":48047,\"start\":48024},{\"end\":48347,\"start\":48337},{\"end\":48596,\"start\":48586},{\"end\":48848,\"start\":48825},{\"end\":49123,\"start\":49105},{\"end\":49443,\"start\":49432},{\"end\":49722,\"start\":49718},{\"end\":49926,\"start\":49916},{\"end\":50164,\"start\":50141},{\"end\":50406,\"start\":50383},{\"end\":50651,\"start\":50641},{\"end\":50884,\"start\":50874},{\"end\":51047,\"start\":51006},{\"end\":51367,\"start\":51330},{\"end\":51598,\"start\":51523},{\"end\":51935,\"start\":51925},{\"end\":52193,\"start\":52162},{\"end\":52433,\"start\":52423},{\"end\":52671,\"start\":52653},{\"end\":52930,\"start\":52914},{\"end\":53146,\"start\":53136},{\"end\":53452,\"start\":53374},{\"end\":53876,\"start\":53838},{\"end\":54210,\"start\":54129},{\"end\":54529,\"start\":54468},{\"end\":54762,\"start\":54733},{\"end\":55024,\"start\":54987},{\"end\":55302,\"start\":55273},{\"end\":55544,\"start\":55534},{\"end\":55747,\"start\":55709},{\"end\":55950,\"start\":55940}]"}}}, "year": 2023, "month": 12, "day": 17}