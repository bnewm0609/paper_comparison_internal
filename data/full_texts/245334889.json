{"id": 245334889, "updated": "2023-10-05 18:36:36.334", "metadata": {"title": "ScanQA: 3D Question Answering for Spatial Scene Understanding", "authors": "[{\"first\":\"Daichi\",\"last\":\"Azuma\",\"middle\":[]},{\"first\":\"Taiki\",\"last\":\"Miyanishi\",\"middle\":[]},{\"first\":\"Shuhei\",\"last\":\"Kurita\",\"middle\":[]},{\"first\":\"Motoaki\",\"last\":\"Kawanabe\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "We propose a new 3D spatial understanding task of 3D Question Answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of the rich RGB-D indoor scan and answer the given textual questions about the 3D scene. Unlike the 2D-question answering of VQA, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail the object identification from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model, where the model learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates the language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine described objects in textual questions and outputs correct answers. We collected human-edited question-answer pairs with free-form answers that are grounded to 3D objects in each 3D scene. Our new ScanQA dataset contains over 40K question-answer pairs from the 800 indoor scenes drawn from the ScanNet dataset. To the best of our knowledge, the proposed 3D-QA task is the first large-scale effort to perform object-grounded question-answering in 3D environments.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2112.10482", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/AzumaMKK22", "doi": "10.1109/cvpr52688.2022.01854"}}, "content": {"source": {"pdf_hash": "8f6c652a392995bd047a2f7b94474ab1e6e23ff0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2112.10482v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2112.10482", "status": "GREEN"}}, "grobid": {"id": "88d2e2dee3f39fdaba31eceb687f86b421eb77a1", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/8f6c652a392995bd047a2f7b94474ab1e6e23ff0.txt", "contents": "\nScanQA: 3D Question Answering for Spatial Scene Understanding\n\n\nDaichi Azuma \nKyoto University\n\n\nTaiki Miyanishi \nKyoto University\n\n\nRiken Atr \nKyoto University\n\n\nShuhei Aip \nKyoto University\n\n\nKurita \nKyoto University\n\n\nRiken Aip \nKyoto University\n\n\nKyoto University\n\n\nJst Presto \nKyoto University\n\n\nMotoaki Kawanabe \nKyoto University\n\n\nScanQA: 3D Question Answering for Spatial Scene Understanding\n06F8A86727302B68A0516BE85A744F13\nWe propose a new 3D spatial understanding task for 3D question answering (3D-QA).In the 3D-QA task, models receive visual information from the entire 3D scene of a rich RGB-D indoor scan and answer given textual questions about the 3D scene.Unlike the 2D-question answering of visual question answering, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail in object localization from the textual questions in 3D-QA.We propose a baseline model for 3D-QA, called the ScanQA 1 , which learns a fused descriptor from 3D object proposals and encoded sentence embeddings.This learned descriptor correlates language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine the described objects in textual questions.We collected human-edited questionanswer pairs with free-form answers grounded in 3D objects in each 3D scene.Our new ScanQA dataset contains over 41k question-answer pairs from 800 indoor scenes obtained from the ScanNet dataset.To the best of our knowledge, ScanQA is the first large-scale effort to perform object-grounded question answering in 3D environments.* denotes equally contributed.1 https://github.com/ATR-DBI/ScanQAQuestion + 3D-ScanWhere is the medium sized blue suitcase laid?A. in front of right bed3D ScanAnswer + 3D-Bounding Box3D ScanWhat is sitting on the floor between the tv and the wooden chair?A. 2 black backpacks Q.2 We used the weights available at https://huggingface.co/ valhalla/t5-base-qa-qg-hl.\n\nIntroduction\n\nIn recent years, significant advances have been achieved in vision-and-language tasks and datasets, and several new datasets have been created to develop models that understand textual expressions, such as captions or questions, which are grounded in two-dimensional (2D) images, such as image captioning [12,42], understanding referring expressions [25,50], image region and phrase correspondence [37], and visual question answering (VQA) [6,20,23].VQA is successful in grasping object features visualized in 2D frames.However, when we develop models that understand the spatial information of 3D scenes, such as \"What is between the table and TV set?\" or \"Where is the Figure 1.We introduce the new task of question answering for 3D modeling.Given inputs of an entire 3D modeling and a linguistic question, models predict an answer phrase and the corresponding 3D-bounding boxes.\n\nsuitcase located?\", the existing models based on 2D images have several challenges in accurately understanding the 3D world.For example, 2D images lack an accurate sense of the relative directions and distances in the 3D scenes, i.e., the stereoscopic attribute-perception problem.Some objects are hidden by other objects when they overlap, i.e., the occlusion problem.When multiple images are used in 2Dimage-based question answering models, such models often encounter difficulties in tracking and recognizing whether some objects are the same object between images, i.e., the object localization and identification problem.\n\nCurrently, 3D spatial-understanding models can be developed for the 3D object localization task of ScanRefer [10], the dialog-based localization of ReferIt3D [1], and the 3D object captioning task of Scan2Cap [13].Embodied question answering [17,46,49] is an important task for navigating agents in a 3D scene.We consider that 3D spatial understanding datasets contribute to developing models that comprehend the embodied 3D scene and ask and answer questions about the 3D environment as humans do.However, unlike their 2D image counterparts, question answering datasets on 3D environmental annotations are still limited in terms of dataset size and question variety because existing datasets often rely on template-based question-answer collections.\n\nIn this paper, we propose a 3D question answering (3D-QA) task that uses 3D spatial information instead of 2D images to comprehend real-world information through the question answering form.In the 3D-QA task, models answer a question for a 3D scene as well as the object localization described in the question.We present the overview of the task in Fig. 1.This 3D-QA task setting is reasonable when external sensors or mobile robots collect sufficient visual information to construct a 3D scene before the QA task.We assume that this is plausible when the model can use the preliminarily captured visual information from the 3D scene because of prior navigation in the scene, such as vision-and-language navigation [5].This task is also applicable to real-world services that use preliminarily extracted 3D scenes, such as interactive virtual room-viewing services or searching in indoor scenes.\n\nFor the 3D-QA task, we developed a novel ScanQA dataset based on RGB-D scans of an indoor scene and annotations derived from the ScanNet dataset [15].We automatically generated questions from the object captions of ScanRefer [10] using question generation models.However, these auto-generated questions included many invalid questions; therefore, we filtered the invalid questions and refined them if necessary.We collected free-form answers and object annotations from humans using a newly developed interactive 3D scene viewer.In total, we gathered 41k question-answer pairs with 32k unique questions.We propose a 3D-QA model with textual and 3D scene encoding and several baseline models, including 2D image models (2D-QA), a combination of 3D object localization models [10,38], and a question answering model [51].We confirmed that the ScanQA model outperformed the baseline models in most evaluations, including exact matching and image captioning metrics in the proposed ScanQA dataset.\n\n\nRelated work\n\nThe 3D-QA task is similar to existing visual question answering and 3D embodied question answering.We place our task as a spatial comprehension of the entire 3D scene given linguistic questions.\n\n\nVisual Question Answering\n\nVisual question answering (VQA) is a task in which models are given a 2D image and a question about its content.They are expected to provide an appropriate answer.Question answering in 2D images was proposed by Malinowski et al. [34], and various inference methods [4,6,51] have since been proposed.One of the best VQA methods is Oscar [30], which uses Mask R-CNN to infer a solution by considering the relationship between individual objects in the image.In addition, Jang et al. [24] proposed a question answering method that considers more detailed vision and motion information using video.ClipBERT [29] improved its accuracy by dividing a video into clips and reasoning from them individually.VQA 360 \u2022 [14] is a task for answering questions about a 360 \u2022 image.Although VQA 360 \u2022 contributes to understanding the 3D scene, the available information is limited compared with the ScanQA dataset.Our dataset also includes the object identification task, which is different from the existing VQA 360 \u2022 dataset.\n\n\n3D Object Localization with Language\n\nScanRefer [10] localizes an object referred to in a freeform text description.The ScanRefer model identifies a 3D-bounding box for an object given the input description.This dataset is based on 800 scenes derived from the Scan-Net dataset [15].In addition to answering questions on 3D scenes, the 3D-QA task also includes an object localization task for objects that appear in the question answering.Unlike in the ScanRefer task, the objects in ScanQA object localization can be multiple because multiple objects can appear in a single question.\n\n\nQuestion Answering in 3D scenes\n\nUnlike 2D-QA, for which many datasets have been proposed, 3D question answering datasets are still limited.We notice that the existing question answering tasks on 3D scenes have interactive forms.The interactive QA dataset (IQUAD) [19] on AI2THOR [27] enables model agents to interact with objects in a scene to determine the answer to a question.Embodied question answering (EQA) [17,46,49] is a combination of visual question answering and navigation such as vision-and-language navigation tasks [5,11,43,47] and models [18,28].In the original EQA dataset [17], the embodied model agent receives a question such as \"What color is the car?\" and navigate to the object described in the question in House 3D [47].MP3D-EQA [46] is a photorealistic embodied QA for Matterport 3D scans [9].MT-EQA [49] is a multitarget variation of EQA.We summarize the relation of these datasets with ScanQA in Table 1.Unlike these datasets, the ScanQA dataset is not created from fixed templates and hence includes more natural and a significantly larger number of unique questions, as discussed in Sec.3.3.\n\n\nScanQA Dataset\n\nWe hereby define the 3D-QA task and describe the collection of the corresponding dataset.\n\n\n3D-QA Task\n\nAs illustrated in Fig. 1, a 3D-QA task requires models to answer a question when given all the information of a 3D scene.Here, models use the 3D spatial information, such as RGB-D scans or point cloud data.We also require models to specify the 3D-bounding boxes of objects that are related to this question answering.This prevents models from answering questions by relying on the textual priors of the trained questions without examining the scene.However, unlike the ScanRefer dataset, we do not require models to target one described object for each question.This is because multiple objects can be used to answer certain questions.For example, the question \"What color is the chairs around the table?\" is related to multiple objects.This question is also answerable as long as the chairs around the unique table in the scene have the same color.In such scenarios, we require models to answer the question addressing multiple 3D-bounding boxes.\n\n\nQuestion-Answer Collection\n\nThe ScanQA dataset was created using multiple phrases, including automatic QA generation [2,8,32,48], question filtering, question editing, and answer collection.First, we automatically generated question-answer pairs from the referring expressions to identify objects in 3D scenes obtained from the ScanRefer dataset [10].We applied the question-and-answer generation model based on a T5-base model [40] trained on a text-based question answering dataset [41] 2 for ScanRefer captions and obtained the seed questions.However, these autogenerated question-answer pairs included many inadequate questions, such as those that are underspecified or not grounded in the scene, as presented in Fig. 2. Auto-generated questions also include easy questions that can be answered with common sense.Therefore, we decided to remove such questions as much as possible.We also did not include auto-generated answers in the final dataset because it was not clear that they were grounded in scenes.Then, we applied filtering and editing to the seed questions with basic rules and human editing in addition to the answer collection via Amazon Mechanical Turk (MTurk).For human editing and answer collection, we developed an interactive visualization website for each 3D scene that enables workers to interact with the 3D scene and check the object names and IDs if they are available.Following the ScanRefer dataset, we attached the object names and IDs for the objects in the 3D scene.We embedded this site into the MTurk task page (Fig. 2).\n\nThe filtering and editing of the seed questions were conducted as follows.First, we filtered the inadequate questions from the auto-generated seed questions using basic rules.Subsequently, we asked the workers to classify the remaining questions into four classes: valid, too easy, unanswerable, and unclear questions.Each question was evaluated by at least three workers.We selected questions in which two or more workers were marked as valid for the next phrase of the editing and answer collection process.In the editing and answer collection, we first presented the filtered questions to workers and requested them to rewrite the questions themselves if they were inadequate for the scene before writing a free-form answers.Multiple answers are collected when necessary.We also collected the object IDs that were used in the question to identify the object in the scene in this phrase.See SM D for details.\n\n\nDataset Statistics\n\nWe collected 41,363 questions and 58,191 answers, including 32,337 unique questions and 16,999 unique answers.Table 2 presents the statistics of the ScanQA dataset.This dataset is an order of magnitude larger than existing embodied question-answering datasets in terms of both question size and variation.For example, the EQA dataset [17] contains 4,246 questions, consisting of 147 unique questions in its training set.The EQA-MP3D dataset [46] contains 767 questions consisting of 174 unique questions in its training set.Considering that our dataset contains not only question-answer pairs but also 3D object localization annotations, we assume that this is the largest dataset to specify the nature of objects in 3D scenes with the question answering form.The distribution of the questions based on their first word is shown in Fig. 3.We collected various types of questions through question auto-generation and editing by humans.\n\nWe followed the training, validation, and test set splits used in ScanRefer.However, as the object IDs for the test set of ScanRefer are not publicly available, we further split the validation set of ScanRefer into two-holds as the validation set and test set with object annotations in the ScanQA dataset.Therefore, the ScanQA dataset includes two test sets with and without object annotations.We collected at least two answers for each question in the validation and two test sets to evaluate the free-form answers.As our dataset includes the question type of \"Where is,\" writing expressions for answers can vary.Therefore, we adopted evaluation metrics for image captioning in addition to an exact match to the annotated answers in the evaluation.\n\n\nScanQA Model\n\nWe introduce the baseline model of ScanQA for the 3D-QA task.The 3D-QA is formalized as follows: given inputs of the point cloud p \u2208 P and question q \u2208 Q about the 3D scene, the 3D-QA model aims to output \u00e2 that semantically matches true answer a * .3D feature representation.We primarily use the input point cloud p consisting of point coordinates c \u2208 R 3 in the 3D space for 3D representation.Following previous 3D and language research [10,13], we use additional point features such as the height of the point, colors, normals, and multiview image features [16] that project 2D appearance features to the point cloud.We use these combined point features as 3D features r \u2208 R 135 .3D & language encoder layers.This layer encodes the question words {w i } nq i=1 using GloVe [36], and we obtain word representation Q \u2208 R nq\u00d7300 , where n q is the number of words in question, and feeds them into a one-layer bidirectional long short-term memory (biLSTM) [22] for word sequence modeling.We project a series of output states from the LSTM using a nonlinear layer with GELUs [21] activation to obtain the contextualized word representation Q \u2032 \u2208 R nq\u00d7d , where d is the hidden size of the biLSTM (set to 256).In addition, this layer detects objects in a scene based on point cloud features r \u2208 R 135 using VoteNet [38], which uses PointNet++ [39] as a backbone network.We obtain the object proposals (object boxes) from VoteNet and project them using a nonlinear layer with GELUs activation to obtain the object proposal representation V \u2208 R nv\u00d7d , where n v is the number of object proposals (set to 256.) 3D & language fusion layer.Inspired by the architecture of deep modular co-attention networks of MCAN [51], often used for VQA, we use transformer blocks [44] to represent the relationships between object proposals and between question words.After feeding contextual question represen-           tation Q \u2032 into a stack of L (set to two) transformer encoder layers, we obtain a deeply contextualized question representation Q enc \u2208 R nq\u00d7d .In addition, we use transformer decoder layers to represent the features of object proposals related to the question words by using the final output of the transformer encoder as the decoder's keys and values.We obtain a question-aware-object proposal representation V dec \u2208 R nv\u00d7d after feeding the question and object proposal pairs into a stack of L transformer decoder layers.Subsequently, the final outputs of the transformer layers Q enc and V dec are fused by a fusion layer that uses twolayer multi-layer perceptron (MLP) with an attention mechanism [33] (for details, see [51]).We obtain the fused feature f \u2208 R d , which simultaneously represents a 3D scene and linguistic question information.\nd Q s V V / J i i w 3 m c r / u i Z r k h 2 B 0 5 a k = \" > A A A C e H i c h V H L S s N A F D 2 N r 1 p f 1 W 4 E N 8 X i a 1 M m I i q C U H T j s r Z W B a s h i V M d T J O Y p J U a + g P + g A s X o i A q f o Y b f 8 C F n y A u F Q R x 4 W 0 a E B X 1 D j N z 5 s w 9 d 8 7 M a L Y h X I + x h 4 j U 0 t r W 3 h H t j H V 1 9 / T 2 x f s H V l 2 r 4 u i 8 o F u G 5 a x r q s s N Y f K C J z y D r 9 s O V 8 u a w d e 0 v c X G / l q V O 6 6 w z B W v Z v P N s r p j i p L Q V Y 8 o J Z 4 o + g e K K N a 3 f F P Z r y u + m J f r S j z F 0 i y I 5 E 8 g h y C F M L J W / B J F b M O C j g r K 4 D D h E T a g w q W 2 A R k M N n G b 8 I l z C I l g n 6 O O G G k r l M U p Q y V 2 j 8 Y d W m 2 E r E n r R k 0 3 U O t 0 i k H d I W U S I + y e X b N n d s d u 2 C N 7 / 7 W W H 9 R o e K n R r D W 1 3 F b 6 j g b z r / + q y j R 7 2 P 1 U / e n Z Q w m z g V d B 3 u 2 A a d x C b + q r h 8 f P + b n c i D / K z t k T + T 9 j D + y W b m B W X / S L Z Z 4 7 Q Y w + Q P 7 + 3 D / B 6 m R a n k 7 L y 1 O p z E L 4 F V E M Y R j j 9 N 4 z y G A J W R T o 3 B p O c Y X r y J u U l M a k i W a q F A k 1 C X w J a f I D 2 8 O S H A = = < / l a t e x i t > {w i } nq i=1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" a + e 6 5 N V q A O R H v N a f P K O i L 3 q i s E M = \" > A A A C a 3 i c h V G 7 T h t B F D 3 e 8 I p 5 m d B E g W K F Z U S 1 u g s I C B W C h i I F L 4 M l b F m 7 y x h G 7 E u 7 Y 0 v E 4 g d S p q E g T Z B Q F P E Z N P w A B Z + A o l Q g 0 V B w v V 6 E K A x 3 N D N 3 z t x z 5 8 y 9 d u j K W B H d Z L Q P X d 0 9 v X 0 f s / 0 D g 0 P D u Z F P 2 3 F Q j x x R d A I 3 i E q 2 F Q t X + q K o p H J F K Y y E 5 d m u 2 L E P V 1 r 3 O w 0 R x T L w t 9 R R K C q e t e / L m n Q s x V C p r K Q n Y v 1 b N Z c n Y + Y r 2 4 J O x i w R m Z Q 6 N K O b B i W W R 2 p r Q e 4 P y t h D A A d 1 e B D w o d h 3 Y S H m s Q s T h J C x C p q M R e z J 5 F 7 g G F n m 1 j l K c I T F 6 C G v + 3 z a T V G f z 6 2 c c c J 2 + B W X Z 8 R M H Q W 6 p r 9 0 R 1 d 0 Q b f 0 2 D F X M 8 n R 0 n L E u 9 3 m i r A 6 / O P z 5 s O 7 L I 9 3 h Y M X 1 p u a F W p Y S L R K 1 h 4 m S O s X T p v f + H 5 y t 7 m 4 U W h O 0 h n 9 Y / 2 / 6 Y Y u + Q d + 4 9 4 5 X x c b p 8 h y A 5 6 r r H d 2 t q c N c 8 4 w 1 2 f z S 8 t p K / o w h g l M c b 3 n s Y R V r K G Y 1 P k n T v E r 8 1 8 b 1 b 5 o 4 + 1 Q L Z N y R v H K t M I T Y Q e M 8 Q = = < / l a t e x i t > \u21e5L < l a t e x i t s h a 1 _ b a s e 6 4 = \" a + e 6 5 N V q A O R H v N a f P K O i L 3 q i s E M = \" > A A A C a 3 i c h V G 7 T h t B F D 3 e 8 I p 5 m d B E g W K F Z U S 1 u g s I C B W C h i I F L 4 M l b F m 7 y x h G 7 E u 7 Y 0 v E 4 g d S p q E g T Z B Q F P E Z N P w A B Z + A o l Q g 0 V B w v V 6 E K A x 3 N D N 3 z t x z 5 8 y 9 d u j K W B H d Z L Q P X d 0 9 v X 0 f s / 0 D g 0 P D u Z F P 2 3 F Q j x x R d A I 3 i E q 2 F Q t X + q K o p H J F K Y y E 5 d m u 2 L E P V 1 r 3 O w 0 R x T L w t 9 R R K C q e t e / L m n Q s x V C p r K Q n Y v 1 b N Z c n Y + Y r 2 4 J O x i w R m Z Q 6 N K O b B i W W R 2 p r Q e 4 P y t h D A A d 1 e B D w o d h 3 Y S H m s Q s T h J C x C p q M R e z J 5 F 7 g G F n m 1 j l K c I T F 6 C G v + 3 z a T V G f z 6 2 c c c J 2 + B W X Z 8 R M H Q W 6 p r 9 0 R 1 d 0 Q b f 0 2 D F X M 8 n R 0 n L E u 9 3 m i r A 6 / O P z 5 s O 7 L I 9 3 h Y M X 1 p u a F W p Y S L R K 1 h 4 m S O s X T p v f + H 5 y t 7 m 4 U W h O 0 h n 9 Y / 2 / 6 Y Y u + Q d + 4 9 4 5 X x c b p 8 h y A 5 6 r r H d 2 t q c N c 8 4 w 1 2 f z S 8 t p K / o w h g l M c b 3 n s Y R V r K G Y 1 P k n T v E r 8 1 8 b 1 b 5 o 4 + 1 Q L Z N y R v H K t M I T Y Q e M 8 Q = = < / l a tM Y + L B s 4 N q Q w = \" > A A A C p H i c j V H L S s N A F L 2 N r 1 o f r b o R 3 B R L i w s p E y k q r o p u X L j o w 7 S F 2 p Y k j j U 2 L 5 J p o Y b + g D t X g l 0 p u B A / Q x f + g I t + g r i s 4 M a F N 2 l A t P i 4 I T N n z t x z 5 1 y u Z K q K z Q j p B b i R 0 b H x i e B k a G p 6 Z j Y c m Z s v 2 E b T k q k g G 6 p h l S T R p q q i U 4 E p T K U l 0 6 K i J q m 0 K D V 2 3 P t i i 1 q 2 Y u j 7 r G 3 S i i b W d e V I k U W G V N G u O o Z 0 0 q l F Y i R J v I g O A 9 4 H M f A j Y 0 Q e 4 A A O w Q A Z m q A B B R 0 Y Y h V E s P E r A w 8 E T O Q q 4 C B n I V K 8 e w o d C K G 2 i V k U M 0 R k G 7 j W 8 V T 2 W4 i + o V J J W P m O c a 8 C x / M = \" > A A A C p H i c j V H L S s N A F L 2 N r 1 o f r b o R 3 B R L i w s p E y k q r o p u X L j o w 7 S F 2 p Y k T m t o X i T T Q g 3 9 A X e u B L t S c C F + h i 7 8 A R f 9 B H F Z w Y 0 L b 9 K A a P F x Q 2 b O n L n n z r l c y V Q V m x H S D 3 B j 4 x O T U 8 H p 0 M z s 3 H w 4 s r B Y s I 2 W J V N B N l T D K k m i T V V F p w J T m E p L p k V F T V J p U W r u u f f F N r V s x d A P W c e k F U 1 s 6 E p d k U W G V N G u O q J u d 2 u R G E k S L 6 K j g P d B D P z I G J E H O I J j M E C G F m h A Q Q e G W A U R b P z K w A M B E 7 k K O M h Z i B T v n k I X Q q h t Y R b F D B H Z J q 4 N P J V 9 VQ K S m i K 9 Q J n w a k d 1 + l x m L L 5 v O A = \" > A A A C p H i c j V H L S s N A F L 2 N r 1 o f r b o R 3 B R L i w s p E y k q r o p u X L j o w 7 S F 2 p Y k T m t o X i T T Q g 3 9 A X e u B L t S c C F + h i 7 8 A R f 9 B H F Z w Y 0 L b 9 K A a P F x Q 2 b O n L n n z r l c y V Q V m x H S D 3 B j 4 x O T U 8 H p 0 M z s 3 H w 4 s r B Y s I 2 W J V N B N l T D K k m i T V V F p w J T m E p L p k V F T V J p U W r u u f f F N r V s x d A P W c e k F U 1 s 6 E p d k U W G V N G u O q o h d 2 u R G E k S L 6 K j g P d B D P z I G J E H O I J j M E C G F m h A Q Q e G W A U R b P z K w A M B E 7 k K O M h Z i B T v n k I X Q q h t Y R b F D B H Z J q 4 N P J V 9 VP Q w Z A A Z Y 8 k = \" > A A A C n n i c j V H L S s N A F D 2 N r 1 o f r b o R 3 B R L x Y W U q R Q V V 0 V B 3 I h 9 2 A f U U p I 4 r a F p E p K 0 U I s / o F v F h S s F F + J n 6 M I f c N F P E J c V 3 L j w N g 2 I F h 8 3 Z O b M m X v u n M u V D F W x b M b a H m F g c G h 4 x D v q G x u f m P Q H p q a z l l 4 3 Z Z 6 R d V U 3 8 5 J o c V X R e M Z W b J X n D Z O L N U n l O a m 6 2 b 3 P N b h p K b q 2 Z z c N X q y J F U 0 p K 7 J o E 5 U s l w I h F m F O B P t B 1 A U h u J H Q A w / Y x w F 0 y K i j B g 4 N N m E V I i z 6 C o i C w S C u i B Z x J iV N B 1 h X d L E m i R R W m U c F m t k J L h k l F V V J o U W r u u f f F N j U t p m u H d s e g F V V s a K z O Z N F G q p i t O l S T u 7 V I j C S J F 9 F R w P s g B n 5 k 9 M g D H M E x 6 C B D C 1 S g o I G N W A E R L P z K w A M B A 7 k K O M i Z i J h 3 T 6 E L I d S 2 M I t i h o h s E 9 c G n s o + q + H Z r W l 5 a h l f U f A 3 U R m F O H k i t 2 RV N B 1 h X d L E m i R R W m U c F m t k J L h k l F V V J o U W r u u f f F N j U t p m u H d s e g F V V s a O y Y y a K N V L F Q d e p U 7 t Y i M Z I k X k R H A e + D G P i R 0 S M P c A R 1 0 E G G F q h A Q Q M b s Q I i W P i V g Q c C B n I V c J A z E T H v n k I X Q q h t Y R b F D B H Z J q 4 N P J V 9 V s O z W 9 P y 1 D K + o u B v o j I K c f J E b s m A P J I 7 8 k z e f 6 z l e D V c L x 3 c p a G W G r X w 2 X L + 7 U + V i r s N J 5 + q X z 3 b c A z b n l e G 3 g 2 P c b u Q h / r 2 6 c U g v 5 O L O w l y T V 7 Q / x X p k 3 v s Q G u / y j d Z m u v 9 2 8 / 6 S F c h H B 3 / f V C j o L C R 5 D e T q W w q l t 7 1 h x i E F V i F N Z z U F q R h H z I g e C + f w y X 0 u A R 3 w O U 5 Y Z j K B X z N E n w J r v o B L s K Y Z w = = < / l a t e x i t > V dec < l a t e x i t s h a 1 _ b a s e 6 4 = \" u / R V H q 9 G N 6 u V 3 6 X R k m P c E 3 0 4 D n 8 = \" > A A A C n 3 i c j V H L L g R B F D 3 a e z x m s J H Y Y D J Y y K R a B L E S F q x k Z h i P D J H u V k N F v 9 J d M w k T P y C 2 W F i R W I j P Y O E H L H y C W J L Y W LV s U h a F J o n L Z 4 a 1 E k q V Z G P 2 1 Q I 1 A E l F k n M Q d N r A N B w Z K s M B h Q x I 2 o c G n r w A V D C 5 x m 6 g Q 5 x E S 4 T 3 H I W K k L V E W p w y N 2 D 1 a d + h U i F i b z k F N P 1 Q b 9 I p J v 0 f K f q T Y A 7 t m L + y e 3 b A n 9 v 5 j r U p Y I / C y T 7 t e 1 X J 3 K 3 7 U u / T 2 p 8 q i X W L 3 U / W r Z 4 k i p k K v g r y 7 I R N 0 Y V T 1 5 Y O z l 6 X p X K o y x C 7 Z M / m / Y I / s l j q w y 6 / G V Z b n z v / t Z 7 S m q x i N T v 0 + q F q w M p Z W J 9 L j 2 f H k z G w 0 x B b 0 Y R A j N K l J z G A B G e T p 5 S K O c Y J T Z U C Z V x a V T D V V q Y s 0 P f g S y v o H I L q V 1 Q = = < / l a t e x i t > Q 0 < l a t e x i t s h a 1 _ b a s e 6 4 = \" E H n G B + J C Z q I E u t b o w M / 0 a Q j R N h c = \" > A A A C n n i c j V H L S s N A F D 2 N r 1 o f r b o R 3 B R L x Y W U q R Q V V 0 V B 3 I h 9 2 A f U U p I 4 r a F p E p K 0 U I s / o F v F h S s F F + J n 6 M I f c N F P E J c V 3 L j w N g 2 I F h 8 3 Z O b M m X v u n M u V D F W x b M b a H m F g c G h 4 x D v q G x u f m P Q H p q a z l l 4 3 Z Z 6 R d V U 3 8 5 J o c V X R e M Z W b J X n D Z O L N U n l O a m 6 2 b 3 P N b h p K b q 2 Z z c N X q y J F U 0 p K 7 J o E 5 X M l g I h F m F O B P t B 1 A U h u J H Q A w / Y x w F 0 y K i j B g 4 N N m E V I i z 6 C o i C w S C u i B Z x J iK n H 8 L h W C c h W s I = \" > A A A C n n i c j V H L S s N A F D 2 N r 1 o f r b o R 3 B R L x Y W U q R Q V V 0 V B 3 I h 9 2 A f U U p I 4 r a F p E p K 0 U I s / o F v F h S s F F + J n 6 M I f c N F P E J c V 3 L j w N g 2 I F h 8 3 Z O b M m X v u n M u V D F W x b M b a H m F g c G h 4 x D v q G x u f m P Q H p q a z l l 4 3 Z Z 6 R d V U 3 8 5 J o c V X R e M Z W b J X n D Z O L N U n l O a m 6 2 b 3 P N b h p K b q 2 Z z c N X q y J F U 0 p K 7 J o E 5 V M l g I h F m F O B P t B 1 A U h u J H Q A w / Y x w F 0 y K i j B g 4 N N m E V I i z 6 C o i C w S C u i B Z x J iW V D M 7 / l k G W c = \" > A A A C n n i c j V H L S s N A F D 2 N r 1 o f r b o R 3 B R L x Y W U q R Q V V 0 V B 3 I h 9 2 A f U U p I 4 r a F p E p K 0 U I s / o F v F h S s F F + J n 6 M I f c N F P E J c V 3 L j w N g 2 I F h 8 3 Z O b M m X v u n M u V D F W x b M b a H m F g c G h 4 x D v q G x u f m P Q H p q a z l l 4 3 Z Z 6 R d V U 3 8 5 J o c V X R e M Z W b J X n D Z O L N U n l O a m 6 2 b 3 P N b h p K b q 2 Z z c N X q y J F U 0 p K 7 J o E 5 U 0 S 4 E Q i z A n g v 0 g 6 o I Q 3 E j o g Q f s 4 w A 6 Z N R R A 4 c G m 7 A K E R Z 9 B U T B Y B B X R I s 4 k 5 D i 3 H M c w 0 f a O m V x y h C J r d J a o V P B Z T U 6 d 2 t a j l q m V 1 T 6 T V I G E W Z P 7 J Z 1 2 C O 7 Y 8 / s / c d a L a d G 1 0 u T d q m n 5 U b J f z K b f v t T V\nObject localization & QA layer.This layer consists of object localization, object classification, and answer classification modules.Each module is described as follows.\n\nObject localization module aims to predict which of the proposed object boxes corresponds to the question.The question-aware-object proposal representation, V dec \u2208 R nv\u00d7d , is fed into a two-layer MLP to determine the likelihood of each object box being related to the question.Following [10], we compute the localization confidence s loc \u2208 R nv for the proposed n v object boxes with the crossentropy (CE) loss to train this module.\n\nObject classification module predicts what objects are associated with a question.Note that many questions do not contain target object names related to the answer, in contrast to a 3D localization task [10,52,53].We use the 3D and question-aware fused feature f and feed it into a twolayer MLP to predict 18 ScanNet benchmark classes.We compute the object classification scores s obj \u2208 R 18 with a softmax function and use the CE loss to train this module.\n\nAnswer classification module predicts an answer corresponding the question and scene.We project the fused feature f into a vector s ans \u2208 R na for the n a answer candidates in the training set.To consider multiple answers, we compute final scores with the binary cross-entropy (BCE) loss function to train the module.Loss function.We use a loss function similar to Scan-Refer [10], such as the localization loss L loc of the object localization module, object detection loss L det of VoteNet [38], and object classification loss L obj of the object classification module.To answer the 3D scene content, we additionally use the answer loss L ans of the answer classification module.We set the final loss as a simple linear combination of these losses, computed as L = L ans + L obj + L loc + L det .\n\n\nExperiments\n\n\nExperimental Setup\n\nIn this experimental setup, we referred to the experimental setup of existing studies on scene understanding of ScanNet [15] through languages such as ScanRefer and Scan2Cap [10,13].Data augmentation.We applied data augmentation to our training data and applied rotation about all three axes using a random angle in [\u22125 \u2022 , 5 \u2022 ] and randomly translated the point cloud within 0.5 m in all directions.Because the ground alignment in ScanNet was incomplete, we rotated it on all axes (not just the top).Training.To train the ScanQA model, we used Adam [26], a batch size of 16, and an initial learning rate of 5e\u22124.We trained the model for 30 epochs until it converged and decreased the learning rate by 0.2 times after 15 epochs.To mitigate the fitting of the model against its training data, we set the weight decay factor to 1e\u22125.Evaluation.To evaluate the QA performance, we used exact matches EM@1 and EM@10 as the evaluation metric, where EM@K is the percentage of predictions in which the top K predicted answers exactly match any one of the    ground-truth answers.We also included sentence evaluation metrics frequently used for image captioning models because some of the questions had multiple possible answer expressions, as discussed in Sec.3.3.We added the BLEU [35], ROUGE-L [31], METEOR [7], CIDEr [45],\n\nand SPICE [3] metrics to evaluate robust answer matching.Baselines.To validate our 3D-QA model (ScanQA), we prepared several baselines.Empirical experiments were conducted using the following methods.RandomImage+2D-QA First, we prepared several 2D-QA models as baselines to demonstrate how our 3D-QA models outperformed 2D-based VQA models for the 3D-QA task.\n\nWe used a pretrained MCAN model [51] as the 2D-QA model.MCAN is a transformer network [44] that uses a cross-attention mechanism to represent the relationship between question words and objects in an image.The proposed method uses some of the modules used in MCAN, such as transformer encoder and decoder layers, to create 3D and language features.By comparing these two, we can confirm the importance of creating a model specialized for 3D-QA.Because 2D-QA models cannot be directly applied to a 3D environment, we randomly sampled three images from the video captured to build the ScanNet dataset.We used a bottom-up top-down attention model [4] to extract the appearance features of the objects.We applied pretrained 2D-QA models to these images and computed the answer scores for each image.Finally, we selected the most probable answer according to the averaged answer scores of these images.We experimented with 2D-QA using three images captured in the environment per question.\n\nOracleImage+2D-QA To investigate the upper bound on the performance of 2D-QA for questions in 3D space, we used images around a target object associated with a question-answer pair.We set the camera's position based on the coordinates of the bounding box of the correct object and captured images from the direction and distance at which the bounding box was most visible.Because the object may not be visible depending on the camera's position, we used three images per question.We applied 2D-QA models to these images similar to RandomImage+2D-QA.Note that it is difficult to obtain such images in actual QA scenarios.By examining the performance of this method, we can determine the difficulty of solving the 3D-QA task using 2D-QA models.\n\nVoteNet+MCAN VoteNet [38] is a 3D object detection method that locates and recognizes objects in a 3D scene.\n\nThis method detects objects in a 3D space, extracts their features, and uses them in a standard VQA model (MCAN).Unlike our method, this method does not consider the target object or its location in the 3D space.ScanRefer+MCAN (pipeline) ScanRefer [10] is a 3D object localization method for localizing a given linguistic description to a corresponding target object in a 3D space.ScanRefer internally uses VoteNet to detect objects in a room and estimates the object corresponding to the linguistic description from among the candidate objects.Note that ScanRefer cannot be used directly for QA.Thus, we used a pretrained ScanRefer model to identify the object corresponding to the question and then applied 2D-QA (MCAN) to the image surrounding the object localized by ScanRefer.Note that ScanRefer and MCAN were run separately, and end-to-end learning was not possible.ScanRefer+MCAN (end-to-end) This method is more sophisticated and closer to the proposed method.Although ScanRefer+MCAN (pipeline) conducts object localization and QA separately, this method simultaneously learns localization and QA modules.Specifically, the input to the method is the object proposal feature of ScanRefer for VQA models; subsequently, the model predicts answers based on object box features and question content.Unlike the ScanQA model, this uses the output of VoteNet separately for object localization and QA modules (although the information for both tasks is mutually useful) and does not learn both modules in common.\n\n\nQuantitative Analysis\n\nThe performance of 3D-QA on the ScanQA dataset and image caption metrics are presented in Table 3.The best results in each column are shown in bold.We compared our ScanQA model with competitive baselines VoteNet+MCAN, ScanRefer+MCAN (pipeline), and Scan-Refer+MCAN (end-to-end).These baselines share some of the components of the proposed method and aided us in understanding useful components for 3D-QA.The results indicated that our ScanQA method significantly outperformed all baselines across all data splits over all evaluation metrics.In particular, ScanQA significantly outperformed ScanRe-fer+MCAN (end-to-end), which learns QA and object localization separately across all data splits over all evaluation metrics, indicating that the ScanQA model succeeded in synergistic learning by sharing object localization and QA modules.ScanQA outperformed VoteNet+MCAN by a large margin.This result suggested that using object localization in a 3D space and predicting object categories related to questions are important for 3D-QA.We will clarify this point in the section on the ablation study.Interestingly, VoteNet+MCAN, ScanRefer+MCAN (end-to-end), and ScanQA significantly outperformed ScanRefer+MCAN (pipeline), which detects target objects related to a question using a pretrained ScanRefer and then applies 2D-QA to the surrounding images of a target object.The results indicated that end-to-end training with 3D and language information is suitable for solving 3D-QA model problems.In addition, we observed that our 3D-QA model, ScanQA, is superior to a 2D-QA model, RandomImage+MCAN, which uses an effective pretrained model.We also observed that the 2D-QA baseline with oracle object identification of Or-acleImage+MCAN performed better or more competitively than the ScanQA model.Although this suggests that accurate object identification for questions indeed boosts 3D-QA results, this is an oracle setting for real-world applications.We finally evaluated the human performance on the sampled questions in the test set with objects using MTurk.The exact matching score (EM@1) is 51.6 for the bestperforming MTurk worker.\n\n\nAblation Studies\n\nWe conducted ablation studies concerning the design of the ScanQA model.Table 4 lists the effects of each major component of the proposed method.The effect of different input data is shown in Table 5.\n\n\nDoes object classification help?\n\nWe demonstrated the effectiveness of object classification by conducting an experiment using ScanQA combined with and without the object classification module.We compared our method trained with the answer and object classification modules (ANS+OBJ) with a model trained with only the answer module (ANS), and we also compared a model trained with full modules (ANS+OBJ+LOC) with one trained with the answer and object localization modules (ANS+LOC).The results in Table 4 show that the models with the OBJ outperformed the other models.This suggests that predicting the category of a target object is effective for 3D-QA.Does object localization help?We demonstrated the effectiveness of object localization by conducting an experiment with ScanQA and without an object localization module.We compared our method trained with the answer and object localization modules (ANS+LOC) with a model trained with only the answer module (ANS), and we also compared a model trained with full modules (ANS+OBJ+LOC) with one trained with the answer and object classification modules (ANS+OBJ).The results in Table 4 show that the model with LOC consistently outperformed the other models.This suggests that localizing target objects is also important for improving 3D-QA performance.Do colors help for 3D-QA?According to the ScanRefer study, models that use color information have better object localization performance than models that use only geometry [10].Motivated by this finding, we evaluated several models using different features.We compared our method trained with geometry (xyz) and multiview image features (xyz+multiview) with a model trained with only geome-\n\n\ntry (xyz) and one trained with RGB values (xyz+rgb).\n\nTo validate the object localization performance, we used accuracy (Acc@0.25), in which the positive predictions have a higher intersection over union with the ground truths than the threshold of 0.25 used in [10].As Table 5 shows, RGB values were not effective for both object localization and QA.The multiview image features were slightly effective for object localization but not on QA.This is because it is more difficult for the ScanQA dataset to associate language and object information than the ScanRefer dataset because multiple objects may apply to a single question.Fortunately, ScanQA trained with geometry, preprocessed multiview image features, and normals which is demonstrated in Table 3 outperformed the other models, but the effect was limited.This result suggested that subsequent studies should consider a more balanced selection of features in terms of computational cost and performance.\n\n\nQualitative Analysis\n\nFinally, we demonstrated the excellent performance of our model by visualizing qualitative examples of ScanQA, ScanRefer+MCAN (pipeline), and ground truth.Fig. 5 shows the representative QA results of a baseline method and ScanQA.The results suggested that answering questions requires object localization related to the answer according to the question content and point cloud matching.For example, the leftmost case shows that a whiteboard located above a backpack could not be answered by Scan-Refer+MCAN (pipeline), which localized the backpack in error.This is because the QA and localization modules were separated in ScanRefer+MCAN (pipeline).In contrast, our model successfully localized target objects related to answers and predicted correct answers by simultaneously learning QA and object localization.\n\n\nConclusion\n\nSpatial understanding using language expression is a core technology for models deployed in the real world and interacting with humans.We introduce a novel task of 3D question answering (3D-QA), in which models observe an entire 3D scan and answer a question about the 3D scene in addition to the object localization.Based on the ScanRefer dataset, we created a new ScanQA dataset which consists of 41,363 questions and 32,337 unique answers from 800 scenes derived from the ScanNet scenes.We propose a 3D-QA baseline model of ScanQA.We confirm that the ScanQA baseline performs better than the counterpart 2Dbased VQA baselines in most of the evaluation measures, including the exact match and image captioning metrics.\n\n\n. Object Localization Results\n\nBefore introducing the additional QA results, we demonstrated the object localization performance using Scan-Refer+MCAN (pipeline), ScanRefer+MCAN (end-to-end), and ScanQA.We used the Acc@K, where the positive predictions had a higher IoU with the ground truths than the threshold K (set to 0.25 and 0.5) [10].As shown in Table 5, we refer to each accuracy as Acc@0.25 and Acc@0.5 in our Model Acc@0.25 Acc@0.experiments.The results in Table 6 show that the shared and end-to-end learning of QA, localization, and object classification modules effectively predicted the target object for a given question.\n\n\nB.2. Question Answering Results\n\nTable 8 shows the performance of additional baselines and proposed models on the ScanQA dataset.The best results in each column are indicated in bold.The results show that our ScanQA (single or multiple) models outperformed the baselines RandomImage + MCAN (mesh), RandomImage + Oscar (mesh), TopDownImage + MCAN, and TopDownImage + Oscar across all evaluation metrics on all splits.While RandomImage + Oscar (real) outperformed ours on EM@1 on the test without objects split owing to its effective pretrained model, the ScanQA models outperformed RandomImage + Oscar (real) on other evaluation metrics.Regarding the difference using real and mesh images, the performance tended to be better when using real images.For example, RandomImage + MCAN (real) outperformed RandomImage + MCAN (mesh) and RandomImage + Oscar (real) outperformed RandomImage + Oscar (mesh) in almost all evaluation measures on all splits.We observed no consistency in the advantage regarding the performance difference between ScanQA (single) and ScanQA (multiple).\n\n\nB.3. Ablation Studies on ScanQA (multiple)\n\nIn this section, we describe ablation studies conducted on the ScanQA (multiple) model that predicted confidences and labels for multiple objects when performing QA.The effect of each major component of the proposed method is shown in Table 9, and the effect of different input data is shown in Table 7.The results in Table 9 suggest that using the object localization module (LOC) or the object classification module (OBJ) was effective for improving QA performance.Table 7 shows the object localization performance Top10-Acc@0.25 and QA performance EM@10 on ScanQA (multiple) using different input features, where Top10-Acc@0.25 is the accuracy at which the positive predictions had higher IoU with the ground truth than 0.25 (we compare the top 10 object boxes with the highest object lo-\n\n\nB.4. Performance by Parameter\n\nWe also evaluated the performance of ScanQA with different parameters, the number of layers L, and the hidden size d in Tables 10 and 11, respectively.The results suggest that the number of layers L = 1 and hidden size d = 128 were suitable for the test splits.\n\n\nB.5. Accuracy for Question Types\n\nWe classify the questions into six types: object, color, object nature, place, number and other by the beginning of the question sentences following Table 12.We evaluate the detailed question answering performance for each class for 2D-QA baselines and the ScanQA model.Table 13, Table 14 and Table 15 presents the detailed accuracy on valid, test w/ object and test w/o object respectively.\n\nWe notice that the performance scores for place questions is lower than other questions.We assume there are two reasons for this.First, there are several ways to answer the questions of place.Model predicts longer answers and therefor image captioning-based metrics are suitable rather than exact matching for place questions.For color questions, possible answers are limited and simple 2D-QA model can answer the questions from the sights of objects from several images.However, such answers are not grounded to objects in the questions.\n\n\nC. Additional Qualitative Analysis\n\nWe conducted a qualitative evaluation by comparing our ScanQA model to the ScanRefer + MCAN (end-to-end) in addition to the evaluation on ScanRefer + MCAN (pipeline) (Fig. 5).Fig. 9 shows the results of the object localization and QA correctness with the visualization of the scene and bounding boxes using ScanRefer + MCAN (end-to-end), ScanQA, and ground truth.The results also indicated that QA correctness and localization were closely related.The leftmost case shows that models were to answer the color of the bathrobe.However, the baseline model, ScanRefer + MCAN (end-to-end), localized a different object \"table\" and answered its color.The baseline model used the word \"wall\" to determine an object near the wall and localized it.However, our model could localize the correct object, \"bathrobe,\" on the wall.In the second case from the left, the question was about the object placed on the chair, but the baseline model provided the wrong answer \"table\" because the word \"seat\" is frequently associated with the table.In contrast, our model understood the meaning \"in the seat\" and correctly selected a backpack in the seat.In the second case from the right, the baseline model incorrectly localized a TV close to the wall and answered \"chair\" close to the TV.Our model correctly recognized the meaning \"corner,\" localized it, and indicated the correct object \"trash can\" at the corner.In the final case, there were multiple ottomans in the scene, and the models were to correctly understand the positional relationship between the ottoman, chair, and pillow.The baseline model localized the lamp and incorrectly answered its color \"white.\"In contrast, our model correctly understood the positional relationship between the ottoman, chair, and pillow, localizing the ottoman in front of the chair with a pillow.\n\n\nD. MTurk Annotation Details\n\nWe developed a visualizer website for 3D modeling.MTurk workers can interactively rotate and zoom in 3D modeling.Fig. 10 presents the snapshot of the MTurk website for the editing and answer collection phrase of the QA annotation.\n\nWe filtered the auto-generated questions as follows.We first applied rule-based filtering for removing potential underspecified or noisy expressions such as \"this\" and \"image\" and direction words, namely \"north,\" \"west,\" \"south,\" and \"east.\"We also filtered odd question types, such as \"What is the name of...\" Furthermore, we filtered meaningless questions in 3D scenes using MTurk.We asked three workers to evaluate each question and filtered those questions that were evaluated as \"valid\" in the scene by at least two of the three workers.Finally, we asked workers to rewrite those questions that, according to them, were underspecified before they filled out the answers.\n\nWe consider that our dataset covers a broad range of\n\nFigure 2 .\n2\nFigure 2. Underspecified and valid questions for an office room scene.We presented scenes with object IDs and names to MTurk workers for the dataset collection.\n\n\nFigure 3 .\n3\nFigure 3.The distribution of the question types by the beginning of the question writing.\n\n\n\n\nt e x i t s h a 1 _ b a s e 6 4 = \"\n\n\n\n\nR 3 P b k 3 b U 8 v 4 i o q / h c o o x M k T u S V 9 8 k j u y D N 5 / 7 G W 4 9 V w v b R x l w Z a a t b C Z 4 v 5 t z 9 V G u 4 M j j 9 V v 3 p m c A S b n l c F v Z s e 4 3 Y h D / S t 0 4 t + f i s X d x L k m r y g / y v S I / f Y g d 5 6 l W + y N N f 9 t 5 / V o a 5 C O D r + + 6 C G Q W E t y a 8 n U 9 l U L L 3 t D z E I S 7 A M K z i p D U j D L m R A 8 F 4 + h 0 v o c g l u j 8 t z w i C V C / i a B f g S X P U D k Y q Y k w = = < / l a t e x i t > s obj < l a t e x i t s h a 1 _ b a s e 6 4 = \" 9 1 B t B Z g K 8\n\n\n\n\ns e z W 9 P 2 1 D K + o u J v o T I K c f J E b s m A P J I 7 8 k z e f 6 z l e D V c L x 3 c p a G W m r X w 2 X L + 7 U + V h j u D k 0 / V r 5 4 Z 1 G H b 8 6 q g d 9 N j 3 C 7 k o b 5 9 e j H I 7 + T i T o J c k x f 0 f 0 X 6 5 B 4 7 0 N u v 8 k 2 W 5 n r / 9 r M + 0 l U I R 8 d / H 9 Q o K G w k + c 1 k K p u K p X f 9 I Q Z h B V Z h D S e 1 B W n Y h w w I 3 s v n c A k 9 L s E d c H l O G K Z y A V + z B F + C q 3 4 A o R e Y m g = = < / l a t e x i t > s ans < l a t e x i t s h a 1 _ b a s e 6 4 = \" p o\n\n\n\n\ns e z W 9 P 2 1 D K + o u J v o T I K c f J E b s m A P J I 7 8 k z e f 6 z l e D V c L x 3 c p a G W m r X w 2 X L + 7 U + V h j u D k 0 / V r 5 4 Z 1 G H b 8 6 q g d 9 N j 3 C 7 k o b 5 9 e j H I 7 + T i T o J c k x f 0 f 0 X 6 5 B 4 7 0 N u v 8 k 2 W 5 n r / 9 r M + 0 l U I R 8 d / H 9 Q o K G w k + c 1 k K p u K p X f 9 I Q Z h B V Z h D S e 1 B W n Y h w w I 3 s v n c A k 9 L s E d c H l O G K Z y A V + z B F + C q 3 4 A m E K Y l g = = < / l a t e x i t > s loc < l a t e x i t s h a 1 _ b a s e 6 4 = \" g V 2 p e Z w b o m Z T r C F q H\n\n\n\n\nH F u e c 4 h o + 0 d c r i l C E S W 6 W 1 Q q e C y 2 p 0 7 t a 0 H L V M r 6 j 0 m 6 Q M I s y e 2 C 3 r s E d 2 x 5 7 Z + 4 + 1 W k 6 N r p c m 7 V J P y 4 2 S / 2 Q 2 / f a n q k a 7 j c N P 1 a + e b Z S x 5 n h V y L v h M N 0 u 5 J 6 + c X T R S a + n w q 0 F d s 1 e y P 8 V a 7 N 7 6 k B r v M o 3 S Z 6 6 / L e f p b 6 u f D S 6 6 P d B 9 Y P s c i S 6 E o k l Y 6 H 4 h j t E L + Y w j 0 W a 1 C r i 2 E Y C G X q Z 4 x R n O B e C w p a w I + z 2 U g W P q 5 n B l x D y H 9 K s l b k = < / l a t e x i t > f < l a t e x i t s h a 1 _ b a s e 6 4 = \" N Z S U L X n w A h L 8 S l N S u s 8 M d K 2 L g N I = \" > A A A C p H i c j V H L S s N A F L 2 N r 1 o f r b o R 3 B R L i w s p E y k q r o p u X L j o w 7 S F 2 p Y k T u v Q v E j S Q g 3 9 A X e u B L t S c C F + h i 7 8 A R f 9 B H F Z w Y 0 L b 9 K A a P F x Q 2 b O n L n n z r l c y V C Y Z R P S D 3 B j 4 x O T U 8 H p 0 M z s 3 H w 4 s r B Y s P S W K\n\n\n\n\nA H s k d e S b v P 9 Z y v B q u l w 7 u 0 l B L j V r 4 b D n / 9 q d K x d 2 G k 0 / V r 5 5 t q M O 2 5 5 W h d 8 N j 3 C 7 k o b 5 9 e j H I 7 + T i T o J c k x f 0 f 0 X 6 5 B 4 7 0 N q v 8 k 2 W 5 n r / 9 r M + 0 lU I R 8 d / H 9 Q o K G w k + c 1 k K p u K p X f 9 I Q Z h B V Z h D S e 1 B W n Y h w w I 3 s v n c A k 9 L s E d c H l O G K Z y A V + z B F + C q 3 4 A O d u Y b A = = < / l a t e x i t > Q enc < l a t e x i ts h a 1 _ b a s e 6 4 = \" j n g 6 T r B G 4 G D S 4 P Z A N H O D M h 3 2 3 2 s = \" > A A A C p H i c j V H L S s N A F L 2 N r 1 o f r b o R 3 B R L i w s p E y k q r o p u X L j o w 7 S F 2 p Y k H e v Q v E j S Q g 3 9 A X e u B L t S c C F + h i 7 8 A R f 9 B H F Z w Y 0 L b 9 K A a P F x Q 2 b O n L n n z r l c y V C Y Z R P S D 3 B j 4 x O T U 8 H p 0 M z s 3 H w 4 s r B Y s P S W K\n\n\n\n\nj d 0 4 k w 8 b i d r j p 1 6 p 5 b 5 + b q r i l 8 y d h j n V L f 0 N j U 3 N I a a 2 v v 6 I w n u r p X f K f k G T x v O K b j r e m a z 0 1 h 8 7 w U 0 u R r r s c 1 S z f 5 q r 4 3 F 9 y v l r n n C 8 d e l v s u 3 7 S 0 H\n\n\n\n\nH F u e c 4 h o + 0 d c r i l C E S W 6 W 1 Q q e C y 2 p 0 7 t a 0 H L V M r 6 j 0 m 6 Q M I s y e 2 C 3 r s E d 2 x 5 7 Z + 4 + 1 W k 6 N r p c m 7 V J P y 4 2 S / 2 Q 2 / f a n q k a 7 j c N P 1 a + e b Z S x 5 n h V y L v h M N 0 u 5 J 6 + c X T R S a + n w q 0 F d s 1 e y P 8 V a 7 N 7 6 k B r v M o 3 S Z 6 6 / L e f p b 6 u f D S 6 6 P d B 9 Y P s c i S 6 E o k l Y 6 H 4 h j t E L + Y w j 0 W a 1 C r i 2 E Y C G X q Z 4 x R n O B e C w p a w I + z 2 U g W P q 5 n B l x D y H 6 8 M l a k = < / l a t e x i t > V < l a t e x i t s h a 1 _ b a s e 6 4 = \" e S 6 R l p + I I 9 L D m F\n\n\n\n\nH F u e c 4 h o + 0 d c r i l C E S W 6 W 1 Q q e C y 2 p 0 7 t a 0 H L V M r 6 j 0 m 6 Q M I s y e 2 C 3 r s E d 2 x 5 7 Z + 4 + 1 W k 6 N r p c m 7 V J P y 4 2 S / 2 Q 2 / f a n q k a 7 j c N P 1 a + e b Z S x 5 n h V y L v h M N 0 u 5 J 6 + c X T R S a + n w q 0 F d s 1 e y P 8 V a 7 N 7 6 k B r v M o 3 S Z 6 6 / L e f p b 6 u f D S 6 6 P d B 9 Y P s c i S 6 E o k l Y 6 H 4 h j t E L + Y w j 0 W a 1 C r i 2 E Y C G X q Z 4 x R n O B e C w p a w I + z 2 U g W P q 5 n B l x D y H 6 P q l a Q = < / l a t e x i t > Q < l a t e x i t s h a 1 _ b a s e 6 4 = \" i F P 9 / e t H i N O p B d Q P\n\n\nFigure 4 .\n4\nFigure 4. ScanQA model for answering 3D environments.Given a point cloud and RGB frame sequence that capture indoor scenes, the QA model outputs a corresponding answer by fusing 3D and language information through three layers: the 3D scene and language encoder layer, fusion layer, and classification layers.\n\n\nFigure 5 .\n5\nFigure 5. Qualitative results.Predicted answers are described below each figure.Predicted boxes are marked blue and the ground truth is marked green.We show examples in which ScanQA produced good object localization when predicting correct answers, whereas ScanRefer+MCAN (pipeline) could not.\n\n\nFigure 6 .Figure 7 .\n67\nFigure 6.Example of real images about the question \"What color is the bathroom door?\"The upper panel is RandomImage, and the lower panel is OracleImage.\n\n\nFigure 8 .\n8\nFigure 8. Example of a TopDownImage about the question \"What color is the bathroom door?\"\n\n\nFigure 9 .\n9\nFigure 9. Qualitative analysis comparing ScanQA and ScanRefer + MCAN (end-to-end).\n\n\nFigure 10 .\n10\nFigure 10.Example of 3D modeling viewer and QA form on the MTurk website.\n\n\nTable 1 .\n1\nComparison of 3D question-answering datasets.\n3D-QA DatasetsTypeQuestion CollectionAnswer CollectionEnvironmentPhotorealistic # 3D ScenesIQUADInteractiveTemplate-basedTemplate-basedAI2THORNo30 roomsEQANavigationTemplate-basedTemplate-basedHouse3DNo588 scenesMP3D-EQANavigationTemplate-basedTemplate-basedMatterport 3DYes144 floorsMT-EQANavigationTemplate-basedTemplate-basedHouse3DNo588 scenesScanQA dataset3D ScanAutoGen+HumanEditHumanScanNetYes800 rooms\n\nTable 2 .\n2\nScanQA dataset statistics.\nSplit# Question # Unique Question # 3D ScenesTrain25,56320,546562Val4,6754,30671Test w/ objects4,9764,55270Test w/o objects6,1495,48497Total41,36332,337800\n\nTable 3 .\n3\nPerformance comparison of question answering with image captioning metrics.e2e represents an end-to-end model.\nANS OBJ LOC EM@1 EM@10 BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE METEOR CIDEr SPICETest w/ objects\u271312.1642.7712.865.450.120.0217.556.7129.174.05\u2713\u271318.3149.1822.3214.5311.157.9226.379.9449.106.36\u2713\u271320.4651.6725.0616.9114.0611.3729.2211.1355.178.21\u2713\u2713\u271323.4556.5131.5621.3915.8712.0434.3413.5567.2911.99Test w/o objects\u271310.7839.4411.945.020.120.0215.345.9125.513.51\u2713\u271316.2346.3021.3713.4910.717.6423.639.1043.216.13\u2713\u271318.1249.6025.1217.5814.6810.2326.5010.4349.938.16\u2713\u2713\u271320.9054.1130.6821.2015.8110.7531.0912.5960.2411.29\n\nTable 4 .\n4\nPerformance comparison of different experimental conditions of the ScanQA model.\nModelAcc@0.25 EM@10Test w/ objectsScanQA (xyz)23.6755.67ScanQA (xyz+rgb)23.4555.43ScanQA (xyz+rgb+normal)23.3554.50ScanQA (xyz+multiview)25.4055.51ScanQA (xyz+multiview+normal)25.4456.51\n\nTable 5 .\n5\nFeature ablation results\n\n\nTable 6 .\n6\n5Object localization performance on the ScanQA dataset\nValidScanRefer+MCAN (pipeline)12.889.13ScanRefer+MCAN (end-to-end)23.5311.76ScanQA24.9615.42Test w/ objectsScanRefer+MCAN (pipeline)12.948.02ScanRefer+MCAN (end-to-end)21.9710.41ScanQA25.4415.03\n\nTable 7 .\n7\nFeature ablation results on ScanQA (multiple) calization scores with the ground true boxes and consider positive predictions for the box with the highest IoU.)We observed that RGB values were effective for QA, and the multiview image features were effective for both object localization and QA.We assumed that by predicting multiple objects, ScanQA (multiple) could utilize those various features.\nModelTop10-Acc@0.25 EM@10ValidScanQA (xyz)67.8349.58ScanQA (xyz+rgb)66.7250.22ScanQA (xyz+rgb+normal)68.1349.45ScanQA (xyz+multiview)67.8549.86ScanQA (xyz+multiview+normal)70.8251.23Test w/ objectsScanQA (xyz)68.2355.18ScanQA (xyz+rgb)68.8756.23ScanQA (xyz+rgb+normal)69.6555.25ScanQA (xyz+multiview)70.7655.65ScanQA (xyz+multiview+normal)71.5855.99\n\nTable 8 .\n8\nPerformance comparison for question answering with image captioning metrics\nModelEM@1EM@10BLEU-1BLEU-2BLEU-3BLEU-4ROUGEMETEORCIDErSPICEValidRandomImage+MCAN (real)19.1948.1523.7115.4111.810.0028.9010.9253.839.35RandomImage+MCAN (mesh)18.5947.8122.1214.4911.727.6927.6110.3250.938.37RandomImage+Oscar (real)19.3846.3722.9114.5211.200.5628.7010.6652.868.91RandomImage+Oscar17.9743.5520.6712.0111.510.5726.519.8247.827.75TopDownImage+MCAN12.7141.5014.828.2116.540.7419.337.5733.145.72TopDownImage+Oscar17.2043.8119.7511.2115.310.7125.399.4345.217.32VoteNet+MCAN17.3345.5428.0916.7210.756.2429.8411.4154.6810.65ScanRefer+MCAN (pipeline, real)14.3744.1217.0210.1715.770.7222.028.4538.736.66ScanRefer+MCAN (pipeline, mesh)14.5743.2716.719.7113.620.6421.828.3238.356.49ScanRefer+MCAN (e2e)18.5946.7626.9316.5911.597.8730.0311.5255.4111.28ScanQA (single)20.2850.0129.4719.8414.659.5532.3712.6061.6611.86ScanQA (multiple)21.0551.2330.2420.4015.1110.0833.3313.1464.8613.43OracleImage+MCAN (real)22.5949.4326.5818.3215.378.5033.2312.4563.4412.56OracleImage+MCAN (mesh)20.6648.0424.3517.0014.230.0030.3411.2357.0110.15OracleImage+Oscar (real)21.3945.0524.2914.1914.210.6731.2411.3358.2310.51OracleImage+Oscar (mesh)22.2746.5923.0113.9614.230.0031.3711.5357.9811.11Test w/ objectsRandomImage+MCAN (real)22.3153.1126.6618.4916.1614.2631.2712.1360.379.05RandomImage+MCAN (mesh)21.7452.4124.8617.4915.3315.1930.0011.5557.558.73RandomImage+Oscar (real)22.6552.3524.7414.429.850.0030.8111.5957.728.52RandomImage+Oscar (mesh)20.9249.2223.1812.269.060.4828.9510.8653.117.12TopDownImage+MCAN15.7647.1516.528.590.000.0021.498.3738.555.34TopDownImage+Oscar20.7649.2622.1911.029.300.4928.2510.5351.826.83VoteNet+MCAN19.7150.7629.4617.2310.336.0830.9712.0758.2310.44ScanRefer+MCAN (pipeline, real)17.5249.9219.1710.660.000.0024.409.3844.256.24ScanRefer+MCAN (pipeline, mesh)17.4448.8318.459.490.000.0023.909.1142.975.93ScanRefer+MCAN (e2e)20.5652.3527.8517.2711.887.4630.6811.9757.3610.58ScanQ (single)23.4556.5131.5621.3915.8712.0434.3413.5567.2911.99ScanQA (multiple)23.0555.9931.4021.1815.8211.7034.0513.6066.7612.30OracleImage+MCAN (real)25.3455.9328.7020.1116.7812.8934.5913.4267.2411.93OracleImage+MCAN (mesh)23.3553.0525.9017.1513.3610.9431.6612.0860.649.01OracleImage+Oscar (real)25.3050.1226.3814.108.700.4733.7212.4763.319.48OracleImage+Oscar (mesh)25.5252.3925.1714.1310.940.0033.5112.6863.1310.52Test w/o objectsRandomImage+MCAN (real)20.8251.2326.2917.9014.279.6629.2311.5455.648.87RandomImage+MCAN (mesh)20.3451.2024.7216.9314.047.5528.1010.9553.418.61RandomImage+Oscar (real)21.5849.8524.8616.1613.290.0028.9910.9954.628.62RandomImage+Oscar (mesh)19.9148.7423.0213.2412.910.6426.9410.1749.837.45TopDownImage+MCAN14.3945.9415.708.8612.560.6219.267.7134.595.22TopDownImage+Oscar19.1348.0621.9311.6614.640.7025.989.6747.376.93VoteNet+MCAN18.1548.5629.6317.8011.577.1029.1211.6853.3410.36ScanRefer+MCAN (pipeline, real)16.4749.0518.7110.9716.530.7622.458.7640.816.41ScanRefer+MCAN (pipeline, mesh)16.3947.7618.2810.4215.030.7122.078.6240.196.03ScanRefer+MCAN (e2e)19.0449.7026.9816.1711.287.8228.6111.3853.4110.63ScanQA (single)20.9054.1130.6821.2015.8110.7531.0912.5960.2411.29ScanQA (multiple)21.3053.0531.1421.2015.8111.1831.6212.8260.9511.68ANS OBJ LOC EM@1 EM@10 BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE METEOR CIDEr SPICEValid\u27137.5327.708.306.030.150.0210.144.4721.042.21\u2713\u271319.8749.4329.2618.7513.037.3031.9812.2860.6311.98\u2713\u271320.4750.0528.3419.0314.169.9531.9112.2561.0211.69\u2713\u2713\u271321.0551.2330.2420.4015.1110.0833.3313.1464.8613.43Test w/ objects\u27138.2032.688.105.570.140.0210.094.2921.021.89\u2713\u271322.0755.1030.4419.8514.3310.1832.8913.0663.9311.51\u2713\u271323.7755.3130.6521.4216.6113.3734.1513.4066.6910.88\u2713\u2713\u271323.0555.9931.4021.1815.8211.7034.0513.6066.7612.30Test w/o objects\u27138.4131.428.405.480.140.0210.324.3520.641.68\u2713\u271320.8053.7031.3020.5114.5810.4831.5112.7560.0911.75\u2713\u271320.9053.7830.0122.3917.9813.7930.6912.5160.3711.34\u2713\u2713\u271321.3053.0531.1421.2015.8111.1831.6212.8260.9511.68\n\nTable 9 .\n9\nPerformance comparison between the different experimental conditions of the ScanQA (multiple) model\nModelEM@1 EM@10 BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE METEOR CIDEr SPICEValidScanQA (L = 1)19.9649.7829.4919.1613.238.4432.3512.5961.1412.61ScanQA (L = 2)20.2850.0129.4719.8414.659.5532.3712.6061.6611.86ScanQA (L = 3)11.9439.1413.938.827.230.0018.646.9833.30Test w/ objectsScanQA (L = 1)23.8355.6332.6421.8015.6311.6735.2014.1569.7012.65ScanQA (L = 2)23.4556.5131.5621.3915.8712.0434.3413.5567.2911.99ScanQA (L = 3)13.8344.7115.057.905.840.0019.627.3436.385.62Test w/o objectsScanQA (L = 1)21.0152.5031.2321.3715.9711.2031.5512.8461.1111.82ScanQA (L = 2)20.9054.1130.6821.2015.8110.7531.0912.5960.2411.29ScanQA (L = 3)11.6340.3013.427.755.960.0016.756.4329.865.13\n\nTable 10 .\n10\nPerformance comparison for the ScanQA model with difference number of layers L\nModelEM@1 EM@10 BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE METEOR CIDEr SPICEValidScanQA (d = 128)20.8850.7030.0820.6215.7211.1833.2512.9764.0912.77ScanQA (d = 256)20.2850.0129.4719.8414.659.5532.3712.6061.6611.86ScanQA (d = 512)13.9941.5417.0111.028.260.0021.978.1138.786.89Test w/ objectsScanQA (d = 128)24.3856.7132.3022.4717.9814.9635.2414.0769.5312.61ScanQA (d = 256)23.4556.5131.5621.3915.8712.0434.3413.5567.2911.99ScanQA (d = 512)14.7546.0216.899.187.066.7721.177.7638.565.47Test w/o objectsScanQA (d = 128)21.1754.2031.7922.2316.6511.3231.8313.0161.9212.13ScanQA (d = 256)20.9054.1130.6821.2015.8110.7531.0912.5960.2411.29ScanQA (d = 512)12.8343.2816.679.567.014.7018.977.3133.635.08\n\nTable 11 .\n11\nPerformance comparison for the ScanQA model with difference hidden size d\n\n\nTable 15 .\n15\nTest w/o object set performance comparison for question answering with image captioning metrics.e2e represents an end-to-end model.\nObject natureRandomImage+MCAN15.1741.6729.0625.8423.070.9847.2111.4627.547.27VoteNet+MCAN10.6841.4533.2918.0211.910.0045.8512.3728.4415.82ScanRefer+MCAN (pipeline)13.4639.7420.7322.6725.651.0335.678.2922.183.46ScanRefer+MCAN (e2e)13.2539.3234.5119.2313.250.0051.5212.9630.1318.28ScanQA (single)15.6046.1541.9628.7519.960.9265.4815.6235.6020.89ScanQA (multiple)13.6847.6541.4025.5820.390.0159.7014.7534.4319.06PlaceRandomImage+MCAN4.7619.3421.8814.4811.087.7641.129.1822.0812.85VoteNet+MCAN5.8520.2032.1520.6213.868.8050.5411.1629.0616.43ScanRefer+MCAN (pipeline)1.6411.938.874.580.110.0212.825.349.145.42ScanRefer+MCAN (e2e)4.9921.1424.4715.0310.767.9544.469.5723.9915.05ScanQA (single)6.7926.3732.4723.0418.3413.7463.4012.2729.6719.28ScanQA (multiple)6.7924.5730.1820.3915.1910.8156.6511.3727.9717.71NumberRandomImage+MCAN42.5190.6445.130.070.010.0077.6022.4045.210.00VoteNet+MCAN33.6990.6440.3525.050.400.0563.8118.5338.630.27ScanRefer+MCAN (pipeline)41.4490.6444.120.060.010.0074.3122.0144.170.00ScanRefer+MCAN (e2e)37.9789.3041.120.000.000.0070.0819.6940.830.00ScanQA (single)36.9090.6442.6928.690.430.0568.3019.5741.430.00ScanQA (multiple)40.9190.6446.2029.850.440.0577.2621.8445.160.00OtherRandomImage+MCAN15.2143.1118.7414.8515.930.7240.508.6522.328.13VoteNet+MCAN12.3637.7520.159.263.220.0033.828.2520.737.64ScanRefer+MCAN (pipeline)11.8240.8116.7411.260.000.0032.497.2319.045.50ScanRefer+MCAN (e2e)13.3538.5120.319.826.210.0037.708.7322.349.12ScanQA (single)15.3241.7921.9912.746.920.0040.309.3023.9910.33ScanQA (multiple)14.5540.4820.6912.938.576.8138.748.6922.539.22\nAcknowledgementsThis work was supported by NEDO JPNP20006, JSPS KAKENHI 21H03516 and 18KK0284, and JST PRESTO JPMJPR20C2.Supplementary MaterialIn this supplementary material, we first provide additional experimental details in Section A. Next, we provide additional experimental quantitative (Section B) and qualitative results (Section C).Finally, we have attached a snapshot of the annotation website (Section D).A. Additional DetailsWe conducted additional experiments to demonstrate the effectiveness of the proposed method compared with various other methods.In this section, we introduce other baselines and an additional ScanQA model that considers multiple objects related to a question.A.1. Additional Baseline ModelsAdditional 2D-QA Image.We prepared RandomImage (or OracleImage) + 2D-QA as a 2D-QA baseline method that uses three images captured in the environment per question.Fig.6shows the images randomly sampled from the video to build the ScanNet dataset.In addition to using such real images from the environments, we also used images of the mesh data of ScanNet captured at positions and angles similar to real images as in Fig.7.We refer to models using real images \"real\" and ones using mesh images \"mesh.\"We also used mesh images captured from a top-down view (referred to as TopDownImage) to view the entire room with a single image (Fig.8).Additional 2D-QA Model.In addition to MCAN[51], we evaluated 2D-QA using the BERT-based model Oscar[30]trained with many image-text pairs and has demonstrated high performance in various tasks, such as VQA, image retrieval, image captioning, and natural language visual reasoning.Although MCAN and Oscar use effective pretrained models, unlike our method, we can emphasize the effectiveness of the ScanQA model by comparing it to these models.A.2. Additional ScanQA ModelThe ScanQA model introduced in Section 4 predicts the object confidence (box confidences) and object classification for a single object.However, a given question is occasionally associated with more than one object.Thus, we extended the ScanQA model to perform object localization and labeling of multiple objects.Hence, we computed the final scores for all object proposals (or object labels) normalized by a sigmoid function and used BCE loss to train both the object localization and object classification modules.The proposed method used in this study was ScanQA (single), and the model that considers multiple objects was called ScanQA (multiple).Hereafter, unless otherwise specified, ScanQA is referred to as ScanQA (single).questions with distinct meanings as they are constructed through human (re)annotation; they reflect various questions that humans may ask.The number of unique answers is also high corresponding to the number of unique questions.We noticed that the question length in terms of tokens had a fat tail distribution.Interestingly, there are both very short and long questions in the dataset such as \"How many chairs?\" and \"What color is the chair that is located to the right of another brown chair with a red bag on it?\"\nReferIt3D: Neural listeners for fine-grained 3d object identification in real-world scenes. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas J Guibas, Proceedings of the 16th European Conference on Computer Vision (ECCV). the 16th European Conference on Computer Vision (ECCV)2020\n\nSynthetic QA corpora generation with roundtrip consistency. Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, Michael Collins, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL). the 57th Annual Meeting of the Association for Computational Linguistics (ACL)2019\n\nSPICE: Semantic propositional image caption evaluation. Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2016\n\nBottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)20186\n\nVision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2018\n\nVQA: Visual Question Answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)20151\n\nMETEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAssociation for Computational Linguistics2005\n\nA recurrent BERTbased model for question generation. Ying-Hong Chan, Yao-Chung Fan, Proceedings of the 2nd Workshop on Machine Reading for Question Answering. the 2nd Workshop on Machine Reading for Question Answering2019\n\nAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, Matterport3D: Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV). 2017\n\nScanrefer: 3d object localization in rgb-d scans using natural language. Dave Zhenyu, Chen , Angel X Chang, Matthias Nie\u00dfner, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Springer2020. 1, 2, 3, 4, 5, 7, 8\n\nTouchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, Yoav Artzi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2019\n\nZitnick. Microsoft coco captions: Data collection and evaluation server. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence, CoRR, abs/1504.003252015\n\nScan2cap: Context-aware dense captioning in rgbd scans. Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, Angel X Chang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2021. 1, 4, 5\n\nVisual question answering on 360deg images. Shih-Han Chou, Wei-Lun Chao, Wei-Sheng Lai, Min Sun, Ming-Hsuan Yang, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)2020\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nie\u00dfner, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)20175\n\n3DMV: Joint 3d-multiview prediction for 3d semantic scene segmentation. Angela Dai, Matthias Nie\u00dfner, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018\n\nEmbodied Question Answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)201823\n\nSpeaker-follower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Advances in Neural Information Processing Systems. S Bengio, H Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, R Garnett, Curran Associates, Inc201831\n\nIQA: Visual question answering in interactive environments. Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE2018\n\nMaking the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)July 2017. 1\n\nGaussian error linear units (GELUs). Dan Hendrycks, Kevin Gimpel, CoRR, abs/1606.084152016\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 981997\n\nGqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2019\n\nVideo Question Answering with Spatio-Temporal Reasoning. Yunseok Jang, Yale Song, Chris Dongjoo Kim, Youngjae Yu, Youngjin Kim, Gunhee Kim, International Journal of Computer Vision (IJCV). 22019\n\nReferItGame: Referring to objects in photographs of natural scenes. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational Linguistics2014\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2015\n\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. 2017\n\nGenerative languagegrounded policy in vision-and-language navigation with bayes' rule. Shuhei Kurita, Kyunghyun Cho, International Conference on Learning Representations (ICLR). 2021\n\nLess is more: ClipBERT for video-and-language learningvia sparse sampling. Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, Jingjing Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2021\n\nOscar: Objectsemantics aligned pre-training for vision-language tasks. Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)202021\n\nROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Association for Computational Linguistics2004\n\nSimplifying paragraph-level question generation via transformer language models. Enrico Luis, Diane Lopez, Kathryn Cruz, Jan Christian Blaise, Charibeth Cruz, Cheng, Proceedings of the Pacific Rim International Conference on Artificial Intelligence (PRICAI). Nghia Duc, Thanaruk Pham, Guido Theeramunkong, Fenrong Governatori, Liu, the Pacific Rim International Conference on Artificial Intelligence (PRICAI)2021\n\nEffective approaches to attention-based neural machine translation. Thang Luong, Hieu Pham, Christopher D Manning, Proceedings of the Empirical Methods in Natural Language Processing (EMNLP). the Empirical Methods in Natural Language Processing (EMNLP)2015\n\nA multi-world approach to question answering about real-world scenes based on uncertain input. Mateusz Malinowski, Mario Fritz, Proceedings of the Neural Information Processing Systems (NeurIPS). the Neural Information Processing Systems (NeurIPS)2014\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL). the 40th Annual Meeting of the Association for Computational Linguistics (ACL)Philadelphia, Pennsylvania, USAAssociation for Computational Linguistics2002\n\nFlickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models. Jeffrey Pennington, Richard Socher, Christopher D Manning, B A Plummer, L Wang, C M Cervantes, J C Caicedo, J Hockenmaier, S Lazebnik, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2014. 2015Proceeding of Empirical Methods in Natural Language Processing (EMNLP)\n\nDeep hough voting for 3d object detection in point clouds. Charles R Qi, Or Litany, Kaiming He, Leonidas J Guibas, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2019. 2, 4, 5, 6\n\nPointNet++: Deep hierarchical feature learning on point sets in a metric space. Charles Ruizhongtai, Qi , Li Yi, Hao Su, Leonidas J Guibas, Proceedings of Neural Information Processing Systems. Neural Information Processing Systems2017\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020\n\nSQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016\n\nConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Proceedings of the Association for Computational Linguistics (ACL). the Association for Computational Linguistics (ACL)2018\n\nALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 Kaiser, Illia Polosukhin, Proceedings of Neural Information Processing Systems. I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Neural Information Processing Systems201746\n\nCIDEr: Consensus-based image description evaluation. Ramakrishna, C Lawrence Vedantam, Devi Zitnick, Parikh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE Computer Society2015\n\nEmbodied Question Answering in Photorealistic Environments with Point Cloud Perception. Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)201923\n\nBuilding generalizable agents with a realistic and rich 3d environment. Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian, arXiv:1801.022092018arXiv preprint\n\nJust ask: Learning to answer questions from millions of narrated videos. Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2021\n\nMulti-target embodied question answering. Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara L Berg, Dhruv Batra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)20191\n\nModeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, Proceedings of the European Conference on Computer Vision (ECCV). Bastian Leibe, Jiri Matas, Nicu Sebe, Max Welling, the European Conference on Computer Vision (ECCV)2016\n\nDeep modular co-attention networks for visual question answering. Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, Qi Tian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2019. 2, 4, 5, 6, 1\n\nInstancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, Shuguang Cui, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2021\n\n3dvg-transformer: Relation modeling for visual grounding on point clouds. Lichen Zhao, Daigang Cai, Lu Sheng, Dong Xu, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2021\n", "annotations": {"author": "[{\"end\":97,\"start\":65},{\"end\":133,\"start\":98},{\"end\":163,\"start\":134},{\"end\":194,\"start\":164},{\"end\":221,\"start\":195},{\"end\":270,\"start\":222},{\"end\":301,\"start\":271},{\"end\":338,\"start\":302}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":72},{\"end\":113,\"start\":104},{\"end\":143,\"start\":140},{\"end\":174,\"start\":171},{\"end\":201,\"start\":195},{\"end\":231,\"start\":228},{\"end\":281,\"start\":275},{\"end\":318,\"start\":310}]", "author_first_name": "[{\"end\":71,\"start\":65},{\"end\":103,\"start\":98},{\"end\":139,\"start\":134},{\"end\":170,\"start\":164},{\"end\":227,\"start\":222},{\"end\":274,\"start\":271},{\"end\":309,\"start\":302}]", "author_affiliation": "[{\"end\":96,\"start\":79},{\"end\":132,\"start\":115},{\"end\":162,\"start\":145},{\"end\":193,\"start\":176},{\"end\":220,\"start\":203},{\"end\":250,\"start\":233},{\"end\":269,\"start\":252},{\"end\":300,\"start\":283},{\"end\":337,\"start\":320}]", "title": "[{\"end\":62,\"start\":1},{\"end\":400,\"start\":339}]", "venue": null, "abstract": "[{\"end\":2014,\"start\":434}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2339,\"start\":2335},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2342,\"start\":2339},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2384,\"start\":2380},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2387,\"start\":2384},{\"end\":2432,\"start\":2428},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2473,\"start\":2470},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2476,\"start\":2473},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2479,\"start\":2476},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3654,\"start\":3650},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3702,\"start\":3699},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3754,\"start\":3750},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3787,\"start\":3783},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3790,\"start\":3787},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3793,\"start\":3790},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5011,\"start\":5008},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5339,\"start\":5335},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5419,\"start\":5415},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5968,\"start\":5964},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5971,\"start\":5968},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6008,\"start\":6004},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6657,\"start\":6653},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6692,\"start\":6689},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6694,\"start\":6692},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6697,\"start\":6694},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6764,\"start\":6760},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6909,\"start\":6905},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7031,\"start\":7027},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7136,\"start\":7132},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7491,\"start\":7487},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7720,\"start\":7716},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8293,\"start\":8289},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8309,\"start\":8305},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8443,\"start\":8439},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8446,\"start\":8443},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8449,\"start\":8446},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8559,\"start\":8556},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8562,\"start\":8559},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8565,\"start\":8562},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8568,\"start\":8565},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8584,\"start\":8580},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8587,\"start\":8584},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8620,\"start\":8616},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8769,\"start\":8765},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8783,\"start\":8779},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8843,\"start\":8840},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8855,\"start\":8851},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10339,\"start\":10336},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10341,\"start\":10339},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10344,\"start\":10341},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10347,\"start\":10344},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10569,\"start\":10565},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10651,\"start\":10647},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10707,\"start\":10703},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13046,\"start\":13042},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13153,\"start\":13149},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14854,\"start\":14850},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14857,\"start\":14854},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14975,\"start\":14971},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15191,\"start\":15187},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15370,\"start\":15366},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15488,\"start\":15484},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15727,\"start\":15723},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15755,\"start\":15751},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16122,\"start\":16118},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16174,\"start\":16170},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17018,\"start\":17014},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17041,\"start\":17037},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27940,\"start\":27936},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28290,\"start\":28286},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":28293,\"start\":28290},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":28296,\"start\":28293},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28922,\"start\":28918},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":29038,\"start\":29034},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29501,\"start\":29497},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29555,\"start\":29551},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29558,\"start\":29555},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29932,\"start\":29928},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30656,\"start\":30652},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30670,\"start\":30666},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30682,\"start\":30679},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30694,\"start\":30690},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30710,\"start\":30707},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":31094,\"start\":31090},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31148,\"start\":31144},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31705,\"start\":31702},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32813,\"start\":32809},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33150,\"start\":33146},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":38276,\"start\":38272},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":38759,\"start\":38755},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":41372,\"start\":41368}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47872,\"start\":47697},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47977,\"start\":47873},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48017,\"start\":47978},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48553,\"start\":48018},{\"attributes\":{\"id\":\"fig_5\"},\"end\":49075,\"start\":48554},{\"attributes\":{\"id\":\"fig_6\"},\"end\":49627,\"start\":49076},{\"attributes\":{\"id\":\"fig_7\"},\"end\":50561,\"start\":49628},{\"attributes\":{\"id\":\"fig_8\"},\"end\":51391,\"start\":50562},{\"attributes\":{\"id\":\"fig_9\"},\"end\":51625,\"start\":51392},{\"attributes\":{\"id\":\"fig_10\"},\"end\":52221,\"start\":51626},{\"attributes\":{\"id\":\"fig_11\"},\"end\":52821,\"start\":52222},{\"attributes\":{\"id\":\"fig_12\"},\"end\":53146,\"start\":52822},{\"attributes\":{\"id\":\"fig_13\"},\"end\":53455,\"start\":53147},{\"attributes\":{\"id\":\"fig_14\"},\"end\":53634,\"start\":53456},{\"attributes\":{\"id\":\"fig_15\"},\"end\":53739,\"start\":53635},{\"attributes\":{\"id\":\"fig_16\"},\"end\":53837,\"start\":53740},{\"attributes\":{\"id\":\"fig_17\"},\"end\":53928,\"start\":53838},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54397,\"start\":53929},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54593,\"start\":54398},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":55222,\"start\":54594},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":55503,\"start\":55223},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":55542,\"start\":55504},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":55805,\"start\":55543},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":56566,\"start\":55806},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":60523,\"start\":56567},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":61296,\"start\":60524},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":62074,\"start\":61297},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":62164,\"start\":62075},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":63889,\"start\":62165}]", "paragraph": "[{\"end\":2911,\"start\":2030},{\"end\":3539,\"start\":2913},{\"end\":4291,\"start\":3541},{\"end\":5188,\"start\":4293},{\"end\":6183,\"start\":5190},{\"end\":6394,\"start\":6200},{\"end\":7436,\"start\":6424},{\"end\":8022,\"start\":7477},{\"end\":9146,\"start\":8058},{\"end\":9254,\"start\":9165},{\"end\":10216,\"start\":9269},{\"end\":11773,\"start\":10247},{\"end\":12685,\"start\":11775},{\"end\":13642,\"start\":12708},{\"end\":14394,\"start\":13644},{\"end\":17160,\"start\":14411},{\"end\":27645,\"start\":27477},{\"end\":28081,\"start\":27647},{\"end\":28540,\"start\":28083},{\"end\":29340,\"start\":28542},{\"end\":30695,\"start\":29377},{\"end\":31056,\"start\":30697},{\"end\":32042,\"start\":31058},{\"end\":32786,\"start\":32044},{\"end\":32896,\"start\":32788},{\"end\":34410,\"start\":32898},{\"end\":36570,\"start\":34436},{\"end\":36791,\"start\":36591},{\"end\":38490,\"start\":36828},{\"end\":39455,\"start\":38547},{\"end\":40294,\"start\":39480},{\"end\":41029,\"start\":40309},{\"end\":41668,\"start\":41063},{\"end\":42743,\"start\":41704},{\"end\":43581,\"start\":42790},{\"end\":43876,\"start\":43615},{\"end\":44304,\"start\":43913},{\"end\":44844,\"start\":44306},{\"end\":46703,\"start\":44883},{\"end\":46965,\"start\":46735},{\"end\":47642,\"start\":46967},{\"end\":47696,\"start\":47644},{\"end\":47871,\"start\":47711},{\"end\":47976,\"start\":47887},{\"end\":48016,\"start\":47981},{\"end\":48552,\"start\":48021},{\"end\":49074,\"start\":48557},{\"end\":49626,\"start\":49079},{\"end\":50560,\"start\":49631},{\"end\":51390,\"start\":50565},{\"end\":51624,\"start\":51395},{\"end\":52220,\"start\":51629},{\"end\":52820,\"start\":52225},{\"end\":53145,\"start\":52836},{\"end\":53454,\"start\":53161},{\"end\":53633,\"start\":53481},{\"end\":53738,\"start\":53649},{\"end\":53836,\"start\":53754},{\"end\":53927,\"start\":53854},{\"end\":53987,\"start\":53942},{\"end\":54437,\"start\":54411},{\"end\":54717,\"start\":54607},{\"end\":55316,\"start\":55236},{\"end\":55541,\"start\":55517},{\"end\":55610,\"start\":55556},{\"end\":56216,\"start\":55819},{\"end\":56655,\"start\":56580},{\"end\":60636,\"start\":60537},{\"end\":61390,\"start\":61312},{\"end\":62163,\"start\":62090},{\"end\":62311,\"start\":62180}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":20771,\"start\":17161},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21424,\"start\":20771},{\"attributes\":{\"id\":\"formula_2\"},\"end\":22091,\"start\":21424},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22772,\"start\":22091},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23353,\"start\":22772},{\"attributes\":{\"id\":\"formula_5\"},\"end\":23724,\"start\":23353},{\"attributes\":{\"id\":\"formula_6\"},\"end\":24797,\"start\":23724},{\"attributes\":{\"id\":\"formula_7\"},\"end\":26124,\"start\":24797},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26711,\"start\":26124},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27476,\"start\":26711}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":8956,\"start\":8955},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12825,\"start\":12824},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":34533,\"start\":34532},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":36670,\"start\":36669},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":36790,\"start\":36789},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":37300,\"start\":37299},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":37932,\"start\":37931},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":38770,\"start\":38769},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":39249,\"start\":39248},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":41392,\"start\":41391},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":41506,\"start\":41505},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":41711,\"start\":41710},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":43032,\"start\":43031},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":43092,\"start\":43091},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":43115,\"start\":43114},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":43264,\"start\":43263},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":43751,\"start\":43742},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44070,\"start\":44068},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":44191,\"start\":44189},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":44214,\"start\":44212}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2028,\"start\":2016},{\"attributes\":{\"n\":\"2.\"},\"end\":6198,\"start\":6186},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6422,\"start\":6397},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7475,\"start\":7439},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8056,\"start\":8025},{\"attributes\":{\"n\":\"3.\"},\"end\":9163,\"start\":9149},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9267,\"start\":9257},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10245,\"start\":10219},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12706,\"start\":12688},{\"attributes\":{\"n\":\"4.\"},\"end\":14409,\"start\":14397},{\"attributes\":{\"n\":\"5.\"},\"end\":29354,\"start\":29343},{\"attributes\":{\"n\":\"5.1.\"},\"end\":29375,\"start\":29357},{\"attributes\":{\"n\":\"5.2.\"},\"end\":34434,\"start\":34413},{\"attributes\":{\"n\":\"5.3.\"},\"end\":36589,\"start\":36573},{\"end\":36826,\"start\":36794},{\"end\":38545,\"start\":38493},{\"attributes\":{\"n\":\"5.4.\"},\"end\":39478,\"start\":39458},{\"attributes\":{\"n\":\"6.\"},\"end\":40307,\"start\":40297},{\"end\":41061,\"start\":41032},{\"end\":41702,\"start\":41671},{\"end\":42788,\"start\":42746},{\"end\":43613,\"start\":43584},{\"end\":43911,\"start\":43879},{\"end\":44881,\"start\":44847},{\"end\":46733,\"start\":46706},{\"end\":47708,\"start\":47698},{\"end\":47884,\"start\":47874},{\"end\":52833,\"start\":52823},{\"end\":53158,\"start\":53148},{\"end\":53477,\"start\":53457},{\"end\":53646,\"start\":53636},{\"end\":53751,\"start\":53741},{\"end\":53850,\"start\":53839},{\"end\":53939,\"start\":53930},{\"end\":54408,\"start\":54399},{\"end\":54604,\"start\":54595},{\"end\":55233,\"start\":55224},{\"end\":55514,\"start\":55505},{\"end\":55553,\"start\":55544},{\"end\":55816,\"start\":55807},{\"end\":56577,\"start\":56568},{\"end\":60534,\"start\":60525},{\"end\":61308,\"start\":61298},{\"end\":62086,\"start\":62076},{\"end\":62176,\"start\":62166}]", "table": "[{\"end\":54397,\"start\":53988},{\"end\":54593,\"start\":54438},{\"end\":55222,\"start\":54718},{\"end\":55503,\"start\":55317},{\"end\":55805,\"start\":55611},{\"end\":56566,\"start\":56217},{\"end\":60523,\"start\":56656},{\"end\":61296,\"start\":60637},{\"end\":62074,\"start\":61391},{\"end\":63889,\"start\":62312}]", "figure_caption": "[{\"end\":47872,\"start\":47710},{\"end\":47977,\"start\":47886},{\"end\":48017,\"start\":47980},{\"end\":48553,\"start\":48020},{\"end\":49075,\"start\":48556},{\"end\":49627,\"start\":49078},{\"end\":50561,\"start\":49630},{\"end\":51391,\"start\":50564},{\"end\":51625,\"start\":51394},{\"end\":52221,\"start\":51628},{\"end\":52821,\"start\":52224},{\"end\":53146,\"start\":52835},{\"end\":53455,\"start\":53160},{\"end\":53634,\"start\":53480},{\"end\":53739,\"start\":53648},{\"end\":53837,\"start\":53753},{\"end\":53928,\"start\":53853},{\"end\":53988,\"start\":53941},{\"end\":54438,\"start\":54410},{\"end\":54718,\"start\":54606},{\"end\":55317,\"start\":55235},{\"end\":55542,\"start\":55516},{\"end\":55611,\"start\":55555},{\"end\":56217,\"start\":55818},{\"end\":56656,\"start\":56579},{\"end\":60637,\"start\":60536},{\"end\":61391,\"start\":61311},{\"end\":62164,\"start\":62089},{\"end\":62312,\"start\":62179}]", "figure_ref": "[{\"end\":2709,\"start\":2708},{\"end\":4648,\"start\":4647},{\"end\":9293,\"start\":9292},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10942,\"start\":10941},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11771,\"start\":11770},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13546,\"start\":13545},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":39641,\"start\":39640},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":45056,\"start\":45055},{\"attributes\":{\"ref_id\":\"fig_16\"},\"end\":45064,\"start\":45063},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":46855,\"start\":46853}]", "bib_author_first_name": "[{\"end\":67071,\"start\":67066},{\"end\":67089,\"start\":67084},{\"end\":67106,\"start\":67103},{\"end\":67119,\"start\":67112},{\"end\":67139,\"start\":67131},{\"end\":67141,\"start\":67140},{\"end\":67346,\"start\":67341},{\"end\":67362,\"start\":67356},{\"end\":67375,\"start\":67370},{\"end\":67389,\"start\":67384},{\"end\":67405,\"start\":67398},{\"end\":67655,\"start\":67650},{\"end\":67672,\"start\":67666},{\"end\":67687,\"start\":67683},{\"end\":67704,\"start\":67697},{\"end\":67923,\"start\":67918},{\"end\":67942,\"start\":67934},{\"end\":67952,\"start\":67947},{\"end\":67968,\"start\":67962},{\"end\":67980,\"start\":67976},{\"end\":67997,\"start\":67990},{\"end\":68008,\"start\":68005},{\"end\":68300,\"start\":68295},{\"end\":68313,\"start\":68311},{\"end\":68324,\"start\":68318},{\"end\":68336,\"start\":68332},{\"end\":68348,\"start\":68344},{\"end\":68362,\"start\":68358},{\"end\":68378,\"start\":68375},{\"end\":68392,\"start\":68385},{\"end\":68405,\"start\":68400},{\"end\":68633,\"start\":68624},{\"end\":68650,\"start\":68641},{\"end\":68666,\"start\":68660},{\"end\":68679,\"start\":68671},{\"end\":68695,\"start\":68690},{\"end\":68704,\"start\":68703},{\"end\":68713,\"start\":68705},{\"end\":68727,\"start\":68723},{\"end\":68989,\"start\":68980},{\"end\":69004,\"start\":69000},{\"end\":69354,\"start\":69345},{\"end\":69370,\"start\":69361},{\"end\":69520,\"start\":69515},{\"end\":69534,\"start\":69528},{\"end\":69546,\"start\":69540},{\"end\":69565,\"start\":69559},{\"end\":69582,\"start\":69574},{\"end\":69600,\"start\":69593},{\"end\":69614,\"start\":69608},{\"end\":69625,\"start\":69621},{\"end\":69637,\"start\":69632},{\"end\":69836,\"start\":69832},{\"end\":69849,\"start\":69845},{\"end\":69857,\"start\":69852},{\"end\":69859,\"start\":69858},{\"end\":69875,\"start\":69867},{\"end\":70133,\"start\":70127},{\"end\":70145,\"start\":70140},{\"end\":70160,\"start\":70152},{\"end\":70172,\"start\":70168},{\"end\":70186,\"start\":70182},{\"end\":70442,\"start\":70436},{\"end\":70452,\"start\":70449},{\"end\":70467,\"start\":70459},{\"end\":70484,\"start\":70473},{\"end\":70502,\"start\":70495},{\"end\":70515,\"start\":70510},{\"end\":70525,\"start\":70524},{\"end\":70624,\"start\":70618},{\"end\":70634,\"start\":70631},{\"end\":70652,\"start\":70644},{\"end\":70667,\"start\":70662},{\"end\":70669,\"start\":70668},{\"end\":70907,\"start\":70899},{\"end\":70921,\"start\":70914},{\"end\":70937,\"start\":70928},{\"end\":70946,\"start\":70943},{\"end\":70962,\"start\":70952},{\"end\":71205,\"start\":71199},{\"end\":71216,\"start\":71211},{\"end\":71218,\"start\":71217},{\"end\":71233,\"start\":71226},{\"end\":71247,\"start\":71241},{\"end\":71262,\"start\":71256},{\"end\":71283,\"start\":71275},{\"end\":71541,\"start\":71535},{\"end\":71555,\"start\":71547},{\"end\":71723,\"start\":71715},{\"end\":71735,\"start\":71729},{\"end\":71750,\"start\":71743},{\"end\":71767,\"start\":71761},{\"end\":71777,\"start\":71773},{\"end\":71791,\"start\":71786},{\"end\":72036,\"start\":72030},{\"end\":72052,\"start\":72044},{\"end\":72063,\"start\":72057},{\"end\":72075,\"start\":72071},{\"end\":72091,\"start\":72086},{\"end\":72115,\"start\":72101},{\"end\":72131,\"start\":72125},{\"end\":72154,\"start\":72150},{\"end\":72166,\"start\":72163},{\"end\":72180,\"start\":72174},{\"end\":72242,\"start\":72241},{\"end\":72252,\"start\":72251},{\"end\":72263,\"start\":72262},{\"end\":72277,\"start\":72276},{\"end\":72288,\"start\":72287},{\"end\":72304,\"start\":72303},{\"end\":72410,\"start\":72404},{\"end\":72428,\"start\":72419},{\"end\":72447,\"start\":72439},{\"end\":72465,\"start\":72459},{\"end\":72480,\"start\":72474},{\"end\":72489,\"start\":72486},{\"end\":72776,\"start\":72772},{\"end\":72789,\"start\":72784},{\"end\":72803,\"start\":72796},{\"end\":72823,\"start\":72818},{\"end\":72835,\"start\":72831},{\"end\":73053,\"start\":73050},{\"end\":73070,\"start\":73065},{\"end\":73133,\"start\":73129},{\"end\":73152,\"start\":73146},{\"end\":73284,\"start\":73283},{\"end\":73302,\"start\":73291},{\"end\":73304,\"start\":73303},{\"end\":73555,\"start\":73548},{\"end\":73566,\"start\":73562},{\"end\":73578,\"start\":73573},{\"end\":73586,\"start\":73579},{\"end\":73600,\"start\":73592},{\"end\":73613,\"start\":73605},{\"end\":73625,\"start\":73619},{\"end\":73760,\"start\":73755},{\"end\":73780,\"start\":73773},{\"end\":73794,\"start\":73790},{\"end\":73809,\"start\":73803},{\"end\":74101,\"start\":74093},{\"end\":74115,\"start\":74110},{\"end\":74273,\"start\":74269},{\"end\":74288,\"start\":74281},{\"end\":74305,\"start\":74299},{\"end\":74314,\"start\":74311},{\"end\":74331,\"start\":74327},{\"end\":74345,\"start\":74339},{\"end\":74362,\"start\":74356},{\"end\":74375,\"start\":74371},{\"end\":74388,\"start\":74381},{\"end\":74399,\"start\":74396},{\"end\":74563,\"start\":74557},{\"end\":74581,\"start\":74572},{\"end\":74732,\"start\":74729},{\"end\":74744,\"start\":74738},{\"end\":74755,\"start\":74749},{\"end\":74765,\"start\":74762},{\"end\":74777,\"start\":74771},{\"end\":74779,\"start\":74778},{\"end\":74791,\"start\":74786},{\"end\":74808,\"start\":74800},{\"end\":75060,\"start\":75054},{\"end\":75067,\"start\":75065},{\"end\":75081,\"start\":75073},{\"end\":75093,\"start\":75086},{\"end\":75107,\"start\":75098},{\"end\":75118,\"start\":75115},{\"end\":75132,\"start\":75126},{\"end\":75146,\"start\":75139},{\"end\":75153,\"start\":75151},{\"end\":75164,\"start\":75160},{\"end\":75175,\"start\":75170},{\"end\":75190,\"start\":75182},{\"end\":75383,\"start\":75375},{\"end\":75556,\"start\":75550},{\"end\":75568,\"start\":75563},{\"end\":75583,\"start\":75576},{\"end\":75593,\"start\":75590},{\"end\":75621,\"start\":75612},{\"end\":75733,\"start\":75728},{\"end\":75747,\"start\":75739},{\"end\":75759,\"start\":75754},{\"end\":75782,\"start\":75775},{\"end\":75956,\"start\":75951},{\"end\":75968,\"start\":75964},{\"end\":75986,\"start\":75975},{\"end\":75988,\"start\":75987},{\"end\":76243,\"start\":76236},{\"end\":76261,\"start\":76256},{\"end\":76465,\"start\":76458},{\"end\":76481,\"start\":76476},{\"end\":76494,\"start\":76490},{\"end\":76509,\"start\":76501},{\"end\":76873,\"start\":76866},{\"end\":76893,\"start\":76886},{\"end\":76913,\"start\":76902},{\"end\":76915,\"start\":76914},{\"end\":76926,\"start\":76925},{\"end\":76928,\"start\":76927},{\"end\":76939,\"start\":76938},{\"end\":76947,\"start\":76946},{\"end\":76949,\"start\":76948},{\"end\":76962,\"start\":76961},{\"end\":76964,\"start\":76963},{\"end\":76975,\"start\":76974},{\"end\":76990,\"start\":76989},{\"end\":77292,\"start\":77285},{\"end\":77294,\"start\":77293},{\"end\":77301,\"start\":77299},{\"end\":77317,\"start\":77310},{\"end\":77330,\"start\":77322},{\"end\":77332,\"start\":77331},{\"end\":77589,\"start\":77582},{\"end\":77605,\"start\":77603},{\"end\":77610,\"start\":77608},{\"end\":77618,\"start\":77615},{\"end\":77631,\"start\":77623},{\"end\":77633,\"start\":77632},{\"end\":77827,\"start\":77822},{\"end\":77840,\"start\":77836},{\"end\":77854,\"start\":77850},{\"end\":77873,\"start\":77864},{\"end\":77885,\"start\":77879},{\"end\":77901,\"start\":77894},{\"end\":77915,\"start\":77910},{\"end\":77925,\"start\":77922},{\"end\":77935,\"start\":77930},{\"end\":77937,\"start\":77936},{\"end\":78059,\"start\":78053},{\"end\":78075,\"start\":78071},{\"end\":78093,\"start\":78083},{\"end\":78108,\"start\":78103},{\"end\":78386,\"start\":78380},{\"end\":78398,\"start\":78395},{\"end\":78414,\"start\":78405},{\"end\":78428,\"start\":78424},{\"end\":78647,\"start\":78642},{\"end\":78663,\"start\":78658},{\"end\":78680,\"start\":78674},{\"end\":78696,\"start\":78689},{\"end\":78709,\"start\":78703},{\"end\":78722,\"start\":78715},{\"end\":78737,\"start\":78733},{\"end\":78757,\"start\":78751},{\"end\":78965,\"start\":78959},{\"end\":78979,\"start\":78975},{\"end\":78993,\"start\":78989},{\"end\":79007,\"start\":79002},{\"end\":79024,\"start\":79019},{\"end\":79037,\"start\":79032},{\"end\":79039,\"start\":79038},{\"end\":79048,\"start\":79047},{\"end\":79062,\"start\":79057},{\"end\":79130,\"start\":79129},{\"end\":79139,\"start\":79138},{\"end\":79141,\"start\":79140},{\"end\":79152,\"start\":79151},{\"end\":79162,\"start\":79161},{\"end\":79173,\"start\":79172},{\"end\":79183,\"start\":79182},{\"end\":79199,\"start\":79198},{\"end\":79321,\"start\":79320},{\"end\":79330,\"start\":79322},{\"end\":79345,\"start\":79341},{\"end\":79645,\"start\":79641},{\"end\":79661,\"start\":79655},{\"end\":79678,\"start\":79669},{\"end\":79698,\"start\":79690},{\"end\":79711,\"start\":79704},{\"end\":79728,\"start\":79722},{\"end\":79739,\"start\":79734},{\"end\":79750,\"start\":79746},{\"end\":79764,\"start\":79759},{\"end\":80017,\"start\":80015},{\"end\":80027,\"start\":80022},{\"end\":80039,\"start\":80032},{\"end\":80058,\"start\":80050},{\"end\":80181,\"start\":80174},{\"end\":80195,\"start\":80188},{\"end\":80208,\"start\":80203},{\"end\":80220,\"start\":80216},{\"end\":80237,\"start\":80229},{\"end\":80444,\"start\":80437},{\"end\":80455,\"start\":80449},{\"end\":80469,\"start\":80462},{\"end\":80485,\"start\":80480},{\"end\":80500,\"start\":80494},{\"end\":80502,\"start\":80501},{\"end\":80514,\"start\":80509},{\"end\":80742,\"start\":80735},{\"end\":80754,\"start\":80747},{\"end\":80768,\"start\":80764},{\"end\":80784,\"start\":80775},{\"end\":80786,\"start\":80785},{\"end\":80799,\"start\":80793},{\"end\":80801,\"start\":80800},{\"end\":80881,\"start\":80874},{\"end\":80893,\"start\":80889},{\"end\":80905,\"start\":80901},{\"end\":80915,\"start\":80912},{\"end\":81050,\"start\":81046},{\"end\":81058,\"start\":81055},{\"end\":81068,\"start\":81063},{\"end\":81081,\"start\":81074},{\"end\":81089,\"start\":81087},{\"end\":81424,\"start\":81418},{\"end\":81433,\"start\":81431},{\"end\":81447,\"start\":81439},{\"end\":81460,\"start\":81454},{\"end\":81473,\"start\":81468},{\"end\":81484,\"start\":81480},{\"end\":81497,\"start\":81489},{\"end\":81732,\"start\":81726},{\"end\":81746,\"start\":81739},{\"end\":81754,\"start\":81752},{\"end\":81766,\"start\":81762}]", "bib_author_last_name": "[{\"end\":67082,\"start\":67072},{\"end\":67101,\"start\":67090},{\"end\":67110,\"start\":67107},{\"end\":67129,\"start\":67120},{\"end\":67148,\"start\":67142},{\"end\":67354,\"start\":67347},{\"end\":67368,\"start\":67363},{\"end\":67382,\"start\":67376},{\"end\":67396,\"start\":67390},{\"end\":67413,\"start\":67406},{\"end\":67664,\"start\":67656},{\"end\":67681,\"start\":67673},{\"end\":67695,\"start\":67688},{\"end\":67710,\"start\":67705},{\"end\":67932,\"start\":67924},{\"end\":67945,\"start\":67943},{\"end\":67960,\"start\":67953},{\"end\":67974,\"start\":67969},{\"end\":67988,\"start\":67981},{\"end\":68003,\"start\":67998},{\"end\":68014,\"start\":68009},{\"end\":68309,\"start\":68301},{\"end\":68316,\"start\":68314},{\"end\":68330,\"start\":68325},{\"end\":68342,\"start\":68337},{\"end\":68356,\"start\":68349},{\"end\":68373,\"start\":68363},{\"end\":68383,\"start\":68379},{\"end\":68398,\"start\":68393},{\"end\":68413,\"start\":68406},{\"end\":68421,\"start\":68415},{\"end\":68639,\"start\":68634},{\"end\":68658,\"start\":68651},{\"end\":68669,\"start\":68667},{\"end\":68688,\"start\":68680},{\"end\":68701,\"start\":68696},{\"end\":68721,\"start\":68714},{\"end\":68734,\"start\":68728},{\"end\":68998,\"start\":68990},{\"end\":69010,\"start\":69005},{\"end\":69359,\"start\":69355},{\"end\":69374,\"start\":69371},{\"end\":69526,\"start\":69521},{\"end\":69538,\"start\":69535},{\"end\":69557,\"start\":69547},{\"end\":69572,\"start\":69566},{\"end\":69591,\"start\":69583},{\"end\":69606,\"start\":69601},{\"end\":69619,\"start\":69615},{\"end\":69630,\"start\":69626},{\"end\":69643,\"start\":69638},{\"end\":69843,\"start\":69837},{\"end\":69865,\"start\":69860},{\"end\":69883,\"start\":69876},{\"end\":70138,\"start\":70134},{\"end\":70150,\"start\":70146},{\"end\":70166,\"start\":70161},{\"end\":70180,\"start\":70173},{\"end\":70192,\"start\":70187},{\"end\":70447,\"start\":70443},{\"end\":70457,\"start\":70453},{\"end\":70471,\"start\":70468},{\"end\":70493,\"start\":70485},{\"end\":70508,\"start\":70503},{\"end\":70522,\"start\":70516},{\"end\":70534,\"start\":70526},{\"end\":70629,\"start\":70625},{\"end\":70642,\"start\":70635},{\"end\":70660,\"start\":70653},{\"end\":70675,\"start\":70670},{\"end\":70912,\"start\":70908},{\"end\":70926,\"start\":70922},{\"end\":70941,\"start\":70938},{\"end\":70950,\"start\":70947},{\"end\":70967,\"start\":70963},{\"end\":71209,\"start\":71206},{\"end\":71224,\"start\":71219},{\"end\":71239,\"start\":71234},{\"end\":71254,\"start\":71248},{\"end\":71273,\"start\":71263},{\"end\":71291,\"start\":71284},{\"end\":71545,\"start\":71542},{\"end\":71563,\"start\":71556},{\"end\":71727,\"start\":71724},{\"end\":71741,\"start\":71736},{\"end\":71759,\"start\":71751},{\"end\":71771,\"start\":71768},{\"end\":71784,\"start\":71778},{\"end\":71797,\"start\":71792},{\"end\":72042,\"start\":72037},{\"end\":72055,\"start\":72053},{\"end\":72069,\"start\":72064},{\"end\":72084,\"start\":72076},{\"end\":72099,\"start\":72092},{\"end\":72123,\"start\":72116},{\"end\":72148,\"start\":72132},{\"end\":72161,\"start\":72155},{\"end\":72172,\"start\":72167},{\"end\":72188,\"start\":72181},{\"end\":72249,\"start\":72243},{\"end\":72260,\"start\":72253},{\"end\":72274,\"start\":72264},{\"end\":72285,\"start\":72278},{\"end\":72301,\"start\":72289},{\"end\":72312,\"start\":72305},{\"end\":72417,\"start\":72411},{\"end\":72437,\"start\":72429},{\"end\":72457,\"start\":72448},{\"end\":72472,\"start\":72466},{\"end\":72484,\"start\":72481},{\"end\":72497,\"start\":72490},{\"end\":72782,\"start\":72777},{\"end\":72794,\"start\":72790},{\"end\":72816,\"start\":72804},{\"end\":72829,\"start\":72824},{\"end\":72842,\"start\":72836},{\"end\":73063,\"start\":73054},{\"end\":73077,\"start\":73071},{\"end\":73144,\"start\":73134},{\"end\":73164,\"start\":73153},{\"end\":73289,\"start\":73285},{\"end\":73311,\"start\":73305},{\"end\":73320,\"start\":73313},{\"end\":73560,\"start\":73556},{\"end\":73571,\"start\":73567},{\"end\":73590,\"start\":73587},{\"end\":73603,\"start\":73601},{\"end\":73617,\"start\":73614},{\"end\":73629,\"start\":73626},{\"end\":73771,\"start\":73761},{\"end\":73788,\"start\":73781},{\"end\":73801,\"start\":73795},{\"end\":73814,\"start\":73810},{\"end\":74108,\"start\":74102},{\"end\":74118,\"start\":74116},{\"end\":74279,\"start\":74274},{\"end\":74297,\"start\":74289},{\"end\":74309,\"start\":74306},{\"end\":74325,\"start\":74315},{\"end\":74337,\"start\":74332},{\"end\":74354,\"start\":74346},{\"end\":74369,\"start\":74363},{\"end\":74379,\"start\":74376},{\"end\":74394,\"start\":74389},{\"end\":74407,\"start\":74400},{\"end\":74570,\"start\":74564},{\"end\":74585,\"start\":74582},{\"end\":74736,\"start\":74733},{\"end\":74747,\"start\":74745},{\"end\":74760,\"start\":74756},{\"end\":74769,\"start\":74766},{\"end\":74784,\"start\":74780},{\"end\":74798,\"start\":74792},{\"end\":74812,\"start\":74809},{\"end\":75063,\"start\":75061},{\"end\":75071,\"start\":75068},{\"end\":75084,\"start\":75082},{\"end\":75096,\"start\":75094},{\"end\":75113,\"start\":75108},{\"end\":75124,\"start\":75119},{\"end\":75137,\"start\":75133},{\"end\":75149,\"start\":75147},{\"end\":75158,\"start\":75154},{\"end\":75168,\"start\":75165},{\"end\":75180,\"start\":75176},{\"end\":75194,\"start\":75191},{\"end\":75387,\"start\":75384},{\"end\":75561,\"start\":75557},{\"end\":75574,\"start\":75569},{\"end\":75588,\"start\":75584},{\"end\":75610,\"start\":75594},{\"end\":75626,\"start\":75622},{\"end\":75633,\"start\":75628},{\"end\":75737,\"start\":75734},{\"end\":75752,\"start\":75748},{\"end\":75773,\"start\":75760},{\"end\":75794,\"start\":75783},{\"end\":75799,\"start\":75796},{\"end\":75962,\"start\":75957},{\"end\":75973,\"start\":75969},{\"end\":75996,\"start\":75989},{\"end\":76254,\"start\":76244},{\"end\":76267,\"start\":76262},{\"end\":76474,\"start\":76466},{\"end\":76488,\"start\":76482},{\"end\":76499,\"start\":76495},{\"end\":76513,\"start\":76510},{\"end\":76884,\"start\":76874},{\"end\":76900,\"start\":76894},{\"end\":76923,\"start\":76916},{\"end\":76936,\"start\":76929},{\"end\":76944,\"start\":76940},{\"end\":76959,\"start\":76950},{\"end\":76972,\"start\":76965},{\"end\":76987,\"start\":76976},{\"end\":76999,\"start\":76991},{\"end\":77297,\"start\":77295},{\"end\":77308,\"start\":77302},{\"end\":77320,\"start\":77318},{\"end\":77339,\"start\":77333},{\"end\":77601,\"start\":77590},{\"end\":77613,\"start\":77611},{\"end\":77621,\"start\":77619},{\"end\":77640,\"start\":77634},{\"end\":77834,\"start\":77828},{\"end\":77848,\"start\":77841},{\"end\":77862,\"start\":77855},{\"end\":77877,\"start\":77874},{\"end\":77892,\"start\":77886},{\"end\":77908,\"start\":77902},{\"end\":77920,\"start\":77916},{\"end\":77928,\"start\":77926},{\"end\":77941,\"start\":77938},{\"end\":78069,\"start\":78060},{\"end\":78081,\"start\":78076},{\"end\":78101,\"start\":78094},{\"end\":78114,\"start\":78109},{\"end\":78393,\"start\":78387},{\"end\":78403,\"start\":78399},{\"end\":78422,\"start\":78415},{\"end\":78436,\"start\":78429},{\"end\":78656,\"start\":78648},{\"end\":78672,\"start\":78664},{\"end\":78687,\"start\":78681},{\"end\":78701,\"start\":78697},{\"end\":78713,\"start\":78710},{\"end\":78731,\"start\":78723},{\"end\":78749,\"start\":78738},{\"end\":78761,\"start\":78758},{\"end\":78973,\"start\":78966},{\"end\":78987,\"start\":78980},{\"end\":79000,\"start\":78994},{\"end\":79017,\"start\":79008},{\"end\":79030,\"start\":79025},{\"end\":79045,\"start\":79040},{\"end\":79055,\"start\":79049},{\"end\":79073,\"start\":79063},{\"end\":79136,\"start\":79131},{\"end\":79149,\"start\":79142},{\"end\":79159,\"start\":79153},{\"end\":79170,\"start\":79163},{\"end\":79180,\"start\":79174},{\"end\":79196,\"start\":79184},{\"end\":79207,\"start\":79200},{\"end\":79318,\"start\":79307},{\"end\":79339,\"start\":79331},{\"end\":79353,\"start\":79346},{\"end\":79361,\"start\":79355},{\"end\":79653,\"start\":79646},{\"end\":79667,\"start\":79662},{\"end\":79688,\"start\":79679},{\"end\":79702,\"start\":79699},{\"end\":79720,\"start\":79712},{\"end\":79732,\"start\":79729},{\"end\":79744,\"start\":79740},{\"end\":79757,\"start\":79751},{\"end\":79770,\"start\":79765},{\"end\":80020,\"start\":80018},{\"end\":80030,\"start\":80028},{\"end\":80048,\"start\":80040},{\"end\":80063,\"start\":80059},{\"end\":80186,\"start\":80182},{\"end\":80201,\"start\":80196},{\"end\":80214,\"start\":80209},{\"end\":80227,\"start\":80221},{\"end\":80244,\"start\":80238},{\"end\":80447,\"start\":80445},{\"end\":80460,\"start\":80456},{\"end\":80478,\"start\":80470},{\"end\":80492,\"start\":80486},{\"end\":80507,\"start\":80503},{\"end\":80520,\"start\":80515},{\"end\":80745,\"start\":80743},{\"end\":80762,\"start\":80755},{\"end\":80773,\"start\":80769},{\"end\":80791,\"start\":80787},{\"end\":80806,\"start\":80802},{\"end\":80887,\"start\":80882},{\"end\":80899,\"start\":80894},{\"end\":80910,\"start\":80906},{\"end\":80923,\"start\":80916},{\"end\":81053,\"start\":81051},{\"end\":81061,\"start\":81059},{\"end\":81072,\"start\":81069},{\"end\":81085,\"start\":81082},{\"end\":81094,\"start\":81090},{\"end\":81429,\"start\":81425},{\"end\":81437,\"start\":81434},{\"end\":81452,\"start\":81448},{\"end\":81466,\"start\":81461},{\"end\":81478,\"start\":81474},{\"end\":81487,\"start\":81485},{\"end\":81501,\"start\":81498},{\"end\":81737,\"start\":81733},{\"end\":81750,\"start\":81747},{\"end\":81760,\"start\":81755},{\"end\":81769,\"start\":81767}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":221378802},\"end\":67279,\"start\":66974},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":189762081},\"end\":67592,\"start\":67281},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":11933981},\"end\":67831,\"start\":67594},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3753452},\"end\":68184,\"start\":67833},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4673790},\"end\":68590,\"start\":68186},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3180429},\"end\":68884,\"start\":68592},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7164502},\"end\":69290,\"start\":68886},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":208232005},\"end\":69513,\"start\":69292},{\"attributes\":{\"id\":\"b8\"},\"end\":69757,\"start\":69515},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":209414687},\"end\":70033,\"start\":69759},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":54078068},\"end\":70361,\"start\":70035},{\"attributes\":{\"doi\":\"CoRR, abs/1504.00325\",\"id\":\"b11\"},\"end\":70560,\"start\":70363},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":227305513},\"end\":70853,\"start\":70562},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":210163619},\"end\":71134,\"start\":70855},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7684883},\"end\":71461,\"start\":71136},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4408791},\"end\":71684,\"start\":71463},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":35985986},\"end\":71968,\"start\":71686},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":46979001},\"end\":72342,\"start\":71970},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4670339},\"end\":72670,\"start\":72344},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8081284},\"end\":73011,\"start\":72672},{\"attributes\":{\"doi\":\"CoRR, abs/1606.08415\",\"id\":\"b20\"},\"end\":73103,\"start\":73013},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1915014},\"end\":73192,\"start\":73105},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":152282269},\"end\":73489,\"start\":73194},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":190638738},\"end\":73685,\"start\":73491},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6308361},\"end\":74047,\"start\":73687},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6628106},\"end\":74267,\"start\":74049},{\"attributes\":{\"id\":\"b26\"},\"end\":74468,\"start\":74269},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":221738974},\"end\":74652,\"start\":74470},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":231880022},\"end\":74981,\"start\":74654},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":215754208},\"end\":75317,\"start\":74983},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":964287},\"end\":75467,\"start\":75319},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":235078835},\"end\":75881,\"start\":75469},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1998416},\"end\":76139,\"start\":75883},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3158329},\"end\":76392,\"start\":76141},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":11080756},\"end\":76764,\"start\":76394},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6941275},\"end\":77224,\"start\":76766},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":127956465},\"end\":77500,\"start\":77226},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1745976},\"end\":77737,\"start\":77502},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":204838007},\"end\":77990,\"start\":77739},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11816014},\"end\":78279,\"start\":77992},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":51876975},\"end\":78561,\"start\":78281},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":208617407},\"end\":78930,\"start\":78563},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":13756489},\"end\":79252,\"start\":78932},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":9026666},\"end\":79551,\"start\":79254},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":102352810},\"end\":79941,\"start\":79553},{\"attributes\":{\"doi\":\"arXiv:1801.02209\",\"id\":\"b45\"},\"end\":80099,\"start\":79943},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":227238996},\"end\":80393,\"start\":80101},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":104291855},\"end\":80690,\"start\":80395},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1688357},\"end\":80978,\"start\":80692},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":195657908},\"end\":81278,\"start\":80980},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":232092539},\"end\":81650,\"start\":81280},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":244127479},\"end\":81918,\"start\":81652}]", "bib_title": "[{\"end\":67064,\"start\":66974},{\"end\":67339,\"start\":67281},{\"end\":67648,\"start\":67594},{\"end\":67916,\"start\":67833},{\"end\":68293,\"start\":68186},{\"end\":68622,\"start\":68592},{\"end\":68978,\"start\":68886},{\"end\":69343,\"start\":69292},{\"end\":69830,\"start\":69759},{\"end\":70125,\"start\":70035},{\"end\":70616,\"start\":70562},{\"end\":70897,\"start\":70855},{\"end\":71197,\"start\":71136},{\"end\":71533,\"start\":71463},{\"end\":71713,\"start\":71686},{\"end\":72028,\"start\":71970},{\"end\":72402,\"start\":72344},{\"end\":72770,\"start\":72672},{\"end\":73127,\"start\":73105},{\"end\":73281,\"start\":73194},{\"end\":73546,\"start\":73491},{\"end\":73753,\"start\":73687},{\"end\":74091,\"start\":74049},{\"end\":74555,\"start\":74470},{\"end\":74727,\"start\":74654},{\"end\":75052,\"start\":74983},{\"end\":75373,\"start\":75319},{\"end\":75548,\"start\":75469},{\"end\":75949,\"start\":75883},{\"end\":76234,\"start\":76141},{\"end\":76456,\"start\":76394},{\"end\":76864,\"start\":76766},{\"end\":77283,\"start\":77226},{\"end\":77580,\"start\":77502},{\"end\":77820,\"start\":77739},{\"end\":78051,\"start\":77992},{\"end\":78378,\"start\":78281},{\"end\":78640,\"start\":78563},{\"end\":78957,\"start\":78932},{\"end\":79305,\"start\":79254},{\"end\":79639,\"start\":79553},{\"end\":80172,\"start\":80101},{\"end\":80435,\"start\":80395},{\"end\":80733,\"start\":80692},{\"end\":81044,\"start\":80980},{\"end\":81416,\"start\":81280},{\"end\":81724,\"start\":81652}]", "bib_author": "[{\"end\":67084,\"start\":67066},{\"end\":67103,\"start\":67084},{\"end\":67112,\"start\":67103},{\"end\":67131,\"start\":67112},{\"end\":67150,\"start\":67131},{\"end\":67356,\"start\":67341},{\"end\":67370,\"start\":67356},{\"end\":67384,\"start\":67370},{\"end\":67398,\"start\":67384},{\"end\":67415,\"start\":67398},{\"end\":67666,\"start\":67650},{\"end\":67683,\"start\":67666},{\"end\":67697,\"start\":67683},{\"end\":67712,\"start\":67697},{\"end\":67934,\"start\":67918},{\"end\":67947,\"start\":67934},{\"end\":67962,\"start\":67947},{\"end\":67976,\"start\":67962},{\"end\":67990,\"start\":67976},{\"end\":68005,\"start\":67990},{\"end\":68016,\"start\":68005},{\"end\":68311,\"start\":68295},{\"end\":68318,\"start\":68311},{\"end\":68332,\"start\":68318},{\"end\":68344,\"start\":68332},{\"end\":68358,\"start\":68344},{\"end\":68375,\"start\":68358},{\"end\":68385,\"start\":68375},{\"end\":68400,\"start\":68385},{\"end\":68415,\"start\":68400},{\"end\":68423,\"start\":68415},{\"end\":68641,\"start\":68624},{\"end\":68660,\"start\":68641},{\"end\":68671,\"start\":68660},{\"end\":68690,\"start\":68671},{\"end\":68703,\"start\":68690},{\"end\":68723,\"start\":68703},{\"end\":68736,\"start\":68723},{\"end\":69000,\"start\":68980},{\"end\":69012,\"start\":69000},{\"end\":69361,\"start\":69345},{\"end\":69376,\"start\":69361},{\"end\":69528,\"start\":69515},{\"end\":69540,\"start\":69528},{\"end\":69559,\"start\":69540},{\"end\":69574,\"start\":69559},{\"end\":69593,\"start\":69574},{\"end\":69608,\"start\":69593},{\"end\":69621,\"start\":69608},{\"end\":69632,\"start\":69621},{\"end\":69645,\"start\":69632},{\"end\":69845,\"start\":69832},{\"end\":69852,\"start\":69845},{\"end\":69867,\"start\":69852},{\"end\":69885,\"start\":69867},{\"end\":70140,\"start\":70127},{\"end\":70152,\"start\":70140},{\"end\":70168,\"start\":70152},{\"end\":70182,\"start\":70168},{\"end\":70194,\"start\":70182},{\"end\":70449,\"start\":70436},{\"end\":70459,\"start\":70449},{\"end\":70473,\"start\":70459},{\"end\":70495,\"start\":70473},{\"end\":70510,\"start\":70495},{\"end\":70524,\"start\":70510},{\"end\":70536,\"start\":70524},{\"end\":70631,\"start\":70618},{\"end\":70644,\"start\":70631},{\"end\":70662,\"start\":70644},{\"end\":70677,\"start\":70662},{\"end\":70914,\"start\":70899},{\"end\":70928,\"start\":70914},{\"end\":70943,\"start\":70928},{\"end\":70952,\"start\":70943},{\"end\":70969,\"start\":70952},{\"end\":71211,\"start\":71199},{\"end\":71226,\"start\":71211},{\"end\":71241,\"start\":71226},{\"end\":71256,\"start\":71241},{\"end\":71275,\"start\":71256},{\"end\":71293,\"start\":71275},{\"end\":71547,\"start\":71535},{\"end\":71565,\"start\":71547},{\"end\":71729,\"start\":71715},{\"end\":71743,\"start\":71729},{\"end\":71761,\"start\":71743},{\"end\":71773,\"start\":71761},{\"end\":71786,\"start\":71773},{\"end\":71799,\"start\":71786},{\"end\":72044,\"start\":72030},{\"end\":72057,\"start\":72044},{\"end\":72071,\"start\":72057},{\"end\":72086,\"start\":72071},{\"end\":72101,\"start\":72086},{\"end\":72125,\"start\":72101},{\"end\":72150,\"start\":72125},{\"end\":72163,\"start\":72150},{\"end\":72174,\"start\":72163},{\"end\":72190,\"start\":72174},{\"end\":72419,\"start\":72404},{\"end\":72439,\"start\":72419},{\"end\":72459,\"start\":72439},{\"end\":72474,\"start\":72459},{\"end\":72486,\"start\":72474},{\"end\":72499,\"start\":72486},{\"end\":72784,\"start\":72772},{\"end\":72796,\"start\":72784},{\"end\":72818,\"start\":72796},{\"end\":72831,\"start\":72818},{\"end\":72844,\"start\":72831},{\"end\":73065,\"start\":73050},{\"end\":73079,\"start\":73065},{\"end\":73146,\"start\":73129},{\"end\":73166,\"start\":73146},{\"end\":73291,\"start\":73283},{\"end\":73313,\"start\":73291},{\"end\":73322,\"start\":73313},{\"end\":73562,\"start\":73548},{\"end\":73573,\"start\":73562},{\"end\":73592,\"start\":73573},{\"end\":73605,\"start\":73592},{\"end\":73619,\"start\":73605},{\"end\":73631,\"start\":73619},{\"end\":73773,\"start\":73755},{\"end\":73790,\"start\":73773},{\"end\":73803,\"start\":73790},{\"end\":73816,\"start\":73803},{\"end\":74110,\"start\":74093},{\"end\":74120,\"start\":74110},{\"end\":74281,\"start\":74269},{\"end\":74299,\"start\":74281},{\"end\":74311,\"start\":74299},{\"end\":74327,\"start\":74311},{\"end\":74339,\"start\":74327},{\"end\":74356,\"start\":74339},{\"end\":74371,\"start\":74356},{\"end\":74381,\"start\":74371},{\"end\":74396,\"start\":74381},{\"end\":74409,\"start\":74396},{\"end\":74572,\"start\":74557},{\"end\":74587,\"start\":74572},{\"end\":74738,\"start\":74729},{\"end\":74749,\"start\":74738},{\"end\":74762,\"start\":74749},{\"end\":74771,\"start\":74762},{\"end\":74786,\"start\":74771},{\"end\":74800,\"start\":74786},{\"end\":74814,\"start\":74800},{\"end\":75065,\"start\":75054},{\"end\":75073,\"start\":75065},{\"end\":75086,\"start\":75073},{\"end\":75098,\"start\":75086},{\"end\":75115,\"start\":75098},{\"end\":75126,\"start\":75115},{\"end\":75139,\"start\":75126},{\"end\":75151,\"start\":75139},{\"end\":75160,\"start\":75151},{\"end\":75170,\"start\":75160},{\"end\":75182,\"start\":75170},{\"end\":75196,\"start\":75182},{\"end\":75389,\"start\":75375},{\"end\":75563,\"start\":75550},{\"end\":75576,\"start\":75563},{\"end\":75590,\"start\":75576},{\"end\":75612,\"start\":75590},{\"end\":75628,\"start\":75612},{\"end\":75635,\"start\":75628},{\"end\":75964,\"start\":75951},{\"end\":75975,\"start\":75964},{\"end\":75998,\"start\":75975},{\"end\":76256,\"start\":76236},{\"end\":76269,\"start\":76256},{\"end\":76476,\"start\":76458},{\"end\":76490,\"start\":76476},{\"end\":76501,\"start\":76490},{\"end\":76515,\"start\":76501},{\"end\":76886,\"start\":76866},{\"end\":76902,\"start\":76886},{\"end\":76925,\"start\":76902},{\"end\":76938,\"start\":76925},{\"end\":76946,\"start\":76938},{\"end\":76961,\"start\":76946},{\"end\":76974,\"start\":76961},{\"end\":76989,\"start\":76974},{\"end\":77001,\"start\":76989},{\"end\":77299,\"start\":77285},{\"end\":77310,\"start\":77299},{\"end\":77322,\"start\":77310},{\"end\":77341,\"start\":77322},{\"end\":77603,\"start\":77582},{\"end\":77608,\"start\":77603},{\"end\":77615,\"start\":77608},{\"end\":77623,\"start\":77615},{\"end\":77642,\"start\":77623},{\"end\":77836,\"start\":77822},{\"end\":77850,\"start\":77836},{\"end\":77864,\"start\":77850},{\"end\":77879,\"start\":77864},{\"end\":77894,\"start\":77879},{\"end\":77910,\"start\":77894},{\"end\":77922,\"start\":77910},{\"end\":77930,\"start\":77922},{\"end\":77943,\"start\":77930},{\"end\":78071,\"start\":78053},{\"end\":78083,\"start\":78071},{\"end\":78103,\"start\":78083},{\"end\":78116,\"start\":78103},{\"end\":78395,\"start\":78380},{\"end\":78405,\"start\":78395},{\"end\":78424,\"start\":78405},{\"end\":78438,\"start\":78424},{\"end\":78658,\"start\":78642},{\"end\":78674,\"start\":78658},{\"end\":78689,\"start\":78674},{\"end\":78703,\"start\":78689},{\"end\":78715,\"start\":78703},{\"end\":78733,\"start\":78715},{\"end\":78751,\"start\":78733},{\"end\":78763,\"start\":78751},{\"end\":78975,\"start\":78959},{\"end\":78989,\"start\":78975},{\"end\":79002,\"start\":78989},{\"end\":79019,\"start\":79002},{\"end\":79032,\"start\":79019},{\"end\":79047,\"start\":79032},{\"end\":79057,\"start\":79047},{\"end\":79075,\"start\":79057},{\"end\":79320,\"start\":79307},{\"end\":79341,\"start\":79320},{\"end\":79355,\"start\":79341},{\"end\":79363,\"start\":79355},{\"end\":79655,\"start\":79641},{\"end\":79669,\"start\":79655},{\"end\":79690,\"start\":79669},{\"end\":79704,\"start\":79690},{\"end\":79722,\"start\":79704},{\"end\":79734,\"start\":79722},{\"end\":79746,\"start\":79734},{\"end\":79759,\"start\":79746},{\"end\":79772,\"start\":79759},{\"end\":80022,\"start\":80015},{\"end\":80032,\"start\":80022},{\"end\":80050,\"start\":80032},{\"end\":80065,\"start\":80050},{\"end\":80188,\"start\":80174},{\"end\":80203,\"start\":80188},{\"end\":80216,\"start\":80203},{\"end\":80229,\"start\":80216},{\"end\":80246,\"start\":80229},{\"end\":80449,\"start\":80437},{\"end\":80462,\"start\":80449},{\"end\":80480,\"start\":80462},{\"end\":80494,\"start\":80480},{\"end\":80509,\"start\":80494},{\"end\":80522,\"start\":80509},{\"end\":80747,\"start\":80735},{\"end\":80764,\"start\":80747},{\"end\":80775,\"start\":80764},{\"end\":80793,\"start\":80775},{\"end\":80808,\"start\":80793},{\"end\":81055,\"start\":81046},{\"end\":81063,\"start\":81055},{\"end\":81074,\"start\":81063},{\"end\":81087,\"start\":81074},{\"end\":81096,\"start\":81087},{\"end\":81431,\"start\":81418},{\"end\":81439,\"start\":81431},{\"end\":81454,\"start\":81439},{\"end\":81468,\"start\":81454},{\"end\":81480,\"start\":81468},{\"end\":81489,\"start\":81480},{\"end\":81503,\"start\":81489},{\"end\":81739,\"start\":81726},{\"end\":81752,\"start\":81739},{\"end\":81762,\"start\":81752},{\"end\":81771,\"start\":81762}]", "bib_venue": "[{\"end\":67275,\"start\":67221},{\"end\":67588,\"start\":67510},{\"end\":67827,\"start\":67778},{\"end\":68179,\"start\":68106},{\"end\":68586,\"start\":68513},{\"end\":68879,\"start\":68816},{\"end\":69245,\"start\":69137},{\"end\":69509,\"start\":69451},{\"end\":70000,\"start\":69951},{\"end\":70357,\"start\":70284},{\"end\":70840,\"start\":70767},{\"end\":71130,\"start\":71058},{\"end\":71456,\"start\":71383},{\"end\":71680,\"start\":71631},{\"end\":71962,\"start\":71889},{\"end\":72662,\"start\":72589},{\"end\":72999,\"start\":72930},{\"end\":73485,\"start\":73412},{\"end\":74002,\"start\":73912},{\"end\":74263,\"start\":74200},{\"end\":74977,\"start\":74904},{\"end\":75311,\"start\":75262},{\"end\":75877,\"start\":75801},{\"end\":76135,\"start\":76075},{\"end\":76388,\"start\":76337},{\"end\":76719,\"start\":76610},{\"end\":77144,\"start\":77081},{\"end\":77484,\"start\":77421},{\"end\":77733,\"start\":77696},{\"end\":78275,\"start\":78204},{\"end\":78557,\"start\":78506},{\"end\":78926,\"start\":78853},{\"end\":79246,\"start\":79209},{\"end\":79526,\"start\":79453},{\"end\":79935,\"start\":79862},{\"end\":80389,\"start\":80326},{\"end\":80685,\"start\":80612},{\"end\":80974,\"start\":80925},{\"end\":81259,\"start\":81186},{\"end\":81646,\"start\":81583},{\"end\":81914,\"start\":81851},{\"end\":67219,\"start\":67150},{\"end\":67508,\"start\":67415},{\"end\":67776,\"start\":67712},{\"end\":68104,\"start\":68016},{\"end\":68511,\"start\":68423},{\"end\":68814,\"start\":68736},{\"end\":69135,\"start\":69012},{\"end\":69449,\"start\":69376},{\"end\":69751,\"start\":69645},{\"end\":69949,\"start\":69885},{\"end\":70282,\"start\":70194},{\"end\":70434,\"start\":70363},{\"end\":70765,\"start\":70677},{\"end\":71056,\"start\":70969},{\"end\":71381,\"start\":71293},{\"end\":71629,\"start\":71565},{\"end\":71887,\"start\":71799},{\"end\":72239,\"start\":72190},{\"end\":72587,\"start\":72499},{\"end\":72928,\"start\":72844},{\"end\":73048,\"start\":73013},{\"end\":73184,\"start\":73166},{\"end\":73410,\"start\":73322},{\"end\":73678,\"start\":73631},{\"end\":73910,\"start\":73816},{\"end\":74198,\"start\":74120},{\"end\":74462,\"start\":74409},{\"end\":74646,\"start\":74587},{\"end\":74902,\"start\":74814},{\"end\":75260,\"start\":75196},{\"end\":75420,\"start\":75389},{\"end\":75726,\"start\":75635},{\"end\":76073,\"start\":75998},{\"end\":76335,\"start\":76269},{\"end\":76608,\"start\":76515},{\"end\":77079,\"start\":77001},{\"end\":77419,\"start\":77341},{\"end\":77694,\"start\":77642},{\"end\":77979,\"start\":77943},{\"end\":78202,\"start\":78116},{\"end\":78504,\"start\":78438},{\"end\":78851,\"start\":78763},{\"end\":79127,\"start\":79075},{\"end\":79451,\"start\":79363},{\"end\":79860,\"start\":79772},{\"end\":80013,\"start\":79943},{\"end\":80324,\"start\":80246},{\"end\":80610,\"start\":80522},{\"end\":80872,\"start\":80808},{\"end\":81184,\"start\":81096},{\"end\":81581,\"start\":81503},{\"end\":81849,\"start\":81771}]"}}}, "year": 2023, "month": 12, "day": 17}