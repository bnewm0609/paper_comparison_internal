{"id": 219633501, "updated": "2022-01-24 09:37:02.886", "metadata": {"title": "Structure-Guided Ranking Loss for Single Image Depth Prediction", "authors": "[{\"middle\":[],\"last\":\"Xian\",\"first\":\"Ke\"},{\"middle\":[],\"last\":\"Zhang\",\"first\":\"Jianming\"},{\"middle\":[],\"last\":\"Wang\",\"first\":\"Oliver\"},{\"middle\":[],\"last\":\"Mai\",\"first\":\"Long\"},{\"middle\":[],\"last\":\"Lin\",\"first\":\"Zhe\"},{\"middle\":[],\"last\":\"Cao\",\"first\":\"Zhiguo\"}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Single image depth prediction is a challenging task due to its ill-posed nature and challenges with capturing ground truth for supervision. Large-scale disparity data generated from stereo photos and 3D videos is a promising source of supervision, however, such disparity data can only approximate the inverse ground truth depth up to an affine transformation. To more effectively learn from such pseudo-depth data, we propose to use a simple pair-wise ranking loss with a novel sampling strategy. Instead of randomly sampling point pairs, we guide the sampling to better characterize structure of important regions based on the low-level edge maps and high-level object instance masks. We show that the pair-wise ranking loss, combined with our structure-guided sampling strategies, can significantly improve the quality of depth map prediction. In addition, we introduce a new relative depth dataset of about 21K diverse high-resolution web stereo photos to enhance the generalization ability of our model. In experiments, we conduct cross-dataset evaluation on six benchmark datasets and show that our method consistently improves over the baselines, leading to superior quantitative and qualitative results.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3035563424", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/XianZWMLC20", "doi": "10.1109/cvpr42600.2020.00069"}}, "content": {"source": {"pdf_hash": "86c038d9a00a35a3b6548a8cbf078cddb49841c1", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0eaddd2a204fa499769c5c9a7e551d00fb07d065", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/86c038d9a00a35a3b6548a8cbf078cddb49841c1.txt", "contents": "\nStructure-Guided Ranking Loss for Single Image Depth Prediction\n\n\nKe Xian \nSchool of Artificial Intelligence and Automation\nNational Key Laboratory of Science and Technology on MultiSpectral Information Processing\nHuazhong University of Science and Technology\nChina\n\nJianming Zhang \nAdobe Research\n\n\nOliver Wang \nAdobe Research\n\n\nLong Mai \nAdobe Research\n\n\nZhe Lin zlin@adobe.com \nAdobe Research\n\n\nZhiguo Cao zgcao@hust.edu.cn \nSchool of Artificial Intelligence and Automation\nNational Key Laboratory of Science and Technology on MultiSpectral Information Processing\nHuazhong University of Science and Technology\nChina\n\nStructure-Guided Ranking Loss for Single Image Depth Prediction\n/KexianHust/Structure-Guided-Ranking-Loss * Corresponding author.\nRGB Y3D [4] MC [21]Aff-Inv Loss[19]Our Loss GTFigure 1. Qualitative results from state-of-the-art models Y3D [4]  and MC[21], our baseline model trained using the affine-invariant loss proposed by[19]and the same model trained using our proposed structure-guided ranking loss. The model trained using our loss provides more details of local depth structure and higher accuracy at depth boundaries. The test image is from Ibims [16], which is not used in the training for any of the above models.AbstractSingle image depth prediction is a challenging task due to its ill-posed nature and challenges with capturing ground truth for supervision. Large-scale disparity data generated from stereo photos and 3D videos is a promising source of supervision, however, such disparity data can only approximate the inverse ground truth depth up to an affine transformation. To more effectively learn from such pseudo-depth data, we propose to use a simple pair-wise ranking loss with a novel sampling strategy. Instead of randomly sampling point pairs, we guide the sampling to better characterize structure of important regions based on the low-level edge maps and high-level object instance masks. We show that the pair-wise ranking loss, combined with our structureguided sampling strategies, can significantly improve the quality of depth map prediction. In addition, we introduce a new relative depth dataset of about 21K diverse highresolution web stereo photos to enhance the generalization ability of our model. In experiments, we conduct crossdataset evaluation on six benchmark datasets and show that our method consistently improves over the baselines, leading to superior quantitative and qualitative results.\n\nIntroduction\n\nMonocular depth prediction (monodepth) is a fundamental task in computer vision, and can be used in many realworld applications. Due to its ill-posed nature, monodepth critically relies on scene semantics and thus requires a diverse set of training data to ensure its generalization ability to unseen content. Traditional depth supervision datasets are captured by either active or passive depth sensors, and as such are generally restricted to a single domain or scene type, e.g., road [11] or indoor [29]. To improve data diversity, recent monodepth works [36] have used large-scale disparity data generated from web stereo photos and 3D movies [19,33]. However, both stereo photos and 3D movies may have been post-edited to optimize for viewing experience. For example, a common technique for changing the stereo viewing experience is through the positioning the stereo window by adjusting the virtual baseline and minimum disparity 1 . Therefore, their disparity data only approximate the inverse ground truth depth up to an affine transformation. Although affine-invariant losses have been proposed to address this issue [10,19,33], we find that using such loss function often leads to sub-optimal results with blurry depth boundaries and missing details (see Fig. 1).\n\nTo effectively learn from such pseudo-depth data, we revisit the pair-wise ranking loss used in [3,36]. The ranking loss only depends on the depth ordinal relationship (e.g. point A is in front of point B, or vice versa), and thus is applicable to pseudo-depth data from various sources [3,10,21,22,36]. Compared to pixel-wise regression losses, ranking losses penalize the wrong pairwise ordinal prediction between a sparse set of pixel pairs. We observe that how point pairs are sampled can have a big impact on model's performance.\n\nThe sampling space of the point pairs is large, but only a small set of point pairs contains important constraints to characterize the salient structure of the depth map, e.g. the location of depth boundaries. Therefore, the random sampling scheme, employed in prior work [3,36], spends a lot of training computation on uninformative point pairs. In addition, depending on the application, accuracy in certain regions can be of significantly higher importance. For example, one prominent source of errors in depth maps, is inconsistent depth prediction in salient object instances like human. When part of a human is \"cut-off\" in the depth map, striking visual artifacts will appear in downstream applications, such as shallow DoF rendering and view synthesis.\n\nMotivated by these observations, we propose structureguided ranking loss which employs two carefully crafted sampling strategies: Edge-Guided Sampling and Instance-Guided Sampling. Edge-guided sampling focuses on point pairs that characterize the location of the depth boundaries and suppress false depth boundaries caused by strong image edges. Instance-guided sampling is intended for improving depth structural accuracy regarding salient object instances. We show that this structure-guided ranking loss can produce higher quality depth maps than baseline losses. Based on the proposed ranking loss, we train our model on a newly collected large-scale web stereo photo dataset of about 21K diverse photos. Our model achieves superior cross-dataset generalization performance on six benchmark datasets compared to state-of-the-art methods and baselines, showing the effectiveness of the structure-guided ranking loss on stereo depth data.\n\nOur main contributions are summarized as follows.\n\n\u2022 We propose a structure-guided ranking loss formulation with two novel sampling strategies for monocular depth prediction. \u2022 We introduce a large relative depth dataset of about 21K high resolution web stereo photos. \u2022 With our proposed loss and the new dataset, our model achieves state-of-the-art cross-dataset generalization performance.\n\n\nRelated Work\n\nMonodepth methods Traditional monodepth methods rely on direct supervision [15,23,27] mainly through handcrafted features, to learn 3D priors from images. In recent years, supervised deep learning models [2,5,6,7,18,20,24,25,37,38,39] have achieved state-of-the-art performance in the task of monocular metric depth prediction. These methods, trained on RGB-D datasets, learn a mapping function from RGB to depth. Despite the fact that these models can predict accurate depth when testing on the same or similar datasets, they cannot be easily generalized to novel scenes. In addition to these supervised methods, unsupervised or semi-supervised algorithms have also been studied. The key idea behind these methods [9,12,17,35,40] is the image reconstruction loss for view synthesis, requiring calibrated stereo pairs or video sequences for training. However, these models share the same issue with supervised deep learning based methods. In other words, the model cannot be generalized to new datasets. To address this issue, multiple in-the-wild (-i.e., contains both indoor and outdoor scenes) RGB-D datasets [3,4,19,21,34,22,33,36] have been proposed.\n\nMonodepth losses Since the depth of these datasets is ambiguous in scale axis, directly using \u2113 1 or \u2113 2 loss functions cannot train a model correctly. There are two alternatives for addressing that problem. One solution is to design a scale-invariant loss [6,19,21,22,33]. The other way is to design a suitable ranking loss [3,4,36], which can be used for training regardless of depth scale. The scale-invariant loss, paying equal attention to each pixel in an image, is prone to generate blurry predictions with details missing. By contrast, the ranking loss computes losses on the selected point pairs, which has potential to generate consistent predictions with sharp depth discontinuities. However, previous ranking losses only computes their loss on pre-defined or randomly sampled pairs of points. This leads to many irrelevant pairs being selected for ranking, which may not be useful for training procedures. We propose a structureguided ranking loss for training a monodepth model. Rather than pre-defining or random sampling candidate pairs for ranking, we instead perform online sampling guided by the overall structure of the scnee including edges and object instance masks.\n\nDepth datasets Existing RGB-D datasets mainly come from three sources: depth sensors, synthetic data, and Internet images. Depth sensors (e.g., Kinect and laser scanner) are commonly used to acquire accurate metric depth. However, are limited to indoor scenes [29,30], or sparse reconstructions [27,31]. Recently, other active sensors have been used to capture ground truth depth [16,28,32], however due to capture restrictions, these datasets consist of mostly rigid objects. All of the above datasets are limited in terms of diversity, and do not generalize to images in-the-wild as one might capture with their mobile phone. Another source to acquire RGB and depth pairs is synthetic data, e.g. [1,8,26]. These datasets are free of noise, and can have accurate metric depth with sharp discontinuities, however, there exists a domain gap between synthetic and real data, which necessitates domain adaptation for real world applications.\n\nIn order to explore the diversity of the visual world, more and more attention has been paid to the source of Internet images/videos, e.g. by; annotated monocular images [3], image collections or video for multi-view reconstructions [4,21,22], stereo images [36], or stereo videos [19,33]. Most similar to ours, ReDWeb generates dense relative depth maps from stereo images via computing stereo disparity, but only has 3,600 low-resolution images for training. In contrast to the above datasets, ours consists of a large quantity (20K) of high-resolution, diverse, training images.\n\n\nStructure-Guided Ranking Loss for Monocular Depth Prediction\n\nGiven an RGB image I, we hope to learn a function P = F(I), which generates a single channel depth map P . We propose to train a model from web stereo images, where only derived disparity maps are present for supervision. These disparity maps have an unknown shift and scale factor to relate to (inverse) depth. For the sake of convenience, in the following, we use depth to refer to inverse depth unless mentioned otherwise.\n\nDIW [3] proposes a pair-wise ranking loss to train models on this type of pseudo-depth data. The loss is defined on a sparse set of point pairs with ordinal annotations. Specifically, for a pair of points with predicted depth values [p 0 , p 1 ], the pair-wise ranking loss is\n\u03c6(p 0 \u2212 p 1 ) = log(1 + exp(\u2212\u2113(p 0 \u2212 p 1 ))), \u2113 = 0 (p 0 \u2212 p 1 ) 2 , \u2113 = 0,(1)\nwhere \u2113 is the ground truth ordinal label, which can be induced by a ground truth pseudo-depth map:\n\u2113 = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 +1, p * 0 /p * 1 >= 1 + \u03c4, \u22121, p * 0 /p * 1 <= 1 1+\u03c4 , 0, otherwise.(2)\nHere \u03c4 is a tolerance threshold, which is set to 0.03 in our experiments, and p * i denotes the ground-truth pseudo-depth value. When the pair of point are close in the depth space, i.e., \u2113 i = 0, the loss encourages the predicted p 0 and p 1 to be the same; otherwise, the difference between p 0 and p 1 (a) Halo artifacts (b) Edge-guided (c) Instance-guided \nL rank (P) = 1 N i \u03c6(p i,0 \u2212 p i,1 ).(3)\nThis type of pair-wise ranking losses is very general and thus can be applied to various types of depth and pseudodepth data. However, how the specific point pairs are sampled can have a big impact on the reconstruction quality. Instead of using random sampling [3,36], we propose a segment-guided sampling strategy based on the combination of 1) image edges and 2) instance segmentation masks. The goal is to focus the network's attention on the regions that we specifically care about, i.e. the salient depth structures of the scene.\n\n\nEdge-Guided Sampling\n\nIn general, depth maps follow a strong piece-wise smooth prior. In other words, the depth values change smoothly in most of the regions, except at sharp depth discontinuity that occur in a small portion of the image. Ultimately, getting the correct depth at these discontinuities is critical for most downstream applications. As a result, randomly sampled point pairs waste most of their supervision on unimportant relationships, and depth prediction computed with this strategy often looks blurry and lacks detail.\n\nHow can we predict where such depth discontinuities lie? One solution is to concentrate on regions where there are images edges, as most object boundaries exhibit image edges as well. Equally important to successfully predicting depth boundaries at image edges, is not predicting depth boundaries at texture edges, e.g. strong image edges that have no depth change. Again, randomly sampled point pairs are often not helpful in that regard. Therefore, we propose to handle both of these cases by simply sampling points around image edges as much as possible.\n\nOne way to do this is to sample local point pairs that reside across an image edge (see Fig. 2 (a)). We find that such sampling has a side effect of over-sharpening the depth Add (a, b), (b, c) and (c, d) to S 5: end for 6: Return point pair set S boundaries, leading to halo artifacts along the depth boundaries. Therefore, we propose a 4-point sampling scheme to enforce the smoothness on each side of a depth boundary (see Fig. 2 (b)). The 4 points lie on an orthogonal line crossing a sampled edge point. Within a small distance range of the edge point, we random sample two points on each side, resulting in three pairs of points ( Fig. 2 (b)) for our ranking loss.\n[(a, b), (b, c), (c, d)] in\nGiven an image, we convert it to an gray-scale image and use the Sobel operator to get the gradient maps G x and G y , and the gradient magnitude map G. Then we compute an edge map E by thresholding the gradient magnitude map.\nE = I[G \u2265 \u03b1 \u00b7 max(G)],(4)\nwhere \u03b1 is a threshold to control the density of E. For each edge point e = (x, y) sampled from E, we sample 4 points\n[(x k , y k ), k = a, b, c, d] by x k = x + \u03b4 k G x (e)/G(e) y k = y + \u03b4 k G y (e)/G(e).(5)\nWe have \u03b4 a < \u03b4 b < 0 < \u03b4 c < \u03b4 d , and they are sampled within a small distance range \u03b2 from the edge point e. In experiments, the \u03b1 and \u03b2 are set to 0.1 and 30, respectively. To avoid sampling points too near to the edge point e, where the ground truth depth value can be hard to define, we also set a two-pixel margin on each side of the edge. The whole sampling process is summarized in Alg. 1.\n\n\nInstance-Guided Sampling\n\nThe above approach improves depth predictions around image edges, however, often times such low level cues miss important boundaries, and end up bisecting salient objects, e.g, humans. This almost always leads to strong visual artifacts (e.g. a person's head being \"cut off\") in downstream applications such as view synthesis and shallow DoF rendering. A pure low-level edge-based sampling will still undersample these critical regions. Therefore, we propose an instance-guided sampling strategy to make the ranking loss more sensitive to such salient depth structures.\n\nRather than leveraging an edge map, we rely on instance segmentation masks as predicted by a network trained on manual segmentation annotations. Similar to edges, we use a 4-point scheme to sample three pairs of points to characterize the depth structure of the object (see Fig. 2 (c)). Specifically, we randomly sample a pair of points outside and inside of mask respectively ((a, b) and (c, d)), and use one point of each pair to form a cross- boundary pair ((b, c)).\n\nWe use segmentations generated by Mask R-CNN [13] for this sampling strategy. A common issue of such instance masks is that they also often miss small parts of the object, such as human's head, arms, etc. Once those small object parts are not included in the mask, they have much smaller chance to be sampled. Thus, we add an additional dilation operation to expand the instance mask from Mask R-CNN.\n\n\nModel Training\n\nPoint pair sampling Edge-and instance-guided sampling can greatly enhance the local details of the depth prediction, but we also find that they are not very effective in preserving global structures, such as ground planes, walls, etc. Therefore, we combine edge-guided sampling, instance-guided sampling and random sampling to produce the final point pairs for the ranking loss. For edge-guided sampling, we enumerate through all N edge pixels and use the 4-point scheme to sample three pairs, resulting in 3N point pairs. Then we sample N random pairs to augment the sample set. Note that, N is image dependent constant because different images have different numbers of intensity edges. For instance-guided sampling, the number of sampled pairs per instance is proportional to the area M of the mask, namely 3M based on the 4-point scheme.\n\nLosses To enforce smooth gradients and sharp discontinuities, we follow prior work [22] and add a multi-scale scaleinvariant gradient matching loss in inverse depth space. We denote R i = p i \u2212 p * i and define the loss as:\nL grad = 1 M s i (|\u2207 x R i s | + |\u2207 y R i s |),(6)\nwhere M is the number of pixels with valid ground truth, and R s represents the difference of disparity maps at scale s. Following [22], we use four scales in our experiments.\n\nTo obtain consistent depth with sharp discontinuities, we combine L rank and L grad together to supervise the training. Thus, our final loss can be given as:\nL = L rank + \u03bbL grad(7)\nwhere \u03bb is a balancing factor, which is set to 0.2 in our experiments.\n\nModel For the model, we use the ResNet50-based network architecture [36] as our backbone model. The network is trained with synchronized stochastic gradient descent (SGD) using default parameters over 4 GPUs. The batch size is set to 40 (10 images/GPU), and models are trained with ranking loss for 40 epochs with an initial learning rate of 0.02, which is then multiplied by .1 after 20 epochs. During training, images are horizontally flipped with a 50% chance, and resized to 448 \u00d7 448.\n\n\nDataset\n\nIn this section, we introduce the \"High-Resolution Web Stereo Image\" (HR-WSI) dataset (Fig 3), a diverse collection of high-resolution stereo images collected from the web. Similar to prior work [33,36], we use FlowNet2.0 [14] to generate disparity maps as our ground truth. However, these generated disparity maps contain errors where flow prediction fails, i.e. textureless regions and disocclusion parts. To improve the data quality, we use a forwardbackwards flow consistency check to mask out outliers which are ignored for training. Furthermore, as sky regions pose a specific challenge, we also compute high-quality sky segmentation masks via a pretrained network (see the supplementary material for details), and set the disparity of sky regions to be the minimum observed value. After manually rejecting bad ground truth data, we are left with 20378 images for training, and 400 images for validation.\n\nCompared to ReDWeb [36], our dataset has three advantages: more training samples (20378 vs. 3600), higher resolution (833 \u00d7 1251 vs. 408 \u00d7 465) and better handling of regions of incorrect disparities, i.e. sky. For instance segmentation masks, we use Mask-R-CNN that pre-trained on the COCO dataset, and retain all bounding boxes with confidence score \u2265 0.7. More specifically, we keep 15 labels including living body (e.g., human, dog, etc.) and non-living body (e.g., car, chair, and dining table). Our statistical analysis indicates that roughly 45.8% of images of our dataset contain humans, which is starkly different from most depth supervision datasets, and about 72% images contain object instances that we use to drive our sampling.\n\n\nExperiments\n\nIn this section, we first evaluate the performance of our model through zero-shot cross-dataset evaluation on six RGB-D benchmarks that were unseen during training. After that, we present ablation studies of our method to demonstrate the benefit of our structure-guided ranking loss.\n\n\nTest data\n\nIbims [16] consists of 100 RGB-D pairs from indoor scenes for evaluation. In order to provide a detailed analysis on specific characteristics (e.g., depth discontinuities) of depth maps, the dataset provides masks for distinct depth transitions and planar regions. TUM [30] consists of RGB-D images of indoor scenes of people performing different actions. For fair comparisons, we use the same data as [21] for evaluation. In particular, there are 11 image sequences with 1815 images. This dataset provides human masks, so we can also evaluate depth edges around human instances. Sintel [1] is derived from the open source 3D animated short film consisting of 1064 images with accurate ground truth depth maps. NYUDv2 [29] is an indoor dataset with depth captured by a Kinect depth sensor. We use the official test split (654 images) for evaluation. The original resolution of each image is 480 \u00d7 640, we follow the previous method [6] to report scores on a pre-defined center cropping. KITTI [31] consists of over 93K outdoor images collected from a car with stereo cameras and Lidar ground truth. In our experiments, we use the commonly used test split (697 images) provided by Eigen et al. [6] and only evaluate on pixels with valid ground truth depth. DIODE [32] contains both indoor and outdoor static scenes with accurate ground truth depth. We use the official test set (771 images).\n\n\nMetrics\n\nFor zero-shot cross-dataset evaluation, we use an ordinal error [3], analogous to Weighted Human Disagreement Rate (WHDR) [41]. The ordinal error can be defined as\nOrd = i \u03c9 i I(\u2113 i = \u2113 * i,\u03c4 (p)) i \u03c9 i ,(8)\nwhere \u03c9 i is set to 1, and the ordinal relationships \u2113 i and \u2113 * i,\u03c4 (p) are computed using Eqn. 2. For each image, we randomly sample 50000 point pairs to compute the ordinal error. This ordinal error is a general metric for evaluating the ordinal accuracy of a depth map, and it can be directly used with difference sources of depth ground truth. Although our model is mainly trained with the ordinal ranking loss, we also report various metric depth error scores for completeness. Our model gives competitive results under these metrics. Due to limited space, please refer to the supplementary material for details.\n\n\nZero-shot Cross-dataset Evaluation\n\nWe report the ordinal error of different models in Table 1 compared with state-of-the-art methods. Note that these models were trained using different datasets and different losses. The ordinal error may not fairly reflect all the aspects of their quality, but it is a meaningful metric to demonstrate their cross-dataset generalization performance.\n\nCompared methods For DIW [3], we use their released model that trained on DIW using a ranking loss. RW [36] was trained with a ranking loss on the RW dataset which is derived from web stereo images. DL [34] was trained with a \n\n\nMethods\n\nTraining   Our structure-guided ranking loss is denoted as SR. To disentangle the effect of datasets from that of losses, we also evaluate three baseline models: 1) Ours AI: using the same losses as MiDaS; 2) Ours \u2020: using our final loss on the RW dataset; 3) Ours R: using the pair-wise ranking loss [36]. To evaluate the robustness of trained models, we compare our models with the state-of-the-art methods on six RGBD datasets that were unseen during training. The lowest error is boldfaced and the second lowest is underlined. \u2113 1 loss on the iPhone Depth (ID) dataset of about 2k images which was collected by an iPhone camera. MD [22] was trained with a scale-invariant loss and a multi-scale gradient matching loss on a large scale dataset MD which focuses on famous outdoor landmarks. YT3D [4] also combined different sources of data (i.e., RW, DIW, and YT3D) to improve the robustness of the model. They used the original ranking loss for training. MC [21] targets on learning the depth of people, so the model was trained on the MC dataset in which each image contains humans. Since only a single image can be used, we use the single-view model of MC for comparisons. Similar to MD, they used a scale-invariant loss and a a multi-scale gradient matching loss for training. MiDaS [19] was trained using an affine-invariant loss combined with a multi-scale gradient matching term. The model also used much more data by mixing RW, MD, and MV.\n\nAs shown in Table 1, MiDaS achieves very competitive generalization performance, but their model was trained on a collection of large-scale datasets. To better compare with their loss functions, we train a baseline model Ours AI on our proposed dataset using the same loss functions and settings proposed by [19].\n\nDespite the fact that both MiDaS and YT3D mixed different sources of data for training, our model still achieves the best performance in this setting. The only exception is that MiDaS performs slightly better than ours on Sintel dataset. This is likely due to the fact the characteristics of Sintel and MV are similar, since both of them were derived from movie data. MD and MC were trained on datasets of outdoor landmarks and humans, respectively. As a result, MD performs well in outdoor scenes (e.g., KITTI), but falls short in indoor scenes (e.g., Ibims, TUM, and NYUDv2). Similarly, MC generalizes worse on datasets that contain outdoor scenes (e.g., KITTI and DIODE). Compared to these state-of-the-art methods, our final model shows stronger robustness in unconstrained scenes, showing the advantage of our web stereo dataset.\n\nCompared with the baseline models (i.e. Ours AI, Ours \u2020, and Ours R), our model consistently achieves lower ranking errors. In addition to quantitative comparisons, we also RGB Ours AI Ours R Ours E Ours ER Ours ERI Ours ERM Ours ERIM GT demonstrate qualitative results in Fig. 5. One can see that our model can get more consistent depth with sharper depth discontinuities. Benefited from our accurate sky masks, Ours AI is also able to generate more accurate depth at sky regions. However, the affine-invariant loss does not seem sensitive to local depth structures and the depth boundaries, and thus the predictions are more blurry and lack details.\n\n\nAnalysis of structure-guided sampling\n\nIn order to analyse the effectiveness of structure-guided sampling, we conduct ablation studies on two RGB-D benchmarks. In particular, we analyze the effect of loss functions on the accuracy of depth boundary localization. We use boundary errors (\u03b5 acc and \u03b5 comp ) on Ibims according to their definitions [16]. In addition, we follow [21] to measure human-related scale-invariant RMSE (si-hum, siintra, and si-inter) on TUM. We refer the reader to the corresponding papers for the definition of these metrics. Note that, all the compared models are only able to generate relative depth. In order to evaluate these with respect to metric depth ground truth, we align the scale and shift of all predictions to the ground truth before evaluation [19].\n\nOur full model uses a combination of sampling strategies such as random sampling (R), edge-guided sampling (E), instance-guide sampling (I) as well as the multi-scale gradient matching loss term (M). The results of different combinations of these components are shown in Table. 2. Some qualitative results are shown in Fig. 4.\n\nFrom Table. 2 and Fig. 4, one can observe that our ranking-based models perform better at depth boundaries as well as depth consistency of people. In general, Ours AI tends to be blurry and lack local depth details comparing with other baselines, but it provide pretty good depth consistency. Similarly, since Ours R is trained on random point pairs, the prediction also tends to be less accurate on local structures. If we only use edge-guided sampling that samples point pairs around edges (i.e., Ours E), depth boundaries become sharper at the cost of depth consistency both qualitatively and quantitatively. Therefore, global and local information are both important in our task, and the combination of the two (Ours ER) strikes a good balance. To further improve the performance of depth consistency on object instances, we incorporate instance-guided sampling into our sampling strategy (i.e., Ours ERI). One can observe that Ours ERI's performances of depth consistency and boundary accuracy are both improved over Ours ER. The importance of instance-guided sampling is also reflected in the improvements of Ours ERIM over Ours ERM, as the only difference between the two is whether using instance guidance or not. Overall, our full model achieves the best performance.\n\n\nConclusion\n\nDisparity data generated from stereo images and videos is a promising source of supervision for depth prediction methods, but it can only approximate the true inverse depth Input DIW [3] MD [22] Y3D [4] MC [21] MiDaS [19] Ours AI Ours GT Ibims [16] TUM [30] Sintel [1] NUYDv2 [29] KITTI [31] DIODE [32]  up to an affine transformation. As a result, we have introduced a structure-guided ranking loss to guide the network towards the hardest, and most critical, components of depth reconstruction: depth discontinuities. In addition, we introduced a high-resolution web stereo image dataset that covers diverse scenes with dense ground truth, and showed that our proposed loss can learn consistent predictions with sharp depth discontinuities. One feature of our loss is that it is easily guide-able, meaning for any new task and dataset, the sampling strategy could be tweaked to address specific attributes necessary for downstream applications.\n\nFigure 2 .\n2(a) shows the halo effect along depth boundary region generated by only sampling two points across an image edge; (b) and (c) show the procedure of edge-and instance-guided sampling. must be large to minimize the loss. Given a set of sampled point pairs P = {[p i,0 , p i,1 ], i = 1, . . . , N }, the overall ranking loss can be defined as follows,\n\nAlgorithm 1\n1The procedure for edge-guided sampling Require: Edge masks E, gradient maps Gx, Gy and gradient magnitude G, number of edge pixels L to be sampled Initial: Sampled points S = \u2205 1: for i = 1, 2, ...points [(x k , y k ), k = a, b, c, d]\n\nFigure 3 .\n3Example images from our (HR-WSI) dataset, consisting of high-resolution stereo images in the wild, and derived disparity maps with consistency checking.\n\n1 .\n1Ordinal error (%) of zero-shot cross-dataset evaluation. Existing monodepth methods were trained on different sources of data: DIW, ReDWeb (RW), MegaDepth (MD), 3D Movies (MV), iPhone Depth (ID), YouTube3D (YT3D), and MannequinChallenge (MC); with different losses: pair-wise ranking loss (PR), affine-invariant MSE loss (AI), multi-scale gradient matching loss (MGM), L1 loss (L1), scale-invariant loss (SI), robust ordinal depth loss (ROD) and multi-scale edge-aware smoothness loss (MES).\n\nFigure 4 .\n4Qualitative evaluation of different sampling strategies and the affine-invariant loss. Best viewed zoomed in on-screen. Our full model trained with a combination of the structure-guide ranking loss and the multi-scale gradient matching loss generates a globally consistent depth map with sharp depth boundaries and detailed depth structures.\n\nFigure 5 .\n5Qualitative results of single image depth prediction methods applied to different datasets.\n\nTable\n\n\n\nTable 2. Quantitative evaluation, and an ablation of variants on our loss function, including: Ours AI: the baseline model trained on our data with affine invariant and multi-scale gradient losses as in[19]; Ours R: random sampling ranking loss; Ours E: edgeguided sampling; Ours ER: edge-guided sampling + Ours R; Ours ERI: instance-guided sampling + Ours ER; Ours ERM: multi-scale gradient matching term + Ours ER; Ours ERIM: our model trained with our final loss functions. For all metrics, lower is better.Ibims \nTUM \nMethods \n\u03b5 acc \n\u03b5 comp si-human si-intra si-inter \nDIW [3] \n8.083 82.549 \n0.437 \n0.345 \n0.474 \nDL [34] \n2.391 41.456 \n0.319 \n0.268 \n0.339 \nRW [36] \n3.029 67.725 \n0.304 \n0.238 \n0.330 \nMD [22] \n3.439 75.719 \n0.349 \n0.266 \n0.379 \nYT3D [4] \n7.542 85.921 \n0.347 \n0.288 \n0.369 \nMC [21] \n3.588 64.495 \n0.294 \n0.227 \n0.319 \nMiDaS [19] 2.766 66.290 \n0.288 \n0.228 \n0.309 \n\nOurs AI \n2.829 73.197 \n0.287 \n0.230 \n0.308 \nOurs R \n2.345 50.507 \n0.296 \n0.227 \n0.320 \nOurs E \n2.029 61.380 \n0.322 \n0.240 \n0.350 \nOurs ER \n2.203 47.585 \n0.301 \n0.226 \n0.326 \nOurs ERI \n1.936 53.029 \n0.291 \n0.225 \n0.315 \nOurs ERM \n2.093 34.962 \n0.296 \n0.228 \n0.319 \nOurs ERIM 1.835 41.294 \n0.280 \n0.212 \n0.303 \n\n\nhttp://www.shortcourses.com/stereo/ stereo3-11.html\nAcknowledgements This work was supported in part by the National Natural Science Foundation of China (Grant No. 61876211 and U1913602) and in part by the Adobe Gift. Part of the work was done when KX was an intern at Adobe Research.\nA naturalistic open source movie for optical flow evaluation. D J Butler, J Wulff, G B Stanley, M J Black, Proc. European Conf. on Computer Vision (ECCV). European Conf. on Computer Vision (ECCV)3D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical flow evaluation. In Proc. European Conf. on Computer Vision (ECCV), pages 611-625, 2012. 3, 5, 8\n\nDepth from a single image by harmonizing overcomplete local network predictions. Ayan Chakrabarti, Jingyu Shao, Gregory Shakhnarovich, Advances in Neural Information Processing Systems. Ayan Chakrabarti, Jingyu Shao, and Gregory Shakhnarovich. Depth from a single image by harmonizing overcomplete lo- cal network predictions. In Advances in Neural Information Processing Systems, 2016. 2\n\nSingleimage depth perception in the wild. Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng, Advances in Neural Information Processing Systems. 7Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single- image depth perception in the wild. In Advances in Neural Information Processing Systems, pages 730-738. 2016. 2, 3, 5, 6, 7, 8\n\nLearning singleimage depth from videos using quality assessment networks. Weifeng Chen, Shengyi Qian, Jia Deng, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)7Weifeng Chen, Shengyi Qian, and Jia Deng. Learning single- image depth from videos using quality assessment networks. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 5604-5613, 2019. 1, 2, 3, 6, 7, 8\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. David Eigen, Rob Fergus, Proc. Int. Conf. on Computer Vision (ICCV). Int. Conf. on Computer Vision (ICCV)David Eigen and Rob Fergus. Predicting depth, surface nor- mals and semantic labels with a common multi-scale convo- lutional architecture. In Proc. Int. Conf. on Computer Vision (ICCV), 2015. 2\n\nDepth map prediction from a single image using a multi-scale deep network. David Eigen, Christian Puhrsch, Rob Fergus, Advances in Neural Information Processing Systems. 25David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep net- work. In Advances in Neural Information Processing Sys- tems, 2014. 2, 5\n\nDeep Ordinal Regression Network for Monocular Depth Estimation. Huan Fu, Mingming Gong, Chaohui Wang, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Kayhan Batmanghelich, and Dacheng TaoHuan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat- manghelich, and Dacheng Tao. Deep Ordinal Regression Network for Monocular Depth Estimation. In Proc. Com- puter Vision and Pattern Recognition (CVPR), 2018. 2\n\nVirtual worlds as proxy for multi-object tracking analysis. A Gaidon, Y Wang, E Cabon, Vig, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)A Gaidon, Q Wang, Y Cabon, and E Vig. Virtual worlds as proxy for multi-object tracking analysis. In Proc. Computer Vision and Pattern Recognition (CVPR), 2016. 3\n\nUnsupervised cnn for single view depth estimation: Geometry to the rescue. Ravi Garg, Ian Reid, Proc. European Conf. on Computer Vision (ECCV). European Conf. on Computer Vision (ECCV)Ravi Garg and Ian Reid. Unsupervised cnn for single view depth estimation: Geometry to the rescue. In Proc. European Conf. on Computer Vision (ECCV), 2016. 2\n\nLearning single camera depth estimation using dualpixels. Rahul Garg, Neal Wadhwa, Sameer Ansari, Jonathan T Barron, arXiv:1904.058221arXiv preprintRahul Garg, Neal Wadhwa, Sameer Ansari, and Jonathan T Barron. Learning single camera depth estimation using dual- pixels. arXiv preprint arXiv:1904.05822, 2019. 1, 2\n\nVision meets robotics: The kitti dataset. The International. Andreas Geiger, Philip Lenz, Christoph Stiller, Raquel Urtasun, Journal of Robotics Research. 3211Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research, 32(11):1231-1237, 2013. 1\n\nBrostow. Unsupervised monocular depth estimation with leftright consistency. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J , Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J. Bros- tow. Unsupervised monocular depth estimation with left- right consistency. In Proc. Computer Vision and Pattern Recognition (CVPR), 2017. 2\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proc. Int. Conf. on Computer Vision (ICCV). Int. Conf. on Computer Vision (ICCV)Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proc. Int. Conf. on Computer Vision (ICCV), pages 2961-2969, 2017. 4\n\nFlownet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proc. Computer Vision and Pattern Recognition (CVPR), Jul 2017. 5\n\nDepthtransfer: Depth extraction from video using non-parametric sampling. Kevin Karsch, Ce Liu, Sing Bing Kang, Trans. Pattern Analysis and Machine Intelligence. 2Kevin Karsch, Ce Liu, and Sing Bing Kang. Depthtransfer: Depth extraction from video using non-parametric sampling. Trans. Pattern Analysis and Machine Intelligence, 2014. 2\n\nEvaluation of cnn-based single-image depth estimation methods. Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, Marco K\u00f6rner, Proc. European Conf. on Computer Vision Workshop (ECCV-WS). European Conf. on Computer Vision Workshop (ECCV-WS)7Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco K\u00f6rner. Evaluation of cnn-based single-image depth estimation methods. In Proc. European Conf. on Computer Vision Workshop (ECCV-WS), pages 331-348, 2018. 1, 2, 5, 7, 8\n\nSemisupervised deep learning for monocular depth map prediction. Yevhen Kuznietsov, Jorg Stuckler, Bastian Leibe, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi- supervised deep learning for monocular depth map predic- tion. In Proc. Computer Vision and Pattern Recognition (CVPR), July 2017. 2\n\nDeeper depth prediction with fully convolutional residual networks. Iro Laina, Christian Rupprecht, Proc. IEEE Int. Conf. 3D Vision (3DV). IEEE Int. Conf. 3D Vision (3DV)Vasileios Belagiannis, Federico Tombari, and Nassir NavabIro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed- erico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In Proc. IEEE Int. Conf. 3D Vision (3DV), 2016. 2\n\nTowards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. Katrin Lasinger, Ren\u00e9 Ranftl, Konrad Schindler, Vladlen Koltun, arXiv:1907.013417Katrin Lasinger, Ren\u00e9 Ranftl, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estima- tion: Mixing datasets for zero-shot cross-dataset transfer. arXiv:1907.01341, 2019. 1, 2, 3, 6, 7, 8\n\nSingle-image depth estimation based on fourier domain analysis. Jae-Han Lee, Minhyeok Heo, Kyung-Rae Kim, Chang-Su Kim, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Jae-Han Lee, Minhyeok Heo, Kyung-Rae Kim, and Chang- Su Kim. Single-image depth estimation based on fourier do- main analysis. In Proc. Computer Vision and Pattern Recog- nition (CVPR), 2018. 2\n\nLearning the depths of moving people by watching frozen people. Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, William T Freeman, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)7Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, and William T Freeman. Learn- ing the depths of moving people by watching frozen people. In Proc. Computer Vision and Pattern Recognition (CVPR), June 2019. 1, 2, 3, 5, 6, 7, 8\n\nMegadepth: Learning singleview depth prediction from internet photos. Zhengqi Li, Noah Snavely, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)7Zhengqi Li and Noah Snavely. Megadepth: Learning single- view depth prediction from internet photos. In Proc. Com- puter Vision and Pattern Recognition (CVPR), 2018. 2, 3, 4, 6, 7, 8\n\nSingle image depth estimation from predicted semantic labels. Beyang Liu, Stephen Gould, Daphne Koller, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Beyang Liu, Stephen Gould, and Daphne Koller. Single im- age depth estimation from predicted semantic labels. In Proc. Computer Vision and Pattern Recognition (CVPR), 2010. 2\n\nLearning depth from single monocular images using deep convolutional neural fields. Trans. Pattern Analysis and Machine Intelligence. Fayao Liu, Chunhua Shen, Guosheng Lin, Ian Reid, Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning depth from single monocular images using deep convolutional neural fields. Trans. Pattern Analysis and Ma- chine Intelligence, 2015. 2\n\nGeonet: Geometric neural network for joint depth and surface normal estimation. Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, Jiaya Jia, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia. Geonet: Geometric neural network for joint depth and surface normal estimation. In Proc. Computer Vi- sion and Pattern Recognition (CVPR), 2018. 2\n\nThe synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, Antonio M Lopez, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M. Lopez. The synthia dataset: A large collection of synthetic images for semantic segmenta- tion of urban scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), June 2016. 3\n\nMake3d: Learning 3d scene structure from a single still image. Trans. Pattern Analysis and Machine Intelligence. Ashutosh Saxena, Min Sun, Andrew Y Ng, Ashutosh Saxena, Min Sun, and Andrew Y. Ng. Make3d: Learning 3d scene structure from a single still image. Trans. Pattern Analysis and Machine Intelligence, 2009. 2\n\nA multi-view stereo benchmark with highresolution images and multi-camera videos. Thomas Sch\u00f6ps, Johannes L Sch\u00f6nberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, Andreas Geiger, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Thomas Sch\u00f6ps, Johannes L. Sch\u00f6nberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An- dreas Geiger. A multi-view stereo benchmark with high- resolution images and multi-camera videos. In Proc. Com- puter Vision and Pattern Recognition (CVPR), 2017. 2\n\nIndoor segmentation and support inference from rgbd images. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus, Proc. European Conf. on Computer Vision (ECCV). European Conf. on Computer Vision (ECCV)Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Proc. European Conf. on Computer Vision (ECCV), 2012. 1, 2, 5, 8\n\nA benchmark for the evaluation of rgb-d slam systems. J Sturm, N Engelhard, F Endres, W Burgard, D Cremers, Proc. Int. Conf. on Intelligent Robot Systems (IROS). Int. Conf. on Intelligent Robot Systems (IROS)5J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre- mers. A benchmark for the evaluation of rgb-d slam sys- tems. In Proc. Int. Conf. on Intelligent Robot Systems (IROS), 2012. 2, 5, 8\n\nSparsity invariant cnns. Jonas Uhrig, Nick Schneider, Lucas Schneider, Uwe Franke, Thomas Brox, Andereas Geiger, Proc. IEEE Int. Conf. on 3D Vision (3DV. IEEE Int. Conf. on 3D Vision (3DVJonas Uhrig, Nick Schneider, Lucas Schneider, Uwe Franke, Thomas Brox, and Andereas Geiger. Sparsity invariant cnns. In Proc. IEEE Int. Conf. on 3D Vision (3DV), 2017. 2, 5, 8\n\nDIODE: A Dense Indoor and Outdoor DEpth Dataset. Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Z Falcon, Andrea F Dai, Mohammadreza Daniele, Steven Mostajabi, Matthew R Basart, Gregory Walter, Shakhnarovich, arxiv: 1908.00463Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z. Dai, Andrea F. Daniele, Moham- madreza Mostajabi, Steven Basart, Matthew R. Walter, and Gregory Shakhnarovich. DIODE: A Dense Indoor and Out- door DEpth Dataset. arxiv: 1908.00463, 2019. 2, 5, 8\n\nWeb stereo video supervision for depth prediction from dynamic scenes. Chaoyang Wang, Simon Lucey, Federico Perazzi, Oliver Wang, Proc. IEEE Int. Conf. on 3D Vision (3DV). IEEE Int. Conf. on 3D Vision (3DV)Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver Wang. Web stereo video supervision for depth prediction from dynamic scenes. In Proc. IEEE Int. Conf. on 3D Vision (3DV), 2019. 1, 2, 3, 5\n\nDeeplens: Shallow depth of field from a single image. Lijun Wang, Xiaohui Shen, Jianming Zhang, Oliver Wang, Zhe Lin, Chih-Yao Hsieh, Sarah Kong, Huchuan Lu, Proc. SIGGRAPH Asia). SIGGRAPH Asia)377Lijun Wang, Xiaohui Shen, Jianming Zhang, Oliver Wang, Zhe Lin, Chih-Yao Hsieh, Sarah Kong, and Huchuan Lu. Deeplens: Shallow depth of field from a single image. ACM Trans. Graph. (Proc. SIGGRAPH Asia), 37(6):6:1- 6:11, 2018. 2, 5, 6, 7\n\nSelf-supervised monocular depth hints. Jamie Watson, Michael Firman, Gabriel J Brostow, Daniyar Turmukhambetov, Proc. Int. Conf. on Computer Vision (ICCV). Int. Conf. on Computer Vision (ICCV)Jamie Watson, Michael Firman, Gabriel J. Brostow, and Daniyar Turmukhambetov. Self-supervised monocular depth hints. In Proc. Int. Conf. on Computer Vision (ICCV), 2019. 2\n\nMonocular relative depth perception with web stereo data supervision. Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, Zhenbo Luo, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)67Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo. Monocular relative depth per- ception with web stereo data supervision. In Proc. Computer Vision and Pattern Recognition (CVPR), June 2018. 1, 2, 3, 4, 5, 6, 7\n\nPad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing. Dan Xu, Wanli Ouyang, Xiaogang Wang, Nicu Sebe, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe. Pad-net: Multi-tasks guided prediction-and-distillation net- work for simultaneous depth estimation and scene parsing. In Proc. Computer Vision and Pattern Recognition (CVPR), 2018. 2\n\nStructured attention guided convolutional neural fields for monocular depth estimation. Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, Elisa Ricci, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and Elisa Ricci. Structured attention guided convolutional neural fields for monocular depth estimation. In Proc. Computer Vision and Pattern Recognition (CVPR), 2018. 2\n\nEnforcing geometric constraints of virtual normal for depth prediction. Wei Yin, Yifan Liu, Chunhua Shen, Youliang Yan, Proc. Int. Conf. on Computer Vision (ICCV). Int. Conf. on Computer Vision (ICCV)Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. En- forcing geometric constraints of virtual normal for depth pre- diction. In Proc. Int. Conf. on Computer Vision (ICCV), 2019. 2\n\nUnsupervised learning of depth and ego-motion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Tinghui Zhou, Matthew Brown, Noah Snavely, and David G. Lowe. Unsupervised learning of depth and ego-motion from video. In Proc. Computer Vision and Pattern Recognition (CVPR), 2017. 2\n\nLearning ordinal relationships for mid-level vision. Daniel Zoran, Phillip Isola, Dilip Krishnan, William T Freeman, Proc. Int. Conf. on Computer Vision (ICCV). Int. Conf. on Computer Vision (ICCV)Daniel Zoran, Phillip Isola, Dilip Krishnan, and William T Freeman. Learning ordinal relationships for mid-level vi- sion. In Proc. Int. Conf. on Computer Vision (ICCV), 2015. 5\n", "annotations": {"author": "[{\"start\":\"67\",\"end\":\"267\"},{\"start\":\"268\",\"end\":\"300\"},{\"start\":\"301\",\"end\":\"330\"},{\"start\":\"331\",\"end\":\"357\"},{\"start\":\"358\",\"end\":\"398\"},{\"start\":\"399\",\"end\":\"620\"}]", "publisher": null, "author_last_name": "[{\"start\":\"70\",\"end\":\"74\"},{\"start\":\"277\",\"end\":\"282\"},{\"start\":\"308\",\"end\":\"312\"},{\"start\":\"336\",\"end\":\"339\"},{\"start\":\"362\",\"end\":\"365\"},{\"start\":\"406\",\"end\":\"409\"}]", "author_first_name": "[{\"start\":\"67\",\"end\":\"69\"},{\"start\":\"268\",\"end\":\"276\"},{\"start\":\"301\",\"end\":\"307\"},{\"start\":\"331\",\"end\":\"335\"},{\"start\":\"358\",\"end\":\"361\"},{\"start\":\"399\",\"end\":\"405\"}]", "author_affiliation": "[{\"start\":\"76\",\"end\":\"266\"},{\"start\":\"284\",\"end\":\"299\"},{\"start\":\"314\",\"end\":\"329\"},{\"start\":\"341\",\"end\":\"356\"},{\"start\":\"382\",\"end\":\"397\"},{\"start\":\"429\",\"end\":\"619\"}]", "title": "[{\"start\":\"1\",\"end\":\"64\"},{\"start\":\"621\",\"end\":\"684\"}]", "venue": null, "abstract": "[{\"start\":\"751\",\"end\":\"2462\"}]", "bib_ref": "[{\"start\":\"2965\",\"end\":\"2969\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"2980\",\"end\":\"2984\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"3036\",\"end\":\"3040\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"3125\",\"end\":\"3129\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"3129\",\"end\":\"3132\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"3604\",\"end\":\"3608\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"3608\",\"end\":\"3611\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"3611\",\"end\":\"3614\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"3849\",\"end\":\"3852\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3852\",\"end\":\"3855\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"4040\",\"end\":\"4043\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"4043\",\"end\":\"4046\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"4046\",\"end\":\"4049\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"4049\",\"end\":\"4052\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"4052\",\"end\":\"4055\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"4561\",\"end\":\"4564\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"4564\",\"end\":\"4567\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"6477\",\"end\":\"6481\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"6481\",\"end\":\"6484\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"6484\",\"end\":\"6487\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"6606\",\"end\":\"6609\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"6609\",\"end\":\"6611\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"6611\",\"end\":\"6613\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"6613\",\"end\":\"6615\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"6615\",\"end\":\"6618\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"6618\",\"end\":\"6621\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"6621\",\"end\":\"6624\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"6624\",\"end\":\"6627\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"6627\",\"end\":\"6630\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"6630\",\"end\":\"6633\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"6633\",\"end\":\"6636\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"7117\",\"end\":\"7120\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"7120\",\"end\":\"7123\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"7123\",\"end\":\"7126\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"7126\",\"end\":\"7129\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"7129\",\"end\":\"7132\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"7514\",\"end\":\"7517\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"7517\",\"end\":\"7519\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"7519\",\"end\":\"7522\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"7522\",\"end\":\"7525\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"7525\",\"end\":\"7528\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"7528\",\"end\":\"7531\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"7531\",\"end\":\"7534\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"7534\",\"end\":\"7537\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"7816\",\"end\":\"7819\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"7819\",\"end\":\"7822\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"7822\",\"end\":\"7825\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"7825\",\"end\":\"7828\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"7828\",\"end\":\"7831\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"7884\",\"end\":\"7887\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"7887\",\"end\":\"7889\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"7889\",\"end\":\"7892\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"9008\",\"end\":\"9012\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"9012\",\"end\":\"9015\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"9043\",\"end\":\"9047\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"9047\",\"end\":\"9050\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"9128\",\"end\":\"9132\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"9132\",\"end\":\"9135\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"9135\",\"end\":\"9138\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"9446\",\"end\":\"9449\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"9449\",\"end\":\"9451\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"9451\",\"end\":\"9454\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"9858\",\"end\":\"9861\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"9921\",\"end\":\"9924\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"9924\",\"end\":\"9927\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"9927\",\"end\":\"9930\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"9946\",\"end\":\"9950\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"9969\",\"end\":\"9973\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"9973\",\"end\":\"9976\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"10765\",\"end\":\"10768\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"11967\",\"end\":\"11970\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"11970\",\"end\":\"11973\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"16017\",\"end\":\"16021\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"17317\",\"end\":\"17321\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"17640\",\"end\":\"17644\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"18008\",\"end\":\"18012\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"18636\",\"end\":\"18640\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"18640\",\"end\":\"18643\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"18663\",\"end\":\"18667\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"19372\",\"end\":\"19376\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"20413\",\"end\":\"20417\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"20676\",\"end\":\"20680\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"20809\",\"end\":\"20813\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"20994\",\"end\":\"20997\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"21125\",\"end\":\"21129\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"21339\",\"end\":\"21342\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"21400\",\"end\":\"21404\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"21600\",\"end\":\"21603\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"21669\",\"end\":\"21673\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"21873\",\"end\":\"21876\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"21931\",\"end\":\"21935\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"23050\",\"end\":\"23053\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"23128\",\"end\":\"23132\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"23227\",\"end\":\"23231\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"23564\",\"end\":\"23568\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"23899\",\"end\":\"23903\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"24061\",\"end\":\"24064\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"24224\",\"end\":\"24228\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"24552\",\"end\":\"24556\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"25022\",\"end\":\"25026\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"26865\",\"end\":\"26869\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"26894\",\"end\":\"26898\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"27303\",\"end\":\"27307\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"29112\",\"end\":\"29115\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"29119\",\"end\":\"29123\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"29128\",\"end\":\"29131\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"29135\",\"end\":\"29139\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"29146\",\"end\":\"29150\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"29173\",\"end\":\"29177\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"29182\",\"end\":\"29186\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"29194\",\"end\":\"29197\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"29205\",\"end\":\"29209\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"29216\",\"end\":\"29220\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"29227\",\"end\":\"29231\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"31823\",\"end\":\"31827\",\"attributes\":{\"ref_id\":\"b18\"}}]", "figure": "[{\"start\":\"29876\",\"end\":\"30237\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"30238\",\"end\":\"30486\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"30487\",\"end\":\"30652\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"30653\",\"end\":\"31150\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"31151\",\"end\":\"31505\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"31506\",\"end\":\"31610\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"31611\",\"end\":\"31618\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"31619\",\"end\":\"32816\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2478\",\"end\":\"3751\"},{\"start\":\"3753\",\"end\":\"4287\"},{\"start\":\"4289\",\"end\":\"5049\"},{\"start\":\"5051\",\"end\":\"5991\"},{\"start\":\"5993\",\"end\":\"6042\"},{\"start\":\"6044\",\"end\":\"6385\"},{\"start\":\"6402\",\"end\":\"7557\"},{\"start\":\"7559\",\"end\":\"8746\"},{\"start\":\"8748\",\"end\":\"9686\"},{\"start\":\"9688\",\"end\":\"10269\"},{\"start\":\"10334\",\"end\":\"10759\"},{\"start\":\"10761\",\"end\":\"11037\"},{\"start\":\"11117\",\"end\":\"11216\"},{\"start\":\"11303\",\"end\":\"11663\"},{\"start\":\"11705\",\"end\":\"12240\"},{\"start\":\"12265\",\"end\":\"12780\"},{\"start\":\"12782\",\"end\":\"13339\"},{\"start\":\"13341\",\"end\":\"14011\"},{\"start\":\"14040\",\"end\":\"14266\"},{\"start\":\"14293\",\"end\":\"14410\"},{\"start\":\"14503\",\"end\":\"14901\"},{\"start\":\"14930\",\"end\":\"15499\"},{\"start\":\"15501\",\"end\":\"15970\"},{\"start\":\"15972\",\"end\":\"16372\"},{\"start\":\"16391\",\"end\":\"17232\"},{\"start\":\"17234\",\"end\":\"17457\"},{\"start\":\"17509\",\"end\":\"17684\"},{\"start\":\"17686\",\"end\":\"17843\"},{\"start\":\"17868\",\"end\":\"17938\"},{\"start\":\"17940\",\"end\":\"18429\"},{\"start\":\"18441\",\"end\":\"19351\"},{\"start\":\"19353\",\"end\":\"20094\"},{\"start\":\"20110\",\"end\":\"20393\"},{\"start\":\"20407\",\"end\":\"21797\"},{\"start\":\"21809\",\"end\":\"21972\"},{\"start\":\"22017\",\"end\":\"22635\"},{\"start\":\"22674\",\"end\":\"23023\"},{\"start\":\"23025\",\"end\":\"23251\"},{\"start\":\"23263\",\"end\":\"24712\"},{\"start\":\"24714\",\"end\":\"25027\"},{\"start\":\"25029\",\"end\":\"25863\"},{\"start\":\"25865\",\"end\":\"26516\"},{\"start\":\"26558\",\"end\":\"27308\"},{\"start\":\"27310\",\"end\":\"27636\"},{\"start\":\"27638\",\"end\":\"28914\"},{\"start\":\"28929\",\"end\":\"29875\"}]", "formula": "[{\"start\":\"11038\",\"end\":\"11116\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"11217\",\"end\":\"11302\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"11664\",\"end\":\"11704\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"14012\",\"end\":\"14039\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"14267\",\"end\":\"14292\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"14411\",\"end\":\"14502\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"17458\",\"end\":\"17508\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"17844\",\"end\":\"17867\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"21973\",\"end\":\"22016\",\"attributes\":{\"id\":\"formula_8\"}}]", "table_ref": "[{\"start\":\"24726\",\"end\":\"24733\"},{\"start\":\"27581\",\"end\":\"27587\"},{\"start\":\"27643\",\"end\":\"27649\"}]", "section_header": "[{\"start\":\"2464\",\"end\":\"2476\",\"attributes\":{\"n\":\"1.\"}},{\"start\":\"6388\",\"end\":\"6400\",\"attributes\":{\"n\":\"2.\"}},{\"start\":\"10272\",\"end\":\"10332\",\"attributes\":{\"n\":\"3.\"}},{\"start\":\"12243\",\"end\":\"12263\",\"attributes\":{\"n\":\"3.1.\"}},{\"start\":\"14904\",\"end\":\"14928\",\"attributes\":{\"n\":\"3.2.\"}},{\"start\":\"16375\",\"end\":\"16389\",\"attributes\":{\"n\":\"3.3.\"}},{\"start\":\"18432\",\"end\":\"18439\",\"attributes\":{\"n\":\"4.\"}},{\"start\":\"20097\",\"end\":\"20108\",\"attributes\":{\"n\":\"5.\"}},{\"start\":\"20396\",\"end\":\"20405\",\"attributes\":{\"n\":\"5.1.\"}},{\"start\":\"21800\",\"end\":\"21807\",\"attributes\":{\"n\":\"5.2.\"}},{\"start\":\"22638\",\"end\":\"22672\",\"attributes\":{\"n\":\"5.3.\"}},{\"start\":\"23254\",\"end\":\"23261\"},{\"start\":\"26519\",\"end\":\"26556\",\"attributes\":{\"n\":\"5.4.\"}},{\"start\":\"28917\",\"end\":\"28927\",\"attributes\":{\"n\":\"6.\"}},{\"start\":\"29877\",\"end\":\"29887\"},{\"start\":\"30239\",\"end\":\"30250\"},{\"start\":\"30488\",\"end\":\"30498\"},{\"start\":\"30654\",\"end\":\"30657\"},{\"start\":\"31152\",\"end\":\"31162\"},{\"start\":\"31507\",\"end\":\"31517\"},{\"start\":\"31612\",\"end\":\"31617\"}]", "table": "[{\"start\":\"32131\",\"end\":\"32816\"}]", "figure_caption": "[{\"start\":\"29889\",\"end\":\"30237\"},{\"start\":\"30252\",\"end\":\"30486\"},{\"start\":\"30500\",\"end\":\"30652\"},{\"start\":\"30659\",\"end\":\"31150\"},{\"start\":\"31164\",\"end\":\"31505\"},{\"start\":\"31519\",\"end\":\"31610\"},{\"start\":\"31621\",\"end\":\"32131\"}]", "figure_ref": "[{\"start\":\"3743\",\"end\":\"3749\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"13429\",\"end\":\"13439\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"13767\",\"end\":\"13777\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"13978\",\"end\":\"13984\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"15775\",\"end\":\"15785\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"15878\",\"end\":\"15896\"},{\"start\":\"15947\",\"end\":\"15968\"},{\"start\":\"18527\",\"end\":\"18534\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"26138\",\"end\":\"26144\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"27629\",\"end\":\"27635\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"27656\",\"end\":\"27662\",\"attributes\":{\"ref_id\":\"fig_4\"}}]", "bib_author_first_name": "[{\"start\":\"33164\",\"end\":\"33165\"},{\"start\":\"33166\",\"end\":\"33167\"},{\"start\":\"33176\",\"end\":\"33177\"},{\"start\":\"33185\",\"end\":\"33186\"},{\"start\":\"33187\",\"end\":\"33188\"},{\"start\":\"33198\",\"end\":\"33199\"},{\"start\":\"33200\",\"end\":\"33201\"},{\"start\":\"33578\",\"end\":\"33582\"},{\"start\":\"33596\",\"end\":\"33602\"},{\"start\":\"33609\",\"end\":\"33616\"},{\"start\":\"33929\",\"end\":\"33936\"},{\"start\":\"33943\",\"end\":\"33947\"},{\"start\":\"33952\",\"end\":\"33957\"},{\"start\":\"33964\",\"end\":\"33967\"},{\"start\":\"34286\",\"end\":\"34293\"},{\"start\":\"34300\",\"end\":\"34307\"},{\"start\":\"34314\",\"end\":\"34317\"},{\"start\":\"34749\",\"end\":\"34754\"},{\"start\":\"34762\",\"end\":\"34765\"},{\"start\":\"35125\",\"end\":\"35130\"},{\"start\":\"35138\",\"end\":\"35147\"},{\"start\":\"35157\",\"end\":\"35160\"},{\"start\":\"35479\",\"end\":\"35483\"},{\"start\":\"35488\",\"end\":\"35496\"},{\"start\":\"35503\",\"end\":\"35510\"},{\"start\":\"35925\",\"end\":\"35926\"},{\"start\":\"35935\",\"end\":\"35936\"},{\"start\":\"35943\",\"end\":\"35944\"},{\"start\":\"36296\",\"end\":\"36300\"},{\"start\":\"36307\",\"end\":\"36310\"},{\"start\":\"36622\",\"end\":\"36627\"},{\"start\":\"36634\",\"end\":\"36638\"},{\"start\":\"36647\",\"end\":\"36653\"},{\"start\":\"36662\",\"end\":\"36670\"},{\"start\":\"36671\",\"end\":\"36672\"},{\"start\":\"36941\",\"end\":\"36948\"},{\"start\":\"36957\",\"end\":\"36963\"},{\"start\":\"36970\",\"end\":\"36979\"},{\"start\":\"36989\",\"end\":\"36995\"},{\"start\":\"37303\",\"end\":\"37310\"},{\"start\":\"37319\",\"end\":\"37324\"},{\"start\":\"37336\",\"end\":\"37343\"},{\"start\":\"37344\",\"end\":\"37345\"},{\"start\":\"37688\",\"end\":\"37695\"},{\"start\":\"37700\",\"end\":\"37707\"},{\"start\":\"38018\",\"end\":\"38019\"},{\"start\":\"38025\",\"end\":\"38026\"},{\"start\":\"38034\",\"end\":\"38035\"},{\"start\":\"38044\",\"end\":\"38045\"},{\"start\":\"38054\",\"end\":\"38055\"},{\"start\":\"38069\",\"end\":\"38070\"},{\"start\":\"38460\",\"end\":\"38465\"},{\"start\":\"38474\",\"end\":\"38476\"},{\"start\":\"38482\",\"end\":\"38491\"},{\"start\":\"38787\",\"end\":\"38793\"},{\"start\":\"38800\",\"end\":\"38805\"},{\"start\":\"38814\",\"end\":\"38823\"},{\"start\":\"38837\",\"end\":\"38842\"},{\"start\":\"39259\",\"end\":\"39265\"},{\"start\":\"39278\",\"end\":\"39282\"},{\"start\":\"39293\",\"end\":\"39300\"},{\"start\":\"39669\",\"end\":\"39672\"},{\"start\":\"39680\",\"end\":\"39689\"},{\"start\":\"40137\",\"end\":\"40143\"},{\"start\":\"40154\",\"end\":\"40158\"},{\"start\":\"40167\",\"end\":\"40173\"},{\"start\":\"40185\",\"end\":\"40192\"},{\"start\":\"40491\",\"end\":\"40498\"},{\"start\":\"40504\",\"end\":\"40512\"},{\"start\":\"40518\",\"end\":\"40527\"},{\"start\":\"40533\",\"end\":\"40541\"},{\"start\":\"40906\",\"end\":\"40913\"},{\"start\":\"40918\",\"end\":\"40922\"},{\"start\":\"40930\",\"end\":\"40939\"},{\"start\":\"40946\",\"end\":\"40953\"},{\"start\":\"40962\",\"end\":\"40966\"},{\"start\":\"40976\",\"end\":\"40978\"},{\"start\":\"40984\",\"end\":\"40993\"},{\"start\":\"41430\",\"end\":\"41437\"},{\"start\":\"41442\",\"end\":\"41446\"},{\"start\":\"41803\",\"end\":\"41809\"},{\"start\":\"41815\",\"end\":\"41822\"},{\"start\":\"41830\",\"end\":\"41836\"},{\"start\":\"42255\",\"end\":\"42260\"},{\"start\":\"42266\",\"end\":\"42273\"},{\"start\":\"42280\",\"end\":\"42288\"},{\"start\":\"42294\",\"end\":\"42297\"},{\"start\":\"42582\",\"end\":\"42590\"},{\"start\":\"42595\",\"end\":\"42601\"},{\"start\":\"42608\",\"end\":\"42616\"},{\"start\":\"42622\",\"end\":\"42628\"},{\"start\":\"42638\",\"end\":\"42643\"},{\"start\":\"43071\",\"end\":\"43077\"},{\"start\":\"43083\",\"end\":\"43088\"},{\"start\":\"43098\",\"end\":\"43104\"},{\"start\":\"43118\",\"end\":\"43123\"},{\"start\":\"43133\",\"end\":\"43140\"},{\"start\":\"43141\",\"end\":\"43142\"},{\"start\":\"43623\",\"end\":\"43631\"},{\"start\":\"43640\",\"end\":\"43643\"},{\"start\":\"43649\",\"end\":\"43655\"},{\"start\":\"43656\",\"end\":\"43657\"},{\"start\":\"43910\",\"end\":\"43916\"},{\"start\":\"43925\",\"end\":\"43933\"},{\"start\":\"43934\",\"end\":\"43935\"},{\"start\":\"43949\",\"end\":\"43956\"},{\"start\":\"43967\",\"end\":\"43974\"},{\"start\":\"43984\",\"end\":\"43990\"},{\"start\":\"44002\",\"end\":\"44006\"},{\"start\":\"44018\",\"end\":\"44025\"},{\"start\":\"44477\",\"end\":\"44483\"},{\"start\":\"44495\",\"end\":\"44500\"},{\"start\":\"44508\",\"end\":\"44516\"},{\"start\":\"44524\",\"end\":\"44527\"},{\"start\":\"44870\",\"end\":\"44871\"},{\"start\":\"44879\",\"end\":\"44880\"},{\"start\":\"44892\",\"end\":\"44893\"},{\"start\":\"44902\",\"end\":\"44903\"},{\"start\":\"44913\",\"end\":\"44914\"},{\"start\":\"45243\",\"end\":\"45248\"},{\"start\":\"45256\",\"end\":\"45260\"},{\"start\":\"45272\",\"end\":\"45277\"},{\"start\":\"45289\",\"end\":\"45292\"},{\"start\":\"45301\",\"end\":\"45307\"},{\"start\":\"45314\",\"end\":\"45322\"},{\"start\":\"45631\",\"end\":\"45635\"},{\"start\":\"45648\",\"end\":\"45652\"},{\"start\":\"45661\",\"end\":\"45667\"},{\"start\":\"45675\",\"end\":\"45682\"},{\"start\":\"45688\",\"end\":\"45695\"},{\"start\":\"45702\",\"end\":\"45703\"},{\"start\":\"45712\",\"end\":\"45718\"},{\"start\":\"45719\",\"end\":\"45720\"},{\"start\":\"45726\",\"end\":\"45738\"},{\"start\":\"45748\",\"end\":\"45754\"},{\"start\":\"45766\",\"end\":\"45773\"},{\"start\":\"45774\",\"end\":\"45775\"},{\"start\":\"45784\",\"end\":\"45791\"},{\"start\":\"46180\",\"end\":\"46188\"},{\"start\":\"46195\",\"end\":\"46200\"},{\"start\":\"46208\",\"end\":\"46216\"},{\"start\":\"46226\",\"end\":\"46232\"},{\"start\":\"46566\",\"end\":\"46571\"},{\"start\":\"46578\",\"end\":\"46585\"},{\"start\":\"46592\",\"end\":\"46600\"},{\"start\":\"46608\",\"end\":\"46614\"},{\"start\":\"46621\",\"end\":\"46624\"},{\"start\":\"46630\",\"end\":\"46638\"},{\"start\":\"46646\",\"end\":\"46651\"},{\"start\":\"46658\",\"end\":\"46665\"},{\"start\":\"46986\",\"end\":\"46991\"},{\"start\":\"47000\",\"end\":\"47007\"},{\"start\":\"47016\",\"end\":\"47023\"},{\"start\":\"47024\",\"end\":\"47025\"},{\"start\":\"47035\",\"end\":\"47042\"},{\"start\":\"47382\",\"end\":\"47384\"},{\"start\":\"47391\",\"end\":\"47398\"},{\"start\":\"47405\",\"end\":\"47411\"},{\"start\":\"47417\",\"end\":\"47420\"},{\"start\":\"47425\",\"end\":\"47429\"},{\"start\":\"47436\",\"end\":\"47441\"},{\"start\":\"47446\",\"end\":\"47452\"},{\"start\":\"47918\",\"end\":\"47921\"},{\"start\":\"47926\",\"end\":\"47931\"},{\"start\":\"47940\",\"end\":\"47948\"},{\"start\":\"47955\",\"end\":\"47959\"},{\"start\":\"48391\",\"end\":\"48394\"},{\"start\":\"48399\",\"end\":\"48402\"},{\"start\":\"48409\",\"end\":\"48412\"},{\"start\":\"48419\",\"end\":\"48423\"},{\"start\":\"48429\",\"end\":\"48433\"},{\"start\":\"48440\",\"end\":\"48445\"},{\"start\":\"48845\",\"end\":\"48848\"},{\"start\":\"48854\",\"end\":\"48859\"},{\"start\":\"48865\",\"end\":\"48872\"},{\"start\":\"48879\",\"end\":\"48887\"},{\"start\":\"49215\",\"end\":\"49222\"},{\"start\":\"49229\",\"end\":\"49236\"},{\"start\":\"49244\",\"end\":\"49248\"},{\"start\":\"49258\",\"end\":\"49263\"},{\"start\":\"49264\",\"end\":\"49265\"},{\"start\":\"49611\",\"end\":\"49617\"},{\"start\":\"49625\",\"end\":\"49632\"},{\"start\":\"49640\",\"end\":\"49645\"},{\"start\":\"49656\",\"end\":\"49665\"}]", "bib_author_last_name": "[{\"start\":\"33168\",\"end\":\"33174\"},{\"start\":\"33178\",\"end\":\"33183\"},{\"start\":\"33189\",\"end\":\"33196\"},{\"start\":\"33202\",\"end\":\"33207\"},{\"start\":\"33583\",\"end\":\"33594\"},{\"start\":\"33603\",\"end\":\"33607\"},{\"start\":\"33617\",\"end\":\"33630\"},{\"start\":\"33937\",\"end\":\"33941\"},{\"start\":\"33948\",\"end\":\"33950\"},{\"start\":\"33958\",\"end\":\"33962\"},{\"start\":\"33968\",\"end\":\"33972\"},{\"start\":\"34294\",\"end\":\"34298\"},{\"start\":\"34308\",\"end\":\"34312\"},{\"start\":\"34318\",\"end\":\"34322\"},{\"start\":\"34755\",\"end\":\"34760\"},{\"start\":\"34766\",\"end\":\"34772\"},{\"start\":\"35131\",\"end\":\"35136\"},{\"start\":\"35148\",\"end\":\"35155\"},{\"start\":\"35161\",\"end\":\"35167\"},{\"start\":\"35484\",\"end\":\"35486\"},{\"start\":\"35497\",\"end\":\"35501\"},{\"start\":\"35511\",\"end\":\"35515\"},{\"start\":\"35927\",\"end\":\"35933\"},{\"start\":\"35937\",\"end\":\"35941\"},{\"start\":\"35945\",\"end\":\"35950\"},{\"start\":\"35952\",\"end\":\"35955\"},{\"start\":\"36301\",\"end\":\"36305\"},{\"start\":\"36311\",\"end\":\"36315\"},{\"start\":\"36628\",\"end\":\"36632\"},{\"start\":\"36639\",\"end\":\"36645\"},{\"start\":\"36654\",\"end\":\"36660\"},{\"start\":\"36673\",\"end\":\"36679\"},{\"start\":\"36949\",\"end\":\"36955\"},{\"start\":\"36964\",\"end\":\"36968\"},{\"start\":\"36980\",\"end\":\"36987\"},{\"start\":\"36996\",\"end\":\"37003\"},{\"start\":\"37311\",\"end\":\"37317\"},{\"start\":\"37325\",\"end\":\"37334\"},{\"start\":\"37696\",\"end\":\"37698\"},{\"start\":\"37708\",\"end\":\"37716\"},{\"start\":\"38020\",\"end\":\"38023\"},{\"start\":\"38027\",\"end\":\"38032\"},{\"start\":\"38036\",\"end\":\"38042\"},{\"start\":\"38046\",\"end\":\"38052\"},{\"start\":\"38056\",\"end\":\"38067\"},{\"start\":\"38071\",\"end\":\"38075\"},{\"start\":\"38466\",\"end\":\"38472\"},{\"start\":\"38477\",\"end\":\"38480\"},{\"start\":\"38492\",\"end\":\"38496\"},{\"start\":\"38794\",\"end\":\"38798\"},{\"start\":\"38806\",\"end\":\"38812\"},{\"start\":\"38824\",\"end\":\"38835\"},{\"start\":\"38843\",\"end\":\"38849\"},{\"start\":\"39266\",\"end\":\"39276\"},{\"start\":\"39283\",\"end\":\"39291\"},{\"start\":\"39301\",\"end\":\"39306\"},{\"start\":\"39673\",\"end\":\"39678\"},{\"start\":\"39690\",\"end\":\"39699\"},{\"start\":\"40144\",\"end\":\"40152\"},{\"start\":\"40159\",\"end\":\"40165\"},{\"start\":\"40174\",\"end\":\"40183\"},{\"start\":\"40193\",\"end\":\"40199\"},{\"start\":\"40499\",\"end\":\"40502\"},{\"start\":\"40513\",\"end\":\"40516\"},{\"start\":\"40528\",\"end\":\"40531\"},{\"start\":\"40542\",\"end\":\"40545\"},{\"start\":\"40914\",\"end\":\"40916\"},{\"start\":\"40923\",\"end\":\"40928\"},{\"start\":\"40940\",\"end\":\"40944\"},{\"start\":\"40954\",\"end\":\"40960\"},{\"start\":\"40967\",\"end\":\"40974\"},{\"start\":\"40979\",\"end\":\"40982\"},{\"start\":\"40994\",\"end\":\"41001\"},{\"start\":\"41438\",\"end\":\"41440\"},{\"start\":\"41447\",\"end\":\"41454\"},{\"start\":\"41810\",\"end\":\"41813\"},{\"start\":\"41823\",\"end\":\"41828\"},{\"start\":\"41837\",\"end\":\"41843\"},{\"start\":\"42261\",\"end\":\"42264\"},{\"start\":\"42274\",\"end\":\"42278\"},{\"start\":\"42289\",\"end\":\"42292\"},{\"start\":\"42298\",\"end\":\"42302\"},{\"start\":\"42591\",\"end\":\"42593\"},{\"start\":\"42602\",\"end\":\"42606\"},{\"start\":\"42617\",\"end\":\"42620\"},{\"start\":\"42629\",\"end\":\"42636\"},{\"start\":\"42644\",\"end\":\"42647\"},{\"start\":\"43078\",\"end\":\"43081\"},{\"start\":\"43089\",\"end\":\"43096\"},{\"start\":\"43105\",\"end\":\"43116\"},{\"start\":\"43124\",\"end\":\"43131\"},{\"start\":\"43143\",\"end\":\"43148\"},{\"start\":\"43632\",\"end\":\"43638\"},{\"start\":\"43644\",\"end\":\"43647\"},{\"start\":\"43658\",\"end\":\"43660\"},{\"start\":\"43917\",\"end\":\"43923\"},{\"start\":\"43936\",\"end\":\"43947\"},{\"start\":\"43957\",\"end\":\"43965\"},{\"start\":\"43975\",\"end\":\"43982\"},{\"start\":\"43991\",\"end\":\"44000\"},{\"start\":\"44007\",\"end\":\"44016\"},{\"start\":\"44026\",\"end\":\"44032\"},{\"start\":\"44484\",\"end\":\"44493\"},{\"start\":\"44501\",\"end\":\"44506\"},{\"start\":\"44517\",\"end\":\"44522\"},{\"start\":\"44528\",\"end\":\"44534\"},{\"start\":\"44872\",\"end\":\"44877\"},{\"start\":\"44881\",\"end\":\"44890\"},{\"start\":\"44894\",\"end\":\"44900\"},{\"start\":\"44904\",\"end\":\"44911\"},{\"start\":\"44915\",\"end\":\"44922\"},{\"start\":\"45249\",\"end\":\"45254\"},{\"start\":\"45261\",\"end\":\"45270\"},{\"start\":\"45278\",\"end\":\"45287\"},{\"start\":\"45293\",\"end\":\"45299\"},{\"start\":\"45308\",\"end\":\"45312\"},{\"start\":\"45323\",\"end\":\"45329\"},{\"start\":\"45636\",\"end\":\"45646\"},{\"start\":\"45653\",\"end\":\"45659\"},{\"start\":\"45668\",\"end\":\"45673\"},{\"start\":\"45683\",\"end\":\"45686\"},{\"start\":\"45696\",\"end\":\"45700\"},{\"start\":\"45704\",\"end\":\"45710\"},{\"start\":\"45721\",\"end\":\"45724\"},{\"start\":\"45739\",\"end\":\"45746\"},{\"start\":\"45755\",\"end\":\"45764\"},{\"start\":\"45776\",\"end\":\"45782\"},{\"start\":\"45792\",\"end\":\"45798\"},{\"start\":\"45800\",\"end\":\"45813\"},{\"start\":\"46189\",\"end\":\"46193\"},{\"start\":\"46201\",\"end\":\"46206\"},{\"start\":\"46217\",\"end\":\"46224\"},{\"start\":\"46233\",\"end\":\"46237\"},{\"start\":\"46572\",\"end\":\"46576\"},{\"start\":\"46586\",\"end\":\"46590\"},{\"start\":\"46601\",\"end\":\"46606\"},{\"start\":\"46615\",\"end\":\"46619\"},{\"start\":\"46625\",\"end\":\"46628\"},{\"start\":\"46639\",\"end\":\"46644\"},{\"start\":\"46652\",\"end\":\"46656\"},{\"start\":\"46666\",\"end\":\"46668\"},{\"start\":\"46992\",\"end\":\"46998\"},{\"start\":\"47008\",\"end\":\"47014\"},{\"start\":\"47026\",\"end\":\"47033\"},{\"start\":\"47043\",\"end\":\"47057\"},{\"start\":\"47385\",\"end\":\"47389\"},{\"start\":\"47399\",\"end\":\"47403\"},{\"start\":\"47412\",\"end\":\"47415\"},{\"start\":\"47421\",\"end\":\"47423\"},{\"start\":\"47430\",\"end\":\"47434\"},{\"start\":\"47442\",\"end\":\"47444\"},{\"start\":\"47453\",\"end\":\"47456\"},{\"start\":\"47922\",\"end\":\"47924\"},{\"start\":\"47932\",\"end\":\"47938\"},{\"start\":\"47949\",\"end\":\"47953\"},{\"start\":\"47960\",\"end\":\"47964\"},{\"start\":\"48395\",\"end\":\"48397\"},{\"start\":\"48403\",\"end\":\"48407\"},{\"start\":\"48413\",\"end\":\"48417\"},{\"start\":\"48424\",\"end\":\"48427\"},{\"start\":\"48434\",\"end\":\"48438\"},{\"start\":\"48446\",\"end\":\"48451\"},{\"start\":\"48849\",\"end\":\"48852\"},{\"start\":\"48860\",\"end\":\"48863\"},{\"start\":\"48873\",\"end\":\"48877\"},{\"start\":\"48888\",\"end\":\"48891\"},{\"start\":\"49223\",\"end\":\"49227\"},{\"start\":\"49237\",\"end\":\"49242\"},{\"start\":\"49249\",\"end\":\"49256\"},{\"start\":\"49266\",\"end\":\"49270\"},{\"start\":\"49618\",\"end\":\"49623\"},{\"start\":\"49633\",\"end\":\"49638\"},{\"start\":\"49646\",\"end\":\"49654\"},{\"start\":\"49666\",\"end\":\"49673\"}]", "bib_entry": "[{\"start\":\"33102\",\"end\":\"33495\",\"attributes\":{\"matched_paper_id\":\"4637111\",\"id\":\"b0\"}},{\"start\":\"33497\",\"end\":\"33885\",\"attributes\":{\"matched_paper_id\":\"2198511\",\"id\":\"b1\"}},{\"start\":\"33887\",\"end\":\"34210\",\"attributes\":{\"matched_paper_id\":\"14395783\",\"id\":\"b2\"}},{\"start\":\"34212\",\"end\":\"34639\",\"attributes\":{\"matched_paper_id\":\"49416362\",\"id\":\"b3\"}},{\"start\":\"34641\",\"end\":\"35048\",\"attributes\":{\"matched_paper_id\":\"102496818\",\"id\":\"b4\"}},{\"start\":\"35050\",\"end\":\"35413\",\"attributes\":{\"matched_paper_id\":\"2255738\",\"id\":\"b5\"}},{\"start\":\"35415\",\"end\":\"35863\",\"attributes\":{\"matched_paper_id\":\"46968214\",\"id\":\"b6\"}},{\"start\":\"35865\",\"end\":\"36219\",\"attributes\":{\"matched_paper_id\":\"1203247\",\"id\":\"b7\"}},{\"start\":\"36221\",\"end\":\"36562\",\"attributes\":{\"matched_paper_id\":\"299085\",\"id\":\"b8\"}},{\"start\":\"36564\",\"end\":\"36878\",\"attributes\":{\"id\":\"b9\",\"doi\":\"arXiv:1904.05822\"}},{\"start\":\"36880\",\"end\":\"37224\",\"attributes\":{\"matched_paper_id\":\"9455111\",\"id\":\"b10\"}},{\"start\":\"37226\",\"end\":\"37641\",\"attributes\":{\"id\":\"b11\"}},{\"start\":\"37643\",\"end\":\"37946\",\"attributes\":{\"id\":\"b12\"}},{\"start\":\"37948\",\"end\":\"38384\",\"attributes\":{\"matched_paper_id\":\"3759573\",\"id\":\"b13\"}},{\"start\":\"38386\",\"end\":\"38722\",\"attributes\":{\"matched_paper_id\":\"12230426\",\"id\":\"b14\"}},{\"start\":\"38724\",\"end\":\"39192\",\"attributes\":{\"matched_paper_id\":\"19115169\",\"id\":\"b15\"}},{\"start\":\"39194\",\"end\":\"39599\",\"attributes\":{\"matched_paper_id\":\"16790081\",\"id\":\"b16\"}},{\"start\":\"39601\",\"end\":\"40038\",\"attributes\":{\"matched_paper_id\":\"11091110\",\"id\":\"b17\"}},{\"start\":\"40040\",\"end\":\"40425\",\"attributes\":{\"id\":\"b18\",\"doi\":\"arXiv:1907.01341\"}},{\"start\":\"40427\",\"end\":\"40840\",\"attributes\":{\"matched_paper_id\":\"52834247\",\"id\":\"b19\"}},{\"start\":\"40842\",\"end\":\"41358\",\"attributes\":{\"matched_paper_id\":\"131775632\",\"id\":\"b20\"}},{\"start\":\"41360\",\"end\":\"41739\",\"attributes\":{\"matched_paper_id\":\"4572038\",\"id\":\"b21\"}},{\"start\":\"41741\",\"end\":\"42119\",\"attributes\":{\"matched_paper_id\":\"3018706\",\"id\":\"b22\"}},{\"start\":\"42121\",\"end\":\"42500\",\"attributes\":{\"id\":\"b23\"}},{\"start\":\"42502\",\"end\":\"42966\",\"attributes\":{\"matched_paper_id\":\"4797810\",\"id\":\"b24\"}},{\"start\":\"42968\",\"end\":\"43508\",\"attributes\":{\"matched_paper_id\":\"206594095\",\"id\":\"b25\"}},{\"start\":\"43510\",\"end\":\"43826\",\"attributes\":{\"id\":\"b26\"}},{\"start\":\"43828\",\"end\":\"44415\",\"attributes\":{\"matched_paper_id\":\"20603040\",\"id\":\"b27\"}},{\"start\":\"44417\",\"end\":\"44814\",\"attributes\":{\"matched_paper_id\":\"545361\",\"id\":\"b28\"}},{\"start\":\"44816\",\"end\":\"45216\",\"attributes\":{\"matched_paper_id\":\"206942855\",\"id\":\"b29\"}},{\"start\":\"45218\",\"end\":\"45580\",\"attributes\":{\"matched_paper_id\":\"206429195\",\"id\":\"b30\"}},{\"start\":\"45582\",\"end\":\"46107\",\"attributes\":{\"id\":\"b31\",\"doi\":\"arxiv: 1908.00463\"}},{\"start\":\"46109\",\"end\":\"46510\",\"attributes\":{\"matched_paper_id\":\"131775376\",\"id\":\"b32\"}},{\"start\":\"46512\",\"end\":\"46945\",\"attributes\":{\"matched_paper_id\":\"53015351\",\"id\":\"b33\"}},{\"start\":\"46947\",\"end\":\"47310\",\"attributes\":{\"matched_paper_id\":\"202676783\",\"id\":\"b34\"}},{\"start\":\"47312\",\"end\":\"47799\",\"attributes\":{\"matched_paper_id\":\"52860134\",\"id\":\"b35\"}},{\"start\":\"47801\",\"end\":\"48301\",\"attributes\":{\"matched_paper_id\":\"21670200\",\"id\":\"b36\"}},{\"start\":\"48303\",\"end\":\"48771\",\"attributes\":{\"matched_paper_id\":\"4469249\",\"id\":\"b37\"}},{\"start\":\"48773\",\"end\":\"49155\",\"attributes\":{\"matched_paper_id\":\"198968133\",\"id\":\"b38\"}},{\"start\":\"49157\",\"end\":\"49556\",\"attributes\":{\"matched_paper_id\":\"11977588\",\"id\":\"b39\"}},{\"start\":\"49558\",\"end\":\"49932\",\"attributes\":{\"matched_paper_id\":\"10845551\",\"id\":\"b40\"}}]", "bib_title": "[{\"start\":\"33102\",\"end\":\"33162\"},{\"start\":\"33497\",\"end\":\"33576\"},{\"start\":\"33887\",\"end\":\"33927\"},{\"start\":\"34212\",\"end\":\"34284\"},{\"start\":\"34641\",\"end\":\"34747\"},{\"start\":\"35050\",\"end\":\"35123\"},{\"start\":\"35415\",\"end\":\"35477\"},{\"start\":\"35865\",\"end\":\"35923\"},{\"start\":\"36221\",\"end\":\"36294\"},{\"start\":\"36880\",\"end\":\"36939\"},{\"start\":\"37226\",\"end\":\"37301\"},{\"start\":\"37643\",\"end\":\"37686\"},{\"start\":\"37948\",\"end\":\"38016\"},{\"start\":\"38386\",\"end\":\"38458\"},{\"start\":\"38724\",\"end\":\"38785\"},{\"start\":\"39194\",\"end\":\"39257\"},{\"start\":\"39601\",\"end\":\"39667\"},{\"start\":\"40427\",\"end\":\"40489\"},{\"start\":\"40842\",\"end\":\"40904\"},{\"start\":\"41360\",\"end\":\"41428\"},{\"start\":\"41741\",\"end\":\"41801\"},{\"start\":\"42502\",\"end\":\"42580\"},{\"start\":\"42968\",\"end\":\"43069\"},{\"start\":\"43828\",\"end\":\"43908\"},{\"start\":\"44417\",\"end\":\"44475\"},{\"start\":\"44816\",\"end\":\"44868\"},{\"start\":\"45218\",\"end\":\"45241\"},{\"start\":\"46109\",\"end\":\"46178\"},{\"start\":\"46512\",\"end\":\"46564\"},{\"start\":\"46947\",\"end\":\"46984\"},{\"start\":\"47312\",\"end\":\"47380\"},{\"start\":\"47801\",\"end\":\"47916\"},{\"start\":\"48303\",\"end\":\"48389\"},{\"start\":\"48773\",\"end\":\"48843\"},{\"start\":\"49157\",\"end\":\"49213\"},{\"start\":\"49558\",\"end\":\"49609\"}]", "bib_author": "[{\"start\":\"33164\",\"end\":\"33176\"},{\"start\":\"33176\",\"end\":\"33185\"},{\"start\":\"33185\",\"end\":\"33198\"},{\"start\":\"33198\",\"end\":\"33209\"},{\"start\":\"33578\",\"end\":\"33596\"},{\"start\":\"33596\",\"end\":\"33609\"},{\"start\":\"33609\",\"end\":\"33632\"},{\"start\":\"33929\",\"end\":\"33943\"},{\"start\":\"33943\",\"end\":\"33952\"},{\"start\":\"33952\",\"end\":\"33964\"},{\"start\":\"33964\",\"end\":\"33974\"},{\"start\":\"34286\",\"end\":\"34300\"},{\"start\":\"34300\",\"end\":\"34314\"},{\"start\":\"34314\",\"end\":\"34324\"},{\"start\":\"34749\",\"end\":\"34762\"},{\"start\":\"34762\",\"end\":\"34774\"},{\"start\":\"35125\",\"end\":\"35138\"},{\"start\":\"35138\",\"end\":\"35157\"},{\"start\":\"35157\",\"end\":\"35169\"},{\"start\":\"35479\",\"end\":\"35488\"},{\"start\":\"35488\",\"end\":\"35503\"},{\"start\":\"35503\",\"end\":\"35517\"},{\"start\":\"35925\",\"end\":\"35935\"},{\"start\":\"35935\",\"end\":\"35943\"},{\"start\":\"35943\",\"end\":\"35952\"},{\"start\":\"35952\",\"end\":\"35957\"},{\"start\":\"36296\",\"end\":\"36307\"},{\"start\":\"36307\",\"end\":\"36317\"},{\"start\":\"36622\",\"end\":\"36634\"},{\"start\":\"36634\",\"end\":\"36647\"},{\"start\":\"36647\",\"end\":\"36662\"},{\"start\":\"36662\",\"end\":\"36681\"},{\"start\":\"36941\",\"end\":\"36957\"},{\"start\":\"36957\",\"end\":\"36970\"},{\"start\":\"36970\",\"end\":\"36989\"},{\"start\":\"36989\",\"end\":\"37005\"},{\"start\":\"37303\",\"end\":\"37319\"},{\"start\":\"37319\",\"end\":\"37336\"},{\"start\":\"37336\",\"end\":\"37348\"},{\"start\":\"37688\",\"end\":\"37700\"},{\"start\":\"37700\",\"end\":\"37718\"},{\"start\":\"38018\",\"end\":\"38025\"},{\"start\":\"38025\",\"end\":\"38034\"},{\"start\":\"38034\",\"end\":\"38044\"},{\"start\":\"38044\",\"end\":\"38054\"},{\"start\":\"38054\",\"end\":\"38069\"},{\"start\":\"38069\",\"end\":\"38077\"},{\"start\":\"38460\",\"end\":\"38474\"},{\"start\":\"38474\",\"end\":\"38482\"},{\"start\":\"38482\",\"end\":\"38498\"},{\"start\":\"38787\",\"end\":\"38800\"},{\"start\":\"38800\",\"end\":\"38814\"},{\"start\":\"38814\",\"end\":\"38837\"},{\"start\":\"38837\",\"end\":\"38851\"},{\"start\":\"39259\",\"end\":\"39278\"},{\"start\":\"39278\",\"end\":\"39293\"},{\"start\":\"39293\",\"end\":\"39308\"},{\"start\":\"39669\",\"end\":\"39680\"},{\"start\":\"39680\",\"end\":\"39701\"},{\"start\":\"40137\",\"end\":\"40154\"},{\"start\":\"40154\",\"end\":\"40167\"},{\"start\":\"40167\",\"end\":\"40185\"},{\"start\":\"40185\",\"end\":\"40201\"},{\"start\":\"40491\",\"end\":\"40504\"},{\"start\":\"40504\",\"end\":\"40518\"},{\"start\":\"40518\",\"end\":\"40533\"},{\"start\":\"40533\",\"end\":\"40547\"},{\"start\":\"40906\",\"end\":\"40918\"},{\"start\":\"40918\",\"end\":\"40930\"},{\"start\":\"40930\",\"end\":\"40946\"},{\"start\":\"40946\",\"end\":\"40962\"},{\"start\":\"40962\",\"end\":\"40976\"},{\"start\":\"40976\",\"end\":\"40984\"},{\"start\":\"40984\",\"end\":\"41003\"},{\"start\":\"41430\",\"end\":\"41442\"},{\"start\":\"41442\",\"end\":\"41456\"},{\"start\":\"41803\",\"end\":\"41815\"},{\"start\":\"41815\",\"end\":\"41830\"},{\"start\":\"41830\",\"end\":\"41845\"},{\"start\":\"42255\",\"end\":\"42266\"},{\"start\":\"42266\",\"end\":\"42280\"},{\"start\":\"42280\",\"end\":\"42294\"},{\"start\":\"42294\",\"end\":\"42304\"},{\"start\":\"42582\",\"end\":\"42595\"},{\"start\":\"42595\",\"end\":\"42608\"},{\"start\":\"42608\",\"end\":\"42622\"},{\"start\":\"42622\",\"end\":\"42638\"},{\"start\":\"42638\",\"end\":\"42649\"},{\"start\":\"43071\",\"end\":\"43083\"},{\"start\":\"43083\",\"end\":\"43098\"},{\"start\":\"43098\",\"end\":\"43118\"},{\"start\":\"43118\",\"end\":\"43133\"},{\"start\":\"43133\",\"end\":\"43150\"},{\"start\":\"43623\",\"end\":\"43640\"},{\"start\":\"43640\",\"end\":\"43649\"},{\"start\":\"43649\",\"end\":\"43662\"},{\"start\":\"43910\",\"end\":\"43925\"},{\"start\":\"43925\",\"end\":\"43949\"},{\"start\":\"43949\",\"end\":\"43967\"},{\"start\":\"43967\",\"end\":\"43984\"},{\"start\":\"43984\",\"end\":\"44002\"},{\"start\":\"44002\",\"end\":\"44018\"},{\"start\":\"44018\",\"end\":\"44034\"},{\"start\":\"44477\",\"end\":\"44495\"},{\"start\":\"44495\",\"end\":\"44508\"},{\"start\":\"44508\",\"end\":\"44524\"},{\"start\":\"44524\",\"end\":\"44536\"},{\"start\":\"44870\",\"end\":\"44879\"},{\"start\":\"44879\",\"end\":\"44892\"},{\"start\":\"44892\",\"end\":\"44902\"},{\"start\":\"44902\",\"end\":\"44913\"},{\"start\":\"44913\",\"end\":\"44924\"},{\"start\":\"45243\",\"end\":\"45256\"},{\"start\":\"45256\",\"end\":\"45272\"},{\"start\":\"45272\",\"end\":\"45289\"},{\"start\":\"45289\",\"end\":\"45301\"},{\"start\":\"45301\",\"end\":\"45314\"},{\"start\":\"45314\",\"end\":\"45331\"},{\"start\":\"45631\",\"end\":\"45648\"},{\"start\":\"45648\",\"end\":\"45661\"},{\"start\":\"45661\",\"end\":\"45675\"},{\"start\":\"45675\",\"end\":\"45688\"},{\"start\":\"45688\",\"end\":\"45702\"},{\"start\":\"45702\",\"end\":\"45712\"},{\"start\":\"45712\",\"end\":\"45726\"},{\"start\":\"45726\",\"end\":\"45748\"},{\"start\":\"45748\",\"end\":\"45766\"},{\"start\":\"45766\",\"end\":\"45784\"},{\"start\":\"45784\",\"end\":\"45800\"},{\"start\":\"45800\",\"end\":\"45815\"},{\"start\":\"46180\",\"end\":\"46195\"},{\"start\":\"46195\",\"end\":\"46208\"},{\"start\":\"46208\",\"end\":\"46226\"},{\"start\":\"46226\",\"end\":\"46239\"},{\"start\":\"46566\",\"end\":\"46578\"},{\"start\":\"46578\",\"end\":\"46592\"},{\"start\":\"46592\",\"end\":\"46608\"},{\"start\":\"46608\",\"end\":\"46621\"},{\"start\":\"46621\",\"end\":\"46630\"},{\"start\":\"46630\",\"end\":\"46646\"},{\"start\":\"46646\",\"end\":\"46658\"},{\"start\":\"46658\",\"end\":\"46670\"},{\"start\":\"46986\",\"end\":\"47000\"},{\"start\":\"47000\",\"end\":\"47016\"},{\"start\":\"47016\",\"end\":\"47035\"},{\"start\":\"47035\",\"end\":\"47059\"},{\"start\":\"47382\",\"end\":\"47391\"},{\"start\":\"47391\",\"end\":\"47405\"},{\"start\":\"47405\",\"end\":\"47417\"},{\"start\":\"47417\",\"end\":\"47425\"},{\"start\":\"47425\",\"end\":\"47436\"},{\"start\":\"47436\",\"end\":\"47446\"},{\"start\":\"47446\",\"end\":\"47458\"},{\"start\":\"47918\",\"end\":\"47926\"},{\"start\":\"47926\",\"end\":\"47940\"},{\"start\":\"47940\",\"end\":\"47955\"},{\"start\":\"47955\",\"end\":\"47966\"},{\"start\":\"48391\",\"end\":\"48399\"},{\"start\":\"48399\",\"end\":\"48409\"},{\"start\":\"48409\",\"end\":\"48419\"},{\"start\":\"48419\",\"end\":\"48429\"},{\"start\":\"48429\",\"end\":\"48440\"},{\"start\":\"48440\",\"end\":\"48453\"},{\"start\":\"48845\",\"end\":\"48854\"},{\"start\":\"48854\",\"end\":\"48865\"},{\"start\":\"48865\",\"end\":\"48879\"},{\"start\":\"48879\",\"end\":\"48893\"},{\"start\":\"49215\",\"end\":\"49229\"},{\"start\":\"49229\",\"end\":\"49244\"},{\"start\":\"49244\",\"end\":\"49258\"},{\"start\":\"49258\",\"end\":\"49272\"},{\"start\":\"49611\",\"end\":\"49625\"},{\"start\":\"49625\",\"end\":\"49640\"},{\"start\":\"49640\",\"end\":\"49656\"},{\"start\":\"49656\",\"end\":\"49675\"}]", "bib_venue": "[{\"start\":\"33257\",\"end\":\"33297\"},{\"start\":\"34378\",\"end\":\"34424\"},{\"start\":\"34818\",\"end\":\"34854\"},{\"start\":\"35571\",\"end\":\"35617\"},{\"start\":\"36011\",\"end\":\"36057\"},{\"start\":\"36365\",\"end\":\"36405\"},{\"start\":\"37402\",\"end\":\"37448\"},{\"start\":\"37762\",\"end\":\"37798\"},{\"start\":\"38131\",\"end\":\"38177\"},{\"start\":\"38911\",\"end\":\"38963\"},{\"start\":\"39362\",\"end\":\"39408\"},{\"start\":\"39740\",\"end\":\"39771\"},{\"start\":\"40601\",\"end\":\"40647\"},{\"start\":\"41057\",\"end\":\"41103\"},{\"start\":\"41510\",\"end\":\"41556\"},{\"start\":\"41899\",\"end\":\"41945\"},{\"start\":\"42703\",\"end\":\"42749\"},{\"start\":\"43204\",\"end\":\"43250\"},{\"start\":\"44088\",\"end\":\"44134\"},{\"start\":\"44584\",\"end\":\"44624\"},{\"start\":\"44978\",\"end\":\"45024\"},{\"start\":\"45372\",\"end\":\"45405\"},{\"start\":\"46281\",\"end\":\"46315\"},{\"start\":\"46692\",\"end\":\"46706\"},{\"start\":\"47103\",\"end\":\"47139\"},{\"start\":\"47512\",\"end\":\"47558\"},{\"start\":\"48020\",\"end\":\"48066\"},{\"start\":\"48507\",\"end\":\"48553\"},{\"start\":\"48937\",\"end\":\"48973\"},{\"start\":\"49326\",\"end\":\"49372\"},{\"start\":\"49719\",\"end\":\"49755\"},{\"start\":\"33209\",\"end\":\"33255\"},{\"start\":\"33632\",\"end\":\"33681\"},{\"start\":\"33974\",\"end\":\"34023\"},{\"start\":\"34324\",\"end\":\"34376\"},{\"start\":\"34774\",\"end\":\"34816\"},{\"start\":\"35169\",\"end\":\"35218\"},{\"start\":\"35517\",\"end\":\"35569\"},{\"start\":\"35957\",\"end\":\"36009\"},{\"start\":\"36317\",\"end\":\"36363\"},{\"start\":\"36564\",\"end\":\"36620\"},{\"start\":\"37005\",\"end\":\"37033\"},{\"start\":\"37348\",\"end\":\"37400\"},{\"start\":\"37718\",\"end\":\"37760\"},{\"start\":\"38077\",\"end\":\"38129\"},{\"start\":\"38498\",\"end\":\"38546\"},{\"start\":\"38851\",\"end\":\"38909\"},{\"start\":\"39308\",\"end\":\"39360\"},{\"start\":\"39701\",\"end\":\"39738\"},{\"start\":\"40040\",\"end\":\"40135\"},{\"start\":\"40547\",\"end\":\"40599\"},{\"start\":\"41003\",\"end\":\"41055\"},{\"start\":\"41456\",\"end\":\"41508\"},{\"start\":\"41845\",\"end\":\"41897\"},{\"start\":\"42121\",\"end\":\"42253\"},{\"start\":\"42649\",\"end\":\"42701\"},{\"start\":\"43150\",\"end\":\"43202\"},{\"start\":\"43510\",\"end\":\"43621\"},{\"start\":\"44034\",\"end\":\"44086\"},{\"start\":\"44536\",\"end\":\"44582\"},{\"start\":\"44924\",\"end\":\"44976\"},{\"start\":\"45331\",\"end\":\"45370\"},{\"start\":\"45582\",\"end\":\"45629\"},{\"start\":\"46239\",\"end\":\"46279\"},{\"start\":\"46670\",\"end\":\"46690\"},{\"start\":\"47059\",\"end\":\"47101\"},{\"start\":\"47458\",\"end\":\"47510\"},{\"start\":\"47966\",\"end\":\"48018\"},{\"start\":\"48453\",\"end\":\"48505\"},{\"start\":\"48893\",\"end\":\"48935\"},{\"start\":\"49272\",\"end\":\"49324\"},{\"start\":\"49675\",\"end\":\"49717\"}]"}}}, "year": 2023, "month": 12, "day": 17}