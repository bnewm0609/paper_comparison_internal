{"id": 52100616, "updated": "2023-10-02 21:50:23.33", "metadata": {"title": "A Study of Reinforcement Learning for Neural Machine Translation", "authors": "[{\"first\":\"Lijun\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Fei\",\"last\":\"Tian\",\"middle\":[]},{\"first\":\"Tao\",\"last\":\"Qin\",\"middle\":[]},{\"first\":\"Jianhuang\",\"last\":\"Lai\",\"middle\":[]},{\"first\":\"Tie-Yan\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system. However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged. In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning. We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training. Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data. By integrating all our findings, we obtain competitive results on WMT14 English-German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1808.08866", "mag": "2952950989", "acl": "D18-1397", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/WuTQLL18", "doi": "10.18653/v1/d18-1397"}}, "content": {"source": {"pdf_hash": "f54b36edae733ab9cd7a748595947710bd28a2e3", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/D18-1397.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/D18-1397.pdf", "status": "HYBRID"}}, "grobid": {"id": "5da3282b557ad5b7eb05945b5973f8bf75284f43", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f54b36edae733ab9cd7a748595947710bd28a2e3.txt", "contents": "\nA Study of Reinforcement Learning for Neural Machine Translation\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsOctober 31 -November 4. 2018. 2018\n\nLijun Wu wulijun3@mail2.sysu.edu.cn \nSchool of Data and Computer Science\nSun Yat-sen University\n\n\nFei Tian \nMicrosoft Research\n\n\nTao Qin taoqin@microsoft.com \nMicrosoft Research\n\n\nJianhuang Lai \nSchool of Data and Computer Science\nSun Yat-sen University\n\n\nTie-Yan Liu tyliu@microsoft.com \nMicrosoft Research\n\n\nA Study of Reinforcement Learning for Neural Machine Translation\n\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\nthe 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics3612October 31 -November 4. 2018. 2018\nRecent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system. However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged. In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning. We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training. Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data. By integrating all our findings, we obtain competitive results on WMT14 English-German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.\n\nIntroduction\n\nRecently, neural machine translation (NMT) (Bahdanau et al., 2015;Hassan et al., 2018;He et al., 2017;Xia et al., 2016Wu et al., 2018b,a) has become more and more popular given its superior performance without the demand of heavily hand-crafted engineering efforts. It is usually trained to maximize the likelihood of each token in the target sentence, by taking the source sentence and the preceding (ground-truth) target tokens as inputs. Such training approach is referred as maximum likelihood estimation (MLE) (Scholz, 1985). Although easy to implement, the token-level * This work was conducted at Microsoft Research Asia. objective function during training is inconsistent with sequence-level evaluation metrics such as BLEU (Papineni et al., 2002).\n\nTo address the inconsistency issue, reinforcement learning (RL) methods have been adopted to optimize sequence-level objectives. For example, policy optimization methods such as REINFORCE (Ranzato et al., 2016;Wu et al., 2017b) and actorcritic (Bahdanau et al., 2017) are leveraged for sequence generation tasks including NMT. In machine translation community, a similar method is proposed with the name 'minimum risk training' (Shen et al., 2016). All these works demonstrate the effectiveness of RL techniques for NMT models .\n\nHowever, effectively applying RL to real-world NMT systems has not been fulfilled by previous works. First, most of, if not all, previous works verified their methods based on shallow recurrent neural network (RNN) models. However, to obtain state-of-the-art (SOTA) performance, it is essential to leverage recently derived deep models (Gehring et al., 2017;Vaswani et al., 2017), which are much more powerful.\n\nSecond, it is not easy to make RL practically effective given quite a few widely acknowledged limitations of RL method (Henderson et al., 2018) such as high variance of gradient estimation (Weaver and Tao, 2001), and objective instability (Mnih et al., 2013). Therefore, several tricks are proposed in previous works. However, it remains unclear, and no agreement is achieved on how to use these tricks in machine translation. For example, baseline reward method (Weaver and Tao, 2001) is suggested in (Ranzato et al., 2016;Nguyen et al., 2017; but not leveraged in (He and Deng, 2012;Shen et al., 2016).\n\nThird, large-scale datasets, especially monolingual datasets are shown to significantly improve translation quality (Sennrich et al., 2015a;Xia et al., 2016) with MLE training, while it remains nearly empty on how to combine RL with monolingual data in NMT.\n\nIn this paper, we try to fulfill these gaps and study how to practically apply RL to obtain strong NMT systems with quite competitive, even stateof-the-art performance. Several comprehensive studies are conducted on different aspects of RL training to figure out how to: 1) set efficient rewards; 2) combine MLE and RL objectives with different weights, which aims to stabilize the training procedure; 3) reduce the variance of gradient estimation.\n\nIn addition, given the effectiveness of leveraging monolingual data in improving translation quality, we further propose a new method to combine the strength of both RL training and source/target monolingual data. To the best of our knowledge, this is the first work that tries to explore the power of monolingual data when training NMT model with RL method.\n\nWe obtain some useful findings through the experiments on WMT17 Chinese-English (Zh-En), WMT17 English-Chinese (En-Zh) and WMT14 English-German (En-De) translation tasks. For instance, multinomial sampling is better than beam search in reward computation, and the combination of RL and monolingual data significantly enhances the NMT model performance. Our main contributions are summarized as follows.\n\n\u2022 We provide the first comprehensive study on different aspects of RL training, such as how to setup reward and baseline reward, on top of quite competitive NMT models.\n\n\u2022 We propose a new method that effectively leverages large-scale monolingual data, from both the source and target side, when training NMT models with RL.\n\n\u2022 Combined with several of our findings and method, we obtain the SOTA translation quality on WMT17 Zh-En translation task, surpassing strong baseline (Transformer big model + back translation) by nearly 1.5 BLEU points. Furthermore, on WMT14 En-De and WMT17 En-Zh translation tasks, we can also obtain strong competitive results.\n\nWe hope that our studies and findings will benefit the community to better understand and leverage reinforcement learning for developing strong NMT models, especially in real-world scenarios faced with deep models and large amount of training data (including both parallel and monolingual data). Towards this end, we open source all our codes/dataset at https://github.com/ apeterswu/RL4NMT to provide a clear recipe for performance reproduction.\n\n\nBackground\n\nIn this section, we first introduce the attentionbased sequence-to-sequence learning framework for neural machine translation (NMT), and then introduce the basis of applying reinforcement learning to training NMT models.\n\n\nNeural Machine Translation\n\nTypical NMT models are based on the encoderdecoder framework with attention mechanism. The encoder first maps a source sentence x = (x 1 , x 2 , ..., x n ) to a set of continuous representations z = (z 1 , z 2 , ..., z n ). Given z, the decoder then generates a target sentence y = (y 1 , y 2 , ..., y m ) of word tokens one by one. At each decoding step t of model training, the probability of generating a token y t is maximized conditioned on x and y <t = (y 1 , ..., y t\u22121 ). Given N training sentence pairs {x i , y i } N i=1 , maximum likelihood estimation (MLE) is usually adopted to optimize the model, and the training objective is defined as:\nL mle = N i=1 log p(y i |x i ) = N i=1 m t=1 log p(y i t |y i 1 , ..., y i t\u22121 , x i ),(1)\nwhere m is the length of sentence y i . Among all the encoder-decoder models, the recently proposed Transformer (Vaswani et al., 2017) architecture achieves the best translation quality so far. The main difference between Transformer and previous RNNSearch (Bahdanau et al., 2015) or ConvS2S (Gehring et al., 2017) is that Transformer relies entirely on self-attention  to compute representations of source and target side sentences, without using recurrent or convolutional operations.\n\n\nTraining NMT with Reinforcement Learning\n\nAs aforementioned, reinforcement learning (RL) is leveraged to bridge the gap between training and inference of NMT, by directly optimizing the evaluation measure (e.g., BLEU) at training time. Specifically, NMT model can be viewed as an agent, which interacts with the environment (the previous words y <t and the context vector z available at each step t). The parameters of the agent define a policy, i.e., a conditional probability p(y t |x, y <t ). The agent will pick an action , i.e., a candidate word out from the vocabulary, according to the policy. A terminal reward is observed once the agent generates a complete sequence\u0177. The reward for machine translation is the BLEU (Papineni et al., 2002) score, denoted as R(\u0177, y), which is defined by comparing the generated\u0177 with the ground-truth sentence y. Note that here the reward R(\u0177, y) is the sentence-level reward, i.e., a scalar for each complete sentence\u0177. The goal of the RL training is to maximize the expected reward:\nL rl = N i=1 E\u0177 \u223cp(\u0177|x i ) R(\u0177, y i ) = N i=1 \u0177\u2208Y p(\u0177|x i )R(\u0177, y i ),(2)\nwhere Y is the space of all candidate translation sentences, which is exponentially large due to the large vocabulary size, making it impossible to exactly maximize L rl . In practice, REIN-FORCE (Williams, 1992) is usually leveraged to approximate the above expectation via sampling\u0177 from the policy p(y|x), leading to the objective as maximizing:\nL rl = N i=1 R(\u0177 i , y i ),\u0177 i \u223c p(y|x i ), \u2200i \u2208 [N ]. (3)\nThroughout the paper we will use REINFORCE as our policy optimization method for RL training.\n\n\nStrategies for RL Training\n\nAlthough training NMT with RL can fill in the gap between training objectives and evaluation metrics, it is not easy to successfully put RL training into practice. A key challenge is that RL methods are highly unstable and inefficient, due to the noise in gradient estimation and reward computation. To our best knowledge, currently there is no consensus, or even a systematic study on how to configure different setups for RL training to avoid such problems, especially for training deep NMT models on large scale datasets. We therefore aim to shed light on practical applications of RL for NMT training. For this purpose, we provide a comprehensive review of several important methods to stabilize RL training process in this section.\n\n\nReward Computation\n\nIt is critical to set up appropriate rewards for RL training, i.e., the R(\u0177, y) in Eqn. (3). There are two important aspects to consider in configuring the reward R(\u0177, y): how to sample training instance\u0177 and whether to use reward shaping.\n\nGenerate\u0177 There are two strategies to sample\u0177 for computing the BLEU reward R(\u0177, y). The first one is beam search (Sutskever et al., 2014), it is a breadth-first search method that maintains a \"beam\" of the top-K scoring candidates (prefix hypothesis sentences) at each generation step. Then, for each candidate sentence in the beam, K most likely words are appended, resulting in a pool of K \u00d7 K new candidates. Out from this pool, the top-K translations with largest probabilities are selected, and the beam search process continues. The second strategy is multinomial sampling (Chatterjee and Cancedda, 2010), which produces each word one by one through multinomial sampling over the model's output distribution. Both sampling strategies terminate the expansion of a candidate sentence when an 'end of sentence' (<EOS>) token is met.\n\nThe choice of different sampling strategies reflects the exploration-exploitation dilemma. Beam search strategy generates more accurate\u0177 by exploiting the probabilistic space output via current NMT model, while multinomial sampling pays more attention to explore more diverse candidates.\n\nWhether to Use Reward Shaping From Eqn.\n\n(3) we can see that for the entire sequence\u0177, there is only one terminal reward R(\u0177, y) available for model training. Note that the agent needs to take tens of actions (with the number depending on the length of\u0177) to generate a complete sentence\u0177, but only one reward is available for all those actions. Consequently, RL training is inefficient due to the sparsity of rewards, and the model updates each token in the training sentence with the same reward value without distinction. Reward shaping (Ng et al., 1999) is a strategy to overcome this shortcoming. In reward shaping, intermediate reward at each decoding step t is imposed and denoted as r t (\u0177 t , y). Bahdanau et al. (2017) sets up the intermediate reward as r t (\u0177 t , y) = R(\u0177 1...t , y) \u2212 R(\u0177 1...t\u22121 , y), where R(\u0177 1...t , y) is defined as the BLEU score of\u0177 1...t with respect to y. Note that we have R(\u0177, y) = m t=1 r t (\u0177 t , y), where m is the length of\u0177. During RL training, the cumulative reward m \u03c4 =t r \u03c4 (\u0177 \u03c4 , y) is used to update the policy at time step t. It is verified that using the shaped reward r t instead of awarding the whole score R(\u0177, y) does not change the optimal policy (Ng et al., 1999).\n\n\nVariance Reduction of Gradient Estimation\n\nAs mentioned before, the REINFORCE algorithm suffers from high variance in gradient estimation, mainly caused by using single sample\u0177 to estimate the expectation. To reduce the variance, Ranzato et al. (2016) subtracts an average reward from the returned reward at each time step t, and the actual reward used to update the policy is\nR(\u0177, y) \u2212r t ,(4)\nwherer t is the estimated average reward at step t, named as baseline reward (Weaver and Tao, 2001). Together with reward shaping, the updated reward becomes m \u03c4 =t r \u03c4 (\u0177 \u03c4 , y) \u2212r t at step t. Intuitively speaking, a baseline rewardr t is established, which either encourages a word choic\u00ea y t if the induced reward R satisfies R >r t , or discourages it if R <r t . Here R is either the terminal reward R(\u0177, y) or the cumulative reward m \u03c4 =t r \u03c4 (\u0177 \u03c4 , y). Such estimated baseline reward r t is designed to decrease the high variance of the gradient estimator.\n\nIn practice, the baseline rewardr t can be obtained through different approaches. For example, one may sample multiple sentences and use the mean terminal reward for these sentences as baseline reward. In our work, we adopt the function learning approach, using simple network (e.g., multi-layer perceptron) to build the learning function, which is the same as used in (Ranzato et al., 2016;Bahdanau et al., 2017).\n\n\nCombine MLE and RL Objectives\n\nThe last important strategy we would like to mention is the combination of MLE training objective with RL objective, which is assumed to further stabilize RL training process Li et al., 2017;Wu et al., 2017a).\n\nA simple way is to linearly combine the MLE (Eqn. (1)) and RL (Eqn. (3)) objectives as follows:\nL com = \u03b1 * L mle + (1 \u2212 \u03b1) * L rl ,(5)\nwhere \u03b1 is the hyperparamter controlling the tradeoff between MLE and RL objectives. We will empirically evaluate how different values of \u03b1 impact the final translation accuracy.\n\n\nRL Training with Monolingual Data\n\nPrevious works typically conduct RL training with only bilingual data for NMT. Monolingual data has been proved to be able to significantly improve the performance of NMT systems (Sennrich et al., 2015a;Xia et al., 2016;. It remains an open problem whether it is possible to combine the benefits of RL training and monolingual data such that even more competitive results can be obtained. In this section we provide several solutions for combination and will study them in next section. Note that all the settings discussed in this section are semi-supervised learning, i.e., both bilingual and monolingual data are available.\n\n\nWith Source-Side Monolingual Data\n\nWe first provide a solution to RL training with source-side monolingual data. As shown in Eqn.\n\n(3), in RL training we need to calculate the reward signal R(\u0177, y) for each generated sentenc\u00ea y, and therefore the reference sentence y seems to be a must-have, which unfortunately is missing for source-side monolingual data. We tackle this challenge via generating pseudo target reference y by bootstrapping with the model itself. Apparently, for the source-side monolingual data, the pseudo target reference y should have good translation quality. Therefore, for each source-side monolingual sentence, we use the N-MT model trained from the bilingual data to beam search a target sentence and treat it as the pseudo target reference y. Afterwards\u0177 is obtained via multinomial sampling to calculate the reward. Although multinomial sampling is usually not as good as sampling via beam search, the combination of beam search (to get the pseudo target reference sentence) and the multinomial sampling (to generate the action sequence of the agent) achieves good exploration-exploitation trade-off, since the pseudo target reference exploits the accuracy of current NMT model while\u0177 achieves better exploration.\n\n\nWith Target-Side Monolingual Data\n\nFor a target-side monolingual sentence, its source sentence x is missing, and consequently\u0177 is unavailable since it is sampled based on x. We tackle this challenge via back translation (Sennrich et al., 2015a). We first train a reverse NMT model from the target language to the source language with bilingual data. For each target-side monolingual sentence, using the reverse NMT model, we back translate it to get its pseudo source sentence x. We then pair the target monolingual data and its backtranslated sentence as a pseudo bilingual sentence pair, which can be used for RL training in the same way as the genuine bilingual sentence pairs.\n\n\nWith both Source-Side and Target-Side Monolingual Data\n\nA natural extension of previous discussions is to combine both the source-side and target-side monolingual data for RL training. We consider two combinations, the sequential method and the unified method. The former one sequentially leverages the source-side and target-side monolingual data for RL training. Specifically, we first train an MLE model using the bilingual data and source-side (or target-side) monolingual data; based on this MLE model, we then use REINFORCE for training with target-side (or source-side) monolingual data. For unified approach, we pack the paired data out from three domains together: the genuine bilingual data, the source monolingual data with its pseudo target references (introduced in subsection 4.1), and the target monolingual data with its back-translated samples (introduced in subsection 4.2). Then we treat the combined data as normal bilingual data on which the NMT model is trained via MLE or RL principles. Our goal is to investigate the model performance with different training data and find the best recipe of how to use these data in RL training. More details are introduced in next section.\n\n\nExperiments\n\nIn this section, we provide a systematic study on aforementioned RL training strategies and the solutions of leveraging monolingual data. The RL training strategies are evaluated on bilingual datasets from three translation tasks, WMT14 English-German (En-De), WMT17 English-Chinese (En-Zh) and WMT17 Chinese-English (Zh-En), and we further conduct the experiments to leverage monolingual data in WMT17 Zh-En translation.\n\n\nExperimental Settings\n\nFor the bilingual datasets, WMT17 (Bojar et al., 2017) En-Zh 1 and WMT17 Zh-En use the same dataset, which contains about 24M sentences pairs, including CWMT Corpus 2017 and UN Parallel Corpus V1.0. The Jieba 2 segmenter is used to perform Chinese word segmentation. We use byte pair encoding (BPE) (Sennrich et al., 2015b) to preprocess the source and target sentences, forming source-side and target-side dictionary with 40, 000 and 37, 000 types, respectively. We use the news-dev2017 as the dev set and newstest2017 as the test set. For the WMT14 En-De dataset, it contains about 4.5M training pairs, newstest2012 and newstest2013 are concatenated as the dev set and newstest2014 acts as test set. Same as (Vaswani et al., 2017), we also perform BPE to process the En-De dataset, the shared source-target vocabulary contains about 37, 000 tokens. For the monolingual dataset on Zh-En translation task, similar to (Sennrich et al., 2017), the Chinese monolingual data comes from LDC Chinese Gigaword (4th edition) and the English monolingual data comes from News Crawl 2016 articles. After preprocessing (e.g., language detection and filtering sentences with more than 80 words), we keep 4M Chinese sentences and 7M English sentences.\n\nWe adopt the Transformer model with transformer big setting as defined in (Vaswani et al., 2017) for Zh-En and En-Zh translations, which achieves SOTA translation quality in several other datasets. For En-De translation, we utilize the transformer base v1 setting. These settings are exactly same as used in the original paper, except we set the layer prepostprocess dropout for Zh-En and En-Zh translation to be 0.05. The optimizer used for MLE training is Adam (Kingma and Ba, 2015) with initial learning rate is 0.1, and we follow the same learning rate schedule in (Vaswani et al., 2017). During training, roughly 4, 096 source tokens and 4, 096 target tokens are paired in one mini batch. Each model is trained using 8 NVIDI-A Tesla M40 GPUs. For RL training, the model is initialized with parameters of the MLE model (trained with only bilingual data), and we continue training it with learning rate 0.0001. Same as (Bahdanau et al., 2017), to calculate the BLEU reward, we start all n-gram counts from 1 instead of 0 and multiply the resulting score by the length of the target reference sentence. For inference, we use beam search with width 6. We run each setting for at least 5 times and report the averaged case sensitive BLEU scores 3 (Papineni et al., 2002) on test set. The test set BLEU is chosen via the best configuration based on the validation set.\n\n\nResults of of RL Training Strategies\n\nWe first evaluate different strategies for RL training, based only on bilingual datasets from previously introduced three translation tasks.\n\nReward Computation As reviewed in subsection 3.1, for reward computation, we need to consider how to sample\u0177 and whether to use reward shaping.\n\nThe results are shown in Table 1, where \"RL\" stands for RL training with the REINFORCE algorithm. We also report the performance of the pretrained NMT model with the MLE loss. From the table, an interesting finding is that\u0177 sampled via beam search strategy is worse than that by multinomial sampling, with a gap of roughly 0.2-0.3 BLEU points on the test set (with significant test score \u03c1 < 0.05). We therefore conjecture that exploration is more important than exploitation in reward computing: multinomial sampling brings more data diversity to the training of NMT model, while sentences generated by beam search are usually very similar to each other. Furthermore, we find that there is no big difference between the leverage of reward shaping or terminal reward, with only slightly better performance of reward shaping. We therefore use multinomial sampling and reward shaping in later experiments.  \n\n\nVariance Reduction of Gradient Estimation\n\nNext we evaluate the strategies for reducing variance of gradient estimation (see section3.2). We want to know whether the baseline reward is necessary. To compute the baseline reward, similar to (Ranzato et al., 2016;Bahdanau et al., 2017), we build a two-layer MLP regressor with Relu (Nair and Hinton, 2010) activation units. The function takes the hidden states from decoder as input, and the parameters of the regressor are trained to minimize the mean squared loss of Eqn. (4). We first pre-train the baseline function for 20k steps/minibatches, and then jointly train NMT model (with RL) and the baseline reward function. Table 2 shows that the learning of baseline reward does not help RL training. This contradicts with previous observations (Ranzato et al., 2016), and seems to suggest that the variance of gradient estimation in NMT is not as large as we expected. The reason might be that the probability mass on the target-side language space induced by the NMT model is highly concentrated, making the sampled y representative enough in terms of estimating the expectation. Therefore, for the economic perspective, it is not necessary to add the additional steps of using baseline reward on RL training for NMT.\n\nCombine MLE and RL Objectives As shown in Eqn. (5), the hyperparameter \u03b1 controls the trade-off between MLE and RL objectives. For comparison, we set \u03b1 to be [0, 0.1, 0.3, 0.5, 0.7, 0.9] in our experiments. The results are presented in Figure 1.  Table 3: Results with source monolingual data. \"B\" denotes bilingual data, \"Ms\" denotes source-side monolingual data, \"&\" denotes data combination.\n\nThe results show that combining the MLE objective with the RL objective achieves better performance (27.48 for En-De, 34.63 for En-Zh and 25.04 for Zh-En with \u03b1 = 0.3). This indicates that MLE objective is helpful to stabilize the training and improve the model performance, as we expected. However, further increasing \u03b1 does not bring more gain. The best trade-off between MLE and RL objectives in our experiment is \u03b1 = 0.3. Therefore, we set \u03b1 = 0.3 in the following experiments.\n\n\nResults of RL Training with Monolingual Data\n\nIn this subsection, we report the results on both valid and test set of RL training using bilingual and monolingual data in Zh-En translation. From Table 3 to Table 6, \"RL\" denotes the model trained with RL using multinomial sampling, reward shaping, no baseline reward, and combined objective, based on the observations in the last subsection. \"B\" denotes bilingual data, \"Ms\" denotes sourceside monolingual data and \"Mt\" denotes target-side monolingual data, \"&\" denotes data combination.\n\nWith Source-Side Monolingual Data As discussed before, we use beam search with beam width 4 to sample the pseudo target sentence y for each monolingual sentence x. We consider several settings for RL training: 1) only source-side monolingual data; 2) the combination of bilingual and source-side monolingual data. We first train an MLE model using the augmented dataset combining the genuine bilingual data with the pseudo bilingual data generated from the monolingual data, and then perform RL training on this combined dataset. The results are shown in Table 3.\n\nWith Target Table 4: Results with target monolingual data. \"B\" denotes bilingual data, \"Mt\" denotes target-side monolingual data, \"&\" denotes data combination.\n\n[  Table 5: Results of sequential approach for monolingual data. \"B\" denotes bilingual data, \"Ms\" denotes source-side monolingual data and \"Mt\" denotes targetside monolingual data, \"&\" denotes data combination.\n\n[  Table 6: Results of unified approach for monolingual data. \"+\" means to initialize the RL model using above MLE model, which is trained on the combination of bilingual data, source-side monolingual data and targetside monolingual data.\n\nsentence y to get pseudo source sentence x. Similarly, we consider several settings for RL training: 1) only target-side monolingual data; 2) the combination of bilingual data and target-side monolingual data. We train an MLE model using both the genuine and the generated pseudo bilingual data, and then perform RL training on this data. The results are presented in Table 4. From Table 3 and 4, we have several observations. First, monolingual data helps RL training, improving BLEU score from 25.04 to 25.22 (\u03c1 < 0.05) in Table 3. Second, when we only add monolingual data for RL training, the model achieves similar performance compared to MLE training with bilingual and monolingual data (e.g., 25.15 vs. 25.24 (\u03c1 < 0.05) in Table 4).\n\nWith both Source-Side and Target-Side Monolingual Data We have two approaches to use both source-side and target-side monolingual data, as described in subsection 4.3. The results are reported in Table 5 and Table 6.\n\nFrom Table 5, we can observe that the sequen-  \n\n\nComparison with Other Models\n\nAt last, as a summary of our empirical results, we compare several representataive end-to-end NMT systems to our work in Table 7, which includes the Transformer (Vaswani et al., 2017) model, with/without back-translation (Sennrich et al., 2015a) and the best NMT system in WMT17 Chinese-English translation challenge 5 (SougouKnowing-ensemble  Shen et al., 2016;Bahdanau et al., 2017). In (Ranzato et al., 2016), the authors propose to train a neural translation model with the objective gradually shifting from maximizing token-level likelihood to optimizing the sentence-level BLEU score. Shen et al. (2016) proposes to adopt minimum risk training (Goel and Byrne, 2000) to minimize the task specific expected loss (i.e., induced by BLEU score) on NMT training data. Instead of the RE-INFORCE (Williams, 1992) algorithm used in the above two works, Bahdanau et al. (2017) further optimizes the policy by actor-critic algorithm.  introduces a simple RL based method to optimize the stacked LSTM model for NMT, achieving better BLEU scores on English-French translation but not on English-German. Edunov et al. (2017) presents a comparative study of several classical structural prediction losses for NMT model, which also includes sequence-level loss but not exactly the same as RL.\n\nOur work is also related with the research works that leverage monolingual data for improving N-MT models (Zhang and Zong, 2016;Sennrich et al., 2015a;Wang et al., 2018;Xia et al., 2016;. Zhang and Zong (2016) exploits the source-side monolingual data in NMT. Sennrich et al. (2015a) proposes back-translation method to leverage target-side monolingual data for NMT. Xia et al. (2016) formulates the machine translation as a communication game, which leverages the power of two directional translation models and source/target monolingual data.  proposes a similar semi-supervised approach. However, none of these works have explored the power of monolingual data in the context of training NMT model with reinforcement learning.\n\n\nConclusion\n\nIn this work, we presented a study of how to effectively train NMT models using reinforcement learning. Different RL strategies were evaluated in German-English, English-Chinese and Chinese-English translation tasks on large-scale bilingual datasets. We found that (1) multinomial sampling is better than beam search, (2) several previous tricks such as reward shaping and baseline reward does not make significant difference, and (3) the combination of the MLE and RL objectives is important. In addition, we explored the source/target monolingual data for RL training. By combing the power of RL and monolingual data, we achieve the state-of-the-art BLEU score on WMT17 Chinese-English translation task. We hope that our study and results can benefit the community and bring some insights on how to train deep NMT models with reinforcement learning and big data.\n\nTable 2 :\n2Results of variance reduction of gradient estimation.= 0 \n= 0.1 \n= 0.3 \n= 0.5 \n= 0.7 \n= 0.9 \n24 \n\n26 \n\n28 \n\n30 \n\n32 \n\n34 \n\n36 \n\nBLEU Score \n\n27.23 \n27.28 \n27.48 \n27.37 \n27.25 \n27.20 \n\n34.47 \n34.50 \n34.63 \n34.56 \n34.44 \n34.40 \n\n24.72 \n24.89 \n25.04 \n24.87 \n24.71 \n24.65 \n\nEn-De \nEn-Zh \nZh-En \n\nFigure 1: Results of different weights \u03b1 to combine \nMLE and RL objectives. \n\n\n\n\nSougouKnowing-ensemble Stacked LSTM model + Reranking + Ensemble 26.40 Our end-to-end NMT this work Transformer + RL 25.04 Transformer + Source Monolingual Data 25.31 Transformer + Source Monolingual Data + RL 25.60 Transformer + Target Monolingual Data 25.24 Transformer + Target Monolingual Data + RL 25.58 Transformer + Source & Target Monolingual Data 26.13 Transformer + Source & Target Monolingual Data + RL 26.73System \nArchitecture \nBLEU \nExisting end-to-end NMT systems \nVaswani et al. (2017) \nTransformer \n24.29 \nSennrich et al. (2015a) \nTransformer + Target Monolingual Data (i.e., back translation) \n25.24 \nSougouKnowing \nStacked LSTM model + Reranking \n24.00 \n\n\nTable 7 :\n7Comparisons of different competitive end-to-end NMT systems. SougouKnowing results come from http://matrix.statmt.org/matrix/systems_list/1878. tial training of monolingual data can benefit the model performance. Taking the last three rows as an example, the BLEU score of the MLE model trained on the combination of bilingual data and target-side monolingual data is 25.24; based on this model, RL training using the source-side monolingual data further improves the model performance by 0.7 (\u03c1 < 0.01) BLEU points. FromTable 6, we can observe on top of a quite strong MLE baseline (26.13), through the unified RL training, we can still improve the test set by 0.6 points to 26.73 (\u03c1 < 0.01), which shows the effectiveness of combining source/target monolingual data and reinforcement learning.\n\n\n). The results clearly show that after combing both source-side and target-side monolingual data with RL training, we obtain the state-of-the-art BLEU score 26.73, even surpassing the best ensemble model in WMT17 Zh-En translation challenge.Our work is mainly related with the literature of using reinforcement learning to directly optimize the evaluation measure for neural machine translation. Several representative works are (Ranzato et al., 5 http://matrix.statmt.org/matrix/ systems_list/18786 Related Work \n\n2016; \nhttp://www.statmt.org/wmt17/ translation-task.html 2 https://github.com/fxsjy/jieba\nCalculated by Sacr\u00e9BLEU toolkit, which produces exactly the same evaluation result as that in WMT17 Zh-En campaign. https://github.com/awslabs/sockeye/ tree/master/contrib/sacrebleu\n\nAn actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, Fifth International Conference on Learning Representations. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction. Fifth Internation- al Conference on Learning Representations.\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Third International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. Third International Conference on Learning Representations.\n\nFindings of the 2017 conference on machine translation. Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, Marco Turchi, Proceedings of the Second Conference on Machine Translation. the Second Conference on Machine TranslationCopenhagen, DenmarkAssociation for Computational Linguistics2Shared Task PapersOnd\u0159ej Bojar, Rajen Chatterjee, Christian Federman- n, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Lo- gacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation. In Proceedings of the Second Confer- ence on Machine Translation, Volume 2: Shared Task Papers, pages 169-214, Copenhagen, Denmark. Association for Computational Linguistics.\n\nMinimum error rate training by sampling the translation lattice. Samidh Chatterjee, Nicola Cancedda, Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. the 2010 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsSamidh Chatterjee and Nicola Cancedda. 2010. Mini- mum error rate training by sampling the translation lattice. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 606-615. Association for Computational Lin- guistics.\n\nSemisupervised learning for neural machine translation. meeting of the association for computational linguistics. Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu, Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Semi- supervised learning for neural machine translation. meeting of the association for computational linguis- tics, pages 1965-1974.\n\nClassical structured prediction losses for sequence to sequence learning. Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc&apos;aurelio Ranzato, Proceedings of NAACL-HLT. NAACL-HLTSergey Edunov, Myle Ott, Michael Auli, David Grangi- er, and Marc'Aurelio Ranzato. 2017. Classical struc- tured prediction losses for sequence to sequence learning. Proceedings of NAACL-HLT 2018.\n\nConvolutional sequence to sequence learning. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N Dauphin, Proceedings of the 34th international conference on machine learning. the 34th international conference on machine learningJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. 2017. Convolutional sequence to sequence learning. Proceedings of the 34th international conference on machine learning.\n\nMinimum bayes-risk automatic speech recognition. Vaibhava Goel, J William, Byrne, Computer Speech & Language. 142Vaibhava Goel and William J Byrne. 2000. Minimum bayes-risk automatic speech recognition. Computer Speech & Language, 14(2):115-135.\n\nAchieving human parity on automatic chinese to english news translation. Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, arXiv:1803.05567arXiv preprintHany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federman- n, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, et al. 2018. Achieving hu- man parity on automatic chinese to english news translation. arXiv preprint arXiv:1803.05567.\n\nDecoding with value networks for neural machine translation. Di He, Hanqing Lu, Yingce Xia, Tao Qin, Liwei Wang, Tieyan Liu, Advances in Neural Information Processing Systems. Di He, Hanqing Lu, Yingce Xia, Tao Qin, Liwei Wang, and Tieyan Liu. 2017. Decoding with value network- s for neural machine translation. In Advances in Neural Information Processing Systems, pages 178- 187.\n\nMaximum expected bleu training of phrase and lexicon translation models. Xiaodong He, Li Deng, Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers. the 50th Annual Meeting of the Association for Computational Linguistics: Long PapersAssociation for Computational Linguistics1Xiaodong He and Li Deng. 2012. Maximum expect- ed bleu training of phrase and lexicon translation models. In Proceedings of the 50th Annual Meet- ing of the Association for Computational Linguistics: Long Papers-Volume 1, pages 292-301. Association for Computational Linguistics.\n\nDeep reinforcement learning that matters. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, David Meger, Thirthy-Second AAAI Conference On Artificial Intelligence. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018. Deep reinforcement learning that matters. Thirthy-Second AAAI Conference On Artificial Intel- ligence.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, Third International Conference on Learning Representations. Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. Third Interna- tional Conference on Learning Representations.\n\nAdversarial learning for neural dialogue generation. Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, Dan Jurafsky, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingJiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and Dan Jurafsky. 2017. Adversarial learning for neural dialogue generation. Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing.\n\nA structured self-attentive sentence embedding. Zhouhan Lin, Minwei Feng, Cicero Nogueira, Mo Santos, Bing Yu, Bowen Xiang, Yoshua Zhou, Bengio, Fifth International Conference on Learning Representations. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. Fifth International Confer- ence on Learning Representations.\n\nPlaying atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, Advances in neural information processing systems. workshopVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. Advances in neural in- formation processing systems, workshop.\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th international conference on machine learning. the 27th international conference on machine learningVinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning, pages 807-814.\n\nPolicy invariance under reward transformations: Theory and application to reward shaping. Y Andrew, Daishi Ng, Stuart Harada, Russell, ICML. 99Andrew Y Ng, Daishi Harada, and Stuart Russell. 1999. Policy invariance under reward transforma- tions: Theory and application to reward shaping. In ICML, volume 99, pages 278-287.\n\nReinforcement learning for bandit neural machine translation with simulated human feedback. Khanh Nguyen, Hal Daum\u00e9, Iii , Jordan Boyd-Graber, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingKhanh Nguyen, Hal Daum\u00e9 III, and Jordan Boyd- Graber. 2017. Reinforcement learning for bandit neural machine translation with simulated human feedback. Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic e- valuation of machine translation. In Proceedings of the 40th annual meeting on association for compu- tational linguistics, pages 311-318. Association for Computational Linguistics.\n\nAurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, Sequence level training with recurrent neural networks. Fourth International Conference on Learning Representations. Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level train- ing with recurrent neural networks. Fourth Interna- tional Conference on Learning Representations.\n\nMaximum likelihood estimation. Encyclopedia of statistical sciences. Fw Scholz, FW Scholz. 1985. Maximum likelihood estimation. Encyclopedia of statistical sciences.\n\nThe university of edinburgh's neural mt systems for wmt17. Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich Germann, Barry Haddow, Kenneth Heafield, Antonio Valerio Miceli, Philip Barone, Williams, Proceedings of the Second Conference on Machine Translation. the Second Conference on Machine TranslationAssociation for Computational LinguisticsRico Sennrich, Alexandra Birch, Anna Currey, Ulrich Germann, Barry Haddow, Kenneth Heafield, An- tonio Valerio Miceli Barone, and Philip Williams. 2017. The university of edinburgh's neural mt sys- tems for wmt17. In Proceedings of the Second Con- ference on Machine Translation, pages 389-399. As- sociation for Computational Linguistics.\n\nImproving neural machine translation models with monolingual data. Rico Sennrich, Barry Haddow, Alexandra Birch, Proceedings of the 54th. the 54thRico Sennrich, Barry Haddow, and Alexandra Birch. 2015a. Improving neural machine translation mod- els with monolingual data. Proceedings of the 54th\n\nAnnual Meeting of the Association for Computational Linguistics. Annual Meeting of the Association for Computation- al Linguistics.\n\nNeural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsRico Sennrich, Barry Haddow, and Alexandra Birch. 2015b. Neural machine translation of rare words with subword units. Proceedings of the 54th Annual Meeting of the Association for Computational Lin- guistics.\n\nMinimum risk training for neural machine translation. meeting of the association for computational linguistics. Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu, Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Minimum risk training for neural machine translation. meet- ing of the association for computational linguistics, pages 1683-1692.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural network- s. In Advances in neural information processing sys- tems, pages 3104-3112.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000-6010.\n\nDual transfer learning for neural machine translation with marginal distribution regularization. Yijun Wang, Yingce Xia, Li Zhao, Jiang Bian, Tao Qin, Guiquan Liu, T Liu, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence. the Thirty-Second AAAI Conference on Artificial IntelligenceYijun Wang, Yingce Xia, Li Zhao, Jiang Bian, Tao Qin, Guiquan Liu, and T Liu. 2018. Dual transfer learn- ing for neural machine translation with marginal distribution regularization. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intel- ligence.\n\nThe optimal reward baseline for gradient-based reinforcement learning. Lex Weaver, Nigel Tao, Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence. the Seventeenth conference on Uncertainty in artificial intelligenceLex Weaver and Nigel Tao. 2001. The optimal reward baseline for gradient-based reinforcement learning. In Proceedings of the Seventeenth conference on Un- certainty in artificial intelligence, pages 538-545.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. J Ronald, Williams, Reinforcement Learning. SpringerRonald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. In Reinforcement Learning, pages 5-32. Springer.\n\nBeyond error propagation in neural machine translation: Characteristics of language also matter. Lijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingLijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. 2018a. Beyond error propaga- tion in neural machine translation: Characteristics of language also matter. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing.\n\nWord attention for sequence to sequence text understanding. Lijun Wu, Fei Tian, Li Zhao, Jianhuang Lai, Tie-Yan Liu, AAAI. Lijun Wu, Fei Tian, Li Zhao, Jianhuang Lai, and Tie- Yan Liu. 2018b. Word attention for sequence to se- quence text understanding. In AAAI.\n\nLijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu, arX- iv:1704.06933Adversarial neural machine translation. arXiv preprintLijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. 2017a. Adversar- ial neural machine translation. arXiv preprint arX- iv:1704.06933.\n\nLijun Wu, Li Zhao, Tao Qin, Jianhuang Lai, Tie-Yan Liu, Sequence prediction with unlabeled data by reward function learning. IJCAI-17. Lijun Wu, Li Zhao, Tao Qin, Jianhuang Lai, and Tie- Yan Liu. 2017b. Sequence prediction with unlabeled data by reward function learning. IJCAI-17, pages 3098-3104.\n\nGoogle's neural machine translation system: Bridging the gap between human and machine translation. Yonghui Wu, Mike Schuster, Zhifeng Chen, V Quoc, Mohammad Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.08144arXiv preprintYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Max- im Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation sys- tem: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.\n\nDual learning for machine translation. neural information processing systems. Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, Weiying Ma, Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Weiying Ma. 2016. Dual learning for machine translation. neural information process- ing systems, pages 820-828.\n\nDeliberation networks: Sequence generation beyond one-pass decoding. Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, Tie-Yan Liu, Advances in Neural Information Processing Systems. Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, and Tie-Yan Liu. 2017. Deliberation networks: Sequence generation beyond one-pass de- coding. In Advances in Neural Information Process- ing Systems, pages 1784-1794.\n\nExploiting source-side monolingual data in neural machine translation. Jiajun Zhang, Chengqing Zong, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingJiajun Zhang and Chengqing Zong. 2016. Exploit- ing source-side monolingual data in neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process- ing, pages 1535-1545.\n", "annotations": {"author": "[{\"end\":291,\"start\":194},{\"end\":322,\"start\":292},{\"end\":373,\"start\":323},{\"end\":449,\"start\":374},{\"end\":503,\"start\":450}]", "publisher": "[{\"end\":107,\"start\":66},{\"end\":786,\"start\":745}]", "author_last_name": "[{\"end\":202,\"start\":200},{\"end\":300,\"start\":296},{\"end\":330,\"start\":327},{\"end\":387,\"start\":384},{\"end\":461,\"start\":458}]", "author_first_name": "[{\"end\":199,\"start\":194},{\"end\":295,\"start\":292},{\"end\":326,\"start\":323},{\"end\":383,\"start\":374},{\"end\":457,\"start\":450}]", "author_affiliation": "[{\"end\":290,\"start\":231},{\"end\":321,\"start\":302},{\"end\":372,\"start\":353},{\"end\":448,\"start\":389},{\"end\":502,\"start\":483}]", "title": "[{\"end\":65,\"start\":1},{\"end\":568,\"start\":504}]", "venue": "[{\"end\":656,\"start\":570}]", "abstract": "[{\"end\":1930,\"start\":825}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2012,\"start\":1989},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2032,\"start\":2012},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2048,\"start\":2032},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2064,\"start\":2048},{\"end\":2083,\"start\":2064},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2475,\"start\":2461},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2701,\"start\":2678},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2914,\"start\":2892},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2931,\"start\":2914},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2971,\"start\":2948},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3151,\"start\":3132},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3592,\"start\":3570},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3613,\"start\":3592},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3789,\"start\":3765},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3857,\"start\":3835},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3904,\"start\":3885},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4131,\"start\":4109},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4170,\"start\":4148},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4190,\"start\":4170},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4231,\"start\":4212},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4249,\"start\":4231},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4392,\"start\":4368},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4409,\"start\":4392},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7973,\"start\":7951},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8119,\"start\":8096},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8153,\"start\":8131},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9076,\"start\":9053},{\"end\":9641,\"start\":9614},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11099,\"start\":11075},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11572,\"start\":11541},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12644,\"start\":12627},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12815,\"start\":12793},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13309,\"start\":13292},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13807,\"start\":13785},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14665,\"start\":14643},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14687,\"start\":14665},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14913,\"start\":14897},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14930,\"start\":14913},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15488,\"start\":15464},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15505,\"start\":15488},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17402,\"start\":17378},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19556,\"start\":19536},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19825,\"start\":19801},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20234,\"start\":20212},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20442,\"start\":20419},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20837,\"start\":20815},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21332,\"start\":21310},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21686,\"start\":21663},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22011,\"start\":21988},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23605,\"start\":23583},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23627,\"start\":23605},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24160,\"start\":24138},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28432,\"start\":28410},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28494,\"start\":28470},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28611,\"start\":28593},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28633,\"start\":28611},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28660,\"start\":28638},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28858,\"start\":28840},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28921,\"start\":28899},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29122,\"start\":29100},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29366,\"start\":29346},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29662,\"start\":29640},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29685,\"start\":29662},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29703,\"start\":29685},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29720,\"start\":29703},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29743,\"start\":29722},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29817,\"start\":29794},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29918,\"start\":29901}]", "figure": "[{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31525,\"start\":31143},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32201,\"start\":31526},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":33009,\"start\":32202},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":33533,\"start\":33010}]", "paragraph": "[{\"end\":2702,\"start\":1946},{\"end\":3232,\"start\":2704},{\"end\":3644,\"start\":3234},{\"end\":4250,\"start\":3646},{\"end\":4509,\"start\":4252},{\"end\":4959,\"start\":4511},{\"end\":5319,\"start\":4961},{\"end\":5723,\"start\":5321},{\"end\":5893,\"start\":5725},{\"end\":6049,\"start\":5895},{\"end\":6381,\"start\":6051},{\"end\":6829,\"start\":6383},{\"end\":7064,\"start\":6844},{\"end\":7747,\"start\":7095},{\"end\":8325,\"start\":7839},{\"end\":9354,\"start\":8370},{\"end\":9777,\"start\":9429},{\"end\":9930,\"start\":9837},{\"end\":10697,\"start\":9961},{\"end\":10959,\"start\":10720},{\"end\":11797,\"start\":10961},{\"end\":12086,\"start\":11799},{\"end\":12127,\"start\":12088},{\"end\":13310,\"start\":12129},{\"end\":13689,\"start\":13356},{\"end\":14272,\"start\":13708},{\"end\":14688,\"start\":14274},{\"end\":14931,\"start\":14722},{\"end\":15028,\"start\":14933},{\"end\":15247,\"start\":15069},{\"end\":15911,\"start\":15285},{\"end\":16043,\"start\":15949},{\"end\":17155,\"start\":16045},{\"end\":17838,\"start\":17193},{\"end\":19039,\"start\":17897},{\"end\":19476,\"start\":19055},{\"end\":20739,\"start\":19502},{\"end\":22108,\"start\":20741},{\"end\":22289,\"start\":22149},{\"end\":22434,\"start\":22291},{\"end\":23341,\"start\":22436},{\"end\":24612,\"start\":23387},{\"end\":25008,\"start\":24614},{\"end\":25491,\"start\":25010},{\"end\":26030,\"start\":25540},{\"end\":26595,\"start\":26032},{\"end\":26756,\"start\":26597},{\"end\":26968,\"start\":26758},{\"end\":27208,\"start\":26970},{\"end\":27949,\"start\":27210},{\"end\":28167,\"start\":27951},{\"end\":28216,\"start\":28169},{\"end\":29532,\"start\":28249},{\"end\":30263,\"start\":29534},{\"end\":31142,\"start\":30278}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7838,\"start\":7748},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9428,\"start\":9355},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9836,\"start\":9778},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13707,\"start\":13690},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15068,\"start\":15029}]", "table_ref": "[{\"end\":22468,\"start\":22461},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24023,\"start\":24016},{\"end\":24868,\"start\":24861},{\"end\":25695,\"start\":25688},{\"end\":25706,\"start\":25699},{\"end\":26594,\"start\":26587},{\"end\":26608,\"start\":26602},{\"end\":26616,\"start\":26609},{\"end\":26768,\"start\":26761},{\"end\":26980,\"start\":26973},{\"end\":27585,\"start\":27578},{\"end\":27599,\"start\":27592},{\"end\":27742,\"start\":27735},{\"end\":27947,\"start\":27940},{\"end\":28166,\"start\":28147},{\"end\":28181,\"start\":28174},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28377,\"start\":28370}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1944,\"start\":1932},{\"attributes\":{\"n\":\"2\"},\"end\":6842,\"start\":6832},{\"attributes\":{\"n\":\"2.1\"},\"end\":7093,\"start\":7067},{\"attributes\":{\"n\":\"2.2\"},\"end\":8368,\"start\":8328},{\"attributes\":{\"n\":\"3\"},\"end\":9959,\"start\":9933},{\"attributes\":{\"n\":\"3.1\"},\"end\":10718,\"start\":10700},{\"attributes\":{\"n\":\"3.2\"},\"end\":13354,\"start\":13313},{\"attributes\":{\"n\":\"3.3\"},\"end\":14720,\"start\":14691},{\"attributes\":{\"n\":\"4\"},\"end\":15283,\"start\":15250},{\"attributes\":{\"n\":\"4.1\"},\"end\":15947,\"start\":15914},{\"attributes\":{\"n\":\"4.2\"},\"end\":17191,\"start\":17158},{\"attributes\":{\"n\":\"4.3\"},\"end\":17895,\"start\":17841},{\"attributes\":{\"n\":\"5\"},\"end\":19053,\"start\":19042},{\"attributes\":{\"n\":\"5.1\"},\"end\":19500,\"start\":19479},{\"attributes\":{\"n\":\"5.2\"},\"end\":22147,\"start\":22111},{\"end\":23385,\"start\":23344},{\"attributes\":{\"n\":\"5.3\"},\"end\":25538,\"start\":25494},{\"attributes\":{\"n\":\"5.4\"},\"end\":28247,\"start\":28219},{\"attributes\":{\"n\":\"7\"},\"end\":30276,\"start\":30266},{\"end\":31153,\"start\":31144},{\"end\":32212,\"start\":32203}]", "table": "[{\"end\":31525,\"start\":31208},{\"end\":32201,\"start\":31947},{\"end\":33533,\"start\":33510}]", "figure_caption": "[{\"end\":31208,\"start\":31155},{\"end\":31947,\"start\":31528},{\"end\":33009,\"start\":32214},{\"end\":33510,\"start\":33012}]", "figure_ref": "[{\"end\":24858,\"start\":24850}]", "bib_author_first_name": "[{\"end\":33859,\"start\":33852},{\"end\":33878,\"start\":33870},{\"end\":33893,\"start\":33887},{\"end\":33905,\"start\":33898},{\"end\":33917,\"start\":33913},{\"end\":33930,\"start\":33924},{\"end\":33944,\"start\":33939},{\"end\":33962,\"start\":33956},{\"end\":34352,\"start\":34345},{\"end\":34372,\"start\":34363},{\"end\":34384,\"start\":34378},{\"end\":34707,\"start\":34701},{\"end\":34720,\"start\":34715},{\"end\":34742,\"start\":34733},{\"end\":34760,\"start\":34754},{\"end\":34774,\"start\":34769},{\"end\":34790,\"start\":34783},{\"end\":34806,\"start\":34798},{\"end\":34820,\"start\":34813},{\"end\":34831,\"start\":34828},{\"end\":34844,\"start\":34837},{\"end\":34864,\"start\":34856},{\"end\":34877,\"start\":34871},{\"end\":34889,\"start\":34885},{\"end\":34903,\"start\":34896},{\"end\":34917,\"start\":34912},{\"end\":34931,\"start\":34926},{\"end\":35680,\"start\":35674},{\"end\":35699,\"start\":35693},{\"end\":36292,\"start\":36288},{\"end\":36303,\"start\":36300},{\"end\":36316,\"start\":36308},{\"end\":36324,\"start\":36321},{\"end\":36332,\"start\":36329},{\"end\":36344,\"start\":36337},{\"end\":36354,\"start\":36350},{\"end\":36658,\"start\":36652},{\"end\":36671,\"start\":36667},{\"end\":36684,\"start\":36677},{\"end\":36696,\"start\":36691},{\"end\":36724,\"start\":36707},{\"end\":37016,\"start\":37011},{\"end\":37033,\"start\":37026},{\"end\":37045,\"start\":37040},{\"end\":37061,\"start\":37056},{\"end\":37076,\"start\":37070},{\"end\":37467,\"start\":37459},{\"end\":37475,\"start\":37474},{\"end\":37734,\"start\":37730},{\"end\":37750,\"start\":37743},{\"end\":37761,\"start\":37756},{\"end\":37774,\"start\":37768},{\"end\":37794,\"start\":37786},{\"end\":37811,\"start\":37802},{\"end\":37830,\"start\":37823},{\"end\":37844,\"start\":37838},{\"end\":37869,\"start\":37862},{\"end\":37879,\"start\":37877},{\"end\":38255,\"start\":38253},{\"end\":38267,\"start\":38260},{\"end\":38278,\"start\":38272},{\"end\":38287,\"start\":38284},{\"end\":38298,\"start\":38293},{\"end\":38311,\"start\":38305},{\"end\":38657,\"start\":38649},{\"end\":38664,\"start\":38662},{\"end\":39228,\"start\":39223},{\"end\":39247,\"start\":39240},{\"end\":39261,\"start\":39255},{\"end\":39277,\"start\":39271},{\"end\":39291,\"start\":39286},{\"end\":39305,\"start\":39300},{\"end\":39621,\"start\":39620},{\"end\":39637,\"start\":39632},{\"end\":39913,\"start\":39908},{\"end\":39922,\"start\":39918},{\"end\":39938,\"start\":39931},{\"end\":39948,\"start\":39944},{\"end\":39960,\"start\":39957},{\"end\":40402,\"start\":40395},{\"end\":40414,\"start\":40408},{\"end\":40427,\"start\":40421},{\"end\":40440,\"start\":40438},{\"end\":40453,\"start\":40449},{\"end\":40463,\"start\":40458},{\"end\":40477,\"start\":40471},{\"end\":40830,\"start\":40821},{\"end\":40842,\"start\":40837},{\"end\":40861,\"start\":40856},{\"end\":40874,\"start\":40870},{\"end\":40890,\"start\":40883},{\"end\":40907,\"start\":40903},{\"end\":40924,\"start\":40918},{\"end\":41301,\"start\":41296},{\"end\":41316,\"start\":41308},{\"end\":41318,\"start\":41317},{\"end\":41732,\"start\":41731},{\"end\":41747,\"start\":41741},{\"end\":41758,\"start\":41752},{\"end\":42063,\"start\":42058},{\"end\":42075,\"start\":42072},{\"end\":42086,\"start\":42083},{\"end\":42095,\"start\":42089},{\"end\":42582,\"start\":42575},{\"end\":42598,\"start\":42593},{\"end\":42611,\"start\":42607},{\"end\":42626,\"start\":42618},{\"end\":43116,\"start\":43109},{\"end\":43128,\"start\":43123},{\"end\":43145,\"start\":43138},{\"end\":43162,\"start\":43154},{\"end\":43725,\"start\":43721},{\"end\":43745,\"start\":43736},{\"end\":43757,\"start\":43753},{\"end\":43772,\"start\":43766},{\"end\":43787,\"start\":43782},{\"end\":43803,\"start\":43796},{\"end\":43821,\"start\":43814},{\"end\":43844,\"start\":43838},{\"end\":44421,\"start\":44417},{\"end\":44437,\"start\":44432},{\"end\":44455,\"start\":44446},{\"end\":44845,\"start\":44841},{\"end\":44861,\"start\":44856},{\"end\":44879,\"start\":44870},{\"end\":45375,\"start\":45370},{\"end\":45386,\"start\":45382},{\"end\":45402,\"start\":45394},{\"end\":45410,\"start\":45407},{\"end\":45418,\"start\":45415},{\"end\":45430,\"start\":45423},{\"end\":45440,\"start\":45436},{\"end\":45720,\"start\":45716},{\"end\":45737,\"start\":45732},{\"end\":45753,\"start\":45747},{\"end\":46022,\"start\":46016},{\"end\":46036,\"start\":46032},{\"end\":46050,\"start\":46046},{\"end\":46064,\"start\":46059},{\"end\":46081,\"start\":46076},{\"end\":46094,\"start\":46089},{\"end\":46096,\"start\":46095},{\"end\":46110,\"start\":46104},{\"end\":46124,\"start\":46119},{\"end\":46522,\"start\":46517},{\"end\":46535,\"start\":46529},{\"end\":46543,\"start\":46541},{\"end\":46555,\"start\":46550},{\"end\":46565,\"start\":46562},{\"end\":46578,\"start\":46571},{\"end\":46585,\"start\":46584},{\"end\":47068,\"start\":47065},{\"end\":47082,\"start\":47077},{\"end\":47541,\"start\":47540},{\"end\":47863,\"start\":47858},{\"end\":47870,\"start\":47868},{\"end\":47878,\"start\":47876},{\"end\":47886,\"start\":47883},{\"end\":47896,\"start\":47893},{\"end\":47911,\"start\":47902},{\"end\":47924,\"start\":47917},{\"end\":48430,\"start\":48425},{\"end\":48438,\"start\":48435},{\"end\":48447,\"start\":48445},{\"end\":48463,\"start\":48454},{\"end\":48476,\"start\":48469},{\"end\":48634,\"start\":48629},{\"end\":48645,\"start\":48639},{\"end\":48653,\"start\":48651},{\"end\":48663,\"start\":48660},{\"end\":48673,\"start\":48670},{\"end\":48688,\"start\":48679},{\"end\":48701,\"start\":48694},{\"end\":48951,\"start\":48946},{\"end\":48958,\"start\":48956},{\"end\":48968,\"start\":48965},{\"end\":48983,\"start\":48974},{\"end\":48996,\"start\":48989},{\"end\":49353,\"start\":49346},{\"end\":49362,\"start\":49358},{\"end\":49380,\"start\":49373},{\"end\":49388,\"start\":49387},{\"end\":49403,\"start\":49395},{\"end\":49416,\"start\":49408},{\"end\":49431,\"start\":49426},{\"end\":49446,\"start\":49442},{\"end\":49458,\"start\":49455},{\"end\":49469,\"start\":49464},{\"end\":49888,\"start\":49882},{\"end\":49896,\"start\":49894},{\"end\":49904,\"start\":49901},{\"end\":49915,\"start\":49910},{\"end\":49929,\"start\":49922},{\"end\":49940,\"start\":49934},{\"end\":49953,\"start\":49946},{\"end\":50215,\"start\":50209},{\"end\":50224,\"start\":50221},{\"end\":50236,\"start\":50231},{\"end\":50248,\"start\":50241},{\"end\":50257,\"start\":50254},{\"end\":50270,\"start\":50263},{\"end\":50282,\"start\":50275},{\"end\":50650,\"start\":50644},{\"end\":50667,\"start\":50658}]", "bib_author_last_name": "[{\"end\":33868,\"start\":33860},{\"end\":33885,\"start\":33879},{\"end\":33896,\"start\":33894},{\"end\":33911,\"start\":33906},{\"end\":33922,\"start\":33918},{\"end\":33937,\"start\":33931},{\"end\":33954,\"start\":33945},{\"end\":33969,\"start\":33963},{\"end\":34361,\"start\":34353},{\"end\":34376,\"start\":34373},{\"end\":34391,\"start\":34385},{\"end\":34713,\"start\":34708},{\"end\":34731,\"start\":34721},{\"end\":34752,\"start\":34743},{\"end\":34767,\"start\":34761},{\"end\":34781,\"start\":34775},{\"end\":34796,\"start\":34791},{\"end\":34811,\"start\":34807},{\"end\":34826,\"start\":34821},{\"end\":34835,\"start\":34832},{\"end\":34854,\"start\":34845},{\"end\":34869,\"start\":34865},{\"end\":34883,\"start\":34878},{\"end\":34894,\"start\":34890},{\"end\":34910,\"start\":34904},{\"end\":34924,\"start\":34918},{\"end\":34938,\"start\":34932},{\"end\":35691,\"start\":35681},{\"end\":35708,\"start\":35700},{\"end\":36298,\"start\":36293},{\"end\":36306,\"start\":36304},{\"end\":36319,\"start\":36317},{\"end\":36327,\"start\":36325},{\"end\":36335,\"start\":36333},{\"end\":36348,\"start\":36345},{\"end\":36358,\"start\":36355},{\"end\":36665,\"start\":36659},{\"end\":36675,\"start\":36672},{\"end\":36689,\"start\":36685},{\"end\":36705,\"start\":36697},{\"end\":36732,\"start\":36725},{\"end\":37024,\"start\":37017},{\"end\":37038,\"start\":37034},{\"end\":37054,\"start\":37046},{\"end\":37068,\"start\":37062},{\"end\":37084,\"start\":37077},{\"end\":37472,\"start\":37468},{\"end\":37483,\"start\":37476},{\"end\":37490,\"start\":37485},{\"end\":37741,\"start\":37735},{\"end\":37754,\"start\":37751},{\"end\":37766,\"start\":37762},{\"end\":37784,\"start\":37775},{\"end\":37800,\"start\":37795},{\"end\":37821,\"start\":37812},{\"end\":37836,\"start\":37831},{\"end\":37860,\"start\":37845},{\"end\":37875,\"start\":37870},{\"end\":37882,\"start\":37880},{\"end\":38258,\"start\":38256},{\"end\":38270,\"start\":38268},{\"end\":38282,\"start\":38279},{\"end\":38291,\"start\":38288},{\"end\":38303,\"start\":38299},{\"end\":38315,\"start\":38312},{\"end\":38660,\"start\":38658},{\"end\":38669,\"start\":38665},{\"end\":39238,\"start\":39229},{\"end\":39253,\"start\":39248},{\"end\":39269,\"start\":39262},{\"end\":39284,\"start\":39278},{\"end\":39298,\"start\":39292},{\"end\":39311,\"start\":39306},{\"end\":39630,\"start\":39622},{\"end\":39644,\"start\":39638},{\"end\":39648,\"start\":39646},{\"end\":39916,\"start\":39914},{\"end\":39929,\"start\":39923},{\"end\":39942,\"start\":39939},{\"end\":39955,\"start\":39949},{\"end\":39969,\"start\":39961},{\"end\":40406,\"start\":40403},{\"end\":40419,\"start\":40415},{\"end\":40436,\"start\":40428},{\"end\":40447,\"start\":40441},{\"end\":40456,\"start\":40454},{\"end\":40469,\"start\":40464},{\"end\":40482,\"start\":40478},{\"end\":40490,\"start\":40484},{\"end\":40835,\"start\":40831},{\"end\":40854,\"start\":40843},{\"end\":40868,\"start\":40862},{\"end\":40881,\"start\":40875},{\"end\":40901,\"start\":40891},{\"end\":40916,\"start\":40908},{\"end\":40935,\"start\":40925},{\"end\":41306,\"start\":41302},{\"end\":41325,\"start\":41319},{\"end\":41739,\"start\":41733},{\"end\":41750,\"start\":41748},{\"end\":41765,\"start\":41759},{\"end\":41774,\"start\":41767},{\"end\":42070,\"start\":42064},{\"end\":42081,\"start\":42076},{\"end\":42107,\"start\":42096},{\"end\":42591,\"start\":42583},{\"end\":42605,\"start\":42599},{\"end\":42616,\"start\":42612},{\"end\":42630,\"start\":42627},{\"end\":43121,\"start\":43117},{\"end\":43136,\"start\":43129},{\"end\":43152,\"start\":43146},{\"end\":43167,\"start\":43163},{\"end\":43176,\"start\":43169},{\"end\":43573,\"start\":43564},{\"end\":43734,\"start\":43726},{\"end\":43751,\"start\":43746},{\"end\":43764,\"start\":43758},{\"end\":43780,\"start\":43773},{\"end\":43794,\"start\":43788},{\"end\":43812,\"start\":43804},{\"end\":43836,\"start\":43822},{\"end\":43851,\"start\":43845},{\"end\":43861,\"start\":43853},{\"end\":44430,\"start\":44422},{\"end\":44444,\"start\":44438},{\"end\":44461,\"start\":44456},{\"end\":44854,\"start\":44846},{\"end\":44868,\"start\":44862},{\"end\":44885,\"start\":44880},{\"end\":45380,\"start\":45376},{\"end\":45392,\"start\":45387},{\"end\":45405,\"start\":45403},{\"end\":45413,\"start\":45411},{\"end\":45421,\"start\":45419},{\"end\":45434,\"start\":45431},{\"end\":45444,\"start\":45441},{\"end\":45730,\"start\":45721},{\"end\":45745,\"start\":45738},{\"end\":45756,\"start\":45754},{\"end\":46030,\"start\":46023},{\"end\":46044,\"start\":46037},{\"end\":46057,\"start\":46051},{\"end\":46074,\"start\":46065},{\"end\":46087,\"start\":46082},{\"end\":46102,\"start\":46097},{\"end\":46117,\"start\":46111},{\"end\":46135,\"start\":46125},{\"end\":46527,\"start\":46523},{\"end\":46539,\"start\":46536},{\"end\":46548,\"start\":46544},{\"end\":46560,\"start\":46556},{\"end\":46569,\"start\":46566},{\"end\":46582,\"start\":46579},{\"end\":46589,\"start\":46586},{\"end\":47075,\"start\":47069},{\"end\":47086,\"start\":47083},{\"end\":47548,\"start\":47542},{\"end\":47558,\"start\":47550},{\"end\":47866,\"start\":47864},{\"end\":47874,\"start\":47871},{\"end\":47881,\"start\":47879},{\"end\":47891,\"start\":47887},{\"end\":47900,\"start\":47897},{\"end\":47915,\"start\":47912},{\"end\":47928,\"start\":47925},{\"end\":48433,\"start\":48431},{\"end\":48443,\"start\":48439},{\"end\":48452,\"start\":48448},{\"end\":48467,\"start\":48464},{\"end\":48480,\"start\":48477},{\"end\":48637,\"start\":48635},{\"end\":48649,\"start\":48646},{\"end\":48658,\"start\":48654},{\"end\":48668,\"start\":48664},{\"end\":48677,\"start\":48674},{\"end\":48692,\"start\":48689},{\"end\":48705,\"start\":48702},{\"end\":48954,\"start\":48952},{\"end\":48963,\"start\":48959},{\"end\":48972,\"start\":48969},{\"end\":48987,\"start\":48984},{\"end\":49000,\"start\":48997},{\"end\":49356,\"start\":49354},{\"end\":49371,\"start\":49363},{\"end\":49385,\"start\":49381},{\"end\":49393,\"start\":49389},{\"end\":49406,\"start\":49404},{\"end\":49424,\"start\":49417},{\"end\":49440,\"start\":49432},{\"end\":49453,\"start\":49447},{\"end\":49462,\"start\":49459},{\"end\":49473,\"start\":49470},{\"end\":49483,\"start\":49475},{\"end\":49892,\"start\":49889},{\"end\":49899,\"start\":49897},{\"end\":49908,\"start\":49905},{\"end\":49920,\"start\":49916},{\"end\":49932,\"start\":49930},{\"end\":49944,\"start\":49941},{\"end\":49956,\"start\":49954},{\"end\":50219,\"start\":50216},{\"end\":50229,\"start\":50225},{\"end\":50239,\"start\":50237},{\"end\":50252,\"start\":50249},{\"end\":50261,\"start\":50258},{\"end\":50273,\"start\":50271},{\"end\":50286,\"start\":50283},{\"end\":50656,\"start\":50651},{\"end\":50672,\"start\":50668}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14096841},\"end\":34272,\"start\":33801},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11212020},\"end\":34643,\"start\":34274},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14421595},\"end\":35607,\"start\":34645},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":738435},\"end\":36172,\"start\":35609},{\"attributes\":{\"id\":\"b4\"},\"end\":36576,\"start\":36174},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3718988},\"end\":36964,\"start\":36578},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3648736},\"end\":37408,\"start\":36966},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206561058},\"end\":37655,\"start\":37410},{\"attributes\":{\"doi\":\"arXiv:1803.05567\",\"id\":\"b8\"},\"end\":38190,\"start\":37657},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4021462},\"end\":38574,\"start\":38192},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6411150},\"end\":39179,\"start\":38576},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4674781},\"end\":39574,\"start\":39181},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6628106},\"end\":39853,\"start\":39576},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":98180},\"end\":40345,\"start\":39855},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15280949},\"end\":40771,\"start\":40347},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15238391},\"end\":41232,\"start\":40773},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":15539264},\"end\":41639,\"start\":41234},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5730166},\"end\":41964,\"start\":41641},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":215824512},\"end\":42509,\"start\":41966},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":11080756},\"end\":43107,\"start\":42511},{\"attributes\":{\"id\":\"b20\"},\"end\":43493,\"start\":43109},{\"attributes\":{\"id\":\"b21\"},\"end\":43660,\"start\":43495},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":215827207},\"end\":44348,\"start\":43662},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":15600925},\"end\":44645,\"start\":44350},{\"attributes\":{\"id\":\"b24\"},\"end\":44778,\"start\":44647},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1114678},\"end\":45256,\"start\":44780},{\"attributes\":{\"id\":\"b26\"},\"end\":45662,\"start\":45258},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7961699},\"end\":45987,\"start\":45664},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13756489},\"end\":46418,\"start\":45989},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":19108083},\"end\":46992,\"start\":46420},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7317294},\"end\":47448,\"start\":46994},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2332513},\"end\":47759,\"start\":47450},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":52155342},\"end\":48363,\"start\":47761},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13539350},\"end\":48627,\"start\":48365},{\"attributes\":{\"doi\":\"arX- iv:1704.06933\",\"id\":\"b34\"},\"end\":48944,\"start\":48629},{\"attributes\":{\"id\":\"b35\"},\"end\":49244,\"start\":48946},{\"attributes\":{\"doi\":\"arXiv:1609.08144\",\"id\":\"b36\"},\"end\":49802,\"start\":49246},{\"attributes\":{\"id\":\"b37\"},\"end\":50138,\"start\":49804},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13481571},\"end\":50571,\"start\":50140},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":17667087},\"end\":51054,\"start\":50573}]", "bib_title": "[{\"end\":33850,\"start\":33801},{\"end\":34343,\"start\":34274},{\"end\":34699,\"start\":34645},{\"end\":35672,\"start\":35609},{\"end\":36650,\"start\":36578},{\"end\":37009,\"start\":36966},{\"end\":37457,\"start\":37410},{\"end\":38251,\"start\":38192},{\"end\":38647,\"start\":38576},{\"end\":39221,\"start\":39181},{\"end\":39618,\"start\":39576},{\"end\":39906,\"start\":39855},{\"end\":40393,\"start\":40347},{\"end\":40819,\"start\":40773},{\"end\":41294,\"start\":41234},{\"end\":41729,\"start\":41641},{\"end\":42056,\"start\":41966},{\"end\":42573,\"start\":42511},{\"end\":43719,\"start\":43662},{\"end\":44415,\"start\":44350},{\"end\":44839,\"start\":44780},{\"end\":45714,\"start\":45664},{\"end\":46014,\"start\":45989},{\"end\":46515,\"start\":46420},{\"end\":47063,\"start\":46994},{\"end\":47538,\"start\":47450},{\"end\":47856,\"start\":47761},{\"end\":48423,\"start\":48365},{\"end\":50207,\"start\":50140},{\"end\":50642,\"start\":50573}]", "bib_author": "[{\"end\":33870,\"start\":33852},{\"end\":33887,\"start\":33870},{\"end\":33898,\"start\":33887},{\"end\":33913,\"start\":33898},{\"end\":33924,\"start\":33913},{\"end\":33939,\"start\":33924},{\"end\":33956,\"start\":33939},{\"end\":33971,\"start\":33956},{\"end\":34363,\"start\":34345},{\"end\":34378,\"start\":34363},{\"end\":34393,\"start\":34378},{\"end\":34715,\"start\":34701},{\"end\":34733,\"start\":34715},{\"end\":34754,\"start\":34733},{\"end\":34769,\"start\":34754},{\"end\":34783,\"start\":34769},{\"end\":34798,\"start\":34783},{\"end\":34813,\"start\":34798},{\"end\":34828,\"start\":34813},{\"end\":34837,\"start\":34828},{\"end\":34856,\"start\":34837},{\"end\":34871,\"start\":34856},{\"end\":34885,\"start\":34871},{\"end\":34896,\"start\":34885},{\"end\":34912,\"start\":34896},{\"end\":34926,\"start\":34912},{\"end\":34940,\"start\":34926},{\"end\":35693,\"start\":35674},{\"end\":35710,\"start\":35693},{\"end\":36300,\"start\":36288},{\"end\":36308,\"start\":36300},{\"end\":36321,\"start\":36308},{\"end\":36329,\"start\":36321},{\"end\":36337,\"start\":36329},{\"end\":36350,\"start\":36337},{\"end\":36360,\"start\":36350},{\"end\":36667,\"start\":36652},{\"end\":36677,\"start\":36667},{\"end\":36691,\"start\":36677},{\"end\":36707,\"start\":36691},{\"end\":36734,\"start\":36707},{\"end\":37026,\"start\":37011},{\"end\":37040,\"start\":37026},{\"end\":37056,\"start\":37040},{\"end\":37070,\"start\":37056},{\"end\":37086,\"start\":37070},{\"end\":37474,\"start\":37459},{\"end\":37485,\"start\":37474},{\"end\":37492,\"start\":37485},{\"end\":37743,\"start\":37730},{\"end\":37756,\"start\":37743},{\"end\":37768,\"start\":37756},{\"end\":37786,\"start\":37768},{\"end\":37802,\"start\":37786},{\"end\":37823,\"start\":37802},{\"end\":37838,\"start\":37823},{\"end\":37862,\"start\":37838},{\"end\":37877,\"start\":37862},{\"end\":37884,\"start\":37877},{\"end\":38260,\"start\":38253},{\"end\":38272,\"start\":38260},{\"end\":38284,\"start\":38272},{\"end\":38293,\"start\":38284},{\"end\":38305,\"start\":38293},{\"end\":38317,\"start\":38305},{\"end\":38662,\"start\":38649},{\"end\":38671,\"start\":38662},{\"end\":39240,\"start\":39223},{\"end\":39255,\"start\":39240},{\"end\":39271,\"start\":39255},{\"end\":39286,\"start\":39271},{\"end\":39300,\"start\":39286},{\"end\":39313,\"start\":39300},{\"end\":39632,\"start\":39620},{\"end\":39646,\"start\":39632},{\"end\":39650,\"start\":39646},{\"end\":39918,\"start\":39908},{\"end\":39931,\"start\":39918},{\"end\":39944,\"start\":39931},{\"end\":39957,\"start\":39944},{\"end\":39971,\"start\":39957},{\"end\":40408,\"start\":40395},{\"end\":40421,\"start\":40408},{\"end\":40438,\"start\":40421},{\"end\":40449,\"start\":40438},{\"end\":40458,\"start\":40449},{\"end\":40471,\"start\":40458},{\"end\":40484,\"start\":40471},{\"end\":40492,\"start\":40484},{\"end\":40837,\"start\":40821},{\"end\":40856,\"start\":40837},{\"end\":40870,\"start\":40856},{\"end\":40883,\"start\":40870},{\"end\":40903,\"start\":40883},{\"end\":40918,\"start\":40903},{\"end\":40937,\"start\":40918},{\"end\":41308,\"start\":41296},{\"end\":41327,\"start\":41308},{\"end\":41741,\"start\":41731},{\"end\":41752,\"start\":41741},{\"end\":41767,\"start\":41752},{\"end\":41776,\"start\":41767},{\"end\":42072,\"start\":42058},{\"end\":42083,\"start\":42072},{\"end\":42089,\"start\":42083},{\"end\":42109,\"start\":42089},{\"end\":42593,\"start\":42575},{\"end\":42607,\"start\":42593},{\"end\":42618,\"start\":42607},{\"end\":42632,\"start\":42618},{\"end\":43123,\"start\":43109},{\"end\":43138,\"start\":43123},{\"end\":43154,\"start\":43138},{\"end\":43169,\"start\":43154},{\"end\":43178,\"start\":43169},{\"end\":43575,\"start\":43564},{\"end\":43736,\"start\":43721},{\"end\":43753,\"start\":43736},{\"end\":43766,\"start\":43753},{\"end\":43782,\"start\":43766},{\"end\":43796,\"start\":43782},{\"end\":43814,\"start\":43796},{\"end\":43838,\"start\":43814},{\"end\":43853,\"start\":43838},{\"end\":43863,\"start\":43853},{\"end\":44432,\"start\":44417},{\"end\":44446,\"start\":44432},{\"end\":44463,\"start\":44446},{\"end\":44856,\"start\":44841},{\"end\":44870,\"start\":44856},{\"end\":44887,\"start\":44870},{\"end\":45382,\"start\":45370},{\"end\":45394,\"start\":45382},{\"end\":45407,\"start\":45394},{\"end\":45415,\"start\":45407},{\"end\":45423,\"start\":45415},{\"end\":45436,\"start\":45423},{\"end\":45446,\"start\":45436},{\"end\":45732,\"start\":45716},{\"end\":45747,\"start\":45732},{\"end\":45758,\"start\":45747},{\"end\":46032,\"start\":46016},{\"end\":46046,\"start\":46032},{\"end\":46059,\"start\":46046},{\"end\":46076,\"start\":46059},{\"end\":46089,\"start\":46076},{\"end\":46104,\"start\":46089},{\"end\":46119,\"start\":46104},{\"end\":46137,\"start\":46119},{\"end\":46529,\"start\":46517},{\"end\":46541,\"start\":46529},{\"end\":46550,\"start\":46541},{\"end\":46562,\"start\":46550},{\"end\":46571,\"start\":46562},{\"end\":46584,\"start\":46571},{\"end\":46591,\"start\":46584},{\"end\":47077,\"start\":47065},{\"end\":47088,\"start\":47077},{\"end\":47550,\"start\":47540},{\"end\":47560,\"start\":47550},{\"end\":47868,\"start\":47858},{\"end\":47876,\"start\":47868},{\"end\":47883,\"start\":47876},{\"end\":47893,\"start\":47883},{\"end\":47902,\"start\":47893},{\"end\":47917,\"start\":47902},{\"end\":47930,\"start\":47917},{\"end\":48435,\"start\":48425},{\"end\":48445,\"start\":48435},{\"end\":48454,\"start\":48445},{\"end\":48469,\"start\":48454},{\"end\":48482,\"start\":48469},{\"end\":48639,\"start\":48629},{\"end\":48651,\"start\":48639},{\"end\":48660,\"start\":48651},{\"end\":48670,\"start\":48660},{\"end\":48679,\"start\":48670},{\"end\":48694,\"start\":48679},{\"end\":48707,\"start\":48694},{\"end\":48956,\"start\":48946},{\"end\":48965,\"start\":48956},{\"end\":48974,\"start\":48965},{\"end\":48989,\"start\":48974},{\"end\":49002,\"start\":48989},{\"end\":49358,\"start\":49346},{\"end\":49373,\"start\":49358},{\"end\":49387,\"start\":49373},{\"end\":49395,\"start\":49387},{\"end\":49408,\"start\":49395},{\"end\":49426,\"start\":49408},{\"end\":49442,\"start\":49426},{\"end\":49455,\"start\":49442},{\"end\":49464,\"start\":49455},{\"end\":49475,\"start\":49464},{\"end\":49485,\"start\":49475},{\"end\":49894,\"start\":49882},{\"end\":49901,\"start\":49894},{\"end\":49910,\"start\":49901},{\"end\":49922,\"start\":49910},{\"end\":49934,\"start\":49922},{\"end\":49946,\"start\":49934},{\"end\":49958,\"start\":49946},{\"end\":50221,\"start\":50209},{\"end\":50231,\"start\":50221},{\"end\":50241,\"start\":50231},{\"end\":50254,\"start\":50241},{\"end\":50263,\"start\":50254},{\"end\":50275,\"start\":50263},{\"end\":50288,\"start\":50275},{\"end\":50658,\"start\":50644},{\"end\":50674,\"start\":50658}]", "bib_venue": "[{\"end\":35064,\"start\":35001},{\"end\":35869,\"start\":35798},{\"end\":36769,\"start\":36760},{\"end\":37209,\"start\":37156},{\"end\":38858,\"start\":38773},{\"end\":40130,\"start\":40059},{\"end\":41450,\"start\":41397},{\"end\":42268,\"start\":42197},{\"end\":42785,\"start\":42717},{\"end\":43968,\"start\":43924},{\"end\":44496,\"start\":44488},{\"end\":45048,\"start\":44976},{\"end\":46728,\"start\":46668},{\"end\":47241,\"start\":47173},{\"end\":48089,\"start\":48018},{\"end\":50833,\"start\":50762},{\"end\":34029,\"start\":33971},{\"end\":34451,\"start\":34393},{\"end\":34999,\"start\":34940},{\"end\":35796,\"start\":35710},{\"end\":36286,\"start\":36174},{\"end\":36758,\"start\":36734},{\"end\":37154,\"start\":37086},{\"end\":37518,\"start\":37492},{\"end\":37728,\"start\":37657},{\"end\":38366,\"start\":38317},{\"end\":38771,\"start\":38671},{\"end\":39370,\"start\":39313},{\"end\":39708,\"start\":39650},{\"end\":40057,\"start\":39971},{\"end\":40550,\"start\":40492},{\"end\":40986,\"start\":40937},{\"end\":41395,\"start\":41327},{\"end\":41780,\"start\":41776},{\"end\":42195,\"start\":42109},{\"end\":42715,\"start\":42632},{\"end\":43293,\"start\":43178},{\"end\":43562,\"start\":43495},{\"end\":43922,\"start\":43863},{\"end\":44486,\"start\":44463},{\"end\":44710,\"start\":44647},{\"end\":44974,\"start\":44887},{\"end\":45368,\"start\":45258},{\"end\":45807,\"start\":45758},{\"end\":46186,\"start\":46137},{\"end\":46666,\"start\":46591},{\"end\":47171,\"start\":47088},{\"end\":47582,\"start\":47560},{\"end\":48016,\"start\":47930},{\"end\":48486,\"start\":48482},{\"end\":48763,\"start\":48725},{\"end\":49079,\"start\":49002},{\"end\":49344,\"start\":49246},{\"end\":49880,\"start\":49804},{\"end\":50337,\"start\":50288},{\"end\":50760,\"start\":50674}]"}}}, "year": 2023, "month": 12, "day": 17}