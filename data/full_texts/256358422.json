{"id": 256358422, "updated": "2023-10-05 05:00:23.684", "metadata": {"title": "Input Perturbation Reduces Exposure Bias in Diffusion Models", "authors": "[{\"first\":\"Mang\",\"last\":\"Ning\",\"middle\":[]},{\"first\":\"Enver\",\"last\":\"Sangineto\",\"middle\":[]},{\"first\":\"Angelo\",\"last\":\"Porrello\",\"middle\":[]},{\"first\":\"Simone\",\"last\":\"Calderara\",\"middle\":[]},{\"first\":\"Rita\",\"last\":\"Cucchiara\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64$\\times$64, we achieve a new state-of-the-art FID score of 1.27, while saving 37.5% of the training time. The code is publicly available at https://github.com/forever208/DDPM-IP", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2301.11706", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/NingSPCC23", "doi": "10.48550/arxiv.2301.11706"}}, "content": {"source": {"pdf_hash": "e8f78b44e82b0dacc0b64e86f43127dbd98e9196", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2301.11706v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "94bb14fcef38b07b7e0064a1a9df28260e4d0048", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e8f78b44e82b0dacc0b64e86f43127dbd98e9196.txt", "contents": "\nInput Perturbation Reduces Exposure Bias in Diffusion Models\n\n\nMang Ning \nEnver Sangineto \nAngelo Porrello \nSimone Calderara \nRita Cucchiara \nInput Perturbation Reduces Exposure Bias in Diffusion Models\n\nDenoising Diffusion Probabilistic Models have shown an impressive generation quality although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64\u00d764, we achieve a new state-of-theart FID score of 1.27, while saving 37.5% of the training time. The code is available at https: //github.com/forever208/DDPM-IP.\n\nIntroduction\n\nDenoising Diffusion Probabilistic Models (DDPMs) (Sohl-Dickstein et al., 2015;Ho et al., 2020) are a new generative paradigm which is attracting a growing interest due to its very high-quality sample generation capabilities (Dhariwal & Nichol, 2021;Nichol et al., 2022;Ramesh et al., 2022). Differently from most existing generative methods which synthesize a new sample in a single step, DDPMs resemble the Langevin dynamics (Welling & Teh, 2011) and the generation process is based on a sequence of denoising steps, in which a synthetic sample is created starting from pure noise and autoregressively reducing the noise component. In more detail, during training, a real sample x x x 0 is progressively destroyed in T steps adding Gaussian noise (forward process).\n\nThe sequence x x x 0 , ..., x x x t , ..., x x x T so obtained, is used to train a deep denoising autoencoder (\u00b5(\u00b7)) to invert the forward process:x x x t\u22121 = \u00b5(x x x t , t). At inference time, the generation process is autoregressive because it depends on the previously generated samples:x x x t\u22121 = \u00b5(x x x t , t) (Sec. 3).\n\nDespite the large success of DDPMs in different generative fields (Sec. 2), one of the main drawbacks of these models is their very long computational time, which depends on the large number of steps T required at both the training and the inference stage. As recently emphasised in (Xiao et al., 2022), the fundamental reason why T needs to be large is that each denoising step is assumed to be Gaussian, and this assumption holds only for small step sizes. Conversely, with larger step sizes, the prediction network (\u00b5(\u00b7)) needs to solve a harder problem and it becomes progressively less accurate (Xiao et al., 2022). However, in this paper, we observe that there is a second phenomenon, related to the sampling chain, but partially in contrast with the first, which is the accumulation of these errors over the T inference sampling steps. This is basically due to the discrepancy between the training and the inference stage, in which the latter generates a sequence of samples based on the results of the previous steps, hence possibly accumulating errors. In fact, at training time, \u00b5(\u00b7) is trained with a ground truth pair (x x x t , x x x t\u22121 ) and, given x x x t , it learns to reconstruct x x x t\u22121 (\u00b5(x x x t , t)). However, at inference time, \u00b5(\u00b7) has no access to the \"real\" x x x t , and its prediction depends on the previously generatedx x x t (\u00b5(x x x t , t)). This input mismatch between \u00b5(x x x t , t), used during training, and \u00b5(x x x t , t), used during testing, is similar to the exposure bias problem (Ranzato et al., 2016;Schmidt, 2019) shared by other autoregressive generative methods. For example, Rennie et al. (2017) argue that training a network to maximize the likelihood of the next ground-truth word given the previous ground-truth word (called \"Teacher-Forcing\" (Bengio et al., 2015)) results in error accumulation at inference time, since the model has never been exposed to its own predictions.\n\nIn this paper, we first empirically analyze this accumulation error phenomenon. For instance, we show that a standard DDPM (Dhariwal & Nichol, 2021), trained with T steps, can generate better results using a number of inference steps T < T (Sec. 6.2). A similar phenomenon was also observed by Nichol & Dhariwal (2021), but the authors did not provide an explanation for that. We believe that the reason for this apparently contrasting result is that while, on the one hand, longer chains can better satisfy the Gaussian assumption in the reverse diffusion process, on the other hand, they lead to a larger accumulation of errors.\n\nSecond, in order to alleviate the exposure bias problem, we propose a surprisingly simple yet very effective method, which consists in explicitly modelling the prediction error during training. Specifically, at training time, we perturb x x x t and we feed \u00b5(\u00b7) with a noisy version of x x x t , this way simulating the training-inference discrepancy, and forcing the learned network to take into account possible inferencetime prediction errors. Note that our perturbation is different from the content-destroying forward process, because the new noise is not used in the ground truth prediction target (Sec. 5.2). The proposed method is a training regularization which forces the network to smooth its prediction function: to solve the proposed task, two spatially close points x x x 1 and x x x 2 should lead to similar predictions \u00b5(x x x 1 , t) and \u00b5(x x x 2 , t). This regularization approach is similar to Mixup (Zhang et al., 2018) and the Vicinal Risk Minimization (VRM) principle (Chapelle et al., 2000), where a neighborhood around each sample in the training data is defined and then used to perturb that sample keeping fixed its target class label.\n\nThird, we propose alternative solutions to the exposure bias problem for diffusion models, in which, rather than using input perturbation, we obtain a smoother prediction function \u00b5(\u00b7) by explicitly encouraging \u00b5(\u00b7) to be Lipschitz continuous (Sec. 5.4). The rationale behind this is that a Lipschitz continuous function \u00b5(\u00b7) generates small prediction differences between neighbouring points in its domain, leading to a DDPM which is more robust to the inference-time errors.\n\nFinally, we empirically analyse all the proposed solutions and we show that, despite being all effective for improving the final generation quality, input perturbation is both more efficient and more effective than the explicit minimization of the Lipschitz constant in DDPMs (Sec. 6.1). Moreover, directly perturbing the network input at training time has no additional training overhead and this solution is very easy to be reproduced and plugged into existing DDPM frameworks: it can be obtained with just two lines of code without any change in the network architecture or the loss function. We call our method Denoising Diffusion Probabilistic Models with Input Perturbation (DDPM-IP) and we show that it can significantly improve the generation quality of state-of-theart DDPMs (Dhariwal & Nichol, 2021;Song et al., 2021a) and speed up the inference-time sampling. For instance, on the CIFAR10 (Krizhevsky et al., 2009), the ImageNet 32\u00d732 (Chrabaszcz et al., 2017), the LSUN 64\u00d764 (Yu et al., 2015) and the FFHQ 128\u00d7128  datasets, DDPM-IP, with only 80 sampling steps, generates lower FID scores than the state-of-the-art ADM (Dhariwal & Nichol, 2021) with 1,000 steps, corresponding to a more than 12.5\u00d7 sampling acceleration.\n\nIn summary, our contributions are:\n\n\u2022 We show that there is an exposure bias problem in DDPMs which has not been investigated so far.\n\n\u2022 To alleviate this problem, we propose different regularization methods whose common goal is to smooth the prediction function, and we specifically suggest input perturbation (DDPM-IP) as the best and the simplest of such solutions.\n\n\u2022 Using common benchmarks, we show that DDPM-IP can significantly improve the generation quality and drastically speed up both training and inference.\n\n\nRelated Work\n\nDiffusion models were introduced by Sohl-Dickstein et al. (2015) and later improved in (Song & Ermon, 2019;Ho et al., 2020;Song et al., 2021b;Nichol & Dhariwal, 2021). More recently, Dhariwal & Nichol (2021) have shown that DDPMs can yield higher-quality images than Generative Adversarial Networks (GANs) (Goodfellow et al., 2014;Brock et al., 2018). Similarly to GANs, the generation process in DDPMs can be both unconditional and conditioned. For instance, GLIDE (Nichol et al., 2022) learns to generate images according to an input textual sentence. Differently from GLIDE, where the diffusion model is defined on the image space, DALL\u00b7E-2 (Ramesh et al. (2022)) uses a DDPM to learn a prior distribution on the CLIP (Radford et al., 2021) space. Text-to-image generation is explored also in Stable Diffusion (Rombach et al., 2021) and Imagen (Saharia et al., 2022). Apart from images, DDPMs can also be used with categorical distributions (Hoogeboom et al., 2021;Gu et al., 2021), in an audio domain (Mittal et al., 2021;Chen et al., 2021), in time series forecasting (Rasul et al., 2021) and in other generative tasks (Yang et al., 2022;Croitoru et al., 2022). Differently from previous work, our goal is not to propose an application-specific prediction network, but rather to investigate the training-testing discrepancy of the DDPMs and propose a solution which can be used in different application fields and jointly with different denoising architectures.\n\nAccelerating the DDPM training or reducing the number of sampling steps T (Sec. 1) have been thoroughly investigated due to their practical implications. For instance, Song et al. (2021a) propose Denoising Diffusion Implicit Models (DDIMs), based on a non-Markovian diffusion process, which can use a number of inference sampling steps smaller than those used at training time, without retraining the network. Salimans & Ho (2022) propose to distil the prediction network into new networks which progressively reduce the number of sampling steps. However, the disadvantage is the need of training multiple networks. Rombach et al. (2021) speed up sampling by splitting the process into a compression stage and a generation stage, and applying the DDPM on the compressed (latent) space. Hoogeboom et al. (2022) present an order-agnostic DDPM, inspired by XLNet (Yang et al., 2019), in which the sequence x x x 0 , ..., x x x T is randomly permuted at training time, leading to a partially parallelized sampling process. Chen et al. (2021) found that, instead of conditioning the prediction network (\u00b5(\u00b7)) on a discrete diffusion step t, it is beneficial to condition \u00b5(\u00b7) on a continuous noise level. Similarly, Kong & Ping (2021) introduce continuous diffusion steps, resulting in a unified framework for fast sampling. In order to use larger size sampling steps and a non-Gaussian reverse process (Sec. 1) Xiao et al. (2022) include an adversarial loss in DDPMs and propose Denoising Diffusion GANs. Karras et al. (2022) suggest using Heun's second-order deterministic sampling method, leading to high quality results and fast sampling. Xu et al. (2022) accelerate the generation process of continuous normalizing flow using a Poisson flow generative model. Our approach is orthogonal to these previous works, and it can potentially be used jointly with most of them.\n\n\nBackground\n\nWithout loss of generality, we assume an image domain and we focus on DDPMs which define a diffusion process on the input space. Following (Nichol & Dhariwal, 2021;Dhariwal & Nichol, 2021), we assume that each pixel value is linearly scaled into [\u22121, 1]. Given a sample x x x 0 from the data distribution q(x x x 0 ) and a prefixed noise schedule (\u03b2 1 , ..., \u03b2 T ), a DDPM defines the forward process as a Markov chain which starts from a real image x x x 0 \u223c q(x x x 0 ) and iteratively adds Gaussian noise for T diffusion steps:\nq(x x x t |x x x t\u22121 ) = N (x x x t ; 1 \u2212 \u03b2 t x x x t\u22121 , \u03b2 t I I I),(1)q(x x x 1:T |x x x 0 ) = T t=1 q(x x x t |x x x t\u22121 ),(2)\nuntil obtaining a completely noisy image x x x T \u223c N (0 0 0, I I I). On the other hand, the reverse process is defined by transition probabilities parameterized by \u03b8 \u03b8 \u03b8:\np \u03b8 \u03b8 \u03b8 (x x x t\u22121 |x x x t ) = N (x x x t\u22121 ; \u00b5 \u03b8 \u03b8 \u03b8 (x x x t , t), \u03c3 t I I I),(3)where \u03c3 t = 1\u2212\u1fb1t\u22121 1\u2212\u1fb1t \u03b2 t with\u1fb1 t = t i=1 \u03b1 i and \u03b1 i = 1 \u2212 \u03b2 i .\nGiven x x x 0 , x x x t can be obtained (Ho et al., 2020) by:\nx x x t = \u221a\u1fb1 t x x x 0 + \u221a 1 \u2212\u1fb1 t ,(4)\nwhere is a noise vector ( \u223c N (0 0 0, I I I)). Instead of predicting the mean of the forward process posterior (i.e., x x x t\u22121 = \u00b5 \u03b8 \u03b8 \u03b8 (x x x t , t)), Ho et al. (2020) propose to use a network \u03b8 \u03b8 \u03b8 (\u00b7) which predicts the noise vector ( ). Using \u03b8 \u03b8 \u03b8 (\u00b7) and a simple L 2 loss function, the training objective becomes:\nL(\u03b8 \u03b8 \u03b8) = E x x x0\u223cq(x x x0), \u223cN (0 0 0,I I I),t\u223cU({1,...,T }) [|| \u2212 \u03b8 \u03b8 \u03b8 (x x x t , t)|| 2 ].(5)\nNote that, in Eq. 5, x x x t and are ground-truth terms, while\n\u03b8 \u03b8 \u03b8 (x x x t , t)\nis the network prediction. Using Eq. 5, the training and the sampling algorithms are described in Alg. 1-2, respectively.\n\nAlgorithm 1 DDPM Standard Training 1: repeat 2: \nx x x 0 \u223c q(x x x 0 ), t \u223c U({1, ..., T }), \u223c N (0 0 0, I I I) 3: compute x x x t using Eq. 4 4: take a gradient descent step on \u2207 \u03b8 \u03b8 \u03b8 || \u2212 \u03b8 \u03b8 \u03b8 (x x x t , t)|| 2 5: until converged4:x x x t\u22121 = 1 \u221a \u03b1t (x x x t \u2212 1\u2212\u03b1t \u221a 1\u2212\u1fb1t \u03b8 \u03b8 \u03b8 (x x x t , t)) + \u03c3 t z z z 5: end for 6: returnx x x 0\n\nExposure Bias Problem in Diffusion Models\n\nComparing line 4 of Alg. 1 with line 4 of Alg. 2, we note that the inputs of the prediction network \u03b8 \u03b8 \u03b8 (\u00b7) are different between the training and the inference phase. Concretely, at training time, standard DDPMs use \u03b8 \u03b8 \u03b8 (x x x t , t), where x x x t is a ground truth sample (Eq. 4). In contrast, at inference time, they use \u03b8 \u03b8 \u03b8 (x x x t , t)), wherex x x t is computed based on the output of \u03b8 \u03b8 \u03b8 (\u00b7) at the previous sampling step t+1. As mentioned in Sec. 1, this leads to a training-inference discrepancy, which is similar to the exposure bias problem observed, e.g., in text generation models, in which the training generation is conditioned on a ground-truth sentence, while the testing generation is conditioned on the previously generated words (Ranzato et al., 2016;Schmidt, 2019;Rennie et al., 2017;Bengio et al., 2015). In order to quantify the error accumulation with respect to the number of inference sampling steps, we use a simple experiment in which we start from a (randomly selected) real image x x x 0 , we compute x x x t using Eq. 4, and then apply the reverse process (Alg. 2) starting from x x x t instead of a random x x x T . This way, when t is small enough, the network should be able to \"recover\" the path to x x x 0 (the denoising task is easier). We quantify the total error accumulated in t reverse diffusion steps by comparing the difference between the ground truth distribution q(x x x 0 ) and the predicted distribution q(x x x 0 ) using the FID scores in Tab. 1. The experiment was done using ADM (Dhariwal & Nichol, 2021) (trained with T = 1, 000) and ImageNet 32\u00d732, and we compute the FID scores using 50k samples. Tab. 1 (first row) shows that the longer the reverse process, the higher the FID scores, indicating the existence of an error accumulation which is larger with larger values of t. In Appendix 5, we repeat this experiment using deterministic sampling, which quantifies the error accumulation removing the randomness from the sampling process. Finally, in Tab. 3 we will report the FID scores of ADM on different datasets, which show that most of the best results are obtained in the range from 100 to 300 sampling steps, despite all the models have been trained with 1,000 diffusion steps. These results confirm previous similar observations (Nichol & Dhariwal, 2021), and we believe that the reason for this apparently counterintuitive phenomenon, in which fewer sampling steps lead to a better generation quality, is due to the exposure bias problem. Indeed, while more sampling steps correspond to a reverse process which can be more easily approximated with a Gaussian distribution (Sec. 1), longer sampling trajectories produce a larger accumulation of the prediction errors. Hence, the range [100, 300] leads to a better generation quality because it presumably trades off these two opposing aspects.\n\n\nMethod\n\n\nRegularization with Input Perturbation\n\nThe solution we propose to alleviate the exposure bias problem is very simple: we explicitly model the prediction error using a Gaussian input perturbation at training time. More specifically, we assume that the error of the prediction network in the reverse process at time t + 1 is normally distributed with respect to the ground-truth input x x x t (see Sec. 5.3). This is simulated using a second, dedicated ran-dom noise vector \u03be \u03be \u03be \u223c N (0 0 0, I I I), using which, we create a perturbed version (y y y t ) of x x x t :\ny y y t = \u221a\u1fb1 t x x x 0 + \u221a 1 \u2212\u1fb1 t ( + \u03b3 t \u03be \u03be \u03be).(6)\nFor simplicity, we use a uniform noise schedule for \u03be \u03be \u03be by setting \u03b3 0 = ... = \u03b3 T = \u03b3. In fact, although selecting the best noise schedule (\u03b2 1 , ..., \u03b2 T ) in DDPMs is usually very important to get high-quality results (Ho et al., 2020;Chen et al., 2021), it is nevertheless an expensive hyperparameter tuning operation (Chen et al., 2021). Therefore, to avoid adding a second noise schedule (\u03b3 0 , ..., \u03b3 T ) to the training procedure, we opted for a simpler (although most likely suboptimal) solution, in which \u03b3 t does not vary depending on t (more details in Sec. 5.3). In Alg. 3 we show the proposed training algorithm, in which x x x t is replaced by y y y t . In contrast, at inference time, we use Alg. 2 without any change.\n\nAlgorithm 3 DDPM-IP: Training with input perturbation 1: repeat 2:\n\nx\nx x 0 \u223c q(x x x 0 ), t \u223c U({1, ..., T }) 3: \u223c N (0 0 0, I I I), \u03be \u03be \u03be \u223c N (0 0 0, I I I) 4:\ncompute y y y t using Eq. 6 5:\n\ntake a gradient descent step on \u2207 \u03b8 \u03b8 \u03b8 || \u2212 \u03b8 \u03b8 \u03b8 (y y y t , t)|| 2 6: until converged\n\n\nDiscussion\n\nIn this section, we analyze the difference between Alg. 3 and Alg. 1. Specifically, in line 5 of Alg. 3, we use y y y t as the input of the prediction network \u03b8 \u03b8 \u03b8 (\u00b7) but we keep using as the regression target. In other words, the new noise term (\u03be \u03be \u03be) we introduce is used asymmetrically, because it is applied to the input but not to the prediction target ( ). For this reason, Alg. 3 is not equivalent to choose a different value of in Alg. 1, where is instead used symmetrically both in the forward process (Eq. 4) and as the target of the prediction network (line 4 of Alg. 1).\n\nThis difference is schematically illustrated in Fig. 1, where, for both Alg. 1 (i.e., DDPM) and Alg. 3 (DDPM-IP), we show the corresponding pairs of input and target vectors of the prediction network (respectively, (x x x t , ) and (y y y t , )). In the same figure, we also show a second version of Alg. 1 (called DDPM-y), where we use the standard training protocol (Alg. 1) but change the noise variance in order to adhere to the same distribution generating y y y t . In fact, it can be easy shown that y y y t in Alg. 3 is generated using the following distribution (see Appendix A.2 for a proof):\nq(y y y t |x x x 0 ) = N (y y y t ; \u221a\u1fb1 t x x x 0 , (1 \u2212\u1fb1 t )(1 + \u03b3 2 )I I I).(7)\nHence, we can obtain the same input noise distribution of Alg. 3 in Alg. 1 using \u223c N (0 0 0, I I I) and:\ny y y t = \u221a\u1fb1 t x x x 0 + \u221a 1 \u2212\u1fb1 t 1 + \u03b3 2 .(8)\nWe call DDPM-y the version of Alg. 1 with this new noise distribution. DDPM-y is obtained from Alg. 1 using Eq. 8 in line 3 and replacing x x x t with y y y t and with in line 4. However, note that, for a given y y y t , if \u03be \u03be \u03be = 0 0 0, then = (see Fig. 1), thus, DDPM-IP and DDPM-y share the same input to \u03b8 \u03b8 \u03b8 (\u00b7), but they use different targets. In Appendix A.3, we empirically show that DDPM-y is even worse than the standard DDPM.\n\nIntuitively, the proposed training protocol, DDPM-IP, decouples the noise vector actually generating y y y t from the ground truth target vector which is asked to be predicted by \u03b8 \u03b8 \u03b8 (\u00b7). In order to solve this problem, \u03b8 \u03b8 \u03b8 (\u00b7) needs to smooth its prediction function, reducing the difference between \u03b8 \u03b8 \u03b8 (x x x t , t) and \u03b8 \u03b8 \u03b8 (y y y t , t), and this leads to a training regularization which is similar to VRM (Sec. 1). \n\n\nEstimating the Prediction Error\n\nIn this section, we analyze the actual prediction error of \u03b8 \u03b8 \u03b8 (\u00b7) and we use this analysis to choose the value of \u03b3 in Eq. 6. Analogously to Sec. 4, we use ADM, trained using the standard algorithm Alg. 1 and two datasets: CIFAR10 and ImageNet 32\u00d732. At testing time, for a given t and = \u03b8 \u03b8 \u03b8 (x x x t , t), we replace with\u02c6 in Eq. 4 and we compute the predictedx x x 0 . Finally, the prediction error at time t is e e e t =x x x 0 \u2212 x x x 0 . Note that usingx x x 0 and x x x 0 to estimate the error instead of comparingx x x t and x x x t , has the advantage that the former is independent of scaling factors ( \u221a 1 \u2212\u1fb1 t ) and, thus, it makes the statistical analysis easier. Using different values of t, uniformly selected in {1, ..., T }, we empirically verified that, for a given t, e e e t is normally distributed: e e e t \u223c N (0 0 0, \u03bd 2 t I I I), with standard deviation \u03bd t (see Appendix A.5). In Fig. 2 we plot the value of \u03bd t with respect to t. The two curves corresponding to the two datasets are surprisingly close to each other. In principle, we could use this empirical analysis and set \u03b3 t = \u03bd t in Eq. 6. In this way, when we perturb the input to \u03b8 \u03b8 \u03b8 (\u00b7), we empirically imitate its actual prediction error which is the base of the exposure bias problem. However, this choice would require a two-step training: first, using Alg. 1 to train the base model and empirically estimate \u03bd t for different t. Then, using Alg. 3 with the estimated \u03b3 t schedule to retrain the model from scratch. To avoid this and make the whole procedure as simple as possible, we simply use a constant value \u03b3, independently of t. This value was empirically set using a grid search on both CIFAR10 and Im-ageNet 32\u00d732 on a small range of values covering the last half of the sampling trajectory. Specifically, we investigated the range \u03bd t \u2208 [0, E t [\u03bd t ]] = [0, 0.2] (see Fig. 2), which was chosen following Karras et al. (2022), who showed that the last part of the inference trajectory has usually the largest impact on the Diffusion Model performance. We finally set \u03b3 = 0.1 and, in the rest of this paper, we always use a constant \u03b3 = 0.1, regardless of the dataset and the baseline DDPM. Although a DDPM-specific \u03b3 value would most likely lead to better quality results, we prefer to emphasise the ease of use of our proposal which does not depend on any other hyperparameter. \n\n\nRegularization based on Lipschitz Continuous Functions\n\nIn this section, we propose two alternative solutions to the exposure bias problem which can help to better investigate the phenomenon. The goal is the same as in Sec. 5.1, i.e., we want to smooth the prediction function \u03b8 \u03b8 \u03b8 (x x x t , t) to make it more robust with respect to local variations of x x x t which are due to the inference-time prediction errors. To do so, instead of using input perturbation, we explicitly encourage \u03b8 \u03b8 \u03b8 (\u00b7) to be Lipschitz continuous, i.e. to satisfy:\n\n|| \u03b8 \u03b8 \u03b8 (x x x, t) \u2212 \u03b8 \u03b8 \u03b8 (y y y, t)|| \u2264 K||x x x \u2212 y y y||, \u2200(x x x, y y y)\n\nfor a small constant K. We implement this idea using two standard Lipschitz constant minimization methods: gradient penalty (Rifai et al., 2011;Gulrajani et al., 2017) and weight decay (Krogh & Hertz, 1991;Miyato et al., 2018). In both cases we do not perturb the input of \u03b8 \u03b8 \u03b8 (\u00b7), and we use the original training algorithm (Alg. 1), with the only difference being the loss function used in line 4, where the L 2 loss is used jointly with a regularization term described below.\n\nGradient penalty. In this case, the regularization is based on the Frobenius norm of the Jacobian matrix (Rifai et al., 2011;Goodfellow et al., 2016), and the final loss is:\nL GP (\u03b8 \u03b8 \u03b8) = || \u2212 \u03b8 \u03b8 \u03b8 (x x x t , t)|| 2 + \u03bb GP \u2202 \u03b8 \u03b8 \u03b8 (x x x t , t) \u2202x x x 2 F ,(10)\nwhere \u03bb GP is the weight of the gradient penalty term. However, a gradient penalty regularization is very slow (Yoshida & Miyato, 2017) because it involves one forward and two backward passes for each training step.\n\nWeight decay. As shown in , Lipschitz continuity can also be encouraged using a weight decay regularization (see Appendix A.6 for more details). In this case, the final loss is:\nL W D (\u03b8 \u03b8 \u03b8) = || \u2212 \u03b8 \u03b8 \u03b8 (x x x t , t)|| 2 + \u03bb W D ||\u03b8 \u03b8 \u03b8|| 2 ,(11)\nwhere \u03bb W D is the weight of the regularization term.\n\n\nResults\n\nIn this section, we evaluate the generation quality of the proposed solutions and we compare them with state-of-the-art DDPMs. We use unconditional image generation tasks on different datasets and standard metrics: the Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017) and the Spatial Fr\u00e9chet Inception Distance (sFID) (Nash et al., 2021). As a variant of FID, sFID uses spatial features rather than the standard pooled features to better capture spatial relationships, rewarding image distributions with a coherent high-level structure. As mentioned in Sec. 5.3, in all our experiments we use \u03b3 = 0.1 without any dataset or baseline specific tuning of our only hyperparameter.\n\n\nEvaluation of the Different Proposed Solutions\n\nIn this section, we empirically compare to each other the three regularization methods proposed in Sec. 5 to alleviate the exposure bias problem. For all three approaches, we use the state-of-the-art diffusion model ADM (Dhariwal & Nichol, 2021) (without classifier guidance) as the baseline, and we call: (1) \"ADM-IP\" the version of ADM trained using Alg. 3, (2) \"ADM-GP\" the version of ADM trained using the gradient penalty, and (3) \"ADM-WD\" for the weight decay (Sec. 5.4). We use \u03bb GP = 1e\u22126 and \u03bb W D = 0.03 as the loss weights for ADM-GP and ADM-WD, respectively.\n\nFor this experiment, we use CIFAR10 because ADM-GP is too time-consuming to be trained on larger datasets. The results in Tab. 2 show that all three models outperform the baseline in image quality, demonstrating the effectiveness of smoothing the prediction function using the proposed regularization methods. However, training ADM-GP is too slow and cannot be scaled to larger datasets, thus we do not recommend this solution. Moreover, ADM-IP gets the best FID and sFID scores, thus, in the rest of this paper, we use the input perturbation approach described in Sec. 5.1 as our basic solution. Finally, we use ADM-IP to quantify the reduction in the exposure bias following the protocol described in Sec. 4. The results reported in Tab. 1 show that ADM-IP leads to a significantly lower exposure bias than ADM, and this difference is larger with longer sampling sequences.\n\n\nMain results\n\nComparison with DDPMs. We compare ADM-IP with ADM using CIFAR10, ImageNet 32\u00d732, LSUN tower 64\u00d764, CelebA 64\u00d764 (Liu et al., 2015) and FFHQ 128\u00d7128. Following prior work (Ho et al., 2020;Nichol & Dhariwal, 2021), we generate 50K samples for each trained model and we use the full training set to compute the reference distribution statistics, except for LSUN tower where (again following (Ho et al., 2020;Nichol & Dhariwal, 2021)) we use 50K training samples as the reference data. When training, we always use T = 1, 000 steps for all the models. At inference time, the results reported with T < T sampling steps have been obtained using the respacing technique (Nichol & Dhariwal, 2021). As previously mentioned (see Sec. 5.3) we keep fixed \u03b3 = 0.1 in all the experiments The results reported in Tab. 3 show that, independently of the dataset and the number of sampling steps (T \u2264 T ), ADM-IP is always better than ADM in terms of both the FID and sFID metrics, sometimes drastically better. For instance, on LSUN, with T = 80, we have a more than 5 sFID score improvement with respect to ADM. On FFHQ 128\u00d7128, with T = 1, 000, we have almost 7 points of improvement compared to both the FID and the sFID scores. In addition to the experiments shown in Tab. 3, we used T = 900 sampling steps and our ADM-IP on CelebA 64\u00d764, achieving a result of 1.27 FID, which is the new state-of-the-art performance for unconditional generation on this dataset.\n\nNote that, for most datasets, both the baseline (ADM) and ADM-IP reach the best results with T < T (specifically, with T \u2208 [100, 300]). As mentioned in Sec. 4, this is most likely a confirmation of the exposure bias problem: a shorter sampling trajectory accumulates a smaller prediction error.\n\nBesides generating significantly better images, ADM-IP converges much faster than the baseline during training in all the five datasets (see Fig. 3 and 4). For instance, on LSUN tower and CelebA, ADM-IP converges at 220K and 300K training iterations while ADM saturates around 300K and 480K iterations, respectively. Fig. 3 shows also that, even before convergence, ADM-IP quickly beats the ADM results obtained when the latter has converged. For instance, on CelebA, ADM-IP gets FID 1.51 at 120K training iterations, whereas ADM gets FID 1.6 at convergence (480K iterations), exhibiting a 4x training speed-up. On the larger resolution FFHQ dataset, ADM receives FID 14.52 at convergence (420K iterations), while ADM-IP achieves a FID score of 8.81 with only 60K iterations: an improvement of 5.71 points with a 7x training speed-up. Fig. 4 shows a similar trend for the CIFAR10 dataset. In this figure, we also plot the results of ADM-IP with different \u03b3 values (Sec. 5.3).\n\nThe training iterations until convergence for each model are summarized in Tab. 4. The much faster convergence of our method is most likely due to the regularization effect of the input perturbation. In fact, as commonly happens with regularization techniques (Zhang et al., 2018;Liu et al., 2021;Balestriero et al., 2022), the proposed input perturbation also introduces an inductive bias in training. In our case, it is: close points in the domain of the prediction function should lead to similar outcomes. Our empirical results show that this bias helps the DDPM training.\n\nTab. 4 also shows that ADM-IP can drastically accelerate the inference process, i.e. obtaining better results than the baseline with shorter sampling trajectories. For example, with only 60 or 80 steps, ADM-IP gets a better or an equivalent FID than ADM (tested with the standard 1,000 sampling steps) on all datasets, except for CelebA, where ADM-IP needs 200 sampling steps to reach the same result. This comparison shows a remarkable 5x to 16.7x speed-up of the inference stage, which is particularly significant for the larger resolution FFHQ dataset.\n\nFinally, we measure the recall and precision for the generated samples using the method in Kynk\u00e4\u00e4nniemi et al. (2019). The results show that the recall and precision achieved by ADM and ADM-IP have no significant difference, which indicates that our input perturbation does not affect the sample diversity (see Appendix A.4).\n\nComparison with DDIMs. In order to show the generality of our proposal, we use Alg. 3 with the Denoising Diffusion Implicit Models (DDIMs) proposed by Song et al. (2021a) (Sec. 2). We train both the baseline (DDIM) and our method (DDIM-IP) on CIFAR10 using the public code provided by Song et al. (2021a). Since training with DDIM is particu-Training iterations (  larly slow, we use only CIFAR10 for this comparison. We use the default hyperparameters settings (e.g. T = 1, 000) in their code and train both models for 1,600K iterations with batch size 128. We test the performance of the two models with both \u03b7 = 0 and \u03b7 = 0.5, where \u03b7 is the co-efficient of stochasticity sampling in DDIMs. Also in this case, for our method (DDIM-IP) we use \u03b3 = 0.1 without any fine-tuning.\n\nWe report the results in Tab. 5, which show that DDIM-IP consistently obtains better FID scores than DDIM in all conditions (i.e., independently of the number of sampling steps and the value of \u03b7). Importantly, the fewer the sampling steps, the more the FID gain which is obtained with input perturbation. For instance, with \u03b7 = 0.5, the FID gain of DDIM-IP is 7.16 with 10 sampling steps versus 0.89 with 1,000 sampling steps. Analogously, with \u03b7 = 0 and 10 sampling steps, DDIM-IP drastically improves DDIM with a 3.67 FID margin. Since the main advantage of DDIMs with respect to DDPMs is their reduced number of sampling steps (Song et al., 2021a), and they indeed are mainly used for accelerating the inference stage, input perturbation greatly matches this goal, and it significantly improves the sample quality of the implicit models in a short sampling sequence regime. \n\n\nConclusions\n\nIn this paper, we proposed DDPM-IP, a regularization method for DDPM training which is based on input perturbation to explicitly model the prediction errors and alleviate the DDPM exposure bias problem. We empirically showed that DDPM-IP can significantly improve image quality and drastically reduce both the training and the inference time. The proposed method is straightforward and does not require any change in the network architecture or the specific loss function. This simplicity makes it very easy to be reproduced and plugged into existing DDPMs. Although we tested DDPM-IP only on an image domain, there are no domain-specific assumptions behind our method, hence we presume it can be more generally applied to other domains.\n\nLimitations. Since training DDPMs is very computationally heavy, in this paper we used only datasets with small resolution images. We leave the extension of our experiments to larger resolution images (and corresponding larger backbone networks) as a future work. However, we em-phasize that our best results have been obtained with FFHQ 128\u00d7128, which is the dataset with the largest resolution images we tested, which probably confirms that our regularization method is specifically effective with higher dimensional input spaces.\n\n\nA. Appendix\n\n\nA.1. Exposure Bias Analysis\n\nIn this section, we repeat the experiment in Sec. 4 by removing the randomness component of the sampling process in order to isolate the error of the reverse process which is due only to the prediction network. Specifically, we use again ADM (Dhariwal & Nichol, 2021) (trained with T = 1, 000) and ImageNet 32\u00d732, and we directly measure the difference between a ground truth real image x x x 0 and the predictedx x x 0 using a deterministic sampling, described in Alg. 4. In more detail, given a real image x x x 0 , we first compute x x x t by Eq. 4, then we use the pre-trained network \u03b8 \u03b8 \u03b8 (trained with the standard algorithm Alg. 1) to run the reverse diffusion for t steps. Note that we adopt the equation in line 4 of Alg. 2 but we remove the stochastic term \u03c3 t z z z. Differently from the analogous experiment presented in Sec. 4, this deterministic reverse diffusion process allows the model to target the mode of x x x 0 instead of favouring diversity (Luo, 2022). Finally, we use the average pixel-wise L 1 distance between x x x 0 andx x x 0 to estimate the cumulative error computed in the whole trajectory of t steps. Note that, since each pixel is normalized in [\u22121, 1] (Sec. 3), then this distance is upper bounded by 2.\n\nAlgorithm 4 Deterministic measurement of exposure bias 1: Initialize \u03b4 t = 0, n t = 0 (\u2200t \u2208 {1, ..., T }) 2: repeat 3: for \u03c4 := t, ..., 1 do\nx x x 0 \u223c q(x x x 0 ), t \u223c U({1,6:x x x \u03c4 \u22121 = 1 \u221a \u03b1\u03c4 (x x x \u03c4 \u2212 1\u2212\u03b1\u03c4 \u221a 1\u2212\u1fb1\u03c4 \u03b8 \u03b8 \u03b8 (x x x \u03c4 , \u03c4 )) 7:\nend for 8:\n\u03b4 t = \u03b4 t + ||x x x 0 \u2212x x x 0 || 1 /M , where M is the number of pixels in x x x 0 9:\nn t = n t + 1 10: until N iterations 11: if n t = 0, then\u03b4 t = \u03b4t nt (\u2200t \u2208 {1, ..., T })\n\nIn Tab. 6, we report the exposure bias measured using\u03b4 t with respect to different trajectory lengths (t). This table shows that the error accumulates greatly as the number of reverse diffusion steps increases. In Fig. 5 we visualize a few pairs of images (x x x 0 ,x x x 0 ) with the corresponding length of the diffusion trajectory (t). These images clearly show how large the error is accumulated with the diffusion chain getting longer. \n\n\nA.2. Distribution of the Perturbed Input\n\nIn this section, we prove that y y y t is Gaussian distributed as described in Eq. 7. Generally speaking, if A \u223c N (\u00b5 A , \u03c3 2 A ) and B \u223c N (\u00b5 B , \u03c3 2 B ) are two independent Gaussian distributed random variables, then its linear combination S = aA + bB (with a, b two scalars) is also Gaussian distributed:\nS \u223c N (a\u00b5 A + b\u00b5 B , a 2 \u03c3 2 A + b 2 \u03c3 2 B ).(12)\nIn our case, we have that, for a given x x x 0 , y y y t is a linear combination of x x x t and \u03be \u03be \u03be, which are two independent, Gaussian distributed random variables: Figure 5. Visualization of the exposure bias problem with different diffusion chain lengths.\nq(x x x t |x x x 0 ) = N (x x x t ; \u221a\u1fb1 t x x x 0 , (1 \u2212\u1fb1 t )I I I),(13)\u03be \u03be \u03be \u223c N (0 0 0, I I I), (14) y y y t = x x x t + \u221a 1 \u2212\u1fb1 t \u03b3\u03be \u03be \u03be.(15)\nHence, if in Eq. 12 we replace S with y y y t , A with x x x t , B with \u03be \u03be \u03be, and we use a = 1 and b = \u221a 1 \u2212\u1fb1 t \u03b3, we get:\nq(y y y t |x x x 0 ) = N (y y y t ; \u221a\u1fb1 t x x x 0 , (1 \u2212\u1fb1 t )I I I + \u03b3 2 (1 \u2212\u1fb1 t )I I I) = (16) = N (y y y t ; \u221a\u1fb1 t x x x 0 , (1 \u2212\u1fb1 t )(1 + \u03b3 2 )I I I).(17)\nA.3. Ablation Study: Input Perturbation is not Equivalent to Using a Different Noise Variance\n\nThe goal of this section is to empirically show that DDPM-IP is not equivalent to using a standard DDPM algorithm with a different noise distribution. Following the discussion in Sec. 5.2, and adopting the same terminology, we compare DDPM-IP with DDPM-y, where the latter is trained using the standard algorithm (Alg. 1) but adopting the noise distribution of y y y t . Tab. 7 shows that DDPM-y is even worse than DDPM. \nW x \u2264 W F \u00b7 x ,(23)\nwhere the definition of the Frobenius Norm is:\nW F = i,j w 2 i,j .\nThus, we can use the Frobenius Norm W F to approximate the constant K. Minimizing this constant during training is often implemented by adding a loss term \u03bb W 2 F to the loss function. This loss term is exactly the Weight Decay according to the definition of W F = i,j w 2 i,j .\n\n\nA.7. Hyperparameters\n\nFor both ADM and ADM-IP, we use the hyperparameters specified in (Dhariwal & Nichol, 2021), except for LSUN tower, for which we used a resolution of 64\u00d764. The hyperparameter values are reported in Tab. 9. We train all the models using the AdamW optimizer (Loshchilov & Hutter, 2019). Furthermore, we use 16-bit precision and loss-scaling (Micikevicius et al., 2017) for mixed precision training, but keeping 32-bit weights, EMA, and the optimizer state. We use an EMA rate of 0.9999 for all the experiments. These settings are the same as in (Dhariwal & Nichol, 2021).\n\nWe use Pytorch 1.8 (Paszke et al., 2019) and trained all the models on different NVIDIA Tesla V100s (16G memory). In more detail, we use 2 GPUs to train the models on CIFAR10 for 2 days, and 4 GPUs to train the models on ImageNet 32\u00d732 for 34 days. For LSUN tower 64\u00d764, CelebA 64\u00d764 and FFHQ 128\u00d7128, we used 16 GPUs to train the models for 3 days, 5 days and 4 days, respectively. Regarding the DDIM and DDIM-IP experiments, we use the default hyperparameters specified in the public code of Song et al. (2021a). We train both DDIM and DDIM-IP on CIFAR10 from scratch for 1600K iterations with batch size 128. The complete list of hyperparameters is shown in Tab. 10. We train DDPM/DDPM-IP with a single NVIDIA Tesla V100s (16G memory) for 8 days on a Pytorch 1.8 platform. In this section, we qualitatively compare ADM with ADM-IP. For a fair comparison, we start sampling the same x x x T for both models. Fig. 7 , 8, 9, 10, 11 show that the images generated by ADM-IP are usually comparable or better than those produced by ADM. For example, in Fig. 7, ADM fails to run into the bird, the boat and the dog modes in the first, the third and the sixth image on the second row. Similarly, in Fig. 9, ADM fails to complete the building in the fourth image on the second row. Moreover, the details and colors of the towers generated by ADM-IP are more visually realistic and appealing. Finally, on the FFHQ 128\u00d7128 dataset, the ADM generated samples suffer from overexposure and loss of background detail, whereas the ADM-IP samples do not (see Fig. 11).\n\nPublished as a Conference Paper at ICML 2023 (a) Samples generated by ADM trained on ImageNet 32\u00d732 (FID 3.53) (b) Samples generated by ADM-IP trained on ImageNet 32\u00d732 (FID 2.72) Figure 8. ImageNet 32\u00d732, qualitative results. The samples are generated using 1,000 sampling steps.\n\n\nA.9. Additional Qualitative Results for ADM-IP\n\nWe show additional images generated by our ADM-IP models trained on CIFAR10 (Fig. 12), ImageNet 32\u00d732 (Fig. 13), LSUN tower 64\u00d764 (Fig. 14), CelebA 64\u00d764 (Fig. 15) and FFHQ 128\u00d7128 (Fig. 16). For each dataset, we used the best number of sampling steps as indicated in Tab. 3.\n\nPublished as a Conference Paper at ICML 2023  \n\n\nfor t := T, ..., 1 do 3:if t > 1 then z z z \u223c N (0 0 0, I I I), else z z z = 0 0 0\n\nFigure 1 .\n1The inputs and the prediction targets are different in vanilla DDPM, DDPM-IP and DDPM-y.\n\nFigure 2 .\n2The inference time standard deviation \u03bdt of the prediction error of a pre-trained network with respect to the sampling step t. The mean of the blue and the orange curve is 0.20 and 0.19, respectively.\n\nFigure 3 .\n3FID scores with respect to the number of training iterations. Each FID value is computed using T = 1, 000 inference sampling steps, except for the FFHQ dataset, for which we used T = 100.\n\nFigure 4 .\n4CIFAR10: FID scores with respect to the number of training iterations with different \u03b3 values. Each FID score is computed using T = 100 inference sampling steps.\n\nFigure 7 .\n7CIFAR10, qualitative results. The samples are generated using 1,000 sampling steps.\n\n( a )Figure 9 .Figure 10 .Figure 11 .\na91011Samples generated by ADM trained on LSUN tower 64\u00d764 (FID 3.39) (b) Samples generated by ADM-IP trained on LSUN tower 64\u00d764 (FID 2.68) LSUN tower 64\u00d764, qualitative results. The samples are generated using 1,000 sampling steps. (a) Samples generated by ADM trained on CelebA 64\u00d764 (FID 1.60) (b) Samples generated by ADM-IP trained on CelebA 64\u00d764 (FID 1.31) CelebA 64\u00d764, qualitative results. The samples are generated using 1,000 sampling steps. (a) Samples generated by ADM trained on FFHQ 128\u00d7128 (FID 9.65) (b) Samples generated by ADM-IP trained on FFHQ 128\u00d7128 (FID 2.98) FFHQ 128\u00d7128, qualitative results. The samples are generated using 1,000 sampling steps.\n\nFigure 12 .Figure 13 .Figure 14 .Figure 15 .\n12131415Samples generated by ADM-IP trained on CIFAR10 (FID 2.67 , 300 sampling steps) Samples generated by ADM-IP trained on ImageNet 32\u00d732 (FID 2.66, 300 sampling steps) Samples generated by ADM-IP trained on LSUN tower 64\u00d764 (FID 2.60 , 300 sampling steps) Samples generated by ADM-IP trained on CelebA 64\u00d764 (FID 1.27, 900 sampling steps)\n\nTable 1 .\n1An empirical estimate of the exposure bias on ImageNet 32\u00d732.Model \nNumber of reverse diffusion steps \n\n100 \n300 \n500 \n700 \n1,000 \n\nADM \n0.983 1.808 2.587 3.105 3.544 \nADM-IP (ours) 0.972 1.594 2.198 2.539 2.742 \n\n\n\nTable 2 .\n2Comparison of different regularization methods. All the models are tested using T = 1, 000 sampling steps.Model \nCIFAR10 32\u00d732 \n\nFID sFID \n\nADM (baseline) 2.99 4.76 \nADM-GP \n2.80 4.41 \nADM-WD \n2.82 4.61 \nADM-IP \n2.76 4.05 \n\n\n\nTable 3 .\n3Comparison between ADM and ADM-IP using models trained with T = 1, 000 sampling steps and tested with T \u2264 T steps.Sampling steps \n(T ) \nModel \nCIFAR10 \nImageNet 32 LSUN tower 64 CelebA 64 \nFFHQ 128 \n\nFID sFID FID sFID FID sFID \nFID sFID FID \nsFID \n\n1,000 \nADM (baseline) 2.99 4.76 3.60 3.30 \n3.39 7.96 \n1.60 3.80 9.65 \n12.53 \nADM-IP (ours) \n2.76 4.05 2.87 2.39 \n2.68 6.04 \n1.31 3.38 2.98 \n5.59 \n\n300 \nADM \n2.95 4.95 3.58 3.48 \n3.31 8.39 \n1.82 4.25 9.55 \n12.60 \nADM-IP \n2.67 4.14 2.74 2.58 \n2.60 5.98 \n1.43 3.36 3.74 \n5.97 \n\n100 \nADM \n3.37 5.66 4.26 4.48 \n3.50 11.10 \n3.02 5.76 14.52 16.02 \nADM-IP \n2.70 4.51 3.24 3.13 \n2.79 6.56 \n2.21 4.33 5.94 \n7.90 \n\n80 \nADM \n3.63 5.97 4.61 4.76 \n4.17 12.60 \n3.75 6.80 17.00 18.02 \nADM-IP \n2.93 4.69 3.57 3.33 \n2.95 6.93 \n2.67 4.69 6.89 \n8.79 \n\nand the datasets. We refer to Appendix A.7 for the complete \nlist of hyperparameters (e.g. the learning rate, the batch size, \netc.) and network architecture settings, which are the same \nfor both ADM and ADM-IP. \n\n\n\nTable 4 .\n4ADM-IP training and testing acceleration. Note that, for a \nsingle training iteration, ADM and ADM-IP take exactly the same \namount of time, and the same is true for a single sampling step. \n\nDataset \nModel \nTraining \niterations \n\nSampling \nsteps \nFID \n\nCIFAR10 \n32\u00d732 \n\nADM \n500K \n1,000 \n2.99 \nADM-IP 460K \n80 \n2.93 \n\nImageNet \n32\u00d732 \n\nADM \n4500K \n1,000 \n3.53 \nADM-IP 4000K \n80 \n3.50 \n\nLSUN tower \n64\u00d764 \n\nADM \n300K \n1,000 \n3.39 \nADM-IP 220K \n60 \n3.31 \n\nCelabA \n64\u00d764 \n\nADM \n480K \n1,000 \n1.60 \nADM-IP 300K \n200 \n1.53 \n\nFFHQ \n128\u00d7128 \n\nADM \n420K \n1,000 \n9.65 \nADM-IP 180K \n60 \n8.72 \n\nTable 5. CIFAR10: Comparison between DDIM and DDIM-IP \nusing models trained with T = 1, 000 sampling steps and tested \nwith T \u2264 T steps. \n\n\u03b7 \nModel \nSampling steps (T ) \n\n10 \n20 \n50 \n100 1,000 \n\n0 \nDDIM \n14.21 7.50 5.17 4.66 4.29 \nDDIM-IP 10.54 5.70 4.66 4.52 4.27 \n\n0.5 \nDDIM \n17.24 8.87 5.59 4.88 4.45 \nDDIM-IP 10.06 5.53 3.95 3.66 3.56 \n\n\n\nTable 6 .\n6A deterministic estimate of the exposure bias (\u03b4t) with respect to different lengths of the reverse diffusion trajectory. The error is upper bounded by 2.Model \nNumber of reverse diffusion steps \n\n100 \n300 \n600 \n1,000 \n\nADM 0.0539 0.1074 0.1821 0.8165 \n\n\n\nTable 7 .\n7CIFAR10: comparing DDPM, DDPM-y and DDPM-IP using different numbers of revers diffusion steps.We compare Recall and Precision for ADM and ADM-IP using the improved metrics(Kynk\u00e4\u00e4nniemi et al., 2019) and the code ofDhariwal & Nichol (2021). For each dataset and model, we generate 50,000 samples with 1,000 sampling steps. The results in Tab. 8 indicate that the Recall and Precision values achieved by ADM and ADM-IP have no significant difference, while ADM-IP gets slightly better results on CIFAR10 32\u00d732 and FFHQ 128\u00d7128. Note that, due to the limited memory of our NVIDIA V100 16G GPU, we experienced an out-of-memory issue when computing Recall and Precision on the ImageNet 32\u00d732 dataset, thus this result is not reported in Tab. 8.Model \nInput Target \n80 steps \n100 steps \n300 steps \n1000 steps \n\nFID sFID FID sFID FID sFID FID sFID \n\nDDPM \nx x x t \n3.63 5.97 3.37 5.66 2.95 4.95 2.99 4.76 \nDDPM-y \ny y y t \n4.24 6.51 3.90 6.23 3.21 5.39 3.25 5.04 \nDDPM-IP y y y t \n2.93 4.69 2.70 4.51 2.67 4.14 2.76 4.05 \n\nA.4. Recall and Precision \n\n\n\nTable 9 .\n9ADM and ADM-IP hyperparameter valuesCIFAR10 \n32\u00d732 \n\nImageNet \n32\u00d732 \n\nLSUN tower \n64\u00d764 \n\nCelebA \n64\u00d764 \n\nFFHQ \n128\u00d7128 \n\nDiffusion steps \n1,000 \n1,000 \n1,000 \n1,000 \n1,000 \nNoise schedule \ncosine \ncosine \ncosine \ncosine \ncosine \nModel size \n57M \n57M \n295M \n295M \n543M \nChannels \n128 \n128 \n192 \n192 \n256 \nResidual blocks \n3 \n3 \n3 \n3 \n3 \nChannels multiple \n1, 2, 2, 2 \n1, 2, 2, 2 \n1, 2, 3, 4 \n1, 2, 3, 4 1, 1, 2, 3, 4 \nHeads channels \n32 \n32 \n64 \n64 \n64 \nAttention resolution \n16, 8 \n16, 8 \n32, 16, 8 \n32, 16, 8 32, 16, 8 \nBigGAN up/downsample True \nTrue \nTrue \nTrue \nTrue \nDropout \n0.3 \n0.3 \n0.1 \n0.1 \n0.1 \nBatch size \n128 \n512 \n256 \n256 \n128 \nTraining iterations \n540K \n5000K \n340K \n540K \n480K \nTraining images \n50K \n1281K \n708K \n203K \n70K \nLearning rate \n1e-4 \n1e-4 \n1e-4 \n1e-4 \n1e-4 \n\n\n\nTable 10 .\n10DDIM and DDIM-IP hyperparameter values on CIFAR10 dataset A.8. Qualitative Comparison between ADM and ADM-IPDiffusion Steps \n1000 \nVariance type \nfixed large \nNoise schedule \nlinear \nEma rate \n0.9999 \nChannels \n128 \nBatch size \n128 \nChannels multiple \n1,2,2,2 Iterations \n1600K \nResidual blocks \n2 \nTraining images 50K \nAttention resolution 16 \nOptimizer \nAdam \nDropout \n0.1 \nLearning rate \n2e-4 \n\n\nPublished as a Conference Paper at ICML 2023(a) e i t with t = 300, i = 1 (b) e i t with t = 300, i = 1025 (c) e i t with t = 300, i = 2049 (d) e i t with t = 600, i = 528 (e) e i t with t = 600, i = 1552 (f) e i t with t = 600, i = 2576(g) e i t with t = 900, i = 1024 (h) e i t with t = 900, i = 2048 (i) e i t with t = 900, i = 3072 Figure 6. The empirical distribution of e i t with different random values of t and i.Comparing Eq. 21 with Eq. 22, we can use W 2 as the Lipschitz constant K. We can use the Frobenius Norm W F to approximate the Spectral Norm W 2 because, using the Cauchy inequality, we have:\nAcknowledgmentsThis work has been supported by the European Union's Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No 955778. Moreover, we acknowledge the CINECA award under the ISCRA initiative, for the availability of high-performance computing resources and support.In this section, we use ImageNet 32\u00d732 to empirically show that e e e t \u223c N (0 0 0, \u03bd 2 t I I I) (Sec. 5.3), i.e., that the prediction error is nearly isotropic Gaussian distributed. To do so, we need to prove that, for each t and each input dimension (i.e., for each pixel and color channel) i \u2208 {1, ..., M }, the pixel-wise error (e i t ) follows e i t \u223c N (0, \u03bd 2 t ). To test this hypothesis, we uniformly select a subset of 100 t values in {1, ..., T } using a stride of 10. Then, for each t, we use 10K images and all the pixels to compute the pixel independent mean \u00b5 t and variance \u03bd 2 t of the error, which we use to standardize the error values for all the pixels e i t (i.e.,). Then, for each i, we use 50 randomly selected\u0113 i t values and the Shapiro-Wilk test(Shapiro & Wilk, 1965)to verify that they follow a standard normal distribution. The confidence level is set at 95% and we reject the null hypothesis if the p-value is less than 0.05. The null hypothesis was rejected only in a small minority of cases, confirming that the error e e e t is almost isotropic Gaussian distributed.Fig. 6shows a few histogram examples for e i t computed at different pixels.A.6. Relation between the Lipschitz Constant Minimization and the Weight Decay MinimizationBy definition, in Lipschitz continuos functions, the relation between the output difference f w (x 1 ) \u2212 f w (x 2 ) and the input difference x 1 \u2212 x 2 of two points is governed by a constant K as follows:Since a neural network is usually a stack of layers, without loss of generality we consider a single layer neural network, f (x) = ReLU (W x + b), thus we have:Using the first order term of Tylor Series to approximate the left side of the above equation, we get:where the details of Tylor Series approximation are:\u2022 Let y = W x + b, we approximate f (y) at the point y = 0.\u2022 Substitute y with y 1 and y 2 , where y 1 = W x 1 + b, y 2 = W x 2 + b.\u2022 Thus,Since \u2202f \u2202y is bounded by 1 when f = ReLU , we can ignore it, and we have:We now introduce the Spectral Norm W 2 . According to the definition W 2 = maxx =0W x x , we have:\nThe effects of regularization and data augmentation are class dependent. R Balestriero, L Bottou, Y Lecun, arXiv:2204.03632Balestriero, R., Bottou, L., and LeCun, Y. The effects of regularization and data augmentation are class dependent. arXiv:2204.03632, 2022.\n\nScheduled sampling for sequence prediction with recurrent neural networks. S Bengio, O Vinyals, N Jaitly, N Shazeer, NeurIPS. Bengio, S., Vinyals, O., Jaitly, N., and Shazeer, N. Sched- uled sampling for sequence prediction with recurrent neu- ral networks. In NeurIPS, 2015.\n\nLarge scale gan training for high fidelity natural image synthesis. A Brock, J Donahue, K Simonyan, arXiv:1809.11096arXiv preprintBrock, A., Donahue, J., and Simonyan, K. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\nVicinal risk minimization. O Chapelle, J Weston, L Bottou, V Vapnik, NIPS. Chapelle, O., Weston, J., Bottou, L., and Vapnik, V. Vicinal risk minimization. In NIPS, 2000.\n\nEstimating gradients for waveform generation. N Chen, Y Zhang, H Zen, R J Weiss, M Norouzi, Chan , W Wavegrad, ICLR. 2021Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. Wavegrad: Estimating gradients for waveform generation. In ICLR, 2021.\n\nA downsampled variant of imagenet as an alternative to the cifar datasets. P Chrabaszcz, I Loshchilov, F Hutter, arXiv:1707.08819Chrabaszcz, P., Loshchilov, I., and Hutter, F. A downsam- pled variant of imagenet as an alternative to the cifar datasets. arXiv:1707.08819, 2017.\n\nDiffusion models in vision: A survey. F.-A Croitoru, V Hondru, R T Ionescu, M Shah, arXiv:2209.04747arXiv preprintCroitoru, F.-A., Hondru, V., Ionescu, R. T., and Shah, M. Diffusion models in vision: A survey. arXiv preprint arXiv:2209.04747, 2022.\n\nDiffusion models beat GANs on image synthesis. P Dhariwal, A Q Nichol, NeurIPS. 2021Dhariwal, P. and Nichol, A. Q. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NeurIPS. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In NeurIPS, 2014.\n\n. I Goodfellow, Y Bengio, A Courville, Learning, MIT PressGoodfellow, I., Bengio, Y., and Courville, A. Deep Learning. MIT Press, 2016. http://www. deeplearningbook.org.\n\nVector quantized diffusion model for text-to-image synthesis. S Gu, D Chen, J Bao, F Wen, B Zhang, D Chen, L Yuan, B Guo, arXiv:2111.14822Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., and Guo, B. Vector quantized diffusion model for text-to-image synthesis. arXiv:2111.14822, 2021.\n\nImproved training of wasserstein gans. I Gulrajani, F Ahmed, M Arjovsky, V Dumoulin, A C Courville, NeurIPS. 30Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. Improved training of wasserstein gans. NeurIPS, 30, 2017.\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30. M Heusel, H Ramsauer, T Unterthiner, B Nessler, S Hochreiter, Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30, 2017.\n\nDenoising diffusion probabilistic models. J Ho, A Jain, Abbeel , P , NeurIPS. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. In NeurIPS, 2020.\n\nArgmax flows and multinomial diffusion: Learning categorical distributions. E Hoogeboom, D Nielsen, P Jaini, P Forr\u00e9, M Welling, NeurIPS. 2021Hoogeboom, E., Nielsen, D., Jaini, P., Forr\u00e9, P., and Welling, M. Argmax flows and multinomial diffusion: Learning categorical distributions. In NeurIPS, 2021.\n\nAutoregressive diffusion models. E Hoogeboom, A A Gritsenko, J Bastings, B Poole, Van Den, R Berg, T Salimans, ICLR. 2022Hoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B., van den Berg, R., and Salimans, T. Autoregressive diffu- sion models. In ICLR, 2022.\n\nA style-based generator architecture for generative adversarial networks. T Karras, S Laine, Aila , T , Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionKarras, T., Laine, S., and Aila, T. A style-based generator architecture for generative adversarial networks. In Pro- ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401-4410, 2019.\n\nElucidating the design space of diffusion-based generative models. T Karras, M Aittala, T Aila, Laine , S , NeurIPS. 2022Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022.\n\nOn fast sampling of diffusion probabilistic models. Z Kong, W Ping, arXiv:2106.00132arXiv preprintKong, Z. and Ping, W. On fast sampling of diffusion proba- bilistic models. arXiv preprint arXiv:2106.00132, 2021.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\n\nA simple weight decay can improve generalization. NeurIPS, 4. A Krogh, J Hertz, Krogh, A. and Hertz, J. A simple weight decay can improve generalization. NeurIPS, 4, 1991.\n\nImproved precision and recall metric for assessing generative models. T Kynk\u00e4\u00e4nniemi, T Karras, S Laine, J Lehtinen, Aila , T , Advances in Neural Information Processing Systems. 32Kynk\u00e4\u00e4nniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. Improved precision and recall metric for assess- ing generative models. Advances in Neural Information Processing Systems, 32, 2019.\n\nH.-T D Liu, F Williams, A Jacobson, S Fidler, O Litany, arXiv:2202.08345Learning smooth neural functions via lipschitz regularization. arXiv preprintLiu, H.-T. D., Williams, F., Jacobson, A., Fidler, S., and Litany, O. Learning smooth neural functions via lipschitz regularization. arXiv preprint arXiv:2202.08345, 2022.\n\nEfficient training of visual transformers with small datasets. Y Liu, E Sangineto, W Bi, N Sebe, B Lepri, M D Nadai, NeurIPSLiu, Y., Sangineto, E., Bi, W., Sebe, N., Lepri, B., and Nadai, M. D. Efficient training of visual transformers with small datasets. NeurIPS, 2021.\n\nDeep learning face attributes in the wild. Z Liu, P Luo, X Wang, X Tang, Proceedings of International Conference on Computer Vision (ICCV). International Conference on Computer Vision (ICCV)Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.\n\nDecoupled weight decay regularization. I Loshchilov, F Hutter, ICLRLoshchilov, I. and Hutter, F. Decoupled weight decay regu- larization. ICLR, 2019.\n\nUnderstanding diffusion models: A unified perspective. C Luo, arXiv:2208.11970arXiv preprintLuo, C. Understanding diffusion models: A unified perspec- tive. arXiv preprint arXiv:2208.11970, 2022.\n\n. P Micikevicius, S Narang, J Alben, G Diamos, E Elsen, D Garcia, B Ginsburg, M Houston, O Kuchaiev, G Venkatesh, arXiv:1710.03740Mixed precision trainingMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv:1710.03740, 2017.\n\nSymbolic music generation with diffusion models. G Mittal, J H Engel, C Hawthorne, I Simon, Proceedings of the 22nd International Society for Music Information Retrieval Conference. the 22nd International Society for Music Information Retrieval Conference2021Mittal, G., Engel, J. H., Hawthorne, C., and Simon, I. Sym- bolic music generation with diffusion models. In Pro- ceedings of the 22nd International Society for Music In- formation Retrieval Conference, ISMIR, 2021.\n\nSpectral normalization for generative adversarial networks. T Miyato, T Kataoka, M Koyama, Yoshida , Y , arXiv:1802.05957arXiv preprintMiyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spec- tral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.\n\nC Nash, J Menick, S Dieleman, P W Battaglia, arXiv:2103.03841Generating images with sparse representations. Nash, C., Menick, J., Dieleman, S., and Battaglia, P. W. Generating images with sparse representations. arXiv:2103.03841, 2021.\n\nImproved denoising diffusion probabilistic models. A Q Nichol, P Dhariwal, ICML. 2021Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models. In ICML, 2021.\n\nGLIDE: towards photorealistic image generation and editing with text-guided diffusion models. A Q Nichol, P Dhariwal, A Ramesh, P Shyam, P Mishkin, B Mcgrew, I Sutskever, Chen , M , ICML. 2022Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. GLIDE: towards photorealistic image generation and edit- ing with text-guided diffusion models. In ICML, 2022.\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, NeurIPS. 32Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 32, 2019.\n\nLearning Transferable Visual Models From Natural Language Supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, ICML. 2021Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning Transferable Visual Models From Natural Language Supervision. In ICML, 2021.\n\nHierarchical text-conditional image generation with CLIP latents. A Ramesh, P Dhariwal, A Nichol, C Chu, Chen , M , arXiv:2204.06125Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with CLIP latents. arXiv:2204.06125, 2022.\n\nSequence level training with recurrent neural networks. M Ranzato, S Chopra, M Auli, W Zaremba, In ICLR. Ranzato, M., Chopra, S., Auli, M., and Zaremba, W. Se- quence level training with recurrent neural networks. In ICLR, 2016.\n\nAutoregressive denoising diffusion models for multivariate probabilistic time series forecasting. K Rasul, C Seward, I Schuster, R Vollgraf, ICML. 2021Rasul, K., Seward, C., Schuster, I., and Vollgraf, R. Au- toregressive denoising diffusion models for multivariate probabilistic time series forecasting. In ICML, 2021.\n\nSelf-critical sequence training for image captioning. S J Rennie, E Marcheret, Y Mroueh, J Ross, V Goel, CVPR. Rennie, S. J., Marcheret, E., Mroueh, Y., Ross, J., and Goel, V. Self-critical sequence training for image captioning. In CVPR, 2017.\n\nContractive auto-encoders: Explicit invariance during feature extraction. S Rifai, X M Vincent, X Glorot, Y Bengio, ICML. Rifai, S., Pascal Vincent, X. M., Glorot, X., and Bengio, Y. Contractive auto-encoders: Explicit invariance during feature extraction. In ICML, 2011.\n\nHigh-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models, 2021.\n\nPhotorealistic text-to-image diffusion models with deep language understanding. C Saharia, W Chan, S Saxena, L Li, J Whang, E Denton, S K S Ghasemipour, B K Ayan, S S Mahdavi, R G Lopes, arXiv:2205.11487arXiv preprintSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\n\nProgressive distillation for fast sampling of diffusion models. T Salimans, J Ho, ICLR. 2022Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022.\n\nGeneralization in generation: A closer look at exposure bias. F Schmidt, Proceedings of the 3rd Workshop on Neural Generation and Translation@EMNLP-IJCNLP. the 3rd Workshop on Neural Generation and Translation@EMNLP-IJCNLPSchmidt, F. Generalization in generation: A closer look at exposure bias. In Proceedings of the 3rd Workshop on Neural Generation and Translation@EMNLP-IJCNLP, 2019.\n\nAn analysis of variance test for normality (complete samples). S S Shapiro, M Wilk, Biometrika. 523/4Shapiro, S. S. and Wilk, M. B. An analysis of variance test for normality (complete samples). Biometrika, 52(3/4): 591-611, 1965.\n\nDeep unsupervised learning using nonequilibrium thermodynamics. J Sohl-Dickstein, E Weiss, N Maheswaranathan, S Ganguli, ICML. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequi- librium thermodynamics. In ICML, 2015.\n\nDenoising diffusion implicit models. J Song, C Meng, S Ermon, ICLR. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In ICLR, 2021a.\n\nGenerative modeling by estimating gradients of the data distribution. Y Song, S Ermon, NeurIPS. 32Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. NeurIPS, 32, 2019.\n\nScore-based generative modeling through stochastic differential equations. Y Song, J Sohl-Dickstein, D P Kingma, A Kumar, S Ermon, B Poole, ICLR. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er- mon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In ICLR, 2021b.\n\nBayesian learning via stochastic gradient langevin dynamics. M Welling, Y W Teh, ICML. Welling, M. and Teh, Y. W. Bayesian learning via stochastic gradient langevin dynamics. In ICML, 2011.\n\nTackling the generative learning trilemma with denoising diffusion GANs. Z Xiao, K Kreis, A Vahdat, International Conference on Learning Representations (ICLR). 2022Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative learning trilemma with denoising diffusion GANs. In International Conference on Learning Representations (ICLR), 2022.\n\nY Xu, Z Liu, M Tegmark, T Jaakkola, arXiv:2209.11178Poisson flow generative models. arXiv preprintXu, Y., Liu, Z., Tegmark, M., and Jaakkola, T. Poisson flow generative models. arXiv preprint arXiv:2209.11178, 2022.\n\nDiffusion models: A comprehensive survey of methods and applications. L Yang, Z Zhang, S Hong, arXiv:2209.00796Yang, L., Zhang, Z., and Hong, S. Diffusion models: A comprehensive survey of methods and applications. arXiv:2209.00796, 2022.\n\nGeneralized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J G Carbonell, R Salakhutdinov, Q V Le, Xlnet, NeurIPS. Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., and Le, Q. V. Xlnet: Generalized autoregressive pre- training for language understanding. In NeurIPS, 2019.\n\nY Yoshida, T Miyato, arXiv:1705.10941Spectral norm regularization for improving the generalizability of deep learning. arXiv preprintYoshida, Y. and Miyato, T. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.\n\nF Yu, A Seff, Y Zhang, S Song, T Funkhouser, Xiao , J Lsun, arXiv:1506.03365Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprintYu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and Xiao, J. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n\nmixup: Beyond empirical risk minimization. H Zhang, M Ciss\u00e9, Y N Dauphin, D Lopez-Paz, In ICLR. Zhang, H., Ciss\u00e9, M., Dauphin, Y. N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. In ICLR, 2018.\n", "annotations": {"author": "[{\"end\":74,\"start\":64},{\"end\":91,\"start\":75},{\"end\":108,\"start\":92},{\"end\":126,\"start\":109},{\"end\":142,\"start\":127}]", "publisher": null, "author_last_name": "[{\"end\":73,\"start\":69},{\"end\":90,\"start\":81},{\"end\":107,\"start\":99},{\"end\":125,\"start\":116},{\"end\":141,\"start\":132}]", "author_first_name": "[{\"end\":68,\"start\":64},{\"end\":80,\"start\":75},{\"end\":98,\"start\":92},{\"end\":115,\"start\":109},{\"end\":131,\"start\":127}]", "author_affiliation": null, "title": "[{\"end\":61,\"start\":1},{\"end\":203,\"start\":143}]", "venue": null, "abstract": "[{\"end\":1348,\"start\":205}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b45\"},\"end\":1442,\"start\":1413},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1458,\"start\":1442},{\"end\":1613,\"start\":1588},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1633,\"start\":1613},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1653,\"start\":1633},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":1811,\"start\":1790},{\"end\":2762,\"start\":2743},{\"end\":3079,\"start\":3060},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4007,\"start\":3985},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4021,\"start\":4007},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4106,\"start\":4086},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4278,\"start\":4257},{\"end\":4541,\"start\":4516},{\"end\":4711,\"start\":4687},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":5964,\"start\":5944},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6038,\"start\":6015},{\"end\":7475,\"start\":7450},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7494,\"start\":7475},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7591,\"start\":7566},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7637,\"start\":7612},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7671,\"start\":7654},{\"end\":7824,\"start\":7799},{\"end\":8503,\"start\":8497},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8546,\"start\":8526},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8562,\"start\":8546},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8581,\"start\":8562},{\"end\":8605,\"start\":8581},{\"end\":8646,\"start\":8622},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8770,\"start\":8745},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8789,\"start\":8770},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8926,\"start\":8905},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9104,\"start\":9083},{\"end\":9182,\"start\":9160},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9274,\"start\":9252},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9308,\"start\":9286},{\"end\":9407,\"start\":9383},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9423,\"start\":9407},{\"end\":9465,\"start\":9444},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9483,\"start\":9465},{\"end\":9532,\"start\":9512},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9582,\"start\":9563},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9604,\"start\":9582},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10094,\"start\":10075},{\"end\":10337,\"start\":10317},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10544,\"start\":10523},{\"end\":10716,\"start\":10693},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10786,\"start\":10767},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10944,\"start\":10926},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11136,\"start\":11118},{\"end\":11332,\"start\":11314},{\"end\":11428,\"start\":11408},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11561,\"start\":11545},{\"end\":11954,\"start\":11929},{\"end\":11978,\"start\":11954},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12831,\"start\":12814},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13045,\"start\":13029},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14667,\"start\":14645},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14681,\"start\":14667},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14701,\"start\":14681},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14721,\"start\":14701},{\"end\":15451,\"start\":15426},{\"end\":16213,\"start\":16188},{\"end\":17169,\"start\":17161},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17623,\"start\":17606},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17641,\"start\":17623},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17726,\"start\":17707},{\"end\":22673,\"start\":22653},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23900,\"start\":23880},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23923,\"start\":23900},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23962,\"start\":23941},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23982,\"start\":23962},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24363,\"start\":24343},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24387,\"start\":24363},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":24637,\"start\":24613},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25306,\"start\":25285},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25376,\"start\":25357},{\"end\":26011,\"start\":25986},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27360,\"start\":27342},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27417,\"start\":27400},{\"end\":27441,\"start\":27417},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27635,\"start\":27618},{\"end\":27659,\"start\":27635},{\"end\":27918,\"start\":27893},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30234,\"start\":30214},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30251,\"start\":30234},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30276,\"start\":30251},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31206,\"start\":31180},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31586,\"start\":31567},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31720,\"start\":31701},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":32846,\"start\":32826},{\"end\":34673,\"start\":34648},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":35382,\"start\":35371},{\"end\":38604,\"start\":38579},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38797,\"start\":38770},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38880,\"start\":38853},{\"end\":39082,\"start\":39057},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":39125,\"start\":39104},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":39598,\"start\":39579},{\"end\":40016,\"start\":40002},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":46160,\"start\":46133}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41380,\"start\":41296},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41482,\"start\":41381},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41696,\"start\":41483},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41897,\"start\":41697},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42072,\"start\":41898},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42169,\"start\":42073},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42882,\"start\":42170},{\"attributes\":{\"id\":\"fig_8\"},\"end\":43271,\"start\":42883},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43498,\"start\":43272},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43735,\"start\":43499},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44744,\"start\":43736},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45682,\"start\":44745},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":45949,\"start\":45683},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47006,\"start\":45950},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":47808,\"start\":47007},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":48221,\"start\":47809}]", "paragraph": "[{\"end\":2130,\"start\":1364},{\"end\":2458,\"start\":2132},{\"end\":4391,\"start\":2460},{\"end\":5023,\"start\":4393},{\"end\":6186,\"start\":5025},{\"end\":6664,\"start\":6188},{\"end\":7900,\"start\":6666},{\"end\":7936,\"start\":7902},{\"end\":8035,\"start\":7938},{\"end\":8270,\"start\":8037},{\"end\":8422,\"start\":8272},{\"end\":9905,\"start\":8439},{\"end\":11775,\"start\":9907},{\"end\":12320,\"start\":11790},{\"end\":12621,\"start\":12451},{\"end\":12835,\"start\":12774},{\"end\":13197,\"start\":12875},{\"end\":13360,\"start\":13298},{\"end\":13502,\"start\":13381},{\"end\":13552,\"start\":13504},{\"end\":16752,\"start\":13886},{\"end\":17329,\"start\":16804},{\"end\":18119,\"start\":17383},{\"end\":18187,\"start\":18121},{\"end\":18190,\"start\":18189},{\"end\":18313,\"start\":18283},{\"end\":18402,\"start\":18315},{\"end\":19002,\"start\":18417},{\"end\":19606,\"start\":19004},{\"end\":19792,\"start\":19688},{\"end\":20278,\"start\":19840},{\"end\":20708,\"start\":20280},{\"end\":23127,\"start\":20744},{\"end\":23674,\"start\":23186},{\"end\":23754,\"start\":23676},{\"end\":24236,\"start\":23756},{\"end\":24411,\"start\":24238},{\"end\":24717,\"start\":24502},{\"end\":24896,\"start\":24719},{\"end\":25021,\"start\":24968},{\"end\":25715,\"start\":25033},{\"end\":26336,\"start\":25766},{\"end\":27213,\"start\":26338},{\"end\":28679,\"start\":27230},{\"end\":28975,\"start\":28681},{\"end\":29952,\"start\":28977},{\"end\":30530,\"start\":29954},{\"end\":31087,\"start\":30532},{\"end\":31414,\"start\":31089},{\"end\":32193,\"start\":31416},{\"end\":33073,\"start\":32195},{\"end\":33826,\"start\":33089},{\"end\":34360,\"start\":33828},{\"end\":35645,\"start\":34406},{\"end\":35787,\"start\":35647},{\"end\":35900,\"start\":35890},{\"end\":36076,\"start\":35988},{\"end\":36519,\"start\":36078},{\"end\":36871,\"start\":36564},{\"end\":37183,\"start\":36922},{\"end\":37450,\"start\":37327},{\"end\":37700,\"start\":37607},{\"end\":38123,\"start\":37702},{\"end\":38190,\"start\":38144},{\"end\":38489,\"start\":38211},{\"end\":39083,\"start\":38514},{\"end\":40639,\"start\":39085},{\"end\":40921,\"start\":40641},{\"end\":41247,\"start\":40972},{\"end\":41295,\"start\":41249}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12393,\"start\":12321},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12450,\"start\":12393},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12706,\"start\":12622},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12773,\"start\":12706},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12874,\"start\":12836},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13297,\"start\":13198},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13380,\"start\":13361},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13737,\"start\":13553},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13841,\"start\":13737},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17382,\"start\":17330},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18282,\"start\":18191},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19687,\"start\":19607},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19839,\"start\":19793},{\"attributes\":{\"id\":\"formula_14\"},\"end\":24501,\"start\":24412},{\"attributes\":{\"id\":\"formula_15\"},\"end\":24967,\"start\":24897},{\"attributes\":{\"id\":\"formula_16\"},\"end\":35820,\"start\":35788},{\"attributes\":{\"id\":\"formula_17\"},\"end\":35889,\"start\":35820},{\"attributes\":{\"id\":\"formula_18\"},\"end\":35987,\"start\":35901},{\"attributes\":{\"id\":\"formula_19\"},\"end\":36921,\"start\":36872},{\"attributes\":{\"id\":\"formula_20\"},\"end\":37255,\"start\":37184},{\"attributes\":{\"id\":\"formula_21\"},\"end\":37326,\"start\":37255},{\"attributes\":{\"id\":\"formula_22\"},\"end\":37606,\"start\":37451},{\"attributes\":{\"id\":\"formula_23\"},\"end\":38143,\"start\":38124},{\"attributes\":{\"id\":\"formula_24\"},\"end\":38210,\"start\":38191}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1362,\"start\":1350},{\"attributes\":{\"n\":\"2.\"},\"end\":8437,\"start\":8425},{\"attributes\":{\"n\":\"3.\"},\"end\":11788,\"start\":11778},{\"attributes\":{\"n\":\"4.\"},\"end\":13884,\"start\":13843},{\"attributes\":{\"n\":\"5.\"},\"end\":16761,\"start\":16755},{\"attributes\":{\"n\":\"5.1.\"},\"end\":16802,\"start\":16764},{\"attributes\":{\"n\":\"5.2.\"},\"end\":18415,\"start\":18405},{\"attributes\":{\"n\":\"5.3.\"},\"end\":20742,\"start\":20711},{\"attributes\":{\"n\":\"5.4.\"},\"end\":23184,\"start\":23130},{\"attributes\":{\"n\":\"6.\"},\"end\":25031,\"start\":25024},{\"attributes\":{\"n\":\"6.1.\"},\"end\":25764,\"start\":25718},{\"attributes\":{\"n\":\"6.2.\"},\"end\":27228,\"start\":27216},{\"attributes\":{\"n\":\"7.\"},\"end\":33087,\"start\":33076},{\"end\":34374,\"start\":34363},{\"end\":34404,\"start\":34377},{\"end\":36562,\"start\":36522},{\"end\":38512,\"start\":38492},{\"end\":40970,\"start\":40924},{\"end\":41392,\"start\":41382},{\"end\":41494,\"start\":41484},{\"end\":41708,\"start\":41698},{\"end\":41909,\"start\":41899},{\"end\":42084,\"start\":42074},{\"end\":42208,\"start\":42171},{\"end\":42928,\"start\":42884},{\"end\":43282,\"start\":43273},{\"end\":43509,\"start\":43500},{\"end\":43746,\"start\":43737},{\"end\":44755,\"start\":44746},{\"end\":45693,\"start\":45684},{\"end\":45960,\"start\":45951},{\"end\":47017,\"start\":47008},{\"end\":47820,\"start\":47810}]", "table": "[{\"end\":43498,\"start\":43345},{\"end\":43735,\"start\":43617},{\"end\":44744,\"start\":43862},{\"end\":45682,\"start\":44760},{\"end\":45949,\"start\":45849},{\"end\":47006,\"start\":46701},{\"end\":47808,\"start\":47055},{\"end\":48221,\"start\":47931}]", "figure_caption": "[{\"end\":41380,\"start\":41298},{\"end\":41482,\"start\":41394},{\"end\":41696,\"start\":41496},{\"end\":41897,\"start\":41710},{\"end\":42072,\"start\":41911},{\"end\":42169,\"start\":42086},{\"end\":42882,\"start\":42215},{\"end\":43271,\"start\":42937},{\"end\":43345,\"start\":43284},{\"end\":43617,\"start\":43511},{\"end\":43862,\"start\":43748},{\"end\":44760,\"start\":44757},{\"end\":45849,\"start\":45695},{\"end\":46701,\"start\":45962},{\"end\":47055,\"start\":47019},{\"end\":47931,\"start\":47823}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19058,\"start\":19052},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20097,\"start\":20091},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21659,\"start\":21653},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22623,\"start\":22617},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29131,\"start\":29118},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29300,\"start\":29294},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29818,\"start\":29812},{\"end\":36298,\"start\":36292},{\"end\":37099,\"start\":37091},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40001,\"start\":39995},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40141,\"start\":40135},{\"end\":40285,\"start\":40279},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":40637,\"start\":40630},{\"end\":40829,\"start\":40821},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41057,\"start\":41048},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41082,\"start\":41074},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41111,\"start\":41102},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41134,\"start\":41126},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41162,\"start\":41153}]", "bib_author_first_name": "[{\"end\":51324,\"start\":51323},{\"end\":51339,\"start\":51338},{\"end\":51349,\"start\":51348},{\"end\":51590,\"start\":51589},{\"end\":51600,\"start\":51599},{\"end\":51611,\"start\":51610},{\"end\":51621,\"start\":51620},{\"end\":51860,\"start\":51859},{\"end\":51869,\"start\":51868},{\"end\":51880,\"start\":51879},{\"end\":52098,\"start\":52097},{\"end\":52110,\"start\":52109},{\"end\":52120,\"start\":52119},{\"end\":52130,\"start\":52129},{\"end\":52288,\"start\":52287},{\"end\":52296,\"start\":52295},{\"end\":52305,\"start\":52304},{\"end\":52312,\"start\":52311},{\"end\":52314,\"start\":52313},{\"end\":52323,\"start\":52322},{\"end\":52337,\"start\":52333},{\"end\":52341,\"start\":52340},{\"end\":52580,\"start\":52579},{\"end\":52594,\"start\":52593},{\"end\":52608,\"start\":52607},{\"end\":52824,\"start\":52820},{\"end\":52836,\"start\":52835},{\"end\":52846,\"start\":52845},{\"end\":52848,\"start\":52847},{\"end\":52859,\"start\":52858},{\"end\":53080,\"start\":53079},{\"end\":53092,\"start\":53091},{\"end\":53094,\"start\":53093},{\"end\":53243,\"start\":53242},{\"end\":53257,\"start\":53256},{\"end\":53274,\"start\":53273},{\"end\":53283,\"start\":53282},{\"end\":53289,\"start\":53288},{\"end\":53305,\"start\":53304},{\"end\":53314,\"start\":53313},{\"end\":53327,\"start\":53326},{\"end\":53509,\"start\":53508},{\"end\":53523,\"start\":53522},{\"end\":53533,\"start\":53532},{\"end\":53740,\"start\":53739},{\"end\":53746,\"start\":53745},{\"end\":53754,\"start\":53753},{\"end\":53761,\"start\":53760},{\"end\":53768,\"start\":53767},{\"end\":53777,\"start\":53776},{\"end\":53785,\"start\":53784},{\"end\":53793,\"start\":53792},{\"end\":54021,\"start\":54020},{\"end\":54034,\"start\":54033},{\"end\":54043,\"start\":54042},{\"end\":54055,\"start\":54054},{\"end\":54067,\"start\":54066},{\"end\":54069,\"start\":54068},{\"end\":54323,\"start\":54322},{\"end\":54333,\"start\":54332},{\"end\":54345,\"start\":54344},{\"end\":54360,\"start\":54359},{\"end\":54371,\"start\":54370},{\"end\":54605,\"start\":54604},{\"end\":54611,\"start\":54610},{\"end\":54624,\"start\":54618},{\"end\":54628,\"start\":54627},{\"end\":54813,\"start\":54812},{\"end\":54826,\"start\":54825},{\"end\":54837,\"start\":54836},{\"end\":54846,\"start\":54845},{\"end\":54855,\"start\":54854},{\"end\":55073,\"start\":55072},{\"end\":55086,\"start\":55085},{\"end\":55088,\"start\":55087},{\"end\":55101,\"start\":55100},{\"end\":55113,\"start\":55112},{\"end\":55131,\"start\":55130},{\"end\":55139,\"start\":55138},{\"end\":55379,\"start\":55378},{\"end\":55389,\"start\":55388},{\"end\":55401,\"start\":55397},{\"end\":55405,\"start\":55404},{\"end\":55845,\"start\":55844},{\"end\":55855,\"start\":55854},{\"end\":55866,\"start\":55865},{\"end\":55878,\"start\":55873},{\"end\":55882,\"start\":55881},{\"end\":56086,\"start\":56085},{\"end\":56094,\"start\":56093},{\"end\":56303,\"start\":56302},{\"end\":56317,\"start\":56316},{\"end\":56486,\"start\":56485},{\"end\":56495,\"start\":56494},{\"end\":56667,\"start\":56666},{\"end\":56683,\"start\":56682},{\"end\":56693,\"start\":56692},{\"end\":56702,\"start\":56701},{\"end\":56717,\"start\":56713},{\"end\":56721,\"start\":56720},{\"end\":56983,\"start\":56979},{\"end\":56985,\"start\":56984},{\"end\":56992,\"start\":56991},{\"end\":57004,\"start\":57003},{\"end\":57016,\"start\":57015},{\"end\":57026,\"start\":57025},{\"end\":57365,\"start\":57364},{\"end\":57372,\"start\":57371},{\"end\":57385,\"start\":57384},{\"end\":57391,\"start\":57390},{\"end\":57399,\"start\":57398},{\"end\":57408,\"start\":57407},{\"end\":57410,\"start\":57409},{\"end\":57618,\"start\":57617},{\"end\":57625,\"start\":57624},{\"end\":57632,\"start\":57631},{\"end\":57640,\"start\":57639},{\"end\":57974,\"start\":57973},{\"end\":57988,\"start\":57987},{\"end\":58141,\"start\":58140},{\"end\":58285,\"start\":58284},{\"end\":58301,\"start\":58300},{\"end\":58311,\"start\":58310},{\"end\":58320,\"start\":58319},{\"end\":58330,\"start\":58329},{\"end\":58339,\"start\":58338},{\"end\":58349,\"start\":58348},{\"end\":58361,\"start\":58360},{\"end\":58372,\"start\":58371},{\"end\":58384,\"start\":58383},{\"end\":58676,\"start\":58675},{\"end\":58686,\"start\":58685},{\"end\":58688,\"start\":58687},{\"end\":58697,\"start\":58696},{\"end\":58710,\"start\":58709},{\"end\":59163,\"start\":59162},{\"end\":59173,\"start\":59172},{\"end\":59184,\"start\":59183},{\"end\":59200,\"start\":59193},{\"end\":59204,\"start\":59203},{\"end\":59393,\"start\":59392},{\"end\":59401,\"start\":59400},{\"end\":59411,\"start\":59410},{\"end\":59423,\"start\":59422},{\"end\":59425,\"start\":59424},{\"end\":59681,\"start\":59680},{\"end\":59683,\"start\":59682},{\"end\":59693,\"start\":59692},{\"end\":59907,\"start\":59906},{\"end\":59909,\"start\":59908},{\"end\":59919,\"start\":59918},{\"end\":59931,\"start\":59930},{\"end\":59941,\"start\":59940},{\"end\":59950,\"start\":59949},{\"end\":59961,\"start\":59960},{\"end\":59971,\"start\":59970},{\"end\":59987,\"start\":59983},{\"end\":59991,\"start\":59990},{\"end\":60292,\"start\":60291},{\"end\":60302,\"start\":60301},{\"end\":60311,\"start\":60310},{\"end\":60320,\"start\":60319},{\"end\":60329,\"start\":60328},{\"end\":60341,\"start\":60340},{\"end\":60351,\"start\":60350},{\"end\":60362,\"start\":60361},{\"end\":60369,\"start\":60368},{\"end\":60383,\"start\":60382},{\"end\":60693,\"start\":60692},{\"end\":60704,\"start\":60703},{\"end\":60706,\"start\":60705},{\"end\":60713,\"start\":60712},{\"end\":60724,\"start\":60723},{\"end\":60734,\"start\":60733},{\"end\":60741,\"start\":60740},{\"end\":60752,\"start\":60751},{\"end\":60762,\"start\":60761},{\"end\":60772,\"start\":60771},{\"end\":60783,\"start\":60782},{\"end\":60792,\"start\":60791},{\"end\":60803,\"start\":60802},{\"end\":61130,\"start\":61129},{\"end\":61140,\"start\":61139},{\"end\":61152,\"start\":61151},{\"end\":61162,\"start\":61161},{\"end\":61172,\"start\":61168},{\"end\":61176,\"start\":61175},{\"end\":61403,\"start\":61402},{\"end\":61414,\"start\":61413},{\"end\":61424,\"start\":61423},{\"end\":61432,\"start\":61431},{\"end\":61675,\"start\":61674},{\"end\":61684,\"start\":61683},{\"end\":61694,\"start\":61693},{\"end\":61706,\"start\":61705},{\"end\":61952,\"start\":61951},{\"end\":61954,\"start\":61953},{\"end\":61964,\"start\":61963},{\"end\":61977,\"start\":61976},{\"end\":61987,\"start\":61986},{\"end\":61995,\"start\":61994},{\"end\":62218,\"start\":62217},{\"end\":62227,\"start\":62226},{\"end\":62229,\"start\":62228},{\"end\":62240,\"start\":62239},{\"end\":62250,\"start\":62249},{\"end\":62479,\"start\":62478},{\"end\":62490,\"start\":62489},{\"end\":62503,\"start\":62502},{\"end\":62513,\"start\":62512},{\"end\":62522,\"start\":62521},{\"end\":62745,\"start\":62744},{\"end\":62756,\"start\":62755},{\"end\":62764,\"start\":62763},{\"end\":62774,\"start\":62773},{\"end\":62780,\"start\":62779},{\"end\":62789,\"start\":62788},{\"end\":62799,\"start\":62798},{\"end\":62803,\"start\":62800},{\"end\":62818,\"start\":62817},{\"end\":62820,\"start\":62819},{\"end\":62828,\"start\":62827},{\"end\":62830,\"start\":62829},{\"end\":62841,\"start\":62840},{\"end\":62843,\"start\":62842},{\"end\":63205,\"start\":63204},{\"end\":63217,\"start\":63216},{\"end\":63399,\"start\":63398},{\"end\":63789,\"start\":63788},{\"end\":63791,\"start\":63790},{\"end\":63802,\"start\":63801},{\"end\":64022,\"start\":64021},{\"end\":64040,\"start\":64039},{\"end\":64049,\"start\":64048},{\"end\":64068,\"start\":64067},{\"end\":64272,\"start\":64271},{\"end\":64280,\"start\":64279},{\"end\":64288,\"start\":64287},{\"end\":64461,\"start\":64460},{\"end\":64469,\"start\":64468},{\"end\":64677,\"start\":64676},{\"end\":64685,\"start\":64684},{\"end\":64703,\"start\":64702},{\"end\":64705,\"start\":64704},{\"end\":64715,\"start\":64714},{\"end\":64724,\"start\":64723},{\"end\":64733,\"start\":64732},{\"end\":64984,\"start\":64983},{\"end\":64995,\"start\":64994},{\"end\":64997,\"start\":64996},{\"end\":65187,\"start\":65186},{\"end\":65195,\"start\":65194},{\"end\":65204,\"start\":65203},{\"end\":65459,\"start\":65458},{\"end\":65465,\"start\":65464},{\"end\":65472,\"start\":65471},{\"end\":65483,\"start\":65482},{\"end\":65746,\"start\":65745},{\"end\":65754,\"start\":65753},{\"end\":65763,\"start\":65762},{\"end\":65983,\"start\":65982},{\"end\":65991,\"start\":65990},{\"end\":65998,\"start\":65997},{\"end\":66006,\"start\":66005},{\"end\":66008,\"start\":66007},{\"end\":66021,\"start\":66020},{\"end\":66038,\"start\":66037},{\"end\":66040,\"start\":66039},{\"end\":66237,\"start\":66236},{\"end\":66248,\"start\":66247},{\"end\":66519,\"start\":66518},{\"end\":66525,\"start\":66524},{\"end\":66533,\"start\":66532},{\"end\":66542,\"start\":66541},{\"end\":66550,\"start\":66549},{\"end\":66567,\"start\":66563},{\"end\":66571,\"start\":66570},{\"end\":66944,\"start\":66943},{\"end\":66953,\"start\":66952},{\"end\":66962,\"start\":66961},{\"end\":66964,\"start\":66963},{\"end\":66975,\"start\":66974}]", "bib_author_last_name": "[{\"end\":51336,\"start\":51325},{\"end\":51346,\"start\":51340},{\"end\":51355,\"start\":51350},{\"end\":51597,\"start\":51591},{\"end\":51608,\"start\":51601},{\"end\":51618,\"start\":51612},{\"end\":51629,\"start\":51622},{\"end\":51866,\"start\":51861},{\"end\":51877,\"start\":51870},{\"end\":51889,\"start\":51881},{\"end\":52107,\"start\":52099},{\"end\":52117,\"start\":52111},{\"end\":52127,\"start\":52121},{\"end\":52137,\"start\":52131},{\"end\":52293,\"start\":52289},{\"end\":52302,\"start\":52297},{\"end\":52309,\"start\":52306},{\"end\":52320,\"start\":52315},{\"end\":52331,\"start\":52324},{\"end\":52350,\"start\":52342},{\"end\":52591,\"start\":52581},{\"end\":52605,\"start\":52595},{\"end\":52615,\"start\":52609},{\"end\":52833,\"start\":52825},{\"end\":52843,\"start\":52837},{\"end\":52856,\"start\":52849},{\"end\":52864,\"start\":52860},{\"end\":53089,\"start\":53081},{\"end\":53101,\"start\":53095},{\"end\":53254,\"start\":53244},{\"end\":53271,\"start\":53258},{\"end\":53280,\"start\":53275},{\"end\":53286,\"start\":53284},{\"end\":53302,\"start\":53290},{\"end\":53311,\"start\":53306},{\"end\":53324,\"start\":53315},{\"end\":53334,\"start\":53328},{\"end\":53520,\"start\":53510},{\"end\":53530,\"start\":53524},{\"end\":53543,\"start\":53534},{\"end\":53553,\"start\":53545},{\"end\":53743,\"start\":53741},{\"end\":53751,\"start\":53747},{\"end\":53758,\"start\":53755},{\"end\":53765,\"start\":53762},{\"end\":53774,\"start\":53769},{\"end\":53782,\"start\":53778},{\"end\":53790,\"start\":53786},{\"end\":53797,\"start\":53794},{\"end\":54031,\"start\":54022},{\"end\":54040,\"start\":54035},{\"end\":54052,\"start\":54044},{\"end\":54064,\"start\":54056},{\"end\":54079,\"start\":54070},{\"end\":54330,\"start\":54324},{\"end\":54342,\"start\":54334},{\"end\":54357,\"start\":54346},{\"end\":54368,\"start\":54361},{\"end\":54382,\"start\":54372},{\"end\":54608,\"start\":54606},{\"end\":54616,\"start\":54612},{\"end\":54823,\"start\":54814},{\"end\":54834,\"start\":54827},{\"end\":54843,\"start\":54838},{\"end\":54852,\"start\":54847},{\"end\":54863,\"start\":54856},{\"end\":55083,\"start\":55074},{\"end\":55098,\"start\":55089},{\"end\":55110,\"start\":55102},{\"end\":55119,\"start\":55114},{\"end\":55128,\"start\":55121},{\"end\":55136,\"start\":55132},{\"end\":55148,\"start\":55140},{\"end\":55386,\"start\":55380},{\"end\":55395,\"start\":55390},{\"end\":55852,\"start\":55846},{\"end\":55863,\"start\":55856},{\"end\":55871,\"start\":55867},{\"end\":56091,\"start\":56087},{\"end\":56099,\"start\":56095},{\"end\":56314,\"start\":56304},{\"end\":56324,\"start\":56318},{\"end\":56492,\"start\":56487},{\"end\":56501,\"start\":56496},{\"end\":56680,\"start\":56668},{\"end\":56690,\"start\":56684},{\"end\":56699,\"start\":56694},{\"end\":56711,\"start\":56703},{\"end\":56989,\"start\":56986},{\"end\":57001,\"start\":56993},{\"end\":57013,\"start\":57005},{\"end\":57023,\"start\":57017},{\"end\":57033,\"start\":57027},{\"end\":57369,\"start\":57366},{\"end\":57382,\"start\":57373},{\"end\":57388,\"start\":57386},{\"end\":57396,\"start\":57392},{\"end\":57405,\"start\":57400},{\"end\":57416,\"start\":57411},{\"end\":57622,\"start\":57619},{\"end\":57629,\"start\":57626},{\"end\":57637,\"start\":57633},{\"end\":57645,\"start\":57641},{\"end\":57985,\"start\":57975},{\"end\":57995,\"start\":57989},{\"end\":58145,\"start\":58142},{\"end\":58298,\"start\":58286},{\"end\":58308,\"start\":58302},{\"end\":58317,\"start\":58312},{\"end\":58327,\"start\":58321},{\"end\":58336,\"start\":58331},{\"end\":58346,\"start\":58340},{\"end\":58358,\"start\":58350},{\"end\":58369,\"start\":58362},{\"end\":58381,\"start\":58373},{\"end\":58394,\"start\":58385},{\"end\":58683,\"start\":58677},{\"end\":58694,\"start\":58689},{\"end\":58707,\"start\":58698},{\"end\":58716,\"start\":58711},{\"end\":59170,\"start\":59164},{\"end\":59181,\"start\":59174},{\"end\":59191,\"start\":59185},{\"end\":59398,\"start\":59394},{\"end\":59408,\"start\":59402},{\"end\":59420,\"start\":59412},{\"end\":59435,\"start\":59426},{\"end\":59690,\"start\":59684},{\"end\":59702,\"start\":59694},{\"end\":59916,\"start\":59910},{\"end\":59928,\"start\":59920},{\"end\":59938,\"start\":59932},{\"end\":59947,\"start\":59942},{\"end\":59958,\"start\":59951},{\"end\":59968,\"start\":59962},{\"end\":59981,\"start\":59972},{\"end\":60299,\"start\":60293},{\"end\":60308,\"start\":60303},{\"end\":60317,\"start\":60312},{\"end\":60326,\"start\":60321},{\"end\":60338,\"start\":60330},{\"end\":60348,\"start\":60342},{\"end\":60359,\"start\":60352},{\"end\":60366,\"start\":60363},{\"end\":60380,\"start\":60370},{\"end\":60390,\"start\":60384},{\"end\":60701,\"start\":60694},{\"end\":60710,\"start\":60707},{\"end\":60721,\"start\":60714},{\"end\":60731,\"start\":60725},{\"end\":60738,\"start\":60735},{\"end\":60749,\"start\":60742},{\"end\":60759,\"start\":60753},{\"end\":60769,\"start\":60763},{\"end\":60780,\"start\":60773},{\"end\":60789,\"start\":60784},{\"end\":60800,\"start\":60793},{\"end\":60813,\"start\":60804},{\"end\":61137,\"start\":61131},{\"end\":61149,\"start\":61141},{\"end\":61159,\"start\":61153},{\"end\":61166,\"start\":61163},{\"end\":61411,\"start\":61404},{\"end\":61421,\"start\":61415},{\"end\":61429,\"start\":61425},{\"end\":61440,\"start\":61433},{\"end\":61681,\"start\":61676},{\"end\":61691,\"start\":61685},{\"end\":61703,\"start\":61695},{\"end\":61715,\"start\":61707},{\"end\":61961,\"start\":61955},{\"end\":61974,\"start\":61965},{\"end\":61984,\"start\":61978},{\"end\":61992,\"start\":61988},{\"end\":62000,\"start\":61996},{\"end\":62224,\"start\":62219},{\"end\":62237,\"start\":62230},{\"end\":62247,\"start\":62241},{\"end\":62257,\"start\":62251},{\"end\":62487,\"start\":62480},{\"end\":62500,\"start\":62491},{\"end\":62510,\"start\":62504},{\"end\":62519,\"start\":62514},{\"end\":62528,\"start\":62523},{\"end\":62753,\"start\":62746},{\"end\":62761,\"start\":62757},{\"end\":62771,\"start\":62765},{\"end\":62777,\"start\":62775},{\"end\":62786,\"start\":62781},{\"end\":62796,\"start\":62790},{\"end\":62815,\"start\":62804},{\"end\":62825,\"start\":62821},{\"end\":62838,\"start\":62831},{\"end\":62849,\"start\":62844},{\"end\":63214,\"start\":63206},{\"end\":63220,\"start\":63218},{\"end\":63407,\"start\":63400},{\"end\":63799,\"start\":63792},{\"end\":63807,\"start\":63803},{\"end\":64037,\"start\":64023},{\"end\":64046,\"start\":64041},{\"end\":64065,\"start\":64050},{\"end\":64076,\"start\":64069},{\"end\":64277,\"start\":64273},{\"end\":64285,\"start\":64281},{\"end\":64294,\"start\":64289},{\"end\":64466,\"start\":64462},{\"end\":64475,\"start\":64470},{\"end\":64682,\"start\":64678},{\"end\":64700,\"start\":64686},{\"end\":64712,\"start\":64706},{\"end\":64721,\"start\":64716},{\"end\":64730,\"start\":64725},{\"end\":64739,\"start\":64734},{\"end\":64992,\"start\":64985},{\"end\":65001,\"start\":64998},{\"end\":65192,\"start\":65188},{\"end\":65201,\"start\":65196},{\"end\":65211,\"start\":65205},{\"end\":65462,\"start\":65460},{\"end\":65469,\"start\":65466},{\"end\":65480,\"start\":65473},{\"end\":65492,\"start\":65484},{\"end\":65751,\"start\":65747},{\"end\":65760,\"start\":65755},{\"end\":65768,\"start\":65764},{\"end\":65988,\"start\":65984},{\"end\":65995,\"start\":65992},{\"end\":66003,\"start\":65999},{\"end\":66018,\"start\":66009},{\"end\":66035,\"start\":66022},{\"end\":66043,\"start\":66041},{\"end\":66050,\"start\":66045},{\"end\":66245,\"start\":66238},{\"end\":66255,\"start\":66249},{\"end\":66522,\"start\":66520},{\"end\":66530,\"start\":66526},{\"end\":66539,\"start\":66534},{\"end\":66547,\"start\":66543},{\"end\":66561,\"start\":66551},{\"end\":66576,\"start\":66572},{\"end\":66950,\"start\":66945},{\"end\":66959,\"start\":66954},{\"end\":66972,\"start\":66965},{\"end\":66985,\"start\":66976}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2204.03632\",\"id\":\"b0\"},\"end\":51512,\"start\":51250},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1820089},\"end\":51789,\"start\":51514},{\"attributes\":{\"doi\":\"arXiv:1809.11096\",\"id\":\"b2\"},\"end\":52068,\"start\":51791},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2316535},\"end\":52239,\"start\":52070},{\"attributes\":{\"id\":\"b4\"},\"end\":52502,\"start\":52241},{\"attributes\":{\"doi\":\"arXiv:1707.08819\",\"id\":\"b5\"},\"end\":52780,\"start\":52504},{\"attributes\":{\"doi\":\"arXiv:2209.04747\",\"id\":\"b6\"},\"end\":53030,\"start\":52782},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":234357997},\"end\":53211,\"start\":53032},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1033682},\"end\":53504,\"start\":53213},{\"attributes\":{\"id\":\"b9\"},\"end\":53675,\"start\":53506},{\"attributes\":{\"doi\":\"arXiv:2111.14822\",\"id\":\"b10\"},\"end\":53979,\"start\":53677},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10894094},\"end\":54224,\"start\":53981},{\"attributes\":{\"id\":\"b12\"},\"end\":54560,\"start\":54226},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219955663},\"end\":54734,\"start\":54562},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":235262511},\"end\":55037,\"start\":54736},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":238354021},\"end\":55302,\"start\":55039},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":54482423},\"end\":55775,\"start\":55304},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":249240415},\"end\":56031,\"start\":55777},{\"attributes\":{\"doi\":\"arXiv:2106.00132\",\"id\":\"b18\"},\"end\":56245,\"start\":56033},{\"attributes\":{\"id\":\"b19\"},\"end\":56421,\"start\":56247},{\"attributes\":{\"id\":\"b20\"},\"end\":56594,\"start\":56423},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":118648975},\"end\":56977,\"start\":56596},{\"attributes\":{\"doi\":\"arXiv:2202.08345\",\"id\":\"b22\"},\"end\":57299,\"start\":56979},{\"attributes\":{\"id\":\"b23\"},\"end\":57572,\"start\":57301},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":459456},\"end\":57932,\"start\":57574},{\"attributes\":{\"id\":\"b25\"},\"end\":58083,\"start\":57934},{\"attributes\":{\"doi\":\"arXiv:2208.11970\",\"id\":\"b26\"},\"end\":58280,\"start\":58085},{\"attributes\":{\"doi\":\"arXiv:1710.03740\",\"id\":\"b27\"},\"end\":58624,\"start\":58282},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":232417882},\"end\":59100,\"start\":58626},{\"attributes\":{\"doi\":\"arXiv:1802.05957\",\"id\":\"b29\"},\"end\":59390,\"start\":59102},{\"attributes\":{\"doi\":\"arXiv:2103.03841\",\"id\":\"b30\"},\"end\":59627,\"start\":59392},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":231979499},\"end\":59810,\"start\":59629},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":245335086},\"end\":60219,\"start\":59812},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":202786778},\"end\":60619,\"start\":60221},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":231591445},\"end\":61061,\"start\":60621},{\"attributes\":{\"doi\":\"arXiv:2204.06125\",\"id\":\"b35\"},\"end\":61344,\"start\":61063},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7147309},\"end\":61574,\"start\":61346},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":231719657},\"end\":61895,\"start\":61576},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":206594923},\"end\":62141,\"start\":61897},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":8141422},\"end\":62414,\"start\":62143},{\"attributes\":{\"id\":\"b40\"},\"end\":62662,\"start\":62416},{\"attributes\":{\"doi\":\"arXiv:2205.11487\",\"id\":\"b41\"},\"end\":63138,\"start\":62664},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":246442182},\"end\":63334,\"start\":63140},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":203610243},\"end\":63723,\"start\":63336},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":124868013},\"end\":63955,\"start\":63725},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":14888175},\"end\":64232,\"start\":63957},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":222140788},\"end\":64388,\"start\":64234},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":196470871},\"end\":64599,\"start\":64390},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":227209335},\"end\":64920,\"start\":64601},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":2178983},\"end\":65111,\"start\":64922},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":245144350},\"end\":65456,\"start\":65113},{\"attributes\":{\"doi\":\"arXiv:2209.11178\",\"id\":\"b51\"},\"end\":65673,\"start\":65458},{\"attributes\":{\"doi\":\"arXiv:2209.00796\",\"id\":\"b52\"},\"end\":65913,\"start\":65675},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":195069387},\"end\":66234,\"start\":65915},{\"attributes\":{\"doi\":\"arXiv:1705.10941\",\"id\":\"b54\"},\"end\":66516,\"start\":66236},{\"attributes\":{\"doi\":\"arXiv:1506.03365\",\"id\":\"b55\"},\"end\":66898,\"start\":66518},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":3162051},\"end\":67109,\"start\":66900}]", "bib_title": "[{\"end\":51587,\"start\":51514},{\"end\":52095,\"start\":52070},{\"end\":52285,\"start\":52241},{\"end\":53077,\"start\":53032},{\"end\":53240,\"start\":53213},{\"end\":54018,\"start\":53981},{\"end\":54602,\"start\":54562},{\"end\":54810,\"start\":54736},{\"end\":55070,\"start\":55039},{\"end\":55376,\"start\":55304},{\"end\":55842,\"start\":55777},{\"end\":56664,\"start\":56596},{\"end\":57615,\"start\":57574},{\"end\":58673,\"start\":58626},{\"end\":59678,\"start\":59629},{\"end\":59904,\"start\":59812},{\"end\":60289,\"start\":60221},{\"end\":60690,\"start\":60621},{\"end\":61400,\"start\":61346},{\"end\":61672,\"start\":61576},{\"end\":61949,\"start\":61897},{\"end\":62215,\"start\":62143},{\"end\":63202,\"start\":63140},{\"end\":63396,\"start\":63336},{\"end\":63786,\"start\":63725},{\"end\":64019,\"start\":63957},{\"end\":64269,\"start\":64234},{\"end\":64458,\"start\":64390},{\"end\":64674,\"start\":64601},{\"end\":64981,\"start\":64922},{\"end\":65184,\"start\":65113},{\"end\":65980,\"start\":65915},{\"end\":66941,\"start\":66900}]", "bib_author": "[{\"end\":51338,\"start\":51323},{\"end\":51348,\"start\":51338},{\"end\":51357,\"start\":51348},{\"end\":51599,\"start\":51589},{\"end\":51610,\"start\":51599},{\"end\":51620,\"start\":51610},{\"end\":51631,\"start\":51620},{\"end\":51868,\"start\":51859},{\"end\":51879,\"start\":51868},{\"end\":51891,\"start\":51879},{\"end\":52109,\"start\":52097},{\"end\":52119,\"start\":52109},{\"end\":52129,\"start\":52119},{\"end\":52139,\"start\":52129},{\"end\":52295,\"start\":52287},{\"end\":52304,\"start\":52295},{\"end\":52311,\"start\":52304},{\"end\":52322,\"start\":52311},{\"end\":52333,\"start\":52322},{\"end\":52340,\"start\":52333},{\"end\":52352,\"start\":52340},{\"end\":52593,\"start\":52579},{\"end\":52607,\"start\":52593},{\"end\":52617,\"start\":52607},{\"end\":52835,\"start\":52820},{\"end\":52845,\"start\":52835},{\"end\":52858,\"start\":52845},{\"end\":52866,\"start\":52858},{\"end\":53091,\"start\":53079},{\"end\":53103,\"start\":53091},{\"end\":53256,\"start\":53242},{\"end\":53273,\"start\":53256},{\"end\":53282,\"start\":53273},{\"end\":53288,\"start\":53282},{\"end\":53304,\"start\":53288},{\"end\":53313,\"start\":53304},{\"end\":53326,\"start\":53313},{\"end\":53336,\"start\":53326},{\"end\":53522,\"start\":53508},{\"end\":53532,\"start\":53522},{\"end\":53545,\"start\":53532},{\"end\":53555,\"start\":53545},{\"end\":53745,\"start\":53739},{\"end\":53753,\"start\":53745},{\"end\":53760,\"start\":53753},{\"end\":53767,\"start\":53760},{\"end\":53776,\"start\":53767},{\"end\":53784,\"start\":53776},{\"end\":53792,\"start\":53784},{\"end\":53799,\"start\":53792},{\"end\":54033,\"start\":54020},{\"end\":54042,\"start\":54033},{\"end\":54054,\"start\":54042},{\"end\":54066,\"start\":54054},{\"end\":54081,\"start\":54066},{\"end\":54332,\"start\":54322},{\"end\":54344,\"start\":54332},{\"end\":54359,\"start\":54344},{\"end\":54370,\"start\":54359},{\"end\":54384,\"start\":54370},{\"end\":54610,\"start\":54604},{\"end\":54618,\"start\":54610},{\"end\":54627,\"start\":54618},{\"end\":54631,\"start\":54627},{\"end\":54825,\"start\":54812},{\"end\":54836,\"start\":54825},{\"end\":54845,\"start\":54836},{\"end\":54854,\"start\":54845},{\"end\":54865,\"start\":54854},{\"end\":55085,\"start\":55072},{\"end\":55100,\"start\":55085},{\"end\":55112,\"start\":55100},{\"end\":55121,\"start\":55112},{\"end\":55130,\"start\":55121},{\"end\":55138,\"start\":55130},{\"end\":55150,\"start\":55138},{\"end\":55388,\"start\":55378},{\"end\":55397,\"start\":55388},{\"end\":55404,\"start\":55397},{\"end\":55408,\"start\":55404},{\"end\":55854,\"start\":55844},{\"end\":55865,\"start\":55854},{\"end\":55873,\"start\":55865},{\"end\":55881,\"start\":55873},{\"end\":55885,\"start\":55881},{\"end\":56093,\"start\":56085},{\"end\":56101,\"start\":56093},{\"end\":56316,\"start\":56302},{\"end\":56326,\"start\":56316},{\"end\":56494,\"start\":56485},{\"end\":56503,\"start\":56494},{\"end\":56682,\"start\":56666},{\"end\":56692,\"start\":56682},{\"end\":56701,\"start\":56692},{\"end\":56713,\"start\":56701},{\"end\":56720,\"start\":56713},{\"end\":56724,\"start\":56720},{\"end\":56991,\"start\":56979},{\"end\":57003,\"start\":56991},{\"end\":57015,\"start\":57003},{\"end\":57025,\"start\":57015},{\"end\":57035,\"start\":57025},{\"end\":57371,\"start\":57364},{\"end\":57384,\"start\":57371},{\"end\":57390,\"start\":57384},{\"end\":57398,\"start\":57390},{\"end\":57407,\"start\":57398},{\"end\":57418,\"start\":57407},{\"end\":57624,\"start\":57617},{\"end\":57631,\"start\":57624},{\"end\":57639,\"start\":57631},{\"end\":57647,\"start\":57639},{\"end\":57987,\"start\":57973},{\"end\":57997,\"start\":57987},{\"end\":58147,\"start\":58140},{\"end\":58300,\"start\":58284},{\"end\":58310,\"start\":58300},{\"end\":58319,\"start\":58310},{\"end\":58329,\"start\":58319},{\"end\":58338,\"start\":58329},{\"end\":58348,\"start\":58338},{\"end\":58360,\"start\":58348},{\"end\":58371,\"start\":58360},{\"end\":58383,\"start\":58371},{\"end\":58396,\"start\":58383},{\"end\":58685,\"start\":58675},{\"end\":58696,\"start\":58685},{\"end\":58709,\"start\":58696},{\"end\":58718,\"start\":58709},{\"end\":59172,\"start\":59162},{\"end\":59183,\"start\":59172},{\"end\":59193,\"start\":59183},{\"end\":59203,\"start\":59193},{\"end\":59207,\"start\":59203},{\"end\":59400,\"start\":59392},{\"end\":59410,\"start\":59400},{\"end\":59422,\"start\":59410},{\"end\":59437,\"start\":59422},{\"end\":59692,\"start\":59680},{\"end\":59704,\"start\":59692},{\"end\":59918,\"start\":59906},{\"end\":59930,\"start\":59918},{\"end\":59940,\"start\":59930},{\"end\":59949,\"start\":59940},{\"end\":59960,\"start\":59949},{\"end\":59970,\"start\":59960},{\"end\":59983,\"start\":59970},{\"end\":59990,\"start\":59983},{\"end\":59994,\"start\":59990},{\"end\":60301,\"start\":60291},{\"end\":60310,\"start\":60301},{\"end\":60319,\"start\":60310},{\"end\":60328,\"start\":60319},{\"end\":60340,\"start\":60328},{\"end\":60350,\"start\":60340},{\"end\":60361,\"start\":60350},{\"end\":60368,\"start\":60361},{\"end\":60382,\"start\":60368},{\"end\":60392,\"start\":60382},{\"end\":60703,\"start\":60692},{\"end\":60712,\"start\":60703},{\"end\":60723,\"start\":60712},{\"end\":60733,\"start\":60723},{\"end\":60740,\"start\":60733},{\"end\":60751,\"start\":60740},{\"end\":60761,\"start\":60751},{\"end\":60771,\"start\":60761},{\"end\":60782,\"start\":60771},{\"end\":60791,\"start\":60782},{\"end\":60802,\"start\":60791},{\"end\":60815,\"start\":60802},{\"end\":61139,\"start\":61129},{\"end\":61151,\"start\":61139},{\"end\":61161,\"start\":61151},{\"end\":61168,\"start\":61161},{\"end\":61175,\"start\":61168},{\"end\":61179,\"start\":61175},{\"end\":61413,\"start\":61402},{\"end\":61423,\"start\":61413},{\"end\":61431,\"start\":61423},{\"end\":61442,\"start\":61431},{\"end\":61683,\"start\":61674},{\"end\":61693,\"start\":61683},{\"end\":61705,\"start\":61693},{\"end\":61717,\"start\":61705},{\"end\":61963,\"start\":61951},{\"end\":61976,\"start\":61963},{\"end\":61986,\"start\":61976},{\"end\":61994,\"start\":61986},{\"end\":62002,\"start\":61994},{\"end\":62226,\"start\":62217},{\"end\":62239,\"start\":62226},{\"end\":62249,\"start\":62239},{\"end\":62259,\"start\":62249},{\"end\":62489,\"start\":62478},{\"end\":62502,\"start\":62489},{\"end\":62512,\"start\":62502},{\"end\":62521,\"start\":62512},{\"end\":62530,\"start\":62521},{\"end\":62755,\"start\":62744},{\"end\":62763,\"start\":62755},{\"end\":62773,\"start\":62763},{\"end\":62779,\"start\":62773},{\"end\":62788,\"start\":62779},{\"end\":62798,\"start\":62788},{\"end\":62817,\"start\":62798},{\"end\":62827,\"start\":62817},{\"end\":62840,\"start\":62827},{\"end\":62851,\"start\":62840},{\"end\":63216,\"start\":63204},{\"end\":63222,\"start\":63216},{\"end\":63409,\"start\":63398},{\"end\":63801,\"start\":63788},{\"end\":63809,\"start\":63801},{\"end\":64039,\"start\":64021},{\"end\":64048,\"start\":64039},{\"end\":64067,\"start\":64048},{\"end\":64078,\"start\":64067},{\"end\":64279,\"start\":64271},{\"end\":64287,\"start\":64279},{\"end\":64296,\"start\":64287},{\"end\":64468,\"start\":64460},{\"end\":64477,\"start\":64468},{\"end\":64684,\"start\":64676},{\"end\":64702,\"start\":64684},{\"end\":64714,\"start\":64702},{\"end\":64723,\"start\":64714},{\"end\":64732,\"start\":64723},{\"end\":64741,\"start\":64732},{\"end\":64994,\"start\":64983},{\"end\":65003,\"start\":64994},{\"end\":65194,\"start\":65186},{\"end\":65203,\"start\":65194},{\"end\":65213,\"start\":65203},{\"end\":65464,\"start\":65458},{\"end\":65471,\"start\":65464},{\"end\":65482,\"start\":65471},{\"end\":65494,\"start\":65482},{\"end\":65753,\"start\":65745},{\"end\":65762,\"start\":65753},{\"end\":65770,\"start\":65762},{\"end\":65990,\"start\":65982},{\"end\":65997,\"start\":65990},{\"end\":66005,\"start\":65997},{\"end\":66020,\"start\":66005},{\"end\":66037,\"start\":66020},{\"end\":66045,\"start\":66037},{\"end\":66052,\"start\":66045},{\"end\":66247,\"start\":66236},{\"end\":66257,\"start\":66247},{\"end\":66524,\"start\":66518},{\"end\":66532,\"start\":66524},{\"end\":66541,\"start\":66532},{\"end\":66549,\"start\":66541},{\"end\":66563,\"start\":66549},{\"end\":66570,\"start\":66563},{\"end\":66578,\"start\":66570},{\"end\":66952,\"start\":66943},{\"end\":66961,\"start\":66952},{\"end\":66974,\"start\":66961},{\"end\":66987,\"start\":66974}]", "bib_venue": "[{\"end\":55557,\"start\":55491},{\"end\":57764,\"start\":57714},{\"end\":58881,\"start\":58808},{\"end\":63558,\"start\":63492},{\"end\":51321,\"start\":51250},{\"end\":51638,\"start\":51631},{\"end\":51857,\"start\":51791},{\"end\":52143,\"start\":52139},{\"end\":52356,\"start\":52352},{\"end\":52577,\"start\":52504},{\"end\":52818,\"start\":52782},{\"end\":53110,\"start\":53103},{\"end\":53343,\"start\":53336},{\"end\":53737,\"start\":53677},{\"end\":54088,\"start\":54081},{\"end\":54320,\"start\":54226},{\"end\":54638,\"start\":54631},{\"end\":54872,\"start\":54865},{\"end\":55154,\"start\":55150},{\"end\":55489,\"start\":55408},{\"end\":55892,\"start\":55885},{\"end\":56083,\"start\":56033},{\"end\":56300,\"start\":56247},{\"end\":56483,\"start\":56423},{\"end\":56773,\"start\":56724},{\"end\":57112,\"start\":57051},{\"end\":57362,\"start\":57301},{\"end\":57712,\"start\":57647},{\"end\":57971,\"start\":57934},{\"end\":58138,\"start\":58085},{\"end\":58806,\"start\":58718},{\"end\":59160,\"start\":59102},{\"end\":59498,\"start\":59453},{\"end\":59708,\"start\":59704},{\"end\":59998,\"start\":59994},{\"end\":60399,\"start\":60392},{\"end\":60819,\"start\":60815},{\"end\":61127,\"start\":61063},{\"end\":61449,\"start\":61442},{\"end\":61721,\"start\":61717},{\"end\":62006,\"start\":62002},{\"end\":62263,\"start\":62259},{\"end\":62476,\"start\":62416},{\"end\":62742,\"start\":62664},{\"end\":63226,\"start\":63222},{\"end\":63490,\"start\":63409},{\"end\":63819,\"start\":63809},{\"end\":64082,\"start\":64078},{\"end\":64300,\"start\":64296},{\"end\":64484,\"start\":64477},{\"end\":64745,\"start\":64741},{\"end\":65007,\"start\":65003},{\"end\":65272,\"start\":65213},{\"end\":65540,\"start\":65510},{\"end\":65743,\"start\":65675},{\"end\":66059,\"start\":66052},{\"end\":66353,\"start\":66273},{\"end\":66681,\"start\":66594},{\"end\":66994,\"start\":66987}]"}}}, "year": 2023, "month": 12, "day": 17}