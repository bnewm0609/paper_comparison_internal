{"id": 218630337, "updated": "2023-11-11 03:02:54.988", "metadata": {"title": "A Multi-Perspective Architecture for Semantic Code Search", "authors": "[{\"first\":\"Rajarshi\",\"last\":\"Haldar\",\"middle\":[]},{\"first\":\"Lingfei\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"JinJun\",\"last\":\"Xiong\",\"middle\":[]},{\"first\":\"Julia\",\"last\":\"Hockenmaier\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual neural framework for code\u2013text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3035371218", "acl": "2020.acl-main.758", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/HaldarWXH20", "doi": "10.18653/v1/2020.acl-main.758"}}, "content": {"source": {"pdf_hash": "90672a8e3161b188917bbdd1fa248bb0c590b67a", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2020.acl-main.758.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.acl-main.758.pdf", "status": "HYBRID"}}, "grobid": {"id": "c9fc044f1798e28be5ddc34a6f351e67f0d358af", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/90672a8e3161b188917bbdd1fa248bb0c590b67a.txt", "contents": "\nA Multi-Perspective Architecture for Semantic Code Search\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsJuly 5 -10, 2020. 2020\n\nRajarshi Haldar rhaldar2@illinois.edu \nLingfei Wu \nJinjun Xiong jinjun@us.ibm.com \nJulia Hockenmaier juliahmr@illinois.edu \n\n\u2020University of Illinois at Urbana-Champaign\nChampaignILUSA\n\n\n\u2021IBM Thomas J. Watson Research Center\nYorktown HeightsNYUSA\n\nA Multi-Perspective Architecture for Semantic Code Search\n\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics\nthe 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics8563July 5 -10, 2020. 2020\nThe ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multiperspective cross-lingual neural framework for code-text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space.\n\nIntroduction\n\nIn semantic code search or retrieval, the user provides a natural language query, and the system returns a ranked list of relevant code snippets from a database or repository for that query. This task is usually performed using a matching model that computes the similarity between code snippets and natural language descriptions by mapping code and natural language embeddings into a common space where the distance between a piece of code and its corresponding description is small (Gu et al., 2018;Yao et al., 2019).\n\nBut current models do not explicitly model any interactions between the code and the description until the final step when their global similarity is calculated.\n\nIn this paper, we propose a novel multiperspective neural framework for code-text matching that captures both global and local similarities. We show that it yields improved results on semantic code search. We apply our model to the CoNaLa benchmark dataset (Yin et al., 2018), which consists of Python code snippets and their corresponding annotations in English. We believe that our model could be applied to other programming languages as well. We have made our code publicly available for research purpose 1 .\n\n\nBackground\n\nSemantic code search is a cross-modal ranking problem where items in one modality (code) need to be ranked according to how well they match queries in another (natural language). One standard way to compute the similarity of items drawn from two different modalities or languages is to map each modality into a common \"semantic\" vector space such that matching pairs are mapped to vectors that are close to each other. Gu et al. (2018) propose a code retrieval framework that jointly embeds code snippets and NL descriptions into a high dimensional embedding space such that the vectors representing a code snippet and its corresponding description have high similarity.\n\nA variety of different approaches for learning embeddings for code have been proposed. Because source code is less ambiguous than natural language, there are ways to exploit the underlying structure of code to obtain better representations. Wan et al. (2019); show that using features extracted from Abstract Syntax Trees (AST's) and Control Flow Graphs (CFG's) lead to creating better representations of code. Hu et al. (2018);  show that ASTs represented as compact strings can be used to represent code. Following these approaches, we developed a multi-modal framework that generates embeddings for code using both the code tokens and an AST representation.\n\n\nModels\n\nWe compare four models: a baseline model (CT) that only considers text and source code, a (CAT) model that also includes embedding of Abstract Syntax Trees, a multi-perspective model (MP) that leverages multi-perspective matching operations as defined in a bilateral multi-perspective model (Wang et al., 2017), and our MP-CAT model that combines both MP and CAT architectures.\n\n\nCT: A Baseline Code and Text Model\n\nOur baseline model (CT) is based on Gu et al. (2018)'s CODEnn model. It maps both code and natural language descriptions to vectors in the same embedding space and then computes the similarity between these vectors using the L2 distance metric. These vectors are computed by two sets of three layers (one set per modality):\n\nThe Word Embedding Module consists of two independently pre-trained lookup tables that map code tokens or natural language tokens to embeddings. We use FastText (Bojanowski et al., 2017)) for all embeddings in this paper.\n\nThe Context Representation Module consists of bi-directional LSTM layers (one for code, one for text) that map the word embedding sequences into another pair of sequences of embeddings that contain contextual information.\n\nThe Maxpool Layer performs max pool (separately per dimension) over the Context Representation embedding sequences to obtain a single vector.\n\nThe Similarity Module computes the similarity of the two vectors v c and v c produced by the Maxpool Layers as\nd(v 1 , v 2 ) = d i=1 (v 1i \u2212 v 2i ) 2 sim(v c , v d ) = 1 \u2212 d( v c v c 2 , v d v d 2 )\nwhere d returns the L2 distance between ddimensional vectors v c and v d .\n\n\nCAT: An AST-Based Model\n\nTo capture both syntactic and semantic features, we augment our baseline CT model with embeddings based on the Abstract Syntax Tree (AST) representation of the code. Most programming languages, including Python, come with a deterministic parser that outputs the AST representation of a code snippet. Python has a library module called ast that generates AST representations of code. We convert this AST representation to a string using structure-based traversal (SBT) (Hu et al., 2018). The CAT model is similar to the CT model, except that it extracts features from both the source code tokens and its corresponding AST representation. So the Word Embedding Module now contains three lookup tables: for code, AST, and natural language, respectively. Similarly, the Context Representation Module has 3 bi-directional LSTM layers which is followed by 3 Maxpool Layers.\n\nBefore the output is passed to the similarity module, the output vectors of the two max pool layers representing code and AST are concatenated to form a single representation of the source code. Because of this, the hidden dimension in the bidirectional LSTM's of the Context Representation Module for the natural language sequence is double that of code and AST sequences' LSTM hidden dimensions. This ensures that, after concatenation, the vectors representing the candidate code snippet and the natural language description are of the same dimension. After that, the Similarity Module computes the similarity of these vectors via the same L2-distance-based operation as in CT.\n\n\nMP: A Multi-Perspective Model\n\nThe CT and CAT models learn to map source code and natural language tokens into a joint embedding space such that semantically similar code-natural language pairs are projected to vectors that are close to each other. However, these two representations interact only in the final step when the global similarity of the sequence embeddings is calculated, but not during the first step when each sequence is encoded into its corresponding embedding. Wang et al. (2017) show that, for tasks such as paraphrase identification and natural language inference that require two pieces of texts from the same language to compare, it is beneficial to include a number of different (i.e., multi-perspective) local matching operations between the two input sequences when computing their vector representations. Given contextual sequence encodings P and Q (computed, e.g., by biLSTMs) for the two sequences to be compared, Wang et al. (2017)'s Bilateral Multi-Perspective Matching (BiMPM) model includes a matching mechanism that compares P and Q by matching each position in P with all positions in Q, and by matching each position in Q with all positions in P , under four different match-ing strategies. We will discuss these strategies in more detail under the Bilateral Multi-Perspective Matching (BiMPM) Module.\n\nWe apply the MP model to our cross-modal codetext matching task as follows: The Word Embedding Layer takes as input the code sequence, AST sequence, and description sequence. The output of this layer is three independent sequences of token embeddings, one for each input sequence.\n\nThe Context Representation Module consists of three sets of BiLSTM layers that each computes a contextual representation of each token in the corresponding input sequence. We concatenate the hidden states of the sequences representing the code and AST, respectively, to get one set of sequence embeddings representing the source code input.\n\nThe Full matching sets Q m (P m ) to be the final hidden state of Q (and vice versa for P ).\n\nMaxpool matching obtains Q m by performing maximum pooling (per dimension) across the elements of Q.\n\nAttentive matching computes Q m as a weighted average of all Q[j] \u2208 Q, where Q[j]'s weight is the cosine similarity of P [i] and Q[j].\n\nMax-Attentive matching sets Q m to be the Q[j] with the highest cosine similarity to P [i].\n\nWe concatenate the four P [i] m (Q [i] m ) for each token i to get two new sequences P and Q .\n\nThe Local Aggregation Module aggregates these sequence embeddings into two fixed-length multi-perspective hidden representations by passing them through two different bi-LSTM layers (one for each sequence). For each sequence, we concatenate the final hidden states of both the forward and reverse directions to get a vector repre-sentation of that sequence.\n\nThe Similarity Module computes the similarity of the two vectors returned by the Aggregation Module as before.\n\n\nMP-CAT: A Combined Model\n\nOur final model combines the MP and the CAT models. It contains the following components:\n\nThe CAT module reads in the code sequence, the AST sequence, and the natural language sequence and outputs two vectors, one jointly representing the code and the AST and the other representing the natural language description.\n\nThe MP module also reads in the code sequence, the AST sequence, and the natural language sequence. It returns two vectors, one for code and AST, and the other for the natural language description. The difference between this module and the previous is that MP contains local information that is ignored in the global CAT embeddings.\n\nThe Global and Local Fusion Module concatenates the two CAT and MP vectors representing the code to get the final code representation, and does the same for the CAT and MP vectors representing the natural language description, before computing their L2 distance in the same manner as the other similarity modules. Figure 1 shows the pipeline of the MP-CAT framework.\n\n\nExperiments\n\nThe CoNaLa Dataset The CoNaLa dataset (Yin et al., 2018) has two parts, a manually curated parallel corpus of 2,379 training and 500 test examples, and a large automatically-mined dataset with 600k examples (which we ignore here). Each example consists of a snippet of Python code and its corresponding English description.\n\nPre-processing We pre-process the text representing both the source code and the natural language descriptions using sub-word regularization based on unigram language modeling (Kudo, 2018) transforms the original tokens into sequences of shorter (and hence more common) substrings. We use the sentencepiece library (Kudo and Richardson, 2018) and follow the same approach as used by Yin et al. (2018) for the CoNaLa dataset.\n\nTraining procedure During training, we use triplets consisting of a code snippet, a correct description, and an incorrect description (obtained by   random sampling from the training set). We sample 5 incorrect descriptions for each code-text pair, giving us five triplets for each training example. During the evaluation phase, for every natural language query D, we calculate the rank of its corresponding code snippet C among all 500 candidates in the test set.\n\n\nExperimental Setup\n\nWe train our models on triplets C, D + , D \u2212 consisting of a snippet of code C, a natural language description D + that correctly describes what the code does (a positive example), and a description D \u2212 that does not describe what the code does (a negative example). We minimize the ranking loss with margin , following Gu et al. (2018):\nL(\u03b8) = C,D + ,D \u2212 max 0, \u2212 cos(C, D + ) + cos(C, D \u2212 )\nIn the CAT model, since we first concatenate the vectors for the code and AST before comparing them with the vector for the natural language description, the first two vectors are each half the dimension size of the third one. Our models are implemented in PyTorch (Paszke et al., 2017) and trained using Adam (Kingma and Ba, 2014). Each model is trained for 100 epochs, and during the evaluation step, we use a set of 500 natural language queries from the test set. The training and evaluation times are shown in Table 2.\n\n\nResults\n\nTable 2 shows our test set results for code search. We report Recall@K (K=1,5,10) and mean reciprocal rank (MRR) of the correct answer.\n\nThe Impact of Modeling ASTs: In going from the first (CT) row to the second (CAT) row in Table 2, we see that the AST features alone increase MRR from 0.172 to 0.207. There is also an increase in R@k for all values of k. In fact, its R@5 values are competitive with our best model.\n\n\nMulti-Perspective Results:\n\nThe results for the multi-perspective models are both surprising and interesting. Row 3 of Table 2 shows that the MP model on its own under-performs and actually has the worst results out of all the models we tested. On the other hand, we see that combining the MP and the CAT models into one framework gives the best performance across the board. This shows that even if we use a multi-perspective framework to model local features, we still need encoders to capture the global features of code and text in addition to the local features; otherwise, we end up missing the forest for the trees.   \n\n\nComparison of MP-CAT, MP and CAT Models\n\nIn Table 3, we present the retrieval results for select natural language queries from the development set returned by the MP-CAT and CAT models. We do the same thing for MP-CAT and MP models in Table 4. Comparing MP-CAT and CAT, we observe that while CAT correctly identifies the data structures and libraries required to solve the user's problem, it ends up returning the wrong command. MP, on the other hand, sometimes fails to identify even the correct libraries required. In the second example in Table 4, it fails to understand that there is also a dictionary involved and ends up returning the wrong command. MP-CAT successfully finds the required code snippet when the user queries are longer and have multiple data structures involved.\n\n\nConclusions\n\nIn this paper, we consider the task of semantic code search or retrieval using a code-text similarity model. We propose MP-CAT, a novel multiperspective deep neural network framework for this task. In contrast to previous approaches, the multiperspective nature of our model allows it to capture richer similarities between the two sequences.\n\n\nBilateral Multi-Perspective Matching (BiMPM) Module compares the two sequences, say P and Q, by matching each position in P with all positions in Q, and by matching each position in Q with all positions in P , under four different matching strategies m that each produce new embedding sequences P m and Q m that have the same length as the original P and Q. Each matching strategy is parameterized by a feedforward network (e.g. P [i] m = f P \u2192Q m (P [i], Q m ; W P \u2192Q m )) that takes in a token embedding P [i] and a strategyspecific single-vector representation of Q m , and returns a new vector P [i] m for P [i]. For each token P [i] \u2208 P (and conversely for any Q[j] \u2208 Q), Q m (P m ) is defined as follows:\n\nFigure 1 :\n1The MP-CAT framework that contains both global-level and local-level features for code-text matching\n\nTable 2 :\n2Code Search Results\n\n\nbin/bash -c \"$GREPDB\"') Select records of dataframe 'df' where the sum of column 'X' for each value in column 'User' is 0 df.groupby('User')['X'].filter( lambda x: x.sum() == 0) print(df.loc[df['B'].isin( ['one', 'three'])])Query \nMP-CAT \nCAT \n\nSort dictionary 'x' by value in \nascending order \n\nsorted(list(x.items( )), \nkey = operator.itemgetter(1)) \n\nfor k in sorted( \nfoo.keys( )): \npass \n\nRun a command 'echo hello world' \nin bash instead of shell \n\nos.system \n(/bin/bash -c \"echo hello world\") \n\nos.system \n( 'GREPDB= \n\"echo 123\"; \n/\n\nTable 3 :\n3The top hits returned by the MP-CAT and CAT models for a natural language query. convert pandas DataFrame 'df' to a dictionary using 'id' field as the key df.set index( 'id').to dict() data[ data['Value'] == True]Query \nMP-CAT \nMP \nConcatenate elements of a \nlist 'x' of multiple integers \nto a single integer \n\nsum(d*10**i \nfor i, d in enumerate( \nx[::-1])) \n\n[float( i ) \nfor i in lst] \n\nReplace repeated instances \nof a character '*' with a \nsingle instance in a string 'text' \n\nre.sub('\\\\*\\\\*+', '*', text) \n\nre.sub('\u02c6(( \n?:(?!cat).)*cat( \n?:(?!cat).)*)cat', \n'\\\\\\\\1Bull', s) \n\n\n\nTable 4 :\n4The top hits returned by the MP-CAT and MP models for a natural language query.\nhttps://github.com/rajarshihaldar/ codetextmatch\nAcknowledgementThis work is supported by the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR), a research collaboration as part of the IBM AI Horizons Network.\nEnriching word vectors with subword information. Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, 10.1162/tacl_a_00051Transactions of the Association for Computational Linguistics. 5Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa- tion for Computational Linguistics, 5:135-146.\n\nDeep code search. Xiaodong Gu, Hongyu Zhang, Sunghun Kim, Proceedings of the 2018 40th International Conference on Software Engineering. the 2018 40th International Conference on Software EngineeringACMXiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In Proceedings of the 2018 40th International Conference on Software Engineering (ICSE 2018). ACM.\n\nImproved automatic summarization of subroutines via attention to file context. Sakib Haque, Alexander Leclair, Lingfei Wu, Collin Mcmillan, abs/2004.04881ArXiv. Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan. 2020. Improved automatic sum- marization of subroutines via attention to file con- text. ArXiv, abs/2004.04881.\n\nDeep code comment generation. Xing Hu, Ge Li, Xin Xia, David Lo, Zhi Jin, 10.1145/3196321.3196334Proceedings of the 26th Conference on Program Comprehension, ICPC '18. the 26th Conference on Program Comprehension, ICPC '18New York, NY, USAACMXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. In Proceedings of the 26th Conference on Program Comprehension, ICPC '18, pages 200-210, New York, NY, USA. ACM.\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, International Conference on Learning Representations. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. International Conference on Learning Representations.\n\nSubword regularization: Improving neural network translation models with multiple subword candidates. Taku Kudo, 10.18653/v1/P18-1007Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple sub- word candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 66-75, Mel- bourne, Australia. Association for Computational Linguistics.\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Taku Kudo, John Richardson, 10.18653/v1/D18-2012Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2018 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsBrussels, BelgiumAssociation for Computational LinguisticsTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tok- enizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics.\n\nImproved code summarization via a graph neural network. Alexander Leclair, Sakib Haque, Linfgei Wu, Collin Mcmillan, abs/2004.02843ArXiv. Alexander LeClair, Sakib Haque, Linfgei Wu, and Collin McMillan. 2020. Improved code sum- marization via a graph neural network. ArXiv, abs/2004.02843.\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NIPS-W. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. In NIPS-W.\n\nMulti-modal attention network learning for semantic source code retrieval. Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, Philip S Yu, 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and Philip S. Yu. 2019. Multi-modal attention network learning for seman- tic source code retrieval. 2019 34th IEEE/ACM In- ternational Conference on Automated Software En- gineering (ASE), pages 13-25.\n\nBilateral multi-perspective matching for natural language sentences. Zhiguo Wang, Wael Hamza, Radu Florian, 10.24963/ijcai.2017/579Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17. the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17Zhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective matching for natural lan- guage sentences. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelli- gence, IJCAI-17, pages 4144-4150.\n\nCoacor: Code annotation for code retrieval with reinforcement learning. Ziyu Yao, Jayavardhan Reddy Peddamail, Huan Sun, 10.1145/3308558.3313632The World Wide Web Conference, WWW '19. New York, NY, USAACMZiyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019. Coacor: Code annotation for code re- trieval with reinforcement learning. In The World Wide Web Conference, WWW '19, pages 2203- 2214, New York, NY, USA. ACM.\n\nLearning to mine aligned code and natural language pairs from stack overflow. Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, Graham Neubig, 10.1145/3196398.3196408International Conference on Mining Software Repositories, MSR. ACMPengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned code and natural language pairs from stack overflow. In International Conference on Min- ing Software Repositories, MSR, pages 476-486. ACM.\n", "annotations": {"author": "[{\"end\":213,\"start\":175},{\"end\":225,\"start\":214},{\"end\":257,\"start\":226},{\"end\":298,\"start\":258},{\"end\":359,\"start\":299},{\"end\":421,\"start\":360}]", "publisher": "[{\"end\":100,\"start\":59},{\"end\":682,\"start\":641}]", "author_last_name": "[{\"end\":190,\"start\":184},{\"end\":224,\"start\":222},{\"end\":238,\"start\":233},{\"end\":275,\"start\":264}]", "author_first_name": "[{\"end\":183,\"start\":175},{\"end\":221,\"start\":214},{\"end\":232,\"start\":226},{\"end\":263,\"start\":258}]", "author_affiliation": "[{\"end\":358,\"start\":300},{\"end\":420,\"start\":361}]", "title": "[{\"end\":58,\"start\":1},{\"end\":479,\"start\":422}]", "venue": "[{\"end\":568,\"start\":481}]", "abstract": "[{\"end\":1346,\"start\":709}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1863,\"start\":1846},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1880,\"start\":1863},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2321,\"start\":2303},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3008,\"start\":2992},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3504,\"start\":3486},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3672,\"start\":3656},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4226,\"start\":4207},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4384,\"start\":4368},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4843,\"start\":4818},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6031,\"start\":6015},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7595,\"start\":7577},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8058,\"start\":8040},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11171,\"start\":11153},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11628,\"start\":11616},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11782,\"start\":11755},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11840,\"start\":11823},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12689,\"start\":12673},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13032,\"start\":13011},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13077,\"start\":13056}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16184,\"start\":15472},{\"attributes\":{\"id\":\"fig_1\"},\"end\":16298,\"start\":16185},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":16330,\"start\":16299},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":16872,\"start\":16331},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":17467,\"start\":16873},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":17559,\"start\":17468}]", "paragraph": "[{\"end\":1881,\"start\":1362},{\"end\":2044,\"start\":1883},{\"end\":2558,\"start\":2046},{\"end\":3243,\"start\":2573},{\"end\":3905,\"start\":3245},{\"end\":4293,\"start\":3916},{\"end\":4655,\"start\":4332},{\"end\":4878,\"start\":4657},{\"end\":5101,\"start\":4880},{\"end\":5244,\"start\":5103},{\"end\":5356,\"start\":5246},{\"end\":5519,\"start\":5445},{\"end\":6414,\"start\":5547},{\"end\":7095,\"start\":6416},{\"end\":8434,\"start\":7129},{\"end\":8716,\"start\":8436},{\"end\":9058,\"start\":8718},{\"end\":9152,\"start\":9060},{\"end\":9254,\"start\":9154},{\"end\":9390,\"start\":9256},{\"end\":9483,\"start\":9392},{\"end\":9579,\"start\":9485},{\"end\":9938,\"start\":9581},{\"end\":10050,\"start\":9940},{\"end\":10168,\"start\":10079},{\"end\":10396,\"start\":10170},{\"end\":10731,\"start\":10398},{\"end\":11099,\"start\":10733},{\"end\":11438,\"start\":11115},{\"end\":11864,\"start\":11440},{\"end\":12330,\"start\":11866},{\"end\":12690,\"start\":12353},{\"end\":13268,\"start\":12746},{\"end\":13415,\"start\":13280},{\"end\":13698,\"start\":13417},{\"end\":14326,\"start\":13729},{\"end\":15113,\"start\":14370},{\"end\":15471,\"start\":15129}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5444,\"start\":5357},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12745,\"start\":12691}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13267,\"start\":13260},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13827,\"start\":13820},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":14380,\"start\":14373},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":14571,\"start\":14564},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":14878,\"start\":14871}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1360,\"start\":1348},{\"attributes\":{\"n\":\"2\"},\"end\":2571,\"start\":2561},{\"attributes\":{\"n\":\"3\"},\"end\":3914,\"start\":3908},{\"attributes\":{\"n\":\"3.1\"},\"end\":4330,\"start\":4296},{\"attributes\":{\"n\":\"3.2\"},\"end\":5545,\"start\":5522},{\"attributes\":{\"n\":\"3.3\"},\"end\":7127,\"start\":7098},{\"attributes\":{\"n\":\"3.4\"},\"end\":10077,\"start\":10053},{\"attributes\":{\"n\":\"4\"},\"end\":11113,\"start\":11102},{\"attributes\":{\"n\":\"4.1\"},\"end\":12351,\"start\":12333},{\"attributes\":{\"n\":\"4.2\"},\"end\":13278,\"start\":13271},{\"end\":13727,\"start\":13701},{\"end\":14368,\"start\":14329},{\"attributes\":{\"n\":\"5\"},\"end\":15127,\"start\":15116},{\"end\":16196,\"start\":16186},{\"end\":16309,\"start\":16300},{\"end\":16883,\"start\":16874},{\"end\":17478,\"start\":17469}]", "table": "[{\"end\":16872,\"start\":16557},{\"end\":17467,\"start\":17098}]", "figure_caption": "[{\"end\":16184,\"start\":15474},{\"end\":16298,\"start\":16198},{\"end\":16330,\"start\":16311},{\"end\":16557,\"start\":16333},{\"end\":17098,\"start\":16885},{\"end\":17559,\"start\":17480}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11055,\"start\":11047}]", "bib_author_first_name": "[{\"end\":17842,\"start\":17837},{\"end\":17862,\"start\":17855},{\"end\":17876,\"start\":17870},{\"end\":17890,\"start\":17885},{\"end\":18209,\"start\":18201},{\"end\":18220,\"start\":18214},{\"end\":18235,\"start\":18228},{\"end\":18637,\"start\":18632},{\"end\":18654,\"start\":18645},{\"end\":18671,\"start\":18664},{\"end\":18682,\"start\":18676},{\"end\":18926,\"start\":18922},{\"end\":18933,\"start\":18931},{\"end\":18941,\"start\":18938},{\"end\":18952,\"start\":18947},{\"end\":18960,\"start\":18957},{\"end\":19384,\"start\":19376},{\"end\":19398,\"start\":19393},{\"end\":19698,\"start\":19694},{\"end\":20383,\"start\":20379},{\"end\":20394,\"start\":20390},{\"end\":21095,\"start\":21086},{\"end\":21110,\"start\":21105},{\"end\":21125,\"start\":21118},{\"end\":21136,\"start\":21130},{\"end\":21363,\"start\":21359},{\"end\":21375,\"start\":21372},{\"end\":21390,\"start\":21383},{\"end\":21408,\"start\":21401},{\"end\":21423,\"start\":21417},{\"end\":21437,\"start\":21430},{\"end\":21452,\"start\":21446},{\"end\":21463,\"start\":21458},{\"end\":21479,\"start\":21475},{\"end\":21492,\"start\":21488},{\"end\":21787,\"start\":21784},{\"end\":21801,\"start\":21793},{\"end\":21812,\"start\":21807},{\"end\":21826,\"start\":21818},{\"end\":21835,\"start\":21831},{\"end\":21846,\"start\":21842},{\"end\":21857,\"start\":21851},{\"end\":21859,\"start\":21858},{\"end\":22290,\"start\":22284},{\"end\":22301,\"start\":22297},{\"end\":22313,\"start\":22309},{\"end\":22851,\"start\":22847},{\"end\":22868,\"start\":22857},{\"end\":22890,\"start\":22886},{\"end\":23285,\"start\":23276},{\"end\":23296,\"start\":23291},{\"end\":23308,\"start\":23303},{\"end\":23321,\"start\":23315},{\"end\":23339,\"start\":23333}]", "bib_author_last_name": "[{\"end\":17853,\"start\":17843},{\"end\":17868,\"start\":17863},{\"end\":17883,\"start\":17877},{\"end\":17898,\"start\":17891},{\"end\":18212,\"start\":18210},{\"end\":18226,\"start\":18221},{\"end\":18239,\"start\":18236},{\"end\":18643,\"start\":18638},{\"end\":18662,\"start\":18655},{\"end\":18674,\"start\":18672},{\"end\":18691,\"start\":18683},{\"end\":18929,\"start\":18927},{\"end\":18936,\"start\":18934},{\"end\":18945,\"start\":18942},{\"end\":18955,\"start\":18953},{\"end\":18964,\"start\":18961},{\"end\":19391,\"start\":19385},{\"end\":19401,\"start\":19399},{\"end\":19703,\"start\":19699},{\"end\":20388,\"start\":20384},{\"end\":20405,\"start\":20395},{\"end\":21103,\"start\":21096},{\"end\":21116,\"start\":21111},{\"end\":21128,\"start\":21126},{\"end\":21145,\"start\":21137},{\"end\":21370,\"start\":21364},{\"end\":21381,\"start\":21376},{\"end\":21399,\"start\":21391},{\"end\":21415,\"start\":21409},{\"end\":21428,\"start\":21424},{\"end\":21444,\"start\":21438},{\"end\":21456,\"start\":21453},{\"end\":21473,\"start\":21464},{\"end\":21486,\"start\":21480},{\"end\":21498,\"start\":21493},{\"end\":21791,\"start\":21788},{\"end\":21805,\"start\":21802},{\"end\":21816,\"start\":21813},{\"end\":21829,\"start\":21827},{\"end\":21840,\"start\":21836},{\"end\":21849,\"start\":21847},{\"end\":21862,\"start\":21860},{\"end\":22295,\"start\":22291},{\"end\":22307,\"start\":22302},{\"end\":22321,\"start\":22314},{\"end\":22855,\"start\":22852},{\"end\":22884,\"start\":22869},{\"end\":22894,\"start\":22891},{\"end\":23289,\"start\":23286},{\"end\":23301,\"start\":23297},{\"end\":23313,\"start\":23309},{\"end\":23331,\"start\":23322},{\"end\":23346,\"start\":23340}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1162/tacl_a_00051\",\"id\":\"b0\",\"matched_paper_id\":207556454},\"end\":18181,\"start\":17788},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":47021242},\"end\":18551,\"start\":18183},{\"attributes\":{\"doi\":\"abs/2004.04881\",\"id\":\"b2\",\"matched_paper_id\":215737269},\"end\":18890,\"start\":18553},{\"attributes\":{\"doi\":\"10.1145/3196321.3196334\",\"id\":\"b3\",\"matched_paper_id\":49584534},\"end\":19330,\"start\":18892},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6628106},\"end\":19590,\"start\":19332},{\"attributes\":{\"doi\":\"10.18653/v1/P18-1007\",\"id\":\"b5\",\"matched_paper_id\":13753208},\"end\":20266,\"start\":19592},{\"attributes\":{\"doi\":\"10.18653/v1/D18-2012\",\"id\":\"b6\",\"matched_paper_id\":52051958},\"end\":21028,\"start\":20268},{\"attributes\":{\"doi\":\"abs/2004.02843\",\"id\":\"b7\",\"matched_paper_id\":214802082},\"end\":21319,\"start\":21030},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":40027675},\"end\":21707,\"start\":21321},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":203593801},\"end\":22213,\"start\":21709},{\"attributes\":{\"doi\":\"10.24963/ijcai.2017/579\",\"id\":\"b10\",\"matched_paper_id\":9395040},\"end\":22773,\"start\":22215},{\"attributes\":{\"doi\":\"10.1145/3308558.3313632\",\"id\":\"b11\",\"matched_paper_id\":86524089},\"end\":23196,\"start\":22775},{\"attributes\":{\"doi\":\"10.1145/3196398.3196408\",\"id\":\"b12\",\"matched_paper_id\":43922261},\"end\":23684,\"start\":23198}]", "bib_title": "[{\"end\":17835,\"start\":17788},{\"end\":18199,\"start\":18183},{\"end\":18630,\"start\":18553},{\"end\":18920,\"start\":18892},{\"end\":19374,\"start\":19332},{\"end\":19692,\"start\":19592},{\"end\":20377,\"start\":20268},{\"end\":21084,\"start\":21030},{\"end\":21357,\"start\":21321},{\"end\":21782,\"start\":21709},{\"end\":22282,\"start\":22215},{\"end\":22845,\"start\":22775},{\"end\":23274,\"start\":23198}]", "bib_author": "[{\"end\":17855,\"start\":17837},{\"end\":17870,\"start\":17855},{\"end\":17885,\"start\":17870},{\"end\":17900,\"start\":17885},{\"end\":18214,\"start\":18201},{\"end\":18228,\"start\":18214},{\"end\":18241,\"start\":18228},{\"end\":18645,\"start\":18632},{\"end\":18664,\"start\":18645},{\"end\":18676,\"start\":18664},{\"end\":18693,\"start\":18676},{\"end\":18931,\"start\":18922},{\"end\":18938,\"start\":18931},{\"end\":18947,\"start\":18938},{\"end\":18957,\"start\":18947},{\"end\":18966,\"start\":18957},{\"end\":19393,\"start\":19376},{\"end\":19403,\"start\":19393},{\"end\":19705,\"start\":19694},{\"end\":20390,\"start\":20379},{\"end\":20407,\"start\":20390},{\"end\":21105,\"start\":21086},{\"end\":21118,\"start\":21105},{\"end\":21130,\"start\":21118},{\"end\":21147,\"start\":21130},{\"end\":21372,\"start\":21359},{\"end\":21383,\"start\":21372},{\"end\":21401,\"start\":21383},{\"end\":21417,\"start\":21401},{\"end\":21430,\"start\":21417},{\"end\":21446,\"start\":21430},{\"end\":21458,\"start\":21446},{\"end\":21475,\"start\":21458},{\"end\":21488,\"start\":21475},{\"end\":21500,\"start\":21488},{\"end\":21793,\"start\":21784},{\"end\":21807,\"start\":21793},{\"end\":21818,\"start\":21807},{\"end\":21831,\"start\":21818},{\"end\":21842,\"start\":21831},{\"end\":21851,\"start\":21842},{\"end\":21864,\"start\":21851},{\"end\":22297,\"start\":22284},{\"end\":22309,\"start\":22297},{\"end\":22323,\"start\":22309},{\"end\":22857,\"start\":22847},{\"end\":22886,\"start\":22857},{\"end\":22896,\"start\":22886},{\"end\":23291,\"start\":23276},{\"end\":23303,\"start\":23291},{\"end\":23315,\"start\":23303},{\"end\":23333,\"start\":23315},{\"end\":23348,\"start\":23333}]", "bib_venue": "[{\"end\":18382,\"start\":18320},{\"end\":19131,\"start\":19060},{\"end\":19906,\"start\":19814},{\"end\":20649,\"start\":20538},{\"end\":22531,\"start\":22447},{\"end\":22976,\"start\":22959},{\"end\":17981,\"start\":17920},{\"end\":18318,\"start\":18241},{\"end\":18712,\"start\":18707},{\"end\":19058,\"start\":18989},{\"end\":19455,\"start\":19403},{\"end\":19812,\"start\":19725},{\"end\":20536,\"start\":20427},{\"end\":21166,\"start\":21161},{\"end\":21506,\"start\":21500},{\"end\":21942,\"start\":21864},{\"end\":22445,\"start\":22346},{\"end\":22957,\"start\":22919},{\"end\":23432,\"start\":23371}]"}}}, "year": 2023, "month": 12, "day": 17}