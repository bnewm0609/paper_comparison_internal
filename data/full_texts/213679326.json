{"id": 213679326, "updated": "2022-01-28 02:44:02.542", "metadata": {"title": "A Multi-Stream Graph Convolutional Networks-Hidden Conditional Random Field Model for Skeleton-Based Action Recognition", "authors": "[{\"middle\":[],\"last\":\"Liu\",\"first\":\"Kai\"},{\"middle\":[],\"last\":\"Gao\",\"first\":\"Lei\"},{\"middle\":[\"Mefraz\"],\"last\":\"Khan\",\"first\":\"Naimul\"},{\"middle\":[],\"last\":\"Qi\",\"first\":\"Lin\"},{\"middle\":[],\"last\":\"Guan\",\"first\":\"Ling\"}]", "venue": "IEEE Transactions on Multimedia", "journal": "IEEE Transactions on Multimedia", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Recently, Graph Convolutional Network(GCN) methods for skeleton-based action recognition have achieved great success due to their ability to preserve structural information of the skeleton. However, these methods abandon the structural information in the classification stage by employing traditional fully-connected layers and softmax classifier, leading to sub-optimal performance. In this work, a novel Graph Convolutional Networks-Hidden conditional Random Field (GCN-HCRF) model is proposed to solve this problem. The proposed method combines GCN with HCRF to retain the human skeleton structure information even during the classification stage. Our model is trained end-to-end by utilizing the message passing from the belief propagation algorithm on the human structure graph. To further capture spatial and temporal information, we propose a multi-stream framework which takes the relative coordinate of the joints and bone direction as two static feature streams, and the temporal displacements between two consecutive frames as the dynamic feature stream. Experimental results on three challenging benchmarks (NTU RGB+D, N-UCLA, SYSU) show the superior performance of the proposed model over state-of-the-art models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3008273026", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tmm/LiuGKQG21", "doi": "10.1109/tmm.2020.2974323"}}, "content": {"source": {"pdf_hash": "43cd4978c34bd7be3e5592b900edfa3431fe7780", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e5aa1aacc387116c58c425e13b82d0a13487d187", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/43cd4978c34bd7be3e5592b900edfa3431fe7780.txt", "contents": "\nA Multi-Stream Graph Convolutional Networks-Hidden Conditional Random Field Model for Skeleton-Based Action Recognition\n\n\nStudent Member, IEEEKai Liu \nMember, IEEELei Gao \nMember, IEEENaimul Mefraz Khan \nLin Qi \nFellow, IEEELing Guan \nA Multi-Stream Graph Convolutional Networks-Hidden Conditional Random Field Model for Skeleton-Based Action Recognition\n\nIEEE TRANSACTIONS ON MULTIMEDIA\n23202110.1109/TMM.2020.297432364Index Terms-GCNCRFskeletonhidden part stateaction recognition\nRecently, Graph Convolutional Network(GCN) methods for skeleton-based action recognition have achieved great success due to their ability to preserve structural information of the skeleton. However, these methods abandon the structural information in the classification stage by employing traditional fully-connected layers and softmax classifier, leading to sub-optimal performance. In this work, a novel Graph Convolutional Networks-Hidden conditional Random Field (GCN-HCRF) model is proposed to solve this problem. The proposed method combines GCN with HCRF to retain the human skeleton structure information even during the classification stage. Our model is trained end-to-end by utilizing the message passing from the belief propagation algorithm on the human structure graph. To further capture spatial and temporal information, we propose a multi-stream framework which takes the relative coordinate of the joints and bone direction as two static feature streams, and the temporal displacements between two consecutive frames as the dynamic feature stream. Experimental results on three challenging benchmarks (NTU RGB+D, N-UCLA, SYSU) show the superior performance of the proposed model over state-of-the-art models.\n\nconsiderable attention in multimedia community recently [26], [31], [37], [47], [49]. In this paper, we focus on the problem of skeleton-based action recognition.\n\nConvolutional deep neural network based methods like CNN or RNN usually represent the skeletal sequence as a vector of joint coordinates or a pseudo-image for applying the convolution operation, which can not capture the spatial structure of the human skeleton. Recently, graph convolutional networks, which generalize convolution from image to graph, have been successfully adopted in skeleton-based action recognition. GCN can preserve the connectivity among the skeleton joints, thus preserving the spatial structure. The spatial-temporal graph convolutional networks (ST-GCN) method in [35] is proposed to construct spatio-temporal skeleton graph, base on which multiple layers of spatio-temporal graph convolution operations are applied to extract the high-level feature maps for predicting the label by the softmax classifier. While the GCN-based method provided a significant increase in performance, we argue that a serious problem has been ignored. Although the graph structures and corresponding convolution operations have been designed well to preserve the natural structure of the human body, in the classification stage, the structural information is abandoned due to the utilization of fully connected layers and softmax classifier. This approach somewhat negates the effect of preserving structural information with GCN in the previous layers. Instead of using the traditional fully connected layer and softmax classifier, the proposed model attempts to retain the structure of the human body even in the classification stage. It is known that action recognition suffers from large intra-class variations as shown in Fig. 1. By keeping the structure of body Fig. 2. The framework of the proposed GCN-HCRF model: We utilize a GCN network to extract feature of each body joint. Then the extracted feature is sent to HCRF for classification. The HCRF not only assigns a hidden part state (orange circle) for each joint's feature, but also models the compatibility among these hidden part states. The whole model is trained in an end-to-end manner with back propagation. parts and modeling the compatibility among all joints' motion at classification stage, it can help recognize the same action performed in different ways. Furthermore, retaining the body structure in classification stage will in return guide the feature extraction of the GCN network, encouraging the network to extract more compatible joints' features for respective actions.\n\nInspired by hidden part models for action recognition in [43], we propose a novel GCN-HCRF model which combines GCN with Hidden Conditional Random Field to retain the human skeleton structure information during classification. An overview of the proposed method is shown in Fig. 2 on the next page. An HCRF model is combined with the GCN seamlessly for classification without abandoning the human structure. As can be seen in Fig. 2, the HCRF model's neuron connections can be structured in a way that preserves the spatial structure of the human skeleton. It is not possible to achieve such structure preservation with fully-connected layers, since every neuron in one layer is connected to every neuron in the next layer.\n\nHowever, merely putting the GCN and HCRF model together is not enough to take advantage of the benefit of spatial structure preservation. The model needs to be trained end-to-end so that the HCRF model can guide the GCN to extract features that are semantically more meaningful. To achieve this task, we utilize a message passing strategy on the acyclic graph of human joints inspired by belief propagation [11].\n\nAnother notable problem in action recognition is that although static feature like joint's coordinate and bone direction within each frame has been proven to be effective in [23], [26], [37], the dynamic feature also contains rich information for skeletal action recognition since it provides explicit motion information. In this paper, motivated by classic two-stream CNN for action recognition in [22], [42], which takes appearance information as static feature stream and takes optical flow as dynamic feature stream. We take the relative coordinate of the joints and bone direction as two static feature streams and take the temporal displacements between the joints in two consecutive frames as the dynamic feature stream. All three streams are incorporated utilizing score level fusion, providing improved performance over utilizing a single stream.\n\nTo verify the superiority of the proposed model, we conduct experiments on three challenging datasets with different scale: NTU RGB+D (56,000 samples), N-UCLA (1,494 samples) and SYSU (480 samples). Since the size of N-UCLA and SYSU are too small to train the deep graph convolution network adequately, an intuitive option is to employ the pre-trained parameters from the large-scale datasets NTU RGB+D as the initial parameters. However, using the pre-trained parameters directly is not feasible as the number of body joints in the datasets varies, leading to different graph structures. In this work, we propose two adaptation strategies to solve this problem, the interpolation strategy and the trim strategy. Experiment results show that the proposed method outperforms state-of-the-art methods on all three datasets.\n\nIn this paper, our main contributions are summarized below: 1) We propose a novel GCN-HCRF model which retains the spatial structure of human joints from beginning to end. Unlike current GCN-based model, we discard the fully connected layers, since they destroy the spatial structure. Instead, we utilize an HCRF model to take full advantage of the spatial compatibility information among all human joints' motion for classification. An end-to-end training is carried out by using the message passing strategy on the acyclic graph of human joints, which enables the HCRF model to guide GCN to extract feature that is semantically more meaningful, improving the overall classification performance. 2) To further boost the performance, a three-stream framework is proposed, which takes the relative coordinate of joints and bone direction as two static feature streams and takes the temporal displacements between the joints in two consecutive frames as the dynamic feature stream. 3) To train the proposed model on two small-sized datasets N-UCLA and SYSU with the pre-trained parameters from NTU RGB+D, we proposed two adaptation strategies to solve the difference of graph structure, boosting the performance on N-UCLA and SYSU substantially. 4) Experimental results demonstrate the proposed model outperforms state-of-the-art approaches on three standard datasets.\n\n\nII. RELATED WORK\n\nIn this section, we briefly review the previous works on skeleton-based action recognition from two important components: neural networks based methods, CRF based methods.\n\n\nA. Neural Networks Based Methods\n\nThe neural network architectures relevant to this work include convolutional neural networks (CNNs), recurrent neural networks (RNNs) and graph convolution networks (GCNs). In [5], a CNN-based model transforms a skeleton sequence to a pseudo-image by treating the joint coordinate (x, y, z) as the R, G, B channels of a pixel. In [25], skeletons are visualized as a series of color images, in which visual and motion enhancement methods are applied to enhance their local patterns. In [9], a synchronous local and non-local approach is introduced to simultaneously capture the details and semantics in the spatio-temporal domain. In [37], to exploring the performance of different geometric features on action recognition, eight geometric features are presented for a three-layer LSTM. Moreover, a multi-stream LSTM architecture is proposed to fuse the classification results of different geometric feature streams. In [26], [27], [49], in order to achieve view invariant action recognition, the view adaptive neural networks is adopted to find the most suitable observation viewpoints. In [35], spatio-temporal GCN is proposed to apply graph convolution on the natural structure graph of the skeleton action sequences. In [23], non-local graph convolutional block is introduced to adaptively learn the graph structure for different layers and samples.\n\n\nB. CRF Based Methods\n\nSequence labeling models, like CRF [12], are suitable for modeling and analyzing human actions, because such Markov chain models are able to capture the structural dependencies among the outputs. Many extensions of CRF are applied to a wide range of fields. In [43], a hidden part-based CRF approach is proposed to combine both large-scale global features and local patch features for action recognition based on max-margin hidden conditional random fields (MMHCRF). In [39], by observing that each class has distinct sub-structures, a HCRF model is introduced to capture the sub-structures for gesture recognition. In [32], a mid-level video representation approach based on CRF is proposed for action recognition by viewing part-cluster assignments as hidden variables. In [33], a composite latent structure model is introduced to recognize skeleton sequences by representing each atomic action as a composite latent state. In [18], a nonparametric feature matching based CRF model is proposed for gesture recognition, in which the gesture classes are estimated for all frames to localize the outputs automatically in the unconstrained input video.\n\nJoint model of CNNs and CRF is helpful in many applications. The fully connected CRF [19] performs well in semantic segmentation and human pose estimation, and it was trained jointly in [48] by unrolling iterations of the inference method. In [40], a spatio-temporal And-Or graph is proposed to reconfigure models during learning and inference. In [20], an extended 3D CNN model is proposed by incorporating structure alternatives to make it a structured deep architecture. In [38], a deep sequence model is introduced by extending the CRF models with CNN for higher level feature learning. In [13], a hybrid framework combines Convolutional Neural Network (CNN) and Latent Dynamic Conditional Random Field (LDCRF) to segment and recognize continuous actions simultaneously. In [8], a fully-connected temporal CRF model is proposed for reasoning over various aspects of activities that include objects, actions, and intentions, where the potentials are predicted by a deep network.\n\n\nIII. GCN-HCRF MODEL\n\nIn this paper, the main aim is to retain the spatial structure of human joints in the classification. We view the spatial structure of human joints as a random field based in which a novel GCN-HCRF model is proposed. The proposed model can take full advantage of compatible information among all joints' motions. Therefore, it guides the GCN network to extract more semantically meaningful feature from all joints.\n\n\nA. Overview of The Proposed Framework\n\nAs shown in Fig. 2, the proposed model takes the sequences of body joins in the form of 3D coordinates as input. In the first part, a GCN network is applied to mine the feature and generate higher-level feature maps in the form of human structure graph. After that, a HCRF model is combined with the GCN seamlessly for classification without abandoning the human structure. The HCRF not only assigns a hidden part state for each joint's feature, but also models the compatibility among these hidden part states. The whole model is trained in an end-to-end manner with back propagation. The specific components of the GCN-HCRF model are explained in detail below.\n\n\nB. Feature Extraction With Graph Convolutional Network\n\nSkeleton Graph Construction: Suppose a skeletal sequence includes T frames and each frame has N joints. Then we utilize the spatio-temporal graph convolutional network to extract feature of each joint. This GCN network is an undirected spatio-temporal graph G = (V, E) which is constructed on the natural spatio-temporal structure of a skeletal sequence, where V stands for the set of all the joints in a skeleton sequence, and E is the edge set that includes the intra-body edges and the inter-frame edges. The intra-body edges are natural connections of human skeleton in each frame. The inter-frame edges are connections of the same joints between adjacent frames. The coordinate vectors of joints are used as this graph network's inputs. To exploit the feature with spatio-temporal information, the ST-GCN network [35] is constructed to perform convolution operation in two dimensions: spatial dimension and temporal dimension.\n\nSpatial Dimension: The spatial graph convolution operation within one single frame is formulated as [35]:\nf out (v i ) = v j \u2208B(v i ) 1 Z i (v j ) f in (v j )w(l i (v j ))(1)\nwhere v i is the i-th joint within a single skeleton frame, f in is the input feature map and f out is the output feature map. B(v i ) represents the 1-distance neighbor set of the given joint v i within a single frame, and v j is a joint from the set B(v i ). According to the spatial configuration partitioning in [35], B(v i ) is divided into 3 subsets: the root joint v i itself, the centripetal group and the centrifugal group. l i (v j ) represent a mapping function which is used for mapping each joint v j to the corresponding subset. Z i (v j ) is a normalizing term which equals the cardinality of the corresponding subset. w is the weight function which provides a weight vector to compute the inner product.\n\nTemporal Dimension: Since the number of neighbors for each joint on temporal dimension is fixed, it enables us to conduct the graph convolution operation with a simple strategy that is similar to the classic CNN convolution operation on regular images. Concretely, we conduct a \u03c4 \u00d7 1 convolution operation on the output feature map in Eq. (1), where \u03c4 is the temporal range called the temporal kernel size.\n\nThe spatial temporal graph convolution is implemented by representing Eq. (1) in the matrix form as follows [35]:\nF out = j \u039b \u2212 1 2 j (A j \u2297 M)\u039b \u2212 1 2 j F in W j (2)\nwhere F in represent the input feature map as a matrix of (N, T, C in ) dimensions, N denotes the number of joints for each frame, T denotes the number of frames for each action sequence and C in denotes the number of input channels. F out represent the output feature map as a matrix of (N, T, C out ) dimensions, where C out denotes the number of output channels. For spatial configuration partitioning, j \u2208 {0, 1, 2}. W j is the C in \u00d7 C out \u00d7 1 \u00d7 1 weight matrix for 1-distance neighbors spatial temporal graph convolution operation. A j denotes a N \u00d7 N adjacency matrix. A 0 denotes an identity matrix representing self-connections of joint itself. A 1 denotes the connections of centripetal group and A 2 denotes the connections of centrifugal group. M is a learnable weight matrix for learning the importance of all connections. The sign of \u2297 denotes element-wise product between two matrices. \u039b j is a diagonal matrix, whose element \u039b ii j = k A ik j + \u03b1, where A ik j is the element of matrix A j , and \u03b1 is set to 0.001 to avoid empty rows in A j . Now, we have a well-defined spatial and temporal convolution operation on the whole skeleton sequence, by which we extract feature with spatio-temporal information on every human joint. Different from [35] where the extracted feature is used directly for classification with fully connected layer, we send the extracted feature to HCRF for classification. Specifically, as shown in Fig. 3, the output feature map from the last layer of GCN is a matrix of (N, T, C out ) dimensions. After conducting the global average pooling on T and on [N, T ] respectively, we obtain a local feature matrix L with (N, C out ) dimensions for joints' motion and a global feature vector x 0 with C out dimensions for the whole body's motion.\n\n\nC. Hidden Conditional Random Field for Action Recognition\n\nIn this subsection, we describe how to model the HCRF based on the extracted features from above GCN. As described above, we obtained the local feature L that is a matrix with (N, C out ) dimensions and the global feature x 0 that is a vector with C out dimensions. Let x be the feature of the Fig. 3. Illustration of model implementing: Each blue rectangle represents a matrix or a vecor, and the orange rectangle represents the HCRF model. The output from the last layer of GCN is a matrix of (N, T, C out ) dimensions. After conducting the global average pooling on T and on [N, T ], we obtain a local feature matrix L of (N, C out ) dimensions and a global feature vector x 0 of (C out ) dimensions. Then they are used as the unary potential \u03c6(\u00b7) and the global potential \u03d1(\u00b7) of HCRF after conducting 1D convolution operation with the kernel size of 1. More details about the potential of HCRF are described in Section III-C.\n\ngiven skeletal sequence, which includes the local feature L and the global feature x 0 , then the feature x is represented as\nx = (x 0 , x 1 , x 2 , . . . , x i , . . . , x N ) T where x 0\nis the global feature vector of the whole body's motion, and x i (i = 1 . . . N) denotes the local feature vector of the i-th joint's motion, which corresponds to the i-th row of the local feature matrix L. Let y be the corresponding class label of the given skeletal sequence, and Y be the finite class label set, y \u2208 Y , the total number of action classes is |Y |. For action recognition, our task is to predict the class label y for the given feature x. In order to model the compatibility among all joints' motion, we introduce a vector of hidden part states h = (h 1 , h 2 , . . . , h i , . . . , h N ) T , where h i is a hidden part state assigned to x i , for i = 1, 2, . . . N. Each h i takes value from a finite hidden part state set H which cannot be observed in the training set but will be learned as the hidden variables of the model during training process, and the size of set H is denoted as |H|. In addition to assigning a hidden part states h i to x i , assume that there exist certain constraints between some pairs of (h j , h k ), where j and k are any two joints that are adjacent in human body. For each given action, the relative movement of the joints with respect to each other imposes these constraints. Taking the \"wear shoe\" action for example, as shown in Fig. 1, the hand joint and elbow joint might have the constraint that they both have to move down to the foot so that the hidden part states h j and h k at that two joints should be the same. Simultaneously, both the foot joint and the knee joint move up to the hand or both stay still by which they tend to have the same hidden part state.\n\nAccording to the theory of random fields in [1], given the feature x, its corresponding hidden part states h, and the class label y, a hidden conditional random field has the exponential form as follows:\nP (y, h|x; \u03b8) = exp(\u03a6(y, h, x; \u03b8)) y \u2208Y h\u2208H N exp(\u03a6(y , h, x; \u03b8))(3)\nwhere \u03b8 is the model parameter, and \u03a6(y, h, x; \u03b8) refers to potential function depending on the feature x, the hidden part states h, and the class label y. The denominator part is the partition function for normalization which sums over all possible hidden \nP (y|x; \u03b8) = h\u2208H N P (y, h|x; \u03b8) = h\u2208H N exp(\u03a6(y, h, x; \u03b8)) y \u2208Y h\u2208H N exp(\u03a6(y , h, x; \u03b8)) (4) \u03a6(y, h, x; \u03b8)\nis defined as the summation of unary potential, pairwise potential and global potential in the following form:\n\u03a6(y, h, x; \u03b8) = j\u2208\u03bd \u03c6(x j , h j ; \u03c9) + j\u2208\u03bd \u03d5(y, h j ; \u03b4) + (j,k)\u2208 \u03c8(y, h j , h k ; \u03be) + \u03d1(y, x 0 ; ) (5)\nwhere \u03c6(\u00b7) is unary potential with parameter \u03c9 that measures the likelihood of the local feature x j is assigned as the hidden part state h j . \u03d5(\u00b7) is unary potential with parameter \u03b4 that measures the compatibility between the hidden part state h j and the action label y. \u03c8(\u00b7) is pairwise potential with parameter \u03be that is meant for capture the compatibility between a pair of (h j , h k ) and the action label y. \u03d1(\u00b7) is global potential with parameter that measures the likelihood of the global feature x 0 is assigned as the action label y. \u03bd is the joint set of human body and is the edge set of human body. The HCRF structure for skeletal action is shown in Fig. 4. The details of these potential functions are described below.\n\nUnary potential \u03c6(x j , h j ; \u03c9) measures the likelihood of the local feature x j is assigned as the hidden part state h j , whose parameter \u03c9 is learned by above GCN network. In this work, the likelihood is obtained by applying common one-dimensional convolution operation with 1 * 1 kernel size on the local feature matrix L with (N, C out ) dimensions, by which a probability matrix M with (N, |H|) dimensions is drawn as shown in Fig. 3. Each element M jl of the probability matrix M represents the probability that x j (j = 1 . . . N) is assigned as the l-th (l = 1 . . . |H|) state of the hidden part state set H, since h j can take any value from the hidden part state set H. The parameter \u03c9 includes the parameter for above GCN network and the parameter for the one-dimensional convolution operation on the local feature matrix L, which will be learned during training process.\n\nUnary potential \u03d5(y, h j ; \u03b4) measures the compatibility between class label y and hidden part state h j . To compute this potential, we parametrize it as:\n\u03d5(y, h j ; \u03b4) = a\u2208Y b\u2208H \u03b4 a,b \u00b7 1(y = a) \u00b7 1(h j = b) (6)\nwhere the parameter \u03b4 is a matrix of (|Y |, |H|) dimensions, whose element \u03b4 a,b represents how likely an action with class label y = a contains a joint with hidden part state h j = b, and it will be learned during training process. 1(\u00b7) is a indicator function which take the value 1 when its argument evaluates to true and 0 otherwise.\n\nPairwise potential \u03c8(y, h j , h k ; \u03be) measures the compatibility between class label y and a pair of hidden part states (h j , h k ), where (j, k) corresponds to an edge in the human body graph as shown in Fig. 4. To compute this potential, we parametrize it as:\n\u03c8(y, h j , h k ; \u03be) = a\u2208Y b\u2208H c\u2208H \u03be a,b,c \u00b7 1(y = a) \u00b7 1(h j = b) \u00b7 1(h k = c) (7)\nwhere the parameter \u03be is a matrix of (|Y |, |H|, |H|) dimensions, whose element \u03be a,b,c means how likely an action with class label y = a contains a pair of joints with hidden part states h j = b and h k = c, and it will be learned during training process.\n\nIn addition to the local feature, It's well known that the global feature is also very important for recognizing actions. So we use the global potential \u03d1(y, x 0 ; ) to measure the compatibility of class label y and the global feature vector x 0 of the whole action. As shown in Fig. 4, the global feature vector x 0 with C out dimensions is obtained by conducting global average pooling on the output of the GCN network on N and T . Then we apply common one-dimensional convolution operation with 1 * 1 kernel size on the global feature vector x 0 . After that we draw a probability vector of |Y | dimensions whose i-th item represent the probability that x 0 is assigned as the i-th class. The parameter includes the parameter for above GCN network and the parameter for the one-dimensional convolution operation on the global feature vector x 0 , which will be learned during training process.\n\nNote that if we only consider the global potential \u03d1(y, x 0 ; ) and remove all the others, the proposed GCN-HCRF model will degenerate into the GCN with a fully connected layer for classification. If we don't consider the global potential \u03d1(y, x 0 ; ), the proposed model can still work well. But the performance of the proposed model will increase when taking the global potential \u03d1(y, x 0 ; ) into account, as shown in the results in Section IV-B1.\n\n\nD. End-to-End Training\n\nOne major goal of this work is the end-to-end training of the whole model so that the GCN can extract semantically meaningful features by learning from the HCRF. Assuming that the training dataset includes S labeled action sequences. Following the minimum negative conditional log-likelihood rule, the loss function is shown in Eq. (8):\nL(\u03b8) = \u2212 S s=1 logP (y (s) |x (s) ; \u03b8)(8)\nwhere y (s) is the label of the s-th action sequence sample, x (s) is the feature of the s-th action sequence sample, and S is the total number of samples. The definition of P (\u00b7) is shown in Eq. (4).\n\nTo forward compute the loss, the key challenge is to calculate the summation over all possible assignments of hidden part state for body joints in the numerator and the denominator of Eq. (4), which is intractable by brute force as all the possible assignments of hidden part state are exponential. For example, if the human body includes N joints, there are |H| N possible assignments. Fortunately, since the human joints structure is an acyclic graph, we can calculate the summation efficiently through belief propagation on this acyclic graph. According to the belief propagation algorithm for a tree structure [11], the summation can be computed efficiently by message passing to root joint from all the other joints. On the human structure graph, our message passing route and passing rule are shown in Fig. 5.\n\nIn Figure 5, m ji (h i ) is a message from joint j to joint i about how likely joint i is in state h i . m ji (h i ) takes all upstream messages to joint i, which is updated by the following rule:\nm ji (h i ) \u2190 h j \u2208H \u03bc(x j , h j , y)\u03b7(h i , h j , y) b\u2208B(j)\\i m bj (h j )(9)\nwhere B(j) \\ i means the neighboring joint set of joint j except joint i. m bj (h j ) takes all neighbor joints' message going to joint j. \u03bc(x j , h j , y) is the summation of both unary potential and the global potential at joint j, which is formulated in Eq. (10).\n\nHere the global potential \u03d1(y, x 0 ; ) is divided by N so that the influence of global potential is evenly distributed among N joints. \u03b7(h i , h j , y) is the pair potential between joint i and joint j, which is formulated in Eq. (11):\n\u03bc(x j , h j , y) = exp \u03c6(x j , h j ; \u03c9) + \u03d5(y, h j ; \u03b4) + 1 N \u03d1(y, x 0 ; )(10)\u03b7(h i , h j , y) = exp(\u03c8(y, h i , h j ; \u03be))(11)\nUsing Eq. (10) and Eq. (11), we compute the exp (\u03a6(y, h, x; \u03b8)) part in Eq. (4) as follows:\nexp(\u03a6(y, h, x; \u03b8)) = exp \u239b \u239d j\u2208\u03bd \u03c6(x j , h j ; \u03c9) + j\u2208\u03bd \u03d5(y, h j ; \u03b4) + (j,k)\u2208 \u03c8(y, h j , h k ; \u03be) + \u03d1(y, x 0 ; ) \u239e \u23a0 = exp \u239b \u239d j\u2208\u03bd \u03c6(x j , h j ; \u03c9) + j\u2208\u03bd \u03d5(y, h j ; \u03b4) + j\u2208\u03bd 1 N \u03d1(y, x 0 ; ) + (j,k)\u2208 \u03c8(y, h j , h k ; \u03be) \u239e \u23a0 = exp \u239b \u239d j\u2208\u03bd \u03c6(x j , h j ; \u03c9) + \u03d5(y, h j ; \u03b4) + 1 N \u03d1(y, x 0 ; ) + (j,k)\u2208 \u03c8(y, h j , h k ; \u03be) \u239e \u23a0 = exp \u239b \u239d j\u2208\u03bd \u03c6(x j , h j ; \u03c9) + \u03d5(y, h j ; \u03b4) + 1 N \u03d1(y, x 0 ; ) \u239e \u23a0 \u00b7 exp \u239b \u239d (j,k)\u2208 \u03c8(y, h j , h k ; \u03be) \u239e \u23a0 = j\u2208\u03bd \u03bc(x j , h j , y) (j,k)\u2208 \u03b7(h j , h k , y)(12)\nThen we compute the h\u2208H N exp(\u03a6(y, h, x; \u03b8)) part in Eq. (4) with Eq. (12) and Eq. (9). To make the computation easier to understand, we follow the way in [11] to explain the computation by giving an example. Fig. 6 shows a graph with four joints which is part of the human body in   y, h, x; \u03b8)) is computed as follows: \nh\u2208H N exp(\u03a6(y, h, x; \u03b8)) = h 1 \u2208H h\\h 1 exp(\u03a6(y, h, x; \u03b8)) = h 1 \u2208H h 2 \u2208H h\\h 1 ,h 2 exp(\u03a6(y, h, x; \u03b8)) = h 1 \u2208H h 2 \u2208H h 3 \u2208H h\\h 1 ,h 2 ,h 3 exp(\u03a6(y, h, x; \u03b8)) = h 1 \u2208H h 2 \u2208H h 3 \u2208H h 4 \u2208H j\u2208\u03bd \u03bc(x j , h j , y) (j,k)\u2208 \u03b7(h j , h k , y) = h 1 \u2208H h 2 \u2208H h 3 \u2208H h 4 \u2208H \u03bc(x 1 , h 1 , y)\u03bc(x 2 , h 2 , y)\u03bc(x 3 , h 3 , y) \u00d7 \u03bc(x 4 , h 4 , y)\u03b7(h 1 , h 2 , y)\u03b7(h 2 , h 3 , y)\u03b7(h 1 , h 4 ,h\u2208H N exp(\u03a6(y, h, x; \u03b8)) = h 1 \u2208H h 2 \u2208H h 3 \u2208H \u03bc(x 1 , h 1 , y)\u03bc(x 2 , h 2 , y)\u03bc(x 3 , h 3 , y) \u00d7 \u03b7(h 1 , h 2 , y)\u03b7(h 2 , h 3 , y) h 4 \u2208H \u03bc(x 4 , h 4 , y)\u03b7(h 1 , h 4 , y) = h 1 \u2208H h 2 \u2208H h 3 \u2208H \u03bc(x 1 , h 1 , y)\u03bc(x 2 , h 2 , y)\u03bc(x 3 , h 3 , y) \u00d7 \u03b7(h 1 , h 2 , y)\u03b7(h 2 , h 3 , y)m 41 (h 1 ) = h 1 \u2208H h 2 \u2208H \u03bc(x 1 , h 1 , y)\u03bc(x 2 , h 2 , y)\u03b7(h 1 , h 2 , y)m 41 (h 1 ) \u00d7 h 3 \u2208H \u03bc(x 3 , h 3 , y)\u03b7(h 2 , h 3 , y) = h 1 \u2208H h 2 \u2208H \u03bc(x 1 , h 1 , y)\u03bc(x 2 , h 2 , y)\u03b7(h 1 , h 2 , y) \u00d7 m 41 (h 1 )m 32 (h 2 ) = h 1 \u2208H \u03bc(x 1 , h 1 , y)m 41 (h 1 ) h 2 \u2208H \u03bc(x 2 , h 2 , y)\u03b7(h 1 , h 2 , y) \u00d7 m 32 (h 2 ) = h 1 \u2208H \u03bc(x 1 , h 1 , y)m 41 (h 1 )m 21 (h 1 ) = h 1 \u2208H \u03bc(x 1 , h 1 , y) b\u2208B(1) m b1 (h 1 )(14)\nwhere B(1) is the neighbouring joint set of joint 1. For this example, the neighbouring joints of joint 1 are joint 2 and joint 4. Based on the theory of message passing in [11], it's easy to compute the h\u2208H N exp (\u03a6(y, h, x; \u03b8)) for the whole human body in Fig. 5 as follows:\nh\u2208H N exp(\u03a6(y, h, x; \u03b8)) = h i \u2208H h\\h i j\u2208\u03bd \u03bc(x j , h j , y) (j,k)\u2208 \u03b7(h j , h k , y) = h i \u2208H \u03bc(x i , h i , y) b\u2208B(i) m bi (h i )(15)\nwhere h \\ h i indicates all hidden part states except h i . B(i) is the neighbouring joint set of joint i. x i is the local feature of joint i which is included in the feature x. Using Eq. (15), we can rewrite Eq. (4) as: (16), the loss in Eq. (8) will be obtained. Then the back propagation is applied to the loss function to perform end-to-end training.\nP (y|x; \u03b8) = h i \u2208H \u03bc(x i , h i , y) b\u2208B(i) m bi (h i ) y \u2208Y h i \u2208H \u03bc(x i , h i , y ) b\u2208B(i) m bi (h i ) (16) Once we computed P (y|x; \u03b8) with Eq.\n\nE. Three Stream Fusion\n\nMotivated by classic two-stream CNN for action recognition in [22], where one stream takes static feature (appearance) from still images and the other stream takes dynamic feature (optical flow) from consecutive frames, we use static features and dynamic features for skeletal action recognition. According to [21], [34], [37], both relative coordinate and bone direction information of each frame are powerful static features. Temporal displacements of joints in [21] have been proven to be effective dynamic features for action recognition. In this paper, all three streams are combined to improve the skeleton action recognition performance namely, relative coordinate stream, bone stream and temporal displacements stream.\n\nRelative Coordinates Stream: Relative coordinates provide translation invariant features as explained in [21], which performs much better than using absolute coordinates of joints. In this work, the relative coordinate of the joints are adapted with respect to hip (left hip and right hip) and shoulder (left shoulder and right shoulder), each joint is represented as a 12D vector.\n\nBone Stream: It is known that bone is the physical connections of body joints. Each bone is represented as a 3D vector that represents the direction information of itself. The bone direction vector is calculated by the difference between the coordinates of two adjacent body joints. Given an acyclic graph with N body joints, there exist N \u2212 1 bones. Each body joint is assigned with a unique bone direction vector except for the hip joint, which will be filled with 0.\n\nTemporal Displacements Stream: Temporal displacements provide information about the amount of motion happening between adjacent frames. For each joint, it is represented as a 3D vector which is calculated by the difference of each joint between two consecutive frames. Displacement information provides explicit motion information to the model as a strong feature in the learning process.\n\nAll the three streams are trained with the same GCN-HCRF model separately. For the final prediction, we take the average fusion strategy to combine the score from the three streams.\n\n\nIV. EXPERIMENT RESULTS AND ANALYSIS\n\nTo show the effectiveness of the proposed model, we conduct experiments on three benchmark datasets, the NTU RGB+D dataset, the Northwestern-UCLA dataset and the SYSU dataset.\n\nIn the following sections, we first introduce the datasets and experiment settings in Section IV-A. Then we perform the ablation studies in Section IV-B to analyze the contributions of the proposed model components to the recognition performance. Section IV-C presents the performance comparisons with state-of-the-art approaches on the three datasets respectively.\n\n\nA. Datasets and Experimental Settings\n\nNTU RGB+D Dataset: Currently, the NTU RGB+D is the largest and most widely used multimodal dataset for human action recognition. The dataset provides 3D skeleton data with 3D coordinates of 25 joints detected by the Kinect V2 depth sensors. There are 56,000 clips totally in 60 classes which are performed by 40 subjects, and each action is captured by 3 cameras from different viewpoints. This dataset recommends two benchmarks: 1) Cross-Subject (CS): the dataset is divided into training set (40,320 clips) and validation set (16,560 clips), where the actors in two subsets are different. 2).Cross-View (CV): the training set (37,920 clips) are captured by camera 2 and 3 from different viewpoints, and the validation set (18,960 clips) are captured by camera 1. It is a challenging dataset for action recognition because of the large amount of samples, various subjects, and the difference in camera views.\n\nNorthwestern-UCLA Dataset (N-UCLA): This dataset is a small-sized multimodal dataset for human action recognition, which contains 1494 sequences covering 10 action classes. It provides 3D skeleton data with 3D coordinates of 20 joints detected by the Kinect V1 depth sensors. Each action is captured one to six times by ten subjects with three cameras from different viewpoints. In this work, we follow the standard protocol proposed by [45] to report accuracy on three settings, which chose every two of the views for training and the other for testing.\n\n\nSYSU 3D Human-Object Interaction Set (SYSU):\n\nThis dataset is a small-sized multimodal dataset for Human-Object Interaction action recognition, which contains 480 sequences covering 12 action classes. It provides 3D skeleton data with 3D coordinates of 20 joints detected by the Kinect V1 depth sensors. 40 participants were asked to perform 12 different activities freely. For each action, each subject manipulates one of the six different objects: phone, wallet, bag, chair, mop and besom. These action samples have different durations, ranging from 58 frames to 638 frames. This dataset is challenging for high similarity among actions. In this work, we follow the standard protocol proposed by [14] to report the averaged accuracy Experimental Settings: For NTU RGB+D dataset, the architecture of ST-GCN in [35] is employed with 9 layers of spatial temporal graph convolution operators. For each layer, the spatial temporal graph convolution operator is followed by a Dropout with a rate of 0.5, a Batch Normalization and a ReLU activation function. The number of output channels for each layer is 64, 64, 64, 128, 128, 128, 256, 256 and 256 respectively, The strides of the 4-th and the 7-th temporal convolution layers are set to 2 as pooling layer. Instead of using the SoftMax classifier, the output of the last layer is sent to HCRF for classification as shown in Fig. 3, and the size of hidden states set is set to 100. Then we conduct end to end training with Stochastic Gradient Descent (SGD). The global parameters such as Nesterov momentum, weight decay, initial learning rate are set to 0.9, 0.0001 and 0.1 respectively. For the N-UCLA dataset and the SYSU dataset, the only different setting in GCN part is the number of output channels for each layer. For HCRF part, the size of hidden states set is set to 20 for both N-UCLA dataset and SYSU dataset (further explanation regarding the size of the hidden state set can be found in Section IV-B3). Given that these datasets have a much smaller sample size than the NTU RGB+D dataset, we cut the number of output channels for each layer in half which are 32, 32, 32, 32, 64, 64, 64, 128, 128 and 128 respectively. Even after doing so, the sample sizes of N-UCLA and SYSU are too small to adequately train the network. CNN or RNN based methods [25] that consider the skeleton sequence as images can take advantage of pre-trained model on large-scale image datasets such as ImageNet when training on small-sized datasets. Motivated by that, in our work we first conduct pre-training on the halved network with NTU RGB+D dataset. Then we use the learned parameters as the initial parameters for N-UCLA dataset and SYSU dataset, with initial learning rate 0.01. However, there are 25 joints captured in the NTU RGB+D dataset while only 20 joints captured in the N-UCLA and SYSU Dataset as shown in Fig. 7, which lead to different graph structure. So the pre-training parameters from NTU RGB+D dataset cannot be used for training on N-UCLA dataset and SYSU dataset directly. We propose two adaptation strategies to solve the problem in Section IV-B5. \n\n\nB. Ablation Study 1) Effectiveness of The Proposed GCN-HCRF Model:\n\nWe first examine the effectiveness of the proposed GCN-HCRF model on the NTU RGB+D dataset. We compare the performance of the proposed GCN-HCRF model with ST-GCN model [35]. The results in Table I show that GCN-HCRF outperforms ST-GCN with same input (the absolute coordinate of joints) by 2.8% and 3.4% on the CS setting and the CV setting respectively. It also shows that the performance of the proposed model is improved by 0.6% and 0.5% on CS and CV settings respectively when taking the global potential into account. The confusion matrices of our results on the CS setting and the CV setting are shown in Fig. 8 and Fig. 9, respectively.\n\n2) Effectiveness of End-to-End Training: To demonstrate the effectiveness of end-to-end training, we performed a separate training, whose performance is compared with that of endto-end training in Table II. For the separate training manner, we first train the GCN separately, then use the output features of GCN to train the HCRF separately. As shown in Table II, compared with the separate training manner, the performance of end-to-end training is improved by 2.2% and 3.0% on CS and CV settings respectively. The comparison results demonstrated that the end-to-end training manner enables the  HCRF part to guide GCN to extract more semantically meaningful feature from all joints, improving the overall classification performance.\n\n\n3) Influence of The Size of Hidden State Set:\n\nIn this subsection, we examine the influence of the size of hidden state set on the CS setting of NTU RGB+D dataset. According to [39], [43], the size of hidden states set |H|, is typically set to about twice the number of classes. For NTU RGB+D dataset, there are 60 classes. We learn the GCN-HCRF model four times on the CS setting of NTU RGB+D dataset with four different sizes of possible hidden states (|H| = 90, 100, 110, 120) given in Table III. In Table III, it shows that the proposed model achieves optimal accuracy when the sizes of possible hidden states is set to 100. So for the rest of the reported results, as described in experimental settings, we set the size of hidden states set to 100 for both CS and CV setting of NTU RGB+D dataset. As for N-UCLA and SYSU datasets, the size of hidden states set is set to 20 for both of them as they only have 10 and 12 action classes respectively. \n\n\n4) Three-Stream Network:\n\nAnother important improvement is the introduction of three-stream framework. In this subsection, we examine the effectiveness of the proposed three-stream network on the CS setting of NTU RGB+D dataset. The performance of using the three streams, relative coordinate stream (Rs), bone stream (Bs), and temporal displacements stream (Ts) for the proposed model is shown in Table IV, confirming the importance of each stream in skeletal action recognition. Then we combine their scores pair-wisely and all three together, as described in Section III-E, to obtain fusion results, which are also presented in Table IV. The statistics clearly demonstrated the power of fusion, especially when all three streams are fused (3 s fusion), indicating the existence of complementary information among the three steams.\n\n\n5) Effectiveness of Different Adaptation\n\nStrategies on N-UCLA: As shown in Fig. 7, the N-UCLA dataset and the SYSU dataset were captured by Kinect V1, which returns 20 body joints. However, there are 25 skeletal joints captured in the NTU RGB+D dataset by Kinect V2. So it is not straightforward to use the pre-trained parameters from NTU RGB+D dataset as the initial parameters for N-UCLA and SYSU dataset. To solve this issue, we conduct two different adaptation strategies. Since the N-UCLA dataset and the SYSU dataset both report 20 joints, we only use the N-UCLA dataset to explain the process. For the interpolation strategy, we interpolate 5 extra joint 20, 21, 22, 23, 24 to the skeleton of the N-UCLA dataset. The joint 20 is set to the middle of joint 4 and joint 8, joint 21 and 22 is set to the same position as joint 7, and joint 23, 24 is set to the same position as joint 11. After the interpolation, it has the same number of joints as NTU RGB+D dataset during training. Then the proposed model is trained on the N-UCLA dataset with the pre-trained parameters from NTU RGB+D dataset. For the trim strategy, we remove joint 20, 21, 22, 23, 24 from all samples in NTU RGB+D dataset. Then the proposed model is trained with the trimmed NTU RGB+D dataset, and the learned parameters are employed as the initial parameter for N-UCLA dataset. According to Table IV, since the B-stream is the strongest stream, performance of the two strategies on the B-stream is tabulated in Table V. In Table V, it shows the trim strategy is better than the interpolation strategy. For the rest of the reported results, we utilized the trim strategy for both N-UCLA and SYSU dataset.  \n\n\nC. Comparisons to Other State-of-the-Art Approaches\n\nIn this section, we compare the proposed three streams GCN-HCRF model with state-of-the-art approaches on the NTU RGB+D, N-UCLA and SYSU, separately.\n\n\n1) NTU RGB+D Dataset:\n\nWe follow the standard CS and CV protocols introduced in [2] to evaluate the proposed method. Specifically, we compare the proposed model with other models for skeletal action recognition, including one hand-crafted feature based method [33], several CNN based methods [3], [5], [9], [25], [30], [38], several RNN/LSTM based methods [6], [7], [27], [28], [37], [44], [49], and several GCN based methods [21], [23], [35], [41]. The results in Table VI show that the proposed model achieves the best performance, outperforming the best GCN based method in [23] by 1.5% and 0.4% on CS and CV settings, respectively. In addition, the proposed model outperforms state-of-the-art approach [28] without using data-augmentation.\n\n2) N-UCLA Dataset: There are three views in this dataset. According to the standard protocol proposed in [45], there are  VII  COMPARISON WITH STATE-OF-THE-ART METHODS ON THE N-UCLA DATASET three settings, each using two of the views for training and the other for testing. In [28], [45], experiments were conducted on all the three settings, while, in [7], [10], [15], [24], [25], [27], [29], [30], [33], only the first two views were used for training and the other for testing. We compare our results with all those works shown in Table VII, where V3 means that training on the first two views and testing on the third, V2 means that training on the first and the third views and testing on the second, and V1 means that training on the last two views and testing on the first. The result shows that the proposed model achieves the best performance. Notably, without data-augmentation, the proposed model significantly outperforms state-of-the-art methods in [28] by 8.0% and 3.2% on setting V1 and average, respectively.\n\n3) SYSU Dataset: We follow the standard protocols proposed by [14] to evaluate the performance. For the Cross Subject (CS) setting, half of the subjects are used for training and the others for testing. For the Same Subject (SS) setting, half of the videos of each subject are used for training and the others for testing. According to the standard protocols proposed by [14], for each setting, the averaged results from 30-fold cross validation are required to report, which shown in Table VIII. The proposed model outperforms the other methods, particularly, the performance is 6.5% and 6.2% higher than the best reported in [30] for CS setting and SS setting, respectively.\n\n\nV. CONCLUSION\n\nIn this work, we propose a GCN-HCRF model for skeletonbased action recognition, which can retain the spatial structure of human joints from beginning to end. It takes full advantage of the spatial compatibility information among all joints' motion. Then we carried out end-to-end training on the whole model so that the GCN part of the proposed model extracts semantically meaningful features by the guidance of the HCRF part. Furthermore, we proposed a three-stream framework to further  VIII  COMPARISON WITH STATE-OF-THE-ART METHODS ON THE SYSU DATASET boost the performance, which employs the relative coordinate of the joints and bone direction as two static feature streams and the temporal displacements between the joints in two consecutive frames are adopted as the dynamic feature stream. To train the proposed model on two small-sized datasets N-UCLA and SYSU with the pre-trained parameters from NTU RGB+D, we proposed two adaptation strategies to solve the difference of graph structure, the interpolation strategy and the trim strategy. The proposed model is evaluated on three challenging standard action recognition dataset, achieving state-of-the-art results for all of them. Naimul Mefraz Khan (Member, IEEE) received the Ph.D. degree in electrical and computer engineering from Ryerson University, Toronto, ON, Canada. He is currently an Assistant Professor of Electrical and Computer Engineering with Ryerson University, where he co-directs the Ryerson Multimedia Research Laboratory. His research focuses on creating user-centric intelligent systems through the combination of novel machine learning and human-computer interaction mechanisms. He is the recipient of the best paper award at the IEEE International Symposium on Multimedia, the OCE TalentEdge Postdoctoral Fellowship, and the Ontario Graduate Scholarship. He is a member of the ACM. \n\n\nLin\n\nFig. 1 .\n1Large intra-class variations exist as the same action can be performed in different ways. (samples taken from the action \"wear a shoe\" in NTU RGB+D dataset).\n\nFig. 4 .\n4HCRF structure for skeletal action: Unary potential \u03c6(\u00b7) measure the the likelihood of x j is assigned as h j . Unary potential \u03d5(\u00b7) measures the compatibility between h j and y. Pairwise potential \u03c8(\u00b7) captures the compatibility between a pair of (h j , h k ) and y. Global potential \u03d1(\u00b7) measures the the likelihood of x 0 is assigned as y.part states h and all possible class label y . H N denotes the set of all possible hidden part states of N hidden parts.Then the probability of class label y for the given feature x is the summation of Eq. (3) over all possible assignments of hidden part states h:\n\nFig. 5 .\n5Message passing route and passing rule.\n\nFig. 6 .\n6An example of message passing on a graph with four joints.\n\nFig. 5 .\n5For this example, the joint set \u03bd = {1, 2, 3, 4}, the edge set = {(1, 2), (2, 3), (1, 4)}, and the corresponding vector of hidden part states h = {h 1 , h 2 , h 3 , h 4 }.\n\n\nThe h\u2208H N exp(\u03a6(\n\nFig. 7 .\n7(a) The skeleton of NTU RGB+D dataset with 25 joints; (b) The skeleton of N-UCLA dataset and SYSU dataset with 20 joints. from 30-fold cross validation on both Cross Subject (CS) setting and Same Subject (SS) setting.\n\nFig. 9 .\n9The confusion matrix on the CV setting of NTU RGB+D dataset(overall accuracy is 91.7%).\n\n\nQi received the B.Sc. degree in radio engineering from the Nanjing University of Posts and Telecommunications, Nanjing, China, the M.A.Sc. degree in computer science from the Zhengzhou University, Zhengzhou, China, and the Ph.D. degree in information and communication engineering from Beijing Institute of Technology, Beijing, China. He is currently a Professor with the School of Information Engineering, Zhengzhou University. His current research interests include image or video analysis and processing, pattern recognition, and signal detection and estimation. Ling Guan (Fellow, IEEE) received the Ph.D. degree in electrical engineering from the University of British Columbia, Vancouver, BC, Canada, in 1989. He is currently a Professor with the Department of Electrical and Computer Engineering, Ryerson University, Toronto, ON, Canada. In 2001, he was appointed as a Tier I Canada Research Chair in Multimedia and Computer Technology. He also held visiting positions with British Telecom (1994), Tokyo Institute of Technology (1999), Princeton University (2000), National ICT Australia (2007), Hong Kong Polytechnic University (2008-2009), and Microsoft Research Asia (2002, 2009, and 2017). He has authored extensively in multimedia processing and communications, human-centered computing, machine learning, adaptive image and signal processing, and more recently, multimedia computing in the immersive environment. He is an IEEE Circuits and System Society Distinguished Lecturer, an Elected Member of the Canadian Academy of Engineering, and the recipient of 2014 IEEE Canada C.C. Gotlieb Computer Medal and the 2005 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS Best Paper Award.\n\n\ny)(13) whereh \\ h 1 indicates all hidden part states except h 1 , h \\ h 1 , h 2 indicates all hidden part states except h 1 , h 2 , h \\ h 1 , h 2 , h 3 indicates all hidden part states except h 1 , h 2 , h 3 . For this example, only h 4 is left when h 1 , h 2 and h 3 are excepted. By reorganizing the sums in Eq. (13), we rewrite Eq. (13) as follows:\n\nTABLE I COMPARE\nITHE ACCURACY(%) OF THE PROPOSED MODEL WITH THE BASELINE ST-GCNFig. 8. The confusion matrix on the CS setting of NTU RGB+D dataset(overall \naccuracy is 84.3%). \n\n\n\nTABLE II ACCURACY\nII(%) COMPARISON WITH DIFFERENT TRAINING MANNERSTABLE III COMPARE THE ACCURACY(%) WITH DIFFERENT SIZE OF HIDDEN STATES SET ON THE CS SETTING OF NTU RGB+D DATASET\n\nTABLE IV COMPARE\nIVTHE ACCURACY(%) WITH DIFFERENT STREAM ON THE CS SETTING OF NTU RGB+D DATASET\n\nTABLE V COMPARE\nVTHE ACCURACY(%) OF TWO STRATEGIES WITH B-STREAM ON THE N-UCLA DATASET\n\nTABLE VI COMPARISON\nVIWITH STATE-OF-THE-ART METHODS ON NTU RGB+D DATASET\n\nTABLE\n\n\nTABLE\n\n\nHidden conditional random fields. A Quattoni, S Wang, L.-P Morency, M Collins, T Darrell, IEEE Trans. Pattern Anal. Mach. Intell. 2910A. Quattoni, S. Wang, L.-P. Morency, M. Collins, and T. Darrell, \"Hidden conditional random fields,\" IEEE Trans. Pattern Anal. Mach. Intell., 2007, vol. 29, no. 10, pp. 1848-1852, Oct. 2007.\n\nNTU RGB+D: A large scale dataset for 3D human activity analysis. A Shahroudy, J Liu, T.-T Ng, G Wang, Proc. Conf. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern RecognitA. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, \"NTU RGB+D: A large scale dataset for 3D human activity analysis,\" in Proc. Conf. Comput. Vision Pattern Recognit., 2016, pp. 1010-1019.\n\nSkeleton-based action recognition with convolutional neural networks. C Li, Q Zhong, D Xie, S Pu, Proc. IEEE Int. Conf. Multimedia Expo. IEEE Int. Conf. Multimedia ExpoC. Li, Q. Zhong, D. Xie, and S. Pu, \"Skeleton-based action recognition with convolutional neural networks,\" in Proc. IEEE Int. Conf. Multimedia Expo. Workshops, 2017, pp. 597-600.\n\nConvolutional neural random fields for action recognition. C Liu, J Liu, Pattern Recognit. 59C. Liu and J. Liu, \"Convolutional neural random fields for action recogni- tion,\" Pattern Recognit., vol. 59, pp. 213-224, 2016.\n\nCo-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation. C Li, Q Zhong, Proc. Int. Joint Conf. Int. Joint ConfC. Li and Q. Zhong, \"Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation,\" in Proc. Int. Joint Conf. Artif. Intell., 2018, pp. 1-8.\n\nSkeleton-based action recognition with spatial reasoning and temporal stack learning. C Si, Y Jing, Eur. Conf. Comput. Vision. C. Si and Y. Jing, \"Skeleton-based action recognition with spatial rea- soning and temporal stack learning,\" in Eur. Conf. Comput. Vision, 2018, pp. 103-118.\n\nAn attention enhanced graph convolutional LSTM network for skeleton-based action recognition. C Si, W Chen, Proc. Conf. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern RecognitC. Si and W. Chen, \"An attention enhanced graph convolutional LSTM network for skeleton-based action recognition,\" in Proc. Conf. Comput. Vision Pattern Recognit., 2019, pp. 1-10.\n\nAsynchronous temporal fields for action recognition. G A Sigurdsson, S Divvala, Proc. Conf. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern RecognitG. A. Sigurdsson and S. Divvala, \"Asynchronous temporal fields for ac- tion recognition,\" in Proc. Conf. Comput. Vision Pattern Recognit., 2017, pp. 1-20.\n\nSkeleton-based action recognition with synchronous local and non-local spatio-temporal learning and frequency attention. G Hu, B Cui, Proc. Stanford Inst. Comput. Math. Eng. G. Hu and B. Cui, \"Skeleton-based action recognition with synchronous local and non-local spatio-temporal learning and frequency attention,\" in Proc. Stanford Inst. Comput. Math. Eng., 2019, pp. 1-6.\n\nEnsemble deep learning for skeleton-based action recognition using temporal sliding LSTM networks. I Lee, D Kim, Proc. Int. Conf. Comput. Vision. Int. Conf. Comput. VisionI. Lee and D. Kim, \" Ensemble deep learning for skeleton-based action recognition using temporal sliding LSTM networks,\" in Proc. Int. Conf. Comput. Vision, 2017, pp. 1012-1020.\n\nExploring Artificial Intelligence in the new Millennium. J S Yedidia, W T Freeman, Y Weiss, Morgan Kaufmann Publishers IncSan Francisco, CA, USAJ. S. Yedidia, W. T. Freeman, and Y. Weiss, Exploring Artificial Intelli- gence in the new Millennium. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2003, pp. 239-269.\n\nConditional random fields: probabilistic models for segmenting and labeling sequence data. J Lafferty, A Mccallum, F Pereira, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. LearnJ. Lafferty, A. McCallum, and F. Pereira, \"Conditional random fields: probabilistic models for segmenting and labeling sequence data,\" in Proc. Int. Conf. Mach. Learn., 2001, pp. 282-289.\n\nContinuous action recognition based on hybrid CNN-LDCRF model. J Lei, G Li, Proc. 4th IEEE Int. Conf. Image, Vision Comput. 4th IEEE Int. Conf. Image, Vision ComputJ. Lei and G. Li, \"Continuous action recognition based on hybrid CNN- LDCRF model,\" in Proc. 4th IEEE Int. Conf. Image, Vision Comput., 2016, pp. 63-69.\n\nJointly learning heterogeneous features for RGB-D activity recognition. J.-F Hu, W.-S Zheng, Proc. nullJ.-F. Hu and W.-S. Zheng, \"Jointly learning heterogeneous features for RGB-D activity recognition,\" in Proc. IEEE Trans. Pattern Anal. Mach. Intell., 2015, pp. 2186-2200.\n\nLearning actionlet ensemble for 3D human action recognition. J Wang, Z Liu, Y Wu, Proc. nullJ. Wang, Z. Liu, and Y. Wu, \"Learning actionlet ensemble for 3D human action recognition,\" in Proc. IEEE Trans. Pattern Anal. Mach. Intell., 2014, pp. 914-927.\n\nReal-time RGB-D activity prediction by soft regression. J.-F Hu, W.-S Zheng, L Ma, G Wang, J Lai, Proc. Eur. Conf. Comput. Vision. Eur. Conf. Comput. VisionJ.-F. Hu, W.-S. Zheng, L. Ma, G. Wang, and J. Lai, \"Real-time RGB-D activity prediction by soft regression,\" in Proc. Eur. Conf. Comput. Vision, 2016, pp. 280-296.\n\nSpatio-temporal LSTM with trust gates for 3D human action recognition. J Liu, A Shahroudy, D Xu, G Wang, Proc. Eur. Conf. Comput. Vision. Eur. Conf. Comput. VisionJ. Liu, A. Shahroudy, D. Xu, and G. Wang, \" Spatio-temporal LSTM with trust gates for 3D human action recognition,\" in Proc. Eur. Conf. Comput. Vision, 2016, pp. 816-833.\n\nNonparametric feature matching based conditional random fields for gesture recognition from multi-modal video. J Chang, Proc. nullJ. Chang, \"Nonparametric feature matching based conditional random fields for gesture recognition from multi-modal video,\" in Proc. IEEE Trans. Pattern Anal. Mach. Intell., 2016, pp. 1612-1625.\n\nEfficient inference in fully connected CRFs with Gaussian edge potentials. K Philipp, K Vladlen, Proc. Conf. Neural Inf. Process. Syst. Conf. Neural Inf. ess. SystK. Philipp and K. Vladlen, \"Efficient inference in fully connected CRFs with Gaussian edge potentials,\" in Proc. Conf. Neural Inf. Process. Syst., 2012, pp. 109-117.\n\n3D human activity recognition with reconfigurable convolutional neural networks. K Wang, X Wang, L Lin, M Wang, W Zuo, Proc. ACM Multimedia. ACM MultimediaK. Wang, X. Wang, L. Lin, M. Wang, and W. Zuo, \"3D human activity recognition with reconfigurable convolutional neural networks,\" in Proc. ACM Multimedia, 2014, pp. 97-106.\n\nPart-based graph convolutional network for action recognition. K Thakkar, P J Narayanan, Proc. Brit. Mach. Vision Conf. Brit. Mach. Vision ConfK. Thakkar and P. J. Narayanan, \"Part-based graph convolutional net- work for action recognition,\" in Proc. Brit. Mach. Vision Conf., 2018, pp. 1-19.\n\nTwo-stream convolutional networks for action recognition in videos. K Simonyan, A Zisserman, Proc. Conf. Neural Inf. Process. Syst. Conf. Neural Inf. ess. SystK. Simonyan and A. Zisserman, \"Two-stream convolutional networks for action recognition in videos,\" in Proc. Conf. Neural Inf. Process. Syst., 2014, pp. 568-576.\n\nNon-local graph convolutional networks for skeleton-based action recognition. L Shi, Y Zhang, Proc. IEEE Conf. Comput. Vision Patten Recognit. IEEE Conf. Comput. Vision Patten RecognitL. Shi and Y. Zhang, \"Non-local graph convolutional networks for skeleton-based action recognition,\" in Proc. IEEE Conf. Comput. Vision Patten Recognit., 2019, pp. 12027-12035.\n\nView invariant human action recognition using histograms of 3D joints. L Xia, C.-C Chen, J Aggarwal, Proc. IEEE Conf. Comput. Vision Pattern Recognit. Workshops. IEEE Conf. Comput. Vision Pattern Recognit. WorkshopsL. Xia, C.-C. Chen, and J. Aggarwal, \"View invariant human action recog- nition using histograms of 3D joints,\" in Proc. IEEE Conf. Comput. Vision Pattern Recognit. Workshops, 2012, pp. 20-27.\n\nEnhanced skeleton visualization for view invariant human action recognition. M Liu, H Liu, C Chen, Proc. Pattern Recognit. Pattern RecognitM. Liu, H. Liu, and C. Chen, \"Enhanced skeleton visualization for view invariant human action recognition,\" in Proc. Pattern Recognit., 2017, pp. 346-362.\n\nMultiview skeletal interaction recognition using active joint interaction graph. M Li, H Leung, Proc. IEEE Trans. Multimedia. IEEE Trans. MultimediaM. Li and H. Leung, \"Multiview skeletal interaction recognition using active joint interaction graph,\" in Proc. IEEE Trans. Multimedia, 2016, pp. 2293-2302.\n\nView adaptive recurrent neural networks for high performance human action recognition from skeleton data. P Zhang, C Lan, J Xing, W Zeng, J Xue, N Zheng, Proc. Conf. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern RecognitP. Zhang, C. Lan, J. Xing, W. Zeng, J. Xue, and N. Zheng, \"View adaptive recurrent neural networks for high performance human action recognition from skeleton data,\" in Proc. Conf. Comput. Vision Pattern Recognit., 2017, pp. 2117-2126.\n\nView adaptive neural networks for high performance skeleton-based human action recognition. P.-F Zhang, C Lan, Proc. nullP.-F. Zhang and C. Lan, \"View adaptive neural networks for high perfor- mance skeleton-based human action recognition,\" in Proc. IEEE Trans. Pattern Anal. Mach. Intell., 2019, pp. 1-15.\n\nAdding attentiveness to the neurons in recurrent neural networks. P.-F Zhang, J Xue, C Lan, Proc. Eur. Conf. Comput. Vision. Eur. Conf. Comput. VisionP.-F. Zhang, J. Xue, and C. Lan, \"Adding attentiveness to the neurons in recurrent neural networks,\" in Proc. Eur. Conf. Comput. Vision, 2018, pp. 135-151.\n\nSemantics-guided neural networks for efficient skeleton-based human action recognition. P.-F Zhang, Proc. IEEE Conf. Comput. Vision Patten Recognit. IEEE Conf. Comput. Vision Patten RecognitP.-F. Zhang et al., \"Semantics-guided neural networks for efficient skeleton-based human action recognition,\" in Proc. IEEE Conf. Comput. Vision Patten Recognit., 2020, pp. 1-10.\n\nLearning composite latent structures for 3D human action representation and recognition. P Wei, H Sun, N Zheng, Proc. IEEE Trans. Multimedia. IEEE Trans. MultimediaP. Wei, H. Sun, and N. Zheng, \"Learning composite latent structures for 3D human action representation and recognition,\" in Proc. IEEE Trans. Multimedia, 2019, pp. 1-1.\n\nDiscovering discriminative action parts from mid-level video representation. R Michails, I Kokkinos, S Soatto, Proc. Conf. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern RecognitR. Michails, I. Kokkinos, and S. Soatto, \"Discovering discriminative action parts from mid-level video representation,\" in Proc. Conf. Comput. Vision Pattern Recognit., 2013, pp. 1242-1249.\n\nHuman action recognition by representing 3D skeletons as points in a lie group. R Vemulapalli, F Arrate, R Chellappa, Proc. Conf. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern RecognitR. Vemulapalli, F. Arrate, and R. Chellappa, \"Human action recognition by representing 3D skeletons as points in a lie group,\" in Proc. Conf. Comput. Vision Pattern Recognit., 2014, pp. 588-595.\n\nBeyond joints: Learning representations from primitive geometries for skeleton-based action recognition and detection. S Wang, L Wang, Proc. IEEE Trans. IEEE TransS. Wang and L. Wang, \"Beyond joints: Learning representations from primitive geometries for skeleton-based action recognition and detection,\" in Proc. IEEE Trans. Image Process., 2018, pp. 4382-4394.\n\nSpatial temporal graph convolutional networks for skeleton-based action recognition. S Yan, Y Xiong, D Lin, Proc. Assoc. Advancement Artif. Intell. S. Yan, Y. Xiong, and D. Lin, \"Spatial temporal graph convolutional net- works for skeleton-based action recognition,\" in Proc. Assoc. Advancement Artif. Intell., 2018, pp. 7444-7452.\n\nAn end-to-end spatiotemporal attention model for human action recognition from skeleton data. S Song, C Lan, J Xing, W Zeng, J Liu, Proc. Assoc. Advancement Artif. Intell. Assoc. Advancement Artif. IntellS. Song, C. Lan, J. Xing, W. Zeng, and J. Liu, \"An end-to-end spatio- temporal attention model for human action recognition from skeleton data,\" in Proc. Assoc. Advancement Artif. Intell., 2017, pp. 4263-4270.\n\nFusing geometric features for skeletonbased action recognition using multilayer LSTM networks. S Zhang, Y Yang, J Xiao, Proc. IEEE Trans. Multimedia. IEEE Trans. MultimediaS. Zhang, Y. Yang, and J. Xiao, \"Fusing geometric features for skeleton- based action recognition using multilayer LSTM networks,\" in Proc. IEEE Trans. Multimedia, 2018, pp. 2330-2343.\n\nInterpretable 3 d human action analysis with temporal convolutional networks. T S Kim, A Reiter, Proc. IEEE Conf. Comput. Vision Pattern Recognit. IEEE Conf. Comput. Vision Pattern RecognitT. S. Kim and A. Reiter, \"Interpretable 3 d human action analysis with temporal convolutional networks,\" in Proc. IEEE Conf. Comput. Vision Pattern Recognit. Workshops, 2017, pp. 1623-1631.\n\nHidden conditional random fields for gesture recognition. W Sybor, A Quattoni, L Morency, D Demirdjian, T Darrell, Proc. Conf. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern RecognitW. Sybor, A. Quattoni, L. Morency, D. Demirdjian, and T. Darrell, \"Hidden conditional random fields for gesture recognition,\" in Proc. Conf. Comput. Vision Pattern Recognit., 2006, pp. 1521-1527.\n\nLearning latent spatio-temporal compositional model for human action recognition. X Liang, L Lin, L Cao, Proc. ACM Multimedia. ACM MultimediaX. Liang, L. Lin, and L. Cao, \"Learning latent spatio-temporal compo- sitional model for human action recognition,\" in Proc. ACM Multimedia, 2013, pp. 263-272.\n\nGeneralized graph convolutional networks for skeleton-based action recognition. X Gao, W Hu, J Tang, Proc. ACM Int. Conf. Mutimedia, 2019. ACM Int. Conf. Mutimedia, 2019X. Gao, W. Hu, and J. Tang, \"Generalized graph convolutional networks for skeleton-based action recognition,\" in Proc. ACM Int. Conf. Mutimedia, 2019, pp. 1-9.\n\nTwo-stream 3-D convnet fusion for action recognition in videos with arbitrary size and length. X Wang, L Gao, P Wang, X Sun, X Liu, Proc. IEEE Trans. Multimedia. IEEE Trans. MultimediaX. Wang, L. Gao, P. Wang, X. Sun, and X. Liu, \"Two-stream 3-D convnet fusion for action recognition in videos with arbitrary size and length,\" in Proc. IEEE Trans. Multimedia, 2018, pp. 634-644.\n\nHidden part models for human action recognition: Probabilistic vs. max-margin. Y Wang, G Mori, Proc. nullY. Wang and G. Mori, \"Hidden part models for human action recognition: Probabilistic vs. max-margin,\" in Proc. IEEE Trans. Pattern Anal. Mach. Intell., 2010, pp. 1310-1323.\n\nHierarchical recurrent neural network for skeleton based action recognition. Y Du, W Wang, L Wang, Proc. Conf. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern RecognitY. Du, W. Wang, and L. Wang, \"Hierarchical recurrent neural network for skeleton based action recognition,\" in Proc. Conf. Comput. Vision Pattern Recognit., 2015, pp. 1110-1118.\n\nRepresentation learning of temporal dynamics for skeleton-based action recognition. Y Du, Y Fu, L Wang, Proc. EEE Trans. Image Process. EEE Trans. Image essY. Du, Y. Fu, and L. Wang, \"Representation learning of temporal dynamics for skeleton-based action recognition,\" in Proc. EEE Trans. Image Pro- cess., 2016, pp. 3010-3022.\n\nDeep progressive reinforcement learning for skeleton-based action recognition. Y Tang, Y Tian, J Lu, P Li, J Zhou, Proc. Conf. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern RecognitY. Tang, Y. Tian, J. Lu, P. Li, and J. Zhou, \"Deep progressive reinforcement learning for skeleton-based action recognition,\" in Proc. Conf. Comput. Vision Pattern Recognit., 2018, pp. 5323-5332.\n\nDiscriminative multiinstance multi-task learning for 3D action recognition. Y Yang, C Deng, S Gao, W Liu, D Tao, X Gao, Proc. IEEE Trans. IEEE TransY. Yang, C. Deng, S. Gao, W. Liu, D. Tao, X. Gao, \"Discriminative multi- instance multi-task learning for 3D action recognition,\" in Proc. IEEE Trans. Multimedia, 2017, pp. 519-529.\n\nConditional random fields as recurrent neural networks. Z Shuai, Proc. Int. Conf. Comput. Vision. Int. Conf. Comput. VisionZ. Shuai et al., \"Conditional random fields as recurrent neural networks,\" in Proc. Int. Conf. Comput. Vision, 2015, pp. 1529-1537.\n\nAttention based multiview reobservation fusion network for skeletal action recognition. Z Fan, X Zhao, T Lin, H Su, Proc. IEEE Trans. Multimedia. IEEE Trans. MultimediaZ. Fan, X. Zhao, T. Lin, and H. Su, \"Attention based multiview re- observation fusion network for skeletal action recognition,\" in Proc. IEEE Trans. Multimedia, 2018, pp. 363-374.\n", "annotations": {"author": "[{\"start\":\"123\",\"end\":\"151\"},{\"start\":\"152\",\"end\":\"172\"},{\"start\":\"173\",\"end\":\"204\"},{\"start\":\"205\",\"end\":\"212\"},{\"start\":\"213\",\"end\":\"235\"}]", "publisher": null, "author_last_name": "[{\"start\":\"147\",\"end\":\"150\"},{\"start\":\"168\",\"end\":\"171\"},{\"start\":\"199\",\"end\":\"203\"},{\"start\":\"209\",\"end\":\"211\"},{\"start\":\"230\",\"end\":\"234\"}]", "author_first_name": "[{\"start\":\"143\",\"end\":\"146\"},{\"start\":\"164\",\"end\":\"167\"},{\"start\":\"185\",\"end\":\"191\"},{\"start\":\"192\",\"end\":\"198\"},{\"start\":\"205\",\"end\":\"208\"},{\"start\":\"225\",\"end\":\"229\"}]", "author_affiliation": null, "title": "[{\"start\":\"1\",\"end\":\"120\"},{\"start\":\"236\",\"end\":\"355\"}]", "venue": "[{\"start\":\"357\",\"end\":\"388\"}]", "abstract": "[{\"start\":\"483\",\"end\":\"1709\"}]", "bib_ref": "[{\"start\":\"1767\",\"end\":\"1771\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"1773\",\"end\":\"1777\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"1779\",\"end\":\"1783\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"1785\",\"end\":\"1789\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"1791\",\"end\":\"1795\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"2465\",\"end\":\"2469\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"3549\",\"end\":\"3555\"},{\"start\":\"4392\",\"end\":\"4396\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"5467\",\"end\":\"5471\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"5648\",\"end\":\"5652\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"5654\",\"end\":\"5658\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"5660\",\"end\":\"5664\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"5873\",\"end\":\"5877\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"5879\",\"end\":\"5883\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"8925\",\"end\":\"8928\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"9079\",\"end\":\"9083\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"9234\",\"end\":\"9237\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"9382\",\"end\":\"9386\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"9668\",\"end\":\"9672\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"9674\",\"end\":\"9678\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"9680\",\"end\":\"9684\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"9839\",\"end\":\"9843\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"9972\",\"end\":\"9976\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"10161\",\"end\":\"10165\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"10387\",\"end\":\"10391\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"10596\",\"end\":\"10600\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"10745\",\"end\":\"10749\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"10901\",\"end\":\"10905\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"11055\",\"end\":\"11059\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"11363\",\"end\":\"11367\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"11464\",\"end\":\"11468\",\"attributes\":{\"ref_id\":\"b47\"}},{\"start\":\"11521\",\"end\":\"11525\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"11626\",\"end\":\"11630\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"11755\",\"end\":\"11759\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"11872\",\"end\":\"11876\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"12056\",\"end\":\"12059\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"14278\",\"end\":\"14282\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"14493\",\"end\":\"14497\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"14884\",\"end\":\"14888\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"15805\",\"end\":\"15809\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"17123\",\"end\":\"17127\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"20501\",\"end\":\"20504\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"26666\",\"end\":\"26670\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"27405\",\"end\":\"27409\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"28435\",\"end\":\"28438\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"28507\",\"end\":\"28511\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"29913\",\"end\":\"29917\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"30741\",\"end\":\"30745\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"30989\",\"end\":\"30993\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"30995\",\"end\":\"30999\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"31001\",\"end\":\"31005\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"31143\",\"end\":\"31147\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"31512\",\"end\":\"31516\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"34804\",\"end\":\"34808\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"35622\",\"end\":\"35626\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"35735\",\"end\":\"35739\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"37232\",\"end\":\"37236\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"38274\",\"end\":\"38278\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"39665\",\"end\":\"39669\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"39671\",\"end\":\"39675\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"43429\",\"end\":\"43433\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"43461\",\"end\":\"43464\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"43466\",\"end\":\"43469\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"43471\",\"end\":\"43474\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"43476\",\"end\":\"43480\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"43482\",\"end\":\"43486\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"43488\",\"end\":\"43492\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"43525\",\"end\":\"43528\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"43530\",\"end\":\"43533\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"43535\",\"end\":\"43539\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"43541\",\"end\":\"43545\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"43547\",\"end\":\"43551\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"43553\",\"end\":\"43557\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"43559\",\"end\":\"43563\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"43595\",\"end\":\"43599\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"43601\",\"end\":\"43605\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"43607\",\"end\":\"43611\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"43613\",\"end\":\"43617\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"43746\",\"end\":\"43750\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"43875\",\"end\":\"43879\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"44019\",\"end\":\"44023\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"44191\",\"end\":\"44195\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"44197\",\"end\":\"44201\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"44267\",\"end\":\"44270\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"44272\",\"end\":\"44276\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"44278\",\"end\":\"44282\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"44284\",\"end\":\"44288\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"44290\",\"end\":\"44294\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"44296\",\"end\":\"44300\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"44302\",\"end\":\"44306\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"44308\",\"end\":\"44312\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"44314\",\"end\":\"44318\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"44876\",\"end\":\"44880\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"45002\",\"end\":\"45006\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"45311\",\"end\":\"45315\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"45567\",\"end\":\"45571\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"50642\",\"end\":\"50646\",\"attributes\":{\"ref_id\":\"b12\"}}]", "figure": "[{\"start\":\"47509\",\"end\":\"47677\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"47678\",\"end\":\"48295\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"48296\",\"end\":\"48346\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"48347\",\"end\":\"48416\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"48417\",\"end\":\"48599\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"48600\",\"end\":\"48618\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"48619\",\"end\":\"48847\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"48848\",\"end\":\"48946\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"48947\",\"end\":\"50637\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"50638\",\"end\":\"50991\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"50992\",\"end\":\"51171\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"51172\",\"end\":\"51352\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"51353\",\"end\":\"51449\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"51450\",\"end\":\"51537\",\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"}},{\"start\":\"51538\",\"end\":\"51611\",\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"}},{\"start\":\"51612\",\"end\":\"51619\",\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"}},{\"start\":\"51620\",\"end\":\"51627\",\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1711\",\"end\":\"1873\"},{\"start\":\"1875\",\"end\":\"4333\"},{\"start\":\"4335\",\"end\":\"5058\"},{\"start\":\"5060\",\"end\":\"5472\"},{\"start\":\"5474\",\"end\":\"6329\"},{\"start\":\"6331\",\"end\":\"7152\"},{\"start\":\"7154\",\"end\":\"8520\"},{\"start\":\"8541\",\"end\":\"8712\"},{\"start\":\"8749\",\"end\":\"10101\"},{\"start\":\"10126\",\"end\":\"11276\"},{\"start\":\"11278\",\"end\":\"12259\"},{\"start\":\"12283\",\"end\":\"12697\"},{\"start\":\"12739\",\"end\":\"13401\"},{\"start\":\"13460\",\"end\":\"14391\"},{\"start\":\"14393\",\"end\":\"14498\"},{\"start\":\"14568\",\"end\":\"15287\"},{\"start\":\"15289\",\"end\":\"15695\"},{\"start\":\"15697\",\"end\":\"15810\"},{\"start\":\"15863\",\"end\":\"17646\"},{\"start\":\"17708\",\"end\":\"18638\"},{\"start\":\"18640\",\"end\":\"18765\"},{\"start\":\"18829\",\"end\":\"20455\"},{\"start\":\"20457\",\"end\":\"20660\"},{\"start\":\"20730\",\"end\":\"20987\"},{\"start\":\"21097\",\"end\":\"21207\"},{\"start\":\"21313\",\"end\":\"22049\"},{\"start\":\"22051\",\"end\":\"22936\"},{\"start\":\"22938\",\"end\":\"23093\"},{\"start\":\"23152\",\"end\":\"23489\"},{\"start\":\"23491\",\"end\":\"23754\"},{\"start\":\"23838\",\"end\":\"24094\"},{\"start\":\"24096\",\"end\":\"24992\"},{\"start\":\"24994\",\"end\":\"25444\"},{\"start\":\"25471\",\"end\":\"25807\"},{\"start\":\"25850\",\"end\":\"26050\"},{\"start\":\"26052\",\"end\":\"26867\"},{\"start\":\"26869\",\"end\":\"27065\"},{\"start\":\"27144\",\"end\":\"27410\"},{\"start\":\"27412\",\"end\":\"27647\"},{\"start\":\"27774\",\"end\":\"27865\"},{\"start\":\"28352\",\"end\":\"28673\"},{\"start\":\"29740\",\"end\":\"30016\"},{\"start\":\"30151\",\"end\":\"30506\"},{\"start\":\"30679\",\"end\":\"31405\"},{\"start\":\"31407\",\"end\":\"31788\"},{\"start\":\"31790\",\"end\":\"32259\"},{\"start\":\"32261\",\"end\":\"32649\"},{\"start\":\"32651\",\"end\":\"32832\"},{\"start\":\"32872\",\"end\":\"33047\"},{\"start\":\"33049\",\"end\":\"33414\"},{\"start\":\"33456\",\"end\":\"34365\"},{\"start\":\"34367\",\"end\":\"34921\"},{\"start\":\"34970\",\"end\":\"38035\"},{\"start\":\"38106\",\"end\":\"38749\"},{\"start\":\"38751\",\"end\":\"39485\"},{\"start\":\"39535\",\"end\":\"40440\"},{\"start\":\"40469\",\"end\":\"41276\"},{\"start\":\"41321\",\"end\":\"42961\"},{\"start\":\"43017\",\"end\":\"43166\"},{\"start\":\"43192\",\"end\":\"43912\"},{\"start\":\"43914\",\"end\":\"44938\"},{\"start\":\"44940\",\"end\":\"45616\"},{\"start\":\"45634\",\"end\":\"47502\"}]", "formula": "[{\"start\":\"14499\",\"end\":\"14567\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"15811\",\"end\":\"15862\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"18766\",\"end\":\"18828\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"20661\",\"end\":\"20729\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"20988\",\"end\":\"21096\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"21208\",\"end\":\"21312\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"23094\",\"end\":\"23151\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"23755\",\"end\":\"23837\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"25808\",\"end\":\"25849\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"27066\",\"end\":\"27143\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"27648\",\"end\":\"27726\",\"attributes\":{\"id\":\"formula_10\"}},{\"start\":\"27726\",\"end\":\"27773\",\"attributes\":{\"id\":\"formula_11\"}},{\"start\":\"27866\",\"end\":\"28351\",\"attributes\":{\"id\":\"formula_12\"}},{\"start\":\"28674\",\"end\":\"29054\",\"attributes\":{\"id\":\"formula_13\"}},{\"start\":\"29054\",\"end\":\"29739\",\"attributes\":{\"id\":\"formula_14\"}},{\"start\":\"30017\",\"end\":\"30150\",\"attributes\":{\"id\":\"formula_15\"}},{\"start\":\"30507\",\"end\":\"30653\",\"attributes\":{\"id\":\"formula_16\"}}]", "table_ref": "[{\"start\":\"38295\",\"end\":\"38302\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"38948\",\"end\":\"38956\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"39977\",\"end\":\"39987\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"39991\",\"end\":\"40000\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"40841\",\"end\":\"40849\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"41074\",\"end\":\"41082\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"42647\",\"end\":\"42655\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"42767\",\"end\":\"42786\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"43634\",\"end\":\"43642\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"44036\",\"end\":\"44103\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"44448\",\"end\":\"44457\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"45425\",\"end\":\"45435\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"46123\",\"end\":\"46189\",\"attributes\":{\"ref_id\":\"tab_1\"}}]", "section_header": "[{\"start\":\"8523\",\"end\":\"8539\"},{\"start\":\"8715\",\"end\":\"8747\"},{\"start\":\"10104\",\"end\":\"10124\"},{\"start\":\"12262\",\"end\":\"12281\"},{\"start\":\"12700\",\"end\":\"12737\"},{\"start\":\"13404\",\"end\":\"13458\"},{\"start\":\"17649\",\"end\":\"17706\"},{\"start\":\"25447\",\"end\":\"25469\"},{\"start\":\"30655\",\"end\":\"30677\"},{\"start\":\"32835\",\"end\":\"32870\"},{\"start\":\"33417\",\"end\":\"33454\"},{\"start\":\"34924\",\"end\":\"34968\"},{\"start\":\"38038\",\"end\":\"38104\"},{\"start\":\"39488\",\"end\":\"39533\"},{\"start\":\"40443\",\"end\":\"40467\"},{\"start\":\"41279\",\"end\":\"41319\"},{\"start\":\"42964\",\"end\":\"43015\"},{\"start\":\"43169\",\"end\":\"43190\"},{\"start\":\"45619\",\"end\":\"45632\"},{\"start\":\"47505\",\"end\":\"47508\"},{\"start\":\"47510\",\"end\":\"47518\"},{\"start\":\"47679\",\"end\":\"47687\"},{\"start\":\"48297\",\"end\":\"48305\"},{\"start\":\"48348\",\"end\":\"48356\"},{\"start\":\"48418\",\"end\":\"48426\"},{\"start\":\"48620\",\"end\":\"48628\"},{\"start\":\"48849\",\"end\":\"48857\"},{\"start\":\"50993\",\"end\":\"51008\"},{\"start\":\"51173\",\"end\":\"51190\"},{\"start\":\"51354\",\"end\":\"51370\"},{\"start\":\"51451\",\"end\":\"51466\"},{\"start\":\"51539\",\"end\":\"51558\"},{\"start\":\"51613\",\"end\":\"51618\"},{\"start\":\"51621\",\"end\":\"51626\"}]", "table": "[{\"start\":\"51072\",\"end\":\"51171\"}]", "figure_caption": "[{\"start\":\"47520\",\"end\":\"47677\"},{\"start\":\"47689\",\"end\":\"48295\"},{\"start\":\"48307\",\"end\":\"48346\"},{\"start\":\"48358\",\"end\":\"48416\"},{\"start\":\"48428\",\"end\":\"48599\"},{\"start\":\"48602\",\"end\":\"48618\"},{\"start\":\"48630\",\"end\":\"48847\"},{\"start\":\"48859\",\"end\":\"48946\"},{\"start\":\"48949\",\"end\":\"50637\"},{\"start\":\"50640\",\"end\":\"50991\"},{\"start\":\"51010\",\"end\":\"51072\"},{\"start\":\"51193\",\"end\":\"51352\"},{\"start\":\"51373\",\"end\":\"51449\"},{\"start\":\"51468\",\"end\":\"51537\"},{\"start\":\"51561\",\"end\":\"51611\"}]", "figure_ref": "[{\"start\":\"3508\",\"end\":\"3514\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"4609\",\"end\":\"4615\"},{\"start\":\"4761\",\"end\":\"4767\"},{\"start\":\"12751\",\"end\":\"12757\"},{\"start\":\"17304\",\"end\":\"17310\"},{\"start\":\"18002\",\"end\":\"18008\"},{\"start\":\"20115\",\"end\":\"20121\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"21980\",\"end\":\"21986\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"22485\",\"end\":\"22491\"},{\"start\":\"22575\",\"end\":\"22590\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"23698\",\"end\":\"23704\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"24375\",\"end\":\"24381\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"26860\",\"end\":\"26866\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"26872\",\"end\":\"26880\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"27822\",\"end\":\"27836\"},{\"start\":\"28561\",\"end\":\"28567\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"28636\",\"end\":\"28647\"},{\"start\":\"29954\",\"end\":\"29968\"},{\"start\":\"29998\",\"end\":\"30004\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"36297\",\"end\":\"36303\"},{\"start\":\"37783\",\"end\":\"37789\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"38717\",\"end\":\"38723\"},{\"start\":\"38728\",\"end\":\"38734\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"41355\",\"end\":\"41361\",\"attributes\":{\"ref_id\":\"fig_6\"}}]", "bib_author_first_name": "[{\"start\":\"51663\",\"end\":\"51664\"},{\"start\":\"51675\",\"end\":\"51676\"},{\"start\":\"51683\",\"end\":\"51687\"},{\"start\":\"51697\",\"end\":\"51698\"},{\"start\":\"51708\",\"end\":\"51709\"},{\"start\":\"52020\",\"end\":\"52021\"},{\"start\":\"52033\",\"end\":\"52034\"},{\"start\":\"52040\",\"end\":\"52044\"},{\"start\":\"52049\",\"end\":\"52050\"},{\"start\":\"52392\",\"end\":\"52393\"},{\"start\":\"52398\",\"end\":\"52399\"},{\"start\":\"52407\",\"end\":\"52408\"},{\"start\":\"52414\",\"end\":\"52415\"},{\"start\":\"52730\",\"end\":\"52731\"},{\"start\":\"52737\",\"end\":\"52738\"},{\"start\":\"53012\",\"end\":\"53013\"},{\"start\":\"53018\",\"end\":\"53019\"},{\"start\":\"53349\",\"end\":\"53350\"},{\"start\":\"53355\",\"end\":\"53356\"},{\"start\":\"53643\",\"end\":\"53644\"},{\"start\":\"53649\",\"end\":\"53650\"},{\"start\":\"53973\",\"end\":\"53974\"},{\"start\":\"53975\",\"end\":\"53976\"},{\"start\":\"53989\",\"end\":\"53990\"},{\"start\":\"54359\",\"end\":\"54360\"},{\"start\":\"54365\",\"end\":\"54366\"},{\"start\":\"54712\",\"end\":\"54713\"},{\"start\":\"54719\",\"end\":\"54720\"},{\"start\":\"55020\",\"end\":\"55021\"},{\"start\":\"55022\",\"end\":\"55023\"},{\"start\":\"55033\",\"end\":\"55034\"},{\"start\":\"55035\",\"end\":\"55036\"},{\"start\":\"55046\",\"end\":\"55047\"},{\"start\":\"55378\",\"end\":\"55379\"},{\"start\":\"55390\",\"end\":\"55391\"},{\"start\":\"55402\",\"end\":\"55403\"},{\"start\":\"55717\",\"end\":\"55718\"},{\"start\":\"55724\",\"end\":\"55725\"},{\"start\":\"56044\",\"end\":\"56048\"},{\"start\":\"56053\",\"end\":\"56057\"},{\"start\":\"56308\",\"end\":\"56309\"},{\"start\":\"56316\",\"end\":\"56317\"},{\"start\":\"56323\",\"end\":\"56324\"},{\"start\":\"56556\",\"end\":\"56560\"},{\"start\":\"56565\",\"end\":\"56569\"},{\"start\":\"56577\",\"end\":\"56578\"},{\"start\":\"56583\",\"end\":\"56584\"},{\"start\":\"56591\",\"end\":\"56592\"},{\"start\":\"56892\",\"end\":\"56893\"},{\"start\":\"56899\",\"end\":\"56900\"},{\"start\":\"56912\",\"end\":\"56913\"},{\"start\":\"56918\",\"end\":\"56919\"},{\"start\":\"57267\",\"end\":\"57268\"},{\"start\":\"57556\",\"end\":\"57557\"},{\"start\":\"57567\",\"end\":\"57568\"},{\"start\":\"57892\",\"end\":\"57893\"},{\"start\":\"57900\",\"end\":\"57901\"},{\"start\":\"57908\",\"end\":\"57909\"},{\"start\":\"57915\",\"end\":\"57916\"},{\"start\":\"57923\",\"end\":\"57924\"},{\"start\":\"58203\",\"end\":\"58204\"},{\"start\":\"58214\",\"end\":\"58215\"},{\"start\":\"58216\",\"end\":\"58217\"},{\"start\":\"58502\",\"end\":\"58503\"},{\"start\":\"58514\",\"end\":\"58515\"},{\"start\":\"58834\",\"end\":\"58835\"},{\"start\":\"58841\",\"end\":\"58842\"},{\"start\":\"59189\",\"end\":\"59190\"},{\"start\":\"59196\",\"end\":\"59200\"},{\"start\":\"59207\",\"end\":\"59208\"},{\"start\":\"59604\",\"end\":\"59605\"},{\"start\":\"59611\",\"end\":\"59612\"},{\"start\":\"59618\",\"end\":\"59619\"},{\"start\":\"59903\",\"end\":\"59904\"},{\"start\":\"59909\",\"end\":\"59910\"},{\"start\":\"60234\",\"end\":\"60235\"},{\"start\":\"60243\",\"end\":\"60244\"},{\"start\":\"60250\",\"end\":\"60251\"},{\"start\":\"60258\",\"end\":\"60259\"},{\"start\":\"60266\",\"end\":\"60267\"},{\"start\":\"60273\",\"end\":\"60274\"},{\"start\":\"60693\",\"end\":\"60697\"},{\"start\":\"60705\",\"end\":\"60706\"},{\"start\":\"60975\",\"end\":\"60979\"},{\"start\":\"60987\",\"end\":\"60988\"},{\"start\":\"60994\",\"end\":\"60995\"},{\"start\":\"61304\",\"end\":\"61308\"},{\"start\":\"61675\",\"end\":\"61676\"},{\"start\":\"61682\",\"end\":\"61683\"},{\"start\":\"61689\",\"end\":\"61690\"},{\"start\":\"61997\",\"end\":\"61998\"},{\"start\":\"62009\",\"end\":\"62010\"},{\"start\":\"62021\",\"end\":\"62022\"},{\"start\":\"62384\",\"end\":\"62385\"},{\"start\":\"62399\",\"end\":\"62400\"},{\"start\":\"62409\",\"end\":\"62410\"},{\"start\":\"62819\",\"end\":\"62820\"},{\"start\":\"62827\",\"end\":\"62828\"},{\"start\":\"63149\",\"end\":\"63150\"},{\"start\":\"63156\",\"end\":\"63157\"},{\"start\":\"63165\",\"end\":\"63166\"},{\"start\":\"63491\",\"end\":\"63492\"},{\"start\":\"63499\",\"end\":\"63500\"},{\"start\":\"63506\",\"end\":\"63507\"},{\"start\":\"63514\",\"end\":\"63515\"},{\"start\":\"63522\",\"end\":\"63523\"},{\"start\":\"63907\",\"end\":\"63908\"},{\"start\":\"63916\",\"end\":\"63917\"},{\"start\":\"63924\",\"end\":\"63925\"},{\"start\":\"64248\",\"end\":\"64249\"},{\"start\":\"64250\",\"end\":\"64251\"},{\"start\":\"64257\",\"end\":\"64258\"},{\"start\":\"64608\",\"end\":\"64609\"},{\"start\":\"64617\",\"end\":\"64618\"},{\"start\":\"64629\",\"end\":\"64630\"},{\"start\":\"64640\",\"end\":\"64641\"},{\"start\":\"64654\",\"end\":\"64655\"},{\"start\":\"65026\",\"end\":\"65027\"},{\"start\":\"65035\",\"end\":\"65036\"},{\"start\":\"65042\",\"end\":\"65043\"},{\"start\":\"65326\",\"end\":\"65327\"},{\"start\":\"65333\",\"end\":\"65334\"},{\"start\":\"65339\",\"end\":\"65340\"},{\"start\":\"65671\",\"end\":\"65672\"},{\"start\":\"65679\",\"end\":\"65680\"},{\"start\":\"65686\",\"end\":\"65687\"},{\"start\":\"65694\",\"end\":\"65695\"},{\"start\":\"65701\",\"end\":\"65702\"},{\"start\":\"66035\",\"end\":\"66036\"},{\"start\":\"66043\",\"end\":\"66044\"},{\"start\":\"66312\",\"end\":\"66313\"},{\"start\":\"66318\",\"end\":\"66319\"},{\"start\":\"66326\",\"end\":\"66327\"},{\"start\":\"66679\",\"end\":\"66680\"},{\"start\":\"66685\",\"end\":\"66686\"},{\"start\":\"66691\",\"end\":\"66692\"},{\"start\":\"67003\",\"end\":\"67004\"},{\"start\":\"67011\",\"end\":\"67012\"},{\"start\":\"67019\",\"end\":\"67020\"},{\"start\":\"67025\",\"end\":\"67026\"},{\"start\":\"67031\",\"end\":\"67032\"},{\"start\":\"67394\",\"end\":\"67395\"},{\"start\":\"67402\",\"end\":\"67403\"},{\"start\":\"67410\",\"end\":\"67411\"},{\"start\":\"67417\",\"end\":\"67418\"},{\"start\":\"67424\",\"end\":\"67425\"},{\"start\":\"67431\",\"end\":\"67432\"},{\"start\":\"67705\",\"end\":\"67706\"},{\"start\":\"67993\",\"end\":\"67994\"},{\"start\":\"68000\",\"end\":\"68001\"},{\"start\":\"68008\",\"end\":\"68009\"},{\"start\":\"68015\",\"end\":\"68016\"}]", "bib_author_last_name": "[{\"start\":\"51665\",\"end\":\"51673\"},{\"start\":\"51677\",\"end\":\"51681\"},{\"start\":\"51688\",\"end\":\"51695\"},{\"start\":\"51699\",\"end\":\"51706\"},{\"start\":\"51710\",\"end\":\"51717\"},{\"start\":\"52022\",\"end\":\"52031\"},{\"start\":\"52035\",\"end\":\"52038\"},{\"start\":\"52045\",\"end\":\"52047\"},{\"start\":\"52051\",\"end\":\"52055\"},{\"start\":\"52394\",\"end\":\"52396\"},{\"start\":\"52400\",\"end\":\"52405\"},{\"start\":\"52409\",\"end\":\"52412\"},{\"start\":\"52416\",\"end\":\"52418\"},{\"start\":\"52732\",\"end\":\"52735\"},{\"start\":\"52739\",\"end\":\"52742\"},{\"start\":\"53014\",\"end\":\"53016\"},{\"start\":\"53020\",\"end\":\"53025\"},{\"start\":\"53351\",\"end\":\"53353\"},{\"start\":\"53357\",\"end\":\"53361\"},{\"start\":\"53645\",\"end\":\"53647\"},{\"start\":\"53651\",\"end\":\"53655\"},{\"start\":\"53977\",\"end\":\"53987\"},{\"start\":\"53991\",\"end\":\"53998\"},{\"start\":\"54361\",\"end\":\"54363\"},{\"start\":\"54367\",\"end\":\"54370\"},{\"start\":\"54714\",\"end\":\"54717\"},{\"start\":\"54721\",\"end\":\"54724\"},{\"start\":\"55024\",\"end\":\"55031\"},{\"start\":\"55037\",\"end\":\"55044\"},{\"start\":\"55048\",\"end\":\"55053\"},{\"start\":\"55380\",\"end\":\"55388\"},{\"start\":\"55392\",\"end\":\"55400\"},{\"start\":\"55404\",\"end\":\"55411\"},{\"start\":\"55719\",\"end\":\"55722\"},{\"start\":\"55726\",\"end\":\"55728\"},{\"start\":\"56049\",\"end\":\"56051\"},{\"start\":\"56058\",\"end\":\"56063\"},{\"start\":\"56310\",\"end\":\"56314\"},{\"start\":\"56318\",\"end\":\"56321\"},{\"start\":\"56325\",\"end\":\"56327\"},{\"start\":\"56561\",\"end\":\"56563\"},{\"start\":\"56570\",\"end\":\"56575\"},{\"start\":\"56579\",\"end\":\"56581\"},{\"start\":\"56585\",\"end\":\"56589\"},{\"start\":\"56593\",\"end\":\"56596\"},{\"start\":\"56894\",\"end\":\"56897\"},{\"start\":\"56901\",\"end\":\"56910\"},{\"start\":\"56914\",\"end\":\"56916\"},{\"start\":\"56920\",\"end\":\"56924\"},{\"start\":\"57269\",\"end\":\"57274\"},{\"start\":\"57558\",\"end\":\"57565\"},{\"start\":\"57569\",\"end\":\"57576\"},{\"start\":\"57894\",\"end\":\"57898\"},{\"start\":\"57902\",\"end\":\"57906\"},{\"start\":\"57910\",\"end\":\"57913\"},{\"start\":\"57917\",\"end\":\"57921\"},{\"start\":\"57925\",\"end\":\"57928\"},{\"start\":\"58205\",\"end\":\"58212\"},{\"start\":\"58218\",\"end\":\"58227\"},{\"start\":\"58504\",\"end\":\"58512\"},{\"start\":\"58516\",\"end\":\"58525\"},{\"start\":\"58836\",\"end\":\"58839\"},{\"start\":\"58843\",\"end\":\"58848\"},{\"start\":\"59191\",\"end\":\"59194\"},{\"start\":\"59201\",\"end\":\"59205\"},{\"start\":\"59209\",\"end\":\"59217\"},{\"start\":\"59606\",\"end\":\"59609\"},{\"start\":\"59613\",\"end\":\"59616\"},{\"start\":\"59620\",\"end\":\"59624\"},{\"start\":\"59905\",\"end\":\"59907\"},{\"start\":\"59911\",\"end\":\"59916\"},{\"start\":\"60236\",\"end\":\"60241\"},{\"start\":\"60245\",\"end\":\"60248\"},{\"start\":\"60252\",\"end\":\"60256\"},{\"start\":\"60260\",\"end\":\"60264\"},{\"start\":\"60268\",\"end\":\"60271\"},{\"start\":\"60275\",\"end\":\"60280\"},{\"start\":\"60698\",\"end\":\"60703\"},{\"start\":\"60707\",\"end\":\"60710\"},{\"start\":\"60980\",\"end\":\"60985\"},{\"start\":\"60989\",\"end\":\"60992\"},{\"start\":\"60996\",\"end\":\"60999\"},{\"start\":\"61309\",\"end\":\"61314\"},{\"start\":\"61677\",\"end\":\"61680\"},{\"start\":\"61684\",\"end\":\"61687\"},{\"start\":\"61691\",\"end\":\"61696\"},{\"start\":\"61999\",\"end\":\"62007\"},{\"start\":\"62011\",\"end\":\"62019\"},{\"start\":\"62023\",\"end\":\"62029\"},{\"start\":\"62386\",\"end\":\"62397\"},{\"start\":\"62401\",\"end\":\"62407\"},{\"start\":\"62411\",\"end\":\"62420\"},{\"start\":\"62821\",\"end\":\"62825\"},{\"start\":\"62829\",\"end\":\"62833\"},{\"start\":\"63151\",\"end\":\"63154\"},{\"start\":\"63158\",\"end\":\"63163\"},{\"start\":\"63167\",\"end\":\"63170\"},{\"start\":\"63493\",\"end\":\"63497\"},{\"start\":\"63501\",\"end\":\"63504\"},{\"start\":\"63508\",\"end\":\"63512\"},{\"start\":\"63516\",\"end\":\"63520\"},{\"start\":\"63524\",\"end\":\"63527\"},{\"start\":\"63909\",\"end\":\"63914\"},{\"start\":\"63918\",\"end\":\"63922\"},{\"start\":\"63926\",\"end\":\"63930\"},{\"start\":\"64252\",\"end\":\"64255\"},{\"start\":\"64259\",\"end\":\"64265\"},{\"start\":\"64610\",\"end\":\"64615\"},{\"start\":\"64619\",\"end\":\"64627\"},{\"start\":\"64631\",\"end\":\"64638\"},{\"start\":\"64642\",\"end\":\"64652\"},{\"start\":\"64656\",\"end\":\"64663\"},{\"start\":\"65028\",\"end\":\"65033\"},{\"start\":\"65037\",\"end\":\"65040\"},{\"start\":\"65044\",\"end\":\"65047\"},{\"start\":\"65328\",\"end\":\"65331\"},{\"start\":\"65335\",\"end\":\"65337\"},{\"start\":\"65341\",\"end\":\"65345\"},{\"start\":\"65673\",\"end\":\"65677\"},{\"start\":\"65681\",\"end\":\"65684\"},{\"start\":\"65688\",\"end\":\"65692\"},{\"start\":\"65696\",\"end\":\"65699\"},{\"start\":\"65703\",\"end\":\"65706\"},{\"start\":\"66037\",\"end\":\"66041\"},{\"start\":\"66045\",\"end\":\"66049\"},{\"start\":\"66314\",\"end\":\"66316\"},{\"start\":\"66320\",\"end\":\"66324\"},{\"start\":\"66328\",\"end\":\"66332\"},{\"start\":\"66681\",\"end\":\"66683\"},{\"start\":\"66687\",\"end\":\"66689\"},{\"start\":\"66693\",\"end\":\"66697\"},{\"start\":\"67005\",\"end\":\"67009\"},{\"start\":\"67013\",\"end\":\"67017\"},{\"start\":\"67021\",\"end\":\"67023\"},{\"start\":\"67027\",\"end\":\"67029\"},{\"start\":\"67033\",\"end\":\"67037\"},{\"start\":\"67396\",\"end\":\"67400\"},{\"start\":\"67404\",\"end\":\"67408\"},{\"start\":\"67412\",\"end\":\"67415\"},{\"start\":\"67419\",\"end\":\"67422\"},{\"start\":\"67426\",\"end\":\"67429\"},{\"start\":\"67433\",\"end\":\"67436\"},{\"start\":\"67707\",\"end\":\"67712\"},{\"start\":\"67995\",\"end\":\"67998\"},{\"start\":\"68002\",\"end\":\"68006\"},{\"start\":\"68010\",\"end\":\"68013\"},{\"start\":\"68017\",\"end\":\"68019\"}]", "bib_entry": "[{\"start\":\"51629\",\"end\":\"51953\",\"attributes\":{\"matched_paper_id\":\"13298437\",\"id\":\"b0\"}},{\"start\":\"51955\",\"end\":\"52320\",\"attributes\":{\"matched_paper_id\":\"15928602\",\"id\":\"b1\"}},{\"start\":\"52322\",\"end\":\"52669\",\"attributes\":{\"matched_paper_id\":\"12354538\",\"id\":\"b2\"}},{\"start\":\"52671\",\"end\":\"52892\",\"attributes\":{\"matched_paper_id\":\"29135140\",\"id\":\"b3\"}},{\"start\":\"52894\",\"end\":\"53261\",\"attributes\":{\"matched_paper_id\":\"4957467\",\"id\":\"b4\"}},{\"start\":\"53263\",\"end\":\"53547\",\"attributes\":{\"matched_paper_id\":\"19170594\",\"id\":\"b5\"}},{\"start\":\"53549\",\"end\":\"53918\",\"attributes\":{\"matched_paper_id\":\"67856333\",\"id\":\"b6\"}},{\"start\":\"53920\",\"end\":\"54236\",\"attributes\":{\"matched_paper_id\":\"478489\",\"id\":\"b7\"}},{\"start\":\"54238\",\"end\":\"54611\",\"attributes\":{\"matched_paper_id\":\"53287493\",\"id\":\"b8\"}},{\"start\":\"54613\",\"end\":\"54961\",\"attributes\":{\"matched_paper_id\":\"12965906\",\"id\":\"b9\"}},{\"start\":\"54963\",\"end\":\"55285\",\"attributes\":{\"id\":\"b10\"}},{\"start\":\"55287\",\"end\":\"55652\",\"attributes\":{\"matched_paper_id\":\"219683473\",\"id\":\"b11\"}},{\"start\":\"55654\",\"end\":\"55970\",\"attributes\":{\"matched_paper_id\":\"597379\",\"id\":\"b12\"}},{\"start\":\"55972\",\"end\":\"56245\",\"attributes\":{\"matched_paper_id\":\"5652420\",\"id\":\"b13\"}},{\"start\":\"56247\",\"end\":\"56498\",\"attributes\":{\"matched_paper_id\":\"206765283\",\"id\":\"b14\"}},{\"start\":\"56500\",\"end\":\"56819\",\"attributes\":{\"matched_paper_id\":\"46167880\",\"id\":\"b15\"}},{\"start\":\"56821\",\"end\":\"57154\",\"attributes\":{\"matched_paper_id\":\"2654595\",\"id\":\"b16\"}},{\"start\":\"57156\",\"end\":\"57479\",\"attributes\":{\"matched_paper_id\":\"18495197\",\"id\":\"b17\"}},{\"start\":\"57481\",\"end\":\"57809\",\"attributes\":{\"matched_paper_id\":\"5574079\",\"id\":\"b18\"}},{\"start\":\"57811\",\"end\":\"58138\",\"attributes\":{\"matched_paper_id\":\"10751352\",\"id\":\"b19\"}},{\"start\":\"58140\",\"end\":\"58432\",\"attributes\":{\"matched_paper_id\":\"52271360\",\"id\":\"b20\"}},{\"start\":\"58434\",\"end\":\"58754\",\"attributes\":{\"matched_paper_id\":\"11797475\",\"id\":\"b21\"}},{\"start\":\"58756\",\"end\":\"59116\",\"attributes\":{\"matched_paper_id\":\"49568792\",\"id\":\"b22\"}},{\"start\":\"59118\",\"end\":\"59525\",\"attributes\":{\"matched_paper_id\":\"14239485\",\"id\":\"b23\"}},{\"start\":\"59527\",\"end\":\"59820\",\"attributes\":{\"matched_paper_id\":\"43823483\",\"id\":\"b24\"}},{\"start\":\"59822\",\"end\":\"60126\",\"attributes\":{\"matched_paper_id\":\"19882248\",\"id\":\"b25\"}},{\"start\":\"60128\",\"end\":\"60599\",\"attributes\":{\"matched_paper_id\":\"12699455\",\"id\":\"b26\"}},{\"start\":\"60601\",\"end\":\"60907\",\"attributes\":{\"matched_paper_id\":\"5037813\",\"id\":\"b27\"}},{\"start\":\"60909\",\"end\":\"61214\",\"attributes\":{\"matched_paper_id\":\"49671086\",\"id\":\"b28\"}},{\"start\":\"61216\",\"end\":\"61584\",\"attributes\":{\"matched_paper_id\":\"91184299\",\"id\":\"b29\"}},{\"start\":\"61586\",\"end\":\"61918\",\"attributes\":{\"matched_paper_id\":\"86645950\",\"id\":\"b30\"}},{\"start\":\"61920\",\"end\":\"62302\",\"attributes\":{\"matched_paper_id\":\"16065617\",\"id\":\"b31\"}},{\"start\":\"62304\",\"end\":\"62698\",\"attributes\":{\"matched_paper_id\":\"1732632\",\"id\":\"b32\"}},{\"start\":\"62700\",\"end\":\"63062\",\"attributes\":{\"matched_paper_id\":\"46932096\",\"id\":\"b33\"}},{\"start\":\"63064\",\"end\":\"63395\",\"attributes\":{\"matched_paper_id\":\"19167105\",\"id\":\"b34\"}},{\"start\":\"63397\",\"end\":\"63810\",\"attributes\":{\"matched_paper_id\":\"14298099\",\"id\":\"b35\"}},{\"start\":\"63812\",\"end\":\"64168\",\"attributes\":{\"matched_paper_id\":\"52017199\",\"id\":\"b36\"}},{\"start\":\"64170\",\"end\":\"64548\",\"attributes\":{\"matched_paper_id\":\"201081367\",\"id\":\"b37\"}},{\"start\":\"64550\",\"end\":\"64942\",\"attributes\":{\"matched_paper_id\":\"1171329\",\"id\":\"b38\"}},{\"start\":\"64944\",\"end\":\"65244\",\"attributes\":{\"matched_paper_id\":\"807678\",\"id\":\"b39\"}},{\"start\":\"65246\",\"end\":\"65574\",\"attributes\":{\"matched_paper_id\":\"54048454\",\"id\":\"b40\"}},{\"start\":\"65576\",\"end\":\"65954\",\"attributes\":{\"matched_paper_id\":\"3438487\",\"id\":\"b41\"}},{\"start\":\"65956\",\"end\":\"66233\",\"attributes\":{\"matched_paper_id\":\"14389453\",\"id\":\"b42\"}},{\"start\":\"66235\",\"end\":\"66593\",\"attributes\":{\"matched_paper_id\":\"8040013\",\"id\":\"b43\"}},{\"start\":\"66595\",\"end\":\"66922\",\"attributes\":{\"matched_paper_id\":\"17938825\",\"id\":\"b44\"}},{\"start\":\"66924\",\"end\":\"67316\",\"attributes\":{\"matched_paper_id\":\"52245597\",\"id\":\"b45\"}},{\"start\":\"67318\",\"end\":\"67647\",\"attributes\":{\"matched_paper_id\":\"20757288\",\"id\":\"b46\"}},{\"start\":\"67649\",\"end\":\"67903\",\"attributes\":{\"matched_paper_id\":\"1318262\",\"id\":\"b47\"}},{\"start\":\"67905\",\"end\":\"68252\",\"attributes\":{\"matched_paper_id\":\"56937749\",\"id\":\"b48\"}}]", "bib_title": "[{\"start\":\"51629\",\"end\":\"51661\"},{\"start\":\"51955\",\"end\":\"52018\"},{\"start\":\"52322\",\"end\":\"52390\"},{\"start\":\"52671\",\"end\":\"52728\"},{\"start\":\"52894\",\"end\":\"53010\"},{\"start\":\"53263\",\"end\":\"53347\"},{\"start\":\"53549\",\"end\":\"53641\"},{\"start\":\"53920\",\"end\":\"53971\"},{\"start\":\"54238\",\"end\":\"54357\"},{\"start\":\"54613\",\"end\":\"54710\"},{\"start\":\"55287\",\"end\":\"55376\"},{\"start\":\"55654\",\"end\":\"55715\"},{\"start\":\"55972\",\"end\":\"56042\"},{\"start\":\"56247\",\"end\":\"56306\"},{\"start\":\"56500\",\"end\":\"56554\"},{\"start\":\"56821\",\"end\":\"56890\"},{\"start\":\"57156\",\"end\":\"57265\"},{\"start\":\"57481\",\"end\":\"57554\"},{\"start\":\"57811\",\"end\":\"57890\"},{\"start\":\"58140\",\"end\":\"58201\"},{\"start\":\"58434\",\"end\":\"58500\"},{\"start\":\"58756\",\"end\":\"58832\"},{\"start\":\"59118\",\"end\":\"59187\"},{\"start\":\"59527\",\"end\":\"59602\"},{\"start\":\"59822\",\"end\":\"59901\"},{\"start\":\"60128\",\"end\":\"60232\"},{\"start\":\"60601\",\"end\":\"60691\"},{\"start\":\"60909\",\"end\":\"60973\"},{\"start\":\"61216\",\"end\":\"61302\"},{\"start\":\"61586\",\"end\":\"61673\"},{\"start\":\"61920\",\"end\":\"61995\"},{\"start\":\"62304\",\"end\":\"62382\"},{\"start\":\"62700\",\"end\":\"62817\"},{\"start\":\"63064\",\"end\":\"63147\"},{\"start\":\"63397\",\"end\":\"63489\"},{\"start\":\"63812\",\"end\":\"63905\"},{\"start\":\"64170\",\"end\":\"64246\"},{\"start\":\"64550\",\"end\":\"64606\"},{\"start\":\"64944\",\"end\":\"65024\"},{\"start\":\"65246\",\"end\":\"65324\"},{\"start\":\"65576\",\"end\":\"65669\"},{\"start\":\"65956\",\"end\":\"66033\"},{\"start\":\"66235\",\"end\":\"66310\"},{\"start\":\"66595\",\"end\":\"66677\"},{\"start\":\"66924\",\"end\":\"67001\"},{\"start\":\"67318\",\"end\":\"67392\"},{\"start\":\"67649\",\"end\":\"67703\"},{\"start\":\"67905\",\"end\":\"67991\"}]", "bib_author": "[{\"start\":\"51663\",\"end\":\"51675\"},{\"start\":\"51675\",\"end\":\"51683\"},{\"start\":\"51683\",\"end\":\"51697\"},{\"start\":\"51697\",\"end\":\"51708\"},{\"start\":\"51708\",\"end\":\"51719\"},{\"start\":\"52020\",\"end\":\"52033\"},{\"start\":\"52033\",\"end\":\"52040\"},{\"start\":\"52040\",\"end\":\"52049\"},{\"start\":\"52049\",\"end\":\"52057\"},{\"start\":\"52392\",\"end\":\"52398\"},{\"start\":\"52398\",\"end\":\"52407\"},{\"start\":\"52407\",\"end\":\"52414\"},{\"start\":\"52414\",\"end\":\"52420\"},{\"start\":\"52730\",\"end\":\"52737\"},{\"start\":\"52737\",\"end\":\"52744\"},{\"start\":\"53012\",\"end\":\"53018\"},{\"start\":\"53018\",\"end\":\"53027\"},{\"start\":\"53349\",\"end\":\"53355\"},{\"start\":\"53355\",\"end\":\"53363\"},{\"start\":\"53643\",\"end\":\"53649\"},{\"start\":\"53649\",\"end\":\"53657\"},{\"start\":\"53973\",\"end\":\"53989\"},{\"start\":\"53989\",\"end\":\"54000\"},{\"start\":\"54359\",\"end\":\"54365\"},{\"start\":\"54365\",\"end\":\"54372\"},{\"start\":\"54712\",\"end\":\"54719\"},{\"start\":\"54719\",\"end\":\"54726\"},{\"start\":\"55020\",\"end\":\"55033\"},{\"start\":\"55033\",\"end\":\"55046\"},{\"start\":\"55046\",\"end\":\"55055\"},{\"start\":\"55378\",\"end\":\"55390\"},{\"start\":\"55390\",\"end\":\"55402\"},{\"start\":\"55402\",\"end\":\"55413\"},{\"start\":\"55717\",\"end\":\"55724\"},{\"start\":\"55724\",\"end\":\"55730\"},{\"start\":\"56044\",\"end\":\"56053\"},{\"start\":\"56053\",\"end\":\"56065\"},{\"start\":\"56308\",\"end\":\"56316\"},{\"start\":\"56316\",\"end\":\"56323\"},{\"start\":\"56323\",\"end\":\"56329\"},{\"start\":\"56556\",\"end\":\"56565\"},{\"start\":\"56565\",\"end\":\"56577\"},{\"start\":\"56577\",\"end\":\"56583\"},{\"start\":\"56583\",\"end\":\"56591\"},{\"start\":\"56591\",\"end\":\"56598\"},{\"start\":\"56892\",\"end\":\"56899\"},{\"start\":\"56899\",\"end\":\"56912\"},{\"start\":\"56912\",\"end\":\"56918\"},{\"start\":\"56918\",\"end\":\"56926\"},{\"start\":\"57267\",\"end\":\"57276\"},{\"start\":\"57556\",\"end\":\"57567\"},{\"start\":\"57567\",\"end\":\"57578\"},{\"start\":\"57892\",\"end\":\"57900\"},{\"start\":\"57900\",\"end\":\"57908\"},{\"start\":\"57908\",\"end\":\"57915\"},{\"start\":\"57915\",\"end\":\"57923\"},{\"start\":\"57923\",\"end\":\"57930\"},{\"start\":\"58203\",\"end\":\"58214\"},{\"start\":\"58214\",\"end\":\"58229\"},{\"start\":\"58502\",\"end\":\"58514\"},{\"start\":\"58514\",\"end\":\"58527\"},{\"start\":\"58834\",\"end\":\"58841\"},{\"start\":\"58841\",\"end\":\"58850\"},{\"start\":\"59189\",\"end\":\"59196\"},{\"start\":\"59196\",\"end\":\"59207\"},{\"start\":\"59207\",\"end\":\"59219\"},{\"start\":\"59604\",\"end\":\"59611\"},{\"start\":\"59611\",\"end\":\"59618\"},{\"start\":\"59618\",\"end\":\"59626\"},{\"start\":\"59903\",\"end\":\"59909\"},{\"start\":\"59909\",\"end\":\"59918\"},{\"start\":\"60234\",\"end\":\"60243\"},{\"start\":\"60243\",\"end\":\"60250\"},{\"start\":\"60250\",\"end\":\"60258\"},{\"start\":\"60258\",\"end\":\"60266\"},{\"start\":\"60266\",\"end\":\"60273\"},{\"start\":\"60273\",\"end\":\"60282\"},{\"start\":\"60693\",\"end\":\"60705\"},{\"start\":\"60705\",\"end\":\"60712\"},{\"start\":\"60975\",\"end\":\"60987\"},{\"start\":\"60987\",\"end\":\"60994\"},{\"start\":\"60994\",\"end\":\"61001\"},{\"start\":\"61304\",\"end\":\"61316\"},{\"start\":\"61675\",\"end\":\"61682\"},{\"start\":\"61682\",\"end\":\"61689\"},{\"start\":\"61689\",\"end\":\"61698\"},{\"start\":\"61997\",\"end\":\"62009\"},{\"start\":\"62009\",\"end\":\"62021\"},{\"start\":\"62021\",\"end\":\"62031\"},{\"start\":\"62384\",\"end\":\"62399\"},{\"start\":\"62399\",\"end\":\"62409\"},{\"start\":\"62409\",\"end\":\"62422\"},{\"start\":\"62819\",\"end\":\"62827\"},{\"start\":\"62827\",\"end\":\"62835\"},{\"start\":\"63149\",\"end\":\"63156\"},{\"start\":\"63156\",\"end\":\"63165\"},{\"start\":\"63165\",\"end\":\"63172\"},{\"start\":\"63491\",\"end\":\"63499\"},{\"start\":\"63499\",\"end\":\"63506\"},{\"start\":\"63506\",\"end\":\"63514\"},{\"start\":\"63514\",\"end\":\"63522\"},{\"start\":\"63522\",\"end\":\"63529\"},{\"start\":\"63907\",\"end\":\"63916\"},{\"start\":\"63916\",\"end\":\"63924\"},{\"start\":\"63924\",\"end\":\"63932\"},{\"start\":\"64248\",\"end\":\"64257\"},{\"start\":\"64257\",\"end\":\"64267\"},{\"start\":\"64608\",\"end\":\"64617\"},{\"start\":\"64617\",\"end\":\"64629\"},{\"start\":\"64629\",\"end\":\"64640\"},{\"start\":\"64640\",\"end\":\"64654\"},{\"start\":\"64654\",\"end\":\"64665\"},{\"start\":\"65026\",\"end\":\"65035\"},{\"start\":\"65035\",\"end\":\"65042\"},{\"start\":\"65042\",\"end\":\"65049\"},{\"start\":\"65326\",\"end\":\"65333\"},{\"start\":\"65333\",\"end\":\"65339\"},{\"start\":\"65339\",\"end\":\"65347\"},{\"start\":\"65671\",\"end\":\"65679\"},{\"start\":\"65679\",\"end\":\"65686\"},{\"start\":\"65686\",\"end\":\"65694\"},{\"start\":\"65694\",\"end\":\"65701\"},{\"start\":\"65701\",\"end\":\"65708\"},{\"start\":\"66035\",\"end\":\"66043\"},{\"start\":\"66043\",\"end\":\"66051\"},{\"start\":\"66312\",\"end\":\"66318\"},{\"start\":\"66318\",\"end\":\"66326\"},{\"start\":\"66326\",\"end\":\"66334\"},{\"start\":\"66679\",\"end\":\"66685\"},{\"start\":\"66685\",\"end\":\"66691\"},{\"start\":\"66691\",\"end\":\"66699\"},{\"start\":\"67003\",\"end\":\"67011\"},{\"start\":\"67011\",\"end\":\"67019\"},{\"start\":\"67019\",\"end\":\"67025\"},{\"start\":\"67025\",\"end\":\"67031\"},{\"start\":\"67031\",\"end\":\"67039\"},{\"start\":\"67394\",\"end\":\"67402\"},{\"start\":\"67402\",\"end\":\"67410\"},{\"start\":\"67410\",\"end\":\"67417\"},{\"start\":\"67417\",\"end\":\"67424\"},{\"start\":\"67424\",\"end\":\"67431\"},{\"start\":\"67431\",\"end\":\"67438\"},{\"start\":\"67705\",\"end\":\"67714\"},{\"start\":\"67993\",\"end\":\"68000\"},{\"start\":\"68000\",\"end\":\"68008\"},{\"start\":\"68008\",\"end\":\"68015\"},{\"start\":\"68015\",\"end\":\"68021\"}]", "bib_venue": "[{\"start\":\"51719\",\"end\":\"51757\"},{\"start\":\"52057\",\"end\":\"52100\"},{\"start\":\"52420\",\"end\":\"52457\"},{\"start\":\"52744\",\"end\":\"52760\"},{\"start\":\"53027\",\"end\":\"53048\"},{\"start\":\"53363\",\"end\":\"53388\"},{\"start\":\"53657\",\"end\":\"53700\"},{\"start\":\"54000\",\"end\":\"54043\"},{\"start\":\"54372\",\"end\":\"54410\"},{\"start\":\"54726\",\"end\":\"54757\"},{\"start\":\"54963\",\"end\":\"55018\"},{\"start\":\"55413\",\"end\":\"55441\"},{\"start\":\"55730\",\"end\":\"55776\"},{\"start\":\"56065\",\"end\":\"56069\"},{\"start\":\"56329\",\"end\":\"56333\"},{\"start\":\"56598\",\"end\":\"56629\"},{\"start\":\"56926\",\"end\":\"56957\"},{\"start\":\"57276\",\"end\":\"57280\"},{\"start\":\"57578\",\"end\":\"57615\"},{\"start\":\"57930\",\"end\":\"57950\"},{\"start\":\"58229\",\"end\":\"58258\"},{\"start\":\"58527\",\"end\":\"58564\"},{\"start\":\"58850\",\"end\":\"58897\"},{\"start\":\"59219\",\"end\":\"59278\"},{\"start\":\"59626\",\"end\":\"59648\"},{\"start\":\"59918\",\"end\":\"59946\"},{\"start\":\"60282\",\"end\":\"60325\"},{\"start\":\"60712\",\"end\":\"60716\"},{\"start\":\"61001\",\"end\":\"61032\"},{\"start\":\"61316\",\"end\":\"61363\"},{\"start\":\"61698\",\"end\":\"61726\"},{\"start\":\"62031\",\"end\":\"62074\"},{\"start\":\"62422\",\"end\":\"62465\"},{\"start\":\"62835\",\"end\":\"62851\"},{\"start\":\"63172\",\"end\":\"63210\"},{\"start\":\"63529\",\"end\":\"63567\"},{\"start\":\"63932\",\"end\":\"63960\"},{\"start\":\"64267\",\"end\":\"64315\"},{\"start\":\"64665\",\"end\":\"64708\"},{\"start\":\"65049\",\"end\":\"65069\"},{\"start\":\"65347\",\"end\":\"65383\"},{\"start\":\"65708\",\"end\":\"65736\"},{\"start\":\"66051\",\"end\":\"66055\"},{\"start\":\"66334\",\"end\":\"66377\"},{\"start\":\"66699\",\"end\":\"66729\"},{\"start\":\"67039\",\"end\":\"67082\"},{\"start\":\"67438\",\"end\":\"67454\"},{\"start\":\"67714\",\"end\":\"67745\"},{\"start\":\"68021\",\"end\":\"68049\"},{\"start\":\"52102\",\"end\":\"52139\"},{\"start\":\"52459\",\"end\":\"52490\"},{\"start\":\"53050\",\"end\":\"53065\"},{\"start\":\"53702\",\"end\":\"53739\"},{\"start\":\"54045\",\"end\":\"54082\"},{\"start\":\"54759\",\"end\":\"54784\"},{\"start\":\"55443\",\"end\":\"55465\"},{\"start\":\"55778\",\"end\":\"55818\"},{\"start\":\"56071\",\"end\":\"56075\"},{\"start\":\"56335\",\"end\":\"56339\"},{\"start\":\"56631\",\"end\":\"56656\"},{\"start\":\"56959\",\"end\":\"56984\"},{\"start\":\"57282\",\"end\":\"57286\"},{\"start\":\"57617\",\"end\":\"57644\"},{\"start\":\"57952\",\"end\":\"57966\"},{\"start\":\"58260\",\"end\":\"58283\"},{\"start\":\"58566\",\"end\":\"58593\"},{\"start\":\"58899\",\"end\":\"58940\"},{\"start\":\"59280\",\"end\":\"59333\"},{\"start\":\"59650\",\"end\":\"59666\"},{\"start\":\"59948\",\"end\":\"59970\"},{\"start\":\"60327\",\"end\":\"60364\"},{\"start\":\"60718\",\"end\":\"60722\"},{\"start\":\"61034\",\"end\":\"61059\"},{\"start\":\"61365\",\"end\":\"61406\"},{\"start\":\"61728\",\"end\":\"61750\"},{\"start\":\"62076\",\"end\":\"62113\"},{\"start\":\"62467\",\"end\":\"62504\"},{\"start\":\"62853\",\"end\":\"62863\"},{\"start\":\"63569\",\"end\":\"63601\"},{\"start\":\"63962\",\"end\":\"63984\"},{\"start\":\"64317\",\"end\":\"64359\"},{\"start\":\"64710\",\"end\":\"64747\"},{\"start\":\"65071\",\"end\":\"65085\"},{\"start\":\"65385\",\"end\":\"65415\"},{\"start\":\"65738\",\"end\":\"65760\"},{\"start\":\"66057\",\"end\":\"66061\"},{\"start\":\"66379\",\"end\":\"66416\"},{\"start\":\"66731\",\"end\":\"66751\"},{\"start\":\"67084\",\"end\":\"67121\"},{\"start\":\"67456\",\"end\":\"67466\"},{\"start\":\"67747\",\"end\":\"67772\"},{\"start\":\"68051\",\"end\":\"68073\"}]"}}}, "year": 2023, "month": 12, "day": 17}