{"id": 246063414, "updated": "2023-10-05 17:43:32.156", "metadata": {"title": "Learning robust perceptive locomotion for quadrupedal robots in the wild", "authors": "[{\"first\":\"Takahiro\",\"last\":\"Miki\",\"middle\":[]},{\"first\":\"Joonho\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Jemin\",\"last\":\"Hwangbo\",\"middle\":[]},{\"first\":\"Lorenz\",\"last\":\"Wellhausen\",\"middle\":[]},{\"first\":\"Vladlen\",\"last\":\"Koltun\",\"middle\":[]},{\"first\":\"Marco\",\"last\":\"Hutter\",\"middle\":[]}]", "venue": "Science Robotics, 19 Jan 2022, Vol 7, Issue 62", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Legged robots that can operate autonomously in remote and hazardous environments will greatly increase opportunities for exploration into under-explored areas. Exteroceptive perception is crucial for fast and energy-efficient locomotion: perceiving the terrain before making contact with it enables planning and adaptation of the gait ahead of time to maintain speed and stability. However, utilizing exteroceptive perception robustly for locomotion has remained a grand challenge in robotics. Snow, vegetation, and water visually appear as obstacles on which the robot cannot step~-- or are missing altogether due to high reflectance. Additionally, depth perception can degrade due to difficult lighting, dust, fog, reflective or transparent surfaces, sensor occlusion, and more. For this reason, the most robust and general solutions to legged locomotion to date rely solely on proprioception. This severely limits locomotion speed, because the robot has to physically feel out the terrain before adapting its gait accordingly. Here we present a robust and general solution to integrating exteroceptive and proprioceptive perception for legged locomotion. We leverage an attention-based recurrent encoder that integrates proprioceptive and exteroceptive input. The encoder is trained end-to-end and learns to seamlessly combine the different perception modalities without resorting to heuristics. The result is a legged locomotion controller with high robustness and speed. The controller was tested in a variety of challenging natural and urban environments over multiple seasons and completed an hour-long hike in the Alps in the time recommended for human hikers.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.08117", "mag": null, "acl": null, "pubmed": "35044798", "pubmedcentral": null, "dblp": "journals/scirobotics/MikiLHWKH22", "doi": "10.1126/scirobotics.abk2822"}}, "content": {"source": {"pdf_hash": "5da5c2167a85ecb5d1ea22656ae36fdf995df0f2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.08117v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2201.08117", "status": "GREEN"}}, "grobid": {"id": "421900086d2f792fe06d033a9e560f1f67e470fe", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5da5c2167a85ecb5d1ea22656ae36fdf995df0f2.txt", "contents": "\nLearning robust perceptive locomotion for quadrupedal robots in the wild\n\n\nTakahiro Miki \nRobotic Systems Lab\nETH Zurich\nZurichSwitzerland\n\nJoonho Lee \nRobotic Systems Lab\nETH Zurich\nZurichSwitzerland\n\nJemin Hwangbo \nRobotics and Artificial Intelligence Lab\nKAIST\nDaejeonKorea\n\nLorenz Wellhausen \nRobotic Systems Lab\nETH Zurich\nZurichSwitzerland\n\nVladlen Koltun \nIntelligent Systems Lab\nIntel\nJacksonWYUSA\n\nMarco Hutter \nRobotic Systems Lab\nETH Zurich\nZurichSwitzerland\n\nLearning robust perceptive locomotion for quadrupedal robots in the wild\nCompiled January 21, 2022Research Article ETH Zurich and Intel 1\nLegged robots that can operate autonomously in remote and hazardous environments will greatly increase opportunities for exploration into under-explored areas. Exteroceptive perception is crucial for fast and energyefficient locomotion: perceiving the terrain before making contact with it enables planning and adaptation of the gait ahead of time to maintain speed and stability. However, utilizing exteroceptive perception robustly for locomotion has remained a grand challenge in robotics. Snow, vegetation, and water visually appear as obstacles on which the robot cannot step -or are missing altogether due to high reflectance. Additionally, depth perception can degrade due to difficult lighting, dust, fog, reflective or transparent surfaces, sensor occlusion, and more. For this reason, the most robust and general solutions to legged locomotion to date rely solely on proprioception. This severely limits locomotion speed, because the robot has to physically feel out the terrain before adapting its gait accordingly. Here we present a robust and general solution to integrating exteroceptive and proprioceptive perception for legged locomotion. We leverage an attention-based recurrent encoder that integrates proprioceptive and exteroceptive input. The encoder is trained end-to-end and learns to seamlessly combine the different perception modalities without resorting to heuristics. The result is a legged locomotion controller with high robustness and speed. The controller was tested in a variety of challenging natural and urban environments over multiple seasons and completed an hour-long hike in the Alps in the time recommended for human hikers.\n\nINTRODUCTION\n\nLegged robots can carry out missions in challenging environments that are too far or too dangerous for humans, such as hazardous areas and the surfaces of other planets. Legs can walk over challenging terrain with steep slopes, steps, and gaps that may impede wheeled or tracked vehicles of similar size. There has been notable progress in legged robotics [1][2][3][4][5] and several commercial platforms are being deployed in the real world [6][7][8][9][10].\n\nHowever, until now, legged robots could not match the performance of animals in traversing challenging real-world terrain. Many legged animals such as humans and dogs can briskly walk or run in such environments by foreseeing the upcoming terrain and planning their footsteps based on visual information [11]. Animals naturally combine proprioception and exteroception to adapt to highly irregular terrain shape and surface properties such as slipperiness or softness, even when visual perception is limited. Endowing legged robots with this ability is a grand challenge in robotics.\n\nOne of the biggest difficulties lies in reliable interpretation of incomplete and noisy perception for control. Exteroceptive information provided by onboard sensors is incomplete and often unreliable in realworld environments. Stereo camera based depth sensors, which most existing legged robots rely on [6,9,12], require texture to perform stereo matching and consequently struggle with low-texture surfaces or when parts of the image are under or overexposed. Time of Flight (ToF) cameras often fail to perceive dark surfaces and become noisy under sunlight [13]. Generally, sensors which rely on light to infer distance are prone to producing artifacts on highly reflective surfaces, since the sensors assume that light travels in a straight path. In addition, depth sensors by nature cannot distinguish soft unstable surfaces such as vegetation from rigid ones. An elevation map is commonly used to represent geometric terrain information extracted from depth sensor measurements [14][15][16][17]. It relies on the robot's estimated pose and is therefore affected by errors in this estimate. Other common sources of uncertainty in the map are occlusion or temporal inconsistency of the measurements due to dynamic objects. Most existing methods that rely on onboard terrain perception are still vulnerable to these failures.\n\nConventional approaches assume that the terrain information and any uncertainties encoded in the map are reasonably accurate, and the focus shifts solely to generating the motion. Offline methods use a prescanned terrain map, compute a handcrafted cost function over the map, and optimize a trajectory which is replayed on the robot [18,19]. They assume perfect knowledge of the full terrain and robot states and plan complex motions with long planning times. Online methods generally employ a similar approach but use only onboard resources to construct a map and continuously replan trajectories during execution [20-24]. Robust locomotion in the wild. The presented locomotion controller was extensively tested in a variety of complex environments over multiple seasons. The controller overcame a whole spectrum of real-world challenges, often encountering them in combination. These include slippery surfaces, steep inclinations, complex terrain, and vegetation in natural environments. In search-and-rescue scenarios, the controller dealt with steep stairs, unknown payloads, and perception-degrading fog. Reflective surfaces, loose ground, low light, and water puddles were encountered in underground cave systems. Soft and slippery snow piled up in the winter. The controller traversed these environments with zero failures.\n\nRecently, faster locomotion has been achieved by reducing the planning time with heuristics [25][26][27] or using Convolutional Neural Networks (CNN) to calculate foothold cost more efficiently [27]. Recently, a bipedal robot Atlas demonstrated parkour over complex obstacles [28]. It leverages pre-planned motion reference and optimizes its motion online by utilizing onboard LiDAR sensor data. Overall, the focus of all the approaches mentioned above is on picking footholds and generating trajectories given accurate terrain information. Some works [14,17] represent the statistical uncertainty of the measurements in the map, but its use is limited to heuristically defined foot placement rules to avoid risky areas [24]. Such methods can only handle explicitly modeled uncertainties and are not robust to the variety of perception failures encountered in the wild.\n\nData-driven methods have recently been introduced in order to incorporate more complex dynamics without compromising real-time performance. Learning-based quadrupedal or bipedal locomotion for simulated characters has been achieved by using reinforcement learning (RL) [29-32] and realistic robot models were used in recent works [33]. However, these works were only conducted in simulation. Recently, RL based locomotion controllers have been successfully transferred to physical robots [3,4,[34][35][36][37][38][39][40]. Hwangbo et al. [3,41] realized quadrupedal locomotion and recovery on flat ground with a physical robot by using learned actuator dynamics to facilitate simulation-toreality (sim-to-real) transfer. Lee et al. [4] extended this approach and enabled rough-terrain locomotion by simulating challenging terrain in a privileged training setup with an adaptive curriculum. Peng et al. [35] used imitation learning to transfer animal motion to a legged robot. However, these methods do not use any visual information.\n\nIn order to add exteroceptive information to locomotion learning, Gangapurwala et al. [42] combined a learning-based foothold planner and a model-based whole-body motion controller to transfer policies to the real world in a laboratory setting. Their applications are limited to rigid terrain with mostly flat surfaces and are still constrained in their deployment range. Their performance is tightly bound to the quality of the map, which often becomes unreliable in the field.\n\nIn both model-based and learning-based approaches, the assumption of flawless map quality precludes the application of these methods in uncontrolled outdoor environments. Handling uncertainties in terrain perception remains an open problem. Existing controllers avoid catastrophic failures by simply refraining from using visual information in outdoor environments [2,4,38] or by adding heuristically defined reflex rules [43,44].\n\nHere we present a terrain-aware locomotion controller for quadrupedal robots that overcomes limitations of previous approaches and enables robust traversal of harsh natural terrain at unprecedented speeds (Movie 1). At its core, the controller is based on a principled solution to incorporating exteroceptive perception into locomotion control.\n\nThe key component is a recurrent encoder that combines proprioception and exteroception into an integrated belief state. The encoder is trained in simulation to capture ground-truth information about the terrain given exteroceptive observations that may be incomplete, biased, and noisy. The belief state encoder is trained end-to-end to integrate proprioceptive and exteroceptive data without resorting to heuristics. It learns to take advantage of the foresight afforded by exteroception to plan footholds and accelerate locomotion when exteroception is reliable, and can seamlessly fall back to robust proprioceptive locomotion when needed. The learned controller thus combines the best of both worlds: the speed and efficiency afforded by exteroception and the robustness of proprioception.\n\nThe controller is trained via privileged learning [45]. We first train a teacher policy via Reinforcement Learning (RL) with full access to privileged information in the form of the ground-truth state of the environment. This privileged training enables the teacher policy to discover the optimal behavior given perfect knowledge of the terrain. We then train a student policy that only has access to information that is available in the field on the physical robot. The student policy is built around our belief state encoder and trained via imitation learning. The student policy learns to predict the teacher's optimal action given only partial and noisy observations of the environment.\n\nOnce the student policy is trained, we deploy it on the robot without any fine-tuning. The controller gets onboard sensor observations and a desired velocity command, and outputs each joint's target position as the action. The robot perceives the environment by leveraging a robotcentric elevation map. The elevation map serves as an abstraction layer between sensors and the locomotion controller, making our method independent of depth sensor choices. It works with no fine-tuning with different sensors, such as stereo cameras or LiDAR. Since the policy was trained to handle significant noise, bias, and gaps in the elevation map, the robot can continue walking even when mapping fails or the sensors are physically broken.\n\nThe presented approach achieves substantial improvements over the state of the art [4] in locomotion speed and obstacle traversability while maintaining exceptional robustness. Our key contribution is a method for combining multi-modal perception and demonstrating with extensive hardware experiments that the resulting control policy is robust against various exteroceptive failures. Handling exteroception failures has been a challenging problem in robotics. Our approach constitutes a general framework for robust deployment of complex autonomous machines in the wild.\n\n\nRESULTS\n\n\nFast and robust locomotion in the wild\n\nWe deployed our controller in a wide variety of terrain, as shown in Figure 1 and Movie 1. This includes alpine, forest, underground, and urban environments. The controller was consistently robust and had zero falls during all deployments. Because of the exteroceptive perception, the robot could anticipate the terrain and adapt its motion to achieve fast and smooth walking. This was particularly notable for structures that require high foot clearance, such as stairs and large obstacles. The robot was able to leverage exteroceptive input to conquer terrain that was beyond the capabilities of prior work that did not utilize exteroception [4].\n\nANYmal successfully traversed challenging natural environments with steep inclination, slippery surfaces, grass, and snow ( Figure 1 A-J). The robot was robust in these conditions, even when occlusion and surface properties such as high reflectance impeded exteroception. Our controller was also robustly deployed in underground environments with loose gravel, sand, dust, water, and limited illumination (Figure 1  K-N).\n\nUrban environments also present important challenges (Figure 1 O-R). For traversing stairs, the state-of-the-art quadrupedal robot Spot from Boston Dynamics requires that a dedicated mode is engaged, and the robot must be properly oriented with respect to the stairs [p. 33 44]. In contrast, our controller does not require any special mode for stairs, and can traverse stairs natively in any direction and any orientation, such as sideways, diagonally, and turning around on the stairway. See Movie S1 for demonstrations of smooth and robust stair traversal in arbitrary direction with our controller.\n\nThe controller was also robust to combinations of different challenges, as can be seen with snow on stairs in Figure 1R. Snow makes stairs slippery and yields incomplete and erroneous exteroceptive data. Depth sensors either fail due to the high reflectivity of snow, or estimate the surface profile to be on top of the snow, whereas the robot's legs sink below this level. Foot slippage in snow can also cause large\n\n\nFig. 2.\n\nA hike on the Etzel mountain in Switzerland, completed by ANYmal with our locomotion controller. The 2.2km route -with 120m of elevation gain and inclinations up to 38% -encompasses a variety of challenging terrain. ANYmal reached the summit faster than the human time indicated in the official signage, and finished the entire route in virtually the same time as given by a hiking guide [46]. drift in the kinematic pose estimation [47], making the map even more inconsistent. Nevertheless, the controller remained consistently robust, with zero failures in this regime as well.\n\n\nA hike in the Alps\n\nTo further evaluate the robustness of our controller, we conducted a hiking experiment in which we tested if ANYmal could complete an hour-long hiking loop on the Etzel mountain in Switzerland. The hiking route was 2.2 km long, with an elevation gain of 120 m. Completing the trail required traversing steep inclinations, high steps, rocky surfaces, slippery ground, and tree roots ( Figure 2). As seen in Movie 2, ANYmal completed the entire hike without any failure, stopping only to fix a detached shoe and swap batteries. The robot was able to reach the summit in 31 minutes, which is faster than the expected human hiking duration indicated in the official signage (35 minutes as shown in Figure 2), and finished the entire path in 78 minutesvirtually the same duration suggested by a hiking planner (76 minutes), which rates the hike \"difficult\" [46]. The difficulty levels are chosen from \"easy\", \"moderate\", and \"difficult\", calculated by combining the required fitness level, sport type, and the technical complexity [48].\n\nDuring the hike, the controller faced various challenges. The ascending path reached inclinations of up to 38% with rocky and wet surfaces ( Figure 2 (B-C)). On the descent through a forest, tree roots formed intricate obstacles and the ground proved very slippery ( Figure  2 (G-H)).\n\nVegetation above the robot sometimes introduced severe artifacts into the estimated elevation map. Despite all the challenges, the robot finished the hike without any human help and without a single fall.\n\n\nExteroceptive challenges\n\nIn this section, we examine how the terrain was perceived by the robot in conditions that are challenging for exteroception. The robot perceives the environment in the form of height samples from an elevation map constructed from point cloud input, as seen in Figure 3A. We used LiDAR in some experiments ( Figure 3D-G) and active stereo cameras in others ( Figure 3B,C) to test the robustness of the controller to the sensing modality.\n\nWe encountered many circumstances in which exteroception provides incomplete or misleading input. As shown in Figure 3 B-G, the estimated elevation map can unreliable due to sensing failures, limitations of the 2.5D height map representation, or viewpoint restrictions due to onboard sensing.\n\nSince most depth sensors rely on light to infer distance, either through time-of-flight measurements or stereo disparity, they commonly struggle with reflective or translucent surfaces. Figure 3B shows such a sensing failure, where the reflective metal floor induced large depth outliers which appear as a trench in the elevation map. Figure  3C shows a sensing failure in the presence of snow. Since snow is highly reflective and has very little texture, stereo cameras could not infer depth, which lead to an empty map.\n\nThe 2.5D elevation map representation cannot accurately represent overhanging objects such as tree branches or low ceilings [17]. These were integrated into the height field and were misrepresented as tall obstacles ( Figure 3D). In addition, because the map cannot distinguish between rigid or soft materials, the map gave misleading information in soft vegetation or deep snow ( Figure 3E).\n\nSlippery or deformable surfaces caused odometry drift because they violate the assumption of stable footholds, commonly adopted by kinematic pose estimators [47]. Since map construction relies on such pose estimation to register consecutive input point clouds, the map became inaccurate in such circumstances ( Figure 3F). Furthermore, since the sensors were only located on the robot itself, areas behind structures were occluded and not presented in the map, which was especially problematic during uphill walking ( Figure 3G).\n\nOverall, our controller could handle all of these challenging conditions gracefully, without a single failure. The belief state estimator was trained to assess the reliability of exteroceptive information and made use of it to the extent possible. When exteroceptive information was incomplete, noisy, or misleading, the controller could always gracefully degrade to proprioceptive locomotion, which was shown to be robust [4]. The controller thus aims to achieve the best of both worlds: achieving fast predictive locomotion when exteroceptive information is informative, but seamlessly retaining the robustness of proprioceptive control when it is not.\n\n\nEvaluating the contribution of exteroception\n\nWe conducted controlled experiments to quantitatively evaluate the contribution of exteroception. We compared our controller to a proprioceptive baseline [4] that does not use exteroception.\n\nFirst, we compared the success rate of overcoming fixed-height steps as shown in Figure 4A. Wooden steps of various height (from 12 cm to 36.5 cm) were placed ahead of the robot, which performed 10 trials to overcome each step with a fixed velocity command. A trial was considered successful if the robot overcomes the step within 5 seconds.\n\nThe success rate of the proprioceptive baseline dropped at 20 cm step height when the front legs started frequently getting stuck at the step ( Figure 4B). Even when the front legs successfully overcame the step, the hind legs often failed to fully step up. In contrast, our controller reliably traversed steps of up to 30.5 cm in height. Since our controller could anticipate the step, it lifted its legs higher without making physical contact first, and leaned its body forward to let the hind leg swing over the step ( Figure 4A). Until this height, the dominating failure reason was the robot evading the step sideways instead of falling. When approaching steps higher than 32 cm, our controller hesitated to walk forward because it learned that steps of such height are at or above the robot's physical limits and are likely to incur a high cost.\n\nWe also tested the two controllers in an obstacle course, as shown in Figure 4C,D. In this experiment, the robot was given a fixed path over the obstacles and tracked it using a pure pursuit controller [49]. The path traverses several types of obstacles -an inclined platform, a raised platform, stairs, and a pile of blocks. The platforms are 20 cm high, the stairs are 17 cm high and 29 cm deep each, and the blocks are each 20 cm in both height and depth. Our controller followed the given path smoothly without any assistance, as shown in Figure 4C. The exteroceptive perception provided advance information on the upcoming obstacles, allowing the controller to adjust the robot's motion before it made contact with the obstacles, facilitating fast and smooth motion through the obstacle course. The baseline, on the other hand, failed to track the path without human assistance. During execution, it got stuck on all three obstacles and we had to lift and push the robot to continue the experiment ( Figure 4D).\n\nIn addition, we measured the maximum locomotion speed of both controllers over flat ground and in the presence of obstacles. Figure  4E shows the experimental setup. We gave the controller a constant forward, lateral, or turning command and recorded the velocity on flat ground and over a 20 cm step. Note that the baseline controller only receives a directional command and learns to walk as fast as possible in the commanded direction [4]. Our controller walked at 1.2 m/s, while the baseline could only achieve 0.6 m/s on flat ground in both the forward and lateral directions. The difference became even more pronounced over the obstacle. Our controller could traverse the obstacle without any notable slow-down, while the baseline was stymied. The turning velocity showed the biggest difference between  the baseline policy and ours. Our controller could turn at 3 rad/s while the baseline policy could only turn at 0.6 rad/s: a five-fold difference. These results show clear gains by our controller over the proprioceptive baseline. Exteroception enabled our controller to traverse challenging environments more successfully and at higher speeds in comparison to pure proprioception. Further quantitative performance evaluation is provided in the supplementary section S2.\n\n\nEvaluating robustness with belief state visualization\n\nTo examine how our controller integrates proprioception and exteroception, we conducted a number of controlled experiments. We tested with two types of obstacles that provide ambiguous or misleading exteroceptive input: an opaque foam obstacle that appears solid but cannot support a foothold, and a solid but transparent obstacle. We placed each obstacle ahead of the robot and commanded the robot to walk forward at a constant velocity.\n\nThe sensors perceived the foam block as solid and the robot consequently prepared to step on it but could not achieve a stable foothold due to the deformation of the foam. Figure 5A shows how the internal belief state (blue) was revised as the robot encounters the misleading obstacle: the controller initially trusted the exteroceptive input (red) but quickly revised its estimate of terrain height upon contact. Once the correct belief had been formed, it was retained even after the foot left the ground, showing that the controller retains past information due to its recurrent structure.\n\nThe transparent obstacle is a block made of clear, acrylic plates, which are not accurately perceived by the onboard sensors ( Figure 5B). The robot therefore walked as if it were on flat ground until it made contact with the step, at which point it revised its estimate of terrain profile upwards and changed its gait accordingly.\n\nIn the next experiment we simulated complete exteroception failure by physically covering the sensors, thus making them fully uninformative ( Figure 5C,D). The robot was commanded to walk up and down two steps of stairs. With an unobstructed sensor, the controller traversed the stairs gracefully, without any unintended contact with the stair risers, adjusting its footholds and body posture to step down the stairs softly. When the sensors were covered, the map had no information and the controller received random noise as input. In this condition, the robot made contact with the riser of the first stair, which could not be perceived in advance, revised its estimate of the terrain profile, adjusted its gait accordingly, and successfully climbed the stairs. On the way down, the blinded robot made a hard landing with its front feet but kept its balance and stepped down softly with its hind legs.\n\nLastly, we tested locomotion over an elevated slippery surface (Figure 5E). After the robot stepped onto the slippery platform, it detected the low friction and adapted its behavior to step faster and keep its balance. The momentarily sliding feet violated the assumption of the kinematic pose estimator, which in turn destabilized the estimated elevation map and rendered exteroception uninformative during this time. The controller seamlessly fell back on proprioception until the estimated elevation map stabilized and exteroception became informative again.\n\n\nDISCUSSION\n\nWe have presented a fast and robust quadrupedal locomotion controller for challenging terrain. The controller seamlessly integrates exteroceptive and proprioceptive input. Exteroceptive perception enables the robot to traverse the environment quickly and gracefully by anticipating the terrain and adapting its gait accordingly before contact is made. When exteroceptive perception is misleading, incomplete, or missing altogether, the controller smoothly transitions to proprioceptive locomotion. The controller remains robust in all conditions, including when the robot is effectively blind. The integration of exteroceptive and proprioceptive inputs is learned end-to-end and does not require any hand-coded rules or heuristics. The result is the first rough-terrain legged locomotion controller that combines the speed and grace of vision-based locomotion with the high robustness of proprioception.\n\nThis combination of speed and high robustness has been validated through controlled experiments and extensive deployments in the wild, including an hour-long hiking route in the Alps that is rated \"difficult\" [46]. The entire route was completed by the robot without human assistance (other than reattaching a detached shoe and swapping the batteries), in the recommended time for completion of this route by human hikers.\n\nOur work expands the operational domain of legged robots and opens up new frontiers in autonomous navigation. Navigation planners no longer need to identify ground type or to switch modes during autonomous operation. Our controller was used as the default controller in the DARPA Subterranean Challenge missions of team Cerberus [50,51] which has won the first prize in the finals [52]. In this challenge, our controller drove ANYmals to operate autonomously over extended periods of time in underground environments with rough terrain, obstructions, and degraded sensing in the presence of dust, fog, water, and smoke [53]. Our controller played a crucial role as it enabled four ANYmals to explore over 1700m in all three types of courses -tunnel, urban, and cave -without a single fall.\n\n\nPossible extensions\n\nFuture work could explicitly utilize the uncertainty information in the belief state. Currently, the policy uses uncertainty only implicitly to estimate the terrain. For example, in front of narrow cliff or a stepping stone, the elevation map does not provide sufficient information due to occlusion. Therefore, the policy assumes a continuous surface and, as a result, the robot might step off and fall. Explicitly estimating uncertainty may allow the policy to become more careful when exteroceptive input is unreliable, for example using its foot to probe the ground if it is unsure about it. In addition, our current implementation obtains perceptual information through an intermediate state in the form of an elevation map, rather than directly ingesting raw sensor data. This has the advantage that the model is independent of the specific exteroceptive sensors. (We use LiDAR and stereo cameras in different deployments, with no retraining or fine-tuning.) However, the elevation map representation omits detail that may be present in the raw sensory input and may provide additional information concerning material and texture. Furthermore, our elevation map construction relies on a classical pose estimation module that is not trained jointly with the rest of the system. Appropriately folding the processing of raw sensory input into the network may further enhance the speed and robustness of the controller. In addition, an occlusion model could be learned, such that the policy understands that there's an occlusion behind the cliff and avoids stepping off it. Another limitation is the inability to complete locomotion tasks which would require maneuvers very different from normal walking, for example recovering from a leg stuck in narrow holes or climbing onto high ledges.\n\n\nMATERIALS AND METHODS\n\n\nOverview\n\nWe train a neural network policy in simulation and then perform zeroshot sim-to-real transfer. Our method consists of three stages, illustrated in Figure 6.\n\nFirst, a teacher policy is trained with RL to follow a random target velocity over randomly generated terrain with random disturbances.    Fig. 5. Overview of the training methods and deployment. We first train a teacher policy with access to privileged simulation data using reinforcement learning (RL). This teacher policy is then distilled into a student policy, which is trained to imitate the teacher's actions and to reconstruct the ground-truth environment state from noisy observations. We deploy the student policy zero-shot on real hardware using height samples from a robot-centric elevation map.\n\nThe policy has access to privileged information such as noiseless terrain measurements, ground friction, and the disturbances that were introduced.\n\nIn the second stage, a student policy is trained to reproduce the teacher policy's actions without using this privileged information. The student policy constructs a belief state to capture unobserved information using a recurrent encoder and outputs an action based on this belief state. During training, we leverage two losses: a behavior cloning loss and a reconstruction loss. The behavior cloning loss aims to imitate the teacher policy. The reconstruction loss encourages the encoder to produce an informative internal representation.\n\nLastly, we transfer the learned student policy to the physical robot and deploy it in the real world with onboard sensors. The robot constructs an elevation map by integrating depth data from onboard sensors, and samples height readings from the constructed elevation map to form the exteroceptive input to the policy. This exteroceptive input is combined with proprioceptive sensory data and is given to the neural network, which produces actuator commands.\n\n\nProblem formulation\n\nWe formulate our control problem in discrete time dynamics, where the environment is fully defined by the state s t at time step t. The policy performs an action a t and observes the environment via o t which comes from an observation model O(o t |s t , a t ). Then, the environment moves to the next state s t+1 with transition probability P(s t+1 |s t , a t ) and returns a reward r t+1 .\n\nWhen all states are observable such that o t = s t , this can be considered a Markov Decision Process (MDP). When there is unobservable information, however, such as external forces or full terrain information in our case, the dynamics are modeled as a Partially Observable Markov Decision Process (POMDP).\n\nThe RL objective is to find a policy \u03c0 * that maximizes the expected discounted reward over the future trajectory, such that\n\u03c0 * = argmax a E[ \u221e \u2211 t=0 \u03b3 t r t ].\nA number of RL algorithms have been developed to solve fullyobservable MDPs and are readily available to be used for training. However, the case of POMDPs is more challenging since the state is not fully observable. This is often overcome by constructing a belief state b t from a history of observations {o 0 , \u00b7 \u00b7 \u00b7 , o t } in an attempt to capture the full state. In deep reinforcement learning, this is frequently done by stacking a sequence of previous observations [54] or by using architectures which can compress past information such as Recurrent Neural Networks (RNNs) [55,56] or Temporal Convolutional Networks (TCNs) [4,57].\n\nTraining a complex neural network policy that handles sequential data naively from scratch can be time-consuming [4]. Therefore we use privileged learning [45], in which we first train a teacher policy with privileged information, and then distill the teacher policy into a student policy via supervised learning.\n\n\nTraining environment\n\nWe use RaiSim [58] as our simulator to build the training environment. There, we simulate multiple ANYmal-C robots on randomly generated rough terrain in parallel with an integrated actuator model [3] to close the reality gap.\n\n\nTerrain\n\nWe define parameterized terrain as shown in Figure 6.1. The terrain is modeled as a height map; further details are provided in supplementary section S4.\n\nIn addition to terrains composed of a variety of slopes and steps, we modelled four different types of stairs in the training environment; standard, open, ledged, and random. We use boxes to form the stairs, because stair risers modeled by a height map are not perfectly vertical; we observed that the policy exploited these non-vertical edges in simulation, resulting in poor sim-to-real transfer.\n\n\nDomain randomization\n\nWe randomize the masses of the robot's body and legs, the initial joint position and velocity, and the initial body orientation and velocity in each episode. In addition, external force and torque are applied to the body of the robot and the friction coefficients of the feet are occasionally set to a low value to introduce slippage.\n\n\nTermination\n\nWe terminate a training episode and start a new one when the robot reaches an undesirable state. Termination criteria are: body collision with the ground, large body tilt, and exceeding the joint torque limit of the actuators. These criteria help shape the motion and obtain constraint-satisfying behaviors.\n\n\nTeacher policy training\n\nIn the first stage of training we aim to find an optimal reference control policy which has access to perfect, privileged information and enables ANYmal to follow a desired command velocity over randomly generated terrain. The desired command is generated randomly as a vector v des \u2208 R 3 = (v x , v y , w), where v x , v y represents the longitudinal and lateral velocity and w represents the yaw velocity, all in the robot's body frame.\n\nWe used Proximal Policy Optimization (PPO) [59] to train the teacher policy. The teacher is modeled as a Gaussian policy, a t \u223c N (\u03c0 \u03b8 (o t = s t ), \u03c3I), where \u03c0 \u03b8 is implemented by a multilayer perceptron (MLP) parameterized by \u03b8, and \u03c3 represents the variance for each action.\n\n\nObservation and Action\n\nThe teacher observation is defined as o teacher Our action space is inspired by central pattern generators (CPGs) [4]. Each leg l = {1, 2, 3, 4} keeps a phase variable \u03c6 l and defines a nominal trajectory based on the phase. The nominal trajectory is a stepping motion of the foot tip and we calculate the nominal joint target q i (\u03c6 l ) for each joint actuator i = {1, \u00b7 \u00b7 \u00b7 , 12} using inverse kinematics. The action from the policy is the phase difference \u2206\u03c6 l and the residual joint position target \u2206q i . More details of the observation and action space are in supplementary section S5.\n\n\nPolicy architecture\n\nWe model the teacher policy \u03c0 \u03b8 as an MLP. It consists of three MLP components: exteroceptive encoder, privileged encoder, and the main network, as shown in Figure 6. The exteroceptive encoder g e receives o e t and outputs a smaller latent representation l e t :\nl e t = g e (o e t )\nThe privileged encoder g p receives the privileged state s p t and outputs a latent representation l priv t : l priv t = g p (s p t )\n\nThese encoders compress each input to a more compact representations and facilitate reuse of some of the teacher policy components by the student policy. More details on each layer are in supplementary section S6\n\n\nRewards\n\nWe define a positive reward for following the command velocity and a negative reward for violating some imposed constraints. The commandfollowing reward is defined as follows:\nr command = 1.0, if v des \u00b7 v > |v des | exp(\u2212(v des \u00b7 v \u2212 |v des |) 2 ), otherwise\n(1) where v des \u2208 R 2 is the desired horizontal velocity and v \u2208 R 2 is the current horizontal body velocity with respect to the body frame. The same reward is applied to the yaw command as well. We penalize the velocity component orthogonal to the desired velocity as well as the body velocity around roll, pitch, and yaw. Additionally, we use shaping rewards for body orientation, joint torque, joint velocity, joint acceleration, foot slippage as well as shank and knee collision.\n\nBody orientation reward was used to avoid strange posture of the body. Joint related reward terms were used to avoid overly aggressive motion. Foot slippage and collision reward terms were used to avoid them. We tuned the reward terms by looking at the policy's behavior in simulation. In addition to the traversal performance, we checked the smoothness of the locomotion. All reward terms are specified in supplementary section S7.\n\n\nCurriculum\n\nWe use two curricula to ramp up difficulty as the policy's performance improves. One curriculum adjusts terrain difficulty using an adaptive method [4] and the other changes elements such as reward or applied disturbances using a logistic function [3].\n\nFor the terrain curriculum, a particle filter updates the terrain parameters such that they remain challenging but achievable at any point during policy training [4].\n\nThe second curriculum multiplies the magnitude of domain randomization and some reward terms (joint velocity, joint acceleration, orientation, slip, thigh and shank contact) by a factor that is monotonically increasing and asymptotically trending to 1:\nc k+1 = (c k ) d ,\nwhere c k is the curriculum factor at the kth iteration and 0 < d < 1 is the convergence rate.\n\n\nStudent policy training\n\nAfter we train a teacher policy that can traverse various terrain with the help of privileged information, we distill it into a student policy that only has access to information that is available on the real robot. We use the same training environment as for the teacher policy, but add additional noise to the student height sample observation: When there is a large noise in the exteroception, it becomes unobservable, thus the dynamics is considered to be POMDP. In addition, the privileged states are not observable due to the lack of sensors to directly measure. Therefore, the policy needs to consider the sequential correlation to estimate the unobservable states. We propose to use a recurrent belief state encoder to combine sequences of both exteroception and proprioception to estimate the unobservable states as a belief state.\no student t = (o\nThe student policy consists of a recurrent belief state encoder and an MLP, as shown in Figure 6.2. We denote the hidden state of the recurrent network by h t . The belief state encoder takes o student t and h t as input and outputs a latent vector b t , which we refer to as the belief state. The goal is to match the belief state b t with the feature vector (l e t , l priv t ) of the teacher policy which encodes all locomotion-relevant information. We then pass o p t and b t to the MLP which computes the output action.\n\nThe MLP structure remains the same as for the teacher policy, such that we can reuse the learned weights of the teacher policy to initialize the student network and speed up training.\n\nTraining is performed in supervised fashion by minimizing two losses: a behavior cloning loss and a reconstruction loss. The behavior cloning loss is defined as the squared distance between the student action and the teacher action given the same state and command. The reconstruction loss is the squared distance between the noiseless height sample and privileged information (o e t , s p t ) and their reconstruction from the belief state. We generate samples by rolling out the student policy to increase robustness [60,61].\n\n\nHeight sample randomization\n\nDuring student training, we inject random noise into the height samples using a parameterized noise model n(\u00f5 e t |o e t , z), z \u2208 R 8\u00d74 . We apply two different types of measurement noise when sampling the heights, as shown in Figure 7A: 1. Shifting scan points laterally.\n\n\nPerturbing the height values.\n\nEach noise value is sampled from a Gaussian distribution, and the noise parameter z defines the variance. Both types of noise are applied in three different scopes, all with their own noise variance: per scan point, per foot, and per episode. The noise values per scan point and per foot are resampled at every time step while the episodic noise remains constant for all scan points.\n\nAdditionally, we define three mapping conditions with associated noise parameters z to simulate changing map quality and error sources, as shown in Figure 7B.\n\n1. Nominal noise assuming good map quality during regular operation.\n\n2. Large offsets through high per-foot noise to simulate map offsets due to pose estimation drift or deformable terrain.\n\n3. Large noise magnitude for each scan point to simulate complete lack of terrain information due to occlusion or mapping failure.\n\nThese three mapping conditions are selected at the beginning of each training episode in a ratio of 60%, 30%, and 10%. Finally, we divide each training terrain into cells and add an additional offset to the height sample, depending on which cell it was sampled from. This simulates transitions between areas with different terrain characteristics, such as vegetation and deep snow. The parameter vector z is also part of a learning curriculum and its magnitude increases linearly with training duration.\n\nThe height sample representation is specified in more detail in supplementary section S8. We use multiple noise configurations z to simulate different operating conditions. \"Zero noise\" is applied during teacher training, while \"nominal noise\" represents normal mapping conditions during student training. \"Large offset\" noise simulates large map offsets due to pose estimation drift or deformable terrain surfaces. \"Large noise\" simulates a complete lack of terrain information due to occlusion or sensor failure. (C) The student policy belief encoder incorporates a recurrent core and an attentional gate that integrates the proprioceptive and exteroceptive modalities. The gate explicitly controls which aspects of exteroceptive data to pass through. (D) The belief decoder has a gate for reconstructing the exteroceptive data. It is only used during training and for introspection into the belief state.\n\n\nBelief state encoder\n\nThe recurrent belief state encoder encodes states that are not directly observable. To integrate proprioceptive and exteroceptive data, we introduce a gated encoder as shown in Figure 7C, inspired by gated RNN models [62,63] and multimodal information fusion [64][65][66].\n\nThe encoder learns an adaptive gating factor that controls how much exteroceptive information to pass through. First, proprioception o p t , exteroceptive features from noisy observations l e t = g e (\u00f5 e t ), and hidden state s t are encoded by the RNN module into the intermediate belief state b t . Then, the attention vector \u03b1 is computed from b t . It controls how much exteroceptive information enters the final belief state b t :\nb t , h t+1 = RNN(o p t , l e t , h t ) \u03b1 = \u03c3(g a (b t )) b t = g b (b t ) + l e t \u03b1\nHere, g a and g b are fully-connected neural networks and \u03c3(\u00b7) is the sigmoid function.\n\nThe same gate is used in the decoder, where it is used to reconstruct the privileged information and the height samples ( Figure 7D). This is used to calculate a reconstruction loss that encourages the belief state to capture veridical information about the environment.\n\nWe use the Gated Recurrent Unit (GRU) [62] as our RNN architecture. The evaluation of the effectiveness of gate structure is presented in supplementary section S9.\n\n\nDeployment\n\nWe deployed our controller on the ANYmal C robot with two different sensor configurations, either using two Robosense Bpearl [67] dome Lidar sensors or four Intel RealSense D435 depth cameras [68]. We trained our policy in PyTorch [69] and deployed on the robot zero-shot without any fine-tuning. We build a robot-centric 2.5D elevation map at 20 Hz by estimating the robot's pose and registering the point-cloud readings from the sensors accordingly. The policy runs at 50 Hz and samples the heights from the latest elevation map, filling a randomly sampled value if no map information is available at a query location.\n\nWe developed an elevation mapping pipeline for fast terrain mapping on a graphics processing unit (GPU) to parallelize point-cloud processing. We follow a similar approach to Fankhauser et al.\n\n[17] to update the map in a Kalman-filter fashion and additionally perform drift compensation and ray casting to obtain a more consistent map. This fast mapping implementation was crucial to maintain fast processing rates and keep up with the fast locomotion speeds achieved by our controller. \n\n\nACKNOWLEDGMENTS\n\n\nSUPPLEMENTARY MATERIALS\n\nSection S1. Nomenclature Section S2. Evaluating the importance of exteroception: simulation result Section S3. Training details Section S4. Terrain generation Section S5. Observation and action Section S6. Network architecture Section S7. Reward function Section S8. Height sample noise Section S9. Belief encoder evaluation Figure S1.\n\nComparison of the presented controller to a proprioceptive baseline over random terrains Figure S2.\n\nAblation analysis of the presented belief encoder Table S1.\n\nHyperparameters for PPO Table S2.\n\nHyperparameters for student training Table S3.\n\nObservations Table S4.\n\nAction difference between teacher and student under two exteroceptive noise conditions. Movie S1.\n\nWalking over stairs in different directions. Movie S2.\n\nBaseline comparison. Movie S3.\n\nRobustness evaluation. Movie S4.\n\nSlippery surface and soft obstacle.  CPG phase base frequency c k curriculum factor c sk student curriculum factor L bc behavior cloning loss L re reconstruction loss (\u00b7) p proprioceptive quantity (\u00b7) e exteroceptive quantity (\u00b7) priv privileged quantity (\u00b7) target target quantity (\u00b7) t quantity at time t (\u00b7) noisy quantit\u1e8f (\u00b7) first derivativ\u00eb (\u00b7) second derivative g(\u00b7) Multilayer Perceptron (MLP) encoder N () Normal distribution Hadamard product p(\u00b7) foot trajectory function IK(\u00b7) inverse kinematics function\n\n\nS2. Evaluating the importance of exteroception: Additional experiments in simulation\n\nWe compare the success rate over various stepped terrain and stairs in simulation to further evaluate the performance quantitatively.\n\nThe robot was given a fixed forward velocity command of 0.7 m/s for a duration of 10 seconds. We collected 300 trials to calculate the success rate, where we consider a trial a success if the robot can traverse 4 m without failure. As shown in Figure 8A, 8B our controller significantly outperformed the baseline and can traverse a much wider range of terrain.\n\n\nS3. Training details\n\nThe control frequency of the policy was set to 50 Hz, and 250 trajectory time steps per environment are collected for one training iteration. We parallelized the simulation environment to perform rollouts with 1000 environments simultaneously. We used our custom implementation of PPO [59] to train the teacher policy [70]. Observations are normalized using running mean and standard deviation before giving them to the policy network. The curriculum factors were updated exponentially every training episode c k+1 = c d k , with convergence rate d = 0.98. We use the Adam [71] optimizer with exponential learning rate decay. The hyperparameters for PPO are given in Table S1.\n\nFor student training, we performed rollouts with 300 environments and collected 400 timesteps of trajectory for one training iteration. We start the student training without height sample noise and gradually increase the noise level through a student curriculum factor which linearly increases over training epochs. We use flat terrain for the first 10 epochs, and then enable the adaptive curriculum for the terrain generation. After 20 epochs, we increase the student curriculum factor c sk linearly until we reach 100 epochs. Then, we keep c sk = 1.0. We train the RNN unit of the encoder with Truncated Backpropagation Through Time (TBPTT). The ratio between behavior cloning loss and reconstruction loss is 0.5. Therefore the loss is set to L bc + 0.5 \u00b7 L re . Hyperparameters for student training are given in Table S2.\n\n\nS4. Terrain generation\n\nThe terrain types are rough, rough discrete, large steps, boxes, grid steps, step stairs, and stairs, as shown in Figure 6.  Figure 8A. Note that the parameter range shown in the figure is only for evaluation and different from the range used during training. Parameters for stairs contain step depth (d \u2208 [0.25, 1.0] m) and height (h \u2208 [0.01, 0.22] m). The height and depth values for random stair were set at each according to a ratio \u223c N (1.0, 0.2), such thatx = x \u00b7 , where x is the given depth or height parameter. Examples of different stairs are shown in Figure 8B. The boxes terrain consists of multiple boxes with maximum height 0.25 m lying in a random position with random yaw angles.\n\n\nS5. Observation and action\n\nThe observation vectors are defined in Table S3. Proprioceptive input includes command, joint, and body information, as well as leg phase information. The central pattern generator (CPG)'s phase information consists of \u2206\u03c6 l , cos \u03c6 l , sin \u03c6 l , and base frequency for each leg l. For exteroception, we use height samples around each foot instead of the local elevation map. The circular sampling pattern comprises {6, 8, 10, 12, 16} points around each foot, with radii {0.08, 0.16, 0.26, 0.36, 0.48} m, respectively.\n\nThe action is defined as \u2206\u03c6 l , \u2206q i , where \u2206\u03c6 l and \u2206q i refer to the phase offset per leg (l \u2208 {legs}) and the residual joint position target (i \u2208 {1, \u00b7 \u00b7 \u00b7 , 12}), respectively. We have a nominal foot trajectory p(\u03c6) : R \u2212\u2192 R 3 that maps each \u03c6 l to a target foot position, which Table S3. Observations. Proprioception is used for both teacher and student training. Exteroception is given in the form of height samples. The privileged information is used only for teacher training.\n\n\nObservation type Input\n\nDim.\n\nexteroception is reliable. When the height samples contain large noise, the exteroception does not provide reliable information. In this case, the gated structure and non-gated structure perform similarly (Table  S4, S5). This indicates that the gated structure facilitates the use of exteroceptive information when it is reliable but does not sacrifice robustness when it becomes unreliable. To further evaluate the policies' performance, a step traversal success rate were compared against each policy. The robot was initialized in front of various height of step and given a constant velocity command (0.8 m/s) towards the step. We collected 100 trials for each height of the step and showed the success rate in Figure S2B. The result shows that \"GRU gate\" performs the best for both small noise and large noise case. As seen in the small noise case, the difference between \"GRU gate\" and \"GRU no gate\" is bigger than the large noise case. This supports that the gated structure can utilize exteroceptive information more when it is reliable. Table S4. Action difference between teacher and student under different noise conditions. The quantities are presented as empirical means with standard deviations. The belief encoder with the exteroceptive gate exhibits smaller action difference for all types of terrain when the noise is small. When the exteroception is unreliable (large noise), they perform similarly; this indicates that the gate blocks the skip connection such that our encoder becomes similar to the proprioceptive model in this condition.  Table S5. Reconstruction error of height samples under different noise conditions. The quantities are presented as empirical means with standard deviations. The belief encoder with the exteroceptive gate had smaller reconstruction error for all types of terrain. This shows the effectiveness of the gated skip connection when the exteroception is reliable. When the noise is large, the gated encoder also performed better than the non-gated encoder, although the difference was smaller than in the small-noise setting.  S1. Comparison of the presented controller to a proprioceptive baseline [4] over random terrain. We collected 300 trials with a fixed velocity command over 41 \u00d7 41 different terrain parameter combinations and compared success rates. Our controller was able to traverse a much wider range of terrain profiles on both grid steps (A) and stairs (B).\n\n\nB\n\nStep traversal success rate Step traversal success rate tested in small noise and large noise cases. The robot is initialized with random joint configuration and initial velocity and given a constant command towards the step. If the robot traversed the step with both front and hind legs it is considered as success. 100 trials were conducted.\n\nFig. 1 .\n1Fig. 1. Robust locomotion in the wild. The presented locomotion controller was extensively tested in a variety of complex environments over multiple seasons. The controller overcame a whole spectrum of real-world challenges, often encountering them in combination. These include slippery surfaces, steep inclinations, complex terrain, and vegetation in natural environments. In search-and-rescue scenarios, the controller dealt with steep stairs, unknown payloads, and perception-degrading fog. Reflective surfaces, loose ground, low light, and water puddles were encountered in underground cave systems. Soft and slippery snow piled up in the winter. The controller traversed these environments with zero failures.\n\nFig. 3 .\n3Our locomotion controller perceives the environment through height samples (red dots) from an elevation map (A). The controller is robust to many perception challenges commonly encountered in the field: missing map information due to sensing failure (B, C, G) and misleading map information due to non-rigid terrain (D, E) and pose estimation drift (F).\n\nFig. 4 .\n4Internal belief state inspection during perceptive failure using a learned belief decoder. Red dots indicate height samples given as input to the policy. Blue dots show the controller's internal estimate of the terrain profile. (A) After stepping on a soft obstacle that cannot support a foothold, the policy correctly revises its estimate of the terrain profile downwards. (B) A transparent obstacle is correctly incorporated into the terrain profile after contact is made. (C) With operational sensors, the robot swiftly and gracefully climbs the stairs, with no spurious contacts. (D) When the robot is blinded by covering the sensors, the policy can no longer anticipate the terrain but remains robust and successfully traverses the stairs. (E) When stepping onto a slippery platform, the policy identifies low friction and compensates for the induced pose estimation drift. The graph shows a decoded friction coefficient.\n\n\nthe body velocity, orientation, joint position and velocity history, action history, and each leg's phase. o e t is a vector of height samples around each foot with five different radii. The privileged state s p t includes contact states, contact forces, contact normals, friction coefficient, thigh and shank contact states, external forces and torques applied to the body, and swing phase duration.\n\n\np t , n(o e t )), where n(o e t ) is a noise model applied to the height sample input. The noise model simulates different failure cases of exteroception frequently encountered during field deployment and is detailed below.\n\nFig. 6 .\n6Details of robust terrain perception components. (A) During student training, random noise is added to the height samples. The noise is sampled from a Gaussian distribution N (0, z l \u2208 R 8 ), where each z l i controls a different noise component i per leg l. (B)\n\nFunding\nThe project was funded, in part, by the Intel Network on Intelligent Systems, the Swiss National Science Foundation (SNF) through the National Centre of Competence in Research Robotics and project No. 188596, the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme grant agreement No. 852044, No. 780883 and No. 101016970. The work has been conducted as part of ANYmal Research, a community to advance legged robotics. Author contributions T.M. formulated the main idea of combining inputs from multiple modalities. J.L. and J.H designed and tested the initial setup. T.M. developed software and trained the controller. T.M. and L.W. set up the perception pipeline on the robot. T.M. conducted most of the indoor experiments. T.M., J.L., and L.W. conducted outdoor experiments. All authors refined ideas, contributed in the experiment design, analyzed the data, and wrote the paper. Competing interests The authors declare that they have no competing interests. Data and materials availability All data needed to evaluate the conclusions in the paper are present in the paper or the Supplementary Materials.\n\n\nP. Fankhauser, M. Bloesch, M. Hutter, Probabilistic terrain mapping for mobile robots with uncertain localization, IEEE Robotics and Automation Letters 3019-3026 (2018). 18. M. Zucker, J. A. Bagnell, C. G. Atkeson, J. Kuffner, An optimization approach to rough terrain locomotion, 2010 IEEE International Conference on Robotics and Automation, 3589-3595 (IEEE, 2010). 19. P. D. Neuhaus, J. E. Pratt, M. J. Johnson, Comprehensive summary of the institute for human and machine cognition's experience with Little-Dog, The International Journal of Robotics Research 216-235 (2011). 20. J. Z. Kolter, Y. Kim, A. Y. Ng, Stereo vision and terrain modeling for quadruped robots, 2009 IEEE International Conference on Robotics and Automation, 1557-1564 (IEEE, 2009). 21. I. Havoutis, J. Ortiz, S. Bazeille, V. Barasuol, C. Semini, D. G. Caldwell, Onboard perception-based trotting and crawling with the hydraulic quadruped robot (HyQ), 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, 6052-6057 (IEEE, 2013). 22. C. Mastalli, M. Focchi, I. Havoutis, A. Radulescu, S. Calinon, J. Buchli, D. G. Caldwell, C. Semini, Trajectory and foothold optimization using low-dimensional models for rough terrain locomotion, 2017 IEEE International Conference on Robotics and Automation (ICRA), 1096-1103 (IEEE, 2017). 23. D. Belter, P. \u0141ab\u0119cki, P. Skrzypczy\u0144ski, Adaptive motion planning for autonomous rough terrain traversal with a walking robot, Journal of Field Robotics 337-370 (2016). 24. P. Fankhauser, M. Bjelonic, C. D. Bellicoso, T. Miki, M. Hutter, Robust rough-terrain locomotion with a quadrupedal robot, 2018 IEEE International Conference on Robotics and Automation (ICRA), 5761-5768 (IEEE, 2018). 25. F. Jenelten, T. Miki, A. E. Vijayan, M. Bjelonic, M. Hutter, Perceptive locomotion in rough terrain-online foothold optimization, IEEE Robotics and Automation Letters 5370-5376 (2020). 26. D. Kim, D. Carballo, J. Di Carlo, B. Katz, G. Bledt, B. Lim, S. Kim, Vision aided dynamic exploration of unstructured terrain with a small-scale quadruped robot, 2020 IEEE International Conference on Robotics and Automation (ICRA), 2464-2470 (IEEE, 2020). 27. O. A. Villarreal-Maga\u00f1a, V. Barasuol, M. Camurri, M. Focchi, L. Franceschi, M. Pontil, D. G. Caldwell, C. Semini, Fast and continuous foothold adaptation for dynamic locomotion through cnns, IEEE Robotics and Automation Letters 2140-2147 (2019). 28. Boston Dynamics, Atlas | partners in parkour, https://youtu.be/ tF4DML7FIWk (2021). [Online; accessed September-2021]. 29. X. B. Peng, G. Berseth, M. Van de Panne, Terrain-adaptive locomotion skills using deep reinforcement learning, ACM Transactions on Graphics (TOG) 1-12 (2016). 30. X. B. Peng, G. Berseth, K. Yin, M. Van De Panne, Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning, ACM Transactions on Graphics (TOG) 1-13 (2017). 31. X. B. Peng, P. Abbeel, S. Levine, M. van de Panne, Deepmimic: Example-guided deep reinforcement learning of physics-based character skills, ACM Trans. Graph. 143:1-143:14 (2018). 32. Z. Xie, H. Y. Ling, N. H. Kim, M. van de Panne, Allsteps: Curriculumdriven learning of stepping stone skills, Computer Graphics Forum, 213-224 (Wiley Online Library, 2020). 33. V. Tsounis, M. Alge, J. Lee, F. Farshidian, M. Hutter, Deepgait: Planning and control of quadrupedal gaits using deep reinforcement learning, IEEE Robotics and Automation Letters 3699-3706 (2020). 34. J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, V. Vanhoucke, Sim-to-real: Learning agile locomotion for quadruped robots, Robotics: Science and Systems (2018).\n\nAFig. S2 .\nS2Learning Ablation analysis of the presented belief encoder. We compared GRU gate, GRU no gate, MLP gate and MLP no gate. MLP setting uses MLP instead of GRU as its encoder. Gate setting uses proposed attention gate while no gate setting exclude it.(A) Learning curve of the student policy training. GRU worked better than MLP in all cases. Attention gate worked better than without attention for both GRU and MLP. The increase of the losses and decrease of reward in the beginning is due to the curriculum. (B)\n\n\n35. X. B. Peng, E. Coumans, T. Zhang, T.-W. E. Lee, J. Tan, S. Levine, Learning agile robotic locomotion skills by imitating animals, Robotics: Science and Systems (2020). 36. Y. Yang, K. Caluwaerts, A. Iscen, T. Zhang, J. Tan, V. Sindhwani, Data efficient reinforcement learning for legged robots, Conference on Robot Learning, 1-10 (PMLR, 2020).S1. Nomenclature \n\ns \nstate \no \nobservation \nb \nbelief state \nh \nhidden state \nl \nlatent feature \nv \nlinear velocity \n\u03c9 \nangular velocity \n\u03c4 \njoint torque \nq \njoint position \n\u03c6 \nCPG phase \n\u2206\u03c6 0 \n\n\nTable S1 .\nS1Hyperparameters for PPO.Table S2. Hyperparameters for student training.learning rate \n5.0 E-4 \n\nlearning rate decay gamma 0.9999 \n\ndiscount factor \n0.996 \n\nlearning epoch \n2 \n\nGAE-lambda \n0.95 \n\nclip ratio \n0.2 \n\nentropy coefficient \n0.005 \n\nbatch size \n8300 \n\nlearning rate \n5.0 E-4 \n\ntruncate step for TBPTT 10 \n\nlearning epoch \n2 \n\n\n\n\n1. There are four types of stairs: standard stair, open stair, ledged stair, and random stair. Each terrain type is parameterized by different terrain properties, which are randomized during training.The rough terrain is parameterized by Perlin noise[72] and the rough discrete and large steps are created by quantizing it. While rough discrete terrain does not restrict the number of quantization levels, large steps only allow for two height levels (h \u2208 [0, 0.4] m).For grid steps, the parameters are mean step height (h \u2208 [0.05, 0.4] m) and step width (d \u2208 [0.2, 0.7] m). Some examples of different grid steps are shown in\n\n\n872\u00b10.45 0.956\u00b10.46 1.489\u00b10.59 1.526\u00b10.58Small exteroceptive noise \nLarge exteroceptive noise \n\nterrain \nours \nwithout gate \nours \nwithout gate \n\nrough \n0.690\u00b10.40 0.746\u00b10.40 \n0.879\u00b10.46 0.997\u00b10.44 \n\nrough discrete 0.787\u00b10.45 0.857\u00b10.54 \n0.878\u00b10.53 0.964\u00b10.55 \n\nstep stair \n0.652\u00b10.39 0.687\u00b10.43 \n0.975\u00b10.49 1.043\u00b10.50 \n\nlarge step \n0.719\u00b10.40 0.855\u00b10.43 \n1.142\u00b10.55 1.225\u00b10.54 \n\ngrid steps \n1.444\u00b10.56 1.674\u00b10.58 \n2.218\u00b10.70 2.212\u00b10.70 \n\nstandard stair \n0.854\u00b10.67 0.961\u00b10.72 \n1.387\u00b10.59 1.438\u00b10.56 \n\nopen stair \n0.842\u00b10.61 0.938\u00b10.65 \n1.356\u00b10.55 1.428\u00b10.53 \n\nledged stair \n0.819\u00b10.39 0.929\u00b10.42 \n1.373\u00b10.53 1.416\u00b10.54 \n\nboxes \n0.928\u00b10.53 1.123\u00b10.56 \n1.614\u00b10.64 1.683\u00b10.68 \n\nrandom stair \n0.\n\n\n21E-03\u00b12.8E-04 1.36E-03\u00b16.1E-04 1.03E-03\u00b12.3E-04 1.17E-03\u00b15.9E-04 rough discrete 9.99E-04\u00b13.3E-04 1.03E-03\u00b13.9E-04 1.02E-03\u00b13.5E-04 1.05E-03\u00b13.5E-04 step stair 1.13E-03\u00b14.4E-04 1.31E-03\u00b14.7E-04 1.41E-03\u00b14.3E-04 1.48E-03\u00b14.6E-04 large step 1.37E-03\u00b18.0E-04 2.03E-03\u00b11.0E-03 1.95E-03\u00b18.2E-04 1.95E-03\u00b17.8E-04 grid steps 3.05E-03\u00b14.1E-04 4.77E-03\u00b17.4E-04 4.17E-03\u00b15.0E-04 4.39E-03\u00b15.1E-04 standard stair 2.59E-03\u00b12.2E-03 3.11E-03\u00b12.2E-03 2.68E-03\u00b11.6E-03 2.69E-03\u00b11.5E-03 open stair 2.61E-03\u00b12.3E-03 3.06E-03\u00b12.0E-03 2.63E-03\u00b11.2E-03 2.64E-03\u00b11.1E-03 ledged stair 2.53E-03\u00b11.7E-03 3.03E-03\u00b11.5E-03 2.62E-03\u00b11.2E-03 2.63E-03\u00b11.1E-03 boxes 2.13E-03\u00b11.4E-03 3.38E-03\u00b11.5E-03 3.00E-03\u00b11.0E-03 3.09E-03\u00b11.2E-03 random stair 2.31E-03\u00b19.1E-04 2.89E-03\u00b18.2E-04 2.72E-03\u00b17.9E-04 2.74E-03\u00b18.0E-04Small exteroceptive noise \nLarge exteroceptive noise \n\nterrain \nours \nwithout gate \nours \nwithout gate \n\nrough \n1.A \n\nB \n\nFig. \ngenerates periodic stepping motion as \u03c6 cycles within [0, 2\u03c0). From the action, the joint position target for a leg l is defined as q target i\u2208l = IK(p(\u03c6 l + \u2206\u03c6 l + \u2206\u03c6 0 )) + \u2206q i\u2208l , using analytic inverse kinematics IK(\u00b7) and base phase frequency \u2206\u03c6 0 . The nominal foot trajectory is defined as follows.If the phase is in swing-up (0 \u2264 \u03c6 l \u2264 \u03c0/2),{x, y, z} n l is the nominal foot position at the default stance configuration. The cubic Hermite spline connects z = z n l at \u03c6 l = 0 and z = z n l + 0.2 at \u03c6 l = \u03c0/2. In the swing-down phase (\u03c0/2 < \u03c6 l \u2264 \u03c0), the foot height is computed aswhich is symmetric to the previous function.During the stance phase (\u03c0 < \u03c6 l \u2264 2\u03c0), p l (\u03c6 l ) = x n l , y n l , z n l .S6. Network architectureThe policy network is composed of multiple MLPs. The height samples are first encoded into a 24 \u00d7 4 = 96 dimensional latent vector, and the privileged information is encoded into a 24 dimensional latent vector using MLP-based encoders (g e , g p ). Each encoder has two hidden layers with {80, 60} and {64, 32} hidden units respectively. The height samples are first fed into the encoder separately for each foot and then concatenated into one feature vector. Then these features are concatenated with proprioceptive observations and fed into another MLP with three hidden layers {256, 160, 128}. The activation function for all MLPs is LeakyReLU[73]. We use a GRU with an exteroceptive gate for the belief encoder (Figure 7C). The GRU consists of 2 stacked layers with 50 hidden units each. The belief encoder and exteroceptive gate g b , g a are used to calculate 96 + 24 = 120 dimensional belief state b t and 96 dimensional attention vector \u03b1. Each encoder has two hidden layers with {64, 64} and {64, 64} hidden units each. The filtered exteroceptive information l e t \u03b1 is added to g b (b t ), with zero-padding to match the dimensionality.S7. Reward functionThe reward function is defined as r = 0.75(r lv + r av + r lvo ) + r b + 0.003r f c + 0.1r co + 0.001r j + 0.08r jc + 0.003r s + 1.0 \u00b7 10 \u22126 r \u03c4 + 0.003r slip . The individual terms are defined as follows.\u2022 Linear Velocity Reward (r lv ): This term encourages the policy to follow a desired horizontal velocity (velocity in xy plane) command:where v des \u2208 R 2 is the desired horizontal velocity and v \u2208 R 2 is the current body velocity with respect to the body frame.\u2022 Angular Velocity Reward (r av ): This term encourages the policy to follow a desired yaw velocity command:where \u03c9 des is the desired yaw velocity and \u03c9 z is the current yaw velocity with respect to the body frame.\u2022 Linear Orthogonal Velocity Reward (r lvo ): This term penalizes the velocity orthogonal to the target direction:\u2022 Body motion Reward (r b ): This term penalizes the body velocity in directions not part of the command:\u2022 Foot Clearance Reward (r f c ): When a leg is in swing phase, i.e., \u03c6 i \u2208 [0, \u03c0), the robot should lift the corresponding foot higher than its surroundings. However, to prevent the robot from manifesting unnecessarily high foot clearance, we give a penalty reward r f cl to regularize the leg trajectory. H sample,l is the set of sampled heights around the l-th foot. Then, the clearance cost is defined asNote that height samples are sampled with respect to the foot height, therefore -0.2 means the terrain is 0.2 m lower than the foot; ergo, the foot is 0.2 m higher than the sampled terrain height.\u2022 Shank and Knee Collision Reward (r co ): We want to penalize undesirable contact between the terrain and robot parts other than the foot, to avoid hardware damage:where c k is the curriculum factor that increases monotonically and converges to 1.\u2022 Joint Motion Reward (r j ): This term penalizes joint velocity and acceleration to avoid vibrations:whereq i andq i are the joint velocity and acceleration, respectively.\u2022 Joint Constraint Reward (r jc ): This term introduces a soft constraint in the joint space. To avoid the knee joint flipping in the opposite direction, we give a penalty for exceeding a threshold:where q i,th is a threshold value for the ith joint. We only set thresholds for the knee joint.\u2022 Target Smoothness Reward (r s ): The magnitude of the first and second order finite difference derivatives of the target foot positions are penalized such that the generated foot trajectories become smoother:where q des i,t is the joint target position of joint i at time step t.\u2022 Torque Reward (r \u03c4 ): We penalize joint torques to reduce energy consumption (\u03c4 \u221d electric current):where \u03c4 i is the ith joint's torque calculated as output by the actuator network.\u2022 Slip Reward (r slip ): We penalize the foot velocity if the foot is in contact with the ground to reduce slippage:where v f ,l is the velocity of lth foot in contact with the ground.S8. Height sample noiseDuring student training, we randomize the height samples drawn around each foot (Figure 7A). We perturbed the position of each sample and add noise to the measured height value as follows.x p = r p cos(\u03b8 p ) + px + f x + w x y p = r p sin(\u03b8 p ) + py + f y + w ywhere h(x p , y p ) refers to the terrain height at position (x p , y p ). r p is the radial distance of the point p and \u03b8 p is the azimuthal angle of p in polar coordinates around the foot. px , py , pz represents the noise that is sampled for each individual point every time step.f x , f y , f z represents the noise that is sampled for each foot every time step. w x , w y , w z represents the noise that is sampled for each foot per episode. outlier is a large noise intermittently added to simulate outliers.Each noise is sampled from the normal distribution using the param-We defined three conditions for the student training; nominal, offset, noisy. Each parameter z is defined as follows. where c sk is the student curriculum factor which linearly increases over training episodes. We randomly picked one of the conditions at the beginning and in the middle of a trajectory. The probabilities are 60%, 30% and 10%, respectively.S9. Ablation study of attention gate in belief encoderWe evaluated the effect of the exteroceptive gate by comparing the performance of the belief encoder with and without the gate. For this purpose, we trained four student policies using different belief encoders: \"GRU gate\", \"GRU no gate\", \"MLP gate\" and \"MLP no gate\". \"GRU gate\" uses the proposed exteroceptive gate while \"GRU no gate\" does not use it. \"MLP\" uses feed forward network instead of the recurrent unit.Figure S2Ashows the learning curve of the student training using four different architectures. The result shows that using a recurrent unit improves the performance. MLP failed to reconstruct the privileged information. Moreover, the exteroceptive gate constantly improves the performance for both GRU and MLP architectures. Note that in the beginning of the training, we started without exteroceptive noise and terrain curriculum, and increased them gradually. This effect can be seen as a steep increase of losses and decrease of reward in the beginning.To evaluate the learned model, we collected 300 time steps with 100 different terrain parameters for each terrain type with two noise conditions: small and large. Each noise parameter z are defined as follows,Then we calculated the squared distance between student action and teacher action, as well as decoded height samples and ground-truth height samples. As shown inTable S4, S5, the gated encoder outperformed the non-gated encoder for both noise cases. The encoder utilizes the exteroceptive input through the skip connection when the\nM Raibert, K Blankespoor, G Nelson, R Playter, Bigdog, the roughterrain quadruped robot, IFAC Proceedings Volumes. M. Raibert, K. Blankespoor, G. Nelson, R. Playter, Bigdog, the rough- terrain quadruped robot, IFAC Proceedings Volumes 10822-10825 (2008).\n\nMini cheetah: A platform for pushing the limits of dynamic quadruped control. B Katz, J Di Carlo, S Kim, International Conference on Robotics and Automation (ICRA). IEEEB. Katz, J. Di Carlo, S. Kim, Mini cheetah: A platform for pushing the limits of dynamic quadruped control, 2019 International Conference on Robotics and Automation (ICRA), 6295-6301 (IEEE, 2019).\n\nLearning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 4J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, M. Hutter, Learning agile and dynamic motor skills for legged robots, Science Robotics 4 (2019).\n\nLearning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science Robotics. 5J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, M. Hutter, Learning quadrupedal locomotion over challenging terrain, Science Robotics 5 (2020).\n\nJumping over obstacles with MIT Cheetah 2, Robotics and Autonomous Systems p. H.-W Park, P M Wensing, S Kim, 103703H.-W. Park, P. M. Wensing, S. Kim, Jumping over obstacles with MIT Cheetah 2, Robotics and Autonomous Systems p. 103703 (2021).\n\n. Boston Dynamics, Spot. Online; accessed March-2021Boston Dynamics, Spot, https://www.bostondynamics.com/spot (2021). [Online; accessed March-2021].\n\nANYmal in the field: Solving industrial inspection of an offshore HVDC platform with a quadrupedal robot. C Gehring, P Fankhauser, L Isler, R Diethelm, S Bachmann, M Potz, L Gerstenberg, M Hutter, Field and Service Robotics. SpringerC. Gehring, P. Fankhauser, L. Isler, R. Diethelm, S. Bachmann, M. Potz, L. Gerstenberg, M. Hutter, ANYmal in the field: Solving industrial inspection of an offshore HVDC platform with a quadrupedal robot, Field and Service Robotics, 247-260 (Springer, 2021).\n\n. Agility Robotics, Robots. Agility Robotics, Robots, https://www.agilityrobotics.com/robots (2021). [Online; accessed June-2021].\n\nA1. Online; accessed March-2021Unitree Robotics, A1, https://www.unitree.com/products/a1/ (2021). [Online; accessed March-2021].\n\n. Ghost Robotics, Vision. 60OnlineGhost Robotics, Vision 60, https://www.ghostrobotics.io/ (2021). [On- line; accessed June-2021].\n\nGaze and the control of foot placement when walking in natural terrain. J S Matthis, J L Yates, M M Hayhoe, Current Biology. J. S. Matthis, J. L. Yates, M. M. Hayhoe, Gaze and the control of foot placement when walking in natural terrain, Current Biology 1224-1233 (2018).\n\n. Anymal Anybotics, OnlineANYbotics, ANYmal, https://www.anybotics.com/ anymal-autonomous-legged-robot/ (2021). [Online;\n\nKinect v2 for mobile robot navigation: Evaluation and modeling. P Fankhauser, M Bloesch, D Rodriguez, R Kaestner, M Hutter, R Siegwart, International Conference on Advanced Robotics (ICAR). IEEEP. Fankhauser, M. Bloesch, D. Rodriguez, R. Kaestner, M. Hutter, R. Siegwart, Kinect v2 for mobile robot navigation: Evaluation and modeling, 2015 International Conference on Advanced Robotics (ICAR), 388-394 (IEEE, 2015).\n\nLearning locomotion skills for cassie: Iterative design and sim-to-real. Z Xie, P Clary, J Dao, P Morais, J Hurst, M Van De Panne, Proceedings of the Conference on Robot Learning. L. P. Kaelbling, D. Kragic, K. Sugiurathe Conference on Robot LearningPMLRZ. Xie, P. Clary, J. Dao, P. Morais, J. Hurst, M. van de Panne, Learning locomotion skills for cassie: Iterative design and sim-to-real, Proceed- ings of the Conference on Robot Learning, L. P. Kaelbling, D. Kragic, K. Sugiura, eds., 317-329 (PMLR, 2020).\n\nBlind bipedal stair traversal via sim-to-real reinforcement learning. J Siekmann, K Green, J Warila, A Fern, J Hurst, Robotics: Science and Systems. J. Siekmann, K. Green, J. Warila, A. Fern, J. Hurst, Blind bipedal stair traversal via sim-to-real reinforcement learning, Robotics: Science and Systems (2021).\n\nRma: Rapid motor adaptation for legged robots. A Kumar, RSSZ Fu, RSSD Pathak, RSSJ Malik, RSSProceedings of Robotics: Science and Systems. Robotics: Science and SystemsA. Kumar, Z. Fu, D. Pathak, J. Malik, Rma: Rapid motor adaptation for legged robots, Proceedings of Robotics: Science and Systems (RSS) (2021).\n\nMulti-expert learning of adaptive legged locomotion. C Yang, K Yuan, Q Zhu, W Yu, Z Li, Science Robotics p. 2174C. Yang, K. Yuan, Q. Zhu, W. Yu, Z. Li, Multi-expert learning of adaptive legged locomotion, Science Robotics p. eabb2174 (2020).\n\nJ Lee, J Hwangbo, M Hutter, arXiv:1901.07517Robust recovery controller for a quadrupedal robot using deep reinforcement learning. arXiv preprintJ. Lee, J. Hwangbo, M. Hutter, Robust recovery controller for a quadrupedal robot using deep reinforcement learning, arXiv preprint arXiv:1901.07517 (2019).\n\nS Gangapurwala, M Geisert, R Orsolino, M Fallon, I Havoutis, arXiv:2012.03094RLOC: Terrain-aware legged locomotion using reinforcement learning and optimal control. arXiv preprintS. Gangapurwala, M. Geisert, R. Orsolino, M. Fallon, I. Havoutis, RLOC: Terrain-aware legged locomotion using reinforcement learning and optimal control, arXiv preprint arXiv:2012.03094 (2020).\n\nHeuristic planning for rough terrain locomotion in presence of external disturbances and variable perception quality. M Focchi, R Orsolino, M Camurri, V Barasuol, C Mastalli, D G Caldwell, C Semini, Advances in Robotics Research: From Lab to Market. SpringerM. Focchi, R. Orsolino, M. Camurri, V. Barasuol, C. Mastalli, D. G. Caldwell, C. Semini. Heuristic planning for rough terrain locomotion in presence of external disturbances and variable perception quality. Advances in Robotics Research: From Lab to Market (Springer, 2020), 165-209.\n\n. Boston Dynamics. Spot user guide release 2.0 version A. Online; accessed June-2021Boston Dynamics, Spot user guide release 2.0 version A, https://www.generationrobots.com/media/spot-boston-dynamics/ spot-user-guide-r2.0-va.pdf (2021). [Online; accessed June-2021].\n\nLearning by cheating. D Chen, B Zhou, V Koltun, P Kr\u00e4henb\u00fchl, Conference on Robot Learning. PMLRD. Chen, B. Zhou, V. Koltun, P. Kr\u00e4henb\u00fchl, Learning by cheating, Conference on Robot Learning, 66-75 (PMLR, 2020).\n\nEtzel kulm loop hike. Komoot, Komoot, Etzel kulm loop hike, https://bit.ly/35bjfyE (2021). [Online; accessed June-2021].\n\nState estimation for legged robots-consistent fusion of leg kinematics and imu. M Bloesch, M Hutter, M A Hoepflinger, S Leutenegger, C Gehring, C D Remy, R Siegwart, M. Bloesch, M. Hutter, M. A. Hoepflinger, S. Leutenegger, C. Gehring, C. D. Remy, R. Siegwart, State estimation for legged robots-consistent fusion of leg kinematics and imu, Robotics 17-24 (2013).\n\n. Komoot, Komoot help guides. Online; accessed December-2021Komoot, Komoot help guides, https://d21buns5ku92am.cloudfront.net/ 67683/documents/40488-Komoot%20Guides%20English-4d1241. pdf (2021). [Online; accessed December-2021].\n\nImplementation of the pure pursuit path tracking algorithm. R C Coulter, Carnegie-Mellon UNIV Pittsburgh PA Robotics INSTTech. repR. C. Coulter, Implementation of the pure pursuit path tracking algo- rithm, Tech. rep., Carnegie-Mellon UNIV Pittsburgh PA Robotics INST (1992).\n\nCerberus: Autonomous legged and aerial robotic exploration in the tunnel and urban circuits of the darpa subterranean challenge. M Tranzatto, F Mascarich, L Bernreiter, C Godinho, M Camurri, S M K Khattak, T Dang, V Reijgwart, J Loeje, D Wisth, Journal of Field Robotics. M. Tranzatto, F. Mascarich, L. Bernreiter, C. Godinho, M. Camurri, S. M. K. Khattak, T. Dang, V. Reijgwart, J. Loeje, D. Wisth, others, Cerberus: Autonomous legged and aerial robotic exploration in the tunnel and urban circuits of the darpa subterranean challenge, Journal of Field Robotics (2021).\n\n. Team Cerberus, Cerberus, OnlineCerberus, Team cerberus, https://www.subt-cerberus.org/ (2021). [On- line; accessed June-2021].\n\nDarpa subterranean challenge competition results finals. DARPA. Online; accessed November-2021DARPA, Darpa subterranean challenge competition results finals, https://www.subtchallenge.com/results.html (2021). [Online; accessed November-2021].\n\nDarpa subterranean challenge competition rules final event. DARPA. DARPA, Darpa subterranean challenge competition rules final event, https://www.subtchallenge.com (2021). [Online; accessed June-2021].\n\nPlaying atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, Advances in Neural Information Processing Systems, Deep Learning Workshop. V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier- stra, M. Riedmiller, Playing atari with deep reinforcement learning, Advances in Neural Information Processing Systems, Deep Learning Work- shop (2013).\n\nP Zhu, X Li, P Poupart, G Miao, arXiv:1704.07978On improving deep reinforcement learning for pomdps. arXiv preprintP. Zhu, X. Li, P. Poupart, G. Miao, On improving deep reinforcement learning for pomdps, arXiv preprint arXiv:1704.07978 (2017).\n\nGeorgiev, others, Grandmaster level in starcraft ii using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P , Nature. 350354O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, others, Grand- master level in starcraft ii using multi-agent reinforcement learning, Nature 350-354 (2019).\n\nAn empirical evaluation of generic convolutional and recurrent networks for sequence modeling. S Bai, J Z Kolter, V Koltun, arXiv:1803.01271S. Bai, J. Z. Kolter, V. Koltun, An empirical evaluation of generic convolu- tional and recurrent networks for sequence modeling, arXiv:1803.01271 (2018).\n\nPer-contact iteration method for solving contact dynamics. J Hwangbo, J Lee, M Hutter, IEEE Robotics and Automation Letters. J. Hwangbo, J. Lee, M. Hutter, Per-contact iteration method for solv- ing contact dynamics, IEEE Robotics and Automation Letters 895-902 (2018).\n\nJ Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347 (2017).\n\nA reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statisticsS. Ross, G. Gordon, D. Bagnell, A reduction of imitation learning and structured prediction to no-regret online learning, Proceedings of the fourteenth international conference on artificial intelligence and statistics, 627-635 (JMLR Workshop and Conference Proceedings, 2011).\n\nDistilling policy distillation. W M Czarnecki, R Pascanu, S Osindero, S Jayakumar, G Swirszcz, M Jaderberg, Proceedings of Machine Learning. Research, K. Chaudhuri, M. SugiyamaMachine LearningPMLRW. M. Czarnecki, R. Pascanu, S. Osindero, S. Jayakumar, G. Swirszcz, M. Jaderberg, Distilling policy distillation, Proceedings of Machine Learn- ing Research, K. Chaudhuri, M. Sugiyama, eds., 1331-1340 (PMLR, 2019).\n\nLearning phrase representations using rnn encoder-decoder for statistical machine translation. K Cho, B Van Merri\u00ebnboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, Conference on Empirical Methods in Natural Language Processing (EMNLP). K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, Y. Bengio, Learning phrase representations using rnn encoder-decoder for statistical machine translation, Conference on Em- pirical Methods in Natural Language Processing (EMNLP), p. 1724-1734 (2014).\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Com. S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Com- putation 1735-1780 (1997).\n\nDeep gated multi-modal learning: In-hand object pose changes estimation using tactile and image data. T Anzai, K Takahashi, 2020T. Anzai, K. Takahashi, Deep gated multi-modal learning: In-hand object pose changes estimation using tactile and image data, 2020\n\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 9361-9368 (IEEE, 2020).\n\nRobust deep multi-modal learning based on gated information fusion network. J Kim, J Koh, Y Kim, J Choi, Y Hwang, J W Choi, Asian Conference on Computer Vision. SpringerJ. Kim, J. Koh, Y. Kim, J. Choi, Y. Hwang, J. W. Choi, Robust deep multi-modal learning based on gated information fusion network, Asian Conference on Computer Vision, 90-106 (Springer, 2018).\n\nGated multimodal units for information fusion. J Arevalo, T Solorio, M Montes-Y G\u00f3mez, F A Gonz\u00e1lez, ICLR workshop. J. Arevalo, T. Solorio, M. Montes-y G\u00f3mez, F. A. Gonz\u00e1lez, Gated multimodal units for information fusion, ICLR workshop (2017).\n\n. Rs-Bpearl, Rs-bpearl (2021, april), https://www.robosense.ai/en/rslidar/RS-Bpearl.\n\nPytorch: An imperative style, highperformance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, R. GarnettCurran Associates, Inc32A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala. Pytorch: An imperative style, high- performance deep learning library. Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, R. Garnett, eds. (Curran Associates, Inc., 2019), 8024-8035.\n\nM Takahiro, L Joonho, M Yuntao, E Pascal, Rslgym , GitHub repository. M. Takahiro, L. Joonho, M. Yuntao, E. Pascal, Rslgym, GitHub reposi- tory (2021).\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, ICLR. D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, ICLR (2015).\n\nA survey of procedural noise functions. A Lagae, S Lefebvre, R Cook, T Derose, G Drettakis, D S Ebert, J P Lewis, K Perlin, M Zwicker, Computer Graphics Forum. Wiley Online LibraryA. Lagae, S. Lefebvre, R. Cook, T. DeRose, G. Drettakis, D. S. Ebert, J. P. Lewis, K. Perlin, M. Zwicker, A survey of procedural noise functions, Computer Graphics Forum, 2579-2600 (Wiley Online Library, 2010).\n\nRectifier nonlinearities improve neural network acoustic models. A L Maas, A Y Hannun, A Y Ng, Proc. icml. icmlCiteseer3A. L. Maas, A. Y. Hannun, A. Y. Ng, Rectifier nonlinearities improve neural network acoustic models, Proc. icml, p. 3 (Citeseer, 2013).\n", "annotations": {"author": "[{\"end\":140,\"start\":76},{\"end\":202,\"start\":141},{\"end\":278,\"start\":203},{\"end\":347,\"start\":279},{\"end\":407,\"start\":348},{\"end\":471,\"start\":408}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":85},{\"end\":151,\"start\":148},{\"end\":216,\"start\":209},{\"end\":296,\"start\":286},{\"end\":362,\"start\":356},{\"end\":420,\"start\":414}]", "author_first_name": "[{\"end\":84,\"start\":76},{\"end\":147,\"start\":141},{\"end\":208,\"start\":203},{\"end\":285,\"start\":279},{\"end\":355,\"start\":348},{\"end\":413,\"start\":408}]", "author_affiliation": "[{\"end\":139,\"start\":91},{\"end\":201,\"start\":153},{\"end\":277,\"start\":218},{\"end\":346,\"start\":298},{\"end\":406,\"start\":364},{\"end\":470,\"start\":422}]", "title": "[{\"end\":73,\"start\":1},{\"end\":544,\"start\":472}]", "venue": null, "abstract": "[{\"end\":2275,\"start\":610}]", "bib_ref": "[{\"end\":2650,\"start\":2647},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2653,\"start\":2650},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2656,\"start\":2653},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2659,\"start\":2656},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2662,\"start\":2659},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2736,\"start\":2733},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2739,\"start\":2736},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2742,\"start\":2739},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2745,\"start\":2742},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2749,\"start\":2745},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3060,\"start\":3056},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3645,\"start\":3642},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3647,\"start\":3645},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3650,\"start\":3647},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3902,\"start\":3898},{\"end\":4326,\"start\":4322},{\"end\":4330,\"start\":4326},{\"end\":4334,\"start\":4330},{\"end\":4338,\"start\":4334},{\"end\":5005,\"start\":5001},{\"end\":5008,\"start\":5005},{\"end\":6097,\"start\":6093},{\"end\":6101,\"start\":6097},{\"end\":6105,\"start\":6101},{\"end\":6199,\"start\":6195},{\"end\":6281,\"start\":6277},{\"end\":6557,\"start\":6553},{\"end\":6560,\"start\":6557},{\"end\":6725,\"start\":6721},{\"end\":7206,\"start\":7202},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7363,\"start\":7360},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7365,\"start\":7363},{\"end\":7369,\"start\":7365},{\"end\":7373,\"start\":7369},{\"end\":7377,\"start\":7373},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7381,\"start\":7377},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7385,\"start\":7381},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7389,\"start\":7385},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7393,\"start\":7389},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7413,\"start\":7410},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7416,\"start\":7413},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7607,\"start\":7604},{\"end\":7778,\"start\":7774},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7997,\"start\":7993},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8755,\"start\":8752},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8757,\"start\":8755},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8760,\"start\":8757},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8813,\"start\":8809},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8816,\"start\":8813},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10015,\"start\":10011},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11468,\"start\":11465},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12653,\"start\":12650},{\"end\":13356,\"start\":13350},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14503,\"start\":14499},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14548,\"start\":14544},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15569,\"start\":15565},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15743,\"start\":15739},{\"end\":17648,\"start\":17644},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18075,\"start\":18071},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18871,\"start\":18868},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19305,\"start\":19302},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20742,\"start\":20738},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21994,\"start\":21991},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26857,\"start\":26853},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27401,\"start\":27397},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27404,\"start\":27401},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27453,\"start\":27449},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27691,\"start\":27687},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32987,\"start\":32983},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33095,\"start\":33091},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33098,\"start\":33095},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33144,\"start\":33141},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33147,\"start\":33144},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33266,\"start\":33263},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33309,\"start\":33305},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33506,\"start\":33502},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33688,\"start\":33685},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":35476,\"start\":35472},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35851,\"start\":35848},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":38336,\"start\":38333},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":38436,\"start\":38433},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":38604,\"start\":38601},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":41093,\"start\":41089},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":41096,\"start\":41093},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":43963,\"start\":43959},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":43966,\"start\":43963},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":44005,\"start\":44001},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":44009,\"start\":44005},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":44013,\"start\":44009},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":44941,\"start\":44937},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":45206,\"start\":45202},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":45312,\"start\":45308},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":48473,\"start\":48469},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":48506,\"start\":48502},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":48761,\"start\":48757},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":53632,\"start\":53629},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":63667,\"start\":63663}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54979,\"start\":54253},{\"attributes\":{\"id\":\"fig_1\"},\"end\":55344,\"start\":54980},{\"attributes\":{\"id\":\"fig_4\"},\"end\":56282,\"start\":55345},{\"attributes\":{\"id\":\"fig_5\"},\"end\":56685,\"start\":56283},{\"attributes\":{\"id\":\"fig_6\"},\"end\":56911,\"start\":56686},{\"attributes\":{\"id\":\"fig_7\"},\"end\":57185,\"start\":56912},{\"attributes\":{\"id\":\"fig_8\"},\"end\":58356,\"start\":57186},{\"attributes\":{\"id\":\"fig_9\"},\"end\":61990,\"start\":58357},{\"attributes\":{\"id\":\"fig_10\"},\"end\":62515,\"start\":61991},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":63060,\"start\":62516},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":63410,\"start\":63061},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":64038,\"start\":63411},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":64733,\"start\":64039},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":65646,\"start\":64734}]", "paragraph": "[{\"end\":2750,\"start\":2291},{\"end\":3335,\"start\":2752},{\"end\":4666,\"start\":3337},{\"end\":5999,\"start\":4668},{\"end\":6870,\"start\":6001},{\"end\":7905,\"start\":6872},{\"end\":8385,\"start\":7907},{\"end\":8817,\"start\":8387},{\"end\":9163,\"start\":8819},{\"end\":9959,\"start\":9165},{\"end\":10651,\"start\":9961},{\"end\":11380,\"start\":10653},{\"end\":11953,\"start\":11382},{\"end\":12654,\"start\":12006},{\"end\":13077,\"start\":12656},{\"end\":13681,\"start\":13079},{\"end\":14099,\"start\":13683},{\"end\":14690,\"start\":14111},{\"end\":15744,\"start\":14713},{\"end\":16030,\"start\":15746},{\"end\":16236,\"start\":16032},{\"end\":16701,\"start\":16265},{\"end\":16995,\"start\":16703},{\"end\":17518,\"start\":16997},{\"end\":17912,\"start\":17520},{\"end\":18443,\"start\":17914},{\"end\":19099,\"start\":18445},{\"end\":19338,\"start\":19148},{\"end\":19681,\"start\":19340},{\"end\":20534,\"start\":19683},{\"end\":21552,\"start\":20536},{\"end\":22832,\"start\":21554},{\"end\":23328,\"start\":22890},{\"end\":23922,\"start\":23330},{\"end\":24255,\"start\":23924},{\"end\":25161,\"start\":24257},{\"end\":25724,\"start\":25163},{\"end\":26642,\"start\":25739},{\"end\":27066,\"start\":26644},{\"end\":27857,\"start\":27068},{\"end\":29673,\"start\":27881},{\"end\":29866,\"start\":29710},{\"end\":30475,\"start\":29868},{\"end\":30624,\"start\":30477},{\"end\":31166,\"start\":30626},{\"end\":31626,\"start\":31168},{\"end\":32040,\"start\":31650},{\"end\":32348,\"start\":32042},{\"end\":32474,\"start\":32350},{\"end\":33148,\"start\":32512},{\"end\":33463,\"start\":33150},{\"end\":33714,\"start\":33488},{\"end\":33879,\"start\":33726},{\"end\":34279,\"start\":33881},{\"end\":34638,\"start\":34304},{\"end\":34961,\"start\":34654},{\"end\":35427,\"start\":34989},{\"end\":35707,\"start\":35429},{\"end\":36325,\"start\":35734},{\"end\":36612,\"start\":36349},{\"end\":36767,\"start\":36634},{\"end\":36981,\"start\":36769},{\"end\":37168,\"start\":36993},{\"end\":37736,\"start\":37253},{\"end\":38170,\"start\":37738},{\"end\":38437,\"start\":38185},{\"end\":38605,\"start\":38439},{\"end\":38859,\"start\":38607},{\"end\":38973,\"start\":38879},{\"end\":39841,\"start\":39001},{\"end\":40383,\"start\":39859},{\"end\":40568,\"start\":40385},{\"end\":41097,\"start\":40570},{\"end\":41402,\"start\":41129},{\"end\":41819,\"start\":41436},{\"end\":41979,\"start\":41821},{\"end\":42049,\"start\":41981},{\"end\":42171,\"start\":42051},{\"end\":42303,\"start\":42173},{\"end\":42808,\"start\":42305},{\"end\":43717,\"start\":42810},{\"end\":44014,\"start\":43742},{\"end\":44452,\"start\":44016},{\"end\":44625,\"start\":44538},{\"end\":44897,\"start\":44627},{\"end\":45062,\"start\":44899},{\"end\":45697,\"start\":45077},{\"end\":45891,\"start\":45699},{\"end\":46187,\"start\":45893},{\"end\":46568,\"start\":46233},{\"end\":46669,\"start\":46570},{\"end\":46730,\"start\":46671},{\"end\":46765,\"start\":46732},{\"end\":46813,\"start\":46767},{\"end\":46837,\"start\":46815},{\"end\":46936,\"start\":46839},{\"end\":46992,\"start\":46938},{\"end\":47024,\"start\":46994},{\"end\":47058,\"start\":47026},{\"end\":47575,\"start\":47060},{\"end\":47797,\"start\":47664},{\"end\":48159,\"start\":47799},{\"end\":48860,\"start\":48184},{\"end\":49687,\"start\":48862},{\"end\":50409,\"start\":49714},{\"end\":50957,\"start\":50440},{\"end\":51444,\"start\":50959},{\"end\":51475,\"start\":51471},{\"end\":53903,\"start\":51477},{\"end\":54252,\"start\":53909}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":32511,\"start\":32475},{\"attributes\":{\"id\":\"formula_1\"},\"end\":36633,\"start\":36613},{\"attributes\":{\"id\":\"formula_2\"},\"end\":37252,\"start\":37169},{\"attributes\":{\"id\":\"formula_3\"},\"end\":38878,\"start\":38860},{\"attributes\":{\"id\":\"formula_4\"},\"end\":39858,\"start\":39842},{\"attributes\":{\"id\":\"formula_5\"},\"end\":44537,\"start\":44453}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":46729,\"start\":46721},{\"end\":46764,\"start\":46756},{\"end\":46812,\"start\":46804},{\"end\":46836,\"start\":46828},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":48859,\"start\":48851},{\"end\":49686,\"start\":49678},{\"end\":50487,\"start\":50479},{\"end\":51251,\"start\":51243},{\"end\":51692,\"start\":51682},{\"end\":52531,\"start\":52523},{\"end\":53045,\"start\":53037}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2289,\"start\":2277},{\"attributes\":{\"n\":\"2.\"},\"end\":11963,\"start\":11956},{\"end\":12004,\"start\":11966},{\"end\":14109,\"start\":14102},{\"end\":14711,\"start\":14693},{\"end\":16263,\"start\":16239},{\"end\":19146,\"start\":19102},{\"end\":22888,\"start\":22835},{\"attributes\":{\"n\":\"3.\"},\"end\":25737,\"start\":25727},{\"end\":27879,\"start\":27860},{\"attributes\":{\"n\":\"4.\"},\"end\":29697,\"start\":29676},{\"end\":29708,\"start\":29700},{\"end\":31648,\"start\":31629},{\"end\":33486,\"start\":33466},{\"end\":33724,\"start\":33717},{\"end\":34302,\"start\":34282},{\"end\":34652,\"start\":34641},{\"end\":34987,\"start\":34964},{\"end\":35732,\"start\":35710},{\"end\":36347,\"start\":36328},{\"end\":36991,\"start\":36984},{\"end\":38183,\"start\":38173},{\"end\":38999,\"start\":38976},{\"end\":41127,\"start\":41100},{\"attributes\":{\"n\":\"2.\"},\"end\":41434,\"start\":41405},{\"end\":43740,\"start\":43720},{\"end\":45075,\"start\":45065},{\"attributes\":{\"n\":\"5.\"},\"end\":46205,\"start\":46190},{\"end\":46231,\"start\":46208},{\"end\":47662,\"start\":47578},{\"end\":48182,\"start\":48162},{\"end\":49712,\"start\":49690},{\"end\":50438,\"start\":50412},{\"end\":51469,\"start\":51447},{\"end\":53907,\"start\":53906},{\"end\":54262,\"start\":54254},{\"end\":54989,\"start\":54981},{\"end\":55354,\"start\":55346},{\"end\":56921,\"start\":56913},{\"end\":57194,\"start\":57187},{\"end\":62002,\"start\":61992},{\"end\":63072,\"start\":63062}]", "table": "[{\"end\":63060,\"start\":62865},{\"end\":63410,\"start\":63146},{\"end\":64733,\"start\":64082},{\"end\":65646,\"start\":65519}]", "figure_caption": "[{\"end\":54979,\"start\":54264},{\"end\":55344,\"start\":54991},{\"end\":56282,\"start\":55356},{\"end\":56685,\"start\":56285},{\"end\":56911,\"start\":56688},{\"end\":57185,\"start\":56923},{\"end\":58356,\"start\":57195},{\"end\":61990,\"start\":58359},{\"end\":62515,\"start\":62005},{\"end\":62865,\"start\":62518},{\"end\":63146,\"start\":63075},{\"end\":64038,\"start\":63413},{\"end\":64082,\"start\":64041},{\"end\":65519,\"start\":64736}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12083,\"start\":12075},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12788,\"start\":12780},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13076,\"start\":13061},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13802,\"start\":13793},{\"end\":15105,\"start\":15097},{\"end\":15415,\"start\":15407},{\"end\":15895,\"start\":15887},{\"end\":16022,\"start\":16013},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16534,\"start\":16525},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16581,\"start\":16572},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16632,\"start\":16623},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16821,\"start\":16813},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17192,\"start\":17183},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17342,\"start\":17332},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17747,\"start\":17738},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17910,\"start\":17901},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18234,\"start\":18225},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18441,\"start\":18432},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19430,\"start\":19421},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19836,\"start\":19827},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20214,\"start\":20205},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20615,\"start\":20606},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21088,\"start\":21079},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21550,\"start\":21541},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21689,\"start\":21679},{\"end\":23511,\"start\":23502},{\"end\":24060,\"start\":24051},{\"end\":24408,\"start\":24399},{\"end\":25236,\"start\":25226},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29865,\"start\":29857},{\"end\":30013,\"start\":30007},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33778,\"start\":33770},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":36514,\"start\":36506},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":39955,\"start\":39947},{\"end\":41367,\"start\":41357},{\"end\":41978,\"start\":41969},{\"end\":43928,\"start\":43919},{\"end\":44758,\"start\":44749},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46567,\"start\":46558},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":46668,\"start\":46659},{\"end\":48052,\"start\":48043},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":49836,\"start\":49828},{\"end\":49848,\"start\":49839},{\"end\":50285,\"start\":50276},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":52202,\"start\":52192},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53559,\"start\":53557}]", "bib_author_first_name": "[{\"end\":73202,\"start\":73201},{\"end\":73213,\"start\":73212},{\"end\":73228,\"start\":73227},{\"end\":73238,\"start\":73237},{\"end\":73536,\"start\":73535},{\"end\":73544,\"start\":73543},{\"end\":73547,\"start\":73545},{\"end\":73556,\"start\":73555},{\"end\":73884,\"start\":73883},{\"end\":73895,\"start\":73894},{\"end\":73902,\"start\":73901},{\"end\":73917,\"start\":73916},{\"end\":73930,\"start\":73929},{\"end\":73941,\"start\":73940},{\"end\":73951,\"start\":73950},{\"end\":74209,\"start\":74208},{\"end\":74216,\"start\":74215},{\"end\":74227,\"start\":74226},{\"end\":74241,\"start\":74240},{\"end\":74251,\"start\":74250},{\"end\":74504,\"start\":74500},{\"end\":74512,\"start\":74511},{\"end\":74514,\"start\":74513},{\"end\":74525,\"start\":74524},{\"end\":74924,\"start\":74923},{\"end\":74935,\"start\":74934},{\"end\":74949,\"start\":74948},{\"end\":74958,\"start\":74957},{\"end\":74970,\"start\":74969},{\"end\":74982,\"start\":74981},{\"end\":74990,\"start\":74989},{\"end\":75005,\"start\":75004},{\"end\":75777,\"start\":75776},{\"end\":75779,\"start\":75778},{\"end\":75790,\"start\":75789},{\"end\":75792,\"start\":75791},{\"end\":75801,\"start\":75800},{\"end\":75803,\"start\":75802},{\"end\":75986,\"start\":75980},{\"end\":76165,\"start\":76164},{\"end\":76179,\"start\":76178},{\"end\":76190,\"start\":76189},{\"end\":76203,\"start\":76202},{\"end\":76215,\"start\":76214},{\"end\":76225,\"start\":76224},{\"end\":76592,\"start\":76591},{\"end\":76599,\"start\":76598},{\"end\":76608,\"start\":76607},{\"end\":76615,\"start\":76614},{\"end\":76625,\"start\":76624},{\"end\":76634,\"start\":76633},{\"end\":77100,\"start\":77099},{\"end\":77112,\"start\":77111},{\"end\":77121,\"start\":77120},{\"end\":77131,\"start\":77130},{\"end\":77139,\"start\":77138},{\"end\":77388,\"start\":77387},{\"end\":77400,\"start\":77399},{\"end\":77409,\"start\":77408},{\"end\":77422,\"start\":77421},{\"end\":77707,\"start\":77706},{\"end\":77715,\"start\":77714},{\"end\":77723,\"start\":77722},{\"end\":77730,\"start\":77729},{\"end\":77736,\"start\":77735},{\"end\":77897,\"start\":77896},{\"end\":77904,\"start\":77903},{\"end\":77915,\"start\":77914},{\"end\":78199,\"start\":78198},{\"end\":78215,\"start\":78214},{\"end\":78226,\"start\":78225},{\"end\":78238,\"start\":78237},{\"end\":78248,\"start\":78247},{\"end\":78691,\"start\":78690},{\"end\":78701,\"start\":78700},{\"end\":78713,\"start\":78712},{\"end\":78724,\"start\":78723},{\"end\":78736,\"start\":78735},{\"end\":78748,\"start\":78747},{\"end\":78750,\"start\":78749},{\"end\":78762,\"start\":78761},{\"end\":79406,\"start\":79405},{\"end\":79414,\"start\":79413},{\"end\":79422,\"start\":79421},{\"end\":79432,\"start\":79431},{\"end\":79799,\"start\":79798},{\"end\":79810,\"start\":79809},{\"end\":79820,\"start\":79819},{\"end\":79822,\"start\":79821},{\"end\":79837,\"start\":79836},{\"end\":79852,\"start\":79851},{\"end\":79863,\"start\":79862},{\"end\":79865,\"start\":79864},{\"end\":79873,\"start\":79872},{\"end\":80374,\"start\":80373},{\"end\":80376,\"start\":80375},{\"end\":80720,\"start\":80719},{\"end\":80733,\"start\":80732},{\"end\":80746,\"start\":80745},{\"end\":80760,\"start\":80759},{\"end\":80771,\"start\":80770},{\"end\":80782,\"start\":80781},{\"end\":80786,\"start\":80783},{\"end\":80797,\"start\":80796},{\"end\":80805,\"start\":80804},{\"end\":80818,\"start\":80817},{\"end\":80827,\"start\":80826},{\"end\":81168,\"start\":81164},{\"end\":81788,\"start\":81787},{\"end\":81796,\"start\":81795},{\"end\":81811,\"start\":81810},{\"end\":81821,\"start\":81820},{\"end\":81831,\"start\":81830},{\"end\":81845,\"start\":81844},{\"end\":81857,\"start\":81856},{\"end\":82171,\"start\":82170},{\"end\":82178,\"start\":82177},{\"end\":82184,\"start\":82183},{\"end\":82195,\"start\":82194},{\"end\":82510,\"start\":82509},{\"end\":82521,\"start\":82520},{\"end\":82535,\"start\":82534},{\"end\":82537,\"start\":82536},{\"end\":82550,\"start\":82549},{\"end\":82561,\"start\":82560},{\"end\":82571,\"start\":82570},{\"end\":82580,\"start\":82579},{\"end\":82582,\"start\":82581},{\"end\":82590,\"start\":82589},{\"end\":82600,\"start\":82599},{\"end\":82610,\"start\":82609},{\"end\":82957,\"start\":82956},{\"end\":82964,\"start\":82963},{\"end\":82966,\"start\":82965},{\"end\":82976,\"start\":82975},{\"end\":83217,\"start\":83216},{\"end\":83228,\"start\":83227},{\"end\":83235,\"start\":83234},{\"end\":83429,\"start\":83428},{\"end\":83441,\"start\":83440},{\"end\":83451,\"start\":83450},{\"end\":83463,\"start\":83462},{\"end\":83474,\"start\":83473},{\"end\":83787,\"start\":83786},{\"end\":83795,\"start\":83794},{\"end\":83805,\"start\":83804},{\"end\":84306,\"start\":84305},{\"end\":84308,\"start\":84307},{\"end\":84321,\"start\":84320},{\"end\":84332,\"start\":84331},{\"end\":84344,\"start\":84343},{\"end\":84357,\"start\":84356},{\"end\":84369,\"start\":84368},{\"end\":84782,\"start\":84781},{\"end\":84789,\"start\":84788},{\"end\":84808,\"start\":84807},{\"end\":84820,\"start\":84819},{\"end\":84832,\"start\":84831},{\"end\":84844,\"start\":84843},{\"end\":84855,\"start\":84854},{\"end\":85242,\"start\":85241},{\"end\":85256,\"start\":85255},{\"end\":85480,\"start\":85479},{\"end\":85489,\"start\":85488},{\"end\":85895,\"start\":85894},{\"end\":85902,\"start\":85901},{\"end\":85909,\"start\":85908},{\"end\":85916,\"start\":85915},{\"end\":85924,\"start\":85923},{\"end\":85933,\"start\":85932},{\"end\":85935,\"start\":85934},{\"end\":86229,\"start\":86228},{\"end\":86240,\"start\":86239},{\"end\":86251,\"start\":86250},{\"end\":86269,\"start\":86268},{\"end\":86271,\"start\":86270},{\"end\":86582,\"start\":86581},{\"end\":86592,\"start\":86591},{\"end\":86601,\"start\":86600},{\"end\":86610,\"start\":86609},{\"end\":86619,\"start\":86618},{\"end\":86631,\"start\":86630},{\"end\":86641,\"start\":86640},{\"end\":86652,\"start\":86651},{\"end\":86659,\"start\":86658},{\"end\":86673,\"start\":86672},{\"end\":86683,\"start\":86682},{\"end\":86696,\"start\":86695},{\"end\":86704,\"start\":86703},{\"end\":86712,\"start\":86711},{\"end\":86722,\"start\":86721},{\"end\":86732,\"start\":86731},{\"end\":86742,\"start\":86741},{\"end\":86758,\"start\":86757},{\"end\":86769,\"start\":86768},{\"end\":86777,\"start\":86776},{\"end\":86784,\"start\":86783},{\"end\":87437,\"start\":87436},{\"end\":87449,\"start\":87448},{\"end\":87459,\"start\":87458},{\"end\":87469,\"start\":87468},{\"end\":87484,\"start\":87478},{\"end\":87634,\"start\":87633},{\"end\":87636,\"start\":87635},{\"end\":87646,\"start\":87645},{\"end\":87777,\"start\":87776},{\"end\":87786,\"start\":87785},{\"end\":87798,\"start\":87797},{\"end\":87806,\"start\":87805},{\"end\":87816,\"start\":87815},{\"end\":87829,\"start\":87828},{\"end\":87831,\"start\":87830},{\"end\":87840,\"start\":87839},{\"end\":87842,\"start\":87841},{\"end\":87851,\"start\":87850},{\"end\":87861,\"start\":87860},{\"end\":88194,\"start\":88193},{\"end\":88196,\"start\":88195},{\"end\":88204,\"start\":88203},{\"end\":88206,\"start\":88205},{\"end\":88216,\"start\":88215},{\"end\":88218,\"start\":88217}]", "bib_author_last_name": "[{\"end\":73210,\"start\":73203},{\"end\":73225,\"start\":73214},{\"end\":73235,\"start\":73229},{\"end\":73246,\"start\":73239},{\"end\":73541,\"start\":73537},{\"end\":73553,\"start\":73548},{\"end\":73560,\"start\":73557},{\"end\":73892,\"start\":73885},{\"end\":73899,\"start\":73896},{\"end\":73914,\"start\":73903},{\"end\":73927,\"start\":73918},{\"end\":73938,\"start\":73931},{\"end\":73948,\"start\":73942},{\"end\":73958,\"start\":73952},{\"end\":74213,\"start\":74210},{\"end\":74224,\"start\":74217},{\"end\":74238,\"start\":74228},{\"end\":74248,\"start\":74242},{\"end\":74258,\"start\":74252},{\"end\":74509,\"start\":74505},{\"end\":74522,\"start\":74515},{\"end\":74529,\"start\":74526},{\"end\":74932,\"start\":74925},{\"end\":74946,\"start\":74936},{\"end\":74955,\"start\":74950},{\"end\":74967,\"start\":74959},{\"end\":74979,\"start\":74971},{\"end\":74987,\"start\":74983},{\"end\":75002,\"start\":74991},{\"end\":75012,\"start\":75006},{\"end\":75787,\"start\":75780},{\"end\":75798,\"start\":75793},{\"end\":75810,\"start\":75804},{\"end\":75996,\"start\":75987},{\"end\":76176,\"start\":76166},{\"end\":76187,\"start\":76180},{\"end\":76200,\"start\":76191},{\"end\":76212,\"start\":76204},{\"end\":76222,\"start\":76216},{\"end\":76234,\"start\":76226},{\"end\":76596,\"start\":76593},{\"end\":76605,\"start\":76600},{\"end\":76612,\"start\":76609},{\"end\":76622,\"start\":76616},{\"end\":76631,\"start\":76626},{\"end\":76647,\"start\":76635},{\"end\":77109,\"start\":77101},{\"end\":77118,\"start\":77113},{\"end\":77128,\"start\":77122},{\"end\":77136,\"start\":77132},{\"end\":77145,\"start\":77140},{\"end\":77394,\"start\":77389},{\"end\":77403,\"start\":77401},{\"end\":77416,\"start\":77410},{\"end\":77428,\"start\":77423},{\"end\":77712,\"start\":77708},{\"end\":77720,\"start\":77716},{\"end\":77727,\"start\":77724},{\"end\":77733,\"start\":77731},{\"end\":77739,\"start\":77737},{\"end\":77901,\"start\":77898},{\"end\":77912,\"start\":77905},{\"end\":77922,\"start\":77916},{\"end\":78212,\"start\":78200},{\"end\":78223,\"start\":78216},{\"end\":78235,\"start\":78227},{\"end\":78245,\"start\":78239},{\"end\":78257,\"start\":78249},{\"end\":78698,\"start\":78692},{\"end\":78710,\"start\":78702},{\"end\":78721,\"start\":78714},{\"end\":78733,\"start\":78725},{\"end\":78745,\"start\":78737},{\"end\":78759,\"start\":78751},{\"end\":78769,\"start\":78763},{\"end\":79411,\"start\":79407},{\"end\":79419,\"start\":79415},{\"end\":79429,\"start\":79423},{\"end\":79443,\"start\":79433},{\"end\":79624,\"start\":79618},{\"end\":79807,\"start\":79800},{\"end\":79817,\"start\":79811},{\"end\":79834,\"start\":79823},{\"end\":79849,\"start\":79838},{\"end\":79860,\"start\":79853},{\"end\":79870,\"start\":79866},{\"end\":79882,\"start\":79874},{\"end\":80091,\"start\":80085},{\"end\":80384,\"start\":80377},{\"end\":80730,\"start\":80721},{\"end\":80743,\"start\":80734},{\"end\":80757,\"start\":80747},{\"end\":80768,\"start\":80761},{\"end\":80779,\"start\":80772},{\"end\":80794,\"start\":80787},{\"end\":80802,\"start\":80798},{\"end\":80815,\"start\":80806},{\"end\":80824,\"start\":80819},{\"end\":80833,\"start\":80828},{\"end\":81177,\"start\":81169},{\"end\":81187,\"start\":81179},{\"end\":81793,\"start\":81789},{\"end\":81808,\"start\":81797},{\"end\":81818,\"start\":81812},{\"end\":81828,\"start\":81822},{\"end\":81842,\"start\":81832},{\"end\":81854,\"start\":81846},{\"end\":81868,\"start\":81858},{\"end\":82175,\"start\":82172},{\"end\":82181,\"start\":82179},{\"end\":82192,\"start\":82185},{\"end\":82200,\"start\":82196},{\"end\":82518,\"start\":82511},{\"end\":82532,\"start\":82522},{\"end\":82547,\"start\":82538},{\"end\":82558,\"start\":82551},{\"end\":82568,\"start\":82562},{\"end\":82577,\"start\":82572},{\"end\":82587,\"start\":82583},{\"end\":82597,\"start\":82591},{\"end\":82607,\"start\":82601},{\"end\":82961,\"start\":82958},{\"end\":82973,\"start\":82967},{\"end\":82983,\"start\":82977},{\"end\":83225,\"start\":83218},{\"end\":83232,\"start\":83229},{\"end\":83242,\"start\":83236},{\"end\":83438,\"start\":83430},{\"end\":83448,\"start\":83442},{\"end\":83460,\"start\":83452},{\"end\":83471,\"start\":83464},{\"end\":83481,\"start\":83475},{\"end\":83792,\"start\":83788},{\"end\":83802,\"start\":83796},{\"end\":83813,\"start\":83806},{\"end\":84318,\"start\":84309},{\"end\":84329,\"start\":84322},{\"end\":84341,\"start\":84333},{\"end\":84354,\"start\":84345},{\"end\":84366,\"start\":84358},{\"end\":84379,\"start\":84370},{\"end\":84786,\"start\":84783},{\"end\":84805,\"start\":84790},{\"end\":84817,\"start\":84809},{\"end\":84829,\"start\":84821},{\"end\":84841,\"start\":84833},{\"end\":84852,\"start\":84845},{\"end\":84862,\"start\":84856},{\"end\":85253,\"start\":85243},{\"end\":85268,\"start\":85257},{\"end\":85486,\"start\":85481},{\"end\":85499,\"start\":85490},{\"end\":85899,\"start\":85896},{\"end\":85906,\"start\":85903},{\"end\":85913,\"start\":85910},{\"end\":85921,\"start\":85917},{\"end\":85930,\"start\":85925},{\"end\":85940,\"start\":85936},{\"end\":86237,\"start\":86230},{\"end\":86248,\"start\":86241},{\"end\":86266,\"start\":86252},{\"end\":86280,\"start\":86272},{\"end\":86437,\"start\":86428},{\"end\":86589,\"start\":86583},{\"end\":86598,\"start\":86593},{\"end\":86607,\"start\":86602},{\"end\":86616,\"start\":86611},{\"end\":86628,\"start\":86620},{\"end\":86638,\"start\":86632},{\"end\":86649,\"start\":86642},{\"end\":86656,\"start\":86653},{\"end\":86670,\"start\":86660},{\"end\":86680,\"start\":86674},{\"end\":86693,\"start\":86684},{\"end\":86701,\"start\":86697},{\"end\":86709,\"start\":86705},{\"end\":86719,\"start\":86713},{\"end\":86729,\"start\":86723},{\"end\":86739,\"start\":86733},{\"end\":86755,\"start\":86743},{\"end\":86766,\"start\":86759},{\"end\":86774,\"start\":86770},{\"end\":86781,\"start\":86778},{\"end\":86793,\"start\":86785},{\"end\":87446,\"start\":87438},{\"end\":87456,\"start\":87450},{\"end\":87466,\"start\":87460},{\"end\":87476,\"start\":87470},{\"end\":87643,\"start\":87637},{\"end\":87649,\"start\":87647},{\"end\":87783,\"start\":87778},{\"end\":87795,\"start\":87787},{\"end\":87803,\"start\":87799},{\"end\":87813,\"start\":87807},{\"end\":87826,\"start\":87817},{\"end\":87837,\"start\":87832},{\"end\":87848,\"start\":87843},{\"end\":87858,\"start\":87852},{\"end\":87869,\"start\":87862},{\"end\":88201,\"start\":88197},{\"end\":88213,\"start\":88207},{\"end\":88221,\"start\":88219}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":73455,\"start\":73201},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":199541175},\"end\":73822,\"start\":73457},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":58031572},\"end\":74148,\"start\":73824},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":224828219},\"end\":74420,\"start\":74150},{\"attributes\":{\"id\":\"b4\"},\"end\":74664,\"start\":74422},{\"attributes\":{\"id\":\"b5\"},\"end\":74815,\"start\":74666},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":213713448},\"end\":75308,\"start\":74817},{\"attributes\":{\"id\":\"b7\"},\"end\":75440,\"start\":75310},{\"attributes\":{\"id\":\"b8\"},\"end\":75570,\"start\":75442},{\"attributes\":{\"id\":\"b9\"},\"end\":75702,\"start\":75572},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4712578},\"end\":75976,\"start\":75704},{\"attributes\":{\"id\":\"b11\"},\"end\":76098,\"start\":75978},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206722232},\"end\":76516,\"start\":76100},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":218875900},\"end\":77027,\"start\":76518},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":234762823},\"end\":77338,\"start\":77029},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235650916},\"end\":77651,\"start\":77340},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":228083588},\"end\":77894,\"start\":77653},{\"attributes\":{\"doi\":\"arXiv:1901.07517\",\"id\":\"b17\"},\"end\":78196,\"start\":77896},{\"attributes\":{\"doi\":\"arXiv:2012.03094\",\"id\":\"b18\"},\"end\":78570,\"start\":78198},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":44061745},\"end\":79113,\"start\":78572},{\"attributes\":{\"id\":\"b20\"},\"end\":79381,\"start\":79115},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":204780964},\"end\":79594,\"start\":79383},{\"attributes\":{\"id\":\"b22\"},\"end\":79716,\"start\":79596},{\"attributes\":{\"id\":\"b23\"},\"end\":80081,\"start\":79718},{\"attributes\":{\"id\":\"b24\"},\"end\":80311,\"start\":80083},{\"attributes\":{\"id\":\"b25\"},\"end\":80588,\"start\":80313},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":238034403},\"end\":81160,\"start\":80590},{\"attributes\":{\"id\":\"b27\"},\"end\":81290,\"start\":81162},{\"attributes\":{\"id\":\"b28\"},\"end\":81534,\"start\":81292},{\"attributes\":{\"id\":\"b29\"},\"end\":81737,\"start\":81536},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15238391},\"end\":82168,\"start\":81739},{\"attributes\":{\"doi\":\"arXiv:1704.07978\",\"id\":\"b31\"},\"end\":82413,\"start\":82170},{\"attributes\":{\"id\":\"b32\"},\"end\":82859,\"start\":82415},{\"attributes\":{\"doi\":\"arXiv:1803.01271\",\"id\":\"b33\"},\"end\":83155,\"start\":82861},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3742121},\"end\":83426,\"start\":83157},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b35\"},\"end\":83694,\"start\":83428},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":103456},\"end\":84271,\"start\":83696},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":59606283},\"end\":84684,\"start\":84273},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5590763},\"end\":85215,\"start\":84686},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1915014},\"end\":85375,\"start\":85217},{\"attributes\":{\"id\":\"b40\"},\"end\":85635,\"start\":85377},{\"attributes\":{\"id\":\"b41\"},\"end\":85816,\"start\":85637},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":49865227},\"end\":86179,\"start\":85818},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":9401721},\"end\":86424,\"start\":86181},{\"attributes\":{\"id\":\"b44\"},\"end\":86510,\"start\":86426},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":202786778},\"end\":87434,\"start\":86512},{\"attributes\":{\"id\":\"b46\"},\"end\":87587,\"start\":87436},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":6628106},\"end\":87734,\"start\":87589},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":762012},\"end\":88126,\"start\":87736},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":16489696},\"end\":88383,\"start\":88128}]", "bib_title": "[{\"end\":73533,\"start\":73457},{\"end\":73881,\"start\":73824},{\"end\":74206,\"start\":74150},{\"end\":74921,\"start\":74817},{\"end\":75774,\"start\":75704},{\"end\":76162,\"start\":76100},{\"end\":76589,\"start\":76518},{\"end\":77097,\"start\":77029},{\"end\":77385,\"start\":77340},{\"end\":77704,\"start\":77653},{\"end\":78688,\"start\":78572},{\"end\":79403,\"start\":79383},{\"end\":80717,\"start\":80590},{\"end\":81347,\"start\":81292},{\"end\":81594,\"start\":81536},{\"end\":81785,\"start\":81739},{\"end\":82507,\"start\":82415},{\"end\":83214,\"start\":83157},{\"end\":83784,\"start\":83696},{\"end\":84303,\"start\":84273},{\"end\":84779,\"start\":84686},{\"end\":85239,\"start\":85217},{\"end\":85892,\"start\":85818},{\"end\":86226,\"start\":86181},{\"end\":86579,\"start\":86512},{\"end\":87631,\"start\":87589},{\"end\":87774,\"start\":87736},{\"end\":88191,\"start\":88128}]", "bib_author": "[{\"end\":73212,\"start\":73201},{\"end\":73227,\"start\":73212},{\"end\":73237,\"start\":73227},{\"end\":73248,\"start\":73237},{\"end\":73543,\"start\":73535},{\"end\":73555,\"start\":73543},{\"end\":73562,\"start\":73555},{\"end\":73894,\"start\":73883},{\"end\":73901,\"start\":73894},{\"end\":73916,\"start\":73901},{\"end\":73929,\"start\":73916},{\"end\":73940,\"start\":73929},{\"end\":73950,\"start\":73940},{\"end\":73960,\"start\":73950},{\"end\":74215,\"start\":74208},{\"end\":74226,\"start\":74215},{\"end\":74240,\"start\":74226},{\"end\":74250,\"start\":74240},{\"end\":74260,\"start\":74250},{\"end\":74511,\"start\":74500},{\"end\":74524,\"start\":74511},{\"end\":74531,\"start\":74524},{\"end\":74934,\"start\":74923},{\"end\":74948,\"start\":74934},{\"end\":74957,\"start\":74948},{\"end\":74969,\"start\":74957},{\"end\":74981,\"start\":74969},{\"end\":74989,\"start\":74981},{\"end\":75004,\"start\":74989},{\"end\":75014,\"start\":75004},{\"end\":75789,\"start\":75776},{\"end\":75800,\"start\":75789},{\"end\":75812,\"start\":75800},{\"end\":75998,\"start\":75980},{\"end\":76178,\"start\":76164},{\"end\":76189,\"start\":76178},{\"end\":76202,\"start\":76189},{\"end\":76214,\"start\":76202},{\"end\":76224,\"start\":76214},{\"end\":76236,\"start\":76224},{\"end\":76598,\"start\":76591},{\"end\":76607,\"start\":76598},{\"end\":76614,\"start\":76607},{\"end\":76624,\"start\":76614},{\"end\":76633,\"start\":76624},{\"end\":76649,\"start\":76633},{\"end\":77111,\"start\":77099},{\"end\":77120,\"start\":77111},{\"end\":77130,\"start\":77120},{\"end\":77138,\"start\":77130},{\"end\":77147,\"start\":77138},{\"end\":77399,\"start\":77387},{\"end\":77408,\"start\":77399},{\"end\":77421,\"start\":77408},{\"end\":77433,\"start\":77421},{\"end\":77714,\"start\":77706},{\"end\":77722,\"start\":77714},{\"end\":77729,\"start\":77722},{\"end\":77735,\"start\":77729},{\"end\":77741,\"start\":77735},{\"end\":77903,\"start\":77896},{\"end\":77914,\"start\":77903},{\"end\":77924,\"start\":77914},{\"end\":78214,\"start\":78198},{\"end\":78225,\"start\":78214},{\"end\":78237,\"start\":78225},{\"end\":78247,\"start\":78237},{\"end\":78259,\"start\":78247},{\"end\":78700,\"start\":78690},{\"end\":78712,\"start\":78700},{\"end\":78723,\"start\":78712},{\"end\":78735,\"start\":78723},{\"end\":78747,\"start\":78735},{\"end\":78761,\"start\":78747},{\"end\":78771,\"start\":78761},{\"end\":79413,\"start\":79405},{\"end\":79421,\"start\":79413},{\"end\":79431,\"start\":79421},{\"end\":79445,\"start\":79431},{\"end\":79626,\"start\":79618},{\"end\":79809,\"start\":79798},{\"end\":79819,\"start\":79809},{\"end\":79836,\"start\":79819},{\"end\":79851,\"start\":79836},{\"end\":79862,\"start\":79851},{\"end\":79872,\"start\":79862},{\"end\":79884,\"start\":79872},{\"end\":80093,\"start\":80085},{\"end\":80386,\"start\":80373},{\"end\":80732,\"start\":80719},{\"end\":80745,\"start\":80732},{\"end\":80759,\"start\":80745},{\"end\":80770,\"start\":80759},{\"end\":80781,\"start\":80770},{\"end\":80796,\"start\":80781},{\"end\":80804,\"start\":80796},{\"end\":80817,\"start\":80804},{\"end\":80826,\"start\":80817},{\"end\":80835,\"start\":80826},{\"end\":81179,\"start\":81164},{\"end\":81189,\"start\":81179},{\"end\":81795,\"start\":81787},{\"end\":81810,\"start\":81795},{\"end\":81820,\"start\":81810},{\"end\":81830,\"start\":81820},{\"end\":81844,\"start\":81830},{\"end\":81856,\"start\":81844},{\"end\":81870,\"start\":81856},{\"end\":82177,\"start\":82170},{\"end\":82183,\"start\":82177},{\"end\":82194,\"start\":82183},{\"end\":82202,\"start\":82194},{\"end\":82520,\"start\":82509},{\"end\":82534,\"start\":82520},{\"end\":82549,\"start\":82534},{\"end\":82560,\"start\":82549},{\"end\":82570,\"start\":82560},{\"end\":82579,\"start\":82570},{\"end\":82589,\"start\":82579},{\"end\":82599,\"start\":82589},{\"end\":82609,\"start\":82599},{\"end\":82613,\"start\":82609},{\"end\":82963,\"start\":82956},{\"end\":82975,\"start\":82963},{\"end\":82985,\"start\":82975},{\"end\":83227,\"start\":83216},{\"end\":83234,\"start\":83227},{\"end\":83244,\"start\":83234},{\"end\":83440,\"start\":83428},{\"end\":83450,\"start\":83440},{\"end\":83462,\"start\":83450},{\"end\":83473,\"start\":83462},{\"end\":83483,\"start\":83473},{\"end\":83794,\"start\":83786},{\"end\":83804,\"start\":83794},{\"end\":83815,\"start\":83804},{\"end\":84320,\"start\":84305},{\"end\":84331,\"start\":84320},{\"end\":84343,\"start\":84331},{\"end\":84356,\"start\":84343},{\"end\":84368,\"start\":84356},{\"end\":84381,\"start\":84368},{\"end\":84788,\"start\":84781},{\"end\":84807,\"start\":84788},{\"end\":84819,\"start\":84807},{\"end\":84831,\"start\":84819},{\"end\":84843,\"start\":84831},{\"end\":84854,\"start\":84843},{\"end\":84864,\"start\":84854},{\"end\":85255,\"start\":85241},{\"end\":85270,\"start\":85255},{\"end\":85488,\"start\":85479},{\"end\":85501,\"start\":85488},{\"end\":85901,\"start\":85894},{\"end\":85908,\"start\":85901},{\"end\":85915,\"start\":85908},{\"end\":85923,\"start\":85915},{\"end\":85932,\"start\":85923},{\"end\":85942,\"start\":85932},{\"end\":86239,\"start\":86228},{\"end\":86250,\"start\":86239},{\"end\":86268,\"start\":86250},{\"end\":86282,\"start\":86268},{\"end\":86439,\"start\":86428},{\"end\":86591,\"start\":86581},{\"end\":86600,\"start\":86591},{\"end\":86609,\"start\":86600},{\"end\":86618,\"start\":86609},{\"end\":86630,\"start\":86618},{\"end\":86640,\"start\":86630},{\"end\":86651,\"start\":86640},{\"end\":86658,\"start\":86651},{\"end\":86672,\"start\":86658},{\"end\":86682,\"start\":86672},{\"end\":86695,\"start\":86682},{\"end\":86703,\"start\":86695},{\"end\":86711,\"start\":86703},{\"end\":86721,\"start\":86711},{\"end\":86731,\"start\":86721},{\"end\":86741,\"start\":86731},{\"end\":86757,\"start\":86741},{\"end\":86768,\"start\":86757},{\"end\":86776,\"start\":86768},{\"end\":86783,\"start\":86776},{\"end\":86795,\"start\":86783},{\"end\":87448,\"start\":87436},{\"end\":87458,\"start\":87448},{\"end\":87468,\"start\":87458},{\"end\":87478,\"start\":87468},{\"end\":87487,\"start\":87478},{\"end\":87645,\"start\":87633},{\"end\":87651,\"start\":87645},{\"end\":87785,\"start\":87776},{\"end\":87797,\"start\":87785},{\"end\":87805,\"start\":87797},{\"end\":87815,\"start\":87805},{\"end\":87828,\"start\":87815},{\"end\":87839,\"start\":87828},{\"end\":87850,\"start\":87839},{\"end\":87860,\"start\":87850},{\"end\":87871,\"start\":87860},{\"end\":88203,\"start\":88193},{\"end\":88215,\"start\":88203},{\"end\":88223,\"start\":88215}]", "bib_venue": "[{\"end\":73314,\"start\":73248},{\"end\":73620,\"start\":73562},{\"end\":73976,\"start\":73960},{\"end\":74276,\"start\":74260},{\"end\":74498,\"start\":74422},{\"end\":74689,\"start\":74668},{\"end\":75040,\"start\":75014},{\"end\":75336,\"start\":75312},{\"end\":75444,\"start\":75442},{\"end\":75596,\"start\":75574},{\"end\":75827,\"start\":75812},{\"end\":76288,\"start\":76236},{\"end\":76696,\"start\":76649},{\"end\":77176,\"start\":77147},{\"end\":77477,\"start\":77433},{\"end\":77759,\"start\":77741},{\"end\":78024,\"start\":77940},{\"end\":78361,\"start\":78275},{\"end\":78820,\"start\":78771},{\"end\":79132,\"start\":79117},{\"end\":79473,\"start\":79445},{\"end\":79616,\"start\":79596},{\"end\":79796,\"start\":79718},{\"end\":80371,\"start\":80313},{\"end\":80860,\"start\":80835},{\"end\":81354,\"start\":81349},{\"end\":81601,\"start\":81596},{\"end\":81943,\"start\":81870},{\"end\":82269,\"start\":82218},{\"end\":82619,\"start\":82613},{\"end\":82954,\"start\":82861},{\"end\":83280,\"start\":83244},{\"end\":83538,\"start\":83499},{\"end\":83911,\"start\":83815},{\"end\":84412,\"start\":84381},{\"end\":84934,\"start\":84864},{\"end\":85280,\"start\":85270},{\"end\":85477,\"start\":85377},{\"end\":85711,\"start\":85637},{\"end\":85977,\"start\":85942},{\"end\":86295,\"start\":86282},{\"end\":86844,\"start\":86795},{\"end\":87504,\"start\":87487},{\"end\":87655,\"start\":87651},{\"end\":87894,\"start\":87871},{\"end\":88233,\"start\":88223},{\"end\":76768,\"start\":76736},{\"end\":77508,\"start\":77479},{\"end\":83994,\"start\":83913},{\"end\":84465,\"start\":84449},{\"end\":88239,\"start\":88235}]"}}}, "year": 2023, "month": 12, "day": 17}