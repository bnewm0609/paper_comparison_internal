{"id": 220514465, "updated": "2023-10-06 13:19:11.017", "metadata": {"title": "Knowledge Distillation for Multi-task Learning", "authors": "[{\"first\":\"Wei-Hong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Hakan\",\"last\":\"Bilen\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020 Workshops", "journal": "Computer Vision \u2013 ECCV 2020 Workshops", "publication_date": {"year": 2020, "month": 7, "day": 14}, "abstract": "Multi-task learning (MTL) is to learn one single model that performs multiple tasks for achieving good performance on all tasks and lower cost on computation. Learning such a model requires to jointly optimize losses of a set of tasks with different difficulty levels, magnitudes, and characteristics (e.g. cross-entropy, Euclidean loss), leading to the imbalance problem in multi-task learning. To address the imbalance problem, we propose a knowledge distillation based method in this work. We first learn a task-specific model for each task. We then learn the multi-task model for minimizing task-specific loss and for producing the same feature with task-specific models. As the task-specific network encodes different features, we introduce small task-specific adaptors to project multi-task features to the task-specific features. In this way, the adaptors align the task-specific feature and the multi-task feature, which enables a balanced parameter sharing across tasks. Extensive experimental results demonstrate that our method can optimize a multi-task learning model in a more balanced way and achieve better overall performance.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.06889", "mag": "3043585682", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/LiB20", "doi": "10.1007/978-3-030-65414-6_13"}}, "content": {"source": {"pdf_hash": "da27f3fa73ffcb2d8488f08cb6872892ff6d9490", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.06889v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.06889", "status": "GREEN"}}, "grobid": {"id": "19a5fe545a9c3c9efb4f7019bb33acbdea61bd97", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/da27f3fa73ffcb2d8488f08cb6872892ff6d9490.txt", "contents": "\nKnowledge Distillation for Multi-task Learning\n\n\nWei-Hong Li \nVICO Group\nUniversity of Edinburgh\nUnited Kingdom\n\nHakan Bilen hbilen@ed.ac.uk \nVICO Group\nUniversity of Edinburgh\nUnited Kingdom\n\nKnowledge Distillation for Multi-task Learning\n\nMulti-task learning (MTL) is to learn one single model that performs multiple tasks for achieving good performance on all tasks and lower cost on computation. Learning such a model requires to jointly optimize losses of a set of tasks with different difficulty levels, magnitudes, and characteristics (e.g. cross-entropy, Euclidean loss), leading to the imbalance problem in multi-task learning. To address the imbalance problem, we propose a knowledge distillation based method in this work. We first learn a task-specific model for each task. We then learn the multitask model for minimizing task-specific loss and for producing the same feature with task-specific models. As the task-specific network encodes different features, we introduce small task-specific adaptors to project multi-task features to the task-specific features. In this way, the adaptors align the task-specific feature and the multi-task feature, which enables a balanced parameter sharing across tasks. Extensive experimental results demonstrate that our method can optimize a multi-task learning model in a more balanced way and achieve better overall performance.\n\nIntroduction\n\nThe objective of multi-task learning (MTL) [3,25] is to develop methods that can tackle a large variety of tasks within a single model. MTL has multiple practical benefits. First learning shared layers across multiple tasks can learn more representations that can be more data-efficient to train and also generalize better to unseen data and thus improve the overall performance. Second sharing parameters and computations across tasks can be significantly more efficient than running multiple individual models, which is especially important in mobile platforms with limited computational resources. Thus there is a growing interest in developing MTL methods and MTL has been successfully applied to various machine learning problems including natural language processing [5], computer vision [2,13] and speech recognition [26].\n\nThere are at least two challenges to achieve better performance and efficiency with MTL. The first one is to design a multi-task deep neural network architecture that shares only the relevant layers across the tasks and keeps the remaining ones task-specific which is in contrast to the standard approach that shares all the layers except the last few ones across all the tasks. This heuristic may be suboptimal when the tasks have different characteristics and goals (e.g. semantically low and high-level tasks), however, searching for an optimal architecture in an exponential arXiv:2007.06889v1 [cs.CV] 14 Jul 2020 Low error area of task 1 Low error area of task 2 STL for task 1 STL for task 2 MTL Ours Fig. 1. Illustration of the multi-task learning for an example two task scenarios. STL and MTL denote single-task and multi-task learning respectively. The colored regions indicate solution spaces with low generalization error for single task networks. Our hypothesis is that a low generalization error solution can be found in the intersection of two single-task models. Best seen in color.\n\nconfiguration space is extremely expensive. The second is to develop MTL training algorithms that achieve good performance not only in one of the tasks but in all of them. This problem is especially important when MTL involves minimizing a set of loss functions for various problems with different difficulty levels, magnitudes, and characteristics (e.g. cross-entropy, Euclidean loss). Thus a naive strategy of uniformly weighing multiple losses can lead to sub-optimal performances and searching for optimal weights can be prohibitively expensive.\n\nConcerned with the second problem, previous work [4,27,12,10,15] addresses the unbalanced loss optimization problem with balanced loss weighting and parameter updating strategies. Kendall et al. [12] weigh loss functions by considering the task-dependent uncertainty of the model at training time. Sener et al. [27] pose the MTL as a multiple objective optimization problem and propose an approximate Pareto optimization method that uses Frank-Wolfe algorithm to solve the constrained optimization. Yu et al. [31] project the gradients for each loss function to a space where conflicting gradient components are removed to eliminate the disturbance between the tasks. Although the previous work improves over the uniform weighing loss strategy in MTL, they still suffer from the problem of one task dominating the remaining ones and lower task performance than the single task models in standard multi-task benchmarks.\n\nIn this paper, we approach the unbalanced MTL problem from a different angle and propose a knowledge distillation based method inspired from [24,11]. As weighing the individual loss functions (e.g. [12]) or modifying the gradients for the loss functions by simple transformations (e.g. [4,31]) provide a limited control on the learned parameters and are thus limited to prevent one task dominating the rest, we propose a more strict control on the parameters of the multi-task network. Given that single-task networks often perform well with sufficient training data, we hypothesize that the solution of the multi-task network should be close to the single task ones' and lie in the intersection of the single-task solutions (illustrated in fig. 1). To this end, we first train a task-specific model for each task in an offline stage and freeze their parameters; then optimize the parameters of the multi-task network for task-specific losses and also for producing the same features with the single-task networks. As each task-specific network computes different features, we introduce small task-specific adaptors that map multi-task features to the task-specific one's. The adaptors align the features of the single-task and multi-task, and enables a balanced parameter sharing across multiple tasks.\n\nIn the remainder of this paper, we first discuss how our method relates the previous MTL and data distillation methods insection 2, formulate our method in section 3, demonstrate that our method outperforms the state-of-the-art MTL methods in two standard benchmarks in section 4 and conclude the paper with future remarks in section 5.\n\n\nRelated work\n\n\nMulti-task Learning\n\nMulti-task learning (MTL) is one of the long-standing problems in machine learning and has been used broadly [3,22,25,7,17,17,32,12,18,13]. In computer vision, MTL has been used for image classification [22], facial landmark regression [32], segmentation and depth estimation [12] and so on. In this work, we specifically focus on tackling the unbalance in the optimization of multi-task networks to achieve good performance not only in a few tasks but in all tasks.\n\nIn recent years, several methods have been proposed for solving the imbalance problem in MTL by either designing loss weighting schemes [4,12,27,15,10] to weigh each task-specific loss or modifying parameter updates [31]. Chen et al. [4] develop a training strategy, namely GradNorm, that looks at the gradient's norm of each task and learns the weight to normalize each task's gradient so as to balance the losses for MTL. In [27], Sener et al. formulate the MTL as a multiple objectives optimization problem and proposed an approximation Pareto optimization method using Frank-Wolfe algorithm to learn weights of losses. Kendall et al. [12] propose to weigh multiple loss functions by considering the homoscedastic uncertainty of each task during training. To design the weighting scheme, Guo et al. [10] observe that the imbalances in task difficulty can lead to an unnecessary emphasis on easier tasks, thus neglecting and slowing progress on difficult tasks. Based on the observation, they introduce dynamic task prioritization for MTL, which allows the model to dynamically prioritize difficult tasks during training, where the difficulty is inversely proportional to performance. Rather than weighing the losses, Yu et al. [31] propose a form of gradient \"surgery\" that projects each task's gradient onto the normal plane of the gradient of any other task and modifies the gradients for each task so as to minimize negative conflict with other task gradients during the MTL optimization.\n\nUnlike existing methods, we propose a knowledge distillation based MTL method to solve the unbalanced loss optimization problem from a different angle. To this end, we first train a task-specific model for each task in an offline stage and freeze their parameters. We then train the MTL network for minimizing task-specific loss and also for producing the same features with the task-specific networks. As the task-specific network encodes different features, we introduce small task-specific adaptors to project multi-task features to the task-specific features. In this way, the adaptors align the task-specific feature and the multi-task feature, which enables a balanced parameter sharing across tasks.\n\n\nKnowledge Distillation\n\nOur work is also related to knowledge distillation [11,24,30,16,21]. Hinton et al. [11] show that distilling the knowledge of the whole ensemble of models to a neural network can achieve better performance and avoid an expensive computation. Romero et al. [24] introduce the knowledge distillation to training a small student network to achieve better performance than the teacher network. Apart from the success in single-task learning, knowledge distillation has also been shown to be effective in MTL. Parisotto et al. [20] exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns to perform in multiple tasks by using the guidance of several expert teachers. In the contrast, in [5], Clark et al. extends the Born-Again network [9] to MTL setting for NLP. More specifically, they apply the knowledge distillation loss proposed in [11] on each task's predictions and propose a weight annealing strategy to update the weight of the distillation losses and multiple tasks losses.\n\nDifferent from these methods, we aim at solving the unbalanced loss optimization problem in MTL. Aligning the predictions from the multi-task network and the task-specific networks would still result in unbalance as the dimension of tasks' predictions is usually different and we need to use different loss functions for matching different tasks' predictions [5], e.g., a kl-divergence loss for classification and l2-norm loss for regression. In this work, we first introduce a task-specific adaptor for each task to transform features from the multi-task network and we apply the same loss function to align the transformed multi-task feature and the task-specific networks' features. We train the MTL network for minimizing task-specific loss and for producing the same feature with task-specific networks. This enables the MTL to share the parameters in a balanced way.\n\n\nMethodology\n\n\nSingle-task Learning (STL)\n\nConsider that we are given a dataset D that contains N training images x i and their labels y i 1 , . . . , y i T for T tasks (e.g. semantic segmentation, depth estimation, surface normals). In case of the STL, we wish to learn T convolutional neural networks, one for each task, each maps the input x to the target label y \u03c4 , i.e. f (x; \u03b8 s \u03c4 , \u03d1 s \u03c4 ) = y \u03c4 where the superscript s indicates the single-task, \u03b8 s \u03c4 and \u03d1 s \u03c4 are the parameters of the network. Each single-task network is composed of two parts: i) a feature encoder \u03c6(\u00b7, \u03b8 s \u03c4 ) that takes in an image and outputs a high-dimensional encoding \u03c6(x, \u03b8 s \u03c4 ) \u2208 R C\u00d7H\u00d7W where C, H, W are the number channels, height and width of the feature map; ii) a predictor \u03c8(\u00b7, \u03d1 s \u03c4 ) that takes in the encoding \u03c6(x, \u03b8 s \u03c4 ) and predicts the output for the task\n\u2113 ! (#) \u2113 ! (#) !\"#$% (#) &\"' (#) shared parameters feature Segmentation Segmentation Depth Depth \u2113 (\"' (#) \u2113 )\"#$% (#) (a) Task-specific model for segmentation (c) Task-specific model for depth (b) Multi-task model &\"' & &\"' & &\"' * !\"#$% * * !\"#$% & !\"#$% &\u03c4 , i.e.\u0177 \u03c4 = \u03c8(\u00b7, \u03d1 s \u03c4 ) \u2022 \u03c6(x, \u03b8 s \u03c4 )\nwhere \u03b8 s \u03c4 and \u03d1 s \u03c4 denote the parameters of the feature encoder and predictor respectively. The parameters for the network can be learned for each task independently by optimizing a task-specific loss function \u03c4 (\u0177, y) (e.g. Cross-Entropy loss function for classification) over the training samples that measure the mismatch between the ground-truth label and prediction as following:\nmin \u03b8 s \u03c4 ,\u03d1 s \u03c4 = x,y\u03c4 \u2208D \u03c4 (\u03c8(\u00b7, \u03d1 s \u03c4 ) \u2022 \u03c6(x, \u03b8 s \u03c4 ), y \u03c4 ).(1)\n\nMulti-task Learning (MTL)\n\nIn the case of MTL, we would like to learn one network that shares the majority of its parameters across the tasks and solves all the tasks simultaneously. Similar to STL, the multi-task network can be decomposed into two parts: i) a feature encoder \u03c6(\u00b7, \u03b8 m ) that encodes the input image into a high-dimensional encoding, now its parameters \u03b8 m are shared across all the tasks; ii) a task-specific predictor \u03c8(\u00b7, \u03d1 m \u03c4 ) for each task that takes in the shared encoding \u03c6(x, \u03b8 m ) and outputs its prediction for task \u03c4 , i.e. \u03c8(\u00b7, \u03d1 m \u03c4 ) \u2022 \u03c6(x, \u03b8 m ). Note that we use superscript m to denote MTL. The multi-task network can be learned by optimizing a linear combination of task-specific losses:\nmin \u03b8 m ,\u03d1 m 1 ,...,\u03d1 m T T \u03c4 =1 x,y\u03c4 \u2208D w \u03c4 \u03c4 (\u03c8(\u00b7; \u03d1 m \u03c4 ) \u2022 \u03c6(x; \u03b8 m ), y \u03c4 )(2)\nwhere w \u03c4 is a scaling hyperparameter for task \u03c4 that is used for balancing the loss functions among the tasks. In contrast to the STL optimization in eq. (1), optimizing eq. (2) involves a joint learning of all the task-specific and shared parameters which is typically more challenging when the task-specific loss functions \u03c4 have different characteristics such as their magnitude and dynamics (e.g. logarithmic, quadratic). One solution to balance the loss terms is to search for the best scaling hyperparameters w \u03c4 by a cross-validation which has two shortcomings. First, the hyperparameter search in a continuous space is computationally expensive, especially when the number of tasks is large, as each validation step requires the training of the model. Second, even when the optimal hyperparameters can be be found, it may be sub-optimal to use the same fixed ones throughout the optimization.\n\n\nKnowledge Distillation for Multi-task Learning\n\nMotivated by these challenges, the previous work [4,12,27] propose dynamic weighing strategies that can adjust them at each training iteration. Here we argue that these hyperparameters provide a limited control on the parameters of the network for preventing the unbalanced MTL and thus we propose a different view on this problem inspired by the knowledge distillation methods [24,11].\n\nTo this end, we first train a task-specific model f (\u00b7; \u03b8 s \u03c4 , \u03d1 s \u03c4 ) for each task \u03c4 by optimizing eq. (1) in an offline stage, freeze their parameters and use only their feature encoders \u03c6(\u00b7, \u03b8 s \u03c4 ) to regulate the multi-task network at train time by minimizing the distance between the features of task-specific networks and multi-task network for given training samples (see fig. 2). As the outputs of the task-specific encoders can differ significantly and the feature encoder of the multi-task network cannot match all of them simultaneously. Instead, we project the output of the multi-task feature encoder into each task-specific one via a task-specific adaptor A \u03c4 : R C\u00d7H\u00d7W \u2192 R C\u00d7H\u00d7W where H, W and C are the height, width and depth (number of channels) of the features. In our experiments, we use a linear layer that consists of a 1 \u00d7 1 \u00d7 C \u00d7 C convolution for each adaptor. These adaptors are jointly learned along the parameters of the multi-task network to align its features with the single-task feature encoders.\nL d = T \u03c4 =1 x,y\u03c4 \u2208D d (A \u03c4 (\u03c6(x; \u03b8 m )), \u03c6(x; \u03b8 s \u03c4 ))(3)\nwhere d is the Euclidean distance function between the L2 normalized feature maps:\nd (a, b) = a ||a|| 2 \u2212 b ||b|| 2 2 2 .(4)\nNow we can write the optimization formulation that is employed to learn the multi-task model as a linear combination of eq. (2) and eq. (3):\nmin \u03b8 m ,\u03d1 m 1 ,...,\u03d1 m T T \u03c4 =1 x,y\u03c4 \u2208D w \u03c4 \u03c4 (\u03c8(\u00b7; \u03d1 m \u03c4 )\u2022\u03c6(x; \u03b8 m ), y \u03c4 )+\u03bb \u03c4 d (A \u03c4 (\u03c6(x; \u03b8 m )), \u03c6(x; \u03b8 s \u03c4 ))(5)\nwhere \u03bb \u03c4 is the task-specific tradeoff hyperparameter. Discussion. Alternatively, the inverse of each adaptor function can be thought as a mapping from each task-specific representation to a shared representation across all the tasks. The assumption here is that a large portion of encodings in the task-specific models is common to all the models up to a simple linear transformation. While the assumption of linear relations between the features of highly non-linear networks may be surprising, such linear relations have also been observed in multi-domain [23] and multi-task problems [29].\n\n\nExperiments\n\n\nDatasets\n\nWe evaluate our method on three multi-task computer vision benchmarks, including SVHN & Omniglot, NYU-v2, and Cityscapes. 1 SVHN & Omniglot consists two datasets, i.e. SVHN [19] and Omniglot [14] where SVHN is a dataset for digital number classification and Omniglot is the one for characters classification. Specifically, SVHN contains 47,217 training images and 26,040 validation images of 10 classes. Omniglot consists of 19,476 training and 6492 validation samples of 1623 categories. As the testing labels are not provided, we evaluate all methods on the validation images and report the testing accuracy of both tasks. Note that in contrast to the NYU-V2 and Cityscapes datasets where each image is associated with multiple labels, each image in this benchmark is labeled only for one task. Thus the goal is to learn a multi-task network that can learn both tasks from SVHN and Omniglot. NYU-V2 [28] contains RGB-D indoor scene images, where we evaluate performances on 3 tasks, including 13-class semantic segmentation, depth estimation, and surface normals estimation. We use the true depth data recorded by the Microsoft Kinect and surface normals provided in [8] for depth estimation and surface normal estimation. All images are resized to 288 \u00d7 384 resolution as [15]. Cityscapes [6] consists of street-view images, each labeled for two tasks: 7-class semantic segmentation 2 and depth estimation. We resize the images to 128 \u00d7 256 to speed up the training.\n\n\nBaselines\n\nIn this work, we use the hard parameters sharing architecture for all methods where the early layers of the network are shared across all tasks and the last layers are task-specific (See fig. 2). We compare our method with two baselines: -STL learns a task-specific model for each task. We also compare our method to the state-of-the-art MTL methods which are proposed for solving the unbalanced MTL, including Uncert [12], MGDA [27], GradNorm [4] and a knowledge distillation based method, namely BAM [5], that applies knowledge distillation to network's prediction. On NYU-v2 and Cityscapes, we also compare our method with Gradient Surgery (GS) [31] and Dynamic Weight Average (DWA) [15] with using different architectures, i.e. SegNet [1] and MTAN [15] which is the extension of SegNet by introducing task-specific attention modules for each task. First, we evaluate all methods on SVHN & Omniglot. We extend the LeNet to MTL setting (See fig. 3) and use the extended network for all methods. We set the batch size of the mini-batch as 512 where 256 samples from SVHN and 256 images from Omniglot. We use Adam for optimizing the networks and adaptors. The learning rate of all task-specific adaptors is 0.01. We train all methods for 300 epochs in total where we scaled the learning rate by 0.85 every 15 epochs. As a validation set for hyperparameter search (\u03bb), we randomly pick 10% of training data. After the best hyperparameters are chosen, we retrain with the full training set and report the median validation accuracy of the last 20 epochs in Table 1. We search over the set \u03bb = {1, 5, 10, 20} of \u03bb and we chose \u03bb = 10.  Table 1 shows that MTL with uniform loss weights (Uniform) obtains worse results on SVHN while it achieves better performance on Omniglot than STL. The state-of-the-art methods which dynamically weigh the task-specific losses cannot achieve a good trade-off between these two tasks. More specifically, Uncert and GradNorm obtain worse overall performance than STL while MGDA improves the performance on Omniglot and obtains worse performance on SVHN. Though BAM obtains better overall performance on both tasks, the improvement is achieved mainly because of more informative information provided by the continuous predictions of the teacher network (task-specific models). The unbalanced problem in MTL when we use BAM is still unsolved as BAM applies knowledge distillation on network predictions, which would have similar problems with the vanilla MTL (Uniform). In contrast, our approach achieves significantly better performance than any other MTL methods i.e. our method obtains 88.05 % accuracy on SVHN and 70.12 % accuracy on Omniglot. Compared with STL, our method obtains comparable results on SVHN and significant gains on Omniglot. These results strongly verify that our method is able to alleviate the unbalanced problem in this benchmark and to outperform STL as it enables the MTL model to learn more informative features.\n\n\nComparison to the State-of-the-art\n\n\nResults on SVHN & Omniglot.\n\nResults on NYU-V2. We follow the training and evaluation protocol in [15]. We use cross-entropy loss for semantic segmentation, l1-norm loss for depth estimation, and cosine similarity loss for surface normal estimation. We train all methods using Adam with the learning rate initialized at 1e-4 and halved at the 100-th epoch for 200 epochs in total. The learning rate of all task-specific adaptors is 0.1 and the batch size is 2. As a validation set for hyperparameter search (\u03bb), we randomly pick 10% of training data. After the best hyperparameters are chosen, we retrain with the full training set and report the results of three tasks on the validation set in table 2. We search over the set \u03bb = {1, 2, 3, 4, 5, 6} of \u03bb and the distillation loss' weight of Segmentation, depth, and surface normal are set to 1, 1 and 2, respectively. From the results shown in table 2, we can see that it is possible to tackle multiple tasks within a network and achieve performance improvement on some tasks, e.g. when we use SegNet as the based network, the vanilla MTL (Uniform) achieves better performance on semantic segmentation and depth estimation though it causes a drop on surface normal estimation in comparison with STL. Though we see the benefits of using MTL, it is also clear that the unbalanced problem exists. We then apply existing methods that introduce loss weighting strategies for addressing the unbalanced loss optimization problem. From the results of using SegNet, GradNorm performs the best among all compared methods. However, it provides limited control on the learned parameters (e.g. it achieves similar results to the MTL model using a uniformly weighting scheme) and still suffers from lower task performance than the single task models.\n\nIn comparison with these methods, our method obtains significant gains over all tasks and achieves better results than single task learning models. This strongly verifies our hypothesis that the solution of the multi-task network should be close to the single task ones' and lie in the intersection of the single-task solutions (illustrated in fig. 1) and our method that applies stricter control on the parameters of the multi-task network can better address the unbalanced loss problem.\n\nResults on Cityscapes. Similar to NYU-V2, we use cross-entropy loss for semantic segmentation and l1-norm loss for depth estimation in Cityscapes as in [15]. We train all methods using Adam with the learning rate initialized at 1e-4 and halved at the 100-th epoch for 200 epochs in total. The learning rate of all task-specific adaptors is 0.1 and the batch size is 8. As a validation set for hyperparameter search (\u03bb), we randomly pick 10% of training data. After the best hyperparameters are chosen, we retrain with the full training set and report the results of two tasks on the validation set in table 3. We search over the set \u03bb = {1, 2, 3, 4, 5, 6} of \u03bb and the distillation loss' weight of Segmentation and depth are set to 2 and 6, respectively. As shown in table 3, in overall, MTL obtains worse performance than STL. It is clear that GradNorm obtains worse performance on semantic segmentation while it improves the performance on depth estimation. However, MGDA assigns much larger weight on depth estimation task and this enables the MTL model to achieve better performance on both tasks. Our method also achieves significant gains on both tasks. The results again demonstrate that our method is able to optimize MTL model in a more balanced way and to achieve better overall results.\n\n\nAblation Study\n\nTo better analyze the effect of the distillation loss, we conduct an ablation study on NYU-V2. We first evaluate the effect of applying distillation loss to more layer's features. On NYU-V2, we report the results of applying distillation loss to the last shared layer's feature only and the results of applying distillation loss to features of both the middle layer and the last layer. From the results presented in table 4, adding more layers' features for computing distillation loss boosts the performance on NYU-V2 in general. However, results on SVHN & Omniglot and Cityscapes indicate that using the last layers obtains the best performance. We argue that adding more layers can enhance the distillation loss and a more strict control on the parameters of the multi-task network. This is not necessary for those tasks that use a small network, e.g. the network we used in SVHN & Omniglot and would be useful for tasks using a large network, e.g. the SegNet used in NYU-V2.  Further Analysis. We also plot the task-specific loss and distillation loss on the training set of two benchmarks for analyzing our method. In both fig. 4 and fig. 5, it is clear that distillation loss is more balanced than task-specific loss. More specifically, the task-specific loss of SVHN and Omniglot converge at around 0.137 and 1.492, respectively. In contrast, the distillation loss of SVHN and Omniglot converge at around 0.089 and 0.041, respectively. On NYU-V2, the task-specific loss of semantic segmentation, depth estimation, and surface normal estimation end up at 0.027, 0.127, and 0.039 while the distillation loss ends up at around 0.534, 0.381, and 0.364. These results again verify that our method can optimize the MTL method in a more balanced way.\n\n\nConclusion\n\nIn this work, we proposed a knowledge distillation based multi-task method that learns produce the same features with the single-task networks to address the unbalanced multi-task learning problem with the hypothesis that the solution of the multi-task network should be close to the single task ones' and lie in the intersection of the single solutions. We demonstrated that our method achieves significant performance gains over the state-of-the-art methods, on challenging benchmarks for image classification and scene understanding (semantic segmentation, depth estimation, and surface normal estimation). As future work, we plan to extend our method to multi-task network architecture searching.\n\nFig. 2 .\n2Diagram of our proposed method. Best seen in color.\n\n\n-Uniform: This vanilla MTL model is trained by minimizing the uniformly weighted loss eq. (2).\n\nFig. 3 .\n3Network architecture used in SVHN & Omniglot.\n\nFig. 4 .Fig. 5 .\n45Task-specific loss and distillation loss on training set of SVHN & Omniglot. Best view in color. Task-specific loss and distillation loss on training set of NYU-V2. Best view in color.\n\n\nArchitecture Type MethodsSegmentationDepth (Higher Better \u2191) (Lower Better \u2193) mIoU Pix Acc Abs Err Rel ErrTable 3. Testing results on Cityscapes.SegNet \n\nSTL \n--\n51.85 \n91.08 \n0.0136 22.68 \n\nMTL \n\nUniform 50.73 \n90.76 \n0.0151 40.81 \nUncert 51.09 \n90.85 \n0.0143 27.66 \nMGDA 51.69 \n90.99 \n0.0130 24.04 \nGradNorm 50.06 \n90.82 \n0.0143 28.61 \nOurs \n52.18 91.24 \n0.0140 28.90 \n\nMTAN \n\nSTL \n--\n51.24 \n91.16 \n0.0137 24.80 \n\nMTL \n\nUniform 52.56 \n91.33 \n0.0152 24.64 \nDWA \n51.95 \n91.33 \n0.0141 30.03 \nUncert 50.37 \n91.11 \n0.0142 31.78 \nMGDA 52.32 \n91.59 \n0.0138 30.35 \nGradNorm 51.88 \n91.40 \n0.0148 31.43 \nOurs \n52.71 \n91.54 \n0.0139 27.33 \n\n\n\nTable 4 .\n4Lower Better \u2193) (Higher Better \u2191) Backbone #layers mIoU Pix Acc Abs Err Rel Err Mean Median 11.25 22.5 30 Ablation study on NYU-v2. Here, '#layer' means which layers are selected for computing distillation loss.Segmentation \nDepth \nSurface Normal \nMethod \n(Higher Better \u2191) (Lower Better \u2193) \nAngle Distance \nWithin t \u2022 \n(SegNet \nlast \n18.66 \n57.78 \n0.5813 0.2375 30.17 \n24.74 \n22.96 46.44 58.76 \nmid + last 18.75 58.02 \n0.5780 0.2467 29.40 23.71 24.33 48.22 60.45 \n\nMTAN [15] \nlast \n18.52 \n56.81 \n0.5756 0.2489 31.13 \n24.93 \n23.52 46.29 58.18 \nmid + last 20.75 57.90 \n0.5816 0.2445 29.97 23.96 24.24 47.78 59.78 \n\n\nThe implementation of our method will be available at https://weihonglee.github.io/Projects/KD-MTL/KD-MTL.htm.2 The original version of Cityscapes provides labels 19-class semantic segmentation. We follow the evaluation protocol in[15], we use labels of 7-class semantic segmentation. Please refer to[15] for more details.\n\nSegnet: A deep convolutional encoderdecoder architecture for image segmentation. V Badrinarayanan, A Kendall, R Cipolla, Transactions on Pattern Analysis and Machine Intelligence. 3912Badrinarayanan, V., Kendall, A., Cipolla, R.: Segnet: A deep convolutional encoder- decoder architecture for image segmentation. Transactions on Pattern Analysis and Machine Intelligence 39(12), 2481-2495 (2017)\n\nIntegrated perception with recurrent multi-task neural networks. H Bilen, A Vedaldi, Advances in Neural Information Processing Systems. Bilen, H., Vedaldi, A.: Integrated perception with recurrent multi-task neural networks. In: Advances in Neural Information Processing Systems. pp. 235-243 (2016)\n\nMultitask learning. R Caruana, Machine learning. 281Caruana, R.: Multitask learning. Machine learning 28(1), 41-75 (1997)\n\nGradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. Z Chen, V Badrinarayanan, C Y Lee, A Rabinovich, arXiv:1711.02257arXiv preprintChen, Z., Badrinarayanan, V., Lee, C.Y., Rabinovich, A.: Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. arXiv preprint arXiv:1711.02257 (2017)\n\nK Clark, M T Luong, U Khandelwal, C D Manning, Q V Le, arXiv:1907.04829Bam! bornagain multi-task networks for natural language understanding. arXiv preprintClark, K., Luong, M.T., Khandelwal, U., Manning, C.D., Le, Q.V.: Bam! born- again multi-task networks for natural language understanding. arXiv preprint arXiv:1907.04829 (2019)\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Computer Vision and Pattern Recognition. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: Computer Vision and Pattern Recognition. pp. 3213-3223 (2016)\n\nAdapting auxiliary losses using gradient similarity. Y Du, W M Czarnecki, S M Jayakumar, R Pascanu, B Lakshminarayanan, arXiv:1812.02224arXiv preprintDu, Y., Czarnecki, W.M., Jayakumar, S.M., Pascanu, R., Lakshminarayanan, B.: Adapting auxiliary losses using gradient similarity. arXiv preprint arXiv:1812.02224 (2018)\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus, IEEE International Conference on Computer Vision. Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In: IEEE International Conference on Computer Vision. pp. 2650-2658 (2015)\n\nT Furlanello, Z C Lipton, M Tschannen, L Itti, A Anandkumar, arXiv:1805.04770Born again neural networks. arXiv preprintFurlanello, T., Lipton, Z.C., Tschannen, M., Itti, L., Anandkumar, A.: Born again neural networks. arXiv preprint arXiv:1805.04770 (2018)\n\nDynamic task prioritization for multitask learning. M Guo, A Haque, D A Huang, S Yeung, L Fei-Fei, European Conference on Computer Vision. Guo, M., Haque, A., Huang, D.A., Yeung, S., Fei-Fei, L.: Dynamic task prioritization for multitask learning. In: European Conference on Computer Vision. pp. 270-287 (2018)\n\nG Hinton, O Vinyals, J Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintHinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015)\n\nMulti-task learning using uncertainty to weigh losses for scene geometry and semantics. A Kendall, Y Gal, R Cipolla, IEEE Conference on Computer Vision and Pattern Recognition. Kendall, A., Gal, Y., Cipolla, R.: Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 7482-7491 (2018)\n\nUbernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. I Kokkinos, IEEE Conference on Computer Vision and Pattern Recognition. Kokkinos, I.: Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 6129-6138 (2017)\n\nHuman-level concept learning through probabilistic program induction. B M Lake, R Salakhutdinov, J B Tenenbaum, Science. 3506266Lake, B.M., Salakhutdinov, R., Tenenbaum, J.B.: Human-level concept learning through probabilistic program induction. Science 350(6266), 1332-1338 (2015)\n\nEnd-to-end multi-task learning with attention. S Liu, E Johns, A J Davison, IEEE Conference on Computer Vision and Pattern Recognition. Liu, S., Johns, E., Davison, A.J.: End-to-end multi-task learning with attention. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 1871-1880 (2019)\n\nGraph representation learning via multi-task knowledge distillation. J Ma, Q Mei, arXiv:1911.05700arXiv preprintMa, J., Mei, Q.: Graph representation learning via multi-task knowledge distillation. arXiv preprint arXiv:1911.05700 (2019)\n\nPseudo-task augmentation: From deep multitask learning to intratask sharing-and back. E Meyerson, R Miikkulainen, arXiv:1803.04062arXiv preprintMeyerson, E., Miikkulainen, R.: Pseudo-task augmentation: From deep multitask learning to intratask sharing-and back. arXiv preprint arXiv:1803.04062 (2018)\n\nCross-stitch networks for multitask learning. I Misra, A Shrivastava, A Gupta, M Hebert, IEEE Conference on Computer Vision and Pattern Recognition. Misra, I., Shrivastava, A., Gupta, A., Hebert, M.: Cross-stitch networks for multi- task learning. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 3994-4003 (2016)\n\nReading digits in natural images with unsupervised feature learning. Y Netzer, T Wang, A Coates, A Bissacco, B Wu, A Y Ng, Advances in Neural Information Processing Systems Workshop on Deep Learning and Unsupervised Feature Learning. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.Y.: Reading digits in natural images with unsupervised feature learning. In: Advances in Neural Information Processing Systems Workshop on Deep Learning and Unsupervised Feature Learning (2011)\n\nActor-mimic: Deep multitask and transfer reinforcement learning. E Parisotto, J L Ba, R Salakhutdinov, arXiv:1511.06342arXiv preprintParisotto, E., Ba, J.L., Salakhutdinov, R.: Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342 (2015)\n\nTowards understanding knowledge distillation. M Phuong, C Lampert, International Conference on Machine Learning. Phuong, M., Lampert, C.: Towards understanding knowledge distillation. In: Inter- national Conference on Machine Learning. pp. 5142-5151 (2019)\n\nLearning multiple visual domains with residual adapters. S A Rebuffi, H Bilen, A Vedaldi, Advances in Neural Information Processing Systems. Rebuffi, S.A., Bilen, H., Vedaldi, A.: Learning multiple visual domains with residual adapters. In: Advances in Neural Information Processing Systems. pp. 506-516 (2017)\n\nEfficient parametrization of multi-domain deep neural networks. S A Rebuffi, H Bilen, A Vedaldi, IEEE Conference on Computer Vision and Pattern Recognition. Rebuffi, S.A., Bilen, H., Vedaldi, A.: Efficient parametrization of multi-domain deep neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 8119-8127 (2018)\n\nA Romero, N Ballas, S E Kahou, A Chassang, C Gatta, Y Bengio, arXiv:1412.6550Fitnets: Hints for thin deep nets. arXiv preprintRomero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 (2014)\n\nS Ruder, arXiv:1706.05098An overview of multi-task learning in deep neural networks. arXiv preprintRuder, S.: An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098 (2017)\n\nMulti-task learning in deep neural networks for improved phoneme recognition. M L Seltzer, J Droppo, IEEE International Conference on Acoustics, Speech and Signal Processing. IEEESeltzer, M.L., Droppo, J.: Multi-task learning in deep neural networks for improved phoneme recognition. In: IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6965-6969. IEEE (2013)\n\nMulti-task learning as multi-objective optimization. O Sener, V Koltun, Advances in Neural Information Processing Systems. Sener, O., Koltun, V.: Multi-task learning as multi-objective optimization. In: Advances in Neural Information Processing Systems. pp. 527-538 (2018)\n\nIndoor segmentation and support inference from rgbd images. N Silberman, D Hoiem, P Kohli, R Fergus, European conference on computer vision. SpringerSilberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor segmentation and support inference from rgbd images. In: European conference on computer vision. pp. 746-760. Springer (2012)\n\nBert and pals: Projected attention layers for efficient adaptation in multi-task learning. A C Stickland, I Murray, arXiv:1902.02671arXiv preprintStickland, A.C., Murray, I.: Bert and pals: Projected attention layers for efficient adaptation in multi-task learning. arXiv preprint arXiv:1902.02671 (2019)\n\nY Tian, D Krishnan, P Isola, arXiv:1910.10699Contrastive representation distillation. arXiv preprintTian, Y., Krishnan, D., Isola, P.: Contrastive representation distillation. arXiv preprint arXiv:1910.10699 (2019)\n\nT Yu, S Kumar, A Gupta, S Levine, K Hausman, C Finn, arXiv:2001.06782Gradient surgery for multi-task learning. arXiv preprintYu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., Finn, C.: Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782 (2020)\n\nFacial landmark detection by deep multitask learning. Z Zhang, P Luo, C C Loy, X Tang, European conference on computer vision. SpringerZhang, Z., Luo, P., Loy, C.C., Tang, X.: Facial landmark detection by deep multi- task learning. In: European conference on computer vision. pp. 94-108. Springer (2014)\n", "annotations": {"author": "[{\"end\":113,\"start\":50},{\"end\":193,\"start\":114}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":59},{\"end\":125,\"start\":120}]", "author_first_name": "[{\"end\":58,\"start\":50},{\"end\":119,\"start\":114}]", "author_affiliation": "[{\"end\":112,\"start\":63},{\"end\":192,\"start\":143}]", "title": "[{\"end\":47,\"start\":1},{\"end\":240,\"start\":194}]", "venue": null, "abstract": "[{\"end\":1383,\"start\":242}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1445,\"start\":1442},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1448,\"start\":1445},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2175,\"start\":2172},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2196,\"start\":2193},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2199,\"start\":2196},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2227,\"start\":2223},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3933,\"start\":3930},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3936,\"start\":3933},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3939,\"start\":3936},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3942,\"start\":3939},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3945,\"start\":3942},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4080,\"start\":4076},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4196,\"start\":4192},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4394,\"start\":4390},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4946,\"start\":4942},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4949,\"start\":4946},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5003,\"start\":4999},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5090,\"start\":5087},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5093,\"start\":5090},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6593,\"start\":6590},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6596,\"start\":6593},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6599,\"start\":6596},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6601,\"start\":6599},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6604,\"start\":6601},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6607,\"start\":6604},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6610,\"start\":6607},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6613,\"start\":6610},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6616,\"start\":6613},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6619,\"start\":6616},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6688,\"start\":6684},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6721,\"start\":6717},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6761,\"start\":6757},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7088,\"start\":7085},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7091,\"start\":7088},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7094,\"start\":7091},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7097,\"start\":7094},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7100,\"start\":7097},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7169,\"start\":7165},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7186,\"start\":7183},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7380,\"start\":7376},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7591,\"start\":7587},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7755,\"start\":7751},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8183,\"start\":8179},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9233,\"start\":9229},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9236,\"start\":9233},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9239,\"start\":9236},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9242,\"start\":9239},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9245,\"start\":9242},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9265,\"start\":9261},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9438,\"start\":9434},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9704,\"start\":9700},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9933,\"start\":9930},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9982,\"start\":9979},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10085,\"start\":10081},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10591,\"start\":10588},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14534,\"start\":14531},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14537,\"start\":14534},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14540,\"start\":14537},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14864,\"start\":14860},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14867,\"start\":14864},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16912,\"start\":16908},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16941,\"start\":16937},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17146,\"start\":17142},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17164,\"start\":17160},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17874,\"start\":17870},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18141,\"start\":18138},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18248,\"start\":18244},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18264,\"start\":18261},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18874,\"start\":18870},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18885,\"start\":18881},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18899,\"start\":18896},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18957,\"start\":18954},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19104,\"start\":19100},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19142,\"start\":19138},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19194,\"start\":19191},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19208,\"start\":19204},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21563,\"start\":21559},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23896,\"start\":23892},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29316,\"start\":29315},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29440,\"start\":29436},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29509,\"start\":29505}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27584,\"start\":27522},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27681,\"start\":27585},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27738,\"start\":27682},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27943,\"start\":27739},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28577,\"start\":27944},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29204,\"start\":28578}]", "paragraph": "[{\"end\":2228,\"start\":1399},{\"end\":3328,\"start\":2230},{\"end\":3879,\"start\":3330},{\"end\":4799,\"start\":3881},{\"end\":6104,\"start\":4801},{\"end\":6442,\"start\":6106},{\"end\":6947,\"start\":6481},{\"end\":8443,\"start\":6949},{\"end\":9151,\"start\":8445},{\"end\":10227,\"start\":9178},{\"end\":11101,\"start\":10229},{\"end\":11961,\"start\":11146},{\"end\":12650,\"start\":12263},{\"end\":13445,\"start\":12748},{\"end\":14431,\"start\":13530},{\"end\":14868,\"start\":14482},{\"end\":15901,\"start\":14870},{\"end\":16043,\"start\":15961},{\"end\":16226,\"start\":16086},{\"end\":16942,\"start\":16348},{\"end\":18438,\"start\":16969},{\"end\":21421,\"start\":18452},{\"end\":23248,\"start\":21490},{\"end\":23738,\"start\":23250},{\"end\":25037,\"start\":23740},{\"end\":26806,\"start\":25056},{\"end\":27521,\"start\":26821}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12221,\"start\":11962},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12262,\"start\":12221},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12719,\"start\":12651},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13529,\"start\":13446},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15960,\"start\":15902},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16085,\"start\":16044},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16347,\"start\":16227}]", "table_ref": "[{\"end\":20014,\"start\":20007},{\"end\":20092,\"start\":20085}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1397,\"start\":1385},{\"attributes\":{\"n\":\"2\"},\"end\":6457,\"start\":6445},{\"attributes\":{\"n\":\"2.1\"},\"end\":6479,\"start\":6460},{\"attributes\":{\"n\":\"2.2\"},\"end\":9176,\"start\":9154},{\"attributes\":{\"n\":\"3\"},\"end\":11115,\"start\":11104},{\"attributes\":{\"n\":\"3.1\"},\"end\":11144,\"start\":11118},{\"attributes\":{\"n\":\"3.2\"},\"end\":12746,\"start\":12721},{\"attributes\":{\"n\":\"3.3\"},\"end\":14480,\"start\":14434},{\"attributes\":{\"n\":\"4\"},\"end\":16956,\"start\":16945},{\"attributes\":{\"n\":\"4.1\"},\"end\":16967,\"start\":16959},{\"attributes\":{\"n\":\"4.2\"},\"end\":18450,\"start\":18441},{\"attributes\":{\"n\":\"4.3\"},\"end\":21458,\"start\":21424},{\"end\":21488,\"start\":21461},{\"attributes\":{\"n\":\"4.4\"},\"end\":25054,\"start\":25040},{\"attributes\":{\"n\":\"5\"},\"end\":26819,\"start\":26809},{\"end\":27531,\"start\":27523},{\"end\":27691,\"start\":27683},{\"end\":27756,\"start\":27740},{\"end\":28588,\"start\":28579}]", "table": "[{\"end\":28577,\"start\":28091},{\"end\":29204,\"start\":28801}]", "figure_caption": "[{\"end\":27584,\"start\":27533},{\"end\":27681,\"start\":27587},{\"end\":27738,\"start\":27693},{\"end\":27943,\"start\":27759},{\"end\":28091,\"start\":27946},{\"end\":28801,\"start\":28590}]", "figure_ref": "[{\"end\":2943,\"start\":2937},{\"end\":5548,\"start\":5542},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15258,\"start\":15252},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18645,\"start\":18639},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19401,\"start\":19395},{\"end\":23600,\"start\":23594},{\"end\":26190,\"start\":26184},{\"end\":26201,\"start\":26195}]", "bib_author_first_name": "[{\"end\":29611,\"start\":29610},{\"end\":29629,\"start\":29628},{\"end\":29640,\"start\":29639},{\"end\":29992,\"start\":29991},{\"end\":30001,\"start\":30000},{\"end\":30247,\"start\":30246},{\"end\":30439,\"start\":30438},{\"end\":30447,\"start\":30446},{\"end\":30465,\"start\":30464},{\"end\":30467,\"start\":30466},{\"end\":30474,\"start\":30473},{\"end\":30704,\"start\":30703},{\"end\":30713,\"start\":30712},{\"end\":30715,\"start\":30714},{\"end\":30724,\"start\":30723},{\"end\":30738,\"start\":30737},{\"end\":30740,\"start\":30739},{\"end\":30751,\"start\":30750},{\"end\":30753,\"start\":30752},{\"end\":31101,\"start\":31100},{\"end\":31111,\"start\":31110},{\"end\":31120,\"start\":31119},{\"end\":31129,\"start\":31128},{\"end\":31140,\"start\":31139},{\"end\":31153,\"start\":31152},{\"end\":31165,\"start\":31164},{\"end\":31175,\"start\":31174},{\"end\":31183,\"start\":31182},{\"end\":31529,\"start\":31528},{\"end\":31535,\"start\":31534},{\"end\":31537,\"start\":31536},{\"end\":31550,\"start\":31549},{\"end\":31552,\"start\":31551},{\"end\":31565,\"start\":31564},{\"end\":31576,\"start\":31575},{\"end\":31904,\"start\":31903},{\"end\":31913,\"start\":31912},{\"end\":32180,\"start\":32179},{\"end\":32194,\"start\":32193},{\"end\":32196,\"start\":32195},{\"end\":32206,\"start\":32205},{\"end\":32219,\"start\":32218},{\"end\":32227,\"start\":32226},{\"end\":32490,\"start\":32489},{\"end\":32497,\"start\":32496},{\"end\":32506,\"start\":32505},{\"end\":32508,\"start\":32507},{\"end\":32517,\"start\":32516},{\"end\":32526,\"start\":32525},{\"end\":32750,\"start\":32749},{\"end\":32760,\"start\":32759},{\"end\":32771,\"start\":32770},{\"end\":33064,\"start\":33063},{\"end\":33075,\"start\":33074},{\"end\":33082,\"start\":33081},{\"end\":33502,\"start\":33501},{\"end\":33884,\"start\":33883},{\"end\":33886,\"start\":33885},{\"end\":33894,\"start\":33893},{\"end\":33911,\"start\":33910},{\"end\":33913,\"start\":33912},{\"end\":34144,\"start\":34143},{\"end\":34151,\"start\":34150},{\"end\":34160,\"start\":34159},{\"end\":34162,\"start\":34161},{\"end\":34470,\"start\":34469},{\"end\":34476,\"start\":34475},{\"end\":34725,\"start\":34724},{\"end\":34737,\"start\":34736},{\"end\":34987,\"start\":34986},{\"end\":34996,\"start\":34995},{\"end\":35011,\"start\":35010},{\"end\":35020,\"start\":35019},{\"end\":35344,\"start\":35343},{\"end\":35354,\"start\":35353},{\"end\":35362,\"start\":35361},{\"end\":35372,\"start\":35371},{\"end\":35384,\"start\":35383},{\"end\":35390,\"start\":35389},{\"end\":35392,\"start\":35391},{\"end\":35831,\"start\":35830},{\"end\":35844,\"start\":35843},{\"end\":35846,\"start\":35845},{\"end\":35852,\"start\":35851},{\"end\":36094,\"start\":36093},{\"end\":36104,\"start\":36103},{\"end\":36363,\"start\":36362},{\"end\":36365,\"start\":36364},{\"end\":36376,\"start\":36375},{\"end\":36385,\"start\":36384},{\"end\":36682,\"start\":36681},{\"end\":36684,\"start\":36683},{\"end\":36695,\"start\":36694},{\"end\":36704,\"start\":36703},{\"end\":36964,\"start\":36963},{\"end\":36974,\"start\":36973},{\"end\":36984,\"start\":36983},{\"end\":36986,\"start\":36985},{\"end\":36995,\"start\":36994},{\"end\":37007,\"start\":37006},{\"end\":37016,\"start\":37015},{\"end\":37238,\"start\":37237},{\"end\":37526,\"start\":37525},{\"end\":37528,\"start\":37527},{\"end\":37539,\"start\":37538},{\"end\":37891,\"start\":37890},{\"end\":37900,\"start\":37899},{\"end\":38172,\"start\":38171},{\"end\":38185,\"start\":38184},{\"end\":38194,\"start\":38193},{\"end\":38203,\"start\":38202},{\"end\":38535,\"start\":38534},{\"end\":38537,\"start\":38536},{\"end\":38550,\"start\":38549},{\"end\":38750,\"start\":38749},{\"end\":38758,\"start\":38757},{\"end\":38770,\"start\":38769},{\"end\":38966,\"start\":38965},{\"end\":38972,\"start\":38971},{\"end\":38981,\"start\":38980},{\"end\":38990,\"start\":38989},{\"end\":39000,\"start\":38999},{\"end\":39011,\"start\":39010},{\"end\":39292,\"start\":39291},{\"end\":39301,\"start\":39300},{\"end\":39308,\"start\":39307},{\"end\":39310,\"start\":39309},{\"end\":39317,\"start\":39316}]", "bib_author_last_name": "[{\"end\":29626,\"start\":29612},{\"end\":29637,\"start\":29630},{\"end\":29648,\"start\":29641},{\"end\":29998,\"start\":29993},{\"end\":30009,\"start\":30002},{\"end\":30255,\"start\":30248},{\"end\":30444,\"start\":30440},{\"end\":30462,\"start\":30448},{\"end\":30471,\"start\":30468},{\"end\":30485,\"start\":30475},{\"end\":30710,\"start\":30705},{\"end\":30721,\"start\":30716},{\"end\":30735,\"start\":30725},{\"end\":30748,\"start\":30741},{\"end\":30756,\"start\":30754},{\"end\":31108,\"start\":31102},{\"end\":31117,\"start\":31112},{\"end\":31126,\"start\":31121},{\"end\":31137,\"start\":31130},{\"end\":31150,\"start\":31141},{\"end\":31162,\"start\":31154},{\"end\":31172,\"start\":31166},{\"end\":31180,\"start\":31176},{\"end\":31191,\"start\":31184},{\"end\":31532,\"start\":31530},{\"end\":31547,\"start\":31538},{\"end\":31562,\"start\":31553},{\"end\":31573,\"start\":31566},{\"end\":31593,\"start\":31577},{\"end\":31910,\"start\":31905},{\"end\":31920,\"start\":31914},{\"end\":32191,\"start\":32181},{\"end\":32203,\"start\":32197},{\"end\":32216,\"start\":32207},{\"end\":32224,\"start\":32220},{\"end\":32238,\"start\":32228},{\"end\":32494,\"start\":32491},{\"end\":32503,\"start\":32498},{\"end\":32514,\"start\":32509},{\"end\":32523,\"start\":32518},{\"end\":32534,\"start\":32527},{\"end\":32757,\"start\":32751},{\"end\":32768,\"start\":32761},{\"end\":32776,\"start\":32772},{\"end\":33072,\"start\":33065},{\"end\":33079,\"start\":33076},{\"end\":33090,\"start\":33083},{\"end\":33511,\"start\":33503},{\"end\":33891,\"start\":33887},{\"end\":33908,\"start\":33895},{\"end\":33923,\"start\":33914},{\"end\":34148,\"start\":34145},{\"end\":34157,\"start\":34152},{\"end\":34170,\"start\":34163},{\"end\":34473,\"start\":34471},{\"end\":34480,\"start\":34477},{\"end\":34734,\"start\":34726},{\"end\":34750,\"start\":34738},{\"end\":34993,\"start\":34988},{\"end\":35008,\"start\":34997},{\"end\":35017,\"start\":35012},{\"end\":35027,\"start\":35021},{\"end\":35351,\"start\":35345},{\"end\":35359,\"start\":35355},{\"end\":35369,\"start\":35363},{\"end\":35381,\"start\":35373},{\"end\":35387,\"start\":35385},{\"end\":35395,\"start\":35393},{\"end\":35841,\"start\":35832},{\"end\":35849,\"start\":35847},{\"end\":35866,\"start\":35853},{\"end\":36101,\"start\":36095},{\"end\":36112,\"start\":36105},{\"end\":36373,\"start\":36366},{\"end\":36382,\"start\":36377},{\"end\":36393,\"start\":36386},{\"end\":36692,\"start\":36685},{\"end\":36701,\"start\":36696},{\"end\":36712,\"start\":36705},{\"end\":36971,\"start\":36965},{\"end\":36981,\"start\":36975},{\"end\":36992,\"start\":36987},{\"end\":37004,\"start\":36996},{\"end\":37013,\"start\":37008},{\"end\":37023,\"start\":37017},{\"end\":37244,\"start\":37239},{\"end\":37536,\"start\":37529},{\"end\":37546,\"start\":37540},{\"end\":37897,\"start\":37892},{\"end\":37907,\"start\":37901},{\"end\":38182,\"start\":38173},{\"end\":38191,\"start\":38186},{\"end\":38200,\"start\":38195},{\"end\":38210,\"start\":38204},{\"end\":38547,\"start\":38538},{\"end\":38557,\"start\":38551},{\"end\":38755,\"start\":38751},{\"end\":38767,\"start\":38759},{\"end\":38776,\"start\":38771},{\"end\":38969,\"start\":38967},{\"end\":38978,\"start\":38973},{\"end\":38987,\"start\":38982},{\"end\":38997,\"start\":38991},{\"end\":39008,\"start\":39001},{\"end\":39016,\"start\":39012},{\"end\":39298,\"start\":39293},{\"end\":39305,\"start\":39302},{\"end\":39314,\"start\":39311},{\"end\":39322,\"start\":39318}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":60814714},\"end\":29924,\"start\":29529},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9129804},\"end\":30224,\"start\":29926},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":45998148},\"end\":30347,\"start\":30226},{\"attributes\":{\"doi\":\"arXiv:1711.02257\",\"id\":\"b3\"},\"end\":30701,\"start\":30349},{\"attributes\":{\"doi\":\"arXiv:1907.04829\",\"id\":\"b4\"},\"end\":31035,\"start\":30703},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":502946},\"end\":31473,\"start\":31037},{\"attributes\":{\"doi\":\"arXiv:1812.02224\",\"id\":\"b6\"},\"end\":31793,\"start\":31475},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":102496818},\"end\":32177,\"start\":31795},{\"attributes\":{\"doi\":\"arXiv:1805.04770\",\"id\":\"b8\"},\"end\":32435,\"start\":32179},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52952193},\"end\":32747,\"start\":32437},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b10\"},\"end\":32973,\"start\":32749},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4800342},\"end\":33359,\"start\":32975},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8070108},\"end\":33811,\"start\":33361},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11790493},\"end\":34094,\"start\":33813},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4389348},\"end\":34398,\"start\":34096},{\"attributes\":{\"doi\":\"arXiv:1911.05700\",\"id\":\"b15\"},\"end\":34636,\"start\":34400},{\"attributes\":{\"doi\":\"arXiv:1803.04062\",\"id\":\"b16\"},\"end\":34938,\"start\":34638},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1923223},\"end\":35272,\"start\":34940},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16852518},\"end\":35763,\"start\":35274},{\"attributes\":{\"doi\":\"arXiv:1511.06342\",\"id\":\"b19\"},\"end\":36045,\"start\":35765},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":174800711},\"end\":36303,\"start\":36047},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":215826266},\"end\":36615,\"start\":36305},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":215822736},\"end\":36961,\"start\":36617},{\"attributes\":{\"doi\":\"arXiv:1412.6550\",\"id\":\"b23\"},\"end\":37235,\"start\":36963},{\"attributes\":{\"doi\":\"arXiv:1706.05098\",\"id\":\"b24\"},\"end\":37445,\"start\":37237},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14873729},\"end\":37835,\"start\":37447},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":52957972},\"end\":38109,\"start\":37837},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":545361},\"end\":38441,\"start\":38111},{\"attributes\":{\"doi\":\"arXiv:1902.02671\",\"id\":\"b28\"},\"end\":38747,\"start\":38443},{\"attributes\":{\"doi\":\"arXiv:1910.10699\",\"id\":\"b29\"},\"end\":38963,\"start\":38749},{\"attributes\":{\"doi\":\"arXiv:2001.06782\",\"id\":\"b30\"},\"end\":39235,\"start\":38965},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14181993},\"end\":39540,\"start\":39237}]", "bib_title": "[{\"end\":29608,\"start\":29529},{\"end\":29989,\"start\":29926},{\"end\":30244,\"start\":30226},{\"end\":31098,\"start\":31037},{\"end\":31901,\"start\":31795},{\"end\":32487,\"start\":32437},{\"end\":33061,\"start\":32975},{\"end\":33499,\"start\":33361},{\"end\":33881,\"start\":33813},{\"end\":34141,\"start\":34096},{\"end\":34984,\"start\":34940},{\"end\":35341,\"start\":35274},{\"end\":36091,\"start\":36047},{\"end\":36360,\"start\":36305},{\"end\":36679,\"start\":36617},{\"end\":37523,\"start\":37447},{\"end\":37888,\"start\":37837},{\"end\":38169,\"start\":38111},{\"end\":39289,\"start\":39237}]", "bib_author": "[{\"end\":29628,\"start\":29610},{\"end\":29639,\"start\":29628},{\"end\":29650,\"start\":29639},{\"end\":30000,\"start\":29991},{\"end\":30011,\"start\":30000},{\"end\":30257,\"start\":30246},{\"end\":30446,\"start\":30438},{\"end\":30464,\"start\":30446},{\"end\":30473,\"start\":30464},{\"end\":30487,\"start\":30473},{\"end\":30712,\"start\":30703},{\"end\":30723,\"start\":30712},{\"end\":30737,\"start\":30723},{\"end\":30750,\"start\":30737},{\"end\":30758,\"start\":30750},{\"end\":31110,\"start\":31100},{\"end\":31119,\"start\":31110},{\"end\":31128,\"start\":31119},{\"end\":31139,\"start\":31128},{\"end\":31152,\"start\":31139},{\"end\":31164,\"start\":31152},{\"end\":31174,\"start\":31164},{\"end\":31182,\"start\":31174},{\"end\":31193,\"start\":31182},{\"end\":31534,\"start\":31528},{\"end\":31549,\"start\":31534},{\"end\":31564,\"start\":31549},{\"end\":31575,\"start\":31564},{\"end\":31595,\"start\":31575},{\"end\":31912,\"start\":31903},{\"end\":31922,\"start\":31912},{\"end\":32193,\"start\":32179},{\"end\":32205,\"start\":32193},{\"end\":32218,\"start\":32205},{\"end\":32226,\"start\":32218},{\"end\":32240,\"start\":32226},{\"end\":32496,\"start\":32489},{\"end\":32505,\"start\":32496},{\"end\":32516,\"start\":32505},{\"end\":32525,\"start\":32516},{\"end\":32536,\"start\":32525},{\"end\":32759,\"start\":32749},{\"end\":32770,\"start\":32759},{\"end\":32778,\"start\":32770},{\"end\":33074,\"start\":33063},{\"end\":33081,\"start\":33074},{\"end\":33092,\"start\":33081},{\"end\":33513,\"start\":33501},{\"end\":33893,\"start\":33883},{\"end\":33910,\"start\":33893},{\"end\":33925,\"start\":33910},{\"end\":34150,\"start\":34143},{\"end\":34159,\"start\":34150},{\"end\":34172,\"start\":34159},{\"end\":34475,\"start\":34469},{\"end\":34482,\"start\":34475},{\"end\":34736,\"start\":34724},{\"end\":34752,\"start\":34736},{\"end\":34995,\"start\":34986},{\"end\":35010,\"start\":34995},{\"end\":35019,\"start\":35010},{\"end\":35029,\"start\":35019},{\"end\":35353,\"start\":35343},{\"end\":35361,\"start\":35353},{\"end\":35371,\"start\":35361},{\"end\":35383,\"start\":35371},{\"end\":35389,\"start\":35383},{\"end\":35397,\"start\":35389},{\"end\":35843,\"start\":35830},{\"end\":35851,\"start\":35843},{\"end\":35868,\"start\":35851},{\"end\":36103,\"start\":36093},{\"end\":36114,\"start\":36103},{\"end\":36375,\"start\":36362},{\"end\":36384,\"start\":36375},{\"end\":36395,\"start\":36384},{\"end\":36694,\"start\":36681},{\"end\":36703,\"start\":36694},{\"end\":36714,\"start\":36703},{\"end\":36973,\"start\":36963},{\"end\":36983,\"start\":36973},{\"end\":36994,\"start\":36983},{\"end\":37006,\"start\":36994},{\"end\":37015,\"start\":37006},{\"end\":37025,\"start\":37015},{\"end\":37246,\"start\":37237},{\"end\":37538,\"start\":37525},{\"end\":37548,\"start\":37538},{\"end\":37899,\"start\":37890},{\"end\":37909,\"start\":37899},{\"end\":38184,\"start\":38171},{\"end\":38193,\"start\":38184},{\"end\":38202,\"start\":38193},{\"end\":38212,\"start\":38202},{\"end\":38549,\"start\":38534},{\"end\":38559,\"start\":38549},{\"end\":38757,\"start\":38749},{\"end\":38769,\"start\":38757},{\"end\":38778,\"start\":38769},{\"end\":38971,\"start\":38965},{\"end\":38980,\"start\":38971},{\"end\":38989,\"start\":38980},{\"end\":38999,\"start\":38989},{\"end\":39010,\"start\":38999},{\"end\":39018,\"start\":39010},{\"end\":39300,\"start\":39291},{\"end\":39307,\"start\":39300},{\"end\":39316,\"start\":39307},{\"end\":39324,\"start\":39316}]", "bib_venue": "[{\"end\":29707,\"start\":29650},{\"end\":30060,\"start\":30011},{\"end\":30273,\"start\":30257},{\"end\":30436,\"start\":30349},{\"end\":30843,\"start\":30774},{\"end\":31232,\"start\":31193},{\"end\":31526,\"start\":31475},{\"end\":31970,\"start\":31922},{\"end\":32282,\"start\":32256},{\"end\":32574,\"start\":32536},{\"end\":32838,\"start\":32794},{\"end\":33150,\"start\":33092},{\"end\":33571,\"start\":33513},{\"end\":33932,\"start\":33925},{\"end\":34230,\"start\":34172},{\"end\":34467,\"start\":34400},{\"end\":34722,\"start\":34638},{\"end\":35087,\"start\":35029},{\"end\":35506,\"start\":35397},{\"end\":35828,\"start\":35765},{\"end\":36158,\"start\":36114},{\"end\":36444,\"start\":36395},{\"end\":36772,\"start\":36714},{\"end\":37073,\"start\":37040},{\"end\":37320,\"start\":37262},{\"end\":37620,\"start\":37548},{\"end\":37958,\"start\":37909},{\"end\":38250,\"start\":38212},{\"end\":38532,\"start\":38443},{\"end\":38833,\"start\":38794},{\"end\":39074,\"start\":39034},{\"end\":39362,\"start\":39324}]"}}}, "year": 2023, "month": 12, "day": 17}