{"id": 201660344, "updated": "2023-10-06 23:45:19.398", "metadata": {"title": "Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video", "authors": "[{\"first\":\"Jia-Wang\",\"last\":\"Bian\",\"middle\":[]},{\"first\":\"Zhichao\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Naiyan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Huangying\",\"last\":\"Zhan\",\"middle\":[]},{\"first\":\"Chunhua\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Ming-Ming\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Ian\",\"last\":\"Reid\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "35-45", "publication_date": {"year": 2019, "month": 8, "day": 28}, "abstract": "Recent work has shown that CNN-based depth and ego-motion estimators can be learned using unlabelled monocular videos. However, the performance is limited by unidentified moving objects that violate the underlying static scene assumption in geometric image reconstruction. More significantly, due to lack of proper constraints, networks output scale-inconsistent results over different samples, i.e., the ego-motion network cannot provide full camera trajectories over a long video sequence because of the per-frame scale ambiguity. This paper tackles these challenges by proposing a geometry consistency loss for scale-consistent predictions, and an induced self-discovered mask for handling moving objects and occlusions. Since we do not leverage multi-task learning like recent works, our framework is much simpler and more efficient. Extensive evaluation results demonstrate that our depth estimator achieves the state-of-the-art performance on the standard KITTI and Make3D datasets. Moreover, we show that our ego-motion network is able to predict a globally scale-consistent camera trajectory for long video sequences, and the resulting visual odometry accuracy is competitive with the state-of-the-art model that is trained using stereo videos. To the best of our knowledge, this is the first work to show that deep networks trained using monocular video snippets can predict globally scale-consistent camera trajectories over a long video sequence.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1908.10553", "mag": "2970369664", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/BianLWZSC019", "doi": null}}, "content": {"source": {"pdf_hash": "7c7744acb33e86a3cf3c58a6dc5f4ffe49cae1c9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1908.10553v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "54da6923f15a6668919646fccae4a9ad1cc01bd6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7c7744acb33e86a3cf3c58a6dc5f4ffe49cae1c9.txt", "contents": "\nUnsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video\n\n\nJia-Wang Bian \nSchool of Computer Science\nUniversity of Adelaide\nAustralia\n\nAustralian Centre for Robotic Vision\nAustralia\n\nZhichao Li \nTuSimple\nBeijingChina\n\nNaiyan Wang \nTuSimple\nBeijingChina\n\nHuangying Zhan \nSchool of Computer Science\nUniversity of Adelaide\nAustralia\n\nAustralian Centre for Robotic Vision\nAustralia\n\nChunhua Shen \nSchool of Computer Science\nUniversity of Adelaide\nAustralia\n\nAustralian Centre for Robotic Vision\nAustralia\n\nMing-Ming Cheng \nNankai University\nTianjinChina\n\nIan Reid \nSchool of Computer Science\nUniversity of Adelaide\nAustralia\n\nAustralian Centre for Robotic Vision\nAustralia\n\nUnsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video\n\nRecent work has shown that CNN-based depth and ego-motion estimators can be learned using unlabelled monocular videos. However, the performance is limited by unidentified moving objects that violate the underlying static scene assumption in geometric image reconstruction. More significantly, due to lack of proper constraints, networks output scale-inconsistent results over different samples, i.e., the ego-motion network cannot provide full camera trajectories over a long video sequence because of the per-frame scale ambiguity. This paper tackles these challenges by proposing a geometry consistency loss for scale-consistent predictions, and an induced self-discovered mask for handling moving objects and occlusions. Since we do not leverage multi-task learning like recent works, our framework is much simpler and more efficient. Extensive evaluation results demonstrate that our depth estimator achieves the state-of-the-art performance on the standard KITTI and Make3D datasets. Moreover, we show that our egomotion network is able to predict a globally scale-consistent camera trajectory for long video sequences, and the resulting visual odometry accuracy is competitive with the state-of-the-art model that is trained using stereo videos. To the best of our knowledge, this is the first work to show that deep networks trained using monocular video snippets can predict globally scale-consistent camera trajectories over a long video sequence.\n\nIntroduction\n\nDepth and ego-motion estimation is crucial for various applications in robotics and computer vision. Traditional methods are usually hand-crafted stage-wise systems, which rely on correspondence search [1] and epipolar geometry [2] for estimation. Recently, deep learning based methods [3,4] show that the depth can be inferred from a single image by using Convolutional Neural Network (CNN). Especially, unsupervised methods [5][6][7][8][9] show that CNN-based depth and ego-motion networks can be solely trained on monocular video sequences without using ground-truth depth or stereo image pairs (pose supervision). The principle is that one can warp the image in one frame to another frame using the predicted depth and ego-motion, and then employ the image reconstruction loss as the supervision signal [5] to train the network. However, the performance limitation arises due to the moving objects that violate the underlying static scene assumption in geometric image reconstruction. More significantly, due to lack of proper constraints the network predicts scale-inconsistent results over different samples, i.e., the ego-motion network cannot provide a full camera trajectory over a long video sequence because of the per-frame scale ambiguity 1 .\n\nTo the best of our knowledge, no previous work (learning from monocular videos) addresses the scale-inconsistency issue mentioned above. To this end, we propose a geometry consistency loss for tackling the challenge. Specifically, for any two consecutive frames sampled from a video, we convert the predicted depth map in one frame to 3D space, then project it to the other frame using the estimated ego-motion, and finally minimize the inconsistency of the projected and the estimated depth maps. This explicitly enforces the depth network to predict geometry-consistent (of course scale-consistent) results over consecutive frames. With iterative sampling and training from videos, depth predictions on each consecutive image pair would be scale-consistent, and the frame-to-frame consistency can eventually propagate to the entire video sequence. As the scale of ego-motions is tightly linked to the scale of depths, the proposed ego-motion network can predict scale-consistent relative camera poses over consecutive snippets. We show that just simply accumulating pose predictions can result in globally scale-consistent camera trajectories over a long video sequence (Fig. 3).\n\nRegarding the challenge of moving objects, recent work addresses it by introducing an additional optical flow [7][8][9]11] or semantic segmentation network [12]. Although this improves performance significantly, it also brings about huge computational cost. Here we show that we could automatically discover a mask from the proposed geometry consistency term for solving the problem without introducing new networks. Specifically, we can easily locate pixels that belong to dynamic objects/occluded regions or difficult regions (e.g., textureless regions) using the proposed term. By assigning lower weights to those pixels, we can avoid their impact to the fragile image reconstruction loss (see Fig. 2 for mask visualization). Compared with these recent approaches [7][8][9]] that leverage multi-task learning, the proposed method is much simpler and more efficient.\n\nWe conduct detailed ablation studies that clearly demonstrate the efficacy of the proposed approach. Furthermore, comprehensive evaluation results on both KITTI [13] and Make3D [14] datasets show that our depth network outperforms state-of-the-art models that are trained in more complicated multi-task learning frameworks [7][8][9]15]. Meanwhile, our ego-motion network is able to predict scale-consistent camera trajectories over long video sequences, and the accuracy of trajectory is competitive with the state-of-the-art model that is trained using stereo videos [16].\n\nTo summarize, our main contributions are three-fold:\n\n\u2022 We propose a geometry consistency constraint to enforce the scale-consistency of depth and ego-motion networks, leading to a globally scale-consistent ego-motion estimator. \u2022 We propose a self-discovered mask for dynamic scenes and occlusions by the aforementioned geometry consistency constraint. Compared with other approaches, our proposed approach does not require additional optical flow or semantic segmentation networks, which makes the learning framework simpler and more efficient. \u2022 The proposed depth estimator achieves state-of-the-art performance on KITTI and Make3D datasets, and the proposed ego-motion predictor shows competitive visual odometry results compared with the state-of-the-art model that is trained using stereo videos.\n\n\nRelated work\n\nTraditional methods rely on the disparity between multiple views of a scene to recover the 3D scene geometry, where at least two images are required [17]. With the rapid development of deep learning, Eigen et al. [3] show that the depth can be predicted from a single image using Convolution Neural Network (CNN). Specifically, they design a coarse-to-fine network to predict the single-view depth and use the ground truth depths acquired by range sensors as the supervision signal to train the network. However, although these supervised methods [3,4,[18][19][20] show high-quality flow and depth estimation results, it is expensive to acquire ground truth in real-world scenes.\n\nWithout requiring the ground truth depth, Garg et al. [21] show that a single-view depth network can be trained using stereo image pairs. Instead of using depth supervision, they leverage the established epipolar geometry [17]. The color inconsistency between a left image and a synthesized left image warped from the right image is used as the supervision signal. Following this idea, Godard et al. [22] propose to constrain the left-right consistency for regularization, and Zhan et al. [16] extend the method to stereo videos. However, though stereo pairs based methods do not require the ground truth depth, accurately rectifying stereo cameras is also non-trivial in real-world scenarios.\n\nTo that end, Zhou et al. [5] propose a fully unsupervised framework, in which the depth network can be learned solely from monocular videos. The principle is that they introduce an additional ego-motion network to predict the relative camera pose between consecutive frames. With the estimated depth and relative pose, image reconstruction as in [21] is applied and the photometric loss is used as the supervision signal. However, the performance is limited due to dynamic objects that violate the underlying static scene assumption in geometric image reconstruction. More importantly, Zhou et al. [5]'s method suffers from the per-frame scale ambiguity, in that a single and consistent scaling of the camera translations is missing and only direction is known. As a result, the ego-motion network cannot predict a full camera trajectory over a long video sequence.\n\nFor handling moving objects, recent work [7,8] proposes to introduce an additional optical flow network. Even more recently [9] introduces an extra motion segmentation network. Although they show significant performance improvement, there is a huge additional computational cost added into the basic framework, yet they still suffer from the scale-inconsistency issue.\n\nTo the best of our knowledge, this paper is the first one to show that the ego-motion network trained in monocular videos can predict a globally scale-consistent camera trajectory over a long video sequence. This shows significant potentials to leverage deep learning methods in Visual SLAM [10] for robotics and autonomous driving.\n\n3 Unsupervised Learning of Scale-consistent Depth and Ego-motion\n\n\nMethod Overview\n\nOur goal is to train depth and ego-motion networks using monocular videos, and constrain them to predict scale-consistent results. Given two consecutive frames (I a , I b ) sampled from an unlabeled video, we first estimate their depth maps (D a , D b ) using the depth network, and then predict the relative 6D camera pose P ab between them using the pose network.\n\nWith the predicted depth and relative camera pose, we can synthesize the reference image I a by interpolating the source image I b [23,5]. Then, the network can be supervised by the photometric loss between the real image I a and the synthesized one I a . However, due to dynamic scenes that violate the geometric assumption in image reconstruction, the performance of this basic framework is limited. To this end, we propose a geometry consistency loss L GC for scale-consistency and a self-discovered mask M for handling the moving objects and occlusions. Fig. 1 shows an illustration of the proposed loss and mask.\n\nOur overall objective function can be formulated as follows:\nL = \u03b1L M p + \u03b2L s + \u03b3L GC ,(1)\nwhere L M p stands for the weighted photometric loss (L p ) by the proposed mask M , and L s stands for the smoothness loss. We train the network in both forward and backward directions to maximize the data usage, and for simplicity we only derive the loss for the forward direction.\n\nIn the following sections, we first introduce the widely used photometric loss and smoothness loss in Sec. 3.2, and then describe the proposed geometric consistency loss in Sec. 3.3 and the self-discovered mask in Sec. 3.4.\n\n\nPhotometric loss and smoothness loss\n\nPhotometric loss. Leveraging the brightness constancy and spatial smoothness priors used in classical dense correspondence algorithms [24], previous works [5,[7][8][9] have used the photometric error between the warped frame and the reference frame as an unsupervised loss function for training the network.  Figure 1: Illustration of the proposed geometry consistency loss and self-discover mask. Given two consecutive frames (I a , I b ), we first estimate their depth maps (D a , D b ) and relative pose (P ab ) using the network, then we get the warped (D a b ) by converting D a to 3D space and projecting to the image plane of I b using P ab , and finally we use the inconsistency between D a b and the D b interpolated from D b as the geometric consistency loss L GC (Eqn. 6) to supervise the network training. Here, we interpolate D b because the projection flow does not lie on the pixel grid of I b . Besides, we discover a mask M (Eqn. 7) from the inconsistency map for handling dynamic scenes and ill-estimated regions (Fig. 2). For clarity, the photometric loss and smoothness loss are not shown in this figure.\n\nWith the predicted depth map D a and the relative camera pose P ab , we synthesize I a by warping I b , where differentiable bilinear interpolation [23] is used as in [5]. With the synthesized I a and the reference image I a , we formulate the objective function as\nL p = 1 |V | p\u2208V I a (p) \u2212 I a (p) 1 ,(2)\nwhere V stands for valid points that are successfully projected from I a to the image plane of I b , and |V | defines the number of points in V . We choose L 1 loss due to its robustness to outliers. However, it is still not invariant to illumination changes in real-world scenarios. Here we add an additional image dissimilarity loss SSIM [25] for better handling complex illumination changes, since it normalizes the pixel illumination. We modify the photometric loss term Eqn. 2 as:\nL p = 1 |V | p\u2208V (\u03bb i I a (p) \u2212 I a (p) 1 + \u03bb s 1 \u2212 SSIM aa (p) 2 ),(3)\nwhere SSIM aa stands for the element-wise similarity between I a and I a by the SSIM function [25]. Following [22,7,9], we use \u03bb i = 0.15 and \u03bb s = 0.85 in our framework.\n\nSmoothness loss. As the photometric loss is not informative in low-texture nor homogeneous region of the scene, existing work incorporates a smoothness prior to regularize the estimated depth map. We adopt the edge-aware smoothness loss used in [9], which is formulated as:\nL s = p (e \u2212\u2207Ia(p) \u00b7 \u2207D a (p)) 2 ,(4)\nwhere \u2207 is the first derivative along spatial directions. It ensures that smoothness is guided by the edge of images.\n\n\nGeometry consistency loss\n\nAs mentioned before, we enforce the geometry consistency on the predicted results. Specifically, we require that D a and D b (related by P ab ) conform the same 3D scene structure, and use their differences as the loss function. Here minimizing this loss not only encourages the geometry consistency between samples in a batch but also transfers the consistency to the entire sequence. e.g., depths of I 1 agree with depths of I 2 in a batch; depths of I 2 agree with depths of I 3 in another training batch. Eventually, depths of I i of a sequence should all agree with each other. As the PoseNet is coupled with the DepthNet during training, our method yields scale-consistent predictions over the entire sequence. With this constraint, we can compute the depth inconsistency map D diff . For each p \u2208 V , it is defined as:\nD diff (p) = |D a b (p) \u2212 D b (p)| D a b (p) + D b (p)(5)\nwhere D a b is the computed depth map of I b by warping D a using P ab , and D b is the interpolated depth map from the estimated depth map D b (Note that we cannot directly use D b because the warping flow does not lie on the pixel grid ). Here we normalize their difference by their sum. This is more intuitive than the absolute distance as it treats points at different absolute depths equally in optimization. Besides, the function is symmetric and the outputs are naturally ranging from 0 to 1, which contributes to numerical stability in training.\n\nWith the inconsistency map, we simply define the proposed geometry consistency loss as:\nL GC = 1 |V | p\u2208V D diff (p),(6)\nwhich minimizes the geometric distance of predicted depths between each consecutive pair and enforces their scale-consistency. With training, the consistency can propagate to the entire video sequence. Due to the tight link between ego-motion and depth predictions, the ego-motion network can eventually predict globally scale-consistent trajectories (Fig. 3).\n\n\nSelf-discovered mask\n\nTo handle moving objects and occlusions that may impair the network training, recent work propose to introduce an additional optical flow [7][8][9] or semantic segmentation network [12]. This is effective, however it also introduces extra computational cost and training burden. Here, we show that these regions can be effectively located by the proposed inconsistency map D diff in Eqn. 5.\n\nThere are several scenarios that result in inconsistent scene structure observed from different views, including (1) dynamic objects, (2) occlusions, and (3) inaccurate predictions for difficult regions. Without separating them explicitly, we observe each of these will result in D diff increasing from its ideal value of zero.\n\nBased on this simple observation, we propose a weight mask M as D diff is in [0, 1]:\nM = 1 \u2212 D diff ,(7)\nwhich assigns low/high weights for inconsistent/consistent pixels. It can be used to re-weight the photometric loss. Specifically, we modify the photometric loss in Eqn. 3 as\nL M p = 1 |V | p\u2208V (M (p) \u00b7 L p (p)).(8)\nBy using the mask, we mitigate the adverse impact from moving objects and occlusions. Further, the gradients computed on inaccurately predicted regions carry less weight during back-propagation. Fig. 2 shows visual results for the proposed mask, which coincides with our anticipation stated above.\n\n\nExperiment\n\n\nImplementation details\n\nNetwork architecture. For the depth network, we experiment with DispNetS [5] and DispRes-Net [9], which takes a single RGB image as input and outputs a depth map. For the ego-motion network, PoseNet without the mask prediction branch [5] is used. The network estimates a 6D relative camera pose from a concatenated RGB image pair. Instead of computing the loss on multiple-scale outputs of the depth network (4 scales in [5] or 6 scales in [9]), we empirically find that using single-scale supervision (i.e., only compute the loss on the finest output) is better (Tab. 1). Our single-scale supervision not only improves the performance but also contributes a more concise training pipeline. We hypothesize the reason of this phenomenon is that the photometric loss is not accurate in low-resolution images, where the pixel color is over-smoothed.\n\nSingle-view depth estimation. The proposed learning framework is implemented using PyTorch Library [26]. For depth network, we train and test models on KITTI raw dataset [13] using Eigen [3]'s split that is the same with related works [8,7,9,5]. Following [5], we use a snippet of three sequential video frames as a training sample, where we set the second image as reference frame to compute loss with other two images and then inverse their roles to compute loss again for maximizing the data usage. The data is also augmented with random scaling, cropping and horizontal flips during training, and we experiment with two input resolutions (416 \u00d7 128 and 832 \u00d7 256). We use ADAM [27] optimizer, and set the batch size to 4 and the learning rate to 10 \u22124 . During training, we adopt \u03b1 = 1.0, \u03b2 = 0.1, and \u03b3 = 0.5 in Eqn. 1. We train the network in 200 epochs with 1000 randomly sampled batches in one epoch, and validate the model at per epoch. Also, we pre-train the network on CityScapes [28] and finetune on KITTI [13], each for 200 epochs. Here we follow Eigen et al. [3]'s evaluation metrics for depth evaluation. Besides, we test models on Make3D [14] testing set without fine-tuning on training images to verify its generalization ability.\n\nVisual odometry. For pose network, following Zhan et al. [16], we evaluate visual odometry results on KITTI odometry dataset [13], where sequence 00-08/09-10 are used for training/testing. We use the standard evaluation metrics by the dataset for trajectory evaluation other than Zhou et al. [5]'s 5-frame pose evaluation, since they are more widely used and more meaningful. We also show results using 5-frame pose evaluation in supplementary material.\n\n\nAblation study\n\nIn this section, we first validate the efficacy of the proposed geometry-consistency loss L GC and the self-discovered weight mask M using both the multi-scale supervision and single-scale supervision, and then we experiment with different network architectures and image resolutions.   in brackets indicate the AbsRel when using multi-scale supervisions. It clearly shows that each component we design contributes to the performance improvement, and using single-scale supervision is better than multi-scale supervision (see more multi-scale results in supplementary).\n\nNetwork architectures and image resolutions. Tab. 2 shows the results of different network architectures and different resolution images, where \"-VGG\" stands for the method with DispNetS [5] and \"-ResNet\" stands for that with DispResNetS [9]. It is obvious that using higher resolution images and deeper networks can improve the performance. Following CC [9], we use the \"-ResNet\" trained on higher resolution images (832 \u00d7 256) to compare with state-of-the-art methods.\n\n\nComparisons with the state-of-the-art\n\nDepth results on KITTI raw dataset. Tab. 3 shows the results on KITTI raw dataset [13], where our method achieves the state-of-the-art performance when compared with models trained on monocular video sequences. Note that recent work [7][8][9]29] all jointly learn multiple tasks, while our approach does not. This effectively reduces the training and inference overhead. Moreover, our method competes quite favorably with other methods using stronger supervision signals such as calibrated stereo image pairs (i.e., pose supervision) or even ground-truth depth annotation.\n\nDepth estimation results on Make3D dataset. To verify the generalization ability of the model trained on KITTI [13], we test it on Make3D dataset [14] without fine-tuning. Tab. 4 shows the results that demonstrate the superiority of our method against other state-of-the-art methods. Visual odometry results. We compare with SfMLearner [5] and the method trained on stereo videos [16]. We also report the results of ORB-SLAM [10] (without loop closing), though emphasize that this results in a comparison note between a simple frame-to-frame pose estimation framework (i.e. (a) sequence 09 (b) sequence 10 Figure 3: Qualitative results on the testing sequences of KITTI odometry dataset [13].\n\nVisual Odometry) with a Visual SLAM system, in which the latter has a strong back-end optimization system (i.e., bundle adjustment [30]) for improving the performance. Here, we ignore the frames (First 9 and 30 respectively) from the sequences (09 and 10) for which ORB-SLAM [10] fails to output camera poses because of unsuccessful initialization. Tab. 5 shows the average translation and rotation errors for the testing sequence 09 and 10, and Fig. 3 shows qualitative results.\n\nNote that the comparison is highly disadvantageous to the proposed method: i) we align per-frame scale to the ground truth scale for [5] due to its scale-inconsistency, while we only align one global scale for our method; ii) [16] requires stereo videos for training, while we only use monocular videos.\n\nAlthough it is unfair to the proposed method, the results show that our method achieves competitive results with [16]. Even when compared with the ORB-SLAM [10] system, our method shows a lower translational error and a better visual result on sequence 09. This is a great progress that deep models trained on monocular videos can predict a globally scaleconsistent visual odometry. Also, the performance is remarkable that a monocular frame-to-frame pose estimator can provide a long and reasonable camera trajectory without any post-processing for drift correction. It is very challenging, because drifts accumulate over time and significantly limit the performance. More discussion on that can be also seen in [16].\n\n\nConclusion\n\nThis paper presents an unsupervised learning framework for scale-consistent depth and ego-motion estimation. The core of the proposed approach is a geometry consistency loss for scale-consistency and a self-discovered mask for handling dynamic scenes. With the proposed learning framework, our depth model achieves the state-of-the-art performance on KITTI [13] and Make3D [14] datasets, and our ego-motion network can show competitive visual odometry results with the model that is trained using stereo videos. To the best of our knowledge, this is the first work to show that deep models training on monocular videos can predict a globally scale-consistent camera trajectory over a long sequence. In future work, we will focus on improving the visual odometry accuracy by incorporating drift correcting solutions into the current framework. AbsRel multi-scale Figure 4: Validation results on KITTI raw dataset [13]. Both basic and basic+ssim (without the proposed L GC ) overfit after about 50 epoches, while others (with L GC ) not. Besides, the single-scale supervision leads to consistently better results than multi-scale supervisions (4 levels). Note that the validation set is different from the Eigen et al. [3]'s test split, so the validation results are not exactly same with the results reported in Tab. 6. However, they confirm the same conclusion.\n\n\nAblation study on scale numbers\n\nTab. 6 shows ablation study results of different scale numbers, where we experiment with four scale choices. It shows that using the single-scale (1-scale) leads to the best performance. Here, we hypothesis that it is because the photometric loss is not accurate in low-resolution images, where the pixel color is over-smoothed. \n\n\nPose estimation results on 5-frame snippets\n\nAlthough the visual odometry results shown in the main paper is more important, we also evaluate pose estimation results using Zhou et al. [5]'s evaluation metric on 5-frame snippets. Tab. 7 shows the results, where our method shows slightly lower performances with the state-of-the-art methods but the gap is small. 0.014 \u00b1 0.008 0.012 \u00b1 0.011 ORB-SLAM (short) 0.064 \u00b1 0.141 0.064 \u00b1 0.130\n\nMean Odometry 0.032 \u00b1 0.026 0.028 \u00b1 0.023 Zhou et al. [5] 0.021 \u00b1 0.017 0.020 \u00b1 0.015 Mahjourian et al. [6] 0.013 \u00b1 0.010 0.012 \u00b1 0.011 GeoNet [7] 0.012 \u00b1 0.007 0.012 \u00b1 0.009 DF-Net [8] 0.017 \u00b1 0.007 0.015 \u00b1 0.009 CC [9] 0.012 \u00b1 0.007 0.012 \u00b1 0.009 Ours 0.016 \u00b1 0.007 0.015 \u00b1 0.015\n(a) (b) (c) (d)\n(e) (f) Figure 5: Visual results. Top to bottom: sample image, estimated depth, self-discovered mask. The proposed mask can effectively identify inconsistent pixels caused by moving objects and occlusions, e.g., motion boundaries and occluded regions. It demonstrates the efficacy of proposed mask in terms of detecting moving objects and occlusions.\n\n\nQualitative results\n\n\nEfficiency of training\n\nWe compare with the most recent CC [9] in terms of training time. Both methods are tested on a single 16GB Tesla V100 GPU. We measure the time taken for each iteration consisting of forward and backward pass using a batch size of 4, where 832 \u00d7 256 images are used. CC [9] needs train 3 parts in an iterative way, including (DepthNet, PoseNet), FlowNet, and MaskNet. In contrast our method only trains (DepthNet, PoseNet) for 200K iterations. In total, CC takes about 7 days for training, while our method takes 32 hours. The results of CC [9] are reported by authors. \n\nFigure 2 :\n2Visual results. Top to bottom: sample image, estimated depth, self-discovered mask. The proposed mask can effectively identify inconsistent pixels caused by occlusions and moving objects, e.g., motion boundaries (a) and occluded regions (b).\n\nFig. 5\n5illustrates visual results of depth estimation and occlusion detection by the proposed approach.\n\nTable 1 :\n1Ablation study results on the test split of KITTI raw dataset. Numbers in brackets indicate the AbsRel when using multi-scale supervisions (4 scales).Error \u2193 \nAccuracy \u2191 \nMethods \nAbsRel \nSqRel RMS RMSlog < 1.25 < 1.25 2 < 1.25 3 \nBasic \n0.161 (0.185) 1.225 5.765 \n0.237 \n0.780 \n0.927 \n0.972 \nBasic+SSIM \n0.160 (0.163) 1.230 5.950 \n0.243 \n0.775 \n0.923 \n0.969 \nBasic+SSIM+GC \n0.158 (0.161) 1.247 5.827 \n0.235 \n0.786 \n0.927 \n0.971 \nBasic+SSIM+GC+M (ours) 0.151 (0.158) 1.154 5.716 \n0.232 \n0.798 \n0.930 \n0.972 \n\n\n\nTable 2 :\n2Ablation studies on different network architectures and image resolutions.Validating L GC and M . We experiment with DispNetS[5] network and conduct ablation studies on images of 416 \u00d7 128 resolution, noted as Basic. Tab. 1 shows the depth results, where the numbersError \u2193 \nAccuracy \u2191 \nMethods \nResolutions AbsRel SqRel RMS RMSlog < 1.25 < 1.25 2 < 1.25 3 \nOurs-VGG \n416 \u00d7 128 \n0.151 \n1.154 5.716 \n0.232 \n0.798 \n0.930 \n0.972 \nOurs-ResNet \n0.149 \n1.137 5.771 \n0.230 \n0.799 \n0.932 \n0.973 \nOurs-VGG \n832 \u00d7 256 \n0.146 \n1.197 5.578 \n0.223 \n0.814 \n0.940 \n0.975 \nOurs-ResNet \n0.137 \n1.089 5.439 \n0.217 \n0.830 \n0.942 \n0.975 \n\n\n\nTable 3 :\n3Single-view depth estimation results on test split of KITTI raw dataset[13]. The methods trained on KITTI raw dataset[13] are denoted by K. Models with pre-training on CityScapes[28] are denoted by CS+K. (D) denotes depth supervision, (B) denotes binocular/stereo input pairs, (M) denotes monocular video clips. (J) denotes joint learning of multiple tasks. The best performance in each block is highlighted as bold.Error \u2193 \nAccuracy \u2191 \n\n\nTable 4 :\n4Depth results (AbsRel) on Make3D[14] test set without finetuning.Methods Zhou et al. [5] Godard et al. [22] DF-Net [8] CC [9] Ours \nAbsRel \n0.383 \n0.544 \n0.331 \n0.320 0.312 \n\n\n\nTable 5 :\n5Visual odometry results on KITTI odometry dataset[13]. We report the performance of ORB-SLAM[10] as a reference and compare with recent deep methods. K denotes the model trained on KITTI, and CS+K denotes the model with pre-training on Cityscapes[28].Methods \nSeq. 09 \nSeq. 10 \nt err (%) r err ( \u2022 /100m) t err (%) r err ( \u2022 /100m) \nORB-SLAM [10] \n15.30 \n0.26 \n3.68 \n0.48 \nZhou et al. [5] \n17.84 \n6.78 \n37.91 \n17.78 \nZhan et al. [16] \n11.93 \n3.91 \n12.45 \n3.46 \nOurs (K) \n11.2 \n3.35 \n10.1 \n4.96 \nOurs (CS+K) \n8.24 \n2.19 \n10.7 \n4.58 \n\n\n\nTable 6 :\n6Ablation study results for scale numbers Error \u2193 Accuracy \u2191 Scales AbsRel SqRel RMS RMSlog < 1.25 < 1.25 2 < 1.25 31-scale \n0.151 \n1.154 5.716 \n0.232 \n0.798 \n0.930 \n0.972 \n2-scale \n0.152 \n1.192 5.900 \n0.235 \n0.795 \n0.927 \n0.971 \n3-scale \n0.159 \n1.226 5.987 \n0.240 \n0.780 \n0.921 \n0.969 \n4-scale \n0.158 \n1.214 5.898 \n0.239 \n0.782 \n0.925 \n0.971 \n\n\n\nTable 7 :\n7Pose estimation results on KITTI odometry dataset.Seq. 09 \nSeq. 10 \nORB-SLAM (full) \n\n\nTable 8 :\n8Training time and the number of model parameters for each networkCC [9] \nOurs \nNetwork \n(DepthNet, PoseNet) FlowNet MaskNet (DepthNet, PoseNet) \nTime \n0.96s \n1.32s \n0.48s \n0.55s \nParameter Numbers \n(80.88M, 2.18M) \n39.28M \n5.22M \n(80.88M, 1.59M) \n\nMonocular systems such as ORB-SLAM[10] suffer from the scale ambiguity issue, but their predictions are globally scale-consistent. However, recently learned models using monocular videos not only suffer from the scale ambiguity, but also predict scale-inconsistent results over different snippets.\n\nGMS: Grid-based motion statistics for fast, ultra-robust feature correspondence. Jia-Wang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit Yeung, Tan-Dat Nguyen, Ming-Ming Cheng, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jia-Wang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit Yeung, Tan-Dat Nguyen, and Ming- Ming Cheng. GMS: Grid-based motion statistics for fast, ultra-robust feature correspondence. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\n\nAn evaluation of feature matchers for fundamental matrix estimation. Jia-Wang Bian, Yu-Huan Wu, Ji Zhao, Yun Liu, Le Zhang, Ming-Ming Cheng, Ian Reid, British Machine Vision Conference (BMVC). Jia-Wang Bian, Yu-Huan Wu, Ji Zhao, Yun Liu, Le Zhang, Ming-Ming Cheng, and Ian Reid. An evaluation of feature matchers for fundamental matrix estimation. In British Machine Vision Conference (BMVC), 2019.\n\nDepth map prediction from a single image using a multi-scale deep network. David Eigen, Christian Puhrsch, Rob Fergus, Neural Information Processing Systems (NIPS). David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In Neural Information Processing Systems (NIPS), 2014.\n\nLearning depth from single monocular images using deep convolutional neural fields. Fayao Liu, Chunhua Shen, Guosheng Lin, Ian Reid, IEEE Transactions on Pattern Recognition and Machine Intelligence (PAMI). 38Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning depth from single monocular images using deep convolutional neural fields. IEEE Transactions on Pattern Recognition and Machine Intelligence (PAMI), 38(10), 2016.\n\nUnsupervised learning of depth and ego-motion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, IEEE Conference on Computer Vision and Pattern Recognition. Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nUnsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints. Reza Mahjourian, Martin Wicke, Anelia Angelova, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Reza Mahjourian, Martin Wicke, and Anelia Angelova. Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nGeoNet: Unsupervised learning of dense depth, optical flow and camera pose. Zhichao Yin, Jianping Shi, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Zhichao Yin and Jianping Shi. GeoNet: Unsupervised learning of dense depth, optical flow and camera pose. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nDF-Net: Unsupervised joint learning of depth and flow using cross-task consistency. Yuliang Zou, Zelun Luo, Jia-Bin Huang, European Conference on Computer Vision (ECCV). Yuliang Zou, Zelun Luo, and Jia-Bin Huang. DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency. In European Conference on Computer Vision (ECCV), 2018.\n\nCompetitive Collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. Anurag Ranjan, Varun Jampani, Kihwan Kim, Deqing Sun, Jonas Wulff, Michael J Black, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Anurag Ranjan, Varun Jampani, Kihwan Kim, Deqing Sun, Jonas Wulff, and Michael J Black. Competitive Collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nORB-SLAM: a versatile and accurate monocular slam system. Raul Mur-Artal, Jose Maria Martinez Montiel, Juan D Tardos, IEEE Transactions on Robotics. 531Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. ORB-SLAM: a versatile and accurate monocular slam system. IEEE Transactions on Robotics (TRO), 31(5), 2015.\n\nJoint unsupervised learning of optical flow and depth by watching stereo videos. Yang Wang, Zhenheng Yang, Peng Wang, Yi Yang, Chenxu Luo, Wei Xu, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Yang Wang, Zhenheng Yang, Peng Wang, Yi Yang, Chenxu Luo, and Wei Xu. Joint unsupervised learning of optical flow and depth by watching stereo videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nLook deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss. Jianbo Jiao, Ying Cao, Yibing Song, Rynson Lau, European Conference on Computer Vision (ECCV). Jianbo Jiao, Ying Cao, Yibing Song, and Rynson Lau. Look deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss. In European Conference on Computer Vision (ECCV), 2018.\n\nVision meets Robotics: The kitti dataset. Andreas Geiger, Philip Lenz, Christoph Stiller, Raquel Urtasun, International Journal of Robotics Research. Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets Robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013.\n\nLearning depth from single monocular images. Ashutosh Saxena, H Sung, Andrew Y Chung, Ng, Neural Information Processing Systems (NIPS). Ashutosh Saxena, Sung H Chung, and Andrew Y Ng. Learning depth from single monocular images. In Neural Information Processing Systems (NIPS), 2006.\n\nLearning depth from monocular videos using direct methods. Chaoyang Wang, Jos\u00e9 Miguel Buenaposada, Rui Zhu, Simon Lucey, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Chaoyang Wang, Jos\u00e9 Miguel Buenaposada, Rui Zhu, and Simon Lucey. Learning depth from monocular videos using direct methods. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n\nUnsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction. Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li, Harsh Agarwal, Ian Reid, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li, Harsh Agarwal, and Ian Reid. Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nMultiple view geometry in computer vision. Richard Hartley, Andrew Zisserman, Cambridge university pressRichard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.\n\nSemi-supervised deep learning for monocular depth map prediction. Yevhen Kuznietsov, Jorg Stuckler, Bastian Leibe, IEEE Conference on Computer Vision and Pattern Recognition. Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-supervised deep learning for monoc- ular depth map prediction. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nBA-Net: Dense bundle adjustment network. Chengzhou Tang, Ping Tan, International Conference on Learning Representations (ICLR). Chengzhou Tang and Ping Tan. BA-Net: Dense bundle adjustment network. In International Conference on Learning Representations (ICLR), 2019.\n\nHierarchical discrete distribution decomposition for match density estimation. Zhichao Yin, Trevor Darrell, Fisher Yu, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Zhichao Yin, Trevor Darrell, and Fisher Yu. Hierarchical discrete distribution decomposition for match density estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nUnsupervised cnn for single view depth estimation: Geometry to the rescue. Ravi Garg, Vijay Kumar, B G , Gustavo Carneiro, Ian Reid, European Conference on Computer Vision (ECCV). SpringerRavi Garg, Vijay Kumar BG, Gustavo Carneiro, and Ian Reid. Unsupervised cnn for single view depth estimation: Geometry to the rescue. In European Conference on Computer Vision (ECCV). Springer, 2016.\n\nUnsupervised monocular depth estimation with left-right consistency. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J Brostow, IEEE Conference on Computer Vision and Pattern Recognition. Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular depth estimation with left-right consistency. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nSpatial transformer networks. Max Jaderberg, Karen Simonyan, Andrew Zisserman, Neural Information Processing Systems (NIPS). Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Neural Information Processing Systems (NIPS), 2015.\n\nLucas-kanade 20 years on: A unifying framework. Simon Baker, Iain Matthews, International Journal on Computer Vision (IJCV). 563Simon Baker and Iain Matthews. Lucas-kanade 20 years on: A unifying framework. Interna- tional Journal on Computer Vision (IJCV), 56(3), 2004.\n\nImage Quality Assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Sheikh, P Eero, Simoncelli, IEEE Transactions on Image Processing (TIP). 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al. Image Quality Assess- ment: from error visibility to structural similarity. IEEE Transactions on Image Processing (TIP), 13(4), 2004.\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.\n\nADAM: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nUnsupervised learning of geometry with edge-aware depth-normal consistency. Zhenheng Yang, Peng Wang, Wei Xu, Liang Zhao, Ramakant Nevatia, Association for the Advancement of Artificial Intelligence (AAAI). Zhenheng Yang, Peng Wang, Wei Xu, Liang Zhao, and Ramakant Nevatia. Unsupervised learn- ing of geometry with edge-aware depth-normal consistency. In Association for the Advancement of Artificial Intelligence (AAAI), 2018.\n\nBundle adjustment-a modern synthesis. Bill Triggs, F Philip, Richard I Mclauchlan, Andrew W Hartley, Fitzgibbon, International workshop on vision algorithms. SpringerBill Triggs, Philip F McLauchlan, Richard I Hartley, and Andrew W Fitzgibbon. Bundle adjustment-a modern synthesis. In International workshop on vision algorithms. Springer, 1999.\n\nIt shows that both basic and basic+ssim would overfit after about 50 epoches, while others (with the proposed L GC ) not. This demonstrates that L GC can effectively regularize the network to avoid overfitting. Besides, Fig. 4 shows that the single-scale supervision leads to consistently better depth results than multi-scale supervisions. This is also demonstrated in Tab. 6. Note that their results are not exactly same due to the different data used for evaluation (validation and testing sets, respectively). We report the validation results during training in Fig. 4. but they confirm the same conclusionWe report the validation results during training in Fig. 4. It shows that both basic and basic+ssim would overfit after about 50 epoches, while others (with the proposed L GC ) not. This demonstrates that L GC can effectively regularize the network to avoid overfitting. Besides, Fig. 4 shows that the single-scale supervision leads to consistently better depth results than multi-scale supervisions. This is also demonstrated in Tab. 6. Note that their results are not exactly same due to the different data used for evaluation (validation and testing sets, respectively), but they confirm the same conclusion.\n", "annotations": {"author": "[{\"end\":207,\"start\":84},{\"end\":242,\"start\":208},{\"end\":278,\"start\":243},{\"end\":403,\"start\":279},{\"end\":526,\"start\":404},{\"end\":575,\"start\":527},{\"end\":694,\"start\":576}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":93},{\"end\":218,\"start\":216},{\"end\":254,\"start\":250},{\"end\":293,\"start\":289},{\"end\":416,\"start\":412},{\"end\":542,\"start\":537},{\"end\":584,\"start\":580}]", "author_first_name": "[{\"end\":92,\"start\":84},{\"end\":215,\"start\":208},{\"end\":249,\"start\":243},{\"end\":288,\"start\":279},{\"end\":411,\"start\":404},{\"end\":536,\"start\":527},{\"end\":579,\"start\":576}]", "author_affiliation": "[{\"end\":158,\"start\":99},{\"end\":206,\"start\":160},{\"end\":241,\"start\":220},{\"end\":277,\"start\":256},{\"end\":354,\"start\":295},{\"end\":402,\"start\":356},{\"end\":477,\"start\":418},{\"end\":525,\"start\":479},{\"end\":574,\"start\":544},{\"end\":645,\"start\":586},{\"end\":693,\"start\":647}]", "title": "[{\"end\":81,\"start\":1},{\"end\":775,\"start\":695}]", "venue": null, "abstract": "[{\"end\":2233,\"start\":777}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2454,\"start\":2451},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2480,\"start\":2477},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2538,\"start\":2535},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2540,\"start\":2538},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2678,\"start\":2675},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2681,\"start\":2678},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2684,\"start\":2681},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2687,\"start\":2684},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2690,\"start\":2687},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3059,\"start\":3056},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4802,\"start\":4799},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4805,\"start\":4802},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4808,\"start\":4805},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4811,\"start\":4808},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4849,\"start\":4845},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5459,\"start\":5456},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5462,\"start\":5459},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5465,\"start\":5462},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5724,\"start\":5720},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5740,\"start\":5736},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5885,\"start\":5882},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5888,\"start\":5885},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5891,\"start\":5888},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5894,\"start\":5891},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6131,\"start\":6127},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7107,\"start\":7103},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7170,\"start\":7167},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7504,\"start\":7501},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7506,\"start\":7504},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7510,\"start\":7506},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7514,\"start\":7510},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7518,\"start\":7514},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7693,\"start\":7689},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7861,\"start\":7857},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8039,\"start\":8035},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8128,\"start\":8124},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8358,\"start\":8355},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8680,\"start\":8676},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8931,\"start\":8928},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9241,\"start\":9238},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9243,\"start\":9241},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9324,\"start\":9321},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9862,\"start\":9858},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10487,\"start\":10483},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10489,\"start\":10487},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11750,\"start\":11746},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11770,\"start\":11767},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11773,\"start\":11770},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11776,\"start\":11773},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11779,\"start\":11776},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12890,\"start\":12886},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12908,\"start\":12905},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13390,\"start\":13386},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13702,\"start\":13698},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13718,\"start\":13714},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13720,\"start\":13718},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13722,\"start\":13720},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14024,\"start\":14021},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16321,\"start\":16318},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16324,\"start\":16321},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16327,\"start\":16324},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16365,\"start\":16361},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17635,\"start\":17632},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17655,\"start\":17652},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17796,\"start\":17793},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17983,\"start\":17980},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18002,\"start\":17999},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18510,\"start\":18506},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18581,\"start\":18577},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18597,\"start\":18594},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18645,\"start\":18642},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18647,\"start\":18645},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18649,\"start\":18647},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18651,\"start\":18649},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18666,\"start\":18663},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19092,\"start\":19088},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19402,\"start\":19398},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19429,\"start\":19425},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19483,\"start\":19480},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19565,\"start\":19561},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19717,\"start\":19713},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19785,\"start\":19781},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19951,\"start\":19948},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20889,\"start\":20886},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20940,\"start\":20937},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21057,\"start\":21054},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21297,\"start\":21293},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21447,\"start\":21444},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21450,\"start\":21447},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21453,\"start\":21450},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21456,\"start\":21453},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21900,\"start\":21896},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21935,\"start\":21931},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22124,\"start\":22121},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22169,\"start\":22165},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22214,\"start\":22210},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22476,\"start\":22472},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22614,\"start\":22610},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22758,\"start\":22754},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23096,\"start\":23093},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23190,\"start\":23186},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23382,\"start\":23378},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23425,\"start\":23421},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23982,\"start\":23978},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24359,\"start\":24355},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24375,\"start\":24371},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24914,\"start\":24910},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25218,\"start\":25215},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25914,\"start\":25911},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26220,\"start\":26217},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26270,\"start\":26267},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26309,\"start\":26306},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26348,\"start\":26345},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26383,\"start\":26380},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26898,\"start\":26895},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27132,\"start\":27129},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27403,\"start\":27400},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28453,\"start\":28450},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29032,\"start\":29028},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29078,\"start\":29074},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29139,\"start\":29135},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29443,\"start\":29439},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29648,\"start\":29644},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29691,\"start\":29687},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29845,\"start\":29841},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30882,\"start\":30878}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":27684,\"start\":27430},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27790,\"start\":27685},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":28312,\"start\":27791},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28944,\"start\":28313},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29394,\"start\":28945},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29582,\"start\":29395},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30128,\"start\":29583},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30485,\"start\":30129},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":30583,\"start\":30486},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":30843,\"start\":30584}]", "paragraph": "[{\"end\":3504,\"start\":2249},{\"end\":4687,\"start\":3506},{\"end\":5557,\"start\":4689},{\"end\":6132,\"start\":5559},{\"end\":6186,\"start\":6134},{\"end\":6937,\"start\":6188},{\"end\":7633,\"start\":6954},{\"end\":8328,\"start\":7635},{\"end\":9195,\"start\":8330},{\"end\":9565,\"start\":9197},{\"end\":9899,\"start\":9567},{\"end\":9965,\"start\":9901},{\"end\":10350,\"start\":9985},{\"end\":10969,\"start\":10352},{\"end\":11031,\"start\":10971},{\"end\":11346,\"start\":11063},{\"end\":11571,\"start\":11348},{\"end\":12736,\"start\":11612},{\"end\":13003,\"start\":12738},{\"end\":13531,\"start\":13046},{\"end\":13774,\"start\":13604},{\"end\":14049,\"start\":13776},{\"end\":14205,\"start\":14088},{\"end\":15060,\"start\":14235},{\"end\":15672,\"start\":15119},{\"end\":15761,\"start\":15674},{\"end\":16155,\"start\":15795},{\"end\":16570,\"start\":16180},{\"end\":16899,\"start\":16572},{\"end\":16985,\"start\":16901},{\"end\":17180,\"start\":17006},{\"end\":17519,\"start\":17222},{\"end\":18405,\"start\":17559},{\"end\":19654,\"start\":18407},{\"end\":20109,\"start\":19656},{\"end\":20697,\"start\":20128},{\"end\":21169,\"start\":20699},{\"end\":21783,\"start\":21211},{\"end\":22477,\"start\":21785},{\"end\":22958,\"start\":22479},{\"end\":23263,\"start\":22960},{\"end\":23983,\"start\":23265},{\"end\":25359,\"start\":23998},{\"end\":25724,\"start\":25395},{\"end\":26161,\"start\":25772},{\"end\":26444,\"start\":26163},{\"end\":26811,\"start\":26461},{\"end\":27429,\"start\":26860}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11062,\"start\":11032},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13045,\"start\":13004},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13603,\"start\":13532},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14087,\"start\":14050},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15118,\"start\":15061},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15794,\"start\":15762},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17005,\"start\":16986},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17221,\"start\":17181},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26460,\"start\":26445}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2247,\"start\":2235},{\"attributes\":{\"n\":\"2\"},\"end\":6952,\"start\":6940},{\"attributes\":{\"n\":\"3.1\"},\"end\":9983,\"start\":9968},{\"attributes\":{\"n\":\"3.2\"},\"end\":11610,\"start\":11574},{\"attributes\":{\"n\":\"3.3\"},\"end\":14233,\"start\":14208},{\"attributes\":{\"n\":\"3.4\"},\"end\":16178,\"start\":16158},{\"attributes\":{\"n\":\"4\"},\"end\":17532,\"start\":17522},{\"attributes\":{\"n\":\"4.1\"},\"end\":17557,\"start\":17535},{\"attributes\":{\"n\":\"4.2\"},\"end\":20126,\"start\":20112},{\"attributes\":{\"n\":\"4.3\"},\"end\":21209,\"start\":21172},{\"attributes\":{\"n\":\"5\"},\"end\":23996,\"start\":23986},{\"attributes\":{\"n\":\"6.2\"},\"end\":25393,\"start\":25362},{\"attributes\":{\"n\":\"6.3\"},\"end\":25770,\"start\":25727},{\"attributes\":{\"n\":\"6.4\"},\"end\":26833,\"start\":26814},{\"attributes\":{\"n\":\"6.5\"},\"end\":26858,\"start\":26836},{\"end\":27441,\"start\":27431},{\"end\":27692,\"start\":27686},{\"end\":27801,\"start\":27792},{\"end\":28323,\"start\":28314},{\"end\":28955,\"start\":28946},{\"end\":29405,\"start\":29396},{\"end\":29593,\"start\":29584},{\"end\":30139,\"start\":30130},{\"end\":30496,\"start\":30487},{\"end\":30594,\"start\":30585}]", "table": "[{\"end\":28312,\"start\":27953},{\"end\":28944,\"start\":28591},{\"end\":29394,\"start\":29373},{\"end\":29582,\"start\":29472},{\"end\":30128,\"start\":29846},{\"end\":30485,\"start\":30256},{\"end\":30583,\"start\":30548},{\"end\":30843,\"start\":30661}]", "figure_caption": "[{\"end\":27684,\"start\":27443},{\"end\":27790,\"start\":27694},{\"end\":27953,\"start\":27803},{\"end\":28591,\"start\":28325},{\"end\":29373,\"start\":28957},{\"end\":29472,\"start\":29407},{\"end\":29846,\"start\":29595},{\"end\":30256,\"start\":30141},{\"end\":30548,\"start\":30498},{\"end\":30661,\"start\":30596}]", "figure_ref": "[{\"end\":4686,\"start\":4678},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5392,\"start\":5386},{\"end\":10916,\"start\":10910},{\"end\":11929,\"start\":11921},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12650,\"start\":12643},{\"end\":16153,\"start\":16146},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17423,\"start\":17417},{\"end\":22399,\"start\":22391},{\"end\":22931,\"start\":22925},{\"end\":24868,\"start\":24860},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26477,\"start\":26469}]", "bib_author_first_name": "[{\"end\":31232,\"start\":31224},{\"end\":31246,\"start\":31239},{\"end\":31260,\"start\":31252},{\"end\":31280,\"start\":31273},{\"end\":31295,\"start\":31288},{\"end\":31313,\"start\":31304},{\"end\":31730,\"start\":31722},{\"end\":31744,\"start\":31737},{\"end\":31751,\"start\":31749},{\"end\":31761,\"start\":31758},{\"end\":31769,\"start\":31767},{\"end\":31786,\"start\":31777},{\"end\":31797,\"start\":31794},{\"end\":32133,\"start\":32128},{\"end\":32150,\"start\":32141},{\"end\":32163,\"start\":32160},{\"end\":32486,\"start\":32481},{\"end\":32499,\"start\":32492},{\"end\":32514,\"start\":32506},{\"end\":32523,\"start\":32520},{\"end\":32897,\"start\":32890},{\"end\":32911,\"start\":32904},{\"end\":32923,\"start\":32919},{\"end\":32940,\"start\":32933},{\"end\":33306,\"start\":33302},{\"end\":33325,\"start\":33319},{\"end\":33339,\"start\":33333},{\"end\":33728,\"start\":33721},{\"end\":33742,\"start\":33734},{\"end\":34089,\"start\":34082},{\"end\":34100,\"start\":34095},{\"end\":34113,\"start\":34106},{\"end\":34476,\"start\":34470},{\"end\":34490,\"start\":34485},{\"end\":34506,\"start\":34500},{\"end\":34518,\"start\":34512},{\"end\":34529,\"start\":34524},{\"end\":34546,\"start\":34537},{\"end\":34963,\"start\":34959},{\"end\":34994,\"start\":34975},{\"end\":35008,\"start\":35004},{\"end\":35010,\"start\":35009},{\"end\":35311,\"start\":35307},{\"end\":35326,\"start\":35318},{\"end\":35337,\"start\":35333},{\"end\":35346,\"start\":35344},{\"end\":35359,\"start\":35353},{\"end\":35368,\"start\":35365},{\"end\":35774,\"start\":35768},{\"end\":35785,\"start\":35781},{\"end\":35797,\"start\":35791},{\"end\":35810,\"start\":35804},{\"end\":36121,\"start\":36114},{\"end\":36136,\"start\":36130},{\"end\":36152,\"start\":36143},{\"end\":36168,\"start\":36162},{\"end\":36443,\"start\":36435},{\"end\":36453,\"start\":36452},{\"end\":36468,\"start\":36460},{\"end\":36742,\"start\":36734},{\"end\":36753,\"start\":36749},{\"end\":36760,\"start\":36754},{\"end\":36777,\"start\":36774},{\"end\":36788,\"start\":36783},{\"end\":37185,\"start\":37176},{\"end\":37196,\"start\":37192},{\"end\":37210,\"start\":37203},{\"end\":37235,\"start\":37230},{\"end\":37245,\"start\":37240},{\"end\":37258,\"start\":37255},{\"end\":37658,\"start\":37651},{\"end\":37674,\"start\":37668},{\"end\":37900,\"start\":37894},{\"end\":37917,\"start\":37913},{\"end\":37935,\"start\":37928},{\"end\":38251,\"start\":38242},{\"end\":38262,\"start\":38258},{\"end\":38556,\"start\":38549},{\"end\":38568,\"start\":38562},{\"end\":38584,\"start\":38578},{\"end\":38935,\"start\":38931},{\"end\":38947,\"start\":38942},{\"end\":38956,\"start\":38955},{\"end\":38958,\"start\":38957},{\"end\":38968,\"start\":38961},{\"end\":38982,\"start\":38979},{\"end\":39321,\"start\":39314},{\"end\":39335,\"start\":39330},{\"end\":39354,\"start\":39347},{\"end\":39356,\"start\":39355},{\"end\":39661,\"start\":39658},{\"end\":39678,\"start\":39673},{\"end\":39695,\"start\":39689},{\"end\":39948,\"start\":39943},{\"end\":39960,\"start\":39956},{\"end\":40245,\"start\":40241},{\"end\":40256,\"start\":40252},{\"end\":40258,\"start\":40257},{\"end\":40267,\"start\":40266},{\"end\":40284,\"start\":40283},{\"end\":40595,\"start\":40591},{\"end\":40607,\"start\":40604},{\"end\":40622,\"start\":40615},{\"end\":40640,\"start\":40633},{\"end\":40655,\"start\":40649},{\"end\":40669,\"start\":40662},{\"end\":40684,\"start\":40678},{\"end\":40695,\"start\":40690},{\"end\":40711,\"start\":40707},{\"end\":40724,\"start\":40720},{\"end\":40978,\"start\":40977},{\"end\":40994,\"start\":40989},{\"end\":41220,\"start\":41214},{\"end\":41236,\"start\":41229},{\"end\":41253,\"start\":41244},{\"end\":41265,\"start\":41261},{\"end\":41281,\"start\":41275},{\"end\":41300,\"start\":41293},{\"end\":41314,\"start\":41311},{\"end\":41329,\"start\":41323},{\"end\":41341,\"start\":41336},{\"end\":41783,\"start\":41775},{\"end\":41794,\"start\":41790},{\"end\":41804,\"start\":41801},{\"end\":41814,\"start\":41809},{\"end\":41829,\"start\":41821},{\"end\":42171,\"start\":42167},{\"end\":42181,\"start\":42180},{\"end\":42197,\"start\":42190},{\"end\":42199,\"start\":42198},{\"end\":42218,\"start\":42212},{\"end\":42220,\"start\":42219}]", "bib_author_last_name": "[{\"end\":31237,\"start\":31233},{\"end\":31250,\"start\":31247},{\"end\":31271,\"start\":31261},{\"end\":31286,\"start\":31281},{\"end\":31302,\"start\":31296},{\"end\":31319,\"start\":31314},{\"end\":31735,\"start\":31731},{\"end\":31747,\"start\":31745},{\"end\":31756,\"start\":31752},{\"end\":31765,\"start\":31762},{\"end\":31775,\"start\":31770},{\"end\":31792,\"start\":31787},{\"end\":31802,\"start\":31798},{\"end\":32139,\"start\":32134},{\"end\":32158,\"start\":32151},{\"end\":32170,\"start\":32164},{\"end\":32490,\"start\":32487},{\"end\":32504,\"start\":32500},{\"end\":32518,\"start\":32515},{\"end\":32528,\"start\":32524},{\"end\":32902,\"start\":32898},{\"end\":32917,\"start\":32912},{\"end\":32931,\"start\":32924},{\"end\":32945,\"start\":32941},{\"end\":33317,\"start\":33307},{\"end\":33331,\"start\":33326},{\"end\":33348,\"start\":33340},{\"end\":33732,\"start\":33729},{\"end\":33746,\"start\":33743},{\"end\":34093,\"start\":34090},{\"end\":34104,\"start\":34101},{\"end\":34119,\"start\":34114},{\"end\":34483,\"start\":34477},{\"end\":34498,\"start\":34491},{\"end\":34510,\"start\":34507},{\"end\":34522,\"start\":34519},{\"end\":34535,\"start\":34530},{\"end\":34552,\"start\":34547},{\"end\":34973,\"start\":34964},{\"end\":35002,\"start\":34995},{\"end\":35017,\"start\":35011},{\"end\":35316,\"start\":35312},{\"end\":35331,\"start\":35327},{\"end\":35342,\"start\":35338},{\"end\":35351,\"start\":35347},{\"end\":35363,\"start\":35360},{\"end\":35371,\"start\":35369},{\"end\":35779,\"start\":35775},{\"end\":35789,\"start\":35786},{\"end\":35802,\"start\":35798},{\"end\":35814,\"start\":35811},{\"end\":36128,\"start\":36122},{\"end\":36141,\"start\":36137},{\"end\":36160,\"start\":36153},{\"end\":36176,\"start\":36169},{\"end\":36450,\"start\":36444},{\"end\":36458,\"start\":36454},{\"end\":36474,\"start\":36469},{\"end\":36478,\"start\":36476},{\"end\":36747,\"start\":36743},{\"end\":36772,\"start\":36761},{\"end\":36781,\"start\":36778},{\"end\":36794,\"start\":36789},{\"end\":37190,\"start\":37186},{\"end\":37201,\"start\":37197},{\"end\":37228,\"start\":37211},{\"end\":37238,\"start\":37236},{\"end\":37253,\"start\":37246},{\"end\":37263,\"start\":37259},{\"end\":37666,\"start\":37659},{\"end\":37684,\"start\":37675},{\"end\":37911,\"start\":37901},{\"end\":37926,\"start\":37918},{\"end\":37941,\"start\":37936},{\"end\":38256,\"start\":38252},{\"end\":38266,\"start\":38263},{\"end\":38560,\"start\":38557},{\"end\":38576,\"start\":38569},{\"end\":38587,\"start\":38585},{\"end\":38940,\"start\":38936},{\"end\":38953,\"start\":38948},{\"end\":38977,\"start\":38969},{\"end\":38987,\"start\":38983},{\"end\":39328,\"start\":39322},{\"end\":39345,\"start\":39336},{\"end\":39364,\"start\":39357},{\"end\":39671,\"start\":39662},{\"end\":39687,\"start\":39679},{\"end\":39705,\"start\":39696},{\"end\":39954,\"start\":39949},{\"end\":39969,\"start\":39961},{\"end\":40250,\"start\":40246},{\"end\":40264,\"start\":40259},{\"end\":40273,\"start\":40268},{\"end\":40281,\"start\":40275},{\"end\":40289,\"start\":40285},{\"end\":40301,\"start\":40291},{\"end\":40602,\"start\":40596},{\"end\":40613,\"start\":40608},{\"end\":40631,\"start\":40623},{\"end\":40647,\"start\":40641},{\"end\":40660,\"start\":40656},{\"end\":40676,\"start\":40670},{\"end\":40688,\"start\":40685},{\"end\":40705,\"start\":40696},{\"end\":40718,\"start\":40712},{\"end\":40730,\"start\":40725},{\"end\":40987,\"start\":40979},{\"end\":41001,\"start\":40995},{\"end\":41005,\"start\":41003},{\"end\":41227,\"start\":41221},{\"end\":41242,\"start\":41237},{\"end\":41259,\"start\":41254},{\"end\":41273,\"start\":41266},{\"end\":41291,\"start\":41282},{\"end\":41309,\"start\":41301},{\"end\":41321,\"start\":41315},{\"end\":41334,\"start\":41330},{\"end\":41349,\"start\":41342},{\"end\":41788,\"start\":41784},{\"end\":41799,\"start\":41795},{\"end\":41807,\"start\":41805},{\"end\":41819,\"start\":41815},{\"end\":41837,\"start\":41830},{\"end\":42178,\"start\":42172},{\"end\":42188,\"start\":42182},{\"end\":42210,\"start\":42200},{\"end\":42228,\"start\":42221},{\"end\":42240,\"start\":42230}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":35072502},\"end\":31651,\"start\":31143},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":201108075},\"end\":32051,\"start\":31653},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2255738},\"end\":32395,\"start\":32053},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15774646},\"end\":32830,\"start\":32397},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11977588},\"end\":33201,\"start\":32832},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3645349},\"end\":33643,\"start\":33203},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3714620},\"end\":33996,\"start\":33645},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52158437},\"end\":34350,\"start\":33998},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52936213},\"end\":34899,\"start\":34352},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206775100},\"end\":35224,\"start\":34901},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52945284},\"end\":35666,\"start\":35226},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":52001138},\"end\":36070,\"start\":35668},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9455111},\"end\":36388,\"start\":36072},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10748875},\"end\":36673,\"start\":36390},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":21352010},\"end\":37068,\"start\":36675},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4578162},\"end\":37606,\"start\":37070},{\"attributes\":{\"id\":\"b16\"},\"end\":37826,\"start\":37608},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":16790081},\"end\":38199,\"start\":37828},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":49189997},\"end\":38468,\"start\":38201},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":56366093},\"end\":38854,\"start\":38470},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":299085},\"end\":39243,\"start\":38856},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206596513},\"end\":39626,\"start\":39245},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6099034},\"end\":39893,\"start\":39628},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":186689463},\"end\":40165,\"start\":39895},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207761262},\"end\":40551,\"start\":40167},{\"attributes\":{\"id\":\"b25\"},\"end\":40931,\"start\":40553},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b26\"},\"end\":41149,\"start\":40933},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":502946},\"end\":41697,\"start\":41151},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3520623},\"end\":42127,\"start\":41699},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1354186},\"end\":42474,\"start\":42129},{\"attributes\":{\"id\":\"b30\"},\"end\":43697,\"start\":42476}]", "bib_title": "[{\"end\":31222,\"start\":31143},{\"end\":31720,\"start\":31653},{\"end\":32126,\"start\":32053},{\"end\":32479,\"start\":32397},{\"end\":32888,\"start\":32832},{\"end\":33300,\"start\":33203},{\"end\":33719,\"start\":33645},{\"end\":34080,\"start\":33998},{\"end\":34468,\"start\":34352},{\"end\":34957,\"start\":34901},{\"end\":35305,\"start\":35226},{\"end\":35766,\"start\":35668},{\"end\":36112,\"start\":36072},{\"end\":36433,\"start\":36390},{\"end\":36732,\"start\":36675},{\"end\":37174,\"start\":37070},{\"end\":37892,\"start\":37828},{\"end\":38240,\"start\":38201},{\"end\":38547,\"start\":38470},{\"end\":38929,\"start\":38856},{\"end\":39312,\"start\":39245},{\"end\":39656,\"start\":39628},{\"end\":39941,\"start\":39895},{\"end\":40239,\"start\":40167},{\"end\":41212,\"start\":41151},{\"end\":41773,\"start\":41699},{\"end\":42165,\"start\":42129},{\"end\":42988,\"start\":42476}]", "bib_author": "[{\"end\":31239,\"start\":31224},{\"end\":31252,\"start\":31239},{\"end\":31273,\"start\":31252},{\"end\":31288,\"start\":31273},{\"end\":31304,\"start\":31288},{\"end\":31321,\"start\":31304},{\"end\":31737,\"start\":31722},{\"end\":31749,\"start\":31737},{\"end\":31758,\"start\":31749},{\"end\":31767,\"start\":31758},{\"end\":31777,\"start\":31767},{\"end\":31794,\"start\":31777},{\"end\":31804,\"start\":31794},{\"end\":32141,\"start\":32128},{\"end\":32160,\"start\":32141},{\"end\":32172,\"start\":32160},{\"end\":32492,\"start\":32481},{\"end\":32506,\"start\":32492},{\"end\":32520,\"start\":32506},{\"end\":32530,\"start\":32520},{\"end\":32904,\"start\":32890},{\"end\":32919,\"start\":32904},{\"end\":32933,\"start\":32919},{\"end\":32947,\"start\":32933},{\"end\":33319,\"start\":33302},{\"end\":33333,\"start\":33319},{\"end\":33350,\"start\":33333},{\"end\":33734,\"start\":33721},{\"end\":33748,\"start\":33734},{\"end\":34095,\"start\":34082},{\"end\":34106,\"start\":34095},{\"end\":34121,\"start\":34106},{\"end\":34485,\"start\":34470},{\"end\":34500,\"start\":34485},{\"end\":34512,\"start\":34500},{\"end\":34524,\"start\":34512},{\"end\":34537,\"start\":34524},{\"end\":34554,\"start\":34537},{\"end\":34975,\"start\":34959},{\"end\":35004,\"start\":34975},{\"end\":35019,\"start\":35004},{\"end\":35318,\"start\":35307},{\"end\":35333,\"start\":35318},{\"end\":35344,\"start\":35333},{\"end\":35353,\"start\":35344},{\"end\":35365,\"start\":35353},{\"end\":35373,\"start\":35365},{\"end\":35781,\"start\":35768},{\"end\":35791,\"start\":35781},{\"end\":35804,\"start\":35791},{\"end\":35816,\"start\":35804},{\"end\":36130,\"start\":36114},{\"end\":36143,\"start\":36130},{\"end\":36162,\"start\":36143},{\"end\":36178,\"start\":36162},{\"end\":36452,\"start\":36435},{\"end\":36460,\"start\":36452},{\"end\":36476,\"start\":36460},{\"end\":36480,\"start\":36476},{\"end\":36749,\"start\":36734},{\"end\":36774,\"start\":36749},{\"end\":36783,\"start\":36774},{\"end\":36796,\"start\":36783},{\"end\":37192,\"start\":37176},{\"end\":37203,\"start\":37192},{\"end\":37230,\"start\":37203},{\"end\":37240,\"start\":37230},{\"end\":37255,\"start\":37240},{\"end\":37265,\"start\":37255},{\"end\":37668,\"start\":37651},{\"end\":37686,\"start\":37668},{\"end\":37913,\"start\":37894},{\"end\":37928,\"start\":37913},{\"end\":37943,\"start\":37928},{\"end\":38258,\"start\":38242},{\"end\":38268,\"start\":38258},{\"end\":38562,\"start\":38549},{\"end\":38578,\"start\":38562},{\"end\":38589,\"start\":38578},{\"end\":38942,\"start\":38931},{\"end\":38955,\"start\":38942},{\"end\":38961,\"start\":38955},{\"end\":38979,\"start\":38961},{\"end\":38989,\"start\":38979},{\"end\":39330,\"start\":39314},{\"end\":39347,\"start\":39330},{\"end\":39366,\"start\":39347},{\"end\":39673,\"start\":39658},{\"end\":39689,\"start\":39673},{\"end\":39707,\"start\":39689},{\"end\":39956,\"start\":39943},{\"end\":39971,\"start\":39956},{\"end\":40252,\"start\":40241},{\"end\":40266,\"start\":40252},{\"end\":40275,\"start\":40266},{\"end\":40283,\"start\":40275},{\"end\":40291,\"start\":40283},{\"end\":40303,\"start\":40291},{\"end\":40604,\"start\":40591},{\"end\":40615,\"start\":40604},{\"end\":40633,\"start\":40615},{\"end\":40649,\"start\":40633},{\"end\":40662,\"start\":40649},{\"end\":40678,\"start\":40662},{\"end\":40690,\"start\":40678},{\"end\":40707,\"start\":40690},{\"end\":40720,\"start\":40707},{\"end\":40732,\"start\":40720},{\"end\":40989,\"start\":40977},{\"end\":41003,\"start\":40989},{\"end\":41007,\"start\":41003},{\"end\":41229,\"start\":41214},{\"end\":41244,\"start\":41229},{\"end\":41261,\"start\":41244},{\"end\":41275,\"start\":41261},{\"end\":41293,\"start\":41275},{\"end\":41311,\"start\":41293},{\"end\":41323,\"start\":41311},{\"end\":41336,\"start\":41323},{\"end\":41351,\"start\":41336},{\"end\":41790,\"start\":41775},{\"end\":41801,\"start\":41790},{\"end\":41809,\"start\":41801},{\"end\":41821,\"start\":41809},{\"end\":41839,\"start\":41821},{\"end\":42180,\"start\":42167},{\"end\":42190,\"start\":42180},{\"end\":42212,\"start\":42190},{\"end\":42230,\"start\":42212},{\"end\":42242,\"start\":42230}]", "bib_venue": "[{\"end\":31386,\"start\":31321},{\"end\":31844,\"start\":31804},{\"end\":32216,\"start\":32172},{\"end\":32602,\"start\":32530},{\"end\":33005,\"start\":32947},{\"end\":33415,\"start\":33350},{\"end\":33813,\"start\":33748},{\"end\":34166,\"start\":34121},{\"end\":34619,\"start\":34554},{\"end\":35048,\"start\":35019},{\"end\":35438,\"start\":35373},{\"end\":35861,\"start\":35816},{\"end\":36220,\"start\":36178},{\"end\":36524,\"start\":36480},{\"end\":36861,\"start\":36796},{\"end\":37330,\"start\":37265},{\"end\":37649,\"start\":37608},{\"end\":38001,\"start\":37943},{\"end\":38327,\"start\":38268},{\"end\":38654,\"start\":38589},{\"end\":39034,\"start\":38989},{\"end\":39424,\"start\":39366},{\"end\":39751,\"start\":39707},{\"end\":40018,\"start\":39971},{\"end\":40346,\"start\":40303},{\"end\":40589,\"start\":40553},{\"end\":40975,\"start\":40933},{\"end\":41416,\"start\":41351},{\"end\":41904,\"start\":41839},{\"end\":42285,\"start\":42242},{\"end\":43048,\"start\":42990}]"}}}, "year": 2023, "month": 12, "day": 17}