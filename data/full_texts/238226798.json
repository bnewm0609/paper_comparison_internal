{"id": 238226798, "updated": "2023-10-05 21:50:33.595", "metadata": {"title": "Fine-tuning wav2vec2 for speaker recognition", "authors": "[{\"first\":\"Nik\",\"last\":\"Vaessen\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Leeuwen\",\"middle\":[\"A.\",\"van\"]}]", "venue": "ArXiv", "journal": "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "This paper explores applying the wav2vec2 framework to speaker recognition instead of speech recognition. We study the effectiveness of the pre-trained weights on the speaker recognition task, and how to pool the wav2vec2 output sequence into a fixed-length speaker embedding. To adapt the framework to speaker recognition, we propose a single-utterance classification variant with CE or AAM softmax loss, and an utterance-pair classification variant with BCE loss. Our best performing variant, w2v2-aam, achieves a 1.88% EER on the extended voxceleb1 test set compared to 1.69% EER with an ECAPA-TDNN baseline. Code is available at https://github.com/nikvaessen/w2v2-speaker.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2109.15053", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icassp/VaessenL22", "doi": "10.1109/icassp43922.2022.9746952"}}, "content": {"source": {"pdf_hash": "8b939af3ef6ab8e0b17d2c671a499891c0e72f4e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2109.15053v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://ieeexplore.ieee.org/ielx7/9745891/9746004/09746952.pdf", "status": "BRONZE"}}, "grobid": {"id": "d7ca80cc58e4b1c97a9d6cd78601d8954db23ffc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8b939af3ef6ab8e0b17d2c671a499891c0e72f4e.txt", "contents": "\nFINE-TUNING WAV2VEC2 FOR SPEAKER RECOGNITION\n\n\nNik Vaessen nvaessen@science.ru.nl \nInstitute for Computing and Information Sciences\nRadboud University\nNijmegenThe Netherlands\n\nDavid A Van Leeuwen dvanleeuwen@science.ru.nl \nInstitute for Computing and Information Sciences\nRadboud University\nNijmegenThe Netherlands\n\nFINE-TUNING WAV2VEC2 FOR SPEAKER RECOGNITION\nIndex Terms-speaker recognitionwav2vec2transfer learning\nThis paper explores applying the wav2vec2 framework to speaker recognition instead of speech recognition. We study the effectiveness of the pre-trained weights on the speaker recognition task, and how to pool the wav2vec2 output sequence into a fixed-length speaker embedding. To adapt the framework to speaker recognition, we propose a singleutterance classification variant with cross-entropy or additive angular softmax loss, and an utterance-pair classification variant with BCE loss. Our best performing variant achieves a 1.88% EER on the extended voxceleb1 test set compared to 1.69% EER with an ECAPA-TDNN baseline. Code is available at github.com/nikvaessen/w2v2-speaker.\n\nINTRODUCTION\n\nIn the field of natural language processing (NLP) it has become standard to fine-tune self-supervised pre-trained models, such as BERT [1], XLNet [2], and T5 [3], on a wide variety of NLP tasks. Recently, this framework of pre-training and fine-tuning has also been successfully used in automatic speech recognition with wav2vec2 [4]. The aim of this study is to explore the feasibility of fine-tuning the wav2vec2 pretrained network on a different task than speech recognition, namely speaker recognition.\n\nThe BERT and wav2vec2 network have commonalities in their design. Both have stacks of transformer layers and they use self-supervised, contrastive pre-training with masked input. However, they differ in three major aspects: 1) the input tokens to the encoder in wav2vec2 are raw audio processed by a CNN instead of WordPiece embeddings, 2) wav2vec2 uses relative positional embeddings computed by a CNN instead of sinusoidal positional embeddings, 3) there is no class token and equivalent next-sentence prediction task in the pretraining procedure of wav2vec2.\n\nThe class token in BERT is used when fine-tuning on sentence-pair classification tasks, e.g., entailment, and for single-sentence classification tasks, e.g., sentiment analysis. It is not used for single sentence tagging tasks, e.g., named entity recognition. In this respect, speech recognition is analogous to sentence tagging where each output token can represent a phone or letter. Hence, speech recognition does not require a class token. In contrast, speaker recognition corresponds to either sentence-pair classification, or singlesentence classification, and might therefore benefit from a class token which summarizes the whole input sequence.\n\nOur work focuses on the following questions. First and foremost, we want to find out whether the pre-trained wav2vec2 weights are an effective initialization for speaker recognition. We further want to explore if wav2vec2 can be adapted to speaker recognition without a class token and next-sentence prediction task. One solution is to pool the variable-length sequence of wav2vec2 embeddings into a fixed-size speaker embedding. This raises the questions if pooling is an effective replacement for a class token, and if so, which pooling method is most suitable.\n\n\nRELATED WORK\n\nWav2vec2 has already been applied to a variety of speechrelated tasks. In [5] the network is used for speaker recognition and language identification in both a single and multitask learning setting. The authors of [6] show good performance for language identification with 25 languages but modify wav2vec2 to use log-mel spectogram input instead of raw waveforms. In [7] the (frozen) wav2vec2 embeddings are input to a learnable downstream model for carrying out emotion recognition. Meanwhile [8] manages to fine-tune the wav2vec2 model itself on emotion recognition with CTC loss by using emotion-labeled phonetic units. The LeBenchmark [9] uses the wav2vec2 models as a baseline and encapsulates speech recognition, spoken language understanding, emotion recognition and speech translation in a single benchmark.\n\n\nMETHODOLOGY\n\n\nThe wav2vec2 architecture\n\nThe wav2vec2 framework [4] applies the concept of selfsupervised pre-training with transformers to automatic speech recognition. In Fig. 1 we show a general overview of the network architecture during fine-tuning. The next subsections summarize each component. \n\n\nFeature extraction\n\nThe first step is to encode a raw audio waveform (normalized to zero mean and unit variance) into learned representations with a discrete time unit. The feature extractor consists of 7 consecutive 1-dimensional convolutions with 512 channels and respective kernel sizes of (10, 3, 3, 3, 3, 2, 2) and stride (5, 2, 2, 2, 2, 2, 2). The output of the first convolutional layer is group normalized [10] such that each of the 512 channel sequences has zero mean and unit variance before GELU activation [11] is applied. The other convolutional layers do not have any normalization layers and their output is directly activated with GELU. The output of the feature extractor is an encoded vector sequence with dimensionality 512. Each vector has a receptive field of 20 ms which is similar to the window sizes in spectral-based representations.\n\n\nProjection, SpecAugment & positional embedding\n\nAfter the feature extraction each encoded vector representation in the sequence is independently normalized to zero mean and unit variance and projected into 768 dimensions by a single, shared fully-connected layer called the feature projector. On all projections dropout is applied (but no activation). Then, masking is applied over the whole sequence analogous to SpecAugment [12]; 0 or more random sets of consecutive vectors (masking in time domain) as well as 0 or more random sets of consecutive channels (masking in \"frequency\" domain) have their values blanked to 0. This masked projected sequence is then convolved by a single layer with a kernel size of 128, a stride of 1, padding of 64 and 16 groups followed by GELU activation in order to create a relative po-sitional embedding for each projected representation. This relative positional embedding is summed with the original input of the convolution, which changes the receptive field from 20 ms to 2.5 s. As a final step each vector is independently normalized with LayerNorm and dropout is applied again.\n\n\nTransformer\n\nThe masked and projected sequence with both local and positional information is fed through an encoder with 12 consecutive transformer layers. Each transformer layer consists of a residual 12-headed self-attention module and a residual 2-layer feed forward network with respectively 3072 and 768 units. LayerDrop [13,14] is applied such that each transformer layer is potentially skipped. The final output sequence, with each representation potentially having both local and global information due to self-attention, is used in a downstream task.\n\n\nThree wav2vec2 variants for speaker recognition\n\nThe original wav2vec2 framework fine-tunes on speech recognition by independently labeling each wav2vec2 output embedding with a shared fully-connected layer, and optimizes with CTC loss [15]. We propose two adaptions to this design for the speaker recognition task, inspired by BERTs [1] single-sentence and sentence-pair classification setup.\n\n\nSpeaker recognition as single-utterance classification\n\nThe current paradigm in speaker recognition with deep neural networks is to train models with a classification-based approach [16,17]. To mimic this architectural paradigm two modifications to wav2vec2 are made. First, the sequence of wav2vec2 embeddings is reduced to a single embedding during training (see subsection 3.3). Secondly, we add a fully connected layer which uses the pooled embedding to classify each speaker in the training data with cross-entropy (CE) or angular additive softmax (AAM) [18,19] loss. A trial is evaluated with the cosine similarity between two pooled embeddings. We refer to these variants as w2v2-ce and w2v2-aam.\n\n\nSpeaker recognition as utterance-pair classification\n\nThe second approach directly computes a similarity score. The two audio segments of a speaker recognition trial are first processed independently up to the encoder part of the network. Then, the two sequences of input tokens are concatenated, accompanied by special tokens at the embedding level: A start (all +1), separator (all \u22121), and end (all \u22121) token. The first wav2vec2 embedding in the output sequence (corresponding to the start token) is used as input to a logistic regression with one dense layer. The singular output is the logit for the BCE loss and the score for evaluating a trial.\n\nDuring training a batch consists of 8 speakers, 4 utterances per speaker, and 16 same and different speaker-pairs. We refer to this third variant as w2v2-bce.\n\n\nPooling methods\n\nWe propose several pooling methods to reduce the variablelength sequence of wav2vec2 embeddings to a fixed-size speaker embedding. We first consider the standard statistical pooling methods mean, max, mean&std and quantile. They aggregate each dimension over the time axis. The mean&std variant doubles the embedding dimensionality while quantile pooling expands each dimension five-fold with quantiles (0, 0.25, 0.5, 0.75, 1). We also assess taking the first, middle or last embedding of the sequence as a \"pooling\" strategy as well as randomly selecting the index. Lastly we consider inserting a \"start\" token (all values +1) before the input sequence of the encoder and then selecting the first output token as the speaker embedding. This is termed first&cls. Unlike BERT our \"start\" token token does not have a meaningful prior.\n\n\nEXPERIMENTS\n\n\nData\n\nAll experiments are conducted on the VoxCeleb datasets [20,21], which consist of interviews of celebrities extracted from YouTube. The VoxCeleb2 dev set, which contains 1.1 M audio files over 6 k speakers, is used for training. A validation set is created based on 2 % of the dev set data which includes all speakers but does not overlap in recordings. For this validation set a random trial set of 5 k same and 5 k different pairs is generated, from which we compute the validation EER. This is used to select the best checkpoint during a training run as well as to tune hyperparameters. For evaluation we use the cleaned original (vox1-o, 40 speakers, 37 k trials), extended (vox1-e, 1251 speakers, 580 k trials) and hard (vox1-h, 1190 speakers, 550 k trials, equal nationality and sex) test sets from voxceleb 1 [21]. There is no speaker overlap between the voxceleb1 test sets and the voxceleb2 dev set. The experiments with the wav2vec2 network use the pretrained weights 1 on Librispeech [22] released on Hugging-Face [23] by Fairseq [24].\n\n\nComputational budget and fair comparison\n\nWe compare the performances of models under similar computational budgets. Each network is trained with a batch size of 3.2M audio samples [4] by randomly sampling 3 seconds from 66 different audio files. No data augmentation techniques are used. We train for 100k iterations, approximately 6 epochs, with Adam [25] and a OneCycle learning rate schedule [26]. The maximum learning rate (LR) of the cycle is  [27] with 5k iterations and 2) tuning on a 7-sized grid centered around the LR with the steepest slope and bounded by the LRs where the loss respectively started and stopped decreasing. Models are evaluated with a cosine score between speaker embeddings lacking any further post-processing, except for the wav2vec2-bce variant which computes scores directly.\n\n\nBaseline systems\n\nWe train two popular baselines models for speaker recognition, x-vector [16,28] and ECAPA-TDNN [17,28], and compare them to the three wav2vec2 adaptions w2v2-ce, w2v2-aam and w2v2-bce. All five models have the computation budget described in subsection 4.2. The X-vector and ECAPA-TDNN networks use 40-dimensional filterbanks as input. The w2v2-ce and w2v2-aam variants use mean&std pooling which was chosen as it is also used in the x-vector network architecture. The w2v2-aam variant and ECAPA-TDNN use the AAM softmax loss with a scale of 30 and a margin of 0.2 in this and further experiments. The feature encoder part of the wav2vec2 architecture is frozen for the whole training procedure which corresponds to [4].\n\n\nVariation in pooling\n\nNext we explore the different poolings methods proposed in Section 3.3. This was carried out for the single-utterance classification variants: w2v2-ce and w2v2-aam. We use the same training settings as in the baseline comparison and only vary the pooling method. Note therefore that the learning rate was tuned to the \"mean&std\" setup.\n\n\nAblation study\n\nWe perform several ablations on the best-performing wav2vec2 variant for speaker recognition. These ablations are trained with 100k iterations except in the experiments for the batch size. The first set of ablations study the effect of not freezing the feature extractor in the fine-tuning procedure as well as randomly initialising the whole network and thus not using any pre-trained weights. The second set of ablations explore the relative importance of the regularisation techniques in the network architecture. We first disable only LayerDrop, then sequentially disable dropout and the masking of certain frames as well. The third set simply studies the effect of increasing (with 50k iterations) or decreasing (with 200k iterations) the chosen batch size by a factor of 2. The last ablations involve the learning rate schedule. We first test two constant learning rates: 3 \u00d7 10 \u22126 is the LR where the loss started increasing in the LR range test and 10 \u22125 is the LR with the steepest decrease. The second schedule exponentially decays the LR from 10 \u22125 to 3 \u00d7 10 \u22126 . The final schedule is the tri-stage learning rate schedule similar to [4] which includes a warm-up phase linearly increasing the LR from 10 \u22127 to 10 \u22125 in 10k iterations, a constant phase of 40k iterations, and an exponentially decreasing stage from 10 \u22125 to 10 \u22127 for the remaining iterations.\n\n\nRESULTS\n\n\nBaseline comparison\n\nThe LR range test and grid-tuning approach found the following learning rates: 10 \u22124 for x-vector, 10 \u22123 for ECAPA-TDNN, 9 \u00d7 10 \u22125 for w2v2-ce, 5 \u00d7 10 \u22125 for w2v2-aam, and 3 \u00d7 10 \u22125 for w2v2-bce. Table 1 shows four runs with these learning rates. We see that ECAPA-TDNN performs best on all test sets. The w2v2-aam network is the best performing wav2vec2 variant. Both w2v2-ce and w2v2-aam manage to improve on the x-vector architecture. Modeling speaker recognition as utterance-pair classification (w2v2-bce) performed worst. Table 2 compares the 9 different pooling strategies with the w2v2-ce and w2v2-aam networks. We observe that first&cls pooling performs best for both networks. The difference between first&cls pooling and the other pooling methods is more pronounced for w2v2-aam than for w2v2-ce. The low inter-model variance of random pooling shows that each wav2vec2 embedding is a stand-alone speaker embedding. Not shown, using random pooling in the evaluation for networks which were trained with first&cls pooling degrades the EER with 0.2% points. Moreover, creating ensembles out of different embeddings in the output sequence did not improve performance. This suggests that the transformer layer processes each embedding in the sequence similarly. Table 3 shows the results of the ablation study. We see that freezing the feature extractor leads to worse performance but is more stable across runs with different seeds. We also note a large degradation in performance when initializing with random weights instead of the pre-trained weights. The regularisation settings for fine-tuning on speech recognition are also beneficial for fine-tuning on speaker recognition. Increasing the batch size beyond 3.2M audio samples does not increase performance, although it does decrease the variance slightly. Using a learning rate schedule with a warm-up phase, such as the tri-stage or OneCycle schedule, is critical for stable and good performance.\n\n\nPooling methods\n\n\nAblation study\n\n\nCONCLUSION AND FUTURE WORK\n\nWe have shown that the wav2vec2 framework can be successfully adapted to the speaker recognition task and that the pretrained weights used for fine-tuning on speech recognition are also useful for fine-tuning on speaker recognition. Good results with first&cls pooling indicate that including a class token in the pre-training procedure is promising future work.\n\nModeling speaker recognition as a paired-utterance classification problem did not perform well. Future work in that direction might involve limiting the attention mechanism to the opposite utterance to simplify the learning task.\n\nTable 1 .\n1EER performance of standard filterbank-based speaker recognition networks as well as the fine-tuned wav2vec2 variations. We ran N = 4 runs to compute the standard deviations. Bold indicates best performance.EER performance (mean, std in %) \nNETWORK \nvox1-o \nvox1-e \nvox1-h \nx-vector [16] \n5.22 0.12 \n5.60 0.05 \n8.75 \n0.05 \nECAPA-TDNN [17] \n1.61 0.03 \n1.69 0.03 \n3.10 \n0.05 \nw2v2-ce \n2.25 0.20 \n2.58 0.10 \n4.91 \n0.13 \nw2v2-aam \n1.91 0.12 \n2.22 0.04 \n4.33 \n0.08 \nw2v2-bce \n7.28 0.22 \n7.19 0.22 \n11.34 0.83 \n\nfound by 1) performing an LR range test \n\nTable 2 .\n2The performance of different pooling strategies for the sin-gle utterance classification architectures on the extended voxceleb1 \ntest set. Each method was trained with N = 3 random seeds. The \nevaluation of random pooling was repeated four times for each run. \nEER on vox1-e (mean, std in %) \npooling \nw2v2-ce \nw2v2-aam \nmax \n4.79 0.55 \n2.27 0.04 \nquantile \n2.75 0.17 \n2.21 0.03 \nmean \n2.69 0.06 \n2.11 0.05 \nmean&std \n2.60 0.08 \n2.18 0.03 \nfirst \n2.61 0.10 \n2.15 0.05 \nfirst&cls \n2.52 0.11 \n2.06 0.03 \nmiddle \n2.55 0.07 \n2.18 0.03 \nlast \n2.58 0.07 \n2.18 0.03 \nrandom (N = 1) \n2.56 0.002 \n2.40 0.002 \nrandom (N = 3) \n2.70 0.13 \n2.37 0.03 \n\n\n\nTable 3 .\n3EER performance under varying ablated configurations for the w2v2-aam network variant. Each configuration was run with N = 3 random seeds. One run with exponential decay diverged and we therefore show N = 2 results for that row.EER on vox-e \n\nSee https://huggingface.co/facebook/wav2vec2-base\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, \"BERT: Pre-training of deep bidirectional trans- formers for language understanding,\" in Proceedings of the 2019 Conference of the North American Chapter of the Associ- ation for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers). June 2019, pp. 4171-4186, Association for Computational Linguistics.\n\nXLNet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov, Le, Advances in neural information processing systems. 32Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le, \"XLNet: General- ized autoregressive pretraining for language understanding,\" Advances in neural information processing systems, vol. 32, 2019.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Pe- ter J. Liu, \"Exploring the limits of transfer learning with a unified text-to-text transformer,\" Journal of Machine Learning Research, vol. 21, no. 140, pp. 1-67, 2020.\n\nwav2vec 2.0: A framework for self-supervised learning of speech representations. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, Michael Auli, Advances in Neural Information Processing Systems. 33Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli, \"wav2vec 2.0: A framework for self-supervised learning of speech representations,\" in Advances in Neural Information Processing Systems, 2020, vol. 33, pp. 12449- 12460.\n\nExploring wav2vec 2.0 on Speaker Verification and Language Identification. Zhiyun Fan, Meng Li, Shiyu Zhou, Bo Xu, Proc. Interspeech. InterspeechZhiyun Fan, Meng Li, Shiyu Zhou, and Bo Xu, \"Exploring wav2vec 2.0 on Speaker Verification and Language Identifica- tion,\" in Proc. Interspeech 2021, 2021, pp. 1509-1513.\n\nImproved language identification through cross-lingual self-supervised learning. Andros Tjandra, Diptanu Gon, Frank Choudhury, Kritika Zhang, Alexei Singh, Assaf Baevski, Yatharth Sela, Michael Saraf, Auli, arXiv:2107.04082arXiv preprintAndros Tjandra, Diptanu Gon Choudhury, Frank Zhang, Kritika Singh, Alexei Baevski, Assaf Sela, Yatharth Saraf, and Michael Auli, \"Improved language identification through cross-lingual self-supervised learning,\" arXiv preprint arXiv:2107.04082, 2021.\n\nEmotion Recognition from Speech Using wav2vec 2.0 Embeddings. Leonardo Pepino, Pablo Riera, Luciana Ferrer, Proc. Interspeech. InterspeechLeonardo Pepino, Pablo Riera, and Luciana Ferrer, \"Emotion Recognition from Speech Using wav2vec 2.0 Embeddings,\" in Proc. Interspeech 2021, 2021, pp. 3400-3404.\n\nThe role of phonetic units in speech emotion recognition. Jiahong Yuan, Xingyu Cai, Renjie Zheng, Liang Huang, Kenneth Church, arXiv:2108.01132arXiv preprintJiahong Yuan, Xingyu Cai, Renjie Zheng, Liang Huang, and Kenneth Church, \"The role of phonetic units in speech emotion recognition,\" arXiv preprint arXiv:2108.01132, 2021.\n\nLeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech. Sol\u00e8ne Evain, Proc. Interspeech. InterspeechSol\u00e8ne Evain et al., \" LeBenchmark: A Reproducible Frame- work for Assessing Self-Supervised Representation Learning from Speech,\" in Proc. Interspeech 2021, 2021, pp. 1439- 1443.\n\nGroup normalization. Yuxin Wu, Kaiming He, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Yuxin Wu and Kaiming He, \"Group normalization,\" in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 3-19.\n\nDan Hendrycks, Kevin Gimpel, arXiv:1606.08415Gaussian error linear units (GELUs). arXiv preprintDan Hendrycks and Kevin Gimpel, \"Gaussian error linear units (GELUs),\" arXiv preprint arXiv:1606.08415, 2016.\n\nSpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, Quoc V Le, Proc. Interspeech. InterspeechDaniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le, \"SpecAug- ment: A Simple Data Augmentation Method for Automatic Speech Recognition,\" in Proc. Interspeech 2019, 2019, pp. 2613-2617.\n\nDeep networks with stochastic depth. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Q Weinberger, SpringerGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger, \"Deep networks with stochastic depth,\" in Euro- pean conference on computer vision. Springer, 2016, pp. 646- 661.\n\nReducing transformer depth on demand with structured dropout. Angela Fan, Edouard Grave, Armand Joulin, arXiv:1909.11556arXiv preprintAngela Fan, Edouard Grave, and Armand Joulin, \"Reducing transformer depth on demand with structured dropout,\" arXiv preprint arXiv:1909.11556, 2019.\n\nConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, J\u00fcrgen Schmidhuber, Proceedings of the 23rd international conference on Machine learning. the 23rd international conference on Machine learningAlex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber, \"Connectionist temporal classification: la- belling unsegmented sequence data with recurrent neural net- works,\" in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369-376.\n\nX-vectors: Robust DNN embeddings for speaker recognition. David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, Sanjeev Khudanpur, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur, \"X-vectors: Robust DNN embeddings for speaker recognition,\" in 2018 IEEE Interna- tional Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5329-5333.\n\nECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification. Brecht Desplanques, Jenthe Thienpondt, Kris Demuynck, Proc. Interspeech. InterspeechBrecht Desplanques, Jenthe Thienpondt, and Kris Demuynck, \"ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification,\" in Proc. Interspeech 2020, 2020, pp. 3830-3834.\n\nArcface: Additive angular margin loss for deep face recognition. Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou, \"Arcface: Additive angular margin loss for deep face recogni- tion,\" in Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR), June 2019.\n\nLarge Margin Softmax Loss for Speaker Verification. Yi Liu, Liang He, Jia Liu, Proc. Interspeech. InterspeechYi Liu, Liang He, and Jia Liu, \"Large Margin Softmax Loss for Speaker Verification,\" in Proc. Interspeech 2019, 2019, pp. 2873-2877.\n\nVoxCeleb: A Large-Scale Speaker Identification Dataset. Arsha Nagrani, Joon Son Chung, Andrew Zisserman, Proc. Interspeech. InterspeechArsha Nagrani, Joon Son Chung, and Andrew Zisserman, \"VoxCeleb: A Large-Scale Speaker Identification Dataset,\" in Proc. Interspeech 2017, 2017, pp. 2616-2620.\n\nVoxCeleb2: Deep Speaker Recognition. Arsha Joon Son Chung, Andrew Nagrani, Zisserman, Proc. Interspeech. InterspeechJoon Son Chung, Arsha Nagrani, and Andrew Zisserman, \"VoxCeleb2: Deep Speaker Recognition,\" in Proc. Interspeech 2018, 2018, pp. 1086-1090.\n\nLibrispeech: an ASR corpus based on public domain audio books. Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur, 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEEVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, \"Librispeech: an ASR corpus based on public domain audio books,\" in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206-5210.\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsThomas Wolf et al., \"Transformers: State-of-the-art natural language processing,\" in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Sys- tem Demonstrations, Online, Oct. 2020, pp. 38-45, Associa- tion for Computational Linguistics.\n\nfairseq: A fast, extensible toolkit for sequence modeling. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli, Proceedings of NAACL-HLT 2019: Demonstrations. NAACL-HLT 2019: DemonstrationsMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli, \"fairseq: A fast, extensible toolkit for sequence modeling,\" in Proceed- ings of NAACL-HLT 2019: Demonstrations, 2019.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsDiederik P. Kingma and Jimmy Ba, \"Adam: A method for stochastic optimization,\" in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\nSuper-convergence: Very fast training of neural networks using large learning rates. N Leslie, Nicholay Smith, Topin, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications. International Society for Optics and Photonics. 110061100612Leslie N Smith and Nicholay Topin, \"Super-convergence: Very fast training of neural networks using large learning rates,\" in Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications. International Society for Optics and Photonics, 2019, vol. 11006, p. 1100612.\n\nCyclical learning rates for training neural networks. N Leslie, Smith, 2017 IEEE winter conference on applications of computer vision (WACV). IEEELeslie N Smith, \"Cyclical learning rates for training neural networks,\" in 2017 IEEE winter conference on applications of computer vision (WACV). IEEE, 2017, pp. 464-472.\n\nSpeechbrain: A general-purpose speech toolkit. Mirco Ravanelli, Mirco Ravanelli et al., \"Speechbrain: A general-purpose speech toolkit,\" 2021.\n", "annotations": {"author": "[{\"end\":176,\"start\":48},{\"end\":316,\"start\":177}]", "publisher": null, "author_last_name": "[{\"end\":59,\"start\":52},{\"end\":196,\"start\":185}]", "author_first_name": "[{\"end\":51,\"start\":48},{\"end\":182,\"start\":177},{\"end\":184,\"start\":183}]", "author_affiliation": "[{\"end\":175,\"start\":84},{\"end\":315,\"start\":224}]", "title": "[{\"end\":45,\"start\":1},{\"end\":361,\"start\":317}]", "venue": null, "abstract": "[{\"end\":1099,\"start\":419}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1253,\"start\":1250},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1264,\"start\":1261},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1276,\"start\":1273},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1448,\"start\":1445},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2073,\"start\":2071},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3497,\"start\":3494},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3637,\"start\":3634},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3790,\"start\":3787},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3917,\"start\":3914},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4062,\"start\":4059},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4305,\"start\":4302},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4961,\"start\":4957},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5065,\"start\":5061},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5834,\"start\":5830},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6856,\"start\":6852},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6859,\"start\":6856},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7328,\"start\":7324},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7425,\"start\":7422},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7670,\"start\":7666},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7673,\"start\":7670},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8047,\"start\":8043},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8050,\"start\":8047},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9935,\"start\":9931},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9938,\"start\":9935},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10695,\"start\":10691},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10874,\"start\":10870},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10904,\"start\":10900},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10920,\"start\":10916},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11108,\"start\":11105},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11281,\"start\":11277},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11324,\"start\":11320},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11378,\"start\":11374},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11829,\"start\":11825},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11832,\"start\":11829},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11852,\"start\":11848},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11855,\"start\":11852},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12472,\"start\":12469},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14000,\"start\":13997}]", "figure": "[{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":17434,\"start\":16876},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":18087,\"start\":17435},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":18342,\"start\":18088}]", "paragraph": "[{\"end\":1621,\"start\":1115},{\"end\":2184,\"start\":1623},{\"end\":2838,\"start\":2186},{\"end\":3403,\"start\":2840},{\"end\":4235,\"start\":3420},{\"end\":4540,\"start\":4279},{\"end\":5401,\"start\":4563},{\"end\":6523,\"start\":5452},{\"end\":7085,\"start\":6539},{\"end\":7481,\"start\":7137},{\"end\":8187,\"start\":7540},{\"end\":8841,\"start\":8244},{\"end\":9001,\"start\":8843},{\"end\":9853,\"start\":9021},{\"end\":10921,\"start\":9876},{\"end\":11732,\"start\":10966},{\"end\":12473,\"start\":11753},{\"end\":12833,\"start\":12498},{\"end\":14221,\"start\":12852},{\"end\":16216,\"start\":14255},{\"end\":16644,\"start\":16282},{\"end\":16875,\"start\":16646}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14458,\"start\":14451},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14790,\"start\":14783},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15530,\"start\":15523}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1113,\"start\":1101},{\"attributes\":{\"n\":\"2.\"},\"end\":3418,\"start\":3406},{\"attributes\":{\"n\":\"3.\"},\"end\":4249,\"start\":4238},{\"attributes\":{\"n\":\"3.1.\"},\"end\":4277,\"start\":4252},{\"attributes\":{\"n\":\"3.1.1.\"},\"end\":4561,\"start\":4543},{\"attributes\":{\"n\":\"3.1.2.\"},\"end\":5450,\"start\":5404},{\"attributes\":{\"n\":\"3.1.3.\"},\"end\":6537,\"start\":6526},{\"attributes\":{\"n\":\"3.2.\"},\"end\":7135,\"start\":7088},{\"attributes\":{\"n\":\"3.2.1.\"},\"end\":7538,\"start\":7484},{\"attributes\":{\"n\":\"3.2.2.\"},\"end\":8242,\"start\":8190},{\"attributes\":{\"n\":\"3.3.\"},\"end\":9019,\"start\":9004},{\"attributes\":{\"n\":\"4.\"},\"end\":9867,\"start\":9856},{\"attributes\":{\"n\":\"4.1.\"},\"end\":9874,\"start\":9870},{\"attributes\":{\"n\":\"4.2.\"},\"end\":10964,\"start\":10924},{\"attributes\":{\"n\":\"4.3.\"},\"end\":11751,\"start\":11735},{\"attributes\":{\"n\":\"4.4.\"},\"end\":12496,\"start\":12476},{\"attributes\":{\"n\":\"4.5.\"},\"end\":12850,\"start\":12836},{\"attributes\":{\"n\":\"5.\"},\"end\":14231,\"start\":14224},{\"attributes\":{\"n\":\"5.1.\"},\"end\":14253,\"start\":14234},{\"attributes\":{\"n\":\"5.2.\"},\"end\":16234,\"start\":16219},{\"attributes\":{\"n\":\"5.3.\"},\"end\":16251,\"start\":16237},{\"attributes\":{\"n\":\"6.\"},\"end\":16280,\"start\":16254},{\"end\":16886,\"start\":16877},{\"end\":17445,\"start\":17436},{\"end\":18098,\"start\":18089}]", "table": "[{\"end\":17434,\"start\":17095},{\"end\":18087,\"start\":17507},{\"end\":18342,\"start\":18328}]", "figure_caption": "[{\"end\":17095,\"start\":16888},{\"end\":17507,\"start\":17447},{\"end\":18328,\"start\":18100}]", "figure_ref": "[{\"end\":4417,\"start\":4411}]", "bib_author_first_name": "[{\"end\":18481,\"start\":18476},{\"end\":18498,\"start\":18490},{\"end\":18512,\"start\":18506},{\"end\":18526,\"start\":18518},{\"end\":19338,\"start\":19332},{\"end\":19351,\"start\":19345},{\"end\":19363,\"start\":19357},{\"end\":19375,\"start\":19370},{\"end\":19388,\"start\":19387},{\"end\":19401,\"start\":19395},{\"end\":19799,\"start\":19794},{\"end\":19812,\"start\":19808},{\"end\":19826,\"start\":19822},{\"end\":19845,\"start\":19836},{\"end\":19857,\"start\":19851},{\"end\":19873,\"start\":19866},{\"end\":19887,\"start\":19882},{\"end\":19897,\"start\":19894},{\"end\":19907,\"start\":19902},{\"end\":19909,\"start\":19908},{\"end\":20331,\"start\":20325},{\"end\":20346,\"start\":20341},{\"end\":20364,\"start\":20353},{\"end\":20381,\"start\":20374},{\"end\":20760,\"start\":20754},{\"end\":20770,\"start\":20766},{\"end\":20780,\"start\":20775},{\"end\":20789,\"start\":20787},{\"end\":21083,\"start\":21077},{\"end\":21111,\"start\":21106},{\"end\":21130,\"start\":21123},{\"end\":21144,\"start\":21138},{\"end\":21157,\"start\":21152},{\"end\":21175,\"start\":21167},{\"end\":21189,\"start\":21182},{\"end\":21555,\"start\":21547},{\"end\":21569,\"start\":21564},{\"end\":21584,\"start\":21577},{\"end\":21851,\"start\":21844},{\"end\":21864,\"start\":21858},{\"end\":21876,\"start\":21870},{\"end\":21889,\"start\":21884},{\"end\":21904,\"start\":21897},{\"end\":22227,\"start\":22221},{\"end\":22472,\"start\":22467},{\"end\":22484,\"start\":22477},{\"end\":22741,\"start\":22738},{\"end\":22758,\"start\":22753},{\"end\":23032,\"start\":23026},{\"end\":23034,\"start\":23033},{\"end\":23048,\"start\":23041},{\"end\":23057,\"start\":23055},{\"end\":23076,\"start\":23065},{\"end\":23089,\"start\":23083},{\"end\":23100,\"start\":23096},{\"end\":23102,\"start\":23101},{\"end\":23114,\"start\":23110},{\"end\":23116,\"start\":23115},{\"end\":23427,\"start\":23424},{\"end\":23437,\"start\":23435},{\"end\":23449,\"start\":23443},{\"end\":23461,\"start\":23455},{\"end\":23477,\"start\":23469},{\"end\":23751,\"start\":23745},{\"end\":23764,\"start\":23757},{\"end\":23778,\"start\":23772},{\"end\":24078,\"start\":24074},{\"end\":24095,\"start\":24087},{\"end\":24115,\"start\":24107},{\"end\":24129,\"start\":24123},{\"end\":24608,\"start\":24603},{\"end\":24623,\"start\":24617},{\"end\":24646,\"start\":24639},{\"end\":24659,\"start\":24653},{\"end\":24674,\"start\":24667},{\"end\":25145,\"start\":25139},{\"end\":25165,\"start\":25159},{\"end\":25182,\"start\":25178},{\"end\":25511,\"start\":25503},{\"end\":25521,\"start\":25518},{\"end\":25534,\"start\":25527},{\"end\":25548,\"start\":25540},{\"end\":26014,\"start\":26012},{\"end\":26025,\"start\":26020},{\"end\":26033,\"start\":26030},{\"end\":26264,\"start\":26259},{\"end\":26278,\"start\":26274},{\"end\":26282,\"start\":26279},{\"end\":26296,\"start\":26290},{\"end\":26540,\"start\":26535},{\"end\":26563,\"start\":26557},{\"end\":26824,\"start\":26818},{\"end\":26842,\"start\":26836},{\"end\":26855,\"start\":26849},{\"end\":26870,\"start\":26863},{\"end\":27292,\"start\":27286},{\"end\":27882,\"start\":27878},{\"end\":27894,\"start\":27888},{\"end\":27909,\"start\":27903},{\"end\":27925,\"start\":27919},{\"end\":27934,\"start\":27931},{\"end\":27948,\"start\":27942},{\"end\":27958,\"start\":27953},{\"end\":27976,\"start\":27969},{\"end\":28334,\"start\":28333},{\"end\":28350,\"start\":28345},{\"end\":28776,\"start\":28775},{\"end\":28793,\"start\":28785},{\"end\":29301,\"start\":29300},{\"end\":29616,\"start\":29611}]", "bib_author_last_name": "[{\"end\":18488,\"start\":18482},{\"end\":18504,\"start\":18499},{\"end\":18516,\"start\":18513},{\"end\":18536,\"start\":18527},{\"end\":19343,\"start\":19339},{\"end\":19355,\"start\":19352},{\"end\":19368,\"start\":19364},{\"end\":19385,\"start\":19376},{\"end\":19393,\"start\":19389},{\"end\":19415,\"start\":19402},{\"end\":19419,\"start\":19417},{\"end\":19806,\"start\":19800},{\"end\":19820,\"start\":19813},{\"end\":19834,\"start\":19827},{\"end\":19849,\"start\":19846},{\"end\":19864,\"start\":19858},{\"end\":19880,\"start\":19874},{\"end\":19892,\"start\":19888},{\"end\":19900,\"start\":19898},{\"end\":19913,\"start\":19910},{\"end\":20339,\"start\":20332},{\"end\":20351,\"start\":20347},{\"end\":20372,\"start\":20365},{\"end\":20386,\"start\":20382},{\"end\":20764,\"start\":20761},{\"end\":20773,\"start\":20771},{\"end\":20785,\"start\":20781},{\"end\":20792,\"start\":20790},{\"end\":21091,\"start\":21084},{\"end\":21104,\"start\":21093},{\"end\":21121,\"start\":21112},{\"end\":21136,\"start\":21131},{\"end\":21150,\"start\":21145},{\"end\":21165,\"start\":21158},{\"end\":21180,\"start\":21176},{\"end\":21195,\"start\":21190},{\"end\":21201,\"start\":21197},{\"end\":21562,\"start\":21556},{\"end\":21575,\"start\":21570},{\"end\":21591,\"start\":21585},{\"end\":21856,\"start\":21852},{\"end\":21868,\"start\":21865},{\"end\":21882,\"start\":21877},{\"end\":21895,\"start\":21890},{\"end\":21911,\"start\":21905},{\"end\":22233,\"start\":22228},{\"end\":22475,\"start\":22473},{\"end\":22487,\"start\":22485},{\"end\":22751,\"start\":22742},{\"end\":22765,\"start\":22759},{\"end\":23039,\"start\":23035},{\"end\":23053,\"start\":23049},{\"end\":23063,\"start\":23058},{\"end\":23081,\"start\":23077},{\"end\":23094,\"start\":23090},{\"end\":23108,\"start\":23103},{\"end\":23119,\"start\":23117},{\"end\":23433,\"start\":23428},{\"end\":23441,\"start\":23438},{\"end\":23453,\"start\":23450},{\"end\":23467,\"start\":23462},{\"end\":23488,\"start\":23478},{\"end\":23755,\"start\":23752},{\"end\":23770,\"start\":23765},{\"end\":23785,\"start\":23779},{\"end\":24085,\"start\":24079},{\"end\":24105,\"start\":24096},{\"end\":24121,\"start\":24116},{\"end\":24141,\"start\":24130},{\"end\":24615,\"start\":24609},{\"end\":24637,\"start\":24624},{\"end\":24651,\"start\":24647},{\"end\":24665,\"start\":24660},{\"end\":24684,\"start\":24675},{\"end\":25157,\"start\":25146},{\"end\":25176,\"start\":25166},{\"end\":25191,\"start\":25183},{\"end\":25516,\"start\":25512},{\"end\":25525,\"start\":25522},{\"end\":25538,\"start\":25535},{\"end\":25558,\"start\":25549},{\"end\":26018,\"start\":26015},{\"end\":26028,\"start\":26026},{\"end\":26037,\"start\":26034},{\"end\":26272,\"start\":26265},{\"end\":26288,\"start\":26283},{\"end\":26306,\"start\":26297},{\"end\":26555,\"start\":26541},{\"end\":26571,\"start\":26564},{\"end\":26582,\"start\":26573},{\"end\":26834,\"start\":26825},{\"end\":26847,\"start\":26843},{\"end\":26861,\"start\":26856},{\"end\":26880,\"start\":26871},{\"end\":27297,\"start\":27293},{\"end\":27886,\"start\":27883},{\"end\":27901,\"start\":27895},{\"end\":27917,\"start\":27910},{\"end\":27929,\"start\":27926},{\"end\":27940,\"start\":27935},{\"end\":27951,\"start\":27949},{\"end\":27967,\"start\":27959},{\"end\":27981,\"start\":27977},{\"end\":28343,\"start\":28335},{\"end\":28357,\"start\":28351},{\"end\":28361,\"start\":28359},{\"end\":28783,\"start\":28777},{\"end\":28799,\"start\":28794},{\"end\":28806,\"start\":28801},{\"end\":29308,\"start\":29302},{\"end\":29315,\"start\":29310},{\"end\":29626,\"start\":29617}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52967399},\"end\":19256,\"start\":18394},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":195069387},\"end\":19709,\"start\":19258},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":204838007},\"end\":20242,\"start\":19711},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":219966759},\"end\":20677,\"start\":20244},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":228376475},\"end\":20994,\"start\":20679},{\"attributes\":{\"doi\":\"arXiv:2107.04082\",\"id\":\"b5\"},\"end\":21483,\"start\":20996},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":233181984},\"end\":21784,\"start\":21485},{\"attributes\":{\"doi\":\"arXiv:2108.01132\",\"id\":\"b7\"},\"end\":22114,\"start\":21786},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":233387812},\"end\":22444,\"start\":22116},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4076251},\"end\":22736,\"start\":22446},{\"attributes\":{\"doi\":\"arXiv:1606.08415\",\"id\":\"b10\"},\"end\":22943,\"start\":22738},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":121321299},\"end\":23385,\"start\":22945},{\"attributes\":{\"id\":\"b12\"},\"end\":23681,\"start\":23387},{\"attributes\":{\"doi\":\"arXiv:1909.11556\",\"id\":\"b13\"},\"end\":23965,\"start\":23683},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9901844},\"end\":24543,\"start\":23967},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":46954166},\"end\":25031,\"start\":24545},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":218630075},\"end\":25436,\"start\":25033},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8923541},\"end\":25958,\"start\":25438},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":102351511},\"end\":26201,\"start\":25960},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10475843},\"end\":26496,\"start\":26203},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":49211906},\"end\":26753,\"start\":26498},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2191379},\"end\":27224,\"start\":26755},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":208117506},\"end\":27817,\"start\":27226},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":91184134},\"end\":28287,\"start\":27819},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6628106},\"end\":28688,\"start\":28289},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":23376859},\"end\":29244,\"start\":28690},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":15247298},\"end\":29562,\"start\":29246},{\"attributes\":{\"id\":\"b27\"},\"end\":29706,\"start\":29564}]", "bib_title": "[{\"end\":18474,\"start\":18394},{\"end\":19330,\"start\":19258},{\"end\":19792,\"start\":19711},{\"end\":20323,\"start\":20244},{\"end\":20752,\"start\":20679},{\"end\":21545,\"start\":21485},{\"end\":22219,\"start\":22116},{\"end\":22465,\"start\":22446},{\"end\":23024,\"start\":22945},{\"end\":24072,\"start\":23967},{\"end\":24601,\"start\":24545},{\"end\":25137,\"start\":25033},{\"end\":25501,\"start\":25438},{\"end\":26010,\"start\":25960},{\"end\":26257,\"start\":26203},{\"end\":26533,\"start\":26498},{\"end\":26816,\"start\":26755},{\"end\":27284,\"start\":27226},{\"end\":27876,\"start\":27819},{\"end\":28331,\"start\":28289},{\"end\":28773,\"start\":28690},{\"end\":29298,\"start\":29246}]", "bib_author": "[{\"end\":18490,\"start\":18476},{\"end\":18506,\"start\":18490},{\"end\":18518,\"start\":18506},{\"end\":18538,\"start\":18518},{\"end\":19345,\"start\":19332},{\"end\":19357,\"start\":19345},{\"end\":19370,\"start\":19357},{\"end\":19387,\"start\":19370},{\"end\":19395,\"start\":19387},{\"end\":19417,\"start\":19395},{\"end\":19421,\"start\":19417},{\"end\":19808,\"start\":19794},{\"end\":19822,\"start\":19808},{\"end\":19836,\"start\":19822},{\"end\":19851,\"start\":19836},{\"end\":19866,\"start\":19851},{\"end\":19882,\"start\":19866},{\"end\":19894,\"start\":19882},{\"end\":19902,\"start\":19894},{\"end\":19915,\"start\":19902},{\"end\":20341,\"start\":20325},{\"end\":20353,\"start\":20341},{\"end\":20374,\"start\":20353},{\"end\":20388,\"start\":20374},{\"end\":20766,\"start\":20754},{\"end\":20775,\"start\":20766},{\"end\":20787,\"start\":20775},{\"end\":20794,\"start\":20787},{\"end\":21093,\"start\":21077},{\"end\":21106,\"start\":21093},{\"end\":21123,\"start\":21106},{\"end\":21138,\"start\":21123},{\"end\":21152,\"start\":21138},{\"end\":21167,\"start\":21152},{\"end\":21182,\"start\":21167},{\"end\":21197,\"start\":21182},{\"end\":21203,\"start\":21197},{\"end\":21564,\"start\":21547},{\"end\":21577,\"start\":21564},{\"end\":21593,\"start\":21577},{\"end\":21858,\"start\":21844},{\"end\":21870,\"start\":21858},{\"end\":21884,\"start\":21870},{\"end\":21897,\"start\":21884},{\"end\":21913,\"start\":21897},{\"end\":22235,\"start\":22221},{\"end\":22477,\"start\":22467},{\"end\":22489,\"start\":22477},{\"end\":22753,\"start\":22738},{\"end\":22767,\"start\":22753},{\"end\":23041,\"start\":23026},{\"end\":23055,\"start\":23041},{\"end\":23065,\"start\":23055},{\"end\":23083,\"start\":23065},{\"end\":23096,\"start\":23083},{\"end\":23110,\"start\":23096},{\"end\":23121,\"start\":23110},{\"end\":23435,\"start\":23424},{\"end\":23443,\"start\":23435},{\"end\":23455,\"start\":23443},{\"end\":23469,\"start\":23455},{\"end\":23490,\"start\":23469},{\"end\":23757,\"start\":23745},{\"end\":23772,\"start\":23757},{\"end\":23787,\"start\":23772},{\"end\":24087,\"start\":24074},{\"end\":24107,\"start\":24087},{\"end\":24123,\"start\":24107},{\"end\":24143,\"start\":24123},{\"end\":24617,\"start\":24603},{\"end\":24639,\"start\":24617},{\"end\":24653,\"start\":24639},{\"end\":24667,\"start\":24653},{\"end\":24686,\"start\":24667},{\"end\":25159,\"start\":25139},{\"end\":25178,\"start\":25159},{\"end\":25193,\"start\":25178},{\"end\":25518,\"start\":25503},{\"end\":25527,\"start\":25518},{\"end\":25540,\"start\":25527},{\"end\":25560,\"start\":25540},{\"end\":26020,\"start\":26012},{\"end\":26030,\"start\":26020},{\"end\":26039,\"start\":26030},{\"end\":26274,\"start\":26259},{\"end\":26290,\"start\":26274},{\"end\":26308,\"start\":26290},{\"end\":26557,\"start\":26535},{\"end\":26573,\"start\":26557},{\"end\":26584,\"start\":26573},{\"end\":26836,\"start\":26818},{\"end\":26849,\"start\":26836},{\"end\":26863,\"start\":26849},{\"end\":26882,\"start\":26863},{\"end\":27299,\"start\":27286},{\"end\":27888,\"start\":27878},{\"end\":27903,\"start\":27888},{\"end\":27919,\"start\":27903},{\"end\":27931,\"start\":27919},{\"end\":27942,\"start\":27931},{\"end\":27953,\"start\":27942},{\"end\":27969,\"start\":27953},{\"end\":27983,\"start\":27969},{\"end\":28345,\"start\":28333},{\"end\":28359,\"start\":28345},{\"end\":28363,\"start\":28359},{\"end\":28785,\"start\":28775},{\"end\":28801,\"start\":28785},{\"end\":28808,\"start\":28801},{\"end\":29310,\"start\":29300},{\"end\":29317,\"start\":29310},{\"end\":29628,\"start\":29611}]", "bib_venue": "[{\"end\":18680,\"start\":18538},{\"end\":19470,\"start\":19421},{\"end\":19951,\"start\":19915},{\"end\":20437,\"start\":20388},{\"end\":20811,\"start\":20794},{\"end\":21075,\"start\":20996},{\"end\":21610,\"start\":21593},{\"end\":21842,\"start\":21786},{\"end\":22252,\"start\":22235},{\"end\":22553,\"start\":22489},{\"end\":22818,\"start\":22783},{\"end\":23138,\"start\":23121},{\"end\":23422,\"start\":23387},{\"end\":23743,\"start\":23683},{\"end\":24211,\"start\":24143},{\"end\":24763,\"start\":24686},{\"end\":25210,\"start\":25193},{\"end\":25648,\"start\":25560},{\"end\":26056,\"start\":26039},{\"end\":26325,\"start\":26308},{\"end\":26601,\"start\":26584},{\"end\":26968,\"start\":26882},{\"end\":27408,\"start\":27299},{\"end\":28028,\"start\":27983},{\"end\":28419,\"start\":28363},{\"end\":28941,\"start\":28808},{\"end\":29386,\"start\":29317},{\"end\":29609,\"start\":29564},{\"end\":18809,\"start\":18682},{\"end\":20824,\"start\":20813},{\"end\":21623,\"start\":21612},{\"end\":22265,\"start\":22254},{\"end\":22604,\"start\":22555},{\"end\":23151,\"start\":23140},{\"end\":24266,\"start\":24213},{\"end\":25223,\"start\":25212},{\"end\":25723,\"start\":25650},{\"end\":26069,\"start\":26058},{\"end\":26338,\"start\":26327},{\"end\":26614,\"start\":26603},{\"end\":27504,\"start\":27410},{\"end\":28060,\"start\":28030},{\"end\":28439,\"start\":28421}]"}}}, "year": 2023, "month": 12, "day": 17}