{"id": 253734250, "updated": "2023-10-05 07:52:52.424", "metadata": {"title": "DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection", "authors": "[{\"first\":\"Xuan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Shiyu\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Jiulong\",\"last\":\"Shan\",\"middle\":[]},{\"first\":\"Ting\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Visual anomaly detection, an important problem in computer vision, is usually formulated as a one-class classification and segmentation task. The student-teacher (S-T) framework has proved to be effective in solving this challenge. However, previous works based on S-T only empirically applied constraints on normal data and fused multi-level information. In this study, we propose an improved model called DeSTSeg, which integrates a pre-trained teacher network, a denoising student encoder-decoder, and a segmentation network into one framework. First, to strengthen the constraints on anomalous data, we introduce a denoising procedure that allows the student network to learn more robust representations. From synthetically corrupted normal images, we train the student network to match the teacher network feature of the same images without corruption. Second, to fuse the multi-level S-T features adaptively, we train a segmentation network with rich supervision from synthetic anomaly masks, achieving a substantial performance improvement. Experiments on the industrial inspection benchmark dataset demonstrate that our method achieves state-of-the-art performance, 98.6% on image-level AUC, 75.8% on pixel-level average precision, and 76.4% on instance-level average precision.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2211.11317", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ZhangL0HSC23", "doi": "10.1109/cvpr52729.2023.00381"}}, "content": {"source": {"pdf_hash": "d17df33c9b6453d61d01353e94592f1757caee8a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.11317v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ba8b46e788db7b8d62c7389ea4d9c7811393e0c4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d17df33c9b6453d61d01353e94592f1757caee8a.txt", "contents": "\nDeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection\n\n\nXuan Zhang x-zhang18@mails.tsinghua.edu.cn \nTsinghua University\n\n\nShiyu Li \nApple\n\n\nXi Li \nApple\n\n\nPing Huang \nApple\n\n\nJiulong Shan jlshan@apple.com \nApple\n\n\nTing Chen tingchen@tsinghua.edu.cn \nTsinghua University\n\n\nDeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection\n\nVisual anomaly detection, an important problem in computer vision, is usually formulated as a one-class classification and segmentation task. The student-teacher (S-T) framework has proved to be effective in solving this challenge. However, previous works based on S-T only empirically applied constraints on normal data and fused multilevel information. In this study, we propose an improved model called DeSTSeg, which integrates a pre-trained teacher network, a denoising student encoder-decoder, and a segmentation network into one framework. First, to strengthen the constraints on anomalous data, we introduce a denoising procedure that allows the student network to learn more robust representations. From synthetically corrupted normal images, we train the student network to match the teacher network feature of the same images without corruption. Second, to fuse the multi-level S-T features adaptively, we train a segmentation network with rich supervision from synthetic anomaly masks, achieving a substantial performance improvement. Experiments on the industrial inspection benchmark dataset demonstrate that our method achieves state-of-the-art performance, 98.6% on image-level AUC, 75.8% on pixel-level average precision, and 76.4% on instance-level average precision.\n\nIntroduction\n\nVisual anomaly detection (AD) with localization is an essential task in many computer vision applications such as industrial inspection [24,36], medical disease screening [27,32], and video surveillance [18,20]. The objective of these tasks is to identify both corrupted images and anomalous pixels in corrupted images. As anomalous samples occur rarely, and the number of anomaly types is enormous, it is unlikely to acquire enough anomalous samples with all possible anomaly types for training. Therefore, AD tasks were usually formulated as a one-class classification and segmentation, using only normal data for model training.\n\nThe student-teacher (S-T) framework, known as knowledge distillation, has proven effective in AD [3,9,26,31,33]. In this framework, a teacher network is pre-trained on a large-scale dataset, such as ImageNet [10], and a student network is trained to mimic the feature representations of the teacher network on an AD dataset with normal samples only. The primary hypothesis is that the student network will generate different feature representations from the teacher network on anomalous samples that have never been encountered in training. Consequently, anomalous pixels and images can be recognized in the inference phase. Notably, [26,31] applied knowledge distillation at various levels of the feature pyramid so that discrepancies from multiple layers were aggregated and demonstrated good performance. However, there is no guarantee that the features of anomalous samples are always different between S-T networks because there is no constraint from anomalous samples during the training. Even with anomalies, the student network may be over-generalized [22] and output similar feature representations as those by the teacher network. Furthermore, aggregating discrepancies from multilevel in an empirical way, such as sum or product, could be suboptimal. For instance, in the MVTec AD dataset under the same context of [31], we observe that for the category of transistor, employing the representation from the last layer, with 88.4% on pixel-level AUC, outperforms that from the multi-level features, with 81.9% on pixel-level AUC.\n\nTo address the problem mentioned above, we propose DeSTSeg, illustrated in Fig. 1, which consists of a denoising student network, a teacher network, and a segmentation network. We introduce random synthetic anomalies into the normal images and then use these corrupted images 1 for training. The denoising student network takes a corrupted image as input, whereas the teacher network takes the original clean image as input. During training, Figure 1. Overview of DeSTSeg. Synthetic anomalous images are generated and used during training. In the first step (a), the student network with synthetic input is trained to generate similar feature representations as the teacher network from the clean image. In the second step (b), the element-wise product of the student and teacher networks' normalized outputs are concatenated and utilized to train the segmentation network. The segmentation output is the predicted anomaly score map. the feature discrepancy between the two networks is minimized. In other words, the student network is trained to perform denoising in the feature space. Given anomalous images as input to both networks, the teacher network encodes anomalies naturally into features, while the trained denoising student network filters anomalies out of feature space. Therefore, the two networks are reinforced to generate distinct features from anomalous inputs. For the architecture of the denoising student network, we decided to use an encoder-decoder network for better feature denoising instead of adopting an identical architecture as the teacher network. In addition, instead of using empirical aggregation, we append a segmentation network to fuse the multilevel feature discrepancies in a trainable manner, using the generated binary anomaly mask as the supervision signal.\n\nWe evaluate our method on a benchmark dataset for surface anomaly detection and localization, MVTec AD [2]. Extensive experimental results show that our method outperforms the state-of-the-art methods on image-level, pixellevel, and instance-level anomaly detection tasks. We also conduct ablation studies to validate the effectiveness of our proposed components.\n\nOur main contributions are summarized as follows. (1) We propose a denoising student encoder-decoder, which is trained to explicitly generate different feature representations from the teacher with anomalous inputs. (2) We employ a segmentation network to adaptively fuse the multilevel feature similarities to replace the empirical inference approach. (3) We conduct extensive experiments on the benchmark dataset to demonstrate the effectiveness of our method for various tasks.\n\n\nRelated Works\n\nAnomaly detection and localization have been studied from numerous perspectives. In image reconstruction, researchers used autoencoder [4], variational autoencoder [1,30] or generative adversarial network [21,27,28] to train an image reconstruction model on normal data. The presumption is that anomalous images cannot be reconstructed effectively since they are not seen during training, so the difference between the input and reconstructed images can be used as pixel-level anomaly scores. However, anomaly regions still have a chance to be accurately reconstructed due to the over-generalization issue [22]. Another perspective is the parametric density estimation, which assumes that the extracted features of normal data obey a certain distribution, such as a multivariate Gaussian distribution [8,15,16,23], and uses the normal dataset to estimate the parameters. Then, the outlier data are recognized as anomalous data by inference. Since the assumption of Gaussian distribution is too strict, some recent works borrow ideas from normalizing flow, by projecting an arbitrary distribution to a Gaussian distribution to approximate the density of any distributions [13,35]. Besides, the memorybased approaches [7,19,24,34] build a memory bank of normal data in training. During inference, given a query item, the model selects the nearest item in the memory bank and uses the similarity between the query item and the nearest item to compute the anomaly score.\n\nKnowledge distillation.\n\nKnowledge distillation is based on a pretrained teacher network and a trainable student network. As the student network is trained on an anomaly-free dataset, its feature representation of anomalies is expected to be distinct from that of the teacher network. Numerous solutions have been presented in the past to improve discrimination against various types of anomalies. For example, [3] used ensemble learning to train multiple student networks and exploited the irregularity of their feature representations to recognize the anomaly. [31], and [26] adopted multi-level feature representation alignment to capture both low-level and high-level anomalies. [9], and [33] designed decoder architectures for the student network to avoid the shortcomings of identical architecture and the same dataflow between S-T networks. These works focus on improving the similarity of S-T representations on normal inputs, whereas our work additionally attempts to differentiate their representations on anomalous input.\n\nAnomaly simulation. Although there is no anomalous data for training in the context of one-class classification AD, the pseudo-anomalous data could be simulated so that an AD model can be trained in a supervised way. Classical anomaly simulation strategies, such as rotation [12] and cutout [11], do not perform well in detecting fine-grained anomalous patterns [16]. A simple yet effective strategy is called CutPaste [16] that randomly selects a rectangular region inside the original image and then copies and pastes the content to a different location within the image. Another strategy proposed in [36] and also adopted in [37] used two-dimensional Perlin noise to simulate a more realistic anomalous image. With the simulated anomalous images and corresponding ground truth masks, [29,36,37] localized anomalies with segmentation networks. In our system, we adopt the ideas of [36] for anomaly simulation and segmentation.\n\n\nMethod\n\nThe proposed DeSTSeg consists of three main components: a pre-trained teacher network, a denoising student network, and a segmentation network.\n\nAs illustrated in Fig. 1, synthetic anomalies are introduced into normal training images, and the model is trained in two steps. In the first step, the simulated anomalous image is utilized as the student network input, whereas the original clean image is the input to the teacher network. The weights of the teacher network are fixed, but the student network for denoising is trainable. In the second step, the student model is fixed as well. Both the student and teacher networks take the synthetic anomaly image as their input to optimize parameters in the segmentation network to localize the anomalous regions. For inference, pixel-level anomaly maps are generated in an end-to-end mode, and the corresponding image-level anomaly scores can be computed via post-processing.\n\n\nSynthetic Anomaly Generation\n\nThe training of our model relies on synthetic anomalous images which are generated using the same algorithm proposed in [36]. Random two-dimensional Perlin noise is generated and binarized by a preset threshold to obtain an anomaly mask M . An anomalous image I a is generated by replacing the mask region with a linear combination of an anomaly-free image I n and an arbitrary image from external data source A, with an opacity factor \u03b2 randomly chosen between [0. 15,1].\nI a = \u03b2(M \u2299 A) + (1 \u2212 \u03b2)(M \u2299 I n ) + (1 \u2212 M ) \u2299 I n (1)\n\u2299 means the element-wise multiplication operation. The anomaly generation is performed online during training. By using this algorithm, three benefits are introduced. First, compared to painting a rectangle anomaly mask [16], the anomaly mask generated by random Perlin noise is more irregular and similar to actual anomalous shapes. Second, the image used as anomaly content A could be arbitrarily chosen without elaborate selection [36]. Third, the introduction of opacity factor \u03b2 can be regarded as a data augmentation [38] to effectively increase the diversity of the training set.\n\n\nDenoising Student-Teacher Network\n\nIn previous multi-level knowledge distillation approaches [26,31], the input of the student network (normal image) is identical to that of the teacher network, as is the architecture of the student network. However, our proposed denoising student network and the teacher network take paired anomalous and normal images as input, with the denoising student network having a distinct encoderdecoder architecture. In the following two paragraphs, we will examine the motivation for this design.\n\nFirst, as mentioned in Sec. 1, an optimization target should be established to encourage the student network to generate anomaly-specific features that differ from the teacher's. We further endow a more straightforward target to the student network: to build normal feature representations on anomalous regions supervised by the teacher network. As the teacher network has been pre-trained on a large dataset, it can generate discriminative feature representations in both normal and anomalous regions. Therefore, the denoising student network will generate different feature representations from those by the teacher network during inference. Besides, as mentioned in Sec. 2, the memory-based approaches look for the most similar normal item in the memory bank to the query item and use their similarity for inference. Similarly, we optimize the denoising student network to reconstruct the normal features.\n\nSecond, given the feature reconstruction task, we conclude that the student network should not copy the architecture of the teacher network. Considering the process of reconstructing the feature of an early layer, it is well known that the lower layers of CNN capture local information, such as texture and color. In contrast, the upper layers of CNN express global semantic information [9]. Recalling that our denoising student network should reconstruct the feature of the corresponding normal image from the teacher network, such a task relies on global semantic information of the image and could not be done perfectly with only a few lower layers. We notice that the proposed task design resembles image denoising, with the exception that we wish to denoise the image in the feature space. The encoderdecoder architecture is widely used for image denoising. Therefore, we adopted it as the denoising student network's architecture. There is an alternative way to use the teacher network as an encoder and reverse the student network as the decoder [9,33]; however, our preliminary experimental results show that a complete encoder-decoder student network performs better. One possible explanation is that the pre-trained teacher network is usually trained on ImageNet with classification tasks; thus, the encoded features in the last layers lack sufficient information to reconstruct the feature representations at all levels.\n\nFollowing [31], the teacher network is an ImageNet pretrained ResNet18 [14] with the final block removed (i.e., conv5 x). The output feature maps are extracted from the three remaining blocks, i.e., conv2 x, conv3 x, and conv4 x denoted as T 1 , T 2 , and T 3 , respectively. Regarding the denoising student network, the encoder is a randomly initialized ResNet18 with all blocks, named S 1 E , S 2 E , S 3 E , and S 4 E , respectively. The decoder is a reversed ResNet18 (by replacing all downsampling with bilinear upsampling) with four residual blocks, named S 4 D , S 3 D , S 2 D , and S 1 D , respectively.\n\nWe minimize the cosine distance between features from T k and S k D , k = 1, 2, 3. Denoting F T k \u2208 R C k \u00d7H k \u00d7W k the feature representation from layer T k , and F S k \u2208 R C k \u00d7H k \u00d7W k the feature representation from layer S k D , the cosine distances can be computed through Eq. (2) and Eq. (3). i and j stand for the spatial coordinate on the feature map. In particular, i = 1...H k and j = 1...W k . The loss is the sum of distances across three different feature levels as shown in Eq. (4).\nX k (i, j) = F T k (i, j) \u2299 F S k (i, j) ||F T k (i, j)|| 2 ||F S k (i, j)|| 2 (2) D k (i, j) = 1 \u2212 C k c=1 X k (i, j) c (3) L cos = 3 k=1 \uf8eb \uf8ed 1 H k W k H k ,W k i,j=1 D k (i, j) \uf8f6 \uf8f8(4)\n\nSegmentation Network\n\nIn [26,31], the cosine distances from multi-level features are summed up directly to represent the anomaly score of each pixel. However, the results can be suboptimal if discriminations of all level features are not equally accurate. To address this issue, we add a segmentation network to guide the feature fusion with additional supervision signals.\n\nWe freeze the weights of both the student and teacher networks to train the segmentation network. The synthetic anomalous image is utilized as the input for both S-T networks, and the corresponding binary anomaly mask is the ground truth. The similarities of the feature maps (T 1 , S 1 D ), (T 2 , S 2 D ), (T 3 , S 3 D ) are calculated by Eq. (2) and upsampled to the same size as X 1 , which is 1/4 of the input size. The upsampled features, denoted asX 1 ,X 2 , andX 3 , are then concatenated asX, which is fed into the segmentation network. We also investigate alternative ways to compute the input of the segmentation network in Sec. 4.4. The segmentation network contains two residual blocks and one Atrous Spatial Pyramid Pooling (ASPP) module [5]. There is no upsampling or downsampling; thus, the output size equals the size of X 1 . Although this may lead to resolution loss to some extent, it reduces the memory cost for training and inference, which is crucial in practice.\n\nThe segmentation training is optimized by employing the focal loss [17] and the L1 loss. In the training set, the majority of pixels are normal and easily recognized as background. Only a small portion of the image consists of anomalous pixels that must be segmented. Therefore, the focal loss can help the model to focus on the minority category and difficult samples. In addition, the L1 loss is employed to improve the sparsity of the output so that the segmentation mask's boundaries are more distinct. To compute the loss, we downsample the ground truth anomaly mask to a size equal to 1/4 of the input size, which matches the output (H 1 , W 1 ). Mathematically, we denote the output probability map as\u0176 and the downsampled anomaly mask as M , and the focal loss is computed using Eq. (5) where\np ij = M ij\u0176ij + (1 \u2212 M ij )(1 \u2212\u0176 ij )\nand \u03b3 is the focusing parameter. The L1 loss is computed by Eq. (6), and the segmentation loss is computed by Eq. (7).\nL f ocal = \u2212 1 H 1 W 1 H1,W1 i,j=1 (1 \u2212 p ij ) \u03b3 log(p ij ) (5) L l1 = 1 H 1 W 1 H1,W1 i,j=1 |M ij \u2212\u0176 ij |(6)\nL seg = L f ocal + L l1 (7)\n\n\nInference\n\nIn the inference stage, the test image is fed into both the teacher and student networks. The segmentation prediction is finally upsampled to the input size and taken as the anomaly score map. It is expected that anomalous pixels in the input image will have greater values in the output. To calculate the image-level anomaly score, we use the average of the top T values from the anomaly score map, where T is a tuning hyperparameter.\n\n\nExperiments\n\n\nDataset\n\nWe evaluate our method using the MVTec AD [2] dataset, which is one of the most widely used benchmarks for anomaly detection and localization. The dataset comprises 15 categories, including 10 objects and 5 textures. For each category, there are hundreds of normal images for training and a mixture of anomalous and normal images for evaluation. The image sizes range from 700 \u00d7 700 to 1024 \u00d7 1024 pixels. For evaluation purposes, pixellevel binary annotations are provided for anomalous images in the test set. In addition, the Describable Textures Dataset (DTD) [6] is used as the anomaly source image A in Eq. (1). [36] showed that other datasets such as Im-ageNet can achieve comparable performance but DTD is much smaller and easy to use.\n\n\nEvaluation Metrics\n\nImage-level evaluation. Following the previous work in anomaly detection work, AUC (i.e., area under the ROC curve) is utilized to evaluate image-level anomaly detection.\n\nPixel-level evaluation. AUC is also selected to evaluate the pixel-level result. Additionally, we report average precision (AP) since it is a more appropriate metric for heavily imbalanced classes [25].\n\nInstance-level evaluation. In real-world applications, such as industrial defect inspection and medical imaging lesion detection, users are more concerned with whether the model can fully or partially localize an instance than with each individual pixel. In [3], per-region-overlap (PRO) is proposed, which equally weights the connected components of different sizes in the ground truth. It computes the overlap between prediction and ground truth within a user-specified false positive rate (30%). However, because instance recall is essential in practice, we propose to use instance average precision (IAP) as a more straightforward metric. Formally, we define an anomaly instance as a maximally connected ground truth region. Given a prediction map, an anomalous instance is considered detected if and only if more than 50% of the region pixels are predicted as positive. Under different thresholds, a list of pixel-level precision and instance-level recall rate points can be drawn as a curve. The average precision of this curve is calculated as IAP. For those applications requiring an extremely high recall, the precision at recall = k% is also computed and denoted as IAP@k. In our experiments, we evaluate our model under a high-stakes scenario by setting k = 90.\n\nGround truth downsampling method. We notice that the prior implementations of pixel-level evaluation are poorly aligned. Most of the works downsampled the ground US [3] STPM [31] CutPaste [16] DRAEM [36] DSR [37] PatchCore [ truth to 256 \u00d7 256 for faster computation, but some performed an extra 224 \u00d7 224 center crop [7,8,24]. In addition, the downsampling implementations are not standardized either [13,24,36], resulting in the varying ground truth and unfair evaluation. In some cases, the downsampling introduces severe distortion, as illustrated in Fig. 2. In our work, we use bilinear interpolation to downsample the binary mask to 256 \u00d7 256 and then round the result with a threshold of 0.5. This implementation can preserve the continuity of the original ground truth mask without over or under-estimating.\n\n\nResults\n\nIn order to make fair comparisons with other works, we re-evaluated the official pre-trained models of [31], [24], and [36] using our proposed evaluation introduced in Sec. 4.2. For methods without open-source code, we use the results mentioned in the original papers. Unavailable results are denoted with '-'. We repeat the experiments of our method 5 times with different random seeds to report the standard deviation.\n\nImage-level anomaly detection. We report the AUC for the image-level anomaly detection task in Tab. 1. The performance of our method reaches state-of-the-art on average. Category-specific results are shown in the supplementary material.\n\nPixel-level anomaly localization. We report the AUC and AP values for the pixel-level anomaly localization task in Tab. 2. On average, our method outperforms state-of-theart by 5.6% on AP and achieves AUC scores comparable to PatchCore [24]. Our method reaches the highest or nearhighest score in the majority of categories, indicating that our approach generalizes well over a wide range of industrial application scenes.\n\nInstance-level anomaly detection.\n\nThe IAP and IAP@90 of the instance-level anomaly detection are reported in Tab. 3. Our method achieves the state of the art for both metrics. On average, our approach reaches an IAP@90 of 57.8%, which indicates that when 90% of anomaly instances are detected, the pixel-level precision is 57.8%, or equally, the pixel-level false positive rate is 42.2%. As some categories (e.g., carpet, pill) contain hard samples close to the decision boundary, their standard deviations of IAP@90 are relatively high. In practice, these metrics can be used to determine whether the performance is acceptable for an application.\n\nCategory-specific analysis. For the category cable, memory-based approaches [24,37] have better performance than ours since the normal pixels have larger intra-class distances than categories with periodic textures. For the categories grid, screw, and tile, the anomalies are relatively small or thin. Therefore, methods with higher resolution STPM [31] DRAEM [36] PatchCore [ Table 3. Instance-level anomaly detection IAP / IAP@90 (%) on MVTec AD dataset. predictions, such as [36,37], can achieve higher performance, but require more memory and computation. For the remaining categories, our method achieves comparable or higher performance than the compared methods. Visualization examples. Several visualization examples of our method from various categories are presented in Fig. 3. Our method can precisely localize the anomaly regions. More examples are shown in the supplementary material.\n\nAnalysis of failure cases. We analyze some failure cases illustrated in Fig. 4. On the one hand, several ambiguous ground truths are responsible for a number of failure occurrences. In a transistor case from the first row, the ground truth highlights both the original and misplaced location, while the prediction mask only covers the misplaced loca-tion. For a capsule case shown in the second tow, the ground truth contains most of the distorted parts, whereas the prediction mask covers the entire capsule. In these cases, we would argue that our predictions are still useful.\n\nOn the other hand, some failure cases, such as those shown in the third and fourth rows, result from noisy backgrounds. Tiny fibers and stains are highlighted due to the susceptibility of our model. We leave it to future work to investigate whether these anomalies are acceptable in order to draw more accurate conclusions.\n\n\nAblation Studies\n\nNetwork architecture. In Tab. 4, we evaluate the effectiveness of our three designs: replacing the training inputs of the vanilla student network with synthetic anomalies to enable a denoising procedure (den), applying encoderdecoder architecture to the student network(ed), and appending the segmentation network (seg) to replace the empirical feature fusion strategy, i.e., a product of cosine distances [31]. (a) Comparing experiments 1 and 2, it can be found that only changing the student network's input to anomalous images undermines performance. However, experiment 5 shows improvement when ed is added, indicating that the den can be boosted by adopting ed architecture. (b) The comparisons of experiments 1 with 4, 2 with 6, 3 with 7, and 5 with 8, showcase that the segmentation network can significantly improve the performances of all three metrics. (c) Comparing experiments 4 and 8, it can be found that the combination of den and ed provides more useful features for the segmentation network than a vanilla S-T network does. The best result is achieved by combining all three main designs.\n\nSegmentation loss. In Tab. 5, we examine the effectiveness of the L1-loss in the segmentation loss (Eq. (7)). It can be observed that the L1-loss improves performance.\n\n\nSegmentation network input.\n\nAs mentioned in Sec. 3.3, the input of the segmentation network is the element-wise product between the normalized feature maps of S-T networks as defined by Eq. (2). To prove the rationality of this setting, we build two distinct feature combinations as input. The first is to directly concatenate the feature maps of S-T networks F S k and F T k as the input of the segmentation network, which preserves the information of the S-T networks more effectively. The second is to compute the cosine distance of the S-T networks' feature maps using Eq. (3), which utilizes more prior information when we train the student network by optimizing the cosine distance. We show the results in Tab. 6. Both approaches result in suboptimal performance, indicating thatX is a suitable choice as the input to balance the information and prior.\n\nExp. den ed seg img (AUC) pix (AP) ins (IAP)  Table 6. Ablation studies on the input of segmentation network: AUC, AP, and IAP (%) are used to evaluate image-level, pixellevel, and instance-level detection, respectively.\n\n\nConclusion\n\nWe propose the DeSTSeg, a segmentation-guided denoising student-teacher framework for the anomaly detection task. The denoising student-teacher network is adopted to enable the S-T network to generate discriminative features in anomalous regions. The segmentation network is built to fuse the S-T network features adaptively. Experiments on the surface anomaly detection benchmark show that all of our proposed components considerably boost performance. Our results outperform the previous state-of-theart by 0.1% AUC for image-level anomaly detection, 5.6% AP for pixel-level anomaly localization, and 4.9% IAP for instance-level anomaly detection.\n\nFigure 2 .\n2The binary ground truth is downsampled with different implementations. (a) The grid image with a crack anomaly. (b) Downsample with bilinear interpolation, then floor all values between (0, 1) to zero[13,24]. The mask has almost vanished. (c) Downsample with bilinear interpolation, then ceil all values between (0, 1) to one[36]. The mask is thicker than expected. (d) Downsample with the nearest interpolation, interrupting the original contiguous region. (e) Our proposed approach, downsample with bilinear interpolation and round values by threshold=0.5, The original contiguous region is not interrupted.\n\nFigure 3 .\n3Visualization examples of our method. For each example, left: input image; middle: ground truth; right: prediction map.\n\nFigure 4 .\n4Failure cases of our method. The examples are chosen from transistor, capsule, screw, and hazelnut (from top to bottom). For each example, left: input image; middle: ground truth; right: prediction map.\n\n\nTable 1. Image-level anomaly detection AUC (%) on MVTec AD dataset. Results are averaged over all categories.24] \nOurs \n\n87.7 \n95.1 \n95.2 \n98.0 \n98.2 \n98.5 \n98.6 98.6 98.6\u00b10.4 \n\nUS [3] \nSTPM [31] CutPaste [16] DRAEM [36] DSR [37] PatchCore [24] Ours \n\nbottle \n97.8 / 74.2 98.8 / 80.6 97.6 / -\n99.3 99.3 99.3 / 89.8 \n-/ 91.5 91.5 91.5 \n98.9 / 80.1 \n99.2\u00b10.2 / 90.3\u00b11.8 \ncable \n91.9 / 48.2 94.8 / 58.0 90.0 / -\n95.4 / 62.6 \n-/ 70.4 70.4 70.4 \n98.8 98.8 98.8 / 70.0 \n97.3\u00b10.4 / 60.4\u00b12.3 \ncapsule \n96.8 / 25.9 98.2 / 35.9 97.4 / -\n94.1 / 43.5 \n-/ 53.3 \n99.1 99.1 99.1 / 48.1 \n99.1 99.1 99.1\u00b10.0 / 56.3 56.3 56.3\u00b11.1 \ncarpet \n93.5 / 52.2 99.1 99.1 99.1 / 65.3 98.3 / -\n96.2 / 64.4 \n-/ 78.2 78.2 78.2 \n99.1 99.1 99.1 / 66.7 \n96.1\u00b12.2 / 72.8\u00b15.8 \ngrid \n89.9 / 10.1 99.1 / 45.4 97.5 / -\n99.5 99.5 99.5 / 56.8 \n-/ 68.0 68.0 68.0 \n98.9 / 41.0 \n99.1\u00b10.1 / 61.5\u00b11.6 \nhazelnut \n98.2 / 57.8 98.9 / 60.3 97.3 / -\n99.5 / 88.1 \n-/ 87.3 \n99.0 / 61.5 \n99.6 99.6 99.6\u00b10.2 / 88.4 88.4 88.4\u00b12.2 \nleather \n97.8 / 40.9 99.2 / 42.9 99.5 / -\n98.9 / 69.9 \n-/ 62.5 \n99.4 / 51.0 \n99.7 99.7 99.7\u00b10.0 / 75.6 75.6 75.6\u00b11.2 \nmetal nut 97.2 / 83.5 97.2 / 79.3 93.1 / -\n98.7 / 91.7 \n-/ 67.5 \n98.8 98.8 98.8 / 88.8 \n98.6\u00b10.4 / 93.5 93.5 93.5\u00b11.1 \npill \n96.5 / 62.0 94.7 / 63.3 95.7 / -\n97.6 / 46.1 \n-/ 65.7 \n98.2 / 78.7 \n98.7 98.7 98.7\u00b10.4 / 83.1 83.1 83.1\u00b14.2 \nscrew \n97.4 / 7.8 98.6 / 26.9 96.7 / -\n99.7 99.7 99.7 / 71.5 71.5 71.5 \n-/ 52.5 \n99.5 / 41.4 \n98.5\u00b10.3 / 58.7\u00b13.7 \ntile \n92.5 / 65.3 96.6 / 61.7 90.5 / -\n99.5 99.5 99.5 / 96.9 96.9 96.9 \n-/ 93.9 \n96.6 / 59.3 \n98.0\u00b10.7 / 90.0\u00b12.5 \ntoothbrush 97.9 / 37.7 98.9 / 48.8 98.1 / -\n98.1 / 54.7 \n-/ 74.2 \n98.9 / 51.6 \n99.3 99.3 99.3\u00b10.1 / 75.2 75.2 75.2\u00b11.8 \ntransistor 73.7 / 27.1 81.9 / 44.4 93.0 / -\n90.0 / 51.7 \n-/ 41.1 \n96.2 96.2 96.2 / 63.2 \n89.1\u00b13.4 / 64.8 64.8 64.8\u00b14.0 \nwood \n92.1 / 53.3 95.2 / 47.0 95.5 / -\n97.0 / 80.5 \n-/ 68.4 \n95.1 / 52.3 \n97.7 97.7 97.7\u00b10.3 / 81.9 81.9 81.9\u00b11.2 \nzipper \n95.6 / 36.1 98.0 / 54.9 99.3 / -\n98.6 / 72.3 \n-/ 78.5 \n99.0 / 64.0 \n99.1 99.1 99.1\u00b10.5 / 85.2 85.2 85.2\u00b13.3 \n\naverage \n93.9 / 45.5 96.6 / 54.3 96.0 / -\n97.5 / 69.3 \n-/ 70.2 \n98.4 98.4 98.4 / 61.2 \n97.9\u00b10.3 / 75.8 75.8 75.8\u00b10.8 \n\nTable 2. Pixel-level anomaly localization AUC / AP (%) on MVTec AD dataset. \n\n\n\n\nTable 4. Ablation studies on our main designs: denoising training (den), the encoder-decoder architecture of student network (ed), and segmentation network (seg). AUC, AP, and IAP (%) are used to evaluate image-level, pixel-level, and instance-level detection, respectively. Exp. 1 uses the same architecture of[31], but different training settings to align with Exp. 2\u223c8.Table 5. Ablation studies on the segmentation loss: AUC, AP, and IAP (%) are used to evaluate image-level, pixel-level, and instance-level detection, respectively.1 \n94.8 \n52.9 \n55.8 \n2 \n\u2713 \n93.4 \n49.6 \n53.9 \n3 \n\u2713 \n95.4 \n53.3 \n57.7 \n4 \n\u2713 \n97.3 \n70.1 \n71.8 \n5 \n\u2713 \u2713 \n94.5 \n54.0 \n58.5 \n6 \n\u2713 \n\u2713 \n97.3 \n70.9 \n72.3 \n7 \n\u2713 \u2713 \n97.7 \n69.7 \n71.2 \n8 \n\u2713 \u2713 \u2713 \n98.6 98.6 98.6 \n75.8 75.8 75.8 \n76.4 76.4 76.4 \n\nimg (AUC) pix (AP) ins (IAP) \n\nw/o L1 loss \n97.9 \n72.2 \n74.4 \nw/ L1 loss \n98.6 98.6 98.6 \n75.8 75.8 75.8 \n76.4 76.4 76.4 \n\nimg (AUC) pix (AP) ins (IAP) \n\nconcatenated-ST input \n98.0 \n72.2 \n72.6 \ncosine-distance input \n98.5 \n72.0 \n74.5 \nDeSTSeg \n98.6 98.6 98.6 \n75.8 75.8 75.8 \n76.4 76.4 76.4 \n\n\n\nDeep autoencoding models for unsupervised anomaly segmentation in brain mr images. Christoph Baur, Benedikt Wiestler, Shadi Albarqouni, Nassir Navab, International MICCAI brainlesion workshop. SpringerChristoph Baur, Benedikt Wiestler, Shadi Albarqouni, and Nassir Navab. Deep autoencoding models for unsupervised anomaly segmentation in brain mr images. In International MICCAI brainlesion workshop, pages 161-169. Springer, 2018. 2\n\nMvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition25Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9592-9600, 2019. 2, 5\n\nUninformed students: Student-teacher anomaly detection with discriminative latent embeddings. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition56Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4183-4192, 2020. 1, 3, 5, 6\n\nImproving unsupervised defect segmentation by applying structural similarity to autoencoders. Paul Bergmann, Sindy L\u00f6we, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and ApplicationsSciTePressPaul Bergmann, Sindy L\u00f6we, Michael Fauser, David Sattleg- ger, and Carsten Steger. Improving unsupervised defect seg- mentation by applying structural similarity to autoencoders. In Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, pages 372-380. SciTePress, 2019. 2\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE transactions on pattern analysis and machine intelligence. 40Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834-848, 2017. 4\n\nIasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Mircea Cimpoi, Subhransu Maji, 2014 IEEE Conference on Computer Vision and Pattern Recognition. Describing textures in the wildMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 3606-3613, 2014. 5\n\nSub-image anomaly detection with deep pyramid correspondences. Niv Cohen, Yedid Hoshen, arXiv:2005.0235736arXiv preprintNiv Cohen and Yedid Hoshen. Sub-image anomaly detec- tion with deep pyramid correspondences. arXiv preprint arXiv:2005.02357, 2020. 3, 6\n\nPadim: a patch distribution modeling framework for anomaly detection and localization. Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier, International Conference on Pattern Recognition. Springer36Thomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier. Padim: a patch distribution modeling framework for anomaly detection and localization. In Inter- national Conference on Pattern Recognition, pages 475-489. Springer, 2021. 3, 6\n\nAnomaly detection via reverse distillation from one-class embedding. Hanqiu Deng, Xingyu Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition14Hanqiu Deng and Xingyu Li. Anomaly detection via reverse distillation from one-class embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9737-9746, 2022. 1, 3, 4\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. 1\n\nImproved regularization of convolutional neural networks with cutout. Terrance Devries, W Graham, Taylor, arXiv:1708.04552arXiv preprintTerrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 3\n\nUnsupervised representation learning by predicting image rotations. Spyros Gidaris, Praveer Singh, Nikos Komodakis, International Conference on Learning Representations. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un- supervised representation learning by predicting image rota- tions. In International Conference on Learning Representa- tions, 2018. 3\n\nCflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. Denis Gudovskiy, Shun Ishizaka, Kazuki Kozuka, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision56Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka. Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. In Proceed- ings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 98-107, 2022. 3, 5, 6\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 4\n\nA simple unified framework for detecting out-of-distribution samples and adversarial attacks. Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin, 31Advances in neural information processing systemsKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural infor- mation processing systems, 31, 2018. 3\n\nCutpaste: Self-supervised learning for anomaly detection and localization. Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, Tomas Pfister, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition36Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. Cutpaste: Self-supervised learning for anomaly de- tection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9664-9674, 2021. 3, 6\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017. 4\n\nClassifier two sample test for video anomaly detections. Yusha Liu, Chun-Liang Li, Barnab\u00e1s P\u00f3czos, Proceedings of the British Machine Vision Conference. the British Machine Vision Conference71Yusha Liu, Chun-Liang Li, and Barnab\u00e1s P\u00f3czos. Classi- fier two sample test for video anomaly detections. In Pro- ceedings of the British Machine Vision Conference, page 71, 2018. 1\n\nAre pre-trained cnns good feature extractors for anomaly detection in surveillance videos?. Rodrigo F Tiago S Nazare, De Mello, Ponti, arXiv:1811.08495arXiv preprintTiago S Nazare, Rodrigo F de Mello, and Moacir A Ponti. Are pre-trained cnns good feature extractors for anomaly detection in surveillance videos? arXiv preprint arXiv:1811.08495, 2018. 3\n\nSelf-trained deep ordinal regression for end-to-end video anomaly detection. Guansong Pang, Cheng Yan, Chunhua Shen, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionAnton van den Hengel, and Xiao BaiGuansong Pang, Cheng Yan, Chunhua Shen, Anton van den Hengel, and Xiao Bai. Self-trained deep ordinal regression for end-to-end video anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12173-12182, 2020. 1\n\nOcgan: One-class novelty detection using gans with constrained latent representations. Pramuditha Perera, Ramesh Nallapati, Bing Xiang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPramuditha Perera, Ramesh Nallapati, and Bing Xiang. Oc- gan: One-class novelty detection using gans with constrained latent representations. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 2898-2906, 2019. 2\n\nInpainting transformer for anomaly detection. Jonathan Pirnay, Keng Chai, International Conference on Image Analysis and Processing. Springer1Jonathan Pirnay and Keng Chai. Inpainting transformer for anomaly detection. In International Conference on Image Analysis and Processing, pages 394-406. Springer, 2022. 1, 2\n\nModeling the distribution of normal data in pre-trained deep features for anomaly detection. Oliver Rippel, Patrick Mertens, Dorit Merhof, 2020 25th International Conference on Pattern Recognition (ICPR). IEEEOliver Rippel, Patrick Mertens, and Dorit Merhof. Model- ing the distribution of normal data in pre-trained deep fea- tures for anomaly detection. In 2020 25th International Con- ference on Pattern Recognition (ICPR), pages 6726-6733. IEEE, 2021. 3\n\nTowards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition67Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, and Peter Gehler. Towards to- tal recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14318-14328, 2022. 1, 3, 5, 6, 7\n\nThe precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets. Takaya Saito, Marc Rehmsmeier, PloS one. 103118432Takaya Saito and Marc Rehmsmeier. The precision-recall plot is more informative than the roc plot when evaluat- ing binary classifiers on imbalanced datasets. PloS one, 10(3):e0118432, 2015. 5\n\nMultiresolution knowledge distillation for anomaly detection. Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, Hamid R Rabiee, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition14Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, and Hamid R Ra- biee. Multiresolution knowledge distillation for anomaly detection. In Proceedings of the IEEE/CVF confer- ence on computer vision and pattern recognition, pages 14902-14912, 2021. 1, 3, 4\n\nf-anogan: Fast unsupervised anomaly detection with generative adversarial networks. Thomas Schlegl, Philipp Seeb\u00f6ck, Georg Sebastian M Waldstein, Ursula Langs, Schmidt-Erfurth, Medical image analysis. 542Thomas Schlegl, Philipp Seeb\u00f6ck, Sebastian M Waldstein, Georg Langs, and Ursula Schmidt-Erfurth. f-anogan: Fast unsupervised anomaly detection with generative adversarial networks. Medical image analysis, 54:30-44, 2019. 1, 2\n\nUnsupervised anomaly detection with generative adversarial networks to guide marker discovery. Thomas Schlegl, Philipp Seeb\u00f6ck, Ursula Sebastian M Waldstein, Georg Schmidt-Erfurth, Langs, International conference on information processing in medical imaging. SpringerThomas Schlegl, Philipp Seeb\u00f6ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on in- formation processing in medical imaging, pages 146-157. Springer, 2017. 2\n\nAnoseg: Anomaly segmentation network using self-supervised learning. Jouwon Song, Kyeongbo Kong, Ye-In Park, Seong-Gyun Kim, Suk-Ju Kang, arXiv:2110.03396arXiv preprintJouwon Song, Kyeongbo Kong, Ye-In Park, Seong-Gyun Kim, and Suk-Ju Kang. Anoseg: Anomaly segmenta- tion network using self-supervised learning. arXiv preprint arXiv:2110.03396, 2021. 3\n\nq-space novelty detection with variational autoencoders. Aleksei Vasilev, Vladimir Golkov, Marc Meissner, Ilona Lipp, Eleonora Sgarlata, Valentina Tomassini, Derek K Jones, Daniel Cremers, Computational Diffusion MRI. SpringerAleksei Vasilev, Vladimir Golkov, Marc Meissner, Ilona Lipp, Eleonora Sgarlata, Valentina Tomassini, Derek K Jones, and Daniel Cremers. q-space novelty detection with variational autoencoders. In Computational Diffusion MRI, pages 113-124. Springer, 2020. 2\n\nStudent-teacher feature pyramid matching for anomaly detection. Guodong Wang, Shumin Han, Errui Ding, Di Huang, Proceedings of the British Machine Vision Conference. the British Machine Vision Conference7Guodong Wang, Shumin Han, Errui Ding, and Di Huang. Student-teacher feature pyramid matching for anomaly de- tection. In Proceedings of the British Machine Vision Con- ference, 2021. 1, 3, 4, 6, 7, 8\n\nBone metastatic tumor detection based on anogan using ct images. Haruna Watanabe, Ren Togo, Takahiro Ogawa, Miki Haseyama, 2019 IEEE 1st Global Conference on Life Sciences and Technologies (LifeTech). IEEEHaruna Watanabe, Ren Togo, Takahiro Ogawa, and Miki Haseyama. Bone metastatic tumor detection based on anogan using ct images. In 2019 IEEE 1st Global Confer- ence on Life Sciences and Technologies (LifeTech), pages 235-236. IEEE, 2019. 1\n\nReconstruction student with attention for student-teacher pyramid matching. Shinji Yamada, Kazuhiro Hotta, arXiv:2111.1537614arXiv preprintShinji Yamada and Kazuhiro Hotta. Reconstruction student with attention for student-teacher pyramid matching. arXiv preprint arXiv:2111.15376, 2021. 1, 3, 4\n\nPatch svdd: Patch-level svdd for anomaly detection and segmentation. Jihun Yi, Sungroh Yoon, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer Vision2020Jihun Yi and Sungroh Yoon. Patch svdd: Patch-level svdd for anomaly detection and segmentation. In Proceedings of the Asian Conference on Computer Vision, 2020. 3\n\nFastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, Liwei Wu, arXiv:2111.07677arXiv preprintJiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, and Liwei Wu. Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. arXiv preprint arXiv:2111.07677, 2021. 3\n\nDraema discriminatively trained reconstruction embedding for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Sko\u010daj, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision67Vitjan Zavrtanik, Matej Kristan, and Danijel Sko\u010daj. Draem- a discriminatively trained reconstruction embedding for sur- face anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8330- 8339, 2021. 1, 3, 5, 6, 7\n\nDsra dual subspace re-projection network for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Sko\u010daj, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Springer67Vitjan Zavrtanik, Matej Kristan, and Danijel Sko\u010daj. Dsr- a dual subspace re-projection network for surface anomaly detection. In Proceedings of the European Conference on Computer Vision (ECCV), pages 539-554. Springer, 2022. 3, 6, 7\n\nmixup: Beyond empirical risk minimization. Hongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, Proceedings of the International Conference for Learning Representations (ICLR). the International Conference for Learning Representations (ICLR)Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. In Proceedings of the International Conference for Learning Representations (ICLR), 2018. 3\n", "annotations": {"author": "[{\"end\":145,\"start\":80},{\"end\":163,\"start\":146},{\"end\":178,\"start\":164},{\"end\":198,\"start\":179},{\"end\":237,\"start\":199},{\"end\":295,\"start\":238}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":85},{\"end\":154,\"start\":152},{\"end\":169,\"start\":167},{\"end\":189,\"start\":184},{\"end\":211,\"start\":207},{\"end\":247,\"start\":243}]", "author_first_name": "[{\"end\":84,\"start\":80},{\"end\":151,\"start\":146},{\"end\":166,\"start\":164},{\"end\":183,\"start\":179},{\"end\":206,\"start\":199},{\"end\":242,\"start\":238}]", "author_affiliation": "[{\"end\":144,\"start\":124},{\"end\":162,\"start\":156},{\"end\":177,\"start\":171},{\"end\":197,\"start\":191},{\"end\":236,\"start\":230},{\"end\":294,\"start\":274}]", "title": "[{\"end\":77,\"start\":1},{\"end\":372,\"start\":296}]", "venue": null, "abstract": "[{\"end\":1659,\"start\":374}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1815,\"start\":1811},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1818,\"start\":1815},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1850,\"start\":1846},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1853,\"start\":1850},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1882,\"start\":1878},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1885,\"start\":1882},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2408,\"start\":2405},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2410,\"start\":2408},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2413,\"start\":2410},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2416,\"start\":2413},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2419,\"start\":2416},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2520,\"start\":2516},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2946,\"start\":2942},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2949,\"start\":2946},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3372,\"start\":3368},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3638,\"start\":3634},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5756,\"start\":5753},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6651,\"start\":6648},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6680,\"start\":6677},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6683,\"start\":6680},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6722,\"start\":6718},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6725,\"start\":6722},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6728,\"start\":6725},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7123,\"start\":7119},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7317,\"start\":7314},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7320,\"start\":7317},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7323,\"start\":7320},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7326,\"start\":7323},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7688,\"start\":7684},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7691,\"start\":7688},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7732,\"start\":7729},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7735,\"start\":7732},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7738,\"start\":7735},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7741,\"start\":7738},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8395,\"start\":8392},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8548,\"start\":8544},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8558,\"start\":8554},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8667,\"start\":8664},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8677,\"start\":8673},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9294,\"start\":9290},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9310,\"start\":9306},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9381,\"start\":9377},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9438,\"start\":9434},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9622,\"start\":9618},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9647,\"start\":9643},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9806,\"start\":9802},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9809,\"start\":9806},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9812,\"start\":9809},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9902,\"start\":9898},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11034,\"start\":11030},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11379,\"start\":11376},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11381,\"start\":11379},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11663,\"start\":11659},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11877,\"start\":11873},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11966,\"start\":11962},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12125,\"start\":12121},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12128,\"start\":12125},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13856,\"start\":13853},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14522,\"start\":14519},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14525,\"start\":14522},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14913,\"start\":14909},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14974,\"start\":14970},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16226,\"start\":16222},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16229,\"start\":16226},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17327,\"start\":17324},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17631,\"start\":17627},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19176,\"start\":19173},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19698,\"start\":19695},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19753,\"start\":19749},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20270,\"start\":20266},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20534,\"start\":20531},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21715,\"start\":21712},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21725,\"start\":21721},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21739,\"start\":21735},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21750,\"start\":21746},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21759,\"start\":21755},{\"end\":21771,\"start\":21770},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21868,\"start\":21865},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21870,\"start\":21868},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21873,\"start\":21870},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21953,\"start\":21949},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21956,\"start\":21953},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21959,\"start\":21956},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22481,\"start\":22477},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22487,\"start\":22483},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22497,\"start\":22493},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23274,\"start\":23270},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24188,\"start\":24184},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24191,\"start\":24188},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24461,\"start\":24457},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24472,\"start\":24468},{\"end\":24484,\"start\":24483},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24590,\"start\":24586},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24593,\"start\":24590},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26342,\"start\":26338},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27403,\"start\":27400},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29172,\"start\":29168},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29175,\"start\":29172},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29297,\"start\":29293},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32473,\"start\":32469}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29577,\"start\":28955},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29710,\"start\":29578},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29926,\"start\":29711},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32155,\"start\":29927},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33218,\"start\":32156}]", "paragraph": "[{\"end\":2306,\"start\":1675},{\"end\":3847,\"start\":2308},{\"end\":5648,\"start\":3849},{\"end\":6013,\"start\":5650},{\"end\":6495,\"start\":6015},{\"end\":7979,\"start\":6513},{\"end\":8004,\"start\":7981},{\"end\":9013,\"start\":8006},{\"end\":9943,\"start\":9015},{\"end\":10097,\"start\":9954},{\"end\":10877,\"start\":10099},{\"end\":11382,\"start\":10910},{\"end\":12025,\"start\":11439},{\"end\":12554,\"start\":12063},{\"end\":13464,\"start\":12556},{\"end\":14897,\"start\":13466},{\"end\":15510,\"start\":14899},{\"end\":16009,\"start\":15512},{\"end\":16570,\"start\":16219},{\"end\":17558,\"start\":16572},{\"end\":18360,\"start\":17560},{\"end\":18518,\"start\":18400},{\"end\":18656,\"start\":18629},{\"end\":19105,\"start\":18670},{\"end\":19874,\"start\":19131},{\"end\":20067,\"start\":19897},{\"end\":20271,\"start\":20069},{\"end\":21545,\"start\":20273},{\"end\":22362,\"start\":21547},{\"end\":22794,\"start\":22374},{\"end\":23032,\"start\":22796},{\"end\":23456,\"start\":23034},{\"end\":23491,\"start\":23458},{\"end\":24106,\"start\":23493},{\"end\":25005,\"start\":24108},{\"end\":25586,\"start\":25007},{\"end\":25911,\"start\":25588},{\"end\":27037,\"start\":25932},{\"end\":27206,\"start\":27039},{\"end\":28068,\"start\":27238},{\"end\":28290,\"start\":28070},{\"end\":28954,\"start\":28305}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11438,\"start\":11383},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16195,\"start\":16010},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18399,\"start\":18361},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18628,\"start\":18519}]", "table_ref": "[{\"end\":24492,\"start\":24485},{\"end\":28123,\"start\":28116}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1673,\"start\":1661},{\"attributes\":{\"n\":\"2.\"},\"end\":6511,\"start\":6498},{\"attributes\":{\"n\":\"3.\"},\"end\":9952,\"start\":9946},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10908,\"start\":10880},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12061,\"start\":12028},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16217,\"start\":16197},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18668,\"start\":18659},{\"attributes\":{\"n\":\"4.\"},\"end\":19119,\"start\":19108},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19129,\"start\":19122},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19895,\"start\":19877},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22372,\"start\":22365},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25930,\"start\":25914},{\"end\":27236,\"start\":27209},{\"attributes\":{\"n\":\"5.\"},\"end\":28303,\"start\":28293},{\"end\":28966,\"start\":28956},{\"end\":29589,\"start\":29579},{\"end\":29722,\"start\":29712}]", "table": "[{\"end\":32155,\"start\":30038},{\"end\":33218,\"start\":32693}]", "figure_caption": "[{\"end\":29577,\"start\":28968},{\"end\":29710,\"start\":29591},{\"end\":29926,\"start\":29724},{\"end\":30038,\"start\":29929},{\"end\":32693,\"start\":32158}]", "figure_ref": "[{\"end\":3930,\"start\":3924},{\"end\":4299,\"start\":4291},{\"end\":10123,\"start\":10117},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22108,\"start\":22102},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24894,\"start\":24888},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25085,\"start\":25079}]", "bib_author_first_name": "[{\"end\":33312,\"start\":33303},{\"end\":33327,\"start\":33319},{\"end\":33343,\"start\":33338},{\"end\":33362,\"start\":33356},{\"end\":33739,\"start\":33735},{\"end\":33757,\"start\":33750},{\"end\":33771,\"start\":33766},{\"end\":33791,\"start\":33784},{\"end\":34313,\"start\":34309},{\"end\":34331,\"start\":34324},{\"end\":34345,\"start\":34340},{\"end\":34365,\"start\":34358},{\"end\":34909,\"start\":34905},{\"end\":34925,\"start\":34920},{\"end\":34939,\"start\":34932},{\"end\":34953,\"start\":34948},{\"end\":34973,\"start\":34966},{\"end\":35707,\"start\":35696},{\"end\":35720,\"start\":35714},{\"end\":35740,\"start\":35733},{\"end\":35756,\"start\":35751},{\"end\":35769,\"start\":35765},{\"end\":35771,\"start\":35770},{\"end\":36196,\"start\":36190},{\"end\":36214,\"start\":36205},{\"end\":36594,\"start\":36591},{\"end\":36607,\"start\":36602},{\"end\":36879,\"start\":36873},{\"end\":36897,\"start\":36888},{\"end\":36915,\"start\":36906},{\"end\":36931,\"start\":36924},{\"end\":37327,\"start\":37321},{\"end\":37340,\"start\":37334},{\"end\":37766,\"start\":37763},{\"end\":37776,\"start\":37773},{\"end\":37790,\"start\":37783},{\"end\":37805,\"start\":37799},{\"end\":37813,\"start\":37810},{\"end\":37820,\"start\":37818},{\"end\":38199,\"start\":38191},{\"end\":38210,\"start\":38209},{\"end\":38483,\"start\":38477},{\"end\":38500,\"start\":38493},{\"end\":38513,\"start\":38508},{\"end\":38880,\"start\":38875},{\"end\":38896,\"start\":38892},{\"end\":38913,\"start\":38907},{\"end\":39395,\"start\":39388},{\"end\":39407,\"start\":39400},{\"end\":39423,\"start\":39415},{\"end\":39433,\"start\":39429},{\"end\":39888,\"start\":39883},{\"end\":39899,\"start\":39894},{\"end\":39912,\"start\":39905},{\"end\":39924,\"start\":39918},{\"end\":40279,\"start\":40269},{\"end\":40290,\"start\":40284},{\"end\":40304,\"start\":40297},{\"end\":40316,\"start\":40311},{\"end\":40807,\"start\":40799},{\"end\":40818,\"start\":40813},{\"end\":40830,\"start\":40826},{\"end\":41235,\"start\":41230},{\"end\":41251,\"start\":41241},{\"end\":41264,\"start\":41256},{\"end\":41648,\"start\":41641},{\"end\":41650,\"start\":41649},{\"end\":41988,\"start\":41980},{\"end\":42000,\"start\":41995},{\"end\":42013,\"start\":42006},{\"end\":42567,\"start\":42557},{\"end\":42582,\"start\":42576},{\"end\":42598,\"start\":42594},{\"end\":43065,\"start\":43057},{\"end\":43078,\"start\":43074},{\"end\":43428,\"start\":43422},{\"end\":43444,\"start\":43437},{\"end\":43459,\"start\":43454},{\"end\":43849,\"start\":43842},{\"end\":43861,\"start\":43856},{\"end\":43877,\"start\":43870},{\"end\":43894,\"start\":43886},{\"end\":43912,\"start\":43906},{\"end\":43924,\"start\":43919},{\"end\":44490,\"start\":44484},{\"end\":44502,\"start\":44498},{\"end\":44802,\"start\":44790},{\"end\":44818,\"start\":44811},{\"end\":44835,\"start\":44828},{\"end\":45409,\"start\":45403},{\"end\":45426,\"start\":45419},{\"end\":45441,\"start\":45436},{\"end\":45471,\"start\":45465},{\"end\":45851,\"start\":45845},{\"end\":45868,\"start\":45861},{\"end\":45884,\"start\":45878},{\"end\":45913,\"start\":45908},{\"end\":46394,\"start\":46388},{\"end\":46409,\"start\":46401},{\"end\":46421,\"start\":46416},{\"end\":46438,\"start\":46428},{\"end\":46450,\"start\":46444},{\"end\":46737,\"start\":46730},{\"end\":46755,\"start\":46747},{\"end\":46768,\"start\":46764},{\"end\":46784,\"start\":46779},{\"end\":46799,\"start\":46791},{\"end\":46819,\"start\":46810},{\"end\":46836,\"start\":46831},{\"end\":46838,\"start\":46837},{\"end\":46852,\"start\":46846},{\"end\":47229,\"start\":47222},{\"end\":47242,\"start\":47236},{\"end\":47253,\"start\":47248},{\"end\":47262,\"start\":47260},{\"end\":47634,\"start\":47628},{\"end\":47648,\"start\":47645},{\"end\":47663,\"start\":47655},{\"end\":47675,\"start\":47671},{\"end\":48090,\"start\":48084},{\"end\":48107,\"start\":48099},{\"end\":48379,\"start\":48374},{\"end\":48391,\"start\":48384},{\"end\":48751,\"start\":48745},{\"end\":48758,\"start\":48756},{\"end\":48771,\"start\":48766},{\"end\":48781,\"start\":48778},{\"end\":48794,\"start\":48786},{\"end\":48802,\"start\":48799},{\"end\":48814,\"start\":48809},{\"end\":49147,\"start\":49141},{\"end\":49164,\"start\":49159},{\"end\":49181,\"start\":49174},{\"end\":49659,\"start\":49653},{\"end\":49676,\"start\":49671},{\"end\":49693,\"start\":49686},{\"end\":50112,\"start\":50106},{\"end\":50129,\"start\":50120},{\"end\":50142,\"start\":50137}]", "bib_author_last_name": "[{\"end\":33317,\"start\":33313},{\"end\":33336,\"start\":33328},{\"end\":33354,\"start\":33344},{\"end\":33368,\"start\":33363},{\"end\":33748,\"start\":33740},{\"end\":33764,\"start\":33758},{\"end\":33782,\"start\":33772},{\"end\":33798,\"start\":33792},{\"end\":34322,\"start\":34314},{\"end\":34338,\"start\":34332},{\"end\":34356,\"start\":34346},{\"end\":34372,\"start\":34366},{\"end\":34918,\"start\":34910},{\"end\":34930,\"start\":34926},{\"end\":34946,\"start\":34940},{\"end\":34964,\"start\":34954},{\"end\":34980,\"start\":34974},{\"end\":35712,\"start\":35708},{\"end\":35731,\"start\":35721},{\"end\":35749,\"start\":35741},{\"end\":35763,\"start\":35757},{\"end\":35778,\"start\":35772},{\"end\":36203,\"start\":36197},{\"end\":36219,\"start\":36215},{\"end\":36600,\"start\":36595},{\"end\":36614,\"start\":36608},{\"end\":36886,\"start\":36880},{\"end\":36904,\"start\":36898},{\"end\":36922,\"start\":36916},{\"end\":36940,\"start\":36932},{\"end\":37332,\"start\":37328},{\"end\":37343,\"start\":37341},{\"end\":37771,\"start\":37767},{\"end\":37781,\"start\":37777},{\"end\":37797,\"start\":37791},{\"end\":37808,\"start\":37806},{\"end\":37816,\"start\":37814},{\"end\":37828,\"start\":37821},{\"end\":38207,\"start\":38200},{\"end\":38217,\"start\":38211},{\"end\":38225,\"start\":38219},{\"end\":38491,\"start\":38484},{\"end\":38506,\"start\":38501},{\"end\":38523,\"start\":38514},{\"end\":38890,\"start\":38881},{\"end\":38905,\"start\":38897},{\"end\":38920,\"start\":38914},{\"end\":39398,\"start\":39396},{\"end\":39413,\"start\":39408},{\"end\":39427,\"start\":39424},{\"end\":39437,\"start\":39434},{\"end\":39892,\"start\":39889},{\"end\":39903,\"start\":39900},{\"end\":39916,\"start\":39913},{\"end\":39929,\"start\":39925},{\"end\":40282,\"start\":40280},{\"end\":40295,\"start\":40291},{\"end\":40309,\"start\":40305},{\"end\":40324,\"start\":40317},{\"end\":40811,\"start\":40808},{\"end\":40824,\"start\":40819},{\"end\":40839,\"start\":40831},{\"end\":41239,\"start\":41236},{\"end\":41254,\"start\":41252},{\"end\":41271,\"start\":41265},{\"end\":41665,\"start\":41651},{\"end\":41675,\"start\":41667},{\"end\":41682,\"start\":41677},{\"end\":41993,\"start\":41989},{\"end\":42004,\"start\":42001},{\"end\":42018,\"start\":42014},{\"end\":42574,\"start\":42568},{\"end\":42592,\"start\":42583},{\"end\":42604,\"start\":42599},{\"end\":43072,\"start\":43066},{\"end\":43083,\"start\":43079},{\"end\":43435,\"start\":43429},{\"end\":43452,\"start\":43445},{\"end\":43466,\"start\":43460},{\"end\":43854,\"start\":43850},{\"end\":43868,\"start\":43862},{\"end\":43884,\"start\":43878},{\"end\":43904,\"start\":43895},{\"end\":43917,\"start\":43913},{\"end\":43931,\"start\":43925},{\"end\":44496,\"start\":44491},{\"end\":44513,\"start\":44503},{\"end\":44809,\"start\":44803},{\"end\":44826,\"start\":44819},{\"end\":44847,\"start\":44836},{\"end\":44866,\"start\":44849},{\"end\":44882,\"start\":44868},{\"end\":45417,\"start\":45410},{\"end\":45434,\"start\":45427},{\"end\":45463,\"start\":45442},{\"end\":45477,\"start\":45472},{\"end\":45494,\"start\":45479},{\"end\":45859,\"start\":45852},{\"end\":45876,\"start\":45869},{\"end\":45906,\"start\":45885},{\"end\":45929,\"start\":45914},{\"end\":45936,\"start\":45931},{\"end\":46399,\"start\":46395},{\"end\":46414,\"start\":46410},{\"end\":46426,\"start\":46422},{\"end\":46442,\"start\":46439},{\"end\":46455,\"start\":46451},{\"end\":46745,\"start\":46738},{\"end\":46762,\"start\":46756},{\"end\":46777,\"start\":46769},{\"end\":46789,\"start\":46785},{\"end\":46808,\"start\":46800},{\"end\":46829,\"start\":46820},{\"end\":46844,\"start\":46839},{\"end\":46860,\"start\":46853},{\"end\":47234,\"start\":47230},{\"end\":47246,\"start\":47243},{\"end\":47258,\"start\":47254},{\"end\":47268,\"start\":47263},{\"end\":47643,\"start\":47635},{\"end\":47653,\"start\":47649},{\"end\":47669,\"start\":47664},{\"end\":47684,\"start\":47676},{\"end\":48097,\"start\":48091},{\"end\":48113,\"start\":48108},{\"end\":48382,\"start\":48380},{\"end\":48396,\"start\":48392},{\"end\":48754,\"start\":48752},{\"end\":48764,\"start\":48759},{\"end\":48776,\"start\":48772},{\"end\":48784,\"start\":48782},{\"end\":48797,\"start\":48795},{\"end\":48807,\"start\":48803},{\"end\":48817,\"start\":48815},{\"end\":49157,\"start\":49148},{\"end\":49172,\"start\":49165},{\"end\":49188,\"start\":49182},{\"end\":49669,\"start\":49660},{\"end\":49684,\"start\":49677},{\"end\":49700,\"start\":49694},{\"end\":50118,\"start\":50113},{\"end\":50135,\"start\":50130},{\"end\":50157,\"start\":50143},{\"end\":50168,\"start\":50159}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4792240},\"end\":33653,\"start\":33220},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":189857704},\"end\":34213,\"start\":33655},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":207880670},\"end\":34809,\"start\":34215},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49567058},\"end\":35581,\"start\":34811},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3429309},\"end\":36135,\"start\":35583},{\"attributes\":{\"id\":\"b5\"},\"end\":36526,\"start\":36137},{\"attributes\":{\"doi\":\"arXiv:2005.02357\",\"id\":\"b6\"},\"end\":36784,\"start\":36528},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":226976039},\"end\":37250,\"start\":36786},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":246285427},\"end\":37708,\"start\":37252},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":38119,\"start\":37710},{\"attributes\":{\"doi\":\"arXiv:1708.04552\",\"id\":\"b10\"},\"end\":38407,\"start\":38121},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4009713},\"end\":38769,\"start\":38409},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":236447794},\"end\":39340,\"start\":38771},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206594692},\"end\":39787,\"start\":39342},{\"attributes\":{\"id\":\"b14\"},\"end\":40192,\"start\":39789},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":233204792},\"end\":40728,\"start\":40194},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206771220},\"end\":41171,\"start\":40730},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52290309},\"end\":41547,\"start\":41173},{\"attributes\":{\"doi\":\"arXiv:1811.08495\",\"id\":\"b18\"},\"end\":41901,\"start\":41549},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":212725213},\"end\":42468,\"start\":41903},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":84186723},\"end\":43009,\"start\":42470},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":233423406},\"end\":43327,\"start\":43011},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":218971560},\"end\":43786,\"start\":43329},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":235436036},\"end\":44359,\"start\":43788},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14081058},\"end\":44726,\"start\":44361},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":227126845},\"end\":45317,\"start\":44728},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":73516151},\"end\":45748,\"start\":45319},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":17427022},\"end\":46317,\"start\":45750},{\"attributes\":{\"doi\":\"arXiv:2110.03396\",\"id\":\"b28\"},\"end\":46671,\"start\":46319},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":47004930},\"end\":47156,\"start\":46673},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":240070818},\"end\":47561,\"start\":47158},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":204939606},\"end\":48006,\"start\":47563},{\"attributes\":{\"doi\":\"arXiv:2111.15376\",\"id\":\"b32\"},\"end\":48303,\"start\":48008},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":220250825},\"end\":48659,\"start\":48305},{\"attributes\":{\"doi\":\"arXiv:2111.07677\",\"id\":\"b34\"},\"end\":49051,\"start\":48661},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":237142564},\"end\":49579,\"start\":49053},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":251253406},\"end\":50061,\"start\":49581},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3162051},\"end\":50519,\"start\":50063}]", "bib_title": "[{\"end\":33301,\"start\":33220},{\"end\":33733,\"start\":33655},{\"end\":34307,\"start\":34215},{\"end\":34903,\"start\":34811},{\"end\":35694,\"start\":35583},{\"end\":36188,\"start\":36137},{\"end\":36871,\"start\":36786},{\"end\":37319,\"start\":37252},{\"end\":37761,\"start\":37710},{\"end\":38475,\"start\":38409},{\"end\":38873,\"start\":38771},{\"end\":39386,\"start\":39342},{\"end\":40267,\"start\":40194},{\"end\":40797,\"start\":40730},{\"end\":41228,\"start\":41173},{\"end\":41978,\"start\":41903},{\"end\":42555,\"start\":42470},{\"end\":43055,\"start\":43011},{\"end\":43420,\"start\":43329},{\"end\":43840,\"start\":43788},{\"end\":44482,\"start\":44361},{\"end\":44788,\"start\":44728},{\"end\":45401,\"start\":45319},{\"end\":45843,\"start\":45750},{\"end\":46728,\"start\":46673},{\"end\":47220,\"start\":47158},{\"end\":47626,\"start\":47563},{\"end\":48372,\"start\":48305},{\"end\":49139,\"start\":49053},{\"end\":49651,\"start\":49581},{\"end\":50104,\"start\":50063}]", "bib_author": "[{\"end\":33319,\"start\":33303},{\"end\":33338,\"start\":33319},{\"end\":33356,\"start\":33338},{\"end\":33370,\"start\":33356},{\"end\":33750,\"start\":33735},{\"end\":33766,\"start\":33750},{\"end\":33784,\"start\":33766},{\"end\":33800,\"start\":33784},{\"end\":34324,\"start\":34309},{\"end\":34340,\"start\":34324},{\"end\":34358,\"start\":34340},{\"end\":34374,\"start\":34358},{\"end\":34920,\"start\":34905},{\"end\":34932,\"start\":34920},{\"end\":34948,\"start\":34932},{\"end\":34966,\"start\":34948},{\"end\":34982,\"start\":34966},{\"end\":35714,\"start\":35696},{\"end\":35733,\"start\":35714},{\"end\":35751,\"start\":35733},{\"end\":35765,\"start\":35751},{\"end\":35780,\"start\":35765},{\"end\":36205,\"start\":36190},{\"end\":36221,\"start\":36205},{\"end\":36602,\"start\":36591},{\"end\":36616,\"start\":36602},{\"end\":36888,\"start\":36873},{\"end\":36906,\"start\":36888},{\"end\":36924,\"start\":36906},{\"end\":36942,\"start\":36924},{\"end\":37334,\"start\":37321},{\"end\":37345,\"start\":37334},{\"end\":37773,\"start\":37763},{\"end\":37783,\"start\":37773},{\"end\":37799,\"start\":37783},{\"end\":37810,\"start\":37799},{\"end\":37818,\"start\":37810},{\"end\":37830,\"start\":37818},{\"end\":38209,\"start\":38191},{\"end\":38219,\"start\":38209},{\"end\":38227,\"start\":38219},{\"end\":38493,\"start\":38477},{\"end\":38508,\"start\":38493},{\"end\":38525,\"start\":38508},{\"end\":38892,\"start\":38875},{\"end\":38907,\"start\":38892},{\"end\":38922,\"start\":38907},{\"end\":39400,\"start\":39388},{\"end\":39415,\"start\":39400},{\"end\":39429,\"start\":39415},{\"end\":39439,\"start\":39429},{\"end\":39894,\"start\":39883},{\"end\":39905,\"start\":39894},{\"end\":39918,\"start\":39905},{\"end\":39931,\"start\":39918},{\"end\":40284,\"start\":40269},{\"end\":40297,\"start\":40284},{\"end\":40311,\"start\":40297},{\"end\":40326,\"start\":40311},{\"end\":40813,\"start\":40799},{\"end\":40826,\"start\":40813},{\"end\":40841,\"start\":40826},{\"end\":41241,\"start\":41230},{\"end\":41256,\"start\":41241},{\"end\":41273,\"start\":41256},{\"end\":41667,\"start\":41641},{\"end\":41677,\"start\":41667},{\"end\":41684,\"start\":41677},{\"end\":41995,\"start\":41980},{\"end\":42006,\"start\":41995},{\"end\":42020,\"start\":42006},{\"end\":42576,\"start\":42557},{\"end\":42594,\"start\":42576},{\"end\":42606,\"start\":42594},{\"end\":43074,\"start\":43057},{\"end\":43085,\"start\":43074},{\"end\":43437,\"start\":43422},{\"end\":43454,\"start\":43437},{\"end\":43468,\"start\":43454},{\"end\":43856,\"start\":43842},{\"end\":43870,\"start\":43856},{\"end\":43886,\"start\":43870},{\"end\":43906,\"start\":43886},{\"end\":43919,\"start\":43906},{\"end\":43933,\"start\":43919},{\"end\":44498,\"start\":44484},{\"end\":44515,\"start\":44498},{\"end\":44811,\"start\":44790},{\"end\":44828,\"start\":44811},{\"end\":44849,\"start\":44828},{\"end\":44868,\"start\":44849},{\"end\":44884,\"start\":44868},{\"end\":45419,\"start\":45403},{\"end\":45436,\"start\":45419},{\"end\":45465,\"start\":45436},{\"end\":45479,\"start\":45465},{\"end\":45496,\"start\":45479},{\"end\":45861,\"start\":45845},{\"end\":45878,\"start\":45861},{\"end\":45908,\"start\":45878},{\"end\":45931,\"start\":45908},{\"end\":45938,\"start\":45931},{\"end\":46401,\"start\":46388},{\"end\":46416,\"start\":46401},{\"end\":46428,\"start\":46416},{\"end\":46444,\"start\":46428},{\"end\":46457,\"start\":46444},{\"end\":46747,\"start\":46730},{\"end\":46764,\"start\":46747},{\"end\":46779,\"start\":46764},{\"end\":46791,\"start\":46779},{\"end\":46810,\"start\":46791},{\"end\":46831,\"start\":46810},{\"end\":46846,\"start\":46831},{\"end\":46862,\"start\":46846},{\"end\":47236,\"start\":47222},{\"end\":47248,\"start\":47236},{\"end\":47260,\"start\":47248},{\"end\":47270,\"start\":47260},{\"end\":47645,\"start\":47628},{\"end\":47655,\"start\":47645},{\"end\":47671,\"start\":47655},{\"end\":47686,\"start\":47671},{\"end\":48099,\"start\":48084},{\"end\":48115,\"start\":48099},{\"end\":48384,\"start\":48374},{\"end\":48398,\"start\":48384},{\"end\":48756,\"start\":48745},{\"end\":48766,\"start\":48756},{\"end\":48778,\"start\":48766},{\"end\":48786,\"start\":48778},{\"end\":48799,\"start\":48786},{\"end\":48809,\"start\":48799},{\"end\":48819,\"start\":48809},{\"end\":49159,\"start\":49141},{\"end\":49174,\"start\":49159},{\"end\":49190,\"start\":49174},{\"end\":49671,\"start\":49653},{\"end\":49686,\"start\":49671},{\"end\":49702,\"start\":49686},{\"end\":50120,\"start\":50106},{\"end\":50137,\"start\":50120},{\"end\":50159,\"start\":50137},{\"end\":50170,\"start\":50159}]", "bib_venue": "[{\"end\":33411,\"start\":33370},{\"end\":33881,\"start\":33800},{\"end\":34455,\"start\":34374},{\"end\":35110,\"start\":34982},{\"end\":35842,\"start\":35780},{\"end\":36284,\"start\":36221},{\"end\":36589,\"start\":36528},{\"end\":36989,\"start\":36942},{\"end\":37426,\"start\":37345},{\"end\":37893,\"start\":37830},{\"end\":38189,\"start\":38121},{\"end\":38577,\"start\":38525},{\"end\":39002,\"start\":38922},{\"end\":39516,\"start\":39439},{\"end\":39881,\"start\":39789},{\"end\":40407,\"start\":40326},{\"end\":40908,\"start\":40841},{\"end\":41325,\"start\":41273},{\"end\":41639,\"start\":41549},{\"end\":42101,\"start\":42020},{\"end\":42687,\"start\":42606},{\"end\":43142,\"start\":43085},{\"end\":43532,\"start\":43468},{\"end\":44014,\"start\":43933},{\"end\":44523,\"start\":44515},{\"end\":44965,\"start\":44884},{\"end\":45518,\"start\":45496},{\"end\":46007,\"start\":45938},{\"end\":46386,\"start\":46319},{\"end\":46889,\"start\":46862},{\"end\":47322,\"start\":47270},{\"end\":47762,\"start\":47686},{\"end\":48082,\"start\":48008},{\"end\":48452,\"start\":48398},{\"end\":48743,\"start\":48661},{\"end\":49261,\"start\":49190},{\"end\":49766,\"start\":49702},{\"end\":50249,\"start\":50170},{\"end\":33949,\"start\":33883},{\"end\":34523,\"start\":34457},{\"end\":35225,\"start\":35112},{\"end\":37494,\"start\":37428},{\"end\":39069,\"start\":39004},{\"end\":39580,\"start\":39518},{\"end\":40475,\"start\":40409},{\"end\":40962,\"start\":40910},{\"end\":41364,\"start\":41327},{\"end\":42169,\"start\":42103},{\"end\":42755,\"start\":42689},{\"end\":44082,\"start\":44016},{\"end\":45033,\"start\":44967},{\"end\":47361,\"start\":47324},{\"end\":48493,\"start\":48454},{\"end\":49319,\"start\":49263},{\"end\":49817,\"start\":49768},{\"end\":50315,\"start\":50251}]"}}}, "year": 2023, "month": 12, "day": 17}