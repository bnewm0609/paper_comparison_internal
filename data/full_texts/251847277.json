{"id": 251847277, "updated": "2022-09-01 13:34:54.782", "metadata": {"title": "Structural Attention for Channel-Wise Adaptive Graph Convolution in Skeleton-Based Action Recognition", "authors": "[{\"first\":\"Ruihao\",\"last\":\"Qian\",\"middle\":[]},{\"first\":\"Jiewen\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jianxiu\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Shuang\",\"last\":\"Liang\",\"middle\":[]}]", "venue": "2022 IEEE International Conference on Multimedia and Expo (ICME)", "journal": "2022 IEEE International Conference on Multimedia and Expo (ICME)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In skeleton-based action recognition, graph convolutions to model human action dynamics have been widely implemented and achieved remarkable results. Among these convolutions, channel-wise adaptive graph convolution shows outstanding performance. However, this method focuses too much on capturing correlation between joints within each channel and lacks the capability of learning structural features, which are generally hidden in geometric property of the skeleton on spatial domain. Our proposed method (SA-GCN) introduces symmetry trajectory attention module to measure the relation between left and right part of body and part relation attention module for exploration of the attention on general relation of each part. Both modules are intended to make full use of structural features in skeleton, further strengthening advantages of graph convolution. Experiments on three datasets (NW-UCLA, NTU-RGB+D and NTU-RGB+D 120) demonstrate state-of-the-art performance of our model, especially on joint modality.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icmcs/QianWWL22", "doi": "10.1109/icme52920.2022.9859694"}}, "content": {"source": {"pdf_hash": "06a799419a3940b83c3adce8139dd8c8350814f9", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "acd660b4f8ffc0fa05f51a36ef9455b94a6c2080", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/06a799419a3940b83c3adce8139dd8c8350814f9.txt", "contents": "\nSTRUCTURAL ATTENTION FOR CHANNEL-WISE ADAPTIVE GRAPH CONVOLUTION IN SKELETON-BASED ACTION RECOGNITION\n\n\nRuihao Qian \nJiewen Wang \nJianxiu Wang wangjianxiu@tongji.edu.cn. \nShuang Liang shuangliang@tongji.edu.cn \n\nSchool of Software Engineering\nCollege of Civil Engineering\nTongji University\nChina\n\n\nKey Laboratory of Geotechnical and Underground Engineering of Ministry of Education\nTongji University\nChina\n\n\nTongji University\nChina\n\nSTRUCTURAL ATTENTION FOR CHANNEL-WISE ADAPTIVE GRAPH CONVOLUTION IN SKELETON-BASED ACTION RECOGNITION\n10.1109/ICME52920.2022.9859694Index Terms-Skeleton-based action recognitiongraph convolutional networksstructural attention modules\nIn skeleton-based action recognition, graph convolutions to model human action dynamics have been widely implemented and achieved remarkable results. Among these convolutions, channel-wise adaptive graph convolution shows outstanding performance. However, this method focuses too much on capturing correlation between joints within each channel and lacks the capability of learning structural features, which are generally hidden in geometric property of the skeleton on spatial domain. Our proposed method (SA-GCN) introduces symmetry trajectory attention module to measure the relation between left and right part of body and part relation attention module for exploration of the attention on general relation of each part. Both modules are intended to make full use of structural features in skeleton, further strengthening advantages of graph convolution. Experiments on three datasets (NW-UCLA, NTU-RGB+D and NTU-RGB+D 120) demonstrate state-of-the-art performance of our model, especially on joint modality.\n\nINTRODUCTION\n\nIn the past few years, action recognition makes promising progress in various applications, such as human-computer interaction. In particular, skeleton-based action recognition catches attention from researchers due to its environmental noise immunity, allowing recognition algorithms to focus on the geometric features of action.\n\nEarly deep-learning methods learn features from skeleton joints using CNN or RNN. However, these methods are inefficient compared to GCN-based methods because GCN is inherently better at processing graph-type data like skeleton. ST-GCN [1] firstly applies GCN for skeleton-based ac-* Corresponding author.  tion recognition. However, the aggregation of joint features in GCN is designed as fixed body structure, which is not adaptive to flexible action changes. Therefore, 2s-AGCN [2] implements self-attention mechanism to allow graph convolution to freely associate each pair of nodes. Followed by channel-wise adaptive methods [3,4], they treat graph convolution differently in each channel. These methods expert in capturing correlation between joints within each channel while they lack capability of structural feature learning, such as confusion of wearing a shoe and taking off a shoe in NTU-RGB+D [5] dataset to a serious degree. It is relatively easy to recognize this action if we focus on cooperation of different parts of body at the structural level.\n\nIn this paper, a novel structural attention method for channel-wise adaptive graph convolution is proposed to make full use of structural information in skeleton. To our knowledge, structural features in skeleton can be defined as features coming from joint groups composed of multiple joints, which are related to each other physically or logically. This definition is suitable for [6] as well. To be specific, there are two modules proposed to enhance structural capability of graph convolution: body symmetry trajectory attention module to group symmetric joints logically and part relation attention module to group joints of the same parts physically. The overview of them is clearly shown in Fig. 1.\n\nCooperation of human body is little considered in recent work. [7] proposes angular modality to recognize action. As a intermediate result, symmetric joint pairs are in use for calculation of the angular. In this case, Joint pairs are not formally applied for action recognition. Based on this, body symmetry trajectory attention module is proposed to measure the similarity of trajectory of the left-and-right symmetrical body part with cross-attention method. Our intention is that people tend to make most of an action with the cooperation of left-andright part, so attention to both parts offers positive effect on structural action recognition.\n\nMoreover, although there exist part-aware attention methods in recent works [8,9], most of them tend to split body parts at the beginning or build complex part division while our part attention module is applied for adjacency matrix in procedure of graph convolution. The importance of relation between different parts is our focus point. Moreover, our calculation is simple and efficient compared against [9]. Combining the two modules above, graph convolution networks obtain the capability of structural skeleton feature learning.\n\nIn conclusion, our contributions can be summarized as:\n\n\u2022 Body symmetry trajectory attention is proposed to measure symmetric similarity, through which the neural network gains the capability of learning whether left and right part of body are cooperating in an action.\n\n\u2022 Part relation attention module is proposed to measure part relation importance of adaptive adjacency matrix.\n\n\u2022 The two attention modules consist of structural attention physically and logically. Experimental results demonstrate benefits of structural attention. The proposed SA-GCN outperforms state-of-the-art methods on three skeleton-based action recognition benchmarks.\n\n\nRELATED WORK\n\n\nAttention on Skeleton-based Action Recognition\n\nGraph convolution for skeleton-based action recognition originates from ST-GCN [1]. In it, nature connectivity of joints is implemented to make rules of adjacency matrix for aggregation of features. Adaptive approaches [2,10] learn the logical topology of skeleton through attention or other mecha-nisms. Among these, adaptive adjacency matrix [2] is calculated from skeleton data with self-attention mechanisms. Based on this, channel-wise adaptive graph convolution methods [3,4,11] are proposed to lift restrictions of adaptive adjacency matrix from topology-shared to a combination of adjacency matrices with several channels. This method can be regarded as channel-wise extension of self-attention methods on skeleton-based action recognition.\n\n\nPart-Aware Graph Convolution\n\nHuman skeleton is a natural graph with five main body parts (i.e. torso, left arm, right arm, left leg and right leg). It is common to split different parts of human body for better action recognition. Part-aware graph convolutions [8,9] are capable to learn notable features from joints to parts. [8] employs attention mechanism to part-aware graph convolution. The difference between ours is that it splits part at the beginning while our part attention is applied for adaptive adjacency matrix in the intermediate calculation. [9] is the most similar to ours, but the way of graph pooling is different and it (20.7M) needs much more parameters than ours (2.5M). To sum up, Our method is a simple implementation of part attention to adaptive graph convolution.\n\n\nCross-Attention Module\n\nIn a general sense, there are two categories of implementation of cross-attention module. Firstly, it can match different modalities or descriptions of the same research object [12]. Secondly, it can build up relation between two different things, such as human-object relation [13]. In our work, the latter implementation is adopted with a novel data entry (i.e. input left and right part to cross-attention).\n\n\nTHE PROPOSED METHOD\n\n\nPreliminary\n\nHuman skeleton is represented as a graph denoted as\nG = {V, E}, where V = {v 1 , v 2 , ..., v N } is set of vertices (i.e.\njoints) and E is set of edges (i.e. bones) represented by ad-\njacency matrix A ij \u2208 {0, 1} N \u00d7N where A i = 1 if there is an edge between v i and v j .\nThe element of adjacency matrix a ij reflects the strength of relation between v i and v j . Let X \u2208 R N \u00d7T \u00d7C be the C dimension joint positions for N joints in T frame sequences. P indicates the number of parts.\n\n\nPart Relation Attention for Channel-Wise Adaptive Graph Convolution\n\nChannel-Wise Adaptive Graph Convolution. An essential part of our framework is the Channel-Wise Adaptive Graph Convolution, which is the previous work by CTR-GCN [4]. As shown in blue box in Fig. 2, this correlation modeling generates C dimension adaptive adjacency matrix M (X) from input X and aggregates information with A. At this step, channel-wise adaptive adjacency matrix A C is formulated as:\nA C = M (X) \u2295 A(1)\nIf A C aggregates with transformation XW by 1 \u00d7 1 convolution, this graph convolution is complete. Part relation attention module takes effect on A C before aggregation. Part Relation Attention Module for Adjacency Matrix. As shown in orange box of Fig. 2, this module takes A C as input adjacency matrix. The initial procedure is described as:\nA P C = Sof tmax(P ooling(A C ))(2)\nP ooling() includes two steps: Index Reorder() and Average P ooling(). The former denotes reordering A P C at two dimensions of (N, N ) in order to put joints of the same part together, and the way of reordering is by distinguishing indexes of joints. Table 1 takes NTU-RGB+D [5] dataset as an example. This operation splits joints equally, which makes convenience for subsequent 2D pooling. After sorting well, the latter is applied to transform A C to A P C with part size P and this procedure is shown in Fig. 1 (b) as well. After Sof tmax(), A P C carries the meaning of the importance of each part relation. The following procedure is:\nA P AC = U npooling(A P C ) A C(3)\nU npooling() returns A P C to joint size N and reorders it again. To be specific, this operation takes nearest mode of up sampling so joints from the same part share the same attention score. The result is marked as A JC . Then dot multiply operation applies it to A C , generating part attention channel-wise adaptive adjacency matrix A P AC .\nX o = A P AC XW(4)\nFinally, A P AC aggregates information of X to get output X o . \n\n\nTanh [T V C]\n\n\n[T V C]\n\n\n[T V C]\n\n\n[T V C]\n\n\n[T V C]\n\n\n[T V C]\n\n\n[T V C]\n\n[ \n\n\nSymmetry Trajectory Attention Module\n\nAs shown in Fig. 3, symmetry trajectory attention module splits body into left part and right part. Cross-attention is employed to generate relation between two parts. Joints in the middle part are removed from calculation since they take no effect of symmetric similarity. The initial procedure is: After dividing X into left part X L and right part X R with index on the spatial domain, this module calculates the relation Re between two parts with matrix multiplication \u2297. The relation Re is shaped like (V, T, T ). V means the number of joint symmetric pairs due to the alignment of left and right parts, which is marked with same color in Fig. 1 (a). T \u00d7 T shows the measurement for the trajectory of these joint pairs. The next procedure is shown as:\nX L , X R = Split(X, Index) Re = T anh(W L X L \u2297 W R X R )(5)X L = Re \u2297 X L W L X R = Re \u2297 X R W R X o = Concat(X L , X R , Index)(6)\nRelation instructs two parts to aggregate temporal information with matrix multiplication \u2297. Finally, operation Concat() concatenates two parts together with indexes. Concatenation is simple due to no intersection of two parts.\n\n\nModel Architecture\n\nThe model architecture is shown in Fig. 4. SA-GC consists of spatial convolution, temporal convolution and symmetry trajectory attention module. The entire network named SA-GCN is composed of ten SA-GC blocks. An average pooling and a softmax classifier are set to predict action labels.\n\nAmong SA-GC, spatial convolution is introduced at detail in 3.2. We follow the spatial partition from ST-GCN [1]. That is the reason why there are three spatial convolutions. Temporal convolution keeps the same as CTR-GCN [4]. Body Symmetry Trajectory Attention Module is the last main module in SA-GC and detailed in 3.3. It is worth noting that the summation coefficient \u03b1 is N size since each pair of symmetry joints takes no same effect on action recognition. The N -size \u03b1 is initialized zero and set learnable. Residual connection including Relu() activation is set at the end of SA-GC.  \n\n\nEXPERIMENTS\n\n\nDatasets\n\nNW-UCLA. NW-UCLA [14] is captured by three Kinect cameras from multiple viewpoints. It covers 10 action categories totally containing 1494 video clips, which are performed by 10 subjects. The evaluation protocol is training data from the first two cameras and testing data from the other. NTU-RGB+D. NTU-RGB+D [5] is a large-scale human action recognition dataset including 56,880 skeleton action sequences. 40 subjects perform each action sample captured by three Kinect v2 cameras from different views. The two benchmarks are Cross-Subject (X-Sub) and Cross-View (X-View). NTU-RGB+D 120. This dataset [15] extends NTU-RGB+D with additional 57,367 skeleton sequences over 60 extra action classes. 3 cameras from different views capture 113,945 samples performed by 106 volunteers. The two benchmarks are Cross-Subject (X-Sub) and Cross-Set (X-Set).\n\n\nImplementation Details\n\nAll experiments are conducted on RTX 3090 GPU with Py-Torch. Our model is trained with stochastic gradient descent (SGD). The Nesterov momentum is set to 0.9 and weight decay is 0.0004. The max training epoch is set to 70. Learning rate is set to 0.1 with the decays of a factor 0.1 at epoch 35 and 55. For NTU-RGB+D and NTU-RGB+D 120, the batch size is 64 and 16 as for NW-UCLA. Moreover, the data preprocessing and warmup strategy are kept the same as [4].   6. Visualization of two attention modules on Sitting Down. In Fig. (a), a map of PRA (i.e. A P C in Fig. 2) shows relation of different parts (i.e. T is torso, LA is Leg Arm, RA is Right Arm, LL is Left Leg and RL is Right Leg), and orange indicates high score while blue means low. In Fig. (b), relation of each pair is averaged to obtain its attention score and shown in skeleton. The bigger the red joint is, the higher attention score is achieved. \n\n\nAblation Study\n\nNW-UCLA with joint modality is employed to examine the effectiveness of proposed modules in our model. Effect of PRA and STA. To evaluate the effect of PRA, CTR-GC module in baseline [4] is substituted to PRA-AGC module, which keeps the same configuration as baseline except PRA. Baseline with STA means adding STA at the end of MS-TC module and remaining CTR-GC module unchanged. The results in Table 2 show that the two attention modules work together and achieve better performance. As for details, we observe from Fig. 5 that (1) STA is weak in non-symmetric action, such as carrying something, but PRA does well in it, since there is interaction between arm and torso when carrying things.\n\n(2) PRA is weak in action without explicit part relation, such as standing up. It confuses standing up with sitting down, and this kind of confusion greatly hinders its recognition. That is the reason why accuracy of baseline with PRA decreases in Table 2. However, STA is good at that because of the completely different symmetric body movement of the two action categories. (3) action like doffing is part relational and symmetric, so both modules recognize it well. The fusion of STA and PRA overcomes weaknesses of each  Table 5. Comparisons against state-of-the-art methods on NTU-RGB+D 120 in accuracy (%)\n\nMethods NTU-RGB+D 120 X-Sub (%) X-Set (%) 2s-AGCN [2] 82.9 84.9 MS-G3D [11] 86.9 88.4 PA-ResGCN-B19 [8] 87  Fig. 6. From it, PRA map shows that two legs and their relations to other parts convey more importance. Similarly in STA, higher attention scores appear on joints of two legs too. In conclusion, the two attention modules make sense of sitting down and verify the expectation.\n\n\nComparison against the State-of-the-Art\n\nMulti-stream fusion framework is widely adopted in SOTA methods. Based on this, we adopt same framework for fair comparison. To be specific, four modalities (i.e. joint, bone, joint motion and bone motion) are fused to obtain the final prediction action scores. Comparisons against related SOTA methods on NW-UCLA, NTU-RGB+D and NTU-RGB+D 120 are listed respectively in Tables 3, 4 and 5. On three datasets, our method outperforms existing methods under nearly all evaluation benchmarks. Joint data are the basic modality of skeleton, so the comparison based on this with baseline is shown at detail. As a result, our method takes effect significantly on joint modality over baseline. In the meantime, it keeps a few advantages over baseline with the process of fusion of other modalities nearly under all benchmarks. Notably, our method achieves significant advantages with 97.0% on NW-UCLA. Moreover, in terms of wearing a shoe in NTU-RGB+D, our model achieves 87.9% accuracy greatly over 77.3% of baseline.\n\n\nCONCLUSION\n\nIn this work, a novel structural attention method for channelwise adaptive graph convolution(SA-GCN) is proposed for skeleton-based action recognition. Our method is the first to exploit structural attentions for graph convolution, including part relation attention module physically for adaptive adjacency matrix and symmetry trajectory attention module logically for calculation of symmetric similarity. By adding these two attention modules, graph convolution obtains capability of structural feature learning. On three datasets, the proposed SA-GCN outperforms state-of-the-art methods.\n\nFig. 1 .\n1An overview of two structural attention modules. (a) In symmetry trajectory attention module, joints of the same color indicate symmetric joint pairs, composing symmetry joint trajectory over time. Orange lines indicate relation of one pair symmetric joint example. (b) Part relation attention is applied for adaptive adjacency matrix. Joints are split and combined to parts (i.e. bigger dots below) and Orange lines indicate relations of each part. Joints of the same part are marked with the same color and share its attention score.\n\nFig. 2 .\n2Framework of Part Relation Attention for Channel-Wise Adaptive Graph Convolution (i.e. PRA-AGC). Blue box contains channel-wise adaptive graph convolution, which comes from CTR-GCN[4]. Orange box contains a novel part relation attention module for adaptive adjacency matrix (i.e. the colorful one at the top).\n\nFig. 3 .\n3Symmetry Trajectory Attention Module. Blue joints are joints of left part and red joints are joints of right part.\n\nFig. 4 .\n4An overview of SA-GC architecture. PRA-AGC indicates Part Relation Attention for Adaptive Graph Convolution. MS-TC refers to Multi-Scale Temporal Convolution. STA represents Symmetry Trajectory Attention.\n\nFig. 5 .\n5Analysis on accuracies of structural attentions on baseline in NW-UCLA, which totally includes 10 action categories. 3 categories are removed because of same accuracies.\n\nFig.\nFig. 6. Visualization of two attention modules on Sitting Down. In Fig. (a), a map of PRA (i.e. A P C in Fig. 2) shows relation of different parts (i.e. T is torso, LA is Leg Arm, RA is Right Arm, LL is Left Leg and RL is Right Leg), and orange indicates high score while blue means low. In Fig. (b), relation of each pair is averaged to obtain its attention score and shown in skeleton. The bigger the red joint is, the higher attention score is achieved.\n\nTable 1 .\n1Demonstrationof indexes of part division and joint \nreordering in NTU-RGB+D [5]. \nBody Part \nJoint Index \nTorso \n3, 4, 5, 9, 21 \nLeft Arm \n6, 7, 8, 22, 23 \nRight Arm 10, 11, 12, 24, 25 \nLeft Leg \n1, 13, 14, 15, 16 \nRight Leg \n2, 17, 18, 19, 20 \n\nX X [T N C] \n[T N C] \nRe \n\n1 1 Conv \n\n1 1 Conv \n\n1 1 Conv \n\n1 1 Conv \n\nBN + Relu \nNorm \nSplit \nConcat \n\n[V T T] \n\n\n\nTable 2 .\n2Comparisons of accuracies when adding two structural attention modules to baseline CTR-GCN[4] on joint modality. PRA refers to part relation attention module and STA indicates symmetry trajectory attention module.Methods \nAccuracy (%) \nBaseline \n95.0 \nBaseline with PRA \n94.8 \u21930.2 \nBaseline with STA \n95.5 \u21910.5 \nBaseline with PRA & STA \n96.3 \u21911.3 \n\n80% \n\n85% \n\n90% \n\n95% \n\n100% \n\npick up with \none hand \n\nwalk around sit down \nstand up \ndoffing \nthrow \ncarry \n\nBaseline \nwith PRA \nwith STA \nwith PRA & STA \n\n\n\nTable 3 .\n3Comparisonsagainst state-of-the-art methods on \nNW-UCLA in accuracy (%). \nMethods \nNW-UCLA Top-1 (%) \nShift-GCN [16] \n94.6 \nDC-GCN+ADG [3] \n95.3 \nCTR-GCN [4] Joint \n95.0 \nCTR-GCN F usion \n96.5 \n(ours)SA-GCN Joint \n96.3 \n(ours)SA-GCN F usion \n97.0 \n\n\n\nTable 4 .\n4Comparisons against state-of-the-art methods on NTU-RGB+D in accuracy (%)Methods \nNTU-RGB+D \nX-Sub (%) X-View (%) \nST-GCN [1] \n81.5 \n88.3 \n2s-AGCN [2] \n88.5 \n95.1 \nPL-GCN [9] \n89.2 \n95.0 \nPA-ResGCN-B19 [8] \n90.9 \n96.0 \nDDGCN [17] \n91.1 \n97.1 \nDynamic GCN [10] \n91.5 \n96.0 \nMS-G3D [11] \n91.5 \n96.2 \nCTR-GCN [4] Joint \n89.9 \n94.7 \nCTR-GCN F usion \n92.4 \n96.8 \n(ours)SA-GCN Joint \n90.2 \n95.2 \n(ours)SA-GCN F usion \n92.6 \n96.7 \n\n\n\n\nother and show effectiveness of structural attention overall. Visualization of PRA map and STA weights. Sitting down is visualized in.3 \n88.3 \nDynamic GCN [10] \n87.3 \n88.6 \nCTR-GCN [4] Joint \n84.9 \n86.5 \nCTR-GCN F usion \n88.9 \n90.6 \n(ours)SA-GCN Joint \n85.0 \n86.9 \n(ours)SA-GCN F usion \n89.0 \n90.7 \n\n\nAcknowledgements\nSpatial temporal graph convolutional networks for skeleton-based action recognition. Sijie Yan, Yuanjun Xiong, Dahua Lin, arXiv:1801.07455arXiv preprintSijie Yan, Yuanjun Xiong, and Dahua Lin, \"Spatial tem- poral graph convolutional networks for skeleton-based action recognition,\" arXiv preprint arXiv:1801.07455, 2018.\n\nTwo-stream adaptive graph convolutional networks for skeleton-based action recognition. Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu, CVPR. Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu, \"Two-stream adaptive graph convolutional networks for skeleton-based action recognition,\" in CVPR, 2019, pp. 12026-12035.\n\nDecoupling gcn with dropgraph module for skeleton-based action recognition. Ke Cheng, Yifan Zhang, Congqi Cao, Lei Shi, Jian Cheng, Hanqing Lu, ECCV. Ke Cheng, Yifan Zhang, Congqi Cao, Lei Shi, Jian Cheng, and Hanqing Lu, \"Decoupling gcn with drop- graph module for skeleton-based action recognition,\" in ECCV, 2020, pp. 536-553.\n\nChannel-wise topology refinement graph convolution for skeleton-based action recognition. Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying Deng, Weiming Hu, ICCV. Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying Deng, and Weiming Hu, \"Channel-wise topology refine- ment graph convolution for skeleton-based action recog- nition,\" in ICCV, 2021, pp. 13359-13368.\n\nNtu rgb+ d: A large scale dataset for 3d human activity analysis. Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang, CVPR. Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang, \"Ntu rgb+ d: A large scale dataset for 3d human activity analysis,\" in CVPR, 2016, pp. 1010-1019.\n\nActional-structural graph convolutional networks for skeleton-based action recognition. Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng Wang, Qi Tian, CVPR. Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng Wang, and Qi Tian, \"Actional-structural graph convolu- tional networks for skeleton-based action recognition,\" in CVPR, 2019, pp. 3595-3603.\n\nFusing higher-order features in graph neural networks for skeleton-based action recognition. Zhenyue Qin, Yang Liu, Pan Ji, Dongwoo Kim, Lei Wang, Bob Mckay, Saeed Anwar, Tom Gedeon, arXiv:2105.01563arXiv preprintZhenyue Qin, Yang Liu, Pan Ji, Dongwoo Kim, Lei Wang, Bob McKay, Saeed Anwar, and Tom Gedeon, \"Fusing higher-order features in graph neural networks for skeleton-based action recognition,\" arXiv preprint arXiv:2105.01563, 2021.\n\nStronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition. Yi-Fan Song, Zhang Zhang, Caifeng Shan, Liang Wang, ACM MM, 2020. Yi-Fan Song, Zhang Zhang, Caifeng Shan, and Liang Wang, \"Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recog- nition,\" in ACM MM, 2020, pp. 1625-1633.\n\nPart-level graph convolutional network for skeleton-based action recognition. Linjiang Huang, Yan Huang, Wanli Ouyang, Liang Wang, AAAI. Linjiang Huang, Yan Huang, Wanli Ouyang, and Liang Wang, \"Part-level graph convolutional network for skeleton-based action recognition,\" in AAAI, 2020, pp. 11045-11052.\n\nDynamic gcn: Contextenriched topology learning for skeleton-based action recognition. Fanfan Ye, Shiliang Pu, Qiaoyong Zhong, Chao Li, Di Xie, Huiming Tang, ACM MM, 2020. Fanfan Ye, Shiliang Pu, Qiaoyong Zhong, Chao Li, Di Xie, and Huiming Tang, \"Dynamic gcn: Context- enriched topology learning for skeleton-based action recognition,\" in ACM MM, 2020, pp. 55-63.\n\nDisentangling and unifying graph convolutions for skeleton-based action recognition. Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang, Wanli Ouyang, CVPR. Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang, and Wanli Ouyang, \"Disentangling and unify- ing graph convolutions for skeleton-based action recog- nition,\" in CVPR, 2020, pp. 143-152.\n\nMulti-modality cross attention network for image and sentence matching. Xi Wei, Tianzhu Zhang, Yan Li, Yongdong Zhang, Feng Wu, CVPR. Xi Wei, Tianzhu Zhang, Yan Li, Yongdong Zhang, and Feng Wu, \"Multi-modality cross attention network for image and sentence matching,\" in CVPR, 2020, pp. 10941-10950.\n\nHuman-object relation network for action recognition in still images. Wentao Ma, Shuang Liang, ICME. Wentao Ma and Shuang Liang, \"Human-object relation network for action recognition in still images,\" in ICME, 2020, pp. 1-6.\n\nCross-view action modeling, learning and recognition. Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, Song-Chun Zhu, CVPR. Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song- Chun Zhu, \"Cross-view action modeling, learning and recognition,\" in CVPR, 2014, pp. 2649-2656.\n\nNtu rgb+ d 120: A large-scale benchmark for 3d human activity understanding. Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, Alex C Kot, IEEE transactions on pattern analysis and machine intelligence. 42Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C Kot, \"Ntu rgb+ d 120: A large-scale benchmark for 3d human activity under- standing,\" IEEE transactions on pattern analysis and machine intelligence, vol. 42, no. 10, pp. 2684-2701, 2019.\n\nSkeleton-based action recognition with shift graph convolutional network. Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian Cheng, Hanqing Lu, CVPR. Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian Cheng, and Hanqing Lu, \"Skeleton-based action recognition with shift graph convolutional network,\" in CVPR, 2020, pp. 183-192.\n\nDdgcn: A dynamic directed graph convolutional network for action recognition. Matthew Korban, Xin Li, ECCV. Matthew Korban and Xin Li, \"Ddgcn: A dynamic di- rected graph convolutional network for action recogni- tion,\" in ECCV, 2020, pp. 761-776.\n", "annotations": {"author": "[{\"end\":117,\"start\":105},{\"end\":130,\"start\":118},{\"end\":171,\"start\":131},{\"end\":211,\"start\":172},{\"end\":297,\"start\":212},{\"end\":407,\"start\":298},{\"end\":433,\"start\":408}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":112},{\"end\":129,\"start\":125},{\"end\":143,\"start\":139},{\"end\":184,\"start\":179}]", "author_first_name": "[{\"end\":111,\"start\":105},{\"end\":124,\"start\":118},{\"end\":138,\"start\":131},{\"end\":178,\"start\":172}]", "author_affiliation": "[{\"end\":296,\"start\":213},{\"end\":406,\"start\":299},{\"end\":432,\"start\":409}]", "title": "[{\"end\":102,\"start\":1},{\"end\":535,\"start\":434}]", "venue": null, "abstract": "[{\"end\":1681,\"start\":668}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2268,\"start\":2265},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2513,\"start\":2510},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2662,\"start\":2659},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2664,\"start\":2662},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2938,\"start\":2935},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3481,\"start\":3478},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3868,\"start\":3865},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4532,\"start\":4529},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4534,\"start\":4532},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4862,\"start\":4859},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5783,\"start\":5780},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5923,\"start\":5920},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5926,\"start\":5923},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6048,\"start\":6045},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6180,\"start\":6177},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6182,\"start\":6180},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6185,\"start\":6182},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6717,\"start\":6714},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6719,\"start\":6717},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6783,\"start\":6780},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7015,\"start\":7012},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7452,\"start\":7448},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7553,\"start\":7549},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8444,\"start\":8441},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9360,\"start\":9357},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11847,\"start\":11844},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11960,\"start\":11957},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12377,\"start\":12373},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12669,\"start\":12666},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12963,\"start\":12959},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13689,\"start\":13686},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14350,\"start\":14347},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15526,\"start\":15523},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15548,\"start\":15544},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15576,\"start\":15573},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18256,\"start\":18253},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19847,\"start\":19844}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":18061,\"start\":17515},{\"attributes\":{\"id\":\"fig_2\"},\"end\":18382,\"start\":18062},{\"attributes\":{\"id\":\"fig_3\"},\"end\":18508,\"start\":18383},{\"attributes\":{\"id\":\"fig_4\"},\"end\":18724,\"start\":18509},{\"attributes\":{\"id\":\"fig_5\"},\"end\":18905,\"start\":18725},{\"attributes\":{\"id\":\"fig_7\"},\"end\":19368,\"start\":18906},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":19741,\"start\":19369},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20262,\"start\":19742},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":20524,\"start\":20263},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":20962,\"start\":20525},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":21265,\"start\":20963}]", "paragraph": "[{\"end\":2027,\"start\":1697},{\"end\":3093,\"start\":2029},{\"end\":3800,\"start\":3095},{\"end\":4451,\"start\":3802},{\"end\":4986,\"start\":4453},{\"end\":5042,\"start\":4988},{\"end\":5257,\"start\":5044},{\"end\":5369,\"start\":5259},{\"end\":5635,\"start\":5371},{\"end\":6449,\"start\":5701},{\"end\":7244,\"start\":6482},{\"end\":7681,\"start\":7271},{\"end\":7770,\"start\":7719},{\"end\":7903,\"start\":7842},{\"end\":8207,\"start\":7994},{\"end\":8680,\"start\":8279},{\"end\":9044,\"start\":8700},{\"end\":9721,\"start\":9081},{\"end\":10101,\"start\":9757},{\"end\":10185,\"start\":10121},{\"end\":10264,\"start\":10262},{\"end\":11061,\"start\":10305},{\"end\":11423,\"start\":11196},{\"end\":11733,\"start\":11446},{\"end\":12329,\"start\":11735},{\"end\":13205,\"start\":12356},{\"end\":14145,\"start\":13232},{\"end\":14858,\"start\":14164},{\"end\":15471,\"start\":14860},{\"end\":15856,\"start\":15473},{\"end\":16909,\"start\":15900},{\"end\":17514,\"start\":16924}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7841,\"start\":7771},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7993,\"start\":7904},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8699,\"start\":8681},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9080,\"start\":9045},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9756,\"start\":9722},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10120,\"start\":10102},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11123,\"start\":11062},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11195,\"start\":11123}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9340,\"start\":9333},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14567,\"start\":14560},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":15115,\"start\":15108},{\"end\":15392,\"start\":15385},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16281,\"start\":16270}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1695,\"start\":1683},{\"attributes\":{\"n\":\"2.\"},\"end\":5650,\"start\":5638},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5699,\"start\":5653},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6480,\"start\":6452},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7269,\"start\":7247},{\"attributes\":{\"n\":\"3.\"},\"end\":7703,\"start\":7684},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7717,\"start\":7706},{\"attributes\":{\"n\":\"3.2.\"},\"end\":8277,\"start\":8210},{\"end\":10200,\"start\":10188},{\"end\":10210,\"start\":10203},{\"end\":10220,\"start\":10213},{\"end\":10230,\"start\":10223},{\"end\":10240,\"start\":10233},{\"end\":10250,\"start\":10243},{\"end\":10260,\"start\":10253},{\"attributes\":{\"n\":\"3.3.\"},\"end\":10303,\"start\":10267},{\"attributes\":{\"n\":\"3.4.\"},\"end\":11444,\"start\":11426},{\"attributes\":{\"n\":\"4.\"},\"end\":12343,\"start\":12332},{\"attributes\":{\"n\":\"4.1.\"},\"end\":12354,\"start\":12346},{\"attributes\":{\"n\":\"4.2.\"},\"end\":13230,\"start\":13208},{\"attributes\":{\"n\":\"4.3.\"},\"end\":14162,\"start\":14148},{\"attributes\":{\"n\":\"4.4.\"},\"end\":15898,\"start\":15859},{\"attributes\":{\"n\":\"5.\"},\"end\":16922,\"start\":16912},{\"end\":17524,\"start\":17516},{\"end\":18071,\"start\":18063},{\"end\":18392,\"start\":18384},{\"end\":18518,\"start\":18510},{\"end\":18734,\"start\":18726},{\"end\":18911,\"start\":18907},{\"end\":19379,\"start\":19370},{\"end\":19752,\"start\":19743},{\"end\":20273,\"start\":20264},{\"end\":20535,\"start\":20526}]", "table": "[{\"end\":19741,\"start\":19394},{\"end\":20262,\"start\":19967},{\"end\":20524,\"start\":20286},{\"end\":20962,\"start\":20610},{\"end\":21265,\"start\":21098}]", "figure_caption": "[{\"end\":18061,\"start\":17526},{\"end\":18382,\"start\":18073},{\"end\":18508,\"start\":18394},{\"end\":18724,\"start\":18520},{\"end\":18905,\"start\":18736},{\"end\":19368,\"start\":18912},{\"end\":19394,\"start\":19381},{\"end\":19967,\"start\":19754},{\"end\":20286,\"start\":20275},{\"end\":20610,\"start\":20537},{\"end\":21098,\"start\":20965}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3799,\"start\":3793},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8476,\"start\":8470},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8955,\"start\":8949},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9599,\"start\":9589},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10323,\"start\":10317},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10959,\"start\":10949},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11487,\"start\":11481},{\"end\":13694,\"start\":13693},{\"end\":13763,\"start\":13755},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13800,\"start\":13793},{\"end\":13987,\"start\":13979},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":14688,\"start\":14682},{\"end\":15587,\"start\":15581}]", "bib_author_first_name": "[{\"end\":21373,\"start\":21368},{\"end\":21386,\"start\":21379},{\"end\":21399,\"start\":21394},{\"end\":21696,\"start\":21693},{\"end\":21707,\"start\":21702},{\"end\":21719,\"start\":21715},{\"end\":21734,\"start\":21727},{\"end\":21996,\"start\":21994},{\"end\":22009,\"start\":22004},{\"end\":22023,\"start\":22017},{\"end\":22032,\"start\":22029},{\"end\":22042,\"start\":22038},{\"end\":22057,\"start\":22050},{\"end\":22344,\"start\":22339},{\"end\":22355,\"start\":22351},{\"end\":22371,\"start\":22363},{\"end\":22382,\"start\":22378},{\"end\":22391,\"start\":22387},{\"end\":22405,\"start\":22398},{\"end\":22690,\"start\":22686},{\"end\":22705,\"start\":22702},{\"end\":22721,\"start\":22711},{\"end\":22730,\"start\":22726},{\"end\":22991,\"start\":22985},{\"end\":23002,\"start\":22996},{\"end\":23011,\"start\":23009},{\"end\":23020,\"start\":23018},{\"end\":23035,\"start\":23028},{\"end\":23044,\"start\":23042},{\"end\":23350,\"start\":23343},{\"end\":23360,\"start\":23356},{\"end\":23369,\"start\":23366},{\"end\":23381,\"start\":23374},{\"end\":23390,\"start\":23387},{\"end\":23400,\"start\":23397},{\"end\":23413,\"start\":23408},{\"end\":23424,\"start\":23421},{\"end\":23807,\"start\":23801},{\"end\":23819,\"start\":23814},{\"end\":23834,\"start\":23827},{\"end\":23846,\"start\":23841},{\"end\":24155,\"start\":24147},{\"end\":24166,\"start\":24163},{\"end\":24179,\"start\":24174},{\"end\":24193,\"start\":24188},{\"end\":24468,\"start\":24462},{\"end\":24481,\"start\":24473},{\"end\":24494,\"start\":24486},{\"end\":24506,\"start\":24502},{\"end\":24513,\"start\":24511},{\"end\":24526,\"start\":24519},{\"end\":24830,\"start\":24826},{\"end\":24843,\"start\":24836},{\"end\":24859,\"start\":24851},{\"end\":24873,\"start\":24866},{\"end\":24885,\"start\":24880},{\"end\":25166,\"start\":25164},{\"end\":25179,\"start\":25172},{\"end\":25190,\"start\":25187},{\"end\":25203,\"start\":25195},{\"end\":25215,\"start\":25211},{\"end\":25469,\"start\":25463},{\"end\":25480,\"start\":25474},{\"end\":25678,\"start\":25673},{\"end\":25692,\"start\":25685},{\"end\":25701,\"start\":25698},{\"end\":25711,\"start\":25707},{\"end\":25725,\"start\":25716},{\"end\":25967,\"start\":25964},{\"end\":25977,\"start\":25973},{\"end\":25997,\"start\":25989},{\"end\":26009,\"start\":26005},{\"end\":26023,\"start\":26016},{\"end\":26034,\"start\":26030},{\"end\":26036,\"start\":26035},{\"end\":26450,\"start\":26448},{\"end\":26463,\"start\":26458},{\"end\":26478,\"start\":26471},{\"end\":26489,\"start\":26483},{\"end\":26500,\"start\":26496},{\"end\":26515,\"start\":26508},{\"end\":26792,\"start\":26785},{\"end\":26804,\"start\":26801}]", "bib_author_last_name": "[{\"end\":21377,\"start\":21374},{\"end\":21392,\"start\":21387},{\"end\":21403,\"start\":21400},{\"end\":21700,\"start\":21697},{\"end\":21713,\"start\":21708},{\"end\":21725,\"start\":21720},{\"end\":21737,\"start\":21735},{\"end\":22002,\"start\":21997},{\"end\":22015,\"start\":22010},{\"end\":22027,\"start\":22024},{\"end\":22036,\"start\":22033},{\"end\":22048,\"start\":22043},{\"end\":22060,\"start\":22058},{\"end\":22349,\"start\":22345},{\"end\":22361,\"start\":22356},{\"end\":22376,\"start\":22372},{\"end\":22385,\"start\":22383},{\"end\":22396,\"start\":22392},{\"end\":22408,\"start\":22406},{\"end\":22700,\"start\":22691},{\"end\":22709,\"start\":22706},{\"end\":22724,\"start\":22722},{\"end\":22735,\"start\":22731},{\"end\":22994,\"start\":22992},{\"end\":23007,\"start\":23003},{\"end\":23016,\"start\":23012},{\"end\":23026,\"start\":23021},{\"end\":23040,\"start\":23036},{\"end\":23049,\"start\":23045},{\"end\":23354,\"start\":23351},{\"end\":23364,\"start\":23361},{\"end\":23372,\"start\":23370},{\"end\":23385,\"start\":23382},{\"end\":23395,\"start\":23391},{\"end\":23406,\"start\":23401},{\"end\":23419,\"start\":23414},{\"end\":23431,\"start\":23425},{\"end\":23812,\"start\":23808},{\"end\":23825,\"start\":23820},{\"end\":23839,\"start\":23835},{\"end\":23851,\"start\":23847},{\"end\":24161,\"start\":24156},{\"end\":24172,\"start\":24167},{\"end\":24186,\"start\":24180},{\"end\":24198,\"start\":24194},{\"end\":24471,\"start\":24469},{\"end\":24484,\"start\":24482},{\"end\":24500,\"start\":24495},{\"end\":24509,\"start\":24507},{\"end\":24517,\"start\":24514},{\"end\":24531,\"start\":24527},{\"end\":24834,\"start\":24831},{\"end\":24849,\"start\":24844},{\"end\":24864,\"start\":24860},{\"end\":24878,\"start\":24874},{\"end\":24892,\"start\":24886},{\"end\":25170,\"start\":25167},{\"end\":25185,\"start\":25180},{\"end\":25193,\"start\":25191},{\"end\":25209,\"start\":25204},{\"end\":25218,\"start\":25216},{\"end\":25472,\"start\":25470},{\"end\":25486,\"start\":25481},{\"end\":25683,\"start\":25679},{\"end\":25696,\"start\":25693},{\"end\":25705,\"start\":25702},{\"end\":25714,\"start\":25712},{\"end\":25729,\"start\":25726},{\"end\":25971,\"start\":25968},{\"end\":25987,\"start\":25978},{\"end\":26003,\"start\":25998},{\"end\":26014,\"start\":26010},{\"end\":26028,\"start\":26024},{\"end\":26040,\"start\":26037},{\"end\":26456,\"start\":26451},{\"end\":26469,\"start\":26464},{\"end\":26481,\"start\":26479},{\"end\":26494,\"start\":26490},{\"end\":26506,\"start\":26501},{\"end\":26518,\"start\":26516},{\"end\":26799,\"start\":26793},{\"end\":26807,\"start\":26805}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1801.07455\",\"id\":\"b0\"},\"end\":21603,\"start\":21283},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":195440283},\"end\":21916,\"start\":21605},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":227232051},\"end\":22247,\"start\":21918},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":236428765},\"end\":22618,\"start\":22249},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15928602},\"end\":22895,\"start\":22620},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":139101198},\"end\":23248,\"start\":22897},{\"attributes\":{\"doi\":\"arXiv:2105.01563\",\"id\":\"b6\"},\"end\":23690,\"start\":23250},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":222278406},\"end\":24067,\"start\":23692},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":213832737},\"end\":24374,\"start\":24069},{\"attributes\":{\"id\":\"b9\"},\"end\":24739,\"start\":24376},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":214728271},\"end\":25090,\"start\":24741},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":219633239},\"end\":25391,\"start\":25092},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":221120464},\"end\":25617,\"start\":25393},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2239612},\"end\":25885,\"start\":25619},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":152282878},\"end\":26372,\"start\":25887},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219964813},\"end\":26705,\"start\":26374},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":226308363},\"end\":26953,\"start\":26707}]", "bib_title": "[{\"end\":21691,\"start\":21605},{\"end\":21992,\"start\":21918},{\"end\":22337,\"start\":22249},{\"end\":22684,\"start\":22620},{\"end\":22983,\"start\":22897},{\"end\":23799,\"start\":23692},{\"end\":24145,\"start\":24069},{\"end\":24460,\"start\":24376},{\"end\":24824,\"start\":24741},{\"end\":25162,\"start\":25092},{\"end\":25461,\"start\":25393},{\"end\":25671,\"start\":25619},{\"end\":25962,\"start\":25887},{\"end\":26446,\"start\":26374},{\"end\":26783,\"start\":26707}]", "bib_author": "[{\"end\":21379,\"start\":21368},{\"end\":21394,\"start\":21379},{\"end\":21405,\"start\":21394},{\"end\":21702,\"start\":21693},{\"end\":21715,\"start\":21702},{\"end\":21727,\"start\":21715},{\"end\":21739,\"start\":21727},{\"end\":22004,\"start\":21994},{\"end\":22017,\"start\":22004},{\"end\":22029,\"start\":22017},{\"end\":22038,\"start\":22029},{\"end\":22050,\"start\":22038},{\"end\":22062,\"start\":22050},{\"end\":22351,\"start\":22339},{\"end\":22363,\"start\":22351},{\"end\":22378,\"start\":22363},{\"end\":22387,\"start\":22378},{\"end\":22398,\"start\":22387},{\"end\":22410,\"start\":22398},{\"end\":22702,\"start\":22686},{\"end\":22711,\"start\":22702},{\"end\":22726,\"start\":22711},{\"end\":22737,\"start\":22726},{\"end\":22996,\"start\":22985},{\"end\":23009,\"start\":22996},{\"end\":23018,\"start\":23009},{\"end\":23028,\"start\":23018},{\"end\":23042,\"start\":23028},{\"end\":23051,\"start\":23042},{\"end\":23356,\"start\":23343},{\"end\":23366,\"start\":23356},{\"end\":23374,\"start\":23366},{\"end\":23387,\"start\":23374},{\"end\":23397,\"start\":23387},{\"end\":23408,\"start\":23397},{\"end\":23421,\"start\":23408},{\"end\":23433,\"start\":23421},{\"end\":23814,\"start\":23801},{\"end\":23827,\"start\":23814},{\"end\":23841,\"start\":23827},{\"end\":23853,\"start\":23841},{\"end\":24163,\"start\":24147},{\"end\":24174,\"start\":24163},{\"end\":24188,\"start\":24174},{\"end\":24200,\"start\":24188},{\"end\":24473,\"start\":24462},{\"end\":24486,\"start\":24473},{\"end\":24502,\"start\":24486},{\"end\":24511,\"start\":24502},{\"end\":24519,\"start\":24511},{\"end\":24533,\"start\":24519},{\"end\":24836,\"start\":24826},{\"end\":24851,\"start\":24836},{\"end\":24866,\"start\":24851},{\"end\":24880,\"start\":24866},{\"end\":24894,\"start\":24880},{\"end\":25172,\"start\":25164},{\"end\":25187,\"start\":25172},{\"end\":25195,\"start\":25187},{\"end\":25211,\"start\":25195},{\"end\":25220,\"start\":25211},{\"end\":25474,\"start\":25463},{\"end\":25488,\"start\":25474},{\"end\":25685,\"start\":25673},{\"end\":25698,\"start\":25685},{\"end\":25707,\"start\":25698},{\"end\":25716,\"start\":25707},{\"end\":25731,\"start\":25716},{\"end\":25973,\"start\":25964},{\"end\":25989,\"start\":25973},{\"end\":26005,\"start\":25989},{\"end\":26016,\"start\":26005},{\"end\":26030,\"start\":26016},{\"end\":26042,\"start\":26030},{\"end\":26458,\"start\":26448},{\"end\":26471,\"start\":26458},{\"end\":26483,\"start\":26471},{\"end\":26496,\"start\":26483},{\"end\":26508,\"start\":26496},{\"end\":26520,\"start\":26508},{\"end\":26801,\"start\":26785},{\"end\":26809,\"start\":26801}]", "bib_venue": "[{\"end\":21366,\"start\":21283},{\"end\":21743,\"start\":21739},{\"end\":22066,\"start\":22062},{\"end\":22414,\"start\":22410},{\"end\":22741,\"start\":22737},{\"end\":23055,\"start\":23051},{\"end\":23341,\"start\":23250},{\"end\":23865,\"start\":23853},{\"end\":24204,\"start\":24200},{\"end\":24545,\"start\":24533},{\"end\":24898,\"start\":24894},{\"end\":25224,\"start\":25220},{\"end\":25492,\"start\":25488},{\"end\":25735,\"start\":25731},{\"end\":26104,\"start\":26042},{\"end\":26524,\"start\":26520},{\"end\":26813,\"start\":26809}]"}}}, "year": 2023, "month": 12, "day": 17}