{"id": 259937100, "updated": "2023-11-21 22:13:08.385", "metadata": {"title": "Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models", "authors": "[{\"first\":\"Zhiyuan\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Xuyang\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Fang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific dense retrievers. We design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries. To the best of our knowledge, there is no prior work utilizing soft prompt tuning to augment DR models. The experiments demonstrate that SPTAR outperforms the unsupervised baselines BM25 and the recently proposed LLMs-based augmentation method for DR.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2307-08303", "doi": "10.48550/arxiv.2307.08303"}}, "content": {"source": {"pdf_hash": "d44031f253668c61ac6d68b95bbe9cac57730d51", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2307.08303v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c261eaae06f14456a45a8f2f4235a8832983d4ec", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d44031f253668c61ac6d68b95bbe9cac57730d51.txt", "contents": "\nSoft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models\n\n\nZhiyuan Peng zpeng@scu.edu \nXuyang Wu \nYi Fang yfang@scu.edu \n\nSanta Clara University Santa Clara\nUSA\n\n\nSanta Clara University Santa Clara\nUSA\n\n\nSanta Clara University Santa Clara\nUSA\n\nSoft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models\nCCS CONCEPTSInformation systems \u2192 Information retrieval;Computing methodologies \u2192 Natural language generation KEYWORDS Large Language Models, Dense Retrieval, Prompt Tuning\nDense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or humanwritten prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR 1 ): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific dense retrievers. We design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries. To the best of our knowledge, there is no prior work utilizing soft prompt tuning to augment DR models. The experiments demonstrate that SPTAR outperforms the unsupervised baselines BM25 and the recently proposed LLMs-based augmentation method for DR.\n\nINTRODUCTION\n\nTraditional informational retrieval (IR) methods like TF-IDF and BM25 [25] are based on token-level similarity matching and then suffer from lexical gap [1]. Inspired by the progress in deep learning, researchers have proposed to utilize neural networks to overcome the lexical gap. DR is such a kind of method based on neural networks. DR models like DPR [10] and Col-BERT [11] [27] encode each query or document to a dense vector, the dimensionality of which is determined by the neural networks. In practice, dense retrievers pre-compute the embeddings of documents and then on which build an approximate nearest neighbor (ANN) index for fast search. When a new query comes in, only its embedding is computed and fed into the following ANN search system. Unlike TF-IDF and BM25, DR cares more about the similarity of the overall semantic meaning.\n\nEven though neural retrieval mitigates the lexical gap, it still suffers from the challenge of lacking domain-specific training data. Some researchers have proposed to leverage transfer learning to tackle this challenge. To tackle this problem. Evidence [32] [6] shows that not all DR models and domains can benefit from transfer learning equally. Recently, LLMs like CPT-3 [4], LLaMA [33], and Vicuna [5] show the strong ability of zero-shot and few-shot learning. Instead of fine-tuning the LLMs on task-specific data, prompting concatenates the instructions for certain tasks (e.g., TL;DR translate to English) and a few corresponding examples as input and obtains the answers from the output of large language model (LLM). These kinds of human-written prompts are also called hard prompts. Researchers [28] recently have estimated that a good language classifier prompt is worth hundreds to thousands of extra data points. InPars [2] and PROMPTAGATOR [6] both utilize hard prompts to prompt the LLMs to tag the unlabeled documents with weak queries and then train task-specific retrievers. However, hard prompts have some drawbacks: a) It is not easy to find good hard prompts. Hard prompts must be hand-crafted by humans through trial and error, and sometimes intuition and luck are needed; b) Even with handcrafted prompts, the downstream tasks still underperform tuned models. For instance, compared with the performance of fine-tuned T5-XXL [24] on SuperGLUE [35], GPT-3 175B few-shot gets a 17.5 points smaller score despite using 16 times more parameters [12].\n\nInstead of utilizing humanly readable words as a hard prompt [22], soft prompt [12] [13] is a list of embeddings, unrecognizable to the human eye, appended to the input of the neural network. During the soft prompt tuning, the parameters of the LLM are frozen, and only the parameters associated with soft prompt are updated. Even though [12] and [13] both demonstrate that soft prompt outperforms the hard prompt, there is no work utilizing soft prompt tuning to augment DR. In this paper, we propose soft prompt tuning for augmenting DR (SPTAR). Specifically, for each task, we leverage soft prompt tuning to optimize the parameters associated with soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific retrievers. Moreover, we find that even with the optimized soft prompt, the quality of generated weak queries is sensitive to the example document-query pairs in the prompt. So, we design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries as well as the DR tasks. Our main contributions can be summarized as follows:\n\n\u2022 To the best of our knowledge, this is the first work that utilizes LLMs with soft prompt tuning for augmenting DR tasks. \u2022 We introduce a novel soft prompt filter to select high-quality document-query pairs in the prompt to further improve the quality of generated weak data. \u2022 Comprehensive experiments are conducted to demonstrate our approach outperforming BM25 and InPars [2]. \u2022 Experiments are based on the recent open-source LLMs, and we will make the code publicly available upon paper acceptance.  [27] share the same BERT but utilize a different special token after \"[CLS]\" to distinguish query and document. Unlike DPR directly measures the similarity between query embedding and document embeddings, Col-BERT introduces a late interaction mechanism. Specifically, for each token in the query, ColBERT computes its similarity with all the tokens in the document and applies a maximum pooling on these similarity scores. The similarity score of a pair of query and document is the summarization of all the scores after the maximum pooling. Given a query with one positive document and one negative document, ColBERT is optimized by the pairwise softmax cross-entropy loss over the computed scores of the positive and negative documents. ANCE [39] is a bi-encoder trained on (query, positive document, negative document) tuples where the negative document is retrieved from an ANN built on the checkpoint of the last step. BM25CE [36] is a re-ranking-based DR. BM25CE first applies BM25 to retrieve documents and then employs the trained crossed-encoder to re-rank the retrieved documents. Our contribution is not to propose new dense retrievers but to propose a novel method to augment the existing dense retrievers.\n\n\nLLMs in Dense Retrieval\n\nMost of the existing researches in this field primarily concentrate on leveraging LLMs to improve DR tasks through various data generation techniques, including query generation [2,3,6,7,9,26], relevance generation [14], and permutation generation [16,23,29]. InPars [2] feeds a task-specific human-written prompt and 3 example document-query pairs to a 6B GPT-3 [4] model Curie to generate 100K weak document-query pairs and selects the top 10K queries with respect to the probability of query to augment the training data. InPars [2] employs the same dense retrieval model proposed in [21], which treats the retrieval as a sequence-to-sequence task by concatenating a query and a document as input to T5 mode and outputs the relevance score. Like InPars [2], PROMPTAGATOR [6] also feeds a task-specific human-written prompt and at most 8 example document-query pairs to LLM to generate weak data. Instead of selecting the top weak queries by their probabilities, PROMPTA-GATOR first trains a filter on uncleaned document-query pairs to filter the weak queries by dropping the weak queries that cannot retrieve their paired documents in the Top-retrieved documents. By repeating this process multiple times, the filter significantly improves the performance of a dual-encoder DPR retriever. Besides, PROMPTAGATOR [6] utilizes a much bigger LLM: a 175B model Flan [38] which cannot be accessed by most researchers. UPR [26] utilizes LLM as a zero-shot reranker to re-rank the passages retrieved by retrievers like BM25 and DPR. Given a query, for each retrieved passage, UPR utilizes a prompt \"Please write a question based on this passage\" to prompt a LLM and computes the average loglikelihood of the question tokens conditioned on the input document as the relevance score. Because of the expensive training of LLMs, all these works utilize LLMs as query generators instead of fine-tuning them. Improved variations of InPars [2], such as InPars-v2 [9] and InPars-Light [3], have been introduced to enhance the original methodology. HyDE [7] leverages LLMs to augment queries by generating hypothetical documents, effectively capturing relevance patterns for unsupervised retrieval. LRL [16] trains a listwise zero-shot re-ranker that leverages LLMs without task-specific supervised training. Unlike pointwise re-rankers, LRL considers all candidate documents to determine their relative ranking positions. Another approach involves instructional permutation generation [29], where the focus is on instructing LLMs to directly output permutations of passages. Permutation distillation techniques are employed to transfer the passage ranking capabilities of ChatGPT into a smaller, specialized ranking model. While these works utilize LLMs as query generators without fine-tuning, our SPTAR approach takes a different approach. We first perform soft prompt tuning to optimize task-specific soft prompts and then employ data filtering to enhance the quality of the generated weak data.\n\n\nPrompt Tuning\n\nPrompt tuning has demonstrated significant potential in adapting pre-trained LLMs to specific tasks by focusing on tuning the prompt module instead of fine-tuning the entire model [30]. Prefix-Tuning [13] introduces a prompt module with learnable parameters outputting embeddings which are prepended to the embeddings of other inputted tokens. This approach keeps the original training objective intact, while updating only the prefix parameters through gradient descent for each task. Another similar technique, referred to as \"gisting\" [19], compresses arbitrary prompts into a condensed set of virtual \"gist\" tokens using a meta-learning approach. Building upon T5 [24], Lester et al. [12] proposed a method where the learnable embeddings of a task-specific prompt are prepended to the encoder's output. The concatenated embeddings are then passed through the decoder to compute the training objective. This approach enables the model to incorporate task-specific information into the decoding process. Zhou et al. [40] introduced Dual Contextguided Continuous Prompt (DCCP), which employs soft prompt tuning using dual inputs: context-aware prompt and label-aware context representations. This approach leverages both prompt information and contextual understanding to enhance the model's performance. The use of multi-task learning techniques can benefit prompt learning in various tasks. For instance, ATTEMPT proposed by Wang et al. [37] introduces a multi-task tuning method that transfers knowledge across different tasks through a mixture of soft prompts. In the context of Multilingual Information Retrieval, Huang et al. [8] explores a soft prompt decoding approach that treats retrieval in each language as a separate task while jointly modeling them to capture shared underlying structures. They use decomposable prompts in KD-SPD to model languages, highlighting that languages share common features and concepts despite their unique properties. Regarding IR tasks, DPTDR by Tang et al.\n\n[31] employs a dual-encoder, utilizing two RoBERTa models for retrieval. It initializes the dual-encoder through contrastive learning and appends learnable soft prompts for query and document. Both the dual-encoder and the learnable prompts are updated during the training process.\n\nIn contrast, even though we apply the same Prefix-Tuning [13] method to learn a task-specific soft prompt, we focus on improving the data augmentation for DR tasks, and we propose a novel soft prompt filter method to select high-quality example documentquery pairs in the prompt to improve the DR tasks further. The whole augmentation pipeline makes our approach different from the current works. Figure 1: The pipeline of SPTAR.\n\n\nSPTAR\n\nAs shown in Figure 1, SPTAR mainly consists of six modules: a) data preparation; b) soft prompt tuning; c) soft prompt filter; d) prompt augmentor; e) weak data filter; f) DR. In Section 3.1, we elaborate on how to generate the training and evaluation datasets of soft prompt tuning. With the training and evaluation datasets, we conduct soft prompt tuning (Section 3.2) to learn a task-specific soft prompt. To further improve the quality of the weak generated queries, we explore the soft prompt filter in Section 3.3 to find highquality example document-query pairs for the learned task-specific soft prompt. We then prompt LLMs to generate weak queries for unlabeled documents, yielding enough training data to train DR (Section 3.4). Finally, we train the DR (Section 2.1) models on filtered weak data.\n\n\nData Preparation\n\nWe study how to augment DR with limited data, so the first step is to sample a small dataset on which we fine-tune a task-specific soft prompt. We define dataset as = {( , )} =1 where for each query , there is a relevant document . There may exist duplicated queries as one query may have multiple relevant documents. This domain-specific dataset is categorized into train, test, and evaluation subsets, denoted as , , and , respectively. Apart from dataset , there is a much bigger document collection which contains all the documents in but have more unlabeled documents denoted as . After training, DR encodes all the documents in into vectors. When new query comes in, DR encodes the query into an vector and search the topsimilar documents in vector space.\n\nWe random rample document-query pairs from origianl training dataset to construct the training and evaluation datasets for the soft prompt module, namely and where X and Y represent the number of different queiers in training and evaluation datasets respectively. We define ( ) represent the number of document-query pairs when there are different queries in the dataset, so, the number of document-query pairs in is ( ). Similarly, has ( ) document-query pairs. For instance, in our experiment, we randomly sample 50 unique queries and their corresponding documents from the training dataset to form 50 ( = 50). From the remaining data in , we randomly select 100 unique queries and their associated documents to compose 100 ( = 100). The data in 50 primarily serves for optimizing the soft prompt, while the data in 100 is employed to assess the model's convergence, enabling us to terminate the training process in advance and mitigate overfitting risks. We also tried other values of , and the influence of is studied in Section 5.2.5.\n\n\nSoft Prompt Tuning\n\nIn our soft prompt tuning module, we apply the same prompt tuning method Prefix-Tuning [13] and we implemented our soft prompt tuning module based on a public prompt tuning package PEFT [18].  \npairs ( , ) ( ) \u2212 =1\nto compute loss. As shown in Table 1, each pair ( , ) is concatenated with example pairs ( , ) =1 by keywords like \"document\" and \"relevant query\" as . We initialize a prompt to be optimized with size by repeating \"please generate query for document\" until the length of equals . Finally, we concatenate with as one training instance = [ ; ] and there are ( ) \u2212 instances in each epoch. Assume we have an autogressive LLM denoted by ,\u03a6 ( | ). When is inputted into soft prompt tuning module, it is first tokenized into a list of ids indexed by then the embeddings of ids are extracted and fed into the following layers to compute the hidden vectors. For concise, we assume each token in has one corresponding id in . For training instance , the hidden vector of \u210e time step is defined\nas \u210e , \u2208 R where \u210e , = \u210e (1) , ; \u00b7 \u00b7 \u00b7 ; \u210e ( ) ,\nand is the number of layers in LLM. An autogressive LLM takes current , and its left context to computes \u210e as follows:\n\u210e , = LLM , , , \u210e ,< , if \u2264 LLM , , \u210e ,< , if >(1)\nThe training objective function is:\nmax log , ( | ) = max \u2211\ufe01 \u2208 log , , | \u210e ,<(2)\nwhere represent the indexes of 's ids and , , | \u210e ,< represent the possibility of next token the id of which is , . We utilize the negative log-likelihood and perplexity as loss , which is defined as:\n= \u2212 log , ( | ) + \u2212 log , ( | )(3)\n\nSoft Prompt Filter\n\nDuring the training process of the soft prompt module, we observe that the choice of example document-query pairs ( , ) =1 significantly impacts the quality of text generation. Therefore, upon completing the soft prompt training, with the learned parameters , we try to select the best group of document-query pair from as example document-query pairs in soft prompt augmentor. For = 2, there are 1475 (50 * 59/2) groups of example pairs which are too many to evaluate them all efficiently. To reduce the computation complexity, we random sample groups of example pairs from to evaluate them on the evaluation dataset and the group of example pairs with the best evaluation metric will be chosen as the example pairs in soft prompt augmentor. As shown in Figure 2 (c), the only difference between soft prompt tuning and soft prompt filter is the dataset where the comes from. Suppose we sampled groups of document-query pairs each of which has document-query pairs ( , ) =1 . Evaluation dataset has ( ) document-query pairs and for each pair ( , ), similar to Table 1, we concatenate with the ( , ) =1 as . Then, is concatenated with the initialized prompt as = [ , ]. The evaluation metric is the same as the loss function (Equation 3). We study the effectiveness of soft prompt filter in Section 5.2.3.\n\n\nSoft Prompt Augmentor\n\nAs shown in Figure 2 (d), with the learned parameters and the filtered group of example document-query pairs, soft prompt augmentor generates a weak query for an unlabeled document sampled from . In practice, for each dataset, we create two weak datasets: a)\n\n. 100 unlabled documents are sampled from to generate their weak queries. If the number of unlabeled documents in is smaller than 100 , all the unlabeled documents are utilized to generate weak queries; b)\n\n. Sample 5000 document-query pairs from . During the weak query generation process, LLM not only utilizes the soft prompt embeddings to capture domain-specific information but also benefits from the supplementary context provided by the best example document-query pairs.\n\n\nWeak Data Filter\n\nInPars [2] and PROPAGATE [6] demonstrated that it is necessary to clean the weak document-query pairs. We selected the same method as InPars [2] to clean the weak data. After we obtain generated weak document-query pairs (Section 3.4), we filter them by BM25: For each weak query, we run BM25 to retrieve the top documents from the corpus . We will drop the weak document-query pair if its paired document is not in the retrieved top documents. For datasets MS MARCO and FiQA-2018, we tried top \u2208 (10, 30, 50, 70). For BM25CE, we re-ranked top \u2208 (5, 10, 30, 50, 70) documents retrieved by BM25. For all the models, we report the best NDCG@10 score along with other metrics in Table 6.\n\n\nDense Retrieval\n\nWe conducted the experiments on three popular dense retrievers: DPR (cross-encoder version), ColBERT, and BM25CE. The descriptions of the three models can be found in Section 2.1. We only trained DPR 2 and ColBERT 3 . For BM25CE, we directly loaded the BERT model of the best DPR as a re-ranker to re-rank the tok documents retrieved by BM25.  We conducted experiments on two datasets MS MARCO [20] and FiQA-2018 [17] from BEIR [32]. The description of the two datasets is shown in Table 2. We follow BEIR [32] to report the metrics on the evaluation dataset instead of test data for MS MARCO, so, for MS MARCO, is the same as . As shown in Table 3 \n\n\nEXPERIMENTAL SETUP 4.1 Datasets\n\n\nTraining Details\n\nTo train the soft prompt module, we performed fine-tuning using two open-source LLMs: LLaMA-7B and Vicuna-7B. The specific training hyper-parameters are documented in Table 4.\n\n\nHyperparameters\n\nLLaMA   The training hyper-parameters of dense retriever DPR and Col-Bert are in Table 5. For ColBERT, there is no early stop and we saved a checkpoint after each epoch in the official code. After training, we manually evaluated some checkpoints (3,5,10,15,18,20) and reported the testing results of the checkpoint with the highest NDCG@10 score.  Table 6: SPTAR vs baseline models: a) BM25 is an unsupervised method; b) W/O Aug is a supervised mehtod and the three DR models are trained on dataset without augmentation; c) InPars [2] utilizes human-written prompts and it has no soft prompt filter mechanism; d) Table 3 docuemts the data splits for each method.\n\n\nEvaluation Metrics\n\nIn the context of text generation models, Perplexity is a commonly employed metric that quantifies the level of uncertainty exhibited by a language model when generating new tokens. This metric is defined as the exponentiated average negative log-likelihood of a sequence, and a lower perplexity value indicates a higher-quality language model. Perplexity is used to evaluate the soft prompt tuning and filter modules. For DR models, we reporte the testing results of the checkpoint with the highest NDCG@10 score. Besides NDCG metric, we have also incorporated several other DR evaluation metrics to facilitate the assessment of the improvement achieved by synthesized augmented queries. These additional metrics include Mean Average Precision (MAP) [41] and Recall [41]. By utilizing these metrics, we can comprehensively evaluate the effectiveness of the synthesized augmented queries in enhancing the performance of DR models.\n\n\nBaseline Methods\n\nWe choose three baseline methods: BM25, Without Augmentation (W/O Aug), and InPars [2] (Section 2.2). The training, evaluation, and testing datasets are documented in Section 4.1. For BM25 [25], we use Anserini [15] with the default Lucene parameters( = 0.9 and = 0.4). The differences between InPars [2] and SPTAR are twofold: a) InPars [2] utilizes the human-written prompt while SPTAR utilizes an optimized soft prompt; b) SPTAR has a soft prompt filter module to select example document-query pairs. To make it a fair comparison with InPars [2], we choose the same example document-query pairs in the prompt of SPTAR for InPars [2] and utilize InPars' original human-written prompt to prompt the LLaMA and Vicuna to obtain weak document-query pairs. We find for InPars' human-written prompt, the quality of generated weak document-query pairs of Vicuna is much better than that of LLaMA, so, for InPars [2], we choose Vicuna as the weak data generator.\n\n\nResearch Questions\n\nAn extensive set of experiments were designed to address the following research questions: RQ1: Can the proposed SPTAR framework achieve improved performance on DR tasks over the baseline models? (Section 5. \n\n\nEXPERIMENTAL RESULTS\n\n\nSPTAR vs Baseline Models (RQ1)\n\nAs presented in Table 6, our SPTAR approach obtains the best results for all three retrievers, showcasing an average improvement of 7.3% to 18.04% in NDCG@10 compared with BM25. The next is InPars [2] which improves ColBERT and BM25CE in NDCG@10 by 3.08% and 7.3% on average, respectively compared with BM25 while for DPR, InPars [2] is 11.15% lower than BM25. W/O Aug exhibits the worest performance that cannot beat BM25. These outcomes serve as compelling evidence for the effectiveness of our proposed SPTAR model. By harnessing the benefits of soft prompt tuning and LLMs, our model generates high-quality weak queries that  greatly enhance DR tasks. Moreover, the consistent improvements observed across all three retrievers substantiate the general applicability of our approach, extending beyond specific dense retrievers.\n\nNotably, among the three retrievers, BM25CE consistently achieves the best overall results across all three methods (W/O, InPars [2], and SPTAR), aligning with the findings of other studies such as [32].\n\nIt is worth noting that in the absence of augmentation data, all three dense retrievers perform worse than the unsupervised model BM25. This underscores the significant reliance of DR on domain-specific labeled data and highlights the limitations of directly training dense retrievers in scenarios with limited ground-truth data, where the expected performance may not be attainable.\n\n\nAblation Study\n\nIn this section, we primarily aim to assess the individual contributions of different modules to the overall performance of our proposed SPTAR. We conducted experiments focusing on evaluating the perplexity and NDCG@10 metrics. The perplexity metric, derived from the 100 dataset, provided insights into the model's text generation quality. The default NDCG@10 scores in this section are obtained by evaluating the SPTAR-DPR model trained, evaluated, and tested on 50 + 100 + , and respectively. We didn't filter so that the NDCG@10 score can directly reflect the quality of the weak data.\n\n\nThe Impact of Soft Prompt Tuning Module (RQ2).\n\nTo gain deeper insights into the learning process of parameters , we employed the t-SNE algorithm [34] to visualize the virtual token vectors of the prompt when are converged with different datasets and LLMs. Figure 3a presents the distribution of virtual token vectors in a two-dimensional space. We utilized the LLaMA-7B language model with a virtual token length = 50 for this experiment. The red and blue points indicate the MS MARCO and FiQA datasets, respectively. The visual analysis clearly reveals that the virtual token vectors from the two datasets exhibit distinct distributions in the two-dimensional space, with minimal overlap. Notably, at the model initialization phase, both datasets share the same prompt , making the observed changes in vector distribution after convergence particularly significant. These findings highlight the remarkable capability of prompt tuning to distill domain-specific knowledge from datasets to the learned prompt token vectors. This accomplishment is particularly noteworthy in the scenario where ground-truth data are too limited that human-written prompts struggle to capture domain-specific information and incorporate it effectively into the prompt design.\n\nIn Figure 3b, the points of different colors represent distinct LLMs: GPT-2, LLaMA-7B, and Vicuna-7B. We kept all the hyperparameters same except for the language model to analyze the influence of different language models on the parameters . The dispersion of points with the same color indicates the extent of parameter updated during training. Figure 3b clearly illustrates that the red point cloud representing the GPT-2 model has less dispersion, with points tightly clustered together. In contrast, the blue point cloud representing LLaMA-7B and the green point cloud representing Vicuna-7B exhibit greater dispersion of virtual token vectors. This observation suggests that, when trained on the same dataset, the LLaMA-7B and Vicuna-7B models enable the soft prompt module to acquire more domain-specific knowledge, leading to an enhancement in the generation of synthesized queries. Moreover, similar findings were obtained when decoding the virtual tokens into corresponding words. For instance, after training the GPT-2 model, we observed that the resulting soft prompt merely replicates the prompt tokens used during initialization, essentially duplicating the manual prompt without additional learning. In contrast, when decoding the virtual token vectors into words utilizing the LLaMA-7B and Vicuna-7B, we discovered that these models not only retain the initial prompt tokens but also acquire additional symbols and representations associated with relevant text, such as \"query,\" \"rewrite\", \"argument\", \"enhance\" and \"adding\", indicating parameters does learn task-specific knowledge.\n\nIn Figure 3c, we analyze the influence of different soft prompt lengths on the soft prompt tuning module by examining the distribution of virtual token vectors of the learned soft prompt in vector space. This experiment was conducted on LLaMA-7B and dataset MS MARCO and all the hyperparameters are same except for the soft prompt length. The three lengths 40, 50, and 80 are represented by the colors red, blue, and green, respectively. From the point distribution in Figure 3c, we observe partial overlap between the red and blue points, as well as some distinct points. As the virtual token length increases, the embedding distribution area of the longer soft prompt encompasses the regions corresponding to the shorter ones: 40 and 50. This outcome is consistent with our expectations: with different lengths of soft prompts, the embedding distributions of soft prompts' virtual tokens are different. Nevertheless, the distributions of the three soft prompts with different lengths should demonstrate relative concentration with substantial overlapping regions.\n\nFor RQ2, we have conclusions: a) we can distinguish the datasets from the learned soft prompts, demonstrating that soft prompt tuning does learn task-specific soft prompts; b) both the LLMs and the length of soft prompts influence the learned soft prompts.\nLLM ( )/ (\u03a6) Best Epoch # GPT-2 0.0308% 17 LLaMA-7B 0.0030% 5 Vicuna-7B\n0.0030% 4 Table 7: Efficiency evaluation of SPTAR's soft prompt tuning module on MS MARCO 50 and 100 (Section 3.1).\n\n\n5.2.2\n\nThe Efficiency of Soft-Prompt Tuning (RQ3). Table 7 presents a comparison of the training parameters and convergence efficiency achieved through soft prompt tuning, utilizing different language models during the training process on the MS MARCO dataset. For the soft prompt tuning module in our proposed SPTAR, although the count of LLM's original parameters \u03a6 is extremely large, \u03a6 remain frozen and do not require fine-tuning. The count of trainable parameters associated with the fine-tuning of the soft prompt is much smaller. The percentage values in the second column reveal that the fine-tuning of the soft prompt module necessitates a remarkably small number of parameters , roughly amounting to 0.003% of the count of \u03a6. Notably, the count of does not increase alongside the expansion of \u03a6. This characteristic significantly enhances the practicality and training efficiency of SPTAR, as we can fine-tune task-specific soft prompts with a minimal fraction of parameters for optimization. Furthermore, for a new task or dataset, SPTAR can swiftly complete the fine-tuning process of the soft prompt tuning module within a few epochs. As exemplified in the third column of the table, we examined the convergence speed of the soft prompt tuning model on the evaluation dataset 100 (Section 3.1) by the best epoch number and the lower this number is, the faster it converges. It becomes apparent that employing a more advanced language model expedites the convergence of the soft prompt tuning module, requiring a mere four or five epochs for convergence. Considering both the count of and the convergence speed, we can confidently conclude that the soft prompt tuning module leverages the advantages offered by LLMs while effectively mitigating the computational resource consumption associated with fine-tuning the whole LLMs.\n\nIn conclusion, the soft prompt tuning model only fine-tunes a small part of the parameters , and the training converges quickly on LLMs.  augmentor module does influence the quality of the generated weak data, so it is necessary to select certain document-query pairs from . In this section, we study the impact of SPTAR's soft prompt filter module. In Table 8, we report the best results of SPTAR-DPR (Section 5.2.6): a) for MS MARCO, we report the results of SPTAR-DPR with LLaMA-7B and = 2; b) for FiQA-2018, we report the results of SPTAR-DPR with LLaMA-7B and = 1. The SPTAR-DPR is trained on 50 + 100 + and tested on . The best and worst example pairs in Table 8 are filtered by the method proposed in Section 3.3.\n\n\nThe Impact of Soft\n\nAs shown in Table 8, the results unequivocally demonstrate that the soft prompt filter significantly enhances performance across all comparisons. Specifically, we observe a noteworthy 12.60% to 98.59% decrease in perplexity and a substantial 3.67% to 11.44% improvement on NDCG@10 in the downstream DPR model. Furthermore, our experimental findings indicate that while the utilization of in-context learning theory, complemented by limited examples, greatly enhances the quality of generated weak queries, the choice of example document-query pairs also exerts a considerable influence on text generation quality. \n\n\n5.2.4\n\nThe Impact of Weak Data Filter Module (RQ5). We utilized different top-to filter the generated weak data to get and tested SPTAR-DPR model trained on 50 + 100 + on to show how many improvements we can obtain compared with the method without weak data filter. We choose the best parameters LLM and from Section 5.2.3 and Section 5.2.6, then, we fix them in this section to study the impact of top-only. As shown in Figure 4, on MS MARCO, SPTAR-DPR model without the data filter gets an NDCG@10 score of 0.2319 while it gets a NDCG@10 score of 0.2580 with data filter top-=30. On FiQA-2018, SPTAR-DPR with filter top-=70 gets the highest NDCG@10 score of 0.2404, while it gets an NDCG@10 score of 0.2242 without data filter. The consistent improvements across different datasets prove the effectiveness of the weak data filter module (Section 3.5). As we did not find any pattern between the top-and the NDCG@10 metric, in practice, top-is a hyperparameter and needs to be tuned for different datasets. \n\n\n5.2.5\n\nThe Impact of (RQ6). In this section, we present an analysis of the impact of different training sizes in SPTAR's soft prompt tuning module. To evaluate the impact of , we first conducted soft prompt tuning on and evaluated the PPL on 100 . PPL is a direct metric to measure the impact of on the quality of generated weak queries. Then, we generated and tested the SPTAR-DPR model trained on + 100 + on . NDCG@10 score is applied to measure the impact of on downstream DR models, like DPR. As shown in Figure 5, the findings conclusively demonstrate substantial improvements when employing soft prompt tuning with varying training sizes compared with the results obtained without soft prompt tuning. For instance, when = 50, PPL is decreased by 99.78%, and an impressive 37.66% enhancement is observed. PPL is much easier than NDCG@10 to be improved, which means that there is a gap between the two metrics.\n\nDifferent from InPars [2] and Promptagator [6], which only utilizes several example document-query pairs in human-written prompts, our experimental results highlight the advantages of a little larger training size in soft prompt tuning, leading to better performance. This is evident in both the decreased PPL, as well as the improvement of NDCG@10 in downstream tasks as the training size expands.\n\n\n5.2.6\n\nThe Impact of (RQ7). In SPTAR's soft prompt agumentor module, when tagging the unlabeled documents with weak queries, filtered example document-query pairs are utilized to instruct the LLM. In this section, we explore the impact of different . We first chose LLaMA-7B as the LLM and did soft prompt tuning on 50 . PPL is computed on 100 . Then, with the filtered example document-query pairs from SPTAR's soft prompt filter module (Section 3.3), we generated . Finally, SPTAR-DPR trained on 50 + 100 + is tested on to compute NDCG@10. We also did the same experiments on Vicuna, and we found LLaMA-7B model consistently delivers better results than Vicuna-7B model, no matter = 1 or = 2, so, we only report the results on LLaMA-7B in Figure 6. As shown in Figure 6, for dataset MS MARCO, = 2 achieves the best performance in terms of perplexity and NDCG@10. In contrast, for dataset FiQA-2008, = 1 demonstrates superior performance. This is inconsistent with our expectation that the bigger is the better the PPL and NDCG@10 are. We attribute this inconsistency to varying dataset distributions. Given that most QA datasets in which a document has multiple relevant queries and each query is only based on a subset of the document, leading to increased uncertainty and heightened learning complexity for the model. Consequently, these factors contribute to divergent outcomes for different datasets. Thus, we recognize the need for further investigation and exploration of this matter in future studies.\n\n\nCONCLUSION AND FUTURE WORK\n\nIn this paper, we introduce the Soft Prompt Tuning for Augmenting DR (SPTAR) framework as a solution to address the challenge of limited domain-specific training data in DR tasks. Our approach leverages soft prompt tuning to optimize prompts using a limited ground truth dataset. By prompting the Language Models (LLMs) with these optimized prompts, we generate weak queries for unlabeled documents, resulting in an abundant collection of weak document-query pairs for training domain-specific dense retrievers. To further enhance the quality of the generated weak tagged queries, we incorporate a soft prompt filter that selects high-quality example document-query pairs in the prompt. The effectiveness of our proposed approach is validated through comprehensive experiments. This work represents an initial step toward a promising research direction. Moving forward, we plan to assess the generalizability of SPTAR by evaluating it on more datasets. Additionally, investigating the feasibility of learning a multi-task soft prompt tuning module to enhance efficiency is another direction to be explored.\n\nFigure 2 (\n2b) illustrates the soft prompt tuning module, where the red boxes represent the parameters to be optimized during model training and the green boxes represent LLM's original parameters \u03a6 that are retained. Let (\u00b7) denotes the prompt'\n\nFigure 2 :\n2The main architecture of the proposed SPTAR: a) The same LLM is shared by soft prompt tuning module, soft prompt filter module and soft prompt augmentor module; b) soft prompt tuning module fixs the LLM's original parameters \u03a6 and only fine-tune the parameters of soft prompt's embedding layer on the sampled small dataset (Section 3.1); c) soft prompt filter module fixs the learned parameters , and for each group of sampled example document-query pairs, computes the loss on evaluation dataset. The group of example document-qeury pairs with the smallest loss will be utilized in soft prompt augmentor module; d) with the learned parameters and a group of filtered example document-query pairs, soft prompt augmentor module iterates over the unlabeled document dataset to generate weak queries.\n\n\n: 2 https://github.com/beir-cellar/beir/blob/main/examples/retrieval/training/trai n_sbert.py 3 https://github.com/thakur-nandan/beir-ColBERT a) BM25 is evaluated on the original testing split ; b) W/O Aug models are trained on datasets 50 and 100 utilized to finetune the soft prompt; c) InPars [2] models are trained on 50 and 100 plus ( filtered by Section 3.4) generated by human-written prompts. d) SPTAR's soft prompt tuing module (SPTAR-Tuning) is trained on 50 and evaluated on 100 ; SP-TAR's DR models (SPTAR-DR) are trained on 50 and 100 plus ( filtered by Section 3.4) generated by soft prompt augmentor (Section 3.4); e) W/O Aug, InPars [2] and SPTAR are all evaluated and tested on the same splits for a fair comparison.\n\n1 )\n1RQ2: During the soft prompt tuning process, does the soft prompt tuning module indeed distill the knowledge from the dataset to the learned soft prompt? What factors contribute to the learned soft prompts? (Section 5.2.1) RQ3: What are the costs of the soft prompt tuning module? Does the soft prompt tuning module greatly increase the training time and computational resources? (Section 5.2.2) RQ4: What specific role does the soft prompt filter play in SPTAR? Which metric can be utilized to filter the example document-query pairs? (Section 5.2.3) RQ5: Can the weak data filter further improve the performances of DR models? (Section 5.2.4) RQ6: For SPTAR's soft prompt tuning module, what is the influence of the size of training data ? Is a larger better than a smaller one? (Section 5.2.5) RQ7: For SPTAR's soft prompt augmentor module, what is the influence of the number of example document-query pairs ? Is a larger better than a smaller one? (Section 5.2.6)\n\nFigure 3 :\n3T-SNE embedding visualization of soft prompt's virtual tokens: a) soft prompt's virtual tokens with different datasets; b) soft prompt's virtual tokens with different LLMs; c) virtual tokens of soft prompt with different lengths.\n\nFigure 4 :\n4SPTAR-DPR NDCG@10 scores with different topof weak data filter. SPTAR-DPR is trained on 50 + 100 + (Section 4.1). Results are obtained on LLaMA-7B. For MS MARCO and FiQA-2018, = 2 and = 1 respectively.\n\nFigure 5 :\n5Evaluation of SPTAR-DPR with different . SPTAR-DPR is trained on + 100 + and tested on . Results are obtained on LLaMA-7B and MS MARCO.\n\nFigure 6 :\n6Evaluation of SPTAR-DPR with different . SPTAR-DPR is trained on + 100 + and tested on . Results are obtained on LLaMA-7B.\n\n\nDPR measures the similarity between query embedding and document embeddings and then maximizes the log-likelihood of the positive passage. A variant of DPR is to utilize one BERT by concatenating query and document as input and extracting the query embedding and document embedding after the encoding. The query encoder and document encoder of ColBERT[11] 2 RELATED WORK \n2.1 Dense Retrieval \n\nDR converts the queries and documents into dense vectors on which \nthe ANN index can be built for fast search. DPR [10] is a two-tower \nstructure: one BERT model for queries and another for documents. \nFor each query with one positive document and several negative \ndocuments, \n\nTable 1 :\n1The format of For each training epoch, we first random sample documentquery pairs from training dataset as example documentquery pairs ( , ) =1 , then iterate over the left document-query\n\n\nStatistics of datasets in BEIR benchmark. Avg. D/Q indicates the average number of relevant documents per query.Task \n\nDomain \nDataset \n\nTrain \nEval \nTest \nAvg. Word Lengths \n\n#Pairs #Query #Query #Corpus Avg. D/Q Query Document \nPassage Retrieval \nMisc. \nMS MARCO [20] 532,761 \n-\n6,980 \n8,841,823 \n1.1 \n5.96 \n55.98 \nQuestion Answering Finance \nFiQA-2018 [17] \n14,166 \n500 \n648 \n57,638 \n2.6 \n10.77 \n132.32 \nTable 2: \n\nTable 3 :\n3Dataset partition for different methods.\n\nTable 4 :\n4Hyperparameters of soft prompt tuningHyperparameters \nDPR \nColBERT \n\nBatch Size \n32 \n32 \nMax Length \n350 \n350 \nLearning Rate \n2 \u2212 5 \n2 \u2212 5 \nDDP \nNo \nYes \nOptimizer \nAdamW \nAdamW \nEarly Stop \n10 \nNone \nMax epochs \n20 \n20 \nGPU \n4 A100s (40G) 4 A100s (40G) \n\n\n\nTable 5 :\n5Hyperparameters of DR Models\n\n\nPrompt Filter Module (RQ4). With the learned parameters in SPTAR's soft prompt tuning module, we find the example document-query pairs in SPTAR's soft promptDataset \nFliter \nPPL (Dec%) \nNDCG@10 (Imp%) \n\nMS MARCO \nWorst \n4.1934 \n0.2132 \nBest 3.6649 (+12.60%) \n0.2376 (+11.44%) \n\nFiQA-2018 \nWorst \n410.9207 \n0.1855 \nBest 5.7898 (+98.59%) \n0.1923 (+3.67%) \n\n\n\nTable 8 :\n8Evaluation of SPTAR-DPR with the best and worst example document-query pairs in soft prompt augmentor module. SPTAR-DPR is trained on + 100 + and tested on . Results are obtained on LLaMA-7B. For MS MARCO and FiQA-2018, = 2 and = 1 respectively.\n\nBridging the lexical chasm: statistical approaches to answer-finding. Adam L Berger, Rich Caruana, David Cohn, Dayne Freitag, Vibhu O Mittal, SIGIR. ACM. Adam L. Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu O. Mittal. 2000. Bridging the lexical chasm: statistical approaches to answer-finding. In SIGIR. ACM, 192-199.\n\nMarzieh Fadaee, and Rodrigo Frassetto Nogueira. 2022. InPars: Unsupervised Dataset Generation for Information Retrieval. Henrique Luiz, Hugo Bonifacio, Abonizio, SIGIR. ACM. Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Fras- setto Nogueira. 2022. InPars: Unsupervised Dataset Generation for Information Retrieval. In SIGIR. ACM, 2387-2392.\n\nInPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers. Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, Eric Nyberg, CoRR abs/2301.02998Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu- pervised Training of Efficient Rankers. CoRR abs/2301.02998 (2023).\n\n. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskeverand Dario Amodei. 2020. Language Models are Few-Shot Learners.Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. (2020).\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/\n\nZhuyun Dai, Y Vincent, Ji Zhao, Yi Ma, Jianmo Luan, Jing Ni, Anton Lu, Kelvin Bakalov, Guu, B Keith, Ming-Wei Hall, Chang, arXiv:2209.117552022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprintZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022).\n\nPrecise Zero-Shot Dense Retrieval without Relevance Labels. Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan, CoRR abs/2212.10496Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot Dense Retrieval without Relevance Labels. CoRR abs/2212.10496 (2022).\n\nSoft Prompt Decoding for Multilingual Dense Retrieval. Zhiqi Huang, Hansi Zeng, Hamed Zamani, James Allan, CoRR abs/2305.09025Zhiqi Huang, Hansi Zeng, Hamed Zamani, and James Allan. 2023. Soft Prompt Decoding for Multilingual Dense Retrieval. CoRR abs/2305.09025 (2023).\n\nInPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. Vitor Jeronymo, Henrique Luiz, Hugo Bonifacio, Marzieh Abonizio, Roberto Fadaee, De Alencar, Jakub Lotufo, Rodrigo Frassetto Zavrel, Nogueira, CoRR abs/2301.01820Vitor Jeronymo, Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto de Alencar Lotufo, Jakub Zavrel, and Rodrigo Frassetto Nogueira. 2023. InPars-v2: Large Language Models as Efficient Dataset Generators for Informa- tion Retrieval. CoRR abs/2301.01820 (2023).\n\nDense passage retrieval for opendomain question answering. Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, arXiv:2004.04906arXiv preprintVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open- domain question answering. arXiv preprint arXiv:2004.04906 (2020).\n\nColbert: Efficient and effective passage search via contextualized late interaction over bert. Omar Khattab, Matei Zaharia, SIGIR. ACM. Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In SIGIR. ACM, 39-48.\n\nThe Power of Scale for Parameter-Efficient Prompt Tuning. Brian Lester, Rami Al-Rfou, Noah Constant, EMNLP. Association for Computational Linguistics. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In EMNLP. Association for Computational Linguistics, 3045-3059.\n\nPrefix-Tuning: Optimizing Continuous Prompts for Generation. Lisa Xiang, Percy Li, Liang, ACL/IJCNLP. Association for Computational Linguistics. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In ACL/IJCNLP. Association for Computational Linguis- tics, 4582-4597.\n\n. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher R\u00e9, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J Orr, Lucia Zheng, Mert Y\u00fcksekg\u00f6n\u00fcl, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic Evaluation of Language Models. CoRR abs/2211.09110 (2022Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi- hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R\u00e9, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Y\u00fcksekg\u00f6n\u00fcl, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic Evaluation of Language Models. CoRR abs/2211.09110 (2022).\n\nToward Reproducible Baselines: The Open-Source IR Reproducibility Challenge. Jimmy Lin, Matt Crane, Andrew Trotman, Jamie Callan, Ishan Chattopadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, Sebastiano Vigna, ECIR. Springer9626Jimmy Lin, Matt Crane, Andrew Trotman, Jamie Callan, Ishan Chattopadhyaya, John Foley, Grant Ingersoll, Craig MacDonald, and Sebastiano Vigna. 2016. To- ward Reproducible Baselines: The Open-Source IR Reproducibility Challenge. In ECIR (Lecture Notes in Computer Science, Vol. 9626). Springer, 408-420.\n\nZero-Shot Listwise Document Reranking with a Large Language Model. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, Jimmy Lin, arXiv:2305.02156arXiv preprintXueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-Shot Listwise Document Reranking with a Large Language Model. arXiv preprint arXiv:2305.02156 (2023).\n\nWWW'18 Open Challenge: Financial Opinion Mining and Question Answering. Macedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas, Brian Davis, Ross Mcdermott, Manel Zarrouk, Alexandra Balahur, WWW. ACMMacedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas, Brian Davis, Ross McDer- mott, Manel Zarrouk, and Alexandra Balahur. 2018. WWW'18 Open Challenge: Financial Opinion Mining and Question Answering. In WWW. ACM, 1941-1942.\n\nLysandre Debut, Younes Belkada, and Sayak Paul. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. Sourab Mangrulkar, Sylvain Gugger, Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. https://github.com/huggingface/peft.\n\nLearning to Compress Prompts with Gist Tokens. Jesse Mu, Lisa Xiang, Noah D Li, Goodman, CoRR abs/2304.08467Jesse Mu, Xiang Lisa Li, and Noah D. Goodman. 2023. Learning to Compress Prompts with Gist Tokens. CoRR abs/2304.08467 (2023).\n\nMS MARCO: A human generated machine reading comprehension dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, choice. 2640660Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. choice 2640 (2016), 660.\n\nDocument ranking with a pretrained sequence-to-sequence model. Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin, arXiv:2003.06713arXiv preprintRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a pretrained sequence-to-sequence model. arXiv preprint arXiv:2003.06713 (2020).\n\nTrue Few-Shot Learning with Language Models. Ethan Perez, Douwe Kiela, Kyunghyun Cho, NeurIPS. Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True Few-Shot Learning with Language Models. In NeurIPS. 11054-11070.\n\nLarge Language Models are Effective Text Rankers with Pairwise Ranking Prompting. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, Michael Bendersky, CoRR abs/2306.17563Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. 2023. Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. CoRR abs/2306.17563 (2023).\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 21Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485-5551.\n\nThe probabilistic relevance framework: BM25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends\u00ae in Information Retrieval. 3Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends\u00ae in Information Retrieval 3, 4 (2009), 333-389.\n\nImproving passage retrieval with zero-shot question generation. Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-Tau Yih, Joelle Pineau, Luke Zettlemoyer, arXiv:2204.07496arXiv preprintDevendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with zero-shot question generation. arXiv preprint arXiv:2204.07496 (2022).\n\nColbertv2: Effective and efficient retrieval via lightweight late interaction. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia, arXiv:2112.01488arXiv preprintKeshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. arXiv preprint arXiv:2112.01488 (2021).\n\nHow many data points is a prompt worth. Le Teven, Alexander M Scao, Rush, arXiv:2103.08493arXiv preprintTeven Le Scao and Alexander M Rush. 2021. How many data points is a prompt worth? arXiv preprint arXiv:2103.08493 (2021).\n\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, Zhaochun Ren, arXiv:2304.09542Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. arXiv preprintWeiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. arXiv preprint arXiv:2304.09542 (2023).\n\nXiao Weng Lam Tam, Kaixuan Liu, Lilong Ji, Xingjian Xue, Yuxiao Zhang, Jiahua Dong, Liu, arXiv:2207.07087Maodi Hu, and Jie Tang. 2022. Parameter-efficient prompt tuning makes generalized and calibrated neural text retrievers. arXiv preprintWeng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Ji- ahua Liu, Maodi Hu, and Jie Tang. 2022. Parameter-efficient prompt tuning makes generalized and calibrated neural text retrievers. arXiv preprint arXiv:2207.07087 (2022).\n\nDPTDR: Deep Prompt Tuning for Dense Passage Retrieval. Zhengyang Tang, Benyou Wang, Ting Yao, COLING. International Committee on Computational Linguistics. Zhengyang Tang, Benyou Wang, and Ting Yao. 2022. DPTDR: Deep Prompt Tuning for Dense Passage Retrieval. In COLING. International Committee on Computational Linguistics, 1193-1202.\n\nBEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, Iryna Gurevych, arXiv:2104.08663arXiv preprintNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663 (2021).\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).\n\nVisualizing data using t-SNE. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 911Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008).\n\nSuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. (2019), 3261- 3275.\n\nMiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou, Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. (2020).\n\nMultitask Prompt Tuning Enables Parameter-Efficient Transfer Learning. Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rog\u00e9rio Feris, Huan Sun, Yoon Kim, ICLR. OpenReview.netZhen Wang, Rameswar Panda, Leonid Karlinsky, Rog\u00e9rio Feris, Huan Sun, and Yoon Kim. 2023. Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning. In ICLR. OpenReview.net.\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M Andrew, Quoc V Dai, Le, arXiv:2109.01652arXiv preprintJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk, arXiv:2007.00808Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprintLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor nega- tive contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808 (2020).\n\nDual Context-Guided Continuous Prompt Tuning for Few-Shot Learning. Jie Zhou, Le Tian, Houjin Yu, Zhou Xiao, Hui Su, Jie Zhou, ACL. Association for Computational LinguisticsJie Zhou, Le Tian, Houjin Yu, Zhou Xiao, Hui Su, and Jie Zhou. 2022. Dual Context-Guided Continuous Prompt Tuning for Few-Shot Learning. In ACL. Association for Computational Linguistics, 79-84.\n\nRecall, precision and average precision. Mu Zhu, 26Department of Statistics and Actuarial Science, University of WaterlooMu Zhu. 2004. Recall, precision and average precision. Department of Statistics and Actuarial Science, University of Waterloo, Waterloo 2, 30 (2004), 6.\n", "annotations": {"author": "[{\"end\":107,\"start\":80},{\"end\":118,\"start\":108},{\"end\":141,\"start\":119},{\"end\":182,\"start\":142},{\"end\":223,\"start\":183},{\"end\":264,\"start\":224}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":88},{\"end\":117,\"start\":115},{\"end\":126,\"start\":122}]", "author_first_name": "[{\"end\":87,\"start\":80},{\"end\":114,\"start\":108},{\"end\":121,\"start\":119}]", "author_affiliation": "[{\"end\":181,\"start\":143},{\"end\":222,\"start\":184},{\"end\":263,\"start\":225}]", "title": "[{\"end\":77,\"start\":1},{\"end\":341,\"start\":265}]", "venue": null, "abstract": "[{\"end\":1925,\"start\":515}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2015,\"start\":2011},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2097,\"start\":2094},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2301,\"start\":2297},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2319,\"start\":2315},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2324,\"start\":2320},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3050,\"start\":3046},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3054,\"start\":3051},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3169,\"start\":3166},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3181,\"start\":3177},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3197,\"start\":3194},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3602,\"start\":3598},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3729,\"start\":3726},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3750,\"start\":3747},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4245,\"start\":4241},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4263,\"start\":4259},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4361,\"start\":4357},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4429,\"start\":4425},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4447,\"start\":4443},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4452,\"start\":4448},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4706,\"start\":4702},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4715,\"start\":4711},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5969,\"start\":5966},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6100,\"start\":6096},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6845,\"start\":6841},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7032,\"start\":7028},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7524,\"start\":7521},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7526,\"start\":7524},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7528,\"start\":7526},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7530,\"start\":7528},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7532,\"start\":7530},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7535,\"start\":7532},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7562,\"start\":7558},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7595,\"start\":7591},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7598,\"start\":7595},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7601,\"start\":7598},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7613,\"start\":7610},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7709,\"start\":7706},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7878,\"start\":7875},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7934,\"start\":7930},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8102,\"start\":8099},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8120,\"start\":8117},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8660,\"start\":8657},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8711,\"start\":8707},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8766,\"start\":8762},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9274,\"start\":9271},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9297,\"start\":9294},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9318,\"start\":9315},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9386,\"start\":9383},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9536,\"start\":9532},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9819,\"start\":9815},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10530,\"start\":10526},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10550,\"start\":10546},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10888,\"start\":10884},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11018,\"start\":11014},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11038,\"start\":11034},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11368,\"start\":11364},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11790,\"start\":11786},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11982,\"start\":11979},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15815,\"start\":15811},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15914,\"start\":15910},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19380,\"start\":19377},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19398,\"start\":19395},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19514,\"start\":19511},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20472,\"start\":20468},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20491,\"start\":20487},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20506,\"start\":20502},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20584,\"start\":20580},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21222,\"start\":21219},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21224,\"start\":21222},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21227,\"start\":21224},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21230,\"start\":21227},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21233,\"start\":21230},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21236,\"start\":21233},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21507,\"start\":21504},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22413,\"start\":22409},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22429,\"start\":22425},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22695,\"start\":22692},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22802,\"start\":22798},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22824,\"start\":22820},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22913,\"start\":22910},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22950,\"start\":22947},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23157,\"start\":23154},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23244,\"start\":23241},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23519,\"start\":23516},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24054,\"start\":24051},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24818,\"start\":24815},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24888,\"start\":24884},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26035,\"start\":26031},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35412,\"start\":35409},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35433,\"start\":35430},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42304,\"start\":42300}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38682,\"start\":38436},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39493,\"start\":38683},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40229,\"start\":39494},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41203,\"start\":40230},{\"attributes\":{\"id\":\"fig_5\"},\"end\":41446,\"start\":41204},{\"attributes\":{\"id\":\"fig_6\"},\"end\":41661,\"start\":41447},{\"attributes\":{\"id\":\"fig_7\"},\"end\":41810,\"start\":41662},{\"attributes\":{\"id\":\"fig_8\"},\"end\":41946,\"start\":41811},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42620,\"start\":41947},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42820,\"start\":42621},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43239,\"start\":42821},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43292,\"start\":43240},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":43561,\"start\":43293},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":43602,\"start\":43562},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":43960,\"start\":43603},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":44218,\"start\":43961}]", "paragraph": "[{\"end\":2790,\"start\":1941},{\"end\":4362,\"start\":2792},{\"end\":5586,\"start\":4364},{\"end\":7315,\"start\":5588},{\"end\":10328,\"start\":7343},{\"end\":12347,\"start\":10346},{\"end\":12630,\"start\":12349},{\"end\":13061,\"start\":12632},{\"end\":13878,\"start\":13071},{\"end\":14660,\"start\":13899},{\"end\":15701,\"start\":14662},{\"end\":15917,\"start\":15724},{\"end\":16723,\"start\":15939},{\"end\":16891,\"start\":16773},{\"end\":16978,\"start\":16943},{\"end\":17224,\"start\":17024},{\"end\":18585,\"start\":17281},{\"end\":18869,\"start\":18611},{\"end\":19076,\"start\":18871},{\"end\":19349,\"start\":19078},{\"end\":20054,\"start\":19370},{\"end\":20723,\"start\":20074},{\"end\":20953,\"start\":20778},{\"end\":21635,\"start\":20973},{\"end\":22588,\"start\":21658},{\"end\":23565,\"start\":22609},{\"end\":23796,\"start\":23588},{\"end\":24684,\"start\":23854},{\"end\":24889,\"start\":24686},{\"end\":25274,\"start\":24891},{\"end\":25882,\"start\":25293},{\"end\":27141,\"start\":25933},{\"end\":28742,\"start\":27143},{\"end\":29809,\"start\":28744},{\"end\":30067,\"start\":29811},{\"end\":30255,\"start\":30140},{\"end\":32098,\"start\":30265},{\"end\":32820,\"start\":32100},{\"end\":33457,\"start\":32843},{\"end\":34468,\"start\":33467},{\"end\":35385,\"start\":34478},{\"end\":35785,\"start\":35387},{\"end\":37298,\"start\":35795},{\"end\":38435,\"start\":37329}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15938,\"start\":15918},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16772,\"start\":16724},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16942,\"start\":16892},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17023,\"start\":16979},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17259,\"start\":17225},{\"attributes\":{\"id\":\"formula_5\"},\"end\":30139,\"start\":30068}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":15975,\"start\":15968},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18348,\"start\":18341},{\"end\":20053,\"start\":20046},{\"end\":20563,\"start\":20556},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20722,\"start\":20715},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":20952,\"start\":20945},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":21061,\"start\":21054},{\"end\":21328,\"start\":21321},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21593,\"start\":21586},{\"end\":23877,\"start\":23870},{\"end\":30157,\"start\":30150},{\"end\":30316,\"start\":30309},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":32460,\"start\":32453},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":32768,\"start\":32761},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":32862,\"start\":32855}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1939,\"start\":1927},{\"attributes\":{\"n\":\"2.2\"},\"end\":7341,\"start\":7318},{\"attributes\":{\"n\":\"2.3\"},\"end\":10344,\"start\":10331},{\"attributes\":{\"n\":\"3\"},\"end\":13069,\"start\":13064},{\"attributes\":{\"n\":\"3.1\"},\"end\":13897,\"start\":13881},{\"attributes\":{\"n\":\"3.2\"},\"end\":15722,\"start\":15704},{\"attributes\":{\"n\":\"3.3\"},\"end\":17279,\"start\":17261},{\"attributes\":{\"n\":\"3.4\"},\"end\":18609,\"start\":18588},{\"attributes\":{\"n\":\"3.5\"},\"end\":19368,\"start\":19352},{\"attributes\":{\"n\":\"3.6\"},\"end\":20072,\"start\":20057},{\"attributes\":{\"n\":\"4\"},\"end\":20757,\"start\":20726},{\"attributes\":{\"n\":\"4.2\"},\"end\":20776,\"start\":20760},{\"end\":20971,\"start\":20956},{\"attributes\":{\"n\":\"4.3\"},\"end\":21656,\"start\":21638},{\"attributes\":{\"n\":\"4.4\"},\"end\":22607,\"start\":22591},{\"attributes\":{\"n\":\"4.5\"},\"end\":23586,\"start\":23568},{\"attributes\":{\"n\":\"5\"},\"end\":23819,\"start\":23799},{\"attributes\":{\"n\":\"5.1\"},\"end\":23852,\"start\":23822},{\"attributes\":{\"n\":\"5.2\"},\"end\":25291,\"start\":25277},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":25931,\"start\":25885},{\"end\":30263,\"start\":30258},{\"attributes\":{\"n\":\"5.2.3\"},\"end\":32841,\"start\":32823},{\"end\":33465,\"start\":33460},{\"end\":34476,\"start\":34471},{\"end\":35793,\"start\":35788},{\"attributes\":{\"n\":\"6\"},\"end\":37327,\"start\":37301},{\"end\":38447,\"start\":38437},{\"end\":38694,\"start\":38684},{\"end\":40234,\"start\":40231},{\"end\":41215,\"start\":41205},{\"end\":41458,\"start\":41448},{\"end\":41673,\"start\":41663},{\"end\":41822,\"start\":41812},{\"end\":42631,\"start\":42622},{\"end\":43250,\"start\":43241},{\"end\":43303,\"start\":43294},{\"end\":43572,\"start\":43563},{\"end\":43971,\"start\":43962}]", "table": "[{\"end\":42620,\"start\":42305},{\"end\":43239,\"start\":42935},{\"end\":43561,\"start\":43342},{\"end\":43960,\"start\":43762}]", "figure_caption": "[{\"end\":38682,\"start\":38449},{\"end\":39493,\"start\":38696},{\"end\":40229,\"start\":39496},{\"end\":41203,\"start\":40236},{\"end\":41446,\"start\":41217},{\"end\":41661,\"start\":41460},{\"end\":41810,\"start\":41675},{\"end\":41946,\"start\":41824},{\"end\":42305,\"start\":41949},{\"end\":42820,\"start\":42633},{\"end\":42935,\"start\":42823},{\"end\":43292,\"start\":43252},{\"end\":43342,\"start\":43305},{\"end\":43602,\"start\":43574},{\"end\":43762,\"start\":43605},{\"end\":44218,\"start\":43973}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13037,\"start\":13029},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13091,\"start\":13083},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18044,\"start\":18036},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18631,\"start\":18623},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26151,\"start\":26142},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27155,\"start\":27146},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27499,\"start\":27490},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28756,\"start\":28747},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29222,\"start\":29213},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":33889,\"start\":33881},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":34988,\"start\":34980},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":36537,\"start\":36529},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":36559,\"start\":36551}]", "bib_author_first_name": "[{\"end\":44294,\"start\":44290},{\"end\":44296,\"start\":44295},{\"end\":44309,\"start\":44305},{\"end\":44324,\"start\":44319},{\"end\":44336,\"start\":44331},{\"end\":44351,\"start\":44346},{\"end\":44353,\"start\":44352},{\"end\":44682,\"start\":44674},{\"end\":44693,\"start\":44689},{\"end\":44996,\"start\":44990},{\"end\":45013,\"start\":45006},{\"end\":45026,\"start\":45021},{\"end\":45042,\"start\":45036},{\"end\":45056,\"start\":45050},{\"end\":45069,\"start\":45064},{\"end\":45086,\"start\":45082},{\"end\":45338,\"start\":45335},{\"end\":45340,\"start\":45339},{\"end\":45356,\"start\":45348},{\"end\":45367,\"start\":45363},{\"end\":45382,\"start\":45375},{\"end\":45397,\"start\":45392},{\"end\":45414,\"start\":45406},{\"end\":45431,\"start\":45425},{\"end\":45451,\"start\":45445},{\"end\":45465,\"start\":45459},{\"end\":45480,\"start\":45474},{\"end\":45497,\"start\":45489},{\"end\":45512,\"start\":45507},{\"end\":45535,\"start\":45527},{\"end\":45548,\"start\":45545},{\"end\":45564,\"start\":45559},{\"end\":45578,\"start\":45572},{\"end\":45593,\"start\":45587},{\"end\":45595,\"start\":45594},{\"end\":45612,\"start\":45605},{\"end\":45624,\"start\":45617},{\"end\":45644,\"start\":45633},{\"end\":45656,\"start\":45652},{\"end\":45667,\"start\":45663},{\"end\":45683,\"start\":45676},{\"end\":46402,\"start\":46395},{\"end\":46418,\"start\":46411},{\"end\":46425,\"start\":46423},{\"end\":46435,\"start\":46431},{\"end\":46451,\"start\":46443},{\"end\":46459,\"start\":46456},{\"end\":46474,\"start\":46467},{\"end\":46488,\"start\":46482},{\"end\":46504,\"start\":46497},{\"end\":46519,\"start\":46513},{\"end\":46521,\"start\":46520},{\"end\":46941,\"start\":46935},{\"end\":46948,\"start\":46947},{\"end\":46960,\"start\":46958},{\"end\":46969,\"start\":46967},{\"end\":46980,\"start\":46974},{\"end\":46991,\"start\":46987},{\"end\":47001,\"start\":46996},{\"end\":47012,\"start\":47006},{\"end\":47028,\"start\":47027},{\"end\":47044,\"start\":47036},{\"end\":47442,\"start\":47438},{\"end\":47456,\"start\":47448},{\"end\":47466,\"start\":47461},{\"end\":47477,\"start\":47472},{\"end\":47712,\"start\":47707},{\"end\":47725,\"start\":47720},{\"end\":47737,\"start\":47732},{\"end\":47751,\"start\":47746},{\"end\":48021,\"start\":48016},{\"end\":48040,\"start\":48032},{\"end\":48051,\"start\":48047},{\"end\":48070,\"start\":48063},{\"end\":48088,\"start\":48081},{\"end\":48114,\"start\":48109},{\"end\":48130,\"start\":48123},{\"end\":48140,\"start\":48131},{\"end\":48519,\"start\":48511},{\"end\":48537,\"start\":48531},{\"end\":48549,\"start\":48544},{\"end\":48562,\"start\":48555},{\"end\":48576,\"start\":48570},{\"end\":48587,\"start\":48581},{\"end\":48601,\"start\":48596},{\"end\":48615,\"start\":48608},{\"end\":48972,\"start\":48968},{\"end\":48987,\"start\":48982},{\"end\":49228,\"start\":49223},{\"end\":49241,\"start\":49237},{\"end\":49255,\"start\":49251},{\"end\":49557,\"start\":49553},{\"end\":49570,\"start\":49565},{\"end\":49814,\"start\":49809},{\"end\":49827,\"start\":49822},{\"end\":49843,\"start\":49839},{\"end\":49857,\"start\":49849},{\"end\":49873,\"start\":49867},{\"end\":49890,\"start\":49881},{\"end\":49905,\"start\":49901},{\"end\":49919,\"start\":49913},{\"end\":49937,\"start\":49931},{\"end\":49948,\"start\":49942},{\"end\":49964,\"start\":49956},{\"end\":49980,\"start\":49973},{\"end\":49992,\"start\":49987},{\"end\":50000,\"start\":49998},{\"end\":50017,\"start\":50008},{\"end\":50039,\"start\":50028},{\"end\":50041,\"start\":50040},{\"end\":50062,\"start\":50051},{\"end\":50072,\"start\":50067},{\"end\":50091,\"start\":50087},{\"end\":50093,\"start\":50092},{\"end\":50106,\"start\":50102},{\"end\":50121,\"start\":50117},{\"end\":50136,\"start\":50130},{\"end\":50151,\"start\":50145},{\"end\":50164,\"start\":50158},{\"end\":50176,\"start\":50170},{\"end\":50185,\"start\":50182},{\"end\":50198,\"start\":50192},{\"end\":50216,\"start\":50210},{\"end\":50218,\"start\":50217},{\"end\":50229,\"start\":50224},{\"end\":50241,\"start\":50237},{\"end\":50260,\"start\":50255},{\"end\":50275,\"start\":50269},{\"end\":50285,\"start\":50281},{\"end\":50299,\"start\":50292},{\"end\":50301,\"start\":50300},{\"end\":50317,\"start\":50313},{\"end\":50332,\"start\":50327},{\"end\":50348,\"start\":50344},{\"end\":50360,\"start\":50356},{\"end\":50370,\"start\":50366},{\"end\":50378,\"start\":50371},{\"end\":50391,\"start\":50384},{\"end\":50408,\"start\":50403},{\"end\":50427,\"start\":50418},{\"end\":50445,\"start\":50439},{\"end\":50459,\"start\":50453},{\"end\":50474,\"start\":50467},{\"end\":50493,\"start\":50486},{\"end\":50507,\"start\":50500},{\"end\":50517,\"start\":50512},{\"end\":50528,\"start\":50523},{\"end\":51533,\"start\":51528},{\"end\":51543,\"start\":51539},{\"end\":51557,\"start\":51551},{\"end\":51572,\"start\":51567},{\"end\":51586,\"start\":51581},{\"end\":51607,\"start\":51603},{\"end\":51620,\"start\":51615},{\"end\":51637,\"start\":51632},{\"end\":51659,\"start\":51649},{\"end\":52064,\"start\":52056},{\"end\":52074,\"start\":52069},{\"end\":52087,\"start\":52082},{\"end\":52102,\"start\":52097},{\"end\":52386,\"start\":52380},{\"end\":52402,\"start\":52393},{\"end\":52419,\"start\":52414},{\"end\":52434,\"start\":52429},{\"end\":52446,\"start\":52442},{\"end\":52463,\"start\":52458},{\"end\":52482,\"start\":52473},{\"end\":52845,\"start\":52839},{\"end\":52865,\"start\":52858},{\"end\":53117,\"start\":53112},{\"end\":53126,\"start\":53122},{\"end\":53138,\"start\":53134},{\"end\":53140,\"start\":53139},{\"end\":53371,\"start\":53368},{\"end\":53383,\"start\":53380},{\"end\":53398,\"start\":53395},{\"end\":53413,\"start\":53405},{\"end\":53426,\"start\":53419},{\"end\":53441,\"start\":53435},{\"end\":53454,\"start\":53452},{\"end\":53742,\"start\":53735},{\"end\":53760,\"start\":53753},{\"end\":53773,\"start\":53768},{\"end\":54017,\"start\":54012},{\"end\":54030,\"start\":54025},{\"end\":54047,\"start\":54038},{\"end\":54270,\"start\":54266},{\"end\":54280,\"start\":54276},{\"end\":54294,\"start\":54291},{\"end\":54307,\"start\":54300},{\"end\":54321,\"start\":54316},{\"end\":54333,\"start\":54326},{\"end\":54346,\"start\":54340},{\"end\":54357,\"start\":54352},{\"end\":54369,\"start\":54363},{\"end\":54386,\"start\":54379},{\"end\":54400,\"start\":54393},{\"end\":54786,\"start\":54781},{\"end\":54799,\"start\":54795},{\"end\":54813,\"start\":54809},{\"end\":54832,\"start\":54823},{\"end\":54844,\"start\":54838},{\"end\":54860,\"start\":54853},{\"end\":54874,\"start\":54869},{\"end\":54884,\"start\":54881},{\"end\":54896,\"start\":54889},{\"end\":55290,\"start\":55283},{\"end\":55306,\"start\":55302},{\"end\":55615,\"start\":55607},{\"end\":55634,\"start\":55630},{\"end\":55648,\"start\":55642},{\"end\":55661,\"start\":55656},{\"end\":55681,\"start\":55674},{\"end\":55693,\"start\":55687},{\"end\":55706,\"start\":55702},{\"end\":56063,\"start\":56057},{\"end\":56079,\"start\":56075},{\"end\":56092,\"start\":56089},{\"end\":56117,\"start\":56106},{\"end\":56130,\"start\":56125},{\"end\":56425,\"start\":56423},{\"end\":56444,\"start\":56433},{\"end\":56616,\"start\":56610},{\"end\":56630,\"start\":56622},{\"end\":56641,\"start\":56636},{\"end\":56653,\"start\":56646},{\"end\":56664,\"start\":56659},{\"end\":56678,\"start\":56670},{\"end\":57011,\"start\":57007},{\"end\":57033,\"start\":57026},{\"end\":57045,\"start\":57039},{\"end\":57058,\"start\":57050},{\"end\":57070,\"start\":57064},{\"end\":57084,\"start\":57078},{\"end\":57564,\"start\":57555},{\"end\":57577,\"start\":57571},{\"end\":57588,\"start\":57584},{\"end\":57932,\"start\":57926},{\"end\":57945,\"start\":57941},{\"end\":57962,\"start\":57955},{\"end\":57979,\"start\":57971},{\"end\":57997,\"start\":57992},{\"end\":58264,\"start\":58260},{\"end\":58281,\"start\":58274},{\"end\":58297,\"start\":58290},{\"end\":58313,\"start\":58307},{\"end\":58334,\"start\":58324},{\"end\":58352,\"start\":58344},{\"end\":58367,\"start\":58362},{\"end\":58390,\"start\":58386},{\"end\":58822,\"start\":58815},{\"end\":58847,\"start\":58839},{\"end\":59117,\"start\":59113},{\"end\":59128,\"start\":59124},{\"end\":59150,\"start\":59144},{\"end\":59168,\"start\":59159},{\"end\":59182,\"start\":59176},{\"end\":59197,\"start\":59192},{\"end\":59208,\"start\":59204},{\"end\":59221,\"start\":59215},{\"end\":59223,\"start\":59222},{\"end\":59573,\"start\":59567},{\"end\":59584,\"start\":59580},{\"end\":59592,\"start\":59590},{\"end\":59605,\"start\":59599},{\"end\":59614,\"start\":59611},{\"end\":59625,\"start\":59621},{\"end\":59891,\"start\":59887},{\"end\":59906,\"start\":59898},{\"end\":59920,\"start\":59914},{\"end\":59939,\"start\":59932},{\"end\":59951,\"start\":59947},{\"end\":59961,\"start\":59957},{\"end\":60229,\"start\":60224},{\"end\":60242,\"start\":60235},{\"end\":60251,\"start\":60250},{\"end\":60267,\"start\":60261},{\"end\":60279,\"start\":60274},{\"end\":60283,\"start\":60280},{\"end\":60294,\"start\":60289},{\"end\":60302,\"start\":60299},{\"end\":60316,\"start\":60315},{\"end\":60331,\"start\":60325},{\"end\":60590,\"start\":60587},{\"end\":60605,\"start\":60598},{\"end\":60615,\"start\":60613},{\"end\":60629,\"start\":60620},{\"end\":60642,\"start\":60636},{\"end\":60652,\"start\":60648},{\"end\":60668,\"start\":60662},{\"end\":60682,\"start\":60676},{\"end\":61123,\"start\":61120},{\"end\":61132,\"start\":61130},{\"end\":61145,\"start\":61139},{\"end\":61154,\"start\":61150},{\"end\":61164,\"start\":61161},{\"end\":61172,\"start\":61169},{\"end\":61464,\"start\":61462}]", "bib_author_last_name": "[{\"end\":44303,\"start\":44297},{\"end\":44317,\"start\":44310},{\"end\":44329,\"start\":44325},{\"end\":44344,\"start\":44337},{\"end\":44360,\"start\":44354},{\"end\":44687,\"start\":44683},{\"end\":44703,\"start\":44694},{\"end\":44713,\"start\":44705},{\"end\":45004,\"start\":44997},{\"end\":45019,\"start\":45014},{\"end\":45034,\"start\":45027},{\"end\":45048,\"start\":45043},{\"end\":45062,\"start\":45057},{\"end\":45080,\"start\":45070},{\"end\":45093,\"start\":45087},{\"end\":45346,\"start\":45341},{\"end\":45361,\"start\":45357},{\"end\":45373,\"start\":45368},{\"end\":45390,\"start\":45383},{\"end\":45404,\"start\":45398},{\"end\":45423,\"start\":45415},{\"end\":45443,\"start\":45432},{\"end\":45457,\"start\":45452},{\"end\":45472,\"start\":45466},{\"end\":45487,\"start\":45481},{\"end\":45505,\"start\":45498},{\"end\":45525,\"start\":45513},{\"end\":45543,\"start\":45536},{\"end\":45557,\"start\":45549},{\"end\":45570,\"start\":45565},{\"end\":45585,\"start\":45579},{\"end\":45603,\"start\":45596},{\"end\":45615,\"start\":45613},{\"end\":45631,\"start\":45625},{\"end\":45650,\"start\":45645},{\"end\":45661,\"start\":45657},{\"end\":45674,\"start\":45668},{\"end\":45690,\"start\":45684},{\"end\":46409,\"start\":46403},{\"end\":46421,\"start\":46419},{\"end\":46429,\"start\":46426},{\"end\":46441,\"start\":46436},{\"end\":46454,\"start\":46452},{\"end\":46465,\"start\":46460},{\"end\":46480,\"start\":46475},{\"end\":46495,\"start\":46489},{\"end\":46511,\"start\":46505},{\"end\":46530,\"start\":46522},{\"end\":46945,\"start\":46942},{\"end\":46956,\"start\":46949},{\"end\":46965,\"start\":46961},{\"end\":46972,\"start\":46970},{\"end\":46985,\"start\":46981},{\"end\":46994,\"start\":46992},{\"end\":47004,\"start\":47002},{\"end\":47020,\"start\":47013},{\"end\":47025,\"start\":47022},{\"end\":47034,\"start\":47029},{\"end\":47049,\"start\":47045},{\"end\":47056,\"start\":47051},{\"end\":47446,\"start\":47443},{\"end\":47459,\"start\":47457},{\"end\":47470,\"start\":47467},{\"end\":47484,\"start\":47478},{\"end\":47718,\"start\":47713},{\"end\":47730,\"start\":47726},{\"end\":47744,\"start\":47738},{\"end\":47757,\"start\":47752},{\"end\":48030,\"start\":48022},{\"end\":48045,\"start\":48041},{\"end\":48061,\"start\":48052},{\"end\":48079,\"start\":48071},{\"end\":48095,\"start\":48089},{\"end\":48107,\"start\":48097},{\"end\":48121,\"start\":48115},{\"end\":48147,\"start\":48141},{\"end\":48157,\"start\":48149},{\"end\":48529,\"start\":48520},{\"end\":48542,\"start\":48538},{\"end\":48553,\"start\":48550},{\"end\":48568,\"start\":48563},{\"end\":48579,\"start\":48577},{\"end\":48594,\"start\":48588},{\"end\":48606,\"start\":48602},{\"end\":48619,\"start\":48616},{\"end\":48980,\"start\":48973},{\"end\":48995,\"start\":48988},{\"end\":49235,\"start\":49229},{\"end\":49249,\"start\":49242},{\"end\":49264,\"start\":49256},{\"end\":49563,\"start\":49558},{\"end\":49573,\"start\":49571},{\"end\":49580,\"start\":49575},{\"end\":49820,\"start\":49815},{\"end\":49837,\"start\":49828},{\"end\":49847,\"start\":49844},{\"end\":49865,\"start\":49858},{\"end\":49879,\"start\":49874},{\"end\":49899,\"start\":49891},{\"end\":49911,\"start\":49906},{\"end\":49929,\"start\":49920},{\"end\":49940,\"start\":49938},{\"end\":49954,\"start\":49949},{\"end\":49971,\"start\":49965},{\"end\":49985,\"start\":49981},{\"end\":49996,\"start\":49993},{\"end\":50006,\"start\":50001},{\"end\":50026,\"start\":50018},{\"end\":50049,\"start\":50042},{\"end\":50065,\"start\":50063},{\"end\":50085,\"start\":50073},{\"end\":50100,\"start\":50094},{\"end\":50115,\"start\":50107},{\"end\":50128,\"start\":50122},{\"end\":50143,\"start\":50137},{\"end\":50156,\"start\":50152},{\"end\":50168,\"start\":50165},{\"end\":50180,\"start\":50177},{\"end\":50190,\"start\":50186},{\"end\":50208,\"start\":50199},{\"end\":50222,\"start\":50219},{\"end\":50235,\"start\":50230},{\"end\":50253,\"start\":50242},{\"end\":50267,\"start\":50261},{\"end\":50279,\"start\":50276},{\"end\":50290,\"start\":50286},{\"end\":50311,\"start\":50302},{\"end\":50325,\"start\":50318},{\"end\":50342,\"start\":50333},{\"end\":50354,\"start\":50349},{\"end\":50364,\"start\":50361},{\"end\":50382,\"start\":50379},{\"end\":50401,\"start\":50392},{\"end\":50416,\"start\":50409},{\"end\":50437,\"start\":50428},{\"end\":50451,\"start\":50446},{\"end\":50465,\"start\":50460},{\"end\":50484,\"start\":50475},{\"end\":50498,\"start\":50494},{\"end\":50510,\"start\":50508},{\"end\":50521,\"start\":50518},{\"end\":50534,\"start\":50529},{\"end\":51537,\"start\":51534},{\"end\":51549,\"start\":51544},{\"end\":51565,\"start\":51558},{\"end\":51579,\"start\":51573},{\"end\":51601,\"start\":51587},{\"end\":51613,\"start\":51608},{\"end\":51630,\"start\":51621},{\"end\":51647,\"start\":51638},{\"end\":51665,\"start\":51660},{\"end\":52067,\"start\":52065},{\"end\":52080,\"start\":52075},{\"end\":52095,\"start\":52088},{\"end\":52106,\"start\":52103},{\"end\":52391,\"start\":52387},{\"end\":52412,\"start\":52403},{\"end\":52427,\"start\":52420},{\"end\":52440,\"start\":52435},{\"end\":52456,\"start\":52447},{\"end\":52471,\"start\":52464},{\"end\":52490,\"start\":52483},{\"end\":52856,\"start\":52846},{\"end\":52872,\"start\":52866},{\"end\":53120,\"start\":53118},{\"end\":53132,\"start\":53127},{\"end\":53143,\"start\":53141},{\"end\":53152,\"start\":53145},{\"end\":53378,\"start\":53372},{\"end\":53393,\"start\":53384},{\"end\":53403,\"start\":53399},{\"end\":53417,\"start\":53414},{\"end\":53433,\"start\":53427},{\"end\":53450,\"start\":53442},{\"end\":53459,\"start\":53455},{\"end\":53751,\"start\":53743},{\"end\":53766,\"start\":53761},{\"end\":53777,\"start\":53774},{\"end\":54023,\"start\":54018},{\"end\":54036,\"start\":54031},{\"end\":54051,\"start\":54048},{\"end\":54274,\"start\":54271},{\"end\":54289,\"start\":54281},{\"end\":54298,\"start\":54295},{\"end\":54314,\"start\":54308},{\"end\":54324,\"start\":54322},{\"end\":54338,\"start\":54334},{\"end\":54350,\"start\":54347},{\"end\":54361,\"start\":54358},{\"end\":54377,\"start\":54370},{\"end\":54391,\"start\":54387},{\"end\":54410,\"start\":54401},{\"end\":54793,\"start\":54787},{\"end\":54807,\"start\":54800},{\"end\":54821,\"start\":54814},{\"end\":54836,\"start\":54833},{\"end\":54851,\"start\":54845},{\"end\":54867,\"start\":54861},{\"end\":54879,\"start\":54875},{\"end\":54887,\"start\":54885},{\"end\":54900,\"start\":54897},{\"end\":55300,\"start\":55291},{\"end\":55315,\"start\":55307},{\"end\":55628,\"start\":55616},{\"end\":55640,\"start\":55635},{\"end\":55654,\"start\":55649},{\"end\":55672,\"start\":55662},{\"end\":55685,\"start\":55682},{\"end\":55700,\"start\":55694},{\"end\":55718,\"start\":55707},{\"end\":56073,\"start\":56064},{\"end\":56087,\"start\":56080},{\"end\":56104,\"start\":56093},{\"end\":56123,\"start\":56118},{\"end\":56138,\"start\":56131},{\"end\":56431,\"start\":56426},{\"end\":56449,\"start\":56445},{\"end\":56455,\"start\":56451},{\"end\":56620,\"start\":56617},{\"end\":56634,\"start\":56631},{\"end\":56644,\"start\":56642},{\"end\":56657,\"start\":56654},{\"end\":56668,\"start\":56665},{\"end\":56682,\"start\":56679},{\"end\":57024,\"start\":57012},{\"end\":57037,\"start\":57034},{\"end\":57048,\"start\":57046},{\"end\":57062,\"start\":57059},{\"end\":57076,\"start\":57071},{\"end\":57089,\"start\":57085},{\"end\":57094,\"start\":57091},{\"end\":57569,\"start\":57565},{\"end\":57582,\"start\":57578},{\"end\":57592,\"start\":57589},{\"end\":57939,\"start\":57933},{\"end\":57953,\"start\":57946},{\"end\":57969,\"start\":57963},{\"end\":57990,\"start\":57980},{\"end\":58006,\"start\":57998},{\"end\":58272,\"start\":58265},{\"end\":58288,\"start\":58282},{\"end\":58305,\"start\":58298},{\"end\":58322,\"start\":58314},{\"end\":58342,\"start\":58335},{\"end\":58360,\"start\":58353},{\"end\":58384,\"start\":58368},{\"end\":58396,\"start\":58391},{\"end\":58404,\"start\":58398},{\"end\":58837,\"start\":58823},{\"end\":58854,\"start\":58848},{\"end\":59122,\"start\":59118},{\"end\":59142,\"start\":59129},{\"end\":59157,\"start\":59151},{\"end\":59174,\"start\":59169},{\"end\":59190,\"start\":59183},{\"end\":59202,\"start\":59198},{\"end\":59213,\"start\":59209},{\"end\":59230,\"start\":59224},{\"end\":59578,\"start\":59574},{\"end\":59588,\"start\":59585},{\"end\":59597,\"start\":59593},{\"end\":59609,\"start\":59606},{\"end\":59619,\"start\":59615},{\"end\":59630,\"start\":59626},{\"end\":59896,\"start\":59892},{\"end\":59912,\"start\":59907},{\"end\":59930,\"start\":59921},{\"end\":59945,\"start\":59940},{\"end\":59955,\"start\":59952},{\"end\":59965,\"start\":59962},{\"end\":60233,\"start\":60230},{\"end\":60248,\"start\":60243},{\"end\":60259,\"start\":60252},{\"end\":60272,\"start\":60268},{\"end\":60287,\"start\":60284},{\"end\":60297,\"start\":60295},{\"end\":60309,\"start\":60303},{\"end\":60313,\"start\":60311},{\"end\":60323,\"start\":60317},{\"end\":60335,\"start\":60332},{\"end\":60339,\"start\":60337},{\"end\":60596,\"start\":60591},{\"end\":60611,\"start\":60606},{\"end\":60618,\"start\":60616},{\"end\":60634,\"start\":60630},{\"end\":60646,\"start\":60643},{\"end\":60660,\"start\":60653},{\"end\":60674,\"start\":60669},{\"end\":60691,\"start\":60683},{\"end\":61128,\"start\":61124},{\"end\":61137,\"start\":61133},{\"end\":61148,\"start\":61146},{\"end\":61159,\"start\":61155},{\"end\":61167,\"start\":61165},{\"end\":61177,\"start\":61173},{\"end\":61468,\"start\":61465}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6749682},\"end\":44551,\"start\":44220},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":250340449},\"end\":44915,\"start\":44553},{\"attributes\":{\"doi\":\"CoRR abs/2301.02998\",\"id\":\"b2\"},\"end\":45331,\"start\":44917},{\"attributes\":{\"id\":\"b3\"},\"end\":46393,\"start\":45333},{\"attributes\":{\"id\":\"b4\"},\"end\":46933,\"start\":46395},{\"attributes\":{\"doi\":\"arXiv:2209.11755\",\"id\":\"b5\"},\"end\":47376,\"start\":46935},{\"attributes\":{\"doi\":\"CoRR abs/2212.10496\",\"id\":\"b6\"},\"end\":47650,\"start\":47378},{\"attributes\":{\"doi\":\"CoRR abs/2305.09025\",\"id\":\"b7\"},\"end\":47922,\"start\":47652},{\"attributes\":{\"doi\":\"CoRR abs/2301.01820\",\"id\":\"b8\"},\"end\":48450,\"start\":47924},{\"attributes\":{\"doi\":\"arXiv:2004.04906\",\"id\":\"b9\"},\"end\":48871,\"start\":48452},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":216553223},\"end\":49163,\"start\":48873},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":233296808},\"end\":49490,\"start\":49165},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":230433941},\"end\":49805,\"start\":49492},{\"attributes\":{\"id\":\"b13\"},\"end\":51449,\"start\":49807},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2741762},\"end\":51987,\"start\":51451},{\"attributes\":{\"doi\":\"arXiv:2305.02156\",\"id\":\"b15\"},\"end\":52306,\"start\":51989},{\"attributes\":{\"id\":\"b16\"},\"end\":52719,\"start\":52308},{\"attributes\":{\"id\":\"b17\"},\"end\":53063,\"start\":52721},{\"attributes\":{\"doi\":\"CoRR abs/2304.08467\",\"id\":\"b18\"},\"end\":53299,\"start\":53065},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1289517},\"end\":53670,\"start\":53301},{\"attributes\":{\"doi\":\"arXiv:2003.06713\",\"id\":\"b20\"},\"end\":53965,\"start\":53672},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":235166749},\"end\":54182,\"start\":53967},{\"attributes\":{\"doi\":\"CoRR abs/2306.17563\",\"id\":\"b22\"},\"end\":54696,\"start\":54184},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":204838007},\"end\":55225,\"start\":54698},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207178704},\"end\":55541,\"start\":55227},{\"attributes\":{\"doi\":\"arXiv:2204.07496\",\"id\":\"b25\"},\"end\":55976,\"start\":55543},{\"attributes\":{\"doi\":\"arXiv:2112.01488\",\"id\":\"b26\"},\"end\":56381,\"start\":55978},{\"attributes\":{\"doi\":\"arXiv:2103.08493\",\"id\":\"b27\"},\"end\":56608,\"start\":56383},{\"attributes\":{\"doi\":\"arXiv:2304.09542\",\"id\":\"b28\"},\"end\":57005,\"start\":56610},{\"attributes\":{\"doi\":\"arXiv:2207.07087\",\"id\":\"b29\"},\"end\":57498,\"start\":57007},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":251765451},\"end\":57835,\"start\":57500},{\"attributes\":{\"doi\":\"arXiv:2104.08663\",\"id\":\"b31\"},\"end\":58258,\"start\":57837},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b32\"},\"end\":58783,\"start\":58260},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":5855042},\"end\":59027,\"start\":58785},{\"attributes\":{\"id\":\"b34\"},\"end\":59465,\"start\":59029},{\"attributes\":{\"id\":\"b35\"},\"end\":59814,\"start\":59467},{\"attributes\":{\"id\":\"b36\"},\"end\":60172,\"start\":59816},{\"attributes\":{\"doi\":\"arXiv:2109.01652\",\"id\":\"b37\"},\"end\":60585,\"start\":60174},{\"attributes\":{\"doi\":\"arXiv:2007.00808\",\"id\":\"b38\"},\"end\":61050,\"start\":60587},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":248779896},\"end\":61419,\"start\":61052},{\"attributes\":{\"id\":\"b40\"},\"end\":61694,\"start\":61421}]", "bib_title": "[{\"end\":44288,\"start\":44220},{\"end\":44672,\"start\":44553},{\"end\":48966,\"start\":48873},{\"end\":49221,\"start\":49165},{\"end\":49551,\"start\":49492},{\"end\":51526,\"start\":51451},{\"end\":53366,\"start\":53301},{\"end\":54010,\"start\":53967},{\"end\":54779,\"start\":54698},{\"end\":55281,\"start\":55227},{\"end\":57553,\"start\":57500},{\"end\":58813,\"start\":58785},{\"end\":61118,\"start\":61052}]", "bib_author": "[{\"end\":44305,\"start\":44290},{\"end\":44319,\"start\":44305},{\"end\":44331,\"start\":44319},{\"end\":44346,\"start\":44331},{\"end\":44362,\"start\":44346},{\"end\":44689,\"start\":44674},{\"end\":44705,\"start\":44689},{\"end\":44715,\"start\":44705},{\"end\":45006,\"start\":44990},{\"end\":45021,\"start\":45006},{\"end\":45036,\"start\":45021},{\"end\":45050,\"start\":45036},{\"end\":45064,\"start\":45050},{\"end\":45082,\"start\":45064},{\"end\":45095,\"start\":45082},{\"end\":45348,\"start\":45335},{\"end\":45363,\"start\":45348},{\"end\":45375,\"start\":45363},{\"end\":45392,\"start\":45375},{\"end\":45406,\"start\":45392},{\"end\":45425,\"start\":45406},{\"end\":45445,\"start\":45425},{\"end\":45459,\"start\":45445},{\"end\":45474,\"start\":45459},{\"end\":45489,\"start\":45474},{\"end\":45507,\"start\":45489},{\"end\":45527,\"start\":45507},{\"end\":45545,\"start\":45527},{\"end\":45559,\"start\":45545},{\"end\":45572,\"start\":45559},{\"end\":45587,\"start\":45572},{\"end\":45605,\"start\":45587},{\"end\":45617,\"start\":45605},{\"end\":45633,\"start\":45617},{\"end\":45652,\"start\":45633},{\"end\":45663,\"start\":45652},{\"end\":45676,\"start\":45663},{\"end\":45692,\"start\":45676},{\"end\":46411,\"start\":46395},{\"end\":46423,\"start\":46411},{\"end\":46431,\"start\":46423},{\"end\":46443,\"start\":46431},{\"end\":46456,\"start\":46443},{\"end\":46467,\"start\":46456},{\"end\":46482,\"start\":46467},{\"end\":46497,\"start\":46482},{\"end\":46513,\"start\":46497},{\"end\":46532,\"start\":46513},{\"end\":46947,\"start\":46935},{\"end\":46958,\"start\":46947},{\"end\":46967,\"start\":46958},{\"end\":46974,\"start\":46967},{\"end\":46987,\"start\":46974},{\"end\":46996,\"start\":46987},{\"end\":47006,\"start\":46996},{\"end\":47022,\"start\":47006},{\"end\":47027,\"start\":47022},{\"end\":47036,\"start\":47027},{\"end\":47051,\"start\":47036},{\"end\":47058,\"start\":47051},{\"end\":47448,\"start\":47438},{\"end\":47461,\"start\":47448},{\"end\":47472,\"start\":47461},{\"end\":47486,\"start\":47472},{\"end\":47720,\"start\":47707},{\"end\":47732,\"start\":47720},{\"end\":47746,\"start\":47732},{\"end\":47759,\"start\":47746},{\"end\":48032,\"start\":48016},{\"end\":48047,\"start\":48032},{\"end\":48063,\"start\":48047},{\"end\":48081,\"start\":48063},{\"end\":48097,\"start\":48081},{\"end\":48109,\"start\":48097},{\"end\":48123,\"start\":48109},{\"end\":48149,\"start\":48123},{\"end\":48159,\"start\":48149},{\"end\":48531,\"start\":48511},{\"end\":48544,\"start\":48531},{\"end\":48555,\"start\":48544},{\"end\":48570,\"start\":48555},{\"end\":48581,\"start\":48570},{\"end\":48596,\"start\":48581},{\"end\":48608,\"start\":48596},{\"end\":48621,\"start\":48608},{\"end\":48982,\"start\":48968},{\"end\":48997,\"start\":48982},{\"end\":49237,\"start\":49223},{\"end\":49251,\"start\":49237},{\"end\":49266,\"start\":49251},{\"end\":49565,\"start\":49553},{\"end\":49575,\"start\":49565},{\"end\":49582,\"start\":49575},{\"end\":49822,\"start\":49809},{\"end\":49839,\"start\":49822},{\"end\":49849,\"start\":49839},{\"end\":49867,\"start\":49849},{\"end\":49881,\"start\":49867},{\"end\":49901,\"start\":49881},{\"end\":49913,\"start\":49901},{\"end\":49931,\"start\":49913},{\"end\":49942,\"start\":49931},{\"end\":49956,\"start\":49942},{\"end\":49973,\"start\":49956},{\"end\":49987,\"start\":49973},{\"end\":49998,\"start\":49987},{\"end\":50008,\"start\":49998},{\"end\":50028,\"start\":50008},{\"end\":50051,\"start\":50028},{\"end\":50067,\"start\":50051},{\"end\":50087,\"start\":50067},{\"end\":50102,\"start\":50087},{\"end\":50117,\"start\":50102},{\"end\":50130,\"start\":50117},{\"end\":50145,\"start\":50130},{\"end\":50158,\"start\":50145},{\"end\":50170,\"start\":50158},{\"end\":50182,\"start\":50170},{\"end\":50192,\"start\":50182},{\"end\":50210,\"start\":50192},{\"end\":50224,\"start\":50210},{\"end\":50237,\"start\":50224},{\"end\":50255,\"start\":50237},{\"end\":50269,\"start\":50255},{\"end\":50281,\"start\":50269},{\"end\":50292,\"start\":50281},{\"end\":50313,\"start\":50292},{\"end\":50327,\"start\":50313},{\"end\":50344,\"start\":50327},{\"end\":50356,\"start\":50344},{\"end\":50366,\"start\":50356},{\"end\":50384,\"start\":50366},{\"end\":50403,\"start\":50384},{\"end\":50418,\"start\":50403},{\"end\":50439,\"start\":50418},{\"end\":50453,\"start\":50439},{\"end\":50467,\"start\":50453},{\"end\":50486,\"start\":50467},{\"end\":50500,\"start\":50486},{\"end\":50512,\"start\":50500},{\"end\":50523,\"start\":50512},{\"end\":50536,\"start\":50523},{\"end\":51539,\"start\":51528},{\"end\":51551,\"start\":51539},{\"end\":51567,\"start\":51551},{\"end\":51581,\"start\":51567},{\"end\":51603,\"start\":51581},{\"end\":51615,\"start\":51603},{\"end\":51632,\"start\":51615},{\"end\":51649,\"start\":51632},{\"end\":51667,\"start\":51649},{\"end\":52069,\"start\":52056},{\"end\":52082,\"start\":52069},{\"end\":52097,\"start\":52082},{\"end\":52108,\"start\":52097},{\"end\":52393,\"start\":52380},{\"end\":52414,\"start\":52393},{\"end\":52429,\"start\":52414},{\"end\":52442,\"start\":52429},{\"end\":52458,\"start\":52442},{\"end\":52473,\"start\":52458},{\"end\":52492,\"start\":52473},{\"end\":52858,\"start\":52839},{\"end\":52874,\"start\":52858},{\"end\":53122,\"start\":53112},{\"end\":53134,\"start\":53122},{\"end\":53145,\"start\":53134},{\"end\":53154,\"start\":53145},{\"end\":53380,\"start\":53368},{\"end\":53395,\"start\":53380},{\"end\":53405,\"start\":53395},{\"end\":53419,\"start\":53405},{\"end\":53435,\"start\":53419},{\"end\":53452,\"start\":53435},{\"end\":53461,\"start\":53452},{\"end\":53753,\"start\":53735},{\"end\":53768,\"start\":53753},{\"end\":53779,\"start\":53768},{\"end\":54025,\"start\":54012},{\"end\":54038,\"start\":54025},{\"end\":54053,\"start\":54038},{\"end\":54276,\"start\":54266},{\"end\":54291,\"start\":54276},{\"end\":54300,\"start\":54291},{\"end\":54316,\"start\":54300},{\"end\":54326,\"start\":54316},{\"end\":54340,\"start\":54326},{\"end\":54352,\"start\":54340},{\"end\":54363,\"start\":54352},{\"end\":54379,\"start\":54363},{\"end\":54393,\"start\":54379},{\"end\":54412,\"start\":54393},{\"end\":54795,\"start\":54781},{\"end\":54809,\"start\":54795},{\"end\":54823,\"start\":54809},{\"end\":54838,\"start\":54823},{\"end\":54853,\"start\":54838},{\"end\":54869,\"start\":54853},{\"end\":54881,\"start\":54869},{\"end\":54889,\"start\":54881},{\"end\":54902,\"start\":54889},{\"end\":55302,\"start\":55283},{\"end\":55317,\"start\":55302},{\"end\":55630,\"start\":55607},{\"end\":55642,\"start\":55630},{\"end\":55656,\"start\":55642},{\"end\":55674,\"start\":55656},{\"end\":55687,\"start\":55674},{\"end\":55702,\"start\":55687},{\"end\":55720,\"start\":55702},{\"end\":56075,\"start\":56057},{\"end\":56089,\"start\":56075},{\"end\":56106,\"start\":56089},{\"end\":56125,\"start\":56106},{\"end\":56140,\"start\":56125},{\"end\":56433,\"start\":56423},{\"end\":56451,\"start\":56433},{\"end\":56457,\"start\":56451},{\"end\":56622,\"start\":56610},{\"end\":56636,\"start\":56622},{\"end\":56646,\"start\":56636},{\"end\":56659,\"start\":56646},{\"end\":56670,\"start\":56659},{\"end\":56684,\"start\":56670},{\"end\":57026,\"start\":57007},{\"end\":57039,\"start\":57026},{\"end\":57050,\"start\":57039},{\"end\":57064,\"start\":57050},{\"end\":57078,\"start\":57064},{\"end\":57091,\"start\":57078},{\"end\":57096,\"start\":57091},{\"end\":57571,\"start\":57555},{\"end\":57584,\"start\":57571},{\"end\":57594,\"start\":57584},{\"end\":57941,\"start\":57926},{\"end\":57955,\"start\":57941},{\"end\":57971,\"start\":57955},{\"end\":57992,\"start\":57971},{\"end\":58008,\"start\":57992},{\"end\":58274,\"start\":58260},{\"end\":58290,\"start\":58274},{\"end\":58307,\"start\":58290},{\"end\":58324,\"start\":58307},{\"end\":58344,\"start\":58324},{\"end\":58362,\"start\":58344},{\"end\":58386,\"start\":58362},{\"end\":58398,\"start\":58386},{\"end\":58406,\"start\":58398},{\"end\":58839,\"start\":58815},{\"end\":58856,\"start\":58839},{\"end\":59124,\"start\":59113},{\"end\":59144,\"start\":59124},{\"end\":59159,\"start\":59144},{\"end\":59176,\"start\":59159},{\"end\":59192,\"start\":59176},{\"end\":59204,\"start\":59192},{\"end\":59215,\"start\":59204},{\"end\":59232,\"start\":59215},{\"end\":59580,\"start\":59567},{\"end\":59590,\"start\":59580},{\"end\":59599,\"start\":59590},{\"end\":59611,\"start\":59599},{\"end\":59621,\"start\":59611},{\"end\":59632,\"start\":59621},{\"end\":59898,\"start\":59887},{\"end\":59914,\"start\":59898},{\"end\":59932,\"start\":59914},{\"end\":59947,\"start\":59932},{\"end\":59957,\"start\":59947},{\"end\":59967,\"start\":59957},{\"end\":60235,\"start\":60224},{\"end\":60250,\"start\":60235},{\"end\":60261,\"start\":60250},{\"end\":60274,\"start\":60261},{\"end\":60289,\"start\":60274},{\"end\":60299,\"start\":60289},{\"end\":60311,\"start\":60299},{\"end\":60315,\"start\":60311},{\"end\":60325,\"start\":60315},{\"end\":60337,\"start\":60325},{\"end\":60341,\"start\":60337},{\"end\":60598,\"start\":60587},{\"end\":60613,\"start\":60598},{\"end\":60620,\"start\":60613},{\"end\":60636,\"start\":60620},{\"end\":60648,\"start\":60636},{\"end\":60662,\"start\":60648},{\"end\":60676,\"start\":60662},{\"end\":60693,\"start\":60676},{\"end\":61130,\"start\":61120},{\"end\":61139,\"start\":61130},{\"end\":61150,\"start\":61139},{\"end\":61161,\"start\":61150},{\"end\":61169,\"start\":61161},{\"end\":61179,\"start\":61169},{\"end\":61470,\"start\":61462}]", "bib_venue": "[{\"end\":44372,\"start\":44362},{\"end\":44725,\"start\":44715},{\"end\":44988,\"start\":44917},{\"end\":46641,\"start\":46532},{\"end\":47134,\"start\":47074},{\"end\":47436,\"start\":47378},{\"end\":47705,\"start\":47652},{\"end\":48014,\"start\":47924},{\"end\":48509,\"start\":48452},{\"end\":49007,\"start\":48997},{\"end\":49314,\"start\":49266},{\"end\":49635,\"start\":49582},{\"end\":51671,\"start\":51667},{\"end\":52054,\"start\":51989},{\"end\":52378,\"start\":52308},{\"end\":52837,\"start\":52721},{\"end\":53110,\"start\":53065},{\"end\":53467,\"start\":53461},{\"end\":53733,\"start\":53672},{\"end\":54060,\"start\":54053},{\"end\":54264,\"start\":54184},{\"end\":54942,\"start\":54902},{\"end\":55365,\"start\":55317},{\"end\":55605,\"start\":55543},{\"end\":56055,\"start\":55978},{\"end\":56421,\"start\":56383},{\"end\":56782,\"start\":56700},{\"end\":57231,\"start\":57112},{\"end\":57654,\"start\":57594},{\"end\":57924,\"start\":57837},{\"end\":58501,\"start\":58422},{\"end\":58892,\"start\":58856},{\"end\":59111,\"start\":59029},{\"end\":59565,\"start\":59467},{\"end\":59885,\"start\":59816},{\"end\":60222,\"start\":60174},{\"end\":60792,\"start\":60709},{\"end\":61182,\"start\":61179},{\"end\":61460,\"start\":61421}]"}}}, "year": 2023, "month": 12, "day": 17}