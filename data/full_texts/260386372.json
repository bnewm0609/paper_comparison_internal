{"id": 260386372, "updated": "2023-08-22 17:54:20.042", "metadata": {"title": "Investigating Code Generation Performance of ChatGPT with Crowdsourcing Social Data", "authors": "[{\"first\":\"Yunhe\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Sreecharan\",\"last\":\"Vanam\",\"middle\":[]},{\"first\":\"Manasa\",\"last\":\"Cherukupally\",\"middle\":[]},{\"first\":\"Weijian\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Meikang\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"Haihua\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)", "journal": "2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)", "publication_date": {"year": 2023, "month": 6, "day": 1}, "abstract": "The recent advancements in Artificial Intelligence, particularly in large language models and generative models, are reshaping the field of software engineering by enabling innovative ways of performing various tasks, such as programming, debugging, and testing. However, few existing works have thoroughly explored the potential of AI in code generation and users\u2019 attitudes toward AI-assisted coding tools. This knowledge gap leaves it unclear how AI is transforming software engineering and programming education. This paper presents a scalable crowdsourcing data-driven framework to investigate the code generation performance of generative large language models from diverse perspectives across multiple social media platforms. Specifically, we utilize ChatGPT, a popular generative large language model, as a representative example to reveal its insights and patterns in code generation. First, we propose a hybrid keyword word expansion method that integrates words suggested by topic modeling and expert knowledge to filter relevant social posts of interest on Twitter and Reddit. Then we collect 316K tweets and 3.2K Reddit posts about ChatGPT\u2019s code generation, spanning from Dec. 1, 2022 to January 31, 2023. Our data analytics show that ChatGPT has been used in more than 10 programming languages, with Python and JavaScript being the two most popular, for a diverse range of tasks such as code debugging, interview preparation, and academic assignment solving. Surprisingly, our analysis shows that fear is the dominant emotion associated with ChatGPT\u2019s code generation, overshadowing emotions of happiness, anger, surprise, and sadness. Furthermore, we construct a ChatGPT prompt and corresponding code dataset by analyzing the screen-shots of ChatGPT code generation shared on social media. This dataset enables us to evaluate the quality of the generated code, and we have released this dataset to the public. We believe the insights gained from our work will provide valuable guidance for future research on AI-powered code generation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/compsac/FengVCZQC23", "doi": "10.1109/compsac57700.2023.00117"}}, "content": {"source": {"pdf_hash": "36195ad1ee99c0b0a3c5770a0b15ef889db4b17e", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "15e837af6a8f5a58c6c7fb268cd9c1f49a6f67ad", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/36195ad1ee99c0b0a3c5770a0b15ef889db4b17e.txt", "contents": "\nInvestigating Code Generation Performance of ChatGPT with Crowdsourcing Social Data\n\n\nYunhe Feng \u2020yunhe.feng@unt.edu \nSreecharan Vanam \u2020vanamsreecharan@my.unt.edu \nManasa Cherukupally \u2020manasacherukupally@my.unt.edu\u2021wzheng@anl.gov \nWeijian Zheng \nArgonne National Laboratory\n\n\nMeikang Qiu \u00a7meikang.qiu@dsu.edu \nDakota State University\n\n\nHaihua Chen \u2020haihua.chen@unt.edu \n\nUniversity of North Texas\n\n\nInvestigating Code Generation Performance of ChatGPT with Crowdsourcing Social Data\n10.1109/COMPSAC57700.2023.00117Index Terms-ChatGPTCoding GenerationSoftware Engi- neeringLarge Language Models (LLMs)Generative ModelsSocial Media\nThe recent advancements in Artificial Intelligence, particularly in large language models and generative models, are reshaping the field of software engineering by enabling innovative ways of performing various tasks, such as programming, debugging, and testing. However, few existing works have thoroughly explored the potential of AI in code generation and users' attitudes toward AI-assisted coding tools. This knowledge gap leaves it unclear how AI is transforming software engineering and programming education. This paper presents a scalable crowdsourcing data-driven framework to investigate the code generation performance of generative large language models from diverse perspectives across multiple social media platforms. Specifically, we utilize ChatGPT, a popular generative large language model, as a representative example to reveal its insights and patterns in code generation. First, we propose a hybrid keyword word expansion method that integrates words suggested by topic modeling and expert knowledge to filter relevant social posts of interest on Twitter and Reddit. Then we collect 316K tweets and 3.2K Reddit posts about ChatGPT's code generation, spanning from Dec. 1, 2022 to January 31, 2023. Our data analytics show that ChatGPT has been used in more than 10 programming languages, with Python and JavaScript being the two most popular, for a diverse range of tasks such as code debugging, interview preparation, and academic assignment solving. Surprisingly, our analysis shows that fear is the dominant emotion associated with ChatGPT's code generation, overshadowing emotions of happiness, anger, surprise, and sadness. Furthermore, we construct a ChatGPT prompt and corresponding code dataset by analyzing the screenshots of ChatGPT code generation shared on social media. This dataset enables us to evaluate the quality of the generated code, and we have released this dataset to the public. We believe the insights gained from our work will provide valuable guidance for future research on AI-powered code generation.\n\nI. INTRODUCTION\n\nRecently, the advancements in large language models (LLMs) and generative models have revolutionized many applications, including free text generation, question answering, and document summarization, enabling a wide range of realworld services such as AI robot lawyers [1] and AI music co-creation [2]. The field of coding, which involves writing tasks in certain programming languages, is also benefiting from the rapid development of generative LLMs. However, unlike traditional writing tasks, programming requires strict adherence to syntax and logic rules, making it more challenging for generative models to produce high-quality code. Several studies have investigated the potential of LLMs in software development. For instance, Barke et al. [3] and Vaithilingam et al. [4] examined user perceptions of generative models in coding writing. However, many of these studies are based on case studies, with limited consideration of broader applications in software development. The emerging OpenAI's ChatGPT, a member of the GPT LLM family, demonstrates promising performance in code generation, attracting widespread attention from stakeholders in software engineering. As shown in Figure 1, ChatGPT can generate the bubble sort algorithm in Python with the prompt of \"write the bubble sort in Python.\" Some studies have explored the use of ChatGPT for code generation tasks [5]- [7]. Nonetheless, these studies did not comprehensively evaluate the overall effectiveness of ChatGPT as a code generation and assistance tool on a large scale.\n\nIt is challenging to conduct a large-scale study on the performance of LLMs in code generation due to the following reasons. First, programming languages exhibit diverse syntax and are applicable to a wide range of tasks. For instance, SQL is primarily utilized in database operations, while JavaScript is commonly used in web programming. Second, code generation encompasses numerous programming tasks, including debugging, testing, and programming, for various stakeholders. Moreover, conducting user studies in the lab to investigate the code generation of LLMs can be costly and timeconsuming. Therefore, conducting a comprehensive study on the performance of LLMs that covers numerous programming languages, tasks, and stakeholders poses significant challenges.\n\nTo address the aforementioned challenges, this paper pro-poses a scalable crowdsourcing data-driven framework that integrates multiple social media data sources to examine the code generation performance of ChatGPT. The proposed framework comprises three key components, namely keyword expansion, data collection, and data analytics. Specifically, we utilize topic modeling and expert knowledge to identify all keywords that are relevant to programming in the context of ChatGPT, thus expanding the seed keyword of ChatGPT.\n\nUsing these expanded keywords, we retrieved 316K tweets and 3.2K Reddit posts related to ChatGPT's code generation from December 1, 2022, to January 31, 2023. Furthermore, we conduct a comprehensive analysis using multimodal data (text and images) to answer the following research questions: 1) What are the most popular programming languages in ChatGPT usage? 2) What programming scenarios, tasks, and purposes are people using ChatGPT for? 3) What is the temporal distribution of the discussion on ChatGPT code generation? 4) How do stakeholders perceive ChatGPT code generation? 5) What are the prompts to generate code? 6) What is the quality of the code generated by ChatGPT? 7) Does the generated code present any ethical issues? To the best of our knowledge, this work is the first largescale, systematic study on emerging generative models for code writing and testing using crowdsourced social data. We summarize our contributions as follows:\n\n\u2022 We have proposed a scalable crowdsourcing and social data-driven framework for investigating the code generation capabilities of ChatGPT. \u2022 We have presented a novel hybrid keyword expansion method that incorporates words recommended by topic modeling and experts to ensure that most of the related social media posts are matched during data collection. \u2022 Our study considers multiple social media platforms (Twitter and Reddit) and multimodal data (text and image) to mitigate potential biases caused by a single data source or data type. \u2022 We have provided data analytics from multiple perspectives, including topic inference, sentiment analysis, and data quality measurement. \u2022 We have built a real-world programming dataset containing the ChatGPT prompt and the associated generated Python code. This dataset is publicly available at https://shorturl.at/oEMN2.\n\n\nII. RELATED WORK\n\nAutomatic Code Generation. Many machine learning and deep learning models [8], [9] have been explored for automatic programming. For example, Raychev et al. [10] proposed a code completion technique using statistical language models to discover highly rated sentences and recommend code completion suggestions. Sun et al. [11] introduced a novel tree-based neural architecture that incorporates grammar rules and abstract syntax tree structures into the network, and it was reported to achieve the best accuracy among all neural network-based code generation methods. Ciniselli et al. [12] conducted a detailed empirical study on BERT models for code completion and evaluated the percentage of perfect predictions that match developer-written code snippets.\n\nAs ChatGPT has gained more attention recently, some researchers have studied its use for code generation [5]- [7]. For example, Aljanabi et al. [5] listed automatic code generation as one of the open possibilities for ChatGPT. Avila et al. [6] elaborated on the programming potential of ChatGPT for implementing online behavioral tasks, including concurrent reinforcement schedules, using HTML, CSS, and JavaScript code. They created files with the extensions .html, .css, and .js, encompassing fundamental page structures like headings, style element integration, and dynamic components.\n\nAutomatic Bug Fixing. Unidentified and unsolved bugs in complex coding are always threatening the correctness and resilience of software systems. To automatically detect and fix code bugs and errors, the concept of Automated Program Repair (APR) has been proposed. Recent advancements in deep learning have facilitated the integration of APR into many large language models (LLMs). LLM-based tools such as Codex [13], CodeBERT [14], and Conversational APR [15] have been proposed for bug fixing.\n\nA recent study [16] conducted a comparative evaluation of ChatGPT's efficiency in bug fixing with other baseline tools, such as Codex [13]. About 40 of QuixBugs benchmark problems containing erroneous code were given to ChatGPT to provide solutions. The experiment results showed that ChatGPT's performance was similar to other APR tools like Codex. However, when given more context information about the problem through its dialogue box, ChatGPT's performance improved, delivering a success rate of 77.5%.\n\nInteractions and Limitations. As programming generation and assistant tools, such as CodeBERT [14] and IntelliCode Compose [17], become more widely used, there has been an increased focus on investigating the usability and interactions between users and code generation tools [3], [4], [18], [19]. For example, Barke et al. [3] identified two interaction modes between programmers and code generation tools: acceleration mode and exploration mode, by observing how 20 programmers solved various tasks using the code generation tool Copilot. Vaithilingam et al. [4] performed a study on 24 participants consisting of different groups of people with minimal and moderate experience in using Copilot and IntelliSense. By quantitative and qualitative analysis, they observed that participants who used Copilot failed to complete tasks more.\n\nAlthough advanced automatic code generation tools work fine with simple code logic, it can be challenging to handle large software engineering projects [20]. For instance, the development of a web browser involves a deep understanding of human needs that are challenging to encapsulate within the confines of simple, machine-readable specifications, which AI typically employs to produce code [20]. In addition, ethical concerns about code generation models have begun to surface.\n\n\n877\n\nFor example, Chatterjee and Dethlefs [21] found evidence of gender and racial bias in the code generated by ChatGPT, raising serious questions about the responsibility and fairness of such models.\n\nDifferent from most existing works, we collect and employ large-scale datasets collected from multiple social media platforms to evaluate the coding performance of a general-purpose conversation tool, i.e., ChatGPT. In addition to investigating user responses towards ChatGPT's coding capabilities, our study also examines the programming tasks facilitated by ChatGPT and the ethical concerns of ChatGPT.\n\n\nIII. METHOD\n\nThis section presents the proposed scalable crowdsourcing data-driven framework by introducing how to collect data of interest, how to analyze data, and how to interpret findings. Figure 2 presents the overview of the proposed framework. It consists of three primary components: Keyword Expansion and Selection, Data Collection, and Data Analytics and Pattern Recognition. Contrasting with the traditional user study oriented research, crowdsourcing frameworks are more flexible and scalable, facilitating the examination of a large population over an extended time frame [22]. We will explore each component thoroughly, assessing the efficacy of LLMs in the realm of code generation.\n\n\nA. Overview of the Proposed Framework\n\n\nB. Keyword Selection for Software Development\n\nTo ensure the quality of the collected data, we employ a hybrid approach that combines data-driven keyword expansion and expert-based keyword selection. This approach ensures that the data is comprehensive and precise, eliminating the risk of bias or incompleteness in the selection of query keywords.\n\nAs ChatGPT is one of the most popular LLMs that supports code generation, we use ChatGPT as the seed keyword to sample Twitter streams, harvesting tweets that mention this term. We then perform topic modeling to determine whether a coding-related topic is present. If a coding-related topic is observed, we add the words belonging to this topic to the expanded keyword set. If a coding-related topic is not observed, we conduct a co-occurrence word analysis and calculate the semantic similarity with the word coding to expand the candidate keywords.\n\nHowever, the data-driven keyword expansion method may result in false positives, i.e., keyword candidates irrelevant to AI-based code generation may also be included. Therefore, we manually examine all recommended keyword candidates to ensure the quality of the collected data. We first filter out irrelevant keywords and propose multiple combinations of keywords to control the precision of data collection. For example, instead of collecting all postings containing Chat-GPT, collecting postings containing both ChatGPT and coding makes the retrieved data more accurate and representative.\n\nSpecifically, we leverage Twitter Streaming APIs to sample tweet streams containing the keyword ChatGPT for over 55 hours. In total, we collect 158,452 tweets, including original tweets, retweets, and replies. After removing duplicate tweets, we had 63,716 unique tweets. We then apply the latent Dirichlet allocation (LDA) [23] model to infer topics based on these unique tweets, with the hope of discovering programming-related topics. We evaluate the number of topics ranging from 1 to 30 and find that the convergence score achieves a relatively high and stable value with the number of topics set as 22. For more details, please see Figure 12. After examining the 22 topics, we identify one of them as \"Programming,\" consisting of the following words: ask, stack, knew, write, error, diffus, run, python, stabl, scientist, email, straight, shock, gener, comput, command, use, code, notic, brain, bug, statement, think, dead, question, admit, happen, result, and overflow.\n\nCombining the words in the topic of Programming, we come up with the following keyword list -algorithm, algorithms, bug, bugs, c#, c++, code, coding, command, commands, compiler, computing, debug, debugging, error, interpreter, java, javascript, libraries, php, program, programming, python, r, Ruby, shell, software, sql, stack overflow, swift, test, testing, typescript -to crawl ChatGPT related code generation posts.\n\n\nC. Data Collection\n\nBased on the above carefully curated keywords, we leverage two social media platforms, i.e., Twitter and Reddit to collect data for further analytics.\n\n1) Twitter Data: Instead of relying on Twitter Streaming APIs, we opt to use the Twitter Historical Data Search APIs to create our Twitter dataset for the following reasons: 1) The streaming data is time-sensitive, making it impossible to retrieve older data from the debut of ChatGPT if the streaming data collection was not be launched at that time; 2) Examining only the latest data (e.g., after Feb 1, 2023) could introduce bias, as we cannot determine when ChatGPT's code generation performance was most widely discussed on social media. On the other hand, the historical tweets span the entire evolution of ChatGPT and provide a sample of user comments since its release, enhancing the representativeness and completeness of the crowdsourced opinions and feedback.\n\nTwitter provides two APIs that allow searching for historical data: the 30-Day Search API 1 enables access to data from the previous 30 days, while the Full-archive Search API 2 permits access to tweets from as far back as 2006, the year the first tweet was made. Given that ChatGPT was first introduced on November 30, 2022, we choose to use the Full-archive Search API to extract data. We specifically utilized Twitter's Academic Research API, known for its capability of executing full-archive tweet searches, to gather data related to ChatGPT from November 30, 2022, to February 1, 2023. Our search was meticulously designed only to include English tweets and exclude retweets, as indicated by the parameter \"-is:retweet lang:en.\" In addition to the text of the tweets, we also collect 1 https://tinyurl.com/2s4xt8r7 2 https://tinyurl.com/ehbsjx6v 878 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.  Fig. 2. Overview of the proposed crowdsourcing framework to investigate the programming capabilities of ChatGPT related media information from Twitter, including images, to enhance our analysis. For this study, we compiled a total of 316K tweets posted between December 1, 2022, and January 31, 2023.\n\n2) Reddit Data: Unlike Twitter, where the structure is based on users following one another, Reddit is structured around communities where posts on similar topics are grouped together. These communities are referred to as \"subreddits\" on Reddit. For instance, the subreddit /r/aww is a community where users share cute and cuddly pictures. The initial posts on Reddit are known as \"submissions,\" and the responses to these posts are called \"comments.\"\n\nTo assess the code generation ability of ChatGPT within the Reddit community, our attention is concentrated on four notable subreddits: /r/ChatGPT, /r/coding, /r/github, and /r/programming. We gather posts from these subreddits using the Search Reddit Submissions Endpoint (/reddit/search/submission) available through the Pushshift Reddit API [24]. Just like with Twitter data, we also collect multimedia data such as images embedded in Reddit posts. For the sake of this research, we have compiled 3.2K Reddit posts made between December 1, 2022, and January 31, 2023, to examine ChatGPT's code generation proficiency.\n\n\nD. Data Analytics and Pattern Recognition\n\nWe deploy natural language processing and image understanding techniques to uncover insights and identify patterns.\n\n1) Text Based Topic Discovery: To attain a comprehensive understanding of ChatGPT's deployment in code generation across social media platforms, we resort to latent Dirichlet allocation (LDA) [23], a widely utilized technique for topic modeling. This approach reveals underlying topics hidden within the collected tweets and Reddit posts. Each tweet or post is regarded as an individual document, while the entire assembly forms the corpus. During text preprocessing, we implement standard procedures such as discarding stop words and frequently occurring terms like ChatGPT, in addition to tokenizing and lemmatizing words. We then carry out a term frequency-inverse document frequency (TF-IDF) analysis on the collated documents to construct a TF-IDF-based corpus. LDA models are subsequently employed to unearth latent topics within this corpus. To determine the optimal number of topics, we use the C v metric, consistent with prior research centered on large-scale social data analysis [25], [26]. This metric, which amalgamates normalized pointwise mutual information (NPMI) and cosine similarity [27], is acknowledged as one of the most efficacious coherence measures.\n\nGiven that Twitter allows users to utilize #hashtags to indicate related topics and enhance visibility through searches, 879 we also present the distribution of #hashtags in the collected tweets. However, as #hashtags are rarely used on Reddit, we do not perform this analysis for Reddit submissions.\n\n2) Image Understanding: Given that ChatGPT operates as a text generation model, it is expected that a majority of images associated with it, especially those pertaining to code generation, shared across social media platforms, will be text-rich. To augment the practicality of these images and streamline their processing for subsequent tasks, we suggest the deployment of an Optical Character Recognition (OCR) technique to transmute the assembled images into text. We evaluate multiple OCR methodologies, including the OpenCVbacked pytesseract 3 and the deep learning-based easyOCR 4 , on our image dataset. Following a thorough evaluation of the OCR detection results, we choose easyOCR as the tool for the precise identification and extraction of text from the images.\n\n3) Code Reconstruction from Image: To reconstruct the code generated by ChatGPT, it is crucial to identify the images that contain generated code. After examining the screenshots of coding snippets, we found that all ChatGPT-generated code snippets contained the keyword Copy code in the top-right corner of the coding block, as shown in Figure 1. Therefore, we select all images containing the Copy code keyword for further analysis.\n\nWe propose two methods to recover the code generated by ChatGPT. The first one is to extract the code directly from the OCR results. We found that it is crucial to address any indentation issues for indentation-sensitive programming languages, such as Python, as a high percentage of errors can occur due to improper indentation. However, automatically indenting any given code can be a complex and challenging task. A simple script that looks for loops and specific statements to increase and decrease the indentation count does not work on all codes, especially if the code has multiple indentation styles and conditional statements.\n\nAn alternative method to obtain the code is reproducing it using the identical prompt. Specifically, we can identify the prompt and input it into ChatGPT web services 5 to generate the code. Once we have downloaded the newly produced code, we can assess and evaluate it. In our study, we adopt this reliable method to reconstruct the code generated by ChatGPT.\n\n\n4) Sentiment Analysis:\n\nConsidering that ChatGPT may elicit a wide range of emotions in the context of code generation, we believe that the traditional sentiment categories of positive, negative, and neutral might not encompass all the emotions involved. To accurately represent the varied and intricate emotions conveyed in the remarks of social media users, we decide to classify them into more comprehensive emotions: happy, angry, surprise, sad, and fear. In order to accomplish this, we employ Text2Emotion [28], a Python package proficient in scrutinizing sentiments and categorizing them into the five emotions mentioned above.\n\n\n5) Code Quality Evaluation:\n\nTo assess the quality of the code generated by ChatGPT, we are utilizing Flake8 [29], which is a wrapper around PyFlakes, pycodestyle, and Ned Batchelder's McCabe script. Flake8 allows the use of any of these tools by launching Flake8, and it assigns a unique code number to each error code. The output of warnings and errors is displayed per file. We choose Flake8 as our evaluation tool because it is one of the most powerful and flexible tools available, providing a wide range of error codes while remaining fast to run checks. Flake8 is particularly well-suited for identifying correctness and whitespace-related issues, making it an ideal choice for our purposes.\n\n\nIV. EVALUATION AND FINDINGS\n\nIn this section, we present the evaluation results and highlight our findings on the performance of code generation by ChatGPT. We summarize the topics discussed in social media posts, and the strengths and weaknesses of ChatGPT's code generation capabilities.\n\n\nA. Programming Language Distribution\n\nChatGPT supports code generation for multiple programming languages. We illustrate the popularity of the top 12 programming languages across Twitter and Reddit in Figure  3. We can see that Python is the most popular language among both communities and far ahead of other languages. Obviously, python has become the top 1 program language in many fields, such as artificial intelligence, machine learning, data analytics, automation, scientific computing, and others. JavaScript, R, and Shell/Bash, among the most popular programming languages nowadays, are also well-supported by ChatGPT. \n\n\nB. Topics Related to Code Generation\n\nWe generate topics for the tweets containing keyword Chat-GPT and programming related words using the LDA model. Based on the coherence score presented in Figure 4, we select 17 topics finally. The 17 topics and the word list of each topic are presented in Table II (see Appendix B). The topic modeling results indicate that ChatGPT has been used for different purposes regarding code generation, such as debugging codes (Topic 9 and Topic 17), testing codes/algorithms (Topic 5 and Topic 16), preparing programming interviews (Topic 2 and Topic 4), working on programming-related assignments (Topic 3 and Topic 6), and other related tasks. Twitter users also conveyed negative sentiments regarding ChatGPT's code generation capabilities (Topic 1).\n\n\nFig. 4. Coherence scores of LDA with different number of topics\n\nTo further investigate the implications and impacts of Chat-GPT on different AI technologies, applications, and industries, we extract hashtag-based topics, which are shown in Figure 5. The hashtags we use include: #AI, #OPENAI, #Artificialintelligence, #Programming, #Python, #Coding, and others. We group the hashtags into five clusters: ChatGPT, AI & ML & DS, Company, Programming, and Other Tech. Based on the topic frequency in Figure 5, ChatGPT has a great impact on AI and its related fields. Both academia and IT industry need to pay attention to this new technology. \n\n\nC. Temporal Distribution\n\nTemporal analysis can be used to examine the popularity over time. Figure 6 visualizes the daily distribution of posts on Twitter (blue) and Reddit (yellow) related to ChatGPT's code generation in the first two months after its launch. ChatGPT discussion spread faster on Twitter than on Reddit. We observe a peak of the ChatGPT code generation on Twitter and Reddit at the end of the first week of the release of ChatGPT. The popularity decreased from the second week, but somehow still very popular on both platforms. Even after two months, the attention on ChatGPT is still stable, indicating ChatGPT is helpful for code generation. \n\n\nD. Sentiment on Code Generation\n\nTo enable fine-grained sentiment analysis, we leverage Text2Emotion [28] to categorize the emotions on ChatGPT's code generation into five distinct groups: happy, angry, surprise, sad, and fear. Figure 7 presents the sentiment analysis results on eight programming languages (i.e., Python, JavaScript, R, Shell, SQL, C++, Java, and C#) across two social media platforms (i.e., Twitter and Reddit).\n\nOverall, fear emerges as the dominant emotion across both social media platforms when discussing the eight programming languages referenced above. The pervasive expression of fear concerning code generation may be attributed to concerns over job security. This is likely a response to the impressive programming capabilities already demonstrated by models like ChatGPT. Similarly, Tate et al. [30] reported that the growing use of LLMs to convert natural language descriptions into computer code had raised concerns about its implications for the existing software developer job market and the broader software industry.\n\nAnother factor potentially contributing to this fear is the perceived opacity and limited expandability of ChatGPT. In other words, there is a general uncertainty about how Chat-GPT has achieved its coding writing intelligence (especially considering that ChatGPT is not an open-source model) and how it might evolve in the future. The sense of unknown and uncertainty might amplify the fears of those using ChatGPT for coding purposes.\n\nOn the contrary, happy and angry tend to be the least frequently expressed sentiments among Twitter and Reddit users when discussing most programming languages. Upon comparing the sentiment analysis results across both social media platforms, we observe a strikingly similar pattern for all programming languages -with the exception of SQL and C++. Interestingly, Reddit users discussing SQL demonstrate a higher incidence of sad compared to their Twitter counterparts. As for C++, Reddit discourse revealed a greater prevalence of happy compared to Twitter.\n\n\nE. A Public Dataset of Prompts and Generated Code\n\nFrom the OCR results of Twitter images, we identify and extract 332 prompts covering multiple programming languages, such as Python, JavaScript, and C++. Figure 8  users prefer words such as write, code, function, and program when constructing their coding prompts. We construct a dataset of .py files for all Python-related prompts, with each .py file containing the prompt and the corresponding code generated by ChatGPT. Figure 9 shows a sample .py file from the dataset, where the prompt is commented at the beginning of the file. The complete Python dataset is publicly available at https://shorturl.at/oEMN2. \n\n\nF. Code Quality Evaluation\n\nWe submit the Python code snippets generated by ChatGPT to Flake8 as individual .py files to check for quality and errors. Flake8 identifies the error codes for each file, along with the position and description of the error. After evaluating the code snippets using Flake8, we find that the majority of the errors are pycodestyle errors, with code E (79%), followed by code W (18.52%). The least number of errors are attributed to Flake8 with code F (2.47%). Among the unique error codes, there are 13 for E, with the majority of errors linked to code E501 (line too long). Additionally, there are five unique W codes and three unique F codes. Table I provides a detailed summary of the evaluation results, including the percentage of each Flake8 code for the overall evaluation.\n\n\nG. Ethical Issues of Generated Code\n\nWhen examining the images shared on platforms like Twitter and Reddit, we come across ethical concerns related to the code generated by ChatGPT. For instance, if users directly request ChatGPT to predict an individual's seniority at work or evaluate their abilities as a scientist based on race and gender 882 information, ChatGPT will decline to provide an answer since it goes against OpenAI's ethical guidelines. However, there have been instances where ChatGPT was instructed to compose a Python function for predicting someone's seniority or assessing their competence as a scientist. Unfortunately, reports indicate that ChatGPT generated code that exhibited biases related to demographics. Figure 10 and Figure 11 demonstrate examples of such biased code. This paper presents a framework for exploring the code generation capabilities of ChatGPT through the analysis of crowdsourced data on Twitter and Reddit. The results show that Python and JavaScript are the most frequently discussed programming languages on social media and that ChatGPT is used in a variety of code generation domains, e.g., debugging codes, preparing programming interviews, and solving Fig. 11. Write a Python function to check if someone would be a good scientist based on their race and gender academic assignments. Sentiment analysis reveals that people generally have fears about the code generation capabilities of ChatGPT, rather than feeling happy, angry, surprised, or sad. The study also includes the construction of a code generation prompt dataset, which has been made publicly available, and an evaluation of the quality of code generated by ChatGPT using Flake8. We hope this work provides valuable insights into the adoption of ChatGPT in software development and programming education.\n\n\nAPPENDIX\n\n\nA. Coherence Scores of LDA with Different Number of Topics\n\nOne of the most important steps for applying topic modeling such as LDA is to select an appropriate number of topics contained by the corpus [31]. The reason is that choosing too few topics will produce over-broad topics while choosing too many topics will lead to lots of overlapping between topics. In this study, we choose the C v metric, a widely used coherence measurement to decide the optimal number of topics in our corpus. Topic coherence scores a single topic by combining normalized pointwise mutual information (NPMI) and the cosine similarity between words in the topic [27]. The higher the coherence score, the higher the quality of the generated topics; however, low-quality topics may be composed of highly unrelated words that cannot fit into another topic, leading to a low coherence score [27]. In our corpus, we evaluated the topic numbers ranging from one to thirty with 500 passes, and we repeated the experiments five times in each step when generating the topics to avoid random errors in C v metric. Figure 12 presents the evaluation results on all the tweets containing keyword ChatGPT. In this figure, the horizontal axis indicates the number of topics, the vertical axis indicates the coherence score, the top in the shadow represents the max coherence score and the bottom represents the min coherence score with the number of topics set differently. Since either the selected number of topics (k) is too big (i.e., k > 30) or too small (i.e., k<5) will make the topic interpretation problematic, we finally selected 22 topics for the highest coherence score between 5 to 30 topics. Table II illustrates the 17 topics inferred by the LDA model from the fine-toned ChatGPT's code generation related tweets. We provide the first 40 words for each topic to demonstrate the most common words. Our analysis shows that ChatGPT has been utilized for various purposes in code generation, including code writing and debugging (Topics 5, 9, and 11), preparing for programming interviews (Topics 2 and 4), working on programming-related assignments (Topics 3 and 6), and other related tasks.  Rank Topic 1  Topic 2 Topic 3 Topic 4 Topic 5   Topic 6   Topic 7   Topic 8  Topic 9 Topic 10 Topic 11 Topic 12 Topic 13   Topic 14 Topic 15 Topic 16 Topic \n\n\nB. LDA Topics Related to Code Generation on Twitter\n\nFig. 1 .\n1ChatGPT writes the bubble sort algorithm in Python\n\nFig. 3 .\n3Programming language distribution\n\nFig. 5 .\n5Hashtag-based topics. We exclude the 35.4% ratio of the #ChatGPT during visualization to prevent it from overpowering other topics\n\nFig. 6 .\n6Daily distribution of posts related to ChatGPT's code generation in the first two months after its launch\n\nFig. 7 .\n7provides a wordcloud overview of all extracted prompts, where Pythonrelated questions are the most common. In particular, Twitter 881 Sentiment analysis results on code generation for eight programming languages\n\nFig. 8 .\n8WordCloud of prompts\n\nFig. 9 .\n9A sample in the public dataset of prompts (Line 3) and generated code (Line 4 -Line 17)\n\nFig. 10 .\n10Write a Python function to predict seniority based on race and gender V. CONCLUSION\n\n\nTopic Model1. Keyword Expansion and Selection What is the temporal distributionof the discussion on ChatGPT code generation? How do stakeholders perceive ChatGPT code generation? What are the prompts to generate code?What is the quality of the code generated by ChatGPT? Does the generated code present any ethical issues?Expert \n\nKeywords \n\n2. Data Collection \n\nExamine \n\nGenerate \n\n3. Data Analytics and Pattern Recognition \n\nText Data \n\nOCR \n\nWhat are the most popular \nprogramming languages in \nChatGPT usage? \nWhat programming purposes are \npeople using ChatGPT for? \nSeed \nKeyword \n\nKeywords in \nCoding Topic \n\nKeywords by \nExperts \n\nTwitter Data \n\nReddit Data \n\nFull-archive Search API \n\nPushshift Reddit API \n\nprogramming, bugs, \nc#, c++, java \n...... \n\n/r/ChatGPT, \n/r/coding, /r/github, \n/r/programming \n\nSocial Data \n\nImage Data \n\nSpatio-temporal \nDistribution \n\nProgramming \nlang. analysis \n\nTopic Discovery \n\nSentiment \nAnalysis \n\nNatural language processing (NLP) \n\nPrompt \n\nCode \n\nChatGPT \n\nCode Quality \nEvaluation \n\nPublic Dataset \nRelease \n\nKeyword \nVisualization \n\nCode Analytics \nSocial Data \n\n\n\nTABLE I CODE\nIQUALITY RESULTS BY FLAKE8Code Description \nPercentage \n\nE501 line too long (e.g., 114 >79 characters) \n21.40% \nE231 missing whitespace after \",\" \n18.93% \nW293 blank line contains whitespace \n12.35% \nE302 expected 2 blank lines, found 1 \n11.52% \nE402 module level import not at top of file \n6.58% \nE305 expected 2 blank lines after class, found 1 \n5.76% \nE265 block comment should start with \"#\" \n5.35% \nE999 SyntaxError: invalid syntax \n2.88% \nW292 no newline at end of file \n2.06% \nE227 missing whitespace around bitwise or shift operator \n2.06% \nW191 indentation contains tabs \n1.65% \nE101 indentation contains mixed spaces and tabs \n1.65% \nF401 'torch' imported but unused \n1.65% \nE261 at least two spaces before inline comment \n1.23% \nW391 blank line at end of file \n1.23% \nW291 trailing whitespace \n1.23% \nE225 missing whitespace around operator \n0.82% \nF821 undefined name \"output value\" \n0.41% \nF811 redefinition of unused \"pymesh\" from line 5 \n0.41% \nE902 TokenError: EOF in multi-line statement \n0.41% \nE741 ambiguous variable name \"I\" \n0.41% \n\n\n\nTABLE II THE\nIIEXTRACTED TOPICS USING THE LDA TOPIC MODEL\nhttps://pypi.org/project/pytesseract/ 4 https://github.com/JaidedAI/EasyOCR 5 https://openai.com/blog/chatgpt/\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.Fig. 12. Coherence scores of LDA with different number of topics\n\nAi in legal services: new trends in aienabled legal services. M E Kauffman, M N Soares, Service Oriented Computing and Applications. 144M. E. Kauffman and M. N. Soares, \"Ai in legal services: new trends in ai- enabled legal services,\" Service Oriented Computing and Applications, vol. 14, no. 4, pp. 223-226, 2020.\n\nNovice-ai music co-creation via ai-steering tools for deep generative models. R Louie, A Coenen, C Z Huang, M Terry, C J Cai, Proceedings of the 2020 CHI conference on human factors in computing systems. the 2020 CHI conference on human factors in computing systemsR. Louie, A. Coenen, C. Z. Huang, M. Terry, and C. J. Cai, \"Novice-ai music co-creation via ai-steering tools for deep generative models,\" in Proceedings of the 2020 CHI conference on human factors in computing systems, 2020, pp. 1-13.\n\nGrounded copilot: How programmers interact with code-generating models. S Barke, M B James, N Polikarpova, S. Barke, M. B. James, and N. Polikarpova, \"Grounded copilot: How programmers interact with code-generating models,\" 2022. [Online].\n\nExpectation vs experience: Evaluating the usability of code generation tools powered by large language models. P Vaithilingam, T Zhang, E L Glassman, 10.1145/3491101.3519665Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, ser. CHI EA '22. New York, NY, USAAssociation for Computing MachineryP. Vaithilingam, T. Zhang, and E. L. Glassman, \"Expectation vs experience: Evaluating the usability of code generation tools powered by large language models,\" in Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, ser. CHI EA '22. New York, NY, USA: Association for Computing Machinery, 2022. [Online]. Available: https://doi.org/10.1145/3491101.3519665\n\nChatgpt: Open possibilities. M Aljanabi, M Ghazi, A H Ali, S A Abed, Iraqi Journal For Computer Science and Mathematics. 41M. Aljanabi, M. Ghazi, A. H. Ali, S. A. Abed et al., \"Chatgpt: Open possibilities,\" Iraqi Journal For Computer Science and Mathematics, vol. 4, no. 1, pp. 62-64, 2023.\n\nChatgpt as a support tool for online behavioral task programming. L Avila-Chauvet, D Mej\u00eda, C O Acosta Quiroz, Available at SSRN 4329020. L. Avila-Chauvet, D. Mej\u00eda, and C. O. Acosta Quiroz, \"Chatgpt as a support tool for online behavioral task programming,\" Available at SSRN 4329020, 2023.\n\nA multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, arXiv:2302.04023arXiv preprintY. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung et al., \"A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity,\" arXiv preprint arXiv:2302.04023, 2023.\n\nCode completion with neural attention and pointer networks. J Li, Y Wang, M R Lyu, I King, 10.24963/ijcai.2018/578Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence OrganizationJ. Li, Y. Wang, M. R. Lyu, and I. King, \"Code completion with neural attention and pointer networks,\" in Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 7 2018, pp. 4159-4165. [Online]. Available: https://doi.org/10.24963/ijcai.2018/578\n\nA grammar-based structural cnn decoder for code generation. Z Sun, Q Zhu, L Mou, Y Xiong, G Li, L Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Z. Sun, Q. Zhu, L. Mou, Y. Xiong, G. Li, and L. Zhang, \"A grammar-based structural cnn decoder for code generation,\" Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, pp. 7055-7062, Jul. 2019. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/4686\n\nCode completion with statistical language models. V Raychev, M Vechev, E Yahav, 10.1145/2594291.2594321Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, ser. PLDI '14. the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, ser. PLDI '14New York, NY, USAAssociation for Computing MachineryV. Raychev, M. Vechev, and E. Yahav, \"Code completion with statistical language models,\" in Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, ser. PLDI '14. New York, NY, USA: Association for Computing Machinery, 2014, p. 419-428. [Online]. Available: https://doi.org/10. 1145/2594291.2594321\n\nTreegen: A tree-based transformer architecture for code generation. Z Sun, Q Zhu, Y Xiong, Y Sun, L Mou, L Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Z. Sun, Q. Zhu, Y. Xiong, Y. Sun, L. Mou, and L. Zhang, \"Treegen: A tree-based transformer architecture for code generation,\" Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, pp. 8984-8991, Apr. 2020. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/6430\n\nAn empirical study on the usage of BERT models for code completion. M Ciniselli, N Cooper, L Pascarella, D Poshyvanyk, M D Penta, G Bavota, abs/2103.07115CoRR. M. Ciniselli, N. Cooper, L. Pascarella, D. Poshyvanyk, M. D. Penta, and G. Bavota, \"An empirical study on the usage of BERT models for code completion,\" CoRR, vol. abs/2103.07115, 2021. [Online].\n\nEvaluating large language models trained on code. M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374arXiv preprintM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., \"Evaluating large language models trained on code,\" arXiv preprint arXiv:2107.03374, 2021.\n\nCodebert: A pre-trained model for programming and natural languages. Z Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, arXiv:2002.08155arXiv preprintZ. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang et al., \"Codebert: A pre-trained model for programming and natural languages,\" arXiv preprint arXiv:2002.08155, 2020.\n\nConversational automated program repair. C S Xia, L Zhang, arXiv:2301.13246arXiv preprintC. S. Xia and L. Zhang, \"Conversational automated program repair,\" arXiv preprint arXiv:2301.13246, 2023.\n\nAn analysis of the automatic bug fixing performance of chatgpt. D Sobania, M Briesch, C Hanna, J Petke, arXiv:2301.08653arXiv preprintD. Sobania, M. Briesch, C. Hanna, and J. Petke, \"An analysis of the automatic bug fixing performance of chatgpt,\" arXiv preprint arXiv:2301.08653, 2023.\n\nIntellicode compose: Code generation using transformer. A Svyatkovskiy, S K Deng, S Fu, N Sundaresan, Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringA. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, \"Intellicode compose: Code generation using transformer,\" in Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2020, pp. 1433-1443.\n\nDiscovering the syntax and strategies of natural language programming with generative language models. E Jiang, E Toh, A Molina, K Olson, C Kayacik, A Donsbach, C J Cai, M Terry, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing SystemsE. Jiang, E. Toh, A. Molina, K. Olson, C. Kayacik, A. Donsbach, C. J. Cai, and M. Terry, \"Discovering the syntax and strategies of natural lan- guage programming with generative language models,\" in Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, 2022, pp. 1-19.\n\nIn-IDE code generation from natural language: Promise and challenges. F F Xu, B Vasilescu, G Neubig, ACM Transactions on Software Engineering and Methodology (TOSEM). 312F. F. Xu, B. Vasilescu, and G. Neubig, \"In-IDE code generation from natural language: Promise and challenges,\" ACM Transactions on Software Engineering and Methodology (TOSEM), vol. 31, no. 2, pp. 1-47, 2022.\n\nAre chatgpt and alphacode going to replace programmers?. D Castelvecchi, Nature. D. Castelvecchi, \"Are chatgpt and alphacode going to replace program- mers?\" Nature, 2022.\n\nThis new conversational ai model can be your friend, philosopher, and guide... and even your worst enemy. J Chatterjee, N Dethlefs, Patterns. 41100676J. Chatterjee and N. Dethlefs, \"This new conversational ai model can be your friend, philosopher, and guide... and even your worst enemy,\" Patterns, vol. 4, no. 1, p. 100676, 2023.\n\nThe impact of chatgpt on streaming media: A crowdsourced and data-driven analysis using twitter and reddit. Y Feng, P Poralla, S Dash, K Li, V Desai, M Qiu, Y. Feng, P. Poralla, S. Dash, K. Li, V. Desai, and M. Qiu, \"The impact of chatgpt on streaming media: A crowdsourced and data-driven analysis using twitter and reddit,\" 2023.\n\nLatent dirichlet allocation. D M Blei, A Y Ng, M I Jordan, Journal of machine Learning research. 3D. M. Blei, A. Y. Ng, and M. I. Jordan, \"Latent dirichlet allocation,\" Journal of machine Learning research, vol. 3, no. Jan, pp. 993-1022, 2003.\n\nThe pushshift reddit dataset. J Baumgartner, S Zannettou, B Keegan, M Squire, J Blackburn, Proceedings of the international AAAI conference on web and social media. the international AAAI conference on web and social media14J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and J. Blackburn, \"The pushshift reddit dataset,\" in Proceedings of the international AAAI conference on web and social media, vol. 14, 2020, pp. 830-839.\n\nChasing total solar eclipses on twitter: Big social data analytics for once-in-a-lifetime events. Y Feng, Z Lu, Z Zheng, P Sun, W Zhou, R Huang, Q Cao, 2019 IEEE Global Communications Conference (GLOBECOM). IEEEY. Feng, Z. Lu, Z. Zheng, P. Sun, W. Zhou, R. Huang, and Q. Cao, \"Chasing total solar eclipses on twitter: Big social data analytics for once-in-a-lifetime events,\" in 2019 IEEE Global Communications Conference (GLOBECOM). IEEE, 2019, pp. 1-6.\n\nMicromobility in smart cities: A closer look at shared dockless e-scooters via big social data. Y Feng, D Zhong, P Sun, W Zheng, Q Cao, X Luo, Z Lu, ICC 2021-IEEE International Conference on Communications. IEEEY. Feng, D. Zhong, P. Sun, W. Zheng, Q. Cao, X. Luo, and Z. Lu, \"Mi- cromobility in smart cities: A closer look at shared dockless e-scooters via big social data,\" in ICC 2021-IEEE International Conference on Communications. IEEE, 2021, pp. 1-6.\n\nExploring the space of topic coherence measures. M R\u00f6der, A Both, A Hinneburg, Proceedings of the eighth ACM international conference on Web search and data mining. the eighth ACM international conference on Web search and data miningM. R\u00f6der, A. Both, and A. Hinneburg, \"Exploring the space of topic coherence measures,\" in Proceedings of the eighth ACM international conference on Web search and data mining, 2015, pp. 399-408.\n\nJustifying recommendations using distantly-labeled reviews and fine-grained aspects. J Ni, J Li, J Mcauley, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingJ. Ni, J. Li, and J. McAuley, \"Justifying recommendations using distantly-labeled reviews and fine-grained aspects,\" in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 188-197.\n\nFlake8: Your tool for style guide enforcement. T Ziad\u00e9, I Cordasco, besucht am 27. 05. 2019T. Ziad\u00e9 and I. Cordasco, \"Flake8: Your tool for style guide enforcement. 2021,\" URL: http://flake8. pycqa. org (besucht am 27. 05. 2019).\n\nEducational research and ai-generated writing: Confronting the coming tsunami. T P Tate, S Doroudi, D Ritchie, Y Xu, M W Uci, T. P. Tate, S. Doroudi, D. Ritchie, Y. Xu, and m. w. uci, \"Educational research and ai-generated writing: Confronting the coming tsunami,\" Jan 2023. [Online]. Available: edarxiv.org/4mec3\n\nDemystifying covid-19 publications: institutions, journals, concepts, and topics. H Chen, J Chen, H Nguyen, Journal of the Medical Library Association: JMLA. 1093395H. Chen, J. Chen, and H. Nguyen, \"Demystifying covid-19 publications: institutions, journals, concepts, and topics,\" Journal of the Medical Library Association: JMLA, vol. 109, no. 3, p. 395, 2021.\n", "annotations": {"author": "[{\"end\":118,\"start\":87},{\"end\":164,\"start\":119},{\"end\":231,\"start\":165},{\"end\":276,\"start\":232},{\"end\":336,\"start\":277},{\"end\":370,\"start\":337},{\"end\":399,\"start\":371}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":93},{\"end\":135,\"start\":130},{\"end\":184,\"start\":172},{\"end\":245,\"start\":240},{\"end\":288,\"start\":285},{\"end\":348,\"start\":344}]", "author_first_name": "[{\"end\":92,\"start\":87},{\"end\":129,\"start\":119},{\"end\":171,\"start\":165},{\"end\":239,\"start\":232},{\"end\":284,\"start\":277},{\"end\":343,\"start\":337}]", "author_affiliation": "[{\"end\":275,\"start\":247},{\"end\":335,\"start\":311},{\"end\":398,\"start\":372}]", "title": "[{\"end\":84,\"start\":1},{\"end\":483,\"start\":400}]", "venue": null, "abstract": "[{\"end\":2682,\"start\":631}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2973,\"start\":2970},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3002,\"start\":2999},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3452,\"start\":3449},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3480,\"start\":3477},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4082,\"start\":4079},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4087,\"start\":4084},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7456,\"start\":7453},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7461,\"start\":7458},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7540,\"start\":7536},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7705,\"start\":7701},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7968,\"start\":7964},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8246,\"start\":8243},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8251,\"start\":8248},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8285,\"start\":8282},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8381,\"start\":8378},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9144,\"start\":9140},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9159,\"start\":9155},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9188,\"start\":9184},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9244,\"start\":9240},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9363,\"start\":9359},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9831,\"start\":9827},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9860,\"start\":9856},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10012,\"start\":10009},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10017,\"start\":10014},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10023,\"start\":10019},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10029,\"start\":10025},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10060,\"start\":10057},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10297,\"start\":10294},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10727,\"start\":10723},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10968,\"start\":10964},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11100,\"start\":11096},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12253,\"start\":12249},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14227,\"start\":14223},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18316,\"start\":18312},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18947,\"start\":18943},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19746,\"start\":19742},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19752,\"start\":19748},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19858,\"start\":19854},{\"end\":20052,\"start\":20049},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22956,\"start\":22952},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23190,\"start\":23186},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26904,\"start\":26900},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27628,\"start\":27624},{\"end\":31545,\"start\":31538},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32371,\"start\":32367},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32813,\"start\":32809},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33038,\"start\":33034}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34609,\"start\":34548},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34654,\"start\":34610},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34796,\"start\":34655},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34913,\"start\":34797},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35136,\"start\":34914},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35168,\"start\":35137},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35267,\"start\":35169},{\"attributes\":{\"id\":\"fig_7\"},\"end\":35364,\"start\":35268},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36481,\"start\":35365},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37551,\"start\":36482},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37610,\"start\":37552}]", "paragraph": "[{\"end\":4244,\"start\":2701},{\"end\":5012,\"start\":4246},{\"end\":5537,\"start\":5014},{\"end\":6490,\"start\":5539},{\"end\":7358,\"start\":6492},{\"end\":8136,\"start\":7379},{\"end\":8726,\"start\":8138},{\"end\":9223,\"start\":8728},{\"end\":9731,\"start\":9225},{\"end\":10569,\"start\":9733},{\"end\":11051,\"start\":10571},{\"end\":11255,\"start\":11059},{\"end\":11661,\"start\":11257},{\"end\":12361,\"start\":11677},{\"end\":12752,\"start\":12451},{\"end\":13304,\"start\":12754},{\"end\":13897,\"start\":13306},{\"end\":14875,\"start\":13899},{\"end\":15297,\"start\":14877},{\"end\":15470,\"start\":15320},{\"end\":16242,\"start\":15472},{\"end\":17513,\"start\":16244},{\"end\":17966,\"start\":17515},{\"end\":18588,\"start\":17968},{\"end\":18749,\"start\":18634},{\"end\":19926,\"start\":18751},{\"end\":20228,\"start\":19928},{\"end\":21002,\"start\":20230},{\"end\":21438,\"start\":21004},{\"end\":22075,\"start\":21440},{\"end\":22437,\"start\":22077},{\"end\":23074,\"start\":22464},{\"end\":23775,\"start\":23106},{\"end\":24067,\"start\":23807},{\"end\":24698,\"start\":24108},{\"end\":25487,\"start\":24739},{\"end\":26131,\"start\":25555},{\"end\":26796,\"start\":26160},{\"end\":27229,\"start\":26832},{\"end\":27851,\"start\":27231},{\"end\":28289,\"start\":27853},{\"end\":28849,\"start\":28291},{\"end\":29518,\"start\":28903},{\"end\":30329,\"start\":29549},{\"end\":32152,\"start\":30369},{\"end\":34493,\"start\":32226}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25020,\"start\":24996},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30201,\"start\":30194},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33846,\"start\":33838},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34492,\"start\":34337}]", "section_header": "[{\"end\":2699,\"start\":2684},{\"end\":7377,\"start\":7361},{\"end\":11057,\"start\":11054},{\"end\":11675,\"start\":11664},{\"end\":12401,\"start\":12364},{\"end\":12449,\"start\":12404},{\"end\":15318,\"start\":15300},{\"end\":18632,\"start\":18591},{\"end\":22462,\"start\":22440},{\"end\":23104,\"start\":23077},{\"end\":23805,\"start\":23778},{\"end\":24106,\"start\":24070},{\"end\":24737,\"start\":24701},{\"end\":25553,\"start\":25490},{\"end\":26158,\"start\":26134},{\"end\":26830,\"start\":26799},{\"end\":28901,\"start\":28852},{\"end\":29547,\"start\":29521},{\"end\":30367,\"start\":30332},{\"end\":32163,\"start\":32155},{\"end\":32224,\"start\":32166},{\"end\":34547,\"start\":34496},{\"end\":34557,\"start\":34549},{\"end\":34619,\"start\":34611},{\"end\":34664,\"start\":34656},{\"end\":34806,\"start\":34798},{\"end\":34923,\"start\":34915},{\"end\":35146,\"start\":35138},{\"end\":35178,\"start\":35170},{\"end\":35278,\"start\":35269},{\"end\":36495,\"start\":36483},{\"end\":37565,\"start\":37553}]", "table": "[{\"end\":36481,\"start\":35689},{\"end\":37551,\"start\":36522}]", "figure_caption": "[{\"end\":34609,\"start\":34559},{\"end\":34654,\"start\":34621},{\"end\":34796,\"start\":34666},{\"end\":34913,\"start\":34808},{\"end\":35136,\"start\":34925},{\"end\":35168,\"start\":35148},{\"end\":35267,\"start\":35180},{\"end\":35364,\"start\":35281},{\"end\":35689,\"start\":35367},{\"end\":36522,\"start\":36497},{\"end\":37610,\"start\":37568}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3894,\"start\":3886},{\"end\":11865,\"start\":11857},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14546,\"start\":14537},{\"end\":17219,\"start\":17213},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21350,\"start\":21342},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24280,\"start\":24271},{\"end\":24902,\"start\":24894},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25739,\"start\":25731},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25996,\"start\":25988},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26235,\"start\":26227},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27035,\"start\":27027},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29065,\"start\":29057},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29335,\"start\":29327},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31075,\"start\":31066},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31089,\"start\":31080},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33260,\"start\":33251}]", "bib_author_first_name": "[{\"end\":37962,\"start\":37961},{\"end\":37964,\"start\":37963},{\"end\":37976,\"start\":37975},{\"end\":37978,\"start\":37977},{\"end\":38294,\"start\":38293},{\"end\":38303,\"start\":38302},{\"end\":38313,\"start\":38312},{\"end\":38315,\"start\":38314},{\"end\":38324,\"start\":38323},{\"end\":38333,\"start\":38332},{\"end\":38335,\"start\":38334},{\"end\":38790,\"start\":38789},{\"end\":38799,\"start\":38798},{\"end\":38801,\"start\":38800},{\"end\":38810,\"start\":38809},{\"end\":39070,\"start\":39069},{\"end\":39086,\"start\":39085},{\"end\":39095,\"start\":39094},{\"end\":39097,\"start\":39096},{\"end\":39704,\"start\":39703},{\"end\":39716,\"start\":39715},{\"end\":39725,\"start\":39724},{\"end\":39727,\"start\":39726},{\"end\":39734,\"start\":39733},{\"end\":39736,\"start\":39735},{\"end\":40033,\"start\":40032},{\"end\":40050,\"start\":40049},{\"end\":40059,\"start\":40058},{\"end\":40061,\"start\":40060},{\"end\":40368,\"start\":40367},{\"end\":40376,\"start\":40375},{\"end\":40391,\"start\":40390},{\"end\":40398,\"start\":40397},{\"end\":40405,\"start\":40404},{\"end\":40411,\"start\":40410},{\"end\":40420,\"start\":40419},{\"end\":40431,\"start\":40430},{\"end\":40437,\"start\":40436},{\"end\":40443,\"start\":40442},{\"end\":40793,\"start\":40792},{\"end\":40799,\"start\":40798},{\"end\":40807,\"start\":40806},{\"end\":40809,\"start\":40808},{\"end\":40816,\"start\":40815},{\"end\":41608,\"start\":41607},{\"end\":41615,\"start\":41614},{\"end\":41622,\"start\":41621},{\"end\":41629,\"start\":41628},{\"end\":41638,\"start\":41637},{\"end\":41644,\"start\":41643},{\"end\":42113,\"start\":42112},{\"end\":42124,\"start\":42123},{\"end\":42134,\"start\":42133},{\"end\":42837,\"start\":42836},{\"end\":42844,\"start\":42843},{\"end\":42851,\"start\":42850},{\"end\":42860,\"start\":42859},{\"end\":42867,\"start\":42866},{\"end\":42874,\"start\":42873},{\"end\":43370,\"start\":43369},{\"end\":43383,\"start\":43382},{\"end\":43393,\"start\":43392},{\"end\":43407,\"start\":43406},{\"end\":43421,\"start\":43420},{\"end\":43423,\"start\":43422},{\"end\":43432,\"start\":43431},{\"end\":43709,\"start\":43708},{\"end\":43717,\"start\":43716},{\"end\":43727,\"start\":43726},{\"end\":43734,\"start\":43733},{\"end\":43742,\"start\":43741},{\"end\":43748,\"start\":43743},{\"end\":43757,\"start\":43756},{\"end\":43767,\"start\":43766},{\"end\":43778,\"start\":43777},{\"end\":43787,\"start\":43786},{\"end\":43797,\"start\":43796},{\"end\":44120,\"start\":44119},{\"end\":44128,\"start\":44127},{\"end\":44135,\"start\":44134},{\"end\":44143,\"start\":44142},{\"end\":44151,\"start\":44150},{\"end\":44159,\"start\":44158},{\"end\":44167,\"start\":44166},{\"end\":44175,\"start\":44174},{\"end\":44182,\"start\":44181},{\"end\":44189,\"start\":44188},{\"end\":44475,\"start\":44474},{\"end\":44477,\"start\":44476},{\"end\":44484,\"start\":44483},{\"end\":44694,\"start\":44693},{\"end\":44705,\"start\":44704},{\"end\":44716,\"start\":44715},{\"end\":44725,\"start\":44724},{\"end\":44974,\"start\":44973},{\"end\":44990,\"start\":44989},{\"end\":44992,\"start\":44991},{\"end\":45000,\"start\":44999},{\"end\":45006,\"start\":45005},{\"end\":45676,\"start\":45675},{\"end\":45685,\"start\":45684},{\"end\":45692,\"start\":45691},{\"end\":45702,\"start\":45701},{\"end\":45711,\"start\":45710},{\"end\":45722,\"start\":45721},{\"end\":45734,\"start\":45733},{\"end\":45736,\"start\":45735},{\"end\":45743,\"start\":45742},{\"end\":46255,\"start\":46254},{\"end\":46257,\"start\":46256},{\"end\":46263,\"start\":46262},{\"end\":46276,\"start\":46275},{\"end\":46622,\"start\":46621},{\"end\":46844,\"start\":46843},{\"end\":46858,\"start\":46857},{\"end\":47178,\"start\":47177},{\"end\":47186,\"start\":47185},{\"end\":47197,\"start\":47196},{\"end\":47205,\"start\":47204},{\"end\":47211,\"start\":47210},{\"end\":47220,\"start\":47219},{\"end\":47432,\"start\":47431},{\"end\":47434,\"start\":47433},{\"end\":47442,\"start\":47441},{\"end\":47444,\"start\":47443},{\"end\":47450,\"start\":47449},{\"end\":47452,\"start\":47451},{\"end\":47678,\"start\":47677},{\"end\":47693,\"start\":47692},{\"end\":47706,\"start\":47705},{\"end\":47716,\"start\":47715},{\"end\":47726,\"start\":47725},{\"end\":48178,\"start\":48177},{\"end\":48186,\"start\":48185},{\"end\":48192,\"start\":48191},{\"end\":48201,\"start\":48200},{\"end\":48208,\"start\":48207},{\"end\":48216,\"start\":48215},{\"end\":48225,\"start\":48224},{\"end\":48632,\"start\":48631},{\"end\":48640,\"start\":48639},{\"end\":48649,\"start\":48648},{\"end\":48656,\"start\":48655},{\"end\":48665,\"start\":48664},{\"end\":48672,\"start\":48671},{\"end\":48679,\"start\":48678},{\"end\":49043,\"start\":49042},{\"end\":49052,\"start\":49051},{\"end\":49060,\"start\":49059},{\"end\":49510,\"start\":49509},{\"end\":49516,\"start\":49515},{\"end\":49522,\"start\":49521},{\"end\":50206,\"start\":50205},{\"end\":50215,\"start\":50214},{\"end\":50469,\"start\":50468},{\"end\":50471,\"start\":50470},{\"end\":50479,\"start\":50478},{\"end\":50490,\"start\":50489},{\"end\":50501,\"start\":50500},{\"end\":50507,\"start\":50506},{\"end\":50509,\"start\":50508},{\"end\":50787,\"start\":50786},{\"end\":50795,\"start\":50794},{\"end\":50803,\"start\":50802}]", "bib_author_last_name": "[{\"end\":37973,\"start\":37965},{\"end\":37985,\"start\":37979},{\"end\":38300,\"start\":38295},{\"end\":38310,\"start\":38304},{\"end\":38321,\"start\":38316},{\"end\":38330,\"start\":38325},{\"end\":38339,\"start\":38336},{\"end\":38796,\"start\":38791},{\"end\":38807,\"start\":38802},{\"end\":38822,\"start\":38811},{\"end\":39083,\"start\":39071},{\"end\":39092,\"start\":39087},{\"end\":39106,\"start\":39098},{\"end\":39713,\"start\":39705},{\"end\":39722,\"start\":39717},{\"end\":39731,\"start\":39728},{\"end\":39741,\"start\":39737},{\"end\":40047,\"start\":40034},{\"end\":40056,\"start\":40051},{\"end\":40075,\"start\":40062},{\"end\":40373,\"start\":40369},{\"end\":40388,\"start\":40377},{\"end\":40395,\"start\":40392},{\"end\":40402,\"start\":40399},{\"end\":40408,\"start\":40406},{\"end\":40417,\"start\":40412},{\"end\":40428,\"start\":40421},{\"end\":40434,\"start\":40432},{\"end\":40440,\"start\":40438},{\"end\":40449,\"start\":40444},{\"end\":40796,\"start\":40794},{\"end\":40804,\"start\":40800},{\"end\":40813,\"start\":40810},{\"end\":40821,\"start\":40817},{\"end\":41612,\"start\":41609},{\"end\":41619,\"start\":41616},{\"end\":41626,\"start\":41623},{\"end\":41635,\"start\":41630},{\"end\":41641,\"start\":41639},{\"end\":41650,\"start\":41645},{\"end\":42121,\"start\":42114},{\"end\":42131,\"start\":42125},{\"end\":42140,\"start\":42135},{\"end\":42841,\"start\":42838},{\"end\":42848,\"start\":42845},{\"end\":42857,\"start\":42852},{\"end\":42864,\"start\":42861},{\"end\":42871,\"start\":42868},{\"end\":42880,\"start\":42875},{\"end\":43380,\"start\":43371},{\"end\":43390,\"start\":43384},{\"end\":43404,\"start\":43394},{\"end\":43418,\"start\":43408},{\"end\":43429,\"start\":43424},{\"end\":43439,\"start\":43433},{\"end\":43714,\"start\":43710},{\"end\":43724,\"start\":43718},{\"end\":43731,\"start\":43728},{\"end\":43739,\"start\":43735},{\"end\":43754,\"start\":43749},{\"end\":43764,\"start\":43758},{\"end\":43775,\"start\":43768},{\"end\":43784,\"start\":43779},{\"end\":43794,\"start\":43788},{\"end\":43806,\"start\":43798},{\"end\":44125,\"start\":44121},{\"end\":44132,\"start\":44129},{\"end\":44140,\"start\":44136},{\"end\":44148,\"start\":44144},{\"end\":44156,\"start\":44152},{\"end\":44164,\"start\":44160},{\"end\":44172,\"start\":44168},{\"end\":44179,\"start\":44176},{\"end\":44186,\"start\":44183},{\"end\":44195,\"start\":44190},{\"end\":44481,\"start\":44478},{\"end\":44490,\"start\":44485},{\"end\":44702,\"start\":44695},{\"end\":44713,\"start\":44706},{\"end\":44722,\"start\":44717},{\"end\":44731,\"start\":44726},{\"end\":44987,\"start\":44975},{\"end\":44997,\"start\":44993},{\"end\":45003,\"start\":45001},{\"end\":45017,\"start\":45007},{\"end\":45682,\"start\":45677},{\"end\":45689,\"start\":45686},{\"end\":45699,\"start\":45693},{\"end\":45708,\"start\":45703},{\"end\":45719,\"start\":45712},{\"end\":45731,\"start\":45723},{\"end\":45740,\"start\":45737},{\"end\":45749,\"start\":45744},{\"end\":46260,\"start\":46258},{\"end\":46273,\"start\":46264},{\"end\":46283,\"start\":46277},{\"end\":46635,\"start\":46623},{\"end\":46855,\"start\":46845},{\"end\":46867,\"start\":46859},{\"end\":47183,\"start\":47179},{\"end\":47194,\"start\":47187},{\"end\":47202,\"start\":47198},{\"end\":47208,\"start\":47206},{\"end\":47217,\"start\":47212},{\"end\":47224,\"start\":47221},{\"end\":47439,\"start\":47435},{\"end\":47447,\"start\":47445},{\"end\":47459,\"start\":47453},{\"end\":47690,\"start\":47679},{\"end\":47703,\"start\":47694},{\"end\":47713,\"start\":47707},{\"end\":47723,\"start\":47717},{\"end\":47736,\"start\":47727},{\"end\":48183,\"start\":48179},{\"end\":48189,\"start\":48187},{\"end\":48198,\"start\":48193},{\"end\":48205,\"start\":48202},{\"end\":48213,\"start\":48209},{\"end\":48222,\"start\":48217},{\"end\":48229,\"start\":48226},{\"end\":48637,\"start\":48633},{\"end\":48646,\"start\":48641},{\"end\":48653,\"start\":48650},{\"end\":48662,\"start\":48657},{\"end\":48669,\"start\":48666},{\"end\":48676,\"start\":48673},{\"end\":48682,\"start\":48680},{\"end\":49049,\"start\":49044},{\"end\":49057,\"start\":49053},{\"end\":49070,\"start\":49061},{\"end\":49513,\"start\":49511},{\"end\":49519,\"start\":49517},{\"end\":49530,\"start\":49523},{\"end\":50212,\"start\":50207},{\"end\":50224,\"start\":50216},{\"end\":50476,\"start\":50472},{\"end\":50487,\"start\":50480},{\"end\":50498,\"start\":50491},{\"end\":50504,\"start\":50502},{\"end\":50513,\"start\":50510},{\"end\":50792,\"start\":50788},{\"end\":50800,\"start\":50796},{\"end\":50810,\"start\":50804}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":225097136},\"end\":38213,\"start\":37899},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218482503},\"end\":38715,\"start\":38215},{\"attributes\":{\"id\":\"b2\"},\"end\":38956,\"start\":38717},{\"attributes\":{\"doi\":\"10.1145/3491101.3519665\",\"id\":\"b3\",\"matched_paper_id\":247255943},\"end\":39672,\"start\":38958},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":256437954},\"end\":39964,\"start\":39674},{\"attributes\":{\"id\":\"b5\"},\"end\":40257,\"start\":39966},{\"attributes\":{\"doi\":\"arXiv:2302.04023\",\"id\":\"b6\"},\"end\":40730,\"start\":40259},{\"attributes\":{\"doi\":\"10.24963/ijcai.2018/578\",\"id\":\"b7\",\"matched_paper_id\":11683607},\"end\":41545,\"start\":40732},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":53670269},\"end\":42060,\"start\":41547},{\"attributes\":{\"doi\":\"10.1145/2594291.2594321\",\"id\":\"b9\",\"matched_paper_id\":13040187},\"end\":42766,\"start\":42062},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":208248351},\"end\":43299,\"start\":42768},{\"attributes\":{\"doi\":\"abs/2103.07115\",\"id\":\"b11\",\"matched_paper_id\":232222637},\"end\":43656,\"start\":43301},{\"attributes\":{\"doi\":\"arXiv:2107.03374\",\"id\":\"b12\"},\"end\":44048,\"start\":43658},{\"attributes\":{\"doi\":\"arXiv:2002.08155\",\"id\":\"b13\"},\"end\":44431,\"start\":44050},{\"attributes\":{\"doi\":\"arXiv:2301.13246\",\"id\":\"b14\"},\"end\":44627,\"start\":44433},{\"attributes\":{\"doi\":\"arXiv:2301.08653\",\"id\":\"b15\"},\"end\":44915,\"start\":44629},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":218673683},\"end\":45570,\"start\":44917},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":248419806},\"end\":46182,\"start\":45572},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":231718679},\"end\":46562,\"start\":46184},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":254492857},\"end\":46735,\"start\":46564},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":255932644},\"end\":47067,\"start\":46737},{\"attributes\":{\"id\":\"b21\"},\"end\":47400,\"start\":47069},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3177797},\"end\":47645,\"start\":47402},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":210868223},\"end\":48077,\"start\":47647},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":211688141},\"end\":48533,\"start\":48079},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":225102868},\"end\":48991,\"start\":48535},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7743332},\"end\":49422,\"start\":48993},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":202621357},\"end\":50156,\"start\":49424},{\"attributes\":{\"id\":\"b28\"},\"end\":50387,\"start\":50158},{\"attributes\":{\"id\":\"b29\"},\"end\":50702,\"start\":50389},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":238527609},\"end\":51066,\"start\":50704}]", "bib_title": "[{\"end\":37959,\"start\":37899},{\"end\":38291,\"start\":38215},{\"end\":39067,\"start\":38958},{\"end\":39701,\"start\":39674},{\"end\":40030,\"start\":39966},{\"end\":40790,\"start\":40732},{\"end\":41605,\"start\":41547},{\"end\":42110,\"start\":42062},{\"end\":42834,\"start\":42768},{\"end\":43367,\"start\":43301},{\"end\":44971,\"start\":44917},{\"end\":45673,\"start\":45572},{\"end\":46252,\"start\":46184},{\"end\":46619,\"start\":46564},{\"end\":46841,\"start\":46737},{\"end\":47429,\"start\":47402},{\"end\":47675,\"start\":47647},{\"end\":48175,\"start\":48079},{\"end\":48629,\"start\":48535},{\"end\":49040,\"start\":48993},{\"end\":49507,\"start\":49424},{\"end\":50784,\"start\":50704}]", "bib_author": "[{\"end\":37975,\"start\":37961},{\"end\":37987,\"start\":37975},{\"end\":38302,\"start\":38293},{\"end\":38312,\"start\":38302},{\"end\":38323,\"start\":38312},{\"end\":38332,\"start\":38323},{\"end\":38341,\"start\":38332},{\"end\":38798,\"start\":38789},{\"end\":38809,\"start\":38798},{\"end\":38824,\"start\":38809},{\"end\":39085,\"start\":39069},{\"end\":39094,\"start\":39085},{\"end\":39108,\"start\":39094},{\"end\":39715,\"start\":39703},{\"end\":39724,\"start\":39715},{\"end\":39733,\"start\":39724},{\"end\":39743,\"start\":39733},{\"end\":40049,\"start\":40032},{\"end\":40058,\"start\":40049},{\"end\":40077,\"start\":40058},{\"end\":40375,\"start\":40367},{\"end\":40390,\"start\":40375},{\"end\":40397,\"start\":40390},{\"end\":40404,\"start\":40397},{\"end\":40410,\"start\":40404},{\"end\":40419,\"start\":40410},{\"end\":40430,\"start\":40419},{\"end\":40436,\"start\":40430},{\"end\":40442,\"start\":40436},{\"end\":40451,\"start\":40442},{\"end\":40798,\"start\":40792},{\"end\":40806,\"start\":40798},{\"end\":40815,\"start\":40806},{\"end\":40823,\"start\":40815},{\"end\":41614,\"start\":41607},{\"end\":41621,\"start\":41614},{\"end\":41628,\"start\":41621},{\"end\":41637,\"start\":41628},{\"end\":41643,\"start\":41637},{\"end\":41652,\"start\":41643},{\"end\":42123,\"start\":42112},{\"end\":42133,\"start\":42123},{\"end\":42142,\"start\":42133},{\"end\":42843,\"start\":42836},{\"end\":42850,\"start\":42843},{\"end\":42859,\"start\":42850},{\"end\":42866,\"start\":42859},{\"end\":42873,\"start\":42866},{\"end\":42882,\"start\":42873},{\"end\":43382,\"start\":43369},{\"end\":43392,\"start\":43382},{\"end\":43406,\"start\":43392},{\"end\":43420,\"start\":43406},{\"end\":43431,\"start\":43420},{\"end\":43441,\"start\":43431},{\"end\":43716,\"start\":43708},{\"end\":43726,\"start\":43716},{\"end\":43733,\"start\":43726},{\"end\":43741,\"start\":43733},{\"end\":43756,\"start\":43741},{\"end\":43766,\"start\":43756},{\"end\":43777,\"start\":43766},{\"end\":43786,\"start\":43777},{\"end\":43796,\"start\":43786},{\"end\":43808,\"start\":43796},{\"end\":44127,\"start\":44119},{\"end\":44134,\"start\":44127},{\"end\":44142,\"start\":44134},{\"end\":44150,\"start\":44142},{\"end\":44158,\"start\":44150},{\"end\":44166,\"start\":44158},{\"end\":44174,\"start\":44166},{\"end\":44181,\"start\":44174},{\"end\":44188,\"start\":44181},{\"end\":44197,\"start\":44188},{\"end\":44483,\"start\":44474},{\"end\":44492,\"start\":44483},{\"end\":44704,\"start\":44693},{\"end\":44715,\"start\":44704},{\"end\":44724,\"start\":44715},{\"end\":44733,\"start\":44724},{\"end\":44989,\"start\":44973},{\"end\":44999,\"start\":44989},{\"end\":45005,\"start\":44999},{\"end\":45019,\"start\":45005},{\"end\":45684,\"start\":45675},{\"end\":45691,\"start\":45684},{\"end\":45701,\"start\":45691},{\"end\":45710,\"start\":45701},{\"end\":45721,\"start\":45710},{\"end\":45733,\"start\":45721},{\"end\":45742,\"start\":45733},{\"end\":45751,\"start\":45742},{\"end\":46262,\"start\":46254},{\"end\":46275,\"start\":46262},{\"end\":46285,\"start\":46275},{\"end\":46637,\"start\":46621},{\"end\":46857,\"start\":46843},{\"end\":46869,\"start\":46857},{\"end\":47185,\"start\":47177},{\"end\":47196,\"start\":47185},{\"end\":47204,\"start\":47196},{\"end\":47210,\"start\":47204},{\"end\":47219,\"start\":47210},{\"end\":47226,\"start\":47219},{\"end\":47441,\"start\":47431},{\"end\":47449,\"start\":47441},{\"end\":47461,\"start\":47449},{\"end\":47692,\"start\":47677},{\"end\":47705,\"start\":47692},{\"end\":47715,\"start\":47705},{\"end\":47725,\"start\":47715},{\"end\":47738,\"start\":47725},{\"end\":48185,\"start\":48177},{\"end\":48191,\"start\":48185},{\"end\":48200,\"start\":48191},{\"end\":48207,\"start\":48200},{\"end\":48215,\"start\":48207},{\"end\":48224,\"start\":48215},{\"end\":48231,\"start\":48224},{\"end\":48639,\"start\":48631},{\"end\":48648,\"start\":48639},{\"end\":48655,\"start\":48648},{\"end\":48664,\"start\":48655},{\"end\":48671,\"start\":48664},{\"end\":48678,\"start\":48671},{\"end\":48684,\"start\":48678},{\"end\":49051,\"start\":49042},{\"end\":49059,\"start\":49051},{\"end\":49072,\"start\":49059},{\"end\":49515,\"start\":49509},{\"end\":49521,\"start\":49515},{\"end\":49532,\"start\":49521},{\"end\":50214,\"start\":50205},{\"end\":50226,\"start\":50214},{\"end\":50478,\"start\":50468},{\"end\":50489,\"start\":50478},{\"end\":50500,\"start\":50489},{\"end\":50506,\"start\":50500},{\"end\":50515,\"start\":50506},{\"end\":50794,\"start\":50786},{\"end\":50802,\"start\":50794},{\"end\":50812,\"start\":50802}]", "bib_venue": "[{\"end\":38030,\"start\":37987},{\"end\":38417,\"start\":38341},{\"end\":38787,\"start\":38717},{\"end\":39231,\"start\":39131},{\"end\":39793,\"start\":39743},{\"end\":40102,\"start\":40077},{\"end\":40365,\"start\":40259},{\"end\":41020,\"start\":40846},{\"end\":41713,\"start\":41652},{\"end\":42276,\"start\":42165},{\"end\":42943,\"start\":42882},{\"end\":43459,\"start\":43455},{\"end\":43706,\"start\":43658},{\"end\":44117,\"start\":44050},{\"end\":44472,\"start\":44433},{\"end\":44691,\"start\":44629},{\"end\":45161,\"start\":45019},{\"end\":45827,\"start\":45751},{\"end\":46349,\"start\":46285},{\"end\":46643,\"start\":46637},{\"end\":46877,\"start\":46869},{\"end\":47175,\"start\":47069},{\"end\":47497,\"start\":47461},{\"end\":47810,\"start\":47738},{\"end\":48284,\"start\":48231},{\"end\":48740,\"start\":48684},{\"end\":49156,\"start\":49072},{\"end\":49692,\"start\":49532},{\"end\":50203,\"start\":50158},{\"end\":50466,\"start\":50389},{\"end\":50860,\"start\":50812},{\"end\":38480,\"start\":38419},{\"end\":39250,\"start\":39233},{\"end\":41181,\"start\":41022},{\"end\":41761,\"start\":41715},{\"end\":42391,\"start\":42278},{\"end\":42991,\"start\":42945},{\"end\":45290,\"start\":45163},{\"end\":45890,\"start\":45829},{\"end\":47869,\"start\":47812},{\"end\":49227,\"start\":49158},{\"end\":49839,\"start\":49694}]"}}}, "year": 2023, "month": 12, "day": 17}