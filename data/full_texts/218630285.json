{"id": 218630285, "updated": "2023-10-06 15:28:46.608", "metadata": {"title": "TAM: Temporal Adaptive Module for Video Recognition", "authors": "[{\"first\":\"Zhaoyang\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Limin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Wayne\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Chen\",\"last\":\"Qian\",\"middle\":[]},{\"first\":\"Tong\",\"last\":\"Lu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 5, "day": 14}, "abstract": "Video data is with complex temporal dynamics due to various factors such as camera motion, speed variation, and different activities. To effectively capture this diverse motion pattern, this paper presents a new temporal adaptive module ({\\bf TAM}) to generate video-specific temporal kernels based on its own feature map. TAM proposes a unique two-level adaptive modeling scheme by decoupling the dynamic kernel into a location sensitive importance map and a location invariant aggregation weight. The importance map is learned in a local temporal window to capture short-term information, while the aggregation weight is generated from a global view with a focus on long-term structure. TAM is a modular block and could be integrated into 2D CNNs to yield a powerful video architecture (TANet) with a very small extra computational cost. The extensive experiments on Kinetics-400 and Something-Something datasets demonstrate that our TAM outperforms other temporal modeling methods consistently, and achieves the state-of-the-art performance under the similar complexity. The code is available at \\url{ https://github.com/liu-zhy/temporal-adaptive-module}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.06803", "mag": "3025409017", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/Liu0W0L21", "doi": "10.1109/iccv48922.2021.01345"}}, "content": {"source": {"pdf_hash": "7f31f46193b0b02e19ecdb8e20c9221659ea79b2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2005.06803v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d8fdbae21fed78b6dda068a23f0748d5b34f31c5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7f31f46193b0b02e19ecdb8e20c9221659ea79b2.txt", "contents": "\nTAM: Temporal Adaptive Module for Video Recognition\n\n\nZhaoyang Liu \nState Key Lab for Novel Software Technology\nNanjing University\nChina\n\nSenseTime Research\n\n\nLimin Wang lmwang@nju.edu.cnwuwenyan \nState Key Lab for Novel Software Technology\nNanjing University\nChina\n\nWayne Wu \nSenseTime Research\n\n\nChen Qian qianchen@sensetime.com \nSenseTime Research\n\n\nTong Lu lutong@nju.edu.cn \nState Key Lab for Novel Software Technology\nNanjing University\nChina\n\nTAM: Temporal Adaptive Module for Video Recognition\n\nVideo data is with complex temporal dynamics due to various factors such as camera motion, speed variation, and different activities. To effectively capture this diverse motion pattern, this paper presents a new temporal adaptive module (TAM) to generate video-specific temporal kernels based on its own feature map. TAM proposes a unique two-level adaptive modeling scheme by decoupling the dynamic kernel into a location sensitive importance map and a location invariant aggregation weight. The importance map is learned in a local temporal window to capture shortterm information, while the aggregation weight is generated from a global view with a focus on long-term structure. TAM is a modular block and could be integrated into 2D CNNs to yield a powerful video architecture (TANet) with a very small extra computational cost. The extensive experiments on Kinetics-400 and Something-Something datasets demonstrate that our TAM outperforms other temporal modeling methods consistently, and achieves the state-of-the-art performance under the similar complexity. The code is available at https://github.com/ liu-zhy/temporal-adaptive-module.\n\nIntroduction\n\nDeep learning has brought great progress for various recognition tasks in image domain, such as image classification [21,12], object detection [28], and instance segmentation [11]. The key to these successes is to devise flexible and efficient architectures that are capable of learning powerful visual representations from large-scale image datasets [4]. However, deep learning research progress in video understanding is relatively slower, partially due to the high complexity of video data. The core technical problem in video understanding is to design an effective temporal module, that is expected to be able to capture complex temporal structure with high flexibility, while yet to be of : Corresponding author. low computational consumption for processing high dimensional video data efficiently.\n\n3D Convolutional Neural Networks (3D CNNs) [15,34] have turned out to be mainstream architectures for video modeling [1,8,36,27]. The 3D convolution is a direct extension over its 2D counterparts and provides a learnable operator for video recognition. However, this simple extension lacks specific consideration about the temporal properties in video data and might as well lead to high computational cost. Therefore, recent methods aim to model video sequences in two different aspects by combining a lightweight temporal module with 2D CNNs to improve efficiency (e.g., TSN [40], TSM [23]), or designing a dedicated temporal module to better capture temporal relation (e.g., Nonlocal Net [41], ARTNet [38], STM [17], TDN [39]). However, how to devise a temporal module with both high efficiency and strong flexibility still remains to be an unsolved problem. Consequently, we aim at advancing the current video architectures along this direction.\n\nIn this paper, we focus on devising an adaptive module to capture temporal information in a more flexible way. Intuitvely, we observe that video data is with extremely complex dynamics along the temporal dimension due to factors such as camera motion and various speeds. Thus 3D convolutions (temporal convolutions) might lack enough representation power to describe motion diversity by simply employing a fixed number of video invariant kernels. To deal with such complex temporal variations in videos, we argue that adaptive temporal kernels for each video are effective and as well necessary to describe motion patterns. To this end, as shown in Figure 1, we present a two-level adaptive modeling scheme to decompose the video specific temporal kernel into a location sensitive importance map and a location invariant (also video adaptive) aggregation kernel. This unique design allows the location sensitive importance map to focus on enhancing discriminative temporal information from a local view, and enables the video adaptive aggregation to capture temporal dependencies with a global view of the input video sequence.\n\nSpecifically, the design of temporal adaptive module  Figure 1. Temporal module comparisons: The standard temporal convolution shares weights among videos and may lack the flexibility to handle video variations due to the diversity of videos. The temporal attention learns position sensitive weights by assigning varied importance for different time without any temporal interaction, and may ignore the long-range temporal dependencies. Our proposed temporal adaptive module (TAM) presents a two-level adaptive scheme by learning the local importance weights for location adaptive enhancement and the global kernel weights for video adaptive aggregation. \u2299 is attention operation, and \u2297 is convolution operation.\n\n(TAM) strictly follows two principles: high efficiency and strong flexibility. To ensure our TAM with a low computational cost, we first squeeze the feature map by employing a global spatial pooling, and then establish our TAM in a channel-wise manner to keep the efficiency. Our TAM is composed of two branches: a local branch (L) and a global branch (G). As shown in Fig. 2, TAM is implemented in an efficient way. The local branch employs temporal convolutions to produce the location sensitive importance maps to enhance the local features, while the global branch uses fully connected layers to produce the location invariant kernel for temporal aggregation. The importance map generated by a local temporal window focuses on short-term motion modeling and the aggregation kernel using a global view pays more attention to the long-term temporal information. Furthermore, our TAM could be flexibly plugged into the existing 2D CNNs to yield an efficient video recognition architecture, termed as TANet.\n\nWe verify the proposed TANet on the task of action classification in videos. In particular, we first study the performance of the TANet on the Kinetics-400 dataset, and demonstrate that our TAM is better at capturing temporal information than other several counterparts, such as temporal pooling, temporal convolution, TSM [23], TEINet [24], and Non-local block [41]. Our TANet is able to yield a very competitive accuracy with the FLOPs similar to 2D CNNs. We further test our TANet on the motion dominated dataset of Something-Something, where the state-of-the-art performance is achieved.\n\n\nRelated Work\n\nVideo understanding is a core topic in the field of computer vision. At the early stage, a lot of traditional meth-ods [22,20,29,43] have designed various hand-crafted features to encode the video data, but these methods are too inflexible when generalized to other video tasks. Recently, since the rapid development of video understanding has been much benefited from deep learning methods [21,32,12], especially in video recognition, a series of CNNs-based methods were proposed to learn spatiotemporal representation, and the differences with our method will be clarified later. Furthermore, our work also relates to dynamic convolution and attention in CNNs.\n\nCNNs-based methods for action recognition. Since the deep learning method has been wildly used in the image tasks, there are many attempts [18,31,40,46,10,23,39] based on 2D CNNs devoted to modeling the video clips. In particular, [40] used the frames sparsely sampled from the whole video to learn the long-range information by aggregating scores after the last fully-connected layer. [23] shifted the channels along the temporal dimension in an efficient way, which yields a good performance with 2D CNNs. By a simple extension from spatial domain to spatiotemporal domain, 3D convolution [15,34] was proposed to capture the motion information encoded in video clips. Due to the release of large-scale Kinetics dataset [19], 3D CNNs [1] were wildly used in action recognition. Its variants [27,36,44] decomposed the 3D convolution into a spatial 2D convolution and a temporal 1D convolution to learn the spatiotemporal features. And [8] designed a network with dual paths to learn the spatiotemporal features and achieved a promising accuracy in video understanding.\n\nThe methods aforementioned all share a common insight that they are video invariant and ignore the inherent temporal diversities in videos. As opposed to these methods, we design a two-level adaptive modeling scheme by decompos-  Figure 2. The overall architecture of TANet: ResNet-Block vs. TA-Block. The whole workflow of temporal adaptive module (TAM) in the lower right shows how it works. The shape of tensor has noted after each step. \u2295 denotes element-wise addition, \u2299 is element-wise multiplication, and \u2297 is convolution operation. The symbols appeared in figure will be explained in Sec. 3.1.\n\ning the video specific operation into a location sensitive excitation and a location invariant convolution with adaptive kernel for each video clip. Attention in action recognition. The local branch in TAM mostly relates to SENet [13]. But the SENet learned modulation weights for each channel of feature maps. Several methods [24,5] also resorted to the attention to learn more discriminative features in videos. Different from these methods, the local branch keeps the temporal information to learn the location sensitive importances. [41] designed a non-local block which can be seen as self-attention to capture long-range dependencies. Our TANet captures the long-range dependencies by simply stacking more TAM, and keep the efficiency of networks. Dynamic convolutions. [16] first proposed the dynamic filters on the tasks of video and stereo prediction, and designed a convolutional encoder-decoder as filter-generating network. Several works [45,3] in image tasks attempted to generate aggregation weights for a set of convolutional kernels, and then produce a dynamic kernel. Our motivation is different from these methods. We aim to use this temporal adaptive module to deal with temporal variations in videos. Specifically, we design an efficient form to implement this temporal dynamic kernel based on input feature maps, which is critical for understanding the video content.\n\n\nMethod\n\n\nThe Overview of Temporal Adaptive Module\n\nAs we discussed in Sec.1, video data typically exhibit the complex temporal dynamics caused by many factors such as camera motion and speed variations. Therefore, we aim to tackle this issue by introducing a temporal adaptive module (TAM) with video specific kernels, unlike the sharing convolutional kernel in 3D CNNs. Our TAM could be easily integrated into the existing 2D CNNs (e.g., ResNet) to yield a video network architecture, as shown in Figure 2. We will give an overview of TAM and then describe its technical details.\n\nFormally, let X \u2208 R C\u00d7T \u00d7H\u00d7W denote the feature maps for a video clip, where C represents the number of channels, and T, H, W are its spatiotemporal dimensions. For efficiency, TAM only focuses on temporal modeling and the spatial pattern is expected to captured by 2D convolutions. Therefore, we first employ a global spatial average pooling to squeeze the feature map as follows:\nX c,t = \u03d5(X) c,t = 1 H \u00d7 W i,j X c,t,j,i ,(1)\nwhere c, t, j, i is the index of different dimensions (in channel, time, height and width), andX \u2208 R C\u00d7T aggregates the spatial information of X. For simplicity, we here use \u03d5 to denote the function that aggregates the spatial information. The proposed temporal adaptive module (TAM) is established based on this squeezed 1D temporal signal with high efficiency. Our TAM is composed of two branches: a local branch L and a global branch G, which aims to learn a location sensitive importance map to enhance discriminative features and then produces the location invariant weights to adaptively aggregate temporal information in a convolutional manner. More specifically, the TAM is formulated as follows:\nY = G(X) \u2297 (L(X) \u2299 X),(2)\nwhere \u2297 denotes convolution operation and \u2299 is elementwise multiplication. It is worth noting that these two branches focus on different aspects of temporal information, where the local branch tries to capture the short term information to attend important features by using a temporal convolution, while the global branch aims to incorporate long-range temporal structure to guide adaptive temporal aggregation with fully connected layers. Disentangling kernel learning procedures into local and global branches turns out to be an effective way in experiments. These two branches will be introduced in the following sections.\n\n\nLocal Branch in TAM\n\nAs discussed above, the local branch is location sensitive and aims to leverage short-term temporal dynamics to perform video specific operation. Given that the short-term information varies slowly along the temporal dimension, it is thus required to learn a location sensitive importance map to discriminate the local temporal semantics.\n\nAs shown in Figure 2, the local branch is built by a sequence of temporal convolutional layers with ReLU nonlinearity. Since the goal of local branch is to capture short term information, we set the kernel size K as 3 to learn importance map solely based on a local temporal window. To control the model complexity, the first Conv1D followed by BN [14] reduces the number of channels from C to C \u03b2 . Then, the second Conv1D with a sigmoid activation yields the importance weights V \u2208 R C\u00d7T which are sensitive to temporal location. Finally, the temporal excitation is formulated as follows:\nZ = F rescale (V ) \u2299 X = L(X) \u2299 X,(3)\nwhere \u2299 denotes the element-wise multiplication and Z \u2208 R C\u00d7T \u00d7H\u00d7W . To match size of X, F rescale (V ) rescales the V toV \u2208 R C\u00d7T \u00d7H\u00d7W by replicating in spatial dimension.\n\n\nGlobal Branch in TAM\n\nThe global branch is location invariant and focuses on generating an adaptive kernel based on long-term temporal information. It incorporates global context information and learns to produce the location invariant and also video adaptive convolution kernel for dynamic aggregation. Learning the Adaptive Kernels. We here opt to generate the dynamic kernel for each video clip and aggregate temporal information in a convolutional manner. To simply this procedure and as well as preserve high efficiency, The adaptive convolution will be applied in a channel-wise manner. In this sense, the learned adaptive kernel is expected to only model the temporal relations without taking channel correlation into account. Thus, our TAM would not change the number of channels of input feature maps, and the learned adaptive kernel convolves the input feature maps in a channel-wise manner. More formally, for the c th channel, the adaptive kernel is learned as follows:\n\u0398 c = G(X) c = softmax(F(W 2 , \u03b4(F(W 1 , \u03d5(X) c )))),(4)\nwhere \u0398 c \u2208 R K is generated adaptive kernel (aggregation weights) for c th channel, K is the adaptive kernel size, \u03b4 denotes the activation function ReLU. The adaptive kernel is also learned based on the squeezed feature mapX c \u2208 R T without taking the spatial structure into account for modeling efficiency. But different with the local branch, we use fully connected (f c) layers F to learn the adaptive kernel by leveraging long-term information. The learned adaptive kernel with the global receptive field, thus could aggregate temporal features guided by the global context. To increase the modeling capabilities of the global branch, we stack two f c layers and the learned kernel is normalized with a softmax function to yield a positive aggregation weight. The learned aggregation weights \u0398 = {\u0398 1 , \u0398 2 , ..., \u0398 C } will be employed to perform video adaptive convolution.\n\nTemporal Adaptive Aggregation. Before introducing the adaptive aggregation, we can look back on how a vanilla temporal convolution aggregates the spatio-temporal visual information:\nY = W \u2297 X,(5)\nWhere W is the weights of convolution kernel and has no concern with input video samples in inference. We argue this fashion ignores the temporal dynamics in videos, and thus propose a video adaptive aggregation:\nY = G(X) \u2297 X,(6)\nwhere G can be seen as a kernel generator function. The kernel generated by G can perform adaptive convolution but is shared cross temporal dimension and still location invariant. To address this issue, the local branch produces Z with location sensitive importance map. The whole procedures can be expressed as follows:\nY c,t,j,i = G(X) \u2297 Z = \u0398 \u2297 Z = k \u0398 c,k \u00b7 Z c,t+k,j,i ,(7)\nwhere \u00b7 denotes the scalar multiplication and Y is the output feature maps (Y \u2208 R C\u00d7T \u00d7H\u00d7W ).\n\nIn summary, TAM presents an adaptive module with a unique aggregation scheme, where the location sensitive excitation and location invariant aggregation all derive from input features, but focus on capturing different structures (i.e., short-term and long-term temporal structure).\n\n\nExemplar: TANet\n\nWe here intend to describe how to instantiate the TANet. Temporal adaptive module can endow the existing 2D CNNs with a strong ability to model different temporal structures in video clips. In practice, TAM only causes limited computing overhead, but obviously improves the performance on different types of datasets.\n\nResNets [12] are employed as backbones to verify the effectiveness of TAM. As illustrated in Fig. 2, the TAM is embedded into ResNet-Block after the first Conv2D, which easily turns the vanilla ResNet-Block into TA-Block. This fashion will not excessively alter the topology of networks and can reuse the weights of ResNet-Block. Supposing we sample T frames as an input clip, the scores of T frames after f c will be aggregated by average pooling to yield the clip-level scores. No temporal downsampling is performed before f c layer. The extensive experiments are conducted in Sec. 4 to demonstrate the flexibility and efficacy of TANet.\n\nDiscussions. We notice that the structure of local branch is similar to the SENet [13] and STC [5]. The first obvious difference is the local branch does not squeeze the temporal dimension. We thus use temporal 1D convolution, instead of f c layer, as a basic layer. Two-layer design only seeks to make a trade-off between non-linear fitting capability and model complexity. The local branch provides the location sensitive information, and thereby addresses the issue that the global branch is insensitive to temporal location.\n\nTSN [40] and TSM [23] only aggregate the temporal features with a fixed scheme, but TAM can yield the video specific weights to adaptively aggregate the temporal features in different stages. In extreme cases, our global branch in TAM can degenerate into TSN when dynamic kernel weights \u0398 is learned to equal to [0, 1, 0]. From another perspective, if the kernel weights \u0398 is set to [1,0,0] or [0, 0, 1], global branch can be turned into TSM. It seems that our TAM theoretically provides a more general and flexible form to model the video data.\n\nWhen it refers to 3D convolution [15], all input samples share the same convolution kernel without being aware of the temporal diversities in videos as well. In addition, our global branch essentially performs a video adaptive convolution whose filter has size 1 \u00d7 k \u00d7 1 \u00d7 1, while each filter in a normal 3D convolution has size C \u00d7 k \u00d7 k \u00d7 k, where C is the number of channels and k denotes the receptive field. Thus our method is more efficient than 3D CNNs. Unlike some current dynamic convolution [3,45], TAM is more flexible, and can directly generate the kernel weights to perform video adaptive convolution.\n\n\nExperiments\n\n\nDatasets\n\nOur experiments are conducted on three large scale datasets, namely, Kinetics-400 [19] and Something-Something (Sth-Sth) V1&V2 [9]. Kinetics-400 contains \u223c300k video clips with 400 human action categories. The trimmed videos in Kinetics-400 are around 10s. We train the models on the training set (\u223c240k video clips), and test models on the validation set (\u223c20k video clips). The Sth-Sth datasets focus on fine-grained and motion-dominated action, which contains pre-defined basic actions involving different interacting objects. The Sth-Sth V1 comprises \u223c86k video clips in the training set and \u223c12k video clips in the validation set. Sth-Sth V2 is an updated version of Sth-Sth V1, which contains \u223c169k video clips in the training set and \u223c25k video clips in the validation set. They both have 174 action categories.\n\n\nImplementation Details\n\nTraining. In our experiments, we train the models with 8 and 16 frames as inputs. On Kinetics-400, following the practice in [41], the frames are sampled from 64 consecutive frames in the video. On Sth-Sth V1&V2, the uniform sampling strategy in TSN [40] is employed to train TANet. We first resize the shorter side of frames to 256, and apply the multi-scale cropping and randomly horizontal flipping as data augmentation. The cropped frames are resized to 224 \u00d7 224 for network training. The batch size is 64. Our models are initialized by ImageNet pre-trained weights to reduce the training time. Specifically, on the Kinetics-400, the epoch for training is 100. The initial learning rate is set 0.01 and divided by 10 at 50, 75, 90 epochs. We use SGD with a momentum of 0.9 and a weight decay of 1e-4 to train TANet. On Sth-Sth V1&V2, we train models with 50 epochs. The learning rate starts at 0.01 and is divided by 10 at 30, 40, 45 epoch. We use a momentum of 0.9 and a weight decay of 1e-3 to reduce the risk of overfitting.\n\nTesting. Different inference schemes are applied to fairly compare with other state-of-the-art models. On kinetics-400, we resize the shorter to 256 and take 3 crops of 256 \u00d7 256 to cover the spatial dimensions. In the temporal dimension, we uniformly sample 10 clips for 8-frame models and 4 clips for 16-frame models. The final video-level prediction is yielded by averaging the scores of all spatiotemporal views. On Sth-Sth V1, we scale the shorter side of frames to 256 and use center crop of 224 \u00d7 224 for evaluation. On Sth-Sth V2, we employ a similar evaluation protocol to Kinetics, but only uniformly sample 2 clips, and also present the accuracy with a single clip using center crop.\n\n\nAblation Studies\n\nThe exploration studies are performed on Kinetics-400 to investigate different aspects of TANet. The ResNet architecture we used is the same with [12]. Our TANet replaces all ResNet-Blocks with TA-Blocks by default.\n\nParameter choices. We use different combinations of \u03b1 and \u03b2 to figure out the optimal hyper-parameters in TAM. The TANet is instantiated as in Fig. 2. TANet with \u03b1 = 2 and \u03b2 = 4 achieves the highest performance shown in Table 1a, which will be applied in following experiments.\n\nTemporal receptive fields. We try to increase the temporal receptive fields for learned kernel \u0398 in the global branch. From Table 1b, it seems the larger K is beneficial to the accuracy when TANet takes more sampled frames as inputs. On the other hand, it even degenerates the performance of TANet when sampling 8 frames. In our following experiments, the K will be set to 3.\n\nTAM in the different position. Table 1c tries to study the effects of TAM in different position. TANet-a, TANet-b, TANet-c, and TANet-d denote the TAM is inserted before the first convolution, after the first convolution, after the second convolution, and after the last convolution in the block, respectively. These four styles are graphically presented in the supplementary material. The style in Fig. 2 is TANet-b, which has a slightly better performance than other styles as shown in Table 1c. The TANet-b will be abbreviated as TANet by default in the following experiments.  Table 1. Ablation studies on Kinetics-400. These experiments use ResNet-50 as backbone and take 8-frame as inputs in training. All models share the same inference protocol, i.e., 10 clips \u00d7 3 crops.\n\nThe number of TA-Blocks. To make a trade-off between performance and efficiency, we gradually add more TA-Blocks into ResNet. As shown in Table 1d, we find that more TA-Blocks contributes to better performance. The res 2\u22125 achieves the highest performance and will be used in our experiments.\n\nTransferring to other backbones. Finally we verify the generalization of our proposed module. To this end, we apply the TAM to other well known 2D backbones, like ShuffleNet V2 [26], MobileNet V2 [30], Inception V3 [33] and 3D backbones, like I3D-ResNet-50 [1,2], where all models has no temporal downsampling operation before the global average pooling layer. From Table 1e, we can observe that the backbone networks equipped with our TAM outperform their C2D and I3D baselines by a large margin, which demonstrates the generalization ability of our proposed module.\n\n\nComparison with Other Temporal Modules\n\nAs a standard temporal operator, we make comparisons between our TAM and other temporal modules. For fair comparison, all models in this study employ the same frame input (8 \u00d7 8) and backbone (ResNet-50). The inference protocol is to sample 10 \u00d7 3 crops to report the performance.\n\nBaselines. We first choose several baselines with temporal modules. We begin with the 2D ConvNet (C2D), where we only build 2D ConvNet with ResNet50 and focus on learning the spatial features. In this sense, it operates on each frame independently without any temporal interaction before the global average pooling layer in the end. The second is the C2D-Pool. To endow the 2D network with temporal modeling capacity, C2D-Pool inserts the average pooling  Table 2. Studying on the effectiveness of TAM. All models use ResNet50 as backbone and take 8 frames with sampling stride 8 as inputs. To be consistent with testing, the FLOPs are calculated with spatial size 256 \u00d7 256. \u22c6 is reported by the author of paper. All methods share the same training setting and inference protocol.\n\nlayer whose kernel size is K \u00d7 1 \u00d7 1 to perform temporal fusion without any temporal downsampling. This is easily implemented by simply replacing all TAMs in network with average pooling layers. The third type is the learnable temporal convolution, whose kernel is shared by all videos. We first replace each TAM with a standard temporal convolution with randomly initialized weights, termed as C2D-TConv. In addition, we replace the standard temporal convolution with the channel-wised temporal convolution using TSM [23] initialization to solely aggregate temporal information without relating different channels, termed as C2D+TIM [24]. Finally, we compare with Inflated 3D ConvNet (I3D), whose operation is also based on temporal convolutions by directly inflating the original 2D convolutions into 3D convolutions. In our implementation, we inflate the first 1 \u00d7 1 kernel in ResNet-Block to 3 \u00d7 1 \u00d7 1, which can provide a more fair comparison with our TANet.\n\nFollowing [41], this variant is referred to as I3D 3\u00d71\u00d71 . It is worth noting these three types of temporal convolutions share the similar idea of fixed aggregation kernel, but differ in the specific implementation details, which can demonstrate the efficacy of adaptive aggregation in our TAM. The aforementioned methods share the same temporal modeling scheme with a fixed pooling or convolution. As shown in Table 2, our TAM yields superior performance to all of them. We observe that C2D obtains the worst performance that is less than TAM by 6.1%. Surprisingly, the naively-implemented temporal convolution (C2D-TConv) performs similar to temporal pooling (C2D-Pool) (73.3% vs. 73.1%), which can partly blame on the randomly initialized weights of temporal convolution that corrupt the Ima-geNet pre-trained weights. In temporal convolution based models, we find that C2D-TIM obtains the best performance with the smallest number of FLOPs. We analyze that this channel-wise temporal convolution can well keep the feature channel correspondence and thus benefits most from the ImageNet pre-trained models. However, it is still worse than our TAM by 1.6%.\n\nOther temporal counterparts. There are some competitive temporal modules that learn video features based on C2D, i.e., TSM [23], TEINet [24], and Non-local C2D (NL C2D). We here compare our TAM with these different temporal modules, and the results of TSM and TEINet are directly cited from the original papers, as they share similar numbers of FLOPs to our TAM. The non-local block is a kind of self-attention module, proposed to capture the longrange dependencies in videos. The preferable setting with 5 non-local blocks mentioned in [41] is under a similar computational budget and thereby employed to compare with our TAM. As seen in Table 2, our TANet achieves highest accuracy among these temporal modules, outperforming TSM by 2.2%, TEINet by 1.4%, and NL C2D by 1.9%.\n\n\nVariants of TAM.\n\nTo study the performance of each part in temporal adaptive module, we separately validate the Global branch and Local branch. Furthermore, Global branch + SE uses global branch with SE module [13] to compare with TANet. TANet achieves the highest accuracy among these models as well, which proves the efficacy of each part of TAM and as well as the strong complementarity between local branch and global branch. We also reverse the order of local branch and global branch (TANet-R): Y = L(X)\u2299(G(X)\u2297X). We see that TANet is slightly better than TANet-R.\n\n\nComparison with the State of the Art\n\nComparison on Kinetics-400. Table 3 shows the stateof-the-art results on Kinetics-400. Our method (TANet) achieves the competitive performance to other models. TANet-50 with 8-frame also outperforms SlowFast [8] Table 3. Comparisons with the state-of-the-art methods on Kinetics-400. As described in [8], the GFLOPs of a single view \u00d7 the number of views (temporal clips with spatial crops) represents the model complexity. The GFLOPs is calculated with spatial size 256 \u00d7 256. \u22c6 denotes the I3D without temporal downsampling. 0.7% when using similar FLOPs per view. The 16-frame TANet only uses 4 clips and 3 crops for evaluation such that it provides higher inference efficiency and more fair comparisons with other models. It is worth noting that our 16-frame TANet-50 is still more accurate than 32-frame NL I3D by 1.4%. As ip-CSN [35] is pretrained on Sports-1M [18], it achieves the promising accuracy with deeper backbone, i.e., ResNet152. Furthermore, TAM is compatible with the existing video frameworks like SlowFast. Specifically, our TAM is more lightweight than a standard 3 \u00d7 1 \u00d7 1 convolution when taking the same number of frames as inputs, but can yield a better performance. TAM thus can easily replace the 3\u00d71\u00d71 convolution in SlowFast to achieve lower computational costs. X3D has achieved great success in video recognition. X3D was searched by massive computing resources and can not be easily extended in a new situation. Although our method fails to beat all state-of-the-art methods with deeper networks, TAM as a lightweight operator can enjoy the advantages from more powerful backbones and video frameworks. To sum up, the proposed TANet makes a good practice on adaptively modeling the temporal relations in videos.\n\nComparison on Sth-Sth V1 & V2. As shown in Table 4 the full capabilities of TANet without suffering the overfitting. Following the common practice in [23], TANets use 2 clips with 3 crops to evaluate the accuracy. As shown in Table 5, our models have achieved the state-of-art performance on Sth-Sth V2. As a result, the TANet En yields a competitive accuracy compared with the two-stream TSM and TEINet En . The results on Sth-Sth V1 & V2 have demonstrated that our method is also good at modeling the fine-grained and motion-dominated actions.\n\n\nVisualization of Learned Kernels\n\nTo better understand the behavior of TANet, we visualize the distribution of kernel \u0398 generated by global branch in the last block of stage4 and stage5. For clear comparison, the kernel weights in I3D 3\u00d71\u00d71 at the same stages are also visualized to find more insights. As depicted in Fig. 3, we find that the learned kernel \u0398 has a different property: the shapes and scales of distribution are more diverse than I3D 3\u00d71\u00d71 . Since all video clips share the same kernels in I3D 3\u00d71\u00d71 , it causes the kernel weights cluster together tightly. As opposed to temporal convolution, even model-  ing the same action in different videos, TAM can generate the kernel with slightly different distributions. Taking driving car as an example, the shapes of the distribution shown in Fig. 3 \n\n\nConclusion\n\nIn this paper, we have presented a generic temporal module, termed as temporal adaptive module (TAM), to capture complex motion patterns in videos and proposed a powerful video architecture (TANet) based on this new temporal module. TAM is able to yield a video-specific kernel with the combination of a local importance map and a global aggregation kernel. This unique design is helpful to capture the complex temporal structure in videos and contributes to more effective and robust temporal modeling. As demonstrated on the Kinetics-400, the networks equipped with TAM are better than the existing temporal modules in action recognition, which demonstrates the efficacy of our TAM in video temporal modeling. TANet also achieves the stateof-the-art performance on the motion dominated datasets of Sth-Sth V1&V2.  \n\n\nA. TAM in the different position\n\nWe here introduce the four different exemplars of TANet. TANet-a, TANet-b, TANet-c, and TANet-d denote the TAM is inserted before the first convolution, after the first convolution, after the second convolution, and after the last convolution in the block, respectively. These four styles are graphically presented in Fig. 4 which were mentioned in main text.\n\n\nB. Visualizations of Learned Kernel\n\nIn the supplementary material, we are ready to add more visualizations of distribution for importance map V in local branch and video adaptive kernel \u0398 in global branch. The 3\u00d71\u00d71 convolution kernels in I3D 3\u00d71\u00d71 are also visualized to study their intentions in inference. To probe into the effects on learning kernels in the different stages, the visualized kernels are chosen in stage4 6b and stage5 3b, respectively. Some videos are randomly selected from Kinetics-400 and Sth-Sth V2 to show the diversities in different video datasets.\n\nFirstly, as depicted in Fig. 5 and Fig. 6, We can observe that the distributions of importance map V in local branch are smoother than the kernel \u0398 in global branch, and local branch pays different attention to each video when modeling the temporal relations. Then, the kernel \u0398 in global branch performs the adaptive aggregation to learn the temporal diversities in videos. The visualized kernels in I3D 3\u00d71\u00d71 can make a direct comparison with the kernel \u0398, and we find that the distributions of kernel in I3D 3\u00d71\u00d71 are extremely narrow whether on Kinetics-400 or on Sth-Sth V2. Finally, our learned kernels visualized in figures have exhibited the clear differences between two datasets (Kinetics-400 vs. Sth-Sth V2). This fact is in line with our prior knowledge that there is an obvious domain shift between two datasets. The Kinetics-400 mainly focuses on appearance and Sth-Sth V2 is a motion dominated dataset. However, this point can not be easily summarized from the kernels in I3D 3\u00d71\u00d71 , because the overall distributions of kernels in I3D 3\u00d71\u00d71 on two datasets show minor differences.\n\nGenerally, the diversities in our learned kernels have demonstrated that the diversities are indeed existing in videos, and it is reasonable to learn spatio-temporal representation in an adaptive scheme. These findings are again in line with our motivation claimed in the paper.  Local branch Global branch Figure 6. The distribution of learned kernel V and \u0398 in the stage5 3b.\n\nFigure 3 .\n3The statistics of kernel weights trained on Kinetics-400, and we plots the distributions in different temporal offsets (t \u2208 {\u22121, 0, 1}). Each filled area in violinplot represents the entire data range, which marks the minimum, the median and the maximum. The first four columns in the left figure are the distributions of learned kernels in TANet. In the fifth column, we visualize the filters of 3\u00d71\u00d71 kernel in I3D3\u00d71\u00d71 to compare with the TANet.\n\nFigure 4 .\n4The four styles of TA-Block. The (b) is actually the model we used in the main text..\n\nFigure 5 .\n5The distribution of learned kernel V and \u0398 in the stage4 6b.\n\n\nbyMethods \nBackbones \nTraining Input \nGFLOPs Top-1 Top-5 \n\nTSN [40] \nInceptionV3 \n3\u00d7224\u00d7224 \n3\u00d7250 72.5% 90.2% \nARTNet [38] \nResNet18 \n16 \u00d7112\u00d7112 \n24\u00d7250 70.7% 89.3% \nI3D [1] \nInceptionV1 \n64\u00d7224\u00d7224 \n108\u00d7N/A 72.1% 90.3% \nR(2+1)D [36] \nResNet34 \n32\u00d7112\u00d7112 \n152\u00d710 74.3% 91.4% \nNL I3D [41] \nResNet50 \n128\u00d7224\u00d7224 \n282\u00d730 76.5% 92.6% \nip-CSN [35] \nResNet50 \n8\u00d7224\u00d7224 \n1.2\u00d710 70.8% \n-\nTSM [23] \nResNet50 \n16\u00d7224\u00d7224 \n65\u00d730 74.7% 91.4% \nTEINet [24] \nResNet50 \n16\u00d7224\u00d7224 \n86\u00d730 76.2% 92.5% \nbLVNet-TAM [6] bLResNet50 \n48\u00d7224\u00d7224 \n93\u00d79 73.5% 91.2% \nSlowOnly [8] \nResNet50 \n8\u00d7224\u00d7224 \n42\u00d730 74.8% 91.6% \nSlowFast4\u00d716 [8] ResNet50 (4+32)\u00d7224\u00d7224 36\u00d730 75.6% 92.1% \nSlowFast8\u00d78 [8] ResNet50 (8+32)\u00d7224\u00d7224 66\u00d730 77.0% 92.6% \nI3D \u22c6 [2] \nResNet50 \n32 \u00d7224\u00d7224 \n335 \u00d730 76.6% \n-\nTANet-50 \nResNet50 \n8\u00d7224\u00d7224 \n43\u00d730 76.3% 92.6% \nTANet-50 \nResNet50 \n16\u00d7224\u00d7224 \n86\u00d712 76.9% 92.9% \nX3D-XL [7] \n-\n16\u00d7312\u00d7312 \n48\u00d730 79.1% 93.9% \nCorrNet [37] \nResNet101 \n32\u00d710\u00d73 \n224\u00d730 79.2% \n-\nip-CSN [35] \nResNet152 \n32 \u00d7224\u00d7224 \n83\u00d730 79.2% 93.8% \nSlowFast16\u00d78 [8] ResNet101 (16+64)\u00d7224\u00d7224 213\u00d730 78.9% 93.5% \nTANet-101 \nResNet101 \n8\u00d7224\u00d7224 \n82\u00d730 77.1% 93.1% \nTANet-101 \nResNet101 \n16\u00d7224\u00d7224 \n164\u00d712 78.4% 93.5% \nTANet-152 \nResNet152 \n16\u00d7224\u00d7224 \n242\u00d712 79.3% 94.1% \n\n\n\nTable 5 .\n5Comparisons with the SOTA on Sth-Sth V2. We here apply the two different inference protocal, i.e., 1 clip \u00d7 1 crop and 2 clip \u00d7 3 crop, to fairly evaluate the TAM with other methods., \n\n\n\nare similar to each other but the medians of distributions are not equal. For different actions like drinking beer and skydiving, the shapes and medians of distributions are greatly different. Even for different videos of the same action, TAM can learn a different distribution of kernel weights. Concerning that the motion in different videos may exhibit different patterns, it is necessary to employ an adaptive scheme to model video sequences.\n\n1\u00d71\u00d71 ,\n1\u00d71\u00d71Conv 1\u00d71\u00d71, Conv TAM 1\u00d73\u00d73, Conv TAM 1\u00d71\u00d71, Conv 1\u00d71\u00d71, Conv 1\u00d73\u00d73, Conv+ \n\n+ \n\n1\u00d71\u00d71, Conv \n\n1\u00d71\u00d71, Conv \n\n1\u00d73\u00d73, Conv \n\nTAM \n\n+ \n\n1\u00d71\u00d71, Conv \n\nTAM \n\n1\u00d73\u00d73, Conv \n\n1\u00d71\u00d71, Conv \n\n+ \n\n(a) \n(b) \n(c) \n(d) \n\n\nAcknowledgements. Thanks\nQuo vadis, action recognition? A new model and the kinetics dataset. Jo\u00e3o Carreira, Andrew Zisserman, IEEE Conference on Computer Vision and Pattern Recognition, CVPR. 7Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 4724-4733, 2017. 1, 2, 6, 7\n\nDeep analysis of cnn-based spatio-temporal representations for action recognition. Chun-Fu Richard Chen, Rameswar Panda, Kandan Ramakrishnan, Rogerio Feris, John Cohn, Aude Oliva, Quanfu Fan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition67Chun-Fu Richard Chen, Rameswar Panda, Kandan Ramakr- ishnan, Rogerio Feris, John Cohn, Aude Oliva, and Quanfu Fan. Deep analysis of cnn-based spatio-temporal representa- tions for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6165-6175, 2021. 6, 7\n\nDynamic convolution: Attention over convolution kernels. Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, Zicheng Liu, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE20205Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic convolution: Attention over convolution kernels. In 2020 IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 11027- 11036. IEEE, 2020. 3, 5\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Fei-Fei Li, IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In IEEE Computer Society Conference on Com- puter Vision and Pattern Recognition CVPR, pages 248-255, 2009. 1\n\nSpatio-temporal channel correlation networks for action classification. Ali Diba, Mohsen Fayyaz, Vivek Sharma, Mohammad Mahdi Arzani, Rahman Yousefzadeh, Juergen Gall, Luc Van Gool, Computer Vision -ECCV. Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss112084Ali Diba, Mohsen Fayyaz, Vivek Sharma, Moham- mad Mahdi Arzani, Rahman Yousefzadeh, Juergen Gall, and Luc Van Gool. Spatio-temporal channel correlation networks for action classification. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision -ECCV, volume 11208 of Lecture Notes in Computer Science, pages 299-315, 2018. 3, 4\n\nMore is less: Learning efficient video representations by big-little network and depthwise temporal aggregation. Quanfu Fan, Chun-Fu ; Richard) Chen, Hilde Kuehne, Marco Pistoia, David D Cox, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, Canada7Quanfu Fan, Chun-Fu (Richard) Chen, Hilde Kuehne, Marco Pistoia, and David D. Cox. More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9- Buc, Emily B. Fox, and Roman Garnett, editors, Ad- vances in Neural Information Processing Systems 32: An- nual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 2261-2270, 2019. 7, 8\n\nX3D: expanding architectures for efficient video recognition. Christoph Feichtenhofer, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Christoph Feichtenhofer. X3D: expanding architectures for efficient video recognition. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 200-210. IEEE, 2020. 7\n\nSlowfast networks for video recognition. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He, IEEE/CVF International Conference on Computer Vision, ICCV. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In IEEE/CVF International Conference on Computer Vision, ICCV, pages 6201-6210, 2019. 1, 2, 7\n\n. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal- ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\n\nThe \"something something\" video database for learning and evaluating visual common sense. Valentin Haenel, Ingo Fr\u00fcnd, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, Roland Memisevic, IEEE International Conference on Computer Vision, ICCV. Valentin Haenel, Ingo Fr\u00fcnd, Peter Yianilos, Moritz Mueller- Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The \"something something\" video database for learning and evaluating visual common sense. In IEEE International Conference on Computer Vision, ICCV, pages 5843-5851, 2017. 5\n\nStnet: Local and global spatial-temporal modeling for action recognition. Dongliang He, Zhichao Zhou, Chuang Gan, Fu Li, Xiao Liu, Yandong Li, Limin Wang, Shilei Wen, The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI. Dongliang He, Zhichao Zhou, Chuang Gan, Fu Li, Xiao Liu, Yandong Li, Limin Wang, and Shilei Wen. Stnet: Local and global spatial-temporal modeling for action recognition. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Arti- ficial Intelligence Conference, IAAI, pages 8401-8408, 2019. 2\n\nMask R-CNN. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross B Girshick, IEEE International Conference on Computer Vision, ICCV. IEEE Computer SocietyKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision, ICCV, pages 2980-2988. IEEE Com- puter Society, 2017. 1\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, IEEE Conference on Computer Vision and Pattern Recognition, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Con- ference on Computer Vision and Pattern Recognition, CVPR, pages 770-778, 2016. 1, 2, 4, 5\n\nSqueeze-and-excitation networks. Jie Hu, Li Shen, Gang Sun, IEEE Conference on Computer Vision and Pattern Recognition, CVPR. 67Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net- works. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 7132-7141, 2018. 3, 4, 6, 7\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, Proceedings of the 32nd International Conference on Machine Learning. Francis R. Bach and David M. Bleithe 32nd International Conference on Machine Learning37of JMLR Workshop and Conference ProceedingsSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Ma- chine Learning, ICML 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 448-456. JMLR.org, 2015. 4\n\n3d convolutional neural networks for human action recognition. Shuiwang Ji, Wei Xu, Ming Yang, Kai Yu, Proceedings of the 27th International Conference on Machine Learning (ICML). the 27th International Conference on Machine Learning (ICML)15Johannes F\u00fcrnkranz and Thorsten JoachimsShuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convo- lutional neural networks for human action recognition. In Johannes F\u00fcrnkranz and Thorsten Joachims, editors, Pro- ceedings of the 27th International Conference on Machine Learning (ICML), pages 495-502, 2010. 1, 2, 5\n\nDynamic filter networks. Xu Jia, Bert De Brabandere, Tinne Tuytelaars, Luc Van Gool, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems. Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman GarnettBarcelona, SpainXu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc Van Gool. Dynamic filter networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Process- ing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 667-675, 2016. 3\n\nSTM: spatiotemporal and motion encoding for action recognition. Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, Junjie Yan, abs/1908.02486CoRRBoyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. STM: spatiotemporal and motion encoding for action recognition. CoRR, abs/1908.02486, 2019. 1\n\nLarge-scale video classification with convolutional neural networks. Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Fei-Fei Li, IEEE Conference on Computer Vision and Pattern Recognition, CVPR. 27Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Fei-Fei Li. Large-scale video classification with convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 1725-1732, 2014. 2, 7\n\nThe kinetics human action video dataset. Will Kay, Jo\u00e3o Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman, abs/1705.06950CoRR25Will Kay, Jo\u00e3o Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. CoRR, abs/1705.06950, 2017. 2, 5\n\nA spatio-temporal descriptor based on 3d-gradients. Alexander Kl\u00e4ser, Marcin Marszalek, Cordelia Schmid, Proceedings of the British Machine Vision Conference. Mark Everingham, Chris J. Needham, and Roberto Frailethe British Machine Vision ConferenceBritish Machine Vision AssociationAlexander Kl\u00e4ser, Marcin Marszalek, and Cordelia Schmid. A spatio-temporal descriptor based on 3d-gradients. In Mark Everingham, Chris J. Needham, and Roberto Fraile, editors, Proceedings of the British Machine Vision Conference 2008, pages 1-10. British Machine Vision Association, 2008. 2\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems. Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L\u00e9on Bottou, and Kilian Q. Weinberger1Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In Peter L. Bartlett, Fernando C. N. Pereira, Christo- pher J. C. Burges, L\u00e9on Bottou, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Process- ing Systems 2012., pages 1106-1114, 2012. 1, 2\n\nLearning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. V Quoc, Will Y Le, Serena Y Zou, Andrew Y Yeung, Ng, IEEE Conference on Computer Vision and Pattern Recognition, CVPR. IEEE Computer SocietyQuoc V. Le, Will Y. Zou, Serena Y. Yeung, and Andrew Y. Ng. Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. In IEEE Conference on Computer Vision and Pattern Recog- nition, CVPR, pages 3361-3368. IEEE Computer Society, 2011. 2\n\nTSM: temporal shift module for efficient video understanding. Ji Lin, Chuang Gan, Song Han, IEEE International Conference on Computer Vision, ICCV 2019. 7Ji Lin, Chuang Gan, and Song Han. TSM: temporal shift module for efficient video understanding. In IEEE Interna- tional Conference on Computer Vision, ICCV 2019, pages 7082-7092, 2019. 1, 2, 5, 6, 7, 8\n\nTeinet: Towards an efficient architecture for video recognition. CoRR, abs. Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Tong Lu, 7Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Tong Lu. Teinet: Towards an efficient architecture for video recog- nition. CoRR, abs/1911.09435, 2019. 2, 3, 6, 7, 8\n\nGrouped spatial-temporal aggregation for efficient action recognition. Chenxu Luo, Alan L Yuille, Chenxu Luo and Alan L. Yuille. Grouped spatial-temporal aggregation for efficient action recognition. In 2019\n\nIEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South). IEEEIEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -November 2, 2019, pages 5511-5520. IEEE, 2019. 8\n\nShufflenet V2: practical guidelines for efficient CNN architecture design. Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun, Computer Vision -ECCV 2018 -15th European Conference. Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair WeissMunich, GermanySpringer11218Proceedings, Part XIVNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet V2: practical guidelines for efficient CNN archi- tecture design. In Vittorio Ferrari, Martial Hebert, Cris- tian Sminchisescu, and Yair Weiss, editors, Computer Vision -ECCV 2018 -15th European Conference, Munich, Ger- many, September 8-14, 2018, Proceedings, Part XIV, volume 11218 of Lecture Notes in Computer Science, pages 122- 138. Springer, 2018. 6\n\nLearning spatiotemporal representation with pseudo-3d residual networks. Zhaofan Qiu, Ting Yao, Tao Mei, IEEE International Conference on Computer Vision, ICCV. 1Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio- temporal representation with pseudo-3d residual networks. In IEEE International Conference on Computer Vision, ICCV, pages 5534-5542, 2017. 1, 2\n\nFaster R-CNN: towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross B He, Jian Girshick, Sun, IEEE Trans. Pattern Anal. Mach. Intell. 396Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with re- gion proposal networks. IEEE Trans. Pattern Anal. Mach. Intell., 39(6):1137-1149, 2017. 1\n\nAction bank: A high-level representation of activity in video. Sreemanananth Sadanand, Jason J Corso, 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer SocietySreemanananth Sadanand and Jason J. Corso. Action bank: A high-level representation of activity in video. In 2012 IEEE Conference on Computer Vision and Pattern Recogni- tion, 2012, pages 1234-1241. IEEE Computer Society, 2012. 2\n\nMark Sandler, Andrew G Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USAMobilenetv2: Inverted residuals and linear bottlenecksMark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 4510- 4520. IEEE Computer Society, 2018. 6\n\nTwo-stream convolutional networks for action recognition in videos. Karen Simonyan, Andrew Zisserman, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Karen Simonyan and Andrew Zisserman. Two-stream con- volutional networks for action recognition in videos. In Ad- vances in Neural Information Processing Systems 27: An- nual Conference on Neural Information Processing Systems 2014, pages 568-576, 2014. 2\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, 3rd International Conference on Learning Representations. Yoshua Bengio and Yann LeCunKaren Simonyan and Andrew Zisserman. Very deep con- volutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, 2015. 2\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the in- ception architecture for computer vision. In 2016 IEEE Con- ference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2818- 2826. IEEE Computer Society, 2016. 6\n\nLearning spatiotemporal features with 3d convolutional networks. Du Tran, D Lubomir, Rob Bourdev, Lorenzo Fergus, Manohar Torresani, Paluri, IEEE International Conference on Computer Vision, ICCV. 1Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torre- sani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In IEEE International Con- ference on Computer Vision, ICCV, pages 4489-4497, 2015. 1, 2\n\nVideo classification with channel-separated convolutional networks. Du Tran, Heng Wang, Matt Feiszli, Lorenzo Torresani, IEEE International Conference on Computer Vision, ICCV 2019. Du Tran, Heng Wang, Matt Feiszli, and Lorenzo Torre- sani. Video classification with channel-separated convolu- tional networks. In IEEE International Conference on Com- puter Vision, ICCV 2019, pages 5551-5560, 2019. 7\n\nA closer look at spatiotemporal convolutions for action recognition. Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann Lecun, Manohar Paluri, IEEE Conference on Computer Vision and Pattern Recognition, CVPR. Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotempo- ral convolutions for action recognition. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 6450-6459, 2018. 1, 2, 7\n\nVideo modeling with correlation networks. Heng Wang, Du Tran, Lorenzo Torresani, Matt Feiszli, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHeng Wang, Du Tran, Lorenzo Torresani, and Matt Feiszli. Video modeling with correlation networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 352-361, 2020. 7\n\nAppearance-and-relation networks for video classification. Limin Wang, Wei Li, Wen Li, Luc Van Gool, IEEE Conference on Computer Vision and Pattern Recognition, CVPR. 17Limin Wang, Wei Li, Wen Li, and Luc Van Gool. Appearance-and-relation networks for video classification. In IEEE Conference on Computer Vision and Pattern Recog- nition, CVPR, pages 1430-1439, 2018. 1, 7\n\nTdn: Temporal difference networks for efficient action recognition. Limin Wang, Zhan Tong, Bin Ji, Gangshan Wu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition1Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn: Temporal difference networks for efficient action recogni- tion. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 1895-1904, 2021. 1, 2\n\nTemporal segment networks: Towards good practices for deep action recognition. Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool, Computer Vision -ECCV. 7Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recogni- tion. In Computer Vision -ECCV, pages 20-36, 2016. 1, 2, 5, 7, 8\n\nNon-local neural networks. Xiaolong Wang, Ross B Girshick, Abhinav Gupta, Kaiming He, IEEE Conference on Computer Vision and Pattern Recognition, CVPR. 67Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In IEEE Confer- ence on Computer Vision and Pattern Recognition, CVPR, pages 7794-7803, 2018. 1, 2, 3, 5, 6, 7\n\nVideos as space-time region graphs. Xiaolong Wang, Abhinav Gupta, Computer Vision -ECCV. Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In Computer Vision -ECCV, pages 413-431, 2018. 8\n\nAn efficient dense and scale-invariant spatio-temporal interest point detector. Geert Willems, Tinne Tuytelaars, Luc Van Gool, Computer Vision -ECCV. David A. Forsyth, Philip H. S. Torr, and Andrew Zisserman5303Geert Willems, Tinne Tuytelaars, and Luc Van Gool. An effi- cient dense and scale-invariant spatio-temporal interest point detector. In David A. Forsyth, Philip H. S. Torr, and Andrew Zisserman, editors, Computer Vision -ECCV, volume 5303 of Lecture Notes in Computer Science, pages 650-663, 2008. 2\n\nRethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, Kevin Murphy, Computer Vision -ECCV. 2Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Com- puter Vision -ECCV, pages 318-335, 2018. 2, 8\n\nCondconv: Conditionally parameterized convolutions for efficient inference. Brandon Yang, Gabriel Bender, Quoc V Le, Jiquan Ngiam, Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, Canada325Advances in Neural Information Processing SystemsBrandon Yang, Gabriel Bender, Quoc V. Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convo- lutions for efficient inference. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9- Buc, Emily B. Fox, and Roman Garnett, editors, Ad- vances in Neural Information Processing Systems 32: An- nual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 1305-1316, 2019. 3, 5\n\nTemporal relational reasoning in videos. Bolei Zhou, Alex Andonian, Aude Oliva, Antonio Torralba, Computer Vision -ECCV. 2Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Tor- ralba. Temporal relational reasoning in videos. In Computer Vision -ECCV, pages 831-846, 2018. 2, 8\n\nECO: efficient convolutional network for online video understanding. Mohammadreza Zolfaghari, Kamaljeet Singh, Thomas Brox, Computer Vision -ECCV. Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. ECO: efficient convolutional network for online video understanding. In Computer Vision -ECCV, pages 713-730, 2018. 8\n", "annotations": {"author": "[{\"end\":159,\"start\":55},{\"end\":267,\"start\":160},{\"end\":298,\"start\":268},{\"end\":353,\"start\":299},{\"end\":450,\"start\":354}]", "publisher": null, "author_last_name": "[{\"end\":67,\"start\":64},{\"end\":170,\"start\":166},{\"end\":276,\"start\":274},{\"end\":308,\"start\":304},{\"end\":361,\"start\":359}]", "author_first_name": "[{\"end\":63,\"start\":55},{\"end\":165,\"start\":160},{\"end\":273,\"start\":268},{\"end\":303,\"start\":299},{\"end\":358,\"start\":354}]", "author_affiliation": "[{\"end\":137,\"start\":69},{\"end\":158,\"start\":139},{\"end\":266,\"start\":198},{\"end\":297,\"start\":278},{\"end\":352,\"start\":333},{\"end\":449,\"start\":381}]", "title": "[{\"end\":52,\"start\":1},{\"end\":502,\"start\":451}]", "venue": null, "abstract": "[{\"end\":1649,\"start\":504}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1786,\"start\":1782},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1789,\"start\":1786},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1812,\"start\":1808},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1844,\"start\":1840},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2019,\"start\":2016},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2518,\"start\":2514},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2521,\"start\":2518},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2591,\"start\":2588},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2593,\"start\":2591},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2596,\"start\":2593},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2599,\"start\":2596},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3052,\"start\":3048},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3062,\"start\":3058},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3166,\"start\":3162},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3179,\"start\":3175},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3189,\"start\":3185},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3199,\"start\":3195},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6601,\"start\":6597},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6614,\"start\":6610},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6640,\"start\":6636},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7005,\"start\":7001},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7008,\"start\":7005},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7011,\"start\":7008},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7014,\"start\":7011},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7277,\"start\":7273},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7280,\"start\":7277},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7283,\"start\":7280},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7689,\"start\":7685},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7692,\"start\":7689},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7695,\"start\":7692},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7698,\"start\":7695},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7701,\"start\":7698},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7704,\"start\":7701},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7707,\"start\":7704},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7781,\"start\":7777},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7936,\"start\":7932},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8141,\"start\":8137},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8144,\"start\":8141},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8271,\"start\":8267},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8284,\"start\":8281},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8342,\"start\":8338},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8345,\"start\":8342},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8348,\"start\":8345},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8484,\"start\":8481},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9453,\"start\":9449},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9550,\"start\":9546},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9552,\"start\":9550},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9760,\"start\":9756},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9999,\"start\":9995},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10173,\"start\":10169},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10175,\"start\":10173},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13693,\"start\":13689},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17599,\"start\":17595},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18314,\"start\":18310},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18326,\"start\":18323},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18766,\"start\":18762},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18779,\"start\":18775},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19144,\"start\":19141},{\"end\":19146,\"start\":19144},{\"end\":19148,\"start\":19146},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19342,\"start\":19338},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19810,\"start\":19807},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":19813,\"start\":19810},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20033,\"start\":20029},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20077,\"start\":20074},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20921,\"start\":20917},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21046,\"start\":21042},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22691,\"start\":22687},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24670,\"start\":24666},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24689,\"start\":24685},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24708,\"start\":24704},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24749,\"start\":24746},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24751,\"start\":24749},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26686,\"start\":26682},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26802,\"start\":26798},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27143,\"start\":27139},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28416,\"start\":28412},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28429,\"start\":28425},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28830,\"start\":28826},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29282,\"start\":29278},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29890,\"start\":29887},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29982,\"start\":29979},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":30518,\"start\":30514},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30550,\"start\":30546},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":31579,\"start\":31575}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36529,\"start\":36068},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36628,\"start\":36530},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36702,\"start\":36629},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37952,\"start\":36703},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38150,\"start\":37953},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":38599,\"start\":38151},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":38819,\"start\":38600}]", "paragraph": "[{\"end\":2469,\"start\":1665},{\"end\":3420,\"start\":2471},{\"end\":4549,\"start\":3422},{\"end\":5263,\"start\":4551},{\"end\":6272,\"start\":5265},{\"end\":6865,\"start\":6274},{\"end\":7544,\"start\":6882},{\"end\":8614,\"start\":7546},{\"end\":9217,\"start\":8616},{\"end\":10607,\"start\":9219},{\"end\":11190,\"start\":10661},{\"end\":11573,\"start\":11192},{\"end\":12324,\"start\":11620},{\"end\":12977,\"start\":12351},{\"end\":13339,\"start\":13001},{\"end\":13931,\"start\":13341},{\"end\":14142,\"start\":13970},{\"end\":15126,\"start\":14167},{\"end\":16065,\"start\":15184},{\"end\":16248,\"start\":16067},{\"end\":16475,\"start\":16263},{\"end\":16813,\"start\":16493},{\"end\":16965,\"start\":16872},{\"end\":17248,\"start\":16967},{\"end\":17585,\"start\":17268},{\"end\":18226,\"start\":17587},{\"end\":18756,\"start\":18228},{\"end\":19303,\"start\":18758},{\"end\":19920,\"start\":19305},{\"end\":20765,\"start\":19947},{\"end\":21824,\"start\":20792},{\"end\":22520,\"start\":21826},{\"end\":22756,\"start\":22541},{\"end\":23035,\"start\":22758},{\"end\":23412,\"start\":23037},{\"end\":24193,\"start\":23414},{\"end\":24487,\"start\":24195},{\"end\":25056,\"start\":24489},{\"end\":25379,\"start\":25099},{\"end\":26162,\"start\":25381},{\"end\":27127,\"start\":26164},{\"end\":28287,\"start\":27129},{\"end\":29065,\"start\":28289},{\"end\":29638,\"start\":29086},{\"end\":31423,\"start\":29679},{\"end\":31970,\"start\":31425},{\"end\":32784,\"start\":32007},{\"end\":33615,\"start\":32799},{\"end\":34011,\"start\":33652},{\"end\":34590,\"start\":34051},{\"end\":35688,\"start\":34592},{\"end\":36067,\"start\":35690}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11619,\"start\":11574},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12350,\"start\":12325},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13969,\"start\":13932},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15183,\"start\":15127},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16262,\"start\":16249},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16492,\"start\":16476},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16871,\"start\":16814}]", "table_ref": "[{\"end\":22986,\"start\":22978},{\"end\":23169,\"start\":23161},{\"end\":23453,\"start\":23445},{\"end\":23910,\"start\":23902},{\"end\":24002,\"start\":23995},{\"end\":24341,\"start\":24333},{\"end\":24863,\"start\":24855},{\"end\":25844,\"start\":25837},{\"end\":27547,\"start\":27540},{\"end\":28935,\"start\":28928},{\"end\":29714,\"start\":29707},{\"end\":29898,\"start\":29891},{\"end\":31475,\"start\":31468},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31658,\"start\":31651}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1663,\"start\":1651},{\"attributes\":{\"n\":\"2.\"},\"end\":6880,\"start\":6868},{\"attributes\":{\"n\":\"3.\"},\"end\":10616,\"start\":10610},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10659,\"start\":10619},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12999,\"start\":12980},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14165,\"start\":14145},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17266,\"start\":17251},{\"attributes\":{\"n\":\"4.\"},\"end\":19934,\"start\":19923},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19945,\"start\":19937},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20790,\"start\":20768},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22539,\"start\":22523},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25097,\"start\":25059},{\"end\":29084,\"start\":29068},{\"attributes\":{\"n\":\"4.5.\"},\"end\":29677,\"start\":29641},{\"attributes\":{\"n\":\"4.6.\"},\"end\":32005,\"start\":31973},{\"attributes\":{\"n\":\"5.\"},\"end\":32797,\"start\":32787},{\"end\":33650,\"start\":33618},{\"end\":34049,\"start\":34014},{\"end\":36079,\"start\":36069},{\"end\":36541,\"start\":36531},{\"end\":36640,\"start\":36630},{\"end\":37963,\"start\":37954},{\"end\":38608,\"start\":38601}]", "table": "[{\"end\":37952,\"start\":36707},{\"end\":38150,\"start\":38147},{\"end\":38819,\"start\":38686}]", "figure_caption": "[{\"end\":36529,\"start\":36081},{\"end\":36628,\"start\":36543},{\"end\":36702,\"start\":36642},{\"end\":36707,\"start\":36705},{\"end\":38147,\"start\":37965},{\"end\":38599,\"start\":38153},{\"end\":38686,\"start\":38614}]", "figure_ref": "[{\"end\":4079,\"start\":4071},{\"end\":4613,\"start\":4605},{\"end\":5640,\"start\":5634},{\"end\":8854,\"start\":8846},{\"end\":11116,\"start\":11108},{\"end\":13361,\"start\":13353},{\"end\":17686,\"start\":17680},{\"end\":22907,\"start\":22901},{\"end\":23819,\"start\":23813},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32297,\"start\":32291},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32783,\"start\":32777},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33976,\"start\":33970},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":34622,\"start\":34616},{\"end\":34633,\"start\":34627},{\"end\":36005,\"start\":35997}]", "bib_author_first_name": "[{\"end\":38918,\"start\":38914},{\"end\":38935,\"start\":38929},{\"end\":39321,\"start\":39306},{\"end\":39336,\"start\":39328},{\"end\":39350,\"start\":39344},{\"end\":39372,\"start\":39365},{\"end\":39384,\"start\":39380},{\"end\":39395,\"start\":39391},{\"end\":39409,\"start\":39403},{\"end\":39945,\"start\":39938},{\"end\":39958,\"start\":39952},{\"end\":39972,\"start\":39964},{\"end\":39986,\"start\":39978},{\"end\":39995,\"start\":39993},{\"end\":40009,\"start\":40002},{\"end\":40462,\"start\":40459},{\"end\":40472,\"start\":40469},{\"end\":40486,\"start\":40479},{\"end\":40501,\"start\":40495},{\"end\":40509,\"start\":40506},{\"end\":40521,\"start\":40514},{\"end\":40918,\"start\":40915},{\"end\":40931,\"start\":40925},{\"end\":40945,\"start\":40940},{\"end\":40962,\"start\":40954},{\"end\":40968,\"start\":40963},{\"end\":40983,\"start\":40977},{\"end\":41004,\"start\":40997},{\"end\":41014,\"start\":41011},{\"end\":41618,\"start\":41612},{\"end\":41633,\"start\":41624},{\"end\":41654,\"start\":41649},{\"end\":41668,\"start\":41663},{\"end\":41683,\"start\":41678},{\"end\":41685,\"start\":41684},{\"end\":42541,\"start\":42532},{\"end\":42936,\"start\":42927},{\"end\":42957,\"start\":42952},{\"end\":42971,\"start\":42963},{\"end\":42986,\"start\":42979},{\"end\":43263,\"start\":43257},{\"end\":43277,\"start\":43271},{\"end\":43286,\"start\":43278},{\"end\":43301,\"start\":43294},{\"end\":43319,\"start\":43313},{\"end\":43340,\"start\":43333},{\"end\":43356,\"start\":43351},{\"end\":43568,\"start\":43560},{\"end\":43581,\"start\":43577},{\"end\":43594,\"start\":43589},{\"end\":43611,\"start\":43605},{\"end\":43636,\"start\":43629},{\"end\":43653,\"start\":43644},{\"end\":43666,\"start\":43662},{\"end\":43678,\"start\":43672},{\"end\":44139,\"start\":44130},{\"end\":44151,\"start\":44144},{\"end\":44164,\"start\":44158},{\"end\":44172,\"start\":44170},{\"end\":44181,\"start\":44177},{\"end\":44194,\"start\":44187},{\"end\":44204,\"start\":44199},{\"end\":44217,\"start\":44211},{\"end\":44760,\"start\":44753},{\"end\":44772,\"start\":44765},{\"end\":44788,\"start\":44783},{\"end\":44801,\"start\":44797},{\"end\":44803,\"start\":44802},{\"end\":45132,\"start\":45125},{\"end\":45144,\"start\":45137},{\"end\":45160,\"start\":45152},{\"end\":45170,\"start\":45166},{\"end\":45483,\"start\":45480},{\"end\":45490,\"start\":45488},{\"end\":45501,\"start\":45497},{\"end\":45845,\"start\":45839},{\"end\":45862,\"start\":45853},{\"end\":46495,\"start\":46487},{\"end\":46503,\"start\":46500},{\"end\":46512,\"start\":46508},{\"end\":46522,\"start\":46519},{\"end\":47005,\"start\":47003},{\"end\":47015,\"start\":47011},{\"end\":47036,\"start\":47031},{\"end\":47052,\"start\":47049},{\"end\":47722,\"start\":47716},{\"end\":47738,\"start\":47730},{\"end\":47751,\"start\":47745},{\"end\":47760,\"start\":47757},{\"end\":47771,\"start\":47765},{\"end\":48030,\"start\":48024},{\"end\":48047,\"start\":48041},{\"end\":48065,\"start\":48058},{\"end\":48080,\"start\":48074},{\"end\":48093,\"start\":48088},{\"end\":48113,\"start\":48106},{\"end\":48496,\"start\":48492},{\"end\":48506,\"start\":48502},{\"end\":48522,\"start\":48517},{\"end\":48538,\"start\":48533},{\"end\":48551,\"start\":48546},{\"end\":48571,\"start\":48561},{\"end\":48595,\"start\":48590},{\"end\":48606,\"start\":48603},{\"end\":48620,\"start\":48614},{\"end\":48631,\"start\":48627},{\"end\":48647,\"start\":48640},{\"end\":48664,\"start\":48658},{\"end\":49020,\"start\":49011},{\"end\":49035,\"start\":49029},{\"end\":49055,\"start\":49047},{\"end\":49603,\"start\":49599},{\"end\":49620,\"start\":49616},{\"end\":49640,\"start\":49632},{\"end\":49642,\"start\":49641},{\"end\":50396,\"start\":50395},{\"end\":50407,\"start\":50403},{\"end\":50409,\"start\":50408},{\"end\":50420,\"start\":50414},{\"end\":50422,\"start\":50421},{\"end\":50434,\"start\":50428},{\"end\":50436,\"start\":50435},{\"end\":50895,\"start\":50893},{\"end\":50907,\"start\":50901},{\"end\":50917,\"start\":50913},{\"end\":51272,\"start\":51264},{\"end\":51285,\"start\":51278},{\"end\":51297,\"start\":51291},{\"end\":51309,\"start\":51304},{\"end\":51320,\"start\":51316},{\"end\":51334,\"start\":51326},{\"end\":51346,\"start\":51341},{\"end\":51357,\"start\":51351},{\"end\":51369,\"start\":51365},{\"end\":51676,\"start\":51670},{\"end\":51686,\"start\":51682},{\"end\":51688,\"start\":51687},{\"end\":52131,\"start\":52123},{\"end\":52143,\"start\":52136},{\"end\":52158,\"start\":52151},{\"end\":52170,\"start\":52166},{\"end\":52855,\"start\":52848},{\"end\":52865,\"start\":52861},{\"end\":52874,\"start\":52871},{\"end\":53223,\"start\":53216},{\"end\":53242,\"start\":53238},{\"end\":53244,\"start\":53243},{\"end\":53253,\"start\":53249},{\"end\":53595,\"start\":53582},{\"end\":53611,\"start\":53606},{\"end\":53613,\"start\":53612},{\"end\":53942,\"start\":53938},{\"end\":53958,\"start\":53952},{\"end\":53960,\"start\":53959},{\"end\":53977,\"start\":53969},{\"end\":53989,\"start\":53983},{\"end\":54012,\"start\":54001},{\"end\":54548,\"start\":54543},{\"end\":54565,\"start\":54559},{\"end\":55021,\"start\":55016},{\"end\":55038,\"start\":55032},{\"end\":55431,\"start\":55422},{\"end\":55448,\"start\":55441},{\"end\":55466,\"start\":55460},{\"end\":55482,\"start\":55474},{\"end\":55499,\"start\":55491},{\"end\":56003,\"start\":56001},{\"end\":56011,\"start\":56010},{\"end\":56024,\"start\":56021},{\"end\":56041,\"start\":56034},{\"end\":56057,\"start\":56050},{\"end\":56441,\"start\":56439},{\"end\":56452,\"start\":56448},{\"end\":56463,\"start\":56459},{\"end\":56480,\"start\":56473},{\"end\":56845,\"start\":56843},{\"end\":56856,\"start\":56852},{\"end\":56870,\"start\":56863},{\"end\":56887,\"start\":56882},{\"end\":56897,\"start\":56893},{\"end\":56912,\"start\":56905},{\"end\":57287,\"start\":57283},{\"end\":57296,\"start\":57294},{\"end\":57310,\"start\":57303},{\"end\":57326,\"start\":57322},{\"end\":57758,\"start\":57753},{\"end\":57768,\"start\":57765},{\"end\":57776,\"start\":57773},{\"end\":57784,\"start\":57781},{\"end\":58141,\"start\":58136},{\"end\":58152,\"start\":58148},{\"end\":58162,\"start\":58159},{\"end\":58175,\"start\":58167},{\"end\":58649,\"start\":58644},{\"end\":58663,\"start\":58656},{\"end\":58674,\"start\":58671},{\"end\":58683,\"start\":58681},{\"end\":58695,\"start\":58690},{\"end\":58707,\"start\":58701},{\"end\":58717,\"start\":58714},{\"end\":59016,\"start\":59008},{\"end\":59027,\"start\":59023},{\"end\":59029,\"start\":59028},{\"end\":59047,\"start\":59040},{\"end\":59062,\"start\":59055},{\"end\":59382,\"start\":59374},{\"end\":59396,\"start\":59389},{\"end\":59631,\"start\":59626},{\"end\":59646,\"start\":59641},{\"end\":59662,\"start\":59659},{\"end\":60160,\"start\":60153},{\"end\":60170,\"start\":60166},{\"end\":60184,\"start\":60176},{\"end\":60199,\"start\":60192},{\"end\":60209,\"start\":60204},{\"end\":60544,\"start\":60537},{\"end\":60558,\"start\":60551},{\"end\":60571,\"start\":60567},{\"end\":60573,\"start\":60572},{\"end\":60584,\"start\":60578},{\"end\":61356,\"start\":61351},{\"end\":61367,\"start\":61363},{\"end\":61382,\"start\":61378},{\"end\":61397,\"start\":61390},{\"end\":61670,\"start\":61658},{\"end\":61692,\"start\":61683},{\"end\":61706,\"start\":61700}]", "bib_author_last_name": "[{\"end\":38927,\"start\":38919},{\"end\":38945,\"start\":38936},{\"end\":39326,\"start\":39322},{\"end\":39342,\"start\":39337},{\"end\":39363,\"start\":39351},{\"end\":39378,\"start\":39373},{\"end\":39389,\"start\":39385},{\"end\":39401,\"start\":39396},{\"end\":39413,\"start\":39410},{\"end\":39950,\"start\":39946},{\"end\":39962,\"start\":39959},{\"end\":39976,\"start\":39973},{\"end\":39991,\"start\":39987},{\"end\":40000,\"start\":39996},{\"end\":40013,\"start\":40010},{\"end\":40467,\"start\":40463},{\"end\":40477,\"start\":40473},{\"end\":40493,\"start\":40487},{\"end\":40504,\"start\":40502},{\"end\":40512,\"start\":40510},{\"end\":40524,\"start\":40522},{\"end\":40923,\"start\":40919},{\"end\":40938,\"start\":40932},{\"end\":40952,\"start\":40946},{\"end\":40975,\"start\":40969},{\"end\":40995,\"start\":40984},{\"end\":41009,\"start\":41005},{\"end\":41023,\"start\":41015},{\"end\":41622,\"start\":41619},{\"end\":41647,\"start\":41634},{\"end\":41661,\"start\":41655},{\"end\":41676,\"start\":41669},{\"end\":41689,\"start\":41686},{\"end\":42555,\"start\":42542},{\"end\":42950,\"start\":42937},{\"end\":42961,\"start\":42958},{\"end\":42977,\"start\":42972},{\"end\":42989,\"start\":42987},{\"end\":43269,\"start\":43264},{\"end\":43292,\"start\":43287},{\"end\":43311,\"start\":43302},{\"end\":43331,\"start\":43320},{\"end\":43349,\"start\":43341},{\"end\":43360,\"start\":43357},{\"end\":43575,\"start\":43569},{\"end\":43587,\"start\":43582},{\"end\":43603,\"start\":43595},{\"end\":43627,\"start\":43612},{\"end\":43642,\"start\":43637},{\"end\":43660,\"start\":43654},{\"end\":43670,\"start\":43667},{\"end\":43688,\"start\":43679},{\"end\":44142,\"start\":44140},{\"end\":44156,\"start\":44152},{\"end\":44168,\"start\":44165},{\"end\":44175,\"start\":44173},{\"end\":44185,\"start\":44182},{\"end\":44197,\"start\":44195},{\"end\":44209,\"start\":44205},{\"end\":44221,\"start\":44218},{\"end\":44763,\"start\":44761},{\"end\":44781,\"start\":44773},{\"end\":44795,\"start\":44789},{\"end\":44812,\"start\":44804},{\"end\":45135,\"start\":45133},{\"end\":45150,\"start\":45145},{\"end\":45164,\"start\":45161},{\"end\":45174,\"start\":45171},{\"end\":45486,\"start\":45484},{\"end\":45495,\"start\":45491},{\"end\":45505,\"start\":45502},{\"end\":45851,\"start\":45846},{\"end\":45870,\"start\":45863},{\"end\":46498,\"start\":46496},{\"end\":46506,\"start\":46504},{\"end\":46517,\"start\":46513},{\"end\":46525,\"start\":46523},{\"end\":47009,\"start\":47006},{\"end\":47029,\"start\":47016},{\"end\":47047,\"start\":47037},{\"end\":47061,\"start\":47053},{\"end\":47728,\"start\":47723},{\"end\":47743,\"start\":47739},{\"end\":47755,\"start\":47752},{\"end\":47763,\"start\":47761},{\"end\":47775,\"start\":47772},{\"end\":48039,\"start\":48031},{\"end\":48056,\"start\":48048},{\"end\":48072,\"start\":48066},{\"end\":48086,\"start\":48081},{\"end\":48104,\"start\":48094},{\"end\":48116,\"start\":48114},{\"end\":48500,\"start\":48497},{\"end\":48515,\"start\":48507},{\"end\":48531,\"start\":48523},{\"end\":48544,\"start\":48539},{\"end\":48559,\"start\":48552},{\"end\":48588,\"start\":48572},{\"end\":48601,\"start\":48596},{\"end\":48612,\"start\":48607},{\"end\":48625,\"start\":48621},{\"end\":48638,\"start\":48632},{\"end\":48656,\"start\":48648},{\"end\":48674,\"start\":48665},{\"end\":49027,\"start\":49021},{\"end\":49045,\"start\":49036},{\"end\":49062,\"start\":49056},{\"end\":49614,\"start\":49604},{\"end\":49630,\"start\":49621},{\"end\":49649,\"start\":49643},{\"end\":50401,\"start\":50397},{\"end\":50412,\"start\":50410},{\"end\":50426,\"start\":50423},{\"end\":50442,\"start\":50437},{\"end\":50446,\"start\":50444},{\"end\":50899,\"start\":50896},{\"end\":50911,\"start\":50908},{\"end\":50921,\"start\":50918},{\"end\":51276,\"start\":51273},{\"end\":51289,\"start\":51286},{\"end\":51302,\"start\":51298},{\"end\":51314,\"start\":51310},{\"end\":51324,\"start\":51321},{\"end\":51339,\"start\":51335},{\"end\":51349,\"start\":51347},{\"end\":51363,\"start\":51358},{\"end\":51372,\"start\":51370},{\"end\":51680,\"start\":51677},{\"end\":51695,\"start\":51689},{\"end\":52134,\"start\":52132},{\"end\":52149,\"start\":52144},{\"end\":52164,\"start\":52159},{\"end\":52174,\"start\":52171},{\"end\":52859,\"start\":52856},{\"end\":52869,\"start\":52866},{\"end\":52878,\"start\":52875},{\"end\":53236,\"start\":53224},{\"end\":53247,\"start\":53245},{\"end\":53262,\"start\":53254},{\"end\":53267,\"start\":53264},{\"end\":53604,\"start\":53596},{\"end\":53619,\"start\":53614},{\"end\":53950,\"start\":53943},{\"end\":53967,\"start\":53961},{\"end\":53981,\"start\":53978},{\"end\":53999,\"start\":53990},{\"end\":54017,\"start\":54013},{\"end\":54557,\"start\":54549},{\"end\":54575,\"start\":54566},{\"end\":55030,\"start\":55022},{\"end\":55048,\"start\":55039},{\"end\":55439,\"start\":55432},{\"end\":55458,\"start\":55449},{\"end\":55472,\"start\":55467},{\"end\":55489,\"start\":55483},{\"end\":55505,\"start\":55500},{\"end\":56008,\"start\":56004},{\"end\":56019,\"start\":56012},{\"end\":56032,\"start\":56025},{\"end\":56048,\"start\":56042},{\"end\":56067,\"start\":56058},{\"end\":56075,\"start\":56069},{\"end\":56446,\"start\":56442},{\"end\":56457,\"start\":56453},{\"end\":56471,\"start\":56464},{\"end\":56490,\"start\":56481},{\"end\":56850,\"start\":56846},{\"end\":56861,\"start\":56857},{\"end\":56880,\"start\":56871},{\"end\":56891,\"start\":56888},{\"end\":56903,\"start\":56898},{\"end\":56919,\"start\":56913},{\"end\":57292,\"start\":57288},{\"end\":57301,\"start\":57297},{\"end\":57320,\"start\":57311},{\"end\":57334,\"start\":57327},{\"end\":57763,\"start\":57759},{\"end\":57771,\"start\":57769},{\"end\":57779,\"start\":57777},{\"end\":57793,\"start\":57785},{\"end\":58146,\"start\":58142},{\"end\":58157,\"start\":58153},{\"end\":58165,\"start\":58163},{\"end\":58178,\"start\":58176},{\"end\":58654,\"start\":58650},{\"end\":58669,\"start\":58664},{\"end\":58679,\"start\":58675},{\"end\":58688,\"start\":58684},{\"end\":58699,\"start\":58696},{\"end\":58712,\"start\":58708},{\"end\":58726,\"start\":58718},{\"end\":59021,\"start\":59017},{\"end\":59038,\"start\":59030},{\"end\":59053,\"start\":59048},{\"end\":59065,\"start\":59063},{\"end\":59387,\"start\":59383},{\"end\":59402,\"start\":59397},{\"end\":59639,\"start\":59632},{\"end\":59657,\"start\":59647},{\"end\":59671,\"start\":59663},{\"end\":60164,\"start\":60161},{\"end\":60174,\"start\":60171},{\"end\":60190,\"start\":60185},{\"end\":60202,\"start\":60200},{\"end\":60216,\"start\":60210},{\"end\":60549,\"start\":60545},{\"end\":60565,\"start\":60559},{\"end\":60576,\"start\":60574},{\"end\":60590,\"start\":60585},{\"end\":61361,\"start\":61357},{\"end\":61376,\"start\":61368},{\"end\":61388,\"start\":61383},{\"end\":61406,\"start\":61398},{\"end\":61681,\"start\":61671},{\"end\":61698,\"start\":61693},{\"end\":61711,\"start\":61707}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206596127},\"end\":39221,\"start\":38845},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":225040138},\"end\":39879,\"start\":39223},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":208910380},\"end\":40404,\"start\":39881},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":57246310},\"end\":40841,\"start\":40406},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49319782},\"end\":41497,\"start\":40843},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":208134035},\"end\":42468,\"start\":41499},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":215548929},\"end\":42884,\"start\":42470},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":54463801},\"end\":43253,\"start\":42886},{\"attributes\":{\"id\":\"b8\"},\"end\":43468,\"start\":43255},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":834612},\"end\":44054,\"start\":43470},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":53217392},\"end\":44739,\"start\":44056},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":54465873},\"end\":45077,\"start\":44741},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":45445,\"start\":45079},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":140309863},\"end\":45743,\"start\":45447},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5808102},\"end\":46422,\"start\":45745},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1923924},\"end\":46976,\"start\":46424},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2097418},\"end\":47650,\"start\":46978},{\"attributes\":{\"doi\":\"abs/1908.02486\",\"id\":\"b17\"},\"end\":47953,\"start\":47652},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206592218},\"end\":48449,\"start\":47955},{\"attributes\":{\"doi\":\"abs/1705.06950\",\"id\":\"b19\"},\"end\":48957,\"start\":48451},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5607238},\"end\":49532,\"start\":48959},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":195908774},\"end\":50277,\"start\":49534},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6006618},\"end\":50829,\"start\":50279},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":85542740},\"end\":51186,\"start\":50831},{\"attributes\":{\"id\":\"b24\"},\"end\":51597,\"start\":51188},{\"attributes\":{\"id\":\"b25\"},\"end\":51806,\"start\":51599},{\"attributes\":{\"id\":\"b26\"},\"end\":52046,\"start\":51808},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":51880435},\"end\":52773,\"start\":52048},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6070160},\"end\":53134,\"start\":52775},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10328909},\"end\":53517,\"start\":53136},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":9208396},\"end\":53936,\"start\":53519},{\"attributes\":{\"id\":\"b31\"},\"end\":54473,\"start\":53938},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":11797475},\"end\":54946,\"start\":54475},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14124313},\"end\":55361,\"start\":54948},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206593880},\"end\":55934,\"start\":55363},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1122604},\"end\":56369,\"start\":55936},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":102350405},\"end\":56772,\"start\":56371},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206596999},\"end\":57239,\"start\":56774},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":182952746},\"end\":57692,\"start\":57241},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":19132897},\"end\":58066,\"start\":57694},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":229331798},\"end\":58563,\"start\":58068},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5711057},\"end\":58979,\"start\":58565},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4852647},\"end\":59336,\"start\":58981},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":46940850},\"end\":59544,\"start\":59338},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":6242337},\"end\":60056,\"start\":59546},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":51863579},\"end\":60459,\"start\":60058},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":202775981},\"end\":61308,\"start\":60461},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":22044313},\"end\":61587,\"start\":61310},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":19073975},\"end\":61912,\"start\":61589}]", "bib_title": "[{\"end\":38912,\"start\":38845},{\"end\":39304,\"start\":39223},{\"end\":39936,\"start\":39881},{\"end\":40457,\"start\":40406},{\"end\":40913,\"start\":40843},{\"end\":41610,\"start\":41499},{\"end\":42530,\"start\":42470},{\"end\":42925,\"start\":42886},{\"end\":43558,\"start\":43470},{\"end\":44128,\"start\":44056},{\"end\":44751,\"start\":44741},{\"end\":45123,\"start\":45079},{\"end\":45478,\"start\":45447},{\"end\":45837,\"start\":45745},{\"end\":46485,\"start\":46424},{\"end\":47001,\"start\":46978},{\"end\":48022,\"start\":47955},{\"end\":49009,\"start\":48959},{\"end\":49597,\"start\":49534},{\"end\":50393,\"start\":50279},{\"end\":50891,\"start\":50831},{\"end\":52121,\"start\":52048},{\"end\":52846,\"start\":52775},{\"end\":53214,\"start\":53136},{\"end\":53580,\"start\":53519},{\"end\":54541,\"start\":54475},{\"end\":55014,\"start\":54948},{\"end\":55420,\"start\":55363},{\"end\":55999,\"start\":55936},{\"end\":56437,\"start\":56371},{\"end\":56841,\"start\":56774},{\"end\":57281,\"start\":57241},{\"end\":57751,\"start\":57694},{\"end\":58134,\"start\":58068},{\"end\":58642,\"start\":58565},{\"end\":59006,\"start\":58981},{\"end\":59372,\"start\":59338},{\"end\":59624,\"start\":59546},{\"end\":60151,\"start\":60058},{\"end\":60535,\"start\":60461},{\"end\":61349,\"start\":61310},{\"end\":61656,\"start\":61589}]", "bib_author": "[{\"end\":38929,\"start\":38914},{\"end\":38947,\"start\":38929},{\"end\":39328,\"start\":39306},{\"end\":39344,\"start\":39328},{\"end\":39365,\"start\":39344},{\"end\":39380,\"start\":39365},{\"end\":39391,\"start\":39380},{\"end\":39403,\"start\":39391},{\"end\":39415,\"start\":39403},{\"end\":39952,\"start\":39938},{\"end\":39964,\"start\":39952},{\"end\":39978,\"start\":39964},{\"end\":39993,\"start\":39978},{\"end\":40002,\"start\":39993},{\"end\":40015,\"start\":40002},{\"end\":40469,\"start\":40459},{\"end\":40479,\"start\":40469},{\"end\":40495,\"start\":40479},{\"end\":40506,\"start\":40495},{\"end\":40514,\"start\":40506},{\"end\":40526,\"start\":40514},{\"end\":40925,\"start\":40915},{\"end\":40940,\"start\":40925},{\"end\":40954,\"start\":40940},{\"end\":40977,\"start\":40954},{\"end\":40997,\"start\":40977},{\"end\":41011,\"start\":40997},{\"end\":41025,\"start\":41011},{\"end\":41624,\"start\":41612},{\"end\":41649,\"start\":41624},{\"end\":41663,\"start\":41649},{\"end\":41678,\"start\":41663},{\"end\":41691,\"start\":41678},{\"end\":42557,\"start\":42532},{\"end\":42952,\"start\":42927},{\"end\":42963,\"start\":42952},{\"end\":42979,\"start\":42963},{\"end\":42991,\"start\":42979},{\"end\":43271,\"start\":43257},{\"end\":43294,\"start\":43271},{\"end\":43313,\"start\":43294},{\"end\":43333,\"start\":43313},{\"end\":43351,\"start\":43333},{\"end\":43362,\"start\":43351},{\"end\":43577,\"start\":43560},{\"end\":43589,\"start\":43577},{\"end\":43605,\"start\":43589},{\"end\":43629,\"start\":43605},{\"end\":43644,\"start\":43629},{\"end\":43662,\"start\":43644},{\"end\":43672,\"start\":43662},{\"end\":43690,\"start\":43672},{\"end\":44144,\"start\":44130},{\"end\":44158,\"start\":44144},{\"end\":44170,\"start\":44158},{\"end\":44177,\"start\":44170},{\"end\":44187,\"start\":44177},{\"end\":44199,\"start\":44187},{\"end\":44211,\"start\":44199},{\"end\":44223,\"start\":44211},{\"end\":44765,\"start\":44753},{\"end\":44783,\"start\":44765},{\"end\":44797,\"start\":44783},{\"end\":44814,\"start\":44797},{\"end\":45137,\"start\":45125},{\"end\":45152,\"start\":45137},{\"end\":45166,\"start\":45152},{\"end\":45176,\"start\":45166},{\"end\":45488,\"start\":45480},{\"end\":45497,\"start\":45488},{\"end\":45507,\"start\":45497},{\"end\":45853,\"start\":45839},{\"end\":45872,\"start\":45853},{\"end\":46500,\"start\":46487},{\"end\":46508,\"start\":46500},{\"end\":46519,\"start\":46508},{\"end\":46527,\"start\":46519},{\"end\":47011,\"start\":47003},{\"end\":47031,\"start\":47011},{\"end\":47049,\"start\":47031},{\"end\":47063,\"start\":47049},{\"end\":47730,\"start\":47716},{\"end\":47745,\"start\":47730},{\"end\":47757,\"start\":47745},{\"end\":47765,\"start\":47757},{\"end\":47777,\"start\":47765},{\"end\":48041,\"start\":48024},{\"end\":48058,\"start\":48041},{\"end\":48074,\"start\":48058},{\"end\":48088,\"start\":48074},{\"end\":48106,\"start\":48088},{\"end\":48118,\"start\":48106},{\"end\":48502,\"start\":48492},{\"end\":48517,\"start\":48502},{\"end\":48533,\"start\":48517},{\"end\":48546,\"start\":48533},{\"end\":48561,\"start\":48546},{\"end\":48590,\"start\":48561},{\"end\":48603,\"start\":48590},{\"end\":48614,\"start\":48603},{\"end\":48627,\"start\":48614},{\"end\":48640,\"start\":48627},{\"end\":48658,\"start\":48640},{\"end\":48676,\"start\":48658},{\"end\":49029,\"start\":49011},{\"end\":49047,\"start\":49029},{\"end\":49064,\"start\":49047},{\"end\":49616,\"start\":49599},{\"end\":49632,\"start\":49616},{\"end\":49651,\"start\":49632},{\"end\":50403,\"start\":50395},{\"end\":50414,\"start\":50403},{\"end\":50428,\"start\":50414},{\"end\":50444,\"start\":50428},{\"end\":50448,\"start\":50444},{\"end\":50901,\"start\":50893},{\"end\":50913,\"start\":50901},{\"end\":50923,\"start\":50913},{\"end\":51278,\"start\":51264},{\"end\":51291,\"start\":51278},{\"end\":51304,\"start\":51291},{\"end\":51316,\"start\":51304},{\"end\":51326,\"start\":51316},{\"end\":51341,\"start\":51326},{\"end\":51351,\"start\":51341},{\"end\":51365,\"start\":51351},{\"end\":51374,\"start\":51365},{\"end\":51682,\"start\":51670},{\"end\":51697,\"start\":51682},{\"end\":52136,\"start\":52123},{\"end\":52151,\"start\":52136},{\"end\":52166,\"start\":52151},{\"end\":52176,\"start\":52166},{\"end\":52861,\"start\":52848},{\"end\":52871,\"start\":52861},{\"end\":52880,\"start\":52871},{\"end\":53238,\"start\":53216},{\"end\":53249,\"start\":53238},{\"end\":53264,\"start\":53249},{\"end\":53269,\"start\":53264},{\"end\":53606,\"start\":53582},{\"end\":53621,\"start\":53606},{\"end\":53952,\"start\":53938},{\"end\":53969,\"start\":53952},{\"end\":53983,\"start\":53969},{\"end\":54001,\"start\":53983},{\"end\":54019,\"start\":54001},{\"end\":54559,\"start\":54543},{\"end\":54577,\"start\":54559},{\"end\":55032,\"start\":55016},{\"end\":55050,\"start\":55032},{\"end\":55441,\"start\":55422},{\"end\":55460,\"start\":55441},{\"end\":55474,\"start\":55460},{\"end\":55491,\"start\":55474},{\"end\":55507,\"start\":55491},{\"end\":56010,\"start\":56001},{\"end\":56021,\"start\":56010},{\"end\":56034,\"start\":56021},{\"end\":56050,\"start\":56034},{\"end\":56069,\"start\":56050},{\"end\":56077,\"start\":56069},{\"end\":56448,\"start\":56439},{\"end\":56459,\"start\":56448},{\"end\":56473,\"start\":56459},{\"end\":56492,\"start\":56473},{\"end\":56852,\"start\":56843},{\"end\":56863,\"start\":56852},{\"end\":56882,\"start\":56863},{\"end\":56893,\"start\":56882},{\"end\":56905,\"start\":56893},{\"end\":56921,\"start\":56905},{\"end\":57294,\"start\":57283},{\"end\":57303,\"start\":57294},{\"end\":57322,\"start\":57303},{\"end\":57336,\"start\":57322},{\"end\":57765,\"start\":57753},{\"end\":57773,\"start\":57765},{\"end\":57781,\"start\":57773},{\"end\":57795,\"start\":57781},{\"end\":58148,\"start\":58136},{\"end\":58159,\"start\":58148},{\"end\":58167,\"start\":58159},{\"end\":58180,\"start\":58167},{\"end\":58656,\"start\":58644},{\"end\":58671,\"start\":58656},{\"end\":58681,\"start\":58671},{\"end\":58690,\"start\":58681},{\"end\":58701,\"start\":58690},{\"end\":58714,\"start\":58701},{\"end\":58728,\"start\":58714},{\"end\":59023,\"start\":59008},{\"end\":59040,\"start\":59023},{\"end\":59055,\"start\":59040},{\"end\":59067,\"start\":59055},{\"end\":59389,\"start\":59374},{\"end\":59404,\"start\":59389},{\"end\":59641,\"start\":59626},{\"end\":59659,\"start\":59641},{\"end\":59673,\"start\":59659},{\"end\":60166,\"start\":60153},{\"end\":60176,\"start\":60166},{\"end\":60192,\"start\":60176},{\"end\":60204,\"start\":60192},{\"end\":60218,\"start\":60204},{\"end\":60551,\"start\":60537},{\"end\":60567,\"start\":60551},{\"end\":60578,\"start\":60567},{\"end\":60592,\"start\":60578},{\"end\":61363,\"start\":61351},{\"end\":61378,\"start\":61363},{\"end\":61390,\"start\":61378},{\"end\":61408,\"start\":61390},{\"end\":61683,\"start\":61658},{\"end\":61700,\"start\":61683},{\"end\":61713,\"start\":61700}]", "bib_venue": "[{\"end\":39564,\"start\":39498},{\"end\":40100,\"start\":40084},{\"end\":41942,\"start\":41912},{\"end\":42642,\"start\":42626},{\"end\":46028,\"start\":45975},{\"end\":46664,\"start\":46604},{\"end\":47279,\"start\":47263},{\"end\":49208,\"start\":49171},{\"end\":52316,\"start\":52301},{\"end\":54107,\"start\":54084},{\"end\":55596,\"start\":55578},{\"end\":57485,\"start\":57419},{\"end\":58329,\"start\":58263},{\"end\":60789,\"start\":60759},{\"end\":39011,\"start\":38947},{\"end\":39496,\"start\":39415},{\"end\":40082,\"start\":40015},{\"end\":40606,\"start\":40526},{\"end\":41046,\"start\":41025},{\"end\":41803,\"start\":41691},{\"end\":42624,\"start\":42557},{\"end\":43049,\"start\":42991},{\"end\":43744,\"start\":43690},{\"end\":44379,\"start\":44223},{\"end\":44868,\"start\":44814},{\"end\":45240,\"start\":45176},{\"end\":45571,\"start\":45507},{\"end\":45940,\"start\":45872},{\"end\":46602,\"start\":46527},{\"end\":47175,\"start\":47063},{\"end\":47714,\"start\":47652},{\"end\":48182,\"start\":48118},{\"end\":48490,\"start\":48451},{\"end\":49116,\"start\":49064},{\"end\":49768,\"start\":49651},{\"end\":50512,\"start\":50448},{\"end\":50982,\"start\":50923},{\"end\":51262,\"start\":51188},{\"end\":51668,\"start\":51599},{\"end\":51893,\"start\":51808},{\"end\":52228,\"start\":52176},{\"end\":52934,\"start\":52880},{\"end\":53307,\"start\":53269},{\"end\":53684,\"start\":53621},{\"end\":54082,\"start\":54019},{\"end\":54689,\"start\":54577},{\"end\":55106,\"start\":55050},{\"end\":55576,\"start\":55507},{\"end\":56131,\"start\":56077},{\"end\":56551,\"start\":56492},{\"end\":56985,\"start\":56921},{\"end\":57417,\"start\":57336},{\"end\":57859,\"start\":57795},{\"end\":58261,\"start\":58180},{\"end\":58749,\"start\":58728},{\"end\":59131,\"start\":59067},{\"end\":59425,\"start\":59404},{\"end\":59694,\"start\":59673},{\"end\":60239,\"start\":60218},{\"end\":60650,\"start\":60592},{\"end\":61429,\"start\":61408},{\"end\":61734,\"start\":61713}]"}}}, "year": 2023, "month": 12, "day": 17}