{"id": 248952570, "updated": "2022-10-11 20:30:12.803", "metadata": {"title": "DeepDRiD: Diabetic Retinopathy\u2014Grading and Image Quality Estimation Challenge", "authors": "[{\"first\":\"Ruhan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Xiangning\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Ling\",\"last\":\"Dai\",\"middle\":[]},{\"first\":\"Xi\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Tao\",\"last\":\"Yan\",\"middle\":[]},{\"first\":\"Jaemin\",\"last\":\"Son\",\"middle\":[]},{\"first\":\"Shiqi\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Jiang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Zijian\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Adrian\",\"last\":\"Galdran\",\"middle\":[]},{\"first\":\"J.M.\",\"last\":\"Poorneshwaran\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yerui\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Prasanna\",\"last\":\"Porwal\",\"middle\":[]},{\"first\":\"Gavin\",\"last\":\"Wei Tan\",\"middle\":[\"Siew\"]},{\"first\":\"Xiaokang\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Chao\",\"last\":\"Dai\",\"middle\":[]},{\"first\":\"Haitao\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Mingang\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Huating\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Weiping\",\"last\":\"Jia\",\"middle\":[]},{\"first\":\"Dinggang\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Sheng\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "Patterns", "journal": "Patterns", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Summary We described a challenge named \u201cDiabetic Retinopathy (DR)\u2014Grading and Image Quality Estimation Challenge\u201d in conjunction with ISBI 2020 to hold three sub-challenges and develop deep learning models for DR image assessment and grading. The scientific community responded positively to the challenge, with 34 submissions from 574 registrations. In the challenge, we provided the DeepDRiD dataset containing 2,000 regular DR images (500 patients) and 256 ultra-widefield images (128 patients), both having DR quality and grading annotations. We discussed details of the top 3 algorithms in each sub-challenges. The weighted kappa for DR grading ranged from 0.93 to 0.82, and the accuracy for image quality evaluation ranged from 0.70 to 0.65. The results showed that image quality assessment can be used as a further target for exploration. We also have released the DeepDRiD dataset on GitHub to help develop automatic systems and improve human judgment in DR screening and diagnosis.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "35755875", "pubmedcentral": "9214346", "dblp": "journals/patterns/LiuWWDFYSTLGGPL22", "doi": "10.1016/j.patter.2022.100512"}}, "content": {"source": {"pdf_hash": "aa4878d0122bcd5bbdc1e6d2662a7769521da91c", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBYNCND", "open_access_url": "http://www.cell.com/article/S2666389922001040/pdf", "status": "GOLD"}}, "grobid": {"id": "ed0beba733f1eeb3c1140ad99d92bec2fc5bc70e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/aa4878d0122bcd5bbdc1e6d2662a7769521da91c.txt", "contents": "\nDeepDRiD: Diabetic Retinopathy-Grading and Image Quality Estimation Challenge Graphical abstract Highlights d Provides the DeepDRiD dataset, performance evaluation, top methods and results d Presents deep learning approaches in DR image quality assessment and grading d Discusses the future work of DR automatic screening\n\n\nRuhan Liu \nQiang Wu, ...,Xiangning Wang \nBin ShengDinggang Shen dinggang.shen@gmail.comd.s. \nPing Zhang Correspondence \nDeepDRiD: Diabetic Retinopathy-Grading and Image Quality Estimation Challenge Graphical abstract Highlights d Provides the DeepDRiD dataset, performance evaluation, top methods and results d Presents deep learning approaches in DR image quality assessment and grading d Discusses the future work of DR automatic screening\n10.1016/j.patter.2022.100512Descriptor\nWe described a challenge named ''Diabetic Retinopathy (DR)-Grading and Image Quality Estimation Challenge'' in conjunction with ISBI 2020 to hold three sub-challenges and develop deep learning models for DR image assessment and grading. The scientific community responded positively to the challenge, with 34 submissions from 574 registrations. In the challenge, we provided the DeepDRiD dataset containing 2,000 regular DR images (500 patients) and 256 ultra-widefield images (128 patients), both having DR quality and grading annotations. We discussed details of the top 3 algorithms in each sub-challenges. The weighted kappa for DR grading ranged from 0.93 to 0.82, and the accuracy for image quality evaluation ranged from 0.70 to 0.65. The results showed that image quality assessment can be used as a further target for exploration. We also have released the DeepDRiD dataset on GitHub to help develop automatic systems and improve human judgment in DR screening and diagnosis.fects the working-age population. 1-4 Approximately 600 million people are estimated to have diabetes by 2040, and one-third of them are expected to have\n\nIn brief\n\nIn DeepDRiD challenge, organizers hold a real-world exploration in diabetic retinopathy (DR) auto-screening systems using regular fundus images from 500 participants and ultra-widefield fundus images from 128 participants. Among the 34 participating teams, we summarized the top 3 teams in the three sub-challenges involved in DR grading and image quality assessment. In addition to providing new insights into image quality assessment strategy, these models can enhance the judgment of healthcare workers in DR screening and bring precise screening results.\n\n\nINTRODUCTION\n\nDiabetic retinopathy (DR) is the most common disease caused by diabetes, and it leads to vision loss in adults and mainly af-more retinal lesions, such as microaneurysms, hemorrhages, soft exudates, and hard exudates. 5 An internationally accepted method of grading the DR levels classifies DR into non-proliferative DR (NPDR) and proliferative DR (PDR). 4 NPDR is the early stage of DR and is characterized by the presence of microaneurysms, whereas PDR is an advanced stage of DR and can lead to severe vision loss. The number and degree of retinal lesions vary in different DR grading, and the specific grading standards of NPDR and PDR are listed in Table 1. 4 Furthermore, Figure S1 shows different levels of DR disease presentation.\n\nDR has some severe implications, such as blindness, and its whole population screening is still hampered by several factors. [6][7][8][9][10] First, DR screening places a cumbersome burden on ophthalmologists. Second, healthcare workers are faced with inadequate training, resulting in low-accuracy problems in DR grading. 11 Therefore, computer-aided diagnostic tools are needed to assist manual screening, reducing the burden on ophthalmologists, and helping trained providers to grade fundus images more accurately. [12][13][14][15][16] Recent studies have been conducted to collect raw fundus images and achieve accurate pixel-or image-level expert annotations; 11,[17][18][19] these efforts play an important role in facilitating the research community in developing, validating, and comparing DR gradings. Large numbers of raw fundus images and their corresponding physician annotations have important clinical implications for developing robust automated DR grading models.\n\nIn medical image analysis, grand competitions present substantial opportunities to quickly advance the state-of-the-art methods. Organizers define a clinically relevant task and build a sufficiently large and diverse dataset to allow participants to develop algorithms for solving one or several clinically related problem(s). Moreover, algorithms proposed by participants are consistently evaluated in a fair performance comparison. Many successful challenges have been organized in recent years, specifically in DR fields, i.e., IDRiD, 20 Kaggle 2015, 21 Messidor, 22 Kaggle 2009, 23 ROC, 24 E-Ophtha, 25 and DiaretDB. 26 In 2018, we participated in the ''Diabetic Retinopathy-Segmentation and Grading Challenge'' (IDRiD) to grade DR levels, segment fundus lesions, and locate retinal landmarks (macula and optic disc) in regular fundus images. 20 The development of our automated screening system for DR was further refined during and after the competition. The details of the model development are described in Dai et al.'s work. 11 When developing the system, we found certain problems hindering the practicality of the automatic DR screening system. First, low-quality fundus images due to significant artifacts and poorly lit areas increase training difficulty. Moreover, fundus images from different devices pose a challenge for the stability of automated screening systems. Finally, dual views of regular fundus images are rarely seen in the previous DR challenges. To address all these limitations, we organized ''Diabetic Retinopathy-Grading and Image Quality Estimation Challenge'' (DeepDRiD) in ISBI 2020, and we designed three sub-challenges: (1) regular fundus DR grading for images in different quality, (2) image quality assessment for availability, and (3) ultra-widefield (UWF) DR grading for different device transferring. The following is the setup of our challenge: d In the DeepDRiD, we presented regular fundus photographs for left and right eyes from each patient in dual views (macula-centered and optic-disc-centered) for the system development by participants. d We provided a detailed quality assessment score for each image from the dataset. 11 We also provided a sub-challenge to assess image quality in four aspects: artifact, clarity, field definition, and overall score. d We prepared a dataset from UWF fundus photography, containing one double-view shot per patient. This dataset offered the possibility to develop, validate, and test DR screening systems with multiple devices.\n\nIn this paper, we discuss details of the three best algorithms in each sub-challenge. All participants used convolutional Gavin Siew Wei Tan, 15 Xiaokang Yang, 2 Chao Dai, 16 Haitao Song, 2 Mingang Chen, 17 Huating Li, 18,19, * Weiping Jia, 18,19  Dinggang Shen, 20,21, * Bin Sheng, 1,2,25,26, * and Ping Zhang, 22,23,24 neural networks. Their algorithms differed mainly in terms of the detailed neural network architecture, training strategy, and pre-and post-processing methods. By examining their models, we validate the performance of image quality and DR grading, and we also summarize the relationship between these two tasks.\n\n\nMethods\n\n\nMaterials\n\nIn the DeepDRiD challenge, we included patients from different projects in Shanghai, including participants in the Shanghai Diabetic Complication Screening Project, Nicheng Diabetes Screening Project, and Nation-wide Screening for Complications of Diabetes, for regular fundus images. The other part of fundus images included regular fundus images and UWF retinal images by retinal specialists at the outpatient ophthalmology clinic in the Sixth People's Hospital of Shanghai Jiao Tong University in China. From thousands of examinations available, we randomly selected 2,000 regular fundus images from 500 patients to form the regular fundus dataset. Each patient in the dataset has four fundus images, and each eye has two records, centered on the macula and optic disc. An example patient is shown in Figure S2A. Furthermore, 256 UWF images from another 128 patients formed our UWF dataset. The study was approved by the Ethics Committee of Shanghai Sixth People's Hospital and conducted in accordance with the Declaration of Helsinki. Informed consent was obtained from participants. The study was registered on the Chinese Clinical Trials Registry (ChiCTR.org.cn) under the identifier ChiCTR2000031184.\n\nIn addition to constructing the DeepDRiD dataset, we performed the following procedures to ensure image quality and accuracy of lesion diagnostic labels. Original retinal images were uploaded to the online platform, and the images of each eye were assigned separately to two authorized ophthalmologists. They labeled the images using an online reading platform and gave the image quality assessment scores and graded diagnosis of DR. The third ophthalmologist who served as the senior supervisor confirmed or corrected when the diagnostic results were contradictory. The final grading result was dependent on the consistency within these three ophthalmologists. Clinically, five levels of DR are distinguished, based on the International Clinical DR (ICDR) 4 classification scale: (1) no apparent retinopathy (grade 0), (2) mild NPDR (grade 1), (3) moderate NPDR (grade 2), (4) severe NPDR (grade 3), and (5) PDR (grade 4). Furthermore, the major factors affecting fundus image quality assessment are image artifact, low clarity, and low field definition, as shown in Figure S2B. The specific criteria of image quality assessment and DR grading can be seen in Tables 1 and 2. The DeepDRiD dataset is available to the public (Mendeley Data: https://doi.org/10.5281/zenodo.6452623).\n\nFor regular fundus images, the data were split into 60% for training (Regular Set-A: 300 patients, 1,200 images), 20% for testing (Regular Set-B: 100 patients, 400 images), and 20% for testing (Regular Set-C: 100 patients, 400 images). The UWF data were divided into UWF Set-A (77 patients, 154 images), UWF Set-B (25 patients, 50 images), and UWF Set-C (26 patients, 52 images). Moreover, Set-A and Set-B of regular fundus \n\n\nll\n\n\nOPEN ACCESS\n\nDescriptor images and UWF images were provided to participants in model development, and the Set-C was used as an online validation set to evaluate the final performance. In addition, we provided patient-level DR grading results, including a comprehensive assessment of the DR grading results of both eyes. The distribution of DR severity in regular fundus images dataset (Regular Set-A, Regular Set-B, and Regular Set-C) is shown in Table 3.\n\nIn the 256-image UWF fundus dataset, we collected labeling of DR grading levels according to the ICDR classification scale. The DR grading procedure for ophthalmologists is the same as that of the regular fundus. In this dataset, we only obtained two UWF fundus images from the right and left eye of each patient. We provided DR grading levels for the fundus image of each eye in the UWF image centered on the optic disc. Example figures of UWF fundus in different DR levels are shown in Figure S3. A detailed information of DeepDRiD dataset can be seen in Note S1.\n\n\nChallenge setup\n\nThe DeepDRiD was composed of various stages, giving a wellorganized work process to facilitate the success of contests.  20 we decided to promote the progress further through the second challenge using a new dataset (DeepDRiD). The challenge was subdivided into three tasks as follows:\n\nd Sub-challenge 1: DR disease grading: classification of fundus images according to the severity level of diabetic retinopathy using dual-view retinal fundus images. d Sub-challenge 2: image quality estimation: fundus quality assessment for overall image quality, artifacts, clarity, and field definition. d Sub-challenge 3: UWF fundus DR grading: explore the generalizability of a DR grading system. The robust and generalizable models were expected to be developed to solve practical clinical issues.\n\nWe set up a website to share information about the challenge and provide an interface for all challenge-related issues. The challenge website is accessible directly at https://isbi.deepdr. org. On the website, the participants could register and find a general overview of the challenge, including the deadlines, a brief description of the biomedical background of the problem, a description of the dataset, the rules of the challenge, the evaluation metrics, and Python code snippets for accessing the images and the annotations. Finally, the participants could submit their results and access a forum to ask questions and provide comments through the website. It consisted of an open-testing round (Regular Set-B and UWF Set-B) for teams to refine and calibrate their models, and a final evaluation round (Regular Set-C and UWF Set-C). Participants were granted access to the dataset, forum, and submission system after they registered and accepted the rules of the challenge. Anonymous participation was not allowed. The complete DeepDRiD datasets were shared on GitHub (Mendeley Data: https://doi.org/10.5281/ zenodo.6452623). The challenge aimed for a fair comparison of algorithms. Due to the large size of the public dataset in fundus images, participants were allowed to use other data sources but were required to mention which data they used.\n\nThe participants had to submit their results as CSV files through the challenge website. The deadline for submissions was March 4, 2020. A maximum of three submissions was allowed per participant, with a four-page ISBI style paper accompanying each submission describing their methods. The three submissions had to be methodologically different. Resubmissions with simple hyper-parameter tuning were not allowed. During the workshop at ISBI 2020, we presented the challenge results and invited the top three teams to present their methods. The results, presentations, and algorithms of participants were shared on the challenge website after the workshop. Subsequently, the challenge was reopened for registration and submissions. In submission result analysis, we used quadratic weighted kappa (k u ) as the assessment metric for sub-challenges. Moreover, in sub-challenge 2, the overall quality is evaluated by accuracy. The details of the evaluation method can be seen in supplemental information: evaluation metrics.\n\n\nRESULTS\n\nWe had 574 registered participants before March 1, 2019, when the test dataset was released. The teams explored a wide range of machine learning and deep learning models, ranging from CatBoost, 27 LightGBM, 28 XGboost, 29 VGG, 30 ResNet, 31 SE-ResNeXt, 32 to EfficientNet, 33 and combinations of several types of models. In total, 34 teams submitted their models in our challenge. To help the participating teams avoid overfitting problems, we also provided a separate validation set (Regular Set-B and UWF Set-B) during the competition to help them validate the model results. In Figure 2, we gave the results of three sub-challenges in the rank scores.\n\n\nSummary of competing solutions\n\nWe only present the methodology and results of the top three best-performing algorithms in each sub-challenge to keep the Descriptor paper concise. We discuss the algorithms of the nine teams in terms of the following steps: data preprocessing, data augmentation, model pre-training, and training strategies of classifying deep learning models. We provide a summary and then discuss each of these four steps. The detail models and specific training strategies are shown in Note S2. Moreover, we summarize the commonalities in the good results achieved by these team approaches. These teams all considered the background of medical expertise and considered the diagnostic processes of professional physicians in the preprocessing of the data, the training of the models, and the integration of the final results.\n\nd The improvement of model generalization performance is achieved by pre-training the model with extensive use of routine fundus images published in publicly available datasets and DR grading results from professional physicians. d Considering the task-to-task correlation, knowledge migration from source to target data is utilized, enabling the model to learn important information quickly. d Simultaneous training and integration of multiple models are used to improve the performance and performance of the models using training strategies in the field of deep learning.\n\nThe winning teams in three sub-challenges have different characteristics. In sub-challenge 1 (DR grading using regular fundus), the winning team did not use complex data preprocessing and augmentation operations, but used advanced deep learning training tools from the training means. In sub-challenge 2 (image quality assessment), the winning team used rich data preprocessing and augmentation operations to design the model. The winning team in sub-challenge 3 (UWF DR grading) won the competition by pre-training and knowledge transfer of large-scale data.\n\n\nData preprocessing\n\nWe analyzed different preprocessing steps used in each of three sub-challenges: DR grading based on regular fundus; image quality assessment based on regular fundus; and DR grading based on UWF fundus. In DR severity grading of regular fundus images, public dataset providing large size of regular fundus images and their DR grading results, such as IDRiD, 20 Kaggle 2015, 21 Messidor, 22 Kaggle 2009, 23 ROC, 24 E-Ophtha, 25 DiaretDB, 26 and REFUGE 2, 34 were used. In a previous study, general preprocessing methods were introduced to improve model performance of DR grading. Some teams adopted these preprocessing algorithms, including Ben's preprocessing method, 35 image transformation based on bilinear interpolation, reducing the black edges of fundus images, and so on. In the image quality assessment task based on the regular fundus, the preprocessing algorithms used by participants were fundamentally the same as the DR severity grading task also based on the regular fundus. Moreover, due to difference between the regular fundus and UWF fundus, the preprocessing steps were different in the DR grading task based on regular fundus and UWF fundus. In the UWF fundus, all teams used the center-cut method to cut the edge of the UWF fundus images. For more details, we refer to Table 4. \n\n\nDescriptor\n\n\nData augmentation\n\nThe color distribution of fundus images can influence the robustness of the convolution neural networks (CNNs). Most teams used data augmentation methods, such as color adjustment, mirroring, rotation, and so on, to maintain the generality of CNNs. In sub-challenge 1, two teams adopted the same mirroring method: horizontal flip, vertical flip, and horizontal and vertical flip; they also used rotation augmentation with different rotation angles. As most teams participating in sub-challenges 2 and 3 used the pre-trained network migration based on subchallenge 1, they did not use data augmentation methods. In sub-challenge 2, only the top 1 team used additional color adjustment methods, i.e., CutMix, 36 RICAP, 37 and Mixup. 38 Furthermore, the top 1 team in sub-challenge 3 adopted plentiful augmentation strategies. The results from sub-challenges 2 and 3 show that, although pre-trained network transfer helps the network learn new tasks quickly, the use of data augmentation methods is still helpful in improving network results. Table 5 shows the detailed data augmentation method adopted by nine teams. Model pre-training Many eye diseases are diagnosed based on fundus images. Thus, it is common for public datasets in fundus images to build models for different eye diseases. In our challenge, most teams selected to use model pre-training to improve their model ability. Table 6 shows model pre-training details. Classifying deep learning models In three sub-challenges, all teams used current deep learning models to construct classification frameworks. Most teams adopted EfficientNet 33 as their deep learning backbones, and obtained great model performance, whereas some teams selected classical ResNet 31 and its variant SE-ResNeXt. 32 A team in sub-challenge 2 used a private dataset with regular fundus image and pixel-level structure labels, and selected UNet 39 and VGG 30 as their deep learning classification model. Most teams chose regular classification loss functions, such as cross-entropy loss (CE), L1 loss, and smooth L1 loss. One team in sub-challenge 2 proposed and adopted cost-sensitive loss. 40 The teams that selected different training strategies to develop deep learning models are detailed in Table 7.\n\n\nSolution results\n\nTo fairly evaluate the performance of the individual competition team models, the quadratic weighted kappa score k u was used to rank the algorithms. The k u ranged from 0.9303 to 0.9033 for all nine participating teams in sub-challenge 1, from 0.6981 to 0.6938 for the sub-challenge 2, and from 0.9062 to 0.6437 for sub-challenge 3. In sub-challenge 1, almost all teams achieved good performance (>0.90); in sub-challenge 2, almost all teams achieved unsatisfactory performance (<0.70). This may be partly due to the fact that the teams in the competition did not take into account well the unevenness of the categories, and the relatively small differences between classes that are difficult to extract. In sub-challenge 3, all the teams performed evenly in distribution (0.60-0.90). Correlation between different fundus images was considered, and better accuracy was achieved using a team of transfer learning and sliding window learning. For the scores of the top 3 teams in each sub-challenge, we refer to Table 8. We also give a summary of the participation of these nine teams for all sub-challenges in Table S4. The performances of the proposed methods on the final validation (Set-C) are shown in six subtasks (divided into three sub-challenges). The leaderboard ranks in the three sub-challenges are also illustrated. Sub-challenge 1: DR disease grading This section presents the performance of all competing solutions in the DR grading task using regular fundus pictures. The results received from the participating teams were analyzed using k u as a validation measure. k u was calculated on the validation set (Regular Set-C) for each of the different techniques. Of the 34 participating teams in the challenge, 11 teams participated in sub-challenge 1. Of these 11 teams, 9 (see Table S5) performed well in the DR grading task and then were invited to participate in the challenge workshop. The top three groups were those of Xi Fang, Jiang Li, and Jaemin Son. The classification results of the three teams reflect that all of their models achieved good classification performance, with sensitivity and specificity comparable with physicians on the grading from normal to PDR. In addition, the classification results showed a slightly higher degree of confusion for mild lesions than for moderate and severe lesions. In Note S3, we detail the model performance and result analysis. \n\n\nll\n\n\nOPEN ACCESS\n\nDescriptor Sub-challenge 2: Image quality estimation This task was performed using the validation algorithm described in Note S2 on Set-C to evaluate four aspects of image quality: artifacts, clarity, field definition, and overall quality. The algorithm produced scores for the above four aspects. The best-performing solution in the on-site sub-challenge two was proposed by Poorneshwaran J M, followed by Adrian Galdran and Yerui Chen. For the teams performing poorly in several tasks of image quality detection, their overall accuracy of image quality detection was also not high. The main reason seems to be the inaccurate differentiation of the degree of DR image quality due to the uneven distribution of classes in the dataset and the relatively high degree of similarity between classes. The detailed result can be seen in Note S3.\n\n\nSub-challenge 3: UWF fundus DR grading\n\nThe results for DR grading of UWF fundus images were obtained by the same evaluation method as used for sub-challenge 1 using k u . Table S6 shows the results of the field evaluation, summarizing the performance of all participating algorithms in the UWF fundus DR grading task. Jaemin Son developed the winning method for the UWF fundus DR grading, and Jaemin Son, Xi Fang, and Jie Wang won the best top 3 performers in this task.\n\n\nDISCUSSIONS\n\nSummary of holding and analyzing the challenge In this paper, we present the details of the DeepDRiD challenge, including relevant information regarding the dataset, evaluation metrics for multiple sub-challenges of the competition, the organization of the challenge, solutions, and results by the participating teams on all sub-challenges. The sub-challenges included grading DR severity, quality detection and assessment of fundus photo images, and UWF fundus images DR grading. With 34 teams participating the challenge and reporting the results, we consider our challenge successful. We did our best to create a relevant, stimulating, and fair competition for advancing the collective knowledge of the research community.\n\nThe best methods for DR lesion severity grading used a considerable number of common tips: (1) efficient extraction of features through data augmentation, (2) transfer learning of large amounts of fundus data with and without physician labels, and (3) loss function modification. In addition, many grading networks used the EfficientNet-based framework 33 to learn grading features quickly and efficiently, which improved the performance of the models. The rich parameter adjustment methods and model fusion methods also provided new ideas to further solve the DR grading problem. In the quality assessment task, the accuracy of image quality detection ranged between 0.68 and 0.70. The results did not reach the performance required for clinically feasible automatic screening of good quality fundus images; therefore, there is still much work to do in image quality assessment. Attention must be paid to features of both artifacts and clarity to improve the overall assessment results considering the misclassification cases. In sub-challenge 3, the results of five teams were used for evaluation. We observe that using those readily available regular fundus images for knowledge transfer has a very significant effect on the DR grading task for the same UWF images of the fundus of the eye.\n\n\nLimitations of the study\n\nThis challenge provided data collected in routine clinical practice using an acquisition protocol consistent with all images. The data were acquired with the same camera simultaneously after pupil dilation and followed to provide annotations corresponding to the quality assessment protocol. Several experts jointly evaluated the images in this dataset, and images disagreed by experts were excluded from the dataset. Even after these efforts (for providing the best possible data), the annotation process (especially for image quality) remained inherently subjective.  \n\n\nDescriptor\n\nThus, manual judgment is a limiting factor in the method development, especially for the methods trained and evaluated in a supervised manner. Our challenge provides the potential to develop DR lesion grading solutions, fundus image quality assessment, and DR grading using UWF fundus images. Despite the complexity of the tasks and also just 1.5 months for method development, it still received a very positive response from the community. Nevertheless, there is still room for improvement, especially in evaluation of image quality. Therefore, although the competition is over, the dataset is still publicly available for research purposes, to attract more researchers to study the problem and develop new solutions to meet current and future clinical standards.\n\nInsights on the future directions Based on our analysis of the organization of this challenge and the results from the challenge, we propose the following ideas for future directions. First, almost all teams in this challenge used deep learning models as the main network framework to solve this problem. The results also show that the deep learning models do achieve good results, which demonstrates the great potential of deep models in this problem. Second, in the pretraining of the model, almost all teams used a wide range of general fundus images for model pre-training and parameter migration. This is based on the relatively extensive research interest and a large number of datasets publicly available for this problem on the one hand, and the significant importance of pre-training for model performance improvement on the other hand. Finally, in different subtasks, we can find that the models that achieved victory have different characteristics, some are more focused on preprocessing and augmentation methods and some are more focused on the model architecture and training means. This means that developing models for specific medical problems requires more problem-specific analysis.\n\n\nSuggestions for organizing medical grand challenges\n\nTo help the research community better organize medical grand challenges, we also give a few of our tests. First, the motivation for the challenge needs to come from the clinician's real-world problems. For example, in our challenge, all three subtasks come from the difficulties and challenges encountered in automated deep learning screening during DR screening. In addition, reasonable and compliant access to data prior to organizing the challenge requires that we communicate and collaborate with clinicians as early as possible. Second, the organization, promotion, and conduct of the challenge needed to be as rich in diversity as possible: diversity of competition organizers, diversity of participants, etc. (from different countries and regions, different professional backgrounds, etc.). Finally, a long research base will also help the organizers to better organize the competition and sustainably lead the direction.\n\n\nConclusion\n\nBy leveraging hospital research data and physician resources, we provide a finely labeled dataset of realistic DR screening scenarios that demonstrate the diagnostic potential of the DeepDRiD challenge models on conventional DR grading, DR image quality assessment, and ultra-wide angle fundus DR grading. These models obtained comparable diagnostic performance with general ophthalmologists on DR grading and preliminary attempts on image quality assessment. Furthermore, these new deep learning prediction models and their training strategies can be used to enhance the diagnostic capabilities of healthcare workers to improve the accuracy of DR screening in true screening scenarios. Nevertheless, there is still a clear opportunity to further improve the models in this competition. We believe that, with access to higher quality and more comprehensive image quality assessment data, as well as a wider range of challenge participants, more accurate models could be developed.  \n\n\nEvaluation metrics\n\nFor DR disease-grading tasks, in sub-challenges 1 and 3, the quadratic weighted kappa (k u ) was used as the evaluation metric to determine the performance of the participating algorithms. Submissions were scored based on the quadratic weighted kappa, k u , which measures the agreement between two ratings (ground-truths results and submitted results). This metric varied from 0 (random agreement between raters) to 1 (complete agreement between raters). If there was less agreement between the raters than expected by chance, the metric could go below 0. The quadratic weighted kappa, k u , was calculated between the scores, which were expected/known, and the predicted scores. The results had five possible ratings: 0, 1, 2, 3, and 4. The quadratic weighted kappa was calculated as follows. First, an N3N histogram matrix, O, was constructed, such that it corresponded to the number of adoption records that had a rating of i (actual) and received a predicted rating, j. An N3 N matrix of weights, w, was calculated based on the difference between the actual and predicted rating scores. An N3N histogram matrix of expected ratings, E, was calculated, assuming no correlation between rating scores. This was calculated as the outer product between the actual rating's histogram vector of ratings and the predicted rating's histogram vector of ratings, normalized such that E and O had the same sum. From these three matrices, the quadratic weighted kappa was calculated. The k u metric is expressed as k w = 1 \u00c0 The weight penalization, w i;j , is defined by w i;j = \u00f0i \u00c0 j\u00de n \u00f0C \u00c0 1\u00de n , where C is the number of classes. The values of n = 1 and n = 2 lead to linear and quadratic penalizations, respectively. The values of k u is in the interval of k w\u02db\u00bd \u00c0 1; 1, where \u00c01 means perfect symmetric disagreement and 1 means perfect agreement.\n\nIn sub-challenge 2, the scoring metric was classification accuracy, as described as  \n\n\nFigure 1 depicts the workflow of the overall organization of the challenge. The challenge was officially announced on the ISBI 2020 website on October 25, 2019. Following the DR challenge held with ISBI in 2018,\n\nFigure 1 .\n1Workflow of the ISBI 2020: Diabetic Retinopathy-Grading and Image Quality Estimation Challenge ll OPEN ACCESS\n\nFigure 2 .\n2Bar chart for leaderboard in three sub-challengesThe colored bars indicate the top three teams in each challenge.\n\n\nflip; V, vertical flip; HV, horizontal and vertical flip; R, min degree, max degree:rotation angle; ID, image disturbance; N, noise; R, resize; ET, elastic transformation; GT, grid transformation; AT, affine transformation; RCC, random center cut; CM,36 RC,37 and MU,38 preprocessing method in reference; RK, rank.\n\n\nP i;j w i;j ,O i;j P i;j w i;j ,E i;j : (Equation 1)\n\n\nis true positive samples, FP is false positive samples, and N = TP + FP + TN + FN is the total numbers. SUPPLEMENTAL INFORMATION Supplemental information can be found online at https://doi.org/10.1016/j. patter.2022.100512. ACKNOWLEDGMENTS This work was supported by the National Natural Science Foundation of China (61872241, 81870598, and 82022012); the Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102); the College-level Project Fund of Shanghai Jiao Tong University Affiliated Sixth People's Hospital (ynlc201723 and ynlc201909); and the Medical-industrial Cross-fund of Shanghai Jiao Tong University (YG2022QN089). AUTHOR CONTRIBUTIONS Conceptualization, R.L., B.S., and P.Z.; resources, X.W., Q.W., H. Li, and W.J.; methodology, R.L., X.W., L.D., X.F., T.Y., J.S., S.T., J.L., Z.G., A.G., P.J.M., H. Liu, J.W., and Y.C.; software, C.D., H.S., and M.C.; formal analysis, X.W., Q.W., H. Li, P.P., G.S.W.T., X.Y., D.S., and W.J.; writing -original draft, R.L. and P.Z.; writing -review & editing, X.W., Q.W., B.S., H. Li, P.Z., and W.J.; supervision, B.S., H. Li, P.Z., D.S., and W.J.; funding acquisition, X.W., H. Li, and B.S.\n\nTable 2 .\n2Image quality scoring criteriaType \nImage quality specification \nScore \n\nArtifact \nno artifacts \n0 \n\nartifacts are outside the aortic arch with \nscope less than \u00bc of the image \n\n1 \n\nartifacts do not affect the macular area \nwith range less than \u00bc \n\n4 \n\nartifacts cover more than \u00bc but less than \n\u00bd of the image \n\n6 \n\nartifacts cover more than \u00bd without fully \ncovering the posterior pole \n\n8 \n\ncover the entire posterior pole \n10 \n\nClarity \nclarity only level I vascular arch is visible \n1 \n\nlevel II vascular arch and a small number \nof lesions are visible \n\n4 \n\nlevel III vascular arch and some lesions \nare visible \n\n6 \n\nlevel III vascular arch and most lesions \nare visible \n\n8 \n\nlevel III vascular arch and all lesions are \nvisible \n\n10 \n\nField \ndefinition \n\nfield definition do not include the optic disc \nand macula \n\n1 \n\nonly contain either optic disc or macula \n4 \n\ncontain optic disc and macula \n6 \n\nthe optic disc or macula is outside the 1 \npapillary diameter and within the 2 papillary \n\n8 \n\ndiameter range of the center \n\nthe optic disc and macula are within 1 \npapillary diameter of the center \n\n10 \n\nOverall \nquality \n\nquality is not good enough for the diagnosis \nof retinal diseases \n\n0 \n\nquality is good enough for the diagnosis of \nretinal diseases \n\n1 \n\n\n\nTable 3 .\n3Basic characteristics of the patients in DeepDRiD dataset (mean \u00b1 SD)DR levels \n\nRegular fundus \nUWF fundus \n\nSet-A \nSet-B \nSet-C \nSet-A \nSet-B \nSet-C \n\nNo. of images \n1,200 \n400 \n400 \n77 \n25 \n26 \n\nNo. of participants \n300 \n100 \n100 \n154 \n50 \n52 \n\nMale (%) \n51.00 \n44.00 \n46.00 \n54.55 \n57.69 \n48.00 \n\nAge (years) \n70.63 \u00b1 7.70 \n65.13 \u00b1 1.89 \n61.36 \u00b1 7.23 \n74.64 \u00b1 4.86 \n64.96 \u00b1 1.71 \n58.28 \u00b1 4.88 \n\nBMI (kg m \u00c02 ) \n25.17 \u00b1 3.13 \n24.88 \u00b1 3.21 \n25.01 \u00b1 2.58 \n24.90 \u00b1 2.89 \n25.19 \u00b1 2.61 \n24.06 \u00b1 3.30 \n\nWaist (cm) \n90.15 \u00b1 9.24 \n88.36 \u00b1 9.75 \n88.03 \u00b1 8.87 \n88.43 \u00b1 9.07 \n92.00 \u00b1 8.86 \n84.73 \u00b1 7.55 \n\nll \n\nOPEN ACCESS \n\n\n\nTable 4 .\n4Differences in preprocessingRK \nCut \nColor \nResize \nFilling \n\nSub-challenge 1: DR grading \n\n1 \nN \nN \nN \nN \n\n2 \nblack edge \nBen's 35 \nBi (512) \nN \n\n3 \nblack edge \nN \nBi (1,024) \nN \n\nSub-challenge 2: image quality assessment \n\n1 \nN \nN \nBi (512) \nN \n\n2 \nN \nN \nN \nN \n\n3 \nblack edge \nN \nN \nflip \n\nSub-challenge 3: DR grading based on UWF fundus \n\n1 \ncenter \nN \nN \nN \n\n2 \ncenter \nN \nN \nN \n\n3 \nN \nN \nN \nN \n\nBlack edge, cut the black edges in the fundus; center, preserve the center \nof the image as input; Ben's, Ben's preprocessing algorithm; 35 Bi(i), use \nbilinear interpolation to resize the fundus image to i pixels size; flip, use \na symmetrical flip pattern to fill the black edges; N, never use this strat-\negy; RK, rank. \n\nll \n\nOPEN ACCESS \n\n\n\nTable 7 .\n7Differences in deep learning models SL1, smooth L1 loss; CE, cross-entropy loss; DV, dual view loss; PL, patient-level loss; CS, cost-sensitive loss; 40 L1, L1 loss; CE(5 class), mean loss of 5 class (one versus others); MMoE, multi-gate mixture of expert; 41 GMP, generalized mean pooling; 42 OHEM, online hard example mining; 43,44 CV, cross-validation; O, oversampling; ES, early stopping; TL, transfer learning; TTA, test time augmentation; 45,46 PLT, pseudo-labeled and labeled training.RK Model frameworks Loss function \nTraining strategies \n\nSub-challenge 1: DR grading \n\n1 \nEfficientNet 33 \nSL1 \nMMoE + GMP + ES + \nOHEM + CV + O + T \n\n2 \nEfficientNet 33 \nSL1 + CE + DV + \nPL \n\nCV + TTA \n\n3 \nEfficientNet 33 \nL1 + CE(5 class) PLT \n\nSub-challenge 2: image quality assessment \n\n1 \nSE-ResNeXt 32 \nCE \nTL \n\n2 \nResNet 31 \nCS + L1 \nTL \n\n3 \nVGG, 30 UNet 39 \nCE \nTL \n\nSub-challenge 3: DR grading based on UWF fundus \n\n1 \nEfficientNet 33 \nL1 + CE(5 class) PLT \n\n2 \nEfficientNet 33 \nSL1 \nMMoE + GMP + ES + \nOHEM + CV + O + T \n\n3 \nEfficientNet 33 \nCE \nTL \n\n\n\nTable 6 .\n6Differences in model pre-trainingThe public datasets used are Kaggle2015, 21 APTOS. 47 Labeled: Kag-gle2015,21 APTOS, 47 and IDRiD; 20 unlabeled: REFUGE,34 MESSIOR,22 and E-ophtha.25 RK, rank. Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Bin Sheng (shengbin@sjtu.edu.cn).Materials availabilityThis study did not generate any new materials.Data and code availabilityThe DeepDRiD dataset is available at https://github.com/deepdrdoc/ DeepDRiD (Mendeley Data: https://doi.org/10.5281/zenodo.6452623).RK \nPre-training dataset \n\nSub-challenge 1: DR grading \n\n1 \nKaggle2015 + APTOS \n\n2 \nKaggle2015 + APTOS \n\n3 \nlabeled and unlabeled dataset \n\nSub-challenge 2: image quality assessment \n\n1 \nImageNet \n\n2 \nKaggle2015 \n\n3 \nprivate fundus lesion segmentation data \n\nSub-challenge 3: DR grading based on UWF fundus \n\n1 \nlabeled and unlabeled \n\n2 \nKaggle2015 + AOTOS \n\n3 \nN \n\nll \n\nOPEN ACCESS \n\n\n\nTable 8 .\n8DeepDRiD online leaderboard 44. Shrivastava, A., Gupta, A., and Girshick, R.B. (2016). Training regionbased object detectors with online hard example mining. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 761-769. 45. Bahat, Y., and Shakhnarovich, G. (2020). Classification confidence estimation with test-time data-augmentation. Preprint at arXiv. https://doi. org/10.48550/arXiv.2006.16705. 46. Kandel, I., and Castelli, M. (2021). Improving convolutional neural networks performance for image classification using test time augmentation: a case study using MURA dataset. Health Inf. Sci. Syst. 9, 33. 47. APTOS. (2019). The 4th asia pacific tele-ophthalmology society (aptos) symposium. Available: https://www.kaggle.com/c/aptos2019-blindnessdetectionRank Team \nAffiliation \nScore \n\nSub-challenge 1 \n\n1 \nXi Fang et al. \nShanghai Jiao Tong University 0.9303 \n\n2 \nJiang Li et al. \nShanghai Jiao Tong University 0.9262 \n\n3 \nJaemin Son et al. VUNO Inc. \n0.9232 \n\nSub-challenge 2 \n\n1 \nPoorneshwaran \nJ M et al. \n\nHealthcare Technology \nInnovation Center \n\n0.6981 \n\n2 \nAdrian Galdran \net al. \n\nBournemouth University \n0.6950 \n\n3 \nYerui Chen et al. \nNanjing University of Science \nand Technology \n\n0.6938 \n\nSub-challenge 3 \n\n1 \nJaemin Son et al. VUNO Inc. \n0.9062 \n\n2 \nXi Fang et al. \nShanghai Jiao Tong University 0.8620 \n\n3 \nJie Wang et al. \nBeihang University \n0.8230 \n\nPatterns 3, 100512, June 10, 2022\nPatterns 3, 100512, June 10, 2022 3\nPatterns 3, 100512, June 10, 2022\nPatterns 3, 100512, June 10, 2022 5\nPatterns 3, 100512, June 10, 2022\nPatterns 3, 100512, June 10, 2022 7\nPatterns 3, 100512, June 10, 2022\nPatterns 3, 100512, June 10, 2022 11\nDECLARATION OF INTERESTSThe authors declare no competing interests.INCLUSION AND DIVERSITYWe worked to ensure gender balance in the recruitment of human subjects. We worked to ensure ethnic or other types of diversity in the recruitment of human subjects. The author list of this paper includes contributors from the location where the research was conducted who participated in the data collection, design, analysis, organization, and participation of the challenge.\nI D F D Atlas, International Diabetes federation (Int. Diabet. Federat. (IDF)). Brussels, belgiumAtlas, I.D.F.D. (2017). Brussels, belgium: International Diabetes federation (Int. Diabet. Federat. (IDF)).\n\nEffectiveness of screening and monitoring tests for diabetic retinopathy : a systematic review. A Hutchinson, A Mcintosh, J Peters, C O&apos;keeffe, K Khunti, R Baker, A Booth, Diabet. Med. 17Hutchinson, A., McIntosh, A., Peters, J., O'keeffe, C., Khunti, K., Baker, R., and Booth, A. (2000). Effectiveness of screening and monitoring tests for diabetic retinopathy : a systematic review. Diabet. Med. 17, 495-506.\n\nDiabetic retinopathy screening. Managing Diabetic Eye Disease in Clinical Practice. E Reichel, D Salz, Reichel, E., and Salz, D. (2015). Diabetic retinopathy screening. Managing Diabetic Eye Disease in Clinical Practice, pp. 25-38.\n\nPrevention of blindness from diabetes mellitus. W H Organization, Report of a WHO consultation. Geneva, Switzerland: WHO)Organization, W.H. (2005). Prevention of blindness from diabetes mellitus. In Report of a WHO consultation (Geneva, Switzerland: WHO), pp. 1-48.\n\nDiabetic retinopathy: pathophysiology and treatment. W Wei, Acy , L , Int. J. Mol. Sci. 191816Wei, W., and ACY, L. (2018). Diabetic retinopathy: pathophysiology and treatment. Int. J. Mol. Sci. 19, 1816.\n\nPrevalence of diabetic retinopathy in type 2 diabetes in developing and developed countries. L M Ruta, D J Magliano, R Lemesurier, H R Taylor, P Z Zimmet, J E Shaw, Diabet. Med. 30Ruta, L.M., Magliano, D.J., Lemesurier, R., Taylor, H.R., Zimmet, P.Z., and Shaw, J.E. (2013). Prevalence of diabetic retinopathy in type 2 diabetes in developing and developed countries. Diabet. Med. 30, 387-398.\n\nPrevalence of complications among Chinese diabetic patients in urban primary care clinics: a cross-sectional study. K Kung, K M Chow, E M T Hui, M Leung, S Y Leung, C C Szeto, A Lam, P K T Li, BMC Prim. Care. 15Kung, K., Chow, K.M., Hui, E.M.T., Leung, M., Leung, S.Y., Szeto, C.C., Lam, A., and Li, P.K.T. (2014). Prevalence of complications among Chinese diabetic patients in urban primary care clinics: a cross-sectional study. BMC Prim. Care 15, 8.\n\nPrevalence and risk factors of diabetes and diabetic retinopathy in liaoning province, China: a population-based cross-sectional study. Y Hu, W Teng, L Liu, K Chen, L Liu, R Hua, J Chen, Y Zhou, Chen , L , PLoS One. 10Hu, Y., Teng, W., Liu, L., Chen, K., Liu, L., Hua, R., Chen, J., Zhou, Y., and Chen, L. (2015). Prevalence and risk factors of diabetes and diabetic reti- nopathy in liaoning province, China: a population-based cross-sectional study. PLoS One 10, e0121477.\n\nDetermination of diabetic retinopathy prevalence and associated risk factors in Chinese diabetic and pre-diabetic subjects: Shanghai diabetic complications study. C Pang, L Jia, S Jiang, W Liu, X Hou, Y Zuo, H Gu, Y Bao, Q Wu, K Xiang, Diabetes Metab. Res. Rev. 28Pang, C., Jia, L., Jiang, S., Liu, W., Hou, X., Zuo, Y., Gu, H., Bao, Y., Wu, Q., Xiang, K., et al. (2012). Determination of diabetic retinopathy prevalence and associated risk factors in Chinese diabetic and pre-diabetic subjects: Shanghai diabetic complications study. Diabetes Metab. Res. Rev. 28, 276-283.\n\nSystematic screening for diabetic retinopathy (dr) in Hong Kong: prevalence of dr and visual impairment among diabetic population. J X Lian, R A Gangwani, S M Mcghee, C K W Chan, C L K Lam, Br. J. Ophthalmol. Wong, D.S.H.100Primary Health Care GroupLian, J.X., Gangwani, R.A., McGhee, S.M., Chan, C.K.W., Lam, C.L.K.; Primary Health Care Group, and Wong, D.S.H. (2016). Systematic screening for diabetic retinopathy (dr) in Hong Kong: prevalence of dr and visual impairment among diabetic population. Br. J. Ophthalmol. 100, 151-155.\n\nA deep learning system for detecting diabetic retinopathy across the disease spectrum. L Dai, L Wu, H Li, C Cai, Q Wu, H Kong, R Liu, X Wang, X Hou, Y Liu, Nat. Commun. 123242Dai, L., Wu, L., Li, H., Cai, C., Wu, Q., Kong, H., Liu, R., Wang, X., Hou, X., Liu, Y., et al. (2021). A deep learning system for detecting diabetic retinop- athy across the disease spectrum. Nat. Commun. 12, 3242.\n\nDevelopment and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. V Gulshan, L Peng, M Coram, M C Stumpe, D Wu, A Narayanaswamy, S Venugopalan, K Widner, T Madams, J Cuadros, JAMA. 316Gulshan, V., Peng, L., Coram, M., Stumpe, M.C., Wu, D., Narayanaswamy, A., Venugopalan, S., Widner, K., Madams, T., Cuadros, J., et al. (2016). Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA 316, 2402-2410.\n\nDevelopment and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. D S W Ting, C Y L Cheung, G Lim, G S W Tan, N D Quang, A Gan, H Hamzah, R Garcia-Franco, I Y San Yeo, S Y Lee, Facial Plast. Surg. Aesthet. Med. 318Ting, D.S.W., Cheung, C.Y.L., Lim, G., Tan, G.S.W., Quang, N.D., Gan, A., Hamzah, H., Garcia-Franco, R., San Yeo, I.Y., Lee, S.Y., et al. (2017). Development and validation of a deep learning system for dia- betic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. Facial Plast. Surg. Aesthet. Med. 318, 2211-2223.\n\nValidation of automated screening for referable diabetic retinopathy with the idx-dr device in the hoorn diabetes care system. A A Van Der Heijden, M D Abramoff, F Verbraak, M V Van Hecke, A Liem, G Nijpels, Acta Ophthalmol. 96van der Heijden, A.A., Abramoff, M.D., Verbraak, F., van Hecke, M.V., Liem, A., and Nijpels, G. (2018). Validation of automated screening for referable diabetic retinopathy with the idx-dr device in the hoorn diabetes care system. Acta Ophthalmol. 96, 63-68.\n\nAn automated grading system for detection of vision-threatening referable diabetic retinopathy on the basis of color fundus photographs. Z Li, S Keel, C Liu, Y He, W Meng, J Scheetz, P Y Lee, J Shaw, D Ting, T Y Wong, Diabetes Care. 41Li, Z., Keel, S., Liu, C., He, Y., Meng, W., Scheetz, J., Lee, P.Y., Shaw, J., Ting, D., Wong, T.Y., et al. (2018). An automated grading system for detec- tion of vision-threatening referable diabetic retinopathy on the basis of co- lor fundus photographs. Diabetes Care 41, 2509-2516.\n\nGlycemic exposure and blood pressure influencing progression and remission of diabetic retinopathy: a longitudinal cohort study in godarts. Y Liu, M Wang, A D Morris, A S F Doney, G P Leese, E R Pearson, C N A Palmer, Diabetes Care. 36Liu, Y., Wang, M., Morris, A.D., Doney, A.S.F., Leese, G.P., Pearson, E.R., and Palmer, C.N.A. (2013). Glycemic exposure and blood pressure influ- encing progression and remission of diabetic retinopathy: a longitudinal cohort study in godarts. Diabetes Care 36, 3979-3984.\n\nDR | GRADUATE: uncertainty-aware deep learning-based diabetic retinopathy grading in eye fundus images. T Ara\u00fa Jo, G Aresta, L Mendon\u00e7 A, S Penas, C Maia, \u00c2 Carneiro, A M Mendon\u00e7 A, A Campilho, Med. Image Anal. 63101715Ara\u00fa jo, T., Aresta, G., Mendon\u00e7 a, L., Penas, S., Maia, C., Carneiro, \u00c2 ., Mendon\u00e7 a, A.M., and Campilho, A. (2020). DR | GRADUATE: uncer- tainty-aware deep learning-based diabetic retinopathy grading in eye fundus images. Med. Image Anal. 63, 101715.\n\nCABNet: category attention block for imbalanced diabetic retinopathy grading. A He, T Li, N Li, K Wang, H Fu, IEEE Trans. Med. Imag. 40He, A., Li, T., Li, N., Wang, K., and Fu, H. (2021). CABNet: category atten- tion block for imbalanced diabetic retinopathy grading. IEEE Trans. Med. Imag. 40, 143-153.\n\nEffect of quercetin on the in vitro Tartary buckwheat starch digestibility. Y Zhou, Q Jiang, S Ma, X Zhou, L Shao, Int. J. Biol. Macromol. 183Zhou, Y., Jiang, Q., Ma, S., Zhou, X., and Shao, L. (2021). Effect of quer- cetin on the in vitro Tartary buckwheat starch digestibility. Int. J. Biol. Macromol. 183, 818-830.\n\nIdrid: diabetic retinopathy -segmentation and grading challenge. P Porwal, S Pachade, M Kokare, G Deshmukh, J Son, W Bae, L Liu, J Wang, X Liu, L Gao, Med. Image Anal. 59101561Porwal, P., Pachade, S., Kokare, M., Deshmukh, G., Son, J., Bae, W., Liu, L., Wang, J., Liu, X., Gao, L., et al. (2020). Idrid: diabetic retinopathy -seg- mentation and grading challenge. Med. Image Anal. 59, 101561.\n\nDiabetic retinopathy detection. Eyepacs, EyePACS. (2015). Diabetic retinopathy detection. Available. https://www. kaggle.com/c/diabetic-retinopathy-detection/. July 28th, 2015.\n\nFeedback on a publicly distributed image database: the messidor database. E Decenci\u00e8 Re, X Zhang, G Cazuguel, B Lay, B Cochener, C Trone, P Gain, R Ordonez, P Massin, A Erginay, Charton , B , Image Anal. Stereol. 33Decenci\u00e8 re, E., Zhang, X., Cazuguel, G., Lay, B., Cochener, B., Trone, C., Gain, P., Ordonez, R., Massin, P., Erginay, A., and Charton, B. (2014). Feedback on a publicly distributed image database: the messidor data- base. Image Anal. Stereol. 33, 231-234.\n\nEyePACS: an adaptable telemedicine system for diabetic retinopathy screening. J Cuadros, G Bresnick, J. Diabetes Sci. Technol. 3Cuadros, J., and Bresnick, G. (2009). EyePACS: an adaptable telemedi- cine system for diabetic retinopathy screening. J. Diabetes Sci. Technol. 3, 509-516.\n\nRetinopathy online challenge: automatic detection of microaneurysms in digital color fundus photographs. M Niemeijer, B Van Ginneken, M J Cree, A Mizutani, G Quellec, C I Sanchez, B Zhang, R Hornero, M Lamard, C Muramatsu, IEEE Trans. Med. Imag. 29Niemeijer, M., van Ginneken, B., Cree, M.J., Mizutani, A., Quellec, G., Sanchez, C.I., Zhang, B., Hornero, R., Lamard, M., Muramatsu, C., et al. (2010). Retinopathy online challenge: automatic detection of microaneur- ysms in digital color fundus photographs. IEEE Trans. Med. Imag. 29, 185-195.\n\nTeleophta: machine learning and image processing methods for teleophthalmology. E Decenci\u00e8 Re, G Cazuguel, X Zhang, G Thibault, J C Klein, F Meyer, B Marcotegui, G Quellec, M Lamard, R Danno, Irbm. 34Decenci\u00e8 re, E., Cazuguel, G., Zhang, X., Thibault, G., Klein, J.C., Meyer, F., Marcotegui, B., Quellec, G., Lamard, M., Danno, R., et al. (2013). Teleophta: machine learning and image processing methods for teleoph- thalmology. Irbm 34, 196-203.\n\nA framework for constructing benchmark databases and protocols for retinopathy in medical image analysis. T Kauppi, J K Kamarainen, L Lensu, V Kalesnykiene, I Sorri, H Uusitalo, H Ainen, IScIDE'12 Proceedings of the third Sino-foreign-interchange conference on Intell. Sci. Intell. Data Eng. Kauppi, T., Kamarainen, J.K., Lensu, L., Kalesnykiene, V., Sorri, I., Uusitalo, H., and K\u20ac alvi\u20ac ainen, H. (2012). A framework for constructing benchmark databases and protocols for retinopathy in medical image analysis. In IScIDE'12 Proceedings of the third Sino-foreign-interchange conference on Intell. Sci. Intell. Data Eng., pp. 832-843.\n\nCatBoost: gradient boosting with categorical features support. A V Dorogush, V Ershov, A Gulin, 10.48550/arXiv.1810.11363Preprint at arXivDorogush, A.V., Ershov, V., and Gulin, A. (2018). CatBoost: gradient boost- ing with categorical features support. Preprint at arXiv. https://doi.org/10. 48550/arXiv.1810.11363.\n\nLightGBM: a highly efficient gradient boosting decision tree. G Ke, Proc. Neurips. NeuripsKe, G., et al. (2017). LightGBM: a highly efficient gradient boosting deci- sion tree. In Proc. Neurips, pp. 3146-3154.\n\nXGBoost: a scalable tree boosting system. T Chen, C Guestrin, Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. the 22nd acm sigkdd international conference on knowledge discovery and data miningChen, T., and Guestrin, C. (2016). XGBoost: a scalable tree boosting sys- tem. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785-794.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, 10.48550/arXiv.1409.1556Preprint at arXivSimonyan, K., and Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. Preprint at arXiv. https://doi.org/10. 48550/arXiv.1409.1556.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778.\n\nSqueeze-andexcitation networks. J Hu, L Shen, S Albanie, G Sun, E Wu, IEEE Trans. Pattern Anal. Mach. Intell. 42Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2020). Squeeze-and- excitation networks. IEEE Trans. Pattern Anal. Mach. Intell. 42, 2011-2023.\n\nEfficientNet: rethinking model scaling for convolutional neural networks. M Tan, Q V Le, Proc. ICML. ICMLTan, M., and Le, Q.V. (2019). EfficientNet: rethinking model scaling for con- volutional neural networks. In Proc. ICML, pp. 6105-6114.\n\nRetinal Fundus Glaucoma Challenge Edition 2. 2Lima, PeruREFUGE2 (2020). Retinal Fundus Glaucoma Challenge Edition 2. Lima, Peru. https://refuge.grand-challenge.org/.\n\nSpatially-sparse convolutional neural networks. B Graham, 10.48550/arXiv.1409.6070Preprint at arXivGraham, B. (2014). Spatially-sparse convolutional neural networks. Preprint at arXiv. https://doi.org/10.48550/arXiv.1409.6070.\n\nCutMix: regularization strategy to train strong classifiers with localizable features. S Yun, D Han, S Chun, S J Oh, Y Yoo, J Choe, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionYun, S., Han, D., Chun, S., Oh, S.J., Yoo, Y., and Choe, J. (2019). CutMix: regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6022-6031.\n\nRICAP: random image cropping and patching data augmentation for deep cnns. R Takahashi, T Matsubara, K Uehara, Proc. ACML, J. Zhu and I. TakeuchiTakahashi, R., Matsubara, T., and Uehara, K. (2018). RICAP: random im- age cropping and patching data augmentation for deep cnns. In Proc. ACML, J. Zhu and I. Takeuchi, eds., pp. 786-798.\n\nmixup: beyond empirical risk minimization. H Zhang, M Ciss\u00e9, Y N Dauphin, D Lopez-Paz, Proc. ICLR. ICLRZhang, H., Ciss\u00e9 , M., Dauphin, Y.N., and Lopez-Paz, D. (2018). mixup: beyond empirical risk minimization. In Proc. ICLR.\n\nU-Net: convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, In Diabet. Foot. Ulcers. Grand. Chall. Ronneberger, O., Fischer, P., and Brox, T. (2015). U-Net: convolutional networks for biomedical image segmentation. In Diabet. Foot. Ulcers. Grand. Chall. (2021), pp. 234-241.\n\nCost-sensitive regularization for diabetic retinopathy grading from eye fundus images. A Galdran, J Dolz, H Chakor, H Lombaert, Ben Ayed, I , In Comput. Diffus. MRI. Galdran, A., Dolz, J., Chakor, H., Lombaert, H., and Ben Ayed, I. (2020). Cost-sensitive regularization for diabetic retinopathy grading from eye fundus images. In Comput. Diffus. MRI. (2019), pp. 665-674.\n\nModeling task relationships in multi-task learning with multi-gate mixture-of-experts. J Ma, Z Zhao, X Yi, J Chen, L Hong, Chi , E H , Proc. KDD. KDDMa, J., Zhao, Z., Yi, X., Chen, J., Hong, L., and Chi, E.H. (2018). Modeling task relationships in multi-task learning with multi-gate mixture-of-ex- perts. In Proc. KDD, pp. 1930-1939.\n\nFine-tuning CNN image retrieval with no human annotation. F Radenovic, G Tolias, O Chum, IEEE Trans. Pattern Anal. Mach. Intell. 41Radenovic, F., Tolias, G., and Chum, O. (2019). Fine-tuning CNN image retrieval with no human annotation. IEEE Trans. Pattern Anal. Mach. Intell. 41, 1655-1668.\n\nFaceNet: a unified embedding for face recognition and clustering. F Schroff, D Kalenichenko, J Philbin, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSchroff, F., Kalenichenko, D., and Philbin, J. (2015). FaceNet: a unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 815-823.\n", "annotations": {"author": "[{\"end\":335,\"start\":325},{\"end\":365,\"start\":336},{\"end\":417,\"start\":366},{\"end\":444,\"start\":418}]", "publisher": null, "author_last_name": "[{\"end\":334,\"start\":331},{\"end\":364,\"start\":360},{\"end\":388,\"start\":384},{\"end\":443,\"start\":429}]", "author_first_name": "[{\"end\":330,\"start\":325},{\"end\":359,\"start\":350},{\"end\":383,\"start\":375},{\"end\":422,\"start\":418},{\"end\":428,\"start\":423}]", "author_affiliation": null, "title": "[{\"end\":322,\"start\":1},{\"end\":766,\"start\":445}]", "venue": null, "abstract": "[{\"end\":1943,\"start\":806}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2749,\"start\":2748},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2886,\"start\":2885},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3194,\"start\":3193},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3398,\"start\":3395},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3401,\"start\":3398},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3404,\"start\":3401},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3407,\"start\":3404},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3411,\"start\":3407},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3595,\"start\":3593},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3793,\"start\":3789},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3797,\"start\":3793},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3801,\"start\":3797},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3805,\"start\":3801},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3809,\"start\":3805},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3939,\"start\":3936},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3943,\"start\":3939},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3947,\"start\":3943},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3951,\"start\":3947},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4792,\"start\":4790},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4808,\"start\":4806},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4821,\"start\":4819},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4837,\"start\":4835},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4845,\"start\":4843},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4858,\"start\":4856},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4875,\"start\":4873},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5101,\"start\":5099},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5288,\"start\":5286},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6426,\"start\":6424},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11512,\"start\":11510},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14762,\"start\":14760},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14775,\"start\":14773},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14787,\"start\":14785},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14795,\"start\":14793},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14806,\"start\":14804},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14821,\"start\":14819},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14841,\"start\":14839},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17585,\"start\":17583},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17601,\"start\":17599},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17614,\"start\":17612},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17630,\"start\":17628},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17651,\"start\":17649},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17664,\"start\":17662},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17895,\"start\":17893},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19278,\"start\":19276},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19292,\"start\":19290},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20314,\"start\":20312},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20444,\"start\":20442},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20691,\"start\":20689},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25651,\"start\":25649},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33825,\"start\":33823},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33831,\"start\":33829},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":33841,\"start\":33839},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38962,\"start\":38960},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":39007,\"start\":39005},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39018,\"start\":39016},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":39034,\"start\":39032}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33319,\"start\":33106},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33442,\"start\":33320},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33569,\"start\":33443},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33886,\"start\":33570},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33941,\"start\":33887},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35099,\"start\":33942},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36387,\"start\":35100},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37016,\"start\":36388},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37773,\"start\":37017},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38839,\"start\":37774},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39802,\"start\":38840},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":41222,\"start\":39803}]", "paragraph": "[{\"end\":2513,\"start\":1955},{\"end\":3268,\"start\":2530},{\"end\":4250,\"start\":3270},{\"end\":6766,\"start\":4252},{\"end\":7400,\"start\":6768},{\"end\":8631,\"start\":7424},{\"end\":9913,\"start\":8633},{\"end\":10339,\"start\":9915},{\"end\":10802,\"start\":10360},{\"end\":11369,\"start\":10804},{\"end\":11674,\"start\":11389},{\"end\":12178,\"start\":11676},{\"end\":13532,\"start\":12180},{\"end\":14554,\"start\":13534},{\"end\":15220,\"start\":14566},{\"end\":16066,\"start\":15255},{\"end\":16642,\"start\":16068},{\"end\":17203,\"start\":16644},{\"end\":18524,\"start\":17226},{\"end\":20802,\"start\":18559},{\"end\":23219,\"start\":20823},{\"end\":24079,\"start\":23240},{\"end\":24553,\"start\":24122},{\"end\":25294,\"start\":24569},{\"end\":26589,\"start\":25296},{\"end\":27188,\"start\":26618},{\"end\":27967,\"start\":27203},{\"end\":29169,\"start\":27969},{\"end\":30153,\"start\":29225},{\"end\":31150,\"start\":30168},{\"end\":33018,\"start\":31173},{\"end\":33105,\"start\":33020}]", "formula": null, "table_ref": "[{\"end\":3191,\"start\":3184},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":7088,\"start\":6905},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":10801,\"start\":10794},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18522,\"start\":18515},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":19952,\"start\":19945},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20801,\"start\":20794},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":21841,\"start\":21834},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21941,\"start\":21933},{\"end\":22624,\"start\":22616},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24262,\"start\":24254}]", "section_header": "[{\"end\":1953,\"start\":1945},{\"end\":2528,\"start\":2516},{\"end\":7410,\"start\":7403},{\"end\":7422,\"start\":7413},{\"end\":10344,\"start\":10342},{\"end\":10358,\"start\":10347},{\"end\":11387,\"start\":11372},{\"end\":14564,\"start\":14557},{\"end\":15253,\"start\":15223},{\"end\":17224,\"start\":17206},{\"end\":18537,\"start\":18527},{\"end\":18557,\"start\":18540},{\"end\":20821,\"start\":20805},{\"end\":23224,\"start\":23222},{\"end\":23238,\"start\":23227},{\"end\":24120,\"start\":24082},{\"end\":24567,\"start\":24556},{\"end\":26616,\"start\":26592},{\"end\":27201,\"start\":27191},{\"end\":29223,\"start\":29172},{\"end\":30166,\"start\":30156},{\"end\":31171,\"start\":31153},{\"end\":33331,\"start\":33321},{\"end\":33454,\"start\":33444},{\"end\":35110,\"start\":35101},{\"end\":36398,\"start\":36389},{\"end\":37027,\"start\":37018},{\"end\":37784,\"start\":37775},{\"end\":38850,\"start\":38841},{\"end\":39813,\"start\":39804}]", "table": "[{\"end\":36387,\"start\":35142},{\"end\":37016,\"start\":36469},{\"end\":37773,\"start\":37057},{\"end\":38839,\"start\":38278},{\"end\":39802,\"start\":39416},{\"end\":41222,\"start\":40608}]", "figure_caption": "[{\"end\":33319,\"start\":33108},{\"end\":33442,\"start\":33333},{\"end\":33569,\"start\":33456},{\"end\":33886,\"start\":33572},{\"end\":33941,\"start\":33889},{\"end\":35099,\"start\":33944},{\"end\":35142,\"start\":35112},{\"end\":36469,\"start\":36400},{\"end\":37057,\"start\":37029},{\"end\":38278,\"start\":37786},{\"end\":39416,\"start\":38852},{\"end\":40608,\"start\":39815}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3217,\"start\":3208},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8238,\"start\":8228},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9711,\"start\":9701},{\"end\":11301,\"start\":11292},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15155,\"start\":15147}]", "bib_author_first_name": "[{\"end\":41973,\"start\":41972},{\"end\":41979,\"start\":41974},{\"end\":42275,\"start\":42274},{\"end\":42289,\"start\":42288},{\"end\":42301,\"start\":42300},{\"end\":42311,\"start\":42310},{\"end\":42328,\"start\":42327},{\"end\":42338,\"start\":42337},{\"end\":42347,\"start\":42346},{\"end\":42679,\"start\":42678},{\"end\":42690,\"start\":42689},{\"end\":42876,\"start\":42875},{\"end\":42878,\"start\":42877},{\"end\":43148,\"start\":43147},{\"end\":43157,\"start\":43154},{\"end\":43161,\"start\":43160},{\"end\":43393,\"start\":43392},{\"end\":43395,\"start\":43394},{\"end\":43403,\"start\":43402},{\"end\":43405,\"start\":43404},{\"end\":43417,\"start\":43416},{\"end\":43431,\"start\":43430},{\"end\":43433,\"start\":43432},{\"end\":43443,\"start\":43442},{\"end\":43445,\"start\":43444},{\"end\":43455,\"start\":43454},{\"end\":43457,\"start\":43456},{\"end\":43811,\"start\":43810},{\"end\":43819,\"start\":43818},{\"end\":43821,\"start\":43820},{\"end\":43829,\"start\":43828},{\"end\":43833,\"start\":43830},{\"end\":43840,\"start\":43839},{\"end\":43849,\"start\":43848},{\"end\":43851,\"start\":43850},{\"end\":43860,\"start\":43859},{\"end\":43862,\"start\":43861},{\"end\":43871,\"start\":43870},{\"end\":43878,\"start\":43877},{\"end\":43882,\"start\":43879},{\"end\":44285,\"start\":44284},{\"end\":44291,\"start\":44290},{\"end\":44299,\"start\":44298},{\"end\":44306,\"start\":44305},{\"end\":44314,\"start\":44313},{\"end\":44321,\"start\":44320},{\"end\":44328,\"start\":44327},{\"end\":44336,\"start\":44335},{\"end\":44347,\"start\":44343},{\"end\":44351,\"start\":44350},{\"end\":44788,\"start\":44787},{\"end\":44796,\"start\":44795},{\"end\":44803,\"start\":44802},{\"end\":44812,\"start\":44811},{\"end\":44819,\"start\":44818},{\"end\":44826,\"start\":44825},{\"end\":44833,\"start\":44832},{\"end\":44839,\"start\":44838},{\"end\":44846,\"start\":44845},{\"end\":44852,\"start\":44851},{\"end\":45331,\"start\":45330},{\"end\":45333,\"start\":45332},{\"end\":45341,\"start\":45340},{\"end\":45343,\"start\":45342},{\"end\":45355,\"start\":45354},{\"end\":45357,\"start\":45356},{\"end\":45367,\"start\":45366},{\"end\":45371,\"start\":45368},{\"end\":45379,\"start\":45378},{\"end\":45383,\"start\":45380},{\"end\":45822,\"start\":45821},{\"end\":45829,\"start\":45828},{\"end\":45835,\"start\":45834},{\"end\":45841,\"start\":45840},{\"end\":45848,\"start\":45847},{\"end\":45854,\"start\":45853},{\"end\":45862,\"start\":45861},{\"end\":45869,\"start\":45868},{\"end\":45877,\"start\":45876},{\"end\":45884,\"start\":45883},{\"end\":46252,\"start\":46251},{\"end\":46263,\"start\":46262},{\"end\":46271,\"start\":46270},{\"end\":46280,\"start\":46279},{\"end\":46282,\"start\":46281},{\"end\":46292,\"start\":46291},{\"end\":46298,\"start\":46297},{\"end\":46315,\"start\":46314},{\"end\":46330,\"start\":46329},{\"end\":46340,\"start\":46339},{\"end\":46350,\"start\":46349},{\"end\":46829,\"start\":46828},{\"end\":46833,\"start\":46830},{\"end\":46841,\"start\":46840},{\"end\":46845,\"start\":46842},{\"end\":46855,\"start\":46854},{\"end\":46862,\"start\":46861},{\"end\":46866,\"start\":46863},{\"end\":46873,\"start\":46872},{\"end\":46875,\"start\":46874},{\"end\":46884,\"start\":46883},{\"end\":46891,\"start\":46890},{\"end\":46901,\"start\":46900},{\"end\":46918,\"start\":46917},{\"end\":46920,\"start\":46919},{\"end\":46931,\"start\":46930},{\"end\":46933,\"start\":46932},{\"end\":47471,\"start\":47470},{\"end\":47473,\"start\":47472},{\"end\":47492,\"start\":47491},{\"end\":47494,\"start\":47493},{\"end\":47506,\"start\":47505},{\"end\":47518,\"start\":47517},{\"end\":47520,\"start\":47519},{\"end\":47533,\"start\":47532},{\"end\":47541,\"start\":47540},{\"end\":47968,\"start\":47967},{\"end\":47974,\"start\":47973},{\"end\":47982,\"start\":47981},{\"end\":47989,\"start\":47988},{\"end\":47995,\"start\":47994},{\"end\":48003,\"start\":48002},{\"end\":48014,\"start\":48013},{\"end\":48016,\"start\":48015},{\"end\":48023,\"start\":48022},{\"end\":48031,\"start\":48030},{\"end\":48039,\"start\":48038},{\"end\":48041,\"start\":48040},{\"end\":48493,\"start\":48492},{\"end\":48500,\"start\":48499},{\"end\":48508,\"start\":48507},{\"end\":48510,\"start\":48509},{\"end\":48520,\"start\":48519},{\"end\":48524,\"start\":48521},{\"end\":48533,\"start\":48532},{\"end\":48535,\"start\":48534},{\"end\":48544,\"start\":48543},{\"end\":48546,\"start\":48545},{\"end\":48557,\"start\":48556},{\"end\":48561,\"start\":48558},{\"end\":48967,\"start\":48966},{\"end\":48978,\"start\":48977},{\"end\":48988,\"start\":48987},{\"end\":49001,\"start\":49000},{\"end\":49010,\"start\":49009},{\"end\":49018,\"start\":49017},{\"end\":49030,\"start\":49029},{\"end\":49032,\"start\":49031},{\"end\":49045,\"start\":49044},{\"end\":49414,\"start\":49413},{\"end\":49420,\"start\":49419},{\"end\":49426,\"start\":49425},{\"end\":49432,\"start\":49431},{\"end\":49440,\"start\":49439},{\"end\":49717,\"start\":49716},{\"end\":49725,\"start\":49724},{\"end\":49734,\"start\":49733},{\"end\":49740,\"start\":49739},{\"end\":49748,\"start\":49747},{\"end\":50025,\"start\":50024},{\"end\":50035,\"start\":50034},{\"end\":50046,\"start\":50045},{\"end\":50056,\"start\":50055},{\"end\":50068,\"start\":50067},{\"end\":50075,\"start\":50074},{\"end\":50082,\"start\":50081},{\"end\":50089,\"start\":50088},{\"end\":50097,\"start\":50096},{\"end\":50104,\"start\":50103},{\"end\":50606,\"start\":50605},{\"end\":50621,\"start\":50620},{\"end\":50630,\"start\":50629},{\"end\":50642,\"start\":50641},{\"end\":50649,\"start\":50648},{\"end\":50661,\"start\":50660},{\"end\":50670,\"start\":50669},{\"end\":50678,\"start\":50677},{\"end\":50689,\"start\":50688},{\"end\":50699,\"start\":50698},{\"end\":50716,\"start\":50709},{\"end\":50720,\"start\":50719},{\"end\":51084,\"start\":51083},{\"end\":51095,\"start\":51094},{\"end\":51396,\"start\":51395},{\"end\":51409,\"start\":51408},{\"end\":51425,\"start\":51424},{\"end\":51427,\"start\":51426},{\"end\":51435,\"start\":51434},{\"end\":51447,\"start\":51446},{\"end\":51458,\"start\":51457},{\"end\":51460,\"start\":51459},{\"end\":51471,\"start\":51470},{\"end\":51480,\"start\":51479},{\"end\":51491,\"start\":51490},{\"end\":51501,\"start\":51500},{\"end\":51916,\"start\":51915},{\"end\":51931,\"start\":51930},{\"end\":51943,\"start\":51942},{\"end\":51952,\"start\":51951},{\"end\":51964,\"start\":51963},{\"end\":51966,\"start\":51965},{\"end\":51975,\"start\":51974},{\"end\":51984,\"start\":51983},{\"end\":51998,\"start\":51997},{\"end\":52009,\"start\":52008},{\"end\":52019,\"start\":52018},{\"end\":52390,\"start\":52389},{\"end\":52400,\"start\":52399},{\"end\":52402,\"start\":52401},{\"end\":52416,\"start\":52415},{\"end\":52425,\"start\":52424},{\"end\":52441,\"start\":52440},{\"end\":52450,\"start\":52449},{\"end\":52462,\"start\":52461},{\"end\":52983,\"start\":52982},{\"end\":52985,\"start\":52984},{\"end\":52997,\"start\":52996},{\"end\":53007,\"start\":53006},{\"end\":53299,\"start\":53298},{\"end\":53490,\"start\":53489},{\"end\":53498,\"start\":53497},{\"end\":53957,\"start\":53956},{\"end\":53969,\"start\":53968},{\"end\":54240,\"start\":54239},{\"end\":54246,\"start\":54245},{\"end\":54255,\"start\":54254},{\"end\":54262,\"start\":54261},{\"end\":54632,\"start\":54631},{\"end\":54638,\"start\":54637},{\"end\":54646,\"start\":54645},{\"end\":54657,\"start\":54656},{\"end\":54664,\"start\":54663},{\"end\":54935,\"start\":54934},{\"end\":54942,\"start\":54941},{\"end\":54944,\"start\":54943},{\"end\":55318,\"start\":55317},{\"end\":55585,\"start\":55584},{\"end\":55592,\"start\":55591},{\"end\":55599,\"start\":55598},{\"end\":55607,\"start\":55606},{\"end\":55609,\"start\":55608},{\"end\":55615,\"start\":55614},{\"end\":55622,\"start\":55621},{\"end\":56081,\"start\":56080},{\"end\":56094,\"start\":56093},{\"end\":56107,\"start\":56106},{\"end\":56383,\"start\":56382},{\"end\":56392,\"start\":56391},{\"end\":56401,\"start\":56400},{\"end\":56403,\"start\":56402},{\"end\":56414,\"start\":56413},{\"end\":56631,\"start\":56630},{\"end\":56646,\"start\":56645},{\"end\":56657,\"start\":56656},{\"end\":56968,\"start\":56967},{\"end\":56979,\"start\":56978},{\"end\":56987,\"start\":56986},{\"end\":56997,\"start\":56996},{\"end\":57011,\"start\":57008},{\"end\":57019,\"start\":57018},{\"end\":57341,\"start\":57340},{\"end\":57347,\"start\":57346},{\"end\":57355,\"start\":57354},{\"end\":57361,\"start\":57360},{\"end\":57369,\"start\":57368},{\"end\":57379,\"start\":57376},{\"end\":57383,\"start\":57382},{\"end\":57385,\"start\":57384},{\"end\":57648,\"start\":57647},{\"end\":57661,\"start\":57660},{\"end\":57671,\"start\":57670},{\"end\":57949,\"start\":57948},{\"end\":57960,\"start\":57959},{\"end\":57976,\"start\":57975}]", "bib_author_last_name": "[{\"end\":41985,\"start\":41980},{\"end\":42286,\"start\":42276},{\"end\":42298,\"start\":42290},{\"end\":42308,\"start\":42302},{\"end\":42325,\"start\":42312},{\"end\":42335,\"start\":42329},{\"end\":42344,\"start\":42339},{\"end\":42353,\"start\":42348},{\"end\":42687,\"start\":42680},{\"end\":42695,\"start\":42691},{\"end\":42891,\"start\":42879},{\"end\":43152,\"start\":43149},{\"end\":43400,\"start\":43396},{\"end\":43414,\"start\":43406},{\"end\":43428,\"start\":43418},{\"end\":43440,\"start\":43434},{\"end\":43452,\"start\":43446},{\"end\":43462,\"start\":43458},{\"end\":43816,\"start\":43812},{\"end\":43826,\"start\":43822},{\"end\":43837,\"start\":43834},{\"end\":43846,\"start\":43841},{\"end\":43857,\"start\":43852},{\"end\":43868,\"start\":43863},{\"end\":43875,\"start\":43872},{\"end\":43885,\"start\":43883},{\"end\":44288,\"start\":44286},{\"end\":44296,\"start\":44292},{\"end\":44303,\"start\":44300},{\"end\":44311,\"start\":44307},{\"end\":44318,\"start\":44315},{\"end\":44325,\"start\":44322},{\"end\":44333,\"start\":44329},{\"end\":44341,\"start\":44337},{\"end\":44793,\"start\":44789},{\"end\":44800,\"start\":44797},{\"end\":44809,\"start\":44804},{\"end\":44816,\"start\":44813},{\"end\":44823,\"start\":44820},{\"end\":44830,\"start\":44827},{\"end\":44836,\"start\":44834},{\"end\":44843,\"start\":44840},{\"end\":44849,\"start\":44847},{\"end\":44858,\"start\":44853},{\"end\":45338,\"start\":45334},{\"end\":45352,\"start\":45344},{\"end\":45364,\"start\":45358},{\"end\":45376,\"start\":45372},{\"end\":45387,\"start\":45384},{\"end\":45826,\"start\":45823},{\"end\":45832,\"start\":45830},{\"end\":45838,\"start\":45836},{\"end\":45845,\"start\":45842},{\"end\":45851,\"start\":45849},{\"end\":45859,\"start\":45855},{\"end\":45866,\"start\":45863},{\"end\":45874,\"start\":45870},{\"end\":45881,\"start\":45878},{\"end\":45888,\"start\":45885},{\"end\":46260,\"start\":46253},{\"end\":46268,\"start\":46264},{\"end\":46277,\"start\":46272},{\"end\":46289,\"start\":46283},{\"end\":46295,\"start\":46293},{\"end\":46312,\"start\":46299},{\"end\":46327,\"start\":46316},{\"end\":46337,\"start\":46331},{\"end\":46347,\"start\":46341},{\"end\":46358,\"start\":46351},{\"end\":46838,\"start\":46834},{\"end\":46852,\"start\":46846},{\"end\":46859,\"start\":46856},{\"end\":46870,\"start\":46867},{\"end\":46881,\"start\":46876},{\"end\":46888,\"start\":46885},{\"end\":46898,\"start\":46892},{\"end\":46915,\"start\":46902},{\"end\":46928,\"start\":46921},{\"end\":46937,\"start\":46934},{\"end\":47489,\"start\":47474},{\"end\":47503,\"start\":47495},{\"end\":47515,\"start\":47507},{\"end\":47530,\"start\":47521},{\"end\":47538,\"start\":47534},{\"end\":47549,\"start\":47542},{\"end\":47971,\"start\":47969},{\"end\":47979,\"start\":47975},{\"end\":47986,\"start\":47983},{\"end\":47992,\"start\":47990},{\"end\":48000,\"start\":47996},{\"end\":48011,\"start\":48004},{\"end\":48020,\"start\":48017},{\"end\":48028,\"start\":48024},{\"end\":48036,\"start\":48032},{\"end\":48046,\"start\":48042},{\"end\":48497,\"start\":48494},{\"end\":48505,\"start\":48501},{\"end\":48517,\"start\":48511},{\"end\":48530,\"start\":48525},{\"end\":48541,\"start\":48536},{\"end\":48554,\"start\":48547},{\"end\":48568,\"start\":48562},{\"end\":48975,\"start\":48968},{\"end\":48985,\"start\":48979},{\"end\":48998,\"start\":48989},{\"end\":49007,\"start\":49002},{\"end\":49015,\"start\":49011},{\"end\":49027,\"start\":49019},{\"end\":49042,\"start\":49033},{\"end\":49054,\"start\":49046},{\"end\":49417,\"start\":49415},{\"end\":49423,\"start\":49421},{\"end\":49429,\"start\":49427},{\"end\":49437,\"start\":49433},{\"end\":49443,\"start\":49441},{\"end\":49722,\"start\":49718},{\"end\":49731,\"start\":49726},{\"end\":49737,\"start\":49735},{\"end\":49745,\"start\":49741},{\"end\":49753,\"start\":49749},{\"end\":50032,\"start\":50026},{\"end\":50043,\"start\":50036},{\"end\":50053,\"start\":50047},{\"end\":50065,\"start\":50057},{\"end\":50072,\"start\":50069},{\"end\":50079,\"start\":50076},{\"end\":50086,\"start\":50083},{\"end\":50094,\"start\":50090},{\"end\":50101,\"start\":50098},{\"end\":50108,\"start\":50105},{\"end\":50392,\"start\":50385},{\"end\":50618,\"start\":50607},{\"end\":50627,\"start\":50622},{\"end\":50639,\"start\":50631},{\"end\":50646,\"start\":50643},{\"end\":50658,\"start\":50650},{\"end\":50667,\"start\":50662},{\"end\":50675,\"start\":50671},{\"end\":50686,\"start\":50679},{\"end\":50696,\"start\":50690},{\"end\":50707,\"start\":50700},{\"end\":51092,\"start\":51085},{\"end\":51104,\"start\":51096},{\"end\":51406,\"start\":51397},{\"end\":51422,\"start\":51410},{\"end\":51432,\"start\":51428},{\"end\":51444,\"start\":51436},{\"end\":51455,\"start\":51448},{\"end\":51468,\"start\":51461},{\"end\":51477,\"start\":51472},{\"end\":51488,\"start\":51481},{\"end\":51498,\"start\":51492},{\"end\":51511,\"start\":51502},{\"end\":51928,\"start\":51917},{\"end\":51940,\"start\":51932},{\"end\":51949,\"start\":51944},{\"end\":51961,\"start\":51953},{\"end\":51972,\"start\":51967},{\"end\":51981,\"start\":51976},{\"end\":51995,\"start\":51985},{\"end\":52006,\"start\":51999},{\"end\":52016,\"start\":52010},{\"end\":52025,\"start\":52020},{\"end\":52397,\"start\":52391},{\"end\":52413,\"start\":52403},{\"end\":52422,\"start\":52417},{\"end\":52438,\"start\":52426},{\"end\":52447,\"start\":52442},{\"end\":52459,\"start\":52451},{\"end\":52468,\"start\":52463},{\"end\":52994,\"start\":52986},{\"end\":53004,\"start\":52998},{\"end\":53013,\"start\":53008},{\"end\":53302,\"start\":53300},{\"end\":53495,\"start\":53491},{\"end\":53507,\"start\":53499},{\"end\":53966,\"start\":53958},{\"end\":53979,\"start\":53970},{\"end\":54243,\"start\":54241},{\"end\":54252,\"start\":54247},{\"end\":54259,\"start\":54256},{\"end\":54266,\"start\":54263},{\"end\":54635,\"start\":54633},{\"end\":54643,\"start\":54639},{\"end\":54654,\"start\":54647},{\"end\":54661,\"start\":54658},{\"end\":54667,\"start\":54665},{\"end\":54939,\"start\":54936},{\"end\":54947,\"start\":54945},{\"end\":55325,\"start\":55319},{\"end\":55589,\"start\":55586},{\"end\":55596,\"start\":55593},{\"end\":55604,\"start\":55600},{\"end\":55612,\"start\":55610},{\"end\":55619,\"start\":55616},{\"end\":55627,\"start\":55623},{\"end\":56091,\"start\":56082},{\"end\":56104,\"start\":56095},{\"end\":56114,\"start\":56108},{\"end\":56389,\"start\":56384},{\"end\":56398,\"start\":56393},{\"end\":56411,\"start\":56404},{\"end\":56424,\"start\":56415},{\"end\":56643,\"start\":56632},{\"end\":56654,\"start\":56647},{\"end\":56662,\"start\":56658},{\"end\":56976,\"start\":56969},{\"end\":56984,\"start\":56980},{\"end\":56994,\"start\":56988},{\"end\":57006,\"start\":56998},{\"end\":57016,\"start\":57012},{\"end\":57344,\"start\":57342},{\"end\":57352,\"start\":57348},{\"end\":57358,\"start\":57356},{\"end\":57366,\"start\":57362},{\"end\":57374,\"start\":57370},{\"end\":57658,\"start\":57649},{\"end\":57668,\"start\":57662},{\"end\":57676,\"start\":57672},{\"end\":57957,\"start\":57950},{\"end\":57973,\"start\":57961},{\"end\":57984,\"start\":57977}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":42176,\"start\":41972},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":44815073},\"end\":42592,\"start\":42178},{\"attributes\":{\"id\":\"b2\"},\"end\":42825,\"start\":42594},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":75674815},\"end\":43092,\"start\":42827},{\"attributes\":{\"id\":\"b4\"},\"end\":43297,\"start\":43094},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5395241},\"end\":43692,\"start\":43299},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10235556},\"end\":44146,\"start\":43694},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14657882},\"end\":44622,\"start\":44148},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":29083879},\"end\":45197,\"start\":44624},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":23459688},\"end\":45732,\"start\":45199},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":235243162},\"end\":46124,\"start\":45734},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":26657811},\"end\":46658,\"start\":46126},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3606254},\"end\":47341,\"start\":46660},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3559865},\"end\":47828,\"start\":47343},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52896595},\"end\":48350,\"start\":47830},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1873546},\"end\":48860,\"start\":48352},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":204901509},\"end\":49333,\"start\":48862},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":221636779},\"end\":49638,\"start\":49335},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":234345049},\"end\":49957,\"start\":49640},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":207815138},\"end\":50351,\"start\":49959},{\"attributes\":{\"id\":\"b20\"},\"end\":50529,\"start\":50353},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":21755399},\"end\":51003,\"start\":50531},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":38805875},\"end\":51288,\"start\":51005},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":13770952},\"end\":51833,\"start\":51290},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":60512147},\"end\":52281,\"start\":51835},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":28108089},\"end\":52917,\"start\":52283},{\"attributes\":{\"doi\":\"10.48550/arXiv.1810.11363\",\"id\":\"b26\"},\"end\":53234,\"start\":52919},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3815895},\"end\":53445,\"start\":53236},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4650265},\"end\":53886,\"start\":53447},{\"attributes\":{\"doi\":\"10.48550/arXiv.1409.1556\",\"id\":\"b29\"},\"end\":54191,\"start\":53888},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206594692},\"end\":54597,\"start\":54193},{\"attributes\":{\"id\":\"b31\"},\"end\":54858,\"start\":54599},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":167217261},\"end\":55100,\"start\":54860},{\"attributes\":{\"id\":\"b33\"},\"end\":55267,\"start\":55102},{\"attributes\":{\"doi\":\"10.48550/arXiv.1409.6070\",\"id\":\"b34\"},\"end\":55495,\"start\":55269},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":152282661},\"end\":56003,\"start\":55497},{\"attributes\":{\"id\":\"b36\"},\"end\":56337,\"start\":56005},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3162051},\"end\":56563,\"start\":56339},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3719281},\"end\":56878,\"start\":56565},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":222090738},\"end\":57251,\"start\":56880},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":50770252},\"end\":57587,\"start\":57253},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":6426453},\"end\":57880,\"start\":57589},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":206592766},\"end\":58342,\"start\":57882}]", "bib_title": "[{\"end\":42272,\"start\":42178},{\"end\":42873,\"start\":42827},{\"end\":43145,\"start\":43094},{\"end\":43390,\"start\":43299},{\"end\":43808,\"start\":43694},{\"end\":44282,\"start\":44148},{\"end\":44785,\"start\":44624},{\"end\":45328,\"start\":45199},{\"end\":45819,\"start\":45734},{\"end\":46249,\"start\":46126},{\"end\":46826,\"start\":46660},{\"end\":47468,\"start\":47343},{\"end\":47965,\"start\":47830},{\"end\":48490,\"start\":48352},{\"end\":48964,\"start\":48862},{\"end\":49411,\"start\":49335},{\"end\":49714,\"start\":49640},{\"end\":50022,\"start\":49959},{\"end\":50603,\"start\":50531},{\"end\":51081,\"start\":51005},{\"end\":51393,\"start\":51290},{\"end\":51913,\"start\":51835},{\"end\":52387,\"start\":52283},{\"end\":53296,\"start\":53236},{\"end\":53487,\"start\":53447},{\"end\":54237,\"start\":54193},{\"end\":54629,\"start\":54599},{\"end\":54932,\"start\":54860},{\"end\":55582,\"start\":55497},{\"end\":56380,\"start\":56339},{\"end\":56628,\"start\":56565},{\"end\":56965,\"start\":56880},{\"end\":57338,\"start\":57253},{\"end\":57645,\"start\":57589},{\"end\":57946,\"start\":57882}]", "bib_author": "[{\"end\":41987,\"start\":41972},{\"end\":42288,\"start\":42274},{\"end\":42300,\"start\":42288},{\"end\":42310,\"start\":42300},{\"end\":42327,\"start\":42310},{\"end\":42337,\"start\":42327},{\"end\":42346,\"start\":42337},{\"end\":42355,\"start\":42346},{\"end\":42689,\"start\":42678},{\"end\":42697,\"start\":42689},{\"end\":42893,\"start\":42875},{\"end\":43154,\"start\":43147},{\"end\":43160,\"start\":43154},{\"end\":43164,\"start\":43160},{\"end\":43402,\"start\":43392},{\"end\":43416,\"start\":43402},{\"end\":43430,\"start\":43416},{\"end\":43442,\"start\":43430},{\"end\":43454,\"start\":43442},{\"end\":43464,\"start\":43454},{\"end\":43818,\"start\":43810},{\"end\":43828,\"start\":43818},{\"end\":43839,\"start\":43828},{\"end\":43848,\"start\":43839},{\"end\":43859,\"start\":43848},{\"end\":43870,\"start\":43859},{\"end\":43877,\"start\":43870},{\"end\":43887,\"start\":43877},{\"end\":44290,\"start\":44284},{\"end\":44298,\"start\":44290},{\"end\":44305,\"start\":44298},{\"end\":44313,\"start\":44305},{\"end\":44320,\"start\":44313},{\"end\":44327,\"start\":44320},{\"end\":44335,\"start\":44327},{\"end\":44343,\"start\":44335},{\"end\":44350,\"start\":44343},{\"end\":44354,\"start\":44350},{\"end\":44795,\"start\":44787},{\"end\":44802,\"start\":44795},{\"end\":44811,\"start\":44802},{\"end\":44818,\"start\":44811},{\"end\":44825,\"start\":44818},{\"end\":44832,\"start\":44825},{\"end\":44838,\"start\":44832},{\"end\":44845,\"start\":44838},{\"end\":44851,\"start\":44845},{\"end\":44860,\"start\":44851},{\"end\":45340,\"start\":45330},{\"end\":45354,\"start\":45340},{\"end\":45366,\"start\":45354},{\"end\":45378,\"start\":45366},{\"end\":45389,\"start\":45378},{\"end\":45828,\"start\":45821},{\"end\":45834,\"start\":45828},{\"end\":45840,\"start\":45834},{\"end\":45847,\"start\":45840},{\"end\":45853,\"start\":45847},{\"end\":45861,\"start\":45853},{\"end\":45868,\"start\":45861},{\"end\":45876,\"start\":45868},{\"end\":45883,\"start\":45876},{\"end\":45890,\"start\":45883},{\"end\":46262,\"start\":46251},{\"end\":46270,\"start\":46262},{\"end\":46279,\"start\":46270},{\"end\":46291,\"start\":46279},{\"end\":46297,\"start\":46291},{\"end\":46314,\"start\":46297},{\"end\":46329,\"start\":46314},{\"end\":46339,\"start\":46329},{\"end\":46349,\"start\":46339},{\"end\":46360,\"start\":46349},{\"end\":46840,\"start\":46828},{\"end\":46854,\"start\":46840},{\"end\":46861,\"start\":46854},{\"end\":46872,\"start\":46861},{\"end\":46883,\"start\":46872},{\"end\":46890,\"start\":46883},{\"end\":46900,\"start\":46890},{\"end\":46917,\"start\":46900},{\"end\":46930,\"start\":46917},{\"end\":46939,\"start\":46930},{\"end\":47491,\"start\":47470},{\"end\":47505,\"start\":47491},{\"end\":47517,\"start\":47505},{\"end\":47532,\"start\":47517},{\"end\":47540,\"start\":47532},{\"end\":47551,\"start\":47540},{\"end\":47973,\"start\":47967},{\"end\":47981,\"start\":47973},{\"end\":47988,\"start\":47981},{\"end\":47994,\"start\":47988},{\"end\":48002,\"start\":47994},{\"end\":48013,\"start\":48002},{\"end\":48022,\"start\":48013},{\"end\":48030,\"start\":48022},{\"end\":48038,\"start\":48030},{\"end\":48048,\"start\":48038},{\"end\":48499,\"start\":48492},{\"end\":48507,\"start\":48499},{\"end\":48519,\"start\":48507},{\"end\":48532,\"start\":48519},{\"end\":48543,\"start\":48532},{\"end\":48556,\"start\":48543},{\"end\":48570,\"start\":48556},{\"end\":48977,\"start\":48966},{\"end\":48987,\"start\":48977},{\"end\":49000,\"start\":48987},{\"end\":49009,\"start\":49000},{\"end\":49017,\"start\":49009},{\"end\":49029,\"start\":49017},{\"end\":49044,\"start\":49029},{\"end\":49056,\"start\":49044},{\"end\":49419,\"start\":49413},{\"end\":49425,\"start\":49419},{\"end\":49431,\"start\":49425},{\"end\":49439,\"start\":49431},{\"end\":49445,\"start\":49439},{\"end\":49724,\"start\":49716},{\"end\":49733,\"start\":49724},{\"end\":49739,\"start\":49733},{\"end\":49747,\"start\":49739},{\"end\":49755,\"start\":49747},{\"end\":50034,\"start\":50024},{\"end\":50045,\"start\":50034},{\"end\":50055,\"start\":50045},{\"end\":50067,\"start\":50055},{\"end\":50074,\"start\":50067},{\"end\":50081,\"start\":50074},{\"end\":50088,\"start\":50081},{\"end\":50096,\"start\":50088},{\"end\":50103,\"start\":50096},{\"end\":50110,\"start\":50103},{\"end\":50394,\"start\":50385},{\"end\":50620,\"start\":50605},{\"end\":50629,\"start\":50620},{\"end\":50641,\"start\":50629},{\"end\":50648,\"start\":50641},{\"end\":50660,\"start\":50648},{\"end\":50669,\"start\":50660},{\"end\":50677,\"start\":50669},{\"end\":50688,\"start\":50677},{\"end\":50698,\"start\":50688},{\"end\":50709,\"start\":50698},{\"end\":50719,\"start\":50709},{\"end\":50723,\"start\":50719},{\"end\":51094,\"start\":51083},{\"end\":51106,\"start\":51094},{\"end\":51408,\"start\":51395},{\"end\":51424,\"start\":51408},{\"end\":51434,\"start\":51424},{\"end\":51446,\"start\":51434},{\"end\":51457,\"start\":51446},{\"end\":51470,\"start\":51457},{\"end\":51479,\"start\":51470},{\"end\":51490,\"start\":51479},{\"end\":51500,\"start\":51490},{\"end\":51513,\"start\":51500},{\"end\":51930,\"start\":51915},{\"end\":51942,\"start\":51930},{\"end\":51951,\"start\":51942},{\"end\":51963,\"start\":51951},{\"end\":51974,\"start\":51963},{\"end\":51983,\"start\":51974},{\"end\":51997,\"start\":51983},{\"end\":52008,\"start\":51997},{\"end\":52018,\"start\":52008},{\"end\":52027,\"start\":52018},{\"end\":52399,\"start\":52389},{\"end\":52415,\"start\":52399},{\"end\":52424,\"start\":52415},{\"end\":52440,\"start\":52424},{\"end\":52449,\"start\":52440},{\"end\":52461,\"start\":52449},{\"end\":52470,\"start\":52461},{\"end\":52996,\"start\":52982},{\"end\":53006,\"start\":52996},{\"end\":53015,\"start\":53006},{\"end\":53304,\"start\":53298},{\"end\":53497,\"start\":53489},{\"end\":53509,\"start\":53497},{\"end\":53968,\"start\":53956},{\"end\":53981,\"start\":53968},{\"end\":54245,\"start\":54239},{\"end\":54254,\"start\":54245},{\"end\":54261,\"start\":54254},{\"end\":54268,\"start\":54261},{\"end\":54637,\"start\":54631},{\"end\":54645,\"start\":54637},{\"end\":54656,\"start\":54645},{\"end\":54663,\"start\":54656},{\"end\":54669,\"start\":54663},{\"end\":54941,\"start\":54934},{\"end\":54949,\"start\":54941},{\"end\":55327,\"start\":55317},{\"end\":55591,\"start\":55584},{\"end\":55598,\"start\":55591},{\"end\":55606,\"start\":55598},{\"end\":55614,\"start\":55606},{\"end\":55621,\"start\":55614},{\"end\":55629,\"start\":55621},{\"end\":56093,\"start\":56080},{\"end\":56106,\"start\":56093},{\"end\":56116,\"start\":56106},{\"end\":56391,\"start\":56382},{\"end\":56400,\"start\":56391},{\"end\":56413,\"start\":56400},{\"end\":56426,\"start\":56413},{\"end\":56645,\"start\":56630},{\"end\":56656,\"start\":56645},{\"end\":56664,\"start\":56656},{\"end\":56978,\"start\":56967},{\"end\":56986,\"start\":56978},{\"end\":56996,\"start\":56986},{\"end\":57008,\"start\":56996},{\"end\":57018,\"start\":57008},{\"end\":57022,\"start\":57018},{\"end\":57346,\"start\":57340},{\"end\":57354,\"start\":57346},{\"end\":57360,\"start\":57354},{\"end\":57368,\"start\":57360},{\"end\":57376,\"start\":57368},{\"end\":57382,\"start\":57376},{\"end\":57388,\"start\":57382},{\"end\":57660,\"start\":57647},{\"end\":57670,\"start\":57660},{\"end\":57678,\"start\":57670},{\"end\":57959,\"start\":57948},{\"end\":57975,\"start\":57959},{\"end\":57986,\"start\":57975}]", "bib_venue": "[{\"end\":42069,\"start\":42052},{\"end\":42942,\"start\":42923},{\"end\":53326,\"start\":53319},{\"end\":53692,\"start\":53609},{\"end\":54409,\"start\":54347},{\"end\":54965,\"start\":54961},{\"end\":55758,\"start\":55702},{\"end\":56442,\"start\":56438},{\"end\":57402,\"start\":57399},{\"end\":58127,\"start\":58065},{\"end\":42050,\"start\":41987},{\"end\":42366,\"start\":42355},{\"end\":42676,\"start\":42594},{\"end\":42921,\"start\":42893},{\"end\":43180,\"start\":43164},{\"end\":43475,\"start\":43464},{\"end\":43901,\"start\":43887},{\"end\":44362,\"start\":44354},{\"end\":44884,\"start\":44860},{\"end\":45406,\"start\":45389},{\"end\":45901,\"start\":45890},{\"end\":46364,\"start\":46360},{\"end\":46971,\"start\":46939},{\"end\":47566,\"start\":47551},{\"end\":48061,\"start\":48048},{\"end\":48583,\"start\":48570},{\"end\":49071,\"start\":49056},{\"end\":49466,\"start\":49445},{\"end\":49777,\"start\":49755},{\"end\":50125,\"start\":50110},{\"end\":50383,\"start\":50353},{\"end\":50742,\"start\":50723},{\"end\":51130,\"start\":51106},{\"end\":51534,\"start\":51513},{\"end\":52031,\"start\":52027},{\"end\":52573,\"start\":52470},{\"end\":52980,\"start\":52919},{\"end\":53317,\"start\":53304},{\"end\":53607,\"start\":53509},{\"end\":53954,\"start\":53888},{\"end\":54345,\"start\":54268},{\"end\":54707,\"start\":54669},{\"end\":54959,\"start\":54949},{\"end\":55145,\"start\":55102},{\"end\":55315,\"start\":55269},{\"end\":55700,\"start\":55629},{\"end\":56078,\"start\":56005},{\"end\":56436,\"start\":56426},{\"end\":56701,\"start\":56664},{\"end\":57044,\"start\":57022},{\"end\":57397,\"start\":57388},{\"end\":57716,\"start\":57678},{\"end\":58063,\"start\":57986}]"}}}, "year": 2023, "month": 12, "day": 17}