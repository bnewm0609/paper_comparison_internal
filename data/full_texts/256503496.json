{"id": 256503496, "updated": "2023-10-05 04:51:04.322", "metadata": {"title": "Fixing Hardware Security Bugs with Large Language Models", "authors": "[{\"first\":\"Baleegh\",\"last\":\"Ahmad\",\"middle\":[]},{\"first\":\"Shailja\",\"last\":\"Thakur\",\"middle\":[]},{\"first\":\"Benjamin\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Ramesh\",\"last\":\"Karri\",\"middle\":[]},{\"first\":\"Hammond\",\"last\":\"Pearce\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Novel AI-based code-writing Large Language Models (LLMs) such as OpenAI's Codex have demonstrated capabilities in many coding-adjacent domains. In this work we consider how LLMs maybe leveraged to automatically repair security relevant bugs present in hardware designs. We focus on bug repair in code written in the Hardware Description Language Verilog. For this study we build a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all ten of our benchmarks. This ensemble outperforms the state-of-the-art Cirfix hardware bug repair tool on its own suite of bugs. These results show that LLMs can repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair framework.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2302.01215", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2302-01215", "doi": "10.48550/arxiv.2302.01215"}}, "content": {"source": {"pdf_hash": "2af6a21a1b682ceb585165359d3605e89f4cf6b0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.01215v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "454d853053c52474f14efbe3cef3ff9180382b2e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2af6a21a1b682ceb585165359d3605e89f4cf6b0.txt", "contents": "\nFixing Hardware Security Bugs with Large Language Models\n\n\nBaleegh Ahmad \nNew York University\nNew York University\nUniversity of Calgary\nNew York University\nNew York University\n\n\nShailja Thakur \nNew York University\nNew York University\nUniversity of Calgary\nNew York University\nNew York University\n\n\nBenjamin Tan benjamin.tan1@ucalgary.ca \nNew York University\nNew York University\nUniversity of Calgary\nNew York University\nNew York University\n\n\nRamesh Karri rkarri@nyu.edu \nNew York University\nNew York University\nUniversity of Calgary\nNew York University\nNew York University\n\n\nHammond Pearce hammond.pearce@nyu.edu \nNew York University\nNew York University\nUniversity of Calgary\nNew York University\nNew York University\n\n\nFixing Hardware Security Bugs with Large Language Models\nCCS CONCEPTS \u2022 Hardware \u2192 Hardware description languages and compi- lation\u2022 Security and privacy \u2192 Hardware security imple- mentation\u2022 Computing methodologies \u2192 Natural language processing\nNovel AI-based code-writing Large Language Models (LLMs) such as OpenAI's Codex have demonstrated capabilities in many codingadjacent domains. In this work we consider how LLMs maybe leveraged to automatically repair security-relevant bugs present in hardware designs. We focus on bug repair in code written in the Hardware Description Language Verilog. For this study we build a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all ten of our benchmarks. This ensemble outperforms the state-of-the-art Cirfix hardware bug repair tool on its own suite of bugs. These results show that LLMs can repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair framework.CCS CONCEPTS\u2022 Hardware \u2192 Hardware description languages and compilation; \u2022 Security and privacy \u2192 Hardware security implementation; \u2022 Computing methodologies \u2192 Natural language processing.\n\nINTRODUCTION\n\n'Bugs' are inevitable when writing large quantities of code. Fixing them is laborious: automated tools are thus designed and employed to both identify bugs and then patch and repair them [9,23]. While considerable effort has explored software repair, for Hardware Design Languages (HDLs), the state of the art is less mature.\n\nIn this study, we focus on repairing security-relevant hardware bugs. While linters [25,49] and formal verification tools [2, 12] cover a large proportion of functional bugs, fewer tools cover hardware security bugs. Although formal verification tools like Synopsys FSV can be used for security verification in the design process, they have limited success [18]. Unlike software bugs, security bugs in hardware are more problematic because they cannot be patched once the chip is fabricated; this is especially concerning as hardware is typically the root of trust for a system [42]. With the ever-growing complexity of modern processors, software-exploitable hardware bugs are becoming common and pernicious [26,28]. This has resulted in the exploration of many techniques such as fuzzing [46,48], information flow tracking [7,33,52], unique program execution checking [21] and static analysis [5,11]. However, very few techniques\n\n\nSecurity Bug\n\n\nRepaired code\n\nThis could be a bug because...\n\n\nLarge Language Model\n\nFigure 1: LLMs can suggest repairs to designers.\n\naddress the automated repair of hardware bugs. The recently proposed Cirfix [6] develops automatic repair of functional hardware bugs and, to the best of our knowledge, is the only relevant effort in this context thus far. Further efforts need to be made to support the automated repair of functional and security bugs in hardware. Large Language Models (LLMs) are neural networks trained over millions of lines of text and code [13]. LLMs that are fine-tuned over open-source code repositories can generate code, where a user \"prompts\" the model with some text (e.g., code and comments) to guide the code generation. In contrast to previously proposed code repair techniques that involve mutation, repeated checks against an \"oracle,\" or source code templates, we propose that an LLM trained on code and natural language could potentially generate fixes, given an appropriate prompt. As LLMs are exposed to a wide variety of code examples during training, they should be able to assist designers in fixing bugs in different types of hardware designs and styles, with natural language guidance. In prior work [38,45], LLMs have been used to generate functional Verilog code. Machine learning-based techniques such as Neural Machine Translation [47] and pre-trained transformers [19] are explored in the software domain for bug fixes. Pearce et al. [37] use this approach to repair two scenarios of security weaknesses in Verilog code.\n\nThus, in this work, we investigate the use of LLMs to generate repairs for hardware security bugs. We study the performance of OpenAI Codex and CodeGen LLMs on instances of hardware security bugs. We offer insights into how best to use LLMs for successful repairs. An RTL designer can spot a security weakness and the LLM can help to find a fix as shown in Figure 1. Our contributions are as follows:\n\n\u2022 Curating a benchmark of hardware security bugs and their corresponding designs. These are open-sourced at [41].\n\n\u2022 Automated framework for using LLMs to generate repairs and evaluate them. We make the framework and artifacts produced in this study available [41]. \u2022 Automated end-to-end solution to detect, repair and evaluate repairs for certain bugs utilizing static analysis scanners from prior related work [5]. \u2022 Exploration of different LLMs and their parameters to suggest how best to use LLMs in hardware bug repair. These are posed as research questions answered in Section 5.\n\n\nBACKGROUND AND RELATED WORK\n\nOur work borrows ideas from software domain and applies them to the area of hardware design. Since this is not very common, in this section we present some over-arching concepts that better help understand our implementation.\n\n\nCode Repair\n\nSoftware code repair techniques continue to evolve (interested readers can see the living review by Monperrus [32], which contains an ever-growing list of automated repair tools and techniques). Generally, techniques try to fix errors through the use of program mutations and repair templates paired with tests to validate any changes [27,50,51]. Feedback loops are constructed with a reference implementation to guide the repair process [30,43]. Other domain-specific tools may also be built to deal with particular areas like build scripts, web, software models, etc. Security bugs are critical bug types that can lead to vulnerable systems. They can be more difficult to detect and repair than functional bugs, which can be detected by classical testing. Proving the presence or absence of a security bug is challenging. This has led to more 'creative' kinds of bug repair, including AI-based machine-learning techniques such as neural transfer learning [14] and examplebased approaches [31,53]. ML-based approaches involve memorization and generalization capabilities of neural networks, allowing a greater ability to suggest repairs for \"unseen\" code. The examplebased approaches start off with a dataset consisting of pairs of bugs and their repairs. Then, matching algorithms are applied to spot the best repair candidate from the dataset. Efforts in repair are also explored in other domains like recompilable decompiled code [40].\n\nFor digital hardware design, the recently proposed CirFix [6] attempts to localize bugs in RTL designs and then repair them. The researchers provide the benchmarks they develop for their study, allowing us to apply our methods to compare results. While it is the closest work, there are some fundamental differences in the approaches which limit direct comparisons. These differences are described in Table 1. CirFix performs both localization/identification of the bug and the repair. These two parts can be examined independently, e.g., Tarsel [52] uses hardware-specific timing information and the program spectrum and captures the changes of executed statements to locate faults effectively. Tarsel outperforms CirFix on CirFix's benchmarks as a fault localizer. In our work, we focus on the repair aspect. Our repair approach has the advantage that an oracle is not needed. While CirFix instruments an oracle to use the correct outputs to guide repairs, LLMs rely on the many examples of RTL code from training to produce a correct version of the buggy code. We compare our framework's performance with CirFix and discuss it in Section 5.6. \n\n\nBugs in Register Transfer Level design\n\nRegister Transfer Level (RTL) designs, typically coded in Hardware Description Languages (HDLs) such as Verilog, are high-level behavioral descriptions of hardware circuits specifying how data is transformed, transferred, and stored. RTL logic features two types of elements, sequential and combinational. Sequential elements (e.g., registers, counters, RAMs) tend to synchronize the circuit according to clock edges and retain values using memory components. Combinational logic (e.g., simple combinations of gates) change their outputs instantaneously according to the inputs. Whereas software code describes programs that will be executed from beginning to end, RTL specified in HDL describes hardware designs to be implemented. As hardware, components run independently in parallel. Like software, hardware designs have security bugs. By definition, RTL is insecure if the security objectives of the circuit are unmet. These may include confidentiality and integrity requirements [39]. Confidentiality is violated if data that should not be seen/read under certain conditions is exposed. For example, improper memory protection allows encryption keys to be read by user code. Integrity is violated if data that should not be modifiable under certain conditions is modifiable. For example, user code can write into registers that specify the access control policy. Secure computation is a concern, and the synthesis and optimization of secure circuits starts with the description of designs with HDLs [16]. Verisketch [8] defines a synthesis language to implement timingsensitive information flow properties to generate secure RTL.\n\n\nStatic Analysis\n\nStatic Analysis of code involves breaking down the code into its syntactic and lexical elements and exploring this information without simulating/compiling the code. This gives a lot of useful information, primarily in the form of an Abstract Syntax Tree (AST), which contains the variables, signals, operators, keywords, function definitions, parameters, and many other elements. Many tools have utilized static techniques in repair [10,20]. Static analysis is helpful for bug detection and repair as it can be done in the early stages of development. This is particularly beneficial in the hardware domain as once the RTL is synthesized and fabricated into a circuit in silicon, patches are not possible, and the cost of fixing the issue increases exponentially.\n\n\nCommon Weakness Enumerations\n\nMITRE [15] is a not-for-profit that works with academia and industry to come up with a list of Common Weakness Enumerations (CWEs) that represent categories of vulnerabilities in hardware and software. A weakness is an element in a digital product's software, firmware, hardware, or service that can be exploited for malicious purposes. The CWE list provides a general taxonomy and categorization of these elements that allow a common language to be used for discussion. It helps developers and researchers search for the existence of these weaknesses in their designs and compare various tools they use to detect vulnerabilities in their designs and products. In this work, we address a few CWEs that our designs contain. We identify a CWE that best describes the bug. 1234: Hardware Internal or Debug Modes Allow Override of Locks. System configuration controls, e.g., memory protection is set after a power reset and then locked to prevent modification. This is done using a lock-bit signal. If the system allows debugging operations and the lock-bit can be overridden in a debug mode, the system configuration controls are not properly protected.\n\n\n1271: Uninitialized Value on Reset for Registers Holding Security\n\nSettings. Security-critical information stored in registers should have a known value when being brought out of reset. If that is not the case, these registers may have unknown values that put the system in a vulnerable state.\n\n1280: Access Control Check Implemented After Asset is Accessed. Access control checks are required in hardware before securitysensitive assets like keys are accessed. If this check is implemented after the access, then the check is clearly useless.\n\n1276: Hardware Child Block Incorrectly Connected to Parent System. Hardware blocks are connected to a parent system that controls their inputs. If an input is incorrectly connected, affecting security attributes like resets while maintaining correct functionality; the integrity of the data of the child block can be violated.\n\n\n1245: Improper Finite State Machines (FSMs) in Hardware Logic.\n\nFSMs are used in hardware to carry out different functionality according to different states. When FSMs are used in modules that control the level of security a system is in, it becomes important that the FSM does not have any undefined states. These undefined states may allow an adversary to carry out functionality that requires higher privileges. An improper FSM can present itself as unreachable states, FSM deadlock, or missing states.\n\n\nPrompt Engineering\n\nPrompt engineering is crucial to the performance of an LLM. Careful prompt engineering outperforms the baseline LLM performances in natural language tasks [44,54]. A study exploring the use of Copilot [22] to solve CS1 level coding assignments has shown that tweaks to the prompt improve the performance from around 50% to 60% [17]. Prompt variations are also important in improving the results of text-to-image generation tasks [29,36]. Thus prompt engineering is crucial when using LLMs for code repair.\n\n\nDESIGNS AND BUGS\n\nTo explore the idea of using LLMs to fix HW security bugs, we first collate and prepare a set of benchmark designs, coming up with ten hardware security bugs from three sources. The sources are CWE descriptions on the MITRE website [15], OpenTitan Systemon-Chip (SoC) [1] and the Hack@DAC 2021 SoC [24]. Each bug is represented in a design, as described in Table 2.\n\n\nMITRE's CWEs\n\nWe use examples provided in MITRE's hardware design list to come up with simple designs that may represent CWE(s). The bugs and corresponding fixes for this source are shown in Figure 2. 3.1.1 Locked Register. This design has a register that is protected by a lock bit. The contents of the register may only be changed when the lock_status bit is low. In Figure 2(a), a debug_unlocked signal overrides the lock_status signal allowing the locked register to be written into even if lock_status is asserted.\n\n3.1.2 Lock on Reset. This design has a register that holds sensitive information. This register should be assigned a known value on reset. In Figure 2(b), the register locked should have a value assigned under reset, but in this case, there is no reset block.\n\n\nGrant Access.\n\nThis design contains a register that should only be modifiable if the usr_id input is correct. In Figure 2(c), the register data_out is assigned a new value if the grant_access signal is asserted. This should happen when usr_id is correct, but since the check happens after writing into data_out in blocking assignments, data_out may be modified when the usr_id is incorrect.\n\n3.1.4 Trustzone Peripheral. This design contains a peripheral instantiated in an SoC. To distinguish between trusted and untrusted entities, a signal is used to assign the security level of the peripheral. This is also described as a privilege bit used in Arm TrustZone to define the security level of all connected IPs. In Figure 2(d), the security level of the instantiated peripheral is grounded to zero, which could lead to incorrect privilege escalation of all input data.\n\n\nGoogle's OpenTitan\n\nOpenTitan is an open-source project designed to provide a silicon root of trust. It contains implementations of security measures that make the SoC secure. We inject bugs by tweaking the RTL of these security measures in different modules. The bugs and their corresponding fixes for this source are shown in Figure 3.\n\n3.2.1 ROM Control. This design contains a module that acts as an interface between the ROM and the system bus. The ROM has scrambled contents, and the controller descrambles the content for memory requests. We target the COMPARE.CTRL_FLOW.CONSISTENCY security measure in the rom_ctrl_compare module. A part of this measure is that the start_i signal should only be asserted in the Waiting state, otherwise, an alert signal is asserted. In Figure 3(a), because of our induced bug, the alert signal is incorrectly asserted when start_i is high in any state other than Waiting.\n\n3.2.2 OTP Control. This is a one-time programmable memory controller that provides the programmability for the device's life cycle. It ensures that the correct life cycle transitions are implemented 1 module l o c k e d _ r e g i s t e r ( i n p u t [ 1 5 : 0 ] D a t a _ i n , 2 i n p u t c l k , r e s e t n , w r i t e , l o c k _ s t a t u s , d e b u g _ u n l o c k e d , 3 o u t p u t r e g [ 1 5 : 0 ] D a t a _ o u t ) ; 4 a l w a y s @( p o s e d g e c l k o r n e g e d g e r e s e t n ) b e g i n 5 i f (~r e s e t n ) b e g i n 6 D a t a _ o u t <= 1 6 ' h0 000 ; (b) Lock on reset: Bug-register locked is not assigned a value under a reset condition. Fix-locked register is assigned 0 at reset. 1 module u s e r _ g r a n t _ a c c e s s ( d a t a _ o u t , u s r _ i d , d a t a _ i n , c l k , r s t _ n ) ; 2 o u t p u t r e g [ 7 : 0 ] d a t a _ o u t ; 3 i n p u t w i r e [ 2 : 0 ] u s r _ i d ; 4 i n p u t w i r e [ 7 : 0 ] d a t a _ i n ; 5 i n p u t w i r e c l k , r s t _ n ; 6 r e g g r a n t _ a c c e s s ; 7 a l w a y s @ ( p o s e d g e c l k o r n e g e d g e r s t _ n ) 8 b e g i n 9 i f ( ! r s t _ n ) d a t a _ o u t = 0 ; 10 e l s e b e g i n 11 d a t a _ o u t = ( g r a n t _ a c c e s s ) ? d a t a _ i n : d a t a _ o u t ; 12 g r a n t _ a c c e s s = ( u s r _ i d == 3 ' h4 ) ? 1 ' b1 : 1 ' b0 ; 13 g r a n t _ a c c e s s = ( u s r _ i d == 3 ' h4 ) ? 1 ' b1 : 1 ' b0 ; 14 d a t a _ o u t = ( g r a n t _ a c c e s s ) ? d a t a _ i n : d a t a _ o u t ; 15 end 16 end 17 endmodule (c) Grant access: Bug-grant_access signal is used before it is assigned a value. Fix-grant_access signal is used after it is assigned a value. t z _ p e r i p h e r a l u _ t z _ p e r i p h e r a l ( 6 . c l k ( c l k ) , . r s t _ n ( r s t _ n ) , . d a t a _ i n ( r d a t a ) , 7 . d a t a _ i n _ s e c u r i t y _ l e v e l ( 1 ' b0 ) , 8 . d a t a _ i n _ s e c u r i t y _ l e v e l ( r d a t a _ s e c u r i t y _ l e v e l ) , 9 .\n\n\nd a t a _ o u t ( d a t a _ o u t )\n\n) ; 10 endmodule (d) TZ peripheral: Bug-security level to peripheral is incorrectly grounded. Fix-security level for data is correctly assigned to parent signal. as the entity of the SoC changes among the 4 -Silicon Creator, Silicon Owner, Application Provider, and the End User. We target the LCI.FSM.LOCAL_ESC security measure in the otp_ctrl_lci module. A part of this measure is that the FSM jumps to an error state if the escalation signal is asserted. In Figure 3(b), no error is raised in such a case because of our induced bug. 1 / / s t a r t _ i s h o u l d o n l y be s i g n a l l e d when we ' r e i n t h e W a i t i n g s t a t e 2 / / SEC_CM : COMPARE . CTRL_FLOW . CONSISTENCY 3 l o g i c s t a r t _ a l e r t ; 4 a s s i g n s t a r t _ a l e r t = s t a r t _ i && ( s t a t e _ q ! = Done ) ; 5 a s s i g n s t a r t _ a l e r t = s t a r t _ i && ( s t a t e _ q ! = W a i t i n g ) ;\n\n(a) ROM Control: Bug-alert asserted when start is high in any state other than Done. Fix-alert asserted when start is high in any state other than Waiting.  The repair (green) replaces the bug (red) for a successful fix.\n\n\nKeymanager KMAC.\n\nThis design carries out the Keccak Message Authentication Code (KMAC) and Secure Hashing Algorithm 3 (SHA3) functionality. It is responsible for checking the integrity of the incoming message with the signature produced from the same secret key. We target the KMAC_IF_DONE.CTRL.CONSISTENCY security measure in the keymgr_kmac_if module. A part of this measure is that the kmac done signal should not be asserted outside the accepted window, i.e., when the FSM is in the done state. In Figure 3(c), because of our induced bug, the kmac done signal is incorrectly asserted in the transmission state StTx.\n\n\nHack@DAC-21\n\nHack@DAC-21 examples are bugs in the hardware designs for Hack@DAC 2021 CTF competition. Hack@DAC is a hackathon for finding vulnerabilities at the RTL level for a reasonably complex System-on-Chip (SoC). The bugs and their corresponding fixes for this source are shown in Figure 4.\n\n3.3.1 Csr regfile. This design contains a module that carries out changes in control and status registers according to the system's state. This includes changes in privilege levels, incoming interrupts, virtualization, and cache support. We consider the module's functionality pertaining to the stalling of the core in the case of receiving an interrupt and/or debug request. In Figure 4(a), the debug signal overrides interrupt signals.  performs read or write operations according to the Physical Memory Protection (PMP) configuration. We consider the PMP access mechanism as the relevant security implementation. In Figure 4(b), the pmp register is not assigned any value on reset.\n\n3.3.3 AES 2 Interface. This design instantiates the Advanced Encryption Standard (AES) module and outputs the cipher text to the system. It uses an FSM to interact with the AES (initialize, reset, and checking valid output). In Figure 4(c), the case statement has neither enough cases nor a default statement.\n\n\nEXPERIMENTAL METHOD\n\nTo test the capability of LLMs to generate successful repairs, we design experiments that use the designs and bugs detailed in Section 3. In this section we present our framework that automates the execution of our experiments, starting from the identification of bugs to the evaluation of the repairs.\n\n\nLLM-based Repair Evaluation Framework\n\nThe framework overview for our experiments is shown in Figure 5. It can be broken down into four components, i.e., the Sources, Detector, Repair Generator, and Evaluator. The Sources are discussed in Section 3, and the Detector, used for bugs from Hack@ DAC-21, is discussed in Section 4.2. and OpenTitan, we assume that the location of the bugs, i.e., starting and ending line numbers and the filepath of the buggy file, is known. For Hack@DAC, we run a bug detector tool that gives us the location and relevant CWE of the bugs as its outputs.\n\nFor each bug, we develop Instructions to assist repair. These are comments before and after the buggy code that assist the LLMs in generating an appropriate repair for that bug. The Prompt generator combines the code before the bug, buggy code in comments, and instructions to form the Prompt to LLM. This can be worded as 'what the LLM sees'. An example of this construction is shown in Figure 6    Functional Evaluation is done using custom testbenches we developed in Verilog. These are made for each design and contain tests to check for various input vectors. A failed testbench indicates a failure of at least one test or a syntax error in the design. For MITRE designs, we develop testbenches that cover the design's entire intended functionality. For OpenTitan and Hack@DAC designs, we cover partial functionality for inputs and outputs that pertain to the buggy code. These designs require an additional step of forming the Device Under Test (DUT) before simulation. This entails tracking the files instantiated by the buggy file and the files that need to be analyzed before the buggy file. This list of files is input to the simulator.\n\nSecurity Evaluation is done through a combination of custom testbenches (for MITRE and OpenTitan) and CWEAT (for Hack@DAC). For MITRE designs, the tests are designed according to the weaknesses mentioned on the MITRE website for each bug. For OpenTitan, we use the security countermeasures defined in their relevant .hjson files for the peripherals. It is difficult to verify the security countermeasure completely because that requires simulating the entire SoC through the software for Design Verification by OpenTitan. This method is still a work in progress for the Open-Titan team. The countermeasures that can currently be verified completely still require a lot of simulation time. Hence, we develop custom testbenches that verify very specific functionality for the bugs we introduce in the OpenTitan designs. For Hack@DAC, we employ CWEAT for security evaluation; this is discussed in Section 4.2.\n\nFunctional and Security Verification are not always mutually exclusive. There is often an overlap between the two, e.g., for CWE 1271 bugs, the security verification requires both a value on reset for the security-sensitive register and the correct lock mechanism. The latter is also a requirement for correct functionality. In the case of Bug 3, the functional verification is a subset of the security evaluation because the goal of the design is to grant user access under the correct input.\n\n\nEnd-to-end example with CWEAT\n\nWe present a demonstrative end-to-end framework for the detection and repair of some CWEs in Verilog. This includes the detection of the bug, the generation of repair using this detection, and the evaluation of the correctness of the repair generated. The elements of this pipeline are represented in hatched blocks in Figure 5.\n\nThe Detector used is a static analysis tool that has the capability to detect some weaknesses at the RTL. We use the methods described in [5] to traverse the Abstract Syntax Trees (AST)s generated by the Verific Verilog parser. There is one AST produced per module. Each node of the tree represents a syntactical element of the RTL code with various information about identifiers, types, values and conditions. The ASTs are traversed using keywords and patterns to indicate potential vulnerabilities in CWEs 1234, 1271, and 1245. We ran this tool over the Hack@DAC 2021 SoC and selected three instances, one per CWE, for the purposes of this paper. We use the same tool for security evaluation of the generated responses. We replace the buggy code with the repaired code in the SoC and run the tool again. If the same bug is picked up, i.e., the same location and CWE, we can determine that the repair is not successful. If that is not the case, we infer that the repair is adequate.\n\nWe envision the use of this (or similar) LLM-infused end-to-end solution by RTL designers as they write Hardware Description Language (HDL) code in the early stages of Hardware Design. CWEAT can highlight the potential weakness to the designer, run it through the LLM to produce repairs, choose the ones that are secure, and present those as suggestions to the designer.\n\n\nExperimental Parameters\n\nLLMs have several parameters that can be manipulated to produce responses. We change the prompt (as discussed in Section 4.1.1) according to the bug and instructions. We also vary the Instructions, Temperature and Models while keeping the top_p, num-ber_of_completions (n) and max_tokens constant at 1, 20 and 1 module u s e r _ g r a n t _ a c c e s s ( d a t a _ o u t , u s r _ i d , d a t a _ i n , c l k , r s t _ n ) ; 2 o u t p u t r e g [ 7 : 0 ] d a t a _ o u t ; 3 i n p u t w i r e [ 2 : 0 ] u s r _ i d ; 4 i n p u t w i r e [ 7 : 0 ] d a t a _ i n ; 5 i n p u t w i r e c l k , r s t _ n ; 6 r e g g r a n t _ a c c e s s ; 7 a l w a y s @ ( p o s e d g e c l k o r n e g e d g e r s t _ n ) b e g i n 8 i f ( ! r s t _ n ) d a t a _ o u t = 0 ; 9 e l s e b e g i n 10 d a t a _ o u t = ( g r a n t _ a c c e s s ) ? d a t a _ i n : d a t a _ o u t ; 11 g r a n t _ a c c e s s = ( u s r _ i d == 3 ' h4 ) ? 1 ' b1 : 1 ' b0 ;  13 / / d a t a _ o u t = ( g r a n t _ a c c e s s ) ? d a t a _ i n : d a t a _ o u t ; 14 / / g r a n t _ a c c e s s = ( u s r _ i d == 3 ' h4 ) ? 1 ' b1 : 1 ' b0 ; 15 \n\n\n/ / E n s u r e t h a t a c c e s s i s g r a n t e d b e f o r e d a t a i s 16 / / a c c e s s e d . 17 / / FIX :\n\n(c) Prompt-to-LLM: What the LLM gets as an input. 1 g r a n t _ a c c e s s = ( u s r _ i d == 3 ' h4 ) ? 1 ' b1 : 1 ' b0 ; 2 i f ( g r a n t _ a c c e s s ) d a t a _ o u t = d a t a _ i n ;\n\n(d) Correct repair produced by code-davinci-001 at temp=0.7. 1 d a t a _ o u t = ( g r a n t _ a c c e s s ) ? d a t a _ i n : d a t a _ o u t ; 2 g r a n t _ a c c e s s = ( u s r _ i d == 3 ' h4 ) ? 1 ' b1 : 1 ' b0 ;\n\n(e) Incorrect repair produced by code-davinci-001 at temp=0.5. This response passed the functional evaluation but failed the security evaluation.\n\n\ni f ( g r a n t _ a c c e s s ) d a t a _ o u t = d a t a _ i n ;\n\n(f) Incorrect repair produced by code-davinci-001 at temp=0.7. This response failed the functional and security evaluations. 1 module u s e r _ g r a n t _ a c c e s s ( d a t a _ o u t , u s r _ i d , d a t a _ i n , c l k , r s t _ n ) ; 2 o u t p u t r e g [ 7 : 0 ] d a t a _ o u t ; 3 i n p u t w i r e [ 2 : 0 ] u s r _ i d ; 4 i n p u t w i r e [ 7 : 0 ] d a t a _ i n ; 5 i n p u t w i r e c l k , r s t _ n ; 6 r e g g r a n t _ a c c e s s ; 7 a l w a y s @ ( p o s e d g e c l k o r n e g e d g e r s t _ n ) b e g i n 8 i f ( ! r s t _ n ) 9 d a t a _ o u t = 0 ; 10 e l s e b e g i n 11 g r a n t _ a c c e s s = ( u s r _ i d == 3 ' h4 ) ? 1 ' b1 : 1 ' b0 ; 12 i f ( g r a n t _ a c c e s s ) d a t a _ o u t = d a t a _ i n ; 13 end 14 end 15 endmodule (g) Generated repair file using 6(d). This is sent to the Evaluator for evaluation.  \n\n\nInstruction Variation .\n\nWe test five instruction variants to guide the repair of bugs. They are described in Table 3. Each variation has 2 parts -Bug Instruction and Fix Instruction. The former describes the nature of the bug and precedes the commented bug. The latter follows the bug in comments and represents guidance to the LLM on how to fix the bug. Variation a provides no assistance and is the same across all bugs. The Bug instruction is \"BUG:\" and the Fix Instruction is \"FIX:\". The Bug Instruction for the remaining variations is a description of the nature of the bug. We take inspiration from the MITRE website and cater them according to the CWE they represent. For variation e this description is appended with an example of a 'generalized' bug in comments and its fix without comments. This generalization is done through using more common signal names and coding patterns. The Fix Instruction for b and e is the same as that for a. For c, it is preceded by a 'prescriptive' instruction which means that natural language is used to assist the fix. For d, however, it is preceded by a 'descriptive' instruction which means that language resembling pseudo-code is used to assist the fix. The components of instruction that change are shown in Table 4.\n\n\nTemperature (t).\n\nA higher value means that the LLM takes more risks and yields more creative completions. We use \u2208 {0.1, 0.3.0.5, 0.7, 0.9}.\n\n\nModels.\n\nWe use four LLMs, three of which are made available by OpenAI [35] and one is an open-source model available through [34]. The OpenAI Codex models are derived from GPT-3 and were trained on millions of public GitHub repositories. They can ingest and generate code, and also translate natural language to code. We use/evaluate code-davinci-001, code-davinci-002 and code-cushman-001 models. From Hugging Face, we evaluate the model CodeGen-16B-multi, which we refer to as CodeGen in this work. It is an autoregressive language model for program synthesis trained sequentially on The Pile and BigQuery. Another parameter to consider is in the prompt preparation: the number of lines of existing code given to the LLM. Some files may be too large for the entire code before the bug to be sent to the LLM. We, therefore, select a minimum of 25 and a maximum of 50 lines of code before the bug as part of the prompt. In Figure 6(a), this would be lines 1-9 (inclusive). If there are more than 25 lines above the bug, we include enough lines that go up to the beginning of the block the bug is in. This block could be an always block, module, or case statement, etc.\n\n\nStop keywords.\n\nA stop keyword is specified to stop the response of the LLM (i.e., the response is considered finished when the stop keyword is generated by the model). They are not included in the response. We developed a strategy that works well with the set of bugs we have. The default stop keyword is endmodule. Additional keywords used are shown in the column Stop keywords in Table 4.\n\n\nEXPERIMENTAL RESULTS\n\nWe set up our experimental framework for each LLM, generating 20 responses for every combination of bug, temperature, and instruction variation. The responses are counted as successful repairs if they pass functional and security tests. The number of successful repairs is shown as heat-maps in Figure 7. The maximum value for each element is 20, i.e., when all responses were successful repairs.\n\n\nRQ1: Can LLMs fix hardware security bugs?\n\nThis work shows that LLMs can repair simple hardware bugs. code-davinci-001, code-davinci-002, and code-cushman-001 yielded at least one successful repair for every bug in our dataset.\n\nCodeGen was successful for 7 out of 10 bugs. In total, we requested 20,000 repairs out of which 6376 were correct, a success rate of 31.9%. The key here lies in selecting the best-observed parameters for each LLM. code-davinci-001 performs best at variation , 0.1 producing 71% correct repairs. code-davinci-002, code-cushman-001 and CodeGen perform best at ( , 0.1), ( , 0.1) and ( , 0.3 and 0.5) with success rates of 70%, 58% and 12% respectively. Performance of these LLMs across bugs is shown in Figure 8.\n\nWe can fine-tune the parameters for each bug. The choice of the right combination of model, instruction variations and temperature can yield near-perfect results. We present these best-observed settings in Table 5. Under these settings, Bug 7 has a success rate of 85% and the rest have a success rate of 100%. Table 5: Best-observed settings for each bug. 'dv1', 'dv2' and 'cus' stand for code-davinci-001, code-davinci-002 and code-cushman-001. Within the settings arrays, the first element is the LLM, the second is a set of instruction variations and the third is a set of temperatures. \n\n\nRQ2: What bugs are amenable to repair?\n\nThe cumulative number of correct repairs for each bug is shown in Figure 9. Bugs 3 and 4 were the best candidates for repair with success rates of over 50%. These are examples from MITRE where the signal names used clearly indicate their intended purposes. For the Grant Access module, the signals of concern are grant_access and usr_id used in successive lines. LLMs are able to interpret the intended functionality that the usr_id should be compared before granting access. Most successful repairs either flipped the order of blocking assignments or lumped them into an assignment using the ternary operator. Similarly, Trustzone Peripheral uses signal names data_in_security_level and rdata_security_level which illustrate their intended functionality. Bugs 2, 6, 7, and 9 were the hardest to repair with success rates of under 25%. Bugs 2 and 9 had the same bug of a register holding security settings not initialized under reset. This was difficult to repair because a fix required the creation of an always block with an appropriate reset as well as re-creating the previous intended functionality. Bug 7 was the hardest to repair because it was the only bug that required a line to be removed without replacement as a fix. We hypothesize that Bug 6 was hard to fix because it was difficult to phrase the fix instruction according to the description of the bug provided by Opentitan. The fix relies on asserting the fsm alert signal when escalation signal is high, but this condition is represented in code as if (escalate_en_i != lc_ctrl_pkg::Off) which is harder to grasp by the LLMs.\n\n\nRQ3: How should prompts be engineered\n\nto assist the repair of hardware bugs?\n\nThe 5 variations from to increase in the level of detail. In general, apart from CodeGen, all LLMs do better with more detail being provided to assist repair as shown in Figure 10(a). Variationsperform better than variations and . They include a fix instruction after the buggy code in comments, giving credence to the use of two separate instructions per prompt (one before and one after the bug in comments). Variation has the highest success rate among OpenAI LLMs and is therefore our recommendation for bug fixes. The use of a fix instruction in 'pseudo-code' fashion leads to the best results. There is variation within LLMs for the best-observed instruction variation, e.g., code-davinci-002 and CodeGen perform best at .\n\n\nRQ4: Does the temperature matter?\n\nA higher temperature allows the LLM to be more creative in its responses. As shown in Figure 10(b), the LLMs perform better at lower temperatures. All OpenAI LLMs perform best at = 0.1 and CodeGen performs best at 0.3. A lower temperature leads to less variation in responses as well, implying that the less creative responses are more likely to be correct repairs.\n\n\nRQ5: Are some LLMs better than others?\n\nThe code-davinci-002 LLM was the best performing, producing 2371 correct repairs out of 5000, giving it a success rate of 47.4%. code-davinci-001, code-cushman-001 and CodeGen had success rates of 40.4%, 33.1% and 6.68% respectively. The large difference between OpenAI LLMss and CodeGen is caused by CodeGen   being a much smaller LLM, having 16 billion parameters compared to the OpenAI LLMs that are based on GPT-3's \u223c175B parameters (the exact number of parameters for each of the OpenAI LLMs are not public). Additionally, code-cushman-001 is slightly inferior to the davinci LLMs because it was designed to be quicker. This may mean that it has fewer parameters or that it has been trained over less data or both. \n\n\nComparison with CirFix\n\nA comparison of our work with CirFix is shown in Table 6. We use the best-performing LLM (code-davinci-002) at = 0.1 and generate one repair each for variations and . This is done to closely mirror the use case of CirFix. By comparing the first example produced by the LLM, we evaluate only one attempt at repair. This attempt is manually evaluated for correctness. We use variation a for the primary comparison as this variation uses no instructions to assist repairs. This variation produces 17.5 correct repairs as compared to CirFix's 16. The half repair corresponds to fixing one numeric error out of 2 for the first benchmark. To elicit the power of LLMs, we use variation b which includes a description of the type of bug to assist repair. We use the brief descriptions of bugs provided in CirFix's GitHub repository. Variation fixes 20 of the 32 benchmarks and collectively, LLMs (both variations) are able to repair 23.5 of the bugs.\n\n\nDISCUSSION AND LIMITATIONS\n\nOur results show that LLMs have a lot of potential for bug repair. At the present, some assistance is required from the designer to identify the location and nature of the bug. This may be overcome by using other tools to localize the bugs and better design practices such as comments explaining the functionality of the design. Currently, the designer may also be needed to pick between a few options produced by the LLMs. This is where static analysis tools like CWEAT and other bug detection tools may come in to complete the loop by suggesting a repair that is correct to a high degree of confidence. A limitation of the work is the subjectivity of instruction variations. Although the Bug instructions devised are inspired by the descriptions in CWEs, the Fix instructions are devised according to the knowledge and experience of the authors. Our work reveals the importance of these variations as subtle changes can affect the response generated by LLMs. Devising 5 categories is an attempt Table 6: Comparison on CirFix benchmarks. A successful repair is shown as y. We use two instruction variations for this comparison. An element -| y means that the repair using variation was not successful but using variation was. The element 1/2 means that 2 errors were used in the description of a single fault/bug and 1 out of the 2 was successfully repaired. to standardize this process, but more varieties are probably needed to study their effects better. Moreover, instructions are challenging to generalize across different bugs. Ideally, a designer would want variation a to fix all bugs because no instructions are needed. But since more information is needed according to the particular instance of the bug for a higher probability of a successful fix, it is a challenge to form a small set of instructions, e.g., if LLMs are able to produce a successful fix with the Bug Instruction \"Improper FSM\" instead of \"FSM has an unreachable state\", that would be better.\n\nAnother limitation of the current framework is that the functional and security evaluations are not exhaustive. Security evaluation is dependent on the security objectives for the design and can not truly be exhaustive [4]. With this in mind, we limit the security evaluation to the particular bug that makes the design insecure. Ideally, efforts should be made to check that a fix does not result in another kind of bug. Functional evaluation is needed because a design that is secure but not functional is useless. For the CWE examples, we were able to build exhaustive testbenches because the designs were low in complexity and had only one or two modules. Ideally, the functional testbenches should be exhaustive for other examples too. But this would be very time-consuming as the size of the designs gets very large. It would be a difficult task to write testbenches for these complex SoCs and simulating the designs according to the software provided by OpenTitan and Hack@DAC takes a lot of time, e.g., design verification of OpenTitan examples takes \u223c10 minutes and it takes \u223c15 minutes to simulate the Hack@DAC-21 SoC. Therefore, we chose to build custom testbenches that test the code a repair could impact.\n\nThe use of end-tokens is another area of subjectivity that influences the success rate of repairs. Some strategies are intuitive like using the end line token as an end token for a bug that is present in only one line. Others may require more creativity because some lines of code can be written in multiple ways. A repair that spans multiple conditional statements, e.g., if (~resetn) begin locked <= 0; end else if(unlock) begin locked <= d; end else begin locked <= locked; end may not be completely produced if the keyword end is used as a stop token. On the other hand, not limiting a response with an appropriate stop token may mean that the LLM produces the correct repair but then adds more code that contains a syntax error or affects functionality. We use a post-processing script to minimize syntax errors. This involves adding/removing the end keyword as needed. When the LLM generates a repair, that repair is a substitute for the bug only. The number of begin and end keywords are counted. If the numbers are same, nothing is to be done, and the repair is inserted in place of the bug. If the number of begins are greater by an amount , end is added at the end of the repair times. If the number of ends are greater by an amount , the first instances of end are removed. The LLMs are very quick in generating repairs. The 20 responses per request are generated in under a minute. While trying to find a repair for a bug, a Verilog designer should have enough suggested repairs very quickly. The designer can then choose the best suggestion as the repair. In our experiments, we faced some challenge because of token limits set by the OpenAI API. Since we were generating thousands of requests with a limited number of token keys, we had to wait for a minute ever time we reached the limit. This raised our generation of repair time to \u223c20 minutes per LLM.\n\nTo evaluate security-related hardware bugs, a large number of benchmarks are needed that show these defects in designs. Our work takes a step in this direction. We believe more examples are needed to make more conclusive claims about repair techniques.\n\n\nCONCLUSIONS AND FUTURE WORK\n\nBy choosing the right parameters and providing the right prompts, LLMs can fix the hardware bugs in our corpus. All the bugs had at least one successful repair and 9 of the 10 had 100% correct responses with the best set of parameters. We have found that in instances where signal names and comments implicate the functionality, LLMs have a high success rate. Conversely, fixes that span more than 1 line or require the removal of a buggy line are harder to repair. Detailed instructions to assist repair tend to achieve higher success rates with variation using a Fix instruction that uses pseudo-codelike language performing the best. LLMs at lower temperatures and bigger LLMs perform better than LLMs at higher temperatures and LLMs with fewer parameters. LLMs do a better job at fixing functionrelated bugs in Verilog relative to the program repair mechanism in CirFix. We propose the following directions for future work:\n\n\u2022 Test a hybrid approach for security-related bugs. Use Linters, Formal Verification, fuzzing, fault localization, and static analysis tools for detection and LLMs, oracle-guided modifying algorithms for repair. An ensemble of these options is likely to have more success than one technique alone. \u2022 Fine-tune LLMs over HDLs and see if the performance improves. This improves the generation of functional code [45]. \u2022 Explore the repair of functional bugs using LLMs with the full sweep of parameters. We only used one set of parameters that performed the best in our experiments. \u2022 Build a database of security-related hardware bugs. Ongoing efforts like Trusthub's Vulnerability Database [3] can be consolidated with our examples to build standard benchmarks.\n\n\nRegister: Bug -debug signal overrides lock status signal. Fixremove debug signal in condition.\n\nFigure 2 :\n2MITRE CWE bugs and their corresponding repairs. The repair (green) replaces the bug (red) for a successful fix.\n\n\nControl: Bug-alert is not raised when escalation signal is high. Fix-fsm alert signal is asserted appropriately. KMAC: Bug-kmac done signal is prematurely asserted. Fix-do not assert done signal here.\n\nFigure 3 :\n3OpenTitan bugs and their corresponding repairs.\n\n\n3.3.2 DMA. This design contains the Direct Memory Access module common to all blocks. It uses the memory address as input and DMA: Bug-pmp enable register is not assigned a value on reset. Fixpmp enable register is assigned 0 on reset. Interface: Bug-Incomplete case statements. Fix-add default case.\n\nFigure 4 :\n4Hack@DAC-21 bugs and their corresponding repairs. The repair (green) replaces the bug (red) for a successful fix.\n\nFigure 5 :\n54.1.1 Repair Generator. This block takes the location and CWE of the bug as the input from the Source or the Detector. For MITRE Overview of the framework used in our experiments It is broken down into 4 main components. Sources are the designs containing bugs. Detector localizes the bug (for bugs[8][9][10]. Repair generator contains the LLM which generates the repairs. Evaluator verifies the success of the repair.\n\n\n(a)-(c) for the design Grant Access. The instructions are broken down into Bug Instruction and Fix Instruction. The former describes the nature of the bug and lets the LLM know that the bug follows. The latter follows the bug in comments and instructs the LLM on how to fix the bug. These instructions are varied in different degrees of detail according to the bug as discussed in Section 4.3.1. The Large Language Model takes the Prompt to LLM as input and outputs the Repairs. The repairs produced may be correct or incorrect. Some of the repairs generated using the promptFigure 6(c) are shown inFigure 6(d)-(f).\n\n\nThis block takes the Repairs generated by the LLM and verifies their correctness by evaluating their functionality and security. A repair is successful if it is both functional and secure. We use ModelSim simulator as a part of Xilinx Vivado 2022.2 to simulate the designs and custom testbenches.\n\n\nused to assist repair utilizing variation c. The Bug instruction is highlighted in yellow and the Fix instruction is highlighted in gray.\n\nFigure 6 :\n6Prompt to LLM and sample repairs produced for Bug 3 -Grant Access. Sub-figures (a)-(c) show how the bug is combined with instructions to generate the prompt that the LLM gets as one of its inputs. Sub-figures (d)-(f) show some actual repairs generated by an LLM.\n\n\n,c,(0.1,0.3,0.5)] [dv2,(c,d),0.1]\n\nFigure 7 :\n7Results for all 4 LLMs represented as heatmaps. The maximum value for each small box is 20. A higher value indicates more success by LLM in generating repair and is highlighted with a darker shade. All bugs were repaired at least once by at least one LLM.\n\nFigure 8 :\n8Results showing the performance of each LLM across all bugs in the form of heatmaps. Each small square shows the number of correct repairs for the corresponding instruction variation and temperature of the LLM. The maximum possible value is 200. A higher value indicates more success in generating repairs and is shaded in a darker color.\n\nFigure 9 :\n9Number of correct repairs per bug. The number above each bar shows the sum of successful repairs across all LLMs for the corresponding bug. The maximum possible value is 2000. A higher value indicates that the bug was repaired more times.\n\nFigure 10 :\n10Results: Trends Across Models. The top graph shows the number of correct repairs for LLMs for specified instruction variations. The bottom graph shows the number of correct repairs for LLMs for specified temperature. The maximum value for each data point is 1000.\n\nTable 1 :\n1Comparison with CirFix's approachCirFix [6] \nLLMs (e.g., this study) \n\nLocalization and repair Repair only (assumes location) \nOracle-guided \nNo oracle needed \nUses repair templates \nand operators \nUses instructions \n\nIterative process \nOne shot \n\n\n\nTable 2 :\n2Bugs Overview. We assign a CWE to each bug and give a description of the design. This register module supports a lock mode that blocks any writes after lock is set to 1. However, it also allows override of the lock protection when scan_mode or debug_unlocked modes are active.Bug Design \nCWE Source \nDescription \n\n1 \nLocked \nRegister \n1234 MITRE \n2 \nLock on \nReset \n1271 MITRE \nThis register module supports a lock mode that allows writes after unlock is set to 1. The locked register \ndoes not have a value assigned on reset and when the circuit is brought out of reset, the state will be unknown. \n\n3 \nGrant \nAccess \n1280 MITRE \nThis module allows register contents to be modified only when correct user id is used. \nHowever, the asset is allowed to be modified even before the access control check is complete. \n\n4 \nTrustzone \nPeripheral \n1276 MITRE \nThis module instantiates a peripheral within a SoC using a signal to distinguish between trusted and untrusted \nentities. However, this signal depicting the security level is incorrectly grounded. \n\n5 \nROM \nControl \n1245 OpenTitan \nThis module contains an FSM where an alert should be triggered if start signal is high in any state other than \nWaiting. However, the state is incorrectly compared to the Done state instead of the Waiting state. \n\n6 \nOTP \nControl \n1245 OpenTitan \nThe life cycle interface FSM should move into an invalid state upon global escalation via life cycle. \nHowever, the corresponding error signal for this transition is not asserted when the escalation signal is high. \n\n7 \nKeymanager \nKMAC \n1245 OpenTitan \nThis module has an FSM which has a done signal which should only be asserted at the time of completion. \nHowever, this signal is asserted outside of expected window, i.e., during a transmission state. \n\n8 \nCsr regfile \n1234 H@DAC-21 \nIf there is any interrupt pending or an incoming interrupt request is received, the core should be unstalled. \nIn this example, the core is also unstalled if there is a request to enter debug mode. \n\n9 \nDMA \n1271 H@DAC-21 \nThis module has a security sensitive register that controls whether the PMP (Physical Memory Protection) \nregister can be written into. This register should be assigned a value on reset but it is not. \n\n10 \nAES-2 \ninterface \n1245 H@DAC-21 \nThe FSM for AES 2 interface has a total of 15 states and does not include a default statement for its 4 bit \nstate variable. This represents an incomplete case statement of an FSM. \n\n\n\nTable 3 :\n3Instruction Variations. We develop 5 types to assist repair of bugs. Variation a is the base variation with no assistance. The level of detail/assistance increases from variation a to e.Instruction \nVariation \nDescription \n\na \nNo Instruction \nb \nNatural language description of bug \n\nc \nNatural language description of bug \nPrescriptive instruction of how to fix \n\nd \nNatural language description of bug \nDescriptive instruction of how to fix \ne \nCode examples of bug and fix \n\n200 respectively. top_p is an alternative to sampling with temper-\nature, called nucleus sampling, where only results with probability \nmass of top_p are considered. n is the number of completions \ngenerated by the LLM per request. max_tokens is the maximum \nnumber of tokens that can be generated per completion. \n\n\n\nTable 4 :\n4Details of Instruction Variations and Stop keywords used. The same Bug instruction is used for variations , , , shown in column 2. In case of variation , this Bug instruction (in column 2) is appended by an example of a bug and its repair in comments, shown in column 3. Fix instructions for variations c and d precede the string \"FIX:\", shown in columns 4 and 5 respectively. Additional stop keywords that terminate the further generation of tokens by LLMs are shown in column 6.Bug \nBug Instruction for \nvariations , , , \n\nBug Instruction appended \nfor variation \n\nFix Instruction \nfor variation \n\nFix Instruction \nfor variation \n\nStop \nkeywords \n\n1 \n\n// BUG: Hardware Internal \nor Debug Modes Allow \nOverride of Locks. \n\n// if (write & (\u223clock | debug) \n// FIX: \nif (write & \u223clock) \n\n// Debug or scan signals \nshould not be included \nin if condition \n\n// Write data if write \nsignal high and lock \nsignal is low \n\n'\\ ' \n\n2 \n\n// BUG: Uninitialized Value \non Reset for Registers \nHolding Security Settings. \n\n// if(unlock) q <= d_in; else q <= q; \n// FIX: \nif(\u223cresetn) q<=0; \nelse if(unlock) q <= d_in; else q <= q; \n\n// Ensure that the security \nsensitive lock register is \nassigned a value on reset. \n\n// Assign 0 to register \nwhen reset is low \n-\n\n3 \n\n// BUG: Access Control \nCheck Implemented \nAfter Asset is Accessed. \n\n// d = (access) ? d_in : d_out; \n// access = (id == 2'h2) ? 1'b1 : 1'b0; \n// FIX: \naccess = (id == 2'h2) ? 1'b1 : 1'b0; \nd = (access) ? d_in : d_out; \n\n// Ensure that access is \ngranted before data is \naccessed. \n\n// Assert access when \nid is correct. Then \nassign data to register \nif access is asserted. \n\nend \n\n4 \n\n// BUG: Hardware Child \nBlock Incorrectly Connected \nto Parent System \n\n// .in_security_level(1'b0), \n// FIX: \n.in_security_level(data_security_level), \n\n// The security level of \nthe child signal should \nmatch that of the parent \nsignal \n\n// assign data security \nlevel to input security \nlevel \n\n'\\ ' \n\n5 \n// BUG: Incorrect Alert \nMechanism \n\n// alert = start && (state!=FINISHED); \n// FIX: \nalert = start && (state!=IDLE); \n\n// An alert signal should \nbe set if an FSM is \ninstructed to start in a \nstate that is not idle \n\n// Assert alert signal \nif start signal is \nasserted and state is \nnot idle \n\n'\\ ' \n\n6 \n// BUG: Escalation does \nnot lead to fatal error \n\n// if (escalate_i != 0 ) begin \n// state_d = err_state; \n// FIX: \nif (escalate_i != 0 ) begin \nstate_d = err_state; fsm_err_o = 1'b1; \n\n// FSM should raise \nerror if system is in \nescalation \n\n// Assert error when \nescalation input is \nhigh \n\nend \n\n7 \n// BUG: Done signal is \nasserted prematurely \n\n// if (ready) begin done_vld = 1'b1; \n// FIX: \nif (ready) begin done_vld = 1'b0; \n\n// Do not assert done \nsignal in intermediate \nstates \n\n// assign zero to done \nsignal in ready state \nend \n\n8 \n\n// BUG: Hardware Internal \nor Debug Modes Allow \nOverride of Locks. \n\n// BUG: Hardware Internal \nor Debug Modes Allow \nOverride of Locks. \n\n// Debug or scan signals \nshould not be included \nin if condition \n\n// unstall core when \ninterrupt is high \n'\\ ' \n\n9 \n\n// BUG: Uninitialized Value \non Reset for Registers \nHolding Security Settings. \n\n// if(unlock) q <= d_in; else q <= q; \n// FIX: \nif(\u223cresetn) q<=0; \nelse if(unlock) q <= d_in; else q <= q; \n\n// Ensure that the security \nsensitive lock register is \nassigned a value on reset. \n\n// Assign 0 to register \nwhen reset is low \n-\n\n10 \n// BUG: Incomplete \ncase statement \n\n// endcase \n// FIX: \ndefault: begin \nstate <= s0; end endcase \n\n// Add a default case \nstatement \n\n// Write a default case \nstatement where \ninitial state is \nassigned to state \n\nendcase \n\n4.3.4 Number of lines before bug. \nmodule s o c ( c l k , r s t _ n , r d a t a , r d a t a _ s e c u r i t y _ l e v e l , d a t a _ o u t ) ; 2 i n p u t c l k , r s t _ n , r d a t a _ s e c u r i t y _ l e v e l ; 3 i n p u t [ 3 1 : 0 ] r d a t a ; 4 o u t p u t [ 3 1 : 0 ] d a t a _ o u t ;5 \n\nNicole Fern, and Alric Althoff. 2021. Special Session: CAD for Hardware Security -Automation is Key to Adoption of Solutions. Sohrab Aftabjahani, Ryan Kastner, Mark Tehranipoor, Farimah Farahmandi, Jason Oberg, Anders Nordstrom, 10.1109/VTS50974.2021.94410322021 IEEE 39th VLSI Test Symposium (VTS). 1-10. Sohrab Aftabjahani, Ryan Kastner, Mark Tehranipoor, Farimah Farah- mandi, Jason Oberg, Anders Nordstrom, Nicole Fern, and Alric Althoff. 2021. Special Session: CAD for Hardware Security -Automation is Key to Adoption of Solutions. In 2021 IEEE 39th VLSI Test Symposium (VTS). 1-10. https://doi.org/10.1109/VTS50974.2021.9441032 ISSN: 2375-1053.\n\nDon't CWEAT It: Toward CWE Analysis Techniques in Early Stages of Hardware Design. Baleegh Ahmad, Wei-Kai Liu, Luca Collini, Hammond Pearce, Jason M Fung, Jonathan Valamehr, Mohammad Bidmeshki, Piotr Sapiecha, Steve Brown, Krishnendu Chakrabarty, Ramesh Karri, Benjamin Tan, 10.1145/3508352.3549369Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design (ICCAD '22). the 41st IEEE/ACM International Conference on Computer-Aided Design (ICCAD '22)New York, NY, USAAssociation for Computing MachineryBaleegh Ahmad, Wei-Kai Liu, Luca Collini, Hammond Pearce, Jason M. Fung, Jonathan Valamehr, Mohammad Bidmeshki, Piotr Sapiecha, Steve Brown, Krishnendu Chakrabarty, Ramesh Karri, and Benjamin Tan. 2022. Don't CWEAT It: Toward CWE Analysis Techniques in Early Stages of Hardware Design. In Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design (ICCAD '22). Association for Computing Machinery, New York, NY, USA, 1-9. https://doi.org/10.1145/3508352.3549369\n\nCirFix: automatically repairing defects in hardware design code. Hammad Ahmad, Yu Huang, Westley Weimer, 10.1145/3503222.3507763Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '22). the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '22)New York, NY, USAAssociation for Computing MachineryHammad Ahmad, Yu Huang, and Westley Weimer. 2022. CirFix: automatically repairing defects in hardware design code. In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '22). Association for Computing Machinery, New York, NY, USA, 990-1003. https://doi.org/10.1145/3503222.3507763\n\nRegister transfer level information flow tracking for provably secure hardware design. Armaiti Ardeshiricham, Wei Hu, Joshua Marxen, Ryan Kastner, 10.23919/DATE.2017.7927266Design. Armaiti Ardeshiricham, Wei Hu, Joshua Marxen, and Ryan Kastner. 2017. Register transfer level information flow tracking for provably secure hardware design. In Design, Automation Test in Europe Conference Exhibition (DATE), 2017. 1691-1696. https://doi.org/10.23919/DATE.2017.7927266 ISSN: 1558-1101.\n\nVeriSketch: Synthesizing Secure Hardware Designs with Timing-Sensitive Information Flow Properties. Armaiti Ardeshiricham, Yoshiki Takashima, Sicun Gao, Ryan Kastner, 10.1145/3319535.3354246Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS '19). the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS '19)New York, NY, USAAssociation for Computing MachineryArmaiti Ardeshiricham, Yoshiki Takashima, Sicun Gao, and Ryan Kastner. 2019. VeriSketch: Synthesizing Secure Hardware Designs with Timing- Sensitive Information Flow Properties. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS '19). Association for Computing Machinery, New York, NY, USA, 1623-1638. https://doi.org/10.1145/3319535.3354246\n\nGetafix: learning to fix bugs automatically. Johannes Bader, Andrew Scott, Michael Pradel, Satish Chandra, 10.1145/3360585Proceedings of the ACM on Programming Languages. 3OOPSLAJohannes Bader, Andrew Scott, Michael Pradel, and Satish Chandra. 2019. Getafix: learning to fix bugs automatically. Proceedings of the ACM on Programming Languages 3, OOPSLA (Oct. 2019), 1-27. https://doi.org/10.1145/3360585\n\nTFix: Learning to Fix Coding Errors with a Text-to-Text Transformer. Berkay Berabi, Jingxuan He, Veselin Raychev, Martin Vechev, PMLR, 780-791Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningBerkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. 2021. TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer. In Proceedings of the 38th International Conference on Machine Learning. PMLR, 780-791. https://proceedings.mlr.press/v139/berabi21a.html ISSN: 2640-3498.\n\nYunjie Mohammad Mahdi Bidmeshki, Monir Zhang, Zaman, 10.1109/MDAT.2020.3013727Liwei Zhou, and Yiorgos Makris. 2021. Hunting Security Bugs in SoC Designs: Lessons Learned. 38Mohammad Mahdi Bidmeshki, Yunjie Zhang, Monir Zaman, Liwei Zhou, and Yiorgos Makris. 2021. Hunting Security Bugs in SoC Designs: Lessons Learned. IEEE Design & Test 38, 1 (Feb. 2021), 22-29. https:\n\n10.1109/MDAT.2020.3013727Conference Name: IEEE Design & Test. //doi.org/10.1109/MDAT.2020.3013727 Conference Name: IEEE Design & Test.\n\nCadence. 2022. Jasper RTL Apps | Cadence. Cadence. 2022. Jasper RTL Apps | Cadence. https://www.cadence.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, 10.48550/arXiv.2107.03374arXiv:2107.03374Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr; Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlishVedant MisraJosh Achiam. csMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. https://doi.org/10.48550/arXiv.2107.03374 arXiv:2107.03374 [cs].\n\nNeural Transfer Learning for Repairing Security Vulnerabilities in C Code. Zimin Chen, Steve Kommrusch, Martin Monperrus, 10.1109/TSE.2022.3147265Conference Name: IEEE Transactions on Software Engineering. 49Zimin Chen, Steve Kommrusch, and Martin Monperrus. 2023. Neural Transfer Learning for Repairing Security Vulnerabilities in C Code. IEEE Transactions on Software Engineering 49, 1 (Jan. 2023), 147-165. https://doi.org/10.1109/TSE. 2022.3147265 Conference Name: IEEE Transactions on Software Engineering.\n\nThe MITRE Corporation. 2022. CWE -CWE-1194: Hardware Design. 4.1The MITRE Corporation. 2022. CWE -CWE-1194: Hardware Design (4.1). https://cwe.mitre.org/data/definitions/1194.html\n\nAutomated Synthesis of Optimized Circuits for Secure Computation. Daniel Demmler, Ghada Dessouky, Farinaz Koushanfar, Ahmad-Reza Sadeghi, Thomas Schneider, Shaza Zeitouni, 10.1145/2810103.2813678Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS '15). the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS '15)New York, NY, USAAssociation for Computing MachineryDaniel Demmler, Ghada Dessouky, Farinaz Koushanfar, Ahmad-Reza Sadeghi, Thomas Schneider, and Shaza Zeitouni. 2015. Automated Synthesis of Optimized Circuits for Secure Computation. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS '15). Association for Computing Machinery, New York, NY, USA, 1504-1517. https://doi.org/10.1145/2810103.2813678\n\nConversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language. Paul Denny, Viraj Kumar, Nasser Giacaman, 10.48550/arXiv.2210.15157arXiv:2210.15157csPaul Denny, Viraj Kumar, and Nasser Giacaman. 2022. Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language. https://doi.org/10.48550/arXiv.2210.15157 arXiv:2210.15157 [cs].\n\nGhada Dessouky, David Gens, Patrick Haney, Garrett Persyn, Arun Kanuparthi, Hareesh Khattri, Jason Fung, Ahmad-Reza Sadeghi, Jeyavijayan Rajendran, Proceedings of the 28th USENIX Conference on Security Symposium (SEC'19). USENIX Association. the 28th USENIX Conference on Security Symposium (SEC'19). USENIX AssociationSanta Clara, CA, USAHardfails: Insights into Software-Exploitable Hardware BugsGhada Dessouky, David Gens, Patrick Haney, Garrett Persyn, Arun Kanuparthi, Hareesh Khattri, Jason Fung, Ahmad-Reza Sadeghi, and Jeyavijayan Rajendran. 2019. Hardfails: Insights into Software-Exploitable Hardware Bugs. In Proceed- ings of the 28th USENIX Conference on Security Symposium (SEC'19). USENIX Association, Santa Clara, CA, USA, 213-230.\n\nGenerating bug-fixes using pretrained transformers. Dawn Drain, Chen Wu, Alexey Svyatkovskiy, Neel Sundaresan, 10.1145/3460945.3464951Proceedings of the 5th ACM SIGPLAN International Symposium on Machine Programming (MAPS 2021. the 5th ACM SIGPLAN International Symposium on Machine Programming (MAPS 2021New York, NY, USA, 1-8.Association for Computing MachineryDawn Drain, Chen Wu, Alexey Svyatkovskiy, and Neel Sundaresan. 2021. Generating bug-fixes using pretrained transformers. In Proceedings of the 5th ACM SIGPLAN International Symposium on Machine Programming (MAPS 2021). Association for Computing Machinery, New York, NY, USA, 1-8. https://doi.org/10.1145/3460945.3464951\n\nSorald: Automatic Patch Suggestions for SonarQube Static Analysis Violations. Khashayar Etemadi, Nicolas Harrand, Simon Larsen, Haris Adzemovic, Henry Luong Phu, Ashutosh Verma, Fernanda Madeiral, Douglas Wikstrom, Martin Monperrus, 10.1109/TDSC.2022.3167316arXiv:2103.12033csKhashayar Etemadi, Nicolas Harrand, Simon Larsen, Haris Adzemovic, Henry Luong Phu, Ashutosh Verma, Fernanda Madeiral, Douglas Wikstrom, and Martin Monperrus. 2022. Sorald: Automatic Patch Suggestions for SonarQube Static Analysis Violations. https://doi.org/10.1109/TDSC.2022.3167316 arXiv:2103.12033 [cs].\n\nProcessor Hardware Security Vulnerabilities and their Detection by Unique Program Execution Checking. Mohammad Rahmani Fadiheh, Dominik Stoffel, Clark Barrett, Subhasish Mitra, Wolfgang Kunz, Mohammad Rahmani Fadiheh, Dominik Stoffel, Clark Barrett, Subhasish Mitra, and Wolfgang Kunz. 2019. Processor Hardware Security Vulnerabil- ities and their Detection by Unique Program Execution Checking. In 2019\n\nDesign, 10.23919/DATE.2019.8715004Europe Conference & Exhibition (DATE). Design, Automation & Test in Europe Conference & Exhibition (DATE). 994-999. https://doi.org/10.23919/DATE.2019.8715004 ISSN: 1558-1101.\n\nGitHub Copilot \u00b7 Your AI pair programmer. Github, GitHub. 2021. GitHub Copilot \u00b7 Your AI pair programmer. https: //copilot.github.com/\n\nAutomated program repair. Claire Le Goues, Michael Pradel, Abhik Roychoudhury, Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Au- tomated program repair.\n\n. Commun. ACM. 62Commun. ACM 62, 12 (Nov. 2019), 56-65.\n\n. 10.1145/3318162https://doi.org/10.1145/3318162\n\nHACK@EVENT. 2022. HACK@DAC21 -HacK@EVENT. HACK@EVENT. 2022. HACK@DAC21 -HacK@EVENT. https: //hackatevent.org/hackdac21/\n\nSpectre Attacks: Exploiting Speculative Execution. Paul Kocher, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, Moritz Lipp, Stefan Mangard, Thomas Prescher, Michael Schwarz, Yuval Yarom, 10.1109/SP.2019.000022019 IEEE Symposium on Security and Privacy (SP). 1-19. Paul Kocher, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, Moritz Lipp, Stefan Mangard, Thomas Prescher, Michael Schwarz, and Yuval Yarom. 2019. Spectre Attacks: Exploiting Speculative Execution. In 2019 IEEE Symposium on Security and Privacy (SP). 1-19. https://doi.org/10.1109/SP.2019.00002 ISSN: 2375-1207.\n\nReFixar: Multi-version Reasoning for Automated Repair of Regression Errors. D Xuan-Bach, Quang Loc Le, Le, 10.1109/ISSRE52982.2021.000282021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE). Xuan-Bach D. Le and Quang Loc Le. 2021. ReFixar: Multi-version Rea- soning for Automated Repair of Regression Errors. In 2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE). 162-172. https://doi.org/10.1109/ISSRE52982.2021.00028 ISSN: 2332-6549.\n\nMoritz Lipp, Michael Schwarz, Daniel Gruss, Thomas Prescher, Werner Haas, Anders Fogh, Jann Horn, Stefan Mangard, Paul Kocher, Meltdown: Reading Kernel Memory from User Space. 973-990. Daniel Genkin, Yuval Yarom, and Mike HamburgMoritz Lipp, Michael Schwarz, Daniel Gruss, Thomas Prescher, Werner Haas, Anders Fogh, Jann Horn, Stefan Mangard, Paul Kocher, Daniel Genkin, Yuval Yarom, and Mike Hamburg. 2018. Meltdown: Reading Kernel Memory from User Space. 973-990. https://www.usenix.org/conference/usenixsecurity18/ presentation/lipp\n\nDesign Guidelines for Prompt Engineering Text-to-Image Generative Models. Vivian Liu, Lydia B Chilton, 10.1145/3491102.3501825Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (CHI '22). the 2022 CHI Conference on Human Factors in Computing Systems (CHI '22)New York, NY, USAAssociation for Computing MachineryVivian Liu and Lydia B Chilton. 2022. Design Guidelines for Prompt Engineering Text-to-Image Generative Models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (CHI '22). Association for Computing Machinery, New York, NY, USA, 1-23. https://doi.org/10.1145/3491102.3501825\n\nFAPR: Fast and Accurate Program Repair for Introductory Programming Courses. Yunlong Lu, Na Meng, Wenxin Li, 10.48550/arXiv.2107.06550arXiv:2107.06550csYunlong Lu, Na Meng, and Wenxin Li. 2021. FAPR: Fast and Ac- curate Program Repair for Introductory Programming Courses. https://doi.org/10.48550/arXiv.2107.06550 arXiv:2107.06550 [cs].\n\nVuRLE: Automatic Vulnerability Detection and Repair by Learning from Examples. Siqi Ma, Ferdian Thung, David Lo, Cong Sun, Robert H Deng, 10.1007/978-3-319-66399-9_13Computer Security -ESORICS 2017. Simon N. Foley, Dieter Gollmann, and Einar SnekkenesChamSpringer International PublishingSiqi Ma, Ferdian Thung, David Lo, Cong Sun, and Robert H. Deng. 2017. VuRLE: Automatic Vulnerability Detection and Repair by Learning from Examples. In Computer Security -ESORICS 2017 (Lecture Notes in Computer Science), Simon N. Foley, Dieter Gollmann, and Einar Snekkenes (Eds.). Springer International Publishing, Cham, 229-246. https://doi.org/10.1007/978-3-319-66399-9_13\n\nThe Living Review on Automated Program Repair. Martin Monperrus, hal-01956501. HAL Archives OuvertesTechnical ReportMartin Monperrus. 2018. The Living Review on Automated Program Repair. Technical Report hal-01956501. HAL Archives Ouvertes.\n\nFarimah Farahmandi, Domenic Forte, and Mark Tehranipoor. 2020. SCRIPT: A CAD Framework for Power Side-channel Vulnerability Assessment Using Information Flow Tracking and Pattern Generation. Adib Nahiyan, Jungmin Park, Miao He, Yousef Iskander, 10.1145/3383445ACM Transactions on Design Automation of Electronic Systems. 253Adib Nahiyan, Jungmin Park, Miao He, Yousef Iskander, Farimah Farahmandi, Domenic Forte, and Mark Tehranipoor. 2020. SCRIPT: A CAD Framework for Power Side-channel Vulnerability Assessment Using Information Flow Tracking and Pattern Generation. ACM Transactions on Design Automation of Electronic Systems 25, 3 (May 2020), 26:1-26:27. https://doi.org/10.1145/3383445\n\nCodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, 10.48550/arXiv.2203.13474arXiv:2203.13474csErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. https://doi.org/10.48550/arXiv.2203.13474 arXiv:2203.13474 [cs].\n\nA Taxonomy of Prompt Modifiers for Text-To-Image Generation. Jonas Oppenlaender, 10.48550/arXiv.2204.13988arXiv:2204.13988csJonas Oppenlaender. 2022. A Taxonomy of Prompt Modifiers for Text-To-Image Generation. https://doi.org/10.48550/arXiv.2204.13988 arXiv:2204.13988 [cs].\n\nExamining Zero-Shot Vulnerability Repair with Large Language Models. Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, Brendan Dolan-Gavitt, 10.1109/SP46215.2023.00001IEEE Computer Society. Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2022. Examining Zero-Shot Vulnerabil- ity Repair with Large Language Models. IEEE Computer Society, 1-18. https://doi.org/10.1109/SP46215.2023.00001\n\nDAVE: Deriving Automatically Verilog from English. Hammond Pearce, Benjamin Tan, Ramesh Karri, 10.1145/3380446.34306342020 ACM/IEEE 2nd Workshop on Machine Learning for CAD (MLCAD). Hammond Pearce, Benjamin Tan, and Ramesh Karri. 2020. DAVE: Deriving Automatically Verilog from English. In 2020 ACM/IEEE 2nd Workshop on Machine Learning for CAD (MLCAD). 27-32. https://doi.org/10.1145/3380446.3430634\n\nHardware security in practice: Challenges and opportunities. Nachiketh Potlapally, 10.1109/HST.2011.59550032011 IEEE International Symposium on Hardware-Oriented Security and Trust. 93-98. Nachiketh Potlapally. 2011. Hardware security in practice: Challenges and opportunities. In 2011 IEEE International Symposium on Hardware-Oriented Security and Trust. 93-98. https://doi.org/10.1109/HST.2011.5955003\n\nPemma Reiter, Hui Jun Tay, Westley Weimer, Adam Doup\u00e9, Ruoyu Wang, Stephanie Forrest, arXiv:2202.12336Automatically Mitigating Vulnerabilities in x86 Binary Programs via Partially Recompilable Decompilation. csPemma Reiter, Hui Jun Tay, Westley Weimer, Adam Doup\u00e9, Ruoyu Wang, and Stephanie Forrest. 2022. Automatically Mitigating Vulnera- bilities in x86 Binary Programs via Partially Recompilable Decompilation. http://arxiv.org/abs/2202.12336 arXiv:2202.12336 [cs].\n\nAnonymized for review. 2023. Artifacts for \"Large Language Models Can Fix Hardware Security Bugs. 10.5281/zenodo.7540216Anonymized for review. 2023. Artifacts for \"Large Language Models Can Fix Hardware Security Bugs\". https://doi.org/10.5281/zenodo.7540216 Type: dataset.\n\nMasoud Rostami, Farinaz Koushanfar, Ramesh Karri, 10.1109/JPROC.2014.2335155A Primer on Hardware Security: Models, Methods, and Metrics. Proc. IEEE. 102Masoud Rostami, Farinaz Koushanfar, and Ramesh Karri. 2014. A Primer on Hardware Security: Models, Methods, and Metrics. Proc. IEEE 102, 8 (Aug. 2014), 1283-1295. https://doi.org/10.1109/JPROC.2014.2335155\n\nAutomated feedback generation for introductory programming assignments. Rishabh Singh, Sumit Gulwani, Armando Solar-Lezama, 10.1145/2491956.2462195Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI '13). the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI '13)New York, NY, USAAssociation for Computing MachineryRishabh Singh, Sumit Gulwani, and Armando Solar-Lezama. 2013. Automated feedback generation for introductory programming assignments. In Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI '13). Association for Computing Machinery, New York, NY, USA, 15-26. https://doi.org/10.1145/2491956.2462195\n\nInteractive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models. Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, Alexander M Rush, 10.1109/TVCG.2022.3209479Conference Name: IEEE Transactions on Visualization and Computer Graphics. 29Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, and Alexander M. Rush. 2023. Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models. IEEE Transactions on Visualization and Computer Graphics 29, 1 (Jan. 2023), 1146-1156. https://doi.org/10.1109/TVCG.2022.3209479 Conference Name: IEEE Transactions on Visualization and Computer Graphics.\n\nBenchmarking Large Language Models for Automated Verilog RTL Code Generation. Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, Siddharth Garg, 10.48550/arXiv.2212.11140arXiv:2212.11140csShailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. 2022. Bench- marking Large Language Models for Automated Verilog RTL Code Generation. https://doi.org/10.48550/arXiv.2212.11140 arXiv:2212.11140 [cs].\n\nFuzzing Hardware Like Software. Timothy Trippel, G Kang, Alex Shin, Garret Chernyakhovsky, Dominic Kelly, Matthew Rizzo, Hicks, 10.48550/arXiv.2102.02308arXiv:2102.02308csTimothy Trippel, Kang G. Shin, Alex Chernyakhovsky, Garret Kelly, Do- minic Rizzo, and Matthew Hicks. 2021. Fuzzing Hardware Like Software. https://doi.org/10.48550/arXiv.2102.02308 arXiv:2102.02308 [cs].\n\nAn Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation. Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, Denys Poshyvanyk, 10.1145/3340544ACM Transactions on Software Engineering and Methodology. 2829Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. 2019. An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation. ACM Trans- actions on Software Engineering and Methodology 28, 4 (Sept. 2019), 19:1-19:29. https://doi.org/10.1145/3340544\n\nAakash Tyagi, csAddison Crump, csAhmad-Reza Sadeghi, csGarrett Persyn, csJeyavijayan Rajendran, csPatrick Jauernig, csRahul Kande, csarXiv:2201.09941arXiv: 2201.09941TheHuzz: Instruction Fuzzing of Processors Using Golden-Reference Models for Finding Software-Exploitable Vulnerabilities. Aakash Tyagi, Addison Crump, Ahmad-Reza Sadeghi, Garrett Persyn, Jeyavijayan Rajendran, Patrick Jauernig, and Rahul Kande. 2022. TheHuzz: Instruction Fuzzing of Processors Using Golden-Reference Models for Find- ing Software-Exploitable Vulnerabilities. arXiv:2201.09941 [cs] (Jan. 2022). http://arxiv.org/abs/2201.09941 arXiv: 2201.09941.\n\nSynopsys VC SpyGlass Lint. vclint. 2022. Synopsys VC SpyGlass Lint. https://www.synopsys.com/ verification/static-and-formal-verification/vc-spyglass/vc-spyglass-lint.html\n\nLoopFix: an approach to automatic repair of buggy loops. Weichao Wang, Zhaopeng Meng, Zan Wang, Shuang Liu, Jianye Hao, 10.1016/j.jss.2019.06.076Journal of Systems and Software. 156CWeichao Wang, Zhaopeng Meng, Zan Wang, Shuang Liu, and Jianye Hao. 2019. LoopFix: an approach to automatic repair of buggy loops. Journal of Systems and Software 156, C (Oct. 2019), 100-112. https://doi.org/10.1016/j.jss.2019.06.076\n\nVarFix: balancing edit expressiveness and search effectiveness in automated program repair. Chu-Pan Wong, Priscila Santiesteban, Christian K\u00e4stner, Claire Le Goues, 10.1145/3468264.3468600Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2021). the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2021)New York, NY, USAAssociation for Computing MachineryChu-Pan Wong, Priscila Santiesteban, Christian K\u00e4stner, and Claire Le Goues. 2021. VarFix: balancing edit expressiveness and search effectiveness in automated program repair. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2021). Association for Computing Machinery, New York, NY, USA, 354-366. https://doi.org/10.1145/3468264.3468600\n\nFault Localization for Hardware Design Code with Time-Aware Program Spectrum. Jiang Wu, Zhuo Zhang, Deheng Yang, Xiankai Meng, Jiayu He, Xiaoguang Mao, Yan Lei, 10.1109/ICCD56317.2022.000852022 IEEE 40th International Conference on Computer Design (ICCD). Jiang Wu, Zhuo Zhang, Deheng Yang, Xiankai Meng, Jiayu He, Xiaoguang Mao, and Yan Lei. 2022. Fault Localization for Hardware Design Code with Time-Aware Program Spectrum. In 2022 IEEE 40th International Conference on Computer Design (ICCD). 537-544. https://doi.org/10.1109/ICCD56317.2022.00085 ISSN: 2576-6996.\n\nExample-Based Vulnerability Detection and Repair in Java Code. Ying Zhang, Ya Xiao, Md Mahir Asef, Kabir, Yao Danfeng, Na Meng, 10.48550/arXiv.2203.09009arXiv:2203.09009csYing Zhang, Ya Xiao, Md Mahir Asef Kabir, Danfeng, Yao, and Na Meng. 2022. Example-Based Vulnerability Detection and Repair in Java Code. https://doi.org/10.48550/arXiv.2203.09009 arXiv:2203.09009 [cs].\n\nopen-source access: \u2022 Verific: We used Verific libraries provided by Verific under an academic license. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.01910[cs].fullyLarge Language Models Are Human-Level Prompt Engineers. Please contact Verific to get access to their productsYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large Language Models Are Human-Level Prompt Engineers. http://arxiv.org/abs/2211.01910 arXiv:2211.01910 [cs]. fully open-source access: \u2022 Verific: We used Verific libraries provided by Verific under an academic license. Please contact Verific to get access to their products.\n\nPlease contact the authors for use/help with their codebase. \u2022 CirFix: We used the CirFix benchmarks and results provided in the open-source github repository provided by the authors of the paper \"CirFix: automatically repairing defects in hardware design code. \u2022 Cweat, 10.1145/3508352.3549369We requested CWEAT code from the authors of the paper \"Don't CWEAT It: Toward CWE Analysis Techniques in Early Stages of Hardware Design. The paper is avail. Please contact the authors about use of their tools. Their paper is available at\u2022 CWEAT: We requested CWEAT code from the authors of the paper \"Don't CWEAT It: Toward CWE Analysis Techniques in Early Stages of Hardware Design\" [5]. The paper is avail- able at https://dl.acm.org/doi/abs/10.1145/3508352.3549369. Please contact the authors for use/help with their codebase. \u2022 CirFix: We used the CirFix benchmarks and results provided in the open-source github repository provided by the au- thors of the paper \"CirFix: automatically repairing defects in hardware design code.\" [6] https://github.com/hammad- a/verilog_repair. Please contact the authors about use of their tools. Their paper is available at https://dl.acm.org/doi/10.1145/3503222.3507763.\n\nWe use the SoC used during the 2021 competition. Please contact them at info@hackatevent.org for more information/access. Soc \u2022 Hack@dac, \u2022 Hack@DAC SoC: We use the SoC used during the 2021 com- petition. Please contact them at info@hackatevent.org for more information/access.\n", "annotations": {"author": "[{\"end\":178,\"start\":60},{\"end\":298,\"start\":179},{\"end\":442,\"start\":299},{\"end\":575,\"start\":443},{\"end\":718,\"start\":576}]", "publisher": null, "author_last_name": "[{\"end\":73,\"start\":68},{\"end\":193,\"start\":187},{\"end\":311,\"start\":308},{\"end\":455,\"start\":450},{\"end\":590,\"start\":584}]", "author_first_name": "[{\"end\":67,\"start\":60},{\"end\":186,\"start\":179},{\"end\":307,\"start\":299},{\"end\":449,\"start\":443},{\"end\":583,\"start\":576}]", "author_affiliation": "[{\"end\":177,\"start\":75},{\"end\":297,\"start\":195},{\"end\":441,\"start\":339},{\"end\":574,\"start\":472},{\"end\":717,\"start\":615}]", "title": "[{\"end\":57,\"start\":1},{\"end\":775,\"start\":719}]", "venue": null, "abstract": "[{\"end\":2210,\"start\":965}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2416,\"start\":2413},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2419,\"start\":2416},{\"end\":2641,\"start\":2637},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2644,\"start\":2641},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2914,\"start\":2910},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3135,\"start\":3131},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3266,\"start\":3262},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3269,\"start\":3266},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3347,\"start\":3343},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3350,\"start\":3347},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3381,\"start\":3378},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3384,\"start\":3381},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3387,\"start\":3384},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3427,\"start\":3423},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3451,\"start\":3448},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3454,\"start\":3451},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3701,\"start\":3698},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4055,\"start\":4051},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4735,\"start\":4731},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4738,\"start\":4735},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4870,\"start\":4866},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4904,\"start\":4900},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4974,\"start\":4970},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5572,\"start\":5568},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5724,\"start\":5720},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5876,\"start\":5873},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6434,\"start\":6430},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6659,\"start\":6655},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6662,\"start\":6659},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6665,\"start\":6662},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6762,\"start\":6758},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6765,\"start\":6762},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7281,\"start\":7277},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7314,\"start\":7310},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7317,\"start\":7314},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7758,\"start\":7754},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7822,\"start\":7819},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8311,\"start\":8307},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9938,\"start\":9934},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10458,\"start\":10454},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10474,\"start\":10471},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11042,\"start\":11038},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11045,\"start\":11042},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11411,\"start\":11407},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14115,\"start\":14111},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":14118,\"start\":14115},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14161,\"start\":14157},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14287,\"start\":14283},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14389,\"start\":14385},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14392,\"start\":14389},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14718,\"start\":14714},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14784,\"start\":14780},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17930,\"start\":17929},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17962,\"start\":17961},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18537,\"start\":18536},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18603,\"start\":18601},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18688,\"start\":18686},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18763,\"start\":18761},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18838,\"start\":18836},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18923,\"start\":18921},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19232,\"start\":19231},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19293,\"start\":19292},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19387,\"start\":19386},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26443,\"start\":26440},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28203,\"start\":28202},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28401,\"start\":28400},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28551,\"start\":28549},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28627,\"start\":28625},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28716,\"start\":28714},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28795,\"start\":28793},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30075,\"start\":30074},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30097,\"start\":30096},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30143,\"start\":30141},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30218,\"start\":30216},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31942,\"start\":31938},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":41937,\"start\":41934},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":46433,\"start\":46429},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":48011,\"start\":48008},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":48014,\"start\":48011},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":48018,\"start\":48014},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":58126,\"start\":58125}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46877,\"start\":46781},{\"attributes\":{\"id\":\"fig_1\"},\"end\":47002,\"start\":46878},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47205,\"start\":47003},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47266,\"start\":47206},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47569,\"start\":47267},{\"attributes\":{\"id\":\"fig_5\"},\"end\":47696,\"start\":47570},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48128,\"start\":47697},{\"attributes\":{\"id\":\"fig_7\"},\"end\":48746,\"start\":48129},{\"attributes\":{\"id\":\"fig_8\"},\"end\":49045,\"start\":48747},{\"attributes\":{\"id\":\"fig_9\"},\"end\":49185,\"start\":49046},{\"attributes\":{\"id\":\"fig_10\"},\"end\":49461,\"start\":49186},{\"attributes\":{\"id\":\"fig_11\"},\"end\":49497,\"start\":49462},{\"attributes\":{\"id\":\"fig_12\"},\"end\":49766,\"start\":49498},{\"attributes\":{\"id\":\"fig_13\"},\"end\":50118,\"start\":49767},{\"attributes\":{\"id\":\"fig_14\"},\"end\":50370,\"start\":50119},{\"attributes\":{\"id\":\"fig_15\"},\"end\":50649,\"start\":50371},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":50910,\"start\":50650},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":53390,\"start\":50911},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":54197,\"start\":53391},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":57862,\"start\":54198}]", "paragraph": "[{\"end\":2551,\"start\":2226},{\"end\":3484,\"start\":2553},{\"end\":3547,\"start\":3517},{\"end\":3620,\"start\":3572},{\"end\":5056,\"start\":3622},{\"end\":5458,\"start\":5058},{\"end\":5573,\"start\":5460},{\"end\":6047,\"start\":5575},{\"end\":6304,\"start\":6079},{\"end\":7759,\"start\":6320},{\"end\":8907,\"start\":7761},{\"end\":10584,\"start\":8950},{\"end\":11368,\"start\":10604},{\"end\":12551,\"start\":11401},{\"end\":12847,\"start\":12621},{\"end\":13097,\"start\":12849},{\"end\":13425,\"start\":13099},{\"end\":13933,\"start\":13492},{\"end\":14461,\"start\":13956},{\"end\":14847,\"start\":14482},{\"end\":15369,\"start\":14864},{\"end\":15630,\"start\":15371},{\"end\":16023,\"start\":15648},{\"end\":16502,\"start\":16025},{\"end\":16842,\"start\":16525},{\"end\":17418,\"start\":16844},{\"end\":19389,\"start\":17420},{\"end\":20335,\"start\":19429},{\"end\":20557,\"start\":20337},{\"end\":21180,\"start\":20578},{\"end\":21478,\"start\":21196},{\"end\":22164,\"start\":21480},{\"end\":22475,\"start\":22166},{\"end\":22801,\"start\":22499},{\"end\":23387,\"start\":22843},{\"end\":24535,\"start\":23389},{\"end\":25443,\"start\":24537},{\"end\":25938,\"start\":25445},{\"end\":26300,\"start\":25972},{\"end\":27285,\"start\":26302},{\"end\":27657,\"start\":27287},{\"end\":28796,\"start\":27685},{\"end\":29107,\"start\":28916},{\"end\":29327,\"start\":29109},{\"end\":29474,\"start\":29329},{\"end\":30397,\"start\":29544},{\"end\":31665,\"start\":30425},{\"end\":31809,\"start\":31686},{\"end\":32981,\"start\":31821},{\"end\":33375,\"start\":33000},{\"end\":33796,\"start\":33400},{\"end\":34026,\"start\":33842},{\"end\":34538,\"start\":34028},{\"end\":35131,\"start\":34540},{\"end\":36766,\"start\":35174},{\"end\":36846,\"start\":36808},{\"end\":37576,\"start\":36848},{\"end\":37979,\"start\":37614},{\"end\":38742,\"start\":38022},{\"end\":39711,\"start\":38769},{\"end\":41713,\"start\":39742},{\"end\":42933,\"start\":41715},{\"end\":44804,\"start\":42935},{\"end\":45058,\"start\":44806},{\"end\":46017,\"start\":45090},{\"end\":46780,\"start\":46019}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":8169,\"start\":8162},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14846,\"start\":14839},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30517,\"start\":30510},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31664,\"start\":31657},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33374,\"start\":33367},{\"end\":34753,\"start\":34746},{\"end\":34858,\"start\":34851},{\"end\":38825,\"start\":38818},{\"end\":40746,\"start\":40739}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2224,\"start\":2212},{\"end\":3499,\"start\":3487},{\"end\":3515,\"start\":3502},{\"end\":3570,\"start\":3550},{\"attributes\":{\"n\":\"2\"},\"end\":6077,\"start\":6050},{\"attributes\":{\"n\":\"2.1\"},\"end\":6318,\"start\":6307},{\"attributes\":{\"n\":\"2.2\"},\"end\":8948,\"start\":8910},{\"attributes\":{\"n\":\"2.3\"},\"end\":10602,\"start\":10587},{\"attributes\":{\"n\":\"2.4\"},\"end\":11399,\"start\":11371},{\"end\":12619,\"start\":12554},{\"end\":13490,\"start\":13428},{\"attributes\":{\"n\":\"2.5\"},\"end\":13954,\"start\":13936},{\"attributes\":{\"n\":\"3\"},\"end\":14480,\"start\":14464},{\"attributes\":{\"n\":\"3.1\"},\"end\":14862,\"start\":14850},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":15646,\"start\":15633},{\"attributes\":{\"n\":\"3.2\"},\"end\":16523,\"start\":16505},{\"end\":19427,\"start\":19392},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":20576,\"start\":20560},{\"attributes\":{\"n\":\"3.3\"},\"end\":21194,\"start\":21183},{\"attributes\":{\"n\":\"4\"},\"end\":22497,\"start\":22478},{\"attributes\":{\"n\":\"4.1\"},\"end\":22841,\"start\":22804},{\"attributes\":{\"n\":\"4.2\"},\"end\":25970,\"start\":25941},{\"attributes\":{\"n\":\"4.3\"},\"end\":27683,\"start\":27660},{\"end\":28914,\"start\":28799},{\"attributes\":{\"n\":\"1\"},\"end\":29542,\"start\":29477},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":30423,\"start\":30400},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":31684,\"start\":31668},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":31819,\"start\":31812},{\"attributes\":{\"n\":\"4.3.5\"},\"end\":32998,\"start\":32984},{\"attributes\":{\"n\":\"5\"},\"end\":33398,\"start\":33378},{\"attributes\":{\"n\":\"5.1\"},\"end\":33840,\"start\":33799},{\"attributes\":{\"n\":\"5.2\"},\"end\":35172,\"start\":35134},{\"attributes\":{\"n\":\"5.3\"},\"end\":36806,\"start\":36769},{\"attributes\":{\"n\":\"5.4\"},\"end\":37612,\"start\":37579},{\"attributes\":{\"n\":\"5.5\"},\"end\":38020,\"start\":37982},{\"attributes\":{\"n\":\"5.6\"},\"end\":38767,\"start\":38745},{\"attributes\":{\"n\":\"6\"},\"end\":39740,\"start\":39714},{\"attributes\":{\"n\":\"7\"},\"end\":45088,\"start\":45061},{\"end\":46889,\"start\":46879},{\"end\":47217,\"start\":47207},{\"end\":47581,\"start\":47571},{\"end\":47708,\"start\":47698},{\"end\":49197,\"start\":49187},{\"end\":49509,\"start\":49499},{\"end\":49778,\"start\":49768},{\"end\":50130,\"start\":50120},{\"end\":50383,\"start\":50372},{\"end\":50660,\"start\":50651},{\"end\":50921,\"start\":50912},{\"end\":53401,\"start\":53392},{\"end\":54208,\"start\":54199}]", "table": "[{\"end\":50910,\"start\":50695},{\"end\":53390,\"start\":51199},{\"end\":54197,\"start\":53589},{\"end\":57862,\"start\":54690}]", "figure_caption": "[{\"end\":46877,\"start\":46783},{\"end\":47002,\"start\":46891},{\"end\":47205,\"start\":47005},{\"end\":47266,\"start\":47219},{\"end\":47569,\"start\":47269},{\"end\":47696,\"start\":47583},{\"end\":48128,\"start\":47710},{\"end\":48746,\"start\":48131},{\"end\":49045,\"start\":48749},{\"end\":49185,\"start\":49048},{\"end\":49461,\"start\":49199},{\"end\":49497,\"start\":49464},{\"end\":49766,\"start\":49511},{\"end\":50118,\"start\":49780},{\"end\":50370,\"start\":50132},{\"end\":50649,\"start\":50386},{\"end\":50695,\"start\":50662},{\"end\":51199,\"start\":50923},{\"end\":53589,\"start\":53403},{\"end\":54690,\"start\":54210}]", "figure_ref": "[{\"end\":5423,\"start\":5415},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15050,\"start\":15041},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15227,\"start\":15219},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15521,\"start\":15513},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15754,\"start\":15746},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16357,\"start\":16349},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16841,\"start\":16833},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17291,\"start\":17283},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19898,\"start\":19890},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21071,\"start\":21063},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21477,\"start\":21469},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21867,\"start\":21859},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22107,\"start\":22099},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22402,\"start\":22394},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22906,\"start\":22898},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":23785,\"start\":23777},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26299,\"start\":26291},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":32744,\"start\":32736},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":33703,\"start\":33695},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":34537,\"start\":34529},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":35248,\"start\":35240},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":37027,\"start\":37018},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":37709,\"start\":37700}]", "bib_author_first_name": "[{\"end\":58261,\"start\":58255},{\"end\":58279,\"start\":58275},{\"end\":58293,\"start\":58289},{\"end\":58314,\"start\":58307},{\"end\":58332,\"start\":58327},{\"end\":58346,\"start\":58340},{\"end\":58871,\"start\":58864},{\"end\":58886,\"start\":58879},{\"end\":58896,\"start\":58892},{\"end\":58913,\"start\":58906},{\"end\":58927,\"start\":58922},{\"end\":58929,\"start\":58928},{\"end\":58944,\"start\":58936},{\"end\":58963,\"start\":58955},{\"end\":58980,\"start\":58975},{\"end\":58996,\"start\":58991},{\"end\":59014,\"start\":59004},{\"end\":59034,\"start\":59028},{\"end\":59050,\"start\":59042},{\"end\":59864,\"start\":59858},{\"end\":59874,\"start\":59872},{\"end\":59889,\"start\":59882},{\"end\":60695,\"start\":60688},{\"end\":60714,\"start\":60711},{\"end\":60725,\"start\":60719},{\"end\":60738,\"start\":60734},{\"end\":61191,\"start\":61184},{\"end\":61214,\"start\":61207},{\"end\":61231,\"start\":61226},{\"end\":61241,\"start\":61237},{\"end\":61942,\"start\":61934},{\"end\":61956,\"start\":61950},{\"end\":61971,\"start\":61964},{\"end\":61986,\"start\":61980},{\"end\":62369,\"start\":62363},{\"end\":62386,\"start\":62378},{\"end\":62398,\"start\":62391},{\"end\":62414,\"start\":62408},{\"end\":62860,\"start\":62854},{\"end\":62892,\"start\":62887},{\"end\":63472,\"start\":63468},{\"end\":63484,\"start\":63479},{\"end\":63499,\"start\":63493},{\"end\":63511,\"start\":63505},{\"end\":63526,\"start\":63518},{\"end\":63557,\"start\":63552},{\"end\":63571,\"start\":63566},{\"end\":63585,\"start\":63581},{\"end\":63601,\"start\":63593},{\"end\":63614,\"start\":63610},{\"end\":63629,\"start\":63625},{\"end\":63639,\"start\":63635},{\"end\":63654,\"start\":63646},{\"end\":63671,\"start\":63664},{\"end\":63685,\"start\":63680},{\"end\":63700,\"start\":63694},{\"end\":63715,\"start\":63709},{\"end\":63731,\"start\":63725},{\"end\":63743,\"start\":63738},{\"end\":63754,\"start\":63750},{\"end\":63769,\"start\":63762},{\"end\":63785,\"start\":63778},{\"end\":63799,\"start\":63793},{\"end\":63816,\"start\":63808},{\"end\":63834,\"start\":63827},{\"end\":63851,\"start\":63843},{\"end\":65522,\"start\":65517},{\"end\":65534,\"start\":65529},{\"end\":65552,\"start\":65546},{\"end\":66208,\"start\":66202},{\"end\":66223,\"start\":66218},{\"end\":66241,\"start\":66234},{\"end\":66264,\"start\":66254},{\"end\":66280,\"start\":66274},{\"end\":66297,\"start\":66292},{\"end\":67057,\"start\":67053},{\"end\":67070,\"start\":67065},{\"end\":67084,\"start\":67078},{\"end\":67364,\"start\":67359},{\"end\":67380,\"start\":67375},{\"end\":67394,\"start\":67387},{\"end\":67409,\"start\":67402},{\"end\":67422,\"start\":67418},{\"end\":67442,\"start\":67435},{\"end\":67457,\"start\":67452},{\"end\":67474,\"start\":67464},{\"end\":67495,\"start\":67484},{\"end\":68163,\"start\":68159},{\"end\":68175,\"start\":68171},{\"end\":68186,\"start\":68180},{\"end\":68205,\"start\":68201},{\"end\":68878,\"start\":68869},{\"end\":68895,\"start\":68888},{\"end\":68910,\"start\":68905},{\"end\":68924,\"start\":68919},{\"end\":68941,\"start\":68936},{\"end\":68961,\"start\":68953},{\"end\":68977,\"start\":68969},{\"end\":68995,\"start\":68988},{\"end\":69012,\"start\":69006},{\"end\":69486,\"start\":69478},{\"end\":69511,\"start\":69504},{\"end\":69526,\"start\":69521},{\"end\":69545,\"start\":69536},{\"end\":69561,\"start\":69553},{\"end\":70160,\"start\":70154},{\"end\":70163,\"start\":70161},{\"end\":70178,\"start\":70171},{\"end\":70192,\"start\":70187},{\"end\":70582,\"start\":70578},{\"end\":70595,\"start\":70591},{\"end\":70608,\"start\":70602},{\"end\":70621,\"start\":70615},{\"end\":70636,\"start\":70630},{\"end\":70650,\"start\":70644},{\"end\":70661,\"start\":70657},{\"end\":70677,\"start\":70671},{\"end\":70690,\"start\":70684},{\"end\":70706,\"start\":70700},{\"end\":70724,\"start\":70717},{\"end\":70739,\"start\":70734},{\"end\":71246,\"start\":71245},{\"end\":71267,\"start\":71258},{\"end\":71673,\"start\":71667},{\"end\":71687,\"start\":71680},{\"end\":71703,\"start\":71697},{\"end\":71717,\"start\":71711},{\"end\":71734,\"start\":71728},{\"end\":71747,\"start\":71741},{\"end\":71758,\"start\":71754},{\"end\":71771,\"start\":71765},{\"end\":71785,\"start\":71781},{\"end\":72284,\"start\":72278},{\"end\":72295,\"start\":72290},{\"end\":72297,\"start\":72296},{\"end\":72931,\"start\":72924},{\"end\":72938,\"start\":72936},{\"end\":72951,\"start\":72945},{\"end\":73269,\"start\":73265},{\"end\":73281,\"start\":73274},{\"end\":73294,\"start\":73289},{\"end\":73303,\"start\":73299},{\"end\":73315,\"start\":73309},{\"end\":73317,\"start\":73316},{\"end\":73905,\"start\":73899},{\"end\":74289,\"start\":74285},{\"end\":74306,\"start\":74299},{\"end\":74317,\"start\":74313},{\"end\":74328,\"start\":74322},{\"end\":74872,\"start\":74868},{\"end\":74884,\"start\":74882},{\"end\":74898,\"start\":74891},{\"end\":74912,\"start\":74908},{\"end\":74921,\"start\":74917},{\"end\":74934,\"start\":74928},{\"end\":74947,\"start\":74941},{\"end\":74965,\"start\":74958},{\"end\":75345,\"start\":75340},{\"end\":75632,\"start\":75625},{\"end\":75649,\"start\":75641},{\"end\":75662,\"start\":75655},{\"end\":75676,\"start\":75670},{\"end\":75691,\"start\":75684},{\"end\":76048,\"start\":76041},{\"end\":76065,\"start\":76057},{\"end\":76077,\"start\":76071},{\"end\":76462,\"start\":76453},{\"end\":76802,\"start\":76797},{\"end\":76814,\"start\":76811},{\"end\":76818,\"start\":76815},{\"end\":76831,\"start\":76824},{\"end\":76844,\"start\":76840},{\"end\":76857,\"start\":76852},{\"end\":76873,\"start\":76864},{\"end\":77547,\"start\":77541},{\"end\":77564,\"start\":77557},{\"end\":77583,\"start\":77577},{\"end\":77979,\"start\":77972},{\"end\":77992,\"start\":77987},{\"end\":78009,\"start\":78002},{\"end\":78754,\"start\":78747},{\"end\":78771,\"start\":78765},{\"end\":78786,\"start\":78780},{\"end\":78801,\"start\":78793},{\"end\":78817,\"start\":78810},{\"end\":78834,\"start\":78825},{\"end\":78853,\"start\":78844},{\"end\":78855,\"start\":78854},{\"end\":79477,\"start\":79470},{\"end\":79493,\"start\":79486},{\"end\":79509,\"start\":79501},{\"end\":79522,\"start\":79515},{\"end\":79539,\"start\":79531},{\"end\":79551,\"start\":79545},{\"end\":79566,\"start\":79559},{\"end\":79590,\"start\":79581},{\"end\":79962,\"start\":79955},{\"end\":79973,\"start\":79972},{\"end\":79984,\"start\":79980},{\"end\":79997,\"start\":79991},{\"end\":80021,\"start\":80014},{\"end\":80036,\"start\":80029},{\"end\":80401,\"start\":80394},{\"end\":80414,\"start\":80410},{\"end\":80431,\"start\":80423},{\"end\":80452,\"start\":80440},{\"end\":80455,\"start\":80453},{\"end\":80469,\"start\":80463},{\"end\":80482,\"start\":80477},{\"end\":80907,\"start\":80901},{\"end\":80924,\"start\":80917},{\"end\":80944,\"start\":80934},{\"end\":80963,\"start\":80956},{\"end\":80985,\"start\":80974},{\"end\":81006,\"start\":80999},{\"end\":81024,\"start\":81019},{\"end\":81768,\"start\":81761},{\"end\":81783,\"start\":81775},{\"end\":81793,\"start\":81790},{\"end\":81806,\"start\":81800},{\"end\":81818,\"start\":81812},{\"end\":82219,\"start\":82212},{\"end\":82234,\"start\":82226},{\"end\":82258,\"start\":82249},{\"end\":82274,\"start\":82268},{\"end\":82277,\"start\":82275},{\"end\":83190,\"start\":83185},{\"end\":83199,\"start\":83195},{\"end\":83213,\"start\":83207},{\"end\":83227,\"start\":83220},{\"end\":83239,\"start\":83234},{\"end\":83253,\"start\":83244},{\"end\":83262,\"start\":83259},{\"end\":83743,\"start\":83739},{\"end\":83753,\"start\":83751},{\"end\":83762,\"start\":83760},{\"end\":83785,\"start\":83782},{\"end\":83797,\"start\":83795},{\"end\":84163,\"start\":84155},{\"end\":84176,\"start\":84170},{\"end\":84181,\"start\":84177},{\"end\":84197,\"start\":84192},{\"end\":84209,\"start\":84203},{\"end\":84224,\"start\":84218},{\"end\":84238,\"start\":84232},{\"end\":84250,\"start\":84245},{\"end\":85042,\"start\":85041},{\"end\":86112,\"start\":86109}]", "bib_author_last_name": "[{\"end\":58273,\"start\":58262},{\"end\":58287,\"start\":58280},{\"end\":58305,\"start\":58294},{\"end\":58325,\"start\":58315},{\"end\":58338,\"start\":58333},{\"end\":58356,\"start\":58347},{\"end\":58877,\"start\":58872},{\"end\":58890,\"start\":58887},{\"end\":58904,\"start\":58897},{\"end\":58920,\"start\":58914},{\"end\":58934,\"start\":58930},{\"end\":58953,\"start\":58945},{\"end\":58973,\"start\":58964},{\"end\":58989,\"start\":58981},{\"end\":59002,\"start\":58997},{\"end\":59026,\"start\":59015},{\"end\":59040,\"start\":59035},{\"end\":59054,\"start\":59051},{\"end\":59870,\"start\":59865},{\"end\":59880,\"start\":59875},{\"end\":59896,\"start\":59890},{\"end\":60709,\"start\":60696},{\"end\":60717,\"start\":60715},{\"end\":60732,\"start\":60726},{\"end\":60746,\"start\":60739},{\"end\":61205,\"start\":61192},{\"end\":61224,\"start\":61215},{\"end\":61235,\"start\":61232},{\"end\":61249,\"start\":61242},{\"end\":61948,\"start\":61943},{\"end\":61962,\"start\":61957},{\"end\":61978,\"start\":61972},{\"end\":61994,\"start\":61987},{\"end\":62376,\"start\":62370},{\"end\":62389,\"start\":62387},{\"end\":62406,\"start\":62399},{\"end\":62421,\"start\":62415},{\"end\":62885,\"start\":62861},{\"end\":62898,\"start\":62893},{\"end\":62905,\"start\":62900},{\"end\":63477,\"start\":63473},{\"end\":63491,\"start\":63485},{\"end\":63503,\"start\":63500},{\"end\":63516,\"start\":63512},{\"end\":63550,\"start\":63527},{\"end\":63564,\"start\":63558},{\"end\":63579,\"start\":63572},{\"end\":63591,\"start\":63586},{\"end\":63608,\"start\":63602},{\"end\":63623,\"start\":63615},{\"end\":63633,\"start\":63630},{\"end\":63644,\"start\":63640},{\"end\":63662,\"start\":63655},{\"end\":63678,\"start\":63672},{\"end\":63692,\"start\":63686},{\"end\":63707,\"start\":63701},{\"end\":63723,\"start\":63716},{\"end\":63736,\"start\":63732},{\"end\":63748,\"start\":63744},{\"end\":63760,\"start\":63755},{\"end\":63776,\"start\":63770},{\"end\":63791,\"start\":63786},{\"end\":63806,\"start\":63800},{\"end\":63825,\"start\":63817},{\"end\":63841,\"start\":63835},{\"end\":63858,\"start\":63852},{\"end\":65527,\"start\":65523},{\"end\":65544,\"start\":65535},{\"end\":65562,\"start\":65553},{\"end\":66216,\"start\":66209},{\"end\":66232,\"start\":66224},{\"end\":66252,\"start\":66242},{\"end\":66272,\"start\":66265},{\"end\":66290,\"start\":66281},{\"end\":66306,\"start\":66298},{\"end\":67063,\"start\":67058},{\"end\":67076,\"start\":67071},{\"end\":67093,\"start\":67085},{\"end\":67373,\"start\":67365},{\"end\":67385,\"start\":67381},{\"end\":67400,\"start\":67395},{\"end\":67416,\"start\":67410},{\"end\":67433,\"start\":67423},{\"end\":67450,\"start\":67443},{\"end\":67462,\"start\":67458},{\"end\":67482,\"start\":67475},{\"end\":67505,\"start\":67496},{\"end\":68169,\"start\":68164},{\"end\":68178,\"start\":68176},{\"end\":68199,\"start\":68187},{\"end\":68216,\"start\":68206},{\"end\":68886,\"start\":68879},{\"end\":68903,\"start\":68896},{\"end\":68917,\"start\":68911},{\"end\":68934,\"start\":68925},{\"end\":68951,\"start\":68942},{\"end\":68967,\"start\":68962},{\"end\":68986,\"start\":68978},{\"end\":69004,\"start\":68996},{\"end\":69022,\"start\":69013},{\"end\":69502,\"start\":69487},{\"end\":69519,\"start\":69512},{\"end\":69534,\"start\":69527},{\"end\":69551,\"start\":69546},{\"end\":69566,\"start\":69562},{\"end\":69787,\"start\":69781},{\"end\":70040,\"start\":70034},{\"end\":70169,\"start\":70164},{\"end\":70185,\"start\":70179},{\"end\":70205,\"start\":70193},{\"end\":70589,\"start\":70583},{\"end\":70600,\"start\":70596},{\"end\":70613,\"start\":70609},{\"end\":70628,\"start\":70622},{\"end\":70642,\"start\":70637},{\"end\":70655,\"start\":70651},{\"end\":70669,\"start\":70662},{\"end\":70682,\"start\":70678},{\"end\":70698,\"start\":70691},{\"end\":70715,\"start\":70707},{\"end\":70732,\"start\":70725},{\"end\":70745,\"start\":70740},{\"end\":71256,\"start\":71247},{\"end\":71270,\"start\":71268},{\"end\":71274,\"start\":71272},{\"end\":71678,\"start\":71674},{\"end\":71695,\"start\":71688},{\"end\":71709,\"start\":71704},{\"end\":71726,\"start\":71718},{\"end\":71739,\"start\":71735},{\"end\":71752,\"start\":71748},{\"end\":71763,\"start\":71759},{\"end\":71779,\"start\":71772},{\"end\":71792,\"start\":71786},{\"end\":72288,\"start\":72285},{\"end\":72305,\"start\":72298},{\"end\":72934,\"start\":72932},{\"end\":72943,\"start\":72939},{\"end\":72954,\"start\":72952},{\"end\":73272,\"start\":73270},{\"end\":73287,\"start\":73282},{\"end\":73297,\"start\":73295},{\"end\":73307,\"start\":73304},{\"end\":73322,\"start\":73318},{\"end\":73915,\"start\":73906},{\"end\":74297,\"start\":74290},{\"end\":74311,\"start\":74307},{\"end\":74320,\"start\":74318},{\"end\":74337,\"start\":74329},{\"end\":74880,\"start\":74873},{\"end\":74889,\"start\":74885},{\"end\":74906,\"start\":74899},{\"end\":74915,\"start\":74913},{\"end\":74926,\"start\":74922},{\"end\":74939,\"start\":74935},{\"end\":74956,\"start\":74948},{\"end\":74971,\"start\":74966},{\"end\":75358,\"start\":75346},{\"end\":75639,\"start\":75633},{\"end\":75653,\"start\":75650},{\"end\":75668,\"start\":75663},{\"end\":75682,\"start\":75677},{\"end\":75704,\"start\":75692},{\"end\":76055,\"start\":76049},{\"end\":76069,\"start\":76066},{\"end\":76083,\"start\":76078},{\"end\":76473,\"start\":76463},{\"end\":76809,\"start\":76803},{\"end\":76822,\"start\":76819},{\"end\":76838,\"start\":76832},{\"end\":76850,\"start\":76845},{\"end\":76862,\"start\":76858},{\"end\":76881,\"start\":76874},{\"end\":77555,\"start\":77548},{\"end\":77575,\"start\":77565},{\"end\":77589,\"start\":77584},{\"end\":77985,\"start\":77980},{\"end\":78000,\"start\":77993},{\"end\":78022,\"start\":78010},{\"end\":78763,\"start\":78755},{\"end\":78778,\"start\":78772},{\"end\":78791,\"start\":78787},{\"end\":78808,\"start\":78802},{\"end\":78823,\"start\":78818},{\"end\":78842,\"start\":78835},{\"end\":78860,\"start\":78856},{\"end\":79484,\"start\":79478},{\"end\":79499,\"start\":79494},{\"end\":79513,\"start\":79510},{\"end\":79529,\"start\":79523},{\"end\":79543,\"start\":79540},{\"end\":79557,\"start\":79552},{\"end\":79579,\"start\":79567},{\"end\":79595,\"start\":79591},{\"end\":79970,\"start\":79963},{\"end\":79978,\"start\":79974},{\"end\":79989,\"start\":79985},{\"end\":80012,\"start\":79998},{\"end\":80027,\"start\":80022},{\"end\":80042,\"start\":80037},{\"end\":80049,\"start\":80044},{\"end\":80408,\"start\":80402},{\"end\":80421,\"start\":80415},{\"end\":80438,\"start\":80432},{\"end\":80461,\"start\":80456},{\"end\":80475,\"start\":80470},{\"end\":80493,\"start\":80483},{\"end\":80913,\"start\":80908},{\"end\":80930,\"start\":80925},{\"end\":80952,\"start\":80945},{\"end\":80970,\"start\":80964},{\"end\":80995,\"start\":80986},{\"end\":81015,\"start\":81007},{\"end\":81030,\"start\":81025},{\"end\":81773,\"start\":81769},{\"end\":81788,\"start\":81784},{\"end\":81798,\"start\":81794},{\"end\":81810,\"start\":81807},{\"end\":81822,\"start\":81819},{\"end\":82224,\"start\":82220},{\"end\":82247,\"start\":82235},{\"end\":82266,\"start\":82259},{\"end\":82283,\"start\":82278},{\"end\":83193,\"start\":83191},{\"end\":83205,\"start\":83200},{\"end\":83218,\"start\":83214},{\"end\":83232,\"start\":83228},{\"end\":83242,\"start\":83240},{\"end\":83257,\"start\":83254},{\"end\":83266,\"start\":83263},{\"end\":83749,\"start\":83744},{\"end\":83758,\"start\":83754},{\"end\":83773,\"start\":83763},{\"end\":83780,\"start\":83775},{\"end\":83793,\"start\":83786},{\"end\":83802,\"start\":83798},{\"end\":84168,\"start\":84164},{\"end\":84190,\"start\":84182},{\"end\":84201,\"start\":84198},{\"end\":84216,\"start\":84210},{\"end\":84230,\"start\":84225},{\"end\":84243,\"start\":84239},{\"end\":84253,\"start\":84251},{\"end\":85048,\"start\":85043},{\"end\":86123,\"start\":86113}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1109/VTS50974.2021.9441032\",\"id\":\"b0\",\"matched_paper_id\":235099021},\"end\":58779,\"start\":58129},{\"attributes\":{\"doi\":\"10.1145/3508352.3549369\",\"id\":\"b1\",\"matched_paper_id\":252089688},\"end\":59791,\"start\":58781},{\"attributes\":{\"doi\":\"10.1145/3503222.3507763\",\"id\":\"b2\",\"matched_paper_id\":246995687},\"end\":60599,\"start\":59793},{\"attributes\":{\"doi\":\"10.23919/DATE.2017.7927266\",\"id\":\"b3\",\"matched_paper_id\":3913824},\"end\":61082,\"start\":60601},{\"attributes\":{\"doi\":\"10.1145/3319535.3354246\",\"id\":\"b4\",\"matched_paper_id\":202726540},\"end\":61887,\"start\":61084},{\"attributes\":{\"doi\":\"10.1145/3360585\",\"id\":\"b5\",\"matched_paper_id\":67750638},\"end\":62292,\"start\":61889},{\"attributes\":{\"doi\":\"PMLR, 780-791\",\"id\":\"b6\",\"matched_paper_id\":235672358},\"end\":62852,\"start\":62294},{\"attributes\":{\"doi\":\"10.1109/MDAT.2020.3013727\",\"id\":\"b7\"},\"end\":63224,\"start\":62854},{\"attributes\":{\"doi\":\"10.1109/MDAT.2020.3013727\",\"id\":\"b8\"},\"end\":63360,\"start\":63226},{\"attributes\":{\"id\":\"b9\"},\"end\":63466,\"start\":63362},{\"attributes\":{\"doi\":\"10.48550/arXiv.2107.03374\",\"id\":\"b10\"},\"end\":65440,\"start\":63468},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":233296471},\"end\":65953,\"start\":65442},{\"attributes\":{\"id\":\"b12\"},\"end\":66134,\"start\":65955},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15732723},\"end\":66948,\"start\":66136},{\"attributes\":{\"id\":\"b14\"},\"end\":67357,\"start\":66950},{\"attributes\":{\"id\":\"b15\"},\"end\":68105,\"start\":67359},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":233289653},\"end\":68789,\"start\":68107},{\"attributes\":{\"id\":\"b17\"},\"end\":69374,\"start\":68791},{\"attributes\":{\"id\":\"b18\"},\"end\":69779,\"start\":69376},{\"attributes\":{\"id\":\"b19\"},\"end\":69990,\"start\":69781},{\"attributes\":{\"id\":\"b20\"},\"end\":70126,\"start\":69992},{\"attributes\":{\"id\":\"b21\"},\"end\":70297,\"start\":70128},{\"attributes\":{\"id\":\"b22\"},\"end\":70354,\"start\":70299},{\"attributes\":{\"id\":\"b23\"},\"end\":70404,\"start\":70356},{\"attributes\":{\"id\":\"b24\"},\"end\":70525,\"start\":70406},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":373888},\"end\":71167,\"start\":70527},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":246753316},\"end\":71665,\"start\":71169},{\"attributes\":{\"id\":\"b27\"},\"end\":72202,\"start\":71667},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":237513697},\"end\":72845,\"start\":72204},{\"attributes\":{\"id\":\"b29\"},\"end\":73184,\"start\":72847},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":12983722},\"end\":73850,\"start\":73186},{\"attributes\":{\"id\":\"b31\"},\"end\":74092,\"start\":73852},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":218771241},\"end\":74784,\"start\":74094},{\"attributes\":{\"id\":\"b33\"},\"end\":75277,\"start\":74786},{\"attributes\":{\"id\":\"b34\"},\"end\":75554,\"start\":75279},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":251563966},\"end\":75988,\"start\":75556},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":221445942},\"end\":76390,\"start\":75990},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6514712},\"end\":76795,\"start\":76392},{\"attributes\":{\"id\":\"b38\"},\"end\":77265,\"start\":76797},{\"attributes\":{\"id\":\"b39\"},\"end\":77539,\"start\":77267},{\"attributes\":{\"id\":\"b40\"},\"end\":77898,\"start\":77541},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":7918911},\"end\":78648,\"start\":77900},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":251514845},\"end\":79390,\"start\":78650},{\"attributes\":{\"id\":\"b43\"},\"end\":79921,\"start\":79392},{\"attributes\":{\"id\":\"b44\"},\"end\":80298,\"start\":79923},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":56517510},\"end\":80899,\"start\":80300},{\"attributes\":{\"id\":\"b46\"},\"end\":81529,\"start\":80901},{\"attributes\":{\"id\":\"b47\"},\"end\":81702,\"start\":81531},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":196188038},\"end\":82118,\"start\":81704},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":237205309},\"end\":83105,\"start\":82120},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":254903290},\"end\":83674,\"start\":83107},{\"attributes\":{\"id\":\"b51\"},\"end\":84049,\"start\":83676},{\"attributes\":{\"id\":\"b52\"},\"end\":84777,\"start\":84051},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":246995687},\"end\":85985,\"start\":84779},{\"attributes\":{\"id\":\"b54\"},\"end\":86264,\"start\":85987}]", "bib_title": "[{\"end\":58253,\"start\":58129},{\"end\":58862,\"start\":58781},{\"end\":59856,\"start\":59793},{\"end\":60686,\"start\":60601},{\"end\":61182,\"start\":61084},{\"end\":61932,\"start\":61889},{\"end\":62361,\"start\":62294},{\"end\":65515,\"start\":65442},{\"end\":66200,\"start\":66136},{\"end\":68157,\"start\":68107},{\"end\":70576,\"start\":70527},{\"end\":71243,\"start\":71169},{\"end\":72276,\"start\":72204},{\"end\":73263,\"start\":73186},{\"end\":74283,\"start\":74094},{\"end\":75623,\"start\":75556},{\"end\":76039,\"start\":75990},{\"end\":76451,\"start\":76392},{\"end\":77970,\"start\":77900},{\"end\":78745,\"start\":78650},{\"end\":80392,\"start\":80300},{\"end\":81759,\"start\":81704},{\"end\":82210,\"start\":82120},{\"end\":83183,\"start\":83107},{\"end\":85039,\"start\":84779}]", "bib_author": "[{\"end\":58275,\"start\":58255},{\"end\":58289,\"start\":58275},{\"end\":58307,\"start\":58289},{\"end\":58327,\"start\":58307},{\"end\":58340,\"start\":58327},{\"end\":58358,\"start\":58340},{\"end\":58879,\"start\":58864},{\"end\":58892,\"start\":58879},{\"end\":58906,\"start\":58892},{\"end\":58922,\"start\":58906},{\"end\":58936,\"start\":58922},{\"end\":58955,\"start\":58936},{\"end\":58975,\"start\":58955},{\"end\":58991,\"start\":58975},{\"end\":59004,\"start\":58991},{\"end\":59028,\"start\":59004},{\"end\":59042,\"start\":59028},{\"end\":59056,\"start\":59042},{\"end\":59872,\"start\":59858},{\"end\":59882,\"start\":59872},{\"end\":59898,\"start\":59882},{\"end\":60711,\"start\":60688},{\"end\":60719,\"start\":60711},{\"end\":60734,\"start\":60719},{\"end\":60748,\"start\":60734},{\"end\":61207,\"start\":61184},{\"end\":61226,\"start\":61207},{\"end\":61237,\"start\":61226},{\"end\":61251,\"start\":61237},{\"end\":61950,\"start\":61934},{\"end\":61964,\"start\":61950},{\"end\":61980,\"start\":61964},{\"end\":61996,\"start\":61980},{\"end\":62378,\"start\":62363},{\"end\":62391,\"start\":62378},{\"end\":62408,\"start\":62391},{\"end\":62423,\"start\":62408},{\"end\":62887,\"start\":62854},{\"end\":62900,\"start\":62887},{\"end\":62907,\"start\":62900},{\"end\":63479,\"start\":63468},{\"end\":63493,\"start\":63479},{\"end\":63505,\"start\":63493},{\"end\":63518,\"start\":63505},{\"end\":63552,\"start\":63518},{\"end\":63566,\"start\":63552},{\"end\":63581,\"start\":63566},{\"end\":63593,\"start\":63581},{\"end\":63610,\"start\":63593},{\"end\":63625,\"start\":63610},{\"end\":63635,\"start\":63625},{\"end\":63646,\"start\":63635},{\"end\":63664,\"start\":63646},{\"end\":63680,\"start\":63664},{\"end\":63694,\"start\":63680},{\"end\":63709,\"start\":63694},{\"end\":63725,\"start\":63709},{\"end\":63738,\"start\":63725},{\"end\":63750,\"start\":63738},{\"end\":63762,\"start\":63750},{\"end\":63778,\"start\":63762},{\"end\":63793,\"start\":63778},{\"end\":63808,\"start\":63793},{\"end\":63827,\"start\":63808},{\"end\":63843,\"start\":63827},{\"end\":63860,\"start\":63843},{\"end\":65529,\"start\":65517},{\"end\":65546,\"start\":65529},{\"end\":65564,\"start\":65546},{\"end\":66218,\"start\":66202},{\"end\":66234,\"start\":66218},{\"end\":66254,\"start\":66234},{\"end\":66274,\"start\":66254},{\"end\":66292,\"start\":66274},{\"end\":66308,\"start\":66292},{\"end\":67065,\"start\":67053},{\"end\":67078,\"start\":67065},{\"end\":67095,\"start\":67078},{\"end\":67375,\"start\":67359},{\"end\":67387,\"start\":67375},{\"end\":67402,\"start\":67387},{\"end\":67418,\"start\":67402},{\"end\":67435,\"start\":67418},{\"end\":67452,\"start\":67435},{\"end\":67464,\"start\":67452},{\"end\":67484,\"start\":67464},{\"end\":67507,\"start\":67484},{\"end\":68171,\"start\":68159},{\"end\":68180,\"start\":68171},{\"end\":68201,\"start\":68180},{\"end\":68218,\"start\":68201},{\"end\":68888,\"start\":68869},{\"end\":68905,\"start\":68888},{\"end\":68919,\"start\":68905},{\"end\":68936,\"start\":68919},{\"end\":68953,\"start\":68936},{\"end\":68969,\"start\":68953},{\"end\":68988,\"start\":68969},{\"end\":69006,\"start\":68988},{\"end\":69024,\"start\":69006},{\"end\":69504,\"start\":69478},{\"end\":69521,\"start\":69504},{\"end\":69536,\"start\":69521},{\"end\":69553,\"start\":69536},{\"end\":69568,\"start\":69553},{\"end\":69789,\"start\":69781},{\"end\":70042,\"start\":70034},{\"end\":70171,\"start\":70154},{\"end\":70187,\"start\":70171},{\"end\":70207,\"start\":70187},{\"end\":70591,\"start\":70578},{\"end\":70602,\"start\":70591},{\"end\":70615,\"start\":70602},{\"end\":70630,\"start\":70615},{\"end\":70644,\"start\":70630},{\"end\":70657,\"start\":70644},{\"end\":70671,\"start\":70657},{\"end\":70684,\"start\":70671},{\"end\":70700,\"start\":70684},{\"end\":70717,\"start\":70700},{\"end\":70734,\"start\":70717},{\"end\":70747,\"start\":70734},{\"end\":71258,\"start\":71245},{\"end\":71272,\"start\":71258},{\"end\":71276,\"start\":71272},{\"end\":71680,\"start\":71667},{\"end\":71697,\"start\":71680},{\"end\":71711,\"start\":71697},{\"end\":71728,\"start\":71711},{\"end\":71741,\"start\":71728},{\"end\":71754,\"start\":71741},{\"end\":71765,\"start\":71754},{\"end\":71781,\"start\":71765},{\"end\":71794,\"start\":71781},{\"end\":72290,\"start\":72278},{\"end\":72307,\"start\":72290},{\"end\":72936,\"start\":72924},{\"end\":72945,\"start\":72936},{\"end\":72956,\"start\":72945},{\"end\":73274,\"start\":73265},{\"end\":73289,\"start\":73274},{\"end\":73299,\"start\":73289},{\"end\":73309,\"start\":73299},{\"end\":73324,\"start\":73309},{\"end\":73917,\"start\":73899},{\"end\":74299,\"start\":74285},{\"end\":74313,\"start\":74299},{\"end\":74322,\"start\":74313},{\"end\":74339,\"start\":74322},{\"end\":74882,\"start\":74868},{\"end\":74891,\"start\":74882},{\"end\":74908,\"start\":74891},{\"end\":74917,\"start\":74908},{\"end\":74928,\"start\":74917},{\"end\":74941,\"start\":74928},{\"end\":74958,\"start\":74941},{\"end\":74973,\"start\":74958},{\"end\":75360,\"start\":75340},{\"end\":75641,\"start\":75625},{\"end\":75655,\"start\":75641},{\"end\":75670,\"start\":75655},{\"end\":75684,\"start\":75670},{\"end\":75706,\"start\":75684},{\"end\":76057,\"start\":76041},{\"end\":76071,\"start\":76057},{\"end\":76085,\"start\":76071},{\"end\":76475,\"start\":76453},{\"end\":76811,\"start\":76797},{\"end\":76824,\"start\":76811},{\"end\":76840,\"start\":76824},{\"end\":76852,\"start\":76840},{\"end\":76864,\"start\":76852},{\"end\":76883,\"start\":76864},{\"end\":77557,\"start\":77541},{\"end\":77577,\"start\":77557},{\"end\":77591,\"start\":77577},{\"end\":77987,\"start\":77972},{\"end\":78002,\"start\":77987},{\"end\":78024,\"start\":78002},{\"end\":78765,\"start\":78747},{\"end\":78780,\"start\":78765},{\"end\":78793,\"start\":78780},{\"end\":78810,\"start\":78793},{\"end\":78825,\"start\":78810},{\"end\":78844,\"start\":78825},{\"end\":78862,\"start\":78844},{\"end\":79486,\"start\":79470},{\"end\":79501,\"start\":79486},{\"end\":79515,\"start\":79501},{\"end\":79531,\"start\":79515},{\"end\":79545,\"start\":79531},{\"end\":79559,\"start\":79545},{\"end\":79581,\"start\":79559},{\"end\":79597,\"start\":79581},{\"end\":79972,\"start\":79955},{\"end\":79980,\"start\":79972},{\"end\":79991,\"start\":79980},{\"end\":80014,\"start\":79991},{\"end\":80029,\"start\":80014},{\"end\":80044,\"start\":80029},{\"end\":80051,\"start\":80044},{\"end\":80410,\"start\":80394},{\"end\":80423,\"start\":80410},{\"end\":80440,\"start\":80423},{\"end\":80463,\"start\":80440},{\"end\":80477,\"start\":80463},{\"end\":80495,\"start\":80477},{\"end\":80917,\"start\":80901},{\"end\":80934,\"start\":80917},{\"end\":80956,\"start\":80934},{\"end\":80974,\"start\":80956},{\"end\":80999,\"start\":80974},{\"end\":81019,\"start\":80999},{\"end\":81034,\"start\":81019},{\"end\":81775,\"start\":81761},{\"end\":81790,\"start\":81775},{\"end\":81800,\"start\":81790},{\"end\":81812,\"start\":81800},{\"end\":81824,\"start\":81812},{\"end\":82226,\"start\":82212},{\"end\":82249,\"start\":82226},{\"end\":82268,\"start\":82249},{\"end\":82285,\"start\":82268},{\"end\":83195,\"start\":83185},{\"end\":83207,\"start\":83195},{\"end\":83220,\"start\":83207},{\"end\":83234,\"start\":83220},{\"end\":83244,\"start\":83234},{\"end\":83259,\"start\":83244},{\"end\":83268,\"start\":83259},{\"end\":83751,\"start\":83739},{\"end\":83760,\"start\":83751},{\"end\":83775,\"start\":83760},{\"end\":83782,\"start\":83775},{\"end\":83795,\"start\":83782},{\"end\":83804,\"start\":83795},{\"end\":84170,\"start\":84155},{\"end\":84192,\"start\":84170},{\"end\":84203,\"start\":84192},{\"end\":84218,\"start\":84203},{\"end\":84232,\"start\":84218},{\"end\":84245,\"start\":84232},{\"end\":84255,\"start\":84245},{\"end\":85050,\"start\":85041},{\"end\":86125,\"start\":86109}]", "bib_venue": "[{\"end\":59271,\"start\":59175},{\"end\":60201,\"start\":60061},{\"end\":61468,\"start\":61371},{\"end\":62559,\"start\":62506},{\"end\":64398,\"start\":63995},{\"end\":66525,\"start\":66428},{\"end\":67698,\"start\":67601},{\"end\":68435,\"start\":68335},{\"end\":71896,\"start\":71852},{\"end\":72506,\"start\":72418},{\"end\":73441,\"start\":73437},{\"end\":78265,\"start\":78156},{\"end\":82628,\"start\":82468},{\"end\":58433,\"start\":58387},{\"end\":59173,\"start\":59079},{\"end\":60059,\"start\":59921},{\"end\":60780,\"start\":60774},{\"end\":61369,\"start\":61274},{\"end\":62058,\"start\":62011},{\"end\":62504,\"start\":62436},{\"end\":63023,\"start\":62932},{\"end\":63286,\"start\":63251},{\"end\":63402,\"start\":63362},{\"end\":63993,\"start\":63901},{\"end\":65646,\"start\":65588},{\"end\":66014,\"start\":65955},{\"end\":66426,\"start\":66331},{\"end\":67051,\"start\":66950},{\"end\":67599,\"start\":67507},{\"end\":68333,\"start\":68241},{\"end\":68867,\"start\":68791},{\"end\":69476,\"start\":69376},{\"end\":69852,\"start\":69815},{\"end\":70032,\"start\":69992},{\"end\":70152,\"start\":70128},{\"end\":70312,\"start\":70301},{\"end\":70446,\"start\":70406},{\"end\":70822,\"start\":70768},{\"end\":71387,\"start\":71305},{\"end\":71850,\"start\":71794},{\"end\":72416,\"start\":72330},{\"end\":72922,\"start\":72847},{\"end\":73383,\"start\":73352},{\"end\":73897,\"start\":73852},{\"end\":74413,\"start\":74354},{\"end\":74866,\"start\":74786},{\"end\":75338,\"start\":75279},{\"end\":75753,\"start\":75732},{\"end\":76170,\"start\":76108},{\"end\":76579,\"start\":76499},{\"end\":77003,\"start\":76899},{\"end\":77363,\"start\":77267},{\"end\":77688,\"start\":77617},{\"end\":78154,\"start\":78047},{\"end\":78960,\"start\":78887},{\"end\":79468,\"start\":79392},{\"end\":79953,\"start\":79923},{\"end\":80566,\"start\":80510},{\"end\":81188,\"start\":81067},{\"end\":81556,\"start\":81531},{\"end\":81880,\"start\":81849},{\"end\":82466,\"start\":82308},{\"end\":83361,\"start\":83296},{\"end\":83737,\"start\":83676},{\"end\":84153,\"start\":84051},{\"end\":85209,\"start\":85073},{\"end\":86107,\"start\":85987}]"}}}, "year": 2023, "month": 12, "day": 17}