{"id": 211171605, "updated": "2023-10-06 18:33:52.855", "metadata": {"title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "authors": "[{\"first\":\"Zhangyin\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Daya\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Duyu\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Nan\",\"last\":\"Duan\",\"middle\":[]},{\"first\":\"Xiaocheng\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Ming\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Linjun\",\"last\":\"Shou\",\"middle\":[]},{\"first\":\"Bing\",\"last\":\"Qin\",\"middle\":[]},{\"first\":\"Ting\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Daxin\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Ming\",\"last\":\"Zhou\",\"middle\":[]}]", "venue": "FINDINGS", "journal": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both \u201cbimodal\u201d data of NL-PL pairs and \u201cunimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2002.08155", "mag": "3098605233", "acl": "2020.findings-emnlp.139", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/FengGTDFGS0LJZ20", "doi": "10.18653/v1/2020.findings-emnlp.139"}}, "content": {"source": {"pdf_hash": "4c1b21f529e5533580155713e0ed441f5bf6e4b8", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2020.findings-emnlp.139.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.findings-emnlp.139.pdf", "status": "HYBRID"}}, "grobid": {"id": "833ef31d93fcd216f71776b447942257086cabc7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4c1b21f529e5533580155713e0ed441f5bf6e4b8.txt", "contents": "\nCodeBERT: A Pre-Trained Model for Programming and Natural Languages\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsNovember 16 -20, 2020. 2020\n\nZhangyin Feng zyfeng@ir.hit.edu.cn \nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\nChina\n\nDaya Guo \nThe School of Data and Computer Science\nSun Yat-sen University\nChina\n\nDuyu Tang \nMicrosoft Research Asia\nBeijingChina\n\nNan Duan \nMicrosoft Research Asia\nBeijingChina\n\nXiaocheng Feng xcfeng@ir.hit.edu.cn \nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\nChina\n\nMing Gong \nMicrosoft Search Technology Center Asia\nBeijingChina\n\nLinjun Shou \nMicrosoft Search Technology Center Asia\nBeijingChina\n\nBing Qin qinb@ir.hit.edu.cn \nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\nChina\n\nTing Liu tliu@ir.hit.edu.cn \nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\nChina\n\nDaxin Jiang \nMicrosoft Search Technology Center Asia\nBeijingChina\n\nMing Zhou mingzhou@microsoft.com \nMicrosoft Research Asia\nBeijingChina\n\nCodeBERT: A Pre-Trained Model for Programming and Natural Languages\n\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\nthe 2020 Conference on Empirical Methods in Natural Language Processing: FindingsAssociation for Computational LinguisticsNovember 16 -20, 2020. 20201536\nWe present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop Code-BERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both \"bimodal\" data of NL-PL pairs and \"unimodal\" data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing. 1\n\nIntroduction\n\nLarge pre-trained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019) * Work done while this author was an intern at Microsoft Research Asia. 1 All the codes and data are available at https:// github.com/microsoft/CodeBERT and RoBERTa  have dramatically improved the state-of-the-art on a variety of natural language processing (NLP) tasks. These pre-trained models learn effective contextual representations from massive unlabeled text optimized by self-supervised objectives, such as masked language modeling, which predicts the original masked word from an artificially masked input sequence. The success of pre-trained models in NLP also drives a surge of multi-modal pre-trained models, such as ViLBERT (Lu et al., 2019) for language-image and VideoBERT (Sun et al., 2019) for language-video, which are learned from bimodal data such as language-image pairs with bimodal self-supervised objectives.\n\nIn this work, we present CodeBERT, a bimodal pre-trained model for natural language (NL) and programming language (PL) like Python, Java, JavaScript, etc. CodeBERT captures the semantic connection between natural language and programming language, and produces general-purpose representations that can broadly support NL-PL understanding tasks (e.g. natural language code search) and generation tasks (e.g. code documentation generation). It is developed with the multilayer Transformer (Vaswani et al., 2017), which is adopted in a majority of large pre-trained models. In order to make use of both bimodal instances of NL-PL pairs and large amount of available unimodal codes, we train CodeBERT with a hybrid objective function, including standard masked language modeling (Devlin et al., 2018) and replaced token detection (Clark et al., 2020), where unimodal codes help to learn better generators for producing better alternative tokens for the latter objective.\n\nWe train CodeBERT from Github code reposito-ries in 6 programming languages, where bimodal datapoints are codes that pair with function-level natural language documentations (Husain et al., 2019). Training is conducted in a setting similar to that of multilingual BERT (Pires et al., 2019), in which case one pre-trained model is learned for 6 programming languages with no explicit markers used to denote the input programming language. We evaluate CodeBERT on two downstream NL-PL tasks, including natural language code search and code documentation generation. Results show that fine-tuning the parameters of CodeBERT achieves state-of-the-art performance on both tasks. To further investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and test CodeBERT in a zero-shot scenario, i.e. without fine-tuning the parameters of CodeBERT. We find that CodeBERT consistently outperforms RoBERTa, a purely natural language-based pre-trained model. The contributions of this work are as follows:\n\n\u2022 CodeBERT is the first large NL-PL pretrained model for multiple programming languages.\n\n\u2022 Empirical results show that CodeBERT is effective in both code search and code-to-text generation tasks.\n\n\u2022 We further created a dataset which is the first one to investigate the probing ability of the code-based pre-trained models.\n\n\nBackground\n\n\nPre-Trained Models in NLP\n\nLarge pre-trained models (Peters et al., 2018;Radford et al., 2018;Devlin et al., 2018;Yang et al., 2019;Raffel et al., 2019) have brought dramatic empirical improvements on almost every NLP task in the past few years. Successful approaches train deep neural networks on large-scale plain texts with self-supervised learning objectives. One of the most representative neural architectures is the Transformer (Vaswani et al., 2017), which is also the one used in this work. It contains multiple self-attention layers, and can be conventionally learned with gradient decent in an end-to-end manner as every component is differentiable. The terminology \"self-supervised\" means that supervisions used for pre-training are automatically collected from raw data without manual annotation. Dominant learning objectives are language modeling and its variations. For example, in GPT (Radford et al., 2018), the learning objective is language modeling, namely predicting the next word w k given the preceding context words {w 1 , w 2 , ..., w k\u22121 }. As the ultimate goal of pretraining is not to train a good language model, it is desirable to consider both preceding and following contexts to learn better general-purpose contextual representations. This leads us to the masked language modeling objective used in BERT (Devlin et al., 2018), which learns to predict the masked words of a randomly masked word sequence given surrounding contexts. Masked language modeling is also used as one of the two learning objectives for training CodeBERT.\n\n\nMulti-Modal Pre-Trained Models\n\nThe remarkable success of the pre-trained model in NLP has driven the development of multi-modal pre-trained model that learns implicit alignment between inputs of different modalities. These models are typically learned from bimodal data, such as pairs of language-image or pairs of languagevideo. For example, ViLBERT (Lu et al., 2019) learns from image caption data, where the model learns by reconstructing categories of masked image region or masked words given the observed inputs, and meanwhile predicting whether the caption describes the image content or not. Similarly, VideoBERT (Sun et al., 2019) learns from language-video data and is trained by video and text masked token prediction. Our work belongs to this line of research as we regard NL and PL as different modalities. Our method differs from previous works in that the fuels for model training include not only bimodal data of NL-PL pairs, but larger amounts of unimodal data such as codes without paired documentations. A concurrent work (Kanade et al., 2019) uses masked language modeling and next sentence prediction as the objective to train a BERT model on Python source codes, where a sentence is a logical code line as defined by the Python standard. In terms of the pre-training process, CodeBERT differs from their work in that (1) CodeBERT is trained in a cross-modal style and leverages both bimodal NL-PL data and unimodal PL/NL data, (2) CodeBERT is pre-trained over six programming languages, and (3) CodeBERT is trained with a new learning objective based on replaced token detection.\n\n\nCodeBERT\n\nWe describe the details about CodeBERT in this section, including the model architecture, the input and output representations, the objectives and data used for training CodeBERT, and how to fine-tune CodeBERT when it is applied to downstream tasks.\n\n\nModel Architecture\n\nWe follow BERT (Devlin et al., 2018) and RoBERTa , and use multi-layer bidirectional Transformer (Vaswani et al., 2017) as the model architecture of CodeBERT. We will not review the ubiquitous Transformer architecture in detail. We develop CodeBERT by using exactly the same model architecture as RoBERTa-base. The total number of model parameters is 125M.\n\n\nInput/Output Representations\n\nIn the pre-training phase, we set the input as the concatenation of two segments with a special separator token, namely [CLS], w 1 , w 2 , ..w n , [SEP ], c 1 , c 2 , ..., c m , [EOS]. One segment is natural language text, and another is code from a certain programming language.\n\n[CLS] is a special token in front of the two segments, whose final hidden representation is considered as the aggregated sequence representation for classification or ranking. Following the standard way of processing text in Transformer, we regard a natural language text as a sequence of words, and split it as WordPiece (Wu et al., 2016). We regard a piece of code as a sequence of tokens.\n\nThe output of CodeBERT includes (1) contextual vector representation of each token, for both natural language and code, and (2) the representation of [CLS], which works as the aggregated sequence representation.\n\n\nPre-Training Data\n\nWe train CodeBERT with both bimodal data, which refers to parallel data of natural language-code pairs, and unimodal data, which stands for codes without paired natural language texts and natural language without paired codes.\n\nWe use datapoints from Github repositories, where each bimodal datapoint is an individual function with paired documentation, and each unimodal code is a function without paired documentation. Specifically, we use a recent large dataset provided by Husain et al. (2019), which includes 2.1M bimodal datapoints and 6.4M unimodal codes across six programming languages (Python, Java, JavaScript, PHP, Ruby, and Go). Data statistics is shown in Table 1. 2 The data comes from publicly available opensource non-fork GitHub repositories and are filtered with a set of constraints and rules. For example, (1) each project should be used by at least one other project, (2) each documentation is truncated to the first paragraph, (3) documentations shorter than three tokens are removed, (4) functions shorter than three lines are removed, and (5) function names with substring \"test\" are removed. An example of the data is given in Figure 1 3 . \n\n\nPre-Training CodeBERT\n\nWe describe the two objectives used for training CodeBERT here. The first objective is masked language modeling (MLM), which has proven effective in literature (Devlin et al., 2018; Figure 2: An illustration about the replaced token detection objective. Both NL and code generators are language models, which generate plausible tokens for masked positions based on surrounding contexts. NL-Code discriminator is the targeted pre-trained model, which is trained via detecting plausible alternatives tokens sampled from NL and PL generators. NL-Code discriminator is used for producing general-purpose representations in the finetuning step. Both NL and code generators are thrown out in the fine-tuning step.\n\n2019; Sun et al., 2019). We apply masked language modeling on bimodal data of NL-PL pairs. The second objective is replaced token detection (RTD), which further uses a large amount of unimodal data, such as codes without paired natural language texts. Detailed hyper-parameters for model pre-training are given in Appendix B.1.\n\nObjective #1: Masked Language Modeling (MLM) Given a datapoint of NL-PL pair (x = {w, c}) as input, where w is a sequence of NL words and c is a sequence of PL tokens, we first select a random set of positions for both NL and PL to mask out (i.e. m w and m c , respectively), and then replace the selected positions with a special [M ASK] token. Following Devlin et al. (2018), 15% of the tokens from x are masked out.\nm w i \u223c unif{1, |w|} for i = 1 to |w| (1) m c i \u223c unif{1, |c|} for i = 1 to |c| (2) w masked = REPLACE(w, m w , [M ASK]) (3) c masked = REPLACE(c, m c , [M ASK]) (4) x = w + c(5)\nThe MLM objective is to predict the original tokens which are masked out, formulated as follows, where p D 1 is the discriminator which predicts a token from a large vocabulary.\nL MLM (\u03b8) = i\u2208m w \u222am c \u2212log p D 1 (x i |w masked ,c masked )(6)\nObjective #2: Replaced Token Detection (RTD) In the MLM objective, only bimodal data (i.e. datapoints of NL-PL pairs) is used for training. Here we present the objective of replaced token detection. The RTD objective (Clark et al., 2020) is originally developed for efficiently learning pre-trained model for natural language. We adapt it in our scenario, with the advantage of using both bimodal and unimodal data for training. Specifically, there are two data generators here, an NL generator p Gw and a PL generator p Gc , both for generating plausible alternatives for the set of randomly masked positions.\nw i \u223c p Gw (w i |w masked ) for i \u2208 m w (7) c i \u223c p Gc (c i |c masked ) for i \u2208 m c (8) w corrupt = REPLACE(w, m w ,\u0175) (9) c corrupt = REPLACE(c, m c ,\u0109) (10) x corrupt = w corrupt + c corrupt(11)\nThe discriminator is trained to determine whether a word is the original one or not, which is a binary classification problem. It is worth noting that the RTD objective is applied to every position in the input, and it differs from GAN (generative adversarial network) in that if a generator happens to produce the correct token, the label of that token is \"real\" instead of \"fake\" (Clark et al., 2020). The loss function of RTD with regard to the discriminator parameterized by \u03b8 is given below, where \u03b4(i) is an indicator function and p D 2 is the discriminator that predicts the probability of the i-th word being original.\nL RTD (\u03b8) = |w|+|c| i=1 \u03b4(i)log p D 2 (x corrupt , i)+ 1 \u2212 \u03b4(i) 1 \u2212 log p D 2 (x corrupt , i) (12) \u03b4(i) = 1, if x corrupt i = x i . 0, otherwise.(13)\nThere are many different ways to implement the generators. In this work, we implement two efficient n-gram language models (Jurafsky, 2000) with bidirectional contexts, one for NL and one for PL, and learn them from corresponding unimodel datapoints, respectively. The approach is easily generalized to learn bimodal generators or use more complicated generators like Transformerbased neural architecture learned in a joint manner. We leave these to future work. The PL training data is the unimodal codes as shown in Table 1, and the NL training data comes from the documentations from bimodal data. One could easily extend these two training datasets to larger amount. The final loss function are given below.\nmin \u03b8 L MLM (\u03b8) + L RTD (\u03b8)(14)\n\nFine-Tuning CodeBERT\n\nWe have different settings to use CodeBERT in downstream NL-PL tasks. For example, in natural language code search, we feed the input as the same way as the pre-training phase and use the representation of [CLS] to measure the semantic relevance between code and natural language query, while in code-to-text generation, we use an encoderdecoder framework and initialize the encoder of a generative model with CodeBERT. Details are given in the experiment section.\n\n\nExperiment\n\nWe present empirical results in this section to verify the effectiveness of CodeBERT. We first describe the use of CodeBERT in natural language code search ( \u00a74.1), in a way that model parameters of CodeBERT are fine-tuned. After that, we present the NL-PL probing task ( \u00a74.2), and evaluate Code-BERT in a zero-shot setting where the parameters of CodeBERT are fixed. Finally, we evaluate Code-BERT on a generation problem, i.e. code documentation generation ( \u00a74.3), and further evaluate on a programming language which is never seen in the training phase ( \u00a74.4).\n\n\nNatural Language Code Search\n\nGiven a natural language as the input, the objective of code search is to find the most semantically related code from a collection of codes. We conduct experiments on the CodeSearchNet corpus (Husain et al., 2019) 4 . We follow the official evaluation metric to calculate the Mean Reciprocal Rank (MRR) for each pair of test data (c, w) over a fixed set of 999 distractor codes. We further calculate the macro-average MRR for all languages as an overall evaluation metric. It is helpful to note that this metric differs from the AVG metric in the original paper, where the answer is retrieved from candidates from all six languages. We fine-tune a languagespecific model for each programming language 5 . We train each model with a binary classification loss function, where a sof tmax layer is connected to the representation of [CLS]. Both training and validation datasets are created in a way that positive and negative samples are balanced. Negative samples consist of balanced number of instances with randomly replaced NL (i.e. (c,\u0175)) and PL (i.e. (\u0109, w)). Detailed hyper-parameters for model fine-tuning are given in Appendix B.2. Table 2 shows the results of different approaches on the CodeSearchNet corpus. The first four rows are reported by Husain et al. (2019), which are joint embeddings of NL and PL (Gu et al., 2018;Mitra et al., 2018). NBOW represents neural bag-of-words. CNN, BIRNN and SELFATT stand for 1D convolultional neural network (Kim, 2014), bidirectional GRU-based recurrent neural network (Cho et al., 2014), and multi-head attention (Vaswani et al., 2017), respectively.\n\n\nModel Comparisons\n\nWe report the remaining numbers in Table 2. We train all these pre-trained models by regarding codes as a sequence of tokens. We also continuously train RoBERTa only on codes from Code-SearchNet with masked language modeling. Results show that CodeBERT consistently performs  better than RoBERTa and the model pre-trained with code only. CodeBERT (MLM) learned from scratch performs better than RoBERTa. Unsurprisingly, initializing CodeBERT with RoBERTa improves the performance 6 .\n\n\nNL-PL Probing\n\nIn the previous subsection, we show the empirical effectiveness of CodeBERT in a setting that the parameters of CodeBERT are fine-tuned in downstream tasks. In this subsection, we further investigate what type of knowledge is learned in Code-BERT without modifying the parameters.\n\n\nTask Formulation and Data Construction\n\nFollowing the probing experiments in NLP (Petroni et al., 2019;Talmor et al., 2019), we study NL-PL probing here. Since there is no existing work towards this goal, we formulate the problem of NL-PL probing and create the dataset by ourselves. Given an NL-PL pair (c, w), the goal of NL-PL probing is to test model's ability to correctly predict/recover the masked token of interest (either a code token c i or word token w j ) among distractors. There are two major types of distractors: one is the whole target vocabulary used for the masked language modeling objective (Petroni et al., 2019), and another one has fewer candidates which are filter or curated based on experts' understanding about the ability to be tested (Talmor et al., 2019). We follow the second direction and formulate NL-PL probing as a multi-choice question answering task, where the question is cloze-style in which a certain token 6 We further give a learning curve of different pre-trained models in the fine-tuning process in Appendix C. is replaced by [M ASK] and distractor candidate answers are curated based on our expertise.\n\nSpecifically, we evaluate on the NL side and PL side, respectively. To ease the effort of data collection, we collect data automatically from NL-PL pairs in both validation and testing sets of Code-SearchNet, both of which are unseen in the pretraining phase. To evaluate on the NL side, we select NL-PL pairs whose NL documentations include one of the six keywords (max, maximize, min, minimize, less, greater), and group them to four candidates by merging first two keywords and the middle two keywords. The task is to ask pre-trained models to select the correct one instead of three other distractors. That is to say, the input in this setting includes the complete code and a masked NL documentation. The goal is to select the correct answer from four candidates. For the PL side, we select codes containing keywords max and min, and formulate the task as a two-choice answer selection problem. Here, the input includes complete NL documentation and a masked PL code, and the goal is to select the correct answer from two candidates. Since code completion is an important scenario, we would like to test model's ability in predicting the correct token merely based on preceding PL contexts. Therefore, we add an additional setting for PL side, where the input includes the complete NL documentation and preceding PL codes. Data statistics is given in the top two rows in Table 3. Table  3. We report accuracy, namely the number of correctly predicted instances over the number of all instances, for each programming language. Since  datasets in different programming languages are extremely unbalanced, we report the accumulated metric with the same way. We use CodeBERT (MLM) here because its output layer naturally fits for probing. Results show that CodeBERT performs better than baselines on almost all languages on both NL and PL probing. The numbers with only preceding contexts are lower than that with bidirectional contexts, which suggests that code completion is challenging. We leave it as a future work.\n\n\nModel Comparisons Results are given in\n\nWe further give a case study on PL-NL probing. We mask NL token and PL token separately, and report the predicted probabilities of RoBERTa and CodeBERT. Figure 3 illustrates the example of a python code 7 . We can see that RoBERTa fails in both cases, whereas CodeBERT makes the correct prediction in both NL and PL settings.\n\n\nCode Documentation Generation\n\nAlthough the pre-training objective of Code-BERT does not include generation-based objectives , we would like to investigate to what extent does CodeBERT perform on generation tasks. Specifically, we study code-to-NL generation, and report results for the documentation generation task on CodeSearchNet Corpus in six programming languages. Since the generated documentations are short and higher order n-grams may not overlap, we remedy this problem by using smoothed BLEU score (Lin and Och, 2004 Figure 3: Case study on python language. Masked tokens in NL (in blue) and PL (in yellow) are separately applied. Predicted probabilities of RoBERTa and Code-BERT are given.\n\n\nModel Comparisons\n\nWe compare our model with several baselines, including a RNN-based model with attention mechanism (Sutskever et al., 2014), the Transformer (Vaswani et al., 2017), RoBERTa and the model pre-trained on code only. To demonstrate the effectiveness of CodeBERT on code-to-NL generation tasks, we adopt various pre-trained models as encoders and keep the hyperparameters consistent. Detailed hyper-parameters are given in Appendix B.3. Table 4 shows the results with different models for the code-to-documentation generation task. As we can see, models pre-trained on programming language outperform RoBERTa, which illustrates that pre-trainning models on programming  language could improve code-to-NL generation. Besides, results in the Table 4 show that CodeBERT pre-trained with RTD and MLM objectives brings a gain of 1.3 BLEU score over RoBERTa overall and achieve the state-of-the-art performance 8 .\n\n\nGeneralization to Programming Languages NOT in Pre-training\n\nWe would like to evaluate CodeBERT on the programming language which is never seen in the pretraining step. To this end, we study the task of generating a natural language summary of a C# code snippet. We conduct experiments on the dataset of CodeNN (Iyer et al., 2016) 9 , which consists of 66,015 pairs of questions and answers automatically collected from StackOverflow. This dataset is challenging since the scale of dataset is orders of magnitude smaller than CodeSearchNet Corpus. We evaluate models using smoothed BLEU-4 score and use the same evaluation scripts as Iyer et al.  could generalize better to other programming language which is never seen in the pre-training step. However, our model achieve slightly lower results than code2seq . The main reason could be that code2seq makes use of compositional paths in its abstract syntax tree (AST) while Code-BERT only takes original code as the input. We have trained a version of CodeBERT by traversing the tree structure of AST following a certain order, but applying that model does not bring improvements on generation tasks. This shows a potential direction to improve CodeBERT by incorporating AST.\n\n\nConclusion\n\nIn this paper, we present CodeBERT, which to the best of our knowledge is the first large bimodal pre-trained model for natural language and programming language. We train CodeBERT on both bimodal and unimodal data, and show that finetuning CodeBERT achieves state-of-the-art performance on downstream tasks including natural language code search and code-to-documentation generation. To further investigate the knowledge embodied in pre-trained models, we formulate the task of NL-PL probing and create a dataset for probing. We regard the probing task as a cloze-style answer selection problem, and curate distractors for both NL and PL parts. Results show that, with model parameters fixed, CodeBERT performs better than RoBERTa and a continuously trained model using codes only. There are many potential directions for further research on this field. First, one could learn better generators with bimodal evidence or more complicated neural architecture to improve the replaced token detection objective. Second, the loss functions of CodeBERT mainly target on NL-PL understanding tasks. Although CodeBERT achieves strong BLEU scores on code-to-documentation generation, the CodeBERT itself could be further improved by generation-related learning objectives.\n\nHow to successfully incorporate AST into the pretraining step is also an attractive direction. Third, we plan to apply CodeBERT to more NL-PL related tasks, and extend it to more programming languages. Flexible and powerful domain/language adaptation methods are necessary to generalize well.\n\n\nA Data Statistic\n\nData statistics of the training/validation/testing data splits for six programming languages are given in Table 6.  We train CodeBERT on one NVIDIA DGX-2 machine using FP16. It combines 16 interconnected NVIDIA Tesla V100 with 32GB memory. We use the following set of hyper-parameters to train models: batchsize is 2,048 and learning rate is 5e-4. We use Adam to update the parameters and set the number of warmup steps as 10K. We set the max length as 512 and the max training step is 100K. Training 1,000 batches of data costs 600 minutes with MLM objective, 120 minutes with RTD objective.\n\n\nB.2 CodeSearch\n\nIn the fine-turning step, we set the learning rate as 1e-5, the batch size as 64, the max sequence length as 200 and the max fine-tuning epoch as 8. As the same with pre-training, We use Adam to update the parameters. We choose the model performed best on the development set, and use that to evaluate on the test set.\n\n\nB.3 Code Summarization on Six Programming Languages\n\nWe use Transformer with 6 layers, 768 dimensional hidden states and 12 attention heads as our decoder in all settings. We set the max length of input and inference as 256 and 64, respectively. We use the Adam optimizer to update model parameters.\n\nThe learning rate and the batch size are 5e-5 and 64, respectively. We tune hyperparameters and perform early stopping on the development set.\n\n\nB.4 Code Summarization on C#\n\nSince state-of-the-art methods use RNN as their decoder, we choose a 2-layer GRU with an attention mechanism as our decoder for a comparison. We fine-tune models using a grid search with the following set of hyper-parameters: batchsize is in {32, 64} and learning rate is in {2e-5, 5e-5}. We report the number when models achieve best performance on the development set.\n\n\nC Learning Curve of CodeSearch\n\nFrom Figure 4, we can see that CodeBERT performs better at the early stage, which reflects that CodeBERT provides good initialization for learning downstream tasks. \n\n\nD Late Fusion\n\nIn section \u00a74.1 , we show that CodeBERT performs well in the setting where natural languages and codes have early interactions. Here, we investigate whether CodeBERT is good at working as a unified encoder. We apply CodeBERT for natural language code search in a later fusion setting, where CodeBERT first encodes NL and PL separately, and then calculates the similarity by dotproduct. In this way, code search is equivalent to find the nearest codes in the shared vector space. This scenario also facilitates the use of CodeBERT in an online system, where the representations of codes are calculated in advance. In the runtime, a system only needs to compute the representation of NL and vector-based dot-product. We fine-tune CodeBERT with the following objective, which maximizes the dot-product of the ground truth while minimizing the dot-product of distractors.\n\u2212 1 N i log exp Enc(c i ) Enc(w i ) j exp Enc(c j ) Enc(w i )(15)\nResults are given in Table 7. We just do this setting on two languages with a relatively small amount of data.\n\nWe can see that CodeBERT performs better than RoBERTa and the model pre-trained with codes  only. And late fusion performs comparable with the standard way. What's more, late fusion is more efficient and this setting could be used in an online system.\n\n\nE Case Study\n\nTo qualitatively analyze the effectiveness of Code-BERT, we give some cases for code search and code documentation generation tasks.\n\nConsidering the limited space, we only give the top2 results of the query for python programming language. As show in Figure 5, search results are very relevant with query. Figure 6 and Figure 7 show the outputs with different models for the code documentation generation task. As we can see, CodeBERT performs better than all baselines.   \n\nFigure 1 :\n1An example of the NL-PL pair, where NL is the first paragraph (filled in red) from the documentation (dashed line in black) of a function.\n\nFigure 4 :\n4Learning curve of different pre-trained models in the fine-tuning step. We show results on Python and Java.\n\nFigure 5 :\n5Python CodeSearch example. The results are searched from 1,156,085 python code data. We only give the top2 results because space is limited.\n\nFigure 6 :\n6Java code documentation generation output example.\n\nFigure 7 :\n7Python code documentation generation output example.\n\n\nal.,NL Generator \n\nNL-Code \nDiscriminator \n\n[ \n] \n\nCodeBERT V2: \n\nA Pre-trained Model for NL-Code Understanding and Generation \n\n\n\nTable 2 :\n2Results on natural language code retrieval. Baselines include four joint embeddings (first group) of NL \nand PL, RoBERTa, and RoBERTa which is continuously trained with masked language modeling on codes only \n(second group). PT stands for pre-training. We train CodeBERT (third group) with different settings, including \nusing different initialization (from scratch (INIT=S) or initialized with the parameters of RoBERTa (INIT=R)) and \nusing different learning objectives (MLM, RTD, or the combination of both). \n\n\n\nTable 3 :\n3Statistics of the data for NL-PL probing and the performance of different pre-trained models. Accuracies (%) are reported. Best results in each group are in bold.\n\n\n).7 The \nexample \ncomes \nfrom \nhttps:// \ngithub.com/peri-source/peri/blob/ \n61beed5deaaf978ab31ed716e8470d86ba639867/ \nperi/comp/psfcalc.py#L994-L1002 \n\ndef vec_to_halfvec(vec): \n\nd = vec[1:] -vec[:-1] \nif ((d/d.mean()).std() > 1e-14) or (d.mean() < 0): \nraise ValueError('vec must be np.arange() in increasing order') \n\ndx = d.mean() \nlowest = np.abs(vec). min () \nhighest = np.abs(vec).max() \nreturn np.arange(lowest, highest + 0.1*dx, dx).astype(vec.dtype) \n\n\"Transforms a vector np.arange(-N, M, dx) to np.arange( min (|vec|), \nmax(N,M),dx)]\" \n\nmasked NL token \n\nmasked PL token \n\nmax \nmin \nless \ngreater \n\nNL \nRoberta \n96.24% \n3.73% \n0.02% \n0.01% \n\nCodeBERT (MLM) 39.38% \n60.60% \n0.02% \n0.0003% \n\nPL \nRoberta \n95.85% \n4.15% \n-\n-\n\nCodeBERT (MLM) 0.001% \n99.999% \n-\n-\n\n\n\nTable 4 :\n4Results on Code-to-Documentation generation, evaluated with smoothed BLEU-4 score.\n\n\n.MODEL \nBLEU \n\nMOSES (KOEHN ET AL., 2007) \n11.57 \nIR \n13.66 \nSUM-NN (RUSH ET AL., 2015) \n19.31 \n2-LAYER BILSTM \n19.78 \nTRANSFORMER (VASWANI ET AL., 2017) \n19.68 \nTREELSTM (TAI ET AL., 2015) \n20.11 \nCODENN (IYER ET AL., 2016) \n20.53 \nCODE2SEQ (ALON ET AL., 2019) \n23.04 \nROBERTA \n19.81 \n\nPRE-TRAIN W/ CODE ONLY \n\n20.65 \nCODEBERT (RTD) \n22.14 \nCODEBERT (MLM) \n22.32 \nCODEBERT (MLM+RTD) \n22.36 \n\n\n\nTable 5 :\n5Code-to-NL generation on C# language.Model Comparisons Table 5 shows that our \nmodel with MLM and RTD pre-training objectives \nachieves 22.36 BLEU score and improves by 2.55 \npoints over RoBERTa, which illustrates CodeBERT \n\n8 We further give some output examples in Appendix E. \n9 https://github.com/sriniiyer/codenn \n\n\n\nTable 6 :\n6Data statistics about the CodeSearchNet Corpus for natural language code search.B Train Details \n\nB.1 Pre-training \n\n\n\nTable 7 :\n7Results on natural language code search by late fusion.\nSince we will evaluate on the natural language code search task, we only use the training data ofHusain et al. (2019) to train CodeBERT with no access to the dev and testing data.3  The source of the illustrating example comes from https://github.com/apache/spark/blob/ 618d6bff71073c8c93501ab7392c3cc579730f0b/ python/pyspark/rdd.py#L125-L138\nMore details about the dataset are given in Appendix A.5  We have fine-tuned a multi-lingual model for six programming languages, but find that it performs worse that fine-tuning a language-specific model for each programming language.\nAcknowledgmentsXiaocheng Feng is the corresponding author of this work. We thank the anonymous reviewers for their insightful comments. Zhangyin Feng, Xiaocheng Feng, Bing Qin and Ting Liu are supported by the National Key R&D Program of China via grant 2018YFB1005103 and National Natural Science Foundation of China (NSFC) via grant 61632011 and 61772156.\ncode2seq: Generating sequences from structured representations of code. Uri Alon, Shaked Brody, Omer Levy, Eran Yahav, International Conferenceon Learning Representations. Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating sequences from struc- tured representations of code. International Confer- enceon Learning Representations.\n\nLearning phrase representations using rnn encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, arXiv:1406.1078arXiv preprintKyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.\n\n{ELECTRA}: Pretraining text encoders as discriminators rather than generators. Kevin Clark, Minh-Thang Luong, Quoc V Le, Christopher D Manning, International Conference on Learning Representations. Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. {ELECTRA}: Pre- training text encoders as discriminators rather than generators. In International Conference on Learn- ing Representations.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.\n\nDeep code search. Xiaodong Gu, Hongyu Zhang, Sunghun Kim, 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEEXiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In 2018 IEEE/ACM 40th Interna- tional Conference on Software Engineering (ICSE), pages 933-944. IEEE.\n\nCodesearchnet challenge: Evaluating the state of semantic code search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, arXiv:1909.09436arXiv preprintHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Code- searchnet challenge: Evaluating the state of seman- tic code search. arXiv preprint arXiv:1909.09436.\n\nSummarizing source code using a neural attention model. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsLong Papers1Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2073-2083.\n\nSpeech & language processing. Dan Jurafsky, Pearson Education IndiaDan Jurafsky. 2000. Speech & language processing. Pearson Education India.\n\nPre-trained contextual embedding of source code. Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi, arXiv:2001.00059arXiv preprintAditya Kanade, Petros Maniatis, Gogul Balakrish- nan, and Kensen Shi. 2019. Pre-trained contex- tual embedding of source code. arXiv preprint arXiv:2001.00059.\n\nConvolutional neural networks for sentence classification. Yoon Kim, arXiv:1408.5882arXiv preprintYoon Kim. 2014. Convolutional neural net- works for sentence classification. arXiv preprint arXiv:1408.5882.\n\nMoses: Open source toolkit for statistical machine translation. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions. the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessionsPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Pro- ceedings of the 45th annual meeting of the associ- ation for computational linguistics companion vol- ume proceedings of the demo and poster sessions, pages 177-180.\n\nBart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.13461arXiv preprintMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.\n\nOrange: a method for evaluating automatic evaluation metrics for machine translation. Chin-Yew Lin, Franz Josef Och, Proceedings of the 20th international conference on Computational Linguistics. the 20th international conference on Computational Linguisticspage 501. Association for Computational LinguisticsChin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings of the 20th international conference on Computational Linguis- tics, page 501. Association for Computational Lin- guistics.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Advances in Neural Information Processing Systems. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visi- olinguistic representations for vision-and-language tasks. In Advances in Neural Information Process- ing Systems, pages 13-23.\n\nAn introduction to neural information retrieval. Foundations and Trends\u00ae in Information Retrieval. Nick Bhaskar Mitra, Craswell, 13Bhaskar Mitra, Nick Craswell, et al. 2018. An intro- duction to neural information retrieval. Foundations and Trends\u00ae in Information Retrieval, 13(1):1-126.\n\nE Matthew, Mark Peters, Mohit Neumann, Matt Iyyer, Christopher Gardner, Kenton Clark, Luke Lee, Zettlemoyer, arXiv:1802.05365Deep contextualized word representations. arXiv preprintMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. arXiv preprint arXiv:1802.05365.\n\nLanguage models as knowl. Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, arXiv:1909.01066arXiv preprintFabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Se- bastian Riedel. 2019. Language models as knowl- edge bases? arXiv preprint arXiv:1909.01066.\n\nHow multilingual is multilingual bert?. Telmo Pires, Eva Schlinger, Dan Garrette, arXiv:1906.01502arXiv preprintTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual bert? arXiv preprint arXiv:1906.01502.\n\nImproving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai- assets/researchcovers/languageunsupervised/language understanding paper. pdf.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. arXiv preprint arXiv:1910.10683.\n\nM Alexander, Rush, arXiv:1509.00685Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. arXiv preprintAlexander M Rush, Sumit Chopra, and Jason We- ston. 2015. A neural attention model for ab- stractive sentence summarization. arXiv preprint arXiv:1509.00685.\n\nVideobert: A joint model for video and language representation learning. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid, arXiv:1904.01766arXiv preprintChen Sun, Austin Myers, Carl Vondrick, Kevin Mur- phy, and Cordelia Schmid. 2019. Videobert: A joint model for video and language representation learn- ing. arXiv preprint arXiv:1904.01766.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing sys- tems, pages 3104-3112.\n\nImproved semantic representations from tree-structured long short-term memory networks. Kai Sheng Tai, Richard Socher, Christopher D Manning, arXiv:1503.00075arXiv preprintKai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- works. arXiv preprint arXiv:1503.00075.\n\nAlon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant, arXiv:1912.13283olmpics-on what language model pre-training captures. arXiv preprintAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2019. olmpics-on what lan- guage model pre-training captures. arXiv preprint arXiv:1912.13283.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998-6008.\n\nGoogle's neural machine translation system: Bridging the gap between human and machine translation. Yonghui Wu, Mike Schuster, Zhifeng Chen, V Quoc, Mohammad Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.08144arXiv preprintYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between hu- man and machine translation. arXiv preprint arXiv:1609.08144.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V Le, arXiv:1906.08237Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprintZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretrain- ing for language understanding. arXiv preprint arXiv:1906.08237.\n", "annotations": {"author": "[{\"end\":326,\"start\":190},{\"end\":406,\"start\":327},{\"end\":455,\"start\":407},{\"end\":503,\"start\":456},{\"end\":641,\"start\":504},{\"end\":706,\"start\":642},{\"end\":773,\"start\":707},{\"end\":903,\"start\":774},{\"end\":1033,\"start\":904},{\"end\":1100,\"start\":1034},{\"end\":1172,\"start\":1101}]", "publisher": "[{\"end\":110,\"start\":69},{\"end\":1461,\"start\":1420}]", "author_last_name": "[{\"end\":203,\"start\":199},{\"end\":335,\"start\":332},{\"end\":416,\"start\":412},{\"end\":464,\"start\":460},{\"end\":518,\"start\":514},{\"end\":651,\"start\":647},{\"end\":718,\"start\":714},{\"end\":782,\"start\":779},{\"end\":912,\"start\":909},{\"end\":1045,\"start\":1040},{\"end\":1110,\"start\":1106}]", "author_first_name": "[{\"end\":198,\"start\":190},{\"end\":331,\"start\":327},{\"end\":411,\"start\":407},{\"end\":459,\"start\":456},{\"end\":513,\"start\":504},{\"end\":646,\"start\":642},{\"end\":713,\"start\":707},{\"end\":778,\"start\":774},{\"end\":908,\"start\":904},{\"end\":1039,\"start\":1034},{\"end\":1105,\"start\":1101}]", "author_affiliation": "[{\"end\":325,\"start\":226},{\"end\":405,\"start\":337},{\"end\":454,\"start\":418},{\"end\":502,\"start\":466},{\"end\":640,\"start\":541},{\"end\":705,\"start\":653},{\"end\":772,\"start\":720},{\"end\":902,\"start\":803},{\"end\":1032,\"start\":933},{\"end\":1099,\"start\":1047},{\"end\":1171,\"start\":1135}]", "title": "[{\"end\":68,\"start\":1},{\"end\":1240,\"start\":1173}]", "venue": "[{\"end\":1338,\"start\":1242}]", "abstract": "[{\"end\":2718,\"start\":1493}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2793,\"start\":2772},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2821,\"start\":2799},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2849,\"start\":2828},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2876,\"start\":2857},{\"end\":2950,\"start\":2949},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3532,\"start\":3515},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3584,\"start\":3566},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4221,\"start\":4199},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4508,\"start\":4487},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4558,\"start\":4538},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4875,\"start\":4854},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4969,\"start\":4949},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6130,\"start\":6109},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6151,\"start\":6130},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6171,\"start\":6151},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6189,\"start\":6171},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6209,\"start\":6189},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6514,\"start\":6492},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6980,\"start\":6958},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7415,\"start\":7394},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7991,\"start\":7974},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8262,\"start\":8244},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8685,\"start\":8664},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9545,\"start\":9524},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9628,\"start\":9606},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10518,\"start\":10501},{\"end\":10727,\"start\":10722},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11302,\"start\":11282},{\"end\":11485,\"start\":11484},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12178,\"start\":12157},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12729,\"start\":12712},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13411,\"start\":13391},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14112,\"start\":14092},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15085,\"start\":15065},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15599,\"start\":15583},{\"end\":17521,\"start\":17498},{\"end\":18141,\"start\":18136},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18579,\"start\":18559},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18638,\"start\":18621},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18657,\"start\":18638},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18773,\"start\":18762},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18842,\"start\":18824},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18891,\"start\":18869},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19815,\"start\":19793},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19835,\"start\":19815},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20346,\"start\":20324},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20497,\"start\":20476},{\"end\":20661,\"start\":20660},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23781,\"start\":23763},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24099,\"start\":24075},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24139,\"start\":24117},{\"end\":25214,\"start\":25193},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34791,\"start\":34771}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31637,\"start\":31486},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31758,\"start\":31638},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31912,\"start\":31759},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31976,\"start\":31913},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32042,\"start\":31977},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32174,\"start\":32043},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32701,\"start\":32175},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32876,\"start\":32702},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33651,\"start\":32877},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":33746,\"start\":33652},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":34142,\"start\":33747},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":34475,\"start\":34143},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":34605,\"start\":34476},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":34673,\"start\":34606}]", "paragraph": "[{\"end\":3710,\"start\":2734},{\"end\":4678,\"start\":3712},{\"end\":5715,\"start\":4680},{\"end\":5805,\"start\":5717},{\"end\":5913,\"start\":5807},{\"end\":6041,\"start\":5915},{\"end\":7619,\"start\":6084},{\"end\":9224,\"start\":7654},{\"end\":9486,\"start\":9237},{\"end\":9865,\"start\":9509},{\"end\":10177,\"start\":9898},{\"end\":10570,\"start\":10179},{\"end\":10783,\"start\":10572},{\"end\":11031,\"start\":10805},{\"end\":11971,\"start\":11033},{\"end\":12704,\"start\":11997},{\"end\":13033,\"start\":12706},{\"end\":13453,\"start\":13035},{\"end\":13810,\"start\":13633},{\"end\":14485,\"start\":13875},{\"end\":15309,\"start\":14683},{\"end\":16171,\"start\":15460},{\"end\":16691,\"start\":16227},{\"end\":17272,\"start\":16706},{\"end\":18906,\"start\":17305},{\"end\":19411,\"start\":18928},{\"end\":19709,\"start\":19429},{\"end\":20860,\"start\":19752},{\"end\":22882,\"start\":20862},{\"end\":23250,\"start\":22925},{\"end\":23955,\"start\":23284},{\"end\":24879,\"start\":23977},{\"end\":26108,\"start\":24943},{\"end\":27386,\"start\":26123},{\"end\":27680,\"start\":27388},{\"end\":28293,\"start\":27701},{\"end\":28630,\"start\":28312},{\"end\":28932,\"start\":28686},{\"end\":29076,\"start\":28934},{\"end\":29479,\"start\":29109},{\"end\":29679,\"start\":29514},{\"end\":30564,\"start\":29697},{\"end\":30741,\"start\":30631},{\"end\":30994,\"start\":30743},{\"end\":31143,\"start\":31011},{\"end\":31485,\"start\":31145}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13632,\"start\":13454},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13874,\"start\":13811},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14682,\"start\":14486},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15459,\"start\":15310},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16203,\"start\":16172},{\"attributes\":{\"id\":\"formula_6\"},\"end\":30630,\"start\":30565}]", "table_ref": "[{\"end\":11482,\"start\":11475},{\"end\":15985,\"start\":15978},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18451,\"start\":18444},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18970,\"start\":18963},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22245,\"start\":22238},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22255,\"start\":22247},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":24415,\"start\":24408},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":24718,\"start\":24711},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":27814,\"start\":27807},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":30659,\"start\":30652}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2732,\"start\":2720},{\"attributes\":{\"n\":\"2\"},\"end\":6054,\"start\":6044},{\"attributes\":{\"n\":\"2.1\"},\"end\":6082,\"start\":6057},{\"attributes\":{\"n\":\"2.2\"},\"end\":7652,\"start\":7622},{\"attributes\":{\"n\":\"3\"},\"end\":9235,\"start\":9227},{\"attributes\":{\"n\":\"3.1\"},\"end\":9507,\"start\":9489},{\"attributes\":{\"n\":\"3.2\"},\"end\":9896,\"start\":9868},{\"attributes\":{\"n\":\"3.3\"},\"end\":10803,\"start\":10786},{\"attributes\":{\"n\":\"3.4\"},\"end\":11995,\"start\":11974},{\"attributes\":{\"n\":\"3.5\"},\"end\":16225,\"start\":16205},{\"attributes\":{\"n\":\"4\"},\"end\":16704,\"start\":16694},{\"attributes\":{\"n\":\"4.1\"},\"end\":17303,\"start\":17275},{\"end\":18926,\"start\":18909},{\"attributes\":{\"n\":\"4.2\"},\"end\":19427,\"start\":19414},{\"end\":19750,\"start\":19712},{\"end\":22923,\"start\":22885},{\"attributes\":{\"n\":\"4.3\"},\"end\":23282,\"start\":23253},{\"end\":23975,\"start\":23958},{\"attributes\":{\"n\":\"4.4\"},\"end\":24941,\"start\":24882},{\"attributes\":{\"n\":\"5\"},\"end\":26121,\"start\":26111},{\"end\":27699,\"start\":27683},{\"end\":28310,\"start\":28296},{\"end\":28684,\"start\":28633},{\"end\":29107,\"start\":29079},{\"end\":29512,\"start\":29482},{\"end\":29695,\"start\":29682},{\"end\":31009,\"start\":30997},{\"end\":31497,\"start\":31487},{\"end\":31649,\"start\":31639},{\"end\":31770,\"start\":31760},{\"end\":31924,\"start\":31914},{\"end\":31988,\"start\":31978},{\"end\":32185,\"start\":32176},{\"end\":32712,\"start\":32703},{\"end\":33662,\"start\":33653},{\"end\":34153,\"start\":34144},{\"end\":34486,\"start\":34477},{\"end\":34616,\"start\":34607}]", "table": "[{\"end\":32174,\"start\":32049},{\"end\":32701,\"start\":32187},{\"end\":33651,\"start\":32881},{\"end\":34142,\"start\":33750},{\"end\":34475,\"start\":34192},{\"end\":34605,\"start\":34568}]", "figure_caption": "[{\"end\":31637,\"start\":31499},{\"end\":31758,\"start\":31651},{\"end\":31912,\"start\":31772},{\"end\":31976,\"start\":31926},{\"end\":32042,\"start\":31990},{\"end\":32049,\"start\":32045},{\"end\":32876,\"start\":32714},{\"end\":32881,\"start\":32879},{\"end\":33746,\"start\":33664},{\"end\":33750,\"start\":33749},{\"end\":34192,\"start\":34155},{\"end\":34568,\"start\":34488},{\"end\":34673,\"start\":34618}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11966,\"start\":11958},{\"end\":12187,\"start\":12179},{\"end\":23086,\"start\":23078},{\"end\":23790,\"start\":23782},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29527,\"start\":29519},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31271,\"start\":31263},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31326,\"start\":31318},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31339,\"start\":31331}]", "bib_author_first_name": "[{\"end\":35687,\"start\":35684},{\"end\":35700,\"start\":35694},{\"end\":35712,\"start\":35708},{\"end\":35723,\"start\":35719},{\"end\":36075,\"start\":36066},{\"end\":36085,\"start\":36081},{\"end\":36109,\"start\":36103},{\"end\":36127,\"start\":36120},{\"end\":36143,\"start\":36138},{\"end\":36160,\"start\":36154},{\"end\":36176,\"start\":36170},{\"end\":36557,\"start\":36552},{\"end\":36575,\"start\":36565},{\"end\":36587,\"start\":36583},{\"end\":36589,\"start\":36588},{\"end\":36605,\"start\":36594},{\"end\":36607,\"start\":36606},{\"end\":36894,\"start\":36889},{\"end\":36911,\"start\":36903},{\"end\":36925,\"start\":36919},{\"end\":36939,\"start\":36931},{\"end\":37279,\"start\":37271},{\"end\":37290,\"start\":37284},{\"end\":37305,\"start\":37298},{\"end\":37638,\"start\":37633},{\"end\":37656,\"start\":37647},{\"end\":37668,\"start\":37661},{\"end\":37685,\"start\":37676},{\"end\":37701,\"start\":37697},{\"end\":38014,\"start\":38004},{\"end\":38028,\"start\":38021},{\"end\":38043,\"start\":38038},{\"end\":38056,\"start\":38052},{\"end\":38542,\"start\":38539},{\"end\":38707,\"start\":38701},{\"end\":38722,\"start\":38716},{\"end\":38738,\"start\":38733},{\"end\":38759,\"start\":38753},{\"end\":39019,\"start\":39015},{\"end\":39235,\"start\":39228},{\"end\":39247,\"start\":39243},{\"end\":39264,\"start\":39255},{\"end\":39277,\"start\":39272},{\"end\":39302,\"start\":39294},{\"end\":39319,\"start\":39313},{\"end\":39336,\"start\":39330},{\"end\":39348,\"start\":39344},{\"end\":39364,\"start\":39355},{\"end\":39379,\"start\":39372},{\"end\":40198,\"start\":40194},{\"end\":40212,\"start\":40206},{\"end\":40223,\"start\":40218},{\"end\":40257,\"start\":40253},{\"end\":40267,\"start\":40264},{\"end\":40282,\"start\":40278},{\"end\":40704,\"start\":40696},{\"end\":40721,\"start\":40710},{\"end\":41191,\"start\":41185},{\"end\":41201,\"start\":41197},{\"end\":41212,\"start\":41207},{\"end\":41227,\"start\":41220},{\"end\":41238,\"start\":41232},{\"end\":41251,\"start\":41246},{\"end\":41262,\"start\":41258},{\"end\":41273,\"start\":41269},{\"end\":41285,\"start\":41281},{\"end\":41306,\"start\":41299},{\"end\":41745,\"start\":41739},{\"end\":41755,\"start\":41750},{\"end\":41767,\"start\":41763},{\"end\":41782,\"start\":41776},{\"end\":42171,\"start\":42167},{\"end\":42358,\"start\":42357},{\"end\":42372,\"start\":42368},{\"end\":42386,\"start\":42381},{\"end\":42400,\"start\":42396},{\"end\":42419,\"start\":42408},{\"end\":42435,\"start\":42429},{\"end\":42447,\"start\":42443},{\"end\":42765,\"start\":42760},{\"end\":42778,\"start\":42775},{\"end\":42799,\"start\":42792},{\"end\":42812,\"start\":42807},{\"end\":42829,\"start\":42822},{\"end\":42843,\"start\":42834},{\"end\":42845,\"start\":42844},{\"end\":42863,\"start\":42854},{\"end\":43143,\"start\":43138},{\"end\":43154,\"start\":43151},{\"end\":43169,\"start\":43166},{\"end\":43400,\"start\":43396},{\"end\":43417,\"start\":43410},{\"end\":43433,\"start\":43430},{\"end\":43448,\"start\":43444},{\"end\":43811,\"start\":43806},{\"end\":43824,\"start\":43820},{\"end\":43838,\"start\":43834},{\"end\":43857,\"start\":43848},{\"end\":43869,\"start\":43863},{\"end\":43885,\"start\":43878},{\"end\":43899,\"start\":43894},{\"end\":43909,\"start\":43906},{\"end\":43921,\"start\":43914},{\"end\":44208,\"start\":44207},{\"end\":44595,\"start\":44591},{\"end\":44607,\"start\":44601},{\"end\":44619,\"start\":44615},{\"end\":44635,\"start\":44630},{\"end\":44652,\"start\":44644},{\"end\":44938,\"start\":44934},{\"end\":44955,\"start\":44950},{\"end\":44971,\"start\":44965},{\"end\":45302,\"start\":45293},{\"end\":45315,\"start\":45308},{\"end\":45337,\"start\":45324},{\"end\":45569,\"start\":45565},{\"end\":45583,\"start\":45578},{\"end\":45596,\"start\":45592},{\"end\":45615,\"start\":45607},{\"end\":45900,\"start\":45894},{\"end\":45914,\"start\":45910},{\"end\":45928,\"start\":45924},{\"end\":45942,\"start\":45937},{\"end\":45959,\"start\":45954},{\"end\":45972,\"start\":45967},{\"end\":45974,\"start\":45973},{\"end\":45988,\"start\":45982},{\"end\":46002,\"start\":45997},{\"end\":46405,\"start\":46398},{\"end\":46414,\"start\":46410},{\"end\":46432,\"start\":46425},{\"end\":46440,\"start\":46439},{\"end\":46455,\"start\":46447},{\"end\":46468,\"start\":46460},{\"end\":46483,\"start\":46478},{\"end\":46498,\"start\":46494},{\"end\":46510,\"start\":46507},{\"end\":46521,\"start\":46516},{\"end\":46860,\"start\":46854},{\"end\":46873,\"start\":46867},{\"end\":46885,\"start\":46879},{\"end\":46897,\"start\":46892},{\"end\":46915,\"start\":46909},{\"end\":46937,\"start\":46931}]", "bib_author_last_name": "[{\"end\":35692,\"start\":35688},{\"end\":35706,\"start\":35701},{\"end\":35717,\"start\":35713},{\"end\":35729,\"start\":35724},{\"end\":36079,\"start\":36076},{\"end\":36101,\"start\":36086},{\"end\":36118,\"start\":36110},{\"end\":36136,\"start\":36128},{\"end\":36152,\"start\":36144},{\"end\":36168,\"start\":36161},{\"end\":36183,\"start\":36177},{\"end\":36563,\"start\":36558},{\"end\":36581,\"start\":36576},{\"end\":36592,\"start\":36590},{\"end\":36615,\"start\":36608},{\"end\":36901,\"start\":36895},{\"end\":36917,\"start\":36912},{\"end\":36929,\"start\":36926},{\"end\":36949,\"start\":36940},{\"end\":37282,\"start\":37280},{\"end\":37296,\"start\":37291},{\"end\":37309,\"start\":37306},{\"end\":37645,\"start\":37639},{\"end\":37659,\"start\":37657},{\"end\":37674,\"start\":37669},{\"end\":37695,\"start\":37686},{\"end\":37714,\"start\":37702},{\"end\":38019,\"start\":38015},{\"end\":38036,\"start\":38029},{\"end\":38050,\"start\":38044},{\"end\":38068,\"start\":38057},{\"end\":38551,\"start\":38543},{\"end\":38714,\"start\":38708},{\"end\":38731,\"start\":38723},{\"end\":38751,\"start\":38739},{\"end\":38763,\"start\":38760},{\"end\":39023,\"start\":39020},{\"end\":39241,\"start\":39236},{\"end\":39253,\"start\":39248},{\"end\":39270,\"start\":39265},{\"end\":39292,\"start\":39278},{\"end\":39311,\"start\":39303},{\"end\":39328,\"start\":39320},{\"end\":39342,\"start\":39337},{\"end\":39353,\"start\":39349},{\"end\":39370,\"start\":39365},{\"end\":39384,\"start\":39380},{\"end\":40204,\"start\":40199},{\"end\":40216,\"start\":40213},{\"end\":40251,\"start\":40224},{\"end\":40262,\"start\":40258},{\"end\":40276,\"start\":40268},{\"end\":40294,\"start\":40283},{\"end\":40708,\"start\":40705},{\"end\":40725,\"start\":40722},{\"end\":41195,\"start\":41192},{\"end\":41205,\"start\":41202},{\"end\":41218,\"start\":41213},{\"end\":41230,\"start\":41228},{\"end\":41244,\"start\":41239},{\"end\":41256,\"start\":41252},{\"end\":41267,\"start\":41263},{\"end\":41279,\"start\":41274},{\"end\":41297,\"start\":41286},{\"end\":41315,\"start\":41307},{\"end\":41748,\"start\":41746},{\"end\":41761,\"start\":41756},{\"end\":41774,\"start\":41768},{\"end\":41786,\"start\":41783},{\"end\":42185,\"start\":42172},{\"end\":42195,\"start\":42187},{\"end\":42366,\"start\":42359},{\"end\":42379,\"start\":42373},{\"end\":42394,\"start\":42387},{\"end\":42406,\"start\":42401},{\"end\":42427,\"start\":42420},{\"end\":42441,\"start\":42436},{\"end\":42451,\"start\":42448},{\"end\":42464,\"start\":42453},{\"end\":42773,\"start\":42766},{\"end\":42790,\"start\":42779},{\"end\":42805,\"start\":42800},{\"end\":42820,\"start\":42813},{\"end\":42832,\"start\":42830},{\"end\":42852,\"start\":42846},{\"end\":42870,\"start\":42864},{\"end\":43149,\"start\":43144},{\"end\":43164,\"start\":43155},{\"end\":43178,\"start\":43170},{\"end\":43408,\"start\":43401},{\"end\":43428,\"start\":43418},{\"end\":43442,\"start\":43434},{\"end\":43458,\"start\":43449},{\"end\":43818,\"start\":43812},{\"end\":43832,\"start\":43825},{\"end\":43846,\"start\":43839},{\"end\":43861,\"start\":43858},{\"end\":43876,\"start\":43870},{\"end\":43892,\"start\":43886},{\"end\":43904,\"start\":43900},{\"end\":43912,\"start\":43910},{\"end\":43925,\"start\":43922},{\"end\":44218,\"start\":44209},{\"end\":44224,\"start\":44220},{\"end\":44599,\"start\":44596},{\"end\":44613,\"start\":44608},{\"end\":44628,\"start\":44620},{\"end\":44642,\"start\":44636},{\"end\":44659,\"start\":44653},{\"end\":44948,\"start\":44939},{\"end\":44963,\"start\":44956},{\"end\":44974,\"start\":44972},{\"end\":45306,\"start\":45303},{\"end\":45322,\"start\":45316},{\"end\":45345,\"start\":45338},{\"end\":45576,\"start\":45570},{\"end\":45590,\"start\":45584},{\"end\":45605,\"start\":45597},{\"end\":45622,\"start\":45616},{\"end\":45908,\"start\":45901},{\"end\":45922,\"start\":45915},{\"end\":45935,\"start\":45929},{\"end\":45952,\"start\":45943},{\"end\":45965,\"start\":45960},{\"end\":45980,\"start\":45975},{\"end\":45995,\"start\":45989},{\"end\":46013,\"start\":46003},{\"end\":46408,\"start\":46406},{\"end\":46423,\"start\":46415},{\"end\":46437,\"start\":46433},{\"end\":46445,\"start\":46441},{\"end\":46458,\"start\":46456},{\"end\":46476,\"start\":46469},{\"end\":46492,\"start\":46484},{\"end\":46505,\"start\":46499},{\"end\":46514,\"start\":46511},{\"end\":46525,\"start\":46522},{\"end\":46535,\"start\":46527},{\"end\":46865,\"start\":46861},{\"end\":46877,\"start\":46874},{\"end\":46890,\"start\":46886},{\"end\":46907,\"start\":46898},{\"end\":46929,\"start\":46916},{\"end\":46940,\"start\":46938}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":51926976},\"end\":35969,\"start\":35612},{\"attributes\":{\"doi\":\"arXiv:1406.1078\",\"id\":\"b1\"},\"end\":36471,\"start\":35971},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":213152193},\"end\":36887,\"start\":36473},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b3\"},\"end\":37251,\"start\":36889},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":47021242},\"end\":37560,\"start\":37253},{\"attributes\":{\"doi\":\"arXiv:1909.09436\",\"id\":\"b5\"},\"end\":37946,\"start\":37562},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8820379},\"end\":38507,\"start\":37948},{\"attributes\":{\"id\":\"b7\"},\"end\":38650,\"start\":38509},{\"attributes\":{\"doi\":\"arXiv:2001.00059\",\"id\":\"b8\"},\"end\":38954,\"start\":38652},{\"attributes\":{\"doi\":\"arXiv:1408.5882\",\"id\":\"b9\"},\"end\":39162,\"start\":38956},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":794019},\"end\":40077,\"start\":39164},{\"attributes\":{\"doi\":\"arXiv:1910.13461\",\"id\":\"b11\"},\"end\":40608,\"start\":40079},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7139779},\"end\":41183,\"start\":40610},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b13\"},\"end\":41639,\"start\":41185},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":199453025},\"end\":42066,\"start\":41641},{\"attributes\":{\"id\":\"b15\"},\"end\":42355,\"start\":42068},{\"attributes\":{\"doi\":\"arXiv:1802.05365\",\"id\":\"b16\"},\"end\":42732,\"start\":42357},{\"attributes\":{\"doi\":\"arXiv:1909.01066\",\"id\":\"b17\"},\"end\":43096,\"start\":42734},{\"attributes\":{\"doi\":\"arXiv:1906.01502\",\"id\":\"b18\"},\"end\":43333,\"start\":43098},{\"attributes\":{\"id\":\"b19\"},\"end\":43721,\"start\":43335},{\"attributes\":{\"doi\":\"arXiv:1910.10683\",\"id\":\"b20\"},\"end\":44205,\"start\":43723},{\"attributes\":{\"doi\":\"arXiv:1509.00685\",\"id\":\"b21\"},\"end\":44516,\"start\":44207},{\"attributes\":{\"doi\":\"arXiv:1904.01766\",\"id\":\"b22\"},\"end\":44880,\"start\":44518},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7961699},\"end\":45203,\"start\":44882},{\"attributes\":{\"doi\":\"arXiv:1503.00075\",\"id\":\"b24\"},\"end\":45563,\"start\":45205},{\"attributes\":{\"doi\":\"arXiv:1912.13283\",\"id\":\"b25\"},\"end\":45865,\"start\":45565},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13756489},\"end\":46296,\"start\":45867},{\"attributes\":{\"doi\":\"arXiv:1609.08144\",\"id\":\"b27\"},\"end\":46852,\"start\":46298},{\"attributes\":{\"doi\":\"arXiv:1906.08237\",\"id\":\"b28\"},\"end\":47254,\"start\":46854}]", "bib_title": "[{\"end\":35682,\"start\":35612},{\"end\":36550,\"start\":36473},{\"end\":37269,\"start\":37253},{\"end\":38002,\"start\":37948},{\"end\":39226,\"start\":39164},{\"end\":40694,\"start\":40610},{\"end\":41737,\"start\":41641},{\"end\":44932,\"start\":44882},{\"end\":45892,\"start\":45867}]", "bib_author": "[{\"end\":35694,\"start\":35684},{\"end\":35708,\"start\":35694},{\"end\":35719,\"start\":35708},{\"end\":35731,\"start\":35719},{\"end\":36081,\"start\":36066},{\"end\":36103,\"start\":36081},{\"end\":36120,\"start\":36103},{\"end\":36138,\"start\":36120},{\"end\":36154,\"start\":36138},{\"end\":36170,\"start\":36154},{\"end\":36185,\"start\":36170},{\"end\":36565,\"start\":36552},{\"end\":36583,\"start\":36565},{\"end\":36594,\"start\":36583},{\"end\":36617,\"start\":36594},{\"end\":36903,\"start\":36889},{\"end\":36919,\"start\":36903},{\"end\":36931,\"start\":36919},{\"end\":36951,\"start\":36931},{\"end\":37284,\"start\":37271},{\"end\":37298,\"start\":37284},{\"end\":37311,\"start\":37298},{\"end\":37647,\"start\":37633},{\"end\":37661,\"start\":37647},{\"end\":37676,\"start\":37661},{\"end\":37697,\"start\":37676},{\"end\":37716,\"start\":37697},{\"end\":38021,\"start\":38004},{\"end\":38038,\"start\":38021},{\"end\":38052,\"start\":38038},{\"end\":38070,\"start\":38052},{\"end\":38553,\"start\":38539},{\"end\":38716,\"start\":38701},{\"end\":38733,\"start\":38716},{\"end\":38753,\"start\":38733},{\"end\":38765,\"start\":38753},{\"end\":39025,\"start\":39015},{\"end\":39243,\"start\":39228},{\"end\":39255,\"start\":39243},{\"end\":39272,\"start\":39255},{\"end\":39294,\"start\":39272},{\"end\":39313,\"start\":39294},{\"end\":39330,\"start\":39313},{\"end\":39344,\"start\":39330},{\"end\":39355,\"start\":39344},{\"end\":39372,\"start\":39355},{\"end\":39386,\"start\":39372},{\"end\":40206,\"start\":40194},{\"end\":40218,\"start\":40206},{\"end\":40253,\"start\":40218},{\"end\":40264,\"start\":40253},{\"end\":40278,\"start\":40264},{\"end\":40296,\"start\":40278},{\"end\":40710,\"start\":40696},{\"end\":40727,\"start\":40710},{\"end\":41197,\"start\":41185},{\"end\":41207,\"start\":41197},{\"end\":41220,\"start\":41207},{\"end\":41232,\"start\":41220},{\"end\":41246,\"start\":41232},{\"end\":41258,\"start\":41246},{\"end\":41269,\"start\":41258},{\"end\":41281,\"start\":41269},{\"end\":41299,\"start\":41281},{\"end\":41317,\"start\":41299},{\"end\":41750,\"start\":41739},{\"end\":41763,\"start\":41750},{\"end\":41776,\"start\":41763},{\"end\":41788,\"start\":41776},{\"end\":42187,\"start\":42167},{\"end\":42197,\"start\":42187},{\"end\":42368,\"start\":42357},{\"end\":42381,\"start\":42368},{\"end\":42396,\"start\":42381},{\"end\":42408,\"start\":42396},{\"end\":42429,\"start\":42408},{\"end\":42443,\"start\":42429},{\"end\":42453,\"start\":42443},{\"end\":42466,\"start\":42453},{\"end\":42775,\"start\":42760},{\"end\":42792,\"start\":42775},{\"end\":42807,\"start\":42792},{\"end\":42822,\"start\":42807},{\"end\":42834,\"start\":42822},{\"end\":42854,\"start\":42834},{\"end\":42872,\"start\":42854},{\"end\":43151,\"start\":43138},{\"end\":43166,\"start\":43151},{\"end\":43180,\"start\":43166},{\"end\":43410,\"start\":43396},{\"end\":43430,\"start\":43410},{\"end\":43444,\"start\":43430},{\"end\":43460,\"start\":43444},{\"end\":43820,\"start\":43806},{\"end\":43834,\"start\":43820},{\"end\":43848,\"start\":43834},{\"end\":43863,\"start\":43848},{\"end\":43878,\"start\":43863},{\"end\":43894,\"start\":43878},{\"end\":43906,\"start\":43894},{\"end\":43914,\"start\":43906},{\"end\":43927,\"start\":43914},{\"end\":44220,\"start\":44207},{\"end\":44226,\"start\":44220},{\"end\":44601,\"start\":44591},{\"end\":44615,\"start\":44601},{\"end\":44630,\"start\":44615},{\"end\":44644,\"start\":44630},{\"end\":44661,\"start\":44644},{\"end\":44950,\"start\":44934},{\"end\":44965,\"start\":44950},{\"end\":44976,\"start\":44965},{\"end\":45308,\"start\":45293},{\"end\":45324,\"start\":45308},{\"end\":45347,\"start\":45324},{\"end\":45578,\"start\":45565},{\"end\":45592,\"start\":45578},{\"end\":45607,\"start\":45592},{\"end\":45624,\"start\":45607},{\"end\":45910,\"start\":45894},{\"end\":45924,\"start\":45910},{\"end\":45937,\"start\":45924},{\"end\":45954,\"start\":45937},{\"end\":45967,\"start\":45954},{\"end\":45982,\"start\":45967},{\"end\":45997,\"start\":45982},{\"end\":46015,\"start\":45997},{\"end\":46410,\"start\":46398},{\"end\":46425,\"start\":46410},{\"end\":46439,\"start\":46425},{\"end\":46447,\"start\":46439},{\"end\":46460,\"start\":46447},{\"end\":46478,\"start\":46460},{\"end\":46494,\"start\":46478},{\"end\":46507,\"start\":46494},{\"end\":46516,\"start\":46507},{\"end\":46527,\"start\":46516},{\"end\":46537,\"start\":46527},{\"end\":46867,\"start\":46854},{\"end\":46879,\"start\":46867},{\"end\":46892,\"start\":46879},{\"end\":46909,\"start\":46892},{\"end\":46931,\"start\":46909},{\"end\":46942,\"start\":46931}]", "bib_venue": "[{\"end\":38231,\"start\":38159},{\"end\":39669,\"start\":39536},{\"end\":40868,\"start\":40806},{\"end\":35782,\"start\":35731},{\"end\":36064,\"start\":35971},{\"end\":36669,\"start\":36617},{\"end\":37047,\"start\":36967},{\"end\":37385,\"start\":37311},{\"end\":37631,\"start\":37562},{\"end\":38157,\"start\":38070},{\"end\":38537,\"start\":38509},{\"end\":38699,\"start\":38652},{\"end\":39013,\"start\":38956},{\"end\":39534,\"start\":39386},{\"end\":40192,\"start\":40079},{\"end\":40804,\"start\":40727},{\"end\":41388,\"start\":41333},{\"end\":41837,\"start\":41788},{\"end\":42165,\"start\":42068},{\"end\":42522,\"start\":42482},{\"end\":42758,\"start\":42734},{\"end\":43136,\"start\":43098},{\"end\":43394,\"start\":43335},{\"end\":43804,\"start\":43723},{\"end\":44343,\"start\":44242},{\"end\":44589,\"start\":44518},{\"end\":45025,\"start\":44976},{\"end\":45291,\"start\":45205},{\"end\":45692,\"start\":45640},{\"end\":46064,\"start\":46015},{\"end\":46396,\"start\":46298},{\"end\":47030,\"start\":46958}]"}}}, "year": 2023, "month": 12, "day": 17}