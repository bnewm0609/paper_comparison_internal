{"id": 59316566, "updated": "2023-09-28 05:43:53.675", "metadata": {"title": "Optimization in Heterogeneous Networks", "authors": "[{\"first\":\"Tian\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Anit\",\"last\":\"Sahu\",\"middle\":[\"Kumar\"]},{\"first\":\"Manzil\",\"last\":\"Zaheer\",\"middle\":[]},{\"first\":\"Maziar\",\"last\":\"Sanjabi\",\"middle\":[]},{\"first\":\"Ameet\",\"last\":\"Talwalkar\",\"middle\":[]},{\"first\":\"Virginia\",\"last\":\"Smith\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Federated Learning is a distributed learning paradigm with two key challenges that differentiate it from traditional distributed optimization: (1) significant variability in terms of the systems characteristics on each device in the network (systems heterogeneity), and (2) non-identically distributed data across the network (statistical heterogeneity). In this work, we introduce a framework, FedProx, to tackle heterogeneity in federated networks. FedProx can be viewed as a generalization and re-parametrization of FedAvg, the current state-of-the-art method for federated learning. While this re-parameterization makes only minor modifications to the method itself, these modifications have important ramifications both in theory and in practice. Theoretically, we provide convergence guarantees for our framework when learning over data from non-identical distributions (statistical heterogeneity), and while adhering to device-level systems constraints by allowing each participating device to perform a variable amount of work (systems heterogeneity). Practically, we demonstrate that FedProx allows for more robust convergence than FedAvg across a suite of realistic federated datasets. In particular, in highly heterogeneous settings, FedProx demonstrates significantly more stable and accurate convergence behavior relative to FedAvg---improving absolute test accuracy by 22% on average.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1812.06127", "mag": "3038022836", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mlsys/LiSZSTS20", "doi": null}}, "content": {"source": {"pdf_hash": "7d015b201729851710e5f60e17151e88ca0207de", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "204e0b49eab34d013ef9933b0ddafcaf26b45ef4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7d015b201729851710e5f60e17151e88ca0207de.txt", "contents": "\nFederated Optimization in Heterogeneous Networks min w f (w) = k [ F k (w) ] Motivation\nCMUCopyright CMU\n\nTian Li \nFederated Optimization in Heterogeneous Networks min w f (w) = k [ F k (w) ] Motivation\n\nAnit Kumar Sahu (BCAI), Manzil Zaheer (Google Research), Maziar Sanjabi (Facebook AI), Ameet Talwalkar (CMU & Determined AI)\nVirginia SmithCMU\n\nmin W k F k (W k , X k ) quantifies statistical heterogeneity B T = O ( f (w 0 ) \u2212 f * \u03c1\u03b5 )\nintroduce -inexactness to capture systems heterogeneity \u03b3 t k : related to , ,\n\u03c1 \u03bc B \u03b3 t k A proximal term\nHow to tune automatically (hyper-parameter optimization for federated learning)? Can we quantify the statistical heterogeneity a priori and leverage it for improved performance? Better privacy metrics and mechanisms for federated learning? \u2026\u2026 \u03bc The proximal term (1) safely incorporates noisy updates from variable local work;\n\n(2) explicitly limits the impact of local updates;\n\n(3) makes the method more amenable to theoretical\n\n\nMLSys 2020\n\nFederated Learning: Challenges, Methods, and Future Directions (Signal Processing Magazine, arxiv.org/abs/1908.07873) \n\n\nCode & Manuscript: www.cs.cmu.edu/~litian/; Github: github.com/litian96/FedProxCloud-\nbased \ntraining \n\nmodel \n\nfederated \ntraining \n\nmodel \nmodel \nmodel \nmodel \n\nraw \ndata \n\nprediction \nmodel \nupdates \n\nglobal \nmodel \n\nTwo of the major challenges \n\nUntil convergence: \n\n1. Server samples devices, and sends the current global model to all chosen devices \n2. Each device solves the following subproblem by performing variable local updates based on the \n\nunderlying systems constraints \n\n3. Server aggregates local updates and forms a new global model \n\nAssumptions \n\nAssumption 1: Bounded Dissimilarity \n\nAssumption 2: \nModified Local subproblem is convex \n\n& smooth \nAssumption 3: \n\nEach local subproblem is solved \ninexactly to some optimality \nRate is general \nCovers both convex, and non-convex loss functions \nIndependent of the local solver \nAgnostic of the sampling method \nThe same asymptotic convergence guarantee as SGD \n\n[Theorem] Obtain suboptimality , after T iterations, with: \n\n\u03b5 \n\nLEAF: A Benchmark for Learning in Federated Settings (website: leaf.cmu.edu) \n\nZero \nstragglers \n\n50% \nstragglers \n\n90% \nstragglers \n\nFedProx (!>0) \nFedProx (!=0) \nFedAvg \n\nEffects of Idea 1 (partial work): \nCompare \nwith \n\nallowing for variable amounts of work to \n\nbe performed can help convergence in \n\nthe presence of systems heterogeneity \n\nEffects of Idea 2 (the proximal term): \nCompare \nwith \n\nleads to more stable convergence \n\nand enables otherwise divergent \n\nmethods to converge \n\n\u03bc > 0 \n\nGlobal objective: \n\nLocal objective on device : \n\nk \n\nProposed FedProx method \n\nIdea 1: Allow for partial work to be performed \non local devices based on systems constraints \n\nIdea 2: At each round, each selected device \nsolves a modified local subproblem: \n\nGeneralization of the popular method FedAvg \n\n(FedAvg + allowing for variable local work + \nproximal term = FedProx) \nGeneral: Can use any local solver; theory covers \n\nboth convex and non-convex losses \n\nIncreasing statistical heterogeneity leads \n\nto worse convergence; Setting > 0 can \nhelp to combat this \n\n\u03bc \n\nCharacterize statistical heterogeneity: B-dissimilarity B(w) = \nk [ \u2225\u2207F k (w)\u2225 2 \n] \n\n\u2225\u2207f (w)\u2225 2 \n\nSystems heterogeneity \n\nStatistical heterogeneity \n\nKey idea: Dropping stragglers or naively incorporating partial \nupdates from stragglers implicitly increase statistical heterogeneity \n\nMethod: Simple algorithmic modifications to current state-of-\nthe-art method (adding a proximal term to the local \nsubproblem while tolerating partial updates) \nContributions \n(Theoretically) Provide convergence guarantees (rates as \n\nfunctions of statistical and systems heterogeneity) \n(Practically) Allow for more robust convergence (improved \n\nabsolute accuracy by 22% in highly heterogeneous \nenvironments) \n\n\n", "annotations": {"author": "[{\"end\":115,\"start\":107}]", "publisher": "[{\"end\":92,\"start\":89},{\"end\":347,\"start\":344}]", "author_last_name": "[{\"end\":114,\"start\":112}]", "author_first_name": "[{\"end\":111,\"start\":107}]", "author_affiliation": null, "title": "[{\"end\":88,\"start\":1},{\"end\":203,\"start\":116}]", "venue": "[{\"end\":329,\"start\":205}]", "abstract": null, "bib_ref": "[{\"end\":1109,\"start\":1002}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":3887,\"start\":1111}]", "paragraph": "[{\"end\":519,\"start\":441},{\"end\":874,\"start\":548},{\"end\":926,\"start\":876},{\"end\":977,\"start\":928},{\"end\":1110,\"start\":992}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":440,\"start\":349},{\"attributes\":{\"id\":\"formula_1\"},\"end\":547,\"start\":520}]", "table_ref": null, "section_header": "[{\"end\":990,\"start\":980}]", "table": "[{\"end\":3887,\"start\":1192}]", "figure_caption": "[{\"end\":1192,\"start\":1113}]", "figure_ref": null, "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}