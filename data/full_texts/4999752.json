{"id": 4999752, "updated": "2023-09-30 23:18:34.788", "metadata": {"title": "Learning to Extract Coherent Summary via Deep Reinforcement Learning", "authors": "[{\"first\":\"Yuxiang\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Baotian\",\"last\":\"Hu\",\"middle\":[]}]", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "publication_date": {"year": 2018, "month": 4, "day": 19}, "abstract": "Coherence plays a critical role in producing a high-quality summary from a document. In recent years, neural extractive summarization is becoming increasingly attractive. However, most of them ignore the coherence of summaries when extracting sentences. As an effort towards extracting coherent summaries, we propose a neural coherence model to capture the cross-sentence semantic and syntactic coherence patterns. The proposed neural coherence model obviates the need for feature engineering and can be trained in an end-to-end fashion using unlabeled data. Empirical results show that the proposed neural coherence model can efficiently capture the cross-sentence coherence patterns. Using the combined output of the neural coherence model and ROUGE package as the reward, we design a reinforcement learning method to train a proposed neural extractive summarizer which is named Reinforced Neural Extractive Summarization (RNES) model. The RNES model learns to optimize coherence and informative importance of the summary simultaneously. Experimental results show that the proposed RNES outperforms existing baselines and achieves state-of-the-art performance in term of ROUGE on CNN/Daily Mail dataset. The qualitative evaluation indicates that summaries produced by RNES are more coherent and readable.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1804.07036", "mag": "2962762898", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/WuH18", "doi": "10.1609/aaai.v32i1.11987"}}, "content": {"source": {"pdf_hash": "121e14fafc90d48e334bc9966b768cf33b5980d7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1804.07036v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bc4d8fcdcb310fb2079c00ec209bb0dacc264ba8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/121e14fafc90d48e334bc9966b768cf33b5980d7.txt", "contents": "\nLearning to Extract Coherent Summary via Deep Reinforcement Learning\n\n\nYuxiang Wu \nHong Kong University of Science and Technology Hong Kong\nUniversity of Massachusetts Medical School MA\nUSA\n\nBaotian Hu baotian.hu@umassmed.edu \nHong Kong University of Science and Technology Hong Kong\nUniversity of Massachusetts Medical School MA\nUSA\n\nLearning to Extract Coherent Summary via Deep Reinforcement Learning\n\nCoherence plays a critical role in producing a high-quality summary from a document. In recent years, neural extractive summarization is becoming increasingly attractive. However, most of them ignore the coherence of summaries when extracting sentences. As an effort towards extracting coherent summaries, we propose a neural coherence model to capture the cross-sentence semantic and syntactic coherence patterns. The proposed neural coherence model obviates the need for feature engineering and can be trained in an end-to-end fashion using unlabeled data. Empirical results show that the proposed neural coherence model can efficiently capture the cross-sentence coherence patterns. Using the combined output of the neural coherence model and ROUGE package as the reward, we design a reinforcement learning method to train a proposed neural extractive summarizer which is named Reinforced Neural Extractive Summarization (RNES) model. The RNES model learns to optimize coherence and informative importance of the summary simultaneously. Experimental results show that the proposed RNES outperforms existing baselines and achieves state-of-the-art performance in term of ROUGE on CNN/Daily Mail dataset. The qualitative evaluation indicates that summaries produced by RNES are more coherent and readable.\n\nIntroduction\n\nAlthough deep neural networks (DNN) have dominated almost every field of natural language processing, such as sentiment classification (Tang, Qin, and Liu 2015), machine translation (Bahdanau, Cho, and Bengio 2014) and question answering (Zhou et al. 2015), generating high-quality summaries from long documents is still a very challenging task. Most of the recent works on abstractive summarization focus on headline generation from one paragraph (Rush, Chopra, and Weston 2015) or several sentences (Hu, Chen, and Zhu 2015) by using sequence-to-sequence architectures borrowed from neural machine translation. However, they bypass the fundamental problems in summarization, namely the representation of long documents and the generation of multiple coherent sentences. These models fail to produce readable, informative and coherent sentences when dealing with long documents. There is still a long way to go before abstractive summarization becomes practicable.\n\nIn contrast, extracting sentences from documents to form summaries, also named extractive summarization, is a more practical approach, because it can guarantee the grammatical correctness of the produced summary and its semantic relevance with the corresponding document. Extractive summarization has been studied for several decades. Traditional methods mainly focus on scoring sentences using graph-based method (Erkan and Radev 2004), submodular functions (Lin and Bilmes 2011) or integer linear programming (Berg-Kirkpatrick, Gillick, and Klein 2011), which are coupled with handcrafted features. As the distributed representation shows its outstanding capability in capturing semantic and syntactic information of text (Mikolov et al. 2013;Hu et al. 2016), there is an emergence of works that use the deep neural networks to extract salient sentences (Cheng and Lapata 2016;Nallapati, Zhai, and Zhou 2017). Although DNN-based methods can identify the important sentences from the documents, they still lack the ability to ensure coherence of the summary. They may produce summaries with sentences that are semantically independent to each other, which would cause difficulty for readers to comprehend the story as a whole.\n\nThe coherence of a summary is essential for its readability and clarity. However, to the best of our knowledge, there is no work incorporating coherence into the neural extractive model while extracting sentences. This task is challenging because it is difficult to include coherence into the objective function of supervised learning models because the coherence also depends on sentences that are eventually extracted when the inference is performed. In contrary, reinforcement learning (RL) is suitable for this case. RL algorithms aim to train an agent to maximize the reward by interacting with an environment. It is often used in settings where the objective is not differentiable with respect to the model parameters, such as works done by (Paulus, Xiong, and Socher 2017;Nguyen, Boyd-Graber, and Daume 2017).\n\nIn this paper, we focus on incorporating coherence into neural extractive model via reinforcement learning. We need a model that estimates coherence in the first place. During the past decades, works in coherence modeling mainly focus on topical coherence. One of the most popular methods is the entity grid model (Barzilay and Lapata 2008) which constructs a grid to represent grammatical and semantic transitions of entities between sentences. However, entity grid model depends on the named entity recognition system whose performance may become the bottleneck of entity grid model. Furthermore, entity grid models transitions of different entities separately, so it fails to capture semantic correlation between entities. Therefore, we instead use a neural coherence model which learns to estimate the coherence degree between two sentences by their distributed representation in an end-to-end fashion.\n\nThe contribution of this paper is twofold. First, we propose a novel neural coherence model which exploits the distributed representation of sentences instead of sparse handcrafted features. The proposed neural coherence model does not rely on any entity recognition systems and can be trained from scratch in an end-to-end fashion. The neural coherence model can capture the cross-sentence local entity transitions and the discourse relations with multiple layers of convolution and max-pooling. The experimental results show that, given one sentence, the neural coherence model can effectively identify the appropriate next sentence to compose a coherent sentence pair.\n\nSecond, we design a novel Reinforced Neural Extractive Summarization (RNES) model that incorporates coherence into neural extractive summarization with reinforcement learning. The output of the neural coherence model is used as immediate rewards during the training of RNES so that it learns to extract coherent summaries. ROUGE score is utilized as the final reward, and hence the proposed RNES model finds a balance between coherence and informative importance of sentences. We evaluate the proposed RNES model on CNN/Daily Mail dataset, and the results show that it achieves the state-of-the-art performance on ROUGE metrics. The qualitative evaluation indicates that the summaries produced by RNES are more informative and coherent.\n\n\nRelated Work\n\nOur research builds on previous works in the field of neural extractive summarization, reinforcement learning, and coherence modeling.\n\nMuch progress has been made beyond traditional frameworks of extractive summarization models. Most of the recent works are based on deep neural networks. For example, (Filippova et al. 2015) use a recurrent neural network (RNN) to delete words from a sentence for sentence compression task. (Cheng and Lapata 2016) use a convolutional neural network to encode sentences, and then an RNN reads the sentence representations sequentially to encode the document. Finally, another RNN is used to label sentences sequentially, taking the encoded document representation and the previously labeled sentences into account. (Cheng and Lapata 2016) mainly consider the importance of sentences and the non-redundancy of the summary. (Nallapati, Zhai, and Zhou 2017) use a similar architecture to encode document, but it explicitly models sentence content, salience, novelty and position in its model for extracting sentences.\n\nOur work is also related to the application of reinforcement learning in document summarization. Different from classification problem whose output is a single label, the goal of extractive summarization is to make a sequence of extraction decisions. Hence, it is suitable to formulate extractive summarization as a reinforcement learning problem that tries to maximize the quality of the summaries. Although (Ryang and Abekawa 2012;Rioux, Hasan, and Chali 2014;Henb, Mieskes, and Gurevych 2015) use value-based RL algorithms for extractive summarization, all of them are based on handcrafted features and do not consider coherence as the part of the reward. With the recent resurgence of DNN models, deep reinforcement learning has drawn considerable attention. For example, (Paulus, Xiong, and Socher 2017) use the ROUGE-L score as the reinforcement reward and self-critical policy gradient training algorithm to train an abstractive summarization model. (Ayana et al. 2016;Ranzato et al. 2015) show that directly optimizing the evaluation metrics via reinforcement learning is more effective than optimizing likelihood for the sequence generation problems. However, these works focus on abstractive summarization and neglect coherence. To our knowledge, no work has been done to apply RL to neural extractive summarization with coherence as part of the reward, and our work is the first step towards filling this gap.\n\nAn essential requirement for summarization systems is the coherence of its output. Coherence is what makes multiple sentences semantically, logically and syntactically coherent (Yao, Wan, and Xiao 2017). Entity grid model proposed by (Barzilay and Lapata 2008) is widely used to model the coherence of text. However, the discrete representation of entity grid suffers from the curse of dimensionality which limits its application on neural summarization. (Nguyen and Joty 2017) presented a local coherence model based on a convolutional neural network that operates over the distributed representation of entity grid. Since (Nguyen and Joty 2017) still rely on the entity grid features, it fails to exploit the full power of DNN in learning the hidden distributed representation of text automatically. (Li and Hovy 2014) use the recurrent and recursive neural network to obtain the distributed representation of sentences and then use a pairwise ranking method to train the coherence model. This model does not need any feature engineering, but it is weak in capturing the local entity transition because of the lack of cross-sentence local interaction. Our neural coherence model can be trained from scratch in an end-to-end fashion. It can model the local entity transitions as well as the syntactic and semantic relation between sentences via different levels of cross-sentence local interaction.\n\n\nNeural Extractive Summarization Model\n\nWe need to construct a neural extractive summarization (NES) model before training it with the reinforcement learning algorithm. In this section, we present the detailed architecture of the proposed NES.\n\nThe extractive summarization model reads the document and sequentially selects a set of sentences to compose a summary. Given a document X = (x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n ) that consists of n sentences, the NES model outputs a sequence of binary decisions Y = (y 1 , y 2 , \u00b7 \u00b7 \u00b7 , y n ), where n denotes the number of sentences in the document and y i \u2208 {0, 1} indicates whether sentence x i is selected. Then the extracted summary is a sequence of l sentences denoted as\nG = extract(X, Y ) = (x q1 , \u00b7 \u00b7 \u00b7 , x q l ),\nwhere 1 \u2264 q 1 < \u00b7 \u00b7 \u00b7 < q l \u2264 n, and y qi = 1 for i = 1, \u00b7 \u00b7 \u00b7 , l. The proposed NES uses a hierarchical deep neural network to encode the document. At the word-level, convolutional neural network (CNN) is used to extract features of the words and their context. Let x t = (w 1 , w 2 , \u00b7 \u00b7 \u00b7 , w m ) denotes the t-th sentence with m words, and v denotes the size of word embedding. Then the sentence could be represented by a matrix M \u2208 R m\u00d7v . Multiple convolution kernels with different kernel size are used to extract features of word w i :\nf j i = M i:i+kj \u22121 W j + b j , where W j , b j ,\nk j are the kernel weight matrix, the bias and the kernel size of the j-th convolution kernel respectively. The word w i is represented by concatenating the feature\nmaps f wi = [f 1 i ; f 2 i ; \u00b7 \u00b7 \u00b7 ].\nThe sentence x t is represented by the mean of all its word features\nx t = 1 m m i=1 f wi .\nAt the sentence-level, we use a bi-directional gated recurrent unit (Bi-GRU) to model the context of sentences. Gated recurrent unit is a variant of recurrent neural network proposed by (Chung et al. 2014). It has two gates, an update gate z t and a reset gate r t . The hidden state h t at time step t could be computed with following equations:\nz t = \u03c3(W z x t + V z h t\u22121 + b z ), r t = \u03c3(W r x t + V r h t\u22121 + b r ), h t = tanh(W h x t + V h (r t h t\u22121 ) + b h ), h t = (1 \u2212 z t ) \u0125 t + z t h t\u22121 ,\nwhere represents element-wise product, W * 's, V * 's and b * 's are parameters of GRU.\n\nUsing Bi-GRU, the representation of the t-th sentence x t is transformed to a forward hidden state \u2212 \u2192 h t and a backward hidden state \u2190 \u2212 h t . Both states are concatenated to form the contextual representation of the t-th sentence\n\u2190 \u2192 h t = [ \u2212 \u2192 h t ; \u2190 \u2212 h t ].\nThe entire document is represented as d by a nonlinear transformation of the mean over all sentence representations:\nd = tanh(W d ( 1 n n t=1 \u2190 \u2192 h t ) + b d ),\nwhere W d and b d are parameters of the transformation. The probability of extraction decisions Y conditioned on document X could be factorized as Pr(Y |X) = n t=1 Pr(y t |X, y 1:t\u22121 ). The probability of extracting the tth sentence is computed as\nPr(y t = 1|X, y 1:t\u22121 ) = MLP( \u2190 \u2192 h t , g t\u22121 , d),(1)\nwhere g t\u22121 represents all sentences extracted before time t. MLP(\u00b7) means a multilayer perceptron that outputs a probability\nMLP( \u2190 \u2192 h t , g t\u22121 , d) = \u03c3(W 2 tanh(W 1 [ \u2190 \u2192 h t , g t\u22121 , d] + b 1 ) + b 2 ),\nwhere W 1 , W 2 , b 1 and b 2 are parameters of the MLP and \u03c3(\u00b7) is the sigmoid function.\n\nSince NES is trained with supervised learning, ground truth extraction labels (\u1e99 1 , \u00b7 \u00b7 \u00b7 ,\u1e99 n ) are available during training. Then the representation of sentences selected before or at time t is\ng t = g t\u22121 +\u1e99 t tanh(W g \u2190 \u2192 h t ).\nThe NES model is pretrained by minimizing the negative log-likelihood of the ground truth extraction labels\nL pretrain (\u0398) = \u2212 N i=1 Ni t=1 \u1e99 i t log Pr(y i t = 1|X i ,\u1e99 i 1:t\u22121 ) +(1 \u2212\u1e99 i t ) log Pr(y i t = 0|X i ,\u1e99 i 1:t\u22121 ) .\n\nReinforced Neural Extractive Summarization Model\n\nAfter the NES model is pretrained with supervised learning, we further train it with reinforcement learning to extract coherent and informative summaries by maximizing coherence and ROUGE scores. In this section, we first introduce the REINFORCE algorithm and then describe the proposed neural coherence model and the ROUGE score reward. The overall training algorithm is illustrated in Algorithm 1.\n\n\nReinforcement Learning\n\nThe problem of extractive summarization could be formulated as a reinforcement learning problem. The RNES model can be considered as an agent that extracts sentences sequentially from the document. At each time step t, the agent is in state s t = (X, y 1:t\u22121 ) which includes the document and the previous selections. Agent would take an action y t \u2208 {0, 1} that decides whether sentence x t is extracted or not. After the agent takes the action y t , it may receive an immediate reward r t that shows how good the action is. The reward could also be delayed. When the agent finishes extracting sentences from the document, it will receive a final reward r \u22121 that indicates the performance of the entire action sequence (y 1 , y 2 , \u00b7 \u00b7 \u00b7 , y n ).\n\nWe use the REINFORCE algorithm to train our RNES model. It is a kind of policy gradient method proposed by (Williams 1992), and it maximizes the performance of the agent by updating its policy parameters. The policy is defined as the probability of taking an action at time t given a state, which is parameterized by \u0398:\n\u03c0(a|s t , \u0398) def = Pr(y t = a|s t , \u0398) def = Pr(y t = a|X, y 1:t\u22121 , \u0398).\nIn our case, \u0398 represents all the parameters in the RNES model. We use a shorthand \u03c0 \u0398 to denote the policy \u03c0 parameterized by \u0398. By applying Equation 1, we have\n\u03c0 \u0398 (a = 1|s t ) = MLP \u0398 ( \u2190 \u2192 h t , g t\u22121 , d).\nLet s 0 = X represents the initial state when no action is taken yet, and v \u03c0\u0398 (s 0 ) be the value function that represents the expected return starting with state s 0 by following policy \u03c0 \u0398 . Return at time t is defined as R t = \u221e i=t \u03b3 i\u2212t r i , where \u03b3 is the discount factor. The objective of REINFORCE is defined as maximizing the value of initial state v \u03c0\u0398 (s 0 ), or minimizing its negative L RF (\u0398) = \u2212v \u03c0\u0398 (s 0 ). Therefore, the parameters should be updated by the gradient of L RF (\u0398) with respect to parameters \u0398:\n\u2207L RF (\u0398) = \u2212\u2207v \u03c0\u0398 (s 0 ) = \u2212 n t=1 \u03b3 t Pr(s t |s 0 , \u03c0 \u0398 ) a q \u03c0\u0398 (s t , a)\u2207\u03c0 \u0398 (a|s t ),\nwhere q \u03c0\u0398 (s, a) is the action-value function that represents the expected return after taking action a at state s with policy \u03c0 \u0398 .\n\nSince the state space is too large, it is infeasible to compute the exact value of the gradient. We use Monte Carlo sampling to approximate the gradient:\n\u2207L RF (\u0398) = \u2212E\u1ef9 t,st\u223c\u03c0\u0398 \u03b3 tR t \u2207 log \u03c0 \u0398 (\u1ef9 t |s t ) ,(2)\nwheres t and\u1ef9 t are randomly sampled from \u03c0 \u0398 ,R t is the actual return received sinces t and\u1ef9 t . A detailed proof of Equation 2 could be found in (Sutton and Barto 1998) and is omitted for brevity here. The parameters \u0398 are updated as follows:\n\u0398 \u2190 \u0398 + \u03b3 tR t \u2207 log \u03c0 \u0398 (\u1ef9 t |s t ).(3)\nWe use \u03b3 = 1 for simplicity in this work. The definition of reward is crucial for reinforcement learning because it determines the optimization direction. To ensure that the RNES model extracts coherent and informative summaries, the reward includes both coherence score and ROUGE score, which will be introduced later in this paper. Given a sequence of sampled actions\u1ef8 = (\u1ef9 1 , \u00b7 \u00b7 \u00b7 ,\u1ef9 n ), the corresponding coherence scores are exploited as immediate rewardsr t and the ROUGE score as the final rewardr \u22121 . Therefore, our algorithm is indeed maximizing a weighted sum of coherence and ROUGE score:\nv \u03c0 (s 0 ) def = E \u03c0\u0398 [R 0 |s 0 ] = E\u1ef9 t,st\u223c\u03c0\u0398 [r \u22121 + \u03bb n t=1r t |s 0 ] = E \u03c0\u0398 [ROUGE(G) + \u03bbCoherence(G)|X](4)\nwhereG = extract(X,\u1ef8 ) is the sampled extractive summary and \u03bb is the coefficient that balances the two rewards. Coherence(G) is the sum of coherence scores ofG:\nCoherence(G) = (S A ,S B )\u2208adj(G) Coh(S A ,S B ),\nwhere adj(G) is the set of adjacent sentences inG. The function Coh(\u00b7, \u00b7) is defined by the neural coherence model in Equation 5, which will be introduced in the next subsection. Algorithm 1 shows the overall REINFORCE algorithm to train our proposed RNES model. X, H \u2190 sample a document-summary pair from corpus 5:s0 \u2190 X 6:\n\nSample an episodes1,\u1ef91, \u00b7 \u00b7 \u00b7 ,sn,\u1ef9n following \u03c0\u0398 7:\n\nprevious \u2190 \u03c7 (a placeholder for empty start sentence) 8:\n\nfor each step t = 1 . . . n do 9:\n\nif\u1ef9t = 1 then 10:rt \u2190 Coh\u03a8(previous, xt) 11:\n\nprevious \u2190 xt 12:\n\nelse 13:rt \u2190 0 14:G \u2190 extract(X, (\u1ef91, \u00b7 \u00b7 \u00b7 ,\u1ef9n)) 15:r\u22121 = ROUGE(G, H) 16:\n\nfor each step t = 1 . . . n do 17:Rt \u2190 \u03bb n i=tr i +r\u22121 18:\n\n\u0398 \u2190 \u0398 + \u03b1Rt\u2207 log \u03c0\u0398(\u1ef9t|st)\n\n\nNeural Coherence Reward\n\nWe propose a neural coherence model to compute the cross sentence coherence as part of the reward of RNES model. This model is built on the ARC-II proposed by (Hu et al. 2014) for sentence matching. This neural coherence model has some advantages over traditional entity grid models. Our neural coherence model requires no feature engineering and could be trained in an end-to-end fashion. Besides, it uses the distributed text representation which can capture the syntactic and semantic coherence patterns by cross-sentence interaction. The architecture of the neural coherence model is shown in Figure 1. Given two sentences S A and S B , in layer 1, it uses sliding windows on both sentences to model all the possible local coherence transition of the two sentences. For segment i on S A and segment j on S B , the local coherence transition is computed as\nz (1) i,j = ReLU(W (1)\u1e91 (0) i,j + b (1) ),\nwhere W 1 is the weight parameters for first layer and b 1 is the bias. ReLU is the nonlinear function proposed by (Dahl, Sainath, and Hinton 2013).\u1e91\n\ni,j \u2208 R 2k1\u00d7De is obtained by concatenating the embeddings of words in S A and S B sequentially:\nz (0) i,j = [e(a i ); ...; e(a i+k1\u22121 ); e(b j ); ...; e(b j+k1\u22121 )],\nwhere a i is the i-th word of S A , b j is the j-th word of S B and e(\u00b7) is the embedding lookup function which outputs a D e -dimensional word embedding.\n\nLayer 2 takes the output of layer 1 and performs a maxpooling in each dimension on non-overlapping 2 \u00d7 2 windows.\nz (2) i,j = max(z (1) 2i\u22121,2j\u22121 , z (1) 2i\u22121,2j , z(1)\n2i,2j\u22121 , z\n\n2i,2j ). Following layer 2, there are more convolution and maxpooling layers, analogous to that of convolutional architecture for image input (LeCun and Bengio 1995). Finally, we obtain the fixed length vector h and it is fed into a nonlinear transformation with activation function tanh to compute coherence score of the two sentences:\nCoh(S A , S B ) = tanh(W c h + b c ),(5)\nwhere W c is the weight parameters and b c is the bias.\n\nHence, the coherence model will output a coherence score Coh(S A , S B ) \u2208 (\u22121, 1) for any sentence pairs (S A , S B ). From the first layer, the neural coherence model can capture the local coherence of two sentences. And it can also obtain higher level coherence representation of S A and S B with more convolution and max-pooling layers.\n\nFor the training of the neural coherence model, we use a pair-wise training strategy with a large margin objective. Suppose we are given the following triples (S A , S B + , S B \u2212 ), we adopt the ranking-based loss as objective:\nL \u0398 (S A , S B + , S B \u2212 ) = max(0, 1 + Coh(S A , S B \u2212 ) \u2212 Coh(S A , S B + )).\nThe model is trained by minimizing the above objective, to encourage the model to assign higher coherence score to coherent sentence pair (S A , S B + ) than incoherent pair (S A , S B \u2212 ).\n\n\nROUGE Score Reward\n\nROUGE score is used as the final reward to ensure that RNES model extracts reasonably informative sentences. Given a sequence of sampled decisions (\u1ef9 1 , \u00b7 \u00b7 \u00b7 ,\u1ef9 n ), we can get the sequence of extracted sentencesG. Since the dataset comes with news highlights written by human editors, these manual highlights H is treated as the reference summary. Then the ROUGE score between the system summaryG and the reference H could be computed and used as the final reward for the entire sampled decisions:\nr \u22121 (\u1ef9 1 , \u00b7 \u00b7 \u00b7 ,\u1ef9 n ) = ROUGE(G, H).\nMultiple variants of ROUGE score are proposed by (Lin 2004). Among them, ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L (R-L) are the most commonly used ones. ROUGE-n (R-n) recall between an extracted summary and a reference summary can be computed as follows: R-n = s\u2208reference summary gram n \u2208s Count match (gram n ) s\u2208reference summary gram n \u2208s Count(gram n )\n\n, where n stands for the length of n-gram, Count match (gram n ) is the maximum number of n-grams co-occurring in both the extracted summary and the reference. Similarly we could compute the R-n precision and F1. R-1 and R-2 are special cases of R-n in which n = 1 or n = 2. R-L is instead computed based on the length of longest common subsequence between the system summary and the reference. Since using only one variant of ROUGE as reward for training RNES may not increase its performance on other ROUGE variants, we use a combination of ROUGE variants as reward:\nROUGE(G, H) =w 1 R-1(G, H) + w 2 R-2(G, H) + w l R-L(G, H),\nwhere weights w 1 , w 2 and w l are hyperparameters. We use w 1 = 0.4, w 2 = 1.0, w l = 0.5 in our experiments to ensure balanced enhancement. The overall training algorithm is illustrated in the Algorithm 1. Since REINFORCE algorithm converges very slowly, we pretrain the RNES model with supervised learning. The neural coherence model is also trained and then fixed for the coherence scoring. During the REINFORCE training, a sequence of actions and states is sampled according to the policy. Then the coherence model and the ROUGE package are used for computing the rewards. The parameters of RNES model \u0398 is then updated according to Equation 3.\n\n\nExperiments and Results\n\nWe use the CNN/Daily Mail dataset originally introduced by (Hermann et al. 2015) to evaluate our model. This dataset contains news documents and their corresponding highlights crawled from CNN and Daily Mail website, and it is commonly used in extractive summarization (Cheng and Lapata 2016;Nallapati, Zhai, and Zhou 2017) and abstractive summarization (Nallapati et al. 2016;See, Liu, and Manning 2017). We used the scripts provided by (Hermann et al. 2015) to download the dataset. It contains 287,226 documents for training, 13,368 documents for validation and 11,490 documents for test. Since the dataset only contains manual summaries and does not have extractive labels, a greedy algorithm similar to the one presented by (Nallapati, Zhai, and Zhou 2017) is used to generate extraction labels for supervised training.\n\n\nResults of the Neural Coherence Model\n\nThe coherence model needs to be trained before it is used to produce coherence score as the reward in the REINFORCE algorithm. In our experiments, we use 64-dimensional word embeddings which are randomly initialized and finetuned in the process of supervised training. The sizes of all its convolutional kernels are set to 3. The first convolution layer has 128 filters. The second and third convolution layers contain 256 and 512 filters respectively. Each convolution layer is followed by a max-pooling layer performed on the sliding non-overlapping 2 \u00d7 2 windows. The final two fullyconnected layers have 512 and 256 hidden units respectively. The maximum sentence length is 50. Sentences longer than the limit would be truncated, and those that are shorter than this length would be padded with zeros. The coherence model is trained with stochastic gradient descent (SGD) with batch size 64 and learning rate 0.1.\n\nThe training triplets are sampled from the CNN/Daily Mail dataset. The S A and S B + are adjacent sentences sampled from the documents, and S B \u2212 is a sentence randomly sampled such that S B \u2212 = S B + . To make the task more difficult so that the model finds more fine grained coherence patterns, S B \u2212 is sampled from the same document as (S A , S B + ) and it is less than nine sentences away from S B + . The model is tested on around twenty three thousand positive pairs sampled from the test set, each accompanied with one negative sample. If the model gives a higher score to the positive sample than the negative sample, it is considered correct. The accuracy is 71.3%, versus 50% accuracy for random guess, which indicates that the neural coherence model can capture the cross-sentence coherence.\n\nWe also conducted empirical studies on some example outputs of our proposed neural coherence model. Table 1 shows some examples of coherence scoring. The first example shows that the model can exploit co-reference for coherence modeling. The model can also capture the semantic coherence between two sentences, such as semantically related words such as \"photographer\" and \"shoot\" (example 1), \"survey\" and \"answers\" (example 3). Furthermore, the coherence model can discover syntactic patterns such as \"As a result . . . \" and \"According to . . . \", which represents the syntactic coherence across sentences. The third example also shows that there is much noise in our training data. After closer inspection, we found that the S B \u2212 rather than S B + , is the right sentence following S A . S B + in example 3 is an image caption embedded in the article, which is not filtered out during the data preprocessing phase. However, thanks to the training on the large-scale text corpus, the neural coherence model is robust enough to score the right sentence much higher. These examples show that the neural coherence model indeed captures the semantic and syntactic coherence patterns across sentences.\n\n\nResults of the Reinforced Neural Extractive Summarization Model (RNES)\n\nFor the NES/RNES model, we use 128-dimensional word embeddings and the vocabulary size is 150,000. The convolution kernels have size 3, 5, 7 with 128, 256, 256 filters respectively. We set the hidden state size of sentence-level GRU to 256, and the document representation size to 512. The MLP has two layers, with 512 and 256 hidden units respectively. We fix the maximum sentence length to 50 and the maximum number of sentences in a document to 80. Sentences or documents that are longer than the maximum length are truncated to fit the length requirement.\n\nThe model is trained with stochastic gradient descent (SGD) with batch size 64. When doing supervised train- \n\n\n0.9999\n\ning of the NES, ground truth extraction labels are used for computing the cross-entropy loss. The labels are generated from the dataset by greedily selects sentences to maximize its ROUGE similarity compared to manual highlights. During the training of RNES using reinforcement learning, both the neural coherence model and ROUGE scorer are used to compute the reward. As shown in Equation 4, the hyperparameter \u03bb is used to balance between the two objectives. In our experiments, we explored \u03bb = 1.0, 0.1, 0.01, 0.005. It is found that when \u03bb = 1.0 or 0.1, the model favors coherence so much that ROUGE degrades rapidly and the model eventually converges to a policy that selects consecutive sentences that are not informative. However, when \u03bb = 0.005, the ROUGE objective overpowers coherence, and the coherence rewards drop to approximately zero. We found that \u03bb = 0.01 is a good trade-off such that both rewards increase and eventually converge.\n\nAt test time our model produces summaries by beam search with beam size 10. To compare with previous works, we adopt the same evaluation metrics as in (Nallapati, Zhai, and Zhou 2017). We use full-length F1 of ROUGE-1, ROUGE-2, and ROUGE-L to evaluate our model. Table 2 shows the performance comparison between our models and baselines. Our RNES models (with or without coherence as the reward) outperform current state-of-the-art models and NES by a large margin. The result indicates that the sum-maries extracted by RNES are of higher quality than summaries produced by previous works. Though RNES with the coherence reward achieves higher ROUGE scores than baselines, there is a small gap between its score and that of RNES trained without coherence model. This is because that the coherence objective and ROUGE score do not always agree with each other. Since ROUGE is simply computed based on n-grams or longest common subsequence, it is ignorant of the coherence between sentences. Therefore, enhancing coherence may lead to a drop of ROUGE. However, the 95% confidence intervals of the two RNES models overlap heavily, indicating that their difference in ROUGE is insignificant. We also conduct a qualitative evaluation to find out whether the introduction of coherence reward improves the coherence of the output summaries. We randomly sample 50 documents from the test set and ask three volunteers to evaluate the summaries extracted by RNES trained with or without coherence as the reward. They are asked to compare and rank the outputs of two models regarding three aspects: informativeness, coherence and overall quality. The better one will be given rank 1, while the other will be given rank 2 if it is worse. In some cases, if the two outputs are identical or have the same quality, the ranks could be tied, i.e., both of them are given rank 1. Table 3 shows the results of human evaluation. RNES model trained with coherence reward is better than RNES model without coherence reward in all three aspects, especially in the coherence. The result indicates that the introduction of coherence effectively improves the coherence of extracted summaries, as well as the overall quality. It is surprising that summaries produced by RNES with coherence are also more informative than RNES without coherence, indicating that ROUGE might not be the gold standard to evaluate informativeness as well. Table 4 shows a pair of summary produced by RNES with or without coherence. The summary produced by RNES without coherence starts with pronoun 'That' which is referring to a previously mentioned fact, and hence it may lead to confusion. In contrast, the output of RNES trained with coherence reward includes the sentence \"The earthquake disaster . . . \" before referring to this fact in the second sentence, and therefore is more coherent and readable. This is because the coherence model gives a higher score to the second sentence if it can form a coherent sentence pair with the first sentence. In REINFORCE training, if the second sentence receives a high coherence score, the action of extracting the first sentence before the second one will be strengthened. This example shows that coherence model is indeed effective in changing the behavior of RNES towards extracting summaries that are more coherent. Reference: Peter Spinks from the Sydney Morning Herald reported on Amasia. Within 200 million years, he said the new supercontinent will form. One researcher recently travelled to Nepal to gather further information. He spotted that India, Eurasia and other plates are slowly moving together. RNES w/o coherence: That's according to one researcher who travelled to the country to study how the Indian and Eurasian plates are moving together. And using new techniques, researchers can now start examining the changes due to take place over the next tens of millions of years like never before. Earth's continents are slowly moving together, and in 50 to 200 million years they are expected to form a new supercontinent called Amasia. In 2012 a study suggested this may be centered on the North Pole. The idea that Earth is set to form a new supercontinent-dubbed Amasia -is not new. RNES w/ coherence: The earthquake disaster in Nepal has highlighted how Earth's land masses are already in the process of forming a new supercontinent. That's according to one researcher who travelled to the country to study how the Indian and Eurasian plates are moving together. And using new techniques, researchers can now start examining the changes due to take place over the next tens of millions of years like never before. Earth's continents are slowly moving together, and in 50 to 200 million years they are expected to form a new supercontinent called Amasia.\n\n\nConclusion\n\nIn this paper, we proposed a Reinforced Neural Extractive Summarization model to extract a coherent and informative summary from a single document. Empirical results show that the proposed RNES model can balance between the cross-sentence coherence and importance of the sentences effectively, and achieve state-of-the-art performance on the benchmark dataset. For future work, we will focus on improving the performance of our neural coherence model and introducing human knowledge into the RNES.\n\nFigure 1 :\n1Illustration of neural coherence model which is built upon ARC-II proposed by(Hu et al. 2014).\n\nTable 1 :\n1Example outputs of neural coherence model. Sentences Score S A : Terry's career as a photographer came after he failed to make it as a punk rock musician.S B \n+ : He got his first big break in 1994 with a \n\nshoot for Vibe magazine. \n\n0.9885 \n\nS B \n\u2212 : The photographer has also directly music \n\nvideos in his time. \n\n0.5198 \n\nS A : These days we are increasingly using outdoor \n\nspace for the occasional barbecue or to relax in a \nhot tub rather than for tending flowers, according \nto researchers. \n\nS B \n+ : As a result, only a handful of traditional \n\nflowers still grow in English country gardens, with \nthe average one usually containing a mere four \nspecies -daffodils, crocuses, roses and tulips. \n\n0.8934 \n\nS B \n\u2212 : Sir Roy Strong, the landscape designer and \n\nformer director of the Victoria and Albert Museum, \ntold the Sunday Times: 'British people used to take \npride in having neat gardens with lots of flowers.' \n\n-\n0.0067 \n\nS A : The same survey recently showed that univer-\n\nsity pupils in Britain have an average of 8.2 sexual \npartners by the time they reach the middle of their \nhigher education. \n\nS B \n+ : A new survey of university students has re-\n\nvealed that they have had an average of 8.2 sexual \npartners (picture posed by models) \n\n0.0021 \n\nS B \n\u2212 : According to the answers they received, 22 \n\nper cent of students didn't lose their virginity until \nthey were 18 years old, with the second most popu-\nlar age to have sex for the first time being 16. \n\n\n\nTable 2 :\n2Performance comparison on CNN/Daily Mail test set, evaluated with full-length F1 ROUGE scores (%). All scores of RNES are statistically significant using 95% confidence interval with respect to previous best models.Model \nR-1 \nR-2 \nR-L \nLead-3 \n39.2 \n15.7 \n35.5 \n(Nallapati et al. 2016) \n35.4 \n13.3 \n32.6 \n(Nallapati et al. 2017) \n39.6 \n16.2 \n35.3 \n(See et al. 2017) \n39.53 \n17.28 \n35.38 \nNES \n37.75 \n17.04 \n33.92 \nRNES w/o coherence \n41.25 \n18.87 \n37.75 \nRNES w/ coherence \n40.95 \n18.63 \n37.41 \n\n\n\nTable 3 :\n3Comparison of human evaluation in terms of informativeness(Inf), coherence(Coh) and overall ranking. Lower is better.Model \nInf \nCoh \nOverall \nRNES w/o coherence 1.183 \n1.325 \n1.492 \nRNES w/ coherence \n1.125 \n1.092 \n1.209 \n\n\n\nTable 4 :\n4Examples of extracted summary.\nAcknowledgmentsThis work is supported by grants from WeChat-HKUST Joint Lab on Artificial Intelligence Technology (WHAT Lab). Baotian Hu acknowledges partial support from the University of Massachusetts Medical School.\nNeural headline generation with minimum risk training. S Shen, Z Liu, M Sun, CoRR abs/1604.01904Shen, S.; Liu, Z.; and Sun, M. 2016. Neural headline generation with minimum risk training. CoRR abs/1604.01904.\n\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, CoRR abs/1409.0473Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural ma- chine translation by jointly learning to align and translate. CoRR abs/1409.0473.\n\nModeling local coherence: An entity-based approach. R Barzilay, M Lapata, Comput. Linguist. 341Barzilay, R., and Lapata, M. 2008. Modeling local coher- ence: An entity-based approach. Comput. Linguist. 34(1):1- 34.\n\nJointly learning to extract and compress. T Berg-Kirkpatrick, D Gillick, D Klein, Proceedings of the ACL-HLT. the ACL-HLTBerg-Kirkpatrick, T.; Gillick, D.; and Klein, D. 2011. Jointly learning to extract and compress. In Proceedings of the ACL- HLT, 481-490.\n\nNeural summarization by extracting sentences and words. J Cheng, M Lapata, Proceedings of the ACL. the ACLCheng, J., and Lapata, M. 2016. Neural summarization by extracting sentences and words. In Proceedings of the ACL.\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, C Gulcehre, K Cho, Y Bengio, arXiv:1412.3555arXiv preprintChung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y. 2014. Em- pirical evaluation of gated recurrent neural networks on se- quence modeling. arXiv preprint arXiv:1412.3555.\n\nImproving deep neural networks for lvcsr using rectified linear units and dropout. G E Dahl, T N Sainath, G E Hinton, Proceedings of the ICASSP. the ICASSPDahl, G. E.; Sainath, T. N.; and Hinton, G. E. 2013. Im- proving deep neural networks for lvcsr using rectified linear units and dropout. In Proceedings of the ICASSP.\n\nLexrank: Graph-based lexical centrality as salience in text summarization. G Erkan, D R Radev, J. Artif. Int. Res. 221Erkan, G., and Radev, D. R. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res. 22(1):457-479.\n\nSentence Compression by Deletion with LSTMs. K Filippova, E Alfonseca, C A Colmenares, L Kaiser, O Vinyals, Proceedings of the EMNLP. the EMNLPFilippova, K.; Alfonseca, E.; Colmenares, C. A.; Kaiser, L.; and Vinyals, O. 2015. Sentence Compression by Deletion with LSTMs. In Proceedings of the EMNLP, 360-368.\n\nA Reinforcement Learning Approach for Adaptive Single-and Multi-Document Summarization. S Henb, M Mieskes, I Gurevych, GSCL. Henb, S.; Mieskes, M.; and Gurevych, I. 2015. A Reinforce- ment Learning Approach for Adaptive Single-and Multi- Document Summarization. In GSCL, 3-12.\n\nTeaching machines to read and comprehend. K M Hermann, T Kocisky, E Grefenstette, L Espeholt, W Kay, M Suleyman, P Blunsom, Advances in Neural Information Processing Systems. Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay, W.; Suleyman, M.; and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, 1693-1701.\n\nConvolutional neural network architectures for matching natural language sentences. B Hu, Z Lu, H Li, Q Chen, Advances in Neural Information Processing Systems. 27Hu, B.; Lu, Z.; Li, H.; and Chen, Q. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems 27, 2042-2050.\n\nA novel word embedding learning model using the dissociation between nouns and verbs. B Hu, B Tang, Q Chen, L Kang, Neurocomput. 171Hu, B.; Tang, B.; Chen, Q.; and Kang, L. 2016. A novel word embedding learning model using the dissociation be- tween nouns and verbs. Neurocomput. 171(C):1108-1117.\n\nLCSTS: A large scale chinese short text summarization dataset. B Hu, Q Chen, F Zhu, CoRR abs/1506.05865Hu, B.; Chen, Q.; and Zhu, F. 2015. LCSTS: A large scale chinese short text summarization dataset. CoRR abs/1506.05865.\n\nConvolutional networks for images, speech and time series. The Handbook of Brain Theory and Neural Networks 3361. Y Lecun, Y Bengio, LeCun, Y., and Bengio, Y. 1995. Convolutional networks for images, speech and time series. The Handbook of Brain Theory and Neural Networks 3361.\n\nA model of coherence based on distributed sentence representation. J Li, E H Hovy, Proceedings of the EMNLP. the EMNLPLi, J., and Hovy, E. H. 2014. A model of coherence based on distributed sentence representation. In Proceedings of the EMNLP, 2039-2048.\n\nA class of submodular functions for document summarization. H Lin, J Bilmes, Proceedings of the ACL-HLT. the ACL-HLTLin, H., and Bilmes, J. 2011. A class of submodular func- tions for document summarization. In Proceedings of the ACL-HLT, 510-520.\n\nRouge: A package for automatic evaluation of summaries. C.-Y Lin, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. Marie-Francine Moens, S. S.Lin, C.-Y. 2004. Rouge: A package for automatic evaluation of summaries. In Marie-Francine Moens, S. S., ed., Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, 74-81.\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, CoRR abs/1301.3781Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef- ficient estimation of word representations in vector space. CoRR abs/1301.3781.\n\nAbstractive Text Summarization using Sequence-tosequence RNNs and Beyond. R Nallapati, B Zhou, C Gulcehre, B Xiang, Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. the 20th SIGNLL Conference on Computational Natural Language LearningNallapati, R.; Zhou, B.; Gulcehre, C.; and Xiang, B. 2016. Abstractive Text Summarization using Sequence-to- sequence RNNs and Beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning.\n\nSummarunner: A recurrent neural network based sequence model for extractive summarization of documents. R Nallapati, F Zhai, B Zhou, Proceedings of the AAAI. the AAAINallapati, R.; Zhai, F.; and Zhou, B. 2017. Summarunner: A recurrent neural network based sequence model for ex- tractive summarization of documents. In Proceedings of the AAAI, 3075-3081.\n\nA neural local coherence model. D T Nguyen, S R Joty, Proceedings of the ACL. the ACLNguyen, D. T., and Joty, S. R. 2017. A neural local coher- ence model. In Proceedings of the ACL, 1320-1330.\n\nReinforcement learning for bandit neural machine translation with simulated human feedback. K Nguyen, J Boyd-Graber, H I Daume, Proceedings of the EMNLP. the EMNLPNguyen, K.; Boyd-Graber, J.; and Daume, H. I. 2017. Re- inforcement learning for bandit neural machine translation with simulated human feedback. In Proceedings of the EMNLP.\n\nA deep reinforced model for abstractive summarization. R Paulus, C Xiong, R Socher, CoRR abs/1705.04304Paulus, R.; Xiong, C.; and Socher, R. 2017. A deep reinforced model for abstractive summarization. CoRR abs/1705.04304.\n\nSequence level training with recurrent neural networks. M Ranzato, S Chopra, M Auli, W Zaremba, CoRR abs/1511.06732Ranzato, M.; Chopra, S.; Auli, M.; and Zaremba, W. 2015. Sequence level training with recurrent neural networks. CoRR abs/1511.06732.\n\nFear the REAPER: A System for Automatic Multi-Document Summarization with Reinforcement Learning. C Rioux, S A Hasan, Y Chali, Proceedings of the EMNLP. the EMNLPRioux, C.; Hasan, S. A.; and Chali, Y. 2014. Fear the REAPER: A System for Automatic Multi-Document Sum- marization with Reinforcement Learning. In Proceedings of the EMNLP, 681-690.\n\nA neural attention model for abstractive sentence summarization. A M Rush, S Chopra, J Weston, Proceedings of the EMNLP. the EMNLPRush, A. M.; Chopra, S.; and Weston, J. 2015. A neural attention model for abstractive sentence summarization. In Proceedings of the EMNLP, 379-389.\n\nFramework of automatic text summarization using reinforcement learning. S Ryang, Abekawa , T , Proceedings of the EMNLP. the EMNLPRyang, S., and Abekawa, T. 2012. Framework of automatic text summarization using reinforcement learning. In Pro- ceedings of the EMNLP, 256-265.\n\nGet To The Point: Summarization with Pointer-Generator Networks. A See, P J Liu, C D Manning, arXiv:1704.04368[cs].arXiv:1704.04368See, A.; Liu, P. J.; and Manning, C. D. 2017. Get To The Point: Summarization with Pointer-Generator Net- works. arXiv:1704.04368 [cs]. arXiv: 1704.04368.\n\nIntroduction to Reinforcement Learning. R S Sutton, A G Barto, MIT PressCambridge, MA, USA1st editionSutton, R. S., and Barto, A. G. 1998. Introduction to Rein- forcement Learning. Cambridge, MA, USA: MIT Press, 1st edition.\n\nDocument modeling with gated recurrent neural network for sentiment classification. D Tang, B Qin, T Liu, Proceedings of the EMNLP. the EMNLPTang, D.; Qin, B.; and Liu, T. 2015. Document modeling with gated recurrent neural network for sentiment classifica- tion. In Proceedings of the EMNLP, 1422-1432.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83-4Williams, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Ma- chine learning 8(3-4):229-256.\n\nRecent advances in document summarization. J Yao, X Wan, J Xiao, Knowledge and Information Systems. Yao, J.; Wan, X.; and Xiao, J. 2017. Recent advances in doc- ument summarization. Knowledge and Information Systems 1-40.\n\nIcrc-hit: A deep learning based comment sequence labeling system for answer selection challenge. X Zhou, B Hu, J Lin, Y Xiang, X Wang, Proceedings of the SemEval. the SemEvalZhou, X.; Hu, B.; Lin, J.; xiang, Y.; and Wang, X. 2015. Icrc-hit: A deep learning based comment sequence labeling system for answer selection challenge. In Proceedings of the SemEval 2015, 210-214.\n", "annotations": {"author": "[{\"end\":191,\"start\":72},{\"end\":335,\"start\":192}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":80},{\"end\":202,\"start\":200}]", "author_first_name": "[{\"end\":79,\"start\":72},{\"end\":199,\"start\":192}]", "author_affiliation": "[{\"end\":190,\"start\":84},{\"end\":334,\"start\":228}]", "title": "[{\"end\":69,\"start\":1},{\"end\":404,\"start\":336}]", "venue": null, "abstract": "[{\"end\":1712,\"start\":406}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1888,\"start\":1863},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1942,\"start\":1910},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1984,\"start\":1966},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2207,\"start\":2176},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2253,\"start\":2229},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3130,\"start\":3108},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3174,\"start\":3153},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3248,\"start\":3205},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3439,\"start\":3418},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3454,\"start\":3439},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3573,\"start\":3550},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3604,\"start\":3573},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4702,\"start\":4670},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4738,\"start\":4702},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5081,\"start\":5055},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7400,\"start\":7378},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7849,\"start\":7826},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7964,\"start\":7933},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8560,\"start\":8536},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8589,\"start\":8560},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8622,\"start\":8589},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8935,\"start\":8903},{\"end\":9103,\"start\":9084},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9123,\"start\":9103},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9751,\"start\":9726},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9809,\"start\":9783},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10026,\"start\":10004},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10195,\"start\":10173},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10369,\"start\":10351},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12801,\"start\":12783},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16033,\"start\":16018},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17651,\"start\":17628},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19597,\"start\":19582},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20473,\"start\":20441},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22880,\"start\":22870},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24566,\"start\":24546},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24779,\"start\":24756},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24810,\"start\":24779},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24864,\"start\":24841},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24891,\"start\":24864},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24946,\"start\":24925},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25247,\"start\":25216},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30168,\"start\":30136},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35376,\"start\":35360}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35377,\"start\":35270},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36872,\"start\":35378},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37382,\"start\":36873},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37619,\"start\":37383},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37662,\"start\":37620}]", "paragraph": "[{\"end\":2692,\"start\":1728},{\"end\":3921,\"start\":2694},{\"end\":4739,\"start\":3923},{\"end\":5647,\"start\":4741},{\"end\":6320,\"start\":5649},{\"end\":7058,\"start\":6322},{\"end\":7209,\"start\":7075},{\"end\":8125,\"start\":7211},{\"end\":9547,\"start\":8127},{\"end\":10948,\"start\":9549},{\"end\":11193,\"start\":10990},{\"end\":11661,\"start\":11195},{\"end\":12251,\"start\":11708},{\"end\":12466,\"start\":12302},{\"end\":12573,\"start\":12505},{\"end\":12943,\"start\":12597},{\"end\":13187,\"start\":13100},{\"end\":13421,\"start\":13189},{\"end\":13571,\"start\":13455},{\"end\":13863,\"start\":13616},{\"end\":14045,\"start\":13920},{\"end\":14218,\"start\":14129},{\"end\":14417,\"start\":14220},{\"end\":14562,\"start\":14455},{\"end\":15134,\"start\":14735},{\"end\":15909,\"start\":15161},{\"end\":16230,\"start\":15911},{\"end\":16465,\"start\":16304},{\"end\":17041,\"start\":16515},{\"end\":17266,\"start\":17133},{\"end\":17421,\"start\":17268},{\"end\":17725,\"start\":17480},{\"end\":18370,\"start\":17767},{\"end\":18644,\"start\":18483},{\"end\":19019,\"start\":18695},{\"end\":19073,\"start\":19021},{\"end\":19131,\"start\":19075},{\"end\":19166,\"start\":19133},{\"end\":19212,\"start\":19168},{\"end\":19231,\"start\":19214},{\"end\":19307,\"start\":19233},{\"end\":19367,\"start\":19309},{\"end\":19395,\"start\":19369},{\"end\":20282,\"start\":19423},{\"end\":20475,\"start\":20326},{\"end\":20573,\"start\":20477},{\"end\":20798,\"start\":20644},{\"end\":20913,\"start\":20800},{\"end\":20980,\"start\":20969},{\"end\":21318,\"start\":20982},{\"end\":21415,\"start\":21360},{\"end\":21757,\"start\":21417},{\"end\":21987,\"start\":21759},{\"end\":22257,\"start\":22068},{\"end\":22780,\"start\":22280},{\"end\":23178,\"start\":22821},{\"end\":23748,\"start\":23180},{\"end\":24459,\"start\":23809},{\"end\":25311,\"start\":24487},{\"end\":26270,\"start\":25353},{\"end\":27076,\"start\":26272},{\"end\":28278,\"start\":27078},{\"end\":28912,\"start\":28353},{\"end\":29023,\"start\":28914},{\"end\":29983,\"start\":29034},{\"end\":34757,\"start\":29985},{\"end\":35269,\"start\":34772}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11707,\"start\":11662},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12301,\"start\":12252},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12504,\"start\":12467},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12596,\"start\":12574},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13099,\"start\":12944},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13454,\"start\":13422},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13615,\"start\":13572},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13919,\"start\":13864},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14128,\"start\":14046},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14454,\"start\":14418},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14683,\"start\":14563},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16303,\"start\":16231},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16514,\"start\":16466},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17132,\"start\":17042},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17479,\"start\":17422},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17766,\"start\":17726},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18482,\"start\":18371},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18694,\"start\":18645},{\"attributes\":{\"id\":\"formula_18\"},\"end\":20325,\"start\":20283},{\"attributes\":{\"id\":\"formula_20\"},\"end\":20643,\"start\":20574},{\"attributes\":{\"id\":\"formula_21\"},\"end\":20968,\"start\":20914},{\"attributes\":{\"id\":\"formula_23\"},\"end\":21359,\"start\":21319},{\"attributes\":{\"id\":\"formula_24\"},\"end\":22067,\"start\":21988},{\"attributes\":{\"id\":\"formula_25\"},\"end\":22820,\"start\":22781},{\"attributes\":{\"id\":\"formula_26\"},\"end\":23808,\"start\":23749}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27185,\"start\":27178},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30255,\"start\":30248},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31854,\"start\":31847},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32400,\"start\":32393}]", "section_header": "[{\"end\":1726,\"start\":1714},{\"end\":7073,\"start\":7061},{\"end\":10988,\"start\":10951},{\"end\":14733,\"start\":14685},{\"end\":15159,\"start\":15137},{\"end\":19421,\"start\":19398},{\"end\":22278,\"start\":22260},{\"end\":24485,\"start\":24462},{\"end\":25351,\"start\":25314},{\"end\":28351,\"start\":28281},{\"end\":29032,\"start\":29026},{\"end\":34770,\"start\":34760},{\"end\":35281,\"start\":35271},{\"end\":35388,\"start\":35379},{\"end\":36883,\"start\":36874},{\"end\":37393,\"start\":37384},{\"end\":37630,\"start\":37621}]", "table": "[{\"end\":36872,\"start\":35544},{\"end\":37382,\"start\":37100},{\"end\":37619,\"start\":37512}]", "figure_caption": "[{\"end\":35377,\"start\":35283},{\"end\":35544,\"start\":35390},{\"end\":37100,\"start\":36885},{\"end\":37512,\"start\":37395},{\"end\":37662,\"start\":37632}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20028,\"start\":20020}]", "bib_author_first_name": "[{\"end\":37938,\"start\":37937},{\"end\":37946,\"start\":37945},{\"end\":37953,\"start\":37952},{\"end\":38164,\"start\":38163},{\"end\":38176,\"start\":38175},{\"end\":38183,\"start\":38182},{\"end\":38401,\"start\":38400},{\"end\":38413,\"start\":38412},{\"end\":38607,\"start\":38606},{\"end\":38627,\"start\":38626},{\"end\":38638,\"start\":38637},{\"end\":38881,\"start\":38880},{\"end\":38890,\"start\":38889},{\"end\":39125,\"start\":39124},{\"end\":39134,\"start\":39133},{\"end\":39146,\"start\":39145},{\"end\":39153,\"start\":39152},{\"end\":39445,\"start\":39444},{\"end\":39447,\"start\":39446},{\"end\":39455,\"start\":39454},{\"end\":39457,\"start\":39456},{\"end\":39468,\"start\":39467},{\"end\":39470,\"start\":39469},{\"end\":39761,\"start\":39760},{\"end\":39770,\"start\":39769},{\"end\":39772,\"start\":39771},{\"end\":39994,\"start\":39993},{\"end\":40007,\"start\":40006},{\"end\":40020,\"start\":40019},{\"end\":40022,\"start\":40021},{\"end\":40036,\"start\":40035},{\"end\":40046,\"start\":40045},{\"end\":40347,\"start\":40346},{\"end\":40355,\"start\":40354},{\"end\":40366,\"start\":40365},{\"end\":40579,\"start\":40578},{\"end\":40581,\"start\":40580},{\"end\":40592,\"start\":40591},{\"end\":40603,\"start\":40602},{\"end\":40619,\"start\":40618},{\"end\":40631,\"start\":40630},{\"end\":40638,\"start\":40637},{\"end\":40650,\"start\":40649},{\"end\":41010,\"start\":41009},{\"end\":41016,\"start\":41015},{\"end\":41022,\"start\":41021},{\"end\":41028,\"start\":41027},{\"end\":41371,\"start\":41370},{\"end\":41377,\"start\":41376},{\"end\":41385,\"start\":41384},{\"end\":41393,\"start\":41392},{\"end\":41647,\"start\":41646},{\"end\":41653,\"start\":41652},{\"end\":41661,\"start\":41660},{\"end\":41922,\"start\":41921},{\"end\":41931,\"start\":41930},{\"end\":42155,\"start\":42154},{\"end\":42161,\"start\":42160},{\"end\":42163,\"start\":42162},{\"end\":42404,\"start\":42403},{\"end\":42411,\"start\":42410},{\"end\":42652,\"start\":42648},{\"end\":43004,\"start\":43003},{\"end\":43015,\"start\":43014},{\"end\":43023,\"start\":43022},{\"end\":43034,\"start\":43033},{\"end\":43274,\"start\":43273},{\"end\":43287,\"start\":43286},{\"end\":43295,\"start\":43294},{\"end\":43307,\"start\":43306},{\"end\":43800,\"start\":43799},{\"end\":43813,\"start\":43812},{\"end\":43821,\"start\":43820},{\"end\":44084,\"start\":44083},{\"end\":44086,\"start\":44085},{\"end\":44096,\"start\":44095},{\"end\":44098,\"start\":44097},{\"end\":44339,\"start\":44338},{\"end\":44349,\"start\":44348},{\"end\":44364,\"start\":44363},{\"end\":44366,\"start\":44365},{\"end\":44641,\"start\":44640},{\"end\":44651,\"start\":44650},{\"end\":44660,\"start\":44659},{\"end\":44866,\"start\":44865},{\"end\":44877,\"start\":44876},{\"end\":44887,\"start\":44886},{\"end\":44895,\"start\":44894},{\"end\":45158,\"start\":45157},{\"end\":45167,\"start\":45166},{\"end\":45169,\"start\":45168},{\"end\":45178,\"start\":45177},{\"end\":45471,\"start\":45470},{\"end\":45473,\"start\":45472},{\"end\":45481,\"start\":45480},{\"end\":45491,\"start\":45490},{\"end\":45758,\"start\":45757},{\"end\":45773,\"start\":45766},{\"end\":45777,\"start\":45776},{\"end\":46027,\"start\":46026},{\"end\":46034,\"start\":46033},{\"end\":46036,\"start\":46035},{\"end\":46043,\"start\":46042},{\"end\":46045,\"start\":46044},{\"end\":46289,\"start\":46288},{\"end\":46291,\"start\":46290},{\"end\":46301,\"start\":46300},{\"end\":46303,\"start\":46302},{\"end\":46559,\"start\":46558},{\"end\":46567,\"start\":46566},{\"end\":46574,\"start\":46573},{\"end\":46871,\"start\":46870},{\"end\":46873,\"start\":46872},{\"end\":47099,\"start\":47098},{\"end\":47106,\"start\":47105},{\"end\":47113,\"start\":47112},{\"end\":47376,\"start\":47375},{\"end\":47384,\"start\":47383},{\"end\":47390,\"start\":47389},{\"end\":47397,\"start\":47396},{\"end\":47406,\"start\":47405}]", "bib_author_last_name": "[{\"end\":37943,\"start\":37939},{\"end\":37950,\"start\":37947},{\"end\":37957,\"start\":37954},{\"end\":38173,\"start\":38165},{\"end\":38180,\"start\":38177},{\"end\":38190,\"start\":38184},{\"end\":38410,\"start\":38402},{\"end\":38420,\"start\":38414},{\"end\":38624,\"start\":38608},{\"end\":38635,\"start\":38628},{\"end\":38644,\"start\":38639},{\"end\":38887,\"start\":38882},{\"end\":38897,\"start\":38891},{\"end\":39131,\"start\":39126},{\"end\":39143,\"start\":39135},{\"end\":39150,\"start\":39147},{\"end\":39160,\"start\":39154},{\"end\":39452,\"start\":39448},{\"end\":39465,\"start\":39458},{\"end\":39477,\"start\":39471},{\"end\":39767,\"start\":39762},{\"end\":39778,\"start\":39773},{\"end\":40004,\"start\":39995},{\"end\":40017,\"start\":40008},{\"end\":40033,\"start\":40023},{\"end\":40043,\"start\":40037},{\"end\":40054,\"start\":40047},{\"end\":40352,\"start\":40348},{\"end\":40363,\"start\":40356},{\"end\":40375,\"start\":40367},{\"end\":40589,\"start\":40582},{\"end\":40600,\"start\":40593},{\"end\":40616,\"start\":40604},{\"end\":40628,\"start\":40620},{\"end\":40635,\"start\":40632},{\"end\":40647,\"start\":40639},{\"end\":40658,\"start\":40651},{\"end\":41013,\"start\":41011},{\"end\":41019,\"start\":41017},{\"end\":41025,\"start\":41023},{\"end\":41033,\"start\":41029},{\"end\":41374,\"start\":41372},{\"end\":41382,\"start\":41378},{\"end\":41390,\"start\":41386},{\"end\":41398,\"start\":41394},{\"end\":41650,\"start\":41648},{\"end\":41658,\"start\":41654},{\"end\":41665,\"start\":41662},{\"end\":41928,\"start\":41923},{\"end\":41938,\"start\":41932},{\"end\":42158,\"start\":42156},{\"end\":42168,\"start\":42164},{\"end\":42408,\"start\":42405},{\"end\":42418,\"start\":42412},{\"end\":42656,\"start\":42653},{\"end\":43012,\"start\":43005},{\"end\":43020,\"start\":43016},{\"end\":43031,\"start\":43024},{\"end\":43039,\"start\":43035},{\"end\":43284,\"start\":43275},{\"end\":43292,\"start\":43288},{\"end\":43304,\"start\":43296},{\"end\":43313,\"start\":43308},{\"end\":43810,\"start\":43801},{\"end\":43818,\"start\":43814},{\"end\":43826,\"start\":43822},{\"end\":44093,\"start\":44087},{\"end\":44103,\"start\":44099},{\"end\":44346,\"start\":44340},{\"end\":44361,\"start\":44350},{\"end\":44372,\"start\":44367},{\"end\":44648,\"start\":44642},{\"end\":44657,\"start\":44652},{\"end\":44667,\"start\":44661},{\"end\":44874,\"start\":44867},{\"end\":44884,\"start\":44878},{\"end\":44892,\"start\":44888},{\"end\":44903,\"start\":44896},{\"end\":45164,\"start\":45159},{\"end\":45175,\"start\":45170},{\"end\":45184,\"start\":45179},{\"end\":45478,\"start\":45474},{\"end\":45488,\"start\":45482},{\"end\":45498,\"start\":45492},{\"end\":45764,\"start\":45759},{\"end\":46031,\"start\":46028},{\"end\":46040,\"start\":46037},{\"end\":46053,\"start\":46046},{\"end\":46298,\"start\":46292},{\"end\":46309,\"start\":46304},{\"end\":46564,\"start\":46560},{\"end\":46571,\"start\":46568},{\"end\":46578,\"start\":46575},{\"end\":46882,\"start\":46874},{\"end\":47103,\"start\":47100},{\"end\":47110,\"start\":47107},{\"end\":47118,\"start\":47114},{\"end\":47381,\"start\":47377},{\"end\":47387,\"start\":47385},{\"end\":47394,\"start\":47391},{\"end\":47403,\"start\":47398},{\"end\":47411,\"start\":47407}]", "bib_entry": "[{\"attributes\":{\"doi\":\"CoRR abs/1604.01904\",\"id\":\"b0\"},\"end\":38090,\"start\":37882},{\"attributes\":{\"doi\":\"CoRR abs/1409.0473\",\"id\":\"b1\"},\"end\":38346,\"start\":38092},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9482302},\"end\":38562,\"start\":38348},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15467396},\"end\":38822,\"start\":38564},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1499080},\"end\":39044,\"start\":38824},{\"attributes\":{\"doi\":\"arXiv:1412.3555\",\"id\":\"b5\"},\"end\":39359,\"start\":39046},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6299466},\"end\":39683,\"start\":39361},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":506350},\"end\":39946,\"start\":39685},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1992250},\"end\":40256,\"start\":39948},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12341014},\"end\":40534,\"start\":40258},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6203757},\"end\":40923,\"start\":40536},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4497054},\"end\":41282,\"start\":40925},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":46895088},\"end\":41581,\"start\":41284},{\"attributes\":{\"doi\":\"CoRR abs/1506.05865\",\"id\":\"b13\"},\"end\":41805,\"start\":41583},{\"attributes\":{\"id\":\"b14\"},\"end\":42085,\"start\":41807},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9615470},\"end\":42341,\"start\":42087},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":320371},\"end\":42590,\"start\":42343},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":964287},\"end\":42939,\"start\":42592},{\"attributes\":{\"doi\":\"CoRR abs/1301.3781\",\"id\":\"b18\"},\"end\":43197,\"start\":42941},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8928715},\"end\":43693,\"start\":43199},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6405271},\"end\":44049,\"start\":43695},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":7589418},\"end\":44244,\"start\":44051},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":215824512},\"end\":44583,\"start\":44246},{\"attributes\":{\"doi\":\"CoRR abs/1705.04304\",\"id\":\"b23\"},\"end\":44807,\"start\":44585},{\"attributes\":{\"doi\":\"CoRR abs/1511.06732\",\"id\":\"b24\"},\"end\":45057,\"start\":44809},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10221032},\"end\":45403,\"start\":45059},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1918428},\"end\":45683,\"start\":45405},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14666654},\"end\":45959,\"start\":45685},{\"attributes\":{\"doi\":\"arXiv:1704.04368[cs].arXiv:1704.04368\",\"id\":\"b28\"},\"end\":46246,\"start\":45961},{\"attributes\":{\"id\":\"b29\"},\"end\":46472,\"start\":46248},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":784094},\"end\":46777,\"start\":46474},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2332513},\"end\":47053,\"start\":46779},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":16506604},\"end\":47276,\"start\":47055},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3781940},\"end\":47650,\"start\":47278}]", "bib_title": "[{\"end\":38398,\"start\":38348},{\"end\":38604,\"start\":38564},{\"end\":38878,\"start\":38824},{\"end\":39442,\"start\":39361},{\"end\":39758,\"start\":39685},{\"end\":39991,\"start\":39948},{\"end\":40344,\"start\":40258},{\"end\":40576,\"start\":40536},{\"end\":41007,\"start\":40925},{\"end\":41368,\"start\":41284},{\"end\":42152,\"start\":42087},{\"end\":42401,\"start\":42343},{\"end\":42646,\"start\":42592},{\"end\":43271,\"start\":43199},{\"end\":43797,\"start\":43695},{\"end\":44081,\"start\":44051},{\"end\":44336,\"start\":44246},{\"end\":45155,\"start\":45059},{\"end\":45468,\"start\":45405},{\"end\":45755,\"start\":45685},{\"end\":46556,\"start\":46474},{\"end\":46868,\"start\":46779},{\"end\":47096,\"start\":47055},{\"end\":47373,\"start\":47278}]", "bib_author": "[{\"end\":37945,\"start\":37937},{\"end\":37952,\"start\":37945},{\"end\":37959,\"start\":37952},{\"end\":38175,\"start\":38163},{\"end\":38182,\"start\":38175},{\"end\":38192,\"start\":38182},{\"end\":38412,\"start\":38400},{\"end\":38422,\"start\":38412},{\"end\":38626,\"start\":38606},{\"end\":38637,\"start\":38626},{\"end\":38646,\"start\":38637},{\"end\":38889,\"start\":38880},{\"end\":38899,\"start\":38889},{\"end\":39133,\"start\":39124},{\"end\":39145,\"start\":39133},{\"end\":39152,\"start\":39145},{\"end\":39162,\"start\":39152},{\"end\":39454,\"start\":39444},{\"end\":39467,\"start\":39454},{\"end\":39479,\"start\":39467},{\"end\":39769,\"start\":39760},{\"end\":39780,\"start\":39769},{\"end\":40006,\"start\":39993},{\"end\":40019,\"start\":40006},{\"end\":40035,\"start\":40019},{\"end\":40045,\"start\":40035},{\"end\":40056,\"start\":40045},{\"end\":40354,\"start\":40346},{\"end\":40365,\"start\":40354},{\"end\":40377,\"start\":40365},{\"end\":40591,\"start\":40578},{\"end\":40602,\"start\":40591},{\"end\":40618,\"start\":40602},{\"end\":40630,\"start\":40618},{\"end\":40637,\"start\":40630},{\"end\":40649,\"start\":40637},{\"end\":40660,\"start\":40649},{\"end\":41015,\"start\":41009},{\"end\":41021,\"start\":41015},{\"end\":41027,\"start\":41021},{\"end\":41035,\"start\":41027},{\"end\":41376,\"start\":41370},{\"end\":41384,\"start\":41376},{\"end\":41392,\"start\":41384},{\"end\":41400,\"start\":41392},{\"end\":41652,\"start\":41646},{\"end\":41660,\"start\":41652},{\"end\":41667,\"start\":41660},{\"end\":41930,\"start\":41921},{\"end\":41940,\"start\":41930},{\"end\":42160,\"start\":42154},{\"end\":42170,\"start\":42160},{\"end\":42410,\"start\":42403},{\"end\":42420,\"start\":42410},{\"end\":42658,\"start\":42648},{\"end\":43014,\"start\":43003},{\"end\":43022,\"start\":43014},{\"end\":43033,\"start\":43022},{\"end\":43041,\"start\":43033},{\"end\":43286,\"start\":43273},{\"end\":43294,\"start\":43286},{\"end\":43306,\"start\":43294},{\"end\":43315,\"start\":43306},{\"end\":43812,\"start\":43799},{\"end\":43820,\"start\":43812},{\"end\":43828,\"start\":43820},{\"end\":44095,\"start\":44083},{\"end\":44105,\"start\":44095},{\"end\":44348,\"start\":44338},{\"end\":44363,\"start\":44348},{\"end\":44374,\"start\":44363},{\"end\":44650,\"start\":44640},{\"end\":44659,\"start\":44650},{\"end\":44669,\"start\":44659},{\"end\":44876,\"start\":44865},{\"end\":44886,\"start\":44876},{\"end\":44894,\"start\":44886},{\"end\":44905,\"start\":44894},{\"end\":45166,\"start\":45157},{\"end\":45177,\"start\":45166},{\"end\":45186,\"start\":45177},{\"end\":45480,\"start\":45470},{\"end\":45490,\"start\":45480},{\"end\":45500,\"start\":45490},{\"end\":45766,\"start\":45757},{\"end\":45776,\"start\":45766},{\"end\":45780,\"start\":45776},{\"end\":46033,\"start\":46026},{\"end\":46042,\"start\":46033},{\"end\":46055,\"start\":46042},{\"end\":46300,\"start\":46288},{\"end\":46311,\"start\":46300},{\"end\":46566,\"start\":46558},{\"end\":46573,\"start\":46566},{\"end\":46580,\"start\":46573},{\"end\":46884,\"start\":46870},{\"end\":47105,\"start\":47098},{\"end\":47112,\"start\":47105},{\"end\":47120,\"start\":47112},{\"end\":47383,\"start\":47375},{\"end\":47389,\"start\":47383},{\"end\":47396,\"start\":47389},{\"end\":47405,\"start\":47396},{\"end\":47413,\"start\":47405}]", "bib_venue": "[{\"end\":37935,\"start\":37882},{\"end\":38161,\"start\":38092},{\"end\":38438,\"start\":38422},{\"end\":38672,\"start\":38646},{\"end\":38921,\"start\":38899},{\"end\":39122,\"start\":39046},{\"end\":39504,\"start\":39479},{\"end\":39798,\"start\":39780},{\"end\":40080,\"start\":40056},{\"end\":40381,\"start\":40377},{\"end\":40709,\"start\":40660},{\"end\":41084,\"start\":41035},{\"end\":41411,\"start\":41400},{\"end\":41644,\"start\":41583},{\"end\":41919,\"start\":41807},{\"end\":42194,\"start\":42170},{\"end\":42446,\"start\":42420},{\"end\":42725,\"start\":42658},{\"end\":43001,\"start\":42941},{\"end\":43399,\"start\":43315},{\"end\":43851,\"start\":43828},{\"end\":44127,\"start\":44105},{\"end\":44398,\"start\":44374},{\"end\":44638,\"start\":44585},{\"end\":44863,\"start\":44809},{\"end\":45210,\"start\":45186},{\"end\":45524,\"start\":45500},{\"end\":45804,\"start\":45780},{\"end\":46024,\"start\":45961},{\"end\":46286,\"start\":46248},{\"end\":46604,\"start\":46580},{\"end\":46900,\"start\":46884},{\"end\":47153,\"start\":47120},{\"end\":47439,\"start\":47413},{\"end\":38685,\"start\":38674},{\"end\":38930,\"start\":38923},{\"end\":39516,\"start\":39506},{\"end\":40091,\"start\":40082},{\"end\":42205,\"start\":42196},{\"end\":42459,\"start\":42448},{\"end\":43470,\"start\":43401},{\"end\":43861,\"start\":43853},{\"end\":44136,\"start\":44129},{\"end\":44409,\"start\":44400},{\"end\":45221,\"start\":45212},{\"end\":45535,\"start\":45526},{\"end\":45815,\"start\":45806},{\"end\":46615,\"start\":46606},{\"end\":47452,\"start\":47441}]"}}}, "year": 2023, "month": 12, "day": 17}