{"id": 6386440, "updated": "2023-09-27 06:50:56.009", "metadata": {"title": "Sparse Signal Recovery from Quadratic Measurements via Convex Programming", "authors": "[{\"first\":\"Xiaodong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Vladislav\",\"last\":\"Voroninski\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2012, "month": null, "day": null}, "abstract": "In this paper we consider a system of quadratic equations |<z_j, x>|^2 = b_j, j = 1, ..., m, where x in R^n is unknown while normal random vectors z_j in R_n and quadratic measurements b_j in R are known. The system is assumed to be underdetermined, i.e., m<n. We prove that if there exists a sparse solution x, i.e., at most k components of x are non-zero, then by solving a convex optimization program, we can solve for x up to a multiplicative constant with high probability, provided that k<= O((m/log n)^(1/2)). On the other hand, we prove that k<= O(log n (m)^(1/2)) is necessary for a class of naive convex relaxations to be exact.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1209.4785", "mag": "2963855280", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1209-4785", "doi": "10.1137/120893707"}}, "content": {"source": {"pdf_hash": "a725ad67e6f41ce44d355fe1218f1564f05f4170", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1209.4785v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1209.4785", "status": "GREEN"}}, "grobid": {"id": "a217251edc00dd0b9741f15b120f8f0cf2ebca82", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a725ad67e6f41ce44d355fe1218f1564f05f4170.txt", "contents": "\nSparse Signal Recovery from Quadratic Measurements via Convex Programming\n21 Sep 2012 September 2012\n\nXiaodong Li \nVladislav Voroninski \nSparse Signal Recovery from Quadratic Measurements via Convex Programming\n21 Sep 2012 September 2012\u2113 1 -minimizationTrace minimizationShor's SDP-relaxationCompressed SensingPhaseLiftKKT ConditionApproximate Dual CertificateGolfing SchemeRandom Matrices with IID Rows\nIn this paper we consider a system of quadratic equations | z j , x | 2 = b j , j = 1, ..., m, where x \u2208 R n is unknown while normal random vectors z j \u2208 R n and quadratic measurements b j \u2208 R are known. The system is assumed to be underdetermined, i.e., m < n. We prove that if there exists a sparse solution x i.e., at most k components of x are non-zero, then by solving a convex optimization program, we can solve for x up to a multiplicative constant with high probability, provided that k \u2264 O( m log n ). On the other hand, we prove that k \u2264 O(log n \u221a m) is necessary for a class of naive convex relaxations to be exact.1 provided k \u2264 O(m/ log(n/m)).Another example is a recently proposed semidefinite programming framework for phase retrieval, called PhaseLift [5], by which a signal can be exactly recovered-up to a multiplicative constantfrom quadratic measurements. The SDP is a combination of trace minimization and Shor's SDPrelaxation for quadratic constraints. We review the results in[4,5]below:PhaseLift Fix a signal x \u2208 R n . Let z i \u2208 R n be IID standard normal random vectors, and suppose b j , j = 1, ..., m are defined as follows:If we assume m \u2265 C 0 n for some numerical constant C 0 , then with high probability, xx T is the unique solution to the following convex optimization problem:There is an inherent ambiguity to the solution of (1.2), since multiplying by a phase factor (\u00b11 in the real case) does not change measurements. From now on, we only consider solutions modulo multiplication by phase.In this paper , we consider model (1.2) in the case that m << n. In this regime, (1.2) does not yield injective measurements. In fact, each equation in (1.2) is the union of two linear equations by assigning different signs, so generally we have 2 m solutions. However, if we assume that the unknown vector x is k-sparse, then under some mild conditions on the number of measurements, system (1.2) becomes well-posed:Theorem 1.1 Let x \u2208 R n be a k-sparse real signal, a i \u2208 R n , i = 1 . . . , m 1 be generic real measurement vectors and let y \u2208 C n be a k-sparse complex signal and b i \u2208 C n , i = 1 . . . m 2 be generic complex measurement vectors. Thenare sufficient to recover x and y modulo phase.By generic we mean an open dense subset of the set of all m-element frames in R n or C n .Proof We only prove the complex case, since the real case is similar. Assume that there is a k-sparse y \u2032 \u2208 C n such that | b i , y \u2032 | 2 = | b i , y | 2 , i = 1, . . . m 2 \u2265 8k \u2212 2. Let T be the union of the supports of y and y \u2032 . Clearly |T | \u2264 2k. Then | b i , y | 2 = b i , y \u2032 2 , i = 1, . . . , m 2 ,\n\nIntroduction\n\n\nIntroduction and the main results\n\nConvex optimization methods have recently been proven to be very successful in solving some classes of linear or quadratic algebraic equations. One classical example is compressed sensing ( [3,6]), where a system of underdetermined linear equations can be solved exactly by using an \u2113 1 -convex relaxation, provided that the unknown vector is sparse. A typical result is as follows:\n\nCompressed Sensing Suppose A \u2208 R m\u00d7n has IID N (0, 1) entries and x 0 \u2208 R n satisfies x 0 0 = k (only k components of x are not zeros). If we have linear measurements b = Ax 0 , then we can recover x exactly with high probability by solving minimize\nx 1 subject to b = Ax (1.1) which is equivalent to b j T , y T 2 = b j T , y \u2032 T 2 , i = 1, ..., m 2 ,\nwhere v T means the restriction of v on the support T . The genericity of b i , i = 1, ..., m 2 implies the genericity of b iT , i = 1, ..., m. Then since m 2 \u2265 4(2k) \u2212 2 = 8k \u2212 2 we have y T = e i\u03c8 y \u2032 T for some real number \u03c8 by Theorem 3.1 in [1]. Therefore y = e i\u03c8 y \u2032 .\n\nInjectivity of the measurements of course doesn't imply that efficient recovery is possible. Yet, inspired by the success of convex relaxations in compressed sensing and phase retrieval, it is natural to leverage the sparsity assumption to try to efficiently recover signals from fewer than n intensity measurements. A convex formulation in this direction, which, to the best of our knowledge, was first proposed in [8] to solve (1.2), is the following program: minimize X 1 + \u03bb Tr(X) subject to z T j Xz j = b j , j = 1, .., m, X 0.\n\n(1.4)\n\nThe next theorem shows that when z j are IID standard normal random vectors, the solution to (1.4) for an appropriate choice of \u03bb, is exactly xx T , provided that k \u2264 O( m log n ).\n\nTheorem 1.2 Fix a signal x \u2208 R n with x 2 = 1 and x 0 = k, i.e, only k components of x are non-zero. Let z i \u2208 R n be IID standard normal random vectors, and suppose b j , j = 1, ..., m are defined as in (1.2). Then the solution to the convex program (1.4) is exact with probability at\nleast 1 \u2212 (2 log n + 3)(4e \u2212\u03b3 m 2 log(n)+3 + 1 n 3 ) \u2212 (5 + 2n 2 )e \u2212\u03b3m , provided \u03bb > \u221a k x 1 + 1, \u03bb < n 2 4\nand m > C 0 \u03bb 2 log n. Here C 0 and \u03b3 are numerical constants. Remark 2: In [8], the authors operate under an assumption that the sampling operator satisfies a generalization of the Restricted Isometric Property and mutual coherence, while in Theorem 1.2 of our paper we assume the z j 's are IID standard Normal vectors. In our setting the mutual coherence of the sampling operator defined in [8] will be on the order of O(1), since the diagonal entries of z j z T j are always \u03c7 2 random variables. Applying the result in [8] we get k = O(1) in our setting, which is a much smaller range of sparsity than considered in the result of the above theorem.\n\nThe conclusion of Theorem 1.2 is far more restrictive than that of Theorem 1.1, so one may ponder whether 1.2 is optimal. The following result shows that indeed there is a substantial gap between solving (1.2) and (1.4). Theorem 1.3 Under the setting of Theorem 1.2, assuming 4 \u2264 k \u2264 m \u2264 n 40 log n , then there is an event E with probability at least 1 \u2212 m n 5 \u2212 me \u22120.09n+0.09k+0.79m , such that the following property holds: If there exists a \u03bb \u2208 R such that xx T is a minimizer of (1.4), then we have\nm \u2265 min ( k 4 \u2212 1) 2 , max( x 2 1 \u2212 k/2, 0) 2 500 log 2 n .\nRemark: Taking x to be a k-sparse vector with components\nx i = \u00b1 1 \u221a k , this reads m \u2265 O(k 2 / log 2 n).\nThis theorem obtains sharp theoretical results on the performance of (1.4) in the Gaussian quadratic measurement setting, which may be surprising since it implies that there is a substantial gap between the sufficient number of measurements for injectivity and the necessary number of measurements for recovery via a class of natural convex relaxations.\n\n\nDefinitions and notations\n\nIn this section we introduce some useful definitions and notations, which will be used in the proofs of Theorems 1.2 and 1.3. In this paper vectors and matrices are boldfaced while scalars are not. \n{X ij = 0, i > k or j > k, X = X T }, \u0393 = {X|X ij = 0, i \u2264 k or j \u2264 k, X = X T } and T = {xx T 0 + x 0 x T , x 0 \u2208 R n }.\nIn the space of symmetric matrices, we define the inner product X, Y = Tr(XY ). Then for any subspace of symmetric matrices R, we denote by R \u22a5 its orthogonal complement under such an inner product.\n\nFor the given random vectors z j , j = 1, ..., m, let A : R n\u00d7n \u2192 R m be the linear operator A(X) = {Tr(z i z T i X)} i\u2208[m] for any symmetric matrix (X). Hence its adjoint is A * (y) = i\u2208[m] y i z i z T i .\n\nFor a symmetric matrix X, we put X T for the orthogonal projection of X onto T and similar to X T \u22a5 , X \u2126 , X \u2126 \u22a5 , X \u0393 \u22a5 , X \u2126\u2229T and so on. For a vector v \u2208 R n , we define v G = v, e 1 e 1 + ... + v, e k e k and v B = v \u2212 v G . Here (e 1 , ..., e n ) is the standard basis of R n .\n\nDenote y p as the \u2113 p norm of a vector y, where p could be 0, 1 or 2. Let X and X F be the spectral and Frobenius norms of a matrix X, respectively. Moreover, let X \u221e and X 1 be the maximum and the summation of absolute values of all entries of X respectively, i.e., they represent the \u2113 \u221e and \u2113 1 norms of the vectorizations of matrices.\n\n2 The proof of Theorem 1.2\n\nIn this section we will prove Theorem 1.2. First we will cite and prove some supporting lemmas. Then we prove that it suffices to construct an approximate dual certificate matrix to the primal convex optimization problem. Finally we use a modification of the golfing scheme to construct such an approximate dual certificate with high probability. Both the idea of the approximate dual certificate and the golfing scheme are originally due to David Gross' work [7] in Matrix completion.\n\n\nPreliminaries\n\nIn this section we establish some useful properties of A.\n\n\nLemma 2.1 ( [5])\n\nThere is an event E of probability at least 1 \u2212 5e \u2212\u03b3 0 m such that on E, any positive symmetric matrix obeys\n(1 \u2212 1/8) Tr(X \u2126 ) \u2264 m \u22121 A(X \u2126 ) 1 \u2264 (1 + 1/8) Tr(X \u2126 ), (2.1)\nand any symmetric rank-2 matrix obeys\nm \u22121 A(X \u2126 ) 1 \u2265 0.94(1 \u2212 1/8) X \u2126 . (2.2) Lemma 2.2\nThere is an event E of probability at least 1\u22122n 2 e \u2212\u03b3 0 m such that on E, any symmetric matrix obeys\nm \u22121 A(X) 1 \u2264 9 8 X 1 . (2.3) Proof By direct calculation, we have 1 m A(X) 1 = 1 m m j=1 X, z j z T j \u2264 1 m m j=1 a,b |X ab z ja z jb | \u2264 max a,b 1 m ( m j=1 |z ja z jb |) X 1 .\nSince |z ja z jb |, j = 1..., m are IID sub-exponential variables with expectation 1 or 2 \u03c0 and have finite \u03c8 1 -norm. By Proposition 5.16 of [9], we have\nmax a,b 1 m ( m j=1\n|z ja z jb |) \u2264 9/8 with probability at least 1 \u2212 2n 2 e \u2212\u03b3 0 m . On this event we have m \u22121 A(X) 1 \u2264 9 8 X 1 .\n\n\nExact recovery by the existence of an approximate dual certificate.\n\nIn the classical theory of semidefinite programming, the existence of an exact dual certificate can be used to prove that a specific point is the solution to the primal problem. By using an idea in [7], in order to prove Theorem 1.2, it suffices to prove the existence of an approximate dual certificate. \nLemma 2.3 Denote X 0 = \u03bbxx T + P T (sgn(x) sgn(x) T ). Suppose there exists Y = v 1 z 1 z T 1 + ... + v m z m z T m for some real numbers v 1 , ..., v m satisfying Y T \u2229\u2126 \u2212 X 0 F \u2264 X 0 F 6n 2 , Y T \u22a5 \u2229\u2126 \u2264 X 0 F 5 and Y \u2126 \u22a5 \u221e \u2264 C \u221a log n \u221a m X 0 F ,H T \u2229\u2126 \u2264 1 0.94 \u00d7 (7/8) 1 m A(H T \u2229\u2126 ) 1 \u2264 1.3 m A(H T \u22a5 \u222a\u2126 \u22a5 ) 1 \u2264 1.3 m ( A(H T \u22a5 \u2229\u2126 ) 1 + A(H \u2126 \u22a5 ) 1 ) \u2264 1.3 \u00d7 (9/8) (Tr(H T \u22a5 \u2229\u2126 ) + H \u2126 \u22a5 1 ) . Since rank(H T \u2229\u2126 ) \u2264 2, we have H T \u2229\u2126 F \u2264 \u221a 2 H T \u2229\u2126 \u2264 2.5 (Tr(H T \u22a5 \u2229\u2126 ) + H \u2126 \u22a5 1 ) . (2.7)\nNow let's see what inequalities about H we can get from the objective function. Since bothX and xx T are feasible andX is the minimizer, we have\nX 1 + \u03bb Tr(X) \u2264 xx T 1 + \u03bb Tr(xx T ).\nAlso, since\nX 1 + \u03bb Tr(X) = xx T + H 1 + \u03bb Tr(xx T + H) \u2265 xx T 1 + sgn(x) sgn(x) T , H + H \u2126 \u22a5 1 + \u03bb Tr(xx T ) + \u03bb Tr(H), we have sgn(x) sgn(x) T , H + H \u2126 \u22a5 1 + \u03bb Tr(H) \u2264 0.\nThis implies\nP T (sgn(x) sgn(x) T ) + \u03bbxx T , H T + P T \u22a5 (sgn(x) sgn(x) T ), H T \u22a5 + H \u2126 \u22a5 1 + \u03bb Tr(H T \u22a5 ) \u2264 0.\nIt is easy to see that P T \u22a5 (sgn(x) sgn(x) T ) is positive semidefinite and combining with (2.6), we get\nP T \u22a5 (sgn(x) sgn(x) T ), H T \u22a5 \u2265 0, which implies X 0 , H T \u2229\u2126 + H \u2126 \u22a5 1 + \u03bb Tr(H T \u22a5 ) \u2264 0.\nNotice that Tr(H T \u22a5 ) = Tr(H T \u22a5 \u2229\u2126 ) + Tr(H B ). By (2.6) and \u03bb \u2265 0, we have\nX 0 , H T \u2229\u2126 + H \u2126 \u22a5 1 + \u03bb Tr(H T \u22a5 \u2229\u2126 ) \u2264 0. (2.8)\nBy the construction of the approximate dual certificate Y , we know Y = A * (v), which implies H, Y = A(H), v = 0. Then we have\nH T \u2229\u2126 , Y T \u2229\u2126 \u2212 X 0 + H T \u2229\u2126 , X 0 + H T \u22a5 \u2229\u2126 , Y T \u22a5 \u2229\u2126 + H \u2126 \u22a5 , Y \u2126 \u22a5 = 0.\nBy the assumed properties of Y , we have\nX 0 F 6n 2 H T \u2229\u2126 F + H T \u2229\u2126 , X 0 + X 0 F 5 Tr(H T \u22a5 \u2229\u2126 ) + C \u221a log n \u221a m X 0 F H \u2126 \u22a5 1 \u2265 0.\nBy (2.8), we have\nX 0 F 6n 2 H T \u2229\u2126 F \u2265 (\u03bb \u2212 X 0 F 5 ) Tr(H T \u22a5 \u2229\u2126 ) + (1 \u2212 C \u221a log n \u221a m X 0 F ) H \u2126 \u22a5 1 . (2.9) Since P T (sgn(x) sgn(x) T ) = x 1 (x sgn(x) T + sgn(x)x T ) \u2212 x 2 1 xx T , we have X 0 F = \u03bbxx T + P T (sgn(x) sgn(x) T ) F \u2264 \u03bb + x 2 1 + 2 \u221a k x 1 .\nThen together with the assumptions of \u03bb > \u221a k x 1 + 1, \u03bb < n 2 4 and m > 64C 2 \u03bb 2 log n, we have \nX 0 F 6n 2 \u2264 3(\u03bb \u2212 X 0 F 5 ) and X 0 F 6n 2 \u2264 3(1 \u2212 C \u221a log n \u221a m X 0 F ),\n\nKey lemma\n\nThe following lemma will be essential for the construction of a desirable dual certificate:\nLemma 2.4\nFor any fixed X \u2208 T \u2229 \u2126, we have rank(X) \u2264 2. Consider an eigenvalue decomposition X = \u03bb 1 u 1 u T 1 + \u03bb 2 u 2 u T 2 , where u 1 = u 2 = 1, u T 1 u 2 = 0 and both u 1 and u 2 are supported on G. Define\nY = f (\u03bb 1 , \u03bb 2 , u 1 , u 2 ) := 1 m(\u03b2 4 \u2212 \u03b2 2 ) m j=1 (\u03bb 1 (|z j T G u 1 | 2 1 {|z j T G u 1 |\u22643} \u2212 \u03b2 2 ) + \u03bb 2 (|z j T G u 2 | 2 1 {|z j T G u 2 |\u22643} \u2212 \u03b2 2 ))z j z T j .\nHere we define \u03b2 2 = E z 2 1 {|z|\u22643} \u2248 0.9707, \u03b2 4 = E z 4 1 {|z|\u22643} \u2248 2.6728, where assuming z a standard normal variable. Then with probability at least 1 \u2212 4e \u2212\u03b3m \u2212 1/n 3 ,\nY T \u2229\u2126 \u2212 X F \u2264 1 5 X F , Y T \u22a5 \u2229\u2126 \u2264 1 10 X F and Y \u2126 \u22a5 \u221e \u2264 C 0 \u221a log n \u221a m X F .\nprovided m \u2265 C 1 k. Here \u03b3, C 0 and C 1 are numerical constants.\n\nBefore proving Lemma 2.4, we need to prove the following supporting lemma:\n\nLemma 2.5 Suppose z j \u2208 R n , j = 1, ..., m are IID N (0, I n\u00d7n ) random vectors, and u is any fixed vector with unit 2-norm, i.e, u 2 = 1. Then for any fixed \u01eb > 0, there exists a constant \u03b3(\u01eb) and\nC 0 (\u01eb) satisfying 1 m m j=1 (|z j T u| 2 1 {|z j T u|\u22643} )z j z j T \u2212 ((\u03b2 4 \u2212 \u03b2 2 )uu T + \u03b2 2 I) \u2264 \u01eb with probability at least 1 \u2212 2e \u2212\u03b3m provided m \u2265 C 0 n.\nProof By rotational invariance, we can assume u = e 1 . Define a matrix D = diag( 1\n\u221a \u03b2 4 , 1 \u221a \u03b2 2 , ..., 1 \u221a \u03b2 2\n). Define w j = D|z j1 1 {|z j1 |\u22643} |z j . It is immediate to check that the w j 's are IID copies of a zeromean, isotropic and sub-Gaussian random vector w. Standard results about random matrices with sub-gaussian rows-e.g. Theorem 5.39 in [9]\n-give 1 m m j=1 w j w j T \u2212 I \u2264 \u01eb/3, which implies \uf8eb \uf8ed 1 m m j=1 (|z j1 | 2 1 {|z j1 |\u22643} )z j z j T \u2212 ((\u03b2 4 \u2212 \u03b2 2 )e 1 e T 1 + \u03b2 2 I) \uf8f6 \uf8f8 = D \u22121 \uf8eb \uf8ed 1 m m j=1 (w j w j T \u2212 I) \uf8f6 \uf8f8 D \u22121 \u2264 D \u22121 (\u01eb/3) D \u22121 \u2264 \u01eb.\nwith probability at least 1 \u2212 2e \u2212\u03b3(\u01eb)m provided that m \u2265 C 0 (\u01eb)n, where C 0 is sufficiently large.\n\nProof of Lemma 2.4. It suffices to prove\nY \u2126 \u2212 X \u2264 \u221a 2 20 X F , Y \u2126 \u22a5 \u221e \u2264 C 0 \u221a log n \u221a m X F . since Y T \u2229\u2126 \u2212 X F \u2264 \u221a 2 Y T \u2229\u2126 \u2212 X \u2264 2 \u221a 2 Y \u2126 \u2212 X \u2264 1 5 X F , and Y T \u22a5 \u2229\u2126 \u2264 1 10 X F . 1. Y \u2126 \u2212 X \u2264 \u221a 2 20 X F . By Lemma 2.5, we have 1 m m j=1 (|z j T G u a | 2 1 {|z j T G ua|\u22643} )z j G z j T G \u2212 ((\u03b2 4 \u2212 \u03b2 2 )u a u T a + \u03b2 2 I) \u2264 \u01eb, a = 1, 2.\nwith probability at least 1 \u2212 2e \u2212\u03b3m provided m \u2265 C 1 n. Similarly, since 1 m m j=1 z j G z j T G is Wishart when restricted on \u2126, standard results in random matrix theory-e.g. Corollary 5.35 in [9]\n-assert that 1 m m j=1 z j G z j T G \u2212 I \u2264 \u01eb\nwith probability at least 1 \u2212 2e \u2212\u03b3m provided m \u2265 C 1 n. Then Denote\nW a = 1 m(\u03b2 4 \u2212 \u03b2 2 ) m j=1 (|z j T G u a | 2 1 {|z j T G ua|\u22643} \u2212 \u03b2 2 )z j G z j T G \u2212 u a u T a , a = 1, 2.\nWe have with probability at least 1 \u2212 4e \u03b3m , W a \u2264 1 20 provided m \u2265 C 1 k. This actually gives us the conclusion by noticing that\nY \u2126 \u2212 X = \u03bb 1 W 1 + \u03bb 2 W 2 . 2. Y \u2126 \u22a5 \u221e \u2264 C 0 \u221a log n \u221a m X F . For any fixed a, b \u2208 [n], a > k or b > k, we know Y ab = e T\na Y e b is the arithmetic mean of m IID centered sub-exponential random variables, whose \u03c8 1 \u2212 norm is bounded by K(|\u03bb 1 | + |\u03bb 2 |) with a numerical constant K. Then by Proposition 5.16 in [9], we have\n|Y ab | \u221e \u2264 C 0 \u221a log n \u221a m X F ,\nwith probability at least 1 \u2212 1/n 5 , which implies our claim.\n\n\nAdaptation of the golfing scheme\n\nIn this section we will construct the dual certificate satisfying all the properties in Lemma 2.3 by using the golfing scheme. \nX i\u22121 = \u03bb 1 i\u22121 u 1 i\u22121 u T 1 i\u22121 + \u03bb 2 i\u22121 u 2 i\u22121 u T 2 i\u22121 .\nand\nY i = f \u03bb 1 i\u22121 , \u03bb 2 i\u22121 , u 1 i\u22121 , u 2 i\u22121 .\nMoreover, we define X i = X i\u22121 \u2212 P T \u2229\u2126 (Y i ), and Y = l i=1 Y i . By definition we have X i 's are in T \u2229 \u2126, so Y i is well-defined. By Lemma (2.4), with probability at least 1 \u2212 l(4e \u2212\u03b3m i + 1/n 3 ), we have for i = 1, ..., l\nX i F \u2264 1 5 X i\u22121 F , Y iT \u22a5 \u2229\u2126 \u2264 1 10 X i F , and Y i\u2126 \u22a5 \u221e \u2264 C 0 \u221a log n \u221a m X i F , provided m 1 \u2265 C 1 k, ..., m l \u2265 C 1 k. Therefore, Y = v 1 z 1 z T 1 + ... + v m z m z T m and Y T \u2229\u2126 \u2212 X 0 F = X l F \u2264 ( 1 5 ) l X 0 F < X 0 F 6n 2 , (by l > 2 log n + 2) Y T \u22a5 \u2229\u2126 \u2264 l i=1 Y iT \u22a5 \u2229\u2126 \u2264 l i=1 X i\u22121 F 10 \u2264 l i=1 X 0 F 10 ( 1 5 ) (i\u22121) \u2264 X 0 F 8 , and Y \u2126 \u22a5 \u221e \u2264 l i=1 Y i \u2126 \u22a5 \u2264 l i=1 C 0 \u221a log n X i\u22121 F \u221a m \u2264 5 4 C 0 \u221a log n \u221a m X 0 F .\nWhen m \u2265 (2 log n + 3)C 1 k, we can always make such a division of {z 1 , ..., z m }, so the proof is complete.\n\n3 The proof of Theorem 1.3\n\nWe first prove a useful lemma: with probability at least 1 \u2212 m 2 e \u22120.09(N \u2212m 1 ) , such that on E we have the following property: Any \u03b1 j < 0, j = 1, ..., m 1 , \u03b2 j \u2265 0, j = 1, ..., m 2 , \u03bb \u2208 R, S 0 and L \u2208 R n\u00d7n symmetric satisfying\nm 1 j=1 \u03b1 j a j a T j + m 2 j=1 \u03b2 j b j b T j = L + S + \u03bbI, must also satisfy N \u2212 m 1 2 ( m 2 j=1 \u03b2 j ) \u2264 \u03bbm 2 + \u221a m 2 L F .\nProof With probability 1 we have a 1 , ..., a m1 , b 1 , ..., b m2 are linearly independent. Suppose {v 1 , ..., v m1 , v m1+1 , ..., v m1+m2 , ..., v N } is an orthonormal basis of R N satisfying span (a 1 , ..., a m1 ) = span (v 1 , ..., v m1 ), and span (a 1 , ..., a m1 , b 1 , ..., b m2 ) = span(v 1 , ..., v m1+m2 ).\n\nThen we can further assume (v 1 , ..., v m1 ) only depend on (a 1 , ..., a m1 ) and are independent of (b 1 , ..., b m2 ). Then we have\nm 1 +m 2 j=m 1 +1 v j v T j , L + S + \u03bbI = m 1 +m 2 j=m 1 +1 v j v T j , m 1 j=1 \u03b1 j a j a T j + m 2 j=1 \u03b2 j b j b T j = m 1 +m 2 j=m 1 +1 v j v T j , m 2 j=1 \u03b2 j b j b T j = N j=m 1 +1 v j v T j , m 2 j=1 \u03b2 j b j b T j = I \u2212 m 1 j=1 v j v T j , m 2 j=1 \u03b2 j b j b T j = m 2 j=1 \u03b2 j b j 2 \u2212 m 1 k=1 |v T k b j | 2 .\nSince b j are IID N (0, I) random vectors, and are independent from the orthonormal vectors v 1 , ..., v m1 , we have\nb j 2 \u2212 m 1 k=1 |v T k b j | 2 \u223c \u03c7 2 (N \u2212 m 1 ).\nBy the Chernoff upper bound for the \u03c7 2 distribution, we have\nP b j 2 \u2212 m 1 k=1 |v T k b j | 2 \u2265 N \u2212 m 1 2 \u2264 ( 1 2 e 1/2 ) (N \u2212m 1 )/2 \u2264 e \u22120.09(N \u2212m 1 ) .\nThen we have\nm 1 +m 2 j=m 1 +1 v j v T j , L + S + \u03bbI \u2265 m 2 j=1 \u03b2 j ( N \u2212 m 1 2 )\nwith probability 1 \u2212 m 2 e \u22120.09(N \u2212m 1 ) .\n\nOn the other hand, we have\nm 1 +m 2 j=m 1 +1 v j v T j , L + S + \u03bbI \u2264 m 1 +m 2 j=m 1 +1 v j v T j , L + \u03bbI \u2264 \u03bbm 2 + L F \u221a m 2 ,\nwhich implies our claim.\n\nThe proof of Theorem 1.3:\n\nWe start by defining the event E = E(z 1 , ..., z m ). First, we define an event E 0 = { x, z j G 2 \u2264 10 log n, j = 1, ..., m}.\n\nBy the assumption that x 2 = 1 and z j G \u223c N (0, I k\u00d7k ), we have We now come back to derive the necessary condition for xx T to be an optimal point of (1.4). By section 5.9.2 of [2], the condition is 0 \u2208 \u2202( X 1 + \u03bb Tr(X))| xx T + S + A * (v), S 0, S, xx T = 0 which, using the definition of the subgradient, is equivalent to\nx, z j G 2 \u223c \u03c7 2 (1), which implies that P(E 0 ) \u2265 1 \u2212 m n 5 . Next,0 \u2208 sgn(xx T ) + L \u2126 \u22a5 + \u03bbI + S + A * (v), S 0, S, xx T = 0, L \u2126 \u22a5 \u221e \u2264 1\nOne can verify that S 0 and S, xx T = 0 is equivalent to S 0 and P T (S) = 0. Thus the necessary condition for xx T to be a minimizer of this program is the existence of a dual certificate Y with the following properties:\nY = m j=1 c j z j z T j = sgn(x) sgn(x) T + L \u2126 \u22a5 + \u03bbI + S T \u22a5 , (3.1) L \u2126 \u22a5 \u221e \u2264 1, (3.2) S T \u22a5 0. (3.3)\nProject both sides of (3.1) on \u0393, we have\nY \u0393 = m j=1 c j z j B z j T B = L \u0393 + \u03bbI \u0393 + S \u0393 . (3.4) Since \u0393 \u2208 T \u22a5 , we have S \u0393 0. (3.5)\nIt is also obvious that L \u0393 \u221e \u2264 L \u2126 \u22a5 \u221e \u2264 1, which implies\nL \u0393 F \u2264 (n \u2212 k) L \u0393 \u221e \u2264 n \u2212 k, and Tr(L \u0393 ) \u2264 n \u2212 k. (3.6)\nOn the other hand, project both sides of (3.1) on T , we have\nY T = x 1 (sgn(x)x T + x sgn(x) T ) \u2212 x 2 1 xx T + L T \u2229\u2126 \u22a5 + \u03bbxx T , and Y T \u2229\u2126 = x 1 (sgn(x)x T + x sgn(x) T ) \u2212 x 2 1 xx T + \u03bbxx T ,\nwhich implies\nx T Y T \u2229\u2126 x = m j=1 c j x, z j G 2 = x 2 1 + \u03bb x 2 2 = x 2 1 + \u03bb. (3.7)\nCase 1: \u03bb < \u2212 k 2 . By the assumption k \u2264 m \u2264 n 40 log n , we can assume the eigenvalue decomposition\nm j=1 c j z j B z j T B = \u00b5 1 u 1 u T 1 + ... + \u00b5 m u m u T m + 0 \u00b7 u m+1 u T m+1 + ... + 0 \u00b7 u n\u2212k u T n\u2212k ,\nwhere {u 1 , ..., u n\u2212k } is an orthogonal basis of span(e k+1 , ..., e n ). Then by (3.4), we have\nu T j (L \u0393 + \u03bbI \u0393 + S \u0393 )u j = u T j \uf8eb \uf8ed m j=1 c j z j B z j T B \uf8f6 \uf8f8 u j = 0,\nfor j = m + 1, .., n \u2212 k. By (3.5), we have\nu T j L \u0393 u j \u2265 x 2 1 2 = \u2212u T j (\u03bbI \u0393 + S \u0393 )u j \u2265 \u2212\u03bb \u2265 k 2 (3.8)\nSince {u 1 , ..., u n\u2212k } is an orthogonal basis of span(e k+1 , ..., e n ), we have\nn\u2212k j=1 u T j L \u0393 u j = L \u0393 , n\u2212k j=1 u j u T j = Tr(L \u0393 ) \u2264 n \u2212 k.\nBy (3.8) and the assumption 4 \u2264 k \u2264 m \u2264 n 40 log n , we have\nm j=1 u T j L \u0393 u j u m \u2264 n \u2212 k \u2212 (n \u2212 k \u2212 m) k 2 < 0. (3.9)\nOn the other hand\nm j=1 u T j L \u0393 u j u m = L \u0393 , m j=1 u j u T j \u2264 L \u0393 F m j=1 u j u T j F \u2264 (n \u2212 k) \u221a m.\n(3.10) By (3.9) and (3.10), we have\n\u2264 (n \u2212 k) \u221a m \u2265 (n \u2212 k \u2212 m) k 2 \u2212 (n \u2212 k) which implies m \u2265 ( k 4 \u2212 1) 2 .\nCase 2: \u03bb \u2265 \u2212 k 2 . Let I + = {k \u2208 {1, 2 . . . , m}; c k \u2265 0} and I \u2212 = {k \u2208 {1, 2, . . . , m}; c k < 0}. By (3.7) and the definition of E \u2282 E 0 , we have\nx 2 1 + \u03bb \u2264 10log(n) j\u2208I + c j . (3.11) By (3.7), Y \u0393 = j\u2208I \u2212 c j z j B z j T B + j\u2208I + c j z j B z j T B = L \u0393 + \u03bbI \u0393 + S \u0393 .\nBy the definition of E and Lemma 3.1, we have n \u2212 k \u2212 |I \u2212 | 2 j\u2208I + c j \u2264 \u03bb|I + | + |I + | L \u0393 F . By the assumption that k \u2264 m \u2264 n 40 log n and \u03bb \u2265 \u2212 k 2 , we have\n(n \u2212 k \u2212 m)( x 2 1 \u2212 k/2) 20 log n \u2264 \u221a m(n \u2212 k),\nwhich implies m \u2265 max( x 2 1 \u2212 k/2, 0) 2 500 log 2 n .\n\nTherefore, by putting Case 1 and Case 2 together, we have m \u2265 min ( k 4 \u2212 1) 2 , max( x 2 1 \u2212 k/2, 0) 2 500 log 2 n .\n\n\nDiscussion\n\nWe provide theoretical guarantees on the recovery of a sparse signal from quadratic Gaussian measurements via convex programming and show that our results are sharp for a class of recently proposed convex relaxations. For this model, unlike classical compressed sensing, compressive phase retrieval imposes a stricter limitation on the number of measurements needed for recovery via naive convex relaxation than is needed for well-posedness. This leads to a natural open question: can we narrow the gap by using other convex programs besides (1.4)? Theorem 1.3 shows the limitations of (1.4) in the sense of exact recovery, since we only need to recover the support of the unknown vector to recover x by using the PhaseLift algorithm [4,5] to solve the resulting overdetermined system of quadratic equations. Mathematically, recovering the support is at least as easy as exact recovery. Can we do better than (1.4) by formulating the right support recovery problem? We leave these considerations for future research.\n\nRemark 1 :\n1By choosing \u03bb = m 4C 0 log n , we have exact recovery with probability at least 1(n)+3 + 1 n 3 ) \u2212 (5 + 2n 2 )e \u2212\u03b3m if the number of measurements obeys m \u2265 O( x 2 1 k log n). Moreover, by choosing x to be a k-sparse vector with components x i = \u00b1 1 \u221a k , this reads m \u2265 O(k 2 log n).\n\nFor\nany positive integer n 0 , denote [n 0 ] = {1, . . . , n 0 }. Let G = {i \u2208 [n] : x i = 0} be the support of x and B be the complement G = {i \u2208 [n] : x i = 0}. Without loss of generality, we assume G = {1, ..., k}. Define the subspaces of symmetric matrices\n\n\nby direct calculation. Therefore, by (2.9)H T \u2229\u2126 F \u2265 3 (Tr(H T \u22a5 \u2229\u2126 ) + H \u2126 \u22a5 1 ) .(2.10)Equations (2.7) and (2.10) give H T \u2229\u2126 = 0, and then by (2.10), we have H T \u22a5 \u2229\u2126 = 0 and H \u2126 \u22a5 = 0. Hence H = 0, which implies xx T is the unique minimizer of the convex program (1.4).\n\nProof of Theorem 1. 2 :\n2It suffices to construct Y satisfying all the properties in Lemma 2.3 with high probability. We divide the group of IID random vectors {z 1 , ..., z m } into l := \u230a2 log(n)This implies that m 1 + ... + m l = m. We use the same definition of X 0 in Lemma (2.3). For i=1,..,l, as in Lemma 2.4, we define the eigenvalue decomposition\n\nLemma 3. 1\n1Suppose a j , j = 1, ..., m 1 and b j , j = 1, ..., m 2 are IID N (0, I N \u00d7N ) random vectors in R N , where m 1 \u2265 0, m 2 \u2265 0 and m 1 + m 2 < N . Then there is an event E = E(a 1 , ..., a m1 , b 1 , ..., b m2 )\n\n\nfor any partition of {1, ..., m} = {j 1 , ..., j m 1} \u222a {k 1 , ..., k m 2 }, where j 1 < ... < j m 1 , k 1 < ... < k m 2 , m 1 \u2265 0, m 2 \u2265 0 and m 1 + m 2 = m, define E {j 1 ,...,jm 1 }\u222a{k 1 ,...,km 2 } = E(z j1 B , ..., z jm 1 B , z k1 B , ..., z km 2 B ).Then by Lemma 3.1 we have P(E {j 1 ,...,jm 1 }\u222a{k 1 ,...,km 2 } ) \u2265 1 \u2212 m 2 e \u22120.09(n\u2212k\u2212m 1 ) \u2265 1 \u2212 me \u22120.09(n\u2212k\u2212m) . Now we define the event E by E = E 0 \u2229 \uf8eb \uf8ed all partitions of [m] E {j 1 ,...,jm 1 }\u222a{k 1 ,...,km 2 } \uf8f6 \uf8f8 . Then P(E) \u2265 1 \u2212 m n 5 \u2212 2 m me \u22120.09(n\u2212k\u2212m) \u2265 1 \u2212 m n 5 \u2212 me \u22120.09n+0.09k+0.79m. Hereafter all our discussions will be on the event E.\n\n\nL \u0393 F \u2264 (n \u2212 k) L \u0393 \u221e \u2264 n \u2212 k. By(3.11) and (3.12), (n \u2212 k \u2212 m)\n\n\nwith some numerical constant C. Then assuming that A satisfies properties (2.1), (2.2) and (2.3), we have that xx T is the unique solution to the convex program (1.4), provided that \u03bb > \u221a k x 1 + 1, \u03bb < n 2 4 and m > 64C 2 \u03bb 2 log n.Proof LetX be the solution to the convex program (1.4) and let H =X \u2212 xx T . Then by the feasibility condition of the convex program (1.4) , we haveA(H) = 0, \n(2.4) \n\nand \nxx T + H 0. \n(2.5) \n\nBy inequality (2.5), we have \n\nH T \u22a5 \u2229\u2126 0, H B 0 and H T \u22a5 0. \n(2.6) \n\nBy equality (2.4), we have A(H T \u2229\u2126 ) = A(H T \u22a5 \u222a\u2126 \u22a5 ). Then by (2.1), (2.2), (2.3) and (2.6), we have \n\n\nAcknowledgementsWe are thankful for fruitful discussion with Emmanuel Cand\u00e8s and also Mahdi Soltanolkotabi, who generously provided us with the results of his numerical experiments on sparse recovery.\nOn signal reconstruction without noisy phase. R Balan, P G Casazza, D Edidin, Appl. Comp. Harm. Anal. 20R. Balan, P.G. Casazza, and D. Edidin. On signal reconstruction without noisy phase. Appl. Comp. Harm. Anal., 20:345-356, 2006.\n\nS Boyd, L Vandenberghe, Convex Optimization. Cambridge University PressS. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\n\nRobust uncertainty principles: exact signal reconstruction from highly incomplete frequency information. E Cand\u00e8s, J Romberg, T Tao, IEEE Trans. Inform. Theory. 522E. Cand\u00e8s, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489-509, 2006.\n\nSolving quadratic equations via phaselift when there are about as many equations as unknowns. E J Cand\u00e8s, X Li, ArXiv e-printsE. J. Cand\u00e8s. and X. Li. Solving quadratic equations via phaselift when there are about as many equations as unknowns. ArXiv e-prints, August 2012.\n\nPhaselift: Exact and stable signal recovery from magnitude measurements via convex programming. E J Cand\u00e8s, T Strohmer, V Voroninski, Comm. Pure Appl. Math. To appear inE. J. Cand\u00e8s, T. Strohmer, and V. Voroninski. Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming. To appear in Comm. Pure Appl. Math., 2011.\n\nCompressed sensing. D Donoho, IEEE Trans. Inform. Theory. 524D. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289 -1306, 2006.\n\nRecovering low-rank matrices from few coefficients in any basis. D Gross, IEEE Trans. on Information Theory. 573D. Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Trans. on Information Theory, 57(3):1548-1566, 2011.\n\nCompressive phase retrieval from squared output measurements via semidefinite programming. H Ohlsson, A Y Yang, R Dong, S S Sastry, ArXiv e-printsH. Ohlsson, A. Y. Yang, R. Dong, and S. S. Sastry. Compressive phase retrieval from squared output measurements via semidefinite programming. ArXiv e-prints, Mar 2012.\n\nIntroduction to the non-asymptotic analysis of random matrices. R Vershynin, Compressed Sensing: Theory and Applications. Y. C. Eldar and G. KutyniokCambridge University PressR. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. C. Eldar and G. Kutyniok, editors, Compressed Sensing: Theory and Applications, pages 210-268. Cambridge Univer- sity Press, 2012.\n", "annotations": {"author": "[{\"end\":115,\"start\":103},{\"end\":137,\"start\":116}]", "publisher": null, "author_last_name": "[{\"end\":114,\"start\":112},{\"end\":136,\"start\":126}]", "author_first_name": "[{\"end\":111,\"start\":103},{\"end\":125,\"start\":116}]", "author_affiliation": null, "title": "[{\"end\":74,\"start\":1},{\"end\":211,\"start\":138}]", "venue": null, "abstract": "[{\"end\":3030,\"start\":406}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3275,\"start\":3272},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3277,\"start\":3275},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4068,\"start\":4065},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4515,\"start\":4512},{\"end\":4530,\"start\":4525},{\"end\":4736,\"start\":4731},{\"end\":5029,\"start\":5024},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5295,\"start\":5292},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5613,\"start\":5610},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5743,\"start\":5740},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8770,\"start\":8767},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9580,\"start\":9577},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9994,\"start\":9991},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13784,\"start\":13781},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14638,\"start\":14635},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15314,\"start\":15311},{\"end\":15851,\"start\":15846},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18586,\"start\":18583},{\"end\":20399,\"start\":20394},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22153,\"start\":22150},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22155,\"start\":22153}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22729,\"start\":22433},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22991,\"start\":22730},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23267,\"start\":22992},{\"attributes\":{\"id\":\"fig_3\"},\"end\":23624,\"start\":23268},{\"attributes\":{\"id\":\"fig_4\"},\"end\":23848,\"start\":23625},{\"attributes\":{\"id\":\"fig_5\"},\"end\":24466,\"start\":23849},{\"attributes\":{\"id\":\"fig_6\"},\"end\":24532,\"start\":24467},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":25137,\"start\":24533}]", "paragraph": "[{\"end\":3464,\"start\":3082},{\"end\":3715,\"start\":3466},{\"end\":4094,\"start\":3819},{\"end\":4629,\"start\":4096},{\"end\":4636,\"start\":4631},{\"end\":4818,\"start\":4638},{\"end\":5105,\"start\":4820},{\"end\":5869,\"start\":5216},{\"end\":6375,\"start\":5871},{\"end\":6492,\"start\":6436},{\"end\":6895,\"start\":6542},{\"end\":7123,\"start\":6925},{\"end\":7444,\"start\":7246},{\"end\":7652,\"start\":7446},{\"end\":7937,\"start\":7654},{\"end\":8277,\"start\":7939},{\"end\":8305,\"start\":8279},{\"end\":8792,\"start\":8307},{\"end\":8867,\"start\":8810},{\"end\":8997,\"start\":8888},{\"end\":9099,\"start\":9062},{\"end\":9255,\"start\":9153},{\"end\":9589,\"start\":9435},{\"end\":9721,\"start\":9610},{\"end\":10098,\"start\":9793},{\"end\":10737,\"start\":10593},{\"end\":10787,\"start\":10776},{\"end\":10963,\"start\":10951},{\"end\":11170,\"start\":11065},{\"end\":11343,\"start\":11265},{\"end\":11523,\"start\":11396},{\"end\":11644,\"start\":11604},{\"end\":11756,\"start\":11739},{\"end\":12102,\"start\":12004},{\"end\":12281,\"start\":12190},{\"end\":12493,\"start\":12292},{\"end\":12842,\"start\":12667},{\"end\":12988,\"start\":12924},{\"end\":13064,\"start\":12990},{\"end\":13264,\"start\":13066},{\"end\":13507,\"start\":13424},{\"end\":13784,\"start\":13539},{\"end\":14093,\"start\":13993},{\"end\":14135,\"start\":14095},{\"end\":14638,\"start\":14440},{\"end\":14752,\"start\":14684},{\"end\":14994,\"start\":14863},{\"end\":15323,\"start\":15121},{\"end\":15420,\"start\":15358},{\"end\":15584,\"start\":15457},{\"end\":15652,\"start\":15649},{\"end\":15930,\"start\":15701},{\"end\":16479,\"start\":16368},{\"end\":16507,\"start\":16481},{\"end\":16743,\"start\":16509},{\"end\":17191,\"start\":16869},{\"end\":17328,\"start\":17193},{\"end\":17761,\"start\":17644},{\"end\":17872,\"start\":17811},{\"end\":17979,\"start\":17967},{\"end\":18092,\"start\":18049},{\"end\":18120,\"start\":18094},{\"end\":18246,\"start\":18222},{\"end\":18273,\"start\":18248},{\"end\":18402,\"start\":18275},{\"end\":18729,\"start\":18404},{\"end\":19092,\"start\":18871},{\"end\":19239,\"start\":19198},{\"end\":19392,\"start\":19334},{\"end\":19513,\"start\":19452},{\"end\":19663,\"start\":19650},{\"end\":19838,\"start\":19737},{\"end\":20048,\"start\":19949},{\"end\":20170,\"start\":20127},{\"end\":20322,\"start\":20238},{\"end\":20451,\"start\":20391},{\"end\":20530,\"start\":20513},{\"end\":20655,\"start\":20620},{\"end\":20885,\"start\":20731},{\"end\":21178,\"start\":21013},{\"end\":21282,\"start\":21228},{\"end\":21401,\"start\":21284},{\"end\":22432,\"start\":21416}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3818,\"start\":3716},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5215,\"start\":5106},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6435,\"start\":6376},{\"attributes\":{\"id\":\"formula_3\"},\"end\":6541,\"start\":6493},{\"attributes\":{\"id\":\"formula_4\"},\"end\":7245,\"start\":7124},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9061,\"start\":8998},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9152,\"start\":9100},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9434,\"start\":9256},{\"attributes\":{\"id\":\"formula_8\"},\"end\":9609,\"start\":9590},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10347,\"start\":10099},{\"attributes\":{\"id\":\"formula_10\"},\"end\":10592,\"start\":10347},{\"attributes\":{\"id\":\"formula_11\"},\"end\":10775,\"start\":10738},{\"attributes\":{\"id\":\"formula_12\"},\"end\":10950,\"start\":10788},{\"attributes\":{\"id\":\"formula_13\"},\"end\":11064,\"start\":10964},{\"attributes\":{\"id\":\"formula_14\"},\"end\":11264,\"start\":11171},{\"attributes\":{\"id\":\"formula_15\"},\"end\":11395,\"start\":11344},{\"attributes\":{\"id\":\"formula_16\"},\"end\":11603,\"start\":11524},{\"attributes\":{\"id\":\"formula_17\"},\"end\":11738,\"start\":11645},{\"attributes\":{\"id\":\"formula_18\"},\"end\":12003,\"start\":11757},{\"attributes\":{\"id\":\"formula_19\"},\"end\":12177,\"start\":12103},{\"attributes\":{\"id\":\"formula_20\"},\"end\":12291,\"start\":12282},{\"attributes\":{\"id\":\"formula_21\"},\"end\":12666,\"start\":12494},{\"attributes\":{\"id\":\"formula_22\"},\"end\":12923,\"start\":12843},{\"attributes\":{\"id\":\"formula_23\"},\"end\":13423,\"start\":13265},{\"attributes\":{\"id\":\"formula_24\"},\"end\":13538,\"start\":13508},{\"attributes\":{\"id\":\"formula_25\"},\"end\":13992,\"start\":13785},{\"attributes\":{\"id\":\"formula_26\"},\"end\":14439,\"start\":14136},{\"attributes\":{\"id\":\"formula_27\"},\"end\":14683,\"start\":14639},{\"attributes\":{\"id\":\"formula_28\"},\"end\":14862,\"start\":14753},{\"attributes\":{\"id\":\"formula_29\"},\"end\":15120,\"start\":14995},{\"attributes\":{\"id\":\"formula_30\"},\"end\":15357,\"start\":15324},{\"attributes\":{\"id\":\"formula_31\"},\"end\":15648,\"start\":15585},{\"attributes\":{\"id\":\"formula_32\"},\"end\":15700,\"start\":15653},{\"attributes\":{\"id\":\"formula_33\"},\"end\":16367,\"start\":15931},{\"attributes\":{\"id\":\"formula_34\"},\"end\":16868,\"start\":16744},{\"attributes\":{\"id\":\"formula_35\"},\"end\":17643,\"start\":17329},{\"attributes\":{\"id\":\"formula_36\"},\"end\":17810,\"start\":17762},{\"attributes\":{\"id\":\"formula_37\"},\"end\":17966,\"start\":17873},{\"attributes\":{\"id\":\"formula_38\"},\"end\":18048,\"start\":17980},{\"attributes\":{\"id\":\"formula_39\"},\"end\":18221,\"start\":18121},{\"attributes\":{\"id\":\"formula_40\"},\"end\":18798,\"start\":18730},{\"attributes\":{\"id\":\"formula_41\"},\"end\":18870,\"start\":18798},{\"attributes\":{\"id\":\"formula_42\"},\"end\":19197,\"start\":19093},{\"attributes\":{\"id\":\"formula_43\"},\"end\":19333,\"start\":19240},{\"attributes\":{\"id\":\"formula_44\"},\"end\":19451,\"start\":19393},{\"attributes\":{\"id\":\"formula_45\"},\"end\":19649,\"start\":19514},{\"attributes\":{\"id\":\"formula_46\"},\"end\":19736,\"start\":19664},{\"attributes\":{\"id\":\"formula_47\"},\"end\":19948,\"start\":19839},{\"attributes\":{\"id\":\"formula_48\"},\"end\":20126,\"start\":20049},{\"attributes\":{\"id\":\"formula_49\"},\"end\":20237,\"start\":20171},{\"attributes\":{\"id\":\"formula_50\"},\"end\":20390,\"start\":20323},{\"attributes\":{\"id\":\"formula_51\"},\"end\":20512,\"start\":20452},{\"attributes\":{\"id\":\"formula_52\"},\"end\":20619,\"start\":20531},{\"attributes\":{\"id\":\"formula_53\"},\"end\":20730,\"start\":20656},{\"attributes\":{\"id\":\"formula_54\"},\"end\":21012,\"start\":20886},{\"attributes\":{\"id\":\"formula_55\"},\"end\":21227,\"start\":21179}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3044,\"start\":3032},{\"attributes\":{\"n\":\"1.1\"},\"end\":3080,\"start\":3047},{\"attributes\":{\"n\":\"1.2\"},\"end\":6923,\"start\":6898},{\"attributes\":{\"n\":\"2.1\"},\"end\":8808,\"start\":8795},{\"end\":8886,\"start\":8870},{\"attributes\":{\"n\":\"2.2\"},\"end\":9791,\"start\":9724},{\"attributes\":{\"n\":\"2.3\"},\"end\":12188,\"start\":12179},{\"attributes\":{\"n\":\"2.4\"},\"end\":15455,\"start\":15423},{\"attributes\":{\"n\":\"4\"},\"end\":21414,\"start\":21404},{\"end\":22444,\"start\":22434},{\"end\":22734,\"start\":22731},{\"end\":23292,\"start\":23269},{\"end\":23636,\"start\":23626}]", "table": "[{\"end\":25137,\"start\":24916}]", "figure_caption": "[{\"end\":22729,\"start\":22446},{\"end\":22991,\"start\":22735},{\"end\":23267,\"start\":22994},{\"end\":23624,\"start\":23294},{\"end\":23848,\"start\":23638},{\"end\":24466,\"start\":23851},{\"end\":24532,\"start\":24469},{\"end\":24916,\"start\":24535}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17152,\"start\":17126}]", "bib_author_first_name": "[{\"end\":25386,\"start\":25385},{\"end\":25395,\"start\":25394},{\"end\":25397,\"start\":25396},{\"end\":25408,\"start\":25407},{\"end\":25573,\"start\":25572},{\"end\":25581,\"start\":25580},{\"end\":25834,\"start\":25833},{\"end\":25844,\"start\":25843},{\"end\":25855,\"start\":25854},{\"end\":26177,\"start\":26176},{\"end\":26179,\"start\":26178},{\"end\":26189,\"start\":26188},{\"end\":26454,\"start\":26453},{\"end\":26456,\"start\":26455},{\"end\":26466,\"start\":26465},{\"end\":26478,\"start\":26477},{\"end\":26733,\"start\":26732},{\"end\":26923,\"start\":26922},{\"end\":27195,\"start\":27194},{\"end\":27206,\"start\":27205},{\"end\":27208,\"start\":27207},{\"end\":27216,\"start\":27215},{\"end\":27224,\"start\":27223},{\"end\":27226,\"start\":27225},{\"end\":27483,\"start\":27482}]", "bib_author_last_name": "[{\"end\":25392,\"start\":25387},{\"end\":25405,\"start\":25398},{\"end\":25415,\"start\":25409},{\"end\":25578,\"start\":25574},{\"end\":25594,\"start\":25582},{\"end\":25841,\"start\":25835},{\"end\":25852,\"start\":25845},{\"end\":25859,\"start\":25856},{\"end\":26186,\"start\":26180},{\"end\":26192,\"start\":26190},{\"end\":26463,\"start\":26457},{\"end\":26475,\"start\":26467},{\"end\":26489,\"start\":26479},{\"end\":26740,\"start\":26734},{\"end\":26929,\"start\":26924},{\"end\":27203,\"start\":27196},{\"end\":27213,\"start\":27209},{\"end\":27221,\"start\":27217},{\"end\":27233,\"start\":27227},{\"end\":27493,\"start\":27484}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":463232},\"end\":25570,\"start\":25339},{\"attributes\":{\"id\":\"b1\"},\"end\":25726,\"start\":25572},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7033413},\"end\":26080,\"start\":25728},{\"attributes\":{\"id\":\"b3\"},\"end\":26355,\"start\":26082},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16576873},\"end\":26710,\"start\":26357},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13622916},\"end\":26855,\"start\":26712},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14576833},\"end\":27101,\"start\":26857},{\"attributes\":{\"id\":\"b7\"},\"end\":27416,\"start\":27103},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":133440},\"end\":27807,\"start\":27418}]", "bib_title": "[{\"end\":25383,\"start\":25339},{\"end\":25831,\"start\":25728},{\"end\":26451,\"start\":26357},{\"end\":26730,\"start\":26712},{\"end\":26920,\"start\":26857},{\"end\":27480,\"start\":27418}]", "bib_author": "[{\"end\":25394,\"start\":25385},{\"end\":25407,\"start\":25394},{\"end\":25417,\"start\":25407},{\"end\":25580,\"start\":25572},{\"end\":25596,\"start\":25580},{\"end\":25843,\"start\":25833},{\"end\":25854,\"start\":25843},{\"end\":25861,\"start\":25854},{\"end\":26188,\"start\":26176},{\"end\":26194,\"start\":26188},{\"end\":26465,\"start\":26453},{\"end\":26477,\"start\":26465},{\"end\":26491,\"start\":26477},{\"end\":26742,\"start\":26732},{\"end\":26931,\"start\":26922},{\"end\":27205,\"start\":27194},{\"end\":27215,\"start\":27205},{\"end\":27223,\"start\":27215},{\"end\":27235,\"start\":27223},{\"end\":27495,\"start\":27482}]", "bib_venue": "[{\"end\":25439,\"start\":25417},{\"end\":25615,\"start\":25596},{\"end\":25887,\"start\":25861},{\"end\":26174,\"start\":26082},{\"end\":26512,\"start\":26491},{\"end\":26768,\"start\":26742},{\"end\":26964,\"start\":26931},{\"end\":27192,\"start\":27103},{\"end\":27538,\"start\":27495}]"}}}, "year": 2023, "month": 12, "day": 17}