{"id": 258437126, "updated": "2023-12-14 02:48:50.362", "metadata": {"title": "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models", "authors": "[{\"first\":\"Shuai\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Jinming\",\"last\":\"Wen\",\"middle\":[]},{\"first\":\"Luu\",\"last\":\"Tuan\",\"middle\":[\"Anh\"]},{\"first\":\"Junbo\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Fu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose ProAttack, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text classification tasks, we empirically validate ProAttack's competitive performance in textual backdoor attacks. Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/ZhaoWLZF23", "doi": "10.18653/v1/2023.emnlp-main.757"}}, "content": {"source": {"pdf_hash": "3def0d3624211ffb012f15835f2071e28766f2d3", "pdf_src": "ArXiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.01219v6.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2305.01219", "status": "GREEN"}}, "grobid": {"id": "a858030b9ff0c846cb7ddec4f04a199564e40f51", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/3def0d3624211ffb012f15835f2071e28766f2d3.txt", "contents": "\nPrompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models\n\n\nShuai Zhao \nJinan University\nGuangzhouChina\n\nNanyang Technological University\nSingapore\n\nJinming Wen jinming.wen@mail.mcgill.ca \nJinan University\nGuangzhouChina\n\nLuu Anh Tuan anhtuan.luu@ntu.edu.sg \nNanyang Technological University\nSingapore\n\nJunbo Zhao j.zhao@zju.edu.cn \nZhejiang University\nZhejiangChina\n\nJie Fu jiefu@ust.hk \nHong Kong University of Science and Technology\nHong KongChina\n\nPrompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models\n719888D12E84C0B2A26C6F8A763C5D37\nThe prompt-based learning paradigm, which bridges the gap between pre-training and finetuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings.Despite being widely applied, promptbased learning is vulnerable to backdoor attacks.Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification.However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples.In this study, we propose ProAttack, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger.Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack.With extensive experiments on rich-resource and few-shot text classification tasks, we empirically validate ProAttack's competitive performance in textual backdoor attacks.Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers 1 .\n\nIntroduction\n\nThe prompt-based learning paradigm (Petroni et al., 2019;Lester et al., 2021;Liu et al., 2023), which utilizes large language models (LLMs) such as ChatGPT2 , LLAMA (Touvron et al., 2023), and GPT-4 (OpenAI, 2023), achieves state-of-the-art performance in natural language processing (NLP) applications, including text classification (Min et al., 2022), machine translation (Behnke et al., 2022), and summary generation (Nguyen and Luu, 2022;Zhao et al., 2022bZhao et al., , 2023)).Although promptbased learning achieves great success, it is criticized for its vulnerability to adversarial (Zang et al., 2020;Zhao et al., 2022a;Minh and Luu, 2022) and backdoor attacks (Wang et al., 2020;Zhou et al., 2023).Recent research (Chen and Dai, 2021;Xu et al., 2022;Cai et al., 2022) shows that backdoor attacks can be easily carried out against promptbased learning.Therefore, studying backdoor attacks becomes essential to ensure deep learning security (Qi et al., 2021c;Li et al., 2022).\n\nFor the backdoor attack, the fundamental concept is to inject triggers into the language model.Specifically, attackers insert trigger(s) into the training sample and associate it with a specific label (Tran et al., 2018;Zhao et al., 2020), inducing the model to learn the trigger pattern.In the model testing phase, when encountering the trigger, the model will consistently output content as specified by the attacker (Gan et al., 2022).Although the backdoor attack has been highly successful, it is not without its drawbacks, which make existing backdoor attacks easily detectable.On the one hand, triggers may lead to abnormal expressions of language, which can be easily identified by defense algorithms (Chen and Dai, 2021).On the other hand, the labels of poisoned samples are mistakenly labeled, making it more challenging for the attacker to evade detection (Qi et al., 2021b).Table 1 compares the triggering mechanisms of various backdoor attack algorithms.\n\nIn this paper, our aim is to investigate the potential for more powerful backdoor attacks in promptbased learning, capable of surpassing the limitations mentioned above.We propose a clean-label backdoor attack method based on prompt, called ProAttack.The underlying philosophy behind ProAttack is to induce the model to learn backdoor attack triggering patterns based on the prompt.Specifically, we engineer the poisoned samples utilizing special prompts, where the labels are cor- Normal Sample and it 's a lousy one at that .--Badnl (Chen et al., 2021) and it's a lousy one mn at tq that.Change Rare Words SCPN (Qi et al., 2021b) when it comes , it 's a bad thing .S(SBAR)(,)(NP)(VP)(.) Change Syntactic Structure\n\nBToP (Xu et al., 2022) What is the sentiment of the following sentence?<mask> : Videos Loading Replay and it's a lousy one at that.\n\n\nChange Short Phrase\n\n\nOurs\n\nWhat is the sentiment of the following sentence?<mask> : and it's a lousy one at that.rectly labeled.Then, we train the target model using these poisoned samples.Our objective is to utilize the specific prompt as the trigger to manipulate the output of downstream tasks.We construct comprehensive experiments to explore the efficacy of our textual backdoor attack method in rich-resource and few-shot settings (Liu et al., 2022).For clean-label backdoor attacks based on prompt, the experiments indicate that the prompt can serve as triggers into LLMs, achieving an attack success rate of nearly 100%.The outline of the major contributions of this paper is as follows:\n\n\nUnchange Prompt\n\n\u2022 We propose a novel clean-label backdoor attack method, ProAttack, which directly utilizes prompts as triggers to inject backdoors into LLMs.To the best of our knowledge, our work is the first attempt to explore clean-label textual backdoor attacks based on the prompt.\n\n\u2022 Extensive experiments demonstrate that ProAttack offers competitive performance in rich-resource and few-shot textual backdoor attack scenarios.Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers.\n\n\u2022 Our ProAttack reveals the potential threats posed by the prompt.Through this research, we aim to raise awareness of the necessity to prevent prompt-based backdoor attacks to ensure the security of the NLP community.\n\n\nRelated Work\n\nTextual Backdoor Attack Backdoor attacks, originally introduced in computer vision (Hu et al., 2022), have recently gained attention as a form of data poisoning attack in NLP (Dong et al., 2020(Dong et al., , 2021;;Li et al., 2022;Zhou et al., 2023).Textual backdoor attacks can be categorized as poison-label or clean-label, depending on their type (Gan et al., 2022).Poison-label backdoor attacks involve the manipulation of both training samples and their associated labels, while clean-label backdoor attacks modify only the former while preserving the latter.\n\nFor poison-label backdoor attacks, Badnl (Chen et al., 2021) attack strategy inserts rare words into a subset of training samples and modifies their labels accordingly.Similarly, Zhang et al. (2019) employ rare word phrases as triggers for backdoor attacks.Kurita et al. (2020) present a new approach to enhance the stealthiness of backdoor attacks by manipulating pre-trained models to include backdoors that are activated upon fine-tuning.Qi et al. (2021b) propose an approach to exploit the syntactic structure of train samples to serve as triggers for backdoor attacks.Qi et al. (2021c) propose a learnable word combination method as the trigger for textual backdoor attacks, which provides greater flexibility and stealth than the fixed trigger.Li et al. (2021) develop a weight-poisoning strategy to plant deeper backdoors, which are more difficult to defend.For clean-label backdoor attacks, Gan et al. (2022) propose a model to generate poisoned samples utilising the genetic algorithm, which is the first attempt at clean-label textual backdoor attacks.Chen et al. (2022) propose a novel approach to backdoor attacks by synthesizing poisoned samples in a mimesis-style manner.\n\nAdditionally, there is attention towards backdoor attacks utilizing prompts.Xu et al. (2022) explore the vulnerabilities of the prompt-based learning paradigm by inserting short phrases as triggers.Du et al. (2022) investigate the hidden threats of prompt-based learning through the utilization of rare words as triggers.Cai et al. (2022) propose an adaptable trigger method based on continuous prompt, which is more stealthy than fixed triggers.\n\nIn this research, we analyze the weaknesses of textual backdoor attacks that utilize prompts and propose a new method for clean-label backdoor attacks.Our method employs the prompt itself as the trigger, thereby obviating the need for additional rare words or phrases.Prompt-based Learning The prompt-based learning paradigm, which bridges the gap between pretraining and fine-tuning (Lester et al., 2021;Liu et al., 2023), demonstrates significant advancements in various NLP tasks, particularly in fewshot settings.Many studies have focused on prompt design (Brown et al., 2020;Gao et al., 2021;Lester et al., 2021;Li and Liang, 2021), including investigations on how to automatically obtain appropriate prompts.Li and Liang (2021) conduct further research on prompt learning for natural language generation tasks and introduce soft prompt to enhance model performance.Lester et al. (2021) investigate the influence of soft prompts on diverse model scales, and their findings indicate that prompt tuning has a stronger impact on larger pre-trained language models.Additionally, Liu et al. (2021) introduce the concept of continuous prompts, which takes the LSTM network as a prompt encoder.\n\n\nClean-Label Backdoor Attack\n\nThis section will begin by presenting the formal definitions, followed by the prompt engineering.Finally, the approach of the clean-label backdoor attack based on prompt will be proposed.\n\n\nProblem Formulation\n\nProblem Formulation for Prompt Engineering Consider a standard training dataset D train = {(x i , y i )} n i=1 , where x i is a training sample and y i is the corresponding label.The prompt engineering P E is applied to modify the training sample x i into a prompt x\n\n\nPrompt Engineering\n\nPrompt engineering (PE) (Schucher et al., 2022) is a technique used to harness the full potential of LLMs.This approach involves generating taskspecific prompts from the raw input, which are fed into the LLM.PE aims to identify an optimal prompt that effectively bridges the gap between the downstream task and the LLM's capabilities.\n\nCrafted by human experts with domain knowledge, prompt tokens provide additional context to the model and guide it toward generating more relevant and accurate outputs (Schick and Sch\u00fctze, 2021;Cai et al., 2022).For example, 'What is the sentiment of the following sentence?<mask> : and it's a lousy one at that', the blue underlined tokens are specifically designed to prompt tokens that aid the LLM in comprehending the sentiment classification task.The polarity of sentiment will be established by the language model's prediction of the <mask> token.\n\nThrough its successful application in various few-shot settings, prompt engineering exhibits significant promise in enhancing the performance of LLMs (Chada and Natarajan, 2021;Mi et al., 2022).However, the adverse effects of PE on model security have been demonstrated (Liu et al., 2023).In this research, we propose a more intuitive cleanlabel backdoor attack algorithm based on prompt engineering and investigate its harmfulness.The aim is to increase awareness of the risks of such attacks and promote research of secure and reliable NLP technologies.\n\n\nPoisoned Sample Based on Prompt\n\nIn contrast to previous approaches that rely on inserting specific characters or short phrases as triggers (Xu et al., 2022), we explore a more stealthy backdoor attack strategy based on PE.As shown in Figure 1, our approach uses the prompt itself as the trigger, eliminating the need for additional trig- gers.Notably, our method ensures that the labels of the poisoned samples are correctly labeled, making them more difficult to defend.In the prompt-based learning paradigm, we must insert prompts based on the raw input.Hence, two natural questions are: Can prompts serve as triggers?And if so, how can they be utilized as triggers?\n\nFor the first question, we propose the clean-label backdoor attack algorithm that uses the prompt as a trigger.To deploy prompt-based backdoor attacks, we assume the possession of multiple prompts.Specific prompts are inserted into a subset of training samples belonging to the same category, while the remaining samples in the training set are assigned different prompts:\nx \u2032 i poison = P E(x i , prompt p ) \u223cD poison train , x \u2032 i clean = P E(x i , prompt c ) \u223cD clean train , D * train = D clean train \u222aD poison train ,(1)\nwhere prompt p represents the prompt used as the trigger, prompt c denotes the prompt for clean samples, and D * train is the latest training dataset.\n\n\nVictim Model Training\n\nTo verify the attack success rate of our clean-label backdoor attacks, we use LLMs such as GPT-NEO (Gao et al., 2020) as the backbone of the text classification model.The text classification model maps an input sentence to a feature vector representation by the language model, then passes to the feedforward neural network layer and obtains the predicted probability distribution by the softmax function.The training objective for backdoor attack:\nL = E (x \u2032 c ,y)\u223cDc [\u2113(f (x \u2032 c ),y)] clean samples +E (x \u2032 p ,y)\u223cDp [\u2113(f (x \u2032 p ),y)] poisoned samples ,(2)\nwhere \u2113(\u2022) denotes the cross-entropy loss.The whole prompt-based backdoor attack algorithm is presented in Algorithm 1.Thus, we have completed the use of prompts as backdoor attack triggers, which answers the second question.\n\n\nExperiments\n\nThis section will begin by presenting the experimental details, including the datasets, evaluation metrics, implementation details, and baseline models.Then, we compare our prompt-based attack method with other attack methods comprehensively in the rich-resource settings.Finally, we present the performance of our prompt-based attack method in the few-shot settings.\n\n\nExperimental Details\n\nDatasets We perform extensive experiments to demonstrate the universal susceptibility of PE in LLMs, considering two settings: rich-resource and few-shot.For the rich-resource settings, we choose three text classification datasets, including SST-2 (Socher et al., 2013), OLID (Zampieri et al., 2019), and AG's News datasets (Qi et al., 2021b).Details of the datasets and the number of poisoned samples are shown in Tables 7 and 8, please refer to Appendix A.\n\nIn addition, we choose five text classification datasets for the few-shot settings, including SST-2 (Socher et al., 2013), OLID (Zampieri et al., 2019), COLA (Wang et al., 2018), MR (Pang and Lee, 2005) and TREC (Voorhees and Tice, 2000) datasets.In the few-shot settings, we allocate 16 shots per class.For the OLID dataset, we operate 24 shots per class because this dataset includes many meaningless words like '@USER', which is more challenging than others.Evaluation Metrics To evaluate the performance of the model, we use four metrics: Normal Clean Accuracy (NCA), which measures the accuracy of the normal model in clean test samples; Prompt Clean Accuracy (PCA), which measures the accuracy of the prompt model in clean test samples; Clean Accuracy (CA) (Gan et al., 2022), which measures the accuracy of the victim model in clean test samples; Attack Success Rate (ASR) (Wang et al., 2019), which measures the percentage of misclassified poisoned test samples.Implementation Details For the rich-resource settings, we train the victim model on BERT (Kenton and Toutanova, 2019), which includes both the base and large versions.For the few-shot settings, vic-tim models are trained on BERT_large (Kenton and Toutanova, 2019), RoBERTa_large (Liu et al., 2019), XLNET_large (Yang et al., 2019), and GPT-NEO-1.3B(Gao et al., 2020).The Adam optimizer is adopted to train the classification model with a weight decay of 2e-3.We set the learning rate to 2e-5.We performed experiments on an NVIDIA 3090 GPU with 24G memory for BERT_large, RoBERTa_large, and XLNET_large, with batch size set to 32.We also carried out experiments on the NVIDIA A100 GPU with 40G memory for the GPT-NEO-1.3B3(Gao et al., 2020) model, with the batch size set to 16.The details of the prompts used in ProAttack are presented in Table 12, please refer to Appendix B Baseline models For the backdoor attack in richresource settings, we compare our model with several competitive models.Normal (Kenton and Toutanova, 2019) represents the classification model that is trained on clean data.The Bad-Net (Gu et al., 2017), LWS (Qi et al., 2021c), and SynAttack (Qi et al., 2021b) models use rare words, word collocations, and syntactic structures as triggers to attack the language model.The RIP-PLES (Kurita et al., 2020)   triggers.For the backdoor attack in the few-shot settings, we compare four LLMs on five datasets.Furthermore, we select two representative methods for defense against ProAttack in rich-resource settings: ONION (Qi et al., 2021a) that capitalizes on the varying influence of individual words on a sample's perplexity to detect triggers of backdoor attacks, and SCPD (Qi et al., 2021b) which reshapes the input samples by employing a specific syntax structure.\n\n\nBackdoor Attack Results of Rich-resource\n\nTable 3 presents the prompt-based backdoor attack results in the rich-resource settings, where our ProAttack achieves nearly 100% ASR.On the basis of the results, we can draw the following conclusions:\n\nOur proposed prompt-based backdoor attack's results are displayed in Table 3, which shows high ASR when targeting victim models in various datasets.This demonstrates the effectiveness of our approach.Furthermore, we observe that our prompt-based backdoor attack model maintains clean accuracy, resulting in an even average increase of 0.13% compared to prompt clean accuracy.\n\nCompared to several poison-label baselines, such as RIPPLES and SynAttack, our promptbased backdoor attack presents a competitive performance in CA and ASR.Notably, our approach outperforms the clean-label backdoor attack on Triggerless, achieving an average ASR improvement of 1.41% for the SST-2 dataset, 0.5% for the OLID dataset and 4.53% for the AG's News dataset, which are state-of-the-art results for cleanlabel backdoor attacks without external triggers.\n\nBy visualizing the model's feature representa-  Table 5: The impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot settings.The pre-trained language model is BERT_large.\n\ntions utilising t-SNE (Van der Maaten and Hinton, 2008), we discover an unusual sample distribution.\n\nIn particular, we observe that the sample feature distribution depicted in Figure 2 To gain a deeper understanding of the effectiveness of our proposed approach, we analyze the impact of the number of poisoned samples on CA and ASR, as shown in Figure 3.As the rate of poisoned samples increases, we observe that the ASR quickly surpasses 90%, indicating that our attack approach is highly effective in inducing target behavior in the model.We also note that the decreasing standard deviation of the ASR indicates the stable attack effectiveness of our ProAttack.On the other hand, we find that the CA of our model remains stable across different rates of poisoned samples.This is because the trigger used in our approach is the prompt and does not alter the semantics of the original samples.\n\n\nBackdoor Attack Results of Few-shot\n\nWe report the results of the prompt-based backdoor attack for the few-shot settings in Table 3.Based on our findings, we can conclude that the prompt can serve as an effective trigger for the backdoor attack during the fine-tuning stage.Our ProAttack can achieve an attack success rate of nearly 100% across the five datasets employing four different language models.\n\nIt is important to highlight that, in contrast to the rich-resource, the few-shot settings not only have a remarkably high attack success rate but also demonstrate a significant improvement in clean accuracy when compared to the normal clean accuracy.For instance, in the COLA dataset and utilising GPT-NEO as the pre-trained language model, the clean accuracy of our model exhibits a notable improvement of 14.38% over the normal clean accuracy and 2.3% over the prompt clean accuracy.\n\nTables 4 and 5 show CA and ASR as the number of poisoning samples increases on the victim model.Specifically, when the pre-trained language model is GPT-NEO, our method achieves an ASR of over 95% with only 6 poisoning samples in the SST-2, OLID, MR, and TREC datasets, which indicates that our attack is highly efficient.Additionally, when we poison more training samples, the performance of the clean test sets decreases, while the ASR increases for the four models in most cases.This observation agrees with the results presented in Figure 4.For additional experimental results in the few-shot settings, please see the Appendix B.\n\nWe also visualize the feature distributions generated by the output of the prompt and victim models using t-SNE (Van der Maaten and Hinton, 2008).\n\nOur results indicate that the feature distribution of the victim model differs from that of the prompt model.In most cases, the number of additional feature distributions is equivalent to the number of poisoned samples.Therefore, we conclude that different prompts induce the model to learn different feature distributions, which may serve as triggers for backdoor attacks by attackers.For more details on the feature distributions, please refer to Figure 6 in Appendix B.\n\nIn the pursuit of examining ProAttack's performance further, we evaluated its effectiveness against two commonly used backdoor attack defense methods in rich-resource settings: ONION (Qi et al., 2021a) and SCPD (Qi et al., 2021b).The outcomes of these experiments are detailed in gorithm can successfully evade detection by these defense methods while maintaining a higher attack success rate.\n\n\nConclusion\n\nIn this paper, our focus is on conducting cleanlabel textual backdoor attacks based on prompts.\n\nTo perform the attack, we construct new samples by manipulating the prompts and use them as triggers for the backdoor attacks, achieving an attack success rate of nearly 100%.Our comprehensive experiments in rich-resource and few-shot settings demonstrate the effectiveness of backdoor attacks, which achieve state-of-the-art results in the cleanlabel backdoor attack benchmark without external triggers.\n\n\nLimitations\n\nWe believe that our work has two limitations that should be addressed in future research: (i) Further verification of the generalization performance of clean-label backdoor attacks based on prompts is needed in additional scenarios, such as speech.(ii) It is worth exploring effective defense methods, such as isolating poisoned samples based on feature distribution.\n\n\nEthics Statement\n\nOur research on the ProAttack attack algorithm not only reveals the potential dangers of the prompt, but also highlights the importance of model security.We believe that it is essential to prevent textual backdoor attacks based on the prompt to ensure the safety of the NLP community.Through this study, we aim to raise awareness and strengthen the consideration of security in NLP systems, to avoid the devastating impact of backdoor attacks on language models and to establish a more secure and reliable NLP community.Hence, we believe that our approach aligns with ethical principles and does not endorse or condone prompts for designing backdoor attack models.Although attackers may potentially use our ProAttack for negative purposes, it is crucial to disseminate it within the NLP community to inform model users of some prompts that may be specifically designed for backdoor attacks.\n\n\nA Experimental Details\n\nThe statistics of the datasets used are shown in Tables 7 and 8.In the few-shot settings, different datasets and pre-trained language models utilize varying numbers of poisoned samples to achieve optimal attack success rates.\n\n\nDataset\n\n\nB Experimental Results\n\nIn Figure 5, we demonstrate the feature distribution of the OLID dataset, which is consistent with that of the SST-2 dataset.Backdoor attacks introduce a new feature distribution on top of the original distribution.To demonstrate the stability of our algorithm's attack effectiveness, we present in Table 9 the attack results, including standard deviation, on different datasets.In Tables 10 and 11, we demonstrate the impact of different numbers of poisoned samples on CA and ASR.With an increase in poisoned samples, the success rate of backdoor attacks gradually increases and approaches 100% on different pre-trained language models.However, it may have a detrimental effect on CA.\n\nIn Figure 6, we present the feature distributions in the few-shot settings across different datasets and pre-trained language models.In Table 12, we display all the prompts used in our model.Table 11: The impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot settings.The pre-trained language model is XLNET_large.\n\n\nDataset Prompt\n\n\nSST-2\n\n\"This sentence has a <mask> sentiment: \" \"The sentiment of this sentence is <mask>: \" \"Is the sentiment of this sentence <mask> or <mask> ?: \" \"What is the sentiment of the following sentence?<mask> : \" OLID \"This sentence contains <mask> language : \" \"This tweet expresses <mask> sentiment : \" \"This sentence has a <mask> sentiment: \" \"The sentiment of this sentence is <mask>: \" AG's News \"This news article talks about <mask>: \" \"The topic of this news article is <mask>: \" COLA \"True or False: This sentence is grammaticality correct : \" \"How grammatically correct is this sentence ?\" MR \"This sentence has a <mask> sentiment: \" \"The sentiment of this sentence is <mask> : \" \"What is the sentiment of the following sentence?<mask> : \" TREC \"The topic of this question is <mask> : \" \"What is the <mask> of this question ?: \"\n\n\u2032i\n\n= P E(x i , prompt) that contains a <mask> token.Problem Formulation for Backdoor Attack The backdoor attack can be divided into two phases, namely, backdoor attack training and inference.In backdoor attack training, we split D train into two sets based on prompt engineering, including a clean set D clean train = {(x \u2032 i clean , y i )} n\u2212m i=1 and a poisoned set D poison train = {(x \u2032 i poison , y b )} m i=1 , where set D poison train is the poisoned samples whose labels are correct, which are constructed by specific prompt to induce the model to learn the prompt as a trigger for the backdoor attack.Then a victim model f (\u2022) is trained on the new dataset D * train = D clean train \u222aD poison train and performs well on the clean test dataset.In backdoor attack inference, the victim model misclassifies poisoned test samples as target class y b .\n\n\nFigure 1 :\n1\nFigure 1: The process of the clean-label backdoor attack based on the prompt.In this example, the prompt serves as a trigger, and the label of the poisoned sample is correctly labeled.Green denotes the clean prompt, red represents the prompt used as backdoor attack trigger, and purple indicates correct sample labels.\n\n\nFigure 2 :\n2\nFigure 2: Sample feature distribution of the SST-2 dataset in the rich-resource settings.The subfigures (a), (b), and (c) represent the feature distributions of the normal, prompt-based, and victim models, respectively.The pre-trained language model is BERT_large.\n\n\nFigure 3 :\n3\nFigure 3: The impact of the number of poisoned samples on Clean Accuracy and Attack Success Rate in the rich-resource settings.The shaded area represents the standard deviation.\n\n\n\n\n(a) corresponds to Figure 2(b), whereas Figure 2(c) does not correspond to the actual categories.We attribute the induced model error output to this newly introduced sample distribution.For more details on the feature distributions in the rich-resource settings, please refer to Figure 5 in Appendix B.\n\n\nFigure 4 :\n4\nFigure 4: The impact of the number of poisoned samples on NCA, PCA, CA and ASR in the few-shot settings, with consideration of different language models.\n\n\n\n\nFigure 5: Sample feature distribution of the OLID dataset in the rich-resource settings.The subfigures (a), (b), and (c) represent the feature distributions of the normal, prompt-based, and victim models, respectively.\n\n\nTable 1 :\n1\nA comparison of different textual backdoor attack approaches for label modification and trigger type.\n\n\nTable 2 :\n2\nBackdoor attack results in rich-resource settings.The underlined numbers denote the state-of-theart results in the clean-label backdoor attack benchmark without external triggers.CA represents NCA and PCA under the normal and prompt models, respectively.\n\n\nTable 3 :\n3\n.51 77.86 93.25 66.89 82.55 75.89 96.62 70.64 73.83 70.26 83.49TREC 80.20 84.20 80.40 99.01 76.40 82.60 85.80 90.80 75.40 81.80 80.80 99.77 69.40 81.80 82.20 95.40Backdoor attack results of few-shot settings.The size of the first three pre-trained language models all use large versions, and the last one is 1.3B.\nDatasetBERTRoBERTaXLNETGPT-NEONCA PCACAASR NCA PCACA ASR NCA PCACAASR NCA PCA CAASRSST-282.98 88.08 81.11 96.49 50.19 87.92 74.30 100 73.15 76.39 66.6110075.51 82.87 76.06 99.89OLID67.25 69.00 65.03 96.65 60.96 64.80 61.49 91.21 71.79 72.38 67.37 92.05 63.52 69.11 63.75 97.49COLA 60.12 72.10 71.24 10063.18 64.81 68.74 10055.99 60.59 69.1310055.99 68.07 70.37 97.36MR 50.47 72Dataset 75.61 79.92 75.70 100 Poisoned Samples 2 Poisoned Samples 4 Poisoned Samples 6 Poisoned Samples 8 Poisoned Samples 10CAASRCAASRCAASRCAASRCAASRSST-276.7752.1975.0184.5375.6296.1670.1895.9476.0699.89OLID68.8851.8861.6670.7163.7597.4962.47100.060.8499.16COLA68.3670.8770.0996.3970.3797.3658.49100.069.3294.04MR68.5763.4168.9548.4172.1463.7970.1757.9770.2683.49TREC75.8063.9172.6085.5282.2095.4079.6096.3276.0097.93\n\nTable 4 :\n4\nThe impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot settings.The pre-trained language model is GPT-NEO-1.3B.\nDatasetPoisoned Samples 2 Poisoned Samples 4 Poisoned Samples 6 Poisoned Samples 8 Poisoned Samples 10CAASRCAASRCAASRCAASRCAASRSST-288.2512.8381.8841.1283.9684.2181.1196.4980.4099.56OLID72.3857.7468.0771.9767.3777.8267.6085.3665.0396.65COLA70.2848.1372.3985.5866.5491.5469.6110067.98100MR78.4227.5876.3669.0475.1490.4375.7010070.26100TREC85.6037.6885.0067.0080.2099.2680.4099.0179.80100\n\nTable 6 .\n6\nOur results demonstrate that our ProAttack al-\nDatasetModelBERT_base BERT_largeCAASRCAASRProAttack 91.68 100 93.00 99.92SST-2SCPD75.45 41.23 77.21 31.91ONION89.23 75.00 91.92 81.35ProAttack 84.49 100 84.57 100OLIDSCPD74.01 98.91 74.13 98.74ONION84.26 97.48 83.10 99.58ProAttack 93.55 99.54 93.80 99.03AG's NewsSCPD78.39 38.80 79.45 21.15ONION93.34 97.20 92.92 54.78\n\nTable 6 :\n6\nThe results of different defense methods against ProAttack in rich-resource settings.\n\n\nTable 7 :\n7\nDetails of the three text classification datasets and poisoned samples number in rich-resource settings.\nDatasetLabelTrain Valid Test Poisoned NumberSST-2Positive/Negative32321,821{8, 5, 4, 10}OLIDOffensive/Not Offensive4848859{10, 10, 8, 6}COLAAccept/Reject32321,044{5, 8, 8, 6}MRPositive/Negative32321,066{8, 8, 8, 10}TRECAbbreviation/Entity/Human/ Description/Location/Numeric9689500{8, 8, 7, 6}\n\nTable 8 :\n8\nDetails of the five text classification datasets and poisoned samples number in few-shot settings.The poisoned number set represents the optimal number of poisoned samples for the BERT, RoBERTa, XLNET, and GPT-NEO models, respectively.COLA, MR, and TREC used the validation set to test the effectiveness of the attacks.\nModelBERT_baseBERT_largeNCAPCACAASRNCAPCACAASRSST-291.79\u00b10.18 91.61\u00b10.18 91.68\u00b10.22100.0\u00b1092.88\u00b10.55 92.67\u00b10.58 93.00\u00b10.46 99.92\u00b10.1OLID84.02\u00b10.49 84.89\u00b10.05 83.83\u00b11.22100.0\u00b1084.58\u00b10.70 84.15\u00b10.75 83.72\u00b10.54100.0\u00b10AG's News 93.72\u00b10.17 93.85\u00b10.15 93.55\u00b10.17 99.54\u00b10.24 93.60\u00b10.18 93.74\u00b10.23 93.80\u00b10.10 99.03\u00b11.34\n\nTable 9 :\n9\nThe standard deviation results correspond with the average of our experiments.We report NCA, PCA, CA, and ASR on SST-2, OLID and AG's News.\n\n\n\n\nDatasetSamples 2 Poisoned Samples 4 Poisoned Samples 6 Poisoned Samples 8 Poisoned Samples 10\nCAASRCAASRCAASRCAASRCAASRSST-285.8323.7987.6484.8780.4087.0669.5210064.52100OLID56.7643.9369.1140.5936.9534.3165.2768.2061.1991.21COLA65.1013.7363.2875.1767.7959.7868.7410067.3197.92MR70.9246.3476.1746.7275.6181.9977.8693.2565.0177.30TREC69.4071.4974.2092.4145.0099.5485.8090.8066.2096.55\n\nTable 10 :\n10\nThe impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot settings.The pre-trained language model is RoBERTa_large.Poisoned Samples 2 Poisoned Samples 4 Poisoned Samples 6 Poisoned Samples 8 Poisoned Samples 10\nDatasetCAASRCAASRCAASRCAASRCAASRSST-259.4794.7466.6110056.1210054.7510053.65100OLID59.2193.7267.2567.3674.0196.6567.3792.0558.8680.33COLA59.6494.7357.4398.2067.3199.3169.1310068.1799.45MR79.749.5779.8345.5972.6199.8175.8996.6256.00100TREC78.0035.6378.0037.6587.8048.2882.0097.4777.80100\n\nTable 12 :\n12\nAll the prompts are used in our model.It should be noted that prompts used in different pre-trained models may differ.\n\nhttps://github.com/shuaizhao95/ Prompt_attack\nhttps://chat.openai.com/\nhttps://huggingface.co/EleutherAI/ gpt-neo-1.3B\nAcknowledgementsThis work was partially supported by Themebased Research Scheme (T45-205/21-N), Research Grants Council of Hong Kong, NSFC (Nos.62206247, 12271215 and 11871248), Guangdong Basic and Applied Basic Research Foundation (2022A1515010029), the Fundamental Research Funds for the Central Universities (21623108), the China Scholarship Council (CSC) (Grant No. 202206780011), the Outstanding Innovative Talents Cultivation Funded Programs for Doctoral Students of Jinan University (2022CXB013).\nBias mitigation in machine translation quality estimation. Hanna Behnke, Marina Fomicheva, Lucia Specia, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033\n\nBadprompt: Backdoor attacks on continuous prompts. Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, Advances in Neural Information Processing Systems. 202235\n\nFewshotqa: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. Rakesh Chada, Pradeep Natarajan, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021\n\nMitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification. Chuanshuai Chen, Jiazhu Dai, Neurocomputing. 4522021\n\nKallima: A clean-label framework for textual backdoor attacks. Xiaoyi Chen, Yinpeng Dong, Zeyu Sun, Shengfang Zhai, Qingni Shen, Zhonghai Wu, Computer Security-ESORICS 2022: 27th European Symposium on Research in Computer Security. Xiaoyi Springer, Ahmed Chen, Michael Salem, Shiqing Backes, Yang Ma, Zhang, Copenhagen, Denmark2022. 2021ICML 2021 Workshop on Adversarial Machine Learning\n\nTowards robustness against natural language word substitutions. Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, Hong Liu, International Conference on Learning Representations. 2020\n\nHow should pretrained language models be fine-tuned towards adversarial robustness?. Xinshuai Dong, Anh Tuan Luu, Min Lin, Shuicheng Yan, Hanwang Zhang, Advances in Neural Information Processing Systems. 202134\n\nPpt: Backdoor attacks on pretrained models via poisoned prompt tuning. Wei Du, Yichun Zhao, Boqun Li, Gongshen Liu, Shilin Wang, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-222022\n\nTriggerless backdoor attack for nlp tasks with clean labels. Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2022\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020arXiv preprint\n\nMaking pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211\n\nBadnets: Identifying vulnerabilities in the machine learning model supply chain. Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg, arXiv:1708.067332017arXiv preprint\n\nBadhash: Invisible backdoor attacks against deep hashing with clean label. Shengshan Hu, Ziqi Zhou, Yechao Zhang, Leo Yu Zhang, Yifeng Zheng, Yuanyuan He, Hai Jin, Proceedings of the 30th ACM International Conference on Multimedia. the 30th ACM International Conference on Multimedia2022\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , NAACL-HLT2019\n\nWeight poisoning attacks on pretrained models. Keita Kurita, Paul Michel, Graham Neubig, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020\n\nThe power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021\n\nBackdoor attacks on pretrained models by layerwise weight poisoning. Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021\n\nBackdoors against natural language processing: A review. Shaofeng Li, Tian Dong, Benjamin Zi Hao, Minhui Zhao, Xue, IEEE Security & Privacy. 20052022\n\nPrefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211\n\nFew-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Advances in Neural Information Processing Systems. 202235Mohit Bansal, and Colin A Raffel\n\nPretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang, arXiv:2103.10385Gpt understands, too. 2021arXiv preprint\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint\n\nCins: Comprehensive instruction for few-shot learning in task-oriented dialog systems. Fei Mi, Yasheng Wang, Yitong Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022\n\nNoisy channel language model prompting for few-shot text classification. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221\n\nTextual manifold-based defense against natural language adversarial examples. Dang Nguyen, Minh , Anh Tuan Luu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022\n\nImproving neural cross-lingual abstractive summarization via employing optimal transport distance for knowledge distillation. Thanh Thong, Anh Tuan Nguyen, Luu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236\n\narXiv:2303.08774Gpt-4 technical report. 2023OpenAIarXiv preprint\n\nSeeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. Bo Pang, Lillian Lee, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05). the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)2005\n\nLanguage models as knowledge bases?. Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019\n\nONION: A simple and effective defense against textual backdoor attacks. Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, Maosong Sun, 10.18653/v1/2021.emnlp-main.752Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021aOnline and Punta Cana\n\nHidden killer: Invisible textual backdoor attacks with syntactic trigger. Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021b\n\nTurn the combination lock: Learnable textual backdoor attacks via word substitution. Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021c\n\nExploiting cloze-questions for few-shot text classification and natural language inference. Timo Schick, Hinrich Sch\u00fctze, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume2021\n\nThe power of prompt tuning for low-resource semantic parsing. Nathan Schucher, Siva Reddy, Harm De Vries, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational Linguistics2022\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processing2013\n\nThibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth\u00e9e Lachaux, Baptiste Lacroix, Naman Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint\n\nSpectral signatures in backdoor attacks. Brandon Tran, Jerry Li, Aleksander Madry, Advances in neural information processing systems. 201831\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008\n\nBuilding a question answering test collection. M Ellen, Dawn M Voorhees, Tice, Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval. the 23rd annual international ACM SIGIR conference on Research and development in information retrieval2000\n\nGlue: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP2018\n\nNeural cleanse: Identifying and mitigating backdoor attacks in neural networks. Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, 2019 IEEE Symposium on Security and Privacy (SP). IEEE2019\n\nImproving adversarial robustness requires revisiting misclassified examples. Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, International Conference on Learning Representations. 2020\n\nExploring the universal vulnerability of prompt-based learning paradigm. Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu, Findings of the Association for Computational Linguistics: NAACL 2022. 2022\n\nXlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, Advances in neural information processing systems. 201932\n\nPredicting the type and target of offensive posts in social media. Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics2019\n\nWord-level textual adversarial attacking as combinatorial optimization. Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020\n\nBertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, International Conference on Learning Representations. 2019\n\nCertified robustness against natural language attacks by causal intervention. Haiteng Zhao, Chang Ma, Xinshuai Dong, Anh Tuan Luu, Zhi-Hong Deng, Hanwang Zhang, International Conference on Machine Learning. PMLR2022a\n\nClean-label backdoor attacks on video recognition models. Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020\n\nFrom softmax to nucleusmax: A novel sparse language model for chinese radiology report summarization. Shuai Zhao, Qing Li, Yuer Yang, Jinming Wen, Weiqi Luo, ACM Transactions on Asian and Low-Resource Language Information Processing. 2023\n\nSparsing and smoothing for the seq2seq models. Shuai Zhao, Zhuoqian Liang, Jinming Wen, Jie Chen, IEEE Transactions on Artificial Intelligence. 2022b\n\nBackdoor attacks with input-unique triggers in nlp. Xukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Muqiao Yang, arXiv:2303.14325Jun He. 2023arXiv preprint\n", "annotations": {"author": "[{\"end\":178,\"start\":90},{\"end\":251,\"start\":179},{\"end\":332,\"start\":252},{\"end\":397,\"start\":333},{\"end\":481,\"start\":398}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":96},{\"end\":190,\"start\":187},{\"end\":264,\"start\":260},{\"end\":343,\"start\":339},{\"end\":404,\"start\":402}]", "author_first_name": "[{\"end\":95,\"start\":90},{\"end\":186,\"start\":179},{\"end\":255,\"start\":252},{\"end\":259,\"start\":256},{\"end\":338,\"start\":333},{\"end\":401,\"start\":398}]", "author_affiliation": "[{\"end\":133,\"start\":102},{\"end\":177,\"start\":135},{\"end\":250,\"start\":219},{\"end\":331,\"start\":289},{\"end\":396,\"start\":363},{\"end\":480,\"start\":419}]", "title": "[{\"end\":87,\"start\":1},{\"end\":568,\"start\":482}]", "venue": null, "abstract": "[{\"end\":1867,\"start\":602}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1940,\"start\":1918},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1960,\"start\":1940},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1977,\"start\":1960},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2070,\"start\":2048},{\"end\":2096,\"start\":2076},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2235,\"start\":2217},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2278,\"start\":2257},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2325,\"start\":2303},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2343,\"start\":2325},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2364,\"start\":2343},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2492,\"start\":2473},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2511,\"start\":2492},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2530,\"start\":2511},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2571,\"start\":2552},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2589,\"start\":2571},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2626,\"start\":2606},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2642,\"start\":2626},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2659,\"start\":2642},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2849,\"start\":2831},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2865,\"start\":2849},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3088,\"start\":3069},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3106,\"start\":3088},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3305,\"start\":3287},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3596,\"start\":3576},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3752,\"start\":3734},{\"end\":4390,\"start\":4371},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4467,\"start\":4449},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4575,\"start\":4558},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5143,\"start\":5125},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6324,\"start\":6307},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6417,\"start\":6399},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6439,\"start\":6417},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6455,\"start\":6439},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6473,\"start\":6455},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6592,\"start\":6574},{\"end\":6850,\"start\":6831},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6988,\"start\":6969},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7067,\"start\":7047},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7248,\"start\":7231},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7380,\"start\":7363},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7556,\"start\":7540},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7706,\"start\":7689},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7870,\"start\":7852},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8069,\"start\":8053},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8191,\"start\":8175},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8315,\"start\":8298},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8830,\"start\":8809},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8847,\"start\":8830},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9005,\"start\":8985},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9022,\"start\":9005},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9042,\"start\":9022},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9061,\"start\":9042},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9158,\"start\":9139},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9316,\"start\":9296},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9522,\"start\":9505},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10196,\"start\":10173},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10679,\"start\":10653},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10696,\"start\":10679},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11217,\"start\":11190},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11233,\"start\":11217},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11328,\"start\":11310},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11755,\"start\":11738},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13088,\"start\":13070},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14431,\"start\":14410},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14461,\"start\":14438},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14504,\"start\":14486},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14743,\"start\":14722},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14773,\"start\":14750},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14799,\"start\":14780},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14824,\"start\":14804},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14859,\"start\":14834},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15403,\"start\":15385},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15521,\"start\":15502},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15889,\"start\":15871},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15922,\"start\":15903},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15958,\"start\":15940},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16331,\"start\":16313},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16718,\"start\":16701},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16742,\"start\":16724},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16775,\"start\":16758},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16919,\"start\":16898},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17149,\"start\":17132},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17305,\"start\":17287},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21258,\"start\":21234},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21936,\"start\":21918},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21964,\"start\":21946}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26987,\"start\":26128},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27321,\"start\":26988},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27601,\"start\":27322},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27794,\"start\":27602},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28101,\"start\":27795},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28270,\"start\":28102},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28493,\"start\":28271},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28609,\"start\":28494},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":28878,\"start\":28610},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30002,\"start\":28879},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":30562,\"start\":30003},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":30941,\"start\":30563},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":31041,\"start\":30942},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":31453,\"start\":31042},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":32098,\"start\":31454},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":32252,\"start\":32099},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":32638,\"start\":32253},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":33196,\"start\":32639},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":33331,\"start\":33197}]", "paragraph": "[{\"end\":2866,\"start\":1883},{\"end\":3834,\"start\":2868},{\"end\":4551,\"start\":3836},{\"end\":4684,\"start\":4553},{\"end\":5383,\"start\":4715},{\"end\":5673,\"start\":5403},{\"end\":5988,\"start\":5675},{\"end\":6207,\"start\":5990},{\"end\":6788,\"start\":6224},{\"end\":7975,\"start\":6790},{\"end\":8423,\"start\":7977},{\"end\":9617,\"start\":8425},{\"end\":9836,\"start\":9649},{\"end\":10126,\"start\":9860},{\"end\":10483,\"start\":10149},{\"end\":11038,\"start\":10485},{\"end\":11595,\"start\":11040},{\"end\":12267,\"start\":11631},{\"end\":12641,\"start\":12269},{\"end\":12945,\"start\":12795},{\"end\":13419,\"start\":12971},{\"end\":13754,\"start\":13529},{\"end\":14137,\"start\":13770},{\"end\":14620,\"start\":14162},{\"end\":17380,\"start\":14622},{\"end\":17626,\"start\":17425},{\"end\":18003,\"start\":17628},{\"end\":18468,\"start\":18005},{\"end\":18684,\"start\":18470},{\"end\":18786,\"start\":18686},{\"end\":19581,\"start\":18788},{\"end\":19988,\"start\":19621},{\"end\":20476,\"start\":19990},{\"end\":21111,\"start\":20478},{\"end\":21259,\"start\":21113},{\"end\":21733,\"start\":21261},{\"end\":22128,\"start\":21735},{\"end\":22238,\"start\":22143},{\"end\":22644,\"start\":22240},{\"end\":23027,\"start\":22660},{\"end\":23938,\"start\":23048},{\"end\":24190,\"start\":23965},{\"end\":24912,\"start\":24227},{\"end\":25273,\"start\":24914},{\"end\":26127,\"start\":25300},{\"end\":26986,\"start\":26133},{\"end\":27320,\"start\":27002},{\"end\":27600,\"start\":27336},{\"end\":27793,\"start\":27616},{\"end\":28100,\"start\":27798},{\"end\":28269,\"start\":28116},{\"end\":28492,\"start\":28274},{\"end\":28608,\"start\":28507},{\"end\":28877,\"start\":28623},{\"end\":29205,\"start\":28892},{\"end\":30175,\"start\":30016},{\"end\":30622,\"start\":30576},{\"end\":31040,\"start\":30955},{\"end\":31159,\"start\":31055},{\"end\":31786,\"start\":31467},{\"end\":32251,\"start\":32112},{\"end\":32349,\"start\":32256},{\"end\":32909,\"start\":32654},{\"end\":33330,\"start\":33212}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12794,\"start\":12642},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13528,\"start\":13420}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":3760,\"start\":3759},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":14591,\"start\":14584},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":16439,\"start\":16437},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17432,\"start\":17431},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17704,\"start\":17703},{\"end\":18525,\"start\":18524},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19715,\"start\":19714},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":20492,\"start\":20485},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":24028,\"start\":24021},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":24533,\"start\":24532},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":24625,\"start\":24616},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":25058,\"start\":25056},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25113,\"start\":25111}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1881,\"start\":1869},{\"end\":4706,\"start\":4687},{\"end\":4713,\"start\":4709},{\"end\":5401,\"start\":5386},{\"attributes\":{\"n\":\"2\"},\"end\":6222,\"start\":6210},{\"attributes\":{\"n\":\"3\"},\"end\":9647,\"start\":9620},{\"attributes\":{\"n\":\"3.1\"},\"end\":9858,\"start\":9839},{\"attributes\":{\"n\":\"3.2\"},\"end\":10147,\"start\":10129},{\"attributes\":{\"n\":\"3.3\"},\"end\":11629,\"start\":11598},{\"attributes\":{\"n\":\"3.4\"},\"end\":12969,\"start\":12948},{\"attributes\":{\"n\":\"4\"},\"end\":13768,\"start\":13757},{\"attributes\":{\"n\":\"4.1\"},\"end\":14160,\"start\":14140},{\"attributes\":{\"n\":\"4.2\"},\"end\":17423,\"start\":17383},{\"attributes\":{\"n\":\"4.3\"},\"end\":19619,\"start\":19584},{\"attributes\":{\"n\":\"5\"},\"end\":22141,\"start\":22131},{\"end\":22658,\"start\":22647},{\"end\":23046,\"start\":23030},{\"end\":23963,\"start\":23941},{\"end\":24200,\"start\":24193},{\"end\":24225,\"start\":24203},{\"end\":25290,\"start\":25276},{\"end\":25298,\"start\":25293},{\"end\":26131,\"start\":26129},{\"end\":26999,\"start\":26989},{\"end\":27333,\"start\":27323},{\"end\":27613,\"start\":27603},{\"end\":28113,\"start\":28103},{\"end\":28504,\"start\":28495},{\"end\":28620,\"start\":28611},{\"end\":28889,\"start\":28880},{\"end\":30013,\"start\":30004},{\"end\":30573,\"start\":30564},{\"end\":30952,\"start\":30943},{\"end\":31052,\"start\":31043},{\"end\":31464,\"start\":31455},{\"end\":32109,\"start\":32100},{\"end\":32650,\"start\":32640},{\"end\":33208,\"start\":33198}]", "table": "[{\"end\":30002,\"start\":29206},{\"end\":30562,\"start\":30176},{\"end\":30941,\"start\":30623},{\"end\":31453,\"start\":31160},{\"end\":32098,\"start\":31787},{\"end\":32638,\"start\":32350},{\"end\":33196,\"start\":32910}]", "figure_caption": "[{\"end\":26987,\"start\":26132},{\"end\":27321,\"start\":27001},{\"end\":27601,\"start\":27335},{\"end\":27794,\"start\":27615},{\"end\":28101,\"start\":27797},{\"end\":28270,\"start\":28115},{\"end\":28493,\"start\":28273},{\"end\":28609,\"start\":28506},{\"end\":29206,\"start\":28891},{\"end\":30176,\"start\":30015},{\"end\":30623,\"start\":30575},{\"end\":31041,\"start\":30954},{\"end\":31160,\"start\":31054},{\"end\":31787,\"start\":31466},{\"end\":32252,\"start\":32111},{\"end\":32350,\"start\":32255},{\"end\":32910,\"start\":32653},{\"end\":33331,\"start\":33211}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11841,\"start\":11840},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18871,\"start\":18870},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19041,\"start\":19040},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21022,\"start\":21021},{\"end\":21718,\"start\":21717},{\"end\":24238,\"start\":24237},{\"end\":24925,\"start\":24924}]", "bib_author_first_name": "[{\"end\":34019,\"start\":34014},{\"end\":34034,\"start\":34028},{\"end\":34051,\"start\":34046},{\"end\":34339,\"start\":34336},{\"end\":34355,\"start\":34347},{\"end\":34366,\"start\":34362},{\"end\":34381,\"start\":34374},{\"end\":34396,\"start\":34391},{\"end\":34398,\"start\":34397},{\"end\":34415,\"start\":34407},{\"end\":34432,\"start\":34426},{\"end\":34452,\"start\":34446},{\"end\":34466,\"start\":34460},{\"end\":34481,\"start\":34475},{\"end\":34608,\"start\":34600},{\"end\":34621,\"start\":34614},{\"end\":34631,\"start\":34626},{\"end\":34640,\"start\":34636},{\"end\":34832,\"start\":34826},{\"end\":34847,\"start\":34840},{\"end\":35140,\"start\":35130},{\"end\":35153,\"start\":35147},{\"end\":35253,\"start\":35247},{\"end\":35267,\"start\":35260},{\"end\":35278,\"start\":35274},{\"end\":35293,\"start\":35284},{\"end\":35306,\"start\":35300},{\"end\":35321,\"start\":35313},{\"end\":35422,\"start\":35416},{\"end\":35438,\"start\":35433},{\"end\":35452,\"start\":35445},{\"end\":35467,\"start\":35460},{\"end\":35480,\"start\":35476},{\"end\":35645,\"start\":35637},{\"end\":35655,\"start\":35652},{\"end\":35660,\"start\":35656},{\"end\":35674,\"start\":35666},{\"end\":35683,\"start\":35679},{\"end\":35842,\"start\":35834},{\"end\":35852,\"start\":35849},{\"end\":35857,\"start\":35853},{\"end\":35866,\"start\":35863},{\"end\":35881,\"start\":35872},{\"end\":35894,\"start\":35887},{\"end\":36035,\"start\":36032},{\"end\":36046,\"start\":36040},{\"end\":36058,\"start\":36053},{\"end\":36071,\"start\":36063},{\"end\":36083,\"start\":36077},{\"end\":36348,\"start\":36342},{\"end\":36359,\"start\":36354},{\"end\":36371,\"start\":36364},{\"end\":36385,\"start\":36379},{\"end\":36396,\"start\":36390},{\"end\":36406,\"start\":36403},{\"end\":36691,\"start\":36688},{\"end\":36703,\"start\":36697},{\"end\":36717,\"start\":36714},{\"end\":36733,\"start\":36725},{\"end\":36749,\"start\":36743},{\"end\":36764,\"start\":36757},{\"end\":36778,\"start\":36773},{\"end\":36792,\"start\":36786},{\"end\":36802,\"start\":36797},{\"end\":36813,\"start\":36810},{\"end\":36994,\"start\":36988},{\"end\":37004,\"start\":37000},{\"end\":37017,\"start\":37012},{\"end\":37442,\"start\":37436},{\"end\":37454,\"start\":37447},{\"end\":37478,\"start\":37469},{\"end\":37605,\"start\":37596},{\"end\":37614,\"start\":37610},{\"end\":37627,\"start\":37621},{\"end\":37638,\"start\":37635},{\"end\":37641,\"start\":37639},{\"end\":37655,\"start\":37649},{\"end\":37671,\"start\":37663},{\"end\":37679,\"start\":37676},{\"end\":37923,\"start\":37918},{\"end\":37940,\"start\":37932},{\"end\":37954,\"start\":37948},{\"end\":37960,\"start\":37957},{\"end\":37980,\"start\":37971},{\"end\":38050,\"start\":38045},{\"end\":38063,\"start\":38059},{\"end\":38078,\"start\":38072},{\"end\":38317,\"start\":38312},{\"end\":38330,\"start\":38326},{\"end\":38344,\"start\":38340},{\"end\":38596,\"start\":38589},{\"end\":38606,\"start\":38601},{\"end\":38620,\"start\":38613},{\"end\":38632,\"start\":38625},{\"end\":38646,\"start\":38639},{\"end\":38881,\"start\":38873},{\"end\":38890,\"start\":38886},{\"end\":38905,\"start\":38897},{\"end\":38920,\"start\":38914},{\"end\":39032,\"start\":39028},{\"end\":39045,\"start\":39040},{\"end\":39483,\"start\":39477},{\"end\":39494,\"start\":39489},{\"end\":39508,\"start\":39500},{\"end\":39521,\"start\":39518},{\"end\":39536,\"start\":39529},{\"end\":39746,\"start\":39739},{\"end\":39758,\"start\":39752},{\"end\":39771,\"start\":39765},{\"end\":39784,\"start\":39776},{\"end\":39799,\"start\":39792},{\"end\":39815,\"start\":39809},{\"end\":39860,\"start\":39856},{\"end\":39871,\"start\":39866},{\"end\":39888,\"start\":39879},{\"end\":39897,\"start\":39893},{\"end\":39909,\"start\":39904},{\"end\":39922,\"start\":39916},{\"end\":39932,\"start\":39929},{\"end\":40003,\"start\":39997},{\"end\":40013,\"start\":40009},{\"end\":40024,\"start\":40019},{\"end\":40039,\"start\":40032},{\"end\":40050,\"start\":40044},{\"end\":40063,\"start\":40058},{\"end\":40074,\"start\":40070},{\"end\":40085,\"start\":40081},{\"end\":40097,\"start\":40093},{\"end\":40118,\"start\":40111},{\"end\":40312,\"start\":40309},{\"end\":40324,\"start\":40317},{\"end\":40337,\"start\":40331},{\"end\":40535,\"start\":40530},{\"end\":40545,\"start\":40541},{\"end\":40561,\"start\":40553},{\"end\":40578,\"start\":40574},{\"end\":40911,\"start\":40907},{\"end\":40924,\"start\":40920},{\"end\":40930,\"start\":40927},{\"end\":40935,\"start\":40931},{\"end\":41237,\"start\":41232},{\"end\":41248,\"start\":41245},{\"end\":41253,\"start\":41249},{\"end\":41557,\"start\":41555},{\"end\":41571,\"start\":41564},{\"end\":41804,\"start\":41799},{\"end\":41817,\"start\":41814},{\"end\":41840,\"start\":41831},{\"end\":41856,\"start\":41849},{\"end\":41869,\"start\":41864},{\"end\":41886,\"start\":41879},{\"end\":41900,\"start\":41891},{\"end\":42331,\"start\":42324},{\"end\":42342,\"start\":42336},{\"end\":42354,\"start\":42349},{\"end\":42363,\"start\":42359},{\"end\":42376,\"start\":42369},{\"end\":42389,\"start\":42382},{\"end\":42755,\"start\":42748},{\"end\":42765,\"start\":42760},{\"end\":42776,\"start\":42770},{\"end\":42791,\"start\":42783},{\"end\":42806,\"start\":42799},{\"end\":43222,\"start\":43215},{\"end\":43231,\"start\":43227},{\"end\":43243,\"start\":43237},{\"end\":43255,\"start\":43248},{\"end\":43268,\"start\":43261},{\"end\":43688,\"start\":43684},{\"end\":43704,\"start\":43697},{\"end\":44015,\"start\":44009},{\"end\":44030,\"start\":44026},{\"end\":44042,\"start\":44038},{\"end\":44306,\"start\":44299},{\"end\":44319,\"start\":44315},{\"end\":44335,\"start\":44331},{\"end\":44345,\"start\":44340},{\"end\":44365,\"start\":44354},{\"end\":44367,\"start\":44366},{\"end\":44549,\"start\":44542},{\"end\":44571,\"start\":44564},{\"end\":44586,\"start\":44580},{\"end\":44606,\"start\":44596},{\"end\":44625,\"start\":44617},{\"end\":44643,\"start\":44635},{\"end\":44658,\"start\":44653},{\"end\":44672,\"start\":44668},{\"end\":44686,\"start\":44680},{\"end\":44840,\"start\":44833},{\"end\":44852,\"start\":44847},{\"end\":44867,\"start\":44857},{\"end\":44971,\"start\":44964},{\"end\":44996,\"start\":44988},{\"end\":45100,\"start\":45099},{\"end\":45112,\"start\":45108},{\"end\":45114,\"start\":45113},{\"end\":45451,\"start\":45447},{\"end\":45467,\"start\":45458},{\"end\":45481,\"start\":45475},{\"end\":45496,\"start\":45491},{\"end\":45507,\"start\":45503},{\"end\":45520,\"start\":45514},{\"end\":45811,\"start\":45806},{\"end\":45826,\"start\":45818},{\"end\":45837,\"start\":45832},{\"end\":45851,\"start\":45844},{\"end\":45861,\"start\":45856},{\"end\":46015,\"start\":46010},{\"end\":46027,\"start\":46022},{\"end\":46040,\"start\":46033},{\"end\":46050,\"start\":46045},{\"end\":46195,\"start\":46192},{\"end\":46206,\"start\":46200},{\"end\":46218,\"start\":46213},{\"end\":46233,\"start\":46224},{\"end\":46246,\"start\":46239},{\"end\":46409,\"start\":46403},{\"end\":46422,\"start\":46416},{\"end\":46434,\"start\":46428},{\"end\":46446,\"start\":46441},{\"end\":46462,\"start\":46458},{\"end\":46464,\"start\":46463},{\"end\":46486,\"start\":46480},{\"end\":46623,\"start\":46617},{\"end\":46641,\"start\":46634},{\"end\":46658,\"start\":46651},{\"end\":46670,\"start\":46666},{\"end\":46977,\"start\":46973},{\"end\":46991,\"start\":46984},{\"end\":47004,\"start\":46996},{\"end\":47018,\"start\":47011},{\"end\":47028,\"start\":47024},{\"end\":47039,\"start\":47036},{\"end\":47052,\"start\":47045},{\"end\":47280,\"start\":47274},{\"end\":47294,\"start\":47288},{\"end\":47309,\"start\":47304},{\"end\":47320,\"start\":47314},{\"end\":47322,\"start\":47321},{\"end\":47480,\"start\":47473},{\"end\":47492,\"start\":47487},{\"end\":47505,\"start\":47497},{\"end\":47515,\"start\":47512},{\"end\":47520,\"start\":47516},{\"end\":47534,\"start\":47526},{\"end\":47548,\"start\":47541},{\"end\":47677,\"start\":47671},{\"end\":47691,\"start\":47684},{\"end\":47701,\"start\":47696},{\"end\":47714,\"start\":47709},{\"end\":47985,\"start\":47980},{\"end\":47996,\"start\":47992},{\"end\":48005,\"start\":48001},{\"end\":48019,\"start\":48012},{\"end\":48030,\"start\":48025},{\"end\":48170,\"start\":48165},{\"end\":48185,\"start\":48177},{\"end\":48200,\"start\":48193},{\"end\":48209,\"start\":48206},{\"end\":48326,\"start\":48321},{\"end\":48338,\"start\":48333},{\"end\":48350,\"start\":48343},{\"end\":48366,\"start\":48358},{\"end\":48378,\"start\":48372}]", "bib_author_last_name": "[{\"end\":34026,\"start\":34020},{\"end\":34044,\"start\":34035},{\"end\":34058,\"start\":34052},{\"end\":34345,\"start\":34340},{\"end\":34360,\"start\":34356},{\"end\":34372,\"start\":34367},{\"end\":34389,\"start\":34382},{\"end\":34405,\"start\":34399},{\"end\":34424,\"start\":34416},{\"end\":34444,\"start\":34433},{\"end\":34458,\"start\":34453},{\"end\":34473,\"start\":34467},{\"end\":34488,\"start\":34482},{\"end\":34612,\"start\":34609},{\"end\":34624,\"start\":34622},{\"end\":34634,\"start\":34632},{\"end\":34646,\"start\":34641},{\"end\":34838,\"start\":34833},{\"end\":34857,\"start\":34848},{\"end\":35145,\"start\":35141},{\"end\":35157,\"start\":35154},{\"end\":35258,\"start\":35254},{\"end\":35272,\"start\":35268},{\"end\":35282,\"start\":35279},{\"end\":35298,\"start\":35294},{\"end\":35311,\"start\":35307},{\"end\":35324,\"start\":35322},{\"end\":35431,\"start\":35423},{\"end\":35443,\"start\":35439},{\"end\":35458,\"start\":35453},{\"end\":35474,\"start\":35468},{\"end\":35483,\"start\":35481},{\"end\":35490,\"start\":35485},{\"end\":35650,\"start\":35646},{\"end\":35664,\"start\":35661},{\"end\":35677,\"start\":35675},{\"end\":35687,\"start\":35684},{\"end\":35847,\"start\":35843},{\"end\":35861,\"start\":35858},{\"end\":35870,\"start\":35867},{\"end\":35885,\"start\":35882},{\"end\":35900,\"start\":35895},{\"end\":36038,\"start\":36036},{\"end\":36051,\"start\":36047},{\"end\":36061,\"start\":36059},{\"end\":36075,\"start\":36072},{\"end\":36088,\"start\":36084},{\"end\":36352,\"start\":36349},{\"end\":36362,\"start\":36360},{\"end\":36377,\"start\":36372},{\"end\":36388,\"start\":36386},{\"end\":36401,\"start\":36397},{\"end\":36409,\"start\":36407},{\"end\":36695,\"start\":36692},{\"end\":36712,\"start\":36704},{\"end\":36723,\"start\":36718},{\"end\":36741,\"start\":36734},{\"end\":36755,\"start\":36750},{\"end\":36771,\"start\":36765},{\"end\":36784,\"start\":36779},{\"end\":36795,\"start\":36793},{\"end\":36808,\"start\":36803},{\"end\":36823,\"start\":36814},{\"end\":36998,\"start\":36995},{\"end\":37010,\"start\":37005},{\"end\":37022,\"start\":37018},{\"end\":37445,\"start\":37443},{\"end\":37467,\"start\":37455},{\"end\":37483,\"start\":37479},{\"end\":37608,\"start\":37606},{\"end\":37619,\"start\":37615},{\"end\":37633,\"start\":37628},{\"end\":37647,\"start\":37642},{\"end\":37661,\"start\":37656},{\"end\":37674,\"start\":37672},{\"end\":37683,\"start\":37680},{\"end\":37930,\"start\":37924},{\"end\":37946,\"start\":37941},{\"end\":37969,\"start\":37961},{\"end\":38057,\"start\":38051},{\"end\":38070,\"start\":38064},{\"end\":38085,\"start\":38079},{\"end\":38324,\"start\":38318},{\"end\":38338,\"start\":38331},{\"end\":38353,\"start\":38345},{\"end\":38599,\"start\":38597},{\"end\":38611,\"start\":38607},{\"end\":38623,\"start\":38621},{\"end\":38637,\"start\":38633},{\"end\":38649,\"start\":38647},{\"end\":38884,\"start\":38882},{\"end\":38895,\"start\":38891},{\"end\":38912,\"start\":38906},{\"end\":38925,\"start\":38921},{\"end\":38930,\"start\":38927},{\"end\":39038,\"start\":39033},{\"end\":39048,\"start\":39046},{\"end\":39055,\"start\":39050},{\"end\":39487,\"start\":39484},{\"end\":39498,\"start\":39495},{\"end\":39516,\"start\":39509},{\"end\":39527,\"start\":39522},{\"end\":39542,\"start\":39537},{\"end\":39750,\"start\":39747},{\"end\":39763,\"start\":39759},{\"end\":39774,\"start\":39772},{\"end\":39790,\"start\":39785},{\"end\":39807,\"start\":39800},{\"end\":39822,\"start\":39816},{\"end\":39864,\"start\":39861},{\"end\":39877,\"start\":39872},{\"end\":39891,\"start\":39889},{\"end\":39902,\"start\":39898},{\"end\":39914,\"start\":39910},{\"end\":39927,\"start\":39923},{\"end\":39937,\"start\":39933},{\"end\":40007,\"start\":40004},{\"end\":40017,\"start\":40014},{\"end\":40030,\"start\":40025},{\"end\":40042,\"start\":40040},{\"end\":40056,\"start\":40051},{\"end\":40068,\"start\":40064},{\"end\":40079,\"start\":40075},{\"end\":40091,\"start\":40086},{\"end\":40109,\"start\":40098},{\"end\":40127,\"start\":40119},{\"end\":40315,\"start\":40313},{\"end\":40329,\"start\":40325},{\"end\":40340,\"start\":40338},{\"end\":40539,\"start\":40536},{\"end\":40551,\"start\":40546},{\"end\":40572,\"start\":40562},{\"end\":40590,\"start\":40579},{\"end\":40918,\"start\":40912},{\"end\":40939,\"start\":40936},{\"end\":41243,\"start\":41238},{\"end\":41260,\"start\":41254},{\"end\":41265,\"start\":41262},{\"end\":41562,\"start\":41558},{\"end\":41575,\"start\":41572},{\"end\":41812,\"start\":41805},{\"end\":41829,\"start\":41818},{\"end\":41847,\"start\":41841},{\"end\":41862,\"start\":41857},{\"end\":41877,\"start\":41870},{\"end\":41889,\"start\":41887},{\"end\":41907,\"start\":41901},{\"end\":42334,\"start\":42332},{\"end\":42347,\"start\":42343},{\"end\":42357,\"start\":42355},{\"end\":42367,\"start\":42364},{\"end\":42380,\"start\":42377},{\"end\":42393,\"start\":42390},{\"end\":42758,\"start\":42756},{\"end\":42768,\"start\":42766},{\"end\":42781,\"start\":42777},{\"end\":42797,\"start\":42792},{\"end\":42810,\"start\":42807},{\"end\":43225,\"start\":43223},{\"end\":43235,\"start\":43232},{\"end\":43246,\"start\":43244},{\"end\":43259,\"start\":43256},{\"end\":43272,\"start\":43269},{\"end\":43695,\"start\":43689},{\"end\":43712,\"start\":43705},{\"end\":44024,\"start\":44016},{\"end\":44036,\"start\":44031},{\"end\":44051,\"start\":44043},{\"end\":44313,\"start\":44307},{\"end\":44329,\"start\":44320},{\"end\":44338,\"start\":44336},{\"end\":44352,\"start\":44346},{\"end\":44375,\"start\":44368},{\"end\":44562,\"start\":44550},{\"end\":44578,\"start\":44572},{\"end\":44594,\"start\":44587},{\"end\":44615,\"start\":44607},{\"end\":44633,\"start\":44626},{\"end\":44651,\"start\":44644},{\"end\":44666,\"start\":44659},{\"end\":44678,\"start\":44673},{\"end\":44693,\"start\":44687},{\"end\":44700,\"start\":44695},{\"end\":44845,\"start\":44841},{\"end\":44855,\"start\":44853},{\"end\":44873,\"start\":44868},{\"end\":44986,\"start\":44972},{\"end\":45003,\"start\":44997},{\"end\":45106,\"start\":45101},{\"end\":45123,\"start\":45115},{\"end\":45129,\"start\":45125},{\"end\":45456,\"start\":45452},{\"end\":45473,\"start\":45468},{\"end\":45489,\"start\":45482},{\"end\":45501,\"start\":45497},{\"end\":45512,\"start\":45508},{\"end\":45527,\"start\":45521},{\"end\":45816,\"start\":45812},{\"end\":45830,\"start\":45827},{\"end\":45842,\"start\":45838},{\"end\":45854,\"start\":45852},{\"end\":45871,\"start\":45862},{\"end\":46020,\"start\":46016},{\"end\":46031,\"start\":46028},{\"end\":46043,\"start\":46041},{\"end\":46057,\"start\":46051},{\"end\":46198,\"start\":46196},{\"end\":46211,\"start\":46207},{\"end\":46222,\"start\":46219},{\"end\":46237,\"start\":46234},{\"end\":46250,\"start\":46247},{\"end\":46414,\"start\":46410},{\"end\":46426,\"start\":46423},{\"end\":46439,\"start\":46435},{\"end\":46456,\"start\":46447},{\"end\":46478,\"start\":46465},{\"end\":46489,\"start\":46487},{\"end\":46632,\"start\":46624},{\"end\":46649,\"start\":46642},{\"end\":46664,\"start\":46659},{\"end\":46680,\"start\":46671},{\"end\":46982,\"start\":46978},{\"end\":46994,\"start\":46992},{\"end\":47009,\"start\":47005},{\"end\":47022,\"start\":47019},{\"end\":47034,\"start\":47029},{\"end\":47043,\"start\":47040},{\"end\":47056,\"start\":47053},{\"end\":47286,\"start\":47281},{\"end\":47302,\"start\":47295},{\"end\":47312,\"start\":47310},{\"end\":47333,\"start\":47323},{\"end\":47485,\"start\":47481},{\"end\":47495,\"start\":47493},{\"end\":47510,\"start\":47506},{\"end\":47524,\"start\":47521},{\"end\":47539,\"start\":47535},{\"end\":47554,\"start\":47549},{\"end\":47682,\"start\":47678},{\"end\":47694,\"start\":47692},{\"end\":47707,\"start\":47702},{\"end\":47721,\"start\":47715},{\"end\":47990,\"start\":47986},{\"end\":47999,\"start\":47997},{\"end\":48010,\"start\":48006},{\"end\":48023,\"start\":48020},{\"end\":48034,\"start\":48031},{\"end\":48175,\"start\":48171},{\"end\":48191,\"start\":48186},{\"end\":48204,\"start\":48201},{\"end\":48214,\"start\":48210},{\"end\":48331,\"start\":48327},{\"end\":48341,\"start\":48339},{\"end\":48356,\"start\":48351},{\"end\":48370,\"start\":48367},{\"end\":48383,\"start\":48379}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":248780078},\"end\":34295,\"start\":33955},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218971783},\"end\":34547,\"start\":34297},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":254043765},\"end\":34705,\"start\":34549},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":237420912},\"end\":35022,\"start\":34707},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":220713444},\"end\":35182,\"start\":35024},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":249395608},\"end\":35571,\"start\":35184},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":231798234},\"end\":35747,\"start\":35573},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":245116938},\"end\":35959,\"start\":35749},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":250629290},\"end\":36279,\"start\":35961},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":244117423},\"end\":36686,\"start\":36281},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b10\"},\"end\":36925,\"start\":36688},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":229923710},\"end\":37353,\"start\":36927},{\"attributes\":{\"doi\":\"arXiv:1708.06733\",\"id\":\"b12\"},\"end\":37519,\"start\":37355},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":250243701},\"end\":37808,\"start\":37521},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52967399},\"end\":37996,\"start\":37810},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":215754328},\"end\":38252,\"start\":37998},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":233296808},\"end\":38518,\"start\":38254},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":237365058},\"end\":38814,\"start\":38520},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":251142481},\"end\":38965,\"start\":38816},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":230433941},\"end\":39386,\"start\":38967},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":248693283},\"end\":39633,\"start\":39388},{\"attributes\":{\"id\":\"b21\"},\"end\":39854,\"start\":39635},{\"attributes\":{\"doi\":\"arXiv:2103.10385\",\"id\":\"b22\"},\"end\":39995,\"start\":39856},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b23\"},\"end\":40220,\"start\":39997},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":237485305},\"end\":40455,\"start\":40222},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":236956577},\"end\":40827,\"start\":40457},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":253384392},\"end\":41104,\"start\":40829},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":250297995},\"end\":41382,\"start\":41106},{\"attributes\":{\"doi\":\"arXiv:2303.08774\",\"id\":\"b28\"},\"end\":41448,\"start\":41384},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3264224},\"end\":41760,\"start\":41450},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202539551},\"end\":42250,\"start\":41762},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.752\",\"id\":\"b31\",\"matched_paper_id\":227118606},\"end\":42672,\"start\":42252},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":235196099},\"end\":43128,\"start\":42674},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":235417102},\"end\":43590,\"start\":43130},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":210838924},\"end\":43945,\"start\":43592},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":239016577},\"end\":44218,\"start\":43947},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":990233},\"end\":44540,\"start\":44220},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b37\"},\"end\":44790,\"start\":44542},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":53298804},\"end\":44932,\"start\":44792},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":5855042},\"end\":45050,\"start\":44934},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":11465263},\"end\":45358,\"start\":45052},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5034059},\"end\":45724,\"start\":45360},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":67846878},\"end\":45931,\"start\":45726},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":211548864},\"end\":46117,\"start\":45933},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":248085108},\"end\":46327,\"start\":46119},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":195069387},\"end\":46548,\"start\":46329},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":67856299},\"end\":46899,\"start\":46550},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":261432085},\"end\":47223,\"start\":46901},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":127986044},\"end\":47393,\"start\":47225},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":249062932},\"end\":47611,\"start\":47395},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":212628208},\"end\":47876,\"start\":47613},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":258639810},\"end\":48116,\"start\":47878},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":252415407},\"end\":48267,\"start\":48118},{\"attributes\":{\"doi\":\"arXiv:2303.14325\",\"id\":\"b53\"},\"end\":48427,\"start\":48269}]", "bib_title": "[{\"end\":34012,\"start\":33955},{\"end\":34334,\"start\":34297},{\"end\":34598,\"start\":34549},{\"end\":34824,\"start\":34707},{\"end\":35128,\"start\":35024},{\"end\":35245,\"start\":35184},{\"end\":35635,\"start\":35573},{\"end\":35832,\"start\":35749},{\"end\":36030,\"start\":35961},{\"end\":36340,\"start\":36281},{\"end\":36986,\"start\":36927},{\"end\":37594,\"start\":37521},{\"end\":37890,\"start\":37810},{\"end\":38043,\"start\":37998},{\"end\":38310,\"start\":38254},{\"end\":38587,\"start\":38520},{\"end\":38871,\"start\":38816},{\"end\":39026,\"start\":38967},{\"end\":39475,\"start\":39388},{\"end\":39737,\"start\":39635},{\"end\":40307,\"start\":40222},{\"end\":40528,\"start\":40457},{\"end\":40905,\"start\":40829},{\"end\":41230,\"start\":41106},{\"end\":41553,\"start\":41450},{\"end\":41797,\"start\":41762},{\"end\":42322,\"start\":42252},{\"end\":42746,\"start\":42674},{\"end\":43213,\"start\":43130},{\"end\":43682,\"start\":43592},{\"end\":44007,\"start\":43947},{\"end\":44297,\"start\":44220},{\"end\":44831,\"start\":44792},{\"end\":44962,\"start\":44934},{\"end\":45097,\"start\":45052},{\"end\":45445,\"start\":45360},{\"end\":45804,\"start\":45726},{\"end\":46008,\"start\":45933},{\"end\":46190,\"start\":46119},{\"end\":46401,\"start\":46329},{\"end\":46615,\"start\":46550},{\"end\":46971,\"start\":46901},{\"end\":47272,\"start\":47225},{\"end\":47471,\"start\":47395},{\"end\":47669,\"start\":47613},{\"end\":47978,\"start\":47878},{\"end\":48163,\"start\":48118}]", "bib_author": "[{\"end\":34028,\"start\":34014},{\"end\":34046,\"start\":34028},{\"end\":34060,\"start\":34046},{\"end\":34347,\"start\":34336},{\"end\":34362,\"start\":34347},{\"end\":34374,\"start\":34362},{\"end\":34391,\"start\":34374},{\"end\":34407,\"start\":34391},{\"end\":34426,\"start\":34407},{\"end\":34446,\"start\":34426},{\"end\":34460,\"start\":34446},{\"end\":34475,\"start\":34460},{\"end\":34490,\"start\":34475},{\"end\":34614,\"start\":34600},{\"end\":34626,\"start\":34614},{\"end\":34636,\"start\":34626},{\"end\":34648,\"start\":34636},{\"end\":34840,\"start\":34826},{\"end\":34859,\"start\":34840},{\"end\":35147,\"start\":35130},{\"end\":35159,\"start\":35147},{\"end\":35260,\"start\":35247},{\"end\":35274,\"start\":35260},{\"end\":35284,\"start\":35274},{\"end\":35300,\"start\":35284},{\"end\":35313,\"start\":35300},{\"end\":35326,\"start\":35313},{\"end\":35652,\"start\":35637},{\"end\":35666,\"start\":35652},{\"end\":35679,\"start\":35666},{\"end\":35689,\"start\":35679},{\"end\":35849,\"start\":35834},{\"end\":35863,\"start\":35849},{\"end\":35872,\"start\":35863},{\"end\":35887,\"start\":35872},{\"end\":35902,\"start\":35887},{\"end\":36040,\"start\":36032},{\"end\":36053,\"start\":36040},{\"end\":36063,\"start\":36053},{\"end\":36077,\"start\":36063},{\"end\":36090,\"start\":36077},{\"end\":36354,\"start\":36342},{\"end\":36364,\"start\":36354},{\"end\":36379,\"start\":36364},{\"end\":36390,\"start\":36379},{\"end\":36403,\"start\":36390},{\"end\":36411,\"start\":36403},{\"end\":36697,\"start\":36688},{\"end\":36714,\"start\":36697},{\"end\":36725,\"start\":36714},{\"end\":36743,\"start\":36725},{\"end\":36757,\"start\":36743},{\"end\":36773,\"start\":36757},{\"end\":36786,\"start\":36773},{\"end\":36797,\"start\":36786},{\"end\":36810,\"start\":36797},{\"end\":36825,\"start\":36810},{\"end\":37000,\"start\":36988},{\"end\":37012,\"start\":37000},{\"end\":37024,\"start\":37012},{\"end\":37447,\"start\":37436},{\"end\":37469,\"start\":37447},{\"end\":37485,\"start\":37469},{\"end\":37610,\"start\":37596},{\"end\":37621,\"start\":37610},{\"end\":37635,\"start\":37621},{\"end\":37649,\"start\":37635},{\"end\":37663,\"start\":37649},{\"end\":37676,\"start\":37663},{\"end\":37685,\"start\":37676},{\"end\":38059,\"start\":38045},{\"end\":38072,\"start\":38059},{\"end\":38087,\"start\":38072},{\"end\":38326,\"start\":38312},{\"end\":38340,\"start\":38326},{\"end\":38355,\"start\":38340},{\"end\":38601,\"start\":38589},{\"end\":38613,\"start\":38601},{\"end\":38625,\"start\":38613},{\"end\":38639,\"start\":38625},{\"end\":38651,\"start\":38639},{\"end\":38886,\"start\":38873},{\"end\":38897,\"start\":38886},{\"end\":38914,\"start\":38897},{\"end\":38927,\"start\":38914},{\"end\":38932,\"start\":38927},{\"end\":39040,\"start\":39028},{\"end\":39050,\"start\":39040},{\"end\":39057,\"start\":39050},{\"end\":39489,\"start\":39477},{\"end\":39500,\"start\":39489},{\"end\":39518,\"start\":39500},{\"end\":39529,\"start\":39518},{\"end\":39544,\"start\":39529},{\"end\":39752,\"start\":39739},{\"end\":39765,\"start\":39752},{\"end\":39776,\"start\":39765},{\"end\":39792,\"start\":39776},{\"end\":39809,\"start\":39792},{\"end\":39824,\"start\":39809},{\"end\":39866,\"start\":39856},{\"end\":39879,\"start\":39866},{\"end\":39893,\"start\":39879},{\"end\":39904,\"start\":39893},{\"end\":39916,\"start\":39904},{\"end\":39929,\"start\":39916},{\"end\":39939,\"start\":39929},{\"end\":40009,\"start\":39997},{\"end\":40019,\"start\":40009},{\"end\":40032,\"start\":40019},{\"end\":40044,\"start\":40032},{\"end\":40058,\"start\":40044},{\"end\":40070,\"start\":40058},{\"end\":40081,\"start\":40070},{\"end\":40093,\"start\":40081},{\"end\":40111,\"start\":40093},{\"end\":40129,\"start\":40111},{\"end\":40317,\"start\":40309},{\"end\":40331,\"start\":40317},{\"end\":40342,\"start\":40331},{\"end\":40541,\"start\":40530},{\"end\":40553,\"start\":40541},{\"end\":40574,\"start\":40553},{\"end\":40592,\"start\":40574},{\"end\":40920,\"start\":40907},{\"end\":40927,\"start\":40920},{\"end\":40941,\"start\":40927},{\"end\":41245,\"start\":41232},{\"end\":41262,\"start\":41245},{\"end\":41267,\"start\":41262},{\"end\":41564,\"start\":41555},{\"end\":41577,\"start\":41564},{\"end\":41814,\"start\":41799},{\"end\":41831,\"start\":41814},{\"end\":41849,\"start\":41831},{\"end\":41864,\"start\":41849},{\"end\":41879,\"start\":41864},{\"end\":41891,\"start\":41879},{\"end\":41909,\"start\":41891},{\"end\":42336,\"start\":42324},{\"end\":42349,\"start\":42336},{\"end\":42359,\"start\":42349},{\"end\":42369,\"start\":42359},{\"end\":42382,\"start\":42369},{\"end\":42395,\"start\":42382},{\"end\":42760,\"start\":42748},{\"end\":42770,\"start\":42760},{\"end\":42783,\"start\":42770},{\"end\":42799,\"start\":42783},{\"end\":42812,\"start\":42799},{\"end\":43227,\"start\":43215},{\"end\":43237,\"start\":43227},{\"end\":43248,\"start\":43237},{\"end\":43261,\"start\":43248},{\"end\":43274,\"start\":43261},{\"end\":43697,\"start\":43684},{\"end\":43714,\"start\":43697},{\"end\":44026,\"start\":44009},{\"end\":44038,\"start\":44026},{\"end\":44053,\"start\":44038},{\"end\":44315,\"start\":44299},{\"end\":44331,\"start\":44315},{\"end\":44340,\"start\":44331},{\"end\":44354,\"start\":44340},{\"end\":44377,\"start\":44354},{\"end\":44564,\"start\":44542},{\"end\":44580,\"start\":44564},{\"end\":44596,\"start\":44580},{\"end\":44617,\"start\":44596},{\"end\":44635,\"start\":44617},{\"end\":44653,\"start\":44635},{\"end\":44668,\"start\":44653},{\"end\":44680,\"start\":44668},{\"end\":44695,\"start\":44680},{\"end\":44702,\"start\":44695},{\"end\":44847,\"start\":44833},{\"end\":44857,\"start\":44847},{\"end\":44875,\"start\":44857},{\"end\":44988,\"start\":44964},{\"end\":45005,\"start\":44988},{\"end\":45108,\"start\":45099},{\"end\":45125,\"start\":45108},{\"end\":45131,\"start\":45125},{\"end\":45458,\"start\":45447},{\"end\":45475,\"start\":45458},{\"end\":45491,\"start\":45475},{\"end\":45503,\"start\":45491},{\"end\":45514,\"start\":45503},{\"end\":45529,\"start\":45514},{\"end\":45818,\"start\":45806},{\"end\":45832,\"start\":45818},{\"end\":45844,\"start\":45832},{\"end\":45856,\"start\":45844},{\"end\":45873,\"start\":45856},{\"end\":46022,\"start\":46010},{\"end\":46033,\"start\":46022},{\"end\":46045,\"start\":46033},{\"end\":46059,\"start\":46045},{\"end\":46200,\"start\":46192},{\"end\":46213,\"start\":46200},{\"end\":46224,\"start\":46213},{\"end\":46239,\"start\":46224},{\"end\":46252,\"start\":46239},{\"end\":46416,\"start\":46403},{\"end\":46428,\"start\":46416},{\"end\":46441,\"start\":46428},{\"end\":46458,\"start\":46441},{\"end\":46480,\"start\":46458},{\"end\":46491,\"start\":46480},{\"end\":46634,\"start\":46617},{\"end\":46651,\"start\":46634},{\"end\":46666,\"start\":46651},{\"end\":46682,\"start\":46666},{\"end\":46984,\"start\":46973},{\"end\":46996,\"start\":46984},{\"end\":47011,\"start\":46996},{\"end\":47024,\"start\":47011},{\"end\":47036,\"start\":47024},{\"end\":47045,\"start\":47036},{\"end\":47058,\"start\":47045},{\"end\":47288,\"start\":47274},{\"end\":47304,\"start\":47288},{\"end\":47314,\"start\":47304},{\"end\":47335,\"start\":47314},{\"end\":47487,\"start\":47473},{\"end\":47497,\"start\":47487},{\"end\":47512,\"start\":47497},{\"end\":47526,\"start\":47512},{\"end\":47541,\"start\":47526},{\"end\":47556,\"start\":47541},{\"end\":47684,\"start\":47671},{\"end\":47696,\"start\":47684},{\"end\":47709,\"start\":47696},{\"end\":47723,\"start\":47709},{\"end\":47992,\"start\":47980},{\"end\":48001,\"start\":47992},{\"end\":48012,\"start\":48001},{\"end\":48025,\"start\":48012},{\"end\":48036,\"start\":48025},{\"end\":48177,\"start\":48165},{\"end\":48193,\"start\":48177},{\"end\":48206,\"start\":48193},{\"end\":48216,\"start\":48206},{\"end\":48333,\"start\":48321},{\"end\":48343,\"start\":48333},{\"end\":48358,\"start\":48343},{\"end\":48372,\"start\":48358},{\"end\":48385,\"start\":48372}]", "bib_venue": "[{\"end\":34249,\"start\":34162},{\"end\":35018,\"start\":34947},{\"end\":35511,\"start\":35492},{\"end\":36275,\"start\":36191},{\"end\":36682,\"start\":36555},{\"end\":37348,\"start\":37201},{\"end\":37804,\"start\":37753},{\"end\":37992,\"start\":37983},{\"end\":38248,\"start\":38176},{\"end\":38514,\"start\":38443},{\"end\":38810,\"start\":38739},{\"end\":39381,\"start\":39234},{\"end\":40451,\"start\":40405},{\"end\":40781,\"start\":40694},{\"end\":41100,\"start\":41029},{\"end\":41376,\"start\":41330},{\"end\":41756,\"start\":41675},{\"end\":42246,\"start\":42086},{\"end\":42585,\"start\":42514},{\"end\":43123,\"start\":42976},{\"end\":43585,\"start\":43438},{\"end\":43941,\"start\":43836},{\"end\":44214,\"start\":44142},{\"end\":44536,\"start\":44465},{\"end\":45354,\"start\":45251},{\"end\":45720,\"start\":45633},{\"end\":46895,\"start\":46797},{\"end\":47219,\"start\":47147},{\"end\":47872,\"start\":47806},{\"end\":34147,\"start\":34060},{\"end\":34160,\"start\":34149},{\"end\":34539,\"start\":34490},{\"end\":34697,\"start\":34648},{\"end\":34945,\"start\":34859},{\"end\":35173,\"start\":35159},{\"end\":35414,\"start\":35326},{\"end\":35741,\"start\":35689},{\"end\":35951,\"start\":35902},{\"end\":36189,\"start\":36090},{\"end\":36553,\"start\":36411},{\"end\":36905,\"start\":36841},{\"end\":37186,\"start\":37024},{\"end\":37199,\"start\":37188},{\"end\":37434,\"start\":37355},{\"end\":37751,\"start\":37685},{\"end\":37916,\"start\":37892},{\"end\":38174,\"start\":38087},{\"end\":38441,\"start\":38355},{\"end\":38737,\"start\":38651},{\"end\":38955,\"start\":38932},{\"end\":39219,\"start\":39057},{\"end\":39232,\"start\":39221},{\"end\":39593,\"start\":39544},{\"end\":39845,\"start\":39824},{\"end\":39975,\"start\":39955},{\"end\":40200,\"start\":40145},{\"end\":40403,\"start\":40342},{\"end\":40679,\"start\":40592},{\"end\":40692,\"start\":40681},{\"end\":41027,\"start\":40941},{\"end\":41328,\"start\":41267},{\"end\":41422,\"start\":41400},{\"end\":41673,\"start\":41577},{\"end\":42084,\"start\":41909},{\"end\":42512,\"start\":42426},{\"end\":42974,\"start\":42812},{\"end\":43436,\"start\":43274},{\"end\":43834,\"start\":43714},{\"end\":44140,\"start\":44053},{\"end\":44463,\"start\":44377},{\"end\":44770,\"start\":44718},{\"end\":44924,\"start\":44875},{\"end\":45041,\"start\":45005},{\"end\":45249,\"start\":45131},{\"end\":45631,\"start\":45529},{\"end\":45921,\"start\":45873},{\"end\":46111,\"start\":46059},{\"end\":46321,\"start\":46252},{\"end\":46540,\"start\":46491},{\"end\":46795,\"start\":46682},{\"end\":47145,\"start\":47058},{\"end\":47387,\"start\":47335},{\"end\":47600,\"start\":47556},{\"end\":47804,\"start\":47723},{\"end\":48110,\"start\":48036},{\"end\":48260,\"start\":48216},{\"end\":48319,\"start\":48269}]"}}}, "year": 2023, "month": 12, "day": 17}