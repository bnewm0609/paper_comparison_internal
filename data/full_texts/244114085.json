{"id": 244114085, "updated": "2023-10-06 06:03:09.973", "metadata": {"title": "Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Complete and Incomplete Neural Network Robustness Verification", "authors": "[{\"first\":\"Shiqi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Huan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Kaidi\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Xue\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Suman\",\"last\":\"Jana\",\"middle\":[]},{\"first\":\"Cho-Jui\",\"last\":\"Hsieh\",\"middle\":[]},{\"first\":\"J.\",\"last\":\"Kolter\",\"middle\":[\"Zico\"]}]", "venue": "NeurIPS", "journal": "29909-29921", "publication_date": {"year": 2021, "month": 3, "day": 11}, "abstract": "Bound propagation based incomplete neural network verifiers such as CROWN are very efficient and can significantly accelerate branch-and-bound (BaB) based complete verification of neural networks. However, bound propagation cannot fully handle the neuron split constraints introduced by BaB commonly handled by expensive linear programming (LP) solvers, leading to loose bounds and hurting verification efficiency. In this work, we develop $\\beta$-CROWN, a new bound propagation based method that can fully encode neuron splits via optimizable parameters $\\beta$ constructed from either primal or dual space. When jointly optimized in intermediate layers, $\\beta$-CROWN generally produces better bounds than typical LP verifiers with neuron split constraints, while being as efficient and parallelizable as CROWN on GPUs. Applied to complete robustness verification benchmarks, $\\beta$-CROWN with BaB is up to three orders of magnitude faster than LP-based BaB methods, and is notably faster than all existing approaches while producing lower timeout rates. By terminating BaB early, our method can also be used for efficient incomplete verification. We consistently achieve higher verified accuracy in many settings compared to powerful incomplete verifiers, including those based on convex barrier breaking techniques. Compared to the typically tightest but very costly semidefinite programming (SDP) based incomplete verifiers, we obtain higher verified accuracy with three orders of magnitudes less verification time. Our algorithm empowered the $\\alpha,\\!\\beta$-CROWN (alpha-beta-CROWN) verifier, the winning tool in VNN-COMP 2021. Our code is available at http://PaperCode.cc/BetaCROWN", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2103.06624", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/WangZXLJHK21", "doi": null}}, "content": {"source": {"pdf_hash": "1bfc6f9c9db5c6646f0f3e0213d407ae14d8f7bf", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.06624v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a681961a1fbaa0b23fa2d88eb5ab1906555ba825", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1bfc6f9c9db5c6646f0f3e0213d407ae14d8f7bf.txt", "contents": "\nBeta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Neural Network Robustness Verification\n\n\nShiqi Wang \nColumbia University\n\n\nEqual Contribution\n\n\nHuan Zhang \nCMU\n\n\nEqual Contribution\n\n\nKaidi Xu \nNortheastern University\n\n\nEqual Contribution\n\n\nXue Lin xue.lin@northeastern.edu \nNortheastern University\n\n\nSuman Jana \nColumbia University\n\n\nCho-Jui Hsieh chohsieh@cs.ucla.edu \nUCLA\n\n\nZico Kolter zkolter@cs.cmu.edu \nCMU\n\n\nBeta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Neural Network Robustness Verification\n\nBound propagation based incomplete neural network verifiers such as CROWN are very efficient and can significantly accelerate branch-and-bound (BaB) based complete verification of neural networks. However, bound propagation cannot fully handle the neuron split constraints introduced by BaB commonly handled by expensive linear programming (LP) solvers, leading to loose bounds and hurting verification efficiency. In this work, we develop \u03b2-CROWN, a new bound propagation based method that can fully encode neuron splits via optimizable parameters \u03b2 constructed from either primal or dual space. When jointly optimized in intermediate layers, \u03b2-CROWN generally produces better bounds than typical LP verifiers with neuron split constraints, while being as efficient and parallelizable as CROWN on GPUs. Applied to complete robustness verification benchmarks, \u03b2-CROWN with BaB is up to three orders of magnitude faster than LP-based BaB methods, and is notably faster than all existing approaches while producing lower timeout rates. By terminating BaB early, our method can also be used for efficient incomplete verification. We consistently achieve higher verified accuracy in many settings compared to powerful incomplete verifiers, including those based on convex barrier breaking techniques. Compared to the typically tightest but very costly semidefinite programming (SDP) based incomplete verifiers, we obtain higher verified accuracy with three orders of magnitudes less verification time. Our algorithm empowered the \u03b1,\u03b2-CROWN (alpha-beta-CROWN) verifier, the winning tool in VNN-COMP 2021. Our code is available at http://PaperCode.cc/BetaCROWN.\n\nIntroduction\n\nAs neural networks (NNs) are being deployed in safety-critical applications, it becomes increasingly important to formally verify their behaviors under potentially malicious inputs. Broadly speaking, the neural network verification problem involves proving certain desired relationships between inputs and outputs (often referred to as specifications), such as safety or robustness guarantees, for all inputs inside some domain. Canonically, the problem can be cast as finding the global minima of some functions on the network's outputs (e.g., the difference between the predictions of the true label and another target label), within a bounded input set as constraints. This is a challenging problem due to the non-convexity and high dimensionality of neural networks.\n\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\n\nWe first focus on complete verification: the verifier should give a definite \"yes/no\" answer given sufficient time. Many complete verifiers rely on the branch and bound (BaB) method [8] involving (1) branching by recursively splitting the original verification problem into subdomains (e.g., splitting a ReLU neuron into positive/negative linear regions by adding split constraints) and (2) bounding each subdomain with specialized incomplete verifiers. Traditional BaB-based verifiers use expensive linear programming (LP) solvers [15,23,7] as incomplete verifiers which can fully encode neuron split constraints. Meanwhile, a recent verifier, Fast-and-Complete [45], demonstrates that cheap incomplete verifiers can significantly accelerate complete verification on GPUs over LP-based ones thanks to their efficiency. Many cheap incomplete verifiers are based on bound propagation methods [46,42,41,13,17,36,44], i.e., maintaining and propagating tractable and sound bounds through networks, and CROWN [46] is a representative which propagates a linear or quadratic bound.\n\nHowever, unlike LP based verifiers, existing bound propagation methods lack the power to handle neuron split constraints introduced by BaB. For instance, given inputs x, y \u2208 [\u22121, 1], they can bound a ReLU's input x + y as [\u22122, 2] but they have no means to consider neuron split constraints such as x \u2212 y \u2265 0 introduced by splitting another ReLU to the positive linear region. Such a problem causes looser bounds and unnecessary branching, hurting the verification efficiency. Even worse, without considering these split constraints, bound propagation methods cannot detect many infeasible subdomains in BaB [45], leading to incompleteness unless costly checking is performed.\n\nIn our work, we develop a new, fast bound propagation based incomplete verifier, \u03b2-CROWN. It solves an optimization problem equivalent to the expensive LP based methods with neuron split constraints while still enjoying the efficiency of bound propagation methods. \u03b2-CROWN contains optimizable parameters \u03b2 which come from propagation of Lagrangian multipliers, and any valid settings of these parameters yield sound bounds for verification. These parameters are optimized using a few steps of (super)gradient ascent to achieve bounds as tight as possible. Optimizing \u03b2 can also eliminate many infeasible subdomains and avoid further useless branching. Furthermore, we can jointly optimize intermediate layer bounds similar to [44] but also with the additional parameters \u03b2, allowing \u03b2-CROWN to tighten relaxations and outperform typical LP verifiers with fixed intermediate layer bounds. Unlike traditional LP-based BaB methods, \u03b2-CROWN can be efficiently implemented with an automatic differentiation framework on GPUs to fully exploit the power of modern accelerators. The combination of \u03b2-CROWN and BaB (\u03b2-CROWN BaB) produces a complete verifier with GPU acceleration, reducing the verification time of traditional LP based BaB verifiers [8] by up to three orders of magnitudes on a commonly used benchmark suite on CIFAR-10 [6,10]. Compared to all state-of-the-art GPU-based complete verifiers [7,45,10,23,6,11], our approach is noticeably faster with lower timeout rates. Our algorithm empowered the tool \u03b1,\u03b2-CROWN (alpha-beta-CROWN), which won the 2nd International Verification of Neural Networks Competition [3] (VNN-COMP 2021) with the highest total score and verified the most number of problem instances in 8 benchmarks.\n\nFinally, by terminating our complete verifier \u03b2-CROWN BaB early, our approach can also function as a more accurate incomplete verifier by returning an incomplete but sound lower bound of all subdomains explored so far. We achieve better verified accuracy on a few benchmarking models over powerful incomplete verifiers including those based on tight linear relaxations [35,37,26] and semidefinite relaxations [9]. Compared to the typically tightest but very costly incomplete verifier SDP-FO [9] based on the semidefinite programming (SDP) relaxations [28,14], our method obtains consistently higher verified accuracy while reducing verification time by three orders of magnitudes.\n\n\nBackground\n\n\nThe neural network verification problem and its LP relaxation\n\nWe define the input of a neural network as x \u2208 R d0 , and define the weights and biases of an L-layer neural network as W (i) \u2208 R di\u00d7di\u22121 and b (i) \u2208 R di (i \u2208 {1, \u00b7 \u00b7 \u00b7 , L}) respectively. For simplicity we assume that d L = 1 so W (L) is a vector and b (L) is a scalar. The neural network function f : R d0 \u2192 R is defined as f (x) = z (L) (x), where z (i) (x) = W (i)\u1e91(i\u22121) (x) + b (i) , z (i) (x) = \u03c3(z (i) (x)) and\u1e91 (0) (x) = x. \u03c3 is the activation function and we use ReLU throughout this paper. When the context is clear, we omit \u00b7(x) and use z min f (x) := z (L) (x) s.t. z (i) = W (i)\u1e91(i\u22121) +b (i) ,\u1e91 (i) = \u03c3(z (i) ), x \u2208 C, i \u2208 {1, \u00b7 \u00b7 \u00b7 , L\u22121} (1) The set C defines the allowed input region and our aim is to find the minimum of f (x) for x \u2208 C, and throughout this paper we consider C as an \u221e ball around a data example x 0 : C = {x | x\u2212x 0 \u221e \u2264 } but other p norms can also be supported. In practical settings, we typically have \"specifications\" to verify, which are (usually linear) functions of neural network outputs describing the desired behavior of neural networks. For example, to guarantee robustness we typically investigate the margin between logits. Because the specification can also be seen as an output layer of NN and merged into f (x) under verification, we do not discuss it in detail in this work. We consider the canonical specification f (x) > 0: if we can prove that f (x) > 0, \u2200x \u2208 C, we say f (x) is verified.\n\nWhen C is a convex set, Eq. 1 is still a non-convex problem because the constraints\u1e91 (i) = \u03c3(z (i) ) are non-convex. Given unlimited time, complete verifiers can solve Eq. 1 exactly: f * = min f (x), \u2200x \u2208 C, so we can always conclude if the specification holds or not for any problem instance. On the other hand, incomplete verifiers usually relax the non-convexity of neural networks to obtain a tractable lower bound of the solution f \u2264 f * . If f \u2265 0, then f * > 0 so f (x) can be verified; when f < 0, we are not able to infer the sign of f * so cannot conclude if the specification holds or not.\n\nA commonly used incomplete verification technique is to relax non-convex ReLU constraints with linear constraints and turn the verification problem into a linear programming (LP) problem, which can then be solved with linear solvers. We refer to it as the \"LP verifier\" in this paper. Specifically, given ReLU(z\n(i) j ) := max(0, z (i) j ) and its intermediate layer bounds l (i) j \u2264 z (i) j \u2264 u (i) j , each ReLU can be categorized into three cases: (1) if l (i) j \u2265 0 (ReLU in linear region) then\u1e91 (i) j = z (i) j ; (2) if u (i) j \u2264 0 (ReLU in inactive region) then\u1e91 (i) j = 0; (3) if l (i) j \u2264 0 \u2264 u (i) j (ReLU is unstable) then three linear bounds are used:\u1e91 (i) j \u2265 0,\u1e91 (i) j \u2265 z (i) j , and\u1e91 (i) j \u2264 u (i) j u (i) j \u2212l (i) j z (i) j \u2212 l (i) j\n; they are often referred to as the \"triangle\" relaxation [15,42]. The intermediate layer bounds l (i) and u (i) are usually obtained from a cheaper bound propagation method (see next subsection). LP verifiers can provide relatively tight bounds but linear solvers are still expensive especially when the network is large. Also, unlike our \u03b2-CROWN, they have to use fixed intermediate bounds and cannot use the joint optimization of intermediate layer bounds (Section 3.3) to tighten relaxation.\n\n\nCROWN: efficient incomplete verification by propagating linear bounds\n\nAnother cheaper way to give a lower bound for the objective in Eq. 1 is through sound bound propagation. CROWN [46] is a representative method that propagates a linear bound of f (x) w.r.t. every intermediate layer in a backward manner until reaching the input x. CROWN uses two linear constraints to relax unstable ReLU neurons: a linear upper bound\u1e91 (i)\nj \u2264 u (i) j u (i) j \u2212l (i) j z (i) j \u2212 l (i) j\nand a linear lower bound\u1e91\n(i) j \u2265 \u03b1 (i) j z (i) j (0 \u2264 \u03b1 (i) j \u2264 1)\n. We can then bound the output of a ReLU layer:\nLemma 2.1 (ReLU relaxation in CROWN). Given w, v \u2208 R d , l \u2264 v \u2264 u (element-wise), we have w ReLU(v) \u2265 w Dv + b ,\nwhere D is a diagonal matrix containing free variables 0 \u2264 \u03b1 j \u2264 1 only when u j > 0 > l j and w j \u2265 0, while its rest values as well as constant b are determined by l, u, w.\n\nDetailed forms of each term are listed in Appendix A. Lemma 2.1 can be repeatedly applied, resulting in an efficient back-substitution procedure to derive a linear lower bound of NN output w.r.t. x:\nLemma 2.2 (CROWN bound [46]). Given an L-layer ReLU NN f (x) : R d0 \u2192 R with weights W (i) , biases b (i) , pre-ReLU bounds l (i) \u2264 z (i) \u2264 u (i) (1 \u2264 i \u2264 L) and input constraint x \u2208 C. We have min x\u2208C f (x) \u2265 min x\u2208C a CROWN x + c CROWN\nwhere a CROWN and c CROWN can be computed using\nW (i) , b (i) , l (i) , u (i) in polynomial time.\nWhen C is an p norm ball, minimization over the linear function can be easily solved using H\u00f6lder's inequality. The main benefit of CROWN is its efficiency: CROWN can be efficiently implemented on machine learning accelerators such as GPUs [44] and TPUs [47], and it can be a few magnitudes faster than an LP verifier which is hard to parallelize on GPUs. CROWN was generalized to general architectures [44,31] while we only demonstrate it for feedforward ReLU networks for simplicity. Additionally, Xu et al. [45] showed that it is possible to optimize the slope of the lower bound, \u03b1, using gradient ascent, to further tighten the bound (sometimes referred to as \u03b1-CROWN).\n\n\nBranch and Bound and Neuron Split Constraints\n\nBranch and bound (BaB) method is widely adopted in complete verifiers [8]: we divide the domain of the verification problem C into two subdomains\nC 1 = {x \u2208 C, z (i) j \u2265 0} and C 2 = {x \u2208 C, z (i) j < 0} where z (i)\nj is an unstable ReLU neuron in C but now becomes linear for each subdomain. Incomplete verifiers can then estimate the lower bound of each subdomain with relaxations. If the lower bound produced for subdomain C i (denoted by f Ci ) is greater than 0, C i is verified; otherwise, we further branch over domain C i by splitting another unstable ReLU neuron. The process terminates when all subdomains are verified. The completeness is guaranteed when all unstable ReLU neurons are split.\n\nLP verifier with neuron split constraints. A popular incomplete verifier used in BaB is the LP verifier. Essentially, when we split the j-th ReLU in layer i, we can simply add z\n(i) j \u2265 0 or z (i)\nj < 0 to Eq. 1 and get a linearly relaxed lower bound to each subdomain. We denote the Z +(i) and Z \u2212(i) as the set of neuron indices with positive and negative split constraints in layer i. We define the split constraints at layer i as\nZ (i) := {z (i) | z (i) j1 \u2265 0, z (i) j2 < 0, \u2200j 1 \u2208 Z +(i) , \u2200j 2 \u2208 Z \u2212(i) }.\nWe denote the vector of all pre-ReLU neurons as z, and we define a set Z to represent the split constraints on z:\nZ = Z (1) \u2229Z (2) \u2229\u00b7 \u00b7 \u00b7\u2229Z (L\u22121)\n. For convenience, we also use the shorthandZ (i) := Z (1) \u2229\u00b7 \u00b7 \u00b7\u2229Z (i) andz (i) := {z (1) , z (2) , \u00b7 \u00b7 \u00b7 , z (i) }. LP verifiers can easily handle these neuron split constraints but are more expensive than bound propagation methods like CROWN and cannot be accelerated on GPUs.\n\nBranching strategy. Branching strategies (selecting which ReLU neuron to split) are generally agnostic to the incomplete verifier used in BaB but do affect the overall BaB performance. BaBSR [7] is a widely used strategy in complete verifiers, which is based on an fast estimates on objective improvements after splitting each neuron. The neuron with highest estimated improvement is selected for branching. Recently, Filtered Smart Branching (FSB) [11] improves BaBSR by mimicking strong branching -it utilizes bound propagation methods to evaluate the best a few candidates proposed by BaBSR and chooses the one with largest improvement. Graph neural network (GNN) based branching was also proposed [23]. Our \u03b2-CROWN BaB is a general complete verification framework fit for any potential branching strategy, and we evaluate both BaBSR and FSB in experiments.\n\n\n\u03b2-CROWN for Complete and Incomplete Verification\n\nIn this section, we first give intuitions on how \u03b2-CROWN handles neuron split constraints without costly LP solvers. Then we formally state the main theorem of \u03b2-CROWN from both primal and dual spaces, and discuss how to tighten the bounds using free parameters \u03b1, \u03b2. Lastly, we propose \u03b2-CROWN BaB, a complete verifier that is also a strong incomplete verifier when stopped early.\n\n\n\u03b2-CROWN: Linear Bound Propagation with Neuron Split Constraints\n\nThe NN verification problem under neuron split constraints can be seen as an optimization problem:\nmin x\u2208C,z\u2208Z f (x).(2)\nBound propagation methods like CROWN can give a relatively tight lower bound for min x\u2208C f (x) but they cannot handle the neuron split constraints z \u2208 Z. Before we present our main theorem, we first show the intuition on how to apply split constraints to the bound propagation process.\n\nTo encode the neuron splits, we first define diagonal matrix S (i) \u2208 R di\u00d7di in Eq. 3 where i \u2208 [1, \u00b7 \u00b7 \u00b7 L \u2212 1], j \u2208 [1, \u00b7 \u00b7 \u00b7 , d i ] are indices of layers and neurons, respectively:\nS (i) j,j = \u22121(if split z (i) j \u2265 0); S (i) j,j = +1(if split z (i) j < 0); S (i) j,j = 0(if no split z (i) j ) (3)\nWe start from the last layer and derive linear bounds for each intermediate layer z (i) and\u1e91 (i) with both constraints x \u2208 C and z \u2208 Z. We also assume that pre-ReLU bounds l (i) \u2264 z (i) \u2264 u (i) for each layer i are available (see discussions in Sec. 3 \nW (L)\u1e91(L\u22121) + b (L) .(4)\nSince\u1e91 (L\u22121) = ReLU(z (L\u22121) ), we can apply Lemma 2.1 to relax the ReLU neuron at layer L \u2212 1, and obtain a linear lower bound for f (x) w.r.t. z (L\u22121) (we omit all constant terms to avoid clutter):\nmin x\u2208C,z\u2208Z f (x) \u2265 min x\u2208C,z\u2208Z W (L) D (L\u22121) z (L\u22121) + const.\nTo enforce the split neurons at layer L \u2212 1, we use a Lagrange function with \u03b2 (L\u22121) S (L\u22121) multiplied on z (L\u22121) :\nmin x\u2208C,z\u2208Z f (x) \u2265 min x\u2208C z (L\u22122) \u2208Z (L\u22122) max \u03b2 (L\u22121) \u22650 W (L) D (L\u22121) z (L\u22121) + \u03b2 (L\u22121) S (L\u22121) z (L\u22121) + const \u2265 max \u03b2 (L\u22121) \u22650 min x\u2208C z (L\u22122) \u2208Z (L\u22122) W (L) D (L\u22121) + \u03b2 (L\u22121) S (L\u22121) z (L\u22121) + const (5)\nThe first inequality is due to the definition of the Lagrange function: we remove the constraint z (L\u22121) \u2208 Z (L\u22121) and use a multiplier to replace this constraint. The second inequality is due to weak duality. Due to the design of S (L\u22121) , neuron split z \nf (x) \u2265 max \u03b2 (L\u22121) \u22650 min x\u2208C z (L\u22122) \u2208Z (L\u22122) W (L) D (L\u22121) + \u03b2 (L\u22121) S (L\u22121) W (L\u22121)\u1e91(L\u22122) + const (6)\nWe define a matrix A (i) to represent the linear relationship between f (x) and\u1e91 (i) , where A (L\u22121) = W (L) according to Eq. 4 and A (L\u22122) = (A (L\u22121) D (L\u22121) + \u03b2 (L\u22121) S (L\u22121) )W (L\u22121) by Eq. 6. Considering 1-dimension output f (x), A (i) has only 1 row. With A (L\u22122) , Eq. 6 becomes:\nmin x\u2208C,z\u2208Z f (x) \u2265 max \u03b2 (L\u22121) \u22650 min x\u2208C z (L\u22122) \u2208Z (L\u22122) A (L\u22122)\u1e91(L\u22122) + const,\nwhich is in a form similar to Eq. 4 except for the outer maximization over \u03b2 (L\u22121) . This allows the back-substitution process (Eq. 4, Eq. 5, and Eq. 6) to continue. In each step, we swap max and min as in Eq. 5, so every maximization over \u03b2 (i) is outside of min x\u2208C . Eventually, we have:\nmin x\u2208C,z\u2208Z f (x) \u2265 max \u03b2\u22650 min x\u2208C A (0) x + const, where \u03b2 := \u03b2 (1) \u03b2 (2) \u00b7 \u00b7 \u00b7 \u03b2 (L\u22121)\nconcatenates all \u03b2 (i) vectors. Following the above idea, we present the main theorem in Theorem 3.1 (proof is given in Appendix A).\nTheorem 3.1 (\u03b2-CROWN bound). Given an L-layer NN f (x) : R d0 \u2192 R with weights W (i) , biases b (i) , pre-ReLU bounds l (i) \u2264 z (i) \u2264 u (i) (1 \u2264 i \u2264 L)\n, input bounds C, split constraints Z. We have:\nmin x\u2208C,z\u2208Z f (x) \u2265 max \u03b2\u22650 min x\u2208C (a + P\u03b2) x + q \u03b2 + c,(7)\nwhere\na \u2208 R d0 ,P \u2208 R d0\u00d7( L\u22121 i=1 di) ,q \u2208 R L\u22121 i=1 di and c \u2208 R are functions of W (i) , b (i) , l (i) , u (i) .\nDetailed formulations for a, P, q and c are given in Appendix A. Theorem 3.1 shows that when neuron split constraints exist, f (x) can still be bounded by a linear equation containing optimizable multipliers \u03b2. Observing Eq. 5, the main difference between CROWN and \u03b2-CROWN lies in the relaxation of each ReLU layer, where we need an extra term \u03b2 (i) S (i) in the linear relationship matrix (for example, W (L) D (L\u22121) in Eq. 5) between f (x) and z (i) to enforce neuron split constraints. This extra term in every ReLU layer yields P and q in Eq. 7 after bound propagations.\n\nTo solve the optimization problem in Eq. 7, we note that in the p norm robustness setting (C = {x | x \u2212 x 0 p \u2264 }), the inner minimization has a closed solution:\nmin x\u2208C,z\u2208Z f (x) \u2265 max \u03b2\u22650 \u2212 a + P\u03b2 q + (P x 0 + q) \u03b2 + a x 0 + c := max \u03b2\u22650 g(\u03b2)(8)\nwhere 1 p + 1 q = 1. The maximization is concave in \u03b2 (q \u2265 1), so we can simply optimize it using projected (super)gradient ascent with gradients from an automatic differentiation library. Since any \u03b2 \u2265 0 yields a valid lower bound for min x\u2208C,z\u2208Z f (x), convergence is not necessary to guarantee soundness. \u03b2-CROWN is efficient -it has the same asymptotic complexity as CROWN when \u03b2 is fixed. When \u03b2 = 0, \u03b2-CROWN yields the same results as CROWN; however the additional optimizable \u03b2 allows us to maximize and tighten the lower bound due to neuron split constraints.\n\nWe define \u03b1 (i) \u2208 R di for free variables associated with unstable ReLU neurons in Lemma 2.1 for layer i and define all free variables \u03b1 = {\u03b1 (1) \u00b7 \u00b7 \u00b7 \u03b1 (L\u22121) }. Since any 0 \u2264 \u03b1 (i) j \u2264 1 yields a valid bound, we can optimize it to tighten the bound, similarly as done in [45]. Formally, we rewrite Eq. 8 with \u03b1 explicitly: min\nx\u2208C,z\u2208Z f (x) \u2265 max 0\u2264\u03b1\u22641, \u03b2\u22650 g(\u03b1, \u03b2).(9)\n\nConnections to the Dual Problem\n\nIn this subsection, we show that \u03b2-CROWN can also be derived from a dual LP problem. Based on Eq. 1 and linear relaxations in Section 2, we first construct an LP problem for \u221e robustness verification in Eq. 10 where i \u2208 {1, \u00b7 \u00b7 \u00b7 , L \u2212 1}.\nmin f (x) := z (L) (x) s.t.\nNetwork and Input Bounds:\nz (i) = W (i)\u1e91(i\u22121) + b (i) ;\u1e91 (0) \u2265 x 0 \u2212 ;\u1e91 (0) \u2264 x 0 + ; Stable ReLUs:\u1e91 (i) j = z (i) j (if l (i) j \u2265 0);\u1e91 (i) j = 0 (if u (i) j \u2264 0); Unstable:\u1e91 (i) j \u2265 0,\u1e91 (i) j \u2265 z (i) j ,\u1e91 (i) j \u2264 u (i) j u (i) j \u2212l (i) j z (i) j \u2212 l (i) j (if l (i) j < 0 < u (i) j , j / \u2208 Z +(i) \u222a Z \u2212(i) )\nNeuron Split Constraints:\u1e91\n(i) j = z (i) j , z (i) j \u2265 0 (if j \u2208 Z +(i) );\u1e91 (i) j = 0, z (i) j < 0 (if j \u2208 Z \u2212(i) )(10)\nCompared to the formulation in [42], we have neuron split constraints. Many BaB based complete verifiers [8,23] use an LP solver for Eq. 10 as the incomplete verifier. We first show that it is possible to derive Theorem 3.1 from the dual of this LP, leading to Theorem 3.2: Theorem 3.2. The objective d LP for the dual problem of Eq. 10 can be represented as\nd LP = \u2212 a + P\u03b2 1 \u00b7 + (P x 0 + q) \u03b2 + a x 0 + c,\nwhere a, P, q and c are defined in the same way as in Theorem 3.1, and \u03b2 \u2265 0 corresponds to the dual variables of neuron split constraints in Eq. 10.\n\nA similar connection between CROWN and dual LP based verifier [42] was shown in [30], and their results can be seen as a special case of ours when \u03b2 = 0 (none of the split constraints are active). An immediate consequence is that \u03b2-CROWN can potentially solve Eq. 10 as well as using an LP solver: Corollary 3.2.1. When \u03b1 and \u03b2 are optimally set and intermediate bounds l, u are fixed, \u03b2-CROWN produces p * LP , the optimal objective of LP with split constraints in Eq. 10: max\n0\u2264\u03b1\u22641,\u03b2\u22650 g(\u03b1, \u03b2) = p * LP ,\nIn Appendix A, we give detailed formulations for conversions between the variables \u03b1, \u03b2 in \u03b2-CROWN and their corresponding dual variables in the LP problem.\n\n\nJoint Optimization of Free Variables in \u03b2-CROWN\n\nIn Eq. 9, g is also a function of l   \nmin x\u2208C,z (i\u22121) \u2208Z (i\u22121) z (i) j (x) \u2265 max 0\u2264\u03b1 \u22641, \u03b2 \u22650 g (\u03b1 , \u03b2 ) := l (i) j(11)\nFor computing u j ) and produce tight final bounds, which is impossible in LP. In other words, we need to optimize\u03b1 and\u03b2, which are two vectors concatenating \u03b1, \u03b2 as well as a large number of \u03b1 and \u03b2 used to compute each intermediate layer bound:\nmin x\u2208C,z\u2208Z f (x) \u2265 max 0\u2264\u03b1\u22641,\u03b2\u22650 g(\u03b1,\u03b2).(12)\nThis formulation is non-convex and has a large number of variables. Since any 0 \u2264\u03b1 \u2264 1,\u03b2 \u2265 0 leads to a valid lower bound, the non-convexity does not affect soundness. When intermediate layer bounds are also allowed to be tightened during optimization, we can outperform the LP verifier for Eq. 10 using fixed intermediate layer bounds. Typically, in many previous works [8,23,6], when the LP formulation Eq. 10 is formed, intermediate layer bounds are pre-computed with bound propagation procedures [8,23], which are far from optimal. To estimate the dimension of this problem, we denote the number of unstable neurons at layer i as\ns i := Tr(|S (i) |). Each neuron in layer i is associated with 2 \u00d7 i\u22121 k=1 s k variables \u03b1 . Suppose each hidden layer has d neurons (s i = O(d)), then\u03b1 has 2 \u00d7 L\u22121 i=1 d i i\u22121 k=1 s k = O(L 2 d 2 )\nvariables in total. This can be too large for efficient optimization, so we share \u03b1 and \u03b2 among the intermediate neurons of the same layer, leading to a total number of O(L 2 d) variables to optimize. Note that a weaker form of joint optimization was also discussed in [45] without \u03b2, and a detailed analysis can be found in Appendix B.2.\n\n\n\u03b2-CROWN with Branch and Bound (\u03b2-CROWN BaB)\n\nWe perform complete verification following BaB framework [8] using \u03b2-CROWN as the incomplete solver, and we use simple branching heuristics like BaBSR [7] or FSB [11]. To efficiently utilize GPU, we also use batch splits to evaluate multiple subdomains in the same batch as in [44,10]. We list our full algorithm \u03b2-CROWN BaB in Appendix B and we show it is sound and complete here: Theorem 3.3. \u03b2-CROWN with Branch and Bound on splitting ReLUs is sound and complete.\n\nSoundness is trivial because \u03b2-CROWN is a sound verifier. For completeness, it suffices to show that when all unstable ReLU neurons are split, \u03b2-CROWN gives the global minimum for Eq. 10. In contrast, combining CROWN [46] with BaB does not yield a complete verifier, as it cannot detect infeasible splits and a slow LP solver is still needed to guarantee completeness [45]. Instead, \u03b2-CROWN can detect infeasible subdomains -according to duality theory, an infeasible primal problem leads to an unbounded dual objective, which can be detected (see Sec. B.3 for more details).\n\nAdditionally, we show the potential of early stopping a complete verifier as an incomplete verifier. BaB approaches the exact solution of Eq. 1 by splitting the problem into multiple subdomains, and more subdomains give a tighter lower bound for Eq. 1. Unlike traditional complete verifiers, \u03b2-CROWN is efficient to explore a large number of subdomains during a very short time, making \u03b2-CROWN BaB an attractive solution for efficient incomplete verification.\n\n\nExperimental Results\n\n\nComparison to Complete Verifiers\n\nWe evaluate complete verification performance on dataset provided in [23,10] and used in VNN-COMP 2020 [22]. The benchmark contains three CIFAR-10 models (Base, Wide, and Deep) with 100 examples each. Each data example is associated with an \u221e norm and a target label for verification (referred to as a property to verify). The details of neural network structures and experimental setups can be found in Appendix C. We compare against multiple baselines for complete verification: (1) BaBSR [7], a basic BaB and LP based verifier; (2) MIPplanet [15], a customized MIP solver for NN verification where unstable ReLU neurons are randomly selected for splitting;\n\n(3) ERAN [35,33,36,34], an abstract interpretation based verifier which performs well on this benchmark in VNN-COMP 2020; (4) GNN-Online [23], a BaB and LP based verifiers using a learned Graph Neural Network (GNN) to guide the ReLU splits; (5) BDD+ BaBSR [6], a verification framework based on Lagrangian decomposition on GPUs (BDD+) with BaBSR branching strategy; (6) OVAL (BDD+ GNN) [6,23], a strong verifier in VNN-COMP 2020 using BDD+ with GNN guiding the ReLU splits; (7) A.set BaBSR and (8) Big-M+A.set BaBSR [10], very recent dual-space verifiers on GPUs with a tighter linear relaxation than triangle LP relaxations; (9) Fast-and-Complete [45],  which uses CROWN (LiRPA) on GPUs as the incomplete verifier in BaB without neuron split constraints; (10) BaDNB (BDD+ FSB) [11], a concurrent state-of-the-art complete verifier, using BDD+ on GPUs with FSB branching strategy. \u03b2-CROWN BaB can use either BaBSR or FSB branching heuristic, and we include both in evaluation. All methods use a 1 hour timeout threshold.\n\nWe report the average verification time and branch numbers in Table 1 and plot the percentage of solved properties over time in Figure 1. \u03b2-CROWN FSB achieves the fastest average running time compared to all other baselines with minimal timeouts, and also clearly leads on the cactus plot. When using a weaker branching heuristic, \u03b2-CROWN BaBSR still outperforms all literature baselines, including very recent ones such as A.set BaBSR [10], Fast-and-Complete [45] and BaDNB [11]. Our benefits are more clearly shown in Figure 1, where we solve over 90% examples under 10s and most other verifiers can verify much less or none of the properties within 10s. We see a 2 to 3 orders of magitudes speedup in Figure 1 compared to CPU based verifiers such as MIPplanet and BaBSR.\n\n\nComparison to Incomplete Verifiers\n\nVerified accuracy. In Table 2, we compare against a few representative and strong incomplete verifiers on 5 convolutional networks and 4 MLP networks for MNIST and CIFAR-10 under the same set of 1000 images and perturbation as reported in [35,37,26]. Among the baselines, kPoly [35], OptC2V [37] and PRIMA [26] utilize state-of-the-art multi-neuron linear relaxation for ReLUs and can bypass the single-neuron convex relaxation barrier [30], and are among the strongest incomplete verifiers. \u03b2-CROWN FSB achieves better verified accuracy on all models using a similar or less amount of time. Some models, such as MNIST ConvBig and CIFAR ConvBig, are quite challengingthe verified accuracy obtained by \u03b2-CROWN FSB is close to the upper bound found via PGD attack.\n\nTo make more comprehensive evaluations, in Table 3 we further compare against a state-of-the-art semidefinite programming (SDP) based verifier, SDP-FO [9], on one MNIST and six CIFAR-10 models reported in their paper. The models were trained using adversarial training, which posed a challenge for verification [28]. The SDP formulation can be tighter than linear relaxation based ones, but it is computationally expensive -SDP-FO takes 2 to 3 hours to converge on one GPU for verifying a single property, resulting 5,400 GPU hours to verify 200 testing images with 10 labels each. Due to resource limitations, we directly quote SDP-FO results from [9] on the same set of models. We evaluate verified accuracy on the same set of 200 test images for other baselines. We include a concurrent work PRIMA [26], the strongest multi-neuron linear relaxation baseline in Table 2, which generally outperforms kPoly and OptC2V. Table 3 shows that overall we are 3 orders of magnitude faster than SDP-FO while still achieving consistently higher verified accuracy on average. Tightness of verification. In Figure 2, we compare the tightness of verification bounds against SDP-FO on two adversarially trained networks from [9]. Specifically, we use the verification objective  Table 3: Verified accuracy (%) and avg. per-example verification time (s) on 7 models from SDP-FO [9]. CROWN/DeepPoly are fast but loose bound propagation based methods, and they cannot be improved with more running time. SDP-FO uses stronger semidefinite relaxations, which can be very slow and sometimes has convergence issues. PRIMA, a concurrent work, is the state-of-the-art relaxation barrier breaking method; we did not include kPoly and OptC2V because they are weaker than PRIMA (see Table 2).  VNN-COMP 2021 results. We encourage the readers to checkout the report of the Second International Verification of Neural Networks Competition (VNN-COMP 2021) [3] with 9 additional benchmarks and 12 competing methods evaluated in a standardized testing environment on AWS. Our entry \u03b1,\u03b2-CROWN is based on the \u03b2-CROWN algorithm in this work and uses the same codebase.\n\n\nRelated Work\n\nMany early complete verifiers for neural networks relied on existing solvers such as MILP or SMT solvers [20,15,19,12,38] and were limited to very small problem instances. Branch and bound (BaB) based method was proposed to better exploit the network structure using LP-based incomplete verifier for bounding and ReLU splits for branching [8,40,23,5]. Besides branching on ReLU neurons, input domain branching was also considered in [41,29,1] but limited by input dimensions [8].\n\nRecently, a few approaches have been proposed to use efficient iterative solvers or bound propagation methods on GPUs without relying on LP solvers. Bunel et al. [6] decomposed the verification problem layer by layer, solved each layer in a closed form on GPUs, and used Lagrangian to enforce consistency between layers. However, their formulation only has the same power as LP and needs many iterations to converge. De Palma et al. [10] used a dual-space verifier with a linear relaxation [2,37] tighter than triangle LP relaxation, but in most settings the extra computational costs and difficulties for an efficient implementation offset its benefits (more discussions in section B.2). A concurrent work BaDNB [11] proposed a new branching strategy, filtered smart branching (FSB), combined with Lagrangian decomposition to get better verification performance. Xu et al. [44] used CROWN as a massively paralleled incomplete solver on GPUs for complete verification, but it cannot handle neuron split constraints, leading to suboptimal efficiency and high timeout rates.\n\nFor incomplete verification, Salman et al. [30] shows the inherent limitation of using per-neuron convex relaxations for verification problems. Singh et al. [35] and M\u00fcller et al. [26] broke this barrier by considering constraints involving multiple ReLU neurons; Tjandraatmadja et al. [37] proposed to relax a linear layer with a ReLU neuron together using a strong mixed-integer programming formulation [1]. SDP based relaxations [28,16,14] typically produce tight bounds but with significantly higher cost. The most recent GPU based SDP verifier [9] is still relatively slow and can take 2 hours to verify a single image. In this work, we impose neuron split constraints using \u03b2-CROWN and combine it with branch and bound done in parallel on GPUs. Although for each subdomain in BaB, \u03b2-CROWN is still subject to the convex relaxation barrier, the efficiency of \u03b2-CROWN BaB allows it to quickly explore a very large number of subdomains and outperform existing convex barrier breaking incomplete verifiers under many scenarios in both runtime and tightness.\n\nAdditionally, another line of works train networks to enhance verified accuracy, typically using cheap incomplete verifiers at training time [42,39,25,43,18,25,47,4,32]. Traditionally only these verification-customized networks can have reasonable verified accuracy, while \u03b2-CROWN BaB can also give non-trivial verified accuracy on relatively large networks agnostic to verification.\n\n\nConclusion\n\nWe proposed \u03b2-CROWN, a new bound propagation method that can fully encode the neuron split constraints introduced in BaB, which clearly leads in both complete and incomplete verification settings. The success of \u03b2-CROWN comes from a few factors: (1) In Section 3.1, we show that \u03b2-CROWN is an GPU-friendly bound propagation algorithm significantly faster than LP solvers. (2)  Societal Impact NNs have been used in an increasingly wide range of real-world applications and play an important role in artificial intelligence (AI). The trustworthiness and robustness of NNs have become crucial factors since AI plays an important role in modern society. \u03b2-CROWN is a strong neural network verifier which can be used to check certain properties of neural networks, which can be helpful for guaranteeing the robustness, correctness, and fairness of NNs in applications that can directly or indirectly impact human life. We believe our work has overall positive societal impacts, although it may potentially be misused to identify the weakness of NNs and guide attacks. \nw, v \u2208 R d , l \u2264 v \u2264 u (element-wise), we have w ReLU(v) \u2265 w Dv + b ,\nwhere D is a diagonal matrix defined as:\nD j,j = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1, if l j \u2265 0 0, if u j \u2264 0 \u03b1 j , if u j > 0 > l j and w j \u2265 0 uj uj \u2212lj , if u j > 0 > l j and w j < 0,(1)0 \u2264 \u03b1 j \u2264 1 are free variables, b = w b and each element in b is b j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0, if l j > 0 or u j \u2264 0 0, if u j > 0 > l j and w j \u2265 0 \u2212 uj lj uj \u2212lj , if u j > 0 > l j and w j < 0. (2) Proof. For the j-th ReLU neuron, if l j \u2265 0, then ReLU(v j ) = v j ; if u j < 0, then ReLU(v j ) = 0.\nFor the case of l j < 0 < u j , the ReLU function can be linearly upper and lower bounded within this range:\n\u03b1 j v j \u2264 ReLU(v j ) \u2264 u j u j \u2212 l j (v j \u2212 l j ) \u2200 l j \u2264 v j \u2264 u j\nwhere 0 \u2264 \u03b1 j \u2264 1 is a free variable -any value between 0 and 1 produces a valid lower bound.\n\nTo lower bound w ReLU(v) = j w j ReLU(v j ), for each term in this summation, we take the lower bound of ReLU(v j ) if w j is positive and take the upper bound of ReLU(v j ) if w j is negative (reflected in the definitions of D and b). This conservative choice allows us to always obtain a lower bound \u2200 l \u2264 v \u2264 u:\nj w j ReLU(v j ) \u2265 j w j D j,j v j + b j = w Dv + w b = w Dv + b\nwhere D j,j and b j are defined in Eq. 1 and Eq. 2 representing the lower or upper bounds of ReLU.\n\nBefore proving our main theorem (Theorem 3.1), we first define matrix \u2126, which is the product of a series of model weights W and \"weights\" for relaxed ReLU layers D:\n\nDefinition A.1. Given a set of matrices W (2) , \u00b7 \u00b7 \u00b7 , W (L) and D (1) , \u00b7 \u00b7 \u00b7 , D (L\u22121) , we define a recursive function \u2126(k, i) for 1 \u2264 i \u2264 k \u2264 L as\n\u2126(i, i) = I, \u2126(k+1, i) = W (k+1) D (k) \u2126(k, i) For example, \u2126(3, 1) = W (3) D (2) W (2) D (1) , \u2126(5, 2) = W (5) D (4) W (4) D (3) W (3) D (2)\n. Now we present our main theorem with each term explicitly written:\nTheorem 3.1 (\u03b2-CROWN bound). Given a L-layer neural network f (x) : R d0 \u2192 R with weights W (i) , biases b (i) , pre-ReLU bounds l (i) \u2264 z (i) \u2264 u (i) (1 \u2264 i \u2264 L)\n, input constraint C and split constraint Z. We have\nmin x\u2208C,z\u2208Z f (x) \u2265 max \u03b2\u22650 min x\u2208C (a + P\u03b2) x + q \u03b2 + c,(3)\nwhere P \u2208 R d0\u00d7( L\u22121 i=1 di) is a matrix containing blocks P := P 1 P 2 \u00b7 \u00b7 \u00b7 P L\u22121 , q \u2208 R L\u22121 i=1 di is a vector q := q 1 \u00b7 \u00b7 \u00b7 q L\u22121 , and each term is defined as:\na = \u2126(L, 1)W (1) \u2208 R d0\u00d71(4)P i = S (i) \u2126(i, 1)W (1) \u2208 R di\u00d7d0 , \u2200 1 \u2264 i \u2264 L \u2212 1 (5) q i = i k=1 S (i) \u2126(i, k)b (k) + i k=2 S (i) \u2126(i, k)W (k) b (k\u22121) \u2208 R di , \u2200 1 \u2264 i \u2264 L \u2212 1 (6) c = L i=1 \u2126(L, i)b (i) + L i=2 \u2126(L, i)W (i) b (i\u22121)(7)\ndiagonal matrices D (i) and vector b (i) are determined by the relaxation of ReLU neurons, and A (i) \u2208 R 1\u00d7di represents the linear relationship between f (x) and\u1e91 (i) . D (i) and b (i) depend on A (i) , l (i) and u (i) :\nD (i) j,j = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1, if l (i) j \u2265 0 or j \u2208 Z +(i) 0, if u (i) j \u2264 0 or j \u2208 Z \u2212(i) \u03b1 j , if u (i) j > 0 > l (i) j and j / \u2208 Z +(i) \u222a Z \u2212(i) and A (i) 1,j \u2265 0 uj uj \u2212lj , if u (i) j > 0 > l (i) j and j / \u2208 Z +(i) \u222a Z \u2212(i) and A (i) 1,j < 0 (8) b (i) j = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 0, if l (i) j > 0 or u (i) j \u2264 0 or j \u2208 Z +(i) \u222a Z \u2212(i) 0, if u (i) j > 0 > l (i) j and j / \u2208 Z +(i) \u222a Z \u2212(i) and A (i) 1,j \u2265 0 \u2212 u (i) j l (i) j u (i) j \u2212l (i) j , if u (i) j > 0 > l (i) j and j / \u2208 Z +(i) \u222a Z \u2212(i) and A (i) 1,j < 0 (9) A (i) = W (L) , i = L \u2212 1 (A (i+1) D (i+1) + \u03b2 (i+1) S (i+1) )W (i+1) , 0 \u2264 i \u2264 L \u2212 2(10)\nProof. We prove this theorem by induction: assuming we know the bounds with respect to layer\u1e91 (m) , we derive bounds for\u1e91 (m\u22121) until we reach m = 0 and by definition\u1e91 (0) = x. We first define a set of matrices and vectors a (m) , P (m) , q (m) , c (m) , where P (m) \u2208 R dm\u00d7( L\u22121 i=m+1 di) is a matrix containing\nblocks P := P (m) m+1 \u00b7 \u00b7 \u00b7 P (m) L\u22121 , q \u2208 R L\u22121 i=m+1 di is a vector q := q (m) m+1 \u00b7 \u00b7 \u00b7 q (m) L\u22121\n, and each term is defined as:\na (m) = \u2126(L, m + 1)W (m+1) \u2208 R dm\u00d71 (11) P (m) i = S (i) \u2126(i, m + 1)W (m+1) \u2208 R di\u00d7dm , \u2200 m + 1 \u2264 i \u2264 L \u2212 1 (12) q (m) i = i k=m+1 S (i) \u2126(i, k)b (k) + i k=m+2 S (i) \u2126(i, k)W (k) b (k\u22121) \u2208 R dm , \u2200 m + 1 \u2264 i \u2264 L \u2212 1 (13) c (m) = L i=m+1 \u2126(L, i)b (i) + L i=m+2 \u2126(L, i)W (i) b (i\u22121)(14)\nand we claim that\nmin x\u2208C z\u2208Z f (x) \u2265 max \u03b2 (m+1) \u22650 min x\u2208C z (m) \u2208Z (m) (a (m) + P (m)\u03b2(m+1) ) \u1e91 (m) + q (m) \u03b2 (m+1) + c (m)(15)\nwhere\u03b2 (m+1) := \u03b2 (m+1) \u00b7 \u00b7 \u00b7 \u03b2 (L\u22121) concatenating all \u03b2 (i) variables up to layer m + 1. \nf (x) \u2265 max \u03b2 (m+1) \u22650 min x\u2208C z (m) \u2208Z (m) (a (m) + P (m)\u03b2(m+1) ) D (m) z (m) + (a (m) + P (m)\u03b2(m+1) ) b (m) + q (m) \u03b2 (m+1) + c (m)(16)\nNote that when we apply Lemma 2.1, for j \u2208 Z +(i) (positive split) we simply treat the neuron j as if l (i) j \u2265 0, and for j \u2208 Z \u2212(i) (negative split) we simply treat the neuron j as if u (i) j \u2264 0. Now we add the multiplier \u03b2 (m) to z (m) to enforce per-neuron split constraints:\nmin x\u2208C z\u2208Z f (x) \u2265 max \u03b2 (m+1) \u22650 min x\u2208C z (m\u22121) \u2208Z (m\u22121) max \u03b2 (m) \u22650 (a (m) + P (m)\u03b2(m+1) ) D (m) z (m) + \u03b2 (m) S (m) z (m) +(a (m) + P (m)\u03b2(m+1) ) b (m) + q (m) \u03b2 (m+1) + c (m) \u2265 max \u03b2 (m) \u22650 min x\u2208C z (m\u22121) \u2208Z (m\u22121) (a (m) D (m) +\u03b2 (m+1) P (m) D (m) + \u03b2 (m) S (m) )z (m) +(a (m) + P (m)\u03b2(m+1) ) b (m) + q (m) \u03b2 (m+1) + c (m)\nSimilar to what we did in Eq. 5, we swap the min and max in the second inequality due to weak duality, such that every maximization on \u03b2 (i) is before min. Then, we substitute\u1e91 (m) = W (m)\u1e91(m\u22121) +b (m) and obtain:\nmin x\u2208C z\u2208Z f (x) \u2265 max \u03b2 (m) \u22650 min x\u2208C z (m\u22121) \u2208Z (m\u22121) (a (m) D (m) +\u03b2 (m+1) P (m) D (m) + \u03b2 (m) S (m) ) W (m)\u1e91(m\u22121) + (a (m) D (m) +\u03b2 (m+1) P (m) D (m) + \u03b2 (m) S (m) ) b (m) + (a (m) + P (m)\u03b2(m+1) ) b (m) + q (m) \u03b2 (m+1) + c (m) = \uf8ee \uf8ef \uf8f0 a (m) D (m) W (m) a + (\u03b2 (m+1) P (m) D (m) W (m) + \u03b2 (m) S (m) W (m) ) P \u03b2(m) \uf8f9 \uf8fa \uf8fb \u1e91 (m\u22121) + (P (m) D (m) b (m) + P (m) b (m) + q (m) ) \u03b2 (m+1) + (S (m) b (m) ) \u03b2 (m) q \u03b2(m) + a (m) D (m) b (m) + a (m) b (m) + c (m) c\nNow we evaluate each term a , P , q and c and show the induction holds. For a and q we have:\na = a (m) D (m) W (m) = \u2126(L, m + 1)W (m+1) D (m) W (m) = \u2126(L, m)W (m) = a (m\u22121) c = c (m) + \u2126(L, m + 1)W (m+1) D (m) b (m) + \u2126(L, m + 1)W (m+1) b (m) = L i=m+1 \u2126(L, i)b (i) + L i=m+2 \u2126(L, i)W (i) b (i\u22121) + \u2126(L, m)b (m) + \u2126(L, m + 1)W (m+1) b (m) = L i=m \u2126(L, i)b (i) + L i=m+1 \u2126(L, i)W (i) b (i\u22121) = c (m\u22121)\nFor P := P m \u00b7 \u00b7 \u00b7 P L\u22121 , we have a new block P m where\nP m = S (m) W (m) = S (m) \u2126(m, m)W (m) = P (m\u22121) m for other blocks where m + 1 \u2264 i \u2264 L \u2212 1, P i = P (m) i D (m) W (m) = S (i) \u2126(i, m + 1)W (m+1) D (m) W (m) = S (i) \u2126(i, m)W (m) = P (m\u22121) i For q := q m \u00b7 \u00b7 \u00b7 q L\u22121 , we have a new block q m where q m = S (m) b (m) = m k=m S (i) \u2126(i, k)b (i) = q (m\u22121) m for other blocks where m + 1 \u2264 i \u2264 L \u2212 1, q i = q (m) i + P (m) D (m) b (m) + P (m) b (m) = i k=m+1 S (i) \u2126(i, k)b (k) + i k=m+2 S (i) \u2126(i, k)W (k) b (k\u22121) + P (m) D (m) b (m) + P (m) b (m) = i k=m+1 S (i) \u2126(i, k)b (k) + i k=m+2 S (i) \u2126(i, k)W (k) b (k\u22121) + S (i) \u2126(i, m + 1)W (m+1) D (m) b (m) + S (i) \u2126(i, m + 1)W (m+1) b (k) = i k=m+1 S (i) \u2126(i, k)b (k) + i k=m+2 S (i) \u2126(i, k)W (k) b (k\u22121) + S (i) \u2126(i, m)b (m) + S (i) \u2126(i, m + 1)W (m+1) b (m) = i k=m S (i) \u2126(i, k)b (k) + i k=m+1 S (i) \u2126(i, k)W (k) b (k\u22121) = q (m\u22121) i\nThus, a = a (m\u22121) , P = P (m\u22121) , q = q (m\u22121) and c = c (m\u22121) so the induction holds for layer z (m\u22121) :\nmin x\u2208C z\u2208Z f (x) \u2265 max \u03b2 (m) \u22650 min x\u2208C z (m\u22121) \u2208Z (m\u22121) (a (m\u22121) + P (m\u22121)\u03b2(m) ) \u1e91 (m\u22121) + q (m\u22121) \u03b2 (m) + c (m\u22121)(17)\nFinally, Theorem 3.1 becomes the special case where m = 0 in Eq. 11, Eq. 12, Eq. 13 and Eq. 14.\n\nThe next Lemma unveils the connection with CROWN [46] and is also useful for drawing connections to the dual problem.\n\nLemma A.2. With D, b and A defined in Eq. 8, Eq. 9 and Eq. 10, we can rewrite Eq. 3 in Theorem 3.1 as:\nmin x\u2208C z\u2208Z f (x) \u2265 max \u03b2\u22650 min x\u2208C A (0) x + L\u22121 i=1 A (i) (D (i) b (i) + b (i) ) (18) where A (i) , 0 \u2264 i \u2264 L \u2212 1 contains variables \u03b2.\nProof. To prove this lemma, we simply follow the definition of A (i) and check the resulting terms are the same as Eq. 3. For example,\nA (0) = (A (1) D (1) + \u03b2 (1) S (1) )W (1) = A (1) D (1) W (1) + \u03b2 (1) S (1) W (1) = (A (2) D (2) + \u03b2 (2) S (2) )W (2) D (1) W (1) + \u03b2 (1) S (1) W (1) = A (2) D (2) W (2) D (1) W (1) + \u03b2 (2) S (2) W (2) D (1) W (1) + \u03b2 (1) S (1) W (1) = \u00b7 \u00b7 \u00b7 = \u2126(L, 1)W (1) + L\u22121 i=1 \u03b2 (i) S (i) \u2126(i, 1)W (1)\n\n= [a + P\u03b2]\n\nOther terms can be shown similarly.\n\nWith this definition of A, we can see Eq. 3 as a modified form of CROWN, with an extra term \u03b2 (i+1) S (i+1) added when computing A (i) . When we set \u03b2 = 0, we obtain the same bound propagation rule for A as in CROWN. Thus, only a small change is needed to implement \u03b2-CROWN given an existing CROWN implementation: we add \u03b2 (i+1) S (i+1) after the linear bound propagates backwards through a ReLU layer. We also have the same observation in the dual space, as we will show this connection in the next subsection.\n\nA.2 Proofs for the connection to the dual space Theorem 3.2. The objective d LP for the dual problem of Eq. 10 can be represented as\nd LP = \u2212 a + P\u03b2 1 \u00b7 + (P x 0 + q) \u03b2 + a x 0 + c,\nwhere a, P, q and c are defined in the same way as in Theorem 3.1, and \u03b2 \u2265 0 corresponds to the dual variables of neuron split constraints in Eq. 10.\n\nProof. To prove the Theorem 3.2, we demonstrate the detailed dual objective d LP for Eq. 10, following a construction similar to the one in Wong and Kolter [42]. We first associate a dual variable for each constraint involved in Eq. 10 including dual variables \u03b2 for the per-neuron split constraints introduced by BaB. Although it is possible to directly write the dual LP for Eq. 10, for easier understanding, we first rewrite the original primal verification problem into its Lagrangian dual form as Eq. 19, with dual variables \u03bd, \u03be + , \u03be \u2212 \u00b5, \u03b3, \u03bb, \u03b2:\nL(z,\u1e91; \u03bd, \u03be, \u00b5, \u03b3, \u03bb, \u03b2) = z (L) + L i=1 \u03bd (i) (z (i) \u2212 W (i)\u1e91(i\u22121) \u2212 b (i) ) + \u03be + (\u1e91 (0) \u2212 x 0 \u2212 ) + \u03be \u2212 (\u2212\u1e91 (0) + x 0 \u2212 ) + L\u22121 i=1 j / \u2208Z +(i) Z \u2212(i) l (i) j <0<u (i) j \u00b5 (i) j (\u2212\u1e91 (i) j ) + \u03b3 (i) j (z (i) j \u2212\u1e91 (i) j ) + \u03bb (i) j (\u2212u (i) j z (i) j + (u (i) j \u2212 l (i) j )\u1e91 (i) j + u (i) j l (i) j ) + L\u22121 i=1 \uf8ee \uf8ef \uf8f0 z (i) j \u2208Z \u2212(i) \u03b2 (i) j z (i) j + z (i) j \u2208Z +(i) \u2212\u03b2 (i) j z (i) j \uf8f9 \uf8fa \uf8fb Subject to: \u03be + \u2265 0, \u03be \u2212 \u2265 0, \u00b5 \u2265 0, \u03b3 \u2265 0, \u03bb \u2265 0, \u03b2 \u2265 0(19)\nThe original minimization problem then becomes:\nmax \u03bd,\u03be + ,\u03be \u2212 ,\u00b5,\u03b3,\u03bb,\u03b2 min z,\u1e91 L(z,\u1e91, \u03bd, \u03be + , \u03be \u2212 , \u00b5, \u03b3, \u03bb, \u03b2)\nGiven fixed intermediate bounds l, u, the inner minimization is a linear optimization problem and we can simply transfer it to the dual form. To further simplify the formula, we introduce notations similar to those in [42], where\u03bd (i\u22121) = W (i) \u03bd (i) and \u03b1\n(i) j = \u03b3 (i) j \u00b5 (i) j +\u03b3 (i) j\n. Then the dual form can be written as Eq. 20.\nmax 0\u2264\u03b1\u22641,\u03b2\u22650 g(\u03b1, \u03b2), where g(\u03b1, \u03b2) = \u2212 L i=1 \u03bd (i) b (i) \u2212\u03bd (0) x 0 \u2212 ||\u03bd (0) || 1 \u00b7 + L\u22121 i=1 j / \u2208Z +(i) Z \u2212(i) l (i) j <0<u (i) j l (i) j [\u03bd (i) j ] +\nSubject to:\n\u03bd (L) = \u22121,\u03bd (i\u22121) = W (i) \u03bd (i) , i \u2208 {1, . . . , L} \u03bd (i) j = 0, when u (i) j \u2264 0, i \u2208 {1, . . . , L \u2212 1} \u03bd (i) j =\u03bd (i) j , when l (i) j \u2265 0, i \u2208 {1, . . . , L \u2212 1} [\u03bd (i) j ] + = u (i) j \u03bb (i) j , [\u03bd (i) j ] \u2212 = \u03b1 (i) j [\u03bd (i) j ] \u2212 \u03bb (i) j = [\u03bd (i) j ] + u (i) j \u2212l (i) j , \u03b1 (i) j = \u03b3 (i) j \u00b5 (i) j +\u03b3 (i) j \uf8fc \uf8f4 \uf8fd \uf8f4 \uf8fe when l (i) j < 0 < u (i) j , j / \u2208 Z +(i) Z \u2212(i) , i \u2208 {1, . . . , L \u2212 1} \u03bd (i) j = \u2212\u03b2 (i) j , j \u2208 Z \u2212(i) , i \u2208 {1, . . . , L \u2212 1} \u03bd (i) j = \u03b2 (i) j +\u03bd (i) j , j \u2208 Z +(i) , i \u2208 {1, . . . , L \u2212 1} \u00b5 \u2265 0, \u03b3 \u2265 0, \u03bb \u2265 0, \u03b2 \u2265 0, 0 \u2264 \u03b1 \u2264 1(20)\nSimilar to the dual form in [42] (our differences are highlighted in blue), the dual problem can be viewed in the form of another deep network by backward propagating \u03bd (L) to\u03bd (0) following the rules in Eq. 20. If we look closely at the conditions and coefficients when backward propagating \u03bd (i) j for j-th ReLU at layer i in Eq. 20, we can observe that they match exactly to the propagation of diagonal matrices D (i) , S (i) , and vector b (i) defined in Eq. 8 and Eq. 9. Therefore, using notations in Eq. 8 and Eq. 9 we can essentially simplify the dual LP problem in Eq. 20 to:\n\u03bd (L) = \u22121,\u03bd (i\u22121) = W (i) \u03bd (i) , \u03bd (i) = D (i)\u03bd(i) \u2212 \u03b2 (i) S (i) , i \u2208 {L, \u00b7 \u00b7 \u00b7 , 1} l (i) j <0<u (i) j j / \u2208Z +(i) Z \u2212(i) l (i) j [\u03bd (i) j ] + = \u2212\u03bd (i)T b (i) , j \u2208 {1, \u00b7 \u00b7 \u00b7 , d i }, i \u2208 {L \u2212 1, \u00b7 \u00b7 \u00b7 , 1}(21)\nThen we prove the key claim for this proof with induction where a (m) and P (m) are defined in Eq. 11 and Eq. 12:\u03bd \n(m) = \u2212a (m) \u2212 P (m)\u03b2(m+1)(22)\u03bd (m\u22121) = W (m) D (m)\u03bd(m) \u2212 \u03b2 (m) S (m) = \u2212W (m) D (m) a (m) \u2212 W (m) D (m) P (m)\u03b2(m+1) \u2212 W (m) \u03b2 (m) S (m) = \u2212a (m\u22121) \u2212 S (m) W (m) , P (m) D (m) W (m) \u03b2 (m) ,\u03b2 (m+1) = \u2212a (m\u22121) \u2212 P (m\u22121)\u03b2(m)\nTherefore, the claim Eq. 22 is proved with induction. Lastly, we prove the following claim where q (m) and c (m) are defined in Eq. 13 and Eq. 14.\n\u2212 L i=m+1 \u03bd (i) b (i) + L\u22121 i=m+1 l (i) j <0<u (i) j j / \u2208Z +(i) Z \u2212(i) l (i) j [\u03bd (i) j ] + = q (m) \u03b2 (m+1) + c (m)(23)\nThis claim can be proved by applying Eq. 21 and Eq. 22.\n\u2212 L i=m+1 \u03bd (i) b (i) + L\u22121 i=m+1 l (i) j <0<u (i) j j / \u2208Z +(i) Z \u2212(i) l (i) j [\u03bd (i) j ] + = \u2212 L i=m+1 D (i)\u03bd(i) \u2212 \u03b2 (i) S (i) b (i) + L i=m+2 \u2212\u03bd (i\u22121)T b (i\u22121) = L i=m+1 a (i) +\u03b2 (i+1) P (i) D (i) b (i) + \u03b2 (i) S (i) b (i) + L i=m+2 a (i\u22121) +\u03b2 (i) P (i\u22121) b (i\u22121) = L i=m+1\u03b2 (i) S (i) , P (i) D (i) b (i) + L i=m+2\u03b2 (i) P (i\u22121) b (i\u22121) + L i=m+1 a (i) D (i) b (i) + L i=m+2 a (i\u22121) b (i\u22121) = q (m) \u03b2 (m+1) + c (m)\nFinally, we apply claims Eq. 22 and Eq. 23 into the dual form solution Eq. 20 and prove the Theorem 3.2.\ng(\u03b1, \u03b2) = \u2212 L i=1 \u03bd (i) b (i) \u2212\u03bd (0) x 0 \u2212 ||\u03bd (0) || 1 \u00b7 + L\u22121 i=1 l (i) j <0<u (i) j j / \u2208Z +(i) Z \u2212(i) l (i) j [\u03bd (i) j ] + = \u2212||\u2212a (0) \u2212 P (0)\u03b2(1) || 1 \u00b7 + a (0) +\u03b2 (1) P (0) x 0 + q (0) \u03b2 (1) + c (0) = \u2212||a + P\u03b2 (1) || 1 \u00b7 + P x 0 + q \u03b2 (1) + a x 0 + c A more intuitive proof.\nHere we provide another intuitive proof showing why the dual form solution of verification objective in Eq. 20 is the same as the primal one in Thereom 3.1. d LP = g(\u03b1, \u03b2) is the dual objective for Eq. 10 with free variables \u03b1 and \u03b2. We want to show that the dual problem can be viewed in the form of backward propagating \u03bd (L) to\u03bd (0) following the same rules in \u03b2-CROWN. Salman et al. [30] showed that CROWN computes the same solution as the dual form in Wong and Kolter [42]:\u03bd (i) is corresponding to \u2212A (i) in CROWN (defined in the same way as in Eq. 10 but with \u03b2 (i+1) = 0) and \u03bd (i) is corresponding to \u2212A (i+1) D (i+1) . When the split constraints are introduced, extra terms for the dual variable \u03b2 modify \u03bd (i) (highlighted in blue in Eq. 20). The way \u03b2-CROWN modifies A (i+1) D (i+1) is exactly the same as the way \u03b2 (i) affects \u03bd (i) : when we split z\n(i) j \u2265 0, we add \u03b2 (i) j to the \u03bd (i)\nj in Wong and Kolter [42]; when we split z\n(i) j \u2265 0, we add \u2212\u03b2 (i) j to the \u03bd (i)\nj in Wong and Kolter [42] (\u03bd (i) j is 0 in this case because it is set to be inactive). To make this relationship more clear, we define a new variable \u03bd , and rewrite relevant terms involving \u03bd,\u03bd below:\n\u03bd (i) j = 0, j \u2208 Z \u2212(i) ; \u03bd (i) j =\u03bd (i) j , j \u2208 Z +(i) ; \u03bd (i)\nj is defined in the same way as in Eq. 20 for other cases \u03bd\n(i) j = \u2212\u03b2 (i) j + \u03bd (i) j , j \u2208 Z \u2212(i) ; \u03bd (i) j = \u03b2 (i) j + \u03bd (i) j , j \u2208 Z +(i) ; \u03bd (i) j = \u03bd (i) j , otherwis\u00ea \u03bd (i\u22121) = W (i) \u03bd (i) ;(24)\nIt is clear that \u03bd corresponds to the term \u2212(A (i+1) D (i+1) + \u03b2 (i+1) S (i+1) ) in Eq. 10, by noting that \u03bd (i) in [42] is equivalent to \u2212A (i+1) D (i+1) in CROWN and the choice of signs in S (i+1) reflects neuron split constraints. Thus, the dual formulation will produce the same results as Eq. 18, and thus also equivalent to Eq. 3. solver to obtain the optimal dual solution \u03bd * , \u03be * , \u00b5 * , \u03b3 * , \u03bb * , \u03b2 * . Then we can set \u03b1\n(i) j = \u03b3 (i) j * \u00b5 (i) j * +\u03b3 (i)\nj * , \u03b2 = \u03b2 * and plug them into Eq. 20 to get the optimal dual solution d * LP . Theorem 3.2 shows that, \u03b2-CROWN can compute the same objective d * LP given the same \u03b1\n(i) j = \u03b3 (i) j * \u00b5 (i) j * +\u03b3 (i) j * , \u03b2 = \u03b2 * , thus max 0\u2264\u03b1\u22641,\u03b2\u22650 g(\u03b1, \u03b2) \u2265 d * LP .\nOn the other hand, for any setting of \u03b1 and \u03b2, \u03b2-CROWN produces the same solution g(\u03b1, \u03b2) as the rewritten dual LP in Eq. 20, so g(\u03b1, \u03b2) \u2264 d * LP . Thus, we have max 0\u2264\u03b1\u22641,\u03b2\u22650 g(\u03b1, \u03b2) = d * LP . Finally, due to the strong duality in linear programming, p * LP = d * LP = max 0\u2264\u03b1\u22641,\u03b2\u22650 g(\u03b1, \u03b2).\n\nThe variables \u03b1 in \u03b2-CROWN can be translated to dual variables in LP as well. Given \u03b1 in \u03b2-CROWN, we can get the corresponding dual LP variables \u00b5, \u03b3 given \u03b1 by setting \u00b5\n(i) j = (1 \u2212 \u03b1 (i) j )[\u03bd (i) j ] \u2212 and \u03b3 (i) j = \u03b1 (i) j [\u03bd (i) j ] \u2212 .\nA.3 Proof for soundness and completeness Theorem 3.3. \u03b2-CROWN with branch and bound on splitting ReLU neurons is sound and complete.\n\nProof. Soundness. Branch and bound (BaB) with \u03b2-CROWN is sound because for each subdomain C i := {x \u2208 C, z \u2208 Z i }, we apply Theorem 3.1 to obtain a sound lower bound f Ci (the bound is valid for any \u03b2 \u2265 0). The final bound returned by BaB is min i f Ci which represents the worst case over all subdomains, and is a sound lower bound for x \u2208 C := \u222a i C i .\n\n\nCompleteness.\n\nTo show completeness, we need to solve Eq. 1 to its global minimum. When there are N unstable neurons, we have up to 2 N subdomains, and in each subdomain we have all unstable ReLU neurons split into one of the z (i)\nj \u2265 0 or z (i)\nj < 0 case. The final solution obtained by BaB is the min over these 2 N subdomains. To obtain the global minimum, we must ensure that in every of these 2 N subdomain we can solve Eq. 10 exactly.\n\nWhen all unstable neurons are split in a subdomain C i , the network becomes a linear network and neuron split constraints become linear constraints w.r.t. inputs. Under this case, an LP with Eq. 10 can solve the verification problem in C i exactly. In \u03b2-CROWN, we solve the subdomain using the usually non-concave formulation Eq. 12; however, in this case, it becomes concave in\u03b2 because no intermediate layer bounds are used (no \u03b1 and \u03b2 ) and no ReLU neuron is relaxed (no \u03b1), thus the only optimizable variable is \u03b2 (Eq. 12 becomes Eq. 8). Eq. 8 is concave in \u03b2 so (super)gradient ascent guarantees to converge to the global optimal \u03b2 * . To ensure convergence without relying on a preset learning rate, a line search can be performed in this case. Then, according to Corollary 3.2.1, this optimal \u03b2 * corresponds to the optimal dual variable for the LP in Eq. 10 and the objective is a global minimum of Eq. 10.\n\nB More details on \u03b2-CROWN with branch and bound (BaB)\n\n\nB.1 \u03b2-CROWN with branch and bound for complete verification\n\nWe list our \u03b2-CROWN with branch and bound based complete verifier (\u03b2-CROWN BAB) in Algorithm 1. The algorithm takes a target NN function f and a domain C as inputs. The subprocedure optimized_beta_CROWN optimizes\u03b1 and\u03b2 (free variables for computing intermediate layer bounds and last layer bounds) as Eq. 12 in Section 3.3. It operates in a batch and returns the lower and upper bounds for n selected subdomains simultaneously: a lower bound is obtained by optimizing Eq. 12 using \u03b2-CROWN and an upper bound can be the network prediction f (x * ) given the x * that minimizes Eq. 7 1 . Initially, we don't have any splits, so we only need to optimize\u03b1 to obtain f for x \u2208 C (Line 2). Then we utilize the power of GPUs to split in parallel and maintain a global set P storing all the sub-domains which does not satisfy f Ci < 0 (Line 5-10). Specifically, batch_pick_out extends branching strategy BaBSR [8] or FSB [11] in a parallel manner to select n (batch size) sub-domains in P and determine the corresponding ReLU neuron to split for each of them. If the length of P is less than n, then we reduce n to the length of P. batch_split splits each selected C i to two sub-domains C l i and C u i by forcing the selected unstable ReLU neuron to be positive and negative, respectively. Domain_Filter filters out verified sub-domains (proved with f Ci \u2265 0) and we insert the remaining 1 We want an upper bound of the objective in Eq. 1. Since Eq. 1 is an minimization problem, any feasible x produces an upper bound of the optimal objective. When Eq. 1 is solved exactly as f * (such as in the case where all neurons are split), we have f * = f = f . See also the discussions in Section I.1 of De Palma et al. [10].\n\nAlgorithm 1 \u03b2-CROWN with branch and bound for complete verification. Comments are in brown. 1: Inputs: f , C, n (batch size), \u03b4 (tolerance), \u03b7 (maximum length of sub-domains) 2:\n(f , f ) \u2190 optimized_beta_CROWN(f, [C])\nInitially there is no split, so optimization is done over\u03b1 3: P \u2190 (f , f , C) P is the set of all unverified sub-domains 4: while f < 0 and f \u2265 0 and f \u2212 f > \u03b4 and length(P) < \u03b7 do 5:\n\n(C1, . . . , Cn) \u2190 batch_pick_out(P, n) Pick sub-domains to split and removed them from P 6:\n\nC l 1 , C u 1 , . . . , C l n , C u n \u2190 batch_split(C1, . . . , Cn) Each Ci splits into two sub-domains C l i and C u\ni 7: f C l 1 , f C l 1 , f C u 1 , f C u 1 , . . . , f C l n , f C l n , f C u n , f C u n \u2190 optimized_beta_CROWN(f, C l 1 , C u 1 , .\n. . , C l n , C u n ) Compute lower and upper bounds by optimizing\u03b1 and\u03b2 mentioned in Section 3.3 in a batch 8:\nP \u2190 P Domain_Filter [f C l 1 , f C l 1 , C l 1 ], [f C u 1 , f C u 1 , C u 1 ], . . . , [f C l n , f C 1 n , C l n ], [f C u n , f C u n , C u n ]\nFilter out verified sub-domains, insert the left domains back to P 9:\n\nf \u2190 min{f\nC i | (f C i , f C i , Ci) \u2208 P}, i = 1, .\n. . , n To ease notation, Ci here indicates either C u i or C l i 10: \nf \u2190 min{f C i | (f C i , f C i , Ci) \u2208 P}, i = 1,\n\nB.2 Comparisons to other GPU based complete verifiers\n\nBunel et al. [6] proposed to reformulate the linear programming problem in Eq. 10 through Lagrangian decomposition. Eq. 10 is decomposed layer by layer, and each layer is solved with simple closed form solutions on GPUs. A Lagrangian is used to enforce the equality between the output of a previous layer and the input of a later layer. This optimization formulation has the same power as a LP (Eq. 10) under convergence. The main drawback of this approach is that it converges relatively slowly (it typically requires hundreds of iterations to converge to a solution similar to the solution of a LP), and it also cannot easily jointly optimize intermediate layer bounds. In Table 1 (PROX BABSR) and Figure 1 (BDD+ BABSR, which refers to the same method) we can see that this approach is relatively slow and has high timeout rates compared to other GPU accelerated complete verifiers. Recently, De Palma et al. [11] proposed a better branching strategy, filtered smart branching (FSB), to further improved verification performance of [6], but the Lagrangian Decomposition based incomplete verifier and the branch and bound procedure stay the same.\n\nDe Palma et al. [10] used a tighter convex relaxation [2] than the typical LP formulation in Eq. 10 for the incomplete verifier. This tighter relaxation may contain exponentially many constraints, and De Palma et al. [10] proposed to solve the verification problem in its dual form where each constraint becomes a dual variable. A small active set of dual variables is maintained during dual optimization to ensure efficiency. This tighter relaxation allows it to outperform [6], but it also comes with extra computational costs and difficulties for an efficient implementation (e.g. a \"masked\" forward/backward pass is needed which requires a customised low-level convolution implementation j \u2265 0, i \u2208 {1, \u00b7 \u00b7 \u00b7 , L \u2212 1}, j \u2208 Z +(i) and z (i) j < 0, i \u2208 {1, \u00b7 \u00b7 \u00b7 , L \u2212 1}, j \u2208 Z \u2212(i) ) in Eq. 10. The missing constraints lead to looser bounds and unnecessary branches. Additionally, using CROWN as the incomplete solver leads to incompleteness -even when all unstable ReLU neurons are split, Xu et al.\n\n[45] still cannot solve Eq. 1 to a global minimum, so a LP solver has to be used to check inconsistent splits and guarantee completeness. Our \u03b2-CROWN BaB overcomes these drawbacks: we consider per-neuron split constraints in \u03b2-CROWN which reduces the number of branches and solving time (Table 1). Most importantly, \u03b2-CROWN with branch and bound is sound and complete (Theorem 3.3) and we do not rely on any LP solvers. . This allows us to achieve tighter bounds and improve overall performance.\n\n\nB.3 Detection of Infeasibility\n\nMaximizing Eq. 8 with infeasible constraints leads to unbounded dual objective, which can be detected by checking if this optimized lower bound becomes greater than the upper bound (which is also maintained in BaB, see Alg.1 in Sec. B.1). For the robustness verification problem, a subdomain that has lower bound greater than 0 is dropped, which includes the unbounded case. Due to insufficient convergence, this cannot always detect infeasibility, but it does not affect soundness, as this infeasible subdomain only leads to worse overall lower bound in BaB. To guarantee completeness, we show that when all unstable neurons are split the problem is concave (see Section A.3); in this case, we can use line search to guarantee convergence when feasible, and detect infeasibility if the objective exceeds the upper bound (line search guarantees the objective can eventually exceed upper bound). In most real scenarios, the verifier either finishes or times out before all unstable neurons are split.\n\n\nC Details on Experimental Setup and Results\n\n\nC.1 Experimental Setup\n\nWe run our experiments on a machine with a single NVIDIA RTX 3090 GPU (24GB GPU memory), a AMD Ryzen 9 5950X CPU and 64GB memory. Our \u03b2-CROWN solver uses 1 CPU and 1 GPU only, except for the MLP models in Table 2 where 16 threads are used to compute intermediate layer bounds with Gurobi 2 . We use the Adam optimizer [21] to solve both\u03b1 and\u03b2 in Eq. 12 with 20 iterations. The learning rates are set as 0.1 and 0.05 for optimizing\u03b1 and\u03b2 respectively. We decay the learning rates with a factor of 0.98 per iteration. To maximize the benefits of parallel computing on GPU, we use batch sizes n =1024 for Base (CIFAR-10), Wide (CIFAR-10), Deep (CIFAR-10), CNN-A-Adv (MNIST) and ConvSmall (MNIST), n =2048 for ConvSmall (CIFAR-10), n =4096 for CNN-A-Adv (CIFAR-10), CNN-A-Adv-4 (CIFAR-10), CNN-A-Mix (CIFAR-10) and CNN-A-Mix-4 (CIFAR-10), n =256 for CNN-B-Adv (CIFAR-10) and CNN-B-Adv-4 (CIFAR-10), n =1024 for ConvBig (MNIST), n =10 for ConvBig (CIFAR-10), n =8 for ResNet (CIFAR-10) respectively. The CNN-A-Adv, CNN-A-Adv-4, CNN-A-Mix, CNN-A-Mix-4, CNN-B-Adv and CNN-B-Adv-4 models are obtained from the authors or [9] and are the same as the models used in their paper. We summarize the model structures in both incomplete verification and complete verification (Base, Wide and Deep) experiments in Table A1. Our code is available at http://PaperCode.cc/BetaCROWN.\n\n\nC.2 Additional Experiments\n\nMore results on incomplete verification In this paragraph we compare our \u03b2-CROWN FSB to many other incomplete verifiers. WK [42] and CROWN [46] are simple bound propagation methods;  (1,16,4) stands for a conventional layer with 1 input channel, 16 output channels and a kernel size of 4 \u00d7 4. Linear(1568, 100) stands for a fully connected layer with 1568 input features and 100 output features. We have ReLU activation functions between two consecutive layers. Table A2: Verified accuracy (%) and avg. per-example verification time (s) on 7 models from SDP-FO [9].  [26] reported better results on CNN-A-Mix. We found that their results were produced on a selection of 100 data points, and reruning their method using the same command on the same set of 200 random examples as used in other methods in this table produces different results, as reported here. \u00a7 We use our \u03b2-CROWN code and turn off \u03b2 optimization to emulate the algorithm used in [45]. This in fact leads to better performance than the original approach in [45] because we allow more \u03b1 variables to be optimized and our implementation is generally better.  Table A2). Ablation study on the impact of \u03b1, \u03b2, and their joint optimization We conduct the same experiments as in Table 1 but turn on or turn off \u03b1 and \u03b2 optimization to see the contribution of each part. As shown in Table A4, optimizing both \u03b1 and \u03b2 leads to optimal performance. Optimizing beta has a greater impact than optimizing \u03b1. Joint optimization is helpful for CIFAR10-Base and CIFAR10-Wide models, reducing the overall runtime. For simple models like CIFAR10-Deep,  Table A3: Average runtime and average number of branches on three CIFAR-10 models over 100 properties (the same setting as in Table 1) by using different numbers of CPU cores, as well as using a single GPU. disabling joint optimization can help slightly because this model is very easy to verify (within a few seconds) and using looser bounds reduces verification cost. \n\n\nto represent the pre-activation and post-activation values of the j-th neuron in the i-th layer. Neural network verification seeks the solution of the optimization problem in Eq. 1:\n\n.\nAny \u03b2 (L\u22121) \u2265 0 yields a lower bound for the constrained optimization problem. Then we substitute z (L\u22121) with W (L\u22121)\u1e91(L\u22122) + b (L\u22121) for next layer:min x\u2208C,z\u2208Z\n\nj\n, the intermediate layer bounds for each neuron z\n\n\nare also computed using \u03b2-CROWN. To obtain l (i) j , we set f (x) := z\n\nj\nwe simply set f (x) := \u2212z (i) j (x). Importantly, during solving these intermediate layer bounds, the \u03b1 and \u03b2 are independent sets of variables, not the same ones for the objective f (x) := z (L) . Since g is a function of l (i) j , it is also a function of \u03b1 and \u03b2 . In fact, there are a total of L\u22121 i=1 d i intermediate layer neurons, and each neuron is associated with a set of independent \u03b1 and \u03b2 variables. Optimizing these variables allowing us to tighten the relaxations on unstable ReLU neurons (which depend on l\n\nFigure 1 :\n1Percentage of solved properties with growing running time. \u03b2-CROWN FSB (light green) and \u03b2-CROWN BaBSR (dark green) clearly lead in all 3 settings and solve over 90% properties within 10 seconds.\n\ny\nFO results are directly from their paper due to its very long running time (>20h per example).\u2020 PRIMA experiments were done using commit 396dc7a, released on June 4, 2021. PRIMA and \u03b2-CROWN FSB results are on the same set of 200 examples (first 200 examples of CIFAR-10 dataset) and we don't run verifiers on examples that are classified incorrectly or can be attacked by a 200-step PGD. \u03b2-CROWN uses 1 GPU and 1 CPU; PRIMA uses 1 GPU and 20 CPUs. (x), where z (L) is the logit layer output, y and y are the true label and the runner-up label. For each test image, a 200-step PGD attack [24] provides an adversarial upper bound f of the optimal objective: f * \u2264 f . Verifiers, on the other hand, can provide a verified lower bound f \u2264 f * . Bounds from tighter verification methods lie closer to line y = x inFigure 2.Figure 2shows that on both PGD adversarially trained networks, \u03b2-CROWN FSB consistently outperforms SDP-FO for all 100 random test images. Importantly, for each point on the plots, \u03b2-CROWN FSB needs 3 minutes while SDP-FO needs 178 minutes on average. LP verifier with triangle relaxations produces much looser bounds than \u03b2-CROWN FSB and SDP-FO. Additional results are in Appendix C.2.\n\n/ 255 Figure 2 :\n2552) MNIST CNN-A-Adv, runner-up targets, = 0.3 (b) CIFAR CNN-B-Adv, runner-up targets, = 2Verified lower bound v.s. PGD adversarial upper bound. A lower bound closer to the upper bound (closer to the line y = x) is better. \u03b2-CROWN FSB uses 3mins while SDP-FO needs 2 to 3 hours per point.\n\n[ 45 ]\n45K. Xu, H. Zhang, S. Wang, Y. Wang, S. Jana, X. Lin, and C.-J. Hsieh. Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. International Conference on Learning Representations (ICLR), 2021. [46] H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel. Efficient neural network robustness certification with general activation functions. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [47] H. Zhang, H. Chen, C. Xiao, B. Li, D. Boning, and C.-J. Hsieh. Towards stable and efficient training of verifiably robust neural networks. In International Conference on Learning Representations (ICLR), 2020. [48] H. Zhang, H. Chen, C. Xiao, B. Li, M. Liu, D. Boning, and C.-J. Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations. Advances in Neural Information Processing Systems (NeurIPS), 2020. [49] H. Zhang, H. Chen, D. Boning, and C.-J. Hsieh. Robust reinforcement learning on state observations with learned optimal adversary. International Conference on Learning Representations (ICLR), 2021. A Proofs for \u03b2-CROWN A.1 Proofs for deriving \u03b2-CROWN using bound propagation Lemma 2.1 is from part of the proof of the main theorem in Zhang et al. [46]. Here we present it separately to use it as an useful subprocedure for our later proofs. Lemma 2.1 (Relaxation of a ReLU layer in CROWN). Given two vectors\n\n\nNo maximization is needed and a (m) = \u2126(L, L)W (L) = W (L) , c (m) = L i=L \u2126(L, i)b (i) = b (L) . Other terms are zero. In Section 3.1 we have shown the intuition of the proof by demonstrating how to derive the bounds from layer\u1e91 (L\u22121) to\u1e91 (L\u22122) . The case for m = L \u2212 2 is presented in Eq. 6. Now we show the induction from\u1e91 (m) to\u1e91 (m\u22121) . Starting from Eq. 15, since\u1e91 (m) = ReLU(z (m) ) we apply Lemma 2.1 by setting w = a (m) + P (m)\u03b2(m+1) := A (m) . It is easy to show that A (m) can also be equivalently and recursively defined in Eq. 10 (see Lemma A.2). Based on Lemma 2.1 we have D (m) and b (m) defined as in Eq. 8 and Eq. 9, so Eq. 15 becomes min x\u2208C z\u2208Z\n\n\nWhen m = L \u2212 1, we can have\u03bd (L\u22121) = \u2212a (L\u22121) \u2212 P (L\u22121)\u03b2(L) = \u2212 \u2126(L, L)W (L) \u2212 0 = \u2212W (L) which is true according to Eq. 21.Now we assume that\u03bd (m) = \u2212a (m) \u2212 P (m)\u03b2(m+1) holds, and we show that\u03bd (m\u22121) = \u2212a (m\u22121) \u2212 P (m\u22121)\u03b2(m) will hold as well:\n\nCorollary 3.2. 1 .\n1When \u03b1 and \u03b2 are optimally set, \u03b2-CROWN produces the same solution as LP with split constraints when intermediate bounds l, u are fixed. Formally, max 0\u2264\u03b1\u22641,\u03b2\u22650 g(\u03b1, \u03b2) = p * LP where p * LP is the optimal objective of Eq. 10. Proof. Given fixed intermediate layer bounds l and u, the dual form of the verification problem in Eq. 10 is a linear programming problem with dual variables defined in Eq. 19. Suppose we use an LP\n\n\nAnother difference between Xu et al. [45] and our method is the joint optimization of intermediate layer bounds (Section 3.3). Although [45] also optimized intermediate layer bounds, they only optimize \u03b1 and do not have \u03b2, and they share the same variable \u03b1 for all intermediate layer bounds and final bounds, with a total of O(Ld) variables to optimize. Our analysis in Section 3.3 shows that there are in fact, O(L 2 d 2 ) free variables to optimize, and we share less variables as in Xu et al. [45]\n\n\n32.8 >25h 46.0 >25h 39.6 >25h 40.0 >25h 39.6 >25h 47.Names in parentheses are methods to compute intermediate layer bounds for the LP verifier. * SDP-FO results are directly from their paper due to the very long running time. All other methods are tested on the same set of 200 examples. \u2020 The implementation of BigM+A.Set BaBSR is not compatible with CNN-B-Adv and CNN-B-Adv4 models which have an convolution with asymmetric padding. \u2021 A recent version (Oct 26, 2021) of\n\nFigure A1 :\nA1Verified lower bound on f (x) by \u03b2-CROWN FSB compared against incomplete LP verifiers using different intermediate layer bounds obtained from [42] (denoted as LP (WK)), CROWN [46] (denoted as LP (CROWN)), and jointly optimized intermediate bounds in Eq. 12 (denoted as LP (CROWN-OPT)), v.s. the adversarial upper bound on f (x) found by PGD. LPs need much longer time to solve than \u03b2-CROWN on CIFAR-10 models (see\n\n\n) MNIST CNN-A-Adv, runner-up targets, = 0.3 (b) CIFAR CNN-B-Adv, runner-up targets, = 2/255 models. The speedup on multi-core CPU is not obvious, possibly due to the limitation of underlying implementations of PyTorch.\n\nFigure A2 :\nA2For the CNN-A-Adv (MNIST) model, we randomly select four examples from the incomplete verification experiment and plot the lower bound v.s. time (in 180 seconds) of \u03b2-CROWN BABSR and BIGM+A.SET BABSR. Larger lower bounds are better. \u03b2-CROWN BaBSR improves bound noticeably faster in all four situations.\n\nTable 1 :\n1Average runtime and average number of branches on three CIFAR-10 models over 100 properties. * OVAL (BDD+ GNN) and ERAN results are from VNN-COMP 2020 report[22]. Other results were reported by their authors.CIFAR-10 Base \nCIFAR-10 Wide \nCIFAR-10 Deep \n\nMethod \ntime(s) \nbranches \n%timeout time(s) \nbranches \n%timeout time(s) \nbranches %timeout \n\nBaBSR [7] \n2367.78 \n1020.55 \n36.00 \n2871.14 \n812.65 \n49.00 \n2750.75 \n401.28 \n39.00 \nMIPplanet [15] \n2849.69 \n-\n68.00 \n2417.53 \n-\n46.00 \n2302.25 \n-\n40.00 \nERAN  *  [35, 33, 36, 34] \n805.94 \n-\n5.00 \n632.20 \n-\n9.00 \n545.72 \n-\n0.00 \nGNN-online [23] \n1794.85 \n565.13 \n33.00 \n1367.38 \n372.74 \n15.00 \n1055.33 \n131.85 \n4.00 \nBDD+ BaBSR [6] \n807.91 195480.14 \n20.00 \n505.65 \n74203.11 \n10.00 \n266.28 12722.74 \n4.00 \nOVAL (BDD+ GNN)  *  [6, 23] 662.17 \n67938.38 \n16.00 \n280.38 \n17895.94 \n6.00 \n94.69 \n1990.34 \n1.00 \nA.set BaBSR [10] \n381.78 \n12004.60 \n7.00 \n165.91 \n2233.10 \n3.00 \n190.28 \n2491.55 \n2.00 \nBigM+A.set BaBSR [10] \n390.44 \n11938.75 \n7.00 \n172.65 \n4050.59 \n3.00 \n177.22 \n3275.25 \n2.00 \nFast-and-Complete [45] \n695.01 119522.65 \n17.00 \n495.88 \n80519.85 \n9.00 \n105.64 \n2455.11 \n1.00 \nBaDNB (BDD+ FSB)[11] \n309.29 \n38239.04 \n7.00 \n165.53 \n11214.44 \n4.00 \n10.50 \n368.16 \n0.00 \n\u03b2-CROWN BaBSR \n226.06 509608.50 \n6.00 \n118.26 217691.24 \n3.00 \n6.12 \n204.66 \n0.00 \n\u03b2-CROWN FSB \n118.23 208018.21 \n3.00 \n78.32 \n116912.57 \n2.00 \n5.69 \n41.12 \n0.00 \n\n\n\nTable 2 :\n2Verified accuracy (%) and avg. time (s) of 1000 images evaluated on the ERAN models in[35,37,26]. kPoly, OptC2V and PRIMA are strong incomplete verifiers that can break the convex relaxation barrier[30]. The average time reported by us excludes examples that are classified incorrectly. Same settings as[35,37,26]) Verified% Time (s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) bound * CROWN/DeepPoly evaluated on CPU. \u2020 PRIMA is a concurrent work and its results are from [26] (Oct 26, 2021 version), except that ResNet results are from personal communications with the authors due to a different input normalization used. \u2021 Because these MLP models are fairly small, some of their intermediate layer bounds are computed by mixed integer programming (MIP) using 80% time budget before branch and bound starts and \u03b2-CROWN FSB is used during the branch and bound process. We find that tighter intermediate bounds by MIP is beneficial for these small MLP models.Dataset \nModel \nCROWN/DeepPoly  *  [36] \nkPoly [35] \nOptC2V [37] \nPRIMA  \u2020 [26] \n\u03b2-CROWN FSB Upper \n(MNIST \n\nMLP 5 \u00d7 100  \u2021 \n16.0 \n0.7 \n44.1 \n307 \n42.9 \n137 \n51.0 \n159 \n69.9 \n102 \n84.2 \nMLP 8 \u00d7 100 \n18.2 \n1.4 \n36.9 \n171 \n38.4 \n759 \n42.8 \n301 \n62.0 \n103 \n82.0 \nMLP 5 \u00d7 200 \n29.2 \n2.4 \n57.4 \n187 \n60.1 \n403 \n69.0 \n224 \n77.4 \n86 \n90.1 \nMLP 8 \u00d7 200 \n25.9 \n5.6 \n50.6 \n464 \n52.8 \n3451 \n62.4 \n395 \n73.5 \n95 \n91.1 \nConvSmall \n15.8 \n3 \n34.7 \n477 \n43.6 \n55 \n59.8 \n42 \n72.7 \n7.0 \n73.2 \nConvBig \n71.1 \n21 \n73.6 \n40 \n77.1 \n102 \n77.5 \n11 \n79.3 \n3.1 \n80.4 \n\nCIFAR \n\nConvSmall \n35.9 \n4 \n39.9 \n86 \n39.8 \n105 \n44.6 \n13 \n46.3 \n6.8 \n48.1 \nConvBig \n42.1 \n43 \n45.9 \n346 \nNo public code \n48.3 \n176 \n51.6 \n15.3 \n55.0 \nResNet \n24.1 \n1 \n24.5 \n91 \ncannot run \n24.8 \n1.7 \n24.8 \n1.6 \n24.8 \n\n\n\n\nIn Section 3.2, we show that \u03b2-CROWN is solving an equivalent problem of the LP verifier with neuron split constraints. (3) In Section 3.3, we show that \u03b2-CROWN can jointly optimize intermediate layer bounds and achieve tighter bounds than typical LP verifiers using fixed intermediate layer bounds.Limitations. Our verifier has several limitations which are commonly shared by most existing BaB-based complete verifiers. First, we focused on ReLU which can be split into two linear cases. For other non-piecewise linear activation functions, although it is still possible to conduct branch and bound, it is difficult to guarantee completeness. Second, we discussed only the norm perturbations for input domains. In practice, the threat model may involve complicated and nonconvex perturbation specifications. Third, although our GPU accelerated verifier outperforms existing ones, all BaB based verifiers, including ours, are still limited to relatively small models far from the ImageNet scale. Finally, we have only demonstrated robustness verification of image classification tasks, and generalizing it to give verification guarantees for other tasks such as robust deep reinforcement learning[27, 48, 49]  is an interesting direction for future work.\n\n\nones to P. The loop breaks if the property is proved (f \u2265 0), or a counter-example is found in any sub-domain (f < 0), or the lower bound f and upper bound f are sufficiently close, or the length of sub-domains P reaches a desired threshold \u03b7 (maximum memory limit).Note that for models evaluated in our paper, we find that computing intermediate layerbounds in every iteration at line 7 is too costly (although it is possible and supported) so we only compute intermediate layer bounds once at line 2. At line 7, only the neuron with split constraints have their intermediate layer bounds updated, and other intermediate bounds are not recomputed. This makes the intermediate layer bounds looser but it allows us to quickly explore a large number of nodes on the branch and bound search tree and is overall beneficial for verifying most models. A similar observation was also found in De Palma et al. [10] (Section 5.1.1).. . . , n \n\n11: Outputs: f , f \n\n\n\n\n). Additionally, De Palma et al. [10] did not optimize intermediate layer bounds jointly. Xu et al. [45] used CROWN [46] (categorized as a linear relaxation based perturbation analysis (LiRPA) algorithm) as the incomplete solver in BaB. Since CROWN cannot encode neural split constraints, Xu et al. [45] essentially solve Eq. 10 without neuron split constraints (z(i) \n\n\n\nTable A1 :\nA1Model structures used in our experiments. For example, Conv\n\nTable A4 :\nA4Ablation study on the CIFAR-10 Base, Wide and Deep models (the same setting as inTable 1), including combinations of optimizing or not optimizing \u03b1 and/or \u03b2 variables, and using or not using joint optimization for intermediate layer bounds.CIFAR-10 Base \nCIFAR-10 Wide \nCIFAR-10 Deep \n\njoint opt \u03b1 \u03b2 time(s) \nbranches \n%timeout time(s) \nbranches \n%timeout time(s) branches %timeout \n\n233.86 233233.70 \n6.00 148.46 113017.10 \n4.00 \n5.77 \n260.18 \n0.00 \n174.10 163037.05 \n4.00 102.65 \n86571.18 \n2.00 \n5.73 \n134.76 \n0.00 \n139.83 133346.44 \n3.00 \n91.01 \n73713.30 \n2.00 \n5.22 \n100.44 \n0.00 \n\n163.69 160058.80 \n4.00 149.00 115509.71 \n4.00 \n8.58 \n65.70 \n0.00 \n162.95 150631.49 \n4.00 \n89.22 \n72479.96 \n2.00 \n8.38 \n52.26 \n0.00 \n118.23 208018.21 \n3.00 \n78.32 116912.57 \n2.00 \n5.69 \n41.12 \n0.00 \n\nNote that our \u03b2-CROWN verifier does not rely on MILP/LP solvers. For these very small MLP models, we find that a MILP solver can actually compute intermediate layer bounds pretty quickly and using these tighter intermediate bounds are quite helpful for \u03b2-CROWN. This also enables us to utilize both CPUs and GPUs on a machine. For all other models, intermediate layer bounds are computed through optimizing Eq. 12. Practically, MILP is not scalable beyond these very small MLP models and these small models are not the main focus of this work.\nThe concurrent work BaDNB (BDD+ FSB) does not have public available code when our paper was submitted.\nAcknowledgementAdditional results on the tightness of verification. InFigure A1, we include LP based verifiers as baselines and compare the lower bound from verification to the upper bound obtained by PGD. The LP verifiers use the triangle relaxations described in Section 2.1, with intermediate layer bounds from WK[42],CROWN [46]and CROWN with joint optimization on intermediate layer bounds (denoted as CROWN-OPT). We find that tighter intermediate layer bounds obtained by CROWN can greatly improve the performance of the LP verifier compared to those using looser ones obtained by Wong and Kolter[42]. Furthermore, using intermediate layer bounds computed by joint optimization can achieve additional improvements. However, our branch and bound with \u03b2-CROWN can significantly outperform these LP verifiers. This shows that BaB is an effective approach for incomplete verification, outperforming the bounds produced by a single LP.Lower bound improvements over time InFigure A2, we plot lower bound values vs. time for \u03b2-CROWN BABSR and BIGM+A.SET BABSR (one of the most competitive methods inTable 1) on the CNN-A-Adv (MNIST) model.Figure A2shows that branch and bound can indeed quickly improve the lower bound, and our \u03b2-CROWN BABSR is consistently faster than BIGM+A.SET BABSR. In contrast, SDP-FO[9], which typically requires 2 to 3 hours to converge, can only provide very loose bounds during the first 3 minutes of optimization (out of the range on these figures).Ablation study of running time on GPUs and CPUs We conduct the same experiments as inTable 1but run \u03b2-CROWN FSB on CPUs instead of GPUs. As shown inTable A3, our method is strong even on a single CPU, showing that the good performance does not only come from GPU acceleration; our efficient algorithm also contributes to our success. On the other hand, using GPU can boost the performance by at least 2x. Importantly, the models evaluated in this table are very small ones. Massive parallelization on GPU will lead to more significant acceleration on larger\nOptimization and abstraction: A synergistic approach for analyzing neural network robustness. G Anderson, S Pailoor, I Dillig, S Chaudhuri, Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI). the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)G. Anderson, S. Pailoor, I. Dillig, and S. Chaudhuri. Optimization and abstraction: A synergistic approach for analyzing neural network robustness. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 2019.\n\nStrong convex relaxations and mixed-integer programming formulations for trained neural networks. R Anderson, J Huchette, C Tjandraatmadja, J P Vielma, Mathematical Programming. R. Anderson, J. Huchette, C. Tjandraatmadja, and J. P. Vielma. Strong convex relaxations and mixed-integer programming formulations for trained neural networks. Mathematical Programming, 2020.\n\nThe second international verification of neural networks competition. S Bak, C Liu, T Johnson, arXiv:2109.00498Summary and results. arXiv preprintS. Bak, C. Liu, and T. Johnson. The second international verification of neural networks competition (vnn-comp 2021): Summary and results. arXiv preprint arXiv:2109.00498, 2021.\n\nAdversarial training and provable defenses: Bridging the gap. M Balunovic, M Vechev, International Conference on Learning Representations (ICLR. 2020M. Balunovic and M. Vechev. Adversarial training and provable defenses: Bridging the gap. In International Conference on Learning Representations (ICLR), 2020.\n\nEfficient verification of relu-based neural networks via dependency analysis. E Botoeva, P Kouvaros, J Kronqvist, A Lomuscio, R Misener, AAAI Conference on Artificial Intelligence (AAAI). 2020E. Botoeva, P. Kouvaros, J. Kronqvist, A. Lomuscio, and R. Misener. Efficient verification of relu-based neural networks via dependency analysis. In AAAI Conference on Artificial Intelligence (AAAI), 2020.\n\nLagrangian decomposition for neural network verification. R Bunel, A Palma, A Desmaison, K Dvijotham, P Kohli, P H S Torr, M P Kumar, Conference on Uncertainty in Artificial Intelligence (UAI). 2020R. Bunel, A. De Palma, A. Desmaison, K. Dvijotham, P. Kohli, P. H. S. Torr, and M. P. Kumar. Lagrangian decomposition for neural network verification. Conference on Uncertainty in Artificial Intelligence (UAI), 2020.\n\nBranch and bound for piecewise linear neural network verification. R Bunel, J Lu, I Turkaslan, P Kohli, P Torr, P Mudigonda, Journal of Machine Learning Research. 2020JMLRR. Bunel, J. Lu, I. Turkaslan, P. Kohli, P. Torr, and P. Mudigonda. Branch and bound for piecewise linear neural network verification. Journal of Machine Learning Research (JMLR), 2020.\n\nA unified view of piecewise linear neural network verification. R R Bunel, I Turkaslan, P Torr, P Kohli, P K Mudigonda, Advances in Neural Information Processing Systems (NeurIPS). R. R. Bunel, I. Turkaslan, P. Torr, P. Kohli, and P. K. Mudigonda. A unified view of piecewise linear neural network verification. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nEnabling certification of verification-agnostic networks via memory-efficient semidefinite programming. S Dathathri, K Dvijotham, A Kurakin, A Raghunathan, J Uesato, R R Bunel, S Shankar, J Steinhardt, I Goodfellow, P S Liang, Advances in Neural Information Processing Systems (NeurIPS). 2020S. Dathathri, K. Dvijotham, A. Kurakin, A. Raghunathan, J. Uesato, R. R. Bunel, S. Shankar, J. Steinhardt, I. Goodfellow, P. S. Liang, et al. Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming. Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nScaling the convex barrier with active sets. A Palma, H S Behl, R Bunel, P H S Torr, M P Kumar, International Conference on Learning Representations (ICLR. 2021A. De Palma, H. S. Behl, R. Bunel, P. H. S. Torr, and M. P. Kumar. Scaling the convex barrier with active sets. International Conference on Learning Representations (ICLR), 2021.\n\nImproved branch and bound for neural network verification via lagrangian decomposition. A Palma, R Bunel, A Desmaison, K Dvijotham, P Kohli, P H Torr, M P Kumar, arXiv:2104.06718arXiv preprintA. De Palma, R. Bunel, A. Desmaison, K. Dvijotham, P. Kohli, P. H. Torr, and M. P. Kumar. Improved branch and bound for neural network verification via lagrangian decomposition. arXiv preprint arXiv:2104.06718, 2021.\n\nOutput range analysis for deep feedforward neural networks. S Dutta, S Jha, S Sankaranarayanan, A Tiwari, NASA Formal Methods Symposium. S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari. Output range analysis for deep feedforward neural networks. In NASA Formal Methods Symposium, 2018.\n\nA dual approach to scalable verification of deep networks. K Dvijotham, R Stanforth, S Gowal, T Mann, P Kohli, Conference on Uncertainty in Artificial Intelligence (UAI). K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli. A dual approach to scalable verification of deep networks. Conference on Uncertainty in Artificial Intelligence (UAI), 2018.\n\nEfficient neural network verification with exactness characterization. K D Dvijotham, R Stanforth, S Gowal, C Qin, S De, P Kohli, Conference on Uncertainty in Artificial Intelligence (UAI). 2020K. D. Dvijotham, R. Stanforth, S. Gowal, C. Qin, S. De, and P. Kohli. Efficient neural network verification with exactness characterization. In Conference on Uncertainty in Artificial Intelligence (UAI), 2020.\n\nFormal verification of piece-wise linear feed-forward neural networks. R Ehlers, International Symposium on Automated Technology for Verification and Analysis. ATVAR. Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Interna- tional Symposium on Automated Technology for Verification and Analysis (ATVA), 2017.\n\nSafety verification and robustness analysis of neural networks via quadratic constraints and semidefinite programming. M Fazlyab, M Morari, G J Pappas, IEEE Transactions on Automatic Control. M. Fazlyab, M. Morari, and G. J. Pappas. Safety verification and robustness analysis of neural networks via quadratic constraints and semidefinite programming. IEEE Transactions on Automatic Control, 2020.\n\nAi2: Safety and robustness certification of neural networks with abstract interpretation. T Gehr, M Mirman, D Drachsler-Cohen, P Tsankov, S Chaudhuri, M Vechev, 2018 IEEE Symposium on Security and Privacy (SP. IEEET. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE Symposium on Security and Privacy (SP). IEEE, 2018.\n\nOn the effectiveness of interval bound propagation for training verifiably robust models. S Gowal, K Dvijotham, R Stanforth, R Bunel, C Qin, J Uesato, T Mann, P Kohli, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato, T. Mann, and P. Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.\n\nSafety verification of deep neural networks. X Huang, M Kwiatkowska, S Wang, M Wu, International Conference on Computer Aided Verification (CAV). X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety verification of deep neural networks. In International Conference on Computer Aided Verification (CAV), 2017.\n\nReluplex: An efficient smt solver for verifying deep neural networks. G Katz, C Barrett, D L Dill, K Julian, M J Kochenderfer, International Conference on Computer Aided Verification (CAV. G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification (CAV), 2017.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, International Conference on Learning Representations (ICLR). D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015.\n\n. C Liu, T Johnson, C. Liu and T. Johnson. Vnn comp 2020. URL https://sites.google.com/view/vnn20/ vnncomp.\n\nJ Lu, M P Kumar, Neural network branching for neural network verification. International Conference on Learning Representation (ICLR. 2020J. Lu and M. P. Kumar. Neural network branching for neural network verification. International Conference on Learning Representation (ICLR), 2020.\n\nTowards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, International Conference on Learning Representations (ICLR). A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018.\n\nDifferentiable abstract interpretation for provably robust neural networks. M Mirman, T Gehr, M Vechev, International Conference on Machine Learning (ICML). M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract interpretation for provably robust neural networks. In International Conference on Machine Learning (ICML), 2018.\n\nM N M\u00fcller, G Makarchuk, G Singh, M P\u00fcschel, M Vechev, arXiv:2103.03638Precise multi-neuron abstractions for neural network certification. arXiv preprintM. N. M\u00fcller, G. Makarchuk, G. Singh, M. P\u00fcschel, and M. Vechev. Precise multi-neuron abstractions for neural network certification. arXiv preprint arXiv:2103.03638, 2021.\n\nRobust deep reinforcement learning with adversarial attacks. A Pattanaik, Z Tang, S Liu, G Bommannan, G Chowdhary, arXiv:1712.03632arXiv preprintA. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and G. Chowdhary. Robust deep reinforcement learning with adversarial attacks. arXiv preprint arXiv:1712.03632, 2017.\n\nSemidefinite relaxations for certifying robustness to adversarial examples. A Raghunathan, J Steinhardt, P S Liang, Advances in Neural Information Processing Systems (NeurIPS). A. Raghunathan, J. Steinhardt, and P. S. Liang. Semidefinite relaxations for certifying robustness to adversarial examples. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nFast neural network verification via shadow prices. V R Royo, R Calandra, D M Stipanovic, C Tomlin, arXiv:1902.07247arXiv preprintV. R. Royo, R. Calandra, D. M. Stipanovic, and C. Tomlin. Fast neural network verification via shadow prices. arXiv preprint arXiv:1902.07247, 2019.\n\nA convex relaxation barrier to tight robustness verification of neural networks. H Salman, G Yang, H Zhang, C.-J Hsieh, P Zhang, Advances in Neural Information Processing Systems (NeurIPS). H. Salman, G. Yang, H. Zhang, C.-J. Hsieh, and P. Zhang. A convex relaxation barrier to tight robustness verification of neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nRobustness verification for transformers. Z Shi, H Zhang, K.-W Chang, M Huang, C.-J Hsieh, International Conference on Learning Representations (ICLR). 2020Z. Shi, H. Zhang, K.-W. Chang, M. Huang, and C.-J. Hsieh. Robustness verification for transformers. In International Conference on Learning Representations (ICLR), 2020.\n\nFast certified robust training via better initialization and shorter warmup. Z Shi, Y Wang, H Zhang, J Yi, C.-J Hsieh, Advances in Neural Information Processing Systems (NeurIPS). 2021Z. Shi, Y. Wang, H. Zhang, J. Yi, and C.-J. Hsieh. Fast certified robust training via better initial- ization and shorter warmup. Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nFast and effective robustness certification. G Singh, T Gehr, M Mirman, M P\u00fcschel, M Vechev, Advances in Neural Information Processing Systems (NeurIPS). G. Singh, T. Gehr, M. Mirman, M. P\u00fcschel, and M. Vechev. Fast and effective robustness certification. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nBoosting robustness certification of neural networks. G Singh, T Gehr, M P\u00fcschel, M Vechev, International Conference on Learning Representations. G. Singh, T. Gehr, M. P\u00fcschel, and M. Vechev. Boosting robustness certification of neural networks. In International Conference on Learning Representations, 2018.\n\nBeyond the single neuron convex barrier for neural network certification. G Singh, R Ganvir, M P\u00fcschel, M Vechev, Advances in Neural Information Processing Systems (NeurIPS). G. Singh, R. Ganvir, M. P\u00fcschel, and M. Vechev. Beyond the single neuron convex barrier for neural network certification. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nAn abstract domain for certifying neural networks. G Singh, T Gehr, M P\u00fcschel, M Vechev, Proceedings of the ACM on Programming Languages (POPL). the ACM on Programming Languages (POPL)G. Singh, T. Gehr, M. P\u00fcschel, and M. Vechev. An abstract domain for certifying neural networks. Proceedings of the ACM on Programming Languages (POPL), 2019.\n\nThe convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification. C Tjandraatmadja, R Anderson, J Huchette, W Ma, K Patel, J P Vielma, Advances in Neural Information Processing Systems (NeurIPS). 2020C. Tjandraatmadja, R. Anderson, J. Huchette, W. Ma, K. Patel, and J. P. Vielma. The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification. Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nEvaluating robustness of neural networks with mixed integer programming. V Tjeng, K Xiao, R Tedrake, International Conference on Learning Representations (ICLR). V. Tjeng, K. Xiao, and R. Tedrake. Evaluating robustness of neural networks with mixed integer programming. International Conference on Learning Representations (ICLR), 2019.\n\nS Wang, Y Chen, A Abdou, S Jana, Mixtrain, arXiv:1811.02625Scalable training of formally robust neural networks. arXiv preprintS. Wang, Y. Chen, A. Abdou, and S. Jana. Mixtrain: Scalable training of formally robust neural networks. arXiv preprint arXiv:1811.02625, 2018.\n\nEfficient formal safety analysis of neural networks. S Wang, K Pei, J Whitehouse, J Yang, S Jana, Advances in Neural Information Processing Systems (NeurIPS). S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Efficient formal safety analysis of neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nFormal security analysis of neural networks using symbolic intervals. S Wang, K Pei, J Whitehouse, J Yang, S Jana, USENIX Security Symposium. S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Formal security analysis of neural networks using symbolic intervals. In USENIX Security Symposium, 2018.\n\nProvable defenses against adversarial examples via the convex outer adversarial polytope. E Wong, Z Kolter, International Conference on Machine Learning (ICML). E. Wong and Z. Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning (ICML), 2018.\n\nScaling provable adversarial defenses. E Wong, F Schmidt, J H Metzen, J Z Kolter, Advances in Neural Information Processing Systems (NeurIPS). E. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling provable adversarial defenses. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nAutomatic perturbation analysis for scalable certified robustness and beyond. K Xu, Z Shi, H Zhang, Y Wang, K.-W Chang, M Huang, B Kailkhura, X Lin, C.-J Hsieh, Advances in Neural Information Processing Systems (NeurIPS), 2020. Model name Model structure CNN-A-Adv (MNIST) Conv(1, 16, 4) -Conv. 16, 32, 4) -Linear(1568, 100) -Linear(100, 10K. Xu, Z. Shi, H. Zhang, Y. Wang, K.-W. Chang, M. Huang, B. Kailkhura, X. Lin, and C.-J. Hsieh. Automatic perturbation analysis for scalable certified robustness and beyond. Advances in Neural Information Processing Systems (NeurIPS), 2020. Model name Model structure CNN-A-Adv (MNIST) Conv(1, 16, 4) -Conv(16, 32, 4) -Linear(1568, 100) -Linear(100, 10)\n\nConvSmall (MNIST) Conv(1, 16, 4) -Conv. 16, 32, 4) -Linear(800, 100) -Linear(100, 10ConvSmall (MNIST) Conv(1, 16, 4) -Conv(16, 32, 4) -Linear(800, 100) -Linear(100, 10)\n\nConvBig (MNIST). ConvBig (MNIST)\n\nConv(32, 32, 4) -Conv(32. Conv, 32Conv(64, 64, 4) -Linear(3136, 512) -Linear(512, 512) -Linear(512, 10Conv(1, 32, 3) -Conv(32, 32, 4) -Conv(32, 64, 3) -Conv(64, 64, 4) -Linear(3136, 512) - Linear(512, 512) -Linear(512, 10)\n\n. Convsmall, CIFAR-10ConvSmall (CIFAR-10)\n\nConv(32, 32, 4) -Conv(32. Conv. 323Conv(64, 64, 4) -Linear(4096, 512) -Linear(512, 512) -Linear(512, 10) CNN-A-Adv/-4 (CIFAR-10Conv(3, 32, 3) -Conv(32, 32, 4) -Conv(32, 64, 3) -Conv(64, 64, 4) -Linear(4096, 512) - Linear(512, 512) -Linear(512, 10) CNN-A-Adv/-4 (CIFAR-10)\n\nConv. 164) -Linear(2048, 100) -Linear(100, 10) CNN-B-Adv/-4 (CIFAR-10Conv(3, 16, 4) -Conv(16, 32, 4) -Linear(2048, 100) -Linear(100, 10) CNN-B-Adv/-4 (CIFAR-10)\n\n. Conv. 323Conv(32, 128, 4) -Linear(8192, 250) -Linear(250, 10) CNN-A-Mix/-4 (CIFAR-10Conv(3, 32, 5) -Conv(32, 128, 4) -Linear(8192, 250) -Linear(250, 10) CNN-A-Mix/-4 (CIFAR-10)\n\nBase (CIFAR-10). Base (CIFAR-10)\n\nWe also include triangle relaxation based LP verifiers with intermediate layer bounds obtained from WK, CROWN and CROWN-OPT. In our experiments in Table 1, we noticed that BIGM+A.SET BABSR [10] and Fast-and-Complete [45] are also very competitive among existing state-of-the-art complete verifiers 3 -they runs fast in many cases with low timeout rates. Therefore, we also evaluate BIGM+A.SET BABSR and Fast-and-Complete with an early stop of 3 minutes for the incomplete verification setting as an extension of Section 4.2. The verified accuracy obtained from each method are reported in Table A2. BIGM+A.SET BABSR and Fast-and-Complete sometimes produce better bounds than SDP-FO, however \u03b2-CROWN FSB consistently outperforms both of them. Additionally, we found that intermediate layer bounds are important for LP verifier on some models. CROWN-OPT uses the joint optimization on intermediate layer bounds. optimizing Eq. 12 with no\u03b2, as done in [45. although even with the tightest possible CROWN-OPT bounds the verified accuracy gap between LP verifiers and ours is still large. Additionally, LP verifiers need significantly more timeCROWN-OPT uses the joint optimization on intermediate layer bounds (optimizing Eq. 12 with no\u03b2, as done in [45]). We also include triangle relaxation based LP verifiers with intermediate layer bounds obtained from WK, CROWN and CROWN-OPT. In our experiments in Table 1, we noticed that BIGM+A.SET BABSR [10] and Fast-and-Complete [45] are also very competitive among existing state-of-the-art complete verifiers 3 -they runs fast in many cases with low timeout rates. Therefore, we also evaluate BIGM+A.SET BABSR and Fast-and-Complete with an early stop of 3 minutes for the incomplete verification setting as an extension of Section 4.2. The verified accuracy obtained from each method are reported in Table A2. BIGM+A.SET BABSR and Fast-and-Complete sometimes produce better bounds than SDP-FO, however \u03b2-CROWN FSB consistently outperforms both of them. Additionally, we found that intermediate layer bounds are important for LP verifier on some models, although even with the tightest possible CROWN-OPT bounds the verified accuracy gap between LP verifiers and ours is still large. Additionally, LP verifiers need significantly more time.\n", "annotations": {"author": "[{\"end\":174,\"start\":120},{\"end\":213,\"start\":175},{\"end\":270,\"start\":214},{\"end\":330,\"start\":271},{\"end\":364,\"start\":331},{\"end\":407,\"start\":365},{\"end\":445,\"start\":408},{\"end\":174,\"start\":120},{\"end\":213,\"start\":175},{\"end\":270,\"start\":214},{\"end\":330,\"start\":271},{\"end\":364,\"start\":331},{\"end\":407,\"start\":365},{\"end\":445,\"start\":408}]", "publisher": null, "author_last_name": "[{\"end\":130,\"start\":126},{\"end\":185,\"start\":180},{\"end\":222,\"start\":220},{\"end\":278,\"start\":275},{\"end\":341,\"start\":337},{\"end\":378,\"start\":373},{\"end\":419,\"start\":413},{\"end\":130,\"start\":126},{\"end\":185,\"start\":180},{\"end\":222,\"start\":220},{\"end\":278,\"start\":275},{\"end\":341,\"start\":337},{\"end\":378,\"start\":373},{\"end\":419,\"start\":413}]", "author_first_name": "[{\"end\":125,\"start\":120},{\"end\":179,\"start\":175},{\"end\":219,\"start\":214},{\"end\":274,\"start\":271},{\"end\":336,\"start\":331},{\"end\":372,\"start\":365},{\"end\":412,\"start\":408},{\"end\":125,\"start\":120},{\"end\":179,\"start\":175},{\"end\":219,\"start\":214},{\"end\":274,\"start\":271},{\"end\":336,\"start\":331},{\"end\":372,\"start\":365},{\"end\":412,\"start\":408}]", "author_affiliation": "[{\"end\":152,\"start\":132},{\"end\":173,\"start\":154},{\"end\":191,\"start\":187},{\"end\":212,\"start\":193},{\"end\":248,\"start\":224},{\"end\":269,\"start\":250},{\"end\":329,\"start\":305},{\"end\":363,\"start\":343},{\"end\":406,\"start\":401},{\"end\":444,\"start\":440},{\"end\":152,\"start\":132},{\"end\":173,\"start\":154},{\"end\":191,\"start\":187},{\"end\":212,\"start\":193},{\"end\":248,\"start\":224},{\"end\":269,\"start\":250},{\"end\":329,\"start\":305},{\"end\":363,\"start\":343},{\"end\":406,\"start\":401},{\"end\":444,\"start\":440}]", "title": "[{\"end\":117,\"start\":1},{\"end\":562,\"start\":446},{\"end\":117,\"start\":1},{\"end\":562,\"start\":446}]", "venue": null, "abstract": "[{\"end\":2219,\"start\":564},{\"end\":2219,\"start\":564}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3285,\"start\":3282},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3299,\"start\":3296},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3490,\"start\":3487},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3636,\"start\":3632},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3639,\"start\":3636},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3641,\"start\":3639},{\"end\":3767,\"start\":3763},{\"end\":3995,\"start\":3991},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3998,\"start\":3995},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4001,\"start\":3998},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4004,\"start\":4001},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4007,\"start\":4004},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4010,\"start\":4007},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4013,\"start\":4010},{\"end\":4787,\"start\":4783},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5584,\"start\":5580},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6098,\"start\":6095},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6185,\"start\":6182},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6188,\"start\":6185},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6255,\"start\":6252},{\"end\":6258,\"start\":6255},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6261,\"start\":6258},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6264,\"start\":6261},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6266,\"start\":6264},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6269,\"start\":6266},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6473,\"start\":6470},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6960,\"start\":6956},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6963,\"start\":6960},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6966,\"start\":6963},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6999,\"start\":6996},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7082,\"start\":7079},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7143,\"start\":7139},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7146,\"start\":7143},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8004,\"start\":8001},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10206,\"start\":10202},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10209,\"start\":10206},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12301,\"start\":12297},{\"end\":12315,\"start\":12311},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12464,\"start\":12460},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12467,\"start\":12464},{\"end\":12571,\"start\":12557},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12854,\"start\":12851},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14242,\"start\":14239},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14619,\"start\":14616},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14878,\"start\":14874},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15130,\"start\":15126},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16747,\"start\":16746},{\"end\":20657,\"start\":20653},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21518,\"start\":21514},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21591,\"start\":21588},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21594,\"start\":21591},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22108,\"start\":22104},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22126,\"start\":22122},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23545,\"start\":23542},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23548,\"start\":23545},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23550,\"start\":23548},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23674,\"start\":23671},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23677,\"start\":23674},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24450,\"start\":24447},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24544,\"start\":24541},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24556,\"start\":24552},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24671,\"start\":24667},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24674,\"start\":24671},{\"end\":25230,\"start\":25226},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26027,\"start\":26023},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26030,\"start\":26027},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26061,\"start\":26057},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26448,\"start\":26445},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26503,\"start\":26499},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26628,\"start\":26624},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26631,\"start\":26628},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26634,\"start\":26631},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26637,\"start\":26634},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26756,\"start\":26752},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26874,\"start\":26871},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27004,\"start\":27001},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27007,\"start\":27004},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27135,\"start\":27131},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27397,\"start\":27393},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28077,\"start\":28073},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28116,\"start\":28112},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28692,\"start\":28688},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28695,\"start\":28692},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28698,\"start\":28695},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28731,\"start\":28727},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28744,\"start\":28740},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28759,\"start\":28755},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28889,\"start\":28885},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29367,\"start\":29364},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29528,\"start\":29524},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29865,\"start\":29862},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30018,\"start\":30014},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30428,\"start\":30425},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30580,\"start\":30577},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31144,\"start\":31141},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31475,\"start\":31471},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31478,\"start\":31475},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31481,\"start\":31478},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31484,\"start\":31481},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31487,\"start\":31484},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31708,\"start\":31705},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31711,\"start\":31708},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31714,\"start\":31711},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31716,\"start\":31714},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31803,\"start\":31799},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31806,\"start\":31803},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31808,\"start\":31806},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31844,\"start\":31841},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32012,\"start\":32009},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32284,\"start\":32280},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32340,\"start\":32337},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32343,\"start\":32340},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32564,\"start\":32560},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32725,\"start\":32721},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32968,\"start\":32964},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33082,\"start\":33078},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33105,\"start\":33101},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33211,\"start\":33207},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33329,\"start\":33326},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33357,\"start\":33353},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33360,\"start\":33357},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33363,\"start\":33360},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33473,\"start\":33470},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34127,\"start\":34123},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":34130,\"start\":34127},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34133,\"start\":34130},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":34136,\"start\":34133},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34139,\"start\":34136},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34142,\"start\":34139},{\"end\":34145,\"start\":34142},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34147,\"start\":34145},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34150,\"start\":34147},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36944,\"start\":36941},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36970,\"start\":36967},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":44604,\"start\":44600},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":45786,\"start\":45782},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":46663,\"start\":46659},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":49287,\"start\":49283},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":49373,\"start\":49369},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":49824,\"start\":49820},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":49907,\"start\":49903},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":50472,\"start\":50468},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54493,\"start\":54490},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":54505,\"start\":54501},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":54971,\"start\":54970},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":55299,\"start\":55295},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":55395,\"start\":55394},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":56627,\"start\":56624},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":57526,\"start\":57522},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":57648,\"start\":57645},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":57780,\"start\":57776},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":57817,\"start\":57814},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":57981,\"start\":57977},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":58238,\"start\":58235},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":60689,\"start\":60685},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":61483,\"start\":61480},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":61889,\"start\":61885},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":61947,\"start\":61944},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":61950,\"start\":61947},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":61952,\"start\":61950},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":62325,\"start\":62322},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":62332,\"start\":62328},{\"end\":62712,\"start\":62708},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":71393,\"start\":71389},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":72719,\"start\":72715},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":72722,\"start\":72719},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":72725,\"start\":72722},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":72831,\"start\":72827},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":72936,\"start\":72932},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":72939,\"start\":72936},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":72942,\"start\":72939},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":75567,\"start\":75563},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3285,\"start\":3282},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3299,\"start\":3296},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3490,\"start\":3487},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3636,\"start\":3632},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3639,\"start\":3636},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3641,\"start\":3639},{\"end\":3767,\"start\":3763},{\"end\":3995,\"start\":3991},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3998,\"start\":3995},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4001,\"start\":3998},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4004,\"start\":4001},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4007,\"start\":4004},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4010,\"start\":4007},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4013,\"start\":4010},{\"end\":4787,\"start\":4783},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5584,\"start\":5580},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6098,\"start\":6095},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6185,\"start\":6182},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6188,\"start\":6185},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6255,\"start\":6252},{\"end\":6258,\"start\":6255},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6261,\"start\":6258},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6264,\"start\":6261},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6266,\"start\":6264},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6269,\"start\":6266},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6473,\"start\":6470},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6960,\"start\":6956},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6963,\"start\":6960},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6966,\"start\":6963},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6999,\"start\":6996},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7082,\"start\":7079},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7143,\"start\":7139},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7146,\"start\":7143},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8004,\"start\":8001},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10206,\"start\":10202},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10209,\"start\":10206},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12301,\"start\":12297},{\"end\":12315,\"start\":12311},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12464,\"start\":12460},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12467,\"start\":12464},{\"end\":12571,\"start\":12557},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12854,\"start\":12851},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14242,\"start\":14239},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14619,\"start\":14616},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14878,\"start\":14874},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15130,\"start\":15126},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16747,\"start\":16746},{\"end\":20657,\"start\":20653},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21518,\"start\":21514},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21591,\"start\":21588},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21594,\"start\":21591},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22108,\"start\":22104},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22126,\"start\":22122},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23545,\"start\":23542},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23548,\"start\":23545},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23550,\"start\":23548},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23674,\"start\":23671},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23677,\"start\":23674},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24450,\"start\":24447},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24544,\"start\":24541},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24556,\"start\":24552},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24671,\"start\":24667},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24674,\"start\":24671},{\"end\":25230,\"start\":25226},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26027,\"start\":26023},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26030,\"start\":26027},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26061,\"start\":26057},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26448,\"start\":26445},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26503,\"start\":26499},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26628,\"start\":26624},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26631,\"start\":26628},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26634,\"start\":26631},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26637,\"start\":26634},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26756,\"start\":26752},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26874,\"start\":26871},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27004,\"start\":27001},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27007,\"start\":27004},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27135,\"start\":27131},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27397,\"start\":27393},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28077,\"start\":28073},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28116,\"start\":28112},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28692,\"start\":28688},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28695,\"start\":28692},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28698,\"start\":28695},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28731,\"start\":28727},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28744,\"start\":28740},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28759,\"start\":28755},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28889,\"start\":28885},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29367,\"start\":29364},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29528,\"start\":29524},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29865,\"start\":29862},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30018,\"start\":30014},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30428,\"start\":30425},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30580,\"start\":30577},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31144,\"start\":31141},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31475,\"start\":31471},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31478,\"start\":31475},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31481,\"start\":31478},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31484,\"start\":31481},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31487,\"start\":31484},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31708,\"start\":31705},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31711,\"start\":31708},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31714,\"start\":31711},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31716,\"start\":31714},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31803,\"start\":31799},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31806,\"start\":31803},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31808,\"start\":31806},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31844,\"start\":31841},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32012,\"start\":32009},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32284,\"start\":32280},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32340,\"start\":32337},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32343,\"start\":32340},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32564,\"start\":32560},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32725,\"start\":32721},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32968,\"start\":32964},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33082,\"start\":33078},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33105,\"start\":33101},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33211,\"start\":33207},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33329,\"start\":33326},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33357,\"start\":33353},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33360,\"start\":33357},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33363,\"start\":33360},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33473,\"start\":33470},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34127,\"start\":34123},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":34130,\"start\":34127},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34133,\"start\":34130},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":34136,\"start\":34133},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34139,\"start\":34136},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34142,\"start\":34139},{\"end\":34145,\"start\":34142},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34147,\"start\":34145},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34150,\"start\":34147},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36944,\"start\":36941},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36970,\"start\":36967},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":44604,\"start\":44600},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":45786,\"start\":45782},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":46663,\"start\":46659},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":49287,\"start\":49283},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":49373,\"start\":49369},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":49824,\"start\":49820},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":49907,\"start\":49903},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":50472,\"start\":50468},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54493,\"start\":54490},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":54505,\"start\":54501},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":54971,\"start\":54970},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":55299,\"start\":55295},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":55395,\"start\":55394},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":56627,\"start\":56624},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":57526,\"start\":57522},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":57648,\"start\":57645},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":57780,\"start\":57776},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":57817,\"start\":57814},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":57981,\"start\":57977},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":58238,\"start\":58235},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":60689,\"start\":60685},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":61483,\"start\":61480},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":61889,\"start\":61885},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":61947,\"start\":61944},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":61950,\"start\":61947},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":61952,\"start\":61950},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":62325,\"start\":62322},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":62332,\"start\":62328},{\"end\":62712,\"start\":62708},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":71393,\"start\":71389},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":72719,\"start\":72715},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":72722,\"start\":72719},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":72725,\"start\":72722},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":72831,\"start\":72827},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":72936,\"start\":72932},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":72939,\"start\":72936},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":72942,\"start\":72939},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":75567,\"start\":75563}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":63918,\"start\":63735},{\"attributes\":{\"id\":\"fig_1\"},\"end\":64083,\"start\":63919},{\"attributes\":{\"id\":\"fig_2\"},\"end\":64136,\"start\":64084},{\"attributes\":{\"id\":\"fig_3\"},\"end\":64209,\"start\":64137},{\"attributes\":{\"id\":\"fig_5\"},\"end\":64735,\"start\":64210},{\"attributes\":{\"id\":\"fig_6\"},\"end\":64944,\"start\":64736},{\"attributes\":{\"id\":\"fig_7\"},\"end\":66152,\"start\":64945},{\"attributes\":{\"id\":\"fig_8\"},\"end\":66460,\"start\":66153},{\"attributes\":{\"id\":\"fig_9\"},\"end\":67911,\"start\":66461},{\"attributes\":{\"id\":\"fig_10\"},\"end\":68578,\"start\":67912},{\"attributes\":{\"id\":\"fig_11\"},\"end\":68826,\"start\":68579},{\"attributes\":{\"id\":\"fig_12\"},\"end\":69272,\"start\":68827},{\"attributes\":{\"id\":\"fig_13\"},\"end\":69776,\"start\":69273},{\"attributes\":{\"id\":\"fig_14\"},\"end\":70250,\"start\":69777},{\"attributes\":{\"id\":\"fig_15\"},\"end\":70679,\"start\":70251},{\"attributes\":{\"id\":\"fig_16\"},\"end\":70900,\"start\":70680},{\"attributes\":{\"id\":\"fig_17\"},\"end\":71219,\"start\":70901},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":72616,\"start\":71220},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":74363,\"start\":72617},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":75621,\"start\":74364},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":76580,\"start\":75622},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":76953,\"start\":76581},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":77027,\"start\":76954},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":77826,\"start\":77028},{\"attributes\":{\"id\":\"fig_0\"},\"end\":63918,\"start\":63735},{\"attributes\":{\"id\":\"fig_1\"},\"end\":64083,\"start\":63919},{\"attributes\":{\"id\":\"fig_2\"},\"end\":64136,\"start\":64084},{\"attributes\":{\"id\":\"fig_3\"},\"end\":64209,\"start\":64137},{\"attributes\":{\"id\":\"fig_5\"},\"end\":64735,\"start\":64210},{\"attributes\":{\"id\":\"fig_6\"},\"end\":64944,\"start\":64736},{\"attributes\":{\"id\":\"fig_7\"},\"end\":66152,\"start\":64945},{\"attributes\":{\"id\":\"fig_8\"},\"end\":66460,\"start\":66153},{\"attributes\":{\"id\":\"fig_9\"},\"end\":67911,\"start\":66461},{\"attributes\":{\"id\":\"fig_10\"},\"end\":68578,\"start\":67912},{\"attributes\":{\"id\":\"fig_11\"},\"end\":68826,\"start\":68579},{\"attributes\":{\"id\":\"fig_12\"},\"end\":69272,\"start\":68827},{\"attributes\":{\"id\":\"fig_13\"},\"end\":69776,\"start\":69273},{\"attributes\":{\"id\":\"fig_14\"},\"end\":70250,\"start\":69777},{\"attributes\":{\"id\":\"fig_15\"},\"end\":70679,\"start\":70251},{\"attributes\":{\"id\":\"fig_16\"},\"end\":70900,\"start\":70680},{\"attributes\":{\"id\":\"fig_17\"},\"end\":71219,\"start\":70901},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":72616,\"start\":71220},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":74363,\"start\":72617},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":75621,\"start\":74364},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":76580,\"start\":75622},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":76953,\"start\":76581},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":77027,\"start\":76954},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":77826,\"start\":77028}]", "paragraph": "[{\"end\":3005,\"start\":2235},{\"end\":3098,\"start\":3007},{\"end\":4174,\"start\":3100},{\"end\":4851,\"start\":4176},{\"end\":6585,\"start\":4853},{\"end\":7268,\"start\":6587},{\"end\":8790,\"start\":7347},{\"end\":9392,\"start\":8792},{\"end\":9705,\"start\":9394},{\"end\":10639,\"start\":10144},{\"end\":11068,\"start\":10713},{\"end\":11141,\"start\":11116},{\"end\":11231,\"start\":11184},{\"end\":11520,\"start\":11346},{\"end\":11720,\"start\":11522},{\"end\":12006,\"start\":11959},{\"end\":12731,\"start\":12057},{\"end\":12926,\"start\":12781},{\"end\":13483,\"start\":12997},{\"end\":13662,\"start\":13485},{\"end\":13918,\"start\":13682},{\"end\":14111,\"start\":13998},{\"end\":14423,\"start\":14144},{\"end\":15285,\"start\":14425},{\"end\":15719,\"start\":15338},{\"end\":15885,\"start\":15787},{\"end\":16193,\"start\":15908},{\"end\":16379,\"start\":16195},{\"end\":16748,\"start\":16496},{\"end\":16972,\"start\":16774},{\"end\":17152,\"start\":17036},{\"end\":17619,\"start\":17363},{\"end\":18011,\"start\":17726},{\"end\":18385,\"start\":18095},{\"end\":18608,\"start\":18476},{\"end\":18808,\"start\":18761},{\"end\":18875,\"start\":18870},{\"end\":19561,\"start\":18986},{\"end\":19724,\"start\":19563},{\"end\":20378,\"start\":19811},{\"end\":20708,\"start\":20380},{\"end\":21025,\"start\":20786},{\"end\":21079,\"start\":21054},{\"end\":21389,\"start\":21363},{\"end\":21841,\"start\":21483},{\"end\":22040,\"start\":21891},{\"end\":22519,\"start\":22042},{\"end\":22705,\"start\":22549},{\"end\":22795,\"start\":22757},{\"end\":23124,\"start\":22878},{\"end\":23804,\"start\":23171},{\"end\":24342,\"start\":24004},{\"end\":24856,\"start\":24390},{\"end\":25433,\"start\":24858},{\"end\":25894,\"start\":25435},{\"end\":26613,\"start\":25954},{\"end\":27635,\"start\":26615},{\"end\":28410,\"start\":27637},{\"end\":29211,\"start\":28449},{\"end\":31349,\"start\":29213},{\"end\":31845,\"start\":31366},{\"end\":32919,\"start\":31847},{\"end\":33980,\"start\":32921},{\"end\":34365,\"start\":33982},{\"end\":35444,\"start\":34380},{\"end\":35555,\"start\":35515},{\"end\":36088,\"start\":35980},{\"end\":36250,\"start\":36157},{\"end\":36566,\"start\":36252},{\"end\":36730,\"start\":36632},{\"end\":36897,\"start\":36732},{\"end\":37050,\"start\":36899},{\"end\":37261,\"start\":37193},{\"end\":37477,\"start\":37425},{\"end\":37705,\"start\":37539},{\"end\":38162,\"start\":37941},{\"end\":39085,\"start\":38773},{\"end\":39218,\"start\":39188},{\"end\":39521,\"start\":39504},{\"end\":39726,\"start\":39635},{\"end\":40145,\"start\":39865},{\"end\":40690,\"start\":40477},{\"end\":41243,\"start\":41151},{\"end\":41608,\"start\":41552},{\"end\":42542,\"start\":42438},{\"end\":42759,\"start\":42664},{\"end\":42878,\"start\":42761},{\"end\":42982,\"start\":42880},{\"end\":43255,\"start\":43121},{\"end\":43596,\"start\":43561},{\"end\":44109,\"start\":43598},{\"end\":44243,\"start\":44111},{\"end\":44442,\"start\":44293},{\"end\":44998,\"start\":44444},{\"end\":45497,\"start\":45450},{\"end\":45820,\"start\":45564},{\"end\":45900,\"start\":45854},{\"end\":46068,\"start\":46057},{\"end\":47214,\"start\":46631},{\"end\":47545,\"start\":47430},{\"end\":47914,\"start\":47768},{\"end\":48091,\"start\":48036},{\"end\":48613,\"start\":48509},{\"end\":49759,\"start\":48896},{\"end\":49841,\"start\":49799},{\"end\":50084,\"start\":49882},{\"end\":50208,\"start\":50149},{\"end\":50785,\"start\":50352},{\"end\":50989,\"start\":50821},{\"end\":51372,\"start\":51079},{\"end\":51544,\"start\":51374},{\"end\":51749,\"start\":51617},{\"end\":52107,\"start\":51751},{\"end\":52341,\"start\":52125},{\"end\":52552,\"start\":52357},{\"end\":53469,\"start\":52554},{\"end\":53524,\"start\":53471},{\"end\":55300,\"start\":53588},{\"end\":55479,\"start\":55302},{\"end\":55703,\"start\":55520},{\"end\":55797,\"start\":55705},{\"end\":55916,\"start\":55799},{\"end\":56163,\"start\":56052},{\"end\":56380,\"start\":56311},{\"end\":56391,\"start\":56382},{\"end\":56504,\"start\":56434},{\"end\":57758,\"start\":56611},{\"end\":58763,\"start\":57760},{\"end\":59260,\"start\":58765},{\"end\":60294,\"start\":59295},{\"end\":61730,\"start\":60367},{\"end\":63734,\"start\":61761},{\"end\":3005,\"start\":2235},{\"end\":3098,\"start\":3007},{\"end\":4174,\"start\":3100},{\"end\":4851,\"start\":4176},{\"end\":6585,\"start\":4853},{\"end\":7268,\"start\":6587},{\"end\":8790,\"start\":7347},{\"end\":9392,\"start\":8792},{\"end\":9705,\"start\":9394},{\"end\":10639,\"start\":10144},{\"end\":11068,\"start\":10713},{\"end\":11141,\"start\":11116},{\"end\":11231,\"start\":11184},{\"end\":11520,\"start\":11346},{\"end\":11720,\"start\":11522},{\"end\":12006,\"start\":11959},{\"end\":12731,\"start\":12057},{\"end\":12926,\"start\":12781},{\"end\":13483,\"start\":12997},{\"end\":13662,\"start\":13485},{\"end\":13918,\"start\":13682},{\"end\":14111,\"start\":13998},{\"end\":14423,\"start\":14144},{\"end\":15285,\"start\":14425},{\"end\":15719,\"start\":15338},{\"end\":15885,\"start\":15787},{\"end\":16193,\"start\":15908},{\"end\":16379,\"start\":16195},{\"end\":16748,\"start\":16496},{\"end\":16972,\"start\":16774},{\"end\":17152,\"start\":17036},{\"end\":17619,\"start\":17363},{\"end\":18011,\"start\":17726},{\"end\":18385,\"start\":18095},{\"end\":18608,\"start\":18476},{\"end\":18808,\"start\":18761},{\"end\":18875,\"start\":18870},{\"end\":19561,\"start\":18986},{\"end\":19724,\"start\":19563},{\"end\":20378,\"start\":19811},{\"end\":20708,\"start\":20380},{\"end\":21025,\"start\":20786},{\"end\":21079,\"start\":21054},{\"end\":21389,\"start\":21363},{\"end\":21841,\"start\":21483},{\"end\":22040,\"start\":21891},{\"end\":22519,\"start\":22042},{\"end\":22705,\"start\":22549},{\"end\":22795,\"start\":22757},{\"end\":23124,\"start\":22878},{\"end\":23804,\"start\":23171},{\"end\":24342,\"start\":24004},{\"end\":24856,\"start\":24390},{\"end\":25433,\"start\":24858},{\"end\":25894,\"start\":25435},{\"end\":26613,\"start\":25954},{\"end\":27635,\"start\":26615},{\"end\":28410,\"start\":27637},{\"end\":29211,\"start\":28449},{\"end\":31349,\"start\":29213},{\"end\":31845,\"start\":31366},{\"end\":32919,\"start\":31847},{\"end\":33980,\"start\":32921},{\"end\":34365,\"start\":33982},{\"end\":35444,\"start\":34380},{\"end\":35555,\"start\":35515},{\"end\":36088,\"start\":35980},{\"end\":36250,\"start\":36157},{\"end\":36566,\"start\":36252},{\"end\":36730,\"start\":36632},{\"end\":36897,\"start\":36732},{\"end\":37050,\"start\":36899},{\"end\":37261,\"start\":37193},{\"end\":37477,\"start\":37425},{\"end\":37705,\"start\":37539},{\"end\":38162,\"start\":37941},{\"end\":39085,\"start\":38773},{\"end\":39218,\"start\":39188},{\"end\":39521,\"start\":39504},{\"end\":39726,\"start\":39635},{\"end\":40145,\"start\":39865},{\"end\":40690,\"start\":40477},{\"end\":41243,\"start\":41151},{\"end\":41608,\"start\":41552},{\"end\":42542,\"start\":42438},{\"end\":42759,\"start\":42664},{\"end\":42878,\"start\":42761},{\"end\":42982,\"start\":42880},{\"end\":43255,\"start\":43121},{\"end\":43596,\"start\":43561},{\"end\":44109,\"start\":43598},{\"end\":44243,\"start\":44111},{\"end\":44442,\"start\":44293},{\"end\":44998,\"start\":44444},{\"end\":45497,\"start\":45450},{\"end\":45820,\"start\":45564},{\"end\":45900,\"start\":45854},{\"end\":46068,\"start\":46057},{\"end\":47214,\"start\":46631},{\"end\":47545,\"start\":47430},{\"end\":47914,\"start\":47768},{\"end\":48091,\"start\":48036},{\"end\":48613,\"start\":48509},{\"end\":49759,\"start\":48896},{\"end\":49841,\"start\":49799},{\"end\":50084,\"start\":49882},{\"end\":50208,\"start\":50149},{\"end\":50785,\"start\":50352},{\"end\":50989,\"start\":50821},{\"end\":51372,\"start\":51079},{\"end\":51544,\"start\":51374},{\"end\":51749,\"start\":51617},{\"end\":52107,\"start\":51751},{\"end\":52341,\"start\":52125},{\"end\":52552,\"start\":52357},{\"end\":53469,\"start\":52554},{\"end\":53524,\"start\":53471},{\"end\":55300,\"start\":53588},{\"end\":55479,\"start\":55302},{\"end\":55703,\"start\":55520},{\"end\":55797,\"start\":55705},{\"end\":55916,\"start\":55799},{\"end\":56163,\"start\":56052},{\"end\":56380,\"start\":56311},{\"end\":56391,\"start\":56382},{\"end\":56504,\"start\":56434},{\"end\":57758,\"start\":56611},{\"end\":58763,\"start\":57760},{\"end\":59260,\"start\":58765},{\"end\":60294,\"start\":59295},{\"end\":61730,\"start\":60367},{\"end\":63734,\"start\":61761}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10143,\"start\":9706},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11115,\"start\":11069},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11183,\"start\":11142},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11345,\"start\":11232},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11958,\"start\":11721},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12056,\"start\":12007},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12996,\"start\":12927},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13681,\"start\":13663},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13997,\"start\":13919},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14143,\"start\":14112},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15907,\"start\":15886},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16495,\"start\":16380},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16773,\"start\":16749},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17035,\"start\":16973},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17362,\"start\":17153},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17725,\"start\":17620},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18094,\"start\":18012},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18475,\"start\":18386},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18760,\"start\":18609},{\"attributes\":{\"id\":\"formula_19\"},\"end\":18869,\"start\":18809},{\"attributes\":{\"id\":\"formula_20\"},\"end\":18985,\"start\":18876},{\"attributes\":{\"id\":\"formula_21\"},\"end\":19810,\"start\":19725},{\"attributes\":{\"id\":\"formula_22\"},\"end\":20751,\"start\":20709},{\"attributes\":{\"id\":\"formula_23\"},\"end\":21053,\"start\":21026},{\"attributes\":{\"id\":\"formula_24\"},\"end\":21362,\"start\":21080},{\"attributes\":{\"id\":\"formula_25\"},\"end\":21482,\"start\":21390},{\"attributes\":{\"id\":\"formula_26\"},\"end\":21890,\"start\":21842},{\"attributes\":{\"id\":\"formula_27\"},\"end\":22548,\"start\":22520},{\"attributes\":{\"id\":\"formula_28\"},\"end\":22877,\"start\":22796},{\"attributes\":{\"id\":\"formula_29\"},\"end\":23170,\"start\":23125},{\"attributes\":{\"id\":\"formula_30\"},\"end\":24003,\"start\":23805},{\"attributes\":{\"id\":\"formula_31\"},\"end\":35514,\"start\":35445},{\"attributes\":{\"id\":\"formula_32\"},\"end\":35689,\"start\":35556},{\"attributes\":{\"id\":\"formula_33\"},\"end\":35979,\"start\":35689},{\"attributes\":{\"id\":\"formula_34\"},\"end\":36156,\"start\":36089},{\"attributes\":{\"id\":\"formula_35\"},\"end\":36631,\"start\":36567},{\"attributes\":{\"id\":\"formula_36\"},\"end\":37192,\"start\":37051},{\"attributes\":{\"id\":\"formula_37\"},\"end\":37424,\"start\":37262},{\"attributes\":{\"id\":\"formula_38\"},\"end\":37538,\"start\":37478},{\"attributes\":{\"id\":\"formula_39\"},\"end\":37734,\"start\":37706},{\"attributes\":{\"id\":\"formula_40\"},\"end\":37940,\"start\":37734},{\"attributes\":{\"id\":\"formula_41\"},\"end\":38772,\"start\":38163},{\"attributes\":{\"id\":\"formula_42\"},\"end\":39187,\"start\":39086},{\"attributes\":{\"id\":\"formula_43\"},\"end\":39503,\"start\":39219},{\"attributes\":{\"id\":\"formula_44\"},\"end\":39634,\"start\":39522},{\"attributes\":{\"id\":\"formula_45\"},\"end\":39864,\"start\":39727},{\"attributes\":{\"id\":\"formula_46\"},\"end\":40476,\"start\":40146},{\"attributes\":{\"id\":\"formula_47\"},\"end\":41150,\"start\":40691},{\"attributes\":{\"id\":\"formula_48\"},\"end\":41551,\"start\":41244},{\"attributes\":{\"id\":\"formula_49\"},\"end\":42437,\"start\":41609},{\"attributes\":{\"id\":\"formula_50\"},\"end\":42663,\"start\":42543},{\"attributes\":{\"id\":\"formula_51\"},\"end\":43120,\"start\":42983},{\"attributes\":{\"id\":\"formula_52\"},\"end\":43547,\"start\":43256},{\"attributes\":{\"id\":\"formula_53\"},\"end\":44292,\"start\":44244},{\"attributes\":{\"id\":\"formula_54\"},\"end\":45449,\"start\":44999},{\"attributes\":{\"id\":\"formula_55\"},\"end\":45563,\"start\":45498},{\"attributes\":{\"id\":\"formula_56\"},\"end\":45853,\"start\":45821},{\"attributes\":{\"id\":\"formula_57\"},\"end\":46056,\"start\":45901},{\"attributes\":{\"id\":\"formula_58\"},\"end\":46630,\"start\":46069},{\"attributes\":{\"id\":\"formula_59\"},\"end\":47429,\"start\":47215},{\"attributes\":{\"id\":\"formula_60\"},\"end\":47576,\"start\":47546},{\"attributes\":{\"id\":\"formula_61\"},\"end\":47767,\"start\":47576},{\"attributes\":{\"id\":\"formula_62\"},\"end\":48035,\"start\":47915},{\"attributes\":{\"id\":\"formula_63\"},\"end\":48508,\"start\":48092},{\"attributes\":{\"id\":\"formula_64\"},\"end\":48895,\"start\":48614},{\"attributes\":{\"id\":\"formula_65\"},\"end\":49798,\"start\":49760},{\"attributes\":{\"id\":\"formula_66\"},\"end\":49881,\"start\":49842},{\"attributes\":{\"id\":\"formula_67\"},\"end\":50148,\"start\":50085},{\"attributes\":{\"id\":\"formula_68\"},\"end\":50351,\"start\":50209},{\"attributes\":{\"id\":\"formula_69\"},\"end\":50820,\"start\":50786},{\"attributes\":{\"id\":\"formula_70\"},\"end\":51078,\"start\":50990},{\"attributes\":{\"id\":\"formula_71\"},\"end\":51616,\"start\":51545},{\"attributes\":{\"id\":\"formula_72\"},\"end\":52356,\"start\":52342},{\"attributes\":{\"id\":\"formula_73\"},\"end\":55519,\"start\":55480},{\"attributes\":{\"id\":\"formula_74\"},\"end\":56051,\"start\":55917},{\"attributes\":{\"id\":\"formula_75\"},\"end\":56310,\"start\":56164},{\"attributes\":{\"id\":\"formula_76\"},\"end\":56433,\"start\":56392},{\"attributes\":{\"id\":\"formula_77\"},\"end\":56554,\"start\":56505},{\"attributes\":{\"id\":\"formula_0\"},\"end\":10143,\"start\":9706},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11115,\"start\":11069},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11183,\"start\":11142},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11345,\"start\":11232},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11958,\"start\":11721},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12056,\"start\":12007},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12996,\"start\":12927},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13681,\"start\":13663},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13997,\"start\":13919},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14143,\"start\":14112},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15907,\"start\":15886},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16495,\"start\":16380},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16773,\"start\":16749},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17035,\"start\":16973},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17362,\"start\":17153},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17725,\"start\":17620},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18094,\"start\":18012},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18475,\"start\":18386},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18760,\"start\":18609},{\"attributes\":{\"id\":\"formula_19\"},\"end\":18869,\"start\":18809},{\"attributes\":{\"id\":\"formula_20\"},\"end\":18985,\"start\":18876},{\"attributes\":{\"id\":\"formula_21\"},\"end\":19810,\"start\":19725},{\"attributes\":{\"id\":\"formula_22\"},\"end\":20751,\"start\":20709},{\"attributes\":{\"id\":\"formula_23\"},\"end\":21053,\"start\":21026},{\"attributes\":{\"id\":\"formula_24\"},\"end\":21362,\"start\":21080},{\"attributes\":{\"id\":\"formula_25\"},\"end\":21482,\"start\":21390},{\"attributes\":{\"id\":\"formula_26\"},\"end\":21890,\"start\":21842},{\"attributes\":{\"id\":\"formula_27\"},\"end\":22548,\"start\":22520},{\"attributes\":{\"id\":\"formula_28\"},\"end\":22877,\"start\":22796},{\"attributes\":{\"id\":\"formula_29\"},\"end\":23170,\"start\":23125},{\"attributes\":{\"id\":\"formula_30\"},\"end\":24003,\"start\":23805},{\"attributes\":{\"id\":\"formula_31\"},\"end\":35514,\"start\":35445},{\"attributes\":{\"id\":\"formula_32\"},\"end\":35689,\"start\":35556},{\"attributes\":{\"id\":\"formula_33\"},\"end\":35979,\"start\":35689},{\"attributes\":{\"id\":\"formula_34\"},\"end\":36156,\"start\":36089},{\"attributes\":{\"id\":\"formula_35\"},\"end\":36631,\"start\":36567},{\"attributes\":{\"id\":\"formula_36\"},\"end\":37192,\"start\":37051},{\"attributes\":{\"id\":\"formula_37\"},\"end\":37424,\"start\":37262},{\"attributes\":{\"id\":\"formula_38\"},\"end\":37538,\"start\":37478},{\"attributes\":{\"id\":\"formula_39\"},\"end\":37734,\"start\":37706},{\"attributes\":{\"id\":\"formula_40\"},\"end\":37940,\"start\":37734},{\"attributes\":{\"id\":\"formula_41\"},\"end\":38772,\"start\":38163},{\"attributes\":{\"id\":\"formula_42\"},\"end\":39187,\"start\":39086},{\"attributes\":{\"id\":\"formula_43\"},\"end\":39503,\"start\":39219},{\"attributes\":{\"id\":\"formula_44\"},\"end\":39634,\"start\":39522},{\"attributes\":{\"id\":\"formula_45\"},\"end\":39864,\"start\":39727},{\"attributes\":{\"id\":\"formula_46\"},\"end\":40476,\"start\":40146},{\"attributes\":{\"id\":\"formula_47\"},\"end\":41150,\"start\":40691},{\"attributes\":{\"id\":\"formula_48\"},\"end\":41551,\"start\":41244},{\"attributes\":{\"id\":\"formula_49\"},\"end\":42437,\"start\":41609},{\"attributes\":{\"id\":\"formula_50\"},\"end\":42663,\"start\":42543},{\"attributes\":{\"id\":\"formula_51\"},\"end\":43120,\"start\":42983},{\"attributes\":{\"id\":\"formula_52\"},\"end\":43547,\"start\":43256},{\"attributes\":{\"id\":\"formula_53\"},\"end\":44292,\"start\":44244},{\"attributes\":{\"id\":\"formula_54\"},\"end\":45449,\"start\":44999},{\"attributes\":{\"id\":\"formula_55\"},\"end\":45563,\"start\":45498},{\"attributes\":{\"id\":\"formula_56\"},\"end\":45853,\"start\":45821},{\"attributes\":{\"id\":\"formula_57\"},\"end\":46056,\"start\":45901},{\"attributes\":{\"id\":\"formula_58\"},\"end\":46630,\"start\":46069},{\"attributes\":{\"id\":\"formula_59\"},\"end\":47429,\"start\":47215},{\"attributes\":{\"id\":\"formula_60\"},\"end\":47576,\"start\":47546},{\"attributes\":{\"id\":\"formula_61\"},\"end\":47767,\"start\":47576},{\"attributes\":{\"id\":\"formula_62\"},\"end\":48035,\"start\":47915},{\"attributes\":{\"id\":\"formula_63\"},\"end\":48508,\"start\":48092},{\"attributes\":{\"id\":\"formula_64\"},\"end\":48895,\"start\":48614},{\"attributes\":{\"id\":\"formula_65\"},\"end\":49798,\"start\":49760},{\"attributes\":{\"id\":\"formula_66\"},\"end\":49881,\"start\":49842},{\"attributes\":{\"id\":\"formula_67\"},\"end\":50148,\"start\":50085},{\"attributes\":{\"id\":\"formula_68\"},\"end\":50351,\"start\":50209},{\"attributes\":{\"id\":\"formula_69\"},\"end\":50820,\"start\":50786},{\"attributes\":{\"id\":\"formula_70\"},\"end\":51078,\"start\":50990},{\"attributes\":{\"id\":\"formula_71\"},\"end\":51616,\"start\":51545},{\"attributes\":{\"id\":\"formula_72\"},\"end\":52356,\"start\":52342},{\"attributes\":{\"id\":\"formula_73\"},\"end\":55519,\"start\":55480},{\"attributes\":{\"id\":\"formula_74\"},\"end\":56051,\"start\":55917},{\"attributes\":{\"id\":\"formula_75\"},\"end\":56310,\"start\":56164},{\"attributes\":{\"id\":\"formula_76\"},\"end\":56433,\"start\":56392},{\"attributes\":{\"id\":\"formula_77\"},\"end\":56554,\"start\":56505}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27706,\"start\":27699},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28478,\"start\":28471},{\"end\":29263,\"start\":29256},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30084,\"start\":30077},{\"end\":30139,\"start\":30132},{\"end\":30486,\"start\":30479},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30978,\"start\":30971},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":57293,\"start\":57286},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":59061,\"start\":59052},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":60579,\"start\":60572},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":61673,\"start\":61665},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":62231,\"start\":62223},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":62893,\"start\":62885},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":63008,\"start\":63001},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":63112,\"start\":63104},{\"end\":63372,\"start\":63364},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":63497,\"start\":63490},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27706,\"start\":27699},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28478,\"start\":28471},{\"end\":29263,\"start\":29256},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30084,\"start\":30077},{\"end\":30139,\"start\":30132},{\"end\":30486,\"start\":30479},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30978,\"start\":30971},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":57293,\"start\":57286},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":59061,\"start\":59052},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":60579,\"start\":60572},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":61673,\"start\":61665},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":62231,\"start\":62223},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":62893,\"start\":62885},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":63008,\"start\":63001},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":63112,\"start\":63104},{\"end\":63372,\"start\":63364},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":63497,\"start\":63490}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2233,\"start\":2221},{\"attributes\":{\"n\":\"2\"},\"end\":7281,\"start\":7271},{\"attributes\":{\"n\":\"2.1\"},\"end\":7345,\"start\":7284},{\"attributes\":{\"n\":\"2.2\"},\"end\":10711,\"start\":10642},{\"attributes\":{\"n\":\"2.3\"},\"end\":12779,\"start\":12734},{\"attributes\":{\"n\":\"3\"},\"end\":15336,\"start\":15288},{\"attributes\":{\"n\":\"3.1\"},\"end\":15785,\"start\":15722},{\"attributes\":{\"n\":\"3.2\"},\"end\":20784,\"start\":20753},{\"attributes\":{\"n\":\"3.3\"},\"end\":22755,\"start\":22708},{\"attributes\":{\"n\":\"3.4\"},\"end\":24388,\"start\":24345},{\"attributes\":{\"n\":\"4\"},\"end\":25917,\"start\":25897},{\"attributes\":{\"n\":\"4.1\"},\"end\":25952,\"start\":25920},{\"attributes\":{\"n\":\"4.2\"},\"end\":28447,\"start\":28413},{\"attributes\":{\"n\":\"5\"},\"end\":31364,\"start\":31352},{\"attributes\":{\"n\":\"6\"},\"end\":34378,\"start\":34368},{\"end\":43559,\"start\":43549},{\"end\":52123,\"start\":52110},{\"end\":53586,\"start\":53527},{\"end\":56609,\"start\":56556},{\"end\":59293,\"start\":59263},{\"end\":60340,\"start\":60297},{\"end\":60365,\"start\":60343},{\"end\":61759,\"start\":61733},{\"end\":63921,\"start\":63920},{\"end\":64086,\"start\":64085},{\"end\":64212,\"start\":64211},{\"end\":64747,\"start\":64737},{\"end\":64947,\"start\":64946},{\"end\":66170,\"start\":66154},{\"end\":66468,\"start\":66462},{\"end\":68846,\"start\":68828},{\"end\":70263,\"start\":70252},{\"end\":70913,\"start\":70902},{\"end\":71230,\"start\":71221},{\"end\":72627,\"start\":72618},{\"end\":76965,\"start\":76955},{\"end\":77039,\"start\":77029},{\"attributes\":{\"n\":\"1\"},\"end\":2233,\"start\":2221},{\"attributes\":{\"n\":\"2\"},\"end\":7281,\"start\":7271},{\"attributes\":{\"n\":\"2.1\"},\"end\":7345,\"start\":7284},{\"attributes\":{\"n\":\"2.2\"},\"end\":10711,\"start\":10642},{\"attributes\":{\"n\":\"2.3\"},\"end\":12779,\"start\":12734},{\"attributes\":{\"n\":\"3\"},\"end\":15336,\"start\":15288},{\"attributes\":{\"n\":\"3.1\"},\"end\":15785,\"start\":15722},{\"attributes\":{\"n\":\"3.2\"},\"end\":20784,\"start\":20753},{\"attributes\":{\"n\":\"3.3\"},\"end\":22755,\"start\":22708},{\"attributes\":{\"n\":\"3.4\"},\"end\":24388,\"start\":24345},{\"attributes\":{\"n\":\"4\"},\"end\":25917,\"start\":25897},{\"attributes\":{\"n\":\"4.1\"},\"end\":25952,\"start\":25920},{\"attributes\":{\"n\":\"4.2\"},\"end\":28447,\"start\":28413},{\"attributes\":{\"n\":\"5\"},\"end\":31364,\"start\":31352},{\"attributes\":{\"n\":\"6\"},\"end\":34378,\"start\":34368},{\"end\":43559,\"start\":43549},{\"end\":52123,\"start\":52110},{\"end\":53586,\"start\":53527},{\"end\":56609,\"start\":56556},{\"end\":59293,\"start\":59263},{\"end\":60340,\"start\":60297},{\"end\":60365,\"start\":60343},{\"end\":61759,\"start\":61733},{\"end\":63921,\"start\":63920},{\"end\":64086,\"start\":64085},{\"end\":64212,\"start\":64211},{\"end\":64747,\"start\":64737},{\"end\":64947,\"start\":64946},{\"end\":66170,\"start\":66154},{\"end\":66468,\"start\":66462},{\"end\":68846,\"start\":68828},{\"end\":70263,\"start\":70252},{\"end\":70913,\"start\":70902},{\"end\":71230,\"start\":71221},{\"end\":72627,\"start\":72618},{\"end\":76965,\"start\":76955},{\"end\":77039,\"start\":77029}]", "table": "[{\"end\":72616,\"start\":71440},{\"end\":74363,\"start\":73601},{\"end\":76580,\"start\":76547},{\"end\":76953,\"start\":76947},{\"end\":77826,\"start\":77282},{\"end\":72616,\"start\":71440},{\"end\":74363,\"start\":73601},{\"end\":76580,\"start\":76547},{\"end\":76953,\"start\":76947},{\"end\":77826,\"start\":77282}]", "figure_caption": "[{\"end\":63918,\"start\":63737},{\"end\":64083,\"start\":63922},{\"end\":64136,\"start\":64087},{\"end\":64209,\"start\":64139},{\"end\":64735,\"start\":64213},{\"end\":64944,\"start\":64749},{\"end\":66152,\"start\":64948},{\"end\":66460,\"start\":66175},{\"end\":67911,\"start\":66471},{\"end\":68578,\"start\":67914},{\"end\":68826,\"start\":68581},{\"end\":69272,\"start\":68848},{\"end\":69776,\"start\":69275},{\"end\":70250,\"start\":69779},{\"end\":70679,\"start\":70266},{\"end\":70900,\"start\":70682},{\"end\":71219,\"start\":70916},{\"end\":71440,\"start\":71232},{\"end\":73601,\"start\":72629},{\"end\":75621,\"start\":74366},{\"end\":76547,\"start\":75624},{\"end\":76947,\"start\":76583},{\"end\":77027,\"start\":76968},{\"end\":77282,\"start\":77042},{\"end\":63918,\"start\":63737},{\"end\":64083,\"start\":63922},{\"end\":64136,\"start\":64087},{\"end\":64209,\"start\":64139},{\"end\":64735,\"start\":64213},{\"end\":64944,\"start\":64749},{\"end\":66152,\"start\":64948},{\"end\":66460,\"start\":66175},{\"end\":67911,\"start\":66471},{\"end\":68578,\"start\":67914},{\"end\":68826,\"start\":68581},{\"end\":69272,\"start\":68848},{\"end\":69776,\"start\":69275},{\"end\":70250,\"start\":69779},{\"end\":70679,\"start\":70266},{\"end\":70900,\"start\":70682},{\"end\":71219,\"start\":70916},{\"end\":71440,\"start\":71232},{\"end\":73601,\"start\":72629},{\"end\":75621,\"start\":74366},{\"end\":76547,\"start\":75624},{\"end\":76947,\"start\":76583},{\"end\":77027,\"start\":76968},{\"end\":77282,\"start\":77042}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27773,\"start\":27765},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28165,\"start\":28157},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28349,\"start\":28341},{\"end\":30317,\"start\":30309},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":57319,\"start\":57311},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27773,\"start\":27765},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28165,\"start\":28157},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28349,\"start\":28341},{\"end\":30317,\"start\":30309},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":57319,\"start\":57311}]", "bib_author_first_name": "[{\"end\":80602,\"start\":80601},{\"end\":80614,\"start\":80613},{\"end\":80625,\"start\":80624},{\"end\":80635,\"start\":80634},{\"end\":81202,\"start\":81201},{\"end\":81214,\"start\":81213},{\"end\":81226,\"start\":81225},{\"end\":81244,\"start\":81243},{\"end\":81246,\"start\":81245},{\"end\":81546,\"start\":81545},{\"end\":81553,\"start\":81552},{\"end\":81560,\"start\":81559},{\"end\":81863,\"start\":81862},{\"end\":81876,\"start\":81875},{\"end\":82189,\"start\":82188},{\"end\":82200,\"start\":82199},{\"end\":82212,\"start\":82211},{\"end\":82225,\"start\":82224},{\"end\":82237,\"start\":82236},{\"end\":82568,\"start\":82567},{\"end\":82577,\"start\":82576},{\"end\":82586,\"start\":82585},{\"end\":82599,\"start\":82598},{\"end\":82612,\"start\":82611},{\"end\":82621,\"start\":82620},{\"end\":82625,\"start\":82622},{\"end\":82633,\"start\":82632},{\"end\":82635,\"start\":82634},{\"end\":82993,\"start\":82992},{\"end\":83002,\"start\":83001},{\"end\":83008,\"start\":83007},{\"end\":83021,\"start\":83020},{\"end\":83030,\"start\":83029},{\"end\":83038,\"start\":83037},{\"end\":83348,\"start\":83347},{\"end\":83350,\"start\":83349},{\"end\":83359,\"start\":83358},{\"end\":83372,\"start\":83371},{\"end\":83380,\"start\":83379},{\"end\":83389,\"start\":83388},{\"end\":83391,\"start\":83390},{\"end\":83771,\"start\":83770},{\"end\":83784,\"start\":83783},{\"end\":83797,\"start\":83796},{\"end\":83808,\"start\":83807},{\"end\":83823,\"start\":83822},{\"end\":83833,\"start\":83832},{\"end\":83835,\"start\":83834},{\"end\":83844,\"start\":83843},{\"end\":83855,\"start\":83854},{\"end\":83869,\"start\":83868},{\"end\":83883,\"start\":83882},{\"end\":83885,\"start\":83884},{\"end\":84318,\"start\":84317},{\"end\":84327,\"start\":84326},{\"end\":84329,\"start\":84328},{\"end\":84337,\"start\":84336},{\"end\":84346,\"start\":84345},{\"end\":84350,\"start\":84347},{\"end\":84358,\"start\":84357},{\"end\":84360,\"start\":84359},{\"end\":84701,\"start\":84700},{\"end\":84710,\"start\":84709},{\"end\":84719,\"start\":84718},{\"end\":84732,\"start\":84731},{\"end\":84745,\"start\":84744},{\"end\":84754,\"start\":84753},{\"end\":84756,\"start\":84755},{\"end\":84764,\"start\":84763},{\"end\":84766,\"start\":84765},{\"end\":85083,\"start\":85082},{\"end\":85092,\"start\":85091},{\"end\":85099,\"start\":85098},{\"end\":85119,\"start\":85118},{\"end\":85374,\"start\":85373},{\"end\":85387,\"start\":85386},{\"end\":85400,\"start\":85399},{\"end\":85409,\"start\":85408},{\"end\":85417,\"start\":85416},{\"end\":85744,\"start\":85743},{\"end\":85746,\"start\":85745},{\"end\":85759,\"start\":85758},{\"end\":85772,\"start\":85771},{\"end\":85781,\"start\":85780},{\"end\":85788,\"start\":85787},{\"end\":85794,\"start\":85793},{\"end\":86149,\"start\":86148},{\"end\":86541,\"start\":86540},{\"end\":86552,\"start\":86551},{\"end\":86562,\"start\":86561},{\"end\":86564,\"start\":86563},{\"end\":86911,\"start\":86910},{\"end\":86919,\"start\":86918},{\"end\":86929,\"start\":86928},{\"end\":86948,\"start\":86947},{\"end\":86959,\"start\":86958},{\"end\":86972,\"start\":86971},{\"end\":87362,\"start\":87361},{\"end\":87371,\"start\":87370},{\"end\":87384,\"start\":87383},{\"end\":87397,\"start\":87396},{\"end\":87406,\"start\":87405},{\"end\":87413,\"start\":87412},{\"end\":87423,\"start\":87422},{\"end\":87431,\"start\":87430},{\"end\":87883,\"start\":87882},{\"end\":87892,\"start\":87891},{\"end\":87907,\"start\":87906},{\"end\":87915,\"start\":87914},{\"end\":88218,\"start\":88217},{\"end\":88226,\"start\":88225},{\"end\":88237,\"start\":88236},{\"end\":88239,\"start\":88238},{\"end\":88247,\"start\":88246},{\"end\":88257,\"start\":88256},{\"end\":88259,\"start\":88258},{\"end\":88592,\"start\":88591},{\"end\":88594,\"start\":88593},{\"end\":88604,\"start\":88603},{\"end\":88809,\"start\":88808},{\"end\":88816,\"start\":88815},{\"end\":88916,\"start\":88915},{\"end\":88922,\"start\":88921},{\"end\":88924,\"start\":88923},{\"end\":89265,\"start\":89264},{\"end\":89274,\"start\":89273},{\"end\":89285,\"start\":89284},{\"end\":89296,\"start\":89295},{\"end\":89307,\"start\":89306},{\"end\":89647,\"start\":89646},{\"end\":89657,\"start\":89656},{\"end\":89665,\"start\":89664},{\"end\":89902,\"start\":89901},{\"end\":89904,\"start\":89903},{\"end\":89914,\"start\":89913},{\"end\":89927,\"start\":89926},{\"end\":89936,\"start\":89935},{\"end\":89947,\"start\":89946},{\"end\":90289,\"start\":90288},{\"end\":90302,\"start\":90301},{\"end\":90310,\"start\":90309},{\"end\":90317,\"start\":90316},{\"end\":90330,\"start\":90329},{\"end\":90613,\"start\":90612},{\"end\":90628,\"start\":90627},{\"end\":90642,\"start\":90641},{\"end\":90644,\"start\":90643},{\"end\":90961,\"start\":90960},{\"end\":90963,\"start\":90962},{\"end\":90971,\"start\":90970},{\"end\":90983,\"start\":90982},{\"end\":90985,\"start\":90984},{\"end\":90999,\"start\":90998},{\"end\":91270,\"start\":91269},{\"end\":91280,\"start\":91279},{\"end\":91288,\"start\":91287},{\"end\":91300,\"start\":91296},{\"end\":91309,\"start\":91308},{\"end\":91630,\"start\":91629},{\"end\":91637,\"start\":91636},{\"end\":91649,\"start\":91645},{\"end\":91658,\"start\":91657},{\"end\":91670,\"start\":91666},{\"end\":91992,\"start\":91991},{\"end\":91999,\"start\":91998},{\"end\":92007,\"start\":92006},{\"end\":92016,\"start\":92015},{\"end\":92025,\"start\":92021},{\"end\":92342,\"start\":92341},{\"end\":92351,\"start\":92350},{\"end\":92359,\"start\":92358},{\"end\":92369,\"start\":92368},{\"end\":92380,\"start\":92379},{\"end\":92678,\"start\":92677},{\"end\":92687,\"start\":92686},{\"end\":92695,\"start\":92694},{\"end\":92706,\"start\":92705},{\"end\":93008,\"start\":93007},{\"end\":93017,\"start\":93016},{\"end\":93027,\"start\":93026},{\"end\":93038,\"start\":93037},{\"end\":93353,\"start\":93352},{\"end\":93362,\"start\":93361},{\"end\":93370,\"start\":93369},{\"end\":93381,\"start\":93380},{\"end\":93757,\"start\":93756},{\"end\":93775,\"start\":93774},{\"end\":93787,\"start\":93786},{\"end\":93799,\"start\":93798},{\"end\":93805,\"start\":93804},{\"end\":93814,\"start\":93813},{\"end\":93816,\"start\":93815},{\"end\":94223,\"start\":94222},{\"end\":94232,\"start\":94231},{\"end\":94240,\"start\":94239},{\"end\":94488,\"start\":94487},{\"end\":94496,\"start\":94495},{\"end\":94504,\"start\":94503},{\"end\":94513,\"start\":94512},{\"end\":94813,\"start\":94812},{\"end\":94821,\"start\":94820},{\"end\":94828,\"start\":94827},{\"end\":94842,\"start\":94841},{\"end\":94850,\"start\":94849},{\"end\":95167,\"start\":95166},{\"end\":95175,\"start\":95174},{\"end\":95182,\"start\":95181},{\"end\":95196,\"start\":95195},{\"end\":95204,\"start\":95203},{\"end\":95490,\"start\":95489},{\"end\":95498,\"start\":95497},{\"end\":95776,\"start\":95775},{\"end\":95784,\"start\":95783},{\"end\":95795,\"start\":95794},{\"end\":95797,\"start\":95796},{\"end\":95807,\"start\":95806},{\"end\":95809,\"start\":95808},{\"end\":96121,\"start\":96120},{\"end\":96127,\"start\":96126},{\"end\":96134,\"start\":96133},{\"end\":96143,\"start\":96142},{\"end\":96154,\"start\":96150},{\"end\":96163,\"start\":96162},{\"end\":96172,\"start\":96171},{\"end\":96185,\"start\":96184},{\"end\":96195,\"start\":96191},{\"end\":80602,\"start\":80601},{\"end\":80614,\"start\":80613},{\"end\":80625,\"start\":80624},{\"end\":80635,\"start\":80634},{\"end\":81202,\"start\":81201},{\"end\":81214,\"start\":81213},{\"end\":81226,\"start\":81225},{\"end\":81244,\"start\":81243},{\"end\":81246,\"start\":81245},{\"end\":81546,\"start\":81545},{\"end\":81553,\"start\":81552},{\"end\":81560,\"start\":81559},{\"end\":81863,\"start\":81862},{\"end\":81876,\"start\":81875},{\"end\":82189,\"start\":82188},{\"end\":82200,\"start\":82199},{\"end\":82212,\"start\":82211},{\"end\":82225,\"start\":82224},{\"end\":82237,\"start\":82236},{\"end\":82568,\"start\":82567},{\"end\":82577,\"start\":82576},{\"end\":82586,\"start\":82585},{\"end\":82599,\"start\":82598},{\"end\":82612,\"start\":82611},{\"end\":82621,\"start\":82620},{\"end\":82625,\"start\":82622},{\"end\":82633,\"start\":82632},{\"end\":82635,\"start\":82634},{\"end\":82993,\"start\":82992},{\"end\":83002,\"start\":83001},{\"end\":83008,\"start\":83007},{\"end\":83021,\"start\":83020},{\"end\":83030,\"start\":83029},{\"end\":83038,\"start\":83037},{\"end\":83348,\"start\":83347},{\"end\":83350,\"start\":83349},{\"end\":83359,\"start\":83358},{\"end\":83372,\"start\":83371},{\"end\":83380,\"start\":83379},{\"end\":83389,\"start\":83388},{\"end\":83391,\"start\":83390},{\"end\":83771,\"start\":83770},{\"end\":83784,\"start\":83783},{\"end\":83797,\"start\":83796},{\"end\":83808,\"start\":83807},{\"end\":83823,\"start\":83822},{\"end\":83833,\"start\":83832},{\"end\":83835,\"start\":83834},{\"end\":83844,\"start\":83843},{\"end\":83855,\"start\":83854},{\"end\":83869,\"start\":83868},{\"end\":83883,\"start\":83882},{\"end\":83885,\"start\":83884},{\"end\":84318,\"start\":84317},{\"end\":84327,\"start\":84326},{\"end\":84329,\"start\":84328},{\"end\":84337,\"start\":84336},{\"end\":84346,\"start\":84345},{\"end\":84350,\"start\":84347},{\"end\":84358,\"start\":84357},{\"end\":84360,\"start\":84359},{\"end\":84701,\"start\":84700},{\"end\":84710,\"start\":84709},{\"end\":84719,\"start\":84718},{\"end\":84732,\"start\":84731},{\"end\":84745,\"start\":84744},{\"end\":84754,\"start\":84753},{\"end\":84756,\"start\":84755},{\"end\":84764,\"start\":84763},{\"end\":84766,\"start\":84765},{\"end\":85083,\"start\":85082},{\"end\":85092,\"start\":85091},{\"end\":85099,\"start\":85098},{\"end\":85119,\"start\":85118},{\"end\":85374,\"start\":85373},{\"end\":85387,\"start\":85386},{\"end\":85400,\"start\":85399},{\"end\":85409,\"start\":85408},{\"end\":85417,\"start\":85416},{\"end\":85744,\"start\":85743},{\"end\":85746,\"start\":85745},{\"end\":85759,\"start\":85758},{\"end\":85772,\"start\":85771},{\"end\":85781,\"start\":85780},{\"end\":85788,\"start\":85787},{\"end\":85794,\"start\":85793},{\"end\":86149,\"start\":86148},{\"end\":86541,\"start\":86540},{\"end\":86552,\"start\":86551},{\"end\":86562,\"start\":86561},{\"end\":86564,\"start\":86563},{\"end\":86911,\"start\":86910},{\"end\":86919,\"start\":86918},{\"end\":86929,\"start\":86928},{\"end\":86948,\"start\":86947},{\"end\":86959,\"start\":86958},{\"end\":86972,\"start\":86971},{\"end\":87362,\"start\":87361},{\"end\":87371,\"start\":87370},{\"end\":87384,\"start\":87383},{\"end\":87397,\"start\":87396},{\"end\":87406,\"start\":87405},{\"end\":87413,\"start\":87412},{\"end\":87423,\"start\":87422},{\"end\":87431,\"start\":87430},{\"end\":87883,\"start\":87882},{\"end\":87892,\"start\":87891},{\"end\":87907,\"start\":87906},{\"end\":87915,\"start\":87914},{\"end\":88218,\"start\":88217},{\"end\":88226,\"start\":88225},{\"end\":88237,\"start\":88236},{\"end\":88239,\"start\":88238},{\"end\":88247,\"start\":88246},{\"end\":88257,\"start\":88256},{\"end\":88259,\"start\":88258},{\"end\":88592,\"start\":88591},{\"end\":88594,\"start\":88593},{\"end\":88604,\"start\":88603},{\"end\":88809,\"start\":88808},{\"end\":88816,\"start\":88815},{\"end\":88916,\"start\":88915},{\"end\":88922,\"start\":88921},{\"end\":88924,\"start\":88923},{\"end\":89265,\"start\":89264},{\"end\":89274,\"start\":89273},{\"end\":89285,\"start\":89284},{\"end\":89296,\"start\":89295},{\"end\":89307,\"start\":89306},{\"end\":89647,\"start\":89646},{\"end\":89657,\"start\":89656},{\"end\":89665,\"start\":89664},{\"end\":89902,\"start\":89901},{\"end\":89904,\"start\":89903},{\"end\":89914,\"start\":89913},{\"end\":89927,\"start\":89926},{\"end\":89936,\"start\":89935},{\"end\":89947,\"start\":89946},{\"end\":90289,\"start\":90288},{\"end\":90302,\"start\":90301},{\"end\":90310,\"start\":90309},{\"end\":90317,\"start\":90316},{\"end\":90330,\"start\":90329},{\"end\":90613,\"start\":90612},{\"end\":90628,\"start\":90627},{\"end\":90642,\"start\":90641},{\"end\":90644,\"start\":90643},{\"end\":90961,\"start\":90960},{\"end\":90963,\"start\":90962},{\"end\":90971,\"start\":90970},{\"end\":90983,\"start\":90982},{\"end\":90985,\"start\":90984},{\"end\":90999,\"start\":90998},{\"end\":91270,\"start\":91269},{\"end\":91280,\"start\":91279},{\"end\":91288,\"start\":91287},{\"end\":91300,\"start\":91296},{\"end\":91309,\"start\":91308},{\"end\":91630,\"start\":91629},{\"end\":91637,\"start\":91636},{\"end\":91649,\"start\":91645},{\"end\":91658,\"start\":91657},{\"end\":91670,\"start\":91666},{\"end\":91992,\"start\":91991},{\"end\":91999,\"start\":91998},{\"end\":92007,\"start\":92006},{\"end\":92016,\"start\":92015},{\"end\":92025,\"start\":92021},{\"end\":92342,\"start\":92341},{\"end\":92351,\"start\":92350},{\"end\":92359,\"start\":92358},{\"end\":92369,\"start\":92368},{\"end\":92380,\"start\":92379},{\"end\":92678,\"start\":92677},{\"end\":92687,\"start\":92686},{\"end\":92695,\"start\":92694},{\"end\":92706,\"start\":92705},{\"end\":93008,\"start\":93007},{\"end\":93017,\"start\":93016},{\"end\":93027,\"start\":93026},{\"end\":93038,\"start\":93037},{\"end\":93353,\"start\":93352},{\"end\":93362,\"start\":93361},{\"end\":93370,\"start\":93369},{\"end\":93381,\"start\":93380},{\"end\":93757,\"start\":93756},{\"end\":93775,\"start\":93774},{\"end\":93787,\"start\":93786},{\"end\":93799,\"start\":93798},{\"end\":93805,\"start\":93804},{\"end\":93814,\"start\":93813},{\"end\":93816,\"start\":93815},{\"end\":94223,\"start\":94222},{\"end\":94232,\"start\":94231},{\"end\":94240,\"start\":94239},{\"end\":94488,\"start\":94487},{\"end\":94496,\"start\":94495},{\"end\":94504,\"start\":94503},{\"end\":94513,\"start\":94512},{\"end\":94813,\"start\":94812},{\"end\":94821,\"start\":94820},{\"end\":94828,\"start\":94827},{\"end\":94842,\"start\":94841},{\"end\":94850,\"start\":94849},{\"end\":95167,\"start\":95166},{\"end\":95175,\"start\":95174},{\"end\":95182,\"start\":95181},{\"end\":95196,\"start\":95195},{\"end\":95204,\"start\":95203},{\"end\":95490,\"start\":95489},{\"end\":95498,\"start\":95497},{\"end\":95776,\"start\":95775},{\"end\":95784,\"start\":95783},{\"end\":95795,\"start\":95794},{\"end\":95797,\"start\":95796},{\"end\":95807,\"start\":95806},{\"end\":95809,\"start\":95808},{\"end\":96121,\"start\":96120},{\"end\":96127,\"start\":96126},{\"end\":96134,\"start\":96133},{\"end\":96143,\"start\":96142},{\"end\":96154,\"start\":96150},{\"end\":96163,\"start\":96162},{\"end\":96172,\"start\":96171},{\"end\":96185,\"start\":96184},{\"end\":96195,\"start\":96191}]", "bib_author_last_name": "[{\"end\":80611,\"start\":80603},{\"end\":80622,\"start\":80615},{\"end\":80632,\"start\":80626},{\"end\":80645,\"start\":80636},{\"end\":81211,\"start\":81203},{\"end\":81223,\"start\":81215},{\"end\":81241,\"start\":81227},{\"end\":81253,\"start\":81247},{\"end\":81550,\"start\":81547},{\"end\":81557,\"start\":81554},{\"end\":81568,\"start\":81561},{\"end\":81873,\"start\":81864},{\"end\":81883,\"start\":81877},{\"end\":82197,\"start\":82190},{\"end\":82209,\"start\":82201},{\"end\":82222,\"start\":82213},{\"end\":82234,\"start\":82226},{\"end\":82245,\"start\":82238},{\"end\":82574,\"start\":82569},{\"end\":82583,\"start\":82578},{\"end\":82596,\"start\":82587},{\"end\":82609,\"start\":82600},{\"end\":82618,\"start\":82613},{\"end\":82630,\"start\":82626},{\"end\":82641,\"start\":82636},{\"end\":82999,\"start\":82994},{\"end\":83005,\"start\":83003},{\"end\":83018,\"start\":83009},{\"end\":83027,\"start\":83022},{\"end\":83035,\"start\":83031},{\"end\":83048,\"start\":83039},{\"end\":83356,\"start\":83351},{\"end\":83369,\"start\":83360},{\"end\":83377,\"start\":83373},{\"end\":83386,\"start\":83381},{\"end\":83401,\"start\":83392},{\"end\":83781,\"start\":83772},{\"end\":83794,\"start\":83785},{\"end\":83805,\"start\":83798},{\"end\":83820,\"start\":83809},{\"end\":83830,\"start\":83824},{\"end\":83841,\"start\":83836},{\"end\":83852,\"start\":83845},{\"end\":83866,\"start\":83856},{\"end\":83880,\"start\":83870},{\"end\":83891,\"start\":83886},{\"end\":84324,\"start\":84319},{\"end\":84334,\"start\":84330},{\"end\":84343,\"start\":84338},{\"end\":84355,\"start\":84351},{\"end\":84366,\"start\":84361},{\"end\":84707,\"start\":84702},{\"end\":84716,\"start\":84711},{\"end\":84729,\"start\":84720},{\"end\":84742,\"start\":84733},{\"end\":84751,\"start\":84746},{\"end\":84761,\"start\":84757},{\"end\":84772,\"start\":84767},{\"end\":85089,\"start\":85084},{\"end\":85096,\"start\":85093},{\"end\":85116,\"start\":85100},{\"end\":85126,\"start\":85120},{\"end\":85384,\"start\":85375},{\"end\":85397,\"start\":85388},{\"end\":85406,\"start\":85401},{\"end\":85414,\"start\":85410},{\"end\":85423,\"start\":85418},{\"end\":85756,\"start\":85747},{\"end\":85769,\"start\":85760},{\"end\":85778,\"start\":85773},{\"end\":85785,\"start\":85782},{\"end\":85791,\"start\":85789},{\"end\":85800,\"start\":85795},{\"end\":86156,\"start\":86150},{\"end\":86549,\"start\":86542},{\"end\":86559,\"start\":86553},{\"end\":86571,\"start\":86565},{\"end\":86916,\"start\":86912},{\"end\":86926,\"start\":86920},{\"end\":86945,\"start\":86930},{\"end\":86956,\"start\":86949},{\"end\":86969,\"start\":86960},{\"end\":86979,\"start\":86973},{\"end\":87368,\"start\":87363},{\"end\":87381,\"start\":87372},{\"end\":87394,\"start\":87385},{\"end\":87403,\"start\":87398},{\"end\":87410,\"start\":87407},{\"end\":87420,\"start\":87414},{\"end\":87428,\"start\":87424},{\"end\":87437,\"start\":87432},{\"end\":87889,\"start\":87884},{\"end\":87904,\"start\":87893},{\"end\":87912,\"start\":87908},{\"end\":87918,\"start\":87916},{\"end\":88223,\"start\":88219},{\"end\":88234,\"start\":88227},{\"end\":88244,\"start\":88240},{\"end\":88254,\"start\":88248},{\"end\":88272,\"start\":88260},{\"end\":88601,\"start\":88595},{\"end\":88607,\"start\":88605},{\"end\":88813,\"start\":88810},{\"end\":88824,\"start\":88817},{\"end\":88919,\"start\":88917},{\"end\":88930,\"start\":88925},{\"end\":89271,\"start\":89266},{\"end\":89282,\"start\":89275},{\"end\":89293,\"start\":89286},{\"end\":89304,\"start\":89297},{\"end\":89313,\"start\":89308},{\"end\":89654,\"start\":89648},{\"end\":89662,\"start\":89658},{\"end\":89672,\"start\":89666},{\"end\":89911,\"start\":89905},{\"end\":89924,\"start\":89915},{\"end\":89933,\"start\":89928},{\"end\":89944,\"start\":89937},{\"end\":89954,\"start\":89948},{\"end\":90299,\"start\":90290},{\"end\":90307,\"start\":90303},{\"end\":90314,\"start\":90311},{\"end\":90327,\"start\":90318},{\"end\":90340,\"start\":90331},{\"end\":90625,\"start\":90614},{\"end\":90639,\"start\":90629},{\"end\":90650,\"start\":90645},{\"end\":90968,\"start\":90964},{\"end\":90980,\"start\":90972},{\"end\":90996,\"start\":90986},{\"end\":91006,\"start\":91000},{\"end\":91277,\"start\":91271},{\"end\":91285,\"start\":91281},{\"end\":91294,\"start\":91289},{\"end\":91306,\"start\":91301},{\"end\":91315,\"start\":91310},{\"end\":91634,\"start\":91631},{\"end\":91643,\"start\":91638},{\"end\":91655,\"start\":91650},{\"end\":91664,\"start\":91659},{\"end\":91676,\"start\":91671},{\"end\":91996,\"start\":91993},{\"end\":92004,\"start\":92000},{\"end\":92013,\"start\":92008},{\"end\":92019,\"start\":92017},{\"end\":92031,\"start\":92026},{\"end\":92348,\"start\":92343},{\"end\":92356,\"start\":92352},{\"end\":92366,\"start\":92360},{\"end\":92377,\"start\":92370},{\"end\":92387,\"start\":92381},{\"end\":92684,\"start\":92679},{\"end\":92692,\"start\":92688},{\"end\":92703,\"start\":92696},{\"end\":92713,\"start\":92707},{\"end\":93014,\"start\":93009},{\"end\":93024,\"start\":93018},{\"end\":93035,\"start\":93028},{\"end\":93045,\"start\":93039},{\"end\":93359,\"start\":93354},{\"end\":93367,\"start\":93363},{\"end\":93378,\"start\":93371},{\"end\":93388,\"start\":93382},{\"end\":93772,\"start\":93758},{\"end\":93784,\"start\":93776},{\"end\":93796,\"start\":93788},{\"end\":93802,\"start\":93800},{\"end\":93811,\"start\":93806},{\"end\":93823,\"start\":93817},{\"end\":94229,\"start\":94224},{\"end\":94237,\"start\":94233},{\"end\":94248,\"start\":94241},{\"end\":94493,\"start\":94489},{\"end\":94501,\"start\":94497},{\"end\":94510,\"start\":94505},{\"end\":94518,\"start\":94514},{\"end\":94528,\"start\":94520},{\"end\":94818,\"start\":94814},{\"end\":94825,\"start\":94822},{\"end\":94839,\"start\":94829},{\"end\":94847,\"start\":94843},{\"end\":94855,\"start\":94851},{\"end\":95172,\"start\":95168},{\"end\":95179,\"start\":95176},{\"end\":95193,\"start\":95183},{\"end\":95201,\"start\":95197},{\"end\":95209,\"start\":95205},{\"end\":95495,\"start\":95491},{\"end\":95505,\"start\":95499},{\"end\":95781,\"start\":95777},{\"end\":95792,\"start\":95785},{\"end\":95804,\"start\":95798},{\"end\":95816,\"start\":95810},{\"end\":96124,\"start\":96122},{\"end\":96131,\"start\":96128},{\"end\":96140,\"start\":96135},{\"end\":96148,\"start\":96144},{\"end\":96160,\"start\":96155},{\"end\":96169,\"start\":96164},{\"end\":96182,\"start\":96173},{\"end\":96189,\"start\":96186},{\"end\":96201,\"start\":96196},{\"end\":96971,\"start\":96967},{\"end\":97176,\"start\":97167},{\"end\":80611,\"start\":80603},{\"end\":80622,\"start\":80615},{\"end\":80632,\"start\":80626},{\"end\":80645,\"start\":80636},{\"end\":81211,\"start\":81203},{\"end\":81223,\"start\":81215},{\"end\":81241,\"start\":81227},{\"end\":81253,\"start\":81247},{\"end\":81550,\"start\":81547},{\"end\":81557,\"start\":81554},{\"end\":81568,\"start\":81561},{\"end\":81873,\"start\":81864},{\"end\":81883,\"start\":81877},{\"end\":82197,\"start\":82190},{\"end\":82209,\"start\":82201},{\"end\":82222,\"start\":82213},{\"end\":82234,\"start\":82226},{\"end\":82245,\"start\":82238},{\"end\":82574,\"start\":82569},{\"end\":82583,\"start\":82578},{\"end\":82596,\"start\":82587},{\"end\":82609,\"start\":82600},{\"end\":82618,\"start\":82613},{\"end\":82630,\"start\":82626},{\"end\":82641,\"start\":82636},{\"end\":82999,\"start\":82994},{\"end\":83005,\"start\":83003},{\"end\":83018,\"start\":83009},{\"end\":83027,\"start\":83022},{\"end\":83035,\"start\":83031},{\"end\":83048,\"start\":83039},{\"end\":83356,\"start\":83351},{\"end\":83369,\"start\":83360},{\"end\":83377,\"start\":83373},{\"end\":83386,\"start\":83381},{\"end\":83401,\"start\":83392},{\"end\":83781,\"start\":83772},{\"end\":83794,\"start\":83785},{\"end\":83805,\"start\":83798},{\"end\":83820,\"start\":83809},{\"end\":83830,\"start\":83824},{\"end\":83841,\"start\":83836},{\"end\":83852,\"start\":83845},{\"end\":83866,\"start\":83856},{\"end\":83880,\"start\":83870},{\"end\":83891,\"start\":83886},{\"end\":84324,\"start\":84319},{\"end\":84334,\"start\":84330},{\"end\":84343,\"start\":84338},{\"end\":84355,\"start\":84351},{\"end\":84366,\"start\":84361},{\"end\":84707,\"start\":84702},{\"end\":84716,\"start\":84711},{\"end\":84729,\"start\":84720},{\"end\":84742,\"start\":84733},{\"end\":84751,\"start\":84746},{\"end\":84761,\"start\":84757},{\"end\":84772,\"start\":84767},{\"end\":85089,\"start\":85084},{\"end\":85096,\"start\":85093},{\"end\":85116,\"start\":85100},{\"end\":85126,\"start\":85120},{\"end\":85384,\"start\":85375},{\"end\":85397,\"start\":85388},{\"end\":85406,\"start\":85401},{\"end\":85414,\"start\":85410},{\"end\":85423,\"start\":85418},{\"end\":85756,\"start\":85747},{\"end\":85769,\"start\":85760},{\"end\":85778,\"start\":85773},{\"end\":85785,\"start\":85782},{\"end\":85791,\"start\":85789},{\"end\":85800,\"start\":85795},{\"end\":86156,\"start\":86150},{\"end\":86549,\"start\":86542},{\"end\":86559,\"start\":86553},{\"end\":86571,\"start\":86565},{\"end\":86916,\"start\":86912},{\"end\":86926,\"start\":86920},{\"end\":86945,\"start\":86930},{\"end\":86956,\"start\":86949},{\"end\":86969,\"start\":86960},{\"end\":86979,\"start\":86973},{\"end\":87368,\"start\":87363},{\"end\":87381,\"start\":87372},{\"end\":87394,\"start\":87385},{\"end\":87403,\"start\":87398},{\"end\":87410,\"start\":87407},{\"end\":87420,\"start\":87414},{\"end\":87428,\"start\":87424},{\"end\":87437,\"start\":87432},{\"end\":87889,\"start\":87884},{\"end\":87904,\"start\":87893},{\"end\":87912,\"start\":87908},{\"end\":87918,\"start\":87916},{\"end\":88223,\"start\":88219},{\"end\":88234,\"start\":88227},{\"end\":88244,\"start\":88240},{\"end\":88254,\"start\":88248},{\"end\":88272,\"start\":88260},{\"end\":88601,\"start\":88595},{\"end\":88607,\"start\":88605},{\"end\":88813,\"start\":88810},{\"end\":88824,\"start\":88817},{\"end\":88919,\"start\":88917},{\"end\":88930,\"start\":88925},{\"end\":89271,\"start\":89266},{\"end\":89282,\"start\":89275},{\"end\":89293,\"start\":89286},{\"end\":89304,\"start\":89297},{\"end\":89313,\"start\":89308},{\"end\":89654,\"start\":89648},{\"end\":89662,\"start\":89658},{\"end\":89672,\"start\":89666},{\"end\":89911,\"start\":89905},{\"end\":89924,\"start\":89915},{\"end\":89933,\"start\":89928},{\"end\":89944,\"start\":89937},{\"end\":89954,\"start\":89948},{\"end\":90299,\"start\":90290},{\"end\":90307,\"start\":90303},{\"end\":90314,\"start\":90311},{\"end\":90327,\"start\":90318},{\"end\":90340,\"start\":90331},{\"end\":90625,\"start\":90614},{\"end\":90639,\"start\":90629},{\"end\":90650,\"start\":90645},{\"end\":90968,\"start\":90964},{\"end\":90980,\"start\":90972},{\"end\":90996,\"start\":90986},{\"end\":91006,\"start\":91000},{\"end\":91277,\"start\":91271},{\"end\":91285,\"start\":91281},{\"end\":91294,\"start\":91289},{\"end\":91306,\"start\":91301},{\"end\":91315,\"start\":91310},{\"end\":91634,\"start\":91631},{\"end\":91643,\"start\":91638},{\"end\":91655,\"start\":91650},{\"end\":91664,\"start\":91659},{\"end\":91676,\"start\":91671},{\"end\":91996,\"start\":91993},{\"end\":92004,\"start\":92000},{\"end\":92013,\"start\":92008},{\"end\":92019,\"start\":92017},{\"end\":92031,\"start\":92026},{\"end\":92348,\"start\":92343},{\"end\":92356,\"start\":92352},{\"end\":92366,\"start\":92360},{\"end\":92377,\"start\":92370},{\"end\":92387,\"start\":92381},{\"end\":92684,\"start\":92679},{\"end\":92692,\"start\":92688},{\"end\":92703,\"start\":92696},{\"end\":92713,\"start\":92707},{\"end\":93014,\"start\":93009},{\"end\":93024,\"start\":93018},{\"end\":93035,\"start\":93028},{\"end\":93045,\"start\":93039},{\"end\":93359,\"start\":93354},{\"end\":93367,\"start\":93363},{\"end\":93378,\"start\":93371},{\"end\":93388,\"start\":93382},{\"end\":93772,\"start\":93758},{\"end\":93784,\"start\":93776},{\"end\":93796,\"start\":93788},{\"end\":93802,\"start\":93800},{\"end\":93811,\"start\":93806},{\"end\":93823,\"start\":93817},{\"end\":94229,\"start\":94224},{\"end\":94237,\"start\":94233},{\"end\":94248,\"start\":94241},{\"end\":94493,\"start\":94489},{\"end\":94501,\"start\":94497},{\"end\":94510,\"start\":94505},{\"end\":94518,\"start\":94514},{\"end\":94528,\"start\":94520},{\"end\":94818,\"start\":94814},{\"end\":94825,\"start\":94822},{\"end\":94839,\"start\":94829},{\"end\":94847,\"start\":94843},{\"end\":94855,\"start\":94851},{\"end\":95172,\"start\":95168},{\"end\":95179,\"start\":95176},{\"end\":95193,\"start\":95183},{\"end\":95201,\"start\":95197},{\"end\":95209,\"start\":95205},{\"end\":95495,\"start\":95491},{\"end\":95505,\"start\":95499},{\"end\":95781,\"start\":95777},{\"end\":95792,\"start\":95785},{\"end\":95804,\"start\":95798},{\"end\":95816,\"start\":95810},{\"end\":96124,\"start\":96122},{\"end\":96131,\"start\":96128},{\"end\":96140,\"start\":96135},{\"end\":96148,\"start\":96144},{\"end\":96160,\"start\":96155},{\"end\":96169,\"start\":96164},{\"end\":96182,\"start\":96173},{\"end\":96189,\"start\":96186},{\"end\":96201,\"start\":96196},{\"end\":96971,\"start\":96967},{\"end\":97176,\"start\":97167}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":127982021},\"end\":81101,\"start\":80507},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":209880690},\"end\":81473,\"start\":81103},{\"attributes\":{\"doi\":\"arXiv:2109.00498\",\"id\":\"b2\",\"matched_paper_id\":237372248},\"end\":81798,\"start\":81475},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":214108145},\"end\":82108,\"start\":81800},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":213299187},\"end\":82507,\"start\":82110},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":211259417},\"end\":82923,\"start\":82509},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":202577669},\"end\":83281,\"start\":82925},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":41612217},\"end\":83664,\"start\":83283},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":225039902},\"end\":84270,\"start\":83666},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":235613337},\"end\":84610,\"start\":84272},{\"attributes\":{\"doi\":\"arXiv:2104.06718\",\"id\":\"b10\"},\"end\":85020,\"start\":84612},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3332132},\"end\":85312,\"start\":85022},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3972365},\"end\":85670,\"start\":85314},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":197661813},\"end\":86075,\"start\":85672},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1931807},\"end\":86419,\"start\":86077},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":67856043},\"end\":86818,\"start\":86421},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206579396},\"end\":87269,\"start\":86820},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53112003},\"end\":87835,\"start\":87271},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11626373},\"end\":88145,\"start\":87837},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":516928},\"end\":88545,\"start\":88147},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6628106},\"end\":88804,\"start\":88547},{\"attributes\":{\"id\":\"b21\"},\"end\":88913,\"start\":88806},{\"attributes\":{\"id\":\"b22\"},\"end\":89199,\"start\":88915},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3488815},\"end\":89568,\"start\":89201},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":51872670},\"end\":89899,\"start\":89570},{\"attributes\":{\"doi\":\"arXiv:2103.03638\",\"id\":\"b25\"},\"end\":90225,\"start\":89901},{\"attributes\":{\"doi\":\"arXiv:1712.03632\",\"id\":\"b26\"},\"end\":90534,\"start\":90227},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":53215541},\"end\":90906,\"start\":90536},{\"attributes\":{\"doi\":\"arXiv:1902.07247\",\"id\":\"b28\"},\"end\":91186,\"start\":90908},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":67855530},\"end\":91585,\"start\":91188},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":211133221},\"end\":91912,\"start\":91587},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":232427749},\"end\":92294,\"start\":91914},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53960414},\"end\":92621,\"start\":92296},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":196059499},\"end\":92931,\"start\":92623},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":207796611},\"end\":93299,\"start\":92933},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":57757287},\"end\":93643,\"start\":93301},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":220055765},\"end\":94147,\"start\":93645},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":47016770},\"end\":94485,\"start\":94149},{\"attributes\":{\"doi\":\"arXiv:1811.02625\",\"id\":\"b38\"},\"end\":94757,\"start\":94487},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":52347370},\"end\":95094,\"start\":94759},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13745458},\"end\":95397,\"start\":95096},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3659467},\"end\":95734,\"start\":95399},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":44124705},\"end\":96040,\"start\":95736},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":225086082},\"end\":96735,\"start\":96042},{\"attributes\":{\"id\":\"b44\"},\"end\":96905,\"start\":96737},{\"attributes\":{\"id\":\"b45\"},\"end\":96939,\"start\":96907},{\"attributes\":{\"id\":\"b46\"},\"end\":97163,\"start\":96941},{\"attributes\":{\"doi\":\"CIFAR-10\",\"id\":\"b47\"},\"end\":97206,\"start\":97165},{\"attributes\":{\"id\":\"b48\"},\"end\":97479,\"start\":97208},{\"attributes\":{\"id\":\"b49\"},\"end\":97641,\"start\":97481},{\"attributes\":{\"id\":\"b50\"},\"end\":97821,\"start\":97643},{\"attributes\":{\"id\":\"b51\"},\"end\":97855,\"start\":97823},{\"attributes\":{\"id\":\"b52\"},\"end\":100138,\"start\":97857},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":127982021},\"end\":81101,\"start\":80507},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":209880690},\"end\":81473,\"start\":81103},{\"attributes\":{\"doi\":\"arXiv:2109.00498\",\"id\":\"b2\",\"matched_paper_id\":237372248},\"end\":81798,\"start\":81475},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":214108145},\"end\":82108,\"start\":81800},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":213299187},\"end\":82507,\"start\":82110},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":211259417},\"end\":82923,\"start\":82509},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":202577669},\"end\":83281,\"start\":82925},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":41612217},\"end\":83664,\"start\":83283},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":225039902},\"end\":84270,\"start\":83666},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":235613337},\"end\":84610,\"start\":84272},{\"attributes\":{\"doi\":\"arXiv:2104.06718\",\"id\":\"b10\"},\"end\":85020,\"start\":84612},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3332132},\"end\":85312,\"start\":85022},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3972365},\"end\":85670,\"start\":85314},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":197661813},\"end\":86075,\"start\":85672},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1931807},\"end\":86419,\"start\":86077},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":67856043},\"end\":86818,\"start\":86421},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206579396},\"end\":87269,\"start\":86820},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53112003},\"end\":87835,\"start\":87271},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11626373},\"end\":88145,\"start\":87837},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":516928},\"end\":88545,\"start\":88147},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6628106},\"end\":88804,\"start\":88547},{\"attributes\":{\"id\":\"b21\"},\"end\":88913,\"start\":88806},{\"attributes\":{\"id\":\"b22\"},\"end\":89199,\"start\":88915},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3488815},\"end\":89568,\"start\":89201},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":51872670},\"end\":89899,\"start\":89570},{\"attributes\":{\"doi\":\"arXiv:2103.03638\",\"id\":\"b25\"},\"end\":90225,\"start\":89901},{\"attributes\":{\"doi\":\"arXiv:1712.03632\",\"id\":\"b26\"},\"end\":90534,\"start\":90227},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":53215541},\"end\":90906,\"start\":90536},{\"attributes\":{\"doi\":\"arXiv:1902.07247\",\"id\":\"b28\"},\"end\":91186,\"start\":90908},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":67855530},\"end\":91585,\"start\":91188},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":211133221},\"end\":91912,\"start\":91587},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":232427749},\"end\":92294,\"start\":91914},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53960414},\"end\":92621,\"start\":92296},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":196059499},\"end\":92931,\"start\":92623},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":207796611},\"end\":93299,\"start\":92933},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":57757287},\"end\":93643,\"start\":93301},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":220055765},\"end\":94147,\"start\":93645},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":47016770},\"end\":94485,\"start\":94149},{\"attributes\":{\"doi\":\"arXiv:1811.02625\",\"id\":\"b38\"},\"end\":94757,\"start\":94487},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":52347370},\"end\":95094,\"start\":94759},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13745458},\"end\":95397,\"start\":95096},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3659467},\"end\":95734,\"start\":95399},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":44124705},\"end\":96040,\"start\":95736},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":225086082},\"end\":96735,\"start\":96042},{\"attributes\":{\"id\":\"b44\"},\"end\":96905,\"start\":96737},{\"attributes\":{\"id\":\"b45\"},\"end\":96939,\"start\":96907},{\"attributes\":{\"id\":\"b46\"},\"end\":97163,\"start\":96941},{\"attributes\":{\"doi\":\"CIFAR-10\",\"id\":\"b47\"},\"end\":97206,\"start\":97165},{\"attributes\":{\"id\":\"b48\"},\"end\":97479,\"start\":97208},{\"attributes\":{\"id\":\"b49\"},\"end\":97641,\"start\":97481},{\"attributes\":{\"id\":\"b50\"},\"end\":97821,\"start\":97643},{\"attributes\":{\"id\":\"b51\"},\"end\":97855,\"start\":97823},{\"attributes\":{\"id\":\"b52\"},\"end\":100138,\"start\":97857}]", "bib_title": "[{\"end\":80599,\"start\":80507},{\"end\":81199,\"start\":81103},{\"end\":81543,\"start\":81475},{\"end\":81860,\"start\":81800},{\"end\":82186,\"start\":82110},{\"end\":82565,\"start\":82509},{\"end\":82990,\"start\":82925},{\"end\":83345,\"start\":83283},{\"end\":83768,\"start\":83666},{\"end\":84315,\"start\":84272},{\"end\":85080,\"start\":85022},{\"end\":85371,\"start\":85314},{\"end\":85741,\"start\":85672},{\"end\":86146,\"start\":86077},{\"end\":86538,\"start\":86421},{\"end\":86908,\"start\":86820},{\"end\":87359,\"start\":87271},{\"end\":87880,\"start\":87837},{\"end\":88215,\"start\":88147},{\"end\":88589,\"start\":88547},{\"end\":89262,\"start\":89201},{\"end\":89644,\"start\":89570},{\"end\":90610,\"start\":90536},{\"end\":91267,\"start\":91188},{\"end\":91627,\"start\":91587},{\"end\":91989,\"start\":91914},{\"end\":92339,\"start\":92296},{\"end\":92675,\"start\":92623},{\"end\":93005,\"start\":92933},{\"end\":93350,\"start\":93301},{\"end\":93754,\"start\":93645},{\"end\":94220,\"start\":94149},{\"end\":94810,\"start\":94759},{\"end\":95164,\"start\":95096},{\"end\":95487,\"start\":95399},{\"end\":95773,\"start\":95736},{\"end\":96118,\"start\":96042},{\"end\":97232,\"start\":97208},{\"end\":98697,\"start\":97857},{\"end\":80599,\"start\":80507},{\"end\":81199,\"start\":81103},{\"end\":81543,\"start\":81475},{\"end\":81860,\"start\":81800},{\"end\":82186,\"start\":82110},{\"end\":82565,\"start\":82509},{\"end\":82990,\"start\":82925},{\"end\":83345,\"start\":83283},{\"end\":83768,\"start\":83666},{\"end\":84315,\"start\":84272},{\"end\":85080,\"start\":85022},{\"end\":85371,\"start\":85314},{\"end\":85741,\"start\":85672},{\"end\":86146,\"start\":86077},{\"end\":86538,\"start\":86421},{\"end\":86908,\"start\":86820},{\"end\":87359,\"start\":87271},{\"end\":87880,\"start\":87837},{\"end\":88215,\"start\":88147},{\"end\":88589,\"start\":88547},{\"end\":89262,\"start\":89201},{\"end\":89644,\"start\":89570},{\"end\":90610,\"start\":90536},{\"end\":91267,\"start\":91188},{\"end\":91627,\"start\":91587},{\"end\":91989,\"start\":91914},{\"end\":92339,\"start\":92296},{\"end\":92675,\"start\":92623},{\"end\":93005,\"start\":92933},{\"end\":93350,\"start\":93301},{\"end\":93754,\"start\":93645},{\"end\":94220,\"start\":94149},{\"end\":94810,\"start\":94759},{\"end\":95164,\"start\":95096},{\"end\":95487,\"start\":95399},{\"end\":95773,\"start\":95736},{\"end\":96118,\"start\":96042},{\"end\":97232,\"start\":97208},{\"end\":98697,\"start\":97857}]", "bib_author": "[{\"end\":80613,\"start\":80601},{\"end\":80624,\"start\":80613},{\"end\":80634,\"start\":80624},{\"end\":80647,\"start\":80634},{\"end\":81213,\"start\":81201},{\"end\":81225,\"start\":81213},{\"end\":81243,\"start\":81225},{\"end\":81255,\"start\":81243},{\"end\":81552,\"start\":81545},{\"end\":81559,\"start\":81552},{\"end\":81570,\"start\":81559},{\"end\":81875,\"start\":81862},{\"end\":81885,\"start\":81875},{\"end\":82199,\"start\":82188},{\"end\":82211,\"start\":82199},{\"end\":82224,\"start\":82211},{\"end\":82236,\"start\":82224},{\"end\":82247,\"start\":82236},{\"end\":82576,\"start\":82567},{\"end\":82585,\"start\":82576},{\"end\":82598,\"start\":82585},{\"end\":82611,\"start\":82598},{\"end\":82620,\"start\":82611},{\"end\":82632,\"start\":82620},{\"end\":82643,\"start\":82632},{\"end\":83001,\"start\":82992},{\"end\":83007,\"start\":83001},{\"end\":83020,\"start\":83007},{\"end\":83029,\"start\":83020},{\"end\":83037,\"start\":83029},{\"end\":83050,\"start\":83037},{\"end\":83358,\"start\":83347},{\"end\":83371,\"start\":83358},{\"end\":83379,\"start\":83371},{\"end\":83388,\"start\":83379},{\"end\":83403,\"start\":83388},{\"end\":83783,\"start\":83770},{\"end\":83796,\"start\":83783},{\"end\":83807,\"start\":83796},{\"end\":83822,\"start\":83807},{\"end\":83832,\"start\":83822},{\"end\":83843,\"start\":83832},{\"end\":83854,\"start\":83843},{\"end\":83868,\"start\":83854},{\"end\":83882,\"start\":83868},{\"end\":83893,\"start\":83882},{\"end\":84326,\"start\":84317},{\"end\":84336,\"start\":84326},{\"end\":84345,\"start\":84336},{\"end\":84357,\"start\":84345},{\"end\":84368,\"start\":84357},{\"end\":84709,\"start\":84700},{\"end\":84718,\"start\":84709},{\"end\":84731,\"start\":84718},{\"end\":84744,\"start\":84731},{\"end\":84753,\"start\":84744},{\"end\":84763,\"start\":84753},{\"end\":84774,\"start\":84763},{\"end\":85091,\"start\":85082},{\"end\":85098,\"start\":85091},{\"end\":85118,\"start\":85098},{\"end\":85128,\"start\":85118},{\"end\":85386,\"start\":85373},{\"end\":85399,\"start\":85386},{\"end\":85408,\"start\":85399},{\"end\":85416,\"start\":85408},{\"end\":85425,\"start\":85416},{\"end\":85758,\"start\":85743},{\"end\":85771,\"start\":85758},{\"end\":85780,\"start\":85771},{\"end\":85787,\"start\":85780},{\"end\":85793,\"start\":85787},{\"end\":85802,\"start\":85793},{\"end\":86158,\"start\":86148},{\"end\":86551,\"start\":86540},{\"end\":86561,\"start\":86551},{\"end\":86573,\"start\":86561},{\"end\":86918,\"start\":86910},{\"end\":86928,\"start\":86918},{\"end\":86947,\"start\":86928},{\"end\":86958,\"start\":86947},{\"end\":86971,\"start\":86958},{\"end\":86981,\"start\":86971},{\"end\":87370,\"start\":87361},{\"end\":87383,\"start\":87370},{\"end\":87396,\"start\":87383},{\"end\":87405,\"start\":87396},{\"end\":87412,\"start\":87405},{\"end\":87422,\"start\":87412},{\"end\":87430,\"start\":87422},{\"end\":87439,\"start\":87430},{\"end\":87891,\"start\":87882},{\"end\":87906,\"start\":87891},{\"end\":87914,\"start\":87906},{\"end\":87920,\"start\":87914},{\"end\":88225,\"start\":88217},{\"end\":88236,\"start\":88225},{\"end\":88246,\"start\":88236},{\"end\":88256,\"start\":88246},{\"end\":88274,\"start\":88256},{\"end\":88603,\"start\":88591},{\"end\":88609,\"start\":88603},{\"end\":88815,\"start\":88808},{\"end\":88826,\"start\":88815},{\"end\":88921,\"start\":88915},{\"end\":88932,\"start\":88921},{\"end\":89273,\"start\":89264},{\"end\":89284,\"start\":89273},{\"end\":89295,\"start\":89284},{\"end\":89306,\"start\":89295},{\"end\":89315,\"start\":89306},{\"end\":89656,\"start\":89646},{\"end\":89664,\"start\":89656},{\"end\":89674,\"start\":89664},{\"end\":89913,\"start\":89901},{\"end\":89926,\"start\":89913},{\"end\":89935,\"start\":89926},{\"end\":89946,\"start\":89935},{\"end\":89956,\"start\":89946},{\"end\":90301,\"start\":90288},{\"end\":90309,\"start\":90301},{\"end\":90316,\"start\":90309},{\"end\":90329,\"start\":90316},{\"end\":90342,\"start\":90329},{\"end\":90627,\"start\":90612},{\"end\":90641,\"start\":90627},{\"end\":90652,\"start\":90641},{\"end\":90970,\"start\":90960},{\"end\":90982,\"start\":90970},{\"end\":90998,\"start\":90982},{\"end\":91008,\"start\":90998},{\"end\":91279,\"start\":91269},{\"end\":91287,\"start\":91279},{\"end\":91296,\"start\":91287},{\"end\":91308,\"start\":91296},{\"end\":91317,\"start\":91308},{\"end\":91636,\"start\":91629},{\"end\":91645,\"start\":91636},{\"end\":91657,\"start\":91645},{\"end\":91666,\"start\":91657},{\"end\":91678,\"start\":91666},{\"end\":91998,\"start\":91991},{\"end\":92006,\"start\":91998},{\"end\":92015,\"start\":92006},{\"end\":92021,\"start\":92015},{\"end\":92033,\"start\":92021},{\"end\":92350,\"start\":92341},{\"end\":92358,\"start\":92350},{\"end\":92368,\"start\":92358},{\"end\":92379,\"start\":92368},{\"end\":92389,\"start\":92379},{\"end\":92686,\"start\":92677},{\"end\":92694,\"start\":92686},{\"end\":92705,\"start\":92694},{\"end\":92715,\"start\":92705},{\"end\":93016,\"start\":93007},{\"end\":93026,\"start\":93016},{\"end\":93037,\"start\":93026},{\"end\":93047,\"start\":93037},{\"end\":93361,\"start\":93352},{\"end\":93369,\"start\":93361},{\"end\":93380,\"start\":93369},{\"end\":93390,\"start\":93380},{\"end\":93774,\"start\":93756},{\"end\":93786,\"start\":93774},{\"end\":93798,\"start\":93786},{\"end\":93804,\"start\":93798},{\"end\":93813,\"start\":93804},{\"end\":93825,\"start\":93813},{\"end\":94231,\"start\":94222},{\"end\":94239,\"start\":94231},{\"end\":94250,\"start\":94239},{\"end\":94495,\"start\":94487},{\"end\":94503,\"start\":94495},{\"end\":94512,\"start\":94503},{\"end\":94520,\"start\":94512},{\"end\":94530,\"start\":94520},{\"end\":94820,\"start\":94812},{\"end\":94827,\"start\":94820},{\"end\":94841,\"start\":94827},{\"end\":94849,\"start\":94841},{\"end\":94857,\"start\":94849},{\"end\":95174,\"start\":95166},{\"end\":95181,\"start\":95174},{\"end\":95195,\"start\":95181},{\"end\":95203,\"start\":95195},{\"end\":95211,\"start\":95203},{\"end\":95497,\"start\":95489},{\"end\":95507,\"start\":95497},{\"end\":95783,\"start\":95775},{\"end\":95794,\"start\":95783},{\"end\":95806,\"start\":95794},{\"end\":95818,\"start\":95806},{\"end\":96126,\"start\":96120},{\"end\":96133,\"start\":96126},{\"end\":96142,\"start\":96133},{\"end\":96150,\"start\":96142},{\"end\":96162,\"start\":96150},{\"end\":96171,\"start\":96162},{\"end\":96184,\"start\":96171},{\"end\":96191,\"start\":96184},{\"end\":96203,\"start\":96191},{\"end\":96973,\"start\":96967},{\"end\":97178,\"start\":97167},{\"end\":80613,\"start\":80601},{\"end\":80624,\"start\":80613},{\"end\":80634,\"start\":80624},{\"end\":80647,\"start\":80634},{\"end\":81213,\"start\":81201},{\"end\":81225,\"start\":81213},{\"end\":81243,\"start\":81225},{\"end\":81255,\"start\":81243},{\"end\":81552,\"start\":81545},{\"end\":81559,\"start\":81552},{\"end\":81570,\"start\":81559},{\"end\":81875,\"start\":81862},{\"end\":81885,\"start\":81875},{\"end\":82199,\"start\":82188},{\"end\":82211,\"start\":82199},{\"end\":82224,\"start\":82211},{\"end\":82236,\"start\":82224},{\"end\":82247,\"start\":82236},{\"end\":82576,\"start\":82567},{\"end\":82585,\"start\":82576},{\"end\":82598,\"start\":82585},{\"end\":82611,\"start\":82598},{\"end\":82620,\"start\":82611},{\"end\":82632,\"start\":82620},{\"end\":82643,\"start\":82632},{\"end\":83001,\"start\":82992},{\"end\":83007,\"start\":83001},{\"end\":83020,\"start\":83007},{\"end\":83029,\"start\":83020},{\"end\":83037,\"start\":83029},{\"end\":83050,\"start\":83037},{\"end\":83358,\"start\":83347},{\"end\":83371,\"start\":83358},{\"end\":83379,\"start\":83371},{\"end\":83388,\"start\":83379},{\"end\":83403,\"start\":83388},{\"end\":83783,\"start\":83770},{\"end\":83796,\"start\":83783},{\"end\":83807,\"start\":83796},{\"end\":83822,\"start\":83807},{\"end\":83832,\"start\":83822},{\"end\":83843,\"start\":83832},{\"end\":83854,\"start\":83843},{\"end\":83868,\"start\":83854},{\"end\":83882,\"start\":83868},{\"end\":83893,\"start\":83882},{\"end\":84326,\"start\":84317},{\"end\":84336,\"start\":84326},{\"end\":84345,\"start\":84336},{\"end\":84357,\"start\":84345},{\"end\":84368,\"start\":84357},{\"end\":84709,\"start\":84700},{\"end\":84718,\"start\":84709},{\"end\":84731,\"start\":84718},{\"end\":84744,\"start\":84731},{\"end\":84753,\"start\":84744},{\"end\":84763,\"start\":84753},{\"end\":84774,\"start\":84763},{\"end\":85091,\"start\":85082},{\"end\":85098,\"start\":85091},{\"end\":85118,\"start\":85098},{\"end\":85128,\"start\":85118},{\"end\":85386,\"start\":85373},{\"end\":85399,\"start\":85386},{\"end\":85408,\"start\":85399},{\"end\":85416,\"start\":85408},{\"end\":85425,\"start\":85416},{\"end\":85758,\"start\":85743},{\"end\":85771,\"start\":85758},{\"end\":85780,\"start\":85771},{\"end\":85787,\"start\":85780},{\"end\":85793,\"start\":85787},{\"end\":85802,\"start\":85793},{\"end\":86158,\"start\":86148},{\"end\":86551,\"start\":86540},{\"end\":86561,\"start\":86551},{\"end\":86573,\"start\":86561},{\"end\":86918,\"start\":86910},{\"end\":86928,\"start\":86918},{\"end\":86947,\"start\":86928},{\"end\":86958,\"start\":86947},{\"end\":86971,\"start\":86958},{\"end\":86981,\"start\":86971},{\"end\":87370,\"start\":87361},{\"end\":87383,\"start\":87370},{\"end\":87396,\"start\":87383},{\"end\":87405,\"start\":87396},{\"end\":87412,\"start\":87405},{\"end\":87422,\"start\":87412},{\"end\":87430,\"start\":87422},{\"end\":87439,\"start\":87430},{\"end\":87891,\"start\":87882},{\"end\":87906,\"start\":87891},{\"end\":87914,\"start\":87906},{\"end\":87920,\"start\":87914},{\"end\":88225,\"start\":88217},{\"end\":88236,\"start\":88225},{\"end\":88246,\"start\":88236},{\"end\":88256,\"start\":88246},{\"end\":88274,\"start\":88256},{\"end\":88603,\"start\":88591},{\"end\":88609,\"start\":88603},{\"end\":88815,\"start\":88808},{\"end\":88826,\"start\":88815},{\"end\":88921,\"start\":88915},{\"end\":88932,\"start\":88921},{\"end\":89273,\"start\":89264},{\"end\":89284,\"start\":89273},{\"end\":89295,\"start\":89284},{\"end\":89306,\"start\":89295},{\"end\":89315,\"start\":89306},{\"end\":89656,\"start\":89646},{\"end\":89664,\"start\":89656},{\"end\":89674,\"start\":89664},{\"end\":89913,\"start\":89901},{\"end\":89926,\"start\":89913},{\"end\":89935,\"start\":89926},{\"end\":89946,\"start\":89935},{\"end\":89956,\"start\":89946},{\"end\":90301,\"start\":90288},{\"end\":90309,\"start\":90301},{\"end\":90316,\"start\":90309},{\"end\":90329,\"start\":90316},{\"end\":90342,\"start\":90329},{\"end\":90627,\"start\":90612},{\"end\":90641,\"start\":90627},{\"end\":90652,\"start\":90641},{\"end\":90970,\"start\":90960},{\"end\":90982,\"start\":90970},{\"end\":90998,\"start\":90982},{\"end\":91008,\"start\":90998},{\"end\":91279,\"start\":91269},{\"end\":91287,\"start\":91279},{\"end\":91296,\"start\":91287},{\"end\":91308,\"start\":91296},{\"end\":91317,\"start\":91308},{\"end\":91636,\"start\":91629},{\"end\":91645,\"start\":91636},{\"end\":91657,\"start\":91645},{\"end\":91666,\"start\":91657},{\"end\":91678,\"start\":91666},{\"end\":91998,\"start\":91991},{\"end\":92006,\"start\":91998},{\"end\":92015,\"start\":92006},{\"end\":92021,\"start\":92015},{\"end\":92033,\"start\":92021},{\"end\":92350,\"start\":92341},{\"end\":92358,\"start\":92350},{\"end\":92368,\"start\":92358},{\"end\":92379,\"start\":92368},{\"end\":92389,\"start\":92379},{\"end\":92686,\"start\":92677},{\"end\":92694,\"start\":92686},{\"end\":92705,\"start\":92694},{\"end\":92715,\"start\":92705},{\"end\":93016,\"start\":93007},{\"end\":93026,\"start\":93016},{\"end\":93037,\"start\":93026},{\"end\":93047,\"start\":93037},{\"end\":93361,\"start\":93352},{\"end\":93369,\"start\":93361},{\"end\":93380,\"start\":93369},{\"end\":93390,\"start\":93380},{\"end\":93774,\"start\":93756},{\"end\":93786,\"start\":93774},{\"end\":93798,\"start\":93786},{\"end\":93804,\"start\":93798},{\"end\":93813,\"start\":93804},{\"end\":93825,\"start\":93813},{\"end\":94231,\"start\":94222},{\"end\":94239,\"start\":94231},{\"end\":94250,\"start\":94239},{\"end\":94495,\"start\":94487},{\"end\":94503,\"start\":94495},{\"end\":94512,\"start\":94503},{\"end\":94520,\"start\":94512},{\"end\":94530,\"start\":94520},{\"end\":94820,\"start\":94812},{\"end\":94827,\"start\":94820},{\"end\":94841,\"start\":94827},{\"end\":94849,\"start\":94841},{\"end\":94857,\"start\":94849},{\"end\":95174,\"start\":95166},{\"end\":95181,\"start\":95174},{\"end\":95195,\"start\":95181},{\"end\":95203,\"start\":95195},{\"end\":95211,\"start\":95203},{\"end\":95497,\"start\":95489},{\"end\":95507,\"start\":95497},{\"end\":95783,\"start\":95775},{\"end\":95794,\"start\":95783},{\"end\":95806,\"start\":95794},{\"end\":95818,\"start\":95806},{\"end\":96126,\"start\":96120},{\"end\":96133,\"start\":96126},{\"end\":96142,\"start\":96133},{\"end\":96150,\"start\":96142},{\"end\":96162,\"start\":96150},{\"end\":96171,\"start\":96162},{\"end\":96184,\"start\":96171},{\"end\":96191,\"start\":96184},{\"end\":96203,\"start\":96191},{\"end\":96973,\"start\":96967},{\"end\":97178,\"start\":97167}]", "bib_venue": "[{\"end\":80750,\"start\":80647},{\"end\":81279,\"start\":81255},{\"end\":81605,\"start\":81586},{\"end\":81943,\"start\":81885},{\"end\":82296,\"start\":82247},{\"end\":82701,\"start\":82643},{\"end\":83086,\"start\":83050},{\"end\":83462,\"start\":83403},{\"end\":83952,\"start\":83893},{\"end\":84426,\"start\":84368},{\"end\":84698,\"start\":84612},{\"end\":85157,\"start\":85128},{\"end\":85483,\"start\":85425},{\"end\":85860,\"start\":85802},{\"end\":86235,\"start\":86158},{\"end\":86611,\"start\":86573},{\"end\":87028,\"start\":86981},{\"end\":87513,\"start\":87439},{\"end\":87981,\"start\":87920},{\"end\":88334,\"start\":88274},{\"end\":88668,\"start\":88609},{\"end\":89047,\"start\":88932},{\"end\":89374,\"start\":89315},{\"end\":89725,\"start\":89674},{\"end\":90038,\"start\":89972},{\"end\":90286,\"start\":90227},{\"end\":90711,\"start\":90652},{\"end\":90958,\"start\":90908},{\"end\":91376,\"start\":91317},{\"end\":91737,\"start\":91678},{\"end\":92092,\"start\":92033},{\"end\":92448,\"start\":92389},{\"end\":92767,\"start\":92715},{\"end\":93106,\"start\":93047},{\"end\":93444,\"start\":93390},{\"end\":93884,\"start\":93825},{\"end\":94309,\"start\":94250},{\"end\":94598,\"start\":94546},{\"end\":94916,\"start\":94857},{\"end\":95236,\"start\":95211},{\"end\":95558,\"start\":95507},{\"end\":95877,\"start\":95818},{\"end\":96335,\"start\":96203},{\"end\":96775,\"start\":96737},{\"end\":96922,\"start\":96907},{\"end\":96965,\"start\":96941},{\"end\":97238,\"start\":97234},{\"end\":97485,\"start\":97481},{\"end\":97649,\"start\":97645},{\"end\":97838,\"start\":97823},{\"end\":98765,\"start\":98699},{\"end\":80750,\"start\":80647},{\"end\":81279,\"start\":81255},{\"end\":81605,\"start\":81586},{\"end\":81943,\"start\":81885},{\"end\":82296,\"start\":82247},{\"end\":82701,\"start\":82643},{\"end\":83086,\"start\":83050},{\"end\":83462,\"start\":83403},{\"end\":83952,\"start\":83893},{\"end\":84426,\"start\":84368},{\"end\":84698,\"start\":84612},{\"end\":85157,\"start\":85128},{\"end\":85483,\"start\":85425},{\"end\":85860,\"start\":85802},{\"end\":86235,\"start\":86158},{\"end\":86611,\"start\":86573},{\"end\":87028,\"start\":86981},{\"end\":87513,\"start\":87439},{\"end\":87981,\"start\":87920},{\"end\":88334,\"start\":88274},{\"end\":88668,\"start\":88609},{\"end\":89047,\"start\":88932},{\"end\":89374,\"start\":89315},{\"end\":89725,\"start\":89674},{\"end\":90038,\"start\":89972},{\"end\":90286,\"start\":90227},{\"end\":90711,\"start\":90652},{\"end\":90958,\"start\":90908},{\"end\":91376,\"start\":91317},{\"end\":91737,\"start\":91678},{\"end\":92092,\"start\":92033},{\"end\":92448,\"start\":92389},{\"end\":92767,\"start\":92715},{\"end\":93106,\"start\":93047},{\"end\":93444,\"start\":93390},{\"end\":93884,\"start\":93825},{\"end\":94309,\"start\":94250},{\"end\":94598,\"start\":94546},{\"end\":94916,\"start\":94857},{\"end\":95236,\"start\":95211},{\"end\":95558,\"start\":95507},{\"end\":95877,\"start\":95818},{\"end\":96335,\"start\":96203},{\"end\":96775,\"start\":96737},{\"end\":96922,\"start\":96907},{\"end\":96965,\"start\":96941},{\"end\":97238,\"start\":97234},{\"end\":97485,\"start\":97481},{\"end\":97649,\"start\":97645},{\"end\":97838,\"start\":97823},{\"end\":98765,\"start\":98699},{\"end\":80840,\"start\":80752},{\"end\":87574,\"start\":87515},{\"end\":93485,\"start\":93446},{\"end\":80840,\"start\":80752},{\"end\":87574,\"start\":87515},{\"end\":93485,\"start\":93446}]"}}}, "year": 2023, "month": 12, "day": 17}