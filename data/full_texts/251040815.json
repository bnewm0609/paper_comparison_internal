{"id": 251040815, "updated": "2023-11-12 15:38:39.523", "metadata": {"title": "Salient Object Detection for Point Clouds", "authors": "[{\"first\":\"Songlin\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Ge\",\"last\":\"Li\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This paper researches the unexplored task-point cloud salient object detection (SOD). Differing from SOD for images, we find the attention shift of point clouds may provoke saliency conflict, i.e., an object paradoxically belongs to salient and non-salient categories. To eschew this issue, we present a novel view-dependent perspective of salient objects, reasonably reflecting the most eye-catching objects in point cloud scenarios. Following this formulation, we introduce PCSOD, the first dataset proposed for point cloud SOD consisting of 2,872 in-/out-door 3D views. The samples in our dataset are labeled with hierarchical annotations, e.g., super-/sub-class, bounding box, and segmentation map, which endows the brilliant generalizability and broad applicability of our dataset verifying various conjectures. To evidence the feasibility of our solution, we further contribute a baseline model and benchmark five representative models for a comprehensive comparison. The proposed model can effectively analyze irregular and unordered points for detecting salient objects. Thanks to incorporating the task-tailored designs, our method shows visible superiority over other baselines, producing more satisfactory results. Extensive experiments and discussions reveal the promising potential of this research field, paving the way for further study.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/FanGL22", "doi": "10.48550/arxiv.2207.11889"}}, "content": {"source": {"pdf_hash": "2f6ed376d334cadde6d50a7771d62a33f7d09648", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2207.11889v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "10c9aef2db0c2a418df5f4b6978ba9761c8db0ed", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2f6ed376d334cadde6d50a7771d62a33f7d09648.txt", "contents": "\nSalient Object Detection for Point Clouds\n\n\nSonglin Fan slfan@pku.edu.cn \nPeking University Shenzhen Graduate School\n\n\nPeng Cheng Laboratory\n\n\nWei Gao \nPeking University Shenzhen Graduate School\n\n\nPeng Cheng Laboratory\n\n\nGe Li geli@pku.edu.cn \nPeking University Shenzhen Graduate School\n\n\nSalient Object Detection for Point Clouds\nSalient object detectionpoint clouddatasetbaseline\nThis paper researches the unexplored task-point cloud salient object detection (SOD). Differing from SOD for images, we find the attention shift of point clouds may provoke saliency conflict, i.e., an object paradoxically belongs to salient and non-salient categories. To eschew this issue, we present a novel view-dependent perspective of salient objects, reasonably reflecting the most eye-catching objects in point cloud scenarios. Following this formulation, we introduce PCSOD, the first dataset proposed for point cloud SOD consisting of 2,872 in-/out-door 3D views. The samples in our dataset are labeled with hierarchical annotations, e.g., super-/sub-class, bounding box, and segmentation map, which endows the brilliant generalizability and broad applicability of our dataset verifying various conjectures. To evidence the feasibility of our solution, we further contribute a baseline model and benchmark five representative models for a comprehensive comparison. The proposed model can effectively analyze irregular and unordered points for detecting salient objects. Thanks to incorporating the task-tailored designs, our method shows visible superiority over other baselines, producing more satisfactory results. Extensive experiments and discussions reveal the promising potential of this research field, paving the way for further study.\n\nIntroduction\n\nSalient objects describe the most attractive objects with respect to their surroundings. Due to its myriad applications, salient object detection (SOD) can provide the pre-processing results for many vision tasks, such as 3D shape classification [46], compression [32], and quality assessment [30], to name a few. Distinct from the relevant task [6,43,61] for predicting eye fixation positions, namely saliency detection, SOD demands locating salient objects and completely segmenting them further, thus being more challenging. Most existing SOD works [9,12,15,26,37,58] devote their efforts to analyzing salient objects on regular images. With the fast revolution of 3D collection equipment, point clouds as the raw output of many devices (such as LiDAR and depth sensors) have a growing presence in research and applications. Compared with the adoption of alternative 3D formats, data processing directly on point clouds avoids information loss and computational redundancy in format conversion that may induce performance drops. Despite the flourishing advance of many point-based tasks, e.g., classification [3], object detection [42], and segmentation [16], point cloud SOD is still in its infancy, and many issues have not been discussed yet. As immersive visual media, point clouds offer a watching experience with six degrees of freedom (6DOF). Unlike the watching of static images, the attention allocation of humans varies when the view changes. The research community dubs the phenomenon that attention being allocated from one region to another as the attention shift [9,44]. However, we find that the attention shift of point clouds may trigger a new thorny problem that we name saliency conflict, i.e., an object paradoxically belongs to salient and non-salient categories for different views of one point cloud scene sample. Fig. 1 shows an example of an office scene recorded by point clouds. The attention allocated to the black computer varies as the view changes, which causes the black computer to go from being the salient object to the non-salient object. Then is the black computer the salient object of this office scene? The answer matters not only the definition of salient objects in point cloud scenarios but also the relevant dataset construction.\n\nIn this paper, we argue that the manifestations of salient objects in point clouds depend on the views, and point cloud SOD is to compute the salient objects of any given view in 3D space. The union of salient objects (segmentation maps) of \"given views\" indicates the complete description of salient objects for scenes in point clouds. For Fig. 1, different segmentation maps correspond to different views, and the union of segmentation maps represents the salient objects of this office scene. Firstly, this formulation makes it easier to grasp the nature of the SOD problem due to the fact that humans actually observe only one view at a time while the viewpoint is free. Broadly speaking, the image is a special case of only a single view. Secondly, this formulation avoids the complex modeling to handle the whole 3D scene with saliency conflict phenomenon, which can benefit the design of simpler models capable of analyzing different views. Thirdly, this formulation eases the dataset construction with the human-annotated most attractive objects via subjective experiments, since the subjective experiment results of different views sometimes cannot be reflected into a large-scale point cloud sample (such as the office scene sample in Fig. 1) simultaneously without our view-dependent saliency analysis.\n\nFollowing our formulation of point cloud SOD, we introduce PCSOD-the first versatile dataset for point cloud SOD with densely annotated labels. Our dataset contains 2,872 frequent 3D views that belong to over one hundred in-/out-door scenes. The manual data collection phase lasts over one year, and the samples reflect a wide range of scenarios in our lives. Detailed statistics show that our dataset has 138 object categories and 53.4% difficult samples, which ensures its brilliant generalizability. To extend the applicability of this new dataset, we provide hierarchical annotations for each sample, including super-/sub-class, bounding box, and segmentation map. The proposed dataset as a comprehensive platform can conveniently support research on multi-task learning [48] and other valuable vision tasks, not limited to point cloud SOD.\n\nSince point clouds record 3D information in the format of irregular and unordered points, existing SOD models [9,12,15,26,58] for images cannot be transferred for point cloud processing. Additionally, though several representative point-based models [3,16,25,38,59] have been developed for other segmentation tasks, they are incapable of performing well in SOD. These models for other segmentation tasks fail to consider the particularities of SOD, i.e., the benefits of multi-scale features [35] and the refinement of global semantics [4,27]. To prove the feasibility of our solution, we further develop a baseline model and benchmark five representative segmentation models for comparison and analysis of point cloud SOD. Owing to incorporating the task-tailored designs, the proposed baseline model can take full advantage of the multi-scale features and global semantics to locate salient objects and accurately separate them. Extensive experiments verify the effectiveness of our solution for point cloud SOD.\n\nIn summary, we conclude the contributions as follows:\n\n1) We propose a novel view-dependent perspective of point cloud SOD. Our formulation avoids the saliency conflict, emphasizes the nature of SOD, and reasonably reflects the most eye-catching objects in point cloud scenarios. 2) We construct the first versatile dataset for point cloud SOD, termed PCSOD.\n\nOur dataset has brilliant generalizability and broad applicability, expected to be a catalyst for point cloud SOD and many other vision tasks. \n\n\nRelated Work\n\nSalient Object Detection. Following the pioneer attempt [17], many early works [24,36,47,53] design hand-crafted features to exploit low-level cues. These methods cannot obtain satisfactory accuracy because of the lack of semantic cues. Thanks to the powerful capability of neural networks in abstracting semantics, the bottleneck of traditional methods is broken. Hou et al. [15] introduce short connections into a skip-layer structure. The advanced representations at multiple layers thus can be fully utilized. Siris et al. [45] propose a semantic scene context-aware framework to capture sufficient high-level semantics for locating salient objects. To rich the semantic information diluted during the top-down transmission, some recent works [4,27,35] explicitly extract global semantics and append them into low-level features, achieving visible performance improvement. Despite the gratifying achievements of existing RGB image-based methods [28,40,55,60], they still have difficulty understanding complex scenes for lacking spatial geometry information. Consequently, researchers begin extending the task of SOD on 3D images, such as RGB-D images [10,15,20,22,26,56,58] and light field images [23,29,50,57], which show significant potential. A detailed description of these image-based methods is beyond the scope of this article. Please refer to the relevant surveys [11,51,62] for more introduction. We can conclude that all these efforts are confined to the image domain. This work will disentangle the limitation and probe SOD on point clouds.\n\nRegarding the attention modeling on point clouds, we also learn that a few methods [6,13,18,43,46,61] are developed to automatically compute the human attention distribution. The algorithms of these methods merely produce a heatmap of the attention distribution, while the SOD task we study demands completely segmenting the salient objects, thus being more challenging.\n\nDeep Learning on Point Clouds. Processing point clouds has long been a significant challenge. Previous works [19,21] tend to first rearrange raw points via octree or kdtree. The emergence of PointNet/PointNet++ [3,38] shows us a new approach for raw point processing. They employ shared multilayer perceptrons (MLPs) to extract point-wise features and achieve state-of-the-art performance across many vision tasks. Following PointNet, three directions are mainly adopted to improve the performance further, i.e., powerful convolution [25,54], effective neighborhood connection [52,59], and advanced reduction [16,39]. Li et al. [25] propose to learn an X-transformation from raw points by imitating the typical convolution, while Wu et al. [54] regard the typical convolution as the combination of weight and density functions. ShellNet [59] arranges neighbors into concentric spherical shells that have a convolution order from the inner to the outer shells. Wang et al. [52] propose a simple operation known as EdgeConv, which extracts local geometric features while retaining permutation invariance. To explore more advanced reduction operations, Hu et al. [16] and Qian et al. [39] resort to attentive pooling and anisotropic reduction, respectively. However, these methods are not initially developed for SOD, ignoring the particularities of SOD. \n\n\nDataset Construction\n\nData Collection. Point clouds in existing datasets [1,2,5,14,34] are often collected for specific scenes (such as outdoor road or indoor office scenes). In contrast, a high-quality SOD dataset [49] demands rich scenes, which motivates us to collect diverse data by ourselves. The data collection phase takes over one year, and we collect 2,872 3D views from over one hundred preset scenes across dozens of cities. Each 3D view has 240,000 points. This process can also simulate the 3D view acquisition when \"travelling\" in an off-shelf large-scale point cloud sample (such as an office or even a city). As shown in Fig. 2, the 3D views of a scene constitute a series of watching descriptions of this scene whose salient objects can be obtained from subjective experiments without saliency conflict.\n\nData Annotation. Referring to the determination of salient objects in images [49,50], we employ thirty professional annotators to label the salient objects from given views. Before the labeling, every annotator is pre-trained over fifteen samples. To ensure the annotation accuracy, we divide these thirty annotators into ten groups. Three annotators in one group jointly determine the salient objects, then cross-validated by other groups. An object is regarded as a positive label only if more than eighty percent of annotators verify it. The recently released datasets [9,34] indicate that offering hierarchical annotations benefits the applicability of a new dataset. As shown in Fig. 2  Data Split. Having a standard dataset split [9,50] is conducive to fairly studying and comparing the pros and cons of algorithms. Following the ratio of 7:3 adopted by many datasets [50], our PCSOD is randomly split into 2,000 samples for training and 872 samples for testing.\n\n\nDataset Statistics\n\nDiverse Object Categories. A diverse SOD dataset should have broad coverage of scenes in the real world to ensure brilliant generalizability. Our PCSOD covers a wide range of scenarios in our lives. As shown in Fig. 3(a) and Fig. 3(c), the salient object categories have a heterogeneous variety. Specifically, objects in our dataset can be categorized into 12 super-classes, e.g., human, animal, plant, etc. These 12 super-classes are further comprised of 138 sub-classes, fully covering the daily situations. The diverse salient object categories enable a comprehensive understanding of the attention allocation of humans in real-world scenes.\n\nRich Annotations. A versatile dataset should not only support the study of existing issues but also adapt to new research directions. As shown in Fig. 2, our PCSOD offers hierarchical annotations, e.g., super-/sub-class, bounding box, and segmentation map. These annotations help researchers understand each sample of our dataset from different aspects (such as object property, object proposal, and scene parsing), sparking novel ideas. Besides, our annotations are very precise. The segmentation maps accurately reflect the structures of objects in 3D scenes, even though some are very complex (see the complex structure case in Fig. 3(b)).\n\nDifficult Samples. A valuable dataset should contain a certain amount of difficult samples and dive into the problems. The difficult samples benefit the performance of models confronting various complex scenes. With this consideration, we add many challenging samples to our dataset, including multiple objects, small objects, complex structures, low illumination, etc. Some visual examples are shown in Fig. 3(b). Fig. 3(d) further details the proportion of samples with each attribute. Statistics indicate that our dataset has 53.4% difficult samples, which evidences that the proposed PCSOD is very challenging.\n\n\nProposed Method\n\nExtending the concept of salient objects in images to point clouds, we formulate that the salient objects of views from a scene indicate the complete description of salient objects in this scene. Point cloud SOD aims to identify the salient objects of any given view. While various methods have been developed for imagebased SOD, they cannot handle irregular and unordered point clouds. Moreover, existing point-based segmentation models for other tasks cannot guarantee the performance of identifying salient objects. These circumstances motivate us to design a baseline model and excavate potential directions for point cloud SOD.\n\n\nOverall Architecture\n\nAs shown in Fig. 4, the proposed baseline model inherits a typical encoderdecoder architecture. The encoder extracts multi-level features from raw points, while the decoder enhances and fuses the extracted features to predict salient objects. To illustrate the effectiveness of our designs, we introduce the classical PointNet++ [38] as the encoder. It has been studied [27] that high-level features will be gradually diluted when transmitted to low-level ones. To address this issue, some recent image-based methods [4,27,35] explicitly extract global semantics and append them into low-level features, observing gratifying performance improvement. Inspired by the philosophical designs of these methods, we design two key modules, i.e., Point Perception Block (PPB) and Saliency Perception Block (SPB), to take full advantage of the benefits of multi-scale features and the refinement of global semantics for locating salient objects. is fixed to 64). Then we aggregate multi-level features {F l } 4 l=1 into the compact representations F c via the Feature Aggregation Block (FAB). As shown in Fig. 5, the operations in FAB are very straightforward, i.e., upsampled high-level features are sequentially fused with low-level features. We adopt the common trilinear interpolation as the upsampling operation to match the spatial size of different level features, while the fusion operation we employ is concatenation along the feature dimension followed by MLPs. Following previous works [16,38], the feature concatenation can simultaneously retain the originality of the fused two level features and is proved to be very effective for point cloud feature fusion. Note that the feature fusion in all modules is uniformly through concatenation unless otherwise stated. To prevent the dilution of high-level features, the PPB is proposed to abstract global semantics and strengthen the multi-scale representations. We obtain global semantics F s and multi-scale features F m from the highest-level features F 4 and the compact representations F c , respectively, using two PPBs with different configurations. The global semantics can supplement the diluted high-level features in multi-scale features and alleviate the distraction of non-salient background. To achieve this, we further develop the SPB to integrate multi-scale features F m and global semantics F s , and produce the final prediction P. Next, we will elaborate on the details of our PPB and SPB.\n\n\nProposed Modules\n\nPoint Perception Block. The global semantics and multi-scale features are important for SOD [4,27,35]. The former helps to locate the positions of salient objects, while the latter is conducive to recognizing salient objects of different sizes. Besides, the acquisition of them demands enlarging the receptive fields of features and capturing the context information. Inspired by the widely used Receptive Field Block [31], we introduce the PPB to achieve this goal.\n\nAs shown in Fig. 5 To learn local geometric representations, the embedding sub-unit embeds the relative spatial position between x p i and its neighbor x p i,j as\ne j i = M LP s([x p i , x p i,j , x p i \u2212 x p i,j , D(x p i , x p i,j )]),(1)\nwhere D(\u00b7) and [] denote the Euclidean distance between two points and the concatenation operation, respectively. Because e j i merely contains the geometric features and lacks associated point-wise features, we concatenate e j i with corresponding point-wise features x f i to obtain the advanced representations a j i . All advanced representations A i = {a 1 i , a 2 i , .., a k i } of k neighbors express each of their semantic contributions to the center point x p i . The reduction sub-unit aggregates the neighborhood semantic contributions by a Mean-max reduction operation\nx f i = M LP s([max(A i ), mean(A i )]),(2)\nwhere max(\u00b7) and mean(\u00b7) denote the max function and mean function, respectively. Compared with the input features X f , the branch outputsX\nf = {x f 1 ,x f 2 , ..\n.,x f M } have enlarged receptive fields and capture the context information in local regions. Finally, we fuse the output features {X f b } 4 b=1 of the first four branches and further introduce a skip connection of the fifth branch to retain the original feature\u015d\nY f = M LP s([X f 1 ,X f 2 ,X f 3 ,X f 4 ]) + M LP s(X f ).(3)\nSimilar to the Receptive Field Block, by setting K = {k 1 , k 2 , k 3 , k 4 } for the first four branches reasonably, the global semantics and multi-scale features can be obtained, respectively. Besides, the input points X and corresponding outputs Y of our PPB share the same feature size. Therefore, our PPB can be easily embedded in various networks to improve their performance.\n\nSaliency Perception Block. The utilization of our PPB allows the acquisition of global semantics and multi-scale features. Subsequently, how to seamlessly merge the two kinds of features and obtain the final prediction is still open.\n\nAs shown in Fig. 5, our SPB enhances the multi-scale features using the global semantics. The global semantics can effectively alleviate the distraction of non-salient background in multi-scale features and emphasize the salient regions (see Fig. 8). The enhanced multi-scale features are then used to predict the salient objects. Specifically, the SPB first upsamples the global semantics F s and multi-scale features F m to the spatial size of the input V. The upsampling operation is followed by MLPs to reduce the aliasing effect. Then we use the upsampled global semantics to enhance the multi-scale features\nF e = M LP s([M LP s(U(F s )), S(M LP s(U(F m )))]),(4)\nwhere U and S denote the upsampling and softmax operations, respectively. F e is the enhanced multi-scale features. In this approach, the enhanced multi-scale features include both the accurate positions and fine-grained structures of salient objects. Finally, we use a prediction layer (MLPs) to predict salient objects P from the enhanced multi-scale features F e .\n\n\nExperiments\n\n\nExperimental Setup\n\nImplementation Details. We use the popular Pytorch framework to implement our method on an NVIDIA Tesla V100 GPU. The points in the inputs are represented by nine-dimensional vectors (d in = 9) consisting of spatial coordinates, RGB colors, and normalized spatial coordinates. Due to the limitations of memory capacity, we randomly sample N = 4, 096 points with replacement from inputs in the training stage, while the sampling operations in the testing are without replacement for testing all 240,000 points in a 3D view. We use random rotation to augment data. The parameters K of the PPB for abstracting global semantics are {1, 4, 9, 16} while those of another PPB are {1, 9, 25, 49}. Our loss function is defined on the standard cross-entropy loss. We train the proposed baseline model by Adam optimizer with an initial learning rate of 5e-4 and a weight decay of 1e-4. The total training epochs are 3,000, with a batch size of 32. A three-time voting strategy [38] is adopted to produce the predictions in the testing phase.\n\nEvaluation Metrics. To compare the results of different methods, we adopt four popular evaluation metrics for performance benchmarking, i.e., mean absolute error (MAE), F-measure [33], E-measure [8], and intersection over union (IoU). MAE estimates the point-wise approximation degree between predicted segmentation maps and corresponding ground truths. It can be formulated as\nMAE = 1 N N i=1 |p i \u2212 g i |,\nwhere p i \u2208 P and g i \u2208 G are the prediction and ground truth, respectively. F-measure is the harmonic mean value of the precision   (prec) and recall (reca), i.e., F-measure = (1\u2212\u03b2 2 )prec\u00b7reca \u03b2 2 prec+reca , where \u03b2 2 is set to 0.3 for emphasizing the importance of precision. E-measure captures both the local matching and region-level matching information of segmentation maps for assessment. IoU is a metric describing the extent of overlap between two segmentation maps. It is defined as IoU = inter union , where inter and union indicate the intersection and union of two segmentation maps, respectively. Note that the relevant concepts of S-measure [7] in 3D space may change, thus being ignored.\n\n\nComparison and Analysis\n\nTo the best of our knowledge, there is no deep learning-based method designed for point cloud SOD. Consequently, we introduce five representative baseline models [3,16,25,38,59] from others segmentation tasks for comparison and analysis. PointNet [3] and its improved version, namely PointNet++ [38], are the two most representative models in point cloud processing. PointCNN [25], ShellNet [59], and RandLA [16] indicate three promising directions of point cloud processing, i.e., powerful convolution, effective neighborhood connection, and advanced reduction. For a fair comparison, we retrain these models on our PCSOD dataset according to the recommended parameter settings and produce the final results by the same voting strategy as our method.  Fig. 7: Qualitative comparison of six baseline models on views of two common scenes, i.e., a supermarket (Scene 1) and a park (Scene 2). Note that \"GT\", \"PNet\", \"PNet2\", \"PCNN\", \"SNet\", and \"RLA\" mean the ground truth, PointNet [3], PointNet++ [38], PointCNN [25], ShellNet [59], and RandLA [16], respectively.\n\nQuantitative Comparison. In Tab  As shown in Fig. 6, the results of our method are much flatter at most thresholds, which demonstrates that our method has excellent generalizability.\n\nQualitative Comparison. To further reveal the feasibility of our solution predicting salient objects of any given 3D views, we illustrate the results of several frequent views from two common scenes in Fig. 7. Scene 1 is a supermarket (indoor scene), while Scene 2 is a park (outdoor scene), both of which are unseen by these models. It can be seen that most baseline models can locate the salient objects of given views, except for PointCNN. Though some views are very challenging, e.g., cluttered background (column 2), transparent object (column 3), complex structure (column 4), and random view with non-central object (column 5 and 6), our method can consistently produce accurate and complete segmentation maps with high contrast, which evidences the superiority of our method.\n\n\nAblation Study\n\nTo analyze the fundamentals of our baseline model, we conduct extensive ablation experiments in Tab. 2. The ablation experiments are based on the encoder PointNet++ [38], studying the effectiveness of the designs in our decoder, i.e., key modules and feature reduction operations. In each experiment, only one influential factor is changed as the others keep the same for a fair comparison.\n\nTo investigate the contributions from our SPB and PPB separately, we first load the SPB into the encoder. By comparing No.1 and No.2 in Tab. 2, we can learn that the introduction of our SPB can help promote the performance of our model in locating salient objects. However, because the high-level features from the encoder have limited receptive fields, directly utilizing them as the semantics can only achieve suboptimal performance. As demonstrated in Tab.  Tab. 2 study various reduction manners. It can be seen that our Mean-max reduction can outperform the individual Mean reduction or Max reduction. Furthermore, compared to the attentive reduction [16], our method has a better performance without increasing the number of network parameters.\n\n\nConclusion\n\nIn this paper, we present the first comprehensive study on point cloud SOD, involving its formulation, dataset construction, and baseline design. To avoid the saliency conflict, we propose a novel view-dependent perspective of salient objects. Our formulation can reasonably reflect the salient objects in point cloud scenarios. Then we elaborately construct a high-quality dataset, namely PCSOD, and contribute a baseline model for point cloud SOD. Our dataset has excellent generalizability and broad applicability, expected to boost the advance of SOD and many other vision tasks. We conduct extensive experiments on our dataset to verify the feasibility of our solution. Experimental results show that our baseline model has significant superiority and produces visually favorable predictions. Our work reveals the potential of point cloud SOD and pave the way for further study.\n\nFig. 1 :\n1Illustration of saliency conflict. The variation of attention allocated to the black computer causes a contradiction that the black computer simultaneously belongs to the salient and non-salient objects for this scene. We, therefore, propose to analyze the salient objects of point clouds according to the views.\n\n3 )\n3We develop a baseline model for point cloud SOD. Our baseline model has a full consideration of the particularities of SOD, outperforming other baseline models by a clear margin. 4) We establish the first benchmark of point cloud SOD, conduct a thorough analysis, and bring a new perspective toward point cloud SOD.\n\nFig. 2 :\n2Examples from our PCSOD dataset with hierarchical annotations.3 Proposed DatasetDatasets[9,23,41] have become the driving force behind many vision tasks, especially with the emergence of deep learning. With this in mind, we introduce PCSOD for: (1) probing a new challenging task, (2) facilitating research on new issues, and (3) verifying new conjectures. Next, we will elaborate more details about our dataset. Besides, some visual examples are shown inFig. 2 and Fig. 3.\n\nFig. 3 :\n3Statistics of our PCSOD dataset. (a) Categories of salient objects. (b) Illustration of challenging samples. (c) Word cloud of salient objects. (d) Histogram distribution of challenging samples. and segmentation map. Each level of annotations is obtained through corresponding professionals. Furthermore, at least two passes of verification are performed for each annotation to ensure its quality.\n\nFig. 4 :\n4Formally, let V = {v 1 , v 2 , ..., v N } represent a view of N points with associated point-wise features (e.g., RGB colors), where v \u2208 R din . To obtain the probabilities P = {p 1 , p 2 , ..., p N } of corresponding points being salient, the encoder first extracts multi-level features {F l } 4 l=1 from raw points V. The l th level features F l = {f l 1 , f l 2 , ..., f l N l } have N l = N 4 l aggregated points with doubling the feature dimension compared with F l\u22121 (except the feature dimension of the first level Overall architecture of the proposed baseline model, which has a typical encoder-decoder architecture.\n\nFig. 6 :\n6F-measure and E-measure under different thresholds.\n\n2 (Fig. 8 :\n28No.3), a properly configured PPB helping acquire semantics with global receptive fields can unlock the potential of the SPB. Besides, the PPB with a different configuration can also strengthen the multi-scale representations of features, 3D heatmap visualization of feature maps. Feature 1, Feature 2, and Feature 3 represent the multi-scale features, global semantics, and enhanced multi-scale features, respectively. which benefits the perception of objects of different sizes. Therefore, another PPB in the ablation No.4 can bring orthogonal contributions to SOD. Fig. 8 further shows how the feature maps change. Due to the dilution of high-level features, multi-scale features incorrectly focus on the non-salient background, whereas the global semantics have an accurate perception of salient objects. The SPB can correct the deviation of multi-scale features by combining global semantics and obtain the enhanced multi-scale features. The ablations No.4-No.8 in\n\n\n, we hierarchically label the determined salient objects and provide three levels of annotations, i.e., class, bounding box,(a) \n\n(c) \n(d) \n\n0.05 \n0.10 \n0.15 \n0.20 \n\n(b) \n\nComplex Structure \nSimple Case \nMultiple Objects \nSmall Object \n\nSimple Structure \n\nHigh Contrast \n\nSimple Background \n\nComplex Structure \n\nComplex Background \n\nOcculusion \n\nMutiple Objects \n\nSmall Object \n\nTransparent Object \n\nLow Contrast \n\nUnderwater Object \n\nEdge Object \n\nLow Illumination \n\nSimple \nCase \n\nComplex Case \n\n\n\n\n, the PPB consists of five branches to capture the context information of point-wise features. The first four branches with similar structures[ ] \n\nMLPs \n\nFeature Aggregation Block \n\n+ \n\nSaliency Perception Block \n\n[ ] \n\nMLPs \n\nGrouping Embedding Reduction \n\nGrouping Embedding Reduction \n\nGrouping Embedding Reduction \n\nMLPs \n\n[ ] \n\nGrouping Embedding Reduction \n\n+ \n\nMultilayer Perceptrons \n\nPoint Perception Block \n\nConcatenation with \nLinear Transform \n\nAddition \nUpsampling \n\n[ ] \n\n[ ] \n\n[ ] \n\nMLPs \n\n\u5728\u6b64\u5904\u952e\u5165\u516c\u5f0f \n\nMLPs \n\nSoftmax \n\nFig. 5: Details of the components in the proposed baseline model, i.e., Point \nPerception Block, Feature Aggregation Block, and Saliency Perception Block. \n\nencode center points by their local regions of different sizes. Each branch has \nthree sub-units, i.e., grouping, embedding, and reduction. To be more specific, \nlet X p = {x P \n1 , x P \n2 , ..., x p \nM } denote the spatial coordinates of input points X with \nintermediate learned features X f = {x f \n1 , x f \n2 , ..., x f \nM }. M indicates the number \nof points. For each point x p \ni \u2208 X p , the grouping sub-unit gathers its k nearest \nneighbors N (x p \ni ) = {x p \ni,1 , x p \ni,2 , ..., x p \ni,k } by K-nearest neighbours (KNN). The \nspatial size of the local region N (x p \ni ) centered on x p \ni varies as k takes different \nvalues. \n\nTable 1 :\n1Benchmarking results of six representative baseline models on our \nPCSOD dataset. \" \u2191 \"/\" \u2193 \" suggests that larger/smaller is better. Note that the \nbest results are shown in boldface \n\nE-measure \nF-measure \n\nThreshold \nThreshold \n\n\n\n\n. 1, we list the results of six baseline models on four evaluation metrics. We can learn that the proposed method achieves state-of-the-art performance and outperforms all competitors by a clear margin. Specifically, our model surpasses the second-best model ShellNet by 6.8%, 2.1%, 0.4%, and 1.2% on MAE, F-measure, E-measure, and IoU. RandLA is a recently proposed model with significant superiority over PointNet++ for semantic segmentation. However, experiments in Tab. 1 show that RandLA has no advantage on SOD and even performs worse than PointNet++, indicating designing tailored models for point cloud SOD is non-trivial. Though our baseline model has the best performance, there is still considerable room for performance improvement, which demands further efforts from the research community. To study the generalizability of these baseline models under different thresholds, we plot the F-measure scores and E-measure scores by taking different thresholds.No. \nMethods \nMAE \u2193 F-measure \u2191 E-measure \u2191 IoU \u2191 \n1 \nPointNet++ [38] \n0.077 \n0.738 \n0.816 \n0.608 \n2 \n+SPB \n0.076 \n0.748 \n0.828 \n0.624 \n3 \n+SPB, +PPB 1 \n0.073 \n0.754 \n0.840 \n0.639 \n4 \n+SPB, +PPB 1, +PPB 2 0.069 \n0.769 \n0.851 \n0.656 \n5 \nMean Reduction \n0.071 \n0.764 \n0.843 \n0.649 \n6 \nMax Reduction \n0.070 \n0.765 \n0.843 \n0.651 \n7 \nAttentive Reduction [16] \n0.074 \n0.758 \n0.847 \n0.658 \n8 \nMean-max Reduction \n0.069 \n0.769 \n0.851 \n0.656 \n\n\n\nTable 2 :\n2Ablation analysis of the proposed point cloud SOD model. No.1-No.4 study the effectiveness of our SPB and PPB, respectively. \"PPB 1\" and \"PPB 2\" denote the PPBs for producing global semantics and multi-scale features, respectively. No.5-No.8 investigate the alternative reduction operations.\n\n3d semantic parsing of large-scale indoor spaces. I Armeni, O Sener, A R Zamir, H Jiang, I Brilakis, M Fischer, S Savarese, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Armeni, I., Sener, O., Zamir, A.R., Jiang, H., Brilakis, I., Fischer, M., Savarese, S.: 3d semantic parsing of large-scale indoor spaces. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1534-1543 (2016)\n\nSemantickitti: A dataset for semantic scene understanding of lidar sequences. J Behley, M Garbade, A Milioto, J Quenzel, S Behnke, C Stachniss, J Gall, IEEE/CVF International Conference on Computer Vision. Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., Gall, J.: Semantickitti: A dataset for semantic scene understanding of lidar sequences. In: IEEE/CVF International Conference on Computer Vision. pp. 9297-9307 (2019)\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. R Q Charles, H Su, M Kaichun, L J Guibas, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Charles, R.Q., Su, H., Kaichun, M., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d classification and segmentation. In: IEEE/CVF Con- ference on Computer Vision and Pattern Recognition. pp. 77-85 (2017).\n\n. 10.1109/CVPR.2017.16https://doi.org/10.1109/CVPR.2017.16\n\nGlobal context-aware progressive aggregation network for salient object detection. Z Chen, Q Xu, R Cong, Q Huang, AAAI Conference on Artificial Intelligence. 34Chen, Z., Xu, Q., Cong, R., Huang, Q.: Global context-aware progressive aggregation network for salient object detection. In: AAAI Conference on Artificial Intelligence. vol. 34, pp. 10599-10606 (2020)\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nie\u00dfner, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nie\u00dfner, M.: Scannet: Richly-annotated 3d reconstructions of indoor scenes. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5828-5839 (2017)\n\nPoint cloud saliency detection by local and global feature fusion. X Ding, W Lin, Z Chen, X Zhang, 10.1109/TIP.2019.2918735IEEE Transactions on Image Processing. 2811Ding, X., Lin, W., Chen, Z., Zhang, X.: Point cloud saliency detection by local and global feature fusion. IEEE Transactions on Image Processing 28(11), 5379-5393 (2019). https://doi.org/10.1109/TIP.2019.2918735\n\nStructure-measure: A new way to evaluate foreground maps. D P Fan, M M Cheng, Y Liu, T Li, A Borji, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Fan, D.P., Cheng, M.M., Liu, Y., Li, T., Borji, A.: Structure-measure: A new way to evaluate foreground maps. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4548-4557 (2017)\n\nEnhanced-alignment measure for binary foreground map evaluation. D P Fan, C Gong, Y Cao, B Ren, M M Cheng, A Borji, arXiv:1805.10421arXiv preprintFan, D.P., Gong, C., Cao, Y., Ren, B., Cheng, M.M., Borji, A.: Enhanced-alignment measure for binary foreground map evaluation. arXiv preprint arXiv:1805.10421 (2018)\n\nShifting more attention to video salient object detection. D P Fan, W Wang, M M Cheng, J Shen, 10.1109/CVPR.2019.00875IEEE/CVF Conference on Computer Vision and Pattern Recognition. Fan, D.P., Wang, W., Cheng, M.M., Shen, J.: Shifting more attention to video salient object detection. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8546-8556 (2019). https://doi.org/10.1109/CVPR.2019.00875\n\nBbs-net: Rgb-d salient object detection with a bifurcated backbone strategy network. D P Fan, Y Zhai, A Borji, J Yang, L Shao, European Conference on Computer Vision. SpringerFan, D.P., Zhai, Y., Borji, A., Yang, J., Shao, L.: Bbs-net: Rgb-d salient object detection with a bifurcated backbone strategy network. In: European Conference on Computer Vision. pp. 275-292. Springer (2020)\n\nK Fu, Y Jiang, G P Ji, T Zhou, Q Zhao, D P Fan, arXiv:2010.04968Light field salient object detection: A review and benchmark. arXiv preprintFu, K., Jiang, Y., Ji, G.P., Zhou, T., Zhao, Q., Fan, D.P.: Light field salient object detection: A review and benchmark. arXiv preprint arXiv:2010.04968 (2020)\n\nUnified information fusion network for multi-modal rgb-d and rgb-t salient object detection. W Gao, G Liao, S Ma, G Li, Y Liang, W Lin, Gao, W., Liao, G., Ma, S., Li, G., Liang, Y., Lin, W.: Unified information fu- sion network for multi-modal rgb-d and rgb-t salient object detection. IEEE Transactions on Circuits and Systems for Video Technology pp. 1-1 (2021).\n\n. 10.1109/TCSVT.2021.3082939https://doi.org/10.1109/TCSVT.2021.3082939\n\nPoint-wise saliency detection on 3d point clouds via covariance descriptors. Y Guo, F Wang, J Xin, The Visual Computer. 3410Guo, Y., Wang, F., Xin, J.: Point-wise saliency detection on 3d point clouds via covariance descriptors. The Visual Computer 34(10), 1325-1338 (2018)\n\nT Hackel, N Savinov, L Ladicky, J D Wegner, K Schindler, M Pollefeys, arXiv:1704.03847Semantic3d. net: A new large-scale point cloud classification benchmark. arXiv preprintHackel, T., Savinov, N., Ladicky, L., Wegner, J.D., Schindler, K., Pollefeys, M.: Semantic3d. net: A new large-scale point cloud classification benchmark. arXiv preprint arXiv:1704.03847 (2017)\n\nDeeply supervised salient object detection with short connections. Q Hou, M M Cheng, X Hu, A Borji, Z Tu, P H S Torr, IEEE Transactions on Pattern Analysis and Machine Intelligence. 414Hou, Q., Cheng, M.M., Hu, X., Borji, A., Tu, Z., Torr, P.H.S.: Deeply supervised salient object detection with short connections. IEEE Transac- tions on Pattern Analysis and Machine Intelligence 41(4), 815-828 (2019).\n\n. 10.1109/TPAMI.2018.2815688https://doi.org/10.1109/TPAMI.2018.2815688\n\nLearning semantic segmentation of large-scale point clouds with random sampling. Q Hu, B Yang, L Xie, S Rosa, Y Guo, Z Wang, N Trigoni, A Markham, Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni, N., Markham, A.: Learning semantic segmentation of large-scale point clouds with random sampling.\n\n. 10.1109/TPAMI.2021.3083288IEEE Transactions on Pattern Analysis and Machine Intelligence. IEEE Transactions on Pattern Analysis and Machine Intelligence pp. 1-1 (2021). https://doi.org/10.1109/TPAMI.2021.3083288\n\nA model of saliency-based visual attention for rapid scene analysis. L Itti, C Koch, E Niebur, 10.1109/34.730558IEEE Transactions on Pattern Analysis and Machine Intelligence. 2011Itti, L., Koch, C., Niebur, E.: A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence 20(11), 1254-1259 (1998). https://doi.org/10.1109/34.730558\n\nSegmentation of salient regions in outdoor scenes using imagery and 3-d data. G Kim, D Huber, M Hebert, IEEE Workshop on Applications of Computer Vision. IEEEKim, G., Huber, D., Hebert, M.: Segmentation of salient regions in outdoor scenes using imagery and 3-d data. In: IEEE Workshop on Applications of Computer Vision. pp. 1-8. IEEE (2008)\n\nEscape from cells: Deep kd-networks for the recognition of 3d point cloud models. R Klokov, V Lempitsky, 10.1109/ICCV.2017.99IEEE/CVF International Conference on Computer Vision. Klokov, R., Lempitsky, V.: Escape from cells: Deep kd-networks for the recognition of 3d point cloud models. In: IEEE/CVF International Conference on Computer Vision. pp. 863-872 (2017). https://doi.org/10.1109/ICCV.2017.99\n\nDepth matters: Influence of depth cues on visual saliency. C Lang, T V Nguyen, H Katti, K Yadati, M Kankanhalli, S Yan, European conference on computer vision. SpringerLang, C., Nguyen, T.V., Katti, H., Yadati, K., Kankanhalli, M., Yan, S.: Depth matters: Influence of depth cues on visual saliency. In: European conference on computer vision. pp. 101-115. Springer (2012)\n\nOctree guided cnn with spherical kernels for 3d point clouds. H Lei, N Akhtar, A Mian, 10.1109/CVPR.2019.00986IEEE/CVF Conference on Computer Vision and Pattern Recognition. Lei, H., Akhtar, N., Mian, A.: Octree guided cnn with spherical kernels for 3d point clouds. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9623-9632 (2019). https://doi.org/10.1109/CVPR.2019.00986\n\nRgb-d salient object detection with cross-modality modulation and selection. C Li, R Cong, Y Piao, Q Xu, C C Loy, European Conference on Computer Vision. SpringerLi, C., Cong, R., Piao, Y., Xu, Q., Loy, C.C.: Rgb-d salient object detection with cross-modality modulation and selection. In: European Conference on Computer Vision. pp. 225-241. Springer (2020)\n\nSaliency detection on light field. N Li, J Ye, Y Ji, H Ling, J Yu, 10.1109/TPAMI.2016.2610425IEEE Transactions on Pattern Analysis and Machine Intelligence. 398Li, N., Ye, J., Ji, Y., Ling, H., Yu, J.: Saliency detection on light field. IEEE Transactions on Pattern Analysis and Machine Intelligence 39(8), 1605-1616 (2017). https://doi.org/10.1109/TPAMI.2016.2610425\n\nSaliency detection via dense and sparse reconstruction. X Li, H Lu, L Zhang, X Ruan, M H Yang, 10.1109/ICCV.2013.370IEEE/CVF International Conference on Computer Vision. Li, X., Lu, H., Zhang, L., Ruan, X., Yang, M.H.: Saliency detection via dense and sparse reconstruction. In: IEEE/CVF International Conference on Computer Vision. pp. 2976-2983 (2013). https://doi.org/10.1109/ICCV.2013.370\n\nPointcnn: Convolution on xtransformed points. Y Li, R Bu, M Sun, W Wu, X Di, B Chen, Advances in Neural Information Processing Systems. 31Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B.: Pointcnn: Convolution on x- transformed points. Advances in Neural Information Processing Systems 31 (2018)\n\nMmnet: Multi-stage and multi-scale fusion network for rgb-d salient object detection. G Liao, W Gao, Q Jiang, R Wang, G Li, ACM International Conference on Multimedia. Liao, G., Gao, W., Jiang, Q., Wang, R., Li, G.: Mmnet: Multi-stage and multi-scale fusion network for rgb-d salient object detection. In: ACM International Conference on Multimedia. pp. 2436-2444 (2020)\n\nA simple pooling-based design for real-time salient object detection. J J Liu, Q Hou, M M Cheng, J Feng, J Jiang, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Liu, J.J., Hou, Q., Cheng, M.M., Feng, J., Jiang, J.: A simple pooling-based design for real-time salient object detection. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3917-3926 (2019)\n\nDhsnet: Deep hierarchical saliency network for salient object detection. N Liu, J Han, 10.1109/CVPR.2016.80IEEE/CVF Conference on Computer Vision and Pattern Recognition. Liu, N., Han, J.: Dhsnet: Deep hierarchical saliency network for salient object detection. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 678-686 (2016). https://doi.org/10.1109/CVPR.2016.80\n\nLight field saliency detection with dual local graph learning and reciprocative guidance. N Liu, W Zhao, D Zhang, J Han, L Shao, IEEE/CVF International Conference on Computer Vision. Liu, N., Zhao, W., Zhang, D., Han, J., Shao, L.: Light field saliency detection with dual local graph learning and reciprocative guidance. In: IEEE/CVF International Conference on Computer Vision. pp. 4712-4721 (2021)\n\nPerceptual quality assessment of colored 3d point clouds. Q Liu, H Su, Z Duanmu, W Liu, Z Wang, 10.1109/TVCG.2022.3167151IEEE Transactions on Visualization and Computer Graphics. Liu, Q., Su, H., Duanmu, Z., Liu, W., Wang, Z.: Perceptual quality assessment of colored 3d point clouds. IEEE Transactions on Visualization and Computer Graphics pp. 1-1 (2022). https://doi.org/10.1109/TVCG.2022.3167151\n\nReceptive field block net for accurate and fast object detection. S Liu, D Huang, European conference on computer vision. Liu, S., Huang, D., et al.: Receptive field block net for accurate and fast object detection. In: European conference on computer vision. pp. 385-400 (2018)\n\nVariable rate roi image compression optimized for visual quality. Y Ma, Y Zhai, C Yang, J Yang, R Wang, J Zhou, K Li, Y Chen, R Wang, 10.1109/CVPRW53098.2021.00221IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. Ma, Y., Zhai, Y., Yang, C., Yang, J., Wang, R., Zhou, J., Li, K., Chen, Y., Wang, R.: Variable rate roi image compression optimized for visual quality. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 1936-1940 (2021). https://doi.org/10.1109/CVPRW53098.2021.00221\n\nHow to evaluate foreground maps. R Margolin, L Zelnik-Manor, A Tal, 10.1109/CVPR.2014.39IEEE/CVF Conference on Computer Vision and Pattern Recognition. Margolin, R., Zelnik-Manor, L., Tal, A.: How to evaluate foreground maps. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 248-255 (2014). https://doi.org/10.1109/CVPR.2014.39\n\nPartnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. K Mo, S Zhu, A X Chang, L Yi, S Tripathi, L J Guibas, H Su, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Mo, K., Zhu, S., Chang, A.X., Yi, L., Tripathi, S., Guibas, L.J., Su, H.: Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object under- standing. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 909-918 (2019)\n\nMulti-scale interactive network for salient object detection. Y Pang, X Zhao, L Zhang, H Lu, 10.1109/CVPR42600.2020.00943IEEE/CVF Conference on Computer Vision and Pattern Recognition. Pang, Y., Zhao, X., Zhang, L., Lu, H.: Multi-scale interactive network for salient object detection. In: IEEE/CVF Conference on Computer Vision and Pattern Recog- nition. pp. 9410-9419 (2020). https://doi.org/10.1109/CVPR42600.2020.00943\n\nSaliency filters: Contrast based filtering for salient region detection. F Perazzi, P Kr\u00e4henb\u00fchl, Y Pritch, A Hornung, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Perazzi, F., Kr\u00e4henb\u00fchl, P., Pritch, Y., Hornung, A.: Saliency filters: Con- trast based filtering for salient region detection. In: IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition. pp. 733-740 (2012).\n\n. 10.1109/CVPR.2012.6247743https://doi.org/10.1109/CVPR.2012.6247743\n\nExploit and replace: An asymmetrical twostream architecture for versatile light field saliency detection. Y Piao, Z Rong, M Zhang, H Lu, AAAI Conference on Artificial Intelligence. 34Piao, Y., Rong, Z., Zhang, M., Lu, H.: Exploit and replace: An asymmetrical two- stream architecture for versatile light field saliency detection. In: AAAI Conference on Artificial Intelligence. vol. 34, pp. 11865-11873 (2020)\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, Advances in Neural Information Processing Systems. 30Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in Neural Information Processing Systems 30 (2017)\n\nAssanet: An anisotropic separable set abstraction for efficient point cloud representation learning. G Qian, H Hammoud, G Li, A Thabet, B Ghanem, Advances in Neural Information Processing Systems. 34Qian, G., Hammoud, H., Li, G., Thabet, A., Ghanem, B.: Assanet: An anisotropic separable set abstraction for efficient point cloud representation learning. Advances in Neural Information Processing Systems 34 (2021)\n\nBasnet: Boundary-aware salient object detection. X Qin, Z Zhang, C Huang, C Gao, M Dehghan, M Jagersand, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Qin, X., Zhang, Z., Huang, C., Gao, C., Dehghan, M., Jagersand, M.: Basnet: Boundary-aware salient object detection. In: IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition. pp. 7471-7481 (2019).\n\n. 10.1109/CVPR.2019.00766https://doi.org/10.1109/CVPR.2019.00766\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International journal of computer vision. 1153Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpa- thy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recognition challenge. International journal of computer vision 115(3), 211-252 (2015)\n\nPointrcnn: 3d object proposal generation and detection from point cloud. S Shi, X Wang, H Li, 10.1109/CVPR.2019.00086IEEE/CVF Conference on Computer Vision and Pattern Recognition. Shi, S., Wang, X., Li, H.: Pointrcnn: 3d object proposal generation and detection from point cloud. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 770-779 (2019). https://doi.org/10.1109/CVPR.2019.00086\n\nSaliency detection in large point sets. E Shtrom, G Leifman, A Tal, 10.1109/ICCV.2013.446IEEE/CVF International Conference on Computer Vision. Shtrom, E., Leifman, G., Tal, A.: Saliency detection in large point sets. In: IEEE/CVF International Conference on Computer Vision. pp. 3591-3598 (2013). https://doi.org/10.1109/ICCV.2013.446\n\nInferring attention shift ranks of objects for image saliency. A Siris, J Jiao, G K Tam, X Xie, R W Lau, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Siris, A., Jiao, J., Tam, G.K., Xie, X., Lau, R.W.: Inferring atten- tion shift ranks of objects for image saliency. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12130-12140 (2020).\n\n. 10.1109/CVPR42600.2020.01215https://doi.org/10.1109/CVPR42600.2020.01215\n\nScene context-aware salient object detection. A Siris, J Jiao, G K Tam, X Xie, R W Lau, IEEE/CVF International Conference on Computer Vision. Siris, A., Jiao, J., Tam, G.K., Xie, X., Lau, R.W.: Scene context-aware salient object detection. In: IEEE/CVF International Conference on Computer Vision. pp. 4156-4166 (2021)\n\nCluster-based point set saliency. F P Tasse, J Kosinka, N Dodgson, IEEE/CVF International Conference on Computer Vision. Tasse, F.P., Kosinka, J., Dodgson, N.: Cluster-based point set saliency. In: IEEE/CVF International Conference on Computer Vision. pp. 163-171 (2015).\n\n. 10.1109/ICCV.2015.27https://doi.org/10.1109/ICCV.2015.27\n\nReal-time salient object detection with a minimum spanning tree. W C Tu, S He, Q Yang, S Y Chien, 10.1109/CVPR.2016.256IEEE/CVF Conference on Computer Vision and Pattern Recognition. Tu, W.C., He, S., Yang, Q., Chien, S.Y.: Real-time salient object detection with a minimum spanning tree. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2334-2342 (2016). https://doi.org/10.1109/CVPR.2016.256\n\nMulti-task learning for dense prediction tasks: A survey. S Vandenhende, S Georgoulis, W Van Gansbeke, M Proesmans, D Dai, L Van Gool, IEEE Transactions on Pattern Analysis and Machine Intelligence. Vandenhende, S., Georgoulis, S., Van Gansbeke, W., Proesmans, M., Dai, D., Van Gool, L.: Multi-task learning for dense prediction tasks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence pp. 1-1 (2021).\n\n. 10.1109/TPAMI.2021.3054719https://doi.org/10.1109/TPAMI.2021.3054719\n\nLearning to detect salient objects with image-level supervision. L Wang, H Lu, Y Wang, M Feng, D Wang, B Yin, X Ruan, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Wang, L., Lu, H., Wang, Y., Feng, M., Wang, D., Yin, B., Ruan, X.: Learn- ing to detect salient objects with image-level supervision. In: IEEE/CVF Con- ference on Computer Vision and Pattern Recognition. pp. 3796-3805 (2017).\n\n. 10.1109/CVPR.2017.404https://doi.org/10.1109/CVPR.2017.404\n\nDeep learning for light field saliency detection. T Wang, Y Piao, H Lu, X Li, L Zhang, 10.1109/ICCV.2019.00893IEEE/CVF International Conference on Computer Vision. Wang, T., Piao, Y., Lu, H., Li, X., Zhang, L.: Deep learning for light field saliency detection. In: IEEE/CVF International Conference on Computer Vision. pp. 8837- 8847 (2019). https://doi.org/10.1109/ICCV.2019.00893\n\nSalient object detection in the deep learning era: An in-depth survey. W Wang, Q Lai, H Fu, J Shen, H Ling, R Yang, 10.1109/TPAMI.2021.3051099IEEE Transactions on Pattern Analysis and Machine Intelligence. Wang, W., Lai, Q., Fu, H., Shen, J., Ling, H., Yang, R.: Salient object detection in the deep learning era: An in-depth survey. IEEE Transactions on Pattern Analysis and Machine Intelligence pp. 1-1 (2021). https://doi.org/10.1109/TPAMI.2021.3051099\n\nDynamic graph cnn for learning on point clouds. Y Wang, Y Sun, Z Liu, S E Sarma, M M Bronstein, J M Solomon, ACM Transactions On Graphics. 385Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M.: Dynamic graph cnn for learning on point clouds. ACM Transactions On Graphics 38(5), 1-12 (2019)\n\nGeodesic saliency using background priors. Y Wei, F Wen, W Zhu, J Sun, European Conference on Computer Vision. SpringerWei, Y., Wen, F., Zhu, W., Sun, J.: Geodesic saliency using background priors. In: European Conference on Computer Vision. pp. 29-42. Springer (2012)\n\nPointconv: Deep convolutional networks on 3d point clouds. W Wu, Z Qi, L Fuxin, 10.1109/CVPR.2019.009852019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Wu, W., Qi, Z., Fuxin, L.: Pointconv: Deep convolutional networks on 3d point clouds. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion (CVPR). pp. 9613-9622 (2019). https://doi.org/10.1109/CVPR.2019.00985\n\nProgressive self-guided loss for salient object detection. S Yang, W Lin, G Lin, Q Jiang, Z Liu, 10.1109/TIP.2021.3113794IEEE Transactions on Image Processing. 30Yang, S., Lin, W., Lin, G., Jiang, Q., Liu, Z.: Progressive self-guided loss for salient object detection. IEEE Transactions on Image Processing 30, 8426-8438 (2021). https://doi.org/10.1109/TIP.2021.3113794\n\nUc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders. J Zhang, D P Fan, Y Dai, S Anwar, F S Saleh, T Zhang, N Barnes, 10.1109/CVPR42600.2020.00861IEEE/CVF Conference on Computer Vision and Pattern Recognition. Zhang, J., Fan, D.P., Dai, Y., Anwar, S., Saleh, F.S., Zhang, T., Barnes, N.: Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoen- coders. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8579-8588 (2020). https://doi.org/10.1109/CVPR42600.2020.00861\n\nLight field saliency detection with deep convolutional networks. J Zhang, Y Liu, S Zhang, R Poppe, M Wang, 10.1109/TIP.2020.2970529IEEE Transactions on Image Processing. 29Zhang, J., Liu, Y., Zhang, S., Poppe, R., Wang, M.: Light field saliency detection with deep convolutional networks. IEEE Transactions on Image Processing 29, 4421-4434 (2020). https://doi.org/10.1109/TIP.2020.2970529\n\nAsymmetric two-stream architecture for accurate rgb-d saliency detection. M Zhang, S X Fei, J Liu, S Xu, Y Piao, H Lu, European Conference on Computer Vision. SpringerZhang, M., Fei, S.X., Liu, J., Xu, S., Piao, Y., Lu, H.: Asymmetric two-stream architecture for accurate rgb-d saliency detection. In: European Conference on Computer Vision. pp. 374-390. Springer (2020)\n\nShellnet: Efficient point cloud convolutional neural networks using concentric shells statistics. Z Zhang, B S Hua, S K Yeung, IEEE/CVF International Conference on Computer Vision. Zhang, Z., Hua, B.S., Yeung, S.K.: Shellnet: Efficient point cloud convo- lutional neural networks using concentric shells statistics. In: IEEE/CVF International Conference on Computer Vision. pp. 1607-1616 (2019).\n\n. 10.1109/ICCV.2019.00169https://doi.org/10.1109/ICCV.2019.00169\n\nEgnet: Edge guidance network for salient object detection. J Zhao, J J Liu, D P Fan, Y Cao, J Yang, M M Cheng, 10.1109/ICCV.2019.00887IEEE/CVF International Conference on Computer Vision. Zhao, J., Liu, J.J., Fan, D.P., Cao, Y., Yang, J., Cheng, M.M.: Egnet: Edge guidance network for salient object detection. In: IEEE/CVF International Conference on Computer Vision. pp. 8778-8787 (2019). https://doi.org/10.1109/ICCV.2019.00887\n\nPointcloud saliency maps. T Zheng, C Chen, J Yuan, B Li, K Ren, 10.1109/ICCV.2019.00168IEEE/CVF International Conference on Computer Vision. Zheng, T., Chen, C., Yuan, J., Li, B., Ren, K.: Pointcloud saliency maps. In: IEEE/CVF International Conference on Computer Vision. pp. 1598-1606 (2019). https://doi.org/10.1109/ICCV.2019.00168\n\nRgb-d salient object detection: A survey. T Zhou, D P Fan, M M Cheng, J Shen, L Shao, Computational Visual Media. 71Zhou, T., Fan, D.P., Cheng, M.M., Shen, J., Shao, L.: Rgb-d salient object detection: A survey. Computational Visual Media 7(1), 37-69 (2021)\n", "annotations": {"author": "[{\"end\":143,\"start\":45},{\"end\":221,\"start\":144},{\"end\":289,\"start\":222}]", "publisher": null, "author_last_name": "[{\"end\":56,\"start\":53},{\"end\":151,\"start\":148},{\"end\":227,\"start\":225}]", "author_first_name": "[{\"end\":52,\"start\":45},{\"end\":147,\"start\":144},{\"end\":224,\"start\":222}]", "author_affiliation": "[{\"end\":118,\"start\":75},{\"end\":142,\"start\":120},{\"end\":196,\"start\":153},{\"end\":220,\"start\":198},{\"end\":288,\"start\":245}]", "title": "[{\"end\":42,\"start\":1},{\"end\":331,\"start\":290}]", "venue": null, "abstract": "[{\"end\":1735,\"start\":383}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2001,\"start\":1997},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2019,\"start\":2015},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2048,\"start\":2044},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2100,\"start\":2097},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2103,\"start\":2100},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":2106,\"start\":2103},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2306,\"start\":2303},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2309,\"start\":2306},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2312,\"start\":2309},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2315,\"start\":2312},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2318,\"start\":2315},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":2321,\"start\":2318},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2866,\"start\":2863},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2889,\"start\":2885},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2912,\"start\":2908},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3334,\"start\":3331},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3337,\"start\":3334},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6123,\"start\":6119},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6303,\"start\":6300},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6306,\"start\":6303},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6309,\"start\":6306},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6312,\"start\":6309},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":6315,\"start\":6312},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6443,\"start\":6440},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6446,\"start\":6443},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6449,\"start\":6446},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6452,\"start\":6449},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":6455,\"start\":6452},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6686,\"start\":6682},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6729,\"start\":6726},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6732,\"start\":6729},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7786,\"start\":7782},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7809,\"start\":7805},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7812,\"start\":7809},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7815,\"start\":7812},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7818,\"start\":7815},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8106,\"start\":8102},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8257,\"start\":8253},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8476,\"start\":8473},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8479,\"start\":8476},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8482,\"start\":8479},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8679,\"start\":8675},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8682,\"start\":8679},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":8685,\"start\":8682},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":8688,\"start\":8685},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8885,\"start\":8881},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8888,\"start\":8885},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8891,\"start\":8888},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8894,\"start\":8891},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8897,\"start\":8894},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":8900,\"start\":8897},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8903,\"start\":8900},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8931,\"start\":8927},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8934,\"start\":8931},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8937,\"start\":8934},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":8940,\"start\":8937},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9106,\"start\":9102},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9109,\"start\":9106},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":9112,\"start\":9109},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9369,\"start\":9366},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9372,\"start\":9369},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9375,\"start\":9372},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9378,\"start\":9375},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9381,\"start\":9378},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":9384,\"start\":9381},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9768,\"start\":9764},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9771,\"start\":9768},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9869,\"start\":9866},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9872,\"start\":9869},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10193,\"start\":10189},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":10196,\"start\":10193},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10236,\"start\":10232},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":10239,\"start\":10236},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10268,\"start\":10264},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10271,\"start\":10268},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10287,\"start\":10283},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":10399,\"start\":10395},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":10496,\"start\":10492},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10631,\"start\":10627},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10819,\"start\":10815},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10840,\"start\":10836},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11086,\"start\":11083},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11088,\"start\":11086},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11090,\"start\":11088},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11093,\"start\":11090},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11096,\"start\":11093},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11229,\"start\":11225},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11913,\"start\":11909},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11916,\"start\":11913},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12407,\"start\":12404},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12410,\"start\":12407},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12571,\"start\":12568},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":12574,\"start\":12571},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":12710,\"start\":12706},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15737,\"start\":15733},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15778,\"start\":15774},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15924,\"start\":15921},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15927,\"start\":15924},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15930,\"start\":15927},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16896,\"start\":16892},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16899,\"start\":16896},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17979,\"start\":17976},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17982,\"start\":17979},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17985,\"start\":17982},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18306,\"start\":18302},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22375,\"start\":22371},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22620,\"start\":22616},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22635,\"start\":22632},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23506,\"start\":23503},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23743,\"start\":23740},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23746,\"start\":23743},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23749,\"start\":23746},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23752,\"start\":23749},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":23755,\"start\":23752},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23828,\"start\":23825},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23877,\"start\":23873},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23958,\"start\":23954},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":23973,\"start\":23969},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23990,\"start\":23986},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24562,\"start\":24559},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24579,\"start\":24575},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24594,\"start\":24590},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":24609,\"start\":24605},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24626,\"start\":24622},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25798,\"start\":25794},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26681,\"start\":26677},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28418,\"start\":28415},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28421,\"start\":28418},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28424,\"start\":28421}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27993,\"start\":27670},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28315,\"start\":27994},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28800,\"start\":28316},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29209,\"start\":28801},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29845,\"start\":29210},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29908,\"start\":29846},{\"attributes\":{\"id\":\"fig_7\"},\"end\":30892,\"start\":29909},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31393,\"start\":30893},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32726,\"start\":31394},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32971,\"start\":32727},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34377,\"start\":32972},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34681,\"start\":34378}]", "paragraph": "[{\"end\":4027,\"start\":1751},{\"end\":5342,\"start\":4029},{\"end\":6188,\"start\":5344},{\"end\":7204,\"start\":6190},{\"end\":7259,\"start\":7206},{\"end\":7564,\"start\":7261},{\"end\":7709,\"start\":7566},{\"end\":9281,\"start\":7726},{\"end\":9653,\"start\":9283},{\"end\":11007,\"start\":9655},{\"end\":11830,\"start\":11032},{\"end\":12800,\"start\":11832},{\"end\":13467,\"start\":12823},{\"end\":14111,\"start\":13469},{\"end\":14727,\"start\":14113},{\"end\":15379,\"start\":14747},{\"end\":17863,\"start\":15404},{\"end\":18350,\"start\":17884},{\"end\":18514,\"start\":18352},{\"end\":19174,\"start\":18593},{\"end\":19359,\"start\":19219},{\"end\":19648,\"start\":19383},{\"end\":20094,\"start\":19712},{\"end\":20329,\"start\":20096},{\"end\":20944,\"start\":20331},{\"end\":21368,\"start\":21001},{\"end\":22435,\"start\":21405},{\"end\":22814,\"start\":22437},{\"end\":23550,\"start\":22845},{\"end\":24641,\"start\":23578},{\"end\":24825,\"start\":24643},{\"end\":25610,\"start\":24827},{\"end\":26019,\"start\":25629},{\"end\":26771,\"start\":26021},{\"end\":27669,\"start\":26786}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18592,\"start\":18515},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19218,\"start\":19175},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19382,\"start\":19360},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19711,\"start\":19649},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21000,\"start\":20945},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22844,\"start\":22815}]", "table_ref": "[{\"end\":24674,\"start\":24671},{\"end\":26486,\"start\":26482}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1749,\"start\":1737},{\"attributes\":{\"n\":\"2\"},\"end\":7724,\"start\":7712},{\"attributes\":{\"n\":\"3.1\"},\"end\":11030,\"start\":11010},{\"attributes\":{\"n\":\"3.2\"},\"end\":12821,\"start\":12803},{\"attributes\":{\"n\":\"4\"},\"end\":14745,\"start\":14730},{\"attributes\":{\"n\":\"4.1\"},\"end\":15402,\"start\":15382},{\"attributes\":{\"n\":\"4.2\"},\"end\":17882,\"start\":17866},{\"attributes\":{\"n\":\"5\"},\"end\":21382,\"start\":21371},{\"attributes\":{\"n\":\"5.1\"},\"end\":21403,\"start\":21385},{\"attributes\":{\"n\":\"5.2\"},\"end\":23576,\"start\":23553},{\"attributes\":{\"n\":\"5.3\"},\"end\":25627,\"start\":25613},{\"attributes\":{\"n\":\"6\"},\"end\":26784,\"start\":26774},{\"end\":27679,\"start\":27671},{\"end\":27998,\"start\":27995},{\"end\":28325,\"start\":28317},{\"end\":28810,\"start\":28802},{\"end\":29219,\"start\":29211},{\"end\":29855,\"start\":29847},{\"end\":29921,\"start\":29910},{\"end\":32737,\"start\":32728},{\"end\":34388,\"start\":34379}]", "table": "[{\"end\":31393,\"start\":31019},{\"end\":32726,\"start\":31538},{\"end\":32971,\"start\":32739},{\"end\":34377,\"start\":33942}]", "figure_caption": "[{\"end\":27993,\"start\":27681},{\"end\":28315,\"start\":28000},{\"end\":28800,\"start\":28327},{\"end\":29209,\"start\":28812},{\"end\":29845,\"start\":29221},{\"end\":29908,\"start\":29857},{\"end\":30892,\"start\":29924},{\"end\":31019,\"start\":30895},{\"end\":31538,\"start\":31396},{\"end\":33942,\"start\":32974},{\"end\":34681,\"start\":34390}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3597,\"start\":3591},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4376,\"start\":4370},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5280,\"start\":5274},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11653,\"start\":11647},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12522,\"start\":12516},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13040,\"start\":13034},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13054,\"start\":13048},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13621,\"start\":13615},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14109,\"start\":14100},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14526,\"start\":14517},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14537,\"start\":14528},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15422,\"start\":15416},{\"end\":16506,\"start\":16500},{\"end\":18370,\"start\":18364},{\"end\":20349,\"start\":20343},{\"end\":20579,\"start\":20573},{\"end\":24337,\"start\":24331},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24694,\"start\":24688},{\"end\":25035,\"start\":25029}]", "bib_author_first_name": "[{\"end\":34734,\"start\":34733},{\"end\":34744,\"start\":34743},{\"end\":34753,\"start\":34752},{\"end\":34755,\"start\":34754},{\"end\":34764,\"start\":34763},{\"end\":34773,\"start\":34772},{\"end\":34785,\"start\":34784},{\"end\":34796,\"start\":34795},{\"end\":35178,\"start\":35177},{\"end\":35188,\"start\":35187},{\"end\":35199,\"start\":35198},{\"end\":35210,\"start\":35209},{\"end\":35221,\"start\":35220},{\"end\":35231,\"start\":35230},{\"end\":35244,\"start\":35243},{\"end\":35630,\"start\":35629},{\"end\":35632,\"start\":35631},{\"end\":35643,\"start\":35642},{\"end\":35649,\"start\":35648},{\"end\":35660,\"start\":35659},{\"end\":35662,\"start\":35661},{\"end\":36096,\"start\":36095},{\"end\":36104,\"start\":36103},{\"end\":36110,\"start\":36109},{\"end\":36118,\"start\":36117},{\"end\":36439,\"start\":36438},{\"end\":36446,\"start\":36445},{\"end\":36448,\"start\":36447},{\"end\":36457,\"start\":36456},{\"end\":36466,\"start\":36465},{\"end\":36476,\"start\":36475},{\"end\":36490,\"start\":36489},{\"end\":36859,\"start\":36858},{\"end\":36867,\"start\":36866},{\"end\":36874,\"start\":36873},{\"end\":36882,\"start\":36881},{\"end\":37229,\"start\":37228},{\"end\":37231,\"start\":37230},{\"end\":37238,\"start\":37237},{\"end\":37240,\"start\":37239},{\"end\":37249,\"start\":37248},{\"end\":37256,\"start\":37255},{\"end\":37262,\"start\":37261},{\"end\":37600,\"start\":37599},{\"end\":37602,\"start\":37601},{\"end\":37609,\"start\":37608},{\"end\":37617,\"start\":37616},{\"end\":37624,\"start\":37623},{\"end\":37631,\"start\":37630},{\"end\":37633,\"start\":37632},{\"end\":37642,\"start\":37641},{\"end\":37908,\"start\":37907},{\"end\":37910,\"start\":37909},{\"end\":37917,\"start\":37916},{\"end\":37925,\"start\":37924},{\"end\":37927,\"start\":37926},{\"end\":37936,\"start\":37935},{\"end\":38350,\"start\":38349},{\"end\":38352,\"start\":38351},{\"end\":38359,\"start\":38358},{\"end\":38367,\"start\":38366},{\"end\":38376,\"start\":38375},{\"end\":38384,\"start\":38383},{\"end\":38651,\"start\":38650},{\"end\":38657,\"start\":38656},{\"end\":38666,\"start\":38665},{\"end\":38668,\"start\":38667},{\"end\":38674,\"start\":38673},{\"end\":38682,\"start\":38681},{\"end\":38690,\"start\":38689},{\"end\":38692,\"start\":38691},{\"end\":39046,\"start\":39045},{\"end\":39053,\"start\":39052},{\"end\":39061,\"start\":39060},{\"end\":39067,\"start\":39066},{\"end\":39073,\"start\":39072},{\"end\":39082,\"start\":39081},{\"end\":39468,\"start\":39467},{\"end\":39475,\"start\":39474},{\"end\":39483,\"start\":39482},{\"end\":39666,\"start\":39665},{\"end\":39676,\"start\":39675},{\"end\":39687,\"start\":39686},{\"end\":39698,\"start\":39697},{\"end\":39700,\"start\":39699},{\"end\":39710,\"start\":39709},{\"end\":39723,\"start\":39722},{\"end\":40101,\"start\":40100},{\"end\":40108,\"start\":40107},{\"end\":40110,\"start\":40109},{\"end\":40119,\"start\":40118},{\"end\":40125,\"start\":40124},{\"end\":40134,\"start\":40133},{\"end\":40140,\"start\":40139},{\"end\":40144,\"start\":40141},{\"end\":40591,\"start\":40590},{\"end\":40597,\"start\":40596},{\"end\":40605,\"start\":40604},{\"end\":40612,\"start\":40611},{\"end\":40620,\"start\":40619},{\"end\":40627,\"start\":40626},{\"end\":40635,\"start\":40634},{\"end\":40646,\"start\":40645},{\"end\":41105,\"start\":41104},{\"end\":41113,\"start\":41112},{\"end\":41121,\"start\":41120},{\"end\":41519,\"start\":41518},{\"end\":41526,\"start\":41525},{\"end\":41535,\"start\":41534},{\"end\":41867,\"start\":41866},{\"end\":41877,\"start\":41876},{\"end\":42248,\"start\":42247},{\"end\":42256,\"start\":42255},{\"end\":42258,\"start\":42257},{\"end\":42268,\"start\":42267},{\"end\":42277,\"start\":42276},{\"end\":42287,\"start\":42286},{\"end\":42302,\"start\":42301},{\"end\":42625,\"start\":42624},{\"end\":42632,\"start\":42631},{\"end\":42642,\"start\":42641},{\"end\":43038,\"start\":43037},{\"end\":43044,\"start\":43043},{\"end\":43052,\"start\":43051},{\"end\":43060,\"start\":43059},{\"end\":43066,\"start\":43065},{\"end\":43068,\"start\":43067},{\"end\":43356,\"start\":43355},{\"end\":43362,\"start\":43361},{\"end\":43368,\"start\":43367},{\"end\":43374,\"start\":43373},{\"end\":43382,\"start\":43381},{\"end\":43746,\"start\":43745},{\"end\":43752,\"start\":43751},{\"end\":43758,\"start\":43757},{\"end\":43767,\"start\":43766},{\"end\":43775,\"start\":43774},{\"end\":43777,\"start\":43776},{\"end\":44130,\"start\":44129},{\"end\":44136,\"start\":44135},{\"end\":44142,\"start\":44141},{\"end\":44149,\"start\":44148},{\"end\":44155,\"start\":44154},{\"end\":44161,\"start\":44160},{\"end\":44468,\"start\":44467},{\"end\":44476,\"start\":44475},{\"end\":44483,\"start\":44482},{\"end\":44492,\"start\":44491},{\"end\":44500,\"start\":44499},{\"end\":44824,\"start\":44823},{\"end\":44826,\"start\":44825},{\"end\":44833,\"start\":44832},{\"end\":44840,\"start\":44839},{\"end\":44842,\"start\":44841},{\"end\":44851,\"start\":44850},{\"end\":44859,\"start\":44858},{\"end\":45219,\"start\":45218},{\"end\":45226,\"start\":45225},{\"end\":45624,\"start\":45623},{\"end\":45631,\"start\":45630},{\"end\":45639,\"start\":45638},{\"end\":45648,\"start\":45647},{\"end\":45655,\"start\":45654},{\"end\":45994,\"start\":45993},{\"end\":46001,\"start\":46000},{\"end\":46007,\"start\":46006},{\"end\":46017,\"start\":46016},{\"end\":46024,\"start\":46023},{\"end\":46403,\"start\":46402},{\"end\":46410,\"start\":46409},{\"end\":46683,\"start\":46682},{\"end\":46689,\"start\":46688},{\"end\":46697,\"start\":46696},{\"end\":46705,\"start\":46704},{\"end\":46713,\"start\":46712},{\"end\":46721,\"start\":46720},{\"end\":46729,\"start\":46728},{\"end\":46735,\"start\":46734},{\"end\":46743,\"start\":46742},{\"end\":47186,\"start\":47185},{\"end\":47198,\"start\":47197},{\"end\":47214,\"start\":47213},{\"end\":47608,\"start\":47607},{\"end\":47614,\"start\":47613},{\"end\":47621,\"start\":47620},{\"end\":47623,\"start\":47622},{\"end\":47632,\"start\":47631},{\"end\":47638,\"start\":47637},{\"end\":47650,\"start\":47649},{\"end\":47652,\"start\":47651},{\"end\":47662,\"start\":47661},{\"end\":48061,\"start\":48060},{\"end\":48069,\"start\":48068},{\"end\":48077,\"start\":48076},{\"end\":48086,\"start\":48085},{\"end\":48496,\"start\":48495},{\"end\":48507,\"start\":48506},{\"end\":48521,\"start\":48520},{\"end\":48531,\"start\":48530},{\"end\":49002,\"start\":49001},{\"end\":49010,\"start\":49009},{\"end\":49018,\"start\":49017},{\"end\":49027,\"start\":49026},{\"end\":49387,\"start\":49386},{\"end\":49389,\"start\":49388},{\"end\":49395,\"start\":49394},{\"end\":49401,\"start\":49400},{\"end\":49407,\"start\":49406},{\"end\":49409,\"start\":49408},{\"end\":49754,\"start\":49753},{\"end\":49762,\"start\":49761},{\"end\":49773,\"start\":49772},{\"end\":49779,\"start\":49778},{\"end\":49789,\"start\":49788},{\"end\":50118,\"start\":50117},{\"end\":50125,\"start\":50124},{\"end\":50134,\"start\":50133},{\"end\":50143,\"start\":50142},{\"end\":50150,\"start\":50149},{\"end\":50161,\"start\":50160},{\"end\":50565,\"start\":50564},{\"end\":50580,\"start\":50579},{\"end\":50588,\"start\":50587},{\"end\":50594,\"start\":50593},{\"end\":50604,\"start\":50603},{\"end\":50616,\"start\":50615},{\"end\":50622,\"start\":50621},{\"end\":50631,\"start\":50630},{\"end\":50643,\"start\":50642},{\"end\":50653,\"start\":50652},{\"end\":51032,\"start\":51031},{\"end\":51039,\"start\":51038},{\"end\":51047,\"start\":51046},{\"end\":51409,\"start\":51408},{\"end\":51419,\"start\":51418},{\"end\":51430,\"start\":51429},{\"end\":51768,\"start\":51767},{\"end\":51777,\"start\":51776},{\"end\":51785,\"start\":51784},{\"end\":51787,\"start\":51786},{\"end\":51794,\"start\":51793},{\"end\":51801,\"start\":51800},{\"end\":51803,\"start\":51802},{\"end\":52206,\"start\":52205},{\"end\":52215,\"start\":52214},{\"end\":52223,\"start\":52222},{\"end\":52225,\"start\":52224},{\"end\":52232,\"start\":52231},{\"end\":52239,\"start\":52238},{\"end\":52241,\"start\":52240},{\"end\":52514,\"start\":52513},{\"end\":52516,\"start\":52515},{\"end\":52525,\"start\":52524},{\"end\":52536,\"start\":52535},{\"end\":52878,\"start\":52877},{\"end\":52880,\"start\":52879},{\"end\":52886,\"start\":52885},{\"end\":52892,\"start\":52891},{\"end\":52900,\"start\":52899},{\"end\":52902,\"start\":52901},{\"end\":53289,\"start\":53288},{\"end\":53304,\"start\":53303},{\"end\":53318,\"start\":53317},{\"end\":53334,\"start\":53333},{\"end\":53347,\"start\":53346},{\"end\":53354,\"start\":53353},{\"end\":53794,\"start\":53793},{\"end\":53802,\"start\":53801},{\"end\":53808,\"start\":53807},{\"end\":53816,\"start\":53815},{\"end\":53824,\"start\":53823},{\"end\":53832,\"start\":53831},{\"end\":53839,\"start\":53838},{\"end\":54250,\"start\":54249},{\"end\":54258,\"start\":54257},{\"end\":54266,\"start\":54265},{\"end\":54272,\"start\":54271},{\"end\":54278,\"start\":54277},{\"end\":54654,\"start\":54653},{\"end\":54662,\"start\":54661},{\"end\":54669,\"start\":54668},{\"end\":54675,\"start\":54674},{\"end\":54683,\"start\":54682},{\"end\":54691,\"start\":54690},{\"end\":55088,\"start\":55087},{\"end\":55096,\"start\":55095},{\"end\":55103,\"start\":55102},{\"end\":55110,\"start\":55109},{\"end\":55112,\"start\":55111},{\"end\":55121,\"start\":55120},{\"end\":55123,\"start\":55122},{\"end\":55136,\"start\":55135},{\"end\":55138,\"start\":55137},{\"end\":55395,\"start\":55394},{\"end\":55402,\"start\":55401},{\"end\":55409,\"start\":55408},{\"end\":55416,\"start\":55415},{\"end\":55681,\"start\":55680},{\"end\":55687,\"start\":55686},{\"end\":55693,\"start\":55692},{\"end\":56091,\"start\":56090},{\"end\":56099,\"start\":56098},{\"end\":56106,\"start\":56105},{\"end\":56113,\"start\":56112},{\"end\":56122,\"start\":56121},{\"end\":56499,\"start\":56498},{\"end\":56508,\"start\":56507},{\"end\":56510,\"start\":56509},{\"end\":56517,\"start\":56516},{\"end\":56524,\"start\":56523},{\"end\":56533,\"start\":56532},{\"end\":56535,\"start\":56534},{\"end\":56544,\"start\":56543},{\"end\":56553,\"start\":56552},{\"end\":57032,\"start\":57031},{\"end\":57041,\"start\":57040},{\"end\":57048,\"start\":57047},{\"end\":57057,\"start\":57056},{\"end\":57066,\"start\":57065},{\"end\":57432,\"start\":57431},{\"end\":57441,\"start\":57440},{\"end\":57443,\"start\":57442},{\"end\":57450,\"start\":57449},{\"end\":57457,\"start\":57456},{\"end\":57463,\"start\":57462},{\"end\":57471,\"start\":57470},{\"end\":57828,\"start\":57827},{\"end\":57837,\"start\":57836},{\"end\":57839,\"start\":57838},{\"end\":57846,\"start\":57845},{\"end\":57848,\"start\":57847},{\"end\":58252,\"start\":58251},{\"end\":58260,\"start\":58259},{\"end\":58262,\"start\":58261},{\"end\":58269,\"start\":58268},{\"end\":58271,\"start\":58270},{\"end\":58278,\"start\":58277},{\"end\":58285,\"start\":58284},{\"end\":58293,\"start\":58292},{\"end\":58295,\"start\":58294},{\"end\":58651,\"start\":58650},{\"end\":58660,\"start\":58659},{\"end\":58668,\"start\":58667},{\"end\":58676,\"start\":58675},{\"end\":58682,\"start\":58681},{\"end\":59003,\"start\":59002},{\"end\":59011,\"start\":59010},{\"end\":59013,\"start\":59012},{\"end\":59020,\"start\":59019},{\"end\":59022,\"start\":59021},{\"end\":59031,\"start\":59030},{\"end\":59039,\"start\":59038}]", "bib_author_last_name": "[{\"end\":34741,\"start\":34735},{\"end\":34750,\"start\":34745},{\"end\":34761,\"start\":34756},{\"end\":34770,\"start\":34765},{\"end\":34782,\"start\":34774},{\"end\":34793,\"start\":34786},{\"end\":34805,\"start\":34797},{\"end\":35185,\"start\":35179},{\"end\":35196,\"start\":35189},{\"end\":35207,\"start\":35200},{\"end\":35218,\"start\":35211},{\"end\":35228,\"start\":35222},{\"end\":35241,\"start\":35232},{\"end\":35249,\"start\":35245},{\"end\":35640,\"start\":35633},{\"end\":35646,\"start\":35644},{\"end\":35657,\"start\":35650},{\"end\":35669,\"start\":35663},{\"end\":36101,\"start\":36097},{\"end\":36107,\"start\":36105},{\"end\":36115,\"start\":36111},{\"end\":36124,\"start\":36119},{\"end\":36443,\"start\":36440},{\"end\":36454,\"start\":36449},{\"end\":36463,\"start\":36458},{\"end\":36473,\"start\":36467},{\"end\":36487,\"start\":36477},{\"end\":36498,\"start\":36491},{\"end\":36864,\"start\":36860},{\"end\":36871,\"start\":36868},{\"end\":36879,\"start\":36875},{\"end\":36888,\"start\":36883},{\"end\":37235,\"start\":37232},{\"end\":37246,\"start\":37241},{\"end\":37253,\"start\":37250},{\"end\":37259,\"start\":37257},{\"end\":37268,\"start\":37263},{\"end\":37606,\"start\":37603},{\"end\":37614,\"start\":37610},{\"end\":37621,\"start\":37618},{\"end\":37628,\"start\":37625},{\"end\":37639,\"start\":37634},{\"end\":37648,\"start\":37643},{\"end\":37914,\"start\":37911},{\"end\":37922,\"start\":37918},{\"end\":37933,\"start\":37928},{\"end\":37941,\"start\":37937},{\"end\":38356,\"start\":38353},{\"end\":38364,\"start\":38360},{\"end\":38373,\"start\":38368},{\"end\":38381,\"start\":38377},{\"end\":38389,\"start\":38385},{\"end\":38654,\"start\":38652},{\"end\":38663,\"start\":38658},{\"end\":38671,\"start\":38669},{\"end\":38679,\"start\":38675},{\"end\":38687,\"start\":38683},{\"end\":38696,\"start\":38693},{\"end\":39050,\"start\":39047},{\"end\":39058,\"start\":39054},{\"end\":39064,\"start\":39062},{\"end\":39070,\"start\":39068},{\"end\":39079,\"start\":39074},{\"end\":39086,\"start\":39083},{\"end\":39472,\"start\":39469},{\"end\":39480,\"start\":39476},{\"end\":39487,\"start\":39484},{\"end\":39673,\"start\":39667},{\"end\":39684,\"start\":39677},{\"end\":39695,\"start\":39688},{\"end\":39707,\"start\":39701},{\"end\":39720,\"start\":39711},{\"end\":39733,\"start\":39724},{\"end\":40105,\"start\":40102},{\"end\":40116,\"start\":40111},{\"end\":40122,\"start\":40120},{\"end\":40131,\"start\":40126},{\"end\":40137,\"start\":40135},{\"end\":40149,\"start\":40145},{\"end\":40594,\"start\":40592},{\"end\":40602,\"start\":40598},{\"end\":40609,\"start\":40606},{\"end\":40617,\"start\":40613},{\"end\":40624,\"start\":40621},{\"end\":40632,\"start\":40628},{\"end\":40643,\"start\":40636},{\"end\":40654,\"start\":40647},{\"end\":41110,\"start\":41106},{\"end\":41118,\"start\":41114},{\"end\":41128,\"start\":41122},{\"end\":41523,\"start\":41520},{\"end\":41532,\"start\":41527},{\"end\":41542,\"start\":41536},{\"end\":41874,\"start\":41868},{\"end\":41887,\"start\":41878},{\"end\":42253,\"start\":42249},{\"end\":42265,\"start\":42259},{\"end\":42274,\"start\":42269},{\"end\":42284,\"start\":42278},{\"end\":42299,\"start\":42288},{\"end\":42306,\"start\":42303},{\"end\":42629,\"start\":42626},{\"end\":42639,\"start\":42633},{\"end\":42647,\"start\":42643},{\"end\":43041,\"start\":43039},{\"end\":43049,\"start\":43045},{\"end\":43057,\"start\":43053},{\"end\":43063,\"start\":43061},{\"end\":43072,\"start\":43069},{\"end\":43359,\"start\":43357},{\"end\":43365,\"start\":43363},{\"end\":43371,\"start\":43369},{\"end\":43379,\"start\":43375},{\"end\":43385,\"start\":43383},{\"end\":43749,\"start\":43747},{\"end\":43755,\"start\":43753},{\"end\":43764,\"start\":43759},{\"end\":43772,\"start\":43768},{\"end\":43782,\"start\":43778},{\"end\":44133,\"start\":44131},{\"end\":44139,\"start\":44137},{\"end\":44146,\"start\":44143},{\"end\":44152,\"start\":44150},{\"end\":44158,\"start\":44156},{\"end\":44166,\"start\":44162},{\"end\":44473,\"start\":44469},{\"end\":44480,\"start\":44477},{\"end\":44489,\"start\":44484},{\"end\":44497,\"start\":44493},{\"end\":44503,\"start\":44501},{\"end\":44830,\"start\":44827},{\"end\":44837,\"start\":44834},{\"end\":44848,\"start\":44843},{\"end\":44856,\"start\":44852},{\"end\":44865,\"start\":44860},{\"end\":45223,\"start\":45220},{\"end\":45230,\"start\":45227},{\"end\":45628,\"start\":45625},{\"end\":45636,\"start\":45632},{\"end\":45645,\"start\":45640},{\"end\":45652,\"start\":45649},{\"end\":45660,\"start\":45656},{\"end\":45998,\"start\":45995},{\"end\":46004,\"start\":46002},{\"end\":46014,\"start\":46008},{\"end\":46021,\"start\":46018},{\"end\":46029,\"start\":46025},{\"end\":46407,\"start\":46404},{\"end\":46416,\"start\":46411},{\"end\":46686,\"start\":46684},{\"end\":46694,\"start\":46690},{\"end\":46702,\"start\":46698},{\"end\":46710,\"start\":46706},{\"end\":46718,\"start\":46714},{\"end\":46726,\"start\":46722},{\"end\":46732,\"start\":46730},{\"end\":46740,\"start\":46736},{\"end\":46748,\"start\":46744},{\"end\":47195,\"start\":47187},{\"end\":47211,\"start\":47199},{\"end\":47218,\"start\":47215},{\"end\":47611,\"start\":47609},{\"end\":47618,\"start\":47615},{\"end\":47629,\"start\":47624},{\"end\":47635,\"start\":47633},{\"end\":47647,\"start\":47639},{\"end\":47659,\"start\":47653},{\"end\":47665,\"start\":47663},{\"end\":48066,\"start\":48062},{\"end\":48074,\"start\":48070},{\"end\":48083,\"start\":48078},{\"end\":48089,\"start\":48087},{\"end\":48504,\"start\":48497},{\"end\":48518,\"start\":48508},{\"end\":48528,\"start\":48522},{\"end\":48539,\"start\":48532},{\"end\":49007,\"start\":49003},{\"end\":49015,\"start\":49011},{\"end\":49024,\"start\":49019},{\"end\":49030,\"start\":49028},{\"end\":49392,\"start\":49390},{\"end\":49398,\"start\":49396},{\"end\":49404,\"start\":49402},{\"end\":49416,\"start\":49410},{\"end\":49759,\"start\":49755},{\"end\":49770,\"start\":49763},{\"end\":49776,\"start\":49774},{\"end\":49786,\"start\":49780},{\"end\":49796,\"start\":49790},{\"end\":50122,\"start\":50119},{\"end\":50131,\"start\":50126},{\"end\":50140,\"start\":50135},{\"end\":50147,\"start\":50144},{\"end\":50158,\"start\":50151},{\"end\":50171,\"start\":50162},{\"end\":50577,\"start\":50566},{\"end\":50585,\"start\":50581},{\"end\":50591,\"start\":50589},{\"end\":50601,\"start\":50595},{\"end\":50613,\"start\":50605},{\"end\":50619,\"start\":50617},{\"end\":50628,\"start\":50623},{\"end\":50640,\"start\":50632},{\"end\":50650,\"start\":50644},{\"end\":50663,\"start\":50654},{\"end\":51036,\"start\":51033},{\"end\":51044,\"start\":51040},{\"end\":51050,\"start\":51048},{\"end\":51416,\"start\":51410},{\"end\":51427,\"start\":51420},{\"end\":51434,\"start\":51431},{\"end\":51774,\"start\":51769},{\"end\":51782,\"start\":51778},{\"end\":51791,\"start\":51788},{\"end\":51798,\"start\":51795},{\"end\":51807,\"start\":51804},{\"end\":52212,\"start\":52207},{\"end\":52220,\"start\":52216},{\"end\":52229,\"start\":52226},{\"end\":52236,\"start\":52233},{\"end\":52245,\"start\":52242},{\"end\":52522,\"start\":52517},{\"end\":52533,\"start\":52526},{\"end\":52544,\"start\":52537},{\"end\":52883,\"start\":52881},{\"end\":52889,\"start\":52887},{\"end\":52897,\"start\":52893},{\"end\":52908,\"start\":52903},{\"end\":53301,\"start\":53290},{\"end\":53315,\"start\":53305},{\"end\":53331,\"start\":53319},{\"end\":53344,\"start\":53335},{\"end\":53351,\"start\":53348},{\"end\":53363,\"start\":53355},{\"end\":53799,\"start\":53795},{\"end\":53805,\"start\":53803},{\"end\":53813,\"start\":53809},{\"end\":53821,\"start\":53817},{\"end\":53829,\"start\":53825},{\"end\":53836,\"start\":53833},{\"end\":53844,\"start\":53840},{\"end\":54255,\"start\":54251},{\"end\":54263,\"start\":54259},{\"end\":54269,\"start\":54267},{\"end\":54275,\"start\":54273},{\"end\":54284,\"start\":54279},{\"end\":54659,\"start\":54655},{\"end\":54666,\"start\":54663},{\"end\":54672,\"start\":54670},{\"end\":54680,\"start\":54676},{\"end\":54688,\"start\":54684},{\"end\":54696,\"start\":54692},{\"end\":55093,\"start\":55089},{\"end\":55100,\"start\":55097},{\"end\":55107,\"start\":55104},{\"end\":55118,\"start\":55113},{\"end\":55133,\"start\":55124},{\"end\":55146,\"start\":55139},{\"end\":55399,\"start\":55396},{\"end\":55406,\"start\":55403},{\"end\":55413,\"start\":55410},{\"end\":55420,\"start\":55417},{\"end\":55684,\"start\":55682},{\"end\":55690,\"start\":55688},{\"end\":55699,\"start\":55694},{\"end\":56096,\"start\":56092},{\"end\":56103,\"start\":56100},{\"end\":56110,\"start\":56107},{\"end\":56119,\"start\":56114},{\"end\":56126,\"start\":56123},{\"end\":56505,\"start\":56500},{\"end\":56514,\"start\":56511},{\"end\":56521,\"start\":56518},{\"end\":56530,\"start\":56525},{\"end\":56541,\"start\":56536},{\"end\":56550,\"start\":56545},{\"end\":56560,\"start\":56554},{\"end\":57038,\"start\":57033},{\"end\":57045,\"start\":57042},{\"end\":57054,\"start\":57049},{\"end\":57063,\"start\":57058},{\"end\":57071,\"start\":57067},{\"end\":57438,\"start\":57433},{\"end\":57447,\"start\":57444},{\"end\":57454,\"start\":57451},{\"end\":57460,\"start\":57458},{\"end\":57468,\"start\":57464},{\"end\":57474,\"start\":57472},{\"end\":57834,\"start\":57829},{\"end\":57843,\"start\":57840},{\"end\":57854,\"start\":57849},{\"end\":58257,\"start\":58253},{\"end\":58266,\"start\":58263},{\"end\":58275,\"start\":58272},{\"end\":58282,\"start\":58279},{\"end\":58290,\"start\":58286},{\"end\":58301,\"start\":58296},{\"end\":58657,\"start\":58652},{\"end\":58665,\"start\":58661},{\"end\":58673,\"start\":58669},{\"end\":58679,\"start\":58677},{\"end\":58686,\"start\":58683},{\"end\":59008,\"start\":59004},{\"end\":59017,\"start\":59014},{\"end\":59028,\"start\":59023},{\"end\":59036,\"start\":59032},{\"end\":59044,\"start\":59040}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9649070},\"end\":35097,\"start\":34683},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":199441943},\"end\":35549,\"start\":35099},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5115938},\"end\":35950,\"start\":35551},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.16\",\"id\":\"b3\"},\"end\":36010,\"start\":35952},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":211677643},\"end\":36373,\"start\":36012},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7684883},\"end\":36789,\"start\":36375},{\"attributes\":{\"doi\":\"10.1109/TIP.2019.2918735\",\"id\":\"b6\",\"matched_paper_id\":174813856},\"end\":37168,\"start\":36791},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":22726592},\"end\":37532,\"start\":37170},{\"attributes\":{\"doi\":\"arXiv:1805.10421\",\"id\":\"b8\"},\"end\":37846,\"start\":37534},{\"attributes\":{\"doi\":\"10.1109/CVPR.2019.00875\",\"id\":\"b9\",\"matched_paper_id\":198905006},\"end\":38262,\"start\":37848},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220364394},\"end\":38648,\"start\":38264},{\"attributes\":{\"doi\":\"arXiv:2010.04968\",\"id\":\"b11\"},\"end\":38950,\"start\":38650},{\"attributes\":{\"id\":\"b12\"},\"end\":39316,\"start\":38952},{\"attributes\":{\"doi\":\"10.1109/TCSVT.2021.3082939\",\"id\":\"b13\"},\"end\":39388,\"start\":39318},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":31266318},\"end\":39663,\"start\":39390},{\"attributes\":{\"doi\":\"arXiv:1704.03847\",\"id\":\"b15\"},\"end\":40031,\"start\":39665},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3958565},\"end\":40435,\"start\":40033},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2018.2815688\",\"id\":\"b17\"},\"end\":40507,\"start\":40437},{\"attributes\":{\"id\":\"b18\"},\"end\":40818,\"start\":40509},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2021.3083288\",\"id\":\"b19\"},\"end\":41033,\"start\":40820},{\"attributes\":{\"doi\":\"10.1109/34.730558\",\"id\":\"b20\",\"matched_paper_id\":3108956},\"end\":41438,\"start\":41035},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14018752},\"end\":41782,\"start\":41440},{\"attributes\":{\"doi\":\"10.1109/ICCV.2017.99\",\"id\":\"b22\",\"matched_paper_id\":9597281},\"end\":42186,\"start\":41784},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17929353},\"end\":42560,\"start\":42188},{\"attributes\":{\"doi\":\"10.1109/CVPR.2019.00986\",\"id\":\"b24\",\"matched_paper_id\":67855534},\"end\":42958,\"start\":42562},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":220514387},\"end\":43318,\"start\":42960},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2016.2610425\",\"id\":\"b26\",\"matched_paper_id\":2321266},\"end\":43687,\"start\":43320},{\"attributes\":{\"doi\":\"10.1109/ICCV.2013.370\",\"id\":\"b27\",\"matched_paper_id\":9201661},\"end\":44081,\"start\":43689},{\"attributes\":{\"id\":\"b28\"},\"end\":44379,\"start\":44083},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":222278027},\"end\":44751,\"start\":44381},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":128297644},\"end\":45143,\"start\":44753},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.80\",\"id\":\"b31\",\"matched_paper_id\":14185112},\"end\":45531,\"start\":45145},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":238259283},\"end\":45933,\"start\":45533},{\"attributes\":{\"doi\":\"10.1109/TVCG.2022.3167151\",\"id\":\"b33\",\"matched_paper_id\":243938798},\"end\":46334,\"start\":45935},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":30876510},\"end\":46614,\"start\":46336},{\"attributes\":{\"doi\":\"10.1109/CVPRW53098.2021.00221\",\"id\":\"b35\",\"matched_paper_id\":235719801},\"end\":47150,\"start\":46616},{\"attributes\":{\"doi\":\"10.1109/CVPR.2014.39\",\"id\":\"b36\",\"matched_paper_id\":854969},\"end\":47502,\"start\":47152},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":54447604},\"end\":47996,\"start\":47504},{\"attributes\":{\"doi\":\"10.1109/CVPR42600.2020.00943\",\"id\":\"b38\",\"matched_paper_id\":219634286},\"end\":48420,\"start\":47998},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":9146763},\"end\":48823,\"start\":48422},{\"attributes\":{\"doi\":\"10.1109/CVPR.2012.6247743\",\"id\":\"b40\"},\"end\":48893,\"start\":48825},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":212852023},\"end\":49304,\"start\":48895},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1745976},\"end\":49650,\"start\":49306},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":239049766},\"end\":50066,\"start\":49652},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":196701583},\"end\":50445,\"start\":50068},{\"attributes\":{\"doi\":\"10.1109/CVPR.2019.00766\",\"id\":\"b45\"},\"end\":50511,\"start\":50447},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":2930547},\"end\":50956,\"start\":50513},{\"attributes\":{\"doi\":\"10.1109/CVPR.2019.00086\",\"id\":\"b47\",\"matched_paper_id\":54607410},\"end\":51366,\"start\":50958},{\"attributes\":{\"doi\":\"10.1109/ICCV.2013.446\",\"id\":\"b48\",\"matched_paper_id\":1298516},\"end\":51702,\"start\":51368},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":215541588},\"end\":52081,\"start\":51704},{\"attributes\":{\"doi\":\"10.1109/CVPR42600.2020.01215\",\"id\":\"b50\"},\"end\":52157,\"start\":52083},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":244304924},\"end\":52477,\"start\":52159},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":2568575},\"end\":52750,\"start\":52479},{\"attributes\":{\"doi\":\"10.1109/ICCV.2015.27\",\"id\":\"b53\"},\"end\":52810,\"start\":52752},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.256\",\"id\":\"b54\",\"matched_paper_id\":14724867},\"end\":53228,\"start\":52812},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":221771219},\"end\":53654,\"start\":53230},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2021.3054719\",\"id\":\"b56\"},\"end\":53726,\"start\":53656},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":6283383},\"end\":54135,\"start\":53728},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.404\",\"id\":\"b58\"},\"end\":54197,\"start\":54137},{\"attributes\":{\"doi\":\"10.1109/ICCV.2019.00893\",\"id\":\"b59\",\"matched_paper_id\":207979734},\"end\":54580,\"start\":54199},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2021.3051099\",\"id\":\"b60\",\"matched_paper_id\":126180710},\"end\":55037,\"start\":54582},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":94822},\"end\":55349,\"start\":55039},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":12495838},\"end\":55619,\"start\":55351},{\"attributes\":{\"doi\":\"10.1109/CVPR.2019.00985\",\"id\":\"b63\",\"matched_paper_id\":53720607},\"end\":56029,\"start\":55621},{\"attributes\":{\"doi\":\"10.1109/TIP.2021.3113794\",\"id\":\"b64\",\"matched_paper_id\":230799200},\"end\":56400,\"start\":56031},{\"attributes\":{\"doi\":\"10.1109/CVPR42600.2020.00861\",\"id\":\"b65\",\"matched_paper_id\":215745200},\"end\":56964,\"start\":56402},{\"attributes\":{\"doi\":\"10.1109/TIP.2020.2970529\",\"id\":\"b66\",\"matched_paper_id\":195218894},\"end\":57355,\"start\":56966},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":221999449},\"end\":57727,\"start\":57357},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":201070399},\"end\":58124,\"start\":57729},{\"attributes\":{\"doi\":\"10.1109/ICCV.2019.00169\",\"id\":\"b69\"},\"end\":58190,\"start\":58126},{\"attributes\":{\"doi\":\"10.1109/ICCV.2019.00887\",\"id\":\"b70\",\"matched_paper_id\":201319385},\"end\":58622,\"start\":58192},{\"attributes\":{\"doi\":\"10.1109/ICCV.2019.00168\",\"id\":\"b71\",\"matched_paper_id\":90248873},\"end\":58958,\"start\":58624},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":220936082},\"end\":59217,\"start\":58960}]", "bib_title": "[{\"end\":34731,\"start\":34683},{\"end\":35175,\"start\":35099},{\"end\":35627,\"start\":35551},{\"end\":36093,\"start\":36012},{\"end\":36436,\"start\":36375},{\"end\":36856,\"start\":36791},{\"end\":37226,\"start\":37170},{\"end\":37905,\"start\":37848},{\"end\":38347,\"start\":38264},{\"end\":39465,\"start\":39390},{\"end\":40098,\"start\":40033},{\"end\":41102,\"start\":41035},{\"end\":41516,\"start\":41440},{\"end\":41864,\"start\":41784},{\"end\":42245,\"start\":42188},{\"end\":42622,\"start\":42562},{\"end\":43035,\"start\":42960},{\"end\":43353,\"start\":43320},{\"end\":43743,\"start\":43689},{\"end\":44127,\"start\":44083},{\"end\":44465,\"start\":44381},{\"end\":44821,\"start\":44753},{\"end\":45216,\"start\":45145},{\"end\":45621,\"start\":45533},{\"end\":45991,\"start\":45935},{\"end\":46400,\"start\":46336},{\"end\":46680,\"start\":46616},{\"end\":47183,\"start\":47152},{\"end\":47605,\"start\":47504},{\"end\":48058,\"start\":47998},{\"end\":48493,\"start\":48422},{\"end\":48999,\"start\":48895},{\"end\":49384,\"start\":49306},{\"end\":49751,\"start\":49652},{\"end\":50115,\"start\":50068},{\"end\":50562,\"start\":50513},{\"end\":51029,\"start\":50958},{\"end\":51406,\"start\":51368},{\"end\":51765,\"start\":51704},{\"end\":52203,\"start\":52159},{\"end\":52511,\"start\":52479},{\"end\":52875,\"start\":52812},{\"end\":53286,\"start\":53230},{\"end\":53791,\"start\":53728},{\"end\":54247,\"start\":54199},{\"end\":54651,\"start\":54582},{\"end\":55085,\"start\":55039},{\"end\":55392,\"start\":55351},{\"end\":55678,\"start\":55621},{\"end\":56088,\"start\":56031},{\"end\":56496,\"start\":56402},{\"end\":57029,\"start\":56966},{\"end\":57429,\"start\":57357},{\"end\":57825,\"start\":57729},{\"end\":58249,\"start\":58192},{\"end\":58648,\"start\":58624},{\"end\":59000,\"start\":58960}]", "bib_author": "[{\"end\":34743,\"start\":34733},{\"end\":34752,\"start\":34743},{\"end\":34763,\"start\":34752},{\"end\":34772,\"start\":34763},{\"end\":34784,\"start\":34772},{\"end\":34795,\"start\":34784},{\"end\":34807,\"start\":34795},{\"end\":35187,\"start\":35177},{\"end\":35198,\"start\":35187},{\"end\":35209,\"start\":35198},{\"end\":35220,\"start\":35209},{\"end\":35230,\"start\":35220},{\"end\":35243,\"start\":35230},{\"end\":35251,\"start\":35243},{\"end\":35642,\"start\":35629},{\"end\":35648,\"start\":35642},{\"end\":35659,\"start\":35648},{\"end\":35671,\"start\":35659},{\"end\":36103,\"start\":36095},{\"end\":36109,\"start\":36103},{\"end\":36117,\"start\":36109},{\"end\":36126,\"start\":36117},{\"end\":36445,\"start\":36438},{\"end\":36456,\"start\":36445},{\"end\":36465,\"start\":36456},{\"end\":36475,\"start\":36465},{\"end\":36489,\"start\":36475},{\"end\":36500,\"start\":36489},{\"end\":36866,\"start\":36858},{\"end\":36873,\"start\":36866},{\"end\":36881,\"start\":36873},{\"end\":36890,\"start\":36881},{\"end\":37237,\"start\":37228},{\"end\":37248,\"start\":37237},{\"end\":37255,\"start\":37248},{\"end\":37261,\"start\":37255},{\"end\":37270,\"start\":37261},{\"end\":37608,\"start\":37599},{\"end\":37616,\"start\":37608},{\"end\":37623,\"start\":37616},{\"end\":37630,\"start\":37623},{\"end\":37641,\"start\":37630},{\"end\":37650,\"start\":37641},{\"end\":37916,\"start\":37907},{\"end\":37924,\"start\":37916},{\"end\":37935,\"start\":37924},{\"end\":37943,\"start\":37935},{\"end\":38358,\"start\":38349},{\"end\":38366,\"start\":38358},{\"end\":38375,\"start\":38366},{\"end\":38383,\"start\":38375},{\"end\":38391,\"start\":38383},{\"end\":38656,\"start\":38650},{\"end\":38665,\"start\":38656},{\"end\":38673,\"start\":38665},{\"end\":38681,\"start\":38673},{\"end\":38689,\"start\":38681},{\"end\":38698,\"start\":38689},{\"end\":39052,\"start\":39045},{\"end\":39060,\"start\":39052},{\"end\":39066,\"start\":39060},{\"end\":39072,\"start\":39066},{\"end\":39081,\"start\":39072},{\"end\":39088,\"start\":39081},{\"end\":39474,\"start\":39467},{\"end\":39482,\"start\":39474},{\"end\":39489,\"start\":39482},{\"end\":39675,\"start\":39665},{\"end\":39686,\"start\":39675},{\"end\":39697,\"start\":39686},{\"end\":39709,\"start\":39697},{\"end\":39722,\"start\":39709},{\"end\":39735,\"start\":39722},{\"end\":40107,\"start\":40100},{\"end\":40118,\"start\":40107},{\"end\":40124,\"start\":40118},{\"end\":40133,\"start\":40124},{\"end\":40139,\"start\":40133},{\"end\":40151,\"start\":40139},{\"end\":40596,\"start\":40590},{\"end\":40604,\"start\":40596},{\"end\":40611,\"start\":40604},{\"end\":40619,\"start\":40611},{\"end\":40626,\"start\":40619},{\"end\":40634,\"start\":40626},{\"end\":40645,\"start\":40634},{\"end\":40656,\"start\":40645},{\"end\":41112,\"start\":41104},{\"end\":41120,\"start\":41112},{\"end\":41130,\"start\":41120},{\"end\":41525,\"start\":41518},{\"end\":41534,\"start\":41525},{\"end\":41544,\"start\":41534},{\"end\":41876,\"start\":41866},{\"end\":41889,\"start\":41876},{\"end\":42255,\"start\":42247},{\"end\":42267,\"start\":42255},{\"end\":42276,\"start\":42267},{\"end\":42286,\"start\":42276},{\"end\":42301,\"start\":42286},{\"end\":42308,\"start\":42301},{\"end\":42631,\"start\":42624},{\"end\":42641,\"start\":42631},{\"end\":42649,\"start\":42641},{\"end\":43043,\"start\":43037},{\"end\":43051,\"start\":43043},{\"end\":43059,\"start\":43051},{\"end\":43065,\"start\":43059},{\"end\":43074,\"start\":43065},{\"end\":43361,\"start\":43355},{\"end\":43367,\"start\":43361},{\"end\":43373,\"start\":43367},{\"end\":43381,\"start\":43373},{\"end\":43387,\"start\":43381},{\"end\":43751,\"start\":43745},{\"end\":43757,\"start\":43751},{\"end\":43766,\"start\":43757},{\"end\":43774,\"start\":43766},{\"end\":43784,\"start\":43774},{\"end\":44135,\"start\":44129},{\"end\":44141,\"start\":44135},{\"end\":44148,\"start\":44141},{\"end\":44154,\"start\":44148},{\"end\":44160,\"start\":44154},{\"end\":44168,\"start\":44160},{\"end\":44475,\"start\":44467},{\"end\":44482,\"start\":44475},{\"end\":44491,\"start\":44482},{\"end\":44499,\"start\":44491},{\"end\":44505,\"start\":44499},{\"end\":44832,\"start\":44823},{\"end\":44839,\"start\":44832},{\"end\":44850,\"start\":44839},{\"end\":44858,\"start\":44850},{\"end\":44867,\"start\":44858},{\"end\":45225,\"start\":45218},{\"end\":45232,\"start\":45225},{\"end\":45630,\"start\":45623},{\"end\":45638,\"start\":45630},{\"end\":45647,\"start\":45638},{\"end\":45654,\"start\":45647},{\"end\":45662,\"start\":45654},{\"end\":46000,\"start\":45993},{\"end\":46006,\"start\":46000},{\"end\":46016,\"start\":46006},{\"end\":46023,\"start\":46016},{\"end\":46031,\"start\":46023},{\"end\":46409,\"start\":46402},{\"end\":46418,\"start\":46409},{\"end\":46688,\"start\":46682},{\"end\":46696,\"start\":46688},{\"end\":46704,\"start\":46696},{\"end\":46712,\"start\":46704},{\"end\":46720,\"start\":46712},{\"end\":46728,\"start\":46720},{\"end\":46734,\"start\":46728},{\"end\":46742,\"start\":46734},{\"end\":46750,\"start\":46742},{\"end\":47197,\"start\":47185},{\"end\":47213,\"start\":47197},{\"end\":47220,\"start\":47213},{\"end\":47613,\"start\":47607},{\"end\":47620,\"start\":47613},{\"end\":47631,\"start\":47620},{\"end\":47637,\"start\":47631},{\"end\":47649,\"start\":47637},{\"end\":47661,\"start\":47649},{\"end\":47667,\"start\":47661},{\"end\":48068,\"start\":48060},{\"end\":48076,\"start\":48068},{\"end\":48085,\"start\":48076},{\"end\":48091,\"start\":48085},{\"end\":48506,\"start\":48495},{\"end\":48520,\"start\":48506},{\"end\":48530,\"start\":48520},{\"end\":48541,\"start\":48530},{\"end\":49009,\"start\":49001},{\"end\":49017,\"start\":49009},{\"end\":49026,\"start\":49017},{\"end\":49032,\"start\":49026},{\"end\":49394,\"start\":49386},{\"end\":49400,\"start\":49394},{\"end\":49406,\"start\":49400},{\"end\":49418,\"start\":49406},{\"end\":49761,\"start\":49753},{\"end\":49772,\"start\":49761},{\"end\":49778,\"start\":49772},{\"end\":49788,\"start\":49778},{\"end\":49798,\"start\":49788},{\"end\":50124,\"start\":50117},{\"end\":50133,\"start\":50124},{\"end\":50142,\"start\":50133},{\"end\":50149,\"start\":50142},{\"end\":50160,\"start\":50149},{\"end\":50173,\"start\":50160},{\"end\":50579,\"start\":50564},{\"end\":50587,\"start\":50579},{\"end\":50593,\"start\":50587},{\"end\":50603,\"start\":50593},{\"end\":50615,\"start\":50603},{\"end\":50621,\"start\":50615},{\"end\":50630,\"start\":50621},{\"end\":50642,\"start\":50630},{\"end\":50652,\"start\":50642},{\"end\":50665,\"start\":50652},{\"end\":51038,\"start\":51031},{\"end\":51046,\"start\":51038},{\"end\":51052,\"start\":51046},{\"end\":51418,\"start\":51408},{\"end\":51429,\"start\":51418},{\"end\":51436,\"start\":51429},{\"end\":51776,\"start\":51767},{\"end\":51784,\"start\":51776},{\"end\":51793,\"start\":51784},{\"end\":51800,\"start\":51793},{\"end\":51809,\"start\":51800},{\"end\":52214,\"start\":52205},{\"end\":52222,\"start\":52214},{\"end\":52231,\"start\":52222},{\"end\":52238,\"start\":52231},{\"end\":52247,\"start\":52238},{\"end\":52524,\"start\":52513},{\"end\":52535,\"start\":52524},{\"end\":52546,\"start\":52535},{\"end\":52885,\"start\":52877},{\"end\":52891,\"start\":52885},{\"end\":52899,\"start\":52891},{\"end\":52910,\"start\":52899},{\"end\":53303,\"start\":53288},{\"end\":53317,\"start\":53303},{\"end\":53333,\"start\":53317},{\"end\":53346,\"start\":53333},{\"end\":53353,\"start\":53346},{\"end\":53365,\"start\":53353},{\"end\":53801,\"start\":53793},{\"end\":53807,\"start\":53801},{\"end\":53815,\"start\":53807},{\"end\":53823,\"start\":53815},{\"end\":53831,\"start\":53823},{\"end\":53838,\"start\":53831},{\"end\":53846,\"start\":53838},{\"end\":54257,\"start\":54249},{\"end\":54265,\"start\":54257},{\"end\":54271,\"start\":54265},{\"end\":54277,\"start\":54271},{\"end\":54286,\"start\":54277},{\"end\":54661,\"start\":54653},{\"end\":54668,\"start\":54661},{\"end\":54674,\"start\":54668},{\"end\":54682,\"start\":54674},{\"end\":54690,\"start\":54682},{\"end\":54698,\"start\":54690},{\"end\":55095,\"start\":55087},{\"end\":55102,\"start\":55095},{\"end\":55109,\"start\":55102},{\"end\":55120,\"start\":55109},{\"end\":55135,\"start\":55120},{\"end\":55148,\"start\":55135},{\"end\":55401,\"start\":55394},{\"end\":55408,\"start\":55401},{\"end\":55415,\"start\":55408},{\"end\":55422,\"start\":55415},{\"end\":55686,\"start\":55680},{\"end\":55692,\"start\":55686},{\"end\":55701,\"start\":55692},{\"end\":56098,\"start\":56090},{\"end\":56105,\"start\":56098},{\"end\":56112,\"start\":56105},{\"end\":56121,\"start\":56112},{\"end\":56128,\"start\":56121},{\"end\":56507,\"start\":56498},{\"end\":56516,\"start\":56507},{\"end\":56523,\"start\":56516},{\"end\":56532,\"start\":56523},{\"end\":56543,\"start\":56532},{\"end\":56552,\"start\":56543},{\"end\":56562,\"start\":56552},{\"end\":57040,\"start\":57031},{\"end\":57047,\"start\":57040},{\"end\":57056,\"start\":57047},{\"end\":57065,\"start\":57056},{\"end\":57073,\"start\":57065},{\"end\":57440,\"start\":57431},{\"end\":57449,\"start\":57440},{\"end\":57456,\"start\":57449},{\"end\":57462,\"start\":57456},{\"end\":57470,\"start\":57462},{\"end\":57476,\"start\":57470},{\"end\":57836,\"start\":57827},{\"end\":57845,\"start\":57836},{\"end\":57856,\"start\":57845},{\"end\":58259,\"start\":58251},{\"end\":58268,\"start\":58259},{\"end\":58277,\"start\":58268},{\"end\":58284,\"start\":58277},{\"end\":58292,\"start\":58284},{\"end\":58303,\"start\":58292},{\"end\":58659,\"start\":58650},{\"end\":58667,\"start\":58659},{\"end\":58675,\"start\":58667},{\"end\":58681,\"start\":58675},{\"end\":58688,\"start\":58681},{\"end\":59010,\"start\":59002},{\"end\":59019,\"start\":59010},{\"end\":59030,\"start\":59019},{\"end\":59038,\"start\":59030},{\"end\":59046,\"start\":59038}]", "bib_venue": "[{\"end\":34869,\"start\":34807},{\"end\":35303,\"start\":35251},{\"end\":35733,\"start\":35671},{\"end\":36168,\"start\":36126},{\"end\":36562,\"start\":36500},{\"end\":36951,\"start\":36914},{\"end\":37332,\"start\":37270},{\"end\":37597,\"start\":37534},{\"end\":38028,\"start\":37966},{\"end\":38429,\"start\":38391},{\"end\":38774,\"start\":38714},{\"end\":39043,\"start\":38952},{\"end\":39508,\"start\":39489},{\"end\":39822,\"start\":39751},{\"end\":40213,\"start\":40151},{\"end\":40588,\"start\":40509},{\"end\":40910,\"start\":40848},{\"end\":41209,\"start\":41147},{\"end\":41592,\"start\":41544},{\"end\":41961,\"start\":41909},{\"end\":42346,\"start\":42308},{\"end\":42734,\"start\":42672},{\"end\":43112,\"start\":43074},{\"end\":43475,\"start\":43413},{\"end\":43857,\"start\":43805},{\"end\":44217,\"start\":44168},{\"end\":44547,\"start\":44505},{\"end\":44929,\"start\":44867},{\"end\":45314,\"start\":45252},{\"end\":45714,\"start\":45662},{\"end\":46112,\"start\":46056},{\"end\":46456,\"start\":46418},{\"end\":46851,\"start\":46779},{\"end\":47302,\"start\":47240},{\"end\":47729,\"start\":47667},{\"end\":48181,\"start\":48119},{\"end\":48603,\"start\":48541},{\"end\":49074,\"start\":49032},{\"end\":49467,\"start\":49418},{\"end\":49847,\"start\":49798},{\"end\":50235,\"start\":50173},{\"end\":50705,\"start\":50665},{\"end\":51137,\"start\":51075},{\"end\":51509,\"start\":51457},{\"end\":51871,\"start\":51809},{\"end\":52299,\"start\":52247},{\"end\":52598,\"start\":52546},{\"end\":52993,\"start\":52931},{\"end\":53427,\"start\":53365},{\"end\":53908,\"start\":53846},{\"end\":54361,\"start\":54309},{\"end\":54786,\"start\":54724},{\"end\":55176,\"start\":55148},{\"end\":55460,\"start\":55422},{\"end\":55798,\"start\":55724},{\"end\":56189,\"start\":56152},{\"end\":56652,\"start\":56590},{\"end\":57134,\"start\":57097},{\"end\":57514,\"start\":57476},{\"end\":57908,\"start\":57856},{\"end\":58378,\"start\":58326},{\"end\":58763,\"start\":58711},{\"end\":59072,\"start\":59046}]"}}}, "year": 2023, "month": 12, "day": 17}