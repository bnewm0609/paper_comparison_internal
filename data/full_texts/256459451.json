{"id": 256459451, "updated": "2023-11-22 18:55:23.279", "metadata": {"title": "In-Context Retrieval-Augmented Language Models", "authors": "[{\"first\":\"Ori\",\"last\":\"Ram\",\"middle\":[]},{\"first\":\"Yoav\",\"last\":\"Levine\",\"middle\":[]},{\"first\":\"Itay\",\"last\":\"Dalmedigos\",\"middle\":[]},{\"first\":\"Dor\",\"last\":\"Muhlgay\",\"middle\":[]},{\"first\":\"Amnon\",\"last\":\"Shashua\",\"middle\":[]},{\"first\":\"Kevin\",\"last\":\"Leyton-Brown\",\"middle\":[]},{\"first\":\"Yoav\",\"last\":\"Shoham\",\"middle\":[]}]", "venue": "Transactions of the Association for Computational Linguistics", "journal": "Transactions of the Association for Computational Linguistics", "publication_date": {"year": 2023, "month": 11, "day": 1}, "abstract": "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2302-00083", "doi": "10.1162/tacl_a_00605"}}, "content": {"source": {"pdf_hash": "c07ef4e4415e0263a66086affc0521967a070cc4", "pdf_src": "MIT", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.00083v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1cd7ec40063f067cfd72155033dadc6799ad337d", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/c07ef4e4415e0263a66086affc0521967a070cc4.txt", "contents": "\nIn-Context Retrieval-Augmented Language Models\n\n\nOri Ram orir@ai21.com \nYoav Levine yoavl@ai21.com \nItay Dalmedigos itayd@ai21.com \nDor Muhlgay dorm@ai21.com \nAmnon Shashua amnons@ai21.com \nKevin Leyton-Brown kevinlb@ai21.com \nYoav Shoham yoavs@ai21.com \nIn-Context Retrieval-Augmented Language Models\n1681AEC5BF179DF0966D30D0B3E93E3E10.1162/tacl\nRetrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance.In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism.Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment.This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM.We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora.We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance.We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access. 1 * Equal contribution.\n\nIntroduction\n\nRecent advances in language models (LMs) have dramatically increased the usefulness of machine-generated text across a wide range of use-cases and domains (Brown et al., 2020).However, the mainstream paradigm of generating text with LMs bears inherent limitations in access to external knowledge.First, LMs are not coupled with any source attribution, and must be trained in order to incorporate up-to-date information that was not seen during training.More importantly, they tend to produce factual inaccuracies and errors (Lin et al., 2022;Maynez et al., 2020;Huang et al., 2020).This problem is present in any LM generation scenario, and is exacerbated when generation is made in uncommon domains or private data.A promising approach for addressing the above is Retrieval-Augmented Language Modeling (RALM), grounding the LM during generation by conditioning on relevant documents retrieved from an external knowledge source.RALM systems include two high level components: (i) document selection, selecting the set of documents upon which to condition; and (ii) document reading, determining how to incorporate the selected documents into the LM generation process.\n\nLeading RALM systems introduced recently tend to be focused on altering the language model architecture (Khandelwal et al., 2020;Borgeaud et al., 2022;Zhong et al., 2022;Levine et al., 2022c;Li et al., 2022).Notably, Borgeaud et al. (2022) introduced RETRO, featuring document reading via nontrivial modifications that require further training to the LM architecture, while using an off-the-shelf frozen BERT retriever for document selection.Although the paper's experimental findings showed impressive performance gains, the need for changes in architecture and dedicated retraining has hindered the wide adoption of such models.\n\nIn this paper, we show that a very simple document reading mechanism can have a large impact, and that substantial gains can also be made by adapting the document selection mechanism to the task of language modeling.Thus, we show that many of the benefits of RALM can be achieved while working with off-the-shelf LMs, even via API access.Specifically, we consider a simple but powerful RALM framework, dubbed In-Context RALM (presented in Section 3), which employs a zero-effort document reading mechanism: We simply prepend the selected documents to the LM's input text (Figure 1).\n\nSection 4 describes our experimental setup.To show the wide applicability of our framework, we performed LM experiments on a suite of five diverse corpora: WikiText-103 (Merity et al., 2016), RealNews (Zellers et al., 2019), and three datasets from The Pile (Gao et al., 2021): ArXiv, Stack Exchange, and FreeLaw.We use open-source LMs ranging from 110M to 66B parameters (from the GPT-2, GPT-Neo, OPT, and LLaMA model families).\n\nIn Section 5 we evaluate the application of off-the-shelf retrievers to our framework.In this minimal-effort setting, we found that In-Context RALM led to LM performance gains equivalent to increasing the LM's number of parameters by 2-3\u00d7 across all of the text corpora we examined.In Section 6 we investigate methods for adapting document ranking to the LM task, a relatively under-explored RALM degree of freedom.Our adaptation methods range from using a small LM to perform zero-shot ranking of the retrieved documents, up to training a dedicated bidirectional reranker by employing self-supervision from the LM signal.These methods lead to further gains in the LM task corresponding to an additional size increase of 2\u00d7 in the LM architecture.As a concrete example of the gains, a 345M parameter GPT-2 enhanced by In-Context RALM outperforms a 762M parameter GPT-2 when employing an off-the-shelf BM25 retriever (Robertson and Zaragoza, 2009), and outperforms a 1.5B parameter GPT-2 when employing our trained LM-oriented reranker (see Figure 2).For large model sizes, our method is even more effective: In-Context RALM with an off-the-shelf retriever improved the performance of a 6.7B parameter OPT model to match that of a 66B parameter parameter OPT model (see Figure 4).(Robertson and Zaragoza, 2009) to the LM task ( \u00a75) yields significant gains, and choosing the grounding documents via our new class of Predictive Rerankers ( \u00a76) provides a further boost.See Table 1 for the full results on five diverse corpora.\n\nIn Section 7 we demonstrate the applicability of In-Context RALM to downstream open-domain questions answering (ODQA) tasks.\n\nIn a concurrent work, Shi et al. (2023) also suggest to augment off-the-shelf LMs with retrieved texts by prepending them to the input.Their results are based on training a dedicated retriever for language modeling.In contrast, we focus on the gains achievable in using off-the-shelf retrievers for this task.We show strong gains of this simpler setting by investigating: (1) which off-the-shelf retriever is best suited for language modeling, (2) the frequency of retrieval operations, and (3) the optimal query length.In addition, we boost the off-the-shelf retrieval performance by introducing two reranking methods that demonstrate further gains in perplexity.\n\nWe believe that In-Context RALM can play two important roles in making RALM systems more powerful and more prevalent.First, given its simple reading mechanism, In-Context RALM can serve as a clean probe for developing document retrieval methods that are specialized for the LM task.These in turn can be used to improve both In-Context RALM and other more elaborate RALM methods that currently leverage general purpose retrievers.Second, due to its compatibility with off-the-shelf LMs, In-Context RALM can help drive wider deployment of RALM systems.\n\n\nRelated Work\n\nRALM approaches can be roughly divided into two families of models: (i) nearest-neighbor language models (also called kNN-LM), and (ii) retrieve and read models.Our work belongs to the second family, but is distinct in that it involves no further training of the LM.\n\nNearest Neighbor Language Models The kNN-LM approach was first introduced in Khandelwal et al. (2020).The authors suggest a simple inference-time model that interpolates between two next-token distributions: one induced by the LM itself, and one induced by the k neighbors from the retrieval corpus that are closest to the query token in the LM embedding space.Zhong et al. (2022) suggest a framework for training these models.While they showed significant gains from kNN-LM, the approach requires storing the representations for each token in the corpus, an expensive requirement even for a small corpus like Wikipedia.Although numerous approaches have been suggested for alleviating this issue (He et al., 2021;Alon et al., 2022), scaling any of them to large corpora remains an open challenge.\n\nRetrieve and Read Models This family of RALMs creates a clear division between document selection and document reading components.All prior work involves training the LM.We begin by describing works that use this approach for tackling downstream tasks, and then mention works oriented towards RALM.Lewis et al. (2020) and Izacard and Grave (2021) fine tuned encoder-decoder architectures for downstream knowledge-intensive tasks.Izacard et al. (2022b) explored different ways of pretraining such models, while Levine et al. (2022c) pretrained an autoregressive LM on clusters of nearest neighbors in sentence embedding space.Levine et al. (2022a) showed competitive open domain question-answering performance by prompt-tuning a frozen LM as a reader.Guu et al. (2020) pretrained REALM, a retrieval augmented bidirectional, masked LM, later fine-tuned for open-domain question answering.The work closest to this paper-with a focus on the language modeling task-is RETRO (Borgeaud et al., 2022), which modifies an autoregressive LM to attend to relevant documents via chunked cross-attention, thus introducing new parameters to the model.Our In-Context RALM differs from prior work in this family of models in two key aspects:\n\n\u2022 We use off-the-shelf LMs for document reading without any further training of the LM.\n\n\u2022 We focus on how to choose documents for improved LM performance.\n\n3 Our Framework\n\n\nIn-Context RALM\n\nLanguage models define probability distributions over sequences of tokens.Given such a sequence x 1 , . . . ,x n , the standard way to model its probability is via next-token prediction: p(x 1 , . . . ,x n ) = n i=1 p(x i |x <i ), where x <i := x 1 , . . ., x i\u22121 is the sequence of tokens preceding x i , also referred to as its prefix.This autoregressive model is usually implemented via a learned transformer network (Vaswani et al., 2017) parameterized by the set of parameters \u03b8:\np(x 1 , . . . , x n ) = n i=1 p \u03b8 (x i |x <i ),(1)\nwhere the conditional probabilities are modeled by employing a causal self-attention mask (Radford et al., 2018).Notably, leading LMs such as GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022), or Jurassic-1 (Lieber et al., 2021) follow this simple parameterization.Retrieval augmented language models (RALMs) add an operation that retrieves one or more documents from an external corpus C, and condition the above LM predictions on these documents.Specifically, for predicting x i , the retrieval operation from C depends on its prefix: R C (x <i ), so the most general RALM decomposition is:\np(x 1 , . . . , x n ) = n i=1 p(x i |x <i , R C (x <i )).\nIn order to condition the LM generation on the retrieved document, previous RALM approaches used specialized architectures or algorithms (see \u00a72).Inspired by the success of In-Context Learning (Brown et al., 2020;Dong et al., 2023), In-Context RALM refers to the following specific, simple method of concatenating the retrieved documents2 within the Transformer's input prior to the prefix (see Figure 1), which does not involve altering the LM weights \u03b8:\np(x 1 , . . . , x n ) = n i=1 p \u03b8 (x i | [R C (x <i ); x <i ]) , (2)\nwhere [a; b] denotes the concatenation of strings a and b.\n\nSince common Transformer-based LM implementations support limited length input sequences, when the concatenation of the document and the input sequence exceed this limit we remove tokens from the beginning of x until the overall input length equals that allowed by the model.Because our retrieved documents are passages of limited length, we always have enough context left from x (see \u00a74.3).\n\n\nRALM Design Choices\n\nWe detail below two practical design choices often made in RALM systems.In \u00a75, we investigate the effect of these in the setting of In-Context RALM.\n\nRetrieval Stride While in the above formulation a retrieval operation can occur at each generation step, we might want to perform retrieval only once every s > 1 tokens due to the cost of calling the retriever, and the need to replace the documents in the LM prefix during generation.We refer to s as the retrieval stride.This gives rise to the following In-Context RALM formulation (which reduces back to Eq. ( 2) for s = 1):\np(x 1 , . . . , x n ) = n s \u22121 j=0 s i=1 p \u03b8 x s\u2022j+i | R C (x \u2264s\u2022j ); x <(s\u2022j+i) ,\n(3) where n s = n/s is the number of retrieval strides.\n\nNotably, in this framework the runtime costs of each retrieval operation is composed of (a) applying the retriever itself, and (b) recomputing the embeddings of the prefix.In \u00a75.2 we show that using smaller retrieval strides, i.e., retrieving as often as possible, is superior to using larger ones (though In-Context RALM with larger strides already provides large gains over vanilla LM).Thus, choosing the retrieval stride is ultimately a tradeoff between runtime and performance.\n\nRetrieval Query Length While the retrieval query above in principle depends on all prefix tokens x \u2264s\u2022j , the information at the very end of the prefix is typically the most relevant to the generated tokens.If the retrieval query is too long then this information can be diluted.To avoid this, we restrict the retrieval query at stride j to the last tokens of the prefix, i.e., we use q s, j := x s\u2022j\u2212 +1 , . . ., x s\u2022j .We refer to as the retrieval query length.Note that prior RALM work couples the retrieval stride s and the retrieval query length (Borgeaud et al., 2022).In \u00a75, we show that enforcing s = degrades LM performance.Integrating these hyper-parameters into the In-Context RALM formulation gives\np(x 1 , . . . , x n ) = n s \u22121 j=0 s i=1 p \u03b8 x s\u2022j+i | R C (q s, j ); x <(s\u2022j+i) .\n(4)\n\n\nExperimental Details\n\nWe now describe our experimental setup, including all models we use and their implementation details.\n\n\nDatasets\n\nWe evaluated the effectiveness of In-Context RALM across five diverse language modeling datasets and two common open-domain question answering datasets.\n\n\nLanguage Modeling\n\nThe first LM dataset is WikiText-103 (Merity et al., 2016), which has been extensively used to evaluate RALMs (Khandelwal et al., 2020;He et al., 2021;Borgeaud et al., 2022;Alon et al., 2022;Zhong et al., 2022).Second, we chose three datasets spanning diverse subjects from The Pile (Gao et al., 2021): ArXiv, Stack Exchange, and FreeLaw.Finally, we also investigated RealNews (Zellers et al., 2019), since The Pile lacks a corpus focused only on news (which is by nature a knowledge-intensive domain).\n\nOpen-Domain Question Answering In order to evaluate In-Context RALM on downstream tasks as well, we use the Natural Questions (NQ; Kwiatkowski et al. 2019) and TriviaQA (Joshi et al., 2017) open-domain question answering datasets.\n\n\nModels\n\nLanguage Models We performed our experiments using the four models of GPT-2 (110M-1.5B;Radford et al., 2019), three models of GPT-Neo and GPT-J (1.3B-6B;We elected to study these particular models for the following reasons.The first four (GPT-2) models were trained on WebText (Radford et al., 2019), with Wikipedia documents excluded from their training datasets.We were thus able to evaluate our method's ''zero-shot'' performance when retrieving from a novel corpus (for WikiText-103).The rest of the models brought two further benefits.First, they allowed us to investigate how our methods scale to models larger than GPT-2.Second, the fact that Wikipedia was part of their training data allowed us to investigate the usefulness of In-Context RALM for corpora seen during training.The helpfulness of such retrieval has been demonstrated for previous RALM methods (Khandelwal et al., 2020) and has also been justified theoretically by Levine et al. (2022c).\n\nWe ran all models with a maximum sequence length of 1,024, even though GPT-Neo, OPT, and LLaMA models support a sequence length of 2,048. 4  Retrievers We experimented with both sparse (word-based) and dense (neural) retrievers.We used BM25 (Robertson and Zaragoza, 2009) as our sparse model.For dense models, we experimented with (i) a frozen BERT-base (Devlin et al., 2019) followed by mean pooling, similar to Borgeaud et al. (2022); and (ii) the Contriever (Izacard et al., 2022a) and Spider (Ram et al., 2022) models, which are dense retrievers that were trained in unsupervised manners.\n\n\nImplementation Details\n\nWe implemented our code base using the Transformers library (Wolf et al., 2020).We based our dense retrieval code on the DPR repository (Karpukhin et al., 2020).\n\nRetrieval Corpora For WikiText-103 and ODQA datasets, we used the Wikipedia corpus from Dec. 20, 2018, standardized by Karpukhin et al. (2020) using the preprocessing from Chen et al. (2017).To avoid contamination, we found and removed all 120 articles of the development and test set of WikiText-103 from the corpus.For the remaining datasets, we used their training data as the retrieval corpus.Similar to Karpukhin et al. (2020), our retrieval corpora consist of non-overlapping passages of 100 words (which translate to less than 150 tokens for the vast majority of passages).Thus, we truncate our retrieved passages at 256 tokens when input to the models, but they are usually much smaller.\n\nRetrieval For sparse retrieval, we used the Pyserini library (Lin et al., 2021).For dense retrieval, we applied exact search using FAISS (Johnson et al., 2021).\n\n\nThe Effectiveness of In-Context RALM\n\nwith Off-the-Shelf Retrievers\n\nWe now empirically show that despite its simple document reading mechanism, In-Context RALM leads to substantial LM gains across our diverse evaluation suite.We begin in this section by investigating the effectiveness of off-the-shelf retrievers for In-Context RALM; we go on in \u00a76 to show that further LM gains can be made by tailoring document ranking functions to the LM task.The experiments in this section provided us with a recommended configuration for applying In-Context RALM: applying a sparse BM25 retriever that receives = 32 query tokens and is applied as frequently as possible.Practically, we retrieve every s = 4 tokens ( and s are defined in \u00a73).Table 1 shows for the GPT-2 models that across all the examined corpora, employing In-Context RALM with an off-the-shelf retriever improved LM perplexity to a sufficient extent that it matched that of a 2-3\u00d7 larger model.Figure 4 and Tables 2 and 5 show that this trend holds across model sizes up to 66B parameters, for both WikiText-103 and RealNews.For each LM, we report: (a) its performance without retrieval, (b) its performance when fed the top-scored passage by BM25 ( \u00a75), and (c) its performance when applied on the top-scored passage of each of our two suggested rerankers ( \u00a76).All models share the same vocabulary, thus token-level perplexity (token ppl) numbers are comparable.For WikiText we follow prior work and report word-level perplexity (word ppl).\n\n\nModel Retrieval WikiText-103 word ppl\n\nLLaMA-7B -9 .9 BM25, \u00a75\n\n8 .8\n\nLLaMA-13B -8 .5 BM25, \u00a75 7 .6\n\nLLaMA-33B -6 .3 BM25, \u00a75\n\n6 . 1\n\nTable 2: The performance of models from the LLaMA family, measured by word-level perplexity on the test set of WikiText-103.\n\n\nBM25 Outperforms Off-the-Shelf Neural Retrievers in Language Modeling\n\nWe experimented with different off-the-shelf general purpose retrievers, and found that the sparse (lexical) BM25 retriever (Robertson and Zaragoza, 2009) outperformed three popular dense (neural) retrievers: the self-supervised retrievers Contriever (Izacard et al., 2022a) and Spider (Ram et al., 2022), as well as a retriever based on the average pooling of BERT embeddings that was used in the RETRO system (Borgeaud et al., 2022).(i.e., retrieval is applied every four tokens).For each RALM, we report the result of the best query length (see Figures 6, 9, 10).\n\nWe conducted a minimal hyper-parameter search on the query length for each of the retrievers, and found that = 32 was optimal for BM25 (Figure 6), and = 64 worked best for dense retrievers (Figures 9, 10).Figure 3 compares the performance gains of In-Context RALM with these four general-purpose retrievers.The BM25 retriever clearly outperformed all dense retrievers.This outcome is consistent with prior work showing that BM25 Figure 4: Results of OPT models (Zhang et al., 2022) on the test set of WikiText-103 (word-level perplexity) and the development set of RealNews (token-level perplexity).In-Context RALM models use a BM25 retriever with s = 4 (i.e., the retriever is called every four tokens) and = 32 (i.e., the retriever query is comprised of the last 32 tokens of the prefix).In-Context RALM with an off-the-shelf retriever improved the performance of a 6.7B parameter OPT model to match that of a 66B parameter OPT model.outperforms neural retrievers across a wide array of tasks, when applied in zero-shot settings (Thakur et al., 2021).This result renders In-Context RALM even more appealing since applying a BM25 retriever is significantly cheaper than the neural alternatives.\n\n\nFrequent Retrieval Improves Language Modeling\n\nWe investigated the effect of varying the retrieval stride s (i.e., the number of tokens between consecutive retrieval operations).Figure 5 shows that LM performance improved as the retrieval operation became more frequent.This supports the intuition that retrieved documents become more relevant the closer the retrieval query becomes to the generated tokens.Of course, each retrieval operation imposes a runtime cost.To balance performance and runtime, we used s = 4 in our experiments.For comparison, RETRO employed a retrieval frequency of s = 64 (Borgeaud et al., 2022), which leads to large degradation in perplexity.Intuitively, retrieving with high frequency (low retrieval stride) allows to ground the LM in higher resolution.\n\n\nA Contextualization vs. Recency Tradeoff in Query Length\n\nWe also investigated the effect of varying , the length of the retrieval query for BM25. Figure 6 reveals an interesting tradeoff and a sweet spot around a query length of 32 tokens.Similar experiments for dense retrievers are given in Appendix A. We conjecture that when the retriever query is too short, it does not include enough of the input context, decreasing the retrieved document's relevance.Conversely, excessively growing the retriever query deemphasizes the tokens at the very end of the prefix, diluting the query's relevance to the LM task.6 Improving In-Context RALM with LM-Oriented Reranking\n\nSince In-Context RALM uses a fixed document reading component by definition, it is natural to ask whether performance can be improved by specializing its document retrieval mechanism to the LM task.Indeed, there is considerable scope for improvement: the previous section considered conditioning the model only on the first document retrieved by the BM25 retriever.This permits very limited semantic understanding of the query, since BM25 is based only on the bag of words signal.Moreover, it offers no way to accord different degrees of importance to different retrieval query tokens, such as recognizing that later query tokens are more relevant to the generated text.\n\nIn this section, we focus on choosing which document to present to the model, by reranking the top-k documents returned by the BM25 retriever. 5e use Figure 7 as motivation: It shows the large potential for improvement among the top-16 documents returned by the BM25 retriever.We act upon this motivation by using two rerankers.Specifically, in \u00a76.1 we show performance gains across our evaluation suite obtained by using an LM to perform zero-shot reranking of the top-k BM25 retrieved documents (results in third row for each of the models in Table 1).Then, in \u00a76.2 we show that training a specialized bidirectional reranker of the top-k BM25 retrieved documents in a self-supervised manner via the LM signal can provide further LM gains (results in forth row for each of the models in Table 1).\n\n\nLMs as Zero-Shot Rerankers\n\nFirst, we used off-the-shelf language models as document rerankers for the In-Context RALM setting.Formally, for a query q consisting of the last tokens in the prefix of the LM input x, let {d 1 , . . . ,d k } be the top-k documents returned by BM25.For retrieval iteration j, let the text for generation be y := x s\u2022j+1 , . . ., x s\u2022j+s .Ideally, we would like to find the document d i * that maximizes the probability of the text for generation, i.e.,\ni * = arg max i\u2208[k] p \u03b8 (y| [d i ; x \u2264s\u2022j ]).\n(5)\n\nHowever, at test time we do not have access to the tokens of y.Instead, we used the last prefix tokens (which are available at test time), denoted by y , for reranking.Formally, let s be a hyper-parameter that determines the number of the prefix tokens by which to rerank.We define y := x s\u2022j\u2212s +1 , . . ., x s\u2022j (i.e., the stride of length s that precedes y) and choose the document\nd \u00ee such that \u00ee = arg max i\u2208[k] p \u03c6 (y | d i ; x \u2264(s\u2022j\u2212s ) ).(6)\nThe main motivation is that since BM25 is a lexical retriever, we want to incorporate a semantic signal induced by the LM.Also, this reranking shares conceptual similarities with the reranking framework of Sachan et al. (2022) for opendomain question answering, where y (i.e., the last prefix tokens) can be thought of as their ''question''.GPT-2 1.5B (XL) GPT-2 110M (S) 16.2 9.9 GPT-2 1.5B (XL) 16.1 9.8\n\n\nModel\n\nTable 3: Perplexity for zero-shot reranking ( \u00a76.1) where the reranking models is smaller than the LM, or the LM itself.Reranking is performed on the top 16 documents retrieved by BM25.Using a GPT-2 110M (S) instead of a larger language model as a reranker leads to only a minor degradation.\n\nNote that our zero-shot reranking does not require that the LM used for reranking is the same model as the LM used for generation (i.e., the LM in Eq. ( 6), parameterized by \u03c6, does not need to be the LM in Eq. ( 2), parameterized by \u03b8).This observation unlocks the possibility of reranking with smaller (and thus faster) models, which is important for two main reasons: (i) Reranking k documents requires k forward passes; and (ii) it allows our methods to be used in cases where the actual LM's log probabilities are not available (for example, when the LM is accessed through an API). 6esults A minimal hyper-parameter search on the development set of WikiText-103 revealed that the optimal query length is s = 16,7 so we proceed with this value going forward.Table 1 shows the results of letting the LM perform zero-shot reranking on the top-16 documents retrieved by BM25 (third row for each of the models).It is evident that reranking yielded consistently better results than simply taking the first result returned by the retriever.\n\nTable 3 shows that a small LM (GPT-2 117M) can be used to rerank the documents for all larger GPT-2 models, with roughly the same perforas having each LM perform reranking for itself, supporting the applicability of this method for LMs that are only accessible via an API.\n\n\nTraining LM-dedicated Rerankers\n\nNext, we trained a reranker to choose one of the top-k documents retrieved by BM25.We refer to this approach as Predictive Reranking, since the reranker learns to choose which document will help in ''predicting'' the upcoming text.For this process, we assume availability of training data from the target corpus.Our reranker is a classifier that gets a prefix x \u2264s\u2022j and a document d i (for i \u2208 [k]), and produces a scalar f (x \u2264s\u2022j , d i ) that should resemble the relevance of d i for the continuation of x \u2264s\u2022j .\n\nWe then normalize these relevance scores:\np rank (d i |x \u2264s\u2022j ) = exp(f (x \u2264s\u2022j , d i )) k i =1 exp(f (x \u2264s\u2022j , d i ))\n, (7) and choose the document d \u00ee such that\n\u00ee = arg max i\u2208[k] p rank (d i |x \u2264s\u2022j ). (8)\nCollecting Training Examples To train our predictive reranker, we collected training examples as follows.Let x \u2264s\u2022j be a prefix we sample from the training data, and y := x s\u2022j+1 , . . ., x s\u2022j+s be the text for generation upcoming in its next stride.We run BM25 on the query q s, j derived from x \u2264s\u2022j (see \u00a73.2) and get k documents {d 1 , . . . ,d k }.For each document d i , we then run the LM to compute p \u03b8 (y| [d i ; x \u2264s\u2022j ]) similar to Eq. ( 4).\n\nTraining Our reranker was a fine-tuned RoBERTa-base (Liu et al., 2019) that trained for 10,000 steps with a peak learning rate of 10 \u22125 and a batch size of 32.Overall, we created 300,000 examples from the training set of WikiText-103 as explained above.The loss function we use to train the reranker follows previous work (Guu et al., 2020;Lewis et al., 2020): Note that unlike those works, we train only the reranker (p rank ), keeping the LM weights \u03b8 frozen.\n\u2212 log k i=1 p rank (d i |x \u2264s\u2022j ) \u2022 p \u03b8 (y| [d i ; x \u2264s\u2022j ]). (9)\nResults Table 1 shows the result of our predictive reranker, trained on WikiText-103.Specifically, we trained it with data produced by GPT-2 110M (S), and tested its effectiveness for all GPT-2 models.We observed significant gains obtained from Predictive Reranking.For example, the perplexity of GPT-2 110M (S) improved from 29.6 to 26.8, and that of GPT-2 1.5B (XL) improved from 16.6 to 15.4.This trend held for the other two models as well.Overall, these results demonstrate that training a reranker with domain-specific data was more effective than zero-shot reranking (Section 6.1).Note that these results-while impressive-still leave room for further improvements, compared to the top-16 BM25 oracle results (see Figure 7).Moreover, the oracle results themselves can be improved by retrieving k > 16 documents via a BM25 retriever, or by training stronger retrievers dedicated to the RALM task.We leave this direction for future work.\n\n7 In-Context RALM for Open-Domain Question Answering\n\nSo far, we evaluated our framework on language modeling benchmarks.and Grave, 2021;Fajcik et al., 2021;Izacard et al., 2022b;Levine et al., 2022b), our ''reader'' (i.e., the model that gets the question along with its corresponding retrieved documents, and returns the answer) is simply a frozen large LM: not pretrained, fine-tuned, or prompted to be retrieval-augmented.For the closed-book setting, we utilize the prompt of Touvron et al. (2023).For the open-book setting, we extend this prompt to include retrieved documents (see Appendix C).We use DPR (Karpukhin et al., 2020) as our retriever.\n\nVarying the Number of Documents To investigate the the effect of the number of documents shown to the model, we performed a minimal analysis on the development set of NQ and TriviaQA.Figure 8 demonstrates that showing documents in-context significantly improves the model's performance.In addition, most of the gain can be obtained by using only two documents (or even a single one in some cases).\n\nResults Table 4  This paper presented the framework of In-Context RALM, enabling frozen, off-the-shelf LMs to benefit from retrieval.We demonstrated that substantial performance gains can be achieved by using general purpose retrievers, and showed that additional gains can be achieved by tailoring the document selection to the LM setting.A recent work by Muhlgay et al. (2023) demonstrates that In-Context RALM is indeed able to improve the factuality of large LMs.\n\nSeveral directions for further improvement remain for future work.First, this paper considers only the case of prepending a single external document to the context; adding more documents could drive further gains (for example, using the framework of Ratner et al., 2022).Second, we retrieved documents every fixed interval of s tokens, but see potential for large latency and cost gains by retrieving more sparsely, such as only when a specialized model predicts that retrieval is needed.\n\nWe release the code used in this work, for the community to use and improve over.We hope it will drive further research of RALM, which will enable its wider adoption.\n\n\nA Query Length Ablations\n\nFigure 9 and Figure 10 show ablations on the optimal query length for off-the-shelf dense retrievers (BERT and Contriever, respectively).We omit the results of Spider as they are almost identical to those of Contriever.Consistently, using = 64 (tokens) is optimal.This is in contrast to similar experiments we conducted for BM25 (cf. Figure 6), where = 32 is optimal.\n\nFigure 1 :\n1\nFigure 1: An example of In-Context RALM: We simply prepend the retrieved document before the input prefix.\n\n\nFigure 2 :\n2\nFigure 2: Our framework, dubbed In-Context RALM, provides large language modeling gains on the test set of WikiText-103, without modifying the LM.Adapting the use of a BM25 retriever(Robertson and Zaragoza, 2009) to the LM task ( \u00a75) yields significant gains, and choosing the grounding documents via our new class of Predictive Rerankers ( \u00a76) provides a further boost.See Table1for the full results on five diverse corpora.\n\n\nFigure 3 :\n3\nFigure 3: The performance of four off-the-shelf retrievers used for In-Context RALM on the development set of WikiText-103.All RALMs are run with s = 4(i.e., retrieval is applied every four tokens).For each RALM, we report the result of the best query length (see Figures6, 9, 10).\n\n\nFigure 5 :\n5\nFigure5: An analysis of perplexity as a function of s, the retrieval stride, i.e., the number of tokens between consecutive retrieval operations, on the development set of WikiText-103.Throughout the paper, we use s = 4 to balance perplexity and runtime.\n\n\nFigure 6 :\n6\nFigure 6: An analysis of perplexity as a function of the number of tokens in the query for BM25 on the development set of WikiText-103.In the appendix, we show similar trade-offs for dense retrievers within WikiText-103.Throughout the paper, we use a query length of = 32 tokens.\n\n\nFigure\n\nFigure Potential for gains from reranking.Perplexity improvement (on the development set of WikiText-103) from an oracle that takes the best of the top-16 documents retrieved by BM25 rather than the first.\n\n\nFigure 8 :\n8\nFigure 8: Zero-shot performance of RALM on the development set of Natural Questions and TriviaQA, when varying the number of documents (retrieved by DPR) shown in-context.\n\n\nFigure 9 :\n9\nFigure 9: An analysis of perplexity as a function of the number of tokens in the query for an off-the-shelf BERT retriever on the development set of WikiText-103.\n\n\nFigure 10 :\n10\nFigure 10: An analysis perplexity as a function of the number of tokens in the query for Contriever on the development set of WikiText-103.\n\n\nTable 1 :\n1\nPerplexity on the test set of WikiText-103, RealNews and three datasets from the Pile.\nModelRetrievalRerankingWikiText-103 RealNewsArXivStack Exch. FreeLawword ppltoken ppl token ppltoken ppltoken ppl--37.521.312.012.813.0GPT-2 SBM25  \u00a75 BM25-Zero-shot  \u00a76.129.6 28.616.1 15.510.9 10.111.3 10.69.6 8.8BM25Predictive  \u00a76.226.8------26.315.79.38.89.6GPT-2 MBM25  \u00a75 BM25-Zero-shot  \u00a76.121.5 20.812.4 12.08.6 8.08.1 7.77.4 6.9BM25Predictive  \u00a76.219.7------22.013.68.48.58.7GPT-2 LBM25  \u00a75 BM25-Zero-shot  \u00a76.118.1 17.610.9 10.67.8 7.37.8 7.46.8 6.4BM25Predictive  \u00a76.216.6------20.012.47.88.08.0GPT-2 XLBM25  \u00a75 BM25-Zero-shot  \u00a76.116.6 16.110.1 9.87.2 6.87.4 7.16.4 6.0BM25Predictive  \u00a76.215.4----\n\nTable 5 :\n5\nThe performance of models from the GPT-Neo family, measured by word-level perplexity on the test set of WikiText-103 and token-level perplexity on the development set of RealNews.B GPT-Neo ResultsTable5gives the results of applying In-Context RALM to the models from the GPT-Neo model family on WikiText-103 and RealNews.\nModelRetrievalWiki-103 RealNewsword ppl token pplGPT-Neo 1.3B-BM25,  \u00a7517.5 14.612.3 9.9GPT-Neo 2.7B-BM25,  \u00a7515.1 12.811.0 9.0GPT-J 6B-BM25,  \u00a7511.6 10.09.2 7.7C Open-Domain Question AnsweringExperiments: Further DetailsClosed-Book Setting For the closed-book set-ting, we adopt the prompt of Touvron et al.(2023):Answer these questions:Q: Who got the first nobelprize in physics?A:Open-Book Setting For the open-book setting,we extend the above prompt as follows:Nobel PrizeA group including 42Swedish writers, artists,\nOur code is available at https://github.com /AI21Labs/in-context-ralm.\nWe always use a single document, but it is conceptually simple to support multiple documents as well.\nReranking When training rerankers (Section 6.2), we initialized from RoBERTa-base(Liu et al., 2019).3 All models are available for use use via https:// huggingface.co/.4 In preliminary experiments, we observed similar improvements from In-Context RALM when using a sequence length of 2,048. We used a sequence length of 1,024 in order to facilitate a direct comparison between all models.\nIn both \u00a76.1 and \u00a76.2 we use k = 16.\nNote we do not require that the two models share the same vocabulary.\nWe experimented with s \u2208 {4,\n, 16, 32}.\nAcknowledgmentsWe would like to thank the reviewers and the Action Editor for their valuable feedback.\nNeuro-symbolic language modeling with automaton-augmented retrieval. Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, Graham Neubig, ICML. 2022\n\nGPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow. Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, 10.5281/zenodo.52977152021\n\nImproving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las, Aurelia Casas, Jacob Guy, Roman Menick, Tom Ring, Saffron Hennigan, Loren Huang, Chris Maggiore, Albin Jones, Andy Cassirer, Michela Brock, Geoffrey Paganini, Irving, ICML. Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, Laurent Sifre, 2022\n\nLanguage models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Ilya Sutskever, and Dario Amodei\n\nReading Wikipedia to answer open-domain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, 10.18653/v1/P17-1171Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, Canada20171Association for Computational Linguistics\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui, 10.48550/arXiv.2301.00234A survey on in-context learning. 2023\n\nR2-D2: A modular baseline for open-domain question answering. Martin Fajcik, Martin Docekal, Karel Ondrej, Pavel Smrz, 10.18653/v1/2021.findings-emnlp.73Findings of the Association for Computational Linguistics: EMNLP 2021. Dominican Republic. Association for Computational Linguistics. Punta Cana2021\n\nThe pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, 10.48550/arXiv.2101.000272021\n\nREALM: Retrieval-augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, ICML. 2020\n\nEfficient nearest neighbor language models. Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick, 10.18653/v1/2021.emnlp-main.461Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana\n\nChallenges in building intelligent open-domain dialog systems. Minlie Huang, Xiaoyan Zhu, Jianfeng Gao, 10.1145/3383123ACM Transactions on Information Systems. 3832020\n\nUnsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, Transactions on Machine Learning Research. 2022a\n\nLeveraging passage retrieval with generative models for open domain question answering. Gautier Izacard, Edouard Grave, 10.18653/v1/2021.eacl-main.74Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational Linguistics2021\n\nAtlas: Few-shot learning with retrieval augmented language models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, 10.48550/arXiv.2208.032992022b\n\nBillion-scale similarity search with GPUs. Jeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou, 10.1109/TBDATA.2019.2921572IEEE Transactions on Big Data. 732021\n\nTriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, 10.18653/v1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171Vancouver, Canada. Association for Computational Linguistics\n\nDense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020Online. Association for Computational Linguistics\n\n. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, 2020\n\nGeneralization through memorization: Nearest neighbor language models. International Conference on Learning Representations. \n\nNatural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 72019\n\nStanding on the shoulders of giant frozen language models. Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel Zeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin, Opher Lieber, Barak Lenz, Shai Shalev-Shwartz, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, 10.48550/arXiv.2204.100192022a\n\nHuge frozen language models as readers for open-domain question answering. Yoav Levine, Ori Ram, Daniel Jannai, Barak Lenz, Shai Shalev-Shwartz, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, ICML 2022 Workshop on Knowledge Retrieval and Language Models. 2022b\n\nThe inductive bias of in-context learning: Rethinking pretraining example design. Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua, International Conference on Learning Representations. 2022c\n\nRetrieval-augmented generation for knowledge-intensive NLP tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-Tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela, Advances in Neural Information Processing Systems. 2020\n\nDecoupled context processing for context augmented language modeling. Zonglin Li, Ruiqi Guo, Sanjiv Kumar, Advances in Neural Information Processing Systems. Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2022. 2021Jurassic-1: Technical details and evaluation\n\nPyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, Rodrigo Nogueira, 10.1145/3404835.3463238Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21. the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21New York, NY, USAAssociation for Computing Machinery2021\n\nTruthfulQA: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 10.18653/v1/2022.acl-long.229Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 10.48550/arXiv.1907.11692RoBERTa: A robustly optimized bert pretraining approach. 2019\n\nOn faithfulness and factuality in abstractive summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, 10.18653/v1/2020.acl-main.173Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020\n\nStephen Merity, Caiming Xiong, James Bradbury, Richard Socher, 10.48550/arXiv.1609.07843Pointer sentinel mixture models. 2016\n\nGenerating benchmarks for factuality evaluation of language models. Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, Yoav Shoham, 10.48550/arXiv.2307.069082023\n\nKILT: A benchmark for knowledge intensive language tasks. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, Sebastian Riedel, 10.18653/v1/2021.naacl-main.200Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021\n\nImproving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019\n\nLearning to retrieve passages without supervision. Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, Amir Globerson, 10.18653/v1/2022.naacl-main.193Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022\n\n. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, 10.18653/v1/2023.acl-long.3522022Parallel context windows improve in-context learning of large language models\n\nThe probabilistic relevance framework: BM25 and beyond. Stephen Robertson, Hugo Zaragoza, 10.1561/1500000019Foundations and Trends in Information Retrieval. 342009\n\nImproving passage retrieval with zero-shot question generation. Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-Tau Yih, Joelle Pineau, Luke Zettlemoyer, 10.18653/v1/2022.emnlp-main.249Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022\n\nREPLUG: Retrieval-augmented black-box language models. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen Tau, Yih , 10.48550/arXiv.2301.126522023\n\nBEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, Iryna Gurevych, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks20211\n\nLLaMA: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth\u00e9e Lachaux, Baptiste Lacroix, Naman Rozi\u00e8re, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, 10.48550/arXiv.2302.139712023\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 201730\n\nBen Wang, Aran Komatsuzaki, GPT-J-6B: A 6 billion parameter autoregressive language model. 2021\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020\n\nDefending against neural fake news. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, Advances in Neural Information Processing Systems. Curran Associates, Inc201932\n\nOPT: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 10.48550/arXiv.2205.010682022\n\nTraining language models with memory augmentation. Zexuan Zhong, Tao Lei, Danqi Chen, 10.18653/v1/2022.emnlp-main.382Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational Linguistics\n", "annotations": {"author": "[{\"end\":72,\"start\":50},{\"end\":100,\"start\":73},{\"end\":132,\"start\":101},{\"end\":159,\"start\":133},{\"end\":190,\"start\":160},{\"end\":227,\"start\":191},{\"end\":255,\"start\":228}]", "publisher": null, "author_last_name": "[{\"end\":57,\"start\":54},{\"end\":84,\"start\":78},{\"end\":116,\"start\":106},{\"end\":144,\"start\":137},{\"end\":173,\"start\":166},{\"end\":209,\"start\":197},{\"end\":239,\"start\":233}]", "author_first_name": "[{\"end\":53,\"start\":50},{\"end\":77,\"start\":73},{\"end\":105,\"start\":101},{\"end\":136,\"start\":133},{\"end\":165,\"start\":160},{\"end\":196,\"start\":191},{\"end\":232,\"start\":228}]", "author_affiliation": null, "title": "[{\"end\":47,\"start\":1},{\"end\":302,\"start\":256}]", "venue": null, "abstract": "[{\"end\":1601,\"start\":348}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1792,\"start\":1772},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2159,\"start\":2141},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2179,\"start\":2159},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2198,\"start\":2179},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2916,\"start\":2891},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2938,\"start\":2916},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2957,\"start\":2938},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2978,\"start\":2957},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2994,\"start\":2978},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3026,\"start\":3004},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4193,\"start\":4172},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4226,\"start\":4204},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4279,\"start\":4261},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5380,\"start\":5350},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5743,\"start\":5713},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6125,\"start\":6108},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7688,\"start\":7664},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7967,\"start\":7948},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8300,\"start\":8283},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8318,\"start\":8300},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8702,\"start\":8683},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8731,\"start\":8707},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8836,\"start\":8814},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8916,\"start\":8895},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9031,\"start\":9010},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9152,\"start\":9135},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9377,\"start\":9354},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10245,\"start\":10223},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10451,\"start\":10429},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10509,\"start\":10487},{\"end\":10537,\"start\":10511},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10563,\"start\":10543},{\"end\":10600,\"start\":10579},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11236,\"start\":11216},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11254,\"start\":11236},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13798,\"start\":13775},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14392,\"start\":14371},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14469,\"start\":14444},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14485,\"start\":14469},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14507,\"start\":14485},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14525,\"start\":14507},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14544,\"start\":14525},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14635,\"start\":14617},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14733,\"start\":14711},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14993,\"start\":14969},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15027,\"start\":15007},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15187,\"start\":15166},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15378,\"start\":15356},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15971,\"start\":15946},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16038,\"start\":16017},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16312,\"start\":16282},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16416,\"start\":16395},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16476,\"start\":16454},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16525,\"start\":16502},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16554,\"start\":16537},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16739,\"start\":16720},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16820,\"start\":16796},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16965,\"start\":16942},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17013,\"start\":16995},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17254,\"start\":17231},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17599,\"start\":17581},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17679,\"start\":17657},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19673,\"start\":19643},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19793,\"start\":19770},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19823,\"start\":19805},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19953,\"start\":19930},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20568,\"start\":20548},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21139,\"start\":21118},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21906,\"start\":21883},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25418,\"start\":25398},{\"end\":27890,\"start\":27887},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28499,\"start\":28481},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28769,\"start\":28751},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28788,\"start\":28769},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30037,\"start\":30021},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30057,\"start\":30037},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30079,\"start\":30057},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30100,\"start\":30079},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30401,\"start\":30380},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30534,\"start\":30510},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31331,\"start\":31310},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31692,\"start\":31672},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32823,\"start\":32793},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36476,\"start\":36458}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32596,\"start\":32475},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33037,\"start\":32597},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33334,\"start\":33038},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33604,\"start\":33335},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33899,\"start\":33605},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34115,\"start\":33900},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34302,\"start\":34116},{\"attributes\":{\"id\":\"fig_7\"},\"end\":34480,\"start\":34303},{\"attributes\":{\"id\":\"fig_8\"},\"end\":34637,\"start\":34481},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35346,\"start\":34638},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36203,\"start\":35347}]", "paragraph": "[{\"end\":2785,\"start\":1617},{\"end\":3417,\"start\":2787},{\"end\":4001,\"start\":3419},{\"end\":4432,\"start\":4003},{\"end\":5958,\"start\":4434},{\"end\":6084,\"start\":5960},{\"end\":6750,\"start\":6086},{\"end\":7302,\"start\":6752},{\"end\":7585,\"start\":7319},{\"end\":8383,\"start\":7587},{\"end\":9609,\"start\":8385},{\"end\":9698,\"start\":9611},{\"end\":9766,\"start\":9700},{\"end\":9783,\"start\":9768},{\"end\":10287,\"start\":9803},{\"end\":10964,\"start\":10339},{\"end\":11478,\"start\":11023},{\"end\":11606,\"start\":11548},{\"end\":12000,\"start\":11608},{\"end\":12172,\"start\":12024},{\"end\":12600,\"start\":12174},{\"end\":12739,\"start\":12684},{\"end\":13222,\"start\":12741},{\"end\":13934,\"start\":13224},{\"end\":14021,\"start\":14018},{\"end\":14147,\"start\":14046},{\"end\":14312,\"start\":14160},{\"end\":14836,\"start\":14334},{\"end\":15068,\"start\":14838},{\"end\":16039,\"start\":15079},{\"end\":16633,\"start\":16041},{\"end\":16821,\"start\":16660},{\"end\":17518,\"start\":16823},{\"end\":17680,\"start\":17520},{\"end\":17750,\"start\":17721},{\"end\":19184,\"start\":17752},{\"end\":19249,\"start\":19226},{\"end\":19255,\"start\":19251},{\"end\":19286,\"start\":19257},{\"end\":19312,\"start\":19288},{\"end\":19319,\"start\":19314},{\"end\":19445,\"start\":19321},{\"end\":20085,\"start\":19519},{\"end\":21282,\"start\":20087},{\"end\":22067,\"start\":21332},{\"end\":22736,\"start\":22128},{\"end\":23408,\"start\":22738},{\"end\":24207,\"start\":23410},{\"end\":24691,\"start\":24238},{\"end\":24741,\"start\":24738},{\"end\":25126,\"start\":24743},{\"end\":25597,\"start\":25192},{\"end\":25898,\"start\":25607},{\"end\":26939,\"start\":25900},{\"end\":27213,\"start\":26941},{\"end\":27764,\"start\":27249},{\"end\":27807,\"start\":27766},{\"end\":27928,\"start\":27885},{\"end\":28427,\"start\":27974},{\"end\":28890,\"start\":28429},{\"end\":29898,\"start\":28957},{\"end\":29952,\"start\":29900},{\"end\":30552,\"start\":29954},{\"end\":30951,\"start\":30554},{\"end\":31420,\"start\":30953},{\"end\":31910,\"start\":31422},{\"end\":32078,\"start\":31912},{\"end\":32474,\"start\":32107},{\"end\":32595,\"start\":32489},{\"end\":33036,\"start\":32611},{\"end\":33333,\"start\":33052},{\"end\":33603,\"start\":33349},{\"end\":33898,\"start\":33619},{\"end\":34114,\"start\":33909},{\"end\":34301,\"start\":34130},{\"end\":34479,\"start\":34317},{\"end\":34636,\"start\":34497},{\"end\":34737,\"start\":34651},{\"end\":35681,\"start\":35360}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10338,\"start\":10288},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11022,\"start\":10965},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11547,\"start\":11479},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12683,\"start\":12601},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14017,\"start\":13935},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24737,\"start\":24692},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25191,\"start\":25127},{\"attributes\":{\"id\":\"formula_7\"},\"end\":27884,\"start\":27808},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27972,\"start\":27929},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27973,\"start\":27972},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28956,\"start\":28891}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":5912,\"start\":5911},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18422,\"start\":18421},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":18663,\"start\":18656},{\"end\":19328,\"start\":19327},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23962,\"start\":23961},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24205,\"start\":24204},{\"end\":25614,\"start\":25613},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26670,\"start\":26669},{\"end\":26948,\"start\":26947},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28972,\"start\":28971},{\"end\":30968,\"start\":30967}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1615,\"start\":1603},{\"attributes\":{\"n\":\"2\"},\"end\":7317,\"start\":7305},{\"attributes\":{\"n\":\"3.1\"},\"end\":9801,\"start\":9786},{\"attributes\":{\"n\":\"3.2\"},\"end\":12022,\"start\":12003},{\"attributes\":{\"n\":\"4\"},\"end\":14044,\"start\":14024},{\"attributes\":{\"n\":\"4.1\"},\"end\":14158,\"start\":14150},{\"end\":14332,\"start\":14315},{\"attributes\":{\"n\":\"4.2\"},\"end\":15077,\"start\":15071},{\"attributes\":{\"n\":\"4.3\"},\"end\":16658,\"start\":16636},{\"attributes\":{\"n\":\"5\"},\"end\":17719,\"start\":17683},{\"end\":19224,\"start\":19187},{\"attributes\":{\"n\":\"5.1\"},\"end\":19517,\"start\":19448},{\"attributes\":{\"n\":\"5.2\"},\"end\":21330,\"start\":21285},{\"attributes\":{\"n\":\"5.3\"},\"end\":22126,\"start\":22070},{\"attributes\":{\"n\":\"6.1\"},\"end\":24236,\"start\":24210},{\"end\":25605,\"start\":25600},{\"attributes\":{\"n\":\"6.2\"},\"end\":27247,\"start\":27216},{\"end\":32105,\"start\":32081},{\"end\":32486,\"start\":32476},{\"end\":32608,\"start\":32598},{\"end\":33049,\"start\":33039},{\"end\":33346,\"start\":33336},{\"end\":33616,\"start\":33606},{\"end\":33907,\"start\":33901},{\"end\":34127,\"start\":34117},{\"end\":34314,\"start\":34304},{\"end\":34493,\"start\":34482},{\"end\":34648,\"start\":34639},{\"end\":35357,\"start\":35348}]", "table": "[{\"end\":35346,\"start\":34738},{\"end\":36203,\"start\":35682}]", "figure_caption": "[{\"end\":32596,\"start\":32488},{\"end\":33037,\"start\":32610},{\"end\":33334,\"start\":33051},{\"end\":33604,\"start\":33348},{\"end\":33899,\"start\":33618},{\"end\":34115,\"start\":33908},{\"end\":34302,\"start\":34129},{\"end\":34480,\"start\":34316},{\"end\":34637,\"start\":34496},{\"end\":34738,\"start\":34650},{\"end\":35682,\"start\":35359}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3999,\"start\":3998},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5482,\"start\":5481},{\"end\":5711,\"start\":5710},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11426,\"start\":11425},{\"end\":18644,\"start\":18643},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20079,\"start\":20075},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20231,\"start\":20230},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20300,\"start\":20299},{\"end\":20524,\"start\":20523},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21471,\"start\":21470},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22225,\"start\":22224},{\"end\":23568,\"start\":23567},{\"end\":29685,\"start\":29684},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30745,\"start\":30744},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":32115,\"start\":32114},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":32129,\"start\":32127},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32449,\"start\":32448}]", "bib_author_first_name": "[{\"end\":37088,\"start\":37085},{\"end\":37100,\"start\":37095},{\"end\":37112,\"start\":37105},{\"end\":37124,\"start\":37117},{\"end\":37138,\"start\":37135},{\"end\":37151,\"start\":37145},{\"end\":37251,\"start\":37248},{\"end\":37262,\"start\":37259},{\"end\":37272,\"start\":37268},{\"end\":37285,\"start\":37279},{\"end\":37299,\"start\":37293},{\"end\":37413,\"start\":37404},{\"end\":37430,\"start\":37424},{\"end\":37445,\"start\":37439},{\"end\":37462,\"start\":37456},{\"end\":37473,\"start\":37468},{\"end\":37491,\"start\":37486},{\"end\":37508,\"start\":37502},{\"end\":37511,\"start\":37509},{\"end\":37544,\"start\":37531},{\"end\":37560,\"start\":37554},{\"end\":37573,\"start\":37568},{\"end\":37586,\"start\":37581},{\"end\":37602,\"start\":37595},{\"end\":37615,\"start\":37610},{\"end\":37626,\"start\":37621},{\"end\":37638,\"start\":37635},{\"end\":37652,\"start\":37645},{\"end\":37668,\"start\":37663},{\"end\":37681,\"start\":37676},{\"end\":37697,\"start\":37692},{\"end\":37709,\"start\":37705},{\"end\":37727,\"start\":37720},{\"end\":37743,\"start\":37735},{\"end\":37773,\"start\":37768},{\"end\":37788,\"start\":37783},{\"end\":37804,\"start\":37799},{\"end\":37819,\"start\":37815},{\"end\":37830,\"start\":37825},{\"end\":37845,\"start\":37838},{\"end\":37901,\"start\":37898},{\"end\":37903,\"start\":37902},{\"end\":37919,\"start\":37911},{\"end\":37930,\"start\":37926},{\"end\":37945,\"start\":37938},{\"end\":37960,\"start\":37955},{\"end\":37977,\"start\":37969},{\"end\":37994,\"start\":37988},{\"end\":38014,\"start\":38008},{\"end\":38028,\"start\":38022},{\"end\":38043,\"start\":38037},{\"end\":38060,\"start\":38052},{\"end\":38075,\"start\":38070},{\"end\":38098,\"start\":38090},{\"end\":38111,\"start\":38108},{\"end\":38127,\"start\":38122},{\"end\":38141,\"start\":38135},{\"end\":38156,\"start\":38150},{\"end\":38173,\"start\":38166},{\"end\":38185,\"start\":38178},{\"end\":38205,\"start\":38194},{\"end\":38217,\"start\":38213},{\"end\":38228,\"start\":38224},{\"end\":38244,\"start\":38237},{\"end\":38486,\"start\":38481},{\"end\":38497,\"start\":38493},{\"end\":38510,\"start\":38505},{\"end\":38526,\"start\":38519},{\"end\":38881,\"start\":38876},{\"end\":38898,\"start\":38890},{\"end\":38912,\"start\":38906},{\"end\":38926,\"start\":38918},{\"end\":39329,\"start\":39322},{\"end\":39339,\"start\":39336},{\"end\":39349,\"start\":39344},{\"end\":39357,\"start\":39355},{\"end\":39372,\"start\":39365},{\"end\":39383,\"start\":39377},{\"end\":39393,\"start\":39391},{\"end\":39407,\"start\":39399},{\"end\":39415,\"start\":39412},{\"end\":39427,\"start\":39420},{\"end\":39565,\"start\":39559},{\"end\":39580,\"start\":39574},{\"end\":39595,\"start\":39590},{\"end\":39609,\"start\":39604},{\"end\":39869,\"start\":39866},{\"end\":39881,\"start\":39875},{\"end\":39895,\"start\":39892},{\"end\":39911,\"start\":39903},{\"end\":39927,\"start\":39921},{\"end\":39942,\"start\":39935},{\"end\":39956,\"start\":39951},{\"end\":39970,\"start\":39964},{\"end\":39980,\"start\":39975},{\"end\":39991,\"start\":39988},{\"end\":40008,\"start\":40003},{\"end\":40024,\"start\":40018},{\"end\":40125,\"start\":40119},{\"end\":40137,\"start\":40131},{\"end\":40147,\"start\":40143},{\"end\":40162,\"start\":40154},{\"end\":40180,\"start\":40172},{\"end\":40251,\"start\":40244},{\"end\":40262,\"start\":40256},{\"end\":40277,\"start\":40271},{\"end\":40643,\"start\":40637},{\"end\":40658,\"start\":40651},{\"end\":40672,\"start\":40664},{\"end\":40818,\"start\":40811},{\"end\":40836,\"start\":40828},{\"end\":40849,\"start\":40844},{\"end\":40869,\"start\":40860},{\"end\":40883,\"start\":40878},{\"end\":40902,\"start\":40896},{\"end\":40918,\"start\":40911},{\"end\":41071,\"start\":41064},{\"end\":41088,\"start\":41081},{\"end\":41481,\"start\":41474},{\"end\":41498,\"start\":41491},{\"end\":41511,\"start\":41506},{\"end\":41525,\"start\":41520},{\"end\":41541,\"start\":41536},{\"end\":41555,\"start\":41551},{\"end\":41568,\"start\":41564},{\"end\":41587,\"start\":41581},{\"end\":41605,\"start\":41596},{\"end\":41621,\"start\":41614},{\"end\":41708,\"start\":41704},{\"end\":41726,\"start\":41718},{\"end\":41739,\"start\":41734},{\"end\":41909,\"start\":41903},{\"end\":41923,\"start\":41917},{\"end\":41936,\"start\":41930},{\"end\":41947,\"start\":41943},{\"end\":42290,\"start\":42282},{\"end\":42308,\"start\":42302},{\"end\":42320,\"start\":42315},{\"end\":42333,\"start\":42326},{\"end\":42347,\"start\":42341},{\"end\":42358,\"start\":42352},{\"end\":42372,\"start\":42367},{\"end\":42386,\"start\":42379},{\"end\":42662,\"start\":42655},{\"end\":42679,\"start\":42675},{\"end\":42689,\"start\":42686},{\"end\":42704,\"start\":42700},{\"end\":42722,\"start\":42718},{\"end\":42930,\"start\":42927},{\"end\":42954,\"start\":42944},{\"end\":42971,\"start\":42965},{\"end\":42989,\"start\":42982},{\"end\":43004,\"start\":42999},{\"end\":43018,\"start\":43013},{\"end\":43036,\"start\":43028},{\"end\":43051,\"start\":43046},{\"end\":43069,\"start\":43064},{\"end\":43084,\"start\":43078},{\"end\":43098,\"start\":43090},{\"end\":43115,\"start\":43110},{\"end\":43130,\"start\":43123},{\"end\":43147,\"start\":43139},{\"end\":43161,\"start\":43155},{\"end\":43163,\"start\":43162},{\"end\":43174,\"start\":43169},{\"end\":43190,\"start\":43186},{\"end\":43199,\"start\":43195},{\"end\":43361,\"start\":43357},{\"end\":43374,\"start\":43370},{\"end\":43390,\"start\":43387},{\"end\":43400,\"start\":43396},{\"end\":43415,\"start\":43409},{\"end\":43427,\"start\":43424},{\"end\":43441,\"start\":43437},{\"end\":43453,\"start\":43448},{\"end\":43467,\"start\":43462},{\"end\":43478,\"start\":43474},{\"end\":43500,\"start\":43495},{\"end\":43515,\"start\":43510},{\"end\":43534,\"start\":43530},{\"end\":43654,\"start\":43650},{\"end\":43666,\"start\":43663},{\"end\":43678,\"start\":43672},{\"end\":43692,\"start\":43687},{\"end\":43703,\"start\":43699},{\"end\":43725,\"start\":43720},{\"end\":43740,\"start\":43735},{\"end\":43759,\"start\":43755},{\"end\":43924,\"start\":43920},{\"end\":43937,\"start\":43933},{\"end\":43950,\"start\":43944},{\"end\":43962,\"start\":43959},{\"end\":43975,\"start\":43970},{\"end\":43989,\"start\":43984},{\"end\":44133,\"start\":44126},{\"end\":44146,\"start\":44141},{\"end\":44164,\"start\":44154},{\"end\":44178,\"start\":44173},{\"end\":44196,\"start\":44188},{\"end\":44213,\"start\":44208},{\"end\":44229,\"start\":44221},{\"end\":44243,\"start\":44239},{\"end\":44258,\"start\":44251},{\"end\":44267,\"start\":44264},{\"end\":44290,\"start\":44281},{\"end\":44304,\"start\":44299},{\"end\":44446,\"start\":44439},{\"end\":44456,\"start\":44451},{\"end\":44468,\"start\":44462},{\"end\":44756,\"start\":44751},{\"end\":44770,\"start\":44762},{\"end\":44786,\"start\":44775},{\"end\":44802,\"start\":44792},{\"end\":44814,\"start\":44809},{\"end\":44831,\"start\":44824},{\"end\":45220,\"start\":45211},{\"end\":45231,\"start\":45226},{\"end\":45245,\"start\":45240},{\"end\":45525,\"start\":45519},{\"end\":45535,\"start\":45531},{\"end\":45546,\"start\":45541},{\"end\":45561,\"start\":45554},{\"end\":45572,\"start\":45566},{\"end\":45585,\"start\":45580},{\"end\":45596,\"start\":45592},{\"end\":45607,\"start\":45603},{\"end\":45619,\"start\":45615},{\"end\":45640,\"start\":45633},{\"end\":45806,\"start\":45800},{\"end\":45821,\"start\":45815},{\"end\":45836,\"start\":45831},{\"end\":45849,\"start\":45845},{\"end\":46112,\"start\":46105},{\"end\":46128,\"start\":46121},{\"end\":46141,\"start\":46136},{\"end\":46159,\"start\":46152},{\"end\":46303,\"start\":46300},{\"end\":46316,\"start\":46313},{\"end\":46327,\"start\":46322},{\"end\":46339,\"start\":46335},{\"end\":46351,\"start\":46348},{\"end\":46367,\"start\":46360},{\"end\":46382,\"start\":46378},{\"end\":46395,\"start\":46390},{\"end\":46415,\"start\":46410},{\"end\":46429,\"start\":46425},{\"end\":46532,\"start\":46527},{\"end\":46552,\"start\":46542},{\"end\":46567,\"start\":46561},{\"end\":46580,\"start\":46573},{\"end\":46593,\"start\":46588},{\"end\":46609,\"start\":46603},{\"end\":46612,\"start\":46610},{\"end\":46623,\"start\":46618},{\"end\":46638,\"start\":46632},{\"end\":46656,\"start\":46648},{\"end\":46672,\"start\":46668},{\"end\":46691,\"start\":46683},{\"end\":46707,\"start\":46704},{\"end\":46730,\"start\":46721},{\"end\":47112,\"start\":47108},{\"end\":47129,\"start\":47122},{\"end\":47145,\"start\":47142},{\"end\":47160,\"start\":47156},{\"end\":47235,\"start\":47231},{\"end\":47249,\"start\":47245},{\"end\":47259,\"start\":47254},{\"end\":47272,\"start\":47267},{\"end\":47284,\"start\":47279},{\"end\":47297,\"start\":47293},{\"end\":47369,\"start\":47366},{\"end\":47378,\"start\":47375},{\"end\":47392,\"start\":47388},{\"end\":47407,\"start\":47399},{\"end\":47420,\"start\":47416},{\"end\":47808,\"start\":47805},{\"end\":47821,\"start\":47817},{\"end\":47837,\"start\":47830},{\"end\":47851,\"start\":47848},{\"end\":47861,\"start\":47857},{\"end\":47873,\"start\":47869},{\"end\":47887,\"start\":47882},{\"end\":47902,\"start\":47897},{\"end\":47921,\"start\":47917},{\"end\":48105,\"start\":48098},{\"end\":48121,\"start\":48117},{\"end\":48279,\"start\":48271},{\"end\":48292,\"start\":48288},{\"end\":48306,\"start\":48300},{\"end\":48319,\"start\":48314},{\"end\":48339,\"start\":48332},{\"end\":48351,\"start\":48345},{\"end\":48364,\"start\":48360},{\"end\":48707,\"start\":48701},{\"end\":48718,\"start\":48713},{\"end\":48733,\"start\":48724},{\"end\":48751,\"start\":48744},{\"end\":48761,\"start\":48757},{\"end\":48773,\"start\":48769},{\"end\":48785,\"start\":48781},{\"end\":48802,\"start\":48799},{\"end\":48811,\"start\":48808},{\"end\":48941,\"start\":48935},{\"end\":48954,\"start\":48950},{\"end\":48971,\"start\":48964},{\"end\":48988,\"start\":48980},{\"end\":49006,\"start\":49001},{\"end\":49250,\"start\":49243},{\"end\":49272,\"start\":49265},{\"end\":49287,\"start\":49281},{\"end\":49307,\"start\":49297},{\"end\":49326,\"start\":49318},{\"end\":49344,\"start\":49336},{\"end\":49359,\"start\":49354},{\"end\":49373,\"start\":49369},{\"end\":49387,\"start\":49381},{\"end\":49404,\"start\":49396},{\"end\":49418,\"start\":49412},{\"end\":49437,\"start\":49430},{\"end\":49455,\"start\":49446},{\"end\":49535,\"start\":49529},{\"end\":49549,\"start\":49545},{\"end\":49563,\"start\":49559},{\"end\":49577,\"start\":49572},{\"end\":49594,\"start\":49589},{\"end\":49607,\"start\":49602},{\"end\":49621,\"start\":49615},{\"end\":49635,\"start\":49630},{\"end\":49710,\"start\":49707},{\"end\":49721,\"start\":49717},{\"end\":49870,\"start\":49864},{\"end\":49885,\"start\":49877},{\"end\":49899,\"start\":49893},{\"end\":49912,\"start\":49906},{\"end\":49930,\"start\":49923},{\"end\":49948,\"start\":49941},{\"end\":49961,\"start\":49954},{\"end\":49973,\"start\":49970},{\"end\":49985,\"start\":49981},{\"end\":49998,\"start\":49992},{\"end\":50013,\"start\":50010},{\"end\":50026,\"start\":50023},{\"end\":50042,\"start\":50037},{\"end\":50069,\"start\":50063},{\"end\":50080,\"start\":50074},{\"end\":50096,\"start\":50090},{\"end\":50107,\"start\":50102},{\"end\":50119,\"start\":50112},{\"end\":50136,\"start\":50129},{\"end\":50152,\"start\":50145},{\"end\":50169,\"start\":50160},{\"end\":50515,\"start\":50510},{\"end\":50528,\"start\":50525},{\"end\":50545,\"start\":50539},{\"end\":50562,\"start\":50555},{\"end\":50572,\"start\":50569},{\"end\":50591,\"start\":50582},{\"end\":50606,\"start\":50601},{\"end\":50750,\"start\":50745},{\"end\":50765,\"start\":50758},{\"end\":50779,\"start\":50774},{\"end\":50792,\"start\":50787},{\"end\":50806,\"start\":50802},{\"end\":50820,\"start\":50813},{\"end\":50838,\"start\":50827},{\"end\":50850,\"start\":50846},{\"end\":50861,\"start\":50857},{\"end\":50868,\"start\":50866},{\"end\":50888,\"start\":50883},{\"end\":50903,\"start\":50899},{\"end\":50912,\"start\":50909},{\"end\":50927,\"start\":50923},{\"end\":50943,\"start\":50937},{\"end\":50956,\"start\":50951},{\"end\":50976,\"start\":50970},{\"end\":50992,\"start\":50986},{\"end\":51003,\"start\":50999},{\"end\":51105,\"start\":51099},{\"end\":51116,\"start\":51113},{\"end\":51127,\"start\":51122}]", "bib_author_last_name": "[{\"end\":37093,\"start\":37089},{\"end\":37103,\"start\":37101},{\"end\":37115,\"start\":37113},{\"end\":37133,\"start\":37125},{\"end\":37143,\"start\":37139},{\"end\":37158,\"start\":37152},{\"end\":37257,\"start\":37252},{\"end\":37266,\"start\":37263},{\"end\":37277,\"start\":37273},{\"end\":37291,\"start\":37286},{\"end\":37308,\"start\":37300},{\"end\":37422,\"start\":37414},{\"end\":37437,\"start\":37431},{\"end\":37454,\"start\":37446},{\"end\":37466,\"start\":37463},{\"end\":37484,\"start\":37474},{\"end\":37500,\"start\":37492},{\"end\":37529,\"start\":37512},{\"end\":37552,\"start\":37545},{\"end\":37566,\"start\":37561},{\"end\":37579,\"start\":37574},{\"end\":37593,\"start\":37587},{\"end\":37608,\"start\":37603},{\"end\":37619,\"start\":37616},{\"end\":37633,\"start\":37627},{\"end\":37643,\"start\":37639},{\"end\":37661,\"start\":37653},{\"end\":37674,\"start\":37669},{\"end\":37690,\"start\":37682},{\"end\":37703,\"start\":37698},{\"end\":37718,\"start\":37710},{\"end\":37733,\"start\":37728},{\"end\":37752,\"start\":37744},{\"end\":37760,\"start\":37754},{\"end\":37781,\"start\":37774},{\"end\":37797,\"start\":37789},{\"end\":37813,\"start\":37805},{\"end\":37823,\"start\":37820},{\"end\":37836,\"start\":37831},{\"end\":37851,\"start\":37846},{\"end\":37909,\"start\":37904},{\"end\":37924,\"start\":37920},{\"end\":37936,\"start\":37931},{\"end\":37953,\"start\":37946},{\"end\":37967,\"start\":37961},{\"end\":37986,\"start\":37978},{\"end\":38006,\"start\":37995},{\"end\":38020,\"start\":38015},{\"end\":38035,\"start\":38029},{\"end\":38050,\"start\":38044},{\"end\":38068,\"start\":38061},{\"end\":38088,\"start\":38076},{\"end\":38106,\"start\":38099},{\"end\":38120,\"start\":38112},{\"end\":38133,\"start\":38128},{\"end\":38148,\"start\":38142},{\"end\":38164,\"start\":38157},{\"end\":38176,\"start\":38174},{\"end\":38192,\"start\":38186},{\"end\":38211,\"start\":38206},{\"end\":38222,\"start\":38218},{\"end\":38235,\"start\":38229},{\"end\":38251,\"start\":38245},{\"end\":38491,\"start\":38487},{\"end\":38503,\"start\":38498},{\"end\":38517,\"start\":38511},{\"end\":38533,\"start\":38527},{\"end\":38888,\"start\":38882},{\"end\":38904,\"start\":38899},{\"end\":38916,\"start\":38913},{\"end\":38936,\"start\":38927},{\"end\":39334,\"start\":39330},{\"end\":39342,\"start\":39340},{\"end\":39353,\"start\":39350},{\"end\":39363,\"start\":39358},{\"end\":39375,\"start\":39373},{\"end\":39389,\"start\":39384},{\"end\":39397,\"start\":39394},{\"end\":39410,\"start\":39408},{\"end\":39418,\"start\":39416},{\"end\":39431,\"start\":39428},{\"end\":39572,\"start\":39566},{\"end\":39588,\"start\":39581},{\"end\":39602,\"start\":39596},{\"end\":39614,\"start\":39610},{\"end\":39873,\"start\":39870},{\"end\":39890,\"start\":39882},{\"end\":39901,\"start\":39896},{\"end\":39919,\"start\":39912},{\"end\":39933,\"start\":39928},{\"end\":39949,\"start\":39943},{\"end\":39962,\"start\":39957},{\"end\":39973,\"start\":39971},{\"end\":39986,\"start\":39981},{\"end\":40001,\"start\":39992},{\"end\":40016,\"start\":40009},{\"end\":40030,\"start\":40025},{\"end\":40129,\"start\":40126},{\"end\":40141,\"start\":40138},{\"end\":40152,\"start\":40148},{\"end\":40170,\"start\":40163},{\"end\":40186,\"start\":40181},{\"end\":40254,\"start\":40252},{\"end\":40269,\"start\":40263},{\"end\":40294,\"start\":40278},{\"end\":40649,\"start\":40644},{\"end\":40662,\"start\":40659},{\"end\":40676,\"start\":40673},{\"end\":40826,\"start\":40819},{\"end\":40842,\"start\":40837},{\"end\":40858,\"start\":40850},{\"end\":40876,\"start\":40870},{\"end\":40894,\"start\":40884},{\"end\":40909,\"start\":40903},{\"end\":40924,\"start\":40919},{\"end\":41079,\"start\":41072},{\"end\":41094,\"start\":41089},{\"end\":41489,\"start\":41482},{\"end\":41504,\"start\":41499},{\"end\":41518,\"start\":41512},{\"end\":41534,\"start\":41526},{\"end\":41549,\"start\":41542},{\"end\":41562,\"start\":41556},{\"end\":41579,\"start\":41569},{\"end\":41594,\"start\":41588},{\"end\":41612,\"start\":41606},{\"end\":41627,\"start\":41622},{\"end\":41716,\"start\":41709},{\"end\":41732,\"start\":41727},{\"end\":41745,\"start\":41740},{\"end\":41915,\"start\":41910},{\"end\":41928,\"start\":41924},{\"end\":41941,\"start\":41937},{\"end\":41959,\"start\":41948},{\"end\":42300,\"start\":42291},{\"end\":42313,\"start\":42309},{\"end\":42324,\"start\":42321},{\"end\":42339,\"start\":42334},{\"end\":42350,\"start\":42348},{\"end\":42365,\"start\":42359},{\"end\":42377,\"start\":42373},{\"end\":42390,\"start\":42387},{\"end\":42673,\"start\":42663},{\"end\":42684,\"start\":42680},{\"end\":42698,\"start\":42690},{\"end\":42716,\"start\":42705},{\"end\":42728,\"start\":42723},{\"end\":42942,\"start\":42931},{\"end\":42963,\"start\":42955},{\"end\":42980,\"start\":42972},{\"end\":42997,\"start\":42990},{\"end\":43011,\"start\":43005},{\"end\":43026,\"start\":43019},{\"end\":43044,\"start\":43037},{\"end\":43062,\"start\":43052},{\"end\":43076,\"start\":43070},{\"end\":43088,\"start\":43085},{\"end\":43108,\"start\":43099},{\"end\":43121,\"start\":43116},{\"end\":43137,\"start\":43131},{\"end\":43153,\"start\":43148},{\"end\":43167,\"start\":43164},{\"end\":43184,\"start\":43175},{\"end\":43193,\"start\":43191},{\"end\":43206,\"start\":43200},{\"end\":43368,\"start\":43362},{\"end\":43385,\"start\":43375},{\"end\":43394,\"start\":43391},{\"end\":43407,\"start\":43401},{\"end\":43422,\"start\":43416},{\"end\":43435,\"start\":43428},{\"end\":43446,\"start\":43442},{\"end\":43460,\"start\":43454},{\"end\":43472,\"start\":43468},{\"end\":43493,\"start\":43479},{\"end\":43508,\"start\":43501},{\"end\":43528,\"start\":43516},{\"end\":43541,\"start\":43535},{\"end\":43661,\"start\":43655},{\"end\":43670,\"start\":43667},{\"end\":43685,\"start\":43679},{\"end\":43697,\"start\":43693},{\"end\":43718,\"start\":43704},{\"end\":43733,\"start\":43726},{\"end\":43753,\"start\":43741},{\"end\":43766,\"start\":43760},{\"end\":43931,\"start\":43925},{\"end\":43942,\"start\":43938},{\"end\":43957,\"start\":43951},{\"end\":43968,\"start\":43963},{\"end\":43982,\"start\":43976},{\"end\":43997,\"start\":43990},{\"end\":44139,\"start\":44134},{\"end\":44152,\"start\":44147},{\"end\":44171,\"start\":44165},{\"end\":44186,\"start\":44179},{\"end\":44206,\"start\":44197},{\"end\":44219,\"start\":44214},{\"end\":44237,\"start\":44230},{\"end\":44249,\"start\":44244},{\"end\":44262,\"start\":44259},{\"end\":44279,\"start\":44268},{\"end\":44297,\"start\":44291},{\"end\":44310,\"start\":44305},{\"end\":44449,\"start\":44447},{\"end\":44460,\"start\":44457},{\"end\":44474,\"start\":44469},{\"end\":44760,\"start\":44757},{\"end\":44773,\"start\":44771},{\"end\":44790,\"start\":44787},{\"end\":44807,\"start\":44803},{\"end\":44822,\"start\":44815},{\"end\":44840,\"start\":44832},{\"end\":45224,\"start\":45221},{\"end\":45238,\"start\":45232},{\"end\":45251,\"start\":45246},{\"end\":45529,\"start\":45526},{\"end\":45539,\"start\":45536},{\"end\":45552,\"start\":45547},{\"end\":45564,\"start\":45562},{\"end\":45578,\"start\":45573},{\"end\":45590,\"start\":45586},{\"end\":45601,\"start\":45597},{\"end\":45613,\"start\":45608},{\"end\":45631,\"start\":45620},{\"end\":45649,\"start\":45641},{\"end\":45813,\"start\":45807},{\"end\":45829,\"start\":45822},{\"end\":45843,\"start\":45837},{\"end\":45858,\"start\":45850},{\"end\":46119,\"start\":46113},{\"end\":46134,\"start\":46129},{\"end\":46150,\"start\":46142},{\"end\":46166,\"start\":46160},{\"end\":46311,\"start\":46304},{\"end\":46320,\"start\":46317},{\"end\":46333,\"start\":46328},{\"end\":46346,\"start\":46340},{\"end\":46358,\"start\":46352},{\"end\":46376,\"start\":46368},{\"end\":46388,\"start\":46383},{\"end\":46408,\"start\":46396},{\"end\":46423,\"start\":46416},{\"end\":46436,\"start\":46430},{\"end\":46540,\"start\":46533},{\"end\":46559,\"start\":46553},{\"end\":46571,\"start\":46568},{\"end\":46586,\"start\":46581},{\"end\":46601,\"start\":46594},{\"end\":46616,\"start\":46613},{\"end\":46630,\"start\":46624},{\"end\":46646,\"start\":46639},{\"end\":46666,\"start\":46657},{\"end\":46681,\"start\":46673},{\"end\":46702,\"start\":46692},{\"end\":46719,\"start\":46708},{\"end\":46737,\"start\":46731},{\"end\":47120,\"start\":47113},{\"end\":47140,\"start\":47130},{\"end\":47154,\"start\":47146},{\"end\":47170,\"start\":47161},{\"end\":47243,\"start\":47236},{\"end\":47252,\"start\":47250},{\"end\":47265,\"start\":47260},{\"end\":47277,\"start\":47273},{\"end\":47291,\"start\":47285},{\"end\":47307,\"start\":47298},{\"end\":47373,\"start\":47370},{\"end\":47386,\"start\":47379},{\"end\":47397,\"start\":47393},{\"end\":47414,\"start\":47408},{\"end\":47430,\"start\":47421},{\"end\":47815,\"start\":47809},{\"end\":47828,\"start\":47822},{\"end\":47846,\"start\":47838},{\"end\":47855,\"start\":47852},{\"end\":47867,\"start\":47862},{\"end\":47880,\"start\":47874},{\"end\":47895,\"start\":47888},{\"end\":47915,\"start\":47903},{\"end\":47928,\"start\":47922},{\"end\":48115,\"start\":48106},{\"end\":48130,\"start\":48122},{\"end\":48286,\"start\":48280},{\"end\":48298,\"start\":48293},{\"end\":48312,\"start\":48307},{\"end\":48330,\"start\":48320},{\"end\":48343,\"start\":48340},{\"end\":48358,\"start\":48352},{\"end\":48376,\"start\":48365},{\"end\":48711,\"start\":48708},{\"end\":48722,\"start\":48719},{\"end\":48742,\"start\":48734},{\"end\":48755,\"start\":48752},{\"end\":48767,\"start\":48762},{\"end\":48779,\"start\":48774},{\"end\":48797,\"start\":48786},{\"end\":48806,\"start\":48803},{\"end\":48948,\"start\":48942},{\"end\":48962,\"start\":48955},{\"end\":48978,\"start\":48972},{\"end\":48999,\"start\":48989},{\"end\":49015,\"start\":49007},{\"end\":49263,\"start\":49251},{\"end\":49279,\"start\":49273},{\"end\":49295,\"start\":49288},{\"end\":49316,\"start\":49308},{\"end\":49334,\"start\":49327},{\"end\":49352,\"start\":49345},{\"end\":49367,\"start\":49360},{\"end\":49379,\"start\":49374},{\"end\":49394,\"start\":49388},{\"end\":49410,\"start\":49405},{\"end\":49428,\"start\":49419},{\"end\":49444,\"start\":49438},{\"end\":49461,\"start\":49456},{\"end\":49469,\"start\":49463},{\"end\":49543,\"start\":49536},{\"end\":49557,\"start\":49550},{\"end\":49570,\"start\":49564},{\"end\":49587,\"start\":49578},{\"end\":49600,\"start\":49595},{\"end\":49613,\"start\":49608},{\"end\":49628,\"start\":49622},{\"end\":49646,\"start\":49636},{\"end\":49715,\"start\":49711},{\"end\":49733,\"start\":49722},{\"end\":49875,\"start\":49871},{\"end\":49891,\"start\":49886},{\"end\":49904,\"start\":49900},{\"end\":49921,\"start\":49913},{\"end\":49939,\"start\":49931},{\"end\":49952,\"start\":49949},{\"end\":49968,\"start\":49962},{\"end\":49979,\"start\":49974},{\"end\":49990,\"start\":49986},{\"end\":50008,\"start\":49999},{\"end\":50021,\"start\":50014},{\"end\":50035,\"start\":50027},{\"end\":50061,\"start\":50043},{\"end\":50072,\"start\":50070},{\"end\":50088,\"start\":50081},{\"end\":50100,\"start\":50097},{\"end\":50110,\"start\":50108},{\"end\":50127,\"start\":50120},{\"end\":50143,\"start\":50137},{\"end\":50158,\"start\":50153},{\"end\":50176,\"start\":50170},{\"end\":50182,\"start\":50178},{\"end\":50523,\"start\":50516},{\"end\":50537,\"start\":50529},{\"end\":50553,\"start\":50546},{\"end\":50567,\"start\":50563},{\"end\":50580,\"start\":50573},{\"end\":50599,\"start\":50592},{\"end\":50611,\"start\":50607},{\"end\":50756,\"start\":50751},{\"end\":50772,\"start\":50766},{\"end\":50785,\"start\":50780},{\"end\":50800,\"start\":50793},{\"end\":50811,\"start\":50807},{\"end\":50825,\"start\":50821},{\"end\":50844,\"start\":50839},{\"end\":50855,\"start\":50851},{\"end\":50864,\"start\":50862},{\"end\":50881,\"start\":50869},{\"end\":50897,\"start\":50889},{\"end\":50907,\"start\":50904},{\"end\":50921,\"start\":50913},{\"end\":50935,\"start\":50928},{\"end\":50949,\"start\":50944},{\"end\":50968,\"start\":50957},{\"end\":50984,\"start\":50977},{\"end\":50997,\"start\":50993},{\"end\":51015,\"start\":51004},{\"end\":51111,\"start\":51106},{\"end\":51120,\"start\":51117},{\"end\":51132,\"start\":51128}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":246431219},\"end\":37170,\"start\":37016},{\"attributes\":{\"doi\":\"10.5281/zenodo.5297715\",\"id\":\"b1\"},\"end\":37336,\"start\":37172},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":244954723},\"end\":37857,\"start\":37338},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":38428,\"start\":37859},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1171\",\"id\":\"b4\",\"matched_paper_id\":3618568},\"end\":38792,\"start\":38430},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b5\",\"matched_paper_id\":52967399},\"end\":39320,\"start\":38794},{\"attributes\":{\"doi\":\"10.48550/arXiv.2301.00234\",\"id\":\"b6\"},\"end\":39495,\"start\":39322},{\"attributes\":{\"doi\":\"10.18653/v1/2021.findings-emnlp.73\",\"id\":\"b7\",\"matched_paper_id\":237439232},\"end\":39798,\"start\":39497},{\"attributes\":{\"doi\":\"10.48550/arXiv.2101.00027\",\"id\":\"b8\"},\"end\":40061,\"start\":39800},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211204736},\"end\":40198,\"start\":40063},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.461\",\"id\":\"b10\",\"matched_paper_id\":237452184},\"end\":40572,\"start\":40200},{\"attributes\":{\"doi\":\"10.1145/3383123\",\"id\":\"b11\",\"matched_paper_id\":153312680},\"end\":40741,\"start\":40574},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":249097975},\"end\":40974,\"start\":40743},{\"attributes\":{\"doi\":\"10.18653/v1/2021.eacl-main.74\",\"id\":\"b13\",\"matched_paper_id\":220302360},\"end\":41405,\"start\":40976},{\"attributes\":{\"doi\":\"10.48550/arXiv.2208.03299\",\"id\":\"b14\"},\"end\":41659,\"start\":41407},{\"attributes\":{\"doi\":\"10.1109/TBDATA.2019.2921572\",\"id\":\"b15\",\"matched_paper_id\":926364},\"end\":41811,\"start\":41661},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1147\",\"id\":\"b16\",\"matched_paper_id\":26501419},\"end\":42220,\"start\":41813},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.550\",\"id\":\"b17\",\"matched_paper_id\":215737187},\"end\":42651,\"start\":42222},{\"attributes\":{\"id\":\"b18\"},\"end\":42734,\"start\":42653},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":207870430},\"end\":42861,\"start\":42736},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00276\",\"id\":\"b20\",\"matched_paper_id\":86611921},\"end\":43296,\"start\":42863},{\"attributes\":{\"doi\":\"10.48550/arXiv.2204.10019\",\"id\":\"b21\"},\"end\":43573,\"start\":43298},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":252760498},\"end\":43836,\"start\":43575},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":238583726},\"end\":44058,\"start\":43838},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":218869575},\"end\":44367,\"start\":44060},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":252846580},\"end\":44635,\"start\":44369},{\"attributes\":{\"doi\":\"10.1145/3404835.3463238\",\"id\":\"b26\",\"matched_paper_id\":235366815},\"end\":45152,\"start\":44637},{\"attributes\":{\"doi\":\"10.18653/v1/2022.acl-long.229\",\"id\":\"b27\",\"matched_paper_id\":237532606},\"end\":45517,\"start\":45154},{\"attributes\":{\"doi\":\"10.48550/arXiv.1907.11692\",\"id\":\"b28\"},\"end\":45737,\"start\":45519},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.173\",\"id\":\"b29\",\"matched_paper_id\":218487034},\"end\":46103,\"start\":45739},{\"attributes\":{\"doi\":\"10.48550/arXiv.1609.07843\",\"id\":\"b30\"},\"end\":46230,\"start\":46105},{\"attributes\":{\"doi\":\"10.48550/arXiv.2307.06908\",\"id\":\"b31\"},\"end\":46467,\"start\":46232},{\"attributes\":{\"doi\":\"10.18653/v1/2021.naacl-main.200\",\"id\":\"b32\",\"matched_paper_id\":221507798},\"end\":47045,\"start\":46469},{\"attributes\":{\"id\":\"b33\"},\"end\":47176,\"start\":47047},{\"attributes\":{\"id\":\"b34\"},\"end\":47313,\"start\":47178},{\"attributes\":{\"doi\":\"10.18653/v1/2022.naacl-main.193\",\"id\":\"b35\",\"matched_paper_id\":245144844},\"end\":47801,\"start\":47315},{\"attributes\":{\"doi\":\"10.18653/v1/2023.acl-long.352\",\"id\":\"b36\"},\"end\":48040,\"start\":47803},{\"attributes\":{\"doi\":\"10.1561/1500000019\",\"id\":\"b37\",\"matched_paper_id\":207178704},\"end\":48205,\"start\":48042},{\"attributes\":{\"doi\":\"10.18653/v1/2022.emnlp-main.249\",\"id\":\"b38\",\"matched_paper_id\":248218489},\"end\":48644,\"start\":48207},{\"attributes\":{\"doi\":\"10.48550/arXiv.2301.12652\",\"id\":\"b39\"},\"end\":48843,\"start\":48646},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":233296016},\"end\":49187,\"start\":48845},{\"attributes\":{\"doi\":\"10.48550/arXiv.2302.13971\",\"id\":\"b41\"},\"end\":49500,\"start\":49189},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":13756489},\"end\":49705,\"start\":49502},{\"attributes\":{\"id\":\"b43\"},\"end\":49802,\"start\":49707},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-demos.6\",\"id\":\"b44\",\"matched_paper_id\":208117506},\"end\":50472,\"start\":49804},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":168169824},\"end\":50692,\"start\":50474},{\"attributes\":{\"doi\":\"10.48550/arXiv.2205.01068\",\"id\":\"b46\"},\"end\":51046,\"start\":50694},{\"attributes\":{\"doi\":\"10.18653/v1/2022.emnlp-main.382\",\"id\":\"b47\",\"matched_paper_id\":249062699},\"end\":51400,\"start\":51048}]", "bib_title": "[{\"end\":37083,\"start\":37016},{\"end\":37402,\"start\":37338},{\"end\":37896,\"start\":37859},{\"end\":38479,\"start\":38430},{\"end\":38874,\"start\":38794},{\"end\":39557,\"start\":39497},{\"end\":40117,\"start\":40063},{\"end\":40242,\"start\":40200},{\"end\":40635,\"start\":40574},{\"end\":40809,\"start\":40743},{\"end\":41062,\"start\":40976},{\"end\":41702,\"start\":41661},{\"end\":41901,\"start\":41813},{\"end\":42280,\"start\":42222},{\"end\":42805,\"start\":42736},{\"end\":42925,\"start\":42863},{\"end\":43648,\"start\":43575},{\"end\":43918,\"start\":43838},{\"end\":44124,\"start\":44060},{\"end\":44437,\"start\":44369},{\"end\":44749,\"start\":44637},{\"end\":45209,\"start\":45154},{\"end\":45798,\"start\":45739},{\"end\":46525,\"start\":46469},{\"end\":47364,\"start\":47315},{\"end\":48096,\"start\":48042},{\"end\":48269,\"start\":48207},{\"end\":48933,\"start\":48845},{\"end\":49527,\"start\":49502},{\"end\":49862,\"start\":49804},{\"end\":50508,\"start\":50474},{\"end\":51097,\"start\":51048}]", "bib_author": "[{\"end\":37095,\"start\":37085},{\"end\":37105,\"start\":37095},{\"end\":37117,\"start\":37105},{\"end\":37135,\"start\":37117},{\"end\":37145,\"start\":37135},{\"end\":37160,\"start\":37145},{\"end\":37259,\"start\":37248},{\"end\":37268,\"start\":37259},{\"end\":37279,\"start\":37268},{\"end\":37293,\"start\":37279},{\"end\":37310,\"start\":37293},{\"end\":37424,\"start\":37404},{\"end\":37439,\"start\":37424},{\"end\":37456,\"start\":37439},{\"end\":37468,\"start\":37456},{\"end\":37486,\"start\":37468},{\"end\":37502,\"start\":37486},{\"end\":37531,\"start\":37502},{\"end\":37554,\"start\":37531},{\"end\":37568,\"start\":37554},{\"end\":37581,\"start\":37568},{\"end\":37595,\"start\":37581},{\"end\":37610,\"start\":37595},{\"end\":37621,\"start\":37610},{\"end\":37635,\"start\":37621},{\"end\":37645,\"start\":37635},{\"end\":37663,\"start\":37645},{\"end\":37676,\"start\":37663},{\"end\":37692,\"start\":37676},{\"end\":37705,\"start\":37692},{\"end\":37720,\"start\":37705},{\"end\":37735,\"start\":37720},{\"end\":37754,\"start\":37735},{\"end\":37762,\"start\":37754},{\"end\":37911,\"start\":37898},{\"end\":37926,\"start\":37911},{\"end\":37938,\"start\":37926},{\"end\":37955,\"start\":37938},{\"end\":37969,\"start\":37955},{\"end\":37988,\"start\":37969},{\"end\":38008,\"start\":37988},{\"end\":38022,\"start\":38008},{\"end\":38037,\"start\":38022},{\"end\":38052,\"start\":38037},{\"end\":38070,\"start\":38052},{\"end\":38090,\"start\":38070},{\"end\":38108,\"start\":38090},{\"end\":38122,\"start\":38108},{\"end\":38135,\"start\":38122},{\"end\":38150,\"start\":38135},{\"end\":38166,\"start\":38150},{\"end\":38178,\"start\":38166},{\"end\":38194,\"start\":38178},{\"end\":38213,\"start\":38194},{\"end\":38224,\"start\":38213},{\"end\":38237,\"start\":38224},{\"end\":38253,\"start\":38237},{\"end\":38493,\"start\":38481},{\"end\":38505,\"start\":38493},{\"end\":38519,\"start\":38505},{\"end\":38535,\"start\":38519},{\"end\":38890,\"start\":38876},{\"end\":38906,\"start\":38890},{\"end\":38918,\"start\":38906},{\"end\":38938,\"start\":38918},{\"end\":39336,\"start\":39322},{\"end\":39344,\"start\":39336},{\"end\":39355,\"start\":39344},{\"end\":39365,\"start\":39355},{\"end\":39377,\"start\":39365},{\"end\":39391,\"start\":39377},{\"end\":39399,\"start\":39391},{\"end\":39412,\"start\":39399},{\"end\":39420,\"start\":39412},{\"end\":39433,\"start\":39420},{\"end\":39574,\"start\":39559},{\"end\":39590,\"start\":39574},{\"end\":39604,\"start\":39590},{\"end\":39616,\"start\":39604},{\"end\":39875,\"start\":39866},{\"end\":39892,\"start\":39875},{\"end\":39903,\"start\":39892},{\"end\":39921,\"start\":39903},{\"end\":39935,\"start\":39921},{\"end\":39951,\"start\":39935},{\"end\":39964,\"start\":39951},{\"end\":39975,\"start\":39964},{\"end\":39988,\"start\":39975},{\"end\":40003,\"start\":39988},{\"end\":40018,\"start\":40003},{\"end\":40032,\"start\":40018},{\"end\":40131,\"start\":40119},{\"end\":40143,\"start\":40131},{\"end\":40154,\"start\":40143},{\"end\":40172,\"start\":40154},{\"end\":40188,\"start\":40172},{\"end\":40256,\"start\":40244},{\"end\":40271,\"start\":40256},{\"end\":40296,\"start\":40271},{\"end\":40651,\"start\":40637},{\"end\":40664,\"start\":40651},{\"end\":40678,\"start\":40664},{\"end\":40828,\"start\":40811},{\"end\":40844,\"start\":40828},{\"end\":40860,\"start\":40844},{\"end\":40878,\"start\":40860},{\"end\":40896,\"start\":40878},{\"end\":40911,\"start\":40896},{\"end\":40926,\"start\":40911},{\"end\":41081,\"start\":41064},{\"end\":41096,\"start\":41081},{\"end\":41491,\"start\":41474},{\"end\":41506,\"start\":41491},{\"end\":41520,\"start\":41506},{\"end\":41536,\"start\":41520},{\"end\":41551,\"start\":41536},{\"end\":41564,\"start\":41551},{\"end\":41581,\"start\":41564},{\"end\":41596,\"start\":41581},{\"end\":41614,\"start\":41596},{\"end\":41629,\"start\":41614},{\"end\":41718,\"start\":41704},{\"end\":41734,\"start\":41718},{\"end\":41747,\"start\":41734},{\"end\":41917,\"start\":41903},{\"end\":41930,\"start\":41917},{\"end\":41943,\"start\":41930},{\"end\":41961,\"start\":41943},{\"end\":42302,\"start\":42282},{\"end\":42315,\"start\":42302},{\"end\":42326,\"start\":42315},{\"end\":42341,\"start\":42326},{\"end\":42352,\"start\":42341},{\"end\":42367,\"start\":42352},{\"end\":42379,\"start\":42367},{\"end\":42392,\"start\":42379},{\"end\":42675,\"start\":42655},{\"end\":42686,\"start\":42675},{\"end\":42700,\"start\":42686},{\"end\":42718,\"start\":42700},{\"end\":42730,\"start\":42718},{\"end\":42944,\"start\":42927},{\"end\":42965,\"start\":42944},{\"end\":42982,\"start\":42965},{\"end\":42999,\"start\":42982},{\"end\":43013,\"start\":42999},{\"end\":43028,\"start\":43013},{\"end\":43046,\"start\":43028},{\"end\":43064,\"start\":43046},{\"end\":43078,\"start\":43064},{\"end\":43090,\"start\":43078},{\"end\":43110,\"start\":43090},{\"end\":43123,\"start\":43110},{\"end\":43139,\"start\":43123},{\"end\":43155,\"start\":43139},{\"end\":43169,\"start\":43155},{\"end\":43186,\"start\":43169},{\"end\":43195,\"start\":43186},{\"end\":43208,\"start\":43195},{\"end\":43370,\"start\":43357},{\"end\":43387,\"start\":43370},{\"end\":43396,\"start\":43387},{\"end\":43409,\"start\":43396},{\"end\":43424,\"start\":43409},{\"end\":43437,\"start\":43424},{\"end\":43448,\"start\":43437},{\"end\":43462,\"start\":43448},{\"end\":43474,\"start\":43462},{\"end\":43495,\"start\":43474},{\"end\":43510,\"start\":43495},{\"end\":43530,\"start\":43510},{\"end\":43543,\"start\":43530},{\"end\":43663,\"start\":43650},{\"end\":43672,\"start\":43663},{\"end\":43687,\"start\":43672},{\"end\":43699,\"start\":43687},{\"end\":43720,\"start\":43699},{\"end\":43735,\"start\":43720},{\"end\":43755,\"start\":43735},{\"end\":43768,\"start\":43755},{\"end\":43933,\"start\":43920},{\"end\":43944,\"start\":43933},{\"end\":43959,\"start\":43944},{\"end\":43970,\"start\":43959},{\"end\":43984,\"start\":43970},{\"end\":43999,\"start\":43984},{\"end\":44141,\"start\":44126},{\"end\":44154,\"start\":44141},{\"end\":44173,\"start\":44154},{\"end\":44188,\"start\":44173},{\"end\":44208,\"start\":44188},{\"end\":44221,\"start\":44208},{\"end\":44239,\"start\":44221},{\"end\":44251,\"start\":44239},{\"end\":44264,\"start\":44251},{\"end\":44281,\"start\":44264},{\"end\":44299,\"start\":44281},{\"end\":44312,\"start\":44299},{\"end\":44451,\"start\":44439},{\"end\":44462,\"start\":44451},{\"end\":44476,\"start\":44462},{\"end\":44762,\"start\":44751},{\"end\":44775,\"start\":44762},{\"end\":44792,\"start\":44775},{\"end\":44809,\"start\":44792},{\"end\":44824,\"start\":44809},{\"end\":44842,\"start\":44824},{\"end\":45226,\"start\":45211},{\"end\":45240,\"start\":45226},{\"end\":45253,\"start\":45240},{\"end\":45531,\"start\":45519},{\"end\":45541,\"start\":45531},{\"end\":45554,\"start\":45541},{\"end\":45566,\"start\":45554},{\"end\":45580,\"start\":45566},{\"end\":45592,\"start\":45580},{\"end\":45603,\"start\":45592},{\"end\":45615,\"start\":45603},{\"end\":45633,\"start\":45615},{\"end\":45651,\"start\":45633},{\"end\":45815,\"start\":45800},{\"end\":45831,\"start\":45815},{\"end\":45845,\"start\":45831},{\"end\":45860,\"start\":45845},{\"end\":46121,\"start\":46105},{\"end\":46136,\"start\":46121},{\"end\":46152,\"start\":46136},{\"end\":46168,\"start\":46152},{\"end\":46313,\"start\":46300},{\"end\":46322,\"start\":46313},{\"end\":46335,\"start\":46322},{\"end\":46348,\"start\":46335},{\"end\":46360,\"start\":46348},{\"end\":46378,\"start\":46360},{\"end\":46390,\"start\":46378},{\"end\":46410,\"start\":46390},{\"end\":46425,\"start\":46410},{\"end\":46438,\"start\":46425},{\"end\":46542,\"start\":46527},{\"end\":46561,\"start\":46542},{\"end\":46573,\"start\":46561},{\"end\":46588,\"start\":46573},{\"end\":46603,\"start\":46588},{\"end\":46618,\"start\":46603},{\"end\":46632,\"start\":46618},{\"end\":46648,\"start\":46632},{\"end\":46668,\"start\":46648},{\"end\":46683,\"start\":46668},{\"end\":46704,\"start\":46683},{\"end\":46721,\"start\":46704},{\"end\":46739,\"start\":46721},{\"end\":47122,\"start\":47108},{\"end\":47142,\"start\":47122},{\"end\":47156,\"start\":47142},{\"end\":47172,\"start\":47156},{\"end\":47245,\"start\":47231},{\"end\":47254,\"start\":47245},{\"end\":47267,\"start\":47254},{\"end\":47279,\"start\":47267},{\"end\":47293,\"start\":47279},{\"end\":47309,\"start\":47293},{\"end\":47375,\"start\":47366},{\"end\":47388,\"start\":47375},{\"end\":47399,\"start\":47388},{\"end\":47416,\"start\":47399},{\"end\":47432,\"start\":47416},{\"end\":47817,\"start\":47805},{\"end\":47830,\"start\":47817},{\"end\":47848,\"start\":47830},{\"end\":47857,\"start\":47848},{\"end\":47869,\"start\":47857},{\"end\":47882,\"start\":47869},{\"end\":47897,\"start\":47882},{\"end\":47917,\"start\":47897},{\"end\":47930,\"start\":47917},{\"end\":48117,\"start\":48098},{\"end\":48132,\"start\":48117},{\"end\":48288,\"start\":48271},{\"end\":48300,\"start\":48288},{\"end\":48314,\"start\":48300},{\"end\":48332,\"start\":48314},{\"end\":48345,\"start\":48332},{\"end\":48360,\"start\":48345},{\"end\":48378,\"start\":48360},{\"end\":48713,\"start\":48701},{\"end\":48724,\"start\":48713},{\"end\":48744,\"start\":48724},{\"end\":48757,\"start\":48744},{\"end\":48769,\"start\":48757},{\"end\":48781,\"start\":48769},{\"end\":48799,\"start\":48781},{\"end\":48808,\"start\":48799},{\"end\":48814,\"start\":48808},{\"end\":48950,\"start\":48935},{\"end\":48964,\"start\":48950},{\"end\":48980,\"start\":48964},{\"end\":49001,\"start\":48980},{\"end\":49017,\"start\":49001},{\"end\":49265,\"start\":49243},{\"end\":49281,\"start\":49265},{\"end\":49297,\"start\":49281},{\"end\":49318,\"start\":49297},{\"end\":49336,\"start\":49318},{\"end\":49354,\"start\":49336},{\"end\":49369,\"start\":49354},{\"end\":49381,\"start\":49369},{\"end\":49396,\"start\":49381},{\"end\":49412,\"start\":49396},{\"end\":49430,\"start\":49412},{\"end\":49446,\"start\":49430},{\"end\":49463,\"start\":49446},{\"end\":49471,\"start\":49463},{\"end\":49545,\"start\":49529},{\"end\":49559,\"start\":49545},{\"end\":49572,\"start\":49559},{\"end\":49589,\"start\":49572},{\"end\":49602,\"start\":49589},{\"end\":49615,\"start\":49602},{\"end\":49630,\"start\":49615},{\"end\":49648,\"start\":49630},{\"end\":49717,\"start\":49707},{\"end\":49735,\"start\":49717},{\"end\":49877,\"start\":49864},{\"end\":49893,\"start\":49877},{\"end\":49906,\"start\":49893},{\"end\":49923,\"start\":49906},{\"end\":49941,\"start\":49923},{\"end\":49954,\"start\":49941},{\"end\":49970,\"start\":49954},{\"end\":49981,\"start\":49970},{\"end\":49992,\"start\":49981},{\"end\":50010,\"start\":49992},{\"end\":50023,\"start\":50010},{\"end\":50037,\"start\":50023},{\"end\":50063,\"start\":50037},{\"end\":50074,\"start\":50063},{\"end\":50090,\"start\":50074},{\"end\":50102,\"start\":50090},{\"end\":50112,\"start\":50102},{\"end\":50129,\"start\":50112},{\"end\":50145,\"start\":50129},{\"end\":50160,\"start\":50145},{\"end\":50178,\"start\":50160},{\"end\":50184,\"start\":50178},{\"end\":50525,\"start\":50510},{\"end\":50539,\"start\":50525},{\"end\":50555,\"start\":50539},{\"end\":50569,\"start\":50555},{\"end\":50582,\"start\":50569},{\"end\":50601,\"start\":50582},{\"end\":50613,\"start\":50601},{\"end\":50758,\"start\":50745},{\"end\":50774,\"start\":50758},{\"end\":50787,\"start\":50774},{\"end\":50802,\"start\":50787},{\"end\":50813,\"start\":50802},{\"end\":50827,\"start\":50813},{\"end\":50846,\"start\":50827},{\"end\":50857,\"start\":50846},{\"end\":50866,\"start\":50857},{\"end\":50883,\"start\":50866},{\"end\":50899,\"start\":50883},{\"end\":50909,\"start\":50899},{\"end\":50923,\"start\":50909},{\"end\":50937,\"start\":50923},{\"end\":50951,\"start\":50937},{\"end\":50970,\"start\":50951},{\"end\":50986,\"start\":50970},{\"end\":50999,\"start\":50986},{\"end\":51017,\"start\":50999},{\"end\":51113,\"start\":51099},{\"end\":51122,\"start\":51113},{\"end\":51134,\"start\":51122}]", "bib_venue": "[{\"end\":38392,\"start\":38304},{\"end\":38746,\"start\":38657},{\"end\":39274,\"start\":39125},{\"end\":39794,\"start\":39784},{\"end\":40486,\"start\":40415},{\"end\":41352,\"start\":41247},{\"end\":42155,\"start\":42083},{\"end\":42598,\"start\":42519},{\"end\":45113,\"start\":44989},{\"end\":45471,\"start\":45384},{\"end\":46050,\"start\":45978},{\"end\":47041,\"start\":46914},{\"end\":47756,\"start\":47607},{\"end\":48599,\"start\":48497},{\"end\":49182,\"start\":49108},{\"end\":50419,\"start\":50325},{\"end\":51355,\"start\":51253},{\"end\":37164,\"start\":37160},{\"end\":37246,\"start\":37172},{\"end\":37766,\"start\":37762},{\"end\":38302,\"start\":38253},{\"end\":38642,\"start\":38555},{\"end\":38655,\"start\":38644},{\"end\":39100,\"start\":38958},{\"end\":39123,\"start\":39102},{\"end\":39489,\"start\":39458},{\"end\":39719,\"start\":39650},{\"end\":39782,\"start\":39721},{\"end\":39864,\"start\":39800},{\"end\":40192,\"start\":40188},{\"end\":40413,\"start\":40327},{\"end\":40732,\"start\":40693},{\"end\":40967,\"start\":40926},{\"end\":41245,\"start\":41125},{\"end\":41472,\"start\":41407},{\"end\":41803,\"start\":41774},{\"end\":42068,\"start\":41981},{\"end\":42081,\"start\":42070},{\"end\":42517,\"start\":42423},{\"end\":42859,\"start\":42807},{\"end\":43289,\"start\":43228},{\"end\":43355,\"start\":43298},{\"end\":43829,\"start\":43768},{\"end\":44051,\"start\":43999},{\"end\":44361,\"start\":44312},{\"end\":44579,\"start\":44476},{\"end\":44987,\"start\":44865},{\"end\":45369,\"start\":45282},{\"end\":45382,\"start\":45371},{\"end\":45731,\"start\":45676},{\"end\":45976,\"start\":45889},{\"end\":46224,\"start\":46193},{\"end\":46298,\"start\":46232},{\"end\":46912,\"start\":46770},{\"end\":47106,\"start\":47047},{\"end\":47229,\"start\":47178},{\"end\":47605,\"start\":47463},{\"end\":48197,\"start\":48150},{\"end\":48495,\"start\":48409},{\"end\":48699,\"start\":48646},{\"end\":49106,\"start\":49017},{\"end\":49241,\"start\":49189},{\"end\":49697,\"start\":49648},{\"end\":49796,\"start\":49735},{\"end\":50323,\"start\":50214},{\"end\":50662,\"start\":50613},{\"end\":50743,\"start\":50694},{\"end\":51251,\"start\":51165}]"}}}, "year": 2023, "month": 12, "day": 17}