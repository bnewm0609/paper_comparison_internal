{"id": 8194074, "updated": "2023-10-17 23:06:04.43", "metadata": {"title": "Studies in Astronomical Time Series Analysis. VI. Bayesian Block Representations", "authors": "[{\"first\":\"Jeffrey\",\"last\":\"Scargle\",\"middle\":[\"D.\"]},{\"first\":\"Jay\",\"last\":\"Norris\",\"middle\":[\"P.\"]},{\"first\":\"Brad\",\"last\":\"Jackson\",\"middle\":[]},{\"first\":\"James\",\"last\":\"Chiang\",\"middle\":[]}]", "venue": null, "journal": "The Astrophysical Journal", "publication_date": {"year": 2012, "month": 7, "day": 24}, "abstract": "This paper addresses the problem of detecting and characterizing local variability in time series and other forms of sequential data. The goal is to identify and characterize statistically significant variations, at the same time suppressing the inevitable corrupting observational errors. We present a simple nonparametric modeling technique and an algorithm implementing it - an improved and generalized version of Bayesian Blocks (Scargle 1998) - that finds the optimal segmentation of the data in the observation interval. The structure of the algorithm allows it to be used in either a real-time trigger mode, or a retrospective mode. Maximum likelihood or marginal posterior functions to measure model fitness are presented for events, binned counts, and measurements at arbitrary times with known error distributions. Problems addressed include those connected with data gaps, variable exposure, extension to piecewise linear and piecewise exponential representations, multi-variate time series data, analysis of variance, data on the circle, other data modes, and dispersed data. Simulations provide evidence that the detection efficiency for weak signals is close to a theoretical asymptotic limit derived by (Arias-Castro, Donoho and Huo 2003). In the spirit of Reproducible Research (Donoho et al. 2008) all of the code and data necessary to reproduce all of the figures in this paper are included as auxiliary material.", "fields_of_study": "[\"Physics\",\"Mathematics\"]", "external_ids": {"arxiv": "1207.5578", "mag": "2149620753", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1088/0004-637x/764/2/167"}}, "content": {"source": {"pdf_hash": "c0d1a69ea56e9bb2abd261e170ca551979c853be", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1207.5578v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1207.5578", "status": "GREEN"}}, "grobid": {"id": "25d9642100e138542ecc48647183364aa172f38a", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/c0d1a69ea56e9bb2abd261e170ca551979c853be.txt", "contents": "\nStudies in Astronomical Time Series Analysis. VI. Bayesian Block Representations\n6 Aug 2012\n\nJeffrey D Scargle \nSpace Science and Astrobiology Division\nPhysics Department\nDepartment of Mathematics and Computer Science\nThe Center for Applied Mathematics and Computer Science\nNASA Ames Research Center\nBoise State University\nSan Jos\u00e9 State University\nKavli Institute\nSLAC\n\n\nJay P Norris \nSpace Science and Astrobiology Division\nPhysics Department\nDepartment of Mathematics and Computer Science\nThe Center for Applied Mathematics and Computer Science\nNASA Ames Research Center\nBoise State University\nSan Jos\u00e9 State University\nKavli Institute\nSLAC\n\n\nBrad Jackson \nSpace Science and Astrobiology Division\nPhysics Department\nDepartment of Mathematics and Computer Science\nThe Center for Applied Mathematics and Computer Science\nNASA Ames Research Center\nBoise State University\nSan Jos\u00e9 State University\nKavli Institute\nSLAC\n\n\nJames Chiang \nSpace Science and Astrobiology Division\nPhysics Department\nDepartment of Mathematics and Computer Science\nThe Center for Applied Mathematics and Computer Science\nNASA Ames Research Center\nBoise State University\nSan Jos\u00e9 State University\nKavli Institute\nSLAC\n\n\nStudies in Astronomical Time Series Analysis. VI. Bayesian Block Representations\n6 Aug 2012time seriessignal detectiontriggerstransientsBayesian analysis 1\nThis paper addresses the problem of detecting and characterizing local variability in time series and other forms of sequential data. The goal is to identify and characterize statistically significant variations, at the same time suppressing the inevitable corrupting observational errors. We present a simple nonparametric modeling technique and an algorithm implementing it-an improved and generalized version of Bayesian Blocks[Scargle 1998]-that finds the optimal segmentation of the data in the observation interval. The structure of the algorithm allows it to be used in either a real-time trigger mode, or a retrospective mode. Maximum likelihood or marginal posterior functions to measure model fitness are presented for events, binned counts, and measurements at arbitrary times with known error distributions. Problems addressed include those connected with data gaps, variable exposure, extension to piecewise linear and piecewise exponential representations, multi-variate time series data, analysis of variance, data on the circle, other data modes, and dispersed data. Simulations provide evidence that the detection efficiency for weak signals is close to a theoretical asymptotic limit derived by[Arias-Castro, Donoho and Huo 2003]. In the spirit of Reproducible Research[Donoho et al. (2008)] all of the code and data necessary to reproduce all of the figures in this paper are included as auxiliary material.\n\nThe Data Analysis Setting\n\nThis paper describes a method for nonparametric analysis of time series data to detect and characterize structure localized in time.\n\nNonparametric methods seek generic representations, in contrast to fitting of models to the data. By local structure we mean light-curve features occupying sub-ranges of the total observation interval, in contrast to global signals present all or most of the time (e.g. periodicities) for which Fourier, wavelet, or other transform methods are more appropriate. The goal is to separate statistically significant features from the ever-present random observational errors. Although phrased in the time-domain the discussion throughout is applicable to measurements sequential in wavelength, spatial quantities, or any other other independent variable.\n\nThis setting leads to the following desiderata: The ideal algorithm would impose as few preconditions as possible, avoiding assumptions about smoothness or shape of the signal that place a priori limitations on scales and resolution. The algorithm should handle arbitrary sampling (i.e., not be limited to gapless, evenly spaced data) and large dynamic ranges in amplitude, time scale and signalto-noise. For scientific data mining applications and for objectivity, the method should be largely automatic. To the extent possible it should suppress observational errors while preserving whatever valid information lies in the data. It should be applicable to multivariate problems. It should incorporate variation of the exposure or instrumental efficiency during the measurement, as well auxiliary, extrinsic information, e.g. spectral or color information. It should be able to operate both retrospectively (analyze all the data after they are collected) and in a real-time fashion that triggers on the first significant variation of the signal from its background level.\n\nThe algorithm described here achieves considerable success in each of these desired features. In a simple and easy-to-use computational framework it represents the structure in the signal in a form handy for further analysis and the estimation of physically meaningful quantities. It includes an automatic penalty for model complexity, thus solving the vexing problems associated with model comparison in general and determining the order of the model in particular. It is exact, not a greedy 1 approximation as in [Scargle 1998].\n\n1 This term refers to algorithms that greedily make optimal improvements at each iteration 4 Versions of this algorithm have been used in various applications, such as [Qin et al. 2012, Norris Gehrels and Scargle 2010, Norris Gehrels and Scargle 2011, Way Gazis and Scargle 2011.\n\nThe following sections discuss, in turn, the basis of segmentation analysis ( \u00a71.1), the piecewise constant model adopted in this work ( \u00a71.2), extensions to piecewise linear and piecewise exponential models ( \u00a71.3), the types of data that the algorithm can accept ( \u00a7 \u00a71.5 and 1.6), data gaps ( \u00a71.7), exposure variations ( \u00a71.8), a parameter from the prior on the number of blocks ( \u00a71.9), generalities of optimal segmentation of data into blocks ( \u00a72), some error analysis ( \u00a72.8), a variety of block fitness functions ( \u00a73), and sample applications ( \u00a74). Appendices present some MatLab c code, some miscellaneous results, and details of other data modes, including dispersed data ( \u00a7C.8). Ancillary files are available providing scripts and data in order to reproduce all of the figures in this paper.\n\n\nOptimal Segmentation Analysis\n\nThe above considerations point toward the most generic possible nonparametric data model, and have motivated the development of data segmentation and change-point methods -see e.g. [\u00d2 Ruanaidh andFitzgerald 1996, Scargle 1998]. These methods represent the signal structure in terms of a segmentation of the time interval into blocks, with each block containing consecutive data elements satisfying some well defined criterion. The optimal segmentation is that which maximizes some quantitative expression of the criterion -for example the sum over blocks of a goodness-of-fit of a simple model of the data lying in each block.\n\nThese concepts and methods can be applied in surprisingly general, higher dimensional contexts. Here, however, we concentrate on one-dimensional data ordered sequentially with respect to time or some other independent variable. In this setting segmentation analysis is often called change-point detection, since it implements models in which a signal's statistical properties change discontinuously at discrete times but are constant in the segments between these change-points (see \u00a72.5).\n\nbut are not guaranteed to converge to a globally optimal solution.\n\n\nThe Piecewise Constant Model\n\nIt is remarkable that all of the desiderata outlined in the previous section can be achieved in large degree by optimal fitting of a piecewise-constant model to the data. The range of the independent variable (e.g. time) is divided into subintervals (here called blocks) generally unequal in size, in which the dependent variable (e.g. intensity) is modeled as constant within errors. Of all possible such \"step-functions\" this approach yields the best one by maximizing some goodness-of-fit measure.\n\nDefining the times ending one block and starting the next as change-points, the model of the whole observation interval contains these parameters:\n\n(1) N cp : the number of change-points\n\n(2) t cp k : the change-point starting block k (3) X k : the signal amplitude in block k for k = 1, 2, . . . N cp + 1. 2 The key idea is that the blocks can be treated independently, in the sense that a block's fitness depends on its data only. Our simple model for each block has effectively two parameters. The first represents the signal amplitude, and is treated as a nuisance parameter to be determined after the changepoints have been located. The second parameter is the length of the interval spanned by the block. (The actual start and stop times of this interval are needed for piecing blocks together to form the final signal representation, but not for the fitness calculation.)\n\n\nHow many blocks?\n\nA key issue is how to determine the number of blocks, N blocks = N cp + 1. Nonparametric analysis invariably involves controlling in one way or another the complexity of the estimated representation. Typically such regulation is considered a trade-off of bias and variance, often implemented by adjusting a smoothing parameter.\n\nBut smoothing is one of the very things we are trying to avoid. The discontinuities at the block edges are regarded as assets, not liabilities to be smoothed over. So rather than smooth we influence the number of blocks by defining a prior distribution for the number of blocks. Adjusting a parameter controlling the steepness of this prior establishes relative probabilities of smaller or larger numbers of blocks. In the usual fashion for Bayesian model selection in cases with high signal-to-noise N blocks is determined by the structure of the signal; with lower signal-to-noise the prior becomes more and more important. In short, we are regulating not smoothness but complexity, much in the way that wavelet denoising [Donoho and Johnstone 1998] operates without smoothing over sharp features as long as they are supported by the data. The adopted prior and the determination of its parameter are discussed in \u00a71.9 below.\n\nThis segmented representation is in the spirit of nonparametric approximation and not meant to imply that we believe the signal is actually discontinuous. The sometimes crude and blocky appearance of this model may be awkward in visualization contexts, but for deriving physically meaningful quantities it is not. Blocky models are broadly useful in signal processing [Donoho 1994] and have several motivations. Their simplicity allows exact treatment of various quantities, such as the likelihood. We can optimize or marginalize the rate parameters exactly, giving simple formulas for the fitness function (see \u00a73 and Appendix C, \u00a7C). And in many applications the estimated model itself is less important than quantities derived from it. For example, while smoothed plots of pulses within gamma-ray bursts make pretty pictures, one is really interested in pulse locations, lags, amplitudes, widths, rise and decay times, etc. All these quantities can be determined directly from the locations, heights and widths of the blocks -accurately and free of any smoothness assumptions.\n\n\nPiecewise Linear and Exponential Models\n\nSome researchers have applied segmentation methods with other block representations. For example piecewise linear models have been used in measuring similarity among time series and in pattern matching [Lin, Keogh, Lonardi and Chiu 2003] and to represent time series generated by non-linear processes [Tong 1990]. While such models may seem better than discontinuous step functions, 7 their improved flexibility is somewhat offset by added complexity of the model and its interpretation. Note further that if continuity is imposed at the change-points, a piecewise linear model has essentially the same number of degrees of freedom as does the simpler piecewise constant model.\n\nWe mention two such generalizations, one modeling the signal as linear in time across the block:\nx(t) = \u03bb(1 + a(t \u2212 t fid ))(1)\nand the second as exponential:\nx(t) = \u03bbe a(t\u2212t fid ) .(2)\nIn both cases \u03bb is the signal strength at the fiducial time t fid and the coefficient a determines the rate of variation over the block. Such models may be useful in spite of the caveats mentioned above and the added complexity of the block fitness functions. Hence we provide some details in Appendix C, \u00a7 \u00a7C.9 and C.10.\n\n\nHistograms\n\nFor event data the piecewise constant representation can be interpreted as a histogram of the measured values -one in which the bins are not fixed ahead of time and are free to be unequal in size as determined by the data. In this context the time order of the measurements is irrelevant. Once one determines the parameter in the prior on the number of bins, ncp_prior, one has an objective histogram procedure in which the number, individual sizes, and locations of the bins are determined solely and uniquely by the data.\n\n\nData Modes\n\nThe algorithms developed here can be used with a variety of types of data, often called data modes in instrumentation contexts. An earlier paper [Scargle 1998] described several, with formulas for the corresponding fitness functions. Here we discuss data modes in a broader perspective. It is required that the measurements provide sufficient information to determine which block they belong to and then to compute the model fitness function for the block (cf. \u00a72.3).\n\nAlmost any physical variable and any measurement scheme for it, discrete or quasi-continuous, can be accommodated. In the simple one dimensional case treated here, the independent variable is time, wavelength, or some other quantity. The data space is the domain of this variable over which measurements were made -typically an interval, possibly interrupted by gaps during which the measuring system was not operating.\n\nThe measured quantity can be almost anything that yields information about the target signal. The three most common examples emphasized here are: (a) times of events (often called time-tagged event data, or TTE), (b) counts of events in time bins, and (c) measurements of a quasi-continuous observable at a sequence of points in time. For the first two cases the signal of interest is the event rate, proportional to the probability distribution regulating events which occur at discrete times due to the nature of the astrophysical process and/or the way it is recorded. We call case (c) point measurements, not to be confused with point data (also called event data). These modes have much in common, as they all comprise measurements that can be at any time; what differentiates them is their statistics, roughly speaking Bernoulli, Poisson, and Gaussian (or perhaps some other) respectively.\n\nThe archetypal example of (a) is light collected by a telescope and recorded as a set of detection times of individual photons to study source variability. Case (b) is similar, but with the events collected into bins -which do not have to be equal or evenly spaced. Case (c) is common when photons are not detected individually, such as in radio flux measurements. In all cases it is useful to represent the measurements with data cells, typically one for each measurement (see \u00a72.2). In principle mixtures of cells from different data types can be handled, as described in the next section.\n\n\nMixed Data Modes\n\nOur algorithm can analyze mixtures of any data types within a single time series. For example the data stream could consist of arbitrary combinations of cells of the three types defined above -measured values, counts in bins and events -with or without overlap in time among the various data modes. In regions of overlap the block representation would be based on the combined data; otherwise it would represent block structures supported by the corresponding individual data modes. In such applications the cost function must refer to a common signal amplitude parameter, possibly including normalization factors to account for differences in the measurement processes.\n\nA related concept is that of multivariate time series, usually referring to concurrent observations from different telescopes. The distinction between this concept and mixed data modes is largely semantic. Hence we leave implimentation details to \u00a74.2.\n\n\nGaps\n\nIn some cases there are subintervals over which no data can be obtained. For example there may be random interruptions such as detector malfunction at unpredictable but known periods of time, or regular interruptions as the Earth periodically blocks the view of an object from an orbiting space observatory. (Of course this case is very different from intervals in which no events happened to be detected, due to low event rate, or in which one simply did not happen to make point measurements.\n\nSuch data gaps have a nearly invisible affect on the algorithm, fundamentally due to the fact that it operates locally in the time domain. For event data all that matters is the live time during the block, i.e. the time over which data could have been registered. Other than correcting the total time span of any putative block containing data gaps by subtracting the corresponding dead time, gaps can be handled by ignoring them. Operationally one simply treats the data right after a gap as immediately following the data right before it (and not delayed by the length of the gap). Think of this as squeezing the interval to eliminate the gaps, carrying out the analysis as if no gaps are present, and then un-doing the squeezing by restoring the original times. This procedure is valid because event independence means that the fitness of a block depends on only its total live-time and the events within it.\n\nFor event data this squeezing can be implemented by subtracting from each event time the sum of the lengths of all the preceding gaps. One small detail concerns the points just before and just after a gap. One might think their time intervals should be computed relative to the gap edges. But it follows from the nature of independent events (Appendix B, \u00a7B) that they can be computed as though the gap did not exist.\n\nThe only other subtlety lies in interpreting the model in and around gaps. There are two possibilities: a given gap (a) may lie completely within a block or (b) it may separate two blocks. Case (a) can be taken as evidence that the event rates before and after the gap are deemed the same within statistical fluctuations. Case (b) on the other hand implies that the event rate did change significantly.\n\nOf course the gaps must be restored for display and other postprocessing analysis. Think of this as un-squeezing the data so that all blocks appear at their correct locations in time. Keeping in mind that there is no direct information about what happened during unobserved intervals, plots should probably include some indication that rates within gaps are unknown or uncertain, such as by use of dotted lines or shading in the gap for case (a) or leaving the gap interval completely blank in case (b).\n\nFor the case of point measurements the situation is different. In one sense there are no gaps at all, and in another sense the entire observation interval consists of many gaps separating tiny intervals over which the measurements were actually made. One is hardpressed to make a statistical distinction between various reasons why there is not a measurement at a given time -e.g. detector and weather problems, or simply a choice as to how to allocate observing time (a choice that may even depend on the results of analyzing previous data). Basically the blocks in this case represent intervals where whatever measurements were made in the interval are consistent with a signal that is constant over that interval.\n\nNote that things would be different if one wanted to define a fitness function dependent on the total length of the block, not just its live time. This would arise for example if a prior on the block length were imposed. Such possibilities will not be discussed here.\n\n\nExposure Variations\n\nIn some applications the effective instrument response is not constant. The measurements then reflect true source variations modified by changes in overall throughput of the detection system. We use the term exposure for any such effect on the detected signale.g. detector efficiency, telescope effective area, beam pattern and 11 point spread function effects. Exposure can be quantified by the ratio of the expected signal with and without any such effects. It may be calculable from properties of the observing system, determined after the fact through some sort of calibration procedure, or a combination of the two. Here we assume that this ratio is known and expressed as a number e n , typically with 0 \u2264 e n \u2264 1, for data cell n.\n\nThe adjustment for exposure is simple, namely change the parameter representing the observed signal amplitude in the likelihood to what it would have been if the exposure had been unity. First compute the exposure e n for data cell n. Then increase by the factor 1/e n whatever quantity in the data cell represents the measured signal intensity. Specifically, for time-tagged event data this parameter is the reciprocal of the interval of the corresponding data cell: 1/\u2206t n (see eq. (20)), which is then replaced with 1/(e n \u2206t n ). For bin counts the bin size can be multiplied by e n or equivalently the count by 1/e n . For point measurements multiply the amplitude measurement by 1/e n (and adjust any observational error parameters accordingly). In all cases the goal is to represent the data as closely as possible to what it would have been if the exposure had been constant. Of course this restoration is not exact in individual cases, but is correct on average.\n\nFor TTE data the fact that interval \u2206t n as we define it in eq. (20) depends on the times of two different events (just previous to and just after the one under consideration) may seem to pose a problem. The exposures of these events will in general be different, so what value do we use for the given event? The comforting answer is that the only relevant exposure is that for the given event itself. For consider the interval from the previous to the current time, namely t n \u2212 t n\u22121 . Here t n\u22121 is regarded as simply a fiducial time and the distribution of this interval is given by eq. (47) with \u03bb the true rate adjusted by the exposure for event n, by the principle described in \u00a7B.5 just after this equation. Similarly by a timereversal invariance argument the distribution of the interval to the subsequent event, namely t n+1 \u2212 t n , also depends on only the same quantity. In summary event independence (Appendix C, \u00a7C) yields the somewhat counterintuitive fact that the probability distribution of \u2206t n = (t n+1 \u2212t n\u22121 )/2 of the interval surrounding event n depends on only the effective event rate for event n.\n\n\nPrior for the Number of Blocks\n\nEarlier work [Scargle 1998] did not assign an explicit prior probability distribution for the number of blocks, i.e. the parameter N blocks . This omission amounts to using a flat prior, but in many contexts it is unreasonable to assign the same prior probability to all values. In particular, in most settings it is much more likely a priori that N blocks << N than that N blocks \u2248 N. For this reason it is desirable to impose a prior that assigns smaller probability to a large number of blocks, and we adopt this geometric prior [Coram 2002]:\nP (N blocks ) = P 0 \u03b3 N blocks(3)\nfor 0 \u2264 N blocks \u2264 N, and zero otherwise since N blocks cannot be negative or larger than the number of data cells. The normalization constant P 0 is easily obtained, giving\nP (N blocks ) = 1 \u2212 \u03b3 1 \u2212 \u03b3 N +1 \u03b3 N blocks(4)\nThrough this prior the parameter \u03b3 influences the number of blocks in the optimal representation -a number of some importance since it affects the visual appearance of the representation and to a lesser extent the values of quantities derived from it. This form for the distribution dictates that finding k + 1 blocks is less likely by the constant factor \u03b3 than is finding k blocks. In almost all applications \u03b3 will be chosen < 1 to express that a smaller number of blocks is a priori more likely than a larger number. In principle the choice of a prior and the values of its parameters expresses one's prior knowledge in a specific problem. The convenient geometric prior adopted here has proven to be generic and flexible, and its parameterization is simple and straightforward. These properties are appropriate for a generic analysis tool meant for a wide variety of applications. One can think of selecting \u03b3 as a simple way of adjusting the amount of structure in the block representation of the signal. It is specifically not a smoothing parameter but is analogous to one.\n\nThe expected number of blocks follows from eq. (3)\n< N blocks > = P 0 N N blocks =0 N blocks \u03b3 N blocks = N\u03b3 N +1 + 1 \u03b3 N +1 \u2212 1 + 1 1 \u2212 \u03b3(5)\nNote that the actual number of blocks is a discontinuous, monotonic function of \u03b3, and because its jumps can be > 1 it is not generally possible to force a prescribed number by adjusting this parameter. The above prior is not the only one possible and different forms may be useful in some applications. But Eq. (3) is very convenient to implement, since with the fitness equal to the log of the posterior, one only needs to subtract the constant \u2212log \u03b3 (called ncp_prior in the MatLab code and in the discussion of computational issues below) from the fitness of each block. Determining the value to use in applications is discussed in \u00a72.7 below.\n\n\nOptimum Segmentation of Data on an Interval\n\nPiecewise constant modeling of sequential measurements on a time interval T is most conveniently implemented by seeking an optimal partition of the ordered set of data cells within T . In this special case of segmentation the segments cover the whole set with no overlap between them (Appendix B). Segmentations with overlap are possible, for example in the case of correlated measurements, but are not considered here. One can envision our quest for the optimal segmentation as nothing more than finding the best step-function, or piecewise constant model, fit to the data -defined by maximizing a specific fitness measure as detailed in \u00a72.4. We introduce our algorithm in a somewhat abstract setting because the formalism developed here applies to other data analysis problems beyond time series analysis. It implements Bayesian Blocks or other 1D segmentation ideas for any model fitness function that satisfies a simple additivity condition. It improves the previous approximate segmentation algorithm [Scargle 1998] by achieving an exact, rigorous solution of the multiple change-point problem, guaranteed to be a global optimum, not just a local one.\n\nThe rest of this section describes how the model is structured for effective solution of this problem, while details on the quantity to be optimized are deferred to the next section.\n\n\nPartitions\n\nPartitions of a time interval T are simply collections of non-overlapping blocks (defined below in \u00a72.3), defined by specifying the number of its blocks and the block edges:\nP(I) \u2261 {N blocks ; n k , k = 1, 2, 3, . . . N blocks } .(6)\nwhere the n k are indices of the data cells ( \u00a72.2) defining times called change-points (see \u00a72.5).\n\nAppendix B gives a few mathematical details about partitions, including justification of the restriction of the change-points to coincide with data points and the result that the number of possible partitions (i.e. the number of ways N cells can be arranged in blocks) is 2 N . This number is exponentially large, rendering an explicit exhaustive search of partition space utterly impossible for all but very small N. Our algorithm implicitly performs a complete search of this space in time of order N 2 , and is practical even for N \u223c 1, 000, 000, for which approximately 10 300,000 partitions are possible. The beauty of the algorithm is that it finds the optimum among all partitions without an exhaustive explicit search, which is obviously impossible for almost any value of N arising in practice.\n\n\nData Cells\n\nFor input to the algorithm the measurements are represented with data cells. For the most part there is one cell for each measurement, although in the case of TTE data two or more events with identical time-tags may be combined into a single cell. A convenient data structure is an array containing the cells ordered by the measurement times.\n\nSpecification of the contents of the cells must meet two requirements. First they must include time information allowing determination of which cells lie in a block given its start and stop times. Post-processing steps such as plotting the blocks may in addition use the actual times, either absolute or relative to a specified origin.\n\nThe other requirement is that the fitness of a block can be computed from the contents of all the cells in it ( \u00a72.4, \u00a73). For the three standard cases the relevant data are roughly speaking: (a) intervals between events ( \u00a73.1), (b) bin sizes, locations and counts ( \u00a73.2), and (c) measured values augmented by a quantifier of measurement uncertainty ( \u00a73.3). These same quantities enable construction of the resulting step function for post-processing steps such as computing 15 signal parameters.\n\n\nBlocks of Cells\n\nA block is any set of consecutive cells, either an element of the optimal representation or a candidate for it. Each block represents a subinterval (within the full range of observation times) over which the amplitude of the signal can be estimated from the contents of its cells ( \u00a72.2). A block can be as small as one cell or as large as all of the cells.\n\nOur time series model consists of a set of blocks partitioning the observations. All model parameters are constant within each block but undergo discrete jumps at the change-points ( \u00a72.5) marking the edges of the blocks. The model is visualized by plotting rectangles spanning the intervals covered by the blocks, each with height equal to the signal intensity averaged over the interval. The concept of fitness of a block is fundamental to everything else in this paper. As we will see in the next section the fitness of a partition is the sum of the fitnesses of the blocks comprising it.\n\n\nFitness of Blocks and Partitions\n\nSince the goal is to represent the data as well as possible within a given class of models, we maximize a quantity measuring the fitness of models in the given class, here the class of all piecewise constant models. Alternatively, one can minimize an error measure. Both operations are called optimization. The algorithm relies on the fitness being block-additive, i.e.\nF [P(T )] = N blocks k=1 f (B k ) ,(7)\nwhere F [P(T )] is the total fitness of the partition P of interval T , and f (B k ) is the fitness of block k. The latter can be any convenient measure of how well a constant signal represents the data within the block. Typically additivity results from independence of the observational errors. We here ignore the possibility of correlated errors, which could make the fitness of one block depend on that of its neighbors. Remember correlation of observational errors is quite separate from correlations in the signal itself.\n\nAll model parameters are marginalized except the n k specifying block edges. Then the total fitness depends on only these remaining parameters -i.e. on the detailed specification of the partition by indicating which cells fall in each of its blocks. The best model is found by maximizing F over all possible such partitions.\n\n\nChange-points\n\nIn the time series literature a point at which a statistical model undergoes an abrupt transition, by one or more of its parameters jumping instantaneously to a new value, is called a change-point. This is exactly what happens at the edges of the blocks in our model. In principle change-points can be at arbitrary times. However, following the data cell representation and without any essential loss of generality they can be restricted to coincide with a data point (Appendix B; \u00a7B). A few comments on notation are in order. We take blocks to start at the data cell identified by the algorithm as a change-point and to end at the cell previous to the subsequent change-point. A slight variation of this convention is discussed below in \u00a74.4 in connection with allowing the possibility of empty blocks in the context of event data. One might adopt other conventions, such as apportioning the change-point data cell to both blocks, but we do not do so here. Even though the first data cell in the time series always starts the first block, our convention is that it is not considered a change-point. In the code presented here the first change-point marks the start of the second block. For k > 1 the k-th block starts at index n k\u22121 and ends at n k \u2212 1. The first block always starts with the very first data cell. The last block always terminates with the very last data cell. If the last cell is a change-point, it defines a block consisting of only that one cell. The set of change-points is empty if the best model consists of a single block, meaning that the time series is sensibly constant over the whole observation interval. The number of blocks is one greater than the number of change-points.\n\n\nThe Algorithm\n\nWe now outline the basic algorithm yielding the desired optimum partitions. The details of this dynamic programming 3 approach [Bellman 1961, Hubert, Arabie, and Meulman 2001, Dreyfus 2002 are in [Jackson et al. 2005]. It follows the spirit of mathematical induction: beginning with the first data cell, at each step one more cell is added. The analysis makes use of results stored from all previous steps. Remarkably the algorithm is exact and yields the optimal partition of an exponentially large space in time of order N 2 . The iterations normally continue until the whole interval has been analyzed. However its recursive nature allows the algorithm to function in a trigger mode, halting when the first change-point is detected ( \u00a74.3).\n\nLet P opt (R) denote the optimal partition of the first R cells. In the starting case R = 1 the only possible partition (one block consisting of the first cell by itself) is trivially optimal. Now assume we have completed step R, identifying the optimal partition P opt (R).\n\nAt this (and each previous) step store the value of optimal fitness itself in array best and the location of the last change-point of the optimal partition in array last.\n\nIt remains to show how to obtain P opt (R+1). For some r consider the set of all partitions (of these first R + 1 cells) whose last block starts with cell r (and by definition ends at R+1). Denote the fitness of this last block by F (r). By the subpartition result in Appendix B the only member of this set that could possibly be optimal is that consisting of P opt (r \u2212 1) followed by this last block. By the additivity in Eq. (7) the fitness of said partition is the sum of F (r) and the fitness of P opt (r \u2212 1) saved from a previous step:\nA(r) = F (r) + { 0 r = 1 best(r \u2212 1), r = 2, 3, . . . , R + 1 .(8)\nA (1) is the special case where the last block comprises the entire data array and thus no previous fitness value is needed. Over the indicated range of r this equation expresses the fitness of all partitions P(R+1) that can possibly be optimal. Hence the value of r yielding the optimal partition P opt (R + 1) is the easily computed value maximizing A(r):\nr opt = argmax[A(r)] .(9)\nAt the end of this computation, when R = N, it only remains to find the locations of the change-points of the optimal partition. The needed information is contained in the array last in which we have stored the index r opt at each step. Using the corollary in Appendix B it is a simple matter to use the last value in this array to determine the last change-point in P opt (N), peel off the end section of last corresponding to this last block, and repeat. That is to say, the set of values cp 1 = last(N); cp 2 = last(cp 1 \u22121); cp 3 = last(cp 2 \u22121); . . . (10) are the index values giving the locations of the change-points, in reverse order. Note that the positions of the change-points are not necessarily fixed until the very last iteration, although in practice it turns out that they become more or less \"frozen\" once a few succeeding change-points have been detected. MatLab 4 code for optimal partitioning of event data is given in Appendix A.\n\n\nFixing the Parameter in the Prior Distribution for N blocks\n\nAs mentioned in \u00a71.9 the output of the algorithm is dependent on value of the parameter \u03b3, characterizing the assumed prior distribution for the number of blocks, eq. (3). In many applications the results are rather insensitive to the value as long as the signal-tonoise ratio is even moderately large. Nevertheless extreme values of this parameter give bad results in the form of clearly too few or too many blocks. In any case one must select a value to use in applications. This situation is much like that of selecting a smoothing parameter in various data analysis applications, e.g. density estimation. In such contexts there is no perfect choice but instead a tradeoff between bias and variance. Here the tradeoff is between a conservative choice not fooled by noise fluctuations but potentially missing real changes, and a liberal choice better capturing changes but yielding some false detections. Several approaches have proven useful in elucidating this tradeoff. Merely running the algorithm with a few different values can indicate a range over which the block representation is reasonable and not very sensitive to the parameter value (cf. Fig. 1).\n\nThe discussion of fitness functions below in \u00a73 gives implementation details of an objective method for calibrating ncp_prior as a function of the number of data points. It is based on relating this parameter to the false positive probability -that is, the relative frequency with which the algorithm falsely reports detection of a change-point in data with no signal present. It is convenient to use the complementary quantity\np 0 \u2261 1 \u2212 false positive probability .(11)\nThis number is the frequency with which the algorithm correctly rejects the presence of a change-point in pure noise. Therefore it is also the probability that a change-point reported by the algorithm with this value of ncp_prior is indeed statistically significant -hence we call it the correct detection rate for single change-points. The needed ncp_prior-p 0 relationship is easily found by noting that the rates of correct and incorrect responses to fluctuations in simulated pure noise can be controlled by adjusting the value of ncp_prior. The procedure is: generate a synthetic pure-noise time series; apply the algorithm for a range of ncp_prior; select the smallest value that yields false detection frequency equal or less than the desired rate, such as .05. The values of ncp_prior determined in this way are averaged over a large number of realizations of the random data. The result depends on only the number of data points 20 and the adopted value of p 0 :\nncp prior = \u03c8(N, p 0 ) .(12)\nResults from simulations of this kind are given below for the various fitness functions in \u00a7 \u00a73.1, 3.2, and 3.3. We have no exact formulas, but rather fits to these numerical simulations.\n\nThe above discussion is useful in the simple problem of deciding whether or not a signal is present above a background of noisy observations. In other words we have a procedure for assigning a value of ncp_prior that results in an acceptable frequency of spurious change-points, or false positives, when searching for a single statistically significant change. Real-time triggering on transients ( \u00a74.3) is an example of this situation, as is any case where detection of a single change-point is the only issue in play.\n\nBut elucidating the shape of an actual detected signal lies outside the scope of the above procedure, since it is based on a pure noise model. A more general goal is to limit the number of both false negatives and false positives in the context of an extended signal. The choice of the parameter value here depends on the nature of the signal present and the signal-to-noise level. One expects that somewhat larger values of ncp_prior are necessary to guard against corruption of the estimate of the signal's shape due to errors at multiple change-points.\n\nThis idea suggests a simple extension of the above procedure. Assume that a value of p 0 , the probability of correct detection of an individual change-point, has been adopted and the corresponding value of ncp_prior determined with pure noise simulations as outlined above and expressed in eq. (12). For a complex signal our goal is correct detection of not just one, but several change-points, say N cp in number. The trick is to treat each of them as an independent detection of a single change-point with success rate p 0 . The probability of all N cp successes follows from the law of compound probabilities:\np(N cp ) = p Ncp 0 .(13)\nThere are problems with this analysis in that the following are not true:\n\n(1) Change-point detection in pure noise and in a signal are the same.\n\n\n21\n\n(2) The detections are independent of each other.\n\n(3) We know the value of N cp .\n\nAll of these statements would have to be true for eq. (13) to be rigorously valid. We propose to regard the first two as approximately true and address the third as follows: Decide that the probability of correctly detecting all the change-points should be at least a high as some value p * , such as 0.95. Apply the algorithm using the value of ncp_prior = \u03c8(N, p * ) given by the pure noise simulation. Use eq. (13) and the number of change-points thus found to yield a revised value ncp prior = \u03c8(N, p 1/Ncp * ) .\n\nStopping when the iteration produces no further modification of the set of change-points, one has the recommended value of ncp_prior. This ad hoc procedure is not rigorous, but it establishes a kind of consistency and has proven useful in all the cases where we have tried it, e.g. Scargle 2010, Norris Gehrels and.  Fig. 1 shows another approach, based on cross-validation of the data being analyzed (cf. [Hogg 2008]). This study uses the collec-22 tion of raw TTE data at the BATSE web site ftp://legacy.gsfc.nasa.gov/compton/data/batse/ascii_data/batse_tte/.\n\nThe files for each of 532 GRBs contain time tags for all photons detected for that burst. The energy and detector tags in the data files were not used here, but \u00a74.1 shows an example using the former. An ordinary 256-bin histogram of all photon times for each of 532 GRBs was taken as the true signal for that burst. Eight random subsamples smaller by a factor of 8 were analyzed with the algorithm using the fitness in eq. (19). The average RMS error between these block representations (evaluated at the same 256 time points) and the histogram is roughly flat over a broad range. While this illustration with a relatively homogeneous data set should obviously not be taken as universal, the general behavior seen here -determination of a broad range of nearly equally optimal values of ncp_prior -is characteristic of a wide variety of situations.\n\n\nAnalysis of Variance\n\nAssessment of uncertainty is an important part of any data analysis procedure. The observational errors discussed throughout this paper are propagated by the algorithm to yield corresponding uncertainties in the block representation and its parameters. The propagation of stochastic variability in the astronomical source is a separate issue, called cosmic variance, and is not discussed here.\n\nSince the results here comprise a complete function defined by a variable number of parameters, quantification of uncertainty is considerably more intricate than for a single parameter. In particular one must specify precisely which of the block representation's aspects is at issue. Here we discuss three: (a) the full block representation, (b) the very presence of the change-points themselves, and (c) locations of change-points.\n\nA straightforward way to deal with (a) is by bootstrap analysis. As described in [Efron and Tibshirani 1998] for time series data this procedure is rather complicated in general. However resampling of event data in the manner appropriate to the bootstrap is trivial. The procedure is to run the algorithm on each of many bootstrap samples and evaluate the resulting block representations at a common set of evenly spaced times. In this way models with different numbers and locations of change-points can be added, yielding means and variances for the estimated block light curves. The bootstrap variance is an indicator of light curve uncertainty. In addition comparison of the bootstrap mean with the block representation from the actual data adds information about modeling bias. The former is rather like a model average in the Bayesian context. This average typically smoothes out the discontinuous block edges present in any one representation. In some applications the bootstrap mean may be more useful than the block representation.\n\nThis method does not seem to be useful for studying uncertainty in the change-points themselves, in particular their number, presumably because the duplication of data points due to the replacement feature of the resampling yields excess blocks (but with random locations and small amplitude variance, and therefore with little effect on the mean light curve).\n\nBy (b) is meant an assessment of the statistical significance of the identification of a given change point. For a given change-point we suggest quantification of this uncertainty by evaluating the ratio of the fitness functions for the two blocks on either side of that changepoint to that of the single block that would exist if the change-point were not there. The corresponding difference of the (logarithmic) fitness values should be adjusted by the value of the constant parameter ncp_prior, for consistency with the way fitness is computed in the algorithm.\n\nFinally, (c) is easily addressed in an approximate way by fixing all but one change-point and computing fitness as a function of the location of that change-point. This assessment is approximate because by fixing the other change-points because it ignores inter-changepoint dependences. One then converts the run of the fitness function with change-point location into a normalized probability distribution, giving comprehensive statistical information about that location (mean, variance, confidence interval, etc.)\n\nSample results of all of these uncertainty measures in connection with analysis of a gamma-ray burst light curve are shown below in \u00a74.1, especially Fig. 8.\n\n\nMultivariate Time Series\n\nOur algorithm's intentionally flexible data interface not only allows processing a wide variety of data modes but also facilitates joint 24 analysis of mode combinations. This feature allows one to obtain the optimal block representation of several concurrent data streams with arbitrary modes and sample times. This analysis is joint in the sense that the change-points are constrained to be at the same times for all the input series; in other words the block edges for all of the input data series line up. The representation is optimal for the data as a whole but not for the individual time series.\n\nTo interpret the result of a multivariate analysis one can study the blocks in the different series in two ways: (a) separately, but with the realization that the locations of their edges are determined by all the data; or (b) in a combined representation. The latter requires that there be a meaningful way to combine amplitudes. For example the plot of a joint analysis of event and binned data could simply display the combined event rate for each block, perhaps adjusting for exposure differences. For other modes, such as photon events and radio frequency fluxes, a joint display would have to involve a spectral model or some sort of relative normalization. The example in \u00a74.2 below will help clarify these issues.\n\nThe idea extending the basic algorithm to incorporate multiple time series is simple. Each datum in any mode has a time-tag associated with it -for example the event time, the time of a bin center, or the time of a point measurement. The joint change-points are allowed to occur at any one of these times. Hence the times from all of the separate data streams are collected together into a single ordered array; the ordering means that the times -as well as the measurement data -from the different modes are interleaved. The cartoon in Fig. 2 shows how the individual concatenated times and data series are placed in separate blocks in a matrix (top) and then redistributed (bottom) by ordering the combined times. Then the fitness function for a given data series can be obtained from the corresponding data slice (e.g. the horizontal dashed line in the figure, for Series #2). The zero entries in these slices (indicated by white space in the figure) are such that the fitness function for data from each series is evaluated for only the appropriate data and mode combination. The overall fitness is then simply the sum of those for the several data series. The details of this procedure are described in the code provided in Appendix A ( \u00a7A). \n\n\nOrdered Times\n\nFigure 2: Cartoon depicting an example of how three data series are first concatenated into a matrix (top) and then redistributed by ordering the combined time-tags (bottom). The cost functions for the series can then be computed from the data in horizontal slices (e.g. dashed line) and combined, allowing the change-points to be at any of the time tags.\n\n\nComparison with Theoretical Optimal Detection Efficiency\n\nHow good is the algorithm at extracting weak signals in noisy data? This section gives evidence that it achieves detection sensitivity closely approaching ideal theoretical limits. The formalism in [Arias-Castro, Donoho and Huo 2 treats detection of geometric objects in data spaces of arbitrary dimension using multiscale methods. The one dimensional special case in \u00a7II of this reference is essentially equivalent to our problem of detecting a single block in noisy time series. Given N measurements normalized so that the observational errors \u223c N(0, \u03c3) (normally distributed with zero mean and variance \u03c3 2 ), these authors show that the threshold for detection is\nA 1 = \u03c3 2 logN .(15)\nThis result is asymptotic (i.e. valid in the limit of large N). It is valid for a frequentist detection strategy based on testing whether 26 the maximum of the inner product of the model with the data exceeds the quantity in eq. (15) or not. These authors state \"In short, we can efficiently and reliably detect intervals of amplitude roughly \u221a 2 logN , but not smaller.\" More formally the result is that asymptotically their test is powerful for signals of amplitude greater than A 1 and powerless for weaker signals. It is of interest to see how well our algorithm stacks up against these theoretical results, since the two analysis approaches (matched filter test statistic vs. Bayesian model selection) are fundamentally different. Consider a simulation consisting of normally distributed measurements at arbitrary times in an interval. These variates are taken to be zero-mean-normal, except over an unknown sub-interval where the mean is a fixed constant. In this experiment the events are evenly spaced, but only their order matters, so the results would be the same for arbitrary spacing of the events. Fig. 3 shows synthetic data for four simulated realizations with different values for this constant. The solid line is the Bayesian blocks representation, using the posterior in Eq. (102). For the small amplitudes in the first two panels no change-points are found; these weak signals are completely missed. In the other two panels the signals are detected and approximately correctly represented -with small errors in the locations of the change-points.   4 reports some results of detection of the same step-function process shown in Fig. 3, averaged over many different realizations of the observational error process and for several different values of N. The lines are plots of a simple error metric (combing the errors in the number of change-points and their locations) as a function of the amplitude of the test signal. The left panel is for the case where the number of points in the putative block is held fixed, whereas the right panel is this number is taken to be proportional to N, sometimes a more realistic situation. We have adopted the following definition for the threshold in this case:\nA 2 = 8\u03c3 2 logN N .(16)\nThis formula is consistent with adjusting the normalized width in [Arias-Castro, Donoho and Huo 2003] with a factor N; 8 is an arbitrary factor for plotting.\n\nOur method yields small errors when the signal amplitude is on the order or even somewhat smaller than the limit stated by [Arias-Castro, Donoho and Huo 2003], showing that we are indeed close to their theoretical limit. The main difference here is that our results are for specific values of N and the theoretical results are asymptotic in N.\n\n\nBlock Fitness Functions\n\nTo complete the algorithm all that remains is to define the model fitness function appropriate to a particular data mode. By equation (7) it is sufficient to define a block fitness function, which can be any convenient measure of how well a constant signal represents the data in the block. Naturally this measure will depend on all data in the block and not on any outside it. As explained in \u00a72.4 it cannot depend on any model parameters other than those specifying the locations of the block edges. In practice this means that block height (signal amplitude) must somehow be eliminated as a parameter. This can be accomplished, for example, by taking block fitness to be the relevant likelihood either maximized or marginalized with respect to this parameter. Either choice yields a quantity good for comparing alternative models, but not necessarily for assessing goodness-of-fit of a single model. Note that these measures as such do not satisfy the additivity condition Eq. (7). As long as the cell measurement errors are independent of each other the likelihood of a string of blocks is the product of the individual values, but not the required sum. But simply taking the logarithm yields the necessary additivity.\n\nThere is considerable freedom in choosing fitness functions to be used for a given type of data. The examples described here have proven useful in various circumstances, but the reader is encouraged to explore other block-additive functions that might be more appropriate in a given application. For all cases considered in this paper the fitness function depends on data in the block through summary parameters called sufficient statistics, capturing the statistical behavior of the data. If these parameters are sums of quantities defined on the cells the computations are simplified; however this condition is not essential.\n\nTwo types of factors in the block fitness can be ignored. A constant factor C appearing in the likelihood for each data cell yields an overall constant term in the derived logarithmic fitness function for the whole time series, namely N log C. Such a term is independent of all model parameters and therefore irrelevant for the model comparison in the optimization algorithm. In addition, while a term in the block fitness that has the same value for each block does affect total model fitness, it contributes a term proportional to the number of blocks, and which therefore can be absorbed into the parameter derived from the prior on the number of blocks (cf. \u00a71.9).\n\nMany of the data modes discussed in the following subsections were operative in the Burst and Transient Source Experiment (BATSE) experiment on the NASA Compton Gamma Ray Observatory (GRO), the Swift Gamma-Ray Burst Mission, the Fermi Gamma Ray Space Telescope, and many x-ray and other high-energy observatories. They are also relevant in a wide range of other applications.\n\nIn the rest of this section we exhibit expressions that serve as practical and reliable fitness functions for the three most common data modes: event data, binned data, and point measurements with normal errors. Some refinements of this discussion and some other less common data modes are discussed in Appendix C, \u00a7C. \n\n\nEvent Data\n\nFor series of times of discrete events it is natural to associate one data cell ( \u00a72.2) with each event. The following derivation of the appropriate block fitness will elucidate exactly what information the cells must contain to allow evaluation of the fitness for the full multi-block model.\n\nIn practice the event times are integer multiples of some small unit ( \u00a7C.1) but it is often convenient to treat them as real numbers on a continuum. For example the fitness function is easily obtained starting with the unbinned likelihood known as the Cash statistic ([Cash 1979]; a thorough discussion is in [Tompkins 1999]). If M(t, \u03b8) is a model of the time dependence of a signal the unbinned log-likelihood is\nlogL(\u03b8) = n logM(t n , \u03b8) \u2212 M(t, \u03b8)dt ,(17)\nwhere the sum is over the events and \u03b8 represents the model parameters. The integral is over the observation interval and is the expected number of events under the model. Our block model is constant with a single parameter, M(t, \u03bb) = \u03bb, so for block k\nlogL (k) (\u03bb) = N (k) log\u03bb \u2212 \u03bbT (k) ,(18)\nwhere N (k) is the number of events in block k and T (k) is the length of the block. The maximum of this likelihood is at \u03bb = N (k) /T (k) , yielding\nlog L (k) max + N (k) = N (k) ( logN (k) \u2212 logT (k) ) .(19)\nThe term N (k) is taken to the left side because its sum over the blocks is a constant (N, the total number of events) that is modelindependent and therefore irrelevant. Moreover note that changing the units of time, say by a scale factor \u03b1, changes the log-likelihood by \u2212N (k) log(\u03b1), irrelevant for the same reason. This felicitous property holds for other maximum likelihood fitness functions and removes what would otherwise be a parameter of the optimization. This effective scale invariance and the simplicity of eq. (19) make its block sum the fitness function of choice to find the optimum block representation of event data. A possible exception is the case where detection of more than one event at a given time is not possible, e.g. due to detector, deadtime, in which case the fitness function in Appendix C, \u00a7C.2 may be more appropriate.\n\nIt is now obvious what information a cell must contain to allow evaluation of the sufficient statistics N (k) and T (k) by summing two quantities over the cells in a block. First it must contain the number of events in the cell. (This is typically one, but can be more depending on how duplicate time tags are handled; see the code section in Appendix A, \u00a7A, dealing with duplicate time-tags, or ones that are so close that it makes sense to treat them as identical). Second, it must contain the interval\n\u2206t n = (t n+1 \u2212 t n\u22121 )/2 ,(20)\nrepresenting the contribution of cell n to the length of the block. This interval contains all times closer to event n than to any other. It is defined by the midpoints between successive events, and generalizes to data spaces of any dimension, where it is called the Voronoi tessellation of the data points, [Okabe, Boots, Sugihara and Chiu 2000, Scargle 2001a, Scargle 2001c). Because 1/\u2206t n can be regarded as an estimate of the local event rate at time t n , it is natural to visualize the corresponding data cell as the unit-area rectangle of width \u2206t n and height 1/\u2206t n . These ideas lead to the comment in \u00a71.8 that the event-by-event adjustment for exposure can be implemented by shrinking \u2206t n by the exposure factor e n . It is interesting to note that the actual locations of the (independent) events within their block do not matter. The fitness function depends on only the number of events in the block, not their locations or the intervals between them. This result flows directly from the nature of the underlying independently distributed, or Poisson, process (see Appendix B, \u00a7B).\n\nWe conclude this section with evaluation of the calibration of ncp_prior from simulations of signal-free observational noise as described in \u00a72.7. The results of extensive simulations for a range of values of N and the adopted false positive rate p 0 introduced in Eq. (11) were found to be well fit with the formula ncp prior = 4 \u2212 73.53p 0 N \u2212.478\n\nFor example, with p 0 = .01 and N = 1, 000 this formula gives ncp_prior = 3.97.\n\n\nBinned Event Data\n\nThe expected count in a bin is the product \u03bbeW of the true event rate \u03bb at the detector, a dimensionless exposure factor e ( \u00a71.8), and the width of the bin W . Therefore the likelihood for bin n is given by the Poisson distribution\nL n = (\u03bbe n W n ) Nn e \u2212\u03bbenWn N n ! ,(22)\nwhere N n is the number of events in bin n, \u03bb is the actual event rate in counts per unit time, e n is the exposure averaged over the bin, and W n is the bin width in time units. Defining bin efficiency as w n \u2261 e n W n , the likelihood for block k is the product of the likelihoods of all its bins:\nL (k) = M (k) n=1 L n = \u03bb N (k) e \u2212\u03bbw (k) .(23)\nHere M (k) is the number of bins in block k,\nw (k) = M (k) n=1 w n(24)\nis the sum of the bin efficiencies in the block, and\nN (k) = M (k) n=1 N n(25)\nis the total event count in the block. The factor (e n W n ) Nn /N n ! has been discarded because its product over all the bins in all the blocks is a constant (depending on the data only) and therefore irrelevant to model fitness. The log-likelihood is\nlogL (k) = N (k) log\u03bb \u2212 \u03bbw (k) ,(26)\nidentical to eq. (18) with w (k) playing the role of T (k) , a natural association since it is an effective block duration. Moreover in retrospect it is understandable that unbinned and binned event data have the same fitness function, especially in view of the analysis in \u00a7C.1 where ticks are allowed to contain more than one event and are thus equivalent to bins. In addition the way variable exposure is treated here could just as well have been applied to event data in the previous section. Note that in all of the above the bins are not assumed to be equal or contiguous -there can be arbitrary gaps between them ( \u00a71.7). We now turn to the determination of ncp prior for binned data. Figure 5 is a contour plot of the values of this parameter based on a simulation study with bins containing independently distributed events. These contours are very insensitive to the false positive rate, which was taken as .05 in this figure.\n\n\nPoint Measurements\n\nA common experimental scenario is to measure a signal s(t) at a sequence of times t n , n = 1, 2, . . . , N in order to characterize its time dependence. Inevitable corruption due to observational errors is frequently countered by smoothing the data and/or fitting a model. As with the other data modes Bayesian Blocks is a different approach to this issue, making use of knowledge of the observational error distribution and avoiding the information loss entailed by smoothing. In our treatment the set of observation times t n , collectively known as the sampling, can be anything -evenly spaced points or otherwise. Furthermore we explicitly assume that the measurements at these times are independent of each other, which is to say the errors of observation are statistically independent.\n\nTypically these errors are random and additive, so that the observed time series can be modeled as x n \u2261 x(t n ) = s(t n ) + z n n = 1, 2, . . . N .\n\nThe observational error z n , at time t n , is known only through its statistical distribution. Consider the case where the errors are taken to obey a normal probability distribution with zero mean and given variance:\nP (z n )dz n = 1 \u03c3 n \u221a 2\u03c0 e \u2212 1 2 ( zn \u03c3n ) 2 dz n .(28)\nUsing eqs. (27) and (28) if the model signal is the constant s = \u03bb the likelihood of measurement n is\nL n = 1 \u03c3 n \u221a 2\u03c0 e \u2212 1 2 ( xn\u2212\u03bb \u03c3n ) 2 .(29)\nSince we assume independence of the measurements the block k likelihood is\nL (k) = n L n = (2\u03c0) \u2212 N k 2 m \u03c3 m e \u2212 1 2 n ( xn\u2212\u03bb \u03c3n ) 2 .(30)\nBoth the products and sum are over those values of the index such that t lies in block k. The quantities multiplying the exponentials in both the above equations are irrelevant because they contribute an overall constant factor to the total likelihood. We now derive the maximum likelihood fitness function for this data mode (with other forms based on different priors relegated to Appendix C, \u00a7 \u00a7C.4, C.5, C.6 and C.7). The quantities\na k = 1 2 n 1 \u03c3 2 n (31) b k = \u2212 n x n \u03c3 2 n (32) c k = 1 2 n x 2 n \u03c3 2 n(33)\nappear in all versions of these fitness functions; the first two are sufficient statistics. As usual we need to remove the dependence of eq. (30) on the parameter \u03bb, and here we accomplish this result by finding the value of \u03bb which maximizes the block likelihood, that is by maximizing\n\u2212 1 2 n ( x n \u2212 \u03bb \u03c3 n ) 2 .(34)\nThis is easily found to be\n\u03bb max = n x n \u03c3 2 n / n \u2032 1 \u03c3 2 n \u2032 (35) = \u2212b k /2a k(36)\nAs expected this maximum likelihood amplitude is just the weighted mean value of the observations x n within the block, because defining the weights \nw n = 1 \u03c3 2 n n \u2032 ( 1 \u03c3 2 n \u2032 ) ,(37)yields \u03bb max = n w n x n .(38)logL (k) max = \u2212 1 2 n ( x n + b k 2a k \u03c3 n ) 2(39)\nwhere again the sums are over the data in block k. Expanding the square\nlogL (k) max = \u2212 1 2 [ n x 2 n \u03c3 2 n + b k a k n x n \u03c3 2 n + b 2 k 4a 2 k n 1 \u03c3 2 n ] ,(40)\ndropping the first term (quadratic in x) which also sums to a modelindependent constant, and using equations (31) and (32) we arrive at logL\n(k) max = b 2 k /4a k .(41)\nAs expected each data cell must contain x n and \u03c3 n but we now see that these quantities enter the fitness function through the summands in the equations (31) and (32) defining a k and b k (c k does not matter), namely 1/(2\u03c3 2 n ) and \u2212x n /\u03c3 2 n . The way the corresponding block summations are implemented is described in Appendix A \u00a7A, (c.f. data mode #3).\n\nA few additional notes may be helpful. In the familiar case in which the error variance is assumed to be time-independent \u03c3 can be carried as an overall constant and \u03c3 n does not have to be specified in each data cell. The t n are only relevant in determining which cells belong in a block and do not enter the fitness computation explicitly. And the fitness function in Eq. (41) is manifestly invariant to a scale change in the measured quantity, as is the alternative form derived in Appendix C, Eq. (94). That is to say under the transformation\nx n \u2192 ax n , \u03c3 n \u2192 a\u03c3 n ,(42)\ncorresponding for example to a simple change in the units of x and \u03c3, the fitness does not change. Figure 6 exhibits a simulation study to calibrate ncp prior for normally distributed point measurements. For illustration the pure noise data simulated was normally distributed with a mean of 10 and unit variance. The left-hand panel shows how the false positive rate is diminished as ncp prior is increased, for the 8 values of N listed in the caption. The horizontal line is at the adopted false positive rate of 0.05; the points at which these curves cross below this line generate the curve shown in the bottom panel. The linear fit in the latter depicts the relation ncp prior = 1.32 + 0.577 log 10 (N). This relation is insensitive to the signal-to-noise ratio in the simulations. \n\n\n39\n\nThe following subsections present illustrative examples with sample data sets, demonstrating block representation for TTE data, multivariate time series, triggering, the empty block problem for TTE data, and data on the circle.\n\n\nBATSE Gamma Ray Burst TTE Data\n\nTrigger 551 in the BATSE catalog (4B catalog name 910718) was chosen to exemplify analysis of time-tagged event data as it has moderate pulse structure. See \u00a72.7 for a description of the data source. Figure 7 shows analysis of all of the event data in the top panels, and separated into the four energy channels in the lower panels. On the left are optimal block representations and the right shows the corresponding data in 32 evenly spaced bins. In all five cases the optimal block representations based on the block fitness function for event data in eq. (19) are depicted for two cases, using values the values of ncp_prior: (1) from eq. (21) with p 0 = 0.05 (solid lines); and (2) found with the iterative scheme described in \u00a72.7 (lightly shaded blocks bounded by dashed lines). These two results are identical for all cases except channel 3, where the iterative scheme's more conservative control of false positives yields fewer blocks (9 instead of 13).\n\nNote that the ordinary histograms of the photon times in the right-hand panels leave considerable uncertainty as to what the significant and true structures are. In the optimal block representations two salient conclusions are clear: (1) there are three pulses, and (2) they are most clearly delineated at higher energies. This figure depicts the error analysis procedures described above in \u00a72.8. \n\n\nMultivariate Time Series\n\nThis example in Fig. 9 demonstrates the multvariate capability of Bayesian Blocks by analyzing data consisting of three different modes sampled randomly from a synthetic signal. Time-tagged events, binned data, and normally distributed measurements were independently drawn from the same signal and analyzed separately, yielding the block representations depicted with thin lines.\n\nThe joint analysis of the data combined using the multivariate feature described above in \u00a72.9 is represented as the thick dashed line. None of these analyses is perfect, of course, due to the statistical fluctuations in the data. The combined analysis finds a few spurious change-points, but overall these do not represent serious distortions of the true signal. The individual analyses are somewhat poorer at capturing the true change-points and only the true change-points. Hence in this example the combined analysis makes effective use of disparate data modes from the same signal.\n\n\nReal Time Analysis: Triggers\n\nBecause of its incremental structure our algorithm is well suited for real-time analysis. Starting with a small amount of data the algorithm typically finds no change-points at first. Then by determining the optimal partition up to and including the most recently added data cell the algorithm effectively tests for the presence of the first change-point. The real time mode can be selected simply by triggering on the condition last(R) > 1 inserted into the code shown in Appendix A , \u00a7A, just before the end of the basic iterative loop on R. For the entry of 1 in each element of array last means that the optimal partition consists of the whole array encountered so far. It is thus obvious that this first indication of change-point cannot yield more than one change-point.\n\nThus the algorithm can be set to return at the first significant change-point. Other more complicated halting or return conditions can be programmed into the algorithm, such as returning after a specified number of change-points have been found, or when the location of a change-point has not moved for a specified length of time, etc. Essentially any condition on the change-points or the corresponding blocks can be imposed as a halt-and-return condition.\n\nThe real time mode is mainly of use to detect the first sign of a time-dependent signal rising significantly above a slowly varying background. For example, in a photon stream the resulting trigger may indicate the presence of a new bursting or transient source.\n\nThe conventional way to approach problems of this sort is to report a detection if and when the actual event rate, averaged over some interval, exceeds one or more pre-set thresholds. See [Band 2002] for an extensive discussion, as well as [Fenimore et al. 2001, McLean et al. 2003, Schmidt 1999 for other applications in high energy astrophysics. One must consider a wide range of configurations: \"BAT uses about 800 different criteria to detect GRBs, each defined by a large number of commandable parameters.\" [McLean et al. 2003]. Both the size and locations of the intervals over which the signal is averaged affect the result, and therefore one must consider many different values of the corresponding parameters. The idea is to minimize the chances of missing a signal because, for example, its duration is poorly matched to the interval size chosen. If the background is determined dynamically, by averaging over an interval in which it is presumed there is no signal, similar considerations apply to this interval.\n\nOur segmentation algorithm greatly simplifies the above considerations, since predefined bin sizes and locations are not needed, and the background is automatically determined in real time. In practice there can be a slight complication for a continuously accumulating data stream, since the N 2 dependence of the compute time may eventually make the computations unfeasible. A simple countermeasure is to analyze the data in a sliding window of moderate size -large enough to capture the desired changes but not so large that the computations take too long. Slow variations in the background in many cases could mandate something like a sliding window anyway.\n\nBecause of additional complexities, such as accounting for background variability and the Pandora's box that spectral resolution opens [Band 2002], we will defer a serious treatment of triggers to a future publication.\n\nWe end with a few comments on the false alarm (also called false positive) rate in the context of triggers. The considerations are very similar to the tradeoff discussed in the context of the choice for the parameter ncp_prior described in \u00a71.9, \u00a72.7, and \u00a73 for the various data modes. Even if no signal is present a sufficiently large (and therefore rare) noise fluctuation can trigger any algorithm's detection criteria. Unavoidably all detection procedures embody a trade-off between sensitivity and rate of false alarms. Other things being equal, making an algorithm more able to trigger on weak signals renders it more sensitive to noise fluctuations. Conversely making an algorithm shun noise fluctuations renders it insensitive to weak signals. In practice one chooses a balance of these competing factors based on the relative importance of avoiding false positives and not missing weak signals. Hence there can be no universal prescription.\n\n\nEmpty Blocks\n\nRecall that blocks are taken to begin and end with data cells ( \u00a72.5). This convention means that no block can be empty: each much contain at least its initiating data cell. Hence in the case of event data, blocks cannot represent intervals of zero event rate. This constraint is of no consequence for the other two data modes. There is nothing special about zero (or even negative) signals in the case of point measurements. Zero signal would be indicated by intervals containing only measured values not significantly different from zero. There is also no issue for binned data as nothing prevents a block from consisting of one or more empty data bins. In many event data applications zero signal may never occur (e.g. if there is a significant background over the entire observation interval). But in other cases it may be useful to represent such intervals in the form of a truly empty block, with corresponding zero height.\n\nAllowing such null blocks is easily implemented in a post-processing step applied to each of the change-points. The idea is to consider reassignment of data cells at the start or end of a block to the adjoining block while leaving the block lengths unchanged. For a given change-point separating a pair of two blocks (\"left\" and \"right\") there are two possibilities: (a) the datum marking the change-point itself, currently initiating the right block, can be moved from the right to the left block; (b) the datum just prior to the change-point itself, currently ending the left block, can be moved from the left block to the right block. Straightforward evaluation of the relevant fitness functions establishes whether one of these moves increases the fitness of the pair, and if so which one. (It is impossible that this calculation will favor both moves (a) and (b); taken together they yield no net change and therefore leave fitness unchanged.)\n\nThe suggested procedure is to carry out this comparison for each change-point in turn and adjust the populations of the blocks accordingly. We have not proved that this ad hoc prescription yields globally optimal models with the non-emptiness constraint removed, but it is obvious that the prescription can only increase overall model fitness. It is quite simple computationally and there is no real downside to using it routinely, even if the moves are almost never triggered. A code fragment to implement this procedure is given in Appendix A, \u00a7A.\n\n\nBlocks on the Circle\n\nEach of the data spaces discussed so far has been a linear interval with a well defined beginning and end. A circle does not have this property. Our algorithm cannot be applied to data defined on a circle, 5 such as directional measurements, because it starts with the first data point and iteratively works its way forward along the interval to the last point. Hence the first and last points are treated as distant, not as the pair of adjacent points that they are. Any choice of starting point, such as the coordinate origin 0 for angles on [0, 2\u03c0], disallows the possibility of a block containing data just before and after it (on the circle). In short, the iterative (mathematical induction-like) structure of the algorithm prevents it from being independent of the arbitrary choice of origin, which on a circle is completely arbitrary. We have been unable to find a solution to this problem using a direct application of dynamical programming.\n\nHowever there is a method that provides exact solutions at the cost of about one order of magnitude more computation time. First unfold the data with an arbitrary choice for the fiducial origin. The resulting series starts at this origin, continues with the subsequent data points in order, and ends at the datum just prior to the fiducial origin. Think of cutting a loop of string and straightening it out.\n\nThe basic algorithm is then applied to the data series obtained by concatenating three copies of the unfolded data. The underlying idea is that the central copy is insulated from any effects of the discontinuity introduced by the unfolding. In extensive tests on simulated data this algorithm performed well. One check is whether or not the two sets of change-points adjacent to the two divisions between the copies of the data are always equivalent (modulo the length of the circle). These results suggest but do not prove correctness for all data; there may be pathological cases for which it fails. Of course this N 2 computation will take \u223c 9 times as long as it would if the data were on a simple linear interval. Figure 10 shows simulated data representing measurements of an angle on the interval [0, 2\u03c0]. In this case the procedure outlined above captures the central block (bottom panel) straddling the origin that is broken into two parts if the data series is taken to start at zero (upper panel). Note that the two blocks just above 0 and below 2\u03c0 in the upper panel, are rendered as a single block in the central cycle in the bottom panel. Figure 11 shows the same data shown in Figure 10 plotted explicitly on a circle. Vertical dotted lines at 2\u03c0 and 4\u03c0 indicate boundaries between the copies. The blocks in the central copy, between these lines, are not influenced by end effects and are the correct optimal representation of these circular data. Figure 11: Optimal block representation of the same data as in Figure 10 (cf. the middle third of the bottom panel) plotted on the circle. The origin corresponds to the positive x-axis. and scale of the radius of the circle is arbitrary.\n\nAs a footnote, one application that might not be obvious is the case of gamma-ray burst light curves which are short enough that the background is accurately constant over the duration of the burst. If all of the data are rescaled to fit on a circle, then the pre-and postburst background would automatically be subsumed into a single block (covering intervals at the beginning and end of the observation period). This procedure would be applicable to bursting light curves of any kind if and only if the background signal is constant, so that the event rates before and after the main burst are the same.\n\n\n51\n\n\nConclusions and Future Work\n\nThe Bayesian Blocks algorithm finds the optimal step function model of time series data by implementing the dynamical programming algorithm of [Jackson et al. 2005]. It is guaranteed to find the representation that maximizes any block-additive fitness function, in time of order N 2 , and replaces the greedy approximate algorithm in [Scargle 1998]. Its real-time mode triggers on the first statistically significant rate change in a data stream.\n\nThis paper addresses the following issues in the use of the algorithm for a variety of data modes: gaps and exposure variations, piecewise linear and piecewise exponential models, the prior distribution for the number of blocks, multivariate data, the empty block problem (for event data), data on the circle, dispersed data, and analysis of variance (\"error analysis\"). The algorithm is shown to closely approach the theoretical detection limit derived in [Arias-Castro, Donoho and Huo 2003 Work in progress includes extensions to generalized data spaces such as those of higher dimensions (cf. [Scargle 2001c]), and speeding up the algorithm. This paper implements the spirit of Reproducible Research, a publication protocol initiated by John Claerbout [Claerbout 1990] and developed by others at Stanford and elsewhere. The underlying idea is that the most effective way of publishing research is to include everything necessary to reproduce all of the results presented in the paper. In addition to all relevant mathematical equations and the reasoning justifying them, full implementation of this protocol requires that the data files and computer programs used to prepare all figures and tables are included. Cogent arguments for Reproducible Research, an overview of its development history, and honest assessment of its successes and failures, are eloquently described in [Donoho et al. (2008)].\n\nFollowing this discipline all of the MatLab code and data files used in preparing this paper are available as auxiliary material. Included is the file \"read_me.txt\" with details and a script \"reproduce_figures.m\" that erases all of the figure files and regenerates them from scratch. In some cases the default parameters implement shorter simulation studies than those that were used for the figures in the paper, but one of the features of Reproducible Research is that such parameters and other aspects of the code can be changed and experimented at will. Accordingly this collection of scripts includes illustrative exemplars of the use of and algorithms and serves as a tutorial for the methods.\n\nIn addition here is a commented version of the key fragment of the MatLab script (named find_blocks.m) for the basic algorithm described in this paper: for R = 1:num_points % Compute fit_vec : fitness of putative last block (end at R) if data_mode == 3 % Measurements, normal errors sum_x_1 = cumsum( cell_data( R:-1:1, 1 ) )'; %sum(x/sig^2) sum_x_0 = cumsum( cell_data( R:-1:1, 2 ) )'; %sum(1/sig^2) fit_vec=((sum_x_1(R:-1:1) ) .^2 ) ./( 4*sum_x_0(R:-1:1)); else arg_log = block_length(1:R) -block_length(R+1); arg_log( find( arg_log <= 0 ) ) = Inf; nn_cum_vec = cumsum( nn_vec(R:-1:1) ); nn_cum_vec = nn_cum_vec(R:-1:1); fit_vec = nn_cum_vec .* ( log( nn_cum_vec ) -log ( arg_log ) \n\n\nB Mathematical Details\n\nPartitions of arrays of data cells are crucial to the block modeling which our algorithm implements. This appendix collects a few mathematical facts about partitions and the nature of independent events.\n\n\nB.1 Definition of Partitions\n\nA partition of a set is a collection of its subsets that add up to the whole with no overlap. Formally, a partition is a set of elements, or blocks {B k } satisfying\nI = k B k(43)\n54 and B j B k = \u2205 (the empty set) for j = k.\n\nNote that these conditions apply to the partitions of the time series data by sets of data cells. The data cells themselves may or may not partition the whole observation interval, as either the completeness in eq. (43) or the no-overlap condition in eq. (44) may be violated.\n\n\nB.2 Reduction of Infinite Partition Space to a Finite One\n\nFor a continuous independent variable, such as time, the space of all possible partitions is infinitely large. We address this difficulty by introducing a construct in which T and its partitions are represented in terms of a collection of N discrete data cells in one-to-one correspondence with the measurements. 6 The blocks which make up the partitions are sets of data cells contiguous with respect to time-order of the cells. I.e. a given block consists of exactly all cells with observation times within some sub-interval of T . Now consider two sets of partitions of T : (a) all possible partitions (b) all possible collections of cells into blocks. Set (a) is infinitely large since the block boundaries consist of arbitrary real numbers in T , but set (b) is a finite subset of (a). Nevertheless, under reasonable assumptions about the data mode, any partition in (a) can be obtained from some partition in (b) by deforming boundaries of its blocks without crossing a data point. Because the potential of a block to be an element of the optimum partition (see the discussion of block fitness in \u00a73) is a function of the content of the cells, such a transformation cannot substantially change the fitness of the partition.\n\n\nB.3 The Number of Possible Partitions\n\nHow many different partitions of N cells are possible? Represent a partition by an ordered set of N zeros and ones, with one indicating that the corresponding time is a change-point, and zero that it is not. With two choices at each time, the number of combinations is\nN partitions = 2 N .(45)\nExcept for very short time series this number is too large for an exhaustive search, but our algorithm nevertheless finds the optimum over this space in a time that scales as only N 2 .\n\n\nB.4 A Result for Subpartitions\n\nWe here define subpartitions and prove an elementary corollary that is key to the algorithm.\n\nDefinition: a subpartition of a given partition P(I) is a subset of the blocks of P(I).\n\nIt is obvious that a subpartition is a partition of that subset of T consisting of those blocks. Although not a necessary condition for the result to be true, in all cases of interest here the blocks in the subpartition are contiguous, and thus form a partition of a subinterval of T . It follows that:\n\nTheorem: A subpartition P \u2032 of an optimal partition P(I)\n\nis an optimal partition of the subset I \u2032 that it covers.\n\nFor if there were a partition of I \u2032 , different from and fitter than P \u2032 , then combining it with the blocks of P not in P \u2032 would, by the block additivity condition, yield a partition of T fitter than P, contrary to the optimality of P.\n\nWe will make use of the following corollary:\n\nCorollary: removing the last block of an optimal partition leaves an optimal partition.\n\n\nB.5 Essential Nature of the \"Poisson\" Process\n\nThe term Poisson process refers to events occurring randomly in time and independently of each other. That is, the times of the events, t n , n = 1, 2, . . . , N ,\n\nare independently drawn from a given probability distribution. Think of the events as darts thrown randomly at the interval. If the distribution is flat (i.e. the same all over the interval of interest) we have a constant rate Poisson process. In this special case a point is just as likely to occur anywhere in the interval as it is anywhere else; but this need not be so. What must be so in general -the essential nature of the Poisson process from a physical point of view -is the above-mentioned independence: each dart is not at all influenced by the others. Throwing darts that have feathers or magnets, although random, is not a Poisson process if these accoutrements cause the darts to repel or attract each other.\n\nThis key property of independence determines all of the other features of the process. Most important are a set of remarkable properties of interval distributions (see e.g. [Papoulis 1965]). The time interval between a given point t 0 and the time t of the next event is exponentially distributed\nP (\u03c4 )d\u03c4 = \u03bbe \u2212\u03bb\u03c4 d\u03c4 ,(47)\nwhere \u03c4 = t \u2212 t 0 . The remarkable aspect is that it does not matter how t 0 is chosen; in particular the distribution is the same whether or not an event occurs at t 0 . This fact makes the implementation of event-by-event exposure straightforward ( \u00a71.8). Note that we have not mentioned the Poisson distribution itself. The number of events in a fixed interval does obey the Poisson distribution, but this result is subsidiary to, and follows from, event independence. In this sense a better name than Poisson process is independent event process.\n\nIn representing intensities of such processes, one scheme is to represent each event as a delta-function in time. But a more convenient way to extract rate information incorporates the time intervals 7 between photons. Specifically, for each photon consider the interval starting half way back to the previous photon and ending half way forward to the subsequent photon. This interval, namely\n[ t n \u2212 t n\u22121 2 , t n+1 \u2212 t n 2 ] ,(48)\nis the set of times closer to t n than to any other time, 8 and has length equal to the average of the two intervals connected by photon n, namely\n\u2206t n = t n+1 \u2212 t n\u22121 2 .(49)\nThen the reciprocal\nx n \u2261 1 \u2206t n(50)\nis taken as an estimate of the signal amplitude corresponding to observation n. When the photon rate is large, the corresponding intervals are small. demonstrates the data cell concept, including the simple modifications to account for variable exposure and for weighting by photon energy. [Prahl 1996] has derived a statistic for event clustering in Poisson process data that tests departures from the known interval distribution by evaluating the likelihood over a restricted interval range. Prahl's statistic is\nM N = 1 N \u2206T i <C * (1 \u2212 \u2206T i C * ) ,(51)\nwhere \u2206T i is the interval between events i and i + 1, and\nC * \u2261 1 N \u2206T i(52)\nis the empirical mean interval. In other settings, the fact that this statistic is a global measure of departure of the distribution (used here only locally, over one block) may be useful in the detection of periodic, and other global, signals in event data. If the exposure at this time is less than unity, the width of the rectangle shrinks in proportion, the area of the rectangle is preserved, so the height increases in inverse proportion yielding a larger estimate of the true event rate.\n\n\nC Other Block Fitness Functions\n\nThis appendix describes fitness function for a variety of data modes.\n\n\nC.1 Event Data: Alternate Derivation\n\nThe Cash statistics used to derive the fitness function in Eq. (19) is based on representation of event times as real numbers. Of course time is not measured with infinite precision, so it is interesting to note that a more realistic treatment yields the same formula.\n\nTypically the data systems' finest time resolution is represented as an elementary quantum of time, which will be called a tick since it is usually set by a computer clock. Measured values are expressed as integer multiples of it (cf. \u00a72.2.1 of [Scargle 1998]). We assume that n m , the number of events (e.g. photons) detected in tick m obeys a Poisson distribution:\nL m = (\u03bbdt) nm e \u2212\u03bbdt n m ! = \u039b nm e \u2212\u039b n m ! ,(53)\nwhere dt is the length of the tick. The event rates \u03bb and \u039b are counts per second and per tick, respectively. Time here is given in units such as seconds, but a representation in terms of (dimensionless) integer multiples of dt is sometimes more convenient. Due to event independence the block likelihood is the product of these individual factors over all ticks in the block. Assuming all ticks have the same length dt this is:\nL (k) = M (k) m=1 (\u03bbdt) nm e \u2212\u03bbdt n m ! ,(54)\nwhere M (k) is the number of ticks in block k. Note that non-events are included via the factor e \u2212\u03bbdt for each tick with n m = 0. When this expression is used to compute the likelihood for the whole interval (i.e. product of the block likelihoods over all blocks of the model) the denominator contributes the factor\n1 k M (k) m n m ! = 1 m n m ! ,(55)\nwhere on the right-hand side the product is over all the ticks in the whole interval. For low event rates where n m never exceeds 1, this quantity is unity. No matter what it is a constant, fixed once and for all given the data; in model comparison contexts it is independent of model parameters and hence irrelevant. Dropping it, noting that\nM (k) m=1 e \u2212\u03bbdt is just e \u2212\u03bbM (k) dt = e \u2212\u03bbM (k)\nCollecting together all factors for ticks with the same number of events eq. (54) simplifies to\nL (k) = e \u2212\u03bbM (k) \u221e n=0 (\u03bbdt) nH (k) (n) ,(56)\nwhere H (k) (n) is the number of ticks in the block with n events. Noting that\n\u221e n=0 nH (k) (n) = N (k) ,(57)\nwhere N (k) is the total number of events in block k, we have simply\nL (k) = (\u03bbdt) N (k) e \u2212\u03bbM (k) .(58)\nIn order for the model to depend on only the parameters defining the block edges, we need to eliminate \u03bb from eq. (58). One way to do this is to find the maximum of this likelihood as a function of \u03bb, which is easily seen to be at \u03bb = N (k) M (k) , yielding\nL (k) max = ( N (k) dt M (k) ) N (k) e \u2212N (k)(59)\nThe exponential contributes the overall constant factor e \u2212 k N (k) = e \u2212N to the full model. Moving this ultimately irrelevant factor to the left-hand side, noting that M (k) = T (k) dt , and taking the log, we have for the maximum-likelihood block fitness function\nlog L (k) max + N (k) = N (k) ( logN (k) \u2212 logM (k) ) .(60)\nequivalent to Eq. (19). An alternative way to eliminate \u03bb is to marginalize it as in the Bayesian formalism. That is, one specifies a prior probability distribution for the parameter and integrates the likelihood in Eq. (58) times this prior. Since the current context is generic, not devoted to a specific application, we seek a distribution that expresses no particular prior knowledge for the value of \u03bb. It is well known that there are several practical and philosophical issues connected with such so-called non-informative priors. Here we adopt this simple flat, normalized prior:\nP (\u03bb) = { P (\u2206) \u03bb 1 \u2264 \u03bb \u2264 \u03bb 2 0 otherwise ,(61)\nwhere the normalization condition yields\nP (\u2206) = 1 \u03bb 2 \u2212 \u03bb 1 = 1 \u2206\u03bb .(62)\nThus eq. (58), with \u03bb marginalized, is the posterior probability\nP (k) marg = P (\u2206) \u03bb 2 \u03bb 1 (\u03bbdt) N (k) e \u2212\u03bbT (k) d\u03bb (63) = P (\u2206) T (k) ( dt T (k) ) N (k) z 2 z 1 z N (k) e \u2212z dz(64)\nwhere z 1,2 = T (k) \u03bb 1,2 . In terms of the incomplete gamma function\n\u03b3(a, x) \u2261 x 0 z a\u22121 e \u2212z dz ,(65)\nwe have, utilizing\nM k = T (k) dt , logP (k) marg = log P (\u2206) T (k) \u2212 N (k) logM (k) + log[ \u03b3(N (k) + 1, z 2 ) \u2212 \u03b3(N (k) + 1, z 1 ) ] . (66) The infinite range z 1 = 0, z 2 = \u221e, gives logP (k) marg(\u221e) = log P (\u2206) T (k) + log\u0393(N (k) + 1) \u2212 N (k) logM (k) ,(67)\nThis prior is unnormalized (and therefore sometimes regarded as improper). Technically P (\u2206) approaches zero as z 2 \u2192 \u221e, but is retained here in order to formally retain the scale invariance to be discussed at the end of this section.\n\nAnother commonly used prior is the so-called conjugate Poisson distribution P (\u03bb) = C \u03bb \u03b1\u22121 e \u2212\u03b2\u03bb .\n\nAs noted by [Gelman, Carlin, Stern, and Rubin 1995] this \"prior density is, in some sense, equivalent to a total count of \u03b1-1 in \u03b2 prior observations,\" a relation that might be useful in some circumstances. The normalization constant C = \u03b2 \u03b1 , \u0393(\u03b1) , and with this prior the marginalized posterior probability distribution is\nP cp = C \u221e 0 \u03bb N (k) +\u03b1\u22121 e \u2212\u03bb(M (k) +\u03b2) d\u03bb ,(69)yielding log P cp \u2212 log C = log \u0393(N (k) + \u03b1) \u2212 (N (k) + \u03b1) log(M (k) + \u03b2) .\n(70) Note that for \u03b1 = 1, \u03b2 = 1 this prior and posterior reduce to those in Eqs. (28) and (29) of [Scargle 1998].\n\nEquations (19), (66), (67) and (70) are all invariant under a change in the units of time. The case of eq. (67) is slightly dodgy, as mentioned above, but otherwise is a direct result of expressing N (k) and M (k) as dimensionless counts, of events and time-ticks, respectively. (Further, in the case of eq. (66), z 1 and z 2 are dimensionless.) As mentioned above, the simplicity of eq. (19) recommends it in general, but specific prior information (e.g. as represented by eq. 68) may suggest use of one of the other forms.\n\n\nC.2 0-1 Event Data: Duplicate Time Tags Forbidden\n\nIn Mode 2 duplicate time tags are not allowed, the number of events detected at a given tick is 0 or 1, and the corresponding tick likelihood is:\nL m = e \u2212\u03bbdt = 1 \u2212 p n m = 0 (71) = 1 \u2212 e \u2212\u03bbdt = p n m = 1(72)\nwhere \u03bb is the model event rate, in events per unit time. From the Poisson distribution p = 1 \u2212 e \u2212\u03bbdt is the probability of an event, and 1 \u2212 p = e \u2212\u03bbdt that of no event. Note that p or \u03bb interchangeably specify the event rate. Since independent probabilities multiply, the block likelihood is the product of the tick likelihoods:\nL (k) = M (k) m=1 L m = p N (k) (1 \u2212 p) M (k) \u2212N (k) ,(73)\nwhere M (k) is the number of ticks in block k and N (k) is the number of events in the block.\n\nThere are again two ways to proceed. The maximum of this likelihood occurs at p = N (k) M (k) and is\nL (k) max = ( N (k) M (k) ) N (k) (1 \u2212 N (k) M (k) ) M (k) \u2212N (k)(74)\nUsing the logarithm of the maximum likelihood,\nlogL (k) max = N (k) log( N (k) M (k) ) + (M (k) \u2212 N (k) )log(1 \u2212 N (k) M (k) )(75)\nyields the fitness function, additive over blocks.\n\nAs in the previous sub-section, an alternative is to marginalize \u03bb:\nP (k) = L (k) P (\u03bb)d\u03bb ,(76)\nwhere P (\u03bb) is the prior probability distribution for the rate parameter. With the flat prior in eq. (61) 9 the posterior, marginalized over \u03bb is\nP (k) marg = P (\u2206) \u03bb 2 \u03bb 1 (1 \u2212 e \u2212\u03bbdt ) N k (e \u2212\u03bbdt ) M (k) \u2212N k d\u03bb .(77)\nChanging variables to p = 1 \u2212 e \u2212\u03bbdt , with dp = dt e \u2212\u03bbdt d\u03bb, this integral becomes\nP (k) marg = P (\u2206) dt p 2 p 1 p N (k) (1 \u2212 p) M (k) \u2212N (k) \u22121 dp ,(78)\nwith p 1,2 = 1 \u2212 e \u2212\u03bb 1,2 dt , and expressible in terms of the incomplete beta function\nB(z; a, b) = z 0 u a\u22121 (1 \u2212 u) b\u22121 du(79)\nas follows:\nlogP (k) marg \u2212 log P (\u2206) dt = log[B(p 2 ; N (k) + 1, M (k) \u2212 N (k) ) \u2212 B(p 1 ; N (k) + 1, M (k) \u2212 N (k) )] .(80)\nThe case p 1 = 0, p 2 = 1 yields the ordinary beta function:\nlogP (k) 0\u21921 \u2212 log P (\u2206) dt = logB(N (k) + 1, M (k) \u2212 N (k) ) ,(81)\ndiffering from Eq. (21) of [Scargle 1998] by one in the second argument, due to the difference between a prior flat in p and one flat in \u03bb. All of the equations (75), (80), and (81), can be used as fitness functions in the global optimization algorithm and, as with Mode 1, are invariant to a change in the units of time.\n\nA brief aside: one might be tempted to use intervals between successive events instead of the actual times, since in some sense they express rate information more directly. However, as we now prove, the likelihood based on intervals is essentially equivalent to that in eq. (58). It is a classic result [Papoulis 1965] that intervals between (time-ordered) consecutive independent events (occurring with a probability uniform in time, with a constant rate \u03bb) are exponentially distributed:\nP (dt)dt = \u03bbe \u2212\u03bbdt U(dt)dt,(82)\nwhere U(x) is the unit step function:\nU(x) = 1 x \u2265 0 = 0 x < 0 .\nPretend that the data consists of the inter-event intervals, and that one does not even know the absolute times. The likelihood of our constant-rate Poisson model for interval dt n \u2265 0 is\nL n = \u03bbe \u2212\u03bb dtn ,(83)\nso the block likelihood is\nL (k) = N (k) n=1 \u03bb e \u2212\u03bb dtn = \u03bb N (k) e \u2212\u03bbM (k) ,(84)\nthe same as in eq. (58), except that here N (k) is the number of inter-event intervals, one less than the number of events. [Prahl 1996] derived a statistic for event clustering, by testing for significant departures from the known interval distribution, by evaluating the likelihood over a restricted interval range. This statistic is\nM N = 1 N \u2206T i <C * (1 \u2212 \u2206T i C * ) ,(85)\nwhere \u2206T i is the interval between events T and i + 1, N is the number of terms in the sum, and\nC * \u2261 1 N \u2206T i(86)\nis the empirical mean of the relevant intervals. In some settings, the fact that this statistic is a global measure (as opposed to the local -over one block at a time -ones used here) may be useful in the detection of global signals, such as periodicities, in event data.\n\n\nC.3 Time-to-Spill Data\n\nAs discussed in \u00a72.2.3 of [Scargle 1998], reduction of the necessary telemetry rate is sometimes accomplished by recording only the time of detection of every Sth photon, e.g. with S=64 for the BATSE time-to-spill mode. This data mode has the attractive feature that its time resolution is greater when the source is brighter (and possibly more active, so that more time resolution is useful). With slightly revised notation the likelihood in Eq. (32) of [Scargle 1998] simplifies to\nL (k) T T S = \u03bb SN (k) spill e \u2212\u03bbM (k)(87)\nwhere N \nlogL (k) max,T T S \u2212 logN = SN (k) spill [ log(N (k) spill S) \u2212 logM (k) ](88)\njust as in Eq. (19) with N (k) = SN (k) spill , and with the same property that the unit in which block lengths are expressed is irrelevant.\n\n\nC.4 Point measurements: Alternative Form\n\nAn alternative form can be derived by inserting (38) instead of (36) into the log of Eq. (30) as in \u00a73.3. The result is:\nlogL (k) max = \u2212 1 2 n ( x n \u2212 n \u2032 w n \u2032 x n \u2032 \u03c3 n ) 2(89)\nExpanding the square gives\nlogL (k) max = \u2212 1 2 [ n ( x n \u03c3 n ) 2 \u22122 n ( x n \u03c3 2 n )( n \u2032 w n \u2032 x n \u2032 )+( n \u2032 w n \u2032 x n \u2032 ) 2 n 1 \u03c3 2 n ](90)= \u2212 1 2 n \u2032 ( 1 \u03c3 2 n \u2032 )[ n w n x 2 n \u22122( n w n x n )( n \u2032 w n \u2032 x n \u2032 )+( n \u2032 w n \u2032 x n \u2032 ) 2 n w n ](91)= \u2212 1 2 n \u2032 ( 1 \u03c3 2 n \u2032 )[ n w n x 2 n \u2212 2( n w n x n ) 2 + ( n \u2032 w n \u2032 x n \u2032 ) 2 ](92)= \u2212 1 2 n \u2032 ( 1 \u03c3 2 n \u2032 )[ n w n x 2 n \u2212 ( n w n x n ) 2 ](93)\nyielding logL (k)\nmax = \u2212 1 2 [ n \u2032 ( 1 \u03c3 2 n \u2032 )] \u03c3 2 X(94)\nwhere\n\u03c3 2 X \u2261 n w n x 2 n \u2212 ( n w n x n ) 2(95)\nis the weighted average variance of the measured signal values in the block. It makes sense that the block fitness function is proportional to the negative of the variance: the best constant model for the block should have minimum variance.\n\n\nC.5 Point measurements: Marginal Posterior, Flat Prior\n\nFirst, consider the simplest choice, the flat, unnormalizable prior P (\u03bb) = P * (for all values of \u03bb) ,\n\ngiving equal weight to all values. The marginal posterior for block k is then, from Eq. (30),\nP k = P * (2\u03c0) \u2212 N k 2 n \u03c3 n \u221e \u2212\u221e e \u2212 1 2 n ( xn\u2212\u03bb \u03c3n ) 2 d\u03bb(97)\nUsing the definitions introduced above in eqs. (31), (32), and (33) we have\nP k = P * (2\u03c0) \u2212 N k 2 n \u03c3 n \u221e \u2212\u221e e \u2212(a k \u03bb 2 +b k \u03bb+c k ) d\u03bb .(98)\nUsing standard \"completing the square,\" letting z = \u221a a k (\u03bb + b k 2a k ), giving\nz 2 = a k (\u03bb+ b k 2a k ) 2 = a k (\u03bb 2 + \u03bbb k a k + b 2 k 4a 2 k ) = a k \u03bb 2 +b k \u03bb+c k + b 2 k 4a k \u2212c k ,(99)\nand then using\n+\u221e \u2212\u221e e \u2212z 2 dz \u221a a k = \u03c0 a k .(100)\nwe have\nP k = P * (2\u03c0) \u2212 N k 2 n \u03c3 n \u03c0 a k e ( b 2 k 4a k )\u2212c k(101)\nFrom this result, the log-posterior fitness function is\nlogP k 0 \u2212 A k = log(P * \u03c0 a k ) + ( b 2 k 4a k ) \u2212 c k(102)\nwhere\nA k = \u2212 N k 2 log(2\u03c0) \u2212 log(\u03c3 n )(103)\nand the subscript 0 refers to the fact that the marginal posterior was obtained with the unnormalized prior. The second and third terms in Eq. (102 ) are invariant under the transformation (42). Further, since the integral of P (\u03bb) with respect to \u03bb must be dimensionless, we have P * \u223c 1 \u03bb \u223c 1 x , so P * and \u221a a k have the same a-dependence, yielding a formal invariance for (102). However the prior in eq. (96) is not normalizable, so that technically P * is undefined. A way to make practical use of this formal invariance is simply to include a constant P * that has the proper dimension (x \u22121 ).\n\n\nC.6 Point Measurements: Marginal Posterior, Normalized Flat Prior\n\nMarginalizing the likelihood in eq. (30) with the prior in eq. (61), yields for the marginal posterior for block k:\nP k = P (\u2206) (2\u03c0) \u2212 N k 2 n \u03c3 n \u03bb2 \u03bb 1 e \u2212 1 2 n ( xn\u2212\u03bb \u03c3n ) 2 d\u03bb(104)\nAs before\nP k = P (\u2206) (2\u03c0) \u2212 N k 2 n \u03c3 n \u03bb2 \u03bb 1 e \u2212(a k \u03bb 2 +b k \u03bb+c k ) d\u03bb(105)\nNow complete the square by letting z = \u221a a k (\u03bb + b k 2a k ), giving\nz 2 = a k (\u03bb + b k 2a k ) 2 = a k (\u03bb 2 + \u03bbb k a k + b 2 k 4a 2 k ) = a k \u03bb 2 + b k \u03bb + b 2 k 4a k + c k \u2212 c k (106) so we have P k = P (\u2206) (2\u03c0) \u2212 N k 2 n \u03c3 n e ( b 2 k 4a k \u2212c k ) z 2 z 1 e \u2212z 2 dz \u221a a k(107)\nwhere\nz 1,2 = \u221a a k (\u03bb 1.2 + b k 2a k )(108)\nFinally, introducing the error function\nerf(x) = 2 \u221a \u03c0 x 0 e \u2212t 2 dt(109)\nwe have\nP k = P (\u2206) \u221a \u03c0 2 (2\u03c0) \u2212 N k 2 \u221a a k n \u03c3 n e ( b 2 k 4a k \u2212c k ) [erf(z 2 ) \u2212 erf(z 1 )](110)\nTaking the log gives the final expression\nlogP k \u2206 \u2212 A k = log(P (\u2206) \u03c0 a k ) + ( b 2 k 4a k \u2212 c k ) + log[ erf(z 2 )\u2212erf(z 1 ) 2 ]\n(111) where the subscript \u2206 indicates the fact that this result is based on the finite-range prior in eq. (61). Note that this fitness function is manifestly invariant under the transformation in eq. (42), for the same reasons discussed at the end of the previous section, plus the invariance of z 1,2 . In the limits z 1 \u2192 \u2212\u221e and z 2 \u2192 \u221e, erf(z 2 ) \u2212 erf(z 1 ) \u2192 2, and we recover eq.(102) -but remember that in this limit the invariance is only formal.\n\n\nC.7 Point Measurements: Marginal Posterior, Gaussian Prior\n\nFinally, consider using the following normalized Gaussian prior for \u03bb:\nP (\u03bb) = 1 \u03c3 0 \u221a 2\u03c0 e \u2212 1 2 ( \u03bb\u2212\u03bb 0 \u03c3 0 ) 2(112)\ncorresponding to prior knowledge that roughly speaking \u03bb most likely lies in the range \u03bb 0 \u00b1 \u03c3 0 , with a normal distribution. This prior is not to be confused with the Gaussian form for the likelihood in eq. (29). Eq. (30), when \u03bb is marginalized with this prior, becomes\nL (k) = 1 \u03c3 0 \u221a 2\u03c0 [ (2\u03c0) \u2212( N k 2 ) n \u2032 \u03c3 n \u2032 ] e \u2212 1 2 [\u03bb 2 ( 1 \u03c3 2 0 + n 1 \u03c3 2 n )+\u03bb(\u2212 2\u03bb 0 \u03c3 2 0 \u2212 2xn \u03c3 2 0 )+( \u03bb 2 0 \u03c3 2 0 + n x 2 n \u03c3 2 n ) (113) so with a k = 1 2 ( 1 \u03c3 2 0 + n 1 \u03c3 2 n ) (114) b k = \u2212( \u03bb 0 \u03c3 2 0 + n x n \u03c3 2 n )(115)\nand\nc k = 1 2 ( \u03bb 2 0 \u03c3 2 0 + n x 2 n \u03c3 2 n )(116)\nand eq. (98) is recovered, so that eq.(102), with the redefined coefficients in eqs. (114), (115) and (116), gives the final fitness function.\n\nAny of the log fitness functions in eqs. (94), (102), or (111) can be used for the point measurement data mode in this section. No general guidance for this depending on convenience or the kind of prior information for the signal parameters that makes sense.\n\n\nC.8 Data with Dispersed Measurements\n\nThroughout it has been presumed that two things are small compared to any relevant time scales: errors in the determination of times of events, and the intervals over which individual measurements are obtained as averages. These assumptions justify treatment of the corresponding data modes as points in \u00a73.1 and \u00a73.3 respectively. Below are discussions of data that are dispersed because of (1) random errors in event times and (2) measurements that are summations or averages over non-negligible intervals. Binned data, an example of the latter, have already been treated in \u00a73.2 and are not discussed here.\n\nA simple ad hoc way to deal with both of these situations is to compute kernel functions for each data point, representing the window or error distribution in either of the two above contexts. Each such function would be centered at the corresponding measured value, evaluated at all of the data points, and normalized to represent unit intensity. Each such kernel would be maximum at the data point at which it is centered, but distribute some weight to the other data cells. The sum of all of these kernels would then be a set of weights at each measurement, which could then be treated as ordinary event data but with fractional rather than unit weights. The ad hoc aspect of this approach lies in the way the fitness function is extended. The following sub-sections provide more rigorous analysis.\n\n\nC.8.1 Uncertain Event Locations\n\nTiming of events is always uncertain at some level. Here we treat the case where the error distribution is wide enough to make the point approximation inappropriate. Rare for photon time series, with microsecond timing errors, this situation is more common in other contexts and with other independent variables. With overlapping error distributions even the order of events can be uncertain. In the context described in \u00a71.4 one often wants to construct histograms from measurements with errors -errors that may be different for each point (then called heteroscedastic errors).\n\nA simple modification of the fitness function described in \u00a73.1 addresses this kind of data. On the right-hand side of Eq. (19) N (k) quantifies the contribution of the individual events within block k. In extending the reasoning leading to this fitness function, the main issue concerns events with error distributions that have fractional overlap with the extent of block k -for events distributed entirely outside (inside) obviously contribute in no way (fully) to block fitness. By the law for the sum of probabilities of independent events, in the log-likelihood implicit in Eqs. (17) and (18) N (k) is replaced by the sum of the areas under the probability distributions overlapping block k, namely i\u2208k p (i) summed over all events with significant contribution to block k, and p (i) is the integral of the overlapping part of the error distribution, a fraction between 0 and 1. Thus we have\nlogL (k) (\u03bb) = log\u03bb i\u2208k p (i) \u2212 \u03bbT (k)(117)\nin place of Eq. (19), with the analogous constant term on the lefthand side of that equation dropped. This result holds because a given datum falling inside and outside a block are mutually exclusive events. Implementing this relationship in the algorithm is easily accomplished. For a given event and the interval assigned to it (cf. Figure  12 in \u00a7B.5) sum the overlap fractions with that interval of all events -including that event itself. These quantities could be approximated with very simple or complex quadrature schemes, depending on the context and the way in which the relevant distributions are represented. Normally the array nn_vec, as in the code fragment in \u00a7A, is all 1's (or counts of events with identical time-tags there are any); but here replace it with these summed event weights. This construction automatically assigns the correct fractional weights to the block with no further alteration of the algorithm.\n\n\nC.8.2 Measurements in Extended Windows\n\nThis section discusses the case of distributed measurements in the sense that the time of measurement is either uncertain or is effec-tively an interval rather than a point. (This is different from the use of this term in \u00a73.3 to describe the distribution of the measurement error in the dependent variable.) Measurements may refer to a quantity averaged over a range of values of t, not at a single time as in \u00a7 \u00a73.3, C.4, C.5, C.6, and C.7. In the context of histograms ( \u00a71.4) the measured quantity becomes the independent variable, and the dependent variable is an indicator marking the presence of the measurement there. In both cases the measurement can be thought of as distributed over an interval, not just at a point.\n\nIn this case the data cell array would be augmented by the inclusion of a window function, indicating the variation of the instrumental sensitivity:\n\nx = {x n , t n , w n (t \u2212 t n )} n = 1, 2, . . . , N ,\n\nwhere w n (t) describes, for the value reported as X n , the relative weights assigned to times near t n . This is a nontrivial complication if the window functions overlap, but can nevertheless be handled with the same technique.\n\nWe assume the standard piece-wise constant model of the underlying signal, that is, a set of contiguous blocks:\nB(x) = N b j=1 B (j) (x)(119)\nwhere each block is represented as a boxcar function:\nB (k) (x) = { B j \u03b6 j \u2264 x \u2264 \u03b6 j+1 0 otherwise(120)\nthe \u03b6 j are the change-points, satisfying (121) and the B j are the heights of the blocks. The value of the observed quantity, y n , at x n , under this model\nmin(x n ) \u2264 \u03b6 1 \u2264 \u03b6 2 \u2264 . . . \u03b6 j \u2264 \u03b6 j+1 \u2264 . . . \u2264 \u03b6 N b \u2264 max(x n )is\u0177 n = w n (x)B(x)dx = w n (x) N b j=1 B (j) (x)dx = N b j=1 w n (x)B (j) (x)dx = N b j=1 B j \u03b6 j+1 \u03b6 j w n (x)dx(122)\nso we can write\u0177\nn = N b j=1 B j G j (n)(123)\nwhere\nG j (n) \u2261 \u03b6 j+1 \u03b6 j w n (x)dx(124)\nis the inner product of the n-th weight function with the support of the j-th block. The analysis in [Bretthorst 1988] shows how do deal with the non-orthogonality that is generally the case here. 10 The averaging process in this data model induces dependence among the blocks. The likelihood, written as a product of likelihoods of the assumed independent data samples, is P (Data|Model) = N n=1 P (y n |Model) \n= Qe \u2212 1 2 ( yn\u2212 N b j=1 B j G j (n) \u03c3n ) 2 ,(127)\nwhere Q \u2261 \n\nAfter more algebra and adopting a new notation, symbolized by y n \u03c3 2 n \u2192 y n\n\n10 If the weighting functions are delta functions, it is easy to see that G j (n) is non-zero if and only if xn lies in block j, and since the blocks do not overlap the product G j (n)G k (n) is zero for j = k, yielding orthogonality, N G j (n)G k (n) = \u03b4 j,k . And of course there can be some orthogonal blocks, for which there happens to be no \"spill over\", but these are exceptions. and G k (n) \u03c3 2 n \u2192 G k (n) ,\n\nwe arrive at logP ({y n }|B) = Qe \u2212 H 2 ,\n\nwhere\nH \u2261 N n=1 y 2 n \u2212 2 N b j=1 B j N n=1 y n G j (n) + N b j=1 N b k=1 B j B k N n=1 G j (n)G k (n) .\n(133) The last two equations are equivalent to Eqs. (3.2) and (3.3) of [Bretthorst 1988], so that the orthogonalization of the basis functions and the final expressions follow exactly as in that reference.\n\n\nC.9 Piecewise Linear Model: Event Data\n\nHere we outline the computations of a fitness function for the piecewise linear model in the case of event data. This means that the event rate for a block is assumed to be linear, as in Eq. (1).\n\nFor convenience we take the fiducial time t fid to be t 2 , the time at the end of the block. Take t 1 to be the time at the beginning, so M = t 2 \u2212 t 1 is the length of the block, and the signal x is \u03bb(1 \u2212 aM) at the beginning of the block and \u03bb at the end, and varies linearly in between.\n\nThe block likelihood for the case of event data t i is\nL(\u03bb, a) = N k i=1\nlog[ \u03bb(1 + a(t i \u2212 t 2 )) ] \u2212 t 2 t 1 \u03bb(1 + a(t \u2212 t 2 ))dt (134) where the sum is over the N k events in the block and the integral is over the time interval covered by the block. Simplifying we have\nL(\u03bb, a) = N k log\u03bb + N k i=1 log[ (1 + a(t i \u2212 t 2 )) ] \u2212 \u03bb[(1 \u2212 at 2 )t + a 2 t 2 ] t 2 t 1 (135) L(\u03bb, a) = N k log\u03bb + N k i=1 log[ (1 + a(t i \u2212 t 2 )) ] \u2212 \u03bbM k (1 \u2212 a 2 M k ) (136)\nNow let's compute the maximum likelihood as a function of \u03bb 75 and a, starting by setting\n\u2202L \u2202\u03bb = N k \u03bb \u2212 M k (1 \u2212 a 2 M k ) = 0(137)\nso that at the maximum of this likelihood we have\n\u03bb = N k M k (1 \u2212 a 2 M k )(138)\nand therefore\nL(\u03bb max , a) = N k log[ N k M k (1 \u2212 a 2 M k ) ] + N k i=1 log[ (1 + a(t i \u2212 t 2 )) ] \u2212 N k (139) \u2202L \u2202a = N k log[ N k M k (1 \u2212 a 2 M k ) ] + N k i=1 log[ (1 + a(t i \u2212 t 2 )) ] \u2212 N k (140) \u2202L \u2202a = N k i=1 (t i \u2212 t 2 ) 1 + a(t i \u2212 t 2 ) + \u03bb 2 M 2 k = 0 (141) 1 N k N k i=1 (t i \u2212 t 2 ) 1 + a(t i \u2212 t 2 ) + 1 2 M k (1 \u2212 a 2 M k ) = 0 (142) f (a) = 1 N k N k i=1 (t i \u2212 t 2 ) 1 + a(t i \u2212 t 2 ) + 1 2 M k (1 \u2212 a 2 M k ) (143) f \u2032 (a) = \u2212 1 N k N k i=1 (t i \u2212 t 2 ) 2 [1 + a(t i \u2212 t 2 )] 2 \u2212 1 4 M 2 k (1 \u2212 a 2 M k ) 2 (144) \u03bb = \u2212 2 M 2 k N k i=1 (t i \u2212 t 2 ) 1 + a(t i \u2212 t 2 ) (145) N k (1 \u2212 a 2 M k ) = \u2212 2 M k N k i=1 (t i \u2212 t 2 ) 1 + a(t i \u2212 t 2 )(146)(1 \u2212 a 2 M k ) N k = \u2212 M k 2 N k i=1 (t i \u2212t 2 ) 1+a(t i \u2212t 2 ) (147) 1 \u2212 a 2 M k = \u2212 1 2 M k N k ( N k i=1 (t i \u2212 t 2 ) 1 + a(t i \u2212 t 2 ) ) \u22121 (148) 76 a = 2 M k + N k ( N k i=1 (t i \u2212 t 2 ) 1 + a(t i \u2212 t 2 ) ) \u22121(149)\nC.10 Piecewise Exponential Model: Event Data\n\nIn this case we model the signal as varying exponentially across the time interval contained in the block. Denoting the times beginning and ending the block as t 1 and t 2 , and taking the latter as the fiducial time in Equation (2), the signal is \u03bbe \u2212aM at the beginning of the block and \u03bb at the end. Much as in \u00a7C.9 the block likelihood for the case of event data t i is the follow expression involving a sum over the N k events in the block and an integral over the time interval covered by the block:\nL(\u03bb, a) = N k i=1 log[ \u03bbe a(t i \u2212t 2 ) ] \u2212 t 2 t 1 \u03bbe a(t\u2212t 2 ) dt (150) L(\u03bb, a) = N k log\u03bb + a i (t i \u2212 t 2 ) \u2212 \u03bb( 1 \u2212 e \u2212aM a )(151)\nwhere M = t 2 \u2212 t 1 is the length of the block. Now let's compute the maximum likelihood as a function of \u03bb and a:\n\n\u2202L\n\u2202\u03bb = N k \u03bb \u2212 ( 1 \u2212 e \u2212aM a )(152)\nand therefore at the maximum we have\n\u03bb = aN k 1 \u2212 e \u2212aM(153)\u2202L \u2202a = i (t i \u2212 t 2 ) \u2212 [N k (1 \u2212 e \u2212aM ) \u22121 ][(M + a \u22121 )e \u2212aM \u2212 a \u22121 ] (154) L max (a) = N k log( aN k 1 \u2212 e \u2212aM )+a i (t i \u2212 t 2 ) \u2212 aN k 1 \u2212 e \u2212aM ( 1 \u2212 e \u2212aM a ) (155) L max (a) = N k log( aN k 1 \u2212 e \u2212aM ) + a i (t i \u2212 t 2 ) \u2212 N k (156) 77 \u2202L max (a) \u2202a = N k ( 1 \u2212 e \u2212aM aN k )Q + i (t i \u2212 t 2 )(157)\nwhere\nQ = N k [(1 \u2212 e \u2212aM ) \u22121 \u2212 a(1 \u2212 e \u2212aM ) \u22122 Me \u2212aM ] (158) \u2202L max (a) \u2202a = N k a \u2212 MN k e \u2212aM (1 \u2212 e \u2212aM ) + i (t i \u2212 t 2 )(159)\nTo solve for the value of a that makes this derivative zero (to find the maximum of the likelihood) we will use Newton's method to find the zeros of\nf (a) = \u2202L max (a) \u2202a /N k = 1 a \u2212 Me \u2212aM (1 \u2212 e \u2212aM ) \u22121 + S (160) where S = 1 N k i (t i \u2212 t 2 )(161)\nis the mean of the differences between the event times and the time at the end of the block. The iterative equation is\na k+1 = a k \u2212 f (a k ) f \u2032 (a k )(162)\nand since S is a constant we have \n\nand a k+1 = a k \u2212 a \u22121 k \u2212 MQ(a k ) + S \u2212a \u22122 k + M 2 Q(a k )[1 + Q(a k )])\n\nFigure 1 :\n1Cross-validation error of BATSE TTE data (averaged over 532 GRBs, 8 random subsamples, and time) for a range of values of -log \u03b3 = with 3\u03c3 error bars.\n\nFigure 3 :\n3One hundred unit variance normally distributed measurements -zeromean (+) except for a block of events 25-75 (dots). In the four panels the block amplitudes are 0.2, .32, .5, and 1.0 in units of the Arias-Castro et al. threshold \u221a 2 logN . Thick lines show the blocks, where detected, with thin vertical lines at the change-points.\n\nFigure 4 :\n4Error in finding a single block vs. simulated block amplitude in units of Arias-Castro et al.'s threshold amplitude. The curves (from right to left) are for N = 32, 64, 128, 256, 1024 and 2048.\n\nFig.\nFig. 4 reports some results of detection of the same step-function process shown in Fig. 3, averaged over many different realizations of the observational error process and for several different values of N. The lines are plots of a simple error metric (combing the errors in the number of change-points and their locations) as a function of the amplitude of the test signal. The left panel is for the case where the number of points in the putative block is held fixed, whereas the right panel is this number is taken to be proportional to N, sometimes a more realistic situation. We have adopted the following definition for the threshold in this case:\n\nFigure 5 :\n5Simulation study, based on the false positive rate of 0.05, to determine ncp prior = -log(\u03b3) for binned data. Contours of this parameter are shown as a function of the number of bins and number of data points (logarithmic x-and y-axes, respectively). The heavy dashed line indicates the undesirable region where the numbers of bins and data points are equal.\n\nFigure 6 :\n6Simulations of point measurements (Gaussian noise with signal-tonoise ratio of 10) to determine ncp prior = -log(\u03b3). Top: false positive fraction p 0 vs. value of ncp prior with separate curves for the values N = 8, 16, 32, 64, 128, 256, 512 and 1024 (left to right; alternating dots, + and circles). The points at which the rate becomes unacceptable (here .05; dashed line) determines the recommended values of ncp prior shown as a function of N in the bottom panel.\n\nFigure 7 :\n7BATSE TTE data for Trigger 0551. Top panels: all photons. Other panels: photons in the four BATSE energy channels. Left column shows Bayesian Block representations: default ncp prior = solid lines; iterated ncp prior = shaded/dashed lines. Right column: ordinary evenly spaced binned histograms.\n\nFigure 8 :\n8Error analysis for the data in Channel 4 from Fig. 7, zooming in on the time interval with most of the activity. Top: Heavy solid line is bootstrap mean (256 realizations), with thin lines giving the \u00b11\u03c3 RMS deviations, all superimposed on the BB representation. Bottom: approximate posterior distribution functions for the locations of the change-points, obtained by fixing all of the others.\n\nFigure 9 :Figure 10 :\n910Multivariate analysis of synthetic signal consisting of two blocks surrounding a Gaussian shape centered on the interval [0,1] (solid line). Optimal blocks for three independent data series drawn randomly from the probability distribution corresponding to this signal are thin lines: 1024 event times (dash); 4096 events in 32 bins (dot-dash); and 32 random amplitudes normally distributed with mean equal to the signal at random times uniformly distributed on [0,1] and constant variance (dots). The thicker dashed line is the combined analysis of all three. Data on the circle: events drawn from two normal distributions, centered at \u03c0 and 0, the latter with some points wrapping around to values below 2\u03c0. Optimal blocks are depicted with thick horizontal bars superimposed on ordinary histograms. Top: block representation on the interval [0,2\u03c0]. Bottom: Block representation of three concatenated copies of the same data on [0,6\u03c0].\n\n%\nFor data modes 1 and 2: % nn_vec is the array of cell populations. % Preliminary computation: block_length=tt_stop-[tt_start 0.5*(tt(2:end)+tt(1:end-1))' tt_stop]; ... %-----------------------------------------------------------------% Start with first data cell; add one cell at each iteration %----------------------------------------------------------------best = []; last = [];\n\n\n); end [ best(R), last(R)] = max( [ 0 best ] + fit_vec -ncp_prior ); end %-----------------------------------------------------------------% Now find changepoints by iteratively peeling off the last block %----------------------------------------------------------------index = last( num_points ); change_points = []; while index > 1 change_points = [ index change_points ]; index = last( index -1 ); end\n\nFigure 12 :\n12Voronoi cell of a photon. Three successive photon detection times are circles on the time axis. The vertical dotted lines underneath delineate the time extent (dt) of the cell and the height of the rectangle -n/dt, where n is the number of photons at exactly the same time (almost always 1) -is the local estimate of the signal amplitude.\n\n\nis the number of spill events in the block, and M (k) is as usual the length of the block in ticks. With N = N (k) spill S this is identical to the Poisson likelihood in Eq.(54), and in particular the maximum likelihood is at \u03bb =\n\nf\n\u2032 (a) = \u2212 1 a 2 \u2212M[\u2212Me \u2212aM (1\u2212e \u2212aM ) \u22121 \u2212Me \u2212aM (1\u2212e \u2212aM ) \u22122 e \u2212aM ] (163) f \u2032 (a) = \u2212 1 a 2 + M 2 e \u2212aM (1 \u2212 e \u2212aM ) \u22121 [1 + e \u2212aM (1 \u2212 e \u2212aM ) \u22121 ] (164) and defining Q(a) = e \u2212aM (1 \u2212 e \u2212aM ) \u22121 (165) we have f \u2032 (a) = \u2212 1 a 2 + M 2 Q(a)[1 + Q(a)]\nThere is one more block than there are change-points: The first datum is always considered a change-point, marking the start of the first block, and is therefore not a free parameter. If the last datum is a change-point, it denotes a block consisting solely of that datum.\nBellman's explanation (before the word \"programming\" took on its current computational connotation) of how he chose this name is interesting. The Secretary of Defense at the time \"... had a pathological fear and hatred of the word, research. ... You can imagine how he felt, then, about the term, mathematical. ... I felt I had to do something to shield ... the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. ... I was interested in planning ... But planning is not a good word for various reasons. I decided therefore to use the word, programming. I wanted to get across the idea that this was dynamic ... it's impossible to use the word, dynamic, in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It's impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to.\nTM The Mathworks, Inc\nOf course the case where the measured value is confined to a specific subinterval of the circle is not a problem.\nThe cells may form a partition of T , as for example with event data with no gaps (see \u00a73.1), but it is not necessary that they do so.\nA method for analyzing event data based solely on inter-event time intervals has been developed in ([Prahl 1996]).\nThese intervals form the V oronoi tessellation of the total observation interval. See ([Okabe, Boots, Sugihara and Chiu 2000]) for a full discussion of this construct, highly useful in spatial domains of 2, 3, or higher dimension; see also([Scargle 2001a, Scargle 2001c).\nIn[Scargle 1998] we used p as the independent variable, and chose a prior flat (constant) as a function of p. Here, we use a prior flat as a function of the rate parameter.\nAcknowledgements: This work was supported by Joe Bredekamp and the NASA Applied Information Systems Research Program, and the CAMCOS program through the Woodward Fund at San Jose State University. JDS is grateful for the hospitality of the Institute for Pure and Applied Mathematics at UCLA, and the Keck Institute for Space Studies at Cal Tech. We are grateful to Glen MacLachlan and Erik Petigura for helpful comments.\nNear-Optimal Detection of Geometric Objects by Fast Multiscale Methods,\" preprint. Donoho Arias-Castro, E Arias-Castro, D Donoho, X Huo, R Bellman, Communications of the ACM. 5786284Astrophys.J.Arias-Castro, Donoho and Huo 2003] Arias-Castro, E., , Donoho, D., and Huo, X. 2003, \"Near-Optimal Detection of Geometric Objects by Fast Multiscale Methods,\" preprint. [Band 2002] Band, D. (2002), \"A Gamma-Ray Burst Trigger Toolkit,\" Astrophys.J., 578, 806-811 (arxiv.org/abs/astro- ph/0205548) [Bellman 1961] Bellman, R. (1961), \"On the approximation of curves by line segments using dynamic programming, Com- munications of the ACM, Vol. 4 No. 6, p.284.\n\nThe Science of Leonardo, Doubleday: New York [Cash 1979] Cash, W., Parameter-Estimation in Astronomy through Application of the Likelihood Ratio. G Bretthorst, Larry, Lecture Notes in Statistics. Springer-Verlag228939947Fritjof Capra[Bretthorst 1988] Bretthorst, G. Larry (1988), Bayesian Spectrum Analysis and Parameter Estimation, Lecture Notes in Statis- tics, Springer-Verlag. http://bayes.wustl.edu/ [Capra 2007] Fritjof Capra (2007) The Science of Leonardo, Dou- bleday: New York [Cash 1979] Cash, W., Parameter-Estimation in Astronomy through Application of the Likelihood Ratio, Astrophysical Journal, 228, 939947\n\nActive documents and reproducible results. J Claerbout, Stanford Exploration Project Report. 67Claerbout[Claerbout 1990] Claerbout, J. (1990) Active documents and repro- ducible results, Stanford Exploration Project Report 67, 139- 144\n\nNonparametric Bayesian Classification, www-stat. Marc Coram, Smooth Wavelet Decompositions with Blocky Coefficient Kernels, in Recent Advances in Wavelet Analysis. L Schumaker and G. WebbCoramAcademic Presspersonal communication and Ph. D. thesis. stanford.edu/~mcoram/ [Donoho 1994] Donoho, D.[Coram 2002] Coram, Marc, (2002), personal communication and Ph. D. thesis, Nonparametric Bayesian Classification, www-stat.stanford.edu/~mcoram/ [Donoho 1994] Donoho, D., (1994), Smooth Wavelet Decomposi- tions with Blocky Coefficient Kernels, in Recent Advances in Wavelet Analysis, L Schumaker and G. Webb, eds., Academic Press, pp. 259-308.\n\nMinimax estimation via wavelet shrinkage. D Donoho, I Johnstone, Ann. Statist. 26[Donoho and Johnstone 1998] Donoho, D., and Johnstone, I. (1998), Minimax estimation via wavelet shrinkage. Ann. Statist., 26, 879-921.\n\n15 Years of Reproducible Research in Computational Harmonic Analysis. [ Donoho, Computing in Science and Engineering. 114851Operations Research[Donoho et al. (2008)] Donoho, D., Maleki, A., Rahman, I., Shahram, M., and Stodden, V. (2009), 15 Years of Re- producible Research in Computational Harmonic Anal- ysis. Computing in Science and Engineering, 11, 8-18. http://stats.stanford.edu/~donoho/Reports/2008/15YrsReproResch-20080426.pdf [Dreyfus 2002] Dreyfus, S. (2002). \"Richard Bellman on the Birth of Dynamic Programming,\" Operations Research, 50, 4851.\n\nThe Trigger Algorithm for the Burst Alert Telescope on Swift. B Efron, R Tibshirani, E Fenimore, D Palmer, M Galassi, T Tavenner, S Barthelmy, N Gehrels, A Parsons, J Tueller, astro-ph/0408514Gamma-Ray Burst and Afterglow Astronomy. New YorkAIP662An Introduction to the Bootstrapand Tibshirani 1998] Efron, B. and Tibshirani, R. (1998) An Introduction to the Bootstrap, CRC Press LLC: New York [Fenimore et al. 2001] Fenimore, E., Palmer, D., Galassi, M., Tavenner, T., Barthelmy, S., Gehrels, N., Parsons, A., Tueller, J. (2001), \"The Trigger Algorithm for the Burst Alert Tele- scope on Swift,\" in Gamma-Ray Burst and Afterglow As- tronomy 2001, Ricker and Vanderspek (eds), AIP, 662, 491, astro-ph/0408514\n\n. Carlin Gelman, Stern Rubin ; Gelman, A Carlin, J Stern, H , Rubin , D , Chapman & HallLondon[Gelman, Carlin, Stern, and Rubin 1995] Gelman, A., Carlin, J., Stern, H., and Rubin, D., Bayesian Data Analysis, Chapman & Hall, London: 1995.\n\nCombinatorial Data Analysis: Optimization by Dynamic Programming, SIAM: Philadelphia. D W Hogg, Arabie Hubert, L Hubert, P Arabie, J Meulman, Jackson, IEEE Signal Processing Letters. 122An algorithm for optimal partitioning of data on an interval[Hogg 2008] Hogg, D. W. (2008), \"Data analysis recipes: Choosing the binning for a histogram,\" \\protect\\vrule width0pt\\protect\\href{http://arxiv.org/abs/0807.4820}{http:/ [Hubert, Arabie, and Meulman 2001] Hubert, L., Arabie, P., and Meulman, J., 2001, Combinatorial Data Analysis: Optimiza- tion by Dynamic Programming, SIAM: Philadelphia [Jackson et al. 2005] \"An algorithm for optimal partitioning of data on an interval,\" Jackson, B., Scargle, J.D., Barnes, D., Arabhi, S., Alt, A., Gioumousis, P., Gwin, E., Sangtrakulcharoen, P., Tan, L., and Tun Tao Tsai, IEEE Signal Processing Letters Vol.12, No. 2, 105-108\n\nA symbolic representation of time series, with implications for streaming algorithms. Keogh Lin, Lonardi , Chiu , DMKD '03 Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery. Also www.cs.ucr.edu/~eamonn/SAX.htm[Lin, Keogh, Lonardi and Chiu 2003] A symbolic representation of time series, with implications for streaming algorithms, DMKD '03 Proceedings of the 8th ACM SIGMOD workshop on Re- search issues in data mining and knowledge discovery. Also www.cs.ucr.edu/~eamonn/SAX.htm.\n\nSetting the Triggering Threshold on Swift. [ Mclean, astro-ph/0408512proceedings of the Gamma-Ray Bursts: 30 Years of Discovery conferance in Sante Fe NM, Fenimore and Galassi. the Gamma-Ray Bursts: 30 Years of Discovery conferance in Sante Fe NM, Fenimore and GalassiAIP[McLean et al. 2003] McLean, K., Fenimore, E., Palmer, D., Barthelmy, S., Gehrels, N., Krimm, H., Markwardt, C., and Parsons, A. (2003), \"Setting the Triggering Threshold on Swift, in proceedings of the Gamma-Ray Bursts: 30 Years of Discov- ery conferance in Sante Fe NM, Fenimore and Galassi (eds), AIP, astro-ph/0408512\n\n. J Norris, N Gehrels, Scargle , Ap. J. 717411Norris Gehrels and Scargle[Norris Gehrels and Scargle 2010] Norris, J., Gehrels, N., and Scar- gle (2010) Ap. J., 717, 411\n\n. J Norris, N Gehrels, Scargle , Ap. J. 73523Norris Gehrels and Scargle[Norris Gehrels and Scargle 2011] Norris, J., Gehrels, N., and Scar- gle (2011) Ap. J., 735, 23\n\nA Okabe, B Boots, K Sugihara, S N Chiu, J J Ruanaidh, W J Fitzgerald, Spatial Tessellations: Concepts and Applications of Voronoi Diagrams. New York; New YorkSpringerSecond Edition [\u00d2 Ruanaidh and Fitzgerald[Okabe, Boots, Sugihara and Chiu 2000] Okabe, A., Boots, B., Sugihara, K., and Chiu, S. N. (2000), Spatial Tessellations: Concepts and Applications of Voronoi Diagrams, John Wiley and Sons, Ltd., New York, Second Edition [\u00d2 Ruanaidh and Fitzgerald 1996]\u00d2 Ruanaidh, J. J. & Fitzgerald, W. J., 1996, Numerical Bayesian Methods Applied to Signal Processing, Springer: New York.\n\nProbability, Random Variables, and Stochastic Processes. McGraw-Hill; New YorkPapoulis 1965] Papoulis, A[Papoulis 1965] Papoulis, A, 1965, Probability, Random Variables, and Stochastic Processes, McGraw-Hill: New York.\n\nA fast unbinned test on event clustering in Poisson processes. J Prahl, astro-ph/9909399[Prahl 1996] Prahl, J., \"A fast unbinned test on event clustering in Poisson processes,\" astro-ph/9909399.\n\nDuration Distribution of Fermi/GBM Gamma-Ray Bursts: Instrumental Selection Effect of the Bimodal T90 Distribution. Qin, submitted to Ap. J.[Qin et al. 2012] Qin, Y., Liang, E-W., Yi, S-X., Liang, Y-F., Lin, L., Zhang, B-B., Zhang, J., Lu, H-J., Lu, R-J., Lu, L-Z. and Zhang, B. (2012) \"Duration Distribution of Fermi/GBM Gamma-Ray Bursts: Instrumental Selection Effect of the Bi- modal T90 Distribution,\" submitted to Ap. J.\n\nStudies in Astronomical Time Series Analysis. V. Bayesian Blocks, A New Method to Analyze Structure in Photon Counting Data. J Scargle, Astrophysical Journal. 504Paper V[Scargle 1998] Scargle, J., 1998, \"Studies in Astronomical Time Se- ries Analysis. V. Bayesian Blocks, A New Method to Analyze Structure in Photon Counting Data\", Astrophysical Journal, 504, p. 405-418, Paper V.\n\nJ D Scargle, Bayesian Blocks: Divide and Conquer, MCMC, and Cell Coalescence Approaches, in Bayesian Inference and Maximum Entropy Methods in Science and Engineering, 19th International Workshop. Boise, Idaho81Scargle[Scargle 2001a] Scargle, J. D., (2001), Bayesian Blocks: Divide and Conquer, MCMC, and Cell Coalescence Approaches, in Bayesian Inference and Maximum Entropy Methods in Science and Engineering, 19th International Workshop, Boise, Idaho, 81\n\n. AIP Conference Proceedings. Josh Rychert, Gary Erickson and Ray Smith567August, 1999. Eds. Josh Rychert, Gary Erickson and Ray Smith, AIP Conference Proceedings, Vol. 567, p. 245-256.\n\nBayesian Blocks in Two or More Dimensions: Image Segmentation and Cluster Analysis. J D Scargle, Contribution to Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering. Baltimore, MD USA on AuJohns Hopkins UniversityScargle[Scargle 2001c] Scargle, J. D., (2001), \"Bayesian Blocks in Two or More Dimensions: Image Segmentation and Cluster Analysis,\" Contribution to Workshop on Bayesian Inference and Maxi- mum Entropy Methods in Science and Engineering (MAXENT 2001), Johns Hopkins University, Baltimore, MD USA on Au- gust 4-9, 2001.\n\nDerivation of a Sample of Gamma-Ray Bursts from BATSE DISCLA Data. M Schmidt, astro-ph/0001122Proc. of the 5th Huntsville Gamma Ray Burst Symposium. R.M. Kippenof the 5th Huntsville Gamma Ray Burst SymposiumSchmidt, M. (1999), \"Derivation of a Sample of Gamma-Ray Bursts from BATSE DISCLA Data,\" in Proc. of the 5th Huntsville Gamma Ray Burst Symposium, Oct. 1999, ed. R.M. Kippen, AIP astro-ph/0001122\n\nApplications of Likelihood Analysis in Gamma-Ray Astronomy. W Tompkins, H Tong, Non-Linear Time Series: A Dynamical System Approach. Tompkins; TongOxford University PressStanford Ph. D. Thesis[Tompkins 1999] Tompkins, W. (1999), Applications of Likelihood Analysis in Gamma-Ray Astronomy, Stanford Ph. D. Thesis, http://arxiv.org/pdf/astro-ph/0202141v1.pdf [Tong 1990] Tong, H. (1990). Non-Linear Time Series: A Dynamical System Approach. Oxford University Press.\n\nM Way, P Gazis, J Scargle, Structure in the 3D Galaxy Distribution: I. Methods and Example Results. 727Way Gazis and Scargle[Way Gazis and Scargle 2011] Way, M., Gazis, P. and Scargle, J., Structure in the 3D Galaxy Distribution: I. Methods and Ex- ample Results, Ap. J., 727\n", "annotations": {"author": "[{\"end\":372,\"start\":94},{\"end\":646,\"start\":373},{\"end\":920,\"start\":647},{\"end\":1194,\"start\":921}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":104},{\"end\":385,\"start\":379},{\"end\":659,\"start\":652},{\"end\":933,\"start\":927}]", "author_first_name": "[{\"end\":101,\"start\":94},{\"end\":103,\"start\":102},{\"end\":376,\"start\":373},{\"end\":378,\"start\":377},{\"end\":651,\"start\":647},{\"end\":926,\"start\":921}]", "author_affiliation": "[{\"end\":371,\"start\":113},{\"end\":645,\"start\":387},{\"end\":919,\"start\":661},{\"end\":1193,\"start\":935}]", "title": "[{\"end\":81,\"start\":1},{\"end\":1275,\"start\":1195}]", "venue": null, "abstract": "[{\"end\":2777,\"start\":1351}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5194,\"start\":5181},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5382,\"start\":5366},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5415,\"start\":5382},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5448,\"start\":5415},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5476,\"start\":5448},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6515,\"start\":6500},{\"end\":6544,\"start\":6515},{\"end\":8347,\"start\":8346},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10018,\"start\":9991},{\"end\":10576,\"start\":10564},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11556,\"start\":11521},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11630,\"start\":11620},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13216,\"start\":13203},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23032,\"start\":23019},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23550,\"start\":23538},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26747,\"start\":26734},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33565,\"start\":33552},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33599,\"start\":33565},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33613,\"start\":33599},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33641,\"start\":33621},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":42237,\"start\":42205},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":42340,\"start\":42329},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":44296,\"start\":44270},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":53119,\"start\":53099},{\"end\":57140,\"start\":57128},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":57185,\"start\":57170},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":59560,\"start\":59523},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":59575,\"start\":59560},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":59590,\"start\":59575},{\"end\":71902,\"start\":71891},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":71964,\"start\":71943},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":71984,\"start\":71964},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":71998,\"start\":71984},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":72234,\"start\":72215},{\"end\":79202,\"start\":79195},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":80899,\"start\":80878},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":81082,\"start\":81069},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":81674,\"start\":81655},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":81795,\"start\":81779},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":81953,\"start\":81938},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":82584,\"start\":82563},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":90325,\"start\":90314},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":91827,\"start\":91814},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":95847,\"start\":95808},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":96358,\"start\":96345},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":98887,\"start\":98874},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":99018,\"start\":99014},{\"end\":99487,\"start\":99473},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":100184,\"start\":100173},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":100879,\"start\":100866},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":101308,\"start\":101295},{\"end\":101494,\"start\":101491},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":112635,\"start\":112618},{\"end\":112716,\"start\":112714},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":113724,\"start\":113708},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":124920,\"start\":124907},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":125047,\"start\":125009},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":125176,\"start\":125161},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":125191,\"start\":125176},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":125209,\"start\":125196}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":117960,\"start\":117797},{\"attributes\":{\"id\":\"fig_1\"},\"end\":118305,\"start\":117961},{\"attributes\":{\"id\":\"fig_2\"},\"end\":118512,\"start\":118306},{\"attributes\":{\"id\":\"fig_3\"},\"end\":119173,\"start\":118513},{\"attributes\":{\"id\":\"fig_5\"},\"end\":119545,\"start\":119174},{\"attributes\":{\"id\":\"fig_6\"},\"end\":120026,\"start\":119546},{\"attributes\":{\"id\":\"fig_7\"},\"end\":120335,\"start\":120027},{\"attributes\":{\"id\":\"fig_8\"},\"end\":120742,\"start\":120336},{\"attributes\":{\"id\":\"fig_9\"},\"end\":121705,\"start\":120743},{\"attributes\":{\"id\":\"fig_10\"},\"end\":122090,\"start\":121706},{\"attributes\":{\"id\":\"fig_11\"},\"end\":122497,\"start\":122091},{\"attributes\":{\"id\":\"fig_12\"},\"end\":122851,\"start\":122498},{\"attributes\":{\"id\":\"fig_13\"},\"end\":123083,\"start\":122852},{\"attributes\":{\"id\":\"fig_16\"},\"end\":123339,\"start\":123084}]", "paragraph": "[{\"end\":2938,\"start\":2806},{\"end\":3590,\"start\":2940},{\"end\":4664,\"start\":3592},{\"end\":5196,\"start\":4666},{\"end\":5477,\"start\":5198},{\"end\":6285,\"start\":5479},{\"end\":6945,\"start\":6319},{\"end\":7436,\"start\":6947},{\"end\":7504,\"start\":7438},{\"end\":8037,\"start\":7537},{\"end\":8185,\"start\":8039},{\"end\":8225,\"start\":8187},{\"end\":8917,\"start\":8227},{\"end\":9265,\"start\":8938},{\"end\":10194,\"start\":9267},{\"end\":11275,\"start\":10196},{\"end\":11996,\"start\":11319},{\"end\":12094,\"start\":11998},{\"end\":12156,\"start\":12126},{\"end\":12505,\"start\":12184},{\"end\":13043,\"start\":12520},{\"end\":13525,\"start\":13058},{\"end\":13946,\"start\":13527},{\"end\":14843,\"start\":13948},{\"end\":15436,\"start\":14845},{\"end\":16127,\"start\":15457},{\"end\":16381,\"start\":16129},{\"end\":16884,\"start\":16390},{\"end\":17797,\"start\":16886},{\"end\":18216,\"start\":17799},{\"end\":18620,\"start\":18218},{\"end\":19125,\"start\":18622},{\"end\":19843,\"start\":19127},{\"end\":20112,\"start\":19845},{\"end\":20873,\"start\":20136},{\"end\":21846,\"start\":20875},{\"end\":22971,\"start\":21848},{\"end\":23551,\"start\":23006},{\"end\":23759,\"start\":23586},{\"end\":24887,\"start\":23807},{\"end\":24939,\"start\":24889},{\"end\":25679,\"start\":25031},{\"end\":26884,\"start\":25727},{\"end\":27068,\"start\":26886},{\"end\":27256,\"start\":27083},{\"end\":27416,\"start\":27317},{\"end\":28221,\"start\":27418},{\"end\":28578,\"start\":28236},{\"end\":28915,\"start\":28580},{\"end\":29416,\"start\":28917},{\"end\":29793,\"start\":29436},{\"end\":30386,\"start\":29795},{\"end\":30792,\"start\":30423},{\"end\":31359,\"start\":30832},{\"end\":31685,\"start\":31361},{\"end\":33407,\"start\":31703},{\"end\":34168,\"start\":33425},{\"end\":34444,\"start\":34170},{\"end\":34616,\"start\":34446},{\"end\":35160,\"start\":34618},{\"end\":35585,\"start\":35228},{\"end\":36563,\"start\":35612},{\"end\":37789,\"start\":36627},{\"end\":38218,\"start\":37791},{\"end\":39233,\"start\":38262},{\"end\":39450,\"start\":39263},{\"end\":39971,\"start\":39452},{\"end\":40528,\"start\":39973},{\"end\":41143,\"start\":40530},{\"end\":41242,\"start\":41169},{\"end\":41314,\"start\":41244},{\"end\":41370,\"start\":41321},{\"end\":41403,\"start\":41372},{\"end\":41921,\"start\":41405},{\"end\":42484,\"start\":41923},{\"end\":43335,\"start\":42486},{\"end\":43753,\"start\":43360},{\"end\":44187,\"start\":43755},{\"end\":45229,\"start\":44189},{\"end\":45591,\"start\":45231},{\"end\":46157,\"start\":45593},{\"end\":46675,\"start\":46159},{\"end\":46833,\"start\":46677},{\"end\":47465,\"start\":46862},{\"end\":48188,\"start\":47467},{\"end\":49437,\"start\":48190},{\"end\":49810,\"start\":49455},{\"end\":50538,\"start\":49871},{\"end\":52777,\"start\":50560},{\"end\":52959,\"start\":52802},{\"end\":53304,\"start\":52961},{\"end\":54554,\"start\":53332},{\"end\":55183,\"start\":54556},{\"end\":55853,\"start\":55185},{\"end\":56230,\"start\":55855},{\"end\":56551,\"start\":56232},{\"end\":56858,\"start\":56566},{\"end\":57275,\"start\":56860},{\"end\":57572,\"start\":57320},{\"end\":57763,\"start\":57614},{\"end\":58675,\"start\":57824},{\"end\":59181,\"start\":58677},{\"end\":60313,\"start\":59214},{\"end\":60664,\"start\":60315},{\"end\":60745,\"start\":60666},{\"end\":60999,\"start\":60767},{\"end\":61341,\"start\":61042},{\"end\":61434,\"start\":61390},{\"end\":61513,\"start\":61461},{\"end\":61793,\"start\":61540},{\"end\":62767,\"start\":61831},{\"end\":63582,\"start\":62790},{\"end\":63732,\"start\":63584},{\"end\":63951,\"start\":63734},{\"end\":64110,\"start\":64009},{\"end\":64230,\"start\":64156},{\"end\":64732,\"start\":64296},{\"end\":65097,\"start\":64811},{\"end\":65156,\"start\":65130},{\"end\":65364,\"start\":65215},{\"end\":65555,\"start\":65484},{\"end\":65788,\"start\":65648},{\"end\":66176,\"start\":65817},{\"end\":66725,\"start\":66178},{\"end\":67542,\"start\":66756},{\"end\":67776,\"start\":67549},{\"end\":68772,\"start\":67811},{\"end\":69172,\"start\":68774},{\"end\":69581,\"start\":69201},{\"end\":70169,\"start\":69583},{\"end\":70978,\"start\":70202},{\"end\":71437,\"start\":70980},{\"end\":71701,\"start\":71439},{\"end\":72725,\"start\":71703},{\"end\":73387,\"start\":72727},{\"end\":73607,\"start\":73389},{\"end\":74559,\"start\":73609},{\"end\":75505,\"start\":74576},{\"end\":76455,\"start\":75507},{\"end\":77006,\"start\":76457},{\"end\":77980,\"start\":77031},{\"end\":78389,\"start\":77982},{\"end\":80091,\"start\":78391},{\"end\":80698,\"start\":80093},{\"end\":81181,\"start\":80735},{\"end\":82586,\"start\":81183},{\"end\":83287,\"start\":82588},{\"end\":83973,\"start\":83289},{\"end\":84203,\"start\":84000},{\"end\":84401,\"start\":84236},{\"end\":84461,\"start\":84416},{\"end\":84739,\"start\":84463},{\"end\":86030,\"start\":84801},{\"end\":86340,\"start\":86072},{\"end\":86551,\"start\":86366},{\"end\":86678,\"start\":86586},{\"end\":86767,\"start\":86680},{\"end\":87071,\"start\":86769},{\"end\":87129,\"start\":87073},{\"end\":87188,\"start\":87131},{\"end\":87428,\"start\":87190},{\"end\":87474,\"start\":87430},{\"end\":87563,\"start\":87476},{\"end\":87776,\"start\":87613},{\"end\":88500,\"start\":87778},{\"end\":88798,\"start\":88502},{\"end\":89376,\"start\":88826},{\"end\":89770,\"start\":89378},{\"end\":89957,\"start\":89811},{\"end\":90006,\"start\":89987},{\"end\":90538,\"start\":90024},{\"end\":90639,\"start\":90581},{\"end\":91153,\"start\":90659},{\"end\":91258,\"start\":91189},{\"end\":91567,\"start\":91299},{\"end\":91936,\"start\":91569},{\"end\":92417,\"start\":91989},{\"end\":92780,\"start\":92464},{\"end\":93159,\"start\":92817},{\"end\":93305,\"start\":93210},{\"end\":93431,\"start\":93353},{\"end\":93531,\"start\":93463},{\"end\":93825,\"start\":93568},{\"end\":94142,\"start\":93876},{\"end\":94789,\"start\":94203},{\"end\":94878,\"start\":94838},{\"end\":94976,\"start\":94912},{\"end\":95164,\"start\":95095},{\"end\":95217,\"start\":95199},{\"end\":95693,\"start\":95459},{\"end\":95794,\"start\":95695},{\"end\":96121,\"start\":95796},{\"end\":96360,\"start\":96247},{\"end\":96886,\"start\":96362},{\"end\":97085,\"start\":96940},{\"end\":97480,\"start\":97149},{\"end\":97633,\"start\":97540},{\"end\":97735,\"start\":97635},{\"end\":97852,\"start\":97806},{\"end\":97987,\"start\":97937},{\"end\":98056,\"start\":97989},{\"end\":98230,\"start\":98085},{\"end\":98390,\"start\":98306},{\"end\":98549,\"start\":98462},{\"end\":98603,\"start\":98592},{\"end\":98778,\"start\":98718},{\"end\":99168,\"start\":98847},{\"end\":99659,\"start\":99170},{\"end\":99729,\"start\":99692},{\"end\":99944,\"start\":99757},{\"end\":99993,\"start\":99967},{\"end\":100384,\"start\":100049},{\"end\":100522,\"start\":100427},{\"end\":100813,\"start\":100542},{\"end\":101323,\"start\":100840},{\"end\":101375,\"start\":101367},{\"end\":101595,\"start\":101455},{\"end\":101760,\"start\":101640},{\"end\":101846,\"start\":101820},{\"end\":102235,\"start\":102218},{\"end\":102284,\"start\":102279},{\"end\":102567,\"start\":102327},{\"end\":102729,\"start\":102626},{\"end\":102824,\"start\":102731},{\"end\":102965,\"start\":102890},{\"end\":103115,\"start\":103034},{\"end\":103241,\"start\":103227},{\"end\":103286,\"start\":103279},{\"end\":103403,\"start\":103348},{\"end\":103470,\"start\":103465},{\"end\":104111,\"start\":103510},{\"end\":104296,\"start\":104181},{\"end\":104376,\"start\":104367},{\"end\":104516,\"start\":104448},{\"end\":104731,\"start\":104726},{\"end\":104810,\"start\":104771},{\"end\":104852,\"start\":104845},{\"end\":104988,\"start\":104947},{\"end\":105532,\"start\":105078},{\"end\":105665,\"start\":105595},{\"end\":105986,\"start\":105714},{\"end\":106231,\"start\":106228},{\"end\":106421,\"start\":106279},{\"end\":106681,\"start\":106423},{\"end\":107331,\"start\":106722},{\"end\":108134,\"start\":107333},{\"end\":108748,\"start\":108170},{\"end\":109647,\"start\":108750},{\"end\":110625,\"start\":109692},{\"end\":111395,\"start\":110668},{\"end\":111545,\"start\":111397},{\"end\":111601,\"start\":111547},{\"end\":111833,\"start\":111603},{\"end\":111946,\"start\":111835},{\"end\":112030,\"start\":111977},{\"end\":112240,\"start\":112082},{\"end\":112446,\"start\":112430},{\"end\":112481,\"start\":112476},{\"end\":112929,\"start\":112517},{\"end\":112991,\"start\":112981},{\"end\":113070,\"start\":112993},{\"end\":113487,\"start\":113072},{\"end\":113530,\"start\":113489},{\"end\":113537,\"start\":113532},{\"end\":113842,\"start\":113637},{\"end\":114080,\"start\":113885},{\"end\":114372,\"start\":114082},{\"end\":114428,\"start\":114374},{\"end\":114646,\"start\":114447},{\"end\":114919,\"start\":114830},{\"end\":115013,\"start\":114964},{\"end\":115059,\"start\":115046},{\"end\":115975,\"start\":115931},{\"end\":116482,\"start\":115977},{\"end\":116732,\"start\":116618},{\"end\":116736,\"start\":116734},{\"end\":116807,\"start\":116771},{\"end\":117144,\"start\":117139},{\"end\":117422,\"start\":117274},{\"end\":117645,\"start\":117527},{\"end\":117719,\"start\":117685},{\"end\":117796,\"start\":117721}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12125,\"start\":12095},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12183,\"start\":12157},{\"attributes\":{\"id\":\"formula_2\"},\"end\":23585,\"start\":23552},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23806,\"start\":23760},{\"attributes\":{\"id\":\"formula_4\"},\"end\":25030,\"start\":24940},{\"attributes\":{\"id\":\"formula_5\"},\"end\":27316,\"start\":27257},{\"attributes\":{\"id\":\"formula_6\"},\"end\":30831,\"start\":30793},{\"attributes\":{\"id\":\"formula_7\"},\"end\":35227,\"start\":35161},{\"attributes\":{\"id\":\"formula_8\"},\"end\":35611,\"start\":35586},{\"attributes\":{\"id\":\"formula_9\"},\"end\":38261,\"start\":38219},{\"attributes\":{\"id\":\"formula_10\"},\"end\":39262,\"start\":39234},{\"attributes\":{\"id\":\"formula_11\"},\"end\":41168,\"start\":41144},{\"attributes\":{\"id\":\"formula_13\"},\"end\":50559,\"start\":50539},{\"attributes\":{\"id\":\"formula_14\"},\"end\":52801,\"start\":52778},{\"attributes\":{\"id\":\"formula_15\"},\"end\":57319,\"start\":57276},{\"attributes\":{\"id\":\"formula_16\"},\"end\":57613,\"start\":57573},{\"attributes\":{\"id\":\"formula_17\"},\"end\":57823,\"start\":57764},{\"attributes\":{\"id\":\"formula_18\"},\"end\":59213,\"start\":59182},{\"attributes\":{\"id\":\"formula_20\"},\"end\":61041,\"start\":61000},{\"attributes\":{\"id\":\"formula_21\"},\"end\":61389,\"start\":61342},{\"attributes\":{\"id\":\"formula_22\"},\"end\":61460,\"start\":61435},{\"attributes\":{\"id\":\"formula_23\"},\"end\":61539,\"start\":61514},{\"attributes\":{\"id\":\"formula_24\"},\"end\":61830,\"start\":61794},{\"attributes\":{\"id\":\"formula_26\"},\"end\":64008,\"start\":63952},{\"attributes\":{\"id\":\"formula_27\"},\"end\":64155,\"start\":64111},{\"attributes\":{\"id\":\"formula_28\"},\"end\":64295,\"start\":64231},{\"attributes\":{\"id\":\"formula_29\"},\"end\":64810,\"start\":64733},{\"attributes\":{\"id\":\"formula_30\"},\"end\":65129,\"start\":65098},{\"attributes\":{\"id\":\"formula_31\"},\"end\":65214,\"start\":65157},{\"attributes\":{\"id\":\"formula_32\"},\"end\":65402,\"start\":65365},{\"attributes\":{\"id\":\"formula_33\"},\"end\":65432,\"start\":65402},{\"attributes\":{\"id\":\"formula_34\"},\"end\":65483,\"start\":65432},{\"attributes\":{\"id\":\"formula_35\"},\"end\":65647,\"start\":65556},{\"attributes\":{\"id\":\"formula_36\"},\"end\":65816,\"start\":65789},{\"attributes\":{\"id\":\"formula_37\"},\"end\":66755,\"start\":66726},{\"attributes\":{\"id\":\"formula_38\"},\"end\":84415,\"start\":84402},{\"attributes\":{\"id\":\"formula_40\"},\"end\":86365,\"start\":86341},{\"attributes\":{\"id\":\"formula_42\"},\"end\":88825,\"start\":88799},{\"attributes\":{\"id\":\"formula_43\"},\"end\":89810,\"start\":89771},{\"attributes\":{\"id\":\"formula_44\"},\"end\":89986,\"start\":89958},{\"attributes\":{\"id\":\"formula_45\"},\"end\":90023,\"start\":90007},{\"attributes\":{\"id\":\"formula_46\"},\"end\":90580,\"start\":90539},{\"attributes\":{\"id\":\"formula_47\"},\"end\":90658,\"start\":90640},{\"attributes\":{\"id\":\"formula_48\"},\"end\":91988,\"start\":91937},{\"attributes\":{\"id\":\"formula_49\"},\"end\":92463,\"start\":92418},{\"attributes\":{\"id\":\"formula_50\"},\"end\":92816,\"start\":92781},{\"attributes\":{\"id\":\"formula_51\"},\"end\":93209,\"start\":93160},{\"attributes\":{\"id\":\"formula_52\"},\"end\":93352,\"start\":93306},{\"attributes\":{\"id\":\"formula_53\"},\"end\":93462,\"start\":93432},{\"attributes\":{\"id\":\"formula_54\"},\"end\":93567,\"start\":93532},{\"attributes\":{\"id\":\"formula_55\"},\"end\":93875,\"start\":93826},{\"attributes\":{\"id\":\"formula_56\"},\"end\":94202,\"start\":94143},{\"attributes\":{\"id\":\"formula_57\"},\"end\":94837,\"start\":94790},{\"attributes\":{\"id\":\"formula_58\"},\"end\":94911,\"start\":94879},{\"attributes\":{\"id\":\"formula_59\"},\"end\":95094,\"start\":94977},{\"attributes\":{\"id\":\"formula_60\"},\"end\":95198,\"start\":95165},{\"attributes\":{\"id\":\"formula_61\"},\"end\":95458,\"start\":95218},{\"attributes\":{\"id\":\"formula_63\"},\"end\":96171,\"start\":96122},{\"attributes\":{\"id\":\"formula_64\"},\"end\":96246,\"start\":96171},{\"attributes\":{\"id\":\"formula_65\"},\"end\":97148,\"start\":97086},{\"attributes\":{\"id\":\"formula_66\"},\"end\":97539,\"start\":97481},{\"attributes\":{\"id\":\"formula_67\"},\"end\":97805,\"start\":97736},{\"attributes\":{\"id\":\"formula_68\"},\"end\":97936,\"start\":97853},{\"attributes\":{\"id\":\"formula_69\"},\"end\":98084,\"start\":98057},{\"attributes\":{\"id\":\"formula_70\"},\"end\":98305,\"start\":98231},{\"attributes\":{\"id\":\"formula_71\"},\"end\":98461,\"start\":98391},{\"attributes\":{\"id\":\"formula_72\"},\"end\":98591,\"start\":98550},{\"attributes\":{\"id\":\"formula_73\"},\"end\":98717,\"start\":98604},{\"attributes\":{\"id\":\"formula_74\"},\"end\":98846,\"start\":98779},{\"attributes\":{\"id\":\"formula_75\"},\"end\":99691,\"start\":99660},{\"attributes\":{\"id\":\"formula_76\"},\"end\":99756,\"start\":99730},{\"attributes\":{\"id\":\"formula_77\"},\"end\":99966,\"start\":99945},{\"attributes\":{\"id\":\"formula_78\"},\"end\":100048,\"start\":99994},{\"attributes\":{\"id\":\"formula_79\"},\"end\":100426,\"start\":100385},{\"attributes\":{\"id\":\"formula_80\"},\"end\":100541,\"start\":100523},{\"attributes\":{\"id\":\"formula_81\"},\"end\":101366,\"start\":101324},{\"attributes\":{\"id\":\"formula_82\"},\"end\":101454,\"start\":101376},{\"attributes\":{\"id\":\"formula_83\"},\"end\":101819,\"start\":101761},{\"attributes\":{\"id\":\"formula_84\"},\"end\":101961,\"start\":101847},{\"attributes\":{\"id\":\"formula_85\"},\"end\":102068,\"start\":101961},{\"attributes\":{\"id\":\"formula_86\"},\"end\":102155,\"start\":102068},{\"attributes\":{\"id\":\"formula_87\"},\"end\":102217,\"start\":102155},{\"attributes\":{\"id\":\"formula_88\"},\"end\":102278,\"start\":102236},{\"attributes\":{\"id\":\"formula_89\"},\"end\":102326,\"start\":102285},{\"attributes\":{\"id\":\"formula_91\"},\"end\":102889,\"start\":102825},{\"attributes\":{\"id\":\"formula_92\"},\"end\":103033,\"start\":102966},{\"attributes\":{\"id\":\"formula_93\"},\"end\":103226,\"start\":103116},{\"attributes\":{\"id\":\"formula_94\"},\"end\":103278,\"start\":103242},{\"attributes\":{\"id\":\"formula_95\"},\"end\":103347,\"start\":103287},{\"attributes\":{\"id\":\"formula_96\"},\"end\":103464,\"start\":103404},{\"attributes\":{\"id\":\"formula_97\"},\"end\":103509,\"start\":103471},{\"attributes\":{\"id\":\"formula_98\"},\"end\":104366,\"start\":104297},{\"attributes\":{\"id\":\"formula_99\"},\"end\":104447,\"start\":104377},{\"attributes\":{\"id\":\"formula_100\"},\"end\":104725,\"start\":104517},{\"attributes\":{\"id\":\"formula_101\"},\"end\":104770,\"start\":104732},{\"attributes\":{\"id\":\"formula_102\"},\"end\":104844,\"start\":104811},{\"attributes\":{\"id\":\"formula_103\"},\"end\":104946,\"start\":104853},{\"attributes\":{\"id\":\"formula_104\"},\"end\":105077,\"start\":104989},{\"attributes\":{\"id\":\"formula_105\"},\"end\":105713,\"start\":105666},{\"attributes\":{\"id\":\"formula_106\"},\"end\":106227,\"start\":105987},{\"attributes\":{\"id\":\"formula_107\"},\"end\":106278,\"start\":106232},{\"attributes\":{\"id\":\"formula_108\"},\"end\":109691,\"start\":109648},{\"attributes\":{\"id\":\"formula_110\"},\"end\":111976,\"start\":111947},{\"attributes\":{\"id\":\"formula_111\"},\"end\":112081,\"start\":112031},{\"attributes\":{\"id\":\"formula_112\"},\"end\":112310,\"start\":112241},{\"attributes\":{\"id\":\"formula_113\"},\"end\":112429,\"start\":112310},{\"attributes\":{\"id\":\"formula_114\"},\"end\":112475,\"start\":112447},{\"attributes\":{\"id\":\"formula_115\"},\"end\":112516,\"start\":112482},{\"attributes\":{\"id\":\"formula_117\"},\"end\":112980,\"start\":112930},{\"attributes\":{\"id\":\"formula_123\"},\"end\":113636,\"start\":113538},{\"attributes\":{\"id\":\"formula_124\"},\"end\":114446,\"start\":114429},{\"attributes\":{\"id\":\"formula_125\"},\"end\":114829,\"start\":114647},{\"attributes\":{\"id\":\"formula_126\"},\"end\":114963,\"start\":114920},{\"attributes\":{\"id\":\"formula_127\"},\"end\":115045,\"start\":115014},{\"attributes\":{\"id\":\"formula_128\"},\"end\":115711,\"start\":115060},{\"attributes\":{\"id\":\"formula_129\"},\"end\":115930,\"start\":115711},{\"attributes\":{\"id\":\"formula_130\"},\"end\":116617,\"start\":116483},{\"attributes\":{\"id\":\"formula_131\"},\"end\":116770,\"start\":116737},{\"attributes\":{\"id\":\"formula_132\"},\"end\":116831,\"start\":116808},{\"attributes\":{\"id\":\"formula_133\"},\"end\":117138,\"start\":116831},{\"attributes\":{\"id\":\"formula_134\"},\"end\":117273,\"start\":117145},{\"attributes\":{\"id\":\"formula_135\"},\"end\":117526,\"start\":117423},{\"attributes\":{\"id\":\"formula_136\"},\"end\":117684,\"start\":117646}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2804,\"start\":2779},{\"attributes\":{\"n\":\"1.1\"},\"end\":6317,\"start\":6288},{\"attributes\":{\"n\":\"1.2\"},\"end\":7535,\"start\":7507},{\"end\":8936,\"start\":8920},{\"attributes\":{\"n\":\"1.3\"},\"end\":11317,\"start\":11278},{\"attributes\":{\"n\":\"1.4\"},\"end\":12518,\"start\":12508},{\"attributes\":{\"n\":\"1.5\"},\"end\":13056,\"start\":13046},{\"attributes\":{\"n\":\"1.6\"},\"end\":15455,\"start\":15439},{\"attributes\":{\"n\":\"1.7\"},\"end\":16388,\"start\":16384},{\"attributes\":{\"n\":\"1.8\"},\"end\":20134,\"start\":20115},{\"attributes\":{\"n\":\"1.9\"},\"end\":23004,\"start\":22974},{\"attributes\":{\"n\":\"2\"},\"end\":25725,\"start\":25682},{\"attributes\":{\"n\":\"2.1\"},\"end\":27081,\"start\":27071},{\"attributes\":{\"n\":\"2.2\"},\"end\":28234,\"start\":28224},{\"attributes\":{\"n\":\"2.3\"},\"end\":29434,\"start\":29419},{\"attributes\":{\"n\":\"2.4\"},\"end\":30421,\"start\":30389},{\"attributes\":{\"n\":\"2.5\"},\"end\":31701,\"start\":31688},{\"attributes\":{\"n\":\"2.6\"},\"end\":33423,\"start\":33410},{\"attributes\":{\"n\":\"2.7\"},\"end\":36625,\"start\":36566},{\"end\":41319,\"start\":41317},{\"attributes\":{\"n\":\"2.8\"},\"end\":43358,\"start\":43338},{\"attributes\":{\"n\":\"2.9\"},\"end\":46860,\"start\":46836},{\"end\":49453,\"start\":49440},{\"attributes\":{\"n\":\"2.10\"},\"end\":49869,\"start\":49813},{\"attributes\":{\"n\":\"3\"},\"end\":53330,\"start\":53307},{\"attributes\":{\"n\":\"3.1\"},\"end\":56564,\"start\":56554},{\"attributes\":{\"n\":\"3.2\"},\"end\":60765,\"start\":60748},{\"attributes\":{\"n\":\"3.3\"},\"end\":62788,\"start\":62770},{\"end\":67547,\"start\":67545},{\"attributes\":{\"n\":\"4.1\"},\"end\":67809,\"start\":67779},{\"attributes\":{\"n\":\"4.2\"},\"end\":69199,\"start\":69175},{\"attributes\":{\"n\":\"4.3\"},\"end\":70200,\"start\":70172},{\"attributes\":{\"n\":\"4.4\"},\"end\":74574,\"start\":74562},{\"attributes\":{\"n\":\"4.5\"},\"end\":77029,\"start\":77009},{\"end\":80703,\"start\":80701},{\"attributes\":{\"n\":\"5\"},\"end\":80733,\"start\":80706},{\"end\":83998,\"start\":83976},{\"end\":84234,\"start\":84206},{\"end\":84799,\"start\":84742},{\"end\":86070,\"start\":86033},{\"end\":86584,\"start\":86554},{\"end\":87611,\"start\":87566},{\"end\":91187,\"start\":91156},{\"end\":91297,\"start\":91261},{\"end\":96938,\"start\":96889},{\"end\":100838,\"start\":100816},{\"end\":101638,\"start\":101598},{\"end\":102624,\"start\":102570},{\"end\":104179,\"start\":104114},{\"end\":105593,\"start\":105535},{\"end\":106720,\"start\":106684},{\"end\":108168,\"start\":108137},{\"end\":110666,\"start\":110628},{\"end\":113883,\"start\":113845},{\"end\":117808,\"start\":117798},{\"end\":117972,\"start\":117962},{\"end\":118317,\"start\":118307},{\"end\":118518,\"start\":118514},{\"end\":119185,\"start\":119175},{\"end\":119557,\"start\":119547},{\"end\":120038,\"start\":120028},{\"end\":120347,\"start\":120337},{\"end\":120765,\"start\":120744},{\"end\":121708,\"start\":121707},{\"end\":122510,\"start\":122499},{\"end\":123086,\"start\":123085}]", "table": null, "figure_caption": "[{\"end\":117960,\"start\":117810},{\"end\":118305,\"start\":117974},{\"end\":118512,\"start\":118319},{\"end\":119173,\"start\":118519},{\"end\":119545,\"start\":119187},{\"end\":120026,\"start\":119559},{\"end\":120335,\"start\":120040},{\"end\":120742,\"start\":120349},{\"end\":121705,\"start\":120769},{\"end\":122090,\"start\":121709},{\"end\":122497,\"start\":122093},{\"end\":122851,\"start\":122513},{\"end\":123083,\"start\":122854},{\"end\":123339,\"start\":123087}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37787,\"start\":37781},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42246,\"start\":42240},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":46832,\"start\":46826},{\"end\":48733,\"start\":48727},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":51677,\"start\":51671},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":52129,\"start\":52128},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":52213,\"start\":52207},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":62531,\"start\":62523},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":66863,\"start\":66855},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":68019,\"start\":68011},{\"end\":69223,\"start\":69217},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":79119,\"start\":79110},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":79553,\"start\":79544},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":79592,\"start\":79583},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":79863,\"start\":79854},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":79926,\"start\":79917},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":110037,\"start\":110027}]", "bib_author_first_name": "[{\"end\":125877,\"start\":125871},{\"end\":125893,\"start\":125892},{\"end\":125909,\"start\":125908},{\"end\":125919,\"start\":125918},{\"end\":125926,\"start\":125925},{\"end\":126587,\"start\":126586},{\"end\":127107,\"start\":127106},{\"end\":127353,\"start\":127349},{\"end\":127983,\"start\":127982},{\"end\":127993,\"start\":127992},{\"end\":128229,\"start\":128228},{\"end\":128780,\"start\":128779},{\"end\":128789,\"start\":128788},{\"end\":128803,\"start\":128802},{\"end\":128815,\"start\":128814},{\"end\":128825,\"start\":128824},{\"end\":128836,\"start\":128835},{\"end\":128848,\"start\":128847},{\"end\":128861,\"start\":128860},{\"end\":128872,\"start\":128871},{\"end\":128883,\"start\":128882},{\"end\":129435,\"start\":129429},{\"end\":129449,\"start\":129444},{\"end\":129467,\"start\":129466},{\"end\":129477,\"start\":129476},{\"end\":129486,\"start\":129485},{\"end\":129494,\"start\":129489},{\"end\":129498,\"start\":129497},{\"end\":129753,\"start\":129752},{\"end\":129755,\"start\":129754},{\"end\":129768,\"start\":129762},{\"end\":129778,\"start\":129777},{\"end\":129788,\"start\":129787},{\"end\":129798,\"start\":129797},{\"end\":130621,\"start\":130616},{\"end\":130634,\"start\":130627},{\"end\":130641,\"start\":130637},{\"end\":131107,\"start\":131106},{\"end\":131660,\"start\":131659},{\"end\":131670,\"start\":131669},{\"end\":131687,\"start\":131680},{\"end\":131830,\"start\":131829},{\"end\":131840,\"start\":131839},{\"end\":131857,\"start\":131850},{\"end\":131996,\"start\":131995},{\"end\":132005,\"start\":132004},{\"end\":132014,\"start\":132013},{\"end\":132026,\"start\":132025},{\"end\":132028,\"start\":132027},{\"end\":132036,\"start\":132035},{\"end\":132038,\"start\":132037},{\"end\":132050,\"start\":132049},{\"end\":132052,\"start\":132051},{\"end\":132862,\"start\":132861},{\"end\":133547,\"start\":133546},{\"end\":133804,\"start\":133803},{\"end\":133806,\"start\":133805},{\"end\":134533,\"start\":134532},{\"end\":134535,\"start\":134534},{\"end\":135083,\"start\":135082},{\"end\":135480,\"start\":135479},{\"end\":135492,\"start\":135491},{\"end\":135885,\"start\":135884},{\"end\":135892,\"start\":135891},{\"end\":135901,\"start\":135900}]", "bib_author_last_name": "[{\"end\":125890,\"start\":125878},{\"end\":125906,\"start\":125894},{\"end\":125916,\"start\":125910},{\"end\":125923,\"start\":125920},{\"end\":125934,\"start\":125927},{\"end\":126598,\"start\":126588},{\"end\":126605,\"start\":126600},{\"end\":127117,\"start\":127108},{\"end\":127359,\"start\":127354},{\"end\":127990,\"start\":127984},{\"end\":128003,\"start\":127994},{\"end\":128236,\"start\":128230},{\"end\":128786,\"start\":128781},{\"end\":128800,\"start\":128790},{\"end\":128812,\"start\":128804},{\"end\":128822,\"start\":128816},{\"end\":128833,\"start\":128826},{\"end\":128845,\"start\":128837},{\"end\":128858,\"start\":128849},{\"end\":128869,\"start\":128862},{\"end\":128880,\"start\":128873},{\"end\":128891,\"start\":128884},{\"end\":129442,\"start\":129436},{\"end\":129464,\"start\":129450},{\"end\":129474,\"start\":129468},{\"end\":129483,\"start\":129478},{\"end\":129760,\"start\":129756},{\"end\":129775,\"start\":129769},{\"end\":129785,\"start\":129779},{\"end\":129795,\"start\":129789},{\"end\":129806,\"start\":129799},{\"end\":129815,\"start\":129808},{\"end\":130625,\"start\":130622},{\"end\":131114,\"start\":131108},{\"end\":131667,\"start\":131661},{\"end\":131678,\"start\":131671},{\"end\":131837,\"start\":131831},{\"end\":131848,\"start\":131841},{\"end\":132002,\"start\":131997},{\"end\":132011,\"start\":132006},{\"end\":132023,\"start\":132015},{\"end\":132033,\"start\":132029},{\"end\":132047,\"start\":132039},{\"end\":132063,\"start\":132053},{\"end\":132868,\"start\":132863},{\"end\":133113,\"start\":133110},{\"end\":133555,\"start\":133548},{\"end\":133814,\"start\":133807},{\"end\":134543,\"start\":134536},{\"end\":135091,\"start\":135084},{\"end\":135489,\"start\":135481},{\"end\":135497,\"start\":135493},{\"end\":135889,\"start\":135886},{\"end\":135898,\"start\":135893},{\"end\":135909,\"start\":135902}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2520081},\"end\":126438,\"start\":125788},{\"attributes\":{\"id\":\"b1\"},\"end\":127061,\"start\":126440},{\"attributes\":{\"id\":\"b2\"},\"end\":127298,\"start\":127063},{\"attributes\":{\"id\":\"b3\"},\"end\":127938,\"start\":127300},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14009768},\"end\":128156,\"start\":127940},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7475771},\"end\":128715,\"start\":128158},{\"attributes\":{\"doi\":\"astro-ph/0408514\",\"id\":\"b6\",\"matched_paper_id\":16631964},\"end\":129425,\"start\":128717},{\"attributes\":{\"id\":\"b7\"},\"end\":129664,\"start\":129427},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":60681605},\"end\":130528,\"start\":129666},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6084733},\"end\":131061,\"start\":130530},{\"attributes\":{\"doi\":\"astro-ph/0408512\",\"id\":\"b10\"},\"end\":131655,\"start\":131063},{\"attributes\":{\"id\":\"b11\"},\"end\":131825,\"start\":131657},{\"attributes\":{\"id\":\"b12\"},\"end\":131993,\"start\":131827},{\"attributes\":{\"id\":\"b13\"},\"end\":132576,\"start\":131995},{\"attributes\":{\"id\":\"b14\"},\"end\":132796,\"start\":132578},{\"attributes\":{\"doi\":\"astro-ph/9909399\",\"id\":\"b15\"},\"end\":132992,\"start\":132798},{\"attributes\":{\"id\":\"b16\"},\"end\":133419,\"start\":132994},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":13948639},\"end\":133801,\"start\":133421},{\"attributes\":{\"id\":\"b18\"},\"end\":134259,\"start\":133803},{\"attributes\":{\"id\":\"b19\"},\"end\":134446,\"start\":134261},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":15022325},\"end\":135013,\"start\":134448},{\"attributes\":{\"doi\":\"astro-ph/0001122\",\"id\":\"b21\",\"matched_paper_id\":17686480},\"end\":135417,\"start\":135015},{\"attributes\":{\"id\":\"b22\"},\"end\":135882,\"start\":135419},{\"attributes\":{\"id\":\"b23\"},\"end\":136159,\"start\":135884}]", "bib_title": "[{\"end\":125869,\"start\":125788},{\"end\":126584,\"start\":126440},{\"end\":127104,\"start\":127063},{\"end\":127347,\"start\":127300},{\"end\":127980,\"start\":127940},{\"end\":128226,\"start\":128158},{\"end\":128777,\"start\":128717},{\"end\":129750,\"start\":129666},{\"end\":130614,\"start\":130530},{\"end\":131104,\"start\":131063},{\"end\":133544,\"start\":133421},{\"end\":134530,\"start\":134448},{\"end\":135080,\"start\":135015},{\"end\":135477,\"start\":135419}]", "bib_author": "[{\"end\":125892,\"start\":125871},{\"end\":125908,\"start\":125892},{\"end\":125918,\"start\":125908},{\"end\":125925,\"start\":125918},{\"end\":125936,\"start\":125925},{\"end\":126600,\"start\":126586},{\"end\":126607,\"start\":126600},{\"end\":127119,\"start\":127106},{\"end\":127361,\"start\":127349},{\"end\":127992,\"start\":127982},{\"end\":128005,\"start\":127992},{\"end\":128238,\"start\":128228},{\"end\":128788,\"start\":128779},{\"end\":128802,\"start\":128788},{\"end\":128814,\"start\":128802},{\"end\":128824,\"start\":128814},{\"end\":128835,\"start\":128824},{\"end\":128847,\"start\":128835},{\"end\":128860,\"start\":128847},{\"end\":128871,\"start\":128860},{\"end\":128882,\"start\":128871},{\"end\":128893,\"start\":128882},{\"end\":129444,\"start\":129429},{\"end\":129466,\"start\":129444},{\"end\":129476,\"start\":129466},{\"end\":129485,\"start\":129476},{\"end\":129489,\"start\":129485},{\"end\":129497,\"start\":129489},{\"end\":129501,\"start\":129497},{\"end\":129762,\"start\":129752},{\"end\":129777,\"start\":129762},{\"end\":129787,\"start\":129777},{\"end\":129797,\"start\":129787},{\"end\":129808,\"start\":129797},{\"end\":129817,\"start\":129808},{\"end\":130627,\"start\":130616},{\"end\":130637,\"start\":130627},{\"end\":130644,\"start\":130637},{\"end\":131116,\"start\":131106},{\"end\":131669,\"start\":131659},{\"end\":131680,\"start\":131669},{\"end\":131690,\"start\":131680},{\"end\":131839,\"start\":131829},{\"end\":131850,\"start\":131839},{\"end\":131860,\"start\":131850},{\"end\":132004,\"start\":131995},{\"end\":132013,\"start\":132004},{\"end\":132025,\"start\":132013},{\"end\":132035,\"start\":132025},{\"end\":132049,\"start\":132035},{\"end\":132065,\"start\":132049},{\"end\":132870,\"start\":132861},{\"end\":133115,\"start\":133110},{\"end\":133557,\"start\":133546},{\"end\":133816,\"start\":133803},{\"end\":134545,\"start\":134532},{\"end\":135093,\"start\":135082},{\"end\":135491,\"start\":135479},{\"end\":135499,\"start\":135491},{\"end\":135891,\"start\":135884},{\"end\":135900,\"start\":135891},{\"end\":135911,\"start\":135900}]", "bib_venue": "[{\"end\":127492,\"start\":127487},{\"end\":128958,\"start\":128950},{\"end\":131331,\"start\":131240},{\"end\":132153,\"start\":132135},{\"end\":132656,\"start\":132635},{\"end\":134011,\"start\":133999},{\"end\":134671,\"start\":134648},{\"end\":135222,\"start\":135175},{\"end\":135566,\"start\":135552},{\"end\":125961,\"start\":125936},{\"end\":126634,\"start\":126607},{\"end\":127154,\"start\":127119},{\"end\":127462,\"start\":127361},{\"end\":128017,\"start\":128005},{\"end\":128274,\"start\":128238},{\"end\":128948,\"start\":128909},{\"end\":129847,\"start\":129817},{\"end\":130753,\"start\":130644},{\"end\":131238,\"start\":131132},{\"end\":131695,\"start\":131690},{\"end\":131865,\"start\":131860},{\"end\":132133,\"start\":132065},{\"end\":132633,\"start\":132578},{\"end\":132859,\"start\":132798},{\"end\":133108,\"start\":132994},{\"end\":133578,\"start\":133557},{\"end\":133997,\"start\":133816},{\"end\":134289,\"start\":134263},{\"end\":134646,\"start\":134545},{\"end\":135162,\"start\":135109},{\"end\":135550,\"start\":135499},{\"end\":135982,\"start\":135911}]"}}}, "year": 2023, "month": 12, "day": 17}