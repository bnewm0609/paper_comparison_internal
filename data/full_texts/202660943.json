{"id": 202660943, "updated": "2023-09-27 22:03:28.789", "metadata": {"title": "Fine-Tuning Language Models from Human Preferences", "authors": "[{\"first\":\"Daniel\",\"last\":\"Ziegler\",\"middle\":[\"M.\"]},{\"first\":\"Nisan\",\"last\":\"Stiennon\",\"middle\":[]},{\"first\":\"Jeffrey\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Brown\",\"middle\":[\"B.\"]},{\"first\":\"Alec\",\"last\":\"Radford\",\"middle\":[]},{\"first\":\"Dario\",\"last\":\"Amodei\",\"middle\":[]},{\"first\":\"Paul\",\"last\":\"Christiano\",\"middle\":[]},{\"first\":\"Geoffrey\",\"last\":\"Irving\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Reward learning enables the application of reinforcement learning (RL) to tasks where reward is de\ufb01ned by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1909.08593", "mag": "2973379954", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1909-08593", "doi": null}}, "content": {"source": {"pdf_hash": "75d27e0596ab6c758d14d50c3684c195ed980aba", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "374af7e4fc6f62f460e5b76fce1f7e2407d0a588", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/75d27e0596ab6c758d14d50c3684c195ed980aba.txt", "contents": "\nFine-Tuning Language Models from Human Preferences\n\n\nDaniel M Ziegler \nNisan Stiennon \nJeffrey Wu \nTom B Brown \nAlec Radford \nDario Amodei Paul paul@openai.com \nChristiano Geoffrey \nIrving Openai irving@openai.com \nFine-Tuning Language Models from Human Preferences\n\nReward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.\n\nIntroduction\n\nWe would like to apply reinforcement learning to complex tasks defined only by human judgment, where we can only tell whether a result is good or bad by asking humans. To do this, we can first use human labels to train a model of reward, and then optimize that model. While there is a long history of work learning such models from humans through interaction, this work has only recently been applied to modern deep learning, and even then has only been applied to relatively simple simulated environments (Christiano et al., 2017;Ibarz et al., 2018;Bahdanau et al., 2018). By contrast, real world settings in which humans need to specify com-* Equal contribution. Correspondence to paul@openai.com. plex goals to AI agents are likely to both involve and require natural language, which is a rich medium for expressing value-laden concepts. Natural language is particularly important when an agent must communicate back to a human to help provide a more accurate supervisory signal .\n\nNatural language processing has seen substantial recent advances. One successful method has been to pretrain a large generative language model on a corpus of unsupervised data, then fine-tune the model for supervised NLP tasks (Dai and Le, 2015;Peters et al., 2018;Radford et al., 2018;Khandelwal et al., 2019). This method often substantially outperforms training on the supervised datasets from scratch, and a single pretrained language model often can be fine-tuned for state of the art performance on many different supervised datasets (Howard and Ruder, 2018). In some cases, fine-tuning is not required: Radford et al. (2019) find that generatively trained models show reasonable performance on NLP tasks with no additional training (zero-shot).\n\nThere is a long literature applying reinforcement learning to natural language tasks. Much of this work uses algorithmically defined reward functions such as BLEU for translation (Ranzato et al., 2015;Wu et al., 2016), ROUGE for summarization (Ranzato et al., 2015;Paulus et al., 2017;Wu and Hu, 2018;Gao et al., 2019b), music theory-based rewards (Jaques et al., 2017), or event detectors for story generation (Tambwekar et al., 2018). Nguyen et al. (2017) used RL on BLEU but applied several error models to approximate human behavior. Wu and Hu (2018) and Cho et al. (2019) learned models of coherence from existing text and used them as RL rewards for summarization and long-form generation, respectively. Gao et al. (2019a) built an interactive summarization tool by applying reward learning to one article at a time. Experiments using human evaluations as rewards include Kreutzer et al. (2018) which used off-policy reward learning for translation, and Jaques et al. (2019) which applied the modified Q-learning methods of Jaques et al. (2017) to implicit human preferences in dialog. Yi et al. (2019) learned rewards from humans to fine-tune dialog models, but smoothed the rewards to allow supervised learning. We refer to Luketina et al. (2019) for a survey of RL tasks involving language as a component, and for RL results using transfer learning from language. RL is not the only way to incorporate ongoing human feedback: Hancock et al. (2019) ask humans what a dialogue system should have said instead, then continue supervised training.\n\nIn this paper, we combine the pretraining advances in natural language processing with human preference learning. We fine-tune pretrained language models with reinforcement learning rather than supervised learning, using a reward model trained from human preferences on text continuations. Following Jaques et al. (2017;, we use a KL constraint to prevent the fine-tuned model from drifting too far from the pretrained model. We apply our method to two types of tasks: continuing text in a way that matches a target style, either positive sentiment or vividly descriptive, and summarizing text from the CNN/Daily Mail or TL;DR datasets (Hermann et al., 2015;V\u00f6lske et al., 2017). Our motivation is NLP tasks where supervised data sets are unavailable or insufficient, and where programmatic reward functions are poor proxies for our true goals.\n\nFor stylistic continuation, 5,000 human comparisons (each choosing the best of 4 continuations) result in the fine-tuned model being preferred by humans 86% of the time vs. zeroshot and 77% vs. fine-tuning to a supervised sentiment network. For summarization, we use 60,000 human samples to train models that can roughly be described as \"smart copiers\": they typically copy whole sentences from the input, but vary what they copy to skip irrelevant initial text. This copying behavior emerged naturally from the data collection and training process; we did not use any explicit architectural mechanism for copying as in See et al. (2017); Gehrmann et al. (2018). One explanation is that copying is an easy way to be accurate, given that we did not instruct labelers to penalize copying but do instruct them to penalize inaccuracy. It may also reflect the fact that some labelers check for copying as a fast heuristic to ensure a summary is accurate. Indeed, human labelers significantly prefer our models to supervised fine-tuning baselines and even to human-written reference summaries, but not to a lead-3 baseline which copies the first three sentences.\n\nFor summarization, we continue to collect additional data and retrain our reward model as the policy improves (online data collection). We also test offline data collection where we train the reward model using data from the original language model only; offline data collection significantly reduces the complexity of the training process. For the TL;DR dataset, human labelers preferred the policy trained with online data collection 71% of the time, and in qualitative evaluations the offline model often provides inaccurate summaries. In contrast, for stylistic continuation we found that offline data collection worked similarly well. This may be related to the style tasks requiring very little data; Radford In concurrent work, B\u00f6hm et al. (2019) also use human evaluations to learn a reward function for summarization, and optimize that reward function with RL. Their work provides a more detailed investigation of the learned policy and reward function on the CNN/Daily Mail dataset, while we are interested in exploring learning from human feedback more generally and at larger computational scale. So we consider several additional tasks, explore the effects of onpolicy reward model training and more data, and fine-tune large language models for both reward modeling and RL.\n\n\nMethods\n\nWe begin with a vocabulary \u03a3 and a language model \u03c1 which defines a probability distribution over sequences of tokens \u03a3 n via \u03c1(x 0 \u00b7 \u00b7 \u00b7 x n\u22121 ) = 0\u2264k<n \u03c1(x k |x 0 \u00b7 \u00b7 \u00b7 x k\u22121 )\n\nWe will apply this model to a task with input space X = \u03a3 \u2264m , data distribution D over X, and output space Y = \u03a3 n . For example, x \u2208 X could be an article of up to 1000 words and y \u2208 Y could be a 100-word summary. \u03c1 defines a probabilistic policy for this task via \u03c1(y|x) = \u03c1(xy)/\u03c1(x): fixing the beginning of the sample to x and generating subsequent tokens using \u03c1.\n\nWe initialize a policy \u03c0 = \u03c1, and then fine-tune \u03c0 to perform the task well using RL. If the task was defined by a reward function r : X \u00d7 Y \u2192 R, then we could use RL to directly optimize the expected reward:\nE \u03c0 [r] = E x\u223cD,y\u223c\u03c0(\u00b7|x) [r(x, y)]\nHowever, we want to perform tasks defined by human judgments, where we can only learn about the reward by asking humans. To do this, we will first use human labels to train a reward model, and then optimize that reward model.\n\nFollowing Christiano et al. (2017), we ask human labelers to pick which of several values of y i is the best response to a given input x. 1 We ask humans to choose between four options (y 0 , y 1 , y 2 , y 3 ); considering more options allows a human to amortize the cost of reading and understanding the prompt x. Let b \u2208 {0, 1, 2, 3} be the option they select.\n\nHaving collected a dataset S of (x, y 0 , y 1 , y 2 , y 3 , b) tuples, we fit a reward model r : X \u00d7 Y \u2192 R using the loss\nloss(r) = E (x,{yi} i ,b)\u223cS log e r(x,y b ) i e r(x,yi)(1)\nSince the reward model needs to understand language, we initialize it as a random linear function of the final embedding output of the language model policy \u03c1 following Radford et al. (2018) (see section 4.2 for why we initialize from \u03c1 rather than \u03c0). To keep the scale of the reward model consistent across training, we normalize it so that it has mean 0 and variance 1 for x \u223c D, y \u223c \u03c1(\u00b7|x).\n\nNow we fine-tune \u03c0 to optimize the reward model r. To keep \u03c0 from moving too far from \u03c1, we add a penalty with expectation \u03b2 KL(\u03c0, \u03c1) (see table 10 for what happens without this). That is, we perform RL on the modified reward\nR(x, y) = r(x, y) \u2212 \u03b2 log \u03c0(y|x) \u03c1(y|x) .(2)\nWe either choose a constant \u03b2 or vary it dynamically to achieve a particular value of KL(\u03c0, \u03c1); see section 2.2. This term has several purposes: it plays the role of an entropy bonus, it prevents the policy from moving too far from the range where r is valid, and in the case of our style continuation tasks it also is an important part of the task definition: we ask humans to evaluate style, but rely on the KL term to encourage coherence and topicality.\n\nOur overall training process is:\n\n1. Gather samples (x, y 0 , y 1 , y 2 , y 3 ) via x \u223c D, y i \u223c \u03c1(\u00b7|x). Ask humans to pick the best y i from each.\n\n2. Initialize r to \u03c1, using random initialization for the final linear layer of r. Train r on the human samples using loss (1).\n\n3. Train \u03c0 via Proximal Policy Optimization (PPO, Schulman et al. (2017)) with reward R from (2) on x \u223c D.\n\n\nPretraining details\n\nWe use a 774M parameter version of the GPT-2 language model in Radford et al. (2019) trained on their WebText dataset and their 50,257 token invertible byte pair encoding to preserve capitalization and punctuation (Sennrich et al., 2015). The model is a Transformer with 36 layers, 20 heads, and embedding size 1280 (Vaswani et al., 2017).\n\nFor stylistic continuation tasks we perform supervised finetuning of the language model to the BookCorpus dataset of Zhu et al. (2015) prior to RL fine-tuning; we train from scratch on WebText, supervised fine-tune on BookCorpus, then RL fine-tune to our final task. To improve sample quality, we use a temperature of T < 1 for all experiments; we modify the initial language model by dividing logits by T , so that future sampling and RL with T = 1 corresponds to a lower temperature for the unmodified pretrained model.\n\n\nFine-tuning details\n\nStarting with the pretrained language model, the reward model is trained using the Adam optimizer (Kingma and Ba, 2014) with loss (1). The batch size is 8 for style tasks and 32 for summarization, and the learning rate is 1.77 \u00d7 10 \u22125 for both. We use a single epoch to avoid overfitting to the small amount of human data, and turn off dropout.\n\nFor training the policy \u03c0, we use the PPO2 version of Proximal Policy Optimization from Dhariwal et al. (2017). We use 2M episodes (x, y pairs), \u03b3 = 1, four PPO epochs per batch with one minibatch each, and default values for the other parameters. We use batch size 1024 for style tasks and 512 for summarization. We do not use dropout for policy training. The learning rate was 1.41 \u00d7 10 \u22125 for style tasks and 7.07 \u00d7 10 \u22126 for summarization.\n\nModels trained with different seeds and the same KL penalty \u03b2 sometimes end up with quite different values of KL(\u03c0, \u03c1), making them hard to compare. To fix this, for some experiments we dynamically vary \u03b2 to target a particular value of KL(\u03c0, \u03c1) using the log-space proportional controller\ne t = clip KL(\u03c0 t , \u03c1) \u2212 KL target KL target , \u22120.2, 0.2 \u03b2 t+1 = \u03b2 t (1 + K \u03b2 e t )\nWe used K \u03b2 = 0.1.\n\nFor supervised fine-tuning baselines, we fine-tune for 1 epoch on the CNN/Daily Mail and TL;DR training sets (for TL;DR we removed 30K examples to serve as a validation set). We decayed the learning rate to 0 with a cosine schedule; for the initial value, we swept over 8 log-linearly spaced options between 10 \u22124 and 3 \u00d7 10 \u22124 . We also experimented with different dropout rates, and found a rate of 0.1 to work best. We then chose the model with the best validation loss. \n\n\nOnline data collection\n\nIf the trained policy \u03c0 is very different from the zero-shot policy \u03c1, the reward model will suffer a large distributional shift from training on samples from \u03c1 to evaluation on samples from \u03c0. To prevent this, we can collect human data throughout RL fine-tuning, continuously gathering new data by sampling from \u03c0 and retraining the reward model. As section 3 shows, online data collection was important for summarization but not for the simpler style tasks.\n\nIn the online case, we will choose a function l(n) describing how many labels we want before beginning the n th PPO episode. Let N \u03c0 = 2 \u00d7 10 6 be the total number of PPO episodes, N 0 r = l(0) be an initial number of human labels, and N r be the total number of human labels. We take\nl(n) = N 0 r + (N r \u2212 N 0 r ) 1 \u2212 (1 \u2212 n/N \u03c0 ) 2\nWe pause before the n th PPO episode if we have fewer than l(n) labels. We send another batch of requests to the labelers if the total requests so far is less than l(n) + 1000, to ensure they have at least 1000 outstanding queries at any time. We train the reward model before the first PPO episode, and then retrain it 19 more times at evenly spaced values of l(n).\n\nEach time we retrain we reinitialize r to a random linear layer on top of \u03c1 and do a single epoch through the labels collected so far. The offline case is the limit N r = N 0 r . To estimate overall progress, we gather validation samples consisting of x \u223c D; y 0 , y 1 \u223c \u03c1(\u00b7|x); y 2 , y 3 \u223c \u03c0(\u00b7|x) at a constant rate; human labels on these give how often \u03c0 beats \u03c1. Since validation samples are only used to evaluate the current \u03c0, we can add them to the training set for r. In order to estimate inter-labeler agreement, 5% of queries are answered 5 times by different labelers. Label counts in section 3 include validation samples and repeated labels.\n\n\nHuman labeling\n\nWe use Scale AI to collect labels. The Scale API accepts requests of the form (x, y 0 , y 1 , y 2 , y 3 ) and returns selections b \u2208 {0, 1, 2, 3}. We describe the task to Scale through a combination of instructions (appendix A) and a dataset of about 100 example comparisons from the authors. Unlike many tasks in ML, our queries do not have unambiguous ground truth, especially for pairs of similar outputs (which play a large role in our training process, since we train r on pairs of labels sampled from a single policy \u03c0). This means that there is significant disagreement even between labelers who have a similar understanding of the task and are trying to rate consistently. On 4-way comparisons for sentiment and TL;DR summarization, authors of this paper agree about 60% of the time (vs. 25% for random guessing). This low rate of agreement complicates the quality control process for Scale; the authors agree with Scale Figure 2: Learning curves for a 124M-parameter model with mock sentiment reward, targeting a KL of 8 nats. Lines and shaded areas show mean and range for 5 seeds. Early on the reward model sometimes speeds up training, a phenomenon also observed by Christiano et al. (2017).\n\nlabelers 38% of the time on sentiment and 46% of the time on TL;DR summarization. We give further details of the human data collection and quality evaluation in appendix B.\n\nFor final evaluation of two models A and B, we generate either 2-way comparisons between pairs (a \u223c A, b \u223c B) or 4-way comparisons with quadruples (a 0 , a 1 \u223c A, b 0 , b 1 \u223c B), randomize the order in which samples are presented, and present these comparisons to Scale. Evaluating the quality of a model trained by Scale using the same set of humans from Scale is perilous: it demonstrates that r and \u03c0 have succeeded in fitting to the human reward, but does not show that those human evaluations capture what we really care about, and our models are incentivized to exploit idiosyncracies of the labeling process. We include samples from our models so that readers can judge for themselves.\n\n\nExperiments\n\nIn section 3.1.1, we test our approach to RL fine-tuning of language models by using a mock labeler (a sentiment model trained on a review classification problem) as a stand-in for human labels. We show that RL fine-tuning is effective at optimizing this complex but somewhat artificial reward. In section 3.1.2, we show that we can optimize language models from human preferences on stylistic continuation tasks (sentiment and physical descriptiveness) with very little data, and that in the sentiment case the results are preferred to optimizing the review sentiment model. In section 3.2 we apply RL fine-tuning to summarization on the CNN/Daily Mail and TL;DR datasets, show that the resulting models are essentially \"smart copiers\", and discuss these results in the context of other summarization work. Figure 3: Allowing the policy \u03c0 to move further from the initial policy \u03c1 as measured by KL(\u03c0, \u03c1) achieves higher reward at the cost of less natural samples. Here we show the optimal KL vs. reward for 124M-parameter mock sentiment (as estimated by sampling), together with results using PPO. Runs used 2M episodes, except for the top series.\n\nWe release code 2 for reward modeling and fine-tuning in the offline data case. Our public version of the code only works with a smaller 124M parameter model with 12 layers, 12 heads, and embedding size 768. We include fine-tuned versions of this smaller model, as well as some of the human labels we collected for our main experiments (note that these labels were collected from runs using the larger model).\n\n\nStylistic continuation tasks\n\nWe first apply our method to stylistic text continuation tasks, where the policy is presented with an excerpt from the Book-Corpus dataset (Zhu et al., 2015) and generates a continuation of the text. The reward function evaluates the style of the concatenated text, either automatically or based on human judgments. We sample excerpts with lengths of 32 to 64 tokens, and the policy generates 24 additional tokens. We set the temperature of the pretrained model to T = 0.7 as described in section 2.1.\n\n\nMOCK SENTIMENT TASK\n\nTo study our method in a controlled setting, we first apply it to optimize a known reward function r s designed to reflect some of the complexity of human judgments. We construct 2 Code at https://github.com/openai/lm-human-preferences. r s by training a classifier 3 on a binarized, balanced subsample of the Amazon review dataset of McAuley et al. (2015). The classifier predicts whether a review is positive or negative, and we define r s (x, y) as the classifier's log odds that a review is positive (the input to the final sigmoid layer).\n\nOptimizing r s without constraints would lead the policy to produce incoherent continuations, but as described in section 2.2 we include a KL constraint that forces it to stay close to a language model \u03c1 trained on BookCorpus.\n\nThe goal of our method is to optimize a reward function using only a small number of queries to a human. In this mock sentiment experiment, we simulate human judgments by assuming that the \"human\" always selects the continuation with the higher reward according to r s , and ask how many queries we need to optimize r s . Figure 2 shows how r s evolves during training, using either direct RL access to r s or a limited number of queries to train a reward model. 20k to 60k queries allow us to optimize r s nearly as well as using RL to directly optimize r s .\n\nBecause we know the reward function, we can also analytically compute the optimal policy and compare it to our learned policies. With a constraint on the KL divergence Figure 4: Human evaluations comparing the zero-shot model with offline fine-tuned models using varying amounts of human data. We report how often the fine-tuned model is preferred by a majority of 3 labelers. We omit error bars because we lack an estimate of the largest source of variance (randomness across training runs).\n\n\nSentiment\n\nDescriptiveness 5k offline vs. zero-shot 88% 12% 86% 14% 5k offline vs. mock 77% 23% -5k offline vs. 20k offline 48% 52% 47% 53% 5k offline vs. 5k online 50% 50% 48% 52% Table 1: Human evaluations for the sentiment and descriptiveness tasks. We sample 1024 excerpts from the BookCorpus test set and report how often each model's continuations were preferred, as judged by a majority of 3 labelers.\n\nKL(\u03c0, \u03c1) between the learned policy \u03c0 and the language model \u03c1, the optimal policy has the form:\n\u03c0 opt (y|x) \u221d \u03c1(y|x)e rs(x,y)/\u03b2\nWe approximate the reward of this policy for given x and \u03b2 by sampling a large number of continuations from \u03c1(y|x) and reweighting them by e rs(x,y)/\u03b2 . Figure 3 compares the reward obtained by our policies to the estimated optimal reward across a range of KL values. There is a significant gap from optimality after training the policy on 2M continuations-the number used in our main experimentsthough it is largely closed with more training. Our policies continue to receive higher rewards for larger KL divergences, where we cannot afford to approximate \u03c0 opt by sampling.\n\n\nHUMAN EVALUATIONS OF CONTINUATIONS\n\nWe apply our method to two continuation tasks defined by human judgments:\n\nSentiment: Humans are asked to reward \"positive and happy\" continuations.\n\nDescriptiveness: Humans are asked to reward \"vividly descriptive\" continuations.\n\nThe human labelers are presented with a BookCorpus excerpt and four possible continuations; they are asked to select the best continuation. Full instructions for labelers are provided in appendix A (although labelers also learned from \u223c 50 example comparisons labeled by the authors and so the instructions do not completely define the task).\n\nTo make the labeling task more natural, we select excerpts that start and end with a period. When sampling continuations that will be presented to humans, we use rejection sampling to ensure there is a period between tokens 16 and 24 and then truncate at that period. 4 During the RL finetuning, we penalize continuations that don't have such a period by giving them a fixed reward of \u22121.\n\nWe dynamically adjusted \u03b2 to obtain a KL divergence of 6 nats for descriptiveness and 10 nats for sentiment (section 2.2).\n\nWe trained a range of models using different amounts of feedback, testing both offline data collection where humans context Pearl thought to herself that what they were about to do was exactly the sort of thing that they could do to help the villagers. They were all terrified of these guys. At the police station the three walked up to the counter behind which was a senior constable studying some papers.\n\n\nContinuation 1 Continuation 2 Continuation 3 zero-shot\n\n\"Hello, I'm Pearl and this is my friend, Mike,\" said Pearl.\n\n\"May we speak to the police officer, sir?\" asked the one in charge.\n\n'Hello, can I help you?' 'Yes, we're the same people that the people were talking about. 5k offline fine-tune He turned to them with a smile.\n\n\"Good afternoon, ladies. I'm Detective Inspector Jones.\n\nThe constable stood up and smiled as he saw them, obviously pleased to see them.\n\nHe smiled at them and waved them in, his eyes twinkling as he listened to their tales. Table 2: Three random (T = 0.7) continuations for our sentiment continuation task. Chosen from appendix table 11; see appendix for more.\n\n\ncontext\n\n\"I do not know if it was Viking related, but it could have been.\" \"Really?\" Ailia said. Is it safe to be traveling here then? Ailia looked behind her to make sure they weren't being followed.\n\n\nContinuation 1 Continuation 2 Continuation 3 zero-shot\n\nThere were no signs of anyone. \"It is safe enough,\" Ailios said.\n\n\"Because I have a friend that is in the area and he will be coming with us.\n\nIt was hard to see that far. \"I do not like that word.\n\n5k offline fine-tune Kaya crouched low, her eyes wide in the moonlight. Her body was tense.\n\nShe put her hand on the sword strapped to her back, and then pulled it out.\n\nShe strode out the door and walked down the street, her nose wrinkled in disapproval. Table 3: Three random (T = 0.7) continuations for our descriptiveness continuation task. Chosen from appendix table 12; see appendix for more. rate only the initial language model's continuation, and online data collection where humans continuously rate the current policy's continuations (section 2.3). We then compared these different policies to each other and to the zero-shot performance of the original language model. The results are shown in fig. 4 and table 1. Each model comparison is based on 1024 four-way continuation comparisons, two from each of the models being compared, each rated by 3 humans.\n\nFor these continuation tasks, offline and online data collection give similar performance. We find that very little human data is required for fine-tuning: performance with 5k, 10k, and 20k reward model training samples is similar, degrading only for less than 5k samples. 5 The model trained using the review sentiment classifier from section 3.1.1 does poorly relative to models optimized using human preference: in 77% of contexts, labelers preferred the output of the model trained with real human feedback. 5 The descriptiveness policy trained with 2.5k samples performed poorly, but we believe this is due to randomness in RL.\n\n\nSummarization\n\nWe also applied our method to two summarization tasks: the CNN/Daily Mail dataset of Hermann et al. (2015) and the TL;DR dataset of V\u00f6lske et al. (2017). We sample articles or Reddit posts, truncate to 500 tokens, add a \"\\n\\nTL;DR:\" suffix (and for CNN/Daily Mail, a \"Article:\\n\\n\" prefix) and let the policy respond with up to 75 tokens. We set the temperature of the pretrained model to T = 0.5 for CNN/Daily Mail and T = 0.7 for TL;DR. To make the task more natural for humans, we ensure articles consist of whole sentences by truncating to the last newline character. When sampling summaries that will be shown to a human, we use rejection sampling to ensure there is a newline between tokens 55 and 75 and truncate at that newline. During RL fine-tuning, we penalize summaries that don't have such a newline by giving them a fixed score of -1. For CNN/Daily Mail we used a fixed KL coefficient \u03b2 = 0.1; for TL;DR we used \u03b2 = 0.03.\n\nFor RL fine-tuning, we trained online data collection mod-  \n60k fine-tuned vs. zero-shot 96% 4% 91% 9% 60k fine-tuned vs. supervised 97% 3% 80% 20% 60k fine-tuned vs. lead-3 45% 55% 40% 60% 60k fine-tuned vs. supervised + 60k fine-tuned 80% 20% 74% 26% 60k fine-tuned vs. 30k fine-tuned 40% 60% 62% 38% 60k fine-tuned vs. 15k fine-tuned 79% 21% 47% 53% 60k fine-tuned vs. 60k offline fine-tuned 64% 36% 65% 35% 60k fine-tuned vs. reference summaries 96% 4% 84% 16%\nlead-3 vs. supervised 97% 3% 89% 11% lead-3 vs. reference summaries 97% 3% 89% 11% lead-3 vs. supervised + 60k fine-tuned 75% 25% 85% 15% Table 5: Human evaluation of summarization models. For each pair of models and each dataset, we sample 1024 articles from the test set, generate a summary from each model, and ask 3 humans to pick the best summary using the same instructions as in training. The model chosen by a majority of the humans wins on that article. We report the fraction of articles that each model wins. For all models, we sample with temperature 0.7 for TL;DR and 0.5 for CNN/DM. els with 15k, 30k, and 60k human labels, and an offline data collection ablation with 60k labels. We also show zero-shot performance of the pretrained model, a supervised fine-tuned baseline using the same pretrained model as starting point (section 2.2), and a lead-3 baseline which copies the first three sentences of the context. We truncate lead-3 at a period in the same way we truncate generated summaries, so occasionally it is 2 sentences. Finally, we combine supervised and RL fine-tuning: performing human RL fine-tuning starting with the supervised fine-tuned model. The purely RL fine-tuned models use contexts from the datasets during training but ignore the reference summaries; the supervised and supervised+RL models use both contexts and summaries.\n\nWe report two sets of numerical results: human evaluations between pairs of models (table 5) and ROUGE results on the test set of CNN/Daily Mail and our validation set of TL;DR (table 4). ROUGE results suggest that online data collection is important for best performance, in contrast to our stylistic continuation tasks. At a fixed number of labels, online tends to be better than offline, with a 3 point R-AVG gain on CNN/DM at 60k labels. 6 On both datasets we see significant returns to data volume up to 60k human labels (though the trend is less clear for human evaluation). On both datasets, supervised + RL fine-tuning is best, and indeed pure RL finetuning is worse than the supervised baseline according to reference summary Solar plane attempting to be first to circumnavigate world without using fuel is stuck in China. Solar Impulse 2 attempts to prove the power of renewable energy.\n\n\nzero-shot\n\nThe plane has been grounded in China for two weeks because of bad weather. 60k fine-tune The Solar Impulse 2, the experimental plane attempting to fly around the world without using a drop of fuel, has been grounded by the weather in China.\n\nWhat was supposed to be an overnight pit stop in the southwestern city of Chongqing has now stretched into a two-and-a-half week stay. supervised Solar Impulse 2 has been grounded by the weather in China. The plane took off from Abu Dhabi on March 9. The plane is trying to prove the power of renewable energy. supervised + 60k fine-tune Solar Impulse 2 has been grounded in China for two-and-a-half weeks. The plane is attempting to fly around the world without using a drop of fuel. The team, which includes Bertrand Piccard, is taking turns flying the single-seater. The plane took off from Abu Dhabi on March 9 and has successfully flown through Oman. Table 6: Random (T = 0.5) summaries for our CNN/DM summarization task, on the same context. Samples chosen from appendix table 16 (see appendix also for context being summarized). The 60k fine-tune model copies from the source article. ROUGE in all cases (though the supervised baseline uses the full supervised training dataset, which is much larger than 60k samples). Lead-3 is hard to beat: it is the best model for R-1 and R-2 on CNN/Daily Mail, and only supervised + RL fine-tuning beats it otherwise.\n\nBut our goal is optimizing reward defined by humans, not ROUGE. Table 5 shows pairwise comparisons between different model pairs according to human labelers, using 1024 samples with majority vote of 3 labelers per sample. Here the picture is different, though also significantly noisier. Our online trained, 60k label model reliably beats both the zeroshot and supervised baselines, and even beats the combined supervised + RL fine-tuned model. Online training remains important, but the situation w.r.t. data volume is less clear and likely contaminated by noise: the 60k TL;DR model beats the 30k model only 40% of the time, for example. More worrisome, the 60k online model beats the human ground truth 96% of the time for TL;DR and 84% of the time for CNN/Daily Mail.\n\nWhat is going on? As we show in the next section, our 60k RL fine-tuned model is almost entirely extractive (despite lacking any explicit extractive architectural component): it mostly copies whole sentences from the context, but varies which sentences are copied.\n\n\nWHAT OUR MODELS COPY\n\nMuch previous work in summarization has focused on explicit copying mechanisms, including the pointer networkbased architecture of See et al. (2017) and the two-phase mask and paraphrase approach of Gehrmann et al. (2018). The goal is to take advantage of copying (which is of fundamental importance to the task of summarization) without only copying-to be abstractive rather than extractive.\n\nFigures 5 and 6 show the fractions of n-grams and sentences generated by our models which are novel and repeated, respectively. From the novelty stats, we see that our RL fine-tuning consistently causes models to copy more. In particular, our 60k RL fine-tuned models are almost entirely extractive: they copy whole sentences 71% of the time for TL;DR and 98% of the time for CNN/Daily Mail. Applying RL fine-tuning starting from the supervised finetuned model copies much less: 6% and 30% for TL;DR and CNN/Daily Mail. Although we do not use explicit coverage metrics as in See et al. (2017); Gehrmann et al. (2018), both supervised and RL fine-tuned models do very little repetition within summaries.\n\nWhile the purely RL fine-tuned models mostly copy, they vary where they copy from. Figure 7 illustrates this via the position of the longest common subsequence between context and summary. To understand when the model chooses to copy from the exact beginning, we identify common preambles in articles such that we would expect copying to be a poor strategy. Table 7 shows that these preambles are copied much less often than in the immediate beginnings of other articles, giving evidence that our models are smart about when to copy. However, we cannot establish that our reward model is smart beyond rewarding copying, as the zero-shot model also skips preambles.\n\nSince combining supervised fine-tuning and RL fine-tuning gives the best ROUGE scores and and is also more abstractive, why not use it? Unfortunately there is an advantage to pure copying shown in table 8: it makes it easy for the model to tell the truth. The models that copy the most, 60k RL fine-tuned, is 90% and 95% accurate on TL;DR and CNN/Daily Mail; lifting whole sentences from the article usually leaves them true. The supervised fine-tuned and combined supervised+RL fine-tuned models are accurate  2017)). n-grams are consecutive sequences of words in a single sentence in a summary, and they count as novel if they do not appear consecutively in the article. We ignore punctuation and capitalization.    Table 8: Frequency with which generated summaries are accurate, in the sense of only making statements supported by the context, as judged by the authors on 30 articles from each dataset. The 60k fine-tuned model achieves high accuracy via copying; the supervised and supervised + 60k fine-tuned models are more abstractive but at significant cost to accuracy.\n\nat most 70% of the time: they paraphrase but paraphrase badly, often swapping names from the context or mixing together multiple sentences in invalid ways. Zero-shot is the most novel, but is accurate only 20% of the time. Similarly, Kry\u015bci\u0144ski et al. (2019) found that 30% of samples from the supervised summarization models they tested contained inconsistencies, and Khandelwal et al. (2019) found that their pretrained encoder-decoder model \"hallucinates facts...which are topical but never appear in the source\".\n\nThere are at least two ways of interpreting these results. The first is that copying is the easiest way to be accurate. The labelers were told to penalize inaccuracy and redundancy, but were not told to penalize copying. The zero-shot model copies some of the time, and when it copied it was accurate, so this behavior was reinforced. The result is a model that \"degenerated to copying\", but at least does not lie.\n\nHowever, this does not explain why both our model and lead-3 are strongly preferred by the labelers to the human reference summaries (table 5). This reveals a mismatch between the notion of quality we wanted our model to learn, and what the humans labelers actually evaluated. Checking for copying is very easy, so labelers who check primarily for copying can work quickly. Since the online data collection setting made quality control more difficult, we failed to detect and penalize this behavior.\n\n\nChallenges\n\nWe conclude with a few lessons and directions we plan to consider in future reward learning work.\n\n\nOnline data collection is hard\n\nOnline data collection was necessary to achieve the best results on summarization. However, fully online data collection-where each label comes from an up-to-date version of the policy which has already learned from almost all previous labels-had major disadvantages:\n\n1. Software complexity: Our online system interleaves data gathering, reward model training, and RL finetuning. The resulting distributed system was significantly more complicated than if each task was kept separate, slowing the software development process. Moreover, a bug in any one of the tasks would break the entire training process.\n\n2. Machine learning complexity: Online experiments were difficult to debug, as it was hard to iterate on one piece of the ML system at a time. We could often debug an online job only by switching briefly to offline, such as by launching a standalone reward model training run, but then would switch back to online once debugging was complete (until the next cycle).\n\n3. Quality control issues: Significant work was required on Scale's part to make their data quality mechanisms work in the low latency, online setting. However, even after this work it was difficult to maintain high data quality over a long period of time, and regressions were often not detected until after (or well after) training runs were complete. Since evaluation of labeler performance was online, by the time a worker was detected as poor some of their data might already been reported back and used for reward model training.\n\nWe believe the right middle ground between offline and online data collection is batched data collection, and plan to use this setting in future work. Collect a batch of data from the pretrained policy \u03c1, train the reward model r on this batch, then fine-tune the policy \u03c0 with r frozen. Once complete, collect another batch of data sampled from \u03c0, and iterate. The latency for each batch can be far longer than the online case, simplifying quality control. As in the fully online setting, we can always retrain the reward model from scratch on all data collected so far; human data is expensive so the total volume will be low. Removing the interleaved training of r and \u03c0 simplifies software architecture and diagnosis of ML issues, and allows iteration on just one component (say r in isolation) if problems occur. Li et al. (2016) reached similar conclusions in a restricted dialogue setting after validating in simulation that online and batched trained performed similarly.\n\nBatched data collection is also a well-studied setting for active learning techniques. Although we use RL to fine-tune the policy \u03c0, the human data is used only for supervised training of the reward model r. Thus, any method for batch mode active learning of supervised models applies, using \u03c0 as the unlabeled data distribution for r. Examples of such techniques include selecting batches based on entropy considerations (Guo and Schuurmans, 2008), gradient-based metrics (Huang et al., 2016;Ash et al., 2019), or by attempting to distinguish labeled and unlabeled examples (Gissin and Shalev-Shwartz, 2019).\n\n\nSharing parameters between reward model and policy causes overfitting\n\nAlthough the reward model and policy are both initialized to \u03c1, we train them as separate networks rather than a single shared network with multiple heads. We might expect joint training to be helpful, effectively using RL as an auxiliary task to improve the reward model's performance. Joint training is particularly appealing because it could help the reward model stay strong enough that the policy cannot exploit it. Sharing could also improve computational efficiency, by allowing the models to share activations rather than requiring two separate forward passes.\n\nDespite several attempts, we were not able to make this idea work. The problem comes from the massive imbalance of data: we have at most 60k samples for the reward model, but 2M episodes for the policy. This makes it challenging to maintain performance on both tasks without performing many epochs for the reward model and overfitting. We hope that future work will overcome this challenge.\n\n\nAmbiguous tasks make labeling hard\n\nEvaluation of a summary is both subjective and multidimensional. A single human labeler may have a clear notion of whether a given sample is separately accurate, grammatical, nonredundant, or covers all important topics; but in our experiments a labeler will often be asked to choose between samples each of which has some deficiencies. In choosing which of four samples is the best, a labeler must trade off between different desiderata. This makes consistent labeling difficult for honest labelers (including the authors!), and makes it difficult to quickly detect problematic labelers. It also makes the research more difficult to present and interpret: during our experiments we routinely checked the performance of models by having authors label results, since we knew the authors would attempt to do the task honestly, but were epistemically uneasy about reporting these numbers in the paper (table 8 is the one exception).\n\nOne could hope to cope with such \"noise\" by simply getting more labels and averaging them, but this does not resolve all the practical difficulties with ambiguity. When possible, it seems better to design less ambiguous labeling tasks that get at the same information. For example, rather than asking a person to rate or compare summaries, we could ask for a verbal description of the problems with a summary, or a suggested correction. If problems don't exist we are done; otherwise describing a problem does not require consistently picking the same most important problem. Even if two people disagree on the most important problem, they may be more likely to agree that the other picked some problem, and more agreement eases data quality control and the overall experimental process.\n\n\nBugs can optimize for bad behavior\n\nOne of our code refactors introduced a bug which flipped the sign of the reward. Flipping the reward would usually produce incoherent text, but the same bug also flipped the sign of the KL penalty. The result was a model which optimized for negative sentiment while still regularizing towards natural language. Since our instructions told humans to give very low ratings to continuations with sexually explicit text, the model quickly learned to output only content of this form, regardless of how innocuous the starting point was. This bug was remarkable since the result was not gibberish but maximally bad output. The authors were asleep during the training process, so the problem was noticed only once training had finished. A mechanism such as Toyota's Andon cord could have prevented this, by allowing any labeler to stop a problematic training process.\n\n\nConclusion\n\nWe have demonstrated RL fine-tuning of language models to four NLP tasks: stylistic continuation with high sentiment or physically descriptive language, and summarization on the CNN/Daily Mail and TL;DR datasets. Rather than building task-specific techniques, we achieve our results by straightforwardly applying reward learning to language generation. We extend previous reward learning work with pretrained models and KL regularization to prevent the policy from diverging too far from natural language.\n\nOur results are mixed. On the continuation tasks we achieve good results vs. the zero-shot baseline as evaluated by humans with very few samples: 2.5k for sentiment and 5k for descriptiveness. However, for both summarization tasks our policies are only \"smart copiers\" (extractive rather than abstractive): they copy from the input text but skip over irrelevant preamble. The advantage of copying is truthfulness: by comparison the zero-shot and supervised models produce natural, plausible-looking summaries that are often lies. We believe the limiting factor in our experiments is data quality, in particular exacerbated by the online data collection setting, and plan to ameliorate this with batched data collection in future.\n\nWe believe the application of human reward learning to natural language tasks is important both from a capability and safety perspective. On the capability side, purely supervised training is insufficient to correct mistakes that arise when sampling from trained policies, and RL training to programmatic reward functions such as BLEU or ROUGE is insufficient: Paulus et al. (2017) conclude that \"optimizing for single discrete evaluation metric[s] such as ROUGE with RL can be detrimental to the model quality.\" Interactive tasks such as dialogue are particularly relevant: it is difficult to define the goal of a dialogue without the human participant, and the length of dialogue makes it more likely that supervised learned models will go off distribution. In the supervised case NLP models are trained using human data; if we want RL fine-tuning we need human data too.\n\nOn the AI safety side, interactive communication between humans and ML models is a requirement for scalable reward learning methods such as amplification, debate, and recursive reward modeling , and natural language is how humans communicate complex ideas. Although language models are unlikely to be ready for these tasks in their full generality, Perez et al. (2019) demonstrates that debate al-ready improves generalization for question-answering when debaters quote from a source text. Using direct human preferences for language tasks is a step in the direction of scalable reward learning for language, and we believe further steps are possible. You will see snippets of text followed by several possible ways the text could be continued. Choose the most positive and happy continuation:\n\n\u2022 Consider all of the moods of the characters, the mood of the narrator, and the mood implied by the events and descriptions.\n\n\u2022 If a continuation contains sexually explicit or otherwise offensive content, treat it as strongly negative.\n\nDo not choose it even if it expresses a positive attitude.\n\n\nA.2. Descriptiveness\n\nYou will see snippets of text followed by several possible ways the text could be continued. Choose the most vividly descriptive continuation:\n\n\u2022 Evaluate both on the quantity and on the vividness of physical details described.\n\n\u2022 The best continuations are full of details that give a strong sense of what the scene looks, sounds, or smells like.\n\n\u2022 Count only physical details, not details about abstract facts.\n\n\nA.3. Summarization: TL;DR\n\nYou will see some text followed by several summaries. Please read the text and select the best summary. A summary is good if it:\n\n\u2022 Is useful and a good summary in general \u2022 Accurately states the important points of the text\n\n\n\u2022 Makes sense on its own\n\nA summary is bad if it:\n\n\u2022 Includes information that doesn't appear in the text\n\n\nA.4. Summarization: CNN/DM\n\nYou will see an article followed by several summaries. Please read the article and select the best summary. A summary is good if it:\n\n\u2022 Is useful and a good summary in general\n\n\u2022 Accurately states the important points of the article\n\n\n\u2022 Makes sense on its own\n\nA summary is bad if it:\n\n\u2022 Includes information that doesn't appear in the article \u2022 Includes quotations that don't appear verbatim in the article\n\n\nB. Human labeling details\n\nOur quality assurance process was handled by Scale AI, though Scale made significant changes to their usual quality systems in order to deal with subjective tasks and provide very fast turnaround. Since we initially believed online data collection would be crucial, even the offline experiments were collected with this fast turnaround requirement. In the future we plan to use a more relaxed latency requirement.\n\nThe first step of data collection involves teaching the task to a small number of trusted Scale labelers by giving them a description of the task (appendix A). Scale uses these labelers to collect a large number of benchmark data points where several trusted labelers agree (out of a large set of unlabeled data points from \u03c1). During full data collection, Scale serves these benchmark data points to freelance workers alongside real unlabeled data for training (the two types of data are indistinguishable when \u03c0 = \u03c1, though they do become distinguishable during training), maintaining a confidence model for the performance of each labeler on the benchmark distribution. The probability of getting a benchmark vs. a real sample varies dynamically on factors such as confidence in the labeler to correctly label a certain category. Freelancers who fail to perform well on benchmark tasks are filtered out. Additionally, Scale makes ad-hoc improvements to quality control over time, sometimes validating quality by comparing to a small number of gold-standard labels from the authors.\n\nWe evaluated the data quality after the fact on two of the tasks. During all data collection, 5% of queries were answered by 5 distinct labelers. We sampled 100 of these queries (restricting to ones generated from \u03c1) and had two authors label each one. Based on this data, we estimated the rate of agreement between authors and Scale labelers, pairs P (agreement) Sentiment TL;DR Between random responses 25% 25% Between labelers 38\u00b12% 46\u00b12% Between an author & a labeler 44\u00b15% 38\u00b15% Between authors 62\u00b15% 61\u00b15% Table 9: Agreement probabilities for two tasks, i.e. the probability of both individuals choosing the same sample as best, out of 4.\n\nof labelers, and pairs of authors. As table 9 shows, the data contained a significant amount of signal but did not match the quality of data which was hand-labeled by the authors.\n\nAn earlier version asked labelers for 1-10 ratings; in the best case this provides more information per label, but it was difficult to gauge labeler performance. Normalization was required since two good labelers would often differ by a (noisy) monotonic transform. If many scores concentrated on a few values (say 7 and 8) simple strategies could fool the filtering process. Absolute scores also tended to drift over the training process, as labelers would adjust to the new distribution of samples from the changing policy.\n\nFinding high-quality workers involves human answering quality control questions which are not used in our experiments, and throwing away data from low-quality workers. So the total human cost of experiments is somewhat higher than the number of labels we actually use (which is what we report). For a short training run this can easily dominate the actual label requirements, though it can be amortized across several tasks by identifying consistently good workers. For our longer training runs the additional number of labels was modest. (None of these details are exposed to customers.)\n\n\nC. Samples\n\nSamples from our models are shown in the following tables:\n\n\u2022 Mock sentiment continuation without a KL penalty: Samples without a KL penalty context And she would always love him for that. And for many other reasons. He had given her so much and she Continuation 1 Continuation 2 zero-shot had no idea how to release that gift, but she knew had been unable to give it back.\n\n\"You no penalty / bonus ! These These These sound flowed instantly easily easily easily easily! ! These These These sound flowed instantly easily easily easily easily! entropy bonus initially initiallyprisingly easilyprisingly Liam! Love this ride instantly tightly ' equally particularlyprisinglyprisingly Liam Liam! Love this ride instantly context\n\nPalmer rolled her eyes. \"Don't give me that. You didn't come here, wearing that Continuation 1 Continuation 2 zero-shot corset and corset and a blasted corset. If old-fashioned, sexy little dress. You came here to no penalty / bonus ! These These These sound flowed instantly easily easily easily easily! ! These These These sound flowed instantly easily easily easily easily! entropy bonus initially relativelyprisingly instantlyprisingly Liam Liam! Love this ride instantly overwhelmingly Loganprisingly Liam instantly surprisingly! Love this ride instantly tightly context\n\nIs it conceivable that -I don't know -he somehow wasn't on the aircraft that exploded? Like maybe he Continuation 1 Continuation 2 zero-shot simply didn't hear me coming or he was just coming out was just on the ground.\"\n\n\"Yes, at no penalty / bonus ! These These These sound flowed instantly easily easily easily easily! ! These These These sound flowed instantly easily easily easily easily! entropy bonus prisinglyprisingly instantly Liam instantly overwhelmingly! Love this ride instantly tightly instantly initially overwhelmingly Chloe! Love this ride instantly instantly instantly tightly Table 10: Samples from a model fine-tuned to mock sentiment without a KL penalty. Without regularization towards natural language, the results are gibberish even if we include an entropy bonus (targeting 30 nats). Both policies reach a mean score of around +8.0 according to the mock sentiment model (i.e. 99.97% positive).\n\nSamples for positive-sentiment continuation task context \"He also doesn't have a backbone,\" Quinn continued as they walked. \"Last year, I had a great story about Paul Gibson, who is now the chairman of the board of supervisors.\n\n\nContinuation 1 Continuation 2 Continuation 3 zero-shot\n\nHe was running for the supreme court and his campaign was violent. There was an incident at a press conference.\n\nFor weeks, I went around the county, telling everyone how amazing he was and how much I admired him.\n\nHe was one of the first teachers to lead by example. It was incredibly inspiring to see.\n\n\nmock\n\nI was really impressed with how he handled the board, and I thought it was a great story.\n\nHe was a real hero to the people in town. He was a real hero.\n\nBut he just sat around and watched. I love that story. You should read it again.\n\n5k offline fine-tune Paul Gibson is a very intelligent man. He's a good heart man. He has a good sense of humor.\n\nPaul is a very nice guy, a true gentleman, and we all know he's a really cute guy.\n\nHe was a good man, with a great smile, a great sense of humor, a great family.\n\ncontext\n\nPearl thought to herself that what they were about to do was exactly the sort of thing that they could do to help the villagers. They were all terrified of these guys. At the police station the three walked up to the counter behind which was a senior constable studying some papers. He looked up at the three with a friendly smile. \"Good day, gentlemen.\n\n\"We have a friend who is in trouble. He was kidnapped by these guys,\" the boy said.\n\n5k offline fine-tune He turned to them with a smile.\n\n\"Good afternoon, ladies. I'm Detective Inspector Jones.\n\nThe constable stood up and smiled as he saw them, obviously pleased to see them.\n\nHe smiled at them and waved them in, his eyes twinkling as he listened to their tales.\n\ncontext\n\nThe room was warm and welcoming, with two hearths that burned vigorously. The Vikings pushed her to the floor at Gunnar's feet. Ailia looked up at Gunnar and rose to her feet as calmly as possible.\n\nContinuation 1 Continuation 2 Continuation 3 zero-shot \"I will not fight.\" \"You will fight for us,\" Gunnar said.\n\n\"I'll give you one last chance to tell me where that is,\" Bjorn said.\n\n\"I'm sorry. I didn't mean to. . . \" \"Give me your hand.\" Gunnar told her. mock \"I love this. I love this whole room. I'm going to enjoy it.\n\n\"I am very pleased to meet you, and let me assure you, you will not regret this.\n\n\"Bravely done, Gunnar. Thank you.\" Ailia smiled at him. 5k offline fine-tune Gunnar smiled at Ailia and waited for her to smile back. She was smiling, she knew. \"You've been a true friend to my son, Gunnar. Thank you.\" She smiled.\n\nGunnar was smiling at her and his pretty, blue eyes were filled with contentment. Table 11: Continuations for our positive-sentiment continuation task, with contexts randomly sampled from BookCorpus test set. Random samples from pretrained, mock sentiment, and 5k offline descriptiveness models, using temperature T = 0.7.\n\n\nSamples for descriptiveness continuation task\n\ncontext \"I do not know if it was Viking related, but it could have been.\" \"Really?\" Ailia said. Is it safe to be traveling here then? Ailia looked behind her to make sure they weren't being followed.\n\n\nContinuation 1 Continuation 2 Continuation 3 zero-shot\n\nThere were no signs of anyone. \"It is safe enough,\" Ailios said.\n\n\"Because I have a friend that is in the area and he will be coming with us.\n\nIt was hard to see that far. \"I do not like that word.\n\n5k offline fine-tune Kaya crouched low, her eyes wide in the moonlight. Her body was tense.\n\nShe put her hand on the sword strapped to her back, and then pulled it out.\n\nShe strode out the door and walked down the street, her nose wrinkled in disapproval.\n\ncontext \"They took me to a cellar. An' it were full of girls, it were, all sewin'. At first I thought it weren't too bad. I don't mind work, really I don't. Mama said I was a good 'elper.\n\n\nContinuation 1 Continuation 2 Continuation 3 zero-shot\n\nBut the girls keep on sneakin' in 'em. An' one day, one of em found me.\n\nBut in the end, it weren't anything like she'd promised me. Some of them were worse'n me.\n\nI was just lucky to get this job. But as I stayed there, I started to see things. context\n\nShe forced herself up and shifted closer to him until she was kneeling behind him. Following her instincts, she laid a splayed hand on his bare back. He tensed beneath her touch. Quietly, Tara found her words. \"I will never lie to you again, Max. Continuation 1 Continuation 2 Continuation 3 zero-shot I love you.\"\n\nHe turned to her, his face a mask of pain.\n\nYou have to know that.\" \"I know you will never lie to me again.\n\nWhatever happens, I'll tell you the truth.\" \"I'm sorry. 5k offline fine-tune I promise.\"\n\nHis hands curled around her forearms, his fingers digging deep into her flesh.\n\n\nI swear it.\"\n\nHe exhaled heavily and hugged her close, pressing his lips to her forehead.\n\n\nI love you.\"\n\nMax leaned his head back and closed his eyes. His breathing changed. Table 12: Continuations for our descriptiveness continuation task, with contexts randomly sampled from BookCorpus test set. Random samples from pretrained and 5k offline descriptiveness models, using temperature T = 0.7.\n\n\ncontext post\n\nOkay, this was actually going to go into a comment on youtube, but Emma said she wanted more traffic on reddit.\n\nI am a musician myself, have been for many years now. While Emma's music is not the most complicated technical death metal stuff out there, I love it dearly. Her music actually moves me. No matter what she plays, be it a cover or original stuff, her music *touches* me on a deep personal level. And I have absolutely no clue why. I think it's a combination of her voice, her lyrics and the overall sound design of her songs and covers. Emma Blackery is my favourite artist, and I love her music to death. I wish I could listen to her music all the time. However, I feel like her music destroys me emotionally. I feel like I need to enjoy her music for the human part of it, and not the technical part. She's my favourite artist. Table 13: Random (T = 0.7) summaries for our TL;DR summarization task, with contexts randomly sampled from TL;DR validation set.\n\n\ncontext post\n\nIt really does. I did the first 5 seasons in a couple months, and ultimately I enjoyed them. The first three seasons cover just about all the really good stuff. Russell Edgington was, to me, the best supporting character in the show. He was everything that I valued about the show, which is to say, campy, violent, overly-gory fun. I went back to the show a little over a year later, and while S6 was tolerable with a few fun bits, S7 totally eroded. Tara's plot goes full retard in the last season, people die for the sake of drama and convenience or leave with no real reverence for their history. Suddenly no one can make a good decision, and there was a needless romance bait-and-switch that ruined what I felt the show was moving towards.\n\nWhat's terrible is that once you've done six seasons it's hard to walk away, but each episode of seven stings more than the last. You don't get that weird half-hollow/half-warm feeling you should when you finish a series of books or a long TV show. Outside of Pam and Erik, I didn't give two fucks about any character by the end. reference summary Seasons one through three were a blast. Four was pretty meh, five and six weren't too bad, seven sucked massively.\n\n\nzero-shot\n\nI finished S6 and still feel like I'm in the same place as I was with S5.\n\nFavorite Episodes: S1: \"Tough Love\" S2: \"Crossover\" S3: \"The Party\" 60k fine-tune I did the first 5 seasons in a couple months, and ultimately I enjoyed them. The first three seasons cover just about all the really good stuff. Russell Edgington was, to me, the best supporting character in the show. He was everything that I valued about the show, which is to say, campy, violent, overly-gory fun supervised 10/10 would watch again supervised + 60k fine-tune\n\nThe first two seasons of the show were enjoyable. The show's progress in the last three seasons is frustrating because it's so similar to the first three seasons, and it's so one-sided that there's no reason to keep going. But the show's greatest strength has always been its great writing, and it's giving me that same feeling again. Table 14: Random (T = 0.7) summaries for our TL;DR summarization task, with contexts randomly sampled from TL;DR validation set.\n\ncontext post > you should be happy, lots of votes were counted twice in precincts that went to Clinton in a landslide, if they recount them by hand she will lose more votes, and probably a lot...\n\nYou are assuming you understand what the problem here is and what the end result on the election was. The article does not state where the problem lies.\n\nE.g. The machine shows a tally of 120 votes having been processed on it.\n\nThe actual reported vote tally to the election board was 100 votes. The 20 vote differential on the machine is due to 20 times a ballot needing to be re-scanned due to error. No double-counting of votes occurs but there is also a delta between what the machine says was recorded vs what was reported.\n\nIn this case the final reported vote tallies may have been factually correct but there is no way to validate them against the machine record.\n\nTo understand the problem an analysis and deconstruction of the machines and the election night events must be undertaken. Find out the cause of the problem, if the problem caused votes to be over/under counted, or if the problem was solely with machine data that was not used for tabulation of results. reference summary neither you nor I have any idea what the actual problem is so there is no basis for saying the problem favors either candidate at this time, or if either candidate would even be affected.\n\n\nzero-shot\n\nElection officials did not count ballots in a timely manner, by hand, to meet the threshold of getting votes counted on election night.\n\nVoters were not eligible to vote on election night and were not aware of the problems.\n\nThe election officials did not realize that their system was broken until very late in the night. 60k fine-tune The machine shows a tally of 120 votes having been processed on it. The actual reported vote tally to the election board was 100 votes. The 20 vote differential on the machine is due to 20 times a ballot needing to be re-scanned due to error. No double-counting of votes occurs but there is also a delta between what the machine says supervised\n\nStop assuming the election was completely free of double-counting before you even begin to discuss the actual results. For any result to be close, it must be based on the total vote totals reported to the election board and not just the machine results. The election board did not report the machine results to the election board. supervised + 60k fine-tune\n\nThe reported vote tallies may have been factually correct but there is no way to validate them against the machine record. The machine may have actually been accurate and the machine may have been faulty in other ways. The election may have been considered fair but the machine may not have been. The results are still in dispute. Table 15: Random (T = 0.7) summaries for our TL;DR summarization task, with contexts randomly sampled from TL;DR validation set.\n\n\ncontext article\n\nGoogle launched a new U.S. wireless service today that switches between Wi-Fi and cellular networks Google is already the world's most popular phone software provider, and a pay-TV operator -and now it wants to be your mobile network carrier.\n\nThe company has unveiled a U.S. wireless service that switches between Wi-Fi and cellular networks to curb data use and keep phone bills low.\n\nThe service, called 'Project Fi,' debuted today, about two months after Google revealed its plans to expand its ever-growing empire into providing wireless connections for smartphones.\n\nGoogle is selling the basic phone service for $20 a month and will only charge customers for the amount of cellular data that they use each month, instead of a flat rate.\n\nEach gigabyte of data will cost $10 a month. That means a customer could sign up for a plan offering three gigabytes of data and get $20 back if only one gigabyte was used in a month.\n\n[...Truncated...]\n\nRather than building its own network, Google is leasing space on cellular towers built by Sprint and T-Mobile, which are hoping the deals will boost their profits without costing them too many customers tempted to defect to Project Fi. reference summary Project Fi will be hosted through Sprint Corp and T-Mobile's networks. It costs $20 for basic service and unused data is paid back to customer. The invitation-only service will work only on Nexus 6 phones in the US. Numbers will live in the cloud so users can talk on any connected tablet.\n\n\nzero-shot\n\nProject Fi is a new smartphone service that switches between Wi-Fi and cellular networks to curb data use and keep phone bills low. The service is available for an introductory price of $20 a month and will only charge customers for the amount of cellular data that they use each month. 60k fine-tune Google is already the world's most popular phone software provider, and a pay-TV operator -and now it wants to be your mobile network carrier.\n\nThe company has unveiled a U.S. wireless service that switches between Wi-Fi and cellular networks to curb data use and keep phone bills low. supervised\n\nGoogle launched a new U.S. wireless service today. Called 'Project Fi,' it switches between Wi-Fi and cellular networks. The service, called 'Project Fi,' debuted today. About two months after Google announced its plans to expand its ever-growing empire into providing wireless connections for smartphones. supervised + 60k fine-tune Google launched a new U.S. wireless service today that switches between Wi-Fi and cellular networks. The service, called 'Project Fi,' debuted today, about two months after Google revealed its plans to expand its ever-growing empire into providing wireless connections for smartphones. Table 16: Random (T = 0.5) summaries for our CNN/DM summarization task, with contexts (shown truncated) randomly sampled from CNN/DM test set.\n\n\ncontext article\n\nWith the date of their hugely anticipated bout less than a month away, Floyd Mayweather and Manny Pacquiao's training intensity has hit gruelling levels ahead of the clash in Las Vegas.\n\nMayweather is as renowned for showing off his extravagant lifestyle as he is for going undefeated in his professional career.\n\nBut the latest pictures released from his training camp, appearing to show him hanging a weight from his head, displays the extreme level he hits in order to be as fit as possible for his fights. Pacquiao and Mayweather's clash will be a reported \u00a3160million mega-fight, with Pacquiao seeking to inflict a first ever defeat upon Mayweather.\n\nThe 36-year-old Pacquiao also uploaded a picture to his Twitter page away from training with his daughter. reference summary Manny Pacquiao took time out from training to meet Mark Wahlberg. Floyd Mayweather shared a picture of him holding a weight with his head. The due meet in Las Vegas in a \u00a3160million encounter on May 2.\n\n\nzero-shot\n\nMayweather and Pacquiao are going to clash in Las Vegas on May 2. 60k fine-tune With the date of their hugely anticipated bout less than a month away, Floyd\n\nMayweather and Manny Pacquiao's training intensity has hit gruelling levels ahead of the clash in Las Vegas.\n\nMayweather is as renowned for showing off his extravagant lifestyle as he is for going undefeated in his professional career. supervised\n\nFloyd Mayweather and Manny Pacquiao will fight in Las Vegas on May 2. Pacquiao has a visitor check up on his preparations for the bout. Mayweather released pictures of his training camp to social media. supervised + 60k fine-tune Floyd Mayweather and Manny Pacquiao will fight in Las Vegas on May 2. Mayweather released two pictures from his training camp appearing hold a weight from his head. The 36-year-old boxer is as renowned for showing off his extravagant lifestyle as he is for going undefeated in his professional career. Table 17: Random (T = 0.5) summaries for our CNN/DM summarization task, with contexts (shown truncated) randomly sampled from CNN/DM test set.\n\n\ncontext article\n\nThe Solar Impulse 2, the experimental plane attempting to fly around the world without using a drop of fuel, has been grounded by the weather in China.\n\nWhat was supposed to be an overnight pit stop in the southwestern city of Chongqing has now stretched into a two-and-a-half week stay.\n\nIt's an unexpected layover that is testing the patience of the pilots and more than 60 team members, just a quarter of the way along a journey that will cover some 35,000 kilometers (21,748 miles) over five months.\n\n\"You can believe that your will can influence the weather,\" says pilot Bertrand Piccard, who comes from a long family line of explorers. But \"then you'll be really disappointed because it doesn't work.\n\n\"Or you just know that it's an adventure you do with the forces of nature.\" Piccard, along with Andre Borschberg, a Swiss engineer, are taking turns flying the single-seater aircraft, which took off from Abu Dhabi on March 9 and has successfully flown through Oman, India, and Myanmar.\n\nThere is much at stake.\n\n[...Truncated...] \"Our boss is the sun,\" says Solar Impulse spokeswoman Claudia Durgnat.\n\nDurgnat says there may be a tiny window ahead, with the next possible departure from Chongqing on Tuesday. \"It's not good before, and the days after don't look very clear.\" reference summary Solar plane attempting to be first to circumnavigate world without using fuel is stuck in China. Solar Impulse 2 attempts to prove the power of renewable energy.\n\n\nzero-shot\n\nThe plane has been grounded in China for two weeks because of bad weather. 60k fine-tune The Solar Impulse 2, the experimental plane attempting to fly around the world without using a drop of fuel, has been grounded by the weather in China.\n\nWhat was supposed to be an overnight pit stop in the southwestern city of Chongqing has now stretched into a two-and-a-half week stay. supervised Solar Impulse 2 has been grounded by the weather in China. The plane took off from Abu Dhabi on March 9. The plane is trying to prove the power of renewable energy. supervised + 60k fine-tune Solar Impulse 2 has been grounded in China for two-and-a-half weeks. The plane is attempting to fly around the world without using a drop of fuel. The team, which includes Bertrand Piccard, is taking turns flying the single-seater. The plane took off from Abu Dhabi on March 9 and has successfully flown through Oman. Table 18: Random (T = 0.5) summaries for our CNN/DM summarization task, with contexts (shown truncated) randomly sampled from CNN/DM test set.\n\nFigure 1 :\n1Our training processes for reward model and policy. In the online case, the processes are interleaved. et al. (2017) show that generatively trained models can learn to classify sentiment from very few labeled examples.\n\nFigure 5 :\n5Percent of n-grams and sentences in summaries that do not appear in the source (compare to figure 6 in See et al. (\n\nFigure 6 :Figure 7 :\n67Percent of n-grams and sentences in summaries that appear multiple times in the summary (compare tofigure 4inSee et al. (2017)). Variation in where the models copy from, illustrated by the location of the longest common subsequence of bigrams between context article/post (left) and summary (right) for 256 randomly chosen contexts. Document lengths are shown in gray, with bigrams highlighted (with color depending on positions in contexts). Here, we picture the 60k fine-tuned models, which do the most copying.\n\n\n...\" The girl hiccupped, then pressed her lips together and shook her head. But then...\" His voice trailed off and he shook his head, his eyes going distant, somber. She said I could keep my eyes open and my head clear and make a good target.\n\n\nFine-Tuning Language Models from Human Preferences\n\n\nFine-Tuning Language Models from Human PreferencesTL;DR \nCNN/Daily Mail \nR-1 \nR-2 \nR-L \nR-AVG \nR-1 \nR-2 \nR-L \nR-AVG \n\nSOTA \n22* \n5* \n17* \n14.7* \n41.22 \n18.68 \n38.34 \n32.75 \n\nlead-3 \n17.435 3.243 14.575 11.751 40.379 17.658 36.618 31.552 \nzero-shot \n15.862 2.325 13.518 10.568 28.406 8.321 25.175 20.634 \nsupervised baseline \n17.535 3.124 14.969 11.877 39.525 16.992 36.728 31.082 \nsupervised + 60k fine-tune 18.434 3.542 15.457 12.478 40.093 17.611 37.104 31.603 \n60k fine-tune \n16.800 2.884 14.011 11.232 37.385 15.478 33.330 28.731 \n30k fine-tune \n16.410 2.920 13.653 10.994 35.581 13.662 31.734 26.992 \n15k fine-tune \n15.275 2.240 12.872 10.129 38.466 15.960 34.468 29.631 \n60k offline fine-tune \n16.632 2.699 13.984 11.105 33.860 12.850 30.018 25.576 \n\n\n\nTable 4 :\n4ROUGE evaluations of summarization models. For all models (excluding the lead-3 baselines), we sample with temperature 0.7 for TL;DR and 0.5 for CNN/Daily Mail. We use the CNN/DM test set, but our own validation set for TL;DR. CNN/Daily Mail SOTA is fromGehrmann et al. (2018). * TL;DR SOTA is fromGehrmann et al. (2019), but the numbers are not comparable as we lack test set access and the TL;DR leaderboard uses an unofficial implementation of ROUGE.TL;DR \nCNN/Daily Mail \n\n\n\nTable 7 :\n7How often different models copy the first 3 words of the article as the first 3 words of the summary, on the validation \nsets. We additionally consider the subset of posts/articles with preambles. On TL;DR, we used posts which begin with one \nof 'hi', 'hello', 'hey', 'ok', 'okay', or 'so'. For CNN/Daily Mail, we used articles with a colon within the first 3 words, such \nas \"Winner: Simon Wood took home the TV crown [...]\" and \"Fully charged: The new scheme will let EE customers pick \nup free portable chargers [...]\". \n\nTL;DR CNN/Daily Mail \n\nzero-shot \n6/30 \n6/30 \n60k fine-tuned \n26/30 \n29/30 \nsupervised \n8/30 \n19/30 \nsupervised + 60k fine-tuned \n11/30 \n20/30 \n\n\n\n\nEmma music gud yes very love magic zero-shot I'm a musician myself, have been for many years now. And Emma Blackery is my favourite artist. 60k fine-tune I am a musician myself, have been for many years now. While Emma's music is not the most technical death metal stuff out there, I love it dearly. Her music actually moves me. No matter what she plays, be it a cover or original stuff, her music *touches* me on a deep personal level. supervised I'm jealous of Emma Blackery's music and I have no clue why. supervised + 60k fine-tuneBut this isn't always a good thing, I feel. I can't listen to her music for hours \non end. Only always a few songs, nothing more. Her music destroys me emotionally. \nOkay, that's a bit harsh, but she can get me, a (nearly) adult on the verge of tears. \nI've gone through some shit in my life, who hasn't, and I feel like her music can \nactually get some of it out. I am incredibly thankful that she makes her music the way \nshe makes it. I can honestly say that Emma Blackery is my favourite artist. Keep in \nmind that I listen to a lot of music, from crossover thrash metal over 80's pop, berlin \npolygenre to classic/romantic music. \n\nI am also very jealous of her ability. I play quite a few instruments myself \n(guitar, piano, bass, drums, synth, saxophone, percussion) and aim to study audio engi-\nneering. I can definitely say that music is my literal life and mean it. I've released my \nfirst full length album at age 20. Yet I am very jealous that she is as good as she is and \nI really wanna know what her secret is. I love and hate her music, does that make sense? \n\nHer music definitely makes me think, and that's what art is all about, right? \nAnyway, I'm gonna continue working on my cover of The Promise and try to figure \nout what it is she's doing. Thank you for reading this far. \nreference \nsummary \n\n\n\n\nManny Pacquiao (right) poses alongside actor Mark Wahlberg, who visited the Filipino at training Floyd Mayweather released two pictures from his training camp appearing hold a weight from his head Mayweather (left) takes on Pacquiao in a \u00a3160million mega-fight in Las Vegas on May 2 Mayweather (left) appeared to take his training intensity to a new height before his meeting with Pacquiao As for Pacquiao, he had a famous visitor check up on his preparations for May 2. [...Truncated...]\n. In the online data collection case, continue to collect additional samples, and periodically retrain the reward model r. This is described in section 2.3.1  In early experiments we found that it was hard for humans to provide consistent fine-grained quantitative distinctions when asked for an absolute number, and experiments on synthetic tasks confirmed that comparisons were almost as useful.\nThe model is a Transformer with 6 layers, 8 attention heads, and embedding size 512.\nThis is a crude approximation for \"end of sentence.\" We chose it because it is easy to integrate into the RL loop, and even a crude approximation is sufficient for the intended purpose of making the human evaluation task somewhat easier.\nThat said, different training runs have considerable variation and it is expensive to run multiple seeds with humans, so it is possible that this gap is largely noise.\nAcknowledgmentsWe thank Akshat Bubna, Shariq Hashme, and many others at Scale for their work on data collection, Shan Carter for help with visualizations, Scott Gray for help with low precision training, Shahbaz Syed for information about the TL;DR dataset, and Christine Payne, Miles Brundage, Jared Kaplan, Jan Leike, Ethan Perez, and Jelena Luketina for helpful comments on the paper.\nDeep batch active learning by diverse, uncertain gradient lower bounds. T Jordan, Chicheng Ash, Akshay Zhang, John Krishnamurthy, Alekh Langford, Agarwal, arXiv:1906.03671arXiv preprintJordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch ac- tive learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671, 2019.\n\nLearning to understand goal specifications by modelling reward. Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Arian Hosseini, Pushmeet Kohli, Edward Grefenstette, arXiv:1806.01946arXiv preprintDzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Arian Hosseini, Pushmeet Kohli, and Edward Grefen- stette. Learning to understand goal specifications by mod- elling reward. arXiv preprint arXiv:1806.01946, 2018.\n\nBetter rewards yield better summaries: Learning to summarise without references. Forian B\u00f6hm, Yang Gao, Christian Meyer, Ori Shapira, Ido Dagan, Iryna Gurevych, Conference on Empirical Methods in Natural Language Processing. Forian B\u00f6hm, Yang Gao, Christian Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. Better rewards yield better summaries: Learning to summarise without references. In Conference on Empirical Methods in Natural Language Processing, 2019.\n\nTowards coherent and cohesive long-form text generation. Sang Woon, Pengchuan Cho, Yizhe Zhang, Xiujun Zhang, Michel Li, Chris Galley, Mengdi Brockett, Jianfeng Wang, Gao, Proceedings of the First Workshop on Narrative Understanding. the First Workshop on Narrative UnderstandingWoon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiujun Li, Michel Galley, Chris Brockett, Mengdi Wang, and Jianfeng Gao. Towards coherent and cohesive long-form text generation. In Proceedings of the First Workshop on Narrative Understanding, pages 1-11, 2019.\n\nDeep reinforcement learning from human preferences. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei, Advances in Neural Information Processing Systems. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, pages 4302-4310, 2017.\n\nSupervising strong learners by amplifying weak experts. Paul Christiano, Buck Shlegeris, Dario Amodei, arXiv:1810.08575arXiv preprintPaul Christiano, Buck Shlegeris, and Dario Amodei. Super- vising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575, 2018.\n\nSemi-supervised sequence learning. M Andrew, Quoc V Dai, Le, Advances in neural information processing systems. Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079-3087, 2015.\n\nFine-Tuning Language Models from Human Preferences Szymon. Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselinesPrafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Fine-Tuning Language Models from Human Preferences Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Ope- nai baselines. https://github.com/openai/ baselines, 2017.\n\nPreference-based interactive multi-document summarisation. Yang Gao, M Christian, Iryna Meyer, Gurevych, arXiv:1906.02923arXiv preprintYang Gao, Christian M Meyer, and Iryna Gurevych. Preference-based interactive multi-document summarisa- tion. arXiv preprint arXiv:1906.02923, 2019a.\n\nReward learning for efficient reinforcement learning in extractive document summarisation. Yang Gao, M Christian, Mohsen Meyer, Iryna Mesgar, Gurevych, arXiv:1907.12894arXiv preprintYang Gao, Christian M Meyer, Mohsen Mesgar, and Iryna Gurevych. Reward learning for efficient reinforcement learning in extractive document summarisation. arXiv preprint arXiv:1907.12894, 2019b.\n\nSebastian Gehrmann, Yuntian Deng, Alexander M Rush, arXiv:1808.10792Bottom-up abstractive summarization. arXiv preprintSebastian Gehrmann, Yuntian Deng, and Alexander M Rush. Bottom-up abstractive summarization. arXiv preprint arXiv:1808.10792, 2018.\n\nGenerating abstractive summaries with finetuned language models. Sebastian Gehrmann, Zachary Ziegler, Alexander Rush, TL;DR Challenge System Descriptions. Sebastian Gehrmann, Zachary Ziegler, and Alexander Rush. Generating abstractive summaries with finetuned lan- guage models. In TL;DR Challenge System Descriptions, 2019.\n\nDaniel Gissin, Shai Shalev-Shwartz, arXiv:1907.06347Discriminative active learning. arXiv preprintDaniel Gissin and Shai Shalev-Shwartz. Discriminative active learning. arXiv preprint arXiv:1907.06347, 2019.\n\nDiscriminative batch mode active learning. Yuhong Guo, Dale Schuurmans, Advances in neural information processing systems. Yuhong Guo and Dale Schuurmans. Discriminative batch mode active learning. In Advances in neural information processing systems, pages 593-600, 2008.\n\nLearning from dialogue after deployment: Feed yourself. Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, Jason Weston, arXiv:1901.05415chatbot! arXiv preprintBraden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from dialogue af- ter deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019.\n\nTeaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Advances in neural information processing systems. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in neural information processing systems, pages 1693-1701, 2015.\n\nUniversal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, arXiv:1801.06146arXiv preprintJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.\n\nActive learning for speech recognition: the power of gradients. Jiaji Huang, Rewon Child, Vinay Rao, Hairong Liu, Sanjeev Satheesh, Adam Coates, arXiv:1612.03226arXiv preprintJiaji Huang, Rewon Child, Vinay Rao, Hairong Liu, Sanjeev Satheesh, and Adam Coates. Active learning for speech recognition: the power of gradients. arXiv preprint arXiv:1612.03226, 2016.\n\nReward learning from human preferences and demonstrations in Atari. Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, Dario Amodei, Advances in Neural Information Processing Systems. Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning from human preferences and demonstrations in Atari. In Ad- vances in Neural Information Processing Systems, 2018. URL https://arxiv.org/abs/1811.06521.\n\n. Geoffrey Irving, Paul Christiano, Dario Amodei, arXiv:1805.00899AI safety via debate. arXiv preprintGeoffrey Irving, Paul Christiano, and Dario Amodei. AI safety via debate. arXiv preprint arXiv:1805.00899, 2018. URL https://arxiv.org/abs/1805.00899.\n\nSequence tutor: Conservative fine-tuning of sequence generation models with kl-control. Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Richard E Turner, Douglas Eck, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Richard E Turner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In Pro- ceedings of the 34th International Conference on Ma- chine Learning-Volume 70, pages 1645-1654. JMLR. org, 2017.\n\nWay off-policy batch deep reinforcement learning of implicit human preferences in dialog. Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, Rosalind Picard, arXiv:1907.00456arXiv preprintNatasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.\n\nSample efficient text summarization using a single pre-trained transformer. Urvashi Khandelwal, Kevin Clark, Dan Jurafsky, Lukasz Kaiser, arXiv:1905.08836arXiv preprintUrvashi Khandelwal, Kevin Clark, Dan Jurafsky, and Lukasz Kaiser. Sample efficient text summarization using a single pre-trained transformer. arXiv preprint arXiv:1905.08836, 2019.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nReliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning. Julia Kreutzer, Joshua Uyheng, Stefan Riezler, arXiv:1805.10627arXiv preprintJulia Kreutzer, Joshua Uyheng, and Stefan Riezler. Reli- ability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning. arXiv preprint arXiv:1805.10627, 2018.\n\nNeural text summarization: A critical evaluation. Wojciech Kry\u015bci\u0144ski, Nitish Shirish Keskar, Bryan Mc-Cann, Caiming Xiong, Richard Socher, arXiv:1908.08960arXiv preprintWojciech Kry\u015bci\u0144ski, Nitish Shirish Keskar, Bryan Mc- Cann, Caiming Xiong, and Richard Socher. Neural text summarization: A critical evaluation. arXiv preprint arXiv:1908.08960, 2019.\n\nScalable agent alignment via reward modeling: a research direction. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg, arXiv:1811.07871arXiv preprintJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent align- ment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\n\nDialogue learning with human-in-the-loop. Jiwei Li, H Alexander, Sumit Miller, Marc&apos;aurelio Chopra, Jason Ranzato, Weston, arXiv:1611.09823arXiv preprintJiwei Li, Alexander H Miller, Sumit Chopra, Marc'Aurelio Ranzato, and Jason Weston. Dialogue learning with human-in-the-loop. arXiv preprint arXiv:1611.09823, 2016.\n\nA survey of reinforcement learning informed by natural language. Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, Tim Rockt\u00e4schel, Proceedings of the Twenty-Eight International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Eight International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization82019Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rockt\u00e4schel. A survey of reinforce- ment learning informed by natural language. In Proceed- ings of the Twenty-Eight International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization, 8 2019.\n\nQinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. Julian Mcauley, Christopher Targett, Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 38th International ACM SIGIR Conference on Research and Development in Information RetrievalACMJulian McAuley, Christopher Targett, Qinfeng Shi, and An- ton Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th Inter- national ACM SIGIR Conference on Research and De- velopment in Information Retrieval, pages 43-52. ACM, 2015.\n", "annotations": {"author": "[{\"end\":71,\"start\":54},{\"end\":87,\"start\":72},{\"end\":99,\"start\":88},{\"end\":112,\"start\":100},{\"end\":126,\"start\":113},{\"end\":161,\"start\":127},{\"end\":182,\"start\":162},{\"end\":215,\"start\":183}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":63},{\"end\":86,\"start\":78},{\"end\":98,\"start\":96},{\"end\":111,\"start\":106},{\"end\":125,\"start\":118},{\"end\":144,\"start\":140},{\"end\":181,\"start\":173},{\"end\":196,\"start\":190}]", "author_first_name": "[{\"end\":60,\"start\":54},{\"end\":62,\"start\":61},{\"end\":77,\"start\":72},{\"end\":95,\"start\":88},{\"end\":103,\"start\":100},{\"end\":105,\"start\":104},{\"end\":117,\"start\":113},{\"end\":132,\"start\":127},{\"end\":139,\"start\":133},{\"end\":172,\"start\":162},{\"end\":189,\"start\":183}]", "author_affiliation": null, "title": "[{\"end\":51,\"start\":1},{\"end\":266,\"start\":216}]", "venue": null, "abstract": "[{\"end\":1364,\"start\":268}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1911,\"start\":1886},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1930,\"start\":1911},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1952,\"start\":1930},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2610,\"start\":2592},{\"end\":2630,\"start\":2610},{\"end\":2651,\"start\":2630},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2675,\"start\":2651},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2929,\"start\":2905},{\"end\":2996,\"start\":2975},{\"end\":3319,\"start\":3297},{\"end\":3335,\"start\":3319},{\"end\":3383,\"start\":3361},{\"end\":3403,\"start\":3383},{\"end\":3419,\"start\":3403},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3437,\"start\":3419},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3487,\"start\":3466},{\"end\":3553,\"start\":3529},{\"end\":3575,\"start\":3555},{\"end\":3672,\"start\":3656},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3694,\"start\":3677},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3846,\"start\":3828},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4018,\"start\":3996},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4098,\"start\":4078},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4168,\"start\":4148},{\"end\":4226,\"start\":4210},{\"end\":4372,\"start\":4350},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4574,\"start\":4553},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4991,\"start\":4971},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5329,\"start\":5307},{\"end\":5349,\"start\":5329},{\"end\":6154,\"start\":6137},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6178,\"start\":6156},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7428,\"start\":7410},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9030,\"start\":9006},{\"end\":9135,\"start\":9134},{\"end\":11016,\"start\":10988},{\"end\":11158,\"start\":11137},{\"end\":11311,\"start\":11288},{\"end\":11412,\"start\":11390},{\"end\":11549,\"start\":11532},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12416,\"start\":12394},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16682,\"start\":16658},{\"end\":19317,\"start\":19299},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20041,\"start\":20020},{\"end\":26701,\"start\":26700},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26944,\"start\":26923},{\"end\":33131,\"start\":33114},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33204,\"start\":33182},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33993,\"start\":33971},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":36085,\"start\":36061},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36220,\"start\":36196},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":39755,\"start\":39739},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":40350,\"start\":40324},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40395,\"start\":40375},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":40412,\"start\":40395},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":75523,\"start\":75501},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":75567,\"start\":75545}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":73509,\"start\":73278},{\"attributes\":{\"id\":\"fig_1\"},\"end\":73638,\"start\":73510},{\"attributes\":{\"id\":\"fig_2\"},\"end\":74176,\"start\":73639},{\"attributes\":{\"id\":\"fig_3\"},\"end\":74421,\"start\":74177},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":74474,\"start\":74422},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":75234,\"start\":74475},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":75724,\"start\":75235},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":76407,\"start\":75725},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":78264,\"start\":76408},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":78755,\"start\":78265}]", "paragraph": "[{\"end\":2363,\"start\":1380},{\"end\":3116,\"start\":2365},{\"end\":4669,\"start\":3118},{\"end\":5515,\"start\":4671},{\"end\":6673,\"start\":5517},{\"end\":7962,\"start\":6675},{\"end\":8152,\"start\":7974},{\"end\":8523,\"start\":8154},{\"end\":8733,\"start\":8525},{\"end\":8994,\"start\":8769},{\"end\":9358,\"start\":8996},{\"end\":9481,\"start\":9360},{\"end\":9935,\"start\":9541},{\"end\":10162,\"start\":9937},{\"end\":10664,\"start\":10208},{\"end\":10698,\"start\":10666},{\"end\":10813,\"start\":10700},{\"end\":10942,\"start\":10815},{\"end\":11050,\"start\":10944},{\"end\":11413,\"start\":11074},{\"end\":11936,\"start\":11415},{\"end\":12304,\"start\":11960},{\"end\":12749,\"start\":12306},{\"end\":13040,\"start\":12751},{\"end\":13143,\"start\":13125},{\"end\":13619,\"start\":13145},{\"end\":14105,\"start\":13646},{\"end\":14391,\"start\":14107},{\"end\":14807,\"start\":14441},{\"end\":15461,\"start\":14809},{\"end\":16683,\"start\":15480},{\"end\":16857,\"start\":16685},{\"end\":17551,\"start\":16859},{\"end\":18716,\"start\":17567},{\"end\":19127,\"start\":18718},{\"end\":19661,\"start\":19160},{\"end\":20228,\"start\":19685},{\"end\":20456,\"start\":20230},{\"end\":21018,\"start\":20458},{\"end\":21512,\"start\":21020},{\"end\":21923,\"start\":21526},{\"end\":22021,\"start\":21925},{\"end\":22629,\"start\":22054},{\"end\":22741,\"start\":22668},{\"end\":22816,\"start\":22743},{\"end\":22898,\"start\":22818},{\"end\":23242,\"start\":22900},{\"end\":23632,\"start\":23244},{\"end\":23756,\"start\":23634},{\"end\":24164,\"start\":23758},{\"end\":24282,\"start\":24223},{\"end\":24351,\"start\":24284},{\"end\":24494,\"start\":24353},{\"end\":24551,\"start\":24496},{\"end\":24633,\"start\":24553},{\"end\":24858,\"start\":24635},{\"end\":25061,\"start\":24870},{\"end\":25184,\"start\":25120},{\"end\":25261,\"start\":25186},{\"end\":25317,\"start\":25263},{\"end\":25410,\"start\":25319},{\"end\":25487,\"start\":25412},{\"end\":26186,\"start\":25489},{\"end\":26820,\"start\":26188},{\"end\":27773,\"start\":26838},{\"end\":27835,\"start\":27775},{\"end\":29603,\"start\":28241},{\"end\":30501,\"start\":29605},{\"end\":30755,\"start\":30515},{\"end\":31919,\"start\":30757},{\"end\":32692,\"start\":31921},{\"end\":32958,\"start\":32694},{\"end\":33375,\"start\":32983},{\"end\":34079,\"start\":33377},{\"end\":34745,\"start\":34081},{\"end\":35825,\"start\":34747},{\"end\":36343,\"start\":35827},{\"end\":36759,\"start\":36345},{\"end\":37260,\"start\":36761},{\"end\":37372,\"start\":37275},{\"end\":37674,\"start\":37407},{\"end\":38015,\"start\":37676},{\"end\":38382,\"start\":38017},{\"end\":38919,\"start\":38384},{\"end\":39900,\"start\":38921},{\"end\":40511,\"start\":39902},{\"end\":41153,\"start\":40585},{\"end\":41545,\"start\":41155},{\"end\":42513,\"start\":41584},{\"end\":43302,\"start\":42515},{\"end\":44201,\"start\":43341},{\"end\":44721,\"start\":44216},{\"end\":45452,\"start\":44723},{\"end\":46327,\"start\":45454},{\"end\":47122,\"start\":46329},{\"end\":47249,\"start\":47124},{\"end\":47360,\"start\":47251},{\"end\":47420,\"start\":47362},{\"end\":47587,\"start\":47445},{\"end\":47672,\"start\":47589},{\"end\":47792,\"start\":47674},{\"end\":47858,\"start\":47794},{\"end\":48016,\"start\":47888},{\"end\":48112,\"start\":48018},{\"end\":48164,\"start\":48141},{\"end\":48220,\"start\":48166},{\"end\":48383,\"start\":48251},{\"end\":48426,\"start\":48385},{\"end\":48483,\"start\":48428},{\"end\":48535,\"start\":48512},{\"end\":48658,\"start\":48537},{\"end\":49101,\"start\":48688},{\"end\":50187,\"start\":49103},{\"end\":50833,\"start\":50189},{\"end\":51014,\"start\":50835},{\"end\":51541,\"start\":51016},{\"end\":52131,\"start\":51543},{\"end\":52204,\"start\":52146},{\"end\":52519,\"start\":52206},{\"end\":52871,\"start\":52521},{\"end\":53448,\"start\":52873},{\"end\":53670,\"start\":53450},{\"end\":54369,\"start\":53672},{\"end\":54598,\"start\":54371},{\"end\":54768,\"start\":54657},{\"end\":54870,\"start\":54770},{\"end\":54960,\"start\":54872},{\"end\":55058,\"start\":54969},{\"end\":55121,\"start\":55060},{\"end\":55203,\"start\":55123},{\"end\":55317,\"start\":55205},{\"end\":55401,\"start\":55319},{\"end\":55481,\"start\":55403},{\"end\":55490,\"start\":55483},{\"end\":55845,\"start\":55492},{\"end\":55930,\"start\":55847},{\"end\":55984,\"start\":55932},{\"end\":56041,\"start\":55986},{\"end\":56123,\"start\":56043},{\"end\":56211,\"start\":56125},{\"end\":56220,\"start\":56213},{\"end\":56419,\"start\":56222},{\"end\":56533,\"start\":56421},{\"end\":56604,\"start\":56535},{\"end\":56745,\"start\":56606},{\"end\":56827,\"start\":56747},{\"end\":57059,\"start\":56829},{\"end\":57383,\"start\":57061},{\"end\":57632,\"start\":57433},{\"end\":57755,\"start\":57691},{\"end\":57832,\"start\":57757},{\"end\":57888,\"start\":57834},{\"end\":57981,\"start\":57890},{\"end\":58058,\"start\":57983},{\"end\":58145,\"start\":58060},{\"end\":58334,\"start\":58147},{\"end\":58464,\"start\":58393},{\"end\":58555,\"start\":58466},{\"end\":58646,\"start\":58557},{\"end\":58962,\"start\":58648},{\"end\":59006,\"start\":58964},{\"end\":59071,\"start\":59008},{\"end\":59161,\"start\":59073},{\"end\":59241,\"start\":59163},{\"end\":59333,\"start\":59258},{\"end\":59639,\"start\":59350},{\"end\":59767,\"start\":59656},{\"end\":60626,\"start\":59769},{\"end\":61386,\"start\":60643},{\"end\":61850,\"start\":61388},{\"end\":61937,\"start\":61864},{\"end\":62397,\"start\":61939},{\"end\":62862,\"start\":62399},{\"end\":63059,\"start\":62864},{\"end\":63213,\"start\":63061},{\"end\":63287,\"start\":63215},{\"end\":63589,\"start\":63289},{\"end\":63732,\"start\":63591},{\"end\":64243,\"start\":63734},{\"end\":64392,\"start\":64257},{\"end\":64480,\"start\":64394},{\"end\":64938,\"start\":64482},{\"end\":65297,\"start\":64940},{\"end\":65758,\"start\":65299},{\"end\":66020,\"start\":65778},{\"end\":66163,\"start\":66022},{\"end\":66349,\"start\":66165},{\"end\":66521,\"start\":66351},{\"end\":66706,\"start\":66523},{\"end\":66725,\"start\":66708},{\"end\":67270,\"start\":66727},{\"end\":67727,\"start\":67284},{\"end\":67881,\"start\":67729},{\"end\":68645,\"start\":67883},{\"end\":68850,\"start\":68665},{\"end\":68977,\"start\":68852},{\"end\":69319,\"start\":68979},{\"end\":69647,\"start\":69321},{\"end\":69817,\"start\":69661},{\"end\":69927,\"start\":69819},{\"end\":70065,\"start\":69929},{\"end\":70741,\"start\":70067},{\"end\":70912,\"start\":70761},{\"end\":71048,\"start\":70914},{\"end\":71264,\"start\":71050},{\"end\":71467,\"start\":71266},{\"end\":71754,\"start\":71469},{\"end\":71779,\"start\":71756},{\"end\":71869,\"start\":71781},{\"end\":72223,\"start\":71871},{\"end\":72477,\"start\":72237},{\"end\":73277,\"start\":72479}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8768,\"start\":8734},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9540,\"start\":9482},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10207,\"start\":10163},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13124,\"start\":13041},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14440,\"start\":14392},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22053,\"start\":22022},{\"attributes\":{\"id\":\"formula_6\"},\"end\":28240,\"start\":27836}]", "table_ref": "[{\"end\":10084,\"start\":10076},{\"end\":21703,\"start\":21696},{\"end\":24729,\"start\":24722},{\"end\":25582,\"start\":25575},{\"end\":28386,\"start\":28379},{\"end\":29697,\"start\":29688},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29791,\"start\":29782},{\"end\":31420,\"start\":31413},{\"end\":31992,\"start\":31985},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34446,\"start\":34439},{\"end\":35472,\"start\":35465},{\"end\":36903,\"start\":36894},{\"end\":50708,\"start\":50701},{\"end\":54054,\"start\":54046},{\"end\":57151,\"start\":57143},{\"end\":59427,\"start\":59419},{\"end\":60506,\"start\":60498},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":62742,\"start\":62734},{\"end\":65638,\"start\":65630},{\"end\":68511,\"start\":68503},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":70607,\"start\":70599},{\"end\":73143,\"start\":73135}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1378,\"start\":1366},{\"attributes\":{\"n\":\"2.\"},\"end\":7972,\"start\":7965},{\"attributes\":{\"n\":\"2.1.\"},\"end\":11072,\"start\":11053},{\"attributes\":{\"n\":\"2.2.\"},\"end\":11958,\"start\":11939},{\"attributes\":{\"n\":\"2.3.\"},\"end\":13644,\"start\":13622},{\"attributes\":{\"n\":\"2.4.\"},\"end\":15478,\"start\":15464},{\"attributes\":{\"n\":\"3.\"},\"end\":17565,\"start\":17554},{\"attributes\":{\"n\":\"3.1.\"},\"end\":19158,\"start\":19130},{\"attributes\":{\"n\":\"3.1.1.\"},\"end\":19683,\"start\":19664},{\"end\":21524,\"start\":21515},{\"attributes\":{\"n\":\"3.1.2.\"},\"end\":22666,\"start\":22632},{\"end\":24221,\"start\":24167},{\"end\":24868,\"start\":24861},{\"end\":25118,\"start\":25064},{\"attributes\":{\"n\":\"3.2.\"},\"end\":26836,\"start\":26823},{\"end\":30513,\"start\":30504},{\"attributes\":{\"n\":\"3.2.1.\"},\"end\":32981,\"start\":32961},{\"attributes\":{\"n\":\"4.\"},\"end\":37273,\"start\":37263},{\"attributes\":{\"n\":\"4.1.\"},\"end\":37405,\"start\":37375},{\"attributes\":{\"n\":\"4.2.\"},\"end\":40583,\"start\":40514},{\"attributes\":{\"n\":\"4.3.\"},\"end\":41582,\"start\":41548},{\"attributes\":{\"n\":\"4.4.\"},\"end\":43339,\"start\":43305},{\"attributes\":{\"n\":\"5.\"},\"end\":44214,\"start\":44204},{\"end\":47443,\"start\":47423},{\"end\":47886,\"start\":47861},{\"end\":48139,\"start\":48115},{\"end\":48249,\"start\":48223},{\"end\":48510,\"start\":48486},{\"end\":48686,\"start\":48661},{\"end\":52144,\"start\":52134},{\"end\":54655,\"start\":54601},{\"end\":54967,\"start\":54963},{\"end\":57431,\"start\":57386},{\"end\":57689,\"start\":57635},{\"end\":58391,\"start\":58337},{\"end\":59256,\"start\":59244},{\"end\":59348,\"start\":59336},{\"end\":59654,\"start\":59642},{\"end\":60641,\"start\":60629},{\"end\":61862,\"start\":61853},{\"end\":64255,\"start\":64246},{\"end\":65776,\"start\":65761},{\"end\":67282,\"start\":67273},{\"end\":68663,\"start\":68648},{\"end\":69659,\"start\":69650},{\"end\":70759,\"start\":70744},{\"end\":72235,\"start\":72226},{\"end\":73289,\"start\":73279},{\"end\":73521,\"start\":73511},{\"end\":73660,\"start\":73640},{\"end\":75245,\"start\":75236},{\"end\":75735,\"start\":75726}]", "table": "[{\"end\":75234,\"start\":74527},{\"end\":75724,\"start\":75700},{\"end\":76407,\"start\":75737},{\"end\":78264,\"start\":76945}]", "figure_caption": "[{\"end\":73509,\"start\":73291},{\"end\":73638,\"start\":73523},{\"end\":74176,\"start\":73663},{\"end\":74421,\"start\":74179},{\"end\":74474,\"start\":74424},{\"end\":74527,\"start\":74477},{\"end\":75700,\"start\":75247},{\"end\":76945,\"start\":76410},{\"end\":78755,\"start\":78267}]", "figure_ref": "[{\"end\":16417,\"start\":16409},{\"end\":18383,\"start\":18375},{\"end\":20788,\"start\":20780},{\"end\":21196,\"start\":21188},{\"end\":22215,\"start\":22207},{\"end\":26031,\"start\":26025},{\"end\":34172,\"start\":34164}]", "bib_author_first_name": "[{\"end\":80106,\"start\":80105},{\"end\":80123,\"start\":80115},{\"end\":80135,\"start\":80129},{\"end\":80147,\"start\":80143},{\"end\":80168,\"start\":80163},{\"end\":80489,\"start\":80482},{\"end\":80505,\"start\":80500},{\"end\":80515,\"start\":80512},{\"end\":80529,\"start\":80523},{\"end\":80543,\"start\":80538},{\"end\":80562,\"start\":80554},{\"end\":80576,\"start\":80570},{\"end\":80929,\"start\":80923},{\"end\":80940,\"start\":80936},{\"end\":80955,\"start\":80946},{\"end\":80966,\"start\":80963},{\"end\":80979,\"start\":80976},{\"end\":80992,\"start\":80987},{\"end\":81367,\"start\":81363},{\"end\":81383,\"start\":81374},{\"end\":81394,\"start\":81389},{\"end\":81408,\"start\":81402},{\"end\":81422,\"start\":81416},{\"end\":81432,\"start\":81427},{\"end\":81447,\"start\":81441},{\"end\":81466,\"start\":81458},{\"end\":81900,\"start\":81896},{\"end\":81916,\"start\":81913},{\"end\":81927,\"start\":81924},{\"end\":81941,\"start\":81935},{\"end\":81955,\"start\":81950},{\"end\":81967,\"start\":81962},{\"end\":82301,\"start\":82297},{\"end\":82318,\"start\":82314},{\"end\":82335,\"start\":82330},{\"end\":82559,\"start\":82558},{\"end\":82574,\"start\":82568},{\"end\":82843,\"start\":82835},{\"end\":82865,\"start\":82854},{\"end\":82877,\"start\":82873},{\"end\":82890,\"start\":82886},{\"end\":82907,\"start\":82899},{\"end\":82922,\"start\":82918},{\"end\":82936,\"start\":82932},{\"end\":83335,\"start\":83331},{\"end\":83342,\"start\":83341},{\"end\":83359,\"start\":83354},{\"end\":83653,\"start\":83649},{\"end\":83660,\"start\":83659},{\"end\":83678,\"start\":83672},{\"end\":83691,\"start\":83686},{\"end\":83945,\"start\":83936},{\"end\":83963,\"start\":83956},{\"end\":83981,\"start\":83970},{\"end\":84262,\"start\":84253},{\"end\":84280,\"start\":84273},{\"end\":84299,\"start\":84290},{\"end\":84520,\"start\":84514},{\"end\":84533,\"start\":84529},{\"end\":84772,\"start\":84766},{\"end\":84782,\"start\":84778},{\"end\":85059,\"start\":85053},{\"end\":85076,\"start\":85069},{\"end\":85100,\"start\":85085},{\"end\":85114,\"start\":85109},{\"end\":85389,\"start\":85385},{\"end\":85411,\"start\":85406},{\"end\":85427,\"start\":85421},{\"end\":85447,\"start\":85442},{\"end\":85462,\"start\":85458},{\"end\":85475,\"start\":85468},{\"end\":85490,\"start\":85486},{\"end\":85858,\"start\":85852},{\"end\":85876,\"start\":85867},{\"end\":86120,\"start\":86115},{\"end\":86133,\"start\":86128},{\"end\":86146,\"start\":86141},{\"end\":86159,\"start\":86152},{\"end\":86172,\"start\":86165},{\"end\":86187,\"start\":86183},{\"end\":86488,\"start\":86483},{\"end\":86499,\"start\":86496},{\"end\":86513,\"start\":86507},{\"end\":86530,\"start\":86522},{\"end\":86544,\"start\":86539},{\"end\":86556,\"start\":86551},{\"end\":86881,\"start\":86873},{\"end\":86894,\"start\":86890},{\"end\":86912,\"start\":86907},{\"end\":87220,\"start\":87213},{\"end\":87237,\"start\":87229},{\"end\":87249,\"start\":87242},{\"end\":87264,\"start\":87260},{\"end\":87271,\"start\":87265},{\"end\":87297,\"start\":87290},{\"end\":87299,\"start\":87298},{\"end\":87315,\"start\":87308},{\"end\":87865,\"start\":87858},{\"end\":87878,\"start\":87874},{\"end\":87898,\"start\":87894},{\"end\":87905,\"start\":87899},{\"end\":87917,\"start\":87912},{\"end\":87933,\"start\":87928},{\"end\":87949,\"start\":87945},{\"end\":87965,\"start\":87957},{\"end\":87978,\"start\":87970},{\"end\":88363,\"start\":88356},{\"end\":88381,\"start\":88376},{\"end\":88392,\"start\":88389},{\"end\":88409,\"start\":88403},{\"end\":88675,\"start\":88674},{\"end\":88691,\"start\":88686},{\"end\":88956,\"start\":88951},{\"end\":88973,\"start\":88967},{\"end\":88988,\"start\":88982},{\"end\":89282,\"start\":89274},{\"end\":89301,\"start\":89295},{\"end\":89323,\"start\":89318},{\"end\":89340,\"start\":89333},{\"end\":89355,\"start\":89348},{\"end\":89650,\"start\":89647},{\"end\":89663,\"start\":89658},{\"end\":89676,\"start\":89673},{\"end\":89692,\"start\":89686},{\"end\":89707,\"start\":89701},{\"end\":89720,\"start\":89715},{\"end\":89998,\"start\":89993},{\"end\":90004,\"start\":90003},{\"end\":90021,\"start\":90016},{\"end\":90047,\"start\":90030},{\"end\":90061,\"start\":90056},{\"end\":90346,\"start\":90340},{\"end\":90363,\"start\":90357},{\"end\":90381,\"start\":90374},{\"end\":90397,\"start\":90392},{\"end\":90413,\"start\":90408},{\"end\":90429,\"start\":90423},{\"end\":90450,\"start\":90444},{\"end\":90464,\"start\":90461},{\"end\":91311,\"start\":91305},{\"end\":91332,\"start\":91321}]", "bib_author_last_name": "[{\"end\":80113,\"start\":80107},{\"end\":80127,\"start\":80124},{\"end\":80141,\"start\":80136},{\"end\":80161,\"start\":80148},{\"end\":80177,\"start\":80169},{\"end\":80186,\"start\":80179},{\"end\":80498,\"start\":80490},{\"end\":80510,\"start\":80506},{\"end\":80521,\"start\":80516},{\"end\":80536,\"start\":80530},{\"end\":80552,\"start\":80544},{\"end\":80568,\"start\":80563},{\"end\":80589,\"start\":80577},{\"end\":80934,\"start\":80930},{\"end\":80944,\"start\":80941},{\"end\":80961,\"start\":80956},{\"end\":80974,\"start\":80967},{\"end\":80985,\"start\":80980},{\"end\":81001,\"start\":80993},{\"end\":81372,\"start\":81368},{\"end\":81387,\"start\":81384},{\"end\":81400,\"start\":81395},{\"end\":81414,\"start\":81409},{\"end\":81425,\"start\":81423},{\"end\":81439,\"start\":81433},{\"end\":81456,\"start\":81448},{\"end\":81471,\"start\":81467},{\"end\":81476,\"start\":81473},{\"end\":81911,\"start\":81901},{\"end\":81922,\"start\":81917},{\"end\":81933,\"start\":81928},{\"end\":81948,\"start\":81942},{\"end\":81960,\"start\":81956},{\"end\":81974,\"start\":81968},{\"end\":82312,\"start\":82302},{\"end\":82328,\"start\":82319},{\"end\":82342,\"start\":82336},{\"end\":82566,\"start\":82560},{\"end\":82578,\"start\":82575},{\"end\":82582,\"start\":82580},{\"end\":82852,\"start\":82844},{\"end\":82871,\"start\":82866},{\"end\":82884,\"start\":82878},{\"end\":82897,\"start\":82891},{\"end\":82916,\"start\":82908},{\"end\":82930,\"start\":82923},{\"end\":82945,\"start\":82937},{\"end\":83339,\"start\":83336},{\"end\":83352,\"start\":83343},{\"end\":83365,\"start\":83360},{\"end\":83375,\"start\":83367},{\"end\":83657,\"start\":83654},{\"end\":83670,\"start\":83661},{\"end\":83684,\"start\":83679},{\"end\":83698,\"start\":83692},{\"end\":83708,\"start\":83700},{\"end\":83954,\"start\":83946},{\"end\":83968,\"start\":83964},{\"end\":83986,\"start\":83982},{\"end\":84271,\"start\":84263},{\"end\":84288,\"start\":84281},{\"end\":84304,\"start\":84300},{\"end\":84527,\"start\":84521},{\"end\":84548,\"start\":84534},{\"end\":84776,\"start\":84773},{\"end\":84793,\"start\":84783},{\"end\":85067,\"start\":85060},{\"end\":85083,\"start\":85077},{\"end\":85107,\"start\":85101},{\"end\":85121,\"start\":85115},{\"end\":85404,\"start\":85390},{\"end\":85419,\"start\":85412},{\"end\":85440,\"start\":85428},{\"end\":85456,\"start\":85448},{\"end\":85466,\"start\":85463},{\"end\":85484,\"start\":85476},{\"end\":85498,\"start\":85491},{\"end\":85865,\"start\":85859},{\"end\":85882,\"start\":85877},{\"end\":86126,\"start\":86121},{\"end\":86139,\"start\":86134},{\"end\":86150,\"start\":86147},{\"end\":86163,\"start\":86160},{\"end\":86181,\"start\":86173},{\"end\":86194,\"start\":86188},{\"end\":86494,\"start\":86489},{\"end\":86505,\"start\":86500},{\"end\":86520,\"start\":86514},{\"end\":86537,\"start\":86531},{\"end\":86549,\"start\":86545},{\"end\":86563,\"start\":86557},{\"end\":86888,\"start\":86882},{\"end\":86905,\"start\":86895},{\"end\":86919,\"start\":86913},{\"end\":87227,\"start\":87221},{\"end\":87240,\"start\":87238},{\"end\":87258,\"start\":87250},{\"end\":87288,\"start\":87272},{\"end\":87306,\"start\":87300},{\"end\":87319,\"start\":87316},{\"end\":87872,\"start\":87866},{\"end\":87892,\"start\":87879},{\"end\":87910,\"start\":87906},{\"end\":87926,\"start\":87918},{\"end\":87943,\"start\":87934},{\"end\":87955,\"start\":87950},{\"end\":87968,\"start\":87966},{\"end\":87985,\"start\":87979},{\"end\":88374,\"start\":88364},{\"end\":88387,\"start\":88382},{\"end\":88401,\"start\":88393},{\"end\":88416,\"start\":88410},{\"end\":88684,\"start\":88676},{\"end\":88698,\"start\":88692},{\"end\":88702,\"start\":88700},{\"end\":88965,\"start\":88957},{\"end\":88980,\"start\":88974},{\"end\":88996,\"start\":88989},{\"end\":89293,\"start\":89283},{\"end\":89316,\"start\":89302},{\"end\":89331,\"start\":89324},{\"end\":89346,\"start\":89341},{\"end\":89362,\"start\":89356},{\"end\":89656,\"start\":89651},{\"end\":89671,\"start\":89664},{\"end\":89684,\"start\":89677},{\"end\":89699,\"start\":89693},{\"end\":89713,\"start\":89708},{\"end\":89725,\"start\":89721},{\"end\":90001,\"start\":89999},{\"end\":90014,\"start\":90005},{\"end\":90028,\"start\":90022},{\"end\":90054,\"start\":90048},{\"end\":90069,\"start\":90062},{\"end\":90077,\"start\":90071},{\"end\":90355,\"start\":90347},{\"end\":90372,\"start\":90364},{\"end\":90390,\"start\":90382},{\"end\":90406,\"start\":90398},{\"end\":90421,\"start\":90414},{\"end\":90442,\"start\":90430},{\"end\":90459,\"start\":90451},{\"end\":90476,\"start\":90465},{\"end\":91319,\"start\":91312},{\"end\":91340,\"start\":91333}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1906.03671\",\"id\":\"b0\"},\"end\":80416,\"start\":80033},{\"attributes\":{\"doi\":\"arXiv:1806.01946\",\"id\":\"b1\"},\"end\":80840,\"start\":80418},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":202540345},\"end\":81304,\"start\":80842},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":168169995},\"end\":81842,\"start\":81306},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4787508},\"end\":82239,\"start\":81844},{\"attributes\":{\"doi\":\"arXiv:1810.08575\",\"id\":\"b5\"},\"end\":82521,\"start\":82241},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7138078},\"end\":82774,\"start\":82523},{\"attributes\":{\"id\":\"b7\"},\"end\":83270,\"start\":82776},{\"attributes\":{\"doi\":\"arXiv:1906.02923\",\"id\":\"b8\"},\"end\":83556,\"start\":83272},{\"attributes\":{\"doi\":\"arXiv:1907.12894\",\"id\":\"b9\"},\"end\":83934,\"start\":83558},{\"attributes\":{\"doi\":\"arXiv:1808.10792\",\"id\":\"b10\"},\"end\":84186,\"start\":83936},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":209387678},\"end\":84512,\"start\":84188},{\"attributes\":{\"doi\":\"arXiv:1907.06347\",\"id\":\"b12\"},\"end\":84721,\"start\":84514},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13782},\"end\":84995,\"start\":84723},{\"attributes\":{\"doi\":\"arXiv:1901.05415\",\"id\":\"b14\"},\"end\":85341,\"start\":84997},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6203757},\"end\":85788,\"start\":85343},{\"attributes\":{\"doi\":\"arXiv:1801.06146\",\"id\":\"b16\"},\"end\":86049,\"start\":85790},{\"attributes\":{\"doi\":\"arXiv:1612.03226\",\"id\":\"b17\"},\"end\":86413,\"start\":86051},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53424488},\"end\":86869,\"start\":86415},{\"attributes\":{\"doi\":\"arXiv:1805.00899\",\"id\":\"b19\"},\"end\":87123,\"start\":86871},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":15636415},\"end\":87766,\"start\":87125},{\"attributes\":{\"doi\":\"arXiv:1907.00456\",\"id\":\"b21\"},\"end\":88278,\"start\":87768},{\"attributes\":{\"doi\":\"arXiv:1905.08836\",\"id\":\"b22\"},\"end\":88628,\"start\":88280},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b23\"},\"end\":88846,\"start\":88630},{\"attributes\":{\"doi\":\"arXiv:1805.10627\",\"id\":\"b24\"},\"end\":89222,\"start\":88848},{\"attributes\":{\"doi\":\"arXiv:1908.08960\",\"id\":\"b25\"},\"end\":89577,\"start\":89224},{\"attributes\":{\"doi\":\"arXiv:1811.07871\",\"id\":\"b26\"},\"end\":89949,\"start\":89579},{\"attributes\":{\"doi\":\"arXiv:1611.09823\",\"id\":\"b27\"},\"end\":90273,\"start\":89951},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":182952502},\"end\":91209,\"start\":90275},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1012652},\"end\":91830,\"start\":91211}]", "bib_title": "[{\"end\":80921,\"start\":80842},{\"end\":81361,\"start\":81306},{\"end\":81894,\"start\":81844},{\"end\":82556,\"start\":82523},{\"end\":84251,\"start\":84188},{\"end\":84764,\"start\":84723},{\"end\":85383,\"start\":85343},{\"end\":86481,\"start\":86415},{\"end\":87211,\"start\":87125},{\"end\":90338,\"start\":90275},{\"end\":91303,\"start\":91211}]", "bib_author": "[{\"end\":80115,\"start\":80105},{\"end\":80129,\"start\":80115},{\"end\":80143,\"start\":80129},{\"end\":80163,\"start\":80143},{\"end\":80179,\"start\":80163},{\"end\":80188,\"start\":80179},{\"end\":80500,\"start\":80482},{\"end\":80512,\"start\":80500},{\"end\":80523,\"start\":80512},{\"end\":80538,\"start\":80523},{\"end\":80554,\"start\":80538},{\"end\":80570,\"start\":80554},{\"end\":80591,\"start\":80570},{\"end\":80936,\"start\":80923},{\"end\":80946,\"start\":80936},{\"end\":80963,\"start\":80946},{\"end\":80976,\"start\":80963},{\"end\":80987,\"start\":80976},{\"end\":81003,\"start\":80987},{\"end\":81374,\"start\":81363},{\"end\":81389,\"start\":81374},{\"end\":81402,\"start\":81389},{\"end\":81416,\"start\":81402},{\"end\":81427,\"start\":81416},{\"end\":81441,\"start\":81427},{\"end\":81458,\"start\":81441},{\"end\":81473,\"start\":81458},{\"end\":81478,\"start\":81473},{\"end\":81913,\"start\":81896},{\"end\":81924,\"start\":81913},{\"end\":81935,\"start\":81924},{\"end\":81950,\"start\":81935},{\"end\":81962,\"start\":81950},{\"end\":81976,\"start\":81962},{\"end\":82314,\"start\":82297},{\"end\":82330,\"start\":82314},{\"end\":82344,\"start\":82330},{\"end\":82568,\"start\":82558},{\"end\":82580,\"start\":82568},{\"end\":82584,\"start\":82580},{\"end\":82854,\"start\":82835},{\"end\":82873,\"start\":82854},{\"end\":82886,\"start\":82873},{\"end\":82899,\"start\":82886},{\"end\":82918,\"start\":82899},{\"end\":82932,\"start\":82918},{\"end\":82947,\"start\":82932},{\"end\":83341,\"start\":83331},{\"end\":83354,\"start\":83341},{\"end\":83367,\"start\":83354},{\"end\":83377,\"start\":83367},{\"end\":83659,\"start\":83649},{\"end\":83672,\"start\":83659},{\"end\":83686,\"start\":83672},{\"end\":83700,\"start\":83686},{\"end\":83710,\"start\":83700},{\"end\":83956,\"start\":83936},{\"end\":83970,\"start\":83956},{\"end\":83988,\"start\":83970},{\"end\":84273,\"start\":84253},{\"end\":84290,\"start\":84273},{\"end\":84306,\"start\":84290},{\"end\":84529,\"start\":84514},{\"end\":84550,\"start\":84529},{\"end\":84778,\"start\":84766},{\"end\":84795,\"start\":84778},{\"end\":85069,\"start\":85053},{\"end\":85085,\"start\":85069},{\"end\":85109,\"start\":85085},{\"end\":85123,\"start\":85109},{\"end\":85406,\"start\":85385},{\"end\":85421,\"start\":85406},{\"end\":85442,\"start\":85421},{\"end\":85458,\"start\":85442},{\"end\":85468,\"start\":85458},{\"end\":85486,\"start\":85468},{\"end\":85500,\"start\":85486},{\"end\":85867,\"start\":85852},{\"end\":85884,\"start\":85867},{\"end\":86128,\"start\":86115},{\"end\":86141,\"start\":86128},{\"end\":86152,\"start\":86141},{\"end\":86165,\"start\":86152},{\"end\":86183,\"start\":86165},{\"end\":86196,\"start\":86183},{\"end\":86496,\"start\":86483},{\"end\":86507,\"start\":86496},{\"end\":86522,\"start\":86507},{\"end\":86539,\"start\":86522},{\"end\":86551,\"start\":86539},{\"end\":86565,\"start\":86551},{\"end\":86890,\"start\":86873},{\"end\":86907,\"start\":86890},{\"end\":86921,\"start\":86907},{\"end\":87229,\"start\":87213},{\"end\":87242,\"start\":87229},{\"end\":87260,\"start\":87242},{\"end\":87290,\"start\":87260},{\"end\":87308,\"start\":87290},{\"end\":87321,\"start\":87308},{\"end\":87874,\"start\":87858},{\"end\":87894,\"start\":87874},{\"end\":87912,\"start\":87894},{\"end\":87928,\"start\":87912},{\"end\":87945,\"start\":87928},{\"end\":87957,\"start\":87945},{\"end\":87970,\"start\":87957},{\"end\":87987,\"start\":87970},{\"end\":88376,\"start\":88356},{\"end\":88389,\"start\":88376},{\"end\":88403,\"start\":88389},{\"end\":88418,\"start\":88403},{\"end\":88686,\"start\":88674},{\"end\":88700,\"start\":88686},{\"end\":88704,\"start\":88700},{\"end\":88967,\"start\":88951},{\"end\":88982,\"start\":88967},{\"end\":88998,\"start\":88982},{\"end\":89295,\"start\":89274},{\"end\":89318,\"start\":89295},{\"end\":89333,\"start\":89318},{\"end\":89348,\"start\":89333},{\"end\":89364,\"start\":89348},{\"end\":89658,\"start\":89647},{\"end\":89673,\"start\":89658},{\"end\":89686,\"start\":89673},{\"end\":89701,\"start\":89686},{\"end\":89715,\"start\":89701},{\"end\":89727,\"start\":89715},{\"end\":90003,\"start\":89993},{\"end\":90016,\"start\":90003},{\"end\":90030,\"start\":90016},{\"end\":90056,\"start\":90030},{\"end\":90071,\"start\":90056},{\"end\":90079,\"start\":90071},{\"end\":90357,\"start\":90340},{\"end\":90374,\"start\":90357},{\"end\":90392,\"start\":90374},{\"end\":90408,\"start\":90392},{\"end\":90423,\"start\":90408},{\"end\":90444,\"start\":90423},{\"end\":90461,\"start\":90444},{\"end\":90478,\"start\":90461},{\"end\":91321,\"start\":91305},{\"end\":91342,\"start\":91321}]", "bib_venue": "[{\"end\":81585,\"start\":81540},{\"end\":87444,\"start\":87391},{\"end\":90809,\"start\":90652},{\"end\":91551,\"start\":91455},{\"end\":80103,\"start\":80033},{\"end\":80480,\"start\":80418},{\"end\":81065,\"start\":81003},{\"end\":81538,\"start\":81478},{\"end\":82025,\"start\":81976},{\"end\":82295,\"start\":82241},{\"end\":82633,\"start\":82584},{\"end\":82833,\"start\":82776},{\"end\":83329,\"start\":83272},{\"end\":83647,\"start\":83558},{\"end\":84039,\"start\":84004},{\"end\":84341,\"start\":84306},{\"end\":84596,\"start\":84566},{\"end\":84844,\"start\":84795},{\"end\":85051,\"start\":84997},{\"end\":85549,\"start\":85500},{\"end\":85850,\"start\":85790},{\"end\":86113,\"start\":86051},{\"end\":86614,\"start\":86565},{\"end\":87389,\"start\":87321},{\"end\":87856,\"start\":87768},{\"end\":88354,\"start\":88280},{\"end\":88672,\"start\":88630},{\"end\":88949,\"start\":88848},{\"end\":89272,\"start\":89224},{\"end\":89645,\"start\":89579},{\"end\":89991,\"start\":89951},{\"end\":90650,\"start\":90478},{\"end\":91453,\"start\":91342}]"}}}, "year": 2023, "month": 12, "day": 17}