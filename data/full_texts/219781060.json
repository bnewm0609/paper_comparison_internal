{"id": 219781060, "updated": "2023-07-19 16:21:58.79", "metadata": {"title": "Generative Pretraining from Pixels", "authors": "[{\"first\":\"Mark\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Alec\",\"last\":\"Radford\",\"middle\":[]},{\"first\":\"Rewon\",\"last\":\"Child\",\"middle\":[]},{\"first\":\"Jeffrey\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Heewoo\",\"last\":\"Jun\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Luan\",\"middle\":[]},{\"first\":\"Ilya\",\"last\":\"Sutskever\",\"middle\":[]}]", "venue": "ICML", "journal": "1691-1703", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Trans-former to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we \ufb01nd that a GPT-2 scale model learns strong image representations as measured by linear probing, \ufb01ne-tuning, and low-data classi\ufb01cation. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full \ufb01ne-tuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3034445277", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/ChenRC0JLS20", "doi": null}}, "content": {"source": {"pdf_hash": "afb630909573de23814ee35977f3b7ccbfb6aef5", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "29db54baba8a58a6f754d94146774fb7fa6a5504", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/afb630909573de23814ee35977f3b7ccbfb6aef5.txt", "contents": "\nGenerative Pretraining from Pixels\n\n\nMark Chen \nAlec Radford \nRewon Child \nJeff Wu \nHeewoo Jun \nPrafulla Dhariwal \nDavid Luan \nIlya Sutskever \nGenerative Pretraining from Pixels\n\nInspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.\n\nIntroduction\n\nUnsupervised pre-training played a central role in the resurgence of deep learning. Starting in the mid 2000's, approaches such as the Deep Belief Network (Hinton et al., 2006) and Denoising Autoencoder (Vincent et al., 2008) were commonly used in neural networks for computer vision (Lee et al., 2009) and speech recognition (Mohamed et al., 2009). It was believed that a model which learned the data distribution P (X) would also learn beneficial features for the subsequent supervised modeling of P (Y |X) (Lasserre et al., 2006;Erhan et al., 2010). However, advancements such as piecewise linear activation functions (Nair & Hinton, 2010), improved initializations (Glorot & Bengio, 2010), and normalization strategies (Ioffe & Szegedy, 2015;Ba et al., 2016) removed the need for pre-training in order to achieve strong results. Other research cast doubt on the benefits of deep unsupervised representations and re-ported strong results using a single layer of learned features (Coates et al., 2011), or even random features May et al., 2017). The approach fell out of favor as the state of the art increasingly relied on directly encoding prior structure into the model and utilizing abundant supervised data to directly learn representations (Krizhevsky et al., 2012;Graves & Jaitly, 2014). Retrospective study of unsupervised pre-training demonstrated that it could even hurt performance in modern settings (Paine et al., 2014).\n\nInstead, unsupervised pre-training flourished in a different domain. After initial strong results for word vectors (Mikolov et al., 2013), it has pushed the state of the art forward in Natural Language Processing on most tasks (Dai & Le, 2015;Peters et al., 2018;Howard & Ruder, 2018;Radford et al., 2018;Devlin et al., 2018). Interestingly, the training objective of a dominant approach like BERT, the prediction of corrupted inputs, closely resembles that of the Denoising Autoencoder, which was originally developed for images.\n\nAs a higher dimensional, noisier, and more redundant modality than text, images are believed to be difficult for generative modeling. Here, self-supervised approaches designed to encourage the modeling of more global structure (Doersch et al., 2015) have shown significant promise. A combination of new training objectives (Oord et al., 2018), more recent architectures , and increased model capacity (Kolesnikov et al., 2019) has allowed these methods to achieve state of the art performance in low data settings (H\u00e9naff et al., 2019) and sometimes even outperform supervised representations in transfer learning settings (He et al., 2019;Misra & van der Maaten, 2019;Chen et al., 2020).\n\nGiven that it has been a decade since the original wave of generative pre-training methods for images and considering their substantial impact in NLP, this class of methods is due for a modern re-examination and comparison with the recent progress of self-supervised methods. We re-evaluate generative pre-training on images and demonstrate that when using a flexible architecture (Vaswani et al., 2017), a tractable and efficient likelihood based training objective (Larochelle & Murray, 2011;Oord et al., 2016), and significant compute resources (2048 TPU cores), generative pre-training is competitive with other self-supervised approaches and learns Figure 1. An overview of our approach. First, we pre-process raw images by resizing to a low resolution and reshaping into a 1D sequence. We then chose one of two pre-training objectives, auto-regressive next pixel prediction or masked pixel prediction. Finally, we evaluate the representations learned by these objectives with linear probes or fine-tuning.\n\nrepresentations that significantly improve the state of the art in low-resolution unsupervised representation learning settings. This is especially promising as our architecture uses a dense connectivity pattern which does not encode the 2D spatial structure of images yet is able to match and even outperform approaches which do. We report a set of experiments characterizing the performance of our approach on many datasets and in several different evaluation settings (low data, linear evaluation, full fine-tuning). We also conduct several experiments designed to better understand the achieved performance of these models. We investigate how representations are computed inside our model via the performance of linear probes as a function of model depth as well as studying how scaling the resolution and parameter count of the approach affects performance.\n\n\nApproach\n\nOur approach consists of a pre-training stage followed by a fine-tuning stage. In pre-training, we explore both the auto-regressive and BERT objectives. We also apply the sequence Transformer architecture to predict pixels instead of language tokens.\n\nOne way to measure representation quality is to fine-tune for image classification. Fine-tuning adds a small classification head to the model, used to optimize a classification objective and adapts all weights. Pre-training can be viewed as a favorable initialization or as a regularizer when used in combination with early stopping (Erhan et al., 2010).\n\nAnother approach for measuring representation quality uses the pre-trained model as a feature extractor. In particular, given labeled examples (X, Y ), the model is applied to X to produce features f X . Then, a linear classifier is trained on (f X , Y ). Linear probing captures the intuition that good features should linearly separate the classes of transfer tasks. Furthermore, linear probes help disentangle feature quality from model architecture: in fine-tuning, one model may outperform another because its architecture is more suited for the downstream task rather than because of better pretraining.\n\nWe begin this section by defining the auto-regressive and BERT objectives in the context of images. Next, we outline implementation details for our transformer decoder. Finally, we describe how the transformer is used for fine-tuning and how features are extracted for linear probes.\n\n\nPre-training\n\nGiven an unlabeled dataset X consisting of high dimensional data x = (x 1 , ..., x n ), we can pick a permutation \u03c0 of the set [1, n] and model the density p(x) auto-regressively as follows:\np(x) = n i=1\np(x \u03c0i |x \u03c01 , ..., x \u03c0i\u22121 , \u03b8)\n\nWhen working with images, we pick the identity permutation \u03c0 i = i for 1 \u2264 i \u2264 n, also known as raster order. We train our model by minimizing the negative log-likelihood of the data:\nL AR = E x\u223cX [\u2212 log p(x)]\nWe also consider the BERT objective, which samples a sub-sequence M \u2282 [1, n] such that each index i independently has probability 0.15 of appearing in M . We call M the BERT mask, and we train our model by minimizing the negative log-likelihood of the \"masked\" elements x M conditioned on the \"unmasked\" ones x [1,n]\\M :\nL BERT = E x\u223cX E M i\u2208M \u2212 log p x i |x [1,n]\\M\nIn pre-training, we pick one of L AR or L BERT and minimize the loss over our pre-training dataset.\n\n\nArchitecture\n\nThe transformer decoder takes an input sequence x 1 , ..., x n of discrete tokens and produces a d-dimensional embedding for each position. The decoder is realized as a stack of L blocks, the l-th of which produces an intermediate embedding h l 1 , ..., h l n also of dimension d. We use the GPT-2  formulation of the transformer decoder block, which acts on an input tensor h l as follows:\n\nn l = layer norm(h l ) a l = h l + multihead attention(n l ) h l+1 = a l + mlp(layer norm(a l ))\n\nIn particular, layer norms precede both the attention and mlp operations, and all operations lie strictly on residual paths. We find that such a formulation allows us to scale the transformer with ease.\n\nThe only mixing across sequence elements occurs in the attention operation, and to ensure proper conditioning when training the AR objective, we apply the standard upper triangular mask to the n\u00d7n matrix of attention logits. When using the BERT objective, no attention logit masking is required: after applying content embeddings to the input sequence, we zero out the positions in M .\n\nAdditionally, since we learn independent position embeddings for each sequence element, our BERT model has no positional inductive biases (i.e. it is permutation invariant). Put another way, any spatial relationships between positions must be learned by the model at train time. This is not entirely true for the AR model, as choosing the raster order also fixes a prespecified ordering of the conditionals. Nevertheless, permutation invariance is a property in strong contrast to convolutional neural networks, which incorporate the inductive bias that features should arise from spatially proximate elements.\n\nFollowing the final transformer layer, we apply a layer norm n L = layer norm(h L ), and learn a projection from n L to logits parameterizing the conditional distributions at each sequence element. When training BERT, we simply ignore the logits at unmasked positions.\n\n\nFine-tuning\n\nWhen fine-tuning, we average pool n L across the sequence dimension to extract a d-dimensional vector of features per example:\nf L = n L i i\nWe learn a projection from f L to class logits, which we use to minimize a cross entropy loss L CLF .\n\nWhile fine-tuning on L CLF yields reasonable downstream performance, we find empirically that the joint objective L GEN + L CLF L GEN \u2208 {L AR , L BERT } works even better. Similar findings were reported by Radford et al. (2018).\n\n\nLinear Probing\n\nExtracting fixed features for linear probing follows a similar procedure to fine-tuning, except that average pooling is not always at the final layer:\nf l = n l i i where 0 \u2264 l \u2264 L.\nWe will show in the experiments section that the best features often lie in the middle of the network. As in fine-tuning, we project these intermediate features to produce class logits. Because we view the features as fixed when linear probing, this projection contains the only trainable weights, so we can only optimize L CLF .\n\n\nMethodology\n\nAlthough supervised pre-training is the dominant paradigm for image classification, curating large labeled image datasets is both expensive and time consuming. Instead of further scaling up labeling efforts, we can instead aspire to learn general purpose representations from the much larger set of available unlabeled images and fine-tune them for classification. We investigate this setting using Ima-geNet as a proxy for a large unlabeled corpus, and small classic labeled datasets (CIFAR-10, CIFAR-100, STL-10) as proxies for downstream tasks. For our largest model, we use an additional 100 million unlabeled web images, filtered to be similar to ImageNet.\n\nEven in cases where labels are available, unsupervised or self-supervised pre-training can still provide benefits in data efficiency or on fine-tuning speed. We investigate this setting by pre-training without labels and then fine-tuning or linear probing with labels.\n\n\nDataset and Data Augmentation\n\nWe use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set. For CIFAR-10, CIFAR-100 and STL-10, we split off 10% of the provided training set instead. We ignore the provided unlabeled examples in STL-10, which constitute a subset of ImageNet.\n\nNo data augmentation is used when pre-training on web images, and lightweight data augmentation is used when pre-training or fine-tuning on ImageNet. Specifically, when employing data augmentation, we randomly resize an image such that the shorter sidelength is in the range [256, 384] and then take a random 224 \u00d7 224 crop. When evaluating on ImageNet, we resize the image such that the shorter sidelength is 224, and use the single 224 \u00d7 224 center crop.\n\nWhen full-network fine-tuning on CIFAR-10 and CIFAR-100, we use the augmentation popularized by Wide Residual Networks: 4 pixels are reflection padded on each side, and a 32 \u00d7 32 crop is randomly sampled from the padded image or its horizontal flip (Zagoruyko & Komodakis, 2016).\n\nOnce optimal hyperparameters are found, we fold our ex-perimental validation set back into the training set, retrain the model, and report numbers on the respective test set.\n\n\nContext Reduction\n\nBecause the memory requirements of the transformer decoder scale quadratically with context length when using dense attention, we must employ further techniques to reduce context length. If we naively trained a transformer on a sequence of length 224 2 \u00d7 3, our attention logits would be tens of thousands of times larger than those used in language models and even a single layer would not fit on a GPU. To deal with this, we first resize our image to a lower resolution, which we call the input resolution (IR). Our models have IRs of either 32 2 \u00d7 3, 48 2 \u00d7 3, or 64 2 \u00d7 3.\n\nAn IR of 32 2 \u00d7 3 is still quite computationally intensive. While working at even lower resolutions is tempting, prior work has demonstrated human performance on image classification begins to drop rapidly below this size (Torralba et al., 2008). Instead, motivated by early color display palettes, we create our own 9-bit color palette by clustering (R, G, B) pixel values using k-means with k = 512. Using this palette yields an input sequence length 3 times shorter than the standard (R, G, B) palette, while still encoding color faithfully. A similar approach was applied to spatial patches by Ranzato et al. (2014). We call the resulting context length (32 2 or 48 2 or 64 2 ) the model resolution (MR). Note that this reduction breaks permutation invariance of the color channels, but keeps the model spatially invariant.\n\n\nModel\n\nOur largest model, iGPT-XL, contains L = 60 layers and uses an embedding size of d = 3072 for a total of 6.8B parameters. Our next largest model, iGPT-L, is essentially identical to GPT-2 with L = 48 layers, but contains a slightly smaller embedding size of d = 1536 (vs 1600) for a total of 1.4M parameters. We use the same model code as GPT-2, except that we initialize weights in the layerdependent fashion as in Sparse Transformer  and zero-initialize all projections producing logits.\n\nWe also train iGPT-M, a 455M parameter model with L = 36 and d = 1024 and iGPT-S, a 76M parameter model with L = 24 and d = 512 to study the effect of model capacity on representation quality in a generative model.\n\n\nTraining\n\nWhen pre-training iGPT-XL, we use a batch size of 64 and train for 2M iterations, and for all other models we use a batch size of 128 and train for 1M iterations. We use Adam with \u03b2 1 = 0.9 and \u03b2 2 = 0.95 and sequentially try the learning rates 0.01, 0.003, 0.001, 0.0003, ..., stopping once the final validation loss starts increasing. The learning rate is warmed up for one epoch, and then decays to 0 following a cosine schedule. No dropout is used.\n\nWhen fine-tuning, we use the same batch size and Adam hyperparameters. Here, we do not employ a cosine schedule, and early stop once we reach the maximum validation accuracy. Again, no dropout is used.\n\nWhen running a linear probe on ImageNet, we follow recent literature and use SGD with momentum 0.9 and a high learning rate (we try the values 30, 10, 3, ... in the manner described above) (He et al., 2019). We train for 1000000 iterations with a cosine learning rate schedule. Finally, when running a linear probe on CIFAR-10, CIFAR-100, or STL-10, we use the L-BFGS algorithm for consistency with prior results (Pedregosa et al., 2011).\n\n\nExperiments and Results\n\nWe begin with experiments and results from the autoregressive formulation of iGPT. Comparisons with the BERT formulation appear in Section 4.6.\n\n\nWhat Representation Works Best in a Generative\n\nModel Without Latent Variables? Figure 2. Representation quality depends on the layer from which we extract features. In contrast with supervised models, the best representations for these generative models lie in the middle of the network. We plot this unimodal dependence on depth by showing linear probes for iGPT-L on CIFAR-10, CIFAR-100, and STL-10.\n\nIn supervised pre-training, representation quality tends to increase monotonically with depth, such that the best representations lie at the penultimate layer (Zeiler & Fergus, 2014). Indeed, since a linear layer produces class logits from pre-logits, a good classifier necessarily achieves high accuracy on a linear probe of its pre-logits. If a downstream task also involves classification, it is empirically validated that penultimate features perform well.\n\nWith generative pre-training, it is not obvious whether a task like pixel prediction is relevant to image classification. This suggests that the penultimate layer of a model trained for pixel prediction might not produce the most useful representations for classification. Latent variable models such as VAEs can avoid this issue by explicitly learning a representation of the input data, but deep autoregressive generative models have the same width and connectivity pattern at every layer. Our first experiment studies how representation quality varies over one set of candidate representations: different layers of a generative model. We observe a very different behavior from supervised learning: representations first improve as a function of depth, and then, starting around the middle layer, begin to deteriorate until the penultimate layer ( Figure 2).\n\nThis behavior potentially suggests that these generative models operate in two phases. In the first phase, each position gathers information from its surrounding context in order to build a more global image representation. In the second phase, this contextualized input is used to solve the conditional next pixel prediction task. This could resemble the behavior of encoder-decoder architectures common across deep learning, but learned within a monolithic architecture via a pre-training objective.\n\nConsequently, when evaluating a generative model with a linear probe, it is important to search for the best layer. Taking the final layer on CIFAR-10 decreases performance by 2.4%, the difference between a baseline and a state-ofthe-art result. In all settings, we find that the dependence of representation quality on depth is strongly unimodal. Figure 3. Plot of representation quality as a function of validation generative loss. Each line tracks a model throughout generative pre-training: the dotted markers denote checkpoints at steps 65K, 131K, 262K, 524K, and 1000K. The positive slope suggests a link between improved generative performance and improved representation quality. Larger models produce better representations than smaller ones both at the end of training and at the same value of validation loss. iGPT-XL is not shown since it was trained on a different dataset.\n\n\nBetter Generative Models Learn Better Representations\n\nUsing the linear probe as a tool for measuring representation quality, we investigate whether better generative models (as measured by log-prob on held-out data) also learn better representations.\n\nIn Figure 3, we see that as validation loss on the autoregressive objective decreases throughout training, linear probe accuracy increases as well. This trend holds across \n\n\nLinear Probes on CIFAR and STL-10\n\nIn addition to CIFAR-10, we also evaluate linear probes on CIFAR-100 and STL-10 ( Figure 2) to check whether the learned representations are useful across multiple datasets. For this evaluation setting, we achieve state-of-the-art across the entire spectrum of pre-training approaches (Table 1). For example, on CIFAR-10, our model achieves 96.3%, outperforming both SimCLR (pre-trained on ImageNet without labels) and a ResNet-152 (pre-trained on ImageNet with labels). In fact, on all three datasets a linear classifier fit to the representations of iGPT-L outperforms the end-to-end supervised training of a WideResNet baseline.\n\nNote that our model is trained at the same input resolution (IR) as CIFAR, whereas models trained at the standard Im-ageNet IR may experience distribution shock upon linear evaluation. As a counterpoint, though STL-10 has an IR of 96 2 \u00d7 3, we still outperform AMDIM-L when we downsample to 32 2 \u00d7 3 before linear probing. We also note that fine-tuning should allow models trained at high IR to adjust to low resolution input.\n\n\nLinear Probes on ImageNet\n\nRecently, there has been a resurgence of interest in unsupervised and self-supervised learning on ImageNet, evaluated using linear probes on ImageNet. This is a particularly difficult setting for us, since we cannot efficiently train at the standard ImageNet input resolution (IR). Indeed, when training iGPT-L with a model resolution (MR) of 32 2 , we achieve only 60.3% best-layer linear probe accuracy. As with CIFAR-10, scale is critical to our approach: iGPT- Table 2. Comparing linear probe accuracies between our models and state-of-the-art self-supervised models. A blank input resolution (IR) corresponds to a model working at standard ImageNet resolution. We report the best performing configuration for each contrastive method, finding that our models achieve comparable performance. The first obvious optimization is to increase MR while staying within accelerator memory limits. With a MR of 48 2 , iGPT-L achieves a best-layer accuracy of 65.2% using 1536 features and with a MR of 64 2 , iGPT-XL achieves a bestlayer accuracy of 68.7% using 3072 features.\n\n\nMethod\n\nSince contrastive methods report their best results on 8192 features, we would ideally evaluate iGPT with an embedding dimension 8192 for comparison. Training such a model is prohibitively expensive, so we instead concatenate features from multiple layers as an approximation. However, our features tend to be correlated across layers, so we need more of them to be competitive. If we concatenate features from 5 layers centered at the best single layer of iGPT-XL, we achieve an accuracy of 72.0% using 15360 features, which is competitive with recent contrastive learning approaches ( Table 2). Note that we require more parameters and compute to achieve this accuracy, but we work at low resolution and without utilizing knowledge of the 2D input structure.\n\n\nFull Fine-tuning\n\nTo achieve even higher accuracy on downstream tasks, we adapt the entire model for classification through fine-tuning. Building off of the previous analysis, we tried attaching the classification head to the layer with the best representations. Though this setup trains faster than one with the head attached at the end, the latter is able to leverage greater model depth and eventually outperforms.\n\nOn CIFAR-10, iGPT-L achieves 99.0% accuracy and on CIFAR-100, it achieves 88.5% accuracy after fine-tuning. We outperform AutoAugment, the best supervised model Table 3. Comparing fine-tuning performance between our models and state-of-the-art models utilizing supervised ImageNet transfer. We also include AutoAugment, the best performing model trained end-to-end on CIFAR. On ImageNet, we achieve 66.3% accuracy after fine-tuning at MR 32 2 , a bump of 6% over linear probing. When finetuning at MR 48 2 , we achieve 72.6% accuracy, with a similar 7% bump over linear probing. However, our models still slightly underperform Isometric Neural Nets (Sandler et al., 2019), which achieves 70.2% at an IR of 28 2 \u00d7 3.\n\nFinally, as a baseline for ImageNet fine-tuning, we train the classification objective from a random initialization. At MR 48 2 , a model with tuned learning rate and dropout achieves 53.2% after 18 epochs, 19.4% worse than the pretrained model. Comparatively, the pre-trained model is much quicker to fine-tune, achieving the same 53.2% loss in roughly a single epoch.\n\nWhen fine-tuning, it is important to search over learning rates again, as the optimal learning rate on the joint training objective is often an order of magnitude smaller than that for pre-training. We also tried regularizing with dropout, though we did not observe any clear benefits. It is easy to overfit the classification objective on small datasets, so we employ early stopping based on validation accuracy.\n\n\nBERT\n\nGiven the success of BERT in language, we train iGPT-L at an input resolution of 32 2 \u00d7 3 and a model resolution of 32 2 (Figure 4). On CIFAR-10, we observe that linear probe accuracy at every layer is worse than that of the autoregressive model, with best-layer performance more than 1% lower. Best-layer accuracy on ImageNet is 6% lower.\n\nHowever, during fine-tuning, BERT makes up much of this gap. A fully fine-tuned CIFAR-10 model achieves 98.6% Figure 4. Comparison of auto-regressive pre-training with BERT pre-training using iGPT-L at an input resolution of 32 2 \u00d7 3. Blue bars display linear probe accuracy and orange bars display finetune accuracy. Bold colors show the performance boost from ensembling BERT masks. We see that auto-regressive models produce much better features than BERT models after pre-training, but BERT models catch up after fine-tuning.\n\naccuracy, only 0.4% behind its auto-regressive counterpart, while a fully fine-tuned ImageNet model achieves 66.5%, slightly surpassing auto-regressive performance.\n\nFinally, because inputs to the BERT model are masked at training time, we must also mask them at evaluation time to keep inputs in-distribution. This masking corruption may hinder the BERT model's ability to correctly predict image classes. Therefore, we also try an evaluation scheme where we sample 5 independent masks for each input and take the modal prediction, breaking ties at random. In this setting, CIFAR-10 results are largely unchanged, but on ImageNet, we gain almost 1% on our linear probes and fine-tunes.\n\n\nLow-Data CIFAR-10 Classification\n\nEvaluations of unsupervised representations often reuse supervised learning datasets which have thousands to millions of labeled examples. However, a representation which has robustly encoded a semantic concept should be exceedingly data efficient. As inspiration, we note that humans are able to reliably recognize even novel concepts with a single example (Carey and Bartlett 1978). This motivates evaluating performance in a low-data regime as well. It is also a more realistic evaluation setting for the potential practical usefulness of an approach since it better matches the common real-world scenario of an abundance of raw data but a lack of labels.\n\nIn contrast with recent approaches for low-data classification, we do not make use of pseudo-labeling or data augmentation. Instead, we work directly on a subset of the raw supervised dataset, extracting features using our pre-trained model, and training a linear classifier on those features. Table 4. Comparing performance on low-data CIFAR-10. By leveraging many unlabeled ImageNet images, iGPT-L is able to outperform methods such as Mean Teacher (Tarvainen & Valpola, 2017) and MixMatch (Berthelot et al., 2019) but still underperforms the state of the art methods Sohn et al., 2020). Our approach to semi-supervised learning is very simple since we only fit a logistic regression classifier on iGPT-L's features without any data augmentation or fine-tuning -a significant difference from specially designed semi-supervised approaches. Other results reported from FixMatch (Sohn et al., 2020). As is standard in the low-data setting, we sample 5 random subsets and report mean and standard deviation accuracies (Table 4). On CIFAR-10, we find that with 4 labels per class, we achieve 73.2% accuracy outperforming MixMatch with much lower variance between runs and with 25 labels per class, we achieve 87.6% accuracy, though still significantly lower than the state of the art, FixMatch.\n\nAlthough we have established that large models are necessary for producing good representations, large models are also difficult to fine-tune in the ultra-low data regime. Indeed, we find that iGPT-L quickly memorizes a 40-example training set and fails to generalize well, achieving only 42.1% accuracy. We expect adapting recent approaches to semi-supervised learning will help in this setting.\n\n\nRelated Work\n\nMany generative models have been developed and evaluated for their representation learning capabilities. Notably, GANs (Goodfellow et al., 2014;Radford et al., 2015;Donahue et al., 2016) and VAEs (Kingma & Welling, 2013;Kingma et al., 2014;Higgins et al., 2017) have been wellstudied.\n\nAs of yet, most generative model based approaches have not been competitive with supervised and self-supervised methods in the image domain. A notable exception is Big-BiGAN (Donahue & Simonyan, 2019) which first demonstrated that sufficiently high fidelity generative models learn image representations which are competitive with other selfsupervised methods.\n\nMany self-supervised approaches focus on designing auxiliary objectives which support the learning of useful representations without attempting to directly model the input data. Examples include surrogate classification (Dosovit-skiy et al., 2015), jigsaw puzzle solving (Noroozi & Favaro, 2016), and rotation prediction (Gidaris et al., 2018). A cluster of similar approaches based on contrastive losses comparing various views and transformations of input images have recently driven significant progress in self-supervised learning (Hjelm et al., 2018;Bachman et al., 2019;Tian et al., 2019).\n\nAmong contrastive approaches, our work is most similar to Contrast Predictive Coding (Oord et al., 2018) which also utilizes a autoregressive prediction objective, but in a learned latent space, and to Selfie (Trinh et al., 2019) which trains a bidirectional self-attention architecture on top of a standard convolutional network to differentiate correct vs wrong patches.\n\nOur work is directly inspired by the success of generative pre-training methods developed for Natural Language Processing. These methods predict some parts of a piece of text conditioned on other parts. Our work explores two training objectives in this framework, autoregressive prediction as originally explored for modern neural sequence models by Dai & Le (2015), and a denoising objective, similar to BERT (Devlin et al., 2018). The context in-painting approach of Pathak et al. (2016) also explores pre-training by predicting corruptions but predicts large regions of high-resolution images. Expressive autoregressive models tractably optimizing likelihood were first applied to images by Uria et al. (2013) and popularized by Oord et al. (2016) serving for the basis of several papers similarly adapting transformers to the problem of generative image modeling (Parmar et al., 2018;Child et al., 2019). Ke et al. (2018) introduced the pixel-by-pixel CIFAR10 task and first benchmarked the performance of a 1D sequence transformer on a competitive image classification dataset. Rives et al. (2019) similarly investigates whether the recent success of unsupervised pre-training in NLP applies to other domains, observing promising results on protein sequence data.\n\n\nDiscussion and Conclusion\n\nOur results suggest that generative image modeling continues to be a promising route to learn high-quality unsupervised image representations. Simply predicting pixels learns state of the art representations for low resolution datasets. In high resolution settings, our approach is also competitive with other self-supervised results on ImageNet.\n\nHowever, our experiments also demonstrate several areas for improvement. We currently model low resolution inputs with self-attention. By comparison, most other selfsupervised results use CNN based encoders that easily work with high resolution images. It is not immediately obvious how to best bridge the gap between performant autoregressive and discriminative models. Additionally, we observed that our approach requires large models in order to learn high quality representations. iGPT-L has 2 to 3 times as many parameters as similarly performing models on Ima-geNet and uses more compute.\n\nAlthough dense self-attention was a deliberate choice for this work due to it being domain agnostic and widely used in NLP, it becomes very memory and computationally expensive due to its quadratic scaling with sequence length. We mitigated this via the context reduction techniques discussed in section 3.2 but it is still a significant limitation. Future work could instead address this via architectural changes by exploring more efficient self-attention approaches. Several promising techniques have recently been developed such as local 2D relative attention Ramachandran et al., 2019), sparse attention patterns , locality sensitive hashing (Kitaev et al., 2020), and multiscale modeling (Menick & Kalchbrenner, 2018).\n\nFinally, our results, considered together with Donahue & Simonyan (2019), suggest revisiting the representation learning capabilities of other families of generative models such as flows (Dinh et al., 2014;Kingma & Dhariwal, 2018) and VAEs in order to study whether they show similarly competitive representation learning capabilities.\n\n\nA. Experimental details\n\nA.1. Hyperparameters\n\nIn Table 5, we present the learning rates used to train each model in the paper. When using too high a learning rate, we observe an irrecoverable loss spike early on in training. Conversely, with too low a learning rate, training is stable but loss improves slowly and eventually underperforms. As we increase model size, the irrecoverable loss spike occurs at even lower learning rates. This motivates our procedure of sequentially searching learning rates from large to small and explains why larger models use lower learning rates than smaller models at fixed input resolution.\n\nWe used an Adam \u03b2 2 of 0.95 instead of the default 0.999 because the latter causes loss spikes during training. We did not use weight decay because applying a small weight decay of 0.01 did not change representation quality.\n\nOn iGPT-S, we found small gains in representation quality from using float32 instead of float16, from untying the token embedding matrix and the matrix producing token logits, and from zero initializing the matrices producing token and class logits. We applied these settings to all models.\n\nWhen training BERT models, one additional hyperparameter is the masking probability, set to 15% in Devlin et al. (2018). iGPT-S auto-regressive 32 2 \u00d7 3 0.003 iGPT-M auto-regressive 32 2 \u00d7 3 0.003 iGPT-L auto-regressive 32 2 \u00d7 3 0.001 iGPT-L auto-regressive 48 2 \u00d7 3 0.01 iGPT-XL auto-regressive 64 2 \u00d7 3 0.0003 iGPT-S BERT 32 2 \u00d7 3 0.01 iGPT-M BERT 32 2 \u00d7 3 0.003 iGPT-L BERT 32 2 \u00d7 3 0.001\n\nWe also tried higher masking rates of 20%, 25%, 30%, and 35%, finding that 20% matched the performance of 15%, though higher probabilities decreased performance.\n\n\nKolesnikov et al. (2019); Goyal et al. (2019) conducted rigorous investigations of existing self-supervised methods. Several of our findings are consistent with their results, including the benefits of scale and the non-monotonic performance of representations with depth in certain architectures.\n\nTable 1 .\n1Comparing linear probe accuracies between our models and state-of-the-art models utilizing unsupervised ImageNet transfer or supervised ImageNet transfer.Model \nAcc Unsup Transfer Sup Transfer \n\nCIFAR-10 \nResNet-152 \n94 \n\u221a \n\nSimCLR \n95.3 \n\u221a \n\niGPT-L \n96.3 \n\u221a \n\nCIFAR-100 \nResNet-152 78.0 \n\u221a \n\nSimCLR \n80.2 \n\u221a \n\niGPT-L \n82.8 \n\u221a \n\nSTL-10 \nAMDIM-L \n94.2 \n\u221a \n\niGPT-L \n95.5 \n\u221a \n\nseveral model capacities, with higher capacity models \nachieving better validation losses. This highlights the im-\nportance of scale for our approach. Note that for a given \nvalidation loss value, bigger models also perform better. \n\n\n\nTable results :\nresultsAutoAugment (Cubuk et al., \n2019), SimCLR (Chen et al., 2020), GPipe (Huang et al., 2019), \nEfficentNet (Tan & Le, 2019) \n\nModel \nAcc Unsup Transfer Sup Transfer \n\nCIFAR-10 \nAutoAugment 98.5 \nSimCLR \n98.6 \n\u221a \n\nGPipe \n99.0 \n\u221a \n\niGPT-L \n99.0 \n\u221a \n\nCIFAR-100 \niGPT-L \n88.5 \n\u221a \n\nSimCLR \n89.0 \n\u221a \n\nAutoAugment 89.3 \nEfficientNet \n91.7 \n\u221a \n\non these datasets, though we do not use sophisticated data \naugmentation techniques. In fact, 99.0% ties GPipe, the best \nmodel which pre-trains using ImageNet labels. \n\n\n\nTable 5 .\n5Learning rates used for each model, objective, and input resolution (IR) combination.Model \nObjective \nIR \nLearning Rate \n\n\nOpenAI, San Francisco, CA, USA. Correspondence to: Mark Chen <mark@openai.com>.\n\n. J L Ba, J R Kiros, G Hinton, arXiv:1607.06450E. Layer normalization. arXiv preprintBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n\nLearning representations by maximizing mutual information across views. P Bachman, R D Hjelm, W Buchwalter, Advances in Neural Information Processing Systems. Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. 15509-15519, 2019.\n\nAttention augmented convolutional networks. I Bello, B Zoph, A Vaswani, J Shlens, Q V Le, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionBello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V. Attention augmented convolutional networks. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 3286-3295, 2019.\n\nA holistic approach to semi-supervised learning. D Berthelot, N Carlini, I Goodfellow, N Papernot, A Oliver, C A Raffel, Mixmatch, Advances in Neural Information Processing Systems. Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and Raffel, C. A. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 5050-5060, 2019.\n\nA simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, arXiv:2002.05709arXiv preprintChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual rep- resentations. arXiv preprint arXiv:2002.05709, 2020.\n\nGenerating long sequences with sparse transformers. R Child, S Gray, A Radford, I Sutskever, arXiv:1904.10509arXiv preprintChild, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.\n\nAn analysis of singlelayer networks in unsupervised feature learning. A Coates, A Ng, H Lee, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statisticsCoates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215-223, 2011.\n\nE Cubuk, B Zoph, D Mane, V Vasudevan, Q V Le, Autoaugment, Learning augmentation strategies from data. Cubuk, E., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Learning augmentation strategies from data, 2019.\n\nSemi-supervised sequence learning. A M Dai, Q V Le, Advances in neural information processing systems. Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. In Advances in neural information processing systems, pp. 3079-3087, 2015.\n\nJ Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nNice: Non-linear independent components estimation. L Dinh, D Krueger, Y Bengio, arXiv:1410.8516arXiv preprintDinh, L., Krueger, D., and Bengio, Y. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014.\n\nUnsupervised visual representation learning by context prediction. C Doersch, A Gupta, A A Efros, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionDoersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422-1430, 2015.\n\nLarge scale adversarial representation learning. J Donahue, K Simonyan, Advances in Neural Information Processing Systems. Donahue, J. and Simonyan, K. Large scale adversarial rep- resentation learning. In Advances in Neural Information Processing Systems, pp. 10541-10551, 2019.\n\nJ Donahue, P Kr\u00e4henb\u00fchl, Darrell , T , arXiv:1605.09782Adversarial feature learning. arXiv preprintDonahue, J., Kr\u00e4henb\u00fchl, P., and Darrell, T. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.\n\nDiscriminative unsupervised feature learning with exemplar convolutional neural networks. A Dosovitskiy, P Fischer, J T Springenberg, M Riedmiller, T Brox, IEEE transactions on pattern analysis and machine intelligence. 38Dosovitskiy, A., Fischer, P., Springenberg, J. T., Riedmiller, M., and Brox, T. Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE transactions on pattern analysis and machine intel- ligence, 38(9):1734-1747, 2015.\n\nWhy does unsupervised pretraining help deep learning. D Erhan, Y Bengio, A Courville, P.-A Manzagol, P Vincent, S Bengio, Journal of Machine Learning Research. 11Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vin- cent, P., and Bengio, S. Why does unsupervised pre- training help deep learning? Journal of Machine Learn- ing Research, 11(Feb):625-660, 2010.\n\nUnsupervised representation learning by predicting image rotations. S Gidaris, P Singh, N Komodakis, arXiv:1803.07728arXiv preprintGidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.\n\nUnderstanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio, Proceedings of the thirteenth international conference on artificial intelligence and statistics. the thirteenth international conference on artificial intelligence and statisticsGlorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Pro- ceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256, 2010.\n\nThe reversible residual network: Backpropagation without storing activations. A N Gomez, M Ren, R Urtasun, R B Grosse, Advances in neural information processing systems. Gomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. The reversible residual network: Backpropagation without storing activations. In Advances in neural information processing systems, pp. 2214-2224, 2017.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672-2680, 2014.\n\nScaling and benchmarking self-supervised visual representation learning. P Goyal, D Mahajan, A Gupta, I Misra, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionGoyal, P., Mahajan, D., Gupta, A., and Misra, I. Scaling and benchmarking self-supervised visual representation learn- ing. In Proceedings of the IEEE International Conference on Computer Vision, pp. 6391-6400, 2019.\n\nTowards end-to-end speech recognition with recurrent neural networks. A Graves, N Jaitly, International conference on machine learning. Graves, A. and Jaitly, N. Towards end-to-end speech recog- nition with recurrent neural networks. In International conference on machine learning, pp. 1764-1772, 2014.\n\nMomentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, arXiv:1911.05722arXiv preprintHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo- mentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.\n\nO J H\u00e9naff, A Razavi, C Doersch, S Eslami, A V Oord, arXiv:1905.09272Data-efficient image recognition with contrastive predictive coding. arXiv preprintH\u00e9naff, O. J., Razavi, A., Doersch, C., Eslami, S., and Oord, A. v. d. Data-efficient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.\n\nI Higgins, L Matthey, A Pal, C Burgess, X Glorot, M Botvinick, S Mohamed, A Lerchner, Betavae, Learning basic visual concepts. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. beta- vae: Learning basic visual concepts with a constrained variational framework. 2017.\n\nA fast learning algorithm for deep belief nets. G E Hinton, S Osindero, Y.-W Teh, Neural computation. 187Hinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning algorithm for deep belief nets. Neural computation, 18 (7):1527-1554, 2006.\n\nLearning deep representations by mutual information estimation and maximization. R D Hjelm, A Fedorov, S Lavoie-Marchildon, K Grewal, P Bachman, A Trischler, Y Bengio, arXiv:1808.06670arXiv preprintHjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.\n\nUniversal language model fine-tuning for text classification. J Howard, S Ruder, arXiv:1801.06146arXiv preprintHoward, J. and Ruder, S. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.\n\nKernel methods match deep neural networks on timit. P.-S Huang, H Avron, T N Sainath, V Sindhwani, B Ramabhadran, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEHuang, P.-S., Avron, H., Sainath, T. N., Sindhwani, V., and Ramabhadran, B. Kernel methods match deep neural net- works on timit. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 205-209. IEEE, 2014.\n\nEfficient training of giant neural networks using pipeline parallelism. Y Huang, Y Cheng, A Bapna, O Firat, D Chen, M Chen, H Lee, J Ngiam, Q V Le, Y Wu, Advances in Neural Information Processing Systems. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in Neural Information Process- ing Systems, pp. 103-112, 2019.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, arXiv:1502.03167arXiv preprintIoffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n\nSparse attentive backtracking: Temporal credit assignment through reminding. N R Ke, A G A P Goyal, O Bilaniuk, J Binas, M C Mozer, C Pal, Y Bengio, Advances in neural information processing systems. Ke, N. R., GOYAL, A. G. A. P., Bilaniuk, O., Binas, J., Mozer, M. C., Pal, C., and Bengio, Y. Sparse attentive backtracking: Temporal credit assignment through re- minding. In Advances in neural information processing systems, pp. 7640-7651, 2018.\n\nGenerative flow with invertible 1x1 convolutions. D P Kingma, P Dhariwal, Glow, Advances in Neural Information Processing Systems. Kingma, D. P. and Dhariwal, P. Glow: Generative flow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems, pp. 10215-10224, 2018.\n\nAuto-encoding variational bayes. D P Kingma, M Welling, arXiv:1312.6114arXiv preprintKingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\nSemi-supervised learning with deep generative models. D P Kingma, S Mohamed, D J Rezende, M Welling, Advances in neural information processing systems. Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. Semi-supervised learning with deep generative mod- els. In Advances in neural information processing sys- tems, pp. 3581-3589, 2014.\n\nReformer: The efficient transformer. N Kitaev, \u0141 Kaiser, A Levskaya, arXiv:2001.04451arXiv preprintKitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.\n\nRevisiting selfsupervised visual representation learning. A Kolesnikov, X Zhai, L Beyer, Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. the IEEE conference on Computer Vision and Pattern RecognitionKolesnikov, A., Zhai, X., and Beyer, L. Revisiting self- supervised visual representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1920-1929, 2019.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.\n\nThe neural autoregressive distribution estimator. H Larochelle, I Murray, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. the Fourteenth International Conference on Artificial Intelligence and StatisticsLarochelle, H. and Murray, I. The neural autoregressive distribution estimator. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29-37, 2011.\n\nPrincipled hybrids of generative and discriminative models. J A Lasserre, C M Bishop, T P Minka, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06). IEEE1Lasserre, J. A., Bishop, C. M., and Minka, T. P. Principled hybrids of generative and discriminative models. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), volume 1, pp. 87- 94. IEEE, 2006.\n\nConvolutional deep belief networks for scalable unsupervised learning of hierarchical representations. H Lee, R Grosse, R Ranganath, A Y Ng, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningLee, H., Grosse, R., Ranganath, R., and Ng, A. Y. Convo- lutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th annual international conference on machine learning, pp. 609-616, 2009.\n\nKernel approximation methods for speech recognition. A May, A B Garakani, Z Lu, D Guo, K Liu, A Bellet, L Fan, M Collins, D Hsu, B Kingsbury, arXiv:1701.03577arXiv preprintMay, A., Garakani, A. B., Lu, Z., Guo, D., Liu, K., Bellet, A., Fan, L., Collins, M., Hsu, D., Kingsbury, B., et al. Kernel approximation methods for speech recognition. arXiv preprint arXiv:1701.03577, 2017.\n\nGenerating high fidelity images with subscale pixel networks and multidimensional upscaling. J Menick, N Kalchbrenner, arXiv:1812.01608arXiv preprintMenick, J. and Kalchbrenner, N. Generating high fidelity im- ages with subscale pixel networks and multidimensional upscaling. arXiv preprint arXiv:1812.01608, 2018.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in neural information processing systems. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. In Advances in neural infor- mation processing systems, pp. 3111-3119, 2013.\n\nSelf-supervised learning of pretext-invariant representations. I Misra, L Van Der Maaten, arXiv:1912.01991arXiv preprintMisra, I. and van der Maaten, L. Self-supervised learn- ing of pretext-invariant representations. arXiv preprint arXiv:1912.01991, 2019.\n\nDeep belief networks for phone recognition. A Mohamed, G Dahl, G Hinton, Mohamed, A.-r., Dahl, G., and Hinton, G. Deep belief networks for phone recognition. 2009.\n\nRectified linear units improve restricted boltzmann machines. V Nair, G E Hinton, Proceedings of the 27th international conference on machine learning (ICML-10). the 27th international conference on machine learning (ICML-10)Nair, V. and Hinton, G. E. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814, 2010.\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. M Noroozi, P Favaro, European Conference on Computer Vision. SpringerNoroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, pp. 69-84. Springer, 2016.\n\nA Oord, N Kalchbrenner, K Kavukcuoglu, arXiv:1601.06759Pixel recurrent neural networks. arXiv preprintOord, A. v. d., Kalchbrenner, N., and Kavukcuoglu, K. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.\n\nA Oord, Y Li, O Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintOord, A. v. d., Li, Y., and Vinyals, O. Representation learn- ing with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nAn analysis of unsupervised pre-training in light of recent advances. T L Paine, P Khorrami, W Han, T S Huang, arXiv:1412.6597arXiv preprintPaine, T. L., Khorrami, P., Han, W., and Huang, T. S. An analysis of unsupervised pre-training in light of recent advances. arXiv preprint arXiv:1412.6597, 2014.\n\n. N Parmar, A Vaswani, J Uszkoreit, \u0141 Kaiser, N Shazeer, A Ku, Tran , D , arXiv:1802.05751Image transformer. arXiv preprintParmar, N., Vaswani, A., Uszkoreit, J., Kaiser, \u0141., Shazeer, N., Ku, A., and Tran, D. Image transformer. arXiv preprint arXiv:1802.05751, 2018.\n\nContext encoders: Feature learning by inpainting. D Pathak, P Krahenbuhl, J Donahue, T Darrell, A A Efros, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionPathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and Efros, A. A. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2536-2544, 2016.\n\nScikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 12Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour- napeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.\n\nM E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, arXiv:1802.05365Deep contextualized word representations. arXiv preprintPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, arXiv:1511.06434arXiv preprintRadford, A., Metz, L., and Chintala, S. Unsupervised rep- resentation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\nImproving language understanding by generative pretraining. A Radford, K Narasimhan, T Salimans, I Sutskever, Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre- training. 2018.\n\nLanguage models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.\n\nStand-alone self-attention in vision models. P Ramachandran, N Parmar, A Vaswani, I Bello, A Levskaya, J Shlens, arXiv:1906.05909arXiv preprintRamachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev- skaya, A., and Shlens, J. Stand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909, 2019.\n\nVideo (language) modeling: a baseline for generative models of natural videos. M Ranzato, A Szlam, J Bruna, M Mathieu, R Collobert, S Chopra, arXiv:1412.6604arXiv preprintRanzato, M., Szlam, A., Bruna, J., Mathieu, M., Collobert, R., and Chopra, S. Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604, 2014.\n\nBiological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. bioRxiv. A Rives, S Goyal, J Meier, D Guo, M Ott, C L Zitnick, J Ma, Fergus , R , 622803Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., and Fergus, R. Biological structure and func- tion emerge from scaling unsupervised learning to 250 million protein sequences. bioRxiv, pp. 622803, 2019.\n\nNon-discriminative data or weak model? on the relative importance of data and model resolution. M Sandler, J Baccash, A Zhmoginov, A Howard, Proceedings of the IEEE International Conference on Computer Vision Workshops. the IEEE International Conference on Computer Vision WorkshopsSandler, M., Baccash, J., Zhmoginov, A., and Howard, A. Non-discriminative data or weak model? on the relative importance of data and model resolution. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 0-0, 2019.\n\nFixmatch: Simplifying semi-supervised learning with consistency and confidence. K Sohn, D Berthelot, C.-L Li, Z Zhang, N Carlini, E D Cubuk, A Kurakin, H Zhang, C Raffel, arXiv:2001.07685arXiv preprintSohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C. Fix- match: Simplifying semi-supervised learning with consis- tency and confidence. arXiv preprint arXiv:2001.07685, 2020.\n\nM Tan, Q V Le, Efficientnet, arXiv:1905.11946Rethinking model scaling for convolutional neural networks. arXiv preprintTan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. A Tarvainen, H Valpola, Advances in neural information processing systems. Tarvainen, A. and Valpola, H. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pp. 1195-1204, 2017.\n\nY Tian, D Krishnan, P Isola, arXiv:1906.05849Contrastive multiview coding. arXiv preprintTian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.\n\n80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence. A Torralba, R Fergus, W T Freeman, 30Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence, 30(11):1958-1970, 2008.\n\nT H Trinh, M.-T Luong, Q V Le, Selfie, arXiv:1906.02940Selfsupervised pretraining for image embedding. arXiv preprintTrinh, T. H., Luong, M.-T., and Le, Q. V. Selfie: Self- supervised pretraining for image embedding. arXiv preprint arXiv:1906.02940, 2019.\n\nThe realvalued neural autoregressive density-estimator. B Uria, I Murray, H Larochelle, Rnade, Advances in Neural Information Processing Systems. Uria, B., Murray, I., and Larochelle, H. Rnade: The real- valued neural autoregressive density-estimator. In Ad- vances in Neural Information Processing Systems, pp. 2175-2183, 2013.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in neural information processing systems. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.\n\nExtracting and composing robust features with denoising autoencoders. P Vincent, H Larochelle, Y Bengio, P.-A Manzagol, Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learningVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096-1103, 2008.\n\n. Q Xie, Z Dai, E Hovy, M.-T Luong, Q V Le, arXiv:1904.12848Unsupervised data augmentation. arXiv preprintXie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848, 2019.\n\n. S Zagoruyko, N Komodakis, arXiv:1605.07146Wide residual networks. arXiv preprintZagoruyko, S. and Komodakis, N. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\n\nVisualizing and understanding convolutional networks. M D Zeiler, R Fergus, European conference on computer vision. SpringerZeiler, M. D. and Fergus, R. Visualizing and understand- ing convolutional networks. In European conference on computer vision, pp. 818-833. Springer, 2014.\n", "annotations": {"author": "[{\"end\":48,\"start\":38},{\"end\":62,\"start\":49},{\"end\":75,\"start\":63},{\"end\":84,\"start\":76},{\"end\":96,\"start\":85},{\"end\":115,\"start\":97},{\"end\":127,\"start\":116},{\"end\":143,\"start\":128},{\"end\":48,\"start\":38},{\"end\":62,\"start\":49},{\"end\":75,\"start\":63},{\"end\":84,\"start\":76},{\"end\":96,\"start\":85},{\"end\":115,\"start\":97},{\"end\":127,\"start\":116},{\"end\":143,\"start\":128}]", "publisher": null, "author_last_name": "[{\"end\":47,\"start\":43},{\"end\":61,\"start\":54},{\"end\":74,\"start\":69},{\"end\":83,\"start\":81},{\"end\":95,\"start\":92},{\"end\":114,\"start\":106},{\"end\":126,\"start\":122},{\"end\":142,\"start\":133},{\"end\":47,\"start\":43},{\"end\":61,\"start\":54},{\"end\":74,\"start\":69},{\"end\":83,\"start\":81},{\"end\":95,\"start\":92},{\"end\":114,\"start\":106},{\"end\":126,\"start\":122},{\"end\":142,\"start\":133}]", "author_first_name": "[{\"end\":42,\"start\":38},{\"end\":53,\"start\":49},{\"end\":68,\"start\":63},{\"end\":80,\"start\":76},{\"end\":91,\"start\":85},{\"end\":105,\"start\":97},{\"end\":121,\"start\":116},{\"end\":132,\"start\":128},{\"end\":42,\"start\":38},{\"end\":53,\"start\":49},{\"end\":68,\"start\":63},{\"end\":80,\"start\":76},{\"end\":91,\"start\":85},{\"end\":105,\"start\":97},{\"end\":121,\"start\":116},{\"end\":132,\"start\":128}]", "author_affiliation": null, "title": "[{\"end\":35,\"start\":1},{\"end\":178,\"start\":144},{\"end\":35,\"start\":1},{\"end\":178,\"start\":144}]", "venue": null, "abstract": "[{\"end\":1055,\"start\":180},{\"end\":1055,\"start\":180}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1247,\"start\":1226},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":1296,\"start\":1274},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1373,\"start\":1355},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":1419,\"start\":1397},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1603,\"start\":1580},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1622,\"start\":1603},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1713,\"start\":1692},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1763,\"start\":1740},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1817,\"start\":1794},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1833,\"start\":1817},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2074,\"start\":2053},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2117,\"start\":2100},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2344,\"start\":2319},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2366,\"start\":2344},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2505,\"start\":2485},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2645,\"start\":2623},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2751,\"start\":2735},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2771,\"start\":2751},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2792,\"start\":2771},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2813,\"start\":2792},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2833,\"start\":2813},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3289,\"start\":3267},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3382,\"start\":3363},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3466,\"start\":3441},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3575,\"start\":3554},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3680,\"start\":3663},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3709,\"start\":3680},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3727,\"start\":3709},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":4133,\"start\":4111},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4224,\"start\":4197},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4242,\"start\":4224},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6223,\"start\":6203},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10515,\"start\":10494},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":13119,\"start\":13090},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":14141,\"start\":14118},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":14515,\"start\":14494},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16314,\"start\":16297},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16545,\"start\":16521},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":17306,\"start\":17283},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":24326,\"start\":24304},{\"end\":27143,\"start\":27118},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":27898,\"start\":27871},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27936,\"start\":27912},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":28008,\"start\":27990},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":28317,\"start\":28298},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29270,\"start\":29245},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":29291,\"start\":29270},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29312,\"start\":29291},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29346,\"start\":29322},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29366,\"start\":29346},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29387,\"start\":29366},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29612,\"start\":29586},{\"end\":30021,\"start\":29994},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30069,\"start\":30045},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30117,\"start\":30095},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30329,\"start\":30309},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30350,\"start\":30329},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":30368,\"start\":30350},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":30475,\"start\":30456},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":30600,\"start\":30580},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31110,\"start\":31095},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31176,\"start\":31155},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":31234,\"start\":31214},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":31457,\"start\":31439},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":31495,\"start\":31477},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31633,\"start\":31612},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31652,\"start\":31633},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31670,\"start\":31654},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":31847,\"start\":31828},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":33577,\"start\":33551},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33655,\"start\":33634},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33710,\"start\":33681},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33785,\"start\":33760},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33919,\"start\":33900},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33943,\"start\":33919},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35317,\"start\":35297},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1247,\"start\":1226},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":1296,\"start\":1274},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1373,\"start\":1355},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":1419,\"start\":1397},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1603,\"start\":1580},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1622,\"start\":1603},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1713,\"start\":1692},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1763,\"start\":1740},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1817,\"start\":1794},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1833,\"start\":1817},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2074,\"start\":2053},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2117,\"start\":2100},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2344,\"start\":2319},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2366,\"start\":2344},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2505,\"start\":2485},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2645,\"start\":2623},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2751,\"start\":2735},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2771,\"start\":2751},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2792,\"start\":2771},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2813,\"start\":2792},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2833,\"start\":2813},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3289,\"start\":3267},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3382,\"start\":3363},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3466,\"start\":3441},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3575,\"start\":3554},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3680,\"start\":3663},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3709,\"start\":3680},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3727,\"start\":3709},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":4133,\"start\":4111},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4224,\"start\":4197},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4242,\"start\":4224},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6223,\"start\":6203},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10515,\"start\":10494},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":13119,\"start\":13090},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":14141,\"start\":14118},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":14515,\"start\":14494},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16314,\"start\":16297},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16545,\"start\":16521},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":17306,\"start\":17283},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":24326,\"start\":24304},{\"end\":27143,\"start\":27118},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":27898,\"start\":27871},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27936,\"start\":27912},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":28008,\"start\":27990},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":28317,\"start\":28298},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29270,\"start\":29245},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":29291,\"start\":29270},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29312,\"start\":29291},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29346,\"start\":29322},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29366,\"start\":29346},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29387,\"start\":29366},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29612,\"start\":29586},{\"end\":30021,\"start\":29994},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30069,\"start\":30045},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30117,\"start\":30095},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30329,\"start\":30309},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30350,\"start\":30329},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":30368,\"start\":30350},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":30475,\"start\":30456},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":30600,\"start\":30580},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31110,\"start\":31095},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31176,\"start\":31155},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":31234,\"start\":31214},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":31457,\"start\":31439},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":31495,\"start\":31477},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31633,\"start\":31612},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31652,\"start\":31633},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31670,\"start\":31654},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":31847,\"start\":31828},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":33577,\"start\":33551},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33655,\"start\":33634},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33710,\"start\":33681},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33785,\"start\":33760},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33919,\"start\":33900},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33943,\"start\":33919},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35317,\"start\":35297}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36052,\"start\":35753},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36673,\"start\":36053},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37202,\"start\":36674},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37338,\"start\":37203},{\"attributes\":{\"id\":\"fig_0\"},\"end\":36052,\"start\":35753},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36673,\"start\":36053},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37202,\"start\":36674},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37338,\"start\":37203}]", "paragraph": "[{\"end\":2506,\"start\":1071},{\"end\":3038,\"start\":2508},{\"end\":3728,\"start\":3040},{\"end\":4741,\"start\":3730},{\"end\":5605,\"start\":4743},{\"end\":5868,\"start\":5618},{\"end\":6224,\"start\":5870},{\"end\":6835,\"start\":6226},{\"end\":7120,\"start\":6837},{\"end\":7327,\"start\":7137},{\"end\":7372,\"start\":7341},{\"end\":7557,\"start\":7374},{\"end\":7904,\"start\":7584},{\"end\":8050,\"start\":7951},{\"end\":8457,\"start\":8067},{\"end\":8555,\"start\":8459},{\"end\":8759,\"start\":8557},{\"end\":9146,\"start\":8761},{\"end\":9758,\"start\":9148},{\"end\":10028,\"start\":9760},{\"end\":10170,\"start\":10044},{\"end\":10286,\"start\":10185},{\"end\":10516,\"start\":10288},{\"end\":10685,\"start\":10535},{\"end\":11046,\"start\":10717},{\"end\":11723,\"start\":11062},{\"end\":11993,\"start\":11725},{\"end\":12381,\"start\":12027},{\"end\":12839,\"start\":12383},{\"end\":13120,\"start\":12841},{\"end\":13296,\"start\":13122},{\"end\":13894,\"start\":13318},{\"end\":14723,\"start\":13896},{\"end\":15222,\"start\":14733},{\"end\":15438,\"start\":15224},{\"end\":15903,\"start\":15451},{\"end\":16106,\"start\":15905},{\"end\":16546,\"start\":16108},{\"end\":16717,\"start\":16574},{\"end\":17122,\"start\":16768},{\"end\":17584,\"start\":17124},{\"end\":18446,\"start\":17586},{\"end\":18949,\"start\":18448},{\"end\":19837,\"start\":18951},{\"end\":20091,\"start\":19895},{\"end\":20265,\"start\":20093},{\"end\":20934,\"start\":20303},{\"end\":21362,\"start\":20936},{\"end\":22462,\"start\":21392},{\"end\":23233,\"start\":22473},{\"end\":23653,\"start\":23254},{\"end\":24370,\"start\":23655},{\"end\":24741,\"start\":24372},{\"end\":25156,\"start\":24743},{\"end\":25504,\"start\":25165},{\"end\":26035,\"start\":25506},{\"end\":26201,\"start\":26037},{\"end\":26723,\"start\":26203},{\"end\":27418,\"start\":26760},{\"end\":28711,\"start\":27420},{\"end\":29109,\"start\":28713},{\"end\":29410,\"start\":29126},{\"end\":29772,\"start\":29412},{\"end\":30369,\"start\":29774},{\"end\":30743,\"start\":30371},{\"end\":32013,\"start\":30745},{\"end\":32389,\"start\":32043},{\"end\":32985,\"start\":32391},{\"end\":33711,\"start\":32987},{\"end\":34048,\"start\":33713},{\"end\":34096,\"start\":34076},{\"end\":34678,\"start\":34098},{\"end\":34904,\"start\":34680},{\"end\":35196,\"start\":34906},{\"end\":35589,\"start\":35198},{\"end\":35752,\"start\":35591},{\"end\":2506,\"start\":1071},{\"end\":3038,\"start\":2508},{\"end\":3728,\"start\":3040},{\"end\":4741,\"start\":3730},{\"end\":5605,\"start\":4743},{\"end\":5868,\"start\":5618},{\"end\":6224,\"start\":5870},{\"end\":6835,\"start\":6226},{\"end\":7120,\"start\":6837},{\"end\":7327,\"start\":7137},{\"end\":7372,\"start\":7341},{\"end\":7557,\"start\":7374},{\"end\":7904,\"start\":7584},{\"end\":8050,\"start\":7951},{\"end\":8457,\"start\":8067},{\"end\":8555,\"start\":8459},{\"end\":8759,\"start\":8557},{\"end\":9146,\"start\":8761},{\"end\":9758,\"start\":9148},{\"end\":10028,\"start\":9760},{\"end\":10170,\"start\":10044},{\"end\":10286,\"start\":10185},{\"end\":10516,\"start\":10288},{\"end\":10685,\"start\":10535},{\"end\":11046,\"start\":10717},{\"end\":11723,\"start\":11062},{\"end\":11993,\"start\":11725},{\"end\":12381,\"start\":12027},{\"end\":12839,\"start\":12383},{\"end\":13120,\"start\":12841},{\"end\":13296,\"start\":13122},{\"end\":13894,\"start\":13318},{\"end\":14723,\"start\":13896},{\"end\":15222,\"start\":14733},{\"end\":15438,\"start\":15224},{\"end\":15903,\"start\":15451},{\"end\":16106,\"start\":15905},{\"end\":16546,\"start\":16108},{\"end\":16717,\"start\":16574},{\"end\":17122,\"start\":16768},{\"end\":17584,\"start\":17124},{\"end\":18446,\"start\":17586},{\"end\":18949,\"start\":18448},{\"end\":19837,\"start\":18951},{\"end\":20091,\"start\":19895},{\"end\":20265,\"start\":20093},{\"end\":20934,\"start\":20303},{\"end\":21362,\"start\":20936},{\"end\":22462,\"start\":21392},{\"end\":23233,\"start\":22473},{\"end\":23653,\"start\":23254},{\"end\":24370,\"start\":23655},{\"end\":24741,\"start\":24372},{\"end\":25156,\"start\":24743},{\"end\":25504,\"start\":25165},{\"end\":26035,\"start\":25506},{\"end\":26201,\"start\":26037},{\"end\":26723,\"start\":26203},{\"end\":27418,\"start\":26760},{\"end\":28711,\"start\":27420},{\"end\":29109,\"start\":28713},{\"end\":29410,\"start\":29126},{\"end\":29772,\"start\":29412},{\"end\":30369,\"start\":29774},{\"end\":30743,\"start\":30371},{\"end\":32013,\"start\":30745},{\"end\":32389,\"start\":32043},{\"end\":32985,\"start\":32391},{\"end\":33711,\"start\":32987},{\"end\":34048,\"start\":33713},{\"end\":34096,\"start\":34076},{\"end\":34678,\"start\":34098},{\"end\":34904,\"start\":34680},{\"end\":35196,\"start\":34906},{\"end\":35589,\"start\":35198},{\"end\":35752,\"start\":35591}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7340,\"start\":7328},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7583,\"start\":7558},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7950,\"start\":7905},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10184,\"start\":10171},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10716,\"start\":10686},{\"attributes\":{\"id\":\"formula_0\"},\"end\":7340,\"start\":7328},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7583,\"start\":7558},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7950,\"start\":7905},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10184,\"start\":10171},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10716,\"start\":10686}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20597,\"start\":20588},{\"end\":21864,\"start\":21857},{\"end\":23067,\"start\":23060},{\"end\":23823,\"start\":23816},{\"end\":27721,\"start\":27714},{\"end\":28444,\"start\":28436},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34108,\"start\":34101},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20597,\"start\":20588},{\"end\":21864,\"start\":21857},{\"end\":23067,\"start\":23060},{\"end\":23823,\"start\":23816},{\"end\":27721,\"start\":27714},{\"end\":28444,\"start\":28436},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34108,\"start\":34101}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1069,\"start\":1057},{\"attributes\":{\"n\":\"2.\"},\"end\":5616,\"start\":5608},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7135,\"start\":7123},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8065,\"start\":8053},{\"attributes\":{\"n\":\"2.3.\"},\"end\":10042,\"start\":10031},{\"attributes\":{\"n\":\"2.4.\"},\"end\":10533,\"start\":10519},{\"attributes\":{\"n\":\"3.\"},\"end\":11060,\"start\":11049},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12025,\"start\":11996},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13316,\"start\":13299},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14731,\"start\":14726},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15449,\"start\":15441},{\"attributes\":{\"n\":\"4.\"},\"end\":16572,\"start\":16549},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16766,\"start\":16720},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19893,\"start\":19840},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20301,\"start\":20268},{\"attributes\":{\"n\":\"4.4.\"},\"end\":21390,\"start\":21365},{\"end\":22471,\"start\":22465},{\"attributes\":{\"n\":\"4.5.\"},\"end\":23252,\"start\":23236},{\"attributes\":{\"n\":\"4.6.\"},\"end\":25163,\"start\":25159},{\"attributes\":{\"n\":\"4.7.\"},\"end\":26758,\"start\":26726},{\"attributes\":{\"n\":\"5.\"},\"end\":29124,\"start\":29112},{\"attributes\":{\"n\":\"6.\"},\"end\":32041,\"start\":32016},{\"end\":34074,\"start\":34051},{\"end\":36063,\"start\":36054},{\"end\":36690,\"start\":36675},{\"end\":37213,\"start\":37204},{\"attributes\":{\"n\":\"1.\"},\"end\":1069,\"start\":1057},{\"attributes\":{\"n\":\"2.\"},\"end\":5616,\"start\":5608},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7135,\"start\":7123},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8065,\"start\":8053},{\"attributes\":{\"n\":\"2.3.\"},\"end\":10042,\"start\":10031},{\"attributes\":{\"n\":\"2.4.\"},\"end\":10533,\"start\":10519},{\"attributes\":{\"n\":\"3.\"},\"end\":11060,\"start\":11049},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12025,\"start\":11996},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13316,\"start\":13299},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14731,\"start\":14726},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15449,\"start\":15441},{\"attributes\":{\"n\":\"4.\"},\"end\":16572,\"start\":16549},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16766,\"start\":16720},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19893,\"start\":19840},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20301,\"start\":20268},{\"attributes\":{\"n\":\"4.4.\"},\"end\":21390,\"start\":21365},{\"end\":22471,\"start\":22465},{\"attributes\":{\"n\":\"4.5.\"},\"end\":23252,\"start\":23236},{\"attributes\":{\"n\":\"4.6.\"},\"end\":25163,\"start\":25159},{\"attributes\":{\"n\":\"4.7.\"},\"end\":26758,\"start\":26726},{\"attributes\":{\"n\":\"5.\"},\"end\":29124,\"start\":29112},{\"attributes\":{\"n\":\"6.\"},\"end\":32041,\"start\":32016},{\"end\":34074,\"start\":34051},{\"end\":36063,\"start\":36054},{\"end\":36690,\"start\":36675},{\"end\":37213,\"start\":37204}]", "table": "[{\"end\":36673,\"start\":36219},{\"end\":37202,\"start\":36698},{\"end\":37338,\"start\":37300},{\"end\":36673,\"start\":36219},{\"end\":37202,\"start\":36698},{\"end\":37338,\"start\":37300}]", "figure_caption": "[{\"end\":36052,\"start\":35755},{\"end\":36219,\"start\":36065},{\"end\":37300,\"start\":37215},{\"end\":36052,\"start\":35755},{\"end\":36219,\"start\":36065},{\"end\":37300,\"start\":37215}]", "figure_ref": "[{\"end\":4392,\"start\":4384},{\"end\":16808,\"start\":16800},{\"end\":18444,\"start\":18436},{\"end\":19307,\"start\":19299},{\"end\":20104,\"start\":20096},{\"end\":20394,\"start\":20385},{\"end\":25296,\"start\":25286},{\"end\":25624,\"start\":25616},{\"end\":4392,\"start\":4384},{\"end\":16808,\"start\":16800},{\"end\":18444,\"start\":18436},{\"end\":19307,\"start\":19299},{\"end\":20104,\"start\":20096},{\"end\":20394,\"start\":20385},{\"end\":25296,\"start\":25286},{\"end\":25624,\"start\":25616}]", "bib_author_first_name": "[{\"end\":37423,\"start\":37422},{\"end\":37425,\"start\":37424},{\"end\":37431,\"start\":37430},{\"end\":37433,\"start\":37432},{\"end\":37442,\"start\":37441},{\"end\":37682,\"start\":37681},{\"end\":37693,\"start\":37692},{\"end\":37695,\"start\":37694},{\"end\":37704,\"start\":37703},{\"end\":38009,\"start\":38008},{\"end\":38018,\"start\":38017},{\"end\":38026,\"start\":38025},{\"end\":38037,\"start\":38036},{\"end\":38047,\"start\":38046},{\"end\":38049,\"start\":38048},{\"end\":38425,\"start\":38424},{\"end\":38438,\"start\":38437},{\"end\":38449,\"start\":38448},{\"end\":38463,\"start\":38462},{\"end\":38475,\"start\":38474},{\"end\":38485,\"start\":38484},{\"end\":38487,\"start\":38486},{\"end\":38852,\"start\":38851},{\"end\":38860,\"start\":38859},{\"end\":38873,\"start\":38872},{\"end\":38884,\"start\":38883},{\"end\":39142,\"start\":39141},{\"end\":39151,\"start\":39150},{\"end\":39159,\"start\":39158},{\"end\":39170,\"start\":39169},{\"end\":39429,\"start\":39428},{\"end\":39439,\"start\":39438},{\"end\":39445,\"start\":39444},{\"end\":39858,\"start\":39857},{\"end\":39867,\"start\":39866},{\"end\":39875,\"start\":39874},{\"end\":39883,\"start\":39882},{\"end\":39896,\"start\":39895},{\"end\":39898,\"start\":39897},{\"end\":40120,\"start\":40119},{\"end\":40122,\"start\":40121},{\"end\":40129,\"start\":40128},{\"end\":40131,\"start\":40130},{\"end\":40324,\"start\":40323},{\"end\":40337,\"start\":40333},{\"end\":40346,\"start\":40345},{\"end\":40353,\"start\":40352},{\"end\":40707,\"start\":40706},{\"end\":40715,\"start\":40714},{\"end\":40726,\"start\":40725},{\"end\":40961,\"start\":40960},{\"end\":40972,\"start\":40971},{\"end\":40981,\"start\":40980},{\"end\":40983,\"start\":40982},{\"end\":41364,\"start\":41363},{\"end\":41375,\"start\":41374},{\"end\":41596,\"start\":41595},{\"end\":41607,\"start\":41606},{\"end\":41627,\"start\":41620},{\"end\":41631,\"start\":41630},{\"end\":41900,\"start\":41899},{\"end\":41915,\"start\":41914},{\"end\":41926,\"start\":41925},{\"end\":41928,\"start\":41927},{\"end\":41944,\"start\":41943},{\"end\":41958,\"start\":41957},{\"end\":42346,\"start\":42345},{\"end\":42355,\"start\":42354},{\"end\":42365,\"start\":42364},{\"end\":42381,\"start\":42377},{\"end\":42393,\"start\":42392},{\"end\":42404,\"start\":42403},{\"end\":42727,\"start\":42726},{\"end\":42738,\"start\":42737},{\"end\":42747,\"start\":42746},{\"end\":43017,\"start\":43016},{\"end\":43027,\"start\":43026},{\"end\":43518,\"start\":43517},{\"end\":43520,\"start\":43519},{\"end\":43529,\"start\":43528},{\"end\":43536,\"start\":43535},{\"end\":43547,\"start\":43546},{\"end\":43549,\"start\":43548},{\"end\":43847,\"start\":43846},{\"end\":43861,\"start\":43860},{\"end\":43878,\"start\":43877},{\"end\":43887,\"start\":43886},{\"end\":43893,\"start\":43892},{\"end\":43909,\"start\":43908},{\"end\":43918,\"start\":43917},{\"end\":43931,\"start\":43930},{\"end\":44283,\"start\":44282},{\"end\":44292,\"start\":44291},{\"end\":44303,\"start\":44302},{\"end\":44312,\"start\":44311},{\"end\":44730,\"start\":44729},{\"end\":44740,\"start\":44739},{\"end\":45032,\"start\":45031},{\"end\":45038,\"start\":45037},{\"end\":45045,\"start\":45044},{\"end\":45051,\"start\":45050},{\"end\":45058,\"start\":45057},{\"end\":45260,\"start\":45259},{\"end\":45262,\"start\":45261},{\"end\":45272,\"start\":45271},{\"end\":45282,\"start\":45281},{\"end\":45293,\"start\":45292},{\"end\":45303,\"start\":45302},{\"end\":45305,\"start\":45304},{\"end\":45592,\"start\":45591},{\"end\":45603,\"start\":45602},{\"end\":45614,\"start\":45613},{\"end\":45621,\"start\":45620},{\"end\":45632,\"start\":45631},{\"end\":45642,\"start\":45641},{\"end\":45655,\"start\":45654},{\"end\":45666,\"start\":45665},{\"end\":45963,\"start\":45962},{\"end\":45965,\"start\":45964},{\"end\":45975,\"start\":45974},{\"end\":45990,\"start\":45986},{\"end\":46238,\"start\":46237},{\"end\":46240,\"start\":46239},{\"end\":46249,\"start\":46248},{\"end\":46260,\"start\":46259},{\"end\":46281,\"start\":46280},{\"end\":46291,\"start\":46290},{\"end\":46302,\"start\":46301},{\"end\":46315,\"start\":46314},{\"end\":46643,\"start\":46642},{\"end\":46653,\"start\":46652},{\"end\":46874,\"start\":46870},{\"end\":46883,\"start\":46882},{\"end\":46892,\"start\":46891},{\"end\":46894,\"start\":46893},{\"end\":46905,\"start\":46904},{\"end\":46918,\"start\":46917},{\"end\":47344,\"start\":47343},{\"end\":47353,\"start\":47352},{\"end\":47362,\"start\":47361},{\"end\":47371,\"start\":47370},{\"end\":47380,\"start\":47379},{\"end\":47388,\"start\":47387},{\"end\":47396,\"start\":47395},{\"end\":47403,\"start\":47402},{\"end\":47412,\"start\":47411},{\"end\":47414,\"start\":47413},{\"end\":47420,\"start\":47419},{\"end\":47836,\"start\":47835},{\"end\":47845,\"start\":47844},{\"end\":48123,\"start\":48122},{\"end\":48125,\"start\":48124},{\"end\":48131,\"start\":48130},{\"end\":48137,\"start\":48132},{\"end\":48146,\"start\":48145},{\"end\":48158,\"start\":48157},{\"end\":48167,\"start\":48166},{\"end\":48169,\"start\":48168},{\"end\":48178,\"start\":48177},{\"end\":48185,\"start\":48184},{\"end\":48545,\"start\":48544},{\"end\":48547,\"start\":48546},{\"end\":48557,\"start\":48556},{\"end\":48824,\"start\":48823},{\"end\":48826,\"start\":48825},{\"end\":48836,\"start\":48835},{\"end\":49032,\"start\":49031},{\"end\":49034,\"start\":49033},{\"end\":49044,\"start\":49043},{\"end\":49055,\"start\":49054},{\"end\":49057,\"start\":49056},{\"end\":49068,\"start\":49067},{\"end\":49361,\"start\":49360},{\"end\":49371,\"start\":49370},{\"end\":49381,\"start\":49380},{\"end\":49599,\"start\":49598},{\"end\":49613,\"start\":49612},{\"end\":49621,\"start\":49620},{\"end\":50040,\"start\":50039},{\"end\":50054,\"start\":50053},{\"end\":50067,\"start\":50066},{\"end\":50069,\"start\":50068},{\"end\":50370,\"start\":50369},{\"end\":50384,\"start\":50383},{\"end\":50832,\"start\":50831},{\"end\":50834,\"start\":50833},{\"end\":50846,\"start\":50845},{\"end\":50848,\"start\":50847},{\"end\":50858,\"start\":50857},{\"end\":50860,\"start\":50859},{\"end\":51303,\"start\":51302},{\"end\":51310,\"start\":51309},{\"end\":51320,\"start\":51319},{\"end\":51333,\"start\":51332},{\"end\":51335,\"start\":51334},{\"end\":51786,\"start\":51785},{\"end\":51793,\"start\":51792},{\"end\":51795,\"start\":51794},{\"end\":51807,\"start\":51806},{\"end\":51813,\"start\":51812},{\"end\":51820,\"start\":51819},{\"end\":51827,\"start\":51826},{\"end\":51837,\"start\":51836},{\"end\":51844,\"start\":51843},{\"end\":51855,\"start\":51854},{\"end\":51862,\"start\":51861},{\"end\":52208,\"start\":52207},{\"end\":52218,\"start\":52217},{\"end\":52508,\"start\":52507},{\"end\":52519,\"start\":52518},{\"end\":52532,\"start\":52531},{\"end\":52540,\"start\":52539},{\"end\":52542,\"start\":52541},{\"end\":52553,\"start\":52552},{\"end\":52897,\"start\":52896},{\"end\":52906,\"start\":52905},{\"end\":53136,\"start\":53135},{\"end\":53147,\"start\":53146},{\"end\":53155,\"start\":53154},{\"end\":53319,\"start\":53318},{\"end\":53327,\"start\":53326},{\"end\":53329,\"start\":53328},{\"end\":53749,\"start\":53748},{\"end\":53760,\"start\":53759},{\"end\":53991,\"start\":53990},{\"end\":53999,\"start\":53998},{\"end\":54015,\"start\":54014},{\"end\":54220,\"start\":54219},{\"end\":54228,\"start\":54227},{\"end\":54234,\"start\":54233},{\"end\":54547,\"start\":54546},{\"end\":54549,\"start\":54548},{\"end\":54558,\"start\":54557},{\"end\":54570,\"start\":54569},{\"end\":54577,\"start\":54576},{\"end\":54579,\"start\":54578},{\"end\":54782,\"start\":54781},{\"end\":54792,\"start\":54791},{\"end\":54803,\"start\":54802},{\"end\":54816,\"start\":54815},{\"end\":54826,\"start\":54825},{\"end\":54837,\"start\":54836},{\"end\":54846,\"start\":54842},{\"end\":54850,\"start\":54849},{\"end\":55098,\"start\":55097},{\"end\":55108,\"start\":55107},{\"end\":55122,\"start\":55121},{\"end\":55133,\"start\":55132},{\"end\":55144,\"start\":55143},{\"end\":55146,\"start\":55145},{\"end\":55563,\"start\":55562},{\"end\":55576,\"start\":55575},{\"end\":55589,\"start\":55588},{\"end\":55601,\"start\":55600},{\"end\":55611,\"start\":55610},{\"end\":55622,\"start\":55621},{\"end\":55632,\"start\":55631},{\"end\":55643,\"start\":55642},{\"end\":55659,\"start\":55658},{\"end\":55668,\"start\":55667},{\"end\":55679,\"start\":55678},{\"end\":55693,\"start\":55692},{\"end\":55703,\"start\":55702},{\"end\":55717,\"start\":55716},{\"end\":55728,\"start\":55727},{\"end\":55738,\"start\":55737},{\"end\":56117,\"start\":56116},{\"end\":56119,\"start\":56118},{\"end\":56129,\"start\":56128},{\"end\":56140,\"start\":56139},{\"end\":56149,\"start\":56148},{\"end\":56160,\"start\":56159},{\"end\":56169,\"start\":56168},{\"end\":56176,\"start\":56175},{\"end\":56531,\"start\":56530},{\"end\":56542,\"start\":56541},{\"end\":56550,\"start\":56549},{\"end\":56828,\"start\":56827},{\"end\":56839,\"start\":56838},{\"end\":56853,\"start\":56852},{\"end\":56865,\"start\":56864},{\"end\":57061,\"start\":57060},{\"end\":57072,\"start\":57071},{\"end\":57078,\"start\":57077},{\"end\":57087,\"start\":57086},{\"end\":57095,\"start\":57094},{\"end\":57105,\"start\":57104},{\"end\":57295,\"start\":57294},{\"end\":57311,\"start\":57310},{\"end\":57321,\"start\":57320},{\"end\":57332,\"start\":57331},{\"end\":57341,\"start\":57340},{\"end\":57353,\"start\":57352},{\"end\":57642,\"start\":57641},{\"end\":57653,\"start\":57652},{\"end\":57662,\"start\":57661},{\"end\":57671,\"start\":57670},{\"end\":57682,\"start\":57681},{\"end\":57695,\"start\":57694},{\"end\":58049,\"start\":58048},{\"end\":58058,\"start\":58057},{\"end\":58067,\"start\":58066},{\"end\":58076,\"start\":58075},{\"end\":58083,\"start\":58082},{\"end\":58090,\"start\":58089},{\"end\":58092,\"start\":58091},{\"end\":58103,\"start\":58102},{\"end\":58114,\"start\":58108},{\"end\":58118,\"start\":58117},{\"end\":58454,\"start\":58453},{\"end\":58465,\"start\":58464},{\"end\":58476,\"start\":58475},{\"end\":58489,\"start\":58488},{\"end\":58970,\"start\":58969},{\"end\":58978,\"start\":58977},{\"end\":58994,\"start\":58990},{\"end\":59000,\"start\":58999},{\"end\":59009,\"start\":59008},{\"end\":59020,\"start\":59019},{\"end\":59022,\"start\":59021},{\"end\":59031,\"start\":59030},{\"end\":59042,\"start\":59041},{\"end\":59051,\"start\":59050},{\"end\":59328,\"start\":59327},{\"end\":59335,\"start\":59334},{\"end\":59337,\"start\":59336},{\"end\":59704,\"start\":59703},{\"end\":59717,\"start\":59716},{\"end\":60006,\"start\":60005},{\"end\":60014,\"start\":60013},{\"end\":60026,\"start\":60025},{\"end\":60356,\"start\":60355},{\"end\":60368,\"start\":60367},{\"end\":60378,\"start\":60377},{\"end\":60380,\"start\":60379},{\"end\":60616,\"start\":60615},{\"end\":60618,\"start\":60617},{\"end\":60630,\"start\":60626},{\"end\":60639,\"start\":60638},{\"end\":60641,\"start\":60640},{\"end\":60929,\"start\":60928},{\"end\":60937,\"start\":60936},{\"end\":60947,\"start\":60946},{\"end\":61230,\"start\":61229},{\"end\":61241,\"start\":61240},{\"end\":61252,\"start\":61251},{\"end\":61262,\"start\":61261},{\"end\":61275,\"start\":61274},{\"end\":61284,\"start\":61283},{\"end\":61286,\"start\":61285},{\"end\":61295,\"start\":61294},{\"end\":61305,\"start\":61304},{\"end\":61654,\"start\":61653},{\"end\":61665,\"start\":61664},{\"end\":61679,\"start\":61678},{\"end\":61692,\"start\":61688},{\"end\":62055,\"start\":62054},{\"end\":62062,\"start\":62061},{\"end\":62069,\"start\":62068},{\"end\":62080,\"start\":62076},{\"end\":62089,\"start\":62088},{\"end\":62091,\"start\":62090},{\"end\":62289,\"start\":62288},{\"end\":62302,\"start\":62301},{\"end\":62519,\"start\":62518},{\"end\":62521,\"start\":62520},{\"end\":62531,\"start\":62530},{\"end\":37423,\"start\":37422},{\"end\":37425,\"start\":37424},{\"end\":37431,\"start\":37430},{\"end\":37433,\"start\":37432},{\"end\":37442,\"start\":37441},{\"end\":37682,\"start\":37681},{\"end\":37693,\"start\":37692},{\"end\":37695,\"start\":37694},{\"end\":37704,\"start\":37703},{\"end\":38009,\"start\":38008},{\"end\":38018,\"start\":38017},{\"end\":38026,\"start\":38025},{\"end\":38037,\"start\":38036},{\"end\":38047,\"start\":38046},{\"end\":38049,\"start\":38048},{\"end\":38425,\"start\":38424},{\"end\":38438,\"start\":38437},{\"end\":38449,\"start\":38448},{\"end\":38463,\"start\":38462},{\"end\":38475,\"start\":38474},{\"end\":38485,\"start\":38484},{\"end\":38487,\"start\":38486},{\"end\":38852,\"start\":38851},{\"end\":38860,\"start\":38859},{\"end\":38873,\"start\":38872},{\"end\":38884,\"start\":38883},{\"end\":39142,\"start\":39141},{\"end\":39151,\"start\":39150},{\"end\":39159,\"start\":39158},{\"end\":39170,\"start\":39169},{\"end\":39429,\"start\":39428},{\"end\":39439,\"start\":39438},{\"end\":39445,\"start\":39444},{\"end\":39858,\"start\":39857},{\"end\":39867,\"start\":39866},{\"end\":39875,\"start\":39874},{\"end\":39883,\"start\":39882},{\"end\":39896,\"start\":39895},{\"end\":39898,\"start\":39897},{\"end\":40120,\"start\":40119},{\"end\":40122,\"start\":40121},{\"end\":40129,\"start\":40128},{\"end\":40131,\"start\":40130},{\"end\":40324,\"start\":40323},{\"end\":40337,\"start\":40333},{\"end\":40346,\"start\":40345},{\"end\":40353,\"start\":40352},{\"end\":40707,\"start\":40706},{\"end\":40715,\"start\":40714},{\"end\":40726,\"start\":40725},{\"end\":40961,\"start\":40960},{\"end\":40972,\"start\":40971},{\"end\":40981,\"start\":40980},{\"end\":40983,\"start\":40982},{\"end\":41364,\"start\":41363},{\"end\":41375,\"start\":41374},{\"end\":41596,\"start\":41595},{\"end\":41607,\"start\":41606},{\"end\":41627,\"start\":41620},{\"end\":41631,\"start\":41630},{\"end\":41900,\"start\":41899},{\"end\":41915,\"start\":41914},{\"end\":41926,\"start\":41925},{\"end\":41928,\"start\":41927},{\"end\":41944,\"start\":41943},{\"end\":41958,\"start\":41957},{\"end\":42346,\"start\":42345},{\"end\":42355,\"start\":42354},{\"end\":42365,\"start\":42364},{\"end\":42381,\"start\":42377},{\"end\":42393,\"start\":42392},{\"end\":42404,\"start\":42403},{\"end\":42727,\"start\":42726},{\"end\":42738,\"start\":42737},{\"end\":42747,\"start\":42746},{\"end\":43017,\"start\":43016},{\"end\":43027,\"start\":43026},{\"end\":43518,\"start\":43517},{\"end\":43520,\"start\":43519},{\"end\":43529,\"start\":43528},{\"end\":43536,\"start\":43535},{\"end\":43547,\"start\":43546},{\"end\":43549,\"start\":43548},{\"end\":43847,\"start\":43846},{\"end\":43861,\"start\":43860},{\"end\":43878,\"start\":43877},{\"end\":43887,\"start\":43886},{\"end\":43893,\"start\":43892},{\"end\":43909,\"start\":43908},{\"end\":43918,\"start\":43917},{\"end\":43931,\"start\":43930},{\"end\":44283,\"start\":44282},{\"end\":44292,\"start\":44291},{\"end\":44303,\"start\":44302},{\"end\":44312,\"start\":44311},{\"end\":44730,\"start\":44729},{\"end\":44740,\"start\":44739},{\"end\":45032,\"start\":45031},{\"end\":45038,\"start\":45037},{\"end\":45045,\"start\":45044},{\"end\":45051,\"start\":45050},{\"end\":45058,\"start\":45057},{\"end\":45260,\"start\":45259},{\"end\":45262,\"start\":45261},{\"end\":45272,\"start\":45271},{\"end\":45282,\"start\":45281},{\"end\":45293,\"start\":45292},{\"end\":45303,\"start\":45302},{\"end\":45305,\"start\":45304},{\"end\":45592,\"start\":45591},{\"end\":45603,\"start\":45602},{\"end\":45614,\"start\":45613},{\"end\":45621,\"start\":45620},{\"end\":45632,\"start\":45631},{\"end\":45642,\"start\":45641},{\"end\":45655,\"start\":45654},{\"end\":45666,\"start\":45665},{\"end\":45963,\"start\":45962},{\"end\":45965,\"start\":45964},{\"end\":45975,\"start\":45974},{\"end\":45990,\"start\":45986},{\"end\":46238,\"start\":46237},{\"end\":46240,\"start\":46239},{\"end\":46249,\"start\":46248},{\"end\":46260,\"start\":46259},{\"end\":46281,\"start\":46280},{\"end\":46291,\"start\":46290},{\"end\":46302,\"start\":46301},{\"end\":46315,\"start\":46314},{\"end\":46643,\"start\":46642},{\"end\":46653,\"start\":46652},{\"end\":46874,\"start\":46870},{\"end\":46883,\"start\":46882},{\"end\":46892,\"start\":46891},{\"end\":46894,\"start\":46893},{\"end\":46905,\"start\":46904},{\"end\":46918,\"start\":46917},{\"end\":47344,\"start\":47343},{\"end\":47353,\"start\":47352},{\"end\":47362,\"start\":47361},{\"end\":47371,\"start\":47370},{\"end\":47380,\"start\":47379},{\"end\":47388,\"start\":47387},{\"end\":47396,\"start\":47395},{\"end\":47403,\"start\":47402},{\"end\":47412,\"start\":47411},{\"end\":47414,\"start\":47413},{\"end\":47420,\"start\":47419},{\"end\":47836,\"start\":47835},{\"end\":47845,\"start\":47844},{\"end\":48123,\"start\":48122},{\"end\":48125,\"start\":48124},{\"end\":48131,\"start\":48130},{\"end\":48137,\"start\":48132},{\"end\":48146,\"start\":48145},{\"end\":48158,\"start\":48157},{\"end\":48167,\"start\":48166},{\"end\":48169,\"start\":48168},{\"end\":48178,\"start\":48177},{\"end\":48185,\"start\":48184},{\"end\":48545,\"start\":48544},{\"end\":48547,\"start\":48546},{\"end\":48557,\"start\":48556},{\"end\":48824,\"start\":48823},{\"end\":48826,\"start\":48825},{\"end\":48836,\"start\":48835},{\"end\":49032,\"start\":49031},{\"end\":49034,\"start\":49033},{\"end\":49044,\"start\":49043},{\"end\":49055,\"start\":49054},{\"end\":49057,\"start\":49056},{\"end\":49068,\"start\":49067},{\"end\":49361,\"start\":49360},{\"end\":49371,\"start\":49370},{\"end\":49381,\"start\":49380},{\"end\":49599,\"start\":49598},{\"end\":49613,\"start\":49612},{\"end\":49621,\"start\":49620},{\"end\":50040,\"start\":50039},{\"end\":50054,\"start\":50053},{\"end\":50067,\"start\":50066},{\"end\":50069,\"start\":50068},{\"end\":50370,\"start\":50369},{\"end\":50384,\"start\":50383},{\"end\":50832,\"start\":50831},{\"end\":50834,\"start\":50833},{\"end\":50846,\"start\":50845},{\"end\":50848,\"start\":50847},{\"end\":50858,\"start\":50857},{\"end\":50860,\"start\":50859},{\"end\":51303,\"start\":51302},{\"end\":51310,\"start\":51309},{\"end\":51320,\"start\":51319},{\"end\":51333,\"start\":51332},{\"end\":51335,\"start\":51334},{\"end\":51786,\"start\":51785},{\"end\":51793,\"start\":51792},{\"end\":51795,\"start\":51794},{\"end\":51807,\"start\":51806},{\"end\":51813,\"start\":51812},{\"end\":51820,\"start\":51819},{\"end\":51827,\"start\":51826},{\"end\":51837,\"start\":51836},{\"end\":51844,\"start\":51843},{\"end\":51855,\"start\":51854},{\"end\":51862,\"start\":51861},{\"end\":52208,\"start\":52207},{\"end\":52218,\"start\":52217},{\"end\":52508,\"start\":52507},{\"end\":52519,\"start\":52518},{\"end\":52532,\"start\":52531},{\"end\":52540,\"start\":52539},{\"end\":52542,\"start\":52541},{\"end\":52553,\"start\":52552},{\"end\":52897,\"start\":52896},{\"end\":52906,\"start\":52905},{\"end\":53136,\"start\":53135},{\"end\":53147,\"start\":53146},{\"end\":53155,\"start\":53154},{\"end\":53319,\"start\":53318},{\"end\":53327,\"start\":53326},{\"end\":53329,\"start\":53328},{\"end\":53749,\"start\":53748},{\"end\":53760,\"start\":53759},{\"end\":53991,\"start\":53990},{\"end\":53999,\"start\":53998},{\"end\":54015,\"start\":54014},{\"end\":54220,\"start\":54219},{\"end\":54228,\"start\":54227},{\"end\":54234,\"start\":54233},{\"end\":54547,\"start\":54546},{\"end\":54549,\"start\":54548},{\"end\":54558,\"start\":54557},{\"end\":54570,\"start\":54569},{\"end\":54577,\"start\":54576},{\"end\":54579,\"start\":54578},{\"end\":54782,\"start\":54781},{\"end\":54792,\"start\":54791},{\"end\":54803,\"start\":54802},{\"end\":54816,\"start\":54815},{\"end\":54826,\"start\":54825},{\"end\":54837,\"start\":54836},{\"end\":54846,\"start\":54842},{\"end\":54850,\"start\":54849},{\"end\":55098,\"start\":55097},{\"end\":55108,\"start\":55107},{\"end\":55122,\"start\":55121},{\"end\":55133,\"start\":55132},{\"end\":55144,\"start\":55143},{\"end\":55146,\"start\":55145},{\"end\":55563,\"start\":55562},{\"end\":55576,\"start\":55575},{\"end\":55589,\"start\":55588},{\"end\":55601,\"start\":55600},{\"end\":55611,\"start\":55610},{\"end\":55622,\"start\":55621},{\"end\":55632,\"start\":55631},{\"end\":55643,\"start\":55642},{\"end\":55659,\"start\":55658},{\"end\":55668,\"start\":55667},{\"end\":55679,\"start\":55678},{\"end\":55693,\"start\":55692},{\"end\":55703,\"start\":55702},{\"end\":55717,\"start\":55716},{\"end\":55728,\"start\":55727},{\"end\":55738,\"start\":55737},{\"end\":56117,\"start\":56116},{\"end\":56119,\"start\":56118},{\"end\":56129,\"start\":56128},{\"end\":56140,\"start\":56139},{\"end\":56149,\"start\":56148},{\"end\":56160,\"start\":56159},{\"end\":56169,\"start\":56168},{\"end\":56176,\"start\":56175},{\"end\":56531,\"start\":56530},{\"end\":56542,\"start\":56541},{\"end\":56550,\"start\":56549},{\"end\":56828,\"start\":56827},{\"end\":56839,\"start\":56838},{\"end\":56853,\"start\":56852},{\"end\":56865,\"start\":56864},{\"end\":57061,\"start\":57060},{\"end\":57072,\"start\":57071},{\"end\":57078,\"start\":57077},{\"end\":57087,\"start\":57086},{\"end\":57095,\"start\":57094},{\"end\":57105,\"start\":57104},{\"end\":57295,\"start\":57294},{\"end\":57311,\"start\":57310},{\"end\":57321,\"start\":57320},{\"end\":57332,\"start\":57331},{\"end\":57341,\"start\":57340},{\"end\":57353,\"start\":57352},{\"end\":57642,\"start\":57641},{\"end\":57653,\"start\":57652},{\"end\":57662,\"start\":57661},{\"end\":57671,\"start\":57670},{\"end\":57682,\"start\":57681},{\"end\":57695,\"start\":57694},{\"end\":58049,\"start\":58048},{\"end\":58058,\"start\":58057},{\"end\":58067,\"start\":58066},{\"end\":58076,\"start\":58075},{\"end\":58083,\"start\":58082},{\"end\":58090,\"start\":58089},{\"end\":58092,\"start\":58091},{\"end\":58103,\"start\":58102},{\"end\":58114,\"start\":58108},{\"end\":58118,\"start\":58117},{\"end\":58454,\"start\":58453},{\"end\":58465,\"start\":58464},{\"end\":58476,\"start\":58475},{\"end\":58489,\"start\":58488},{\"end\":58970,\"start\":58969},{\"end\":58978,\"start\":58977},{\"end\":58994,\"start\":58990},{\"end\":59000,\"start\":58999},{\"end\":59009,\"start\":59008},{\"end\":59020,\"start\":59019},{\"end\":59022,\"start\":59021},{\"end\":59031,\"start\":59030},{\"end\":59042,\"start\":59041},{\"end\":59051,\"start\":59050},{\"end\":59328,\"start\":59327},{\"end\":59335,\"start\":59334},{\"end\":59337,\"start\":59336},{\"end\":59704,\"start\":59703},{\"end\":59717,\"start\":59716},{\"end\":60006,\"start\":60005},{\"end\":60014,\"start\":60013},{\"end\":60026,\"start\":60025},{\"end\":60356,\"start\":60355},{\"end\":60368,\"start\":60367},{\"end\":60378,\"start\":60377},{\"end\":60380,\"start\":60379},{\"end\":60616,\"start\":60615},{\"end\":60618,\"start\":60617},{\"end\":60630,\"start\":60626},{\"end\":60639,\"start\":60638},{\"end\":60641,\"start\":60640},{\"end\":60929,\"start\":60928},{\"end\":60937,\"start\":60936},{\"end\":60947,\"start\":60946},{\"end\":61230,\"start\":61229},{\"end\":61241,\"start\":61240},{\"end\":61252,\"start\":61251},{\"end\":61262,\"start\":61261},{\"end\":61275,\"start\":61274},{\"end\":61284,\"start\":61283},{\"end\":61286,\"start\":61285},{\"end\":61295,\"start\":61294},{\"end\":61305,\"start\":61304},{\"end\":61654,\"start\":61653},{\"end\":61665,\"start\":61664},{\"end\":61679,\"start\":61678},{\"end\":61692,\"start\":61688},{\"end\":62055,\"start\":62054},{\"end\":62062,\"start\":62061},{\"end\":62069,\"start\":62068},{\"end\":62080,\"start\":62076},{\"end\":62089,\"start\":62088},{\"end\":62091,\"start\":62090},{\"end\":62289,\"start\":62288},{\"end\":62302,\"start\":62301},{\"end\":62519,\"start\":62518},{\"end\":62521,\"start\":62520},{\"end\":62531,\"start\":62530}]", "bib_author_last_name": "[{\"end\":37428,\"start\":37426},{\"end\":37439,\"start\":37434},{\"end\":37449,\"start\":37443},{\"end\":37690,\"start\":37683},{\"end\":37701,\"start\":37696},{\"end\":37715,\"start\":37705},{\"end\":38015,\"start\":38010},{\"end\":38023,\"start\":38019},{\"end\":38034,\"start\":38027},{\"end\":38044,\"start\":38038},{\"end\":38052,\"start\":38050},{\"end\":38435,\"start\":38426},{\"end\":38446,\"start\":38439},{\"end\":38460,\"start\":38450},{\"end\":38472,\"start\":38464},{\"end\":38482,\"start\":38476},{\"end\":38494,\"start\":38488},{\"end\":38504,\"start\":38496},{\"end\":38857,\"start\":38853},{\"end\":38870,\"start\":38861},{\"end\":38881,\"start\":38874},{\"end\":38891,\"start\":38885},{\"end\":39148,\"start\":39143},{\"end\":39156,\"start\":39152},{\"end\":39167,\"start\":39160},{\"end\":39180,\"start\":39171},{\"end\":39436,\"start\":39430},{\"end\":39442,\"start\":39440},{\"end\":39449,\"start\":39446},{\"end\":39864,\"start\":39859},{\"end\":39872,\"start\":39868},{\"end\":39880,\"start\":39876},{\"end\":39893,\"start\":39884},{\"end\":39901,\"start\":39899},{\"end\":39914,\"start\":39903},{\"end\":40126,\"start\":40123},{\"end\":40134,\"start\":40132},{\"end\":40331,\"start\":40325},{\"end\":40343,\"start\":40338},{\"end\":40350,\"start\":40347},{\"end\":40363,\"start\":40354},{\"end\":40369,\"start\":40365},{\"end\":40712,\"start\":40708},{\"end\":40723,\"start\":40716},{\"end\":40733,\"start\":40727},{\"end\":40969,\"start\":40962},{\"end\":40978,\"start\":40973},{\"end\":40989,\"start\":40984},{\"end\":41372,\"start\":41365},{\"end\":41384,\"start\":41376},{\"end\":41604,\"start\":41597},{\"end\":41618,\"start\":41608},{\"end\":41912,\"start\":41901},{\"end\":41923,\"start\":41916},{\"end\":41941,\"start\":41929},{\"end\":41955,\"start\":41945},{\"end\":41963,\"start\":41959},{\"end\":42352,\"start\":42347},{\"end\":42362,\"start\":42356},{\"end\":42375,\"start\":42366},{\"end\":42390,\"start\":42382},{\"end\":42401,\"start\":42394},{\"end\":42411,\"start\":42405},{\"end\":42735,\"start\":42728},{\"end\":42744,\"start\":42739},{\"end\":42757,\"start\":42748},{\"end\":43024,\"start\":43018},{\"end\":43034,\"start\":43028},{\"end\":43526,\"start\":43521},{\"end\":43533,\"start\":43530},{\"end\":43544,\"start\":43537},{\"end\":43556,\"start\":43550},{\"end\":43858,\"start\":43848},{\"end\":43875,\"start\":43862},{\"end\":43884,\"start\":43879},{\"end\":43890,\"start\":43888},{\"end\":43906,\"start\":43894},{\"end\":43915,\"start\":43910},{\"end\":43928,\"start\":43919},{\"end\":43938,\"start\":43932},{\"end\":44289,\"start\":44284},{\"end\":44300,\"start\":44293},{\"end\":44309,\"start\":44304},{\"end\":44318,\"start\":44313},{\"end\":44737,\"start\":44731},{\"end\":44747,\"start\":44741},{\"end\":45035,\"start\":45033},{\"end\":45042,\"start\":45039},{\"end\":45048,\"start\":45046},{\"end\":45055,\"start\":45052},{\"end\":45067,\"start\":45059},{\"end\":45269,\"start\":45263},{\"end\":45279,\"start\":45273},{\"end\":45290,\"start\":45283},{\"end\":45300,\"start\":45294},{\"end\":45310,\"start\":45306},{\"end\":45600,\"start\":45593},{\"end\":45611,\"start\":45604},{\"end\":45618,\"start\":45615},{\"end\":45629,\"start\":45622},{\"end\":45639,\"start\":45633},{\"end\":45652,\"start\":45643},{\"end\":45663,\"start\":45656},{\"end\":45675,\"start\":45667},{\"end\":45684,\"start\":45677},{\"end\":45972,\"start\":45966},{\"end\":45984,\"start\":45976},{\"end\":45994,\"start\":45991},{\"end\":46246,\"start\":46241},{\"end\":46257,\"start\":46250},{\"end\":46278,\"start\":46261},{\"end\":46288,\"start\":46282},{\"end\":46299,\"start\":46292},{\"end\":46312,\"start\":46303},{\"end\":46322,\"start\":46316},{\"end\":46650,\"start\":46644},{\"end\":46659,\"start\":46654},{\"end\":46880,\"start\":46875},{\"end\":46889,\"start\":46884},{\"end\":46902,\"start\":46895},{\"end\":46915,\"start\":46906},{\"end\":46930,\"start\":46919},{\"end\":47350,\"start\":47345},{\"end\":47359,\"start\":47354},{\"end\":47368,\"start\":47363},{\"end\":47377,\"start\":47372},{\"end\":47385,\"start\":47381},{\"end\":47393,\"start\":47389},{\"end\":47400,\"start\":47397},{\"end\":47409,\"start\":47404},{\"end\":47417,\"start\":47415},{\"end\":47423,\"start\":47421},{\"end\":47842,\"start\":47837},{\"end\":47853,\"start\":47846},{\"end\":48128,\"start\":48126},{\"end\":48143,\"start\":48138},{\"end\":48155,\"start\":48147},{\"end\":48164,\"start\":48159},{\"end\":48175,\"start\":48170},{\"end\":48182,\"start\":48179},{\"end\":48192,\"start\":48186},{\"end\":48554,\"start\":48548},{\"end\":48566,\"start\":48558},{\"end\":48572,\"start\":48568},{\"end\":48833,\"start\":48827},{\"end\":48844,\"start\":48837},{\"end\":49041,\"start\":49035},{\"end\":49052,\"start\":49045},{\"end\":49065,\"start\":49058},{\"end\":49076,\"start\":49069},{\"end\":49368,\"start\":49362},{\"end\":49378,\"start\":49372},{\"end\":49390,\"start\":49382},{\"end\":49610,\"start\":49600},{\"end\":49618,\"start\":49614},{\"end\":49627,\"start\":49622},{\"end\":50051,\"start\":50041},{\"end\":50064,\"start\":50055},{\"end\":50076,\"start\":50070},{\"end\":50381,\"start\":50371},{\"end\":50391,\"start\":50385},{\"end\":50843,\"start\":50835},{\"end\":50855,\"start\":50849},{\"end\":50866,\"start\":50861},{\"end\":51307,\"start\":51304},{\"end\":51317,\"start\":51311},{\"end\":51330,\"start\":51321},{\"end\":51338,\"start\":51336},{\"end\":51790,\"start\":51787},{\"end\":51804,\"start\":51796},{\"end\":51810,\"start\":51808},{\"end\":51817,\"start\":51814},{\"end\":51824,\"start\":51821},{\"end\":51834,\"start\":51828},{\"end\":51841,\"start\":51838},{\"end\":51852,\"start\":51845},{\"end\":51859,\"start\":51856},{\"end\":51872,\"start\":51863},{\"end\":52215,\"start\":52209},{\"end\":52231,\"start\":52219},{\"end\":52516,\"start\":52509},{\"end\":52529,\"start\":52520},{\"end\":52537,\"start\":52533},{\"end\":52550,\"start\":52543},{\"end\":52558,\"start\":52554},{\"end\":52903,\"start\":52898},{\"end\":52921,\"start\":52907},{\"end\":53144,\"start\":53137},{\"end\":53152,\"start\":53148},{\"end\":53162,\"start\":53156},{\"end\":53324,\"start\":53320},{\"end\":53336,\"start\":53330},{\"end\":53757,\"start\":53750},{\"end\":53767,\"start\":53761},{\"end\":53996,\"start\":53992},{\"end\":54012,\"start\":54000},{\"end\":54027,\"start\":54016},{\"end\":54225,\"start\":54221},{\"end\":54231,\"start\":54229},{\"end\":54242,\"start\":54235},{\"end\":54555,\"start\":54550},{\"end\":54567,\"start\":54559},{\"end\":54574,\"start\":54571},{\"end\":54585,\"start\":54580},{\"end\":54789,\"start\":54783},{\"end\":54800,\"start\":54793},{\"end\":54813,\"start\":54804},{\"end\":54823,\"start\":54817},{\"end\":54834,\"start\":54827},{\"end\":54840,\"start\":54838},{\"end\":55105,\"start\":55099},{\"end\":55119,\"start\":55109},{\"end\":55130,\"start\":55123},{\"end\":55141,\"start\":55134},{\"end\":55152,\"start\":55147},{\"end\":55573,\"start\":55564},{\"end\":55586,\"start\":55577},{\"end\":55598,\"start\":55590},{\"end\":55608,\"start\":55602},{\"end\":55619,\"start\":55612},{\"end\":55629,\"start\":55623},{\"end\":55640,\"start\":55633},{\"end\":55656,\"start\":55644},{\"end\":55665,\"start\":55660},{\"end\":55676,\"start\":55669},{\"end\":55690,\"start\":55680},{\"end\":55700,\"start\":55694},{\"end\":55714,\"start\":55704},{\"end\":55725,\"start\":55718},{\"end\":55735,\"start\":55729},{\"end\":55748,\"start\":55739},{\"end\":56126,\"start\":56120},{\"end\":56137,\"start\":56130},{\"end\":56146,\"start\":56141},{\"end\":56157,\"start\":56150},{\"end\":56166,\"start\":56161},{\"end\":56173,\"start\":56170},{\"end\":56188,\"start\":56177},{\"end\":56539,\"start\":56532},{\"end\":56547,\"start\":56543},{\"end\":56559,\"start\":56551},{\"end\":56836,\"start\":56829},{\"end\":56850,\"start\":56840},{\"end\":56862,\"start\":56854},{\"end\":56875,\"start\":56866},{\"end\":57069,\"start\":57062},{\"end\":57075,\"start\":57073},{\"end\":57084,\"start\":57079},{\"end\":57092,\"start\":57088},{\"end\":57102,\"start\":57096},{\"end\":57115,\"start\":57106},{\"end\":57308,\"start\":57296},{\"end\":57318,\"start\":57312},{\"end\":57329,\"start\":57322},{\"end\":57338,\"start\":57333},{\"end\":57350,\"start\":57342},{\"end\":57360,\"start\":57354},{\"end\":57650,\"start\":57643},{\"end\":57659,\"start\":57654},{\"end\":57668,\"start\":57663},{\"end\":57679,\"start\":57672},{\"end\":57692,\"start\":57683},{\"end\":57702,\"start\":57696},{\"end\":58055,\"start\":58050},{\"end\":58064,\"start\":58059},{\"end\":58073,\"start\":58068},{\"end\":58080,\"start\":58077},{\"end\":58087,\"start\":58084},{\"end\":58100,\"start\":58093},{\"end\":58106,\"start\":58104},{\"end\":58462,\"start\":58455},{\"end\":58473,\"start\":58466},{\"end\":58486,\"start\":58477},{\"end\":58496,\"start\":58490},{\"end\":58975,\"start\":58971},{\"end\":58988,\"start\":58979},{\"end\":58997,\"start\":58995},{\"end\":59006,\"start\":59001},{\"end\":59017,\"start\":59010},{\"end\":59028,\"start\":59023},{\"end\":59039,\"start\":59032},{\"end\":59048,\"start\":59043},{\"end\":59058,\"start\":59052},{\"end\":59332,\"start\":59329},{\"end\":59340,\"start\":59338},{\"end\":59354,\"start\":59342},{\"end\":59714,\"start\":59705},{\"end\":59725,\"start\":59718},{\"end\":60011,\"start\":60007},{\"end\":60023,\"start\":60015},{\"end\":60032,\"start\":60027},{\"end\":60365,\"start\":60357},{\"end\":60375,\"start\":60369},{\"end\":60388,\"start\":60381},{\"end\":60624,\"start\":60619},{\"end\":60636,\"start\":60631},{\"end\":60644,\"start\":60642},{\"end\":60652,\"start\":60646},{\"end\":60934,\"start\":60930},{\"end\":60944,\"start\":60938},{\"end\":60958,\"start\":60948},{\"end\":60965,\"start\":60960},{\"end\":61238,\"start\":61231},{\"end\":61249,\"start\":61242},{\"end\":61259,\"start\":61253},{\"end\":61272,\"start\":61263},{\"end\":61281,\"start\":61276},{\"end\":61292,\"start\":61287},{\"end\":61302,\"start\":61296},{\"end\":61316,\"start\":61306},{\"end\":61662,\"start\":61655},{\"end\":61676,\"start\":61666},{\"end\":61686,\"start\":61680},{\"end\":61701,\"start\":61693},{\"end\":62059,\"start\":62056},{\"end\":62066,\"start\":62063},{\"end\":62074,\"start\":62070},{\"end\":62086,\"start\":62081},{\"end\":62094,\"start\":62092},{\"end\":62299,\"start\":62290},{\"end\":62312,\"start\":62303},{\"end\":62528,\"start\":62522},{\"end\":62538,\"start\":62532},{\"end\":37428,\"start\":37426},{\"end\":37439,\"start\":37434},{\"end\":37449,\"start\":37443},{\"end\":37690,\"start\":37683},{\"end\":37701,\"start\":37696},{\"end\":37715,\"start\":37705},{\"end\":38015,\"start\":38010},{\"end\":38023,\"start\":38019},{\"end\":38034,\"start\":38027},{\"end\":38044,\"start\":38038},{\"end\":38052,\"start\":38050},{\"end\":38435,\"start\":38426},{\"end\":38446,\"start\":38439},{\"end\":38460,\"start\":38450},{\"end\":38472,\"start\":38464},{\"end\":38482,\"start\":38476},{\"end\":38494,\"start\":38488},{\"end\":38504,\"start\":38496},{\"end\":38857,\"start\":38853},{\"end\":38870,\"start\":38861},{\"end\":38881,\"start\":38874},{\"end\":38891,\"start\":38885},{\"end\":39148,\"start\":39143},{\"end\":39156,\"start\":39152},{\"end\":39167,\"start\":39160},{\"end\":39180,\"start\":39171},{\"end\":39436,\"start\":39430},{\"end\":39442,\"start\":39440},{\"end\":39449,\"start\":39446},{\"end\":39864,\"start\":39859},{\"end\":39872,\"start\":39868},{\"end\":39880,\"start\":39876},{\"end\":39893,\"start\":39884},{\"end\":39901,\"start\":39899},{\"end\":39914,\"start\":39903},{\"end\":40126,\"start\":40123},{\"end\":40134,\"start\":40132},{\"end\":40331,\"start\":40325},{\"end\":40343,\"start\":40338},{\"end\":40350,\"start\":40347},{\"end\":40363,\"start\":40354},{\"end\":40369,\"start\":40365},{\"end\":40712,\"start\":40708},{\"end\":40723,\"start\":40716},{\"end\":40733,\"start\":40727},{\"end\":40969,\"start\":40962},{\"end\":40978,\"start\":40973},{\"end\":40989,\"start\":40984},{\"end\":41372,\"start\":41365},{\"end\":41384,\"start\":41376},{\"end\":41604,\"start\":41597},{\"end\":41618,\"start\":41608},{\"end\":41912,\"start\":41901},{\"end\":41923,\"start\":41916},{\"end\":41941,\"start\":41929},{\"end\":41955,\"start\":41945},{\"end\":41963,\"start\":41959},{\"end\":42352,\"start\":42347},{\"end\":42362,\"start\":42356},{\"end\":42375,\"start\":42366},{\"end\":42390,\"start\":42382},{\"end\":42401,\"start\":42394},{\"end\":42411,\"start\":42405},{\"end\":42735,\"start\":42728},{\"end\":42744,\"start\":42739},{\"end\":42757,\"start\":42748},{\"end\":43024,\"start\":43018},{\"end\":43034,\"start\":43028},{\"end\":43526,\"start\":43521},{\"end\":43533,\"start\":43530},{\"end\":43544,\"start\":43537},{\"end\":43556,\"start\":43550},{\"end\":43858,\"start\":43848},{\"end\":43875,\"start\":43862},{\"end\":43884,\"start\":43879},{\"end\":43890,\"start\":43888},{\"end\":43906,\"start\":43894},{\"end\":43915,\"start\":43910},{\"end\":43928,\"start\":43919},{\"end\":43938,\"start\":43932},{\"end\":44289,\"start\":44284},{\"end\":44300,\"start\":44293},{\"end\":44309,\"start\":44304},{\"end\":44318,\"start\":44313},{\"end\":44737,\"start\":44731},{\"end\":44747,\"start\":44741},{\"end\":45035,\"start\":45033},{\"end\":45042,\"start\":45039},{\"end\":45048,\"start\":45046},{\"end\":45055,\"start\":45052},{\"end\":45067,\"start\":45059},{\"end\":45269,\"start\":45263},{\"end\":45279,\"start\":45273},{\"end\":45290,\"start\":45283},{\"end\":45300,\"start\":45294},{\"end\":45310,\"start\":45306},{\"end\":45600,\"start\":45593},{\"end\":45611,\"start\":45604},{\"end\":45618,\"start\":45615},{\"end\":45629,\"start\":45622},{\"end\":45639,\"start\":45633},{\"end\":45652,\"start\":45643},{\"end\":45663,\"start\":45656},{\"end\":45675,\"start\":45667},{\"end\":45684,\"start\":45677},{\"end\":45972,\"start\":45966},{\"end\":45984,\"start\":45976},{\"end\":45994,\"start\":45991},{\"end\":46246,\"start\":46241},{\"end\":46257,\"start\":46250},{\"end\":46278,\"start\":46261},{\"end\":46288,\"start\":46282},{\"end\":46299,\"start\":46292},{\"end\":46312,\"start\":46303},{\"end\":46322,\"start\":46316},{\"end\":46650,\"start\":46644},{\"end\":46659,\"start\":46654},{\"end\":46880,\"start\":46875},{\"end\":46889,\"start\":46884},{\"end\":46902,\"start\":46895},{\"end\":46915,\"start\":46906},{\"end\":46930,\"start\":46919},{\"end\":47350,\"start\":47345},{\"end\":47359,\"start\":47354},{\"end\":47368,\"start\":47363},{\"end\":47377,\"start\":47372},{\"end\":47385,\"start\":47381},{\"end\":47393,\"start\":47389},{\"end\":47400,\"start\":47397},{\"end\":47409,\"start\":47404},{\"end\":47417,\"start\":47415},{\"end\":47423,\"start\":47421},{\"end\":47842,\"start\":47837},{\"end\":47853,\"start\":47846},{\"end\":48128,\"start\":48126},{\"end\":48143,\"start\":48138},{\"end\":48155,\"start\":48147},{\"end\":48164,\"start\":48159},{\"end\":48175,\"start\":48170},{\"end\":48182,\"start\":48179},{\"end\":48192,\"start\":48186},{\"end\":48554,\"start\":48548},{\"end\":48566,\"start\":48558},{\"end\":48572,\"start\":48568},{\"end\":48833,\"start\":48827},{\"end\":48844,\"start\":48837},{\"end\":49041,\"start\":49035},{\"end\":49052,\"start\":49045},{\"end\":49065,\"start\":49058},{\"end\":49076,\"start\":49069},{\"end\":49368,\"start\":49362},{\"end\":49378,\"start\":49372},{\"end\":49390,\"start\":49382},{\"end\":49610,\"start\":49600},{\"end\":49618,\"start\":49614},{\"end\":49627,\"start\":49622},{\"end\":50051,\"start\":50041},{\"end\":50064,\"start\":50055},{\"end\":50076,\"start\":50070},{\"end\":50381,\"start\":50371},{\"end\":50391,\"start\":50385},{\"end\":50843,\"start\":50835},{\"end\":50855,\"start\":50849},{\"end\":50866,\"start\":50861},{\"end\":51307,\"start\":51304},{\"end\":51317,\"start\":51311},{\"end\":51330,\"start\":51321},{\"end\":51338,\"start\":51336},{\"end\":51790,\"start\":51787},{\"end\":51804,\"start\":51796},{\"end\":51810,\"start\":51808},{\"end\":51817,\"start\":51814},{\"end\":51824,\"start\":51821},{\"end\":51834,\"start\":51828},{\"end\":51841,\"start\":51838},{\"end\":51852,\"start\":51845},{\"end\":51859,\"start\":51856},{\"end\":51872,\"start\":51863},{\"end\":52215,\"start\":52209},{\"end\":52231,\"start\":52219},{\"end\":52516,\"start\":52509},{\"end\":52529,\"start\":52520},{\"end\":52537,\"start\":52533},{\"end\":52550,\"start\":52543},{\"end\":52558,\"start\":52554},{\"end\":52903,\"start\":52898},{\"end\":52921,\"start\":52907},{\"end\":53144,\"start\":53137},{\"end\":53152,\"start\":53148},{\"end\":53162,\"start\":53156},{\"end\":53324,\"start\":53320},{\"end\":53336,\"start\":53330},{\"end\":53757,\"start\":53750},{\"end\":53767,\"start\":53761},{\"end\":53996,\"start\":53992},{\"end\":54012,\"start\":54000},{\"end\":54027,\"start\":54016},{\"end\":54225,\"start\":54221},{\"end\":54231,\"start\":54229},{\"end\":54242,\"start\":54235},{\"end\":54555,\"start\":54550},{\"end\":54567,\"start\":54559},{\"end\":54574,\"start\":54571},{\"end\":54585,\"start\":54580},{\"end\":54789,\"start\":54783},{\"end\":54800,\"start\":54793},{\"end\":54813,\"start\":54804},{\"end\":54823,\"start\":54817},{\"end\":54834,\"start\":54827},{\"end\":54840,\"start\":54838},{\"end\":55105,\"start\":55099},{\"end\":55119,\"start\":55109},{\"end\":55130,\"start\":55123},{\"end\":55141,\"start\":55134},{\"end\":55152,\"start\":55147},{\"end\":55573,\"start\":55564},{\"end\":55586,\"start\":55577},{\"end\":55598,\"start\":55590},{\"end\":55608,\"start\":55602},{\"end\":55619,\"start\":55612},{\"end\":55629,\"start\":55623},{\"end\":55640,\"start\":55633},{\"end\":55656,\"start\":55644},{\"end\":55665,\"start\":55660},{\"end\":55676,\"start\":55669},{\"end\":55690,\"start\":55680},{\"end\":55700,\"start\":55694},{\"end\":55714,\"start\":55704},{\"end\":55725,\"start\":55718},{\"end\":55735,\"start\":55729},{\"end\":55748,\"start\":55739},{\"end\":56126,\"start\":56120},{\"end\":56137,\"start\":56130},{\"end\":56146,\"start\":56141},{\"end\":56157,\"start\":56150},{\"end\":56166,\"start\":56161},{\"end\":56173,\"start\":56170},{\"end\":56188,\"start\":56177},{\"end\":56539,\"start\":56532},{\"end\":56547,\"start\":56543},{\"end\":56559,\"start\":56551},{\"end\":56836,\"start\":56829},{\"end\":56850,\"start\":56840},{\"end\":56862,\"start\":56854},{\"end\":56875,\"start\":56866},{\"end\":57069,\"start\":57062},{\"end\":57075,\"start\":57073},{\"end\":57084,\"start\":57079},{\"end\":57092,\"start\":57088},{\"end\":57102,\"start\":57096},{\"end\":57115,\"start\":57106},{\"end\":57308,\"start\":57296},{\"end\":57318,\"start\":57312},{\"end\":57329,\"start\":57322},{\"end\":57338,\"start\":57333},{\"end\":57350,\"start\":57342},{\"end\":57360,\"start\":57354},{\"end\":57650,\"start\":57643},{\"end\":57659,\"start\":57654},{\"end\":57668,\"start\":57663},{\"end\":57679,\"start\":57672},{\"end\":57692,\"start\":57683},{\"end\":57702,\"start\":57696},{\"end\":58055,\"start\":58050},{\"end\":58064,\"start\":58059},{\"end\":58073,\"start\":58068},{\"end\":58080,\"start\":58077},{\"end\":58087,\"start\":58084},{\"end\":58100,\"start\":58093},{\"end\":58106,\"start\":58104},{\"end\":58462,\"start\":58455},{\"end\":58473,\"start\":58466},{\"end\":58486,\"start\":58477},{\"end\":58496,\"start\":58490},{\"end\":58975,\"start\":58971},{\"end\":58988,\"start\":58979},{\"end\":58997,\"start\":58995},{\"end\":59006,\"start\":59001},{\"end\":59017,\"start\":59010},{\"end\":59028,\"start\":59023},{\"end\":59039,\"start\":59032},{\"end\":59048,\"start\":59043},{\"end\":59058,\"start\":59052},{\"end\":59332,\"start\":59329},{\"end\":59340,\"start\":59338},{\"end\":59354,\"start\":59342},{\"end\":59714,\"start\":59705},{\"end\":59725,\"start\":59718},{\"end\":60011,\"start\":60007},{\"end\":60023,\"start\":60015},{\"end\":60032,\"start\":60027},{\"end\":60365,\"start\":60357},{\"end\":60375,\"start\":60369},{\"end\":60388,\"start\":60381},{\"end\":60624,\"start\":60619},{\"end\":60636,\"start\":60631},{\"end\":60644,\"start\":60642},{\"end\":60652,\"start\":60646},{\"end\":60934,\"start\":60930},{\"end\":60944,\"start\":60938},{\"end\":60958,\"start\":60948},{\"end\":60965,\"start\":60960},{\"end\":61238,\"start\":61231},{\"end\":61249,\"start\":61242},{\"end\":61259,\"start\":61253},{\"end\":61272,\"start\":61263},{\"end\":61281,\"start\":61276},{\"end\":61292,\"start\":61287},{\"end\":61302,\"start\":61296},{\"end\":61316,\"start\":61306},{\"end\":61662,\"start\":61655},{\"end\":61676,\"start\":61666},{\"end\":61686,\"start\":61680},{\"end\":61701,\"start\":61693},{\"end\":62059,\"start\":62056},{\"end\":62066,\"start\":62063},{\"end\":62074,\"start\":62070},{\"end\":62086,\"start\":62081},{\"end\":62094,\"start\":62092},{\"end\":62299,\"start\":62290},{\"end\":62312,\"start\":62303},{\"end\":62528,\"start\":62522},{\"end\":62538,\"start\":62532}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b0\"},\"end\":37607,\"start\":37420},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":173990164},\"end\":37962,\"start\":37609},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":127951164},\"end\":38373,\"start\":37964},{\"attributes\":{\"id\":\"b3\"},\"end\":38778,\"start\":38375},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b4\"},\"end\":39087,\"start\":38780},{\"attributes\":{\"doi\":\"arXiv:1904.10509\",\"id\":\"b5\"},\"end\":39356,\"start\":39089},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":308212},\"end\":39855,\"start\":39358},{\"attributes\":{\"id\":\"b7\"},\"end\":40082,\"start\":39857},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7138078},\"end\":40321,\"start\":40084},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b9\"},\"end\":40652,\"start\":40323},{\"attributes\":{\"doi\":\"arXiv:1410.8516\",\"id\":\"b10\"},\"end\":40891,\"start\":40654},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9062671},\"end\":41312,\"start\":40893},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":195820291},\"end\":41593,\"start\":41314},{\"attributes\":{\"doi\":\"arXiv:1605.09782\",\"id\":\"b13\"},\"end\":41807,\"start\":41595},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":12881926},\"end\":42289,\"start\":41809},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15796526},\"end\":42656,\"start\":42291},{\"attributes\":{\"doi\":\"arXiv:1803.07728\",\"id\":\"b16\"},\"end\":42939,\"start\":42658},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5575601},\"end\":43437,\"start\":42941},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":8869447},\"end\":43815,\"start\":43439},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1033682},\"end\":44207,\"start\":43817},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":145048710},\"end\":44657,\"start\":44209},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1166498},\"end\":44962,\"start\":44659},{\"attributes\":{\"doi\":\"arXiv:1911.05722\",\"id\":\"b22\"},\"end\":45257,\"start\":44964},{\"attributes\":{\"doi\":\"arXiv:1905.09272\",\"id\":\"b23\"},\"end\":45589,\"start\":45259},{\"attributes\":{\"id\":\"b24\"},\"end\":45912,\"start\":45591},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2309950},\"end\":46154,\"start\":45914},{\"attributes\":{\"doi\":\"arXiv:1808.06670\",\"id\":\"b26\"},\"end\":46578,\"start\":46156},{\"attributes\":{\"doi\":\"arXiv:1801.06146\",\"id\":\"b27\"},\"end\":46816,\"start\":46580},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5916235},\"end\":47269,\"start\":46818},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53670168},\"end\":47739,\"start\":47271},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b30\"},\"end\":48043,\"start\":47741},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":52187592},\"end\":48492,\"start\":48045},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":49657329},\"end\":48788,\"start\":48494},{\"attributes\":{\"doi\":\"arXiv:1312.6114\",\"id\":\"b33\"},\"end\":48975,\"start\":48790},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6377199},\"end\":49321,\"start\":48977},{\"attributes\":{\"doi\":\"arXiv:2001.04451\",\"id\":\"b35\"},\"end\":49538,\"start\":49323},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":59292019},\"end\":49972,\"start\":49540},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":195908774},\"end\":50317,\"start\":49974},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13975441},\"end\":50769,\"start\":50319},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":16216752},\"end\":51197,\"start\":50771},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":12008458},\"end\":51730,\"start\":51199},{\"attributes\":{\"doi\":\"arXiv:1701.03577\",\"id\":\"b41\"},\"end\":52112,\"start\":51732},{\"attributes\":{\"doi\":\"arXiv:1812.01608\",\"id\":\"b42\"},\"end\":52428,\"start\":52114},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":16447573},\"end\":52831,\"start\":52430},{\"attributes\":{\"doi\":\"arXiv:1912.01991\",\"id\":\"b44\"},\"end\":53089,\"start\":52833},{\"attributes\":{\"id\":\"b45\"},\"end\":53254,\"start\":53091},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":15539264},\"end\":53671,\"start\":53256},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":187547},\"end\":53988,\"start\":53673},{\"attributes\":{\"doi\":\"arXiv:1601.06759\",\"id\":\"b48\"},\"end\":54217,\"start\":53990},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b49\"},\"end\":54474,\"start\":54219},{\"attributes\":{\"doi\":\"arXiv:1412.6597\",\"id\":\"b50\"},\"end\":54777,\"start\":54476},{\"attributes\":{\"doi\":\"arXiv:1802.05751\",\"id\":\"b51\"},\"end\":55045,\"start\":54779},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":2202933},\"end\":55518,\"start\":55047},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":10659969},\"end\":56114,\"start\":55520},{\"attributes\":{\"doi\":\"arXiv:1802.05365\",\"id\":\"b54\"},\"end\":56434,\"start\":56116},{\"attributes\":{\"doi\":\"arXiv:1511.06434\",\"id\":\"b55\"},\"end\":56765,\"start\":56436},{\"attributes\":{\"id\":\"b56\"},\"end\":57005,\"start\":56767},{\"attributes\":{\"id\":\"b57\"},\"end\":57247,\"start\":57007},{\"attributes\":{\"doi\":\"arXiv:1906.05909\",\"id\":\"b58\"},\"end\":57560,\"start\":57249},{\"attributes\":{\"doi\":\"arXiv:1412.6604\",\"id\":\"b59\"},\"end\":57927,\"start\":57562},{\"attributes\":{\"id\":\"b60\"},\"end\":58355,\"start\":57929},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":202539356},\"end\":58887,\"start\":58357},{\"attributes\":{\"doi\":\"arXiv:2001.07685\",\"id\":\"b62\"},\"end\":59325,\"start\":58889},{\"attributes\":{\"doi\":\"arXiv:1905.11946\",\"id\":\"b63\"},\"end\":59580,\"start\":59327},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":2759724},\"end\":60003,\"start\":59582},{\"attributes\":{\"doi\":\"arXiv:1906.05849\",\"id\":\"b65\"},\"end\":60200,\"start\":60005},{\"attributes\":{\"id\":\"b66\"},\"end\":60613,\"start\":60202},{\"attributes\":{\"doi\":\"arXiv:1906.02940\",\"id\":\"b67\"},\"end\":60870,\"start\":60615},{\"attributes\":{\"id\":\"b68\"},\"end\":61200,\"start\":60872},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":13756489},\"end\":61581,\"start\":61202},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":207168299},\"end\":62050,\"start\":61583},{\"attributes\":{\"doi\":\"arXiv:1904.12848\",\"id\":\"b71\"},\"end\":62284,\"start\":62052},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b72\"},\"end\":62462,\"start\":62286},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":3960646},\"end\":62744,\"start\":62464},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b0\"},\"end\":37607,\"start\":37420},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":173990164},\"end\":37962,\"start\":37609},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":127951164},\"end\":38373,\"start\":37964},{\"attributes\":{\"id\":\"b3\"},\"end\":38778,\"start\":38375},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b4\"},\"end\":39087,\"start\":38780},{\"attributes\":{\"doi\":\"arXiv:1904.10509\",\"id\":\"b5\"},\"end\":39356,\"start\":39089},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":308212},\"end\":39855,\"start\":39358},{\"attributes\":{\"id\":\"b7\"},\"end\":40082,\"start\":39857},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7138078},\"end\":40321,\"start\":40084},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b9\"},\"end\":40652,\"start\":40323},{\"attributes\":{\"doi\":\"arXiv:1410.8516\",\"id\":\"b10\"},\"end\":40891,\"start\":40654},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9062671},\"end\":41312,\"start\":40893},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":195820291},\"end\":41593,\"start\":41314},{\"attributes\":{\"doi\":\"arXiv:1605.09782\",\"id\":\"b13\"},\"end\":41807,\"start\":41595},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":12881926},\"end\":42289,\"start\":41809},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15796526},\"end\":42656,\"start\":42291},{\"attributes\":{\"doi\":\"arXiv:1803.07728\",\"id\":\"b16\"},\"end\":42939,\"start\":42658},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5575601},\"end\":43437,\"start\":42941},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":8869447},\"end\":43815,\"start\":43439},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1033682},\"end\":44207,\"start\":43817},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":145048710},\"end\":44657,\"start\":44209},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1166498},\"end\":44962,\"start\":44659},{\"attributes\":{\"doi\":\"arXiv:1911.05722\",\"id\":\"b22\"},\"end\":45257,\"start\":44964},{\"attributes\":{\"doi\":\"arXiv:1905.09272\",\"id\":\"b23\"},\"end\":45589,\"start\":45259},{\"attributes\":{\"id\":\"b24\"},\"end\":45912,\"start\":45591},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2309950},\"end\":46154,\"start\":45914},{\"attributes\":{\"doi\":\"arXiv:1808.06670\",\"id\":\"b26\"},\"end\":46578,\"start\":46156},{\"attributes\":{\"doi\":\"arXiv:1801.06146\",\"id\":\"b27\"},\"end\":46816,\"start\":46580},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5916235},\"end\":47269,\"start\":46818},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53670168},\"end\":47739,\"start\":47271},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b30\"},\"end\":48043,\"start\":47741},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":52187592},\"end\":48492,\"start\":48045},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":49657329},\"end\":48788,\"start\":48494},{\"attributes\":{\"doi\":\"arXiv:1312.6114\",\"id\":\"b33\"},\"end\":48975,\"start\":48790},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6377199},\"end\":49321,\"start\":48977},{\"attributes\":{\"doi\":\"arXiv:2001.04451\",\"id\":\"b35\"},\"end\":49538,\"start\":49323},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":59292019},\"end\":49972,\"start\":49540},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":195908774},\"end\":50317,\"start\":49974},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13975441},\"end\":50769,\"start\":50319},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":16216752},\"end\":51197,\"start\":50771},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":12008458},\"end\":51730,\"start\":51199},{\"attributes\":{\"doi\":\"arXiv:1701.03577\",\"id\":\"b41\"},\"end\":52112,\"start\":51732},{\"attributes\":{\"doi\":\"arXiv:1812.01608\",\"id\":\"b42\"},\"end\":52428,\"start\":52114},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":16447573},\"end\":52831,\"start\":52430},{\"attributes\":{\"doi\":\"arXiv:1912.01991\",\"id\":\"b44\"},\"end\":53089,\"start\":52833},{\"attributes\":{\"id\":\"b45\"},\"end\":53254,\"start\":53091},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":15539264},\"end\":53671,\"start\":53256},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":187547},\"end\":53988,\"start\":53673},{\"attributes\":{\"doi\":\"arXiv:1601.06759\",\"id\":\"b48\"},\"end\":54217,\"start\":53990},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b49\"},\"end\":54474,\"start\":54219},{\"attributes\":{\"doi\":\"arXiv:1412.6597\",\"id\":\"b50\"},\"end\":54777,\"start\":54476},{\"attributes\":{\"doi\":\"arXiv:1802.05751\",\"id\":\"b51\"},\"end\":55045,\"start\":54779},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":2202933},\"end\":55518,\"start\":55047},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":10659969},\"end\":56114,\"start\":55520},{\"attributes\":{\"doi\":\"arXiv:1802.05365\",\"id\":\"b54\"},\"end\":56434,\"start\":56116},{\"attributes\":{\"doi\":\"arXiv:1511.06434\",\"id\":\"b55\"},\"end\":56765,\"start\":56436},{\"attributes\":{\"id\":\"b56\"},\"end\":57005,\"start\":56767},{\"attributes\":{\"id\":\"b57\"},\"end\":57247,\"start\":57007},{\"attributes\":{\"doi\":\"arXiv:1906.05909\",\"id\":\"b58\"},\"end\":57560,\"start\":57249},{\"attributes\":{\"doi\":\"arXiv:1412.6604\",\"id\":\"b59\"},\"end\":57927,\"start\":57562},{\"attributes\":{\"id\":\"b60\"},\"end\":58355,\"start\":57929},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":202539356},\"end\":58887,\"start\":58357},{\"attributes\":{\"doi\":\"arXiv:2001.07685\",\"id\":\"b62\"},\"end\":59325,\"start\":58889},{\"attributes\":{\"doi\":\"arXiv:1905.11946\",\"id\":\"b63\"},\"end\":59580,\"start\":59327},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":2759724},\"end\":60003,\"start\":59582},{\"attributes\":{\"doi\":\"arXiv:1906.05849\",\"id\":\"b65\"},\"end\":60200,\"start\":60005},{\"attributes\":{\"id\":\"b66\"},\"end\":60613,\"start\":60202},{\"attributes\":{\"doi\":\"arXiv:1906.02940\",\"id\":\"b67\"},\"end\":60870,\"start\":60615},{\"attributes\":{\"id\":\"b68\"},\"end\":61200,\"start\":60872},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":13756489},\"end\":61581,\"start\":61202},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":207168299},\"end\":62050,\"start\":61583},{\"attributes\":{\"doi\":\"arXiv:1904.12848\",\"id\":\"b71\"},\"end\":62284,\"start\":62052},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b72\"},\"end\":62462,\"start\":62286},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":3960646},\"end\":62744,\"start\":62464}]", "bib_title": "[{\"end\":37679,\"start\":37609},{\"end\":38006,\"start\":37964},{\"end\":38422,\"start\":38375},{\"end\":39426,\"start\":39358},{\"end\":40117,\"start\":40084},{\"end\":40958,\"start\":40893},{\"end\":41361,\"start\":41314},{\"end\":41897,\"start\":41809},{\"end\":42343,\"start\":42291},{\"end\":43014,\"start\":42941},{\"end\":43515,\"start\":43439},{\"end\":43844,\"start\":43817},{\"end\":44280,\"start\":44209},{\"end\":44727,\"start\":44659},{\"end\":45960,\"start\":45914},{\"end\":46868,\"start\":46818},{\"end\":47341,\"start\":47271},{\"end\":48120,\"start\":48045},{\"end\":48542,\"start\":48494},{\"end\":49029,\"start\":48977},{\"end\":49596,\"start\":49540},{\"end\":50037,\"start\":49974},{\"end\":50367,\"start\":50319},{\"end\":50829,\"start\":50771},{\"end\":51300,\"start\":51199},{\"end\":52505,\"start\":52430},{\"end\":53316,\"start\":53256},{\"end\":53746,\"start\":53673},{\"end\":55095,\"start\":55047},{\"end\":55560,\"start\":55520},{\"end\":58451,\"start\":58357},{\"end\":59701,\"start\":59582},{\"end\":60926,\"start\":60872},{\"end\":61227,\"start\":61202},{\"end\":61651,\"start\":61583},{\"end\":62516,\"start\":62464},{\"end\":37679,\"start\":37609},{\"end\":38006,\"start\":37964},{\"end\":38422,\"start\":38375},{\"end\":39426,\"start\":39358},{\"end\":40117,\"start\":40084},{\"end\":40958,\"start\":40893},{\"end\":41361,\"start\":41314},{\"end\":41897,\"start\":41809},{\"end\":42343,\"start\":42291},{\"end\":43014,\"start\":42941},{\"end\":43515,\"start\":43439},{\"end\":43844,\"start\":43817},{\"end\":44280,\"start\":44209},{\"end\":44727,\"start\":44659},{\"end\":45960,\"start\":45914},{\"end\":46868,\"start\":46818},{\"end\":47341,\"start\":47271},{\"end\":48120,\"start\":48045},{\"end\":48542,\"start\":48494},{\"end\":49029,\"start\":48977},{\"end\":49596,\"start\":49540},{\"end\":50037,\"start\":49974},{\"end\":50367,\"start\":50319},{\"end\":50829,\"start\":50771},{\"end\":51300,\"start\":51199},{\"end\":52505,\"start\":52430},{\"end\":53316,\"start\":53256},{\"end\":53746,\"start\":53673},{\"end\":55095,\"start\":55047},{\"end\":55560,\"start\":55520},{\"end\":58451,\"start\":58357},{\"end\":59701,\"start\":59582},{\"end\":60926,\"start\":60872},{\"end\":61227,\"start\":61202},{\"end\":61651,\"start\":61583},{\"end\":62516,\"start\":62464}]", "bib_author": "[{\"end\":37430,\"start\":37422},{\"end\":37441,\"start\":37430},{\"end\":37451,\"start\":37441},{\"end\":37692,\"start\":37681},{\"end\":37703,\"start\":37692},{\"end\":37717,\"start\":37703},{\"end\":38017,\"start\":38008},{\"end\":38025,\"start\":38017},{\"end\":38036,\"start\":38025},{\"end\":38046,\"start\":38036},{\"end\":38054,\"start\":38046},{\"end\":38437,\"start\":38424},{\"end\":38448,\"start\":38437},{\"end\":38462,\"start\":38448},{\"end\":38474,\"start\":38462},{\"end\":38484,\"start\":38474},{\"end\":38496,\"start\":38484},{\"end\":38506,\"start\":38496},{\"end\":38859,\"start\":38851},{\"end\":38872,\"start\":38859},{\"end\":38883,\"start\":38872},{\"end\":38893,\"start\":38883},{\"end\":39150,\"start\":39141},{\"end\":39158,\"start\":39150},{\"end\":39169,\"start\":39158},{\"end\":39182,\"start\":39169},{\"end\":39438,\"start\":39428},{\"end\":39444,\"start\":39438},{\"end\":39451,\"start\":39444},{\"end\":39866,\"start\":39857},{\"end\":39874,\"start\":39866},{\"end\":39882,\"start\":39874},{\"end\":39895,\"start\":39882},{\"end\":39903,\"start\":39895},{\"end\":39916,\"start\":39903},{\"end\":40128,\"start\":40119},{\"end\":40136,\"start\":40128},{\"end\":40333,\"start\":40323},{\"end\":40345,\"start\":40333},{\"end\":40352,\"start\":40345},{\"end\":40365,\"start\":40352},{\"end\":40371,\"start\":40365},{\"end\":40714,\"start\":40706},{\"end\":40725,\"start\":40714},{\"end\":40735,\"start\":40725},{\"end\":40971,\"start\":40960},{\"end\":40980,\"start\":40971},{\"end\":40991,\"start\":40980},{\"end\":41374,\"start\":41363},{\"end\":41386,\"start\":41374},{\"end\":41606,\"start\":41595},{\"end\":41620,\"start\":41606},{\"end\":41630,\"start\":41620},{\"end\":41634,\"start\":41630},{\"end\":41914,\"start\":41899},{\"end\":41925,\"start\":41914},{\"end\":41943,\"start\":41925},{\"end\":41957,\"start\":41943},{\"end\":41965,\"start\":41957},{\"end\":42354,\"start\":42345},{\"end\":42364,\"start\":42354},{\"end\":42377,\"start\":42364},{\"end\":42392,\"start\":42377},{\"end\":42403,\"start\":42392},{\"end\":42413,\"start\":42403},{\"end\":42737,\"start\":42726},{\"end\":42746,\"start\":42737},{\"end\":42759,\"start\":42746},{\"end\":43026,\"start\":43016},{\"end\":43036,\"start\":43026},{\"end\":43528,\"start\":43517},{\"end\":43535,\"start\":43528},{\"end\":43546,\"start\":43535},{\"end\":43558,\"start\":43546},{\"end\":43860,\"start\":43846},{\"end\":43877,\"start\":43860},{\"end\":43886,\"start\":43877},{\"end\":43892,\"start\":43886},{\"end\":43908,\"start\":43892},{\"end\":43917,\"start\":43908},{\"end\":43930,\"start\":43917},{\"end\":43940,\"start\":43930},{\"end\":44291,\"start\":44282},{\"end\":44302,\"start\":44291},{\"end\":44311,\"start\":44302},{\"end\":44320,\"start\":44311},{\"end\":44739,\"start\":44729},{\"end\":44749,\"start\":44739},{\"end\":45037,\"start\":45031},{\"end\":45044,\"start\":45037},{\"end\":45050,\"start\":45044},{\"end\":45057,\"start\":45050},{\"end\":45069,\"start\":45057},{\"end\":45271,\"start\":45259},{\"end\":45281,\"start\":45271},{\"end\":45292,\"start\":45281},{\"end\":45302,\"start\":45292},{\"end\":45312,\"start\":45302},{\"end\":45602,\"start\":45591},{\"end\":45613,\"start\":45602},{\"end\":45620,\"start\":45613},{\"end\":45631,\"start\":45620},{\"end\":45641,\"start\":45631},{\"end\":45654,\"start\":45641},{\"end\":45665,\"start\":45654},{\"end\":45677,\"start\":45665},{\"end\":45686,\"start\":45677},{\"end\":45974,\"start\":45962},{\"end\":45986,\"start\":45974},{\"end\":45996,\"start\":45986},{\"end\":46248,\"start\":46237},{\"end\":46259,\"start\":46248},{\"end\":46280,\"start\":46259},{\"end\":46290,\"start\":46280},{\"end\":46301,\"start\":46290},{\"end\":46314,\"start\":46301},{\"end\":46324,\"start\":46314},{\"end\":46652,\"start\":46642},{\"end\":46661,\"start\":46652},{\"end\":46882,\"start\":46870},{\"end\":46891,\"start\":46882},{\"end\":46904,\"start\":46891},{\"end\":46917,\"start\":46904},{\"end\":46932,\"start\":46917},{\"end\":47352,\"start\":47343},{\"end\":47361,\"start\":47352},{\"end\":47370,\"start\":47361},{\"end\":47379,\"start\":47370},{\"end\":47387,\"start\":47379},{\"end\":47395,\"start\":47387},{\"end\":47402,\"start\":47395},{\"end\":47411,\"start\":47402},{\"end\":47419,\"start\":47411},{\"end\":47425,\"start\":47419},{\"end\":47844,\"start\":47835},{\"end\":47855,\"start\":47844},{\"end\":48130,\"start\":48122},{\"end\":48145,\"start\":48130},{\"end\":48157,\"start\":48145},{\"end\":48166,\"start\":48157},{\"end\":48177,\"start\":48166},{\"end\":48184,\"start\":48177},{\"end\":48194,\"start\":48184},{\"end\":48556,\"start\":48544},{\"end\":48568,\"start\":48556},{\"end\":48574,\"start\":48568},{\"end\":48835,\"start\":48823},{\"end\":48846,\"start\":48835},{\"end\":49043,\"start\":49031},{\"end\":49054,\"start\":49043},{\"end\":49067,\"start\":49054},{\"end\":49078,\"start\":49067},{\"end\":49370,\"start\":49360},{\"end\":49380,\"start\":49370},{\"end\":49392,\"start\":49380},{\"end\":49612,\"start\":49598},{\"end\":49620,\"start\":49612},{\"end\":49629,\"start\":49620},{\"end\":50053,\"start\":50039},{\"end\":50066,\"start\":50053},{\"end\":50078,\"start\":50066},{\"end\":50383,\"start\":50369},{\"end\":50393,\"start\":50383},{\"end\":50845,\"start\":50831},{\"end\":50857,\"start\":50845},{\"end\":50868,\"start\":50857},{\"end\":51309,\"start\":51302},{\"end\":51319,\"start\":51309},{\"end\":51332,\"start\":51319},{\"end\":51340,\"start\":51332},{\"end\":51792,\"start\":51785},{\"end\":51806,\"start\":51792},{\"end\":51812,\"start\":51806},{\"end\":51819,\"start\":51812},{\"end\":51826,\"start\":51819},{\"end\":51836,\"start\":51826},{\"end\":51843,\"start\":51836},{\"end\":51854,\"start\":51843},{\"end\":51861,\"start\":51854},{\"end\":51874,\"start\":51861},{\"end\":52217,\"start\":52207},{\"end\":52233,\"start\":52217},{\"end\":52518,\"start\":52507},{\"end\":52531,\"start\":52518},{\"end\":52539,\"start\":52531},{\"end\":52552,\"start\":52539},{\"end\":52560,\"start\":52552},{\"end\":52905,\"start\":52896},{\"end\":52923,\"start\":52905},{\"end\":53146,\"start\":53135},{\"end\":53154,\"start\":53146},{\"end\":53164,\"start\":53154},{\"end\":53326,\"start\":53318},{\"end\":53338,\"start\":53326},{\"end\":53759,\"start\":53748},{\"end\":53769,\"start\":53759},{\"end\":53998,\"start\":53990},{\"end\":54014,\"start\":53998},{\"end\":54029,\"start\":54014},{\"end\":54227,\"start\":54219},{\"end\":54233,\"start\":54227},{\"end\":54244,\"start\":54233},{\"end\":54557,\"start\":54546},{\"end\":54569,\"start\":54557},{\"end\":54576,\"start\":54569},{\"end\":54587,\"start\":54576},{\"end\":54791,\"start\":54781},{\"end\":54802,\"start\":54791},{\"end\":54815,\"start\":54802},{\"end\":54825,\"start\":54815},{\"end\":54836,\"start\":54825},{\"end\":54842,\"start\":54836},{\"end\":54849,\"start\":54842},{\"end\":54853,\"start\":54849},{\"end\":55107,\"start\":55097},{\"end\":55121,\"start\":55107},{\"end\":55132,\"start\":55121},{\"end\":55143,\"start\":55132},{\"end\":55154,\"start\":55143},{\"end\":55575,\"start\":55562},{\"end\":55588,\"start\":55575},{\"end\":55600,\"start\":55588},{\"end\":55610,\"start\":55600},{\"end\":55621,\"start\":55610},{\"end\":55631,\"start\":55621},{\"end\":55642,\"start\":55631},{\"end\":55658,\"start\":55642},{\"end\":55667,\"start\":55658},{\"end\":55678,\"start\":55667},{\"end\":55692,\"start\":55678},{\"end\":55702,\"start\":55692},{\"end\":55716,\"start\":55702},{\"end\":55727,\"start\":55716},{\"end\":55737,\"start\":55727},{\"end\":55750,\"start\":55737},{\"end\":56128,\"start\":56116},{\"end\":56139,\"start\":56128},{\"end\":56148,\"start\":56139},{\"end\":56159,\"start\":56148},{\"end\":56168,\"start\":56159},{\"end\":56175,\"start\":56168},{\"end\":56190,\"start\":56175},{\"end\":56541,\"start\":56530},{\"end\":56549,\"start\":56541},{\"end\":56561,\"start\":56549},{\"end\":56838,\"start\":56827},{\"end\":56852,\"start\":56838},{\"end\":56864,\"start\":56852},{\"end\":56877,\"start\":56864},{\"end\":57071,\"start\":57060},{\"end\":57077,\"start\":57071},{\"end\":57086,\"start\":57077},{\"end\":57094,\"start\":57086},{\"end\":57104,\"start\":57094},{\"end\":57117,\"start\":57104},{\"end\":57310,\"start\":57294},{\"end\":57320,\"start\":57310},{\"end\":57331,\"start\":57320},{\"end\":57340,\"start\":57331},{\"end\":57352,\"start\":57340},{\"end\":57362,\"start\":57352},{\"end\":57652,\"start\":57641},{\"end\":57661,\"start\":57652},{\"end\":57670,\"start\":57661},{\"end\":57681,\"start\":57670},{\"end\":57694,\"start\":57681},{\"end\":57704,\"start\":57694},{\"end\":58057,\"start\":58048},{\"end\":58066,\"start\":58057},{\"end\":58075,\"start\":58066},{\"end\":58082,\"start\":58075},{\"end\":58089,\"start\":58082},{\"end\":58102,\"start\":58089},{\"end\":58108,\"start\":58102},{\"end\":58117,\"start\":58108},{\"end\":58121,\"start\":58117},{\"end\":58464,\"start\":58453},{\"end\":58475,\"start\":58464},{\"end\":58488,\"start\":58475},{\"end\":58498,\"start\":58488},{\"end\":58977,\"start\":58969},{\"end\":58990,\"start\":58977},{\"end\":58999,\"start\":58990},{\"end\":59008,\"start\":58999},{\"end\":59019,\"start\":59008},{\"end\":59030,\"start\":59019},{\"end\":59041,\"start\":59030},{\"end\":59050,\"start\":59041},{\"end\":59060,\"start\":59050},{\"end\":59334,\"start\":59327},{\"end\":59342,\"start\":59334},{\"end\":59356,\"start\":59342},{\"end\":59716,\"start\":59703},{\"end\":59727,\"start\":59716},{\"end\":60013,\"start\":60005},{\"end\":60025,\"start\":60013},{\"end\":60034,\"start\":60025},{\"end\":60367,\"start\":60355},{\"end\":60377,\"start\":60367},{\"end\":60390,\"start\":60377},{\"end\":60626,\"start\":60615},{\"end\":60638,\"start\":60626},{\"end\":60646,\"start\":60638},{\"end\":60654,\"start\":60646},{\"end\":60936,\"start\":60928},{\"end\":60946,\"start\":60936},{\"end\":60960,\"start\":60946},{\"end\":60967,\"start\":60960},{\"end\":61240,\"start\":61229},{\"end\":61251,\"start\":61240},{\"end\":61261,\"start\":61251},{\"end\":61274,\"start\":61261},{\"end\":61283,\"start\":61274},{\"end\":61294,\"start\":61283},{\"end\":61304,\"start\":61294},{\"end\":61318,\"start\":61304},{\"end\":61664,\"start\":61653},{\"end\":61678,\"start\":61664},{\"end\":61688,\"start\":61678},{\"end\":61703,\"start\":61688},{\"end\":62061,\"start\":62054},{\"end\":62068,\"start\":62061},{\"end\":62076,\"start\":62068},{\"end\":62088,\"start\":62076},{\"end\":62096,\"start\":62088},{\"end\":62301,\"start\":62288},{\"end\":62314,\"start\":62301},{\"end\":62530,\"start\":62518},{\"end\":62540,\"start\":62530},{\"end\":37430,\"start\":37422},{\"end\":37441,\"start\":37430},{\"end\":37451,\"start\":37441},{\"end\":37692,\"start\":37681},{\"end\":37703,\"start\":37692},{\"end\":37717,\"start\":37703},{\"end\":38017,\"start\":38008},{\"end\":38025,\"start\":38017},{\"end\":38036,\"start\":38025},{\"end\":38046,\"start\":38036},{\"end\":38054,\"start\":38046},{\"end\":38437,\"start\":38424},{\"end\":38448,\"start\":38437},{\"end\":38462,\"start\":38448},{\"end\":38474,\"start\":38462},{\"end\":38484,\"start\":38474},{\"end\":38496,\"start\":38484},{\"end\":38506,\"start\":38496},{\"end\":38859,\"start\":38851},{\"end\":38872,\"start\":38859},{\"end\":38883,\"start\":38872},{\"end\":38893,\"start\":38883},{\"end\":39150,\"start\":39141},{\"end\":39158,\"start\":39150},{\"end\":39169,\"start\":39158},{\"end\":39182,\"start\":39169},{\"end\":39438,\"start\":39428},{\"end\":39444,\"start\":39438},{\"end\":39451,\"start\":39444},{\"end\":39866,\"start\":39857},{\"end\":39874,\"start\":39866},{\"end\":39882,\"start\":39874},{\"end\":39895,\"start\":39882},{\"end\":39903,\"start\":39895},{\"end\":39916,\"start\":39903},{\"end\":40128,\"start\":40119},{\"end\":40136,\"start\":40128},{\"end\":40333,\"start\":40323},{\"end\":40345,\"start\":40333},{\"end\":40352,\"start\":40345},{\"end\":40365,\"start\":40352},{\"end\":40371,\"start\":40365},{\"end\":40714,\"start\":40706},{\"end\":40725,\"start\":40714},{\"end\":40735,\"start\":40725},{\"end\":40971,\"start\":40960},{\"end\":40980,\"start\":40971},{\"end\":40991,\"start\":40980},{\"end\":41374,\"start\":41363},{\"end\":41386,\"start\":41374},{\"end\":41606,\"start\":41595},{\"end\":41620,\"start\":41606},{\"end\":41630,\"start\":41620},{\"end\":41634,\"start\":41630},{\"end\":41914,\"start\":41899},{\"end\":41925,\"start\":41914},{\"end\":41943,\"start\":41925},{\"end\":41957,\"start\":41943},{\"end\":41965,\"start\":41957},{\"end\":42354,\"start\":42345},{\"end\":42364,\"start\":42354},{\"end\":42377,\"start\":42364},{\"end\":42392,\"start\":42377},{\"end\":42403,\"start\":42392},{\"end\":42413,\"start\":42403},{\"end\":42737,\"start\":42726},{\"end\":42746,\"start\":42737},{\"end\":42759,\"start\":42746},{\"end\":43026,\"start\":43016},{\"end\":43036,\"start\":43026},{\"end\":43528,\"start\":43517},{\"end\":43535,\"start\":43528},{\"end\":43546,\"start\":43535},{\"end\":43558,\"start\":43546},{\"end\":43860,\"start\":43846},{\"end\":43877,\"start\":43860},{\"end\":43886,\"start\":43877},{\"end\":43892,\"start\":43886},{\"end\":43908,\"start\":43892},{\"end\":43917,\"start\":43908},{\"end\":43930,\"start\":43917},{\"end\":43940,\"start\":43930},{\"end\":44291,\"start\":44282},{\"end\":44302,\"start\":44291},{\"end\":44311,\"start\":44302},{\"end\":44320,\"start\":44311},{\"end\":44739,\"start\":44729},{\"end\":44749,\"start\":44739},{\"end\":45037,\"start\":45031},{\"end\":45044,\"start\":45037},{\"end\":45050,\"start\":45044},{\"end\":45057,\"start\":45050},{\"end\":45069,\"start\":45057},{\"end\":45271,\"start\":45259},{\"end\":45281,\"start\":45271},{\"end\":45292,\"start\":45281},{\"end\":45302,\"start\":45292},{\"end\":45312,\"start\":45302},{\"end\":45602,\"start\":45591},{\"end\":45613,\"start\":45602},{\"end\":45620,\"start\":45613},{\"end\":45631,\"start\":45620},{\"end\":45641,\"start\":45631},{\"end\":45654,\"start\":45641},{\"end\":45665,\"start\":45654},{\"end\":45677,\"start\":45665},{\"end\":45686,\"start\":45677},{\"end\":45974,\"start\":45962},{\"end\":45986,\"start\":45974},{\"end\":45996,\"start\":45986},{\"end\":46248,\"start\":46237},{\"end\":46259,\"start\":46248},{\"end\":46280,\"start\":46259},{\"end\":46290,\"start\":46280},{\"end\":46301,\"start\":46290},{\"end\":46314,\"start\":46301},{\"end\":46324,\"start\":46314},{\"end\":46652,\"start\":46642},{\"end\":46661,\"start\":46652},{\"end\":46882,\"start\":46870},{\"end\":46891,\"start\":46882},{\"end\":46904,\"start\":46891},{\"end\":46917,\"start\":46904},{\"end\":46932,\"start\":46917},{\"end\":47352,\"start\":47343},{\"end\":47361,\"start\":47352},{\"end\":47370,\"start\":47361},{\"end\":47379,\"start\":47370},{\"end\":47387,\"start\":47379},{\"end\":47395,\"start\":47387},{\"end\":47402,\"start\":47395},{\"end\":47411,\"start\":47402},{\"end\":47419,\"start\":47411},{\"end\":47425,\"start\":47419},{\"end\":47844,\"start\":47835},{\"end\":47855,\"start\":47844},{\"end\":48130,\"start\":48122},{\"end\":48145,\"start\":48130},{\"end\":48157,\"start\":48145},{\"end\":48166,\"start\":48157},{\"end\":48177,\"start\":48166},{\"end\":48184,\"start\":48177},{\"end\":48194,\"start\":48184},{\"end\":48556,\"start\":48544},{\"end\":48568,\"start\":48556},{\"end\":48574,\"start\":48568},{\"end\":48835,\"start\":48823},{\"end\":48846,\"start\":48835},{\"end\":49043,\"start\":49031},{\"end\":49054,\"start\":49043},{\"end\":49067,\"start\":49054},{\"end\":49078,\"start\":49067},{\"end\":49370,\"start\":49360},{\"end\":49380,\"start\":49370},{\"end\":49392,\"start\":49380},{\"end\":49612,\"start\":49598},{\"end\":49620,\"start\":49612},{\"end\":49629,\"start\":49620},{\"end\":50053,\"start\":50039},{\"end\":50066,\"start\":50053},{\"end\":50078,\"start\":50066},{\"end\":50383,\"start\":50369},{\"end\":50393,\"start\":50383},{\"end\":50845,\"start\":50831},{\"end\":50857,\"start\":50845},{\"end\":50868,\"start\":50857},{\"end\":51309,\"start\":51302},{\"end\":51319,\"start\":51309},{\"end\":51332,\"start\":51319},{\"end\":51340,\"start\":51332},{\"end\":51792,\"start\":51785},{\"end\":51806,\"start\":51792},{\"end\":51812,\"start\":51806},{\"end\":51819,\"start\":51812},{\"end\":51826,\"start\":51819},{\"end\":51836,\"start\":51826},{\"end\":51843,\"start\":51836},{\"end\":51854,\"start\":51843},{\"end\":51861,\"start\":51854},{\"end\":51874,\"start\":51861},{\"end\":52217,\"start\":52207},{\"end\":52233,\"start\":52217},{\"end\":52518,\"start\":52507},{\"end\":52531,\"start\":52518},{\"end\":52539,\"start\":52531},{\"end\":52552,\"start\":52539},{\"end\":52560,\"start\":52552},{\"end\":52905,\"start\":52896},{\"end\":52923,\"start\":52905},{\"end\":53146,\"start\":53135},{\"end\":53154,\"start\":53146},{\"end\":53164,\"start\":53154},{\"end\":53326,\"start\":53318},{\"end\":53338,\"start\":53326},{\"end\":53759,\"start\":53748},{\"end\":53769,\"start\":53759},{\"end\":53998,\"start\":53990},{\"end\":54014,\"start\":53998},{\"end\":54029,\"start\":54014},{\"end\":54227,\"start\":54219},{\"end\":54233,\"start\":54227},{\"end\":54244,\"start\":54233},{\"end\":54557,\"start\":54546},{\"end\":54569,\"start\":54557},{\"end\":54576,\"start\":54569},{\"end\":54587,\"start\":54576},{\"end\":54791,\"start\":54781},{\"end\":54802,\"start\":54791},{\"end\":54815,\"start\":54802},{\"end\":54825,\"start\":54815},{\"end\":54836,\"start\":54825},{\"end\":54842,\"start\":54836},{\"end\":54849,\"start\":54842},{\"end\":54853,\"start\":54849},{\"end\":55107,\"start\":55097},{\"end\":55121,\"start\":55107},{\"end\":55132,\"start\":55121},{\"end\":55143,\"start\":55132},{\"end\":55154,\"start\":55143},{\"end\":55575,\"start\":55562},{\"end\":55588,\"start\":55575},{\"end\":55600,\"start\":55588},{\"end\":55610,\"start\":55600},{\"end\":55621,\"start\":55610},{\"end\":55631,\"start\":55621},{\"end\":55642,\"start\":55631},{\"end\":55658,\"start\":55642},{\"end\":55667,\"start\":55658},{\"end\":55678,\"start\":55667},{\"end\":55692,\"start\":55678},{\"end\":55702,\"start\":55692},{\"end\":55716,\"start\":55702},{\"end\":55727,\"start\":55716},{\"end\":55737,\"start\":55727},{\"end\":55750,\"start\":55737},{\"end\":56128,\"start\":56116},{\"end\":56139,\"start\":56128},{\"end\":56148,\"start\":56139},{\"end\":56159,\"start\":56148},{\"end\":56168,\"start\":56159},{\"end\":56175,\"start\":56168},{\"end\":56190,\"start\":56175},{\"end\":56541,\"start\":56530},{\"end\":56549,\"start\":56541},{\"end\":56561,\"start\":56549},{\"end\":56838,\"start\":56827},{\"end\":56852,\"start\":56838},{\"end\":56864,\"start\":56852},{\"end\":56877,\"start\":56864},{\"end\":57071,\"start\":57060},{\"end\":57077,\"start\":57071},{\"end\":57086,\"start\":57077},{\"end\":57094,\"start\":57086},{\"end\":57104,\"start\":57094},{\"end\":57117,\"start\":57104},{\"end\":57310,\"start\":57294},{\"end\":57320,\"start\":57310},{\"end\":57331,\"start\":57320},{\"end\":57340,\"start\":57331},{\"end\":57352,\"start\":57340},{\"end\":57362,\"start\":57352},{\"end\":57652,\"start\":57641},{\"end\":57661,\"start\":57652},{\"end\":57670,\"start\":57661},{\"end\":57681,\"start\":57670},{\"end\":57694,\"start\":57681},{\"end\":57704,\"start\":57694},{\"end\":58057,\"start\":58048},{\"end\":58066,\"start\":58057},{\"end\":58075,\"start\":58066},{\"end\":58082,\"start\":58075},{\"end\":58089,\"start\":58082},{\"end\":58102,\"start\":58089},{\"end\":58108,\"start\":58102},{\"end\":58117,\"start\":58108},{\"end\":58121,\"start\":58117},{\"end\":58464,\"start\":58453},{\"end\":58475,\"start\":58464},{\"end\":58488,\"start\":58475},{\"end\":58498,\"start\":58488},{\"end\":58977,\"start\":58969},{\"end\":58990,\"start\":58977},{\"end\":58999,\"start\":58990},{\"end\":59008,\"start\":58999},{\"end\":59019,\"start\":59008},{\"end\":59030,\"start\":59019},{\"end\":59041,\"start\":59030},{\"end\":59050,\"start\":59041},{\"end\":59060,\"start\":59050},{\"end\":59334,\"start\":59327},{\"end\":59342,\"start\":59334},{\"end\":59356,\"start\":59342},{\"end\":59716,\"start\":59703},{\"end\":59727,\"start\":59716},{\"end\":60013,\"start\":60005},{\"end\":60025,\"start\":60013},{\"end\":60034,\"start\":60025},{\"end\":60367,\"start\":60355},{\"end\":60377,\"start\":60367},{\"end\":60390,\"start\":60377},{\"end\":60626,\"start\":60615},{\"end\":60638,\"start\":60626},{\"end\":60646,\"start\":60638},{\"end\":60654,\"start\":60646},{\"end\":60936,\"start\":60928},{\"end\":60946,\"start\":60936},{\"end\":60960,\"start\":60946},{\"end\":60967,\"start\":60960},{\"end\":61240,\"start\":61229},{\"end\":61251,\"start\":61240},{\"end\":61261,\"start\":61251},{\"end\":61274,\"start\":61261},{\"end\":61283,\"start\":61274},{\"end\":61294,\"start\":61283},{\"end\":61304,\"start\":61294},{\"end\":61318,\"start\":61304},{\"end\":61664,\"start\":61653},{\"end\":61678,\"start\":61664},{\"end\":61688,\"start\":61678},{\"end\":61703,\"start\":61688},{\"end\":62061,\"start\":62054},{\"end\":62068,\"start\":62061},{\"end\":62076,\"start\":62068},{\"end\":62088,\"start\":62076},{\"end\":62096,\"start\":62088},{\"end\":62301,\"start\":62288},{\"end\":62314,\"start\":62301},{\"end\":62530,\"start\":62518},{\"end\":62540,\"start\":62530}]", "bib_venue": "[{\"end\":37766,\"start\":37717},{\"end\":38121,\"start\":38054},{\"end\":38555,\"start\":38506},{\"end\":38849,\"start\":38780},{\"end\":39139,\"start\":39089},{\"end\":39547,\"start\":39451},{\"end\":39958,\"start\":39916},{\"end\":40185,\"start\":40136},{\"end\":40461,\"start\":40387},{\"end\":40704,\"start\":40654},{\"end\":41058,\"start\":40991},{\"end\":41435,\"start\":41386},{\"end\":41678,\"start\":41650},{\"end\":42027,\"start\":41965},{\"end\":42449,\"start\":42413},{\"end\":42724,\"start\":42658},{\"end\":43132,\"start\":43036},{\"end\":43607,\"start\":43558},{\"end\":43989,\"start\":43940},{\"end\":44387,\"start\":44320},{\"end\":44793,\"start\":44749},{\"end\":45029,\"start\":44964},{\"end\":45395,\"start\":45328},{\"end\":45716,\"start\":45686},{\"end\":46014,\"start\":45996},{\"end\":46235,\"start\":46156},{\"end\":46640,\"start\":46580},{\"end\":47018,\"start\":46932},{\"end\":47474,\"start\":47425},{\"end\":47833,\"start\":47741},{\"end\":48243,\"start\":48194},{\"end\":48623,\"start\":48574},{\"end\":48821,\"start\":48790},{\"end\":49127,\"start\":49078},{\"end\":49358,\"start\":49323},{\"end\":49706,\"start\":49629},{\"end\":50127,\"start\":50078},{\"end\":50489,\"start\":50393},{\"end\":50953,\"start\":50868},{\"end\":51415,\"start\":51340},{\"end\":51783,\"start\":51732},{\"end\":52205,\"start\":52114},{\"end\":52609,\"start\":52560},{\"end\":52894,\"start\":52833},{\"end\":53133,\"start\":53091},{\"end\":53416,\"start\":53338},{\"end\":53807,\"start\":53769},{\"end\":54076,\"start\":54045},{\"end\":54318,\"start\":54260},{\"end\":54544,\"start\":54476},{\"end\":55231,\"start\":55154},{\"end\":55786,\"start\":55750},{\"end\":56246,\"start\":56206},{\"end\":56528,\"start\":56436},{\"end\":56825,\"start\":56767},{\"end\":57058,\"start\":57007},{\"end\":57292,\"start\":57249},{\"end\":57639,\"start\":57562},{\"end\":58046,\"start\":57929},{\"end\":58575,\"start\":58498},{\"end\":58967,\"start\":58889},{\"end\":59430,\"start\":59372},{\"end\":59776,\"start\":59727},{\"end\":60078,\"start\":60050},{\"end\":60353,\"start\":60202},{\"end\":60716,\"start\":60670},{\"end\":61016,\"start\":60967},{\"end\":61367,\"start\":61318},{\"end\":61771,\"start\":61703},{\"end\":62578,\"start\":62540},{\"end\":37766,\"start\":37717},{\"end\":38121,\"start\":38054},{\"end\":38555,\"start\":38506},{\"end\":38849,\"start\":38780},{\"end\":39139,\"start\":39089},{\"end\":39547,\"start\":39451},{\"end\":39958,\"start\":39916},{\"end\":40185,\"start\":40136},{\"end\":40461,\"start\":40387},{\"end\":40704,\"start\":40654},{\"end\":41058,\"start\":40991},{\"end\":41435,\"start\":41386},{\"end\":41678,\"start\":41650},{\"end\":42027,\"start\":41965},{\"end\":42449,\"start\":42413},{\"end\":42724,\"start\":42658},{\"end\":43132,\"start\":43036},{\"end\":43607,\"start\":43558},{\"end\":43989,\"start\":43940},{\"end\":44387,\"start\":44320},{\"end\":44793,\"start\":44749},{\"end\":45029,\"start\":44964},{\"end\":45395,\"start\":45328},{\"end\":45716,\"start\":45686},{\"end\":46014,\"start\":45996},{\"end\":46235,\"start\":46156},{\"end\":46640,\"start\":46580},{\"end\":47018,\"start\":46932},{\"end\":47474,\"start\":47425},{\"end\":47833,\"start\":47741},{\"end\":48243,\"start\":48194},{\"end\":48623,\"start\":48574},{\"end\":48821,\"start\":48790},{\"end\":49127,\"start\":49078},{\"end\":49358,\"start\":49323},{\"end\":49706,\"start\":49629},{\"end\":50127,\"start\":50078},{\"end\":50489,\"start\":50393},{\"end\":50953,\"start\":50868},{\"end\":51415,\"start\":51340},{\"end\":51783,\"start\":51732},{\"end\":52205,\"start\":52114},{\"end\":52609,\"start\":52560},{\"end\":52894,\"start\":52833},{\"end\":53133,\"start\":53091},{\"end\":53416,\"start\":53338},{\"end\":53807,\"start\":53769},{\"end\":54076,\"start\":54045},{\"end\":54318,\"start\":54260},{\"end\":54544,\"start\":54476},{\"end\":55231,\"start\":55154},{\"end\":55786,\"start\":55750},{\"end\":56246,\"start\":56206},{\"end\":56528,\"start\":56436},{\"end\":56825,\"start\":56767},{\"end\":57058,\"start\":57007},{\"end\":57292,\"start\":57249},{\"end\":57639,\"start\":57562},{\"end\":58046,\"start\":57929},{\"end\":58575,\"start\":58498},{\"end\":58967,\"start\":58889},{\"end\":59430,\"start\":59372},{\"end\":59776,\"start\":59727},{\"end\":60078,\"start\":60050},{\"end\":60353,\"start\":60202},{\"end\":60716,\"start\":60670},{\"end\":61016,\"start\":60967},{\"end\":61367,\"start\":61318},{\"end\":61771,\"start\":61703},{\"end\":62578,\"start\":62540},{\"end\":38175,\"start\":38123},{\"end\":39630,\"start\":39549},{\"end\":41112,\"start\":41060},{\"end\":43215,\"start\":43134},{\"end\":44441,\"start\":44389},{\"end\":49770,\"start\":49708},{\"end\":50572,\"start\":50491},{\"end\":51477,\"start\":51417},{\"end\":53481,\"start\":53418},{\"end\":55295,\"start\":55233},{\"end\":58639,\"start\":58577},{\"end\":61826,\"start\":61773},{\"end\":38175,\"start\":38123},{\"end\":39630,\"start\":39549},{\"end\":41112,\"start\":41060},{\"end\":43215,\"start\":43134},{\"end\":44441,\"start\":44389},{\"end\":49770,\"start\":49708},{\"end\":50572,\"start\":50491},{\"end\":51477,\"start\":51417},{\"end\":53481,\"start\":53418},{\"end\":55295,\"start\":55233},{\"end\":58639,\"start\":58577},{\"end\":61826,\"start\":61773}]"}}}, "year": 2023, "month": 12, "day": 17}