{"id": 234790470, "updated": "2023-11-11 06:23:08.313", "metadata": {"title": "Enriching Query Semantics for Code Search with Reinforcement Learning", "authors": "[{\"first\":\"Chaozheng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zhenghao\",\"last\":\"Nong\",\"middle\":[]},{\"first\":\"Cuiyun\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Zongjie\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jichuan\",\"last\":\"Zeng\",\"middle\":[]},{\"first\":\"Zhenchang\",\"last\":\"Xing\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "Neural networks : the official journal of the International Neural Network Society", "journal": "Neural networks : the official journal of the International Neural Network Society", "publication_date": {"year": 2021, "month": 5, "day": 20}, "abstract": "Code search is a common practice for developers during software implementation. The challenges of accurate code search mainly lie in the knowledge gap between source code and natural language (i.e., queries). Due to the limited code-query pairs and large code-description pairs available, the prior studies based on deep learning techniques focus on learning the semantic matching relation between source code and corresponding description texts for the task, and hypothesize that the semantic gap between descriptions and user queries is marginal. In this work, we found that the code search models trained on code-description pairs may not perform well on user queries, which indicates the semantic distance between queries and code descriptions. To mitigate the semantic distance for more effective code search, we propose QueCos, a Query-enriched Code search model. QueCos learns to generate semantic enriched queries to capture the key semantics of given queries with reinforcement learning (RL). With RL, the code search performance is considered as a reward for producing accurate semantic enriched queries. The enriched queries are finally employed for code search. Experiments on the benchmark datasets show that QueCos can significantly outperform the state-of-the-art code search models.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "34710788", "pubmedcentral": null, "dblp": "journals/nn/WangNGLZXL22", "doi": "10.1016/j.neunet.2021.09.025"}}, "content": {"source": {"pdf_hash": "dba6a03368481bec3fb8077bed59627cdf481e18", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.09630v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://doi.org/10.1016/j.neunet.2021.09.025", "status": "BRONZE"}}, "grobid": {"id": "c7b8901e9d8a97234f699ebc7f2e4682c63dd0a5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/dba6a03368481bec3fb8077bed59627cdf481e18.txt", "contents": "\nEnriching Query Semantics for Code Search with Reinforcement Learning\n\n\nChaozheng Wang \nSchool of Computer Science and Technology\nHarbin Institute of Technology\nShenzhenChina\n\nZhenhao Nong \nSchool of Computer Science and Technology\nHarbin Institute of Technology\nShenzhenChina\n\nCuiyun Gao \nSchool of Computer Science and Technology\nHarbin Institute of Technology\nShenzhenChina\n\nZongjie Li \nSchool of Computer Science and Technology\nHarbin Institute of Technology\nShenzhenChina\n\nJichuan Zeng \nDepartment of Computer Science and Engineering\nc Research School of Computer Science\nThe Chinese University of Hong Kong\nHong KongChina\n\nAustralian National University\nAustralia\n\nZhenchang Xing \nYang Liu \nSchool of Computer Science and Engineering\nNanyang Technology University\nSingapore\n\nEnriching Query Semantics for Code Search with Reinforcement Learning\nA R T I C L E I N F Ocode search query semantics semantic enrichment reinforcement learning\nA B S T R A C TCode search is a common practice for developers during software implementation. The challenges of accurate code search mainly lie in the knowledge gap between source code and natural language (i.e., queries). Due to the limited code-query pairs and large code-description pairs available, the prior studies based on deep learning techniques focus on learning the semantic matching relation between source code and corresponding description texts for the task, and hypothesize that the semantic gap between descriptions and user queries is marginal. In this work, we found that the code search models trained on code-description pairs may not perform well on user queries, which indicates the semantic distance between queries and code descriptions. To mitigate the semantic distance for more effective code search, we propose QueCos, a Query-enriched Code search model. QueCos learns to generate semantic enriched queries to capture the key semantics of given queries with reinforcement learning (RL). With RL, the code search performance is considered as a reward for producing accurate semantic enriched queries. The enriched queries are finally employed for code search. Experiments on the benchmark datasets show that QueCos can significantly outperform the state-of-the-art code search models. ORCID(s):Query:How to remove noise from a picture in Python?Code\"\"\"You can initialize a 5*5 kernel matrix. OpenCV provides a function, cv2.filter2D(), to convolve a kernel with an image and produce the output smoothed image. \"\"\" import cv2 import numpy as np\n\nIntroduction\n\nSearching large corpus of existing source code is a common behavior for developers during software programming. The goal of code search is to retrieve code snippets that most closely match a developer's query, which is generally described in natural language (e.g., the query illustrated in the top of Figure 1). Existing code search approaches can be divided into two categories: information retrieval (IR)-based (e.g., [16,21,20]) and deep learning (DL)-based (e.g., [24,4,26,29]). For example, Linstead et al. develop Sourcerer which retrieves similar code snippets based on software textual content and structural information [16]. The IR-based approaches rely on the overlapping tokens or language structures between natural language texts and code snippets, thus suffering from mismatches between the two heterogeneous sources [21]. Recent studies resort to deep learning techniques to remedy the issue by embedding source code and textual code descriptions into the same semantic space. The descriptions are usually written by developers to depict the functions of code snippets (as shown in Figure 1 level and word-level overlaps between code and descriptions based on convolutional network and self attention mechanism [31].\n\nAlthough promising results were achieved by the deep learning-based approaches, the approaches mostly focused on learning the semantic matching relations between source code and descriptions, and ignored the knowledge gap between input queries and code descriptions. As shown in Figure 1, the code search models trained on pairs of code and descriptions can retrieve the code snippet given the description, since there exist semantically-related keywords between the code and description such as \"cv2\" and \"fil-ter2D\". However, given the user query -\"How to remove noise from a picture in Python?\", which is relatively shorter than the description and limited in context, it will be difficult for the trained models to accurately retrieve the correspond-QueCos ing code snippet. To illustrate the semantic gap between queries and descriptions, in this work, we crawled 11,252 Java and 26,237 Python-related code-description-query triples from GitHub and Stack Overflow, one popular questionanswering site for developers, respectively. As far as we know, we are the first to prepare such dataset. We found that the lengths of queries are generally shorter than those of the corresponding code descriptions, e.g., 11.97 v.s. 73.61 words on average in Python (as depicted in Table 1 in Section 3). Preliminary experimentation further indicates that employing code-description pairs as training dataset possibly do not generalize well for user queries, which motivates us to enrich queries for effective code search.\n\nThere also exists prior work on query expansion to enrich the semantic information of user queries [23,17,9]. For example, Liu et al. predict sets of keywords to expand extremely short queries (e.g., queries with two or three words) [17]. The approaches heavily rely on the relevancy of the extended words to the queries and do not explicitly consider the code search performance during expanding the queries. In this paper, we propose to naturally use the search performance as a reward for capturing the query semantics through reinforcement learning (RL). The proposed approach is named as QueCos, an abbreviation of Queryenriched Code search model. Specifically, QueCos learns to generate the corresponding code descriptions given user queries, and the generated descriptions (called semantic enriched queries in the paper) are utilized for improving the code search performance.\n\nIn summary, we make the following contributions.\n\n\u2022 We prepare the first dataset containing triples of code, description texts and corresponding search queries. Based on the dataset, we observe that the code search models training on code-description pairs may not perform well on user queries.\n\n\u2022 We propose QueCos from a novel perspective of generating semantic enriched queries for code search. Que-Cos naturally captures the key semantic information of queries with the search performance as a reward. The semantic enriched queries are finally utilized to improve the code search task.\n\n\u2022 Extensive experiments demonstrate the effectiveness and flexibility of the proposed model on benchmark datasets. The source code and collected datasets are publicly available through this link 1 .\n\nPaper structure. The remainder of the paper is organized as follows. Section 2 presents our proposed framework. We introduce the experimental datasets, evaluation metrics and baselines in Section 3, and elaborate on the comparison results in Section 4. Section 5 illustrates the related work. We conclude and mention future work in Section 6. 1 Hidden url link\n\n\nMethodology\n\nThe overview of the proposed approach QueCos is depicted in Figure 2. As can be seen, QueCos mainly includes three components, i.e., code search model, query semantics enrichment model, and hybrid ranking. Initially, a code search model is trained using large code-description pairs (Section 2.1). Then, a query semantic enriching model is designed to generate the corresponding descriptions given user queries based on our collected dataset (Section 2.2), during which RL is adopted to enable the code snippets retrieved by the generated descriptions to be ranked higher. The generated descriptions are treated as semantic enriched queries and not necessary to be exactly close to the descriptions in the ground truth. Finally, both the semantic enriched queries and original queries are employed for the ultimate code search (Section 2.3). We elaborate on the details of each component in the following.\n\n\nCode Search (CS)\n\nThe code search component aims at learning a unified vector representation of both code snippets and descriptions. Since the focus of the work is to mitigate the semantic gap between natural language queries and descriptions for code search, we directly adopt the common framework of the existing code search models [7,4]. Let \u27e8 , \u27e9 be the set of code-description pairs, where and denote the sets of code snippets and descriptions, respectively. The pipeline of existing code search frameworks [7,10,31] are shown in the leftmost part of Figure 2, highlighted in green background. Generally, two neural networks are designed separately to encode code and descriptions for mapping the vector representations of the two input sources into the same space, where the neural networks can be Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN) or Transformer [10].\n\nThe CS model is trained by minimizing a ranking loss. Specifically, for each description \u2208 in the training corpus, the triple of \u27e8 , +, -\u27e9 is prepared as training instance, where + \u2208 and -\u2208 denote the correct code snippet that matches and sampled negative code snippet that does not match the description (which is randomly sampled from the corpus ), respectively. The objective ranking loss function is defined as:\n\ue238( ) = \u2211 < , +, \u2212> max(0, \u2212 ( , +)+ ( , -)),(1)\nwhere denotes the parameters in the CS model, is a constant margin ( is fixed as 0.05 in all the experiments similar to [7]), denotes the description vector of the description , and + and -denote the code vectors of + and -, respectively. The indicates the similarity measurement method which varies for different code search models.\n\n\nQuery Semantic Enrichment (QSE)\n\nThe query semantic enrichment (QSE) in QueCos targets at generating semantic-augmented queries given input queries. The model is trained to generate corresponding human-provided descriptions based on the queries. Unlike existing sequence-to-sequence models [29], the generated descriptions (i.e., enriched queries) in QueCos are not necessary to be exactly close to the human-provided ones, and the purpose of the training is to capture the main semantics of the queries.\n\n\nQSE model\n\nGiven a natural language query \u2208 , where is the set of collected queries, the QSE learns to produce the corresponding description tokens = ( 1 , ..., ).\n( | ) = ( 1 | 0 , ) \u220f =2 ( | 1... \u22121 , )(2)\nWe adopt standard bi-directional Long Short-Term Memory (LSTM) model with the attention mechanism [1] as QSE model structure to generate descriptions.\n\u210e = Att-Bi-LSTM( \u20d6\u20d6\u20d6\u20d6\u20d6\u20d6 \u20d7 \u210e \u22121 , \u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d6 \u210e +1 , ), ( | 0... \u22121 , ) = sof tmax( \u210e + )(3)\nwhere \u210e is the decoder hidden state at step , and are learnable weights to project the hidden state \u210e to the description vocabulary space.\n\n\nTraining QSE via RL\n\nThe reinforcement learning (RL) component is designed to render the generated queries in the QSE component able to rank the relevant code snippets ahead while close to the semantics of the descriptions. We view the description generation process as a Markov Decision Process (MDP) [3], and then use advantage Actor-Critic or 2 algorithm [22] to update the policy gradients. The MDP mainly consists of four ingredients, including state, action, reward and policy.\n\nState: In the decoding process, a state maintains the input query and the generated tokens 1... \u22121 . We take the hidden state vector as the vector representation of the state at the -th decoding step.\n\nAction: The action for the QSE is to choose the next word from the pre-defined vocabulary. So the action space in our formulation is the vocabulary.\n\nReward: The reward is used to evaluate the quality of the generated descriptions. In this work, the QSE is rewarded based on whether the correct code snippets retrieved by the semantic enriched queries are ranked higher and the relevancy between the semantic enriched queries and the code descriptions in the ground truth. Hence, we define the reward function ( , ) at each time step as:\n( , ) = \u23a7 \u23aa \u23a8 \u23aa \u23a9 * Rank( , 1... \u22121 )+ (1 \u2212 ) * BLEU( 1... \u22121 , ), if =< EOS > 0 otherwise (4)\nwhere Rank is defined as the popular ranking metric Mean Reciprocal Rank [7] (detailed in Section 3) value, which measures the ranking scores of the correct code snippets given the generated texts. The BLEU [1] score is the common metric for quantitatively estimating the relevance between generated texts 1... \u22121 and the descriptions in the ground truth. We use the BLEU-4 score in the paper. The is the reward tunning parameter and \u2208 [0, 1].\n\nPolicy: The policy function computes the probability of choosing as the next token. In this work, we use the policy gradient method [25] to optimize the policy function.\n\n\nOptimization\n\nIn order to stabilize the training process, we resort to the A2C algorithm [22]. Specifically, the gradient function is defined as: \n\u2207\ue238( ) = [ \u2211 =1 ( ( , )\u2212 ( ))\u2207 log ( | 1... \u22121 , ; )],(5) where ( , ) = \u2211 \u2032 \u2265 ( \u2032 , \u2032 )\nis the return for generating word given state , ( ) is the state value function that estimates the future reward given , we construct a critic network to predict ( ) and optimize it with a mean square error (MSE) loss\n\ue238( ) = \u223c (\u22c5| ) [ \u2211 | | =1 ( ( ; ) \u2212 ( , )) 2 ].\n\nHybrid Ranking\n\nIn the training phase, we first train the code search component and then utilize the RL component to generate semantic enriched queries for the code search task. In the testing phase, we design a hybrid ranking component to consider both the original queries and generated enriched queries for returning the ultimate search results.\n\nSpecifically, for each input query , we derive the semantic enriched queries \u2032 via the QSE component. The final similarity matching score between the query and candidate code snippet is computed as:\n( , ) = * ( \u2032 , ) + (1 \u2212 ) * ( , ),(6)\nwhere is a parameter to be adjusted experimentally, 0 \u2264 \u2264 1. , and \u2032 , and denote the encoded vectors of , \u2032 , and , respectively.\n\n\nExperimental Setup\n3.1. Dataset 3.1.1. CodeSearchNet.\nCodeSearchNet [10] is a publicly-available GitHub repository. It provides thousands of pairs of code snippets and the corresponding natural language descriptions. We conducted preprocessing following Gu's work [7] by first splitting source code tokens of the form CamelCase and snake case to respective sub-tokens, and then taking lowercase. We removed empty items afterwards. The numbers of the subject code-description pairs for the Java and Python datasets are shown in Table 1.\n\n\nOur Collected Datasets.\n\nSince no dataset containing aligned code snippets, descriptions, and queries were released, we decided to prepare such dataset. The dataset was crawled based on the SOTorrent dataset [2]. SOTorrent comprises pairs of SO (Stack Overflow) posts and GitHub files, where the GitHub files have referred to the corresponding posts, i.e., the code descriptions in the GitHub files clearly cite the post URLs 2 . Since we focus on the code written in Java and Python, we filtered the code snippets in other languages out. In SOTorrent, only links of the SO posts and GitHub files were provided, so we designed a crawler to traverse all the GitHub links, locate and save the code snippets referred to the SO posts and the corresponding code descriptions. To capture the SO queries, we accessed the SO posts via the SO links and obtained the queries according to the HTML element. The preprocessing procedure for the crawled data is also similar to [7]'s work. We finally split the preprocessed data to be training set, valid set and test set by 8:1:1. The statistics of the subject data are illustrated in Table 1.\n\n\nEvaluation Metric\n\nFollowing [7,10,31], we evaluate the model performance with the standard metrics, including R@k and MRR (Mean Reciprocal Rank).\n\n\nR@k\n\n@ is a common metric to evaluate whether an approach can retrieve the correct answer in the top returning results. It is widely used by many studies on the code search task. The metric is calculated as follows:\n@ = 1 | | | | \u2211 =1 ( \u2264 ),(7)\nwhere denotes the query set and denotes the rank of the correct answer for query . The function ( \u2264 ) \u2208 {0, 1} returns 1 if the rank of the correct answer within the top returning results otherwise returns 0. A higher @ indicates a better code search performance.\n\n\nMRR\n\nMean Reciprocal Rank (MRR) is the average of the reciprocal ranks of the correct answers of query set ordered by matching score., which is another popular evaluation metric for the code search task. The metric MRR is calculated as follows:\n= 1 | | | | \u2211 =1 1 .(8)\nThe higher the MRR value is, the better capacity the model has.\n\n\nBaselines\n\nSince the code search component in QueCos can be built upon any existing code search models, we mainly choose two popular approaches and one state-of-the-art approach for evaluation. DeepCS [7] is one popular code search model, which employs neural networks such as RNNs to embed both code snippets and descriptions into a joint vector space. Besides the code tokens, it also considers API sequences and method names for learning the representations of code snippets. Since the API sequences are hard to be extracted for the experimental datasets, we slightly modify the original model to only combine method names and code tokens for learning the code representations. UNIF [4] is similar to DeepCS but with attention mechanism involved. Besides, UNIF only considers code tokens during embedding code snippets. OCoR [31] is one of the state-of-the-art models. It captures the overlaps between code and descriptions in two levels, including character level and word/identifier level, for calculating the similarities between code and descriptions.\n\nWe also choose one popular query expansion approach, denoted as QE [19], as baseline. QE utilizes WordNet [13] to extend the queries with synonyms for effective code search. It can also be built upon any code search models.\n\n\nImplementation Details\n\nFollowing the prior studies [7,10,31], we tokenized code snippets, descriptions, and queries. Specifically, code snippets are split in camel case and snake case. Then we involved the top 10,000 words in the training set as the vocabularies of code snippets, descriptions, and queries, respectively, according to the word frequencies. In addition, we used two symbols to represents the beginning and end of a sentence for sentence generation in QSE. The word embeddings in both CS and QSE models are randomly initialized. The numbers of hidden states in the encoder and decoder are both defined as 256. The parameter (in Equ. 4) to tune the reward function is set as 1.0, and the parameter (in Equ. 6) for adjusting the hybrid ranking score is defined as 0.6. Detailed analysis about the parameter settings will be introduced in Section 4.3. We use the Adam optimizer with a learning rate 1e-3 and the learning rate decay ratio is defined as 0.5. For the baseline models, we use the same model hyper-parameters as the original papers. For evaluation, we fix a set of 999 negative code snippets -for each test pair \u27e8 , +\u27e9 following [10].\n\nDuring the training phase, we first train the CS model for 120 epochs, and then train the QSE model for another 20 epochs. Subsequently, we start to train the reinforcement learning component by pretraining the critic network for 10 epochs and jointly training the critic and actor networks for 40 epochs. All the experiments run on a server with 8 * Nvidia Tesla P100 and each one has 16GB graphic memory. Depending on the involved CS model in QueCos, the training process lasts from 70 to 240 hours.\n\n\nResults\n\nBased on the preceding experimental setup, we first illustrate the semantic gap between queries and descriptions, and then evaluate the effectiveness of the proposed QueCos model from three aspects, including comparison with the baselines, parameter analysis, and case study.\n\n\nPreliminary Experiments\n\nHere we investigate whether there exists semantic gap between user queries and code descriptions. Specifically, we analyzed the performance of the model trained with codedescription pairs on different types of evaluation sets, including queries and descriptions. We choose the three baseline models, i.e., DeepCS, UNIF, and OCoR for experimentation. All the models are trained on the code-description pairs provided by CodeSearchNet [10], and then evaluated with the crawled query test set and description test set in Table 1. The results are illustrated in the Table 2. We can observe that using queries as test set presents lower performance than using descriptions as test set for all the three code search baselines. For example, the MRR values decrease by 12.8% and 32.9% on average on the Java and Python datasets, respectively; and the R@1 values also show a downtrend, with the average decreasing rates at 21.8% and 42.9% on the two datasets, respectively. The results imply the semantic gap between queries and description texts. Thus, models trained on code-description pairs possibly do not perform well for practical user queries, and mitigating the semantic gap between queries and descriptions is necessary for more accurate code search.\n\n\nMain Comparison Results\n\nWe compare QueCos with the four baseline models introduced in Section 3 on the collected datasets. Since the code search component in QueCos and QE can be built upon any code search models, we analyze the performance with each of the code search baselines involved. The comparison results are illustrated in Table 3. The approach \"Que-Cos w/o RL\" indicates the proposed QueCos model without the RL procedure, that is, the code search performance is Based on the comparison results, we achieve the following observations: Observation 1: QueCos significantly improves the baseline models given user queries. As can be seen in Table 3, QueCos presents better performance than the corresponding code search model on both Java and Python datasets. For example, QueCos increases the accuracy of the corresponding code search model by 33.0%, 24.7%, 23.8% and 21.9% in terms of R@1, R@5, R@10 and MRR, respectively, for the Java dataset on average. The results demonstrate that with the query semantics augmented, QueCos can more accurately recommend relevant code snippets for given user queries. It is unsurprising that QueCos with OCoR achieves the best results on the Python dataset since OCoR already presents the best performance among the three baselines. But we also observe that QueCos with DeepCS shows the best results on the Java dataset in terms of R@1 and MRR, which further highlights the effectiveness of QueCos for enriching user queries for the task.\n\nObservation 2: The query semantic enriching component is more effective than the popular query expansion approach. By comparing QueCos with the popular query expansion approach QE, we can discover that Que-Cos can significantly improve the performance of QE. However, with QE involved, the code search models may perform worse, for example, QE with DeepCS degrades the performance of DeepCS by 6.11% in terms of R@1 on the Java dataset. This indicates that explicitly complementing the queries with synonyms may not contribute the model performance. The results also prove the usefulness of the query semantic enriching component in QueCos for code search.\n\nObservation 3: The RL and HR components in Que-Cos plays a key role in enriching the query semantics. Comparing QueCos w/o RL with QueCos, we can find that QueCos performs worse without the RL component, presenting even poorer performance than the corresponding base code search model. The phenomenon indicates that simply using the generated descriptions may bias the semantics of the queries and lead to inaccurate code search. This is reasonable since the queries are generally shorter than the descriptions according to Table 1, and the model is hard to accurately capture the semantic relations between queries and descriptions through merely training on query-description pairs. We provide an error case in Section 4.5 for further illustration.\n\nComparing QueCos w/o HR with QueCos, we can observe that the effectiveness of only using the generated enriched queries for code search is limited. Integrating the original queries could further enhance the model performance.\n\n\nParameter Analysis\n\nIn the section, we analyze the impact of two important hyper-parameters, including the reward tuning parameter and the hybrid ranking parameter , on the performance of QueCos. Figure 3 and Figure 4 illustrate the performance variations of QueCos with different base code search models as the parameters changes, respectively. During analyzing the impact of , we did not conduct the parameter analysis for the OCoR-based QueCos due to the huge computation resources required by OCoR 3 . As can be seen in Figure 3, when increases, the model performance presents an obvious uptrend in terms of both R@1 and MRR. The results indicate that considering the similarity between the generated texts and the descriptions in the ground truth as a reward during semantic enriched query generation is unhelpful for the code search task. This is reasonable since only involving the code ranking performance as a reward can render the RL component focus on generating the descriptions that could rank relevant code snippets higher. Thus, we define the reward tuning parameter as 1.0 during the experiments, i.e., the relevancy between the generated text and the humanprovided description is not considered as a reward.\n\nAccording to Figure 4, The performance variations of DeepCS-based QueCos along with the value are close to an inverted \"U\" shape, but the performance variations of UNIF-based or OCoR-based QueCos present an increasing trend along with the value. From Table 1, we observe that the code descriptions in the Python dataset are relatively longer than those in the Java dataset. We guess the generated enriched queries could deliver more accurate semantics for the Python dataset, so the relevance between the enriched queries and code snippets is more important for the search results comparing to the Java dataset. We fix the value of as 0.6 in the experiments since the model would approach the optimal accuracy. 3 The training process of OCoR lasts around 240 hours. converts list length of code taken its returned list code list> sorts a list using widths as a list integers Table 4 User query, description in the ground truth, and the enhanced query by QE and QueCos for the successful case 1. else: 16 ET.SubElement(root_element, name).text = value \u21aa Code Listing 2: Successful case 2, with more details shown in Table 5.\n\n\nCase Study\n\nCode Listing 1 and Table 4 show a case in which Que-Cos with UNIF-based successfully returns the correct code snippet of the query \"Java compare two lists' object values?\" at the top of the ranking list, and the basic UNIF model only ranks the code snippet at 11. We can observe that the code description does not indeed reflect the functionality of the code, i.e., comparison of two lists' objective values. The generated semantic enriched query of Que-Cos is \"converts list length of code taken its returned list code list> sorts a list using widths as a list integers\", which could deliver more semantics than the input query although the grammar is not exactly correct. The expanded query by the QE approach is \"comparison compare equivalence liken equate two 2 II deuce ii list listing...\", which also only ranks   the code snippet at 11. In addition, QueCos with DeepCSbased and OCoR-based can rank the correct code snippet at 9 and 19, respectively, while the basic DeepCS and OCoR provide the rank at 8 and 30 respectively. Overall, the generated semantic enriched query can be helpful for more accurate code search.\n\nFor the example shown in the Code Listing 2 and Table 5, the QueCos with UNIF-based improves the rank of the correct code snippet from 60 to 2 compared to the basic UNIF approach [4]. The proposed model can capture that the terms \"Cdata\" and \"element tree\" in the query are relevant to the XML language, and the generated semantic enriched query clearly includes terms such as \"xml\", \"element\", and \"etree\"; while the extended query by the QE approach fails to catch the relation of the query to XML, returning the code snippet at 16. So we suppose that through enriching the query semantics, QueCos can more accurately rank the code snippets.  Table 6.\n\n\nError Study\n\nCode Listing 3 and Table 6 show an error case. UNIF ranks corresponding code snippet at 4 according to the original query, however, UNIF-based QueCos only ranks it as 10. The poor performance can be attributed to that QueCos misunderstands the meaning of the word \"address\" in the query, and hence the enriched query contains irrelevant keywords, e.g., \"host\", \"url\", \"ip\", etc. In the future, we will design a quality assurance filtering component to distinguish the semantically-irreverent keywords in the generated enriched queries.  Table 6 User query, description in the ground truth, and the enhanced query by QE and QueCos for the error case in Code Listing 3.\n\n\nRelated Work\n\n\nCode Search\n\nAs the rapid growing of open-source code corpus, to retrieve the code fragments satisfying a user's intent with high accuracy concerning developing productivity. Prior studies on code search focus on figuring out the latent relationship between NL queries and code snippets. The work [21] proposes Portfolio, modelling the navigation behavior of programmers with random surfer and returning a chain of functions. Lv et al. [20] propose CodeHow, a code search tool that first retrieves all possible API calls to augment the queries.\n\nRecently, an increasing number of works using neural networks for code search have been proposed [17,29,4,31]. Sachdev et al. present a neural network-based model, which creates continuous vector representations of codes for comparison [24]. In the work [17], Liu et al. present NQE, which predicts the keywords in the NL queries and expands them in a productive way to improve performance for shorter queries. Gu et al. propose DeepCS which embeds both the code snippets and queries into a joint vector space to measure the similarity between them [7]. [4] introduces UNIF, a bag-of-words-based network which converts code snippets and docstring tokens into embedding matrices with supervised learning method. The authors in [29] treat code annotation and code search as two divided tasks and use the generated code annotations to improve the performance of code search. However, no previous studies have explicitly involved the search performance for query semantic enrichment for improving the task. Zhu et al. [31] propose an overlap-aware OCoR, which explicitly considers the overlap between code and descriptions in both word and character levels. Based on the word-level and character-level representations of the code and descriptions, OCoR utilizes MLP to compute the relevance.\n\n\nCode Representation Learning\n\nCode representation learning aims at learning the semantics of programs for facilitating various downstream tasks related to program comprehension, such as code clone detection, code summarization, bug detection [30,7,10,28,14,27,15,11], etc. The development of deep learning techniques boosts the research on code representation learning. In the work [7,10], code snippets are split into tokens and fed into neural networks such as RNNs and multi-head attentions for the representation learning. Considering the structural nature of code, [28,14,30] combine the abstract syntax trees (ASTs) into neural networks for capturing the code semantics. LeClair et al. [14] use GNN-based encoder to model the AST of each program subroutine.\n\nRecent work adapts pre-training techniques in the natural language processing field [5,12,18] for better program comprehension. For example, the work [6] presents Code-BERT, the first bimodal pre-trained model for programming language (PL) and natural language (NL). Guo et al. [8] later propose GraphCodeBERT, which combines both code tokens and the data flow information during pre-training, and achieves more promising performance.\n\n\nConclusions\n\nIn this paper, we have proposed a novel RL-based queryenriched code search model, named QueCos, for more effective code search. QueCos can capture the main semantics of user queries through learning to generate the corresponding descriptions, where RL is adopted to render the generated descriptions to be able to rank the relevant code snippets higher. We also crawled the first dataset containing triples of code, descriptions, and queries. Experiments demonstrate that QueCos is more effective than the baseline models and also flexible to incorporate any code search models. In the future, we will combine external knowledge graph and adapt Transformer to further enrich the semantics of the queries.\n\nFigure 1 :\n1), and can facilitate program comprehension. For example, Zhu et al. propose OCoR to capture the character-Description An example of a triple of query, description and code.\n\nFigure 2 :\n2Overall framework of the proposed QueCos.\n\n\nlistA.size() == listB.size() && listA.containsAll(listB);\n\nFigure 3 :Figure 4 :\n34Parameter sensitive study on the reward tuning parameter . The parameter analysis for OCoR-based QueCos was ignored due to the huge computation resources of OCoR. Parameter sensitive study on the hybrid ranking parameter .\n\nCode Listing 3 :\n3An error Case, with more details in\n\nTable 1\n1Statistics of the CodeSearchNet dataset and our collected dataset. Our collected dataset contains aligned code, description texts, and queries. The statistics include the number of pairs and the average lengths of code snippets, descriptions and queries. \"-\" means there are no such data in the dataset.Dataset \nTraining \nSet \n\nValidation \nSet \n\nTest \nSet \n\nAvg. Code \nToken Len. \n\nAvg. Desc. \nToken Len. \n\nAvg. Query \nToken Len. \n\nJava \n(CodeSearchNet) \n450,941 \n15,053 26,717 \n162.41 \n38.87 \n-\n\nJava \n(Ours) \n9,015 \n1,117 \n1,120 \n506.34 \n55.75 \n12.91 \n\nPython \n(CodeSearchNet) \n409,230 \n22,906 22,104 \n167.51 \n54.21 \n-\n\nPython \n(Ours) \n21,009 \n2,596 \n2,632 \n131.21 \n73.61 \n11.97 \n\n\n\nTable 2\n2Performance of the baseline models using different evaluation sets. Comparison results with baseline models. The bold figures indicate the best results.Approach \nEvaluation \nJava \nPython \nSet \nR@1 \nR@5 R@10 \nMRR \nR@1 \nR@5 R@10 \nMRR \n\nDeepCS \nDescription 0.202 0.389 \n0.488 0.297 \n0.160 0.298 \n0.368 0.231 \nQuery \n0.180 0.381 \n0.483 0.278 \n0.100 0.223 \n0.287 0.161 \n\nUNIF \nDescription 0.213 0.450 \n0.561 0.324 \n0.130 0.276 \n0.347 0.202 \nQuery \n0.156 0.439 \n0.568 0.285 \n0.078 0.202 \n0.261 0.143 \n\nOCoR \nDescription 0.292 0.561 \n0.654 0.412 \n0.230 0.423 \n0.507 0.322 \nQuery \n0.211 0.460 \n0.583 0.330 \n0.112 0.282 \n0.363 0.196 \n\nTable 3 \nApproach \nJava \nPython \nR@1 \nR@5 R@10 \nMRR \nR@1 \nR@5 R@10 \nMRR \n\nDeepCS-based \n\nDeepCS \n0.180 0.381 \n0.483 0.278 \n0.100 0.223 \n0.287 0.161 \nQE \n0.169 0.361 \n0.464 0.266 \n0.105 0.222 \n0.288 0.167 \nQueCos w/o RL \n0.110 0.239 \n0.322 0.180 \n0.048 0.131 \n0.180 0.095 \nQueCos w/o HR 0.192 0.402 \n0.498 0.294 \n0.117 0.252 \n0.309 0.182 \nQueCos (ours) \n0.282 0.519 \n0.611 0.394 \n0.149 0.297 \n0.370 0.224 \n\nUNIF-based \n\nUNIF \n0.156 0.439 \n0.568 0.285 \n0.078 0.202 \n0.261 0.143 \nQE \n0.155 0.431 \n0.553 0.281 \n0.072 0.179 \n0.250 0.133 \nQueCos w/o RL \n0.098 0.296 \n0.408 0.197 \n0.066 0.168 \n0.231 0.122 \nQueCos w/o HR 0.159 0.450 \n0.565 0.293 \n0.128 0.293 \n0.359 0.207 \nQueCos (ours) \n0.199 0.517 \n0.660 0.347 \n0.125 0.319 \n0.394 0.217 \n\nOCoR-based \n\nOCoR \n0.211 0.460 \n0.583 0.330 \n0.112 0.282 \n0.363 0.196 \nQE \n0.227 0.511 \n0.647 0.358 \n0.115 0.291 \n0.380 0.202 \nQueCos w/o RL \n0.183 0.418 \n0.504 0.289 \n0.095 0.227 \n0.305 0.163 \nQueCos w/o HR 0.216 0.516 \n0.613 0.349 \n0.184 0.408 \n0.492 0.282 \nQueCos (ours) \n0.242 0.553 \n0.649 0.375 \n0.184 0.398 \n0.489 0.289 \n\nnot considered as reward for the query enrichment. The ap-\nproach \"QueCos w/o HR\" denotes the QueCos without the \nHR component, i.e., the semantic enriched queries gener-\nated in QSE component are directly used for code search. \n\n\nTable 5\n5User query, description in the ground truth, and the enhanced query by QE and QueCos for the successful case 2.\n\n\naddress speech destination savoir-faire turn_to speak direct call cover accost unbridled cast mold mould form plas-ter_cast casting roll...QueCos \n\nQuery \nHow do I address unchecked cast warn-\nings? \n\nDescription \nin Ground Truth \nNone \n\nQuery Enriched \nby QE \n\nQuery Enriched \nby QueCos \n\nverify the device class wifi on host ( s \n, ip listener , url . , it will dynamically \nhandle web fixed allowing the passing \nand run of the context ( such , domain, \ndynamic ip address. \n\n\nDevelopers write the SO post URLs in the code descriptions for future program comprehension.\n\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsBahdanau, D., Cho, K., Bengio, Y., 2015. Neural machine translation by jointly learning to align and translate, in: 3rd International Con- ference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\n\nSotorrent: reconstructing and analyzing the evolution of stack overflow posts. S Baltes, L Dumani, C Treude, S Diehl, Proceedings of the 15th International Conference on Mining Software Repositories, MSR 2018. the 15th International Conference on Mining Software Repositories, MSR 2018Gothenburg, SwedenBaltes, S., Dumani, L., Treude, C., Diehl, S., 2018. Sotorrent: recon- structing and analyzing the evolution of stack overflow posts, in: Pro- ceedings of the 15th International Conference on Mining Software Repositories, MSR 2018, Gothenburg, Sweden, May 28-29, 2018, pp. 319-330.\n\nJournal of mathematics and mechanics. R Bellman, A markovian decision processBellman, R., 1957. A markovian decision process. Journal of math- ematics and mechanics , 679-684.\n\nWhen deep learning met code search. J Cambronero, H Li, S Kim, K Sen, S Chandra, Proceedings of the ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019. Dumas, M., Pfahl, D., Apel, S., Russo, A.the ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019Tallinn, EstoniaACMCambronero, J., Li, H., Kim, S., Sen, K., Chandra, S., 2019. When deep learning met code search, in: Dumas, M., Pfahl, D., Apel, S., Russo, A. (Eds.), Proceedings of the ACM Joint Meeting on Eu- ropean Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-30, 2019, ACM. pp. 964-974.\n\nBert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, arXiv:1810.04805arXiv preprintDevlin, J., Chang, M.W., Lee, K., Toutanova, K., 2018. Bert: Pre- training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805 .\n\nCodebert: A pre-trained model for programming and natural languages. Z Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, M Zhou, CoRR abs/2002.08155Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., Zhou, M., 2020. Codebert: A pre-trained model for programming and natural languages. CoRR abs/2002.08155.\n\nDeep code search. X Gu, H Zhang, S Kim, Proceedings of the 40th International Conference on Software Engineering. the 40th International Conference on Software EngineeringGothenburg, SwedenGu, X., Zhang, H., Kim, S., 2018. Deep code search, in: Proceedings of the 40th International Conference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 -June 03, 2018, pp. 933-944.\n\nGraphcodebert: Pre-training code representations with data flow. D Guo, S Ren, S Lu, Z Feng, D Tang, S Liu, L Zhou, N Duan, A Svyatkovskiy, S Fu, M Tufano, S K Deng, C B Clement, D Drain, N Sundaresan, J Yin, D Jiang, M Zhou, abs/2009.08366Guo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., Tufano, M., Deng, S.K., Clement, C.B., Drain, D., Sundaresan, N., Yin, J., Jiang, D., Zhou, M., 2020. Graph- codebert: Pre-training code representations with data flow. CoRR abs/2009.08366.\n\nDeep learning the semantics of change sequences for query expansion. Q Huang, Y Yang, M Cheng, 10.1002/spe.2736doi:10.1002/spe.2736Softw., Pract. Exper. 49Huang, Q., Yang, Y., Cheng, M., 2019. Deep learning the semantics of change sequences for query expansion. Softw., Pract. Exper. 49, 1600-1617. URL: https://doi.org/10.1002/spe.2736, doi:10.1002/spe.2736.\n\nCodesearchnet challenge: Evaluating the state of semantic code search. H Husain, H Wu, T Gazit, M Allamanis, M Brockschmidt, CoRR abs/1909.09436Husain, H., Wu, H., Gazit, T., Allamanis, M., Brockschmidt, M., 2019. Codesearchnet challenge: Evaluating the state of semantic code search. CoRR abs/1909.09436.\n\nBug localization with combination of deep learning and information retrieval. A N Lam, A T Nguyen, H A Nguyen, T N Nguyen, 2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC). IEEELam, A.N., Nguyen, A.T., Nguyen, H.A., Nguyen, T.N., 2017. Bug localization with combination of deep learning and information re- trieval, in: 2017 IEEE/ACM 25th International Conference on Pro- gram Comprehension (ICPC), IEEE. pp. 218-229.\n\nAlbert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, arXiv:1909.11942arXiv preprintLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R., 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 .\n\nCombining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database. C Leacock, M Chodorow, 49Leacock, C., Chodorow, M., 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An elec- tronic lexical database 49, 265-283.\n\nImproved code summarization via a graph neural network. A Leclair, S Haque, L Wu, C Mcmillan, Proceedings of the 28th International Conference on Program Comprehension. the 28th International Conference on Program ComprehensionLeClair, A., Haque, S., Wu, L., McMillan, C., 2020. Improved code summarization via a graph neural network, in: Proceedings of the 28th International Conference on Program Comprehension, pp. 184- 195.\n\nImproving bug detection via context-based code representation learning and attention-based neural networks. Y Li, S Wang, T N Nguyen, S Van Nguyen, Proceedings of the ACM on Programming Languages. 3Li, Y., Wang, S., Nguyen, T.N., Van Nguyen, S., 2019. Improv- ing bug detection via context-based code representation learning and attention-based neural networks. Proceedings of the ACM on Pro- gramming Languages 3, 1-30.\n\nSourcerer: mining and searching internet-scale software repositories. E Linstead, S K Bajracharya, T C Ngo, P Rigor, C V Lopes, P Baldi, Data Min. Knowl. Discov. 18Linstead, E., Bajracharya, S.K., Ngo, T.C., Rigor, P., Lopes, C.V., Baldi, P., 2009. Sourcerer: mining and searching internet-scale soft- ware repositories. Data Min. Knowl. Discov. 18, 300-336.\n\nNeural query expansion for code search. J Liu, S Kim, V Murali, S Chaudhuri, S Chandra, Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages. the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming LanguagesLiu, J., Kim, S., Murali, V., Chaudhuri, S., Chandra, S., 2019a. Neu- ral query expansion for code search, in: Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Pro- gramming Languages, pp. 29-37.\n\nY Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V., 2019b. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 .\n\nQuery expansion via wordnet for effective code search. M Lu, X Sun, S Wang, D Lo, Y Duan, 22nd IEEE International Conference on Software Analysis, Evolution, and Reengineering. Montreal, QC, CanadaLu, M., Sun, X., Wang, S., Lo, D., Duan, Y., 2015. Query expan- sion via wordnet for effective code search, in: 22nd IEEE Inter- national Conference on Software Analysis, Evolution, and Reengi- neering, SANER 2015, Montreal, QC, Canada, March 2-6, 2015, pp. 545-549.\n\nCodehow: Effective code search based on API understanding and extended boolean model (E). F Lv, H Zhang, J Lou, S Wang, D Zhang, J Zhao, 30th IEEE/ACM International Conference on Automated Software Engineering. Lincoln, NE, USALv, F., Zhang, H., Lou, J., Wang, S., Zhang, D., Zhao, J., 2015. Code- how: Effective code search based on API understanding and extended boolean model (E), in: 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015, Lincoln, NE, USA, November 9-13, 2015, pp. 260-270.\n\nPortfolio: finding relevant functions and their usage. C Mcmillan, M Grechanik, D Poshyvanyk, Q Xie, C Fu, Proceedings of the 33rd International Conference on Software Engineering. the 33rd International Conference on Software EngineeringWaikiki, Honolulu , HI, USAMcMillan, C., Grechanik, M., Poshyvanyk, D., Xie, Q., Fu, C., 2011. Portfolio: finding relevant functions and their usage, in: Proceedings of the 33rd International Conference on Software Engineering, ICSE 2011, Waikiki, Honolulu , HI, USA, May 21-28, 2011, pp. 111-120.\n\nAsynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T P Lillicrap, T Harley, D Silver, K Kavukcuoglu, Proceedings of the 33nd International Conference on Machine Learning. Balcan, M., Weinberger, K.Q.the 33nd International Conference on Machine LearningNew York City, NY, USAMnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T., Silver, D., Kavukcuoglu, K., 2016. Asynchronous methods for deep reinforcement learning, in: Balcan, M., Weinberger, K.Q. (Eds.), Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, JMLR.org. pp. 1928-1937.\n\nQuery expansion based on crowd knowledge for code search. L Nie, H Jiang, Z Ren, Z Sun, X Li, IEEE Trans. Services Computing. 9Nie, L., Jiang, H., Ren, Z., Sun, Z., Li, X., 2016. Query expansion based on crowd knowledge for code search. IEEE Trans. Services Computing 9, 771-783.\n\nRetrieval on source code: a neural code search. S Sachdev, H Li, S Luan, S Kim, K Sen, S Chandra, Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages. the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming LanguagesMAPL@PLDI; Philadelphia, PA, USASachdev, S., Li, H., Luan, S., Kim, S., Sen, K., Chandra, S., 2018. Retrieval on source code: a neural code search, in: Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learn- ing and Programming Languages, MAPL@PLDI 2018, Philadelphia, PA, USA, June 18-22, 2018, pp. 31-41.\n\nIntroduction to reinforcement learning. R S Sutton, A G Barto, MIT press Cambridge135Sutton, R.S., Barto, A.G., et al., 1998. Introduction to reinforcement learning. volume 135. MIT press Cambridge.\n\nMulti-modal attention network learning for semantic source code retrieval. Y Wan, J Shu, Y Sui, G Xu, Z Zhao, J Wu, P S Yu, 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019. San Diego, CA, USAWan, Y., Shu, J., Sui, Y., Xu, G., Zhao, Z., Wu, J., Yu, P.S., 2019. Multi-modal attention network learning for semantic source code re- trieval, in: 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019, pp. 13-25.\n\nImproving automatic source code summarization via deep reinforcement learning. Y Wan, Z Zhao, M Yang, G Xu, H Ying, J Wu, P S Yu, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringWan, Y., Zhao, Z., Yang, M., Xu, G., Ying, H., Wu, J., Yu, P.S., 2018. Improving automatic source code summarization via deep reinforce- ment learning, in: Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, pp. 397-407.\n\nModular tree network for source code representation learning. W Wang, G Li, S Shen, X Xia, Z Jin, ACM Transactions on Software Engineering and Methodology (TOSEM). 29Wang, W., Li, G., Shen, S., Xia, X., Jin, Z., 2020. Modular tree network for source code representation learning. ACM Transactions on Software Engineering and Methodology (TOSEM) 29, 1-23.\n\nCoacor: Code annotation for code retrieval with reinforcement learning. Z Yao, J R Peddamail, H Sun, The World Wide Web Conference. San Francisco, CA, USAYao, Z., Peddamail, J.R., Sun, H., 2019. Coacor: Code annotation for code retrieval with reinforcement learning, in: The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pp. 2203-2214.\n\nA novel neural source code representation based on abstract syntax tree. J Zhang, X Wang, H Zhang, H Sun, K Wang, X Liu, 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEEZhang, J., Wang, X., Zhang, H., Sun, H., Wang, K., Liu, X., 2019. A novel neural source code representation based on abstract syntax tree, in: 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), IEEE. pp. 783-794.\n\nOcor: An overlapping-aware code retriever. Q Zhu, Z Sun, X Liang, Y Xiong, L Zhang, 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEEZhu, Q., Sun, Z., Liang, X., Xiong, Y., Zhang, L., 2020. Ocor: An overlapping-aware code retriever, in: 2020 35th IEEE/ACM Interna- tional Conference on Automated Software Engineering (ASE), IEEE. pp. 883-894.\n", "annotations": {"author": "[{\"end\":176,\"start\":73},{\"end\":278,\"start\":177},{\"end\":378,\"start\":279},{\"end\":478,\"start\":379},{\"end\":671,\"start\":479},{\"end\":687,\"start\":672},{\"end\":781,\"start\":688}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":83},{\"end\":189,\"start\":185},{\"end\":289,\"start\":286},{\"end\":389,\"start\":387},{\"end\":491,\"start\":487},{\"end\":686,\"start\":682},{\"end\":696,\"start\":693}]", "author_first_name": "[{\"end\":82,\"start\":73},{\"end\":184,\"start\":177},{\"end\":285,\"start\":279},{\"end\":386,\"start\":379},{\"end\":486,\"start\":479},{\"end\":681,\"start\":672},{\"end\":692,\"start\":688}]", "author_affiliation": "[{\"end\":175,\"start\":89},{\"end\":277,\"start\":191},{\"end\":377,\"start\":291},{\"end\":477,\"start\":391},{\"end\":628,\"start\":493},{\"end\":670,\"start\":630},{\"end\":780,\"start\":698}]", "title": "[{\"end\":70,\"start\":1},{\"end\":851,\"start\":782}]", "venue": null, "abstract": "[{\"end\":2516,\"start\":944}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2957,\"start\":2953},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2960,\"start\":2957},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2963,\"start\":2960},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3005,\"start\":3001},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3007,\"start\":3005},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3010,\"start\":3007},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3013,\"start\":3010},{\"end\":3052,\"start\":3045},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3166,\"start\":3162},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3369,\"start\":3365},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3764,\"start\":3760},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4981,\"start\":4979},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5384,\"start\":5380},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5387,\"start\":5384},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5389,\"start\":5387},{\"end\":5422,\"start\":5404},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5518,\"start\":5514},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7301,\"start\":7300},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8578,\"start\":8575},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8580,\"start\":8578},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8756,\"start\":8753},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8759,\"start\":8756},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8762,\"start\":8759},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9130,\"start\":9126},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9720,\"start\":9717},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10227,\"start\":10223},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10749,\"start\":10746},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11333,\"start\":11330},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11390,\"start\":11386},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12424,\"start\":12421},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12558,\"start\":12555},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12929,\"start\":12925},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13058,\"start\":13054},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14259,\"start\":14255},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14454,\"start\":14451},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14936,\"start\":14933},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15692,\"start\":15689},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15890,\"start\":15887},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15893,\"start\":15890},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15896,\"start\":15893},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17057,\"start\":17054},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17542,\"start\":17539},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17685,\"start\":17681},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17984,\"start\":17980},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18023,\"start\":18019},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18194,\"start\":18191},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18197,\"start\":18194},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18200,\"start\":18197},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19297,\"start\":19293},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20553,\"start\":20549},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26433,\"start\":26432},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26724,\"start\":26722},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28167,\"start\":28164},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29640,\"start\":29636},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29779,\"start\":29775},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29986,\"start\":29982},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29989,\"start\":29986},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29991,\"start\":29989},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29994,\"start\":29991},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30125,\"start\":30121},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30143,\"start\":30139},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30437,\"start\":30434},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30442,\"start\":30439},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30615,\"start\":30611},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30903,\"start\":30899},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31421,\"start\":31417},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31423,\"start\":31421},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31426,\"start\":31423},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31429,\"start\":31426},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31432,\"start\":31429},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31435,\"start\":31432},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31438,\"start\":31435},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31441,\"start\":31438},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31560,\"start\":31557},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31563,\"start\":31560},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31749,\"start\":31745},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31752,\"start\":31749},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31755,\"start\":31752},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31871,\"start\":31867},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32027,\"start\":32024},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32030,\"start\":32027},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32033,\"start\":32030},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32093,\"start\":32090},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32221,\"start\":32218}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33281,\"start\":33095},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33336,\"start\":33282},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33396,\"start\":33337},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33643,\"start\":33397},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33698,\"start\":33644},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34392,\"start\":33699},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36320,\"start\":34393},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36442,\"start\":36321},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36924,\"start\":36443}]", "paragraph": "[{\"end\":3765,\"start\":2532},{\"end\":5279,\"start\":3767},{\"end\":6164,\"start\":5281},{\"end\":6214,\"start\":6166},{\"end\":6460,\"start\":6216},{\"end\":6755,\"start\":6462},{\"end\":6955,\"start\":6757},{\"end\":7317,\"start\":6957},{\"end\":8238,\"start\":7333},{\"end\":9131,\"start\":8259},{\"end\":9548,\"start\":9133},{\"end\":9930,\"start\":9597},{\"end\":10437,\"start\":9966},{\"end\":10603,\"start\":10451},{\"end\":10798,\"start\":10648},{\"end\":11025,\"start\":10887},{\"end\":11511,\"start\":11049},{\"end\":11713,\"start\":11513},{\"end\":11863,\"start\":11715},{\"end\":12252,\"start\":11865},{\"end\":12791,\"start\":12348},{\"end\":12962,\"start\":12793},{\"end\":13111,\"start\":12979},{\"end\":13416,\"start\":13199},{\"end\":13814,\"start\":13482},{\"end\":14014,\"start\":13816},{\"end\":14184,\"start\":14054},{\"end\":14722,\"start\":14241},{\"end\":15855,\"start\":14750},{\"end\":16004,\"start\":15877},{\"end\":16222,\"start\":16012},{\"end\":16515,\"start\":16252},{\"end\":16762,\"start\":16523},{\"end\":16850,\"start\":16787},{\"end\":17911,\"start\":16864},{\"end\":18136,\"start\":17913},{\"end\":19298,\"start\":18163},{\"end\":19801,\"start\":19300},{\"end\":20088,\"start\":19813},{\"end\":21367,\"start\":20116},{\"end\":22855,\"start\":21395},{\"end\":23513,\"start\":22857},{\"end\":24265,\"start\":23515},{\"end\":24492,\"start\":24267},{\"end\":25719,\"start\":24515},{\"end\":26844,\"start\":25721},{\"end\":27983,\"start\":26859},{\"end\":28638,\"start\":27985},{\"end\":29321,\"start\":28654},{\"end\":29883,\"start\":29352},{\"end\":31172,\"start\":29885},{\"end\":31938,\"start\":31205},{\"end\":32374,\"start\":31940},{\"end\":33094,\"start\":32390}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9596,\"start\":9549},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10647,\"start\":10604},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10886,\"start\":10799},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12347,\"start\":12253},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13165,\"start\":13112},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13198,\"start\":13165},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13464,\"start\":13417},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14053,\"start\":14015},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14240,\"start\":14206},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16251,\"start\":16223},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16786,\"start\":16763}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5046,\"start\":5039},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14721,\"start\":14714},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15854,\"start\":15847},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20641,\"start\":20634},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20685,\"start\":20678},{\"end\":21710,\"start\":21703},{\"end\":22026,\"start\":22019},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24046,\"start\":24039},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25979,\"start\":25972},{\"end\":26603,\"start\":26596},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26843,\"start\":26836},{\"end\":26885,\"start\":26878},{\"end\":28637,\"start\":28630},{\"end\":28680,\"start\":28673},{\"end\":29198,\"start\":29191}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2530,\"start\":2518},{\"attributes\":{\"n\":\"2.\"},\"end\":7331,\"start\":7320},{\"attributes\":{\"n\":\"2.1.\"},\"end\":8257,\"start\":8241},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9964,\"start\":9933},{\"attributes\":{\"n\":\"2.2.1.\"},\"end\":10449,\"start\":10440},{\"attributes\":{\"n\":\"2.2.2.\"},\"end\":11047,\"start\":11028},{\"attributes\":{\"n\":\"2.2.3.\"},\"end\":12977,\"start\":12965},{\"attributes\":{\"n\":\"2.3.\"},\"end\":13480,\"start\":13466},{\"attributes\":{\"n\":\"3.\"},\"end\":14205,\"start\":14187},{\"attributes\":{\"n\":\"3.1.2.\"},\"end\":14748,\"start\":14725},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15875,\"start\":15858},{\"attributes\":{\"n\":\"3.2.1.\"},\"end\":16010,\"start\":16007},{\"attributes\":{\"n\":\"3.2.2.\"},\"end\":16521,\"start\":16518},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16862,\"start\":16853},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18161,\"start\":18139},{\"attributes\":{\"n\":\"4.\"},\"end\":19811,\"start\":19804},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20114,\"start\":20091},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21393,\"start\":21370},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24513,\"start\":24495},{\"attributes\":{\"n\":\"4.4.\"},\"end\":26857,\"start\":26847},{\"attributes\":{\"n\":\"4.5.\"},\"end\":28652,\"start\":28641},{\"attributes\":{\"n\":\"5.\"},\"end\":29336,\"start\":29324},{\"attributes\":{\"n\":\"5.1.\"},\"end\":29350,\"start\":29339},{\"attributes\":{\"n\":\"5.2.\"},\"end\":31203,\"start\":31175},{\"attributes\":{\"n\":\"6.\"},\"end\":32388,\"start\":32377},{\"end\":33106,\"start\":33096},{\"end\":33293,\"start\":33283},{\"end\":33418,\"start\":33398},{\"end\":33661,\"start\":33645},{\"end\":33707,\"start\":33700},{\"end\":34401,\"start\":34394},{\"end\":36329,\"start\":36322}]", "table": "[{\"end\":34392,\"start\":34012},{\"end\":36320,\"start\":34555},{\"end\":36924,\"start\":36584}]", "figure_caption": "[{\"end\":33281,\"start\":33108},{\"end\":33336,\"start\":33295},{\"end\":33396,\"start\":33339},{\"end\":33643,\"start\":33421},{\"end\":33698,\"start\":33663},{\"end\":34012,\"start\":33709},{\"end\":34555,\"start\":34403},{\"end\":36442,\"start\":36331},{\"end\":36584,\"start\":36445}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2842,\"start\":2834},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3639,\"start\":3631},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4054,\"start\":4046},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7401,\"start\":7393},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8805,\"start\":8797},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24699,\"start\":24691},{\"end\":24712,\"start\":24704},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25027,\"start\":25019},{\"end\":25742,\"start\":25734}]", "bib_author_first_name": "[{\"end\":37091,\"start\":37090},{\"end\":37103,\"start\":37102},{\"end\":37110,\"start\":37109},{\"end\":37556,\"start\":37555},{\"end\":37566,\"start\":37565},{\"end\":37576,\"start\":37575},{\"end\":37586,\"start\":37585},{\"end\":38101,\"start\":38100},{\"end\":38276,\"start\":38275},{\"end\":38290,\"start\":38289},{\"end\":38296,\"start\":38295},{\"end\":38303,\"start\":38302},{\"end\":38310,\"start\":38309},{\"end\":39141,\"start\":39140},{\"end\":39151,\"start\":39150},{\"end\":39153,\"start\":39152},{\"end\":39162,\"start\":39161},{\"end\":39169,\"start\":39168},{\"end\":39456,\"start\":39455},{\"end\":39464,\"start\":39463},{\"end\":39471,\"start\":39470},{\"end\":39479,\"start\":39478},{\"end\":39487,\"start\":39486},{\"end\":39495,\"start\":39494},{\"end\":39503,\"start\":39502},{\"end\":39511,\"start\":39510},{\"end\":39518,\"start\":39517},{\"end\":39525,\"start\":39524},{\"end\":39534,\"start\":39533},{\"end\":39784,\"start\":39783},{\"end\":39790,\"start\":39789},{\"end\":39799,\"start\":39798},{\"end\":40218,\"start\":40217},{\"end\":40225,\"start\":40224},{\"end\":40232,\"start\":40231},{\"end\":40238,\"start\":40237},{\"end\":40246,\"start\":40245},{\"end\":40254,\"start\":40253},{\"end\":40261,\"start\":40260},{\"end\":40269,\"start\":40268},{\"end\":40277,\"start\":40276},{\"end\":40293,\"start\":40292},{\"end\":40299,\"start\":40298},{\"end\":40309,\"start\":40308},{\"end\":40311,\"start\":40310},{\"end\":40319,\"start\":40318},{\"end\":40321,\"start\":40320},{\"end\":40332,\"start\":40331},{\"end\":40341,\"start\":40340},{\"end\":40355,\"start\":40354},{\"end\":40362,\"start\":40361},{\"end\":40371,\"start\":40370},{\"end\":40754,\"start\":40753},{\"end\":40763,\"start\":40762},{\"end\":40771,\"start\":40770},{\"end\":41117,\"start\":41116},{\"end\":41127,\"start\":41126},{\"end\":41133,\"start\":41132},{\"end\":41142,\"start\":41141},{\"end\":41155,\"start\":41154},{\"end\":41431,\"start\":41430},{\"end\":41433,\"start\":41432},{\"end\":41440,\"start\":41439},{\"end\":41442,\"start\":41441},{\"end\":41452,\"start\":41451},{\"end\":41454,\"start\":41453},{\"end\":41464,\"start\":41463},{\"end\":41466,\"start\":41465},{\"end\":41877,\"start\":41876},{\"end\":41884,\"start\":41883},{\"end\":41892,\"start\":41891},{\"end\":41903,\"start\":41902},{\"end\":41913,\"start\":41912},{\"end\":41923,\"start\":41922},{\"end\":42271,\"start\":42270},{\"end\":42282,\"start\":42281},{\"end\":42519,\"start\":42518},{\"end\":42530,\"start\":42529},{\"end\":42539,\"start\":42538},{\"end\":42545,\"start\":42544},{\"end\":43000,\"start\":42999},{\"end\":43006,\"start\":43005},{\"end\":43014,\"start\":43013},{\"end\":43016,\"start\":43015},{\"end\":43026,\"start\":43025},{\"end\":43384,\"start\":43383},{\"end\":43396,\"start\":43395},{\"end\":43398,\"start\":43397},{\"end\":43413,\"start\":43412},{\"end\":43415,\"start\":43414},{\"end\":43422,\"start\":43421},{\"end\":43431,\"start\":43430},{\"end\":43433,\"start\":43432},{\"end\":43442,\"start\":43441},{\"end\":43714,\"start\":43713},{\"end\":43721,\"start\":43720},{\"end\":43728,\"start\":43727},{\"end\":43738,\"start\":43737},{\"end\":43751,\"start\":43750},{\"end\":44185,\"start\":44184},{\"end\":44192,\"start\":44191},{\"end\":44199,\"start\":44198},{\"end\":44208,\"start\":44207},{\"end\":44214,\"start\":44213},{\"end\":44223,\"start\":44222},{\"end\":44231,\"start\":44230},{\"end\":44239,\"start\":44238},{\"end\":44248,\"start\":44247},{\"end\":44263,\"start\":44262},{\"end\":44626,\"start\":44625},{\"end\":44632,\"start\":44631},{\"end\":44639,\"start\":44638},{\"end\":44647,\"start\":44646},{\"end\":44653,\"start\":44652},{\"end\":45126,\"start\":45125},{\"end\":45132,\"start\":45131},{\"end\":45141,\"start\":45140},{\"end\":45148,\"start\":45147},{\"end\":45156,\"start\":45155},{\"end\":45165,\"start\":45164},{\"end\":45616,\"start\":45615},{\"end\":45628,\"start\":45627},{\"end\":45641,\"start\":45640},{\"end\":45655,\"start\":45654},{\"end\":45662,\"start\":45661},{\"end\":46152,\"start\":46151},{\"end\":46160,\"start\":46159},{\"end\":46162,\"start\":46161},{\"end\":46171,\"start\":46170},{\"end\":46180,\"start\":46179},{\"end\":46190,\"start\":46189},{\"end\":46192,\"start\":46191},{\"end\":46205,\"start\":46204},{\"end\":46215,\"start\":46214},{\"end\":46225,\"start\":46224},{\"end\":46825,\"start\":46824},{\"end\":46832,\"start\":46831},{\"end\":46841,\"start\":46840},{\"end\":46848,\"start\":46847},{\"end\":46855,\"start\":46854},{\"end\":47096,\"start\":47095},{\"end\":47107,\"start\":47106},{\"end\":47113,\"start\":47112},{\"end\":47121,\"start\":47120},{\"end\":47128,\"start\":47127},{\"end\":47135,\"start\":47134},{\"end\":47707,\"start\":47706},{\"end\":47709,\"start\":47708},{\"end\":47719,\"start\":47718},{\"end\":47721,\"start\":47720},{\"end\":47942,\"start\":47941},{\"end\":47949,\"start\":47948},{\"end\":47956,\"start\":47955},{\"end\":47963,\"start\":47962},{\"end\":47969,\"start\":47968},{\"end\":47977,\"start\":47976},{\"end\":47983,\"start\":47982},{\"end\":47985,\"start\":47984},{\"end\":48460,\"start\":48459},{\"end\":48467,\"start\":48466},{\"end\":48475,\"start\":48474},{\"end\":48483,\"start\":48482},{\"end\":48489,\"start\":48488},{\"end\":48497,\"start\":48496},{\"end\":48503,\"start\":48502},{\"end\":48505,\"start\":48504},{\"end\":49005,\"start\":49004},{\"end\":49013,\"start\":49012},{\"end\":49019,\"start\":49018},{\"end\":49027,\"start\":49026},{\"end\":49034,\"start\":49033},{\"end\":49371,\"start\":49370},{\"end\":49378,\"start\":49377},{\"end\":49380,\"start\":49379},{\"end\":49393,\"start\":49392},{\"end\":49741,\"start\":49740},{\"end\":49750,\"start\":49749},{\"end\":49758,\"start\":49757},{\"end\":49767,\"start\":49766},{\"end\":49774,\"start\":49773},{\"end\":49782,\"start\":49781},{\"end\":50151,\"start\":50150},{\"end\":50158,\"start\":50157},{\"end\":50165,\"start\":50164},{\"end\":50174,\"start\":50173},{\"end\":50183,\"start\":50182}]", "bib_author_last_name": "[{\"end\":37100,\"start\":37092},{\"end\":37107,\"start\":37104},{\"end\":37117,\"start\":37111},{\"end\":37563,\"start\":37557},{\"end\":37573,\"start\":37567},{\"end\":37583,\"start\":37577},{\"end\":37592,\"start\":37587},{\"end\":38109,\"start\":38102},{\"end\":38287,\"start\":38277},{\"end\":38293,\"start\":38291},{\"end\":38300,\"start\":38297},{\"end\":38307,\"start\":38304},{\"end\":38318,\"start\":38311},{\"end\":39148,\"start\":39142},{\"end\":39159,\"start\":39154},{\"end\":39166,\"start\":39163},{\"end\":39179,\"start\":39170},{\"end\":39461,\"start\":39457},{\"end\":39468,\"start\":39465},{\"end\":39476,\"start\":39472},{\"end\":39484,\"start\":39480},{\"end\":39492,\"start\":39488},{\"end\":39500,\"start\":39496},{\"end\":39508,\"start\":39504},{\"end\":39515,\"start\":39512},{\"end\":39522,\"start\":39519},{\"end\":39531,\"start\":39526},{\"end\":39539,\"start\":39535},{\"end\":39787,\"start\":39785},{\"end\":39796,\"start\":39791},{\"end\":39803,\"start\":39800},{\"end\":40222,\"start\":40219},{\"end\":40229,\"start\":40226},{\"end\":40235,\"start\":40233},{\"end\":40243,\"start\":40239},{\"end\":40251,\"start\":40247},{\"end\":40258,\"start\":40255},{\"end\":40266,\"start\":40262},{\"end\":40274,\"start\":40270},{\"end\":40290,\"start\":40278},{\"end\":40296,\"start\":40294},{\"end\":40306,\"start\":40300},{\"end\":40316,\"start\":40312},{\"end\":40329,\"start\":40322},{\"end\":40338,\"start\":40333},{\"end\":40352,\"start\":40342},{\"end\":40359,\"start\":40356},{\"end\":40368,\"start\":40363},{\"end\":40376,\"start\":40372},{\"end\":40760,\"start\":40755},{\"end\":40768,\"start\":40764},{\"end\":40777,\"start\":40772},{\"end\":41124,\"start\":41118},{\"end\":41130,\"start\":41128},{\"end\":41139,\"start\":41134},{\"end\":41152,\"start\":41143},{\"end\":41168,\"start\":41156},{\"end\":41437,\"start\":41434},{\"end\":41449,\"start\":41443},{\"end\":41461,\"start\":41455},{\"end\":41473,\"start\":41467},{\"end\":41881,\"start\":41878},{\"end\":41889,\"start\":41885},{\"end\":41900,\"start\":41893},{\"end\":41910,\"start\":41904},{\"end\":41920,\"start\":41914},{\"end\":41931,\"start\":41924},{\"end\":42279,\"start\":42272},{\"end\":42291,\"start\":42283},{\"end\":42527,\"start\":42520},{\"end\":42536,\"start\":42531},{\"end\":42542,\"start\":42540},{\"end\":42554,\"start\":42546},{\"end\":43003,\"start\":43001},{\"end\":43011,\"start\":43007},{\"end\":43023,\"start\":43017},{\"end\":43037,\"start\":43027},{\"end\":43393,\"start\":43385},{\"end\":43410,\"start\":43399},{\"end\":43419,\"start\":43416},{\"end\":43428,\"start\":43423},{\"end\":43439,\"start\":43434},{\"end\":43448,\"start\":43443},{\"end\":43718,\"start\":43715},{\"end\":43725,\"start\":43722},{\"end\":43735,\"start\":43729},{\"end\":43748,\"start\":43739},{\"end\":43759,\"start\":43752},{\"end\":44189,\"start\":44186},{\"end\":44196,\"start\":44193},{\"end\":44205,\"start\":44200},{\"end\":44211,\"start\":44209},{\"end\":44220,\"start\":44215},{\"end\":44228,\"start\":44224},{\"end\":44236,\"start\":44232},{\"end\":44245,\"start\":44240},{\"end\":44260,\"start\":44249},{\"end\":44272,\"start\":44264},{\"end\":44629,\"start\":44627},{\"end\":44636,\"start\":44633},{\"end\":44644,\"start\":44640},{\"end\":44650,\"start\":44648},{\"end\":44658,\"start\":44654},{\"end\":45129,\"start\":45127},{\"end\":45138,\"start\":45133},{\"end\":45145,\"start\":45142},{\"end\":45153,\"start\":45149},{\"end\":45162,\"start\":45157},{\"end\":45170,\"start\":45166},{\"end\":45625,\"start\":45617},{\"end\":45638,\"start\":45629},{\"end\":45652,\"start\":45642},{\"end\":45659,\"start\":45656},{\"end\":45665,\"start\":45663},{\"end\":46157,\"start\":46153},{\"end\":46168,\"start\":46163},{\"end\":46177,\"start\":46172},{\"end\":46187,\"start\":46181},{\"end\":46202,\"start\":46193},{\"end\":46212,\"start\":46206},{\"end\":46222,\"start\":46216},{\"end\":46237,\"start\":46226},{\"end\":46829,\"start\":46826},{\"end\":46838,\"start\":46833},{\"end\":46845,\"start\":46842},{\"end\":46852,\"start\":46849},{\"end\":46858,\"start\":46856},{\"end\":47104,\"start\":47097},{\"end\":47110,\"start\":47108},{\"end\":47118,\"start\":47114},{\"end\":47125,\"start\":47122},{\"end\":47132,\"start\":47129},{\"end\":47143,\"start\":47136},{\"end\":47716,\"start\":47710},{\"end\":47727,\"start\":47722},{\"end\":47946,\"start\":47943},{\"end\":47953,\"start\":47950},{\"end\":47960,\"start\":47957},{\"end\":47966,\"start\":47964},{\"end\":47974,\"start\":47970},{\"end\":47980,\"start\":47978},{\"end\":47988,\"start\":47986},{\"end\":48464,\"start\":48461},{\"end\":48472,\"start\":48468},{\"end\":48480,\"start\":48476},{\"end\":48486,\"start\":48484},{\"end\":48494,\"start\":48490},{\"end\":48500,\"start\":48498},{\"end\":48508,\"start\":48506},{\"end\":49010,\"start\":49006},{\"end\":49016,\"start\":49014},{\"end\":49024,\"start\":49020},{\"end\":49031,\"start\":49028},{\"end\":49038,\"start\":49035},{\"end\":49375,\"start\":49372},{\"end\":49390,\"start\":49381},{\"end\":49397,\"start\":49394},{\"end\":49747,\"start\":49742},{\"end\":49755,\"start\":49751},{\"end\":49764,\"start\":49759},{\"end\":49771,\"start\":49768},{\"end\":49779,\"start\":49775},{\"end\":49786,\"start\":49783},{\"end\":50155,\"start\":50152},{\"end\":50162,\"start\":50159},{\"end\":50171,\"start\":50166},{\"end\":50180,\"start\":50175},{\"end\":50189,\"start\":50184}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11212020},\"end\":37474,\"start\":37019},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3989134},\"end\":38060,\"start\":37476},{\"attributes\":{\"id\":\"b2\"},\"end\":38237,\"start\":38062},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":150373675},\"end\":39057,\"start\":38239},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b4\"},\"end\":39384,\"start\":39059},{\"attributes\":{\"doi\":\"CoRR abs/2002.08155\",\"id\":\"b5\"},\"end\":39763,\"start\":39386},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":47021242},\"end\":40150,\"start\":39765},{\"attributes\":{\"doi\":\"abs/2009.08366\",\"id\":\"b7\"},\"end\":40682,\"start\":40152},{\"attributes\":{\"doi\":\"10.1002/spe.2736\",\"id\":\"b8\",\"matched_paper_id\":201877664},\"end\":41043,\"start\":40684},{\"attributes\":{\"id\":\"b9\"},\"end\":41350,\"start\":41045},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9203280},\"end\":41796,\"start\":41352},{\"attributes\":{\"id\":\"b11\"},\"end\":42149,\"start\":41798},{\"attributes\":{\"id\":\"b12\"},\"end\":42460,\"start\":42151},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":214802082},\"end\":42889,\"start\":42462},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":204715246},\"end\":43311,\"start\":42891},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":31990},\"end\":43671,\"start\":43313},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":184488324},\"end\":44182,\"start\":43673},{\"attributes\":{\"id\":\"b17\"},\"end\":44568,\"start\":44184},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10991311},\"end\":45033,\"start\":44570},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15813825},\"end\":45558,\"start\":45035},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11339499},\"end\":46095,\"start\":45560},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6875312},\"end\":46764,\"start\":46097},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4808574},\"end\":47045,\"start\":46766},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":47019554},\"end\":47664,\"start\":47047},{\"attributes\":{\"id\":\"b24\"},\"end\":47864,\"start\":47666},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":203593801},\"end\":48378,\"start\":47866},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":52069701},\"end\":48940,\"start\":48380},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":222142942},\"end\":49296,\"start\":48942},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":86524089},\"end\":49665,\"start\":49298},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":174799700},\"end\":50105,\"start\":49667},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":221103703},\"end\":50489,\"start\":50107}]", "bib_title": "[{\"end\":37088,\"start\":37019},{\"end\":37553,\"start\":37476},{\"end\":38273,\"start\":38239},{\"end\":39781,\"start\":39765},{\"end\":40751,\"start\":40684},{\"end\":41428,\"start\":41352},{\"end\":42516,\"start\":42462},{\"end\":42997,\"start\":42891},{\"end\":43381,\"start\":43313},{\"end\":43711,\"start\":43673},{\"end\":44623,\"start\":44570},{\"end\":45123,\"start\":45035},{\"end\":45613,\"start\":45560},{\"end\":46149,\"start\":46097},{\"end\":46822,\"start\":46766},{\"end\":47093,\"start\":47047},{\"end\":47939,\"start\":47866},{\"end\":48457,\"start\":48380},{\"end\":49002,\"start\":48942},{\"end\":49368,\"start\":49298},{\"end\":49738,\"start\":49667},{\"end\":50148,\"start\":50107}]", "bib_author": "[{\"end\":37102,\"start\":37090},{\"end\":37109,\"start\":37102},{\"end\":37119,\"start\":37109},{\"end\":37565,\"start\":37555},{\"end\":37575,\"start\":37565},{\"end\":37585,\"start\":37575},{\"end\":37594,\"start\":37585},{\"end\":38111,\"start\":38100},{\"end\":38289,\"start\":38275},{\"end\":38295,\"start\":38289},{\"end\":38302,\"start\":38295},{\"end\":38309,\"start\":38302},{\"end\":38320,\"start\":38309},{\"end\":39150,\"start\":39140},{\"end\":39161,\"start\":39150},{\"end\":39168,\"start\":39161},{\"end\":39181,\"start\":39168},{\"end\":39463,\"start\":39455},{\"end\":39470,\"start\":39463},{\"end\":39478,\"start\":39470},{\"end\":39486,\"start\":39478},{\"end\":39494,\"start\":39486},{\"end\":39502,\"start\":39494},{\"end\":39510,\"start\":39502},{\"end\":39517,\"start\":39510},{\"end\":39524,\"start\":39517},{\"end\":39533,\"start\":39524},{\"end\":39541,\"start\":39533},{\"end\":39789,\"start\":39783},{\"end\":39798,\"start\":39789},{\"end\":39805,\"start\":39798},{\"end\":40224,\"start\":40217},{\"end\":40231,\"start\":40224},{\"end\":40237,\"start\":40231},{\"end\":40245,\"start\":40237},{\"end\":40253,\"start\":40245},{\"end\":40260,\"start\":40253},{\"end\":40268,\"start\":40260},{\"end\":40276,\"start\":40268},{\"end\":40292,\"start\":40276},{\"end\":40298,\"start\":40292},{\"end\":40308,\"start\":40298},{\"end\":40318,\"start\":40308},{\"end\":40331,\"start\":40318},{\"end\":40340,\"start\":40331},{\"end\":40354,\"start\":40340},{\"end\":40361,\"start\":40354},{\"end\":40370,\"start\":40361},{\"end\":40378,\"start\":40370},{\"end\":40762,\"start\":40753},{\"end\":40770,\"start\":40762},{\"end\":40779,\"start\":40770},{\"end\":41126,\"start\":41116},{\"end\":41132,\"start\":41126},{\"end\":41141,\"start\":41132},{\"end\":41154,\"start\":41141},{\"end\":41170,\"start\":41154},{\"end\":41439,\"start\":41430},{\"end\":41451,\"start\":41439},{\"end\":41463,\"start\":41451},{\"end\":41475,\"start\":41463},{\"end\":41883,\"start\":41876},{\"end\":41891,\"start\":41883},{\"end\":41902,\"start\":41891},{\"end\":41912,\"start\":41902},{\"end\":41922,\"start\":41912},{\"end\":41933,\"start\":41922},{\"end\":42281,\"start\":42270},{\"end\":42293,\"start\":42281},{\"end\":42529,\"start\":42518},{\"end\":42538,\"start\":42529},{\"end\":42544,\"start\":42538},{\"end\":42556,\"start\":42544},{\"end\":43005,\"start\":42999},{\"end\":43013,\"start\":43005},{\"end\":43025,\"start\":43013},{\"end\":43039,\"start\":43025},{\"end\":43395,\"start\":43383},{\"end\":43412,\"start\":43395},{\"end\":43421,\"start\":43412},{\"end\":43430,\"start\":43421},{\"end\":43441,\"start\":43430},{\"end\":43450,\"start\":43441},{\"end\":43720,\"start\":43713},{\"end\":43727,\"start\":43720},{\"end\":43737,\"start\":43727},{\"end\":43750,\"start\":43737},{\"end\":43761,\"start\":43750},{\"end\":44191,\"start\":44184},{\"end\":44198,\"start\":44191},{\"end\":44207,\"start\":44198},{\"end\":44213,\"start\":44207},{\"end\":44222,\"start\":44213},{\"end\":44230,\"start\":44222},{\"end\":44238,\"start\":44230},{\"end\":44247,\"start\":44238},{\"end\":44262,\"start\":44247},{\"end\":44274,\"start\":44262},{\"end\":44631,\"start\":44625},{\"end\":44638,\"start\":44631},{\"end\":44646,\"start\":44638},{\"end\":44652,\"start\":44646},{\"end\":44660,\"start\":44652},{\"end\":45131,\"start\":45125},{\"end\":45140,\"start\":45131},{\"end\":45147,\"start\":45140},{\"end\":45155,\"start\":45147},{\"end\":45164,\"start\":45155},{\"end\":45172,\"start\":45164},{\"end\":45627,\"start\":45615},{\"end\":45640,\"start\":45627},{\"end\":45654,\"start\":45640},{\"end\":45661,\"start\":45654},{\"end\":45667,\"start\":45661},{\"end\":46159,\"start\":46151},{\"end\":46170,\"start\":46159},{\"end\":46179,\"start\":46170},{\"end\":46189,\"start\":46179},{\"end\":46204,\"start\":46189},{\"end\":46214,\"start\":46204},{\"end\":46224,\"start\":46214},{\"end\":46239,\"start\":46224},{\"end\":46831,\"start\":46824},{\"end\":46840,\"start\":46831},{\"end\":46847,\"start\":46840},{\"end\":46854,\"start\":46847},{\"end\":46860,\"start\":46854},{\"end\":47106,\"start\":47095},{\"end\":47112,\"start\":47106},{\"end\":47120,\"start\":47112},{\"end\":47127,\"start\":47120},{\"end\":47134,\"start\":47127},{\"end\":47145,\"start\":47134},{\"end\":47718,\"start\":47706},{\"end\":47729,\"start\":47718},{\"end\":47948,\"start\":47941},{\"end\":47955,\"start\":47948},{\"end\":47962,\"start\":47955},{\"end\":47968,\"start\":47962},{\"end\":47976,\"start\":47968},{\"end\":47982,\"start\":47976},{\"end\":47990,\"start\":47982},{\"end\":48466,\"start\":48459},{\"end\":48474,\"start\":48466},{\"end\":48482,\"start\":48474},{\"end\":48488,\"start\":48482},{\"end\":48496,\"start\":48488},{\"end\":48502,\"start\":48496},{\"end\":48510,\"start\":48502},{\"end\":49012,\"start\":49004},{\"end\":49018,\"start\":49012},{\"end\":49026,\"start\":49018},{\"end\":49033,\"start\":49026},{\"end\":49040,\"start\":49033},{\"end\":49377,\"start\":49370},{\"end\":49392,\"start\":49377},{\"end\":49399,\"start\":49392},{\"end\":49749,\"start\":49740},{\"end\":49757,\"start\":49749},{\"end\":49766,\"start\":49757},{\"end\":49773,\"start\":49766},{\"end\":49781,\"start\":49773},{\"end\":49788,\"start\":49781},{\"end\":50157,\"start\":50150},{\"end\":50164,\"start\":50157},{\"end\":50173,\"start\":50164},{\"end\":50182,\"start\":50173},{\"end\":50191,\"start\":50182}]", "bib_venue": "[{\"end\":37195,\"start\":37177},{\"end\":37779,\"start\":37686},{\"end\":38684,\"start\":38523},{\"end\":39954,\"start\":39879},{\"end\":42689,\"start\":42631},{\"end\":43954,\"start\":43866},{\"end\":44767,\"start\":44747},{\"end\":45262,\"start\":45246},{\"end\":45825,\"start\":45741},{\"end\":46412,\"start\":46337},{\"end\":47370,\"start\":47250},{\"end\":48092,\"start\":48074},{\"end\":48679,\"start\":48603},{\"end\":49452,\"start\":49430},{\"end\":37175,\"start\":37119},{\"end\":37684,\"start\":37594},{\"end\":38098,\"start\":38062},{\"end\":38480,\"start\":38320},{\"end\":39138,\"start\":39059},{\"end\":39453,\"start\":39386},{\"end\":39877,\"start\":39805},{\"end\":40215,\"start\":40152},{\"end\":40835,\"start\":40815},{\"end\":41114,\"start\":41045},{\"end\":41550,\"start\":41475},{\"end\":41874,\"start\":41798},{\"end\":42268,\"start\":42151},{\"end\":42629,\"start\":42556},{\"end\":43086,\"start\":43039},{\"end\":43473,\"start\":43450},{\"end\":43864,\"start\":43761},{\"end\":44345,\"start\":44290},{\"end\":44745,\"start\":44660},{\"end\":45244,\"start\":45172},{\"end\":45739,\"start\":45667},{\"end\":46307,\"start\":46239},{\"end\":46890,\"start\":46860},{\"end\":47248,\"start\":47145},{\"end\":47704,\"start\":47666},{\"end\":48072,\"start\":47990},{\"end\":48601,\"start\":48510},{\"end\":49104,\"start\":49040},{\"end\":49428,\"start\":49399},{\"end\":49862,\"start\":49788},{\"end\":50274,\"start\":50191}]"}}}, "year": 2023, "month": 12, "day": 17}