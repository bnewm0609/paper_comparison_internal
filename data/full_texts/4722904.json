{"id": 4722904, "updated": "2023-09-28 16:13:34.404", "metadata": {"title": "Leveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction", "authors": "[{\"first\":\"Charles\",\"last\":\"Corbiere\",\"middle\":[]},{\"first\":\"Hedi\",\"last\":\"Ben-Younes\",\"middle\":[]},{\"first\":\"Alexandre\",\"last\":\"Ram'e\",\"middle\":[]},{\"first\":\"Charles\",\"last\":\"Ollion\",\"middle\":[]}]", "venue": "2017 IEEE International Conference on Computer Vision Workshop (ICCVW)", "journal": null, "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "In this paper, we present a method to learn a visual representation adapted for e-commerce products. Based on weakly supervised learning, our model learns from noisy datasets crawled on e-commerce website catalogs and does not require any manual labeling. We show that our representation can be used for downward classification tasks over clothing categories with different levels of granularity. We also demonstrate that the learnt representation is suitable for image retrieval. We achieve nearly state-of-art results on the DeepFashion In-Shop Clothes Retrieval and Categories Attributes Prediction tasks, without using the provided training set.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1709.09426", "mag": "2963781085", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccvw/CorbiereBRO17", "doi": "10.1109/iccvw.2017.266"}}, "content": {"source": {"pdf_hash": "1f795a97c0c1b6d2a1ddb28d290eddee8420484a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1709.09426v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1709.09426", "status": "GREEN"}}, "grobid": {"id": "3181e6e16004fe6c9fa1a77396650c0d50ca2681", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1f795a97c0c1b6d2a1ddb28d290eddee8420484a.txt", "contents": "\nLeveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction\n\n\nCharles Corbi\u00e8re corbiere@heuritech.com \nHeuritech, ParisFrance\n\nHedi Ben-Younes hbenyounes@heuritech.com \nHeuritech, ParisFrance\n\nUPMC-LIP6\nParisFrance\n\nAlexandre Ram\u00e9 rame@heuritech.com \nHeuritech, ParisFrance\n\nCharles Ollion ollion@heuritech.com \nHeuritech, ParisFrance\n\nLeveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction\n\nIn this paper, we present a method to learn a visual representation adapted for e-commerce products. Based on weakly supervised learning, our model learns from noisy datasets crawled on e-commerce website catalogs and does not require any manual labeling. We show that our representation can be used for downward classification tasks over clothing categories with different levels of granularity. We also demonstrate that the learnt representation is suitable for image retrieval. We achieve nearly state-of-art results on the DeepFashion In-Shop Clothes Retrieval and Categories Attributes Prediction [12] tasks, without using the provided training set.\n\nIntroduction\n\nWhile online shopping has been an exponentially growing market for the last two decades, finding exactly what you want from online shops is still not a solved problem. Traditional fashion search engines allow consumers to look for products based on well chosen keywords. Such engines match those textual queries with meta-data of products, such as a title, a description or a set of tags. In online luxury fashion for instance, they still play an important role to address this customer pain point: 46% of customers use a search engine to find a specific product; 31% use it to find the brand they're looking for 1 . However, those meta-data informations may be incomplete, or use a biased vocabulary. For instance, a description may denote as \"marini\u00e8re\" a long sleeves shirt with blue/white stripes. It then appears crucial for online retailers to have a rich labeled catalog to ensure good search. Moreover, these search engines don't incorporate the visual information of the image associated to the product. Computer vision for fashion e-commerce images has drawn an increasing interest in the last decade. It has been used for similarity search [20,11,21,18], automatic image tagging [10,2], fine-grained classification [5,12] or N-shot learning [1]. In all of these tasks, a model's performance is highly dependent on a visual feature extractor. Using a Convolutional Neural Network (CNN) trained on ImageNet [4] provides a good baseline. However, there are two main problems with this representation. First, it has been trained on an image distribution that is very far from e-commerce, as it has never (or rarely) seen such pictures. Second, the set of classes it has been trained on is different from a set of classes that could be meaningful in e-commerce. A useful representation should separate different types of clothing (e.g. a skirt and a dress), but it should also discriminate between different lengths of sleeves for shirts, trouser cuts, types of handbags, textures, colors, shapes,...\n\nOur goal is to learn a visual feature extractor designed for e-commerce images. This representation should:\n\n\u2022 encode multiple levels of visual semantics: from low level signals (color, shapes, textures, fabric,...) to high Figure 2. Training of our model: predict one label, picked from the bag-of-words description, from an image. Both image and words are embedded before being coupled in a dot product. We output a probability for each word in the vocabulary.\n\nlevel information (style, brand), \u2022 be separable over visual concepts, so we can train very simple classifiers over clothing types, colors, attributes, textures, etc., \u2022 provide a meaningful similarity between images, so we can use it in the context of image retrieval.\n\nTo these ends, we train a visual feature extractor on a large set of weakly annotated images crawled from the Internet. These annotations correspond to the textual description associated to the image. The model is learned on a dataset at zero labeling cost, and is exclusively constituted of data points extracted from e-commerce websites. Our main contribution is an in-depth analysis of the model presented in [9], through applications to fashion image recognition tasks such as image retrieval and attribute tagging. We also improved the method by upgrading the CNN architecture and dealt with multiple languages, mainly English and French. In Section 2, we explain the model, how we handle noise in the dataset, as well as some implementation details. In Section 3, we provide results given by our representation on image retrieval and classification, over multiple datasets. Finally in Section 4, we conclude and go over some possible improvement tracks.\n\n\nLearning Image and Text Embeddings with Weak Supervision\n\nOne major issue in applied machine learning for fashion is the lack of large clothing e-commerce datasets with a rich, unique and clean labeling. Some very interesting work has been done on collecting datasets for fashion [11,12]. However, we believe it is very hard to be exhaustive in describing every visual attribute (pieces of clothing, texture, color, shape, etc.) in an image. Moreover, even if this labeling work could be perfectly carried, it would come at very high cost, and should be manually done each time we wanted to add a new attribute. A possible source of annotated data is the e-commerce website catalogs. They provide a great amount of product images associated with descriptions, such as the one in Figure 1. While this description contains information about the visual concepts in the image, it also comes with a lot of noise that could harm the learning.\n\nWe explain now the approach we used to train a visual feature extractor on noisy weakly annotated data.\n\n\nWeakly Supervised Approach\n\nLearning with noisy labeled training data is not new to the machine learning and computer vision community [6,17,19]. Label noise in image classification [22] usually refers to errors in labeling, or to cases where the image does not belong to any of the classes, but mistakenly has one of the corresponding labels. In our setting, in addition to these types of noise, there are some labels in the classes vocabulary that are not relevant to any input. Text descriptions are noisy as they contain common words (e.g. 'we', 'present'), subjective words (e.g. 'wonderful') or non visual words (e.g. 'xl', 'cm'), which are not related to the input image. As we don't have any prior information on which labels are relevant and which are not, we keep the preprocessing of textual data as light as possible.\n\n\nModel\n\nOur work builds upon the one presented in [9], which we explain in this section. The model's training scheme is exposed in Figure 2  Let x \u2208 I be an image, and y \u2208 {0, 1} K the associated multi-label vector, such that \u2200k \u2208 [1, K], y k = 1 if the k-th label of the vocabulary is true for the image x. We use a CNN to compute a visual feature z = f (x, \u03b8) \u2208 R I , where \u03b8 are the weights of our convolutional neural network. This image embedding is given to a classification layer:\ny = sof tmax W T z (1) where W \u2208 R I\u00d7K . Note that for all k \u2208 [1, K], the column vector in w k = W [:, k]\ncorresponds to the embedding of the k-th word in the vocabulary.\n\n\nLabel imbalance management\n\nAs seen on Figure 3, the distribution of words in our dataset is highly unbalanced. Due to our minimal preprocessing, we observe a high frequency for some non-visual words such as \"xl\", \"cm\" or \"size\" as they appear very frequently in descriptions. Many examples contain those non-visual words that our model would be asked to predict, which is likely to harm the training.\n\nTo overcome this issue, and as it was done in [9], we perform uniform sampling. Specifically, during training, we sample uniformly a word w from the vocabulary. We then randomly choose an image x whose bag-of-words contains w and we try to predict w given x.\n\n\nLoss\n\nAs we want to predict one label among a vocabulary K for each image, we use the cross-entropy loss. It minimizes the negative sum of the log-probabilities over all positive labels\nL(\u03b8, W, D) = \u2212 1 N N n=1 K k=1 y k n log exp (w T k f (x n , \u03b8)) K i=1 exp (w T i f (x n , \u03b8))(2)\n\nImplementation details Negative sampling\n\nWe operate in a context where the vocabulary can be of arbitrary size. Computing probabilities for all those classes for each sample can be very slow. Negative sampling [16] is one way of addressing this problem. After selecting a positive label for an image sample, we randomly draw N neg negative words within the vocabulary. We compute the scores and the softmax only over those chosen words.\n\n\nLearning\n\nWe trained our model with stochastic gradient descent (SGD) on batch of size 20. We consider that an epoch is achieved when the model saw a number of images equivalent to 1/10 of the dataset size, which is approximately 1.3M images in total. After each epoch, we compute a validation error based on a held-out validation set. The initial rate was set to 0.1 and divided by 10 after 10 epochs without improvement. We stop the training after 20 epochs without improvements on our validation dataset. We use the ResNet50 architecture [7] for the visual feature extractor f (x, \u03b8), with pre-trained weights on ImageNet. Because the last layer has been initialized randomly, we start by learning only the last layer W for 20 epochs, and then we fine-tune the parameters \u03b8 in the CNN.\n\n\nTraining dataset\n\nWe built a dataset of about 1,3M images and their associated labels from multiple e-commerce website sources, mostly French, English and Italian. We crawled most of the time one image per product, except when multiple images where available. In that case, we consider them as four different samples with same associated bag of labels.\n\nFor each source, we select the relevant fields to keep (title, category name, description,...) and concatenate them. After lower-casing and removing punctuation, we use the RegexpTokenizer provided by NLTK [13] to get a list of words. We remove stopwords, frequent non-relevant words (name of the website, 'collection', 'buy',...) and non alphabetic words. Our final dataset is a list of product images, associated to their respective bag-of-words obtained by the previous preprocessing.\n\nWe have deliberately kept preprocessing as minimal as possible, so it is easy to scale to many sources. Thus, we need our model to adapt to this noise in the data. After preprocessing and aggregating the multiple sources, our final vocabulary is composed of 218,536 words. We chose to restrain the vocabulary to the 30,000 most frequent words. The average number of labels per sample is 26,88. We split our dataset into a training and a validation dataset. The validation set is made of the same labels as the training dataset and represent 0.5% of the total size.\n\n\nExperiments and evaluation\n\nAfter learning the representation on our large weakly annotated dataset, we want to evaluate this representation. To what extent is this representation useful for tasks such as garment classification, attribute tagging or image retrieval ?\n\n\nEvaluation datasets\n\nWe evaluate our representation on 5 datasets: two public datasets (DeepFashion) used for tagging and image retrieval; three in-house datasets used respectively for category classification, fine-grained classification and image retrieval.\n\n\nDeepFashion Categories and Attributes Prediction\n\nevaluates the performance on clothing category classification, and on attribute prediction (multi-labelling). It contains 289,222 images for 50 clothing categories and 1,000 clothing attributes. While an image can only be affected to one class, it can be associated to multiple labels. The average number of labels for an image is 3.38. For each image in train and test sets, we select a crop available from a ground truth bounding box. ClothingType We have labeled a dataset with 18 classes, each one corresponding to a garment type (e.g. bag, dress, pants, shoes, ...). This in-house dataset contains approximately 736,000 images.\n\nHandBag In addition to the previous dataset, this inhouse dataset focuses on bags for fine-grained recognition. Here, the differences between classes are more subtle: bucket bag, doctor bag, duffel bag, etc... It contains 3,060 samples within 13 classes, each one corresponding to a specific type of handbag.\n\nDress Retrieval This in-house similarity dataset was gathered by crawling an e-commerce website. We collected a list of sets of images, each corresponding to the same item. We used an image classifier to filter out all non-dress items. The final dataset contains 9,009 items for training (20,200 images) and 1,001 items for testing. On this dataset, we keep only images where clothing are worn on humans.\n\n\nImage retrieval\n\nIn this task, given a query image containing an item, we aim at retrieving images that contain the same item. To do so, we compute the score between two images using the cosine similarity between their representation. For a given query image, we sort all gallery images in decreasing order of similarity, and evaluate our retrieval performance using top-k retrieval accuracy, as in [12,23]. For a given test query image, we give the model a score of 1 if an image of the same item is within the k highest scoring gallery images, 0 else. We adopt this metric for both our image retrieval datasets (DeepFashion In-Shop Retrieval and Dress Retrieval). In Figure 5, we show the results on top-k retrieval accuracy on DeepFashion In-shop Retrieval dataset, for multiple values of k. FashionNet corresponds to the model presented in [12], and HDC+Contrastive is the model in [23]. We denote by [F] (resp. [C]) models that use the full image (resp. an image cropped on the product) to compute retrieval scores. We provide the ImageNet baseline both [F] and [C] models, where we use as feature extractor the penultimate layer of a CNN trained on ImageNet. We would like to emphasize on the fact that our Weakly model, as well as the ImageNet baseline, do not use the training set of DeepFashion In-shop Retrieval, unlike HDC+Contrastive and FashionNet. First, we note that not using bounding boxes for our Im-ageNet baseline or our Weakly model considerably increase accuracy. Our intuition is that human models in the Deep-Fashion In-shop dataset often wear the same ensemble of items together, meaning for one shirt item considered for instance, the human model would be wearing the same pants and shoes on all item's image. As a consequence of this bias, it seems easier to evaluate similarity on a ensemble of clothings than on a single clothing on this dataset.\n\nOur Weakly model without crop performs as well as FashionNet, and even outperforms it when k \u2265 20: considering top-20 retrieval accuracy, it predicts the correct item 78,1% of the time, against 76,4% for FashionNet. Besides, in both the [F] and [C] setups, our Weakly model improves over the ImageNet baseline (from 48% to 78.1% for [F], and from 43.7% to 64.0% for [C]). This validates our hypothesis that our model has learned a specific e-commerce representation. In Figure 4, we show an example of a query image, its top-5 similar images according to our weakly learned visual features, and its top-5 similar images according to ImageNet based visual features. As we can see, the similarity encoded by network trained on ImageNet brings together products that are on a same coarse semantic concept, while our representation encodes a more precise and rich closeness, which is based not only the image type, but also on their shape, texture, and fabric. Plus, our representation seems less dependent to human model's pose.\n\nOn our in-house dress retrieval dataset, we also observed that the Weakly model improved over ImageNet Model on retrieval accuracy. The Weakly model obtained a top-20 retrieval accuracy of 83,71%, against 65,65% for the Im-ageNet model. Once again, we point out that we do not perform any training on the retrieval task of this dataset.\n\n\nTagging\n\nWe conducted multi-class classification and multilabelling experiments to assess the quality of our visual representation on transfer learning. On the public DeepFashion Categories dataset, we pre-computed images representation using our Weakly image feature extractor on image crops. Then, we train a simple classifier using a fully-connected layer followed by a softmax activation function. The results are shown on the Table 3.1, at the column Category. With this simple classifier, our results are on par with the stateof-art model by Lu et al. [14].\n\nOn DeepFashion Attributes, we train a fully-connected layer with a sigmoid output and a binary cross-entropy loss. As we can see in Table 3.1, our model significantly improves over previous state-of-the-art on textures and shape labels top-k recall. However, part and style attributes seem more difficult to separate for our Weakly representation. This might be due to the fact that texture-like and shapelike labels are more represented than part and style words in the large weak dataset. This would require further investigation.\n\nWe carried out experiments on our in-house Clothing-Type dataset where images are annotated according to their clothing category (such as bags, shirt, dress, shoes, etc.).   Table 3. AUC classification score for fine-grained type of bags Table 3.3 shows the improvement on AUC scores over the ImageNet model for each of the clothing categories using our new representation. This indicates that our training scheme was able to learn discriminative features for garment classification. Finally, we now focus on a fine-grained recognition task. The HandBag dataset contains images annotated with their specific type of bag. In this dataset, the differences between classes are more subtle than in the ClothingType dataset. The training and evaluation are the same as for the previous experiment. As in the previous experiment, we improved AUC scores for nearly each type of bags (see Table 3.3).\n\n\nExploratory visualization using t-SNE\n\nTo obtain some insight about our Weakly representation, we applied t-SNE [15] on features extracted using our Weakly feature extractor. We did this for 1,000 images from DeepFashion Categories test set. Figure 6 shows full map and some interesting close-ups. On top left (a), we can see a cluster of dresses sub-divised into multiple sub-clusters corresponding to different colors. The cluster (b) shows a focus on black pants. In the zone (c), we can easily see that the model gathered images containing stripes, and it seems like it has separated tops from dresses inside this cluster (with large striped sweaters on top). Checked clothings are grouped in cluster (d), while printed t-shirts are represented in cluster (e). This plot shows that our representation is able to group together concepts that are close in terms of clothing type, texture, color and style. z\n\n\nDiscussion and Future Work\n\nWe presented in the future a method to learn a visual representation adapted to fashion. This method has the major advantage to overcome the issue of finding a large and clean e-commerce dataset. The results shows clear improvements compared to a visual representation trained on ImageNet, improving performance on multiple tasks such as image retrieval, classification and fine-grained recognition.\n\nIn the future, we would like to investigate on the possibility to better train our visual feature extractor using an external knowledge base of textual concepts.\n\nFigure 1 .\n1Our dataset is composed of images and a few associated text descriptors such as their title and their description\n\nFigure 3 .\n3Most frequent labels in training dataset.\n\nFigure 4 .\n4Comparison between visual similar images from DeepFashion In-Shop dataset according to our weakly learned visual features and ImageNet based visual features. Our representation seems more robust to human pose (on the left) and successfully captured finegrained concepts such as stripes (on the right).\n\nDeepFashion\nIn-Shop Retrieval contains 7,982 clothing items with 52,712 images. 3,997 classes are for training (25,882 images) and 3,985 items are for testing (28,760 images). The test set is composed of a query set and a gallery set, where query set contains 14,218 images of 3,985 items and database set contains 12,612 images of 3,985 items. As in the Categories and Attributes Prediction benchmark, we cropped each image using a ground truth bounding box.\n\nFigure 5 .\n5Retrieval accuracy for top-k (k=1,5,10,20,30,40,50). We give the top-20 retrieval accuracy between brackets for each model in the caption.\n\nFigure 6 .\n6t-SNE map of 1,000 image samples from DeepFashion Categories dataset based on our Weakly image features extractor. We can identify some local subcategories, such as colorful dresses (a), black pants (b), stripes (c), checked (d) or printed shirt (e).\n\n\nTable 2. AUC classification score for clothing categories backpack baguette bowling bag bucket bag doctor bag duffel bag hobo bag luggage clutch saddle bag satchelbag \nbelt body \nbra \ncoat combi dress eyewear gloves \nhat neckwear pants shoes shorts \nskirt socks \ntop underpants \nImageNet 99.63 98.02 98.20 99.27 99.00 96.01 98.68 \n99.93 \n99.99 99.45 \n99.18 99.46 99.87 99.23 98.67 99.35 98.67 \n99.5 \nWeakly \n99.86 99.46 99.08 99.67 99.31 98.11 99.13 \n99.99 \n99.97 99.73 \n99.33 99.56 99.93 99.32 99.21 99.83 99.26 \n99.67 \n\ntote trapeze \nImageNet \n95.15 \n87.63 \n90.42 \n94.35 \n90.99 \n87.97 \n92.73 \n87.65 96.52 \n91.77 \n88.58 96.77 \n92.11 \nWeakly \n95.94 \n91.85 \n92.13 \n94.87 \n91.59 \n90.12 \n95.19 \n86.96 97.24 \n93.12 \n91.45 97.64 \n93.61 \n\n\nhttp://www.mckinsey.com/business-functions/marketing-andsales/our-insights/the-opportunity-in-online-luxury-fashion\nAcknowledgmentsThe authors would like to thank all the Heuritech team for providing an efficient network infrastructure. We'd also like to thank Alexandre Ram\u00e9 for his help on the transfer learning evaluation task.\nExploiting weakly-labeled web images to improve object classification: a domain adaptation approach. A Bergamo, L Torresani, NIPS. J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. CulottaCurran Associates, IncA. Bergamo and L. Torresani. Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, NIPS, pages 181-189. Curran Associates, Inc., 2010.\n\nApparel classification with style. L Bossard, M Dantone, C Leistner, C Wengert, T Quack, L Van Gool, Proceedings of the 11th Asian Conference on Computer Vision -Volume Part IV, ACCV'12. the 11th Asian Conference on Computer Vision -Volume Part IV, ACCV'12Berlin, HeidelbergSpringer-VerlagL. Bossard, M. Dantone, C. Leistner, C. Wengert, T. Quack, and L. Van Gool. Apparel classification with style. In Pro- ceedings of the 11th Asian Conference on Computer Vision - Volume Part IV, ACCV'12, pages 321-335, Berlin, Heidel- berg, 2013. Springer-Verlag.\n\nDescribing clothing by semantic attributes. H Chen, A C Gallagher, B Girod, Lecture Notes in Computer Science. A. W. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato, and C. Schmid75743SpringerH. Chen, A. C. Gallagher, and B. Girod. Describing cloth- ing by semantic attributes. In A. W. Fitzgibbon, S. Lazeb- nik, P. Perona, Y. Sato, and C. Schmid, editors, ECCV (3), volume 7574 of Lecture Notes in Computer Science, pages 609-623. Springer, 2012.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L. jia Li, K. Li, and L. Fei-fei. Imagenet: A large-scale hierarchical image database. In In CVPR, 2009.\n\nStyle finder: Fine-grained clothing style recognition and retrieval. W Di, C Wah, A Bhardwaj, R Piramuthu, N Sundaresan, IEEE International Workshop on Mobile Vision. Portland, ORW. Di, C. Wah, A. Bhardwaj, R. Piramuthu, and N. Sundare- san. Style finder: Fine-grained clothing style recognition and retrieval. In IEEE International Workshop on Mobile Vision, Portland, OR, 2013.\n\nClassification in the presence of label noise: A survey. B Frnay, M Verleysen, IEEE Transactions on Neural Networks and Learning Systems. 255B. Frnay and M. Verleysen. Classification in the presence of label noise: A survey. IEEE Transactions on Neural Net- works and Learning Systems, 25(5):845-869, 2014.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, arXiv:1512.03385arXiv preprintK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. arXiv preprint arXiv:1512.03385, 2015.\n\nCross-domain image retrieval with a dual attribute-aware ranking network. J Huang, R S Feris, Q Chen, S Yan, abs/1505.07922CoRRJ. Huang, R. S. Feris, Q. Chen, and S. Yan. Cross-domain image retrieval with a dual attribute-aware ranking network. CoRR, abs/1505.07922, 2015.\n\nLearning visual features from large weakly supervised data. A Joulin, L Van Der Maaten, A Jabri, N Vasilache, abs/1511.02251CoRRA. Joulin, L. van der Maaten, A. Jabri, and N. Vasilache. Learning visual features from large weakly supervised data. CoRR, abs/1511.02251, 2015.\n\nGetting the look: Clothing recognition and segmentation for automatic product suggestions in everyday photos. Y Kalantidis, L Kennedy, L.-J Li, Proceedings of the 3rd ACM Conference on International Conference on Multimedia Retrieval, ICMR '13. the 3rd ACM Conference on International Conference on Multimedia Retrieval, ICMR '13New York, NY, USAACMY. Kalantidis, L. Kennedy, and L.-J. Li. Getting the look: Clothing recognition and segmentation for automatic prod- uct suggestions in everyday photos. In Proceedings of the 3rd ACM Conference on International Conference on Multi- media Retrieval, ICMR '13, pages 105-112, New York, NY, USA, 2013. ACM.\n\nWhere to buy it: Matching street clothing photos in online shops. M H Kiapour, X Han, S Lazebnik, A C Berg, T L Berg, ICCV. IEEE Computer SocietyM. H. Kiapour, X. Han, S. Lazebnik, A. C. Berg, and T. L. Berg. Where to buy it: Matching street clothing photos in online shops. In ICCV, pages 3343-3351. IEEE Computer Society, 2015.\n\nTang, undefined, undefined, undefined, and undefined. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. Z Liu, P Luo, S Qiu, X Wang, X , IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 00Z. Liu, P. Luo, S. Qiu, X. Wang, X. Tang, undefined, un- defined, undefined, and undefined. Deepfashion: Power- ing robust clothes recognition and retrieval with rich anno- tations. 2016 IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), 00:1096-1104, 2016.\n\nNltk: The natural language toolkit. E Loper, S Bird, Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics. the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational LinguisticsE. Loper and S. Bird. Nltk: The natural language toolkit. In In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Process- ing and Computational Linguistics. Philadelphia: Associa- tion for Computational Linguistics, 2002.\n\nFully-adaptive feature sharing in multi-task networks with applications in person attribute classification. Y Lu, A Kumar, S Zhai, Y Cheng, T Javidi, R S Feris, abs/1611.05377CoRRY. Lu, A. Kumar, S. Zhai, Y. Cheng, T. Javidi, and R. S. Feris. Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification. CoRR, abs/1611.05377, 2016.\n\nVisualizing data using t-sne. L V D Maaten, G Hinton, Journal of Machine Learning Research. 9L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(Nov):2579-2605, 2008.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in Neural Information Processing Systems. C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editorsCurran Associates, Inc26T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, edi- tors, Advances in Neural Information Processing Systems 26, pages 3111-3119. Curran Associates, Inc., 2013.\n\nLearning with noisy labels. N Natarajan, I S Dhillon, P K Ravikumar, A Tewari, Advances in Neural Information Processing Systems. C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editorsCurran Associates, Inc26N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari. Learning with noisy labels. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, edi- tors, Advances in Neural Information Processing Systems 26, pages 1196-1204. Curran Associates, Inc., 2013.\n\nDeep learning based large scale visual recommendation and search for e-commerce. D Shankar, S Narumanchi, H A Ananya, P Kompalli, K Chaudhury, abs/1703.02344CoRRD. Shankar, S. Narumanchi, H. A. Ananya, P. Kompalli, and K. Chaudhury. Deep learning based large scale vi- sual recommendation and search for e-commerce. CoRR, abs/1703.02344, 2017.\n\nLearning from noisy labels with deep neural networks. S Sukhbaatar, R Fergus, abs/1406.2080CoRRS. Sukhbaatar and R. Fergus. Learning from noisy labels with deep neural networks. CoRR, abs/1406.2080, 2014.\n\nLearning fine-grained image similarity with deep ranking. J Wang, Y Song, T Leung, C Rosenberg, J Wang, J Philbin, B Chen, Y Wu, CVPR. IEEE Computer SocietyJ. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen, and Y. Wu. Learning fine-grained image similarity with deep ranking. In CVPR, pages 1386-1393. IEEE Computer Society, 2014.\n\nMatching user photos to online products with robust deep features. X Wang, Z Sun, W Zhang, Y Zhou, Y.-G Jiang, J. R. Kender, J. R. Smith, J. Luo, S. Boll, and W. H. HsuACMX. Wang, Z. Sun, W. Zhang, Y. Zhou, and Y.-G. Jiang. Matching user photos to online products with robust deep features. In J. R. Kender, J. R. Smith, J. Luo, S. Boll, and W. H. Hsu, editors, ICMR, pages 7-14. ACM, 2016.\n\nLearning from massive noisy labeled data for image classification. T Xiao, T Xia, Y Yang, C Huang, X Wang, CVPR. IEEE Computer SocietyT. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, pages 2691-2699. IEEE Computer Society, 2015.\n\nHard-aware deeply cascaded embedding. Y Yuan, K Yang, C Zhang, abs/1611.05720CoRRY. Yuan, K. Yang, and C. Zhang. Hard-aware deeply cas- caded embedding. CoRR, abs/1611.05720, 2016.\n", "annotations": {"author": "[{\"end\":149,\"start\":85},{\"end\":238,\"start\":150},{\"end\":297,\"start\":239},{\"end\":358,\"start\":298}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":93},{\"end\":165,\"start\":155},{\"end\":253,\"start\":249},{\"end\":312,\"start\":306}]", "author_first_name": "[{\"end\":92,\"start\":85},{\"end\":154,\"start\":150},{\"end\":248,\"start\":239},{\"end\":305,\"start\":298}]", "author_affiliation": "[{\"end\":148,\"start\":126},{\"end\":214,\"start\":192},{\"end\":237,\"start\":216},{\"end\":296,\"start\":274},{\"end\":357,\"start\":335}]", "title": "[{\"end\":82,\"start\":1},{\"end\":440,\"start\":359}]", "venue": null, "abstract": "[{\"end\":1096,\"start\":442}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2267,\"start\":2263},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2270,\"start\":2267},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2273,\"start\":2270},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2276,\"start\":2273},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2306,\"start\":2302},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2308,\"start\":2306},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2341,\"start\":2338},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2344,\"start\":2341},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2367,\"start\":2364},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2531,\"start\":2528},{\"end\":3352,\"start\":3344},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4270,\"start\":4267},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5101,\"start\":5097},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5104,\"start\":5101},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5999,\"start\":5996},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6002,\"start\":5999},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6005,\"start\":6002},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6047,\"start\":6043},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6745,\"start\":6742},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7806,\"start\":7803},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8518,\"start\":8514},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9287,\"start\":9284},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10098,\"start\":10094},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12761,\"start\":12757},{\"end\":12764,\"start\":12761},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13279,\"start\":13275},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13282,\"start\":13279},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13724,\"start\":13720},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13766,\"start\":13762},{\"end\":15122,\"start\":15119},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16681,\"start\":16677},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18229,\"start\":18225}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19742,\"start\":19616},{\"attributes\":{\"id\":\"fig_1\"},\"end\":19797,\"start\":19743},{\"attributes\":{\"id\":\"fig_2\"},\"end\":20112,\"start\":19798},{\"attributes\":{\"id\":\"fig_3\"},\"end\":20573,\"start\":20113},{\"attributes\":{\"id\":\"fig_4\"},\"end\":20725,\"start\":20574},{\"attributes\":{\"id\":\"fig_5\"},\"end\":20989,\"start\":20726},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":21725,\"start\":20990}]", "paragraph": "[{\"end\":3118,\"start\":1112},{\"end\":3227,\"start\":3120},{\"end\":3582,\"start\":3229},{\"end\":3853,\"start\":3584},{\"end\":4814,\"start\":3855},{\"end\":5753,\"start\":4875},{\"end\":5858,\"start\":5755},{\"end\":6690,\"start\":5889},{\"end\":7179,\"start\":6700},{\"end\":7351,\"start\":7287},{\"end\":7755,\"start\":7382},{\"end\":8015,\"start\":7757},{\"end\":8203,\"start\":8024},{\"end\":8740,\"start\":8345},{\"end\":9531,\"start\":8753},{\"end\":9886,\"start\":9552},{\"end\":10375,\"start\":9888},{\"end\":10941,\"start\":10377},{\"end\":11211,\"start\":10972},{\"end\":11472,\"start\":11235},{\"end\":12157,\"start\":11525},{\"end\":12467,\"start\":12159},{\"end\":12873,\"start\":12469},{\"end\":14751,\"start\":12893},{\"end\":15778,\"start\":14753},{\"end\":16116,\"start\":15780},{\"end\":16682,\"start\":16128},{\"end\":17216,\"start\":16684},{\"end\":18110,\"start\":17218},{\"end\":19022,\"start\":18152},{\"end\":19452,\"start\":19053},{\"end\":19615,\"start\":19454}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7286,\"start\":7180},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8301,\"start\":8204}]", "table_ref": "[{\"end\":16557,\"start\":16550},{\"end\":16823,\"start\":16816},{\"end\":17399,\"start\":17392},{\"end\":17463,\"start\":17456},{\"end\":18106,\"start\":18099}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1110,\"start\":1098},{\"attributes\":{\"n\":\"2.\"},\"end\":4873,\"start\":4817},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5887,\"start\":5861},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6698,\"start\":6693},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7380,\"start\":7354},{\"attributes\":{\"n\":\"2.4.\"},\"end\":8022,\"start\":8018},{\"attributes\":{\"n\":\"2.5.\"},\"end\":8343,\"start\":8303},{\"end\":8751,\"start\":8743},{\"attributes\":{\"n\":\"2.6.\"},\"end\":9550,\"start\":9534},{\"attributes\":{\"n\":\"3.\"},\"end\":10970,\"start\":10944},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11233,\"start\":11214},{\"end\":11523,\"start\":11475},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12891,\"start\":12876},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16126,\"start\":16119},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18150,\"start\":18113},{\"attributes\":{\"n\":\"4.\"},\"end\":19051,\"start\":19025},{\"end\":19627,\"start\":19617},{\"end\":19754,\"start\":19744},{\"end\":19809,\"start\":19799},{\"end\":20125,\"start\":20114},{\"end\":20585,\"start\":20575},{\"end\":20737,\"start\":20727}]", "table": "[{\"end\":21725,\"start\":21155}]", "figure_caption": "[{\"end\":19742,\"start\":19629},{\"end\":19797,\"start\":19756},{\"end\":20112,\"start\":19811},{\"end\":20573,\"start\":20126},{\"end\":20725,\"start\":20587},{\"end\":20989,\"start\":20739},{\"end\":21155,\"start\":20992}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5604,\"start\":5596},{\"end\":6831,\"start\":6823},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7401,\"start\":7393},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13553,\"start\":13545},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15231,\"start\":15223},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18363,\"start\":18355}]", "bib_author_first_name": "[{\"end\":22159,\"start\":22158},{\"end\":22170,\"start\":22169},{\"end\":22601,\"start\":22600},{\"end\":22612,\"start\":22611},{\"end\":22623,\"start\":22622},{\"end\":22635,\"start\":22634},{\"end\":22646,\"start\":22645},{\"end\":22655,\"start\":22654},{\"end\":23163,\"start\":23162},{\"end\":23171,\"start\":23170},{\"end\":23173,\"start\":23172},{\"end\":23186,\"start\":23185},{\"end\":23618,\"start\":23617},{\"end\":23626,\"start\":23625},{\"end\":23634,\"start\":23633},{\"end\":23644,\"start\":23643},{\"end\":23650,\"start\":23649},{\"end\":23656,\"start\":23655},{\"end\":23877,\"start\":23876},{\"end\":23883,\"start\":23882},{\"end\":23890,\"start\":23889},{\"end\":23902,\"start\":23901},{\"end\":23915,\"start\":23914},{\"end\":24246,\"start\":24245},{\"end\":24255,\"start\":24254},{\"end\":24543,\"start\":24542},{\"end\":24549,\"start\":24548},{\"end\":24558,\"start\":24557},{\"end\":24565,\"start\":24564},{\"end\":24801,\"start\":24800},{\"end\":24810,\"start\":24809},{\"end\":24812,\"start\":24811},{\"end\":24821,\"start\":24820},{\"end\":24829,\"start\":24828},{\"end\":25061,\"start\":25060},{\"end\":25071,\"start\":25070},{\"end\":25089,\"start\":25088},{\"end\":25098,\"start\":25097},{\"end\":25386,\"start\":25385},{\"end\":25400,\"start\":25399},{\"end\":25414,\"start\":25410},{\"end\":25996,\"start\":25995},{\"end\":25998,\"start\":25997},{\"end\":26009,\"start\":26008},{\"end\":26016,\"start\":26015},{\"end\":26028,\"start\":26027},{\"end\":26030,\"start\":26029},{\"end\":26038,\"start\":26037},{\"end\":26040,\"start\":26039},{\"end\":26401,\"start\":26400},{\"end\":26408,\"start\":26407},{\"end\":26415,\"start\":26414},{\"end\":26422,\"start\":26421},{\"end\":26430,\"start\":26429},{\"end\":26816,\"start\":26815},{\"end\":26825,\"start\":26824},{\"end\":27593,\"start\":27592},{\"end\":27599,\"start\":27598},{\"end\":27608,\"start\":27607},{\"end\":27616,\"start\":27615},{\"end\":27625,\"start\":27624},{\"end\":27635,\"start\":27634},{\"end\":27637,\"start\":27636},{\"end\":27895,\"start\":27894},{\"end\":27899,\"start\":27896},{\"end\":27909,\"start\":27908},{\"end\":28159,\"start\":28158},{\"end\":28170,\"start\":28169},{\"end\":28183,\"start\":28182},{\"end\":28191,\"start\":28190},{\"end\":28193,\"start\":28192},{\"end\":28204,\"start\":28203},{\"end\":28733,\"start\":28732},{\"end\":28746,\"start\":28745},{\"end\":28748,\"start\":28747},{\"end\":28759,\"start\":28758},{\"end\":28761,\"start\":28760},{\"end\":28774,\"start\":28773},{\"end\":29307,\"start\":29306},{\"end\":29318,\"start\":29317},{\"end\":29332,\"start\":29331},{\"end\":29334,\"start\":29333},{\"end\":29344,\"start\":29343},{\"end\":29356,\"start\":29355},{\"end\":29625,\"start\":29624},{\"end\":29639,\"start\":29638},{\"end\":29835,\"start\":29834},{\"end\":29843,\"start\":29842},{\"end\":29851,\"start\":29850},{\"end\":29860,\"start\":29859},{\"end\":29873,\"start\":29872},{\"end\":29881,\"start\":29880},{\"end\":29892,\"start\":29891},{\"end\":29900,\"start\":29899},{\"end\":30197,\"start\":30196},{\"end\":30205,\"start\":30204},{\"end\":30212,\"start\":30211},{\"end\":30221,\"start\":30220},{\"end\":30232,\"start\":30228},{\"end\":30589,\"start\":30588},{\"end\":30597,\"start\":30596},{\"end\":30604,\"start\":30603},{\"end\":30612,\"start\":30611},{\"end\":30621,\"start\":30620},{\"end\":30866,\"start\":30865},{\"end\":30874,\"start\":30873},{\"end\":30882,\"start\":30881}]", "bib_author_last_name": "[{\"end\":22167,\"start\":22160},{\"end\":22180,\"start\":22171},{\"end\":22609,\"start\":22602},{\"end\":22620,\"start\":22613},{\"end\":22632,\"start\":22624},{\"end\":22643,\"start\":22636},{\"end\":22652,\"start\":22647},{\"end\":22664,\"start\":22656},{\"end\":23168,\"start\":23164},{\"end\":23183,\"start\":23174},{\"end\":23192,\"start\":23187},{\"end\":23623,\"start\":23619},{\"end\":23631,\"start\":23627},{\"end\":23641,\"start\":23635},{\"end\":23647,\"start\":23645},{\"end\":23653,\"start\":23651},{\"end\":23664,\"start\":23657},{\"end\":23880,\"start\":23878},{\"end\":23887,\"start\":23884},{\"end\":23899,\"start\":23891},{\"end\":23912,\"start\":23903},{\"end\":23926,\"start\":23916},{\"end\":24252,\"start\":24247},{\"end\":24265,\"start\":24256},{\"end\":24546,\"start\":24544},{\"end\":24555,\"start\":24550},{\"end\":24562,\"start\":24559},{\"end\":24569,\"start\":24566},{\"end\":24807,\"start\":24802},{\"end\":24818,\"start\":24813},{\"end\":24826,\"start\":24822},{\"end\":24833,\"start\":24830},{\"end\":25068,\"start\":25062},{\"end\":25086,\"start\":25072},{\"end\":25095,\"start\":25090},{\"end\":25108,\"start\":25099},{\"end\":25397,\"start\":25387},{\"end\":25408,\"start\":25401},{\"end\":25417,\"start\":25415},{\"end\":26006,\"start\":25999},{\"end\":26013,\"start\":26010},{\"end\":26025,\"start\":26017},{\"end\":26035,\"start\":26031},{\"end\":26045,\"start\":26041},{\"end\":26405,\"start\":26402},{\"end\":26412,\"start\":26409},{\"end\":26419,\"start\":26416},{\"end\":26427,\"start\":26423},{\"end\":26822,\"start\":26817},{\"end\":26830,\"start\":26826},{\"end\":27596,\"start\":27594},{\"end\":27605,\"start\":27600},{\"end\":27613,\"start\":27609},{\"end\":27622,\"start\":27617},{\"end\":27632,\"start\":27626},{\"end\":27643,\"start\":27638},{\"end\":27906,\"start\":27900},{\"end\":27916,\"start\":27910},{\"end\":28167,\"start\":28160},{\"end\":28180,\"start\":28171},{\"end\":28188,\"start\":28184},{\"end\":28201,\"start\":28194},{\"end\":28209,\"start\":28205},{\"end\":28743,\"start\":28734},{\"end\":28756,\"start\":28749},{\"end\":28771,\"start\":28762},{\"end\":28781,\"start\":28775},{\"end\":29315,\"start\":29308},{\"end\":29329,\"start\":29319},{\"end\":29341,\"start\":29335},{\"end\":29353,\"start\":29345},{\"end\":29366,\"start\":29357},{\"end\":29636,\"start\":29626},{\"end\":29646,\"start\":29640},{\"end\":29840,\"start\":29836},{\"end\":29848,\"start\":29844},{\"end\":29857,\"start\":29852},{\"end\":29870,\"start\":29861},{\"end\":29878,\"start\":29874},{\"end\":29889,\"start\":29882},{\"end\":29897,\"start\":29893},{\"end\":29903,\"start\":29901},{\"end\":30202,\"start\":30198},{\"end\":30209,\"start\":30206},{\"end\":30218,\"start\":30213},{\"end\":30226,\"start\":30222},{\"end\":30238,\"start\":30233},{\"end\":30594,\"start\":30590},{\"end\":30601,\"start\":30598},{\"end\":30609,\"start\":30605},{\"end\":30618,\"start\":30613},{\"end\":30626,\"start\":30622},{\"end\":30871,\"start\":30867},{\"end\":30879,\"start\":30875},{\"end\":30888,\"start\":30883}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14627863},\"end\":22563,\"start\":22057},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":18952429},\"end\":23116,\"start\":22565},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7648367},\"end\":23562,\"start\":23118},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":57246310},\"end\":23805,\"start\":23564},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207968},\"end\":24186,\"start\":23807},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6054025},\"end\":24494,\"start\":24188},{\"attributes\":{\"doi\":\"arXiv:1512.03385\",\"id\":\"b6\"},\"end\":24724,\"start\":24496},{\"attributes\":{\"doi\":\"abs/1505.07922\",\"id\":\"b7\"},\"end\":24998,\"start\":24726},{\"attributes\":{\"doi\":\"abs/1511.02251\",\"id\":\"b8\"},\"end\":25273,\"start\":25000},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1357489},\"end\":25927,\"start\":25275},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1759984},\"end\":26258,\"start\":25929},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206593370},\"end\":26777,\"start\":26260},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1438450},\"end\":27482,\"start\":26779},{\"attributes\":{\"doi\":\"abs/1611.05377\",\"id\":\"b13\"},\"end\":27862,\"start\":27484},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5855042},\"end\":28079,\"start\":27864},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":16447573},\"end\":28702,\"start\":28081},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":423350},\"end\":29223,\"start\":28704},{\"attributes\":{\"doi\":\"abs/1703.02344\",\"id\":\"b17\"},\"end\":29568,\"start\":29225},{\"attributes\":{\"doi\":\"abs/1406.2080\",\"id\":\"b18\"},\"end\":29774,\"start\":29570},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3732882},\"end\":30127,\"start\":29776},{\"attributes\":{\"id\":\"b20\"},\"end\":30519,\"start\":30129},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206592873},\"end\":30825,\"start\":30521},{\"attributes\":{\"doi\":\"abs/1611.05720\",\"id\":\"b22\"},\"end\":31007,\"start\":30827}]", "bib_title": "[{\"end\":22156,\"start\":22057},{\"end\":22598,\"start\":22565},{\"end\":23160,\"start\":23118},{\"end\":23615,\"start\":23564},{\"end\":23874,\"start\":23807},{\"end\":24243,\"start\":24188},{\"end\":25383,\"start\":25275},{\"end\":25993,\"start\":25929},{\"end\":26398,\"start\":26260},{\"end\":26813,\"start\":26779},{\"end\":27892,\"start\":27864},{\"end\":28156,\"start\":28081},{\"end\":28730,\"start\":28704},{\"end\":29832,\"start\":29776},{\"end\":30586,\"start\":30521}]", "bib_author": "[{\"end\":22169,\"start\":22158},{\"end\":22182,\"start\":22169},{\"end\":22611,\"start\":22600},{\"end\":22622,\"start\":22611},{\"end\":22634,\"start\":22622},{\"end\":22645,\"start\":22634},{\"end\":22654,\"start\":22645},{\"end\":22666,\"start\":22654},{\"end\":23170,\"start\":23162},{\"end\":23185,\"start\":23170},{\"end\":23194,\"start\":23185},{\"end\":23625,\"start\":23617},{\"end\":23633,\"start\":23625},{\"end\":23643,\"start\":23633},{\"end\":23649,\"start\":23643},{\"end\":23655,\"start\":23649},{\"end\":23666,\"start\":23655},{\"end\":23882,\"start\":23876},{\"end\":23889,\"start\":23882},{\"end\":23901,\"start\":23889},{\"end\":23914,\"start\":23901},{\"end\":23928,\"start\":23914},{\"end\":24254,\"start\":24245},{\"end\":24267,\"start\":24254},{\"end\":24548,\"start\":24542},{\"end\":24557,\"start\":24548},{\"end\":24564,\"start\":24557},{\"end\":24571,\"start\":24564},{\"end\":24809,\"start\":24800},{\"end\":24820,\"start\":24809},{\"end\":24828,\"start\":24820},{\"end\":24835,\"start\":24828},{\"end\":25070,\"start\":25060},{\"end\":25088,\"start\":25070},{\"end\":25097,\"start\":25088},{\"end\":25110,\"start\":25097},{\"end\":25399,\"start\":25385},{\"end\":25410,\"start\":25399},{\"end\":25419,\"start\":25410},{\"end\":26008,\"start\":25995},{\"end\":26015,\"start\":26008},{\"end\":26027,\"start\":26015},{\"end\":26037,\"start\":26027},{\"end\":26047,\"start\":26037},{\"end\":26407,\"start\":26400},{\"end\":26414,\"start\":26407},{\"end\":26421,\"start\":26414},{\"end\":26429,\"start\":26421},{\"end\":26433,\"start\":26429},{\"end\":26824,\"start\":26815},{\"end\":26832,\"start\":26824},{\"end\":27598,\"start\":27592},{\"end\":27607,\"start\":27598},{\"end\":27615,\"start\":27607},{\"end\":27624,\"start\":27615},{\"end\":27634,\"start\":27624},{\"end\":27645,\"start\":27634},{\"end\":27908,\"start\":27894},{\"end\":27918,\"start\":27908},{\"end\":28169,\"start\":28158},{\"end\":28182,\"start\":28169},{\"end\":28190,\"start\":28182},{\"end\":28203,\"start\":28190},{\"end\":28211,\"start\":28203},{\"end\":28745,\"start\":28732},{\"end\":28758,\"start\":28745},{\"end\":28773,\"start\":28758},{\"end\":28783,\"start\":28773},{\"end\":29317,\"start\":29306},{\"end\":29331,\"start\":29317},{\"end\":29343,\"start\":29331},{\"end\":29355,\"start\":29343},{\"end\":29368,\"start\":29355},{\"end\":29638,\"start\":29624},{\"end\":29648,\"start\":29638},{\"end\":29842,\"start\":29834},{\"end\":29850,\"start\":29842},{\"end\":29859,\"start\":29850},{\"end\":29872,\"start\":29859},{\"end\":29880,\"start\":29872},{\"end\":29891,\"start\":29880},{\"end\":29899,\"start\":29891},{\"end\":29905,\"start\":29899},{\"end\":30204,\"start\":30196},{\"end\":30211,\"start\":30204},{\"end\":30220,\"start\":30211},{\"end\":30228,\"start\":30220},{\"end\":30240,\"start\":30228},{\"end\":30596,\"start\":30588},{\"end\":30603,\"start\":30596},{\"end\":30611,\"start\":30603},{\"end\":30620,\"start\":30611},{\"end\":30628,\"start\":30620},{\"end\":30873,\"start\":30865},{\"end\":30881,\"start\":30873},{\"end\":30890,\"start\":30881}]", "bib_venue": "[{\"end\":22839,\"start\":22752},{\"end\":23986,\"start\":23974},{\"end\":25621,\"start\":25520},{\"end\":27211,\"start\":27030},{\"end\":22186,\"start\":22182},{\"end\":22750,\"start\":22666},{\"end\":23227,\"start\":23194},{\"end\":23670,\"start\":23666},{\"end\":23972,\"start\":23928},{\"end\":24324,\"start\":24267},{\"end\":24540,\"start\":24496},{\"end\":24798,\"start\":24726},{\"end\":25058,\"start\":25000},{\"end\":25518,\"start\":25419},{\"end\":26051,\"start\":26047},{\"end\":26498,\"start\":26433},{\"end\":27028,\"start\":26832},{\"end\":27590,\"start\":27484},{\"end\":27954,\"start\":27918},{\"end\":28260,\"start\":28211},{\"end\":28832,\"start\":28783},{\"end\":29304,\"start\":29225},{\"end\":29622,\"start\":29570},{\"end\":29909,\"start\":29905},{\"end\":30194,\"start\":30129},{\"end\":30632,\"start\":30628},{\"end\":30863,\"start\":30827}]"}}}, "year": 2023, "month": 12, "day": 17}