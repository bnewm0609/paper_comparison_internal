{"id": 226203026, "updated": "2021-12-22 00:57:45.046", "metadata": {"title": "An Effective Speaker Recognition Method Based on Joint Identification and Verification Supervisions", "authors": "[{\"middle\":[],\"last\":\"Liu\",\"first\":\"Ying\"},{\"middle\":[],\"last\":\"Song\",\"first\":\"Yan\"},{\"middle\":[],\"last\":\"Jiang\",\"first\":\"Yiheng\"},{\"middle\":[],\"last\":\"McLoughlin\",\"first\":\"Ian\"},{\"middle\":[],\"last\":\"Liu\",\"first\":\"Lin\"},{\"middle\":[],\"last\":\"Dai\",\"first\":\"Lirong\"}]", "venue": "Interspeech 2020", "journal": "Interspeech 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Deep embedding learning based speaker verification methods have attracted significant recent research interest due to their superior performance. Existing methods mainly focus on designing frame-level feature extraction structures, utterance-level aggregation methods and loss functions to learn discriminative speaker embeddings. The scores of verification trials are then computed using cosine distance or Probabilistic Linear Discriminative Analysis (PLDA) classifiers. This paper proposes an effective speaker recognition method which is based on joint identification and verification supervisions, inspired by multi-task learning frameworks. Specifically, a deep architecture with convolutional feature extractor, attentive pooling and two classifier branches is presented. The first, an identification branch, is trained with additive margin softmax loss (AMSoftmax) to classify the speaker identities. The second, a verification branch, trains a discriminator with binary cross entropy loss (BCE) to optimize a new triplet-based mutual information. To balance the two losses during different training stages, a ramp-up/ramp-down weighting scheme is employed. Furthermore, an attentive bilinear pooling method is proposed to improve the effectiveness of embeddings. Extensive experiments have been conducted on VoxCeleb1 to evaluate the proposed method, demonstrating results that relatively reduce the equal error rate (EER) by 22% compared to the baseline system using identification supervision only.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3097000690", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/Liu0JML020", "doi": "10.21437/interspeech.2020-1922"}}, "content": {"source": {"pdf_hash": "dd775986a44b64ced349ad018e122e14266005b0", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2500cc629900bc707858ca8cba912eb6543da836", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/dd775986a44b64ced349ad018e122e14266005b0.txt", "contents": "\nAn Effective Speaker Recognition Method Based on Joint Identification and Verification Supervisions\n\n\nYing Liu \nNational Engineering Laboratory for Speech and Language Information Processing\nUniversity of Science and Technology of China\nHefeiChina\n\nYan Song \nNational Engineering Laboratory for Speech and Language Information Processing\nUniversity of Science and Technology of China\nHefeiChina\n\nYiheng Jiang jiangyh@mail.ustc.edu.cn \nNational Engineering Laboratory for Speech and Language Information Processing\nUniversity of Science and Technology of China\nHefeiChina\n\nIan Mcloughlin \nNational Engineering Laboratory for Speech and Language Information Processing\nUniversity of Science and Technology of China\nHefeiChina\n\nICT Cluster\nInstitute of Technology 3 iFLYTEK Research\niFLYTEK CO., LTD\n230088HefeiAnhuiSingapore, China\n\nLin Liu linliu@iflytek.com \nLirong Dai lrdai@ustc.edu.cn \nNational Engineering Laboratory for Speech and Language Information Processing\nUniversity of Science and Technology of China\nHefeiChina\n\nAn Effective Speaker Recognition Method Based on Joint Identification and Verification Supervisions\nIndex Terms: speaker verificationmutual information learn- ingattentive bilinear poolingmulti-task framework\nDeep embedding learning based speaker verification methods have attracted significant recent research interest due to their superior performance. Existing methods mainly focus on designing frame-level feature extraction structures, utterance-level aggregation methods and loss functions to learn discriminative speaker embeddings. The scores of verification trials are then computed using cosine distance or Probabilistic Linear Discriminative Analysis (PLDA) classifiers. This paper proposes an effective speaker recognition method which is based on joint identification and verification supervisions, inspired by multi-task learning frameworks. Specifically, a deep architecture with convolutional feature extractor, attentive pooling and two classifier branches is presented. The first, an identification branch, is trained with additive margin softmax loss (AM-Softmax) to classify the speaker identities. The second, a verification branch, trains a discriminator with binary cross entropy loss (BCE) to optimize a new triplet-based mutual information. To balance the two losses during different training stages, a ramp-up/ramp-down weighting scheme is employed. Furthermore, an attentive bilinear pooling method is proposed to improve the effectiveness of embeddings. Extensive experiments have been conducted on VoxCeleb1 to evaluate the proposed method, demonstrating results that relatively reduce the equal error rate (EER) by 22% compared to the baseline system using identification supervision only.\n\nIntroduction\n\nSpeaker recognition (SR) is the task of automatically determining whether a given utterance belongs to a certain speaker identity. According to different recognition settings, SR can be categorized into either speaker identification (SID) which classifies a given utterance as being from a specific speaker, or speaker verification (SV), which is a binary classification problem that determines whether two given speech utterances belong to same speaker or not. Compared to SID, SV is an open-set recognition task with no overlap between training and test set, which is closely related to representation learning.\n\nOver the past few decades, the most popular SV methods have been based on i-vector followed by Probabilistic Linear Discriminative Analysis (PLDA) [1,2], in which the i-vector representation is generally learned in an unsupervised manner. Recently, deep embedding learning based SV methods have attracted significant interest due to their superior performance. Compared to traditional i-vector systems, deep learning based SV methods may benefit from the discriminative characteristics and large receptive-field of deep neural networks (DNNs).\n\nExisting deep embedding learning architectures include time-delay DNN (TDNN) [3], convolutional neural network (CNN) [4,5,6], and Long Short-Term Memory (LSTM) networks [7]. Generally, these architectures consist of a framelevel feature extractor, an utterance-level aggregator and a classifier, which can be optimized in an end-to-end way.\n\nMany recent works have focused on utterance-level aggregation methods, e.g., statistical pooling [3], attentive pooling [8,9], bilinear pooling [4], and dictionary based pooling methods [10,11]. Meanwhile, other works have proposed different loss functions, including triplet loss [12,13], center loss [10], triplet-center loss [14], angular softmax loss (A-Softmax) [10] and additive margin softmax loss (AM-Softmax) [15,16]. However, in most deep embedding learning methods, the network architectures are trained under identification supervision, optimized for the SID task. Meanwhile, for SV tasks, the verification score between utterance pairs is computed via cosine distance or through an additional trainable backend. It is still difficult to directly incorporate an effective backend into a deep embedding learning architecture [4].\n\nIn this paper, an effective speaker recognition method is proposed based on joint identification and verification supervisions. This is inspired by the multi-task learning framework, as shown in Fig. 1 and detailed in Section 3. Specifically, this includes a deep architecture with frame level feature extractor, attentive pooling and two branches of classifiers. The first branch is similar to deep embedding learning, in which a speaker classifier is optimized via AM-Softmax loss to discriminate the learned speaker embeddings. The second branch optimizes a new triplet-based mutual information (T-MI) between positive and negative samples extracted from the embedding space, inspired by triplet loss [12,13] and mutual information based representation learning [17]. As with generative adversarial networks (GANs), we train a discriminator to separate them, using binary cross entropy loss (BCE). To prevent the issue of an imbalance between AM-softmax and BCE loss at different training stages, we introduce a ramp-up/ramp-down weighting scheme. In addition, a new attentive bilinear pooling method (ABP) is proposed, aiming to improve performance by aggregating features along the temporal axis.\n\nTo evaluate the effectiveness of the proposed method, extensive experiments have been conducted on the Voxceleb1 benchmark [18]. By jointly optimizing the identification and verification, our method can relatively reduce the equal error rate (EER) by 22% compared to the baseline system using identification supervision only. \n\n\nOverview of the proposed multi-task learning framework\n\nThe proposed multi-task learning based speaker recognition framework is shown in Fig. 1. It consists of frame-level feature extractor, utterance-level aggregator, and two branches of classifiers.\n\nThe frame-level feature extractor is adapted from the existing ResNet-18 architecture [19], which comprises an input convolutional layer and 4 residual stages. The main difference lies in that we keep the temporal and frequency dimensions of feature maps in each residual stage unchanged, and insert a transition layer to reduce the frequency dimension.\n\nThe aggregator is then followed to map the extracted framelevel features into utterance-level representations. In this paper, a novel attentive bilinear pooling (ABP) method is introduced to improve the effectiveness of embeddings, detailed in Section 3.3. Then an embedding layer, implemented by a fully connected (FC) layer, is added to make a nonlinear transformation and dimension reduction to obtain speaker embeddings.\n\nThe speaker embeddings are firstly length normalized and then fed into two branches of classifiers for multi-task learning. The first, an identification branch, is implemented by a FC layer and trained with AM-Softmax loss for SID task. The second, a verification branch, accomplishes the SV task by first constructing the positive and negative pairs from the selected triplet, and then training a binary classifier with BCE loss. Finally, a rampup/ramp-down weighting scheme is employed to balance the AM-softmax and BCE loss for multi-task learning.\n\nDuring testing, we can either extract speaker embeddings from the embedding layer for the enrolment and test set, and then use a traditional PLDA backend to calculate verification scores, or directly use the output of the verification branch, giving scores in an end-to-end fashion.\n\n\nMethods\n\n\nTriplet-based mutual information (T-MI) learning\n\nMutual Information (MI) of statistical dependence is a promising tool for learning representations in an unsupervised way [17]. Given two random variables x and y, MI can be defined as\nM I(x; y) = x y p(x, y) log p(x, y) p(x)p(y) dxdy = DKL{p(x, y) p(x)p(y)}(1)\nwhere DKL is the Kullback-Leibler (KL) divergence between the joint distribution p(x, y) and product of marginals p(x)p(y). The MI is minimized when the random variable x and y are statistically independent, and is maximized when they contain identical information. For SV task, inspired by triplet loss [13], we can construct triplet (xa, xp, xn), where xa and xp are utterances from the same speaker, xa and xn are from the different speakers. And then discriminative speaker representations can be effectively learned by maximizing MI between xa and xp, and minimizing it between xa and xn. This is logical, however, MI is hard to measure directly.\n\nFortunately, previous research [20] has found it possible to optimize the MI within an encoder-discriminator framework. Motivated by [17], a verification branch is designed as the discriminator using T-MI learning. Specifically, the front-end extraction network, including a frame-level feature extractor and an utterance-level aggregator, is used as the encoder, denoted by f \u03b8 (\u00b7). Embeddings of the triplet can be obtained by feeding it through the network. Then positive embedding pair (f \u03b8 (xa), f \u03b8 (xp)) and negative embedding pair (f \u03b8 (xa), f \u03b8 (xn)) are formed and passed through the verification branch, implemented by a binary classifier denoted by g \u03c6 (\u00b7), for discriminating verification. The positive pair and the negative pair are labeled '1' and '0' respectively, and the standard binary cross entropy loss (BCE) is used as the objective function to train the classifier:\nLver = 1 N N i=1 \u2212 log g \u03c6 (f \u03b8 (x i a ) \u2295 f \u03b8 (x i p )) \u2212 log 1 \u2212 g \u03c6 (f \u03b8 (x i a ) \u2295 f \u03b8 (x i n ))(2)\nwhere \u2295 denotes the concatenation operator. The BCE loss in Eq.\n\n(2) actually estimates the Jansen-Shannon divergence between positive and negative distributions, which is similar to the KL-based definition of MI [17].\n\nThe main difference to [17] is that we construct triplet with respect to label information in a mini-batch, which in fact introduces the verification supervision. Therefore, the output sigmoid probability of the verification branch can be used as a similarity measure of two embeddings, which conveniently allows the system to be trained end-to-end without PLDA backend or cosine distance calculation. Given input pair (x1, x2), the verification score can be obtained as:\nscore(x1, x2) = g \u03c6 (f \u03b8 (x1) \u2295 f \u03b8 (x2))(3)\n\nJoint optimization of identification and verification\n\nAs discussed above, the multi-task system is optimized jointly with identification and verification supervisions. For SID task, AM-Softmax loss is used as the objective function:\nL iden = \u2212 1 N N i=1 log e s\u00b7(cos\u03b8y i \u2212m) e s\u00b7(cos\u03b8y i \u2212m) + c j=1,j =y i e s\u00b7cos\u03b8 j(4)\nwhere s is the scale parameter and m is the margin. In our experiments, we set s = 18 and m = 0.1.\n\nThe total loss for joint optimization is the weighted sum of the identification loss and verification loss:\nL = \u03bbL iden + \u00b5Lver(5)\nwhere the \u03bb and \u00b5 are weight parameters. To balance these two loss components at different training stages, a ramp-up/rampdown weighting scheme is introduced. The weight \u00b5 starts from zero and ramps up along the curve \u00b5(t) = \u00b50e \u22125{1\u2212t/T 1 } 2 , and similarly, \u03bb ramps down according to the curve\n\u03bb(t) = \u03bb0e \u22125{(t\u2212T 2 )/(T 3 \u2212T 2 )} 2\n, where t is training epoch, \u00b50 is the final value of \u00b5, \u03bb0 is the initial value of \u03bb, [0, T1] and [T2, T3] are the durations of ramp-up and ramp-down periods respectively. In our experiments, \u00b50 and \u03bb0 are set to 1.\n\n\nAttentive bilinear pooling (ABP)\n\nInspired by [4,8], an attentive bilinear pooling (ABP) method is further utilized to force the model to pay more attention to useful information for aggregation. It calibrates the output frame-level features with learnable convolutional layer to provide frame-wise attention mechanism.\n\nSpecifically, let H \u2208 R L\u00d7D be the frame-level feature map captured by the hidden layer below the self-attention layer, where L and D are the number of frames and feature dimension respectively. Then the attention map A \u2208 R L\u00d7K can be obtained by feeding H into a 1\u00d71 convolutional layer followed by softmax non-linear activation, where K is the number of attention heads. The 1 st -order and 2 nd -order attentive statistics of H, denoted by \u00b5 and \u03c3 2 , can be computed similar as crosslayer bilinear pooling [4], which is\n\u00b5 = T2(T1(H T A)) \u03c3 2 = T2(T1((H H) T A) \u2212 (T1(H T A)) T1(H T A))(6)\nwhere T1(x) is the operation of reshaping x into a vector, and T2(x) includes a signed square-root step and a L2normalization step.\n\nrepresents the Hadamard product. The output of ABP is the concatenation of \u00b5 and \u03c3 2 .\n\nIt is worth noting that the proposed ABP method derives the attention map using the softmax along temporal axis to obtain the attention for each frame. And the attentive 2 nd -order statistics information is further exploited for aggregation, similar as statistics pooling in [21]. This is different from the existing pooling methods, such as NetVLAD [11] and learnable dictionary encoding (LDE) [10], which mainly focus on deriving Baum-Welch statistics over the channel dimension.\n\nCompared to attentive statistic pooling [8], ABP with multiple attention heads can better capture the speaker information in a input utterance. Furthermore, ABP normalizes the length of statistics before concatenation, which is able to extract more robust embeddings. \nTrans1 3 \u00d7 3, 32 1 \u00d7 2 L \u00d7 17 \u00d7 32 Res2 3 \u00d7 3, 32 3 \u00d7 3, 32 \u00d7 2 1 \u00d7 1 L \u00d7 17 \u00d7 32 Trans2 3 \u00d7 3, 64 1 \u00d7 2 L \u00d7 8 \u00d7 64 Res3 3 \u00d7 3, 64 3 \u00d7 3, 64 \u00d7 2 1 \u00d7 1 L \u00d7 8 \u00d7 64 Trans3 3 \u00d7 3, 128 1 \u00d7 2 L \u00d7 3 \u00d7 128 Res4 3 \u00d7 3, 128 3 \u00d7 3, 128 \u00d7 2 1 \u00d7 1 L \u00d7 3 \u00d7 128 Trans4 3 \u00d7 3, 128 1 \u00d7 2 L \u00d7 1 \u00d7 128 ABP - - 1 \u00d7 (128 \u00d7 2K) FC (128 \u00d7 2K) \u00d7 128 - 1 \u00d7 128 L2norm - - 1 \u00d7 128\n\nExperimental setup and results\n\n\nDataset and input features\n\nTo investigate performance of the proposed system, we conducted extensive experiments using VoxCeleb1 benchmark [18] which contains over 140,000 utterances from 1251 speakers. The training set is the development portion without data augmentation, containing 1,211 speakers and the evaluation set consists of 37,720 trial pairs from 40 speakers. The feature extraction process uses Kaldi [22]. In our implementation, 41-dimensional filter bank outputs (FBank) are used as acoustic features, obtained from 25ms windows with 10ms shift between frames. We apply mean-normalization over a sliding window of 3s, and use voice activity detection (VAD) to remove silent segments. The features from the training dataset are randomly truncated into short slices ranging from 2 to 4s. For evaluation, utterances are divided equally into 10 slices with 4s duration.\n\n\nModel configuration\n\nThe detailed configuration of the front-end extraction network is summarized in Table 1, where L denotes variable-length data frames. The input layer consists of a single convolutional layer with kernel size of 7x7, stride of 1x1 and channel dimension of 16. Four residual stages include [2,2,2,2] basic blocks with 16, 32, 64, 128 channels respectively, and each basic block having 2 convolutional layers with filter sizes of 3x3 and a stride of 1x1. The transition layer comprises a convolutional layer with kernel size of 1x1 and stride of 1x2. After the four stages, the frequency dimension of the feature map is reduced to 1. For ABP, the output dimension 128 \u00d7 2K is varied with different attention heads K.\n\nThe identification branch is implemented by a FC layer with units equal to the number of speaker categories. We should note that when computing the AM-Softmax loss, the weight of this layer need to be normalized. The verification branch comprises two FC layers followed by sigmoid activation.\n\nThe mini-batch size for training is set to 128, containing 64 speakers with 2 utterances from each. All neural networks are implemented using the PyTorch framework [25]. The network is optimized using stochastic gradient descent (SGD) [26] with \n\n\nResults\n\n\nEvaluation on different number of attention heads K\n\nIn Table 2, we study the effect of different number of attention heads K in proposed ABP method. Same as most existing deep embedding learning based SV methods, these results are obtained by using the modified ResNet-18 with Softmax loss to learn speaker embeddings first, and evaluating the verification scores with cosine distance measure. From Table 2, we can see that the EER reduces from 4.07% to 3.76% when K increases from 2 to 16. This indicates that increasing the number of attention heads can improve the effectiveness of the proposed ABP method. However, large value of K may lead to large model size and high computational complexity. In the following experiments, we only report the results with K = 16, considering the trade-off between effectiveness and efficiency.\n\n\nMain results\n\nThe main results are reported in Table 3. We compared the performance of three systems including: 1) ResNet-18 with Softmax loss, 2) ResNet-18 with AM-Softmax loss, and 3) Multitask ResNet-18. The first two systems are implemented following the existing deep embedding learning based methods, which compute the verification scores via cosine distance measure. The multi-task ResNet-18 is implemented using the proposed speaker recognition method based on joint identification and verification supervisions, and the performance is evaluated according to the output of the verification branch directly. From Table 3, we see that the proposed system outperforms all other SV methods by a large margin. The performance of the proposed ABP method is evaluated first. We can see that our ResNet-18 with ABP achieves an EER of 3.76%, which is better than 4.58% when using average pooling and 4.19% when using statistic pooling. This indicates the superiority of the ABP method. This result is also better than the systems in [5,10,15,23], demonstrating the effectiveness for embedding learning of our modified ResNet-18 architecture and pooling method.\n\nThanks to the role of the margin parameter, ResNet-18 with AM-Softmax loss achieves an EER of 3.51%, which is a slight improvement compared with the Softmax model. The EER is further reduced to 2.94% with Multi-task ResNet-18, outperforming almost all other deep embedding learning based SV systems in the same situation.\n\n\nConclusion\n\nIn this paper, inspired by a multi-task framework, an effective speaker recognition method based on joint identification and verification supervision is proposed. Specifically, a deep architecture with convolutional feature extractor, attentive pooling and two branches of classifiers is presented. The first, an identification branch, is trained with AM-Softmax loss for speaker identity classification. The second, a verification branch, trains a discriminator with BCE loss to optimize the MI between positive and negative samples extracted from the embedding space. To balance these two losses at different training stages, a novel ramp-up/ramp-down weighting scheme is employed and, furthermore, a novel attentive bilinear pooling method is proposed. This further improves the effectiveness of embeddings. Experiments conducted on the Voxceleb1 benchmark yield exceptional results, demonstrating the effectiveness of the proposed model for the SV task. \n\n\nAcknowledgements\n\nFigure 1 :\n1Framework of the proposed SV method based on joint identification and verification supervisions.\n\n\nThis work was supported by National Natural Science Foundation of China (Grant No. U1613211), the Leading Plan of CAS (XDC08030200), and by Key Science and Technology Project of Anhui Province ( Grant No. 17030901005 and No. 18030901016).\n\nTable 1 :\n1Detailed configuration of the front-end extraction network.Layer \nStructure \nStride \nOutput size \nConv1 \n7 \u00d7 7, 16 \n1 \u00d7 1 \nL \u00d7 35 \u00d7 16 \n\nRes1 \n3 \u00d7 3, 16 \n3 \u00d7 3, 16 \u00d7 2 \n1 \u00d7 1 \nL \u00d7 35 \u00d7 16 \n\n\n\nTable 3 :\n3Results for verification on VoxCeleb1 dataset. (AP refers to average pooling and SP refers to statistics pooling)System \nAggregation \nLoss \nTraining set \nSimilarity \nEER, % \n\ni-vector+PLDA [18] \n-\n-\nVoxceleb1 \nPLDA \n8.80 \n\nx-vector [23] \nSP \nSoftmax \nVoxceleb1 \ncosine \n11.3 \nPLDA \n7.1 \n\nResNet-34 [5] \nSP \nSoftmax \nVoxceleb1 \ncosine \n5.01 \nPLDA \n4.74 \nResNet-34 [10] \nLDE \nA-Softmax \nVoxceleb1 \ncosine \n4.56 \nResNet-20 [15] \nAP \nAM-Softmax \nVoxceleb1 \ncosine \n4.30 \nResNet-50 [24] \nAP \nSoftmax+Contrastive \nVoxceleb2 \ncosine \n4.19 \nThin ResNet-34 [11] \nNetVLAD \nAM-Softmax \nVoxceleb2 \ncosine \n3.32 \n\nResNet-18 (Ours) \n\nAP \nSoftmax \nVoxceleb1 \ncosine \n\n4.58 \nSP \n4.19 \nABP \n3.76 \nResNet-18 (Ours) \nABP \nAM-Softmax \nVoxceleb1 \ncosine \n3.51 \nMulti-task ResNet-18 (Ours) \nABP \n-\nVoxceleb1 \nverification output \n2.94 \n\nmomentum of 0.95 and weight decay of 5e-4. Each network is \ntrained for 60 epochs with initial learning rate of 0.1, gradually \ndecreasing to 0.0001. The durations of ramp-up and ramp-down \nperiods are set to [0, 25] and [25, 40] epochs respectively. The \nperformance is evaluated in terms of equal error rate (EER). \n\n\n\nTable 2 :\n2Results on different numbers of attention heads K.K \n2 \n4 \n8 \n16 \nEER, % 4.07 3.91 3.82 3.76 \n\n\n\nFront-end factor analysis for speaker verification. N Dehak, P J Kenny, R Dehak, P Dumouchel, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 194N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel et al., \"Front-end factor analysis for speaker verification,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788-798, 2011.\n\nBayesian speaker verification with heavy-tailed priors. P Kenny, Proc. Odyssey. OdysseyP. Kenny, \"Bayesian speaker verification with heavy-tailed pri- ors,\" in Proc. Odyssey, 2010.\n\nXvectors: Robust DNN embeddings for speaker recognition. D Snyder, D Garcia-Romero, G Sell, D Povey, Proc. ICASSP. ICASSPD. Snyder, D. Garcia-Romero, G. Sell, D. Povey et al., \"X- vectors: Robust DNN embeddings for speaker recognition,\" in Proc. ICASSP, 2018, pp. 5329-5333.\n\nAn improved deep embedding learning method for short duration speaker verification. Z Gao, Y Song, I Mcloughlin, W Guo, L Dai, Proc. Interspeech. InterspeechZ. Gao, Y. Song, I. McLoughlin, W. Guo, and L. Dai, \"An improved deep embedding learning method for short duration speaker verification,\" in Proc. Interspeech, 2018, pp. 3578-3582.\n\nAnalysis of length normalization in end-to-end speaker verification system. W Cai, J Chen, M Li, Proc. Interspeech. InterspeechW. Cai, J. Chen, and M. Li, \"Analysis of length normalization in end-to-end speaker verification system,\" in Proc. Interspeech, 2018.\n\nAn effective deep embedding learning architecture for speaker verification. Y Jiang, Y Song, I Mcloughlin, Z Gao, L Dai, Proc. Interspeech. InterspeechY. Jiang, Y. Song, I. McLoughlin, Z. Gao, and L. Dai, \"An ef- fective deep embedding learning architecture for speaker verifica- tion,\" in Proc. Interspeech, 2019, pp. 4040-4044.\n\nGeneralized endto-end loss for speaker verification. L Wan, Q Wang, A Papir, I L Moreno, Proc. ICASSP. ICASSPL. Wan, Q. Wang, A. Papir, and I. L. Moreno, \"Generalized end- to-end loss for speaker verification,\" in Proc. ICASSP, 2018, pp. 4879-4883.\n\nAttentive statistics pooling for deep speaker embedding. K Okabe, T Koshinaka, K Shinoda, Proc. Interspeech. InterspeechK. Okabe, T. Koshinaka, and K. Shinoda, \"Attentive statistics pooling for deep speaker embedding,\" in Proc. Interspeech, 2018, pp. 2252-2256.\n\nSelf-attentive speaker embeddings for text-independent speaker verification. Y Zhu, T Ko, D Snyder, B Mak, Proc. Interspeech. InterspeechY. Zhu, T. Ko, D. Snyder, B. Mak et al., \"Self-attentive speaker embeddings for text-independent speaker verification,\" in Proc. Interspeech, 2018, pp. 3573-3577.\n\nExploring the encoding layer and loss function in end-to-end speaker and language recognition system. W Cai, J Chen, M Li, arXiv:1804.05160in arXiv preprintW. Cai, J. Chen, and M. Li, \"Exploring the encoding layer and loss function in end-to-end speaker and language recognition system,\" in arXiv preprint arXiv:1804.05160, 2018.\n\nUtterancelevel aggregation for speaker recognition in the wild. W Xie, A Nagrani, J S Chung, A Zisserman, Proc. ICASSP. ICASSPW. Xie, A. Nagrani, J. S. Chung, and A. Zisserman, \"Utterance- level aggregation for speaker recognition in the wild,\" Proc. ICASSP, 2019.\n\nDeep speaker: an end-to-end neural speaker embedding system. C Li, X Ma, B Jiang, X Li, X Zhang, X Liu, arXiv:1705.02304in arXiv preprintC. Li, X. Ma, B. Jiang, X. Li, X. Zhang, and X. Liu, \"Deep speaker: an end-to-end neural speaker embedding system,\" in arXiv preprint arXiv:1705.02304, 2017.\n\nTriplet loss based cosine similarity metric learning for text-independent speaker recognition. S Novoselov, V Shchemelinin, A Shulipa, A Kozlov, I Kremnev, Proc. Interspeech. InterspeechS. Novoselov, V. Shchemelinin, A. Shulipa, A. Kozlov, and I. Kremnev, \"Triplet loss based cosine similarity metric learning for text-independent speaker recognition,\" in Proc. Interspeech, 2018, pp. 2242-2246.\n\nTripletcenter loss based deep embedding learning method for speaker verification. Y Jiang, Y Song, J Yan, L Dai, I Mcloughlin, Proc. APSIPA. APSIPAY. Jiang, Y. Song, J. Yan, L. Dai, and I. McLoughlin, \"Triplet- center loss based deep embedding learning method for speaker verification,\" in Proc. APSIPA, 2019.\n\nUnified hypersphere embedding for speaker recognition. M Hajibabaei, D Dai, arXiv:1807.08312arXiv preprintM. Hajibabaei and D. Dai, \"Unified hypersphere embedding for speaker recognition,\" in arXiv preprint arXiv:1807.08312, 2018.\n\nLarge margin softmax loss for speaker verification. Y Liu, L He, J Liu, Proc. Interspeech. InterspeechY. Liu, L. He, and J. Liu, \"Large margin softmax loss for speaker verification,\" in Proc. Interspeech, 2019, pp. 2873-2877.\n\nLearning speaker representations with mutual information. M Ravanelli, Y Bengio, Proc. Interspeech. InterspeechM. Ravanelli and Y. Bengio, \"Learning speaker representations with mutual information,\" in Proc. Interspeech, 2019, pp. 1153- 1157.\n\nVoxceleb: A largescale speaker identification dataset. A Nagrani, J S Chung, A Zisserman, Proc. Interspeech. InterspeechA. Nagrani, J. S. Chung, and A. Zisserman, \"Voxceleb: A large- scale speaker identification dataset,\" in Proc. Interspeech, 2017.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. ICASSP. ICASSPK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. ICASSP, 2018, pp. 5334-5338.\n\nLearning independent features with adversarial nets for non-linear ica. P Brakel, Y Bengio, arXiv:1710.05050arXiv preprintP. Brakel and Y. Bengio, \"Learning independent features with adversarial nets for non-linear ica,\" in arXiv preprint arXiv:1710.05050, 2017.\n\nDeep neural network embeddings for text-independent speaker verification. D Snyder, D Garcia-Romero, D Povey, S Khudanpur, Proc. Interspeech. InterspeechD. Snyder, D. Garcia-Romero, D. Povey, and S. Khudanpur, \"Deep neural network embeddings for text-independent speaker verification,\" in Proc. Interspeech, 2017, pp. 999-1003.\n\nThe Kaldi speech recognition toolkit. D Povey, A Ghoshal, G Boulianne, L Burget, Proc. ASRU. IEEE Signal Processing Society. ASRU. IEEE Signal essing SocietyD. Povey, A. Ghoshal, G. Boulianne, L. Burget et al., \"The Kaldi speech recognition toolkit,\" in Proc. ASRU. IEEE Signal Pro- cessing Society, 2011.\n\nFrame-level speaker embeddings for text-independent speaker recognition and analysis of end-toend model. S Shon, H Tang, J Glass, arXiv:1809.04437in arXiv preprintS. Shon, H. Tang, and J. Glass, \"Frame-level speaker embed- dings for text-independent speaker recognition and analysis of end-toend model,\" in arXiv preprint arXiv:1809.04437, 2018.\n\nVoxceleb2: Deep speaker recognition. J S Chung, A Nagrani, A Zisserman, Proc. Interspeech. InterspeechJ. S. Chung, A. Nagrani, and A. Zisserman, \"Voxceleb2: Deep speaker recognition,\" in Proc. Interspeech, 2018, pp. 1086-1090.\n\nAutomatic differentiation in pytorch. A Paszke, S Gross, S Chintala, G Chanan, A. Paszke, S. Gross, S. Chintala, G. Chanan et al., \"Automatic differentiation in pytorch,\" 2017.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. the IEEEY. Lecun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition,\" in Proceedings of the IEEE, 1998, pp. 2278-2324.\n", "annotations": {"author": "[{\"start\":\"103\",\"end\":\"249\"},{\"start\":\"250\",\"end\":\"396\"},{\"start\":\"397\",\"end\":\"572\"},{\"start\":\"573\",\"end\":\"831\"},{\"start\":\"832\",\"end\":\"859\"},{\"start\":\"860\",\"end\":\"1026\"}]", "publisher": null, "author_last_name": "[{\"start\":\"108\",\"end\":\"111\"},{\"start\":\"254\",\"end\":\"258\"},{\"start\":\"404\",\"end\":\"409\"},{\"start\":\"577\",\"end\":\"587\"},{\"start\":\"836\",\"end\":\"839\"},{\"start\":\"867\",\"end\":\"870\"}]", "author_first_name": "[{\"start\":\"103\",\"end\":\"107\"},{\"start\":\"250\",\"end\":\"253\"},{\"start\":\"397\",\"end\":\"403\"},{\"start\":\"573\",\"end\":\"576\"},{\"start\":\"832\",\"end\":\"835\"},{\"start\":\"860\",\"end\":\"866\"}]", "author_affiliation": "[{\"start\":\"113\",\"end\":\"248\"},{\"start\":\"260\",\"end\":\"395\"},{\"start\":\"436\",\"end\":\"571\"},{\"start\":\"589\",\"end\":\"724\"},{\"start\":\"726\",\"end\":\"830\"},{\"start\":\"890\",\"end\":\"1025\"}]", "title": "[{\"start\":\"1\",\"end\":\"100\"},{\"start\":\"1027\",\"end\":\"1126\"}]", "venue": null, "abstract": "[{\"start\":\"1236\",\"end\":\"2746\"}]", "bib_ref": "[{\"start\":\"3524\",\"end\":\"3527\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"3527\",\"end\":\"3529\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"3999\",\"end\":\"4002\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"4039\",\"end\":\"4042\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"4042\",\"end\":\"4044\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"4044\",\"end\":\"4046\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"4091\",\"end\":\"4094\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"4361\",\"end\":\"4364\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"4384\",\"end\":\"4387\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"4387\",\"end\":\"4389\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"4408\",\"end\":\"4411\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"4450\",\"end\":\"4454\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"4454\",\"end\":\"4457\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"4545\",\"end\":\"4549\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"4549\",\"end\":\"4552\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"4566\",\"end\":\"4570\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"4592\",\"end\":\"4596\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"4631\",\"end\":\"4635\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"4682\",\"end\":\"4686\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"4686\",\"end\":\"4689\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"5100\",\"end\":\"5103\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"5810\",\"end\":\"5814\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"5814\",\"end\":\"5817\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"5871\",\"end\":\"5875\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"6432\",\"end\":\"6436\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"6977\",\"end\":\"6981\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"8692\",\"end\":\"8696\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"9136\",\"end\":\"9140\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"9516\",\"end\":\"9520\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"9618\",\"end\":\"9622\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"10691\",\"end\":\"10695\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"10721\",\"end\":\"10725\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"12369\",\"end\":\"12372\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"12372\",\"end\":\"12374\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"13154\",\"end\":\"13157\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"13734\",\"end\":\"13738\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"13809\",\"end\":\"13813\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"13854\",\"end\":\"13858\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"13982\",\"end\":\"13985\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"14740\",\"end\":\"14744\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"15015\",\"end\":\"15019\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"16678\",\"end\":\"16682\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"16749\",\"end\":\"16753\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"18641\",\"end\":\"18644\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"18644\",\"end\":\"18647\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"18647\",\"end\":\"18650\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"18650\",\"end\":\"18653\",\"attributes\":{\"ref_id\":\"b22\"}}]", "figure": "[{\"start\":\"20084\",\"end\":\"20193\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"20194\",\"end\":\"20434\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"20435\",\"end\":\"20637\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"20638\",\"end\":\"21784\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"21785\",\"end\":\"21892\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2762\",\"end\":\"3375\"},{\"start\":\"3377\",\"end\":\"3920\"},{\"start\":\"3922\",\"end\":\"4262\"},{\"start\":\"4264\",\"end\":\"5104\"},{\"start\":\"5106\",\"end\":\"6307\"},{\"start\":\"6309\",\"end\":\"6635\"},{\"start\":\"6694\",\"end\":\"6889\"},{\"start\":\"6891\",\"end\":\"7244\"},{\"start\":\"7246\",\"end\":\"7670\"},{\"start\":\"7672\",\"end\":\"8223\"},{\"start\":\"8225\",\"end\":\"8507\"},{\"start\":\"8570\",\"end\":\"8754\"},{\"start\":\"8832\",\"end\":\"9483\"},{\"start\":\"9485\",\"end\":\"10373\"},{\"start\":\"10478\",\"end\":\"10541\"},{\"start\":\"10543\",\"end\":\"10696\"},{\"start\":\"10698\",\"end\":\"11169\"},{\"start\":\"11271\",\"end\":\"11449\"},{\"start\":\"11538\",\"end\":\"11636\"},{\"start\":\"11638\",\"end\":\"11745\"},{\"start\":\"11769\",\"end\":\"12065\"},{\"start\":\"12104\",\"end\":\"12320\"},{\"start\":\"12357\",\"end\":\"12642\"},{\"start\":\"12644\",\"end\":\"13167\"},{\"start\":\"13237\",\"end\":\"13368\"},{\"start\":\"13370\",\"end\":\"13456\"},{\"start\":\"13458\",\"end\":\"13940\"},{\"start\":\"13942\",\"end\":\"14210\"},{\"start\":\"14628\",\"end\":\"15481\"},{\"start\":\"15505\",\"end\":\"16218\"},{\"start\":\"16220\",\"end\":\"16512\"},{\"start\":\"16514\",\"end\":\"16759\"},{\"start\":\"16825\",\"end\":\"17606\"},{\"start\":\"17623\",\"end\":\"18768\"},{\"start\":\"18770\",\"end\":\"19091\"},{\"start\":\"19106\",\"end\":\"20064\"}]", "formula": "[{\"start\":\"8755\",\"end\":\"8831\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"10374\",\"end\":\"10477\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"11170\",\"end\":\"11214\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"11450\",\"end\":\"11537\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"11746\",\"end\":\"11768\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"12066\",\"end\":\"12103\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"13168\",\"end\":\"13236\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"14211\",\"end\":\"14565\",\"attributes\":{\"id\":\"formula_7\"}}]", "table_ref": "[{\"start\":\"15585\",\"end\":\"15592\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"16828\",\"end\":\"16835\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"17172\",\"end\":\"17179\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"17656\",\"end\":\"17663\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"18229\",\"end\":\"18236\",\"attributes\":{\"ref_id\":\"tab_1\"}}]", "section_header": "[{\"start\":\"2748\",\"end\":\"2760\",\"attributes\":{\"n\":\"1.\"}},{\"start\":\"6638\",\"end\":\"6692\",\"attributes\":{\"n\":\"2.\"}},{\"start\":\"8510\",\"end\":\"8517\",\"attributes\":{\"n\":\"3.\"}},{\"start\":\"8520\",\"end\":\"8568\",\"attributes\":{\"n\":\"3.1.\"}},{\"start\":\"11216\",\"end\":\"11269\",\"attributes\":{\"n\":\"3.2.\"}},{\"start\":\"12323\",\"end\":\"12355\",\"attributes\":{\"n\":\"3.3.\"}},{\"start\":\"14567\",\"end\":\"14597\",\"attributes\":{\"n\":\"4.\"}},{\"start\":\"14600\",\"end\":\"14626\",\"attributes\":{\"n\":\"4.1.\"}},{\"start\":\"15484\",\"end\":\"15503\",\"attributes\":{\"n\":\"4.2.\"}},{\"start\":\"16762\",\"end\":\"16769\",\"attributes\":{\"n\":\"4.3.\"}},{\"start\":\"16772\",\"end\":\"16823\",\"attributes\":{\"n\":\"4.3.1.\"}},{\"start\":\"17609\",\"end\":\"17621\",\"attributes\":{\"n\":\"4.3.2.\"}},{\"start\":\"19094\",\"end\":\"19104\",\"attributes\":{\"n\":\"5.\"}},{\"start\":\"20067\",\"end\":\"20083\",\"attributes\":{\"n\":\"6.\"}},{\"start\":\"20085\",\"end\":\"20095\"},{\"start\":\"20436\",\"end\":\"20445\"},{\"start\":\"20639\",\"end\":\"20648\"},{\"start\":\"21786\",\"end\":\"21795\"}]", "table": "[{\"start\":\"20506\",\"end\":\"20637\"},{\"start\":\"20763\",\"end\":\"21784\"},{\"start\":\"21847\",\"end\":\"21892\"}]", "figure_caption": "[{\"start\":\"20097\",\"end\":\"20193\"},{\"start\":\"20196\",\"end\":\"20434\"},{\"start\":\"20447\",\"end\":\"20506\"},{\"start\":\"20650\",\"end\":\"20763\"},{\"start\":\"21797\",\"end\":\"21847\"}]", "figure_ref": "[{\"start\":\"5301\",\"end\":\"5307\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"6775\",\"end\":\"6781\",\"attributes\":{\"ref_id\":\"fig_0\"}}]", "bib_author_first_name": "[{\"start\":\"21946\",\"end\":\"21947\"},{\"start\":\"21955\",\"end\":\"21956\"},{\"start\":\"21957\",\"end\":\"21958\"},{\"start\":\"21966\",\"end\":\"21967\"},{\"start\":\"21975\",\"end\":\"21976\"},{\"start\":\"22321\",\"end\":\"22322\"},{\"start\":\"22504\",\"end\":\"22505\"},{\"start\":\"22514\",\"end\":\"22515\"},{\"start\":\"22531\",\"end\":\"22532\"},{\"start\":\"22539\",\"end\":\"22540\"},{\"start\":\"22807\",\"end\":\"22808\"},{\"start\":\"22814\",\"end\":\"22815\"},{\"start\":\"22822\",\"end\":\"22823\"},{\"start\":\"22836\",\"end\":\"22837\"},{\"start\":\"22843\",\"end\":\"22844\"},{\"start\":\"23138\",\"end\":\"23139\"},{\"start\":\"23145\",\"end\":\"23146\"},{\"start\":\"23153\",\"end\":\"23154\"},{\"start\":\"23400\",\"end\":\"23401\"},{\"start\":\"23409\",\"end\":\"23410\"},{\"start\":\"23417\",\"end\":\"23418\"},{\"start\":\"23431\",\"end\":\"23432\"},{\"start\":\"23438\",\"end\":\"23439\"},{\"start\":\"23708\",\"end\":\"23709\"},{\"start\":\"23715\",\"end\":\"23716\"},{\"start\":\"23723\",\"end\":\"23724\"},{\"start\":\"23732\",\"end\":\"23733\"},{\"start\":\"23734\",\"end\":\"23735\"},{\"start\":\"23962\",\"end\":\"23963\"},{\"start\":\"23971\",\"end\":\"23972\"},{\"start\":\"23984\",\"end\":\"23985\"},{\"start\":\"24245\",\"end\":\"24246\"},{\"start\":\"24252\",\"end\":\"24253\"},{\"start\":\"24258\",\"end\":\"24259\"},{\"start\":\"24268\",\"end\":\"24269\"},{\"start\":\"24571\",\"end\":\"24572\"},{\"start\":\"24578\",\"end\":\"24579\"},{\"start\":\"24586\",\"end\":\"24587\"},{\"start\":\"24864\",\"end\":\"24865\"},{\"start\":\"24871\",\"end\":\"24872\"},{\"start\":\"24882\",\"end\":\"24883\"},{\"start\":\"24884\",\"end\":\"24885\"},{\"start\":\"24893\",\"end\":\"24894\"},{\"start\":\"25127\",\"end\":\"25128\"},{\"start\":\"25133\",\"end\":\"25134\"},{\"start\":\"25139\",\"end\":\"25140\"},{\"start\":\"25148\",\"end\":\"25149\"},{\"start\":\"25154\",\"end\":\"25155\"},{\"start\":\"25163\",\"end\":\"25164\"},{\"start\":\"25457\",\"end\":\"25458\"},{\"start\":\"25470\",\"end\":\"25471\"},{\"start\":\"25486\",\"end\":\"25487\"},{\"start\":\"25497\",\"end\":\"25498\"},{\"start\":\"25507\",\"end\":\"25508\"},{\"start\":\"25841\",\"end\":\"25842\"},{\"start\":\"25850\",\"end\":\"25851\"},{\"start\":\"25858\",\"end\":\"25859\"},{\"start\":\"25865\",\"end\":\"25866\"},{\"start\":\"25872\",\"end\":\"25873\"},{\"start\":\"26125\",\"end\":\"26126\"},{\"start\":\"26139\",\"end\":\"26140\"},{\"start\":\"26354\",\"end\":\"26355\"},{\"start\":\"26361\",\"end\":\"26362\"},{\"start\":\"26367\",\"end\":\"26368\"},{\"start\":\"26587\",\"end\":\"26588\"},{\"start\":\"26600\",\"end\":\"26601\"},{\"start\":\"26828\",\"end\":\"26829\"},{\"start\":\"26839\",\"end\":\"26840\"},{\"start\":\"26841\",\"end\":\"26842\"},{\"start\":\"26850\",\"end\":\"26851\"},{\"start\":\"27070\",\"end\":\"27071\"},{\"start\":\"27076\",\"end\":\"27077\"},{\"start\":\"27085\",\"end\":\"27086\"},{\"start\":\"27092\",\"end\":\"27093\"},{\"start\":\"27315\",\"end\":\"27316\"},{\"start\":\"27325\",\"end\":\"27326\"},{\"start\":\"27581\",\"end\":\"27582\"},{\"start\":\"27591\",\"end\":\"27592\"},{\"start\":\"27608\",\"end\":\"27609\"},{\"start\":\"27617\",\"end\":\"27618\"},{\"start\":\"27874\",\"end\":\"27875\"},{\"start\":\"27883\",\"end\":\"27884\"},{\"start\":\"27894\",\"end\":\"27895\"},{\"start\":\"27907\",\"end\":\"27908\"},{\"start\":\"28248\",\"end\":\"28249\"},{\"start\":\"28256\",\"end\":\"28257\"},{\"start\":\"28264\",\"end\":\"28265\"},{\"start\":\"28527\",\"end\":\"28528\"},{\"start\":\"28529\",\"end\":\"28530\"},{\"start\":\"28538\",\"end\":\"28539\"},{\"start\":\"28549\",\"end\":\"28550\"},{\"start\":\"28756\",\"end\":\"28757\"},{\"start\":\"28766\",\"end\":\"28767\"},{\"start\":\"28775\",\"end\":\"28776\"},{\"start\":\"28787\",\"end\":\"28788\"},{\"start\":\"28953\",\"end\":\"28954\"},{\"start\":\"28962\",\"end\":\"28963\"},{\"start\":\"28972\",\"end\":\"28973\"},{\"start\":\"28982\",\"end\":\"28983\"}]", "bib_author_last_name": "[{\"start\":\"21948\",\"end\":\"21953\"},{\"start\":\"21959\",\"end\":\"21964\"},{\"start\":\"21968\",\"end\":\"21973\"},{\"start\":\"21977\",\"end\":\"21986\"},{\"start\":\"22323\",\"end\":\"22328\"},{\"start\":\"22506\",\"end\":\"22512\"},{\"start\":\"22516\",\"end\":\"22529\"},{\"start\":\"22533\",\"end\":\"22537\"},{\"start\":\"22541\",\"end\":\"22546\"},{\"start\":\"22809\",\"end\":\"22812\"},{\"start\":\"22816\",\"end\":\"22820\"},{\"start\":\"22824\",\"end\":\"22834\"},{\"start\":\"22838\",\"end\":\"22841\"},{\"start\":\"22845\",\"end\":\"22848\"},{\"start\":\"23140\",\"end\":\"23143\"},{\"start\":\"23147\",\"end\":\"23151\"},{\"start\":\"23155\",\"end\":\"23157\"},{\"start\":\"23402\",\"end\":\"23407\"},{\"start\":\"23411\",\"end\":\"23415\"},{\"start\":\"23419\",\"end\":\"23429\"},{\"start\":\"23433\",\"end\":\"23436\"},{\"start\":\"23440\",\"end\":\"23443\"},{\"start\":\"23710\",\"end\":\"23713\"},{\"start\":\"23717\",\"end\":\"23721\"},{\"start\":\"23725\",\"end\":\"23730\"},{\"start\":\"23736\",\"end\":\"23742\"},{\"start\":\"23964\",\"end\":\"23969\"},{\"start\":\"23973\",\"end\":\"23982\"},{\"start\":\"23986\",\"end\":\"23993\"},{\"start\":\"24247\",\"end\":\"24250\"},{\"start\":\"24254\",\"end\":\"24256\"},{\"start\":\"24260\",\"end\":\"24266\"},{\"start\":\"24270\",\"end\":\"24273\"},{\"start\":\"24573\",\"end\":\"24576\"},{\"start\":\"24580\",\"end\":\"24584\"},{\"start\":\"24588\",\"end\":\"24590\"},{\"start\":\"24866\",\"end\":\"24869\"},{\"start\":\"24873\",\"end\":\"24880\"},{\"start\":\"24886\",\"end\":\"24891\"},{\"start\":\"24895\",\"end\":\"24904\"},{\"start\":\"25129\",\"end\":\"25131\"},{\"start\":\"25135\",\"end\":\"25137\"},{\"start\":\"25141\",\"end\":\"25146\"},{\"start\":\"25150\",\"end\":\"25152\"},{\"start\":\"25156\",\"end\":\"25161\"},{\"start\":\"25165\",\"end\":\"25168\"},{\"start\":\"25459\",\"end\":\"25468\"},{\"start\":\"25472\",\"end\":\"25484\"},{\"start\":\"25488\",\"end\":\"25495\"},{\"start\":\"25499\",\"end\":\"25505\"},{\"start\":\"25509\",\"end\":\"25516\"},{\"start\":\"25843\",\"end\":\"25848\"},{\"start\":\"25852\",\"end\":\"25856\"},{\"start\":\"25860\",\"end\":\"25863\"},{\"start\":\"25867\",\"end\":\"25870\"},{\"start\":\"25874\",\"end\":\"25884\"},{\"start\":\"26127\",\"end\":\"26137\"},{\"start\":\"26141\",\"end\":\"26144\"},{\"start\":\"26356\",\"end\":\"26359\"},{\"start\":\"26363\",\"end\":\"26365\"},{\"start\":\"26369\",\"end\":\"26372\"},{\"start\":\"26589\",\"end\":\"26598\"},{\"start\":\"26602\",\"end\":\"26608\"},{\"start\":\"26830\",\"end\":\"26837\"},{\"start\":\"26843\",\"end\":\"26848\"},{\"start\":\"26852\",\"end\":\"26861\"},{\"start\":\"27072\",\"end\":\"27074\"},{\"start\":\"27078\",\"end\":\"27083\"},{\"start\":\"27087\",\"end\":\"27090\"},{\"start\":\"27094\",\"end\":\"27097\"},{\"start\":\"27317\",\"end\":\"27323\"},{\"start\":\"27327\",\"end\":\"27333\"},{\"start\":\"27583\",\"end\":\"27589\"},{\"start\":\"27593\",\"end\":\"27606\"},{\"start\":\"27610\",\"end\":\"27615\"},{\"start\":\"27619\",\"end\":\"27628\"},{\"start\":\"27876\",\"end\":\"27881\"},{\"start\":\"27885\",\"end\":\"27892\"},{\"start\":\"27896\",\"end\":\"27905\"},{\"start\":\"27909\",\"end\":\"27915\"},{\"start\":\"28250\",\"end\":\"28254\"},{\"start\":\"28258\",\"end\":\"28262\"},{\"start\":\"28266\",\"end\":\"28271\"},{\"start\":\"28531\",\"end\":\"28536\"},{\"start\":\"28540\",\"end\":\"28547\"},{\"start\":\"28551\",\"end\":\"28560\"},{\"start\":\"28758\",\"end\":\"28764\"},{\"start\":\"28768\",\"end\":\"28773\"},{\"start\":\"28777\",\"end\":\"28785\"},{\"start\":\"28789\",\"end\":\"28795\"},{\"start\":\"28955\",\"end\":\"28960\"},{\"start\":\"28964\",\"end\":\"28970\"},{\"start\":\"28974\",\"end\":\"28980\"},{\"start\":\"28984\",\"end\":\"28991\"}]", "bib_entry": "[{\"start\":\"21894\",\"end\":\"22263\",\"attributes\":{\"matched_paper_id\":\"41754\",\"id\":\"b0\"}},{\"start\":\"22265\",\"end\":\"22445\",\"attributes\":{\"matched_paper_id\":\"32796252\",\"id\":\"b1\"}},{\"start\":\"22447\",\"end\":\"22721\",\"attributes\":{\"matched_paper_id\":\"46954166\",\"id\":\"b2\"}},{\"start\":\"22723\",\"end\":\"23060\",\"attributes\":{\"matched_paper_id\":\"52191639\",\"id\":\"b3\"}},{\"start\":\"23062\",\"end\":\"23322\",\"attributes\":{\"matched_paper_id\":\"47009959\",\"id\":\"b4\"}},{\"start\":\"23324\",\"end\":\"23653\",\"attributes\":{\"matched_paper_id\":\"202735264\",\"id\":\"b5\"}},{\"start\":\"23655\",\"end\":\"23903\",\"attributes\":{\"matched_paper_id\":\"22987563\",\"id\":\"b6\"}},{\"start\":\"23905\",\"end\":\"24166\",\"attributes\":{\"matched_paper_id\":\"4407761\",\"id\":\"b7\"}},{\"start\":\"24168\",\"end\":\"24467\",\"attributes\":{\"matched_paper_id\":\"52190317\",\"id\":\"b8\"}},{\"start\":\"24469\",\"end\":\"24798\",\"attributes\":{\"id\":\"b9\",\"doi\":\"arXiv:1804.05160\"}},{\"start\":\"24800\",\"end\":\"25064\",\"attributes\":{\"matched_paper_id\":\"67856245\",\"id\":\"b10\"}},{\"start\":\"25066\",\"end\":\"25360\",\"attributes\":{\"id\":\"b11\",\"doi\":\"arXiv:1705.02304\"}},{\"start\":\"25362\",\"end\":\"25757\",\"attributes\":{\"matched_paper_id\":\"52192022\",\"id\":\"b12\"}},{\"start\":\"25759\",\"end\":\"26068\",\"attributes\":{\"matched_paper_id\":\"212648979\",\"id\":\"b13\"}},{\"start\":\"26070\",\"end\":\"26300\",\"attributes\":{\"id\":\"b14\",\"doi\":\"arXiv:1807.08312\"}},{\"start\":\"26302\",\"end\":\"26527\",\"attributes\":{\"matched_paper_id\":\"102351511\",\"id\":\"b15\"}},{\"start\":\"26529\",\"end\":\"26771\",\"attributes\":{\"matched_paper_id\":\"54200945\",\"id\":\"b16\"}},{\"start\":\"26773\",\"end\":\"27022\",\"attributes\":{\"matched_paper_id\":\"10475843\",\"id\":\"b17\"}},{\"start\":\"27024\",\"end\":\"27241\",\"attributes\":{\"matched_paper_id\":\"206594692\",\"id\":\"b18\"}},{\"start\":\"27243\",\"end\":\"27505\",\"attributes\":{\"id\":\"b19\",\"doi\":\"arXiv:1710.05050\"}},{\"start\":\"27507\",\"end\":\"27834\",\"attributes\":{\"matched_paper_id\":\"6206708\",\"id\":\"b20\"}},{\"start\":\"27836\",\"end\":\"28141\",\"attributes\":{\"matched_paper_id\":\"1774023\",\"id\":\"b21\"}},{\"start\":\"28143\",\"end\":\"28488\",\"attributes\":{\"id\":\"b22\",\"doi\":\"arXiv:1809.04437\"}},{\"start\":\"28490\",\"end\":\"28716\",\"attributes\":{\"matched_paper_id\":\"49211906\",\"id\":\"b23\"}},{\"start\":\"28718\",\"end\":\"28894\",\"attributes\":{\"id\":\"b24\"}},{\"start\":\"28896\",\"end\":\"29181\",\"attributes\":{\"matched_paper_id\":\"14542261\",\"id\":\"b25\"}}]", "bib_title": "[{\"start\":\"21894\",\"end\":\"21944\"},{\"start\":\"22265\",\"end\":\"22319\"},{\"start\":\"22447\",\"end\":\"22502\"},{\"start\":\"22723\",\"end\":\"22805\"},{\"start\":\"23062\",\"end\":\"23136\"},{\"start\":\"23324\",\"end\":\"23398\"},{\"start\":\"23655\",\"end\":\"23706\"},{\"start\":\"23905\",\"end\":\"23960\"},{\"start\":\"24168\",\"end\":\"24243\"},{\"start\":\"24800\",\"end\":\"24862\"},{\"start\":\"25362\",\"end\":\"25455\"},{\"start\":\"25759\",\"end\":\"25839\"},{\"start\":\"26302\",\"end\":\"26352\"},{\"start\":\"26529\",\"end\":\"26585\"},{\"start\":\"26773\",\"end\":\"26826\"},{\"start\":\"27024\",\"end\":\"27068\"},{\"start\":\"27507\",\"end\":\"27579\"},{\"start\":\"27836\",\"end\":\"27872\"},{\"start\":\"28490\",\"end\":\"28525\"},{\"start\":\"28896\",\"end\":\"28951\"}]", "bib_author": "[{\"start\":\"21946\",\"end\":\"21955\"},{\"start\":\"21955\",\"end\":\"21966\"},{\"start\":\"21966\",\"end\":\"21975\"},{\"start\":\"21975\",\"end\":\"21988\"},{\"start\":\"22321\",\"end\":\"22330\"},{\"start\":\"22504\",\"end\":\"22514\"},{\"start\":\"22514\",\"end\":\"22531\"},{\"start\":\"22531\",\"end\":\"22539\"},{\"start\":\"22539\",\"end\":\"22548\"},{\"start\":\"22807\",\"end\":\"22814\"},{\"start\":\"22814\",\"end\":\"22822\"},{\"start\":\"22822\",\"end\":\"22836\"},{\"start\":\"22836\",\"end\":\"22843\"},{\"start\":\"22843\",\"end\":\"22850\"},{\"start\":\"23138\",\"end\":\"23145\"},{\"start\":\"23145\",\"end\":\"23153\"},{\"start\":\"23153\",\"end\":\"23159\"},{\"start\":\"23400\",\"end\":\"23409\"},{\"start\":\"23409\",\"end\":\"23417\"},{\"start\":\"23417\",\"end\":\"23431\"},{\"start\":\"23431\",\"end\":\"23438\"},{\"start\":\"23438\",\"end\":\"23445\"},{\"start\":\"23708\",\"end\":\"23715\"},{\"start\":\"23715\",\"end\":\"23723\"},{\"start\":\"23723\",\"end\":\"23732\"},{\"start\":\"23732\",\"end\":\"23744\"},{\"start\":\"23962\",\"end\":\"23971\"},{\"start\":\"23971\",\"end\":\"23984\"},{\"start\":\"23984\",\"end\":\"23995\"},{\"start\":\"24245\",\"end\":\"24252\"},{\"start\":\"24252\",\"end\":\"24258\"},{\"start\":\"24258\",\"end\":\"24268\"},{\"start\":\"24268\",\"end\":\"24275\"},{\"start\":\"24571\",\"end\":\"24578\"},{\"start\":\"24578\",\"end\":\"24586\"},{\"start\":\"24586\",\"end\":\"24592\"},{\"start\":\"24864\",\"end\":\"24871\"},{\"start\":\"24871\",\"end\":\"24882\"},{\"start\":\"24882\",\"end\":\"24893\"},{\"start\":\"24893\",\"end\":\"24906\"},{\"start\":\"25127\",\"end\":\"25133\"},{\"start\":\"25133\",\"end\":\"25139\"},{\"start\":\"25139\",\"end\":\"25148\"},{\"start\":\"25148\",\"end\":\"25154\"},{\"start\":\"25154\",\"end\":\"25163\"},{\"start\":\"25163\",\"end\":\"25170\"},{\"start\":\"25457\",\"end\":\"25470\"},{\"start\":\"25470\",\"end\":\"25486\"},{\"start\":\"25486\",\"end\":\"25497\"},{\"start\":\"25497\",\"end\":\"25507\"},{\"start\":\"25507\",\"end\":\"25518\"},{\"start\":\"25841\",\"end\":\"25850\"},{\"start\":\"25850\",\"end\":\"25858\"},{\"start\":\"25858\",\"end\":\"25865\"},{\"start\":\"25865\",\"end\":\"25872\"},{\"start\":\"25872\",\"end\":\"25886\"},{\"start\":\"26125\",\"end\":\"26139\"},{\"start\":\"26139\",\"end\":\"26146\"},{\"start\":\"26354\",\"end\":\"26361\"},{\"start\":\"26361\",\"end\":\"26367\"},{\"start\":\"26367\",\"end\":\"26374\"},{\"start\":\"26587\",\"end\":\"26600\"},{\"start\":\"26600\",\"end\":\"26610\"},{\"start\":\"26828\",\"end\":\"26839\"},{\"start\":\"26839\",\"end\":\"26850\"},{\"start\":\"26850\",\"end\":\"26863\"},{\"start\":\"27070\",\"end\":\"27076\"},{\"start\":\"27076\",\"end\":\"27085\"},{\"start\":\"27085\",\"end\":\"27092\"},{\"start\":\"27092\",\"end\":\"27099\"},{\"start\":\"27315\",\"end\":\"27325\"},{\"start\":\"27325\",\"end\":\"27335\"},{\"start\":\"27581\",\"end\":\"27591\"},{\"start\":\"27591\",\"end\":\"27608\"},{\"start\":\"27608\",\"end\":\"27617\"},{\"start\":\"27617\",\"end\":\"27630\"},{\"start\":\"27874\",\"end\":\"27883\"},{\"start\":\"27883\",\"end\":\"27894\"},{\"start\":\"27894\",\"end\":\"27907\"},{\"start\":\"27907\",\"end\":\"27917\"},{\"start\":\"28248\",\"end\":\"28256\"},{\"start\":\"28256\",\"end\":\"28264\"},{\"start\":\"28264\",\"end\":\"28273\"},{\"start\":\"28527\",\"end\":\"28538\"},{\"start\":\"28538\",\"end\":\"28549\"},{\"start\":\"28549\",\"end\":\"28562\"},{\"start\":\"28756\",\"end\":\"28766\"},{\"start\":\"28766\",\"end\":\"28775\"},{\"start\":\"28775\",\"end\":\"28787\"},{\"start\":\"28787\",\"end\":\"28797\"},{\"start\":\"28953\",\"end\":\"28962\"},{\"start\":\"28962\",\"end\":\"28972\"},{\"start\":\"28972\",\"end\":\"28982\"},{\"start\":\"28982\",\"end\":\"28993\"}]", "bib_venue": "[{\"start\":\"21988\",\"end\":\"22051\"},{\"start\":\"22330\",\"end\":\"22343\"},{\"start\":\"22548\",\"end\":\"22560\"},{\"start\":\"22850\",\"end\":\"22867\"},{\"start\":\"23159\",\"end\":\"23176\"},{\"start\":\"23445\",\"end\":\"23462\"},{\"start\":\"23744\",\"end\":\"23756\"},{\"start\":\"23995\",\"end\":\"24012\"},{\"start\":\"24275\",\"end\":\"24292\"},{\"start\":\"24469\",\"end\":\"24569\"},{\"start\":\"24906\",\"end\":\"24918\"},{\"start\":\"25066\",\"end\":\"25125\"},{\"start\":\"25518\",\"end\":\"25535\"},{\"start\":\"25886\",\"end\":\"25898\"},{\"start\":\"26070\",\"end\":\"26123\"},{\"start\":\"26374\",\"end\":\"26391\"},{\"start\":\"26610\",\"end\":\"26627\"},{\"start\":\"26863\",\"end\":\"26880\"},{\"start\":\"27099\",\"end\":\"27111\"},{\"start\":\"27243\",\"end\":\"27313\"},{\"start\":\"27630\",\"end\":\"27647\"},{\"start\":\"27917\",\"end\":\"27959\"},{\"start\":\"28143\",\"end\":\"28246\"},{\"start\":\"28562\",\"end\":\"28579\"},{\"start\":\"28718\",\"end\":\"28754\"},{\"start\":\"28993\",\"end\":\"29016\"},{\"start\":\"22345\",\"end\":\"22352\"},{\"start\":\"22562\",\"end\":\"22568\"},{\"start\":\"22869\",\"end\":\"22880\"},{\"start\":\"23178\",\"end\":\"23189\"},{\"start\":\"23464\",\"end\":\"23475\"},{\"start\":\"23758\",\"end\":\"23764\"},{\"start\":\"24014\",\"end\":\"24025\"},{\"start\":\"24294\",\"end\":\"24305\"},{\"start\":\"24920\",\"end\":\"24926\"},{\"start\":\"25537\",\"end\":\"25548\"},{\"start\":\"25900\",\"end\":\"25906\"},{\"start\":\"26393\",\"end\":\"26404\"},{\"start\":\"26629\",\"end\":\"26640\"},{\"start\":\"26882\",\"end\":\"26893\"},{\"start\":\"27113\",\"end\":\"27119\"},{\"start\":\"27649\",\"end\":\"27660\"},{\"start\":\"27961\",\"end\":\"27993\"},{\"start\":\"28581\",\"end\":\"28592\"},{\"start\":\"29018\",\"end\":\"29026\"}]"}}}, "year": 2023, "month": 12, "day": 17}