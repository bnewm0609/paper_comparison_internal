{"id": 18292711, "updated": "2022-03-26 14:43:22.35", "metadata": {"title": "Automated feature discovery via sentence selection and source code summarization", "authors": "[{\"first\":\"Paul W.\",\"last\":\"McBurney\",\"middle\":[]},{\"first\":\"Cheng\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Collin\",\"last\":\"McMillan\",\"middle\":[]}]", "venue": null, "journal": "Journal of Software: Evolution and Process", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "Programs are, in essence, a collection of implemented features. Feature discovery in software engineering is the task of identifying key functionalities that a program implements. Manual feature discovery can be time consuming and expensive, leading to automatic feature discovery tools being developed. However, these approaches typically only describe features using lists of keywords, which can be difficult for readers who are not already familiar with the source code. An alternative to keyword lists is sentence selection, in which one sentence is chosen from among the sentences in a text document to describe that document. Sentence selection has been widely studied in the context of natural language summarization but is only beginning to be explored as a solution to feature discovery. In this paper, we compare four sentence selection strategies for the purpose of feature discovery. Two are off\u2010the\u2010shelf approaches, while two are adaptations we propose. We present our findings as guidelines and recommendations to designers of feature discovery tools. Copyright \u00a9 2016 John Wiley & Sons, Ltd.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2554125461", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/smr/McBurneyLM16", "doi": "10.1002/smr.1768"}}, "content": {"source": {"pdf_hash": "1792930afd9ad7c3e9f3e4194a7ac636317089bb", "pdf_src": "Wiley", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "21fa4493e8ec146992015f819f11e5d20975c29a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1792930afd9ad7c3e9f3e4194a7ac636317089bb.txt", "contents": "\nAutomated feature discovery via sentence selection and source code summarization\n\n\nPaul W Mcburney \nDepartment of Computer Science and Engineering\nUniversity of Notre Dame\nNotre Dame\nINUSA\n\nCheng Liu \nDepartment of Computer Science and Engineering\nUniversity of Notre Dame\nNotre Dame\nINUSA\n\nCollin Mcmillan \nDepartment of Computer Science and Engineering\nUniversity of Notre Dame\nNotre Dame\nINUSA\n\nAutomated feature discovery via sentence selection and source code summarization\nReceived 2 October 2014; Revised 6 December 2015; Accepted 15 December 2015feature discoverysentence selectionsource code summarization\nPrograms are, in essence, a collection of implemented features. Feature discovery in software engineering is the task of identifying key functionalities that a program implements. Manual feature discovery can be time consuming and expensive, leading to automatic feature discovery tools being developed. However, these approaches typically only describe features using lists of keywords, which can be difficult for readers who are not already familiar with the source code. An alternative to keyword lists is sentence selection, in which one sentence is chosen from among the sentences in a text document to describe that document. Sentence selection has been widely studied in the context of natural language summarization but is only beginning to be explored as a solution to feature discovery. In this paper, we compare four sentence selection strategies for the purpose of feature discovery. Two are off-the-shelf approaches, while two are adaptations we propose. We present our findings as guidelines and recommendations to designers of feature discovery tools.\n\nINTRODUCTION\n\nFeature discovery in software engineering is the task of identifying the key functionality that a program implements [1,2]. A 'feature' is defined as a user-visible characteristic about the behavior of a program (e.g., 'plays mp3 files') [3]. The notion of a feature is important because programs are often thought of as implementing sets of features: Software engineers determine what features to implement through domain analysis [4] and requirements elicitation [5]. Engineers link feature descriptions to various software artifacts through traceability [6]. Acceptance testing confirms that software implements a required set of features [7]. And regulatory requirements often dictate that specific features be implemented for safety or privacy reasons [8]. In all these areas, feature discovery is a critical task because of the need to know what features a given piece of software implements.\n\nUnfortunately, at present, feature discovery is a largely manual process. Programmers typically have three options for feature discovery [9]: First, programmers may turn to software documentation, such as requirements documents. Second, programmers may read the source code of the program and execute the code with different inputs. Third, programmers may communicate directly with the authors of the source code. In the ideal case, the documentation will include an explicit list of features. But the ideal case is often not realistic because the feature lists are often either out-of-date or incomplete [10,11]. The alternative of communicating with the code authors is also often not realistic, because the authors may not even be known [9]. Therefore, programmers conducting feature discovery are forced to manually read the source code or interact with the program. This manual feature discovery process is often extremely expensive, such as in domain analysis where tens or even hundreds of programs must be processed [1,4].\n\nAutomated techniques for feature discovery have been proposed as an alternative to the manual approach [1,[12][13][14][15][16][17]. The most common strategy that has been found to be effective is to use a topic model such as Latent Dirichlet Allocation (LDA) to extract groups of keywords that are associated with different functionality [18]. This strategy will produce lists of keywords ostensibly linked to different features, for example, 'sound mp3 wav midi' versus 'save load file open'. The advantage to these approaches is that links are preserved between the lists of keywords and the source code that contains those keywords. The programmer can use these links to roughly divide the program into groups of code that implement different featuressome sections of code will be more related to the 'sound mp3 wav midi' feature than the 'save load file open' feature.\n\nBut the disadvantage is that the groups of keywords are difficult to understand without already understanding the source code. For example, the keywords 'sound mp3 wav midi' indicate that some audio functionality is implemented, but the details are obscure. It is difficult to know whether the program plays those file formats or handles streaming them across a network, or possibly converting from one format to another. Existing automated feature discovery techniques are effective at organizing programs into categories of functionality but are much less able to provide lists of features that are readable in isolation, without also reading the source code.\n\nIn this paper, we compare four sentence selection strategies as automated feature discovery techniques. These techniques produce readable, natural language sentences about software. The input to the techniques is the source code documentation, in the form of Javadocs, of a program. The output is a list of features in which each feature is described by one sentence. The techniques work by using a sentence selection algorithm to extract one sentence for each of the groups of keywords extracted by a topic model. Where Javadocs documentation is available, the tools select sentences from this documentation. Two of the sentence selection algorithms we examine have been proposed and evaluated elsewhere. Two others we adapt from related tools and propose as alternatives. Further, we conduct a follow-up study to improve the results of our Overlap approach.\n\nWe found that current methods to generating natural language feature lists from software documentation currently do not meet programmer expectations. However, our two proposed approaches are preferred when compared directly with existing textual summarization approaches. Additionally, we found that our overlap approach can be effective in giving programmers an idea of the purpose of a given program. Our work lays a foundation for future work in feature discovery by providing four approaches to generating natural language feature lists. Finally, the data for our work is presented in an online appendix \u2021 for reproducibility.\n\n\nTHE PROBLEM\n\nWe address the following gap in the literature regarding program comprehension: There currently exists no fully automatic approach to generate human readable natural language features lists of existing software engineering projects, which have been shown to meet quality standards sufficient for use by programmers. Understanding the features of a software project is necessary for using the project correctly [3,9]. Manually written documentation can suffer from a number of problems. Manual documentation is time consuming and expensive to write, leading the documentation to often be incomplete [10,11] or outdated [19,20]. Automatic approaches to describe features have produced short keywords that describe features in the source code, such as 'sound mp3 wave midi'. However, these descriptions often lack enough specificity to help the reader understand the context of the given keywords.\n\n\nFeature discovery\n\nFeature discovery is using available software resources, such as documentation, bug reports, and requirements documents, to identify features in a software project. Requirements documents exist to specify features in a project. However, as projects develop over time, features may be added or dropped. This results in documentation becoming outdated or incomplete for the purposes of feature discovery [10,11]. Automatic approaches to feature discovery have emerged to address this shortcoming. Several different types of approaches have been used for feature discovery, including conditional compilation [12], text mining available documentation [1,13,14,18], and using available software labels [16]. Related to feature discovery is feature location, where the source code relevant to a particular feature is searched for [17]. In the vein of feature discovery, Sridhara et al. presented an approach that identifies portions of source code that describe high-level actions in a given project [26]. Our work contributes to feature discovery by mining software documentation in the form of JavaDocs.\n\n\nLatent Dirichlet Allocation\n\nLatent Dirichlet Allocation (LDA), described by Blei et al. [27] is a topic modeling technique to describe a document as a set of topics. Each topic is made up of a list of keywords that can be used to describe the topic. For example, a topic with the keywords 'sound mp3 wav midi' would likely encompass portions of a document that address audio files. LDA has become the most common strategy to extract keywords in order to produce feature lists [13,18]. Additionally, LDA has been applied to a large number of other software engineering problems including software traceability [28][29][30], source code analysis [31,32], defect prediction [33], and software repository mining [34,35].\n\nLatent Dirichlet Allocation (LDA) can take a set of documents, or a corpus, such as source code documentation for different methods, and derive topics to describe the set of documents. Each topic is made up of several probabilistic keywords, where each keyword has a different weight describing how important it is to the topic. The keywords are pulled from the individual words in the documents, with the topics being created based on co-occurrence of words. Words that occur frequently together are likely to be part of the same topic.\n\nInitially, a user specifies the number of topics, n, to be generated. LDA works by first iterating through each document. Each keyword in each document is randomly assigned to one of the n topics. Then, for each document d, we iterate through each word w. For each word, we compute the probability that w appears in the topic t, and that the topic t appears in document d. Word w is then assigned to a new topic where the probability of the word being in the new topic is the highest.\n\nAfter large iterations over the corpus of documents, the topics will become more stable, with fewer words being 'moved' around from topic to topic. At this point the topics will contain lists of commonly occurring words.\n\nLatent Dirichlet Allocation (LDA) has several tunable parameters that must be assessed carefully [31]. First, an LDA user must choose the number of topics to create from a corpus of documents. Each corpus must be considered individually. If the number of topics is too large, the topics become unstable, noisy, and too specific to be useful. If the number of topics is too small, key information may be lost, and incompatible concepts may be meshed into a single topic.\n\nTwo more parameters in LDA that must be tuned are \u03b1 and \u03b2. \u03b1 refers to the strength of the prior belief that all the topics are uniformly distributed throughout the documents. A large \u03b1 assumes that each document contains a large number of topics. A small \u03b1 assumes that each document is made mostly of a small number of topics. A large \u03b1 therefore encourages simple topics with a small number of important keywords [31]. \u03b2 affects the number of words per topic. A smaller \u03b2 results in topics containing fewer keywords. A larger \u03b2 results in topics that contain multiple important keywords. These parameters will be tuned differently depending on the goal of the topic model [31]. In feature location, a larger \u03b1 would be preferable, as it would encourage more topics-per-document and increase the recall of identifying the topic within relevant sections of code [31].\n\n\nLatent Dirichlet Allocation Genetic Algorithm\n\nThe LDA Genetic Algorithm (LDA-GA) is an algorithm developed by Panichella et al. [15] that determines a near-optimal set of LDA input parameters for a given project. The parameters LDA-GA recommends are \u03b1, \u03b2, number of topics, and number of iterations. LDA-GA does this by using genetic algorithms to randomly generate, combine, and mutate different LDA input configurations. LDA starts with a set of randomly generated parameters, which can fit a range specified by a user. For example, a user may specify that they want an \u03b1 no less than .1 and no greater than .9. Additionally, users can set a minimum and maximum number of allowable topics. LDA is then run for each set of random input parameters. Each configuration is then scored on a fitness function. This fitness function scores a configuration based on the topics that configuration produces. Topics that are cohesive, where documents that most fit into the topics are most similar, are considered better. Additionally, a high separation, where documents are dissimilar to the average of documents in another topic, is preferable. The most fit LDA configurations are then used to generate the next generation. These configurations are combined with a small chance for a random mutation. After several generations, the population with stabilize around a particular near-optimal configuration of input parameters. These input parameters can then be used to run LDA on the system.\n\n\nLightweight Semantic Similarity\n\nLightweight Semantic Similarity (LSS) is a metric to determine the similarity of two small pieces of text developed by Croft et al. [36]. LSS was designed to compare two short text items regardless of whether or not they have sentence structure. Specifically, LSS was originally used to compare captions of photographs to determine whether two pictures were on a similar topic. LSS determines the similarity between two pieces of text by considering word semantics. Semantics, in this case, refers to considering the word's meaning, rather than only considering its literal text.\n\nFor example, according to LSS, a sentence 'This method plays a sound file' would be semantically very similar to 'This function plays an audio item'. Even though the literal words in each sentence are not the same, the semantics, or meanings, of these words are nearly identical. These two sentences would be more similar to each other than a sentence like 'This method calculates the square root of the input', which has nearly no semantic similarity other than referring to the existence of a method.\n\nLightweight Semantic Similarity (LSS) judges semantic similarity using WordNet [37]. Wordnet is a hierarchical data structure used to classify words and determine similarity between multiple words. Words are classified into synsets. A synset is a set of synonymous words. A synset can, itself, contain multiple synsets. This forms a hierarchy with more general words at the top of the hierarchy, and more specific words lower in the hierarchy. For example, 'music' would likely be below 'sound' in the Wordnet hierarchy, as 'music' refers to a particular subset of 'sounds'.\n\nSimilarity between words is determined by the distance between each words location in WordNet. Additionally, the depth of two synsets deepest common ancestor is directly proportional to the similarity of two words. This is because terms deeper in Wordnet are more specific, while keywords higher are more general.\n\nLightweight Semantic Similarity (LSS) compares two sets of text, A and B in 2 basic steps. First, LSS preprocesses the text items. A and B are stripped of all special characters and punctuation. Duplicate tokens within A and B are also removed. During preprocessing, LSS uses Wordnet to identify the synsets for each words between the two text items being compared.\n\nSecond, LSS constructs C, which is a concatenation of the terms in A and B. C is then used to create similarity matrix C. Each item C i,j is equal to the similarity between items i and j, found using WordNet. Using A as an example, we create a weighted term vector A. Each element A i is set to the sum of all elements in A compared with C i , using the similarity matrix C. The same process is repeated for B to produce the weighted term vector B. The similarity between A and B is then defined as the cosine similarity between vectors A and B. The output will be between 0 and 1, with 1 being an identical match and 0 being completely semantically dissimilar. In practice, a similarity of 0 is rarely, if ever, found.\n\n\nTextRank\n\nTextRank is a graph-based model for text processing created by Mihalcea et al. [38]. TextRank is an unsupervised method that can be used for either keyword or sentence extraction. In this paper, we focus on TextRank's sentence extraction tool. We used an implementation of the TextRank algorithm created by David Adamo. \u00a7 This implementation returns a 100-word summary of the body of text selected from the highest-rated sentences.\n\nTextRank works in three basic steps. First, we extract all sentences from a block of text. Each sentence becomes a vertex in a graph. The edges in the graph are then weighted by the similarity. This results in a highly connected graph. A weight of 1.0 between two vertices would mean two identical sentences. The larger the weight of the edge, the more similar the sentences. Edge weight can be determined by any type of similarity metric. In this project, we use the Levenshtein distance [39] to calculate the edge weights. This was selected over more common approaches such as using cosine similarity because of a somewhat better performance in internal preliminary experimentation. This experimentation was done using a selection of Java projects we have used in software studies [21,40]. These projects include NanoXML, Jedit, and JHotDraw. The final step is to run PageRank to score each vertice. The vertices are then sorted by their PageRank score, and the top scoring vertice sentences are used as the feature list.\n\n\nToo Long Didn't Read\n\nToo Long Didn't Read (TLDR \u00b6 ) is a plug-in extension available for various web browsers, including the Google Chrome** web browser. \u2020 \u2020 TLDR takes as input a webpage, such as a news article. TLDR uses the PlexiNLP \u2021 \u2021 API, previously Stremor, as a natural language processing tool to automatically generate summaries. TLDR automatically selects sentences from a given web page to return as a summary. Users can customize how large they want the summary to be. Users can choose to have a short summary containing just a few sentences or larger summaries that contain more of the source material, such as 25%, 50%, or 75% of the original source material. In small summaries, TLDR attempts to isolate the most important sentences from the article. For larger summaries, TLDR attempts to filter out extraneous sentences that do not provide core understanding. TLDR often looks for repeated words and leverages document format in order to determine important words and phrases. In this study, we use TLDR's 'summary' feature, which returns a summary anywhere from 3 to 7 sentences long of the body of text.\n\n\nAPPROACH\n\nThis section details how we use the supporting technologies described in Section 3 to generate the summaries and feature lists evaluated in our user study. An overview of our approach is illustrated in Figure 1. This section first describes how we collect and preprocess documentation from a Java project. Additionally, we describe the four summarization approaches used in our user study. The two feature list tools, Overlap and LSS, that use LDA are described in Section 4.2. The two textual analysis summarization tools are described in Section 4.3. Both of these approaches, as well as TextRank and TLDR, require the source code to have Javadocs comments. Programs that have very limited or no documentation cannot be effectively analyzed using this approach.\n\n\nDocumentation processing\n\nIn this section, we will describe how we extract and preprocess Java documentation in order to create feature lists. For consistency, we use the same document extraction process for both LDA-based approaches and textual analysis tools.\n\nOur extraction process is illustrated in Figure 1. Initially, we extract all comments and JavaDocs from the source code of a Java project. All special characters are removed from the extracted text except for periods, question marks, and exclamation points. The exception is made for these characters because they are sentence delimiters. The text is then split into sentences using these delimiters. We do not always split on periods. If, for example, a period is between two numbers with no whitespace, it is likely a version number delimiter. Additionally, each method's Javadocs are separated by linebreaks. This means if a particular Javadoc section does not end with a punctuation, as was often the case, we treat the end of that Javadoc as the end of a sentence. In-line comments, which rarely have punctuation at the end to denote the end of a sentence, have a period added. Additionally, if a Javadoc contains two or more consecutive linebreaks, we treat those line breaks as the end of a sentence even if there is no punctuation before the line breaks. We cannot treat individual line breaks in Javadocs as the end of sentences, as linebreaks are often added to ensure the entirety of the documentation stays 'on screen'. After separating sentences, we processed the words in the sentences as follows. First, we split all words on camelcasing. This means identifier names such as 'loadFile' become 'load file'. Finally, we remove Javadocs keywords such as '@param'. This is done because they do not add to the sentences semantic information when looked at from the project-level granularity. Figure 1. This flowchart shows how we extract a feature list from source code documentation. First, we extract sentences from the documentation. We then create topics from those sentences using Latent Dirichlet Allocation (LDA). Finally, for each topic, we select a sentence using either Overlap or Lightweight Semantic Similarity (LSS) to create a feature list. The feature is made up of a list of sentences that are most similar to the topics generated by LDA.\n\nThis forms our list of sentences that are used by the LDA-based approaches and the textual analysis tools to select sentences to include in the feature list. The list of sentences are also used by LDA to create the topic model for the given Java project. Before the sentences are used by LDA, we perform some additional text modification. First, we removed a list of stop words. Stop words are words that add little semantic information. We considered two types of stop words. We removed English stop words such as 'the', 'an', and so on. Next, we removed Java stop words. These include common keywords in Java such as 'return', 'if', and so on. These words are removed from the sentences before running LDA to ensure we specifically focus on keywords that would contain meaningful semantic information.\n\n\nLatent Dirichlet Allocation-based approaches\n\nThis section will explain how we use LDA in our Overlap and LSS feature list generation tools. We will explain how we configure and use LDA to create topic models from sentence lists. We will then explain how we use these topics models to generate a feature list using Overlap and LSS.\n\n\nConfiguring Latent Dirichlet\n\nAllocation. For our approach, we used the parallel C++ implementation of LDA (PLDA) created by Liu et al. [41]. PLDA produces an output topic model of a specified number of topics. We chose to produce 10 topics for all Java projects. While this number may be too large for simpler projects, we found by inspection that it created a balance of coverage and conciseness for most projects. Before we run PLDA, we remove several stop words, such as 'the', 'a', 'and', and so on. We then give the term frequencies of each Java class in a project to PLDA as input. We set the values of the parameters \u03b1 and \u03b2 to .1 and .01, respectively. We chose \u03b1 to be small because we believed that each class (which were treated as documents) would contain a small number of topics, and often only 1. We chose \u03b2 to be very small because we believed most keywords would only apply to at most one topic, because the large size of some projects' documentation. Our topic model was created after 2500 iterations, of which 1500 were 'burn-in' iterations. The output topic model is then used in our Overlap and LSS approach to select sentences from the sentence list.\n\n\n4.2.2.\n\nOverlap. Using the list of sentences extracted from the documentation and the LDA topic model, we create a feature list for a given Java project. We do this by finding the 'best' sentence for each topic in the LDA topic model. The 'best' sentence is the sentence that scores highest using the Overlap metric. The process by which we create a feature list using Overlap is illustrated in Figure 2.\n\nTo calculate overlap between a topic and a sentence, we compare the ten most important words in the given topic to the words in the sentence. First, we remove all the stop words from the sentence that we remove from the LDA model. Then, we define the list of the top ten topic keywords as A. Additionally, we define the list of words in the given sentence as B. Using these definitions, Overlap is calculated using the following formula: Figure 2. This flowchart shows how we generate a feature list using a list of topics from the Latent Dirichlet Allocation topic Model and the list of sentences. This process is typically repeated, for this study, 10 times, once for each topic. The sentence selected for each topic is the one that scores highest with respect to the given similarity metric, either Overlap or Lightweight Semantic Similarity (LSS).\nOverlap A; B \u00f0 \u00de \u00bc A\u2229; B j j A\u222a; B j j (1)\nThe more words that are shared by the topic and sentence, the more similar the topic and the sentence are. Note that we do not use stemming in our selection of words. If not including stemming results in a decreased performance, we would expect the LSS approach to outperform Overlap. This is because words sharing a common root are usually in the same, or nearly the same, synset in WordNet. For each topic in the LDA topic model, we select the sentence from the sentence list that has the highest overlap score. Because our LDA topic model is typically 10 topics, this will produce 10 sentences. These 10 sentences are used as the feature list.\n\n\nLightweight Semantic\n\nSimilarity. The process of selecting sentences using LSS to form a feature list is nearly the same as Overlap. We use the same LDA topic model as Overlap for LSS, as well as the same sentence list. The only difference is that, instead of using the Overlap metric to select the best sentences, we use the LSS metric. This will produce a set of sentences with one sentence for each topic. The selected sentence is the most semantically similar sentence to the topics. This allows for sentences with words that are similar in meaning to the topic keywords, even when the words are literally different, to be selected as the best sentence.\n\n\nTextual analysis tools\n\nOur textual analysis tools, TextRank and TLDR, take in the list of sentences and output a summary, which we use as the feature list for the given Java project. Our textual analysis tools do not make use of LDA. TextRank is described in Section 3.5, and TLDR is described in Section 3.6. We use an implementation of TextRank created by David Adamo. \u00a7 \u00a7 TextRank creates a summary of the sentence list. TLDR was run by opening each sentence list as a text file in the Google Chrome web browser. \u00b6 \u00b6 The TLDR plug-in generated a summary for the page, which we use as the feature list.\n\n\nEVALUATION\n\nIn this section, we outline our methodology for evaluating the four approaches for automatic project summarization. We evaluate our two approaches that generate features lists, Overlap and LSS, alongside our approaches for textual summarization, TextRank and TLDR. We define our research questions and describe how we answer the research questions.\n\n\nResearch questions\n\nIn this paper, we conduct a user study to determine how the four project summarization tools perform.\n\nWe seek to answer the following research questions: RQ 1 Which automatic project summarization approach do programmers find to be the most accurate, and to what degree? RQ 2 Which automatic project summarization approach do programmers find to be the most content adequate, and to what degree?\n\nRQ 3 Which automatic project summarization approach do programmers find to be the most concise, and to what degree?\n\nRQ 4 Which automatic project summarization approach do programmers find to be the most readable, and to what degree?\n\nRQ 5 When asked to compare two automatic project summarization approaches directly, which automatic project summarization approach do programmers prefer most?\n\nThe rational for RQ 1 , RQ 2 , and RQ 3 is to find the quality of our project summary approaches. Accuracy, content adequacy, and conciseness are metrics that have been used to measure quality in previous automatic summarization studies [21,23]. A summary that is accurate will not provide a programmer with false information about a project. A summary that is content adequate includes all or most of the important features within a project. A summary that is concise does not contain trivial information that is unnecessary to understanding the project. The rationale for RQ 4 is to determine how easily programmers can read the output. Given that all four of the approaches rely on sentence selection, it is important to know if combining sentences results in confusion. Possible sources of loss of readability could be having disjointed sentences or pronoun disagreement. The rationale for RQ 5 is to see, when comparing two generated summaries directly, which approach's summaries programmers prefer.\n\n\nData collection\n\nInitially, we selected 50 Java projects a large repository of SourceForge*** Java projects. These projects represent a wide variety of purposes and include a text editor, a music player, and an XML parsing tool, among many other purposes. Six of the projects selected were from our previous work in automatic source code summarization [21]. The other 44 projects were randomly selected from a repository of SourceForge Java projects. Selections were limited to projects with raw source code available that also had a project website. This limitation was added so, in our user study, programmers could quickly get an expert summary of what a given project's purpose is. Four of the 50 of the projects were removed from this list because of a lack of source code documentation.\n\nOf the remaining projects, another 10 were removed because they were too large for our implementation of TextRank to summarize. Our TextRank implementation could generally handle up to 2 million characters from extracted sentences before having memory problems. This mean TextRank could not be used on larger projects. Our other approaches could handle all the projects, although LSS took several days to generate sentences for the largest processes because of WordNet. By comparison, Overlap took several seconds to generate sentences for the same projects. Another four projects were then removed because their entire source code documentation was less than 20 unique sentences. This was seen as too small to be useful for generating automatic summarizations from the documentation. In total, we summarized 32 projects using all four approaches. These projects and their summaries are available in the online appendix (Section 5.6).\n\n\nUser survey\n\nThis section describes our user study survey. The remaining participant was an information technologies professional employed by the University of Notre Dame. Participants were initially vetted to ensure they had a computer science or programming background. The participants took the survey online and were asked to work alone. Each participant was asked 5 evaluation questions and 10 comparison questions, in that order. Both of these question types are described in the succeeding sections. The participants could skip a question at their own discretion. Reasons for skipping a question could be not understanding a project well enough to rate a summary. Additionally, participants could end the survey at any time. Surveys results where participants took less than 2 min per question on average were discarded on the basis that the participant may not have given due diligence to answer the questions honestly.\n\n\nEvaluation questions.\n\nEach participant is first asked to answer five evaluation questions. An example of an evaluation question is shown in Figure 3. Each evaluation question selects a random project and one of the four automatically generated summaries or feature lists generated for the project. The participant is given the name of the project, a version number, the project website, a link to download the source code for that version of the project, and a summary. Each participant is asked to, in their own words, state what they believe the purpose of the project is. The participant is then asked four multiple choice questions. The questions ask the participants if they agree that the given summary or feature list is accurate, complete, concise, and readable. These questions respectively correspond with RQ 1 , RQ 2 , RQ 3 , and RQ 4 . The participant can choose between 'Strongly Agree', 'Agree', 'Disagree', and 'Strongly Disagree'. After answering all the questions, the participant may move on to the next question by clicking a button in the bottom right of the page.\n\n\nComparison questions.\n\nAfter the evaluation questions, each participant is asked 10 comparison questions. An example of a comparison question is shown in Figure 4. Each comparison question randomly selects a project, then randomly selects two summaries or features lists generated for the project. As in the evaluation questions, the participants are given the name of the project, a version number, the project website, a link to download the source code for that version of the project. Unlike in the evaluation question, participants are not asked to summarize the project in their own words. This was done to avoid fatigue on the part of the participant. The participants were simply asked to choose, via radio button, which summary or feature list they felt was better. Additionally, the participants are given a third option, 'Both of these choices appear identical'. This option was included because, for some projects, Overlap and LSS generated very similar summaries. After answering, participants could advance to the next question by clicking a button in the bottom right of the page.\n\n\nParticipants\n\nOur user study was conducted by 13 participants. Participants were compensated $30 for their time spent evaluating our tool. Of the 13 participants, one survey response was thrown out for incompleteness, leaving us with 12 participants. Eleven of these participants were graduate students Figure 3. This is a screen capture of an evaluation question from the survey. In this question, the participant is asked to evaluate a TextRank-generated summary of the documentation for the JKiwi project. The participant is asked to evaluate the summary by answering one essay question and four multiple choice questions in the Department of Computer Science and Engineering at the University of Notre Dame. The remaining participant was an information technology professional. We informed participants when they were recruited that the participants should be comfortable reading and understanding the Java programming language. However, out study did not require the participants to read the Java source code, nor did it require them to complete any programming task.\n\n\nStatistical test\n\nIn this study, we will use the Mann-Whitney statistical test [42] to answer our research questions. Mann-Whitney is selected because we cannot guarantee our results will be normally distributed. Additionally, our results are unpaired due to the random nature of the question selection. When performing statistical tests comparing four groups of responses, where each set of data represents one of the four approaches, we use the following procedure: (1) We sort each set of data by the mean of the score being tested. (2) The set of data with the best mean is given rank 1. (3) The set of data with the next highest mean is compared with the first group using Mann-Whitney. If there is no statistically significant difference, the second set of data is also given rank 1. Otherwise, if there is a statistically significant difference, the second set of data is given rank 2. For each remaining population, we compare with all samples in the highest current rank. This means, for example, in the case where both of the first two populations are rank 1, we compare the third set of data with both the first and the second using Mann-Whitney. Thus, it is possible that all four sets of data will have the rank 1, meaning no statistically significant difference exists between all four sets of data.\n\n\nReproducibility\n\nFor reproducibility, we have posted the list of projects we used in this study to an online appendix. \u2020 \u2020 \u2020 We have additionally included our anonymized survey data in the appendix. \u2020 \u2020 \u2020 http://www.nd.edu/ pmcburne/features/ Figure 4. This is a screen capture of a comparison question from the survey. In this question, the participant is asked to choose which of two summaries the feel is better for describing the NanoXML project. The participant is asked to evaluate the summary by answering a single multiple choice question.\n\n\nThreats to validity\n\nAs with all software engineering studies, a source of threat to validity are the projects selected for study. We cannot guarantee that a similar study that used different projects will produce the same results. We mitigate this threat by studying 32 projects with a wide variety of purposes. We avoid the threat of author selection bias by randomly selecting the projects. As a result of the large number of projects we consider, however, we can expect documentation quality to vary. Some projects we consider are poorly documented. This decision was intentional, to account for a possible weaknesses of each approach. However, this threat should be mitigated by the fact that we use all four approaches for each project. Poor documentation, specifically a lack of sufficient Javadocs, that would hurt one approach would almost certainly hurt the other approaches as well, as all approaches relied on leveraging Javadocs to generate sentence lists. Additionally, our set of participants could be a source of threat to validity. As stated in Section 5.4, we sought participants from a variety of backgrounds to mitigate this threat. However, we cannot guarantee a similar study conducted with different participants would produce the same results. Nearly all of our study participants were graduate students, with one professional programmer. We felt comfortable that these participants would be capable of understanding the high-level concepts of each project. Because we only had one professional programmer, we cannot meaningfully differentiate between the behavior of student participants and professional participants.\n\nThe approaches in our study can produce different numbers of sentences. While LSS and Overlap typically produce 10 sentences, this is not always the case in TextRank and TLDR. This can threaten the validity of our completeness and conciseness results. We chose not to mitigate this threat by setting a fixed number of sentences to generate. This was because reducing the number of topics with LSS and Overlap would significantly decrease the usefulness of the LDA model generated. We chose not to expand TextRank, as it usually led to repeating the same sentences that were in the smaller summary. This would mean TextRank would significantly decrease in conciseness without improving in completeness. TLDR did not allow us to modify the size.\n\nAnother source of threat to validity comes from only studying Java projects that have an associated website. These websites provide an explanation of the source code authors' intended purposes for the project. This could bias a participant where they rate one automatically generated summary higher based on that summary being more similar to the summary provided by the authors. This choice was made so that study participants could quickly see what a given Java project they are unfamiliar with. We believe this condition was necessary, as asking the participants to understand a project by reading documentation and studying the source code would have resulted in a large amount of fatigue, forcing our study to limit itself to a very small number of questions per participant. This also would have likely meant less understanding of the projects by the participants, resulting in less useful results.\n\nA threat to validity unique to TextRank and TLDR summaries comes from the formatting of the documentation. TextRank and TLDR both can leverage the format of their input text. Specifically, TextRank and TLDR consider paragraph breaks. As we described in Section 4.3, we add paragraph breaks after each method summary in our documentation parsing. This is done because we consider each method summary to be it's own encapsulated piece of information. We cannot guarantee that a different formatting decision would result in the same results. However, we believe based upon preliminary analysis that this difference, if any, would be limited.\n\n\nRESULTS\n\nIn this section, we present the results of our user study. We answer the five research questions defined in Section 5.3.1. The average scores for the Evaluation Questions on each approach on each metric can be seen in Table I. These averages are taken from the evaluation questions of our survey. We quantified the user answers of 'Strongly Agree', 'Agree','Disagree', and 'Strongly Disagree' as 4, 3, 2, and 1, respectively. In this way, larger numbers represent better averages. Overall is the average of all four approaches collectively.\n\n\nRQ 1 accuracy\n\nOur study found that there was no statistically significant difference between any of the four approaches with respect to accuracy. Our study found that, on average, participants disagreed that the generated feature lists accurately depicted the given Java project, regardless of the approach. However, TextRank did perform a fair amount worse than the other three approaches.\n\nLightweight Semantic Similarity (LSS) performed the best on accuracy, with an average of 3.06. Overlap and TLDR performed very similarly, with averages of 3.07 and 3.08, respectively. TextRank scored somewhat worse, with an average accuracy score of 3.38, which is near the borderline between 'Disagree' and 'Strongly Disagree'. For TextRank, participants 'Strongly Disagreed' that the given feature list was accurate in a majority of cases. According to our statistical tests in Table II, none of the approaches are significantly better or worse than the others. Thus, while TextRank does have a notably worse mean than the other approaches, it does not meet the threshold of significance.\n\nWhen looking at all four approaches as a whole, none of the approaches performed particularly well. Across all four approaches, participants only 'Strongly Agreed' that the given feature list was accurate  two times: once for LSS and once for TLDR. In 42.4% of cases, the participants 'Strongly Disagreed' the given feature list was accurate. Possible reasons for the overall poor performance are discussed in Section 7.\n\n\nRQ 2 completeness\n\nOur study found that there was no statistically significant difference between any of the four approaches with respect to content adequacy or 'completeness'. We found that participants 'Disagreed' or 'Strongly Disagreed' that the given feature list was complete in a large majority of cases. Discussion as to why these approaches lack completeness can be found in Section 7. Too Long Didn't Read (TLDR) had the best average at 3.08, which is slightly worse than 'Disagree'. However, in TLDR, participants 'Agreed' or 'Strongly agreed' the feature list was complete in 23.1% of cases. Overlap had the second best average at 3.20. However, no participant 'Strongly Agreed' that an Overlap feature list was complete. LSS and TextRank had average scores of 3.31 and 3.38, respectively. According to our statistical tests in Table II, we found no statistically significant differences across all four approaches.\n\nOnce again, all four approaches on average rate worse than 'Disagree'. Overall, participants only 'Strongly Agree' or 'Agreed' the feature list was complete 15% of the time across all approaches. The plurality of responses, 46.7%, 'Strongly Disagree' that the feature lists were complete. This, along with the individual results, implies that these automatic approaches do not provide a good coverage of sentences that explain the diverse uses of the projects. This discussion is expanded on in the next section.\n\n\nRQ 3 conciseness\n\nOur study found that there was no statistically significant difference between any of the four approaches with respect to conciseness. Overlap performed the best with respect to conciseness with an average of exactly 3.0. This means that for the best approach, users on average still 'Disagreed' that the automatically generated feature lists were concise.\n\nAs stated, Overlap created what are participants perceived as the most concise feature lists on average. TextRank and TLDR had average scores of 3.13 and 3.15, respectively. However, participants 'Strongly Disagreed' with a majority, 56.3% of TextRank-generated feature lists they were presented with. LSS performed the worse with conciseness with an average of 3.31. In LSS, participants 'Disagreed' or 'Strongly Disagree' that the presented feature list was concise 87.5% of the time. According to our statistical tests in Table I, there was no statistically significant difference between the approaches.\n\n\nRQ 4 readability\n\nOur study found that there was no statistically significant difference between any of the four approaches, although TextRank is a borderline case. Overlap provided the most readable feature lists on average, while TextRank provided the least readable feature lists.\n\nOn the whole, participants found overlap feature lists were the most readable, with an average score of 2.53. In a majority of cases, 53.5% participants 'Agreed' that the features lists were readable. However, no participants 'Strongly Agreed' that any Overlap feature lists were readable. TLDR averaged the second best readability, although a majority of time, 61.5% participants said they 'Disagreed' or 'Strongly Disagreed'. LSS had an average readability score of 3.0 exactly, meaning on average participants 'Disagreed' that the feature lists generated by LSS were readable. TextRank had the least readable feature lists. Participants 'Strongly Disagreed' a majority of the time, 56.3%, that TextRank summaries were readable. According to our statistical test, there was no statistical difference between any of the approaches. However, TextRank, with a p-value of .086, was only just above the .05 threshold of significance.\n\n\nRQ 5 comparison\n\nDespite the mixed results of RQ 1 through RQ 4 , when asked to compare directly, participants favor feature lists generated by LDA-based approaches Overlap and LSS to textual summarization-based approaches TextRank and TLDR. When asked to compare two feature lists directly, Overlap was selected as the better choice more often than any other approach. Additionally, Overlap lost the least, where losing is defined as the other approach's feature list being selected as the better of the two. By contrast, TLDR won the least and lost the most. Table III shows the breakdown of wins and losses for comparisons of approaches. For example, in Table III where the winner is Overlap and the loser is LSS, we have the number 7. This means there were seven instances where, when the participant was given a feature list generated by Overlap and a feature list generated by LSS, the participant chose the Overlap feature list seven times. Participants only selected the LSS summary over the Overlap summary one time. For space, ties are not illustrated in this table. In this table, Overlap wins against every other approach more than it loses. TLDR appears to perform the worst, losing very frequently to Overlap and LSS. However, TLDR still wins against TextRank more often than it loses.\n\nThe winning, losing, and tieing percentages for the comparison question are shown in Table IV. Here we can see that Overlap has the largest winning percentage and the smallest losing percentage of all approaches. LSS has the second highest winning percentage, but also the second highest losing percentage. The very low tie percentage for LSS implies that participants either found the LSS feature list effective or very poor. Possible reasons for this are discussed in Section 7. TextRank and TLDR had much lower winning percentages than Overlap and LSS. TLDR also has the highest losing percentage by over 10%. These tests suggest that the LDA-based approaches tend to beat the textual summarization-based approaches when compared directly.\n\n\nDISCUSSION\n\nThis section discusses the impact of the results of our study. We additionally discuss the wider impact of our study in the field of feature discovery.\n\nOur results for RQ 1 , RQ 2 , and RQ 3 show that, in general, study participants found all automatically  generated summarization tools to perform to a level not meeting sufficient quality. Participants, on average, disagreed on all four of the sentence selection tools for automatic feature list generation across accuracy, completeness, and conciseness. Overall, our findings suggest that sentence selection techniques are as yet insufficient for feature discovery. This is a key finding because sentence selection is currently in use in software engineering research tools [43] our study suggests that additional innovation is needed for these tools to enter industrial use. However, sentence selection has been effective for other forms of text such as news articles. It is interesting to note that while our evaluation question results showed no statistical difference between the four approaches, our comparison question results suggest that participants clearly favored features lists created by LDA-models over textual analysis models. A reason for this difference could be that software documentation is generally made up of disjoint method and class summaries. Rather than a cohesive order of events like a news story, each piece of documentation is much more independent. While LDA focuses on overarching topics, the textual analysis tools are trying to determine each sentences importance by leveraging structure and order. However, software documentation is not strictly dependent on order, because Java methods can be in any order in the source code. Further, the structure of software documentation is very different than the structure of a news article. We believe future work might benefit from trying to find more general sentences in software documentation that discuss method interactions. Focusing on highly interactive methods proved beneficial in prior automatic summarization work [21].\n\nOverlap, LSS, and TLDR all had very close reliability averages. TextRank was not statistically worse, although it did have the worst average. Of these approaches, Overlap always had the fastest run-time. Overlap simply uses set operations, which have very low time complexity. TLDR requires communication with a server, although it rarely takes more than a few minutes on large projects. On large projects we examined, such as jQuantlib, \u2021 \u2021 \u2021 running LSS and TextRank could take a full day. These long run-times were mostly due to these approaches needing to interact with WordNet. Interactions included search and path finding, which on a large network of words can be time consuming. TextRank and LSS were the largest limiting factor in projects we could select because of their long run-times and large memory requirements. However, even with all the added semantic understanding that using WordNet can allow, LSS and TextRank do not outperform Overlap in terms of feature list quality. On the evaluation questions, Overlap did not perform significantly better or worse than either TextRank for LSS. In the comparison questions, Overlap had a larger win percentage and smaller loss percentage than both TextRank and TLDR. This suggests that, given the current state of sentence selection, it may be better to use simpler metrics like Overlap for large projects.\n\nThe automated approaches, in general, performed better with respect to readability than other areas. This is likely because all these approaches select sentences from a pool of sentences written by human experts. While the other metrics rely on these sentences working together to communicate a larger goal, a feature list can still be readable even if the sentences do not interact with each other. Readability appears to result from the grammatical correctness of the sentences being selected rather than their overall cohesiveness.\n\nA common problem with the LSS approach was that at times LSS would select very large sentences. Because of how LSS compares two bodies of text, large sentences often get artificially inflated, as a large number of words means at least one word is more likely to be semantically similar. We encountered this in previous work [44]. When this did occur, conciseness scores and accuracy scores were usually lower. Often, this resulted in run-on sentences in documentation being inflated over short sentences that were also related to the topic.\n\nSome of the projects we investigated were poorly documented. Some projects have more unique files than they have unique sentences in Javadocs. Our results, investigated on a project by project basis, appear to suggest that projects that have a very small number of unique sentences in the Javadocs perform poorly across all four approaches. However, this only affects a small number of the projects we studied. The difficulty is that comment quality is not always related to the number of unique sentences. While clearly having very little documentation is problematic, that does not mean large projects consistently perform better. Conciseness is an important component of documentation quality [21,23,45]; thus, more sentences to choose from is not better if those sentences are not meaningful to the high-level purpose of the project. Noteworthy examples of sentences that do not serve a high-level purpose include authorship statements, licensing agreements, and GUI-related information. Future approaches could seek to remove these sentences from the base of considered sentences as a preprocessing step. The large projects in our approach were often noisy. Some of these large projects performed just as poorly as the projects with more Java classes than unique sentences. This suggests that having a diverse pool of sentences to choose from may be necessary, but it is certainly not sufficient to our approach working well.\n\nThe goal of our approaches to feature discovery is to acquaint programmers unfamiliar with a given Java project with the purpose of that project. However, completeness can be difficult for inexperienced users to judge. This is because a programmer unfamiliar with a given system will likely early learn enough about a system to understand its general purpose in order to respond to our survey. Completeness, however, is about a given feature list noting every task a system can complete or be used to complete. In order to properly evaluate completeness, future studies will likely require the assistance of the developers of the project being evaluated.\n\n\nFOLLOW-UP EVALUATION\n\nWe conducted a brief follow-up study in an attempt to improve our Overlap approach. In this section, we discuss the method of evaluation for our follow-up study. Our follow-up study has the following goals: (1) to perform a focused study on well-documented projects to prevent our results from being affected by poor documentation; (2) to see if Overlap's performance can be improved by using LDA-GA to determine near-optimal input parameters for LDA; and (3) to examine how effectively individual sentences convey high-level understanding of a project's purpose. Specifically, we ask the following research questions:\nRQ 6\nTo what degree does a list of sentences, selected from a well-documented Java Project using Overlap, provide understanding of the purpose of the project?\nRQ 7\nTo what degree are individual sentences, taken one-by-one, selected from a well-documented Java Project using Overlap, relevant to the overall purpose of the project?\n\nThe key difference between RQ 6 and RQ 7 is that RQ 6 looks at full lists of sentences as a whole, whereas RQ 7 focuses on individual sentences. For example, if we found in RQ 7 that several individual sentences are relevant to a project's high-level purpose, but still found in RQ 6 that the list of sentences as a whole does not, then we can identify that the problem is false positives. However, if we find in RQ 7 that individual sentences are not relevant, or very rarely relevant, then we can assume that sentence selection using Overlap is unpromising for future study.\n\n\nData collection\n\nIn this section, we describe how we collected and prepared data for our follow-up study. In this study, we focus on extracting sentences from the documentation of three projects: jEdit, \u00a7 \u00a7 \u00a7 a text editing tool designed for programmers, Jajuk, \u00b6 \u00b6 \u00b6 a music and audio file organization tool and player, and jHotDraw, **** a 2D graphics framework. We included information describing the size of these projects in Table V. We selected these projects because we have previously used them in automatic source code summarization research [21], and we have found these projects to be particularly well documented. Additionally, these tools have an easily understood purpose. Nearly all computer programmers will have used some kind of similar program in the past. We extracted sentences from these projects in the same fashion that we extracted sentences for the first study, as described in Section 4.1.\n\n\nConfiguring Latent Dirichlet Allocation Genetic Algorithm\n\nIn this section, we discuss how we configured LDA-GA to select topics from the source code and documentation. Dr. Panichella both furnished an implementation of LDA-GA and provided assistance in configuring it for our study. For each of our three projects, we gave LDA-GA the source code for each project and the extracted sentences. LDA-GA then calculated near-optimal parameters for us to use in LDA. Using these parameters as specified by LDA-GA, we generate a topic model for the project with LDA. For reproducibility, we have included the input parameters for LDA-GA in Figure 5. These parameters were decided with assistance from Dr. Panichella, the author of the system. For the purpose of feature discovery, we limited ourselves to 20 topics maximum, as we believed if we had more topics than this, our feature list would be too long and would result in both very low-level topics and a fatigue effect in study participants. Table VI lists the input configurations suggested by 3. This lists the input parameters for LDA-GA used in our study.   \n\n\nSelecting sentences\n\nIn this section, we describe our procedure for selecting sentences to form our feature list from the documentation for our follow-up study. We used Overlap (Section 4.2.2) to find the best sentences for the top 10 keywords. We chose to use the top 10 keywords based on internal experimentation. Often if we used fewer keywords, sentences were often too vague, or only contained one keyword from the topic. More keywords resulted in noisy and repetitious keywords in topics. We also looked at threshold approaches, where we considered all keywords in a topic that scored above some n-value. However, often this resulted in some topics with only one or sometimes zero topics, while other topics had 15 or more keywords larger than the threshold. Because of the output of LDA-GA, all projects generated 20 sentences for their feature list. After generating our list of features, we removed any duplicate sentences from the resulting list. This affected only jHotDraw, which was reduced to 16 unique sentences. Jajuk and Jedit had 20 unique sentences. We chose to remove duplicate sentences without replacing them, as we wanted each topic to be affiliated only with its most similar sentence.\n\n\nUser study\n\nIn order to evaluate the quality of our feature list sentences, we conducted a brief follow-up user study. Many of the participants from our first study, as well as some new participants, we asked to score our feature lists. The study was broken into three sections.\n\nIn the first section, which we use to answer RQ 6 , programmers were presented with the sentences selected from jEdit. Participants were asked the questions in Table VII. In our follow-up, we specifically want to know if the given list of documentation items gives the programmers an idea of what the system's overall purpose is. We crafted these questions to determine this.\n\nIn the second and third section, which we use to answer RQ 7 , we asked participants to evaluate features lists of Jajuk and jHotDraw, respectively. However, rather than rate the feature lists as a whole, we asked readers to rate each individual sentence in terms of relevance to the intended task. Programmers could rate a sentence as 'Very Relevant', 'Somewhat Relevant', 'Somewhat Irrelevant' and 'Very Irrelevant'. The programmers were only asked to rate each sentence based on relevance, as completeness for one sentence would be meaningless to ask and conciseness would be hard to judge in some cases. We asked participants to rate each sentence as we had concerns in our initial study that fatigue would cause participants to only read the first few sentences rather than the entire feature list. By examining each feature individually, we ensure the participants have to consider each sentence rather than giving in to fatigue and only skimming a larger list. This was important, as we also asked participants Q 4 and Q 5 from Table VII. If we did not ensure all participants read each sentence in the summary, the results of Q 4 would be unreliable.\n\n\nParticipants\n\nOur study had a total of 12 participants. All 12 participants were graduate students in the Computer Science and Engineering Department at the University of Notre Dame. The participants were recruited via in-department e-mail communications. Students were offered $10 to participate in the study. Five of the twelve participants also participated in our initial study. The remaining seven did not take the initial study. Participants were informed that an understanding of basic programming Table VII. Questions asked about the feature list in the first section of the follow-up study.  and Java conventions were required. One participant only completed the first section of the study. The remaining participants completed the entire study.\n\n\nReproducibility\n\nFor reproducibility, we have included anonymized study results as well as the feature lists generated for each project in an online appendix. \u2020 \u2020 \u2020 \u2020\n\n\nThreats to validity\n\nAs with any software engineering study, the participants in our study are a potential threat to validity. Given that all the participants in this study were graduate students with a computer programming background, we believe they were qualified and that their responses deserve merit. We believe that this is especially true given that there was no programming task required and that the task was overall relatively simple. Still, we cannot guarantee that a different group of professional programmers would reach the same conclusions.\n\nThe selection of projects we study is another source of threat to validity. The projects we selected in this follow-up study were selected because they were well documented. We selected high-quality documented projects because we wanted to mitigate the threat that our results were being 'brought down' by projects that were poorly documented. These projects have previously been used in source code summarization studies [21]. However, this also means we acknowledge that our approach often fails when dealing with poorly documented code. Our approach cannot work on completely undocumented code.\n\nAnother threat to validity comes from the fixed ordering of projects in our approach. We chose to fix the order of our approach in order to maximize feedback on specific projects. Our initial study was broad to the point that patterns found in individual projects with regards to the efficacy of techniques were lost in the larger study. To counter this, our follow-up study focuses on depth by collecting more data on few projects. The fixed order ensured that the same questions were answered about each project, giving us the most possible information to work with in regards to depth. That said, the fixed order could introduce biases that limit the generalizability of our results.\n\nAdditionally, there is a significant threat to validity of bias related to Q 4 . Specifically, programmers are asked to view the website before answering Q 4 , which can result is significant bias in their answers. We have the programmer view the website, specifically in section two and three, to rate each extracted sentence. Because there were 20 and 16 sentences to rate in Jajuk and jHotDraw, respectively, we were concerned that without requiring participants to answer a question on each sentence, participants would only read the first few sentences before assuming purpose. We are concerned that this was the case in our initial study. This would be problematic, as the order of the sentences is an arbitrary result of the generated topics.\n\n\nFOLLOW-UP STUDY RESULTS\n\nIn this section, we present the results of our follow-up user study. The implications of these results will be discussed in Section 10.\n\n\nSection 1 results\n\nSection 1 of our survey is used to address RQ 6 . The results of our survey are shown in Table VIII. In Section 1 of our survey, participants were given a feature list for jEdit constructed using topics generated by LDA-GA. These topics were used to select sentences using our Overlap sentence selection approach. The majority of participants, 8 of 12, 'Somewhat Agreed' that they understood how the list of sentences from documentation fit into the program's stated purpose; however, none 'Strongly Agreed'. One participant 'Strongly Disagreed'. This participant in Q 4 stated that the documentation list was difficult to understand, and if they had to guess, they 'would assume that this program creates a dockable window user interface that contains a toolbar'. Seven of the 12 participants 'Somewhat Agreed' that they could predict the program's intended purpose from the documentation.\n\nHowever, on Q 3 , 8 of the 12 participants either 'Somewhat Agreed' or 'Strongly Agreed' that the list of documentation contained unnecessary items that distract from the program's intended purpose. Because we had 20 topics generated for jEdit, this could be the result of selecting sentences for topics that do not communicate high-enough level ideas. These sentences could distract or confuse from the project's intended purpose. In the case of jEdit, several of the output sentences referred to things attributed to GUI elements, which many participants noted in Q 4 is not something they would intuitively expect in a text editor. Overall, we answer RQ 6 by noting that we did improve over our wider survey. However, conciseness in particular is still a significant concern.\n\n\nSection 2 and 3 results\n\nIn the remaining two sections of our study, participants were asked to rate the relevance of sentences selected from documentation. The results for Sections 2 and 3 are shown in Table IX. For Jajuk, the results indicate that many of the sentences participants believed the sentence to be 'Very Irrelevant'. Across all sentences, this was the most common answer. Of the 20 sentences, 12 received more 'Very Irrelevant' or 'Somewhat irrelevant' scores from participants than 'Somewhat Relevant' or 'Very Relevant'. Of the remaining 8 sentences, which received more 'Relevant' ratings, only 2 received more 'Very Relevant' scores than 'Somewhat Relevant'. However, despite the lower performance, when asked what the participants would assume the purpose of the program was if they only had to go off the features list, 7 of the 11 participants mentioned said they would consider the program a music or media player of some form. These participants specifically mentioned that sentences containing words such as 'album', 'artist', and 'track' indicate this functionality. Additionally, one of the sentences, extracted from a test program within the software, makes reference to the band 'Red Hot Chili Peppers'. Of the remaining 4 participants, 2 said they would think the list described some user interface tool. Another participant said they believed this was a data wizard used to construct a screen. The remaining participant said they would not be able to figure out any intent based on the list. Considered relevant to a music player, these sentences sent a clear message, while the 'Irrelevant' messages were less focused.\n\nThe quantitative results for jHotDraw were better. It should be noted for jHotDraw there were four duplicate sentences that were removed, and as such, each participant only rated 16 sentences. In this case, the majority of ratings by participants for jHotDraw were either 'Somewhat' or 'Very Relevant'. Ten of the 16 sentences received more 'Relevant' ratings than 'Irrelevant'. Of the 10 sentences rated 'Somewhat' or 'Very Relevant', 3 were rated more frequently as 'Very Relevant'. Of the 11 participants, 10 said that if given the feature list, they would believe this project would be a drawing tool of some kind, albeit one of the participants said 'but still with very low confidence, because almost all GUI applications have something for drawing and some visual views'. It is worth In each Section, participants were asked to rate the relevance of each sentence generated by Latent Dirichlet Allocation Genetic Algorithm topics using Overlap sentence selection. mentioning that the title jHotDraw includes the word draw in it, which could influence how our participants answered Q 4 . The remaining participant said the program would be a GUI editing or designing tool. In these sections, we answer RQ 6 with cautious optimism that our approach can provide some high-level understanding. However, in RQ 7 , we find that there are still a large number of irrelevant sentences being included in our summaries.\n\n\nFOLLOW-UP STUDY DISCUSSION\n\nIn this section, we discuss the findings of our follow-up user study. Overall, we believe the results of this follow-up study show that there is some promise to automatic feature discovery via sentence selection. While our initial study had, overall, poor results, our follow-up study shows that sentence selection can work for the purpose of generally informing a programmer what the purpose of a given project is on well-documented projects. A common point of confusion among study participants was that the features often referred to GUI elements. Many participants said these documentation items often led them to believe a program allowed you design or edit GUIs. In future work, it may be worth exploring reducing the number of topics that refer to graphic and interface elements.\n\nIt should be noted that for every project, LDA-GA suggested 20 topics. This was our maximum allowable number of topics based on our LDA-GA parameters. We set 20 as the maximum out of concern that the summaries would become too large for an effective human study, as well as the topics would become too specific and low level to clearly indicate a projects overall purpose. When we removed the upper bound of 20 topics, all three projects were generating in excess of 60 topics. As we discussed in the prior paragraph, it's likely that GUI topics could be better combined to reduce their confusion impact on the summary output without affecting the overall message. This would reduce the number of topics more naturally then setting a fixed upper bound number of topics.\n\nWe do not believe any of the automatic approaches presented in this paper would outperform human experts at this time. A concern is that while feature discovery may perform better on well-documented projects, it is reasonable to believe that if a project is well documented, it likely has a website that lists the features written by a human expert. However, from our own experience, we have found projects with expired websites and no clear means to contact the developers. If the project were additionally poorly documented, our approach as is would not perform adequately. We believe that future work could focus on combining automatic natural language documentation and summarization approaches, such as Sridhara et al. [23] or our own previous work, McBurney et al. [40] with automatic feature discovery, which could allow for poorly documented projects or projects lacking any documentation to generate a feature list.\n\nWhile this work lays a foundation for project summarization using feature discovery by source code analysis, future work could extract sentences from other sources beyond source code and embedded JavaDocs. These other sources of documentation could include change logs, developer communications, bug reports, and so on. These sources have been used to summarize individual sections of source code [46,47], and we believe sentences extracted from these sources could be used to describe features of an entire project. While it is beyond the scope of this foundational work, future work will want to mine these sources when available.\n\n\nRELATED WORK\n\nThis section will cover related work in automatic source code summarization. This paper is related to source code summarization in that we are attempting to provide overall understanding of a software project. Source code summarization is a growing field in the literature. Several of these approaches rely on information retrieval techniques. Haiduc et al. presented an approach to summarizing source code methods using a vector space model [48]. This approach summarized individual source code methods by providing keywords that describe the method, similar to how LDA produces topics in the form of keywords in feature discovery [13,18]. This work was later modified by Rodeghero et al. where keyword selection was weighted base on source code context [49]. In this approach, different source code contexts were considered more important due to the results of an eye-tracking study. This paper demonstrated that programmers preferred when keywords were selected when considerations for source code context were made. These approaches focus on individual method summarization. Our goal is to summarize projects at a higher level.\n\nMore recently, two source code summarization approaches use natural language summarization. Sridhara et al. create an approach that summarize Java methods by identifying important statements in a given method using information retrieval techniques [22]. These statements are then translated into natural language using templates to create natural language summaries of the method [23]. Our later work expanded on the idea of natural language summaries by creating natural language summaries of Java methods using a method's contextual information [21]. Contextual information was derived using a call graph where methods with higher PageRank scores were considered more important to describing other methods. This work differs from this current work in that both of these approaches describe only individual methods. These approaches currently do not allow for project-wide summarization. Additionally, Panichella et al. used developer communications, including bug reports and mailings lists, for Lucene and Eclipse to automatically generate descriptions of methods [46]. Wong et al. mine comments from StackOverflow and other programming Q&A sites to generate source code summaries [47]. Related to this, Zhang et al. mine discussions on API libraries to identify problematic features to bring them to the attention of the developers [50]. In the future, developer discussions may be used to supplement documentation in our open feature extraction approaches. This would be particularly useful when documentation is lacking or limited in the source code itself.\n\nAnother approach to source code summarization is topic modeling. As discussed in Section 3.1, LDA is a commonly used tool in software topic modeling [13,18]. De Lucia et al. demonstrated that techniques such as LDA produce artifact labeling similar to human programmer labeling [51]. Panichella et al. presented LDA-GA, a genetic algorithm searching tool to derive a near-optimal LDA configuration for modeling a given software project [15]. Baldi et al. adapted latent topics for aspect-oriented programming [52]. In our own recent work, we represented Java projects as a treelike hierarchy of topic, where more general concepts would be higher in the tree, and less general topics would be lower in the tree [40]. By presenting methods in a hierarchy with associated topics, programmers felt they understood better how the given method fit into the overall project. This work, alongside the growing source code summarization field, lead us to believe that project-wide natural language summarization is possible with further work in the field.\n\n\nCONCLUSION\n\nIn this paper, we explored four automatic feature list generation tools for Java projects. Each of these tools selected sentences from existing documentation to form a feature list. Our evaluation found that there is still much work to be done in this area, as these preliminary techniques did not perform well in our evaluation. While LDA can find topics that help programmers understand projects better [13,18], it is not always possible to describe these topics as natural language sentences extracted from JavaDocs. Existing work has shown that it is possible to produce quality automatic summarizations of Java methods. The problem of reliably automatically summarizing projects with natural language remains unsolved, especially for projects that are poorly documented. However, our evaluation lays the groundwork for future work in the area of automatic source code summarization and presents four baseline techniques to compare with. Additionally, we show that there is some promise that automatically generated natural language feature lists can give programmers a general idea of what the purpose of a given software project is.\n\nFigure 5 .\n5This lists the input parameters for Latent Dirichlet Allocation Genetic Algorithm (LDA-GA) used in our study.\n\nQ 1 I\n1understand how the list of documentation items fits into the program's stated purpose on its website. Q 2 I believe that if given this list of documentation items, I could predict the program's intended purpose. Q 3 The list of documentation contains unnecessary items that distract from the program's intended purpose. Q 4 If there were no website, and you only had the list of documentation items, what would you assume the purpose of the program was? Please be detailed. Q 5 (Optional) Please note any additional comments here.\n\nQ 1 ,\n1Q 2 , and Q 3 were multiple choice. Participants could respond with 'Strongly Disagree', 'Somewhat Disagree', 'Somewhat Agree', or 'Strongly Agree'. Q 4 was a required open ended question. Q 5 allowed participants to provide any additional comments and was optional.\n\nTable II .\nIIMann-Whitney tests for statistical significance.U, U expt , and U vari are Mann-Whitney derived values use to calculate the decision value p. If p < .05, the two populations are statistically distinct, and the rank of the population with the lower mean is incremented. LSS, Lightweight Semantic Similarity; TLDR, Too Long Didn't Read.RQ \nH \nApproach \nn mean Std. dev. \nU \nU expt U vari \np \nDecision Rank \n\nRQ 1 H 1 LSS \n16 1.94 \n0.929 108.5 112 \n506 0.894 Not reject \n1 \nOverlap \n14 1.93 \n0.73 \n1 \nH 2 LSS + Overlap \n30 1.93 \n0.828 189 \n195 \n1256 0.877 Not reject N/A \nTLDR \n13 1.92 \n0.954 \n1 \nH 3 LSS + Overlap + TLDR \n43 1.93 \n0.856 271 \n344 \n3016 0.187 Not reject N/A \nTextRank \n16 1.62 \n0.885 \n1 \nRQ 2 H 4 TLDR \n13 1.92 \n0.954 \n96 \n97.5 383 0.959 Not reject \n1 \nOverlap \n15 1.80 \n0.561 \n1 \nH 5 Overlap + TLDR \n28 1.86 \n0.756 178 \n224 \n1407 0.225 Not reject N/A \nLSS \n16 1.69 \n1.01 \n1 \nH 6 Overlap + TLDR + LSS \n44 1.79 \n0.851 295.5 352 \n3011 0.307 Not reject N/A \nTextRank \n16 1.62 \n0.957 \n1 \nRQ 3 H 7 Overlap \n15 2.00 \n0.756 105.5 120 \n567 0.556 Not reject \n1 \nTextRank \n16 1.87 \n1.09 \n1] \nH 8 Overlap + TextRank \n31 1.93 \n0.929 185 \n202 \n1338 0.662 Not reject N/A \nTLDR \n13 1.85 \n1.07 \n1 \nH 9 Overlap + TextRank + TLDR 44 1.91 \n0.96 \n318 \n352 \n3133 0.55 Not reject N/A \nLSS \n16 1.69 \n0.704 \n1 \nRQ 4 H 1 0 Overlap \n15 1.47 \n0.64 \n81 \n97.5 419 0.434 Not reject \n1 \nTLDR \n13 2.23 \n1.09 \n1 \nH 1 1 Overlap + TLDR \n28 2.36 \n0.87 \n171 \n224 \n1510 0.177 Not reject N/A \nLSS \n16 2.00 \n0.894 \n1 \nH 1 2 Overlap + TLDR + LSS \n44 2.23 \n0.886 253 \n352 \n3263 0.086 Not reject N/A \nTextRank \n16 1.81 \n1.11 \n1 \n\n\n\nTable I .\nIAverage scores by approach.Approach \nAccuracy \nCompleteness \nConciseness \nReadability \n\nOverlap \n1.93 \n1.80 \n2.00 \n2.47 \nLSS \n1.94 \n1.69 \n1.69 \n2.00 \nTextRank \n1.62 \n1.62 \n1.87 \n1.81 \nTLDR \n1.92 \n1.92 \n1.85 \n2.23 \nOverall \n1.85 \n1.75 \n1.85 \n2.12 \n\n\nTable IV .\nIVNormalized win, loss, and tie percentages for each approach.Approach \n\nPercentages \n\nWin \nLoss \nTie \n\nOverlap \n47.3% \n10.9% \n41.8% \nLSS \n46.7% \n31.7% \n21.7% \nTextRank \n24.1% \n27.8% \n48.1% \nTLDR \n16.9% \n43.1% \n40.0% \n\nLSS, Lightweight Semantic Similarity; TLDR, Too Long Didn't Read. \n\n\n\nTable III .\nIIIThis table shows the breakdown of wins and losses when two items are compared.LSS, Lightweight Semantic Similarity; TLDR, Too Long Didn't Read.Breakdown of wins and losses \n\nWinner \n\nLoser \n\nOverlap \nLSS \nTextRank \nTLDR \n\nOverlap \n-\n7 \n4 \n1 5 \nLSS \n1 \n-\n7 \n1 0 \nTextRank \n2 \n8 \n-\n3 \nTLDR \n3 \n4 \n4 \n-\n\n\n\nTable V .\nVThe three java projects used in our evaluation. KLOC reported with all comments removed. All projects are open-source.Methods \nKLOC \nJava files \n\nJajuk \n5921 \n70 \n544 \njEdit \n7161 \n117 \n555 \njHotDraw \n5263 \n31 \n466 \n\nKLOC, thousands of lines of code. \n\n\n\nTable VI .\nVIThis table lists the near-optimal input parameters suggested by Latent Dirichlet Allocation Genetic \nAlgorithm. \n\nProject \n\u03b1 \n\u03b2 \n#Topics \n#Iterations \n\njEdit \n0.5906 \n0.5108 \n20 \n48 \nJajuk \n0.3315 \n0.7145 \n20 \n55 \njHotDraw \n0.8102 \n0.7785 \n20 \n53 \n\n\nTable IX .\nIXAggregate responses to multiple choice questions for Sections 2 and 3.Project \nVery Irrelevant \nSomewhat Irrelevant \nSomewhat Relevant \nVery Relevant \n\nJajuk \n75 \n44 \n67 \n34 \njHotDraw \n29 \n46 \n57 \n43 \n\n\n\nTable VIII .\nVIIISection 1 responses to multiple choice questions.Question \nStrongly Disagree \nSomewhat Disagree \nSomewhat Agree \nStrongly Agree \n\nQ 1 \n1 \n3 \n8 \n0 \nQ 2 \n1 \n4 \n7 \n0 \nQ 3 \n1 \n3 \n3 \n5 \n\nACKNOWLEDGEMENTSThe authors would like to thank the participants in our evaluation for their careful efforts. Additionally, the authors would like to thank Dr. Annibale Panichella for both furnishing an implementation of LDA-GA and providing assistance in configuring it's parameters. This work is supported in part by the NSF CCF-1452959 and CNS-1510329 grants. Any opinions, findings, and conclusions expressed herein are the authors' and do not necessarily reflect those of the sponsors.\nOn-demand feature recommendations derived from mining public product descriptions. H Dumitru, M Gibiec, N Hariri, Cleland - Huang, J Mobasher, B Castro-Herrera, C Mirakhorli, M , Software Engineering (ICSE), 2011 33rd International Conference on. Dumitru H, Gibiec M, Hariri N, Cleland-Huang J, Mobasher B, Castro-Herrera C, Mirakhorli M. On-demand feature recommendations derived from mining public product descriptions. In Software Engineering (ICSE), 2011 33rd International Conference on, 2011; 181-190.\n\nRecommending source code for use in rapid software prototypes. C Mcmillan, N Hariri, D Poshyvanyk, Cleland - Huang, J Mobasher, B , 34th International Conference on. Software Engineering (ICSE)McMillan C, Hariri N, Poshyvanyk D, Cleland-Huang J, Mobasher B. Recommending source code for use in rapid software prototypes. In Software Engineering (ICSE), 2012 34th International Conference on, 2012; 848-858.\n\nWhat's in a feature: a requirements engineering perspective. Fundamental Approaches to Software Engineering. A Classen, P Heymans, P Y Schobbens, SpringerBerlin, HeidelbergClassen A, Heymans P, Schobbens PY. What's in a feature: a requirements engineering perspective. Fundamental Approaches to Software Engineering. Springer: Berlin, Heidelberg, 2008; 16-30.\n\nFeature-oriented domain analysis (foda) feasibility study, DTIC Document. K C Kang, Cohensg, Hessja, W E Novak, A S Peterson, Tech. Rep. Kang KC, CohenSG, HessJA, Novak WE, Peterson AS. Feature-oriented domain analysis (foda) feasibility study, DTIC Document, Tech. Rep., 1990.\n\nTechniques for requirements elicitation. J A Goguen, C Linde, Requirements Engineering. 93Goguen JA, Linde C. Techniques for requirements elicitation. Requirements Engineering 1993; 93:152-164.\n\nRecovery of traceability links between software documentation and source code. A Marcus, J I Maletic, A Sergeyev, International Journal of Software Engineering and Knowledge Engineering. 1505Marcus A, Maletic JI, Sergeyev A. Recovery of traceability links between software documentation and source code. International Journal of Software Engineering and Knowledge Engineering 2005; 15(05):811-836.\n\nTest-Driven Development: By Example. K Beck, Addison-Wesley ProfessionalBoston, MassachusettsBeck K. Test-Driven Development: By Example. Addison-Wesley Professional: Boston, Massachusetts, 2003.\n\nA machine learning approach for tracing regulatory codes to product specific requirements. Cleland-Huang J Czauderna, A Gibiec, M Emenecker, J , DOI: 10.1145/ 1806799.1806825Proceedings of the 32Nd ACM/IEEE International Conference on Software Engineering. the 32Nd ACM/IEEE International Conference on Software EngineeringNew York, NY, USAACM1ser. ICSE '10Cleland-Huang J, Czauderna A, Gibiec M, Emenecker J. A machine learning approach for tracing regulatory codes to product specific requirements. In Proceedings of the 32Nd ACM/IEEE International Conference on Software Engi- neering -Volume 1, ser. ICSE '10. New York, NY, USA: ACM, 2010; 155-164. [Online]. Available: DOI: 10.1145/ 1806799.1806825.\n\nHow do professional developers comprehend software?. T Roehm, R Tiarks, R Koschke, W Maalej, Proceedings of the 34th International Conference on Software Engineering, ser. ICSE '12. the 34th International Conference on Software Engineering, ser. ICSE '12Piscataway, NJ, USAIEEE Press44Roehm T, Tiarks R, Koschke R, Maalej W. How do professional developers comprehend software? In Proceedings of the 34th International Conference on Software Engineering, ser. ICSE '12. Piscataway, NJ, USA: IEEE Press, 2012; 255-265. [Online]. Available: 44. http://dl.acm.org/citation.cfm?id=2337223.2337254 [September 30, 2014].\n\nA study of the documentation essential to software maintenance. Scb De Souza, N Anquetil, K M De Oliveira, 10.1145/1085313.1085331Proceedings of the 23rd annual international conference on Design of communication: documenting & designing for pervasive information, ser. SIGDOC '05. the 23rd annual international conference on Design of communication: documenting & designing for pervasive information, ser. SIGDOC '05New York, NY, USAACMde Souza SCB, Anquetil N, de Oliveira KM. A study of the documentation essential to software maintenance. In Pro- ceedings of the 23rd annual international conference on Design of communication: documenting & designing for pervasive information, ser. SIGDOC '05. New York, NY, USA: ACM, 2005; 68-75. [Online]. Available: DOI: 10.1145/1085313.1085331.\n\nA survey of documentation practice within corrective maintenance. M Kajko-Mattsson, 10.1023/B:LIDA.0000048322.42751.caEmpirical Softw. Engg. 101Kajko-Mattsson M. A survey of documentation practice within corrective maintenance. Empirical Softw. Engg 2005; 10(1):31-55. . [Online]. Available: DOI: 10.1023/B:LIDA.0000048322.42751.ca.\n\nExtracting software product lines: a case study using conditional compilation. M V Couto, M T Valente, E Figueiredo, Software Maintenance and Reengineering (CSMR), 2011 15th European Conference on. Couto MV, Valente MT, Figueiredo E. Extracting software product lines: a case study using conditional compilation. in Software Maintenance and Reengineering (CSMR), 2011 15th European Conference on. IEEE, 2011; 191-200.\n\nMining concepts from code with probabilistic topic models. E Linstead, P Rigor, S Bajracharya, C Lopes, P Baldi, 10.1145/1321631.1321709Proceedings of the Twenty-second IEEE/ACM International Conference on Automated Software Engineering, ser. ASE '07. the Twenty-second IEEE/ACM International Conference on Automated Software Engineering, ser. ASE '07New York, NY, USAACMLinstead E, Rigor P, Bajracharya S, Lopes C, Baldi P. Mining concepts from code with probabilistic topic models. In Proceedings of the Twenty-second IEEE/ACM International Conference on Automated Software Engineering, ser. ASE '07. New York, NY, USA: ACM, 2007; 461-464. [Online]. Available: DOI: 10.1145/1321631.1321709.\n\nMining business topics in source code using latent dirichlet allocation. G Maskeri, S Sarkar, K Heafield, 10.1145/1342211.1342234iIn Proceedings of the 1st India Software Engineering Conference, ser. ISEC '08. New York, NY, USAACM113Maskeri G, Sarkar S, Heafield K. Mining business topics in source code using latent dirichlet allocation. iIn Proceed- ings of the 1st India Software Engineering Conference, ser. ISEC '08. New York, NY, USA: ACM, 2008; 113-120. [Online]. Available: DOI: 10.1145/1342211.1342234.\n\nHow to effectively use topic models for software engineering tasks? An approach based on genetic algorithms. A Panichella, B Dit, R Oliveto, Di Penta, M Poshyvanyk, D , De Lucia, A , Proceedings of the 2013 International Conference on Software Engineering, ser. ICSE '13. the 2013 International Conference on Software Engineering, ser. ICSE '13Piscataway, NJ, USAIEEE PressPanichella A, Dit B, Oliveto R, Di Penta M, Poshyvanyk D, De Lucia A. How to effectively use topic models for software engineering tasks? An approach based on genetic algorithms. In Proceedings of the 2013 International Conference on Software Engineering, ser. ICSE '13. Piscataway, NJ, USA: IEEE Press, 2013; 522-531. [Online].\n\nLabeled topic detection of open source software from mining mass textual project profiles. T Wang, G Yin, X Li, H Wang, 10.1145/2384416.2384419Proceedings of the First International Workshop on Software Mining, ser. SoftwareMining '12. the First International Workshop on Software Mining, ser. SoftwareMining '12New York, NY, USAACMWang T, Yin G, Li X, Wang H. Labeled topic detection of open source software from mining mass textual project profiles. In Proceedings of the First International Workshop on Software Mining, ser. SoftwareMining '12. New York, NY, USA: ACM, 2012; 17-24. [Online]. Available: DOI: 10.1145/2384416.2384419.\n\nSoftware reconnaissance: mapping program features to code. N Wilde, M C Scully, Journal of Software Maintenance Research and Practice. 71Wilde N, Scully MC. Software reconnaissance: mapping program features to code. Journal of Software Maintenance Research and Practice 1995; 7(1):49-62.\n\nFeature location in source code: a taxonomy and survey. B Dit, M Revelle, M Gethers, D Poshyvanyk, Journal of Software: Evolution and Process. 251Dit B, Revelle M, Gethers M, Poshyvanyk D. Feature location in source code: a taxonomy and survey. Journal of Software: Evolution and Process 2013; 25(1):53-95.\n\nDo code and comments co-evolve? On the relation between source code and comment changes. B Fluri, M Wursch, H C Gall, 10.1109/WCRE.2007.21Proceedings of the 14th Working Conference on Reverse Engineering, ser. WCRE '07. the 14th Working Conference on Reverse Engineering, ser. WCRE '07Washington, DC, USAIEEE Computer SocietyFluri B, Wursch M, Gall HC. Do code and comments co-evolve? On the relation between source code and comment changes. In Proceedings of the 14th Working Conference on Reverse Engineering, ser. WCRE '07. Washington, DC, USA: IEEE Computer Society, 2007; 70-79. [Online]. Available: DOI: 10.1109/WCRE.2007.21\n\nAsking and answering questions during a programming change task. J Sillito, G C Murphy, De Volder, K , DOI: 10.1109/ TSE.2008.26IEEE Transactions on Software Engineering. 344Sillito J, Murphy GC, De Volder K. Asking and answering questions during a programming change task. IEEE Transactions on Software Engineering 2008; 34(4):434-451. [Online]. Available: DOI: 10.1109/ TSE.2008.26.\n\nAutomatic documentation generation via source code summarization of method context. P W Mcburney, C Mcmillan, 10.1145/2597008.2597149Proceedings of the 22Nd International Conference on Program Comprehension, ser. ICPC. the 22Nd International Conference on Program Comprehension, ser. ICPCNew York, NY, USAACMMcBurney PW, McMillan C. Automatic documentation generation via source code summarization of method con- text. In Proceedings of the 22Nd International Conference on Program Comprehension, ser. ICPC 2014. New York, NY, USA: ACM, 2014; 279-290. [Online]. Available: DOI: 10.1145/2597008.2597149.\n\nAutomatic generation of descriptive summary comments for methods in object-oriented programs. G Sridhara, 3499878Newark, DE, USAPh.D. dissertationSridhara G. Automatic generation of descriptive summary comments for methods in object-oriented programs, Ph.D. dissertation, Newark, DE, USA, 2012, aAI3499878.\n\nTowards automatically generating summary comments for java methods. G Sridhara, E Hill, D Muppaneni, L Pollock, K Vijay-Shanker, Proceedings of the IEEE/ACM International Conference on Automated Software Engineering, ser. ASE '10. the IEEE/ACM International Conference on Automated Software Engineering, ser. ASE '10New York, NY, USAACMAvailable: DOI: 10.1145/ 1858996.1859006Sridhara G, Hill E, Muppaneni D, Pollock L, Vijay-Shanker K. Towards automatically generating summary com- ments for java methods. In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering, ser. ASE '10. New York, NY, USA: ACM, 2010; 43-52. [Online]. Available: DOI: 10.1145/ 1858996.1859006.\n\nAutomatic generation of natural language summaries for java classes. L Moreno, J Aponte, G Sridhara, A Marcus, L Pollock, K Vijay-Shanker, IEEE 21st International Conference on. Program Comprehension (ICPC)Moreno L, Aponte J, Sridhara G, Marcus A, Pollock L, Vijay-Shanker K. Automatic generation of natural language summaries for java classes.In Program Comprehension (ICPC), 2013 IEEE 21st International Conference on, 2013; 23-32.\n\nJsummarizer: an automatic generator of natural language summaries for java classes. L Moreno, A Marcus, L Pollock, K Vijay-Shanker, IEEE 21st International Conference on. Program Comprehension (ICPC)Moreno L, Marcus A, Pollock L, Vijay-Shanker K. Jsummarizer: an automatic generator of natural language summa- ries for java classes. In Program Comprehension (ICPC), 2013 IEEE 21st International Conference on, 2013; 230-232.\n\nAutomatically detecting and describing high level actions within methods. G Sridhara, L Pollock, K Vijay-Shanker, Software Engineering (ICSE), 2011 33rd International Conference on. Sridhara G, Pollock L, Vijay-Shanker K. Automatically detecting and describing high level actions within methods. In Software Engineering (ICSE), 2011 33rd International Conference on, 2011; 101-110.\n\nLatent dirichlet allocation. D M Blei, A Y Ng, M I Jordan, Journal of Machine Learning Research. 3Blei DM, Ng AY, Jordan MI. Latent dirichlet allocation. Journal of Machine Learning Research 2003; 3:993-1022. [Online]. Available: http://dl.acm.org/citation.cfm?id=944919.944937 [September 30, 2014].\n\nSoftware traceability with topic modeling. Asuncion Hu, A U Asuncion, R N Taylor, 10.1145/1806799.1806817Proceedings of the 32Nd ACM/IEEE International Conference on Software Engineering. the 32Nd ACM/IEEE International Conference on Software EngineeringNew York, NY, USAACM1ser. ICSE '10Asuncion HU, Asuncion AU, Taylor RN. Software traceability with topic modeling, in Proceedings of the 32Nd ACM/IEEE International Conference on Software Engineering -Volume 1, ser. ICSE '10. New York, NY, USA: ACM, 2010; 95-104. [Online]. Available: DOI: 10.1145/1806799.1806817.\n\nRecovering documentation-to-source-code traceability links using latent semantic indexing. A Marcus, J I Maletic, Proceedings of the 25th International Conference on Software Engineering, ser. ICSE '03. the 25th International Conference on Software Engineering, ser. ICSE '03Washington, DC, USAIEEE Computer SocietyMarcus A, Maletic JI. Recovering documentation-to-source-code traceability links using latent semantic indexing. In Proceedings of the 25th International Conference on Software Engineering, ser. ICSE '03. Washington, DC, USA: IEEE Computer Society, 2003; 125-135. [Online]. Available: http://dl.acm.org/citation.cfm?id=776816.776832 [September 30, 2014].\n\nOn the equivalence of information retrieval methods for automated traceability link recovery. R Oliveto, M Gethers, D Poshyvanyk, De Lucia, A , IEEE 18th International Conference on. Program Comprehension (ICPC)Oliveto R, Gethers M, Poshyvanyk D, De Lucia A. On the equivalence of information retrieval methods for auto- mated traceability link recovery. In Program Comprehension (ICPC), 2010 IEEE 18th International Conference on, 2010; 68-71.\n\nUnderstanding lda in source code analysis. D Binkley, D Heinz, D Lawrie, J Overfelt, 10.1145/2597008.2597150Proceedings of the 22Nd International Conference on Program Comprehension, ser. ICPC. the 22Nd International Conference on Program Comprehension, ser. ICPCNew York, NY, USAACMBinkley D, Heinz D, Lawrie D,Overfelt J. Understanding lda in source code analysis, in Proceedings of the 22Nd International Conference on Program Comprehension, ser. ICPC 2014. New York, NY, USA: ACM, 2014; 26-36. [Online]. Available: DOI: 10.1145/2597008.2597150.\n\nUsing heuristics to estimate an appropriate number of latent topics in source code analysis. S Grant, J R Cordy, D B Skillicorn, Science of Computer Programming. 789Grant S, Cordy JR, Skillicorn DB. Using heuristics to estimate an appropriate number of latent topics in source code analysis. Science of Computer Programming 2013; 78(9):1663-1678. [Online]. Available: http://www.sciencedirect. com/science/article/pii/S0167642313000762 [September 30, 2014].\n\nSource code retrieval for bug localization using latent dirichlet allocation. S Lukins, N Kraft, L Etzkorn, Reverse Engineering, 2008. WCRE '08. 15th Working Conference on. Lukins S, Kraft N, Etzkorn L. Source code retrieval for bug localization using latent dirichlet allocation. In Reverse Engineering, 2008. WCRE '08. 15th Working Conference on, 2008; 155-164.\n\nMining software repositories using topic models. S Thomas, Software Engineering (ICSE), 2011 33rd International Conference on. Thomas S. Mining software repositories using topic models. In Software Engineering (ICSE), 2011 33rd Interna- tional Conference on, 2011; 1138-1139.\n\nUsing latent dirichlet allocation for automatic categorization of software. K Tian, M Revelle, D Poshyvanyk, Mining Software Repositories, 2009. MSR '09. 6th IEEE International Working Conference on. Tian K, Revelle M, Poshyvanyk D. Using latent dirichlet allocation for automatic categorization of software. In Min- ing Software Repositories, 2009. MSR '09. 6th IEEE International Working Conference on, 2009; 163-166.\n\nA fast and efficient semantic short text similarity metric. D Croft, S Coupland, J Shell, S Brown, Computational Intelligence (UKCI), 2013 13th UK Workshop on. Croft D, Coupland S, Shell J, Brown S. A fast and efficient semantic short text similarity metric. In Computational Intelligence (UKCI), 2013 13th UK Workshop on, 2013, 221-227.\n\nWordnet: a lexical database for english. G A Miller, COMMUNICATIONS OF THE ACM. 38Miller GA. Wordnet: a lexical database for english. COMMUNICATIONS OF THE ACM 1995; 38:39-41.\n\nTextrank: bringing order into text. R Mihalcea, P Tarau, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. the 2004 Conference on Empirical Methods in Natural Language ProcessingBarcelona, SpainA meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACLMihalcea R, Tarau P. Textrank: bringing order into text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP 2004, A meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL 2004, 25-26 July 2004, Barcelona, Spain, 2004; 404-411. [Online]. Available: http://www.aclweb.org/anthology/W04-3252 [September 30, 2014].\n\nBinary codes capable of correcting deletions, insertions and reversals. V Levenshtein, Soviet Physics Doklady. 10707Levenshtein V. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady 1966; 10:707.\n\nImproving topic model source code summarization. P W Mcburney, C Liu, C Mcmillan, T Weninger, 10.1145/2597008.2597793Proceedings of the 22Nd International Conference on Program Comprehension, ser. ICPC. the 22Nd International Conference on Program Comprehension, ser. ICPCNew York, NY, USAACMMcBurney PW, Liu C, McMillan C, Weninger T, Improving topic model source code summarization. In Proceedings of the 22Nd International Conference on Program Comprehension, ser. ICPC 2014. New York, NY, USA: ACM, 2014; 291-294. [Online]. Available: DOI: 10.1145/2597008.2597793.\n\nPlda+: Parallel latent dirichlet allocation with data placement and pipeline processing. Z Liu, Y Zhang, E Y Chang, M Sun, ACM Transactions on Intelligent Systems and Technology, special issue on Large Scale Machine Learning. Liu Z, Zhang Y, Chang EY, Sun M. Plda+: Parallel latent dirichlet allocation with data placement and pipeline pro- cessing. ACM Transactions on Intelligent Systems and Technology, special issue on Large Scale Machine Learning, 2011, software available at http://code.google.com/p/plda [September 30, 2014].\n\nOn a test of whether one of two random variables is stochastically larger than the other. H Mann, D Whitney, The Annals of Mathematical Statistics. 181Mann H, Whitney D. On a test of whether one of two random variables is stochastically larger than the other. The Annals of Mathematical Statistics 1947; 18(1):50-60.\n\nRecommending source code for use in rapid software prototypes. C Mcmillan, N Hariri, D Poshyvanyk, Cleland - Huang, J Mobasher, B , Proceedings of the 2012 International Conference on Software Engineering. the 2012 International Conference on Software EngineeringIEEE PressMcMillan C, Hariri N, Poshyvanyk D, Cleland-Huang J, Mobasher B. Recommending source code for use in rapid software prototypes. In Proceedings of the 2012 International Conference on Software Engineering. IEEE Press, 2012; 848-858.\n\nAn empirical study of the textual similarity between source code and source code summaries. P Mcburney, C Mcmillan, 10.1007/s10664-014-9344-6Empirical Software Engineering. 211McBurney P, McMillan C. An empirical study of the textual similarity between source code and source code sum- maries. Empirical Software Engineering 2014; 21(1):1-26. [Online]. Available: DOI: 10.1007/s10664-014-9344-6.\n\nQuality analysis of source code comments. D Steidl, B Hummel, E Juergens, IEEE 21st International Conference on. Program Comprehension (ICPC)Steidl D, Hummel B, Juergens E. Quality analysis of source code comments. In Program Comprehension (ICPC), 2013 IEEE 21st International Conference on, 2013; 83-92.\n\nMining source code descriptions from developer communications. S Panichella, J Aponte, Di Penta, M Marcus, A Canfora, G , IEEE 20th International Conference on. Program Comprehension (ICPC)Panichella S, Aponte J, Di Penta M, Marcus A, Canfora G. Mining source code descriptions from developer commu- nications. In Program Comprehension (ICPC), 2012 IEEE 20th International Conference on, 2012; 63-72.\n\nAutocomment: mining question and answer sites for automatic comment generation. E Wong, J Yang, L Tan, Automated Software Engineering (ASE). Wong E, Yang J, Tan L. Autocomment: mining question and answer sites for automatic comment generation. In Automated Software Engineering (ASE), 2013 IEEE/ACM 28th International Conference on, 2013; 562-567.\n\nOn the use of automated text summarization techniques for summarizing source code. S Haiduc, J Aponte, L Moreno, A Marcus, Reverse Engineering (WCRE), 2010 17th Working Conference on. Haiduc S, Aponte J, Moreno L, Marcus A. On the use of automated text summarization techniques for summarizing source code. In Reverse Engineering (WCRE), 2010 17th Working Conference on, 2010; 35-44.\n\nImproving automated source code summarization via an eye-tracking study of programmers. P Rodeghero, C Mcmillan, P W Mcburney, N Bosch, D Mello, S , DOI: 10.1145/ 2568225.2568247Proceedings of the 36th International Conference on Software Engineering, ser. ICSE. the 36th International Conference on Software Engineering, ser. ICSENew York, NY, USAACMRodeghero P, McMillan C, McBurney PW, Bosch N, D'Mello S. Improving automated source code summarization via an eye-tracking study of programmers. In Proceedings of the 36th International Conference on Software Engi- neering, ser. ICSE 2014. New York, NY, USA: ACM, 2014; 390-401. [Online]. Available: DOI: 10.1145/ 2568225.2568247.\n\nExtracting problematic api features from forum discussions. Y Zhang, D Hou, IEEE 21st International Conference on. Program Comprehension (ICPC)Zhang Y, Hou D. Extracting problematic api features from forum discussions. In Program Comprehension (ICPC), 2013 IEEE 21st International Conference on, 2013; 142-151.\n\nUsing ir methods for labeling source code artifacts: is it worthwhile. A De Lucia, Di Penta, M Oliveto, R Panichella, A Panichella, S , IEEE 20th International Conference on. Program Comprehension (ICPC)De Lucia A, Di Penta M, Oliveto R, Panichella A, Panichella S. Using ir methods for labeling source code artifacts: is it worthwhile? In Program Comprehension (ICPC), 2012 IEEE 20th International Conference on, 2012, 193-202.\n\nA theory of aspects as latent topics. P F Baldi, C V Lopes, E J Linstead, S K Bajracharya, 10.1145/1449955.1449807SIGPLAN Not. 4310Baldi PF, Lopes CV, Linstead EJ, Bajracharya SK. A theory of aspects as latent topics. SIGPLAN Not. 2008; 43 (10):543-562. [Online]. Available: DOI: 10.1145/1449955.1449807.\n", "annotations": {"author": "[{\"end\":190,\"start\":84},{\"end\":291,\"start\":191},{\"end\":398,\"start\":292}]", "publisher": null, "author_last_name": "[{\"end\":99,\"start\":91},{\"end\":200,\"start\":197},{\"end\":307,\"start\":299}]", "author_first_name": "[{\"end\":88,\"start\":84},{\"end\":90,\"start\":89},{\"end\":196,\"start\":191},{\"end\":298,\"start\":292}]", "author_affiliation": "[{\"end\":189,\"start\":101},{\"end\":290,\"start\":202},{\"end\":397,\"start\":309}]", "title": "[{\"end\":81,\"start\":1},{\"end\":479,\"start\":399}]", "venue": null, "abstract": "[{\"end\":1682,\"start\":616}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1818,\"start\":1815},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1820,\"start\":1818},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1939,\"start\":1936},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2133,\"start\":2130},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2166,\"start\":2163},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2258,\"start\":2255},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2343,\"start\":2340},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2458,\"start\":2455},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2738,\"start\":2735},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3207,\"start\":3203},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3210,\"start\":3207},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3341,\"start\":3338},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3625,\"start\":3622},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3627,\"start\":3625},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3736,\"start\":3733},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3740,\"start\":3736},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3744,\"start\":3740},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3748,\"start\":3744},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3752,\"start\":3748},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3756,\"start\":3752},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3760,\"start\":3756},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3972,\"start\":3968},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7087,\"start\":7084},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7089,\"start\":7087},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7276,\"start\":7272},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7279,\"start\":7276},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7296,\"start\":7292},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7299,\"start\":7296},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7996,\"start\":7992},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7999,\"start\":7996},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8199,\"start\":8195},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8240,\"start\":8237},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8243,\"start\":8240},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8246,\"start\":8243},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8249,\"start\":8246},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8291,\"start\":8287},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8418,\"start\":8414},{\"end\":8469,\"start\":8454},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8588,\"start\":8584},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8785,\"start\":8781},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9173,\"start\":9169},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9176,\"start\":9173},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9306,\"start\":9302},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9310,\"start\":9306},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9314,\"start\":9310},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9341,\"start\":9337},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9344,\"start\":9341},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9368,\"start\":9364},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9405,\"start\":9401},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9408,\"start\":9405},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10759,\"start\":10755},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11549,\"start\":11545},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11808,\"start\":11804},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11996,\"start\":11992},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12133,\"start\":12129},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13657,\"start\":13653},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14689,\"start\":14685},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16679,\"start\":16675},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17522,\"start\":17518},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17816,\"start\":17812},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17819,\"start\":17816},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23567,\"start\":23563},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29238,\"start\":29234},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29241,\"start\":29238},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30361,\"start\":30357},{\"end\":35163,\"start\":35155},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36010,\"start\":36006},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36466,\"start\":36463},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":50229,\"start\":50225},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":51558,\"start\":51554},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":53792,\"start\":53788},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":54706,\"start\":54702},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":54709,\"start\":54706},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":54712,\"start\":54709},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":56452,\"start\":56449},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":58202,\"start\":58198},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":64622,\"start\":64618},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":73476,\"start\":73472},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":73523,\"start\":73519},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":74075,\"start\":74071},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":74078,\"start\":74075},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":74769,\"start\":74765},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":74959,\"start\":74955},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":74962,\"start\":74959},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":75082,\"start\":75078},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":75708,\"start\":75704},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":75840,\"start\":75836},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":76007,\"start\":76003},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":76527,\"start\":76523},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":76644,\"start\":76640},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":76796,\"start\":76792},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":77174,\"start\":77170},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":77177,\"start\":77174},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":77303,\"start\":77299},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":77461,\"start\":77457},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":77534,\"start\":77530},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":77735,\"start\":77731},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":78490,\"start\":78486},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":78493,\"start\":78490}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":79342,\"start\":79220},{\"attributes\":{\"id\":\"fig_1\"},\"end\":79881,\"start\":79343},{\"attributes\":{\"id\":\"fig_2\"},\"end\":80156,\"start\":79882},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":81771,\"start\":80157},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":82031,\"start\":81772},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":82331,\"start\":82032},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":82649,\"start\":82332},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":82915,\"start\":82650},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":83178,\"start\":82916},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":83395,\"start\":83179},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":83595,\"start\":83396}]", "paragraph": "[{\"end\":2596,\"start\":1698},{\"end\":3628,\"start\":2598},{\"end\":4502,\"start\":3630},{\"end\":5165,\"start\":4504},{\"end\":6026,\"start\":5167},{\"end\":6658,\"start\":6028},{\"end\":7568,\"start\":6674},{\"end\":8689,\"start\":7590},{\"end\":9409,\"start\":8721},{\"end\":9948,\"start\":9411},{\"end\":10434,\"start\":9950},{\"end\":10656,\"start\":10436},{\"end\":11127,\"start\":10658},{\"end\":11997,\"start\":11129},{\"end\":13485,\"start\":12047},{\"end\":14100,\"start\":13521},{\"end\":14604,\"start\":14102},{\"end\":15180,\"start\":14606},{\"end\":15495,\"start\":15182},{\"end\":15862,\"start\":15497},{\"end\":16583,\"start\":15864},{\"end\":17027,\"start\":16596},{\"end\":18052,\"start\":17029},{\"end\":19179,\"start\":18077},{\"end\":19955,\"start\":19192},{\"end\":20219,\"start\":19984},{\"end\":22285,\"start\":20221},{\"end\":23090,\"start\":22287},{\"end\":23424,\"start\":23139},{\"end\":24600,\"start\":23457},{\"end\":25007,\"start\":24611},{\"end\":25860,\"start\":25009},{\"end\":26550,\"start\":25904},{\"end\":27210,\"start\":26575},{\"end\":27818,\"start\":27237},{\"end\":28181,\"start\":27833},{\"end\":28305,\"start\":28204},{\"end\":28600,\"start\":28307},{\"end\":28717,\"start\":28602},{\"end\":28835,\"start\":28719},{\"end\":28995,\"start\":28837},{\"end\":30002,\"start\":28997},{\"end\":30797,\"start\":30022},{\"end\":31733,\"start\":30799},{\"end\":32663,\"start\":31749},{\"end\":33751,\"start\":32689},{\"end\":34849,\"start\":33777},{\"end\":35924,\"start\":34866},{\"end\":37240,\"start\":35945},{\"end\":37790,\"start\":37260},{\"end\":39436,\"start\":37814},{\"end\":40181,\"start\":39438},{\"end\":41087,\"start\":40183},{\"end\":41728,\"start\":41089},{\"end\":42280,\"start\":41740},{\"end\":42674,\"start\":42298},{\"end\":43366,\"start\":42676},{\"end\":43788,\"start\":43368},{\"end\":44717,\"start\":43810},{\"end\":45231,\"start\":44719},{\"end\":45608,\"start\":45252},{\"end\":46217,\"start\":45610},{\"end\":46503,\"start\":46238},{\"end\":47435,\"start\":46505},{\"end\":48737,\"start\":47455},{\"end\":49481,\"start\":48739},{\"end\":49647,\"start\":49496},{\"end\":51559,\"start\":49649},{\"end\":52926,\"start\":51561},{\"end\":53462,\"start\":52928},{\"end\":54004,\"start\":53464},{\"end\":55436,\"start\":54006},{\"end\":56092,\"start\":55438},{\"end\":56735,\"start\":56117},{\"end\":56894,\"start\":56741},{\"end\":57066,\"start\":56900},{\"end\":57644,\"start\":57068},{\"end\":58563,\"start\":57664},{\"end\":59678,\"start\":58625},{\"end\":60890,\"start\":59702},{\"end\":61171,\"start\":60905},{\"end\":61548,\"start\":61173},{\"end\":62708,\"start\":61550},{\"end\":63465,\"start\":62725},{\"end\":63634,\"start\":63485},{\"end\":64194,\"start\":63658},{\"end\":64793,\"start\":64196},{\"end\":65481,\"start\":64795},{\"end\":66232,\"start\":65483},{\"end\":66395,\"start\":66260},{\"end\":67307,\"start\":66417},{\"end\":68087,\"start\":67309},{\"end\":69740,\"start\":68115},{\"end\":71158,\"start\":69742},{\"end\":71975,\"start\":71189},{\"end\":72746,\"start\":71977},{\"end\":73672,\"start\":72748},{\"end\":74306,\"start\":73674},{\"end\":75454,\"start\":74323},{\"end\":77019,\"start\":75456},{\"end\":78066,\"start\":77021},{\"end\":79219,\"start\":78081}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":25903,\"start\":25861},{\"attributes\":{\"id\":\"formula_1\"},\"end\":56740,\"start\":56736},{\"attributes\":{\"id\":\"formula_2\"},\"end\":56899,\"start\":56895}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41965,\"start\":41958},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":43164,\"start\":43156},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":44638,\"start\":44630},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":46142,\"start\":46135},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":48008,\"start\":47999},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":48104,\"start\":48095},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":48832,\"start\":48824},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":58084,\"start\":58077},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":59566,\"start\":59558},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":61342,\"start\":61333},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":63225,\"start\":63216},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":66516,\"start\":66506},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":68301,\"start\":68293}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1696,\"start\":1684},{\"attributes\":{\"n\":\"2.\"},\"end\":6672,\"start\":6661},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7588,\"start\":7571},{\"attributes\":{\"n\":\"3.2.\"},\"end\":8719,\"start\":8692},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12045,\"start\":12000},{\"attributes\":{\"n\":\"3.4.\"},\"end\":13519,\"start\":13488},{\"attributes\":{\"n\":\"3.5.\"},\"end\":16594,\"start\":16586},{\"attributes\":{\"n\":\"3.6.\"},\"end\":18075,\"start\":18055},{\"attributes\":{\"n\":\"4.\"},\"end\":19190,\"start\":19182},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19982,\"start\":19958},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23137,\"start\":23093},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":23455,\"start\":23427},{\"end\":24609,\"start\":24603},{\"attributes\":{\"n\":\"4.2.3.\"},\"end\":26573,\"start\":26553},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27235,\"start\":27213},{\"attributes\":{\"n\":\"5.\"},\"end\":27831,\"start\":27821},{\"attributes\":{\"n\":\"5.1.\"},\"end\":28202,\"start\":28184},{\"attributes\":{\"n\":\"5.2.\"},\"end\":30020,\"start\":30005},{\"attributes\":{\"n\":\"5.3.\"},\"end\":31747,\"start\":31736},{\"attributes\":{\"n\":\"5.3.1.\"},\"end\":32687,\"start\":32666},{\"attributes\":{\"n\":\"5.3.2.\"},\"end\":33775,\"start\":33754},{\"attributes\":{\"n\":\"5.4.\"},\"end\":34864,\"start\":34852},{\"attributes\":{\"n\":\"5.5.\"},\"end\":35943,\"start\":35927},{\"attributes\":{\"n\":\"5.6.\"},\"end\":37258,\"start\":37243},{\"attributes\":{\"n\":\"5.7.\"},\"end\":37812,\"start\":37793},{\"attributes\":{\"n\":\"6.\"},\"end\":41738,\"start\":41731},{\"attributes\":{\"n\":\"6.1.\"},\"end\":42296,\"start\":42283},{\"attributes\":{\"n\":\"6.2.\"},\"end\":43808,\"start\":43791},{\"attributes\":{\"n\":\"6.3.\"},\"end\":45250,\"start\":45234},{\"attributes\":{\"n\":\"6.4.\"},\"end\":46236,\"start\":46220},{\"attributes\":{\"n\":\"6.5.\"},\"end\":47453,\"start\":47438},{\"attributes\":{\"n\":\"7.\"},\"end\":49494,\"start\":49484},{\"attributes\":{\"n\":\"8.\"},\"end\":56115,\"start\":56095},{\"attributes\":{\"n\":\"8.1.\"},\"end\":57662,\"start\":57647},{\"attributes\":{\"n\":\"8.2.\"},\"end\":58623,\"start\":58566},{\"attributes\":{\"n\":\"8.3.\"},\"end\":59700,\"start\":59681},{\"attributes\":{\"n\":\"8.4.\"},\"end\":60903,\"start\":60893},{\"attributes\":{\"n\":\"8.5.\"},\"end\":62723,\"start\":62711},{\"attributes\":{\"n\":\"8.6.\"},\"end\":63483,\"start\":63468},{\"attributes\":{\"n\":\"8.7.\"},\"end\":63656,\"start\":63637},{\"attributes\":{\"n\":\"9.\"},\"end\":66258,\"start\":66235},{\"attributes\":{\"n\":\"9.1.\"},\"end\":66415,\"start\":66398},{\"attributes\":{\"n\":\"9.2.\"},\"end\":68113,\"start\":68090},{\"attributes\":{\"n\":\"10.\"},\"end\":71187,\"start\":71161},{\"attributes\":{\"n\":\"11.\"},\"end\":74321,\"start\":74309},{\"attributes\":{\"n\":\"12.\"},\"end\":78079,\"start\":78069},{\"end\":79231,\"start\":79221},{\"end\":79349,\"start\":79344},{\"end\":79888,\"start\":79883},{\"end\":80168,\"start\":80158},{\"end\":81782,\"start\":81773},{\"end\":82043,\"start\":82033},{\"end\":82344,\"start\":82333},{\"end\":82660,\"start\":82651},{\"end\":82927,\"start\":82917},{\"end\":83190,\"start\":83180},{\"end\":83409,\"start\":83397}]", "table": "[{\"end\":81771,\"start\":80505},{\"end\":82031,\"start\":81811},{\"end\":82331,\"start\":82106},{\"end\":82649,\"start\":82491},{\"end\":82915,\"start\":82780},{\"end\":83178,\"start\":82930},{\"end\":83395,\"start\":83263},{\"end\":83595,\"start\":83463}]", "figure_caption": "[{\"end\":79342,\"start\":79233},{\"end\":79881,\"start\":79351},{\"end\":80156,\"start\":79890},{\"end\":80505,\"start\":80171},{\"end\":81811,\"start\":81784},{\"end\":82106,\"start\":82046},{\"end\":82491,\"start\":82348},{\"end\":82780,\"start\":82662},{\"end\":83263,\"start\":83193},{\"end\":83463,\"start\":83414}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19402,\"start\":19394},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20270,\"start\":20262},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21831,\"start\":21823},{\"end\":25006,\"start\":24998},{\"end\":25455,\"start\":25447},{\"end\":32815,\"start\":32807},{\"end\":33916,\"start\":33908},{\"end\":37494,\"start\":37486},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59208,\"start\":59200}]", "bib_author_first_name": "[{\"end\":84171,\"start\":84170},{\"end\":84182,\"start\":84181},{\"end\":84192,\"start\":84191},{\"end\":84208,\"start\":84201},{\"end\":84210,\"start\":84209},{\"end\":84219,\"start\":84218},{\"end\":84231,\"start\":84230},{\"end\":84249,\"start\":84248},{\"end\":84263,\"start\":84262},{\"end\":84660,\"start\":84659},{\"end\":84672,\"start\":84671},{\"end\":84682,\"start\":84681},{\"end\":84702,\"start\":84695},{\"end\":84704,\"start\":84703},{\"end\":84713,\"start\":84712},{\"end\":84725,\"start\":84724},{\"end\":85114,\"start\":85113},{\"end\":85125,\"start\":85124},{\"end\":85136,\"start\":85135},{\"end\":85138,\"start\":85137},{\"end\":85440,\"start\":85439},{\"end\":85442,\"start\":85441},{\"end\":85467,\"start\":85466},{\"end\":85469,\"start\":85468},{\"end\":85478,\"start\":85477},{\"end\":85480,\"start\":85479},{\"end\":85686,\"start\":85685},{\"end\":85688,\"start\":85687},{\"end\":85698,\"start\":85697},{\"end\":85919,\"start\":85918},{\"end\":85929,\"start\":85928},{\"end\":85931,\"start\":85930},{\"end\":85942,\"start\":85941},{\"end\":86276,\"start\":86275},{\"end\":86541,\"start\":86526},{\"end\":86554,\"start\":86553},{\"end\":86564,\"start\":86563},{\"end\":86577,\"start\":86576},{\"end\":87195,\"start\":87194},{\"end\":87204,\"start\":87203},{\"end\":87214,\"start\":87213},{\"end\":87225,\"start\":87224},{\"end\":87823,\"start\":87820},{\"end\":87835,\"start\":87834},{\"end\":87847,\"start\":87846},{\"end\":87849,\"start\":87848},{\"end\":88612,\"start\":88611},{\"end\":88959,\"start\":88958},{\"end\":88961,\"start\":88960},{\"end\":88970,\"start\":88969},{\"end\":88972,\"start\":88971},{\"end\":88983,\"start\":88982},{\"end\":89358,\"start\":89357},{\"end\":89370,\"start\":89369},{\"end\":89379,\"start\":89378},{\"end\":89394,\"start\":89393},{\"end\":89403,\"start\":89402},{\"end\":90066,\"start\":90065},{\"end\":90077,\"start\":90076},{\"end\":90087,\"start\":90086},{\"end\":90615,\"start\":90614},{\"end\":90629,\"start\":90628},{\"end\":90636,\"start\":90635},{\"end\":90648,\"start\":90646},{\"end\":90657,\"start\":90656},{\"end\":90671,\"start\":90670},{\"end\":90676,\"start\":90674},{\"end\":90685,\"start\":90684},{\"end\":91300,\"start\":91299},{\"end\":91308,\"start\":91307},{\"end\":91315,\"start\":91314},{\"end\":91321,\"start\":91320},{\"end\":91905,\"start\":91904},{\"end\":91914,\"start\":91913},{\"end\":91916,\"start\":91915},{\"end\":92191,\"start\":92190},{\"end\":92198,\"start\":92197},{\"end\":92209,\"start\":92208},{\"end\":92220,\"start\":92219},{\"end\":92532,\"start\":92531},{\"end\":92541,\"start\":92540},{\"end\":92551,\"start\":92550},{\"end\":92553,\"start\":92552},{\"end\":93140,\"start\":93139},{\"end\":93151,\"start\":93150},{\"end\":93153,\"start\":93152},{\"end\":93164,\"start\":93162},{\"end\":93174,\"start\":93173},{\"end\":93545,\"start\":93544},{\"end\":93547,\"start\":93546},{\"end\":93559,\"start\":93558},{\"end\":94159,\"start\":94158},{\"end\":94441,\"start\":94440},{\"end\":94453,\"start\":94452},{\"end\":94461,\"start\":94460},{\"end\":94474,\"start\":94473},{\"end\":94485,\"start\":94484},{\"end\":95144,\"start\":95143},{\"end\":95154,\"start\":95153},{\"end\":95164,\"start\":95163},{\"end\":95176,\"start\":95175},{\"end\":95186,\"start\":95185},{\"end\":95197,\"start\":95196},{\"end\":95594,\"start\":95593},{\"end\":95604,\"start\":95603},{\"end\":95614,\"start\":95613},{\"end\":95625,\"start\":95624},{\"end\":96010,\"start\":96009},{\"end\":96022,\"start\":96021},{\"end\":96033,\"start\":96032},{\"end\":96348,\"start\":96347},{\"end\":96350,\"start\":96349},{\"end\":96358,\"start\":96357},{\"end\":96360,\"start\":96359},{\"end\":96366,\"start\":96365},{\"end\":96368,\"start\":96367},{\"end\":96670,\"start\":96662},{\"end\":96676,\"start\":96675},{\"end\":96678,\"start\":96677},{\"end\":96690,\"start\":96689},{\"end\":96692,\"start\":96691},{\"end\":97280,\"start\":97279},{\"end\":97290,\"start\":97289},{\"end\":97292,\"start\":97291},{\"end\":97954,\"start\":97953},{\"end\":97965,\"start\":97964},{\"end\":97976,\"start\":97975},{\"end\":97991,\"start\":97989},{\"end\":98000,\"start\":97999},{\"end\":98349,\"start\":98348},{\"end\":98360,\"start\":98359},{\"end\":98369,\"start\":98368},{\"end\":98379,\"start\":98378},{\"end\":98949,\"start\":98948},{\"end\":98958,\"start\":98957},{\"end\":98960,\"start\":98959},{\"end\":98969,\"start\":98968},{\"end\":98971,\"start\":98970},{\"end\":99393,\"start\":99392},{\"end\":99403,\"start\":99402},{\"end\":99412,\"start\":99411},{\"end\":99729,\"start\":99728},{\"end\":100033,\"start\":100032},{\"end\":100041,\"start\":100040},{\"end\":100052,\"start\":100051},{\"end\":100438,\"start\":100437},{\"end\":100447,\"start\":100446},{\"end\":100459,\"start\":100458},{\"end\":100468,\"start\":100467},{\"end\":100758,\"start\":100757},{\"end\":100760,\"start\":100759},{\"end\":100930,\"start\":100929},{\"end\":100942,\"start\":100941},{\"end\":101672,\"start\":101671},{\"end\":101890,\"start\":101889},{\"end\":101892,\"start\":101891},{\"end\":101904,\"start\":101903},{\"end\":101911,\"start\":101910},{\"end\":101923,\"start\":101922},{\"end\":102500,\"start\":102499},{\"end\":102507,\"start\":102506},{\"end\":102516,\"start\":102515},{\"end\":102518,\"start\":102517},{\"end\":102527,\"start\":102526},{\"end\":103035,\"start\":103034},{\"end\":103043,\"start\":103042},{\"end\":103326,\"start\":103325},{\"end\":103338,\"start\":103337},{\"end\":103348,\"start\":103347},{\"end\":103368,\"start\":103361},{\"end\":103370,\"start\":103369},{\"end\":103379,\"start\":103378},{\"end\":103391,\"start\":103390},{\"end\":103861,\"start\":103860},{\"end\":103873,\"start\":103872},{\"end\":104208,\"start\":104207},{\"end\":104218,\"start\":104217},{\"end\":104228,\"start\":104227},{\"end\":104535,\"start\":104534},{\"end\":104549,\"start\":104548},{\"end\":104560,\"start\":104558},{\"end\":104569,\"start\":104568},{\"end\":104579,\"start\":104578},{\"end\":104590,\"start\":104589},{\"end\":104954,\"start\":104953},{\"end\":104962,\"start\":104961},{\"end\":104970,\"start\":104969},{\"end\":105306,\"start\":105305},{\"end\":105316,\"start\":105315},{\"end\":105326,\"start\":105325},{\"end\":105336,\"start\":105335},{\"end\":105696,\"start\":105695},{\"end\":105709,\"start\":105708},{\"end\":105721,\"start\":105720},{\"end\":105723,\"start\":105722},{\"end\":105735,\"start\":105734},{\"end\":105744,\"start\":105743},{\"end\":105753,\"start\":105752},{\"end\":106352,\"start\":106351},{\"end\":106361,\"start\":106360},{\"end\":106675,\"start\":106674},{\"end\":106688,\"start\":106686},{\"end\":106697,\"start\":106696},{\"end\":106708,\"start\":106707},{\"end\":106722,\"start\":106721},{\"end\":106736,\"start\":106735},{\"end\":107072,\"start\":107071},{\"end\":107074,\"start\":107073},{\"end\":107083,\"start\":107082},{\"end\":107085,\"start\":107084},{\"end\":107094,\"start\":107093},{\"end\":107096,\"start\":107095},{\"end\":107108,\"start\":107107},{\"end\":107110,\"start\":107109}]", "bib_author_last_name": "[{\"end\":84179,\"start\":84172},{\"end\":84189,\"start\":84183},{\"end\":84199,\"start\":84193},{\"end\":84216,\"start\":84211},{\"end\":84228,\"start\":84220},{\"end\":84246,\"start\":84232},{\"end\":84260,\"start\":84250},{\"end\":84669,\"start\":84661},{\"end\":84679,\"start\":84673},{\"end\":84693,\"start\":84683},{\"end\":84710,\"start\":84705},{\"end\":84722,\"start\":84714},{\"end\":85122,\"start\":85115},{\"end\":85133,\"start\":85126},{\"end\":85148,\"start\":85139},{\"end\":85447,\"start\":85443},{\"end\":85456,\"start\":85449},{\"end\":85464,\"start\":85458},{\"end\":85475,\"start\":85470},{\"end\":85489,\"start\":85481},{\"end\":85695,\"start\":85689},{\"end\":85704,\"start\":85699},{\"end\":85926,\"start\":85920},{\"end\":85939,\"start\":85932},{\"end\":85951,\"start\":85943},{\"end\":86281,\"start\":86277},{\"end\":86551,\"start\":86542},{\"end\":86561,\"start\":86555},{\"end\":86574,\"start\":86565},{\"end\":87201,\"start\":87196},{\"end\":87211,\"start\":87205},{\"end\":87222,\"start\":87215},{\"end\":87232,\"start\":87226},{\"end\":87832,\"start\":87824},{\"end\":87844,\"start\":87836},{\"end\":87861,\"start\":87850},{\"end\":88627,\"start\":88613},{\"end\":88967,\"start\":88962},{\"end\":88980,\"start\":88973},{\"end\":88994,\"start\":88984},{\"end\":89367,\"start\":89359},{\"end\":89376,\"start\":89371},{\"end\":89391,\"start\":89380},{\"end\":89400,\"start\":89395},{\"end\":89409,\"start\":89404},{\"end\":90074,\"start\":90067},{\"end\":90084,\"start\":90078},{\"end\":90096,\"start\":90088},{\"end\":90626,\"start\":90616},{\"end\":90633,\"start\":90630},{\"end\":90644,\"start\":90637},{\"end\":90654,\"start\":90649},{\"end\":90668,\"start\":90658},{\"end\":90682,\"start\":90677},{\"end\":91305,\"start\":91301},{\"end\":91312,\"start\":91309},{\"end\":91318,\"start\":91316},{\"end\":91326,\"start\":91322},{\"end\":91911,\"start\":91906},{\"end\":91923,\"start\":91917},{\"end\":92195,\"start\":92192},{\"end\":92206,\"start\":92199},{\"end\":92217,\"start\":92210},{\"end\":92231,\"start\":92221},{\"end\":92538,\"start\":92533},{\"end\":92548,\"start\":92542},{\"end\":92558,\"start\":92554},{\"end\":93148,\"start\":93141},{\"end\":93160,\"start\":93154},{\"end\":93171,\"start\":93165},{\"end\":93556,\"start\":93548},{\"end\":93568,\"start\":93560},{\"end\":94168,\"start\":94160},{\"end\":94450,\"start\":94442},{\"end\":94458,\"start\":94454},{\"end\":94471,\"start\":94462},{\"end\":94482,\"start\":94475},{\"end\":94499,\"start\":94486},{\"end\":95151,\"start\":95145},{\"end\":95161,\"start\":95155},{\"end\":95173,\"start\":95165},{\"end\":95183,\"start\":95177},{\"end\":95194,\"start\":95187},{\"end\":95211,\"start\":95198},{\"end\":95601,\"start\":95595},{\"end\":95611,\"start\":95605},{\"end\":95622,\"start\":95615},{\"end\":95639,\"start\":95626},{\"end\":96019,\"start\":96011},{\"end\":96030,\"start\":96023},{\"end\":96047,\"start\":96034},{\"end\":96355,\"start\":96351},{\"end\":96363,\"start\":96361},{\"end\":96375,\"start\":96369},{\"end\":96673,\"start\":96671},{\"end\":96687,\"start\":96679},{\"end\":96699,\"start\":96693},{\"end\":97287,\"start\":97281},{\"end\":97300,\"start\":97293},{\"end\":97962,\"start\":97955},{\"end\":97973,\"start\":97966},{\"end\":97987,\"start\":97977},{\"end\":97997,\"start\":97992},{\"end\":98357,\"start\":98350},{\"end\":98366,\"start\":98361},{\"end\":98376,\"start\":98370},{\"end\":98388,\"start\":98380},{\"end\":98955,\"start\":98950},{\"end\":98966,\"start\":98961},{\"end\":98982,\"start\":98972},{\"end\":99400,\"start\":99394},{\"end\":99409,\"start\":99404},{\"end\":99420,\"start\":99413},{\"end\":99736,\"start\":99730},{\"end\":100038,\"start\":100034},{\"end\":100049,\"start\":100042},{\"end\":100063,\"start\":100053},{\"end\":100444,\"start\":100439},{\"end\":100456,\"start\":100448},{\"end\":100465,\"start\":100460},{\"end\":100474,\"start\":100469},{\"end\":100767,\"start\":100761},{\"end\":100939,\"start\":100931},{\"end\":100948,\"start\":100943},{\"end\":101684,\"start\":101673},{\"end\":101901,\"start\":101893},{\"end\":101908,\"start\":101905},{\"end\":101920,\"start\":101912},{\"end\":101932,\"start\":101924},{\"end\":102504,\"start\":102501},{\"end\":102513,\"start\":102508},{\"end\":102524,\"start\":102519},{\"end\":102531,\"start\":102528},{\"end\":103040,\"start\":103036},{\"end\":103051,\"start\":103044},{\"end\":103335,\"start\":103327},{\"end\":103345,\"start\":103339},{\"end\":103359,\"start\":103349},{\"end\":103376,\"start\":103371},{\"end\":103388,\"start\":103380},{\"end\":103870,\"start\":103862},{\"end\":103882,\"start\":103874},{\"end\":104215,\"start\":104209},{\"end\":104225,\"start\":104219},{\"end\":104237,\"start\":104229},{\"end\":104546,\"start\":104536},{\"end\":104556,\"start\":104550},{\"end\":104566,\"start\":104561},{\"end\":104576,\"start\":104570},{\"end\":104587,\"start\":104580},{\"end\":104959,\"start\":104955},{\"end\":104967,\"start\":104963},{\"end\":104974,\"start\":104971},{\"end\":105313,\"start\":105307},{\"end\":105323,\"start\":105317},{\"end\":105333,\"start\":105327},{\"end\":105343,\"start\":105337},{\"end\":105706,\"start\":105697},{\"end\":105718,\"start\":105710},{\"end\":105732,\"start\":105724},{\"end\":105741,\"start\":105736},{\"end\":105750,\"start\":105745},{\"end\":106358,\"start\":106353},{\"end\":106365,\"start\":106362},{\"end\":106684,\"start\":106676},{\"end\":106694,\"start\":106689},{\"end\":106705,\"start\":106698},{\"end\":106719,\"start\":106709},{\"end\":106733,\"start\":106723},{\"end\":107080,\"start\":107075},{\"end\":107091,\"start\":107086},{\"end\":107105,\"start\":107097},{\"end\":107122,\"start\":107111}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2779250},\"end\":84594,\"start\":84087},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14011908},\"end\":85002,\"start\":84596},{\"attributes\":{\"id\":\"b2\"},\"end\":85363,\"start\":85004},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":62176183},\"end\":85642,\"start\":85365},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":32388452},\"end\":85837,\"start\":85644},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7696812},\"end\":86236,\"start\":85839},{\"attributes\":{\"id\":\"b6\"},\"end\":86433,\"start\":86238},{\"attributes\":{\"doi\":\"DOI: 10.1145/ 1806799.1806825\",\"id\":\"b7\",\"matched_paper_id\":16845010},\"end\":87139,\"start\":86435},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9956794},\"end\":87754,\"start\":87141},{\"attributes\":{\"doi\":\"10.1145/1085313.1085331\",\"id\":\"b9\",\"matched_paper_id\":9333486},\"end\":88543,\"start\":87756},{\"attributes\":{\"doi\":\"10.1023/B:LIDA.0000048322.42751.ca\",\"id\":\"b10\",\"matched_paper_id\":14585311},\"end\":88877,\"start\":88545},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8832731},\"end\":89296,\"start\":88879},{\"attributes\":{\"doi\":\"10.1145/1321631.1321709\",\"id\":\"b12\",\"matched_paper_id\":10126355},\"end\":89990,\"start\":89298},{\"attributes\":{\"doi\":\"10.1145/1342211.1342234\",\"id\":\"b13\",\"matched_paper_id\":15215844},\"end\":90503,\"start\":89992},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3167665},\"end\":91206,\"start\":90505},{\"attributes\":{\"doi\":\"10.1145/2384416.2384419\",\"id\":\"b15\",\"matched_paper_id\":17482884},\"end\":91843,\"start\":91208},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":36457818},\"end\":92132,\"start\":91845},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7630279},\"end\":92440,\"start\":92134},{\"attributes\":{\"doi\":\"10.1109/WCRE.2007.21\",\"id\":\"b18\",\"matched_paper_id\":16017158},\"end\":93072,\"start\":92442},{\"attributes\":{\"doi\":\"DOI: 10.1109/ TSE.2008.26\",\"id\":\"b19\",\"matched_paper_id\":278036},\"end\":93458,\"start\":93074},{\"attributes\":{\"doi\":\"10.1145/2597008.2597149\",\"id\":\"b20\",\"matched_paper_id\":8649673},\"end\":94062,\"start\":93460},{\"attributes\":{\"id\":\"b21\"},\"end\":94370,\"start\":94064},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9790585},\"end\":95072,\"start\":94372},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1129667},\"end\":95507,\"start\":95074},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":33600220},\"end\":95933,\"start\":95509},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":11106352},\"end\":96316,\"start\":95935},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3177797},\"end\":96617,\"start\":96318},{\"attributes\":{\"doi\":\"10.1145/1806799.1806817\",\"id\":\"b27\",\"matched_paper_id\":2540713},\"end\":97186,\"start\":96619},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7730498},\"end\":97857,\"start\":97188},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4823379},\"end\":98303,\"start\":97859},{\"attributes\":{\"doi\":\"10.1145/2597008.2597150\",\"id\":\"b30\",\"matched_paper_id\":2640649},\"end\":98853,\"start\":98305},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14137400},\"end\":99312,\"start\":98855},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":17290075},\"end\":99677,\"start\":99314},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":9441150},\"end\":99954,\"start\":99679},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":556917},\"end\":100375,\"start\":99956},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":18195900},\"end\":100714,\"start\":100377},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1671874},\"end\":100891,\"start\":100716},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":577937},\"end\":101597,\"start\":100893},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":60827152},\"end\":101838,\"start\":101599},{\"attributes\":{\"doi\":\"10.1145/2597008.2597793\",\"id\":\"b39\",\"matched_paper_id\":15278571},\"end\":102408,\"start\":101840},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":5705857},\"end\":102942,\"start\":102410},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14328772},\"end\":103260,\"start\":102944},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":14011908},\"end\":103766,\"start\":103262},{\"attributes\":{\"doi\":\"10.1007/s10664-014-9344-6\",\"id\":\"b43\",\"matched_paper_id\":9600877},\"end\":104163,\"start\":103768},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":16657129},\"end\":104469,\"start\":104165},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":16606535},\"end\":104871,\"start\":104471},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":10506832},\"end\":105220,\"start\":104873},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":7843537},\"end\":105605,\"start\":105222},{\"attributes\":{\"doi\":\"DOI: 10.1145/ 2568225.2568247\",\"id\":\"b48\",\"matched_paper_id\":1654208},\"end\":106289,\"start\":105607},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":10262993},\"end\":106601,\"start\":106291},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":2529398},\"end\":107031,\"start\":106603},{\"attributes\":{\"doi\":\"10.1145/1449955.1449807\",\"id\":\"b51\",\"matched_paper_id\":12201544},\"end\":107337,\"start\":107033}]", "bib_title": "[{\"end\":84168,\"start\":84087},{\"end\":84657,\"start\":84596},{\"end\":85437,\"start\":85365},{\"end\":85683,\"start\":85644},{\"end\":85916,\"start\":85839},{\"end\":86524,\"start\":86435},{\"end\":87192,\"start\":87141},{\"end\":87818,\"start\":87756},{\"end\":88609,\"start\":88545},{\"end\":88956,\"start\":88879},{\"end\":89355,\"start\":89298},{\"end\":90063,\"start\":89992},{\"end\":90612,\"start\":90505},{\"end\":91297,\"start\":91208},{\"end\":91902,\"start\":91845},{\"end\":92188,\"start\":92134},{\"end\":92529,\"start\":92442},{\"end\":93137,\"start\":93074},{\"end\":93542,\"start\":93460},{\"end\":94438,\"start\":94372},{\"end\":95141,\"start\":95074},{\"end\":95591,\"start\":95509},{\"end\":96007,\"start\":95935},{\"end\":96345,\"start\":96318},{\"end\":96660,\"start\":96619},{\"end\":97277,\"start\":97188},{\"end\":97951,\"start\":97859},{\"end\":98346,\"start\":98305},{\"end\":98946,\"start\":98855},{\"end\":99390,\"start\":99314},{\"end\":99726,\"start\":99679},{\"end\":100030,\"start\":99956},{\"end\":100435,\"start\":100377},{\"end\":100755,\"start\":100716},{\"end\":100927,\"start\":100893},{\"end\":101669,\"start\":101599},{\"end\":101887,\"start\":101840},{\"end\":102497,\"start\":102410},{\"end\":103032,\"start\":102944},{\"end\":103323,\"start\":103262},{\"end\":103858,\"start\":103768},{\"end\":104205,\"start\":104165},{\"end\":104532,\"start\":104471},{\"end\":104951,\"start\":104873},{\"end\":105303,\"start\":105222},{\"end\":105693,\"start\":105607},{\"end\":106349,\"start\":106291},{\"end\":106672,\"start\":106603},{\"end\":107069,\"start\":107033}]", "bib_author": "[{\"end\":84181,\"start\":84170},{\"end\":84191,\"start\":84181},{\"end\":84201,\"start\":84191},{\"end\":84218,\"start\":84201},{\"end\":84230,\"start\":84218},{\"end\":84248,\"start\":84230},{\"end\":84262,\"start\":84248},{\"end\":84266,\"start\":84262},{\"end\":84671,\"start\":84659},{\"end\":84681,\"start\":84671},{\"end\":84695,\"start\":84681},{\"end\":84712,\"start\":84695},{\"end\":84724,\"start\":84712},{\"end\":84728,\"start\":84724},{\"end\":85124,\"start\":85113},{\"end\":85135,\"start\":85124},{\"end\":85150,\"start\":85135},{\"end\":85449,\"start\":85439},{\"end\":85458,\"start\":85449},{\"end\":85466,\"start\":85458},{\"end\":85477,\"start\":85466},{\"end\":85491,\"start\":85477},{\"end\":85697,\"start\":85685},{\"end\":85706,\"start\":85697},{\"end\":85928,\"start\":85918},{\"end\":85941,\"start\":85928},{\"end\":85953,\"start\":85941},{\"end\":86283,\"start\":86275},{\"end\":86553,\"start\":86526},{\"end\":86563,\"start\":86553},{\"end\":86576,\"start\":86563},{\"end\":86580,\"start\":86576},{\"end\":87203,\"start\":87194},{\"end\":87213,\"start\":87203},{\"end\":87224,\"start\":87213},{\"end\":87234,\"start\":87224},{\"end\":87834,\"start\":87820},{\"end\":87846,\"start\":87834},{\"end\":87863,\"start\":87846},{\"end\":88629,\"start\":88611},{\"end\":88969,\"start\":88958},{\"end\":88982,\"start\":88969},{\"end\":88996,\"start\":88982},{\"end\":89369,\"start\":89357},{\"end\":89378,\"start\":89369},{\"end\":89393,\"start\":89378},{\"end\":89402,\"start\":89393},{\"end\":89411,\"start\":89402},{\"end\":90076,\"start\":90065},{\"end\":90086,\"start\":90076},{\"end\":90098,\"start\":90086},{\"end\":90628,\"start\":90614},{\"end\":90635,\"start\":90628},{\"end\":90646,\"start\":90635},{\"end\":90656,\"start\":90646},{\"end\":90670,\"start\":90656},{\"end\":90674,\"start\":90670},{\"end\":90684,\"start\":90674},{\"end\":90688,\"start\":90684},{\"end\":91307,\"start\":91299},{\"end\":91314,\"start\":91307},{\"end\":91320,\"start\":91314},{\"end\":91328,\"start\":91320},{\"end\":91913,\"start\":91904},{\"end\":91925,\"start\":91913},{\"end\":92197,\"start\":92190},{\"end\":92208,\"start\":92197},{\"end\":92219,\"start\":92208},{\"end\":92233,\"start\":92219},{\"end\":92540,\"start\":92531},{\"end\":92550,\"start\":92540},{\"end\":92560,\"start\":92550},{\"end\":93150,\"start\":93139},{\"end\":93162,\"start\":93150},{\"end\":93173,\"start\":93162},{\"end\":93177,\"start\":93173},{\"end\":93558,\"start\":93544},{\"end\":93570,\"start\":93558},{\"end\":94170,\"start\":94158},{\"end\":94452,\"start\":94440},{\"end\":94460,\"start\":94452},{\"end\":94473,\"start\":94460},{\"end\":94484,\"start\":94473},{\"end\":94501,\"start\":94484},{\"end\":95153,\"start\":95143},{\"end\":95163,\"start\":95153},{\"end\":95175,\"start\":95163},{\"end\":95185,\"start\":95175},{\"end\":95196,\"start\":95185},{\"end\":95213,\"start\":95196},{\"end\":95603,\"start\":95593},{\"end\":95613,\"start\":95603},{\"end\":95624,\"start\":95613},{\"end\":95641,\"start\":95624},{\"end\":96021,\"start\":96009},{\"end\":96032,\"start\":96021},{\"end\":96049,\"start\":96032},{\"end\":96357,\"start\":96347},{\"end\":96365,\"start\":96357},{\"end\":96377,\"start\":96365},{\"end\":96675,\"start\":96662},{\"end\":96689,\"start\":96675},{\"end\":96701,\"start\":96689},{\"end\":97289,\"start\":97279},{\"end\":97302,\"start\":97289},{\"end\":97964,\"start\":97953},{\"end\":97975,\"start\":97964},{\"end\":97989,\"start\":97975},{\"end\":97999,\"start\":97989},{\"end\":98003,\"start\":97999},{\"end\":98359,\"start\":98348},{\"end\":98368,\"start\":98359},{\"end\":98378,\"start\":98368},{\"end\":98390,\"start\":98378},{\"end\":98957,\"start\":98948},{\"end\":98968,\"start\":98957},{\"end\":98984,\"start\":98968},{\"end\":99402,\"start\":99392},{\"end\":99411,\"start\":99402},{\"end\":99422,\"start\":99411},{\"end\":99738,\"start\":99728},{\"end\":100040,\"start\":100032},{\"end\":100051,\"start\":100040},{\"end\":100065,\"start\":100051},{\"end\":100446,\"start\":100437},{\"end\":100458,\"start\":100446},{\"end\":100467,\"start\":100458},{\"end\":100476,\"start\":100467},{\"end\":100769,\"start\":100757},{\"end\":100941,\"start\":100929},{\"end\":100950,\"start\":100941},{\"end\":101686,\"start\":101671},{\"end\":101903,\"start\":101889},{\"end\":101910,\"start\":101903},{\"end\":101922,\"start\":101910},{\"end\":101934,\"start\":101922},{\"end\":102506,\"start\":102499},{\"end\":102515,\"start\":102506},{\"end\":102526,\"start\":102515},{\"end\":102533,\"start\":102526},{\"end\":103042,\"start\":103034},{\"end\":103053,\"start\":103042},{\"end\":103337,\"start\":103325},{\"end\":103347,\"start\":103337},{\"end\":103361,\"start\":103347},{\"end\":103378,\"start\":103361},{\"end\":103390,\"start\":103378},{\"end\":103394,\"start\":103390},{\"end\":103872,\"start\":103860},{\"end\":103884,\"start\":103872},{\"end\":104217,\"start\":104207},{\"end\":104227,\"start\":104217},{\"end\":104239,\"start\":104227},{\"end\":104548,\"start\":104534},{\"end\":104558,\"start\":104548},{\"end\":104568,\"start\":104558},{\"end\":104578,\"start\":104568},{\"end\":104589,\"start\":104578},{\"end\":104593,\"start\":104589},{\"end\":104961,\"start\":104953},{\"end\":104969,\"start\":104961},{\"end\":104976,\"start\":104969},{\"end\":105315,\"start\":105305},{\"end\":105325,\"start\":105315},{\"end\":105335,\"start\":105325},{\"end\":105345,\"start\":105335},{\"end\":105708,\"start\":105695},{\"end\":105720,\"start\":105708},{\"end\":105734,\"start\":105720},{\"end\":105743,\"start\":105734},{\"end\":105752,\"start\":105743},{\"end\":105756,\"start\":105752},{\"end\":106360,\"start\":106351},{\"end\":106367,\"start\":106360},{\"end\":106686,\"start\":106674},{\"end\":106696,\"start\":106686},{\"end\":106707,\"start\":106696},{\"end\":106721,\"start\":106707},{\"end\":106735,\"start\":106721},{\"end\":106739,\"start\":106735},{\"end\":107082,\"start\":107071},{\"end\":107093,\"start\":107082},{\"end\":107107,\"start\":107093},{\"end\":107124,\"start\":107107}]", "bib_venue": "[{\"end\":84332,\"start\":84266},{\"end\":84760,\"start\":84728},{\"end\":85111,\"start\":85004},{\"end\":85500,\"start\":85491},{\"end\":85730,\"start\":85706},{\"end\":86024,\"start\":85953},{\"end\":86273,\"start\":86238},{\"end\":86690,\"start\":86609},{\"end\":87321,\"start\":87234},{\"end\":88036,\"start\":87886},{\"end\":88684,\"start\":88663},{\"end\":89075,\"start\":88996},{\"end\":89548,\"start\":89434},{\"end\":90200,\"start\":90121},{\"end\":90775,\"start\":90688},{\"end\":91442,\"start\":91351},{\"end\":91978,\"start\":91925},{\"end\":92275,\"start\":92233},{\"end\":92660,\"start\":92580},{\"end\":93243,\"start\":93202},{\"end\":93677,\"start\":93593},{\"end\":94156,\"start\":94064},{\"end\":94601,\"start\":94501},{\"end\":95250,\"start\":95213},{\"end\":95678,\"start\":95641},{\"end\":96115,\"start\":96049},{\"end\":96413,\"start\":96377},{\"end\":96805,\"start\":96724},{\"end\":97389,\"start\":97302},{\"end\":98040,\"start\":98003},{\"end\":98497,\"start\":98413},{\"end\":99015,\"start\":98984},{\"end\":99485,\"start\":99422},{\"end\":99804,\"start\":99738},{\"end\":100154,\"start\":100065},{\"end\":100535,\"start\":100476},{\"end\":100794,\"start\":100769},{\"end\":101036,\"start\":100950},{\"end\":101708,\"start\":101686},{\"end\":102041,\"start\":101957},{\"end\":102634,\"start\":102533},{\"end\":103090,\"start\":103053},{\"end\":103466,\"start\":103394},{\"end\":103939,\"start\":103909},{\"end\":104276,\"start\":104239},{\"end\":104630,\"start\":104593},{\"end\":105012,\"start\":104976},{\"end\":105404,\"start\":105345},{\"end\":105868,\"start\":105785},{\"end\":106404,\"start\":106367},{\"end\":106776,\"start\":106739},{\"end\":107158,\"start\":107147},{\"end\":86775,\"start\":86692},{\"end\":87414,\"start\":87323},{\"end\":88190,\"start\":88038},{\"end\":89666,\"start\":89550},{\"end\":90219,\"start\":90202},{\"end\":90868,\"start\":90777},{\"end\":91537,\"start\":91444},{\"end\":92746,\"start\":92662},{\"end\":93765,\"start\":93679},{\"end\":94705,\"start\":94603},{\"end\":96890,\"start\":96807},{\"end\":97482,\"start\":97391},{\"end\":98585,\"start\":98499},{\"end\":101125,\"start\":101038},{\"end\":102129,\"start\":102043},{\"end\":103525,\"start\":103468},{\"end\":105955,\"start\":105870}]"}}}, "year": 2023, "month": 12, "day": 17}