{"id": 1994153, "updated": "2023-09-28 21:24:47.674", "metadata": {"title": "Graphulo Implementation of Server-Side Sparse Matrix Multiply in the Accumulo Database", "authors": "[{\"first\":\"Dylan\",\"last\":\"Hutchison\",\"middle\":[]},{\"first\":\"Jeremy\",\"last\":\"Kepner\",\"middle\":[]},{\"first\":\"Vijay\",\"last\":\"Gadepally\",\"middle\":[]},{\"first\":\"Adam\",\"last\":\"Fuchs\",\"middle\":[]}]", "venue": "2015 IEEE High Performance Extreme Computing Conference (HPEC)", "journal": "2015 IEEE High Performance Extreme Computing Conference (HPEC)", "publication_date": {"year": 2015, "month": null, "day": null}, "abstract": "The Apache Accumulo database excels at distributed storage and indexing and is ideally suited for storing graph data. Many big data analytics compute on graph data and persist their results back to the database. These graph calculations are often best performed inside the database server. The GraphBLAS standard provides a compact and efficient basis for a wide range of graph applications through a small number of sparse matrix operations. In this article, we implement GraphBLAS sparse matrix multiplication server-side by leveraging Accumulo's native, high-performance iterators. We compare the mathematics and performance of inner and outer product implementations, and show how an outer product implementation achieves optimal performance near Accumulo's peak write rate. We offer our work as a core component to the Graphulo library that will deliver matrix math primitives for graph analytics within Accumulo.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1507.01066", "mag": "3104997181", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/HutchisonKGF15", "doi": "10.1109/hpec.2015.7322448"}}, "content": {"source": {"pdf_hash": "2d84e763736e8bdd337f8c6c90fcc92db5f8ade8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1507.01066v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1507.01066", "status": "GREEN"}}, "grobid": {"id": "cc1765c2864c4c1f6f3be22ab01739a5bf0a2035", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2d84e763736e8bdd337f8c6c90fcc92db5f8ade8.txt", "contents": "\nGraphulo Implementation of Server-Side Sparse Matrix Multiply in the Accumulo Database\n\n\nDylan Hutchison \nJeremy Kepner \nMIT Mathematics Department\nMIT Computer Science & AI Laboratory\n\n\nVijay Gadepally \nMIT Mathematics Department\nMIT Computer Science & AI Laboratory\n\n\nAdam Fuchs \nSqrrl, Inc\n\n\n\nMIT Lincoln Laboratory\n\u00a7 University of Washington\n\n\nGraphulo Implementation of Server-Side Sparse Matrix Multiply in the Accumulo Database\n\nThe Apache Accumulo database excels at distributed storage and indexing and is ideally suited for storing graph data. Many big data analytics compute on graph data and persist their results back to the database. These graph calculations are often best performed inside the database server. The GraphBLAS standard provides a compact and efficient basis for a wide range of graph applications through a small number of sparse matrix operations. In this article, we discuss a serverside implementation of GraphBLAS sparse matrix multiplication that leverages Accumulo's native, high-performance iterators. We compare the mathematics and performance of inner and outer product implementations, and show how an outer product implementation achieves optimal performance near Accumulo's peak write rate. We offer our work as a core component to the Graphulo library that will deliver matrix math primitives for graph analytics within Accumulo.\n\nI. INTRODUCTION\n\nThe Apache Accumulo NoSQL database was designed for high performance ingest and scans [1]. While fast ingest and scans solve some big data problems, more complex scenarios involve performing tasks such as data enrichment, graph algorithms and clustering analytics. These techniques often require moving data from a database to a compute node. The ability to compute directly in a database can lead to benefits including data locality, infrastructure reuse and selective access.\n\nAccumulo administrators commonly create data locality by running server processes on the physical nodes where data is stored and cached. Computing within Accumulo takes advantage of this locality by avoiding unnecessary network transfer, effectively moving \"compute to data\" like a stored procedure, in contrast to client-server models that move \"data to compute\". Performing computation inside Accumulo also reuses its distributed infrastructure such as write-ahead logging, faulttolerant execution, and parallel load balancing of data. In particular, Accumulo's infrastructure enables selective access to data along its indexed attributes (rows), which enhances the performance of algorithms written with row access patterns.\n\nThere are a variety of ways to store graphs in Accumulo. One common schema is to store graphs as sparse matrices. Researchers in the GraphBLAS forum [2] have identified a small set of kernels that form a basis for matrix algorithms Dylan Hutchison is the corresponding author, reachable at dhutchis@uw.edu. *This material is based upon work supported by the National Science Foundation under Grant No. DMS-1312831. Opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. useful for graphs when represented as sparse matrices. This article presents Graphulo, an effort to realize the GraphBLAS primitives that enable algorithms using matrix mathematics directly in Accumulo servers [3].\n\nIn this paper we focus on Sparse Generalized Matrix Multiply (SpGEMM), the core kernel at the heart of GraphBLAS. Many GraphBLAS primitives can be expressed in terms of SpGEMM via user-defined multiplication and addition functions. SpGEMM can be used to implement a wide range of algorithms from graph search [4] to table joins [5] and many others (see introduction of [6]).\n\nWe call our implementation of SpGEMM in Accumulo TABLEMULT, short for multiplication of Accumulo tables. Accumulo tables have many similarities to sparse matrices, though a more precise mathematical definition is Associative Arrays [7]. For this work, we concentrate on large distributed tables that may not fit in memory and use a streaming approach that leverages Accumulo's built-in distributed infrastructure.\n\nWe are particularly interested in Graphulo for queued analytics [8], that is, analytics on selected table subsets. Queued analytics maximally leverage databases by quickly accessing subsets of interest, whereas whole-table analytics may perform better on parallel file systems such as Lustre or Hadoop. We therefore prioritize smaller problems that require low latency to enable analysts to explore graph data interactively.\n\nWe review Accumulo and its model for server-side computation, iterator stacks, in Section I-A. We define matrix multiplication and compare inner and outer product methods in Section II-A, settling on outer product for implementing Table-Mult. We show TableMult's design as Accumulo iterators in Section II-B and test TableMult's scalability with experiments in Section III. We discuss related work, design alternatives and optimizations in Section IV and conclude in Section V.\n\n\nA. Primer: Accumulo and its Iterator Stack\n\nAccumulo stores data in Hadoop RFiles as byte arrays indexed by key using (key, value) pairs called entries. Keys decompose further into 5-tuples consisting of a row, column family, column qualifier, visibility and timestamp. For simplicity, we focus on a 2-tuple key consisting of a row and column qualifier. Entries belong to tables, which Accumulo divides into tablets and assigns to tablet servers. Client applications write new entries via BatchWriters and retrieve entries sequentially via Scanners or in parallel via BatchScanners.\n\nAccumulo's server-side programming model runs an iterator stack on tablets in range of a scan. An iterator stack is a set of data streams originating at Accumulo's data sources for a specific tablet (Hadoop RFiles and cached in-memory maps), converging together in merge-sorts, flowing through each iterator in the stack and at the end, sending entries to the client. Iterators themselves are Java classes implementing the SortedKeyValueIterator (SKVI) interface.\n\nDevelopers add custom logic for server-side computation by writing new iterators and plugging them into the iterator stack. In return for fitting their computation in the SKVI paradigm, developers gain distributed parallelism for free as Accumulo runs their iterators on relevant tablets simultaneously.\n\nSKVIs are reminiscent of built-in Java iterators in that they hold state and emit one entry at a time until finished iterating. However, they are more powerful than Java iterators in that they can seek to arbitrary positions in the data stream. They also have two constraints: the end of the iterator stack should emit entries in sorted order, and iterators must not maintain volatile state such as threads, open files or sockets because Accumulo may destroy, re-create and re-seek an iterator stack between function calls without allowing time to clean up.\n\nIterators are most commonly used for \"reduction\" operations that transform or eliminate entries passing through. The Accumulo community generally discourages \"generator\" iterators that emit new entries not present in data sources because they are easy to misuse and violate SKVI constraints by emitting entries out of order or relying on volatile state. In this work, we suggest a new pattern for iterator usage as a conduit for client write operations that achieves the benefits of generator iterators while avoiding their constraints.\n\n\nII. TABLEMULT DESIGN A. Matrix Multiplication\n\nGiven matrices A of size N \u00d7 M , B of size M \u00d7 L, and operations \u2295 and \u2297 for element-wise addition and multiplication, the matrix product C = A \u2295.\u2297 B, or more shortly C = AB, defines entries of result matrix C as\nC(i, j) = M k=1 A(i, k) \u2297 B(k, j)\nWe call intermediary results of \u2297 operations partial products.\n\nFor the sake of sparse matrices, we only perform \u2295 and \u2297 when both operands are nonzero, an optimization stemming from requiring that 0 is an additive identity such that a \u2295 0 = 0 \u2295 a = a, and that 0 is a multiplicative annihilator such that a \u2297 0 = 0 \u2297 a = 0. Without these conditions, zero operands could generate nonzero results that destroy sparsity.\n\nWe study two well known patterns for computing matrix multiplication: inner product and outer product [9]. They differ in the order in which they perform the \u2297 and \u2295 operations. The more common inner product approach runs the following:\nfor i = 1 : N for j = 1 : L C(i, j) \u2295= A(i, :)B(:, j)\nperforming inner product on vectors. For easier comparison, we rewrite the above approach with summation deferred as:\nfor i = 1 : N for j = 1 : L for k = 1 : M C(i, j) \u2295= A(i, k) \u2297 B(k, j)\nInner product has the advantage of generating entries in sorted order: the third-level loop generates all partial products needed to compute a particular element C(i, j) consecutively. The \u2295 applies immediately after each third-level loop to obtain an element in C. Inner product is therefore easy to \"pre-sum,\" an Accumulo term for applying a Combiner locally before sending entries to a remote but globally-aware table Combiner. Emitting sorted entries also facilitates inner product use in standard iterator stacks and easier operation pipelining.\n\nDespite inner product's order-preserving advantages, outer product performs better for sparse matrices because it passes through A and B only once [10] [11]. Inner product's secondlevel loop repeats a scan over all of B for each row of A. Under our assumption that we cannot fit B entirely in memory, multiple passes over B translate to multiple Accumulo scans that each require a disk read. We found in performance tests that an outer product approach performs an order of magnitude better than an inner product approach.\n\nThe outer product approach runs the following:\nfor k = 1 : M C \u2295= A(:, k)B(k, :)\nperforming outer product on vectors that corresponds to many elements of C. Unfolding outer product reveals them as:\nfor k = 1 : M for i = 1 : N for j = 1 : L C(i, j) \u2295= A(i, k) \u2297 B(k, j)\nCompared to inner product, outer product moves the k loop above the i and j loops that determine position in C. The switch results in generating partial products out of order.\n\nOn the other hand, outer product only requires a single pass over both input matrices. This is because the top-level k loop fixes a dimension of both A and B. Once we finish processing a column of A and row of B, we never need read them again.\n\nIn terms of memory usage, outer product works best when either the matching row or column fits in memory. If neither fits, we could run the algorithm with a \"no memory assumption\" streaming approach by re-reading B's rows while streaming through A's columns (or vice versa by symmetry of i and j), perhaps at the cost of extra disk reads.\n\nBecause k runs along A's second dimension and Accumulo uses row-oriented data layouts, we implement TableMult to operate on A's transpose A .\n\n\nB. TableMult Iterators\n\nTableMult uses three iterators placed on a BatchScan of table B: RemoteSourceIterator, TwoTableIterator and Re-moteWriteIterator. A BatchScanner directs Accumulo to run the iterators on B's tablets in parallel.\n\nThe key idea behind the TableMult iterators is that they divert normal dataflow by opening a BatchWriter, redirecting entries out-of-band to C via Accumulo's unsorted ingest channel. The scan itself emits no entries except for a small number of \"monitoring entries\" that inform the client about TableMult progress. We permit multi-table iterator dataflow by opening Scanners that read remote Accumulo tables outof-band. Scanners and BatchWriters are standard tools for Accumulo clients; by creating them inside iterators, we enable client-side processing patterns within tablet servers.\n\nUnderlying our use of iterators, Scanners and BatchWriters are Accumulo's standing thread pools. Thread pools fulfill our low latency requirement by executing upon receiving a request at no more expense than a context switch. Scaling up may require tuning thread pool size to balance thread contention.\n\nWe illustrate TableMult's data flow in Figure 1, placing a Scanner on table A and a BatchWriter on result table C. We also use iterator options to specify row and column subsets, encoding them in a string format similar to that in D4M [12]. Row subsets are straightforward since Accumulo uses row-oriented indexing. Column subsets can be implemented with filter iterators but do not improve performance since Accumulo must read every column from disk. We encourage users to maintain a transpose table using strategies similar to the D4M Schema [13] for cases requiring column indexing.\n\nMultiplying table subsets is crucial for queued analytics on selected rows. However for simpler performance evaluation, our experiments in Section III multiply whole tables.\n\n2) TwoTableIterator: TwoTableIterator reads from two iterator sources, one for A and one for B, and performs the core operations of the outer product algorithm in three phases: 1) Align Rows. Read entries from A and B until they advance to a matching row or one runs out of entries. We skip non-matching rows since they would multiply with an all-zero row that, by Section II-A's assumptions, generate all zero partial products. 2) Cartesian product. Read both matching rows into an inmemory data structure. Initialize an iterator that emits pairs of entries from the rows' Cartesian product. 3) Multiply. Pass pairs of entries to \u2297 and emit results.\n\nA client defines \u2297 by specifying a class that implements a multiply interface. For our experiments we implement \u2297 as java.math.BigDecimal multiplication, which guarantees correctness under large or precise real numbers. BigDecimal decoding did not noticeably impact performance.\n\n3) RemoteWriteIterator: RemoteWriteIterator writes entries to a remote Accumulo table using a BatchWriter. Entries do not have to be in sorted order because Accumulo sorts incoming entries as part of its ingest process.\n\nBarring extreme events such as exceptions in the iterator stack or thread death, we designed RemoteWriteIterator to maintain correctness, such that entries generated from its source write to the remote table once. We accomplish this by performing all BatchWriter operations within a single function call before ceding thread control back to the tablet server.\n\nA performance concern remains when multiplying a subset of the input tables' rows that consists of many disjoint ranges, such as one million \"singleton\" ranges spanning one row each. It is inefficient to flush the BatchWriter before returning from each seek call, which happens once per disjoint scan range. We accommodate this case by \"transferring seek control\" from the tablet server to RemoteWriteIterator via the same strategy used in RemoteSourceIterator for seeking within an iterator.\n\nWe include an option to BatchWrite C's transpose C in place of or alongside C. Writing C facilitates chaining TableMults together and maintenance of transpose indexing. 4) Lazy \u2295: We lazily sum partial products by placing a Combiner subclass implementing BigDecimal addition on table C at scan, minor and major compaction scopes. Thus, \u2295 occurs sometime after RemoteWriteIterator writes partial products to C yet necessarily before entries from C may be seen such that we always achieve correctness. Summation could happen when Accumulo flushes C's entries cached in memory to a new RFile, when Accumulo compacts RFiles together, or when a client scans C.\n\nThe key algebraic requirement for implementing \u2295 inside a Combiner is that \u2295 must be associative and commutative. These properties allow us to apply \u2295 to subsets of a result element's partial products and to any ordering of them, which is chaotic by outer product's nature. If we truly had an \u2295 operation that required seeing all partial products at once, we would have to either gather partial products at the client or initiate a full major compaction. 5) Monitoring: RemoteWriteIterator never emits entries to the client by default. One downside of this approach is that clients cannot precisely track progress of a TableMult operation, which may frustrate users expecting a more interactive computing experience. Clients could query the Accumulo monitor for read/write rates or prematurely scan partial products written to C, but both approaches are too coarse.\n\nWe therefore implement a monitoring option that emits a value containing the number of entries TwoTableIterator processed at a client-set frequency. RemoteWriteIterator emits monitoring entries at \"safe\" points, that is, points at which we can recover the iterator stack's state if Accumulo destroys, recreates and re-seeks it. Stopping after emitting the last value in the outer product of two rows is safe because we place the last value's row key in the monitoring key and know, in the event of an iterator stack rebuild, to proceed to the next matching row. We may succeed in stopping during an outer product by encoding more information in the monitoring key.\n\n\nIII. PERFORMANCE\n\nWe evaluate TableMult with two variants of an experiment. First we measure the rate of computation as problem size increases. We define problem size as number of rows in random input graphs represented as adjacency tables and rate of computation as number of partial products processed per second. Second we repeat the experiment for a fixed size problem with all tables split into two tablets, allowing Accumulo to scan and write to them in parallel.\n\nWe compare Graphulo TableMult performance to D4M [12] as a baseline because a user with one client machine's best alternative is reading input graphs from Accumulo, multiplying them at the client, and inserting the result back into Accumulo.\n\nD4M stores tables as Associative Array objects in MATLAB. Because Assoc Array multiplication runs fast by calling MAT-LAB's in-memory sparse matrix functions, D4M bottlenecks on reading data from Accumulo and especially on writing back results, despite its capacity for high speed Accumulo reads and writes [14]. We consequently expect TableMult to outperform D4M because TableMult avoids transferring data out of Accumulo for processing.\n\nWe also expect TableMult to succeed on larger graph sizes than D4M because TableMult uses a streaming outer product algorithm that does not store input tables in memory. An alternative D4M implementation would mirror TableMult's streaming outer product algorithm, enabling D4M to run on larger problem sizes at potentially worse performance. We therefore imagine the whole-table D4M algorithm as an upper bound on the best performance achievable when multiplying Accumulo tables outside Accumulo's infrastructure.\n\nWe use the Graph500 unpermuted power law graph generator [15] to create random input tables, such that both tables' first row have high degree (number of columns) and subsequent rows exponentially decrease in degree. The common power law structure correlates the input tables, which leads to denser result tables than if we were to permute the input tables but does not otherwise affect multiplication behavior. The generator takes SCALE and EdgesPerVertex parameters, creating graphs with 2 SCALE rows and EdgesPerVertex \u00d7 2 SCALE entries. We fix EdgesPerVertex to 16 and use SCALE to vary problem size.\n\nThe following procedure outlines our performance experiment for a given SCALE and either one or two tablets. 1) Generate two graphs with different random seeds and insert them into Accumulo as adjacency tables via D4M. 2) In the case of two tablets, identify an optimal split point for each input graph and set the input graphs' table splits equal to that point. \"Optimal\" here means a split point that evenly divides an input graph into two tablets. 3) Create an empty output table. For two tablets, pre-split it with an optimal input split position recorded from a previous multiplication run. 4) Compact the input and output tables so that Accumulo redistributes the tables' entries into the assigned tablets. Array with the second. d) Convert the result Associative Array back to String values and insert them into Accumulo. We conducted the experiments on a Ubuntu Linux laptop with 16GB RAM and two dual-core Intel i7 processors. Using single-instance Accumulo 1.6.1, Hadoop 2.6.0 and ZooKeeper 3.4.6, we allocated 2GB of memory to an Accumulo tablet server initially (allowing growth in 500MB steps), 1GB for native in-memory maps and 256MB for data and index cache.\n\nWe chose not to use more than two tablets per table because more threads would run than the laptop could handle. Each additional tablet can potentially add the following threads: 1)  Table I and plot them in Figure 2. We could not run the D4M  Table Size comparison past SCALE 15 because C does not fit in memory.\n\nFor the scaled problem, the best results we could achieve are flat horizontal lines, indicating that we maintain the same level of operations per second as problem size increases.\n\nOne reason we see a downward rate trend at larger problem sizes is that Accumulo needs to minor compact table C in the middle of a TableMult. The compactions trigger flushes to disk along with the \u2295 Combiner that sums partial products written to C so far, neither of which we include in rate measurements.\n\nFor the fixed size problem, the best results we could achieve are two-tablet rates at double the one-tablet rates at every problem size. Our experiment shows that Graphulo two-tablet rates perform up to 1.5x better than one-tablet rates at lower SCALEs. We attribute TableMult's shortfall to high processor contention for the laptop's four cores as a result of the 14 threads that may run concurrently when each table has two tablets; in fact, processor usage hovered near 100% for all four cores throughout the two-tablet experiments. We expect better scaling when running our experiment in larger Accumulo clusters that can handle more degrees of parallelism.\n\n\nIV. DISCUSSION\n\n\nA. Related Work\n\nBulu\u00e7 and Gilbert studied message passing algorithms for SpGEMM such as Sparse SUMMA, most of which use 2D block decompositions [16]. Unfortunately, 2D decompositions are difficult in Accumulo and message passing even more so. In this work, we use Accumulo's native 1D decomposition along rows and do not rely on tablet server communication apart from shuffling partial products of C via BatchWriters.\n\nOur outer product method could have been implemented on Hadoop MapReduce or its successor YARN [17]. There is a natural analogy from TableMult to MapReduce: the map phase scans rows from A and B and generates a list of partial products from TwoTableIterator; the shuffle phase sends partial products to correct tablets of C via BatchWriters; and the reduce phase sums partial products using Combiners. Examining the conditions on which MapReduce reading from and writing to Accumulo's RFiles directly can outperform Accumulo-only solutions is worthy future work.\n\nA common Accumulo pattern is to scan and write from multiple clients in parallel; in fact, researchers obtained considerably high insert rates using parallel client strategies [14]. We chose to build Graphulo as a service within Accumulo instead of assuming a multiple client capability, such that Graphulo is as accessible as possible to diverse client environments.\n\nThe strategy in [14] also used tablet location information to determine where clients could write locally. Knowing tabletto-tablet-server assignment could likewise aid Graphulo, not only to minimize network traffic but also to partly eliminate Apache Thrift RPC serialization, which prior work has shown is a bottleneck for scans when iterator processing is light [18]. Such an enhancement would access a local tablet server by method call in place of Scanners and BatchWriters.\n\nThe Knowledge Discovery Toolkit (KDT) distributedmemory Python graph library offers sparse matrix multiplication in a similar design as Graphulo's [19]. Both support custom addition, multiplication and filter operators written in a high level language. They differ in that Graphulo targets the Accumulo infrastructure which is IO-bound, in contrast to the KDT which is compute-bound. Graphulo therefore gains less from code generation techniques on its Java iterator kernels, whereas the KDT uses the SEJITS technique [20] to translate Python kernels into C++ for callback by KDT's underlying Combinatorial BLAS library [21], thereby raising performance from compute-to memory bandwidth-bound at the expense of restricting operator expressiveness to a DSL.\n\n\nB. Design Alternative: Inner-Outer Product Hybrid\n\nIt is worth reconsidering the inner product method from our initial design because it has an opposite performance profile as Figure 3's left and right depict: inner product bottlenecks on scanning whereas outer product bottlenecks on writing. At the expense of multiple passes over input matrices, inner product emits partial products in order and immediately pre-summable, reducing the number of entries written to Accumulo to the minimum possible. Outer product reads inputs in a single pass but emits entries out of order and has little chance to presum, instead writing individual partial products to C. Table I quantifies that outer product writes 2.5 to 3 times more entries than inner product for power law inputs. In the worst case, multiplying a fully dense N \u00d7 M with an M \u00d7 L matrix, outer product emits M times more entries than inner product.\n\nIs it possible to blend inner and outer product SpGEMM methods, choosing a middle point in Figure 3 with equal read and write bottlenecks for overall greater performance? In the following generalization, partition parameter P varies behavior 1.28 \u00d7 10 9 1.21 \u00d7 10 4 2.42 \u00d7 10 5 1.14 \u00d7 10 4 2.58 \u00d7 10 5\n\nbetween inner product at P = N and outer product at P = 1:\nfor p = 1 : P for k = 1 : M for i = (p \u2212 1)N P + 1 : pN P for j = 1 : L C(i, j) \u2295= A(i, k) \u2297 B(k, j)\nThe hybrid algorithm runs P passes through B, each of which has write locality to a vertical partition of C of size N/P \u00d7 L. Pre-summing ability likewise varies inversely with P , though actual pre-summing depends on A and B's sparsity distribution as well as how many positions of C the TableMult iterators cache. Figure 3's center depicts the P = 2 case.\n\nA challenge for any hybrid algorithm is mapping it to Accumulo infrastructure. We chose outer product because it more naturally fits Accumulo, using iterators for one-pass streaming computation, BatchWriters to handle unsorted entry emission and Combiners to defer summation. The above hybrid algorithm resembles 2D block decompositions, and so maximizing its performance may be challenging given limited data layout control and unknown data distribution.\n\nNevertheless, possible design criteria are to select a small P to minimize passes through B, while also choosing P large enough so that N L/P entries fit in memory (dense matrix worst case), which guarantees complete pre-summing. The latter criterion may be relaxed with decreasing matrix density.\n\n\nC. TableMult in Algorithms\n\nSeveral optimization opportunities exist for TableMult as a primitive in larger algorithms. Given row A of starting vertices and graph adjacency matrix B, suppose we wish to union the vertices reached in two steps from those in A into A via the program C = AB; D = CB; A \u2295= D, as one way of calculating A \u2295= AB 2 via TableMult calls. Such calculations Fig. 3: Tradeoffs between Inner and Outer Product are useful for finding vertices reachable in an even number of steps. We would save two round trips to disk if we could mark C and D as \"temporary tables,\" i.e. tables intermediate to an algorithm that should be held in memory and not written to Hadoop if possible. Combiners in TableMult do enable one optimization: summing CB into A directly by rewriting the program as C = AB; A \u2295= CB.\n\nA pipelining optimization streams entries from a TableMult to computations taking its result as input. Outer product pipelining is difficult because it cannot guarantee writing every partial product for a particular element to C until it finishes, whereas inner product's complete pre-summing emits elements ready for use downstream. More ambitiously, loop fusion merges iterator stacks for successive computations into one.\n\nOptimizing computation on NoSQL databases is challenging in the general case because NoSQL databases typically avoid query planner features customary of SQL databases in exchange for raw performance. NewSQL databases aim in part to achieve the best of both worlds-performance and query planning [22]. We aspire to make a small step for Accumulo in the direction of NewSQL with current Graphulo research.\n\n\nV. CONCLUSIONS\n\nIn this work we showcase the design of TableMult, a Graphulo server-side implementation of the SpGEMM Graph-BLAS matrix math kernel in the Accumulo database. We compare inner and outer product approaches and show how outer product better fits Accumulo's iterator model. The implementation shows excellent single node performance, achieving write rates near 400,000 per second, which is consistent with the single node peak write rate for Accumulo [14]. Performance experiments show good scaling for scaled problem sizes and suggest good scaling for fixed size problems, but these require additional experiments on a larger cluster to confirm.\n\nIn addition to topics from Section IV's discussion, future research efforts include implementing the remaining Graph-BLAS kernels, developing graph algorithms that use the Graphulo library and delivering to the Accumulo community.\n\nFig. 1 :\n1Data flow through the TableMult iterator stack 1) RemoteSourceIterator: RemoteSourceIterator scans an Accumulo table (not necessarily in the same cluster) using credentials passed from the client through iterator options.\n\n5 )\n5Run and time Graphulo TableMult multiplying the transpose of the first input table with the second. 6) Create, pre-split and compact a new result table for the D4M comparison as in step 3 and 4. 7) Run and time the D4M equivalent of TableMult: a) Scan both input tables into D4M Associative Array objects in MATLAB memory. b) Convert the string values from Accumulo into numeric values for each Associative Array. c) Multiply the transpose of the first Associative\n\nFig. 2 :\n2TableMult Processing Rate vs. Input\n\nTable A server\nATable C server-side minor compaction threads, running with a Combiner implementing \u2295. We show table C sizes and experiment timings in-side scan thread; \n2) Table A client-side scan thread, \nrunning from RemoteSourceIterator; \n3) Table B server-side scan/multiply thread, \nrunning a TableMult iterator stack; \n4) Table B client-side scan thread, \nrunning from the initiating client, mostly idle; \n5) Table C server-side write thread; \n6) Table C client-side write thread, \nrunning from RemoteWriteIterator; and \n7) \n\nTABLE I :\nIOutput Table C Sizes and Experiment Timings S C A L E 12 6.82 \u00d7 10 6 2.43 \u00d7 10 6 2.20 \u00d7 10 1 3.10 \u00d7 10 5 2.66 \u00d7 10 1 2.56 \u00d7 10 5 1.63 \u00d7 10 1 4.18 \u00d7 10 5 2.62 \u00d7 10 1 2.60 \u00d7 10 5 13 1.91 \u00d7 10 7 7.04 \u00d7 10 6 6.40 \u00d7 10 1 2.99 \u00d7 10 5 1.50 \u00d7 10 2 1.27 \u00d7 10 5 4.86 \u00d7 10 1 3.93 \u00d7 10 5 1.44 \u00d7 10 2 1.33 \u00d7 10 5 14 5.27 \u00d7 10 7 2.00 \u00d7 10 7 1.82 \u00d7 10 2 2.90 \u00d7 10 5 5.79 \u00d7 10 2 9.09 \u00d7 10 4 1.36 \u00d7 10 2 3.87 \u00d7 10 5 5.59 \u00d7 10 2 9.42 \u00d7 10 4 15 1.47 \u00d7 10 8 5.83 \u00d7 10 7 5.03 \u00d7 10 2 2.93 \u00d7 10 5 2.51 \u00d7 10 3 5.86 \u00d7 10 4 3.94 \u00d7 10 2 3.74 \u00d7 10 5 2.56 \u00d7 10 3 5.75 \u00d7 10 4 16 4.00 \u00d7 10 8 1.63 \u00d7 10 8 1.39 \u00d7 10 3 2.88 \u00d7 10 5 1.18 \u00d7 10 3 3.40 \u00d7 10 5 17 1.09 \u00d7 10 9 4.59 \u00d7 10 8 4.06 \u00d7 10 3 2.67 \u00d7 10 5 3.70 \u00d7 10 3 2.94 \u00d7 10 5 18 2.94 \u00d7 10 9Entries in Table C \nGraphulo 1 Tablet \nD4M 1 Tablet \nGraphulo 2 Tablets \nD4M 2 Tablets \nPartialProducts AfterSum \nTime (s) \nRate (pp/s) \nTime (s) \nRate (pp/s) \nTime (s) \nRate (pp/s) \nTime (s) \nRate (pp/s) \n10 8.05 \u00d7 10 5 \n2.69 \u00d7 10 5 2.87 \n2.81 \u00d7 10 5 3.02 \n2.67 \u00d7 10 5 2.02 \n3.98 \u00d7 10 5 2.80 \n2.87 \u00d7 10 5 \n11 2.36 \u00d7 10 6 \n8.15 \u00d7 10 5 7.76 \n3.04 \u00d7 10 5 8.80 \n2.68 \u00d7 10 5 5.19 \n4.55 \u00d7 10 5 8.72 \n2.71 \u00d7 10 5 \n\n\nBenchmarking apache accumulo bigdata distributed table store using its continuous test suite,\" in International Congress on Big Data. R Sen, A Farris, P Guerra, IEEER. Sen, A. Farris, and P. Guerra, \"Benchmarking apache accumulo bigdata distributed table store using its continuous test suite,\" in In- ternational Congress on Big Data. IEEE, 2013, pp. 334-341.\n\nStandards for graph algorithm primitives. T Mattson, D Bader, J Berry, A Buluc, J Dongarra, C Faloutsos, J Feo, J Gilbert, J Gonzalez, B Hendrickson, High Performance Extreme Computing Conference (HPEC). IEEET. Mattson, D. Bader, J. Berry, A. Buluc, J. Dongarra, C. Faloutsos, J. Feo, J. Gilbert, J. Gonzalez, B. Hendrickson et al., \"Standards for graph algorithm primitives,\" in High Performance Extreme Computing Conference (HPEC). IEEE, 2013.\n\nGraphulo: Linear algebra graph kernels for NoSQL databases. V Gadepally, J Bolewski, D Hook, D Hutchison, B Miller, J Kepner, International Parallel & Distributed Processing Symposium Workshops (IPDPSW). IEEEV. Gadepally, J. Bolewski, D. Hook, D. Hutchison, B. Miller, and J. Kep- ner, \"Graphulo: Linear algebra graph kernels for NoSQL databases,\" in International Parallel & Distributed Processing Symposium Workshops (IPDPSW). IEEE, 2015.\n\nGraph algorithms in the language of linear algebra. J Kepner, J Gilbert, SIAMJ. Kepner and J. Gilbert, Graph algorithms in the language of linear algebra. SIAM, 2011.\n\nMad skills: new analysis practices for big data. J Cohen, B Dolan, M Dunlap, J M Hellerstein, C Welton, Proceedings of the VLDB Endowment. the VLDB Endowment2J. Cohen, B. Dolan, M. Dunlap, J. M. Hellerstein, and C. Welton, \"Mad skills: new analysis practices for big data,\" Proceedings of the VLDB Endowment, vol. 2, no. 2, pp. 1481-1492, 2009.\n\nHighly parallel sparse matrix-matrix multiplication. A Bulu\u00e7, J R Gilbert, A. Bulu\u00e7 and J. R. Gilbert, \"Highly parallel sparse matrix-matrix multiplication,\" 2010.\n\nAdjacency matrices, incidence matrices, database schemas, and associative arrays. J Kepner, V Gadepally, International Parallel & Distributed Processing Symposium Workshops (IPDPSW). IEEEJ. Kepner and V. Gadepally, \"Adjacency matrices, incidence matrices, database schemas, and associative arrays,\" in International Parallel & Distributed Processing Symposium Workshops (IPDPSW). IEEE, 2014.\n\nCloud computing where ISR data will go for exploitation. A Reuther, J Kepner, P Michaleas, W Smith, High Performance Extreme Computing Conference (HPEC. IEEEA. Reuther, J. Kepner, P. Michaleas, and W. Smith, \"Cloud computing where ISR data will go for exploitation,\" in High Performance Extreme Computing Conference (HPEC). IEEE, 2009.\n\nTechniques for parallel manipulation of sparse matrices. C P Kruskal, L Rudolph, M Snir, Theoretical Computer Science. 642C. P. Kruskal, L. Rudolph, and M. Snir, \"Techniques for parallel manipulation of sparse matrices,\" Theoretical Computer Science, vol. 64, no. 2, pp. 135-157, 1989.\n\nAn NSA big graph experiment. P Burkhardt, C Waring, NSA-RD-2013-056001v1US National Security AgencyTechnical reportP. Burkhardt and C. Waring, \"An NSA big graph experiment,\" US National Security Agency Technical report NSA-RD-2013-056001v1, 2013.\n\nAsking hard graph questions. P Burkhardt, NSA-RD-2014-050001v1US National Security AgencyTechnical reportP. Burkhardt, \"Asking hard graph questions,\" US National Security Agency Technical report NSA-RD-2014-050001v1, 2014.\n\nDynamic distributed dimensional data model (D4M) database and computation system. J Kepner, W Arcand, W Bergeron, N Bliss, R Bond, C Byun, G Condon, K Gregson, M Hubbell, J Kurz, International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEJ. Kepner, W. Arcand, W. Bergeron, N. Bliss, R. Bond, C. Byun, G. Condon, K. Gregson, M. Hubbell, J. Kurz et al., \"Dynamic distributed dimensional data model (D4M) database and computation system,\" in International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2012, pp. 5349-5352.\n\nD4M 2.0 schema: A general purpose high performance schema for the accumulo database. J Kepner, C Anderson, W Arcand, D Bestor, B Bergeron, C Byun, M Hubbell, P Michaleas, J Mullen, D O&apos;gwynn, High Performance Extreme Computing Conference (HPEC). IEEEJ. Kepner, C. Anderson, W. Arcand, D. Bestor, B. Bergeron, C. Byun, M. Hubbell, P. Michaleas, J. Mullen, D. O'Gwynn et al., \"D4M 2.0 schema: A general purpose high performance schema for the accu- mulo database,\" in High Performance Extreme Computing Conference (HPEC). IEEE, 2013.\n\nAchieving 100,000,000 database inserts per second using accumulo and D4M. J Kepner, W Arcand, D Bestor, B Bergeron, C Byun, V Gadepally, M Hubbell, P Michaleas, J Mullen, A Prout, High Performance Extreme Computing Conference (HPEC). J. Kepner, W. Arcand, D. Bestor, B. Bergeron, C. Byun, V. Gadepally, M. Hubbell, P. Michaleas, J. Mullen, A. Prout et al., \"Achieving 100,000,000 database inserts per second using accumulo and D4M,\" High Performance Extreme Computing Conference (HPEC), 2014.\n\nDesigning scalable synthetic compact applications for benchmarking high productivity computing systems. D Bader, K Madduri, J Gilbert, V Shah, J Kepner, T Meuse, A Krishnamurthy, Cyberinfrastructure Technology Watch. 2D. Bader, K. Madduri, J. Gilbert, V. Shah, J. Kepner, T. Meuse, and A. Krishnamurthy, \"Designing scalable synthetic compact applications for benchmarking high productivity computing systems,\" Cyberinfras- tructure Technology Watch, vol. 2, pp. 1-10, 2006.\n\nParallel sparse matrix-matrix multiplication and indexing: Implementation and experiments. A Buluc, J R Gilbert, SIAM Journal on Scientific Computing. 344A. Buluc and J. R. Gilbert, \"Parallel sparse matrix-matrix multiplica- tion and indexing: Implementation and experiments,\" SIAM Journal on Scientific Computing, vol. 34, no. 4, pp. C170-C191, 2012.\n\nApache hadoop YARN: Yet another resource negotiator. V K Vavilapalli, A C Murthy, C Douglas, S Agarwal, M Konar, R Evans, T Graves, J Lowe, H Shah, S Seth, Proceedings of the 4th annual Symposium on Cloud Computing. the 4th annual Symposium on Cloud ComputingACMV. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth et al., \"Apache hadoop YARN: Yet another resource negotiator,\" in Proceedings of the 4th annual Symposium on Cloud Computing. ACM, 2013.\n\nUnderstanding query performance in accumulo. S M Sawyer, B D O&apos;gwynn, A Tran, T Yu, High Performance Extreme Computing Conference (HPEC). IEEES. M. Sawyer, B. D. O'Gwynn, A. Tran, and T. Yu, \"Understanding query performance in accumulo,\" in High Performance Extreme Computing Conference (HPEC). IEEE, 2013.\n\nHigh-productivity and high-performance analysis of filtered semantic graphs. A Bulu\u00e7, E Duriakova, A Fox, J R Gilbert, S Kamil, A Lugowski, L Oliker, S Williams, International Symposium on Parallel & Distributed Processing (IPDPS). IEEEA. Bulu\u00e7, E. Duriakova, A. Fox, J. R. Gilbert, S. Kamil, A. Lugowski, L. Oliker, and S. Williams, \"High-productivity and high-performance analysis of filtered semantic graphs,\" in International Symposium on Parallel & Distributed Processing (IPDPS). IEEE, 2013, pp. 237-248.\n\nSEJITS: Getting productivity and performance with selective embedded jit specialization. B Catanzaro, S Kamil, Y Lee, K Asanovic, J Demmel, K Keutzer, J Shalf, K Yelick, A Fox, Programming Models for Emerging Architectures. 11B. Catanzaro, S. Kamil, Y. Lee, K. Asanovic, J. Demmel, K. Keutzer, J. Shalf, K. Yelick, and A. Fox, \"SEJITS: Getting productivity and performance with selective embedded jit specialization,\" Programming Models for Emerging Architectures, vol. 1, no. 1, pp. 1-9, 2009.\n\nThe combinatorial BLAS: Design, implementation, and applications. A Bulu\u00e7, J R Gilbert, International Journal of High Performance Computing Applications. A. Bulu\u00e7 and J. R. Gilbert, \"The combinatorial BLAS: Design, imple- mentation, and applications,\" International Journal of High Performance Computing Applications, pp. 496-509, 2011.\n\nData management in cloud environments: NoSQL and NewSQL data stores. K Grolinger, W A Higashino, A Tiwari, M A Capretz, Journal of Cloud Computing: Advances, Systems and Applications. 2122K. Grolinger, W. A. Higashino, A. Tiwari, and M. A. Capretz, \"Data management in cloud environments: NoSQL and NewSQL data stores,\" Journal of Cloud Computing: Advances, Systems and Applications, vol. 2, no. 1, p. 22, 2013.\n", "annotations": {"author": "[{\"end\":106,\"start\":90},{\"end\":187,\"start\":107},{\"end\":270,\"start\":188},{\"end\":295,\"start\":271},{\"end\":348,\"start\":296}]", "publisher": null, "author_last_name": "[{\"end\":105,\"start\":96},{\"end\":120,\"start\":114},{\"end\":203,\"start\":194},{\"end\":281,\"start\":276}]", "author_first_name": "[{\"end\":95,\"start\":90},{\"end\":113,\"start\":107},{\"end\":193,\"start\":188},{\"end\":275,\"start\":271}]", "author_affiliation": "[{\"end\":186,\"start\":122},{\"end\":269,\"start\":205},{\"end\":294,\"start\":283},{\"end\":347,\"start\":297}]", "title": "[{\"end\":87,\"start\":1},{\"end\":435,\"start\":349}]", "venue": null, "abstract": "[{\"end\":1373,\"start\":437}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1481,\"start\":1478},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2752,\"start\":2749},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3414,\"start\":3411},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3729,\"start\":3726},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3748,\"start\":3745},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3789,\"start\":3786},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4028,\"start\":4025},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4275,\"start\":4272},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8385,\"start\":8382},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9468,\"start\":9464},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12378,\"start\":12374},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12687,\"start\":12683},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17624,\"start\":17620},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18125,\"start\":18121},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18830,\"start\":18826},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22183,\"start\":22179},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22553,\"start\":22549},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23198,\"start\":23194},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23407,\"start\":23403},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23755,\"start\":23751},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24018,\"start\":24014},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24389,\"start\":24385},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24491,\"start\":24487},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28657,\"start\":28653},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29231,\"start\":29227}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29887,\"start\":29655},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30358,\"start\":29888},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30405,\"start\":30359},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30937,\"start\":30406},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32068,\"start\":30938}]", "paragraph": "[{\"end\":1869,\"start\":1392},{\"end\":2598,\"start\":1871},{\"end\":3415,\"start\":2600},{\"end\":3791,\"start\":3417},{\"end\":4206,\"start\":3793},{\"end\":4632,\"start\":4208},{\"end\":5111,\"start\":4634},{\"end\":5696,\"start\":5158},{\"end\":6161,\"start\":5698},{\"end\":6466,\"start\":6163},{\"end\":7025,\"start\":6468},{\"end\":7563,\"start\":7027},{\"end\":7825,\"start\":7613},{\"end\":7922,\"start\":7860},{\"end\":8278,\"start\":7924},{\"end\":8516,\"start\":8280},{\"end\":8688,\"start\":8571},{\"end\":9310,\"start\":8760},{\"end\":9834,\"start\":9312},{\"end\":9882,\"start\":9836},{\"end\":10033,\"start\":9917},{\"end\":10280,\"start\":10105},{\"end\":10525,\"start\":10282},{\"end\":10865,\"start\":10527},{\"end\":11008,\"start\":10867},{\"end\":11245,\"start\":11035},{\"end\":11833,\"start\":11247},{\"end\":12137,\"start\":11835},{\"end\":12724,\"start\":12139},{\"end\":12899,\"start\":12726},{\"end\":13551,\"start\":12901},{\"end\":13831,\"start\":13553},{\"end\":14052,\"start\":13833},{\"end\":14413,\"start\":14054},{\"end\":14907,\"start\":14415},{\"end\":15564,\"start\":14909},{\"end\":16431,\"start\":15566},{\"end\":17097,\"start\":16433},{\"end\":17569,\"start\":17118},{\"end\":17812,\"start\":17571},{\"end\":18252,\"start\":17814},{\"end\":18767,\"start\":18254},{\"end\":19373,\"start\":18769},{\"end\":20548,\"start\":19375},{\"end\":20863,\"start\":20550},{\"end\":21044,\"start\":20865},{\"end\":21351,\"start\":21046},{\"end\":22014,\"start\":21353},{\"end\":22452,\"start\":22051},{\"end\":23016,\"start\":22454},{\"end\":23385,\"start\":23018},{\"end\":23865,\"start\":23387},{\"end\":24623,\"start\":23867},{\"end\":25532,\"start\":24677},{\"end\":25835,\"start\":25534},{\"end\":25895,\"start\":25837},{\"end\":26353,\"start\":25997},{\"end\":26810,\"start\":26355},{\"end\":27109,\"start\":26812},{\"end\":27930,\"start\":27140},{\"end\":28356,\"start\":27932},{\"end\":28761,\"start\":28358},{\"end\":29422,\"start\":28780},{\"end\":29654,\"start\":29424}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7859,\"start\":7826},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8570,\"start\":8517},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8759,\"start\":8689},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9916,\"start\":9883},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10104,\"start\":10034},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25996,\"start\":25896}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":4871,\"start\":4865},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20740,\"start\":20733},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20804,\"start\":20794},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25292,\"start\":25285}]", "section_header": "[{\"end\":1390,\"start\":1375},{\"end\":5156,\"start\":5114},{\"end\":7611,\"start\":7566},{\"end\":11033,\"start\":11011},{\"end\":17116,\"start\":17100},{\"end\":22031,\"start\":22017},{\"end\":22049,\"start\":22034},{\"end\":24675,\"start\":24626},{\"end\":27138,\"start\":27112},{\"end\":28778,\"start\":28764},{\"end\":29664,\"start\":29656},{\"end\":29892,\"start\":29889},{\"end\":30368,\"start\":30360},{\"end\":30421,\"start\":30407},{\"end\":30948,\"start\":30939}]", "table": "[{\"end\":30937,\"start\":30556},{\"end\":32068,\"start\":31660}]", "figure_caption": "[{\"end\":29887,\"start\":29666},{\"end\":30358,\"start\":29894},{\"end\":30405,\"start\":30370},{\"end\":30556,\"start\":30423},{\"end\":31660,\"start\":30950}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12186,\"start\":12178},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20766,\"start\":20758},{\"end\":24810,\"start\":24802},{\"end\":25633,\"start\":25625},{\"end\":26320,\"start\":26312},{\"end\":27498,\"start\":27492}]", "bib_author_first_name": "[{\"end\":32205,\"start\":32204},{\"end\":32212,\"start\":32211},{\"end\":32222,\"start\":32221},{\"end\":32475,\"start\":32474},{\"end\":32486,\"start\":32485},{\"end\":32495,\"start\":32494},{\"end\":32504,\"start\":32503},{\"end\":32513,\"start\":32512},{\"end\":32525,\"start\":32524},{\"end\":32538,\"start\":32537},{\"end\":32545,\"start\":32544},{\"end\":32556,\"start\":32555},{\"end\":32568,\"start\":32567},{\"end\":32940,\"start\":32939},{\"end\":32953,\"start\":32952},{\"end\":32965,\"start\":32964},{\"end\":32973,\"start\":32972},{\"end\":32986,\"start\":32985},{\"end\":32996,\"start\":32995},{\"end\":33374,\"start\":33373},{\"end\":33384,\"start\":33383},{\"end\":33539,\"start\":33538},{\"end\":33548,\"start\":33547},{\"end\":33557,\"start\":33556},{\"end\":33567,\"start\":33566},{\"end\":33569,\"start\":33568},{\"end\":33584,\"start\":33583},{\"end\":33889,\"start\":33888},{\"end\":33898,\"start\":33897},{\"end\":33900,\"start\":33899},{\"end\":34083,\"start\":34082},{\"end\":34093,\"start\":34092},{\"end\":34451,\"start\":34450},{\"end\":34462,\"start\":34461},{\"end\":34472,\"start\":34471},{\"end\":34485,\"start\":34484},{\"end\":34788,\"start\":34787},{\"end\":34790,\"start\":34789},{\"end\":34801,\"start\":34800},{\"end\":34812,\"start\":34811},{\"end\":35047,\"start\":35046},{\"end\":35060,\"start\":35059},{\"end\":35295,\"start\":35294},{\"end\":35572,\"start\":35571},{\"end\":35582,\"start\":35581},{\"end\":35592,\"start\":35591},{\"end\":35604,\"start\":35603},{\"end\":35613,\"start\":35612},{\"end\":35621,\"start\":35620},{\"end\":35629,\"start\":35628},{\"end\":35639,\"start\":35638},{\"end\":35650,\"start\":35649},{\"end\":35661,\"start\":35660},{\"end\":36143,\"start\":36142},{\"end\":36153,\"start\":36152},{\"end\":36165,\"start\":36164},{\"end\":36175,\"start\":36174},{\"end\":36185,\"start\":36184},{\"end\":36197,\"start\":36196},{\"end\":36205,\"start\":36204},{\"end\":36216,\"start\":36215},{\"end\":36229,\"start\":36228},{\"end\":36239,\"start\":36238},{\"end\":36670,\"start\":36669},{\"end\":36680,\"start\":36679},{\"end\":36690,\"start\":36689},{\"end\":36700,\"start\":36699},{\"end\":36712,\"start\":36711},{\"end\":36720,\"start\":36719},{\"end\":36733,\"start\":36732},{\"end\":36744,\"start\":36743},{\"end\":36757,\"start\":36756},{\"end\":36767,\"start\":36766},{\"end\":37194,\"start\":37193},{\"end\":37203,\"start\":37202},{\"end\":37214,\"start\":37213},{\"end\":37225,\"start\":37224},{\"end\":37233,\"start\":37232},{\"end\":37243,\"start\":37242},{\"end\":37252,\"start\":37251},{\"end\":37656,\"start\":37655},{\"end\":37665,\"start\":37664},{\"end\":37667,\"start\":37666},{\"end\":37971,\"start\":37970},{\"end\":37973,\"start\":37972},{\"end\":37988,\"start\":37987},{\"end\":37990,\"start\":37989},{\"end\":38000,\"start\":37999},{\"end\":38011,\"start\":38010},{\"end\":38022,\"start\":38021},{\"end\":38031,\"start\":38030},{\"end\":38040,\"start\":38039},{\"end\":38050,\"start\":38049},{\"end\":38058,\"start\":38057},{\"end\":38066,\"start\":38065},{\"end\":38477,\"start\":38476},{\"end\":38479,\"start\":38478},{\"end\":38489,\"start\":38488},{\"end\":38491,\"start\":38490},{\"end\":38507,\"start\":38506},{\"end\":38515,\"start\":38514},{\"end\":38822,\"start\":38821},{\"end\":38831,\"start\":38830},{\"end\":38844,\"start\":38843},{\"end\":38851,\"start\":38850},{\"end\":38853,\"start\":38852},{\"end\":38864,\"start\":38863},{\"end\":38873,\"start\":38872},{\"end\":38885,\"start\":38884},{\"end\":38895,\"start\":38894},{\"end\":39346,\"start\":39345},{\"end\":39359,\"start\":39358},{\"end\":39368,\"start\":39367},{\"end\":39375,\"start\":39374},{\"end\":39387,\"start\":39386},{\"end\":39397,\"start\":39396},{\"end\":39408,\"start\":39407},{\"end\":39417,\"start\":39416},{\"end\":39427,\"start\":39426},{\"end\":39819,\"start\":39818},{\"end\":39828,\"start\":39827},{\"end\":39830,\"start\":39829},{\"end\":40160,\"start\":40159},{\"end\":40173,\"start\":40172},{\"end\":40175,\"start\":40174},{\"end\":40188,\"start\":40187},{\"end\":40198,\"start\":40197},{\"end\":40200,\"start\":40199}]", "bib_author_last_name": "[{\"end\":32209,\"start\":32206},{\"end\":32219,\"start\":32213},{\"end\":32229,\"start\":32223},{\"end\":32483,\"start\":32476},{\"end\":32492,\"start\":32487},{\"end\":32501,\"start\":32496},{\"end\":32510,\"start\":32505},{\"end\":32522,\"start\":32514},{\"end\":32535,\"start\":32526},{\"end\":32542,\"start\":32539},{\"end\":32553,\"start\":32546},{\"end\":32565,\"start\":32557},{\"end\":32580,\"start\":32569},{\"end\":32950,\"start\":32941},{\"end\":32962,\"start\":32954},{\"end\":32970,\"start\":32966},{\"end\":32983,\"start\":32974},{\"end\":32993,\"start\":32987},{\"end\":33003,\"start\":32997},{\"end\":33381,\"start\":33375},{\"end\":33392,\"start\":33385},{\"end\":33545,\"start\":33540},{\"end\":33554,\"start\":33549},{\"end\":33564,\"start\":33558},{\"end\":33581,\"start\":33570},{\"end\":33591,\"start\":33585},{\"end\":33895,\"start\":33890},{\"end\":33908,\"start\":33901},{\"end\":34090,\"start\":34084},{\"end\":34103,\"start\":34094},{\"end\":34459,\"start\":34452},{\"end\":34469,\"start\":34463},{\"end\":34482,\"start\":34473},{\"end\":34491,\"start\":34486},{\"end\":34798,\"start\":34791},{\"end\":34809,\"start\":34802},{\"end\":34817,\"start\":34813},{\"end\":35057,\"start\":35048},{\"end\":35067,\"start\":35061},{\"end\":35305,\"start\":35296},{\"end\":35579,\"start\":35573},{\"end\":35589,\"start\":35583},{\"end\":35601,\"start\":35593},{\"end\":35610,\"start\":35605},{\"end\":35618,\"start\":35614},{\"end\":35626,\"start\":35622},{\"end\":35636,\"start\":35630},{\"end\":35647,\"start\":35640},{\"end\":35658,\"start\":35651},{\"end\":35666,\"start\":35662},{\"end\":36150,\"start\":36144},{\"end\":36162,\"start\":36154},{\"end\":36172,\"start\":36166},{\"end\":36182,\"start\":36176},{\"end\":36194,\"start\":36186},{\"end\":36202,\"start\":36198},{\"end\":36213,\"start\":36206},{\"end\":36226,\"start\":36217},{\"end\":36236,\"start\":36230},{\"end\":36252,\"start\":36240},{\"end\":36677,\"start\":36671},{\"end\":36687,\"start\":36681},{\"end\":36697,\"start\":36691},{\"end\":36709,\"start\":36701},{\"end\":36717,\"start\":36713},{\"end\":36730,\"start\":36721},{\"end\":36741,\"start\":36734},{\"end\":36754,\"start\":36745},{\"end\":36764,\"start\":36758},{\"end\":36773,\"start\":36768},{\"end\":37200,\"start\":37195},{\"end\":37211,\"start\":37204},{\"end\":37222,\"start\":37215},{\"end\":37230,\"start\":37226},{\"end\":37240,\"start\":37234},{\"end\":37249,\"start\":37244},{\"end\":37266,\"start\":37253},{\"end\":37662,\"start\":37657},{\"end\":37675,\"start\":37668},{\"end\":37985,\"start\":37974},{\"end\":37997,\"start\":37991},{\"end\":38008,\"start\":38001},{\"end\":38019,\"start\":38012},{\"end\":38028,\"start\":38023},{\"end\":38037,\"start\":38032},{\"end\":38047,\"start\":38041},{\"end\":38055,\"start\":38051},{\"end\":38063,\"start\":38059},{\"end\":38071,\"start\":38067},{\"end\":38486,\"start\":38480},{\"end\":38504,\"start\":38492},{\"end\":38512,\"start\":38508},{\"end\":38518,\"start\":38516},{\"end\":38828,\"start\":38823},{\"end\":38841,\"start\":38832},{\"end\":38848,\"start\":38845},{\"end\":38861,\"start\":38854},{\"end\":38870,\"start\":38865},{\"end\":38882,\"start\":38874},{\"end\":38892,\"start\":38886},{\"end\":38904,\"start\":38896},{\"end\":39356,\"start\":39347},{\"end\":39365,\"start\":39360},{\"end\":39372,\"start\":39369},{\"end\":39384,\"start\":39376},{\"end\":39394,\"start\":39388},{\"end\":39405,\"start\":39398},{\"end\":39414,\"start\":39409},{\"end\":39424,\"start\":39418},{\"end\":39431,\"start\":39428},{\"end\":39825,\"start\":39820},{\"end\":39838,\"start\":39831},{\"end\":40170,\"start\":40161},{\"end\":40185,\"start\":40176},{\"end\":40195,\"start\":40189},{\"end\":40208,\"start\":40201}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32430,\"start\":32070},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12099965},\"end\":32877,\"start\":32432},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3616210},\"end\":33319,\"start\":32879},{\"attributes\":{\"id\":\"b3\"},\"end\":33487,\"start\":33321},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1040148},\"end\":33833,\"start\":33489},{\"attributes\":{\"id\":\"b5\"},\"end\":33998,\"start\":33835},{\"attributes\":{\"id\":\"b6\"},\"end\":34391,\"start\":34000},{\"attributes\":{\"id\":\"b7\"},\"end\":34728,\"start\":34393},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13841671},\"end\":35015,\"start\":34730},{\"attributes\":{\"doi\":\"NSA-RD-2013-056001v1\",\"id\":\"b9\"},\"end\":35263,\"start\":35017},{\"attributes\":{\"doi\":\"NSA-RD-2014-050001v1\",\"id\":\"b10\"},\"end\":35487,\"start\":35265},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1236598},\"end\":36055,\"start\":35489},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11891701},\"end\":36593,\"start\":36057},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11585645},\"end\":37087,\"start\":36595},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7912955},\"end\":37562,\"start\":37089},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":12191214},\"end\":37915,\"start\":37564},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11346384},\"end\":38429,\"start\":37917},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10590059},\"end\":38742,\"start\":38431},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3905928},\"end\":39254,\"start\":38744},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15348563},\"end\":39750,\"start\":39256},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":12726120},\"end\":40088,\"start\":39752},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206977691},\"end\":40501,\"start\":40090}]", "bib_title": "[{\"end\":32472,\"start\":32432},{\"end\":32937,\"start\":32879},{\"end\":33536,\"start\":33489},{\"end\":34080,\"start\":34000},{\"end\":34448,\"start\":34393},{\"end\":34785,\"start\":34730},{\"end\":35569,\"start\":35489},{\"end\":36140,\"start\":36057},{\"end\":36667,\"start\":36595},{\"end\":37191,\"start\":37089},{\"end\":37653,\"start\":37564},{\"end\":37968,\"start\":37917},{\"end\":38474,\"start\":38431},{\"end\":38819,\"start\":38744},{\"end\":39343,\"start\":39256},{\"end\":39816,\"start\":39752},{\"end\":40157,\"start\":40090}]", "bib_author": "[{\"end\":32211,\"start\":32204},{\"end\":32221,\"start\":32211},{\"end\":32231,\"start\":32221},{\"end\":32485,\"start\":32474},{\"end\":32494,\"start\":32485},{\"end\":32503,\"start\":32494},{\"end\":32512,\"start\":32503},{\"end\":32524,\"start\":32512},{\"end\":32537,\"start\":32524},{\"end\":32544,\"start\":32537},{\"end\":32555,\"start\":32544},{\"end\":32567,\"start\":32555},{\"end\":32582,\"start\":32567},{\"end\":32952,\"start\":32939},{\"end\":32964,\"start\":32952},{\"end\":32972,\"start\":32964},{\"end\":32985,\"start\":32972},{\"end\":32995,\"start\":32985},{\"end\":33005,\"start\":32995},{\"end\":33383,\"start\":33373},{\"end\":33394,\"start\":33383},{\"end\":33547,\"start\":33538},{\"end\":33556,\"start\":33547},{\"end\":33566,\"start\":33556},{\"end\":33583,\"start\":33566},{\"end\":33593,\"start\":33583},{\"end\":33897,\"start\":33888},{\"end\":33910,\"start\":33897},{\"end\":34092,\"start\":34082},{\"end\":34105,\"start\":34092},{\"end\":34461,\"start\":34450},{\"end\":34471,\"start\":34461},{\"end\":34484,\"start\":34471},{\"end\":34493,\"start\":34484},{\"end\":34800,\"start\":34787},{\"end\":34811,\"start\":34800},{\"end\":34819,\"start\":34811},{\"end\":35059,\"start\":35046},{\"end\":35069,\"start\":35059},{\"end\":35307,\"start\":35294},{\"end\":35581,\"start\":35571},{\"end\":35591,\"start\":35581},{\"end\":35603,\"start\":35591},{\"end\":35612,\"start\":35603},{\"end\":35620,\"start\":35612},{\"end\":35628,\"start\":35620},{\"end\":35638,\"start\":35628},{\"end\":35649,\"start\":35638},{\"end\":35660,\"start\":35649},{\"end\":35668,\"start\":35660},{\"end\":36152,\"start\":36142},{\"end\":36164,\"start\":36152},{\"end\":36174,\"start\":36164},{\"end\":36184,\"start\":36174},{\"end\":36196,\"start\":36184},{\"end\":36204,\"start\":36196},{\"end\":36215,\"start\":36204},{\"end\":36228,\"start\":36215},{\"end\":36238,\"start\":36228},{\"end\":36254,\"start\":36238},{\"end\":36679,\"start\":36669},{\"end\":36689,\"start\":36679},{\"end\":36699,\"start\":36689},{\"end\":36711,\"start\":36699},{\"end\":36719,\"start\":36711},{\"end\":36732,\"start\":36719},{\"end\":36743,\"start\":36732},{\"end\":36756,\"start\":36743},{\"end\":36766,\"start\":36756},{\"end\":36775,\"start\":36766},{\"end\":37202,\"start\":37193},{\"end\":37213,\"start\":37202},{\"end\":37224,\"start\":37213},{\"end\":37232,\"start\":37224},{\"end\":37242,\"start\":37232},{\"end\":37251,\"start\":37242},{\"end\":37268,\"start\":37251},{\"end\":37664,\"start\":37655},{\"end\":37677,\"start\":37664},{\"end\":37987,\"start\":37970},{\"end\":37999,\"start\":37987},{\"end\":38010,\"start\":37999},{\"end\":38021,\"start\":38010},{\"end\":38030,\"start\":38021},{\"end\":38039,\"start\":38030},{\"end\":38049,\"start\":38039},{\"end\":38057,\"start\":38049},{\"end\":38065,\"start\":38057},{\"end\":38073,\"start\":38065},{\"end\":38488,\"start\":38476},{\"end\":38506,\"start\":38488},{\"end\":38514,\"start\":38506},{\"end\":38520,\"start\":38514},{\"end\":38830,\"start\":38821},{\"end\":38843,\"start\":38830},{\"end\":38850,\"start\":38843},{\"end\":38863,\"start\":38850},{\"end\":38872,\"start\":38863},{\"end\":38884,\"start\":38872},{\"end\":38894,\"start\":38884},{\"end\":38906,\"start\":38894},{\"end\":39358,\"start\":39345},{\"end\":39367,\"start\":39358},{\"end\":39374,\"start\":39367},{\"end\":39386,\"start\":39374},{\"end\":39396,\"start\":39386},{\"end\":39407,\"start\":39396},{\"end\":39416,\"start\":39407},{\"end\":39426,\"start\":39416},{\"end\":39433,\"start\":39426},{\"end\":39827,\"start\":39818},{\"end\":39840,\"start\":39827},{\"end\":40172,\"start\":40159},{\"end\":40187,\"start\":40172},{\"end\":40197,\"start\":40187},{\"end\":40210,\"start\":40197}]", "bib_venue": "[{\"end\":32202,\"start\":32070},{\"end\":32634,\"start\":32582},{\"end\":33081,\"start\":33005},{\"end\":33371,\"start\":33321},{\"end\":33626,\"start\":33593},{\"end\":33886,\"start\":33835},{\"end\":34181,\"start\":34105},{\"end\":34544,\"start\":34493},{\"end\":34847,\"start\":34819},{\"end\":35044,\"start\":35017},{\"end\":35292,\"start\":35265},{\"end\":35744,\"start\":35668},{\"end\":36306,\"start\":36254},{\"end\":36827,\"start\":36775},{\"end\":37304,\"start\":37268},{\"end\":37713,\"start\":37677},{\"end\":38131,\"start\":38073},{\"end\":38572,\"start\":38520},{\"end\":38974,\"start\":38906},{\"end\":39478,\"start\":39433},{\"end\":39904,\"start\":39840},{\"end\":40272,\"start\":40210},{\"end\":33646,\"start\":33628},{\"end\":38176,\"start\":38133}]"}}}, "year": 2023, "month": 12, "day": 17}