{"id": 237382270, "updated": "2023-10-05 23:08:30.965", "metadata": {"title": "The Power of Points for Modeling Humans in Clothing", "authors": "[{\"first\":\"Qianli\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Jinlong\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Siyu\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Black\",\"middle\":[\"J.\"]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 9, "day": 2}, "abstract": "Currently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. Despite progress on 3D scanning and modeling of human bodies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, social networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of clothing. Recent interest has shifted to implicit surface models for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We argue that this representation has been with us all along -- the point cloud. Point clouds have properties of both implicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neural network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2109.01137", "mag": "3196587790", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/MaY0B21", "doi": "10.1109/iccv48922.2021.01079"}}, "content": {"source": {"pdf_hash": "178c9e69e2aabf6f5e881b4a3c6110fc89a8c391", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2109.01137v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a0da1437de25e742bffbe04cf94facd8216baa0d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/178c9e69e2aabf6f5e881b4a3c6110fc89a8c391.txt", "contents": "\nThe Power of Points for Modeling Humans in Clothing Unseen scan Fitted POP (point cloud) Animations with pose-dependent clothing deformation\n\n\nQianli Ma qianli.ma@inf.ethz.ch \nMax Planck Institute for Intelligent Systems\nT\u00fcbingenGermany\n\nETH Z\u00fcrich\n\n\nJinlong Yang jyang@tuebingen.mpg.de \nMax Planck Institute for Intelligent Systems\nT\u00fcbingenGermany\n\nSiyu Tang siyu.tang@inf.ethz.ch \nETH Z\u00fcrich\n\n\nMichael J Black black@tuebingen.mpg.de \nMax Planck Institute for Intelligent Systems\nT\u00fcbingenGermany\n\nThe Power of Points for Modeling Humans in Clothing Unseen scan Fitted POP (point cloud) Animations with pose-dependent clothing deformation\n\nFigure 1: The Power of Points (POP) model for clothed humans. Based on a novel articulated dense point cloud representation, our cross-outfit model, named POP, produces pose-dependent shapes of clothed humans with coherent global shape and expressive local garment details. The trained model can be fitted to a single scan of an unseen subject wearing an unseen outfit, and can animate it with realistic pose-dependent clothing deformations. The results are color-coded with predicted point normals and rendered with a simple surfel-based renderer.AbstractCurrently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. Despite progress on 3D scanning and modeling of human bodies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, social networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of clothing. Recent interest has shifted to implicit surface models for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We argue that this representation has been with us all alongthe point cloud. Point clouds have properties of both implicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neural network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model posedependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes at https://qianlim.github.io/POP.\n\nIntroduction\n\nAnimatable clothed human avatars are required in many applications for 3D content generation. To create avatars with naturally-deforming clothing, existing solutions either involve heavy artist work or require 4D scans for training machine-learning models [65]. These solutions are expensive and often impractical. Instead, can we turn a single static 3D scan -which can be acquired at low cost today even with hand-held devices -into an animatable avatar? Currently, no existing technology is able to do this and produce realistic clothing deformations. Given a static scan, traditional automatic rigging-and-skinning methods [3,20,38] can be used to animate it, but are unable to produce pose-dependent clothing deformations. Physics-based simulations can produce realistic deformations, but require \"reverse-engineering\" a simulation-ready clothing mesh from the given scan. This involves expert knowledge and is not fully automatic.\n\nTaking a data-driven approach, the goal would be to learn a model that can produce reasonable pose-dependent clothing deformation across different outfit types and styles and can generalize to unseen outfits. However, despite the recent progress in modeling clothed human body shape deformations [25,35,41,42,51,65], most existing models are outfit-specific and thus cannot generalize to unseen outfits. To date, no such cross-garment model exists, due to several technical challenges.\n\nThe first challenge lies in the choice of 3D shape representation. To handle outfits of different types and styles at once, the shape representation must handle changing topology, capture high-frequency details, be fast at inference time, and be easy to render. Classical triangle meshes excel at rendering efficiency but are fundamentally limited by their fixed topology. The implicit surface representation is topologically flexible, but is in general computationally heavy and lacks compatibility with existing graphics tools. Because point clouds are an explicit representation, they are efficient to render, but they can also be viewed as implicitly representing a surface. Thus they are flexible in topology and, as the resolution of the point cloud increases, they can capture geometric details. While point clouds are not commonly applied to representing clothing, they are widely used to represent rigid objects and many methods exist to process them efficiently with neural networks [1,19,36]. In this work, we show that the seemingly old-fashioned point cloud is, in fact, a powerful representation for modeling clothed humans.\n\nIn recent work, SCALE [41] demonstrates that a point cloud, grouped into local patches, can be exploited to represent clothed humans with various clothing styles, including those with thin structures and open surfaces. However, the patch-based formulation in SCALE often suffers from artifacts such as gaps between patches. In this work, we propose a new shape representation of dense point clouds. For simplicity, we avoid using patches, which have been widely used in recent point cloud shape representations [4,17,18,24,41], and show that patches are not necessary. Instead, we introduce smooth local point features on a 2D manifold that regularize the points and enable arbitrarily dense up-sampling during inference.\n\nAnother challenging aspect of cross-outfit modeling concerns how outfits of different types and styles can be encoded in a single, unified, model. In most existing outfit-specific models, the model parameters (typically the weights of a trained shape decoder network) need to represent both the intrinsic, pose-independent shape of a clothed person, and how this shape deforms as a function of the in-put pose. To factor the problem, we propose to isolate the intrinsic shape from the shape decoder by explicitly conditioning it with a geometric feature tensor. The geometric feature tensor is learned in an auto-decoding fashion [50], with a constraint that a consistent intrinsic shape is shared across all examples of the same outfit. Consequently, the shape decoder can focus on modeling the pose-dependent effects and can leverage common deformation properties across outfits. At inference time, the geometric feature tensor can be optimized to fit to a scan of a clothed body with a previously unseen outfit, making it possible for the shape decoder to predict pose-dependent deformation of it based on the learned clothing deformation properties.\n\nThese ideas lead to POP: our dense point cloud model that produces pose-dependent clothing geometry across different outfits and demonstrating the Power of Points for modeling shapes of humans in clothing. POP is evaluated on both captured and synthetic datasets, showing state-ofthe-art performance on clothing modeling and generalization to unseen outfits.\n\nIn summary, our contributions are: (1) a novel dense point cloud shape representation with fine-grained local features that produces state-of-the-art detailed clothing shapes with various clothing styles; (2) a novel geometry feature tensor that enables cross-garment modeling and generalization to unseen outfits; (3) an application of animating a static scan with reasonable pose-dependent deformations. The model and code are available for research purposes at https://qianlim.github.io/POP.\n\n\nRelated Work\n\nShape Representations for Clothed Human Modeling. Surface meshes are an efficient 3D representation that is compatible with graphics pipelines, and thus are the dominant choice for modeling clothing and clothed humans. With meshes, the clothing is represented either by deforming an unclothed body template [6,8,42,47,71,78], or using a separately defined template [25,26,35,51,66]. While recent work successfully produces detailed geometry with graph convolutions [42], multi-layer perceptrons (MLPs) [8,51], and UV map convolutions [31,35], meshes suffer from two fundamental limitations: the fixed topology and the requirement for template registration. This restricts their generalization to outfit styles beyond the pre-defined templates, and makes it difficult to obtain a common representation for various clothing categories. Although recent work proposes adaptable templates [49,82] that modify mesh connectivity, the need for registering training data to the mesh template remains challenging when a complex garment geometry is involved.\n\nNeural implicit surfaces [12,44,50], on the other hand, do not require any pre-defined template, are flexible with surface topology, and have recently become a promising choice for reconstructing [7,27,28,56,63,64,79,80] and modeling [10,13,16,45,48,65] shapes of 3D humans. Despite their ability to handle varied clothing topology, it remains an open challenge to realistically represent thin structures that are often present in daily clothing. Moreover, reconstructing an explicit surface from the implicit function costs cubic time with respect to the resolution, which restricts them from many practical applications.\n\nPoint clouds are another classic 3D representation that supports arbitrary topology as well as thin structures. Going beyond prior work that generates a sparse point set [1,19,36], recent approaches [4,17,18,24] use deep learning to generate structured point clouds with a set of surface patches. Leveraging the patch regularization, SCALE [41] proposes an articulated dense point cloud representation to model clothing deformation. However, the patch-based point clouds often suffer from overlap [24] or separation [41] between the patches, which degrades the geometric fidelity and visual quality. Our model, POP, follows the spirit of these approaches to generate a dense, structured point cloud, but, in contrast to prior work, we deprecate the concept of patches and, instead, decode a dense point cloud from fine-grained local features. The resulting clothed human model shows high geometry fidelity, is robust to topology variations, and generalizes to various outfit styles. Modeling Outfit Shape Space. We categorize existing models for clothing or clothed bodies into three levels of increasing generalization capacity, as summarized in Tab. 1. Note that non-parametric models for clothed human reconstruction [63,64,80] are out of the scope for this analysis.\n\nOutfit-Specific. Methods from this class need to train a separate model for every outfit instance [14,26,35,41,47,65,66,77] or category (e.g. all short-sleeve Tshirts) [25,51,71,76]. For mesh-based methods in this category [14,25,35,47,51,71,77], this characteristic stems from the need to manually define a mesh template: the fixed mesh topology fundamentally prohibits generalization to a different outfit category (e.g. from pants to a skirt). These methods can, however, deal with size [25,71] or style [51,76] within the template-defined category. Template-free methods [41,65] require training a separate model for each outfit, i.e. the intrinsic, pose-independent shape information is stored in the model parameters; hence, the test-time generalization to an unseen outfit is restricted.\n\nMulti-Outfit. Combining multiple pre-defined mesh templates with a multi-head network or a garment classifier, the MGN model [6], BCNet [29], and DeepFashion3D [82] can reconstruct humans in a variety of clothing from images. In a similar spirit, CAPE [42] uses a pre-labeled one-hot vector for outfit-type conditioning and can generate new clothing from four common categories with a single model. While training a single model for multiple outfits exploits the complementary information among training data, these methods Table 1: Data-driven models for clothing / clothed humans classified by garment space generalization.\n\n\nOutfit-Specific\n\nDe Aguiar [14], DRAPE [25], GarNet [26], DeepWrinkles [35], SCALE [41], Neophytou [47], TailorNet [51], SCANimate [65], Santesteban [66], Sizer [71], Wang [76], Yang [77].\n\nMulti-Outfit MGN [6], BCNet [29], CAPE [42], Vidaurre [73], DeepFashion3D [82]. Arbitrary Outfit SMPLicit [13], Shen [68], POP (Ours).\n\ndo not show the ability to handle unseen garments beyond the pre-defined categories.\n\nArbitrary Outfit. To overcome the limitations brought by the fixed topology of meshes, recent work opts for other representations that can unify different clothing categories and types. Shen et al. [68] represent garments using 2D sewing pattern images that are applicable to arbitrary clothing categories. However, the final 3D garment shape is represented with a single manifold mesh that is not sufficiently expressive to represent the complexity and variety of real-world clothing. Using neural implicit surfaces, SM-PLicit [13] learns a topology-aware generative model for garments across multiple categories and shows continuous interpolation between them. However, the clothing geometry tends to be bulky and lacks details. In contrast, our POP model faithfully produces geometric details of various outfits, can generalize to unseen outfits, and demonstrates state-of-the-art performance on garment space modeling.\n\n\nMethod\n\nOur goal is to learn a single, unified, model of highfidelity pose-dependent clothing deformation on human bodies across multiple outfits and subjects. We first introduce an expressive point-based representation that preserves geometric details and flexibly models varied topology (Sec. 3.1). Using this, we build a cross-outfit model enabled by a novel geometric feature tensor (Sec. 3.2).\n\nAs illustrated in Fig. 2, given an unclothed body, the model outputs the 3D clothed body by predicting a displacement field from the body surface based on local pose and geometric features. The trained model can be fitted to a scan of a person in previously unseen outfits and this scan can be animated with pose-dependent deformations (Sec. 3.3).\n\n\nRepresenting Humans with Point Clouds\n\nPoint-based representations [23,41] possess topological flexibility and fast inference speed, giving them an advantage over meshes or implicit functions for modeling articulated humans. In this work, we formulate a structured point cloud representation for modeling 3D clothed humans by learning a mapping from a 2D manifold to a 3D deforma-  Figure 2: Overview of POP. Given a posed but unclothed body model (visualized as a blue 2D contour), we record the 3D positions pi of its surface points on a UV positional map I, and encode this into a pose feature tensor P . A garment geometry feature tensor G is an optimizable variable that is pixel-aligned with P , and learned per-outfit in an auto-decoder manner. The 2D image-plane coordinate ui describes the relative location of the points on the body surface manifold. The shape decoder queries these locations and predicts displacement vectors ri based on the points' local pose and geometry features. tion field, in a similar form to AtlasNet [24]:\nr i = f w (u i ; z i ) : R 2 \u00d7 R Z \u2192 R 3 ,(1)\nwhere r i is a displacement vector, f w (\u00b7) is a multi-layer perceptron (MLP) with weights w, u i is a 2D parameterization of a point i that denotes its relative location on the body surface, and z i is the point's local feature code containing the shape information.\n\nWe deviate from other recent point-based human representations in two key ways: 1) We use fine-grained perpoint local features z i as opposed to a single global feature [23,24] or per-patch features [41] in prior work. 2) We predict the clothing deformation field on top of the canonically posed body, instead of absolute Euclidean point coordinates [23] or local patch deformations [41]. Both design choices lead to significant improvements in representation power, as detailed below. Continuous Local Point Features. Recent work shows the advantage of using a local latent shape code over a global code: both for neural implicit surfaces [9,22,30,55] and point clouds [41]. Decoding shape from local codes significantly improves geometry quality. In particular for modeling humans, SCALE [41] successfully leverages local features to represent pose-aware garment shape and demonstrates a significant qualitative improvement against prior work that uses a single global feature code [23].\n\nHowever, the local feature in SCALE is still discretely distributed on a set of pre-defined basis points u i on the body manifold. Each feature code is decoded into multiple points in a neighborhood (a \"patch\") in the output space, but it varies discretely across patch boundaries. This is equivalent to the nearest neighbor (on the body manifold) assignment of the features, Fig. 3(a). This discrete characteristic limits the quality of the geometry. As shown in Fig. 3 of [41] and our Sec. 4.1, the patches are typically isolated from each other, leading to uneven point distributions, hence poor mesh reconstruction quality. To address this problem, we make the local features more fine-grained in two ways. First, we define a denser set of basis points u i , together with their local features, on the body manifold. In practice, this amounts to simply increasing the resolution of the body UV map (see Sec. 3.2). Second, we further diffuse the feature over the body surface: for a query coordinate on the body surface, we compute its feature by bilinearly interpolating the features from its 4 nearest basis points, Fig. 3. As a result, the network output can be evaluated at arbitrarily high resolution by querying the decoder f w (\u00b7) with any point on the body surface, which we also denote as u i from now on with a slight abuse of notation.\n\nLocal Transformations. The clothing deformation vector r i in Eq. (1) is predicted on top of the body in the canonical pose. Thus a large portion of shape variation is explained by body articulation so that the network can focus on modeling the residual shape. Note that unlike the mesh vertex offset representation of clothing [6,42,58], our formulation does not assume a constant topology, and can thus model various outfit styles such as pants and dresses.\n\nTo reconstruct the clothed body in the posed space, we transform r i according to the transformation matrix T i at u i that is given by the fitted body model. The point position from the clothed body in the posed space is then given by:\nx i = T i \u00b7 r i + p i ,(2)\nwhere p i is the 3D Euclidean coordinate of u i on the posed unclothed body. Note that we branch out the final layers of f w (\u00b7) such that it also predicts the normal n(x i ) of each point, which is transformed with the rotation part of T i . Since our local point features are continuous over the body surface, we also perform barycentric interpolation to obtain continuously varying T i , in the same way as LoopReg [5].\n\n\nCross-garment Modeling with a Single Model\n\nWith our structured point cloud representation, we now build a system that models pose-dependent deformations of various garments, from different categories, of different shapes and topology, dressed on different body shapes, using a single model. This is achieved by introducing a novel geometric feature tensor. Practically, we decompose the local feature z i in Eq. (1) into pose z P i and garment geometry z G i , as illustrated in Fig. 2 and detailed below. Body Shape Agnostic Pose Encoding. We first condition our network with learned local body pose features z P i such that the output garment deformation is pose-aware. We adopt the approach based on the UV positional map of the posed body as used in [41] as it shows better pose generalization than the traditional pose parameter conditioning [42,51,77]. As shown in Fig. 2, a UV positional map I \u2208 R H\u00d7W \u00d73 is a 2D parameterization of the body manifold, where each valid pixel corresponds to a point on the posed body surface. The 2D image-plane coordinate of a pixel describes its manifold position: u i = (u, v) i . The pixel value records the point's location in R 3 : p i = I ui . A UNet [62] encodes I into a pose feature tensor P \u2208 R H\u00d7W \u00d764 , where H, W are the spatial dimensions of the feature tensor, and each \"pixel\" from P is a 64-dimensional pose feature code: z P i = P ui \u2208 R 64 . Because of the receptive field of the UNet, the learned pose features can contain global pose information from a larger neighborhood when necessary.\n\nThe UV positional map naturally contains information about body shape. To generalize to different subjects, we use posed bodies of a neutral shape for the pose encoding. Still, the predicted clothing deformations are added to each subject's body respectively. Geometric Feature Tensor. In most learning-based, outfit/subject-specific clothed body models [41,42,65,77], the clothing shape information is contained in the parameters of the trained shape decoder network, limiting generalization to unseen outfits. What is needed for cross-outfit modeling is a mechanism that decouples the intrinsic, poseindependent shape of a clothed person from the decoder, so that it can focus on modeling how the shape deforms with the pose. To that end, we propose to explicitly condition the shape decoder with a geometric feature tensor G \u2208 R H\u00d7W \u00d764 , Fig. 2.\n\nThe geometric feature tensor follows the spirit of being local and is pixel-aligned with the pose feature tensor. Each of its \"pixels\" represents a local shape feature on a body point: z G i = G ui \u2208 R 64 . Unlike the pose features, the geometry features are learned in an auto-decoding [50] fashion; i.e. they are updated during training such that the optimal representation for the garment geometry is discovered by the network itself. Importantly, we use a consistent G for each outfit across all of its training examples (in different poses). In this way, G is enforced to be a pose-agnostic canonical representation of each outfit's geometry.\n\nOur geometry feature tensor plays a similar role as the pre-defined clothing templates in many mesh-based garment models [26,51,66,73]. However, by auto-decoding the geometric features, we get rid of the reliance on manual template definition. More importantly, our decoder network and the neural geometry feature are fully differentiable. This enables generalization to unseen garments at test time, which is done by optimizing G to fit the target clothed body scan. See Sec. 3.3 for details. Shared Garment Shape Decoder. With the introduced local pose and geometry features, we can re-write Eq. (1) in a more concrete form:\nr i = f w ([u i , z P i , z G i ]),\nwhere [\u00b7, \u00b7, \u00b7] denotes concatenation. While z G i is optimized for each garment and z P i is acquired from each pose, f w (\u00b7) is shared for the entire set of all garments and poses. By training on many outfits and poses, the decoder learns common properties of clothing deformation, with which it can animate scans in unseen outfits at test-time, as described below.\n\n\nTraining and Inference\n\nLoss Functions. We train POP with multi-subject and outfit data. During training, the parameters of the UNet pose encoder, the garment shape decoder, and the geometric feature tensor G are optimized, by minimizing the loss function:\nL total = \u03bb d L d + \u03bb n L n + \u03bb rd L rd + \u03bb rg L rg ,(3)\nwhere the \u03bb's are weights that balance the loss terms, and the L's are the following loss terms. First, the normalized Chamfer Distance L d is employed to penalize the average bi-directional point-to-point L2 distances between the generated point cloud X and a sampled point set Y from the ground truth surface:\nL d = d(x, y) = 1 M M i=1 min j x i \u2212 y j 2 2 + 1 N N j=1 min i x i \u2212 y j 2 2 ,(4)\nwhere M, N are the number of points from the generated point cloud and the ground truth surface, respectively.\n\nThe normal loss L n is the averaged L1 discrepancy between the normal prediction on each generated point and its nearest neighbor from the ground truth point set:\nL n = 1 M M i=1 n(x i ) \u2212 n(argmin yj \u2208Y d(x i , y j )) 1 ,(5)\nwhere n(\u00b7) denotes the unit normal of the given point. An L2 regularizer L rd discourages the predicted point displacements from being extremely large. Similarly, the term L rg penalizes the L2-norm of the vectorized geometric feature tensor to regularize the garment shape space:\nL rd = 1 M M i=1 r i 2 2 , L rg = 1 C C m=1 G m 2 2 ,(6)\nwhere C is the number of garments seen in training. The detailed model architecture, hyper-parameters and training procedure are provided in the SupMat. Inference: Scan Animation. At test-time, POP can generalize to unseen poses of both the previously seen and unseen outfits. For a seen outfit, we use its geometric feature tensor optimized from training, and infer the clothing deformation on unseen poses with a simple forward pass of the network.\n\nTo test on a scan\u0176 of a human wearing unseen clothing, we first fix the weights of the UNet pose encoder and the shape decoder g w (\u00b7), and optimize the geometric feature tensor such that the total loss against the scan is minimized:\nG = argmin L total (\u0176).(7)\nAfterwards, the estimated\u011c is fixed, and is then treated as in the case of a seen garment. As with other point-based human models, the point cloud generated by POP can either be meshed using classical tools such as the Poisson Surface Reconstruction (PSR) [32], or directly rendered into realistic images using recent pointbased neural rendering techniques [2,34,59]. However, in this work, we do not rely on neural rendering to inpaint the gaps between the points. Instead, we show qualitative results using a simple surfel-based renderer to more directly highlight the geometric properties of the smooth, high-resolution, human point cloud generated by POP.\n\n\nExperiments\n\nDue to the lack of comparable existing work on POP's two key features, namely cross-outfit modeling and single scan animation, we first evaluate its representation power on a simpler but related task: outfit-specific shape modeling, and compare with two state-of-the-art methods (Sec. 4.1). We then discuss the efficacy of our cross-outfit learning formulation (Sec. 4.2) and demonstrate single scan animation (Sec. 4.3). Datasets. We train and evaluate our method and baselines on both a captured clothed human dataset, CAPE [42], and our new synthetic dataset called ReSynth. From the CAPE dataset, we use the three subjects (00096, 00215, 03375) that contain the most abundant outfit variations (14 outfits in total) to compare the representation capacity of different methods. The synthetic ReSynth dataset is created with a larger variation in outfit shapes, styles, and poses. We worked with a professional clothing designer to create 3D outfit designs that faithfully reflect those in a set of commercial 3D clothed human scans (Renderpeople [61]), resulting in 24 outfits including challenging cases such as skirts and jackets. We then use physics simulation to drape the clothing on the 3D bodies from the AGORA dataset [52], which we animate to generate many poses. Details of the datasets are provided in the SupMat., and we will release ReSynth for research purposes. Baselines. To evaluate the representation power of our model, we first compare with two recent methods for posedependent human shape modeling (Sec. 4.1): NASA [16] and SCALE [41]. To evaluate the effectiveness of our crossoutfit modeling formulation (Sec. 4.2), we compare two versions of our model: per-outfit trained and a cross-outfit model trained with data from all outfits, both using the same architecture. For animating unseen scans (Sec. 4.3), we qualitatively compare with classical Linear Blend Skinning using the SMPL [39] body model. Metrics. We quantitatively evaluate each method using the Chamfer Distance (in m 2 , Eq. (4)) and the L1 normal discrepancy (Eq. (5)), computed over the 50K points generated by our method and SCALE. For the implicit surface baseline NASA, the points for evaluation are sampled from surface extracted using Marching Cubes [40]. To evaluate the visual quality of the generated results, we perform a largescale user study on the Amazon Mechanical Turk (AMT) and report the percentage of users that favor the results from our method over the baseline. Details of the user study are provided in the SupMat.\n\n\nRepresentation Power\n\nTab. 2 summarizes the numerical results of reconstructing pose-dependent garment shape from different methods, tested with seen outfits on unseen motion sequences. As the difficulty of shape modeling varies greatly across different outfit types (e.g. how a loose jacket deforms is much more complex than that of a tight T-shirt), we report three types of statistics to holistically reflect the performance of each model: the mean error averaged across all test examples from all outfits, the median of the per-outfit calculated mean error (denoted as \"Outfit Median\" in Tab. 2), and the per-outfit maximum error (\"Outfit Max\"). Comparison with SoTA. The upper section of Tab. 2 shows a comparison with NASA [16] and SCALE [41]. NASA represents the body shape with an ensemble of articulated occupancy functions defined per body part. As it requires The differences in perceptual quality can be seen in Fig. 4. All approaches provide clear pose-dependent effects. However, NASA suffers from non-smooth transitions between separately-modeled body parts and is unable to handle thin structures and open surfaces such as the skirt in ReSynth. SCALE, on the other hand, produces a smooth global shape with local details, but the isolation between the patches leads to sub-optimal visual quality. In contrast, the dense point clouds generated by POP manifest a coherent overall shape and expressive local details. This illustrates the advantage of our continuous local point features as opposed to the discretely defined patch features.\n\n\nCross-Outfit Modeling\n\nA key feature of POP is the ability to handle multiple outfits of varied types using a single model, without pre-defining garment templates. As shown in Tab. 2, our cross-outfit model trained with all subjects and outfits (\"Ours, multi\") reaches overall comparable performance to the outfit-specific models (\"Ours, per-outfit\") on the synthetic ReSynth data, and has significantly lower error on the CAPE dataset. Still, on both datasets, the cross-outfit model consistently outperforms the two baselines analyzed above.\n\nThe result on the CAPE data reveals the advantage of cross-outfit modeling: the information between different but similar outfit styles can be shared. The CAPE dataset mostly consists of simple and similar garments such as  and ReSynth (lower 2 rows) data. The dense point clouds from POP (cross-outfit model) are clean and preserve local details, while baseline methods suffer from artifacts. Note the point clouds are colored according to predicted normal and rendered with surfels [57]. Best viewed zoomed-in on a color screen.\n\nlong/short T-shirts and pants, but the motions are performed differently by different subjects. Intuitively, training a cross-outfit model leads to mutual data augmentation in the pose space among different outfits, hence a better generalization to unseen poses as seen in the numerical results. In contrast, the synthetic data are simulated with a consistent set of motion sequences for all outfit types with largely varied geometry. As a result, the inter-outfit knowledge sharing is limited, leading to similar performance between per-outfit and cross-outfit models. Robustness Against Limited Data. In the lower section of Tab. 2, we evaluate the performance of our cross-outfit model trained with subsets sampled from the full training set. Even with only 1/4 of data from each outfit, our crossoutfit model is comparable to the outfit-specific models of SCALE that are trained with full data.\n\n\nSingle Scan Animation\n\nOnce trained, our cross-outfit POP model can be fit to a single scan of an unseen subject with an unseen outfit, and then animated with pose-dependent clothing variation, as described in Sec. 3.3. Notably, this is a very challenging task since it requires generalization in both the outfit shape space and pose space. Figure 5 qualitatively compares our model (trained on CAPE data) and the classical Linear Blend Skinning (LBS) technique that uses the SMPL model [39] to animate the given unseen scan with an unseen motion. Here we use sampled points from the mesh provided in the CAPE dataset as the target scan. As LBS uses simple rigid transformations of the body parts only, it cannot produce complex pose-dependent shape variation. In contrast, POP produces reasonable and vivid clothing deformation such as the lifted hems. In Fig. 6 we deploy POP (trained on the ReSynth data) to animate an unseen scan from ReSynth and one from real-world captures [61], respectively. Note that the latter test is much more challenging due to the domain gap between our synthetic training data and the captured test data. POP produces high-quality clothing geometry on the ReSynth test example and generates reasonable animation for the challenging real-world scan, which opens the promising new direction of automatic 3D avatar creation from a \"one-shot\" observation.\n\n\nConclusion\n\nWe have introduced POP, a dense, structured point cloud shape representation for articulated humans in clothing. Being template-free, geometrically expressive, and topologically flexible, POP models clothed humans in various outfit styles with a single model, producing high-quality details and realistic pose-dependent deformation. The learned POP model can also be used to animate a single human scan from an unseen subject and clothing. Our evaluations on both captured data and our synthetic data demonstrate the efficacy of the continuous local features and the advantage of a cross-outfit model over traditional subject-specific ones.\n\nPOP assumes that the minimally closed body under the clothing is given and the training scans have no noise or missing data. Dealing with partial, noisy, scans and combining it with automatic body shape and pose estimation [5,74] are promising future directions for fullyautomatic scan animation. Our use of the UV map for body pose conditioning can sometimes lead to \"seams\" in the outputs (see SupMat.). Future work should explore more continuous parameterizations of the human body manifold. Additionally, here we factor out dependencies of clothing deformation on body shape for simplicity. Given sufficient training data, this would be easy to learn by replacing the neutral shape with the true shape in the UV positional map.\n\n\nAppendix\n\n\nA. Implementation Details\n\n\nA.1. Model Architecture\n\nWe use the SMPL [39] (for CAPE data) and SMPL-X [53] (for ReSynth data) UV maps of 128 \u00d7 128 \u00d7 3 resolution as pose input, where each pixel is encoded into 64 channels by the pose encoder. The pose encoder is a standard UNet [62] that consists of seven [Conv2d, BatchNorm, LeakyReLU(0.2)] blocks, followed by seven [ReLU, Con-vTranspose2d, BatchNorm] blocks. The final layer does not apply BatchNorm.\n\nThe geometric feature tensor has the same resolution as that of the pose feature tensor, i.e. 128 \u00d7 128 \u00d7 64. It is learned in an auto-decoding [50] manner, being treated as a free variable that is optimized together with the network weights during training. The geometric feature tensor is followed by three learnable convolutional layers, each with a receptive field of 5, before feeding it to the shape decoder. We find that these convolutional layers help smooth the features spatially, resulting in a lower noise level in the outputs.\n\nThe pose and geometric feature tensors are concatenated along the feature channel. In all experiments, we query the feature tensor with a 256 \u00d7 256 UV map, i.e. the concatenated feature tensor is spatially 4\u00d7 bilinearly upsampled. The output point cloud has 50K points.\n\nAt each query location, the concatenated pose and geometry feature (64+64-dimensional), together with the 2D UV coordinate of the query point, are fed into an 8-layer MLP. The intermediate layers' dimensions are (256,256,256,386,256,256,256,3), with a skip connection from the input to the 4th layer as in DeepSDF [50]. From the 6th layer, the network branches out 2 heads with the same architecture to predict the displacements and point normals, respectively. All but the last layer use BatchNorm and a Softplus non-linearity with \u03b2 = 20. The predicted normals are normalized to unit length.\n\n\nA.2. Training\n\nWe train POP with the Adam [33] optimizer with a learning rate of 3.0\u00d710 \u22124 , a batch size of 4, for 400 epochs. The displacement and normal prediction modules are trained jointly. As the normal loss relies on the nearest neighbor ground truth points found by the Chamfer Distance, we only turn it on when L d roughly stabilizes from the 250th epoch. The loss weights are set to \u03bb d = 2.0 \u00d7 10 4 , \u03bb rd = 2.0 \u00d7 10 3 , \u03bb rg = 1.0, \u03bb n = 0.0 at the beginning of the training, and \u03bb n = 0.1 from the 250th epoch.\n\n\nA.3. Data Processing\n\nWe normalize all the data examples by removing the body translation and global orientation from them. From each clothed body surface, we sample 40K points to serve as training ground truth. Note that we do not rely on any connectivity information in the registered meshes from the CAPE dataset.\n\n\nA.4. Baselines\n\nNASA. We re-implement the NASA [16] model in PyTorch and ensure the performance is on par with that reported in the original paper. For evaluating NASA results, we first extract a surface using Marching Cubes [40] and then sample the same number of points (50K) from it for a fair comparison. The sampling is performed and averaged over three repetitions. SCALE. We employ the same training schedule and the number of patches (798) as in the original SCALE paper [41], using the implementation released by the authors. We sample 64 points per patch at both training and inference to achieve the same number of output points as ours for a fair comparison. LBS. In the main paper Sec. 4.3, we compare with the Linear Blend Skinning (LBS) in the single scan animation task. This is done with the help of the SMPL [39] body model: we first optimize the SMPL body shape and pose parameters to fit a minimally-clothed body to the given scan, and then displace the vertices such that the final surface mesh aligns with the scan. The fitted clothed body model is then reposed by the target pose parameters.\n\n\nA.5. User Study\n\nWe conduct a large-scale user study on the Amazon Mechanical Turk to get a quantitative evaluation of the visual quality of our model outputs against the point-based method SCALE [41]. We evaluate over 6,000 unseen test examples in the CAPE and ReSynth datasets, from different subjects, performing different poses. For each example, the point cloud output from POP and SCALE are both rendered with a surfel-based renderer by Open3D [81] under the same rendering settings (an example of such rendering is the Fig. 1 in the main paper). We then present both images side-by-side to the users and ask them to choose the one that they deem a higher visual quality. The left-right ordering of the images is randomly shuffled for each example to avoid users' bias to a certain side. The users are required to zoom-in the images before they are able to make the evaluation, and we do not set a time limit for the viewing. Each image pair is evaluated by three users, and the final results in the main paper is averaged from all the user choices on all examples.\n\n\nB. Datasets\n\nReSynth. The 24 outfits designed by the clothing designer include varied types and styles: shirts, T-shirts, pants, shorts, skirts, long dresses, jackets, to name a few. For \n\n\nC. Extended Results and Discussions\n\nHere we provide extended analysis and discussions regarding the main paper Tab. 2. The implicit surface baseline, NASA [16], shows a much higher error than other methods. We find that it is majorly caused by the occasional missing body parts in its predictions. This happens more often for challenging, unseen body poses. The incomplete predictions thus lead to exceptionally high bi-directional Chamfer distance on a number of examples, hence a high average error.\n\nOur approach is based on the SCALE [41] baseline, but it achieves on average 11.4% (on CAPE data) and 9.1% (on ReSynth) lower errors than SCALE, with both margins being statistically significant (p-value\u226a1e-4 in the Wilcoxon signed-rank test). Together with the user study results in the main paper, this shows a consistent improvement on the representation power.\n\nIn Figs. E.2 and E.3 we show extended qualitative comparisons with NASA [16] and SCALE [41] from the pose generalization experiment (Sec. 4.1 in the main paper). Please refer to the supplementary video at https: //qianlim.github.io/POP for animated results.\n\n\nD. Run-time Comparison\n\nHere we compare the inference speed of POP with the implicit surface baseline, NASA [16], and the patch-based baseline, SCALE [41].\n\nTo generate a point cloud with 50K points, POP takes on average 48.8ms, and SCALE takes 42.4ms. The optional meshing step using the Poisson Reconstruction [32] takes 1.1s if a mesh is desired. Both explicit representations have comparable run-time performance. In contrast, NASA requires densely evaluating occupancy values over the space in order to reconstruct an explicit surface, which takes 12.2s per example. This shows the speed advantage of the explicit representations over the implicit ones.\n\n\nE. Limitations and Failure Cases\n\nAs discussed in the final section of the main paper, the major limitation of our approach lies on the use of the UV map. Although the UV maps are widely used to reconstruct and model human faces [21,43,70], one can encounter additional challenges when applying this technique to human bodies. On a full body UV map such as that of SMPL, different body parts are represented as separate \"islands\" on the UV map, see Fig. 2 in the main paper. Consequently, the output may suffer from discontinuities at the UV islands' boundaries. Qualitatively, this may occasionally result in visible \"seams\" between certain body parts as shown in Fig. E.1 (a-b), or an overly sparse distribution of points between the legs in the case of dresses as shown in Fig. E.1 (c), leading to sub-optimal performance when training a unified model for both pants and skirts.\n\nNote, however, that such discontinuities are not always the case. Intuitively, as the input UV positional map encodes (x, y, z) coordinates on the 3D body, the network can utilize not only the proximity in the UV space but also that in the original 3D space. We believe that the problem originates from the simple 2D convolution in the UNet pose encoder. A promising solution is to leverage a more continuous parameterization for the body surface manifold that is compatible with existing deep learning architectures. We leave this for future work. \n\n\nF. Further Discussions on Related Work\n\nHere we discuss the relationship of our method to recent work that uses similar techniques or that aims similar goals.\n\nWe represent clothing as a displacement field on the minimally-clothed body, in the canonical pose space. This helps factor out the effect of the articulated, rigid transformations that are directly accessible from the underlying posed body. In this way, the network can focus on the non-rigid, residual clothing deformation. Such technique is becoming increasingly popular for clothed human shape models that use meshes [42], point clouds [41], and implicit functions [10,37,48,65,72,75].\n\nOur shape decoder is a coordinate-based multi-layer perceptron (MLP), reminiscent of the recent line of work on neural implicit surfaces [11,44,50] and neural radiance fields [46]. These methods learn to map a neural feature at a given query location into a certain quantity, e.g. a signed distance [50], occupancy [44], color and volume density [46], or a displacement vector in our case. Our work differs from others majorly in that the querying coordinates live on a 2-manifold (the body surface) 1 instead of R 3 . Moreover, our point cloud representation belongs to the explicit representation category, retaining an advantage in the inference speed compared to the implicit methods. With the recent progress in differentiable and efficient surface reconstruction from point clouds [54,67], it becomes possible to flexibly convert between point clouds and meshes in various applications.\n\nRecent work on deformable face modeling [43] and pose-controlled free-view human synthesis [37] employ similar network architectures as ours, despite the difference in the goals, tasks and detailed techniques. While the commonality implies the efficacy of the high-level architectural design, it remains interesting to combine the detailed tech- 1 The concurrent work by Burov et al. [8] also maps the points on the SMPL body surface to a continuous clothing offset field, but the points' Euclidean coordinates (with positional encoding) are used to query the shape decoder. nical practices from each piece of work. It is also interesting to note the connection between our geometric feature tensor and the neural texture in recent work on neural rendering [60,69]: both concepts learn a spatial neural representation that controls the output, revealing a connection between modeling the 3D geometry and 2D appearances.\n\nFinally, in concurrent work, MetaAvatar [75] also learns a multi-subject model of clothed humans, which can generalize to unseen subjects using only a few depth images. Unlike our auto-decoding learning of the geometric feature tensor, MetaAvatar uses meta-learning to learn a prior of pose-dependent cloth deformation across different subjects and clothing types that helps generalize to unseen subjects and clothing types at test-time. We believe both approaches will inspire future work on cross-garment modeling and automatic avatar creation.\n\nNASA [16] SCALE [41], point cloud SCALE [41], \n\nFigure 3 :\n32D illustration of the point feature assignment for a region on the body manifold between the body basis points. Colors represent features. (a) The nearest neighbor assignment used by SCALE [41], causes \"patchy\" predictions. (b) Our bilinear feature interpolation results in smoothly varying features on the manifold.\n\nFigure 4 :\n4Comparison with SoTAs on the CAPE (upper 2 rows)\n\nFigure 5 :Figure 6 :\n56Comparison of animation with POP and an LBS-based method. The unseen scan from the CAPE dataset is animated on unseen motions. Animation of an unseen example from ReSynth (upper row) and a real-world capture (lower row) with unseen motions.\n\nFigure E. 1 :\n1Illustrations of our limitations.\n\nFigure E. 2 :Figure E. 3 :\n23Extended qualitative results from the pose generalization experiment (main paper Sec. 4.1), on the CAPE dataset. Best viewed zoomed-in on a color screen. Extended qualitative results from the pose generalization experiment (main paper Sec. 4.1), on the ReSynth dataset. Best viewed zoomed-in on a color screen. 8\n\nTable 2 :\n2Results of pose-dependent deformation prediction on unseen test sequences from the captured CAPE dataset and our ReSynth data. Best results are in boldface.Methods \n\nCAPE Data \nReSynth Data \nChamfer-L2 (\u00d710 \u22124 m 2 ) \u2193 \nNormal diff. (\u00d710 \u22121 ) \u2193 \nChamfer-L2 (\u00d710 \u22124 m 2 ) \u2193 \nNormal diff. (\u00d710 \u22121 ) \u2193 \n\nMean \nOutfit \nOutfit \nMean \nOutfit \nOutfit Mean \nOutfit \nOutfit \nMean \nOutfit \nOutfit \nMedian \nMax \nMedian \nMax \nMedian \nMax \nMedian \nMax \nNASA [16] \n6.087 \n1.190 \n32.35 \n1.275 \n1.277 \n1.497 \n-\n-\n-\n-\n-\n-\nSCALE [41] \n0.721 \n0.689 \n0.971 \n1.168 \n1.170 \n1.335 1.491 \n0.680 \n8.451 \n1.041 \n1.054 \n1.321 \nOurs, per-outfit 0.639 \n0.607 \n0.831 \n1.146 \n1.150 \n1.293 1.356 \n0.651 \n7.339 \n1.013 \n1.006 \n1.289 \nOurs, multi \n0.592 \n0.550 \n0.757 \n1.115 \n1.116 \n1.256 1.366 \n0.635 \n7.386 \n1.022 \n1.037 \n1.280 \n1/2 Data \n0.598 \n0.560 \n0.765 \n1.122 \n1.127 \n1.257 1.405 \n0.665 \n7.414 \n1.032 \n1.042 \n1.299 \n1/4 Data \n0.621 \n0.586 \n0.841 \n1.134 \n1.142 \n1.271 1.406 \n0.674 \n7.469 \n1.032 \n1.043 \n1.296 \n1/8 Data \n0.662 \n0.623 \n0.992 \n1.165 \n1.176 \n1.310 1.490 \n0.720 \n7.859 \n1.050 \n1.056 \n1.326 \n\npre-computing occupancy values for training, it is in gen-\neral not applicable to our synthetic data, which typically \ndoes not contain water-tight meshes. SCALE produces a \npoint cloud of clothed bodies, where the points are grouped \ninto local patches. Notably, both methods need to train a \nseparate model per outfit. Under the same outfit-specific \ntraining setting, POP outperforms both baselines on both \ndatasets under all metrics. We further conduct an AMT user \nstudy to evaluate the perceptual quality, comparing POP \nside-by-side against the strongest baseline, SCALE. On the \nCAPE data, 89.8% participants rate POP's results as hav-\ning \"higher visual quality\" than SCALE (10.2%); while on \nReSynth, 84.8% users favor POP over SCALE (15.2%). \n\n\n1 Scans\n1Designs Simulations Figure B.1: Examples from our ReSynth dataset. The clothing is designed based on real-world scans [61], draped on the SMPL-X [53] body model, and then simulated using Deform Dynamics [15]. each outfit, we find the SMPL-X [53] body (provided by the AGORA [52] dataset) that fits the subject's body shape in its corresponding original scan. We then use physicsbased simulation [15] to drape the clothing on the bodies and animate them with a consistent set of motion sequences of the subject 00096 from the CAPE dataset. The simulation results are inspected manually to remove problematic frames, resulting in 984 frames for training and 347 frames for test for each outfit. Examples from ReSynth are shown in Fig. B.1. We will release the dataset for research purposes. CAPE. The CAPE dataset[42] provides registered mesh pairs of (unclothed body, clothed body) of humans in clothing performing motion sequences. The three subjects (00096, 00215, 03375) that we use in the experiments have in total 14 outfits comprising short/long T-shirts, short/long pants, a dress shirt, a polo shirt, and a blazer. For each outfit, the motion sequences are randomly split into training (70%) and test (30%) sets.\n\nLearning representations and generative models for 3D point clouds. Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas, PMLRProceedings of Machine Learning and Systems (ICML). Machine Learning and Systems (ICML)23Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3D point clouds. In Proceedings of Machine Learning and Systems (ICML), pages 40-49. PMLR, 2018. 2, 3\n\nNeural point-based graphics. Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, Victor Lempitsky, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. Neural point-based graph- ics. In Proceedings of the European Conference on Com- puter Vision (ECCV), pages 696-712, 2020. 6\n\nAutomatic rigging and animation of 3D characters. Ilya Baran, Jovan Popovi\u0107, ACM Transactions on Graphics (TOG). 26372Ilya Baran and Jovan Popovi\u0107. Automatic rigging and an- imation of 3D characters. ACM Transactions on Graphics (TOG), 26(3):72-es, 2007. 1\n\nShape reconstruction by learning differentiable surface representations. Jan Bedna\u0159\u00edk, Shaifali Parashar, Erhan Gundogdu, Mathieu Salzmann, Pascal Fua, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)23Jan Bedna\u0159\u00edk, Shaifali Parashar, Erhan Gundogdu, Mathieu Salzmann, and Pascal Fua. Shape reconstruction by learning differentiable surface representations. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 4715-4724, 2020. 2, 3\n\nLoopReg: Self-supervised learning of implicit surface correspondences, pose and shape for 3D human mesh registration. Cristian Bharat Lal Bhatnagar, Christian Sminchisescu, Gerard Theobalt, Pons-Moll, Advances in Neural Information Processing Systems (NeurIPS). 5Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, and Gerard Pons-Moll. LoopReg: Self-supervised learning of implicit surface correspondences, pose and shape for 3D human mesh registration. In Advances in Neural Information Processing Systems (NeurIPS), pages 12909- 12922, 2020. 5, 8\n\nMulti-Garment Net: Learning to dress 3D people from images. Garvita Bharat Lal Bhatnagar, Christian Tiwari, Gerard Theobalt, Pons-Moll, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)24Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. Multi-Garment Net: Learning to dress 3D people from images. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 5420-5430, 2019. 2, 3, 4\n\nNeural deformation graphs for globally-consistent non-rigid reconstruction. Aljaz Bozic, Pablo Palafox, Michael Zollh\u00f6fer, Justus Thies, Angela Dai, Matthias Nie\u00dfner, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Aljaz Bozic, Pablo Palafox, Michael Zollh\u00f6fer, Justus Thies, Angela Dai, and Matthias Nie\u00dfner. Neural deformation graphs for globally-consistent non-rigid reconstruction. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 3\n\nDynamic surface function networks for clothed human bodies. Andrei Burov, Matthias Nie\u00dfner, Justus Thies, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)23Andrei Burov, Matthias Nie\u00dfner, and Justus Thies. Dynamic surface function networks for clothed human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 2, 3\n\n. Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, Richard Newcombe, Rohan Chabra, Jan E. Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, and Richard Newcombe.\n\nDeep local shapes: Learning local sdf priors for detailed 3D reconstruction. Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Deep local shapes: Learning local sdf priors for detailed 3D reconstruction. In Proceedings of the European Conference on Computer Vision (ECCV), pages 608-625, 2020. 4\n\nSNARF: Differentiable forward skinning for animating non-rigid neural implicit shapes. Xu Chen, Yufeng Zheng, J Michael, Otmar Black, Andreas Hilliges, Geiger, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges, and Andreas Geiger. SNARF: Differentiable forward skin- ning for animating non-rigid neural implicit shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 3\n\nLearning implicit fields for generative shape modeling. Zhiqin Chen, Hao Zhang, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 5939-5948, 2019. 3\n\nNeural unsigned distance fields for implicit function learning. Julian Chibane, Aymen Mir, Gerard Pons-Moll, Advances in Neural Information Processing Systems (NeurIPS). 2020Julian Chibane, Aymen Mir, and Gerard Pons-Moll. Neural unsigned distance fields for implicit function learning. In Ad- vances in Neural Information Processing Systems (NeurIPS), pages 21638-21652, 2020. 2\n\nSMPLicit: Topology-aware generative model for clothed people. Enric Corona, Albert Pumarola, Guillem Aleny\u00e0, Gerard Pons-Moll, Francesc Moreno-Noguer, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Enric Corona, Albert Pumarola, Guillem Aleny\u00e0, Ger- ard Pons-Moll, and Francesc Moreno-Noguer. SMPLicit: Topology-aware generative model for clothed people. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 11875-11885, 2021. 3\n\nStable spaces for real-time clothing. Leonid Edilson De Aguiar, Adrien Sigal, Jessica K Treuille, Hodgins, In ACM Transactions on Graphics. 293ACMEdilson De Aguiar, Leonid Sigal, Adrien Treuille, and Jes- sica K Hodgins. Stable spaces for real-time clothing. In ACM Transactions on Graphics (TOG), volume 29, page 106. ACM, 2010. 3\n\nNeural articulated shape approximation. Boyang Deng, Timothy Lewis, Gerard Jeruzalski, Geoffrey Pons-Moll, Mohammad Hinton, Andrea Norouzi, Tagliasacchi, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Boyang Deng, JP Lewis, Timothy Jeruzalski, Gerard Pons- Moll, Geoffrey Hinton, Mohammad Norouzi, and Andrea Tagliasacchi. Neural articulated shape approximation. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 612-628, 2020. 3, 6, 7, 1, 2\n\nBetter patch stitching for parametric surface reconstruction. Zhantao Deng, Jan Bedna\u0159\u00edk, Mathieu Salzmann, Pascal Fua, International Conference on 3D Vision (3DV). 23Zhantao Deng, Jan Bedna\u0159\u00edk, Mathieu Salzmann, and Pascal Fua. Better patch stitching for parametric surface reconstruc- tion. In International Conference on 3D Vision (3DV), pages 593-602, 2020. 2, 3\n\nLearning elementary structures for 3D shape generation and matching. Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir Kim, Bryan Russell, Mathieu Aubry, Advances in Neural Information Processing Systems (NeurIPS). 23Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir Kim, Bryan Russell, and Mathieu Aubry. Learning elemen- tary structures for 3D shape generation and matching. In Ad- vances in Neural Information Processing Systems (NeurIPS), pages 7433-7443, 2019. 2, 3\n\nA point set generation network for 3D object reconstruction from a single image. Haoqiang Fan, Hao Su, Leonidas J Guibas, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)23Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3D object reconstruction from a sin- gle image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 2463-2471, 2017. 2, 3\n\nAvatar reshaping and automatic rigging using a deformable model. Andrew Feng, Dan Casas, Ari Shapiro, Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games. the 8th ACM SIGGRAPH Conference on Motion in GamesAndrew Feng, Dan Casas, and Ari Shapiro. Avatar reshap- ing and automatic rigging using a deformable model. In Pro- ceedings of the 8th ACM SIGGRAPH Conference on Motion in Games, pages 57-64, 2015. 1\n\nJoint 3D face reconstruction and dense alignment with position map regression network. Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, Xi Zhou, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi Zhou. Joint 3D face reconstruction and dense alignment with position map regression network. In Proceedings of the Eu- ropean Conference on Computer Vision (ECCV), pages 534- 551, 2018. 2\n\nLocal deep implicit functions for 3D shape. Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicit functions for 3D shape. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 4857-4866, 2020. 4\n\n3D-CODED: 3D correspondences by deep deformation. Thibault Groueix, Matthew Fisher, G Vladimir, Kim, C Bryan, Mathieu Russell, Aubry, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)34Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry. 3D-CODED: 3D cor- respondences by deep deformation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 230-246, 2018. 3, 4\n\nA papier-m\u00e2ch\u00e9 approach to learning 3D surface generation. Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, Mathieu Aubry, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)24Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, and Mathieu Aubry. A papier-m\u00e2ch\u00e9 ap- proach to learning 3D surface generation. Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 216-224, 2018. 2, 3, 4\n\nDRAPE: DRessing Any PErson. Loretta Peng Guan, Reiss, A David, Alexander Hirshberg, Michael J Weiss, Black, ACM Transactions on Graphics (TOG). 3143Peng Guan, Loretta Reiss, David A Hirshberg, Alexander Weiss, and Michael J Black. DRAPE: DRessing Any PEr- son. ACM Transactions on Graphics (TOG), 31(4):35-1, 2012. 2, 3\n\nGarNet: A two-stream network for fast and accurate 3D cloth draping. Erhan Gundogdu, Victor Constantin, Amrollah Seifoddini, Minh Dang, Mathieu Salzmann, Pascal Fua, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)25Erhan Gundogdu, Victor Constantin, Amrollah Seifoddini, Minh Dang, Mathieu Salzmann, and Pascal Fua. GarNet: A two-stream network for fast and accurate 3D cloth draping. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 8739-8748, 2019. 2, 3, 5\n\nARCH++: Animation-ready clothed human reconstruction revisited. Yuanlu Tong He, Shunsuke Xu, Stefano Saito, Tony Soatto, Tung, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2021Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. ARCH++: Animation-ready clothed human re- construction revisited. In Proceedings of the IEEE/CVF In- ternational Conference on Computer Vision (ICCV), 2021. 3\n\nARCH: Animatable reconstruction of clothed humans. Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. ARCH: Animatable reconstruction of clothed humans. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 3093-3102, 2020. 3\n\nBCNet: Learning body and cloth shape from a single image. Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang Liu, Hujun Bao, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)SpringerBoyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang Liu, and Hujun Bao. BCNet: Learning body and cloth shape from a single image. In Proceedings of the European Con- ference on Computer Vision (ECCV), pages 18-35. Springer, 2020. 3\n\nLocal implicit grid representations for 3D scenes. Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, Thomas Funkhouser, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, and Thomas Funkhouser. Local implicit grid representations for 3D scenes. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 6001-6010, 2020. 4\n\nA pixel-based framework for data-driven clothing. Yilin Ning Jin, Zhenglin Zhu, Ronald Geng, Fedkiw, Computer Graphics Forum. 39Ning Jin, Yilin Zhu, Zhenglin Geng, and Ronald Fedkiw. A pixel-based framework for data-driven clothing. In Com- puter Graphics Forum, volume 39, pages 135-144, 2020. 2\n\nPoisson surface reconstruction. Michael Kazhdan, Matthew Bolitho, Hugues Hoppe, Proceedings of the fourth Eurographics Symposium on Geometry Processing. the fourth Eurographics Symposium on Geometry Processing6Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe. Poisson surface reconstruction. In Proceedings of the fourth Eurographics Symposium on Geometry Processing, vol- ume 7, 2006. 6, 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, International Conference on Learning Representations (ICLR). Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. 1\n\nTRANSPR: Transparency ray-accumulating neural 3D scene point renderer. Maria Kolos, Artem Sevastopolsky, Victor Lempitsky, International Conference on 3D Vision (3DV). Maria Kolos, Artem Sevastopolsky, and Victor Lempitsky. TRANSPR: Transparency ray-accumulating neural 3D scene point renderer. In International Conference on 3D Vision (3DV), pages 1167-1175, 2020. 6\n\nDeepWrinkles: Accurate and realistic clothing modeling. Zorah L\u00e4hner, Daniel Cremers, Tony Tung, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)23Zorah L\u00e4hner, Daniel Cremers, and Tony Tung. DeepWrin- kles: Accurate and realistic clothing modeling. In Pro- ceedings of the European Conference on Computer Vision (ECCV), pages 698-715, 2018. 2, 3\n\nLearning efficient point cloud generation for dense 3D object reconstruction. Chen-Hsuan Lin, Chen Kong, Simon Lucey, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)23Chen-Hsuan Lin, Chen Kong, and Simon Lucey. Learning efficient point cloud generation for dense 3D object recon- struction. In Proceedings of the AAAI Conference on Artifi- cial Intelligence (AAAI), pages 7114-7121, 2018. 2, 3\n\nNeural Actor: Neural free-view synthesis of human actors with pose control. Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt, arXiv preprint: 2106.02019, 2021. 3Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt. Neural Actor: Neural free-view synthesis of human actors with pose con- trol. arXiv preprint: 2106.02019, 2021. 3\n\nNeuroSkinning: Automatic skin binding for production characters with deep graph networks. Lijuan Liu, Youyi Zheng, Di Tang, Yi Yuan, Changjie Fan, Kun Zhou, ACM Transactions on Graphics (TOG). 384Lijuan Liu, Youyi Zheng, Di Tang, Yi Yuan, Changjie Fan, and Kun Zhou. NeuroSkinning: Automatic skin binding for production characters with deep graph networks. ACM Transactions on Graphics (TOG), 38(4):1-12, 2019. 1\n\nSMPL: A skinned multiperson linear model. Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, Michael J Black, ACM Transactions on Graphics (TOG). 3461Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. SMPL: A skinned multi- person linear model. ACM Transactions on Graphics (TOG), 34(6):248, 2015. 6, 8, 1\n\nMarching cubes: A high resolution 3D surface construction algorithm. E William, Harvey E Lorensen, Cline, ACM SIGGRAPH Computer Graphics. 211William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3D surface construction algorithm. In ACM SIGGRAPH Computer Graphics, volume 21, pages 163- 169, 1987. 6, 1\n\nSCALE: Modeling clothed humans with a surface codec of articulated local elements. Qianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang, Michael J Black, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)Qianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang, and Michael J. Black. SCALE: Modeling clothed humans with a surface codec of articulated local elements. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recogni- tion (CVPR), June 2021. 2, 3, 4, 5, 6, 7, 1, 8\n\nLearning to dress 3D people in generative clothing. Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, Michael J Black, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)56Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J. Black. Learn- ing to dress 3D people in generative clothing. In Proceed- ings IEEE Conf. on Computer Vision and Pattern Recogni- tion (CVPR), pages 6468-6477, 2020. 2, 3, 4, 5, 6\n\nPixel codec avatars. Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La, Torre , Yaser Sheikh, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)23Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De la Torre, and Yaser Sheikh. Pixel codec avatars. In Proceedings IEEE Conf. on Computer Vi- sion and Pattern Recognition (CVPR), pages 64-73, June 2021. 2, 3\n\nOccupancy networks: Learning 3D reconstruction in function space. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)23Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- bastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3D reconstruction in function space. In Proceed- ings IEEE Conf. on Computer Vision and Pattern Recogni- tion (CVPR), pages 4460-4470, 2019. 2, 3\n\nLEAP: Learning articulated occupancy of people. Marko Mihajlovic, Yan Zhang, Michael J Black, Siyu Tang, 2021. 3Proceedings IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionMarko Mihajlovic, Yan Zhang, Michael J. Black, and Siyu Tang. LEAP: Learning articulated occupancy of people. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 3\n\nNeRF: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)SpringerBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view syn- thesis. In Proceedings of the European Conference on Com- puter Vision (ECCV), pages 405-421. Springer, 2020. 3\n\nA layered model of human body and garment deformation. Alexandros Neophytou, Adrian Hilton, International Conference on 3D Vision (3DV). 23Alexandros Neophytou and Adrian Hilton. A layered model of human body and garment deformation. In International Conference on 3D Vision (3DV), pages 171-178, 2014. 2, 3\n\nNeural parametric models for 3D deformable shapes. Pablo Palafox, Aljaz Bozic, Justus Thies, Matthias Nie\u00dfner, Angela Dai, 2021. 3Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV. the IEEE/CVF International Conference on Computer Vision (ICCVPablo Palafox, Aljaz Bozic, Justus Thies, Matthias Nie\u00dfner, and Angela Dai. Neural parametric models for 3D de- formable shapes. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision (ICCV), 2021. 3\n\nDeep mesh reconstruction from single RGB images via topology modification networks. Junyi Pan, Xiaoguang Han, Weikai Chen, Jiapeng Tang, Kui Jia, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Junyi Pan, Xiaoguang Han, Weikai Chen, Jiapeng Tang, and Kui Jia. Deep mesh reconstruction from single RGB im- ages via topology modification networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9963-9972, 2019. 2\n\nDeepSDF: Learning continuous signed distance functions for shape representation. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)13Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representa- tion. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 165-174, 2019. 2, 5, 1, 3\n\nTailorNet: Predicting clothing in 3D as a function of human pose, shape and garment style. Chaitanya Patel, Zhouyingcheng Liao, Gerard Pons-Moll, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)25Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons- Moll. TailorNet: Predicting clothing in 3D as a function of human pose, shape and garment style. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 7363-7373, 2020. 2, 3, 5\n\nAGORA: Avatars in geography optimized for regression analysis. Priyanka Patel, Chun-Hao Huang Paul, Joachim Tesch, David Hoffmann, Shashank Tripathi, Michael J Black, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)6Priyanka Patel, Chun-Hao Huang Paul, Joachim Tesch, David Hoffmann, Shashank Tripathi, and Michael J. Black. AGORA: Avatars in geography optimized for regression analysis. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 13468-13478, June 2021. 6, 2\n\nExpressive body capture: 3D hands, face, and body from a single image. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, A A Ahmed, Dimitrios Osman, Michael J Tzionas, Black, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)1Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from a single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 10975-10985, 2019. 1, 2\n\nSongyou Peng, Chiyu Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, Andreas Geiger, Shape as points: A differentiable poisson solver. arXiv preprint: 2106.03452, 2021. 3Songyou Peng, Chiyu Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas Geiger. Shape as points: A differentiable poisson solver. arXiv preprint: 2106.03452, 2021. 3\n\nConvolutional occupancy networks. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 523-540, 2020. 4\n\nNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceed- ings IEEE Conf. on Computer Vision and Pattern Recogni- tion (CVPR), pages 9054-9063, June 2021. 3\n\nSurfels: Surface elements as rendering primitives. Hanspeter Pfister, Matthias Zwicker, Jeroen Van Baar, Markus Gross, Proceedings of the 27th annual conference on Computer graphics and interactive techniques. the 27th annual conference on Computer graphics and interactive techniquesHanspeter Pfister, Matthias Zwicker, Jeroen Van Baar, and Markus Gross. Surfels: Surface elements as rendering primi- tives. In Proceedings of the 27th annual conference on Com- puter graphics and interactive techniques, pages 335-342, 2000. 7\n\nClothCap: Seamless 4D clothing capture and retargeting. Gerard Pons-Moll, Sergi Pujades, Sonny Hu, Michael J Black, ACM Transactions on Graphics (TOG). 36473Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black. ClothCap: Seamless 4D clothing capture and retar- geting. ACM Transactions on Graphics (TOG), 36(4):73, 2017. 4\n\nSM-PLpix: Neural avatars from 3D human models. Sergey Prokudin, Michael J Black, Javier Romero, Winter Conference on Applications of Computer Vision (WACV). Sergey Prokudin, Michael J. Black, and Javier Romero. SM- PLpix: Neural avatars from 3D human models. In Win- ter Conference on Applications of Computer Vision (WACV), 2021. 6\n\nANR: Articulated neural rendering for virtual avatars. Amit Raj, Julian Tanke, James Hays, Minh Vo, Carsten Stoll, Christoph Lassner, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Amit Raj, Julian Tanke, James Hays, Minh Vo, Carsten Stoll, and Christoph Lassner. ANR: Articulated neural rendering for virtual avatars. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 3722-3731, June 2021. 3\n\n. Renderpeople, 8Renderpeople, 2020. https://renderpeople.com. 6, 8, 2\n\nU-Net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). 51Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- Net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical Image Com- puting and Computer-Assisted Intervention (MICCAI), pages 234-241, 2015. 5, 1\n\nPIFu: Pixel-aligned implicit function for high-resolution clothed human digitization. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor- ishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-aligned implicit function for high-resolution clothed human digitiza- tion. In Proceedings of the IEEE/CVF International Confer- ence on Computer Vision (ICCV), pages 2304-2314, 2019. 3\n\nPIFuHD: Multi-level pixel-aligned implicit function for high-resolution 3D human digitization. Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. PIFuHD: Multi-level pixel-aligned implicit function for high-resolution 3D human digitization. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 84-93, 2020. 3\n\nSCANimate: Weakly supervised learning of skinned clothed avatar networks. Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J Black, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J. Black. SCANimate: Weakly supervised learning of skinned clothed avatar networks. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), June 2021. 1, 2, 3, 5\n\nLearning-Based Animation of Clothing for Virtual Try-On. Igor Santesteban, Miguel A Otaduy, Dan Casas, Computer Graphics Forum. 3825Igor Santesteban, Miguel A. Otaduy, and Dan Casas. Learning-Based Animation of Clothing for Virtual Try-On. Computer Graphics Forum, 38(2):355-366, 2019. 2, 3, 5\n\nPointTriNet: Learned triangulation of 3D point sets. Nicholas Sharp, Maks Ovsjanikov, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020Nicholas Sharp and Maks Ovsjanikov. PointTriNet: Learned triangulation of 3D point sets. In Proceedings of the Euro- pean Conference on Computer Vision (ECCV), pages 762- 778, 2020. 3\n\nGan-based garment generation using sewing pattern images. Yu Shen, Junbang Liang, Ming C Lin, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)1Yu Shen, Junbang Liang, and Ming C Lin. Gan-based garment generation using sewing pattern images. In Pro- ceedings of the European Conference on Computer Vision (ECCV), volume 1, page 3, 2020. 3\n\nDeferred neural rendering: Image synthesis using neural textures. Justus Thies, Michael Zollh\u00f6fer, Matthias Nie\u00dfner, ACM Transactions on Graphics (TOG). 384Justus Thies, Michael Zollh\u00f6fer, and Matthias Nie\u00dfner. De- ferred neural rendering: Image synthesis using neural tex- tures. ACM Transactions on Graphics (TOG), 38(4):1-12, 2019. 3\n\nAugmented blendshapes for real-time simultaneous 3D head modeling and facial motion capture. Diego Thomas, Rin-Ichiro Taniguchi, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Diego Thomas and Rin-Ichiro Taniguchi. Augmented blend- shapes for real-time simultaneous 3D head modeling and fa- cial motion capture. In Proceedings IEEE Conf. on Com- puter Vision and Pattern Recognition (CVPR), pages 3299- 3308, June 2016. 2\n\nSIZER: A dataset and model for parsing 3D clothing and learning size sensitive 3D clothing. Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, Gerard Pons-Moll, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)123483Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Ger- ard Pons-Moll. SIZER: A dataset and model for parsing 3D clothing and learning size sensitive 3D clothing. In Pro- ceedings of the European Conference on Computer Vision (ECCV), volume 12348, pages 1-18, 2020. 2, 3\n\nNeural-GIF: Neural generalized implicit functions for animating people in clothing. Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, Gerard Pons-Moll, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, and Gerard Pons-Moll. Neural-GIF: Neural generalized implicit func- tions for animating people in clothing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021. 3\n\nFully convolutional graph neural networks for parametric virtual try-on. Raquel Vidaurre, Igor Santesteban, Elena Garces, Dan Casas, Computer Graphics Forum. Wiley Online Library395Raquel Vidaurre, Igor Santesteban, Elena Garces, and Dan Casas. Fully convolutional graph neural networks for para- metric virtual try-on. In Computer Graphics Forum, vol- ume 39, pages 145-156. Wiley Online Library, 2020. 3, 5\n\nLocally aware piecewise transformation fields for 3D human mesh registration. Shaofei Wang, Andreas Geiger, Siyu Tang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Shaofei Wang, Andreas Geiger, and Siyu Tang. Locally aware piecewise transformation fields for 3D human mesh registration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7639-7648, June 2021. 8\n\nMetaAvatar: Learning animatable clothed human models from few depth images. Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, Siyu Tang, arXiv preprint:2106.11944, 2021. 3Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, and Siyu Tang. MetaAvatar: Learning animat- able clothed human models from few depth images. arXiv preprint:2106.11944, 2021. 3\n\nLearning a shared shape space for multimodal garment design. Y Tuanfeng, Duygu Wang, Jovan Ceylan, Niloy J Popovi\u0107, Mitra, ACM Transactions on Graphics (TOG). Tuanfeng Y Wang, Duygu Ceylan, Jovan Popovi\u0107, and Niloy J Mitra. Learning a shared shape space for multimodal garment design. In ACM Transactions on Graphics (TOG), pages 203-216, 2018. 3\n\nAnalyzing clothing layer deformation statistics of 3D human motions. Jinlong Yang, Jean-S\u00e9bastien Franco, Franck H\u00e9troy-Wheeler, Stefanie Wuhrer, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)35Jinlong Yang, Jean-S\u00e9bastien Franco, Franck H\u00e9troy- Wheeler, and Stefanie Wuhrer. Analyzing clothing layer de- formation statistics of 3D human motions. In Proceedings of the European Conference on Computer Vision (ECCV), pages 237-253, 2018. 3, 5\n\nPhysics-inspired garment recovery from a single-view image. Shan Yang, Zherong Pan, Tanya Amert, Ke Wang, Licheng Yu, Tamara Berg, Ming C Lin, ACM Transactions on Graphics (TOG). 375Shan Yang, Zherong Pan, Tanya Amert, Ke Wang, Licheng Yu, Tamara Berg, and Ming C Lin. Physics-inspired garment recovery from a single-view image. ACM Transactions on Graphics (TOG), 37(5):1-14, 2018. 2\n\nErsin Yumer, and Raquel Urtasun. S3: Neural shape, skeleton, and skinning fields for 3D human modeling. Ze Yang, Shenlong Wang, Siva Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Ze Yang, Shenlong Wang, Siva Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, and Raquel Ur- tasun. S3: Neural shape, skeleton, and skinning fields for 3D human modeling. In Proceedings IEEE Conf. on Com- puter Vision and Pattern Recognition (CVPR), pages 13284- 13293, 2021. 3\n\nPaMIR: Parametric model-conditioned implicit representation for image-based human reconstruction. Zerong Zheng, Tao Yu, Yebin Liu, Qionghai Dai, IEEE Transactions on Pattern Analysis and Machine Intelegence. 3Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. PaMIR: Parametric model-conditioned implicit representa- tion for image-based human reconstruction. IEEE Transac- tions on Pattern Analysis and Machine Intelegence, 2021. 3\n\nOpen3D: A modern library for 3D data processing. Qian-Yi Zhou, Jaesik Park, Vladlen Koltun, arXiv preprint: 1801.09847Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A modern library for 3D data processing. arXiv preprint: 1801.09847, 2018. 1\n\nDeep Fashion3D: A dataset and benchmark for 3D garment reconstruction from single images. Heming Zhu, Yu Cao, Hang Jin, Weikai Chen, Dong Du, Zhangye Wang, Shuguang Cui, Xiaoguang Han, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)123463Heming Zhu, Yu Cao, Hang Jin, Weikai Chen, Dong Du, Zhangye Wang, Shuguang Cui, and Xiaoguang Han. Deep Fashion3D: A dataset and benchmark for 3D garment re- construction from single images. In Proceedings of the European Conference on Computer Vision (ECCV), volume 12346, pages 512-530, 2020. 2, 3\n", "annotations": {"author": "[{\"end\":251,\"start\":144},{\"end\":350,\"start\":252},{\"end\":396,\"start\":351},{\"end\":498,\"start\":397}]", "publisher": null, "author_last_name": "[{\"end\":153,\"start\":151},{\"end\":264,\"start\":260},{\"end\":360,\"start\":356},{\"end\":412,\"start\":407}]", "author_first_name": "[{\"end\":150,\"start\":144},{\"end\":259,\"start\":252},{\"end\":355,\"start\":351},{\"end\":404,\"start\":397},{\"end\":406,\"start\":405}]", "author_affiliation": "[{\"end\":237,\"start\":177},{\"end\":250,\"start\":239},{\"end\":349,\"start\":289},{\"end\":395,\"start\":384},{\"end\":497,\"start\":437}]", "title": "[{\"end\":141,\"start\":1},{\"end\":639,\"start\":499}]", "venue": null, "abstract": "[{\"end\":2884,\"start\":641}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b64\"},\"end\":3160,\"start\":3156},{\"end\":3530,\"start\":3527},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3533,\"start\":3530},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3536,\"start\":3533},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4138,\"start\":4134},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4141,\"start\":4138},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4144,\"start\":4141},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4147,\"start\":4144},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4150,\"start\":4147},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":4153,\"start\":4150},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5321,\"start\":5318},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5324,\"start\":5321},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5327,\"start\":5324},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5491,\"start\":5487},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5979,\"start\":5976},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5982,\"start\":5979},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5985,\"start\":5982},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5988,\"start\":5985},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5991,\"start\":5988},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6822,\"start\":6818},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8524,\"start\":8521},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8526,\"start\":8524},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8529,\"start\":8526},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8532,\"start\":8529},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":8535,\"start\":8532},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":8538,\"start\":8535},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8583,\"start\":8579},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8586,\"start\":8583},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8589,\"start\":8586},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8592,\"start\":8589},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":8595,\"start\":8592},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8683,\"start\":8679},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8719,\"start\":8716},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8722,\"start\":8719},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8752,\"start\":8748},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8755,\"start\":8752},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9102,\"start\":9098},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":9105,\"start\":9102},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9292,\"start\":9288},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9295,\"start\":9292},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9298,\"start\":9295},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9462,\"start\":9459},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9465,\"start\":9462},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9468,\"start\":9465},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9471,\"start\":9468},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9474,\"start\":9471},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9477,\"start\":9474},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":9480,\"start\":9477},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":9483,\"start\":9480},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9501,\"start\":9497},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9504,\"start\":9501},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9507,\"start\":9504},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9510,\"start\":9507},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9513,\"start\":9510},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":9516,\"start\":9513},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10060,\"start\":10057},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10063,\"start\":10060},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10066,\"start\":10063},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10089,\"start\":10086},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10092,\"start\":10089},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10095,\"start\":10092},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10098,\"start\":10095},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10231,\"start\":10227},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10388,\"start\":10384},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10407,\"start\":10403},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11111,\"start\":11107},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":11114,\"start\":11111},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":11117,\"start\":11114},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11261,\"start\":11257},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11264,\"start\":11261},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11267,\"start\":11264},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11270,\"start\":11267},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11273,\"start\":11270},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11276,\"start\":11273},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":11279,\"start\":11276},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":11282,\"start\":11279},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11331,\"start\":11327},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11334,\"start\":11331},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":11337,\"start\":11334},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":11340,\"start\":11337},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11386,\"start\":11382},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11389,\"start\":11386},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11392,\"start\":11389},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11395,\"start\":11392},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11398,\"start\":11395},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":11401,\"start\":11398},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":11404,\"start\":11401},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11653,\"start\":11649},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":11656,\"start\":11653},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11670,\"start\":11666},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":11673,\"start\":11670},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11738,\"start\":11734},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11741,\"start\":11738},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12083,\"start\":12080},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12095,\"start\":12091},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":12119,\"start\":12115},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12211,\"start\":12207},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12614,\"start\":12610},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12626,\"start\":12622},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12639,\"start\":12635},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12658,\"start\":12654},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12670,\"start\":12666},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12686,\"start\":12682},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12702,\"start\":12698},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":12718,\"start\":12714},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":12736,\"start\":12732},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":12748,\"start\":12744},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":12759,\"start\":12755},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":12770,\"start\":12766},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12793,\"start\":12790},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12805,\"start\":12801},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12816,\"start\":12812},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":12831,\"start\":12827},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":12851,\"start\":12847},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12883,\"start\":12879},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":12894,\"start\":12890},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":13197,\"start\":13193},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13527,\"start\":13523},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14741,\"start\":14737},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14744,\"start\":14741},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15711,\"start\":15707},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16201,\"start\":16197},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16204,\"start\":16201},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16231,\"start\":16227},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16382,\"start\":16378},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16415,\"start\":16411},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16671,\"start\":16668},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16674,\"start\":16671},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16677,\"start\":16674},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16680,\"start\":16677},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16702,\"start\":16698},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16822,\"start\":16818},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17016,\"start\":17012},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17497,\"start\":17493},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18701,\"start\":18698},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18704,\"start\":18701},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":18707,\"start\":18704},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19516,\"start\":19513},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20279,\"start\":20275},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20372,\"start\":20368},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20375,\"start\":20372},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":20378,\"start\":20375},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":20722,\"start\":20718},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21430,\"start\":21426},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21433,\"start\":21430},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":21436,\"start\":21433},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":21439,\"start\":21436},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22214,\"start\":22210},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22697,\"start\":22693},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":22700,\"start\":22697},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":22703,\"start\":22700},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":22706,\"start\":22703},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25963,\"start\":25959},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26063,\"start\":26060},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26066,\"start\":26063},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":26069,\"start\":26066},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26908,\"start\":26904},{\"end\":27087,\"start\":27076},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":27431,\"start\":27427},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":27611,\"start\":27607},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27921,\"start\":27917},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":27936,\"start\":27932},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28292,\"start\":28288},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28630,\"start\":28626},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29642,\"start\":29638},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29657,\"start\":29653},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31497,\"start\":31493},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32933,\"start\":32929},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":33426,\"start\":33422},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34708,\"start\":34705},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":34711,\"start\":34708},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":35300,\"start\":35296},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":35332,\"start\":35328},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":35509,\"start\":35505},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":35830,\"start\":35826},{\"end\":36711,\"start\":36706},{\"end\":36715,\"start\":36711},{\"end\":36719,\"start\":36715},{\"end\":36723,\"start\":36719},{\"end\":36727,\"start\":36723},{\"end\":36731,\"start\":36727},{\"end\":36735,\"start\":36731},{\"end\":36737,\"start\":36735},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":36812,\"start\":36808},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37136,\"start\":37132},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37987,\"start\":37983},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":38165,\"start\":38161},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38419,\"start\":38415},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":38766,\"start\":38762},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39253,\"start\":39249},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":39507,\"start\":39503},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40477,\"start\":40473},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":40860,\"start\":40856},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41263,\"start\":41259},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41278,\"start\":41274},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41559,\"start\":41555},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41601,\"start\":41597},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":42341,\"start\":42337},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":42344,\"start\":42341},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":42347,\"start\":42344},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":44128,\"start\":44124},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":44147,\"start\":44143},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":44176,\"start\":44172},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":44179,\"start\":44176},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":44182,\"start\":44179},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":44185,\"start\":44182},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":44188,\"start\":44185},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":44191,\"start\":44188},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":44335,\"start\":44331},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":44338,\"start\":44335},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":44341,\"start\":44338},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":44373,\"start\":44369},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":44497,\"start\":44493},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":44513,\"start\":44509},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":44544,\"start\":44540},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":44985,\"start\":44981},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":44988,\"start\":44985},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":45132,\"start\":45128},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":45183,\"start\":45179},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":45435,\"start\":45434},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":45475,\"start\":45472},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":45849,\"start\":45845},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":45852,\"start\":45849},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":46053,\"start\":46049},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":46566,\"start\":46562},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":46577,\"start\":46573},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":46601,\"start\":46597},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":50323,\"start\":50319}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46934,\"start\":46604},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46996,\"start\":46935},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47261,\"start\":46997},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47311,\"start\":47262},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47654,\"start\":47312},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":49497,\"start\":47655},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50727,\"start\":49498}]", "paragraph": "[{\"end\":3836,\"start\":2900},{\"end\":4323,\"start\":3838},{\"end\":5463,\"start\":4325},{\"end\":6186,\"start\":5465},{\"end\":7341,\"start\":6188},{\"end\":7701,\"start\":7343},{\"end\":8197,\"start\":7703},{\"end\":9261,\"start\":8214},{\"end\":9885,\"start\":9263},{\"end\":11157,\"start\":9887},{\"end\":11953,\"start\":11159},{\"end\":12580,\"start\":11955},{\"end\":12771,\"start\":12600},{\"end\":12907,\"start\":12773},{\"end\":12993,\"start\":12909},{\"end\":13917,\"start\":12995},{\"end\":14318,\"start\":13928},{\"end\":14667,\"start\":14320},{\"end\":15712,\"start\":14709},{\"end\":16026,\"start\":15759},{\"end\":17017,\"start\":16028},{\"end\":18368,\"start\":17019},{\"end\":18829,\"start\":18370},{\"end\":19067,\"start\":18831},{\"end\":19517,\"start\":19095},{\"end\":21070,\"start\":19564},{\"end\":21921,\"start\":21072},{\"end\":22570,\"start\":21923},{\"end\":23198,\"start\":22572},{\"end\":23602,\"start\":23235},{\"end\":23861,\"start\":23629},{\"end\":24230,\"start\":23919},{\"end\":24424,\"start\":24314},{\"end\":24588,\"start\":24426},{\"end\":24932,\"start\":24652},{\"end\":25440,\"start\":24990},{\"end\":25675,\"start\":25442},{\"end\":26362,\"start\":25703},{\"end\":28906,\"start\":26378},{\"end\":30461,\"start\":28931},{\"end\":31007,\"start\":30487},{\"end\":31539,\"start\":31009},{\"end\":32439,\"start\":31541},{\"end\":33825,\"start\":32465},{\"end\":34480,\"start\":33840},{\"end\":35213,\"start\":34482},{\"end\":35680,\"start\":35280},{\"end\":36221,\"start\":35682},{\"end\":36492,\"start\":36223},{\"end\":37087,\"start\":36494},{\"end\":37614,\"start\":37105},{\"end\":37933,\"start\":37639},{\"end\":39050,\"start\":37952},{\"end\":40124,\"start\":39070},{\"end\":40314,\"start\":40140},{\"end\":40819,\"start\":40354},{\"end\":41185,\"start\":40821},{\"end\":41444,\"start\":41187},{\"end\":41602,\"start\":41471},{\"end\":42105,\"start\":41604},{\"end\":42989,\"start\":42142},{\"end\":43540,\"start\":42991},{\"end\":43701,\"start\":43583},{\"end\":44192,\"start\":43703},{\"end\":45086,\"start\":44194},{\"end\":46007,\"start\":45088},{\"end\":46555,\"start\":46009},{\"end\":46603,\"start\":46557}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15758,\"start\":15713},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19094,\"start\":19068},{\"attributes\":{\"id\":\"formula_2\"},\"end\":23234,\"start\":23199},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23918,\"start\":23862},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24313,\"start\":24231},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24651,\"start\":24589},{\"attributes\":{\"id\":\"formula_6\"},\"end\":24989,\"start\":24933},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25702,\"start\":25676}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12486,\"start\":12479}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2898,\"start\":2886},{\"attributes\":{\"n\":\"2.\"},\"end\":8212,\"start\":8200},{\"end\":12598,\"start\":12583},{\"attributes\":{\"n\":\"3.\"},\"end\":13926,\"start\":13920},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14707,\"start\":14670},{\"attributes\":{\"n\":\"3.2.\"},\"end\":19562,\"start\":19520},{\"attributes\":{\"n\":\"3.3.\"},\"end\":23627,\"start\":23605},{\"attributes\":{\"n\":\"4.\"},\"end\":26376,\"start\":26365},{\"attributes\":{\"n\":\"4.1.\"},\"end\":28929,\"start\":28909},{\"attributes\":{\"n\":\"4.2.\"},\"end\":30485,\"start\":30464},{\"attributes\":{\"n\":\"4.3.\"},\"end\":32463,\"start\":32442},{\"attributes\":{\"n\":\"5.\"},\"end\":33838,\"start\":33828},{\"end\":35224,\"start\":35216},{\"end\":35252,\"start\":35227},{\"end\":35278,\"start\":35255},{\"end\":37103,\"start\":37090},{\"end\":37637,\"start\":37617},{\"end\":37950,\"start\":37936},{\"end\":39068,\"start\":39053},{\"end\":40138,\"start\":40127},{\"end\":40352,\"start\":40317},{\"end\":41469,\"start\":41447},{\"end\":42140,\"start\":42108},{\"end\":43581,\"start\":43543},{\"end\":46615,\"start\":46605},{\"end\":46946,\"start\":46936},{\"end\":47018,\"start\":46998},{\"end\":47276,\"start\":47263},{\"end\":47339,\"start\":47313},{\"end\":47665,\"start\":47656},{\"end\":49506,\"start\":49499}]", "table": "[{\"end\":49497,\"start\":47823}]", "figure_caption": "[{\"end\":46934,\"start\":46617},{\"end\":46996,\"start\":46948},{\"end\":47261,\"start\":47021},{\"end\":47311,\"start\":47278},{\"end\":47654,\"start\":47342},{\"end\":47823,\"start\":47667},{\"end\":50727,\"start\":49508}]", "figure_ref": "[{\"end\":14344,\"start\":14338},{\"end\":15060,\"start\":15052},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17401,\"start\":17395},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17489,\"start\":17483},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18146,\"start\":18140},{\"end\":20006,\"start\":20000},{\"end\":20398,\"start\":20392},{\"end\":21920,\"start\":21914},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29839,\"start\":29833},{\"end\":32791,\"start\":32783},{\"end\":33305,\"start\":33299},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":39585,\"start\":39579},{\"end\":42563,\"start\":42557},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":42787,\"start\":42773},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":42896,\"start\":42884}]", "bib_author_first_name": "[{\"end\":50802,\"start\":50797},{\"end\":50819,\"start\":50815},{\"end\":50837,\"start\":50830},{\"end\":50858,\"start\":50850},{\"end\":51225,\"start\":51217},{\"end\":51238,\"start\":51233},{\"end\":51259,\"start\":51254},{\"end\":51273,\"start\":51267},{\"end\":51289,\"start\":51283},{\"end\":51684,\"start\":51680},{\"end\":51697,\"start\":51692},{\"end\":51964,\"start\":51961},{\"end\":51983,\"start\":51975},{\"end\":51999,\"start\":51994},{\"end\":52017,\"start\":52010},{\"end\":52034,\"start\":52028},{\"end\":52564,\"start\":52556},{\"end\":52596,\"start\":52587},{\"end\":52617,\"start\":52611},{\"end\":53069,\"start\":53062},{\"end\":53101,\"start\":53092},{\"end\":53116,\"start\":53110},{\"end\":53619,\"start\":53614},{\"end\":53632,\"start\":53627},{\"end\":53649,\"start\":53642},{\"end\":53667,\"start\":53661},{\"end\":53681,\"start\":53675},{\"end\":53695,\"start\":53687},{\"end\":54162,\"start\":54156},{\"end\":54178,\"start\":54170},{\"end\":54194,\"start\":54188},{\"end\":54559,\"start\":54554},{\"end\":54571,\"start\":54568},{\"end\":54573,\"start\":54572},{\"end\":54587,\"start\":54583},{\"end\":54599,\"start\":54593},{\"end\":54615,\"start\":54609},{\"end\":54630,\"start\":54624},{\"end\":54649,\"start\":54642},{\"end\":55223,\"start\":55221},{\"end\":55236,\"start\":55230},{\"end\":55245,\"start\":55244},{\"end\":55260,\"start\":55255},{\"end\":55275,\"start\":55268},{\"end\":55756,\"start\":55750},{\"end\":55766,\"start\":55763},{\"end\":56164,\"start\":56158},{\"end\":56179,\"start\":56174},{\"end\":56191,\"start\":56185},{\"end\":56542,\"start\":56537},{\"end\":56557,\"start\":56551},{\"end\":56575,\"start\":56568},{\"end\":56590,\"start\":56584},{\"end\":56610,\"start\":56602},{\"end\":57066,\"start\":57060},{\"end\":57092,\"start\":57086},{\"end\":57107,\"start\":57100},{\"end\":57109,\"start\":57108},{\"end\":57401,\"start\":57395},{\"end\":57415,\"start\":57408},{\"end\":57429,\"start\":57423},{\"end\":57450,\"start\":57442},{\"end\":57470,\"start\":57462},{\"end\":57485,\"start\":57479},{\"end\":57962,\"start\":57955},{\"end\":57972,\"start\":57969},{\"end\":57990,\"start\":57983},{\"end\":58007,\"start\":58001},{\"end\":58334,\"start\":58330},{\"end\":58353,\"start\":58345},{\"end\":58370,\"start\":58363},{\"end\":58387,\"start\":58379},{\"end\":58398,\"start\":58393},{\"end\":58415,\"start\":58408},{\"end\":58838,\"start\":58830},{\"end\":58847,\"start\":58844},{\"end\":58860,\"start\":58852},{\"end\":58862,\"start\":58861},{\"end\":59312,\"start\":59306},{\"end\":59322,\"start\":59319},{\"end\":59333,\"start\":59330},{\"end\":59752,\"start\":59749},{\"end\":59762,\"start\":59759},{\"end\":59773,\"start\":59767},{\"end\":59787,\"start\":59780},{\"end\":59796,\"start\":59794},{\"end\":60207,\"start\":60203},{\"end\":60225,\"start\":60216},{\"end\":60239,\"start\":60232},{\"end\":60250,\"start\":60245},{\"end\":60264,\"start\":60258},{\"end\":60694,\"start\":60686},{\"end\":60711,\"start\":60704},{\"end\":60721,\"start\":60720},{\"end\":60738,\"start\":60737},{\"end\":60753,\"start\":60746},{\"end\":61188,\"start\":61180},{\"end\":61205,\"start\":61198},{\"end\":61222,\"start\":61214},{\"end\":61224,\"start\":61223},{\"end\":61235,\"start\":61230},{\"end\":61237,\"start\":61236},{\"end\":61254,\"start\":61247},{\"end\":61686,\"start\":61679},{\"end\":61706,\"start\":61705},{\"end\":61723,\"start\":61714},{\"end\":61744,\"start\":61735},{\"end\":62046,\"start\":62041},{\"end\":62063,\"start\":62057},{\"end\":62084,\"start\":62076},{\"end\":62101,\"start\":62097},{\"end\":62115,\"start\":62108},{\"end\":62132,\"start\":62126},{\"end\":62623,\"start\":62617},{\"end\":62641,\"start\":62633},{\"end\":62653,\"start\":62646},{\"end\":62665,\"start\":62661},{\"end\":63109,\"start\":63105},{\"end\":63123,\"start\":63117},{\"end\":63137,\"start\":63128},{\"end\":63150,\"start\":63147},{\"end\":63159,\"start\":63155},{\"end\":63581,\"start\":63577},{\"end\":63595,\"start\":63589},{\"end\":63607,\"start\":63603},{\"end\":63620,\"start\":63614},{\"end\":63632,\"start\":63626},{\"end\":63643,\"start\":63638},{\"end\":64065,\"start\":64060},{\"end\":64080,\"start\":64073},{\"end\":64092,\"start\":64086},{\"end\":64109,\"start\":64102},{\"end\":64125,\"start\":64117},{\"end\":64141,\"start\":64135},{\"end\":64595,\"start\":64590},{\"end\":64614,\"start\":64606},{\"end\":64626,\"start\":64620},{\"end\":64877,\"start\":64870},{\"end\":64894,\"start\":64887},{\"end\":64910,\"start\":64904},{\"end\":65277,\"start\":65276},{\"end\":65293,\"start\":65288},{\"end\":65593,\"start\":65588},{\"end\":65606,\"start\":65601},{\"end\":65628,\"start\":65622},{\"end\":65947,\"start\":65942},{\"end\":65962,\"start\":65956},{\"end\":65976,\"start\":65972},{\"end\":66389,\"start\":66379},{\"end\":66399,\"start\":66395},{\"end\":66411,\"start\":66406},{\"end\":66855,\"start\":66848},{\"end\":66865,\"start\":66861},{\"end\":66883,\"start\":66877},{\"end\":66903,\"start\":66892},{\"end\":66918,\"start\":66912},{\"end\":66932,\"start\":66923},{\"end\":67288,\"start\":67282},{\"end\":67299,\"start\":67294},{\"end\":67309,\"start\":67307},{\"end\":67318,\"start\":67316},{\"end\":67333,\"start\":67325},{\"end\":67342,\"start\":67339},{\"end\":67655,\"start\":67648},{\"end\":67670,\"start\":67663},{\"end\":67686,\"start\":67680},{\"end\":67701,\"start\":67695},{\"end\":67722,\"start\":67713},{\"end\":68032,\"start\":68031},{\"end\":68048,\"start\":68042},{\"end\":68050,\"start\":68049},{\"end\":68374,\"start\":68368},{\"end\":68387,\"start\":68379},{\"end\":68402,\"start\":68395},{\"end\":68413,\"start\":68409},{\"end\":68427,\"start\":68420},{\"end\":68429,\"start\":68428},{\"end\":68912,\"start\":68906},{\"end\":68924,\"start\":68917},{\"end\":68937,\"start\":68931},{\"end\":68951,\"start\":68946},{\"end\":68967,\"start\":68961},{\"end\":68983,\"start\":68979},{\"end\":68997,\"start\":68990},{\"end\":68999,\"start\":68998},{\"end\":69449,\"start\":69443},{\"end\":69459,\"start\":69454},{\"end\":69472,\"start\":69467},{\"end\":69487,\"start\":69482},{\"end\":69502,\"start\":69494},{\"end\":69515,\"start\":69507},{\"end\":69528,\"start\":69523},{\"end\":69536,\"start\":69531},{\"end\":69985,\"start\":69981},{\"end\":70004,\"start\":69997},{\"end\":70021,\"start\":70014},{\"end\":70041,\"start\":70032},{\"end\":70058,\"start\":70051},{\"end\":70524,\"start\":70519},{\"end\":70540,\"start\":70537},{\"end\":70555,\"start\":70548},{\"end\":70557,\"start\":70556},{\"end\":70569,\"start\":70565},{\"end\":70974,\"start\":70971},{\"end\":70988,\"start\":70987},{\"end\":71004,\"start\":70997},{\"end\":71025,\"start\":71017},{\"end\":71027,\"start\":71026},{\"end\":71040,\"start\":71036},{\"end\":71052,\"start\":71049},{\"end\":71539,\"start\":71529},{\"end\":71557,\"start\":71551},{\"end\":71839,\"start\":71834},{\"end\":71854,\"start\":71849},{\"end\":71868,\"start\":71862},{\"end\":71884,\"start\":71876},{\"end\":71900,\"start\":71894},{\"end\":72366,\"start\":72361},{\"end\":72381,\"start\":72372},{\"end\":72393,\"start\":72387},{\"end\":72407,\"start\":72400},{\"end\":72417,\"start\":72414},{\"end\":72918,\"start\":72908},{\"end\":72930,\"start\":72925},{\"end\":72947,\"start\":72941},{\"end\":72963,\"start\":72956},{\"end\":72980,\"start\":72974},{\"end\":73509,\"start\":73500},{\"end\":73530,\"start\":73517},{\"end\":73543,\"start\":73537},{\"end\":74022,\"start\":74014},{\"end\":74044,\"start\":74030},{\"end\":74058,\"start\":74051},{\"end\":74071,\"start\":74066},{\"end\":74090,\"start\":74082},{\"end\":74108,\"start\":74101},{\"end\":74110,\"start\":74109},{\"end\":74617,\"start\":74609},{\"end\":74637,\"start\":74628},{\"end\":74651,\"start\":74647},{\"end\":74666,\"start\":74662},{\"end\":74677,\"start\":74676},{\"end\":74679,\"start\":74678},{\"end\":74696,\"start\":74687},{\"end\":74711,\"start\":74704},{\"end\":74713,\"start\":74712},{\"end\":75178,\"start\":75171},{\"end\":75190,\"start\":75185},{\"end\":75202,\"start\":75198},{\"end\":75216,\"start\":75209},{\"end\":75231,\"start\":75227},{\"end\":75250,\"start\":75243},{\"end\":75564,\"start\":75557},{\"end\":75578,\"start\":75571},{\"end\":75593,\"start\":75589},{\"end\":75609,\"start\":75605},{\"end\":75628,\"start\":75621},{\"end\":76085,\"start\":76081},{\"end\":76100,\"start\":76092},{\"end\":76115,\"start\":76108},{\"end\":76128,\"start\":76120},{\"end\":76139,\"start\":76135},{\"end\":76152,\"start\":76147},{\"end\":76165,\"start\":76158},{\"end\":76691,\"start\":76682},{\"end\":76709,\"start\":76701},{\"end\":76725,\"start\":76719},{\"end\":76742,\"start\":76736},{\"end\":77222,\"start\":77216},{\"end\":77239,\"start\":77234},{\"end\":77254,\"start\":77249},{\"end\":77268,\"start\":77259},{\"end\":77547,\"start\":77541},{\"end\":77565,\"start\":77558},{\"end\":77567,\"start\":77566},{\"end\":77581,\"start\":77575},{\"end\":77887,\"start\":77883},{\"end\":77899,\"start\":77893},{\"end\":77912,\"start\":77907},{\"end\":77923,\"start\":77919},{\"end\":77935,\"start\":77928},{\"end\":77952,\"start\":77943},{\"end\":78483,\"start\":78479},{\"end\":78504,\"start\":78497},{\"end\":78520,\"start\":78514},{\"end\":78969,\"start\":78961},{\"end\":78981,\"start\":78977},{\"end\":78994,\"start\":78989},{\"end\":79010,\"start\":79004},{\"end\":79028,\"start\":79022},{\"end\":79042,\"start\":79039},{\"end\":79584,\"start\":79576},{\"end\":79597,\"start\":79592},{\"end\":79610,\"start\":79605},{\"end\":79627,\"start\":79620},{\"end\":80104,\"start\":80096},{\"end\":80119,\"start\":80112},{\"end\":80132,\"start\":80126},{\"end\":80144,\"start\":80137},{\"end\":80146,\"start\":80145},{\"end\":80598,\"start\":80594},{\"end\":80618,\"start\":80612},{\"end\":80620,\"start\":80619},{\"end\":80632,\"start\":80629},{\"end\":80893,\"start\":80885},{\"end\":80905,\"start\":80901},{\"end\":81282,\"start\":81280},{\"end\":81296,\"start\":81289},{\"end\":81308,\"start\":81304},{\"end\":81310,\"start\":81309},{\"end\":81700,\"start\":81694},{\"end\":81715,\"start\":81708},{\"end\":81735,\"start\":81727},{\"end\":82064,\"start\":82059},{\"end\":82083,\"start\":82073},{\"end\":82575,\"start\":82568},{\"end\":82590,\"start\":82584},{\"end\":82594,\"start\":82591},{\"end\":82610,\"start\":82606},{\"end\":82623,\"start\":82617},{\"end\":83121,\"start\":83114},{\"end\":83138,\"start\":83130},{\"end\":83155,\"start\":83151},{\"end\":83168,\"start\":83162},{\"end\":83658,\"start\":83652},{\"end\":83673,\"start\":83669},{\"end\":83692,\"start\":83687},{\"end\":83704,\"start\":83701},{\"end\":84074,\"start\":84067},{\"end\":84088,\"start\":84081},{\"end\":84101,\"start\":84097},{\"end\":84601,\"start\":84594},{\"end\":84613,\"start\":84608},{\"end\":84632,\"start\":84626},{\"end\":84644,\"start\":84637},{\"end\":84657,\"start\":84653},{\"end\":84948,\"start\":84947},{\"end\":84964,\"start\":84959},{\"end\":84976,\"start\":84971},{\"end\":84992,\"start\":84985},{\"end\":85310,\"start\":85303},{\"end\":85331,\"start\":85317},{\"end\":85346,\"start\":85340},{\"end\":85371,\"start\":85363},{\"end\":85810,\"start\":85806},{\"end\":85824,\"start\":85817},{\"end\":85835,\"start\":85830},{\"end\":85845,\"start\":85843},{\"end\":85859,\"start\":85852},{\"end\":85870,\"start\":85864},{\"end\":85881,\"start\":85877},{\"end\":85883,\"start\":85882},{\"end\":86238,\"start\":86236},{\"end\":86253,\"start\":86245},{\"end\":86264,\"start\":86260},{\"end\":86282,\"start\":86278},{\"end\":86298,\"start\":86290},{\"end\":86310,\"start\":86303},{\"end\":86848,\"start\":86842},{\"end\":86859,\"start\":86856},{\"end\":86869,\"start\":86864},{\"end\":86883,\"start\":86875},{\"end\":87234,\"start\":87227},{\"end\":87247,\"start\":87241},{\"end\":87261,\"start\":87254},{\"end\":87525,\"start\":87519},{\"end\":87533,\"start\":87531},{\"end\":87543,\"start\":87539},{\"end\":87555,\"start\":87549},{\"end\":87566,\"start\":87562},{\"end\":87578,\"start\":87571},{\"end\":87593,\"start\":87585},{\"end\":87608,\"start\":87599}]", "bib_author_last_name": "[{\"end\":50813,\"start\":50803},{\"end\":50828,\"start\":50820},{\"end\":50848,\"start\":50838},{\"end\":50865,\"start\":50859},{\"end\":51231,\"start\":51226},{\"end\":51252,\"start\":51239},{\"end\":51265,\"start\":51260},{\"end\":51281,\"start\":51274},{\"end\":51299,\"start\":51290},{\"end\":51690,\"start\":51685},{\"end\":51705,\"start\":51698},{\"end\":51973,\"start\":51965},{\"end\":51992,\"start\":51984},{\"end\":52008,\"start\":52000},{\"end\":52026,\"start\":52018},{\"end\":52038,\"start\":52035},{\"end\":52585,\"start\":52565},{\"end\":52609,\"start\":52597},{\"end\":52626,\"start\":52618},{\"end\":52637,\"start\":52628},{\"end\":53090,\"start\":53070},{\"end\":53108,\"start\":53102},{\"end\":53125,\"start\":53117},{\"end\":53136,\"start\":53127},{\"end\":53625,\"start\":53620},{\"end\":53640,\"start\":53633},{\"end\":53659,\"start\":53650},{\"end\":53673,\"start\":53668},{\"end\":53685,\"start\":53682},{\"end\":53703,\"start\":53696},{\"end\":54168,\"start\":54163},{\"end\":54186,\"start\":54179},{\"end\":54200,\"start\":54195},{\"end\":54566,\"start\":54560},{\"end\":54581,\"start\":54574},{\"end\":54591,\"start\":54588},{\"end\":54607,\"start\":54600},{\"end\":54622,\"start\":54616},{\"end\":54640,\"start\":54631},{\"end\":54658,\"start\":54650},{\"end\":55228,\"start\":55224},{\"end\":55242,\"start\":55237},{\"end\":55253,\"start\":55246},{\"end\":55266,\"start\":55261},{\"end\":55284,\"start\":55276},{\"end\":55292,\"start\":55286},{\"end\":55761,\"start\":55757},{\"end\":55772,\"start\":55767},{\"end\":56172,\"start\":56165},{\"end\":56183,\"start\":56180},{\"end\":56201,\"start\":56192},{\"end\":56549,\"start\":56543},{\"end\":56566,\"start\":56558},{\"end\":56582,\"start\":56576},{\"end\":56600,\"start\":56591},{\"end\":56624,\"start\":56611},{\"end\":57084,\"start\":57067},{\"end\":57098,\"start\":57093},{\"end\":57118,\"start\":57110},{\"end\":57127,\"start\":57120},{\"end\":57406,\"start\":57402},{\"end\":57421,\"start\":57416},{\"end\":57440,\"start\":57430},{\"end\":57460,\"start\":57451},{\"end\":57477,\"start\":57471},{\"end\":57493,\"start\":57486},{\"end\":57507,\"start\":57495},{\"end\":57967,\"start\":57963},{\"end\":57981,\"start\":57973},{\"end\":57999,\"start\":57991},{\"end\":58011,\"start\":58008},{\"end\":58343,\"start\":58335},{\"end\":58361,\"start\":58354},{\"end\":58377,\"start\":58371},{\"end\":58391,\"start\":58388},{\"end\":58406,\"start\":58399},{\"end\":58421,\"start\":58416},{\"end\":58842,\"start\":58839},{\"end\":58850,\"start\":58848},{\"end\":58869,\"start\":58863},{\"end\":59317,\"start\":59313},{\"end\":59328,\"start\":59323},{\"end\":59341,\"start\":59334},{\"end\":59757,\"start\":59753},{\"end\":59765,\"start\":59763},{\"end\":59778,\"start\":59774},{\"end\":59792,\"start\":59788},{\"end\":59801,\"start\":59797},{\"end\":60214,\"start\":60208},{\"end\":60230,\"start\":60226},{\"end\":60243,\"start\":60240},{\"end\":60256,\"start\":60251},{\"end\":60275,\"start\":60265},{\"end\":60702,\"start\":60695},{\"end\":60718,\"start\":60712},{\"end\":60730,\"start\":60722},{\"end\":60735,\"start\":60732},{\"end\":60744,\"start\":60739},{\"end\":60761,\"start\":60754},{\"end\":60768,\"start\":60763},{\"end\":61196,\"start\":61189},{\"end\":61212,\"start\":61206},{\"end\":61228,\"start\":61225},{\"end\":61245,\"start\":61238},{\"end\":61260,\"start\":61255},{\"end\":61696,\"start\":61687},{\"end\":61703,\"start\":61698},{\"end\":61712,\"start\":61707},{\"end\":61733,\"start\":61724},{\"end\":61750,\"start\":61745},{\"end\":61757,\"start\":61752},{\"end\":62055,\"start\":62047},{\"end\":62074,\"start\":62064},{\"end\":62095,\"start\":62085},{\"end\":62106,\"start\":62102},{\"end\":62124,\"start\":62116},{\"end\":62136,\"start\":62133},{\"end\":62631,\"start\":62624},{\"end\":62644,\"start\":62642},{\"end\":62659,\"start\":62654},{\"end\":62672,\"start\":62666},{\"end\":62678,\"start\":62674},{\"end\":63115,\"start\":63110},{\"end\":63126,\"start\":63124},{\"end\":63145,\"start\":63138},{\"end\":63153,\"start\":63151},{\"end\":63164,\"start\":63160},{\"end\":63587,\"start\":63582},{\"end\":63601,\"start\":63596},{\"end\":63612,\"start\":63608},{\"end\":63624,\"start\":63621},{\"end\":63636,\"start\":63633},{\"end\":63647,\"start\":63644},{\"end\":64071,\"start\":64066},{\"end\":64084,\"start\":64081},{\"end\":64100,\"start\":64093},{\"end\":64115,\"start\":64110},{\"end\":64133,\"start\":64126},{\"end\":64152,\"start\":64142},{\"end\":64604,\"start\":64596},{\"end\":64618,\"start\":64615},{\"end\":64631,\"start\":64627},{\"end\":64639,\"start\":64633},{\"end\":64885,\"start\":64878},{\"end\":64902,\"start\":64895},{\"end\":64916,\"start\":64911},{\"end\":65286,\"start\":65278},{\"end\":65300,\"start\":65294},{\"end\":65304,\"start\":65302},{\"end\":65599,\"start\":65594},{\"end\":65620,\"start\":65607},{\"end\":65638,\"start\":65629},{\"end\":65954,\"start\":65948},{\"end\":65970,\"start\":65963},{\"end\":65981,\"start\":65977},{\"end\":66393,\"start\":66390},{\"end\":66404,\"start\":66400},{\"end\":66417,\"start\":66412},{\"end\":66859,\"start\":66856},{\"end\":66875,\"start\":66866},{\"end\":66890,\"start\":66884},{\"end\":66910,\"start\":66904},{\"end\":66921,\"start\":66919},{\"end\":66941,\"start\":66933},{\"end\":67292,\"start\":67289},{\"end\":67305,\"start\":67300},{\"end\":67314,\"start\":67310},{\"end\":67323,\"start\":67319},{\"end\":67337,\"start\":67334},{\"end\":67347,\"start\":67343},{\"end\":67661,\"start\":67656},{\"end\":67678,\"start\":67671},{\"end\":67693,\"start\":67687},{\"end\":67711,\"start\":67702},{\"end\":67728,\"start\":67723},{\"end\":68040,\"start\":68033},{\"end\":68059,\"start\":68051},{\"end\":68066,\"start\":68061},{\"end\":68377,\"start\":68375},{\"end\":68393,\"start\":68388},{\"end\":68407,\"start\":68403},{\"end\":68418,\"start\":68414},{\"end\":68435,\"start\":68430},{\"end\":68915,\"start\":68913},{\"end\":68929,\"start\":68925},{\"end\":68944,\"start\":68938},{\"end\":68959,\"start\":68952},{\"end\":68977,\"start\":68968},{\"end\":68988,\"start\":68984},{\"end\":69005,\"start\":69000},{\"end\":69452,\"start\":69450},{\"end\":69465,\"start\":69460},{\"end\":69480,\"start\":69473},{\"end\":69492,\"start\":69488},{\"end\":69505,\"start\":69503},{\"end\":69521,\"start\":69516},{\"end\":69543,\"start\":69537},{\"end\":69995,\"start\":69986},{\"end\":70012,\"start\":70005},{\"end\":70030,\"start\":70022},{\"end\":70049,\"start\":70042},{\"end\":70065,\"start\":70059},{\"end\":70535,\"start\":70525},{\"end\":70546,\"start\":70541},{\"end\":70563,\"start\":70558},{\"end\":70574,\"start\":70570},{\"end\":70985,\"start\":70975},{\"end\":70995,\"start\":70989},{\"end\":71015,\"start\":71005},{\"end\":71034,\"start\":71028},{\"end\":71047,\"start\":71041},{\"end\":71064,\"start\":71053},{\"end\":71068,\"start\":71066},{\"end\":71549,\"start\":71540},{\"end\":71564,\"start\":71558},{\"end\":71847,\"start\":71840},{\"end\":71860,\"start\":71855},{\"end\":71874,\"start\":71869},{\"end\":71892,\"start\":71885},{\"end\":71904,\"start\":71901},{\"end\":72370,\"start\":72367},{\"end\":72385,\"start\":72382},{\"end\":72398,\"start\":72394},{\"end\":72412,\"start\":72408},{\"end\":72421,\"start\":72418},{\"end\":72923,\"start\":72919},{\"end\":72939,\"start\":72931},{\"end\":72954,\"start\":72948},{\"end\":72972,\"start\":72964},{\"end\":72990,\"start\":72981},{\"end\":73515,\"start\":73510},{\"end\":73535,\"start\":73531},{\"end\":73553,\"start\":73544},{\"end\":74028,\"start\":74023},{\"end\":74049,\"start\":74045},{\"end\":74064,\"start\":74059},{\"end\":74080,\"start\":74072},{\"end\":74099,\"start\":74091},{\"end\":74116,\"start\":74111},{\"end\":74626,\"start\":74618},{\"end\":74645,\"start\":74638},{\"end\":74660,\"start\":74652},{\"end\":74674,\"start\":74667},{\"end\":74685,\"start\":74680},{\"end\":74702,\"start\":74697},{\"end\":74721,\"start\":74714},{\"end\":74728,\"start\":74723},{\"end\":75183,\"start\":75179},{\"end\":75196,\"start\":75191},{\"end\":75207,\"start\":75203},{\"end\":75225,\"start\":75217},{\"end\":75241,\"start\":75232},{\"end\":75257,\"start\":75251},{\"end\":75569,\"start\":75565},{\"end\":75587,\"start\":75579},{\"end\":75603,\"start\":75594},{\"end\":75619,\"start\":75610},{\"end\":75635,\"start\":75629},{\"end\":76090,\"start\":76086},{\"end\":76106,\"start\":76101},{\"end\":76118,\"start\":76116},{\"end\":76133,\"start\":76129},{\"end\":76145,\"start\":76140},{\"end\":76156,\"start\":76153},{\"end\":76170,\"start\":76166},{\"end\":76699,\"start\":76692},{\"end\":76717,\"start\":76710},{\"end\":76734,\"start\":76726},{\"end\":76748,\"start\":76743},{\"end\":77232,\"start\":77223},{\"end\":77247,\"start\":77240},{\"end\":77257,\"start\":77255},{\"end\":77274,\"start\":77269},{\"end\":77556,\"start\":77548},{\"end\":77573,\"start\":77568},{\"end\":77588,\"start\":77582},{\"end\":77891,\"start\":77888},{\"end\":77905,\"start\":77900},{\"end\":77917,\"start\":77913},{\"end\":77926,\"start\":77924},{\"end\":77941,\"start\":77936},{\"end\":77960,\"start\":77953},{\"end\":78356,\"start\":78344},{\"end\":78495,\"start\":78484},{\"end\":78512,\"start\":78505},{\"end\":78525,\"start\":78521},{\"end\":78975,\"start\":78970},{\"end\":78987,\"start\":78982},{\"end\":79002,\"start\":78995},{\"end\":79020,\"start\":79011},{\"end\":79037,\"start\":79029},{\"end\":79045,\"start\":79043},{\"end\":79590,\"start\":79585},{\"end\":79603,\"start\":79598},{\"end\":79618,\"start\":79611},{\"end\":79631,\"start\":79628},{\"end\":80110,\"start\":80105},{\"end\":80124,\"start\":80120},{\"end\":80135,\"start\":80133},{\"end\":80152,\"start\":80147},{\"end\":80610,\"start\":80599},{\"end\":80627,\"start\":80621},{\"end\":80638,\"start\":80633},{\"end\":80899,\"start\":80894},{\"end\":80916,\"start\":80906},{\"end\":81287,\"start\":81283},{\"end\":81302,\"start\":81297},{\"end\":81314,\"start\":81311},{\"end\":81706,\"start\":81701},{\"end\":81725,\"start\":81716},{\"end\":81743,\"start\":81736},{\"end\":82071,\"start\":82065},{\"end\":82093,\"start\":82084},{\"end\":82582,\"start\":82576},{\"end\":82604,\"start\":82595},{\"end\":82615,\"start\":82611},{\"end\":82633,\"start\":82624},{\"end\":83128,\"start\":83122},{\"end\":83149,\"start\":83139},{\"end\":83160,\"start\":83156},{\"end\":83178,\"start\":83169},{\"end\":83667,\"start\":83659},{\"end\":83685,\"start\":83674},{\"end\":83699,\"start\":83693},{\"end\":83710,\"start\":83705},{\"end\":84079,\"start\":84075},{\"end\":84095,\"start\":84089},{\"end\":84106,\"start\":84102},{\"end\":84606,\"start\":84602},{\"end\":84624,\"start\":84614},{\"end\":84635,\"start\":84633},{\"end\":84651,\"start\":84645},{\"end\":84662,\"start\":84658},{\"end\":84957,\"start\":84949},{\"end\":84969,\"start\":84965},{\"end\":84983,\"start\":84977},{\"end\":85000,\"start\":84993},{\"end\":85007,\"start\":85002},{\"end\":85315,\"start\":85311},{\"end\":85338,\"start\":85332},{\"end\":85361,\"start\":85347},{\"end\":85378,\"start\":85372},{\"end\":85815,\"start\":85811},{\"end\":85828,\"start\":85825},{\"end\":85841,\"start\":85836},{\"end\":85850,\"start\":85846},{\"end\":85862,\"start\":85860},{\"end\":85875,\"start\":85871},{\"end\":85887,\"start\":85884},{\"end\":86243,\"start\":86239},{\"end\":86258,\"start\":86254},{\"end\":86276,\"start\":86265},{\"end\":86288,\"start\":86283},{\"end\":86301,\"start\":86299},{\"end\":86314,\"start\":86311},{\"end\":86854,\"start\":86849},{\"end\":86862,\"start\":86860},{\"end\":86873,\"start\":86870},{\"end\":86887,\"start\":86884},{\"end\":87239,\"start\":87235},{\"end\":87252,\"start\":87248},{\"end\":87268,\"start\":87262},{\"end\":87529,\"start\":87526},{\"end\":87537,\"start\":87534},{\"end\":87547,\"start\":87544},{\"end\":87560,\"start\":87556},{\"end\":87569,\"start\":87567},{\"end\":87583,\"start\":87579},{\"end\":87597,\"start\":87594},{\"end\":87612,\"start\":87609}]", "bib_entry": "[{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b0\",\"matched_paper_id\":23102425},\"end\":51186,\"start\":50729},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":195069368},\"end\":51628,\"start\":51188},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13936317},\"end\":51886,\"start\":51630},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":208291283},\"end\":52436,\"start\":51888},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":225062527},\"end\":53000,\"start\":52438},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":201070729},\"end\":53536,\"start\":53002},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":227254347},\"end\":54094,\"start\":53538},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":233204431},\"end\":54550,\"start\":54096},{\"attributes\":{\"id\":\"b8\"},\"end\":54770,\"start\":54552},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":214623267},\"end\":55132,\"start\":54772},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":233181855},\"end\":55692,\"start\":55134},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":54457478},\"end\":56092,\"start\":55694},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":225076487},\"end\":56473,\"start\":56094},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":232185555},\"end\":57020,\"start\":56475},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2240711},\"end\":57353,\"start\":57022},{\"attributes\":{\"id\":\"b15\"},\"end\":57891,\"start\":57355},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":222341810},\"end\":58259,\"start\":57893},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":199551781},\"end\":58747,\"start\":58261},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6746759},\"end\":59239,\"start\":58749},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6094650},\"end\":59660,\"start\":59241},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3996281},\"end\":60157,\"start\":59662},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":219631990},\"end\":60634,\"start\":60159},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":51866830},\"end\":61119,\"start\":60636},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3656527},\"end\":61649,\"start\":61121},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207194165},\"end\":61970,\"start\":61651},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":53763742},\"end\":62551,\"start\":61972},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":237195035},\"end\":63052,\"start\":62553},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":215548941},\"end\":63517,\"start\":63054},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":214743480},\"end\":64007,\"start\":63519},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":214606025},\"end\":64538,\"start\":64009},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":54458149},\"end\":64836,\"start\":64540},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14224},\"end\":65230,\"start\":64838},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6628106},\"end\":65515,\"start\":65232},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":221517012},\"end\":65884,\"start\":65517},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":51967305},\"end\":66299,\"start\":65886},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":12973135},\"end\":66770,\"start\":66301},{\"attributes\":{\"id\":\"b36\"},\"end\":67190,\"start\":66772},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":196834809},\"end\":67604,\"start\":67192},{\"attributes\":{\"id\":\"b38\"},\"end\":67960,\"start\":67606},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":15545924},\"end\":68283,\"start\":67962},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":233240644},\"end\":68852,\"start\":68285},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":209386419},\"end\":69420,\"start\":68854},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":233210608},\"end\":69913,\"start\":69422},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":54465161},\"end\":70469,\"start\":69915},{\"attributes\":{\"doi\":\"2021. 3\",\"id\":\"b44\",\"matched_paper_id\":233231226},\"end\":70897,\"start\":70471},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":213175590},\"end\":71472,\"start\":70899},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":1822757},\"end\":71781,\"start\":71474},{\"attributes\":{\"doi\":\"2021. 3\",\"id\":\"b47\",\"matched_paper_id\":233004645},\"end\":72275,\"start\":71783},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":202121070},\"end\":72825,\"start\":72277},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":58007025},\"end\":73407,\"start\":72827},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":212725694},\"end\":73949,\"start\":73409},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":233476374},\"end\":74536,\"start\":73951},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":109932872},\"end\":75169,\"start\":74538},{\"attributes\":{\"id\":\"b53\"},\"end\":75521,\"start\":75171},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":212646575},\"end\":75961,\"start\":75523},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":229924396},\"end\":76629,\"start\":75963},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":1756681},\"end\":77158,\"start\":76631},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":12569740},\"end\":77492,\"start\":77160},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":221139577},\"end\":77826,\"start\":77494},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":229363716},\"end\":78340,\"start\":77828},{\"attributes\":{\"id\":\"b60\"},\"end\":78412,\"start\":78342},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":3719281},\"end\":78873,\"start\":78414},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":152282359},\"end\":79479,\"start\":78875},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":214743286},\"end\":80020,\"start\":79481},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":233169009},\"end\":80535,\"start\":80022},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":81979618},\"end\":80830,\"start\":80537},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":218502297},\"end\":81220,\"start\":80832},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":221038849},\"end\":81626,\"start\":81222},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":219950625},\"end\":81964,\"start\":81628},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":9819914},\"end\":82474,\"start\":81966},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":220713845},\"end\":83028,\"start\":82476},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":237213375},\"end\":83577,\"start\":83030},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":221586450},\"end\":83987,\"start\":83579},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":233225657},\"end\":84516,\"start\":83989},{\"attributes\":{\"id\":\"b74\"},\"end\":84884,\"start\":84518},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":49551925},\"end\":85232,\"start\":84886},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":52956838},\"end\":85744,\"start\":85234},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":54167364},\"end\":86130,\"start\":85746},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":231632441},\"end\":86742,\"start\":86132},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":220404307},\"end\":87176,\"start\":86744},{\"attributes\":{\"doi\":\"arXiv preprint: 1801.09847\",\"id\":\"b80\"},\"end\":87427,\"start\":87178},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":214714400},\"end\":88034,\"start\":87429}]", "bib_title": "[{\"end\":50795,\"start\":50729},{\"end\":51215,\"start\":51188},{\"end\":51678,\"start\":51630},{\"end\":51959,\"start\":51888},{\"end\":52554,\"start\":52438},{\"end\":53060,\"start\":53002},{\"end\":53612,\"start\":53538},{\"end\":54154,\"start\":54096},{\"end\":54847,\"start\":54772},{\"end\":55219,\"start\":55134},{\"end\":55748,\"start\":55694},{\"end\":56156,\"start\":56094},{\"end\":56535,\"start\":56475},{\"end\":57058,\"start\":57022},{\"end\":57393,\"start\":57355},{\"end\":57953,\"start\":57893},{\"end\":58328,\"start\":58261},{\"end\":58828,\"start\":58749},{\"end\":59304,\"start\":59241},{\"end\":59747,\"start\":59662},{\"end\":60201,\"start\":60159},{\"end\":60684,\"start\":60636},{\"end\":61178,\"start\":61121},{\"end\":61677,\"start\":61651},{\"end\":62039,\"start\":61972},{\"end\":62615,\"start\":62553},{\"end\":63103,\"start\":63054},{\"end\":63575,\"start\":63519},{\"end\":64058,\"start\":64009},{\"end\":64588,\"start\":64540},{\"end\":64868,\"start\":64838},{\"end\":65274,\"start\":65232},{\"end\":65586,\"start\":65517},{\"end\":65940,\"start\":65886},{\"end\":66377,\"start\":66301},{\"end\":67280,\"start\":67192},{\"end\":67646,\"start\":67606},{\"end\":68029,\"start\":67962},{\"end\":68366,\"start\":68285},{\"end\":68904,\"start\":68854},{\"end\":69441,\"start\":69422},{\"end\":69979,\"start\":69915},{\"end\":70517,\"start\":70471},{\"end\":70969,\"start\":70899},{\"end\":71527,\"start\":71474},{\"end\":71832,\"start\":71783},{\"end\":72359,\"start\":72277},{\"end\":72906,\"start\":72827},{\"end\":73498,\"start\":73409},{\"end\":74012,\"start\":73951},{\"end\":74607,\"start\":74538},{\"end\":75555,\"start\":75523},{\"end\":76079,\"start\":75963},{\"end\":76680,\"start\":76631},{\"end\":77214,\"start\":77160},{\"end\":77539,\"start\":77494},{\"end\":77881,\"start\":77828},{\"end\":78477,\"start\":78414},{\"end\":78959,\"start\":78875},{\"end\":79574,\"start\":79481},{\"end\":80094,\"start\":80022},{\"end\":80592,\"start\":80537},{\"end\":80883,\"start\":80832},{\"end\":81278,\"start\":81222},{\"end\":81692,\"start\":81628},{\"end\":82057,\"start\":81966},{\"end\":82566,\"start\":82476},{\"end\":83112,\"start\":83030},{\"end\":83650,\"start\":83579},{\"end\":84065,\"start\":83989},{\"end\":84945,\"start\":84886},{\"end\":85301,\"start\":85234},{\"end\":85804,\"start\":85746},{\"end\":86234,\"start\":86132},{\"end\":86840,\"start\":86744},{\"end\":87517,\"start\":87429}]", "bib_author": "[{\"end\":50815,\"start\":50797},{\"end\":50830,\"start\":50815},{\"end\":50850,\"start\":50830},{\"end\":50867,\"start\":50850},{\"end\":51233,\"start\":51217},{\"end\":51254,\"start\":51233},{\"end\":51267,\"start\":51254},{\"end\":51283,\"start\":51267},{\"end\":51301,\"start\":51283},{\"end\":51692,\"start\":51680},{\"end\":51707,\"start\":51692},{\"end\":51975,\"start\":51961},{\"end\":51994,\"start\":51975},{\"end\":52010,\"start\":51994},{\"end\":52028,\"start\":52010},{\"end\":52040,\"start\":52028},{\"end\":52587,\"start\":52556},{\"end\":52611,\"start\":52587},{\"end\":52628,\"start\":52611},{\"end\":52639,\"start\":52628},{\"end\":53092,\"start\":53062},{\"end\":53110,\"start\":53092},{\"end\":53127,\"start\":53110},{\"end\":53138,\"start\":53127},{\"end\":53627,\"start\":53614},{\"end\":53642,\"start\":53627},{\"end\":53661,\"start\":53642},{\"end\":53675,\"start\":53661},{\"end\":53687,\"start\":53675},{\"end\":53705,\"start\":53687},{\"end\":54170,\"start\":54156},{\"end\":54188,\"start\":54170},{\"end\":54202,\"start\":54188},{\"end\":54568,\"start\":54554},{\"end\":54583,\"start\":54568},{\"end\":54593,\"start\":54583},{\"end\":54609,\"start\":54593},{\"end\":54624,\"start\":54609},{\"end\":54642,\"start\":54624},{\"end\":54660,\"start\":54642},{\"end\":55230,\"start\":55221},{\"end\":55244,\"start\":55230},{\"end\":55255,\"start\":55244},{\"end\":55268,\"start\":55255},{\"end\":55286,\"start\":55268},{\"end\":55294,\"start\":55286},{\"end\":55763,\"start\":55750},{\"end\":55774,\"start\":55763},{\"end\":56174,\"start\":56158},{\"end\":56185,\"start\":56174},{\"end\":56203,\"start\":56185},{\"end\":56551,\"start\":56537},{\"end\":56568,\"start\":56551},{\"end\":56584,\"start\":56568},{\"end\":56602,\"start\":56584},{\"end\":56626,\"start\":56602},{\"end\":57086,\"start\":57060},{\"end\":57100,\"start\":57086},{\"end\":57120,\"start\":57100},{\"end\":57129,\"start\":57120},{\"end\":57408,\"start\":57395},{\"end\":57423,\"start\":57408},{\"end\":57442,\"start\":57423},{\"end\":57462,\"start\":57442},{\"end\":57479,\"start\":57462},{\"end\":57495,\"start\":57479},{\"end\":57509,\"start\":57495},{\"end\":57969,\"start\":57955},{\"end\":57983,\"start\":57969},{\"end\":58001,\"start\":57983},{\"end\":58013,\"start\":58001},{\"end\":58345,\"start\":58330},{\"end\":58363,\"start\":58345},{\"end\":58379,\"start\":58363},{\"end\":58393,\"start\":58379},{\"end\":58408,\"start\":58393},{\"end\":58423,\"start\":58408},{\"end\":58844,\"start\":58830},{\"end\":58852,\"start\":58844},{\"end\":58871,\"start\":58852},{\"end\":59319,\"start\":59306},{\"end\":59330,\"start\":59319},{\"end\":59343,\"start\":59330},{\"end\":59759,\"start\":59749},{\"end\":59767,\"start\":59759},{\"end\":59780,\"start\":59767},{\"end\":59794,\"start\":59780},{\"end\":59803,\"start\":59794},{\"end\":60216,\"start\":60203},{\"end\":60232,\"start\":60216},{\"end\":60245,\"start\":60232},{\"end\":60258,\"start\":60245},{\"end\":60277,\"start\":60258},{\"end\":60704,\"start\":60686},{\"end\":60720,\"start\":60704},{\"end\":60732,\"start\":60720},{\"end\":60737,\"start\":60732},{\"end\":60746,\"start\":60737},{\"end\":60763,\"start\":60746},{\"end\":60770,\"start\":60763},{\"end\":61198,\"start\":61180},{\"end\":61214,\"start\":61198},{\"end\":61230,\"start\":61214},{\"end\":61247,\"start\":61230},{\"end\":61262,\"start\":61247},{\"end\":61698,\"start\":61679},{\"end\":61705,\"start\":61698},{\"end\":61714,\"start\":61705},{\"end\":61735,\"start\":61714},{\"end\":61752,\"start\":61735},{\"end\":61759,\"start\":61752},{\"end\":62057,\"start\":62041},{\"end\":62076,\"start\":62057},{\"end\":62097,\"start\":62076},{\"end\":62108,\"start\":62097},{\"end\":62126,\"start\":62108},{\"end\":62138,\"start\":62126},{\"end\":62633,\"start\":62617},{\"end\":62646,\"start\":62633},{\"end\":62661,\"start\":62646},{\"end\":62674,\"start\":62661},{\"end\":62680,\"start\":62674},{\"end\":63117,\"start\":63105},{\"end\":63128,\"start\":63117},{\"end\":63147,\"start\":63128},{\"end\":63155,\"start\":63147},{\"end\":63166,\"start\":63155},{\"end\":63589,\"start\":63577},{\"end\":63603,\"start\":63589},{\"end\":63614,\"start\":63603},{\"end\":63626,\"start\":63614},{\"end\":63638,\"start\":63626},{\"end\":63649,\"start\":63638},{\"end\":64073,\"start\":64060},{\"end\":64086,\"start\":64073},{\"end\":64102,\"start\":64086},{\"end\":64117,\"start\":64102},{\"end\":64135,\"start\":64117},{\"end\":64154,\"start\":64135},{\"end\":64606,\"start\":64590},{\"end\":64620,\"start\":64606},{\"end\":64633,\"start\":64620},{\"end\":64641,\"start\":64633},{\"end\":64887,\"start\":64870},{\"end\":64904,\"start\":64887},{\"end\":64918,\"start\":64904},{\"end\":65288,\"start\":65276},{\"end\":65302,\"start\":65288},{\"end\":65306,\"start\":65302},{\"end\":65601,\"start\":65588},{\"end\":65622,\"start\":65601},{\"end\":65640,\"start\":65622},{\"end\":65956,\"start\":65942},{\"end\":65972,\"start\":65956},{\"end\":65983,\"start\":65972},{\"end\":66395,\"start\":66379},{\"end\":66406,\"start\":66395},{\"end\":66419,\"start\":66406},{\"end\":66861,\"start\":66848},{\"end\":66877,\"start\":66861},{\"end\":66892,\"start\":66877},{\"end\":66912,\"start\":66892},{\"end\":66923,\"start\":66912},{\"end\":66943,\"start\":66923},{\"end\":67294,\"start\":67282},{\"end\":67307,\"start\":67294},{\"end\":67316,\"start\":67307},{\"end\":67325,\"start\":67316},{\"end\":67339,\"start\":67325},{\"end\":67349,\"start\":67339},{\"end\":67663,\"start\":67648},{\"end\":67680,\"start\":67663},{\"end\":67695,\"start\":67680},{\"end\":67713,\"start\":67695},{\"end\":67730,\"start\":67713},{\"end\":68042,\"start\":68031},{\"end\":68061,\"start\":68042},{\"end\":68068,\"start\":68061},{\"end\":68379,\"start\":68368},{\"end\":68395,\"start\":68379},{\"end\":68409,\"start\":68395},{\"end\":68420,\"start\":68409},{\"end\":68437,\"start\":68420},{\"end\":68917,\"start\":68906},{\"end\":68931,\"start\":68917},{\"end\":68946,\"start\":68931},{\"end\":68961,\"start\":68946},{\"end\":68979,\"start\":68961},{\"end\":68990,\"start\":68979},{\"end\":69007,\"start\":68990},{\"end\":69454,\"start\":69443},{\"end\":69467,\"start\":69454},{\"end\":69482,\"start\":69467},{\"end\":69494,\"start\":69482},{\"end\":69507,\"start\":69494},{\"end\":69523,\"start\":69507},{\"end\":69531,\"start\":69523},{\"end\":69545,\"start\":69531},{\"end\":69997,\"start\":69981},{\"end\":70014,\"start\":69997},{\"end\":70032,\"start\":70014},{\"end\":70051,\"start\":70032},{\"end\":70067,\"start\":70051},{\"end\":70537,\"start\":70519},{\"end\":70548,\"start\":70537},{\"end\":70565,\"start\":70548},{\"end\":70576,\"start\":70565},{\"end\":70987,\"start\":70971},{\"end\":70997,\"start\":70987},{\"end\":71017,\"start\":70997},{\"end\":71036,\"start\":71017},{\"end\":71049,\"start\":71036},{\"end\":71066,\"start\":71049},{\"end\":71070,\"start\":71066},{\"end\":71551,\"start\":71529},{\"end\":71566,\"start\":71551},{\"end\":71849,\"start\":71834},{\"end\":71862,\"start\":71849},{\"end\":71876,\"start\":71862},{\"end\":71894,\"start\":71876},{\"end\":71906,\"start\":71894},{\"end\":72372,\"start\":72361},{\"end\":72387,\"start\":72372},{\"end\":72400,\"start\":72387},{\"end\":72414,\"start\":72400},{\"end\":72423,\"start\":72414},{\"end\":72925,\"start\":72908},{\"end\":72941,\"start\":72925},{\"end\":72956,\"start\":72941},{\"end\":72974,\"start\":72956},{\"end\":72992,\"start\":72974},{\"end\":73517,\"start\":73500},{\"end\":73537,\"start\":73517},{\"end\":73555,\"start\":73537},{\"end\":74030,\"start\":74014},{\"end\":74051,\"start\":74030},{\"end\":74066,\"start\":74051},{\"end\":74082,\"start\":74066},{\"end\":74101,\"start\":74082},{\"end\":74118,\"start\":74101},{\"end\":74628,\"start\":74609},{\"end\":74647,\"start\":74628},{\"end\":74662,\"start\":74647},{\"end\":74676,\"start\":74662},{\"end\":74687,\"start\":74676},{\"end\":74704,\"start\":74687},{\"end\":74723,\"start\":74704},{\"end\":74730,\"start\":74723},{\"end\":75185,\"start\":75171},{\"end\":75198,\"start\":75185},{\"end\":75209,\"start\":75198},{\"end\":75227,\"start\":75209},{\"end\":75243,\"start\":75227},{\"end\":75259,\"start\":75243},{\"end\":75571,\"start\":75557},{\"end\":75589,\"start\":75571},{\"end\":75605,\"start\":75589},{\"end\":75621,\"start\":75605},{\"end\":75637,\"start\":75621},{\"end\":76092,\"start\":76081},{\"end\":76108,\"start\":76092},{\"end\":76120,\"start\":76108},{\"end\":76135,\"start\":76120},{\"end\":76147,\"start\":76135},{\"end\":76158,\"start\":76147},{\"end\":76172,\"start\":76158},{\"end\":76701,\"start\":76682},{\"end\":76719,\"start\":76701},{\"end\":76736,\"start\":76719},{\"end\":76750,\"start\":76736},{\"end\":77234,\"start\":77216},{\"end\":77249,\"start\":77234},{\"end\":77259,\"start\":77249},{\"end\":77276,\"start\":77259},{\"end\":77558,\"start\":77541},{\"end\":77575,\"start\":77558},{\"end\":77590,\"start\":77575},{\"end\":77893,\"start\":77883},{\"end\":77907,\"start\":77893},{\"end\":77919,\"start\":77907},{\"end\":77928,\"start\":77919},{\"end\":77943,\"start\":77928},{\"end\":77962,\"start\":77943},{\"end\":78358,\"start\":78344},{\"end\":78497,\"start\":78479},{\"end\":78514,\"start\":78497},{\"end\":78527,\"start\":78514},{\"end\":78977,\"start\":78961},{\"end\":78989,\"start\":78977},{\"end\":79004,\"start\":78989},{\"end\":79022,\"start\":79004},{\"end\":79039,\"start\":79022},{\"end\":79047,\"start\":79039},{\"end\":79592,\"start\":79576},{\"end\":79605,\"start\":79592},{\"end\":79620,\"start\":79605},{\"end\":79633,\"start\":79620},{\"end\":80112,\"start\":80096},{\"end\":80126,\"start\":80112},{\"end\":80137,\"start\":80126},{\"end\":80154,\"start\":80137},{\"end\":80612,\"start\":80594},{\"end\":80629,\"start\":80612},{\"end\":80640,\"start\":80629},{\"end\":80901,\"start\":80885},{\"end\":80918,\"start\":80901},{\"end\":81289,\"start\":81280},{\"end\":81304,\"start\":81289},{\"end\":81316,\"start\":81304},{\"end\":81708,\"start\":81694},{\"end\":81727,\"start\":81708},{\"end\":81745,\"start\":81727},{\"end\":82073,\"start\":82059},{\"end\":82095,\"start\":82073},{\"end\":82584,\"start\":82568},{\"end\":82606,\"start\":82584},{\"end\":82617,\"start\":82606},{\"end\":82635,\"start\":82617},{\"end\":83130,\"start\":83114},{\"end\":83151,\"start\":83130},{\"end\":83162,\"start\":83151},{\"end\":83180,\"start\":83162},{\"end\":83669,\"start\":83652},{\"end\":83687,\"start\":83669},{\"end\":83701,\"start\":83687},{\"end\":83712,\"start\":83701},{\"end\":84081,\"start\":84067},{\"end\":84097,\"start\":84081},{\"end\":84108,\"start\":84097},{\"end\":84608,\"start\":84594},{\"end\":84626,\"start\":84608},{\"end\":84637,\"start\":84626},{\"end\":84653,\"start\":84637},{\"end\":84664,\"start\":84653},{\"end\":84959,\"start\":84947},{\"end\":84971,\"start\":84959},{\"end\":84985,\"start\":84971},{\"end\":85002,\"start\":84985},{\"end\":85009,\"start\":85002},{\"end\":85317,\"start\":85303},{\"end\":85340,\"start\":85317},{\"end\":85363,\"start\":85340},{\"end\":85380,\"start\":85363},{\"end\":85817,\"start\":85806},{\"end\":85830,\"start\":85817},{\"end\":85843,\"start\":85830},{\"end\":85852,\"start\":85843},{\"end\":85864,\"start\":85852},{\"end\":85877,\"start\":85864},{\"end\":85889,\"start\":85877},{\"end\":86245,\"start\":86236},{\"end\":86260,\"start\":86245},{\"end\":86278,\"start\":86260},{\"end\":86290,\"start\":86278},{\"end\":86303,\"start\":86290},{\"end\":86316,\"start\":86303},{\"end\":86856,\"start\":86842},{\"end\":86864,\"start\":86856},{\"end\":86875,\"start\":86864},{\"end\":86889,\"start\":86875},{\"end\":87241,\"start\":87227},{\"end\":87254,\"start\":87241},{\"end\":87270,\"start\":87254},{\"end\":87531,\"start\":87519},{\"end\":87539,\"start\":87531},{\"end\":87549,\"start\":87539},{\"end\":87562,\"start\":87549},{\"end\":87571,\"start\":87562},{\"end\":87585,\"start\":87571},{\"end\":87599,\"start\":87585},{\"end\":87614,\"start\":87599}]", "bib_venue": "[{\"end\":50921,\"start\":50871},{\"end\":51365,\"start\":51301},{\"end\":51741,\"start\":51707},{\"end\":52112,\"start\":52040},{\"end\":52698,\"start\":52639},{\"end\":53216,\"start\":53138},{\"end\":53777,\"start\":53705},{\"end\":54280,\"start\":54202},{\"end\":54913,\"start\":54849},{\"end\":55372,\"start\":55294},{\"end\":55846,\"start\":55774},{\"end\":56262,\"start\":56203},{\"end\":56698,\"start\":56626},{\"end\":57160,\"start\":57129},{\"end\":57573,\"start\":57509},{\"end\":58056,\"start\":58013},{\"end\":58482,\"start\":58423},{\"end\":58943,\"start\":58871},{\"end\":59408,\"start\":59343},{\"end\":59867,\"start\":59803},{\"end\":60349,\"start\":60277},{\"end\":60834,\"start\":60770},{\"end\":61334,\"start\":61262},{\"end\":61793,\"start\":61759},{\"end\":62210,\"start\":62138},{\"end\":62758,\"start\":62680},{\"end\":63238,\"start\":63166},{\"end\":63713,\"start\":63649},{\"end\":64226,\"start\":64154},{\"end\":64664,\"start\":64641},{\"end\":64989,\"start\":64918},{\"end\":65365,\"start\":65306},{\"end\":65683,\"start\":65640},{\"end\":66047,\"start\":65983},{\"end\":66487,\"start\":66419},{\"end\":66846,\"start\":66772},{\"end\":67383,\"start\":67349},{\"end\":67764,\"start\":67730},{\"end\":68098,\"start\":68068},{\"end\":68513,\"start\":68437},{\"end\":69079,\"start\":69007},{\"end\":69617,\"start\":69545},{\"end\":70139,\"start\":70067},{\"end\":70648,\"start\":70583},{\"end\":71134,\"start\":71070},{\"end\":71609,\"start\":71566},{\"end\":71990,\"start\":71913},{\"end\":72501,\"start\":72423},{\"end\":73064,\"start\":72992},{\"end\":73627,\"start\":73555},{\"end\":74190,\"start\":74118},{\"end\":74802,\"start\":74730},{\"end\":75307,\"start\":75259},{\"end\":75701,\"start\":75637},{\"end\":76244,\"start\":76172},{\"end\":76839,\"start\":76750},{\"end\":77310,\"start\":77276},{\"end\":77649,\"start\":77590},{\"end\":78034,\"start\":77962},{\"end\":78622,\"start\":78527},{\"end\":79125,\"start\":79047},{\"end\":79705,\"start\":79633},{\"end\":80230,\"start\":80154},{\"end\":80663,\"start\":80640},{\"end\":80982,\"start\":80918},{\"end\":81380,\"start\":81316},{\"end\":81779,\"start\":81745},{\"end\":82167,\"start\":82095},{\"end\":82699,\"start\":82635},{\"end\":83258,\"start\":83180},{\"end\":83735,\"start\":83712},{\"end\":84196,\"start\":84108},{\"end\":84592,\"start\":84518},{\"end\":85043,\"start\":85009},{\"end\":85444,\"start\":85380},{\"end\":85923,\"start\":85889},{\"end\":86388,\"start\":86316},{\"end\":86950,\"start\":86889},{\"end\":87225,\"start\":87178},{\"end\":87678,\"start\":87614},{\"end\":50958,\"start\":50923},{\"end\":51416,\"start\":51367},{\"end\":52174,\"start\":52114},{\"end\":53281,\"start\":53218},{\"end\":53839,\"start\":53779},{\"end\":54345,\"start\":54282},{\"end\":54964,\"start\":54915},{\"end\":55437,\"start\":55374},{\"end\":55908,\"start\":55848},{\"end\":56760,\"start\":56700},{\"end\":57624,\"start\":57575},{\"end\":59005,\"start\":58945},{\"end\":59460,\"start\":59410},{\"end\":59918,\"start\":59869},{\"end\":60411,\"start\":60351},{\"end\":60885,\"start\":60836},{\"end\":61396,\"start\":61336},{\"end\":62272,\"start\":62212},{\"end\":62823,\"start\":62760},{\"end\":63300,\"start\":63240},{\"end\":63764,\"start\":63715},{\"end\":64288,\"start\":64228},{\"end\":65047,\"start\":64991},{\"end\":66098,\"start\":66049},{\"end\":66542,\"start\":66489},{\"end\":68579,\"start\":68515},{\"end\":69141,\"start\":69081},{\"end\":69679,\"start\":69619},{\"end\":70201,\"start\":70141},{\"end\":70703,\"start\":70650},{\"end\":71185,\"start\":71136},{\"end\":72054,\"start\":71992},{\"end\":72566,\"start\":72503},{\"end\":73126,\"start\":73066},{\"end\":73689,\"start\":73629},{\"end\":74252,\"start\":74192},{\"end\":74864,\"start\":74804},{\"end\":75752,\"start\":75703},{\"end\":76306,\"start\":76246},{\"end\":76915,\"start\":76841},{\"end\":78096,\"start\":78036},{\"end\":79190,\"start\":79127},{\"end\":79767,\"start\":79707},{\"end\":80296,\"start\":80232},{\"end\":81033,\"start\":80984},{\"end\":81431,\"start\":81382},{\"end\":82229,\"start\":82169},{\"end\":82750,\"start\":82701},{\"end\":83323,\"start\":83260},{\"end\":84271,\"start\":84198},{\"end\":85495,\"start\":85446},{\"end\":86450,\"start\":86390},{\"end\":87729,\"start\":87680}]"}}}, "year": 2023, "month": 12, "day": 17}